Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 10?18,Baltimore, Maryland USA, June 27 2014.c?2014 Association for Computational LinguisticsThe error-driven ranking model of the acquisition of phonotactics:how to keep the faithfulness constraints at bayGiorgio MagriSFL (CNRS and University of Paris 8)UiL OTS (Utrecht University)magrigrg@gmail.comAbstractA problem which arises in the theory ofthe error-driven ranking model of the ac-quisition of phonotactics is that the faith-fulness constraints need to be promotedbut should not be promoted too high.
Thispaper motivates this technical problem andshows how to tune the promotion compo-nent of the re-ranking rule so as to keep thefaithfulness constraints at bay.Sections 1-2 introduce the algorithmic frame-work considered in the paper, namely the error-driven ranking model of the acquisition of phono-tactics.
Section 3 motivates a specific problemwhich arises in the design and analysis of thismodel, namely the problem of controlling theheight reached by the faithfulness (F) constraints.Sections 4-6 sketch the theory of F-controlling.Magri (2014a) presents the theory in more detail.1 The acquisition of phonotacticsGenerative linguistics assumes that the learner isprovided with a typology of grammars G1, G2...The language-learning problem thus consists ofindividuating the target adult grammar G?withinthe typology, on the basis of a finite set of datagenerated by that grammar.
Various formulationsof this problem differ for the structural assump-tions about the underlying typology, for the typeof data fed to the learner, and for the criteria ofsuccess used to evaluate the grammar?G chosenby the learner relative to the target grammar G?.In this paper, I focus on the following spe-cific formulation of this general language learn-ing problem.
The typology consists of the phono-logical grammars defined in Optimality Theoretic(OT) terms through the rankings of a given set ofconstraints (Prince and Smolensky, 2004).
Thedata fed to the learner consist of surface formssampled from the language L?generated by thetarget OT grammar G?, namely the set of surfaceforms which are the phonological realizations ofsome underlying forms according to G?.
The cri-teria for success is that the OT grammar?G chosenby the learner generates a language?L which coin-cides with the target one:?L = L?.This specific formulation is called the problemof the acquisition of phonotactics.
In fact, phono-tactics is the knowledge of the distinction betweenlicit and illicit forms.
Assuming that the distinc-tion is categorical (Gorman, 2013), knowledge ofphonotactics reduces to knowledge of the set oflicit forms (the set of illicit forms is just the com-plement).
And the set of licit forms relative to anOT grammarG is the corresponding language LG.2 The EDRA modelIn this paper, I focus on a specific algorithmic ap-proach to the problem of the acquisition of phono-tactics, based on error-driven ranking algorithms(EDRAs).
This approach is summarized belowand explained in the rest of this section.Algorithm 1 The EDRA modelInitializethe ranking values of F constraints to zerothe ranking values ofM constraints to ?init>0Repeat1 get a surface form [y] from the target language2 pick a loser form [z]3 check whether the current ranking vector ?
isconsistent with the underlying/winner/loserform triplet (/y/, [y], [z])4 if it isn?t, update the current ranking vector ?until no more mistakes are made at step 3The EDRA model maintains a current hypothe-sis of the target adult grammar, namely a currentconstraint ranking.
This ranking is represented nu-merically through a ranking vector ?
= (?1, , ?n)10which assigns to each constraint Cka numericalranking value ?k.
A constraint Ckis ranked aboveanother constraint Chaccording to a ranking vec-tor ?
provided the ranking value ?kof the formeris (strictly) larger than the ranking value ?hof thelatter (Boersma, 1998; Boersma, 2009).The current ranking (vector) is initialized insuch a way that the corresponding initial languageis as small as possible.
OT constraints come in twovarieties: faithfulness (F) and markedness (M)constraints.
A smallest language corresponds toa ranking which assigns all F constraints under-neath all M constraints.
Thus, the F constraintsare assigned a small initial ranking value, say zerofor concreteness; and theM constraints start witha large positive initial ranking value ?init> 0.
Thealgorithm then loops through the three steps 1-4.At step 1, the EDRA model receives a piece ofdata, namely a surface form [y] sampled from thetarget language L?.
Assuming that the underly-ing typology satisfies Tesar?s (2013) Surface Ori-entedness Condition, this piece of data providesevidence that the target grammar G?maps thisphonological form (construed as the underlyingform /y/) into itself (construed as the surface form[y]) rather than reducing it to some non-faithfulcandidate [z] (as a mnemonic, I strike out a candi-date construed as a loser).
In other words, the tar-get adult ranking (vector) ?
?is consistent with theunderlying/winner/loser form triplet (/y/, [y], [z])for any loser [z], namely it satisfies condition (1).Here,W (L) is the set of winner-preferring (loser-preferring) constraints, namely those which assignless (more) violations to the faithful mapping of/y/ to [y] than to the neutralization of /y/ to [z].
(1) maxCk?W?
?k> maxCh?L?
?hThis consistency condition (1) says that there isat least a winner-preferring constraint which isranked above all loser-preferring constraints bythe target ranking (vector) ?
?= (?
?1, .
.
.
, ?
?n).At steps 2 and 3, the EDRA model thus picksa specific loser [z] and checks whether its cur-rent ranking vector ?
satisfies the correspondingconsistency condition (1).
Failure to satisfy thiscondition means that the current ranking valuesof the loser-preferring (winner-preferring) con-straints are too large (too small).
The algorithmthus promotes the winner-preferring constraintsby a small promotion amount and demotes theloser-preferring constraints by a small demotionamount.
What matters is not the actual values ofthe promotion and demotion amounts, but rathertheir ratio.
Thus, the demotion amount can beset equal to 1 for concreteness, letting instead thepromotion amount be equal to an arbitrary non-negative constant p ?
0, as in (2).
(2) a.
Increase the ranking value of each winner-preferring constraint by p ?
0;b. decrease the ranking value of each undom-inated loser-preferring constraint by 1.Crucially, not all loser-preferring constraints aredemoted by (2b), but only those that need to bedemoted, namely the undominated ones (Tesar andSmolensky, 1998), whose current ranking value isat least as large as the ranking value of all winner-preferring constraints and thus are responsible forflouting the consistency condition (1).3 The problem of F-controlThe crucial implementation parameter of theEDRA model is the promotion amount p ?
0 usedin the promotion component (2a) of the re-rankingrule.
How should this parameter be tuned so as tooptimize the performance of the EDRA model ofthe acquisition of phonotactics?
This section ex-plains how this question leads to the problem ofcontrolling the height of the F constraints.3.1 Some initial guaranteesThe problem of the acquisition of phonotacticsin OT is intractable: no algorithm can solve effi-ciently an arbitrary instance of the problem cor-responding to an arbitrary constraint set (Magri,2013a).
Prompted by this intractability result, Ma-gri (2013b) starts to tackle the problem by lookingat a class of ?easy?
cases.The intuitive idea is that the relative rankingof the F constraints might often be irrelevant forphonotactics, namely for drawing the line betweenlicit and illicit forms (although it is of course al-ways crucial for phonology, namely for the spe-cific way in which illicit forms are repaired).
Thisintuition that the relative ranking of the F con-straints is not relevant to describe a certain phono-tactic pattern can be formalized as follows.
Apartial constraint ranking is any partial order onthe constraint set.
A partial ranking generatesa language provided each one of its total refine-ments generates that language in the usual OTsense (Yanovich, 2012).
A language is called F-irrelevant provided it can be generated in this tech-11nical sense by a partial ranking which does notrank any two F constraints relative to each other(see subsection 3.2 for an example).Suppose that the EDRA model is trained on atarget language L?which is F-irrelevant.
TheM constraints start out high, with an initial rank-ing value ?initusually larger than the number mof markedness constraints.
The F constraints in-stead start out low, with a null initial rankingvalue.
Throughout learning, the F constraints willraise, if the algorithm adopts a non-null promotionamount p > 0.
Theorem 1 provides guaranteesthat the EDRA model learns the target phonotac-tics, as long as the F constraints don?t raise toohigh, namely their ranking values remain smallerby at least m than the initial ranking value ?initoftheM constraints, as stated in (3).Theorem 1 Suppose that the underlying OT ty-pology satisfies the following two assumptions.First, if a surface form [y] is a non-faithful candi-date of an underlying form /x/, then there existsat least one faithfulness constraint which assignsat least one violation to the mapping of /x/ into[y] (F-discernibility assumption).
Second, a form[y] is a candidate of an underlying form /x/ if andonly if the latter form construed as the surfaceform [x] is vice versa a candidate of the formerform construed as the underlying form /y/ (sym-metric candidacy assumption).
Consider a lan-guage in this OT typology which is F-irrelevant.Suppose that the EDRA model only makes a fi-nite number of errors and then converges to a fi-nal ranking vector which is never updated again.Suppose furthermore that the ranking value ?Fofany F constraint F at any time in the run satisfiescondition (3), where m is the number ofM con-straints and ?init> m their initial ranking value.
(3) ?F?
?init?mThen, the language generated by (an arbitrary re-finement of) the final ranking vector learned by theEDRA model coincides with the target languagethe EDRA model has been trained on.
The two assumptions of F-discernibility andsymmetric candidacy required by theorem 1 areextremely mild.
Magri (2013b; 2014b) conjec-tures that the relative ranking of the faithfulnessconstraints turns out to matter for phonotacticsonly in very special configurations, so that theF-irrelevancy assumption might plausibly hold inthe vast majority of cases.
Theorem 1 thus pro-vides guarantees that the EDRA model succeedsat the problem of the acquisition of phonotacticsin a large class of cases under two crucial assump-tions.
One assumption is that it can only make afinite number of errors before it converges to a fi-nal ranking which is consistent with any form andthus never updated.
The other assumption is thecondition (3) that the height of the F-constraintscan be properly controlled.3.2 Some examplesTo illustrate the issues raised by convergence andF-control, consider the following OT typology.The set of forms consists of only four forms{apsa, apza, absa, abza}.
The faithfulness con-straints are the two identity constraints for voicingin stops and fricatives (F1, F2).
The markednessconstraints are the two corresponding constraintsagainst stop and fricative voicing (M1,M2) plusan additional constraint M which bans sequencesof stops and fricatives which agree in voicing,namely it is violated by the two forms apsa andabza.
The candidacy relation is total: the fourforms are all candidates of each other.The OT typology just described contains in par-ticular the language L = {[absa], [apza]}.
Thislanguage is generated by any ranking which satis-fies the ranking conditions (4).
(4)MF2F1M1M2These ranking conditions (4) say nothing about therelative ranking of the two F constraints F1andF2.
The language L thus qualifies as F-irrelevant.When trained on this language, the EDRAmodel will be provided at step 1 with a sequenceof the two licit forms [absa] and [apza].
It will thencomplete them into an underlying/winner/loserform triplet at steps 2 and 3 by assuming a faith-ful underlying form and a non-faithful loser form.The list of all possible such triplets that the algo-rithm can consider is provided in (5).(5)???????
?F1F2M1M2M(/absa/, [absa], [apsa]) W e L e W(/absa/, [absa], [abza]) e W e W W(/absa/, [absa], [apza]) W W L W e(/apza/, [apza], [abza]) W e W e W(/apza/, [apza], [apsa]) e W e L W(/apza/, [apza], [absa]) W W W L e???????
?12Each triplet is described here in ERC notation(Prince, 2002): constraints which are winner- orloser-preferring or even relative to a triplet aremarked with a corresponding W or L or e.The triplets where the constraint M is winner-preferring will trigger virtually no update, sincethat constraint starts high and is never demoted,and will thus always ensure consistency with thosetriplets.
The learning run is thus driven by the tworemaining triplets, boldfaced in (5), which I as-sume are fed one after the other to the algorithm.Suppose the promotion amount is non-null, sayequal to the demotion amount: p = 1.
The re-sulting learning run is described in (6).(6)??
?F10F20M15M25M 5???????11465???????22555???????33465???????44555???????55465???????66555??
?The two F constraints end up too high, namelywith a final ranking value ?F1= ?F2= 6 whichis larger than the initial ranking value ?init= 5of theM constraints.
And indeed the EDRA hasfailed at learning the target phonotactics: since theF constraints are ranked at the top, the model hasincorrectly learned that any form is licit.A trivial strategy to enforce the F-control con-dition (3) would be to threshold the promotioncomponent (2a) of the re-ranking rule, as in (2a?).
(2) a?.
Increase the ranking value of eachwinner-preferring constraint by p, exceptfor an F constraint which is alreadyclose to the forbidden threshold ?init?m.Yet, suppose we tried to remedy to the failurein (6) by thresholding the promotions as in (2a?
).When an F constraint reaches the height ?init?m = 5 ?
3 = 2, we stop promoting it, as bold-faced in the learning run (7).(7)??
?F10F20M15M25M 5???????11465???????22555???????22465???????22555???????22465???????22555???.
.
.In this run, the constraint M stays put at its initialposition.
The constraints M1and M2oscillate upand down, because promoted and demoted by thetwo boldfaced triplets in (5).
The constraints F1and F2raise a bit until they hit the threshold, andthen settle.
The EDRA model will thus keep mak-ing mistakes forever, without ever converging to aranking vector consistent with the data.3.3 Against a null promotion amountThese difficulties with convergence and the F-control condition (3) would disappear if the pro-motion amount p was set equal to zero, so thatthe EDRA performs no constraint promotion atall.
In fact, Tesar and Smolensky (1998) guaranteeconvergence for the demotion-only case.
And theF constraints could not possibly be promoted toohigh, as they would not be promoted at all.Unfortunately, the option of a null promotionamount is not viable, as argued in Magri (2012;2014b).
In fact, recall that the EDRA model atstep 3 always considers underlying/winner/loserform triplets (/y/, [y], [z]) which have an underly-ing form /y/ faithful to the winner [y].
This meansthat the F constraints are never loser-preferringand are therefore never demoted.
If the promotionamount is set equal to zero, then they will not bepromoted either.
In the end, the F constraints willthus never be re-ranked.
This hampers the abilityof the EDRA model to learn the correct relativeranking of the F constraints when trained on a F-relevant language, namely when it needs to learn aphonotactic pattern which crucially does require aspecific relative ranking of the F constraints.3.4 Convergence through calibrationAs recalled above, Tesar and Smolensky (1998)show that the EDRA model converges when thepromotion amount is null and the algorithm per-forms only constraint demotion.
It could in princi-ple be the case that convergence does not extend tothe demotion/promotion case, because any amountof promotion disrupts convergence.
But Magri(2012) shows that is not the case: convergence ex-tends to EDRAs which perform constraint promo-tion as well, as long as the promotion amount issmall enough.
In particular, consider a promotionamount p which scales as in (8) with the numbers` and w of currently undominated loser-preferringconstraints and of winner-preferring constraints.
(8) p =`w + ?It turns out that the EDRA model converges ef-ficiently if (and only if) the promotion amountis calibrated, namely has the shape in (8) corre-sponding to some strictly positive calibration con-stant ?
> 0.
The larger the calibration constant ?,the smaller the promotion amount.
The case of anull promotion amount corresponds to the limitingcase ?
=?.133.5 F-control through calibration as wellLet?s take stock.
Theorem 1 provides some ini-tial guarantees of success of the EDRA model ofthe acquisition of phonotactics.
These guaranteeshold under two crucial assumptions: convergenceand theF-control condition (3).
Do these assump-tions hold when the promotion amount is non-null?
Convergence does hold, if the promotionamount, although not null, is nonetheless small,namely calibrated as in (8).
What about the F-control condition (3)?
Can we play the same trickof a small promotion amount?
Or is it the casethat, no matter how small the promotion amount,as soon as it is allowed to be non-null, the F con-straints raise too high through a long sequence ofvery small promotions?
Section 4 shows that thelatter scenario can never arise: the F constraintscan never raise too high if the promotion amountis small enough.
More precisely, it assumes a cal-ibrated promotion amount as in (8).
And it showsthat the F-control condition (3) holds when thecalibration constant ?
is large enough, namely itgrows as the number m ofM constraints.As the calibration constant increases as thenumber m of markedness constraints, the promo-tion amount decreases quickly.
Is it possible toimprove on the analysis of section 4 and guaran-tee the F-control condition (3) with a calibrationconstant ?
which does not grow with the numberm of markedness constraints?
Unfortunately, sec-tion 5 shows that the calibration constant must in-crease with m. More precisely, this section con-siders the very simple case where there is a singleF constraint and where theM constraints are al-ways loser-preferring (or even) but never winner-preferring.
In this case, the F-control conditionfails if the calibration constant ?
does not growwith m at least asmlogm.Interestingly, the derivative of the functionmlogmgoes to zero as m grows.
In other words, althoughthe function increases with m, the rate of increasebecomes smaller and smaller, making this func-tion as close as possible to a constant.
Is this par-ticularly favorable choice of the calibration con-stant only possible in the peculiar case consideredin section 5?
or does this favorable choice of thecalibration constant ensure F-calibration also inthe general case?
Section 6 shows how to relax atleast one of the two restrictive assumptions madein section 5, namely the assumption that the Mconstraints are never winner-preferring.4 F-controlling with a non-nullpromotion amountThe most basic question of the theory ofF-controlis as follows: is it possible to guarantee the F-control condition (3) despite a non-null promotionamount?
This section provides a positive answerto this question.
In particular, assume the promo-tion amount p is calibrated as in (8), through thecalibration constant ?.
The F-control conditionthen holds provided the calibration constant ?
sat-isfies the bound (9), where m is the number ofMconstraints and ?initis their initial ranking value.
(9) ?
?2m+m?init?init?mTo get a sense of the bound (9), assume that theinitial ranking value ?initof the M constraints issome power of the number m of M constraints:?init= mkfor some k > 1.
The bound (9) thusbecomesmk+2mk?1?1, which is approximately m.At each update, each of the ` currently undom-inated loser-preferring constraints is demoted by1 and each of the w winner-preferring constraintsis promoted by p. Because of the specific shape(8) of the promotion amount p, the sum of thecurrent ranking values decreases by ` ?
wp =` ?w`w+?=`?w+?.
And the latter is at least?w+?,as every update requires at least one undominatedloser-preferring constraint, namely ` ?
1.
Let ?ibe the number of updates triggered by the ith ERCin the run considered up to the time considered(note that there is only a finite number of ERCsrelative to a finite number of constraints).
Thus,the sum?k?kof the current ranking values hasoverall decreased by at least?i?i?wi+?relativeto the the sum?k?initkof the initial ranking val-ues, as stated in (10).(10)?k?k??k?initk?
?i?i?wi+ ?The sum?k?initkof the initial ranking valuescan be computed explicitly as in (11), as the mM constraints start with the initial ranking value?initwhile theF constraints start with a null initialranking value.
(11)?k?initk= m?initThe sum?k?kof the current ranking values canbe lower bounded as in (12).14(12)?k?k(a)=?F?F+?M?M(b)?
0 +?M?M(c)> 0 +m(?2) = ?2mIn step (11a), I have split the sum over all con-straints into the sum over the faithfulness and themarkedness constraints.
In step (11b), I have notedthat the ranking value ?Fof any faithfulness con-straint F is always at least as large as 0.
In fact,the faithfulness constraints start with a null ini-tial ranking value and are never demoted, becausethe EDRA model always assumes an underlyingform faithful to the winner, so that the faithful-ness constraints are never loser-preferring.
In step(11c), I have noted that the ranking value ?Mofa markedness constraint M can never get smallerthan ?2.
In fact, suppose by contradiction that Mmanaged to be demoted that low.
That would im-ply that some ERC triggers an update that demotesM despite the fact that its current ranking valueis strictly smaller than 0.
And that is impossible.In fact, at least one faithfulness constraint F mustbe winner-preferring relative to that ERC, becauseof the F-discernibility assumption.
Furthermore,that constraint F must already dominate M , be-cause F has a non-negative current ranking valuewhile M has a negative current ranking value.Using the expressions for the sum of the ini-tial and the current ranking values obtained in (11)and (12) respectively, the original inequality (10)yields the bound in (13).
(13)?i?i1wi+ ?<2m+m?init?The ranking value ?Fof a generic faithfulnessconstraint F can now be bound as in (14).
(14) ?F(a)?
?i?i1wi+ ?(b)<2m+m?init?(c)?
?init?mIn step (14a), I have used the fact that the faithful-ness constraint F starts with a null initial rankingvalue and is promoted by1wi+?for each one of the?iupdates triggered by the ith ERC, as long as Fis winner-preferring relative to that ERC.
In step(14b), I have used the bound computed in (13).And in step (14c), I have used the choice (9) of thecalibration constant ?.The bound obtained in (14) guarantees thatthe generic faithfulness constraint F never raisesabove the forbidden threshold ?init?m, thus com-plying with the F-control condition (3).
In otherwords, we have obtained the following sufficientsolution to the problem of F-controlling.Theorem 2 Suppose the underlying typology sat-isfies the F-discernibility assumption.
Consider arun of the EDRA model on an arbitrary languagein that typology.
Assume that the F constraintsstart out with a null initial ranking value while them M constraints start out with an initial rank-ing value ?init> m. Assume furthermore that thepromotion amount is calibrated as in (8) and thatthe calibration constant ?
is large enough to sat-isfy the bound (9).
Then, the ranking values of theF constraints remain smaller than the forbiddenthreshold ?init?m throughout the entire run.
5 F-controlling on the diagonal caseThe preceding section has established the F-control condition (3) when the promotion amountis not null, provided it is small enough, namely itcorresponds to a calibration constant which growsas the numberm ofM constraints.
Is it possible todo better?
In particular, is it possible to guaranteeF-control when the calibration constant does notincrease with m?
This section sketches a coun-terexample which provides a negative answer tothis question; see Magri (2014a) for details.At every iteration, the EDRA model receivesa winner form sampled from the target language,assumes a corresponding faithful underlying formand picks a corresponding loser candidate.
At ev-ery iteration, the model thus constructs an under-lying/winner/loser form triplet, which can be de-scribed in terms of the corresponding ERC, as ex-emplified in (5) above.
Since there are only a finitenumber of ERCs corresponding to a finite numberof constraints, the ERCs considered in a run of themodel can be stacked one on top of the other intoan input ERC matrix.Without loss of generality, assume that eachinput ERC has a unique loser-preferring con-straint.
Next, let me make two crucial assump-tions.
First, assume that the constraint set containsa single faithfulness constraint F ?
plus of coursea certain number m of markedness constraintsM1, .
.
.
,Mm.
Second, assume that M1, .
.
.
,Mm15Figure 1: First three stages of the learning dynamics where each diagonal ERC is fed persistently in turnERC 1?init/30?initFM1M2, ,MmERC 1 ERC 20?init?init/329?initFM1M2M3, ,MmERC 1 ERC 2 ERC 30?init13?init29?init427?initFM1M2M3M4, ,Mmare either loser-preferring or even in the inputERCs, but never winner-preferring.
The inputERC matrix thus is (a subset of) the matrix (15).(15)?
?F M1... MmERC 1 W L e... |...ERC m W e L?
?The column corresponding to F consists of allW?s.
The entries corresponding to M1, .
.
.
,Mmare all equal to e?s but for the diagonal of L?s.
ThisERC matrix is thus called diagonal.What is the maximum height that the constraintF can reach in a run of the EDRA model on theinput diagonal ERC matrix (15)?
To address thisquestion, consider the following special run.
Tostart, we persistently feed ERC 1 to the algorithm,until the markedness constraint M1is demotedunderneath the faithfulness constraint F and thatERC cannot trigger any further update.
Only atthat point, we stop feeding ERC 1 to the algo-rithm, and persistently feed ERC 2 instead, againuntil it cannot trigger any further update.
Only atthat point, we stop feeding ERC 2 and persistentlyfeed ERC 3.
And so on.Assume that the promotion amount has theshape (8) and suppose for concreteness that thecalibration constant is ?
= 1, so that the faith-fulness constraint is promoted by 1/2 with eachupdate.
The dynamics of the ranking values isdepicted in Figure 1 for the first three learningstages.
Throughout stage 1, it is ERC 1 that trig-gers updates, whereby the markedness constraintM1is demoted and the faithfulness constraint ispromoted by13?
=12+?
?init, until the two con-straints meet.
Throughout stage 2, it is ERC 2 thattriggers updates, whereby the markedness con-straint M2is demoted and the faithfulness con-straint is promoted by another29?
=1+?(2+?
)2?init,until the two constraints meet.
Throughout thegeneric kth stage, it is the kth ERC that trig-gers updates, whereby the markedness constraintMkis demoted and the faithfulness constraint pro-moted by an amount that turns out to be equal to(1+?)k?1(2+?)k?init.
The height ?Freached by the faith-fulness constraint at the end of the special run con-sidered is thus?mk=1(1+?)k?1(2+?)k?init.
It turns outthat this is indeed the maximum height reacheableby the faithfulness constraint F on any run on thediagonal ERC matrix (15).The F-control condition (3) thus boils down tothe inequality?mk=1(1+?)k?1(2+?)k?init?
?init?
m.Assume that the m markedness constraints startout with the initial ranking value ?init= mk.
Thisinequality can then be solved analytically yielding?
(m) = (1?
exp {(?k logm)/m})?1.
By a firstorder Taylor expansion exp(x) ?
1+x+o(x2) ofthe exponential function, the latter expression canbe approximated as in (16).
(16) ?
= ?
(m) ?mk logmThe latter bound for the calibration thresholdis substantially smaller than the linear bound?
(m) ?
m obtained through the elementary anal-ysis of section 6.
In particular, although (16) is notbounded as a function of m, its derivative goes tozero as 1/ logm.6 F-controlling when the promotionamount decreases slowlyThe preceding section has made two restrictive as-sumptions.
First, that there is a unique F con-straint.
Second, that the M constraints are neverwinner-preferring.
Under these assumptions, ithas shown that the F-control condition (3) holdswhen the calibration constant grows only veryslowly with m, namely as in (16).
Does this favor-able result also hold when we relax the two restric-tive assumptions?
This section shows how to relaxone of the two assumptions, namely the assump-tion that the M constraints cannot be winner-preferring.
At this stage, I do know how to relaxthe other assumption that there is a unique F con-straint.
Again, the reasoning here is only sketched;see Magri (2014a) for details.16To illustrate the core idea, suppose that theEDRA model is trained on the input ERC matrix(17a) and walks through the run (18a).
Here, Iam assuming that the promotion amount p has theshape in (8), with the calibration constant ?
= 0set equal to zero for concreteness.(17)a.
b.
[F M1M2ERC 1 W L eERC 2 W W L] [F M1M2ERC 1 W L eERC 2 W e L](18) a.FM1M2??01010????1910????2810????2.58.59????398?
?ERC 1 ERC 1 ERC 2 ERC 2b.FM1M2??01010????1910????299????398?
?ERC 1 ERC 2 ERC 2Consider the diagonal ERC matrix (17b) corre-sponding to m = 2 markedness constraints.
Theoriginal run (18a) on the original ERC matrix(17a) can be simulated with the run (18b) on thediagonal ERC matrix (17b) in such a way that allconstraints end up at the same high in the two runs.This reasoning holds in complete generality.
In-deed, under the assumption that there is a uniqueF constraint but no restrictions on the M con-straints, the input ERC matrix looks like (19).(19)???F1M1...
Mm|... ...W L, e,W|...
...??
?Any run of the EDRA model on this input ERCmatrix (19) can be mimicked by a correspondingrun on the diagonal ERC matrix (15).
This reduc-tion to the diagonal case holds provided the pro-motion amount is calibrated, namely has the shapein (8), no matter the choice of the calibration con-stant ?
?
0.
This reduction fails if the promotionamount is not calibrated.Another crucial condition needed for the re-duction to the diagonal case is the following: inthe original run, the markedness constraints areallowed to raise only slightly above their initialranking value ?init.
Indeed, if a markedness con-straint could raise arbitrarily high above its initialranking value in the original run, there would beno way to mimic that increasing ranking dynam-ics with a derived run on the diagonal ERC ma-trix (15), as the latter only demotes but never pro-motes the markedness constraints.
The fact thatthe markedness constraints can raise by a smallamount does not threaten the reduction to the diag-onal case, because the markedness constraints canbe assigned a slightly larger initial ranking valuein the derived run on the diagonal ERC matrix.Fortunately, the markedness constraintsM1, .
.
.
,Mmindeed can raise above their initialranking value ?initonly by a small amount,namely never by more than m, as stated in (20).
(20) ?1, .
.
.
, ?m?
?init+mObviously, this bound (20) holds at the beginningof the run.
It is thus sufficient to prove that thisbound is an invariant of the algorithm: if it holdsof the current ranking values at some time t ?
1,then it also holds at the subsequent time t. Thechallenge is that a winner-preferring markednessconstraint M1sitting right at ?init+ m at timet ?
1 could in principle be promoted above thatforbidden threshold, so that the bound (20) wouldhold at time t ?
1 but fail at time t. Yet, in orderfor such an update to happen, there has got to ex-ist another constraintM2which is loser-preferringand is ranked at time t ?
1 at least as high asthe winer-preferring constraint M1.
This meansin turn that the sum ?t?11+ ?t?12of the two rank-ing values of M1and M2at time t ?
1 is at least(?init+m) + (?init+m).
This suggests to copewith the difficulty just highlighted by strengthen-ing the invariant.
Not only a single ranking valuecannot get larger than ?init+m, but also the sumof any two ranking values can never reach (?init+m)+(?init+m).
For instance, let?s say it can neverget larger than (?init+m) + (?init+m?
1).
Butnow again, in order to prove that the latter boundon the sum of two ranking values holds at time t,I need an assumption about the sum of three rank-ing values at time t ?
1.
And so on.
Indeed, thesum ?i1+ .
.
.+?ikof the current ranking values ofany number k of different markedness constraintsMi1, .
.
.
,Mikcan be bound as in (21).
This boundholds for any promotion amount with the shape (8)corresponding to a calibration constant ?
which isnot too small, namely ?
?
1.(21)k?h=1?ih?k?h=1(?init+m?
h+ 1)For k = 1, (21) yields the desired bound (20).AcknowledgmentsThis research was supported by a Marie Curie In-tra European Fellowship within the 7th European17Community Framework Programme.ReferencesPaul Boersma.
1998.
Functional Phonology.
Ph.D.thesis, University of Amsterdam, The Netherlands.The Hague: Holland Academic Graphics.Paul Boersma.
2009.
Some correct error-driven ver-sions of the constraint demotion algorithm.
Linguis-tic Inquiry, 40:667?686.Kyle Gorman.
2013.
Generative phonotactics.
Ph.D.thesis, University of Pennsylvania.Giorgio Magri.
2012.
Convergence of error-drivenranking algorithms.
Phonology, 29(2):213?269.Giorgio Magri.
2013a.
The complexity of learning inOT and its implications for the acquisition of phono-tactics.
Linguistic Inquiry, 44.3:433468.Giorgio Magri.
2013b.
An initial result on the re-strictiveness of the error-driven ranking model of theearly stage of the acquisition of phonotactics.
InHsin-Lun Huang, Ethan Poole, and Amanda Rys-ling, editors, Proceedings of NELS 43: the 43rd an-nual meeting of the North East Linguistic Society.Giorgio Magri.
2014a.
The error-driven ranking modelof the acquisition of phonotactics: how to control theheight of the faithfulness constraints.
CNRS, UiL-OTS ms.Giorgio Magri.
2014b.
Error-driven versus batch mod-els of the acquisition of phonotactics: David defeatsGoliath.
In John Kingston, Claire Moore-Cantwell,Joe Pater, and Robert Staubs, editors, SupplementalProceedings of Phonology 2013, Washington DC.Linguistic Society of America.Joe Pater.
2009.
Weighted constraints in GenerativeLinguistics.
Cognitive Science, 33:999?1035.Alan Prince and Paul Smolensky.
2004.
OptimalityTheory: Constraint Interaction in generative gram-mar.
Blackwell, Oxford.
As Technical Report CU-CS-696-93, Department of Computer Science, Uni-versity of Colorado at Boulder, and Technical ReportTR-2, Rutgers Center for Cognitive Science, Rut-gers University, New Brunswick, NJ, April 1993.Also available as ROA 537 version.Alan Prince.
2002.
Entailed ranking arguments.Ms., Rutgers University, New Brunswick, NJ.
Rut-gers Optimality Archive, ROA 500.
Available athttp://www.roa.rutgers.edu.Bruce Tesar and Paul Smolensky.
1998.
Learnabilityin Optimality Theory.
Linguistic Inquiry, 29:229?268.Bruce Tesar.
2013.
Output-Driven Phonology: Theoryand Learning.
Cambridge Studies in Linguistics.Igor Yanovich.
2012.
The logic of OT rankings.
MITmanuscript.18
