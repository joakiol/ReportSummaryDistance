Proceedings of ACL-08: HLT, pages 888?896,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsJoint Word Segmentation and POS Tagging using a Single PerceptronYue Zhang and Stephen ClarkOxford University Computing LaboratoryWolfson Building, Parks RoadOxford OX1 3QD, UK{yue.zhang,stephen.clark}@comlab.ox.ac.ukAbstractFor Chinese POS tagging, word segmentationis a preliminary step.
To avoid error propa-gation and improve segmentation by utilizingPOS information, segmentation and taggingcan be performed simultaneously.
A challengefor this joint approach is the large combinedsearch space, which makes efficient decod-ing very hard.
Recent research has exploredthe integration of segmentation and POS tag-ging, by decoding under restricted versions ofthe full combined search space.
In this paper,we propose a joint segmentation and POS tag-ging model that does not impose any hard con-straints on the interaction between word andPOS information.
Fast decoding is achievedby using a novel multiple-beam search algo-rithm.
The system uses a discriminative sta-tistical model, trained using the generalizedperceptron algorithm.
The joint model givesan error reduction in segmentation accuracy of14.6% and an error reduction in tagging ac-curacy of 12.2%, compared to the traditionalpipeline approach.1 IntroductionSince Chinese sentences do not contain explicitlymarked word boundaries, word segmentation is anecessary step before POS tagging can be performed.Typically, a Chinese POS tagger takes segmented in-puts, which are produced by a separate word seg-mentor.
This two-step approach, however, has anobvious flaw of error propagation, since word seg-mentation errors cannot be corrected by the POS tag-ger.
A better approach would be to utilize POS in-formation to improve word segmentation.
For ex-ample, the POS-word pattern ?number word?
+ ??
(a common measure word)?
can help in segmentingthe character sequence ??|?
into the word se-quence ? (one) ?
(measure word) | (person)?instead of ? (one) ?| (personal; adj)?.
More-over, the comparatively rare POS pattern ?numberword?
+ ?number word?
can help to prevent seg-menting a long number word into two words.In order to avoid error propagation and make useof POS information for word segmentation, segmen-tation and POS tagging can be viewed as a singletask: given a raw Chinese input sentence, the jointPOS tagger considers all possible segmented andtagged sequences, and chooses the overall best out-put.
A major challenge for such a joint system isthe large search space faced by the decoder.
For asentence with n characters, the number of possibleoutput sequences is O(2n?1 ?
Tn), where T is thesize of the tag set.
Due to the nature of the com-bined candidate items, decoding can be inefficienteven with dynamic programming.Recent research on Chinese POS tagging hasstarted to investigate joint segmentation and tagging,reporting accuracy improvements over the pipelineapproach.
Various decoding approaches have beenused to reduce the combined search space.
Ng andLow (2004) mapped the joint segmentation and POStagging task into a single character sequence taggingproblem.
Two types of tags are assigned to eachcharacter to represent its segmentation and POS.
Forexample, the tag ?b NN?
indicates a character atthe beginning of a noun.
Using this method, POSfeatures are allowed to interact with segmentation.888Since tagging is restricted to characters, the searchspace is reduced to O((4T )n), and beam search de-coding is effective with a small beam size.
How-ever, the disadvantage of this model is the difficultyin incorporating whole word information into POStagging.
For example, the standard ?word + POStag?
feature is not explicitly applicable.
Shi andWang (2007) introduced POS information to seg-mentation by reranking.
N -best segmentation out-puts are passed to a separately-trained POS tagger,and the best output is selected using the overall POS-segmentation probability score.
In this system, thedecoding for word segmentation and POS taggingare still performed separately, and exact inferencefor both is possible.
However, the interaction be-tween POS and segmentation is restricted by rerank-ing: POS information is used to improve segmenta-tion only for the N segmentor outputs.In this paper, we propose a novel joint modelfor Chinese word segmentation and POS tagging,which does not limiting the interaction betweensegmentation and POS information in reducing thecombined search space.
Instead, a novel multiplebeam search algorithm is used to do decoding effi-ciently.
Candidate ranking is based on a discrimina-tive joint model, with features being extracted fromsegmented words and POS tags simultaneously.
Thetraining is performed by a single generalized percep-tron (Collins, 2002).
In experiments with the Chi-nese Treebank data, the joint model gave an errorreduction of 14.6% in segmentation accuracy and12.2% in the overall segmentation and tagging accu-racy, compared to the traditional pipeline approach.In addition, the overall results are comparable to thebest systems in the literature, which exploit knowl-edge outside the training data, even though our sys-tem is fully data-driven.Different methods have been proposed to reduceerror propagation between pipelined tasks, both ingeneral (Sutton et al, 2004; Daume?
III and Marcu,2005; Finkel et al, 2006) and for specific problemssuch as language modeling and utterance classifica-tion (Saraclar and Roark, 2005) and labeling andchunking (Shimizu and Haas, 2006).
Though ourmodel is built specifically for Chinese word segmen-tation and POS tagging, the idea of using the percep-tron model to solve multiple tasks simultaneouslycan be generalized to other tasks.1 word w2 word bigram w1w23 single-character word w4 a word of length l with starting character c5 a word of length l with ending character c6 space-separated characters c1 and c27 character bigram c1c2 in any word8 the first / last characters c1 / c2 of any word9 word w immediately before character c10 character c immediately before word w11 the starting characters c1 and c2 of two con-secutive words12 the ending characters c1 and c2 of two con-secutive words13 a word of length l with previous word w14 a word of length l with next word wTable 1: Feature templates for the baseline segmentor2 The Baseline SystemWe built a two-stage baseline system, using the per-ceptron segmentation model from our previous work(Zhang and Clark, 2007) and the perceptron POS tag-ging model from Collins (2002).
We use baselinesystem to refer to the system which performs seg-mentation first, followed by POS tagging (using thesingle-best segmentation); baseline segmentor to re-fer to the segmentor from (Zhang and Clark, 2007)which performs segmentation only; and baselinePOStagger to refer to the Collins tagger which per-forms POS tagging only (given segmentation).
Thefeatures used by the baseline segmentor are shown inTable 1.
The features used by the POS tagger, someof which are different to those from Collins (2002)and are specific to Chinese, are shown in Table 2.The word segmentation features are extractedfrom word bigrams, capturing word, word lengthand character information in the context.
The wordlength features are normalized, with those more than15 being treated as 15.The POS tagging features are based on contex-tual information from the tag trigram, as well as theneighboring three-word window.
To reduce overfit-ting and increase the decoding speed, templates 4, 5,6 and 7 only include words with less than 3 charac-ters.
Like the baseline segmentor, the baseline tag-ger also normalizes word length features.8891 tag t with word w2 tag bigram t1t23 tag trigram t1t2t34 tag t followed by word w5 word w followed by tag t6 word w with tag t and previous character c7 word w with tag t and next character c8 tag t on single-character word w in charac-ter trigram c1wc29 tag t on a word starting with char c10 tag t on a word ending with char c11 tag t on a word containing char c (not thestarting or ending character)12 tag t on a word starting with char c0 andcontaining char c13 tag t on a word ending with char c0 andcontaining char c14 tag t on a word containing repeated char cc15 tag t on a word starting with character cat-egory g16 tag t on a word ending with character cate-gory gTable 2: Feature templates for the baseline POS taggerTemplates 15 and 16 in Table 2 are inspired by theCTBMorph feature templates in Tseng et al (2005),which gave the most accuracy improvement in theirexperiments.
Here the category of a character isthe set of tags seen on the character during train-ing.
Other morphological features from Tseng et al(2005) are not used because they require extra webcorpora besides the training data.During training, the baseline POS tagger storesspecial word-tag pairs into a tag dictionary (Ratna-parkhi, 1996).
Such information is used by the de-coder to prune unlikely tags.
For each word occur-ring more than N times in the training data, the de-coder can only assign a tag the word has been seenwith in the training data.
This method led to im-provement in the decoding speed as well as the out-put accuracy for English POS tagging (Ratnaparkhi,1996).
Besides tags for frequent words, our base-line POS tagger also uses the tag dictionary to storeclosed-set tags (Xia, 2000) ?
those associated onlywith a limited number of Chinese words.3 Joint Segmentation and Tagging ModelIn this section, we build a joint word segmentationand POS tagging model that uses exactly the samesource of information as the baseline system, by ap-plying the feature templates from the baseline wordsegmentor and POS tagger.
No extra knowledge isused by the joint model.
However, because wordsegmentation and POS tagging are performed simul-taneously, POS information participates in word seg-mentation.3.1 Formulation of the joint modelWe formulate joint word segmentation and POS tag-ging as a single problem, which maps a raw Chi-nese sentence to a segmented and POS tagged output.Given an input sentence x, the output F (x) satisfies:F (x) = argmaxy?GEN(x)Score(y)where GEN(x) represents the set of possible outputsfor x.Score(y) is computed by a feature-based linearmodel.
Denoting the global feature vector for thetagged sentence y with ?
(y), we have:Score(y) = ?
(y) ?
~wwhere ~w is the parameter vector in the model.
Eachelement in ~w gives a weight to its corresponding el-ement in ?
(y), which is the count of a particularfeature over the whole sentence y.
We calculate the~w value by supervised learning, using the averagedperceptron algorithm (Collins, 2002), given in Fig-ure 1.
1We take the union of feature templates from thebaseline segmentor (Table 1) and POS tagger (Ta-ble 2) as the feature templates for the joint system.All features are treated equally and processed to-gether according to the linear model, regardless ofwhether they are from the baseline segmentor or tag-ger.
In fact, most features from the baseline POStagger, when used in the joint model, represent seg-mentation patterns as well.
For example, the afore-mentioned pattern ?number word?
+ ??
?, which is1In order to provide a comparison for the perceptron algo-rithm we also tried SVMstruct (Tsochantaridis et al, 2004) forparameter estimation, but this training method was prohibitivelyslow.890Inputs: training examples (xi, yi)Initialization: set ~w = 0Algorithm:for t = 1..T , i = 1..Ncalculate zi = argmaxy?GEN(xi) ?
(y) ?
~wif zi 6= yi~w = ~w + ?
(yi) ?
?
(zi)Outputs: ~wFigure 1: The perceptron learning algorithmuseful only for the POS ?number word?
in the base-line tagger, is also an effective indicator of the seg-mentation of the two words (especially ???)
in thejoint model.3.2 The decoding algorithmOne of the main challenges for the joint segmenta-tion and POS tagging system is the decoding algo-rithm.
The speed and accuracy of the decoder isimportant for the perceptron learning algorithm, butthe system faces a very large search space of com-bined candidates.
Given the linear model and featuretemplates, exact inference is very hard even with dy-namic programming.Experiments with the standard beam-search de-coder described in (Zhang and Clark, 2007) resultedin low accuracy.
This beam search algorithm pro-cesses an input sentence incrementally.
At eachstage, the incoming character is combined with ex-isting partial candidates in all possible ways to gen-erate new partial candidates.
An agenda is used tocontrol the search space, keeping only the B bestpartial candidates ending with the current charac-ter.
The algorithm is simple and efficient, with alinear time complexity of O(BTn), where n is thesize of input sentence, and T is the size of the tagset (T = 1 for pure word segmentation).
It workedwell for word segmentation alone (Zhang and Clark,2007), even with an agenda size as small as 8, anda simple beam search algorithm also works well forPOS tagging (Ratnaparkhi, 1996).
However, whenapplied to the joint model, it resulted in a reductionin segmentation accuracy (compared to the baselinesegmentor) even with B as large as 1024.One possible cause of the poor performance of thestandard beam search method is the combined natureof the candidates in the search space.
In the base-Input: raw sentence sent ?
a list of charactersVariables: candidate sentence item ?
a list of(word, tag) pairs;maximum word-length recordmaxlen for each tag;the agenda list agendas;the tag dictionary tagdict;start index for current word;end index for current wordInitialization: agendas[0] = [??
],agendas[i] = [] (i!
= 0)Algorithm:for end index = 1 to sent.length:foreach tag:for start index =max(1, end index ?
maxlen[tag] + 1)to end index:word = sent[start index..end index]if (word, tag) consistent with tagdict:for item ?
agendas[start index ?
1]:item1 = itemitem1.append((word,tag))agendas[end index].insert(item1)Outputs: agendas[sent.length].best itemFigure 2: The decoding algorithm for the joint word seg-mentor and POS taggerline POS tagger, candidates in the beam are taggedsequences ending with the current word, which canbe compared directly with each other.
However, forthe joint problem, candidates in the beam are seg-mented and tagged sequences up to the current char-acter, where the last word can be a complete word ora partial word.
A problem arises in whether to givePOS tags to incomplete words.
If partial words aregiven POS tags, it is likely that some partial wordsare ?justified?
as complete words by the current POSinformation.
On the other hand, if partial words arenot given POS tag features, the correct segmentationfor long words can be lost during partial candidatecomparison (since many short completed words withPOS tags are likely to be preferred to a long incom-plete word with no POS tag features).22We experimented with both assigning POS features to par-tial words and omitting them; the latter method performed betterbut both performed significantly worse than the multiple beamsearch method described below.891Another possible cause is the exponential growthin the number of possible candidates with increasingsentence size.
The number increases from O(Tn)for the baseline POS tagger to O(2n?1Tn) for thejoint system.
As a result, for an incremental decod-ing algorithm, the number of possible candidates in-creases exponentially with the current word or char-acter index.
In the POS tagging problem, a new in-coming word enlarges the number of possible can-didates by a factor of T (the size of the tag set).For the joint problem, however, the enlarging fac-tor becomes 2T with each incoming character.
Thespeed of search space expansion is much faster, butthe number of candidates is still controlled by a sin-gle, fixed-size beam at any stage.
If we assumethat the beam is not large enough for all the can-didates at at each stage, then, from the newly gen-erated candidates, the baseline POS tagger can keep1/T for the next processing stage, while the jointmodel can keep only 1/2T , and has to discard therest.
Therefore, even when the candidate compar-ison standard is ignored, we can still see that thechance for the overall best candidate to fall out ofthe beam is largely increased.
Since the search spacegrowth is exponential, increasing the fixed beam sizeis not effective in solving the problem.To solve the above problems, we developed a mul-tiple beam search algorithm, which compares candi-dates only with complete tagged words, and enablesthe size of the search space to scale with the inputsize.
The algorithm is shown in Figure 2.
In thisdecoder, an agenda is assigned to each character inthe input sentence, recording the B best segmentedand tagged partial candidates ending with the char-acter.
The input sentence is still processed incremen-tally.
However, now when a character is processed,existing partial candidates ending with any previouscharacters are available.
Therefore, the decoder enu-merates all possible tagged words ending with thecurrent character, and combines each word with thepartial candidates ending with its previous charac-ter.
All input characters are processed in the sameway, and the final output is the best candidate in thefinal agenda.
The time complexity of the algorithmis O(WTBn), with W being the maximum wordsize, T being the total number of POS tags and n thenumber of characters in the input.
It is also linearin the input size.
Moreover, the decoding algorithmgives competent accuracy with a small agenda sizeof B = 16.To further limit the search space, two optimiza-tions are used.
First, the maximum word lengthfor each tag is recorded and used by the decoderto prune unlikely candidates.
Because the major-ity of tags only apply to words with length 1 or2, this method has a strong effect.
Developmenttests showed that it improves the speed significantly,while having a very small negative influence on theaccuracy.
Second, like the baseline POS tagger, thetag dictionary is used for Chinese closed set tags andthe tags for frequent words.
To words outside the tagdictionary, the decoder still tries to assign every pos-sible tag.3.3 Online learningApart from features, the decoder maintains othertypes of information, including the tag dictionary,the word frequency counts used when building thetag dictionary, the maximum word lengths by tag,and the character categories.
The above data canbe collected by scanning the corpus before trainingstarts.
However, in both the baseline tagger and thejoint POS tagger, they are updated incrementally dur-ing the perceptron training process, consistent withonline learning.3The online updating of word frequencies, max-imum word lengths and character categories isstraightforward.
For the online updating of the tagdictionary, however, the decision for frequent wordsmust be made dynamically because the word fre-quencies keep changing.
This is done by cachingthe number of occurrences of the current most fre-quent word M , and taking all words currently abovethe threshold M/5000 + 5 as frequent words.
5000is a rough figure to control the number of frequentwords, set according to Zipf?s law.
The parameter5 is used to force all tags to be enumerated before aword is seen more than 5 times.4 Related WorkNg and Low (2004) and Shi and Wang (2007) weredescribed in the Introduction.
Both models reduced3We took this approach because we wanted the whole train-ing process to be online.
However, for comparison purposes,we also tried precomputing the above information before train-ing and the difference in performance was negligible.892the large search space by imposing strong restric-tions on the form of search candidates.
In particu-lar, Ng and Low (2004) used character-based POStagging, which prevents some important POS tag-ging features such as word + POS tag; Shi and Wang(2007) used an N -best reranking approach, whichlimits the influence of POS tagging on segmentationto the N -best list.
In comparison, our joint modeldoes not impose any hard limitations on the inter-action between segmentation and POS information.4Fast decoding speed is achieved by using a novelmultiple-beam search algorithm.Nakagawa and Uchimoto (2007) proposed a hy-brid model for word segmentation and POS taggingusing an HMM-based approach.
Word information isused to process known-words, and character infor-mation is used for unknown words in a similar wayto Ng and Low (2004).
In comparison, our modelhandles character and word information simultane-ously in a single perceptron model.5 ExperimentsThe Chinese Treebank (CTB) 4 is used for the exper-iments.
It is separated into two parts: CTB 3 (420Kcharacters in 150K words / 10364 sentences) is usedfor the final 10-fold cross validation, and the rest(240K characters in 150K words / 4798 sentences)is used as training and test data for development.The standard F-scores are used to measure boththe word segmentation accuracy and the overall seg-mentation and tagging accuracy, where the overallaccuracy is TF = 2pr/(p + r), with the precisionp being the percentage of correctly segmented andtagged words in the decoder output, and the recall rbeing the percentage of gold-standard tagged wordsthat are correctly identified by the decoder.
For di-rect comparison with Ng and Low (2004), the POStagging accuracy is also calculated by the percentageof correct tags on each character.5.1 Development experimentsThe learning curves of the baseline and joint modelsare shown in Figure 3, Figure 4 and Figure 5, respec-tively.
These curves are used to show the conver-4Apart from the beam search algorithm, we do impose someminor limitations on the search space by methods such as the tagdictionary, but these can be seen as optional pruning methodsfor optimization.0.880.890.90.910.921 2 3 4 5 6 7 8 9 10Number of training iterationsF-scoreFigure 3: The learning curve of the baseline segmentor0.860.870.880.890.91 2 3 4 5 6 7 8 9 10Number of training iterationsF-scoreFigure 4: The learning curve of the baseline tagger0.80.820.840.860.880.90.921 2 3 4 5 6 7 8 9 10Number of training iterationsF-scoresegmentation accuracyoverall accuracyFigure 5: The learning curves of the joint systemgence of perceptron and decide the number of train-ing iterations for the test.
It should be noticed thatthe accuracies from Figure 4 and Figure 5 are notcomparable because gold-standard segmentation isused as the input for the baseline tagger.
Accord-ing to the figures, the number of training iterations893Tag Seg NN NR VV AD JJ CDNN 20.47 ?
0.78 4.80 0.67 2.49 0.04NR 5.95 3.61 ?
0.19 0.04 0.07 0VV 12.13 6.51 0.11 ?
0.93 0.56 0.04AD 3.24 0.30 0 0.71 ?
0.33 0.22JJ 3.09 0.93 0.15 0.26 0.26 ?
0.04CD 1.08 0.04 0 0 0.07 0 ?Table 3: Error analysis for the joint modelfor the baseline segmentor, POS tagger, and the jointsystem are set to 8, 6, and 7, respectively for the re-maining experiments.There are many factors which can influence theaccuracy of the joint model.
Here we consider thespecial character category features and the effect ofthe tag dictionary.
The character category features(templates 15 and 16 in Table 2) represent a Chinesecharacter by all the tags associated with the charac-ter in the training data.
They have been shown to im-prove the accuracy of a Chinese POS tagger (Tsenget al, 2005).
In the joint model, these features alsorepresent segmentation information, since they con-cern the starting and ending characters of a word.Development tests showed that the overall taggingF-score of the joint model increased from 84.54% to84.93% using the character category features.
In thedevelopment test, the use of the tag dictionary im-proves the decoding speed of the joint model, reduc-ing the decoding time from 416 seconds to 256 sec-onds.
The overall tagging accuracy also increasedslightly, consistent with observations from the purePOS tagger.The error analysis for the development test isshown in Table 3.
Here an error is counted whena word in the standard output is not produced by thedecoder, due to incorrect segmentation or tag assign-ment.
Statistics about the six most frequently mis-taken tags are shown in the table, where each rowpresents the analysis of one tag from the standardoutput, and each column gives a wrongly assignedvalue.
The column ?Seg?
represents segmentationerrors.
Each figure in the table shows the percentageof the corresponding error from all the errors.It can be seen from the table that the NN-VV andVV-NN mistakes were the most commonly made bythe decoder, while the NR-NN mistakes are also fre-Baseline Joint# SF TF TA SF TF TA1 96.98 92.91 94.14 97.21 93.46 94.662 97.16 93.20 94.34 97.62 93.85 94.793 95.02 89.53 91.28 95.94 90.86 92.384 95.51 90.84 92.55 95.92 91.60 93.315 95.49 90.91 92.57 96.06 91.72 93.256 93.50 87.33 89.87 94.56 88.83 91.147 94.48 89.44 91.61 95.30 90.51 92.418 93.58 88.41 90.93 95.12 90.30 92.329 93.92 89.15 91.35 94.79 90.33 92.4510 96.31 91.58 93.01 96.45 91.96 93.45Av.
95.20 90.33 92.17 95.90 91.34 93.02Table 4: The accuracies by 10-fold cross validationSF ?
segmentation F-score,TF ?
overall F-score,TA ?
tagging accuracy by character.quent.
These three types of errors significantly out-number the rest, together contributing 14.92% of allthe errors.
Moreover, the most commonly mistakentags are NN and VV, while among the most frequenttags in the corpus, PU, DEG and M had compara-tively less errors.
Lastly, segmentation errors con-tribute around half (51.47%) of all the errors.5.2 Test results10-fold cross validation is performed to test the ac-curacy of the joint word segmentor and POS tagger,and to make comparisons with existing models in theliterature.
Following Ng and Low (2004), we parti-tion the sentences in CTB 3, ordered by sentence ID,into 10 groups evenly.
In the nth test, the nth groupis used as the testing data.Table 4 shows the detailed results for the crossvalidation tests, each row representing one test.
Ascan be seen from the table, the joint model outper-forms the baseline system in each test.Table 5 shows the overall accuracies of the base-line and joint systems, and compares them to the rel-evant models in the literature.
The accuracy of eachmodel is shown in a row, where ?Ng?
represents themodels from Ng and Low (2004) and ?Shi?
repre-sents the models from Shi and Wang (2007).
Eachaccuracy measure is shown in a column, includingthe segmentation F-score (SF ), the overall tagging894Model SF TF TABaseline+ (Ng) 95.1 ?
91.7Joint+ (Ng) 95.2 ?
91.9Baseline+* (Shi) 95.85 91.67 ?Joint+* (Shi) 96.05 91.86 ?Baseline (ours) 95.20 90.33 92.17Joint (ours) 95.90 91.34 93.02Table 5: The comparison of overall accuracies by 10-foldcross validation using CTB+ ?
knowledge about sepcial characters,* ?
knowledge from semantic net outside CTB.F-score (TF ) and the tagging accuracy by characters(TA).
As can be seen from the table, our joint modelachieved the largest improvement over the baseline,reducing the segmentation error by 14.58% and theoverall tagging error by 12.18%.The overall tagging accuracy of our joint modelwas comparable to but less than the joint model ofShi and Wang (2007).
Despite the higher accuracyimprovement from the baseline, the joint system didnot give higher overall accuracy.
One likely reasonis that Shi and Wang (2007) included knowledgeabout special characters and semantic knowledgefrom web corpora (which may explain the higherbaseline accuracy), while our system is completelydata-driven.
However, the comparison is indirect be-cause our partitions of the CTB corpus are different.Shi and Wang (2007) also chunked the sentences be-fore doing 10-fold cross validation, but used an un-even split.
We chose to follow Ng and Low (2004)and split the sentences evenly to facilitate furthercomparison.Compared with Ng and Low (2004), our baselinemodel gave slightly better accuracy, consistent withour previous observations about the word segmen-tors (Zhang and Clark, 2007).
Due to the large ac-curacy gain from the baseline, our joint model per-formed much better.In summary, when compared with existing jointword segmentation and POS tagging systems in theliterature, our proposed model achieved the best ac-curacy boost from the cascaded baseline, and com-petent overall accuracy.6 Conclusion and Future WorkWe proposed a joint Chinese word segmentation andPOS tagging model, which achieved a considerablereduction in error rate compared to a baseline two-stage system.We used a single linear model for combined wordsegmentation and POS tagging, and chose the gen-eralized perceptron algorithm for joint training.
andbeam search for efficient decoding.
However, theapplication of beam search was far from trivial be-cause of the size of the combined search space.
Mo-tivated by the question of what are the compara-ble partial hypotheses in the space, we developeda novel multiple beam search decoder which effec-tively explores the large search space.
Similar tech-niques can potentially be applied to other problemsinvolving joint inference in NLP.Other choices are available for the decoding ofa joint linear model, such as exact inference withdynamic programming, provided that the range offeatures allows efficient processing.
The baselinefeature templates for Chinese segmentation and POStagging, when added together, makes exact infer-ence for the proposed joint model very hard.
How-ever, the accuracy loss from the beam decoder, aswell as alternative decoding algorithms, are worthfurther exploration.The joint system takes features only from thebaseline segmentor and the baseline POS tagger toallow a fair comparison.
There may be additionalfeatures that are particularly useful to the joint sys-tem.
Open features, such as knowledge of numbersand European letters, and relationships from seman-tic networks (Shi and Wang, 2007), have been re-ported to improve the accuracy of segmentation andPOS tagging.
Therefore, given the flexibility of thefeature-based linear model, an obvious next step isthe study of open features in the joint segmentor andPOS tagger.AcknowledgementsWe thank Hwee-Tou Ng and Mengqiu Wang fortheir helpful discussions and sharing of experimen-tal data, and the anonymous reviewers for their sug-gestions.
This work is supported by the ORS andClarendon Fund.895ReferencesMichael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe EMNLP conference, pages 1?8, Philadelphia, PA.Hal Daume?
III and Daniel Marcu.
2005.
Learning assearch optimization: Approximate large margin meth-ods for structured prediction.
In Proceedings of theICML Conference, pages 169?176, Bonn, Germany.Jenny Rose Finkel, Christopher D. Manning, and An-drew Y. Ng.
2006.
Solving the problem of cascadingerrors: Approximate Bayesian inference for linguisticannotation pipelines.
In Proceedings of the EMNLPConference, pages 618?626, Sydney, Australia.Tetsuji Nakagawa and Kiyotaka Uchimoto.
2007.
Ahybrid approach to word segmentation and pos tag-ging.
In Proceedings of ACL Demo and Poster Ses-sion, pages 217?220, Prague, Czech Republic.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinesepart-of-speech tagging: One-at-a-time or all-at-once?Word-based or character-based?
In Proceedings ofthe EMNLP Conference, pages 277?284, Barcelona,Spain.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging.
In Proceedings of theEMNLP Conference, pages 133?142, Philadelphia,PA.Murat Saraclar and Brian Roark.
2005.
Joint discrimi-native language modeling and utterance classification.In Proceedings of the ICASSP Conference, volume 1,Philadelphia, USA.Yanxin Shi and Mengqiu Wang.
2007.
A dual-layer CRFbased joint decoding method for cascade segmentationand labelling tasks.
In Proceedings of the IJCAI Con-ference, Hyderabad, India.Nobuyuki Shimizu and Andrew Haas.
2006.
Exact de-coding for jointly labeling and chunking sequences.
InProceedings of the COLING/ACL Conference, PosterSessions, Sydney, Australia.Charles Sutton, Khashayar Rohanimanesh, and AndrewMcCallum.
2004.
Dynamic conditional randomfields: Factorized probabilistic models for labelingand segmenting sequence data.
In Proceedings of theICML Conference, Banff, Canada.Huihsin Tseng, Daniel Jurafsky, and Christopher Man-ning.
2005.
Morphological features help POS taggingof unknown words across language varieties.
In Pro-ceedings of the Fourth SIGHAN Workshop, Jeju Island,Korea.I.
Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.2004.
Support vector machine learning for interdepen-dent and structured output spaces.
In Proceedings ofthe ICML Conference, Banff, Canada.Fei Xia.
2000.
The part-of-speech tagging guidelines forthe Chinese Treebank (3.0).
IRCS Report, Universityof Pennsylvania.Yue Zhang and Stephen Clark.
2007.
Chinese segmen-tation with a word-based perceptron algorithm.
InProceedings of the ACL Conference, pages 840?847,Prague, Czech Republic.896
