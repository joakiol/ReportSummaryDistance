Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 166?170,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsA Two-step Approach to Sentence Compression of Spoken UtterancesDong Wang, Xian Qian, Yang LiuThe University of Texas at Dallasdongwang,qx,yangl@hlt.utdallas.eduAbstractThis paper presents a two-step approach tocompress spontaneous spoken utterances.
Inthe first step, we use a sequence labelingmethod to determine if a word in the utterancecan be removed, and generate n-best com-pressed sentences.
In the second step, weuse a discriminative training approach to cap-ture sentence level global information fromthe candidates and rerank them.
For evalua-tion, we compare our system output with mul-tiple human references.
Our results show thatthe new features we introduced in the firstcompression step improve performance uponthe previous work on the same data set, andreranking is able to yield additional gain, espe-cially when training is performed to take intoaccount multiple references.1 IntroductionSentence compression aims to preserve the most im-portant information in the original sentence withfewer words.
It can be used for abstractive summa-rization where extracted important sentences oftenneed to be compressed and merged.
For summariza-tion of spontaneous speech, sentence compressionis especially important, since unlike fluent and well-structured written text, spontaneous speech containsa lot of disfluencies and much redundancy.
The fol-lowing shows an example of a pair of source andcompressed spoken sentences1 from human annota-tion (removed words shown in bold):[original sentence]1For speech domains, ?sentences?
are not clearly defined.We use sentences and utterances interchangeably when there isno ambiguity.and then um in terms of the source the things uh theonly things that we had on there I believe were whether...[compressed sentence]and then in terms of the source the only things that wehad on there were whether...In this study we investigate sentence compres-sion of spoken utterances in order to remove re-dundant or unnecessary words while trying to pre-serve the information in the original sentence.
Sen-tence compression has been studied from formaltext domain to speech domain.
In text domain,(Knight and Marcu, 2000) applies noisy-channelmodel and decision tree approaches on this prob-lem.
(Galley and Mckeown, 2007) proposes to use asynchronous context-free grammars (SCFG) basedmethod to compress the sentence.
(Cohn and La-pata, 2008) expands the operation set by includinginsertion, substitution and reordering, and incorpo-rates grammar rules.
In speech domain, (Clarke andLapata, 2008) investigates sentence compression inbroadcast news using an integer linear programmingapproach.
There is only a few existing work in spon-taneous speech domains.
(Liu and Liu, 2010) mod-eled it as a sequence labeling problem using con-ditional random fields model.
(Liu and Liu, 2009)compared the effect of different compression meth-ods on a meeting summarization task, but did notevaluate sentence compression itself.We propose to use a two-step approach in this pa-per for sentence compression of spontaneous speechutterances.
The contributions of our work are:?
Our proposed two-step approach allows us toincorporate features from local and global lev-els.
In the first step, we adopt a similar se-quence labeling method as used in (Liu andLiu, 2010), but expanded the feature set, which166results in better performance.
In the secondstep, we use discriminative reranking to in-corporate global information about the com-pressed sentence candidates, which cannot beaccomplished by word level labeling.?
We evaluate our methods using different met-rics including word-level accuracy and F1-measure by comparing to one reference com-pression, and BLEU scores comparing withmultiple references.
We also demonstrate thattraining in the reranking module can be tailedto the evaluation metrics to optimize systemperformance.2 CorpusWe use the same corpus as (Liu and Liu, 2010)where they annotated 2,860 summary sentences in26 meetings from the ICSI meeting corpus (Murrayet al, 2005).
In their annotation procedure, filledpauses such as ?uh/um?
and incomplete words areremoved before annotation.
In the first step, 8 anno-tators were asked to select words to be removed tocompress the sentences.
In the second step, 6 an-notators (different from the first step) were askedto pick the best one from the 8 compressions fromthe previous step.
Therefore for each sentence, wehave 8 human compressions, as well a best one se-lected by the majority of the 6 annotators in the sec-ond step.
The compression ratio of the best humanreference is 63.64%.In the first step of our sentence compression ap-proach (described below), for model training weneed the reference labels for each word, which rep-resents whether it is preserved or deleted in the com-pressed sentence.
In (Liu and Liu, 2010), they usedthe labels from the annotators directly.
In this work,we use a different way.
For each sentence, we stilluse the best compression as the gold standard, butwe realign the pair of the source sentence and thecompressed sentence, instead of using the labelsprovided by annotators.
This is because when thereare repeated words, annotators sometimes randomlypick removed ones.
However, we want to keep thepatterns consistent for model training ?
we alwayslabel the last appearance of the repeated words as?preserved?, and the earlier ones as ?deleted?.
An-other difference in our processing of the corpus fromthe previous work is that when aligning the originaland the compressed sentence, we keep filled pausesand incomplete words since they tend to appear to-gether with disfluencies and thus provide useful in-formation for compression.3 Sentence Compression ApproachOur compression approach has two steps: in thefirst step, we use Conditional Random Fields (CRFs)to model this problem as a sequence labeling task,where the label indicates whether the word should beremoved or not.
We select n-best candidates (n = 25in our work) from this step.
In the second step weuse discriminative training based on a maximum En-tropy model to rerank the candidate compressions,in order to select the best one based on the qualityof the whole candidate sentence, which cannot beperformed in the first step.3.1 Generate N-best CandidatesIn the first step, we cast sentence compression asa sequence labeling problem.
Considering that inmany cases phrases instead of single words aredeleted, we adopt the ?BIO?
labeling scheme, simi-lar to the name entity recognition task: ?B?
indicatesthe first word of the removed fragment, ?I?
repre-sents inside the removed fragment (except the firstword), and ?O?
means outside the removed frag-ment, i.e., words remaining in the compressed sen-tence.
Each sentence with n words can be viewed asa word sequence X1, X2, ..., Xn, and our task is tofind the best label sequence Y1, Y2, ..., Yn where Yiis one of the three labels.
Similar to (Liu and Liu,2010), for sequence labeling we use linear-chainfirst-order CRFs.
These models define the condi-tional probability of each labeling sequence giventhe word sequence as:p(Y |X) ?expPnk=1(Pj ?jfj(yk, yk?1, X) +Pi ?igi(xk, yk, X))where fj are transition feature functions (here first-order Markov independence assumption is used); giare observation feature functions; ?j and ?i are theircorresponding weights.
To train the model for thisstep, we use the best reference compression to obtainthe reference labels (as described in Section 2).In the CRF compression model, each word is rep-resented by a feature vector.
We incorporate mostof the features used in (Liu and Liu, 2010), includ-ing unigram, position, length of utterance, part-of-speech tag as well as syntactic parse tree tags.
Wedid not use the discourse parsing tree based featuresbecause we found they are not useful in our exper-iments.
In this work, we further expand the featureset in order to represent the characteristics of disflu-encies in spontaneous speech as well as model theadjacent output labels.
The additional features we167introduced are:?
the distance to the next same word and the nextsame POS tag.?
a binary feature to indicate if there is a filledpause or incomplete word in the following 4-word window.
We add this feature since filledpauses or incomplete words often appear afterdisfluent words.?
the combination of word/POS tag and its posi-tion in the sentence.?
language model probabilities: the bigram prob-ability of the current word given the previousone, and followed by the next word, and theirproduct.
These probabilities are obtained fromthe Google Web 1T 5-gram.?
transition features: a combination of the currentoutput label and the previous one, together withsome observation features such as the unigramand bigrams of word or POS tag.3.2 Discriminative RerankingAlthough CRFs is able to model the dependencyof adjacent labels, it does not measure the qualityof the whole sentence.
In this work, we proposeto use discriminative training to rerank the candi-dates generated in the first step.
Reranking has beenused in many tasks to find better global solutions,such as machine translation (Wang et al, 2007),parsing (Charniak and Johnson, 2005), and disflu-ency detection (Zwarts and Johnson, 2011).
We usea maximum Entropy reranker to learn distributionsover a set of candidates such that the probability ofthe best compression is maximized.
The conditionalprobability of output y given observation x in themaximum entropy model is defined as:p(y|x) = 1Z(x)exp[?ki=1 ?if(x, y)]where f(x, y) are feature functions and ?i are theirweighting parameters; Z(x) is the normalizationfactor.In this reranking model, every compression can-didate is represented by the following features:?
All the bigrams and trigrams of words and POStags in the candidate sentence.?
Bigrams and trigrams of words and POS tags inthe original sentence in combination with theirbinary labels in the candidate sentence (deletethe word or not).
For example, if the origi-nal sentence is ?so I should go?, and the can-didate compression sentence is ?I should go?,then ?so I 10?, ?so I should 100?
are includedin the features (1 means the word is deleted).?
The log likelihood of the candidate sentencebased on the language model.?
The absolute difference of the compression ra-tio of the candidate sentence with that of thefirst ranked candidate.
This is because we tryto avoid a very large or small compression ra-tio, and the first candidate is generally a goodcandidate with reasonable length.?
The probability of the label sequence of thecandidate sentence given by the first step CRFs.?
The rank of the candidate sentence in 25 bestlist.For discriminative training using the n-best can-didates, we need to identify the best candidate fromthe n-best list, which can be either the referencecompression (if it exists on the list), or the mostsimilar candidate to the reference.
Since we have8 human compressions and also want to evaluatesystem performance using all of them (see exper-iments later), we try to use multiple references inthis reranking step.
In order to use the same train-ing objective (maximize the score for the single bestamong all the instances), for the 25-best list, if mreference compressions exist, we split the list intom groups, each of which is a new sample containingone reference as positive and several negative can-didates.
If no reference compression appears in 25-best list, we just keep the entire list and label the in-stance that is most similar to the best reference com-pression as positive.4 ExperimentsWe perform a cross-validation evaluation where onemeeting is used for testing and the rest of them areused as the training set.
When evaluating the systemperformance, we do not consider filled pauses andincomplete words since they can be easily identi-fied and removed.
We use two different performancemetrics in this study.?
Word-level accuracy and F1 score based on theminor class (removed words).
This was usedin (Liu and Liu, 2010).
These measures are ob-tained by comparing with the best compression.In evaluation we map the result using ?BIO?
la-bels from the first-step compression to binarylabels that indicate a word is removed or not.168?
BLEU score.
BLEU is a widely used metricin evaluating machine translation systems thatoften use multiple references.
Since there is agreat variation in human compression results,and we have 8 reference compressions, we ex-plore using BLEU for our sentence compres-sion task.
BLEU is calculated based on the pre-cision of n-grams.
In our experiments we useup to 4-grams.Table 1 shows the averaged scores of the crossvalidation evaluation using the above metrics forseveral methods.
Also shown in the table is the com-pression ratio of the system output.
For ?reference?,we randomly choose one compression from 8 ref-erences, and use the rest of them as references incalculating the BLEU score.
This represents humanperformance.
The row ?basic features?
shows theresult of using all features in (Liu and Liu, 2010)except discourse parsing tree based features, and us-ing binary labels (removed or not).
The next rowuses this same basic feature set and ?BIO?
labels.Row ?expanded features?
shows the result of our ex-panded feature set using ?BIO?
label set from thefirst step of compression.
The last two rows showthe results after reranking, trained using one best ref-erence or 8 reference compressions, respectively.accuracy F1 BLEU ratio (%)reference 81.96 69.73 95.36 76.78basic features (Liuand Liu, 2010)76.44 62.11 91.08 73.49basic features, BIO 77.10 63.34 91.41 73.22expanded features 79.28 67.37 92.70 72.17rerankingtrain w/ 1 ref 79.01 67.74 91.90 70.60rerankingtrain w/ 8 refs 78.78 63.76 94.21 77.15Table 1: Compression results using different systems.Our result using the basic feature set is similar tothat in (Liu and Liu, 2010) (their accuracy is 76.27%when compression ratio is 0.7), though the experi-mental setups are different: they used 6 meetings asthe test set while we performed cross validation.
Us-ing the ?BIO?
label set instead of binary labels hasmarginal improvement for the three scores.
Fromthe table, we can see that our expanded feature set isable to significantly improve the result, suggestingthe effectiveness of the new introduced features.Regarding the two training settings in reranking,we find that there is no gain from reranking whenusing only one best compression, however, train-ing with multiple references improves BLEU scores.This indicates the discriminative training used inmaximum entropy reranking is consistent with theperformance metrics.
Another reason for the per-formance gain for this condition is that there is lessdata imbalance in model training (since we split then-best list, each containing fewer negative exam-ples).
We also notice that the compression ratio af-ter reranking is more similar to the reference.
Assuggested in (Napoles et al, 2011), it is not appro-priate to compare compression systems with differ-ent compression ratios, especially when consideringgrammars and meanings.
Therefore for the com-pression system without reranking, we generated re-sults with the same compression ratio (77.15%), andfound that using reranking still outperforms this re-sult, 1.19% higher in BLEU score.For an analysis, we check how often our sys-tem output contains reference compressions basedon the 8 references.
We found that 50.8% of sys-tem generated compressions appear in the 8 refer-ences when using CRF output with a compressionration of 77.15%; and after reranking this numberincreases to 54.8%.
This is still far from the oracleresult ?
for 84.7% of sentences, the 25-best list con-tains one or more reference sentences, that is, thereis still much room for improvement in the rerankingprocess.
The results above also show that the tokenlevel measures by comparing to one best referencedo not always correlate well with BLEU scores ob-tained by comparing with multiple references, whichshows the need of considering multiple metrics.5 ConclusionThis paper presents a 2-step approach for sentencecompression: we first generate an n-best list for eachsource sentence using a sequence labeling method,then rerank the n-best candidates to select the bestone based on the quality of the whole candidate sen-tence using discriminative training.
We evaluate thesystem performance using different metrics.
Our re-sults show that our expanded feature set improvesthe performance across multiple metrics, and rerank-ing is able to improve the BLEU score.
In futurework, we will incorporate more syntactic informa-tion in the model to better evaluate sentence quality.We also plan to perform a human evaluation for thecompressed sentences, and use sentence compres-sion in summarization.1696 AcknowledgmentThis work is partly supported by DARPA un-der Contract No.
HR0011-12-C-0016 and NSFNo.
0845484.
Any opinions expressed in this ma-terial are those of the authors and do not necessarilyreflect the views of DARPA or NSF.ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meetingon Association for Computational Linguistics, pages173?180, Stroudsburg, PA, USA.
Proceedings of ACL.James Clarke and Mirella Lapata.
2008.
Global infer-ence for sentence compression an integer linear pro-gramming approach.
Journal of Artificial IntelligenceResearch, 31:399?429, March.Trevor Cohn and Mirella Lapata.
2008.
Sentence com-pression beyond word deletion.
In Proceedings ofCOLING.Michel Galley and Kathleen R. Mckeown.
2007.
Lex-icalized Markov grammars for sentence compression.In Proceedings of HLT-NAACL.Kevin Knight and Daniel Marcu.
2000.
Statistics-basedsummarization-step one: Sentence compression.
InProceedings of AAAI.Fei Liu and Yang Liu.
2009.
From extractive to abstrac-tive meeting summaries: can it be done by sentencecompression?
In Proceedings of the ACL-IJCNLP.Fei Liu and Yang Liu.
2010.
Using spoken utterancecompression for meeting summarization: a pilot study.In Proceedings of SLT.Gabriel Murray, Steve Renals, and Jean Carletta.
2005.Extractive summarization of meeting recordings.
InProceedings of EUROSPEECH.Courtney Napoles, Benjamin Van Durme, and ChrisCallison-Burch.
2011.
Evaluating Sentence Com-pression: Pitfalls and Suggested Remedies.
In Pro-ceedings of the Workshop on Monolingual Text-To-TextGeneration, pages 91?97, Portland, Oregon, June.
As-sociation for Computational Linguistics.Wen Wang, A. Stolcke, and Jing Zheng.
2007.
Rerank-ing machine translation hypotheses with structuredand web-based language models.
In Proceedings ofIEEE Workshop on Speech Recognition and Under-standing, pages 159?164, Kyoto.Simon Zwarts and Mark Johnson.
2011.
The impact oflanguage models and loss functions on repair disflu-ency detection.
In Proceedings of ACL.170
