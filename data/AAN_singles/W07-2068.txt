Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 314?317,Prague, June 2007. c?2007 Association for Computational LinguisticsSussx: WSD using Automatically Acquired Predominant SensesRob Koeling and Diana McCarthyDepartment of InformaticsUniversity of SussexBrighton BN1 9QJ, UKrobk,dianam@sussex.ac.uk1 IntroductionWe introduced a method for discovering the predom-inant sense of words automatically using raw (unla-belled) text in (McCarthy et al, 2004) and partici-pated with this system in SENSEVAL3.
Since then,we worked on further developing ideas to improveupon the base method.
In the current paper we tar-get two areas where we believe there is potential forimprovement.
In the first one we address the fine-grained structure of WordNet?s (WN) sense inven-tory (i.e.
the topic of the task in this particular track).The second issue we address here, deals with topicdomain specilisation of the base method.Error analysis tought us that the method is sensi-tive to the fine-grained nature of WN.
When two dis-tinct senses in the WN sense inventory are closely re-lated, the method often has difficulties discriminat-ing between the two senses.
If, for example, sense 1and sense 7 for a word are closely related, choosingsense 7 in stead of sense 1 has serious consequencesif you are using a first-sense heuristic (consideringthe highly skewed distribution of word senses).
Weexpect that applying our method on a coarser grainedsense inventory might help us resolve some of themore unfortunate errors.
(Magnini et al, 2002) have shown that informa-tion about the domain of a document is very usefulfor WSD.
This is because many concepts are spe-cific to particular domains, and for many words theirmost likely meaning in context is strongly correlatedto the domain of the document they appear in.
Thus,since word sense distributions are skewed and de-pend on the domain at hand we would like to exploreif we can estimate the most likely sense of a wordfor each domain of application and exploit this ina WSD system.2 Predominant Sense AcquisitionWe use the method described in (McCarthy et al,2004) for finding predominant senses from raw text.The method uses a thesaurus obtained from thetext by parsing, extracting grammatical relations andthen listing each word (w) with its top k nearestneighbours, where k is a constant.
Like (McCarthyet al, 2004) we use k = 50 and obtain our thesaurususing the distributional similarity metric describedby (Lin, 1998) and we use WordNet (WN) as oursense inventory.
The senses of a word w are eachassigned a ranking score which sums over the dis-tributional similarity scores of the neighbours andweights each neighbour?s score by a WN Similarityscore (Patwardhan and Pedersen, 2003) between thesense of w and the sense of the neighbour that max-imises the WN Similarity score.
This weight is nor-malised by the sum of such WN similarity scores be-tween all senses of w and the senses of the neighbourthat maximises this score.
We use the WN Similarityjcn score (Jiang and Conrath, 1997) since this gavereasonable results for (McCarthy et al, 2004) and itis efficient at run time given precompilation of fre-quency information.
The jcn measure needs wordfrequency information, which we obtained from theBritish National Corpus (BNC) (Leech, 1992).
Thedistributional thesaurus was constructed using sub-ject, direct object adjective modifier and noun mod-ifier relations.3143 Coarse Sense Inventory AdaptationWe contrasted ranking of the original WordNetsenses with ranking produced using the coarsegrained mapping between WordNet senses and theclusters provided for this task.
In the first, which werefer to as fine-grained training (SUSSX-FR), we usethe original method as described in section 2 usingWordNet 2.1 as our sense inventory.
For the secondmethod which we refer to as coarse-grained train-ing (SUSSX-CR), we use the clusters of the targetword as our senses.
The distributional similarity ofeach neighbour is apportioned to these clusters us-ing the maximum WordNet similarity between anyof the WordNet senses in the cluster and any of thesenses of the neighbour.
This WordNet similarity isnormalised as in the original method, but for the de-nominator we use the sum of the WordNet similarityscores between this neighbour and all clusters of thetarget word.4 Domain AdaptationThe topic domain of a document has a strong influ-ence on the sense distribution of words.
Unfortu-nately, it is not feasible to produce large manuallysense-annotated corpora for every domain of inter-est.
Since the method described in section 2 workswith raw text, we can specialize our sense rank-ings for a particular topic domain, simply by feed-ing a domain specific corpus to the algorithm.
Pre-vious experiments have shown that unsupervised es-timation of the predominant sense of certain wordsusing corpora whose domain has been determinedby hand outperforms estimates based on domain-independent text for a subset of words and evenoutperforms the estimates based on counting oc-currences in an annotated corpus (Koeling et al,2005).
A later experiment (using SENSEVAL2 and3 data) showed that using domain specific predomi-nant senses can slightly improve the results for somedomains (Koeling et al, 2007).
However, a firm ideaof when domain specilisation should be consideredcould not (yet) be given.4.1 Creating the Domain CorporaIn order to estimate topic domain specific senserankings, we need to specify what we consider ?do-mains?
and we need to collect corpora of texts forthese domains.
We decided to use text classifica-tion for determining the topic domain and adoptedthe domain hierarchy as defined for the topic domainextension for WN (Subject Field Codes or WordNetDomains (WN-DOMAINS) (Magnini et al, 2002)).Domains In WN-DOMAINS the Princeton EnglishWordNet is augmented with domain labels.
Ev-ery synset in WN?s sense inventory is annotatedwith at least one domain label, selected from a setof about 200 labels hierarchically organized (basedon the Dewey Decimal Classification (Diekema, )).Each synset of Wordnet 1.6 was labeled with oneor more labels.
The label ?factotum?
was assignedif any other was inadequate.
The first level con-sists of 5 main categories (e.g.
?doctrines?
and ?so-cial science?)
and ?factotum?.
?doctrines?, for exam-ple, has subcategories such as ?art?, ?religion?
and?psychology?.
Some subcategories are divided insub-subcategories, e.g.
?dance?, ?music?
or ?theatre?are subcategories of ?art?.Classifier We extracted bags of domain-specificwords from WordNet for all the defined domains bycollecting all the word senses (synsets) and corre-sponding glosses associated with a certain domainlabel.
These bags of words define the domains andwe used them to train a Support Vector Machine(SVM) text classifier using ?TwentyOne?1.The classifier distinguishes between 48 classes(first and second level of the WN-DOMAINS hierar-chy).
When a document is evaluated by the classi-fier, it returns a list of all the classes (domains) itrecognizes and an associated confidence score re-flecting the certainty that the document belongs tothat particular domain.Corpora We used the Gigaword English Corpusas our data source.
This corpus is a comprehen-sive archive of newswire text data that has beenacquired over several years by the Linguistic DataConsortium, at the University of Pennsylvania.
Forthe experiments described in this paper, we use thefirst 20 months worth of data of all four sources(Agence France Press English Service, AssociatedPress Worldstream English Service, The New YorkTimes Newswire Service and The Xinhua NewsAgency English Service).
There are 4 different types1TwentyOne Classifier is an Irion Technologies product:www.irion.ml/products/english/products classify.html315Doc.Id.
Class Conf.
Scored001 Medicine (Economy) 0.75 (0.75)d002 Economy (Politics) 0.76 (0.74)d003 Transport (Biology) 0.75 (0.68)d004 Comp-Sci (Architecture) 0.81 (0.68)d005 Psychology (Art) 0.78 (0.74)Table 1: Output of the classifier for the 5 docu-ments.
The classifiers second choice is given be-tween brackets.of documents identified in the corpus.
The vast ma-jority of the documents are of type ?story?.
We areusing all the data.The five documents were fed to the classifier.
Theresults are given in table 1.
Unfortunately, only onedocument (d004) was considered to be a clear-cutexample of a particular domain by the classifier (i.e.a high score is given to the first class and a muchlower score to the following classes).4.2 Domain rankingsWe created domain corpora by feeding the Giga-Word documents to the classifier and adding eachdocument to the domain corpus corresponding tothe classifier?s first choice.
The five corpora weneeded for these documents were parsed usingRASP (Briscoe and Carroll, 2002) and the result-ing grammatical relations were used to create a dis-tributional similarity thesaurus, which in turn wasused for computing the predominant senses (see sec-tion 2).
The only pre-processing we performedwas stripping the XML codes from the documents.No other filtering was undertaken.
This resulted infive sets of sense inventories with domain-dependentsense rankings.
Each of them has a slightly differentset of words.
The words they have in common dohave the same senses, but not necessarily the sameestimated most frequently used sense.5 Results from SemevalCoarse Disambiguation of coarse-grained senses isobviously an easier task than fine grained training.We had hoped that the coarse-grained training mightshow superior performance by removing the noisecreated by related but less frequent senses.
Sincethe mapping between fine-grained senses and clus-ters is used anyway in the scorer the noise fromrelated senses does not seem to be an issue.
Re-lated senses are scored correctly.
Indeed the per-formance of the fine-grained training is superior tothat of the coarse-grained training.
We believe thisis because predominant meanings have more relatedsenses.
There are therefore more chances that thedistributional similarity of the neighbours will getapportioned to one of the related senses when thereare more related senses.
The coarse grained rank-ing would have an advantage on occasions when inthe fine-grained ranking the credit between relatedsenses is split and an unrelated sense ends up witha higher ranking score.
Since the coarse-grainedranker lumps the credit for related sense together itwould be at an advantage.
Clearly this doesn?t hap-pen enough in the data to outweigh the beneficialeffect of the number of related senses compensatingfor other noise in the data.Doc.Id.
Class SUSSX-FR SUSSX-C-WDd001 Medicine 0.556 0.560d002 Economy 0.508 0.515d003 Transport 0.487 0.454d004 Comp-Sci 0.407 0.424d005 Psychology 0.356 0.372Table 2: Impact of domain specialisation for each ofthe five documents (F1 scores).Domain Unfortuately, the system specialised fordomain (SUSSX-C-WD) did not improve the resultsover the 5 documents significantly.
However, if welook at the contributions made by each document,we might learn something about the relation beteenthe output of the classifier and the impact on theWSDresults.
Table 2 shows the per-document resultsfor the systems SUSSX-FR and SUSSX-C-WD.
Thefirst two documents show very little difference withthe domain independent results.
The documents?d004?
and ?d005?
show a small but clear improvedperformance for the domain results.
Unfortunately,document ?d003?
displays a very disappointing dropof more than 3% in performance, and cancels out allthe gains made by the last two documents.The output of the classifier seems to be indica-tive of the results for all documents except ?d003?.The classifier doesn?t seem to find enough evidencefor a marked preference for a particular domain316for documents ?d001?
and ?d002?.
This could bean indication that there is no strong domain ef-fect to be expected.
The strong preference for the?computer science?
domain for ?d004?
is reflected ingood performance of SUSSX-C-WD and even thoughthe confidence scores for the first 2 alternatives of?d005?
are fairly close, there is a clear drop in con-fidence score for the third alternative, which mightindicate that the topic of this document is related toboth first choices of the classifier.
It will be interest-ing to evaluate the results for ?d005?
using the ?Art?sense rankings.
One would expect those results to besimilar to the results found here.
Finally, the resultsfor ?d003?
are hard to explain.
We will need to do anextensive error analysis as soon as the gold-standardis available.6 ConclusionsIn this paper we investigated two directions wherewe expect potential for improving the performanceof our method for acquiring predominant senses.In order to fully appreciate what the effects of thecoarse grained sense inventory are (i.e.
whethersome of the more unfortunate errors are resolved),we will have to do an extensive error analysis assoon as the gold standard becomes available.
Con-sidering the fairly low number of attempted tokens(only 72.8% of the tokens are attempted), we are ata disadvantage compared to systems that back-off to(for example) the first sense in WN.
However, we arewell pleased with the high precision (71.7%) of themethod SUSSX-FR, considering this is a completelyunsupervised method.
There seems to be potentialgains for domain adaptation, but applying it to eachdocument does not seem to be advisable.
More re-search needs to be done to identify in which cases aperformance boost can be expected.
Five documentsis not enough to fully investigate the matter.
At themoment we are performing a larger scale experimentwith the documents in SemCor.
These documentsseem to cover a fairly wide range of domains (ac-cording to our text classifier) and many domains arerepresented by several documents.AcknowledgementsThis work was funded by UK EPSRC projectEP/C537262 ?Ranking Word Senses for Disam-biguation: Models and Applications?, and by a UKRoyal Society Dorothy Hodgkin Fellowship to thesecond author.
We would also like to thank PiekVossen for giving us access to the Irion Technolo-gies text categoriser.ReferencesTed Briscoe and John Carroll.
2002.
Robust accuratestatistical annotation of general text.
In Proceedingsof LREC-2002, pages 1499?1504, Las Palmas de GranCanaria.Anne Diekema.
http://www.oclc.org/dewey/.Jay Jiang and David Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
In In-ternational Conference on Research in ComputationalLinguistics, Taiwan.Rob Koeling, Diana McCarthy, and John Carroll.
2005.Domain-specific sense distributions and predominantsense acquisition.
In Proceedings of the HumanLanguage Technology Conference and Conference onEmpirical Methods in Natural Language Processing.,pages 419?426, Vancouver, Canada.Rob Koeling, Diana McCarthy, and John Carroll.
2007.Text categorization for improved priors of word mean-ing.
In Proceedings of the Eighth International Con-ference on Intelligent Text Processing and Compu-tational Linguistics (Cicling 2007), pages 241?252,Mexico City, Mexico.Geoffrey Leech.
1992.
100 million words of English:the British National Corpus.
Language Research,28(1):1?13.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of COLING-ACL 98,Montreal, Canada.Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo,and Alfio Gliozzo.
2002.
The role of domain informa-tion in word sense disambiguation.
Natural LanguageEngineering, 8(4):359?373.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant senses in un-tagged text.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics,pages 280?287, Barcelona, Spain.Siddharth Patwardhan and Ted Pedersen.2003.
The cpan wordnet::similarity package.http://search.cpan.org/s?id/WordNet-Similarity/.317
