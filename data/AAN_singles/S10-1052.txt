Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 234?237,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsUCD-PN: Selecting General Paraphrases Using Conditional ProbabilityPaul NultyUniversity College DublinDublin, Irelandpaul.nulty@ucd.ieFintan CostelloUniversity College DublinDublin, Irelandfintan.costello@ucd.ieAbstractWe describe a system which ranks human-provided paraphrases of noun compounds,where the frequency with which a givenparaphrase was provided by human volun-teers is the gold standard for ranking.
Oursystem assigns a score to a paraphrase ofa given compound according to the num-ber of times it has co-occurred with otherparaphrases in the rest of the dataset.
Weuse these co-occurrence statistics to com-pute conditional probabilities to estimate asub-typing or Is-A relation between para-phrases.
This method clusters togetherparaphrases which have similar meaningsand also favours frequent, general para-phrases rather than infrequent paraphraseswith more specific meanings.1 IntroductionSemEval 2010 Task 9, ?Noun Compound Inter-pretation Using Paraphrasing Verbs?, requires sys-tems to rank paraphrases of noun compoundsaccording to which paraphrases were most fre-quently produced for each compound by humanannotators (Butnariu et al, 2010).
This paper de-scribes a system which ranks a paraphrase for agiven compound by computing the probability ofthe paraphrase occurring given that we have previ-ously observed that paraphrase co-occurring withother paraphrases in the candidate paraphrase list.These co-occurrence statistics can be built usingeither the compounds from the test set or the train-ing set, with no significant difference in results.The model is informed by two observations:people tend to use general, semantically light para-phrases more often than detailed, semanticallyheavy ones, and most paraphrases provided for aspecific compound indicate the same interpreta-tion of that compound, varying mainly accordingto level of semantic detail.Given these two properties of the data, the ob-jective of our system was to test the theory thatconditional probabilities can be used to estimate asub-typing or Is-A relation between paraphrases.No information about the compounds was used,nor were the frequencies provided in the trainingset used.2 MotivationMost research on the disambiguation of noun com-pounds involves automatically categorizing thecompound into one of a pre-defined list of seman-tic relations.
Paraphrasing compounds is an alter-native approach to the disambiguation task whichhas been explored by (Lauer, 1995) and (Nakov,2008).
Paraphrases of semantic relations may beverbs, prepositions, or ?prepositional verbs?
likefound in and caused by.
(Lauer, 1995) catego-rized compounds using only prepositions.
(Nakov,2008) and the current task use only verbs andprepositional verbs, however, many of the para-phrases in the task data are effectively just prepo-sitions with a copula, e.g.
be in, be for, be of.The paraphrasing approach may be easier tointegrate into applications such as translation,query-expansion and question-answering ?
itsoutput is a set of natural language phrases ratherthan an abstract relation category.
Also, mostsets of pre-defined semantic relations have onlyone or maybe two levels of granularity.
Thiscan often lead to semantically converse relationsfalling under the same abstract category, for ex-ample a headache tablet is a tablet for prevent-ing headaches, while headache weather is weatherthat induces headaches ?
but both compoundswould be assigned the same relation (perhaps in-strumental or causal) in many taxonomies of se-mantic relations.
Paraphrases of compounds usingverbs or verb-preposition combinations can pro-vide as much or as little detail as is required toadequately disambiguate the compound.2342.1 General paraphrases are frequentThe object of SemEval 2010 Task 9 is to rank para-phrases for noun compounds given by 50-100 hu-man annotators.
When deciding on a model wetook into account several observations about thedata.Firstly, the model does not need to produceplausible paraphrases for noun compounds, it sim-ply needs to rank paraphrases that have been pro-vided.
Given that all of the paraphrases in thetraining and test sets have been produced by peo-ple, we presume that all of them will have atleast some plausible interpretation, and most para-phrases for a given compound will indicate gen-erally the same interpretation of that compound.This will not always be the case; some compoundsare genuinely ambiguous rather than vague.
Forexample a stone bowl could be a bowl for hold-ing stones or a bowl made of stone.
However, themere fact that a compound has occurred in text isevidence that the speaker who produced the textbelieved that the compound was unambiguous, atleast in the given context.Given that most of the compounds in the datasethave one clear plausible meaning to readers, whenasked to paraphrase a compound people tend toobserve the Grician maxim of brevity (Grice,1975) by using simple, frequent terms rather thandetailed, semantically weighty paraphrases.
Forthe compound alligator leather in the trainingdata, the two most popular paraphrases were bemade from and come from.
Also provided asparaphrases for this compound were hide of andbe skinned from.
These are more detailed, spe-cific, and more useful than the most popular para-phrases, but they were only produced once each,while be made from and come from were pro-vided by 28 and 20 annotators respectively.
Thistrend is noticeable in most of the compounds inthe training data - the most specific and detailedparaphrases are not the most frequently produced.According to the lesser-known of Zipf?s laws ?the law of meaning (Zipf, 1945) ?
words that aremore frequent overall in a language tend to havemore sub-senses.
Frequent terms have a shorterlexical access time (Broadbent, 1967), so to min-imize the effort required to communicate mean-ing of a compound, speakers should tend to usethe most common words - which tend to be se-mantically general and have many possible sub-senses.
This seems to hold for paraphrasing verbsand prepositions; terms that have a high overallfrequency in English such as be in, have and be ofare vague ?
there are many more specific para-phrases which could be considered sub-senses ofthese common terms.2.2 Using conditional probability to detectsubtypesOur model uses conditional probabilities to detectthis sub-typing structure based on the theory thatobserving a specific, detailed paraphrase is goodevidence that a more general parent sense of thatparaphrase would be acceptable in the same con-text.
The reverse is not true - observing a fre-quently occurring, semantically light paraphraseis not strong evidence that any sub-sense of thatparaphrase would be acceptable in the same con-text.
For example, consider the spatial and tempo-ral sub-senses of the paraphrase be in.
A possiblespatial sub-sense of this paraphrase is be locatedin, while a possible temporal sub-sense would beoccur during.
The fact that occur during is pro-vided as a paraphrase for a compound almost al-ways means that be in is also a plausible para-phrase.
However, observing be in as a paraphrasedoes not provide such strong evidence for occurduring also being plausible, as we do not knowwhich sub-sense of in is intended.If this is correct, then we would expect that theconditional probability of a paraphrase B occur-ring given that we have observed another para-phrase A in the same context is a measure of theextent to which B is a more general type (parentsense) of A.3 System DescriptionThe first step in our model is to generate a condi-tional probability table by going over all the com-pounds in the data and calculating the probabil-ity of each paraphrase occurring given that we ob-served another given paraphrase co-occurring forthe same compound.
We compute the conditionalprobability of every paraphrase with all other para-phrases individually.
We could use either the train-ing or the test set to collect these co-occurrencestatistics, as the frequencies with which the para-phrases are ranked are not used ?
we simply notehow many times each paraphrase co-occurred as apossible paraphrase for the same compound witheach other paraphrase.
For the submitted systemwe used the test data, but subsequently we con-235firmed that using only the training data for this stepis not detrimental to the system?s performance.For each paraphrase in the data, the conditionalprobability of that paraphrase is computed with re-spect to all other paraphrases in the data.
For anytwo paraphrases B and A:P (B|A) =P (A ?B)P (A)As described in the previous section, we antic-ipate that more general, less specific paraphraseswill be produced more often than their more de-tailed sub-senses.
Therefore, we score each para-phrase by summing its conditional probabilitywith each other paraphrase provided for the samecompound.For a list of paraphrases A provided for a givencompound, we score a paraphrase b in that list bysumming its conditional probability individuallywith every other paraphrase in the list.score(b) =?a?AP (b|a)This gives the more general, broad coverage,paraphrases a higher score, and also has a cluster-ing effect whereby paraphrases that have not co-occurred with the other paraphrases in the list veryoften for other compounds are given a lower score?
they are unusual in the context of this para-phrase list.4 Results and Analysis4.1 Task resultsTable 1 shows the results of the top 3 systems inthe task.
Our system achieved the second high-est correlation according to the official evaluationmeasure, Spearman?s rank correlation coefficient.Results were also provided using Pearson?s corre-lation coefficient and the cosine of the vector ofscores for the gold standard and submitted pre-dictions.
Our system performed best using thecosine measure, which measures how closely thepredicted scores match the gold standard frequen-cies, rather than the rank correlation.
This couldbe helpful as the scores provide a scale of accept-ability.As mentioned in the system description, wecollected the co-occurrence statistics for our sub-mitted prediction from the test set of paraphrasesalone.
Since our model does not use the frequen-cies provided in the training set, we chose to useSystem Spearman Pearson CosineUVT .450 .411 .635UCD-PN .441 .361 .669UCD-GOG .432 .395 .652baseline .425 .344 .524Table 1: Results for the top three systems.the test set as it was larger and had more annota-tors.
This could be perceived as an unfair use ofthe test data, as we are using all of the test com-pounds and their paraphrases to calculate the po-sition of a given paraphrase relative to other para-phrases.This is a kind of clustering which would not bepossible if only a few test cases were provided.
Tocheck that our system did not need to collect co-occurrence probabilities on exactly the same dataas it made predictions on, we submitted a secondset of predictions for the test based on the proba-bilities from the training compounds alone.1These predictions actually achieved a slightlybetter score for the official evaluation measure,with a Spearman rho of 0.444, and a cosine of0.631.
This suggests that the model does not needto collect co-occurrence statistics from the samecompounds as it makes predictions on, as long assufficient data is available.4.2 Error AnalysisThe most significant drawback of this system isthat it cannot generate paraphrases for noun com-pounds - it is designed to rank paraphrases thathave already been provided.Using the conditional probability to rank para-phrases has two effects.
Firstly there is a cluster-ing effect which favours paraphrases that are moresimilar to the other paraphrases in a list for a givencompound.
Secondly, paraphrases which are morefrequent overall receive a higher score, as frequentverbs and prepositions may co-occur with a widevariety of more specific terms.These effects lead to two possible drawbacks.Firstly, the system would not perform well if de-tailed, specific paraphrases of compounds wereneeded.
Although less frequent, more specificparaphrases may be more useful for some appli-cations, these are not the kind of paraphrases thatpeople seem to produce spontaneously.1Thanks to Diarmuid?O S?eaghdha for pointing this outand scoring the second set of predictions236Also, because of the clustering effect, this sys-tem would not work well for compounds that aregenuinely ambiguous e.g.
stone bowl (bowl madeof stone vs bowl contains stones).
Most examplesare not this ambiguous, and therefore almost allof the provided paraphrases for a given compoundare plausible, and indicate the same relation.
Theyvary mainly in how specific/detailed their explana-tion of the relation is.The three compounds which our system pro-duced the worst rank correlation for were dieselengine, midnight train, and bathing suit.
With-out access to the gold-standard scores for thesecompounds it is difficult to explain the poor per-formance, but examining the list of possible para-phrases for the first two of these suggests that theannotators identified two distinct senses for each:diesel engine is paraphrased by verbs of contain-ment (e.g.
be in) and verbs of function (e.g.
runson), while midnight train is paraphrased by verbsof location (e.g.
be found in, be located in) andverbs of movement (e.g.
run in, arrive at).
Ourmodel works by separating paraphrases accordingto granularity, and cannot disambiguate these dis-tinct senses.
The list of possible paraphrases forbathing suit suggests that our model is not robustif implausible paraphrases are in the candidate list- the model ranked be in, be found in and emergefrom among the top 8 paraphrases for this com-pound, even though they are barely comprehensi-ble as plausible paraphrases.
The difficulty hereis that even if only one annotator suggests a para-phrase, it is deemed to have co-occurred with otherparaphrases in that list, since we do not use the fre-quencies from the training set.The compounds for which the highest correla-tions were achieved were wilderness areas, conso-nant systems and fiber optics.
The candidate para-phrases for the first two of these seem to be fairlyhomogeneous in semantic intent.
Fiber opticsis probably a lexicalised compound which hardlyneeds paraphrasing.
This would lead people to useshort and semantically general paraphrases.5 ConclusionWe have described a system which uses a simplestatistical method, conditional probability, to es-timate a sub-typing relationship between possibleparaphrases of noun compounds.
From a list ofcandidate paraphrases for each noun compound,those which were judged by this method to begood ?parent senses?
of other paraphrases in thelist were scored highly in the rankings.The system does require a large dataset of com-pounds with associated plausible paraphrases, butit does not require a training set of human pro-vided rankings and does not use any informationabout the noun compound itself, aside from the listof plausible paraphrases that were provided by thehuman annotators.Given the simplicity of our model and its per-formance compared to other systems which usedmore intensive approaches, we believe that our ini-tial observations on the data are valid: people tendto produce general, semantically light paraphrasesmore often than specific or detailed paraphrases,and most of the paraphrases provided for a givencompound indicate a similar interpretation, vary-ing instead mainly in level of semantic weight ordetail.We have also shown that conditional probabil-ity is an effective way to compute the sub-typingrelation between paraphrases.AcknowledgementThis research was supported by a grant under theFP6 NEST Programme of the European Commis-sion (ANALOGY: Humans the Analogy-MakingSpecies: STREP Contr.
No 029088).ReferencesDonald E. Broadbent 1967.
Word-frequency effectand response bias..
Psychological Review, 74,Cristina Butnariu and Su Nam Kim and Preslav Nakovand Diarmuid?O S?eaghdha and Stan Szpakowicz andTony Veale.
2010.
SemEval-2 Task 9: The In-terpretation of Noun Compounds Using Paraphras-ing Verbs and Prepositions, Proceedings of the 5thSIGLEX Workshop on Semantic Evaluation, Upp-sala, SwedenPaul Grice.
1975.
Studies in the Way of Words.
Har-vard University Press, Cambridge, Mass.Mark Lauer 1995.
Designing statistical languagelearners: experiments on noun compound, PhD The-sis Macquarie University, AustraliaPreslav Nakov and Marti Hearst 2008.
Solving Re-lational Similarity Problems using the Web as aCorpus.
In Proceedings of the 46th Annual Meet-ing of the Association for Computational Linguistics(ACL-08), Columbus, OH.George Kingsley Zipf.
1945.
The Meaning-FrequencyRelationship of Words.
Journal of General Psychol-ogy, 33,237
