Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 568?575,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsImproving the Interpretation of Noun Phrases with Cross-linguisticInformationRoxana GirjuUniversity of Illinois at Urbana-Champaigngirju@uiuc.eduAbstractThis paper addresses the automatic classifi-cation of semantic relations in noun phrasesbased on cross-linguistic evidence from aset of five Romance languages.
A setof novel semantic and contextual English?Romance NP features is derived based onempirical observations on the distributionof the syntax and meaning of noun phraseson two corpora of different genre (Europarland CLUVI).
The features were employedin a Support Vector Machines algorithmwhich achieved an accuracy of 77.9% (Eu-roparl) and 74.31% (CLUVI), an improve-ment compared with two state-of-the-artmodels reported in the literature.1 IntroductionSemantic knowledge is very important for any ap-plication that requires a deep understanding of natu-ral language.
The automatic acquisition of semanticinformation in text has become increasingly impor-tant in ontology development, information extrac-tion, question answering, and other advanced naturallanguage processing applications.In this paper we present a model for the auto-matic semantic interpretation of noun phrases (NPs),which is the task of determining the semantic re-lation among the noun constituents.
For example,family estate encodes a POSSESSION relation, whiledress of silk refers to PART-WHOLE.
The problem,while simple to state is hard to solve.
The rea-son is that the meaning of these constructions ismost of the time ambiguous or implicit.
InterpretingNPs correctly requires various types of informationfrom world knowledge to complex context features.Moreover, the extension of this task to other natu-ral languages brings forward new issues and prob-lems.
For instance, beer glass translates into tarrode cerveza in Spanish, bicchiere da birra in Italian,verre a` bie`re in French, and pahar de bere in Roma-nian.
Thus, an important research question is howdo the syntactic constructions in the target languagecontribute to the preservation of meaning in context.In this paper we investigate noun phrases based oncross-linguistic evidence and present a domain inde-pendent model for their semantic interpretation.
Weaim at uncovering the general aspects that governthe semantics of NPs in English based on a set offive Romance languages: Spanish, Italian, French,Portuguese, and Romanian.
The focus on Romancelanguages is well motivated.
It is mostly true thatEnglish noun phrases translate into constructions ofthe form N P N in Romance languages where, aswe will show below, the P (preposition) varies inways that correlate with the semantics.
Thus Ro-mance languages will give us another source of evi-dence for disambiguating the semantic relations inEnglish NPs.
We also present empirical observa-tions on the distribution of the syntax and meaningof noun phrases on two different corpora based ontwo state-of-the-art classification tag sets: Lauer?sset of 8 prepositions (Lauer, 1995) and our list of 22semantic relations.
We show that various crosslin-gual cues can help in the NP interpretation task whenemployed in an SVM model.
The results are com-pared against two state of the art approaches: a su-568pervised machine learning model, Semantic Scatter-ing (Moldovan and Badulescu, 2005), and a web-based probabilistic model (Lapata and Keller, 2004).The paper is organized as follows.
In Section 2we present a summary of the previous work.
Sec-tion 3 lists the syntactic and semantic interpretationcategories used along with observations regardingtheir distribution on the two different cross-lingualcorpora.
Sections 4 and 5 present a learning modeland results for the interpretation of English nounphrases.
Finally, in Section 6 we offer some dis-cussion and conclusions.2 Related WorkCurrently, the best-performing NP interpretationmethods in computational linguistics focus mostlyon two consecutive noun instances (noun com-pounds) and rely either on rather ad-hoc, domain-specific semantic taxonomies, or on statistical mod-els on large collections of unlabeled data.
Recentresults have shown that symbolic noun compoundinterpretation systems using machine learning tech-niques coupled with a large lexical hierarchy per-form with very good accuracy, but they are most ofthe time tailored to a specific domain (Rosario andHearst, 2001).
On the other hand, the majority ofcorpus statistics approaches to noun compound in-terpretation collect statistics on the occurrence fre-quency of the noun constituents and use them in aprobabilistic model (Lauer, 1995).
More recently,(Lapata and Keller, 2004) showed that simple unsu-pervised models perform significantly better whenthe frequencies are obtained from the web, ratherthan from a large standard corpus.
Other researchers(Pantel and Pennacchiotti, 2006), (Snow et al, 2006)use clustering techniques coupled with syntactic de-pendency features to identify IS-A relations in largetext collections.
(Kim and Baldwin, 2006) and (Tur-ney, 2006) focus on the lexical similarity of unseennoun compounds with those found in training.However, although the web-based solution mightovercome the data sparseness problem, the currentprobabilistic models are limited by the lack of deeplinguistic information.
In this paper we investigatethe role of cross-linguistic information in the taskof English NP semantic interpretation and show theimportance of a set of novel linguistic features.3 Corpus AnalysisFor a better understanding of the meaning of theN N and N P N instances, we analyzed the seman-tic behavior of these constructions on a large cross-linguistic corpora of examples.
We are interestedin what syntactic constructions are used to trans-late the English instances to the target Romance lan-guages and vice-versa, what semantic relations dothese constructions encode, and what is the corpusdistribution of the semantic relations.3.1 Lists of semantic classification relationsAlthough the NP interpretation problem has beenstudied for a long time, researchers haven?t agreedon the number and the level of abstraction of thesesemantic categories.
They can vary from a fewprepositions (Lauer, 1995) to hundreds or thousandsspecific semantic relations (Finin, 1980).
The moreabstract the categories, the more noun phrases arecovered, but also the more room for variation as towhich category a phrase should be assigned.In this paper we experiment with two state of theart classification sets used in NP interpretation.
Thefirst is a core set of 22 semantic relations (22 SRs)identified by us from the computational linguisticsliterature.
This list, presented in Table 1 along withexamples is general enough to cover a large major-ity of text semantics while keeping the semantic re-lations to a manageable number.
The second set isLauer?s list of 8 prepositions (8 PP) and can be ap-plied only to noun compounds (of, for, with, in, on,at, about, and from ?
e.g., according to this classifi-cation, love story can be classified as story aboutlove).
We selected these sets as they are of differentsize and contain semantic classification categories atdifferent levels of abstraction.
Lauer?s list is moreabstract and, thus capable of encoding a large num-ber of noun compound instances, while the 22-SRlist contains finer grained semantic categories.
Weshow below the coverage of these semantic lists ontwo different corpora and how well they solve theinterpretation problem of noun phrases.3.2 The dataThe data was collected from two text collectionswith different distributions and of different genre,569POSSESSION (family estate); KINSHIP (sister of the boy); PROPERTY (lubricant viscosity); AGENT (return of the natives);THEME (acquisition of stock); TEMPORAL (morning news); DEPICTION-DEPICTED (a picture of my niece); PART-WHOLE(brush hut); HYPERNYMY (IS-A) (daisy flower); CAUSE (scream of pain); MAKE/PRODUCE (chocolate factory); INSTRUMENT(laser treatment); LOCATION (castle in the desert); PURPOSE (cough syrup); SOURCE (grapefruit oil); TOPIC (weather report);MANNER (performance with passion); beneficiary (rights of citizens); MEANS (bus service); EXPERIENCER (fear of the girl);MEASURE (cup of sugar); TYPE (framework law);Table 1: The list of 22 semantic relations (22-SRs).Europarl1 and CLUVI2.
The Europarl data was as-sembled by combining the Spanish-English, Italian-English, French-English and Portuguese-Englishcorpora which were automatically aligned based onexact matches of English translations.
Then, weconsidered only the English sentences which ap-peared verbatim in all four language pairs.
The re-sulting English corpus contained 10,000 sentenceswhich were syntactically parsed (Charniak, 2000).From these we extracted the first 3,000 NP instances(N N: 48.82% and N P N: 51.18%).CLUVI is an open text repository of parallel cor-pora of contemporary oral and written texts in someof the Romance languages.
Here, we focused onlyon the English-Portuguese and English-Spanish par-allel texts from the works of John Steinbeck, H. G.Wells, J. Salinger, and others.
Using the CLUVIsearch interface we created a sentence-aligned par-allel corpus of 2,800 English-Spanish and English-Portuguese sentences.
The English versions wereautomatically parsed after which each N N andN P N instance thus identified was manually mappedto the corresponding translations.
The resulting cor-pus contains 2,200 English instances with a distribu-tion of 26.77% N N and 73.23% N P N.3.3 Corpus AnnotationFor each corpus, each NP instance was presentedseparately to two experienced annotators in a webinterface in context along with the English sentenceand its translations.
Since the corpora do not coversome of the languages (Romanian in Europarl andCLUVI, and Italian and French in CLUVI), threeother native speakers of these languages and flu-ent in English provided the translations which were1http://www.isi.edu/koehn/europarl/.
This corpus containsover 20 million words in eleven official languages of the Euro-pean Union covering the proceedings of the European Parlia-ment from 1996 to 2001.2CLUVI - Linguistic Corpus of the University of Vigo - Par-allel Corpus 2.1 - http://sli.uvigo.es/CLUVI/added to the list.
The two computational semanticsannotators had to tag each English constituent nounwith its corresponding WordNet sense and each in-stance with the corresponding semantic category.
Ifthe word was not found in WordNet the instance wasnot considered.
Whenever the annotators found anexample encoding a semantic category other thanthose provided or they didn?t know what interpre-tation to give, they had to tag it as ?OTHER-SR?, andrespectively ?OTHER-PP?3.
The details of the anno-tation task and the observations drawn from there arepresented in a companion paper (Girju, 2007).The corpus instances used in the corpus analy-sis phase have the following format: <NPEn ;NPEs;NPIt; NPFr; NPPort; NPRo; target>.
The wordtarget is one of the 23 (22 + OTHER-SR) seman-tic relations and one of the eight prepositions con-sidered or OTHER-PP (with the exception of thoseN P N instances that already contain a preposi-tion).
For example, <development cooperation;cooperacio?n para el desarrollo; cooperazione allosviluppo; coope?ration au de?veloppement; cooperarepentru dezvoltare; PURPOSE / FOR>.The annotators?
agreement was measured usingKappa statistics: K = Pr(A)?Pr(E)1?Pr(E) , where Pr(A)is the proportion of times the annotators agree andPr(E) is the probability of agreement by chance.The Kappa values were obtained on Europarl (N N:0.80 for 8-PP and 0.61 for 22-SR; N P N: 0.67 for22-SR) and CLUVI (N N: 0.77 for 8-PP and 0.56 for22-SR; N P N: 0.68 for 22-SR).
We also computedthe number of pairs that were tagged with OTHERby both annotators for each semantic relation andpreposition paraphrase, over the number of exam-ples classified in that category by at least one of thejudges (in Europarl: 91% for 8-PP and 78% for 22-SR; in CLUVI: 86% for 8-PP and 69% for 22-SR).The agreement obtained on the Europarl corpus is3The annotated corpora resulted in this research is availableat http://apfel.ai.uiuc.edu.570higher than the one on CLUVI on both classificationsets.
This is partially explained by the distribution ofsemantic relations in both corpora, as will be shownin the next subsection.3.4 Cross-linguistic distribution of SyntacticConstructionsFrom the sets of 2,954 (Europarl) and 2,168(CLUVI) instances resulted after annotation, thedata show that over 83% of the translation patternsfor both text corpora on all languages were of thetype N N and N P N. However, while their distribu-tion is balanced in the Europarl corpus (about 45%,with a 64% N P N ?
26% N N ratio for Romanian),in CLUVI the N P N constructions occur in morethan 85% of the cases (again, with the exception ofRomanian ?
50%).
It is interesting to note here thatsome of the English NPs are translated into bothnoun?noun and noun?adjective compounds in thetarget languages.
For example, love affair translatesin Italian as storia d?amore or the noun?adjectivecompound relazione amorosa.
There are also in-stances that have just one word correspondent inthe target language (e.g., ankle boot is bottine inFrench).
The rest of the data is encoded by othersyntactic paraphrases (e.g., bomb site is luogo dovee` esplosa la bomba (It.)).
4.From the initial corpus we considered those En-glish instances that had all the translations encodedonly by N N and N P N. Out of these, we selectedonly 1,023 Europarl and 1,008 CLUVI instances en-coded by N N and N P N in all languages consideredand resulted after agreement.4 Model4.1 Feature spaceWe have identified and experimented with 13 NPfeatures presented below.
With the exceptions offeatures F1-F5 (Girju et al, 2005), all the other fea-tures are novel.A.
English FeaturesF1 and F2.
Noun semantic class specifies the Word-Net sense of the head (F1) and modifier noun (F2)and implicitly points to all its hypernyms.
For ex-ample, the hypernyms of car#1 are: {motor vehi-4?the place where the bomb is exploded?
(It.
)cle}, .. {entity}.
This feature helps generalize overthe semantic classes of the two nouns in the corpus.F3 and F4.
WordNet derivationally related formspecifies if the head (F3) and the modifier (F4) nounsare related to a corresponding WordNet verb (e.g.statement derived from to state; cry from to cry).F5.
Prepositional cues that link the two nouns in anNP.
These can be either simple or complex preposi-tions such as ?of?
or ?according to?.
In case of N Ninstances, this feature is ???
(e.g., framework law).F6 and F7.
Type of nominalized noun indicates thespecific class of nouns the head (F6) or modifier (F7)belongs to depending on the verb it derives from.First, we check if the noun is a nominalization.
ForEnglish we used NomLex-Plus (Meyers et al, 2004)to map nouns to corresponding verbs.5 For exam-ple, ?destruction of the city?, where destruction isa nominalization.
F6 and F7 may overlap with fea-tures F3 and F4 which are used in case the noun to bechecked does not have an entry in the NomLex-Plusdictionary.
These features are of particular impor-tance since they impose some constraints on the pos-sible set of relations the instance can encode.
Theytake the following values (identified based on list ofverbs extracted from VerbNet (Kipper et al, 2000)):a.
Active form nouns which have an intrinsicactive voice predicate-argument structure.
(Giorgiand Longobardi, 1991) argue that in English this is anecessary restriction.
Most of the time, they rep-resent states of emotion, such as fear, desire, etc.These nouns mark their internal argument throughof and require most of the time prepositions like porand not de when translated in Romance.
Our obser-vations on the Romanian translations (captured byfeatures F12 and F13 below) show that the possiblecases of ambiguity are solved by the type of syntac-tic construction used.
For example, N N genitive-marked constructions are used for EXPERIENCER?encoding instances, while N de N or N pentru N (Nfor N) are used for other relations.
Such examplesare the love of children ?
THEME (and not the love bythe children).
(Giorgi and Longobardi, 1991) men-tion that with such nouns that resist passivisation,5NomLex-Plus is a hand-coded database of 5,000 verb nom-inalizations, de-adjectival, and de-adverbial nouns including thecorresponding subcategorization frames (verb-argument struc-ture information).571the preposition introducing the internal argument,even if it is of, has always a semantic content, andis not a bare case-marker realizing the genitive case.b.
Unaccusative (ergative) nouns which are de-rived from ergative verbs that take only internal ar-guments (e.g., not agentive ones).
For example, thetransitive verb to disband allows the subject to bedeleted as in the following sentences (1) ?The leadsinger disbanded the group in 1991.?
and (2) ?Thegroup disbanded.?.
Thus, the corresponding erga-tive nominalization the disbandment of the group en-codes a THEME relation and not AGENT.c.
Unergative (intransitive) nouns are derivedfrom intransitive verbs and take only AGENT seman-tic relations.
For example, the departure of the girl.d.
Inherently passive nouns such as the cap-ture of the soldier.
These nouns, like the verbs theyare derived from, assume a default AGENT (subject)and being transitive, associate to their internal argu-ment (introduced by ?of?
in the example above) theTHEME relation.B.
Romance FeaturesF8, F9, F10, F11 and F12.
Prepositional cues thatlink the two nouns are extracted from each transla-tion of the English instance: F8 (Es.
), F9 (Fr.
), F10(It.
), F11 (Port.
), and F12 (Ro.).
These can be eithersimple or complex prepositions (e.g., de, in materiade (Es.))
in all five Romance languages, or the Ro-manian genitival article a/ai/ale.
In Romanian thegenitive case is assigned by the definite article of thefirst noun to the second noun, case realized as a suf-fix if the second noun is preceded by the definite arti-cle or as one of the genitival articles a/ai/ale.
For ex-ample, the noun phrase the beauty of the girl is trans-lated as frumuset?ea fetei (beauty-the girl-gen), andthe beauty of a girl as frumuset?ea unei fete (beauty-the gen girl).
For N N instances, this feature is ???.F13.
Noun inflection is defined only for Romanianand shows if the modifier noun is inflected (indicatesthe genitive case).
This feature is used to help differ-entiate between instances encoding IS-A and othersemantic relations in N N compounds in Romanian.It also helps in features F6 and F7, case a) when thechoice of syntactic construction reflects different se-mantic content.
For example, iubirea pentru copii(N P N) (the love for children) and not iubirea copi-ilor (N N) (love expressed by the children).4.2 Learning ModelsWe have experimented with the support vector ma-chines (SVM) model6 and compared the resultsagainst two state-of-the-art models: a supervisedmodel, Semantic Scattering (SS), (Moldovan andBadulescu, 2005), and a web-based unsupervisedmodel (Lapata and Keller, 2004).
The SVM and SSmodels were trained and tested on the Europarl andCLUVI corpora using a 8:2 ratio.
The test datasetwas randomly selected from each corpus and the testnouns (only for English) were tagged with the cor-responding sense in context using a state of the artWSD tool (Mihalcea and Faruque, 2004).After the initial NP instances in the training andtest corpora were expanded with the correspondingfeatures, we had to prepare them for SVM and SS.The method consists of a set of automatic iterativeprocedures of specialization of the English nouns onthe WordNet IS-A hierarchy.
Thus, after a set of nec-essary specialization iterations, the method producesspecialized examples which through supervised ma-chine learning are transformed into sets of seman-tic rules.
This specialization procedure improvesthe system?s performance since it efficiently sepa-rates the positive and negative noun-noun pairs inthe WordNet hierarchy.Initially, the training corpus consists of examplesin the format exemplified by the feature space.
Notethat for the English NP instances, each noun con-stituent was expanded with the corresponding Word-Net top semantic class.
At this point, the general-ized training corpus contains two types of examples:unambiguous and ambiguous.
The second situationoccurs when the training corpus classifies the samenoun ?
noun pair into more than one semantic cat-egory.
For example, both relationships ?chocolatecake?-PART-WHOLE and ?chocolate article?-TOPICare mapped into the more general type <entity#1,entity#1, PART-WHOLE/TOPIC>7.
We recursivelyspecialize these examples to eliminate the ambigu-ity.
By specialization, the semantic class is replacedwith the corresponding hyponym for that particularsense, i.e.
the concept immediately below in the hi-erarchy.
These steps are repeated until there are no6We used the package LIBSVM with a radial-based kernelhttp://www.csie.ntu.edu.tw/?cjlin/libsvm/7The specialization procedure applies only to features 1, 2.572more ambiguous examples.
For the example above,the specialization stops at the first hyponym of en-tity: physical entity (for cake) and abstract entity(for article).
For the unambiguous examples in thegeneralized training corpus (those that are classifiedwith a single semantic relation), constraints are de-termined using cross validation on SVM.A.
Semantic Scattering uses a training data setto establish a boundary G?
on WordNet noun hier-archies such that each feature pair of noun ?
nounsenses fij on this boundary maps uniquely into oneof a predefined list of semantic relations, and anyfeature pair above the boundary maps into more thanone semantic relation.
For any new pair of noun?noun senses, the model finds the closest WordNetboundary pair.The authors define with SCm = {fmi } andSCh = {fhj } the sets of semantic class featuresfor modifier noun and, respectively head noun.
Apair of <modifier ?
head> nouns maps uniquelyinto a semantic class feature pair < fmi , fhj >,denoted as fij .
The probability of a semantic re-lation r given feature pair fij , P (r|fij) = n(r,fij)n(fij) ,is defined as the ratio between the number of oc-currences of a relation r in the presence of fea-ture pair fij over the number of occurrences offeature pair fij in the corpus.
The most proba-ble semantic relation r?
is arg maxr?R P (r|fij) =arg maxr?R P (fij |r)P (r).B.
(Lapata and Keller, 2004)?s web-based un-supervised model classifies noun - noun instancesbased on Lauer?s list of 8 prepositions and usesthe web as training corpus.
They show that thebest performance is obtained with the trigram modelf(n1, p, n2).
The count used for a given trigram isthe number of pages returned by Altavista on the tri-gram corresponding queries.
For example, for thetest instance war stories, the best number of hits wasobtained with the query stories about war.For the Europarl and CLUVI test sets, we repli-cated Lapata & Keller?s experiments using Google8.We formed inflected queries with the patterns theyproposed and searched the web.8As Google limits the number of queries to 1,000 per day,we repeated the experiment for a number of days.
Although(Lapata and Keller, 2004) used Altavista in their experiments,they showed there is almost no difference between the correla-tions achieved using Google and Altavista counts.5 Experimental resultsTable 2 shows the results obtained against SS andLapata & Keller?s model on both corpora and thecontribution the features exemplified in one baselineand six versions of the SVM model.
The baseline isdefined only for the English part of the NP featureset and measures the the contribution of the Word-Net IS-A lexical hierarchy specialization.
The base-line does not differentiate between unambiguous andambiguous training examples (after just one levelspecialization) and thus, does not specialize the am-biguous ones.
Moreover, here we wanted to see whatis the difference between SS and SVM, and what isthe contribution of the other English features, suchas preposition and nominalization (F1?F7).The table shows that, overall the performance isbetter for the Europarl corpus than for CLUVI.
Forthe Baseline and SV M1, SS [F1 + F2] gives bet-ter results than SVM.
The inclusion of other Englishfeatures (SVM [F1?F7]) adds more than 15% (witha higher increase in Europarl) for SV M1.The contribution of Romance linguistic features.Since our intuition is that the more translations areprovided for an English noun phrase instance, thebetter the results, we wanted to see what is the im-pact of each Romance language on the overall per-formance.
Thus, SV M2 shows the results obtainedfor English and the Romance language that con-tributed the least to the performance (F1?F12).
Herewe computed the performance on all five English ?Romance language combinations and chose the Ro-mance language that provided the best result.
Thus,SVM #2, #3, #4, #5, and #6 add Spanish, French,Italian, Portuguese, and Romanian in this order andshow the contribution of each Romance prepositionand all features for English.The language ranking in Table 2 shows that Ro-mance languages considered here have a differentcontribution to the overall performance.
While theaddition of Italian in Europarl decreases the per-formance, Portuguese doesn?t add anything.
How-ever, a closer analysis of the data shows that thisis mostly due to the distribution of the corpus in-stances.
For example, French, Italian, Spanish, andPortuguese are most of the time consistent in thechoice of preposition (e.g.
most of the time, if thepreposition ?de?
(?of?)
is used in French, then the573Learning models Results [%]CLUVI Europarl8-PP 22-SR 8-PP 22-SRBaseline (En.)
(no specializ.)
SS (F1+F2) 44.11 48.03 38.7 38SVM (F1+F2) 36.37 40.67 31.18 34.81SVM (F1-F7) ?
52.15 ?
47.37SVM1 (En.)
SS (F1+F2) 56.22 61.33 53.1 56.81SVM (F1+F2) 45.08 46.1 40.23 42.2SVM (F1-F7) ?
62.54 ?
74.19SVM2 (En.
+ Es.)
SVM (F1-F8) ?
64.18 ?
75.74SVM3 (En.+Es.+Fr.)
SVM (F1-F9) ?
67.8 ?
76.52SVM4 (En.+Es.+Fr.+It.)
SVM (F1-F10) ?
66.31 ?
75.74SVM5 (En.+Es.+Fr.+It+Port.)
SVM (F1-F11) ?
67.12 ?
75.74SVM6 (En.+Romance: F1?F13) ?
74.31 ?
77.9Lapata & Keller?s unsupervised model (En.)
44.15 ?
45.31 ?Table 2: The performance of the cross-linguistic SVM models compared against one baseline, SS model andLapata & Keller?s unsupervised model.
Accuracy (number of correctly labeled instances over the number ofinstances in the test set).corresponding preposition is used in the other fourlanguage translations).
A notable exception hereis Romanian which provides two possible construc-tions: the N P N and the genitive-marked N N. Thetable shows (in the increase in performance betweenSV M5 and SV M6) that this choice is not random,but influenced by the meaning of the instances (fea-tures F12, F13).
This observation is also supportedby the contribution of each feature to the overall per-formance.
For example, in Europarl, the WordNetverb and nominalization features of the head noun(F3, F6) have a contribution of 4.08%, while for themodifier nouns it decreases by about 2%.
The prepo-sition (F5) contributes 4.41% (Europarl) and 5.24%(CLUVI) to the overall performance.A closer analysis of the data shows that in Eu-roparl most of the N N instances were naming nouncompounds such as framework law (TYPE) and,most of the time, are encoded by N N patterns inthe target languages (e.g., legge quadro (It.)).
Inthe CLUVI corpus, on the other hand, the N N Ro-mance translations represented only 1% of the data.A notable exception here is Romanian where mostNPs are represented as genitive?marked noun com-pounds.
However, there are instances that are en-coded mostly or only as N P N constructions and thischoice correlates with the meaning of the instance.For example, the milk glass (PURPOSE) translatesas paharul de lapte (glass-the of milk) and not aspaharul laptelui (glass-the milk-gen), the olive oil(SOURCE) translates as uleiul de ma?sline (oil-the ofolive) and not as uleiul ma?slinei (oil-the olive-gen).Other examples include CAUSE and TOPIC.Lauer?s set of 8 prepositions represents 94.5%(Europarl) and 97% (CLUVI) of the N P N in-stances.
From these, the most frequent prepositionis ?of?
with a coverage of 70.31% (Europarl) and85.08% (CLUVI).
Moreover, in the Europarl cor-pus, 26.39% of the instances are synthetic phrases(where one of the nouns is a nominalization) encod-ing AGENT, EXPERIENCER, THEME, BENEFICIARY.Out of these instances, 74.81% use the prepositionof.
In CLUVI, 11.71% of the examples were ver-bal, from which the preposition of has a coverage of82.20%.
The many-to-many mappings of the prepo-sitions (especially of/de) to the semantic classes addsto the complexity of the interpretation task.
Thus,for the interpretation of these constructions a systemmust rely on the semantic information of the prepo-sition and two constituent nouns in particular, andon context in general.In Europarl, the most frequently occurring re-lations are PURPOSE, TYPE, and THEME that to-gether represent about 57% of the data followed byPART-WHOLE, PROPERTY, TOPIC, AGENT, and LO-CATION with an average coverage of about 6.23%.Moreover, other relations such as KINSHIP, DE-PICTION, MANNER, MEANS did not occur in thiscorpus and 5.08% represented OTHER-SR relations.This semantic distribution contrasts with the onein CLUVI, which uses a more descriptive lan-guage.
Here, the most frequent relation by far574is PART-WHOLE (32.14%), followed by LOCATION(12.40%), THEME (9.23%) and OTHER-SR (7.74%).It is interesting to note here that only 5.70% of theTYPE relation instances in Europarl were unique.This is in contrast with the other relations in bothcorpora, where instances were mostly unique.We also report here our observations on Lap-ata & Keller?s unsupervised model.
An analysisof these results showed that the order of the con-stituent nouns in the N P N paraphrase plays an im-portant role.
For example, a search for blood ves-sels generated similar frequency counts for vesselsof blood and blood in vessels.
About 30% noun -noun paraphrasable pairs preserved the order in thecorresponding N P N paraphrases.
We also manuallychecked the first five entries generated by Google foreach most frequent prepositional paraphrase for 50instances and noticed that about 35% of them werewrong due to syntactic and/or semantic ambiguities.Thus, since we wanted to measure the impact ofthese ambiguities of noun compounds on the inter-pretation performance, we further tested the prob-abilistic web-based model on four distinct test setsselected from Europarl, each containing 30 noun -noun pairs encoding different types of ambiguity:in set#1 the noun constituents had only one part ofspeech and one WordNet sense; in set#2 the nounshad at least two possible parts of speech and weresemantically unambiguous, in set#3 the nouns wereambiguous only semantically, and in set#4 they wereambiguous both syntactically and semantically.
Forunambiguous noun-noun pairs (set#1), the modelobtained an accuracy of 35.01%, while for more se-mantically ambiguous compounds it obtained an ac-curacy of about 48.8%.
This shows that for moresemantically ambiguous noun - noun pairs, the web-based probabilistic model introduces a significantnumber of false positives.
Thus, the more abstractthe categories, the more noun compounds are cov-ered, but also the more room for variation as towhich category a compound should be assigned.6 Discussion and ConclusionsIn this paper we presented a supervised, knowledge-intensive interpretation model which takes advan-tage of new linguistic information from English anda list of five Romance languages.
Our approach toNP interpretation is novel in several ways.
We de-fined the problem in a cross-linguistic frameworkand provided empirical observations on the distribu-tion of the syntax and meaning of noun phrases ontwo different corpora based on two state-of-the-artclassification tag sets.As future work we consider the inclusion of otherfeatures such as the semantic classes of Romancenouns from aligned EuroWordNets, and other sen-tence features.
Since the results obtained can be seenas an upper bound on NP interpretation due to per-fect English - Romance NP alignment, we will ex-periment with automatic translations generated forthe test data.
Moreover, we like to extend the anal-ysis to other set of languages whose structures arevery different from English and Romance.ReferencesT.
W. Finin.
1980.
The Semantic Interpretation of CompoundNominals.
Ph.D. thesis, University of Illinois at Urbana-Champaign.A.
Giorgi and G. Longobardi.
1991.
The syntax of nounphrases.
Cambridge University Press.R.
Girju, D. Moldovan, M. Tatu, and D. Antohe.
2005.
Onthe semantics of noun compounds.
Computer Speech andLanguage, 19(4):479?496.R.
Girju.
2007.
Experiments with an annotation scheme for aknowledge-rich noun phrase interpretation system.
The Lin-guistic Annotation Workshop at ACL, Prague.Su Nam Kim and T. Baldwin.
2006.
Interpreting semantic rela-tions in noun compounds via verb semantics.
COLING-ACL.K.
Kipper, H. Dong, and M. Palmer.
2000.
Class-based con-struction of a verb lexicon.
AAAI Conference, Austin.M.
Lapata and F. Keller.
2004.
The Web as a baseline: Eval-uating the performance of unsupervised Web-based modelsfor a range of NLP tasks.
HLT-NAACL.M.
Lauer.
1995.
Corpus statistics meet the noun compound:Some empirical results.
ACL, Cambridge, Mass.A.
Meyers, R. Reeves, C. Macleod, R. Szekeley V. Zielinska,and B.
Young.
2004.
The cross-breeding of dictionaries.LREC-2004, Lisbon, Portugal.R.
Mihalcea and E. Faruque.
2004.
Senselearner: Minimallysupervised word sense disambiguation for all words in opentext.
ACL/SIGLEX Senseval-3, Barcelona, Spain.D.
Moldovan and A. Badulescu.
2005.
A semantic scat-tering model for the automatic interpretation of genitives.HLT/EMNLP Conference, Vancouver, Canada.P.
Pantel and M. Pennacchiotti.
2006.
Espresso: Leveraginggeneric patterns for automatically harvesting semantic rela-tions.
COLING/ACL, Sydney, Australia.B.
Rosario and M. Hearst.
2001.
Classifying the semantic rela-tions in noun compounds.
EMNLP Conference.R.
Snow, D. Jurafsky, and A. Ng.
2006.
Semantic taxonomyinduction from heterogenous evidence.
COLING-ACL.P.
Turney.
2006.
Expressing implicit semantic relations withoutsupervision.
COLING/ACL, Sydney, Australia.575
