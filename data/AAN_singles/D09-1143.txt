Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1378?1387,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPConvolution Kernels on Constituent, Dependency and SequentialStructures for Relation ExtractionTruc-Vien T. Nguyen and Alessandro Moschitti and Giuseppe Riccardinguyenthi,moschitti,riccardi@disi.unitn.itDepartment of Information Engineering and Computer ScienceUniversity of Trento38050 Povo (TN), ItalyAbstractThis paper explores the use of innovativekernels based on syntactic and semanticstructures for a target relation extractiontask.
Syntax is derived from constituentand dependency parse trees whereas se-mantics concerns to entity types and lex-ical sequences.
We investigate the effec-tiveness of such representations in the au-tomated relation extraction from texts.
Weprocess the above data by means of Sup-port Vector Machines along with the syn-tactic tree, the partial tree and the wordsequence kernels.
Our study on the ACE2004 corpus illustrates that the combina-tion of the above kernels achieves high ef-fectiveness and significantly improves thecurrent state-of-the-art.1 IntroductionRelation Extraction (RE) is defined in ACE as thetask of finding relevant semantic relations betweenpairs of entities in texts.
Figure 1 shows partof a document from ACE 2004 corpus, a collec-tion of news articles.
In the text, the relation be-tween president and NBC?s entertainment divisiondescribes the relationship between the first entity(person) and the second (organization) where theperson holds a managerial position.Several approaches have been proposed for au-tomatically learning semantic relations from texts.Among others, there has been increased interest inthe application of kernel methods (Zelenko et al,2002; Culotta and Sorensen, 2004; Bunescu andMooney, 2005a; Bunescu and Mooney, 2005b;Zhang et al, 2005; Wang, 2008).
Their main prop-erty is the ability of exploiting a huge amount ofThis work has been partially funded by the LiveMemo-ries project (http://www.livememories.org/) and Expert Sys-tem (http://www.expertsystem.net/) research grant.Jeff Zucker, the longtime executive producer ofNBC?s ?Today?
program, will be named Fridayas the new president of NBC?s entertainmentdivision, replacing Garth Ancier, NBC execu-tives said.Figure 1: A document from ACE 2004 with allentity mentions in bold.features without an explicit feature representation.This can be done by computing a kernel functionbetween a pair of linguistic objects, where suchfunction is a kind of similarity measure satisfy-ing certain properties.
An example is the sequencekernel (Lodhi et al, 2002), where the objects arestrings of characters and the kernel function com-putes the number of common subsequences ofcharacters in the two strings.
Such substrings arethen weighted according to a decaying factor pe-nalizing longer ones.
In the same line, Tree Ker-nels count the number of subtree shared by two in-put trees.
An example is that of syntactic (or sub-set) tree kernel (SST) (Collins and Duffy, 2001),where trees encode grammatical derivations.Previous work on the use of kernels for REhas exploited some similarity measures over di-verse features (Zelenko et al, 2002; Culotta andSorensen, 2004; Zhang et al, 2005) or subse-quence kernels over dependency graphs (Bunescuand Mooney, 2005a; Wang, 2008).
More specif-ically, (Bunescu and Mooney, 2005a; Culottaand Sorensen, 2004) use kernels over depen-dency trees, which showed much lower accuracythan feature-based methods (Zhao and Grishman,2005).
One problem of the dependency kernelsabove is that they do not exploit the overall struc-tural aspects of dependency trees.
A more effec-tive solution is the application of convolution ker-nels to constituent parse trees (Zhang et al, 2006)but this is not satisfactory from a general per-1378spective since dependency structures offer someunique advantages, which should be exploited byan appropriate kernel.Therefore, studying convolution tree kernels fordependency trees is worthwhile also consideringthat, to the best of our knowledge, these modelshave not been previously used for relation extrac-tion1task.
Additionally, sequence kernels shouldbe included in such global study since some oftheir forms have not been applied to RE.In this paper, we study and evaluate diverse con-volution and sequence kernels for the RE problemby providing several kernel combinations on con-stituent and dependency trees and sequential struc-tures.
To fully exploit the potential of dependencytrees, in addition to the SST kernel, we appliedthe partial tree (PT) kernel proposed in (Moschitti,2006), which is a general convolution tree kerneladaptable for dependency structures.
We also in-vestigate various sequence kernels (e.g.
the wordsequence kernel (WSK) (Cancedda et al, 2003))by incorporating dependency structures into wordsequences.
These are also enriched by includinginformation from constituent parse trees.We conduct experiments on the standard ACE2004 newswire and broadcast news domain.
Theresults show that although some kernels are lesseffective than others, they exhibit properties thatare complementary to each other.
In particu-lar, we found that relation extraction can benefitfrom increasing the feature space by combiningkernels (with a simple summation) exploiting thetwo different parsing paradigms.
Our experimentson RE show that the current composite kernel,which is constituent-based is more effective thanthose based on dependency trees and individualsequence kernel but at the same time their com-binations, i.e.
dependency plus constituent trees,improve the state-of-the-art in RE.
More interest-ingly, also the combinations of various sequencekernels gain significant better performance thanthe current state-of-the-art (Zhang et al, 2005).Overall, these results are interesting for thecomputational linguistics research since they showthat the above two parsing paradigms provide dif-ferent and important information for a semantictask such as RE.
Regarding sequence-based ker-nels, the WSK gains better performance than pre-vious sequence and dependency models for RE.1The function defined on (Culotta and Sorensen, 2004),although on dependency trees, is not a convolution tree ker-nel.A review of previous work on RE is describedin Section 2.
Section 3 introduces support vec-tor machines and kernel methods whereas our spe-cific kernels for RE are described is Section 4.
Theexperiments and conclusions are presented in sec-tions 5 and 6, respectively.2 Related WorkTo identify semantic relations using machinelearning, three learning settings have mainly beenapplied, namely supervised methods (Miller etal., 2000; Zelenko et al, 2002; Culotta andSorensen, 2004; Kambhatla, 2004; Zhou et al,2005), semi supervised methods (Brin, 1998;Agichtein and Gravano, 2000), and unsupervisedmethod (Hasegawa et al, 2004).
In a supervisedlearning setting, representative related work canbe classified into generative models (Miller et al,2000), feature-based (Roth and tau Yih, 2002;Kambhatla, 2004; Zhao and Grishman, 2005;Zhou et al, 2005) or kernel-based methods (Ze-lenko et al, 2002; Culotta and Sorensen, 2004;Bunescu and Mooney, 2005a; Zhang et al, 2005;Wang, 2008; Zhang et al, 2006).The learning model employed in (Miller et al,2000) used statistical parsing techniques to learnsyntactic parse trees.
It demonstrated that a lexi-calized, probabilistic context-free parser with headrules can be used effectively for information ex-traction.
Meanwhile, feature-based approachesoften employ various kinds of linguistic, syntac-tic or contextual information and integrate intothe feature space.
(Roth and tau Yih, 2002) ap-plied a probabilistic approach to solve the prob-lems of named entity and relation extraction withthe incorporation of various features such as word,part-of-speech, and semantic information fromWordNet.
(Kambhatla, 2004) employed maximumentropy models with diverse features includingwords, entity and mention types and the numberof words (if any) separating the two entities.Recent work on Relation Extraction has mostlyemployed kernel-based approaches over syntac-tic parse trees.
Kernels on parse trees were pi-oneered by (Collins and Duffy, 2001).
Thiskernel function counts the number of commonsubtrees, weighted appropriately, as the measureof similarity between two parse trees.
(Culottaand Sorensen, 2004) extended this work to cal-culate kernels between augmented dependencytrees.
(Zelenko et al, 2002) proposed extracting1379relations by computing kernel functions betweenparse trees.
(Bunescu and Mooney, 2005a) pro-posed a shortest path dependency kernel by stipu-lating that the information to model a relationshipbetween two entities can be captured by the short-est path between them in the dependency graph.Although approaches in RE have been domi-nated by kernel-based methods, until now, mostof research in this line has used the kernel as somesimilarity measures over diverse features (Zelenkoet al, 2002; Culotta and Sorensen, 2004; Bunescuand Mooney, 2005a; Zhang et al, 2005; Wang,2008).
These are not convolution kernels and pro-duce a much lower number of substructures thanthe PT kernel.
A recent approach successfully em-ploys a convolution tree kernel (of type SST) overconstituent syntactic parse tree (Zhang et al, 2006;Zhou et al, 2007), but it does not capture gram-matical relations in dependency structure.
We be-lieve that an efficient and appropriate kernel canbe used to solve the RE problem, exploiting theadvantages of dependency structures, convolutiontree kernels and sequence kernels.3 Support Vector Machines and KernelMethodsIn this section we give a brief introduction to sup-port vector machines, kernel methods, diverse treeand sequence kernel spaces, which can be appliedto the RE task.3.1 Support Vector Machines (SVMs)Support Vector Machines refer to a supervised ma-chine learning technique based on the latest resultsof the statistical learning theory (Vapnik, 1998).Given a vector space and a set of training points,i.e.
positive and negative examples, SVMs find aseparating hyperplane H(~x) = ~?
?
~x + b = 0where ?
?
Rnand b ?
R are learned by applyingthe Structural Risk Minimization principle (Vap-nik, 1995).
SVMs is a binary classifier, but it canbe easily extended to multi-class classifier, e.g.
bymeans of the one-vs-all method (Rifkin and Pog-gio, 2002).One strong point of SVMs is the possibility toapply kernel methods (robert Mller et al, 2001)to implicitly map data in a new space where theexamples are more easily separable as describedin the next section.3.2 Kernel MethodsKernel methods (Schlkopf and Smola, 2001) arean attractive alternative to feature-based methodssince the applied learning algorithm only needsto compute a product between a pair of objects(by means of kernel functions), avoiding the ex-plicit feature representation.
A kernel functionis a scalar product in a possibly unknown featurespace.
More precisely, The object o is mapped in~x with a feature function ?
: O ?
<n, whereO isthe set of the objects.The kernel trick allows us to rewrite the deci-sion hyperplane as:H(~x) =(?i=1..lyi?i~xi)?
~x+ b =?i=1..lyi?i~xi?
~x+ b =?i=1..lyi?i?
(oi) ?
?
(o) + b,where yiis equal to 1 for positive and -1 for neg-ative examples, ?i?
< with ?i?
0, oi?i ?
{1, .., l} are the training instances and the productK(oi, o) = ??
(oi) ?
?(o)?
is the kernel functionassociated with the mapping ?.Kernel engineering can be carried out by com-bining basic kernels with additive or multiplica-tive operators or by designing specific data objects(vectors, sequences and tree structures) for the tar-get tasks.Regarding NLP applications, kernel methodshave attracted much interest due to their abilityof implicitly exploring huge amounts of structuralfeatures automatically extracted from the origi-nal object representation.
The kernels for struc-tured natural language data, such as parse treekernel (Collins and Duffy, 2001) and string ker-nel (Lodhi et al, 2002) are examples of the well-known convolution kernels used in many NLP ap-plications.Tree kernels represent trees in terms of theirsubstructures (called tree fragments).
Such frag-ments form a feature space which, in turn, ismapped into a vector space.
Tree kernels mea-sure the similarity between pair of trees by count-ing the number of fragments in common.
Thereare three important characterizations of fragmenttype (Moschitti, 2006): the SubTrees (ST), theSubSet Trees (SST) and the Partial Trees (PT).
Forsake of space, we do not report the mathematicaldescription of them, which is available in (Vish-wanathan and Smola, 2002), (Collins and Duffy,13802001) and (Moschitti, 2006), respectively.
In con-trast, we report some descriptions in terms of fea-ture space that may be useful to understand thenew engineered kernels.In principle, a SubTree (ST) is defined by tak-ing any node along with its descendants.
A Sub-Set Tree (SST) is a more general structure whichdoes not necessarily include all the descendants.
Itmust be generated by applying the same grammat-ical rule set, which generated the original tree.
APartial Tree (PT) is a more general form of sub-structures obtained by relaxing constraints overthe SST.4 Kernels for Relation ExtractionIn this section we describe the previous kernelsbased on constituent trees as well as new kernelsbased on diverse types of trees and sequences forrelation extraction.
As mentioned in the previ-ous section, we can engineer kernels by combin-ing tree and sequence kernels.
Thus we focus onthe problem to define structure embedding the de-sired syntactic relational information between twonamed entities (NEs).4.1 Constituent and Dependency StructuresSyntactic parsing (or syntactic analysis) aims atidentifying grammatical structures in a text.
Aparser thus captures the hidden hierarchy of theinput text and processes it into a form suitable forfurther processing.
There are two main paradigmsfor representing syntactic information: constituentand dependency parsing, which produces two dif-ferent tree structures.Constituent tree encodes structural propertiesof a sentence.
The parse tree contains constituents,such as noun phrases (NP) and verb phrases (VP),as well as terminals/part-of-speech tags, such asdeterminers (DT) or nouns (NN).
Figure 2.a showsthe constituent tree of the sentence: In Washing-ton, U.S. officials are working overtime.Dependency tree encodes grammatical rela-tions between words in a sentence with the wordsas nodes and dependency types as edges.
An edgefrom a word to another represents a grammaticalrelation between these two.
Every word in a de-pendency tree has exactly one parent except theroot.
Figure 2.b shows and example of the depen-dency tree of the previous sentence.Given two NEs, such as Washington and offi-cials, both the above trees can encode the syntacticdependencies between them.
However, since eachparse tree corresponds to a sentence, there may bemore than two NEs and many relations expressedin a sentence.
Thus, the use of the entire parsetree of the whole sentence holds two major draw-backs: first, it may be too computationally expen-sive for kernel calculation since the size of a com-plete parse tree may be very large (up to 300 nodesin the Penn Treebank (Marcus et al, 1993)); sec-ond, there is ambiguity on the target pairs of NEs,i.e.
different NEs associated with different rela-tions are described by the same parse tree.
There-fore, it is necessary to identify the portion of theparse tree that best represent the useful syntacticinformation.Let e1and e2be two entity mentions in the samesentence such that they are in a relationship R.For the constituent parse tree, we used the path-enclosed tree (PET), which was firstly proposedin (Moschitti, 2004) for Semantic Role Labelingand then adapted by (Zhang et al, 2005) for re-lation extraction.
It is the smallest common sub-tree including the two entities of a relation.
Thedashed frame in Figure 2.a surrounds PET associ-ated with the two mentions, officials and Washing-ton.
Moreover, to improve the representation, twoextra nodes T1-PER, denoting the type PERSON,and T2-LOC, denoting the type LOCATION, areadded to the parse tree, above the two target NEs,respectively.
In this example, the above PET is de-signed to capture the relation Located-in betweenthe entities ?officials?
and ?Washington?
from theACE corpus.
Note that, a third NE, U.S., is char-acterized by the node GPE (GeoPolitical Entity),where the absence of the prefix T1 or T2 beforethe NE type (i.e.
GPE), denotes that the NE doesnot take part in the target relation.In previous work, some dependency trees havebeen used (Bunescu and Mooney, 2005a; Wang,2008) but the employed kernel just exploited thesyntactic information concentrated in the path be-tween e1and e2.
In contrast, we defined and stud-ied three different dependency structures whosepotential can be fully exploited by our convolutionpartial tree kernel:- Dependency Words (DW) tree is similar toPET adapted for dependency tree constitutedby simple words.
We select the minimal sub-tree which includes e1and e2, and we insertan extra node as father of the NEs, labeledwith the NE category.
For example, given1381Figure 2: The constituent and dependency parse trees integrated with entity informationthe tree in Figure 2.b, we design the tree inFigure 2.c surrounded by the dashed frames,where T1-PER, T2-LOC and GPE are the ex-tra nodes inserted as fathers of Washington,soldier and U.S..- Grammatical Relation (GR) tree, i.e.
the DWtree in which words are replaced by theirgrammatical functions, e.g.
prep, pobj andnsubj.
For example, Figure 2.d, shows theGR tree for the previous relation: In is re-placed by prep , U.S. by nsubj and so on.- Grammatical Relation and Words (GRW)tree, words and grammatical functions areboth used in the tree, where the latter are in-serted as a father node of the former.
Forexample, Figure 2.e, shows such tree for theprevious relation.4.2 Sequential StructuresSome sequence kernels have been used on depen-dency structures (Bunescu and Mooney, 2005b;Wang, 2008).
These kernels just used lexicalwords with some syntactic information.
To fullyexploit syntactic and semantic information, we de-fined and studied six different sequences (in a stylesimilar to what proposed in (Moschitti, 2008)),which include features from constituent and de-pendency parse trees and NEs:1.
Sequence of terminals (lexical words) in thePET (SK1), e.g.
:T2-LOC Washington , U.S. T1-PER officials.2.
Sequence of part-of-speech (POS) tags in thePET (SK2), i.e.
the SK1in which words arereplaced by their POS tags, e.g.
:T2-LOC NN , NNP T1-PER NNS.3.
Sequence of grammatical relations in the1382PET (SK3), i.e.
the SK1in which words arereplaced by their grammatical functions, e.g.
:T2-LOC pobj , nn T1-PER nsubj.4.
Sequence of words in the DW (SK4), e.g.
:Washington T2-LOC In working T1-PER of-ficials GPE U.S..5.
Sequence of grammatical relations in the GR(SK5), i.e.
the SK4in which words are re-placed by their grammatical functions, e.g.
:pobj T2-LOC prep ROOT T1-PER nsubj GPEnn.6.
Sequence of POS tags in the DW (SK6), i.e.the SK4in which words are replaced by theirPOS tags, e.g.
:NN T2-LOC IN VBP T1-PER NNS GPENNP.It is worth noting that the potential informationcontained in such sequences can be fully exploitedby the word sequence kernel.4.3 Combining KernelsGiven that syntactic information from differentparse trees may have different impact on relationextraction (RE), the viable approach to study therole of dependency and constituent parsing is toexperiment with different syntactic models andmeasuring the impact in terms of RE accuracy.For this purpose we compared the composite ker-nel described in (Zhang et al, 2006) with the par-tial tree kernels applied to DW , GR, and GRWand sequence kernels based on six sequences de-scribed above.
The composite kernels includepolynomial kernel applied to entity-related featurevector.
The word sequence kernel (WSK) is al-ways applied to sequential structures.
The usedkernels are described in more detail below.4.3.1 Polynomial KernelThe basic kernel between two named entities ofthe ACE documents is defined as:KP(R1, R2) =?i=1,2KE(R1.Ei, R2.Ei),where R1and R2are two relation instances, Eiisthe ithentity of a relation instance.
KE(?, ?)
is akernel over entity features, i.e.
:KE(E1, E2) = (1 + ~x1?
~x2)2,where ~x1and ~x2are two feature vectors extractedfrom the two NEs.For the ACE 2004, the features used include:entity headword, entity type, entity subtype, men-tion type, and LDC2mention type.
The last fourattributes are taken from the ACE corpus 2004.
InACE, each mention has a head annotation and anextent annotation.4.3.2 Kernel Combinations1.
Polynomial kernel plus a tree kernel:CK1= ?
?KP+ (1?
?)
?Kx,where ?
is a coefficient to give more impactto KPand Kxis either the partial tree ker-nel applied to one the possible dependencystructures, DW, GR or GRW or the SST ker-nel applied to PET, described in the previoussection.2.
Polynomial kernel plus constituent plus de-pendency tree kernels:CK2= ?
?KP+ (1?
?)
?
(KSST+KPT)where KSSTis the SST kernel and KPTisthe partial tree kernel (applied to the relatedstructures as in point 1).3.
Constituent tree plus square of polynomialkernel and dependency tree kernel:CK3= ?
?KSST+ (1??)
?
(KP+KPT)24.
Dependency word tree plus grammatical re-lation tree kernels:CK4= KPT?DW+KPT?GRwhere KPT?DWand KPT?GRare the par-tial tree kernels applied to dependency struc-tures DW and GR.5.
Polynomial kernel plus dependency wordplus grammatical relation tree kernels:CK5= ??KP+(1??)?
(KPT?DW+KPT?GR)Some preliminary experiments on a validation setshowed that the second, the fourth and the fifthcombinations yield the best performance with ?
=0.4 while the first and the third combinations yieldthe best performance with ?
= 0.23.Regarding WSK, the following combinationsare applied:2Linguistic Data Consortium (LDC):http://www.ldc.upenn.edu/Projects/ACE/13831.
SK3+ SK42.
SK3+ SK63.
SSK =?i=1,..,6SKi4.
KSST+ SSK5.
CSK = ?
?KP+ (1??)
?
(KSST+SSK)Preliminary experiments showed that the last com-bination yields the best performance with ?
=0.23.We used a polynomial expansion to explore thebi-gram features of i) the first and the second en-tity participating in the relation, ii) grammaticalrelations which replace words in the dependencytree.
Since the kernel function set is closed un-der normalization, polynomial expansion and lin-ear combination (Schlkopf and Smola, 2001), allthe illustrated composite kernels are also properkernels.5 ExperimentsOur experiments aim at investigating the effec-tiveness of convolution kernels adapted to syntac-tic parse trees and various sequence kernels forthe RE task.
For this purpose, we use the sub-set and partial tree kernel over different kinds oftrees, namely constituent and dependency syntac-tic parse trees.
Diverse sequences are applied indi-vidually and in combination together.
We considerour task of relation extraction as a classificationproblem where categories are relation types.
Allpairs of entity mentions in the same sentence aretaken to generate potential relations, which will beprocessed as positive and negative examples.5.1 Experimental setupWe use the newswire and broadcast news domainin the English portion of the ACE 2004 corpusprovided by LDC.
This data portion includes 348documents and 4400 relation instances.
It definesseven entity types and seven relation types.
Everyrelation is assigned one of the seven types: Phys-ical, Person/Social, Employment/Membership/-Subsidiary, Agent-Artifact, PER/ORG Affiliation,GPE Affiliation, and Discourse.
For sake of space,we do not explain these relationships here, never-theless, they are explicitly described in the ACEdocument guidelines.
There are 4400 positive and38,696 negative examples when generating pairsof entity mentions as potential relations.Documents are parsed using StanfordParser (Klein and Manning, 2003) to pro-duce parse trees.
Potential relations are generatedby iterating all pairs of entity mentions in the samesentence.
Entity information, namely entity type,is integrated into parse trees.
To train and test ourbinary relation classifier, we used SVMs.
Here,relation detection is formulated as a multiclassclassification problem.
The one vs. rest strategyis employed by selecting the instance with largestmargin as the final answer.
For experimentation,we use 5-fold cross-validation with the TreeKernel Tools (Moschitti, 2004) (available athttp://disi.unitn.it/?moschitt/Tree-Kernel.htm).5.2 ResultsIn this section, we report the results of differentkernels setup over constituent (CT) and depen-dency (DP) parse trees and sequences taken fromthese parse trees.
The tree kernel (TK), compos-ite kernel (CK1, CK2, CK3, CK4, and CK5corresponding to five combination types in Sec-tion 4.3.2) were employed over these two syntactictrees.
For the tree kernel, we apply the SST kernelfor the path-enclosed tree (PET) of the constituenttree and the PT kernel for three kinds of depen-dency tree DW, GR, and GRW, described in theprevious section.
The two composite kernels CK2and CK3are applied over both two parse trees.The word sequence kernels are applied over sixsequences SK1, SK2, SK3, SK4, SK5, and SK6(described in Section 4.3).The results are shown in Table 1 and Table 2.In the first table, the first column indicates thestructure used in the combination shown in thesecond column, e.g.
PET associated with CK1means that the SST kernel is applied on PET (aportion of the constituent tree) and combined withthe CK1schema whereas PET and GR associatedwith CK5means that SST kernel is applied toPET and PT kernel is applied to GR in CK5.
Theremaining three columns report Precision, Recalland F1 measure.
The interpretation of the secondtable is more immediate since the only tree ker-nel involved is the SST kernel applied to PET andcombined by means of CK1.We note that: first, the dependency kernels,i.e.
the results on the rows from 3 to 6 are be-low the composite kernel CK1, i.e.
68.9.
Thisis the state-of-the-art in RE, designed by (Zhanget al, 2006), where our implementation provides1384Parse Tree Kernel P R FPET CK169.5 68.3 68.9DW CK153.2 59.7 56.3GR CK158.8 61.7 60.2GRW CK156.1 61.2 58.5DW and GR CK559.7 64.1 61.8PET and GRCK270.7 69.0 69.8CK370.8 70.2 70.5Table 1: Results on the ACE 2004 evaluation testset.
Six structures were experimented over theconstituent and dependency trees.Kernel P R FCK169.5 68.3 68.9SK172.0 52.8 61.0SK261.7 60.0 60.8SK362.6 60.7 61.6SK473.1 50.3 59.7SK559.0 60.7 59.8SK657.7 61.8 59.7SK3+ SK475.0 63.4 68.8SK3+ SK666.8 65.1 65.9SSK =?iSKi73.8 66.2 69.8CSK 75.6 66.6 70.8CK1+ SSK 76.6 67.0 71.5(Zhou et al, 2007)82.2 70.2 75.8CK1with HeuristicsTable 2: Performance comparison on the ACE2004 data with different kernel setups.a slightly smaller result than the original version(i.e.
an F1 of about 72 using a different syntacticparser).Second, CK1improves to 70.5, when the con-tribution of PT kernel applied to GR (dependencytree built using grammatical relations) is added.This suggests that dependency structures are effec-tively exploited by PT kernel and that such infor-mation is somewhat complementary to constituenttrees.Third, in the second table, the model CK1+SSK, which adds to CK1the contribution of di-verse sequence kernels, outperforms the state-of-the-art by 2.6%.
This suggests that the sequentialinformation encoded by several sequence kernelscan better represents the dependency information.Finally, we also report in the last row (in italic)the superior RE result by (Zhou et al, 2007).However, to achieve this outcome the authors usedthe composite kernel CK1with several heuristicsto define an effective portion of constituent trees.Such heuristics expand the tree and remove unnec-essary information allowing a higher improvementon RE.
They are tuned on the target RE task so al-though the result is impressive, we cannot use it tocompare with pure automatic learning approaches,such us our models.6 Conclusion and Future WorkIn this paper, we study the use of several typesof syntactic information: constituent and depen-dency syntactic parse trees.
A relation is repre-sented by taking the path-enclosed tree (PET) ofthe constituent tree or of the path linking two enti-ties of the dependency tree.
For the design of auto-matic relation classifiers, we have investigated theimpact of dependency structures to the RE task.Our novel composite kernels, which account forthe two syntactic structures, are experimented withthe appropriate convolution kernels and show sig-nificant improvement with respect to the state-of-the-art in RE.Regarding future work, there are many researchline that may be followed:i) Capturing more features by employing ex-ternal knowledge such as ontological, lexical re-source or WordNet-based features (Basili et al,2005a; Basili et al, 2005b; Bloehdorn et al, 2006;Bloehdorn and Moschitti, 2007) or shallow se-mantic trees, (Giuglea and Moschitti, 2004; Giu-glea and Moschitti, 2006; Moschitti and Bejan,2004; Moschitti et al, 2007; Moschitti, 2008;Moschitti et al, 2008).ii) Design a new tree-based structures, whichcombines the information of both constituent anddependency parses.
From dependency trees wecan extract more precise but also more sparserelationships (which may cause overfit).
Fromconstituent trees, we can extract subtrees consti-tuted by non-terminal symbols (grammar sym-bols), which provide a better generalization (witha risk of underfitting).iii) Design a new kernel which can integrate theadvantages of the constituent and dependency tree.The new tree kernel should inherit the benefits ofthe three available tree kernels: ST, SST or PT.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snow-ball: Extracting relations from large plain-text col-1385lections.
In Proceedings of the 5th ACM Interna-tional Conference on Digital Libraries.Roberto Basili, Marco Cammisa, and Alessandro Mos-chitti.
2005a.
Effective use of WordNet semanticsvia kernel-based learning.
In Proceedings of theNinth Conference on Computational Natural Lan-guage Learning (CoNLL-2005), pages 1?8, Ann Ar-bor, Michigan, June.
Association for ComputationalLinguistics.Roberto Basili, Marco Cammisa, and Alessandro Mos-chitti.
2005b.
A semantic kernel to classify textswith very few training examples.
In In Proceedingsof the Workshop on Learning in Web Search, at the.Stephan Bloehdorn and Alessandro Moschitti.
2007.Structure and semantics for expressive text ker-nels.
In CIKM ?07: Proceedings of the sixteenthACM conference on Conference on information andknowledge management, pages 861?864, New York,NY, USA.
ACM.Stephan Bloehdorn, Roberto Basili, Marco Cammisa,and Alessandro Moschitti.
2006.
Semantic kernelsfor text classification based on topological measuresof feature similarity.
In Proceedings of the 6th IEEEInternational Conference on Data Mining (ICDM06), Hong Kong, 18-22 December 2006, DEC.Sergey Brin.
1998.
Extracting patterns and relationsfrom world wide web.
In Proceeding of WebDBWorkshop at 6th International Conference on Ex-tending Database Technology, pages 172?183.Razvan C. Bunescu and Raymond J. Mooney.
2005a.A shortest path dependency kernel for relation ex-traction.
In Proceedings of EMNLP, pages 724?731.Razvan C. Bunescu and Raymond J. Mooney.
2005b.Subsequence kernels for relation extraction.
In Pro-ceedings of EMNLP.Nicola Cancedda, Eric Gaussier, Cyril Goutte, andJean Michel Renders.
2003.
Word sequence ker-nels.
Journal of Machine Learning Research, pages1059?1082.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In Proceedings of Neu-ral Information Processing Systems (NIPS?2001).Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedingsof the 42nd Annual Meeting on ACL, Barcelona,Spain.Ana-Maria Giuglea and Alessandro Moschitti.
2004.Knowledge discovery using framenet, verbnet andpropbank.
In A. Meyers, editor, Workshop on On-tology and Knowledge Discovering at ECML 2004,Pisa, Italy.Ana-Maria Giuglea and Alessandro Moschitti.
2006.Semantic Role Labeling via Framenet, Verbnet andPropbank.
In Proceedings of ACL 2006, Sydney,Australia.Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-man.
2004.
Discovering relations among named en-tities from large corpora.
In Proceedings of the 42ndAnnual Meeting on ACL, Barcelona, Spain.Nanda Kambhatla.
2004.
Combining lexical, syntacticand semantic features with maximum entropy mod-els for extracting relations.
In Proceedings of theACL 2004 on Interactive poster and demonstrationsessions, Barcelona, Spain.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Meeting of the ACL, pages 423?430.Huma Lodhi, Craig Saunders, John Shawe-Taylor,Nello Cristianini, , and Chris Watkins.
2002.
Textclassification using string kernels.
Journal of Ma-chine Learning Research, pages 419?444.Mitchell P. Marcus, Beatrice Santorini, , and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: the penn treebank.
Computa-tional Linguistics, 19(2):313?330.Scott Miller, Heidi Fox, Lance Ramshaw, , and RalphWeischedel.
2000.
A novel use of statistical parsingto extract information from text.
In Proceedings ofthe 1st conference on North American chapter of theACL, pages 226?233, Seattle, USA.Alessandro Moschitti and Cosmin Bejan.
2004.
A se-mantic kernel for predicate argument classification.In CoNLL-2004, Boston, MA, USA.Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
Exploit-ing syntactic and shallow semantic kernels forquestion/answer classification.
In Proceedings ofACL?07, Prague, Czech Republic.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2008.
Tree kernels for semantic role label-ing.
Computational Linguistics, 34(2):193?224.Alessandro Moschitti.
2004.
A study on convolutionkernels for shallow semantic parsing.
In Proceed-ings of the 42nd Meeting of the ACL, Barcelona,Spain.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In Proceedings of the 17th European Conference onMachine Learning, Berlin, Germany.Alessandro Moschitti.
2008.
Kernel methods, syntaxand semantics for relational text categorization.
InCIKM ?08: Proceeding of the 17th ACM conferenceon Information and knowledge management, pages253?262, New York, NY, USA.
ACM.Ryan Michael Rifkin and Tomaso Poggio.
2002.
Ev-erything old is new again: a fresh look at historicalapproaches in machine learning.
PhD thesis, Mas-sachusetts Institute of Technology.1386Klaus robert Mller, Sebastian Mika, Gunnar Rtsch,Koji Tsuda, , and Bernhard Schlkopf.
2001.
Anintroduction to kernel-based learning algorithms.IEEE Transactions on Neural Networks, 12(2):181?201.Dan Roth and Wen tau Yih.
2002.
Probabilistic rea-soning for entity and relation recognition.
In Pro-ceedings of the COLING-2002, Taipei, Taiwan.Bernhard Schlkopf and Alexander J. Smola.
2001.Learning with Kernels: Support Vector Machines,Regularization, Optimization, and Beyond.
MITPress, Cambridge, MA, USA.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer?Verlag, New York.Vladimir N. Vapnik.
1998.
Statistical Learning The-ory.
John Wiley and Sons, New York.S.V.N.
Vishwanathan and Alexander J. Smola.
2002.Fast kernels on strings and trees.
In Proceedings ofNeural Information Processing Systems.Mengqiu Wang.
2008.
A re-examination of depen-dency path kernels for relation extraction.
In Pro-ceedings of the 3rd International Joint Conferenceon Natural Language Processing-IJCNLP.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2002.
Kernel methods for relationextraction.
In Proceedings of EMNLP-ACL, pages181?201.Min Zhang, Jian Su, Danmei Wang, Guodong Zhou,and Chew Lim Tan.
2005.
Discovering relations be-tween named entities from a large raw corpus usingtree similarity-based clustering.
In Proceedings ofIJCNLP?2005, Lecture Notes in Computer Science(LNCS 3651), pages 378?389, Jeju Island, SouthKorea.Min Zhang, Jie Zhang, Jian Su, , and Guodong Zhou.2006.
A composite kernel to extract relations be-tween entities with both flat and structured features.In Proceedings of COLING-ACL 2006, pages 825?832.Shubin Zhao and Ralph Grishman.
2005.
Extractingrelations with integrated information using kernelmethods.
In Proceedings of the 43rd Meeting of theACL, pages 419?426, Ann Arbor, Michigan, USA.GuoDong Zhou, Jian Su, Jie Zhang, , and Min Zhang.2005.
Exploring various knowledge in relation ex-traction.
In Proceedings of the 43rd Meeting of theACL, pages 427?434, Ann Arbor, USA, June.GuoDong Zhou, Min Zhang, DongHong Ji, andQiaoMing Zhu.
2007.
Tree kernel-based relationextraction with context-sensitive structured parsetree information.
In Proceedings of EMNLP-CoNLL2007, pages 728?736.1387
