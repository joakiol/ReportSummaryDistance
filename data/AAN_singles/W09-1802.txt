Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 10?18,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA Scalable Global Model for SummarizationDan Gillick1,2, Benoit Favre21 Computer Science Division, University of California Berkeley, USA2 International Computer Science Institute, Berkeley, USA{dgillick,favre}@icsi.berkeley.eduAbstractWe present an Integer Linear Program forexact inference under a maximum coveragemodel for automatic summarization.
We com-pare our model, which operates at the sub-sentence or ?concept?-level, to a sentence-level model, previously solved with an ILP.Our model scales more efficiently to largerproblems because it does not require aquadratic number of variables to address re-dundancy in pairs of selected sentences.
Wealso show how to include sentence compres-sion in the ILP formulation, which has thedesirable property of performing compressionand sentence selection simultaneously.
Theresulting system performs at least as well asthe best systems participating in the recentText Analysis Conference, as judged by a va-riety of automatic and manual content-basedmetrics.1 IntroductionAutomatic summarization systems are typically ex-tractive or abstractive.
Since abstraction is quitehard, the most successful systems tested at the TextAnalysis Conference (TAC) and Document Under-standing Conference (DUC)1, for example, are ex-tractive.
In particular, sentence selection representsa reasonable trade-off between linguistic quality,guaranteed by longer textual units, and summarycontent, often improved with shorter units.Whereas the majority of approaches employ agreedy search to find a set of sentences that is1TAC is a continuation of DUC, which ran from 2001-2007.both relevant and non-redundant (Goldstein et al,2000; Nenkova and Vanderwende, 2005), some re-cent work focuses on improved search (McDonald,2007; Yih et al, 2007).
Among them, McDonald isthe first to consider a non-approximated maximiza-tion of an objective function through Integer LinearProgramming (ILP), which improves on a greedysearch by 4-12%.
His formulation assumes that thequality of a summary is proportional to the sum ofthe relevance scores of the selected sentences, penal-ized by the sum of the redundancy scores of all pairsof selected sentences.
Under a maximum summarylength constraint, this problem can be expressed asa quadratic knapsack (Gallo et al, 1980) and manymethods are available to solve it (Pisinger et al,2005).
However, McDonald reports that the methodis not scalable above 100 input sentences and dis-cusses more practical approximations.
Still, an ILPformulation is appealing because it gives exact so-lutions and lends itself well to extensions throughadditional constraints.Methods like McDonald?s, including the well-known Maximal Marginal Relevance (MMR) algo-rithm (Goldstein et al, 2000), are subject to an-other problem: Summary-level redundancy is notalways well modeled by pairwise sentence-level re-dundancy.
Figure 1 shows an example where thecombination of sentences (1) and (2) overlaps com-pletely with sentence (3), a fact not captured by pair-wise redundancy measures.
Redundancy, like con-tent selection, is a global problem.Here, we discuss a model for sentence selectionwith a globally optimal solution that also addressesredundancy globally.
We choose to represent infor-10(1) The cat is in the kitchen.
(2) The cat drinks the milk.
(3) The cat drinks the milk in the kitchen.Figure 1: Example of sentences redundant as a group.Their redundancy is only partially captured by sentence-level pairwise measurement.mation at a finer granularity than sentences, withconcepts, and assume that the value of a summary isthe sum of the values of the unique concepts it con-tains.
While the concepts we use in experiments areword n-grams, we use the generic term to emphasizethat this is just one possible definition.
Only credit-ing each concept once serves as an implicit globalconstraint on redundancy.
We show how the result-ing optimization problem can be mapped to an ILPthat can be solved efficiently with standard software.We begin by comparing our model to McDonald?s(section 2) and detail the differences between the re-sulting ILP formulations (section 3), showing thatours can give competitive results (section 4) and of-fer better scalability2 (section 5).
Next we demon-strate how our ILP formulation can be extended toinclude efficient parse-tree-based sentence compres-sion (section 6).
We review related work (section 7)and conclude with a discussion of potential improve-ments to the model (section 8).2 ModelsThe model proposed by McDonald (2007) considersinformation and redundancy at the sentence level.The score of a summary is defined as the sum ofthe relevance scores of the sentences it contains mi-nus the sum of the redundancy scores of each pair ofthese sentences.
If si is an indicator for the presenceof sentence i in the summary, Reli is its relevance,and Redij is its redundancy with sentence j, then asummary is scored according to:?iRelisi ?
?ijRedijsisjGenerating a summary under this model involvesmaximizing this objective function, subject to a2Strictly speaking, exact inference for the models discussedin this paper is NP-hard.
Thus we use the term ?scalable?
in apurely practical sense.length constraint.
A variety of choices for Reli andRedij are possible, from simple word overlap met-rics to the output of feature-based classifiers trainedto perform information retrieval and textual entail-ment.As an alternative, we consider information and re-dundancy at a sub-sentence, ?concept?
level, model-ing the value of a summary as a function of the con-cepts it covers.
While McDonald uses an explicitredundancy term, we model redundancy implicitly:a summary only benefits from including each con-cept once.
With ci an indicator for the presence ofconcept i in the summary, and its weight wi, the ob-jective function is:?iwiciWe generate a summary by choosing a set of sen-tences that maximizes this objective function, sub-ject to the usual length constraint.In summing over concept weights, we assume thatthe value of including a concept is not effected by thepresence of any other concept in the summary.
Thatis, concepts are assumed to be independent.
Choos-ing a suitable definition for concepts, and a map-ping from the input documents to concept weights,is both important and difficult.
Concepts could bewords, named entities, syntactic subtrees or seman-tic relations, for example.
While deeper semanticsmake more appealing concepts, their extraction andweighting are much more error-prone.
Any error inconcept extraction can result in a biased objectivefunction, leading to poor sentence selection.3 Inference by ILPEach model presented above can be formalized as anInteger Linear Program, with a solution represent-ing an optimal selection of sentences under the ob-jective function, subject to a length constraint.
Mc-Donald observes that the redundancy term makes fora quadratic objective function, which he coerces toa linear function by introducing additional variablessij that represent the presence of both sentence i andsentence j in the summary.
Additional constraintsensure the consistency between the sentence vari-ables (si, sj) and the quadratic term (sij).
With lithe length of sentence i and L the length limit for11the whole summary, the resulting ILP is:Maximize:?iRelisi ?
?ijRedijsijSubject to:?jljsj ?
Lsij ?
si sij ?
sj ?i, jsi + sj ?
sij ?
1 ?i, jsi ?
{0, 1} ?isij ?
{0, 1} ?i, jTo express our concept-based model as an ILP, wemaintain our notation from section 2, with ci an in-dicator for the presence of concept i in the summaryand sj an indicator for the presence of sentence j inthe summary.
We add Occij to indicate the occur-rence of concept i in sentence j, resulting in a newILP:Maximize:?iwiciSubject to:?jljsj ?
LsjOccij ?
ci, ?i, j (1)?jsjOccij ?
ci ?i (2)ci ?
{0, 1} ?isj ?
{0, 1} ?jNote that Occ, like Rel and Red, is a constant pa-rameter.
The constraints formalized in equations (1)and (2) ensure the logical consistency of the solu-tion: selecting a sentence necessitates selecting allthe concepts it contains and selecting a concept isonly possible if it is present in at least one selectedsentence.
Constraint (1) also prevents the inclusionof concept-less sentences.4 PerformanceHere we compare both models on a common sum-marization task.
The data is part of the Text Analy-sis Conference (TAC) multi-document summariza-tion evaluation and involves generating 100-wordsummaries from 10 newswire documents, each ona given topic.
While the 2008 edition of TAC alsoincludes an update task?additional summaries as-suming some prior knowledge?we focus only onthe standard task.
This includes 48 topics, averag-ing 235 input sentences (ranging from 47 to 652).Since the mean sentence length is around 25 words,a typical summary consists of 4 sentences.In order to facilitate comparison, we generatesummaries from both models using a commonpipeline:1.
Clean input documents.
A simple set of rulesremoves headers and formatting markup.2.
Split text into sentences.
We use the unsuper-vised Punkt system (Kiss and Strunk, 2006).3.
Prune sentences shorter than 5 words.4.
Compute parameters needed by the models.5.
Map to ILP format and solve.
We use an opensource solver3.6.
Order sentences picked by the ILP for inclusionin the summary.The specifics of step 4 are described in detail in(McDonald, 2007) and (Gillick et al, 2008).
Mc-Donald?s sentence relevance combines word-levelcosine similarity with the source document and theinverse of its position (early sentences tend to bemore important).
Redundancy between a pair of sen-tences is their cosine similarity.
For sentence i indocument D,Reli = cosine(i,D) + 1/pos(i,D)Redij = cosine(i, j)In our concept-based model, we use word bi-grams, weighted by the number of input documentsin which they appear.
While word bigrams stretchthe notion of a concept a bit thin, they are eas-ily extracted and matched (we use stemming to al-low slightly more robust matching).
Table 1 pro-vides some justification for document frequency as aweighting function.
Note that bigrams gave consis-tently better performance than unigrams or trigramsfor a variety of ROUGE measures.
Normalizingby document frequency measured over a generic set(TFIDF weighting) degraded ROUGE performance.3gnu.org/software/glpk12Bigrams consisting of two stopwords are pruned, asare those appearing in fewer than three documents.We largely ignore the sentence ordering problem,sorting the resulting sentences first by source docu-ment date, and then by position, so that the order oftwo originally adjacent sentences is preserved, forexample.Doc.
Freq.
(D) 1 2 3 4 5 6In Gold Set 156 48 25 15 10 7Not in Gold Set 5270 448 114 42 21 11Relevant (P ) 0.03 0.10 0.18 0.26 0.33 0.39Table 1: There is a strong relationship between the docu-ment frequency of input bigrams and the fraction of thosebigrams that appear in the human generated ?gold?
set:Let di be document frequency i and pi be the percent ofinput bigrams with di that are actually in the gold set.Then the correlation ?
(D,P ) = 0.95 for DUC 2007 and0.97 for DUC 2006.
Data here averaged over all prob-lems in DUC 2007.The summaries produced by the two systemshave been evaluated automatically with ROUGE andmanually with the Pyramid metric.
In particular,ROUGE-2 is the recall in bigrams with a set ofhuman-written abstractive summaries (Lin, 2004).The Pyramid score arises from a manual alignmentof basic facts from the reference summaries, calledSummary Content Units (SCUs), in a hypothesissummary (Nenkova and Passonneau, 2004).
Weused the SCUs provided by the TAC evaluation.Table 2 compares these results, alongside a base-line that uses the first 100 words of the most re-cent document.
All the scores are significantlydifferent, showing that according to both humanand automatic content evaluation, the concept-based model outperforms McDonald?s sentence-based model, which in turn outperforms the base-line.
Of course, the relevance and redundancy func-tions used for McDonald?s formulation in this exper-iment are rather primitive, and results would likelyimprove with better relevance features as used inmany TAC systems.
Nonetheless, our system basedon word bigram concepts, similarly primitive, per-formed at least as well as any in the TAC evaluation,according to two-tailed t-tests comparing ROUGE,Pyramid, and manually evaluated ?content respon-siveness?
(Dang and Owczarzak, 2008) of our sys-tem and the highest scoring system in each category.System ROUGE-2 PyramidBaseline 0.058 0.186McDonald 0.072 0.295Concepts 0.110 0.345Table 2: Scores for both systems and a baseline on TAC2008 data (Set A) for ROUGE-2 and Pyramid evalua-tions.5 ScalabilityMcDonald?s sentence-level formulation correspondsto a quadratic knapsack, and he shows his particu-lar variant is NP-hard by reduction to 3-D matching.The concept-level formulation is similar in spirit tothe classical maximum coverage problem: Given aset of items X , a set of subsets S of X , and an in-teger k, the goal is to pick at most k subsets fromS that maximizes the size of their union.
Maximumcoverage is known to be NP-hard by reduction to theset cover problem (Hochbaum, 1996).Perhaps the simplest way to show that our formu-lation is NP-hard is by reduction to the knapsackproblem (Karp, 1972).
Consider the special casewhere sentences do not share any overlapping con-cepts.
Then, the value of each sentence to the sum-mary is independent of every other sentence.
This isa knapsack problem: trying to maximize the valuein a container of limited size.
Given a solver for ourproblem, we could solve all knapsack problem in-stances, so our problem must also be NP-hard.With n input sentences and m concepts, bothformulations generate a quadratic number of con-straints.
However, McDonald?s has O(n2) variableswhile ours has O(n + m).
In practice, scalabilityis largely determined by the sparsity of the redun-dancy matrix Red and the sentence-concept matrixOcc.
Efficient solutions thus depend heavily on thechoice of redundancy measure in McDonald?s for-mulation and the choice of concepts in ours.
Prun-ing to reduce complexity involves removing low-relevance sentences or ignoring low redundancy val-ues in the former, and corresponds to removing low-weight concepts in the latter.
Note that pruning con-cepts may be more desirable: Pruned sentences areirretrievable, but pruned concepts may well appearin the selected sentences through co-occurrence.Figure 2 compares ILP run-times for the two13formulations, using a set of 25 topics from DUC2007, each of which have at least 500 input sen-tences.
These are very similar to the TAC 2008topics, but more input documents are provided foreach topic, which allowed us to extend the analysisto larger problems.
While the ILP solver finds opti-mal solutions efficiently for our concept-based for-mulation, run-time for McDonald?s approach growsvery rapidly.
The plot includes timing results for250-word summaries as well, showing that our ap-proach is fast even for much more complex prob-lems: A rough estimate for the number of possiblesummaries has(5004) = 2.6?109 for 100-word sum-maries and(50010) = 2.5 ?
1020 for 250 words sum-maries.While exact solutions are theoretically appealing,they are only useful in practice if fast approxima-tions are inferior.
A greedy approximation of ourobjective function gives 10% lower ROUGE scoresthan the exact solution, a gap that separates the high-est scoring systems from the middle of the pack inthe TAC evaluation.
The greedy solution (linear inthe number of sentences, assuming a constant sum-mary length) marks an upper bound on speed anda lower bound on performance; The ILP solutionmarks an upper bound on performance but is subjectto the perils of exponential scaling.
While we havenot experimented with much larger documents, ap-proximate methods will likely be valuable in bridg-ing the performance gap for complex problems.
Pre-liminary experiments with local search methods arepromising in this regard.6 ExtensionsHere we describe how our ILP formulation canbe extended with additional constraints to incor-porate sentence compression.
In particular, weare interested in creating compressed alternativesfor the original sentence by manipulating its parsetree (Knight and Marcu, 2000).
This idea has beenapplied with some success to summarization (Turnerand Charniak, 2005; Hovy et al, 2005; Nenkova,2008) with the goal of removing irrelevant or redun-dant details, thus freeing space for more relevant in-formation.
One way to achieve this end is to gen-erate compressed candidates for each sentence, cre-ating an expanded pool of input sentences, and em-50 100 150 200 250 300 350 400 450 500012345678Number of SentencesAverageTime per Problem(seconds)100 word summaries250 word summaries100 word summaries (McDonald)Figure 2: A comparison of ILP run-times (on an AMD1.8Ghz desktop machine) of McDonald?s sentence-basedformulation and our concept-based formulation with anincreasing number of input sentences.ploy some redundancy removal on the final selec-tion (Madnani et al, 2007).We adapt this approach to fit the ILP formulationsso that the optimization procedure decides whichcompressed alternatives to pick.
Formally, eachcompression candidate belongs to a group gk corre-sponding to its original sentence.
We can then crafta constraint to ensure that at most one sentence canbe selected from group gk, which also includes theoriginal:?i?gksi ?
1,?gkAssuming that all the compressed candidates arethemselves well-formed, meaningful sentences, wewould expect this approach to generate higher qual-ity summaries.
In general, however, compressionalgorithms can generate an exponential number ofcandidates.
Within McDonald?s framework, thiscan increase the number of variables and constraintstremendously.
Thus, we seek a compact representa-tion for compression in our concept framework.Specifically, we assume that compression in-volves some combination of three basic operationson sentences: extraction, removal, and substitution.In extraction, a sub-sentence (perhaps the content ofa quotation) may be used independently, and the restof the sentence is dropped.
In removal, a substring14is dropped (a temporal clause, for example) that pre-serves the grammaticality of the sentence.
In sub-stitution, one substring is replaced by another (USreplaces United States, for example).Arbitrary combinations of these operations aretoo general to be represented efficiently in an ILP.In particular, we need to compute the length of asentence and the concepts it covers for all compres-sion candidates.
Thus, we insist that the operationscan only affect non-overlapping spans of text, andend up with a tree representation of each sentence:Nodes correspond to compression operations andleaves map to the words.
Each node holds the lengthit contributes to the sentence recursively, as the sumof the lengths of its children.
Similarly, the conceptscovered by a node are the union of the concepts cov-ered by its children.
When a node is activated in theILP, we consider that the text attached to it is presentin the summary and update the length constraint andconcept selection accordingly.
Figure 3 gives an ex-ample of this tree representation for a sentence fromthe TAC data, showing the derivations of some com-pressed candidates.For a given sentence j, let Nj be the set of nodesin its compression tree, Ej ?
Nj be the set ofnodes that can be extracted (used as independentsentences), Rj ?
Nj be the set of nodes that canbe removed, and Sj ?
Nj be the set of substitu-tion group nodes.
Let x and y be nodes from Nj ; wecreate binary variables nx and ny to represent the in-clusion of x or y in the summary.
Let x ?
y denotethe fact that x ?
Nj is a direct parent of y ?
Nj .The constraints corresponding to the compressiontree are:?x?Ejnx ?
1 ?j (3)?x?yny = nx ?x ?
Sj ?j (4)nx ?
ny ?
(y ?
x ?
x /?
{Rj ?
Sj}) ?j (5)nx ?
ny ?
(y ?
x ?
x /?
{Ej ?
Sj}) ?j (6)Eq.
(3) enforces that only one sub-sentence is ex-tracted from the original sentence; eq.
(4) enforcesthat one child of a substitution group is selected ifand only if the substitution node is selected; eq.
(5)ensures that a child node is selected when its parentis selected unless the child is removable (or a substi-tution group); eq.
(6) ensures that if a child node isselected, its parent is also selected unless the child isan extraction node (that can be used as a root).Each node is associated with the words and theconcepts it contains directly (which are not con-tained by a child node) in order to compute the newlength constraints and activate concepts in the ob-jective function.
We set Occix to represent the oc-currence of concept i in node x as a direct child.Let lx be the length contributed to node x as directchildren.
The resulting ILP for performing sentencecompression jointly with sentence selection is:Maximize:?iwiciSubject to:?jlxnx ?
LnxOccix ?
ci, ?i, x?xnxOccix ?
ci ?iidem constraints (3) to (6)ci ?
{0, 1} ?inx ?
{0, 1} ?xWhile this framework can be used to imple-ment a wide range of compression techniques, wechoose to derive the compression tree from thesentence?s parse tree, extracted with the Berkeleyparser (Petrov and Klein, 2007), and use a set ofrules to label parse tree nodes with compression op-erations.
For example, declarative clauses contain-ing a subject and a verb are labeled with the extract(E) operation; adverbial clauses and non-mandatoryprepositional clauses are labeled with the remove(R) operation; Acronyms can be replaced by theirfull form by using substitution (S) operations and aprimitive form of co-reference resolution is used toallow the substitution of noun phrases by their refer-ent.System R-2 Pyr.
LQNo comp.
0.110 0.345 2.479Comp.
0.111 0.323 2.021Table 3: Scores of the system with and without sentencecompression included in the ILP (TAC?08 Set A data).When implemented in the system presented insection 4, this approach gives a slight improvement15countries are    planning to hold the euroas part of their    reservesthe magazine quoted    chief Wilm Disenbergas sayingalready (ECB | European Central Bank)foreign currency(1):E(2):E(6):R (7):R(8):R(4):R(3):SA number of(5):RNode Len.
Concepts(1):E 6 {the magazine, magazine quoted, chief Wilm, Wilm Disenberg}(2):E 7 {countries are, planning to, to hold, hold the, the euro}(3):S 0 {}(3a) 1 {ECB}(3b) 3 {European Central, Central Bank}(4):R 2 {as saying}(5):R 3 {a number, number of}(6):R 1 {}(7):R 5 {as part, part of, reserves}(8):R 2 {foreign currency}?
Original: A number of Countriesare already planning to hold theeuro as part of their foreign cur-rency reserves, the magazine quotedEuropean Central Bank chief WimDuisenberg as saying.?
[1,2,5,3a]: A number of countries areplanning to hold the euro, the maga-zine quoted ECB chief Wim Duisen-berg.?
[2,5,6,7,8]: A number of countriesare already planning to hold the euroas part of their foreign currency re-serves.?
[2,7,8]: Countries are planning tohold the euro as part of their foreigncurrency reserves.?
[2]: Countries are planning to holdthe euro.Figure 3: A compression tree for an example sentence.
E-nodes (diamonds) can be extracted and used as an indepen-dent sentences, R-nodes (circles) can be removed, and S-nodes (squares) contain substitution alternatives.
The tableshows the word bigram concepts covered by each node and the length it contributes to the summary.
Examples ofresulting compression candidates are given on the right side, with the list of nodes activated in their derivations.in ROUGE-2 score (see Table 3), but a reduction inPyramid score.
An analysis of the resulting sum-maries showed that the rules used for implementingsentence compression fail to ensure that all com-pression candidates are valid sentences, and about60% of the summaries contain ungrammatical sen-tences.
This is confirmed by the linguistic qual-ity4 score drop for this system.
The poor qualityof the compressed sentences explains the reductionin Pyramid scores: Human judges tend to not givecredit to ungrammatical sentences because they ob-scure the SCUs.We have shown in this section how sentence com-pression can be implemented in a more scalable wayunder the concept-based model, but it remains to beshown that such a technique can improve summaryquality.7 Related workIn addition to proposing an ILP for the sentence-level model, McDonald (2007) discusses a kind ofsummary-level model: The score of a summary is4As measured according to the TAC?08 guidelines.determined by its cosine similarity to the collectionof input documents.
Though this idea is only imple-mented with approximate methods, it is similar inspirit to our concept-based model since it relies onweights for individual summary words rather thansentences.Using a maximum coverage model for summa-rization is not new.
Filatova (2004) formalizes theidea, discussing its similarity to the classical NP-hard problem, but in the end uses a greedy approxi-mation to generate summaries.
More recently, Yih etal.
(2007) employ a similar model and uses a stackdecoder to improve on a greedy search.
Globallyoptimal summaries are also discussed by Liu (2006)and Jaoua Kallel (2004) who apply genetic algo-rithms for finding selections of sentences that maxi-mize summary-level metrics.
Hassel (2006) uses hillclimbing to build summaries that maximize a globalinformation criterion based on random indexing.The general idea of concept-level scoring forsummarization is employed in the SumBasic sys-tem (Nenkova and Vanderwende, 2005), whichchooses sentences greedily according to the sumof their word values (values are derived from fre-16quency).
Conroy (2006) describes a bag-of-wordsmodel, with the goal of approximating the distribu-tion of words from the input documents in the sum-mary.
Others, like (Yih et al, 2007) train a model tolearn the value of each word from a set of featuresincluding frequency and position.
Filatova?s modelis most theoretically similar to ours, though the con-cepts she chooses are ?events?.8 Conclusion and Future WorkWe have synthesized a number of ideas fromthe field of automatic summarization, includingconcept-level weighting, a maximum coveragemodel to minimize redundancy globally, and sen-tence compression derived from parse trees.
Whilean ILP formulation for summarization is not novel,ours provides reasonably scalable, efficient solutionsfor practical problems, including those in recentTAC and DUC evaluations.
We have also shownhow it can be extended to perform sentence com-pression and sentence selection jointly.In ROUGE and Pyramid evaluation, our systemsignificantly outperformed McDonald?s ILP sys-tem.
However, we would note that better design ofsentence-level scoring would likely yield better re-sults as suggested by the success of greedy sentence-based methods at the DUC and TAC conferences(see for instance (Toutanova et al, 2007)).
Still, theperformance of our system, on par with the currentstate-of-the-art, is encouraging.There are three principal directions for futurework.
First, word bigram concepts are convenient,but semantically unappealing.
We plan to exploreconcepts derived from parse trees, where weightsmay be a function of frequency as well as hierar-chical relationships.Second, our current approach relies entirely onword frequency, a reasonable proxy for relevance,but likely inferior to learning weights from train-ing data.
A number of systems have shown im-provements by learning word values, though prelim-inary attempts to improve on our frequency heuristicby learning bigram values have not produced sig-nificant gains.
Better features may be necessary.However, since the ILP gives optimal solutions soquickly, we are more interested in discriminativetraining where we learn weights for features thatpush the resulting summaries in the right direction,as opposed to the individual concept values.Third, our rule-based sentence compression ismore of a proof of concept, showing that joint com-pression and optimal selection is feasible.
Betterstatistical methods have been developed for produc-ing high quality compression candidates (McDon-ald, 2006), that maintain linguistic quality, some re-cent work even uses ILPs for exact inference (Clarkeand Lapata, 2008).
The addition of compressed sen-tences tends to yield less coherent summaries, mak-ing sentence ordering more important.
We wouldlike to add constraints on sentence ordering to theILP formulation to address this issue.AcknowledgmentsThis work is supported by the Defense AdvancedResearch Projects Agency (DARPA) GALE project,under Contract No.
HR0011-06-C-0023.
Any opin-ions, findings, conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of DARPA.ReferencesJames Clarke and Mirella Lapata.
2008.
Global infer-ence for sentence compression: An integer linear pro-gramming approach.
Journal of Artificial IntelligenceResearch, 31:273?381.John M. Conroy, Judith D. Schlesinger, and Dianne P.O?Leary.
2006.
Topic-focused multi-document sum-marization using an approximate oracle score.
In Pro-ceedings of COLING/ACL.Hoa Trang Dang and Karolina Owczarzak.
2008.Overview of the TAC 2008 Update SummarizationTask.
In Proceedings of Text Analysis Conference.E.
Filatova and V. Hatzivassiloglou.
2004.
Event-basedextractive summarization.
In Proceedings of ACLWorkshop on Summarization, volume 111.G.
Gallo, PL Hammer, and B. Simeone.
1980.
Quadraticknapsack problems.
Mathematical ProgrammingStudy, 12:132?149.D.
Gillick, B. Favre, and D. Hakkani-Tur.
2008.
TheICSI Summarization System at TAC 2008.
In Pro-ceedings of the Text Understanding Conference.Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and MarkKantrowitz.
2000.
Multi-document summarization bysentence extraction.
Proceedings of the ANLP/NAACLWorkshop on Automatic Summarization, pages 40?48.17Martin Hassel and Jonas Sjo?bergh.
2006.
Towardsholistic summarization: Selecting summaries, not sen-tences.
In Proceedings of Language Resources andEvaluation.D.S.
Hochbaum.
1996.
Approximating covering andpacking problems: set cover, vertex cover, indepen-dent set, and related problems.
PWS Publishing Co.Boston, MA, USA, pages 94?143.E.
Hovy, C.Y.
Lin, and L. Zhou.
2005.
A BE-basedmulti-document summarizer with sentence compres-sion.
In Proceedings of Multilingual SummarizationEvaluation.Fatma Jaoua Kallel, Maher Jaoua, Lamia Bel-guith Hadrich, and Abdelmajid Ben Hamadou.
2004.Summarization at LARIS Laboratory.
In Proceedingsof the Document Understanding Conference.Richard Manning Karp.
1972.
Reducibility among com-binatorial problems.
Complexity of Computer Compu-tations, 43:85?103.Tibor Kiss and Jan Strunk.
2006.
Unsupervised multi-lingual sentence boundary detection.
ComputationalLinguistics, 32.K.
Knight and D. Marcu.
2000.
Statistics-BasedSummarization-Step One: Sentence Compression.
InProceedings of the National Conference on ArtificialIntelligence, pages 703?710.
Menlo Park, CA; Cam-bridge, MA; London; AAAI Press; MIT Press; 1999.Chin-Yew Lin.
2004.
ROUGE: A Package for Auto-matic Evaluation of Summaries.
In Proceedings of theWorkshop on Text Summarization Branches Out (WAS2004), pages 25?26.D.
Liu, Y. Wang, C. Liu, and Z. Wang.
2006.
MultipleDocuments Summarization Based on Genetic Algo-rithm.
Lecture Notes in Computer Science, 4223:355.N.
Madnani, D. Zajic, B. Dorr, N.F.
Ayan, and J. Lin.2007.
Multiple Alternative Sentence Compressionsfor Automatic Text Summarization.
In Proceed-ings of the Document Understanding Conference atNLT/NAACL.R.
McDonald.
2006.
Discriminative sentence compres-sion with soft syntactic constraints.
In Proceedings ofthe 11th EACL, pages 297?304.R.
McDonald.
2007.
A Study of Global Inference Al-gorithms in Multi-document Summarization.
LectureNotes in Computer Science, 4425:557.Ani Nenkova and Rebecca Passonneau.
2004.
Evaluat-ing content selection in summarization: The pyramidmethod.
In Proceedings of HLT-NAACL.A.
Nenkova and L. Vanderwende.
2005.
The impact offrequency on summarization.
Technical Report MSR-TR-2005-101, Microsoft Research, Redmond, Wash-ington.A.
Nenkova.
2008.
Entity-driven rewrite for multidocu-ment summarization.
Proceedings of IJCNLP.Slav Petrov and Dan Klein.
2007.
Learning and infer-ence for hierarchically split PCFGs.
In AAAI 2007(Nectar Track).D.
Pisinger, A.B.
Rasmussen, and R. Sandvik.
2005.Solution of large-sized quadratic knapsack problemsthrough aggressive reduction.
INFORMS Journal onComputing.K.
Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,H.
Suzuki, and L. Vanderwende.
2007.
The PythySummarization System: Microsoft Research at DUC2007.
In Proceedings of the Document UnderstandingConference.J.
Turner and E. Charniak.
2005.
Supervised and Unsu-pervised Learning for Sentence Compression.
In Pro-ceedings of ACL.W.
Yih, J. Goodman, L. Vanderwende, and H. Suzuki.2007.
Multi-document summarization by maximiz-ing informative content-words.
In International JointConference on Artificial Intelligence.18
