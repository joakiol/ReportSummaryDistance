Unsupervised Declarative Knowledge Induction for Constraint-BasedLearning of Information Structure in Scientific DocumentsYufan GuoDTALUniversity of Cambridge, UKyg244@cam.ac.ukRoi ReichartTechnion - IITHaifa, Israelroiri@ie.technion.ac.ilAnna KorhonenDTALUniversity of Cambridge, UKalk23@cam.ac.ukAbstractInferring the information structure of scien-tific documents is useful for many NLP appli-cations.
Existing approaches to this task re-quire substantial human effort.
We proposea framework for constraint learning that re-duces human involvement considerably.
Ourmodel uses topic models to identify latent top-ics and their key linguistic features in inputdocuments, induces constraints from this in-formation and maps sentences to their domi-nant information structure categories througha constrained unsupervised model.
Whenthe induced constraints are combined with afully unsupervised model, the resulting modelchallenges existing lightly supervised feature-based models as well as unsupervised mod-els that use manually constructed declarativeknowledge.
Our results demonstrate that use-ful declarative knowledge can be learned fromdata with very limited human involvement.1 IntroductionAutomatic analysis of scientific text can help scien-tists find information from literature faster, savingvaluable research time.
In this paper we focus onthe analysis of the information structure (IS) of sci-entific articles where the aim is to assign each unit ofan article (typically a sentence) into a category thatrepresents the information type it conveys.
By infor-mation structure we refer to a particular type of dis-course structure that focuses on the functional roleof a unit in the discourse (Webber et al., 2011).
Forinstance, in the scientific literature, the functionalrole of a sentence could be the background or moti-vation of the research, the methods used, the experi-ments carried out, the observations on the results, orthe author?s conclusions.Readers of scientific literature find information inIS-annotated articles much faster than in unanno-tated articles (Guo et al., 2011b).
ArgumentativeZoning (AZ) ?
an information structure scheme thathas been applied successfully to many scientific do-mains (Teufel et al., 2009) ?
has improved taskssuch as summarization and information extractionand retrieval (Teufel and Moens, 2002; Tbahriti etal., 2006; Ruch et al., 2007; Liakata et al., 2012;Contractor et al., 2012).Existing approaches to information structure anal-ysis require substantial human effort.
Most usefeature-based machine learning, such as SVMs andCRFs (e.g.
(Teufel and Moens, 2002; Lin et al.,2006; Hirohata et al., 2008; Shatkay et al., 2008;Guo et al., 2010; Liakata et al., 2012)) which relyon thousands of manually annotated training sen-tences.
Also the performance of such methods israther limited: Liakata et al.
(2012) reported per-class F-scores ranging from .53 to .76 in the bio-chemistry and chemistry domains and Guo et al.
(2013a) reported substantially lower numbers for thechallenging Introduction and Discussion sections inbiomedical domain.Guo et al.
(2013a) recently applied the General-ized Expectation (GE) criterion (Mann and McCal-lum, 2007) to information structure analysis usingexpert knowledge in the form of discourse and lexi-cal constraints.
Their model produces promising re-sults, especially for sections and categories where131Transactions of the Association for Computational Linguistics, vol.
3, pp.
131?143, 2015.
Action Editor: Masaaki Nagata.Submission batch: 10/2014; Revision batch 1/2015; Published 3/2015.
c?2015 Association for Computational Linguistics.feature-based models perform poorly.
Even theunsupervised version which uses constraints undera maximum-entropy criterion without any feature-based model, outperforms fully-supervised feature-based models in detecting challenging low fre-quency categories across sections.
However, this ap-proach still requires substantial human effort in con-straint generation.
Particularly, lexical constraintswere constructed by creating a detailed word list foreach information structure category.
For example,words such as ?assay?
were carefully selected andused as a strong indicator of the ?Method?
category:p(Method|assay) was constrained to be high (above0.9).
Such a constraint (developed for the biomedi-cal domain) may not be applicable to a new domain(e.g.
computer science) with a different vocabularyand writing style.In fact, most existing works on learning withdeclarative knowledge rely on manually constructedconstraints.
Little work exists on automatic declar-ative knowledge induction.
A notable exceptionis (McClosky and Manning, 2012) that proposeda constraint learning model for timeline extraction.This approach, however, requires human supervi-sion in several forms including task specific con-straint templates (see Section 2).We present a novel framework for learning declar-ative knowledge which requires very limited humaninvolvement.
We apply it to information structureanalysis, based on two key observations: 1) Eachinformation structure category defines a distributionover a section-specific and an article-level set of lin-guistic features.
2) Each sentence in a scientific doc-ument, while having a dominant category, may con-sist of features mostly related to other categories.This flexible view enables us to make use of topicmodels which have not proved useful in previous re-lated works (Varga et al., 2012; Reichart and Korho-nen, 2012).We construct topic models at both the individualsection and article level and apply these models todata, identifying latent topics and their key linguis-tic features.
This information is used to constrain orbias unsupervised models for the task in a straight-forward way: we automatically generate constraintsfor a GE model and a bias term for a graph clus-tering objective, such that the resulting models as-sign each of the input sentences to one informationZone DefinitionBackground (BKG) the background of the studyProblem (PROB) the research problemMethod (METH) the methods usedResult (RES) the results achievedConclusion (CON) the authors?
conclusionsConnection (CN) work consistent with the current workDifference (DIFF) work inconsistent with the current workFuture work (FUT) the potential future direction of the researchTable 1: The AZ categorization scheme of this paperstructure category.
Both models provide high qual-ity sentence-based classification, demonstrating thegenerality of our approach.We experiment with the AZ scheme for the anal-ysis of the logical structure, scientific argumenta-tion and intellectual attribution of scientific papers(Teufel and Moens, 2002), using an eight-categoryversion of this scheme for biomedicine ((Mizuta etal., 2006; Guo et al., 2013b), Table 1).
In evalu-ation against gold standard annotations, our modelrivals the model of Guo et al.
(2013a) which relieson manually constructed constraints, as well as astrong supervised feature-based model trained withup to 2000 sentences.
In task-based evaluation wemeasure the usefulness of the induced categories forcustomized summarization (Contractor et al., 2012)from specific types of information in an article.
TheAZ categories induced by our model prove morevaluable than those of (Guo et al., 2013a) and thosein the gold standard.
Our work demonstrates thegreat potential of automatically induced declarativeknowledge in both improving the performance of in-formation structure analysis and reducing reliance ofhuman supervision.2 Previous WorkAutomatic Declarative Knowledge InductionLearning with declarative knowledge offers effectivemeans of reducing human supervision and improv-ing performance.
This framework augments feature-based models with domain and expert knowledgein the form of, e.g., linear constraints, posteriorprobabilities and logical formulas (e.g.
(Chang etal., 2007; Mann and McCallum, 2007; Mann andMcCallum, 2008; Ganchev et al., 2010)).
It hasproven useful for many NLP tasks including unsu-pervised and semi-supervised POS tagging, parsing(Druck et al., 2008; Ganchev et al., 2010; Rush etal., 2012) and information extraction (Chang et al.,1322007; Mann and McCallum, 2008; Reichart and Ko-rhonen, 2012; Reichart and Barzilay, 2012).However, declarative knowledge is still created ina costly manual process.
We propose inducing suchknowledge directly from text with minimal humaninvolvement.
This idea could be applied to almostany NLP task.
We apply it here to information struc-ture analysis of scientific documents.Little prior work exists on automatic constraintlearning.
Recently, (McClosky and Manning, 2012)investigated the approach for timeline extraction.They used a set of gold relations and their temporalspans and applied distant learning to find approxi-mate instances for classifier training.
A set of con-straint templates specific to temporal learning werealso specified.
In contrast, we do not use manuallyspecified guidance in constraint learning.
Particu-larly, we construct constraints from latent variables(topics in topic modeling) estimated from raw textrather than applying maximum likelihood estimationover observed variables (fluents and temporal ex-pressions) in labeled data.
Our method is thereforeless dependent on human supervision.
Even morerecently, (Anzaroot et al., 2014) presented a super-vised dual-decomposition based method, in the con-text of citation field extraction, which automaticallygenerates large families of constraints and learn theircosts with a convex optimization objective duringtraining.
Our work is unsupervised, as opposed totheir model which requires a manually annotatedtraining corpus for constraint learning.Information Structure Analysis Variousschemes have been proposed for analysing theinformation structure of scientific documents, inparticular the patterns of topics, functions and re-lations at sentence level.
Existing schemes includeargumentative zones (Teufel and Moens, 2002;Mizuta et al., 2006; Teufel et al., 2009), discoursestructure (Burstein et al., 2003; Webber et al.,2011), qualitative dimensions (Shatkay et al., 2008),scientific claims (Blake, 2009), scientific concepts(Liakata et al., 2010), and information status (Mark-ert et al., 2012), among others.
Most previous workon automatic analysis of information structure relieson supervised learning (Teufel and Moens, 2002;Burstein et al., 2003; Mizuta et al., 2006; Shatkayet al., 2008; Guo et al., 2010; Liakata et al., 2012;Markert et al., 2012).
Given the prohibitive costof manual annotation, unsupervised and minimallysupervised techniques such as clustering (Kiela etal., 2014) and topic modeling (Varga et al., 2012;O?
Se?aghdha and Teufel, 2014) are highly important.However, the performance of such approachesshows a large room for improvement.
Our work isspecifically aimed at addressing this problem.Information Structure Learning with Declar-ative Knowledge Recently, Reichart and Korhonen(2012) and Guo et al.
(2013a) developed constrainedmodels that integrate rich linguistic knowledge (e.g.discourse patterns, syntactic features and sentencesimilarity information) for more reliable unsuper-vised or transductive learning of information cate-gories in scientific abstracts and articles.
Guo et al.
(2013a) used detailed lexical constraints developedvia human supervision.
Whether automatically in-duced declarative knowledge can rival such manualconstraints is a question we address in this work.While Reichart and Korhonen (2012) used moregeneral constraints, their most effective discourseconstraints were tailored to scientific abstracts andare less relevant to full papers.3 ModelWe introduce a topic-model based approach todeclarative knowledge (DK) acquisition and describehow this knowledge can be applied to two unsuper-vised models for our task.
Section 3.1 describes howtopic models are used to induce topics that serve asthe main building blocks of our DK.
Section 3.2 ex-plains how the resulting topics and their key featuresare transformed into DK ?
constraints in the general-ized expectation (GE) model and bias functions in agraph clustering algorithm.3.1 Inducing Information Structure Categorieswith Latent Dirichlet AllocationLatent Dirichlet Allocation (LDA) LDA is a gener-ative process widely used for discovering latent top-ics in text documents (Blei et al., 2003).
It assumesthe following generative process for each document:1.
Choose ?i ?
Dirichlet(?
), i ?
{1, ...,M}2.
Choose ?k ?
Dirichlet(?
), k ?
{1, ...,K}3.
For each word wij , j ?
{1, ..., Ni}(a) Choose a topic zij ?Multinomial(?i)(b) Choose a word wij ?Multinomial(?zij ),133where ?i is the distribution of topics in document i,?k is the distribution of observed features (usuallywords) for topic k, zij is the topic of the j-th wordin document i, and wij is the j-th word in documenti.
A number of inference techniques have been pro-posed for the parameter estimation of this process,e.g.
variational Bayes (Blei et al., 2003) and Gibbssampling (Griffiths and Steyvers, 2004) which weuse in this work.Topics and Information Structure CategoriesA key challenge in the application of LDA to in-formation structure analysis is defining the observedfeatures generated by the model.
Topics are usuallydefined to be distributions over all the words in adocument, but in our task this can lead to undesiredtopics.
Consider, for example, the following sen-tences from the Introduction section of an article:- First, exposure to BD-diol via inhalation causes an increasein Hprt mutation frequency in both mice and rats (25).- Third, BD-diol is a precursor to MI, an important urinarymetabolite in humans exposed to BD (19).In a word-based topic model we can expect that mostof the content words in these sentences will be gen-erated by a single topic that can be titled as ?BD-diol?, or by two different topics related to ?micerat?
and ?human?.
However, information structurecategories should reflect the role of the sentence ine.g.
the discourse or argument structure of the pa-per.
For example, given the AZ scheme both sen-tences should belong to the background (BKG) cate-gory (Table 1).
The same requirement applies to thetopics induced by the topic models.Features In applying LDA to AZ, we define top-ics as distributions over: (a) words of particular syn-tactic categories; (b) syntactic (POS tag) patterns;and (c) discourse markers (citations, tables and fig-ures).
Below we list our features, among which Pro-noun, Conjunction, Adjective and Adverb are noveland the rest are adapted from (Guo et al., 2013a):Citation A single feature that aggregates togetherthe various citation formats in scientific articles (e.g.
[20] or (Tudek 2007)).Table, Figure A single feature representing any ref-erences to tables or figures in a sentence.Verb Verbs are central to the meaning of a sentence.Each of the base forms of the verbs in the corpus isa unique feature.Pronoun Personal (e.g.
?we?)
and possessive pro-nouns (e.g.
?our?)
and the following adjectives (asin e.g.
?our recent?
or ?our previous?)
may indicatethe ownership of the work (e.g.
the author?s ownvs.
other people?s work), which is important for ourtask.
Each of the above words or word combinationsis a unique feature.Conjunction Conjunctions indicate the relationshipbetween different sentences in text.
We consider twotypes of conjunctions: (1) coordinating conjunctions(indicated by the POS tag ?CC?
in the output of theC&C POS tagger); and (2) saturated clausal modi-fiers (indicated by the POS tag ?IN?
and the corre-sponding grammatical relation ?cmod?
in the outputof the C&C parser).
Each word that forms a con-junction according to this definition is a unique fea-ture.Adjective and Adverb Adjectives provide descrip-tive information about objects, while adverbs maychange or qualify the meaning of verbs or adjectives.Each adverb and adjective that appears in more than5 articles in the corpus is a unique feature.1Modal, Tense, Voice Previous work has demon-strated a strong correlation between tense, voice,modals and information categories (e.g.
(Guo et al.,2011a; Liakata et al., 2012)).
These features are in-dicated by the part-of-speech (POS) tag of verbs.For example, the phrase ?may have been investi-gated?
is represented as ?may-MD have-VBZ be-VBN verb-VBN?.As a pre-processing step, each sentence in the in-put corpus was represented with the list of featuresit consists of.
Consider, for example, the followingsentence from a Discussion section in our data-set:- In a previous preliminary study we reported that the resultsof a limited proof of concept human clinical trial using sulin-dac (1-5%) and hydrogen peroxide (25%) gels applied daily forthree weeks on actinic keratoses (AK) involving the upper ex-tremities [27].
Before running the Discussion sectiontopic model (see below for the features consideredby this model), this sentence is converted to the fol-lowing representation:[cite] previous preliminary we limitedThe topic models we construct are assumed to gen-1We collapsed adverbs ending with -ly into the correspond-ing adjectives to reduce data sparsity.
Verbs were spared thefrequency cut-off because rarely occurring verbs are likely tocorrespond to domain-specific actions that are probably indica-tive of the METH category.134Model FeaturesArticle Verb, Table, Figure, Modal, Tense, VoiceIntroduction Citation, Pronoun, Verb, Modal, Tense, VoiceDiscussion Citation, Pronoun, Conjunction, Adjective, AdverbTable 2: The features used in the article-level and thesection-specific topic models in this papererate these features rather than bag-of-words.Topic Models Construction Looking at the cate-gories in Table 1 it is easy to see that different com-binations of the features in topic model generationwill be relevant for different category distinctions.For example, personal pronouns are particularly rel-evant for distinguishing between categories relatedto current vs. previous works.Some distinctions between categories are, in turn,more relevant for some sections than for others.
Forexample, the distinction between the background(BKG) and the definition of the research problem(PROB) is important for the Introduction section, butless important for the results section.
Similarly thedistinction between conclusions (CON) and differ-ence from previous work (DIFF) is more relevant forthe Discussion section than other sections.We therefore constructed two types of topic mod-els: section-specific and article-level models, rea-soning that some distinctions apply globally at thearticle level while some apply more locally at thesection level.
Section-specific models were con-structed for the Introduction section and for the Dis-cussion section.2 Table 2 presents the features thatare used with each topic model.A key issue in the application of topic models toour task is the definition of the unit of text for which?i, the distribution over topics, is drawn from theDirichlet distribution (step 1 of the algorithm).
Thischoice is data dependent, and the standard choiceis the document level.
However, for scientific arti-cles the paragraph level is a better choice, because aparagraph contains only a small subset of informa-tion structure categories while in a full article cat-egories are more evenly distributed.
We thereforeadopted the paragraph as our basic unit of text.
Thesection-level and the article-level models are applied2The Methods section is less suitable for a section-leveltopic model as 97.5% of its sentences belong to its dominantcategory (METH) (Table 3).
Preliminary experiments withsection-level topic models for the Methods and Results sectionsdid not lead to improved performance.to the collection of paragraphs in the specific sectionacross the test set articles or in the entire set of testarticles, respectively.3.2 Declarative Knowledge InductionMost sentence-based information structure analysisapproaches associate each sentence with a uniquecategory.
However, since the MAP assignment oftopics to features associates each sentence with mul-tiple topics, we cannot directly interpret the resultingtopics as categories of input sentences.3In this section we present two methods for in-corporating the information conveyed by the topicmodels (see Section 3.1) in unsupervised models.The first method biases a graph clustering algorithmwhile the second generates constraints that can beused with a GE criterion.Graph Clustering We use the graph clusteringobjective of Dhillon et al.
(2007) which can be opti-mized efficiently, without eigenvalues calculations:maxY?trace(Y?
TW?1/2AW?1/2Y?
)where A is a similarity matrix, W is a diagonalmatrix of the weight of each cluster, and Y?
is anorthonormal matrix, indicating cluster membership,which is proportional to the square root of W .To make use of topics to bias the graph clusteringtowards the desired solution, we define the similaritymatrix A, whose (i, j)?
th entry corresponds to thei-th and j-th test set sentences as follows:A(i, j) = f(Si, Sj) + ?g(Si, Sj , T ),whereSi = {All the features extracted from sentence i }T = {Tk|Tk = {top N features associated with topic k}}f(Si, Sj) = |Si ?
Sj |g(Si, Sj , T ) ={1 ?x ?
Si?y ?
Sj?k x ?
Tk ?
y ?
Tk0 Otherwisewhere Tk consists of the N features that are as-signed the maximum probability according to the k-th topic.
Under this formulation, the topic modelterm g(?)
is defined to be the indicator of whethertwo sentences share features associated with thesame topic.
If this is true, the algorithm is encour-aged to assign these sentences to the same cluster.Generalized Expectation A generalized expecta-tion (GE) criterion is a term in an objective function3Our preliminary experiments demonstrated that assigningthe learned topics to the test sentences performs poorly.135that assigns a score to model expectations (Mannand McCallum, 2008; Druck et al., 2008; Bellareet al., 2009).
Given a score function g(?
), a discrim-inative model p?
(y|x), a vector of feature functionsf?(?
), and an empirical distribution p?
(x), the valueof a GE criterion is:g(Ep?(x)[Ep?(y|x)[f?
(x, y)]])A popular choice of g(?)
is a measure of distance(e.g.
L2 norm) between model and reference expec-tations.
The feature functions f?(?)
and the refer-ence expectations of f?(?)
are traditionally specifiedby experts, which provides a way to integrate declar-ative knowledge into machine learning.Consider a Maximum Entropy (MaxEnt) modelp?
(y|x) = 1Z?
exp(?
?
f(x, y)), where f(?)
is a vec-tor of feature functions, ?
the feature weights, andZ?
the partition function.
The following objectivefunction can be used for training MaxEnt with GEcriteria on unlabeled data:max??g(Ep?(x)[Ep?(y|x)[f?
(x, y)]])?
?j?2j2?2where the second term is a zero-mean ?2-varianceGaussian prior on parameters.Let the k-th feature function f?k (?)
be an indicatorfunction:f?k (x, y) = 1{xik=1?y=yk}(x, y)where xik is the ik-th element/feature in the featurevector x.
The model expectation of f?k (?)
becomes:Ep?(x)[Ep?
(y|x)[f?k (x, y)]] = p?
(xik = 1)p?
(yk|xik = 1)To calculate g(?
), a reference expectation of f?k (?
)can be obtained after specifying (the upper andlower limits of) p(yk|xik = 1):lk ?
p(yk|xik = 1) ?
ukThis type of constraints, for example, 0.9 ?p(CON|suggest) ?
1, have been successfully ap-plied to GE-based information structure analysis byGuo et al.
(2013a).
Here we build on their frame-work and our contribution is the automatic inductionof such constraints by topic modeling.The association between features and topics canbe transformed into constraints as follows.
Let Wzbe a set of top N key features associated with topicz ?
the N features that are assigned the maximumprobability according to the topic.
We compute thefollowing topic-specific feature sets:Az = {w|w ?
Wz ?
?t 6= z w 6?
Wt} ?
the set offeatures associated with topic z but not with any ofthe other topics;Bz =?t6=zWt ?
the set of features associated with atleast one topic other than z.For every topic-feature pair (zk, wk) we thereforewrite the following constraint:lk ?
p(zk|wk = 1) ?
ukWe set the probability range for the k-th pair as fol-lows:If wk ?
Azk then lk = 0.9, uk = 1,If wk ?
Bzk then lk = 0, uk = 0.1,In any other case lk = 0, uk = 1.The values of lk and uk were selected such that theyreflect the strong association between the key fea-tures and their topics.
Our basic reasoning is that ifa sentence is represented by one of the key uniquefeatures of a given topic, it is highly likely to be as-sociated with that topic.
Likewise, a sentence is un-likely to be associated with the topic of interest if ithas a key feature for any other topics.3.3 Summary of ContributionLearning with declarative knowledge is an active re-cent research avenue in the NLP community.
Inthis framework feature-based models are augmentedwith domain and expert knowledge encoded mostoften by constraints of various types.
The humaneffort involved with this framework is the manualspecification of the declarative knowledge.
This re-quires deep understanding of the domain and task inquestion.
The resulting constraints typically spec-ify detailed associations between lexical, grammat-ical and discourse elements and the information tobe learned (see, e.g., tables 2 and 3 of (Guo et al.,2013a) and table 1 of (Chang et al., 2007)).Our key contribution is the automatic induction ofdeclarative knowledge that can be easily integratedinto unsupervised models in the form of constraintsand bias functions.
Our model requires minimal do-main and task knowledge.
We do not specify listsof words or discourse markers (as in (Guo et al.,2013a)) but, instead, our model automatically asso-ciates latent variables both with linguistic features,taken from a very broad and general feature set (e.g.136BKG PROB METH RES CON CN DIFF FUTArticle 16.9 2.8 34.8 17.9 22.3 4.3 0.8 0.2(8171)Introduction 74.8 13.2 5.4 0.6 5.9 0.1 - -(1160)Methods 0.5 0.2 97.5 1.4 0.2 0.2 0.1 -(2557)Results 4.0 2.1 11.7 68.9 12.1 1.1 0.1 -(2054)Discussion 16.9 1.1 0.7 1.5 63.5 13.3 2.4 0.7(2400)Table 3: Distribution of sentences (shown in percentages)in articles and individual sections in the AZ-annotatedcorpus.
The total number of sentences in each sectionappears in parentheses below the section name.all the words that belong to a given set of POS tags),and with sentences in the input text.
In the next sec-tion we present our experiments which demonstratethe usefulness of this declarative knowledge.4 ExperimentsData and Models We used the full paper cor-pus earlier employed in (Guo et al., 2013a) whichincludes 8171 annotated sentences (with reportedinter-annotator agreement: ?
= .83) from 50biomedical journal articles from the cancer risk as-sessment domain.
One third of this corpus was savedfor a development set on which our model was de-signed and its hyperparameters were tuned (see be-low).
The corpus is annotated according to the Argu-mentative Zoning (AZ) scheme (Teufel and Moens,2002; Mizuta et al., 2006) described in Table 1.
Ta-ble 3 shows the distribution of AZ categories and thetotal number of sentences in each individual section.Since section names vary across articles, we groupedsimilar sections before calculating the statistics (e.g.Materials and Methods sections were grouped underMethod).
The table demonstrates that although thereis a dominant category in each section (e.g.
BKG inIntroduction), up to 36.5% of the sentences in eachsection fall into other categories.Feature Extraction We used the C&C POS tag-ger and parser trained on biomedical literature (Cur-ran et al., 2007; Rimell and Clark, 2009) in the fea-ture extraction process.
Lemmatization was donewith Morpha (Minnen et al., 2001).Baselines We compared our models (TopicGCand TopicGE) against the following baselines: (a)an unconstrained unsupervised model ?
the unbiasedversion of the graph clustering we use for TopicGC(i.e.
where g(?)
is omitted, GC); (b) the unsuper-vised constrained GE method of (Guo et al., 2013a)where the constraints were created by experts (Ex-pertGE); (c) supervised unconstrained MaximumEntropy models, each trained to predict categoriesin a particular section using 150 sentences from thatsection, as in the lightly supervised case in (Guo etal., 2013a) (MaxEnt); and (d) a baseline that assignsall the sentences in a given section to the most fre-quent gold-standard category of that section (Table3).
This baseline emulates the use of section namesfor information structure classification.Our constraints, which we use in the TopicGEand TopicGC models, are based on topics that arelearned on the test corpus.
While having access tothe raw test text at training time is a standard as-sumption in many unsupervised NLP works (e.g.
(Klein and Manning, 2004; Goldwater and Grif-fiths, 2007; Lang and Lapata, 2014)), it is impor-tant to quantify the extent to which our method de-pends on its access to the test set.
We therefore con-structed the TopicGE* model which is identical toTopicGE except that the topics are learned from an-other collection of 47 biomedical articles contain-ing 9352 sentences.
Like our test set, these articlesare from the cancer risk assessment domain - all ofthem were published in the Toxicol.
Sci.
journalin the years 2009-2012 and were retrieved using thePubMed search engine with the key words ?cancerrisk assessment?.
There is no overlap between thisnew dataset and our test set (Guo et al., 2013a).Models and Parameters For graph clustering,we used the Graclus software (Dhillon et al., 2007).For GE and MaxEnt, we used the Mallet software(McCallum, 2002).
The ?
parameter in the graphclustering was set to 10 using the development data.Several values of this parameter in the range of[10, 1000] yielded very similar performance.
Thenumber of key features considered for each topic,N , was set to 40, 20 and 15 for the article, Introduc-tion section, and Discussion section topic models,respectively.
This difference reflects the number offeature types (Table 2) and the text volume (Table 3)of the respective models.Evaluation We evaluated the overall accuracy aswell as the category-level precision, recall and F-score for each section.
TopicGC, TopicGE, Top-icGE* and the baseline GC methods are unsuper-137Introduction Method Result DiscussionGC TGC TGE TGE* EGE MFC GC TGC TGE TGE* EGE MFC GC TGC TGE TGE* EGE MFC GC TGC TGE TGE* EGE MFCF1BKG .78 .83 .89 .86 .87 .86 - - - - .07 - - - - - .46 - .47 .47 .45 .49 .46 -PROB .34 .16 .31 .19 .24 - - - - - .33 - - - - - .04 - - - - - .32 -METH - .16 .12 .16 .35 - .98 .98 .98 .98 .93 .99 .29 - .25 .32 .29 - - - - - .14 -RES - - - - .07 - - - - - .27 - .67 .82 .81 .77 .80 .82 - - - - .14 -CON - .10 .26 .03 .28 - - - - - - - .39 .28 .27 .29 .42 - .82 .83 .82 .82 .71 .78CN - - - - - - - - - - - - - - - - .25 - - .21 .23 .11 .20 -DIFF - - - - - - - - - - - - - - - - - - - - - - .12 -FUT - - - - - - - - - - - - - - - - - - - - - - .36 -Acc.
.61 .68 .77 .74 .72 .75 .97 .97 .97 .97 .87 .97 .51 .68 .67 .62 .64 .69 .66 .67 .67 .67 .56 .63Table 4: Performance (class based F1-score and overall accuracy (Acc.))
of unbiased Graph Clustering (GC), GraphClustering with declarative knowledge learned from topic modeling (TopicGC model, TGC column), GeneralizedExpectation using constraints learned from topic modeling (TopicGE, TGE) and the same model where constraints arelearned using an external set of articles (TopicGE*, TGE*), GE with constraints created by experts (ExpertGE, EGE -a replication of (Guo et al., 2013a)) and the most frequent gold standard category of the section (MFC)vised and therefore induce unlabeled categories.
Toevaluate their output against the gold standard AZannotation we first apply a standard greedy many-to-one mapping (naming) scheme in which each in-duced category is mapped to the gold category thatshares the highest number of elements (sentence)with it (Reichart and Rappoport, 2009).
The to-tal number of induced topics was 9 with each topicmodel inducing three topics.4 For light supervision,a ten-fold cross-validation scheme was applied.In addition, we compare the quality of the auto-matically induced and manually constructed declar-ative knowledge in the context of customized sum-marization (Contractor et al., 2012) where sum-maries of specific types of information in an articleare to be generated (we focused on the article?s con-clusions).
While an intuitive solution would be tosummarize the Discussion section of a paper, only63.5% of its sentences belong to the gold standardConclusion category (Table 3).For our experiment, we first generated five setsof sentences.
The first four sets consist of the ar-ticle sentences annotated with the CON category ac-cording to: TopicGE or TopicGC or ExpertGE or thegold standard annotation.
The fifth set is the Discus-sion section.
We then used Microsoft AutoSumma-rize (Microsoft, 2007) to select sentences from eachof the five sets such that the number of words in eachsummary amounts for 10% of the words in the input.4The number of gold standard AZ categories is 8.
However,we wanted each of our topic models to induce the same numberof topics in order to reduce the number of parameters to therequired minimum.For evaluation, we asked an expert to summarizethe conclusions of each article in the corpus.
Wethen evaluated the five summaries against the gold-standard summaries written by the expert in terms ofvarious ROUGE scores (Lin, 2004).5 ResultsWe report here the results for our constrained unsu-pervised models compared to the baselines.
We startwith quantitative evaluation and continue with qual-itative demonstration of the topics learned by thetopic models and their key features which providethe substance for the constraints and bias functionsused in our information structure models.Unsupervised Learning Results Table 4 presentsthe performance of the four main unsupervisedlearning models discussed in this paper: GC, Top-icGC, TopicGE, and ExpertGE of (Guo et al.,2013a).
Our models (TopicGC and TopicGE) out-perform the ExpertGE when considering categorybased F-score for the dominant categories of eachsection.
ExpertGE is most useful in identifying theless frequent categories of each section (Table 3),which is in line with (Guo et al., 2013a).
The overallsentence-based accuracy of TopicGE is significantlyhigher than that of ExpertGE for all four sections(bottom line of the table).
Furthermore, for all foursections it is one of our models (TopicGC or Top-icGE) that provides the best result under this mea-sure, among the unsupervised models.The table further provides a comparison of the un-supervised models to the MFC baseline which as-signs all the sentences of a section to its most fre-138Introduction Method Result DiscussionTopicGE Light TopicGE Light TopicGE Light TopicGE LightP R F P R F P R F P R F P R F P R F P R F P R FBKG .84 .95 .89 .78 .99 .87 - - - - - - - - - - - - .41 .51 .45 .38 .19 .25PROB .33 .30 .31 .57 .11 .18 - - - - - - - - - .25 .02 .04 - - - - - -METH .40 .07 .12 .50 .21 .30 .97 1 .98 .97 1 .98 .34 .20 .25 .62 .14 .23 - - - - - -RES - - - - - - - - - - - - .74 .90 .81 .71 .98 .82 - - - - - -CON .44 .18 .26 .80 .06 .11 - - - - - - .30 .25 .27 .57 .16 .25 .78 .87 .82 .69 .96 .80CN - - - - - - - - - - - - - - - - - - .32 .18 .23 .35 .06 .10DIFF - - - - - - - - - - - - - - - - - - - - - - - -FUT - - - - - - - - - - - - - - - - - - - - - - - -Acc.
0.77 0.77 0.97 0.97 0.67 0.70 0.67 0.66Table 5: Performance (class based Precision, Recall and F-score as well as overall accuracy (Acc.))
of the TopicGEmodel and of an unconstrained MaxEnt model trained with Light supervision (total of 600 sentences - 150 trainingsentences for each section-level model).
The same pattern of results holds when the MaxEnt is trained with up to 2000sentences (500 sentences for each section-level model).TopicGE TopicGC ExpertGE Section GoldR P F R P F R P F R P F R P FROUGE-1 45.2 54.0 46.8 43.5 55.1 46.1 43.7 49.1 43.8 46.7 43.8 42.6 43.3 55.4 46.2ROUGE-2 30.0 35.8 30.8 28.4 35.7 29.8 25.5 28.2 25.2 28.6 26.3 25.8 27.8 35.1 29.3ROUGE-L 43.3 51.6 44.8 41.6 52.6 44.1 41.3 46.2 41.3 44.2 41.3 40.3 41,1 52.3 43.7Table 6: ROUGE scores of zone (TopicGE, TopicGC, ExpertGE or gold standard) and Discussion section based sum-maries.
TopicGE provides the best summaries.
TopicGC outperforms ExpertGE and the Discussion section systemsand in two measures the gold categorization based system as well.
Result patterns with ROUGE(3,4,W-1.2, S* andSU*) are very similar to those of the table.
The differences between TopicGE and ExpertGE are statistically significantusing t-test with p < 0.05.
The differences between TopicGE and gold, as well as between ExpertGE and gold are notstatistically significant.quent category according to the gold standard.
Thisbaseline sheds light on the usefulness of sectionnames for our task.
As is evident from the table,while this baseline is competitive with the unsuper-vised models in terms of accuracy, its class-basedF-score performance is quite poor.
Not only does itlag behind the unsupervised models in terms of theF-score of the most frequent classes of the Introduc-tion and Discussion sections, but it does not iden-tify any of the classes except from the most frequentones in any of the sections - a task the unsupervisedmodels often perform with reasonable quality.Finally, the table also presents the performanceof the TopicGE* model for which constraints areleaned from an external data set - different from thetest set.
The results show that there is no substantialdifference between the performance of the TopicGEand TopicGE* models.
While TopicGE achievesbetter F-scores in five of the cases in the table, Top-icGE* is better in four cases and the performance isidentical in two cases.
Section level accuracies arebetter for TopicGE in two of the four sections, butthe difference is only 3-5%.Comparison with Supervised Learning Table 5compares the quality of unsupervised constrained-based learning with that of lightly supervisedfeature-based learning.
Since our models, TopicGCand TopicGE, perform quite similarly, we includedonly TopicGE in this evaluation.
The lightly su-pervised models (MaxEnt classifiers) were trainedwith a total of 600 sentences - 150 for each section-specific classifier.
The table demonstrates that Top-icGE outperforms MaxEnt with light supervision interms of class based F-scores in the Introduction andDiscussion sections.
In the Methods section, where97.5% of the sentences belong to the most frequentcategory, and in the Results section, the models per-form quite similarly.
Overall accuracy numbers arequite similar for both models with MaxEnt doingbetter for the Results section and TopicGE for theDiscussion section.
These results further demon-strate that unsupervised constrained learning pro-vides a practical solution to information structureanalysis of scientific articles.Extractive Summarization Evaluation Table6 presents the average ROUGE scores for zone-based (TopicGE, TopicGC, ExpertGE and gold) andsection-based summaries across our test set articles.139Topic Features1 {do} be {done} {doing} {be done} {have been done} induce {may do} {to do} show have {have done} increase{did} suggest indicate report cause include inhibit find observe involve associate activate demonstrate result uselead play {could do} know {do do} form contribute {can do} {would do} promote reduce2 {were done} {done} {doing} {did} use be describe contain perform incubate {do} determine analyze follow addisolate purchase wash accord {to do} treat collect remove prepare obtain measure store stain centrifuge transferdetect purify assess supplement carry dissolve plate receive kill3 {did} {done} be {doing} {were done} [tab fig] {do} show increase observe compare {to do} expose use have find{did do} treat {be done} report follow drink reduce result administer decrease determine measure include evaluateaffect detect induce indicate associate provide reveal suggest occurTable 7: Topics and key features extracted by the article-level model (including modal, tense and voice marked incurly brackets, reference to tables or figures marked in square brackets, and verbs in the base form)Topic Features1 [no cite] {did} (we) {done} {do} {doing} use {were done} (present) {to do} investigate be (mammary) determineprovide (our) treat compare examine2 {did} {done} [cite] {doing} {were done} be expose find [no cite] drink increase report (recent) (previous) admin-ister {do} contain evaluate (early)3 {do} [cite] be {done} [no cite] {doing} {be done} {have been done} induce {have done} (it) show {may do}have {to do} include increase (their) associateTable 8: Topics and key features extracted by the section-specific topic model of the Introduction section (includingcitations marked in square brackets, pronouns and the follow-up adjective modifiers marked in parentheses, modal,tense and voice marked in curly brackets, and verbs in their base form)Topic Features1 (we) [no cite] (our) higher (mammary) as because (first) significant possible high (early) (positive) most2 [cite] present (present) (previous) similar different (its) although consistent furthermore greater due most whereas3 [no cite] not also (it) but however more (their) both therefore only thus significant lowerTable 9: Topics and key features extracted by the section-specific topic model of the Discussion section (includingcitations marked in square brackets, pronouns and the follow-up adjective modifiers marked in parentheses, and con-junctions, adjectives and adverbs)TopicGE and TopicGC based summaries outperformthe other systems, even the one that uses gold stan-dard information structure categorization.
A poten-tial explanation for the better performance of ourmodels compared to ExpertGE is that the relativestrength of our models is in identifying the majorcategory of each section while ExpertGE is better atidentifying low or medium frequency categories.Qualitative Analysis We next provide a qualita-tive analysis of the topics induced by our topic mod-els ?
the article-level model as well as the section-level models ?
and their key features.
Note thatboth our models, TopicGE and TopicGC, assumethat the induced topics provide a good approxima-tion of the information structure categories and buildtheir constraints (expert knowledge) from these top-ics accordingly.
Below we examine this assumption.Table 7 presents the topics and key features ob-tained from global topic modeling applied to full ar-ticles.
The table reveals a strong correlation betweenpresent/future tense and topic 1, and between pasttense and topics 2 and 3 (Modal, Tense and Voicefeatures).
The table further demonstrates that top-ics 1 and 3 are linked to verbs that describe researchfindings, such as ?show?
and ?demonstrate?
in topic1, and ?report?
and ?indicate?
in topic 3, whereastopic 2 seems related to verbs that describe methodsand experiments such as ?use?
and ?prepare?.
Thefeature corresponding to tables and figures [tab fig]is only seen in topic 3.
Based on these observations,topics 1, 2 and 3 seem to be related to AZ categoriesCON, METH and RES respectively.Tables 8 and 9 present the topics and the key fea-tures obtained from the section-specific topic mod-140eling for the Introduction and Discussion sections.Due to space limitations we cannot provide a de-tailed analysis of the information included in thesetables, but it is easy to see that they provide evi-dence for the correlation between topics in the sec-tion specific models and AZ categories.
Table 8demonstrates that for the Introduction section topic1 correlates with the author?s work and topics 2 and3 with previous work.
Table 9 shows that for theDiscussion section topics 1 and 3 well correlate withthe AZ CON category and topic 2 with the BKG, CNand DIFF categories.
Our analysis therefore demon-strates that the induced topics are well aligned withthe actual categories of the AZ classification schemeor with distinctions (e.g.
the author?s own workvs.
works of others) that are very relevant for thisscheme.
Note that we have not seeded our modelswith word-lists and the induced topics are thereforepurely data-driven.6 DiscussionWe presented a new framework for automatic in-duction of declarative knowledge and applied it toconstraint-based modeling of the information struc-ture analysis of scientific documents.
Our main con-tribution is a topic-model based method for unsuper-vised acquisition of lexical, syntactic and discourseknowledge guided by the notion of topics and theirkey features.
We demonstrated that the induced top-ics and key features can be used with two differ-ent unsupervised learning methods ?
a constrainedunsupervised generalized expectation model and agraph clustering formulation.
Our results show thatthis novel framework rivals more supervised alterna-tives.
Our work therefore contributes to the impor-tant challenge of automatically inducing declarativeknowledge that can reduce the dependence of MLalgorithms on manually annotated data.The next natural step in this research is generaliz-ing our framework and make it applicable to moreapplications, domains and machine learning mod-els.
We are currently investigating a number of ideaswhich will hopefully lead to better natural languagelearning with reduced human supervision.ReferencesSam Anzaroot, Alexandre Passos, David Belanger, andAndrew McCallum.
2014.
Learning soft linear con-straints with application to citation field extraction.
InACL, pages 593?602.Kedar Bellare, Gregory Druck, and Andrew McCallum.2009.
Alternating projections for learning with expec-tation constraints.
In Proceedings of the 25th Con-ference on Uncertainty in Artificial Intelligence, pages43?50.Catherine Blake.
2009.
Beyond genes, proteins, andabstracts: Identifying scientific claims from full-textbiomedical articles.
Journal of Biomedical Informat-ics, 43(2):173?189.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet allocation.
Journal of MachineLearning Research, 3:993?1022.Jill Burstein, Daniel Marcu, and Kevin Knight.
2003.Finding the write stuff: Automatic identification ofdiscourse structure in student essays.
IEEE IntelligentSystems, 18(1):32?39.Ming-Wei Chang, Lev Ratinov, and Dan Roth.2007.
Guiding semi-supervision with constraint-driven learning.
In ACL, pages 280?287.Danish Contractor, Yufan Guo, and Anna Korhonen.2012.
Using argumentative zones for extractive sum-marization of scientific articles.
In COLING, pages663?678.James Curran, Stephen Clark, and Johan Bos.
2007.Linguistically motivated large-scale nlp with c&c andboxer.
In Proceedings of the ACL 2007 Demo andPoster Sessions, pages 33?36.Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis.2007.
Weighted graph cuts without eigenvectors:A multilevel approach.
IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 29(11):1944?1957.Gregory Druck, Gideon Mann, and Andrew McCallum.2008.
Learning from labeled features using gener-alized expectation criteria.
In Proceedings of the31st annual international ACM SIGIR conference onResearch and development in information retrieval,pages 595?602.Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
Journal of MachineLearning Research, 11:2001?2049.Sharon Goldwater and Tom Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speech tag-ging.
In ACL, pages 744?751.Thomas L Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101(suppl 1):5228?5235.141Yufan Guo, Anna Korhonen, Maria Liakata, Ilona SilinsKarolinska, Lin Sun, and Ulla Stenius.
2010.
Identify-ing the information structure of scientific abstracts: aninvestigation of three different schemes.
In BioNLP,pages 99?107.Yufan Guo, Anna Korhonen, and Thierry Poibeau.2011a.
A weakly-supervised approach to argumenta-tive zoning of scientific documents.
In EMNLP, pages273?283.Yufan Guo, Anna Korhonen, Ilona Silins, and Ulla Ste-nius.
2011b.
Weakly-supervised learning of infor-mation structure of scientific abstracts?is it accurateenough to benefit real-world tasks in biomedicine?Bioinformatics, 27(22):3179?3185.Yufan Guo, Roi Reichart, and Anna Korhonen.
2013a.Improved information structure analysis of scientificdocuments through discourse and lexical constraints.In NAACL HLT, pages 928?937.Yufan Guo, Ilona Silins, Ulla Stenius, and Anna Korho-nen.
2013b.
Active learning-based information struc-ture analysis of full scientific articles and two applica-tions for biomedical literature review.
Bioinformatics,29(11):1440?1447.Kenji Hirohata, Naoaki Okazaki, Sophia Ananiadou, andMitsuru Ishizuka.
2008.
Identifying sections in sci-entific abstracts using conditional random fields.
InIJCNLP, pages 381?388.Douwe Kiela, Yufan Guo, Ulla Stenius, and Anna Ko-rhonen.
2014.
Unsupervised discovery of informa-tion structure in biomedical documents.
Bioinformat-ics, page btu758.Dan Klein and Christopher Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In ACL, pages 478?485.Joel Lang and Mirella Lapata.
2014.
Similarity-drivensemantic role induction via graph partitioning.
Com-putational Linguistics, 40(3):633?669.Maria Liakata, Simone Teufel, Advaith Siddharthan, andColin Batchelor.
2010.
Corpora for conceptualizationand zoning of scientific papers.
In LREC, pages 2054?2061.Maria Liakata, Shyamasree Saha, Simon Dobnik, ColinBatchelor, and Dietrich Rebholz-Schuhmann.
2012.Automatic recognition of conceptualization zones inscientific articles and two life science applications.Bioinformatics, 28(7):991?1000.Jimmy Lin, Damianos Karakos, Dina Demner-Fushman,and Sanjeev Khudanpur.
2006.
Generative contentmodels for structural analysis of medical abstracts.
InBioNLP, pages 65?72.Chin-Yew Lin.
2004.
ROUGE: A package for auto-matic evaluation of summaries.
In Text SummarizationBranches Out: Proceedings of the ACL-04 Workshop,pages 74?81.Gideon S Mann and Andrew McCallum.
2007.
Simple,robust, scalable semi-supervised learning via expecta-tion regularization.
In ICML, pages 593?600.Gideon S. Mann and Andrew McCallum.
2008.
General-ized expectation criteria for semi-supervised learningof conditional random fields.
In ACL, pages 870?878.Katja Markert, Yufang Hou, and Michael Strube.
2012.Collective classification for fine-grained informationstatus.
In ACL, pages 795?804.Andrew Kachites McCallum.
2002.
MAL-LET: A machine learning for language toolkit.http://mallet.cs.umass.edu.David McClosky and Christopher D. Manning.
2012.Learning constraints for consistent timeline extraction.In EMNLP-CoNLL, pages 873?882.Microsoft.
2007.
AutoSummarize: Automaticallysummarize a document.
https://support.office.com/en-us/article/Automatically-summarize-a-document-b43f20ae-ec4b-41cc-b40a-753eed6d7424.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of english.
NaturalLanguage Engineering, 7(3):207?223.Yoko Mizuta, Anna Korhonen, Tony Mullen, and NigelCollier.
2006.
Zone analysis in biology articles as abasis for information extraction.
International Journalof Medical Informatics on Natural Language Process-ing in Biomedicine and Its Applications, 75(6):468?487.Diarmuid O?
Se?aghdha and Simone Teufel.
2014.
Unsu-pervised learning of rhetorical structure with un-topicmodels.
In Proceedings of COLING 2014: TechnicalPapers, pages 2?13.Roi Reichart and Regina Barzilay.
2012.
Multi-event ex-traction guided by global constraints.
In NAACL HLT,pages 70?79.Roi Reichart and Anna Korhonen.
2012.
Document andcorpus level inference for unsupervised and transduc-tive learning of information structure of scientific doc-uments.
In COLING, pages 995?1006.Roi Reichart and Ari Rappoport.
2009.
The nvi cluster-ing evaluation measure.
In CoNLL, pages 165?173.Laura Rimell and Stephen Clark.
2009.
Porting alexicalized-grammar parser to the biomedical domain.Journal of Biomedical Informatics, 42(5):852?865.Patrick Ruch, Clia Boyer, Christine Chichester, ImadTbahriti, Antoine Geissbhler, Paul Fabry, Julien Gob-eill, Violaine Pillet, Dietrich Rebholz-Schuhmann,Christian Lovis, and Anne-Lise Veuthey.
2007.
Usingargumentation to extract key sentences from biomedi-cal abstracts.
International Journal of Medical Infor-matics, 76(2-3):195?200.142Alexander Rush, Roi Reichart, Michael Collins, andAmir Globerson.
2012.
Improved parsing and pos tag-ging using inter-sentence consistency constraints.
InEMNLP-CoNLL, pages 1434?1444.Hagit Shatkay, Fengxia Pan, Andrey Rzhetsky, andW John Wilbur.
2008.
Multi-dimensional classifica-tion of biomedical text: Toward automated, practicalprovision of high-utility text to diverse users.
Bioin-formatics, 24(18):2086?2093.Imad Tbahriti, Christine Chichester, Fre?de?rique Lisacek,and Patrick Ruch.
2006.
Using argumentation toretrieve articles with similar citations.
InternationalJournal of Medical Informatics, 75(6):488?495.Simone Teufel and Marc Moens.
2002.
Summariz-ing scientific articles: Experiments with relevanceand rhetorical status.
Computational Linguistics,28(4):409?445.Simone Teufel, Advaith Siddharthan, and Colin Batche-lor.
2009.
Towards domain-independent argumenta-tive zoning: Evidence from chemistry and computa-tional linguistics.
In EMNLP, pages 1493?1502.Andrea Varga, Daniel Preotiuc-Pietro, and FabioCiravegna.
2012.
Unsupervised document zone iden-tification using probabilistic graphical models.
InLREC, pages 1610?1617.Bonnie Webber, Markus Egg, and Valia Kordoni.
2011.Discourse structure and language technology.
NaturalLanguage Engineering, 18(4):437?490.143144
