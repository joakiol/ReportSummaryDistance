Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 718?726,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPUnsupervised Tokenization for Machine TranslationTagyoung Chung and Daniel GildeaComputer Science DepartmentUniversity of RochesterRochester, NY 14627AbstractTraining a statistical machine translationstarts with tokenizing a parallel corpus.Some languages such as Chinese do not in-corporate spacing in their writing system,which creates a challenge for tokenization.Moreover, morphologically rich languagessuch as Korean present an even biggerchallenge, since optimal token boundariesfor machine translation in these languagesare often unclear.
Both rule-based solu-tions and statistical solutions are currentlyused.
In this paper, we present unsuper-vised methods to solve tokenization prob-lem.
Our methods incorporate informa-tion available from parallel corpus to de-termine a good tokenization for machinetranslation.1 IntroductionTokenizing a parallel corpus is usually the firststep of training a statistical machine translationsystem.
With languages such as Chinese, whichhas no spaces in its writing system, the main chal-lenge is to segment sentences into appropriate to-kens.
With languages such as Korean and Hun-garian, although the writing systems of both lan-guages incorporate spaces between ?words?, thegranularity is too coarse compared with languagessuch as English.
A single word in these lan-guages is composed of several morphemes, whichoften correspond to separate words in English.These languages also form compound nouns morefreely.
Ideally, we want to find segmentations forsource and target languages that create a one-to-one mapping of words.
However, this is not al-ways straightforward for two major reasons.
First,what the optimal tokenization for machine trans-lation should be is not always clear.
Zhang et al(2008b) and Chang et al (2008) show that get-ting the tokenization of one of the languages inthe corpus close to a gold standard does not nec-essarily help with building better machine trans-lation systems.
Second, even statistical methodsrequire hand-annotated training data, which meansthat in resource-poor languages, good tokenizationis hard to achieve.In this paper, we explore unsupervised methodsfor tokenization, with the goal of automaticallyfinding an appropriate tokenization for machinetranslation.
We compare methods that have ac-cess to parallel corpora to methods that are trainedsolely using data from the source language.
Unsu-pervised monolingual segmentation has been stud-ied as a model of language acquisition (Goldwateret al, 2006), and as model of learning morphol-ogy in European languages (Goldsmith, 2001).Unsupervised segmentation using bilingual datahas been attempted for finding new translationpairs (Kikui and Yamamoto, 2002), and for findinggood segmentation for Chinese in machine trans-lation using Gibbs sampling (Xu et al, 2008).
Inthis paper, further investigate the use of bilingualinformation to find tokenizations tailored for ma-chine translation.
We find a benefit not only forsegmentation of languages with no space in thewriting system (such as Chinese), but also for thesmaller-scale tokenization problem of normaliz-ing between languages that include more or lessinformation in a ?word?
as defined by the writ-ing system, using Korean-English for our exper-iments.
Here too, we find a benefit from usingbilingual information, with unsupervised segmen-tation rivaling and in some cases surpassing su-pervised segmentation.
On the modeling side,we use dynamic programming-based variationalBayes, making Gibbs sampling unnecessary.
Wealso develop and compare various factors in themodel to control the length of the tokens learned,and find a benefit from adjusting these parame-ters directly to optimize the end-to-end translationquality.7182 TokenizationTokenization is breaking down text into lexemes?
a unit of morphological analysis.
For relativelyisolating languages such as English and Chinese, aword generally equals a single token, which is usu-ally a clearly identifiable unit.
English, especially,incorporates spaces between words in its writingsystem, which makes tokenization in English usu-ally trivial.
The Chinese writing system does nothave spaces between words, but there is less am-biguity where word boundaries lie in a given sen-tence compared to more agglutinative languages.In languages such as Hungarian, Japanese, andKorean, what constitutes an optimal token bound-ary is more ambiguous.
While two tokens are usu-ally considered two separate words in English, thismay be not be the case in agglutinative languages.Although what is considered a single morpholog-ical unit is different from language to language,if someone were given a task to align words be-tween two languages, it is desirable to have one-to-one token mapping between two languages inorder to have the optimal problem space.
For ma-chine translation, one token should not necessarilycorrespond to one morphological unit, but rathershould reflect the morphological units and writingsystem of the other language involved in transla-tion.For example, consider a Korean ?word?
meok-eoss-da, which means ate.
It is written as a sin-gle word in Korean but consists of three mor-phemes eat-past-indicative.
If one uses morpho-logical analysis as the basis for Korean tokeniza-tion, meok-eoss-da would be split into three to-kens, which is not desirable if we are translat-ing Korean to English, since English does nothave these morphological counterparts.
However,a Hungarian word szekr?enyemben, which means inmy closet, consists of three morphemes closet-my-inessive that are distinct words in English.
In thiscase, we do want our tokenizer to split this ?word?into three morphemes szekr?eny em ben.In this paper, we use segmentation and to-kenization interchangeably as blanket terms tocover the two different problems we have pre-sented here.
The problem of segmenting Chinesesentences into words and the problem of segment-ing Korean or Hungarian ?words?
into tokens ofright granularity are different in their nature.
How-ever, our models presented in section 3 handle theboth problems.3 ModelsWe present two different methods for unsuper-vised tokenization.
Both are essentially unigramtokenization models.
In the first method, we trylearning tokenization from word alignments witha model that bears resemblance to Hidden Markovmodels.
We use IBMModel 1 (Brown et al, 1993)for the word alignment model.
The second modelis a relatively simpler monolingual tokenizationmodel based on counts of substrings which servesas a baseline of unsupervised tokenization.3.1 Learning tokenization from alignmentWe use expectation maximization as our primarytools in learning tokenization form parallel text.Here, the observed data provided to the algorithmare the tokenized English string en1and the unto-kenized string of foreign characters cm1.
The un-observed variables are both the word-level align-ments between the two strings, and the tokeniza-tion of the foreign string.
We represent the tok-enization with a string sm1of binary variables, withsi= 1 indicating that the ith character is the finalcharacter in a word.
The string of foreign wordsf?1can be thought of as the result of applying thetokenization s to the character string c:f = s ?
c where ?
=m?i=1siWe use IBM Model 1 as our word-level align-ment model, following its assumptions that eachforeign word is generated independently from oneEnglish word:P (f |e) =?aP (f ,a | e)=?a?iP (fi| eai)P (a)=?i?jP (fi| ej)P (ai= j)and that all word-level alignments a are equallylikely: P (a) =1nfor all positions.
While Model 1has a simple EM update rule to compute posteri-ors for the alignment variables a and from themlearn the lexical translation parameters P (f | e),we cannot apply it directly here because f itself isunknown, and ranges over an exponential numberof possibilities depending on the hidden segmenta-tion s. This can be addressed by applying dynamicprograming over the sequence s. We compute the719posterior probability of a word beginning at posi-tion i, ending at position j, and being generated byEnglish word k:P (si...j= (1, 0, .
.
.
, 0, 1), a = k | e)=?
(i)P (f | ek)P (a = k)?
(j)P (c | e)where f = ci.
.
.
cjis the word formed by con-catenating characters i through j, and a is a vari-able indicating which English position generatedf .
Here ?
and ?
are defined as:?
(i) = P (ci1, si= 1 | e)?
(j) = P (cmj+1, sj= 1 | e)These quantities resemble forward and backwardprobabilities of hidden Markov models, and canbe computed with similar dynamic programmingrecursions:?
(i) =L??=1?(i?
?
)?aP (a)P (cii?
?| ea)?
(j) =L?
?=1?aP (a)P (cj+?j| ea)?
(j + ?
)where L is the maximum character length for aword.Then, we can calculate the expected counts ofindividual word pairs being aligned (cji, ek) by ac-cumulating these posteriors over the data:ec(cji, ek) +=?
(i)P (a)P (cji| ek)?(j)?
(m)The M step simply normalizes the counts:?P (f | e) =ec(f, e)?eec(f, e)Our model can be compared to a hiddenMarkovmodel in the following way: a target word gen-erates a source token which spans a zeroth orderMarkov chain of characters in source sentence,where a ?transition?
represents a segmentation anda ?emission?
represents an alignment.
The modeluses HMM-like dynamic programming to do in-ference.
For the convenience, we refer to thismodel as the bilingual model in the rest of thepaper.
Figure 1 illustrates our first model withan small example.
Under this model we are notlearning segmentation directly, but rather we arelearning alignments between two sentences.
Thec1c2c3c4f1f2e1e2Figure 1: The figure shows a source sentencef = f1, f2= s ?
c1.
.
.
c4where s = (0, 0, 1, 1)and a target sentence e = e1, e2.
There is a seg-mentation between c3and c4; thus c1, c2, c3formf1and c3forms f2.
f1is generated by e2and f2isgenerated by e1.segmentation is by-product of learning the align-ment.
We can find the optimal segmentation ofa new source language sentence using the Viterbialgorithm.
Given two sentences e and f ,a?= argmaxaP (f ,a | e)and segmentation s?implied by alignment a?isthe optimal segmentation of f found by this model.3.2 Learning tokenization from substringcountsThe second tokenization model we propose ismuch simpler.
More sophisticated unsupervisedmonolingual tokenization models using hierarchi-cal Bayesian models (Goldwater et al, 2006)and using the minimum description length prin-ciple (Goldsmith, 2001; de Marcken, 1996) havebeen studied.
Our model is meant to serve asa computationally efficient baseline for unsuper-vised monolingual tokenization.
Given a corpusof only source language of unknown tokenization,we want to find the optimal s given c ?
s thatgives us the highest P (s | c).
According to Bayes?rule,P (s | c) ?
P (c | s)P (s)Again, we assume that all P (s) are equally likely.Let f = s?c = f1.
.
.
f?, where fiis a word undersome possible segmentation s. We want to find thes that maximizes P (f).
We assume thatP (f) = P (f1)?
.
.
.?
P (f?
)To calculate P (fi), we count every possible720substring ?
every possible segmentation of char-acters ?
from the sentences.
We assume thatP (fi) =count(fi)?kcount(fk)We can compute these counts by making a sin-gle pass through the corpus.
As in the bilingualmodel, we limit the maximum size of f for prac-tical reasons and to prevent our model from learn-ing unnecessarily long f .
With P (f), given a se-quence of characters c, we can calculate the mostlikely segmentation using the Viterbi algorithm.s?= argmaxsP (f)Our rationale for this model is that if a span ofcharacters f = ci.
.
.
cjis an independent token, itwill occur often enough in different contexts thatsuch a span of characters will have higher prob-ability than other spans of characters that are notmeaningful.
For the rest of the paper, this modelwill be referred to as the monolingual model.3.3 Tokenizing new dataSince the monolingual tokenization only uses in-formation from a monolingual corpus, tokenizingnew data is not a problem.
However, with thebilingual model, we are learning P (f | e).
We arerelying on information available from e to get thebest tokenization for f. However, the parallel sen-tences will not be available for new data we wantto translate.
Therefore, for the new data, we haveto rely only on P (f) to tokenize any new data,which can be obtained by calculatingP (f) =?eP (f | e)P (e)With P (f) from the bilingual model, we can runthe Viterbi algorithm in the same manner as mono-lingual tokenization model for monolingual data.We hypothesize that we can learn valuable infor-mation on which token boundaries are preferablein language f when creating a statistical machinetranslation system that translates from language fto language e.4 Preventing overfittingWe introduce two more refinements to our word-alignment induced tokenization model and mono-lingual tokenization model.
Since we are consid-ering every possible token f that can be guessedfrom our corpus, the data is very sparse.
For thebilingual model, we are also using the EM algo-rithm to learn P (f | e), which means there is adanger of the EM algorithm memorizing the train-ing data and thereby overfitting.
We put a Dirichletprior on our multinomial parameter for P (f | e)to control this situation.
For both models, we alsowant a way to control the distribution of tokenlength after tokenization.
We address this problemby adding a length factor to our models.4.1 Variational BayesBeal (2003) and Johnson (2007) describe vari-ational Bayes for hidden Markov model in de-tail, which can be directly applied to our bilingualmodel.
With this Bayesian extension, the emissionprobability of our first model can be summarizedas follows:?e| ?
?
Dir(?
),fi| ei= e ?
Multi(?e).Johnson (2007) and Zhang et al (2008a) showhaving small ?
helps to control overfitting.
Fol-lowing this, we set our Dirichlet prior to be assparse as possible.
It is set at ?
= 10?6, the num-ber we used as floor of our probability.For the model incorporating the length factor,which is described in the next section, we do notplace a prior on our transition probability, sincethere are only two possible states, i.e.
P (s = 1)and P (s = 0).
This distribution is not as sparse asthe emission probability.Comparing variational Bayes to the traditionalEM algorithm, the E step stays the same but theM step for calculating the emission probabilitychanges as follows:?P (f | e) =exp(?
(ec(f, e) + ?))exp(?
(?eec(f, e) + s?
))where ?
is the digamma function, and s is the sizeof the vocabulary from which f is drawn.
Sincewe do not accurately know s, we set s to be thenumber of all possible tokens.
As can be seen fromthe equation, by setting ?
to a small value, we arediscounting the expected count with help of thedigamma function.
Thus, having lower ?
leads toa sparser solution.4.2 Token lengthWe now add a parameter that can adjust the to-kenizer?s preference for longer or shorter tokens.72100.10.20.30.40.50.61  2  3  4  5  6refP(s)=0.55lambda=3.1600.10.20.30.40.50.61  2  3  4  5  6refP(s)=0.58lambda=2.13Figure 2: Distribution of token length for (from left to right) Chinese, and Korean.
?ref?
is the empiricaldistribution from supervised tokenization.
Two length factors ?
?1and ?2are also shown.
For ?1, theparameter to geometric distribution P (s) is set to the value learned from our bilingual model.
For ?2, ?is set using the criterion described in the experiment section.This parameter is beneficial because we want ourdistribution of token length after tokenization toresemble the real distribution of token length.
Thisparameter is also useful because we also want toincorporate information on the number of tokensin the other language in the parallel corpus.
This isbased on the assumption that, if tokenization cre-ates a one-to-one mapping, the number of tokensin both languages should be roughly the same.
Wecan force the two languages to have about the samenumber of tokens by adjusting this parameter.
Thethird reason is to further control overfitting.
Ourobservation is that certain morphemes are verycommon, such that they will be always observedattached to other morphemes.
For example, in Ko-rean, a noun attached with nominative case markeris very common.
Our model is likely to learn anoun attached with the morpheme ?
nominativecase marker ?
rather than noun itself.
This is notdesirable when the noun occurs with less commonmorphemes; in these cases the morpheme will besplit off creating inconsistencies.We have experimented with two different lengthfactors, each with one adjustable parameter:?1(?)
= P (s)(1?
P (s))??1?2(?)
= 2??
?The first, ?1, is the geometric distribution, wherel is length of a token and P (s) is probability ofsegmentation between two characters.
The secondlength factor ?2was acquired through several ex-periments and was found to work well.
As canbeen seen from Figure 2, the second factor dis-counts longer tokens more heavily than the geo-metric distribution.
We can adjust the value of ?and P (s) to increase or decrease number of tokensafter segmentation.For our monolingual model, incorporating thesefactors is straightforward.
We assume thatP (f) ?
P (f1)?(?1)?
.
.
.?
P (fn)?
(?n)where ?iis the length of fi.
Then, we use the sameViterbi algorithm to select the f1.
.
.
fnthat max-imizes P (f), thereby selecting the optimal s ac-cording to our monolingual model with a lengthfactor.
We pick the value of ?
and P (s) thatproduces about the same number of tokens in thesource side as in the target side, thereby incorpo-rating some information about the target language.For our bilingual model, we modify our modelslightly to incorporate ?1, creating a hybridmodel.
Now, our forward probability of forward-backward algorithm is:?
(i) =L??=1?(i?
l)?1(?
)?aP (a)P (cii?
?| ea)and the expected count of (cji, ek) isec(cji, ek) +=?
(i)P (a)P (cji| ek)?
(j)?1(j ?
i)?
(m)For ?1, we can learn P (s) for the geometric dis-tribution from the model itself:1P (s) =1mm?i?(i)?(i)?
(m)1The equation is for one sentence, but in practice, we sumover all sentences in the training data to calculate P (s).722We can also fix P (s) instead of learning it throughEM.
We incorporate ?2into the bilingual modelas follows: after learning P (f) from the bilingualmodel, we pick the ?
in the same manner as themonolingual model and run the Viterbi algorithm.After applying the length factor, what we haveis a log-linear model for tokenization, with twofeature functions with equal weights: the lengthfactor and P (f) learned from model.5 Experiments5.1 DataWe tested our tokenization methods on two differ-ent language pairs: Chinese-English, and Korean-English.
For Chinese-English, we used FBISnewswire data.
The Korean-English parallel datawas collected from news websites and sentence-aligned using two different tools described byMoore (2002) and Melamed (1999).
We used sub-sets of each parallel corpus consisting of about 2Mwords and 60K sentences on the English side.
Forour development set and test set, Chinese-Englishhad about 1000 sentences each with 10 referencetranslations taken from the NIST 2002 MT eval-uation.
For Korean-English, 2200 sentence pairswere randomly sampled from the parallel corpus,and held out from the training data.
These weredivided in half and used for test set and develop-ment set respectively.
For all language pairs, veryminimal tokenization ?
splitting off punctuation?
was done on the English side.5.2 Experimental setupWe used Moses (Koehn et al, 2007) to trainmachine translation systems.
Default parameterswere used for all experiments except for the num-ber of iterations for GIZA++ (Och and Ney, 2003).GIZA++ was run until the perplexity on develop-ment set stopped decreasing.
For practical rea-sons, the maximum size of a token was set at threefor Chinese, and four for Korean.2Minimum errorrate training (Och, 2003) was run on each systemafterwards and BLEU score (Papineni et al, 2002)was calculated on the test sets.For the monolingual model, we tested two ver-sions with the length factor ?1, and ?2.
We picked?
and P (s) so that the number of tokens on sourceside (Chinese, and Korean) will be about the same2In the Korean writing system, one character is actuallyone syllable block.
We do not decompose syllable blocksinto individual consonants and vowels.as the number of tokens in the target side (En-glish).For the bilingual model, as explained in themodel section, we are learning P (f | e), but onlyP (f) is available for tokenizing any new data.
Wecompared two conditions: using only the sourcedata to tokenize the source language training dataaccording to P (f) (which is consistent with theconditions at test time), and using both the sourceand English data to tokenize the source languagetraining data (which might produce better tok-enization by using more information).
For the firstlength factor ?1, we ran an experiment where themodel learns P (s) as described in the model sec-tion, and we also had experiments where P (s)waspre-set at 0.9, 0.7, 0.5, and 0.3 for comparison.
Wealso ran an experiment with the second length fac-tor ?2where ?
was picked as the same manner asthe monolingual model.We varied tokenization of development set andtest set to match the training data for each ex-periment.
However, as we have implied in theprevious paragraph, in the one experiment whereP (f | e) was used to segment training data, di-rectly incorporating information from target cor-pus, tokenization for test and development set isnot exactly consistent with tokenization of train-ing corpus.
Since we assume only source corpusis available at the test time, the test and the devel-opment set was tokenized only using informationfrom P (f).We also trained MT systems using supervisedtokenizations and tokenization requiring a mini-mal effort for the each language pair.
For Chinese-English, the minimal effort tokenization is maxi-mal tokenization where every Chinese character issegmented.
Since a number of Chinese tokeniz-ers are available, we have tried four different to-kenizations for the supervised tokenizations.
Thefirst one is the LDC Chinese tokenizer available atthe LDC website3, which is compiled by ZhibiaoWu.
The second tokenizer is a maxent-based to-kenizer described by Xue (2003).
The third andfourth tokenizations come from the CRF-basedStanford Chinese segmenter described by Changet al (2008).
The difference between third andfourth tokenization comes from the different goldstandard, the third one is based on Beijing Uni-versity?s segmentation (pku) and the fourth one isbased on Chinese Treebank (ctb).
For Korean-3http://projects.ldc.upenn.edu/Chinese/LDC ch.htm723Chinese KoreanBLEU F-score BLEUSupervisedRule-based morphological analyzer 7.27LDC segmenter 20.03 0.94Xue?s segmenter 23.02 0.96Stanford segmenter (pku) 21.69 0.96Stanford segmenter (ctb) 22.45 1.00UnsupervisedSplitting punctuation only 6.04Maximal (Character-based MT) 20.32 0.75Bilingual P (f | e) with ?1P (s) = learned 19.25 6.93Bilingual P (f) with ?1P (s) = learned 20.04 0.80 7.06Bilingual P (f) with ?1P (s) = 0.9 20.75 0.87 7.46Bilingual P (f) with ?1P (s) = 0.7 20.59 0.81 7.31Bilingual P (f) with ?1P (s) = 0.5 19.68 0.80 7.18Bilingual P (f) with ?1P (s) = 0.3 20.02 0.79 7.38Bilingual P (f) with ?222.31 0.88 7.35Monolingual P (f) with ?120.93 0.83 6.76Monolingual P (f) with ?220.72 0.85 7.02Table 1: BLEU score results for Chinese-English and Korean-English experiments and F-score of seg-mentation compared against Chinese Treebank standard.
The highest unsupervised score is highlighted.English, the minimal effort tokenization splittingoff punctuation and otherwise respecting the spac-ing in the Korean writing system.
A Korean mor-phological analysis tool4was used to create the su-pervised tokenization.For Chinese-English, since a gold standard forChinese segmentation is available, we ran an addi-tional evaluation of tokenization from each meth-ods we have tested.
We tokenized the raw textof Chinese Treebank (Xia et al, 2000) using allof the methods (supervised/unsupervised) we havedescribed in this section except for the bilingualtokenization using P (f | e) because the Englishtranslation of the Chinese Treebank data was notavailable.
We compared the result against the goldstandard segmentation and calculated the F-score.6 ResultsResults from Chinese-English and Korean-Englishexperiments are presented in Table 1.
Note thatnature of data and number of references are dif-ferent for the two language pairs, and thereforethe BLEU scores are not comparable.
For bothlanguage pairs, our models perform equally wellas supervised baselines, or even better.
We can4http://nlp.kookmin.ac.kr/HAM/eng/main-e.htmlobserve three things from the result.
First, tok-enization of training data using P (f | e) tested ona test set tokenized with P (f) performed worsethan any other experiments.
This affirms our be-lief that consistency in tokenization is importantfor machine translation, which was alsomentionedby Chang et al (2008).
Secondly, we are learningvaluable information by looking at the target lan-guage.
Compare the result of the bilingual modelwith ?2as the length factor to the result of themonolingual model with the same length factor.The bilingual version consistently performed bet-ter than the monolingual model in all languagepairs.
This tells us we can learn better tokenboundaries by using information from the targetlanguage.
Thirdly, our hypothesis on the needfor heavy discount for longer tokens is confirmed.The value for P (s) learned by the model was 0.55,and 0.58 for Chinese, and Korean respectively.
Forboth language pairs, this accurately reflects theempirical distribution of token length, as can beseen in Figure 2.
However, experiments whereP (s) was directly optimized performed better, in-dicating that this parameter should be optimizedwithin the context of a complete system.
The sec-ond length factor ?2, which discounts longer to-kens even more heavily, generally performed bet-724English the two presidents will hold a joint press conference at the end of their summit talks .Untokenized Korean ???????????????????????????????
.Supervised ????
???
???
????????
??
?????
?????
?
??
.Bilingual P (f | e) with ?1???????
??????????
???????
??????
?
.Bilingual P (f) with ?2????
???
??????????
???????
?????
??
.Monolingual P (f) with ?1???
?
???
??????????
?????????????
?
.Monolingual P (f) with ?2????
???
??????????
????????????
??
.Figure 3: Sample tokenization results for Korean-English data.
The underscores are added to clearlyvisualize where the breaks are.ter than the first length factor when used in con-junction with the bilingual model.
Lastly, F-scoresof Chinese segmentations compared against thegold standard shows higher segmentation accuracydoes not necessarily lead to higher BLEU score.F-scores presented in Table 1 are not directly com-parable for all different experiments because thetest data (Chinese Treebank) is used in training forsome of the supervised segmenters, but these num-bers do show how close unsupervised segmenta-tions are to the gold standard.
It is interesting tonote that our highest unsupervised segmentationresult does make use of bilingual information.Sample tokenization results for Korean-Englishexperiments are presented in Figure 3.
We observethat different configurations produce different tok-enizations, and the bilingual model produced gen-erally better tokenizations for translation com-pared to the monolingual models or the super-vised tokenizer.
In this example, the tokenizationobtained from the supervised tokenizer, althoughmorphologically correct, is too fine-grained for thepurpose of translation to English.
For example,it correctly tokenized the attributive suffix ?
-nhowever, this is not desirable since English has nosuch counterpart.
Both variations of the monolin-gual tokenization have errors such as incorrectlynot segmenting ???
gyeol-gwa-reul, which isa compound of a noun and a case marker, into??
?
gyeol-gwa reul as the bilingual model wasable to do.6.1 Conclusion and future workWe have shown that unsupervised tokenization formachine translation is feasible and can outperformrule-based methods that rely on lexical analysis,or supervised statistical segmentations.
The ap-proach can be applied both to morphological anal-ysis of Korean and the segmentation of sentencesinto words for Chinese, which may at first glaceappear to be quite different problems.
We haveonly shown how our methods can be applied toone language of the pair, where one language isgenerally isolating and the other is generally syn-thetic.
However, our methods could be extendedto tokenization for both languages by iterating be-tween languages.
We also used the most simpleword-alignment model, but more complex wordalignment models could be incorporated into ourbilingual model.Acknowledgments This work was supported byNSF grants IIS-0546554 and ITR-0428020.ReferencesMatthew J. Beal.
2003.
Variational Algorithms for Ap-proximate Bayesian Inference.
Ph.D. thesis, Univer-sity College London.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263?311.Pi-Chuan Chang, Michel Galley, and Christopher Man-ning.
2008.
Optimizing Chinese word segmentationfor machine translation performance.
In Proceed-ings of the Third Workshop on Statistical MachineTranslation, pages 224?232.Carl de Marcken.
1996.
Linguistic structure as compo-sition and perturbation.
In Meeting of the Associa-tion for Computational Linguistics, pages 335?341.Morgan Kaufmann Publishers.John Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
ComputationalLinguistics, 27(2):153?198.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2006.
Contextual dependencies in un-supervised word segmentation.
In Proceedings ofthe International Conference on Computational Lin-guistics/Association for Computational Linguistics(COLING/ACL-06), pages 673?680.725Mark Johnson.
2007.
Why doesn?t EM find goodHMM POS-taggers?
In 2007 Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 296?305, Prague, Czech Republic,June.
Association for Computational Linguistics.Genichiro Kikui and Hirofumi Yamamoto.
2002.Finding translation pairs from english-japanese un-tokenized aligned corpora.
In Proceedings of the40th Annual Conference of the Association forComputational Linguistics (ACL-02) workshop onSpeech-to-speech translation: algorithms and sys-tems, pages 23?30.
Association for ComputationalLinguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Confer-ence of the Association for Computational Linguis-tics (ACL-07), Demonstration Session, pages 177?180.I.
Dan Melamed.
1999.
Bitext maps and alignmentvia pattern recognition.
Computational Linguistics,25:107?130.Robert C. Moore.
2002.
Fast and accurate sentencealignment of bilingual corpora.
In AMTA ?02: Pro-ceedings of the 5th Conference of the Association forMachine Translation in the Americas on MachineTranslation: From Research to Real Users, pages135?144, London, UK.
Springer-Verlag.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingfor statistical machine translation.
In Proceedingsof the 41th Annual Conference of the Association forComputational Linguistics (ACL-03).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Conference of the Association forComputational Linguistics (ACL-02).Fei Xia, Martha Palmer, Nianwen Xue, Mary EllenOkurowski, John Kovarik, Shizhe Huang, TonyKroch, and Mitch Marcus.
2000.
DevelopingGuidelines and Ensuring Consistency for ChineseText Annotation.
In Proc.
of the 2nd InternationalConference on Language Resources and Evaluation(LREC-2000), Athens, Greece.Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-mann Ney.
2008.
Bayesian semi-supervised chineseword segmentation for statistical machine transla-tion.
In Proceedings of the 22nd InternationalConference on Computational Linguistics (Coling2008), pages 1017?1024, Manchester, UK, August.Coling 2008 Organizing Committee.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
In International Journal of Com-putational Linguistics and Chinese Language Pro-cessing, volume 8, pages 29?48.Hao Zhang, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008a.
Bayesian learning of non-compositional phrases with synchronous parsing.
InProceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics (ACL-08),pages 97?105, Columbus, Ohio.Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.2008b.
Improved statistical machine translation bymultiple Chinese word segmentation.
In Proceed-ings of the Third Workshop on Statistical MachineTranslation, pages 216?223.726
