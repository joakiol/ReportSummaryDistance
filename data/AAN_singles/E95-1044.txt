Cooperative Error Handling and Shallow ProcessingTanya BowdenComputer LaboratoryUniversity of CambridgePembroke St.Cambridge CB2 3QGU.K.Tanya.
Bow den @ cl.
cain.
ac.
ukAbstractThis paper is concerned with the de-tection and correction of sub-sententialEnglish text errors.
Previous spellingprograms, unless restricted to a verysmall set of words, have operated aspost-processors.
And to date, gram-mar checkers and other programs whichdeal with ill-formed input usually stepdirectly from spelling considerations toa full-scale parse, assuming a completesentence.
Work described below is aimedat evaluating the effectiveness of shal-low (sub-sentential) processing and thefeasibility of cooperative rror checking,through building and testing appropri-ately an error-processing system.
A sys-tem under construction is outlined whichincorporates morphological checks (us-ing new two-level error rules) over a di-rected letter graph, tag positional tri-grams and partial par~tng.
Intendedtesting is discussed.Unless a keyboard user is particularly proficient,a frustrating amount of time is usually spent back-tracking to pick up mis-typed or otherwise mis-taken input.
Work described in this paper startedfrom an idea of an error processor that would siton top of an editor, detecting/correcting errorsjust after entry, while the user continued withfurther text, relieved from tedious backtracking.Hence 'co-operative' rror processing.
But if aprogram is to catch such errors very soon afterthey are entered, it will have to operate with lessthan the complete sentence.Work underway focuses on shallow processing:how far error detection and correction can proceedwhen the system purview is set to a stretch of textwhich does not admit complete sentential nalysis.To date, grammar checkers and other programs.which deal with illformed input usually step di-rectly from spelling considerations to a full-scalesentence parse.
However treating the sentence asa basic unit loses meaning when the 'sentence' isincomplete or illformed.
Shallow processing is alsointeresting because it should be cheaper and fasterthan a complete analysis of the whole sentence.To investigate issues involved in shallow pro-cessing and cooperative rror handling, the pet(processing errors in text) system is being built.The focus is on these two issues; no attempt isbeing made to produce a complete product 1.
Petoperates over a shifting window of text (it can beattached simply and asynchronously to the Emacseditor).
One word in this purview is in focus at atime.
Pet will give one of three responses to thisword; it will accept the word, suggest a correc-tion, or indicate that it found an error it couldn'tcorrect.
Below follow an outline and discussion ofthe (linguistic) components of pet and discussionof testing and evaluation of the system.
: /Pet  Sys temMorphological Processing ~ Spelling CheckingThe word in focus is first passed through a two-level morphological analysis stage, based on anadaption of (Pulman, 1991).
Two purposes areserved here: checking the word is lexica\] (i.e.
inthe lexicon or a permissible inflection of a word inthe lexicon) and collecting the possible categories,which are represented as sets of feature specifica-tions (Grover, 1993).This morphological lookup operates over a char-acter trie which has been compressed into a (di-rected) graph.
Common endings are shared andcategory information is stored on the first uniquetransition.
The advantages of this compressionare that (1) a word/morpheme is recognised (andcategory affixation rules (Grove L 1993) checked)as soon as the initial letters allow uniqueness,rather than at the end of the word, and (2) thereis an immense saving of space.
There was a reduc-tion of over half the transitions on the trie formedfrom the Alvey lexicon.If the word is unknown, the system reconsidersanalysis from the point where it broke down with1In particular, there axe many HCI issues associ-ated with such a system, which are beyond the scopeof this paper.297the added possibility of an error rule.
There arecurrently four error rules, corresponding to thefour Damerau transformations: omission, inser-tion, transposition, substitution (Damerau, 1964)- considered in that order (Pollock, 1983).
Theerror rules are in two level format and integrateseamlessly into morphological nalysis.~k_ X_  $_..+ ~__  ?This says that any letter ('X') can be inserted, withasterisks indicating that it can occur in any context(compare with (Pulman, 1991)).
The right hand siderepresents the 'error surface' and the left hand sidethe surface with error removed.If this doesn't succeed, it backtracks to try an er-ror rule at an earlier point in the analysis.
Atpresent it will not apply more than one error ruleper word, in keeping with findings on error fre-quencies (Pollock, 1983).As an alternative, a program was developedwhich uses positional binary trigrams (Rise-man,1974) (p.b.t.
's) to spot the error position andto check candidate corrections generated by re-verse Damerau transformations.
This should havethe advantage over the two level error rules in thatit uses a good method of calculating likely errorpositions and because a set of correction possibil-ities can be generated fairly cheaply.
(Correctionpossibilities are ranked using frequency informa-tion on Damerau errors and by giving preferenceto very common words.)
However initial tests overa small file of constructed errors showed that theerror rules did just as well (slightly better in fact)at choosing the 'correct correction'.The error rules are applied when ordinary mor-phological rules fail - which is usually a placep.b.t.
's would mark as in error - but the rulesdon't ignore error locations p.b.t.
's accept as al-lowable letter combinations.
Most importantly,the error rules operate over a letter graph of thelexicon, so only ever consider lexical words (un-known letters are instantiated to the letters as-sociated with the transition options).
The dis-advantage remains that generating many correc-tion possibilities (with SICStus backtracking) istime-consuming.
At present his phase postulatesonly one grapheme at a time, although all itspossible categories are passed along together tolater stages.
If all of these categories eventuallyfail analysis, backtracking to alternative correc-tion candidates (different graphemes) will occur.Tag Checking 8J Partial ParsingThe Alvey features are mapped on to theCLAWS tagset used in the LOB corpus (Garside,1987).
Tag transitions are checked against an oc-currence matrix of the tagged LOB corpus usingpositional binary trigrams imilar to those used inthe spelling checks mentioned above.
Tag checksthough the current set of categories top whenone category passes, but backtrack and continueif parsing then fails.The Core Language Engine (CLE) is an ap-plication independent, unification based "generalpurpose device for mapping between atural an-guage sentences and logical form representations"(Alshawi, 1992).
Its intermediate syntactic stagesinvolve phrasal parsing followed by full syntacticanalysis (top-down, left-corner).
If the latter stagefails, CLE invokes partial parsing.The phrasal phase and partial parsing havebeen extracted and are being adapted to thepresent purpose.
After mapping onto CLEtags, application of the phrasal phase, which im-plements bottom-up arsing, is straightforward.CLE partial parsing, using left-corner analysiscombined with top-down prediction on the resultsof the phrasal phase, looks for complete phrasesand breaks down a wordstring into maximal seg-ments.
(a) the the brown bear ~ the I the brown bear(b) ate the nice friendly --~ ate I the I nice \] friendlyFor example, (a) produces 1 segment and (b) pro-duces 4 segments- whereas "ate the nice friendlycat" would produce 1 segment.Partial parsing needs to be adapted to supportthe idea of the pet purview; partial parsing thataccepts any string likely to constitute part of asentence.
To achieve this the ends of the word-string delimited by the purview need to be treateddifferently.
On the right hand end, 'can start rule'possibilities of words can be considered, using theprediction facility already built into the parsingprocess.
The left hand side could be treated by'can end' possibilities, but a better idea shouldbe to keep within the purview ('remember') pre-viously derived constituents hat involve currentwords.There is a phase to be added after detectionof a tag or partial parsing error.
Currently pro-cessing will just backtrack to the intraword cor-rection level, but particularly if there has been nocorrection yet made, pet should consider here thepossibility of a simple phrase error.
Examples areword doubling and omission of a common functionword.Various ExtensionsDamerau transformations involving the spacecharacter (e.g.
splitting a word) have not beenimplemented yet.
Handling deletion of a space,or substitution of another character for a space,are straightforward additions to the morpholog-ical process.
Transposition of a space could bedealt with by setting up an expectation upon dis-covering deletion of the last character of a wordthat the 'deleted' character may be attached tothe beginning of the next word.
Addition of aspace is trickier because of the focus on the wordas a processing unit, e.g.
corrections for "the re"298could include "there" or "the red", but the presentsystem will not generate the former possibility.At present the word in focus is always thenewest word in the purview.
Altering this wouldprovide some right hand context information,which would among other things facilitate han-dling space addition.
Allowing this change wouldnecessitate a more complex backtracking mecha-nism, as there would be a focus lag between mor-phological processing and later phases.It would be sensible to keep a reference to thewider context, i.e.
be able to refer to earlier de-tections/corrections.
With respect o the editorthat pet is attached to, this could correspond toa log of errors already encountered in the file be-ing edited.
A recent Microsoft product 2 keeps arecord of personal habitual mistakes.
Either couldbe a valuable aid in choosing the correct correc-tion.The system could possibly make better use ofthe graph state of its lexicon.
Word transforma-tion implies either implicit or explicit string com-parison.
The advantage of a graph over a trie isthat it allows for comparison from the end of theword and well as the beginning.Testing and EvaluationWith the aim of evaluating the effectiveness ofshallow processing, tests will be carried out tosee what proportion of different ypes of errorscan be dealt with elegantly, adequately and/orefficiently.
Under examination will be the num-ber of errors missed/caught and wrongly/rightlycorrected.
Different components and configura-tions of the system will be compared, for examplethe error rules v.
p.b.t.'s.
Parameters of the sys-tem will be varied, for example the breadth of thepurview, the position of the purview focus, thenumber of correction candidates and the timingof their generation.
Will shallow processing misstoo many of the errors cooperative error process-ing is aimed at?There are two significant difficulties with col-lecting test data.
The central difficulty is findinga representative sample of genuine rrors by na-tive speakers, in context, with the correct versionof the text attached.
Apart from anything else,'representative' is hard to decide - spectrum of er-rors or distribution of errors ?
Secondly, any cor-pus of text usually contains only those errors thatwere left undetected in the text.
Cooperative pro-cessing deals with errors that one backtracks tocatch; if not a different class or range, these atleast might have a different distribution of errortypes.The ideal data would be records of peoples'keystrokes when interacting with an editor whilecreating or editing a piece of text.
This would2Microsoft Word 6.0 Autocorrect Wizardallow one measure of the (linguistic) feasibilityof cooperative error processing: the effectivenessof shallow processing over errors revealed by thekeystroke-record data.
There does not appear tobe an English source of this kind, so it is plannedto compile one.For comparison, a variety of other data has beencollected.
Preliminary tests used generated errors,from a program that produces random Damerauslips according to an observed istribution (Pol-lock, 1983), using confusion matrices where ap-propriate (Kernighan, 1990).
Assembled ata in-cludes the Birkbeck corpus (Mitton, 1986) andmultifarious misspelling lists (without context).Suggestions have been made to look for low fre-quency words in corpora and news/mail archives,and to the Longmans learner corpus (not nativespeakers).AcknowledgementsThanks to all who offered advice on finding data,and to Doug Mcllroy, Sue Blackwell and NeilRowe for sending me their misspelling lists.This work is supported by a British Tele-com Scholarship, administered by the CambridgeCommonwealth Trust in conjunction with the For-eign and Commonwealth Office.ReferencesHiyan Alshawi.
1992.
The Core Language Engine.Cambridge, Massachusetts: The MIT Press.Fred J. Damerau.
1964.
"A Technique for Com-puter Detection and Correction of Spelling Er-rors",Roger Garside, Geoffrey Leech and GeoffreySampson, eds.
1987.
The Computational Anal-ysis of English.
Longman.
Commun.
A CM,7(3):171-176.Claire Grover, John Carroll and Ted Briscoe.1993.
"The Alvey Natural Language ToolsGrammar (4th Release)", Tech.
Rep. 284, Com-puter Lab, University of Cambridge.Mark D. Kernighan, Kenneth W. Church andWilliam A. Gale.
1990.
"A Spelling CorrectionProgram Based on a Noisy Channel Model",Proc.
Coling-90, pp 205-210.Roger Mitton, ed.
1986.
A Col-lection of Computer-Readable Corpora of En-glish Spelling Errors (vet.
2).
Birkbeek College,University of London.Joseph J. Pollock and Antonio Zamora.
1983.
"Collection and Characterization of SpellingErrors in Scientific and Scholarly Text", J. Am.Soc.
Inf.
Sci., 34(1):51-58.299Stephen G. Pulman and Mark R. Hepple.
1993.
"Afeature-based formalism for two-level phonol-ogy: a description and implementation", Com-pufer Speech and Language, 7(4):333-358.Edward M. Riseman and Allen R. Hanson.
1974.
"A Contextual Postprocessing System for Er-ror Correction Using Binary n-Grams", IEEETrans.
Comput, C-23(5):480-493.300
