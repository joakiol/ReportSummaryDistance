Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1069?1080, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsFast Large-Scale Approximate Graph Construction for NLPAmit Goyal and Hal Daume?
IIIDept.
of Computer ScienceUniversity of MarylandCollege Park, MD{amit,hal}@umiacs.umd.eduRaul GuerraDept.
of Computer ScienceUniversity of MarylandCollege Park, MDrguerra@cs.umd.eduAbstractMany natural language processing problemsinvolve constructing large nearest-neighborgraphs.
We propose a system called FLAGto construct such graphs approximately fromlarge data sets.
To handle the large amountof data, our algorithm maintains approximatecounts based on sketching algorithms.
Tofind the approximate nearest neighbors, ouralgorithm pairs a new distributed online-PMIalgorithm with novel fast approximate near-est neighbor search algorithms (variants ofPLEB).
These algorithms return the approxi-mate nearest neighbors quickly.
We show oursystem?s efficiency in both intrinsic and ex-trinsic experiments.
We further evaluate ourfast search algorithms both quantitatively andqualitatively on two NLP applications.1 IntroductionMany natural language processing (NLP) prob-lems involve graph construction.
Examples in-clude constructing polarity lexicons based on lexi-cal graphs from WordNet (Rao and Ravichandran,2009), constructing polarity lexicons from web data(Velikovich et al 2010) and unsupervised part-of-speech tagging using label propagation (Das andPetrov, 2011).
The later two approaches con-struct nearest-neighbor graphs between word pairsby computing nearest neighbors between word pairsfrom large corpora.
These nearest neighbors formthe edges of the graph, with weights given by thedistributional similarity (Turney and Pantel, 2010)between terms.
Unfortunately, computing the distri-butional similarity between all words in a large vo-cabulary is computationally and memory intensivewhen working with large amounts of data (Pantel etal., 2009).
This bottleneck is typically addressed bymeans of commodity clusters.
For example, Pantelet al(2009) compute distributional similarity be-tween 500 million terms over a 200 billion words in50 hours using 100 quad-core nodes, explicitly stor-ing a similarity matrix between 500 million terms.In this work, we propose Fast Large-Scale Ap-proximate Graph (FLAG) construction, a sys-tem that constructs a fast large-scale approximatenearest-neighbor graph from a large text corpus.
Tobuild this system, we exploit recent developmentsin the area of approximation, randomization andstreaming for large-scale NLP problems (Ravichan-dran et al 2005; Goyal et al 2009; Levenberg etal., 2010).
More specifically we exploit work on Lo-cality Sensitive Hashing (LSH) (Charikar, 2002) forcomputing word-pair similarities from large text col-lections (Ravichandran et al 2005; Van Durme andLall, 2010).
However, Ravichandran et al(2005)approach stored an enormous matrix of all uniquewords and their contexts in main memory, which isinfeasible for very large data sets.
A more efficientonline framework to locality sensitive hashing (VanDurme and Lall, 2010; Van Durme and Lall, 2011)computes distributional similarity in a streaming set-ting.
Unfortunately, their approach can handle onlyadditive features like raw-counts, and not non-linearassociation scores like pointwise mutual information(PMI), which generates better context vectors fordistributional similarity (Ravichandran et al 2005;Pantel et al 2009; Turney and Pantel, 2010).In FLAG, we first propose a novel distributedonline-PMI algorithm (Section 3.1).
It is a stream-ing method that processes large data sets in one passwhile distributing the data over commodity clusters1069and returns context vectors weighted by pointwisemutual information (PMI) for all the words.
Ourdistributed online-PMI algorithm makes use of theCount-Min (CM) sketch algorithm (Cormode andMuthukrishnan, 2004) (previously shown effectivefor computing distributional similarity in our ear-lier work (Goyal and Daume?
III, 2011)) to store thecounts of all words, contexts and word-context pairsusing only 8GB of main memory.
The main motiva-tion for using the CM sketch comes from its linear-ity property (see last paragraph of Section 2) whichmakes CM sketch to be implemented in distributedsetting for large data sets.
In our implementation,FLAG scaled up to 110 GB of web data with 866million sentences in less than 2 days using 100 quad-core nodes.
Our intrinsic and extrinsic experimentsdemonstrate the effectiveness of distributed online-PMI.After generating context vectors from distributedonline-PMI algorithm, our goal is to use them to findfast approximate nearest neighbors for all words.
Toachieve this goal, we exploit recent developments inthe area of existing randomized algorithms for ran-dom projections (Achlioptas, 2003; Li et al 2006),Locality Sensitive Hashing (LSH) (Charikar, 2002)and improve on previous work done on PLEB (PointLocation in Equal Balls) (Indyk and Motwani, 1998;Charikar, 2002).
We propose novel variants of PLEBto address the issue of reducing the pre-processingtime for PLEB.
One of the variants of PLEB (FAST-PLEB) with considerably less pre-processing timehas effectiveness comparable to PLEB.
We evaluatethese variants of PLEB both quantitatively and qual-itatively on large data sets.
Finally, we show the ap-plicability of large-scale graphs built from FLAG ontwo applications: the Google-Sets problem (Ghahra-mani and Heller, 2005), and learning concrete andabstract words (Turney et al 2011).2 Count-Min sketchThe Count-Min (CM) sketch (Cormode andMuthukrishnan, 2004) belongs to a class of ?sketch?algorithms that represents a large data set with acompact summary, typically much smaller than thefull size of the input by processing the data inone pass.
The following surveys comprehensivelyreview the streaming literature (Rusu and Dobra,2007; Cormode and Hadjieleftheriou, 2008) andsketch techniques (Charikar et al 2004; Li et al2008; Cormode and Muthukrishnan, 2004; Rusuand Dobra, 2007).
In our another recent paper(Goyal et al 2012), we conducted a systematicstudy and compare many sketch techniques whichanswer point queries with focus on large-scale NLPtasks.
In that paper, we empirically demonstratedthat CM sketch performs the best among all thesketches on three large-scale NLP tasks.CM sketch uses hashing to store the approximatefrequencies of all items from the large data set onto asmall sketch vector that can be updated and queriedin constant time.
CM has two parameters  and ?
: controls the amount of tolerable error in the returnedcount and ?
controls the probability with which theerror exceeds the bound .CM sketch with parameters (,?)
is representedas a two-dimensional array with width w and depthd; where w and d depends on  and ?
respectively.We set w=2 and d=log(1?
).
The depth d denotesthe number of pairwise-independent hash functionsemployed by the CM sketch; and the width w de-notes the range of the hash functions.
Given aninput stream of items of length N (x1, x2 .
.
.
xN ),each of the hash functions hk:{x1, x2 .
.
.
xN} ?
{1 .
.
.
w},?1 ?
k ?
d, takes an item from the in-put stream and maps it into a position indexed by thecorresponding hash function.UPDATE: For each new item ?x?
with count c, thesketch is updated as:sketch[k, hk(x)]?
sketch[k, hk(x)]+c, ?1 ?
k ?
d.QUERY: Since multiple items can be hashed to thesame index for each row of the array, hence thestored frequency in each row is guaranteed to over-estimate the true count, which makes it a biased esti-mator.
Therefore, to answer the point query (QUERY(x)), CM returns the minimum over all the d posi-tions indexed by the hash functions.c?
(x) = mink sketch[k, hk(x)], ?1 ?
k ?
d.All reported frequencies by CM exceed the truefrequencies by at most N with probability of atleast 1 ?
?.
The space used by the algorithm isO(1 log1?
).
Constant time of O(log(1? ))
per eachupdate and query operation.CM sketch has a linearity property which statesthat: Given two sketches s1 and s2 computed (us-1070ing the same parameters w and d, and the same setof d hash functions) over different input streams; thesketch of the combined data stream can be easily ob-tained by adding the individual sketches in O(d?w)time which is independent of the stream size.
Thisproperty enables sketches to be implemented in dis-tributed setting, where each machine computes thesketch over a small portion of the corpus and makesit scalable to large datasets.The idea of conservative update (Estan and Vargh-ese, 2002) is to only increase counts in the sketchby the minimum amount needed to ensure that theestimate remains accurate.
We (Goyal and Daume?III, 2011) used CM sketch with conservative update(CM-CU sketch) to show that the update reducesthe amount of over-estimation error by a factor ofat least 1.5 on NLP data and showed the effective-ness of CM-CU on three important NLP tasks.
TheQUERY procedure for CM-CU is identical to Count-Min.
However, to UPDATE an item ?x?
with fre-quency c, first we compute the frequency c?
(x) of thisitem from the existing data structure:(?1 ?
k ?
d, c?
(x) = mink sketch[k, hk(x)])and the counts are updated according to:sketch[k, hk(x)]?
max{sketch[k, hk(x)], c?
(x) + c}.The intuition is that, since the point query returnsthe minimum of all the d values, we will updatea counter only if it is necessary as indicated bythe above equation.
This heuristic avoids theunnecessary updating of counter values to reducethe over-estimation error.3 FLAG: Fast Large-Scale ApproximateGraph ConstructionWe describe a system, FLAG, for generating a near-est neighbor graph from a large corpus.
For ev-ery node (word), our system returns top l approxi-mate nearest neighbors, which implicitly defines thegraph.
Our system operates in four steps.
First, forevery word ?z?, our system generates a sparse con-text vector (?
(c1, v1); (c2, v2) .
.
.
; (cd, vd)?)
of sized where cd denotes the context and vd denotes thePMI (strength of association) between the contextcd and the word ?z?.
The context can be lexical,semantic, syntactic, and/or dependency units thatco-occur with the word ?z?.
We compute this ef-ficiently using a new distributed online PointwiseMutual Information algorithm (Section 3.1).
Sec-ond, we project all the words with context vectorsize d onto k random vectors and then binarize theserandom projection vectors (Section 3.2).
Third, wepropose novel variants of PLEB (Section 3.3) withless pre-processing time to represent data for fastquery retrieval.
Fourth, using the output of vari-ants of PLEB, we generate a small set of potentialnearest neighbors for every word ?z?
(Section 3.4).From this small set, we can compute the Hammingdistance between every word ?z?
and its potentialnearest neighbors to return the l nearest-neighborsfor all unique words.3.1 Distributed online-PMIWe propose a new distributed online Pointwise Mu-tual Information (PMI) algorithm motivated by theonline-PMI algorithm (Van Durme and Lall, 2009b)(page 5).
This is a streaming algorithm which pro-cesses the input corpus in one pass.
After onepass over the data set, it returns the context vec-tors for all query words.
The original online-PMIalgorithm was used to find the top-d verbs for aquery verb using the highest approximate online-PMI values using a Talbot-Osborne-Morris-Bloom1(TOMB) Counter (Van Durme and Lall, 2009a).Unfortunately, this algorithm is prohibitively slowwhen computing contexts for all words, rather thanjust a small query set.
This motivates us to proposea distributed variant that enables us to scale to largedata and large vocabularies.2We make three modifications to the originalonline-PMI algorithm and refer to it as the ?modifiedonline-PMI algorithm?
shown in Algorithm 1.
First,we use Count-Min with conservative update (CM-CU) sketch (Goyal and Daume?
III, 2011) instead ofTOMB.
We prefer CM because it enables distribu-tion due to its linearity property (Section 2) and foot-note #1.
Distribution using TOMB is not known inliterature and we will like to explore that direction infuture.
Second, we store the counts of words (?z?
),contexts (?y?)
and word-context pairs all together in1TOMB is a variant of CM sketch which focuses on reduc-ing the bit size of each counter (in addition to the number ofcounters) at the cost of incurring more error in the counts.2The serialized online-PMI algorithm took a week to gener-ate context vectors for all the words from GW (Section 4.1).1071Algorithm 1 Modified online-PMIRequire: Data set D, buffer size BEnsure: context vectors V , mapping word z to d-bestcontexts in priority queue ?
y,PMI(z, y)?1: initialize CM-CU sketch to store approximate countsof words, context and word-context pairs2: for each buffer B in the data set D do3: initialize S to store ?z,y?
observed in B4: for ?z,y?
in B do5: set S (?z,y?)
=16: insert z, y and pair ?z,y?
in sketch7: end for8: for x in set S do9: recompute vectors V(x) using current contextsin priority queue and {y|S(?z,y?
)=1}10: end for11: end for12: return context vectors Vthe CM-CU sketch (in the original online-PMI al-gorithm, exact counts of words and contexts werestored in a hash table; only the pairs were stored inthe TOMB data structure).
Third, in the original al-gorithm, for each ?z?
a vector of top-d contexts aremodified at the end of each buffer (refer Algorithm1).
However, in our algorithm, we only modify thelist of those ?z?
?s which appeared in the recent bufferrather than modifying for all the ?z?
?s (Note, if ?z?does not appear in the recent buffer, then its top-dcontexts cannot be changed.
Hence, we only modifythose ?z?s which appear in the recent buffer).In our distributed online-PMI algorithm, first wesplit the data into chunks of 10 million sentences.Second, we run the modified online-PMI algorithmon each chunk in distributed setting.
This storescounts of all words (?z?
), contexts (?y?)
and word-context pairs in the CM-CU sketch, and store top-dcontexts for each word in priority queues.
In thirdstep, we merge all the sketches using linearity prop-erty to sum the counts of the words, contexts andword-context pairs.
Additionally we merge the listsof top-d contexts for each word.
In the last step, weuse the single merged sketch and merged top-d con-texts list to generate the final distributed online-PMItop-d contexts list.It takes around one day to compute context vec-tors for all the words from a chunk of 10 millionsentences using first step of distributed online-PMI.We generated context vectors for all the 87 chunks(110 GB data with 866 million sentences: see Table1) in one day by running one process per chunk overa cluster.
The first step of the algorithm involvestraversing the data set and is the most time intensivestep.
For the second step, the merging of sketches isfast, since sketches are two dimensional array datastructures (we used the sketch of size 2 billion coun-ters with 3 hash functions).
Merging the lists of top-d contexts for each word is embarrassingly paralleland fast.
The last step to generate the final top-dcontexts list is again embarrassingly parallel and fastand takes couple of hours to generate the top-d con-texts for all the words from all the chunks.
If im-plemented serially the ?modified online-PMI algo-rithm?
on 110 GB data with 866 million sentenceswould take approximately 3 months.The downside of the distributed online-PMI is thatit splits the data into small chunks and loses infor-mation about the global best contexts for a wordover all the chunks.
The algorithm locally computesthe best contexts for each chunk, that can be bad ifthe algorithm misses out globally good contexts andthat can affect the accuracy of downstream applica-tion.
We will demonstrate in our experiments (Sec-tion 4.2) by using distributed online-PMI, we do notlose any significant information about global con-texts and perform comparable to offline-PMI overan intrinsic and extrinsic evaluation.3.2 Dimensionality Reduction from RD to RkWe are given context vectors for Z words, our goalis to use k random projections to project the con-text vectors from RD to Rk.
There are total Dunique contexts (D >> k) for all Z words.
Let(?
(c1, v1); (c2, v2) .
.
.
; (cd, vd)?)
be sparse contextvectors of size d for Z words.
For each word, we usehashing to project the context vectors onto k direc-tions.
We use k pairwise independent hash functionsthat maps each of the d context (cd) dimensions onto?d,k ?
{?1,+1}; and compute inner product be-tween ?d,k and vd.
Next, ?k,?d ?d,k.vd returns thek random projections for each word ?z?.
We storethe k random projections for all words (mapped tointegers) as a matrix A of size of k ?
Z.The mechanism described above generates ran-dom projections by implicitly creating a randomprojection matrix from a set of {?1,+1}.
Thisidea of creating implicit random projection matrix1072????
?1 2 ?
?
?
Zk1 ?z1, 26?
?z2, 80?
?
?
?
?zZ , 3?k2 ?z1,?28?
?z2, 6?
?
?
?
?zZ , 111?...
... ... .
.
.
...kK ?z1, 78?
?z2, 69?
?
?
?
?zZ , 92??????Sort=?
(a) Matrix A????
?Smallest to Largest?zZ , 3?
?z1, 26?
?
?
?
?zm, 700??zr,?50?
?z2, 6?
?
?
?
?zZ , 111?...
... .
.
.
...?z1, 78?
?zZ , 92?
?
?
?
?zu, 432???????
(b) Matrix A????
?1 2 ?
?
?
ZzZ z1 ?
?
?
zmzr z2 ?
?
?
zZ... ... .
.
.
...z1 zZ ?
?
?
zu??????
(c) Matrix A???
?z1 z2 ?
?
?
zZ2 60 ?
?
?
155 2 ?
?
?
Z... ... .
.
.
...1 90 ?
?
?
2????
(d) Matrix CFigure 1: First matrix pairs the words 1 ?
?
?Z and their random projection values.
Second matrix sorts each row by the randomprojection values from smallest to largest.
Third matrix throws away the projection values leaving only the words.
Fourth matrixmaps the words 1 ?
?
?Z to their sorted position in the third matrix for each k. This allows constant query time for all the words.is motivated by the work on stable random projec-tions (Li et al 2006; Li et al 2008), Count sketch(Charikar et al 2004), feature hashing (Weinbergeret al 2009) and online Locality Sensitive Hashing(LSH) (Van Durme and Lall, 2010).
The idea of gen-erating random projections from the set {?1,+1}was originally proposed by Achlioptas (2003).Next we create a binary matrix B using matrixA by taking sign of each of the entries of the ma-trix A.
If A(i, j) ?
0, then B(i, j) = 1; elseB(i, j) = 0.
This binarization creates Locality Sen-sitive Hash (LSH) function that preserves the cosinesimilarity between every pair of word vectors.
Thisidea was first proposed by Charikar (2002) and usedin NLP for large-scale noun clustering (Ravichan-dran et al 2005).
However, in large-scale nounclustering work, their approach had to store the ran-dom projection matrix of size D ?
k; where D de-notes the number of all unique contexts (which isgenerally large and D >> Z) and in this paper, wedo not explicitly require storing a random projectionmatrix.3.3 Representation for Fast-SearchWe describe three approaches to represent the data(matrix A and B from Section 3.2) in such a mannerthat finding nearest neighbors is fast.
These threeapproaches differ in amount of pre-processing time.First, we propose a naive baseline approach usingrandom projections independently with the best pre-processing time.
Second, we describe PLEB (PointLocation in Equal Balls) (Indyk and Motwani, 1998;Charikar, 2002) with the worst pre-processing time.Third, we propose a variant of PLEB to reduce itspre-processing time.3.3.1 Independent Random Projections (IRP)Here, we describe a naive baseline approach toarrange nearest neighbors next to each other by us-ing Independent Random Projections (IRP).
In thisapproach, we pre-process the matrix A.
First formatrix A, we pair the words z1 ?
?
?
zZ and their ran-dom projection values as shown in Fig.
1(a).
Sec-ond, we sort the elements of each row of matrix Aby their random projection values from smallest tolargest (shown in Fig.
1(b)).
The sorting step takesO(ZlogZ) time (We can assume k to be a constant).The sorting operation puts all the nearest neighborwords (for each k independent projections) next toeach other.
After sorting the matrix A, we throwaway the projection values leaving only the words(see Fig.
1(c)).
To search for a word in matrix Ain constant time, we create another matrix C of size(k ?
Z) (see Fig.
1(d)).
Matrix C maps the wordsz1 ?
?
?
zZ to their sorted position in the matrix A (seeFig.
1(c)) for each k.3.3.2 PLEBPLEB (Point Location in Equal Balls) was firstproposed by Indyk and Motwani (1998) and furtherimproved by Charikar (2002).
The improved PLEBalgorithm puts in operation all k random projectionstogether.
It randomly permutes the ordering of k bi-nary LSH bits (stored in matrix B) for all the wordsp times.
For each permutation it sorts all the wordslexicographically based on their permuted LSH rep-resentation of size k. The sorting operation puts allthe nearest neighbor words (using k projections to-gether) next to each other for all the permutations.In practice p is generally large, Ravichandran et al(2005) used p = 1000 in their work.In our implementation of PLEB, we have a matrixA of size (p ?
Z) similar to the first matrix in Fig.1(a).
The main difference to the first matrix in Fig.1(a) is that bit vectors of size k are used for sortingrather than using scalar projection values.
Similar toFig.
1(c) after sorting, bit vectors are discarded and1073a matrix C of size (p?
Z) is used to map the words1 ?
?
?Z to their sorted position in the matrixA.
Note,in IRP approach, the size of A and C matrix is (k ?Z).
In PLEB generating random permutations andsorting the bit vectors of size k involves worse pre-processing time than using IRP.
However, spendingmore time in pre-processing leads to finding betterapproximate nearest neighbors.3.3.3 FAST-PLEBTo reduce the pre-processing time for PLEB, wepropose a variant of PLEB (FAST-PLEB).
In PLEB,while generating random permutations, it uses allthe k bits.
In this variant, for each random permu-tation we randomly sample without replacement q(q << k) bits out of k. We use q bits to repre-sent each permutation and sort based on these q bits.This makes pre-processing faster for PLEB.
Section4.3 shows that FAST-PLEB only needs q = 10 toperform comparable to PLEB with q = 3000 (thatmakes FAST-PLEB 300 times faster than PLEB).Here, again we store matrices A and C of size(p?
Z).3.4 Finding Approximate Nearest NeighborsThe goal here is to exploit three representations dis-cussed in Section 3.3 to find approximate nearestneighbors quickly.
For all the three methods (IRP,PLEB, FAST-PLEB), we can use the same fast ap-proximate search which is simple and fast.
To searcha word ?z?, first, we can look up matrix C to locatethe k positions where ?z?
is stored in matrix A. Thiscan be done in constant time (Again assuming k (forIRP) and p (for PLEB and FAST-PLEB) to be a con-stant.).
Once, we find ?z?
in each row, we can selectb (beam parameter) neighbors (b/2 neighbors fromleft and b/2 neighbors from right of the query word.
)for all the k or p rows.
This can be done in constanttime (Assuming k, p and b to be constants.).
Thissearch procedure produces a set of bk (IRP) or bp(PLEB and FAST-PLEB) potential nearest neighborsfor a query word ?z?.
Next, we compute Hammingdistance between query word ?z?
and the set of po-tential nearest neighbors from matrix B to return lclosest nearest neighbors.
For computing hammingdistance, all the approaches discussed in Section 3.3require all k random projection bits.4 ExperimentsWe evaluate our system FLAG for fast large-scaleapproximate graph construction.
First, we show thatusing distributed online-PMI algorithm is as effec-tive as offline-PMI.
Second, we compare the approx-imate nearest neighbors lists generated by FLAGagainst the exact nearest neighbor lists.
Finally, weshow the quality of our approximate similarity listsgenerated by FLAG from the web corpus.4.1 Experimental SetupData sets: We use two data sets: Gigaword (Graff,2003) and a copy of news web (Ravichandran etal., 2005).
For both the corpora, we split the textinto sentences, tokenize and convert into lower-case.To evaluate our approximate graph construction, weevaluate on three data sets: Gigaword (GW), Giga-word + 50% of web data (GWB50) and Gigaword+ 100% ((GWB100)) of web data.
Corpus statisticsare shown in Table 1.
We define the context for agiven word ?z?
as the surrounding words appearingin a window of 2 words to the left and 2 words tothe right.
The context words are concatenated alongwith their positions -2, -1, +1, and +2.Corpus GW GWB50 GWB100Unzipped12 60 110Size (GB)# of sentences57 463 866(Million)# of tokens2.1 10.9 20.0(Billion)Table 1: Corpus Description4.2 Evaluating Distributed online-PMIExperimental Setup: First we do an intrinsicevaluation to quantitatively evaluate the distributedonline-PMI vectors against the offline-PMI vectorscomputed from Gigaword (GW).
Offline-PMI com-puted from the sketches have been shown as effec-tive as exact PMI by Goyal and Daume?
III (2011).To compute offline-PMI vectors, we do two passesover the corpus.
In the first pass, we store the countsof words, contexts and word-context pairs computedfrom GW in the Count-Min with conservative up-date (CM-CU) sketch.
We use the CM-CU sketchof size 2 billion counters (bounded 8 GB memory)1074with 3 hash functions.
In second pass, using theaggregated counts from the sketch, we generate theoffline-PMI vectors of size d = 1000 for every word.For rest of this paper for distributed online-PMI, weset d = 1000 and the size of the buffer=10, 000 andwe split the data sets into small chunks of 10 millionsentences.Intrinsic Evaluation: We use four kinds of mea-sures: precision (P), recall (R), f-measure (F1) andPearson?s correlation (?)
to measure the overlap inthe context vectors obtained using online and offlinePMI.
?
is computed between contexts that are foundin offline and online context vectors.
We do thisevaluation on 447 words selected from the concate-nation of four test-sets mentioned in the next para-graph.
On these 447 words, we achieve an average Pof .97, average R of .96 and average F1 of .97 and aperfect average ?
of 1.
This evaluation show that thevectors obtained using online-PMI are as effectiveas offline-PMI.Extrinsic Evaluation: We also compare online-PMI effectiveness on four test sets which consist ofword pairs, and their corresponding human rank-ings.
We generate the word pair rankings usingonline-PMI and offline-PMI strategies.
We reportthe Pearson?s correlation (?)
between the human andsystem generated similarity rankings.
The four testsets are: WS-353 (Finkelstein et al 2002) is a setof 353 word pairs.
WS-203: A subset of WS-353with 203 word pairs (Agirre et al 2009).
RG-65:(Rubenstein and Goodenough, 1965) has 65 wordpairs.
MC-30: A subset of RG-65 dataset with 30word pairs (Miller and Charles, 1991).The results in Table 2 shows that by using dis-tributed online-PMI (by making a single pass overthe corpus) is comparable to offline-PMI (which iscomputed by making two passes over the corpus).For generating context vectors from GW, for bothoffline-PMI and online-PMI, we use a frequencycutoff of 5 for word-context pairs to throw away therare terms as they are sensitive to PMI (Church andHanks, 1989).
Next, FLAG generates online-PMIvectors from GWB50 and GWB100 and uses fre-quency cutoffs of 15 and 25.
The higher frequencycutoffs are selected based on the intuition that, withmore data, we get more noise, and hence not con-sidering word-context pairs with frequency less than25 will be better for the system.
As FLAG is go-ing to use the context vectors to find nearest neigh-bors, we also throw away all those words which have?
50 contexts associated with them.
This generatescontext vectors for 57, 930 words from GW; 95, 626from GWB50 and 106, 733 from GWB100.Test Set WS-353 WS-203 RG-65 MC-30Offline-PMI .41 .55 .40 .52Online-PMI .41 .56 .39 .51Table 2: Evaluating word pairs ranking with online and offlinePMI.
Scores are evaluated using ?
metric.10 25 50 100R ?
R ?
R ?
R ?IRP .40 .53 .38 .51 .35 .54 .34 .51q=1 .24 .62 .20 .63 .18 .59 .17 .54q=5 .47 .60 .43 .57 .40 .57 .37 .53q=10 .53 .58 .49 .56 .45 .55 .42 .53q=100 .53 .60 .50 .59 .46 .56 .43 .53q=3000 .54 .58 .50 .59 .46 .56 .43 .54Table 4: Varying parameter q for FAST-PLEB with fixed p =1000, k = 3000 and b = 40.
Results reported on recall and ?.4.3 Evaluating Approximate Nearest NeighborExperimental Setup: To evaluate approximatenearest neighbor similarity lists generated byFLAG, we conduct three experiments.
We evaluateall the three experiments on 447 words (test set) asused in Section 4.2.
For each word, both exact andapproximate methods return l = 100 nearest neigh-bors.
The exact similarity lists for 447 test words iscomputed by calculating cosine similarity between447 test words with respect to all other words.
Wealso compare the LSH (computed using Hammingdistance between all words and test set.)
approxi-mate nearest neighbor similarity lists against the ex-act similarity lists.
LSH provides an upper boundon the performance of our approximate search rep-resentations (IRP, PLEB, and FAST-PLEB) for fast-search from Section 3.3) .
We set the number ofprojections k = 3000 for all three methods and forPLEB and FAST-PLEB, we set number of permuta-tions p = 1000 as used in large-scale noun cluster-ing work (Ravichandran et al 2005).Evaluation Metric: We use two kinds of mea-sures, recall and Pearson?s correlation to measurethe overlap in the approximate and exact similaritylists.
Intuitively, recall (R) captures the number of1075IRP PLEB FAST-PLEB10 25 50 100 10 25 50 100 10 25 50 100R ?
R ?
R ?
R ?
R ?
R ?
R ?
R ?
R ?
R ?
R ?
R ?LSH .55 .57 .52 .56 .49 .54 .46 .52 .55 .57 .52 .56 .49 .54 .46 .52 .55 .57 .52 .56 .49 .54 .46 .5220 .29 .50 .26 .55 .25 .54 .24 .50 .50 .59 .45 .60 .41 .57 .37 .55 .48 .58 .42 .58 .38 .58 .35 .5530 .36 .55 .33 .56 .31 .55 .30 .52 .53 .59 .48 .59 .44 .56 .41 .54 .51 .57 .47 .57 .42 .56 .40 .5440 .40 .53 .38 .51 .35 .54 .34 .51 .54 .58 .50 .59 .46 .56 .43 .54 .53 .58 .49 .56 .45 .55 .42 .5350 .44 .56 .42 .54 .39 .54 .37 .52 .54 .58 .51 .57 .47 .56 .44 .53 .54 .58 .50 .56 .46 .55 .44 .53100 .53 .59 .49 .54 .46 .55 .43 .53 .55 .56 .52 .56 .48 .54 .46 .53 .55 .57 .52 .56 .48 .54 .46 .53Table 3: Evaluation results on comparing LSH, IRP, PLEB, and FAST-PLEB with k = 3000 and b = {20, 30, 40, 50, 100} withexact nearest neighbors over GW data set.
For PLEB and FAST-PLEB, we set p = 1000 and for FAST-PLEB, we set q = 10.
Wereport results on recall (R) and ?
metric.
For IRP, we sample first p rows and only use p rows rather than k.GW GWB50 GWB10010 25 50 100 10 25 50 100 10 25 50 100R ?
R ?
R ?
R ?
R ?
R ?
R ?
R ?
R ?
R ?
R ?
R ?LSH .55 .57 .52 .56 .49 .54 .46 .52 .51 .55 .46 .54 .44 .52 .42 .48 .48 .58 .45 .52 .42 .49 .40 .47IRP .40 .53 .37 .53 .35 .54 .34 .51 .29 .50 .27 .51 .25 .51 .24 .47 .26 .57 .24 .49 .23 .48 .22 .45PLEB .54 .58 .50 .59 .46 .56 .43 .54 .46 .58 .42 .56 .38 .53 .36 .51 .44 .57 .40 .56 .36 .52 .33 .49FAST-PLEB .53 .58 .49 .56 .45 .55 .42 .53 .46 .56 .41 .56 .37 .54 .35 .51 .43 .57 .38 .55 .35 .52 .32 .50Table 5: Evaluation results on comparing LSH, IRP, PLEB, and FAST-PLEB with k = 3000, b = 40, p = 1000 and q = 10 withexact nearest neighbors across three different data sets: GW, GWB50, and GWB100.
We report results on recall (R) and ?
metric.The gray color row is the system that we use for further evaluations.nearest neighbors that are found in both the lists andthen Pearson?s (?)
correlation captures if the rela-tive order of these lists is preserved in both the sim-ilarity lists.
We also compute R and ?
at variousl = {10, 25, 50, 100}.Results: For the first experiment, we evaluateIRP, PLEB, and FAST-PLEB against the exact near-est neighbor similarity lists.
For IRP, we samplefirst p rows and only use p rather than k, this en-sures that all the three methods (IRP, PLEB, andFAST-PLEB) take the same query time.
We varythe approximate nearest neighbor beam parameterb = {20, 30, 40, 50, 100} that controls the numberof closest neighbors for a word with respect to eachindependent random projection.
Note, with increas-ing b, our algorithm approaches towards LSH (com-puting Hamming distance with respect to all thewords).
For FAST-PLEB, we set q = 10 (q << k)that is the number of random bits selected out of k togenerate p permuted bit vectors of size q.
The resultsare reported in Table 3, where the first row com-pares the LSH approach against the exact similar-ity list for test set words.
Across three columns wecompare IRP, PLEB, and FAST-PLEB.
For all meth-ods, increasing b means better recall.
If we movedown the table, with b = 100, IRP, PLEB, and FAST-PLEB get results comparable to LSH (reaches an up-per bound).
However, using large b implies gener-ating a long potential nearest neighbor list close tothe size of the unique context vectors.
If we focuson the gray color row with b = 40 (This will havecomparatively small potential list and return nearestneighbors in less time), IRP has worse recall withbest pre-processing time.
FAST-PLEB (q = 10) iscomparable to PLEB (using all bits q = 3000) withpre-processing time 300 times faster than PLEB.
Forrest of this work, FLAG will use FAST-PLEB as ithas best recall and pre-processing time with fixedb = 40.For the second experiment, we vary parameterq = {1, 5, 10, 100, 3000} for FAST-PLEB in Table4.
Table 4 demonstrates using q = {1, 5} result inworse recall, however using q = 5 for FAST-PLEBis better than IRP.
q = 10 has comparable recallto q = {100, 3000}.
For rest of this work, we fixq = 10 as it has best recall and pre-processing time.For the third experiment, we increase the size ofthe data set across the Table 5.
With the increasein size of the data set, LSH, IRP, PLEB, and FAST-PLEB (q = 10) have worse recall.
The reason forsuch a behavior is that the number of unique contextvectors is greater for big data sets.
Across all the1076jazz yale soccer physics wednesdayreggae harvard basketball chemistry tuesdayrockabilly cornell hockey mathematics thursdayrock fordham lacrosse biology mondaybluegrass rutgers handball biochemistry fridayindie dartmouth badminton science saturdaybaroque nyu softball microbiology sundayska ucla football geophysics yesterdayfunk princeton tennis economics tuesbanjo stanford wrestling psychology octoberblues loyola rugby neuroscience weekTable 6: Sample Top 10 similarity lists returned by FAST-PLEBwith k = 3000, p = 1000, b = 40 and q = 10 from GWB100.three data sets, FAST-PLEB has recall comparable toPLEB with best pre-processing time.
Hence, for thenext evaluation to show the quality of final lists weuse FAST-PLEB with q = 10 for GWB100 data set.In Table 6, we list the top 10 most similar wordsfor some words found by our system FLAG usingGWB100 data set.
Even though FLAG?s approxi-mate nearest neighbor algorithm has less recall withrespect to exact but still the quality of these nearestneighbor lists is excellent.For the final experiment, we demonstrate the pre-processing and query time results comparing LSH,IRP, PLEB, and FAST-PLEB with k = 3000, p =1000, b = 40 and q = 10 parameter settings.
Forpre-processing timing results, we perform all the ex-periments (averaged over 5 runs) on GWB100 dataset with 106, 733 words.
The second pre-processingstep of the system FLAG (Section 3.2) that is di-mensionality reduction from RD to Rk took 8.8hours.
The pre-processing time differences amongIRP, PLEB, and FAST-PLEB from third step (Section3.3) are shown in second column of Table 7.
Ex-perimental results show that the naive baseline IRPis the fastest and FAST-PLEB has 120 times fasterpre-processing time compared to PLEB.For comparing query time among several meth-ods, we evaluate over 447 words (Section 4.2).
Wereport average timing results (averaged over 10 runsand 447 words) to find top 100 nearest neighbors forsingle query word.
The results are shown in thirdcolumn of Table 7.
Comparing first and second rowsshow that LSH is 87 times faster than computingexact top-100 (cosine similarity) nearest neighbors.Comparing second, third, fourth and fifth rows of thetable demonstrate that IRP, PLEB and FAST-PLEBMethods Preprocessing Query (seconds)Exact n/a 87LSH 8.8 hours 0.59IRP 7.5 minutes 0.28PLEB 1.8 days 0.28FAST-PLEB 22 minutes 0.26Table 7: Preprocessing and query time results compar-ing exact, LSH, IRP, PLEB, and FAST-PLEB methods onGWB100 data set.Language english chinese japanese spanish russianPlace africa america washington london pacificNationality american european french british westernDate january may december october juneOrganization ford microsoft sony disneyland googleTable 8: Query terms for Google Sets Problem evaluationmethods are twice as fast as LSH.5 ApplicationsWe use the graph constructed by FLAG fromGWB100 data set (110 GB) by applying FAST-PLEB with parameters k = 3000, p = 1000, q = 10and b = 40.
The graph has 106, 733 nodes (words),with each node having 100 edges that denote the topl = 100 approximate nearest neighbors associatedwith each node.
However, FLAG applied FAST-PLEB (approximate search) to find these neighbors.Therefore many of these edges can be noisy for ourapplications.
Hence for each node, we only considertop 10 edges.
In general for graph-based NLP prob-lems; for example, constructing web-derived polar-ity lexicons (Velikovich et al 2010), top 25 edgeswere used, and for unsupervised part-of-speech tag-ging using label propagation (Das and Petrov, 2011),top 5 edges were used.5.1 Google Sets ProblemGoogle Sets problem (Ghahramani and Heller,2005) can be defined as: given a set of query words,return top t similar words with respect to querywords.
To evaluate the quality of our approximatelarge-scale graph, we return top 25 words whichhave best aggregated similarity scores with respectto query words.
We take 5 classes and their queryterms (McIntosh and Curran, 2008) shown in Table8 and our goal is to learn 25 new words which aresimilar with these 5 query words.1077Language: german, french, estonian, hungarian, bulgarianPlace: scandinavia, mongolia, mozambique, zambia, namibiaNationality: german, hungarian, estonian, latvian, lithuanianDate: september, february, august, july, novemberOrganization: looksmart, hotbot, lycos, webcrawler, allthewebTable 9: Learned terms for Google Sets ProblemConcrete car, house, tree, horse, animalseeds man, table, bottle, woman, computerAbstract idea, bravery, deceit, trust, dedicationseeds anger, humour, luck, inflation, honestyTable 10: Example seeds for bootstrapping.We conduct a manual evaluation to directly mea-sure the quality of returned words.
We recruited 1annotator and developed annotation guidelines thatinstructed each recruiter to judge whether learnedvalues are similar to query words or not.
Overall theannotator found almost all the learned words to besimilar to the query words.
However, the algorithmcan not differentiate between different senses of theword.
For example, ?French?
can be a language anda nationality.
Table 9 shows the top ranked wordswith respect to query words.5.2 Learning Concrete and Abstract WordsOur goal is to automatically learn concrete and ab-stract words (Turney et al 2011).
We apply boot-strapping (Kozareva et al 2008) on the word graphsby manually selecting 10 seeds for concrete and ab-stract words (see Table 10).
We use in-degree (sumof weights of incoming edges) to compute the scorefor each node which has connections with known(seeds) or automatically labeled nodes, previouslyexploited to learn hyponymy relations from the web(Kozareva et al 2008).
We learn concrete and ab-stract words together (known as mutual exclusionprinciple in bootstrapping (Thelen and Riloff, 2002;McIntosh and Curran, 2008)), and each word is as-signed to only one class.
Moreover, after each it-eration, we harmonically decrease the weight of thein-degree associated with instances learned in lateriterations.
We add 25 new instances at each itera-tion and ran 100 iterations of bootstrapping, yielding2506 concrete nouns and 2498 abstract nouns.
Toevaluate our learned words, we searched in WordNetwhether they had ?abstraction?
or ?physical?
as theirhypernym.
Out of 2506 learned concrete nouns,Concrete: girl, person, bottles, wife, gentleman, mi-crocomputer, neighbor, boy, foreigner, housewives,texan, granny, bartender, tables, policeman, chubby,mature, trees, mainframe, backbone, truckAbstract: perseverance, tenacity, sincerity, profes-sionalism, generosity, heroism, compassion, commit-ment, openness, resentment, treachery, deception, no-tion, jealousy, loathing, hurry, valourTable 11: Learned concrete/abstract words.1655 were found in WordNet.
According to Word-Net, 74% of those are concrete and 26% are ab-stract.
Out of 2498 learned abstract nouns, 942 werefound in WordNet.
According to WordNet, 5% ofthose are concrete and 95% are abstract.
Table 11shows the top ranked concrete and abstract words.6 ConclusionWe proposed a system, FLAG which constructsfast large-scale approximate graphs from large datasets.
To build this system we proposed a distributedonline-PMI algorithm that scaled up to 110 GB ofweb data with 866 million sentences in less than 2days using 100 quad-core nodes.
Our both intrinsicand extrinsic experiments demonstrated that online-PMI algorithm not at all loses globally good con-texts and perform comparable to offline-PMI.
Next,we proposed FAST-PLEB (a variant of PLEB) andempirically demonstrated that it has recall compa-rable to PLEB with 120 times faster pre-processingtime.
Finally, we show the applicability of FLAG ontwo applications: Google-Sets problem and learningconcrete and abstract words.In future, we will apply FLAG to construct graphsusing several kinds of contexts like lexical, seman-tic, syntactic and dependency relations or a combi-nation of them.
Moreover, we will apply graph theo-retic models on graphs constructed using FLAG forsolving a large variety of NLP applications.AcknowledgmentsThis work was partially supported by NSF AwardIIS-1139909.
Thanks to Graham Cormode andSuresh Venkatasubramanian for useful discussionsand the anonymous reviewers for many helpful com-ments.1078ReferencesDimitris Achlioptas.
2003.
Database-friendly randomprojections: Johnson-lindenstrauss with binary coins.J.
Comput.
Syst.
Sci., 66(4):671?687.Eneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.
Astudy on similarity and relatedness using distributionaland wordnet-based approaches.
In NAACL ?09: Pro-ceedings of HLT-NAACL.Moses Charikar, Kevin Chen, and Martin Farach-Colton.2004.
Finding frequent items in data streams.
Theor.Comput.
Sci., 312:3?15, January.Moses S. Charikar.
2002.
Similarity estimation tech-niques from rounding algorithms.
In In Proc.
of 34thSTOC, pages 380?388.
ACM.K.
Church and P. Hanks.
1989.
Word Associa-tion Norms, Mutual Information and Lexicography.In Proceedings of ACL, pages 76?83, Vancouver,Canada, June.Graham Cormode and Marios Hadjieleftheriou.
2008.Finding frequent items in data streams.
In VLDB.Graham Cormode and S. Muthukrishnan.
2004.
An im-proved data stream summary: The count-min sketchand its applications.
J. Algorithms.Dipanjan Das and Slav Petrov.
2011.
Unsupervisedpart-of-speech tagging with bilingual graph-based pro-jections.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 600?609, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Cristian Estan and George Varghese.
2002.
New di-rections in traffic measurement and accounting.
SIG-COMM Comput.
Commun.
Rev., 32(4).L.
Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,Z.
Solan, G. Wolfman, and E. Ruppin.
2002.
Plac-ing search in context: The concept revisited.
In ACMTransactions on Information Systems.Zoubin Ghahramani and Katherine A. Heller.
2005.Bayesian Sets.
In in Advances in Neural InformationProcessing Systems, volume 18.Amit Goyal and Hal Daume?
III.
2011.
Approximatescalable bounded space sketch for large data NLP.
InEmpirical Methods in Natural Language Processing(EMNLP).Amit Goyal, Hal Daume?
III, and Suresh Venkatasubra-manian.
2009.
Streaming for large scale NLP: Lan-guage modeling.
In NAACL.Amit Goyal, Graham Cormode, and Hal Daume?
III.2012.
Sketch algorithms for estimating point queriesin NLP.
In Empirical Methods in Natural LanguageProcessing (EMNLP).D.
Graff.
2003.
English Gigaword.
Linguistic Data Con-sortium, Philadelphia, PA, January.Piotr Indyk and Rajeev Motwani.
1998.
Approximatenearest neighbors: towards removing the curse of di-mensionality.
In Proceedings of the thirtieth annualACM symposium on Theory of computing, STOC ?98,pages 604?613.
ACM.Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008.Semantic class learning from the web with hyponympattern linkage graphs.
In Proceedings of ACL-08:HLT, pages 1048?1056, Columbus, Ohio, June.
As-sociation for Computational Linguistics.Abby Levenberg, Chris Callison-Burch, and Miles Os-borne.
2010.
Stream-based translation models forstatistical machine translation.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, HLT ?10, pages 394?402.
As-sociation for Computational Linguistics.Ping Li, Trevor J. Hastie, and Kenneth W. Church.
2006.Very sparse random projections.
In Proceedings ofthe 12th ACM SIGKDD international conference onKnowledge discovery and data mining, KDD ?06,pages 287?296.
ACM.Ping Li, Kenneth Ward Church, and Trevor Hastie.
2008.One sketch for all: Theory and application of condi-tional random sampling.
In Neural Information Pro-cessing Systems, pages 953?960.Tara McIntosh and James R Curran.
2008.
Weightedmutual exclusion bootstrapping for domain indepen-dent lexicon and template acquisition.
In Proceedingsof the Australasian Language Technology AssociationWorkshop 2008, pages 97?105, December.G.A.
Miller and W.G.
Charles.
1991.
Contextual corre-lates of semantic similarity.
Language and CognitiveProcesses, 6(1):1?28.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity set expansion.
InProceedings of EMNLP.Delip Rao and Deepak Ravichandran.
2009.
Semi-supervised polarity lexicon induction.
In Proceedingsof the 12th Conference of the European Chapter of theACL (EACL 2009), pages 675?682, Athens, Greece,March.
Association for Computational Linguistics.Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.2005.
Randomized algorithms and nlp: using localitysensitive hash function for high speed noun clustering.In Proceedings of ACL.H.
Rubenstein and J.B. Goodenough.
1965.
Contextualcorrelates of synonymy.
Computational Linguistics,8:627?633.Florin Rusu and Alin Dobra.
2007.
Statistical analysis ofsketch estimators.
In SIGMOD ?07.
ACM.1079M.
Thelen and E. Riloff.
2002.
A Bootstrapping Methodfor Learning Semantic Lexicons Using Extraction Pat-tern Contexts.
In Proceedings of the Empirical Meth-ods in Natural Language Processing, pages 214?221.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of seman-tics.
JOURNAL OF ARTIFICIAL INTELLIGENCERESEARCH, 37:141.Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-hen.
2011.
Literal and metaphorical sense identifi-cation through concrete and abstract context.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 680?690.Association for Computational Linguistics.Benjamin Van Durme and Ashwin Lall.
2009a.
Prob-abilistic counting with randomized storage.
In IJ-CAI?09: Proceedings of the 21st international jontconference on Artifical intelligence.Benjamin Van Durme and Ashwin Lall.
2009b.
Stream-ing pointwise mutual information.
In Advances inNeural Information Processing Systems 22.Benjamin Van Durme and Ashwin Lall.
2010.
Onlinegeneration of locality sensitive hash signatures.
InProceedings of the ACL 2010 Conference Short Pa-pers, pages 231?235, July.Benjamin Van Durme and Ashwin Lall.
2011.
Efficientonline locality sensitive hashing via reservoir count-ing.
In Proceedings of the ACL 2011 Conference ShortPapers, June.Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-nan, and Ryan McDonald.
2010.
The viability of web-derived polarity lexicons.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 777?785, Los Angeles, Cal-ifornia, June.
Association for Computational Linguis-tics.Kilian Weinberger, Anirban Dasgupta, John Langford,Alex Smola, and Josh Attenberg.
2009.
Feature hash-ing for large scale multitask learning.
In Proceedingsof the 26th Annual International Conference on Ma-chine Learning, ICML ?09, pages 1113?1120.
ACM.1080
