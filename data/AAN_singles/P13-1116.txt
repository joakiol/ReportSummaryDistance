Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1180?1189,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsImpAr: A Deterministic Algorithm for Implicit Semantic Role LabellingEgoitz LaparraIXA GroupUniversity of the Basque CountrySan Sebastian, Spainegoitz.laparra@ehu.esGerman RigauIXA GroupUniversity of the Basque CountrySan Sebastian, Spaingerman.rigau@ehu.esAbstractThis paper presents a novel deterministicalgorithm for implicit Semantic Role La-beling.
The system exploits a very sim-ple but relevant discursive property, the ar-gument coherence over different instancesof a predicate.
The algorithm solves theimplicit arguments sequentially, exploit-ing not only explicit but also the implicitarguments previously solved.
In addition,we empirically demonstrate that the algo-rithm obtains very competitive and robustperformances with respect to supervisedapproaches that require large amounts ofcostly training data.1 IntroductionTraditionally, Semantic Role Labeling (SRL) sys-tems have focused in searching the fillers of thoseexplicit roles appearing within sentence bound-aries (Gildea and Jurafsky, 2000, 2002; Carrerasand Ma`rquez, 2005; Surdeanu et al, 2008; Hajic?et al, 2009).
These systems limited their search-space to the elements that share a syntactical re-lation with the predicate.
However, when the par-ticipants of a predicate are implicit this approachobtains incomplete predicative structures with nullarguments.
The following example includes thegold-standard annotations for a traditional SRLprocess:(1) [arg0 The network] had been expected to have [nplosses] [arg1 of as much as $20 million] [arg3 on base-ball this year].
It isn?t clear how much those [np losses]may widen because of the short Series.The previous analysis includes annotations forthe nominal predicate loss based on the NomBankstructure (Meyers et al, 2004).
In this case theannotator identifies, in the first sentence, the argu-ments arg0, the entity losing something, arg1, thething lost, and arg3, the source of that loss.
How-ever, in the second sentence there is another in-stance of the same predicate, loss, but in this caseno argument has been associated with it.
Tradi-tional SRL systems facing this type of examplesare not able to fill the arguments of a predicatebecause their fillers are not in the same sentenceof the predicate.
Moreover, these systems also letunfilled arguments occurring in the same sentence,like in the following example:(2) Quest Medical Inc said it adopted [arg1 a sharehold-ers?
rights] [np plan] in which rights to purchase sharesof common stock will be distributed as a dividend toshareholders of record as of Oct 23.For the predicate plan in the previous sentence,a traditional SRL process only returns the filler forthe argument arg1, the theme of the plan.However, in both examples, a reader could eas-ily infer the missing arguments from the surround-ing context of the predicate, and determine thatin (1) both instances of the predicate share thesame arguments and in (2) the missing argumentcorresponds to the subject of the verb that domi-nates the predicate, Quest Medical Inc. Obviously,this additional annotations could contribute posi-tively to its semantic analysis.
In fact, Gerber andChai (2010) pointed out that implicit argumentscan increase the coverage of argument structuresin NomBank by 71%.
However, current automaticsystems require large amounts of manually anno-tated training data for each predicate.
The effortrequired for this manual annotation explains theabsence of generally applicable tools.
This prob-lem has become a main concern for many NLPtasks.
This fact explains a new trend to developaccurate unsupervised systems that exploit sim-ple but robust linguistic principles (Raghunathanet al, 2010).In this work, we study the coherence of thepredicate and argument realization in discourse.
Inparticular, we have followed a similar approach to1180the one proposed by Dahl et al (1987) who filledthe arguments of anaphoric mentions of nominalpredicates using previous mentions of the samepredicate.
We present an extension of this ideaassuming that in a coherent document the differ-ent ocurrences of a predicate, including both ver-bal and nominal forms, tend to be mentions ofthe same event, and thus, they share the sameargument fillers.
Following this approach, wehave developed a deterministic algorithm that ob-tains competitive results with respect to supervisedmethods.
That is, our system can be applied to anypredicate without training data.The main contributions of this work are the fol-lowing:?
We empirically prove that there exists astrong discourse relationship between the im-plicit and explicit argument fillers of the samepredicates.?
We propose a deterministic approach that ex-ploits this discoursive property in order to ob-tain the fillers of implicit arguments.?
We adapt to the implicit SRL problem a clas-sic algorithm for pronoun resolution.?
We develop a robust algorithm, ImpAr, thatobtains very competitive results with respectto existing supervised systems.
We releasean open source prototype implementing thisalgorithm1.The paper is structured as follows.
Section 2discusses the related work.
Section 3 presents indetail the data used in our experiments.
Section4 describes our algorithm for implicit argumentresolution.
Section 5 presents some experimentswe have carried out to test the algorithm.
Section6 discusses the results obtained.
Finally, section7 offers some concluding remarks and presentssome future research lines.2 Related WorkThe first attempt for the automatic annotation ofimplicit semantic roles was proposed by Palmeret al (1986).
This work applied selectional restric-tions together with coreference chains, in a veryspecific domain.
In a similar approach, Whitte-more et al (1991) also attempted to solve implicit1http://adimen.si.ehu.es/web/ImpArarguments using some manually described seman-tic constraints for each thematic role they tried tocover.
Another early approach was presented byTetreault (2002).
Studying another specific do-main, they obtained some probabilistic relationsbetween some roles.
These early works agree thatthe problem is, in fact, a special case of anaphoraor coreference resolution.Recently, the task has been taken up againaround two different proposals.
On the onehand, Ruppenhofer et al (2010) presented a taskin SemEval-2010 that included an implicit argu-ment identification challenge based on FrameNet(Baker et al, 1998).
The corpus for this taskconsisted in some novel chapters.
They covereda wide variety of nominal and verbal predicates,each one having only a small number of instances.Only two systems were presented for this sub-task obtaining quite poor results (F1 below 0,02).VENSES++ (Tonelli and Delmonte, 2010) applieda rule based anaphora resolution procedure and se-mantic similarity between candidates and thematicroles using WordNet (Fellbaum, 1998).
The sys-tem was tuned in (Tonelli and Delmonte, 2011)improving slightly its performance.
SEMAFOR(Chen et al, 2010) is a supervised system thatextended an existing semantic role labeler to en-large the search window to other sentences, replac-ing the features defined for regular arguments withtwo new semantic features.
Although this systemobtained the best performance in the task, datasparseness strongly affected the results.
Besidesthe two systems presented to the task, some othersystems have used the same dataset and evaluationmetrics.
Ruppenhofer et al (2011), Laparra andRigau (2012), Gorinski et al (2013) and Laparraand Rigau (2013) explore alternative linguistic andsemantic strategies.
These works obtained signifi-cant gains over previous approaches.
Silberer andFrank (2012) adapted an entity-based coreferenceresolution model to extend automatically the train-ing corpus.
Exploiting this additional data, theirsystem was able to improve previous results.
Fol-lowing this approach Moor et al (2013) present acorpus of predicate-specific annotations for verbsin the FrameNet paradigm that are aligned withPropBank and VerbNet.On the other hand, Gerber and Chai (2010,2012) studied the implicit argument resolution onNomBank.
They uses a set of syntactic, semanticand coreferential features to train a logistic regres-1181sion classifier.
Unlike the dataset from SemEval-2010 (Ruppenhofer et al, 2010), in this work theauthors focused on a small set of ten predicates.But for those predicates, they annotated a largeamount of instances in the documents from theWall Street Journal that were already annotatedfor PropBank (Palmer et al, 2005) and NomBank.This allowed them to avoid the sparseness prob-lems and generalize properly from the trainingset.
The results of this system were far betterthan those obtained by the systems that faced theSemEval-2010 dataset.
This works represent thedeepest study so far of the features that charac-terizes the implicit arguments 2.
However, manyof the most important features are lexically depen-dent on the predicate and cannot been generalized.Thus, specific annotations are required for eachnew predicate to be analyzed.All the works presented in this section agree thatimplicit arguments must be modeled as a particu-lar case of coreference together with features thatinclude lexical-semantic information, to build se-lectional preferences.
Another common point isthe fact that these works try to solve each instanceof the implicit arguments independently, withouttaking into account the previous realizations ofthe same implicit argument in the document.
Wepropose that these realizations, together with theexplicit ones, must maintain a certain coherencealong the document and, in consequence, the fillerof an argument remains the same along the fol-lowing instances of that argument until a strongerevidence indicates a change.
We also propose thatthis feature can be exploited independently fromthe predicate.3 DatasetsIn our experiments, we have focused on the datasetdeveloped in Gerber and Chai (2010, 2012).
Thisdataset (hereinafter BNB which stands for ?Be-yond NomBank?)
extends existing predicate an-notations for NomBank and ProbBank.BNB presented the first annotation work of im-plicit arguments based on PropBank and Nom-Bank frames.
This annotation was an extensionof the standard training, development and testingsections of Penn TreeBank that have been typi-cally used for SRL evaluation and were alreadyannotated with PropBank and NomBank predicate2Gerber and Chai (2012) includes a set of 81 different fea-tures.structures.
The authors selected a limited set ofpredicates.
These predicates are all nominaliza-tions of other verbal predicates, without sense am-biguity, that appear frequently in the corpus andtend to have implicit arguments associated withtheir instances.
These constraints allowed them tomodel enough occurrences of each implicit argu-ment in order to cover adequately all the possiblecases appearing in a test document.
For each miss-ing argument position they went over all the pre-ceding sentences and annotated all mentions of thefiller of that argument.
In tables 3 and 4 we showthe list of predicates and the resulting figures ofthis annotation.In this work we also use the corpus providedfor the CoNLL-2008 task.
These corpora coverthe same BNB documents and include annotatedpredictions for syntactic dependencies and Super-Sense labels as semantic tags.
Unlike Gerber andChai (2010, 2012) we do not use the constituentanalysis from the Penn TreeBank.4 ImpAr algorithm4.1 Discoursive coherence of predicatesExploring the training dataset of BNB, we ob-served a very strong discourse effect on the im-plicit and explicit argument fillers of the predi-cates.
That is, if several instances of the samepredicate appear in a well-written discourse, it isvery likely that they maintain the same argumentfillers.
This property holds when joining the dif-ferent parts-of-speech of the predicates (nominalor verbal) and the explicit or implicit realizationsof the argument fillers.
For instance, we observedthat 46% of all implicit arguments share the samefiller with the previous instance of the same predi-cate while only 14% of them have a different filler.The remaining 40% of all implicit arguments cor-respond to first occurrences of their predicates.That is, these fillers can not be recovered from pre-vious instances of their predicates.The rationale behind this phenomena seems tobe simple.
When referring to different aspects ofthe same event, the writer of a coherent documentdoes not repeat redundant information.
They re-fer to previous predicate instances assuming thatthe reader already recalls the involved participants.That is, the filler of the different instances of apredicate argument maintain a certain discoursecoherence.
For instance, in example (1), all the ar-gument positions of the second occurrence of the1182predicate loss are missing, but they can be easilyinferred from the previous instance of the samepredicate.
(1) [arg0 The network] had been expected to have [nplosses] [arg1 of as much as $20 million] [arg3 on base-ball this year].
It isn?t clear how much those [np losses]may widen because of the short Series.Therefore, we propose to exploit this propertyin order to capture correctly how the fillers of allpredicate arguments evolve through a document.Our algorithm, ImpAr, processes the docu-ments sentence by sentence, assuming that se-quences of the same predicate (in its nominal orverbal form) share the same argument fillers (ex-plicit or implicit)3.
Thus, for every core argumentargn of a predicate, ImpAr stores its previousknown filler as a default value.
If the argumentsof a predicate are explicit, they always replace de-fault fillers previously captured.
When there is noantecedent for a particular implicit argument argn,the algorithm tries to find in the surrounding con-text which participant is the most likely to be thefiller according to some salience factors (see Sec-tion 4.2).
For the following instances, without anexplicit filler for a particular argument position,the algorithm repeats the same selection processand compares the new implicit candidate with thedefault one.
That is, the default implicit argumentof a predicate with no antecedent can change ev-ery time the algorithm finds a filler with a greatersalience.
A damping factor is applied to reduce thesalience of distant predicates.4.2 Filling arguments without explicitantecedentsFilling the implicit arguments of a predicate hasbeen identified as a particular case of corefer-ence, very close to pronoun resolution (Silbererand Frank, 2012).
Consequently, for those implicitarguments that have not explicit antecedents, wepropose an adaptation of a classic algorithm fordeterministic pronoun resolution.
This componentof our algorithm follows the RAP approach (Lap-pin and Leass, 1994).
When our algorithm needsto fill an implicit predicate argument without anexplicit antecedent it considers a set of candidateswithin a window formed by the sentence of thepredicate and the two previous sentences.
Then,the algorithm performs the following steps:3Note that the algorithm could also consider sequences ofclosely related predicates.1.
Apply two constraints to the candidate list:(a) All candidates that are already explicit argumentsof the predicate are ruled out.
(b) All candidates commanded by the predicate inthe dependency tree are ruled out.2.
Select those candidates that are semantically consistentwith the semantic category of the implicit argument.3.
Assign a salience score to each candidate.4.
Sort the candidates by their proximity to the predicateof the implicit argument.5.
Select the candidate with the highest salience value.As a result, the candidate with the highestsalience value is selected as the filler of the im-plicit argument.
Thus, this filler with its corre-sponding salience weight will be also consideredin subsequent instances of the same predicate.Now, we explain each step in more detail usingexample (2).
In this example, arg0 is missing forthe predicate plan:(2) Quest Medical Inc said it adopted [arg1 a sharehold-ers?
rights] [np plan] in which rights to purchase sharesof common stock will be distributed as a dividend toshareholders of record as of Oct 23.Filtering.
In the first step, the algorithm fil-ters out the candidates that are actual explicit argu-ments of the predicate or have a syntactic depen-dency with the predicate, and therefore, they are inthe search space of a traditional SRL system.In our example, the filtering process would re-move [a shareholders?
rights] because it is alreadythe explicit argument arg1, and [in which rightsto purchase shares of common stock will be dis-tributed as a dividend to shareholders of record asof Oct 23] because it is syntactically commandedby the predicate plan.Semantic consistency.
To determine the se-mantic coherence between the potential candidatesand a predicate argument argn, we have exploitedthe selectional preferences in the same way asin previous SRL and implicit argument resolutionworks.
First, we have designed a list of verygeneral semantic categories.
Second, we havesemi-automatically assigned one of them to everypredicate argument argn in PropBank and Nom-Bank.
For this, we have used the semantic an-notation provided by the training documents ofthe CoNLL-2008 dataset.
This annotation wasperformed automatically using the SuperSense-Tagger (Ciaramita and Altun, 2006) and includes1183named-entities and WordNet Super-Senses4.
Wehave also defined a mapping between the semanticclasses provided by the SuperSenseTagger and ourseven semantic categories (see Table 1 for moredetails).
Then, we have acquired the most com-mon categories of each predicate argument argn.ImpAr algorithm also uses the SuperSenseTaggerover the documents to be processed from BNBto check if the candidate belongs to the expectedsemantic category of the implicit argument to befilled.Following the example above, [Quest Medi-cal Inc] is tagged as an ORGANIZATION by theSuperSenseTagger.
Therefore, it belongs to oursemantic category COGNITIVE.
As the seman-tic category for the implicit argument arg0 forthe predicate plan has been recognized to be alsoCOGNITIVE, [Quest Medical Inc] remains in thelist of candidates as a possible filler.Semantic category Name-entities Super-SensesCOGNITIVEPERSON noun.personORGANIZATION noun.groupANIMAL noun.animal... ...TANGIBLE PRODUCT noun.artifactSUBSTANCE noun.object... ...EVENTIVE GAME noun.actDISEASE noun.communication...
...RELATIVE noun.shapenoun.attribute...LOCATIVE LOCATION noun.locationTIME DATE noun.timeMESURABLE QUANTITY noun.quantityPERCENT...Table 1: Links between the semantic categories and somename-entities and super-senses.Salience weighting.
In this process, the algo-rithm assigns to each candidate a set of saliencefactors that scores its prominence.
The sentencerecency factor prioritizes the candidates that oc-cur close to the same sentence of the predicate.The subject, direct object, indirect object and non-adverbial factors weight the salience of the candi-date depending on the syntactic role they belongto.
Additionally, the head of these syntactic rolesare prioritized by the head factor.
We have usedthe same weights, listed in table 2, proposed byLappin and Leass (1994).In the example, candidate [Quest Medical Inc]is in the same sentence as the predicate plan, it4Lexicographic files according to WordNet terminology.Factor type weightSentence recency 100Subject 80Direct object 50Indirect object 40Head 80Non-adverbial 50Table 2: Weights assigned to each salience factor.belongs to a subject, and, indeed, it is the headof that subject.
Hence, the salience score for thiscandidate is: 100 + 80 + 80 = 260.4.3 Damping the salience of the defaultcandidateAs the algorithm maintains the default candidateuntil an explicit filler appears, potential errors pro-duced in the automatic selection process explainedabove can spread to distant implicit instances, spe-cially when the salience score of the default can-didate is high.
In order to reduce the impact ofthese errors we have included a damping factorthat is applied sentence by sentence to the saliencevalue of the default candidate.
ImpAr applies thatdamping factor, r, as follows.
It assumes that, in-dependently of the initial salience assigned, 100points of the salience score came from the sen-tence recency factor.
Then, the algorithm changesthis value multiplying it by r. So, given a saliencescore s, the value of the score in a following sen-tence, s?, is:s?
= s?
100 + 100 ?
rObviously, the value of r must be defined with-out harming excessively those cases where the de-fault candidate has been correctly identified.
Forthis, we studied in the training dataset the casesof implicit arguments filled with the default can-didate.
Figure 1 shows that the influence of thedefault filler is much higher in near sentences thatin more distance ones.We tried to mimic a damping factor followingthis distribution.
That is, to maintain high scoresalience for the near sentences while strongly de-creasing them in the subsequent ones.
In this way,if the filler of the implicit argument is wronglyidentified, the error only spreads to the nearest in-stances.
If the identification is correct, a lowerscore for more distance sentences is not too harm-ful.
The distribution shown in figure 1 followsan exponential decay, therefore we have describedthe damping factor as a curve like the following,where ?
must be a value within 0 and 1:1184Figure 1: Distances between the implicit argument and thedefault candidate.
The y axis indicate the percentage of casesoccurring in each sentence distance, expressed in xr = ?dIn this function, d stands for the sentence dis-tance and r for the damping factor to apply in thatsentence.
In this paper, we have decided to set thevalue of ?
to 0.5.r = 0.5dThis value maintains the influence of the defaultfillers with high salience in near sentences.
But itdecreases that influence strongly in the following.In order to illustrate the whole process we willuse the previous example.
In that case, [QuestMedical Inc] is selected as the arg0 of plan witha salience score of 260.
Therefore [Quest Medi-cal Inc] becomes the default arg0 of plan.
In thefollowing sentence the damping factor is:0.5 = 0.51Therefore, its salience score changes to 260 ?100+100?0.5 = 210.
Then, the algorithm changesthe default filler for arg0 only if it finds a candi-date that scores higher in their current context.
Attwo sentence distance, the resulting score for thedefault filler is 260 ?
100 + 100 ?
0.25 = 185.
Inthis way, at more distance sentences, the influenceof the default filler of arg0 becomes smaller.5 EvaluationIn order to evaluate the performance of the Im-pAr algorithm, we have followed the evaluationmethod presented by Gerber and Chai (2010,2012).
For every argument position in the gold-standard the scorer expects a single predicted con-stituent to fill in.
In order to evaluate the correctspan of a constituent, a prediction is scored usingthe Dice coefficient:2|Predicted ?
True||Predicted| + |True|The function above relates the set of tokens thatform a predicted constituent, Predicted, and theset of tokens that are part of an annotated con-stituent in the gold-standard, True.
For eachmissing argument, the gold-standard includes thewhole coreference chain of the filler.
Therefore,the scorer selects from all coreferent mentions thehighest Dice value.
If the predicted span does notcover the head of the annotated filler, the scorer re-turns zero.
Then, Precision is calculated by thesum of all prediction scores divided by the numberof attempts carried out by the system.
Recall isequal to the sum of the prediction scores dividedby the number of actual annotations in the gold-standard.
F-measure is calculated as the harmonicmean of recall and precision.Traditionally, there have been two approachesto develop SRL systems, one based on constituenttrees and the other one based on syntactic depen-dencies.
Additionally, the evaluation of both typesof systems has been performed differently.
Forconstituent based SRL systems the scorers eval-uate the correct span of the filler, while for depen-dency based systems the scorer just check if thesystems are able to capture the head token of thefiller.
As shown above, previous works in implicitargument resolution proposed a metric that in-volves the correct identification of the whole spanof the filler.
ImpAr algorithm works with syntac-tic dependencies and therefore it only returns thehead token of the filler.
In order to compare ourresults with previous works, we had to apply somesimple heuristics to guess the correct span of thefiller.
Obviously, this process inserts some noisein the final evaluation.We have performed a first evaluation over thetest set used in (Gerber and Chai, 2010).
Thisdataset contains 437 predicate instances but just246 argument positions are implicitly filled.
Table3 includes the results obtained by ImpAr, the re-sults of the system presented by Gerber and Chai(2010) and the baseline proposed for the task.
Bestresults are marked in bold5.
For all predicates,ImpAr improves over the baseline (19.3 pointshigher in the overall F1).
Our system also out-performs the one presented by Gerber and Chai(2010).
Interestingly, both systems present verydifferent performances predicate by predicate.
For5No proper significance test can be carried out without thethe full predictions of all systems involved.1185Baseline Gerber & Chai ImpAr#Inst.
#Imp.
F1 P R F1 P R F1sale 64 65 36.2 47.2 41.7 44.2 41.2 39.4 40.3price 121 53 15.4 36.0 32.6 34.2 53.3 53.3 53.3investor 78 35 9.8 36.8 40.0 38.4 43.0 39.5 41.2bid 19 26 32.3 23.8 19.2 21.3 52.9 51.0 52.0plan 25 20 38.5 78.6 55.0 64.7 40.7 40.7 40.7cost 25 17 34.8 61.1 64.7 62.9 56.1 50.2 53.0loss 30 12 52.6 83.3 83.3 83.3 68.4 63.5 65.8loan 11 9 18.2 42.9 33.3 37.5 25.0 20.0 22.2investment 21 8 0.0 40.0 25.0 30.8 47.6 35.7 40.8fund 43 6 0.0 14.3 16.7 15.4 66.7 33.3 44.4Overall 437 246 26.5 44.5 40.4 42.3 47.9 43.8 45.8Table 3: Evaluation with the test.
The results from (Gerber and Chai, 2010) are included.Baseline Gerber & Chai ImpAr#Inst.
#Imp.
F1 P R F1 P R F1sale 184 181 37.3 59.2 44.8 51.0 44.3 43.3 43.8price 216 138 34.6 56.0 48.7 52.1 55.0 54.5 54.7investor 160 108 5.1 46.7 39.8 43.0 28.2 27.0 27.6bid 88 124 23.8 60.0 36.3 45.2 48.4 41.8 45.0plan 100 77 32.3 59.6 44.1 50.7 47.0 47.0 47.0cost 101 86 17.8 62.5 50.9 56.1 49.2 43.7 46.2loss 104 62 54.7 72.5 59.7 65.5 63.0 58.2 60.5loan 84 82 31.2 67.2 50.0 57.3 56.4 45.6 50.6investment 102 52 15.5 32.9 34.2 33.6 41.2 30.9 35.4fund 108 56 15.5 80.0 35.7 49.4 55.6 44.6 49.5Overall 1,247 966 28.9 57.9 44.5 50.3 47.7 43.0 45.3Table 4: Evaluation with the full dataset.
The results from (Gerber and Chai, 2012) are included.instance, our system obtains much higher resultsfor the predicates bid and fund, while much lowerfor loss and loan.
In general, ImpAr seems to bemore robust since it obtains similar performancesfor all predicates.
In fact, the standard deviation,?
, of F1 measure is 10.98 for ImpAr while thisvalue for the (Gerber and Chai, 2010) system is20.00.In a more recent work, Gerber and Chai (2012)presented some improvements of their previousresults.
In this work, they extended the evalua-tion of their model using the whole dataset andnot just the testing documents.
Applying a cross-validated approach they tried to solve some prob-lems that they found in the previous evaluation,like the small size of the testing set.
For this work,they also studied a wider set of features, specially,they experimented with some statistics learnt fromparts of GigaWord automatically annotated.
Table4 shows that the improvement over their previoussystem was remarkable.
The system also seemsto be more stable across predicates.
For compar-ison purposes, we also included the performanceof ImpAr applied over the whole dataset.The results in table 4 show that, although ImpArstill achieves the best results in some cases, thistime, it cannot beat the overall results obtained bythe supervised model.
In fact, both systems obtaina very similar recall, but the system from (Gerberand Chai, 2012) obtains much higher precision.In both cases, the ?
value of F1 is reduced, 8.81for ImpAr and 8.21 for (Gerber and Chai, 2012).However, ImpAr obtains very similar performanceindependently of the testing dataset what provesthe robustness of the algorithm.
This suggeststhat our algorithm can obtain strong results alsofor other corpus and predicates.
Instead, the su-pervised approach would need a large amount ofmanual annotations for every predicate to be pro-cessed.6 Discussion6.1 Component AnalysisIn order to assess the contribution of each sys-tem component, we also tested the performanceof ImpAr algorithm when disabling only one ofits components.
With this evaluations we pretendto sight the particular contribution of each compo-nent.
In table 5 we present the results obtained inthe following experiments for the two testing setsexplained in section 5:?
Exp1: The damping factor is disabled.
All se-lected fillers maintain the same salience over1186all sentences.?
Exp2: Only explicit fillers are considered ascandidates6.?
Exp3: No default fillers are considered ascandidates.As expected, we observe a very similar perfor-mances in both datasets.
Additionally, the high-est loss appears when the default fillers are ruledout (Exp3).
In particular, it also seems that theexplicit information from previous predicates pro-vides the most correct evidence (Exp2).
Also notethat for Exp2, the system obtains the highest preci-sion.
This means that the most accurate cases areobtained by previous explicit antecedents.test fullP R F1 P R F1full 47.9 43.8 45.8 47.7 43.0 45.3Exp1 45.7 41.8 43.6 47.1 42.5 44.8Exp2 51.2 24.6 33.2 55.3 25.5 34.9Exp3 34.6 29.7 31.9 34.8 28.9 31.5Exp4 42.6 37.9 40.1 37.5 31.2 34.1Exp5 38.8 34.5 36.5 35.7 29.7 32.4Exp6 53.3 48.7 50.9 52.4 47.2 49.6Table 5: Exp1, Exp2 and Exp3 correspond to ablations of thecomponents.
Exp3 and Exp4 are experiments over the casesthat are not solved by explicit antecedents.
Exp6 evaluatesthe system capturing just the head tokens of the constituents.As Exp1 also includes instances with explicitantecedents, and for these cases the damping fac-tor component has no effect, we have designed twoadditional experiments:?
Exp4: Full system for the cases not solved byexplicit antecedents.?
Exp5: As in Exp4 but with the damping fac-tor disabled.As expected, now the contribution of the dump-ing factor seems to be more relevant, in particular,for the test dataset.6.2 Correct span of the fillersAs explained in Section 5, our algorithm workswith syntactic dependencies and its predictionsonly return the head token of the filler.
Obtainingthe correct constituents from syntactic dependen-cies is not trivial.
In this work we have applieda simple heuristic that returns all the descendant6That is, implicit arguments without explicit antecedentsare not filled.tokens of the predicted head token.
This naiveprocess inserts some noise to the evaluation of thesystem.
For example, from the following sentenceour system gives the following prediction for animplicit arg1 of an instance of the predicate sale:Ports of Call Inc. reached agreements to sell its re-maining seven aircraft [arg1 to buyers] that weren?tdisclosed.But the actual gold-standard annotation is:[arg1 buyers that weren?t disclosed].
Although thehead of the constituent, buyers, is correctly cap-tured by ImpAr, the final prediction is heavily pe-nalized by the scoring method.
Table 5 presentsthe results of ImpAr when evaluating the head to-kens of the constituents only (Exp6).
These resultsshow that the current performance of our systemcan be easily improved applying a more accurateprocess for capturing the correct span.7 Conclusions and Future WorkIn this work we have presented a robust determin-istic approach for implicit Semantic Role Label-ing.
The method exploits a very simple but rel-evant discoursive coherence property that holdsover explicit and implicit arguments of closely re-lated nominal and verbal predicates.
This prop-erty states that if several instances of the samepredicate appear in a well-written discourse, it isvery likely that they maintain the same argumentfillers.
We have shown the importance of this phe-nomenon for recovering the implicit informationabout semantic roles.
To our knowledge, this is thefirst empirical study that proves this phenomenon.Based on these observations, we have devel-oped a new deterministic algorithm, ImpAr, thatobtains very competitive and robust performanceswith respect to supervised approaches.
That is, itcan be applied where there is no available manualannotations to train.
The code of this algorithm ispublicly available and can be applied to any docu-ment.
As input it only needs the document withexplicit semantic role labeling and Super-Senseannotations.
These annotations can be easily ob-tained from plain text using available tools7, whatmakes this algorithm the first effective tool avail-able for implicit SRL.As it can be easily seen, ImpAr has a largemargin for improvement.
For instance, providingmore accurate spans for the fillers.
We also plan7We recommend mate-tools (Bjo?rkelund et al, 2009) andSuperSenseTagger (Ciaramita and Altun, 2006).1187to test alternative approaches to solve the argu-ments without explicit antecedents.
For instance,our system can also profit from additional annota-tions like coreference, that has proved its utility inprevious works.
Finally, we also plan to study ourapproach on different languages and datasets (forinstance, the SemEval-2010 dataset).8 AcknowledgmentWe are grateful to the anonymous reviewersfor their insightful comments.
This work hasbeen partially funded by SKaTer (TIN2012-38584-C06-02), OpeNER (FP7-ICT-2011-SME-DCL-296451) and NewsReader (FP7-ICT-2011-8-316404), as well as the READERS projectwith the financial support of MINECO, ANR(convention ANR-12-CHRI-0004-03) and EPSRC(EP/K017845/1) in the framework of ERA-NETCHIST-ERA (UE FP7/2007-2013).ReferencesBaker, C. F., C. J. Fillmore, and J.
B. Lowe (1998).The berkeley framenet project.
In Proceedingsof the 36th Annual Meeting of the Associationfor Computational Linguistics and 17th Inter-national Conference on Computational Linguis-tics, ACL ?98, Montreal, Quebec, Canada, pp.86?90.Bjo?rkelund, A., L. Hafdell, and P. Nugues (2009).Multilingual semantic role labeling.
In Pro-ceedings of the Thirteenth Conference on Com-putational Natural Language Learning: SharedTask, CoNLL ?09, Boulder, Colorado, USA, pp.43?48.Carreras, X. and L. Ma`rquez (2005).
Introductionto the conll-2005 shared task: Semantic role la-beling.
In Proceedings of the 9th Conferenceon Computational Natural Language Learning,CoNLL ?05, Ann Arbor, Michigan, USA, pp.152?164.Chen, D., N. Schneider, D. Das, and N. A. Smith(2010).
Semafor: Frame argument resolutionwith log-linear models.
In Proceedings of the5th International Workshop on Semantic Eval-uation, SemEval ?10, Los Angeles, California,USA, pp.
264?267.Ciaramita, M. and Y. Altun (2006).
Broad-coverage sense disambiguation and informationextraction with a supersense sequence tagger.
InProceedings of the 2006 Conference on Empir-ical Methods in Natural Language Processing,EMNLP ?06, Sydney, Australia, pp.
594?602.Dahl, D. A., M. S. Palmer, and R. J. Passonneau(1987).
Nominalizations in pundit.
In In Pro-ceedings of the 25th Annual Meeting of the As-sociation for Computational Linguistics, ACL?87, Stanford, California, USA, pp.
131?139.Fellbaum, C. (1998).
WordNet: an electronic lexi-cal database.
MIT Press.Gerber, M. and J. Chai (2012, December).
Se-mantic role labeling of implicit arguments fornominal predicates.
Computational Linguis-tics 38(4), 755?798.Gerber, M. and J. Y. Chai (2010).
Beyond nom-bank: a study of implicit arguments for nomi-nal predicates.
In Proceedings of the 48th An-nual Meeting of the Association for Computa-tional Linguistics, ACL ?10, Uppsala, Sweden,pp.
1583?1592.Gildea, D. and D. Jurafsky (2000).
Automatic la-beling of semantic roles.
In Proceedings of the38th Annual Meeting on Association for Com-putational Linguistics, ACL ?00, Hong Kong,pp.
512?520.Gildea, D. and D. Jurafsky (2002, September).Automatic labeling of semantic roles.
Compu-tational Linguistics 28(3), 245?288.Gorinski, P., J. Ruppenhofer, and C. Sporleder(2013).
Towards weakly supervised resolutionof null instantiations.
In Proceedings of the 10thInternational Conference on Computational Se-mantics, IWCS ?13, Potsdam, Germany, pp.119?130.Hajic?, J., M. Ciaramita, R. Johansson, D. Kawa-hara, M. A.
Mart?
?, L. Ma`rquez, A. Meyers,J.
Nivre, S.
Pado?, J.
S?te?pa?nek, P. Stran?a?k,M.
Surdeanu, N. Xue, and Y. Zhang (2009).The CoNLL-2009 shared task: Syntactic andsemantic dependencies in multiple languages.In Proceedings of the Thirteenth Conferenceon Computational Natural Language Learning:Shared Task, CoNLL ?09, Boulder, Colorado,USA, pp.
1?18.Laparra, E. and G. Rigau (2012).
Exploiting ex-plicit annotations and semantic types for im-plicit argument resolution.
In 6th IEEE Inter-national Conference on Semantic Computing,ICSC ?12, Palermo, Italy, pp.
75?78.1188Laparra, E. and G. Rigau (2013).
Sources of evi-dence for implicit argument resolution.
In Pro-ceedings of the 10th International Conferenceon Computational Semantics, IWCS ?13, Pots-dam, Germany, pp.
155?166.Lappin, S. and H. J. Leass (1994, December).
Analgorithm for pronominal anaphora resolution.Computational Linguistics 20(4), 535?561.Meyers, A., R. Reeves, C. Macleod, R. Szekely,V.
Zielinska, B.
Young, and R. Grishman(2004).
The nombank project: An interim re-port.
In In Proceedings of the NAACL/HLTWorkshop on Frontiers in Corpus Annota-tion, HLT-NAACL ?04, Boston, Massachusetts,USA, pp.
24?31.Moor, T., M. Roth, and A. Frank (2013).Predicate-specific annotations for implicit rolebinding: Corpus annotation, data analysis andevaluation experiments.
In Proceedings ofthe 10th International Conference on Compu-tational Semantics, IWCS ?13, Potsdam, Ger-many, pp.
369?375.Palmer, M., D. Gildea, and P. Kingsbury (2005,March).
The proposition bank: An annotatedcorpus of semantic roles.
Computational Lin-guistics 31(1), 71?106.Palmer, M. S., D. A. Dahl, R. J. Schiffman,L.
Hirschman, M. Linebarger, and J. Dowding(1986).
Recovering implicit information.
InProceedings of the 24th annual meeting on As-sociation for Computational Linguistics, ACL?86, New York, New York, USA, pp.
10?19.Raghunathan, K., H. Lee, S. Rangarajan,N.
Chambers, M. Surdeanu, D. Jurafsky, andC.
Manning (2010).
A multi-pass sieve forcoreference resolution.
In Proceedings of the2010 Conference on Empirical Methods in Nat-ural Language Processing, EMNLP ?10, Cam-bridge, Massachusetts, USA, pp.
492?501.Ruppenhofer, J., P. Gorinski, and C. Sporleder(2011).
In search of missing arguments: A lin-guistic approach.
In Proceedings of the Inter-national Conference Recent Advances in Nat-ural Language Processing 2011, RANLP ?11,Hissar, Bulgaria, pp.
331?338.Ruppenhofer, J., C. Sporleder, R. Morante,C.
Baker, and M. Palmer (2010).
Semeval-2010task 10: Linking events and their participantsin discourse.
In Proceedings of the 5th Inter-national Workshop on Semantic Evaluation, Se-mEval ?10, Los Angeles, California, USA, pp.45?50.Silberer, C. and A. Frank (2012).
Casting implicitrole linking as an anaphora resolution task.
InProceedings of the First Joint Conference onLexical and Computational Semantics, *SEM?12, Montre?al, Canada, pp.
1?10.Surdeanu, M., R. Johansson, A. Meyers,L.
Ma`rquez, and J. Nivre (2008).
The CoNLL-2008 shared task on joint parsing of syntacticand semantic dependencies.
In Proceedings ofthe Twelfth Conference on Natural LanguageLearning, CoNLL ?08, Manchester, UnitedKingdom, pp.
159?177.Tetreault, J. R. (2002).
Implicit role reference.In International Symposium on Reference Res-olution for Natural Language Processing, Ali-cante, Spain, pp.
109?115.Tonelli, S. and R. Delmonte (2010).
Venses++:Adapting a deep semantic processing system tothe identification of null instantiations.
In Pro-ceedings of the 5th International Workshop onSemantic Evaluation, SemEval ?10, Los Ange-les, California, USA, pp.
296?299.Tonelli, S. and R. Delmonte (2011).
Desperatelyseeking implicit arguments in text.
In Proceed-ings of the ACL 2011 Workshop on RelationalModels of Semantics, RELMS ?11, Portland,Oregon, USA, pp.
54?62.Whittemore, G., M. Macpherson, and G. Carlson(1991).
Event-building through role-filling andanaphora resolution.
In Proceedings of the 29thannual meeting on Association for Computa-tional Linguistics, ACL ?91, Berkeley, Califor-nia, USA, pp.
17?24.1189
