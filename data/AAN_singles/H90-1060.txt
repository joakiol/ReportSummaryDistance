A New Paradigmfor Speaker-Independent Trainingand Speaker AdaptationFrancis Kubala and Richard SchwartzBBN Systems and Technologies CorporationCambridge, MA 02138AbstractThis paper reports on two contributions to large vocabu-lary continuous peech recognition.
First, we present anew paradigm for speaker-independent (SI) training of hid-den Markov models (HMM), which uses a large amount ofspeech from a few speakers instead of the traditional prac-tice of using a little speech from many speakers.
In addition,combination of the training speakers is done by averagingthe statistics of independently rained models rather than theusual pooling of all the speech data from many speakers priorto training.
With only 12 training speakers for SI recogni-tion, we achieved a 7.5% word error rate on a standardgrammar and test set from the DARPA Resource Manage-ment corpus.
This performance is comparable to our bestcondition for this test suite, using 109 training speakers.Second, we show a significant improvement for speakeradaptation (SA) using the new SI corpus and a small amountof speech from the new (target) speaker.
A probabilisticspectral mapping is estimated independently for each train-ing (reference) speaker and the target speaker.
Each refer-ence model is transformed tothe space of the target speakerand combined by averaging.
Using only 40 utterances fromthe target speaker for adaptation, the error rate dropped to4.1% - -  a 45% reduction in error compared to the SI result.1.
IntroductionOne important scenario for the use of spoken language sys-tems (SLS) by new speakers is to start with a SI corpus ormodel and have the system adapt as the new users interactwith the system.
Once the interaction has begun, the sys-tem has the opportunity to collect speaker-dependent data ofknown orthographic transcription from the target speaker.After a small sample of speech has been collected, the sys-tem should be able to adapt so as to significantly increaseperformance ompared to the original SI model.
The successof this scenario depends on the adaptation being powerfulenough to generalize from a small sample of speaker-specificspeech in which most of the phonetic ontexts of the lan-guage are not observed.
Furthermore, it depends on havinga SI speech corpus which is amenable to speaker adaptation.It is a widely held belief that speech used for training SImodels must be collected from many speakers.
It is alsocommonly accepted that collecting only a small sample ofspeech from each training speaker is a reasonable compro-mise to make in the effort to collect as many speakers aspossible.
While this compromise may be reasonable for SIrecognition, several efforts to use such a corpus as a ba-sis for speaker adaptation have failed to make significantimprovements.Recently, we have discovered that adequate SI perfor-mance can be achieved with far less speaker coverage thanconventionally thought necessary, but with much better sam-piing of each training speaker's peech.
Specifically, weshow that it is possible to achieve near state-of-the-art SIperformance on a 1000-word continuous peech recognitiontask using only 12 training speakers.
Furthermore, we willshow that it is possible and advantageous to create the SImodel from a set of independently rained speaker-dependent(SD) models, without retraining on the entire pooled datasetat one time.
Most importantly, we show that such a SI cor-pus is an effective basis for speaker adaptation.
By combin-ing the adapted models of 11 reference speakers, we wereable to reduce the error rate by 45% compared to the SIperformance.
This method succeeds because we are able toapply a robust probabilistic speaker-transformation o well-trained andhighly discriminating SD training models.In section 2, we describe the new SI training paradigm andpresent comparative r sults for SI recognition using only 12training speakers.
In section 3, we describe three previousattempts to adapt from a corpus of many training speakers.Then we describe our approach for adapting to new speak-ers from the 12 speaker SI corpus and discuss experimentalresults.2.
Speaker-Independent Training109 Speaker SI Tra in ingFor several years, the DARPA Resource Management con-tinuous speech corpus has provided a testbed for SI recog-nition.
109 speakers are designated as training speakers andare each represented by a sample of 40 utterances.
Typi-cally, the data from all the speakers is pooled at the outset,as ff it all came from one speaker.
Although the training dataoriginates from many diverse sources, the forward-backward(Baum-Welch) training procedure is robust enough to do areasonable job of modeling the pooled data.
When used with306a standard word-pair grammar of perplexity 60, state-of-the-m SI recognition performance for this corpus is 6--7% worderror rate.This performance is 3 times worse than our current SDperformance using 600 training utterances.
Also, the sen-tence error rate at this level of perfommnce is greater than30% - -  a level of error that we assume is far too high for theacoustic omponent of a spoken language system.
Further-more, this performance has been achieved with an artificialand non-robust grammar of modest perplexity which willnot work within an SLS context.
Combining the need forhigher absolute performance with the need to use less pow-erful gratrmmrs indicates that the current SI error rate mayneed to be reduced by a factor of at least 4 to be acceptablefor SLS applications.Results of SI ExperimentsResults for several SI experiment are shown in table 1.
Allresults are from first runs of the designated Feb. '89 SItest set on the given system configuration.
This test setconsists of 10 speakers (4 females) with 30 utterances each.All runs used the standard word-pair grammar of perplexity60.
System parameters were fixed before running any ofthe conditions in this experiment.
The limited developmenttesting which we did perform was done only on the June'88 SD/SI test set using only the 109 speaker SI model.For each condition we show the number of training speak-ers, and the manner in which the models were trained andsmoothed.
The training was done either on pooled data(joint training) or on individual speakers' data (indep train-ing).
The smoothing was either not done, or was appliedto either the jointly or independently trained model.
Foreach condition, the word error rate, which includes insertionerrors, and sentence rror rate are given.12 Speaker SI TrainingSince we planned to perform adaptation from 12 referencespeakers, we needed to mna SI control condition by usingthe data in the usual pooled fashion.
We ran a comparativetest using data from only the 12 speakers from the SD seg-ment of the DARPA database.
The training for each speakerconsisted of 600 training utterances.
Seven of the speakersare male.We did have some indication that pooling the data of evena few speakers could make large improvements from an ex-periment conducted at IBM and described in \[5\].
However,12 speakers could hardly be expected to contain an exam-ple of all speaker types in the general population (includingboth genders), so we could anticipate the need for somekind of smoothing before we began.
Our usual techniquefor smoothing across the bins of the discrete densities, tri-phone cooccurrence smoothing \[7\], has proven to be an ef-fective method for dealing with the widely varying amountsof training data for the detailed context models in the sys-tem.
When used in a SD training scenario, it has allowed usto observe a performance gain for explicitly modeling sev-eral thousand triphones which were observed only once ortwice in the training.However, the cooccurrence smoothing is not appropriatefor models derived from the pooled data of many speakers.Spectra from different speakers will cooccur much more ran-domly than spectra from a single speaker.
This will yieldpoorer estimates of the smoothing matrices.
As such, 11S-phone cooccurrence smoothing is a speaker-specific model-ing technique.
If the data is pooled prior to training, wecannot effectively apply our best smoothing to the model.This realization has led us to examine the practice of pool-ing the data in the first place.
A straightforward altemafiveto pooling the data is to keep the speakers separated until thespeaker-specific operations of training and smoothing havebeen completed and then combine the multiple SD models.To allow the model combination to be done by averagingthe model statistics, we constructed a SI codebook whichwas used in common for all speakers.#Spkrs109 joint109 joint12 joint12 joint12 joint12 indepTraining Smoothing Word Err Sent Errnonejointnonejointindep7.16.59.08.57.8indep 7.5363442413737Table 1.
Comparison of SI training scenarios on the Feb.'89 test set with word-pair grammar.The 109 speaker conditions were run to calibrate the BY-BLOS system with published results for the same test set.We observe a small improvement, from 7.1% to 6.5% worderror, for using smoothing on the jointly trained model.
The6.5% error rate is comparable to the best performance onrecord (6.1%) for this test set which was achieved by Leeas noted in \[4\].
Furthermore, the sentence rror rates areidentical.
Lee's system used a corrective gaining and rein-forcement procedure to increase the discrimination ability ofthe model for confusable words.
No corrective training wasused for the BYBLOS results given in table 1.The system configuration for the 109 condition was iden-tical to that which we use for SD recognition except forone difference.
One new system parameter was added todecrease the lambda factors used for combining the context-dependent models into interpolated triphones \[6\] by a factorof eight to account for the larger corpus.Next we repeated the same conditions for the 12 speakerSI model.
Simply pooling the 12 speakers without smooth-ing does not perform as well as the 109 speaker model.
Andonce again, smoothing the jointly trained model has a ratherweak effect on performance.
However, we were surprisedthat the 12 speaker model should have only 25% more errorthan the 109 speaker model.The final two results show the effect of independentlysmoothing the 12 speaker model after either joint or indepen-dent training.
To independently smooth the jointly trainedmodel, we first trained on the pooled data as usual.
Then a307SD model was made, for each training speaker, by runningthe forward-backward algorithm on the combined SI modelbut on data from only one speaker in turn.
This allowedus to generate a set of SD models for smoothing, whichshared a common alignment.
The smoothed models werethen recombined by averaging the model statistics.The approach used on the final result is the most straight-forward - -  we train multiple independent SD models allow-ing each to align optimally for the specific speaker, smootheach model to model spectral variation within each speaker,and then combine the models by averaging correspondingprobabilities in the,models.As is evident from the table, both of the final methodsimprove due to the increased effectiveness of the smoothingwhen it is applied to a speaker-specific model.
In a final sur-prise, we find that constraining all the speakers to a commonalignment does not help.
Further, the word error rate of thissimple model is only 15% worse than our best performancewith the 109 speaker model and the sentence rror rates arestatistically indistinguishable.Some caution is required in comparing results of the 12and 109 speaker models due to two, possibly important dif-ferences.
The total amount of training speech used is dif-ferent as is the number of different sentence texts containedin the training script.
The 109 speaker model is trained ona total of 4360 utterances drawn from 2800 sentence texts.The 12 speaker model is trained on 7200 utterances drawnfrom only 600 sentence texts.
While the additional speechmay benefit he 12 speaker condition, the greater ichnessof the sentence texts may help the 109 speaker model.
Theeffect of the additional sentence texts can be seen in thedifferent numbers of triphone contexts observed in the twotraining scripts: 5000 triphones for 600 sentences vs. 7000for the 2800--sentence script.Discussion of SI ResultsWe have observed that the forward-backward algorithmfreely re-defines ome of the phonemes to model peculiar-ities of a given speaker.
If we constrain all speakers toa common alignment, the training procedure must makea compromise between these speaker-specific adjuslments.Both forward-backward and triphone coocurrence smooth-ing are arguably speaker-specific procedures - -  they workbest when the training distributions are generated by a singlesource.
Some compromise must be made for SI recognition,where the training is not homogeneous and the test distribu-tion is, by definition, different han the training.
It appears,from these results, that the least damaging compromise maybe to delay pooling of the data/models until the last possiblestage in the processing.Such a simple SI paradigm has several attractive at-tributes.
It makes the data collection effort easier.
It istrivial to add new training speakers to the SI model; no re-training is required.
Therefore the system can easily makeuse of any speakers who have already committed to givingenough speech to train a high-performance SD model.
Thereis a large payoff for being one of the training speakers inthis scenario - -  highly accurate SD performance.
In con-trast, there is no benefit for being a training speaker for the109 speaker model.
Finally, by delaying the stage at whichthe data or model parameters are pooled, new opportunitiesarise to use speaker-specific modeling approaches such asthe multiple-reference adaptation procednre described in thenext section.3.
Speaker AdaptationAdaptation from 109 SpeakersAs mentioned above, previous attempts to use large popu-lation SI corpora for speaker adaptation have met with lit-tle success.
In \[3\], Lee tried to cluster over 100 trainingspeakers into a small number of groups which were thentrained independently.
In recognition, the test speaker wasfirst classified into one of the speaker groups, based on 1known utterance, and decoded with the appropriate model.This approach failed to improve over the SI performancesince it reduced the amount of training data available toeach speaker-group-specific model.
In another attempt, Leedevised an interpolated re-estimation procedure which com-bined the SI model with 4 other models derived from asmall sample of known speech from the target speaker.
In-terpolation weights for the 5 models were computed from adeleted sample of the training data.
The reduction in worderror rate was less than 10%, however, when 30 utterancesfrom the target speaker were used.
The gain was small forthis approach because only a small amount of new infor-mation, robustly estimated in the 4 speaker-specific models,was added to an already robust SI model.We have also attempted to use the same SI corpus ofover 100 speakers for speaker adaptation as reported in \[2\].In this work, we estimated a deterministic transformationon the speech parameters of each of the training speakerswhich projected them onto the feature space of a single pro-tolypical training speaker.
We then trained on all of thetransformed speech as if it came from a single speaker.
Thetarget speaker was similarly projected onto the prototypi-cal speaker and recognition proceeded using the prototypi-cal model.
This procedure reduced the word error rate by10% compared to the SI result; a minor improvement for asignificant increase in the complexity of the scenario.
Webelieve that this method did no better because the featuretransformation was not powerful enough to superimpose apair of speakers without significant loss of information.
Thisresulted in a prototypical model whose densities were notsignificantly sharper than the comparable SI model madefrom the original data.Adaptation from 12 SpeakersOur experience with the 109 corpus led us to rethink our ap-proach to speaker adaptation from multiple reference speak-ers.We already have a powerful speaker adaptation proce-dure which effectively transforms a single well-trained SDreference model into an adapted model of the target speaker\[1\].
The transformation is estimated from a small amount of308adaptation data (40 utterances) given by the target speaker.The approach is powerful for two reasons: first, the estimateof the probabilistic spectral mapping between two speakersis robust and generalizes well to phonetic ontexts not ob-served in the adaptation speech, and second, the transfor-mation can be applied to the well-estirnated, iscriminatingdensities of the SD reference model without undue loss ofdetail.A natural extension of this approach to multiple refer-ences would be to combine the parameters of several SDmodels after they had been independently adapted to thesame target speaker.
We can assume from our 12 speakerSI experiments hat the transformation will perform better ifestimated independently between each speaker-pair in tumrather than from a pooled dataset, since the transformationis a speaker-pair-specific operation.
We also know that wecan successfully combine the multiple adapted models byaveraging the model statistics.Results of Adaptation ExperimentsTable 2 shows results for development tests on the June '88SD/SI test set and word-pair grammar.
The test set consistsof 12 speakers (7 males) and 25 utterances each.#Re~1111Training Word Err Sent Err30 rain 6.2 312 hr 5.6 305.5 hr 4.1 23Table 2.
Comparison of speaker-adapatation results onthe June '88 test set with word-pair grammar.Adapting from a single male reference speaker trained on30 minutes of speech (600 utterances) gives a word errorrate of 6.2%.
The reference speaker in this case is, LPN,from the designated RM2 database.In the second row, a small improvement is realized forincreasing the reference speaker training to 2 hours (2400utterances).
We intend to make this comparison more reli-able by using the three other speakers in the RM2 databaseas references.The third condition shows the result of combining modelsfrom 11 reference speakers after adapting them to the 12thspeaker and jackknifing over all the reference speakers.
Theresult is a significant improvement i  both word and sentenceerror rates over the single reference perfonnance.Discussion of Adaptation ResultsSpeaker adaptation from a single reference speaker continuesto be an economical solution for systems which are forced toretrain due to changes in channel, environmental conditions,or task domain.
With only 40 utterances from the systemusers and 600 training utterances from the reference speaker,a speaker-adaptive system can be rapidly re-configured anddeliver performance equal to the best current SI performancetrained on 4000 utterances.We can also make a comparison between the multi-reference adapted result, tested on the June '88 SD/SI testset, and the 12 speaker SI result ested on the Feb. '89 SI testset, since roughly the same population of training speakersare used (except for the held-out one).
The two test sets givethe same performance when tested using the 109 speaker SImodel.
Comparing to the 12 speaker SI model, the 11 ref-erence adapted model has reduced the word error by 45%.We are encouraged by this large improvement for astraightforward application of our basic speaker adaptationalgorithm to multiple references.
Individual speaker perfor-rnance ranged from 0.6% to 7.7% error indicating that themultiple-reference model was very effective at eliminatingthe poorest outliers.
Two speakers performed equal to orbetter than their SD models trained on 600 utterances.We intend to continue investigating the potential ofspeaker adaptation from multiple references.
If we can con-tinue to improve our adaptation algorithm, and understandwhat constitutes good reference speakers, itmay be possibleto bring our speaker-adaptive performance very close to ourSD performance.ConclusionsWe have shown that it is possible to achieve near currentstate-of-the-art SI performance with a model trained fromonly 12 speakers.
This result is possible due to two impor-tant changes to the usual SI training paradigm - -  a largeamount of speech is available from each training speakerand the data is not pooled before training.Having a large sample of data from each training speakerand keeping it separate allows us to train detailed, highlydiscriminating, densities in a SD model and make the mosteffective use of speaker-specific modeling techniques such astriphone cooccurrence smoothing and probabilistic spectraltransfonnation.Furthermore, the new paradigm eases the burden of datacollection for SI recognition and allows new training speak-ers to be added to the SI model with ease.Most importantly, the new SI corpus lends itself wellto speaker adaptation.
By combining multiple referencespeaker models which have been independently transformedto the target speaker, we have cut the SI word error ratefrom 7.5% to 4.1% using only 40 utterances of adaptationspeech.AcknowledgementThis work was supported by the Defense Advanced Re-search Projects Agency and monitored by the Office of NavalResearch under Contract No.
N00014-85-C-0279.References\[1\] Fe:lg, M., F. Kubala, R. Schwartz, J. Makhoul, "Im-proved Speaker Adaptation Using Text DependentSpectral Mappings", IEEE ICASSP-88, paper $3.9.\[2\] Kubala, F., R. Schwartz, C. Barry, "Speaker Adap-tation from a Speaker-Independent Training Corpus",IEEE ICASSP-90, Apr.
1990, paper $3.3.309[3] Lee, K., "Large-Vocabulary Speaker-Independent Co -tinuous Speech Recognition: The SPHINX System",PHD dissertation, Carnegie-Mellon University, Apr.1988, CMU-CS-88-148[4] Lee, K., H. Won, M. Hwang, "Recent Progress in theSphinx Speech Recognition System", Proceedings ofthe DARPA Speech and Natural Language Workshop,Morgan Kaufmann Publishers, Inc., Feb. 1989, pp.125-130.
[5] Rtischev, D., "Speaker Adaptation in a Large-Vocabulary Speech Recognition System", Masters the-sis, Massachusetts Institute of Technology, Jan.
1989.
[6] Schwartz, R., Y. Chow, O. Kimball, S. Roucos,M.
Krasner, J. Makhoul, "Context-Dependent Model-ing for Acoustic-Phonetic Recognition of ContinuousSpeech"; IEEE ICASSP-85, Mar.
1985, paper 31.3[7] Schwartz, R., O. Kimball, F. Kubala, M. Feng, Y.Chow, C. Barry, J. Makhoul, "Robust SmoothingMethods for Discrete Hidden Markov Models", IEEEICASSP-89, May 1989, paper S10b.9.310
