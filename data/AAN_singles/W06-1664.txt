Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 542?550,Sydney, July 2006. c?2006 Association for Computational LinguisticsGraph-based Word Clustering using a Web Search EngineYutaka MatsuoNational Institute of AdvancedIndustrial Science and Technology1-18-13 Sotokanda, Tokyo 101-0021y.matsuo@aist.go.jpTakeshi SakakiUniversity of Tokyo7-3-1 HongoTokyo 113-8656Ko?ki UchiyamaHottolink Inc.2-11-17 Nishi-gotandaTokyo 141-0031uchi@hottolink.co.jpMitsuru IshizukaUniversity of Tokyo7-3-1 HongoTokyo 113-8656ishizuka@i.u-tokyo.ac.jpAbstractWord clustering is important for automaticthesaurus construction, text classification,and word sense disambiguation.
Recently,several studies have reported using theweb as a corpus.
This paper proposesan unsupervised algorithm for word clus-tering based on a word similarity mea-sure by web counts.
Each pair of wordsis queried to a search engine, which pro-duces a co-occurrence matrix.
By calcu-lating the similarity of words, a word co-occurrence graph is obtained.
A new kindof graph clustering algorithm called New-man clustering is applied for efficientlyidentifying word clusters.
Evaluations aremade on two sets of word groups derivedfrom a web directory and WordNet.1 IntroductionThe web is a good source of linguistic informa-tion for several natural language techniques suchas question answering, language modeling, andmultilingual lexicon acquisition.
Numerous stud-ies have examined the use of the web as a corpus(Kilgarriff, 2003).Web-based models perform especially wellagainst the sparse data problem: Statistical tech-niques perform poorly when the words are rarelyused.
For example, F. Keller et al (2002) use theweb to obtain frequencies for unseen bigrams ina given corpus.
They count for adjective-noun,noun-noun, and verb-object bigrams by queryinga search engine, and demonstrate that web fre-quencies (web counts) correlate with frequenciesfrom a carefully edited corpus such as the BritishNational Corpus (BNC).
Aside from counting bi-grams, various tasks are attainable using web-based models: spelling correction, adjective order-ing, compound noun bracketing, countability de-tection, and so on (Lapata and Keller, 2004).
Forsome tasks, simple unsupervised models performbetter when n-gram frequencies are obtained fromthe web rather than from a standard large corpus;the web yields better counts than the BNC.The web is an excellent source of informationon new words.
Therefore, automatic thesaurusconstruction (Curran, 2002) offers great potentialfor various useful NLP applications.
Several stud-ies have addressed the extraction of hypernymsand hyponyms from the web (Miura et al, 2004;Cimiano et al, 2004).
P. Turney (2001) presents amethod to recognize synonyms by obtaining wordcounts and calculating pointwise mutual informa-tion (PMI).
For further development of automaticthesaurus construction, word clustering is benefi-cial, e.g.
for obtaining synsets.
It also contributesto word sense disambiguation (Li and Abe, 1998)and text classification (Dhillon et al, 2002) be-cause the dimensionality is reduced efficiently.This paper presents an unsupervised algorithmfor word clustering based on a word similaritymeasure by web counts.
Given a set of words, thealgorithm clusters the words into groups so thatthe similar words are in the same cluster.
Each pairof words is queried to a search engine, which re-sults in a co-occurrence matrix.
By calculating thesimilarity of words, a word co-occurrence graphis created.
Then, a new kind of graph clusteringalgorithm, called Newman clustering, is applied.Newman clustering emphasizes betweenness of anedge and identifies densely connected subgraphs.To the best of our knowledge, this is the firstattempt to obtain word groups using web counts.Our contributions are summarized as follows:542?
A new algorithm for word clustering is de-scribed.
It has few parameters and thus iseasy to implement as a baseline method.?
We evaluate the algorithm on two sets ofword groups derived from a web directoryand WordNet.
The chi-square measure andNewman clustering are both used in our al-gorithm, they are revealed to outperform PMIand hierarchical clustering.We target Japanese words in this paper.
The re-mainder of this paper is organized as follows: Weoverview the related studies in the next section.Our proposed algorithm is described in Section 3.Sections 4 and 5 explain evaluations and advancediscussion.
Finally, we conclude the paper.2 Related WorksA number of studies have explained the use ofthe web for NLP tasks e.g., creating multilingualtranslation lexicons (Cheng et al, 2004), text clas-sification (Huang et al, 2004), and word sense dis-ambiguation (Turney, 2004).
M. Baroni and M.Ueyama summarize three approaches to use theweb as a corpus (Baroni and Ueyama, 2005): us-ing web counts as frequency estimates, buildingcorpora through search engine queries, and crawl-ing the web for linguistic purposes.
Commercialsearch engines are optimized for ordinary users.Therefore, it is desirable to crawl the web and todevelop specific search engines for NLP applica-tions (Cafarella and Etzioni, 2005).
However, con-sidering that great efforts are taken in commercialsearch engines to maintain quality of crawling andindexing, especially against spammers, it is stillimportant to pursue the possibility of using thecurrent search engines for NLP applications.P.
Turney (Turney, 2001) presents an unsu-pervised learning algorithm for recognizing syn-onyms by querying a web search engine.
Thetask of recognizing synonyms is, given a targetword and a set of alternative words, to choose theword that is most similar in meaning to the tar-get word.
The algorithm uses pointwise mutualinformation (PMI-IR) to measure the similarity ofpairs of words.
It is evaluated using 80 synonymtest questions from the Test of English as a ForeignLanguage (TOEFL) and 50 from the English as aSecond Language test (ESL).
The algorithm ob-tains a score of 74%, contrasted to that of 64% byLatent Semantic Analysis (LSA).
Terra and Clarke(Terra and Clarke, 2003) provide a comparative in-vestigation of co-occurrence frequency estimationon the performance of synonym tests.
They reportthat PMI (with a certain window size) performsbest on average.
Also, PMI-IR is useful for cal-culating semantic orientation and rating reviews(Turney, 2002).As described, PMI is one of many measures tocalculate the strength of word similarity or wordassociation (Manning and Schu?tze, 2002).
Animportant assumption is that similarity betweenwords is a consequence of word co-occurrence, orthat the proximity of words in text is indicative ofrelationship between them, such as synonymy orantonymy.
A commonly used technique to obtainword groups is distributional clustering (Baker andMcCallum, 1998).
Distributional clustering ofwords was first proposed by Pereira Tishby & Leein (Pereira et al, 1993): They cluster nouns ac-cording to their conditional verb distributions.Graphic representations for word similarityhave also been advanced by several researchers.Kageura et al (2000) propose automatic thesaurusgeneration based on a graphic representation.
Byapplying a minimum edge cut, the correspondingEnglish terms and Japanese terms are identifiedas a cluster.
Widdows and Dorow (2002) use agraph model for unsupervised lexical acquisition.A graph is produced by linking pairs of wordswhich participate in particular syntactic relation-ships.
An incremental cluster-building algorithmachieves 82% accuracy at a lexical acquisitiontask, evaluated against WordNet classes.
Anotherstudy builds a co-occurrence graph of terms anddecomposes it to identify relevant terms by dupli-cating nodes and edges (Tanaka-Ishii and Iwasaki,1996).
It focuses on transitivity: if transitivitydoes not hold between three nodes (e.g., if edgea-b and b-c exist but edge a-c does not), the nodesshould be in separate clusters.A network of words (or named entities) on theweb is investigated also in the context of the Se-mantic Web (Cimiano et al, 2004; Bekkerman andMcCallum, 2005).
Especially, a social network ofpersons is mined from the web using a search en-gine (Kautz et al, 1997; Mika, 2005; Matsuo etal., 2006).
In these studies, the Jaccard coefficientis often used to measure the co-occurrence of enti-ties.
We compare Jaccard coefficients in our eval-uations.In the research field on complex networks,543Table 1: Web counts for each word.printer print InterLaser ink TV Aquos Sharp17000000 103000000 215 18900000 69100000 1760000000 2410000 186000000Table 2: Co-occurrence matrix by web counts.printer print InterLaser ink TV Aquos Sharpprinter ?
4780000 179 4720000 4530000 201000 990000print 4780000 ?
183 4800000 8390000 86400 1390000InterLaser 179 183 ?
116 65 0 0ink 4720000 4800000 116 ?
10600000 144000 656000TV 4530000 8390000 65 10600000 ?
1660000 42300000Aquos 201000 86400 0 144000 1660000 ?
1790000Sharp 990000 1390000 0 656000 42300000 1790000 ?structures of various networks are investigated indetail.
For example, Motter (2002) targeted aconceptual network from a thesaurus and demon-strated its small-world structure.
Recently, nu-merous works have identified communities (ordensely-connected subgraphs) from large net-works (Newman, 2004; Girvan and Newman,2002; Palla et al, 2005) as explained in the nextsection.3 Word Clustering using Web Counts3.1 Co-occurrence by a Search EngineA typical word clustering task is described as fol-lows: given a set of words (nouns), cluster wordsinto groups so that the similar words are in thesame cluster 1.
Let us take an example.
As-sume a set of words is given: ????
(printer),??
(print), ????????
(InterLaser), ???
(ink), TV (TV), Aquos (Aquos), and Sharp(Sharp).
Apparently, the first four words are re-lated to a printer, and the last three words are re-lated to a TV 2.
In this case, we would like to havetwo word groups: the first four and the last three.We query a search engine3 to obtain wordcounts.
Table 1 shows web counts for each word.Table 2 shows the web counts for pairs of words.For example, we submit a query printer AND In-terLaser to a search engine, and are directed to 179documents.
Thereby, nC2 queries are necessary toobtain the matrix if we have n words.
We call Ta-ble 2 a co-occurrence matrix.We can calculate the pointwise mutual informa-1In this paper, we limit our scope to clustering nouns.
Wediscuss the extension in Section 4.2InterLaser is a laser printer made by Epson Corp. Aquosis a liquid crystal TV made by Sharp Corp.3Google (www.google.co.jp) is used in our study.tion between word w1 and w2 asPMI(w1, w2) = log2p(w1, w2)p(w1)p(w2).Probability p(w1) is estimated by fw1/N , wherefw1 represents the web count of w1 and N repre-sents the number of documents on the web.
Prob-ability of co-occurrence p(w1, w2) is estimated byfw1,w2/N where fw1,w2 represents the web countof w1 AND w2.The PMI values are shown in Table 3.
We setN = 1010 according to the number of indexedpages on Google.
Some values are inconsistentwith our intuition: Aquos is inferred to have highPMI to TV and Sharp, but also to printer.
Noneof the words has high PMI with TV.
These are be-cause the range of the word count is broad.
Gen-erally, mutual information tends to provide a largevalue if either word is much rarer than the other.Various statistical measures based on co-occurrence analysis have been proposed for es-timating term association: the DICE coefficient,Jaccard coefficient, chi-square test, and the log-likelihood ratio (Manning and Schu?tze, 2002).
Inour algorithm, we use the chi-square (?2) value in-stead of PMI.
The chi-square value is calculated asfollows: We denote the number of pages contain-ing both w1 and w2 as a.
We also denote b, c, d asfollows4.w2 ?w2w1 a b?w1 c dThereby, the expected frequency of (w1, w2) is(a+ c)(a+ b)/N .
Eventually, chi-square is calcu-lated as follows (Manning and Schu?tze, 2002).4Note that N = a + b + c + d.544Table 3: A matrix of pointwise mutual information.printer print InterLaser ink TV Aquos Sharpprinter ?
4.771 8.936 7.199 0.598 5.616 1.647print 4.771 ?
6.369 4.624 -1.111 1.799 -0.463InterLaser 8.936 6.369 ?
8.157 0.781 ??
* ??
*ink 7.199 4.624 8.157 ?
1.672 4.983 0.900TV 0.598 -1.111 0.781 1.672 ?
1.969 0.370Aquos 5.616 1.799 ??*.
4.983 1.969 ?
5.319Sharp 1.647 -0.463 ??
* 0.900 0.370 5.319 ?
* represents that the PMI is not available because the co-occurrence web count is zero, in which case we set ?
?.Table 4: A matrix of chi-square values.printer print InterLaser ink TV Aquos Sharpprinter ?
6880482.6 399.2 5689710.7 0.0* 0.0* 0.0*print 6880482.6 ?
277.8 3321184.6 176855.5 0.0* 0.0*InterLaser 399.2 277.8 ?
44.8 0.0* 0.0 0.0ink 5689710.7 3321184.6 44.8 ?
1419485.5 0.0* 0.0*TV 0.0* 176855.5 0.0* 1419485.5 ?
26803.2 70790877.6Aquos 0.0* 0.0* 0.0 0.0* 26803.2 ?
729357.7Sharp 0.0* 0.0* 0.0 0.0* 70790877.6 729357.7 ?
* represents that the observed co-occurrence frequency is below the expected value, in which case we set 0.0.Figure 1: Examples of Newman clustering.
?2(w1, w2)= N ?
(a?
d?
b?
c)2(a + b)?
(a + c)?
(b + d)?
(c + d)However, N is a huge number on the web andsometimes it is difficult to know exactly.
There-fore we regard the co-occurrence matrix as a con-tingency table:b?
=?w?W ;w 6=w2fw1,w , c?
=?w?W ;w 6=w1fw2,w;d?
=?w,w?
?W ;w and w?
6=w1 nor w2fw,w?
, N ?
=?w,w??Wfw,w?
,where W represents a given set of words.
Thenchi-square (within the word list W ) is defined as?2W (w1, w2) =N ?
?
(a?
d?
?
b?
?
c?
)2(a + b?)?
(a + c?)?
(b?
+ d?)?
(c?
+ d?)
.We should note that ?2W depends on a wordset W .
It calculates the relative strength of co-occurrences.
Table 4 shows the ?2W values.
Aquoshas high values only with TV and Sharp as ex-pected.3.2 Clustering on Co-occurrence GraphRecently, a series of effective graph clusteringmethods has been advanced.
Pioneering work thatspecifically emphasizes edge betweenness wasdone by Girvan and Newman (2002): we call themethod as GN algorithm.
Betweenness of an edgeis the number of shortest paths between pairs ofnodes that run along it.
Figure 1 (i) shows thattwo ?communities?
(in Girvan?s term), i.e.
{a,b,c}and {d,e,f,g}, which are connected by edge c-d.Edge c-d has high betweenness because numerousshortest paths (e.g., from a to d, from b to e, .
.
.
)traverse the edge.
The graph is likely to be sepa-rated into densely connected subgraphs if we cutthe high betweenness edge.The GN algorithm is different from the mini-mum edge cut.
For (i), the results are identical: Bycutting edge c-d, which is a minimum edge cut, wecan obtain two clusters.
However in case of (ii),there are two candidates for the minimum edgecut, whereas the highest betweenness edge is stillonly edge c-d. Girvan et al (2002) shows that thisclustering works well to various networks frombiological to social networks.
Numerous studieshave been inspired by that work.
One prominenteffort is a faster variant of GN algorithm (New-man, 2004), which we call Newman clustering in545Figure 2: An illustration of graph-based wordclustering.this paper.In Newman clustering, instead of explicitly cal-culating high-betweenness edges (which is com-putationally demanding), an objective function isdefined as follows:Q =?i(eii ?
(?jeij)2)(1)We assume that we have separate clusters, and thateij is the fraction5 of edges in the network thatconnect nodes in cluster i to those in cluster j.The term eii denotes the fraction of edges withinthe clusters.
The term?j eij represents the ex-pected fraction of edges within the cluster.
If a par-5We can calculate eij using the number of edges betweencluster i and j divided by the number of all edges.Figure 3: A word graph for 88 Japanese words.ticular division gives no more within-communityedges than would be expected by random chance,then we would obtain Q = 0.
In practice, valuesgreater than about 0.3 appear to indicate signifi-cant group structure (Newman, 2004).Newman clustering is agglomerative (althoughwe can intuitively understand that a graph with-out high betweenness edges is ultimately ob-tained).
We repeatedly join clusters together inpairs, choosing at each step the joint that providesthe greatest increase in Q.
Currently, Newmanclustering is one of the most efficient methods forgraph-based clustering.The illustration of our algorithm is shown inFig.
2.
First, we obtain web counts among a givenset of words using a search engine.
Then PMI orthe chi-square values are calculated.
If the value isabove a certain threshold6, we invent an edge be-tween the two nodes.
Then, we apply graph clus-tering and finally identify groups of words.
This il-lustration shows that the chi-square measure yieldsthe correct clusters.The algorithm is described in Fig.
4.
The pa-rameters are few: a threshold dthre for a graph and,optionally, the number of clusters nc.
This enableseasy implementation of the algorithm.
Figure 3is a small network of 88 Japanese words obtainedthrough 3828 search queries.
We can see that someparts in the graph are densely connected.4 Experimental ResultsThis section addresses evaluation.
Two sets ofword groups are used for the evaluation: one isderived from documents on a web directory; an-other is from WordNet.
We first evaluate the co-6In this example, 4.0 for PMI and 200 for ?2.546?
?1.
Input A set of words is given.
The number of wordsis denoted as n.2.
Obtain frequencies Put a query for each pair ofwords to a search engine, and obtain a co-occurrence matrix.
Then calculate the chi-squarematrix (alternatively a PMI matrix, or a Jaccardmatrix.)3.
Make a graph Set a node for each word, and anedge to a pair of nodes whose ?2 value is above athreshold.
The threshold is determined so that thenetwork density (the number of edges divided bynC2) is dthre.4.
Apply Newman clustering Initially set each nodeas a cluster.
Then merge two clusters repeatedlyso that Q is maximized.
Terminate if Q doesnot increase anymore, or when a given numberof clusters nc is obtained.
(Alternatively, applyaverage-link hierarchical clustering.)5.
Output Output groups of words.?
?Figure 4: Our algorithm for word clustering.occurrence measures, then we evaluate the cluster-ing methods.4.1 Word Groups from an Open DirectoryWe collected documents from the Japanese OpenDirectory (dmoz.org/World/Japanese).
Thedmoz japanese category contains about 130,000documents and more than 10,000 classes.
Wechose 9 categories out of the top 12 categories:art, sports, computer, game, society, family, sci-ence, and health.
We crawled 1000 documents foreach category, i.e., 9000 documents in all.For each category, a word group is obtainedthrough the procedure in Fig.
5.
We considerthat the specific words to a category are relevantto some extent, and that they can therefore be re-garded as a word group.
Examples are shown inTable 5.
In all, 90 word sets are obtained andmerged.
We call the word set DMOZ-J data.Our task is, given 90 words, to cluster the wordsinto the correct nine groups.
Here we investigatewhether the correct nine words are selected foreach word using the co-occurrence measure.
Wecompare pointwise mutual information (PMI), theJaccard coefficient (Jaccard), and chi-square (?2).We chose these methods for comparison becausePMI performs best in (Terra and Clarke, 2003).The Jaccard coefficient is often used in social net-work mining from the web.
Table 7 shows the pre-cision of each method.
Experiments are repeatedfive times.
We keep each method that outputs the?
?1.
For each category, crawl 1000 documents ran-domlya2.
Apply the Japanese morphological analysis sys-tem ChaSen (Matsumoto et al, 2000) to the doc-uments.
Calculate the score of each word w incategory c similarly to TF-IDF:score(w, c) = fc(w)?
log(Nall/fall(w))where fc denotes the document frequency ofword w in category c, Nall denotes the number ofall documents, and fall(w) denotes the frequencyof word w in all documents.3.
For each category, the top 10 words are selectedas the word group.aWe first get al urls, sort them, and select a samplerandomly.?
?Figure 5: Procedure for obtaining word groups fora category.Table 7: Precision for DMOZ-J set.PMI Jaccard ?2Mean 0.415 0.402 0.537Min 0.396 0.376 0.493Max 0.447 0.424 0.569SD 0.020 0.020 0.032highest nine words for each word, groups of tenwords.
Therefore, recall is the same as the preci-sion.
From the table, the chi-square performs best.PMI is slightly better than the Jaccard coefficient.4.2 Word Groups from WordNetNext, we make a comparison using WordNet 7.
Byextracting 10 words that have the same hypernym(i.e.
coordinates), we produce a word group.
Ex-amples are shown in Table 6.
Nine word groupsare merged into one, as with DMOZ-J.
The exper-iments are repeated 10 times.
Table 8 shows theresult.
Again, the chi-square performs best amongthe methods that were compared.Detailed analyses of the results revealed thatword groups such as bacteria and diseases are clus-tered correctly.
However, word groups such ascomputers (in which homepage, server and clientare included) are not well clustered: these wordstend to be polysemic, which causes difficulty.4.3 Evaluation of ClusteringWe compare two clustering methods: Newmanclustering and average-link agglomerative cluster-7We use a partly-translated version of WordNet.547Table 5: Examples of word groups from DMOZ-J.category specific words to a category as a word group???
(art) ??
(gallery),??
(artwork),??
(theater),????
(saxophone),??
(verse),???
(live con-cert),???
(guitar),??
(performance),???
(ballet),??
(personal exhibition)????????(recreation)??
(raising), ??
(poult), ?????
(hamster), ???
(travel diary), ????
(national park),??
(brewing),??
(boat race),??
(competition),???
(fishing pond)??
(health) ??
(illness),??
(patient),??
(myositis),??
(surgery),??
(dialysis),?????
(steroid),??
(test),??
(medical ward),???
(collagen disease),??
(clinic)Table 6: Examples of word groups from WordNet.hypernym hyponyms as a word group??
(gem) ?????
(amethyst),??????
(aquamarine),??????
(diamond),?????
(emer-ald),???????
(moonstone),?????
(peridot),???
(ruby),?????
(sapphire),????
(topaz),?????
(tourmaline)??
(academic field) ????
(natural science),??
(mathematics),??
(agronomics),???
(architectonics),???
(geology),???
(psychology),????
(computer science),????
(cognitive science),???
(sociology),???
(linguistics)???
(drink) ??
(milk),?????
(alcohol),????
(cooling beverage),????
(carbonated beverage),????
(soda),???
(cocoa),????????
(fruit juice),????
(coffee),??
(tea),?????????
(mineral water)Table 8: Precision of WordNet set.PMI Jaccard ?2Mean 0.549 0.484 0.584Min 0.473 0.415 0.498Max 0.593 0.503 0.656SD 0.037 0.027 0.048Table 9: Precision, recall and the F-measure foreach clustering.PMI Jaccard ?2Average precision 0.633 0.603 0.486-link recall 0.102 0.101 0.100F-measure 0.179 0.173 0.164Newman precision 0.751 0.739 0.546recall 0.103 0.103 0.431F-measure 0.182 0.181 0.480ing, which is often used in word clustering.A word co-occurrence graph is created usingPMI, Jaccard, and chi-square measures.
Thethreshold is determined so that the network den-sity dthre is 0.3.
Then, we apply clustering to ob-tain nine clusters; nc = 9.
Finally, we comparethe resultant clusters with the correct categories.Clustering results for DMOZ-J sets are shownin Table 9.
Newman clustering produces higherprecision and recall.
Especially, the combinationof chi-square and Newman is the best in our ex-periments.5 DiscussionIn this paper, the scope of co-occurrence isdocument-wide.
One reason is that major com-mercial search engines do not support a type ofquery w1 NEAR w2.
Another reason is in (Terraand Clarke, 2003) document-wide co-occurrencesperform comparable to other Windows-based co-occurrences.Many types of co-occurrence exist other thannoun-noun.
We limit our scope to noun-nounco-occurrences in this paper.
Other types of co-occurrence such as verb-noun can be investigatedin future studies.
Also, co-occurrence for thesecond-order similarity can be sought.
Becauseweb documents are sometimes difficult to analyze,we keep our algorithm as simple as possible.
An-alyzing semantic relations and applying distribu-tional clustering is another goal for future work.A salient weak point of our algorithm is thenumber of necessary queries allowed to a searchengine.
For obtaining a graph of n words, O(n2)queries are required, which discourages us fromundertaking large experiments.
However some de-vices are possible: if we analyze the texts of thetop retrieved pages by query w, we can guess whatwords are likely to co-occur with w. This prepro-cessing seems promising at least in social networkextraction: we can eliminate 85% of queries inthe 500 nodes case while retaining more than 90%precision (Asada et al, 2005).In our evaluation, the chi-square measure per-formed well.
One reason is that the PMI performsworse when a word group contains rare or frequentwords, as is generally known for mutual informa-tion measure (Manning and Schu?tze, 2002).
An-other reason is that if we put one word and twowords to a search engine, the result might be in-consistent.
In an extreme case, the web count ofw1 is below the web count of w1ANDw2.
This548phenomenon depends on how a search engine pro-cesses AND operator, and results in unstable val-ues for the PMI.
On the other hand, our methodby the chi-square uses a co-occurrence matrix as acontingency table.
For that reason, it suffers lessfrom the problem.
Other statistical measures suchas the likelihood ratio are also applicable.6 ConclusionThis paper describes a new approach for wordclustering using a search engine.
The chi-squaremeasure is used to overcome the broad range ofword counts for a given set of words.
We also ap-ply recently-developed Newman clustering, whichyields promising results through our evaluations.Our algorithm has few parameters.
Therefore,it can be used easily as a baseline, as suggested by(Lapata and Keller, 2004).
New words are gener-ated day by day on the web.
We believe that toautomatically identify new words and obtain wordgroups potentially enhances many NLP applica-tions.ReferencesYohei Asada, Yutaka Matsuo, and Mitsuru Ishizuka.2005.
Increasing scalability of researcher networkextraction from the web.
Journal of Japanese Soci-ety for Artificial Intelligence, 20(6).D.
Baker and A. McCallum.
1998.
Distributionalclustering of words for text classification.
In Proc.SIGIR-98.M.
Baroni and M. Ueyama.
2005.
Building general-and special-purpose corpora by web crawling.
InProc.
NIJL International Workshop on LanguageCorpora.R.
Bekkerman and A. McCallum.
2005.
Disambiguat-ing web appearances of people in a social network.In Proc.
WWW 2005.M.
Cafarella and O. Etzioni.
2005.
A search engine fornatural language applications.
In Proc.
WWW2005.P.
Cheng, W. Lu, J. Teng, and L. Chien.
2004.
Cre-ating multilingual translation lexicons with regionalvariations using web corpora.
In Proc.
ACL 2004,pages 534?541.P.
Cimiano, S. Handschuh, and S. Staab.
2004.
To-wards the self-annotating web.
In Proc.
WWW2004,pages 462?471.J.
Curran.
2002.
Ensemble methods for automatic the-saurus extraction.
In Proc.
EMNLP 2002.I.
Dhillon, S. Mallela, and R. Kumar.
2002.
Enhancedword clustering for hierarchical text classification.In Proc.
KDD-2002, pages 191?200.Michelle Girvan and M. E. J. Newman.
2002.
Com-munity structure in social and biological networks.Proceedings of National Academy of Sciences USA,99:8271?8276.C.
Huang, S. Chuang, and L. Chien.
2004.
Categoriz-ing unknown text segments for information extrac-tion using a search result mining approach.
In Proc.IJCNLP 2004, pages 576?586.K.
Kageura, K. Tsuji, and A. Aizawa.
2000.
Auto-matic thesaurus generation through multiple filter-ing.
In Proc.
COLING 2000.H.
Kautz, B. Selman, and M. Shah.
1997.
The hiddenWeb.
AI magazine, 18(2):27?35.F.
Keller, M. Lapata, and O. Ourioupina.
2002.
Usingthe web to overcome data sparseness.
In EMNLP-02, pages 230?237.A.
Kilgarriff.
2003.
Introduction to the special issueon the web as corpus.
Computer Linguistics, 29(3).M.
Lapata and F. Keller.
2004.
The web as a base-line: Evaluating the performance of unsupervisedweb-based models for a range of nlp tasks.
In Proc.HLT-NAACL 2004, pages 121?128.H.
Li and N. Abe.
1998.
Word clustering and dis-ambiguation based on co-occurrence data.
In Proc.COLING-ACL98.C.
D. Manning and H. Schu?tze.
2002.
Foundationsof statistical natural language processing.
The MITPress, London.Y.
Matsumoto, A. Kitauchi, T. Yamashita, Y. Hi-rano, H. Matsuda, K. Takaoka, and M. Asahara.2000.
Morphological analysis system ChaSen ver-sion 2.2.1 manual.
Technical report, NIST.Y.
Matsuo, J. Mori, M. Hamasaki, H. Takeda,T.
Nishimura, K. Hasida, and M. Ishizuka.
2006.POLYPHONET: An advanced social network ex-traction system.
In Proc.
WWW 2006.P.
Mika.
2005.
Flink: Semantic web technology for theextraction and analysis of social networks.
Journalof Web Semantics, 3(2).K.
Miura, Y. Tsuruoka, and J. Tsujii.
2004.
Auto-matic acquisition of concept relations from web doc-uments with sense clustering.
In Proc.
IJCNLP04.A.
Motter, A. de Moura, Y. Lai, and P. Dasgupta.
2002.Topology of the conceptual network of language.Physical Review E, 65.M.
Newman.
2004.
Fast algorithm for detecting com-munity structure in networks.
Phys.
Rev.
E, 69.549G.
Palla, I. Derenyi, I. Farkas, and T. Vicsek.
2005.Uncovering the overlapping community structure ofcomplex networks in nature and society.
Nature,435:814.F.
Pereira, N. Tishby, and L. Lee.
1993.
Distributionalclustering of English words.
In Proc.
ACL93, pages183?190.K.
Tanaka-Ishii and H. Iwasaki.
1996.
Clustering co-occurrence graph using transitivity.
In Proc.
16th In-ternational Conference on Computational Linguis-tics, pages 680?585.E.
Terra and C. Clarke.
2003.
Frequency estimatesfor statistical word similarity measures.
In Proc.HLT/NAACL 2003.P.
Turney.
2001.
Mining the web for synonyms: PMI-IR versus LSA on TOEFL.
In Proc.
ECML-2001,pages 491?502.P.
Turney.
2002.
Thumbs up or thumbs down?
seman-tic orientation applied to unsupervised classificationof reviews.
In Proc.
ACL?02, pages 417?424.P.
Turney.
2004.
Word sense disambiguation by webmining for word co-occurrence probabilities.
InProc.
SENSEVAL-3.D.
Widdows and B. Dorow.
2002.
A graph model forunsupervised lexical acquisition.
In Proc.
COLING2002.550
