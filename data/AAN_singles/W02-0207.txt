Annotating Semantic Consistency of Speech Recognition HypothesesIryna Gurevych, Robert Porzel and Michael StrubeEuropean Media Laboratory GmbHSchlo?-Wolfsbrunnenweg 33D-69118 Heidelberg, Germanye-mail: {gurevych,porzel,strube}@eml.villa-bosch.deAbstractRecent work on natural language processingsystems is aimed at more conversational,context-adaptive systems in multiple do-mains.
An important requirement for such asystem is the automatic detection of the do-main and a domain consistency check of thegiven speech recognition hypotheses.
Wereport a pilot study addressing these tasks,the underlying data collection and investi-gate the feasibility of annotating the data re-liably by human annotators.1 IntroductionThe complete understanding of naturally oc-curring discourse is still an unsolved task incomputational linguistics.
Several large re-search efforts are underway to build multi-domain and multimodal information systems,e.g.
the DARPA Communicator Program1, theSmartKom research framework2 (Wahlster etal., 2001), the AT&T interactive speech andmultimodal user interface program3.Dialogue systems which deal with com-plex dialogues require the interaction of multi-ple knowledge sources, e.g.
domain, discourseand user model (Flycht-Eriksson, 1999).
Fur-thermore NLP systems have to adapt to differ-ent environments and applications.
This canonly be achieved if the system is able to de-termine how well a given speech recognitionhypothesis (SRH) fits within the respectivedomain model and what domain should beconsidered by the system currently in focus.The purpose of this paper is to developan annotation scheme for annotating a corpusof SRH with information on semantic consis-tency and domain specificity.
We investigate1http://fofoca.mitre.org2http://www.smartkom.com3http://www.research.att.com/news/2002/January/ISMUI.htmlthe feasibility of an automatic solution by firstlooking at how reliably human annotators cansolve the task.The structure of the paper is as follows:Section 2 gives an overview of the domainmodeling component in the SmartKom system.In Section 3 we report on the data collectionunderlying our study.
A description of thesuggested annotation scheme is given inSection 4.
Section 5 presents the results of anexperiment in which the reliability of humanannotations is investigated.2 Domain Modeling in SmartKomThe SmartKom research project (a consortiumof twelve academic and industrial partners)aims at developing a multi-modal and multi-domain information system.
Domains includecinema information, home electronic devicecontrol, etc.
A central goal is the developmentof new computational methods fordisambiguating different modalities onsemantic and pragmatic levels.The information flow in SmartKom isorganized as follows: On the input side theparser picks an N-best list of hypotheses out ofthe speech recognizer?s word lattice (Oerderand Ney, 1993).
This list is sent to the mediafusion component and then handed over to theintention recognition component.The main task of intention recognitionin SmartKom is to select the best hypothesisfrom the N-best list produced by the parser.This is then sent to the dialogue managementcomponent for computing an appropriateaction.
In order to find the best hypothesis, theintention recognition module consults anumber of other components involved inlanguage, discourse and domain analysis andrequests confidence scores to make anappropriate decision (s. Fig.
1).Tasks of the domain modelingcomponent are:Philadelphia, July 2002, pp.
46-49.
Association for Computational Linguistics.Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,?
to supply a confidence score on theconsistency of SRH with respect to thedomain model;?
to detect the domain currently in focus.Figure 1.
Information flowThese tasks are inherently related to eachother: It is possible to assign SRH to certaindomains only if they are consistent with thedomain model.
On the other hand, aconsistency score can only be useful when it isgiven with respect to certain domains.3 DataWe consider semantic consistency scoring anddomain detection a classification task.
Thequestion is whether it is feasible to solve thistask automatically.
As a first step towards ananswer we reformulate the problem: automaticclassification of SRH is possible only ifhumans are able to do that reliably.3.1 Data CollectionIn order to test the reliability of suchannotations we collected a corpus of SRH.
Thedata collection was conducted by means of ahidden operator test (Rapp and Strube, 2002).In the test the SmartKom system wassimulated.
We had 29 subjects prompted to saycertain inputs in 8 dialogues.
1479 turns wererecorded.
Each user-turn in the dialoguecorresponded to a single intention, e.g.
routerequest or sights information request.3.2 Data PreprocessingThe data obtained from the hidden operatortests had to be prepared for our study to com-pose a corpus with N-best SRH.
For this pur-pose we sent the audio files to the speech rec-ognizer.
The input for the domain modelingcomponent, i.e.
N-best lists of SRH were re-corded in log-files and then processed with acouple of Perl scripts.
The final corpus con-sisted of ca.
2300 SRH.
This corresponds to ca.1.55 speech recognition hypotheses per user?sturn.The SRH corpus was then transformedinto a set of annotation files which could beread into MMAX, the annotation tool adoptedfor this task (Mueller and Strube, 2001).4 Annotation SchemeFor our study, a markable, i.e.
an expression tobe annotated, is a single SRH.
The annotatorsas well as the domain modeling component inSmartKom currently do not take the dialoguecontext into account and do not performcontext-dependent analysis.
Hence, wepresented the markables completely out ofdialogue order  and thus prevented theannotators from interpreting SRH context-dependently.4.1 Semantic ConsistencyIn the first step, the annotators had to classifymarkables with respect to semantic consis-tency.
Semantic consistency is defined as well-formedness of an SRH on an abstract semanticlevel.
We differentiate three classes of seman-tic consistency: consistent, semi-consistent, orinconsistent.
First, all nouns and verbs con-tained in the hypothesis are extracted and cor-responding concepts are retrieved from alemma-concept dictionary (lexicon) suppliedfor the annotators.
The decision regarding con-sistency, semi-consistency and inconsistencyhas to be done on the basis of evaluating theset of concepts corresponding to the individualhypothesis.?
Consistent means that all concepts aresemantically related to each other, e.g.
"ich moechte die kuerzeste Route"4 ismapped to the concepts "self", "wish","route" all of which are related to eachother.
Therefore the hypothesis is con-sidered consistent.?
The label semi-consistent is used if atleast a fragment of the hypothesis is4 I?d like the shortest route.LanguageparsingDiscoursemodelDomainmodelIntentionrecognitionMedia FusionDialogueManagementmeaningful.
For example, the hypothe-sis "ich moechte das Video sind"5 isconsidered semi-consistent as thefragment "ich moechte das Video", i.e.a set of corresponding concepts "self","want", "video" is semantically well-formed.?
Inconsistent hypotheses are thosewhose conceptual mappings are notsemantically related within the domainmodel.
E.g.
"ich wuerde die Karte jaWiedersehen"6 is conceptualized as"self", "map", "parting".
This set ofconcepts does not semantically makesense and the hypothesis should be re-jected.4.2 Domain DetectionOne of our considerations was that it is princi-pally not always feasible to detect domainsfrom an SRH.
This is because the output ofspeech recognition is often corrupt, whichmay, in many cases, lead to false domain as-signments.
We argue that domain detection isdependent on the semantic consistency score.Therefore, according to our annotation schemeno domain analysis should be given to the se-mantically inconsistent SRH.If the hypothesis is considered eitherconsistent or semi-consistent, certain domainswill be assigned to it.
The list of SmartKomdomains for this study is finite and includes thefollowing: route planning, sights information,cinema information, electronic program guide,home electronic device control, personal assis-tance, interaction management, small-talk andoff-talk.In some cases multiple domains can beassigned to a single markable.
The reason isthat some domains are inherently so close toeach other, e.g.
cinema information and elec-tronic program guide, that the distinction canonly be made when the context is taken intoaccount.
As this is not the case for our studywe allow for the specification of multiple do-mains per SRH.5 I?d like the video are.6 I would the map yes good-bye.5 Reliability of Annotations5.1 The Kappa StatisticTo measure the reliability of annotations weused the Kappa statistic (Carletta, 1996).The value of Kappa statistic (K) for  se-mantic consistency in our experiment was0.58, which shows that there was not a highlevel of agreement between annotators7.
In thefield of content analysis, where the Kappastatistic originated,  K>0.8 is usually taken toindicate good reliability, 0.68<K<0.8 allows todraw tentative conclusions.The distribution of semantic consistencyclasses and domain assignments is given inFig.
2.Domain %Route planning 33,1Sights info 13,3Cinema info 10,8Electr.
Program guide 15,9Home device control 12,0Personal assistance 1,1Interaction Management 13,1Other 0,7Figure 2.
Distribution of Classes5.2 Discussion of the resultsOne reason for the relatively low coefficient ofagreement between annotators could be a smallnumber of annotators (two) as compared torather fine distinction between the classes in-consistent vs. semi-consistent and semi-consistent vs. consistent respectively.Another reason arises from the analysisof disagreements among annotators.
We findmany annotation errors caused by the fact thatthe annotators were not able to interpret theconceptualized SRH correctly.
In spite of thefact that we emphasized the necessity of care-7Results on the reliability of domain assignmentsare not the subject of the present paper and will bepublished elsewhere.Type %Consistent 51Semi-consistent 10,3Inconsistent 38,7ful examination for high-quality annotations,the annotators tended to take functional wordslike prepositions into account.
According toour annotation scheme, however, they had tobe ignored during the analysis.5.3 Revisions to the annotation schemeAs already noted, one possible reason for dis-agreements among annotators is a rather finedistinction between the classes inconsistent vs.semi-consistent and semi-consistent vs. consis-tent.
We had difficulties in defining strict crite-ria for separating semi-consistent as a class onits own.
The percentage of its use is rather lowas compared to the other two and amounts to10.3% on average.A possible solution to this problemmight be to merge the class semi-consistentwith either consistent or inconsistent.
We con-ducted a corresponding experiment with theavailable annotations.In the first case we merged the classesinconsistent and semi-consistent.
We then ranthe Kappa statistic over the data and obtainedK=0.7.
We found this to be a considerableimprovement as compared to earlier K=0.58.In the second case we merged theclasses consistent and semi-consistent.
TheKappa statistic with this data amounted to0.59, which could not be considered an im-provement.6 Concluding RemarksIn this work we raised the question whether itis possible to reliably annotate speech recogni-tion hypotheses with information about seman-tic consistency and domain specificity.
Themotivation for that was to find out whether it isfeasible to develop and evaluate a computerprogram addressing the same task and imple-menting the algorithm reflected in the annota-tion scheme.We found that humans principally hadproblems in looking solely at the conceptual-ized speech recognition hypotheses.
This,however, should not be a problem for a ma-chine where the word-to-concept mapping isdone automatically and all so-called functionwords are discarded.
In the future it would beinteresting to have humans annotate not speechrecognition hypotheses per se, but only theirautomatically generated conceptual mappings.Another finding was that the originallyproposed annotation scheme does not allow fora high level of agreement between human an-notators with respect to semantic consistency.Eliminating the class semi-consistent led us,however, to a considerably better reliability ofannotations.We consider this study as a first attemptto show the feasibility of determining semanticconsistency of the output of the speech recog-nizer.
We plan to integrate the results into thedomain modeling component and conductfurther experiments on semantic consistencyand domain detection.AcknowledgementsThe work presented in this paper was con-ducted within the SmartKom project partlyfounded by the German Ministry of Researchand Technology under grant 01IL95I7 and bythe Klaus Tschira Foundation.ReferencesCarletta, J.
1996.
Assessing agreement on classifi-cation tasks: The kappa statistic.
ComputationalLinguistics, 22 (2):249-254.Flycht-Eriksson, A.
1999.
A Survey of KnowledgeSources in Dialogue Systems.
In: Proc.
ofIJCAI'99 Workshop on Knowledge and Reason-ing in Practical Dialogue Systems.
Stockholm,Sweden, pp.
41-48.Mueller, C., Strube, M. 2001.
Annotating anaphoricand bridging expressions with MMAX.
In:Proc.
of the 2nd SIGdial Workshop on Discourseand Dialogue.
Aalborg, Denmark, 2001, pp.
90-95.Oerder, M., Ney, H. 1993.
Word Graphs: An Effi-cient Interface between Continuous SpeechRecognition and Language Understanding.In:Proc.
of the International Conf.
on Acoustics,Speech and Signal Processing.
IEEE SignalProcessing Society.Rapp, S., Strube, M. 2002.
An iterative data collec-tion approach for multimodal dialogue sys-tems.
In: Proc.
of the 3rd International Con-ference on Language Resources and Evalua-tion.
Las Palmas, Canary Islands, Spain.
Toappear.Wahlster, W., Reithinger, N., Blocher, A.
2001.SmartKom: Multimodal Communication with aLife-Like Character.
In: Proc.
of Eurospeech2001.
Aalborg, Danemark, pp.
1547-1550.
