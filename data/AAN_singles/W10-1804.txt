Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 29?37,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsAgile Corpus Annotation in Practice:An Overview of Manual and Automatic Annotation of CVsBea Alex Claire Grover Rongzhou ShenSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UKMijail KabadjovJoint Research CentreEuropean CommissionVia E. Fermi 2749, Ispra (VA), ItalyContact: balex@staffmail.ed.ac.ukAbstractThis paper describes work testing agiledata annotation by moving away from thetraditional, linear phases of corpus cre-ation towards iterative ones and by recog-nizing the potential for sources of error oc-curring throughout the annotation process.1 IntroductionAnnotated data sets are an important resources forvarious research fields, including natural languageprocessing (NLP) and text mining (TM).
While thedetection of annotation inconsistencies in differentdata sets has been investigated (e.g.
Nova?k andRaz?
?mova?, 2009) and their effect on NLP perfor-mance has been studied (e.g.
Alex et al 2006), verylittle work has been done on deriving better methodsof annotation as a whole process in order to maxi-mize both the quality and quantity of annotated data.This paper describes our annotation project in whichwe tested the relatively new approach of agile cor-pus annotation (Voormann and Gut, 2008) of mov-ing away from the traditional, linear phases of cor-pus creation towards iterative ones and of recogniz-ing the fact that sources of error can occur through-out the annotation process.We explain agile annotation and discuss relatedwork in Section 2.
Section 3 describes the en-tire annotation process and all its aspects.
We pro-vide details on the data collection and preparation,the annotation tool, the annotators and the annota-tion phases.
Section 4 describes the final annota-tion scheme and Section 5 presents inter-annotator-agreement (IAA) figures measured throughout theannotation.
In Section 6, we summarize the per-formance of the machine-learning (ML)-based TMcomponents which were trained and evaluated on theannotated data.
We discuss our findings and con-clude in Section 7.2 Background and Related WorkThe manual and automatic annotation work de-scribed in this paper was conducted as part of theTXV project.
The technology used was basedon TM components that were originally developedfor the biomedical domain during its predecessorproject (Alex et al, 2008b).
In TXV we adaptedthe tools to the recruitment domain in a short timeframe.
The aim was to extract key information fromcurricula vitae (CVs) for matching applicants to jobadverts and to each other.
The TM output is visu-alized in a web application with search navigationthat captures relationships between candidates, theirskills and organizations etc.
This web interface al-lows recruiters to find hidden information in largevolumes of unstructured text.Both projects were managed using agile, test-driven software development, i.e.
solutions werecreated based on the principles of rapid-prototypingand iterative development cycles of deliverable ver-sions of the TM system and the web application.1The same principles were also applied to otherproject work, including the manual annotation.
Theaim of this annotation was to produce annotated datafor training ML-based TM technology as well asevaluating system components.Collecting data, drawing up annotation guidelinesand getting annotators to annotate this data in se-quential steps is similar to the waterfall model insoftware engineering (Royce, 1970).
This approachcan be inefficient and costly if annotators unknow-ingly carried out work that could have been avoidedand it can lead to difficulties if at the end of the pro-cess the requirements no longer match the annota-tions.
Instead we applied agile software engineeringmethods to the process of creating annotated data.This is a relatively recent philosophy in software1The agile software development principles are explained inthe Agile Manifesto: http://agilemanifesto.org/29Figure 1: The phases of traditional corpus creation (a) and the cyclic approach in agile corpus creation (b).Reproduction of Figure 2 in Voormann and Gut (2008).development which was inspired to overcome thedrawbacks of the waterfall model.
The idea of ap-plying agile methods to corpus creation and annota-tion was first inspired by Voormann and Gut (2008)but was not tested empirically.
Cyclic annotationwas already proposed by Atkins et al (1992) andBiber (1993) with a focus on data creation ratherthan data annotation.
In this paper, we describe away of testing this agile annotation in practice.The idea behind an agile annotation process isto produce useable manually annotated data fast aswell as discover and correct flaws in either the an-notation guidelines or the annotation setup early on.Voormann and Gut (2008) propose query-driven an-notation, a cyclic corpus creation and annotationprocess that begins with formulating a query.
Themain advantages of this approach are:?
The annotation scheme evolves over timewhich ensures that annotations are consistentand remain focussed on the research that iscarried out.
An iterative annotation processtherefore improves the annotation guidelinesbut keeps the annotations suitable to the rele-vant research questions.?
Problems with the annotation guidelines, er-rors in the annotation and issues with the setupbecome apparent immediately and can be cor-rected early on.
This can avoid difficulties lateron and will save time and cost.?
Some annotation data is available early on.Voormann and Gut compare the cyclical approachin agile annotation to traditional linear-phrase cor-pus creation depicted in Figure 1.
In the followingsection we describe the annotation process in ourproject which followed the principles of agile cor-pus creation.3 Annotation ProcessThis section provides an overview of all aspect in-volved in the annotation of a data set of CVs forvarious types of semantic information useful to re-cruiters when analysing CVs and placing candidateswith particular jobs or organizations.
We provideinformation on the data collection, the documentpreparation, the annotation tool and the annotationprocess following agile methods.3.1 Data CollectionWe automatically collected a set of CVs of soft-ware engineers and programmers which are publiclyavailable online.
This data set was created by firstlyquerying Google using the Google API2 for worddocuments containing either the terms ?CV?, ?re-sume?
or ?curriculum vitae?
as well as the terms?developer?, ?programmer?
or ?software?
but ex-cluding documents containing the word ?template?or ?sample?.
Furthermore, the query was restrictedto a 3-month period from 30/03 to 30/06/2008.3We automatically downloaded the Word docu-ments returned by this query, resulting in a pool of1,000 candidate CVs available for annotation.
Wesplit these documents randomly into a TRAIN, a DE-VTEST and a TEST set in a ratio of approximately64:16:20.
We used the annotated TRAIN data fortraining ML-based models and deriving rules andthe DEVTEST data for system development and op-timization.
We set aside the blind TEST set forevaluating the final performance of our named en-tity recognition (NER) and relation extraction (RE)2http://code.google.com/apis/ajaxsearch3The exact Google query is: ?
(CV OR resume OR ?cur-riculum vitae?)
AND (developer OR programmer OR soft-ware) AND filetype:doc AND -template AND -sample ANDdaterange:2454466-2454647?.30CV data setSet TRAIN DEVTEST TEST ALLFiles 253 72 78 403Annotations 279 84 91 454Table 1: Number of files and annotated files in eachsection of the CV data set.components (see Section 6).The final manually annotated data set contains403 files, of which 352 are singly and 51 doubly an-notated, resulting in an overall total of 454 annota-tions (see Table 1).
This does not include the filesused during the pilot annotation.
The doubly an-notated CVs were used to determine inter-annotatoragreement (IAA) in regular intervals (see Section 5).Some of the documents in the pool were not gen-uine CVs but either job adverts or CV writing ad-vice.
We let the annotators carry out the filteringprocess of only choosing genuine CVs of softwaredevelopers and programmers for annotation and re-ject but record any documents that did not fit this cat-egory.
The annotators rejected 99 files as being ei-ther not CVs at all (49) or being out-of-domain CVsfrom other types of professionals (50).
Therefore,just over 50% of the documents in the pool wereused up during the annotation process.3.2 Document PreparationBefore annotation, all candidate CVs were then au-tomatically converted from Word DOC format toOpenOffice ODT as well as to Acrobat PDF formatin a batch process using OpenOffice macros.
Theresulting contents.xml files for each ODT version ofthe documents contain the textual information of theoriginal CVs.
An XSLT stylesheet was used to sim-plify this format to a simpler in-house XML format,as the input into our pre-processing pipeline.
We re-tained all formatting and style information in spanelements for potential later use.The pre-processing includes tokenization, sen-tence boundary detection, part-of-speech tagging,lemmatization, chunking, abbreviation detectionand rule-based NER for person, location names anddates.
This information extraction system is a mod-ular pipeline built around the LT-XML24 and LT-TTT25 toolsets.
The NER output is stored as stand-4http://www.ltg.ed.ac.uk/software/ltxml25http://www.ltg.ed.ac.uk/software/lt-ttt2off annotations in the XML.
These pre-processedfiles were used as the basis for annotation.3.3 Annotation ToolFor annotating the text of the CVs we choseMMAX2, the Java-based open source tool (Mu?llerand Strube, 2006).6 MMAX2 supports multiple lev-els of annotation by way of stand-off annotation.As a result MMAX2 creates one separate file foreach level of annotation for each given base data file.Only the annotation level files get edited during theannotation phase.
The base data files which con-tain the textual information of the documents do notchange.
In our project, we were interested in threelevels of annotation, one for named entities (NEs),one for zones and one for relations between NEs.The MMAX2 GUI allows annotators to mark upnested structures as well as intra- and inter-sententialrelations.
Both of these functionalities were crucialto our annotation effort.As the files used for annotation already con-tained some NEs which were recognized automat-ically using the rule-based NER system and storedin standoff XML, the conversion into and out of theMMAX2 format was relatively straightforward.
Foreach file to be annotated, we created one base filecontaining the tokenized text and one entity file con-taining the rule-based NEs.73.4 Annotation PhasesWe employed 3 annotators with various degrees ofexperience in annotation and computer science andtherefore familiar with software engineering skillsand terminology.
The lead researcher of the project,the first author of this paper, managed the annotatorsand organized regular meetings with them.We followed the agile corpus creation approachand carried out cycles of annotations, starting witha simple paper-based pilot annotation.
This first an-notation of 10 documents enabled us to get a firstimpression of the type of information contained inCVs of software engineers and programmers as wellas the type of information we wanted to capture inthe manual and automatic annotation.
We drew up afirst set of potential types of zones that occur within6http://mmax2.sourceforge.net7For more information on how this is done see Mu?ller andStrube (2006).31CVs and the types of NEs that can be found withineach zone (e.g.
an EDUCATION zone containing NEsof type LOC, ORG and QUAL).Using this set of potential markables, we decidedon a subset of NEs and zones to be annotated in fu-ture rounds.
Regarding the zones, we settled on an-notating zone titles in a similar way as NEs.
Ourassumption was that recognizing the beginning of azone can sufficiently identify zone boundaries.
Wedid not include relations between NEs at this stages,as we wanted to get a clearer idea of the definitionsof relevant NEs first before proceeding to relations.We then carried out a second pilot annotation us-ing 10 more CVs selected from the candidate pool.We used the revised annotation scheme and thistime the annotation was done electronically usingMMAX2.
The annotators also had access to thePDF and DOC versions of each file in case crucialstructural or formatting information was lost in theconversion.
Files were annotated for NEs and zonetitles.
We also asked the annotators to answer thefollowing questions:?
Does it make sense to annotate the proposedmarkables and what are the difficulties in doingso??
Are there any interesting markables missingfrom the list??
Are there are any issues with using the annota-tion tool?Half way through the second pilot we scheduled afurther meeting to discuss their answers, addressedany question, comments or issues with regard to theannotation and adjusted the annotation guidelinesaccordingly.
At this point, as we felt that the defini-tions of NEs were sufficiently clear and added guide-lines for annotating various types of binary relationsbetween NEs, for example a LOC-ORG relation re-ferring to a particular organization situated at a par-ticular location, e.g.
Google - Dublin.
We list thefinal set of markables as defined at the end of theannotation process in Tables 2 and 3.During the second half of the second pilot weasked the annotators to time their annotation and es-tablished that it can take between 30 minutes and 1.5hours to annotate a CV.
We then calculated pairwiseIAA for two doubly annotated files which allowedus to get some evidence for which definition of NEs,zone titles and relations were still ambiguous or notactually relevant.In parallel with both pilots, we also liaised closelywith a local recruitment company to gain a first-hand understanding of what information recruitersare interested in when matching candidates to em-ployments or employers.
This consultation as wellas the conclusions made after the second pilot ledto further adaptions of the annotation scheme beforethe main annotation phase began.Based on the feedback from the second pilot an-notation, we also made some changes to the dataconversion and the annotation tool setup to reducethe amount of work for annotators but without re-stricting the set of markables.
In the case of somenested NEs, we propagated relations between em-bedded NEs that could be referred from the relationsof the containing NEs.
For example, two DATE enti-ties nested within a DATERANGE entity, the latter ofwhich the annotator related to an ORG entity, wererelated to the same ORG entity automatically.
Wealso introduced a general GROUP entity which couldbe used by the annotators to mark up lists of NEs,for example, if they were all related to a differentNE mention of type X.
In that case, the annotatorsonly had to mark up a relation between the GROUPand X.
All implicit relations between the NEs nestedin the GROUP and X were propagated during the con-version from the MMAX2 format back into the in-house XML format.
This proved particularly usefulfor annotating relations between SKILL entities andother types of NEs.Once those changes had been made, the main an-notation phase began.
Each in-domain CV that wasloaded into the annotation tool already containedsome NEs pre-annotated by the rule-based NER sys-tem (see Section 3.2).
The annotators had to correctthe annotations in case they were erroneous.
Over-all, the annotators reported this pre-annotation to beuseful rather than hindering as they did not have todo too many corrections.
At the end of each day, theannotators checked in their work into the project?ssubversion (SVN) repository.
This provided us withadditional control and backup in case we needed togo back to previous versions at later stages.The annotation guidelines still evolved during themain annotation.
Regular annotation meetings were32held in case the annotators had questions on theguidelines or if they wanted to discuss specific ex-amples.
If a change was made to the annotationguidelines, all annotators were informed and askedto update their annotations accordingly.
Moreover,IAA was calculated regularly on sub-sections of thedoubly annotated data.
This provided more empiri-cal evidence for the types of markables the annota-tors found difficult to mark up and where clarifica-tions where necessary.
The reasons for this were thattheir definitions were ambiguous or underspecified.We deliberately kept the initial annotation schemesimple.
The idea was for the annotators to shape theannotation scheme based on evidence in the actualdata.
We believe that this approach made the dataset more useful for its final use to train and evaluateTM components.
As a result of this agile annotationapproach, we became aware of any issues very earlyon and were able to correct them accordingly.4 Annotation SchemeIn this section, we provide a summary of the finalannotation scheme as an overview of all the mark-ables present in the annotated data set.4.1 Named EntitiesIn general, we asked the annotators to mark up ev-ery mention of all NE types throughout the entireCV, even if they did not refer to the CV owner.
Withsome exceptions (DATE in DATERANGE and LOC orORG in ADDRESS), annotators were asked to avoidnested NEs and aim for a flat annotation.
Discontin-uous NEs in coordinated structures had to be markedas such, i.e.
the NE should only contain strings thatrefer to it.
Finally, abbreviations and their defini-tions had to be annotated as two separate NEs.
TheNE types in the final annotation guidelines are listedin Table 2.
While carrying out the NE annotation,the annotators were also asked to set the NE at-tribute of type CANDIDATE (by default set to true)to false if a certain NE was not an attribute of theCV owner (e.g.
the ADDRESS of a referee).4.2 Zone TitlesRegarding the zone titles, we provided a list of syn-onyms for each type as context (see Table 2).
Theannotators were asked only to annotate main zonetitles, ignoring sub-zones.
They were also asked toEntity Type DescriptionADDRESS Addresses with streets or postcodes.DATE Absolute (e.g.
10/04/2010), underspec-ified (e.g.
April 2010) or relative dates(e.g.
to date) including DATE entitieswithin DATERANGE entities.DATERANGE Date ranges with a specific start and enddate including ones with either point notexplicitly stated (e.g.
since 2008).DOB Dates of birth.EMAIL Email addresses.JOB Job titles and roles referring to the of-ficial name a post (e.g.
software devel-oper) but not a skill (e.g.
software de-velopment).LOC Geo-political place names.ORG Names of companies, institutions andorganizations.PER Person names excluding titles.PHONE Telephone and fax numbers.POSTCODE Post codes.QUAL Qualifications achieved or working to-wards.SKILL Skills and areas of expertise incl.
hardskills (e.g.
Java, C++, French) or gen-eral areas of expertise (e.g.
software de-velopment) but not soft or interpersonalskills (e.g.
networking, team work).TIMESPAN Durations of time (e.g.
7 years, 2months, over 2 years).URL URLsGROUP Dummy NE to group several NEs forannotating multiple relations at once.The individual NEs contained withinthe group still have to be annotated.Zone Title Type SynonymsEDUCATION Education, Qualifications, Training,Certifications, CoursesSKILLS Skills, Qualifications, Experience,CompetenciesSUMMARY Summary, ProfilePERSONAL Personal Information, Personal DataEMPLOYMENT Employment, Employment History,Work History, Career, Career RecordREFERENCES References, RefereesOTHER Other zone titles not covered by this list,e.g.
Publications, Patents, Grants, As-sociations, Interests, Additional.Table 2: The types of NEs and zone titles annotated.mark up only the relevant sub-string of the text re-ferring to the zone title and not the entire title if itcontained irrelevant information.4.3 RelationsThe binary relations that were annotated (see Table3) always link two different types of NE mentions.Annotators were asked to mark up relations withinthe same zone but not across zones.33Relation Type DescriptionTEMP-SKILL A skill related to a temporal expression(e.g.
Java - 7 years).
TEMP includes anytemporal NE types (DATE, DATERANGEand TIMESPAN).TEMP-LOC A location related to a temporal expres-sion (e.g.
Dublin - summer 2004).TEMP-ORG An organization related to a temporalexpression (e.g.
Google - 2001-2004).TEMP-JOB A job title related to a temporal ex-pression (e.g.
Software Engineer -Sep. 2001 to Jun.
2004).TEMP-QUAL A qualification related to a temporal ex-pression (e.g.
PhD - June 2004).LOC-ORG An organization related to a location(e.g.
Google - Dublin).LOC-JOB A job title related to a location(e.g.
Software Engineer - Dublin).LOC-QUAL A qualification related to a location(e.g.
PhD - Dublin).ORG-JOB A job title related to an organization(e.g.
Software Engineer - Google).ORG-QUAL A qualification related to an organiza-tion (e.g.
PhD - University of Toronto).GROUP-X A relation that can be assigned in casea group of NEs all relate to another NEX.
GROUP-X can be any of the relationpairs mentioned in this list.Table 3: The types of relations annotated.5 Inter-Annotator AgreementWe first calculated pairwise IAA for all markablesat the end of the 2nd pilot and continued doing sothroughout the main annotation phase.
For each pairof annotations on the same document, IAA was cal-culated by scoring one annotator against another us-ing precision (P), recall (R) and F1.8 An overall IAAwas calculated by micro-averaging across all anno-tated document pairs.9 We used F1 rather than theKappa score (Cohen, 1960) to measure IAA as thelatter requires comparison with a random baseline,which does not make sense for tasks such as NER.Table 4 compares the IAA figures we obtained for2 doubly annotated documents during the 2nd pilotphase, i.e.
the first time we measured IAA, to thosewe obtained on 9 different files once the main an-notation was completed.
For NEs and zone titles,IAA was calculated using P, R and F1, defining twomentions as equal if they had the same left and right8P, R and F1 are calculated in standard fashion from thenumber of true positives, false positives and false negatives.9Micro-averaging was chosen over macro-averaging, sincewe felt that the latter would give undue weight to documentswith fewer markables.boundaries and the same type.
Although this com-parison is done over different sub-sets of the corpus,it is still possible to conclude that the NE IAA im-proved considerably over the course of the annota-tion process.The IAA scores for the majority of NEs were in-creased considerably at the end, with the exceptionof SKILL for which the IAA ended up being slightlylower as well as DOB and PER of which there arenot sufficient examples in either sets to obtain re-liably results.10 There are very large increases inIAA for JOB and ORG entities, as we discovered dur-ing the pilot annotation that the guidelines for thosemarkables were not concrete enough regarding theirboundaries and definitions.
Their final IAA figuresshow that both of these types of NEs were still mostdifficult to annotate at the end.
However, a final totalIAA of 84.8 F1 for all NEs is a relatively high score.In comparison, the final IAA score of 97.1 F1 for thezone titles shows that recognizing zone titles is aneven easier task for humans to perform compared torecognizing NEs.When calculating IAA for relations, only those re-lations for which both annotators agreed on the NEswere included.
This is done to get an idea of thedifficulty of the RE task independently of NER.
Re-lation IAA was also measured using F1, where rela-tions are counted as equal if they connect exactly thesame NE pair.
The IAA for relations between NEswithin CVs is relatively high both during the pilotannotation and at the end of the main annotation andonly increased slightly over this time.
These figuresshow that this task is much easier than annotating re-lations in other domains, e.g.
in biomedical researchpapers (Alex et al, 2008a).The IAA figures show that even with cyclic anno-tation, evolving guidelines and continuous updating,human annotators can find it challenging to annotatesome markables consistently.
This has an effect onthe results of the automatic annotation where the an-notated data is used to train ML-based models andto evaluate their performance.10The reason why there are no figures for POSTCODE andTIMESPAN entities for the pilot annotation is that none appearedin those documents.34(1) 2nd Pilot Annotation (2) End of Main Annotation (3) Automatic AnnotationType P R F1 TPs P R F1 TPs P R F1 TPsNamed EntitiesADDRESS 100.0 100.0 100.0 1 100.0 100.0 100.0 10 13.8 16.0 14.8 8DATE 62.5 92.6 74.6 25 98.5 98.5 98.5 191 94.1 95.7 94.9 1,850DATERANGE 91.3 95.5 93.3 21 98.6 97.3 97.9 71 91.4 87.0 89.2 637DOB 100.0 100.0 100.0 1 75.0 100.0 85.7 3 70.8 70.8 70.8 17EMAIL 100.0 100.0 100.0 2 100.0 100.0 100.0 8 95.9 100.0 97.9 93JOB 39.1 52.9 45.0 9 72.5 69.9 71.2 95 70.5 61.4 65.6 742LOC 88.9 100.0 94.1 16 100.0 95.8 97.9 137 83.2 87.3 85.2 1,259ORG 68.0 81.0 73.9 17 93.4 86.4 89.8 171 57.1 44.7 50.2 749PER 100.0 100.0 100.0 2 100.0 95.0 97.4 19 69.8 40.5 51.2 196PHONE 100.0 100.0 100.0 4 100.0 100.0 100.0 16 90.9 85.7 88.2 90POSTCODE - - - - 90.9 90.9 90.9 10 98.3 71.3 82.6 57QUAL 9.1 7.7 8.3 1 68.4 81.3 74.3 13 53.9 27.2 36.1 56SKILL 76.6 86.8 81.4 210 79.3 79.0 79.2 863 67.9 66.5 67.2 5,645TIMESPAN - - - - 91.7 91.7 91.7 33 74.0 76.8 75.4 179URL 100.0 100.0 100.0 2 100.0 100.0 100.0 43 97.2 90.5 93.7 209All 73.0 84.1 78.1 311 85.4 84.2 84.8 1,683 73.5 69.4 71.4 11,787Zone TitlesEDUCATION 100.0 100.0 100.0 3 100.0 100.0 100.0 9 86.3 75.0 80.3 63EMPLOYMENT 100.0 100.0 100.0 1 100.0 88.9 94.1 8 83.1 69.7 75.8 69OTHER 100.0 100.0 100.0 1 - - - - 39.3 28.2 32.8 22PERSONAL 25.0 25.0 25.0 1 100.0 100.0 100.0 4 65.4 53.1 58.6 17REFERENCES 100.0 100.0 100.0 1 100.0 100.0 100.0 3 94.4 89.5 91.9 17SKILLS 33.3 40.0 36.4 2 100.0 100.0 100.0 7 63.8 38.9 48.4 44SUMMARY - - - - 75.0 100.0 85.7 3 82.2 64.9 72.6 37All 56.3 60.0 58.1 9 97.1 97.1 97.1 34 72.7 55.8 63.2 269RelationsDATE-JOB - - - - 100.0 83.3 90.9 10 28.1 44.7 34.5 110DATE-LOC - - - - 88.9 72.7 80.0 8 71.3 52.7 60.6 223DATE-ORG - - - - 100.0 88.2 93.8 15 53.0 51.5 52.3 218DATE-QUAL - - - - 100.0 100.0 100.0 6 60.6 73.1 66.3 57DATERANGE-JOB 77.8 100.0 87.5 7 91.7 100.0 95.7 66 80.4 72.5 76.2 663DATERANGE-LOC 91.7 100.0 95.7 11 85.4 79.6 82.4 70 82.0 82.7 82.4 735DATERANGE-ORG 93.8 100.0 96.8 15 80.2 76.2 78.2 77 72.2 76.4 74.2 644DATERANGE-QUAL 100.0 100.0 100.0 1 100.0 100.0 100.0 21 71.1 62.1 66.3 59DATERANGE-SKILL 89.0 98.1 93.3 105 82.2 100.0 90.5 352 61.1 33.7 43.4 1,574DATE-SKILL 100.0 9.1 16.7 1 95.0 67.1 78.6 57 23.6 54.5 33.0 368JOB-LOC NaN 0.0 NaN 0 91.8 65.6 76.5 78 77.0 69.1 72.8 932JOB-ORG 87.5 100.0 93.3 7 86.8 73.3 79.5 99 64.6 50.7 56.8 758JOB-TIMESPAN - - - - 85.7 54.6 66.7 6 56.0 61.8 58.8 47LOC-ORG NaN 0.0 NaN 0 89.6 71.4 79.5 120 79.7 78.9 79.3 1,044LOC-QUAL NaN 0.0 NaN 0 100.0 100.0 100.0 19 75.6 78.7 77.1 133LOC-TIMESPAN - - - - 100.0 75.0 85.7 3 48.2 36.1 41.3 13ORG-QUAL NaN 0.0 NaN 0 95.2 95.2 95.2 20 77.8 71.4 74.5 140ORG-TIMESPAN - - - - 83.3 55.6 66.7 5 55.9 33.3 41.8 19SKILL-TIMESPAN - - - - 86.1 74.0 79.6 37 59.5 52.6 55.8 280All 85.5 83.1 84.2 147 86.8 82.6 84.6 1,069 63.1 55.3 59.0 8,017Table 4: IAA for NEs, zone titles and relations in precision (P), recall (R) and F1 at two stages in theannotation process: (1) at the end of the second pilot annotation and (2) at the end of the main annotationphase; as well as automatic annotation scores (3) on the blind TEST set.
The total number of true positives(TPs) is shown to provide an idea of the quantities of markables in each set.356 Automatic AnnotationTable 4 also lists the final scores of the automaticML-based NER and RE components (Alex et al,2008b) which were adapted to the recruitment do-main during the TXV project.
Following agilemethods, we trained and evaluated models veryearly into the annotation process.
During the sys-tem optimization, learning curves helped to investi-gate for which markables having more training dataavailable would improve performance.The NER component recognizes NEs and zonetitles simultaneously with an overall F1 of 71.4(84.2% of IAA) and 63.2 (65.0% of IAA), respec-tively.
Extremely high or higher than average scoreswere obtained for DATE, DATERANGE, EMAIL, LOC,PHONE, POSTCODE, TIMESPAN and URL entities.Mid-range to lower scores were obtained for AD-DRESS, DOB, JOB, ORG, PER, QUAL and SKILL enti-ties.
One reason is the similarity between NE types,e.g.
DOB is difficult to differentiate from DATE.
Thelayout of CVs and the lack of full sentences alsopose a challenge as the NER component is trainedusing contextual features surrounding NEs that areoften not present in CV data.
Finally, the strict eval-uation counts numerous boundary errors for NEswhich can be considered correct, e.g.
the systemoften recognizes organization names like ?Sun Mi-crosystems, Inc?
whereas the annotator included thefull stop at the end (?Sun Microsystems, Inc.?
).The RE component (Haddow, 2008) performswith an overall F1 of 59.0 on the CV TEST set(69.7% of IAA).
It yields high or above aver-age scores for 10 relation types (DATE-LOC, DATE-QUAL, DATERANGE-JOB, DATERANGE-LOC, DAT-ERANGE-ORG, DATERANGE-QUAL, JOB-LOC, LOC-ORG, LOC-QUAL, ORG-QUAL).
It yields mid-rangeto low scores for the other relation types (DATE-JOB, DATE-ORG, DATERANGE-SKILL, DATE-SKILL,JOB-ORG, JOB-TIMESPAN, LOC-TIMESPAN, ORG-TIMESPAN, SKILL-TIMESPAN).
The most frequenttype is DATERANGE-SKILL, a skill obtained during aparticular time period.
Its entities tend to be found inthe same zone but not always in immediate context.Such relations are inter-sentential, i.e.
their entitiesare in different sentences or what is perceived as sen-tences by the system.
Due to nature of the data, thereare few intra-sentential relations, relations betweenNEs in the same sentence.
The further apart two re-lated NEs are, the more difficult it is to recognizethem.
Similarly to NER, one challenge for RE fromCVs is their diverse structure and formatting.7 Discussion and ConclusionThe increase in the IAA figures for the markablesover time show that agile corpus annotation resultedin more qualitative annotations.
It is difficult toprove that the final annotation quality is higher thanit would have been had we followed the traditionalway of annotation.
Comparing two such methods inparallel is very difficult to achieve as the main aimof annotation is usually to create a corpus and not toinvestigate the best and most efficient method.However, using the agile approach we identifiedproblems early on and made improvements to theannotation scheme and the setup during the processrather than at the end.
Given a fixed annotation timeframe and the proportion of time we spent on cor-recting errors throughout the annotation process, onemight conclude that we annotated less data than wemay have done, had we not followed the agile ap-proach.
However, Voormann and Hut (2008) arguethat agile annotation actually results in more useabledata at the end and in less data being thrown away.Had we followed the traditional approach, wewould unlikely have planned a correction phase atthe end.
The two main reason for that are costand the general belief that the more annotated datathe better.
A final major correction phase is usu-ally viewed as too expensive during an annotationproject.
In order to avoid this cost, the traditional ap-proach taken tends to be to create a set of annotationguidelines when starting out and hold off the mainannotation until the guidelines are finalized and con-sidered sufficiently defined.
This approach does notlend itself well to changes and adjustments later onwhich are inevitable when dealing with natural lan-guage.
As a result the final less accurate annotatedcorpus tends to be accepted as the ground truth orgold standard and may not be as suitable and usefulfor a given purpose as it could have been follow-ing the agile annotation approach.
Besides changingthe way in which annotators work, we recognize theneed for more flexible annotation tools that allow an-notators to implement changes more rapidly.36ReferencesBeatrice Alex, Malvina Nissim, and Claire Grover.
2006.The impact of annotation on the performance of pro-tein tagging in biomedical text.
In Proceedings of the5th International Conference on Language Resourcesand Evaluation (LREC 2006), Genoa, Italy.Bea Alex, Claire Grover, Barry Haddow, Mijail Kabad-jov, Ewan Klein, Michael Matthews, Richard Tobin,and Xinglong Wang.
2008a.
The ITI TXM corpora:Tissue expressions and protein-protein interactions.
InProceedings of the Workshop on Building and Evalu-ating Resources for Biomedical Text Mining at LREC2008, Marrakech, Morocco.Beatrice Alex, Claire Grover, Barry Haddow, MijailKabadjov, Ewan Klein, Michael Matthews, RichardTobin, and Xinglong Wang.
2008b.
Automating cu-ration using a natural language processing pipeline.Genome Biology, 9(Suppl 2):S10.Sue Atkins, Jeremy Clear, and Nicholas Ostler.
1992.Corpus design criteria.
Literary and Linguistic Com-puting, 7(1):1?16.Douglas Biber.
1993.
Representativeness in corpus de-sign.
Literary and Linguistic Computing, 8(4):243?257.Jacob Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Measure-ment, 20:37?46.Barry Haddow.
2008.
Using automated feature optimisa-tion to create an adaptable relation extraction system.In Proceedings of BioNLP 2008, Columbus, Ohio.Christoph Mu?ller and Michael Strube.
2006.
Multi-levelannotation of linguistic data with MMAX2.
In SabineBraun, Kurt Kohn, and Joybrato Mukherjee, editors,Corpus Technology and Language Pedagogy.
New Re-sources, New Tools, New Methods., pages 197?214.Peter Lang, Frankfurt.
(English Corpus Linguistics,Vol.3).Va?clav Nova?k and Magda Raz??mova?.
2009.
Unsu-pervised detection of annotation inconsistencies usingapriori algorithm.
In Proceedings of the Third Linguis-tic Annotation Workshop (LAW III), pages 138?141,Suntec, Singapore.Winston Royce.
1970.
Managing the developmentof large software systems.
In Proceedings of IEEEWESCON, pages 1?9.Holger Voormann and Ulrike Gut.
2008.
Agile corpuscreation.
Corpus Linguistics and Linguistic Theory,4(2):235?251.37
