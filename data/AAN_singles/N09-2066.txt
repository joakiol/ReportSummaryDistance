Proceedings of NAACL HLT 2009: Short Papers, pages 261?264,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsReverse Revision and Linear Tree Combination for Dependency ParsingGiuseppe AttardiDipartimento di InformaticaUniversita` di PisaPisa, Italyattardi@di.unipi.itFelice Dell?OrlettaDipartimento di InformaticaUniversita` di PisaPisa, Italyfelice.dellorletta@di.unipi.it1 IntroductionDeterministic transition-based Shift/Reduce depen-dency parsers make often mistakes in the analysis oflong span dependencies (McDonald & Nivre, 2007).Titov and Henderson (2007) address this accuracydrop by using a beam search instead of a greedy al-gorithm for predicting the next parser transition.We propose a parsing method that allows reduc-ing several of these errors, although maintaining aquasi linear complexity.
The method consists in twosteps: first the sentence is parsed by a determinis-tic Shift/Reduce parser, then a second deterministicShift/Reduce parser analyzes the sentence in reverseusing additional features extracted from the parsetrees produced by the first parser.Right-to-left parsing has been used as part ofensemble-based parsers (Sagae & Lavie, 2006; Hallet al, 2007).
Nivre and McDonald (2008) insteaduse hints from one parse as features in a secondparse, exploiting the complementary properties ofgraph-based parsers (Eisner, 1996; McDonald et al,2005) and transition-based dependency parsers (Ya-mada & Matsumoto, 2003; Nivre & Scholz, 2004).Also our method uses input from a previous parserbut only uses parsers of a single type, determin-istic transition-based Shift/Reduce, maintaining anoverall linear complexity.
In fact both the en-semble parsers and the stacking solution of Nivre-McDonald involve the computation of the maximumspanning tree (MST) of a graph, which require algo-rithms of quadratic time complexity (e.g.
(Chu &Liu, 1965; Edmonds, 1967)).We introduce an alternative linear combinationmethod.
The algorithm is greedy and works by com-bining the trees top down.
We tested it on the de-pendency trees produced by three parsers, a Left-to-Right (LR ), a Right-to-Left (RL ) and a stackedRight-to-Left parser, or Reverse Revision parser(Rev2 ).
1 The experiments show that in practiceits output often outperforms the results produced bycalculating the MST.2 ExperimentsIn the reported experiments we used DeSR (Attardiat al., 2007), a freely available implementation ofa transition-based parser.
The parser processes in-put tokens advancing on the input with Shift actionsand accumulates processed tokens on a stack withReduce actions.
The parsing algorithm is fully de-terministic and linear.For the LR parser and the Rev2 parser we em-ployed an SVM classifier while aMaximum Entropyclassifier, with lower accuracy, was used to createthe training set for the Rev2 parser.
The reason forthis appears to be that the output of a low accuracyparser with many errors provides a better source oflearning to the stacked parser.The Rev2 parser exploits the same basic set offeatures as in the LR parser plus the additional fea-tures extracted from the output of the LR parserlisted in Table 1, where: PHLEMMA is the lemmaof the predicted head, PHPOS is the Part of Speechof the predicted head, PDEP is the predicted depen-dency label of a token to its predicted head, PHDISTindicates whether a token is located before or after1The stacked Left-to-Right parser produced slightly worseresults than Rev2.261Feature TokensPHHLEMMA w0 w1PHDEP w0 w1PHPOS s0 w0 w1PHLEMMA s0 w0 w1PDEP s0 w0 w1PHDIST s0 w0 w1Table 1: Additional features used in training the Revisionparser.its predicted head, PHHLEMMA is the lemma ofthe predicted grandparent and PHDEP is the pre-dicted dependency label of the predicted head of atoken to the predicted grandparent.
s0 refers to a to-ken on top of the stack, wi refers to word at the i-threlative position with respect to the current word andparsing direction.
This feature model was used forall languages in our tests.We present experiments and comparative erroranalysis on three representative languages from theCoNLL 2007 shared task (Nivre at al., 2007): Ital-ian, Czech and English.
We also report an evaluationon all thirteen languages of the CoNLL-X sharedtask (Buchholz &Marsi, 2006), for comparison withthe results by Nivre and McDonald (2008).Table 2 shows the Labeled Attachment Score(LAS), for the Left-to-right parser (LR ), Right-to-Left (RL ), Reverse Revision parser (Rev2 ), linearparser combination (Comb) and MST parser combi-nation (CombMST).Figure 1 and 2 present the accuracies of the LRand Rev2 parsers for English relative to the depen-dency length and the length of sentences, respec-tively.
For Czech and Italian the RL parser achieveshigher accuracy than the LR parser and the Rev2parser even higher.
The error analysis for Czechshowed that the Rev2 parser improves over the LRparser everywhere except in the Recall for depen-dencies of length between 10 and 14.
Such an im-provement has positive impact on the analysis ofsentences longer than 10 tokens, like for Italian.2.1 CoNLL-X ResultsFor direct comparison with the approach by Nivreand McDonald (2008), we present the results on theCoNLL-X corpora (Table 3): MST and MSTMaltare the results achieved by the MST parser and theMST parser using hints from Maltparser, Malt and0.40.50.60.70.80.9 1  05101520253035F-MeasureDependencyLengthLeft-to-Right_DeSRRevision_DeSRFigure 1: English.
F-Measure relative to dependencylength.808284868890929496  102030405060Dependency AccuracySentence LengthLeft-to-Right_DeSRRevision_DeSRFigure 2: English.
Accuracy relative to sentence length.MaltMST the results of the opposite stacking.2.2 RemarksThe Rev2 parser, informed with data from the LRparser, achieves better accuracy in twelwe cases, sta-tistically significantly better in eight.The error analysis confirms that indeed the Rev2parser is able to reduce the number of errors made onlong dependency links, which are a major weaknessof a deterministic Shift/Reduce parser.
The accuracyof the Rev2 parser might be further improved bymore sophisticated feature selection, choosing fea-tures that better represent hints to the second parsingstage.3 Linear Voting CombinationOur final improvements arise by combining the out-puts of the three parser models: the LR parser, the262Language LR RL Rev2 Comb CombMST CoNLL2007 BestCzech 77.12 78.20 79.95 80.57 80.25 80.19English 86.94 87.44 88.34 89.00 88.79 89.61Italian 81.40 82.89 83.52 84.56 84.28 84.40Table 2: LAS for selected CoNLL 2007 languages.Language LR RL Rev2 Comb CombMST Conll-XBestMST MSTMalt Malt MaltMSTarabic 67.27 66.05 67.54 68.38 68.50 66.91 66.91 68.64 66.71 67.80bulgarian 86.83 87.13 87.41 88.11 87.85 87.57 87.57 89.05 87.41 88.59chinese 87.44 85.77 87.51 87.77 87.75 89.96 85.90 88.43 86.92 87.44czech 79.84 79.46 81.78 82.22 82.22 80.18 80.18 82.26 78.42 81.18danish 83.89 83.63 84.85 85.47 85.25 84.79 84.79 86.67 84.77 85.43dutch 75.71 77.27 78.77 79.55 80.19 79.19 79.19 81.63 78.59 79.91german 85.34 85.20 86.50 87.40 87.38 87.34 87.34 88.46 85.82 87.66japanese 90.03 90.63 90.87 91.67 91.59 91.65 90.71 91.43 91.65 92.20portuguese 86.84 87.00 87.86 88.14 88.20 87.60 86.82 87.50 87.60 88.64slovene 73.64 74.40 75.32 75.72 75.48 73.44 73.44 75.94 70.30 74.24spanish 81.63 81.61 81.85 83.33 83.13 82.25 82.25 83.99 81.29 82.41swedish 82.95 81.62 82.91 83.69 83.69 84.58 82.55 84.66 84.58 84.31turkish 64.91 61.92 63.33 65.27 65.23 65.68 63.19 64.29 65.58 66.28Average 80.49 80.13 81.27 82.05 82.03 81.63 80.83 82.53 80.74 82.01Table 3: Labeled attachment scores for CoNLL-X corpora.RL parser and the Rev2 parser.Instead of using a general algorithm for calcu-lating the MST of a graph, we exploit the fact thatwe are combining trees and hence we developed anapproximate algorithm that has O(kn) complexity,where n is the number of nodes in a tree and k is thenumber of trees being combined.The algorithm builds the combined tree T incre-mentally, starting from the empty tree.
We will ar-gue that an invariant of the algorithm is that the par-tial result T is always a tree.The algorithm exploits the notion of fringe F , i.e.the set of arcs whose parent is in T and that can beadded to T without affecting the invariant.
InitiallyF consists of the roots of all trees to be combined.The weight of each arc a in the fringe is the numberof parsers that predicted a.At each step, the algorithm selects from F an arca = (h, d, r) among those with maximum weight,having h ?
T .
Then it:1. adds a to T2.
removes from F all arcs whose child is d3.
adds to F all arcs (h?, d?, r?)
in the original treeswhere h?
?
T and d?
/?
T .Step 3 guarantees that no cycles are present in T .The final T is connected because each added nodeis connected to a node in T .
T is a local maximumbecause if there were another tree with higher scoreincluding arc (h, n, r), either it is present in T or itsweight is smaller than the weight for node (h?, n, r?
)in T , as chosen by the algorithm.The algorithm hasO(kn) complexity.
A sketch ofthe proof can be given as follows.
Step 3 guaranteesthat the algorithm is iterated n times, where n is thenumber of nodes in a component tree.
Using appro-priate data structures to represent the fringe F , in-sert or delete operations take constant time.
At eachiteration of the algorithm the maximum number ofremovals from F (step 2) is constant and it is equalto k, hence the overall cost is O(nk).Table 2 shows the results for the three languagesfrom CoNLL 2007.
With respect to the best resultsat the CoNLL 2007 Shared Task, the linear parsercombination achieves the best LAS for Czech andItalian, the second best for English.The results for the CoNLL-X languages (Table 3)show also improvements: the Rev2 parser is more263accurate than MST, except for Bulgarian, Dutch,German, and Spanish, where the difference is within1%, and it is often better than the MaltMST stacking.The improvements of the Rev2 over the LR parserrange from 0.38% for Chinese to 3.84% for Dutch.The column CombMST shows the results of com-bining parsers using the Chu-Liu-Edmonds MST al-gorithm and the same weighting scheme of Lin-ear Combination algorithm.
For most languagesthe Linear Combination algorithm leads to a bet-ter accuracy than the MST algorithm.
The some-what surprising result might be due indeed to the topdown processing of the algorithm: since the algo-rithm chooses the best among the connections thatare higher in the parse tree, this leads to a prefer-ence to long spanning links over shorter links evenif these contribute higher weights to the MST.Finally, the run time of the linear combination al-gorithm on the whole CoNLL-X test set is 11.2 sec,while the MST combination requires 92.5 sec.We also tested weights based on the accuracyscore of each parser for the POS of an arc head, butthis produced less accurate results.4 ConclusionsWe presented a method for improving the accuracyof a dependency parser by using a parser that ana-lyzes a sentence in reverse using hints from the treesproduced by a forward parser.We also introduced a new linear algorithm to per-form parser combination.Experiments on the corpora of languages fromthe CoNLL-X and the CoNLL 2007 shared tasksshow that reverse revision parsing improves the ac-curacy over a transition-based dependency parser inall the tested languages.
Further improvements areobtained by using a linear parser combination algo-rithm on the outputs of three parsers: a LR parser, aRL parser and a Rev2 parser.The combination parser achieves accuracies thatare best or second best with respect to the resultsof the CoNLL 2007 shared task.
Since all the indi-vidual parsers as well as the combination algorithmis linear, the combined parser maintains an overalllinear computational time.
On the languages fromthe CoNLL-X shared task the combination parserachieves often the best accuracy in ten out of thirteenlanguages but falls short of the accuracy achievedby integrating a graph-based with a transition basedparser.We expect that further tuning of the method mighthelp reduce these differences.ReferencesG.
Attardi, F. Dell?Orletta, M. Simi, A. Chanev andM.
Ciaramita.
2007.
Multilingual Dependency Parsingand Domain Adaptation using DeSR.
In Proc.
of theCoNLL Shared Task Session of EMNLP-CoNLL 2007.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proc.
ofCoNLL, 149?164.Y.
J. Chu and T. H. Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica(14), 1396?1400.J.
Edmonds.
1967.
Optimum branchings.
Journal of Re-search of the National Bureau of Standards (71B),233?240.J.
M. Eisner.
1996.
Three new probabilistic models fordependency parsing: An exploration.
In Proc.
of COL-ING 1996, 340?345.J.
Hall, et al 2007.
Single Malt or Blended?
A Studyin Multilingual Parser Optimization.
In Proc.
of theCoNLL Shared Task Session of EMNLP-CoNLL 2007.R.
McDonald and J. Nivre.
2007.
Characterizing the Er-rors of Data-Driven Dependency Parsing Models InProc.
of EMNLP-CoNLL 2007.R.
McDonald, F. Pereira, K. Ribarov and J. Hajic?.
2005.Non-projective Dependency Parsing using SpanningTree Algorithms.
In Proc.
of HLT-EMNLP 2005.R.
McDonald and F. Pereira.
2006.
Online Learningof Approximate Dependency Parsing Algorithms.
InProc.
of EACL 2006.J.
Nivre, et al 2007.
The CoNLL 2007 Shared Task onDependency Parsing.
In Proc.
of the CoNLL SharedTask Session of EMNLP/CoNLL-2007.J.
Nivre and R. McDonald.
2008.
Integrating Graph-Based and Transition-Based Dependency Parsers.
InProc.
of ACL 2008.J.
Nivre and M. Scholz.
2004.
Deterministic DependencyParsing of English Text.
In Proc.
of COLING 2004.K.
Sagae and A. Lavie.
2006.
Parser Combination byReparsing.
In Proc.
of HLT-NAACL 2006.I.
Titov and J. Henderson.
2007.
Fast and Robust Multi-lingual Dependency Parsing with a Generative LatentVariable Model In Proc.
of the CoNLL Shared TaskSession of EMNLP/CoNNL-2007.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis using support vector machines.
In Proc.of the 8th IWPT.
Nancy, France.264
