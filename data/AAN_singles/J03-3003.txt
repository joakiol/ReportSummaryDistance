c?
2003 Association for Computational LinguisticsEmbedding Web-Based StatisticalTranslation Models in Cross-LanguageInformation RetrievalWessel Kraaij?
Jian-Yun Nie?
Michel Simard?TNO TPD Universite?
de Montre?al Universite?
de Montre?alAlthough more and more language pairs are covered by machine translation (MT) services, thereare still many pairs that lack translation resources.
Cross-language information retrieval (CLIR)is an application that needs translation functionality of a relatively low level of sophistication,since current models for information retrieval (IR) are still based on a bag of words.
The Webprovides a vast resource for the automatic construction of parallel corpora that can be used to trainstatistical translation models automatically.
The resulting translation models can be embedded inseveral ways in a retrieval model.
In this article, we will investigate the problem of automaticallymining parallel texts from the Web and different ways of integrating the translation modelswithin the retrieval process.
Our experiments on standard test collections for CLIR show that theWeb-based translation models can surpass commercial MT systems in CLIR tasks.
These resultsopen the perspective of constructing a fully automatic query translation device for CLIR at a verylow cost.1.
IntroductionFinding relevant information in any language on the increasingly multilingual WorldWide Web poses a real challenge for current information retrieval (IR) systems.
Wewill argue that the Web itself can be used as a translation resource in order to buildeffective cross-language IR systems.1.1 Information Retrieval and Cross-Language Information RetrievalThe goal of IR is to find relevant documents from a large collection of documents orfrom the World Wide Web.
To do this, the user typically formulates a query, often infree text, to describe the information need.
The IR system then compares the querywith each document in order to evaluate its similarity (or probability of relevance)to the query.
The retrieval result is a list of documents presented in decreasing orderof similarity.
The key problem in IR is that of effectiveness, that is, how good an IRsystem is at retrieving relevant documents and discarding irrelevant ones.Because of the information explosion that has occurred on the Web, people aremore in need of effective IR systems than ever before.
The search engines currentlyavailable on the Web are IR systems that have been created to answer this need.
Byquerying these search engines, users are able to identify quickly documents contain-ing the same keywords as the query they enter.
However, the existing search enginesprovide only monolingual IR; that is, they retrieve documents only in the same lan-?
TNO TPD, PO BOX 155, 2600 AD Delft, The Netherlands.
E-mail: kraaij@tpd.tno.nl?
DIRO, Universite?
de Montre?al, CP.
6128, succ.
Centre-ville, Montreal, Qc.
H3C 3J7 Canada.
E-mail:{nie, simardm}@iro.umontreal.ca382Computational Linguistics Volume 29, Number 3guage as the query.
To be more precise: Search engines usually do not consider thelanguage of the keywords when the keywords of a query are matched against thoseof the documents.
Identical keywords are matched, whatever their languages are.
Forexample, the English word son can match the French word son (?his?
or ?her?).
Currentsearch engines do not provide the functionality for cross-language IR (CLIR), that is,the ability to retrieve relevant documents written in languages different from that ofthe query (without the query?s being translated manually into the other language(s)of interest).As the Web has grown, more and more documents on the Web have been written inlanguages other than English, and many Internet users are non-native English speak-ers.
For many users, the barrier between tbe language of the searcher and the langagein which documents are written represents a serious problem.
Although many userscan read and understand rudimentary English, they feel uncomfortable formulatingqueries in English, either because of their limited vocabulary in English, or becauseof the possible misusage of English words.
For example, a Chinese user may use eco-nomic instead of cheap or economical or inexpensive in a query because these words havea similar translation in Chinese.
An automatic query translation tool would be veryhelpful to such a user.
On the other hand, even if a user masters several languages, itis still a burden for him or her to formulate several queries in different languages.
Aquery translation tool would also allow such a user to retrieve relevant documents inall the languages of interest with only one query.
Even for users with no understand-ing of a foreign language, a CLIR system might still be useful.
For example, someonemonitoring a competitor?s developments with regard to products similar to those hehimself produces might be interested in retrieving documents describing the possibleproducts, even if he does not understand them.
Such a user might use machine trans-lation systems to get the gist of the contents of the documents he retrieves throughhis query.
For all these types of users, CLIR would represent a useful tool.1.2 Possible Approaches to CLIRFrom an implementation point of view, the only difference between CLIR and theclassical IR task is that the query language differs from the document language.
It isobvious that to perform in an effective way the task of retrieving documents that arerelevant to a query when the documents are written in a different language than thequery, some form of translation is required.
One might conjecture that a combinationof two existing fields, IR and machine translation (MT), would be satisfactory foraccomplishing the combined translation and retrieval task.
One could simply translatethe query by means of an MT system, then use existing IR tools, obviating the needfor a special CLIR system.This approach, although feasible, is not the only possible approach, nor is it neces-sarily the best one.
MT systems try to translate text into a well-readable form governedby morphological, syntactic, and semantic constraints.
However, current IR models arebased on bag-of-words models.
They are insensitive to word order and to the syntac-tic structure of queries.
For example, with current IR models, the query ?computerscience?
will usually produce the same retrieval results as ?science computer.?
Thecomplex process used in MT for producing a grammatical translation is not fully ex-ploited by current IR models.
This means that a simpler translation approach maysuffice to implement the translation step.On the other hand, MT systems are far from perfect.
They often produce incorrect383Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRtranslations.
For example, Systran1 translates the word drug as drogue (illegal substance)in French for both drug traffic and drug administration office.
Such a translation errorwill have a substantial negative impact on the effectiveness of any CLIR system thatincorporates it.
So even if MT systems are used as translation devices, they may need tobe complemented by other, more robust translation tools to improve their effectiveness.In the current study, we will use statistical translation models as such a complementarytool.Queries submitted to IR systems or search engines are often very short.
In par-ticular, the average length of queries submitted to the search engines on the Web isabout two words (Jansen et al 2001).
Such short queries are generally insufficient todescribe the user?s information need in a precise and unambiguous way.
Many im-portant words are missing from them.
For example, a user might formulate the query?Internet connection?
in order to retrieve documents about computer networks, Inter-net service providers, or proxies.
However, under the current bag-of-words approach,the relevant documents containing these terms are unlikely to be retrieved.
To solvethis problem, a common approach used in IR is query expansion, which tries to addsynonyms or related words to the original query, making the expanded query a moreexhaustive description of the information need.
The words added to the query dur-ing query expansion do not need to be strict synonyms to improve the query results.However, they do have to be related, to some degree, to the user?s information need.Ideally, the degree of the relatedness should be weighted, with a strongly related wordweighted more heavily in the expanded query than a less related one.MT systems act in a way opposite to the query expansion process: Only one trans-lation is generally selected to express a particular meaning.
2 In doing so, MT systemsemployed in IR systems in fact restrict the possible query expansion effect duringthe translation process.
We believe that CLIR can benefit from query translation thatprovides multiple translations for the same meaning.
In this regard, the tests carriedout by Kwok (1999) with a commercial MT system for Chinese-English CLIR are quiteinteresting.
His experiments show that it is much better to use the intermediate transla-tion data produced by the MT system than the final translation itself.
The intermediatedata contain, among other things, all the possible translation words for query terms.Kwok?s work clearly demonstrates that using an MT system as a black box is not themost effective choice for query translation in CLIR.
However, few MT systems allowone to access the intermediate stages of the translations they produce.Apart from the MT approach, queries can also be translated by using a machine-readable bilingual dictionary or by exploiting a set of parallel texts (texts with theirtranslations).
High-quality bilingual dictionaries are expensive, but there are many freeon-line translation dictionaries available on the Web that can be used for query trans-lation.
This approach has been applied in several studies (e.g., Hull and Grefenstette1996; Hiemstra and Kraaij 1999).
However, free bilingual dictionaries often suffer froma poor coverage of the vocabulary in the two languages with which they deal, and fromthe problem of translation ambiguity, because usually no information is provided toallow for disambiguation.
Several previous studies (e.g., Nie et al 1999), have shownthat using a translation dictionary alone would produce much lower effectiveness thanan MT system.
However, a dictionary complemented by a statistical language model(Gao et al 2001; Xu, Weischedel, and Nguyen 2001) has produced much better resultsthan when the dictionary is used alone.1 We used the free translation service provided at ?http://babelfish.altavista.com/?
in October 2002.2 Although there is no inherent obstacle preventing MT systems from generating multiple translations, inpractice, only one translation is produced.384Computational Linguistics Volume 29, Number 3In this article, the use of a bilingual dictionary is not our focus.
We will concentrateon a third alternative for query translation: an approach based on parallel texts.
Paral-lel texts are texts accompanied by their translation in one or several other languages (Ve?ronis2000).
They contain valuable translation examples for both human and machine trans-lation.
A number of studies in recent years (e.g., Nie et al 1999; Franz et al 2001;Sheridan, Ballerini, and Scha?uble 1998; Yang et al 1998) have explored the possibil-ity of using parallel texts for query translation in CLIR.
One potential advantage ofsuch an approach is that it provides multiple translations for the same meaning.
Thetranslation of a query would then contain not only words that are true translationsof the query, but also related words.
This is the query expansion effect that we wantto produce in IR.
Our experimental results have confirmed that this approach can bevery competitive with the MT approach and yield much better results than a simpledictionary-based approach, while keeping the development cost low.However, one major obstacle to the use of parallel texts for CLIR is the unavail-ability of large parallel corpora for many language pairs.
Hence, our first goal in theresearch presented here was to develop an automatic mining system that collects par-allel pages on the Web.
The collected parallel Web pages are used to train statisticaltranslation models (TMs) that are then applied to query translation.
Such an approachoffers the advantage of enabling us to build a CLIR system for a new language pairwithout waiting for the release of an MT system for that language pair.
The numberof potential language pairs supported by Web-based translation models is large if oneincludes transitive translation using English as a pivot language.
English is often oneof the languages of those Web pages for which parallel translations are available.The main objectives of this article are twofold: (1) We will show that it is possibleto obtain large parallel corpora from the Web automatically that can form the basis foran effective CLIR system, and (2) we will compare several ways to embed translationmodels in an IR system to exploit these corpora for cross-language query expansion.Our experiments will show that these translation tools can result in CLIR of com-parable effectiveness to MT systems.
This in turn will demonstrate the feasibility ofexploiting the Web as a large parallel corpus for the purpose of CLIR.1.3 Problems in Query TranslationNow let us turn to query translation problems.
Previous studies on CLIR have iden-tified three problems for query translation (Grefenstette 1998): identifying possibletranslations, pruning unlikely translations, and weighting the translation words.Finding translations.
First of all, whatever translation tool is employed in trans-lating queries has to provide a good coverage of the source and target vocabularies.In a dictionary-based approach to CLIR, we will encounter the same problems thathave been faced in MT research: phrases, collocations, idioms, and domain-specificterminology are often translated incorrectly.
These classes of expressions require a so-phisticated morphological analysis, and furthermore, domain-specific terms challengethe lexical coverage of the transfer dictionaries.
A second important class of wordsthat can pose problems for CLIR, particularly that involving news article retrieval,is proper names.
The names of entities such as persons or locations are frequentlyused in queries for news articles, and their translation is not always trivial.
Often, themore commonly used geographical names of countries or their capitals have a dif-ferent spelling in different languages (e.g., Milan/Milano/Milaan) or translations thatare not related to the same morphological root (e.g., Germany/Allemagne/Duitsland).The names of organizations and their abbreviations are also a notorious problem; forexample, the United Nations can be referred to as UN, ONU, VN, etc.
(disregardingthe problem of morphological normalization of abbreviations).
When proper names385Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRhave to be translated from one language to another with a different script, like Cyril-lic, Arabic, or Chinese, this problem is even more acute.
The process of defining thespelling of a named entity in a language with a different script from the originatinglanguage is called transliteration and is based on a phonemic representation of thenamed entity.
Unfortunately different national ?standards?
are used for transliterationin different languages that use the same alphabet (e.g., the former Russian president?sname in Latin script has been transliterated as Jeltsin, Eltsine, Yeltsin, and Jelzin.Pruning translation alternatives.
A word or a term often has multiple transla-tions.
Some of them are appropriate for a particular query and the others are not.
Animportant question is how to keep the appropriate translations while eliminating theinappropriate ones.
Because of the particularities of IR, it might improve the resultsto retain multiple translations that display small differences in sense, as in query ex-pansion.
So it could be beneficial to keep all related senses for the matching process,together with their probabilities.Weighting translation alternatives.
Closely related to the previous point is thequestion of how to deal with translation alternatives.
The weighting of words in doc-uments and in the query is of crucial importance in IR.
A word with a heavy weightwill influence the results of retrieval more than a low-weight word.
In CLIR it is alsoimportant to assign appropriate weights to translation words.
Pruning translations canbe viewed as an extreme Boolean way of weighting translations.
The intuition is that,just as in query expansion, it may well be beneficial to assign a heavier weight to the?main?
translation and a lighter weight to related translations.1.4 Integration of Query Translation with RetrievalThe problem of ?weighting of translation alternatives,?
identified by Grefenstette,refers to the more general problem of designing an architecture for a CLIR systemin which translation and document ranking are integrated in a way that maximizesretrieval effectiveness.The MT approach clearly separates translation from retrieval: A query is firsttranslated, and the result of the translation is subsequently submitted to an IR systemas a new query.
At the retrieval phase, one no longer knows how certain a translatedword is with respect to the other translated words in the translated query.
All thetranslation words are treated as though they are totally certain.
Indeed, an MT systemis used as a black box.
In this article, we consider translation to be an integral part ofthe IR process that has to be considered together with the retrieval step.From a more theoretical point of view, CLIR is a process that, taken as a whole, iscomposed of query translation, document indexing, and document matching.
The twofirst subprocesses try to transform the query and the documents into a comparableinternal representation form.
The third subprocess tries to compare the representa-tions to evaluate the similarity.
In previous studies on CLIR, the first subprocess isclearly separated from the latter two, which are integrated in classical IR systems.
Anapproach that considers all three subprocesses together will have the advantage ofaccounting better for the uncertainty of translation during retrieval.
More analysis onthis point is provided in Nie (2002).
This article follows the same direction as Nie?s.We will show in our experiments that an integrated approach can produce very highCLIR effectiveness.An attractive framework for integrating translation and retrieval is the probabilisticframework, although estimating translation probabilities is not always straightforwardusing this framework.In summary, because CLIR does not necessarily require a unique translation of atext (as MT does), approaches other than fully automatic MT might provide interesting386Computational Linguistics Volume 29, Number 3characteristics for CLIR that are complementary to those of MT approaches.
This couldresult in greater precision,3 since an MT system might choose the wrong translationfor the query term(s), and/or a higher rate of recall,4 since multiple translations areaccommodated, which could retrieve documents via related terminology.In this article we will investigate the effectiveness of CLIR systems based onprobabilistic translation models trained on parallel texts mined from the Web.
Glob-ally, our approach to the CLIR problem can be viewed informally as ?cross-lingualsense matching.?
Both query and documents are modeled as a distribution over se-mantic concepts, which in reality is approximated by a distribution over words.
Thechallenge for CLIR is to measure to what extent these concepts (or word senses) arerelated.
From this point of view, our approach is similar in principle to that usinglatent semantic analysis (LSI) (Dumais et al 1997), which also tries to create semanticsimilarity between documents, queries, and terms by transposing them into a newvector space.
An alternative way of integrating translation and IR is to create ?struc-tured queries,?
in which translations are modeled as synonyms (Pirkola 1998).
Sincethis approach is simple and effective, we will use it as one of the reference systems inour experiments.The general approach of this article will be implemented in several different ways,each fully embedded in the retrieval models tested.
A series of experiments on CLIRwill be conducted in order to evaluate these models.
The results will clearly show thatWeb-based translation models are as effective as (and sometimes more effective than)off-the-shelf commercial MT systems.The remainder of the article is organized as follows: Section 2 discusses the pro-cedure we used to construct parallel corpora from the Web, and Section 3 describesthe procedure we used to train the translation models.
Section 4 describes the proba-bilistic IR model that we employed and various ways of embedding translation intoa retrieval model.
Section 5 presents our experimental results.
The article ends with adiscussion and conclusion section.2.
PTMinerIt has been shown that by using a large parallel corpus, one can produce CLIR ef-fectiveness close to that obtained with an MT system (Nie et al 1999).
In previousstudies, parallel texts have been exploited in several ways: using a pseudofeedback ap-proach, capturing global cross-language term associations, transposing to a language-independent semantic space, and training a statistical translation model.Using a pseudofeedback approach.
In Yang et al (1998) parallel texts are usedas follows.
A given query in the source language is first used to retrieve a subsetof texts from the parallel corpus.
The corresponding subset in the target language isconsidered to provide a description of the query in the target language.
From thissubset of documents, a set of weighted words is extracted, and this set of words isused as the query ?translation.
?Capturing global cross-language term associations.
A more advanced and theo-retically better-motivated approach is to index concatenated parallel documents in thedual space of the generalized vector space model (GVSM), where terms are indexedby documents (Yang et al 1998).
An approach related to GVSM is to build a so-calledsimilarity thesaurus on the parallel or comparable corpus.
A similarity thesaurus is an3 Precision is defined as the proportion of relevant documents among all the retrieved documents.4 Recall is the proportion of relevant documents retrieved among all the relevant documents in acollection.387Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRinformation structure (also based on the dual space of indexing terms by documents)in which associated terms are computed on the basis of global associations betweenterms as measured by term co-occurrence on the document level (Sheridan, Ballerini,and Scha?uble 1998).
Recently, the idea of using the dual space of parallel documentsfor cross-lingual query expansion has been recast in a language-modeling framework(Lavrenko, Choquette, and Croft 2002).Transposing to a language-independent semantic space.
The concatenated doc-uments can also be transposed in a language-independent space by applying latentsemantic indexing (Dumais et al 1997; Yang et al 1998).
The disadvantage of thisapproach is that the concepts in this space are hard to interpret and that LSI is com-putationally demanding.
It is currently not feasible to perform such a transposition ona Web scale.Training a statistical translation model.
Approaches that involve training a statis-tical translation model have been explored in, for example, Nie et al (1999) and Franzet al (2001).
In Nie et al?s approach, statistical translation models (usually IBM model1) are trained on a parallel corpus.
The models are used in a straightforward way: Thesource query is submitted to the translation model, which proposes a set of translationequivalents, together with their probability.
The latter are then used as a query for theretrieval process, which is based on a vector space model.
Franz et al?s approach usesa better founded theoretical framework: the OKAPI probabilistic IR model (Robert-son and Walker 1994).
The present study uses a different probabilistic IR model, onebased on statistical language models (Hiemstra 2001; Xu, Weischedel, and Nguyen2001).
This IR model facilitates a tighter integration of translation and retrieval.
Animportant difference between statistical translation approaches and approaches basedon document alignment discussed in the previous paragraph is that translation modelsperform alignment at a much more refined level.
Consequently, the alignments canbe used to estimate translation relations in a reliable way.
On the other hand, the ad-vantage of the CLIR approaches that rely simply on alignment at the document levelis that they can also handle comparable corpora, that is, documents that discuss thesame topic but are not necessarily translations of each other (Laffling 1992).Most previous work on parallel texts has been conducted on a few manuallyconstructed parallel corpora, notably the Canadian Hansard corpus.
This corpus5 con-tains many years?
debates in the Canadian parliament in both English and French,amounting to several dozens of millions of words in each language.
The Europeanparliament documents represent another large parallel corpus in several Europeanlanguages.
However, the availability of this corpus is much more restricted than theCanadian Hansard.
The Hong Kong government publishes official documents in bothChinese and English.
They form a Chinese-English parallel corpus, but again, its sizeis much smaller than that of the Canadian Hansard.
For many other languages, nolarge parallel corpora are available for the training of statistical models.LDC has tried to collect additional parallel corpora, resorting at times to man-ual collection (Ma 1999).
Several other research groups (for example, the RALI labat Universite?
de Montre?al) have also tried to acquire manually constructed parallelcorpora.
However, manual collection of large corpora is a tedious task that is time-and resource-consuming.
On the other hand, we observe that the increasing usage ofdifferent languages on the Web results in more and more bilingual and multilingualsites.
Many Web pages are now translated into different languages.
The Web contains5 LDC provides a version containing texts from the mid-1970s through 1988; see?http://www.ldc.upenn.edu/?.388Computational Linguistics Volume 29, Number 3a large number of parallel Web pages in many languages (usually with English).
Ifthese can be extracted automatically, then this would help solve, to some extent, theproblem of parallel corpora.
PTMiner (for Parallel Text Miner) was built precisely forthis purpose.Of course, an automatic mining program is unable to understand the texts it ex-tracts and hence to judge in a totally reliable way whether they are parallel.
However,CLIR is quite error-tolerant.
As we will show, a noisy parallel corpus can still be veryuseful for CLIR.2.1 General Principles of Automatic MiningParallel Web pages usually are not published in isolation; they are often linked toone another in some way.
For example, Resnik (1998) observed that some parallelWeb pages are often referenced in the same parent index Web page.
In addition,the anchor text of such links usually identifies the language.
For example, if a Webpage ?index.html?
provides links to both English and French versions of a page itreferences, and the anchor texts of the links are respectively ?English version?
and?French version,?
then the referenced versions are probably parallel pages in Englishand French.
To locate such pages, Resnik first sends a query of the following form tothe Web search engine AltaVista, which returns the parent indexing pages:anchor: English AND anchor: FrenchThen the referenced pages in both languages are retrieved and considered to be par-allel.
Applying this method, Resnik was able to mine 2,491 pairs of English-FrenchWeb pages.
Other researchers have adapted his system to mine 3,376 pairs of English-Chinese pages and 59 pairs of English-Basque pages.We observe, however, that only a small portion of parallel Web sites are organizedin this way.
Many other parallel pages cannot be found with Resnik?s method.
Themining system we employ in the research presented here uses different criteria fromResnik?s; and we also incorporate an exploration process (i.e., a host crawler) in orderto discover Web pages that have not been indexed by the existing search engines.The mining process in PTMiner is divided into two main steps: identification ofcandidate parallel pages, and verification of their parallelism.
The overall process isorganized into the following steps:1.
Determining candidate sites.
Identify Web sites that may containparallel pages.
In our approach, we adopt a simple definition of Website: a host corresponding to a distinct DNS (domain name system)address (e.g., ?www.altavista.com?
and ?geocities.yahoo.com?).2.
File name fetching.
Identify a set of Web pages on each Web site that areindexed by search engines.3.
Host crawling.
Use the URLs collected in the previous step as seeds tofurther crawl each candidate site for more URLs.4.
Pair scanning by names.
Construct pairs of Web pages on the basis ofpattern matching between URLs (e.g., ?index.html?
vs. ?index f.html?).5.
Text filtering.
Filter the candidate parallel pages further according toseveral criteria that operate on their contents.In the following subsections, we describe each of these steps in more detail.389Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR2.2 Identification of Candidate Web SitesIn addition to the organization of parallel Web pages exploited by Resnik?s method,another common characteristic of parallel Web pages is that they cross-reference oneanother.
For example, an English Web page may contain a pointer to the French ver-sion, and vice versa, and the anchor text of these pointers usually indicates the lan-guage of the other page.
This phenomenon is common because such an anchor textshows the reader that a version in another language is available.In considering both ways of organizing parallel Web pages, we see that a commonfeature is the existence of a link with an anchor text identifying a language.
This isthe criterion we use in PTMiner to detect candidate Web sites: the existence of at leastone Web page containing such a link.
Candidate Web sites are identified via requestssent to a search engine (e.g., AltaVista or Google).
For example, the following requestasks for pages in English that contain a link with one of the required anchor texts.anchor: French version, in French, en Franc?ais, .
.
.language: EnglishThe hosts extracted from the responses are considered to be candidate sites.2.3 File Name FetchingIt is assumed that parallel pages are stored on the same Web site.
This is not alwaystrue, but this assumption allows us to minimize the exploration of the Web and toavoid considering many unlikely candidates.To search for parallel pairs of pages from each candidate site, PTMiner first asksthe search engine for all the Web pages from a particular site that it has indexed, viaa request of the following form:host: <hostname>However, the results of this step may not be exhaustive, because?
search engines typically do not index all the Web pages of a site.?
most search engines allow users to retrieve a limited number ofdocuments (e.g., 1,000 in AltaVista).Therefore, we continue our search with a host crawler, which uses the Web pagesfound by the search engines as seeds.2.4 Host CrawlingA host crawler is slightly different from a Web crawler or a robot in that a host crawlercan only exploit one Web site at a time.
A breadth-first crawling algorithm is used inthe host-crawling step of PTMiner?s mining process.
The principle is that if a retrievedWeb page contains a link to an unexplored document on the same site, this documentis added to the list of pages to be explored later.
This crawling step allows us to obtainmore Web pages from the candidate sites.2.5 Pair Scanning by NamesOnce a large set of URLs has been identified, the next task is to find parallel pairsamong them.
In our experience, many parallel Web pages have very similar file names.390Computational Linguistics Volume 29, Number 3For example, an English Web page with the file name ?index.html?
often correspondsto a French translation with a file name such as ?index f.html?.
The only differencebetween the two file names is a segment that identifies the language of the file.
Thissimilarity in file names is by no means an accident.
In fact, this is a common way forWebmasters to keep track of a large number of documents in different versions.This same observation also applies to URL paths.
For example, the following twoURLs are also similar in name:?http://www.asite.ca/en/afile.html?
and ?http://www.asite.ca/fr/afile.html?.To find similarly named URLs, we define lists of prefixes and suffixes for both thesource and the target languages.
For example:EnglishPrefix = {(emptychar), e, en, english, e , en , english , .
.
.
}Once a possible source language prefix is identified in an URL, it is replaced with aprefix in the target language, and we then test if this URL is found on the Web site.2.6 Filtering by ContentsThe file pairs identified in previous steps are further verified in regard to their contents.In PTMiner, the following criteria are used for verification: file length, HTML structure,and language and character set.2.6.1 File Length.
The ratio of the lengths of a pair of parallel pages is usually com-parable to the typical length ratio of the two languages (especially when the text islong enough).
Hence, a simple verification is to compare the lengths of the two files.As many Web documents are quite short, we tolerate some difference (up to 40% fromthe typical ratio).2.6.2 HTML Structure.
Parallel Web pages are usually designed to have similar lay-outs.
This often means that the two parallel pages have similar HTML structures.However, the HTML structures of parallel pages may also be quite different from oneanother.
Pages may look similar and still have different HTML markups.
Therefore, acertain amount of flexibility is also employed in this step.In our approach, we first determine a set of meaningful HTML tags that affectthe appearance of the page and extract them from both files (e.g., <p> and <H1>, butnot <meta> and <font>).
A ?diff?-style comparison will reveal how different the twoextracted sequences of tags are.
A threshold is set to filter out the pairs of pages thatare not similar enough in HTML structure.At this stage, nontextual parts of the pages are also removed.
If a page does notcontain enough text, it is also discarded.2.6.3 Language and Character Set.
When we query search engines for documents inone specific language, the returned documents may actually be in a different languagefrom the one we specified.
This problem is particularly serious for Asian languages.When we ask for Chinese Web pages, we often obtain Korean Web pages, because thelanguage of the documents has not been identified accurately by the search engines.Another, more important factor that makes it necessary to use a language detector isthat during host crawling and pair scanning, no verification is done with regard tolanguages.
All files with an en suffix in their names, for example, are assumed to beEnglish pages, which may be an erroneous assumption.391Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRTo filter out the files not in the required languages, the SILC system (Isabelle,Simard, and Plamondon 1998) is used.
SILC employs n-gram statistical language mod-els to determine the most probable language and encoding schema for a text.
It hasbeen trained on a number of large corpora for several languages.
The accuracy of thesystem is very high.
When a text contains at least 50 characters, its accuracy is almostperfect.
SILC can filter out a set of file pairs that are not in the required languages.Our utilization of HTML structure to determine whether two pages are parallelis similar to that of Resnik (1998), who also exploits an additional criterion similar tolength-based sentence alignment in order to determine whether the segments in corre-sponding HTML structures have similar lengths.
In the current PTMiner, this criterionis not incorporated.
However, we have included the sentence-alignment criterion as alater filtering step in Nie and Cai (2001): If a pair of texts cannot be aligned reasonablywell, then that pair is removed.
This technique is shown to bring a large improvementfor the English-Chinese corpus.
A similar approach could also be envisioned for thecorpora of European languages, but in the present study, such an approach is not used.2.7 Mining ResultsPTMiner uses heuristics that are mostly language-independent.
This allows us to adaptit easily for different language pairs by changing a few parameters (e.g., prefix andsuffix lists of file name).
It is surprising that so simple an approach is nevertheless veryeffective.
We have been able, using PTMiner, to construct large parallel corpora fromthe Web for the following language pairs: English-French, English-Italian, English-German, English-Dutch, and English-Chinese.
The sizes of these corpora are shown inTable 1.One question that may be raised is how accurate the mining results are, or howparallel the pages identified are.
Actually, it is very difficult to answer this question.
Wehave not undertaken an extensive evaluation but have only performed a simple evalu-ation with a set of samples.
For English-French, from 60 randomly selected candidatesites, AltaVista indexed about 8,000 pages in French.
From these, the pair-scanningstep identified 4,000 pages with equivalents in English.
This showed that the lowerbound of recall of pairscanning is 50%.
The equivalence of the pair pages identifiedwas judged by an undergraduate student who participated in developing the prelim-inary version of PTMiner.
The criterion used to judge the equivalence of two pageswas subjective, with the general guideline being whether two pages describe the samecontents and whether they have similar structures.
To evaluate precision, 164 pairsof pages from the 4,000 identified were randomly selected and manually checked.
ItTable 1Automatically mined corpora.
n.a.
= not available.English-French English-German English-ItalianNumber of pairs 18,807 10,200 8,504Size (MB) 174/198 77/100 50/68Number of words (M) 6.7/7.1 1.8/1.8 1.2/1.3English-Dutch English-Chinese24,738 14,820n.a.
74/51n.a.
9.2/9.9392Computational Linguistics Volume 29, Number 3turned out that 162 of them were truly parallel.
This shows that the precision is closeto 99%.For an English-Chinese corpus, a similar evaluation has been reported in Chenand Nie (2000).
This evaluation was done by a graduate student working on PTMiner.Among 383 pairs randomly selected at the pair-scanning step, 302 pairs were foundto be really parallel.
The precision ratio is 79%, which is not as good as that of theEnglish-French case.
There are several reasons for this:?
Incorrect links.
It may be that a page is outdated but still indexed by thesearch engines.
A pair including that page will be eliminated in thecontent-filtering step.?
Pages that are designed to be parallel, although the contents are not all translatedyet.
One version of a page may be a simplified version of the other.
Somecases of this type can also be filtered out in the content-filtering step, butsome will still remain.?
Pages that are valid parallel pairs yet consist mostly of graphics rather than text.These pages cannot be used for the training of translation models.?
Pairs that are not parallel at all.
Filenames of some nonparallel pages mayaccidentally match the naming rules.
For example, ?
.
.
./et.html?
versus?
.
.
./etc.html?.Related to the last reason, we also observed that the names of parallel Chineseand English pages may be very different from one another.
For example, it is frequentpractice to use the Pinyin translation as the name of a Chinese page of the correspond-ing English file name (e.g., ?fangwen.html?
vs.
?visit.html?).
Another convention is touse numbers as the filenames.
For example ?1.html?
would correspond to ?2.html?.In either of these cases, our pair-scanning approach based on name similarity willfail to recognize the pair.
Overall, the naming of Chinese files is much more variableand flexible than the naming of files for European languages.
Hence, there exist fewerevident heuristics for Chinese than for the European languages that would allow usto enlarge the coverage and improve the precision of pair scanning.Given the potentially large number of erroneously identified parallel pairs, a ques-tion naturally arises: Can such a noisy corpus actually help CLIR?
We will examinethis question in Section 4.
In the next section we will briefly describe how statisticaltranslation models are trained on parallel corpora.
We will focus in our discussion onthe following languages: English, French, and Italian.
The resulting translation modelswill be evaluated in a CLIR task.3.
Building the Translation ModelsBilingual pairs of documents collected from the Web are used as training materialfor the statistical translation models that we exploit for CLIR.
In practice, this mate-rial must be organized into a set of small pairs of corresponding segments (typically,sentences), each consisting of a sequence of word tokens.
We start by presenting thedetails of this preparatory step and then discuss the actual construction of the trans-lation models.3.1 Preparing the Corpus3.1.1 Format Conversion, Text Segmentation, and Sentence Alignment.
The collec-tion process described in the previous section provides us with a set of pairs of HTML393Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRfiles.
The first step in preparing this material is to extract the textual data from thefiles and organize them into small, manageable chunks (sentences).In doing so, we try to take advantage of the HTML markup.
For instance, weknow that <P> tags normally identify paragraphs, <LI> tags mark list items that canalso often be interpreted as paragraphs, <Hn> tags are normally used to mark sectionheaders and may therefore be taken as sentences, and so on.Unfortunately, a surprisingly large number of HTML files on the Web are badlyformatted, which calls for much flexibility on the part of Web browsers.
To help copewith this situation, we employ a freely distributed tool called tidy (Ragget 1998), whichattempts to clean up HTML files, so as to make them XML-compliant.
This cleanupprocess mostly consists in normalizing tag names to the standard XHTML lower-case convention, wrapping tag attributes within double quotes and, most importantly,adding missing tags so as to end up with documents with balancing opening andclosing tags.Once this cleanup is done, we can parse the files with a standard SGML parser(we use nsgmls [Clark 2001]) and use the output to produce documents in the standardcesAna format.
This SGML format, proposed as part of the Corpus Encoding Standard(CES) (Ide, Priest-Dorman, and Ve?ronis 1995) has provisions for annotating simpletextual structures such as sections, paragraphs, and sentences.
In addition to the cuesprovided by the HTML tags, we employ a number of heuristics, as well as language-specific lists of common abbreviations and acronyms, to locate sentence boundarieswithin paragraphs.
When, as sometimes happens, the tidy program fails to make senseof its input on a particular file, we simply remove all SGML markup from the fileand treat the document as plain text, which means that we must rely solely on ourheuristics to locate paragraph and sentence boundaries.Once the textual data have been extracted from pairs of documents and are neatlysegmented into paragraphs and sentences, we can proceed with sentence alignment.This operation produces what we call couples, that is, minimal-size pairs of corre-sponding segments between two documents.
In the vast majority of cases, couplesconsist of a single pair of sentences that are translations of one another (what wecall 1-to-1 couples).
However, there are sometimes ?larger?
couples, as when a singlesentence in one language translates into two or more sentences in the other language(1-to-N or N-to-1), or when sentences translate many to many (N-to-M).
Conversely,there are also ?smaller?
couples, such as when a sentence from either one of the twotexts does not appear in the other (0-to-1 or 1-to-0).Our sentence alignments are carried out by a program called sfial, an improvedimplementation of the method described in Simard, Foster, and Isabelle (1992).
For agiven pair of documents, this program uses dynamic programming to compute thealignment that globally maximizes a statistical-based scoring function.
This functiontakes into account the statistical distribution of translation patterns (1-to-1, 1-to-N, etc.
)and the relative sizes of the aligned text segments, as well as the number of ?cognate?words within couples, that is, pairs of words with similar orthographies in the twolanguages (e.g.
statistical in English vs. statistique in French).The data produced up to this point in the preparation process constitutes whatwe call a Web-aligned corpus (WAC).3.1.2 Tokenization, Lemmatization, and Stopwords.
Since our goal is to use trans-lation models in an IR context, it seems natural to have both the translation modelsand the IR system operate on the same type of data.
The basic indexing units of ourIR systems are word stems.
Stemming is an IR technique whereby morphologicallyrelated word forms are reduced to a common form: a stem.
Such a stem does not394Computational Linguistics Volume 29, Number 3necessarily have to be a linguistic root form.
The principal function of the stem isto serve as an index term in the vocabulary of index terms.
Stemming is a form ofconflation: Equivalence classes of tokens help to reduce the variance in index terms.Most stemming algorithms fall into two categories: (1) suffix strippers, and (2) fullmorphological normalization (sometimes referred to as ?linguistic stemming?
in theIR literature).
Suffix strippers remove suffixes in an iterative fashion using rudimentalmorphological knowledge encoded in context-sensitive patterns.
The advantage of al-gorithms of this type (e.g., Porter 1980) is their simplicity and efficiency, although thisadvantage applies principally to languages with a relatively simple morphology, likeEnglish.
A different way of generating conflation classes is to employ full morpholog-ical analysis.
This process usually consists of two steps: First the texts are POS-taggedin order to eliminate each token?s part-of-speech ambiguity, and then word forms arereduced to their root form, a process that we refer to as lemmatization.
More informa-tion about the relative utility of morphological normalization techniques in IR systemscan be found in, for example, Hull (1996), Kraaij and Pohlmann (1996), and Braschlerand Ripplinger (2003).Lemmatizing and removing stopwords from the training material is also beneficialfor statistical translation modeling, helping to reduce the problem of data sparsenessin the training set.
Furthermore, function words and morpho-syntactic features typi-cally arise from grammatical constraints intrinsic to a language, rather than as directrealizations of translated concepts.
Therefore, we expect that removing them helpsthe translation model focus on meaning rather than form.
In fact, it has been shownin Chen and Nie (2000) that the removal of stopwords from English-Chinese train-ing material improves both the translation accuracy of the translation models and theeffectiveness of CLIR.
We expect a similar effect for European languages.We also have to tokenize the texts, that is, to identify individual word forms.Because we are dealing with Romance languages, this step is fairly straightforward:6We essentially segment the text using blank spaces and punctuation.
In addition, werely on a small number of language-specific rules to deal, for example, with elisionsin French (l?amour ?
l?
+ amour) and Italian (dell?arte ?
dell?
+ arte), contractions inFrench (au ?
a` + le), possessives in English (Bob?s ?
Bob + ?s), etc.Once we have identified word tokens, we can lemmatize or stem them.
For Italian,we relied on a simple, freely distributed stemmer from the Open Muscat project.7For French and English, we have access to more sophisticated tools that computeeach token?s lemma based on its part of speech (we use the HMM-based POS taggerproposed in Foster (1991) and extensive dictionaries with morphological information.As a final step, we remove stopwords.Usually, 1-1 alignments are more reliable than other types of alignment.
It is acommon practice to use only these alignments for model training, and this is what wedo.Table 2 provides some statistics on the processed corpora.3.2 Translation ModelsIn statistical translation modeling, we take the view that each possible target languagetext is a potential translation for any given source language text, but that some trans-lations are more likely than others.
In the terms of Brown et al (1990), a noisy-channeltranslation model is one that captures this state of affairs in a statistical distribution6 The processing on Chinese is described in Chen and Nie (2000).7 Currently distributed by OMSEEK:?http://cvs.sourceforge.net/cgi-bin/viewcvs.cgi/omseek/om/languages/?.395Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRTable 2Sentence-aligned corpora.English-French English-ItalianNumber of 1-1 alignments 1018K 196KNumber of tokens 6.7M/7.1M 1.2M/1.3MNumber of unique stems 200K/173K 102K/87KP(T | S), where S is a source language text and T is a target language text.8 With sucha model, translating S amounts to finding the target language text T?
that maximizesP(T | S).Modeling P(T | S) is, of course, complicated by the fact that there is an infinitenumber of possible source and target language texts, and so much of the work of thelast 15 years or so in statistical machine translation has been aimed at finding waysto overcome this complexity by making various simplifying assumptions.
Typically,P(T | S) is rewritten asP(T | S) = P(T)P(S | T)P(S)following Bayes?
law.
This decomposition of P(T | S) is useful in two ways: first,it makes it possible to ignore P(S) when searching for T?
; second, it allows us toconcentrate our efforts on the lexical aspects of P(S | T), leaving it to P(T) (the ?targetlanguage model?)
to take care of syntactic and other language-specific aspects.In one of the simplest and earliest statistical translation models, IBM?s Model 1,it is assumed that P(S | T) can be approximated by a computation that uses only?lexical?
probabilities P(s | t) over source and target language words s and t. In otherwords, this model completely disregards the order in which the individual words ofS and T appear.
Although this model is known to be too weak for general translation,it appears that it can be quite useful for an application such as CLIR, because manyIR systems also disregard word order, viewing documents and queries as unorderedbags of words.The P(s | t) distribution is estimated from a corpus of aligned sentences like theone we have produced from our Web-mined collection of bilingual documents, usingthe expectation maximization (EM) algorithm (Baum 1972) to find the parametersthat maximize the likelihood of the training set.
As in all machine-learning problems,especially those related to natural language, data sparseness is a critical issue in thisprocess.
Even with a large training corpus, many pairs of words (s, t) occur at verylow frequencies, and most never occur at all, making it impossible to obtain reliableestimates for the corresponding P(s | t).
Without adequate smoothing techniques, low-frequency events can have disastrous effects on the global behavior of the model, andunfortunately, in natural languages, low-frequency events are the norm rather thanthe exception.The goal of translation in CLIR is different from that in general language process-ing.
In the latter case it is important to enable a model to handle low-frequency wordsand unknown words.
For CLIR the coverage of low-frequency words or unknownwords by the model is less problematic.
Even if a low-frequency word is translated8 The model is referred to as noisy-channel because it takes the view that S is the result of some inputsignal T?s being corrupted while passing through a noisy channel.
In this context, the goal is to recoverthe initial input, given the corrupted output.396Computational Linguistics Volume 29, Number 3incorrectly, the global IR effectiveness will often not be significantly affected, becauselow-frequency words likely do not appear often in the document collection to besearched or other terms in the query could compensate for this gap.
Most IR algo-rithms are based on a term-weighting function that favors terms that occur frequentlyin a document but occur infrequently in the document collection.
This means that thebest index terms have a medium frequency (Salton and McGill 1983).
Stopwords and(near) hapaxes are less important for IR; limited coverage of very infrequent words ina translation model is therefore not critical for the performance of a CLIR system.Proper nouns are special cases of unknown words.
When they appear in a query,they usually denote an important part of the user?s intention.
However, we can adopta special approach to cope with these unknown words in CLIR without integratingthem as the generalized case in the model.
For example, one can simply retain all theunknown words in the query translation.
This approach works well for most casesin European languages.
We have previously shown that a fuzzy-matching approachbased on n-grams offers an effective means of overcoming small spelling variations inproper noun spelling (Kraaij, Pohlmann, and Hiemstra 2000).The model pruning techniques developed in computational linguistics are alsouseful for the models used in CLIR.
The beneficial effect is that unreliable (or low-probability) translations can be removed.
In Section 4, model smoothing will be moti-vated from a more theoretical point of view.
Here, let us first outline the two variationswe used to prune the models.The first one is simple, yet effective in our application: We consider unreliable allparameters (translation probabilities) whose value falls below some preset threshold(in practice, 0.1 works well).
These parameters are simply discarded from the model.The remaining parameters are then renormalized so that all marginal distributionssum to one.Another pruning technique is based on the relative contribution to the entropyof the model.
We retain the N most reliable parameters (in practice, N = 100K workswell).
The reliability of a parameter is measured with regard to its contribution to themodel?s entropy (Foster 2000).
In other words, we discard the parameters that leastaffect the overall probability of the training set.
The remaining parameters are thenrenormalized so that all marginal distributions sum to one.Of course, as a result of this, most pairs of words (s, t) are unknown to the trans-lation model (translation probability equals zero).
As previously discussed, however,this will not have a disastrous effect on CLIR; on the contrary, some positive effect canbe expected as long as there is at least one translation for each source term.One important characteristic of these noisy-channel models is that they are ?di-rectional.?
Depending on the intended use, it must be determined beforehand whichlanguage is the source and which the target for each pair of languages.
Although ?re-verse?
parameters can theoretically be obtained from the model through Bayes?
rule,it is often more practical to train two separate models if both directions are needed.This topic is also discussed in the next section.4.
Embedding Translation into the IR ModelWhen CLIR is considered simply as a combination of separate MT and IR components,the embedding of the two functions is not a problem.
However, as we explained inSection 1, there are theoretical motivations for embedding translation into the retrievalmodel.
Since translation models provide more than one translation, we will try toexploit this extra information, in order to enhance retrieval effectiveness.
In Section 4.1we will first introduce a monolingual probabilistic IR model based on cross entropy397Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRbetween a unigram language model for the query and one for the document.
Wediscuss the relationship of this model to IR models based on generative languagemodels.
Subsequently, we show several ways to add translation to the model: Onecan either translate the query language model from the source language into the targetlanguage (i.e., the document language) before measuring the cross entropy, or translatethe document model from the target language into the source language and thenmeasure the cross entropy.4.1 Monolingual IR Based on Unigram Language ModelsRecently, a new approach to IR based on statistical language models has gained wideacceptance.
The approach was developed independently by several groups (Ponteand Croft 1998; Miller, Leek, and Schwartz 1999; Hiemstra 1998) and has yieldedresults on several IR standardized evaluation tasks that are comparable to or betterthan those obtained using the existing OKAPI probabilistic model.
In comparisonwith the OKAPI model, the IR model based on generative language models has someimportant advantages: It contains fewer collection-dependent tuning parameters andis easy to extend.
For a more detailed discussion of the relationships between theclassical (discriminative) probabilistic IR models and recent generative probabilistic IRmodels, we refer the reader to Kraaij and Spitters (2003).
Probably the most importantidea in the language-modeling approach to IR is that documents are scored on theprobability that they generate the query; that is, the problem is reversed, an idea thathas successfully been applied in speech recognition.
There are various reasons thatthis approach has proven fruitful, probably the most important being that documentscontain much more data for estimating the parameters of a probabilistic model than doad hoc queries (Lafferty and Zhai 2001b).
For ad hoc retrieval, one could describe thequery formulation process as follows: A user has an ideal relevant document in mindand tries to describe it by mentioning some of the salient terms that he thinks occur inthe document, interspersed with some query stop phrasing like ?Relevant documentsmention.
.
.
.?
For each document in the collection, we can compute the probabilitythat the query is generated from a model representing that document.
This generationprocess can serve as a coarse way of modeling the user?s query formulation process.The query likelihood given each document can directly be used as a document-rankingfunction.
Formula (1) shows the basic language model, in which a query Q consists ofa sequence of terms T1, T2, .
.
.
, Tm that are sampled independently from a documentunigram model for document dk (Table 3 presents an explanation of the most importantsymbols used in equations (1)?
(12)):P(Q | Dk) = P(T1, T2, .
.
.
, Tm | Dk) ?m?j=1P(Tm | MDk) (1)In this formula MDk denotes a language model of Dk.
It is indeed an approximation ofDk.
Now, if a query is more probable given a language model based on document D1than given a language model based on document D2, we can then hypothesize thatdocument D1 is more likely to be relevant to the query than document D2.
Thus theprobability of generating a certain query given a document-based language model canserve as a score for ranking documents with respect to topical relevance.
It is commonpractice to work with log probabilities, which has the advantage of reducing productsto summations.
We will therefore rewrite (1) in logarithmic form.
Since terms mightoccur more than once in a query, we prefer to work with types ?i instead of tokens398Computational Linguistics Volume 29, Number 3Ti.
So c(Q, ?i) is the number of occurrences of ?i in Q (query term frequency); we willalso omit the document subscript k in the following presentation:log P(Q | D) =n?i=1c(Q, ?i) log P(?i | MD) (2)A second core technique from speech recognition that plays a vital role in languagemodels for IR is smoothing.
One obvious reason for smoothing is to avoid assigningzero probabilities for terms that do not occur in a document because the term prob-abilities are estimated using maximum-likelihood estimation.9 If a single query termdoes not occur in a document, this would result in a zero probability of generatingthat query, which might not be desirable in many cases, since documents discuss acertain topic using only a finite set of words.
It is very well possible that a term thatis highly relevant for a particular topic may not appear in a given document, sinceit is a synonym for other terms that are also highly relevant.
Longer documents willin most cases have a better coverage of relevant index terms (and consequently betterprobability estimates) than short documents, so one could let the level of smoothingdepend on the length of the document (e.g., Dirichlet priors).
A second reason forsmoothing probability estimates of a generative model for queries is that queries con-sist of (1) terms that have a high probability of occurrence in relevant documents and(2) terms that are merely used to formulate a proper query statement (e.g., ?Docu-ments discussing only X are not relevant?).
A mixture of a document language modeland a language model of typical query terminology (estimated on millions of queries)would probably give good results (in terms of a low perplexity).We have opted for a simple approach that addresses both issues, namely, applyinga smoothing step based on linear interpolation with a background model estimated ona large document collection, since we do not have a collection of millions of queries:log P(Q | D) =n?i=1c(Q, ?i) log((1 ?
?
)P(?i | MD) + ?P(?i | MC)) (3)Here, P(?i | MC) denotes the marginal probability of observing the term ?i, which can beestimated on a large background corpus, and ?
is the smoothing parameter.
A commonrange for ?
is 0.5?0.7, which means that document models have to be smoothed quiteheavily for optimal performance.
We hypothesize that this is mainly due to the query-modeling role of smoothing.
Linear interpolation with a background model has beenfrequently used to smooth document models (e.g., Miller, Leek, and Schwartz 1999;Hiemstra 1998).
Recently other smoothing techniques (Dirichlet, absolute discounting)have also been evaluated.
An initial attempt to account for the two needs for smoothing(sparse data problem, query modeling) with separate specialized smoothing functionsyielded positive results (Zhai and Lafferty 2002).We have tested the model corresponding to formula (3) in several different IRapplications: monolingual information retrieval, filtering, topic detection, and topictracking (cf.
Allen [2002] for a task description of the latter two tasks).
For severalof these applications (topic tracking, topic detection, collection fusion), it is important9 The fact that language models have to be smoothed seems to contradict the discussion in Section 3, inwhich we stated that rare terms are not critical for IR effectiveness, but it actually does not.
Smoothinghelps to make the distinction between absent important terms (middle-frequency terms) and absentnonimportant terms (high-frequency terms).
The score of a document that misses important termsshould be lowered more than that of a document that misses an unimportant term.399Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRTable 3Common symbols used in equations (1)?
(12) and theirexplanations.Symbol ExplanationQ Query has representation Q = {T1, T2, .
.
.
, Tn}D Query has representation D = {T1, T2, .
.
.
, Tn}MQ Query language modelMD Document language modelMC Background language model?i index termsi term in the source languageti term in the target language?
smoothing parameterc(x) counts of xthat scores be comparable across different queries (Spitters and Kraaij 2001).
The basicmodel does not provide such comparability of scores, so it has to be extended withscore normalization.
There are two important steps in doing this.
First of all, we wouldlike to normalize across query specificity.
The generative model will produce low scoresfor specific queries (since the average probability of occurrence is low) and higherscores for more general queries.
Normalization can be accomplished by modeling theIR task as a likelihood ratio (Ng 2000).
For each term in the query, the log-likelihoodratio (LLR) model judges how surprising it is to see the term, given the documentmodel in comparison with the background model:LLR(Q | D) = log P(Q | MD)P(Q | MC)=n?i=1c(Q, ?i) log((1 ?
?
)P(?i | MD) + ?P(?i | MC))P(?i | MC)(4)In (4), P(Q | MC) denotes the generative probability of the query given a languagemodel estimated on a large background corpus C. Note that P(Q | MC) is a query-dependent constant and does not affect document ranking.
Actually, model (4) has abetter justification than model (3), since it can be seen as a direct derivative of thelog-odds of relevance if we assume uniform priors for document relevance:logP(R | D, Q)P(R?
| D, Q)= logP(Q | R, D)P(Q | R?, D)+ logP(R | D)P(R?
| D)?
log P(Q | MD)P(Q | MC)+ K (5)In (5), R refers to the event that a user likes a particular document (i.e., the documentis relevant).The scores of model (4) still depend on the query length, which can be easilynormalized by dividing the scores by the query length (?i c(Q, ?i)).
This results informula (6) for the normalized log-likelihood ratio (NLLR) of the query:NLLR(Q | D) =n?i=1c(Q, ?i)?i c(Q, ?i)log((1 ?
?
)P(?i | MD) + ?P(?i | MC))P(?i | MC)(6)A next step is to view the normalized query term counts c(Q, ?i)/?i c(Q, ?i) asmaximum-likelihood estimates of a probability distribution representing the queryP(?i | MQ).
The NLLR formula can now be reinterpreted as a relationship between the400Computational Linguistics Volume 29, Number 3two probability distributions P(?
| MQ), P(?
| MD) normalized by the the third distribu-tion P(?
| MC).
The model measures how much better than the background model thedocument model can encode events from the query model; or in information-theoreticterms, it can be interpreted as the difference between two cross entropies:NLLR(Q | D) =n?i=1P(?i | Q) logP(?i | Dk)P(?i | C)= H(X | c)?
H(X | d) (7)In (7), X is a random variable with the probability distribution p(?i) = p(?i | MQ), andc and d are probability mass functions representing the marginal distribution and thedocument model.
Cross entropy is a measure of our average surprise, so the better adocument model ?fits?
a particular query distribution, the higher its score will be.10The representation of both the query and a document as samples from a dis-tribution representing, respectively, the user?s request and the document author?s?mindset?
has several advantages.
Traditional IR techniques like query expansionand relevance feedback can be reinterpreted in an intuitive framework of probabil-ity distributions (Lafferty and Zhai 2001a; Lavrenko and Croft 2001).
The frameworkalso seems suitable for cross-language retrieval.
We need only to extend the modelwith a translation function, which relates the probability distribution in one languageto the probability distribution function in another language.
We will present severalsolutions for this extension in the next section.The NLLR also has a disadvantage: It is less easy in the NLLR to integrate priorinformation about relevance into the model (Kraaij, Westerveld, and Hiemstra 2002),which can be done in a straightforward way in formula (1), by simple multiplication.CLIR is a special case of ad hoc retrieval, and usually a document length?based priorcan enhance results significantly.
A remedy that has proven to be effective is linearinterpolation of the NLLR score with a prior log-odds ratio log (P(R | D)/P(?R | D)(Kraaij 2002).
For reasons of clarity, we have chosen not to include this technique inthe experiments presented here.In the following sections, we will describe several ways to extend the monolingualIR model with translation.
The section headings include the run tags that will be usedin Section 5 to describe the experimental results.4.2 Estimating the Query Model in the Target Language (QT)In Section 4.1, we have seen that the basic retrieval model measures the cross entropybetween two language models: a language model of the query and a language modelof the document.11 Instead of translating a query before estimating a query model(the external approach), we propose to estimate the query model directly in the targetlanguage.
We will do this by decomposing the problem into two components that areeasier to estimate:P(ti | MQs) =L?jP(sj, ti | MQs) =L?jP(ti | sj, MQs)P(sj | MQs) ?L?jP(ti | sj)P(sj | MQs)(8)where L is the size of the source vocabulary.
Thus, P(ti | MQs) can be approximated bycombining the translation model P(ti | sj), which we can estimate on the parallel Webcorpus, and the familiar P(sj | MQs), which can be estimated using relative frequencies.10 The NLLR can also be reformulated as a difference of two Kullback-Leibler divergences (Ng 2000).11 We omit the normalization with the background model in the formula for presentation reasons.401Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRThis simplified model, from which we have dropped the dependency of P(ti | sj)on Q, can be interpreted as a way of mapping the probability distribution functionin the source language event space P(sj | MQs) onto the event space of the targetlanguage vocabulary.
Since this probabilistic mapping function involves a summationover all possible translations, mapping the query model from the source language canbe implemented as the matrix product of a vector representing the query probabilitydistribution over source language terms with the translation matrix P(ti | sj).12 Theresult is a probability distribution function over the target language vocabulary.Now we can substitute the query model P(?i | MQ) in formula (7) with the targetlanguage query model in (8) and, after a similar substitution operation for P(?i | MC),we arrive at CLIR model QT:NLLR-QT(Qs | Dt) =n?i=1L?j=1P(ti | sj)P(sj | MQs) log(1 ?
?
)P(ti | MDt) + ?P(ti | MCt)P(ti | MCt)(9)4.3 Estimating the Document Model in the Source Language (DT)Another way to embed translation into the IR model is to estimate the documentmodel in the source language:P(si | MDt) =N?jP(si, tj | MDt) =N?jP(si | tj, MDt)P(tj | MDt) ?N?jP(si | tj)P(tj | MDt)(10)where N is the size of the target vocabulary.
Obviously, we need a translation modelin the reverse direction for this approach.
Now we can substitute (10) for P(?i | MD)in formula (6), yielding CLIR model DT:NLLR-DT(Qs | Dt) =n?i=1P(si | MQs) log?Nj=1 P(si | tj)((1 ?
?
)P(tj | MDt) + ?P(tj | MCt))?Nj=1 P(si | tj)P(tj | MCt)(11)It is important to realize that both the QT and DT models are based on context-insensitive translation, since translation is added to the IR model after the indepen-dence assumption (1) has been made.
Recently, a more complex CLIR model based onrelaxed assumptions?context-sensitive translation but term independence?based IR?has been proposed in Federico and Bertoldi (2002).
In experiments on the CLEF testcollections, the aforementioned model also proved to be more effective; however, it hasthe disadvantage of reducing efficiency through its use of a Viterbi search procedure.4.4 Variant Models and BaselinesIn this section we will discuss several variant instantiations of the QT and DT modelsthat help us measure the importance of the number of translations (pruning) and theweighting of translation alternatives.
We also present several baseline CLIR algorithmstaken from the literature and discuss their relationship to the QT and DT models.12 For presentation reasons, we have replaced the variable ?
used in Section 4.1 with s and t for a term inthe source and target language, respectively.402Computational Linguistics Volume 29, Number 34.4.1 External Translation (MT, NAIVE).
As we argued in Section 1, the most simplesolution to CLIR is to use an MT system to translate the query and use the translationas the basis for a monolingual search operation in the target language.
This solutiondoes not require any modification to the standard IR model as presented in Section 4.1.We will refer to this model as the external-translation approach.
The translated queryis used to estimate a probability distribution for the query in the target language.
Thus,the order of operations is: (1) translate the query using an external tool; (2) estimatethe parameters P(ti | MQt) of a language model based on this translated query.In our experimental section below, we will list results with two different instantia-tions of the external-translation approach: (1) MT: query translation by Systran, whichattempts to use high-level linguistic analysis, context-sensitive translation, extensivedictionaries, etc., and (2) NAIVE: naive replacement of each query term with its trans-lations (not weighted).
The latter approach is often implemented using bilingual wordlists for CLIR.
It is clear that this approach can be problematic for terms with manytranslations, since they would then be assigned a higher relative importance.
TheNAIVE method is included here only to study the effect of the number of translationson the effectiveness of various models.4.4.2 Best-Match Translation (QT-BM).
In Section 3.2 we explained that there aredifferent possible strategies for pruning the translation model.
An extreme pruningmethod is best match, in which only the best translation is kept.
A best-match transla-tion model for query model translation (QT-BM) could also be viewed as an instanceof the external translation model, but one that uses a corpus-based disambiguationmethod.
Each query term is translated by the most frequent translation in the Webcorpus, disregarding the query context.4.4.3 Equal Probabilities (QT-EQ).
If we don?t know the precise probability of eachtranslation alternative for a given term, the best thing to do is to fall back on uniformtranslation probabilities.
This situation arises, for example, if we have only standardbilingual dictionaries.
We hypothesize that this approach will be more effective thanNAIVE but less effective than QT.4.4.4 Synonym-Based Translation (SYN).
An alternative way to embed translationinto the retrieval model is to view translation alternatives as synonyms.
This is, ofcourse, something of an idealization, yet there is certainly some truth to the approachwhen translations are looked up in a standard bilingual dictionary.
Strictly speaking,when terms are pure synonyms, they can be substituted for one another.
Combiningtranslation alternatives with the synonym operator of the INQUERY IR system (Broglioet al 1995), which conflates terms on the fly, has been shown to be an effective wayof improving the performance of dictionary-based CLIR systems (Pirkola 1998).
Inour study of stemming algorithms (Kraaij and Pohlmann 1996), we independentlyimplemented the synonym operator in our system.
This on-line conflation functionreplaces the members of the equivalence class with a class ID, usually a morphologicalroot form.
We have used this function to test the effectiveness of a synonymy-basedCLIR model in a language model IR setting.The synonym operator for CLIR can be formalized as the following class equiva-lence model (assuming n translations tj for term si and N unique terms in the targetlanguage):P(class(si) | MDt) =?nj c(tj, Dt)?Nj c(tj, Dt)=N?j?
(si, tj)P(tj | MDt) (12)403Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRwhere P(class(si) | MDt) is the probability that a member of the equivalence class of siis generated by the language model MDt and?
(si, tj) ={1 if tj ?
class(si)0 if tj /?
class(si)(13)Here c(tj, Dt) is the term frequency (counts) of term tj in document Dt.The synonym class function ?
(si, tj) can be interpreted as a special instantiationof the translation model P(si | tj) in (10), namely, P(si | tj) = 1 for all translations tjof si.
Of course, this does not yield a valid probability function, since the translationprobabilities for all translations si of a certain tj do not sum to one, because the pseudo?synonym classes are not disjunct because of sense ambiguity.
But the point is that thestructure of a probabilistic version of the SYN model is similar to that of the DT model,namely, one in which all translations have a reverse translation probability P(si | tj)equal to one.
This is obviously just an approximation of reality.
We therefore expect thatthis model will be less effective than the QT and DT models.
In our implementationof the SYN model, we formed equivalence classes by looking up all translations of asource term si in the translation model P(tj | si).
The translations receive a weight ofone and are used as pseudo translation?probabilities13 in the model corresponding toformula (11).4.5 Related WorkIn dictionary-based approaches, the number of translation alternatives is usually notas high as in (unpruned) translation models, so these alternatives can be used insome form of query expansion (Hull and Grefenstette 1996; Savoy 2002).
However, itis well known that most IR models break down when the number of translations ishigh.
To remedy this, researchers have tried to impose query structure, for example,by collecting translation alternatives in an equivalence class (Pirkola 1998), or via aquasi-Boolean structure (Hull 1997).The idea of embedding a translation step into an IR model based on query like-lihood was developed independently by several researchers (Hiemstra and de Jong1999; Kraaij, Pohlmann, and Hiemstra 2000; Berger and Lafferty 2000).
Initially trans-lation probabilities were estimated from machine-readable dictionaries, using simpleheuristics (Hiemstra et al 2001).
Other researchers have successfully used models sim-ilar to DT, in combination with translation models trained on parallel corpora, thoughnot from the Web (McNamee and Mayfield 2001; Xu, Weischedel, and Nguyen 2001).5.
ExperimentsWe carried out several contrastive experiments to gain more insight into the relativeeffectiveness of the various CLIR models presented in Sections 4.2?4.4.
We will firstoutline our research questions, before describing the experiments in more detail.5.1 Research QuestionsThe research questions we are hoping to answer are the following:1.
How do CLIR systems based on translation models perform with respectto reference systems (e.g., monolingual, MT )?13 It may be better to view them as mixing weights in this case.404Computational Linguistics Volume 29, Number 32.
Which manner of embedding a translation model is most effective forCLIR?
How does a probabilistically motivated embedding compare witha synonym-based embedding?3.
Is there a query expansion effect, and if so, how can we exploit it?4.
What is the relative importance of pruning versus weighting?5.
Which models are robust against noisy translations?The first two questions concern the main goal of our experiments: What is the effec-tiveness of a probabilistic CLIR system in which translation models mined from theWeb are an integral part of the model, compared to that of CLIR models in whichtranslation is merely an external component?
The remaining questions help us to un-derstand the relative importance of various design choices in our approach, such aspruning and translation model orientation.5.2 Experimental ConditionsWe have defined a set of contrastive experiments in order to help us answer theresearch questions presented above.
These experiments seek to compare:1.
The effectiveness of approaches incorporating a translation modelproduced from the Web to that of a monolingual baseline and anoff-the-shelf external query translation approach based on Systran (MT).2.
The effectiveness of embedding query model translation (QT) and that ofdocument model translation (DT).3.
The effectiveness of using the entire set of translations, each of which isweighted, (QT) to that of using just the most probable translation(QT-BM).4.
The effectiveness of weighted query model translation (QT) to that ofequally weighted translations (QT-EQ) and nonweighted translations(NAIVE).5.
The effectiveness of treating translations as synonyms (SYN) with that ofweighted translations (QT) and equally weighted translations (QT-EQ).6.
Different translation model pruning strategies: best N parameters orthresholding probabilities.Each strategy is represented by a run tag, as shown in Table 4.Table 5 illustrates the differences among the different translation methods.
It lists,for several CLIR models, the French translations of the word drug taken from one ofthe test queries that talks about drug policy.The translations in Table 5 are provided by the translation models P(e | f ) andP(f | e).
The translation models have been pruned by discarding the translations withP < 0.1 and renormalizing the model (except for SYN), or by retaining the 100K bestparameters of the translation model.
The first pruning method (probability threshold)has a very different effect on the DT method than on the QT method: The number ofterms that translate into drug, according to P(e | f ), is much larger than the numberof translations of drug found in P(f | e).
There are several possible explanations forthis: Quite a few French terms, including the verb droguer and the compounds pharma-core?sistance and pharmacothe?rapie, all translate into an English expression or compound405Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRTable 4Explanation of run tags.MatchingRun Tag Short Description Language SectionMONO monolingual run 4.1, 5.5MT Systran external query translation target 4.4.1, 5.5NAIVE equal probabilities target 4.4.1QT translation of the query language model target 4.2DT translation of the document language model source 4.3QT-BM best match, one translation per word target 4.4.2QT-EQ equal probabilities target 4.4.3SYN synonym run based on forward equal probabilities source 4.4.4Table 5Example translations: Stems and probabilities with different CLIR methods.Run ID Translation TranslationModelMT droguesQT <drogue, 0.55; medicament, 0.45> P(f | e) ?
0.1QT-EQ <drogue, 0.5; medicament, 0.5>QT-BM <drogue, 1.0>SYN <drogue, 1.0; medicament, 1.0>NAIVE <drogue, 1.0; medicament, 1.0>DT <antidrogue, 1.0; drogue, 1.0; droguer, 1.0; drug,1.0; me?dicament, 0.79; drugs, 0.70; drogue?, 0.61;narcotrafiquants, 0.57; relargage, 0.53; pharmacovigi-lance, 0.49; pharmacore?sistance, 0.47; me?dicamenteux,0.36; ste?ro?
?diens, 0.35, stupe?fiant, 0.34; assurance-me?dicaments, 0.33; surdose, 0.28; pharmacore?sistants,0.28; pharmacode?pendance, 0.27; pharmacothe?rapie,0.25; alcoolisme, 0.24; toxicomane, 0.23; bounce, 0.23; an-ticance?reux, 0.22; anti-inflammatoire, 0.17; selby, 0.16; es-cherichia, 0.14; homelessness, 0.14; anti-drogues, 0.14; an-tidiarrhe?ique, 0.12; imodium, 0.12; surprescription, 0.10>P(e | f ) ?
0.1QT <drogue, 0.45; medicament, 0.35; consommation, 0.06; re-lier, 0.03; consommer, 0.02; drug, 0.02; usage, 0.02; toxico-manie, 0.01; substance, 0.01; antidrogue, 0.01; utilisation,0.01; lier, 0.01; the?rapeutique, 0.01; actif, 0.01; pharmaceu-tique, 0.01>P(e | f ), 100KDT <reflexions, 1; antidrogue, 1; narcotrafiquants, 1;drug, 1; droguer, 0.87; drogue, 0.83; drugs, 0.81;me?dicament, 0.67; pharmacore?sistance, 0.47; pharma-core?sistants, 0.44; me?dicamenteux, 0.36; stupe?fiant, 0.34;assurance-me?dicaments, 0.33; pharmacothe?rapie, 0.33;amphe?tamine, 0.18; toxicomane, 0.17; me?morandum,0.10; toxicomanie, 0.08; architectural, 0.08; pharmacie,0.07; pharmaceutique, 0.06; the?rapeutique, 0.04; sub-stance, 0.01>P(f | e), 100K406Computational Linguistics Volume 29, Number 3involving the word drug.
Since our translation model is quite simple, these compound-compound translations are not learned.14 A second factor that might play a role is thegreater verbosity of French texts compared to their English equivalents (cf.
Table 2).For the models that have been pruned using the 100K-best-parameters criterion, thedifferences between QT and DT are smaller.
Both methods yield multiple translations,most of which seem related to drug, so there is a clear potential for improved recall asa result of the query expansion effect.
Notice, however, that the expansion concernsboth the medical and the narcotic senses of the word drug.
We will see in the followingsection that the CLIR model is able to take advantage of this query expansion effect,even if the expansion set is noisy and not disambiguated.5.3 The CLEF Test CollectionTo answer the research questions stated in section 5.1, we carried out a series of ex-periments on a combination of the CLEF-2000, -2001 and -2002 test collections.15 Thisjoint test collection consists of documents in several languages (articles from majorEuropean newspapers from the year 1994), 140 topics describing different informa-tion needs (also in several languages) and their corresponding relevance judgments.
(Relevance judgments are a human-produced resource that states, for a subset of adocument collection, whether a document is relevant for a particular query.)
We usedonly the English, Italian, and French data for the CLIR experiments reported here.
Themain reason for this limitation was that the IR experiments and translation modelswere developed at two different sites equipped with different proprietary tools.
Wechose language pairs for which the lemmatization/stemming step for both the trans-lation model training and indexing system were equivalent.
A single test collectionwas created by merging the three topic sets in order to increase the reliability of ourresults and sensitivity of significance tests.
Each CLEF topic consists of three parts:title, description, and narrative.
An example is given below:<num> C001<title> Architecture in Berlin<description> Find documents on architecture in Berlin.<narrative> Relevant documents report, in general, on thearchitectural features of Berlin or, in particular, on thereconstruction of some parts of the city after the fall of theWall.We used only the title and description parts of the topics and concatenatedthese simply to form the queries.
Table 6 lists some statistics on the test collection.16The documents were submitted to the preprocessing (stemming/lemmatization)procedure we described in Section 3.1.2.
However, for English and French lemmatiza-tion, we used the Xelda tools from XRCE,17 which perform morphological normaliza-tion slightly differently from the one described in Section 3.1.2.
However, since the two14 A more extreme case is query C044 about the ?tour de france.?
According to the P(e | f ) > 0.1translation model, there are 902 French words that translate into the ?English?
word de.
This is mostlydue to French proper names, which are left untranslated in the English parallel text.15 CLEF = Cross Language Evaluation Forum, ?www.clef-campaign.org?.16 Topics without relevant documents in a subcollection were discarded.17 Available at ?http://www.xrce.xerox.com/competencies/ats/xelda/summary.html?.407Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRTable 6Statistics on the test collection.French English ItalianDocument Source Le Monde Los Angeles Times La StampaNumber of documents 44,013 110,250 58,051Number of topics 124 122 125Number of relevant documents 1,189 2,256 1,878lemmatization strategies are based on the same principle (POS tagging plus inflectionremoval), the small differences in morphological dictionaries and POS tagging had nosignificant influence on retrieval effectiveness.18All runs use a smoothing parameter ?
= 0.3.
This value had been shown to workwell for CLIR experiments with several other collections.5.4 Measuring Retrieval EffectivenessThe effectiveness of retrieval systems can be evaluated using several measures.
Thebasic measures are precision and recall, which cannot be applied directly, since theyassume clearly separated classes of relevant and nonrelevant documents.
The mostwidely accepted measure for evaluating effectiveness of ranked retrieval systems isthe average uninterpolated precision, most often referred to as mean average precision(MAP), since the measure is averaged first over relevant documents and then acrosstopics.
Other measures, such as precision at a fixed rank, interpolated precision, orR-precision, are strongly correlated to the mean average precision, so they do not reallyprovide additional information (Tague-Sutcliffe and Blustein 1995; Voorhees 1998).The average uninterpolated precision for a given query and a given system versioncan be computed as follows: First identify the rank number n of each relevant docu-ment in a retrieval run.
The corresponding precision at this rank number is definedas the number of relevant documents found in the ranks equal to or higher than therespective rank r divided by n. Relevant documents that are not retrieved are assigneda precision of zero.
The average precision for a given query is defined as the averagevalue of the precision pr over all known relevant documents dij for that query.
Finally,the mean average precision can be calculated by averaging the average precision overall M queries:MAP =1MM?j=11NjNj?i=1pr(dij), where pr(dij) ={ rnini, if dij retrieved and ni ?
C0, in other cases(14)Here, ni denotes the rank of the document dij, which has been retrieved and is relevantfor query j, rni is the number of relevant documents found up to and including rankni, Nj is the total number of relevant documents of query j, M is the total number ofqueries, and C is the cutoff rank (C is 1,000 for TREC experiments).18 We have not been able to substantiate this claim with quantitative figures but did analyze the lemmasthat were not found in the translation dictionaries during query translation.
We did not find anystructural mismatches.408Computational Linguistics Volume 29, Number 3Since we compared many different system versions, which do not always dis-play a large difference in effectiveness, it is desirable to perform significance tests onthe results.
However, it is well known that parametric tests for data resulting fromIR experiments are not very reliable, since the assumptions of these tests (normal orsymmetric distribution, homogeneity of variances) are usually not met.
We checkedthe assumptions for an analysis of variance (by fitting a linear model for a within-subjects design) and found that indeed the distribution of the residual error was quiteskewed, even after transformation of the data.
Therefore, we resorted to a nonpara-metric alternative for the analysis of variance, the Friedman test (Conover 1980).
Thistest is preferable, for the analysis of groups of runs instead, to multiple sign-tests orWilcoxon signed-rank tests, since it provides overall alpha protection.
This means thatwe first test whether there is any significant difference at all between the runs, beforeapplying multiple-comparison tests.
Applying just a large number of paired signifi-cance tests at the 0.05 significance level without a global test leads very quickly to ahigh overall alpha.
After applying the Friedman test, we ran Fisher?s LSD multiple-comparison tests (recommended by Hull) to identify equivalence classes of runs (Hull1993; Hull, Kantor, and Ng 1999).
An equivalence class is a group of runs that donot differ significantly (e.g., in terms of mean average precision) from one another interms of performance.5.5 Baseline SystemsWe decided to have two types of baseline runs.
It is standard practice to take a mono-lingual run as a baseline.
This run is based on an IR system using document rankingformula (6).
Contrary to runs described in Kraaij (2002), we did not use any additionalperformance-enhancing devices, like document length?based priors or fuzzy match-ing, in order to focus on the basic retrieval model extensions, avoiding interactions.Systran was used as an additional cross-language baseline, to serve as a referencepoint for cross-language runs.
Notice that the lexical coverage of MT systems variesconsiderably across language pairs; in particular, the French-English version of Systranis quite good in comparison with those available for other language pairs.
We accessedthe Web-based version of Systran (December 2002), marketed as Babelfish, using thePerl utility babelfish.pm and converted the Unicode output to the ISO-Latin1 characterset to make it compatible with the Xelda-based morphology.5.6 ResultsTable 7 shows the results for the different experimental conditions in combinationwith a translation model pruned with the probability threshold criterion P > 0.1 (cf.Section 3.2).
For each run, we computed the mean average precision using the standardevaluation tool trec eval.
We performed Friedman tests on all the runs based on theWeb translation models, because these are the runs in which we are most interested;furthermore, one should avoid adding runs that are quite different to a group that isrelatively homogeneous, since this can easily lead to a false global-significance test.The Friedman test (as measured on the F distribution) proved significant at the p <0.05 level in all cases, so we created equivalence classes using Fisher?s LSD method,which are denoted by letters.
Letters are assigned to classes in decreasing order ofperformance; so if a run is a member of equivalence class a, it is one of the best runsfor that particular experimental condition.The last four rows of the table provide some additional statistics on the querytranslation process.
For both the forward (P(t | s),fw) and the reverse (P(s | t),rev)409Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRTable 7Mean average precision and translation statistics (p > 0.1).English- French- English- Italian-Run ID French English Italian EnglishMONO 0.4233 0.4705 0.4542 0.4705MT 0.3478 0.4043 0.3060 0.3249QT a:0.3760 a:0.4126 a,b:0.3298 a:0.3526DT a:0.3677 a,b:0.4090 a:0.3386 a,b:0.3328SYN a:0.3730 b,c:0.3987 a,b:0.3114 b:0.3498QT-EQ a:0.3554 a,b:0.3987 c,d:0.3035 b,c:0.3299QT-BM a:0.3463 c,d:0.3769 b,c:0.3213 b:0.3221NAIVE b:0.3303 d:0.3596 d:0.2881 c:0.3183Percentage of missed forward 9.6 13.54 16.79 9.17Percentage of missed reverse 9.08 14.04 15.48 11.31Number of translations forward 1.65 1.66 1.86 2.13Number of translations reverse 22.72 29.6 12.00 22.95Table 8Mean average precision and translation statistics (best 100K parameters).English- French- English- Italian-Run ID French English Italian EnglishMONO 0.4233 0.4705 0.4542 0.4705MT 0.3478 0.4043 0.3060 0.3249DT a:0.3909 a:0.4073 a:0.3728 a:0.3547QT a,b:0.3878 a:0.4194 a:0.3519 a:0.3678QT-BM b:0.3436 b:0.3702 b:0.3236 b:0.3124SYN c:0.3270 b:0.3643 b:0.2958 c:0.2808QT-EQ c:0.3102 b:0.3725 c:0.2602 c:0.2595NAIVE d:0.2257 c:0.2329 d:0.2281 d:0.2021Percentage of missed forward 11.04 14.65 16.06 9.36Percentage of missed reverse 10.39 16.81 15.76 10.53Number of translations forward 7.04 7.00 6.36 7.23Number of translations reverse 10.51 12.34 13.32 17.20translation model, we list the percentage of missed translations19 of unique queryterms and the average number of translations per unique query term.
Table 8 showsthe results for the same experimental conditions, but this time the translation modelswere pruned by taking the n best translation relations according to an entropy criterion,where n = 100, 000.Several other similar pruning methods have also been tested on the CLEF-2000subset of the data (e.g.
P > 0.01, P > 0.05, 1M parameters, 10K parameters).
How-ever, the two cases shown in Tables 7 and 8 represent the best of the two familiesof pruning techniques.
Our goal was not to do extensive parameter tuning in or-der to find the best-performing combination of models, but rather to detect somebroad characteristics of the pruning methods and their interactions with the retrievalmodel.19 This figure includes proper nouns.410Computational Linguistics Volume 29, Number 3Table 9Mean average precision of combination run, compared to baselines.Run ID English-French French-English English-Italian Italian-EnglishMONO 0.4233 0.4705 0.4542 0.4705MT 0.3478 (82%) 0.4043 (86%) 0.3060 (67%) 0.3249 (69%)DT+QT 0.4042 (96%) 0.4273 (87%) 0.3837 (84%) 0.3785 (80%)Since the pruned forward and reverse translation models yield different translationrelations (cf.
Table 5), we hypothesized that it might be effective to combine them.Instead of combining the translation probabilities directly, we chose to combine theresults of the QT and DT models by interpolation of the document scores.
Resultsfor combinations based on the 100K models are shown in Table 9.
Indeed, for allthe language pairs, the combination run improves upon each of its component runs.This means that each component run can compensate for missing translations in thecompanion translation model.5.7 Discussion5.7.1 Web-Based CLIR versus MT-Based CLIR.
Our first observation when examiningthe data is that the runs based on translation models are comparable to or better thanthe Systran run.
Sign tests showed that there was no significant difference between theMT and QT runs for English-French and French-English language pairs.
The QT runswere significantly better at the p = 0.01 level for the Italian-English and English-Italianlanguage pairs.This is a very significant result, particularly since the performance of CLIR withSystran has often been among the best in the previous CLIR experiments in TREC andCLEF.
These results show that the Web-based translation models are effective means forCLIR tasks.
The better results obtained with the Web-based translation models confirmour intuition, stated in Section 1, that there are better tools for query translation inCLIR than off-the-shelf commercial MT systems.Compared to the monolingual runs, the best CLIR performance with Web-basedtranslation models varies from 74.1% to 93.7% (80% to 96% for the combined QT+DTmodels) of the monolingual run.
This is within the typical range of CLIR performance.More generally, this research successfully demonstrates the enormous potential of par-allel Web pages and Web-based MT.We cannot really compare performance across target languages, since the relevantdocuments are not distributed in a balanced way: Some queries do not yield any rele-vant document in some languages.
This partly explains why the retrieval effectivenessof the monolingual Italian-Italian run is much higher than the monolingual Frenchand English runs.
We can, however, compare methods within a given language pair.5.7.2 Comparison of Query Model Translation (QT), Document Model Translation(DT), and Translations Modeled as Synonyms (SYN).
Our second question in Section5.1 concerned the relative effectiveness of the QT and DT models.
The experimentalresults show that there is no clear winner; differences are small and not significant.There seems to be some correlation with translation direction, however: The QT modelsperform better than DT on the X-English pairs, and the DT models perform betteron the English-X pairs.
This might indicate that the P(e | f ) and P(e | i) translationmodels are more reliable than their reverse counterparts.
A possible explanation for411Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRthis could be that the average English sentence is shorter than the correspondingFrench and Italian sentence.
The average number of tokens per sentence is, respectively,6.6/6.9 and 5.9/6.9 for English/French and English/Italian corpora.
This may lead tomore reliable estimates for P(e | f ) and P(e | i) than the reverse.
However, furtherinvestigation is needed to confirm this, since differences in morphology could alsocontribute to the observed effect.
Still, the fact that QT models perform just as well asDT models in combination with translation models is a new result.We also compared our QT and DT models to the synonym-based approach (SYN)(Pirkola 1998).
Both the QT and DT models were significantly more effective than thesynonym-based model.
The latter seems to work well when the number of translationsis relatively small but cannot effectively handle the large number of (pseudo)translationsproduced by our 100K translation models.
The synonym-based model usually per-forms better than the models based on query translation with uniform probabilities,but the differences are not significant in most cases.5.7.3 Query Expansion Effect.
In Section 1 we argued that using just one translation(as MT does) is probably a suboptimal strategy for CLIR, since there is usually morethan one good translation for a particular term.
Looking at probabilistic dictionaries,we have also seen that the distinction between a translation and a closely related termcannot really be made on the basis of some thresholding criterion.
Since it is wellknown in IR that adding closely related terms can improve retrieval effectiveness, wehypothesized that adding more than one translation would also help.
The experimen-tal results confirm this effect.
In all but one case (English-French, P > 0.1), using alltranslations (QT) yielded significantly better performance than choosing just the mostprobable translation (QT-BM).
For the P > 0.1 models, the average number of transla-tions in the forward direction is only 1.65, so the potential for a query expansion effectis limited, which could explain the nonsignificant difference for the English-Frenchcase.Unfortunately, we cannot say whether the significant improvement in effectivenessoccurs mainly because the probability of giving at least one good translation (which isprobably the most important factor for retrieval effectiveness [Kraaij 2002; McNameeand Mayfield 2002]) is higher for QT or indeed because of the query expansion effect.A simulation experiment is needed to quantify the relative contributions.
Still, it isof great practical importance that more (weighted) translations can enhance retrievaleffectiveness significantly.5.7.4 Pruning and Weighting.
A related issue is the question of whether it is moreimportant to prune translations or to weight them.
Grefenstette (cf.
Section 1) originallypointed out the importance of pruning and weighting translations for dictionary-basedCLIR.
Pruning was seen as a means of removing unwanted senses in a dictionary-based CLIR application.
Our experiments confirm the importance of pruning andweighting, but in a slightly different manner.
In a CLIR approach based on a Webtranslation model, the essential function of pruning is to remove spurious translations.Polluted translation models will result in a very poor retrieval effectiveness.
As faras sense disambiguation is concerned, we believe that our CLIR models can handlesense ambiguity quite well.
Our best-performing runs, based on the 100K models,have on average seven translations per term!
Too much pruning (e.g., best match) issuboptimal.
However, the more translation alternatives we add, the more importanttheir relative weighting becomes.We have compared weighted translations (QT) with uniform translation proba-bilities (QT-EQ).
In each of the eight comparisons (four language pairs, two pruning412Computational Linguistics Volume 29, Number 3techniques), weighting results in an improved retrieval effectiveness.
The differenceis significant in six of the eight cases.
Differences are not significant for the P < 0.1English-French and French-English translation models.
We think this is due to thesmall number of translations; a uniform translation probability will not differ radi-cally from the estimated translation probabilities.The importance of weighting is most evident when the 100K translation modelsare used.
These models yield seven translations on average for each term.
The CLIRmodels based on weighted translations are able to exploit the additional informationand show improved effectiveness with respect to the P < 0.1 models.
The performanceof unweighted CLIR models (QT-EQ and SYN) is seriously impaired by the highernumber of translations.The comparison of the naive dictionary-like replacement method, which does notinvolve any normalization for the number of translations per term (NAIVE), with QT-EQ shows that normalization (i.e.
a minimal probabilistic embedding) is essential.
TheNAIVE runs have the lowest effectiveness of all variant systems (with significant dif-ferences).
Interestingly, it seems better to select just the one most probable translationthan taking all translations unweighted.5.7.5 Robustness.
We pointed out in the previous section that the weighted modelsare more robust, in the sense that they can handle a large number of translations.We found, however, that the query model translation method (QT) and the docu-ment model translation method (DT) display a considerable difference in robustnessto noisy translations.
Initially we expected that the DT method (in which the match-ing takes place in the source language) would yield the best results, since this modelhas previously proven to be successful for several quite different language pairs (e.g.,European languages, Chinese, and Arabic using parallel corpora or dictionaries astranslation devices [McNamee and Mayfield 2001; Xu, Weischedel, and Nguyen 2001;Hiemstra et al 2001]).However, our initial DT runs obtained extremely poor results.
We discovered thatthis was largely due to noisy translations from the translation model (pruned by theP < 0.1 or 100K method), which is based on Web data.
There are many terms inthe target language that occur very rarely in the parallel Web corpus.
The translationprobabilities for these terms (based on the most probable alignments) are thereforeunreliable.
Often these rare terms (and nonwords like xc64) are aligned with morecommon terms in the other language and are not pruned by the default pruning criteria(P > 0.1 or best 100K parameters), since they have high translation probabilities.This especially poses a problem for the DT model, since it includes a summationover all terms in the target language that occur in the document and have a nonzerotranslation probability.
We devised a supplementary pruning criterion to remove thesenoisy translations, discarding all translations for which the source term has a marginalprobability in the translation model that is below a particular value (typically between10?6 and 10?5).
Later we discovered that a simple pruning method was even moreeffective: discarding all translations for which either the source or target term containsa digit.
The results in Tables 7 and 8 are based on the latter additional pruning criterion.The QT approach is less sensitive to noisy translations arising from rare terms in thetarget language, because it is easy to remove these translations using a probabilitythreshold.
We deduce that extra care therefore has to be taken to prune translationmodels for the document model translation approach to CLIR.413Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR6.
ConclusionsStatistical translation models require large parallel corpora, and unfortunately, onlya few manually constructed ones are available.
In this article, we have explored thepossibility of automatically mining the Web for parallel texts in order to constructsuch corpora.
Translation models are then trained on these corpora.
We subsequentlyexamined different ways to embed the resulting translation models in a cross-languageinformation retrieval system.To mine parallel Web pages, we constructed a mining system called PTMiner.
Thissystem employs a series of heuristics to locate candidate parallel pages and determinewhether they are indeed parallel.
We have successfully used PTMiner to construct cor-pora for a number of different language pairs: English-French, English-Italian, English-German, English-Dutch, and English-Chinese.
The language-independent characteris-tics of PTMiner allowed us to adapt it quite easily to different language pairs.The heuristics used in the mining process seem to be effective.
Although the systemcannot collect all pairs of parallel pages, our preliminary evaluation shows that itsprecision is quite high.
(The recall ratio is less important in this context because of theabundance of parallel pages on the Web.
)The mining results?parallel corpora?are subsequently used to train statisticaltranslation models, which are exploited in a CLIR system.
The major advantage ofthis approach is that it can be fully automated, avoiding the tedious work of man-ual collection of parallel corpora.
On the other hand, compared to manually preparedparallel corpora, our mining results contain more noise (i.e., nonparallel pages).
Fora general translation task this may be problematic; for CLIR, however, the noise con-tained in the corpora is less dramatic.
In fact, IR is strongly error tolerant.
A smallproportion of incorrect translation words can be admitted without a major impacton global effectiveness.
Our experiments showed that a CLIR approach based on themined Web corpora can in fact outperform a good MT system (Systran).
This confirmsour initial hypothesis that noisy parallel corpora can be very useful for applicationssuch as CLIR.
Our demonstration that the Web can indeed be used as a large parallelcorpus for tasks such as CLIR is the main contribution of this article.Most previous work on CLIR has separated the translation stage from the retrievalstage (i.e., query translation is considered as a preprocessing step for monolingual IR).In this article, we have integrated translation and retrieval within the same framework.The advantage of this integration is that we do not need to obtain the optimal transla-tion of a source query, and then an optimal retrieval result given a query translation,but instead aim for the optimal global effect.
The comparisons between our approachand simulated external approaches clearly show that an integrated approach performsbetter.We also compared two ways of embedding translation models within a CLIR sys-tem: (1) translating the source query model into the target (document) language, and(2) translating the document model into the source language.20 Both embedding meth-ods produced very good results compared to our reference run with Systran.
However,it is still too early to assert which embedding method is superior.
We did observe asignificant difference in robustness between the two methods: The document modeltranslation method is much more sensitive to spurious translations, since the modelincorporates into a query term all source terms that have a nonzero translation proba-bility.
We devised two supplementary pruning techniques that effectively removed the20 Another method that interprets multiple translations as synonyms is a special case of the latter.414Computational Linguistics Volume 29, Number 3noisy terms: removing terms containing digits, and removing translations based onsource terms with a low marginal probability.
(This latter approach is perhaps moreprincipled.
)On the use of statistical translation models for CLIR, we have demonstrated thatthis naturally produces a desired query expansion effect, resulting in more relateddocuments being found.
In our experimental evaluation, we saw that it is usuallybetter to include more than one translation, and to weigh these translations accordingto the translation probabilities, rather than using the resulting translation model asa bilingual lexicon for external translation.
This effect partly accounts for the successof our approach in comparison with an MT-based approach, which retains only onetranslation per sense.
However, this technique should not be exaggerated; otherwise,too much noise will be introduced.
To avoid this, it is important to incorporate pruning.We investigated several ways to prune translation models.
The best results wereobtained with a pruning method based on the top 100K parameters of the transla-tion model.
The translation models pruned with the best 100K parameters methodproduced more than seven translations per word on average, demonstrating the ca-pability of the CLIR model to handle translation ambiguity and exploit co-occurrenceinformation from the parallel corpus for query expansion.There are several ways in which our approach can be improved.
First, regardingPTMiner, more or better heuristics could be integrated into the mining algorithm.
Aswe mentioned, parallel Web sites are not always organized in the ways we wouldexpect.
This is particularly the case for those in non-European languages such as Chi-nese and Japanese.
Hence, one of the questions we wish to investigate is how toextend the coverage of PTMiner to more parallel Web pages.
One possible improve-ment would be to integrate a component that ?learns?
the organization patterns ofa particular Web site (assuming, of course, that the Web site is organized in a con-sistent way).
Preliminary tests have shown that this is possible to some extent: Wecan recognize dynamically that the parallel pages on ?www.operationid.com?
are at?www.operationcarte.com?
or that the file ?index1.html?
corresponds to ?index2.html?.Such criteria complement the ones currently employed in PTMiner.In its current form, PTMiner scans candidates for parallel Web sites according tosimilarities in their file names.
This step does not exploit the hyperlinks between thepages, whereas we know that two pages that are referenced at comparable structuralpositions in two parallel pages have a very high chance of themselves being parallel.Exploiting hyperlink structure to (help) find parallel Web pages could well improvethe quality of PTMiner.When the mining results are not fully parallel, it would be interesting to attemptto clean them in order to obtain a higher-quality training material.
One possible ap-proach for doing this would be to use sentence alignment as an additional filter, aswe mentioned earlier.
This approach has been applied successfully to our English-Chinese Web corpus.
The cleaned corpus results in both higher translation accuracyand higher CLIR effectiveness.
However, this approach has still to be tested for theEuropean languages.In this study, we hypothesized that IBM Model 1 is appropriate for CLIR, primarilybecause word order is not important for IR.
Although it is true that word order is notimportant in current IR approaches, it is definitely important to consider context wordsduring the translation.
For example, when deciding how to translate the French wordtableau (which may refer to a painting, a blackboard, a table [of data], etc.
), if weobserve artistique (?artistic?)
next to it, then it is pretty certain that tableau refers to apainting.
A more sophisticated translation model than IBM Model 1 could produce abetter selection of translation words.415Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRWe also rely solely on word translation in our approach, although it is well knownthat this simplistic approach cannot correctly translate compound terms such as pommede terre (?potato?)
and cul de sac (?no exit?).
Incorporating the translation of compoundterms in a translation model should result in additional improvements for CLIR.
Ourpreliminary experiments (Nie and Dufort 2002) on integrating the translation of com-pounds certainly showed this, with improvement of up to 70% over a word-basedapproach.
This direction warrants further investigation.Finally, all our efforts thus far to mine parallel Web pages have involved English.How can we deal with CLIR between, say, Chinese and German, for which there arefew parallel Web sites?
One possible solution would be to use English as a pivotlanguage, even though the two-step translation involved would certainly reduce ac-curacy and introduce more noise.
Nevertheless, several authors have shown that apivot approach can still produce effective retrieval and can at least complement adictionary-based approach (Franz, McCarley, and Ward 2000; Gollins and Sanderson2001; Lehtokangas and Airio 2002).AcknowledgmentsThis work was partly funded by a researchgrant from the Dutch Telematics Institute:DRUID project.
We would like to thankXerox Research Center Europe (XRCE) formaking its Xelda toolkit available to us.
Wewould also like to thank George Foster formaking his statistical MT toolkit availableand for many interesting discussions.Special thanks are due to Jiang Chen, whocontributed to the building of PTMiner.Finally, we want to thank ElliottMacklovitch and the two anonymousreviewers for their constructive commentsand careful review.
Part of this work wascarried out while the first author wasvisiting the RALI laboratory at Universite?de Montre?al.ReferencesAllen, James, editor.
2002.
Event-BasedInformation Organization.
KluwerAcademic, Boston.Baum, L. E. 1972.
An inequality andassociated maximization technique instatistical estimations of probabilisticfunctions of Markov processes.Inequalities, 3:1?8.Berger, Adam and John Lafferty.
2000.
TheWeaver system for document retrieval.
InEllen M. Voorhees and Donna K. Harman,editors, The Eighth Text Retrieval Conference(TREC-8), volume 8.
National Institute ofStandards and Technology SpecialPublication 500-246, Gaithersburg, MD.Braschler, Martin and Ba?rbel Ripplinger.2003.
Stemming and decompounding forGerman text retrieval.
In FabrizioSebastiani, editor, Advances in InformationRetrieval: 25th European Conference on IRResearch (ECIR 2003), Pisa, Italy, April 2003,Proceedings.
Lecture Notes in ComputerScience 2633.
Springer, Berlin.Broglio, John, James P. Callan, W. BruceCroft, and Daniel W. Nachbar.
1995.Document retrieval and routing using theINQUERY system.
In Donna K. Harman,editor, The Third Text Retrieval Conference,volume 4.
National Institute of Standardsand Technology Special Publication500-236, Gaithersburg, MD, pages 29?38.Brown, Peter F., John Cocke, StephenA.
Della Pietra, Vincent J. Della Pietra,Fredrick Jelinek, John D. Lafferty,Robert L. Mercer, and Paul S. Roossin.1990.
A statistical approach to machinetranslation.
Computational Linguistics,16(2):79?85.Chen, Jiang and Jian-Yun Nie.
2000.
Webparallel text mining for Chinese-Englishcross-language information retrieval.
InProceedings of NAACL-ANLP, Seattle.Clark, James.
2001.
SP?An SGML SystemConforming to International Standard ISO8879?Standard Generalized MarkupLanguage.
Available at?http://www.jclark.com/sp/?.Conover, William Jay.
1980.
PracticalNonparametric Statistics.
Wiley, London.Croft, W. Bruce, Alistair Moffat,C.
J.
?Keith?
van Rijsbergen, RossWilkinson, and Justin Zobel, editors.
1998.Proceedings of the 21st Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR?98).
ACM Press.Dumais, Susan T., Todd A. Letsche,Michael L. Littman, and Thomas K.Landauer.
1997.
Automatic cross-languageretrieval using latent semantic indexing.In AAAI Spring Symposium on416Computational Linguistics Volume 29, Number 3Cross-Language Text and Speech Retrieval,Palo Alto, CA.Federico, Marcello and Nicola Bertoldi.2002.
Statistical cross-languageinformation retrieval using N-best querytranslations.
In Micheline Beaulieu,Ricardo Baeza-Yates, Sung Hyon Myaeng,and Kalervo Ja?rvelin, editors, Proceedingsof the 25th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval (SIGIR 2002).
ACMPress, New York.Foster, George F. 1991.
Statistical lexicaldisambiguation.
Master?s thesis, McGillUniversity, School of Computer Science.Foster, George.
2000.
A maximumentropy/minimum divergence translationmodel.
In Proceedings of the 38th AnnualMeeting of the Association for ComputationalLinguistics (ACL), Hong Kong.Franz, Martin, J. Scott McCarley, andR.
Todd Ward.
2000.
Ad hoc,cross-language and spoken documentretrieval at IBM.
In Ellen M. Voorhees andDonna K. Harman, editors, The Eighth TextRetrieval Conference (TREC-8), volume 8.National Institute of Standards andTechnology Special Publication 500-246,Gaithersburg, MD.Franz, Martin, J. Scott McCarley, Todd Ward,and Wei-Jing Zhu.
2001.
Quantifying theutility of parallel corpora.
In W. BruceCroft, David J. Harper, Donald H. Kraft,and Justin Zobel, editors, Proceedings of the24th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval (SIGIR 2001).
ACMPress, New York.Gao, Jianfeng, Jian-Yun Nie, Endong Xun,Jian Zhang, Ming Zhou, and ChangningHuang.
2001.
Improving query translationfor cross-language information retrievalusing statistical models.
In W. BruceCroft, David J. Harper, Donald H. Kraft,and Justin Zobel, editors, Proceedings of the24th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval (SIGIR 2001).
ACMPress, New York.Gollins, Tim and Mark Sanderson.
2001.Improving cross language retrieval withtriangulated translation.
In W. BruceCroft, David J. Harper, Donald H. Kraft,and Justin Zobel, editors, Proceedings of the24th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval (SIGIR 2001).
ACMPress, New York.Grefenstette, Gregory.
1998.
The problem ofcross-language information retrieval.
InGregory Grefenstette, editor,Cross-Language Information Retrieval.Kluwer Academic, Boston, pages 1?9.Harman, Donna K., editor.
1995.
The ThirdText Retrieval Conference (TREC-3),volume 4.
National Institute of Standardsand Technology Special Publication500-236.Hearst, Marti, Fred Gey, and Richard Tong,editors.
1999.
Proceedings of the 22ndAnnual International ACM SIGIR Conferenceon Research and Development in InformationRetrieval (SIGIR ?99).
ACM Press.Hiemstra, Djoerd.
1998.
A linguisticallymotivated probabilistic model ofinformation retrieval.
In ChristosNicolaou and Constantine Stephanides,editors, Research and Advanced Technologyfor Digital Libraries?Second EuropeanConference (ECDL?98), Proceedings.
LectureNotes in Computer Science 1513.
SpringerVerlag, Berlin.Hiemstra, Djoerd.
2001.
Using LanguageModels for Information Retrieval.
Ph.D.thesis, University of Twente, Enschede,the Netherlands.Hiemstra, Djoerd and Franciska de Jong.1999.
Disambiguation strategies forcross-language information retrieval.
InEuropean Conference on Digital Libraries,pages 274?293.Hiemstra, Djoerd and Wessel Kraaij.
1999.Twenty-one at TREC-7: Ad hoc and crosslanguage track.
In Ellen M. Voorhees andDonna K. Harman, editors, The SeventhText Retrieval Conference (TREC-7),volume 7.
National Institute of Standardsand Technology Special Publication500-242, Gaithersburg, MD.Hiemstra, Djoerd, Wessel Kraaij, Rene?ePohlmann, and Thijs Westerveld.
2001.Translation resources, merging strategiesand relevance feedback.
In Carol Peters,editor, Cross-Language Information Retrievaland Evaluation.
Lecture Notes in ComputerScience 2069.
Springer Verlag, Berlin.Hull, David.
1993.
Using statistical testingin the evaluation of retrieval experiments.In Robert Korfhage, Edie Rasmussen, andPeter Willett, editors, Proceedings of the16th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval (SIGIR ?93), pages329?338.
ACM Press, New York.Hull, David.
1996.
Stemming algorithms?acase study for detailed evaluation.
Journalof the American Society for InformationScience, 47(1): 47?84.Hull, David.
1997.
Using structured queriesfor disambiguation in cross-languageinformation retrieval.
In David Hull andDouglas Oard, editors, AAAI Symposium417Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRon Cross-Language Text and Speech Retrieval.American Association for ArtificialIntelligence.
Available at?http://www.aaai.org/Press/Reports/Symposia/Spring/ss-97-05.html?.Hull, David and Gregory Grefenstette.
1996.Querying across languages: Adictionary-based approach to multilingualinformation retrieval.
In Hans-Peter Frei,Donna Harman, Peter Scha?uble, and RossWilkinson, editors, Proceedings of the19th Annual International ACM SIGIRConference on Research and Developmentin Information Retrieval (SIGIR ?96).ACM Press, New York, pages 49?57.Hull, David, Paul B. Kantor, andKwong Bor Ng.
1999.
Advancedapproaches to the statistical analysis ofTREC information retrieval experiments.Unpublished report, Rutgers University,New Brunswick, NJ.Ide, Nancy, G. Priest-Dorman and JeanVe?ronis.
1995.
Corpus encoding standard.Available at ?http://www.cs.vassar.edu/CES/?.Isabelle, Pierre, Michel Simard, and PierrePlamondon.
1998.
Demo.
Available at?http://www.rali.iro.umontreal.ca/SILC/SILC.en.cgi?.Jansen, Bernard J., Amanda Spink, DeitmarWolfram, and Tefko Saracevic.
2001.Searching the Web: The public and theirqueries.
Journal of the American Society forInformation Science and Technology,53(3):226?234.Kraaij, Wessel.
2002.
TNO at CLEF-2001:Comparing translation resources.
In CarolPeters, Martin Braschler, Julio Gonzalo,and Michael Kluck, editors, Evaluation ofCross-Language Information RetrievalSystems: Second Workshop of theCross-Language Evaluation Forum (CLEF2001).
Springer Verlag, Berlin.Kraaij, Wessel and Rene?e Pohlmann.
1996.Viewing stemming as recall enhancement.In Hans-Peter Frei, Donna Harman, PeterScha?uble, and Ross Wilkinson, editors,Proceedings of the 19th AnnualInternational ACM SIGIR Conference onResearch and Development inInformation Retrieval (SIGIR ?96).
ACMPress, New York, pages 40?48.Kraaij, Wessel, Rene?e Pohlmann, and DjoerdHiemstra.
2000.
Twenty-one at TREC-8:Using language technology forinformation retrieval.
In Ellen M.Voorhees and Donna K. Harman, editors,The Eighth Text Retrieval Conference(TREC-8), volume 8.
National Institute ofStandards and Technology SpecialPublication 500-246, Gaithersburg, MD.Kraaij, Wessel and Martijn Spitters.
2003.Language models for topic tracking.
InBruce Croft and John Lafferty, editors,Language Models for Information Retrieval.Kluwer Academic, Boston.Kraaij, Wessel, Thijs Westerveld, and DjoerdHiemstra.
2002.
The importance of priorprobabilities for entry page search.
InMicheline Beaulieu, Ricardo Baeza-Yates,Sung Hyon Myaeng, and KalervoJa?rvelin, editors, Proceedings of the 25thAnnual International ACM SIGIR Conferenceon Research and Development in InformationRetrieval (SIGIR 2002).
ACM Press, NewYork.Kwok, K. L. 1999.
English-Chinesecross-language retrieval based on atranslation package.
In Workshop: MachineTranslation for Cross Language InformationRetrieval, Singapore, Machine TranslationSummit VII, pages 8?13.Lafferty, John and Chengxiang Zhai.
2001a.Document language models, querymodels, and risk minimization forinformation retrieval.
In W. Bruce Croft,David J. Harper, Donald H. Kraft, andJustin Zobel, editors, Proceedings of the 24thAnnual International ACM SIGIR Conferenceon Research and Development in InformationRetrieval (SIGIR 2001).
ACM Press, NewYork.Lafferty, John and Chengxiang Zhai.
2001b.Probabilistic IR models based ondocument and query generation.
In JamieCallan, Bruce Croft, and John Lafferty,editors, Proceedings of the Workshop onLanguage Modeling and InformationRetrieval, Pittsburgh.Laffling, John.
1992.
On constructing atransfer dictionary for man and machine.Target, 4(1):17?31.Lavrenko, Victor, Martin Choquette, and W.Bruce Croft.
2002.
Cross-lingual relevancemodels.
In Micheline Beaulieu, RicardoBaeza-Yates, Sung Hyon Myaeng, andKalervo Ja?rvelin, editors, Proceedings of the25th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval (SIGIR 2002).
ACMPress, New York.Lavrenko, Victor and W. Bruce Croft.
2001.Relevance-based language models.
In W.Bruce Croft, David J. Harper, Donald H.Kraft, and Justin Zobel, editors,Proceedings of the 24th Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR2001).
ACM Press, New York.Lehtokangas, Raija and Eija Airio.
2002.Translation via a pivot languagechallenges direct translation in CLIR.
In418Computational Linguistics Volume 29, Number 3Proceedings of the SIGIR 2002 Workshop:Cross-Language Information Retrieval: AResearch Roadmap, Tampere, Finland.Ma, Xiaoyi.
1999.
Parallel text collections atthe Linguistic Data Consortium.
InMachine Translation Summit VII, Singapore.McNamee, Paul and James Mayfield.
2001.A language-independent approach toEuropean text retrieval.
In Carol Peters,editor, Cross-Language Information Retrievaland Evaluation.
Lecture Notes in ComputerScience 2069.
Springer Verlag, Berlin.McNamee, Paul and James Mayfield.
2002.Comparing cross-language queryexpansion techniques by degradingtranslation reources.
In MichelineBeaulieu, Ricardo Baeza-Yates, Sung HyonMyaeng, and Kalervo Ja?rvelin, editors,Proceedings of the 25th Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR2002).
ACM Press, New York.Miller, David R. H., Tim Leek, andRichard M. Schwartz.
1999.
A hiddenMarkov model information retrievalsystem.
In Marti Hearst, Fred Gey, andRichard Tong, editors, Proceedings of the22nd Annual International ACM SIGIRConference on Research and Developmentin Information Retrieval (SIGIR ?99).ACM Press, New York, pages 214?221.Ng, Kenney.
2000.
A maximum likelihoodratio information retrieval model.
In EllenM.
Voorhees and Donna K. Harman,editors, The Eighth Text Retrieval Conference(TREC-8), volume 8.
National Institute ofStandards and Technology SpecialPublication 500-246, Gaithersburg, MD.Nie, Jian-Yun.
2002.
Query expansion andquery translation as logical inference.Journal of the American Society forInformation Science and Technology, 54(4):340?351.Nie, Jian-Yun and Jian Cai.
2001.
Filteringnoisy parallel corpora of Web pages.
InIEEE Symposium on NLP and KnowledgeEngineering, Tucson, AZ, pages 453?458.Nie, Jian-Yun and Jean-Franc?ois Dufort.2002.
Combining words and compoundterms for monolingual and cross-languageinformation retrieval.
In Proceedings ofInformation 2002, Beijing, pages 453?458.Nie, Jian-Yun, Michel Simard, PierreIsabelle, and Richard Durand.
1999.Cross-language information retrievalbased on parallel texts and automaticmining of parallel texts from the Web.
InMarti Hearst, Fred Gey, and RichardTong, editors, Proceedings of the 22ndAnnual International ACM SIGIRConference on Research and Developmentin Information Retrieval (SIGIR ?99).ACM Press, New York, pages 74?81.Pirkola, Ari.
1998.
The effects of querystructure and dictionary setups indictionary-based cross-languageinformation retrieval.
In W. Bruce Croft,Alistair Moffat, C. J.
?Keith?
vanRijsbergen, Ross Wilkinson, and JustinZobel, editors, Proceedings of the 21stAnnual International ACM SIGIR Conferenceon Research and Development in InformationRetrieval (SIGIR ?98).
ACM Press, NewYork, pages 55?63.Ponte, Jay M. and W. Bruce Croft.
1998.
Alanguage modeling approach toinformation retrieval.
In W. Bruce Croft,Alistair Moffat, C. J.
?Keith?
vanRijsbergen, Ross Wilkinson, and JustinZobel, editors, Proceedings of the 21stAnnual International ACM SIGIR Conferenceon Research and Development in InformationRetrieval (SIGIR ?98).
ACM Press, NewYork, pages 275?281.Porter, Martin F. 1980.
An algorithm forsuffix stripping.
Program, 14(3):130?137.Ragget, Dave.
1998.
Clean up your Webpages with HTML TIDY.
Available at?http://www.w3.org/People/Raggett/tidy/?.Resnik, Philip.
1998.
Parallel stands: Apreliminary investigation into mining theWeb for bilingual text.
In Proceedings ofAMTA.
Lecture Notes in ComputerScience 1529.
Springer, Berlin.Robertson, Stephen E. and Steve Walker.1994.
Some simple effectiveapproximations to the 2-Poisson modelfor probabilistic weighted retrieval.
In W.Bruce Croft and C. J.
?Keith?
vanRijsbergen, editors, Proceedings of the 17thAnnual International ACM SIGIR Conferenceon Research and Development in InformationRetrieval (SIGIR ?94), pages 232?241.
ACMPress, New York.Salton, G. and M. J. McGill.
1983.Introduction to Modern Information Retrieval.McGraw-Hill, New York.Savoy, Jacques.
2002.
Report on CLEF-2001experiments.
In Carol Peters, MartinBraschler, Julio Gonzalo, and MichaelKluck, editors, Evaluation of Cross-LanguageInformation Retrieval Systems: SecondWorkshop of the Cross-Language EvaluationForum (CLEF 2001).
Springer Verlag,Berlin.Sheridan, Paraic, Jean Paul Ballerini, andPeter Scha?uble.
1998.
Building a largemultilingual text collection fromcomparable news documents.
In GregoryGrefenstette, editor, Cross-LanguageInformation Retrieval.
Kluwer Academic,419Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIRpages 137?150.Simard, Michel, George Foster, and PierreIsabelle.
1992.
Using Cognates to AlignSentences in Bilingual Corpora.
InProceedings of the Fourth Conference onTheoretical and Methodological Issues inMachine Translation (TMI), pages 67?82,Montre?al, Que?bec.Spitters, Martijn and Wessel Kraaij.
2001.Using language models for trackingevents of interest over time.
In Proceedingsof the Workshop on Language Models forInformation Retrieval (LMIR2001),Pittsburgh.Tague-Sutcliffe, Jean and James Blustein.1995.
A statistical analysis of the TREC-3data.
In Donna K. Harman, editor, TheThird Text Retrieval Conference, volume 4.National Institute of Standards andTechnology Special Publication 500-236,Gaithersburg, MD, pages 385?398.Ve?ronis, Jean, editor.
2000.
Parallel TextProcessing.
Kluwer Academic, Dordrecht,the Netherlands.Voorhees, Ellen M. 1998.
Variations inrelevance judgements and themeasurement of retrieval effectiveness.
InW.
Bruce Croft, Alistair Moffat, C.
J.?Keith?
van Rijsbergen, Ross Wilkinson,and Justin Zobel, editors, Proceedings of the21st Annual International ACM SIGIRConference on Research and Development inInformation Retrieval (SIGIR ?98).
ACMPress, New York.
pages 315?323.Xu, Jinxi, Ralph Weischedel, and ChanhNguyen.
2001.
Evaluating a probabilisticmodel for cross-lingual informationretrieval.
In W. Bruce Croft, David J.Harper, Donald H. Kraft, and JustinZobel, editors, Proceedings of the 24thAnnual International ACM SIGIR Conferenceon Research and Development in InformationRetrieval (SIGIR 2001).
ACM Press, NewYork.Yang, Yiming, Jaime G. Carbonell, RalphBrown, and Robert E. Frederking.
1998.Translingual information retrieval:Learning from bilingual corpora.
ArtificialIntelligence Journal, 103(1?2):323?345.Zhai, ChengXiang and John Lafferty.
2002.Two-stage language models forinformation retrieval.
In MichelineBeaulieu, Ricardo Baeza-Yates, Sung HyonMyaeng, and Kalervo Ja?rvelin, editors,Proceedings of the 25th Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR2002).
ACM Press, New York.
