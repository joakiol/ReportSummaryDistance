Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1600?1610,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsExploiting Parse Structures for Native Language IdentificationSze-Meng Jojo WongCentre for Language TechnologyMacquarie UniversitySydney, Australiasze.wong@mq.edu.auMark DrasCentre for Language TechnologyMacquarie UniversitySydney, Australiamark.dras@mq.edu.auAbstractAttempts to profile authors according to theircharacteristics extracted from textual data, in-cluding native language, have drawn attentionin recent years, via various machine learn-ing approaches utilising mostly lexical fea-tures.
Drawing on the idea of contrastiveanalysis, which postulates that syntactic er-rors in a text are to some extent influenced bythe native language of an author, this paperexplores the usefulness of syntactic featuresfor native language identification.
We taketwo types of parse substructure as features?horizontal slices of trees, and the more gen-eral feature schemas from discriminative parsereranking?and show that using this kind ofsyntactic feature results in an accuracy scorein classification of seven native languages ofaround 80%, an error reduction of more than30%.1 IntroductionInferring characteristics of authors from their tex-tual data, often termed authorship profiling, has seena number of computational approaches proposed inrecent years.
The problem is typically treated as aclassification task, where an author is classified withrespect to characteristics such as gender, age, nativelanguage, and so on.
This profile information is of-ten of interest to marketing organisations for prod-uct promotional reasons as well as governments orlaw enforcements for crime investigation purposes.The particular application that motivates the presentstudy is detection of phishing (Myers, 2007), the at-tempt to defraud through texts that are designed todeceive Internet users into giving away confidentialdetails.
One class of countermeasures to phishingconsists of technical methods such as email authen-tication; another looks at profiling of the text?s au-thor(s) (Fette et al, 2007; Zheng et al, 2003), tofind any indications of the source of the text.In this paper we investigate classification of a textwith respect to an author?s native language, wherethis is not the language that that text is written in(which is often the case in phishing); we refer tothis as native language identification.
Initial workby Koppel et al (2005) was followed by Tsur andRappoport (2007), Estival et al (2007), van Halteren(2008), and Wong and Dras (2009).
By and large,the problem was tackled using various supervisedmachine learning approaches, with mostly lexicalfeatures over characters, words, and parts of speech,as well as some document structure.Syntactic features, in contrast, in particular thosethat capture grammatical errors, which might po-tentially be useful for this task, have received lit-tle attention.
Koppel et al (2005) did suggest usingsyntactic errors in their work but did not investigatethem in any detail.
Wong and Dras (2009) notedthe relevance of the concept of contrastive analy-sis (Lado, 1957), which postulates that native lan-guage constructions lead to characteristic errors in asecond language.
In their experimental work, how-ever, they used only three manual syntactic construc-tions drawn from the literature; an ANOVA analysisshowed a detectable effect, but they did not improveclassification accuracy over purely lexical features.In this paper, we investigate syntactic features fornative language identification that are more general1600than, and that do not require the manual constructionof, the above approach.
Taking the trees producedby statistical parsers, we use tree cross-sections asfeatures in a machine learning approach to deter-mine which ones characterise non-native speaker er-rors.
Specifically, we look at two types of parsetree substructure to use as features: horizontal slicesof the trees?that is, characterising parse trees assets of context-free grammar production rules?andthe features schemas used in discriminative parsereranking.
The goal of the present study is thereforeto investigate the influence to which syntactic fea-tures represented by parse structures would have onthe classification task of identifying an author?s na-tive language relative to, and in combination with,lexical features.The remainder of this paper is structured as fol-lows.
In Section 2, we discuss some related work onthe two key topics of this paper: primarily on com-parable work in native language identification, andthen on how the notion of contrastive analysis can beapplicable here.
We then describe the models exam-ined in Section 3, followed by experimental setup inSection 4.
Section 5 presents results, and Section 6discussion of those results.2 Related Work2.1 Native Language IdentificationThe earliest work on native language identificationin this classification paradigm is that of Koppel etal.
(2005), in which they deployed a machine learn-ing approach to the task, using as features func-tion words, character n-grams, and part-of-speech(PoS) bi-grams, as well as some spelling mistakes.With five different groups of English authors (of na-tive languages Bulgarian, Czech, French, Russian,and Spanish) selected from the first version of In-ternational Corpus of Learner English (ICLE), theygained a relatively high classification accuracy of80%.
Koppel et al (2005) also suggested that syn-tactic features (syntactic errors) might be useful fea-tures, but only investigated this idea at a shallowlevel by treating rare PoS bigrams as ungrammati-cal structures.Tsur and Rappoport (2007) replicated the workof Koppel et al (2005) to investigate the hypothe-sis that the choice of words in second language writ-ing is highly influenced by the frequency of nativelanguage syllables ?
the phonology of the nativelanguage.
Approximating this by character bi-gramsalone, they managed to achieve a classification accu-racy of 66%.Native language is also amongst the characteris-tics investigated in the task of authorship profilingby Estival et al (2007), as well as other demographicand personality characteristics.
This study used a va-riety of lexical and document structure features.
Forthe native language identification classification task,their model yielded a reasonably high accuracy of84%, but this was over a set of only three languages(Arabic, English and Spanish) and against a mostfrequent baseline of 62.9%.Another related work is that of van Halteren(2008), who used the Europarl corpus of parliamen-tary speeches.
In Europarl, one original languageis transcribed, and the others translated from it; thetask was to identify the original language.
On thebasis of frequency counts of word-based n-grams,surprisingly high classification accuracies within therange of 87-97% were achieved across six languages(English, German, French, Dutch, Spanish, and Ital-ian).
This turns out, however, to be significantlyinfluenced by the use of particular phrases used byspeakers of different languages in the parliamentarycontext (e.g.
the way Germans typically address thechamber).To our knowledge, Wong and Dras (2009) is theonly work that has investigated the usefulness ofsyntactic features for the task of native languageidentification.
They first replicated the work ofKoppel et al (2005) with the three types of lex-ical feature, namely function words, character n-grams, and PoS bi-grams.
They then examined theliterature on contrastive analysis (see Section 2.2),from the field of second language acquisition, andselected three syntactic errors commonly observedin non-native English users?subject-verb disagree-ment, noun-number disagreement and misuse ofdeterminers?that had been identified as being in-fluenced by the native language.
An ANOVA anal-ysis showed that the native language identificationconstructions were identifiable; however, the over-all classification was not improved over the lexi-cal features by using just the three manually de-tected syntactic errors.
The best overall accuracy re-1601ported was 73.71%; this was on the second versionof ICLE, across seven languages (those of Koppelet al (2005), plus the two Asian languages Chineseand Japanese).As a possible approach that would improve theclassification accuracy over just the three manuallydetected syntactic errors, Wong and Dras (2009)suggested deploying (but did not carry out) an ideaput forward by Gamon (2004) (citing Baayen et al(1996)) for the related task of identifying the authorof a text: to use CFG production rules to characterisesyntactic structures used by authors.1 We note thatsimilar ideas have been used in the task of sentencegrammaticality judgement, which utilise parser out-puts (both trees and by-products) as classificationfeatures (Mutton et al, 2007; Sun et al, 2007; Fos-ter et al, 2008; Wagner et al, 2009; Tetreault et al,2010; Wong and Dras, 2010).
We combine this ideawith one we introduce in this paper, of using dis-criminative reranking features as a broader charac-terisation of the parse tree.2.2 Contrastive analysisContrastive analysis (Lado, 1957) was an early at-tempt in the field of second language acquisitionto explain the kinds and source of errors that non-native speakers make.
It arose out of behaviouristpsychology, and saw language learning as an issueof habit formation that could be inhibited by previ-ous habits inculcated in learning the native language.The theory was also tied to structural linguistics:it compared the syntactic structures of the nativeand second languages to find differences that mightcause learning difficulties.
The Lado work postu-lated the Contrastive Analysis Hypothesis (CAH),claiming that ?those elements which are similar to[the learner?s] native language will be simple forhim, and those elements that are different will bedifficult?
; the consequence is that there will be moreerrors made in those difficult elements.While contrastive analysis was influential at first,it was increasingly noticed that many errors were1It is not entirely clear how this might work for author-ship identification: would the Bronte?
sisters, the corpus Gamonworked with, have used a significant number of different syntac-tic constructions from each other?
In the context of native lan-guage identification, however, constrastive analysis postulatesthat this is exactly the case for the different classes.common across all language learners regardless ofnative language, which could not be explained un-der contrastive analysis.
Corder (1967) then de-scribed an alternative, error analysis, where con-trastive analysis-style errors were seen as only onetype of error, ?interlanguage?
or ?interference?
er-rors; other types were ?intralingual?
and ?develop-mental?
errors, which are not specific to the nativelanguage (Richards, 1971).In an overview of contrastive analysis after theemergence of error analysis, Wardhaugh (1970)noted that there were two interpretations of theCAH, termed the strong and weak forms.
Under thestrong form, all errors were attributed to the nativelanguage, and clearly that was not tenable in light oferror analysis evidence.
In the weak form, these dif-ferences have an influence but are not the sole deter-minant of language learning difficulty.
Wardhaughnoted claims at the time that the hypothesis was nolonger useful in either the strong or the weak ver-sion: ?Such a claim is perhaps unwarranted, but aperiod of quiescence is probable for CA itself?.
Thisappears to be the case, with the then-dominant erroranalysis giving way to newer, more specialised theo-ries of second language acquisition, such as the com-petition model of MacWhinney and Bates (1989)or the processability theory of Pienemann (1998).Nevertheless, smaller studies specifically of inter-language errors have continued to be carried out,generally restricted in their scope to a specific gram-matical aspect of English in which the native lan-guage of the learners might have an influence.
Togive some examples, Granger and Tyson (1996) ex-amined the usage of connectors in English by a num-ber of different native speakers ?
French, German,Dutch, and Chinese; Vassileva (1998) investigatedthe employment of first person singular and pluralby another different set of native speakers ?
Ger-man, French, Russian, and Bulgarian; Slabakova(2000) explored the acquisition of telicity markingin English by Spanish and Bulgarian learners; Yangand Huang (2004) studied the impact of the ab-sence of grammatical tense in Chinese on the acqui-sition of English tense-aspect system (i.e.
telicitymarking); Franck et al (2002) and Vigliocco et al(1996) specifically examined the usage of subject-verb agreement in English by French and Spanish,respectively.
There are also a few teaching resources1602for English language teachers that collate such phe-nomena, such as that of Swan and Smith (2001).NLP techniques and a probabilistic view of na-tive language identification now let us revisit andmake use of the weak form of the CAH.
Interlan-guage errors, as represented by differences in parsetrees, may be characteristic of the native languageof a learner; we can use the occurrence of these tocome up with a revised likelihood of the native lan-guage.
In this paper, we use machine learning in aprediction task as our approach to this.3 ModelsThis section describes the three basic models inves-tigated: the lexical model, based on Koppel et al(2005), as the baseline; and then the two models thatexploit syntactic information.
In Section 5 we lookat the performance of each model independently andalso in combination: to combine, we just concate-nate feature vectors.Lexical As Wong and Dras (2009), we replicatethe features of Koppel et al (2005) to produce ourLEXICAL model.
These are of three types: functionwords,2 character n-grams, and PoS n-grams.
Wefollow Wong and Dras (2009) in resolving some un-clear issues from Koppel et al (2005).
Specifically,we use the same list of function words, left unspec-ified in Koppel et al (2005), that were empiricallydetermined by Wong and Dras (2009) to be the bestof three candidates; we used character bi-grams, asthe best performing n-grams, although this also hadbeen left unspecified by Koppel et al (2005); andwe used the most frequently occurring PoS bi-gramsand tri-grams, obtained by using the Brill tagger pro-vided in NLTK (Bird et al, 2009) being trained onthe Brown corpus.
In total, there are 798 featuresof this class with 398 function words, 200 most fre-quently occurring character bi-grams, and 200 mostfrequently occurring PoS bi-grams.
Both functionwords and PoS bi-grams have feature values of bi-nary type; while for character bi-grams, the featurevalue is the relative frequency.
(These types of fea-ture value are the best performing one for each lexi-2As with most work in authorship profiling, only functionwords are used, so that the result is not tied to a particular do-main, and no clues are obtained from different topics that dif-ferent authors might write about.cal feature.
)We omitted the 250 rare bi-grams used by Koppelet al (2005), as an ablative analysis showed that theycontributed nothing to classification accuracy.Production Rules Under this model (PROD-RULE), we take as features horizontal slices of parsetrees, in effect treating them as sets of CFG produc-tion rules.
Feature values are binary.
We look atall possible rules as features, but also present resultsfor subsets of features chosen using feature selec-tion.
For each language in our dataset, we identifythe n rules most characteristic of the language usingInformation Gain (IG).
For m classes, we use theformulation of Yang and Pedersen (1997):IG(r) = ?
?mi=1 Pr (ci) log Pr (ci)+Pr (r)?mi=1 Pr (ci|r) log Pr (ci|r)+Pr (r?
)?mi=1 Pr (ci|r?)
log Pr (ci|r?)
(1)We also investigated simple frequencies, fre-quency ratios, and pointwise mutual information; asin much other work, IG performed best, so we do notpresent results for the others.
Bi-normal separation(Forman, 2003), often competitive with IG, is onlysuitable for binary classification.It is worth noting that the production rules beingused here are all non-lexicalised ones, except thoselexicalised with function words and punctuation, toavoid topic-related clues.Reranking Features As opposed to the horizontalparse production rules, features used for discrimina-tive reranking are cross-sections of parse trees thatmight capture other aspects of ungrammatical struc-tures.
For these we use the 13 feature schemas de-scribed in Charniak and Johnson (2005), which wereinspired by earlier work in discriminative estimationtechniques, such as Johnson et al (1999) and Collins(2000).
Examples of these feature schemas includetuples covering head-to-head dependencies, preter-minals together with their closest maximal projec-tion ancestors, and subtrees rooted in the least com-mon ancestor.These feature schemas are not the only possibleones?they were empirically selected for the spe-cific purpose of augmenting the Charniak parser.However, much subsequent work has tended to use1603these same features, albeit sometimes with exten-sions for specific purposes (e.g.
Johnson and Ural(2010) for the Berkeley parser (Petrov et al, 2006),Ng et al (2010) for the C&C parser (Clark and Cur-ran, 2007)).
We also use this standard set, specif-ically the set of instantiated feature schemas fromthe parser from Charniak and Johnson (2005) astrained on the Wall Street Journal (WSJ), whichgives 1,333,837 potential features.4 Experimental Setup4.1 DataWe use the International Corpus of Learner English(ICLE) compiled by Granger et al (2009) for theprecise purpose of studying the English writings ofnon-native English learners from diverse countries.All the contributors to the corpus are claimed topossess similar English proficiency levels (rangingfrom intermediate to advanced learners) and are inthe same age group (all in their twenties at the timeof corpus collection.)
This was also the data used byKoppel et al (2005) and Tsur and Rappoport (2007),although where they used the first version of the cor-pus, we use version 2.Briefly, the first version contains 11 sub-corporaof English essays contributed by second-year andthird-year university students of different native lan-guage backgrounds (mostly European and Slaviclanguages) ?
Bulgarian, Czech, Dutch, Finnish,French, German, Italian, Polish, Russian, Spanish,and Swedish; the second version has been extendedto additional 5 other native languages (includingAsian languages) ?
Chinese, Japanese, Norwegian,Turkish, and Tswana.As per Wong and Dras (2009), we examine sevenlanguages, namely Bulgarian, Czech, French, Rus-sian, Spanish, Chinese, and Japanese.
For each na-tive language, we randomly select from amongst es-says with length of 500-1000 words.
For the purposeof the present study, we have 95 essays per nativelanguage.
For the same reason as highlighted byWong and Dras (2009), we intentionally use feweressays as compared to Koppel et al (2005)3 with aview to reserving more data for future work.
Wedivide these into training sets of 70 essays per lan-3Koppel et al (2005) took all 258 texts per language fromICLE Version 1 and evaluated using 10-fold cross valiadation.guage, with a held-out test set of 25 essays perlanguage.
There are 17,718 training sentences and6,791 testing sentences.4.2 ParsersWe use two parsers: the Stanford parser (Kleinand Manning, 2003) and the Charniak and John-son (henceforth C&J) parser (Charniak and Johnson,2005).
Both are widely used, and produce relativelyaccurate parses: the Stanford parser gets a labelledf-score of 85.61 on the WSJ, and the C&J 91.09.With the Stanford parser, there are 26,284 uniqueparse production rules extractable from our ICLEtraining set of 490 texts, while the C&J parser pro-duces 27,705.
For reranking, we use only the C&Jparser?since the parser stores these features duringparsing, we can use them directly as classificationfeatures.
On the ICLE training data, there are 6,230features with frequency >10, and 19,659 with fre-quency >5.4.3 ClassifiersFor our experiments we used a maximum entropy(MaxEnt) machine learner, MegaM4 (fifth release)by Hal Daume?
III.
(We also used an SVM for com-parison, but the results were uniformly worse, anddegraded more quickly as number of features in-creased, so we only report the MaxEnt results here).The classifier is tuned to obtain an optimal classifi-cation model.4.4 Evaluation MethodologyGiven our relatively small amount of data, we use k-fold cross-validation, choosing k = 5.
While testingfor statistical significance of classification results isoften not carried out in NLP, we do so here becausethe quantity of data could raise questions about thecertainty of any effect.
In an encyclopedic survey ofcross-validation in machine learning contexts, Re-faeilzadeh et al (2009) note that there is as yet nouniversal standard for testing of statistical signifi-cance; and that while more sophisticated techniqueshave been proposed, none is more widely acceptedthan a paired t-test over folds.
We therefore use thispaired t-test over folds, as formulated of Alpaydin4MegaM is available on http://www.cs.utah.edu/?hal/megam/.1604(2004).
Under this cross-validation, 5 separate train-ing feature sets are constructed, excluding the testfold; 3 folds are used for training, 1 fold for tuningand 1 fold for testing.We also use a held-out test set for comparison,as it is well-known that cross-validation can over-estimate prediction error (Hastie et al, 2009).
Wedo not carry out significance testing here?with thisheld-out test set size (n = 125), two models wouldhave to differ by a great deal to be significant.
Weonly use it as a check on the effect of applying tocompletely new data.5 ResultsTable 1 presents the results for the three models in-dividually under cross-validation.
The first pointto note is that PROD-RULE, under both parsers,is a substantial improvement over LEXICAL when(non-lexicalised) parse rules together with rules lex-icalised with function words are used (rows markedwith * in Table 1), with the largest difference asmuch as 77.75% for PROD-RULE[both]* (n = all)versus 64.29% for LEXICAL; these differences withrespect to LEXICAL are statistically significant.
(Togive an idea, the paired t-test standard error for thislargest difference is 2.52%.)
In terms of error reduc-tion, this is over 30%.There appears to be no difference according to theparser used, regardless of their differing accuracy onthe WSJ.
Using the selection metric for PROD-RULEwithout rules lexicalised with function words pro-duces results all around those for LEXICAL; usingfewer reranking features is worse as the quality ofRERANKING declines as feature cut-offs are raised.Another, somewhat surprising point is that theRERANKING results are also generally around thoseof LEXICAL even though like PROD-RULE they arealso using cross-sections of the parse tree.
We con-sider there might be two possible reasons for this.The first is that the feature schemas used were orig-inally chosen for the specific purpose of augment-ing the performance of the Charniak parser; perhapsothers might be more appropriate here.
The secondis that we selected only those instantiated featureschemas that occurred in the WSJ, and then appliedthem to ICLE.
As the WSJ is filled with predomi-nantly grammatical text, perhaps those that were notFeatures MaxEntLEXICAL (n = 798) 64.29PROD-RULE[Stanford] (n = 1000) 65.72PROD-RULE[Stanford]* (n = 1000) 74.08PROD-RULE[Stanford]* (n = all) 74.49PROD-RULE[C&J] (n = 1000) 62.25PROD-RULE[C&J]* (n = 1000) 71.84PROD-RULE[C&J]* (n = all) 71.63PROD-RULE[both] (n = 2000) 67.96PROD-RULE[both]* (n = 2000) 74.69PROD-RULE[both]* (n = all) 77.75RERANKING (all features) 67.96RERANKING (>5 counts) 66.33RERANKING (>10 counts) 64.90Table 1: Classification results based on 5-fold cross vali-dation with parse rules as syntactic features (accuracy %)Features MaxEntLexical features (n = 798) 75.43PROD-RULE[Stanford] (n = 1000) 74.29PROD-RULE[Stanford]* (n = 1000) 79.43PROD-RULE[Stanford]* (n = all) 78.86PROD-RULE[C&J] (n = 1000) 73.71PROD-RULE[C&J] (n = 1000)* 79.43PROD-RULE[C&J] (n = all)* 80.00PROD-RULE[both] (n = 2000) 77.71PROD-RULE[both] (n = 2000)* 78.85PROD-RULE[both] (n = all)* 80.00RERANKING (all features) 77.14RERANKING (>5 counts) 76.57RERANKING (>10 counts) 75.43Table 2: Classification results based on hold-out valida-tion with parse rules as syntactic features (accuracy %)seen on the WSJ are precisely those that might indi-cate ungrammaticality.
In contrast, the productionrules of PROD-RULE were selected only from theICLE training data.Table 2 presents the results for the individualmodels on the held-out test set.
The results are gen-erally higher than for cross-validation?this is notsurprising, as the texts are of the same type, but allthe training data is used (rather than the 1?1/k pro-portion for cross-validation).
Overall, the pattern isstill the same, with PROD-RULE best, then RERANK-ING and LEXICAL broadly similar; as expected, nodifferences are significant with this smaller dataset.The gap has narrowed, but without significance test-1605Features MaxEntLEXICAL (n = 798) 64.29LEXICAL + PROD-RULE[both] (n = 2000) 63.06LEXICAL + PROD-RULE[both]* (n = 2000) 72.45LEXICAL + PROD-RULE[both]* (n = all) 70.82LEXICAL + RERANKING (n = all) 68.17Table 3: Classification results based on 5-fold cross vali-dation for combined models (accuracy %)Features MaxEntLEXICAL (n = 798) 75.43LEXICAL + PROD-RULE[both] (n = 2000) 80.57LEXICAL + PROD-RULE[both]* (n = 2000) 81.14LEXICAL + PROD-RULE[both]* (n = all) 81.71LEXICAL + RERANKING (n = all) 76.00Table 4: Classification results based on hold-out valida-tion for combined models (accuracy %)ing it is difficult to say whether this is a genuinephenomenon.
The accuracy rate for LEXICAL hereis in line with Wong and Dras (2009); and giventhe smaller dataset and larger set of languages, alsobroadly in line with Koppel et al (2005).Tables 3 and 4 present results for model combina-tions.
It can be seen that the model combinations donot produce results better than PROD-RULE alone.Combining all features (results not presented here)seems to degrade the overall performance even ofthe MegaM: perhaps we need to derive feature vec-tors more compactly than by feature concatenation.6 DiscussionAs illustrated in the confusion matrices (Table 5for the PROD-RULE model, and Table 6 for theLEXICAL model), misclassifications occur largely inSpanish and Slavic languages, Bulgarian and Rus-sian in particular.
Unsurprisingly, Chinese is al-most completely identified since it comes from aentirely different language family, Sino-Tibetan, ascompared to the rest of the languages which are fromthe branches of the Indo-European family (withJapanese as the exception).
Japanese and Frenchalso appear to be easily distinguished, which couldprobably be attributed to their word order or sen-tence structure which are, to some extent, quite dif-ferent from English.
Japanese is a ?subject-object-verb?
language; and French, although having thesame word order as English, heads of phrases inBL CZ FR RU SP CN JPBL [14] 6 2 3 - - -CZ 1 [20] - 3 1 - -FR - - [25] - - - -RU 1 4 3 [17] - - -SP 2 1 3 1 [18] - -CN - - - - - [24] 1JP - - - - 1 2 [22]Table 5: Confusion matrix based on all non-lexicalisedparse rules from both parsers on the held-out set(BL:Bulgarian, CZ:Czech, FR:French, RU:Russian,SP:Spanish, CN:Chinese, JP:Japanese)BL CZ FR RU SP CN JPBL [14] 3 2 4 2 - -CZ 6 [16] - 2 1 - -FR 1 - [24] - - - -RU 3 2 3 [16] 1 - -SP 1 2 3 1 [17] - 1CN - - - - - [24] 1JP - - - - 1 3 [21]Table 6: Confusion matrix based on lexical features onthe held-out set (BL:Bulgarian, CZ:Czech, FR:French,RU:Russian, SP:Spanish, CN:Chinese, JP:Japanese)French typically come before modifiers as opposedto English.
Overall, the PROD-RULE model resultsin fewer misclassifications compared to the LEXI-CAL model; there are mostly only incremental im-provements for each language, with perhaps the ex-ception of the reduction in confusion in the Slaviclanguages.We looked at some of the data, to see what kindof syntactic substructure is useful in classifying na-tive language.
Although using feature selection withonly 1000 features did not improve performance,the information gain ranking does identify particu-lar constructions as characteristic of one of the lan-guages, and so are useful for inspection.A phenomenon that the literature has noted as oc-curring with Chinese speakers is that of the missingdeterminer.5 This corresponds to a higher frequencyof NP rules without determiners.
These rules maybe valid in other contexts, but are also used to de-scribe ungrammatical constituents.
One example is5This does happen with native speakers of some other lan-guages, such as Slavic ones, but not generally (from our knowl-edge of the literature) with native speakers of others, such asRomance ones.1606Rules CountsBL CZ FR RU SP CN JPNNP ?
<R> 0 0 3 0 0 67 0: ?
- 55 51 23 39 10 9 4PRN ?
-LRB- X -RRB- 0 1 7 2 0 42 0SYM ?
* 0 1 7 3 1 42 0: ?
: 30 39 58 46 47 11 6X ?
SYM 0 2 7 4 4 42 6NP ?
NNP NNP NNS 0 3 1 0 0 31 0S ?
S : S .
36 34 53 39 41 5 9PP ?
VBG PP 9 15 16 12 13 54 13: ?
... 16 13 39 11 24 1 3Table 7: Top 10 rules for the Stanford parser according to Information Gain on the held-out set(ROOT(S(NP(NP (DT The) (NN development))(PP (IN of)(NP (NN country) (NN park))))(VP (MD can)(ADVP (RB directly))(VP (VB elp)(S(VP (TO to)(VP (VB alleviate)(NP (NNS overcrowdedness)(CC and)(NN overpopulation))(PP (IN in)(NP (JJ urban)(NN area))))))))(.
.
)))Figure 1: Parse from Chinese-speaking authors, illustrat-ing missing determiner(ROOT(S(PP (VBG According)(PP (TO to)(NP (NNP <R>))))(, ,)(NP(NP (NN burning))(PP (IN of)(NP (JJ plastic)(NN waste))))(VP (VBZ generates)(NP (JJ toxic)(NNS by-products)))(.
.
)))Figure 2: Parse from Chinese-speaking authors, illustrat-ing according toNP ?
NN NN.
In Figure 1 we give the parse (fromthe Stanford parser) of the sentence The develop-ment of country park can directly elp to alleviateovercrowdedness and overpopulation in urban area.The phrase country park should either have a deter-miner or be plural (in which case the appropriate rulewould be NP ?
NN NNS).
There is a similar phe-nomenon with in urban area, although this is an in-stance of the rule NP ?
JJ NN.Another production rule that occurs typically?in fact, almost exclusively?in the texts of nativeChinese speakers is PP ?
VBG PP (by the Stan-ford parser), which almost always corresponds to thephrase according to.
In Figure 2 we give the parseof a short sentence (According to <R>, burning of1607(S1(S(ADVP (RB Overall))(, ,)(NP (NNP cyber))(VP (VBD cafeis)(NP (DT a) (JJ good) (NN place))(PP (IN as)(NP (JJ recreational)(NNP centre)))(PP (IN with)(NP(NP(DT a) (NN bundle))(PP (IN of)(NP (JJ up-to-dated)(NN information))))))(.
.
)))Figure 3: Parse illustrating parser correctionplastic waste generates toxic by-products?<R>isan in-text citation that was removed in the prepa-ration of ICLE) that illustrates this particular con-struction.
It appears that speakers of Chinese fre-quently use this phrase as a translation of ge?n ju`.So in this case, what is identified is not the sort oferror that is of interest to contrastive analysis, butjust a particular construction that is characteristic ofa certain native speaker?s language, one that is per-fectly grammatical but which is used relatively infre-quently by others and has a slightly unusual analysisby the parser.We had expected to see more rules that displayedobvious ungrammaticality, such as VP ?
DT IN.However, both parsers appear to be good at ?ig-noring?
errors, and producing relatively grammati-cal structures (albeit ones with different frequenciesfor different native languages).
Figure 3 gives theC&J parse for Overall, cyber cafeis a good place asrecreational centre with a bundle of up-to-dated in-formation.
The correction of up-to-dated rather thanup-to-date is straightforward, but the simple typo-graphical error of running together cafe and is leadsto more complex problems for the parser.
Neverthe-less, the parser produces a solid grammatical tree,specifically assigning the category VBD to the com-pound cafeis.
This appears to be because both theStanford and C&J parsers have implicit linguisticconstraints such as assumptions about heads; theseare imposed even when the text does not provide ev-idence for them.We also present in Table 7 the top 10 rules chosenunder the IG feature selection for the Stanford parseron the held-out set.
A number of these, and thoseranked lower, are concerned with punctuation: theseseem unlikely to be related to native language, butperhaps rather to how students of a particular lan-guage background are taught.
Others are more typi-cal of the sorts of example we illustrated above: PP?
VBG PP, for example, is typically connected tothe according to construction discussed in connec-tion with Figure 2, and it can be seen that the dom-inant frequency count there is for native Chinesespeakers (column 6 of the counts).7 ConclusionIn this paper we have shown that, using cross-sections of parse trees, we can improve above an al-ready good baseline in the task of native languageidentification.
While we do not make any strongclaims for the Contrastive Analysis Hypothesis, theusefulness of syntax in the context of this problemdoes provide some support.The best features arising from the classificationhave been horizontal cross-sections of trees, ratherthan the more general discriminative parse rerankingfeatures that might have been expected to perform atleast as well.
This relatively poorer performance bythe reranking features may be due to a number offactors, all of which could be investigated in futurework.
One is the use of feature schema instances thatdid not appear in the largely grammatical WSJ; an-other is the extension of feature schemas; and a thirdis the use of a parser that does not enforce linguisticconstraints such as the Berkeley parser (Petrov et al,2006).Examining some of the substructures showedsome errors that were expected; other constructionsthat were grammatical, but were just characteris-tic translations of constructions that were commonin the native language; and a large number wheregrammatical errors were glossed over by the parser?slinguistic constraints, suggesting another purposefor further work with the Berkeley parser.
Overall,the use of these led to an error reduction in over 30%1608in the cross-validation evaluation with significancetesting.AcknowledgmentsThe authors would like to acknowledge the supportof ARC Linkage Grant LP0776267 and ARC Dis-covery Grant DP1095443, and thank the reviewersfor useful feedback.
Much gratitude is due to MarkJohnson for his guidance on the extraction of rerank-ing features.ReferencesEthem Alpaydin.
2004.
Introduction to Machine Learn-ing.
MIT Press, Cambridge, MA, USA.Harald Baayen, Hans van Halteren, and Fiona Tweedie.1996.
Outside the Cave of Shadows: Using SyntacticAnnotation to Enhance Authorship Attribution.
Liter-ary and Linguistic Computing, 11(3):121?131.Stephen Bird, Ewan Klein, and Edward Loper.
2009.Natural Language Processing with Python: AnalyzingText with the Natural Language Toolkit.
O?Reilly Me-dia, Inc.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting on As-sociation for Computational Linguistics, pages 173?180, Ann Arbor, Michigan.Stephen Clark and James R. Curran.
2007.
Wide-Coverage Efficient Statistical Parsing with CCG andLog-Linear Models.
Computational Linguistics,33(4):493?552.Michael Collins.
2000.
Discriminative reranking for nat-ural language processing.
In Proceedings of the Seven-teenth International Conference on Machine Learning(ICML?00), Stanford, CA.Stephen P. Corder.
1967.
The significance of learners?errors.
International Review of Applied Linguistics inLanguage Teaching (IRAL), 5(4):161?170.Dominique Estival, Tanja Gaustad, Son-Bao Pham, WillRadford, and Ben Hutchinson.
2007.
Author profilingfor English emails.
In Proceedings of the 10th Con-ference of the Pacific Association for ComputationalLinguistics (PACLING), pages 263?272.Ian Fette, Norman Sadeh, and Anthony Tomasic.
2007.Learning to detect phishing emails.
In Proceedings ofthe 16th International World Wide Web Conference.George Forman.
2003.
An extensive empirical study offeature selection metrics for text classification.
Jour-nal of Machine Learning Research, 3:1289?1305.Jennifer Foster, JoachimWagner, and Josef van Genabith.2008.
Adapting a WSJ-trained parser to grammati-cally noisy text.
In Proceedings of ACL-08: HLT,Short Papers, pages 221?224, Columbus, Ohio.Julie Franck, Gabriella Vigliocco, and Janet Nicol.
2002.Subject-verb agreement errors in French and English:The role of syntactic hierarchy.
Language and Cogni-tive Processes, 17(4):371?404.Michael Gamon.
2004.
Linguistic correlates of style:Authorship classification with deep linguistic analy-sis features.
In Proceedings of the 20th InternationalConference on Computational Linguistics (COLING),pages 611?617.Sylviane Granger and Stephanie Tyson.
1996.
Connec-tor usage in the English essay writing of native andnon-native EFL speakers of English.
World Englishes,15(1):17?27.Sylviane Granger, Estelle Dagneaux, Fanny Meunier,and Magali Paquot.
2009. International Corpus ofLearner English (Version 2).
Presses Universitaires deLouvain, Louvian-la-Neuve.Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-man.
2009.
The Elements of Statistical Learning:Data Mining, Inference, and Prediction.
Springer.Mark Johnson and Ahmet Engin Ural.
2010.
Rerank-ing the Berkeley and Brown Parsers.
In Proceed-ings of Human Language Technologies: the 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics(HLT/NAACL-10), pages 665?668, Los Angeles, CA,USA, June.Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,and Stefan Riezler.
1999.
Estimators for stochasticunification-based grammars.
In Proceedings of the37th Annual Meeting of the Association for Compu-tational Linguistics (ACL?99), College Park, MD.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 423?430, Sapporo, Japan.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005.Automatically determining an anonymous author?s na-tive language.
In Intelligence and Security Informat-ics, volume 3495 of Lecture Notes in Computer Sci-ence, pages 209?217.
Springer-Verlag.Robert Lado.
1957.
Linguistics Across Cultures: Ap-plied Linguistics for Language Teachers.
Universityof Michigan Press, Ann Arbor, MI, US.Brian MacWhinney and Elizabeth Bates.
1989.
TheCrosslinguistic Study of Sentence Processing.
Cam-bridge University Press, New York, NY, USA.Andrew Mutton, Mark Dras, Stephen Wan, and RobertDale.
2007.
GLEU: Automatic evaluation of1609sentence-level fluency.
In Proceedings of the 45th An-nual Meeting of the Association of Computational Lin-guistics, pages 344?351, Prague, Czech Republic.Steven Myers.
2007.
Introduction to phishing.
InMarkus Jakobsson and Steven Myers, editors, Phish-ing and Countermeasures: Understanding the In-creasing Problem of Electronic Identity Theft.
JohnWiley & Sons, Inc., Hoboken, NJ, USA.Dominick Ng, Matthew Honnibal, and James R. Cur-ran.
2010.
Reranking a Wide-Coverage CCG Parser.In Proceedings of Australasian Language TechnologyAssociation Workshop (ALTA?10), pages 90?98, Mel-bourne, Australia.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, and Inter-pretable Tree Annotation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics (COLING/ACL?06), pages433?440, Sydney, Australia, July.Manfred Pienemann.
1998.
Language Processing andSecond Language Development: Processability The-ory.
John Benjamins, Amsterdam, The Netherlands.Payam Refaeilzadeh, Lei Tang, and Huan Liu.
2009.Cross-validation.
In Ling Liu and M. Tamer O?zsu, ed-itors, Encyclopedia of Database Systems, pages 532?538.
Springer, US.Jack C. Richards.
1971.
A non-contrastive approach toerror analysis.
ELT Journal, 25(3):204?219.Roumyana Slabakova.
2000.
L1 transfer revisited:the L2 acquisition of telicity marking in English bySpanish and Bulgarian native speakers.
Linguistics,38(4):739?770.Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,Zhongyang Xiong, John Lee, and Chin-Yew Lin.2007.
Detecting erroneous sentences using automat-ically mined sequential patterns.
In Proceedings of the45th Annual Meeting of the Association of Computa-tional Linguistics, pages 81?88, Prague, Czech Repub-lic.Michael Swan and Bernard Smith, editors.
2001.Learner English: A teacher?s guide to interference andother problems.
Cambridge University Press, 2nd edi-tion.Joel Tetreault, Jennifer Foster, and Martin Chodorow.2010.
Using parse features for preposition selectionand error detection.
In Proceedings of the ACL 2010Conference Short Papers, ACLShort ?10, pages 353?358.
Association for Computational Linguistics.Oren Tsur and Ari Rappoport.
2007.
Using classifier fea-tures for studying the effect of native language on thechoice of written second language words.
In Proceed-ings of the Workshop on Cognitive Aspects of Compu-tational Language Acquisition, pages 9?16.Hans van Halteren.
2008.
Source language markers inEUROPARL translations.
In Proceedings of the 22ndInternational Conference on Computational Linguis-tics (COLING), pages 937?944.Irena Vassileva.
1998. Who am I/how are we in aca-demic writing?
A contrastive analysis of authorialpresence in English, German, French, Russian andBulgarian.
International Journal of Applied Linguis-tics, 8(2):163?185.Garbriella Vigliocco, Brian Butterworth, and Merrill F.Garrett.
1996.
Subject-verb agreement in Spanishand English: Differences in the role of conceptual con-straints.
Cognition, 61(3):261?298.JoachimWagner, Jennifer Foster, and Josef van Genabith.2009.
Judging grammaticality: Experiments in sen-tence classification.
CALICO Journal, 26(3):474?490.Richard Wardhaugh.
1970.
The Contrastive AnalysisHypothesis.
TESOL Quarterly, 4(2):123?130.Sze-Meng Jojo Wong and Mark Dras.
2009.
Contrastiveanalysis and native language identification.
In Pro-ceedings of the Australasian Language Technology As-sociation Workshop 2009, pages 53?61, Sydney, Aus-tralia, December.Sze-Meng Jojo Wong and Mark Dras.
2010.
Parserfeatures for sentence grammaticality classification.
InProceedings of the Australasian Language TechnologyAssociation Workshop 2010, pages 67?75, Melbourne,Australia, December.Suying Yang and Yue-Yuan Huang.
2004.
The impact ofthe absence of grammatical tense in L1 on the acqui-sition of the tense-aspect system in L2.
InternationalReview of Applied Linguistics in Language Teaching(IRAL), 42(1):49?70.Yiming Yang and Jan O. Pedersen.
1997.
A compara-tive study on feature selection in text categorization.In Proceedings of the Fourteenth International Con-ference on Machine Learning (ICML?97), pages 412?420.Rong Zheng, Yi Qin, Zan Huang, and Hsinchun Chen.2003.
Authorship analysis in cybercrime investiga-tion.
In Intelligence and Security Informatics, volume2665 of Lecture Notes in Computer Science, pages 59?73.
Springer-Verlag.1610
