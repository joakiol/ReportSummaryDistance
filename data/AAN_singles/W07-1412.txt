Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 72?77,Prague, June 2007. c?2007 Association for Computational LinguisticsShallow Semantics in Fast Textual Entailment Rule LearnersFabio Massimo ZanzottoDISPUniversity of Rome ?Tor Vergata?Roma, Italyzanzotto@info.uniroma2.itMarco PennacchiottiComputerlinguistikUniversita?t des Saarlandes,Saarbru?cken, Germanypennacchiotti@coli.uni-sb.deAlessandro MoschittiDITUniversity of TrentoPovo di Trento, Italymoschitti@dit.unitn.itAbstractIn this paper, we briefly describe twoenhancements of the cross-pair similaritymodel for learning textual entailment rules:1) the typed anchors and 2) a faster compu-tation of the similarity.
We will report andcomment on the preliminary experimentsand on the submission results.1 IntroductionResults of the second RTE challenge (Bar Haim etal., 2006) have suggested that both deep semanticmodels and machine learning approaches can suc-cessfully be applied to solve textual entailment.
Theonly problem seems to be the size of the knowledgebases.
The two best systems (Tatu et al, 2005; Hicklet al, 2005), which are significantly above all theothers (more than +10% accuracy), use implicit orexplicit knowledge bases larger than all the othersystems.
In (Tatu et al, 2005), a deep semanticrepresentation is paired with a large amount of gen-eral and task specific semantic rules (explicit knowl-edge).
In (Hickl et al, 2005), the machine learningmodel is trained over a large amounts of examples(implicit knowledge).In contrast, Zanzotto&Moschitti (2006) proposeda machine-learning based approach which reaches ahigh accuracy by only using the available RTE data.The key idea is the cross-pair similarity, i.e.
a simi-larity applied to two text and hypothesis pairs whichconsiders the relations between the words in the twotexts and between the words in the two hypotheses.This is obtained by using placeholders to link the re-lated words.
Results in (Bar Haim et al, 2006) arecomparable with the best machine learning systemwhen this latter is trained only on the RTE exam-ples.Given the high potential of the cross-pair similar-ity model, for the RTE3 challenge, we built on it byincluding some features of the two best systems: 1)we go towards a deeper semantic representation oflearning pairs including shallow semantic informa-tion in the syntactic trees using typed placeholders;2) we reduce the computational cost of the cross-pairsimilarity computation algorithm to allow the learn-ing over larger training sets.The paper is organized as follows: in Sec.
2 wereview the cross-pair similarity model and its limits;in Sec.
3, we introduce our model for typed anchors;in Sec.
4 we describe how we limit the computa-tional cost of the similarity; in Sec.
5 we present thetwo submission experiments, and in Sec.
6 we drawsome conclusions.2 Cross-pair similarity and its limits2.1 Learning entailment rules with syntacticcross-pair similarityThe cross-pair similarity model (Zanzotto andMoschitti, 2006) proposes a similarity measureaiming at capturing rewrite rules from train-ing examples, computing a cross-pair similarityKS((T ?,H ?
), (T ?
?,H ??)).
The rationale is that if twopairs are similar, it is extremely likely that they havethe same entailment value.
The key point is the useof placeholders to mark the relations between thesentence words.
A placeholder co-indexes two sub-structures in the parse trees of text and hypothesis,72indicating that such substructures are related.
Forexample, the sentence pair, ?All companies file an-nual reports?
implies ?All insurance companies fileannual reports?, is represented as follows:T1 (S (NP: 1 (DT All) (NNS: 1 compa-nies)) (VP: 2 (VBP: 2 file) (NP: 3 (JJ: 3annual) (NNS: 3 reports))))H1 (S (NP: 1 (DT All) (NNP Fortune)(CD 50) (NNS: 1 companies)) (VP: 2(VBP: 2 file) (NP: 3 (JJ: 3 annual)(NNS: 3 reports))))(E1)where the placeholders 1 , 2 , and 3 indicate the rela-tions between the structures of T and of H .Placeholders help to determine if two pairs sharethe same rewriting rule by looking at the subtreesthat they have in common.
For example, supposewe have to determine if ?In autumn, all leaves fall?implies ?In autumn, all maple leaves fall?.
The re-lated co-indexed representation is:T2 (S (PP (IN In) (NP (NN: a automn)))(, ,) (NP: b (DT all) (NNS: b leaves))(VP: c (VBP: c fall)))H2 (S (PP (IN In) (NP: a (NN: a automn)))(, ,) (NP: b (DT all) (NN maple)(NNS: a leaves)) (VP: c (VBP: c fall)))(E2)E1 and E2 share the following subtrees:T3 (S (NP: x (DT all) (NNS: x )) (VP: y(VBP: y )))H3 (S (NP: x (DT all) (NN) (NNS: x ))(VP: x (VBP: x )))(R3)This is the rewrite rule they have in common.
Then,E2 can be likely classified as a valid entailment, asit shares the rule with the valid entailment E1.The cross-pair similarity model uses: (1) a treesimilarity measure KT (?1, ?2) (Collins and Duffy,2002) that counts the subtrees that ?1 and ?2 havein common; (2) a substitution function t(?, c) thatchanges names of the placeholders in a tree accord-ing to a set of correspondences between placehold-ers c. Given C as the collection of all correspon-dences between the placeholders of (T ?,H ?)
and(T ?
?,H ??
), the cross-pair similarity is computed as:KS((T ?, H ?
), (T ?
?, H ??))
=maxc?C(KT (t(T ?, c), t(T ?
?, c)) + KT (t(H ?, c), t(H ?
?, c)))(1)The cross-pair similarity KS , used in a kernel-basedlearning model as the support vector machines, al-lows the exploitation of implicit true and false en-tailment rewrite rules described in the examples.2.2 Limits of the syntactic cross-pair similarityLearning from examples using cross-pair similarityis an attractive and effective approach.
However,the cross-pair strategy, as any machine learning ap-proach, is highly sensitive on how the examples arerepresented in the feature space, as this can stronglybias the performance of the classifier.Consider for example the following text-hypothesis pair, which can lead to an incorrect rule,if misused.T4 ?For my younger readers, Chapmankilled John Lennon more than twentyyears ago.
?H4 ?John Lennon died more than twentyyears ago.?
(E4)In the basic cross-pair similarity model, the learntrule would be the following:T5 (S (NP: x ) (VP: y (VBD: y ) (NP: z )(ADVP: k )))H5 (S (NP: z ) (VP: y (VBD: y )(ADVP: k )))(R5)where the verbs kill and die are connected by the yplaceholder.
This rule is useful to classify exampleslike:T6 ?Cows are vegetarian but, to savemoney on mass-production, farmers fedcows animal extracts.
?H6 ?Cows have eaten animal extracts.?
(E6)but it will clearly fail when used for:T7 ?FDA warns migraine medicine makersthat they are illegally selling migrainemedicines without federal approval.
?H7 ?Migraine medicine makers declaredthat their medicines have been ap-proved.?
(E7)where warn and declare are connected as genericallysimilar verbs.The problem of the basic cross-pair similaritymeasure is that placeholders do not convey thesemantic knowledge needed in cases such as theabove, where the semantic relation between con-nected verbs is essential.2.3 Computational cost of the cross-similaritymeasureLet us go back to the computational cost of KS (eq.1).
It heavily depends on the size of C. We de-fine p?
and p??
as the placeholders of, respectively,(T ?,H ?)
and (T ?
?,H ??).
As C is combinatorial withrespect to |p?| and |p?
?|, |C| rapidly grows.
Assigningplaceholders only to chunks helps controlling their73number.
For example, in the RTE data the numberof placeholders hardly goes beyond 7, as hypothe-ses are generally short sentences.
But, even in thesecases, the number of KT computations grows.
Asthe trees t(?, c) are obtained from a single tree ?
(containing placeholder) applying different c ?
C,it is reasonable to think that they will share com-mon subparts.
Then, during the iterations of c ?C, KT (t(?
?, c), t(??
?, c)) will compute the similaritybetween subtrees that have already been evaluated.The reformulation of the cross-pair similarity func-tion we present takes advantage of this.3 Adding semantic information tocross-pair similarityThe examples in the previous section show thatthe cross-pairs approach lacks the lexical-semanticknowledge connecting the words in a placeholder.In the examples, the missed knowledge is the typeof semantic relation between the main verbs.
Therelation that links kill and die is not a generic sim-ilarity, as a WordNet based similarity measure cansuggest, but a more specific causal relation.
Thelearnt rewrite rule R5 holds only for verbs in suchrelation.
In facts, it is correctly applied in exampleE6, as feed causes eat, but it gives a wrong sugges-tion in example E7, since warn and declare are onlyrelated by a generic similarity relation.We then need to encode this information in thesyntactic trees in order to learn correct rules.3.1 Defining anchor typesThe idea of introducing anchor types should be inprinciple very simple and effective.
Yet, this may benot the case: simpler attempts to introduce semanticinformation in RTE systems have often failed.
Toinvestigate the validity of our idea, we then need tofocus on a small set of relevant relation types, and tocarefully control ambiguity for each type.A valuable source of relation types among wordsis WordNet.
We choose to integrate in our systemthree important relation standing at the word level:part-of, antinomy, and verb entailment.
We also de-fine two more general anchor types: similarity andthe surface matching.
The first type links wordswhich are similar according to some WordNet simi-larity measure.
Specifically, this type is intended toRank Relation Type Symbol1.
antinomy ?2.
part-of ?3.
verb entailment ?4.
similarity ?5.
surface matching =Table 1: Ranked anchor typescapture the semantic relations of synonymy and hy-peronymy.
The second type is activated when wordsor lemmas match: then, it captures cases in whichwords are semantically equivalent.
The complete setof relation types used in the experiments is given inTable 1.3.2 Type anchors in the syntactic treeTo learn more correct rewrite rules by using the an-chor types defined in the previous section, we needto add this information to syntactic trees.
The bestposition would be in the same nodes of the anchors.Also, to be more effective, this information shouldbe inserted in as many subtrees as possible.
Thus wedefine the typed-anchor climbing-up rules.
We thenimplement in our model the following climbing uprule:if two typed anchors climb up to the samenode, give precedence to that with the high-est ranking in Tab.
1.This rule can be easily showed to be consistent withcommon sense intuitions.
For an example like ?Johnis a tall boy?
that does not entail ?John is a shortboy?, our strategy will produce these trees:(E8)T8 H8S ?
3NP = 1NNP = 1JohnVP ?
2AUXisNP ?
3DTaJJ ?
2tallNN = 3boyS ?
3NP = 1NNP = 1JohnVP ?
2AUXisNP ?
3DTaJJ ?
2shortNN = 3boyThis representation can be used to derive a correctrewrite rule, such as:if two fragments have the same syntactic struc-ture S(NP1, V P (AUX,NP2)), and there is anantonym type (?)
on the S and NP2 , then the74c1 = {(a, 1), (b, 2), (c, 3)} c2 = {(a, 1), (b, 2), (d, 3)}?1 t(?1, c1) t(?1, c2)X1 aA2 aB3 aw1aC4 bw2bD5 dD6 cw3cC7 dw4dX1 a:1A2 a:1B3 a:1w1a:1C4 b:2w2b:2D5 dD6 c:3w3c:3C7 dw4dX1 a:1A2 a:1B3 a:1w1a:1C4 b:2w2b:2D5 d:3D6 cw3cC7 d:3w4d:3?2 t(?2, c1) t(?2, c2)X1 1A2 1B3 1m11C4 2m22D5D6 3m33C7m4X1 a:1A2 a:1B3 a:1m1a:1C4 b:2m2b:2D5D6 c:3m3c:3C7m4X1 a:1A2 a:1B3 a:1m1a:1C4 b:2m2b:2D5D6 d:3m3d:3C7m4Figure 1: Tree pairs with placeholders and t(T, c) transformationentailment does not hold.4 Reducing computational cost of thecross-pair similarity computation4.1 The original kernel functionIn this section, we describe more in detail the simi-larity function KS (Eq.
1).
To simplify, we focus onthe computation of only one KT of the kernel sum.KS(??,???)
= maxc?C KT (t(?
?, c), t(??
?, c)), (2)where the (??,???)
pair can be either (T ?, T ??)
or(H ?,H ??).
We apply this simplification since weare interested in optimizing the evaluation of theKT with respect to different sets of correspondencesc ?
C.To better explain KS , we need to analyze the roleof the substitution function t(?, c) and to review thetree kernel function KT .The aim of t(?, c) is to coherently replace place-holders in two trees ??
and ???
so that these two treescan be compared.
The substitution is carried outaccording to the set of correspondences c. Let p?and p??
be placeholders of ??
and ??
?, respectively,if p??
?
p?
then c is a bijection between a subsetp??
?
p?
and p??.
For example (Fig.
1), the trees ?1has p1 ={ a , b , c , d } as placeholder set and ?2 hasp2 ={ 1 , 2 , 3 }.
In this case, a possible set of corre-spondence is c1 = {(a, 1), (b, 2), (c, 3)}.
In Fig.
1the substitution function replaces each placeholdera of the tree ?1with the new placeholder a:1 byt(?, c) obtaining the transformed tree t(?1, c1), andeach placeholder 1 of ?2 with a:1 .
After these sub-stitutions, the labels of the two trees can be matchedand the similarity function KT is applicable.KT (?
?, ?
??
), as defined in (Collins and Duffy,2002), computes the number of common subtreesbetween ?
?
and ?
?
?.4.2 An observation to reduce thecomputational costThe above section has shown that the similarityfunction KS firstly applies the transformation t(?, c)and then computes the tree kernel KT .
The overallprocess can be optimized by factorizing redundantKT computations.Indeed, two trees, t(?, c?)
and t(?, c??
), obtainedby applying two sets of correspondences c?, c??
?
C,may partially overlap since c?
and c??
can share a non-empty set of common elements.
Let us consider thesubtree set S shared by t(?, c?)
and t(?, c??)
suchthat they contain placeholders in c?
?
c??
= c, thent(?, c) = t(?, c?)
= t(?, c??)
??
?
S. Therefore ifwe apply a tree kernel function KT to a pair (??,???
),we can find a c such that subtrees of ??
and subtreesof ???
are invariant with respect to c?
and c??.
There-fore, KT (t(?
?, c), t(??
?, c)) = KT (t(?
?, c?
), t(??
?, c?
))= KT (t(?
?, c??
), t(??
?, c??)).
This implies that it ispossible to refine the dynamic programming algo-rithm used to compute the ?
matrices while com-75puting the kernel KS(??,???
).To better explain this idea let us considerFig.
1 that represents two trees, ?1 and ?2,and the application of two different transforma-tions c1 = {(a, 1), (b, 2), (c, 3)} and c2 ={(a, 1), (b, 2), (d, 3)}.
Nodes are generally in theform Xi z where X is the original node label, z isthe placeholder, and i is used to index nodes of thetree.
Two nodes are equal if they have the same nodelabel and the same placeholder.
The first column ofthe figure represents the original trees ?1 and ?2.The second and third columns contain respectivelythe transformed trees t(?, c1) and t(?, c2)Since the subtree of ?1 starting from A2 a con-tains only placeholders that are in c, in the trans-formed trees, t(?1, c1) and t(?1, c2), the subtreesrooted in A2 a:1 are identical.
The same happensfor ?2 with the subtree rooted in A2 1 .
In the trans-formed trees, t(?2, c1) and t(?2, c2), subtrees rootedin A2 a:1 are identical.
The computation of KTapplied to the above subtrees gives an identical re-sult.
Then, this computation can be avoided.
If cor-rectly used in a dynamic programming algorithm,the above observation can produce an interesting de-crease in the time computational cost.
More de-tails on the algorithm and the decrease in computa-tional cost may be found in (Moschitti and Zanzotto,2007).5 Experimental Results5.1 Experimental SetupWe implemented the novel cross-similarity kernelin the SVM-light-TK (Moschitti, 2006) that en-codes the basic syntactic kernel KT in SVM-light(Joachims, 1999).To assess the validity of the typed anchor model(tap), we evaluated two sets of systems: the plainand lexical-boosted systems.
The plain systems are:-tap: our tree-kernel approach using typed place-holders with climbing in the syntactic tree;-tree: the cross-similarity model described in Sec.2.Its comparison with tap indicates the effectivenessof our approaches;The lexical-boosted systems are:-lex: a standard approach based on lexical over-lap.
The classifier uses as the only feature the lexi-cal overlap similarity score described in (Corley andMihalcea, 2005);-lex+tap: these configurations mix lexical overlapand our typed anchor approaches;-lex+tree: the comparison of this configuration withlex+tap should further support the validity of our in-tuition on typed anchors;Preliminary experiments have been performed us-ing two datasets: RTE2 (the 1600 entailment pairsfrom the RTE-2 challenge) and RTE3d (the devel-opment dataset of this challenge).
We randomlydivided this latter in two halves: RTE3d0 andRTE3d1.5.2 Investigatory Results Analysis andSubmission ResultsTable 2 reports the results of the experiments.
Thefirst column indicates the training set whereas thesecond one specifies the used test set.
The third andthe forth columns represent the accuracy of basicmodels: the original tree model and the enhancedtap model.
The latter three columns report the basiclex model and the two combined models, lex+treeand lex+tap.
The second and the third rows repre-sent the accuracy of the models with respect to thefirst randomly selected half of RTE3d whilst thelast two rows are related to the second half.The experimental results show some interestingfacts.
In the case of the plain systems (tree and tap),we have the following observations:- The use of the typed anchors in the model seemsto be effective.
All the tap model results are higherthan the corresponding tree model results.
This sug-gests that the method used to integrate this kind ofinformation in the syntactic tree is effective.- The claim that using more training material helpsseems not to be supported by these experiments.
Thegap between tree and tap is higher when learn-ing with RTE2 + RTE3d0 than when learningwith RTE30.
This supports the claim.
How-ever, the result is not kept when learning withRTE2 + RTE3d1 with respect to when learningwith RTE31.
This suggests that adding not veryspecific information, i.e.
derived from corpora dif-ferent from the target one (RTE3), may not help thelearning of accurate rules.On the other hand, in the case of the lexical-boosted systems (lex, lex+tree, and lex+tap), wesee that:76Train Test tree tap lex lex+tree lex+tapRTE3d0 RTE3d1 62.97 64.23 69.02 68.26 69.02RTE2 +RTE3d0 RTE3d1 62.22 62.47 71.03 71.28 71.79RTE3d1 RTE3d0 62.03 62.78 70.22 70.22 71.22RTE2 +RTE3d0 RTE3d0 63.77 64.76 71.46 71.22 72.95Table 2: Accuracy of the systems on two folds of RTE3 development- There is an extremely high accuracy result for thepure lex model.
This result is counterintuitive.
Amodel like lex has been likely used by QA or IEsystems to extract examples for the RTE3d set.
Ifthis is the case we may expect that positive andnegative examples should have similar values forthis lex distance indicator.
It is then not clear whythis model results in so high accuracy.- Given the high results of the lex model, the modellex+tree does not increase the performances.- On the contrary, the model lex+tap is always better(or equal) than the lex model.
This suggests thatfor this particular set of examples the typed anchorsare necessary to effectively use the rewriting rulesimplicitly encoded in the examples.- When the tap model is used in combination withthe lex model, it seems that the claim ?the moretraining examples the better?
is valid.
The gapsbetween lex and lex+tap are higher when the RTE2is used in combination with the RTE3d related set.Given this analysis we submitted two systemsboth based on the lex+tap model.
We did two differ-ent training: one using RTE3d and the other usingRTE2 +RTE3d.
Results are reported in the Tablebelow:Train AccuracyRTE3d 66.75%RTE2 +RTE3d 65.75%Such results seem too low to be statistically consis-tent with our development outcome.
This suggeststhat there is a clear difference between the contentof RTE3d and the RTE3 test set.
Moreover, incontrast with what expected, the system trained withonly the RTE3d data is more accurate than the oth-ers.
Again, this suggests that the RTE corpora (fromall the challenges) are most probably very different.6 Conclusions and final remarksThis paper demonstrates that it is possible to ef-fectively include shallow semantics in syntax-basedlearning approaches.
Moreover, as it happened inRTE2, it is not always true that more learning ex-amples increase the accuracy of RTE systems.
Thisclaim is still under investigation.ReferencesRoy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Gi-ampiccolo, Bernardo Magnini, and Idan Szpektor.
2006.The II PASCAL RTE challenge.
In PASCAL ChallengesWorkshop, Venice, Italy.Michael Collins and Nigel Duffy.
2002.
New ranking algo-rithms for parsing and tagging: Kernels over discrete struc-tures, and the voted perceptron.
In Proceedings of ACL02.Courtney Corley and Rada Mihalcea.
2005.
Measuring the se-mantic similarity of texts.
In Proc.
of the ACL Workshopon Empirical Modeling of Semantic Equivalence and Entail-ment, pages 13?18, Ann Arbor, Michigan, June.
Associationfor Computational Linguistics.Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,Bryan Rink, and Ying Shi.
2005.
Recognizing textual en-tailment with LCCs GROUNDHOG system.
In Proceedingsof the Second PASCAL Challenges Workshop on Recognis-ing Textual Entailment, Venice, Italy.Thorsten Joachims.
1999.
Making large-scale svm learningpractical.
In B. Schlkopf, C. Burges, and A. Smola, editors,Advances in Kernel Methods-Support Vector Learning.
MITPress.Alessandro Moschitti and Fabio Massimo Zanzotto.
2007.Fast and effective kernels for relational learning from texts.In Proceedings of the International Conference of MachineLearning (ICML), Corvallis, Oregon.Alessandro Moschitti.
2006.
Making tree kernels practicalfor natural language learning.
In Proceedings of EACL?06,Trento, Italy.Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi, andDan Moldovan.
2005.
COGEX at the second recognizingtextual entailment challenge.
In Proceedings of the SecondPASCAL Challenges Workshop on Recognising Textual En-tailment, Venice, Italy.Fabio Massimo Zanzotto and Alessandro Moschitti.
2006.
Au-tomatic learning of textual entailments with cross-pair sim-ilarities.
In Proceedings of the 21st Coling and 44th ACL,pages 401?408, Sydney, Australia, July.77
