Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 92?96,Dublin, Ireland, August 23-24, 2014.AI-KU: Using Co-Occurrence Modeling for Semantic SimilarityOsman Bas?kayaArtificial Intelligence LaboratoryKoc?
University, Istanbul, Turkeyobaskaya@ku.edu.trAbstractIn this paper, we describe our unsupervisedmethod submitted to the Cross-Level Se-mantic Similarity task in Semeval 2014 thatcomputes semantic similarity between twodifferent sized text fragments.
Our methodmodels each text fragment by using the co-occurrence statistics of either occurred wordsor their substitutes.
The co-occurrence mod-eling step provides dense, low-dimensionalembedding for each fragment which allowsus to calculate semantic similarity usingvarious similarity metrics.
Although ourcurrent model avoids the syntactic infor-mation, we achieved promising results andoutperformed all baselines.1 IntroductionSemantic similarity is a measure that specifies thesimilarity of one text?s meaning to another?s.
Se-mantic similarity plays an important role in vari-ous Natural Language Processing (NLP) tasks suchas textual entailment (Berant et al., 2012), summa-rization (Lin and Hovy, 2003), question answering(Surdeanu et al., 2011), text classification (Sebas-tiani, 2002), word sense disambiguation (Sch?utze,1998) and information retrieval (Park et al., 2005).There are three main approaches to computingthe semantic similarity between two text fragments.The first approach uses Vector Space Models (seeTurney & Pantel (2010) for an overview) whereeach text is represented as a bag-of-word model.The similarity between two text fragments can thenbe computed with various metrics such as cosinesimilarity.
Sparseness in the input nature is thekey problem for these models.
Therefore, laterworks such as Latent Semantic Indexing (?)
andThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/Topic Models (Blei et al., 2003) overcome spar-sity problems via reducing the dimensionality ofthe model by introducing latent variables.
The sec-ond approach blends various lexical and syntacticfeatures and attacks the problem through machinelearning models.
The third approach is based onword-to-word similarity alignment (Pilehvar et al.,2013; Islam and Inkpen, 2008).The Cross-Level Semantic Similarity (CLSS) taskin SemEval 20141(Jurgens et al., 2014) providesan evaluation framework to assess similarity meth-ods for texts in different volumes (i.e., lexical lev-els).
Unlike previous SemEval and *SEM tasksthat were interested in comparing texts with simi-lar volume, this task consists of four subtasks (para-graph2sentence, sentence2phrase, phrase2word andword2sense) that investigate the performance ofsystems based on pairs of texts of different sizes.A system should report the similarity score of agiven pair, ranging from 4 (two items have verysimilar meanings and the most important ideas,concepts, or actions in the larger text are repre-sented in the smaller text) to 0 (two items do notmean the same thing and are not on the same topic).In this paper, we describe our two unsupervisedsystems that are based on co-occurrence statisticsof words.
The only difference between the sys-tems is the input they use.
The first system uses thewords directly (after lemmatization, stop-word re-moval and excluding the non-alphanumeric char-acters) in text while the second system utilizes themost likely substitutes consulted by a 4-gram lan-guage model for each observed word position (i.e.,context).
Note that we participated two subtaskswhich are paragraph2sentence and sentence2phrase.The remainder of the paper proceeds as follows.Section 2 explains the preprocessing part, the dif-ference between the systems, co-occurrence mod-eling, and how we calculate the similarity between1http://alt.qcri.org/semeval2014/task3/92Type-ID LemmaSent-33 chooseSent-33 buySent-33 giftSent-33 cardSent-33 hardSent-33 decisionTable 1: Instance id-word pairs for a given sen-tence.two texts after co-occurrence modeling has beendone.
Section 3 discusses the results of our sys-tems and compares them to other participants?.
Sec-tion 4 discusses the findings and concludes withplans for future work.2 AlgorithmThis section explains preprocessing steps of thedata and the details of our two systems2.
Bothsystems rely on the co-occurrence statistics.
Theslight difference between the two is that the firstone uses the words that occur in the given textfragment (e.g., paragraph, sentence), whereas thelatter employs co-occurrence statistics on 100 sub-stitute samples for each word within the given textfragment.2.1 Data PreprocessingTwo AI-KU systems can be distinguished by theirinputs.
One uses the raw input words, whereas theother uses words?
likely substitutes according to alanguage model.AI-KU1: This system uses the words that werein the text.
All words are transformed into lower-case equivalents.
Lemmatization3and stop-wordremoval were performed, and non-alphanumericcharacters were excluded.
Table 1 displays thepairs for the following sentence which is an in-stance from paragraph2sentence test set:?Choosing what to buy with a $35 giftcard is a hard decision.
?Note that the input that we used to model co-occurrence statistics consists of all such pairs foreach fragment in a given subtask.2The code to replicate our work can be foundat https://github.com/osmanbaskaya/semeval14-task3.3Lemmatization is carried out with Stanford CoreNLPand transforms a word into its canonical or base form.AI-KU2: Previously, the utilization of high prob-ability substitutes and their co-occurrence statis-tics achieved notable performance on Word SenseInduction (WSI) (Baskaya et al., 2013) and Part-of-Speech Induction (Yatbaz et al., 2012) prob-lems.
AI-KU2represents each context of a wordby finding the most likely 100 substitutes suggestedby the 4-gram language model we built from ukWaC4(Ferraresi et al., 2008), a 2-billion word web-gatheredcorpus.
Since S-CODE algorithm works with dis-crete input, for each context we sample 100 substi-tute words with replacement using their probabili-ties.
Table 2 illustrates the context and substitutesof each context using a bigram language model.No lemmatization, stop-word removal and lower-case transformation were performed.2.2 Co-Occurrence ModelingThis subsection will explain the unsupervised methodwe employed to model co-occurrence statistics: theCo-occurrence data Embedding (CODE) method(Globerson et al., 2007) and its spherical exten-sion (S-CODE) proposed by Maron et al.
(2010).Unlike in our WSI work, where we ended up withan embedding for each word in the co-occurrencemodeling step in this task, we model each text unitsuch as a paragraph, a sentence or a phrase, to ob-tain embeddings for each instance.Input data for S-CODE algorithm consist of instance-id and each word in the text unit for the first sys-tem (Table 1 illustrates the pairs for only one textfragment) instance-ids and 100 substitute samplesof each word in text for the second system.
Inthe initial step, S-CODE puts all instance-ids andwords (or substitutes, depending on the system)randomly on an n-dimensional sphere.
If two dif-ferent instances have the same word or substitute,then these two instances attract one another ?
oth-erwise they repel each other.
When S-CODE con-verges, instances that have similar words or sub-stitutes will be closely located or else, they will bedistant from each other.AI-KU1: According to the training set perfor-mances for various n (i.e., number of dimensionsfor S-CODE algorithm), we picked 100 for bothtasks.AI-KU2: We picked n to be 200 and 100 forparagraph2sentence and sentence2phrase subtasks,respectively.4Available here: http://wacky.sslmit.unibo.it93Word Context Substitutesthe <s> dog The (0.12), A (0.11), If (0.02), As (0.07), Stray (0.001),..., wn(0.02)dog the cat (0.007), dog (0.005), animal (0.002), wolve (0.001), ..., wn(0.01)bites dog .
runs (0.14), bites (0.13), catches (0.04), barks (0.001), ..., wn(0.01)Table 2: Contexts and substitute distributions when a bigram language model is used.
w and n denote anarbitrary word in the vocabulary and the vocabulary size, respectively.System Pearson SpearmanParagraph-2-SentenceAI-KU10.671 0.676AI-KU20.542 0.531LCS 0.499 0.602lch 0.584 0.596lin 0.568 0.562JI 0.613 0.644Table 3: Paragraph-2-Sentence subtask scores forthe training data.
Subscripts in AI-KU systemsspecify the run number.Since this step is unsupervised, we tried to en-rich the data with ukWaC, however, enrichmentwith ukWaC did not work well on the training data.To this end, proposed scores were obtained usingonly the training and the test data provided by or-ganizers.2.3 Similarity CalculationWhen the S-CODE converges, there is an n-dimen-sional embedding for each textual level (e.g., para-graph, sentence, phrase) instance.
We can use asimilarity metric to calculate the similarity betweenthese embeddings.
For this task, systems shouldreport only the similarity between two specific crosslevel instances.
Note that we used cosine simi-larity to calculate similarity between two textualunits.
This similarity is the eventual similarity fortwo instances; no further processing (e.g., scaling)has been done.In this task, two correlation metrics were usedto evaluate the systems: Pearson correlation andSpearman?s rank correlation.
Pearson correlationtests the degree of similarity between the system?ssimilarity ratings and the gold standard ratings.
Spear-man?s rank correlation measures the degree of sim-ilarity between two rankings; similarity ratings pro-vided by a system and the gold standard ratings.System Pearson SpearmanSentence-2-PhraseAI-KU10.607 0.568AI-KU20.620 0.579LCS 0.500 0.582lch 0.484 0.491lin 0.492 0.470JI 0.465 0.465Table 4: Sentence2phrase subtask scores for thetraining data.3 Evaluation ResultsTables 3 and 4 show the scores for Paragraph-2-Sentence and Sentence-2-Phrase subtasks on thetraining data, respectively.
These tables containthe best individual scores for the performance met-rics, Normalized Longest Common Substring (LCS)baseline, which was given by task organizers, andthree additional baselines: lin (Lin, 1998), lch (Lea-cock and Chodorow, 1998), and the Jaccard In-dex (JI) baseline.
lin uses the information content(Resnik, 1995) of the least common subsumer ofconcepts A and B.
Information content (IC) indi-cates the specificity of a concept; the least com-mon subsumer of a concept A and B is the mostspecific concept from which A and B are inherited.lin similarity5returns the difference between twotimes of the IC of the least common subsumer ofA and B, and the sum of IC of both concepts.
Onthe other hand, lch is a score denoting how similartwo concepts are, calculated by using the shortestpath that connects the concept and the maximumdepth of the taxonomy in which the concepts oc-cur6(please see Pedersen et al.
(2004) for furtherdetails of these measures).
These two baselineswere calculated as follows.
First, using the Stan-5lin similarity = 2 ?
IC(lcs)/(IC(A) + IC(B)) wherelcs indicates the least common subsumer of concepts A andB.6The exact formulation is ?log(L/2d) where L is theshortest path length and d is the taxonomy depth.94System Pearson SpearmanParagraph-2-SentenceBest 0.837 0.8212ndBest 0.834 0.8203rdBest 0.826 0.817AI-KU10.732 0.727AI-KU20.698 0.700LCS 0.527 0.613lch 0.629 0.627lin 0.612 0.601JI 0.640 0.687Table 5: Paragraph-2-Sentence subtask scores forthe test data.
Best indicates the best correlationscore for the subtask.
LCS stands for NormalizedLongest Common Substring.
Subscripts in AI-KUsystems specify the run number.ford Part-of-Speech Tagger (Toutanova and Man-ning, 2000) we tagged words across all textual lev-els.
After tagging, we found the synsets of eachword matched with its part-of-speech using Word-Net 3.0 (Miller and Fellbaum, 1998).
For eachsynset of a word in the shorter textual unit (e.g.,sentence is shorter than paragraph), we calculatedthe lin/lch measure of each synset of all wordsin the longer textual unit and picked the highestscore.
When we found the scores for all words,we calculated the mean to find out the similaritybetween one pair in the test set.
Finally, JaccardIndex baseline was used to simply calculate thenumber of words in common (intersection) withtwo cross textual levels, normalized by the totalnumber of words (union).
Table 5 and 6 demon-strate the AI-KU runs on the test data.
Next, wepresent our results pertaining to the test data.Paragraph2Sentence: Both systems outperformedall the baselines for both metrics.
The best scorefor this subtask was .837 and our systems achieved.732 and .698 on Pearson and did similar on Spear-man metric.
These scores are promising since ourcurrent unsupervised systems are based on bag-of-words approach ?
they do not utilize any syntac-tic information.Sentence2Phrase: In this subtask, AI-KU sys-tems outperformed all baselines with the excep-tion of the AI-KU2system which performed slightlyworse than LCS on Spearman metric.
Performancesof systems and baselines were lower than Para-System Pearson SpearmanSentence-2-PhraseBest 0.777 0.6422ndBest 0.771 0.7603rdBest 0.760 0.757AI-KU10.680 0.646AI-KU20.617 0.612LCS 0.562 0.626lch 0.526 0.544lin 0.501 0.498JI 0.540 0.555Table 6: Sentence2phrase subtask scores for thetest data.graph2Sentence subtask, since smaller textual units(such as phrases) make the problem more difficult.4 ConclusionIn this work, we introduced two unsupervised sys-tems that utilize co-occurrence statistics and rep-resent textual units as dense, low dimensional em-beddings.
Although current systems are based onbag-of-word approach and discard the syntactic in-formation, they achieved promising results in bothparagraph2sentence and sentence2phrase subtasks.For future work, we will extend our algorithm byadding syntactic information (e.g, dependency pars-ing output) into the co-occurrence modeling step.ReferencesOsman Baskaya, Enis Sert, Volkan Cirik, and DenizYuret.
2013.
AI-KU: Using substitute vectors andco-occurrence modeling for word sense inductionand disambiguation.
In Proceedings of the SecondJoint Conference on Lexical and Computational Se-mantics (*SEM), Volume 2: Seventh InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 300?306.Jonathan Berant, Ido Dagan, and Jacob Goldberger.2012.
Learning entailment relations by global graphstructure optimization.
Computational Linguistics,38(1):73?111.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet allocation.
The Journal ofMachine Learning Research, 3:993?1022.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluatingukwac, a very large web-derived corpus of english.In In Proceedings of the 4th Web as Corpus Work-shop (WAC-4).95Amir Globerson, Gal Chechik, Fernando Pereira, andNaftali Tishby.
2007.
Euclidean embedding of co-occurrence data.
Journal of Machine Learning Re-search, 8(10).Aminul Islam and Diana Inkpen.
2008.
Semantic textsimilarity using corpus-based word similarity andstring similarity.
ACM Transactions on KnowledgeDiscovery from Data (TKDD), 2(2):10.David Jurgens, Mohammed Taher Pilehvar, andRoberto Navigli.
2014.
Semeval-2014 task 3:Cross-level semantic similarity.
In Proceedings ofthe 8th International Workshop on Semantic Evalu-ation (SemEval-2014).
August 23-24, 2014, Dublin,Ireland.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context and wordnet similarity for wordsense identification.
WordNet: An electronic lexicaldatabase, 49(2):265?283.Chin-Yew Lin and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003Conference of the North American Chapter of theAssociation for Computational Linguistics on Hu-man Language Technology-Volume 1, pages 71?78.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In ICML, volume 98, pages 296?304.Yariv Maron, Michael Lamar, and Elie Bienenstock.2010.
Sphere Embedding: An Application to Part-of-Speech Induction.
In J Lafferty, C K I Williams,J Shawe-Taylor, R S Zemel, and A Culotta, editors,Advances in Neural Information Processing Systems23, pages 1567?1575.George Miller and Christiane Fellbaum.
1998.
Word-net: An electronic lexical database.Eui-Kyu Park, Dong-Yul Ra, and Myung-Gil Jang.2005.
Techniques for improving web retrieval ef-fectiveness.
Information processing & management,41(5):1207?1223.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet:: Similarity: measuring the re-latedness of concepts.
In Demonstration Papers atHLT-NAACL 2004, pages 38?41.Mohammad Taher Pilehvar, David Jurgens, andRoberto Navigli.
2013.
Align, disambiguate andwalk: A unified approach for measuring semanticsimilarity.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(ACL 2013).Philip Resnik.
1995.
Using information content toevaluate semantic similarity in a taxonomy.
arXivpreprint cmp-lg/9511007.Hinrich Sch?utze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?123.Fabrizio Sebastiani.
2002.
Machine learning in auto-mated text categorization.
ACM computing surveys(CSUR), 34(1):1?47.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2011.
Learning to rank answers to non-factoid questions from web collections.
Computa-tional Linguistics, 37(2):351?383.Kristina Toutanova and Christopher D Manning.
2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger.
In Proceedingsof the 2000 Joint SIGDAT conference on Empiricalmethods in natural language processing and verylarge corpora: held in conjunction with the 38th An-nual Meeting of the Association for ComputationalLinguistics-Volume 13, pages 63?70.Peter D. Turney and Patrick Pantel.
2010.
From Fre-quency to Meaning: Vector Space Models of Se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret.
2012.Learning syntactic categories using paradigmaticrepresentations of word context.
In Proceedings ofthe 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 940?951.96
