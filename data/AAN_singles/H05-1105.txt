Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 835?842, Vancouver, October 2005. c?2005 Association for Computational LinguisticsUsing the Web as an Implicit Training Set:Application to Structural Ambiguity ResolutionPreslav Nakov and Marti HearstEECS and SIMSUniversity of California at BerkeleyBerkeley, CA 94720nakov@cs.berkeley.edu, hearst@sims.berkeley.eduAbstractRecent work has shown that very largecorpora can act as training data for NLPalgorithms even without explicit labels.
Inthis paper we show how the use of sur-face features and paraphrases in queriesagainst search engines can be used to inferlabels for structural ambiguity resolutiontasks.
Using unsupervised algorithms, weachieve 84% precision on PP-attachmentand 80% on noun compound coordination.1 IntroductionResolution of structural ambiguity problems suchas noun compound bracketing, prepositional phrase(PP) attachment, and noun phrase coordination re-quires using information about lexical items andtheir cooccurrences.
This in turn leads to the datasparseness problem, since algorithms that rely onmaking decisions based on individual lexical itemsmust have statistics about every word that may beencountered.
Past approaches have dealt with thedata sparseness problem by attempting to generalizefrom semantic classes, either manually built or auto-matically derived.More recently, Banko and Brill (2001) have ad-vocated for the creative use of very large text col-lections as an alternative to sophisticated algorithmsand hand-built resources.
They demonstrate the ideaon a lexical disambiguation problem for which la-beled examples are available ?for free?.
The prob-lem is to choose which of 2-3 commonly confusedwords (e.g., {principle, principal}) are appropriatefor a given context.
The labeled data comes ?forfree?
by assuming that in most edited written text,the words are used correctly, so training can be donedirectly from the text.
Banko and Brill (2001) showthat even using a very simple algorithm, the resultscontinue to improve log-linearly with more trainingdata, even out to a billion words.
A potential limita-tion of this approach is the question of how applica-ble it is for NLP problems more generally ?
how canwe treat a large corpus as a labeled collection for awide range of NLP tasks?In a related strand of work, Lapata and Keller(2004) show that computing n-gram statistics oververy large corpora yields results that are competi-tive with if not better than the best supervised andknowledge-based approaches on a wide range ofNLP tasks.
For example, they show that for theproblem of noun compound bracketing, the perfor-mance of an n-gram based model computed usingsearch engine statistics was not significantly differ-ent from the best supervised algorithm whose pa-rameters were tuned and which used a taxonomy.They find however that these approaches generallyfail to outperform supervised state-of-the-art modelsthat are trained on smaller corpora, and so concludethat web-based n-gram statistics should be the base-line to beat.We feel the potential of these ideas is not yet fullyrealized.
We are interested in finding ways to furtherexploit the availability of enormous web corpora asimplicit training data.
This is especially importantfor structural ambiguity problems in which the de-cisions must be made on the basis of the behavior835of individual lexical items.
The trick is to figure outhow to use information that is latent in the web as acorpus, and web search engines as query interfacesto that corpus.In this paper we describe two techniques ?
sur-face features and paraphrases ?
that push the ideasof Banko and Brill (2001) and Lapata and Keller(2004) farther, enabling the use of statistics gatheredfrom very large corpora in an unsupervised man-ner.
In recent work (Nakov and Hearst, 2005) weshowed that a variation of the techniques, when ap-plied to the problem of noun compound bracketing,produces higher accuracy than Lapata and Keller(2004) and the best supervised results.
In this pa-per we adapt the techniques to the structural disam-biguation problems of prepositional phrase attach-ment and noun compound coordination.2 Prepositional Phrase AttachmentA long-standing challenge for syntactic parsers isthe attachment decision for prepositional phrases.
Ina configuration where a verb takes a noun comple-ment that is followed by a PP, the problem arises ofwhether the PP attaches to the noun or to the verb.Consider the following contrastive pair of sentences:(1) Peter spent millions of dollars.
(noun)(2) Peter spent time with his family.
(verb)In the first example, the PP millions of dollars at-taches to the noun millions, while in the second thePP with his family attaches to the verb spent.Past work on PP-attachment has often cast theseassociations as the quadruple (v, n1, p, n2), where vis the verb, n1 is the head of the direct object, p is thepreposition (the head of the PP) and n2 is the headof the NP inside the PP.
For example, the quadruplefor (2) is (spent, time, with, family).2.1 Related WorkEarly work on PP-attachment ambiguity resolu-tion relied on syntactic (e.g., ?minimal attachment?and ?right association?)
and pragmatic considera-tions.
Most recent work can be divided into su-pervised and unsupervised approaches.
Supervisedapproaches tend to make use of semantic classesor thesauri in order to deal with data sparsenessproblems.
Brill and Resnik (1994) used the su-pervised transformation-based learning method andlexical and conceptual classes derived from Word-Net, achieving 82% precision on 500 randomly se-lected examples.
Ratnaparkhi et al (1994) cre-ated a benchmark dataset of 27,937 quadruples(v, n1, p, n2), extracted from the Wall Street Jour-nal.
They found the human performance on thistask to be 88%1.
Using this dataset, they trained amaximum entropy model and a binary hierarchy ofword classes derived by mutual information, achiev-ing 81.6% precision.
Collins and Brooks (1995)used a supervised back-off model to achieve 84.5%precision on the Ratnaparkhi test set.
Stetina andMakoto (1997) use a supervised method with a deci-sion tree and WordNet classes to achieve 88.1% pre-cision on the same test set.
Toutanova et al (2004)use a supervised method that makes use of morpho-logical and syntactic analysis and WordNet synsets,yielding 87.5% accuracy.In the unsupervised approaches, the attachmentdecision depends largely on co-occurrence statisticsdrawn from text collections.
The pioneering workin this area was that of Hindle and Rooth (1993).Using a partially parsed corpus, they calculate andcompare lexical associations over subsets of the tu-ple (v, n1, p), ignoring n2, and achieve 80% preci-sion at 80% recall.More recently, Ratnaparkhi (1998) developed anunsupervised method that collects statistics fromtext annotated with part-of-speech tags and mor-phological base forms.
An extraction heuristic isused to identify unambiguous attachment decisions,for example, the algorithm can assume a noun at-tachment if there is no verb within k words to theleft of the preposition in a given sentence, amongother conditions.
This extraction heuristic uncov-ered 910K unique tuples of the form (v, p, n2) and(n, p, n2), although the results are very noisy, sug-gesting the correct attachment only about 69% of thetime.
The tuples are used as training data for clas-sifiers, the best of which achieves 81.9% precisionon the Ratnaparkhi test set.
Pantel and Lin (2000)describe an unsupervised method that uses a collo-cation database, a thesaurus, a dependency parser,and a large corpus (125M words), achieving 84.3%precision on the Ratnaparkhi test set.
Using sim-1When presented with a whole sentence, average humansscore 93%.836ple combinations of web-based n-grams, Lapata andKeller (2005) achieve lower results, in the low 70?s.Using a different collection consisting of GermanPP-attachment decisions, Volk (2000) uses the webto obtain n-gram counts.
He compared Pr(p|n1) toPr(p|v), where Pr(p|x) = #(x, p)/#(x).
Here xcan be n1 or v. The bigram frequencies #(x, p)were obtained using the Altavista NEAR operator.The method was able to make a decision on 58%of the examples with a precision of 75% (baseline63%).
Volk (2001) then improved on these resultsby comparing Pr(p, n2|n1) to Pr(p, n2|v).
Usinginflected forms, he achieved P=75% and R=85%.Calvo and Gelbukh (2003) experimented with avariation of this, using exact phrases instead of theNEAR operator.
For example, to disambiguate Veoal gato con un telescopio, they compared frequen-cies for phrases such as ?ver con telescopio?
and?gato con telescopio?.
They tested this idea on 181randomly chosen Spanish disambiguation examples,labelling 89.5% recall with a precision of 91.97%.2.2 Models and Features2.2.1 n-gram ModelsWe computed two co-occurrence models;(i) Pr(p|n1) vs. Pr(p|v)(ii) Pr(p, n2|n1) vs. Pr(p, n2|v).Each of these was computed two different ways:using Pr (probabilities) and # (frequencies).
We es-timate the n-gram counts using exact phrase queries(with inflections, derived from WordNet 2.0) usingthe MSN Search Engine.
We also allow for deter-miners, where appropriate, e.g., between the prepo-sition and the noun when querying for #(p, n2).
Weadd up the frequencies for all possible variations.Web frequencies were reliable enough and did notneed smoothing for (i), but for (ii), smoothing usingthe technique described in Hindle and Rooth (1993)led to better recall.
We also tried back-off from (ii)to (i), as well as back-off plus smoothing, but did notfind improvements over smoothing alone.
We foundn-gram counts to be unreliable when pronouns ap-pear in the test set rather than nouns, and disabledthem in these cases.
Such examples can still be han-dled by paraphrases or surface features (see below).2.2.2 Web-Derived Surface FeaturesAuthors sometimes (consciously or not) disam-biguate the words they write by using surface-levelmarkers to suggest the correct meaning.
We havefound that exploiting these markers, when they oc-cur, can prove to be very helpful for making dis-ambiguation decisions.
The enormous size of websearch engine indexes facilitates finding such mark-ers frequently enough to make them useful.For example, John opened the door with a key isa difficult verb attachment example because doors,keys, and opening are all semantically related.
Todetermine if this should be a verb or a noun attach-ment, we search for cues that indicate which of theseterms tend to associate most closely.
If we see paren-theses used as follows:?open the door (with a key)?this suggests a verb attachment, since the parenthe-ses signal that ?with a key?
acts as its own unit.Similarly, hyphens, colons, capitalization, and otherpunctuation can help signal disambiguation deci-sions.
For Jean ate spaghetti with sauce, if we see?eat: spaghetti with sauce?this suggests a noun attachment.Table 1 illustrates a wide variety of surface fea-tures, along with the attachment decisions they areassumed to suggest (events of frequency 1 have beenignored).
The surface features for PP-attachmenthave low recall: most of the examples have no sur-face features extracted.We gather the statistics needed by issuing queriesto web search engines.
Unfortunately, search en-gines usually ignore punctuation characters, thuspreventing querying directly for terms containinghyphens, brackets, etc.
We collect these numbersindirectly by issuing queries with exact phrases andthen post-processing the top 1,000 resulting sum-maries2, looking for the surface features of interest.We use Google for both the surface feature and para-phrase extractions (described below).2.2.3 ParaphrasesThe second way we extend the use of web countsis by paraphrasing the relation of interest and see-ing if it can be found in its alternative form, which2We often obtain more than 1,000 summaries per examplebecause we usually issue multiple queries per surface pattern,by varying inflections and inclusion of determiners.837suggests the correct attachment decision.
We usethe following patterns along with their associated at-tachment predictions:(1) v n2 n1 (noun)(2) v p n2 n1 (verb)(3) p n2 * v n1 (verb)(4) n1 p n2 v (noun)(5) v pronoun p n2 (verb)(6) be n1 p n2 (noun)The idea behind Pattern (1) is to determineif ?n1 p n2?
can be expressed as a noun com-pound; if this happens sufficiently often, we canpredict a noun attachment.
For example, meet/vdemands/n1 from/p customers/n2 becomes meet/vthe customers/n2 demands/n1.Note that the pattern could wrongly target ditran-sitive verbs: e.g., it could turn gave/v an apple/n1to/p him/n2 into gave/v him/n2 an apple/n1.
To pre-vent this, we do not allow a determiner before n1,but we do require one before n2.
In addition, wedisallow the pattern if the preposition is to and werequire both n1 and n2 to be nouns (as opposed tonumbers, percents, pronouns, determiners etc.
).Pattern (2) predicts a verb attachment.
It presup-poses that ?p n2?
is an indirect object of the verb vand tries to switch it with the direct object n1, e.g.,had/v a program/n1 in/p place/n2 would be trans-formed into had/v in/p place/n2 a program/n1.
Werequire n1 to be preceded by a determiner (to prevent?n2 n1?
forming a noun compound).Pattern (3) looks for appositions, where the PP hasmoved in front of the verb, e.g., to/p him/n2 I gave/van apple/n1.
The symbol * indicates a wildcard po-sition where we allow up to three intervening words.Pattern (4) looks for appositions, where the PP hasmoved in front of the verb together with n1.
It wouldtransform shaken/v confidence/n1 in/p markets/n2into confidence/n1 in/p markets/n2 shaken/v.Pattern (5) is motivated by the observation thatif n1 is a pronoun, this suggests a verb attach-ment (Hindle and Rooth, 1993).
(A separate featurechecks if n1 is a pronoun.)
The pattern substitutesn1 with a dative pronoun (we allow him and her),e.g., it will convert put/v a client/n1 at/p odds/n2into put/v him at/p odds/n2.Pattern (6) is motivated by the observation that theverb to be is typically used with a noun attachment.
(A separate feature checks if v is a form of the verbto be.)
The pattern substitutes v with is and are, e.g.it will turn eat/v spaghetti/n1 with/p sauce/n2 into isspaghetti/n1 with/p sauce/n2.These patterns all allow for determiners where ap-propriate, unless explicitly stated otherwise.
For agiven example, a prediction is made if at least oneinstance of the pattern has been found.2.3 EvaluationFor the evaluation, we used the test part (3,097 ex-amples) of the benchmark dataset by Ratnaparkhi etal.
(1994).
We used all 3,097 test examples in orderto make our results directly comparable.Unfortunately, there are numerous errors in thetest set3.
There are 149 examples in which a baredeterminer is labeled as n1 or n2 rather than the ac-tual head noun.
Supervised algorithms can compen-sate for this problem by learning from the trainingset that ?the?
can act as a noun in this collection, butunsupervised algorithms cannot.In addition, there are also around 230 examplesin which the nouns contain special symbols like: %,slash, &, ?, which are lost when querying against asearch engine.
This poses a problem for our algo-rithm but is not a problem with the test set itself.The results are shown in Table 2.
Following Rat-naparkhi (1998), we predict a noun attachment if thepreposition is of (a very reliable heuristic).
The tableshows the performance for each feature in isolation(excluding examples whose preposition is of).
Thesurface features are represented by a single score inTable 2: for a given example, we sum up separatelythe number of noun- and verb-attachment patternmatches, and assign the attachment with the largernumber of matches.We combine the bold rows of Table 2 in a majorityvote (assigning noun attachment to all of instances),obtaining P=85.01%, R=91.77%.
To get 100% re-call, we assign all undecided cases to verb (sincethe majority of the remaining non-of instances at-tach to the verb, yielding P=83.63%, R=100%.
Weshow 0.95-level confidence intervals for the preci-sion, computed by a general method based on con-stant chi-square boundaries (Fleiss, 1981).A test for statistical significance reveals that ourresults are as strong as those of the leading unsuper-3Ratnaparkhi (1998) notes that the test set contains errors,but does not correct them.838Example Predicts P(%) R(%)open Door with a key noun 100.00 0.13(open) door with a key noun 66.67 0.28open (door with a key) noun 71.43 0.97open - door with a key noun 69.70 1.52open / door with a key noun 60.00 0.46open, door with a key noun 65.77 5.11open: door with a key noun 64.71 1.57open; door with a key noun 60.00 0.23open.
door with a key noun 64.13 4.24open?
door with a key noun 83.33 0.55open!
door with a key noun 66.67 0.14open door With a Key verb 0.00 0.00(open door) with a key verb 50.00 0.09open door (with a key) verb 73.58 2.44open door - with a key verb 68.18 2.03open door / with a key verb 100.00 0.14open door, with a key verb 58.44 7.09open door: with a key verb 70.59 0.78open door; with a key verb 75.00 0.18open door.
with a key verb 60.77 5.99open door!
with a key verb 100.00 0.18Table 1: PP-attachment surface features.
Preci-sion and recall shown are across all examples, notjust the door example shown.vised approach on this collection (Pantel and Lin,2000).
Unlike that work, we do not require a collo-cation database, a thesaurus, a dependency parser,nor a large domain-dependent text corpus, whichmakes our approach easier to implement and to ex-tend to other languages.3 CoordinationCoordinating conjunctions (and, or, but, etc.)
posemajor challenges to parsers and their proper han-dling is essential for the understanding of the sen-tence.
Consider the following ?cooked?
example:The Department of Chronic Diseases and HealthPromotion leads and strengthens global efforts toprevent and control chronic diseases or disabilitiesand to promote health and quality of life.Conjunctions can link two words, two con-stituents (e.g., NPs), two clauses or even two sen-tences.
Thus, the first challenge is to identify theboundaries of the conjuncts of each coordination.The next problem comes from the interaction ofthe coordinations with other constituents that attachto its conjuncts (most often prepositional phrases).In the example above we need to decide between[health and [quality of life]] and [[health and qual-Model P(%) R(%)Baseline (noun attach) 41.82 100.00#(x, p) 58.91 83.97Pr(p|x) 66.81 83.97Pr(p|x) smoothed 66.81 83.97#(x, p, n2) 65.78 81.02Pr(p, n2|x) 68.34 81.62Pr(p, n2|x) smoothed 68.46 83.97(1) ?v n2 n1?
59.29 22.06(2) ?p n2 v n1?
57.79 71.58(3) ?n1 * p n2 v?
65.78 20.73(4) ?v p n2 n1?
81.05 8.75(5) ?v pronoun p n2?
75.30 30.40(6) ?be n1 p n2?
63.65 30.54n1 is pronoun 98.48 3.04v is to be 79.23 9.53Surface features (summed) 73.13 9.26Maj.
vote, of ?
noun 85.01?1.21 91.77Maj.
vote, of ?
noun, N/A ?
verb 83.63?1.30 100.00Table 2: PP-attachment results, in percentages.ity] of life].
From a semantic point of view, weneed to determine whether the or in chronic dis-eases or disabilities really means or or is used as anand (Agarwal and Boggess, 1992).
Finally, we needto choose between a non-elided and an elided read-ing: [[chronic diseases] or disabilities] vs. [chronic[diseases or disabilities]].Below we focus on a special case of the latterproblem: noun compound (NC) coordination.
Con-sider the NC car and truck production.
Its realmeaning is car production and truck production.However, due to the principle of economy of ex-pression, the first instance of production has beencompressed out by means of ellipsis.
By contrast,in president and chief executive, president is simplylinked to chief executive.
There is also an all-way co-ordination, where the conjunct is part of the whole,as in Securities and Exchange Commission.More formally, we consider configurations of thekind n1 c n2 h, where n1 and n2 are nouns, c is acoordination (and or or) and h is the head noun4.The task is to decide whether there is an ellipsis ornot, independently of the local context.
Syntacti-cally, this can be expressed by the following brack-etings: [[n1 c n2] h] versus [n1 c [n2 h]].
(Collins?parser (Collins, 1997) always predicts a flat NP forsuch configurations.)
In order to make the task more4The configurations of the kind n h1 c h2 (e.g., company/ncars/h1 and/c trucks/h2) can be handled in a similar way.839realistic (from a parser?s perspective), we ignore theoption of all-way coordination and try to predict thebracketing in Penn Treebank (Marcus et al, 1994)for configurations of this kind.
The Penn Treebankbrackets NCs with ellipsis as, e.g.,(NP car/NN and/CC truck/NN production/NN).and without ellipsis as(NP (NP president/NN) and/CC (NP chief/NN exec-utive/NN))The NPs with ellipsis are flat, while the others con-tain internal NPs.
The all-way coordinations can ap-pear bracketed either way and make the task harder.3.1 Related WorkCoordination ambiguity is under-explored, despitebeing one of the three major sources of structuralambiguity (together with prepositional phrase at-tachment and noun compound bracketing), and be-longing to the class of ambiguities for which thenumber of analyses is the number of binary treesover the corresponding nodes (Church and Patil,1982), and despite the fact that conjunctions areamong the most frequent words.Rus et al (2002) present a deterministic rule-based approach for bracketing in context of coor-dinated NCs of the kind n1 c n2 h, as a necessarystep towards logical form derivation.
Their algo-rithm uses POS tagging, syntactic parses, semanticsenses of the nouns (manually annotated), lookupsin a semantic network (WordNet) and the type of thecoordination conjunction to make a 3-way classifi-cation: ellipsis, no ellipsis and all-way coordination.Using a back-off sequence of 3 different heuristics,they achieve 83.52% precision (baseline 61.52%) ona set of 298 examples.
When 3 additional context-dependent heuristics and 224 additional exampleswith local contexts are added, the precision jumpsto 87.42% (baseline 52.35%), with 71.05% recall.Resnik (1999) disambiguates two kinds of pat-terns: n1 and n2 n3 and n1 n2 and n3 n4(e.g., [food/n1 [handling/n2 and/c storage/n3]procedures/n4]).
While there are two options forthe former (all-way coordinations are not allowed),there are 5 valid bracketings for the latter.
Follow-ing Kurohashi and Nagao (1992), Resnik makes de-cisions based on similarity of form (i.e., numberagreement: P=53%, R=90.6%), similarity of mean-ing (P=66%, R=71.2%) and conceptual associationExample Predicts P(%) R(%)(buy) and sell orders NO ellipsis 33.33 1.40buy (and sell orders) NO ellipsis 70.00 4.67buy: and sell orders NO ellipsis 0.00 0.00buy; and sell orders NO ellipsis 66.67 2.80buy.
and sell orders NO ellipsis 68.57 8.18buy[...] and sell orders NO ellipsis 49.00 46.73buy- and sell orders ellipsis 77.27 5.14buy and sell / orders ellipsis 50.54 21.73(buy and sell) orders ellipsis 92.31 3.04buy and sell (orders) ellipsis 90.91 2.57buy and sell, orders ellipsis 92.86 13.08buy and sell: orders ellipsis 93.75 3.74buy and sell; orders ellipsis 100.00 1.87buy and sell.
orders ellipsis 93.33 7.01buy and sell[...] orders ellipsis 85.19 18.93Table 3: Coordination surface features.
Precisionand recall shown are across all examples, not just thebuy and sell orders shown.
(P=75.0%, R=69.3%).
Using a decision tree to com-bine the three information sources, he achieves 80%precision (baseline 66%) at 100% recall for the 3-noun coordinations.
For the 4-noun coordinationsthe precision is 81.6% (baseline 44.9%), 85.4% re-call.Chantree et al (2005) cover a large set of ambi-guities, not limited to nouns.
They allow the headword to be a noun, a verb or an adjective, and themodifier to be an adjective, a preposition, an ad-verb, etc.
They extract distributional informationfrom the British National Corpus and distributionalsimilarities between words, similarly to (Resnik,1999).
In two different experiments they achieveP=88.2%, R=38.5% and P=80.8%, R=53.8% (base-line P=75%).Goldberg (1999) resolves the attachment of am-biguous coordinate phrases of the kind n1 p n2 cn3, e.g., box/n1 of/p chocolates/n2 and/c roses/n3.Using an adaptation of the algorithm proposed byRatnaparkhi (1998) for PP-attachment, she achievesP=72% (baseline P=64%), R=100.00%.Agarwal and Boggess (1992) focus on the identi-fication of the conjuncts of coordinate conjunctions.Using POS and case labels in a deterministic algo-rithm, they achieve P=81.6%.
Kurohashi and Na-gao (1992) work on the same problem for Japanese.Their algorithm looks for similar word sequencesamong with sentence simplification, and achieves aprecision of 81.3%.8403.2 Models and Features3.2.1 n-gram ModelsWe use the following n-gram models:(i) #(n1, h) vs. #(n2, h)(ii) #(n1, h) vs. #(n1, c, n2)Model (i) compares how likely it is that n1 mod-ifies h, as opposed to n2 modifying h. Model (ii)checks which association is stronger: between n1and h, or between n1 and n2.
Regardless of whetherthe coordination is or or and, we query for both andwe add up the corresponding counts.3.2.2 Web-Derived Surface FeaturesThe set of surface features is similar to the one weused for PP-attachment.
These are brackets, slash,comma, colon, semicolon, dot, question mark, ex-clamation mark, and any character.
There are twoadditional ellipsis-predicting features: a dash aftern1 and a slash after n2, see Table 3.3.2.3 ParaphrasesWe use the following paraphrase patterns:(1) n2 c n1 h (ellipsis)(2) n2 h c n1 (NO ellipsis)(3) n1 h c n2 h (ellipsis)(4) n2 h c n1 h (ellipsis)If matched frequently enough, each of these pat-terns predicts the coordination decision indicated inparentheses.
If found only infrequently or not foundat all, the opposite decision is made.
Pattern (1)switches the places of n1 and n2 in the coordinatedNC.
For example, bar and pie graph can easily be-come pie and bar graph, which favors ellipsis.
Pat-tern (2) moves n2 and h together to the left of thecoordination conjunction, and places n1 to the right.If this happens frequently enough, there is no ellip-sis.
Pattern (3) inserts the elided head h after n1 withthe hope that if there is ellipsis, we will find the fullphrase elsewhere in the data.
Pattern (4) combinespattern (1) and pattern (3); it not only inserts h aftern1 but also switches the places of n1 and n2.As shown in Table 4, we included four of theheuristics by Rus et al (2002).
Heuristic 1 predictsno coordination when n1 and n2 are the same, e.g.,milk and milk products.
Heuristics 2 and 3 perform alookup in WordNet and we did not use them.
Heuris-tics 4, 5 and 6 exploit the local context, namely theModel P(%) R(%)Baseline: ellipsis 56.54 100.00(n1, h) vs. (n2, h) 80.33 28.50(n1, h) vs. (n1, c, n2) 61.14 45.09(n2, c, n1, h) 88.33 14.02(n2, h, c, n1) 76.60 21.96(n1, h, c, n2, h) 75.00 6.54(n2, h, c, n1, h) 78.67 17.52Heuristic 1 75.00 0.93Heuristic 4 64.29 6.54Heuristic 5 61.54 12.15Heuristic 6 87.09 7.24Number agreement 72.22 46.26Surface sum 82.80 21.73Majority vote 83.82 80.84Majority vote, N/A ?
no ellipsis 80.61 100.00Table 4: Coordination results, in percentages.adjectives modifying n1 and/or n2.
Heuristic 4 pre-dicts no ellipsis if both n1 and n2 are modified byadjectives.
Heuristic 5 predicts ellipsis if the coor-dination is or and n1 is modified by an adjective,but n2 is not.
Heuristic 6 predicts no ellipsis if n1is not modified by an adjective, but n2 is.
We usedversions of heuristics 4, 5 and 6 that check for deter-miners rather than adjectives.Finally, we included the number agreement fea-ture (Resnik, 1993): (a) if n1 and n2 match in num-ber, but n1 and h do not, predict ellipsis; (b) if n1and n2 do not match in number, but n1 and h do,predict no ellipsis; (c) otherwise leave undecided.3.3 EvaluationWe evaluated the algorithms on a collection of 428examples extracted from the Penn Treebank.
On ex-traction, determiners and non-noun modifiers wereallowed, but the program was only presented withthe quadruple (n1, c, n2, h).
As Table 4 shows, ouroverall performance of 80.61 is on par with other ap-proaches, whose best scores fall into the low 80?s forprecision.
(Direct comparison is not possible, as thetasks and datasets all differ.
)As Table 4 shows, n-gram model (i) performswell, but n-gram model (ii) performs poorly, proba-bly because the (n1, c, n2) contains three words, asopposed to two for the alternative (n1, h), and thusa priori is less likely to be observed.The surface features are less effective for resolv-ing coordinations.
As Table 3 shows, they are verygood predictors of ellipsis, but are less reliable when841predicting NO ellipsis.
We combine the bold rowsof Table 4 in a majority vote, obtaining P=83.82%,R=80.84%.
We assign all undecided cases to no el-lipsis, yielding P=80.61%, R=100%.4 Conclusions and Future WorkWe have shown that simple unsupervised algorithmsthat make use of bigrams, surface features and para-phrases extracted from a very large corpus are ef-fective for several structural ambiguity resolutionstasks, yielding results competitive with the best un-supervised results, and close to supervised results.The method does not require labeled training data,nor lexicons nor ontologies.
We think this is apromising direction for a wide range of NLP tasks.In future work we intend to explore better-motivatedevidence combination algorithms and to apply theapproach to other NLP problems.Acknowledgements.
This research was supportedby NSF DBI-0317510 and a gift from Genentech.ReferencesRajeev Agarwal and Lois Boggess.
1992.
A simple but usefulapproach to conjunct identification.
In Proceedings of ACL.Michele Banko and Eric Brill.
2001.
Scaling to very very largecorpora for natural language disambiguation.
In Proceed-ings of ACL.Eric Brill and Philip Resnik.
1994.
A rule-based approachto prepositional phrase attachment disambiguation.
In Pro-ceedings of COLING.Hiram Calvo and Alexander Gelbukh.
2003.
Improving prepo-sitional phrase attachment disambiguation using the web ascorpus.
In Progress in Pattern Recognition, Speech andImage Analysis: 8th Iberoamerican Congress on PatternRecognition, CIARP 2003.Francis Chantree, Adam Kilgarriff, Anne De Roeck, and Alis-tair Willis.
2005.
Using a distributional thesaurus to resolvecoordination ambiguities.
In Technical Report 2005/02.
TheOpen University, UK.Kenneth Church and Ramesh Patil.
1982.
Coping with syntac-tic ambiguity or how to put the block in the box on the table.Amer.
J. of Computational Linguistics, 8(3-4):139?149.Michael Collins and James Brooks.
1995.
Prepositional phraseattachment through a backed-off model.
In Proceedings ofEMNLP, pages 27?38.M.
Collins.
1997.
Three generative, lexicalised models for sta-tistical parsing.
In Proceedings of ACL, pages 16?23.Joseph Fleiss.
1981.
Statistical Methods for Rates and Propor-tions (2nd Ed.).
John Wiley & Sons, New York.Miriam Goldberg.
1999.
An unsupervised model for statis-tically determining coordinate phrase attachment.
In Pro-ceedings of ACL.Donald Hindle and Mats Rooth.
1993.
Structural ambiguityand lexical relations.
Computational Linguistics, 19(1):103?120.Sadao Kurohashi and Makoto Nagao.
1992.
Dynamic pro-gramming method for analyzing conjunctive structures injapanese.
In Proceedings of COLING, volume 1.Mirella Lapata and Frank Keller.
2004.
The Web as a base-line: Evaluating the performance of unsupervised Web-based models for a range of NLP tasks.
In Proceedings ofHLT-NAACL, pages 121?128, Boston.Mirella Lapata and Frank Keller.
2005.
Web-based models fornatural language processing.
ACM Transactions on Speechand Language Processing, 2:1?31.Mitchell Marcus, Beatrice Santorini, and Mary Marcinkiewicz.1994.
Building a large annotated corpus of English: ThePenn Treebank.
Computational Linguistics, 19(2):313?330.Preslav Nakov and Marti Hearst.
2005.
Search engine statisticsbeyond the n-gram: Application to noun compound bracket-ing.
In Proceedings of CoNLL 2005.Patrick Pantel and Dekang Lin.
2000.
An unsupervised ap-proach to prepositional phrase attachment using contextuallysimilar words.
In Proceedings of ACL.Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994.A maximum entropy model for prepositional phrase attach-ment.
In Proceedings of the ARPA Workshop on Human Lan-guage Technology., pages 250?255.Adwait Ratnaparkhi.
1998.
Statistical models for unsuper-vised prepositional phrase attachment.
In Proceedings ofCOLING-ACL, volume 2, pages 1079?1085.Philip Resnik.
1993.
Selection and information: a class-basedapproach to lexical relationships.
Ph.D. thesis, Universityof Pennsylvania, UMI Order No.
GAX94-13894.Philip Resnik.
1999.
Semantic similarity in a taxonomy: Aninformation-based measure and its application to problemsof ambiguity in natural language.
JAIR, 11:95?130.Vasile Rus, Dan Moldovan, and Orest Bolohan.
2002.
Brack-eting compound nouns for logic form derivation.
In Su-san M. Haller and Gene Simmons, editors, FLAIRS Confer-ence.
AAAI Press.Jiri Stetina and Makoto.
1997.
Corpus based PP attachmentambiguity resolution with a semantic dictionary.
In Proceed-ings of WVLC, pages 66?80.Kristina Toutanova, Christopher D. Manning, and Andrew Y.Ng.
2004.
Learning random walk models for inducing worddependency distributions.
In Proceedings of ICML.Martin Volk.
2000.
Scaling up.
using the WWW to resolve PPattachment ambiguities.
In Proceedings of Konvens-2000.Sprachkommunikation.Martin Volk.
2001.
Exploiting the WWW as a corpus to resolvePP attachment ambiguities.
In Proc.
of Corpus Linguistics.842
