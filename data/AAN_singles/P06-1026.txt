Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 201?208,Sydney, July 2006. c?2006 Association for Computational LinguisticsLearning the Structure of Task-driven Human-Human DialogsSrinivas BangaloreAT&T Labs-Research180 Park AveFlorham Park, NJ 07932srini@research.att.comGiuseppe Di FabbrizioAT&T Labs-Research180 Park AveFlorham Park, NJ 07932pino@research.att.comAmanda StentDept of Computer ScienceStony Brook UniversityStony Brook, NYstent@cs.sunysb.eduAbstractData-driven techniques have been usedfor many computational linguistics tasks.Models derived from data are generallymore robust than hand-crafted systemssince they better reflect the distributionof the phenomena being modeled.
Withthe availability of large corpora of spo-ken dialog, dialog management is nowreaping the benefits of data-driven tech-niques.
In this paper, we compare two ap-proaches to modeling subtask structure indialog: a chunk-based model of subdialogsequences, and a parse-based, or hierarchi-cal, model.
We evaluate these models us-ing customer agent dialogs from a catalogservice domain.1 IntroductionAs large amounts of language data have becomeavailable, approaches to sentence-level process-ing tasks such as parsing, language modeling,named-entity detection and machine translationhave become increasingly data-driven and empiri-cal.
Models for these tasks can be trained to cap-ture the distributions of phenomena in the dataresulting in improved robustness and adaptabil-ity.
However, this trend has yet to significantlyimpact approaches to dialog management in dia-log systems.
Dialog managers (both plan-basedand call-flow based, for example (Di Fabbrizio andLewis, 2004; Larsson et al, 1999)) have tradition-ally been hand-crafted and consequently some-what brittle and rigid.
With the ability to record,store and process large numbers of human-humandialogs (e.g.
from call centers), we anticipatethat data-driven methods will increasingly influ-ence approaches to dialog management.A successful dialog system relies on the syn-ergistic working of several components: speechrecognition (ASR), spoken language understand-ing (SLU), dialog management (DM), languagegeneration (LG) and text-to-speech synthesis(TTS).
While data-driven approaches to ASR andSLU are prevalent, such approaches to DM, LGand TTS are much less well-developed.
In on-going work, we are investigating data-driven ap-proaches for building all components of spokendialog systems.In this paper, we address one aspect of this prob-lem ?
inferring predictive models to structure task-oriented dialogs.
We view this problem as a firststep in predicting the system state of a dialog man-ager and in predicting the system utterance duringan incremental execution of a dialog.
In particular,we learn models for predicting dialog acts of ut-terances, and models for predicting subtask struc-tures of dialogs.
We use three different dialog acttag sets for three different human-human dialogcorpora.
We compare a flat chunk-based modelto a hierarchical parse-based model as models forpredicting the task structure of dialogs.The outline of this paper is as follows: In Sec-tion 2, we review current approaches to buildingdialog systems.
In Section 3, we review relatedwork in data-driven dialog modeling.
In Section 4,we present our view of analyzing the structure oftask-oriented human-human dialogs.
In Section 5,we discuss the problem of segmenting and label-ing dialog structure and building models for pre-dicting these labels.
In Section 6, we report ex-perimental results on Maptask, Switchboard and adialog data collection from a catalog ordering ser-vice domain.2 Current Methodology for BuildingDialog systemsCurrent approaches to building dialog systemsinvolve several manual steps and careful craft-ing of different modules for a particular domainor application.
The process starts with a smallscale ?Wizard-of-Oz?
data collection where sub-jects talk to a machine driven by a human ?behindthe curtains?.
A user experience (UE) engineer an-alyzes the collected dialogs, subject matter expertinterviews, user testimonials and other evidences(e.g.
customer care history records).
This hetero-geneous set of information helps the UE engineerto design some system functionalities, mainly: the201semantic scope (e.g.
call-types in the case of callrouting systems), the LG model, and the DM strat-egy.
A larger automated data collection follows,and the collected data is transcribed and labeled byexpert labelers following the UE engineer recom-mendations.
Finally, the transcribed and labeleddata is used to train both the ASR and the SLU.This approach has proven itself in many com-mercial dialog systems.
However, the initial UErequirements phase is an expensive and error-prone process because it involves non-trivial de-sign decisions that can only be evaluated after sys-tem deployment.
Moreover, scalability is compro-mised by the time, cost and high level of UE know-how needed to reach a consistent design.The process of building speech-enabled auto-mated contact center services has been formalizedand cast into a scalable commercial environmentin which dialog components developed for differ-ent applications are reused and adapted (Gilbertet al, 2005).
However, we still believe that ex-ploiting dialog data to train/adapt or complementhand-crafted components will be vital for robustand adaptable spoken dialog systems.3 Related WorkIn this paper, we discuss methods for automati-cally creating models of dialog structure using di-alog act and task/subtask information.
Relevantrelated work includes research on automatic dia-log act tagging and stochastic dialog management,and on building hierarchical models of plans usingtask/subtask information.There has been considerable research on statis-tical dialog act tagging (Core, 1998; Jurafsky etal., 1998; Poesio and Mikheev, 1998; Samuel etal., 1998; Stolcke et al, 2000; Hastie et al, 2002).Several disambiguation methods (n-gram models,hidden Markov models, maximum entropy mod-els) that include a variety of features (cue phrases,speaker ID, word n-grams, prosodic features, syn-tactic features, dialog history) have been used.
Inthis paper, we show that use of extended contextgives improved results for this task.Approaches to dialog management includeAI-style plan recognition-based approaches (e.g.
(Sidner, 1985; Litman and Allen, 1987; Richand Sidner, 1997; Carberry, 2001; Bohus andRudnicky, 2003)) and information state-based ap-proaches (e.g.
(Larsson et al, 1999; Bos et al,2003; Lemon and Gruenstein, 2004)).
In recentyears, there has been considerable research onhow to automatically learn models of both typesfrom data.
Researchers who treat dialog as a se-quence of information states have used reinforce-ment learning and/or Markov decision processesto build stochastic models for dialog managementthat are evaluated by means of dialog simulations(Levin and Pieraccini, 1997; Scheffler and Young,2002; Singh et al, 2002; Williams et al, 2005;Henderson et al, 2005; Frampton and Lemon,2005).
Most recently, Henderson et al showedthat it is possible to automatically learn good dia-log management strategies from automatically la-beled data over a large potential space of dialogstates (Henderson et al, 2005); and Frampton andLemon showed that the use of context informa-tion (the user?s last dialog act) can improve theperformance of learned strategies (Frampton andLemon, 2005).
In this paper, we combine the useof automatically labeled data and extended contextfor automatic dialog modeling.Other researchers have looked at probabilisticmodels for plan recognition such as extensions ofHidden Markov Models (Bui, 2003) and proba-bilistic context-free grammars (Alexandersson andReithinger, 1997; Pynadath and Wellman, 2000).In this paper, we compare hierarchical grammar-style and flat chunking-style models of dialog.In recent research, Hardy (2004) used a largecorpus of transcribed and annotated telephoneconversations to develop the Amities dialog sys-tem.
For their dialog manager, they trained sepa-rate task and dialog act classifiers on this corpus.For task identification they report an accuracy of85% (true task is one of the top 2 results returnedby the classifier); for dialog act tagging they report86% accuracy.4 Structural Analysis of a DialogWe consider a task-oriented dialog to be the re-sult of incremental creation of a shared plan bythe participants (Lochbaum, 1998).
The sharedplan is represented as a single tree that encap-sulates the task structure (dominance and prece-dence relations among tasks), dialog act structure(sequences of dialog acts), and linguistic structureof utterances (inter-clausal relations and predicate-argument relations within a clause), as illustratedin Figure 1.
As the dialog proceeds, an utterancefrom a participant is accommodated into the tree inan incremental manner, much like an incrementalsyntactic parser accommodates the next word intoa partial parse tree (Alexandersson and Reithinger,1997).
With this model, we can tightly couplelanguage understanding and dialog managementusing a shared representation, which leads to im-proved accuracy (Taylor et al, 1998).In order to infer models for predicting the struc-ture of task-oriented dialogs, we label human-human dialogs with the hierarchical informationshown in Figure 1 in several stages: utterancesegmentation (Section 4.1), syntactic annotation(Section 4.2), dialog act tagging (Section 4.3) and202subtask labeling (Section 5).DialogTaskTopic/SubtaskTopic/SubtaskTask TaskClauseUtteranceUtteranceUtteranceTopic/SubtaskDialogAct,Pred?Args DialogAct,Pred?Args DialogAct,Pred?ArgsFigure 1: Structural analysis of a dialog4.1 Utterance SegmentationThe task of ?cleaning up?
spoken language utter-ances by detecting and removing speech repairsand dysfluencies and identifying sentence bound-aries has been a focus of spoken language parsingresearch for several years (e.g.
(Bear et al, 1992;Seneff, 1992; Shriberg et al, 2000; Charniak andJohnson, 2001)).
We use a system that segmentsthe ASR output of a user?s utterance into clauses.The system annotates an utterance for sentenceboundaries, restarts and repairs, and identifiescoordinating conjunctions, filled pauses and dis-course markers.
These annotations are done usinga cascade of classifiers, details of which are de-scribed in (Bangalore and Gupta, 2004).4.2 Syntactic AnnotationWe automatically annotate a user?s utterance withsupertags (Bangalore and Joshi, 1999).
Supertagsencapsulate predicate-argument information in alocal structure.
They are composed with eachother using the substitution and adjunction oper-ations of Tree-Adjoining Grammars (Joshi, 1987)to derive a dependency analysis of an utteranceand its predicate-argument structure.4.3 Dialog Act TaggingWe use a domain-specific dialog act tag-ging scheme based on an adapted version ofDAMSL (Core, 1998).
The DAMSL scheme isquite comprehensive, but as others have also found(Jurafsky et al, 1998), the multi-dimensionalityof the scheme makes the building of models fromDAMSL-tagged data complex.
Furthermore, thegenerality of the DAMSL tags reduces their util-ity for natural language generation.
Other taggingschemes, such as the Maptask scheme (Carletta etal., 1997), are also too general for our purposes.We were particularly concerned with obtainingsufficient discriminatory power between differenttypes of statement (for generation), and to includean out-of-domain tag (for interpretation).
We pro-vide a sample list of our dialog act tags in Table 2.Our experiments in automatic dialog act taggingare described in Section 6.3.5 Modeling Subtask StructureFigure 2 shows the task structure for a sample di-alog in our domain (catalog ordering).
An orderplacement task is typically composed of the se-quence of subtasks opening, contact-information,order-item, related-offers, summary.
Subtasks canbe nested; the nesting structure can be as deep asfive levels.
Most often the nesting is at the left orright frontier of the subtask tree.OpeningOrder PlacementContact InfoDelivery InfoShipping InfoClosingSummaryPayment InfoOrder ItemFigure 2: A sample task structure in our applica-tion domain.Contact Info Order Item Payment Info Summary ClosingShipping Info Delivery InfoOpeningFigure 3: An example output of the chunk model?stask structureThe goal of subtask segmentation is to predict ifthe current utterance in the dialog is part of the cur-rent subtask or starts a new subtask.
We comparetwo models for recovering the subtask structure?
a chunk-based model and a parse-based model.In the chunk-based model, we recover the prece-dence relations (sequence) of the subtasks but notdominance relations (subtask structure) among thesubtasks.
Figure 3 shows a sample output from thechunk model.
In the parse model, we recover thecomplete task structure from the sequence of ut-terances as shown in Figure 2.
Here, we describeour two models.
We present our experiments onsubtask segmentation and labeling in Section 6.4.5.1 Chunk-based modelThis model is similar to the second one describedin (Poesio and Mikheev, 1998), except that weuse tasks and subtasks rather than dialog games.We model the prediction problem as a classifica-tion task as follows: given a sequence of utter-ances   in a dialog   	 			  and a203subtask label vocabulary  ffflfi , we needto predict the best subtask label sequence ffi "!
	#			%$ as shown in equation 1.&('*)+-,/.10/23,/45	6 798&:'ff; <*= (1)Each subtask has begin, middle (possibly ab-sent) and end utterances.
If we incorporate thisinformation, the refined vocabulary of subtask la-bels is ff"> @?
BA   $  /BCED  -FffflG .
Inour experiments, we use a classifier to assign toeach utterance a refined subtask label conditionedon a vector of local contextual features ( H ).
Inthe interest of using an incremental left-to-rightdecoder, we restrict the contextual features to befrom the preceding context only.
Furthermore, thesearch is limited to the label sequences that re-spect precedence among the refined labels (beginImiddle I end).
This constraint is expressedin a grammar G encoded as a regular expression( JKML fi ON/BA$fi!
/BCfi!
).
However, in orderto cope with the prediction errors of the classifier,we approximate J3ML fi with an P -gram languagemodel on sequences of the refined tag labels:&:')Q+R,/.S0/2K,45	61T5	6ST1U	VWYX[Z798&('Q; <*= (2)\,/.S0/2K,45	6T5	6ST1U	VWYX[Z]^`_798baBc_; dff= (3)In order to estimate the conditional distributioneDHfi we use the general technique of choos-ing the maximum entropy (maxent) distributionthat properly estimates the average of each featureover the training data (Berger et al, 1996).
Thiscan be written as a Gibbs distribution parameter-ized with weights f , where g is the size of thelabel set.
Thus,798ba%c_; dff=+ h`i1jlkbmon pqsrtvuxw:yhi1jlk%n p(4)We use the machine learning toolkitLLAMA (Haffner, 2006) to estimate the con-ditional distribution using maxent.
LLAMAencodes multiclass maxent as binary maxent, inorder to increase the speed of training and to scalethis method to large data sets.
Each of the gclasses in the set z{> is encoded as a bit vectorsuch that, in the vector for class | , the |B}~ bit is oneand all other bits are zero.
Then, g one-vs-otherbinary classifiers are used as follows.798x?
; ??=?+????798???
; ??=+hi1?
?n ?hi?n ?9?hi?
?n ?+??
?h	?iS?
?n ?
(5)where f???
is the parameter vector for the anti-label ??
and f???
 f ???
f ??
.
In order to computeeDHfi , we use class independence assumptionand require that ?
 ??
and for all ??
? | ???
??
.798ba%c_; ?
?=+798x?_; ??=r^?`?w_798x??
; ?
?=5.2 Parse-based ModelAs seen in Figure 3, the chunk model doesnot capture dominance relations among subtasks,which are important for resolving anaphoric refer-ences (Grosz and Sidner, 1986).
Also, the chunkmodel is representationally inadequate for center-embedded nestings of subtasks, which do occurin our domain, although less frequently than themore prevalent ?tail-recursive?
structures.In this model, we are interested in finding themost likely plan tree ( e  ) given the sequence ofutterances:7')+-,/.S0/2K,/4?6 798?7'z; <*= (6)For real-time dialog management we use a top-down incremental parser that incorporates bottom-up information (Roark, 2001).We rewrite equation (6) to exploit the subtasksequence provided by the chunk model as shownin Equation 7.
For the purpose of this paper, weapproximate Equation 7 using one-best (or k-best)chunk output.17'*)?+ ,/.S0/2K,4?6 ?5	6798&('ff; <*=798?7'ff; &('?= (7)\,/.S0/2K,4?6 798?7'ff; &(')= (8)where&(')+-,/.S0/2K,/45/6 798&:'ff; <*= (9)6 Experiments and ResultsIn this section, we present the results of our exper-iments for modeling subtask structure.6.1 DataAs our primary data set, we used 915 telephone-based customer-agent dialogs related to the taskof ordering products from a catalog.
Each dia-log was transcribed by hand; all numbers (tele-phone, credit card, etc.)
were removed for pri-vacy reasons.
The average dialog lasted for 3.711However, it is conceivable to parse the multiple hypothe-ses of chunks (encoded as a weighted lattice) produced by thechunk model.204minutes and included 61.45 changes of speaker.
Asingle customer-service representative might par-ticipate in several dialogs, but customers are rep-resented by only one dialog each.
Although themajority of the dialogs were on-topic, some wereidiosyncratic, including: requests for order cor-rections, transfers to customer service, incorrectlydialed numbers, and long friendly out-of-domainasides.
Annotations applied to these dialogs in-clude: utterance segmentation (Section 4.1), syn-tactic annotation (Section 4.2), dialog act tag-ging (Section 4.3) and subtask segmentation (Sec-tion 5).
The former two annotations are domain-independent while the latter are domain-specific.6.2 FeaturesOffline natural language processing systems, suchas part-of-speech taggers and chunkers, rely onboth static and dynamic features.
Static featuresare derived from the local context of the text be-ing tagged.
Dynamic features are computed basedon previous predictions.
The use of dynamic fea-tures usually requires a search for the globally op-timal sequence, which is not possible when doingincremental processing.
For dialog act tagging andsubtask segmentation during dialog management,we need to predict incrementally since it wouldbe unrealistic to wait for the entire dialog beforedecoding.
Thus, in order to train the dialog act(DA) and subtask segmentation classifiers, we useonly static features from the current and left con-text as shown in Table 1.2 This obviates the needfor constructing a search network and performinga dynamic programming search during decoding.In lieu of the dynamic context, we use larger staticcontext to compute features ?
word trigrams andtrigrams of words annotated with supertags com-puted from up to three previous utterances.Label Type FeaturesDialog Speaker, word trigrams fromActs current/previous utterance(s)supertagged utteranceSubtask Speaker, word trigrams from currentutterance, previous utterance(s)/turnTable 1: Features used for the classifiers.6.3 Dialog Act LabelingFor dialog act labeling, we built models fromour corpus and from the Maptask (Carletta et al,1997) and Switchboard-DAMSL (Jurafsky et al,1998) corpora.
From the files for the Maptask cor-pus, we extracted the moves, words and speakerinformation (follower/giver).
Instead of using the2We could use dynamic contexts as well and adopt agreedy decoding algorithm instead of a viterbi search.
Wehave not explored this approach in this paper.raw move information, we augmented each movewith speaker information, so that for example,the instruct move was split into instruct-giver andinstruct-follower.
For the Switchboard corpus, weclustered the original labels, removing most ofthe multidimensional tags and combining togethertags with minimum training data as described in(Jurafsky et al, 1998).
For all three corpora, non-sentence elements (e.g., dysfluencies, discoursemarkers, etc.)
and restarts (with and without re-pairs) were kept; non-verbal content (e.g., laughs,background noise, etc.)
was removed.As mentioned in Section 4, we use a domain-specific tag set containing 67 dialog act tags forthe catalog corpus.
In Table 2, we give examplesof our tags.
We manually annotated 1864 clausesfrom 20 dialogs selected at random from our cor-pus and used a ten-fold cross-validation schemefor testing.
In our annotation, a single utterancemay have multiple dialog act labels.
For our ex-periments with the Switchboard-DAMSL corpus,we used 42 dialog act tags obtained by clusteringover the 375 unique tags in the data.
This cor-pus has 1155 dialogs and 218,898 utterances; 173dialogs, selected at random, were used for testing.The Maptask tagging scheme has 12 unique dialogact tags; augmented with speaker information, weget 24 tags.
This corpus has 128 dialogs and 26181utterances; ten-fold cross validation was used fortesting.Type SubtypeAsk InfoExplain Catalog, CC Related, Discount, Order InfoOrder Problem, Payment Rel, Product InfoPromotions, Related Offer, ShippingConvers- Ack, Goodbye, Hello, Help, Hold,-ational YoureWelcome, Thanks, Yes, No, Ack,Repeat, Not(Information)Request Code, Order Problem, Address, Catalog,CC Related, Change Order, Conf, Credit,Customer Info, Info, Make Order, Name,Order Info, Order Status, Payment Rel,Phone Number, Product Info, Promotions,Shipping, Store InfoYNQ Address, Email, Info, Order Info,Order Status,Promotions, Related OfferTable 2: Sample set of dialog act labelsTable 3 shows the error rates for automatic dia-log act labeling using word trigram features fromthe current and previous utterance.
We compareerror rates for our tag set to those of Switchboard-DAMSL and Maptask using the same features andthe same classifier learner.
The error rates for thecatalog and the Maptask corpus are an averageof ten-fold cross-validation.
We suspect that thelarger error rate for our domain compared to Map-task and Switchboard might be due to the smallsize of our annotated corpus (about 2K utterancesfor our domain as against about 20K utterances for205Maptask and 200K utterances for DAMSL).The error rates for the Switchboard-DAMSLdata are significantly better than previously pub-lished results (28% error rate) (Jurafsky et al,1998) with the same tag set.
This improvementis attributable to the richer feature set we use and adiscriminative modeling framework that supportsa large number of features, in contrast to the gener-ative model used in (Jurafsky et al, 1998).
A sim-ilar obeservation applies to the results on Maptaskdialog act tagging.
Our model outperforms previ-ously published results (42.8% error rate) (Poesioand Mikheev, 1998).In labeling the Switchboard data, long utter-ances were split into slash units (Meteer et.al.,1995).
A speaker?s turn can be divided in one ormore slash units and a slash unit can extend overmultiple turns, for example:sv B.64 utt3: C but, F uh ?b A.65 utt1: Uh-huh.
/+ B.66 utt1: ?
people want all of that /sv B.66 utt2: C and not all of those are necessities.
/b A.67 utt1: Right .
/The labelers were instructed to label on the ba-sis of the whole slash unit.
This makes, for ex-ample, the dysfluency turn B.64 a Statement opin-ion (sv) rather than a non-verbal.
For the pur-pose of discriminative learning, this could intro-duce noisy data since the context associated to thelabeling decision shows later in the dialog.
To ad-dress this issue, we compare 2 classifiers: the first(non-merged), simply propagates the same labelto each continuation, cross turn slash unit; the sec-ond (merged) combines the units in one single ut-terance.
Although the merged classifier breaks theregular structure of the dialog, the results in Table3 show better overall performance.Tagset current + stagged + 3 previousutterance utterance (stagged)utteranceCatalog 46.3 46.1 42.2DomainDAMSL 24.7 23.8 19.1(non-merged)DAMSL 22.0 20.6 16.5(merged)Maptask 34.3 33.9 30.3Table 3: Error rates in dialog act tagging6.4 Subtask Segmentation and LabelingFor subtask labeling, we used a random partitionof 864 dialogs from our catalog domain as thetraining set and 51 dialogs as the test set.
Allthe dialogs were annotated with subtask labels byhand.
We used a set of 18 labels grouped as shownin Figure 4.Type Subtask Labels1 opening, closing2 contact-information, delivery-information,payment-information, shipping-address,summary3 order-item, related-offer, order-problemdiscount, order-change, check-availability4 call-forward, out-of-domain, misc-other, sub-callTable 4: Subtask label set6.4.1 Chunk-based ModelTable 5 shows error rates on the test set whenpredicting refined subtask labels using word P -gram features computed on different dialog con-texts.
The well-formedness constraint on the re-fined subtask labels significantly improves predic-tion accuracy.
Utterance context is also very help-ful; just one utterance of left-hand context leads toa 10% absolute reduction in error rate, with fur-ther reductions for additional context.
While theuse of trigram features helps, it is not as helpful asother contextual information.
We used the dialogact tagger trained from Switchboard-DAMSL cor-pus to automatically annotate the catalog domainutterances.
We included these tags as features forthe classifier, however, we did not see an improve-ment in the error rates, probably due to the higherror rate of the dialog act tagger.Feature Utterance ContextContextCurrent +prev +three prevutt/with DA utt/with DA utt/with DAUnigram 42.9/42.4 33.6/34.1 30.0/30.3(53.4/52.8) (43.0/43.0) (37.6/37.6)Trigram 41.7/41.7 31.6/31.4 30.0/29.1(52.5/52.0) (42.9/42.7) (37.6/37.4)Table 5: Error rate for predicting the refined sub-task labels.
The error rates without the well-formedness constraint is shown in parenthesis.The error rates with dialog acts as features are sep-arated by a slash.6.4.2 Parsing-based ModelWe retrained a top-down incrementalparser (Roark, 2001) on the plan trees in thetraining dialogs.
For the test dialogs, we usedthe ?
-best (k=50) refined subtask labels for eachutterance as predicted by the chunk-based classi-fier to create a lattice of subtask label sequences.For each dialog we then created P -best sequences(100-best for these experiments) of subtask labels;these were parsed and (re-)ranked by the parser.3We combine the weights of the subtask labelsequences assigned by the classifier with the parsescore assigned by the parser and select the top3Ideally, we would have parsed the subtask label latticedirectly, however, the parser has to be reimplemented to parsesuch lattice inputs.206Features ConstraintsNo Constraint Sequence Constraint Parser ConstraintCurrent Utt 54.4 42.0 41.5+ DA 53.8 40.5 40.2Current+Prev Utt 41.6 27.7 27.7+DA 40.0 28.8 28.1Current+3 Prev Utt 37.5 24.7 24.7+DA 39.7 29.6 28.9Table 6: Error rates for task structure prediction, with no constraints, sequence constraints and parserconstraintsscoring sequence from the list for each dialog.The results are shown in Table 6.
It can be seenthat using the parsing constraint does not help thesubtask label sequence prediction significantly.The chunk-based model gives almost the sameaccuracy, and is incremental and more efficient.7 DiscussionThe experiments reported in this section have beenperformed on transcribed speech.
The audio forthese dialogs, collected at a call center, were storedin a compressed format, so the speech recognitionerror rate is high.
In future work, we will assessthe performance of dialog structure prediction onrecognized speech.The research presented in this paper is but onestep, albeit a crucial one, towards achieving thegoal of inducing human-machine dialog systemsusing human-human dialogs.
Dialog structure in-formation is necessary for language generation(predicting the agents?
response) and dialog statespecific text-to-speech synthesis.
However, thereare several challenging problems that remain to beaddressed.The structuring of dialogs has another applica-tion in call center analytics.
It is routine practice tomonitor, analyze and mine call center data basedon indicators such as the average length of dialogs,the task completion rate in order to estimate the ef-ficiency of a call center.
By incorporating structureto the dialogs, as presented in this paper, the anal-ysis of dialogs can be performed at a more fine-grained (task and subtask) level.8 ConclusionsIn order to build a dialog manager using a data-driven approach, the following are necessary: amodel for labeling/interpreting the user?s currentaction; a model for identifying the current sub-task/topic; and a model for predicting what thesystem?s next action should be.
Prior research inplan identification and in dialog act labeling hasidentified possible features for use in such models,but has not looked at the performance of differentfeature sets (reflecting different amounts of con-text and different views of dialog) across differentdomains (label sets).
In this paper, we comparedthe performance of a dialog act labeler/predictoracross three different tag sets: one using very de-tailed, domain-specific dialog acts usable for inter-pretation and generation; and two using general-purpose dialog acts and corpora available to thelarger research community.
We then comparedtwo models for subtask labeling: a flat, chunk-based model and a hierarchical, parsing-basedmodel.
Findings include that simpler chunk-basedmodels perform as well as hierarchical models forsubtask labeling and that a dialog act feature is nothelpful for subtask labeling.In on-going work, we are using our best per-forming models for both DM and LG components(to predict the next dialog move(s), and to selectthe next system utterance).
In future work, we willaddress the use of data-driven dialog managementto improve SLU.9 AcknowledgmentsWe thank Barbara Hollister and her team for theireffort in annotating the dialogs for dialog acts andsubtask structure.
We thank Patrick Haffner forproviding us with the LLAMA machine learningtoolkit and Brian Roark for providing us with histop-down parser used in our experiments.
We alsothank Alistair Conkie, Mazin Gilbert, NarendraGupta, and Benjamin Stern for discussions duringthe course of this work.ReferencesJ.
Alexandersson and N. Reithinger.
1997.
Learning dia-logue structures from a corpus.
In Proceedings of Eu-rospeech?97.S.
Bangalore and N. Gupta.
2004.
Extracting clauses in di-alogue corpora : Application to spoken language under-standing.
Journal Traitement Automatique des Langues(TAL), 45(2).S.
Bangalore and A. K. Joshi.
1999.
Supertagging: Anapproach to almost parsing.
Computational Linguistics,25(2).J.
Bear et al 1992.
Integrating multiple knowledge sourcesfor detection and correction of repairs in human-computerdialog.
In Proceedings of ACL?92.207A.
Berger, S.D.
Pietra, and V.D.
Pietra.
1996.
A MaximumEntropy Approach to Natural Language Processing.
Com-putational Linguistics, 22(1):39?71.D.
Bohus and A. Rudnicky.
2003.
RavenClaw: Dialog man-agement using hierarchical task decomposition and an ex-pectation agenda.
In Proceedings of Eurospeech?03.J.
Bos et al 2003.
DIPPER: Description and formalisation ofan information-state update dialogue system architecture.In Proceedings of SIGdial.H.H.
Bui.
2003.
A general model for online probabalisticplan recognition.
In Proceedings of IJCAI?03.S.
Carberry.
2001.
Techniques for plan recognition.
UserModeling and User-Adapted Interaction, 11(1?2).J.
Carletta et al 1997.
The reliability of a dialog structurecoding scheme.
Computational Linguistics, 23(1).E.
Charniak and M. Johnson.
2001.
Edit detection and pars-ing for transcribed speech.
In Proceedings of NAACL?01.M.
Core.
1998.
Analyzing and predicting patterns ofDAMSL utterance tags.
In Proceedings of the AAAIspring symposium on Applying machine learning to dis-course processing.M.
Meteer et.al.
1995.
Dysfluency annotation stylebook forthe switchboard corpus.
Distributed by LDC.G.
Di Fabbrizio and C. Lewis.
2004.
Florence: a dialoguemanager framework for spoken dialogue systems.
In IC-SLP 2004, 8th International Conference on Spoken Lan-guage Processing, Jeju, Jeju Island, Korea, October 4-8.M.
Frampton and O.
Lemon.
2005.
Reinforcement learningof dialogue strategies using the user?s last dialogue act.
InProceedings of the 4th IJCAI workshop on knowledge andreasoning in practical dialogue systems.M.
Gilbert et al 2005.
Intelligent virtual agents for con-tact center automation.
IEEE Signal Processing Maga-zine, 22(5), September.B.J.
Grosz and C.L.
Sidner.
1986.
Attention, intentions andthe structure of discoursep.
Computational Linguistics,12(3).P.
Haffner.
2006.
Scaling large margin classifiers for spokenlanguage understanding.
Speech Communication, 48(4).H.
Hardy et al 2004.
Data-driven strategies for an automateddialogue system.
In Proceedings of ACL?04.H.
Wright Hastie et al 2002.
Automatically predicting dia-logue structure using prosodic features.
Speech Commu-nication, 36(1?2).J.
Henderson et al 2005.
Hybrid reinforcement/supervisedlearning for dialogue policies from COMMUNICATORdata.
In Proceedings of the 4th IJCAI workshop on knowl-edge and reasoning in practical dialogue systems.A.
K. Joshi.
1987.
An introduction to tree adjoining gram-mars.
In A. Manaster-Ramer, editor, Mathematics of Lan-guage.
John Benjamins, Amsterdam.D.
Jurafsky et al 1998.
Switchboard discourse languagemodeling project report.
Technical Report Research Note30, Center for Speech and Language Processing, JohnsHopkins University, Baltimore, MD.S.
Larsson et al 1999.
TrindiKit manual.
Technical report,TRINDI Deliverable D2.2.O.
Lemon and A. Gruenstein.
2004.
Multithreaded con-text for robust conversational interfaces: Context-sensitivespeech recognition and interpretation of corrective frag-ments.
ACM Transactions on Computer-Human Interac-tion, 11(3).E.
Levin and R. Pieraccini.
1997.
A stochastic model ofcomputer-human interaction for learning dialogue strate-gies.
In Proceedings of Eurospeech?97.D.
Litman and J. Allen.
1987.
A plan recognition model forsubdialogs in conversations.
Cognitive Science, 11(2).K.
Lochbaum.
1998.
A collaborative planning model of in-tentional structure.
Computational Linguistics, 24(4).M.
Poesio and A. Mikheev.
1998.
The predictive power ofgame structure in dialogue act recognition: experimentalresults using maximum entropy estimation.
In Proceed-ings of ICSLP?98.D.V.
Pynadath and M.P.
Wellman.
2000.
Probabilistic state-dependent grammars for plan recognition.
In In Proceed-ings of the 16th Conference on Uncertainty in ArtificialIntelligence (UAI-2000).C.
Rich and C.L.
Sidner.
1997.
COLLAGEN: When agentscollaborate with people.
In Proceedings of the First Inter-national Conference on Autonomous Agents (Agents?97).B.
Roark.
2001.
Probabilistic top-down parsing and lan-guage modeling.
Computational Linguistics, 27(2).K.
Samuel et al 1998.
Computing dialogue acts from fea-tures with transformation-based learning.
In Proceedingsof the AAAI spring symposium on Applying machine learn-ing to discourse processing.K.
Scheffler and S. Young.
2002.
Automatic learning of di-alogue strategy using dialogue simulation and reinforce-ment learning.
In Proceedings of HLT?02.S.
Seneff.
1992.
A relaxation method for understandingspontaneous speech utterances.
In Proceedings of theSpeech and Natural Language Workshop, San Mateo, CA.E.
Shriberg et al 2000.
Prosody-based automatic segmenta-tion of speech into sentences and topics.
Speech Commu-nication, 32, September.C.L.
Sidner.
1985.
Plan parsing for intended response recog-nition in discourse.
Computational Intelligence, 1(1).S.
Singh et al 2002.
Optimizing dialogue management withreinforcement learning: Experiments with the NJFun sys-tem.
Journal of Artificial Intelligence Research, 16.A.
Stolcke et al 2000.
Dialogue act modeling for automatictagging and recognition of conversational speech.
Com-putational Linguistics, 26(3).P.
Taylor et al 1998.
Intonation and dialogue context asconstraints for speech recognition.
Language and Speech,41(3).J.
Williams et al 2005.
Partially observable Markov deci-sion processes with continuous observations for dialoguemanagement.
In Proceedings of SIGdial.208
