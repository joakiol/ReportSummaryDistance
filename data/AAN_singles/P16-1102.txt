Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1075?1084,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsOff-topic Response Detection for Spontaneous Spoken EnglishAssessmentAndrey Malinin, Rogier C. Van Dalen, Yu Wang, Kate M. Knill, Mark J. F. GalesUniversity of Cambridge, Department of EngineeringTrumpington St, Cambridge CB2 1PZ, UK{am969, yw396, kate.knill, mjfg}@eng.cam.ac.ukAbstractAutomatic spoken language assessmentsystems are becoming increasingly impor-tant to meet the demand for English sec-ond language learning.
This is a challeng-ing task due to the high error rates of, evenstate-of-the-art, non-native speech recog-nition.
Consequently current systems pri-marily assess fluency and pronunciation.However, content assessment is essentialfor full automation.
As a first stage it isimportant to judge whether the speaker re-sponds on topic to test questions designedto elicit spontaneous speech.
Standard ap-proaches to off-topic response detectionassess similarity between the response andquestion based on bag-of-words represen-tations.
An alternative framework basedon Recurrent Neural Network LanguageModels (RNNLM) is proposed in this pa-per.
The RNNLM is adapted to the topicof each test question.
It learns to asso-ciate example responses to questions withpoints in a topic space constructed usingthese example responses.
Classificationis done by ranking the topic-conditionalposterior probabilities of a response.
TheRNNLMs associate a broad range of re-sponses with each topic, incorporate se-quence information and scale better withadditional training data, unlike standardmethods.
On experiments conducted ondata from the Business Language TestingService (BULATS) this approach outper-forms standard approaches.1 IntroductionAs English has become the global lingua franca,there is growing demand worldwide for assess-ment of English as a second language (Seidlhofer,2005).
To assess spoken communication, sponta-neous speech is typically elicited through a seriesof questions such as ?describe the photo?
or ?plana meeting?.
Grades are awarded based on a candi-date?s responses.Automatic assessment systems are becoming at-tractive as they allow second language assessmentprogrammes to economically scale their opera-tions while decreasing throughput time and pro-vide testing on demand.
Features for automaticgraders are derived from the audio and from hy-potheses produced by automatic speech recogni-tion (ASR) systems.
The latter is highly errorfuldue to the large variability in the input speech;disfluencies common to spontaneous speech, non-native accents and pronunciations.
Current sys-tems, such as ETS?
SpeechRater (Zechner et al,2009) and Pearson?s AZELLA (Metallinou andCheng, 2014), primarily assess pronunciation andfluency.
Although these are clearly indicative ofspoken language ability, full assessment of spo-ken communication requires judgement of high-level content and communication skills, such as re-sponse construction and relevance.
The first stageof this is to assess whether the responses are off-topic, that is, has the candidate misunderstood thequestion and/or memorised a response.While there has been little work done on de-tecting off-topic responses for spoken languageassessment, detection of off-topic responses andcontent assessment has been studied for essay as-sessment.
One approach for essay content as-sessment uses features based on semantic similar-ity metrics between vector space representationsof responses.
Common vector representations in-clude lexical Vector Space Models and Latent Se-mantic Analysis (LSA) (Yannakoudakis, 2013).This approach was first applied to spoken assess-1075ment in (Xie et al, 2012) and then in (Evanini etal., 2013).
Following this, (Yoon and Xie, 2014)investigated the detection of responses for whichan automatic assessment system will have diffi-culty in assigning a valid score, of which off-topic responses are a specific type.
A decisiontree classifier is used with features based on co-sine similarity between a test response and tf-idfvectors of both aggregate example responses andquestions, as well as pronunciation and fluency.In (Evanini and Wang, 2014) text reuse and pla-giarism in spoken responses are detected using adecision tree classifier based on vector similarityand lexical matching features which compare a re-sponse to a set of example ?source texts?
.
Thistask is similar to off-topic response detection inthat it is based on comparing a test response toexample responses.
Thus, a standard approachto off-topic response detection would be based onmeasuring the similarity between vector represen-tations of a spoken response and the test ques-tion.
A major deficiency of this approach is thatit is based on bag-of-words vector representations,which loses information about the sequential na-ture of speech, which is important to evaluatingresponse construction and relevance.
Addition-ally, adapting the approach to model a range ofresponses for each topic causes classification timeto scale poorly with training data size and the num-ber of questions.To address these issues a general off-topiccontent detection framework based on topicadapted Recurrent Neural Network language mod-els (RNNLM) has been developed and applied tooff-topic response detection for spoken languageassessment.
This framework uses example re-sponses to test questions in training of the lan-guage model and construction of the topic-space.The RNNLM learns to associate the example re-sponses with points in the topic-space.
Classi-fication is done by ranking the topic-conditionalposterior probabilities of a response.
The advan-tage of this approach is that sequence informationcan be taken into account and broad ranges of re-sponses can be associated with each topic with-out affecting classifcation speed.
Two topic vec-tor representations are investigated: Latent Dirich-let Allocation (LDA) (Blei et al, 2003; Griffithsand Steyvers, 2004) and Latent Semantic Analysis(LSA) (Landauer et al, 1998).
They are comparedto standard approaches on data from the Cam-bridge Business English (BULATS) exam.The rest of this paper is structured as follows:Section 2 discusses the RNNLM adaptation andtopic spaces; Section 3 discusses approaches totopic detection; Section 4 presents data sets andexperimental infrastructure; Section 5 analyzesexperimental results; Section 6 concludes the pa-per.2 Topic Adapted RNNLMs2.1 RNNLM ArchitectureA statistical language model is used to model thesemantic and syntactic information in text in theform of a probability distribution over word se-quences.
It assigns a probability to a word se-quence w = {w0, w1, ?
?
?
, wL} as follows:P(wi|wi?1, ?
?
?
, w0) = P(wi|hi?10)P(w) =L?i=1P(wi|hi?10)(1)(2)where w0is the start of sentence symbol <s>.
Inthis work a language model is trained to modelexample responses to questions on a spoken lan-guage assessment test.
P(wi|hi?10) can be es-timated by a number of approaches, most no-tably N-grams and Recurrent Neural Networks(Mikolov et al, 2010).Recurrent Neural Network language models(RNNLMs) (Figure 1) (Mikolov, 2012) are avariable context length language model, capableof representing the entire context efficiently, un-like N-grams.
RNNLMs represent the full un-truncated history hi?10= {wi?1, ?
?
?
, w0} forword wias the hidden layer si?1, a form of short-term memory, whose representation is learnedfrom the data.
Words and phrases are representedin a continuous space, which gives RNNLMsgreater generalization abilities than other languagemodels, such as N-grams.RNNLMs can be adapted by adding a featurevector f which represents information absent fromthe RNN (Mikolov and Zweig, 2012).
In thiswork, the vector representation of a spoken lan-guage test question topic fqis used for the con-text vector f .
Architecturally, a context adaptedRNNLM is described by equations 3-5. e(x) andg(x) are element-wise sigmoid and softmax acti-1076Figure 1: Context adapted RNN language modelvation functions.P(wi|hi?10, f) = PRNN(wi|wi?1, si?2, f)PRNN(wi|wi?1, si?2, f) = g(Vsi?1+ Hf)si?1= e(Uwi?1+ Wsi?2+ Gf)(3)(4)(5)Through the process of adaptation the RNNLMlearns to associate particular types of responseswith particular topics, thereby becoming more dis-criminative.
Thus, a sentence?s topic-conditionalprobability PRNN(w|fq) will be higher if it corre-sponds to the topic q than if it does not.2.2 Example Response Based Topic SpaceIn order for the topic vectors fqto be informa-tive they must span the space of all question top-ics in the test.
Thus a topic space needs to be de-fined.
Example responses, which are necessary totrain the RNNLM, are used to define a topic spacebecause typical responses to a question will bedefinitive of the question?s topic.
Multiple exam-ple responses to a particular question are mergedinto one aggregate response to capture a broadrange of response variations and increase the ro-bustness of the vector representation estimation.By default a topic t is defined for each ques-tion q.
However, multi-part questions are com-mon, where candidates are given a scenario suchas providing tourist information in which indi-vidual questions ask about food, hotels or sights.Since the underlying topic is related this can con-fuse a classifier.
The responses for all these relatedquestions could be merged to form a single aggre-gate vector, but the statistics of the responses toeach question can be sufficiently different that lessdistinct topics are formed.
Instead the aggregrateexample responses for each question are assignedthe same topic label.
Thus, a mapping betweenquestions and topics and its inverse is introduced:M : q ?
tM?1t: {q ?
Q|M(q) = t}(6)(7)A vector representation of a question topic iscomputed using the aggregate example responses.As mentioned in Section 1, two common represen-tations are LDA and LSA; both are investigated inthis work.LDA is a generative model which allows docu-ments to be modelled as distributions over latenttopics z ?
Z.
Each latent topic z is described by amultinomial distribution over words P(wi|z), andeach word in a document is attributed to a particu-lar latent topic (Blei et al, 2003).
Thus, the adap-tation vector fwrepresents a vector of posteriorprobabilities over latent topics for word sequencew:fw= [P(z = 1|w), ?
?
?
, P(z = K|w)]TP(z = k|w) =?Ni=1?
(zwi= k)N(8)(9)LDA was found to perform better for RNNLMadaptation than other representations in (Mikolovand Zweig, 2012; Chen et al, 2015).LSA (Landauer et al, 1998) is a popular repre-sentation for information retrieval tasks.
A word-document matrix F is constructed using exampleresponses and each word is weighted by its termfrequency-inverse document frequency (TF-IDF).Then a low-rank approximation Fkis computedusing Singular Value Decomposition (SVD):Fk= Uk?kVTkFk= [f1, ?
?
?
, fQ]Tfw= ?
?1kUTkftfidf(10)(11)(12)Only the k largest singular values of the singularvalue matrix ?
are kept.
Fkis a representation ofthe data which retains only the k most significantfactors of variation.
Each row is a topic vector fq.New vectors fwfor test responses can be projectedinto the LSA space via equation 12, where ftfidfisthe TF-IDF weighted bag-of-word representationof a test response.10773 Topic Detection and FeaturesThis section discusses the standard, vector similar-ity feature-based, and the proposed topic adaptedRNNLM approaches for topic detection.3.1 Vector Similarity FeaturesThe essence of the standard approach to topic de-tection is to assess the semantic distance Dsembe-tween the test response w and the aggregate exam-ple response wqby approximating it using a vec-tor distance metric Dvecbetween vector represen-tations of the response fwand the topic fq.
Clas-sification is done by selecting the topic closest tothe test response:?tw=M(arg minq{Dsem(w,wq)})Dsem(w,wq) ?
Dvec(fw, fq)(13)(14)The selection of an appropriate distance metricDvec(fw, fq) can have a large effect on the classifi-cation outcome.
A common metric used in topicclassification and information retrieval is cosinesimilarity, which measures the cosine of the an-gle between two vectors.
A distance metric basedon this, cosine distance, can be defined as:Dcos(fw, fq) = 1?fwTfq|fw||fq|(15)While topics are robustly defined, this approachfails to capture the range of responses which canbe given to a question.
A different approach wouldbe to maintain a separate point in this topic spacefor every example response.
This retains the ro-bust topic definition while allowing each topic tobe represented by a cloud of points in topic space,thereby capturing a range of responses which canbe given.
A K-nearest neighbour (KNN) classifiercan be used to detect the response topic by com-puting distances of the test response to each of thetraining points in topic space.
However, classifi-cation may become impractical for large data sets,as the number of response points scales with thesize of the training data.
Low-cost distance mea-sures, such as cosine distance, allow this approachto be used on large data sets before it becomescomputationally infeasible.
This approach is usedas the baseline for comparison with the proposedRNNLM based method.
For multi-part questions,topic vectors relating to the same overall topic aresimply given the same topic label.The classification rate can be improved by tak-ing the top N?tN= {?t1, ?
?
?
,?tN} results intoaccount.
The KNN classifier can be modifiedto yield the N-best classification by removing alltraining points from the 1-best class from the KNNclassifier and re-running the classification to getthe 2-best results, and so on.One of the main deficiencies of methods basedon computing distances between vector represen-tations is that commonly used representations,such as LSA and LDA, ignore word-order in docu-ments, thereby throwing away information whichis potentially useful for topic classification.
Ad-ditionally, if any of the test or example responseutterances are short, then their topic vector repre-sentations may not be robustly estimated.3.2 Topic Adapted RNNLM FrameworkThe RNNLM based approach to topic detectionis based on different principles.
By combiningequations 2 and 3 the log-probability L(q) of aresponse sentence given a particular topic vectorPRNN(w|fq) is computed.
For each response w inthe test set L(q) is computed (equation 16) for alltopic vectors fq.
L(q) is calculated using equa-tion 17 for multi-part questions with responses wpwhere p ?
t. Classification is done by rankinglog-probability L(q) for an utterance w and L(q)for all q ?M?1tare averaged (equation 18).L(q) =????
?log[PRNN(w|fq)]?p1Nplog[PRNN(wp|fq)](16)(17)?tw= arg maxt{1|M?1t|?q?M?1tL(q)} (18)It is trivial to extend this approach to yield the N-best solutions by simply taking the top N outputsof equation 18.The RNNLM approach has several benefits overstandard approaches.
Firstly, this approach explic-itly takes account of word-order in determiningthe topical similarity of the response.
Secondly,there is no need to explicitly select a distance met-ric.
Thirdly, the problems of robustly estimatinga vector representation fwof the test response aresidestepped.
Furthermore, the RNNLM accountsfor a broad range of responses because it is trainedon individual response utterances which it asso-ciates with a question topic vector.
This makes1078it more scalable than the KNN approach becausethe number of comparisons which need to be madescales only with the number of questions, not thesize of the training data.
Thus, arbitrarily largedata sets can be used to train the model withoutaffecting classification time.The RNNLM could be used in a KNN-style ap-proach, where it associates each example responsewith its individual topic vector, using L(q) as adistance metric.
However, this is computationallyinfeasible since computing L(q) is significantlymore expensive than cosine distance and the pre-viously mentioned scalability would be lost.4 Data and Experimental Set-upData from the Business Language Testing Service(BULATS) English tests is used for training andtesting.
At test time, each response is recognisedusing an ASR system and the 1-best hypothesis ispassed to the topic classifier.
The topic detectionsystem decides whether the candidate has spokenoff topic by comparing the classifier output to thetopic of the question being answered.4.1 BULATS Test Format and DataThe BULATS Online Speaking Test has five sec-tions (Chambers and Ingham, 2011):A Candidates respond to eight simple questionsabout themselves and their work (e.g.
what isyour name, where do you come from?
).B Candidates read aloud six short texts appro-priate to a work environment.C Candidates talk about a work-related topic(e.g.
the perfect office) with the help ofprompts which appear on the screen.D Candidates must describe a graph or chartsuch as a pie or a bar chart related to a busi-ness situation (e.g.
company exports).E Candidates are asked to respond to five open-ended questions related to a single contextprompt.
For example a set of five questionsabout organizing a stall at a trade fair.Candidates are given a test consisting of 21 ques-tions, however, only the last three sections, con-sisting of 7 questions, are spontaneously con-structed responses to open ended question, andtherefore of relevance to this work.
Each uniqueset of 7 questions is a question script.Training, development and evaluation data setscomposed of predominantly Gujarati L1 candi-dates are used in these experiments.
The data setsare designed to have an (approximately) even dis-tribution over grades as well as over the differentquestion scripts.During operation the system will detect off-topic responses based on ASR transcriptions, sofor the system to be matched it needs to be trainedon ASR transcriptions as well.
Thus, two train-ing sets are made by using the ASR architec-ture described in section 4.2 to transcribe candi-date responses.
Each training set covers the sameset of 282 unique topics.
The first training setconsists of data from 490 candidates, containing9.9K responses, with an average of 35.1 responsesper topic.
The second, much larger, training setconsists of data from 10004 candidates, contain-ing 202K responses, with an average of 715.5 re-sponses per topic.CharacteristicSectionA B C D E# Unique Topics 18 144 17 18 85# Questions/Section 6 8 1 1 5Av.
# Words/Resp.
10 10 61 77 20Table 1: Data Characteristics.As Table 1 shows, the average response lengthvaries across sections due to the nature of the sec-tions.
Shorter responses to questions are observedfor sections A, B and E, with longer responses toC and D. Estimating topic representations for sec-tions A, B and E questions based on individual re-sponses would be problematic due to the short re-sponse lengths.
However, by aggregating exampleresponses across candidates, as described in sec-tion 2.2, the average length of responses in all sec-tions is significantly longer, allowing the example-response topic space to be robustly defined.Section E topics correspond to topics of sub-questions relating to an overall question, thus thereare only 15 unique questions in section E. How-ever, the sub-questions are sufficiently distinct tomerit their own topic vectors.
At classificationtime confusions between sub-questions of an over-all section E question are not considered mistakes.Held-out test sets are used for development,DEV, and evaluation, EVAL, composed of 84 and223 candidates, respectively.
ASR transcriptionsare used for these test sets, as per the operating1079scenario.
A version of the DEV set with pro-fessionally produced transcriptions, DEV REF, isalso used in training and development.The publicly available Gibbs-LDAtoolkit (Phan and Nguyen, 2007) is used toestimate LDA posterior topic vectors and thescikit-learn 17.0 toolkit (Pedregosa et al, 2011)to estimate LSA topic representations.
The topicadapted RNNLM uses a 100-dimensional hiddenlayer.
DEV REF is used as a validation set forearly stopping to prevent over-fitting.
The CUEDRNNLM toolkit v0.1 (Chen et al, 2016) is usedfor RNNLM training, details of which can befound in (Chen et al, 2014; Mikolov et al, 2010)4.2 ASR SystemA hybrid deep neural network DNN-HMM systemis used for ASR (Wang et al, 2015).
The acousticmodels are trained on 108.6 hours of BULATS testdata (Gujarati L1 candidates) using an extendedversion of the HTK v3.4.1 toolkit (Young et al,2009; Zhang and Woodland, 2015).
A Kneser-Neytrigram LM is trained on 186K words of BULATstest data and interpolated with a general EnglishLM trained on a large broadcast news corpus, us-ing the SRILM toolkit (Stolcke, 2002).
Latticesare re-scored with an interpolated trigram+RNNLM (Mikolov et al, 2010) by applying the 4-gram history approximation described in (Liu etal., 2014), where the RNNLM is trained using theCUED RNNLM toolkit (Chen et al, 2016).
Inter-polation weights are optimized on the DEV REFdata set.
Table 2 shows the word error rate (WER)on the DEV test set relative to the DEV REF ref-erences for each section and the combined sponta-neous speech sections (C-E).% WERA B C D E C-E30.6 23.2 32.0 29.9 32.3 31.5Table 2: ASR performance on DEV.5 ExperimentsTwo forms of experiment are conducted in or-der to assess the performance of the topic-adaptedRNNLM.
First, a topic classification experimentis run where the ability of the system to accuratelyrecognize the topic of a response is evaluated.
Sec-ond, a closed-set off-topic response detection ex-periment is done.In the experimental configuration used here aresponse is classified into a topic and the accuracyis measured.
The topic of the question being an-swered is known and all responses are actually on-topic.
A label (on-topic/off-topic) is given for eachresponse based on the output of the classifier rela-tive to the question topic.
Thus, results presentedare in terms of false rejection (FR) and false accep-tance (FA) rates rather than precision and recall.Initial topic detection experiments were run us-ing the DEV test set with both the referencetranscriptions (REF) and recognition hypotheses(ASR) to compare different KNN and RNN sys-tems.
After this, performance is evaluated on theEVAL test set.
The systems were trained usingdata sets of 490 and 10004 candidates, as de-scribed in section 4.1.5.1 Topic ClassificationPerformance of the topic-adapted RNNLM iscompared to the KNN classifier in Table 3.
TheRNN1 system outperforms the KNN system by20-35 % using the LDA topic representation.
Fur-thermore, the KNN system performs worse on sec-tion E than it does on section C, while RNN1 per-formance is better on section E by 7-10% than onsection C. The LSA topic representation consis-tently yields much better performance than LDAby 25-50% for both systems.
Thus, the LDA rep-resentation is not further investigated in any exper-iments.When using the LSA representation the RNN1system outperforms the KNN system onlymarginally, due to better performance on sectionE.
Additionally, unlike the KNN-LDA system, theKNN-LSA system does not have a performancedegradation on section E relative to section C. No-tably, the RNN1 system performs better on sec-tion E by 5-13% than on section C. Clearly, sec-tion C questions are hardest to assess.
Combiningboth representations through concatenation doesnot effect performance of the KNN system andslightly degrades RNN1 performance on sectionC.
KNN and RNN1 systems with either topic rep-resentation perform comparably on REF and ASR.This suggests that the systems are quite robustto WER rates of 31.5% and the differences aremostly noise.Training the RNN2 system on 20 times as muchdata leads to greatly improved performance overKNN and RNN1 systems, almost halving the over-1080TopicSystem # Cands.C D E ALL (C-E)Repn.
REF ASR REF ASR REF ASR REF ASRLDAKNN49075.0 81.0 37.0 42.0 91.8 91.1 68.0 71.4RNN1 61.9 58.3 28.4 25.9 48.8 51.2 46.6 45.4LSAKNN49032.1 28.6 2.5 3.7 31.3 33.3 22.0 21.9RNN1 29.8 31.0 4.9 6.2 23.8 23.8 19.7 20.5RNN2 10004 19.0 19.0 3.7 3.7 9.5 10.7 10.8 11.2LDAKNN49030.9 29.8 2.5 3.7 31.5 33.3 21.7 22.3+LSARNN1 32.1 35.7 4.9 4.9 23.8 22.6 20.5 21.3RNN2 10004 25.0 22.6 4.9 4.9 10.7 10.7 13.7 12.9Table 3: % False rejection in topic detection using KNN classifier with 6 nearest neighbour and distanceweights and RNNLM classifier on the DEV test set.
280 dim.
topic spaces for LDA and LSA, and 560dim.
for LDA+LSA.all error rate.
The KNN system could not be eval-uated effectively in reasonable time using 20 timesas many example responses and results are notshown, while RNN2 evaluation times are unaf-fected.
Notably, RNN performance using the LSArepresentation scales with training data size betterthan with the LDA+LSA representation.
Thus, wefurther investigate systems only with the LSA rep-resentation.
Interestingly, section D performanceis improved only marginally.Performance on section D is always best, as sec-tion D questions relate to discussion of charts andgraphs for different conditions for which the vo-cabulary is very specific.
Section C and E ques-tions are the less distinct because they have free-form answers to broad questions, leading to higherresponse variability.
This makes the linking oftopic from the training data to the test data morechallenging, particularly for 1-best classification,leading to higher error rates.Figure 2 shows the topic classification confu-sion matrix for the RNN1 LSA system.
A similarmatrix is observed for the KNN LSA system.
Mostconfusions are with topics from the same section.This is because each section has a distinct styleof questions and some questions within a sectionare similar.
An example is shown below.
Ques-tion SC-EX1 relates to personal local events in theworkplace.
SC-EX2, which relates to similar is-sues, is often confused with it.
On the other hand,SC-EX3 is rarely confused with SC-EX1 as it isabout non-personal events on a larger scale.?
SC-EX1: Talk about some advice from a colleague.You should say: what the advice was, how it helpedyou and whether you would give the same advice toanother colleague.?
SC-EX2: Talk about a socially challenging day you hadat work.
You should say: what was the challengingsituation, how you resolved it and why you found itchallenging.?
SC-EX3: Talk about a company in your local townwhich you admire.
You should say: what company itis, what they do, why you admire them, and how thecompany impacts life in your town.Figure 2: RNN1 LSA confusion matrix on DEVASR.System performance can be increased by con-sidering N -best results, as described in Section 3.Results for systems trained on 490 and 10004 can-didates are presented in Table 4.
The error ratedecreases as the value of N increases for all sys-tems.
However, performance scales better with Nfor the RNN systems than for KNN.
Notably, forvalues of N > 1 performance of all systems onREF is better, which suggests that ASR errors dohave a minor impact on system performance.5.2 Off-topic response detectionIn the second experiment off-topic response de-tection is investigated.
Performance is measured1081N System # Cands.
REF ASR1KNN49022.1 21.9RNN1 19.7 20.5RNN2 10004 10.8 11.22KNN49015.9 16.0RNN1 13.7 16.1RNN2 10004 6.8 7.63KNN49013.5 14.3RNN1 10.4 11.2RNN2 10004 6.4 7.24KNN49011.1 12.5RNN1 8.8 10.0RNN2 10004 5.2 6.4Table 4: N -Best % false rejection performance ofKNN and RNNLM classifiers with the LSA topicspace on the DEV test setin terms of the false acceptance (FA) probabilityof an off-topic response and false rejection (FR)probability of an on-topic response.
The experi-ment is run on DEV and EVAL test sets.
Sinceneither DEV nor EVAL contain real off-topic re-sponses, a pool Wqof such responses is synthet-ically generated for each question by using validresponses to other questions in the data set.
Off-topic responses are then selected from this pool.A selection strategy defines which responses arepresent in Wq.
Rather than using a single se-lection of off-topic responses, an expected per-formance over all possible off-topic response se-lections is estimated.
The overall probability offalsely accepting an off-topic response can be ex-pressed using equation 19.P(FA) =Q?q=1?w?WqP(FA|w, q)P(w|q)P(q) (19)In equation 19, the question q is selected with uni-form probability from the set Q of possible ques-tions.
The candidate randomly selects with uni-form probability P(w|q) a response w from thepool Wq.
The correct response to the question isnot present in the pool.
The conditional probabil-ity of false accept P(FA|w, q) = 1 ifM(q) ?
?tN,andM(q) is not the real topic of the response w,otherwise P(FA|w, q) = 0.As shown in Figure 2, the main confusions willoccur if the response is from the same section asthe question.
Two strategies for selecting off-topicresponses are considered based on this: naive,where an incorrect response can be selected fromany section; and directed, where an incorrectresponse can only be selected from the same sec-tion as the question.
The naive strategy rep-resents candidates who have little knowledge ofthe system and memorise responses unrelated tothe test, while the directed strategy representsthose who are familiar with the test system andhave access to real responses from previous tests.Test Set System% Equal Error RateDirected NaiveDEVKNN 13.5 10.0RNN1 10.0 7.5RNN2 7.5 6.0EVALKNN 12.5 9.0RNN1 8.0 6.0RNN2 5.0 4.5Table 5: % Equal Error Rate for LSA topic spacesystems on the DEV and EVAL test sets.Figure 3: ROC curves of LSA topic space systemson the EVAL test set.A Receiver Operating Characteristic (ROC)curve (Figure 3) can be constructed by plotting theFA and FR rates for a range of N .
The RNN1 sys-tem performs better at all operating points than theKNN system for both selection strategies and eval-uation test sets.
Equal Error Rates (EER), whereFA = FR, are given in Table 5.
Results on EVALare more representative of the difference betweenthe KNN and RNN performance, as they are eval-uated on nearly 3 times as many candidates.
The1082RNN2 system achieves the lowest EER.
It is in-teresting that for better systems the difference inperformance against the naive and directedstrategies decreases.
This indicates that the sys-tems become increasingly better at discriminatingbetween similar questions.As expected, the equal error rate for thedirected strategy is higher than for the naivestrategy.
In relation to the stated task of detect-ing when a test candidate is giving a memorizedresponse, the naive strategy represents a lower-bound on realistic system performance, as studentsare not likely to respond with a valid response toa different question.
Most likely they will failto construct a valid response or will add com-pletely unrelated phrases memorised beforehand,which, unlike responses from other sections, maynot come from the same domain as the test (eg:Business for BULATs).6 Conclusion and Future WorkIn this work a novel off-topic content detectionframework based on topic-adapted RNNLMs wasdeveloped.
The system was evaluated on the taskof detecting off-topic spoken responses on the BU-LATS test.
The proposed approach achieves bettertopic classification and off-topic detection perfor-mance than the standard approaches.A limitation of both the standard and proposedapproach is that if a new question is created bythe test-makers, then it will be necessary to col-lect example responses before it can be widely de-ployed.
However, since the system can be trainedon ASR transcriptions, the example responses donot need to be hand-transcribed.
This is an at-tractive deployment scenario, as only a smallerhand-transcribed data set is needed to train an ASRsystem with which to cost-effectively transcribe alarge number of candidate recordings.Further exploration of different topic vector rep-resentations and their combinations is necessary infuture work.AcknowledgementsThis research was funded under the ALTA Insti-tute, University of Cambridge.
Thanks to Cam-bridge English, University of Cambridge, for sup-porting this research and providing access to theBULATS data.ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022, March.Lucy Chambers and Kate Ingham.
2011.
The BU-LATS online speaking test.
Research Notes, 43:21?25.Xie Chen, Yongqiang Wang, Xunying Liu, Mark J.F.Gales, and P.C.
Woodland.
2014.
Efficient GPU-based Training of Recurrent Neural Network Lan-guage Models Using Spliced Sentence Bunch.
InProc.
INTERSPEECH.Xie Chen, Tian Tan, Xunying Liu, Pierre Lanchantin,Moquan Wan, Mark J.F.
Gales, and Philip C. Wood-land.
2015.
Recurrent Neural Network Lan-guage Model Adaptation for Multi-Genre BroadcastSpeech Recognition.
In Proc.
INTERSPEECH.X.
Chen, X. Liu, Y. Qian, M.J.F.
Gales, and P.C.
Wood-land.
2016.
CUED-RNNLM ?
an open-sourcetoolkit for efficient training and evaluation of re-current neural network language models.
In Proc.ICASSP.Keelan Evanini and Xinhao Wang.
2014.
Automaticdetection of plagiarized spoken responses.
In Pro-ceedings of the Ninth Workshop on Innovative Useof NLP for Building Educational Applications.Keelan Evanini, Shasha Xie, and Klaus Zechner.
2013.Prompt-based Content Scoring for Automated Spo-ken Language Assessment.
In Proc.
NAACL-HLT.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing Scientific Topics.
Proceedings of the NationalAcademy of Sciences, 101:5228?5235.Thomas K Landauer, Peter W. Foltz, and Darrell La-ham.
1998.
Introduction to Latent Semantic Analy-sis.
Discourse Processes, 25:259?284.Xunying Liu, Y. Wang, Xie Chen, Mark J.F.
Gales,and Philip C. Woodland.
2014.
Efficient LatticeRescoring using Recurrent Neural Network Lan-guage Models.
In Proc.
INTERSPEECH.Angeliki Metallinou and Jian Cheng.
2014.
UsingDeep Neural Networks to Improve Proficiency As-sessment for Children English Language Learners.In Proc.
INTERSPEECH.Tomas Mikolov and Geoffrey Zweig.
2012.
ContextDependent Recurrent Neural Network LanguageModel.
In Proc.
IEEE Spoken Language Technol-ogy Workshop (SLT).Tomas Mikolov, Martin Karafi?at, Luk?as Burget, JanCernock?y, and Sanjeev Khudanpur.
2010.
Recur-rent Neural Network Based Language Model.
InProc.
INTERSPEECH.1083Tomas Mikolov.
2012.
Statistical Language ModelsBased on Neural Networks.
Ph.D. thesis, Brno Uni-versity of Technology.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duch-esnay.
2011.
Scikit-learn: Machine Learning inPython.
Journal of Machine Learning Research,12:2825?2830.Xuan-Hieu Phan and Cam-Tu Nguyen.
2007.
Gibb-sLDA++: A C/C++ implementation of latent Dirich-let alocation (LDA).
http://gibbslda.sourceforge.net/.Barbara Seidlhofer.
2005.
English as a lingua franca.ELT journal, 59(4):339.A Stolcke.
2002.
SRILM ?
an extensible languagemodelling toolkit.
In Proc.
ICSLP.Haipeng Wang, Anton Ragni, Mark J. F. Gales, Kate M.Knill, Philip C. Woodland, and Chao Zhang.
2015.Joint Decoding of Tandem and Hybrid Systems forImproved Keyword Spotting on Low Resource Lan-guages.
In Proc.
INTERSPEECH.Shasha Xie, Keelan Evanini, and Klaus Zechner.
2012.Exploring Content Features for Automated SpeechScoring.
In Proc.
NAACL-HLT.Helen Yannakoudakis.
2013.
Automated assess-ment of English-learner writing.
Technical ReportUCAM-CL-TR-842, University of Cambridge Com-puter Laboratory.Su-Youn Yoon and Shasha Xie.
2014.
Similarity-Based Non-Scorable Response Detection for Auto-mated Speech Scoring.
In Proceedings of the NinthWorkshop on Innovative Use of NLP for BuildingEducational Applications.Steve Young, Gunnar Evermann, Mark J. F. Gales,Thomas Hain, Dan Kershaw, Xunying (Andrew)Liu, Gareth Moore, Julian Odell, Dave Ollason, DanPovey, Valtcho Valtchev, and Phil Woodland.
2009.The HTK book (for HTK Version 3.4.1).
Universityof Cambridge.Klaus Zechner, Derrick Higgins, Xiaoming Xi, andDavid M. Williamson.
2009.
Automatic scoringof non-native spontaneous speech in tests of spokenEnglish.
Speech Communication, 51(10):883?895.Spoken Language Technology for Education SpokenLanguage.Chau Zhang and Philip C. Woodland.
2015.
A GeneralArtificial Neural Network Extension for HTK.
InProc.
INTERSPEECH.1084
