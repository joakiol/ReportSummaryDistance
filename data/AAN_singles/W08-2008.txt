Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 53?56Manchester, August 2008Concept-graph based Biomedical Automatic Summarization usingOntologiesLaura Plaza MoralesAlberto D?
?az EstebanPablo Gerv?asUniversidad Complutense de MadridC/Profesor Jos?e Garc?
?a Santesmases, s/n, Madrid 28040, Spainlplazam@pas.ucm.es, albertodiaz@fdi.ucm.es,pgervas@sip.ucm.esAbstractOne of the main problems in research onautomatic summarization is the inaccu-rate semantic interpretation of the source.Using specific domain knowledge can con-siderably alleviate the problem.
In this pa-per, we introduce an ontology-based ex-tractive method for summarization.
It isbased on mapping the text to conceptsand representing the document and its sen-tences as graphs.
We have applied ourapproach to summarize biomedical litera-ture, taking advantages of free resources asUMLS.
Preliminary empirical results arepresented and pending problems are iden-tified.1 IntroductionIn recent years, the amount of electronic biomedi-cal literature has increased explosively.
Physiciansand researchers constantly have to consult up-todate information according to their needs, but theprocess is time-consuming.
In order to tackle thisoverload of information, text summarization canundoubtedly play a role.Simultaneously, a big deal of resources, suchas biomedical terminologies and ontologies, haveemerged.
They can significantly benefit the deve-lopment of NLP systems, and in particular, whenused in automatic summarization, they can in-crease the quality of summaries.In this paper, we present an ontology-based ex-tractive method for the summarization of biomed-ical literature, based on mapping the text to con-cepts in UMLS and representing the document andits sentences as graphs.
To assess the importancec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.of the sentences, we compute the centrality of theirconcepts in the document graph.2 Previous WorkTraditionally, automatic summarization methodshave been classified in those which generate ex-tracts and those which generate abstracts.
Al-though human summaries are typically abstracts,most of existing systems produce extracts.Extractive methods build summaries on a super-ficial analysis of the source.
Early summariza-tion systems are based on simple heuristic fea-tures, as the position of sentences in the docu-ment (Brandow et al, 1995), the frequency ofthe words they contain (Luhn, 1958; Edmundson,1969), or the presence of certain cue words or in-dicative phrases (Edmundson, 1969).
Some ad-vanced approaches also employ machine learningtechniques to determine the best set of attributesfor extraction (Kupiec et al, 1995).
Recently,several graph-based methods have been proposedto rank sentences for extraction.
LexRank (Erkanand Radev, 2004) is an example of a centroid-based method to multi-document summarizationthat assess sentence importance based on the con-cept of eigenvector centrality.
It represents thesentences in each document by its tf*idf vectorsand computes sentence connectivity using the co-sine similarity.
Even if results are promising, mostof these approaches exhibit important deficiencieswhich are consequences of not capturing the se-mantic relations between terms (synonymy, hyper-onymy, homonymy, and co-occurs and associated-with relations).We present an extractive method for summariza-tion which attempts to solve this deficiencies.
Un-like researches conducted by (Yoo et al, 2007;Erkan and Radev, 2004), which cluster sentencesto identify shared topics in multiple documents, inthis work we apply clustering to identify groups53of concepts closely related.
We hypothesize thateach cluster represents a theme or topic in the do-cument, and we evaluate three different heuristicsto ranking sentences.3 Biomedical Ontologies.
UMLSBiomedical ontologies organize domain conceptsand knowledge in a system of hierarchical and as-sociative relations.
One of the most widespreadin NLP applications is UMLS1(Unified Medi-cal Language System).
UMLS consists of threecomponents: the Metathesaurus, a collection ofconcepts and terms from various vocabularies andtheir relationships; the Semantic Network, a set ofcategories and relations used to classify and relatethe entries in the Metathesaurus; and the Special-ist Lexicon, a database of lexicographic informa-tion for use in NLP.
In this work, we have se-lected UMLS for several reasons.
First, it pro-vides a mapping structure between different ter-minologies, including MeSH or SNOMED, andthus allows to translate between them.
Secondly, itcontains vocabularies in various languages, whichallows to process multilingual information.4 Summarization MethodThe method proposed consists of three steps.
Eachstep is discussed in detail below.
A preliminarysystem has been implemented and tested on severaldocuments from the corpus developed by BioMedCentral2.As the preprocessing, text is split into sentencesusing GATE3, and generic words and high fre-quency terms are removed, as they are not usefulin discriminating between relevant and irrelevantsentences.4.1 Graph-based Document RepresentationThis step consists in representing each documentas a graph, where the vertices are the concepts inUMLS associated to the terms, and the edges indi-cate the relations between them.
Firstly, each sen-tence is mapped to the UMLSMetathesaurus usingMetaMap (Aronson, 2001).
MetaMap allowsto map terms to UMLS concepts, using n-gramsfor indexing in the ULMS Metathesaurus, andperforming disambiguation to identify the correct1NLM Unified Medical Language System (UMLS).
URL:http://www.nlm.nih.gov/research/umls2BioMed Central: http://www.biomedcentral.com/3GATE (Generic Architecture for Text Engineering):http://gate.ac.uk/concept for a term.
Secondly, the UMLS conceptsare extended with their hyperonyms.
Figure 1shows the graph for sentence ?The goal of the trialwas to assess cardiovascular mortality and mor-bidity for stroke, coronary heart disease and con-gestive heart failure, as an evidence-based guidefor clinicians who treat hypertension.
?Next, each edge is assigned a weight, which isdirectly proportional to the deep in the hierarchy atwhich the concepts lies (Figure 1).
That is to say,the more specific the concepts connected are, themore weight is assigned to them.
Expression (1)shows how these values are computed.|?
?
?||?
?
?|=|?||?|(1)where ?
is the set of all the parents of a con-cept, including the concept, and ?
is the set of allthe parents of its immediate higher-level concept,including the concept.Finally, the sentence graphs are merged intoa document graph, enriched with the associated-with relations between the semantic types inUMLS corresponding to the concepts (Figure 1).Weights for the new edges are computed using ex-pression (1).4.2 Concept Clustering and ThemeRecognitionThe second step consists of clustering concepts inthe document graph, using a degree-based method(Erkan and Radev, 2004).
Each cluster is com-posed by a set of concepts that are closely relatedin meaning, and can be seen as a theme in the do-cument.
The most central concepts in the clustergive the sufficient and necessary information re-lated to its theme.
We hypothesize that the docu-ment graph is an instance of a scale-free network(Barabasi, 1999).
Following (Yoo et al, 2007),we introduce the salience of vertices.
Mathemati-cally, the salience of a vertex (vi) is calculated asfollows.salience(vi) =?ej|?vk?ejconecta(vi,vk)weight(ej)(2)Within the set of vertices, we select the nthat present the higher salience and iterativelygroup them in Hub Vertex Sets (HVS).
A HVSrepresents a group of vertices that are stronglyrelated to each other.
The remaining vertices are54Figure 1: Sentence graphassigned to that cluster to which they are moreconnected.Finally, we assign each sentence to a cluster.
Tomeasure the similarity between a cluster and a sen-tence graph, we use a vote mechanism (Yoo et al,2007).
Each vertex (vk) of a sentence (Oj) gives toeach cluster (Ci) a different number of votes (pi,j)depending on whether the vertex belongs to HVSor non-HVS (3).similarity(Ci, Oj) =?vk|vk?Ojwk,j(3)where{wk,j=0 si vk6?Ciwk,j=1.0,si vk?HV S(Ci)wk,j=0.5,si vk6?HV S(Ci)4.3 Sentence SelectionThe last step consists of selecting significant sen-tences for the summary, based on the similaritybetween sentences and clusters.
We investigatedthree alternatives for this step.?
Heuristic 1: For each cluster, the top nisen-tences are selected, where niis proportionalto its size.?
Heuristic 2: We accept the hypothesis thatthe cluster with more concepts represents themain theme in the document, and select thetop N sentences from this cluster.?
Heuristic 3: We compute a single score foreach sentence, as the sum of the votes as-signed to each cluster adjusted to their sizes,and select theN sentences with higher scores.5 Results and EvaluationIn order to evaluate the method, we analyze thesummaries generated by the three heuristics overa document4from the BioMed Central Corpus,using a compression rate of 20%.
Table 1 showsthe sentences selected along with their scores.Although results are not statistically significant,they show some aspects in which our method be-haves satisfactorily.
Heuristics 1 and 3 extract sen-tence 0, and assign to it the higher score.
Thissupports the positional criterion of selecting thefirst sentence in the document, as the one that con-tains the most significant information.
Sentence 58represents an example of sentence, situated at theend, which gathers the conclusions of the author.In general, these sentences are highly informative.Sentence 19, in turn, evidences how the methodsystematically gives preference to long sentences.Moreover, while summaries by heuristics 1 and 3have a lot of sentences in common (9 out of 12),heuristic 2 generates a summary considerably dif-ferent and ignores important topics in the docu-ment.
Finally, we have compared these summarieswith the author?s abstract.
It can be observed thatheuristics 1 and 3 cover all topics in the author?sabstract (see sentences 0, 4, 15, 17, 19, 20 and 25).4BioMed Central: www.biomedcentral.com/content/download/xml/cvm-2-6-254.xml55Sentences 0 4 19 58 7 28 25 20 21 8 43 15Heuristic 1 99.0 20.0 19.0 18.5 17.0 16.5 16.0 15.5 15.5 13.5 13.5 12.0Heuristic 2 19.0 16.5 15.5 12.5 12.0 10.5 9.0 9.0 7.5 7.0 7.0 7.0Heuristic 3 98.8 18.7 17.9 16.3 15.3 14.5 13.4 13.0 13.0 12.7 12.7 12.2Table 1: ResultsAs far as heuristic 2 is concerned, it does not coveradequately the information in the abstract.6 Conclusions and Future WorkIn this paper we introduce a method for summa-rizing biomedical literature.
We represent the do-cument as an ontology-enriched scale-free graph,using UMLS concepts and relations.
This way weget a richer representation than the one provided bya vector space model.
In section 5 we have evalu-ated several heuristics for sentence extraction.
Wehave determined that heuristic 2 does not cover allrelevant topics and selects sentences with a low rel-ative significance.
Conversely, heuristics 1 and 3,present very similar results and cover all importanttopics.Nonetheless, we have identified several prob-lems and some possible improvements.
Firstly, asour method extracts whole sentences, long oneshave higher probability to be selected, becausethey contain more concepts.
The alternative couldbe to normalise the sentences scores by the numberof concepts.
Secondly, concepts associated withgeneral semantic types in UMLS, as functionalconcept, temporal concept, entity and language,could be ignored, since they do not contribute todistinguish what sentences are significant.Finally, in order to formally evaluate the methodand the different heuristics, a large-scale evalua-tion on the BioMed Corpus is under way, based oncomputing the ROUGE measures (Lin, 2004).AcknowledgementsThis research is funded by the Ministerio de Edu-caci?on y Ciencia (TIN2006-14433-C02-01), Uni-versidad Complutense de Madrid and Direcci?onGeneral de Universidades e Investigaci?on de la Co-munidad de Madrid (CCG07-UCM/TIC 2803).ReferencesAronson A. R. Effective Mapping of Biomedical Textto the UMLS Metathesaurus: The MetaMap Pro-gram.
2001.
In Proceedings of American MedicalInformatics Association.Barabasi A.L.
and Albert R. Emergence of scaling inrandom networks.
1999.
Science,286?509.Brandow R. and Mitze K. and Rau L. F. AutomaticCondensation of Electronic Publications by Sen-tence Selection.
1995.
Information Processing andManagement,5(31):675?685.Edmundson H.P.
New Methods in Automatic Extract-ing.
1969.
Journal of the Association for ComputingMachinery,2(16):264?285.Erkan G. and Radev D. R. LexRank: Graph-basedLexical Centrality as Salience in Text Summariza-tion.
2004.
Journal of Artificial Intelligence Re-search (JAIR),22:457?479.Kupiec J. and Pedersen J.O.
and Chen F. A TrainableDocument Summarizer.
1995.
In Proceedings ofthe 18th Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval,68?73.Lin C-Y.
ROUGE: A Package for Automatic Eval-uation of Summaries.
2004.
In Proceedings ofWorkshop on Text Summarization Branches Out,Post-Conference Workshop of ACL 2004, Barcelona,Spain.Luhn H.P.
The Automatic Creation of LiteratureAbstracts.
1958.
IBM Journal of ResearchDevelopment,2(2):159?165.Sparck-Jones K. Automatic Summarizing: Factors andDirections.
1999.
I. Mani y M.T.
Maybury, Advancesin Automatic Text Summarization.
The MIT Press.Yoo I. and Hu X. and Song I.Y.
A coherent graph-basedsemantic clustering and summarization approach forbiomedical literature and a new summarization eval-uation method.
2007.
BMC Bioinformatics,8(9).56
