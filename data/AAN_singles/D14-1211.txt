Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1965?1976,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsGender and Power:How Gender and Gender Environment Affect Manifestations of PowerVinodkumar PrabhakaranDept.
of Computer ScienceColumbia UniversityNew York, NY, USAvinod@cs.columbia.eduEmily E. ReidDept.
of Computer ScienceColumbia UniversityNew York, NY, USAeer2137@columbia.eduOwen RambowCCLSColumbia UniversityNew York, NY, USArambow@ccls.columbia.eduAbstractWe investigate the interaction of power,gender, and language use in the Enronemail corpus.
We present a freely avail-able extension to the Enron corpus, withthe gender of senders of 87% messagesreliably identified.
Using this data, wetest two specific hypotheses drawn fromthe sociolinguistic literature pertaining togender and power: women managers useface-saving communicative strategies, andwomen use language more explicitly thanmen to create and maintain social rela-tions.
We introduce the notion of ?genderenvironment?
to the computational studyof written conversations; we interpret thisnotion as the gender makeup of an emailthread, and show that some manifestationsof power differ significantly between gen-der environments.
Finally, we show theutility of gender information in the prob-lem of automatically predicting the direc-tion of power between pairs of participantsin email interactions.1 IntroductionIt has long been observed that men and womencommunicate differently in different contexts.This phenomenon has been studied by sociolin-guists, who typically rely on case studies or sur-veys.
The availability of large corpora of nat-urally occurring social interactions has given usthe opportunity to study language use at a broaderlevel than before.
In this paper, we use the EnronCorpus of work-related emails to examine writtencommunication in a corporate setting.
We inves-tigate three factors that affect choices in commu-nication: the writer?s gender, the gender of his orher fellow discourse participants (what we call the?gender environment?
), and the relations of orga-nizational power he or she has to the discourse par-ticipants.
We concentrate on modeling the writer?schoices related to discourse structure, rather thanlexical choice.
Specifically, our goal is to showthat gender, gender environment, and power all af-fect individuals?
choices in complex ways, result-ing in patterns in the discourse that reveal the un-derlying factors.This paper makes three major contributions.First, we introduce an extension to the well-knownEnron corpus of emails: we semi-automaticallyidentify the sender?s gender of 87% of email mes-sages in the corpus.
This extension will be madepublicly available.
Second, we use this enrichedversion of the corpus to investigate the interactionof hierarchical power and gender.
We formalizethe notion of ?gender environment?, which reflectsthe gender makeup of the discourse participantsof a particular conversation.
We study how gen-der, power, and gender environment influence dis-course participants?
choices in dialog.
We inves-tigate two specific hypotheses from the sociolin-guistic literature, relating to face-saving use of lan-guage, and to the use of language to strengthen so-cial relations.
This contribution does not exhaustthe possibilities of our corpus, but it shows howsocial science can benefit from advanced naturallanguage processing techniques in analyzing cor-pora, allowing social scientists to tackle corporasuch as the Enron corpus which cannot be exam-ined in its entirety by hand.
Third, we show thatthe gender information in the enriched corpus canbe useful for computational tasks, specifically fortraining a system that predicts the direction of hier-archical power between participants in an interac-tion.
Our use of the gender-based features booststhe accuracy of predicting the direction of powerbetween pairs of email interactants from 68.9% to70.2% on an unseen test set.1965The paper is structured as follows.
We reviewrelated work in Section 2.
We present the GenderIdentified Enron Corpus (our first contribution) inSection 3.
Section 4 defines the problem of pre-dicting power and the various dimensions of in-teraction we analyze.
We turn to our second con-tribution, the analysis of the data, in Sections 5and 6.
Section 7 describes our third contribution,the machine learning experiments using gender-related features in the prediction of hierarchicalpower.
We then conclude and discuss future work.2 Related WorkThere is much sociolinguistic background relatedto gender and language use, some of it specificallyrelated to language use in the work environment(Kendall and Tannen, 1997; Holmes and Stubbe,2003; Kendall, 2003; Herring, 2008).
We do notprovide a full discussion of this work for lack ofspace, but single out one paper which has partic-ularly influenced our work.
Holmes and Stubbe(2003) provide two case studies that do not lookat the differences between male and female man-agers?
communication, but at the difference be-tween female managers?
communication in moreheavily female vs. more heavily male environ-ments.
They find that, while female managers tendto break many stereotypes of ?feminine?
commu-nication, they have different strategies in connect-ing with employees and exhibiting power in thetwo gender environments.
This work has inspiredus to look at this phenomenon by including ?Gen-der Environment?
in our study.
By finding the ra-tios of males to females on a thread, we can look atwhether indicators change within a more heavilymale or female thread.
This notion of gender envi-ronment is supported by an idea in recent Twitter-based sociolinguistic research on gender identityand lexical variation (Bamman et al., 2014).
Oneof the many insights from their work is that gen-dered linguistic behavior is oriented by a numberof factors, one of which includes the speaker?s au-dience.
Their work looks at Twitter users whoselinguistic style fails to identify their gender in clas-sification experiments, and finds that the linguis-tic gender norms can be influenced by the style oftheir interlocutors.Within the NLP community, there has beensubstantial research exploring language use andpower.
A large number of these studies are per-formed in the domain of organizational emailwhere the notion of power is well defined in termsof organizational hierarchy.
It is also aided by theavailability of the moderately large Enron emailcorpus which captures email interactions in an or-ganizational setting.
Earlier approaches used sim-ple lexical features alone (e.g.
(Bramsen et al.,2011; Gilbert, 2012)) as a means to predict power.Later studies have used more complex linguisticand structural features, such as formality (Petersonet al., 2011), dialog acts (Prabhakaran and Ram-bow, 2013), and thread structure (Prabhakaran andRambow, 2014).
Our work is also on the Enronemail corpus, and our baseline features are derivedfrom some of this prior work.
Researchers havealso studied power and influence in other genresof interactions, such as online forums (Danescu-Niculescu-Mizil et al., 2012; Biran et al., 2012),multi-party chats (Strzalkowski et al., 2012) andoff-line interactions such as presidential debates(Nguyen et al., 2013; Prabhakaran et al., 2013;Prabhakaran et al., 2014).There is also some work within the NLP fieldon analyzing language use in relation to gender.Mohammad and Yang (2011) analyzed the waygender affects the expression of sentiments in text,while we are interested in how gender relates tomanifestations of organizational power.
For theirstudy, they assigned gender for the core employeesin the Enron email corpus based on whether thefirst name of the person was easily gender iden-tifiable or not.
If the person had an unfamiliarname or a name that could be of either gender,they marked his/her gender as unknown and ex-cluded them from their study.1For example, thegender of the employee Kay Mann was marked asunknown in their gender assignment.
However, inour work, we manually research and determine thegender of every core employee.Researchers have also attempted to automati-cally predict the gender of email senders using su-pervised learning techniques based on linguisticfeatures (Corney et al., 2002; Cheng et al., 2011;Deitrick et al., 2012), a task we do not address inthis paper.
These studies use datasets that are rel-atively smaller in size.
Corney et al.
(2002) usearound 4K emails from 325 gender identified au-thors.
Cheng et al.
(2011) use around 9K emailsfrom 108 gender identified authors.
Deitrick et al.
(2012) use around 18K emails from 144 gender1http://www.saifmohammad.com/WebDocs/dir-email-gender.txt1966identified authors.
The dataset we offer is muchlarger in size, with around 97K emails whose au-thors are gender identified.
We believe that ourresource will aid further research in this area.3 Gender Identified Enron Corpus3.1 Enron CorpusIn our work, we use the version of Enron emailcorpus released by Yeh and Harnly (2006).
Thecorpus contains emails from the mailboxes of 145core employees who held top managerial positionswithin Enron at the time of bankruptcy.
Yeh andHarnly (2006) preprocessed the corpus to combinemultiple email addresses belonging to the sameentity and identify each entity in the corpus witha unique identifier.
The corpus contains a total of111,933 messages.
This version of the corpus hasbeen enriched later by Agarwal et al.
(2012) withgold organizational power relations, manually de-termined using information from Enron organiza-tional charts.
It includes relations of 1,518 em-ployees and captures dominance relations between13,724 pairs of them.
This information enables usto study the manifestations of power in these inter-actions, in relation to gender.In this version of the corpus, the thread structureof email messages is reconstructed, with the miss-ing messages restored from other emails in whichthey were quoted.
This allows us to go beyondisolated messages and study the dialog structurewithin email threads.
There were 34,156 uniquediscourse participants across all the email threadspresent in the corpus.
Manually determining thegender of all the discourse participants in the cor-pus is not feasible.
Hence, we adopt a two-stepapproach through which we reliably identify thegender of a large majority of entities in the emailthreads within the corpus.
We manually deter-mine the gender of the 145 core employees whohave a bigger representation in the corpus, and wesystemically determine the gender of the rest ofthe discourse participants using the Social Secu-rity Administration?s baby names database.
Weadopt a conservative approach so that we assigna gender only when the name of the participantmeets a very low ambiguity threshold.3.2 Manual Gender AssignmentWe researched each of the 145 core employees us-ing web search and found public records aboutthem or articles referring to them.
In order tomake sure that the results are about the same per-son we want, we added the word ?enron?
to thesearch queries.
Within the public records returnedfor each core employee, we looked for instancesin which they were being referred to either using agender revealing pronoun (he/him/his vs. she/her)or using a gender revealing addressing form (Mr.vs.
Mrs./Ms./Miss).
Since these employees heldtop managerial positions within Enron at the timeof bankruptcy, it was fairly easy to find publicrecords or articles referring to them.
For example,the page we found for Kay Mann clearly identifiesher gender.2We were able to correctly determinethe gender of each of the 145 core employees inthis manner.
A benefit of manually determiningthe gender of these core employees is that it en-sures a high coverage of 100% confident genderassignments in the corpus.3.3 Automatic Gender AssignmentAs mentioned in Section 3.1, our corpus containsa large number of discourse participants in addi-tion to the 145 core employees for which we man-ually identified the gender.
To attempt to findthe gender of these other discourse participants,we first determine their first names and then findhow ambiguous the names are by querying the So-cial Security Administration?s (SSA) baby namesdataset.
We first describe how we calculate an am-biguity score for a name using the SSA dataset andthen describe how we use it to determine the gen-der of discourse participants in our corpus.3.3.1 SSA Names and Gender DatasetThe US Social Security Administration maintainsa dataset of baby names, gender, and name countfor each year starting with the 1880s, for nameswith at least five counts.3We used this datasetin order to determine the gender ambiguity of aname.
The Enron data set contains emails from1998 to 2001.
We estimate the common age rangefor a large, corporate firm like Enron at 24-67,4sowe used the SSA data from 1931-1977 to calculateambiguity scores for our purposes.For each name n in the database, let mp(n)and fp(n) denote the percentages of males and fe-males with the name n. Then, we calculate theambiguity score AS (n) as 100?|mp(n)?
fp(n)|.2http://www.prnewswire.com/news-releases/kay-mann-joins-noble-as-general-counsel-57073687.html3http://www.ssa.gov/oact/babynames/limits.html4http://www.bls.gov/cps/demographics.htm1967The value of AS (n) varies between 0 and 100.
Aname that is ?perfectly unambiguous?
would havean ambiguity score of 0, while a ?perfectly am-biguous?
name (i.e., 50%/50% split between gen-ders) would have an ambiguity score of 100.
Weassign the likely gender of the name to be the onewith the higher percentage, if the ambiguity scoreis below a threshold AST.G(n) ={M, if AS(n) ?
ASTand mp(n) > fp(n)F, if AS(n) ?
ASTand mp(n) ?
fp(n)I, if AS(n) > ASTAround 88% of the names in the SSA datasethave AS (n) = 0.
We choose a very conserva-tive threshold of AST= 10 for our gender assign-ments, which assigns gender to around 93% namesin the SSA dataset.53.3.2 Identifying the First NameEach discourse participant in our corpus has atleast one email address and zero or more namesassociated with it.
The name field is automaticallyassembled by Yeh and Harnly (2006), where theycaptured the different names from email headers,which are populated from individual email clientsand do not follow a standard format.
Not all dis-course participants are human; some may refer toorganizational groups (e.g., HR Department) oranonymous corporate email accounts (e.g., a web-master account, do-not-reply address etc.).
Thename field may sometimes be empty, contain mul-tiple names, contain an email address, or showother irregularities.
Hence, it is nontrivial to deter-mine the first name of our discourse participants.We used the heuristics below to extract the mostlikely first name for each discourse participant.?
If the name field contains two words, pick thesecond or first word, depending on whether acomma separates them or not.?
If the name field contains three words and acomma, choose the second and third words(a likely first and middle name, respectively).If the name field contains three words but nocomma, choose the first and second words(again, a likely first and middle name).?
If the name field contains an email address,pick the portion from the beginning of thestring to a ?.?,?
?
or ?-?
; if the email addressis in camel case, take portion from the begin-ning of the string to the first upper case letter.5In the corpus that will be released, we retain the AS(n)of each name, so that the users of this resource can decide thethreshold that suit their needs.?
If the name field is empty, apply the aboverule to the email address field to pick a name.The above heuristics create a list of candidatenames for each discourse participant which wethen query for an ambiguity score (Section 3.3.1)and the likely gender.
We find the candidatename with the lowest ambiguity score that passesthe threshold and assign the associated gender tothe discourse participant.
If none of the candi-date names for a discourse participant passes thethreshold, we assign the gender to be ?I?
(Indeter-minate).
We also assign the gender to be ?I?, ifnone of the candidate names is present in the SSAdataset.
This will occur if the name is a first namethat is not in the database (an unusual or interna-tional name; e.g., Vladi), or if no true first namewas found (e.g., the name field was empty and theemail address was only a pseudonym).
This willalso include most of the cases where the discourseparticipant is not a human.3.3.3 Coverage and AccuracyWe evaluated the coverage and accuracy of ourgender assignment system on the manually as-signed gender data of the 145 core people.
Weobtained a coverage of 90.3%, i.e., for 14 of the145 core people, the ambiguity score was higherthan the threshold.
Of the 131 people the sys-tem assigned a gender to, we obtained an accu-racy of 89.3% in correctly identifying the gender.We investigated the errors and found that all er-rors were caused due to incorrectly identifying thefirst name.
These errors arise because the namefields are automatically populated and sometimesthe core discourse participants?
name fields in-clude their secretaries.
While this is common forpeople in higher managerial positions, we expectthis not to happen in the middle management andbelow, to which most of the automatically gender-assigned discourse participants belong.3.4 Corpus Statistics and DivisionsWe apply the gender assignment system describedabove to all discourse participants of all emailthreads in the entire Enron corpus described inSection 3.1.
Table 1 shows the coverage of gen-der assignment in our corpus at different lev-els: unique discourse participants, messages andthreads.
In Table 2, we show the male/female per-centage split of all unique discourse participants,as well as the split at the level of messages (i.e.,messages sent by males vs. females).1968Count (%)Total unique discourse participants 34,156- gender identified 23,009 (67.3%)Total messages 111,933- senders gender identified 97,255 (86.9%)Total threads 36,615- all senders gender identified 26,015 (71.1%)- all participants gender identified 18,030 (49.2%)Table 1: Coverage of Gender Identification at various level:unique discourse participants, messages and threadsMale FemaleUnique Discourse Participants 66.1% 33.9%Message Senders 58.2% 41.8%Table 2: Male/Female split across a) all unique participantswho were gender identified, b) all messages whose senderswere gender identifiedWe divide the entire corpus into Train, Dev andTest sets at the thread level, through random sam-pling, with a distribution of 50%, 25% and 25%each.
The number of threads and messages in eachsubdivision is shown in Table 3.Total Train Dev TestThreads 36,615 18,498 8,973 9,144Messages 111,933 56,447 27,565 27,921Table 3: Train/Test/Dev breakup of the entire corpusWe also create a sub-corpus of the threads calledAll Participants Gender Identified (APGI), con-taining the 18,030 threads for which the gender as-signment system succeeded in assigning the gen-ders of all participants, including senders and allrecipients (To and CC).
For the analysis and ex-periments presented in the rest of this paper, weuse 17,788 threads from this APGI subset, exclud-ing the remaining 242 threads that were used forprevious manual annotation efforts.4 Manifestations of PowerWe use the gender information of the participantsto investigate how the gender of the sender andrecipients affect the manifestations of hierarchicalpower in interactions.
In order to do this, we usethe interaction analysis framework from our priorwork (Prabhakaran and Rambow, 2014).
In thissection, we give a brief overview of the problemformulation and the structural features we used.4.1 Hierarchically Related Interacting PairsLet t denote an email thread and Mtdenote theset of all messages in t .
Also, let Ptbe the setof all participants in t , i.e., the union of sendersand recipients (To and CC) of all messages in Mt.We are interested in analyzing the power relationsbetween pairs of participants who interact withina given email thread.
Not every pair of partic-ipants (p1, p2) ?
Pt?
Ptinteract with one an-other within t .
Let IMt(p1, p2) denote the set ofInteraction Messages ?
non-empty messages int in which either p1is the sender and p2is oneof the recipients or vice versa.
We call the setof (p1, p2) such that |IMt(p1, p2)| > 0 the inter-acting participant pairs of t (IPPt).
For every(p1, p2) ?
IPPt, we query the set of dominancerelations in the gold hierarchy and assign their hi-erarchical power relation (HP(p1, p2)) to be su-perior if p1dominates p2, and subordinate if p2dominates p1.
We exclude pairs that do not existin the gold hierarchy from our analysis and callthe remaining set related interacting participantpairs (RIPPt).
Table 4 shows the total numberof pairs in IPPtand RIPPtfrom all the threadsin the APGI subset of our corpus and across Train,Dev and Test sets.Description Total Train Dev Test# of threads 17,788 8,911 4,328 4,549?t|IPPt| 74,523 36,528 18,540 19,455?t|RIPPt| 4,649 2,260 1,080 1,309Table 4: Data StatisticsRow 1 presents the total number of threads in differentsubsets of the corpus.
Row 2 and 3 present the number ofinteracting participant pairs (IPP ) and related interactingparticipant pairs (RIPP ) in those subsets.4.2 Structural FeaturesNow, we describe various features that capturethe structure of interaction between the pairs ofparticipants in a thread.
Each feature f is ex-tracted with respect to a person p over a refer-ence set of messages M (denoted fpM).
For a pair(p1, p2), we extract 4 versions of each feature f :fp1IMt(p1,p2), fp2IMt(p1,p2), fp1Mtand fp2Mt.
The first twocapture behavior of each person of the pair in in-teractions between themselves, while the third andfourth capture their overall behavior in the entirethread.
We group our features into three categories?
THRSTR, THRMETAand DIA.
THRSTRcap-tures the thread structure in terms of verbosity and1969positional features of messages (e.g., how manyemails did a person send).
THRMETAcontainemail header meta-data based features that cap-ture the thread structure (e.g., how many recipientswere there).
Both sets of features do not performany NLP analysis on the the content of the emails.DIA captures the pragmatics of the dialog and re-quires a deeper analysis of the email content (e.g.,did they issue any requests).THRSTR: This feature set includes two kindsof features ?
positional and verbosity.
The po-sitional features are a boolean feature to denotewhether p sent the first message (Initiate), andthe relative positions of p?s first and last messages(FirstMsgPos and LastMsgPos) in M .
The ver-bosity features are p?s message count (MsgCount),message ratio (MsgRatio), token count (Token-Count), token ratio (TokenRato) and tokens permessage (TokenPerMsg), all calculated over M .THRMETA: This feature set includes the av-erage number of recipients (AvgRecipients) andTo recipients (AvgToRecipients) in emails sent byp, the percentage of emails p received in whichhe/she was in the To list (InToList%), boolean fea-tures denoting whether p added or removed peo-ple when responding to a message (AddPersonand RemovePerson), average number of replies re-ceived per message sent by p (ReplyRate) and av-erage number of replies received from the otherperson of the pair to messages where he/she wasa To recipient (ReplyRateWithinPair).
ReplyRate-WithinPair applies only to IMt(p1, p2).DIA: We use dialog acts (DA) and overt dis-plays of power (ODP) tags to model the struc-ture of interactions within the message content.We obtain DA and ODP tags using automatic tag-gers trained on manual annotations.
The DA tag-ger (Omuya et al., 2013) obtained an accuracy of92%.
The ODP tagger (Prabhakaran et al., 2012)obtained an accuracy of 96% and F-measure of54%.
The DA tagger labels each sentence to beone of the 4 dialog acts: Request Action, RequestInformation, Inform, and Conventional.
The ODPTagger identifies sentences (mostly requests) thatexpress additional constraints on their addressee,beyond those introduced by the dialog act.
Forexample, the sentence ?Please come to my of-fice right now?
is considered as an ODP, while?It would be great if you could come to my of-fice now?
is not, even though both issue the samerequest.
For more details on ODP, we refer theFeature Name Mean(fXIMt)|X =FsubFsupMsubMsupTHRMETAAvgRecipients??
?4.76 5.74 5.58 4.98AvgToRecipients??
?3.63 4.73 3.84 3.80InToList%.0.83 0.86 0.84 0.83ReplyRate??
?0.72 0.86 0.70 0.61AddPerson 0.58 0.66 0.59 0.68RemovePerson 0.55 0.60 0.54 0.65THRSTRInitiate 0.38 0.24 0.39 0.30FirstMsgPos?0.18 0.25 0.19 0.22LastMsgPos?
?0.34 0.33 0.34 0.39MsgCount??
?0.92 0.61 0.93 0.91MsgRatio??
?0.33 0.23 0.33 0.32TokenCount 76.5 41.0 102.0 54.3TokenRatio 0.38 0.23 0.40 0.27TokenPerMsg??
?90.2 67.9 118.2 53.2DIAPRConventional 0.55 0.43 0.64 0.56Inform 3.50 1.96 4.51 2.53ReqAction?
?0.07 0.06 0.05 0.10ReqInform 0.29 0.21 0.20 0.16DanglingReq% 0.06 0.12 0.07 0.18ODPCount??
?0.10 0.07 0.09 0.13Table 5: ANOVA results and group means for HierarchicalPower and GenderFsub: Female subordinates; Fsup: Female superiors;Msub: Male subordinates; Msup: Male superiors;* (p < .05 ); ** (p < .01 ); *** (p < .001 )reader to (Prabhakaran et al., 2012).
We use 5features: ReqAction, ReqInform, Inform, Conven-tional, and ODPCount to capture the number ofsentences in messages sent by p that have each ofthese labels.
We also use a feature to capture thenumber of p?s messages with a request that did notget a reply, i.e., dangling request percentage (Dan-glingReq%), over all messages sent by p.5 Gender and PowerIn this subsection, we analyze the impact of gen-der on the expression of power in email.
We per-form an ANOVA test on all features described inSection 4.2 keeping both Hierarchical Power andGender as independent variables.
We perform thison the Train subset of the APGI subset of our cor-pus.
Table 5 shows the results for thread level ver-sion of the features (we obtain similar significanceresults at the interaction level as well).
As can beseen from the ANOVA results, the mean values ofmany features differ significantly for the factorial19700.091?0.114?0.086?0.113?0.096?0.072?0.086?0.135?0?0.04?0.08?0.12?0.16?Subordinates?
Superiors?
Female?
Male?
Female?Subordinates?Female?Superiors?Male?Subordinates?Male?Superiors?Figure 1: Mean values of ODPCounts in different groups: Subordinates vs. Superiors; Female vs.
Male;across all combinations of Hierarchical Power and Gender.groups of Hierarchical Power and Gender.
For ex-ample, ReplyRate was highly significant; femalesuperiors obtain the highest reply rate.It is crucial to note that ANOVA only deter-mines that there is a significant difference betweengroups, but does not tell which groups are signifi-cantly different.
In order to ascertain that, we mustuse the Tukey?s HSD (Honest Significant Differ-ence) Test.
We do not describe the analysis ofall our features to that depth in this paper due tospace limitations.
Instead, we investigate specifichypotheses which we have derived from sociolin-guistic literature.
The first hypothesis we investi-gate is:?
Hypothesis 1: Female superiors tend to use?face-saving?
strategies at work that includeconventionally polite requests and imperson-alized directives, and that avoid imperatives(Herring, 2008).As a stand-in for a face-threatening communica-tive strategy, we use our ?Overt Display of Power?feature (ODP).
An ODP limits the addressee?srange of possible responses, and thus threatens hisor her (negative) face.6We thus reformulate ourhypothesis as follows: the use of ODP by superi-ors changes when looking at the splits by gender,with female superiors using fewer ODPs than malesuperiors.
We look further into the ANOVA anal-ysis of the thread-level ODPCount treating Hierar-chical Power and Gender as independent variables.Figure 1 shows the mean values of ODP counts in6For a discussion of the notion of ?face?, see (Brown andLevinson, 1987).each group of participants.
A summary of the re-sults follows.Hierarchical Power was significant.
Subordi-nates had an average of 0.091 ODP counts and Su-periors had an average of 0.114 ODP counts.
Gen-der was also significant; Females had an averageof 0.086 ODP counts and Males had an average of0.113 ODP counts.
When looking at the factorialgroups of Hierarchical Power and Gender, how-ever, several results were very highly significant.The significantly different pairs of groups, as perthe Tukey?s HSD test, are Male Superiors/MaleSubordinates, Male Superiors/Female Superiors,and Male Superiors/Female Subordinates.
MaleSuperiors used the most ODPs, with an averageof 0.135 counts.
Somewhat surprisingly, FemaleSuperiors used the least of the entire group, withan average of 0.072 counts.
Among Subordinates,Females actually used slightly more ODP, with anaverage of 0.096 counts.
Male Subordinates hadan average of 0.086 ODP counts.
However, thedifferences among these three groups (Female Su-periors, Female Subordinates, and Male Subordi-nates) are not significant.The results confirm our hypothesis: femalesuperiors use fewer ODPs than male superiors.However, we also see that among women, thereis no significant difference between superiors andsubordinates, and the difference between superi-ors and subordinates in general (which is signif-icant) is entirely due to men.
This in fact showsthat a more specific (and more interesting) hypoth-esis than our original hypothesis is validated: onlymale superiors use more ODPs than subordinates.19716 Gender Environment and PowerWe now turn to gender environments and their re-lation to the expression of power in written di-alogs.
We again start with a hypothesis based onthe sociolinguistic literature.?
Hypothesis 2: Women use language to cre-ate and maintain social relations, for exam-ple, they use more small talk (based on a re-ported ?stereotype?
in (Holmes and Stubbe,2003)).We first define more formally what we mean by?gender environment?
(Section 6.1), and then in-vestigate our hypothesis (Section 6.2).6.1 The Notion of ?Gender Environment?The notion of ?gender environment?
refers to thegender composition of a group who are communi-cating.
In the sociolinguistic studies we have con-sulted (Holmes and Stubbe, 2003; Herring, 2008),the notion refers to a stable work group who in-teract regularly.
Since we are interested in study-ing email conversations (threads), we adapt thenotion to refer to a single thread at a time.
Fur-thermore, we assume that a discourse participantmakes communicative decisions based on (amongother factors) his or her own gender, and basedon the genders of the people he or she is commu-nicating with in a given conversation (i.e., emailthread).
We therefore consider the ?gender envi-ronment?
to be specific to each discourse partic-ipant and to describe the other participants fromhis or her point of view.
Put differently, we use thenotion of ?gender environment?
to model a dis-course participant?s (potential) audience in a con-versation.
For example, a conversation among fivewomen and one man looks like an all-female audi-ence from the man?s point of view, but a majority-female audience from the women?s points of view.We define the gender environment of a dis-course participant p in a thread t as follows.
Asdiscussed, we assume that the gender environmentis a property of each discourse participant p inthread t. We take the set of all discourse partic-ipants of the thread t, Pt(see Section 4.1), andexclude p from it: Pt\ {p}.
We then calculatethe percentage of women in this set.7We obtain7We note that one could also define the notion of genderenvironment at the level of individual emails: not all emailsin a thread involve the same set of participants.
We leave thisto future work.three groups by setting thresholds on these per-centages.
Finer-grained gender environments re-sulted in partitions of the data with very few in-stances, since most of our data involves fairly bal-anced gender ratios.
The three gender environ-ments we use are the following:?
Female Environment: if the percentage ofwomen in Pt\ {p} is above 66.7%.?
Mixed Environment: if the percentage ofwomen in Pt\ {p} is between 33.3% and66.7%.?
Male Environment: if the percentage ofwomen in Pt\ {p} is below 33.3%Across all threads and discourse participants inthe threads, we have 791 female, 2087 mixed and1642 male gender environments.6.2 Gender Environment and ConventionalDialog ActsWe now turn to testing Hypothesis 2.
We have atpresent no way of testing for ?small talk?
as op-posed to work-related talk, so we instead test Hy-pothesis 2 by asking how many conventional dia-log acts a person performs.
Conventional dialogacts serve not to convey information or requests(both of which would typically be work-related inthe Enron corpus), but to establish communication(greetings) and to manage communication (sign-offs); since communication is an important way ofcreating and maintaining social relations, we cansay that conventional dialog acts serve the purposeof easing conversations and thus of maintainingsocial relations.
Since this aspect of language isspecifically dependent on a group of people (it isan inherently social function), we assume that therelevant feature is not simply Gender, but GenderEnvironment.
Specifically, we make our Hypothe-sis 2 more precise by saying that a higher numberof conventional dialog acts is used in Female En-vironments.
We use the thread level version of thefeature ConventionalCount.Figure 2 shows the mean values of Conven-tionalCount in each sub-group of participants.Hierarchical Power was highly significant asper ANOVA results.
Subordinates use conven-tional language more (0.60 counts) than Superiors(0.52).
Gender is a very highly significant vari-able; Males use 0.60 counts on average, whereas19720.60?0.52?0.61?0.54?
0.56?0.79?0.48?0.57?
0.51?
0.56?
0.55?0.00?0.25?0.50?0.75?1.00?Subordinates?
Superiors?
Female?Env?
Mixed?Env?
Male?Env?
Subordinates?in?Female?Env?Superiors?in?Female?Env?Subordinates?in?Mixed?Env?Superiors?in?Mixed?Env?Subordinates?in?Male?Env?Superiors?in?Male?Env?Figure 2: Mean values of Conventional Counts: Subordinates vs. Superiors; across all GenderEnvironments; across all combinations of Hierarchical Power and Gender Environments.Females use 0.50.
This result is somewhat sur-prising, but does not invalidate our Hypothesis 2,since our hypothesis is not formulated in terms ofGender, but in terms of Gender Environment.
Theanalysis of Gender Environment at first appears tobe a negative result: while the averages by GenderEnvironment differ, the differences are not signif-icant.
However, the groups defined by both Hi-erarchical Power and Gender Environment havehighly significant differences.
Subordinates in Fe-male Environments use the most conventional lan-guage of all six groups, with an average of 0.79.Superiors in Female Environments use the least,with an average of 0.48.
Mixed Environments andMale Environments differ, but are more similar toeach other than to Female Environments.
In fact,in the Tukey HSD test, the only significant pairsare exactly the set of subordinates in Female En-vironments paired with each other group (Supe-riors in Female Environments, and Subordinatesand Superiors in Mixed Environments and MaleEnvironments).
That is, Subordinates in Femaleenvironments use significantly more conventionallanguage than any other group, but the remaininggroups do not differ significantly from each other.Our hypothesis is thus only partially verified:while gender environment is a crucial aspect of theuse of conventional DAs, we also need to look atthe power status of the writer.
In fact only sub-ordinates in female environments use more con-ventional DAs than any other group (as defined bypower status and gender environment).
While ourhypothesis is not fully verified, we interpret theresults to mean that subordinates are more com-fortable in female environments to use a style ofcommunication which includes more conventionalDAs than outside the female environments.7 Predicting Power in Participant PairsIn this section, we use the formulation ofthe power prediction problem presented in ourprior work (Prabhakaran and Rambow, 2014).Given a thread t and a pair of participants(p1, p2) ?
RIPPt, we want to automatically de-tect HP(p1, p2).
We use the SVM-based su-pervised learning system from (Prabhakaran andRambow, 2014) that can predict HP(p1, p2) tobe either superior or subordinate based on the in-teraction within a thread t for any pair of partici-pants (p1, p2) ?
RIPPt.
The order of participantsin (p1, p2) is fixed such that p1is the sender ofthe first message in IMt(p1, p2).
The power pre-diction system is built using the ClearTK (Ogrenet al., 2008) wrapper for SVMLight (Joachims,1999) package.
It uses a quadratic kernel to cap-ture feature-feature interactions, which is very im-portant as we see in Section 5 and 6.
We use theTrain, Dev and Test subsets of the APGI subsetof our corpus for our experiments.
We use the re-lated interacting participant pairs in threads fromthe Train set to train our models and optimize ourperformance on those from the Dev set.
We reportresults on both Dev and Test sets.In addition to the features described in Sec-tion 4.2, the power prediction system presentedin (Prabhakaran and Rambow, 2014) uses a lexi-cal feature set (LEX) that captures word ngrams,POS (part of speech) ngrams and mixed ngrams,since lexical features have been established to bevery useful for power prediction.
Mixed ngramsare word ngrams where words belonging to openclasses are replaced with their POS tags.
We addtwo gender-based feature sets: GEN containingthe gender of both persons of the pair and ENVcontaining the gender environment feature.1973Table 6 presents the results obtained using vari-ous feature combinations.
We experimented usingall subsets of {LEX, THRSTR, THRMETA, DIA,GEN, ENV } on the Dev set; we report the mostinteresting results here.
The majority baseline(subordinate) obtains an accuracy of 55.8%.
Us-ing the gender-based features alone performs onlyslightly better than the majority baseline.
We usethe best performing feature subset from (Prab-hakaran and Rambow, 2014) (LEX + THRMETA)as another baseline, which obtains an accuracyof 68.2%.
Adding the GEN features improvesthe performance to 70.6%.
Further adding theENV features improves the performance, but onlymarginally to 70.7% (our overall best result, animprovment of 2.4% points).
The best perform-ing feature set without using LEX was the combi-nation of DIA, THRMETAand GEN (67.3%).
Re-moving the gender features from this reduced theperformance to 64.6%.
Similarly, the best per-forming feature set which do not use the contentof emails at all was THRSTR+ THRMETA+ GEN(66.6).
Removing the gender features decreasesthe accuracy by a larger margin (5.4% accuracyreduction to 63.0).We interpret the differences in absolute im-provement as follows: the gender-based featureson their own are not very useful, and gain predic-tive value only when paired with other features.This is because the other features in fact makequite different predictions depending on genderand/or gender environment.
However, the contentfeatures (and in particular the lexical features) areso powerful on their own that the relative contribu-tion of the gender-based features decreases again.Nonetheless, we take these results as validation ofthe claim that gender-based features enhance thevalue of other features in the task of predictingpower relations.We performed another experiment where wepartitioned the data into two subsets according tothe gender of the first person of the pair and trainedtwo separate models to predict power.
At test time,we chose the appropriate model based on the gen-der of the first person of the pair.
However, thisdid not improve the performance.On our blind test set, the majority baseline ob-tains an accuracy of 57.9% and the (Prabhakaranand Rambow, 2014) baseline obtains an accuracyof 68.9%.
On adding the gender-based features,the accuracy of the system improves to 70.2%.Description AccuracyMajority (Always Subordinate) 55.83GEN 57.59GEN + ENV 57.59Baseline (LEX + THRMETA) 68.24Baseline (LEX + THRMETA) + GEN 70.56Baseline (LEX + THRMETA) + GEN + ENV 70.74DIA + THRMETA+ GEN 67.31DIA + THRMETA64.63THRSTR+ THRMETA+ GEN 66.57THRSTR+ THRMETA62.96Table 6: Accuracies on feature subsets (Dev set).THRMETA: meta-data; THRSTR: structural; DIA: dialog-act;GEN: gender; ENV: gender environment; LEX: ngrams;8 ConclusionWe presented a new, freely available resource: theGender Identified Enron Corpus, and explored therelation between power, gender, and language us-ing this resource.
We also introduced the notionof gender environment, and showed that the man-ifestations of power differ significantly betweengender environments.
We also showed that thegender-related features helps in improving powerprediction.
In future work, we will explore ma-chine learning algorithms which capture the inter-actions between features better than our SVM withquadratic kernel.We expect our corpus to be a rich resource forsocial scientists interested in the effect of powerand gender on language use.
We will investi-gate several other sociolinguistic-inspired researchquestions; for example, do the strategies managersuse for ?effectiveness?
of communication differbased on gender environments?While our findings pertain to the Enron dataset, we believe that the insights and techniquesfrom this study can be extended to other genresin which there is an independent notion of hierar-chical power, such as moderated online forums.AcknowledgmentsThis paper is based upon work supported by theDARPA DEFT Program.
The views expressed arethose of the authors and do not reflect the officialpolicy or position of the Department of Defenseor the U.S. Government.
We thank several anony-mous reviewers for their constructive feedback.1974ReferencesApoorv Agarwal, Adinoyi Omuya, Aaron Harnly, andOwen Rambow.
2012.
A comprehensive gold stan-dard for the enron organizational hierarchy.
In Pro-ceedings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 2: ShortPapers), pages 161?165, Jeju Island, Korea, July.Association for Computational Linguistics.David Bamman, Jacob Eisenstein, and Tyler Schnoe-belen.
2014.
Gender identity and lexical varia-tion in social media.
Journal of Sociolinguistics,18(2):135?160.Or Biran, Sara Rosenthal, Jacob Andreas, KathleenMcKeown, and Owen Rambow.
2012.
Detectinginfluencers in written online conversations.
In Pro-ceedings of the Second Workshop on Language inSocial Media, pages 37?45, Montr?eal, Canada, June.Association for Computational Linguistics.Philip Bramsen, Martha Escobar-Molano, Ami Patel,and Rafael Alonso.
2011.
Extracting social powerrelationships from natural language.
In ACL, pages773?782.
The Association for Computational Lin-guistics.Penelope Brown and Stephen C. Levinson.
1987.Politeness : Some Universals in Language Usage(Studies in Interactional Sociolinguistics).
Cam-bridge University Press, February.Na Cheng, R. Chandramouli, and K. P. Subbalakshmi.2011.
Author gender identification from text.
Digit.Investig., 8(1):78?88, July.Malcolm Corney, Olivier de Vel, Alison Anderson, andGeorge Mohay.
2002.
Gender-preferential text min-ing of e-mail discourse.
In Computer Security Ap-plications Conference, 2002.
Proceedings.
18th An-nual, pages 282?289.
IEEE.Cristian Danescu-Niculescu-Mizil, Lillian Lee,Bo Pang, and Jon Kleinberg.
2012.
Echoes ofpower: language effects and power differences insocial interaction.
In Proceedings of the 21st in-ternational conference on World Wide Web, WWW?12, New York, NY, USA.
ACM.William Deitrick, Zachary Miller, Benjamin Valyou,Brian Dickinson, Timothy Munson, and Wei Hu.2012.
Author gender prediction in an email streamusing neural networks.
Journal of Intelligent Learn-ing Systems & Applications, 4(3).Eric Gilbert.
2012.
Phrases that signal workplace hier-archy.
In Proceedings of the ACM 2012 conferenceon Computer Supported Cooperative Work, CSCW?12, pages 1037?1046, New York, NY, USA.
ACM.Susan C Herring.
2008.
Gender and power in on-line communication.
The handbook of language andgender, page 202.Janet Holmes and Maria Stubbe.
2003. feminine work-places: stereotype and reality.
The handbook of lan-guage and gender, pages 572?599.Thorsten Joachims.
1999.
Making Large-Scale SVMLearning Practical.
In Bernhard Sch?olkopf, Christo-pher J.C. Burges, and A. Smola, editors, Advancesin Kernel Methods - Support Vector Learning, Cam-bridge, MA, USA.
MIT Press.Shari Kendall and Deborah Tannen.
1997.
Genderand language in the workplace.
In Gender and Dis-course, pages 81?105.
Sage, London.Shari Kendall.
2003.
Creating gendered demeanorsof authority at work and at home.
The handbook oflanguage and gender, page 600.Saif Mohammad and Tony Yang.
2011.
Tracking sen-timent in mail: How genders differ on emotionalaxes.
In Proceedings of the 2nd Workshop on Com-putational Approaches to Subjectivity and SentimentAnalysis (WASSA 2.011), pages 70?79, Portland,Oregon, June.
Association for Computational Lin-guistics.Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,Deborah A. Cai, Jennifer E. Midberry, and YuanxinWang.
2013.
Modeling topic control to detect in-fluence in conversations using nonparametric topicmodels.
Machine Learning, pages 1?41.Philip V. Ogren, Philipp G. Wetzler, and StevenBethard.
2008.
ClearTK: A UIMA toolkit for sta-tistical natural language processing.
In TowardsEnhanced Interoperability for Large HLT Systems:UIMA for NLP workshop at Language Resourcesand Evaluation Conference (LREC).Adinoyi Omuya, Vinodkumar Prabhakaran, and OwenRambow.
2013.
Improving the quality of minor-ity class identification in dialog act tagging.
In Pro-ceedings of the 2013 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages802?807, Atlanta, Georgia, June.
Association forComputational Linguistics.Kelly Peterson, Matt Hohensee, and Fei Xia.
2011.Email formality in the workplace: A case studyon the enron corpus.
In Proceedings of the Work-shop on Language in Social Media (LSM 2011),pages 86?95, Portland, Oregon, June.
Associationfor Computational Linguistics.Vinodkumar Prabhakaran and Owen Rambow.
2013.Written dialog and social power: Manifestations ofdifferent types of power in dialog behavior.
In Pro-ceedings of the IJCNLP, pages 216?224, Nagoya,Japan, October.
Asian Federation of Natural Lan-guage Processing.Vinodkumar Prabhakaran and Owen Rambow.
2014.Predicting power relations between participants inwritten dialog from a single thread.
In Proceed-ings of the 52nd Annual Meeting of the Association1975for Computational Linguistics (Volume 2: Short Pa-pers), pages 339?344, Baltimore, Maryland, June.Association for Computational Linguistics.Vinodkumar Prabhakaran, Owen Rambow, and MonaDiab.
2012.
Predicting Overt Display of Power inWritten Dialogs.
In Human Language Technolo-gies: The 2012 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, Montreal, Canada, June.
Associ-ation for Computational Linguistics.Vinodkumar Prabhakaran, Ajita John, and Dor?ee D.Seligmann.
2013. Who had the upper hand?
rank-ing participants of interactions based on their rela-tive power.
In Proceedings of the IJCNLP, pages365?373, Nagoya, Japan, October.
Asian Federationof Natural Language Processing.Vinodkumar Prabhakaran, Ashima Arora, and OwenRambow.
2014.
Power of confidence: How pollscores impact topic dynamics in political debates.In Proceedings of the ACL 2014 Workshop on Lan-guage Technologies and Computational Social Sci-ence, page 49, Baltimore, MD, USA, June.
Associa-tion for Computational Linguistics.Tomek Strzalkowski, Samira Shaikh, Ting Liu,George Aaron Broadwell, Jenny Stromer-Galley,Sarah Taylor, Umit Boz, Veena Ravishankar, andXiaoai Ren.
2012.
Modeling leadership and influ-ence in multi-party online discourse.
In Proceedingsof COLING, pages 2535?2552, Mumbai, India, De-cember.
The COLING 2012 Organizing Committee.Jen-Yuan Yeh and Aaron Harnly.
2006.
Email threadreassembly using similarity matching.
In CEAS2006 - The Third Conference on Email and Anti-Spam, July 27-28, 2006, Mountain View, California,USA, Mountain View, California, USA, July.1976
