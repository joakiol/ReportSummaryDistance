Selecting Sentences for Multidocument Summaries usingRandomized Local SearchMichael WhiteCoGenTex, Inc.840 Hanshaw RoadIthaca, NY 14850, USAmike@cogentex.comClaire CardieDept.
of Computer ScienceCornell UniversityIthaca, NY 14850, USAcardie@cs.cornell.eduAbstractWe present and evaluate a randomized localsearch procedure for selecting sentences to in-clude in a multidocument summary.
The searchfavors the inclusion of adjacent sentences whilepenalizing the selection of repetitive material,in order to improve intelligibility without un-duly affecting informativeness.
Sentence simi-larity is determined using both surface-orientedmeasures and semantic groups obtained frommerging the output templates of an informationextraction subsystem.
In a comparative evalu-ation against two DUC-like baselines and threesimpler versions of our system, we found thatour randomized local search method providedsubstantial improvements in both content andintelligibility, while the use of the IE groups alsoappeared to contribute a small further improve-ment in content.1 IntroductionImproving the intellibility of multidocumentsummaries remains a significant challenge.While most previous approaches to multidoc-ument summarization have addressed the prob-lem of reducing repetition, less attention hasbeen paid to problems of coherence and co-hesion.
In a typical extractive system (e.g.Goldstein et al (2000)), sentences are selectedfor inclusion in the summary one at a time,with later choices sensitive to their similarityto earlier ones; the selected sentences are thenordered either chronologically or by relevance.The resulting summaries often jump incoher-ently from topic to topic, and contain brokencohesive links, such as dangling anaphors or un-met presuppositions.Barzilay et al (2001) present an improvedmethod of ordering sentences in the contextof MultiGen, a multidocument summarizerthat identifies sets of similar sentences, termedthemes, and reformulates their common phrasesas new text.
In their approach, topically relatedthemes are identified and kept together in theresulting summary, in order to help improve co-hesion and reduce topic switching.In this paper, we pursue a related but simpleridea in an extractive context, namely to favorthe selection of blocks of adjacent sentences inconstructing a multidocument summary.
Here,the challenge is to improve intelligibility with-out unduly sacrificing informativeness; for ex-ample, selecting the beginning of the most re-cent article in a document set will usually pro-duce a highly intelligible text, but one that isnot very representative of the document set asa whole.To manage this tradeoff, we have developed arandomized local search procedure (cf.
Selmanand Kautz (1994)) to select the highest rankingset of sentences for the summary, where the in-clusion of adjacent sentences is favored and theselection of repetitive material is penalized.
Themethod involves greedily searching for the bestcombination of sentences to swap in and out ofthe current summary until no more improve-ments are possible; noise strategies include oc-casionally adding a sentence to the current sum-mary, regardless of its score, and restarting thelocal search from random starting points for afixed number of iterations.
In determining sen-tence similarity, we have used surface-orientedsimilarity measures obtained from Columbia?sSimFinder tool (Hatzivassiloglou et al, 2001),as well as semantic groups obtained from merg-ing the output templates of an information ex-traction (IE) subsystem.In related work, Marcu (2001) describes anapproach to balancing informativeness and in-telligibility that also involves searching throughPhiladelphia, July 2002, pp.
9-18.
Association for Computational Linguistics.Proceedings of the Workshop on Automatic Summarization (including DUC 2002),sets of sentences to select.
In contrast toour approach, Marcu employs a beam searchthrough possible summaries of progressivelygreater length, which seems less amenable toan anytime formulation; this may be an im-portant practical consideration, since Marcu re-ports search times in hours, whereas we havefound that less than a minute of searching isusually effective.
In other related work, Linand Hovy (2002) suggest pairing extracted sen-tences with their corresponding lead sentences;we have not directly compared our search-basedapproach to Lin and Hovy?s simpler method.In order to evaluate our approach, we com-pared 200-word summaries generated by oursystem to those of two baselines that are similarto those used in DUC 2001 (Harman, 2001), andto three simpler versions of the system, wherea simple marginal relevance selection procedurewas used instead of the selection search, and/orthe IE groups were ignored.
In general, wefound that our randomized local search methodprovided substantial improvements in both con-tent and intelligibility over the DUC-like base-lines and the simplest variant of our system,which used marginal relevance selection and noIE groups (with the exception that the last arti-cle baseline was always ranked first in intelligi-bility).
The use of the IE groups also appearedto contribute a small further improvement incontent when used with our selection search.We discuss these results in greater detail in thefinal section of the paper.2 System DescriptionWe have implemented our randomized localsearch method for sentence selection as partof the RIPTIDES (White et al, 2001) sys-tem.
RIPTIDES combines information extrac-tion (IE) in the domain of natural disasters andmultidocument summarization to produce hy-pertext summaries.
The hypertext summariesinclude a high-level textual overview; tables ofall comparable numeric estimates, organized tohighlight discrepancies; and targeted access tosupporting information from the original arti-cles.
In White et al (2002), we showed that thehypertext summaries can help to identify dis-repancies in numeric estimates, and provide asignificantly more complete picture of the avail-able information than the latest article.
Thenext subsection walks through a sample hyper-text summary; it is followed by descriptions ofthe IE and Summarizer system components.2.1 ExampleFigure 1 shows a textual overview of the firstdozen or so articles in a corpus of news arti-cles gathered from the web during the first weekafter the January 2001 earthquake in CentralAmerica.
Clicking on the magnifying glass iconbrings up the original article in the right frame,with the extracted sentences highlighted.The index to the hypertext summary appearsin the left frame of figure 1.
Links to theoverview and to the lead sentences of the arti-cles are followed by links to tables that displaythe base level extraction slots for the main event(here, an earthquake) including its description,date, location, epicenter and magnitude.
Accessto the overall damage estimates appears next,with separate tables for types of human effects(e.g.
dead, missing) and for object types (e.g.villages, bridges, houses) with physical effects.Figure 2 shows the extracted estimates of theoverall death toll.
In order to help identify dis-crepancies, the high and low current estimatesare shown at the top, followed by other cur-rent estimates and then all extracted estimates.Heuristics are used to determine which esti-mates to consider current, taking into accountthe source (either news source or attributedsource), specificity (e.g.
hundreds vs. at least200) and confidence level, as indicated by thepresence of hedge words such as perhaps or as-sumed.
The tables also provide links to the orig-inal articles, allowing the user to quickly anddirectly determine the accuracy of any estimatein the table.2.2 IE SystemThe IE system combines existing language tech-nology components (Bikel et al, 1997; Char-niak, 1999; Day et al, 1997; Fellbaum, 1998) ina traditional IE architecture (Cardie, 1997; Gr-ishman, 1996).
Unique features of the systeminclude a weakly supervised extraction pattern-learning component, Autoslog-XML, which isbased on Autoslog-TS (Riloff, 1996), but op-erates in an XML framework and acquires pat-terns for extracting text elements beyond nounphrases (e.g.
verb groups, adjectives, adverbs,and single-noun modifiers).
In addition, aFigure 1: Hypertext Summary OverviewFigure 2: Tables of Death Toll Estimatesheuristic-based clustering algorithm organizesthe extracted concepts into output templatesspecifically designed to support multi-documentsummarization: the IE system, for example, dis-tinguishes different reports or views of the sameevent from multiple sources (White et al, 2001).Output templates from the IE system for eachtext to be covered in the multi-document sum-mary are provided as input to the summariza-tion component along with all linguistic anno-tations accrued in the IE phase.2.3 SummarizerThe Summarizer operates in three main stages.In the first stage, the IE output templates aremerged into an event-oriented structure wherecomparable facts are semantically grouped.
To-wards the same objective, surface-oriented clus-tering is used to group sentences from differentdocuments into clusters that are likely to reportsimilar content.
In the second stage, importancescores are assigned to the sentences based on thefollowing indicators: position in document, doc-ument recency, presence of quotes, average sen-tence overlap, headline overlap, size of cluster(if any), size of semantic groups (if any), speci-ficity of numeric estimates, and whether theseestimates are deemed current.
In the third andfinal stage, the hypertext summary is generatedfrom the resulting content pool.
Further detailson each stage follow in the paragraphs below;see White et al (2002) for a more complete de-scription.In the analysis stage, we use Columbia?sSimFinder tool (Hatzivassiloglou et al, 2001) toobtain surface-oriented similarity measures andclusters for the sentences in the input articles.To obtain potentially more accurate partitionsusing the IE output, we semantically merge theextracted slots into comparable groups, i.e.
oneswhose members can be examined for discrepan-cies.
This requires distinguishing (i) differenttypes of damage; (ii) overall damage estimatesvs.
those that pertain to a specific locale; and(iii) damage due to related events, such as previ-ous quakes in the same area.
During this stage,we also analyze the numeric estimates for speci-ficity and confidence level, and determine whichestimates to consider current.In the scoring stage, SimFinder?s similaritymeasures and clusters are combined with thesemantic groupings obtained from merging theIE templates in order to score the input sen-tences.
The scoring of the clusters and seman-tic groups is based on their size, and the scoresare combined at the sentence level by includ-ing the score of all semantic groups that con-tain a phrase extracted from a given sentence.More precisely, the scores are assigned in threephases, according to a set of hand-tuned pa-rameter weights.
First, a base score is assignedto each sentence according to a weighted sumof the position in document, document recency,presence of quotes, average sentence overlap,and headline overlap.
The average sentenceoverlap is the average of all pairwise sentencesimilarity measures; we have found this measureto be a useful counterpart to sentence positionin reliably identifying salient sentences, with theother factors playing a lesser role.
In the secondscoring phase, the clusters and semantic groupsare assigned a score according to the sum of thebase sentence scores.
After normalization in thethird scoring phase, the weighted cluster andgroup scores are used to boost the base scores,thereby favoring sentences from the more im-portant clusters and semantic groups.
Finally,a small boost is applied for currenten and morespecific numeric estimates.In the generation stage, the overview is con-structed by selecting a set of sentences in acontext-sensitive fashion, and then ordering theblocks of adjacent sentences according to theirimportance scores.
The summarization scoringmodel begins with the sum of the scores forthe candidate sentences, which is then adjustedto penalize the inclusion of multiple sentencesfrom the same cluster or semantic group, or sen-tences whose similarity measure is above a cer-tain threshold, and to favor the inclusion of ad-jacent sentences from the same article, in orderto boost intelligibility.
A larger bonus is appliedwhen including a sentence that begins with aninitial pronoun as well as the previous one, andan even bigger bonus is added when includinga sentence that begins with a strong rhetoricalmarker (e.g.
however) as well as its predecessor;corresponding penalties are also used when thepreceding sentence is missing, or when a shortsentence appears without an adjacent one.To select the sentences for the overview ac-cording to this scoring model, we use an itera-tive randomized local search procedure inspiredby Selman and Kautz (1994).
Two noise strate-gies are employed to lessen the problem of lo-cal maxima in the search space: (i) the localsearch is restarted from random starting points,for a fixed number of iterations, and (ii) duringeach local search iteration, greedy steps are in-terleaved with random steps, where a sentenceis added regardless of its score.
In the first localsearch iteration, the initial sentence collectionconsists of the highest scoring sentences up tothe word limit.
In subsequent iterations, the ini-tial collection is composed of randomly selectedsentences, weighted according to their scores,up to the word limit.
During each local searchiteration, a random step or a greedy step (cho-sen at random) is repeatedly performed until agreedy step fails to improve upon the currentcollection of sentences.
In each greedy step, onesentence is chosen to add to the collection, andzero or more (typically one) sentences are cho-sen to remove from the collection, such that theword limit is still met, and this combination ofsentences represents the best swap available ac-cording to the scoring model.
After the prede-termined number of iterations, the best combi-nation of sentences found during the search isoutput; note that the algorithm could easily beformulated in an anytime fashion as well.
Froma practical perspective, we have found that 10iterations often suffices to find a reasonable col-lection of sentences, taking well under a minuteon a desktop PC.Once the overview sentences have been se-lected, the hypertext summary is generated as acollection of HTML files, using a series of XSLTtransformations.2.4 Training and TuningFor the evaluation below, the IE system wastrained on 12 of 25 texts from topic 89 of theTDT2 corpus, a set of newswires that describethe May 1998 earthquake in Afganistan.
Itachieves 42% recall and 61% precision whenevaluated on the remaining 13 topic 89 texts.The parameter settings of the Summarizer werechosen by hand using the complete TDT2 topic89 document set as input.3 Evaluation Method and ResultsTo select the inputs for the evaluation, we tookfive subsets of the articles from TDT2 topic89 ?
all the articles up to the end of days 1through 5 after the quake.
We chose to useTDT2 topic 89 so that we could assess the im-pact of the IE quality on the results, given thatwe had previously created manual IE annota-tions for these articles (White et al, 2001).11Although our decision to use subsets of TDT2 topic89 as inputs meant that our training/tuning and testdata overlapped, we do not believe that this choice overlycompromises our results, since ?
as will be discussed inthis section and the next ?
the impact of the IE groupsFor each input document set, we ran the RIP-TIDES system to produce overview summariesof 200 words or less.
For comparison purposes,we also ran two baselines, similar to those usedin DUC 2001 (Harman, 2001), and three sim-pler versions of the system, for a total of sixsummary types:Last The first N sentences of the latest articlein the document set, up to the word limit.Leads The lead sentences from the latest arti-cles in the document set, up to the wordlimit, listed in chronological order.MR The top ranking sentences selected accord-ing to their thresholded marginal relevance,up to the word limit, listed in chronologicalorder, using RIPTIDES to score the sen-tences, except with the IE groups zeroedout.MR+IE The MR summarization method, butwith the IE groups included for the RIP-TIDES sentence scorer.Search The RIPTIDES overview, except withthe IE groups zeroed out for sentence scor-ing.Search+IE The RIPTIDES overview.The marginal relevance systems (MR andMR+IE) used a simple selection mechanismwhich does not involve search, inspired by themaximal marginal relevance (MMR) approach(Goldstein et al, 2000).
This selection mech-anism begins by selecting the top ranking sen-tence for inclusion, then determines whether toinclude the second ranking sentence dependingon whether it is sufficiently dissimilar from thefirst one, based on comparing the SimFindersimilarity measure against a hard threshold(0.85), and likewise for lower ranked sentences,comparing them against all sentences includedso far, up to the word limit.
The selected sen-tences are then gathered into blocks of adjacentsentences, and ordered chronologically.2turned out to be small, while with our selection search,we ran into a couple of problems on the test data thatdid not show up in tuning the parameter settings.2In trying out the MR systems on all the articles inTDT2 topic 89, we found chronological ordering to usu-ally be more coherent than importance ordering.Content Rank, Simulated IELast Leads MR MR+IE Search Search+IEDay 1 5, 6 5, 5 4, 4 3, 3 1, 1 1, 1Day 2 5, 4 6, 6 1, 4 4, 3 2, 1 1, 2Day 3 6, 6 3, 3 3, 3 5, 5 1, 1 1, 1Day 4 6, 5 3, 6 3, 4 3, 2 2, 2 1, 1Day 5 6, 6 2, 5 2, 4 2, 2 2, 2 1, 1Average 5.5 ?0.7 4.4 ?1.5 3.2 ?1.0 3.1 ?1.2 1.6 ?0.7 1.1 ?0.3Table 1: Content Rankings on TDT2 Topic 89, using Simulated IE.
The scores for the two judgesat each time point are separated by commas.Intelligibility Rank, Simulated IELast Leads MR MR+IE Search Search+IEDay 1 1, 1 5, 2 6, 6 3, 2 2, 4 3, 5Day 2 1, 1 5, 5 6, 5 4, 4 2, 2 2, 3Day 3 1, 1 6, 5 5, 6 3, 4 3, 1 2, 1Day 4 1, 1 6, 6 5, 5 3, 3 3, 4 2, 2Day 5 1, 1 4, 6 4, 5 4, 3 2, 4 2, 2Average 1 ?0 5 ?1.2 5.3 ?0.7 3.3 ?0.7 2.7 ?1.1 2.4 ?1.1Table 2: Intelligibility Rankings on TDT2 Topic 89, using Simulated IE.Content Rank, Actual IELast Leads MR MR+IE Search Search+IEDay 1 5, 6 5, 5 4, 4 3, 3 2, 1 1, 1Day 2 5, 4 6, 6 1, 4 3, 3 3, 2 1, 1Day 3 6, 6 1, 2 1, 2 5, 5 3, 1 3, 2Day 4 6, 5 4, 6 1, 2 5, 4 1, 1 1, 2Day 5 6, 6 2, 5 4, 2 4, 4 2, 2 1, 1Average 5.5 ?0.7 4.2 ?1.9 2.5 ?1.4 3.9 ?0.9 1.8 ?0.8 1.4 ?0.7Table 3: Content Rankings on TDT2 Topic 89, using Actual IE.Intelligibility Rank, Actual IELast Leads MR MR+IE Search Search+IEDay 1 1, 1 5, 2 6, 6 3, 2 2, 4 3, 5Day 2 1, 1 5, 5 6, 5 4, 4 2, 2 2, 2Day 3 1, 1 6, 4 5, 6 2, 3 2, 2 2, 4Day 4 1, 1 6, 6 4, 4 4, 3 3, 2 2, 4Day 5 1, 1 4, 6 4, 5 4, 2 3, 4 2, 3Average 1 ?0 4.9 ?1.3 5.1 ?0.9 3.1 ?0.9 2.6 ?0.8 2.9 ?1.1Table 4: Intelligibility Rankings on TDT2 Topic 89, using Actual IE.TDT2 Topic 89, Simulated IE11.522.533.544.555.56LastLeads MRMR+IESearchSearch+IESystemAvgRank ContentIntelligibilityFigure 3: Average System Rank for Content and Intelligibility on TDT2 Topic 89, using SimulatedIE.
The ranks are averaged across two judges and five time points; manual IE annotations wereused with the MR+IE and Search+IE systems.TDT2 Topic 89, Actual IE11.522.533.544.555.56LastLeads MRMR+IESearchSearch+IESystemAvgRank ContentIntelligibilityFigure 4: Average System Rank for Content and Intelligibility on TDT2 Topic 89, using ActualIE.
The ranks are averaged across two judges and five time points; actual IE annotations were usedwith the MR+IE and Search+IE systems, making all systems fully automatic.For each of the five time points, we ran thesix systems on two versions of the input docu-ment sets, one with the manual IE annotations(simulated IE) and one with the automatic IEannotations (actual IE).
Note that with each ofthe first three systems, the output did not differfrom one version of the input to the other, sincethese systems did not depend on the IE anno-tations and did not involve randomized search.Next, for each document set, we had two judges3rank the summaries from best to worst, with3The authors were the judges.Significant Differences in System VersionsLast Leads MR MR+IE Search Search+IELast Int(Sim,Act) Con(Sim,Act) Con(Sim,Act) Con(Sim,Act) Con(Sim,Act)Int(Sim,Act) Int(Sim,Act) Int(Sim,Act) Int(Sim,Act)Leads - Int(Sim,Act) Con(Sim,Act) Con(Sim,Act)Int(Sim,Act) Int(Sim,Act)MR Int(Sim,Act) Con(Sim) Con(Sim)Int(Sim,Act) Int(Sim,Act)MR+IE Con(Act) Con(Sim,Act)Search -Table 5: Significant Differences in System Versions based on Pairwise t-Tests.
Table entries indicatestatistically significant differences (at the 95% confidence level) in mean Content and Intelligibilityrank on TDT2 Topic 89 texts using Simulated or Actual output from the IE system.ties allowed, in two categories, content and in-telligibility.
In the case of ties, the tied systemsshared the appropriate ranking; for example, iftwo summaries tied for the best content, eachreceived a rank of 1, with the next best sum-mary receiving a rank of 3 (cf.
Olympic pairsskating).The charts in figures 3 and 4 show the sys-tem rank for content and intelligibility for thesimulated IE and actual IE versions of the doc-ument sets, respectively, averaged across thetwo judges and five time points.
Tables 1through 4 list all the judgements together withtheir means and standard deviations.In general, we found that Search andSearch+IE provided substantial improvementsin both content and intelligibility over Last,Leads and MR, with the exception thatLast was always ranked first in intelligibility.Search+IE also appeared to show a small fur-ther improvement in content.Determining the significance of the improve-ments is somewhat complex, due to the smallnumber of data points and the use of multiplecomparisons.
To judge the significance levels,we calculated pairwise t-tests for all the meanslisted in tables 1 through 4, and applied theBonferroni adjustment, which is a conservativeway to perform multiple comparisons where thetotal chance of error is spread across all com-parisons.44With the total ?
equal to 0.05, the Bonferroni ad-justment provides a 95% confidence level that all thepairwise judgements are correct.
In our case, a total ?of 0.05 corresponds to an individual ?
of 0.0033, whichis difficult to exceed with a small number of data points.Statisitically significant differences in systemperformance at the 95% confidence level appearin table 5.
Turning first to the content rankings,with the simulated IE output, we found thatboth Search and Search+IE scored significantlyhigher than Last, Leads and MR.
While the dif-ference between Search and Search+IE was notsignificant, only Search+IE achieved a signifi-cantly higher average rank than MR+IE.
Withthe actual IE output, Search and Search+IEagain scored significantly higher than Last andLeads and, although these two systems did notshow a significant improvement over MR, bothsystems did improve significantly over Leadsand MR+IE, in contrast to MR.Turning now to the intelligibility rankings,with both the simulated and actual IE and, wefound that Search and Search+IE improved sig-nificantly over Leads and MR.
The differencebetween Search and Search+IE was not signifi-cant.
Surprisingly, MR+IE scored significantlyhigher than MR, and not significantly worsethan Search and Search+IE.4 DiscussionWe were pleased with the substantial improve-ments in both content and intelligibility thatour randomized local search method providedover the DUC-like baselines and the simplestvariant of our system, the one using marginalrelevance selection and no IE groups (with theexception that the last article baseline was al-ways ranked first in intelligibility).
We did notexpect to find that the selection search wouldyield substantial improvements over marginalrelevance selection in the content rankings, sincethe search method was designed to improve in-telligibility without unduly affecting content.At the same time though, we were somewhatdisappointed that the use of the IE groups ap-peared to only contribute a small further im-provement in content when used with our selec-tion search.It is not entirely clear why our selectionsearch method led to improvements in the con-tent rankings when compared to the marginalrelevance variants.
One possibility is that therandomized local search was able to find sen-tences with greater information density.
An-other possibility is that the use of a hard thresh-old by the marginal relevance variants led tosome poor sentence selections fairly far down onthe list of ranked sentences; the marginal rele-vance selection may have worked better had weused a smaller threshold, or if we had re-rankedthe sentences following each selection accordingto a redundancy penalty, rather than simply us-ing a threshold.It is also not clear why the IE groups didnot help more with content selection.
It maywell be that a more elaborate evaluation, involv-ing more systems and judgements, would indeedshow that the IE groups yielded significant im-provements in content rankings.
On the otherhand, our results may indicate that shallow ex-tractive techniques can pick up much the sameinformation as IE techniques, at least for thepurpose of selecting sentences for generic extrac-tive summaries.
Note that for purposes of dis-crepancy detection, the ability of IE techniquesto more completely extract relevant phrases isclearly demonstrated in White et al (2002).On the intelligibility side, we were surprisedto find that the IE groups led to improvementsin intelligibility when used with marginal rele-vance selection.
One likely explanation for thisimprovement is that this system variant jumpedaround less from topic to topic than its counter-part that did not make use of the IE info.Another question is why the selection searchdid not yield further improvements in intelli-bility.
One reason is that the search methodalways selected sentences up to the word limit,even when this yielded highly repetitive sum-maries ?
as was the case with the first two testsets, which only contained a handful of articles.Another reason is that the search routine wasprone to selecting a couple of sentences from anarticle that was largely off topic, only containinga brief mention of the quake.These deficiencies point to possible improve-ments in the search method: informativenesscould perhaps be balanced with conciseness bydeselecting sentences that do not improve theoverall score; and off-topic sentences could per-haps be avoided by taking into account the cen-trality of the document in the sentence scores.More speculatively, it would be interesting toextend the approach to work with sub-sententialunits, and to make use of a greater variety ofinter-sentential cohesive links.AcknowledgmentsThis work was supported in part by DARPATIDES contract N66001-00-C-8009 and NSFGrants 0081334 and 0074896.
We thankTanya Korelsky, Daryl McCullough, VincentNg, David Pierce and Kiri Wagstaff for theirhelp with earlier versions of the system.ReferencesRegina Barzilay, Noemie Elhadad, and Kath-leen McKeown.
2001.
Sentence Ordering inMultidocument Summarization.
In Proceed-ings of the First International Conference onHuman Language Technology Research, SanDiego, CA.D.
Bikel, S. Miller, R. Schwartz, andR.
Weischedel.
1997.
Nymble: A High-Performance Learning Name-Finder.
In Pro-ceedings of the Fifth Conference on AppliedNatural Language Processing, pages 194?201,San Francisco, CA.
Morgan Kaufmann.C.
Cardie.
1997.
Empirical Methods in Infor-mation Extraction.
AI Magazine, 18(4):65?79.Eugene Charniak.
1999.
A maximum-entropy-inspired parser.
Technical Report CS99-12,Brown University.D.
Day, J. Aberdeen, L. Hirschman,R.
Kozierok, P. Robinson, and M. Vi-lain.
1997.
Mixed-Initiative Developmentof Language Processing Systems.
In Pro-ceedings of the Fifth Conference on AppliedNatural Language Processing.
Association forComputational Linguistics.C.
Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge,MA.J.
Goldstein, V. Mittal, J. Carbonell, andM.
Kantrowitz.
2000.
Multi-document sum-marization by sentence extraction.
In Pro-ceedings of the ANLP/NAACL Workshop onAutomatic Summarization, Seattle, WA.R.
Grishman.
1996.
TIPSTER Archi-tecture Design Document Version 2.2.Technical report, DARPA.
Available athttp://www.tipster.org/.Donna Harman.
2001.
Proceedings of the 2001Document Understanding Conference (DUC-2001).
NIST.Vasileios Hatzivassiloglou, Judith L. Klavans,Melissa L. Holcombe, Regina Barzilay, Min-Yen Kan, and Kathleen R. McKeown.
2001.Simfinder: A flexible clustering tool for sum-marization.
In Proceedings of the NAACL2001 Workshop on Automatic Summariza-tion, Pittsburgh, PA.Chin-Yew Lin and Eduard Hovy.
2002.
Au-tomated Multi-document Summarization inNeATS.
In Proceedings of the Second In-ternational Conference on Human LanguageTechnology Research, San Diego, CA.
To ap-pear.Daniel Marcu.
2001.
Discourse-Based Sum-marization in DUC-2001.
In Proceedings ofthe 2001 Document Understanding Confer-ence (DUC-2001).E.
Riloff.
1996.
Automatically Generating Ex-traction Patterns from Untagged Text.
InProceedings of the Thirteenth National Con-ference on Artificial Intelligence, pages 1044?1049, Portland, OR.
AAAI Press / MITPress.Bart Selman and Henry Kautz.
1994.
NoiseStrategies for Improving Local Search.
InProceedings of AAAI-94.Michael White, Tanya Korelsky, Claire Cardie,Vincent Ng, David Pierce, and Kiri Wagstaff.2001.
Multidocument Summarization via In-formation Extraction.
In Proceedings of theFirst International Conference on HumanLanguage Technology Research, San Diego,CA.Michael White, Claire Cardie, Vincent Ng, andDaryl McCullough.
2002.
Detecting Discrep-ancies in Numeric Estimates Using Multidoc-ument Hypertext Summaries.
In Proceedingsof the Second International Conference onHuman Language Technology Research, SanDiego, CA.
To appear.
