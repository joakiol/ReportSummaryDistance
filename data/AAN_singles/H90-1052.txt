Structural Ambiguity and Lexical RelationsDonald Hindle and Mats RoothAT&T Bell Labs600 Mountain Ave.Murray Hill, NJ 07974IntroductionFrom a certain (admittedly narrow) perspective, one ofthe annoying features of natural language is the ubiq-uitous syntactic ambiguity.
For a computational modelintended to assign syntactic descriptions to natural lan-guage text, this seem like a design defect.
In general,when context and lexical content are taken into account,such syntactic ambiguity can be resolved: sentences usedin context show, for the most part, little ambiguity.
Butthe grammar provides many alternative analyses, andgives little guidance about resolving the ambiguity.Prepositional phrase attachment is the canonical caseof structural ambiguity, as in the time worn example,(1) I saw the man with the telescopeThe problem arises because the grammar provides sev-eral sources for prepositional phrases.
The prepositionalphrase with the telescope has two central attachment pos-sibilities (the seeing is by means of a telescope or theman has a telescope), licensed by two different phrasestructure rules, namelyV P - .
N P  P PandN P  + N' P P(The prepositional phrase might also attach to thesubject noun phrase I; in this paper we will concentrateon the most important binary choice between attach-ment to the adjacent Noun Phrase, and attachment tothe preceding Verb.
)The existence of such ambiguity raises problems forunderstanding and for language models.
It looks like itmight require extremely complex computation to deter-mine what attaches to what.
Indeed, one recent pro-posal suggests that resolving attachment ambiguity re-quires the construction of a discourse model in whichthe entities referred to in a text must be reasoned about(Altmann and Steedman 1988).Of course, if attachment ambiguity demands referenceto semantics and discourse models, there is little hopein the near term of building computational models forunrestricted text to resolve the ambiguity.Structure based ambiguity resolutionThere have been several structure-based proposals aboutambiguity resolution in the literature; they are particu-larly attractive because they are simple and don't de-mand calculations in the semantic or discourse domains.The two main ones are:Right Association - a constituent tends to attach toanother constituent immediately to its right (Kim-ball 1973).Minimal Attachment - a constituent tends to attachso as to involve the fewest additional syntactic nodes(Frazier 1978).For the particular case we are concerned with, attach-ment of a prepositional phrase in a verb + object con-text as in sentence (I),  these two principles - at leastin the version of syntax that Frazier assumes - makeopposite predictions: Right Association predicts nounattachment, while Minimal Attachment predicts verb at-tachment.Unfortunately, these structure-based disambiguationproposals seem not to account for attachment prefer-ences very well.
A recent study of attachment of prepo-sitional phrases in a sample of written responses to a"Wizard of Oz" travel information experiment showsthat niether Right Association nor Minimal Attachmentaccount for more than 55% of the cases (Whittemore etal.
1990).
And experiments by Taraban and McClelland(1988) show that the structural models are not in factgood predictors of people's behavior in resolving ambi-guity.Resolving ambiguity through lexical asso-ciationsWhittemore et al (1990) found lexical preferences tobe the key to resolving attachment ambiguity.
Similarly,Taraban and McClelland found lexical content was key inexplaining people's behavior.
Various previous propos-als for guiding attachment disambiguation by the lexicalcontent of specific words have appeared (e.g.
Ford, Bres-nan, and Kaplan 1982; Marcus 1980).
Unfortunately, itis not clear where the necessary information about lexi-cal preferences is to be found.
In the Whittemore et alstudy, the judgement of attachment preferences had tobe made by hand for exactly the cases that their studycovered; no precompiled list of lexical preferences wasavailable.
Thus, we are posed with the problem: howcan we get a good list of lexical preferences.Our proposal is to use cooccurrence of with preposi-tions in text as an indicator of lexical preference.
Thus,for example, the preposition to occurs frequently in thecontext send NP --, i.e., after the object of the verbsend, and this is evidence of a lexical association of theverb send with to.
Similarly, from occurs frequently inthe context withdrawal --, and this is evidence of a lex-ical association of the noun withdrawal with the prepo-sition from.
Of course, this kind of association is, unlikelexical preference, a symmetric notion.
Cooccurrenceprovides no indication of whether the verb is selectingthe preposition or vice versa.
We will treat the associa-tion as a property of the pair of words.
It is a separatematter, which we unfortunately cannot pursue here, toassign the association to a particular linguistic licens-ing relation.
The suggestion which we want to exploreis that the association revealed by textual distribution- whether its source is a complementation relation, amodification relation, or something else - gives us infor-mation needed to resolve the prepositional attachment.Discovering Lexical Associat ion inTextA 13 million word sample of Associated Press new sto-ries from 1989 were automatical ly parsed by the Fidditchparser (Hindle 1983), using Church's part of speech an-alyzer as a preprocessor (Church 1988).
From the syn-tactic analysis provided by the parser for each sentence,we extracted a table containiffg all the heads of all nounphrases.
For each noun phrase head, we recorded thefollowing preposition if any occurred (ignoring whetheror not the parser attached the preposition to the nounphrase), and the preceding verb if the noun phrase wasthe object of that verb.
Thus, we generated a table withentries including those shown in Table 1.VERBblamecontrolenragegrantHEAD NOUN PREPPASSIVE formoney fordevelopmentgovernmentmil itaryaccordradicalWHPl~Oitconcession toTable h A sample of the Verb-Noun-Preposition table.In this Table 1, the first line represents a passivizedinstance of the verb blame followed by the prepositionfor.
The second line is an instance of a noun phrasewhose head is money; this noun phrase is not an objectof any verb, but is followed by the preposition for.
Thethird line represents an instance of a noun phrase withhead noun development which neither has a followingpreposition or is the object of a verb.
The fourth lineis an instance of a noun phrase with head government,which is the object of the verb control but is followed byno preposition.
The last line represents an instance ofthe ambiguity we are concerned with resolving: a nounphrase (head is concession), which is the object of a verb(grant), followed by a preposition (to).From the 13 million word sample, 2,661,872 nounphrases were identified.
Of these, 467,920 were recog-nized as the object of a verb, and 753,843 were followedby a preposition.
Of the noun phrase objects identified,223,666 were ambiguous verb-noun-preposition triples.Est imating attachment prefer-encesOf course, the table of verbs, nouns and prepositionsdoes not directly tell us what the lexical associationsare.
This is because when a preposition follows a nounphrase, it may or may not be structurally related to thatnoun phrase (in our terms, it may attach to that nounphrase or it may attach somewhere lse).
What  we wantto do is use the verb-noun-preposition table to derivea table of bigrams, where the first term is a noun orverb, and the second term is an associated preposition(or no preposition).
To do this we need to try to assigneach preposition that occurs either to the noun or tothe verb that it occurs with.
In some cases it is fairlycertain that the preposition attaches to the noun or theverb; in other cases, it is far less certain.
Our approachis to assign the clear cases first, then to use these todecide the unclear cases that can be decided, and finallyto arbitrarily assign the remaining cases.
The procedurefor assigning prepositions in our sample to noun or verbis as follows:1.
No Preposition - if there is no preposition, the nounor verb is simply counted with the null preposition.2.
Sure Verb Attach 1 - preposition is attached to theverb if the noun phrase head is a pronoun.3.
Sure Verb Attach 2 - preposition is attached to theverb if the verb is passivized (unless the prepositionis by.
The instances of by following a passive verbwere left unassigned.)4.
Sure Noun Attach - preposition is attached to thenoun, if the noun phrase occurs in a context whereno verb could license the prepositional phrase (i.e.,the noun phrase is in subject or pre-verbal position.)5.
Ambiguous Attach 1 - Using the table of attachmentso far, if a t-score for the ambiguity (see below) is258greater than 2.1 or less than -2.1, then assign thepreposition according to the t-score.
Iterate throughthe ambiguous triples until all such attachments aredone.Ambiguous Attach 2 - for the remaining ambiguoustriples, split the attachment between the noun andthe verb, assigning .5 to the noun and .5 to the verb.Unsure Attach - for the remaining pairs (all of whichare either attached to  the preceding noun or to someunknown element), assign them to  the noun.This procedure gives us a table of bigrams representingour guess about what prepositions associate with whatnouns or verbs, made on the basis of the distribution ofverbs nouns and prepositions in our corpus.The procedure for guessing attachmentGiven the table of bigrams, derived as described above,we can define a simple procedure for determining the at-tachment for an instance of verb-noun-preposition am-biguity.
Consider the example of sentence (2), where wehave to choose the attachment given verb send, nounsoldier, and preposition into.
(2) Moscow sent more than 100,000 soldiersinto Afganistan .
.
.The idea is to contrast the probability with which intooccurs with the noun soldier with the probability withwhich into occurs with the verb send.
A t-score is anappropriate way to  make this contrast (see Church etal.
to appear).
In general, we want to calculate thecontrast between the conditional probability of seeing aparticular preposition given a noun with the conditionalprobability of seeing that preposition given a verb.P(prep I noun) - P(prep I verb)t EJ u 2 ( ~ ( ~ r e ~  ( noun)) + a2(P(prep I verb))We use the "Expected Likelihood Estimate" (Churchet al, to appear) to estimate the probabilities, in or-der to adjust for small frequencies; that is, we simplyadd 112 to all frequency counts (and adjust the denom-inator appropriately).
This method leaves the order oft-scores nearly intact, though their magnitude is inflatedby about 30%.
To compensate for this, the 1.65 thresh-old for significance a t  the 95% level should be adjustedup to about 2.15.Consider how we determine attachment for sentence(4).
We use a t-score derived from the adjusted frequen-cies in our corpus to decide whether the prepositionalphrase into Afganistan is attached to the verb (root)send/V or to the noun (root) soldier/N.
In our cor-pus, soldier/N has an adjusted frequency of 1488.5, andsend/V has an adjusted frequency of 1706.5; soldier/Noccurred in 32 distinct preposition contexts, and send/Vin 60 distinct preposition contexts; f(send/V into) = 84,f(soldier/N into) = 1.5.From this we calculate the t-score as fo1lows:lP(wlsoldier/N) - P(w)send/ V) t rdu2(~(wlsoldier /N))  + u2(P(wlsend/ V))j(soldier/N into)+l/2 - j(soldier/N)+V/2Mf soldier N into +I 2 send V into +1 2 J ~ ( s r l d i b / i i ) + ?
/ ~ ~  +This figure of -8.81 represents a significant associationof the preposition into with the verb send, and on thisbasis, the procedure would (correctly) decide that intoshould attach to send rather than to soldier.Testing Attachment PreferenceWe have outlined a simple procedure for determiningprepositional phrase attachment in a verb-object con-text.
To evaluate the performance of this procedure, weneed a graded set of attachment ambiguities.
First, thetwo authors graded a set of verb-noun-preposition triplesas follows.
From the AP new stories, we randomly st+lected 1000 test sentences in which the parser identifiedan ambiguous verb-noun-preposition triple.
(These sen-tences were selected from stories included in the 13 mil-lion word sample, but the particular sentences were ex-cluded from the calculation of lexical associations.)
Forevery such triple , each author made a judgement of thecorrect attachment on the basis of the three words alone(forced choice - preposition attaches to  noun or verb).This task is in essence the one that we will give the com-puter - i.e., to judge the attachment without any moreinformation than the preposition and the head of the twopossible attachment sites, the noun and the verb.
Thisgave us two sets ofjudgements to compare the algorithmsperformance to.Judging correct attachmentWe also wanted a standard of correctness for these testsentences.
To derive this standard, each author inde-pendently judged the attachment for the 1000 triples asecond time, this time using the full sentence context.It  turned out to be a surprisingly difficult task toassign attachment preferences for the test sample.
Ofcourse, many decisions were straightforward, but morethan 10% of the sentences seemed problematic to a t  leastone author.
There are two main sources of such difficulty.First, it is unclear where the preposition is attached inidiomatic phrases such as :'V is the number of distinct prepositioncontexts for either sol-dier/N or send/V; in this case V = 70.
It is required by theExpected Likelihood Estimator method so that the sum of theestimated probabilities will be one.
(3) But over time , misery has given way tomending.
(4) The meeting will take place in QuanticoEva luat ing  per fo rmanceA second major source of difficulty arose from caseswhere the attachment either seemed to make no differ-ence semantically or it was impossible to decide whichattachment was correct, as(5) We don't have preventive detention in theUnited States.
(6) Inaugural officials reportedly were trying toarrange a reunion for Bush and his old subma-rine buddies .
.
.It seems to us that this difficulty in assigning attach-ment decisions is an important fact that deserves furtherexploration.
If it is difficult to decide what licenses aprepositional phrase a significant proportion of the time,then we need to develop language models that appropri-ately capture this vagueness.
For our present purpose,we decided to force an attachment choice in all cases, insome cases making this choice arbitrarily.In addition to the problematic ases, a significantnumber (111) of the 1000 triples identified automaticallyas instances of the verb-object-preposition c figurationturned out in fact to be other constructions.
Thesemisidentifications were mostly due to parsing errors, andin part due to our underspecifying for the parser ex-actly what configuration to identify.
Examples of thesemisidentifications include: identifying the subject of thecomplement clause of say as its object, as in (7), whichwas identified as (say ministers from); misparsing twoconstituents as a single object noun phrase, as in (8),which was identified as (make subject o); and countingnon-object noun phrases as the object as in (9), identi-fied as (get hell out_o\]).
(7) Ortega also said deputy foreign ministersfrom the five governments would meet Tuesdayin Managua, .
.
.
(8) Congress made a deliberate choice to makethis commission subject o the open meeting re-quirements .
.
.
(9) Student Union, get the hell out of China!First, consider how the simple structural attachmentpreference schemas do at predicting the outcome in ourtest set.
Right Association, which predicts noun attach-ment does better, since there are more noun attach-ments, but it still has an error rate of 36%.
MinimalAttachment, interpreted to mean verb attachment hasthe complementary error rate of 64%.
Obviously, neitherof these procedures is particularly impressive.
For oursample, the simple strategy of attaching a prepositionalphrase to the nearest constituent is the more successfulstrategy.Now consider the performance of our attachment pro-cedure for the 889 standard test sentences.
Table 2shows the results on the test sentences for the two humanjudges and for the attachment procedure.\] choice \[ % correct \[N V N V totalJudge 1 ~Judge 2LATable 2: Performance on the test sentences for 2 humanjudges and the lexical association procedure (LA).Of course these errors are folded into the calculationof associations.
No doubt our bigram model would bebetter if we could eliminate these items, but many ofthem represent parsing errors that obviously cannot beidentified by the parser, so we proceed with these errorsincluded in the bigrams.After agreeing on the "correct" attachment for thesample of 1000 triples, we are left with 889 verb-noun-preposition triples (having discarded the 111 parsing er-rors).
Of these, 568 are noun attachments and 321 verbattachments.First, we note that the task of judging attachment onthe basis of verb, noun and preposition alone is not easy.Both human judges had overall error rates of nearly 15%.
(Of course this is considerably better than always choos-ing the nearest attachment site.)
The lexical associationprocedure based on t-scores is somewhat worse than thehuman judges, with an error rate of 22%, again an im-provement over simply choosing the nearest attachmentsite.260If we restrict the lexical association procedure tochoose attachment only in cases where its confidence isgreater than about 95% (i.e., where t is greater than2.1), we get attachment judgements on 608 of the 889test sentences, with an overall error rate of 15% (Ta-ble 3).
On these same sentences, one human judge alsoshowed slight improvement.choice I % correctN I V I N I V 1 totalTable 3: Performance on the test sentences for 2 humanjudges and the lexical association procedure (LA) for testtriples where t > 2.1Comparison with a DictionaryThe idea that lexical preference is a key factor in re-solving structural ambiguity leads us naturally to askwhether existing dictionaries can provide useful informa-tion for disambiguation.
To investigate this question, weturn to the Collins Cobuild English Language Dictionary(Sinclair et al 1987).
This dictionary is appropriate forcomparing with the AP sample for several reasons: itwas compiled on the basis of a large text corpus, andthus may be less subject to idiosyncrasy than more arbi-trarily constructed works; and it provides, in a separatefield, a direct indication of prepositions typically associ-ated with many nouns and verbs.From a machine-readable version of the dictionary, weextracted a list of 1535 nouns associated with a particu-lar preposition, and of 1193 verbs associated with a par-ticular preposition after an object noun phrase.
These2728 associations are many fewer than the number ofassociations found in the AP sample.
(see Table 4.
)Of course, most of the preposition association pairsfrom the AP sample end up being non-significant; ofthe 88,860 pairs, fewer than half (40,869) occur witha frequency greater than 1, and only 8337 have a t-score greater than 1.65.
So our sample gives about threetimes as many significant preposition associations as theCOBUILD dictionary.
Note however, as Table 4 shows,the overlap is remarkably good, considering the largespace of possible bigrams.
(In our bigram table there areover 20,000 nouns, over 5000 verbs, and over 90 prepo-sitions.)
On the other hand, the lack of overlap for somany cases - assuming that the dictionary and the sig-nificant bigrams actually record important prepositionassociations - indicates that 1) our sample is too small,and 2) the dictionary coverage is widely scattered.First, we note that the dictionary chooses attachmentsin 182 cases of the 889 test sentences.
Seven of these arecases where the dictionary finds an association betweenthe preposition and both the noun and the verb.
In thesecases, of course, the dictionary provides no informationto help in choosing the correct attachment.Looking at the 175 cases where the dictionary findsone and only one association for the preposition, we canask how well it does in predicting the correct attachment.Here the results are no better than our human judges orthan our bigram procedure.
Of the 175 cases, in 25 casesthe dictionary finds a verb association when the correctassociation is with the noun.
In 3 cases, the dictionaryfinds a noun association when the correct associationis with the verb.
Thus, overall, the dictionary is 86%correct.It may be unfair to use a dictionary as a source ofdisambiguation information.
There is no reason to ex-pect that the dictionary aims to provide information onall significant associations; it may record only associa-tions that are interesting for some reason (perhaps be-cause they are semantically unpredictable.)
But fromthe standpoint of a language model, the fact that thedictionary provides no help in disambiguation for about80% of the ambiguous triples considerably diminishes itsusefulness.ConclusionOur attempt to use lexical associations derived from dis-tribution of lexical items in text shows promising results.Despite the errors in parsing introduced by automati-cally analyzing text, we are able to extract a good list ofassociations with preposition, overlapping significantlywith an existing dictionary.
This information could eas-ily be incorporated into an automatic parser, and ad-ditional sorts of lexical associations could similarly bederived from text.
The particular approach to decid-ing attachment by t-score gives results nearly as goodas human judges given the same information.
Thus, weconclude that it may not be necessary to resort to a com-plete semantics or to discourse models to resolve manypernicious cases of attachment ambiguity.It is clear however, that the simple model of attach-ment preference that we have proposed, based only onthe verb, noun and preposition, is too weak to makecorrect attachments in many cases.
We need to exploreways to enter more complex calculations into the proce-dure.
In particular, it will be necessary to include infor-mation about the object of the preposition, which willallow us to determine for example whether the preposi-tion in is functioning as a temporal or locative modifierin (10).
And information about the premodifiers of theobject noun phrase will help decide disambiguation incases like ( l l ) ,  where the as phrase depends in the pre-nominal modifier such.
(10) Jefferson Smurfit Inc. of Alton , Ill. ,bought the company in 1983 .
.
.
(11) The guidelines would affect such routinetasks as using ladders to enter manholes .
.
.References[I] Altmann, Gerry, and Mark Steedman.
1988.
Interac-tion with context during human sentence processing.Cognition, 30, 191-238.
[2] Church, Kenneth W. 1988.
A stochastic parts pro-gram and noun phrase parser for unrestricted text,Proceedings of the Second Conference on Applied Nat-ural Language Processing, Austin, Texas.
[3] Church, Kenneth W., William A. Gale, PatrickHanks, and Donald Hindle.
(to appear).
Using statis-tics in lexical analysis.
in Zernik (ed.)
Lexical acquisi-tion: using on-line resources to build a lezicon.
[4] Ford, Marilyn, Joan Bresnan and Ronald M. Ka-plan.
1982.
A competence based theory of syntacticclosure, in Bresnan, J .
(ed.)
The Mental Representa-tion of Grammatical Relations.
MIT Press.
[5] Frazier, L. 1978.
On comprehending sentences: Syn-tactic parsing strategies.
PhD.
dissertation, Universityof Connecticut.
[6] Hindle, Donald.
1983.
User manual for fidditch, a de-terministic' parser.
Naval Research Laboratory Tech-nical Memorandum 7590-142.
[7] Kimball, J .
1973.
Seven principles of surface struc-ture parsing in natural language, Cognition, 2, 15-47.
[8] Marcus, Mitchell P. 1980.
A theory of syntactic recog-nition for natural language.
MIT Press.
[9] Sinclair, J., P. Hanks, G. Fox, R. Moon, P. Stock,et al 1987.
Collins Cobuild English Language Dictio-nary.
Collins, London and Glasgow.
[lo] Taraban, Roman and James L. McClelland.
1988.Constituent attachment and thematic role assignmentin sentence processing: influences of content-based ex-pectations, Journal of Memory and Language, 27,597-632.Source Tot a1COBUILD 2728AP sample 88,860AP sample (f > 1) 40,869A P  sample ( t  > 1.65) 8,337NOUN I VERBTable 4: Count of noun and verb associations forCOBUILD and the AP sample[ l l ]  Whittemore, Greg, Kathleen Ferrara and HansBrunner.
1990.
Empirical study of predictive powersof simple attachment schemes for post-modifier prepo-sitional phrases.
Proceedings of the 28th Annual Meet-ing of the Association for Computational Linguistics,23-30.
