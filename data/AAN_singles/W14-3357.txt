Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 437?444,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsUsing Comparable Corpora to Adapt MT Models to New DomainsAnn IrvineCenter for Language and Speech ProcessingJohns Hopkins UniversityChris Callison-BurchComputer and Information Science Dept.University of PennsylvaniaAbstractIn previous work we showed that when us-ing an SMT model trained on old-domaindata to translate text in a new-domain,most errors are due to unseen sourcewords, unseen target translations, and in-accurate translation model scores (Irvineet al., 2013a).
In this work, we target er-rors due to inaccurate translation modelscores using new-domain comparable cor-pora, which we mine from Wikipedia.
Weassume that we have access to a large old-domain parallel training corpus but onlyenough new-domain parallel data to tunemodel parameters and do evaluation.
Weuse the new-domain comparable corporato estimate additional feature scores overthe phrase pairs in our baseline models.Augmenting models with the new featuresimproves the quality of machine transla-tions in the medical and science domainsby up to 1.3 BLEU points over very strongbaselines trained on the 150 million wordCanadian Hansard dataset.1 IntroductionDomain adaptation for machine translation isknown to be a challenging research problem thathas substantial real-world application.
In this set-ting, we have access to training data in some old-domain of text but very little or no training datain the domain of the text that we wish to translate.For example, we may have a large corpus of par-allel newswire training data but no training data inthe medical domain, resulting in low quality trans-lations at test time due to the mismatch.In Irvine et al.
(2013a), we introduced a tax-onomy for classifying machine translation errorsrelated to lexical choice.
Our ?S4?
taxonomy in-cludes seen, sense, score, and search errors.
Seenerrors result when a source language word orphrase in the test set was not observed at all duringtraining.
Sense errors occur when the source lan-guage word or phrase was observed during train-ing but not with the correct target language trans-lation.
If the source language word or phrase wasobserved with its correct translation during train-ing, but an incorrect alternative outweighs the cor-rect translation, then a score error has occurred.Search errors are due to pruning in beam searchdecoding.
We measured the impact of each errortype in a domain adaptation setting and concludedthat seen and sense errors are the most frequent butthat there is also room for improving errors due toinaccurate translation model scores (Irvine et al.,2013a).
In this work, we target score errors, usingcomparable corpora to reduce their frequency in adomain adaptation setting.We assume the setting where we have an old-domain parallel training corpus but no new domaintraining corpus.1We do, however, have accessto a mixed-domain comparable corpus.
We iden-tify new-domain text within our comparable cor-pus and use that data to estimate new translationfeatures on the translation models extracted fromold-domain training data.
Specifically, we focuson the French-English language pair because care-fully curated datasets exist in several domains fortuning and evaluation.
Following our prior work,we use the Canadian Hansard parliamentary pro-ceedings as our old-domain and adapt models toboth the medical and the science domains (Irvineet al., 2013a).
At over 8 million sentence pairs,1Some prior work has referred to old-domain and new-domain corpora as out-of-domain and in-domain, respec-tively.437the Canadian Hansard dataset is one of the largestpublicly available parallel corpora and provides avery strong baseline.
We give details about eachdataset in Section 4.1.We use comparable corpora to estimate sev-eral signals of translation equivalence.
In partic-ular, we estimate the contextual, topic, and or-thographic similarity of each phrase pair in ourbaseline old-domain translation model.
In Sec-tion 3, we describe each feature in detail.
Us-ing just 5 thousand comparable new-domain doc-ument pairs, which we mine from Wikipedia, andfive new phrase table features, we observe perfor-mance gains of up to 1.3 BLEU points on the sci-ence and medical translation tasks over very strongbaselines.2 Related WorkRecent work on machine translation domain adap-tation has focused on either the language model-ing component or the translation modeling com-ponent of an SMT model.
Language modeling re-search has explored methods for subselecting new-domain data from a large monolingual target lan-guage corpus for use as language model trainingdata (Lin et al., 1997; Klakow, 2000; Gao et al.,2002; Moore and Lewis, 2010; Mansour et al.,2011).
Translation modeling research has typi-cally assumed that either (1) two parallel datasetsare available, one in the old domain and one in thenew, or (2) a large, mixed-domain parallel trainingcorpus is available.
In the first setting, the goal isto effectively make use of both the old-domain andthe new-domain parallel training corpora (Civeraand Juan, 2007; Koehn and Schroeder, 2007; Fos-ter and Kuhn, 2007; Foster et al., 2010; Haddowand Koehn, 2012; Haddow, 2013).
In the sec-ond setting, it has been shown that, in some cases,training a translation model on a subset of new-domain parallel training data within a larger train-ing corpus can be more effective than using thecomplete dataset (Mansour et al., 2011; Axelrodet al., 2011; Sennrich, 2012; Gasc?o et al., 2012).For many language pairs and domains, no new-domain parallel training data is available.
Wu etal.
(2008) machine translate new-domain sourcelanguage monolingual corpora and use the syn-thetic parallel corpus as additional training data.Daum?e and Jagarlamudi (2011), Zhang and Zong(2013), and Irvine et al.
(2013b) use new-domaincomparable corpora to mine translations for un-seen words.
That work follows a long line of re-search on bilingual lexicon induction (e.g.
Rapp(1995), Schafer and Yarowsky (2002), Koehn andKnight (2002), Haghighi et al.
(2008), Irvine andCallison-Burch (2013), Razmara et al.
(2013)).These efforts improve S4 seen, and, in some in-stances, sense error types.
To our knowledge,no prior work has focused on fixing errors dueto inaccurate translation model scores in the set-ting where no new-domain parallel training data isavailable.In Klementiev et al.
(2012), we used compara-ble corpora to estimate several features for a givenphrase pair that indicate translation equivalence,including contextual, temporal, and topical simi-larity.
The definitions of phrasal and lexical con-textual and topic similarity that we use here aretaken from our prior work, where we replacedbilingually estimated phrase table features withthe new features and cited applications to low re-source SMT.
In this work we also focus on scoringa phrase table using comparable corpora.
How-ever, here we work in a domain adaptation settingand seek to augment, not replace, an existing setof bilingually estimated phrase table features.3 Phrase Table ScoringWe begin with a scored phrase table estimated us-ing our old-domain parallel training corpus.
Thephrase table contains about 201 million uniquesource phrases up to length seven and about 479million total phrase pairs.
We use Wikipedia as asource for comparable document pairs (details aregiven in Section 4.1).
We augment the bilinguallyestimated features with the following: (1) lexicaland phrasal contextual similarity estimated over acomparable corpus, (2) lexical and phrasal topi-cal similarity estimated over a comparable corpus,and (3) lexical orthographic similarity.Contextual Similarity We estimate contextualsimilarity2by first computing a context vector foreach source and target word and phrase in ourphrase table using the source and target sides ofour comparable corpus, respectively.
We begin bycollecting vectors of counts of words that appearin the context of each source and target phrase, psand pt.
We use a bag-of-words context consist-ing of the two words to the left and two words to2Similar to distributional similarity, which is typically de-fined monolingually.438the right of each occurrence of each phrase.
Vari-ous means of computing the component values ofcontext vectors from raw context frequency countshave been proposed (e.g.
Rapp (1999), Fung andYee (1998)).
Following Fung and Yee (1998), wecompute the value of the k-th component of ps?scontextual vector, Cps, as follows:Cpsk?
nps,k?
plogpn{nkq ` 1qwhere nps,kand nkare the number of times thek-th source word, sk, appears in the context of psand in the entire corpus, and n is the maximumnumber of occurrences of any word in the data.Intuitively, the more frequently skappears with psand the less common it is in the corpus in general,the higher its component value.
The context vectorfor ps, Cps, is M -dimensional, where M is thesize of the source language vocabulary.
Similarly,we compute N -dimensional context vectors for alltarget language words and phrases, where N is thesize of the target language vocabulary.We identify the most probable translation t foreach of the M source language words, s, as thetarget word with the highest ppt|sq under our wordaligned old-domain training corpus.
Given thisdictionary of unigram translations, we then projecteach M -dimensional source language context vec-tor into the N -dimensional target language contextvector space.
To compare a given pair of sourceand target context vectors, Cpsand Cpt, respec-tively, we compute their cosine similarity, or theirdot product divided by the product of their magni-tudes:simcontextualpps, ptq ?Cps?
Cpt||Cps||||Cpt||For a given phrase pair in our phrase table, weestimate phrasal contextual similarity by directlycomparing the context vectors of the two phrasesthemselves.
Because context vectors for phrases,which tend to be less frequent than words, can besparse, we also compute lexical contextual simi-larity over phrase pairs.
We define lexical con-textual similarity as the average of the contextualsimilarity between all word pairs within the phrasepair.Topic Similarity Phrases and their translationsare likely to appear in articles written about thesame topic in two languages.
We estimate topicsimilarity using the distribution of words andphrases across Wikipedia pages, for which wehave interlingual French-English links.
Specif-ically, we compute topical vectors by countingthe number of occurrences of each word andphrase across Wikipedia pages.
That is, for eachsource and target phrase, psand pt, we collect M -dimensional topic vectors, where M is the numberof Wikipedia page pairs used (in our experiments,M is typically 5, 000).
We use Wikipedia?s inter-lingual links to align the French and English topicvectors and normalize each topic vector by the to-tal count.
As with contextual similarity, we com-pare a pair of source and target topic vectors, Tpsand Tpt, respectively, using cosine similarity:simtopicpps, ptq ?Tps?
Tpt||Tps||||Tpt||We estimate both phrasal and lexical topic simi-larity for each phrase pair.
As before, lexical topicsimilarity is estimated by taking an average topicsimilarity across all word pairs in a given phrasepair.Orthographic Similarity We make use of oneadditional signal of translation equivalence: ortho-graphic similarity.
In this case, we do not refer-ence comparable corpora but simply compute theedit distance between a given pair of phrases.
Thissignal is often useful for identifying translationsof technical terms, which appear frequently in ourmedical and science domain corpora.
However,because of word order variation, we do not mea-sure edit distance on phrase pairs directly.
For ex-ample, French embryon humain translates as En-glish human embryo; embryon translates as em-bryo and humain translates as human.
Althoughboth word pairs are cognates, the words appearin opposite orders in the two phrases.
Therefore,directly measuring string edit distance across thephrase pair would not effectively capture the relat-edness of the words.
Hence, we only measure lex-ical orthographic similarity, not phrasal.
We com-pute lexical orthographic similarity by first com-puting the edit distance between each word pair,wsand wt, within a given phrase pair, normalizedby the lengths of the two words:simorthpws, wtq ?edpws, wtq|ws||wt|2We then compute the average normalized edit dis-tance across all word pairs.The above similarity metrics all allow for scoresof zero, which can be problematic for our log-439Corpus Source Words Target WordsTrainingCanadian Hansard 161.7 m 144.5 mTune-1 / Tune-2 / TestMedical 53k / 43k / 35k 46k / 38k / 30kScience 92k / 120k / 120k 75k / 101k / 101kLanguage Modeling and Comparable Corpus SelectionMedical - 5.9 mScience - 3.6 mTable 1: Summary of the size of each corpus of text usedin this work in terms of the number of source and target wordtokens.linear translation models.
We describe our ex-periments with different minimum score cutoffs inSection 4.2.4 Experimental Setup4.1 DataWe assume that the following data is available inour translation setting:?
Large old-domain parallel corpus for training?
Small new-domain parallel corpora for tun-ing and testing?
Large new-domain English monolingual cor-pus for language modeling and identifyingnew-domain-like comparable corpora?
Large mixed-domain comparable corpus,which includes some text from the new-domainThese data conditions are typical for many real-world uses of machine translation.
A summary ofthe size of each corpus is given in Table 1.Our old-domain training data is taken fromthe Canadian Hansard parliamentary proceedingsdataset, which consists of manual transcriptionsand translations of meetings of the Canadian par-liament.
The dataset is substantially larger thanthe commonly used Europarl corpus, containingover 8 million sentence pairs and about 150 mil-lion word tokens of French and English.For tuning and evaluation, we use new-domainmedical and science parallel datasets released byIrvine et al.
(2013a).
The medical texts con-sist of documents from the European MedicalAgency (EMEA), originally released by Tiede-mann (2009).
This data is primarily taken fromprescription drug label text.
The science data ismade up of translated scientific abstracts from thefields of physics, biology, and computer science.For both the medical and science domains, weuse three held-out parallel datasets of about 40and 100 thousand words,3respectively, releasedby Irvine et al.
(2013a).
We do tuning on dev1,additional parameter selection on test2, and blindtesting on test1.We use large new-domain monolingual Englishcorpora for language modeling and for selectingnew-domain-like comparable corpora from ourmixed domain comparable corpus.
Specifically,we use the English side of the medical and sciencetraining datasets released by Irvine et al.
(2013a).We do not use the parallel French side of the train-ing data at all; our data setting assumes that nonew-domain parallel data is available for training.We use Wikipedia as a source of compara-ble corpora.
There are over half a millionpairs of inter-lingually linked French and EnglishWikipedia documents.4We assume that we haveenough monolingual new-domain data in one lan-guage to rank Wikipedia pages according to hownew-domain-like they are.
In particular, we useour new-domain English language modeling datato measure new-domain-likeness.
We could havetargeted our learning even more by using our new-domain French test sets to select comparable cor-pora.
Doing so may increase the similarity be-tween our test data and comparable corpora.
How-ever, to avoid overfitting any particular test set, weuse our large English new-domain language mod-eling corpus instead.For each inter-lingually linked pair of Frenchand English Wikipedia documents, we computethe percent of English phrases up to length fourthat are observed in the English monolingual new-domain corpus and rank document pairs by the ge-ometric mean of the four overlap measures.
Moresophisticated ways to identify new-domain-likeWikipedia pages (e.g.
(Moore and Lewis, 2010))may yield additional performance gains, but, qual-itatively, the ranked Wikipedia pages seem rea-sonable for the purposes of generating a large setof top-k new-domain document pairs.
The top-10ranked pages for each domain are listed in Table 2.The top ranked science domain pages are primar-ily related to concepts from the field of physicsbut also include computer science and chemistry3Or about 4 thousand lines each.
The sentences in themedical domain text are much shorter than those in the sci-ence domain.4As of January 2014.440Science MedicalDiagnosis (artificial intelligence) PregabalinAbsorption spectroscopy CetuximabSpectral line FluconazoleChemical kinetics CalcitoninMahalanobis distance Pregnancy categoryDynamic light scattering TrazodoneAmorphous solid RivaroxabanMagnetic hyperthermia SpironolactonePhotoelasticity AnakinraGalaxy rotation curve CladribineTable 2: Top 10 Wikipedia articles ranked by their similar-ity to large new-domain English monolingual corpora.topics.
The top ranked medical domain pages arenearly all prescription drugs, which makes sensegiven the content of the EMEA medical corpus.4.2 Phrase-based Machine TranslationWe word align our old-domain training corpususing GIZA++ and use the Moses SMT toolkit(Koehn et al., 2007) to extract a translation gram-mar.
In this work, we focus on phrase-basedSMT models, however our approach to using new-domain comparable corpora to estimate translationscores is theoretically applicable to any type oftranslation grammar.Our baseline models use a phrase limit of sevenand the standard phrase-based SMT feature set, in-cluding forward and backward phrase and lexicaltranslation probabilities.
Additionally, we use thestandard lexicalized reordering model.
We exper-iment with two 5-gram language models trainedusing SRILM with Kneser-Ney smoothing on (1)the English side of the Hansard training corpus,and (2) the relevant new-domain monolingual En-glish corpus.
We experiment with using, first, onlythe old-domain language model and, then, both theold-domain and the new-domain language models.Our first comparison system augments the stan-dard feature set with the orthographic similarityfeature, which is not based on comparable cor-pora.
Our second comparison system uses boththe orthographic feature and the contextual andtopic similarity features estimated over a randomset of comparable document pairs.
The third sys-tem estimates contextual and topic similarity usingnew-domain-like comparable corpora.
We tuneour phrase table feature weights for each modelseparately using batch MIRA (Cherry and Fos-ter, 2012) and new-domain tuning data.
Resultsare averaged over three tuning runs, and we usethe implementation of approximate randomizationreleased by Clark et al.
(2011) to measure thestatistical significance of each feature-augmentedmodel compared with the baseline model that usesthe same language model(s).As noted in Section 3, the features that weestimate from comparable corpora may be zero-valued.
We use our second tuning sets5to tunea minimum threshold parameter for our new fea-tures.
We measure performance in terms of BLEUscore on the second tuning set as we vary the newfeature threshold between 1e?07 and 0.5 for eachdomain.
A threshold of 0.01, for example, meansthat we replace all feature with values less than0.01 with 0.01.
For both new-domains, perfor-mance drops when we use thresholds lower than0.01 and higher than 0.25.
We use a minimumthreshold of 0.1 for all experiments presented be-low for both domains.5 ResultsTable 3 presents a summary of our results on thetest set in each domain.
Using only the old-domainlanguage model, our baselines yield BLEU scoresof 22.70 and 21.29 on the medical and science testsets, respectively.
When we add the orthographicsimilarity feature, BLEU scores increase signifi-cantly, by about 0.4 on the medical data and 0.6 onscience.
Adding the contextual and topic featuresestimated over a random selection of comparabledocument pairs improves BLEU scores slightly inboth domains.
Finally, using the most new-domainlike document pairs to estimate the contextual andtopic features yields a 1.3 BLEU score improve-ment over the baseline in both domains.
For bothdomains, this result is a statistically significant im-provement6over each of the first three systems.In both domains, the new-domain languagemodels contribute substantially to translation qual-ity.
Baseline BLEU scores increase by about6 and 5 BLEU score points in the medical andscience domains, respectively, when we add thenew-domain language models.
In the medical do-main, neither the orthographic feature nor the or-thographic feature in combination with contextualand topic features estimated over random docu-ment pairs results in a significant BLEU scoreimprovement.
However, using the orthographicfeature and the contextual and topic features es-timated over new-domain document pairs yields a5test2 datasets released by Irvine et al.
(2013a)6p-value ?
0.01441Language Model(s) System Medical ScienceOldBaseline 22.70 21.29+ Orthographic Feature 23.09* (`0.4) 21.86* (`0.6)+ Orthographic & Random CC Features 23.22* (`0.5) 21.88* (`0.6)+ Orthographic & New-domain CC Features 23.98* (`1.3) 22.55* (`1.3)Old+NewBaseline 28.82 26.18+ Orthographic Feature 29.02 (`0.2) 26.40* (`0.2)+ Orthographic & Random CC Features 28.86 (`0.0) 26.52* (`0.3)+ Orthographic & New-domain CC Features 29.16* (`0.3) 26.50* (`0.3)Table 3: Comparison between the performance of baseline old-domain translation models and domain-adapted models intranslating science and medical domain text.
We experiment with two language models: old, trained on the English side of ourHansard old-domain training corpus and new, trained on the English side of the parallel training data in each new domain.
Weuse comparable corpora of 5, 000 (1) random, and (2) the most new-domain-like document pairs to score phrase tables.
Allresults are averaged over three tuning runs, and we perform statistical significance testing comparing each system augmentedwith additional features with the baseline system that uses the same language model(s).
* indicates that the BLEU scores arestatistically significant with p ?
0.01.small but significant improvement of 0.3 BLEU.In the science domain, in contrast, all three aug-mented models perform statistically significantlybetter than the baseline.
Contextual and topic fea-tures yield only a slight improvement above themodel that uses only the orthographic feature, butthe difference is statistically significant.
For thescience domain, when we use the new domain lan-guage model, there is no difference between esti-mating the contextual and topic features over ran-dom comparable document pairs and those chosenfor their similarity with new-domain data.Differences across domains may be due to thefact that the medical domain corpora are muchmore homogenous, containing the often boiler-plate text of prescription drug labels, than the sci-ence domain corpora.
The science domain cor-pora, in contrast, contain abstracts from severaldifferent scientific fields; because that data is morediverse, a randomly chosen mixed-domain set ofcomparable corpora may still be relevant and use-ful for adapting a translation model.We experimented with varying the number ofcomparable document pairs used for estimatingcontextual and topic similarity but saw no sig-nificant gains from using more than 5, 000 in ei-ther domain.
In fact, performance dropped in themedical domain when we used more than a fewthousand document pairs.
Our proposed approachorders comparable document pairs by how new-domain-like they are and augments models withnew features estimated over the top-k. As a result,using more comparable document pairs means thatthere is more data from which to estimate sig-nals, but it also means that the data is less new-domain like overall.
Using a domain similaritythreshold to choose a subset of comparable doc-ument pairs may prove useful in future work, asthe ideal amount of comparable data will dependon the type and size of the initial mixed-domaincomparable corpus as well as the homogeneity ofthe text domain of interest.We also experimented with using a third lan-guage model estimated over the English side ofour comparable corpora.
However, we did not seeany significant improvements in translation qual-ity when we used this language model in combina-tion with the old and new domain language mod-els.6 ConclusionIn this work, we targeted SMT errors dueto translation model scores using new-domaincomparable corpora.
Our old-domain French-English baseline model was trained on the Cana-dian Hansard parliamentary proceedings dataset,which, at 8 million sentence pairs, is one of thelargest publicly available parallel datasets.
Ourtask was to adapt this baseline to the medical andscientific text domains using comparable corpora.We used new-domain parallel data only to tunemodel parameters and do evaluation.
We minedWikipedia for new-domain-like comparable docu-ment pairs, over which we estimated several addi-tional features scores: contextual, temporal, andorthographic similarity.
Augmenting the strongbaseline with our new feature set improved thequality of machine translations in the medical andscience domains by up to 1.3 BLEU points.4427 AcknowledgementsThis material is based on research sponsored byDARPA under contract HR0011-09-1-0044 andby the Johns Hopkins University Human Lan-guage Technology Center of Excellence.
Theviews and conclusions contained in this publica-tion are those of the authors and should not beinterpreted as representing official policies or en-dorsements of DARPA or the U.S. Government.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domaindata selection.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP).Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of the Conference of the North AmericanChapter of the Association for Computational Lin-guistics (NAACL).Jorge Civera and Alfons Juan.
2007.
Domain adap-tation in statistical machine translation with mixturemodelling.
In Proceedings of the Workshop on Sta-tistical Machine Translation (WMT).Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testingfor statistical machine translation: controlling foroptimizer instability.
In Proceedings of the Confer-ence of the Association for Computational Linguis-tics (ACL).Hal Daum?e, III and Jagadeesh Jagarlamudi.
2011.Domain adaptation for machine translation by min-ing unseen words.
In Proceedings of the Confer-ence of the Association for Computational Linguis-tics (ACL).George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for SMT.
In Proceedings ofthe Workshop on Statistical Machine Translation(WMT).G.
Foster, C. Goutte, and R. Kuhn.
2010.
Discrimi-native instance weighting for domain adaptation inSMT.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP).Pascale Fung and Lo Yuen Yee.
1998.
An IR approachfor translating new words from nonparallel, compa-rable texts.
In Proceedings of the Conference of theAssociation for Computational Linguistics (ACL).Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-Fu Lee.
2002.
Toward a unified approach to sta-tistical language modeling for chinese.
ACM Trans-actions on Asian Language Information Processing(TALIP).Guillem Gasc?o, Martha-Alicia Rocha, Germ?anSanchis-Trilles, Jes?us Andr?es-Ferrer, and FranciscoCasacuberta.
2012.
Does more data always yieldbetter translations?
In Proceedings of the Confer-ence of the European Association for ComputationalLinguistics (EACL).Barry Haddow and Philipp Koehn.
2012.
Analysingthe effect of out-of-domain data on SMT systems.
InProceedings of the Workshop on Statistical MachineTranslation (WMT).Barry Haddow.
2013.
Applying pairwise ranked op-timisation to improve the interpolation of transla-tion models.
In Proceedings of the Conference ofthe North American Chapter of the Association forComputational Linguistics (NAACL).Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexi-cons from monolingual corpora.
In Proceedings ofthe Conference of the Association for ComputationalLinguistics (ACL).Ann Irvine and Chris Callison-Burch.
2013.
Su-pervised bilingual lexicon induction with multiplemonolingual signals.
In Proceedings of the Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics (NAACL).Ann Irvine, John Morgan, Marine Carpuat, Hal Daum?eIII, and Dragos Munteanu.
2013a.
Measuring ma-chine translation errors in new domains.
Transac-tions of the Association for Computational Linguis-tics, 1(October).Ann Irvine, Chris Quirk, and Hal Daume III.
2013b.Monolingual marginal matching for translationmodel adaptation.
In Proceedings of the Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP).Dietrich Klakow.
2000.
Selecting articles from thelanguage model training corpus.
In Proceedingsof the IEEE International Conference on Acoustics,Speech, and Signal Processing (ICASSP).Alex Klementiev, Ann Irvine, Chris Callison-Burch,and David Yarowsky.
2012.
Toward statistical ma-chine translation without parallel corpora.
In Pro-ceedings of the Conference of the European Associ-ation for Computational Linguistics (EACL).Philipp Koehn and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
InACL Workshop on Unsupervised Lexical Acquisi-tion.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin domain adaptation for statistical machine transla-tion.
In Proceedings of the Workshop on StatisticalMachine Translation (WMT).Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,443Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the Conference of the Association forComputational Linguistics (ACL).Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, Ker-Jiann Chen, and Lin-Shan Lee.
1997.
Chinese lan-guage model adaptation based on document classifi-cation and multiple domain-specific language mod-els.
In Fifth European Conference on Speech Com-munication and Technology.Saab Mansour, Joern Wuebker, and Hermann Ney.2011.
Combining translation and language modelscoring for domain-specific data filtering.
In Pro-ceedings of the International Workshop on SpokenLanguage Translation (IWSLT).Robert C. Moore and William Lewis.
2010.
Intelli-gent selection of language model training data.
InProceedings of the Conference of the Association forComputational Linguistics (ACL).Reinhard Rapp.
1995.
Identifying word translationsin non-parallel texts.
In Proceedings of the Confer-ence of the Association for Computational Linguis-tics (ACL).Reinhard Rapp.
1999.
Automatic identification ofword translations from unrelated English and Ger-man corpora.
In Proceedings of the Conferenceof the Association for Computational Linguistics(ACL).Majid Razmara, Maryam Siahbani, Reza Haffari, andAnoop Sarkar.
2013.
Graph propagation for para-phrasing out-of-vocabulary words in statistical ma-chine translation.
In Proceedings of the Confer-ence of the Association for Computational Linguis-tics (ACL).Charles Schafer and David Yarowsky.
2002.
Inducingtranslation lexicons via diverse similarity measuresand bridge languages.
In Proceedings of the Confer-ence on Natural Language Learning (CoNLL).Rico Sennrich.
2012.
Perplexity minimization fortranslation model domain adaptation in statisticalmachine translation.
In Proceedings of the Confer-ence of the European Association for ComputationalLinguistics (EACL).J?org Tiedemann.
2009.
News from OPUS - A collec-tion of multilingual parallel corpora with tools andinterfaces.
In N. Nicolov, K. Bontcheva, G. An-gelova, and R. Mitkov, editors, Recent Advances inNatural Language Processing (RANLP).Hua Wu, Haifeng Wang, and Chengqing Zong.
2008.Domain adaptation for statistical machine transla-tion with domain dictionary and monolingual cor-pora.
In Proceedings of the International Confer-ence on Computational Linguistics (COLING).Jiajun Zhang and Chengqing Zong.
2013.
Learninga phrase-based translation model from monolingualdata with application to domain adaptation.
In Pro-ceedings of the Conference of the Association forComputational Linguistics (ACL).444
