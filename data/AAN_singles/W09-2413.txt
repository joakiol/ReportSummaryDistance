Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 82?87,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSemEval-2010 Task 3: Cross-lingual Word Sense DisambiguationEls Lefever1,2 and Veronique Hoste1,21LT3, Language and Translation Technology Team, University College GhentGroot-Brittannie?laan 45, 9000 Gent, Belgium2Department of Applied Mathematics and Computer Science, Ghent UniversityKrijgslaan 281 (S9), 9000 Gent, Belgium{Els.Lefever, Veronique.Hoste}@hogent.beAbstractWe propose a multilingual unsupervised WordSense Disambiguation (WSD) task for a sample ofEnglish nouns.
Instead of providing manually sense-tagged examples for each sense of a polysemousnoun, our sense inventory is built up on the basis ofthe Europarl parallel corpus.
The multilingual setupinvolves the translations of a given English polyse-mous noun in five supported languages, viz.
Dutch,French, German, Spanish and Italian.The task targets the following goals: (a) the man-ual creation of a multilingual sense inventory for alexical sample of English nouns and (b) the eval-uation of systems on their ability to disambiguatenew occurrences of the selected polysemous nouns.For the creation of the hand-tagged gold standard,all translations of a given polysemous English nounare retrieved in the five languages and clustered bymeaning.
Systems can participate in 5 bilingualevaluation subtasks (English - Dutch, English - Ger-man, etc.)
and in a multilingual subtask covering alllanguage pairs.As WSD from cross-lingual evidence is gainingpopularity, we believe it is important to create a mul-tilingual gold standard and run cross-lingual WSDbenchmark tests.1 IntroductionThe Word Sense Disambiguation (WSD) task,which consists in selecting the correct sense of agiven word in a given context, has been widelystudied in computational linguistics.
For a recentoverview of WSD algorithms, resources and appli-cations, we refer to Agirre and Edmonds (2006)and Navigli (2009).
Semantic evaluation competi-tions such as Senseval1 and its successor Semevalrevealed that supervised approaches to WSDusually achieve better results than unsupervisedmethods (Ma`rquez et al, 2006).
The former usemachine learning techniques to induce a classifierfrom manually sense-tagged data, where eachoccurrence of a polysemous word gets assigned asense label from a predefined sense inventory suchas WordNet (Fellbaum, 1998).
These supervisedmethods, however, heavily rely on large sense-tagged corpora which are very time consuming andexpensive to build.
This phenomenon, well knownas the knowledge acquisition bottleneck (Gale etal., 1992), explains the modest use and success ofsupervised WSD in real applications.Although WSD has long time been studied as astand-alone NLP task, there is a growing feelingin the WSD community that WSD should prefer-ably be integrated in real applications such asMachine Translation or multilingual informationretrieval (Agirre and Edmonds, 2006).
Severalstudies have demonstrated that for instance Sta-tistical Machine Translation (SMT) benefits fromincorporating a dedicated WSD module (Chan et al,2007; Carpuat and Wu, 2007).
Using translationsfrom a corpus instead of human-defined senselabels is one way of facilitating the integration ofWSD in multilingual applications.
It also implic-1http://www.senseval.org/82itly deals with the granularity problem as finersense distinctions are only relevant as far as theyare lexicalized in the translations.
Furthermore,this type of corpus-based approach is language-independent, which makes it a valid alternativefor languages lacking sufficient sense inventoriesand sense-tagged corpora, although one couldargue that the lack of parallel corpora for certainlanguage pairs might be problematic as well.
Themethodology to deduce word senses from parallelcorpora starts from the hypothesis that the differentsense distinctions of a polysemous word are oftenlexicalized cross-linguistically.
For instance, if wequery the English noun ?bill?
in the English-DutchEuroparl, the following top four translations areretrieved: ?rekening?
(Eng.
: ?invoice?)
(198 occur-rences), ?kosten?
(Eng.
: ?costs?)
(100 occ.
), ?Bill?
(96 occ.)
and ?wetsvoorstel?
(Eng.
: ?piece oflegislation?)
(77 occ.).
If we make the simplifyingassumption for our example that (i) these are theonly Dutch translations of our focus word and that(ii) all sense distinctions of ?bill?
are lexicalizedin Dutch, we can infer that the English noun ?bill?has at most four different senses.
These differentsenses in turn can be grouped in case of synonymy.In the Dutch-French Europarl, for example, both?rekening?
and ?kosten?, are translated by theFrench ?frais?, which might indicate that bothDutch words are synonymous.Several WSD studies are based on the idea ofcross-lingual evidence.
Gale et al (1993) use abilingual parallel corpus for the automatic creationof a sense-tagged data set, where target words in thesource language are tagged with their translationof the word in the target language.
Diab andResnik (2002) present an unsupervised approachto WSD that exploits translational correspondencesin parallel corpora that were artificially created byapplying commercial MT systems on a sense-taggedEnglish corpus.
Ide et al (2002) use a multilingualparallel corpus (containing seven languages fromfour language families) and show that sense dis-tinctions derived from translation equivalents are atleast as reliable as those made by human annotators.Moreover, some studies present multilingual WSDsystems that attain state-of-the-art performance inall-words disambiguation (Ng et al, 2003).
Theproposed Cross-lingual Word Sense Disambigua-tion task differs from earlier work (e.g.
Ide et al(2002)) through its independence from an externallydefined sense set.The remainder of this paper is organized as follows.In Section 2, we present a detailed description ofthe cross-lingual WSD task.
It introduces the par-allel corpus we used, informs on the developmentand test data and discusses the annotation procedure.Section 3 gives an overview of the different scoringstrategies that will be applied.
Section 4 concludesthis paper.2 Task set upThe cross-lingual Word Sense Disambiguation taskinvolves a lexical sample of English nouns.
We pro-pose two subtasks, i.e.
systems can either partici-pate in the bilingual evaluation task (in which theanswer consists of translations in one language) orin the multilingual evaluation task (in which the an-swer consists of translations in all five supported lan-guages).
Table 1 shows an example of the bilingualsense labels for two test occurrences of the Englishnoun bank in our parallel corpus which will be fur-ther described in Section 2.1.
Table 2 presents themultilingual sense labels for the same sentences.... giving fish to people living on the [bank] of theriverLanguage Sense labelDutch (NL) oever/dijkFrench (F) rives/rivage/bord/bordsGerman (D) UferItalian (I) rivaSpanish (ES) orillaThe [bank] of Scotland ...Language Sense labelDutch (NL) bank/kredietinstellingFrench (F) banque/e?tablissement de cre?ditGerman (D) Bank/KreditinstitutItalian (I) bancaSpanish (ES) bancoTable 1: Example of bilingual sense labels for the Englishnoun bank83... giving fish to people living on the [bank] of theriverLanguage Sense labelNL,F,D,I,ES oever/dijk,rives/rivage/bord/bords,Ufer, riva, orillaThe [bank] of Scotland ...Language Sense labelNL,F,D,I,ES bank/kredietinstelling, banque/e?tablissement de cre?dit, Bank/Kreditinstitut, banca, bancoTable 2: Example of multi-lingual sense labels for theEnglish noun bank2.1 Corpus and word selectionThe document collection which serves as the basisfor the gold standard construction and systemevaluation is the Europarl parallel corpus2, whichis extracted from the proceedings of the EuropeanParliament (Koehn, 2005).
We selected 6 languagesfrom the 11 European languages represented inthe corpus: English (our target language), Dutch,French, German, Italian and Spanish.
All sentencesare aligned using a tool based on the Gale andChurch (1991) algorithm.
We only consider the 1-1sentence alignments between English and the fiveother languages (see also Tufis et al (2004) fora similar strategy).
These 1-1 alignments will bemade available to all task participants.
Participantsare free to use other training corpora, but additionaltranslations which are not present in Europarl willnot be included in the sense inventory that is usedfor evaluation.For the competition, two data sets will be developed.The development and test sentences will be selectedfrom the JRC-ACQUIS Multilingual Parallel Cor-pus3.
The development data set contains 5 poly-semous nouns, for which we provide the manuallybuilt sense inventory based on Europarl and 50 ex-ample instances, each annotated with one sense label(cluster that contains all translations that have beengrouped together for that particular sense) per target2http://www.statmt.org/europarl/3http://wt.jrc.it/lt/Acquis/language.
The manual construction of the sense in-ventory will be discussed in Section 2.2.
The testdata contains 50 instances for 20 nouns from the testdata as used in the Cross-Lingual Lexical Substitu-tion Task4.
In this task, annotators and systems areasked to provide as many correct Spanish transla-tions as possible for an English target word.
Theyare not bound to a predefined parallel corpus, butcan freely choose the translations from any availableresource.
Selecting the target words from the set ofnouns thats will be used for the Lexical SubstitutionTask should make it easier for systems to participatein both tasks.2.2 Manual annotationThe sense inventory for the 5 target nouns in the de-velopment data and the 20 nouns in the test data ismanually built up in three steps.1.
In the first annotation step, the 5 translationsof the English word are identified per sentenceID.
In order to speed up this identification,GIZA++ (Och and Ney, 2003) is used to gen-erate the initial word alignments for the 5 lan-guages.
All word alignments are manually ver-ified.In this step, we might come across multiwordtranslations, especially in Dutch and Germanwhich tend to glue parts of compounds togetherin one orthographic unit.
We decided to keepthese translations as such, even if they do notcorrespond exactly to the English target word.In following sentence, the Dutch translationwitboek corresponds in fact to the English com-pound white paper, and not to the English tar-get word paper:English: the European Commissionpresented its white paperDutch: de presentatie van hetwitboek door de Europese Com-missieAlthough we will not remove these compoundtranslations from our sense inventory, we willmake sure that the development and test sen-tences do not contain target words that are part4http://lit.csci.unt.edu/index.php/Semeval 201084of a larger multiword unit, in order not to dis-advantage systems that do not deal with decom-pounding.2.
In the second step, three annotators per lan-guage will cluster the retrieved translations pertarget language.
On the basis of the sentenceIDs, the translations in all languages will be au-tomatically coupled.
Only translations above apredefined frequency threshold are consideredfor inclusion in a cluster.
Clustering will hap-pen in a trilingual setting, i.e.
annotators al-ways cluster two target languages simultane-ously (with English being the constant sourcelanguage)5.After the clustering of the translations, the an-notators perform a joint evaluation per lan-guage in order to reach a consensus clusteringfor each target language.
In case the annota-tors do not reach a consensus, we apply soft-clustering for that particular translation, i.e.
weassign the translation to two or more differentclusters.3.
In a last step, there will be a cross-lingual con-flict resolution in which the resulting cluster-ings are checked cross-lingually by the humanannotators.The resulting sense inventory is used to annotate thesentences in the development set and the test set.This implies that a given target word is annotatedwith the appropriate sense cluster.
This annotationis done by the same native annotators as in steps2 and 3.
The goal is to reach a consensus clusterper sentence.
But again, if no consensus is reached,soft-clustering is applied and as a consequence, thecorrect answer for this particular test instance con-sists of one of the clusters that were considered forsoft-clustering.The resulting clusters are used by the three nativeannotators to select their top 3 translations persentence.
These potentially different translationsare kept to calculate frequency information for allanswer translations (discussed in section 3).5The annotators will be selected from the master studentsat the ?University College Ghent ?
Faculty of Translation?
thattrains certified translators in all six involved languages.Table 3 shows an example of how the translationclusters for the English noun ?paper?
could looklike in a trilingual setting.3 System evaluationAs stated before, systems can participate in twotasks, i.e.
systems can either participate in one ormore bilingual evaluation tasks or they can partici-pate in the multilingual evaluation task incorporat-ing the five supported languages.
The evaluation ofthe multilingual evaluation task is simply the aver-age of the system scores on the five bilingual evalu-ation tasks.3.1 Evaluation strategiesFor the evaluation of the participating systems wewill use an evaluation scheme which is inspiredby the English lexical substitution task in SemEval2007 (McCarthy and Navigli, 2007).
The evaluationwill be performed using precision and recall (P andR in the equations that follow).
We perform both abest result evaluation and a more relaxed evaluationfor the top five results.Let H be the set of annotators, T be the set of testitems and hi be the set of responses for an item i ?
Tfor annotator h ?
H .
Let A be the set of items fromT where the system provides at least one answer andai : i ?
A be the set of guesses from the system foritem i.
For each i, we calculate the multiset union(Hi) for all hi for all h ?
H and for each uniquetype (res) in Hi that has an associated frequency(freqres).
In the formula of (McCarthy and Navigli,2007), the associated frequency (freqres) is equalto the number of times an item appears in Hi.
Aswe define our answer clusters by consensus, this fre-quency would always be ?1?.
In order to overcomethis, we ask our human annotators to indicate theirtop 3 translations, which enables us to also obtainmeaningful associated frequencies (freqres) (?1?
incase the translation is not chosen by any annotator,?2?
in case a translation is picked by 1 annotator, ?3?if picked by two annotators and ?4?
if chosen by allthree annotators).Best result evaluation For the best result evalu-ation, systems can propose as many guesses as thesystem believes are correct, but the resulting score is85divided by the number of guesses.
In this way, sys-tems that output a lot of guesses are not favoured.P =?ai:i?A?res?aifreqres|ai||Hi||A| (1)R =?ai:i?T?res?aifreqres|ai||Hi||T | (2)Relaxed evaluation For the more relaxed evalu-ation, systems can propose up to five guesses.
Forthis evaluation, the resulting score is not divided bythe number of guesses.P =?ai:i?A?res?aifreqres|Hi||A| (3)R =?ai:i?T?res?aifreqres|Hi||T | (4)3.2 BaselineWe will produce two, both frequency-based, base-lines.
The first baseline, which will be used for thebest result evaluation, is based on the output of theGIZA++ word alignments on the Europarl corpusand just returns the most frequent translation of agiven word.
The second baseline outputs the fivemost frequent translations of a given word accord-ing to the GIZA++ word alignments.
This baselinewill be used for the relaxed evaluation.
As a thirdbaseline, we will consider using a baseline based onEuroWordNet6, which is available in the five targetlanguages.4 ConclusionsWe presented a multilingual unsupervised WordSense Disambiguation task for a sample of Englishnouns.
The lack of supervision refers to the con-struction of the sense inventory, that is built up onthe basis of translations retrieved from the Europarlcorpus in five target languages.
Systems can partici-pate in a bilingual or multilingual evaluation and areasked to provide correct translations in one or five6http://www.illc.uva.nl/EuroWordNettarget languages for new instances of the selectedpolysemous target nouns.ReferencesE.
Agirre and P. Edmonds, editors.
2006.
Word SenseDisambiguation.
Text, Speech and Language Tech-nology.
Springer, Dordrecht.M.
Carpuat and D. Wu.
2007.
Improving statisticalmachine translation using word sense disambiguation.In Proceedings of the 2007 Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL), pages 61?72, Prague, Czech Republic.Y.S.
Chan, H.T.
Ng, and D. Chiang.
2007.
Word sensedisambiguation improves statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 33?40, Prague, Czech Republic.M.
Diab and P. Resnik.
2002.
An unsupervised methodfor word sense tagging using parallel corpora.
In Pro-ceedings of ACL, pages 255?262.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.William A. Gale and Kenneth W. Church.
1991.
A pro-gram for aligning sentences in bilingual corpora.
InComputational Linguistics, pages 177?184.W.A.
Gale, K. Church, and D. Yarowsky.
1992.
Esti-mating upper and lower bounds on the performanceof word-sense disambiguation programs.
In Proceed-ings of the 30th Annual Meeting of the Association forComputational Linguistics, pages 249?256.W.A.
Gale, K.W.
Church, and D. Yarowsky.
1993.
Amethod for disambiguating word senses in a large cor-pus.
In Computers and the Humanities, volume 26,pages 415?439.N.
Ide, T. Erjavec, and D. Tufis.
2002.
Sense dis-crimination with parallel corpora.
In Proceedings ofACL Workshop on Word Sense Disambiguation: Re-cent Successes and Future Directions, pages 54?60.P.
Koehn.
2005.
Europarl: A parallel corpus for statis-tical machine translation.
In Proceedings of the MTSummit.L.
Ma`rquez, G. Escudero, D. Mart?`nez, and G. Rigau.2006.
Supervised corpus-based methods for WSD.
InE.
Agirre and P. Edmonds, editors, Word Sense Disam-biguation: Algorithms and Applications, pages 167?216.
Eds Springer, New York, NY.D.
McCarthy and R. Navigli.
2007.
Semeval-2007 task10: English lexical substitution task.
In Proceedingsof the 4th International Workshop on Semantic Evalu-ations (SemEval-2007), pages 48?53.86R.
Navigli.
2009.
Word sense disambiguation: a survey.In ACM Computing Surveys, volume 41, pages 1?69.H.T.
Ng, B. Wang, and Y.S.
Chan.
2003.
Exploitingparallel texts for word sense disambiguation: An em-pirical study.
In Proceedings of the 41st Annual Meet-ing of the Association for Computational Linguistics,pages 455?462, Santa Cruz.F.J.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.Dan Tufis?, Radu Ion, and Nancy Ide.
2004.
Fine-GrainedWord Sense Disambiguation Based on Parallel Cor-pora, Word Alignment, Word Clustering and AlignedWordnets.
In Proceedings of the 20th InternationalConference on Computational Linguistics (COLING2004), pages 1312?1318, Geneva, Switzerland, Au-gust.
Association for Computational Linguistics.English ?paper?
Dutch French ItalianCluster 1 boek, verslag, wetsvoorstel livre, document, librogreen paper kaderbesluit paquetCluster 2 document, voorstel, paper document, rapport, travail documento, rapportopresent a paper nota, stuk, notitie publication, note testo, notaproposition, avisCluster 3 krant, dagblad journal, quotidien giornale, quotidiano,read a paper weekblad hebdomadaire settimanale, rivistaCluster 4 papier papier carta, cartinareams of paperCluster 5 papieren, papier papeterie, papetie`re cartastraccia, cartaceoof paper, paper prullenmand papier cartieraindustry, paper basketCluster 6 stembiljet, bulletin, vote scheda, scheda di votovoting paper, stembriefjeballot paperCluster 7 papiertje papier volant foglio, fogliettopiece of paperCluster 8 papier, administratie paperasse, paperasserie carta, amministrativoexcess of paper, administratief papier, administratif burocratico, cartaceogenerate paper bureaucratieCluster 9 in theorie, op papier, en the?orie, in teoria,on paper papieren, bij woorden conceptuellement di paroleCluster 10 op papier e?crit, dans les textes, nero su bianco, (di natura)on paper de nature typographique, par voie tipografica, per iscritto,e?pistolaire, sur (le) papier cartaceo, di paroleCluster 11 agenda, zittingstuk, ordre du jour, ordine del giornoorder paper stuk ordre des votesTable 3: translation clusters for the English noun ?paper?87
