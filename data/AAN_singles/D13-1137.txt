Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1372?1376,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsSimple Customization of Recursive Neural Networksfor Semantic Relation ClassificationKazuma Hashimoto?, Makoto Miwa?
?, Yoshimasa Tsuruoka?, and Takashi Chikayama?
?The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan{hassy, tsuruoka, chikayama}@logos.t.u-tokyo.ac.jp?
?The University of Manchester, 131 Princess Street, Manchester, M1 7DN, UKmakoto.miwa@manchester.ac.ukAbstractIn this paper, we present a recursive neuralnetwork (RNN) model that works on a syn-tactic tree.
Our model differs from previousRNN models in that the model allows for anexplicit weighting of important phrases for thetarget task.
We also propose to average param-eters in training.
Our experimental results onsemantic relation classification show that bothphrase categories and task-specific weightingsignificantly improve the prediction accuracyof the model.
We also show that averaging themodel parameters is effective in stabilizing thelearning and improves generalization capacity.The proposed model marks scores competitivewith state-of-the-art RNN-based models.1 IntroductionRecursive Neural Network (RNN) models arepromising deep learning models which have beenapplied to a variety of natural language processing(NLP) tasks, such as sentiment classification, com-pound similarity, relation classification and syntacticparsing (Hermann and Blunsom, 2013; Socher et al2012; Socher et al 2013).
RNN models can repre-sent phrases of arbitrary length in a vector space ofa fixed dimension.
Most of them use minimal syn-tactic information (Socher et al 2012).Recently, Hermann and Blunsom (2013) pro-posed a method for leveraging syntactic informa-tion, namely CCG combinatory operators, to guidecomposition of phrases in RNN models.
While theirmodels were successfully applied to binary senti-ment classification and compound similarity tasks,there are questions yet to be answered, e.g., whethersuch enhancement is beneficial in other NLP tasksas well, and whether a similar improvement canbe achieved by using syntactic information of morecommonly available types such as phrase categoriesand syntactic heads.In this paper, we present a supervised RNN modelfor a semantic relation classification task.
Our modelis different from existing RNN models in that impor-tant phrases can be explicitly weighted for the task.Syntactic information used in our model includespart-of-speech (POS) tags, phrase categories andsyntactic heads.
POS tags are used to assign vec-tor representations to word-POS pairs.
Phrase cate-gories are used to determine which weight matricesare chosen to combine phrases.
Syntactic heads areused to determine which phrase is weighted duringcombining phrases.
To incorporate task-specific in-formation, phrases on the path between entity pairsare further weighted.The second contribution of our work is the intro-duction of parameter averaging into RNN models.In our preliminary experiments, we observed thatthe prediction performance of the model often fluc-tuates significantly between training iterations.
Thisfluctuation not only leads to unstable performanceof the resulting models, but also makes it difficult tofine-tune the hyperparameters of the model.
Inspiredby Swersky et al(2010), we propose to average themodel parameters in the course of training.
A re-cent technique for deep learning models of similarvein is dropout (Hinton et al 2012), but averagingis simpler to implement.Our experimental results show that our model per-1372Figure 1: A recursive representations of a phrase ?aword vector?
with POS tags of the words (DT, NN andNN respectively).
For example, the two word-POS pairs?word NN?
and ?vector NN?
with a syntactic categoryN are combined to represent the phrase ?word vector?.forms better than standard RNN models.
By av-eraging the model parameters, our model achievesperformance competitive with the MV-RNN modelin Socher et al(2012), without using computation-ally expensive word-dependent matrices.2 An Averaged RNN Model with SyntaxOur model is a supervised RNN that works on a bi-nary syntactic tree.
As our first step to leverage in-formation available in the tree, we distinguish wordswith the same spelling but POS tags in the vectorspace.
Our model also uses different weight ma-trices dependent on the phrase categories of childnodes (phrases or words) in combining phrases.
Ourmodel further weights those nodes that appear to beimportant.Compositional functions of our model followthose of the SU-RNN model in Socher et al(2013).2.1 Word-POS Vector RepresentationsOur model simply assigns vector representations toword-POS pairs.
For example, a word ?caused?can be represented in two ways: ?caused VBD?
and?caused VBN?.
The vectors are represented as col-umn vectors in a matrix We ?
Rd?|V|, where d isthe dimension of a vector and V is a set of all word-POS pairs we consider.2.2 Compositional Functions with SyntaxIn construction of parse trees, we associate each ofthe tree node with its d-dimensional vector represen-tation computed from vector representations of itssubtrees.
For leaf nodes, we look up word-POS vec-tor representations in V. Figure 1 shows an exampleof such recursive representations.
A parent vectorp ?
Rd?1 is computed from its direct child vectorscl and cr?
Rd?1:p = tanh(?lWTcl ,Tcrl cl+?rWTcl ,Tcrr cr+bTcl ,Tcr ),where W Tcl ,Tcrl and WTcl ,Tcrr ?
Rd?d are weightmatrices that depend on the phrase categories of cland cr.
Here, cl and cr have phrase categories Tcland Tcr respectively (such as N, V, etc.).
bTcl ,Tcr ?Rd?1 is a bias vector.
To incorporate the impor-tance of phrases into the model, two subtrees of anode may have different weights ?l ?
[0, 1] and?r(= 1 ?
?l), taking phrase importance into ac-count.
The value of ?l is manually specified andautomatically applied to all nodes based on priorknowledge about the task.
In this way, we can com-pute vector representations for phrases of arbitrarylength.
We denote a set of such matrices as Wlr andbias vectors as b.2.3 Objective Function and LearningAs with other RNN models, we add on the top of anode x a softmax classifier.
The classifier is used topredict a K-class distribution d(x) ?
RK?1 over aspecific task to train our model:d(x) = softmax(W labelx+ blabel), (1)where W label ?
RK?d is a weight matrix andblabel ?
RK?1 is a bias vector.
We denote t(x) ?RK?1 as the target distribution vector at node x.t(x) has a 0-1 encoding: the entry at the correct la-bel of t(x) is 1, and the remaining entries are 0.
Wethen compute the cross entropy error between d(x)and t(x):E(x) = ?K?k=1tk(x)logdk(x),and define an objective function as the sum of E(x)over all training data:J(?)
=?xE(x) + ?2??
?2,where ?
= (We,Wlr, b,W label, blabel) is the set ofour model parameters that should be learned.
?
is avector of regularization parameters.1373To compute d(x), we can directly leverage anyother nodes?
feature vectors in the same tree.
Wedenote such additional feature vectors as x?i ?
Rd?1,and extend Eq.
(1):d(x) = softmax(W labelx+?iW addi x?i +blabel),where W addi ?
RK?d are weight matrices for addi-tional features.
We denote these matrices W addi asW add.
We also add W add to ?:?
= (We,Wlr, b,W label,W add, blabel).The gradient of J(?)?J(?)??=?x?E(x)?
?+ ?
?is efficiently computed via backpropagation throughstructure (Goller and Ku?chler, 1996).
To minimizeJ(?
), we use batch L-BFGS1 (Hermann and Blun-som, 2013; Socher et al 2012).2.4 AveragingWe use averaged model parameters?
= 1T + 1T?t=0?tat test time, where ?t is the vector of model parame-ters after t iterations of the L-BFGS optimization.Our preliminary experimental results suggest thataveraging ?
except We works well.3 Experimental SettingsWe used the Enju parser (Miyao and Tsujii, 2008)for syntactic parsing.
We used 13 phrase categoriesgiven by Enju.3.1 Task: Semantic Relation ClassificationWe evaluated our model on a semantic relation clas-sification task: SemEval 2010 Task 8 (Hendrickx etal., 2010).
Following Socher et al(2012), we re-garded the task as a 19-class classification problem.There are 8,000 samples for training, and 2,717 for1We used libLBFGS provided at http://www.chokkan.org/software/liblbfgs/.Figure 2: Classifying the relation between two entities.test.
For the validation set, we randomly sampled2,182 samples from the training data.To predict a class label, we first find the minimalphrase that covers the target entities and then use thevector representation of the phrase (Figure 2).As explained in Section 2.3, we can directly con-nect features on any other nodes to the softmax clas-sifier.
In this work, we used three such internal fea-tures: two vector representations of target entitiesand one averaged vector representation of words be-tween the entities2.3.2 Weights on PhrasesWe tuned the weight ?l (or ?r) introduced in Sec-tion 2.2 for this particular task.
There are two fac-tors: syntactic heads and syntactic path between tar-get entities.
Our model puts a weight ?
?
[0.5, 1]on head phrases, and 1 ?
?
on the others.
For re-lation classification tasks, syntactic paths betweentarget entities are important (Zhang et al 2006), soour model also puts another weight ?
?
[0.5, 1] onphrases on the path, and 1 ?
?
on the others.
Whenboth child nodes are on the path or neither of themon the path, we set ?
= 0.5.
The two weight fac-tors are summed up and divided by 2 to be the finalweights ?l and ?r to combine the phrases.
For ex-ample, we set ?l = (1??
)+?2 and ?r =?+(1??
)2when the right child node is the head and the leftchild node is on the path.3.3 Initialization of Model Parameters andTuning of HyperparametersWe initialized We with 50-dimensional word vec-tors3 trained with the model of Collobert et2Socher et al(2012) used richer features including wordsaround entity pairs in their implementation.3The word vectors are provided at http://ronan.collobert.com/senna/.
We used the vectors without anymodifications such as normalization.1374Method F1 (%)Our model 79.4RNN 74.8MV-RNN 79.1RNN w/ POS, WordNet, NER 77.6MV-RNN w/ POS, WordNet, NER 82.4SVM w/ bag-of-words 73.1SVM w/ lexical and semantic features 82.2Table 1: Comparison of our model with other methods onSemEval 2010 task 8.Method F1 (%)Our model 79.4Our model w/o phrase categories (PC) 77.7Our model w/o head weights (HW) 78.8Our model w/o path weights (PW) 78.7Our model w/o averaging (AVE) 76.9Our model w/o PC, HW, PW, AVE 74.1Table 2: Contributions of syntactic and task-specific in-formation and averaging.al.
(2011), and Wlr with I2 + ?, where I ?
Rd?dis an identity matrix.
Here, ?
is zero-mean gaussianrandom variable with a variance of 0.01.
The ini-tialization of Wlr is the same as that of Socher etal.
(2013).
The remaining model parameters wereinitialized with 0.We tuned hyperparameters in our model using thevalidation set for each experimental setting.
The hy-perparameters include the regularization parametersfor We, Wlr, W label and W add, and the weights ?and ?.
For example, the best performance for ourmodel with all the proposed methods was obtainedwith the values: 10?6, 10?4, 10?3, 10?3, 0.7 and0.9 respectively.4 Results and DiscussionTable 1 shows the performance of our model and thatof previously reported systems on the test set.
Theperformance of an SVM system with bag-of-wordsfeatures was reported in Rink and Harabagiu (2010),and the performance of the RNN and MV-RNNmodels was reported in Socher et al(2012).
Ourmodel achieves an F1 score of 79.4% and outper-forms the RNN model (74.8% F1) as well as thesimple SVM-based system (73.1% F1).
More no-Figure 3: F1 vs Training iterations.tably, the score of our model is competitive with thatof the MV-RNN model (79.1% F1), which is com-putationally much more expensive.
Readers are re-ferred to Hermann and Blunsom (2013) for the dis-cussion about the computational complexity of theMV-RNN model.
We improved the performance ofRNN models on the task without much increasingthe complexity.
This is a significant practical advan-tage of our model, although its expressive power isnot the same as that of the MV-RNN model.Our model outperforms the RNN model with onelexical and two semantic external features: POStags, WordNet hypernyms and named entity tags(NER) of target word pairs (external features).
TheMV-RNN model with external features shows bet-ter performance than our model.
An SVM with richlexical and semantic features (Rink and Harabagiu,2010) also outperforms ours.
Note, however, thatthis is not a fair comparison because those mod-els use rich external resources such as WordNet andnamed entity tags.4.1 Contributions of Proposed MethodsWe conducted additional experiments to quantify thecontributions of phrase categories, heads, paths andaveraging to our classification score.
As shown inTable 2, our model without phrase categories, headsor paths still outperforms the RNN model with ex-ternal features.
On the other hand, our model with-out averaging yields a lower score than the RNNmodel with external features, though it is still bet-1375ter than the RNN model alone.
Without utiliz-ing these four properties, our model obtained only74.1% F1.
These results indicate that syntactic andtask-specific information and averaging contributeto the performance improvement.
The improvementis achieved by a simple modification of composi-tional functions in RNN models.4.2 Effects of Averaging in TrainingFigure 3 shows the training curves in terms of F1scores.
These curves clearly demonstrate that pa-rameter averaging helps to stabilize the learning andimprove generalization capacity.5 ConclusionWe have presented an averaged RNN model for se-mantic relation classification.
Our experimental re-sults show that syntactic information such as phrasecategories and heads improves the performance, andthe task-specific weighting is also beneficial.
Theresults also demonstrate that averaging the modelparameters not only stabilizes the learning but alsoimproves the generalization capacity of the model.As future work, we plan to combine deep learningmodels with richer information such as predicate-argument structures.AcknowledgmentsWe thank the anonymous reviewers for their insight-ful comments.ReferencesRonan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural Language Processing (almost) from Scratch.In JMLR, 12:2493?2537.Christoph Goller and Andreas Ku?chler.
1996.
LearningTask-Dependent Distributed Representations by Back-propagation Through Structure.
In ICNN.Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, PreslavNakov, Diarmuid ?O Se?aghdha, Sebastian Pado?, MarcoPennacchiotti, Lorenza Romano and Stan Szpakowicz.2010.
SemEval-2010 Task 8: Multi-Way Classicationof Semantic Relations Between Pairs of Nominals.
InSemEval 2010.Karl Moritz Hermann and Phil Blunsom.
2013.
The Roleof Syntax in Vector Space Models of Compositional Se-mantics.
In ACL.Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,Ilya Sutskever and Ruslan R. Salakhutdinov.
2012.Improving neural networks by preventing co-adaptation of feature detectors.
In arXiv:1207.0580.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature ForestModels for Probabilistic HPSG Parsing.
In Computa-tional Linguistics, 34(1):35?80, MIT Press.Bryan Rink and Sanda Harabagiu.
2010.
UTD: Clas-sifying Semantic Relations by Combining Lexical andSemantic Resources.
In SemEval 2010.Richard Socher, Brody Huval, Christopher D. Manningand Andrew Y. Ng.
2012.
Semantic CompositionalityThrough Recursive Matrix-Vector Spaces.
In EMNLP.Richard Socher, John Bauer, Christopher D. Manning andAndrew Y. Ng.
2013.
Parsing with CompositionalVector Grammars.
In ACL.Kevin Swersky, Bo Chen, Ben Marlin and Nando de Fre-itas.
2010.
A tutorial on stochastic approximation al-gorithms for training Restricted Boltzmann Machinesand Deep Belief Nets.
In ITA workshop.Min Zhang, Jie Zhang, Jian Su and Guodong Zhou.
2006.A Composite Kernel to Extract Relations between En-tities with Both Flat and Structured Features.
In COL-ING/ACL.1376
