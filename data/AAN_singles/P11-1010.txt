Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 93?101,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsSemi-Supervised SimHash for Efficient Document Similarity SearchQixia Jiang and Maosong SunState Key Laboratory on Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Sci.
and Tech., Tsinghua University, Beijing 100084, Chinaqixia.jiang@gmail.com, sms@tsinghua.edu.cnAbstractSearching documents that are similar to aquery document is an important componentin modern information retrieval.
Some ex-isting hashing methods can be used for effi-cient document similarity search.
However,unsupervised hashing methods cannot incor-porate prior knowledge for better hashing.Although some supervised hashing methodscan derive effective hash functions from priorknowledge, they are either computationallyexpensive or poorly discriminative.
This pa-per proposes a novel (semi-)supervised hash-ing method named Semi-Supervised SimHash(S3H) for high-dimensional data similaritysearch.
The basic idea of S3H is to learn theoptimal feature weights from prior knowledgeto relocate the data such that similar data havesimilar hash codes.
We evaluate our methodwith several state-of-the-art methods on twolarge datasets.
All the results show that ourmethod gets the best performance.1 IntroductionDocument Similarity Search (DSS) is to find sim-ilar documents to a query doc in a text corpus oron the web.
It is an important component in mod-ern information retrieval since DSS can improve thetraditional search engines and user experience (Wanet al, 2008; Dean et al, 1999).
Traditional searchengines accept several terms submitted by a useras a query and return a set of docs that are rele-vant to the query.
However, for those users whoare not search experts, it is always difficult to ac-curately specify some query terms to express theirsearch purposes.
Unlike short-query based search,DSS queries by a full (long) document, which allowsusers to directly submit a page or a document to thesearch engines as the description of their informa-tion needs.
Meanwhile, the explosion of informationhas brought great challenges to traditional methods.For example, Inverted List (IL) which is a primarykey-term access method would return a very largeset of docs for a query document, which leads to thetime-consuming post-processing.
Therefore, a neweffective algorithm is required.Hashing methods can perform highly efficient butapproximate similarity search, and have gained greatsuccess in many applications such as Content-BasedImage Retrieval (CBIR) (Ke et al, 2004; Kulis etal., 2009b), near-duplicate data detection (Ke etal., 2004; Manku et al, 2007; Costa et al, 2010),etc.
Hashing methods project high-dimensional ob-jects to compact binary codes called fingerprints andmake similar fingerprints for similar objects.
Thesimilarity search in the Hamming space1 is muchmore efficient than in the original attribute space(Manku et al, 2007).Recently, several hashing methods have been pro-posed.
Specifically, SimHash (SH) (Charikar M.S.,2002) uses random projections to hash data.
Al-though it works well with long fingerprints, SH haspoor discrimination power for short fingerprints.
Akernelized variant of SH, called Kernelized Local-ity Sensitive Hashing (KLSH) (Kulis et al, 2009a),is proposed to handle non-linearly separable data.These methods are unsupervised thus cannot incor-porate prior knowledge for better hashing.
Moti-1Hamming space is a set of binary strings of length L.93vated by this, some supervised methods are pro-posed to derive effective hash functions from priorknowledge, i.e., Spectral Hashing (Weiss et al,2009) and Semi-Supervised Hashing (SSH) (Wanget al, 2010a).
Regardless of different objectives,both methods derive hash functions via PrincipleComponent Analysis (PCA) (Jolliffe, 1986).
How-ever, PCA is computationally expensive, which lim-its their usage for high-dimensional data.This paper proposes a novel (semi-)supervisedhashing method, Semi-Supervised SimHash (S3H),for high-dimensional data similarity search.
Un-like SSH that tries to find a sequence of hash func-tions, S3H fixes the random projection directionsand seeks the optimal feature weights from priorknowledge to relocate the objects such that simi-lar objects have similar fingerprints.
This is im-plemented by maximizing the empirical accuracyon the prior knowledge (labeled data) and the en-tropy of hash functions (estimated over labeled andunlabeled data).
The proposed method avoids us-ing PCA which is computationally expensive espe-cially for high-dimensional data, and leads to anefficient Quasi-Newton based solution.
To evalu-ate our method, we compare with several state-of-the-art hashing methods on two large datasets, i.e.,20 Newsgroups (20K points) and Open DirectoryProject (ODP) (2.4 million points).
All experimentsshow that S3H gets the best search performance.This paper is organized as follows: Section 2briefly introduces the background and some relatedworks.
In Section 3, we describe our proposed Semi-Supervised SimHash (S3H).
Section 4 provides ex-perimental validation on two datasets.
The conclu-sions are given in Section 5.2 Background and Related WorksSuppose we are given a set of N documents, X ={xi | xi ?
RM}Ni=1.
For a given query doc q, DSStries to find its nearest neighbors in X or a subsetX ?
?
X in which distance from the documents tothe query doc q is less than a give threshold.
How-ever, such two tasks are computationally infeasiblefor large-scale data.
Thus, it turns to the approxi-mate similarity search problem (Indyk et al, 1998).In this section, we briefly review some related ap-proximate similarity search methods.2.1 SimHashSimHash (SH) is first proposed by Charikar(Charikar M.S., 2002).
SH uses random projectionsas hash functions, i.e.,h(x) = sign(wTx) ={+1, if wTx ?
0?1, otherwise (1)where w ?
RM is a vector randomly generated.
SHspecifies the distribution on a family of hash func-tions H = {h} such that for two objects xi and xj ,Prh?H{h(xi) = h(xj)} = 1??(xi,xj)?
(2)where ?
(xi,xj) is the angle between xi and xj .
Ob-viously, SH is an unsupervised hashing method.2.2 Kernelized Locality Sensitive HashingA kernelized variant of SH, named KernelizedLocality Sensitive Hashing (KLSH) (Kulis et al,2009a), is proposed for non-linearly separable data.KLSH approximates the underling Gaussian distri-bution in the implicit embedding space of data basedon central limit theory.
To calculate the value ofhashing fuction h(?
), KLSH projects points onto theeigenvectors of the kernel matrix.
In short, the com-plete procedure of KLSH can be summarized as fol-lows: 1) randomly select P (a small value) pointsfrom X and form the kernel matrix, 2) for each hashfunction h(?
(x)), calculate its weight ?
?
RP justas Kernel PCA (Scho?lkopf et al, 1997), and 3) thehash function is defined as:h(?
(x)) = sign(P?i=1?i ?
?
(x,xi)) (3)where ?
(?, ?)
can be any kernel function.KLSH can improve hashing results via the kerneltrick.
However, KLSH is unsupervised, thus design-ing a data-specific kernel remains a big challenge.2.3 Semi-Supervised HashingSemi-Supervised Hashing (SSH) (Wang et al,2010a) is recently proposed to incorporate priorknowledge for better hashing.
Besides X , priorknowledge in the form of similar and dissimilarobject-pairs is also required in SSH.
SSH tries tofind L optimal hash functions which have maximum94empirical accuracy on prior knowledge and maxi-mum entropy by finding the top L eigenvectors ofan extended covariance matrix2 via PCA or SVD.However, despite of the potential problems of nu-merical stability, SVD requires massive computa-tional space and O(M3) computational time whereM is feature dimension, which limits its usage forhigh-dimensional data (Trefethen et al, 1997).
Fur-thermore, the variance of directions obtained byPCA decreases with the decrease of the rank (Jol-liffe, 1986).
Thus, lower hash functions tend to havesmaller entropy and larger empirical errors.2.4 OthersSome other related works should be mentioned.
Anotable method is Locality Sensitive Hashing (LSH)(Indyk et al, 1998).
LSH performs a randomlinear projection to map similar objects to similarhash codes.
However, LSH suffers from the effi-ciency problem that it tends to generate long codes(Salakhutdinov et al, 2007).
LAMP (Mu et al,2009) considers each hash function as a binary par-tition problem as in SVMs (Burges, 1998).
Spec-tral Hashing (Weiss et al, 2009) maintains similar-ity between objects in the reduced Hamming spaceby minimizing the averaged Hamming distance3 be-tween similar neighbors in the original Euclideanspace.
However, spectral hashing takes the assump-tion that data should be distributed uniformly, whichis always violated in real-world applications.3 Semi-Supervised SimHashIn this section, we present our hashing method,named Semi-Supervised SimHash (S3H).
Let XL ={(x1, c1) .
.
.
(xu, cu)} be the labeled data, c ?
{1 .
.
.
C}, x ?
RM , and XU = {xu+1 .
.
.xN} theunlabeled data.
Let X = XL ?
XU .
Given thelabeled data XL, we construct two sets, attractionset ?a and repulsion set ?r.
Specifically, any pair(xi,xj) ?
?a, i, j ?
u, denotes that xi and xjare in the same class, i.e., ci = cj , while any pair(xi,xj) ?
?r, i, j ?
u, denotes that ci ?= cj .
Unlike2The extended covariance matrix is composed of two com-ponents, one is an unsupervised covariance term and another isa constraint matrix involving labeled information.3Hamming distance is defined as the number of bits that aredifferent between two binary strings.previews works that attempt to find L optimal hyper-planes, the basic idea of S3H is to fix L random hy-perplanes and to find an optimal feature-weight vec-tor to relocate the objects such that similar objectshave similar codes.3.1 Data RepresentationSince L random hyperplanes are fixed, we can rep-resent a object x ?
X as its relative position to theserandom hyperplanes, i.e.,D = ?
?V (4)where the element Vml ?
{+1,?1, 0} of V indi-cates that the object x is above, below or just in thel-th hyperplane with respect to them-th feature, and?
= diag(|x1|, |x2|, .
.
.
, |xM |) is a diagonal matrixwhich, to some extent, reflects the distance from xto these hyperplanes.3.2 FormulationHashing maps the data set X to an L-dimensionalHamming space for compact representations.
If werepresent each object as Equation (4), the l-th hashfunction is then defined as:hl(x) = ~l(D) = sign(wTdl) (5)where w ?
RM is the feature weight to be deter-mined and dl is the l-th column of the matrixD.Intuitively, the ?contribution?
of a specific featureto different classes is different.
Therefore, we hopeto incorporate this side information in S3H for betterhashing.
Inspired by (Madani et al, 2009), we canmeasure this contribution overXL as in Algorithm 1.Clearly, if objects are represented as the occurrencenumbers of features, the output of Algorithm 1 isjust the conditional probability Pr(class|feature).Finally, each object (x, c) ?
XL can be representedas anM ?
L matrixG:G = diag(?1,c, ?2,c, .
.
.
, ?M,c) ?D (6)Note that, one pair (xi,xj) in ?a or ?r correspondsto (Gi,Gj) while (Di,Dj) if we ignore features?contribution to different classes.Furthermore, we also hope to maximize the em-pirical accuracy on the labeled data ?a and ?r and95Algorithm 1: Feature Contribution Calculationfor each (x, c) ?
XL dofor each f ?
x do?f ?
?f + xf ;?f,c ?
?f,c + xf ;endendfor each feature f and class c do?f,c ?
?f,c?f;endmaximize the entropy of hash functions.
So, we de-fine the following objective for ~(?
)s:J(w) = 1NpL?l=1{?(xi,xj)??a~l(xi)~l(xj)??(xi,xj)?
?r~l(xi)~l(xj)}+ ?1L?l=1H(~l)(7)where Np = |?a| + |?r| is the number of attrac-tion and repulsion pairs and ?1 is a tradeoff betweentwo terms.
Wang et al have proven that hash func-tions with maximum entropy must maximize thevariance of the hash values, and vice-versa (Wanget al, 2010b).
Thus, H(~(?))
can be estimated overthe labeled and unlabeled data, XL and XU .Unfortunately, direct solution for above problemis non-trivial since Equation (7) is not differentiable.Thus, we relax the objective and add an additionalregularization term which could effectively avoidoverfitting.
Finally, we obtain the total objective:L(w) = 1NpL?l=1{?(Gi,Gj)??a?(wTgi,l)?(wTgj,l)??(Gi,Gj)??r?(wTgi,l)?
(wTgj,l)}+ ?12NL?l=1{u?i=1?2(wTgi,l) +N?i=u+1?2(wTdi,l)}?
?22?w?22(8)where gi,l and di,l denote the l-th column ofGi andDi respectively, and ?
(t) is a piece-wise linear func-tion defined as:?
(t) =??
?Tg t > Tgt ?Tg ?
t ?
Tg?Tg t < ?Tg(9)This relaxation has a good intuitive explanation.That is, similar objects are desired to not only havethe similar fingerprints but also have sufficient largeprojection magnitudes, while dissimilar objects aredesired to not only differ in their fingerprints but alsohave large projection margin.
However, we do nothope that a small fraction of object-pairs with verylarge projection magnitude or margin dominate thecomplete model.
Thus, a piece-wise linear function?(?)
is applied in S3H.As a result, Equation (8) is a simply uncon-strained optimization problem, which can be ef-ficiently solved by a notable Quasi-Newton algo-rithm, i.e., L-BFGS (Liu et al, 1989).
For descrip-tion simplicity, only attraction set ?a is consideredand the extension to repulsion set ?r is straightfor-ward.
Thus, the gradient of L(w) is as follows:?L(w)?w =1NpL?l=1{?
(Gi,Gj) ?
?a,|wT gi,l| ?
Tg?
(wTgj,l) ?
gi,l+?
(Gi,Gj) ?
?a,|wT gj,l| ?
Tg?
(wTgi,l) ?
gj,l}(10)+ ?1NL?l=1{ u?i = 1,|wT gi,l| ?
Tg?
(wTgi,l) ?
gi,l+N?i = u + 1,|wTdi,l| ?
Tg?
(wTdi,l) ?
di,l}?
?2wNote that ??
(t)/?t = 0 when |t| > Tg.3.3 Fingerprint GenerationWhen we get the optimal weight w?, we generatefingerprints for given objects through Equation (5).Then, it tunes to the problem how to efficiently ob-tain the representation as in Figure 4 for a object.After analysis, we find: 1) hyperplanes are randomlygenerated and we only need to determine whichsides of these hyperplanes the given object lies on,and 2) in real-world applications, objects such asdocs are always very sparse.
Thus, we can avoidheavy computational demands and efficiently gener-ate fingerprints for objects.In practice, given an object x, the procedure ofgenerating anL-bit fingerprint is as follows: it main-tains an L-dimensional vector initialized to zero.Each feature f ?
x is firstly mapped to an L-bithash value by Jenkins Hashing Function4.
Then,4http://www.burtleburtle.net/bob/hash/doobs.html96Algorithm 2: Fast Fingerprint GenerationINPUT: x and w?
;initialize ??
0,?
?
0, ?,?
?
RL;for each f ?
x dorandomly project f to hf ?
{?1,+1}L;??
?+ xf ?
w?f ?
hf ;endfor l = 1 to L doif ?l > 0 then?l ?
1;endendRETURN ?
;these L bits increment or decrement the L compo-nents of the vector by the value xf ?
w?f .
After allfeatures processed, the signs of components deter-mine the corresponding bits of the final fingerprint.The complete algorithm is presented in Algorithm 2.3.4 Algorithmic AnalysisThis section briefly analyzes the relation betweenS3H and some existing methods.
For analysis sim-plicity, we assume ?
(t) = t and ignore the regular-ization terms.
So, Equation (8) can be rewritten asfollows:J(w)S3H =12wT [L?l=1?l(?+ ???
)?Tl ]w (11)where ?+ij equals to 1 when (xi,xj) ?
?a otherwise0, ?
?ij equals to 1 when (xi,xj) ?
?r otherwise0, and ?l = [g1,l .
.
.gu,l,du+1,l .
.
.dN,l].
We de-note?l ?l?+?Tl and?l ?l??
?Tl as S+ and S?respectively.
Therefore, maximizing above functionis equivalent to maximizing the following:?J(w)S3H =|wTS+w||wTS?w|(12)Clearly, Equation (12) is analogous to Linear Dis-criminant Analysis (LDA) (Duda et al, 2000) ex-cept for the difference: 1) measurement.
S3H usessimilarity while LDA uses distance.
As a result, theobjective function of S3H is just the reciprocal ofLDA?s.
2) embedding space.
LDA seeks the bestseparative direction in the original attribute space.
Incontrast, S3H firstly maps data from RM to RM?Lthrough the following projection function?
(x) = x ?
[diag(sign(r1)), .
.
.
,diag(sign(rL))] (13)where rl ?
RM , l = 1, .
.
.
, L, are L random hyper-planes.
Then, in that space (RM?L), S3H seeks adirection5 that can best separate the data.From this point of view, it is obvious that the basicSH is a special case of S3H when w is set to e =[1, 1, .
.
.
, 1].
That is, SH firstly maps the data via?(?)
just as S3H.
But then, SH directly separates thedata in that feature space at the direction e.Analogously, we ignore the regularization termsin SSH and rewrite the objective of SSH as:J(W)SSH =12tr[WTX(?+ ???
)XTW] (14)where W = [w1, .
.
.
,wL] ?
RM?L are L hyper-planes and X = [x1, .
.
.
,xN ].
Maximizing this ob-jective is equivalent to maximizing the following:?J(W)SSH =| tr[WTS?+W]|| tr[WTS?
?W]|(15)where S?+ = X?+XT and S??
= X?
?XT .
Equa-tion (15) shows that SSH is analogous to MultipleDiscriminant Analysis (MDA) (Duda et al, 2000).In fact, SSH uses top L best-separative hyperplanesin the original attribute space found via PCA to hashthe data.
Furthermore, we rewrite the projectionfunction ?(?)
in S3H as:?
(x) = x ?
[R1, .
.
.
,RL] (16)where Rl = diag(sign(rl)).
Each Rl is a mappingfrom RM to RM and corresponds to one embeddingspace.
From this perspective, unlike SSH, S3H glob-ally seeks a direction that can best separate the datain L different embedding spaces simultaneously.4 ExperimentsWe use two datasets 20 Newsgroups and Open Di-rectory Project (ODP) in our experiments.
Each doc-ument is represented as a vector of occurrence num-bers of the terms within it.
The class informationof docs is considered as prior knowledge that twodocs within a same class should have more similarfingerprints while two docs within different classesshould have dissimilar fingerprints.
We will demon-strate that our S3H can effectively incorporate thisprior knowledge to improve the DSS performance.5The direction is determined by concatenating w L times.9724 32 40 48 56 640.10.20.30.40.5MeanAveragedPrecision(MAP)Number of bitsS3HS3HfSSHSHKLSH(a)24 32 40 48 56 640.10.20.30.40.5MeanAveragedPrecision(MAP)Number of bitsS3HS3HfSSHSHKLSH(b)Figure 1: Mean Averaged Precision (MAP) for differentnumber of bits for hash ranking on 20 Newsgroups.
(a)10K features.
(b) 30K features.We use Inverted List (IL) (Manning et al, 2002)as the baseline.
In fact, given a query doc, IL re-turns all the docs that contain any term within it.We also compare our method with three state-of-the-art hashing methods, i.e., KLSH, SSH and SH.In KLSH, we adopt the RBF kernel ?
(xi,xj) =exp(?
?xi?xj?22?2 ), where the scaling factor ?2 takes0.5 and the other two parameters p and t are set tobe 500 and 50 respectively.
The parameter ?
in SSHis set to 1.
For S3H, we simply set the parameters ?1and ?2 in Equation (8) to 4 and 0.5 respectively.
Toobjectively reflect the performance of S3H, we eval-uate our S3H with and without Feature ContributionCalculation algorithm (FCC) (Algorithm 1).
Specif-ically, FCC-free S3H (denoted as S3Hf ) is just asimplification whenGs in S3H are simply set toDs.For quantitative evaluation, as in literature (Wanget al, 2010b; Mu et al, 2009), we calculate the pre-cision under two scenarios: hash lookup and hashranking.
For hash lookup, the proportion of goodneighbors (have the same class label as the query)among the searched objects within a given Hammingradius is calculated as precision.
Similarly to (Wanget al, 2010b; Weiss et al, 2009), for a query doc-ument, if no neighbors within the given Hammingradius can be found, it is considered as zero preci-sion.
Note that, the precision of IL is the propor-tion of good neighbors among the whole searchedobjects.
For hash ranking, all the objects in X areranked in terms of their Hamming distance from thequery document, and the top K nearest neighborsare returned as the result.
Then, Mean Averaged Pre-cision (MAP) (Manning et al, 2002) is calculated.We also calculate the averaged intra- and inter- classHamming distance for various hashing methods.
In-24 32 40 48 56 640.000.050.100.150.200.250.300.350.40PrecisionwithinHammingradius3Number of bitsS3HS3HfSSHSHKLSHIL(a)24 32 40 48 56 640.000.050.100.150.200.250.300.350.40PrecisionwithinHammingradius3Number of bitsS3HS3HfSSHSHKLSHIL(b)Figure 2: Precision within Hamming radius 3 for hashlookup on 20 Newsgroups.
(a) 10K features.
(b) 30Kfeatures.tuitively, a good hashing method should have smallintra-class distance while large inter-class distance.We test all the methods on a PC with a 2.66 GHzprocessor and 12GB RAM.
All experiments repeate10 times and the averaged results are reported.4.1 20 Newsgroups20 Newsgroups6 contains 20K messages, about 1Kmessages from each of 20 different newsgroups.The entire vocabulary includes 62,061 words.
Toevaluate the performance for different feature di-mensions, we use Chi-squared feature selection al-gorithm (Forman, 2003) to select 10K and 30K fea-tures.
The averaged message length is 54.1 for 10Kfeatures and 116.2 for 30K features.
We randomlyselect 4K massages as the test set and the remain16K as the training set.
To train SSH and S3H,from the training set, we randomly generate 40Kmessage-pairs as ?a and 80K message-pairs as ?r.For hash ranking, Figure 1 shows MAP for vari-ous methods using different number of bits.
It showsthat performance of SSH decreases with the grow-ing of hash bits.
This is mainly because the varianceof the directions obtained by PCA decreases withthe decrease of their ranks.
Thus, lower bits havelarger empirical errors.
For S3H, FCC (Algorithm 1)can significantly improve the MAP just as discussedin Section 3.2.
Moreover, the MAP of FCC-freeS3H (S3Hf ) is affected by feature dimensions whileFCC-based (S3H) is relatively stable.
This impliesFCC can also improve the satiability of S3H.
As wesee, S3Hf ignores the contribution of features to dif-ferent classes.
However, besides the local descrip-tion of data locality in the form of object-pairs, such6http://www.cs.cmu.edu/afs/cs/project/theo-3/www/98S3HS3HfSSHSHKLSHIL24 32 40 48 56 6410-1100101102103104NumberofsearcheddataNumber of bits(a)S3HS3HfSSHSHKLSHIL24 32 40 48 56 6410-1100101102103104NumberofsearcheddataNumber of bits(b)Figure 3: Averaged searched sample numbers using 4Kquery messages for hash lookup.
(a) 10K features.
(b)30K features.
(global) information also provides a proper guidancefor hashing.
So, for S3Hf , the reason why its re-sults with 30K features are worse than the resultswith 10K features is probably because S3Hf learnsto hash only according to the local description ofdata locality and many not too relevant features leadto relatively poor description.
In contrast, S3H canutilize global information to better understand thesimilarity among objects.
In short, S3H obtains thebest MAP for all bits and feature dimensions.For hash lookup, Figure 2 presents the precisionwithin Hamming radius 3 for different number ofbits.
It shows that IL even outperforms SH.
Thisis because few objects can be hashed by SH into onehash bucket.
Thus, for many queries, SH fails toreturn any neighbor even in a large Hamming radiusof 3.
Clearly, S3H outperforms all the other methodsfor different number of hash bits and features.The number of messages searched by differentmethods are reported in Figure 3.
We find that thenumber of searched data of S3H (with/without FCC)decreases much more slowly than KLSH, SH andSSHwith the growing of the number of hash bits.
Asdiscussed in Section 3.4, this mainly benefits fromthe design of S3H that S3H (globally) seeks a di-rection that can best separate the data in L embed-ding spaces simultaneously.
We also find IL returnsa large number of neighbors of each query messagewhich leads to its poor efficiency.The averaged intra- and inter- class Hamming dis-tance of different methods are reported in Table 1.As it shows, S3H has relatively larger margin (?
)between intra- and inter-class Hamming distance.This indicates that S3H is more effective to makesimilar points have similar fingerprints while keepintra-class inter-class ?S3H 13.1264 15.6342 2.5078S3Hf 12.5754 13.3479 0.7725SSH 6.4134 6.5262 0.1128SH 15.3908 15.6339 0.2431KLSH 10.2876 10.8713 0.5841Table 1: Averaged intra- and inter- class Hamming dis-tance of 20 Newsgroups for 32-bit fingerprint.
?
is thedifference between the averaged inter- and intra- classHamming distance.
Large ?
implies good hashing.10 20 30 40100101102103Time(sec.
)Feature dimension (K)S3HSSHSHKLSHIL(a)10 20 30 40101102103104Space(MB)Feature dimension (K)S3HSSHSHKLSHIL(b)Figure 4: Computational complexity of training for dif-ferent feature dimensions for 32-bit fingerprint.
(a) Train-ing time (sec).
(b) Training space cost (MB).the dissimilar points away enough from each other.Figure 4 shows the (training) computational com-plexity of different methods.
We find that the timeand space cost of SSH grows much faster than SH,KLSH and S3H with the growing of feature dimen-sion.
This is mainly because SSH requires SVD tofind the optimal hashing functions which is compu-tational expensive.
Instead, S3H seeks the optimalfeature weights via L-BFGS, which is still efficienteven for very high-dimensional data.4.2 Open Directory Project (ODP)Open Directory Project (ODP)7 is a multilingualopen content directory of web links (docs) organizedby a hierarchical ontology scheme.
In our exper-iment, only English docs8 at level 3 of the cate-gory tree are utilized to evaluate the performance.In short, the dataset contains 2,483,388 docs within6,008 classes.
There are totally 862,050 distinctwords and each doc contains 14.13 terms on aver-age.
Since docs are too short, we do not conduct7http://rdf.dmoz.org/8The title together with the corresponding short descriptionof a page are considered as a document in our experiments.991 10 100 1k 10k 100k0.000.010.020.030.04PercentageClass size(a)0 20 40 60 80 100 1200.000.020.040.060.080.10PercentageDocument length(b)Figure 5: Overview of ODP data set.
(a) Class distribu-tion at level 3.
(b) Distribution of document length.intra-class inter-class ?S3H 14.0029 15.9508 1.9479S3Hf 14.3801 15.5260 1.1459SH 14.7725 15.6432 0.8707KLSH 9.3382 10.5700 1.2328Table 2: Averaged intra- and inter- class Hamming dis-tance of ODP for 32-bit fingerprint (860K features).
?is the difference between averaged intra- and inter- classHamming distance.feature selection9.
An overview of ODP is shown inFigure 5.
We randomly sample 10% docs as the testset and the remain as the training set.
Furthermore,from training set, we randomly generate 800K doc-pairs as ?a, and 1 million doc-pairs as ?r.
Notethat, since there are totally over 800K features, itis extremely inefficient to train SSH.
Therefore, weonly compare our S3H with IL, KLSH and SH.The search performance is given in Figure 6.
Fig-ure 6(a) shows the MAP for various methods usingdifferent number of bits.
It shows KLSH outper-forms SH, which mainly contributes to the kerneltrick.
S3H and S3Hf have higher MAP than KLSHand SH.
Clearly, FCC algorithm can improve theMAP of S3H for all bits.
Figure 6(b) presents theprecision within Hamming radius 2 for hash lookup.We find that IL outperforms SH since SH fails formany queries.
It also shows that S3H (with FCC)can obtain the best precision for all bits.Table 2 reports the averaged intra- and inter-classHamming distance for various methods.
It showsthat S3H has the largest margin (?).
This demon-9We have tested feature selection.
However, if we select40K features via Chi-squared feature selection method, docu-ments are represented by 3.15 terms on average.
About 44.9%documents are represented by no more than 2 terms.24 32 40 48 56 640.150.200.250.300.35MeanAveragedPrecision(MAP)Number of bitsS3HS3HfSHKLSH(a)24 32 40 48 56 640.030.060.090.120.150.18PrecisionwithinHammingradius2Number of bitsS3HS3HfSHKLSHIL(b)Figure 6: Retrieval performance of different methods onODP.
(a) Mean Averaged Precision (MAP) for differentnumber of bits for hash ranking.
(b) Precision withinHamming radius 2 for hash lookup.strates S3H can measure the similarity among thedata better than KLSH and SH.We should emphasize that KLSH needs 0.3msto return the results for a query document for hashlookup, and S3H needs <0.1ms.
In contrast, IL re-quires about 75ms to finish searching.
This is mainlybecause IL always returns a large number of ob-jects (dozens or hundreds times more than S3H andKLSH) and requires much time for post-processing.All the experiments show S3H is more effective,efficient and stable than the baseline method and thestate-of-the-art hashing methods.5 ConclusionsWe have proposed a novel supervised hashingmethod named Semi-Supervised Simhash (S3H) forhigh-dimensional data similarity search.
S3H learnsthe optimal feature weights from prior knowledgeto relocate the data such that similar objects havesimilar fingerprints.
This is implemented by max-imizing the empirical accuracy on labeled data to-gether with the entropy of hash functions.
Theproposed method leads to a simple Quasi-Newtonbased solution which is efficient even for very high-dimensional data.
Experiments performed on twolarge datasets have shown that S3H has better searchperformance than several state-of-the-art methods.6 AcknowledgementsWe thank Fangtao Li for his insightful suggestions.We would also like to thank the anonymous review-ers for their helpful comments.
This work is sup-ported by the National Natural Science Foundationof China under Grant No.
60873174.100ReferencesChristopher J.C. Burges.
1998.
A tutorial on supportvector machines for pattern recognition.
Data Miningand Knowledge Discovery, 2(2):121-167.Moses S. Charikar.
2002.
Similarity estimation tech-niques from rounding algorithms.
In Proceedingsof the 34th annual ACM symposium on Theory ofcomputing, pages 380-388.Gianni Costa, Giuseppe Manco and Riccardo Ortale.2010.
An incremental clustering scheme for data de-duplication.
Data Mining and Knowledge Discovery,20(1):152-187.Jeffrey Dean and Monika R. Henzinge.
1999.
FindingRelated Pages in the World Wide Web.
ComputerNetworks, 31:1467-1479.Richard O. Duda, Peter E. Hart and David G. Stork.2000.
Pattern classification, 2nd edition.
Wiley-Interscience.George Forman 2003.
An extensive empirical study offeature selection metrics for text classification.
TheJournal of Machine Learning Research, 3:1289-1305.Piotr Indyk and Rajeev Motwani.
1998.
Approximatenearest neighbors: towards removing the curse ofdimensionality.
In Proceedings of the 30th annualACM symposium on Theory of computing, pages604-613.Ian Jolliffe.
1986.
Principal Component Analysis.Springer-Verlag, New York.Yan Ke, Rahul Sukthankar and Larry Huston.
2004.Efficient near-duplicate detection and sub-imageretrieval.
In Proceedings of the ACM InternationalConference on Multimedia.Brian Kulis and Kristen Grauman.
2009.
Kernelizedlocality-sensitive hashing for scalable image search.In Proceedings of the 12th International Conferenceon Computer Vision, pages 2130-2137.Brian Kulis, Prateek Jain and Kristen Grauman.
2009.Fast similarity search for learned metrics.
IEEETransactions on Pattern Analysis and Machine Intelli-gence, pages 2143-2157.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical programming, 45(1): 503-528.Omid Madani, Michael Connor and Wiley Greiner.2009.
Learning when concepts abound.
The Journalof Machine Learning Research, 10:2571-2613.Gurmeet Singh Manku, Arvind Jain and Anish DasSarma.
2007.
Detecting near-duplicates for webcrawling.
In Proceedings of the 16th internationalconference on World Wide Web, pages 141-150.Christopher D. Manning, Prabhakar Raghavan and Hin-rich Schu?tze.
2002.
An introduction to informationretrieval.
Spring.Yadong Mu, Jialie Shen and Shuicheng Yan.
2010.Weakly-Supervised Hashing in Kernel Space.
In Pro-ceedings of International Conference on ComputerVision and Pattern Recognition, pages 3344-3351.Ruslan Salakhutdinov and Geoffrey Hintona.
2007.Semantic hashing.
In SIGIR workshop on InformationRetrieval and applications of Graphical Models.Bernhard Scho?lkopf, Alexander Smola and Klaus-RobertMu?ller.
1997.
Kernel principal component analysis.Advances in Kernel Methods - Support Vector Learn-ing, pages 583-588.
MIT.Lloyd N. Trefethen and David Bau.
1997.
Numericallinear algebra.
Society for Industrial Mathematics.Xiaojun Wan, Jianwu Yang and Jianguo Xiao.
2008.Towards a unified approach to document similaritysearch using manifold-ranking of blocks.
InformationProcessing & Management, 44(3):1032-1048.Jun Wang, Sanjiv Kumar and Shih-Fu Chang.
2010a.Semi-Supervised Hashing for Scalable Image Re-trieval.
In Proceedings of International Conferenceon Computer Vision and Pattern Recognition, pages3424-3431.Jun Wang, Sanjiv Kumar and Shih-Fu Chang.
2010b.Sequential Projection Learning for Hashing withCompact Codes.
In Proceedings of InternationalConference on Machine Learning.Yair Weiss, Antonio Torralba and Rob Fergus.
2009.Spectral hashing.
In Proceedings of Advances in Neu-ral Information Processing Systems.101
