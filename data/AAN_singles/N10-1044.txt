Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 317?320,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsTime-Efficient Creation of an Accurate Sentence Fusion CorpusKathleen McKeown, Sara Rosenthal, Kapil Thadani and Coleman MooreColumbia UniversityNew York, NY 10027, USA{kathy,sara,kapil}@cs.columbia.edu, cjm2140@columbia.eduAbstractSentence fusion enables summarization andquestion-answering systems to produce out-put by combining fully formed phrases fromdifferent sentences.
Yet there is little datathat can be used to develop and evaluate fu-sion techniques.
In this paper, we present amethodology for collecting fusions of simi-lar sentence pairs using Amazon?s Mechani-cal Turk, selecting the input pairs in a semi-automated fashion.
We evaluate the resultsusing a novel technique for automatically se-lecting a representative sentence from multi-ple responses.
Our approach allows for rapidconstruction of a high accuracy fusion corpus.1 IntroductionSummarization and question-answering systemsmust transform input text to produce useful outputtext, condensing an input document or document setin the case of summarization and selecting text thatmeets the question constraints in the case of questionanswering.
While many systems use sentence ex-traction to facilitate the task, this approach risks in-cluding additional, irrelevant or non-salient informa-tion in the output, and the original sentence wordingmay be inappropriate for the new context in whichit appears.
Instead, recent research has investigatedmethods for generating new sentences using a tech-nique called sentence fusion (Barzilay and McKe-own, 2005; Marsi and Krahmer, 2005; Filippova andStrube, 2008) where output sentences are generatedby fusing together portions of related sentences.While algorithms for automated fusion have beendeveloped, there is no corpus of human-generatedfused sentences available to train and evaluate suchsystems.
The creation of such a dataset could pro-vide insight into the kinds of fusions that peopleproduce.
Furthermore, since research in the relatedtask of sentence compression has benefited fromthe availability of training data (Jing, 2000; Knightand Marcu, 2002; McDonald, 2006; Cohn and La-pata, 2008), we expect that the creation of this cor-pus might encourage the development of supervisedlearning techniques for automated sentence fusion.In this work, we present a methodology for cre-ating such a corpus using Amazon?s MechanicalTurk1, a widely used online marketplace for crowd-sourced task completion.
Our goal is the generationof accurate fusions between pairs of sentences thathave some information in common.
To ensure thatthe task is performed consistently, we abide by thedistinction proposed by Marsi and Krahmer (2005)between intersection fusion and union fusion.
In-tersection fusion results in a sentence that containsonly the information that the sentences had in com-mon and is usually shorter than either of the originalsentences.
Union fusion, on the other hand, resultsin a sentence that contains all information contentfrom the original two sentences.
An example of in-tersection and union fusion is shown in Figure 1.We solicit multiple annotations for both union andintersection tasks separately and leverage the differ-ent responses to automatically choose a representa-tive response.
Analysis of the responses shows thatour approach yields 95% accuracy on the task ofunion fusion.
This is a promising first step and indi-cates that our methodology can be applied towardsefficiently building a highly accurate corpus for sen-tence fusion.1https://www.mturk.com3171.
Palin actually turned against the bridge project only after itbecame a national symbol of wasteful spending.2.
Ms. Palin supported the bridge project while running forgovernor, and abandoned it after it became a national scandal.Intersection: Palin turned against the bridge project after itbecame a national scandal.Union: Ms. Palin supported the bridge project while runningfor governor, but turned against it when it became a nationalscandal and a symbol of wasteful spending.Figure 1: Examples of intersection and union2 Related WorkThe combination of fragments of sentences on acommon topic has been studied in the domain of sin-gle document summarization (Jing, 2000; Daume?
IIIand Marcu, 2002; Xie et al, 2008).
In contrast tothese approaches, sentence fusion was introduced tocombine fragments of sentences with common infor-mation for multi-document summarization (Barzilayand McKeown, 2005).
Automated fusion of sen-tence pairs has since received attention as an inde-pendent task (Marsi and Krahmer, 2005; Filippovaand Strube, 2008).
Although generic fusion of sen-tence pairs based on importance does not yield highagreement when performed by humans (Daume?
IIIand Marcu, 2004), fusion in the context of a queryhas been shown to produce better agreement (Krah-mer et al, 2008).
We examine similar fusion an-notation tasks in this paper, but we asked workersto provide two specific types of fusion, intersectionand union, thus avoiding the less specific definitionbased on importance.
Furthermore, as our goal isthe generation of corpora, our target for evaluationis accuracy rather than agreement.This work studies an approach to the automaticconstruction of large fusion corpora using workersthrough Amazon?s Mechanical Turk service.
Previ-ous studies using this online task marketplace haveshown that the collective judgments of many work-ers are comparable to those of trained annotatorson labeling tasks (Snow et al, 2008) although thesejudgments can be obtained at a fraction of the costand effort.
However, our task presents an additionalchallenge: building a corpus for sentence fusion re-quires workers to enter free text rather than simplychoose between predefined options; the results areprone to variation and this makes comparing and ag-gregating multiple responses problematic.A.
After a decade on the job, Gordon had become an experi-enced cop.B.
Gordon has a lot of experience in the police force.Figure 2: An example of sentences that were judged to betoo similar for inclusion in the dataset3 Collection MethodologyData collection involved the identification of thetypes of sentence pairs that would make suitablecandidates for fusion, the development of a sys-tem to automatically identify good pairs and manualfiltering of the sentence pairs to remove erroneouschoices.
The selected sentence pairs were then pre-sented to workers on Mechanical Turk in an inter-face that required them to manually type in a fusedsentence (intersection or union) for each case.Not all pairs of related sentences are useful for thefusion task.
When sentences are too similar, the re-sult of fusion is simply one of the input sentences.For example (Fig.
2), if sentence A contains all theinformation in sentence B but not vice versa, thenB is also their intersection while A is their unionand no sentence generation is required.
On the otherhand, if the two sentences are too dissimilar, thenno intersection is possible and the union is just theconjunction of the sentences.We experimented with different similarity metricsaimed at identifying pairs of sentences that were in-appropriate for fusion.
The sentences in this studywere drawn from clusters of news articles on thesame event from the Newsblaster summarizationsystem (McKeown et al, 2002).
While these clus-ters are likely to contain similar sentences, they willcontain many more dissimilar than similar pairs andthus a metric that emphasizes precision over recallis important.
We computed pairwise similarity be-tween sentences within each cluster using three stan-dard metrics: word overlap, n-gram overlap and co-sine similarity.
Bigram overlap yielded the best pre-cision in our experiments.
We empirically arrived ata lower threshold of .35 to remove dissimilar sen-tences and an upper threshold of .65 to avoid near-identical sentences, yielding a false-positive rate of44.4%.
The remaining inappropriate pairs were thenmanually filtered.
This semi-automated procedureenabled fast selection of suitable sentence pairs: oneperson was able to select 30 pairs an hour yieldingthe 300 pairs for the full experiment in ten hours.318Responses Intersection UnionAll (1500) 0.49 0.88Representatives (300) 0.54 0.95Table 1: Union and intersection accuracy3.1 Using Amazon?s Mechanical TurkBased on a pilot study with 20 sentence pairs, wedesigned an interface for the full study.
For inter-section tasks, the interface posed the question ?Howwould you combine the following two sentences intoa single sentence conveying only the informationthey have in common??.
For union tasks, the ques-tion was ?How would you combine the following twosentences into a single sentence that contains ALL ofthe information in each?
?.We used all 300 pairs of similar sentences forboth union and intersection and chose to collect fiveworker responses per pair, given the diversity ofresponses that we found in the pilot study.
Thisyielded a total of 3000 fused sentences with 1500intersections and 1500 unions.3.2 Representative ResponsesUsing multiple workers provides little benefit unlesswe are able to harness the collective judgments oftheir responses.
To this end, we experiment witha simple technique to select one representative re-sponse from all responses for a case, hypothesizingthat such a response would have a lower error rate.We test the hypothesis by comparing the accuracy ofrepresentative responses with the average accuracyover all responses.Our strategy for selecting representatives drawson the common assumption used in human com-putation that human agreement in independently-generated labels implies accuracy (von Ahn andDabbish, 2004).
We approximate agreement be-tween responses using a simple and transparentmeasure for overlap: cosine similarity over stemsweighted by tf-idf where idf values are learned overthe Gigawords corpus2.
After comparing all re-sponses in a pairwise fashion, we need to choose arepresentative response.
As using the centroid di-rectly might not be robust to the presence of er-roneous responses, we first select the pair of re-sponses with the greatest overlap as candidates and2LDC Catalog No.
LDC2003T05Errors Intersection UnionMissing clause 2 7Union/Intersection 46 6S1/S2 21 8Additional clause 10 1Lexical 3 1Table 2: Errors seen in 30 random cases (150 responses)then choose the candidate which has the greatest to-tal overlap with all other responses.4 Results and Error AnalysisFor evaluating accuracy, fused sentences were man-ually compared to the original sentence pairs.
Due tothe time-consuming nature of the evaluation, 50% ofthe 300 cases were randomly selected for analysis.10% were initially analyzed by two of the authors; ifa disagreement occurred, the authors discussed theirdifferences and came to a unified decision.
The re-maining 40% were then analyzed by one author.
Inaddition to this high-level analysis, we further ana-lyzed 10% of the cases to identify the the types oferrors made in fusion as well as the techniques usedand the effect of task difficulty on performance.The accuracy for intersection and union tasks isshown in Table 1.
For both tasks, accuracy of the se-lected representatives significantly exceeded the av-erage response accuracy.
In our error analysis, wefound that workers often answered the intersectiontask by providing a union, possibly due to a misin-terpretation of the question.
This caused intersectionaccuracy to be significantly worse than union.
Weanalyzed the impact of this error by computing ac-curacy on the first 30 cases (10%) without this errorand the accuracy for intersection increased 22%.Error types were categorized as ?missing clause?,?using union for intersection and vice versa?,?choosing an input sentence (S1/S2)?, ?additionalclause?
and ?lexical error?.
Table 2 shows the num-ber of occurrences of each in 10% of the cases.We binned the sentence pairs according tothe difficulty of the fusion task for each pair(easy/medium/hard) and found that performancewas not dependent on difficulty level; accuracy wasrelatively similar across bins.
We also observed thatworkers typically performed fusion by selecting onesentence as a base and removing clauses or mergingin additional clauses from the other sentence.319Figure 3: Number of cases in which x/5 workers pro-vided accurate responses for fusionIn order to determine the benefit of using manyworkers, we studied the number of workers who an-swered correctly for each case.
Figure 3 reveals that2/5 or more workers (summing across columns) re-sponded accurately in 99% of union cases and 82%of intersection cases.
The intersection results areskewed due to the question misinterpretation issuewhich, though it was the most common error, wasmade by 3/5 workers only 17% of the time.
Thus, inthe majority of the cases, accurate fusions can stillbe found using the representative method.5 ConclusionWe presented a methodology to build a fusion cor-pus which uses semi-automated techniques to selectsimilar sentence pairs for annotation on MechanicalTurk3.
Additionally, we showed how multiple re-sponses for each fusion task can be leveraged by au-tomatically selecting a representative response.
Ourapproach yielded 95% accuracy for union tasks, andwhile intersection fusion accuracy was much lower,our analysis showed that workers sometimes pro-vided unions instead of intersections and we sus-pect that an improved formulation of the questioncould lead to better results.
Construction of the fu-sion dataset was relatively fast; it required only tenhours of labor on the part of a trained undergraduateand seven days of active time on Mechanical Turk.AcknowledgementsThis material is based on research supported in partby the U.S. National Science Foundation (NSF) un-der IIS-05-34871 Any opinions, findings and con-clusions or recommendations expressed in this ma-terial are those of the authors and do not necessarilyreflect the views of the NSF.3The corpus described in this work is available athttp://www.cs.columbia.edu/?kathy/fusioncorpusReferencesRegina Barzilay and Kathleen R. McKeown.
2005.
Sen-tence fusion for multidocument news summarization.Computational Linguistics, 31(3):297?328.Trevor Cohn and Mirella Lapata.
2008.
Sentence com-pression beyond word deletion.
In Proceedings ofCOLING, pages 137?144.Hal Daume?
III and Daniel Marcu.
2002.
A noisy-channelmodel for document compression.
In Proceedings ofACL, pages 449?456.Hal Daume?
III and Daniel Marcu.
2004.
Generic sen-tence fusion is an ill-defined summarization task.
InProceedings of the ACL Text Summarization BranchesOut Workshop, pages 96?103.Katja Filippova and Michael Strube.
2008.
Sentence fu-sion via dependency graph compression.
In Proceed-ings of EMNLP, pages 177?185.Hongyan Jing.
2000.
Sentence reduction for automatictext summarization.
In Proceedings of Applied Natu-ral Language Processing, pages 310?315.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: a probabilistic ap-proach to sentence compression.
Artificial Intelli-gence, 139(1):91?107.Emiel Krahmer, Erwin Marsi, and Paul van Pelt.
2008.Query-based sentence fusion is better defined andleads to more preferred results than generic sentencefusion.
In Proceedings of ACL, pages 193?196.Erwin Marsi and Emiel Krahmer.
2005.
Explorations insentence fusion.
In Proceedings of the European Work-shopon Natural Language Generation, pages 109?117.Ryan McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In Proceedingsof EACL, pages 297?304.Kathleen R. McKeown, Regina Barzilay, David Evans,Vasileios Hatzivassiloglou, Judith L. Klavans, AniNenkova, Carl Sable, Barry Schiffman, and SergeySigelman.
2002.
Tracking and summarizing news ona daily basis with Columbia?s Newsblaster.
In Pro-ceedings of HLT, pages 280?285.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast?but is itgood?
: Evaluating non-expert annotations for natu-ral language tasks.
In Proceedings of EMNLP, pages254?263.Luis von Ahn and Laura Dabbish.
2004.
Labeling im-ages with a computer game.
In Proceedings of theSIGCHI conference on Human Factors in ComputingSystems, pages 319?326.Zhuli Xie, Barbara Di Eugenio, and Peter C. Nel-son.
2008.
From extracting to abstracting: Gener-ating quasi-abstractive summaries.
In Proceedings ofLREC, May.320
