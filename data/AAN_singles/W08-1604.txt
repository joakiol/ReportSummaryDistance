Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 25?32Manchester, August 2008Context Modeling for IQA: The Role of Tasks and EntitiesRaffaella Bernardi and Manuel KirschnerKRDB, Faculty of Computer ScienceFree University of Bozen-Bolzano, Italy{bernardi, kirschner}@inf.unibz.itAbstractIn a realistic Interactive Question Answer-ing (IQA) setting, users frequently askfollow-up questions.
By modeling how thequestions?
focus evolves in IQA dialogues,we want to describe what makes a partic-ular follow-up question salient.
We intro-duce a new focus model, and describe animplementation of an IQA system that weuse for exploring our theory.
To learn prop-erties of salient focus transitions from data,we use logistic regression models that wevalidate on the basis of predicted answercorrectness.1 Questions within a ContextQuestion Answering (QA) systems have reached ahigh level of performance within the scenario orig-inally described in the TREC competitions, andare ready to tackle new challenges as shown bythe new tracks proposed in recent instantiations(Voorhees, 2004).
To answer these challenges, at-tention is moving towards adding semantic infor-mation at different levels.
Our work is about con-text modeling for Interactive Question Answering(IQA) systems.
Our research hypothesis is that a)knowledge about the dialogue history, and b) lexi-cal knowledge about semantic arguments improvean IQA system?s ability to answer follow-up ques-tions.
In this paper we use logistic regression mod-eling to verify our claims and evaluate how the per-formance of our Q?A mapping algorithm variesbased on whether such knowledge is taken into ac-count.c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.Actual IQA dialogues often exhibit ?context-dependent?
follow-up questions (FU Qs) contain-ing anaphoric devices, like Q2 below.
Such ques-tions are potentially difficult to process by meansof standard QA techniques, and it is for these casesthat we claim that predicting the FU question?s fo-cus (here, the entity ?library card?)
will help a sys-tem find the correct answer (cf.
Sec.
6 for empiricalbackup).Q1: Can high-school students use the library?A1: Yes, if they got a library card.Q2: So, how do I get it?Following (Stede and Schlangen, 2004), we re-fer to the type of IQA dialogues we are studyingas ?information-seeking chat?, and conjecture thatthis kind of dialogue can be handled by means of asimple model of discourse structure.
Our assump-tion is that in general the user engages in a coherentdialogue with the system.
As proposed in (Ahren-berg et al, 1995), we model the dialogues in termsof pairs of initiatives (questions) and responses(answers), ignoring other intentional acts.The approach we adopt aims at answering thefollowing questions: (a) In what way does infor-mation about the previous user questions and pre-vious system answers help in predicting the nextFU Q?
(b) Does the performance of an IQA sys-tem improve if it has structure/history-based infor-mation?
(c) Which is the role that each part of thisinformation plays for determining the correct an-swer to a FU Q?This paper is structured as follows.
Section 2gives an overview of some theories of focus used indialogue and IQA.
Section 3 then gives a detailedaccount of our theory, explaining what a questioncan focus on, and what patterns of focus changewe expect a FU Q will trigger.
Hence, this first25part answers our question (a) above.
We then moveto more applied issues in Sec.
4, where we showhow questions and answers were annotated withfocus information.
The next Section 5 explains theQ?A algorithm we use to test our theory so as toanswer (b), while Section 6 covers the logistic re-gression models with which we learn optimal val-ues for the algorithm from data, addressing ques-tion (c).2 Coherence in IQA dialoguesIn the area of Discourse processing, much workhas been devoted to formulating rules that accountfor the coherence of dialogues.
This coherencecan often be defined in terms of focus and focusshifts.
In the following, we adopt the definitionfrom (Lec?uche et al, 1999): focus stands for the?set of all the things to which participants in a di-alogue are attending to at a certain point in a dia-logue?.1In general, all theories of dialogue focusconsidered by Lec?uche et al claim that the focuschanges according to some specific and well de-fined patterns, following the rules proposed by therespective theory.
The main difference betweenthese theories lies in how these rules are formu-lated.A major distinguishing feature of different fo-cus theories has been the question whether they ad-dress global or local focus.
While the latter explaincoherence between consecutive sentences, the for-mer are concerned with how larger parts of the di-alogue can be coherent.
We claim that in ?infor-mation seeking dialogue?
this distinction is moot,and the two kinds of foci collapse into one.
Fur-thermore, our empirical investigation shows that itsuffices to consider a rather short history of the di-alogue, i.e.
the previous user question and previoussystem answer, when looking for relations betweenprevious dialogue and a FU Q.Salient transitions between two consecutivequestions are defined in (Chai and Jin, 2004) un-der the name of ?informational transitions?.
Theauthors aim to describe how the topic within a di-1This definition is in line with how focus has been used inComputational Linguistics and Artificial Intelligence (hence,?AI focus?
), originating in the work of Grosz and Sidner ondiscourse entity salience.
We follow Lec?uche et al in thatfocused elements could also be actions/tasks.
We see the mostsalient focused element (corresponding to the ?Backward-looking center?
in Centering Theory) as the topic of the ut-terance.
Accordingly, in the following we will use the termsfocus and topic interchangeably; cf.
(Vallduvi, 1990) for a sur-vey of these rather overloaded terms.alogue evolves.
They take ?entities?
and ?activi-ties?
as the main possible focus of a dialogue.
AFU Q can be used to ask (i) a similar question asthe previous one but with different constraints ordifferent participants (topic extension); (ii) a ques-tion concerning a different aspect of the same topic(topic exploration); (iii) a question concerning arelated activity or a related entity (topic shift).
Wetake this analysis as our starting point, extend itand propose an algorithm to automatically detectthe kind of focus transition a user performs whenasking a FU Q, and evaluate our extended theorywith real dialogue data.
Following (Bertomeu etal., 2006) we consider also the role of the systemanswer, and we analyze the thematic relations be-tween the current question and previous question,and the current question and previous answer.
Un-like (Bertomeu et al, 2006), we attempt to learn amodel of naturally occurring thematic relations inrelatively unconstrained IQA dialogues.3 Preliminary Observations3.1 What ?things?
do users focus on?For all forthcoming examples of dialogues, ques-tions and answers, we will base our discussionon an actual prototype IQA system we have beendeveloping; this system is supposed to providelibrary-related information in a university librarysetting.In the dialogues collected via an earlier Wizard-of-Oz (WoZ) experiment (Kirschner and Bernardi,2007), we observed that users either seem tohave some specific library-related task (action, e.g.?search?)
in mind that they want to ask the systemabout, or they want to retrieve information on somespecific entity (e.g., ?guided tour?).
People tendto use FU Qs to ?zoom into?
(i.e., find out moreabout) either of the two.
In line with this analysis,the focus of a FU Q might move from the task (ac-tion/verb) to the entities that are possible fillers ofthe verb?s semantic argument slots.Based on these simple observations, we pro-pose a task/entity-based model for describing thefocus of questions and answers in our IQA set-ting.
Our theory of focus structure is related to thetask-based theory of (Grosz, 1977).
Tasks corre-spond to verbs, which are inherently connected toan argument structure defining the verb?s semanticroles.
By consulting lexical resources like Prop-Bank (Palmer et al, 2005), we can use existingknowledge about possible semantic arguments of26the tasks we have identified.We claim that actions/verbs form a suitableand robust basis for describing the (informational)meaning of utterances in IQA.
Taking the mainverb along with its semantic arguments to repre-sent the core meaning of user questions seems tobe a more feasible alternative to deep semantic ap-proaches that still lack the robustness for dealingwith unconstrained user input.Further, we claim that analyzing user questionson the basis of their task/entity structure provides auseful level of abstraction and granularity for em-pirically studying informational transitions in IQAdialogues.
We back up this claim in Section 6.Along the lines of (Kirschner and Bernardi, 2007),we aim for a precise definition of focus structurefor IQA questions.
Our approach is similar in spiritto (Chai and Jin, 2004), whereas we need to re-duce the complexity of their discourse representa-tion (i.e., their number of possible question ?top-ics?)
so that we arrive at a representation of focusstructure that lends itself to implementation in apractical IQA system.3.2 How focus evolves in IQAWe try to formulate our original question, ?Givena user question and a system response, what doesa salient FU Q focus on??
more precisely.
Wewant to know whether the FU Q initiates one ofthe following three transitions:2Topic zoom asking about a different aspect ofwhat was previously focused1.
asking about the same task and same ar-gument, but different question type (e.g.,search for books: Q: where, FU Q: how)2. asking about the same entity (e.g.,guided tour: Q: when, FU Q: where)3. asking about the same task but differentargument (e.g., Q: search for books, FUQ: search for journals)4. asking about an entity introduced in theprevious system answerCoherent shift to a ?related?
(semantically, or:verb?its semantic argument) focus1.
from task to semantically related task2.
from task to related entity: entity is a se-mantic argument of the task2Comparing our points to (Chai and Jin, 2004), Topiczoom: 1. and 2. are cases of topic exploration, 3. of topicextension, and 4. is new.
Coherent shift: 1. and 2. are cases oftopic shift, and 3. and 4. are new.3.
from entity to semantically related entity4.
from entity to related task: entity is a se-mantic argument of the taskShift to an unrelated focusFrom the analysis of our WoZ data we get cer-tain intuitions about salient focus flow betweensome preceding dialogue and a FU Q.
First of all,we learn that a dialogue context of just one previ-ous user question and one previous system answergenerally provides enough information to resolvecontext-dependent FU Qs.
In the remainder of thissection, we describe the other intuitions by propos-ing alternative ways of detecting the focus of a FUQ that follows a salient relation (?Topic zoom?
or?Coherent shift?).
Later in this paper we show howwe implement these intuitions as features, and howwe use a regression model to learn the importanceof these features from data.Exploiting task/entity structure Knowingwhich entities are possible semantic argumentsof a library-related task can help in detecting thefocused task.
Even if the task is not expressedexplicitly in the question, the fact that a number ofparticipant entities are found in the question couldhelp identify the task at hand.Exploiting (immediate) dialogue context: pre-vious user question It might prove useful toknow the things that the immediately precedinguser question focused on.
If users tend to con-tinue focusing on the same task, entity or questiontype, this focus information can help in ?complet-ing?
context-dependent FU Qs where the focusedthings cannot be detected easily since they are notmentioned explicitly.
This way of using dialoguecontext has been used in previous IQA systems,e.g., the Ritel system (van Schooten et al, forth-coming).Exploiting (immediate) dialogue context: pre-vious system answer Whereas the role of thesystem answer has been ignored in some pre-vious accounts of FU Qs (e.g., (Chai and Jin,2004) and even in the highly influential TREC task(Voorhees, 2004)), our data suggest that the systemanswer does play a role for predicting what a FUQ will focus on: it seems that the system answercan introduce entities that a salient FU Q will askmore information about.
(van Schooten and op denAkker, 2005) and (Bertomeu et al, 2006) describeIQA systems that also consider the previous sys-tem answer.27Exploiting task/entity structure combined withdialogue context It might be useful to com-bine knowledge about the task/entity structure withknowledge about the previously focused task orentity.
E.g., a previously focused task might makea ?coherent shift?
to a participant entity likely;likewise, a previously focused entity might enablea coherent shift to a task in which that entity couldplay a semantic role.The questions to be addressed in the remain-der of the paper now are the following.
Does theperformance of an IQA system improve if it hasstructure/history-based information as mentionedabove?
Which is the role that each part of this in-formation plays for determining the correct answerto a FU Q?4 Tagging focus on three levelsFollowing the discussion in Section 3.1, and hav-ing studied the user dialogues from our WoZ data,we propose to represent the (informational) mean-ing of a user question by identifying the taskand/or entity that the question is about (focuseson).
Besides task and entity, we have QuestionType (QType) as a third level on which to describea question?s focus.
The question type relates towhat type of information the user asks about thefocused task/entity, and equivalently describes theexact type of answer (e.g., why, when, how) thatthe user hopes to get about the focused task/entity.Thus, we can identify the focus of a question withthe triple <Task, Entity, QType>.We have been manually building a smalldomain-dependent lexical resource that in the fol-lowing we will call ?task/entity structure?.
Wesee it as a miniature version of the PropBank, re-stricted to the small number of verbs/tasks that wehave identified to be relevant in our domain, butextended with some additional semantic argumentslots if required.
Most importantly, the argumentslots have been assigned to possible filler entities,each of which can be described with a number ofsynonymous names.Tasks By analyzing a previously acquired exten-sive list of answers to frequently-asked library-related questions, we identified a list of 11 tasksthat library users might ask about (e.g.
search, re-serve, pick up, browse, read, borrow, etc.).
Ourunderlying assumption is that the focus (as identi-fied by the focus triple) of a question is identical tothat of the corresponding answer.
Thus, we assumethe focus triple describing a user question also de-scribes its correct answer.
For example, in Table 1,A1 would share the same focus triple as Q1.We think of the tasks as abstract descriptions ofactions that users can perform in the library con-text.
A user question focuses on a specific task if iteither explicitly contains that verb (or a synonym),or implicitly refers to the same ?action frame?
thatthe verb instantiates.Entities Starting from the information about se-mantic arguments of these verbs available inPropBank, and extending it when necessary fordomain-specific use of the verbs, for each task wedetermined its argument slots.
Again by inspect-ing our list of FAQ answers, we started assign-ing library-related entities to these argument slots,when we found that the answer focuses on boththe task and the semantic argument entity.
Wefound that many answers focus on some library-related entity without referring to any task.
Thus,we explicitly provide for the possibility of a ques-tion/answer being about just an entity, e.g.
: ?Whatare the opening times??.
A user question focuseson a specific entity if it refers to it explicitly orvia some reference phenomenon (anaphora, ellip-sis, etc.)
linked to the dialogue history.Question Types We compiled a list of question(or answer) types by inspecting our FAQ answerslist, and thinking about the types of questions thatcould have given rise to these answers.
We aimedfor a compromise between potentially more fine-grained distinctions of question semantics, andbetter distinguishability of the resulting set of la-bels (for a human annotator or a computer pro-gram).We defined each question type by providing atypical question template, e.g.
: ?where: wherecan I find $Entity?
?, ?whatis: what is $Entity?
?,?yesno: can I $Task $Entity?
?, ?howto: how do I$Task $Entity??.
Note how some question typescapture questions that focus on some task alongwith some participant entity, while others focus onjust an entity.
We also devised some question typesfor questions focusing on just a task, where we as-sume an implicit semantic argument which is notexpressed, e.g., ?how can I borrow??
(where in thespecific context of our application we can imply asemantic argument like ?item?).
A question has aspecific question type if it can be paraphrased withthe corresponding question template.
An answer28has a specific type if it is the correct answer to thatquestion template.4.1 A repository of annotated answersFrom our original collection of answers to libraryFAQs, we have annotated around 200 with focustriples.
The triples we selected include all poten-tial answers to the FU Qs from the free FU Q elic-itation experiment described in the next section.Some of the actual answers were annotated withmore than one focus triple, e.g., often the answercorresponded to more than one question type.
Thetotal of 207 focus triples include all 11 tasks and23 different question types (where the 4 most fre-quent types were the ones mentioned as examplesabove, accounting for just over 50% of all focustriples).For instance, the answer: ?You can restrict yourquery in the OPAC on individual Library locations.The search will then be restricted e.g.
to the Li-brary of Bressanone-Brixen or the library of the?Museion?.?
is marked by: <Task: search, Entity:specific library location, QType: yesno>.The algorithm we introduce in Section 5 usesthis answer repository as the setA of potential can-didates from which it chooses the answer to a newuser question.
Again, we assume that if we can de-termine the correct focus triple of a user question,the answer from our collection that has been an-notated with that same triple will correctly answerthe question.4.2 Annotated user questionsHaving created an answer repository annotatedwith focus triples, we need user questions anno-tated on the same three levels, which we can thenuse for training and evaluating the Q?A algorithmthat we introduce in Section 5.
We acquired thesedata in two steps: 1. eliciting free FU Qs from sub-jects in a web-based experiment, 2. annotating thequestions with focus triples.Dialogue Collection Experiment We set up aweb-based experiment to collect genuine FU Qs.We adopted the experimental setup proposed in(van Schooten and op den Akker, 2005)), in thatwe presented to our subjects short dialogues con-sisting of a first library-related question, and a cor-responding correct answer, as exemplified by ?Q1?and ?A1?
in Table 1.We asked the subjects to provide a FU Q ?Q2?such that it will help further serve their informationneed in the situation defined by the given previousquestion-answer exchange.
In this way, we col-lected 88 FU Qs from 8 subjects and 11 contexts(first questions and answers).3Annotating the questions We annotated these88 FU Qs, along with the 11 first questions thatwere presented to the subjects, with focus triples.By (informally) analyzing the differences betweendifferent annotators?
results, we continuously triedto disambiguate and improve the annotation in-structions.
As a result, we present a pre-compiledlist of entities from which the annotator selects theone they consider to be in focus, and that of allpossible candidates is the one least ?implied?
bythe context.
Table 1 shows one example annota-tion of one of the 11 first user questions and two ofthe 8 corresponding FU Qs.5 A feature-based Q?A algorithmWe now present an algorithm for mapping a userquestion to a canned-text answer from our answerrepository.
The decision about which answer to se-lect is based on a score that the algorithm assigns toeach answer, which in turn depends on the valuesof the features we have introduced in the previoussection.
Thus, the purpose of the algorithm is toselect the best answer focus triple from the repos-itory, based on feature values.
In this way, we canuse the algorithm as a test bed for identifying fea-tures that are good indicators for a correct answer.Our goal is to evaluate the algorithm based on itsaccuracy in finding correct focus triples (which arethe ?keys?
to the actual system answers) for userquestions (see Section 5.2).For each new user question q that is entered, thealgorithm iterates through all focus triples a in theannotated answer repository A (cf.
Section 4.1).For each combination of q and a, all 10 featuresx1,q,a.
.
.
x10,q,aare evaluated.
Each feature thatevaluates to true (?
= 1) or some positive value,contributes with this score ?
towards the overallscore of a.
The algorithm then returns the highest-scoring answer a?.a?
= argmaxa?A(?1x1,q,a+ ?
?
?+ ?10x10,q,a)3In the future, we plan to collect real FU Qs from users ofour online IQA system, which will solve the potential prob-lem of these questions being somewhat artificial due to theexperimental setting.
However, we still expect our currentdata to be highly relevant for studying what users would askabout next.29ID Q/A Task Entity QTypeQ1 Can I get search results for a specific search specific library location yesnolibrary location?A1 You can restrict your query in the OPACon individual Library locations.
(...)Q2a How can I do that?
search specific library location howtoQ2b How long is my book reserved there if I reserve my book howlongwant to get it?Table 1: Example annotation of one first question and two corresponding FU Qs5.1 FeaturesBased on the intuitions presented in Section 3.2,we now describe the 10 features x1,q,a, .
.
.
, x10,q,athat our algorithm uses as predictors for answercorrectness.
All Task and Entity matching is doneusing string matching over word stems.
QTypematching uses regular expression matching witha set of simple regex patterns we devised for ourQTypes.3 surface-based features x1,q,a, .
.
.
, x3,q,a:whether {Taska,Entitya,QTypea} arematched in q.
Entity feature returns thelength in tokens of the matched entity.1 task/entity structure-based feature x4,q,a:how many of the participant entities of Taska(as encoded in our task/entity structure) arematched in q.4 focus continuity features x5,q,a, .
.
.
, x8,q,a:whether {Taska,Entitya,QTypea} are con-tinued in q, wrt.
previous dialogue as fol-lows:4?
Task, Entity, QType continuity wrt.
pre-vious user question.?
Entity continuity wrt.
previous systemanswer.2 task/entity structure + focus continuity fea-tures x9,q,a, x10,q,a:?
Focused Task of previous user questionhas Entityaas a participant.?
Taskahas focused Entity of previousquestion as a participant.5.2 First EvaluationTable 2 shows manually set feature scores?1, .
.
.
, ?10we used for a first evaluation of the al-4Both entity continuity features evaluate to ?2?
when ex-actly the same entity is used again, but to ?1?
when a synonymof the first entity is used.k xk,q,arange(xk,q,a) ?k1 qTypeMatch 0,1 42 taskMatch 0,1 33 lenEntityMatch n 24 nEntitiesInTask n 15 taskContinuity 0,1 16 entityContinuity 0,1,2 17 qTypeContinuity 0,1 18 entityInPrevAnsw 0,1,2 29 entityInPrevTask 0,1 110 prevEntityInTask 0,1 1Table 2: Manually set feature scoresgorithm; we chose these particular scores after in-specting our WoZ data.
With these scores, we ranthe Q?A algorithm on the annotated questions ofannotator 1, who had provided a ?gold standard?annotation for 78 of the 99 user questions (the re-mainder of the questions are omitted because theannotator did not know how to assign a focus tripleto them).
For 24 out of 78 questions, the algorithmfound the exact focus triple (from a total of 207focus triples in the answer repository), yielding anaccuracy of 30.8%.6 Logistic Regression ModelTo improve the accuracy of the Q?A algorithmand to learn about the importance of the singlefeatures for predicting whether an answer fromA is correct, we want to learn optimal scores?1, .
.
.
, ?10from data.
We use a logistic regressionmodel (cf.
(Agresti, 2002)).
Logistic regressionmodels describe the relationship between somepredictors (i.e., our features) and an outcome (an-swer correctness).We use the logit ?
coefficients ?1, .
.
.
, ?kthatthe logistic regression model estimates (from train-ing data, using maximum likelihood estimation)30Coeff.
95% C.I.lenEntityMatch 6.76 5.26?8.26qTypeMatch 2.54 2.02?3.06taskContinuity 2.17 1.39?2.94entityInPrevAnsw 1.78 1.06?2.49taskMatch 1.37 0.80?1.94prevEntityInTask -1.24 -2.06?
-0.43Table 3: Model M2: Magnitudes of significant ef-fectsfor the predictors as empirically motivated scores.In contrast to other supervised machine learn-ing techniques, regression models yield human-readable coefficients that show the individual ef-fect of each predictor on the outcome variable.6.1 Generating Training dataWe generate the training data for learning the lo-gistic regression model from our annotated answerrepository A (Sec.
4.1) and annotated questions(Sec.
4.2) as follows.
For each human-annotatedquestion q and each candidate answer focus triplefrom our repository (a ?
A), we evaluate our fea-tures x1,q,a, .
.
.
, x10,q,a.
If the focus triples of qand a are identical, we take the particular featurevalues as a training instance for a correct answer; ifthe focus triples differ, we have a training instancefor a wrong answer.56.2 Results and interpretationWe fit model M1based on the annotation of anno-tator 2 using all 10 features.6We then fit a secondmodel M2, this time including only the 6 featuresthat correspond to coefficients from modelM1thatare significantly different from zero.
Table 3 showsthe resulting logit ?
coefficients with their 95%confidence intervals.
Using these coefficients asnew scores in our Q?A algorithm (and setting allnon-significant coefficients?
feature scores to 0), itfinds the correct focus triple for 47 out of 78 testquestions (as before, annotated by annotator 1);answer accuracy now reaches 60.3%.We interpret the results in Table 3 as follows.All three surface-based features are significant pre-dictors of a correct answer.
The length of the5Although in this way we get imbalanced data sets with|A| ?
1 negative training instances for each positive one, wehave not yet explored this issue further.6We use annotator 2?s data for training, and annotator 1?sfor testing throughout this paper.matched entity contributes more than the othertwo; we attribute this to the fact that there aremore cases where our simple implementations ofqTypeMatch and taskMatch fail to detect the cor-rect QType or task.
While the task/entity structure-based nEntitiesInTask clearly misses to reach sig-nificance, the history-based features taskContinu-ity and entityInPrevAnsw are useful indicators fora correct answer.
The first is evidence for ?Topiczoom?, with the FU Q asking about a different as-pect of the previously focused task, while the sec-ond shows the influence of the previous answer inshaping the entity focus of the FU Q.
From the two?task/entity structure + focus continuity?
features,we find that if a FU Q focuses on a task that inour task/entity structure has an argument slot filledwith the previously focused entity, it actually indi-cates a false answer; the implications of this find-ing will have to be explored in future work.Finally, to pinpoint the important contributionsof structure- and/or focus continuity features, wefit a new model M3, this time including only the 3(significant) surface-based features.
Evaluating theresulting coefficients in the same way as above, weget only 24 out of 78 correct answer focus triples,an accuracy of 30.8%.
This result supports our ini-tial claim that an IQA system improves if it has away of predicting the focus of a FU Q.7 ConclusionOur original hypothesis was that a) knowledgeabout the dialogue history, and b) lexical knowl-edge about semantic arguments could improve anIQA system?s ability to answer FU Qs.
We opera-tionalized these notions by formulating a set of 10features that evaluate whether a candidate answeris the correct one given a new (FU) user question.We then used regression modeling to investigatethe usefulness of each individual feature by learn-ing from annotated IQA dialogue data, showingthat certain knowledge about the dialogue history(the previously focused task, and the entities men-tioned in the previous system answer) and aboutsemantic arguments are useful for distinguishingcorrect from wrong answers to a FU Q. Finally,we evaluated these results by showing how ourQ?A mapping algorithm?s answer accuracy im-proved by using the empirically learned scores forall statistically significant predictors/features.
Thefeatures and the Q?A algorithm as a whole arebased on a simple way to describe IQA questions31in terms of focus triples.
By showing how wehave improved an actual system with learned fea-ture scores, we demonstrated this representation?sviability for implementation and for empiricallystudying informational transitions in IQA.Although the IQA system used in our project isin several ways limited, our findings about howfocus evolves in real IQA dialogues should scaleup to any new or existing IQA system that allowsusers to ask context-dependent FU Qs in a type of?information seeking?
paradigm.
It would be in-teresting to see how this type of knowledge couldbe added to other IQA or dialogue systems in gen-eral.We see several directions for future work.
Re-garding coherent focus transitions, we have to lookinto which transitions to different tasks/entities aremore coherent than others, possibly based on se-mantic similarity.
A major desideratum for show-ing the scaleability of our work is to explore theinfluence of the subjects on our data annotation.We are currently working on getting an objectiveinter-annotator agreement measure, using externalannotators.
Finally, we plan to collect a large cor-pus of IQA dialogues via a publicly accessible IQAsystem, and have these dialogues annotated.
Withmore data, coming from genuinely interested usersinstead of experimental subjects, and having thesedata annotated by external annotators, we expectto have more power to find significant and gener-ally valid patterns of how focus evolves in IQA di-alogues.AcknowledgmentsWe thank Marco Baroni, Oliver Lemon, MassimoPoesio and Bonnie Webber for helpful discussions.ReferencesAgresti, Alan.
2002.
Categorical Data Analysis.Wiley-Interscience, New York.Ahrenberg, L., N. Dahlb?ack, and A. J?onsson.
1995.Coding schemes for studies of natural language dia-logue.
In Working Notes from AAAI Spring Sympo-sium, Stanford.Bertomeu, N?uria, Hans Uszkoreit, Anette Frank, Hans-Ulrich Krieger, and Brigitte J?org.
2006.
Contex-tual phenomena and thematic relations in databaseQA dialogues: results from a wizard-of-oz experi-ment.
In Proc.
of the Interactive Question Answer-ing Workshop at HLT-NAACL 2006, pages 1?8, NewYork, NY.Chai, Joyce Y. and Rong Jin.
2004.
Discourse structurefor context question answering.
In Proc.
of the HLT-NAACL 2004 Workshop on Pragmatics in QuestionAnswering, Boston, MA.Grosz, Barbara Jean.
1977.
The representation anduse of focus in dialogue understanding.
Ph.D. thesis,University of California, Berkeley.Kirschner, Manuel and Raffaella Bernardi.
2007.
Anempirical view on iqa follow-up questions.
In Proc.of the 8th SIGdial Workshop on Discourse and Dia-logue, Antwerp, Belgium.Lec?uche, Renaud, Chris Mellish, Catherine Barry,and Dave Robertson.
1999.
User-system dia-logues and the notion of focus.
Knowl.
Eng.
Rev.,13(4):381?408.Palmer, Martha, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpusof semantic roles.
Comput.
Linguist., 31(1):71?106.Stede, Manfred and David Schlangen.
2004.Information-seeking chat: Dialogue management bytopic structure.
In Proc.
of SemDial?04 (Catalog),Barcelona, Spain.Vallduvi, Enric.
1990.
The Informational Component.Ph.D.
thesis, University of Pennsylvania, Philadel-phia, PA.van Schooten, Boris and Rieks op den Akker.
2005.Follow-up utterances in QA dialogue.
TraitementAutomatique des Langues, 46(3):181?206.van Schooten, Boris, R. op den Akker, R. Rosset,O.
Galibert, A. Max, and G. Illouz.
forthcoming.Follow-up question handling in the IMIX and Ritelsystems: A comparative study.
Journal of NaturalLanguage Engineering.Voorhees, Ellen M. 2004.
Overview of the TREC 2004question answering track.
In Proc.
of the 13th TextREtrieval Conference.32
