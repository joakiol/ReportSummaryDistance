Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 49?56,Prague, Czech Republic, June 2007 c?2007 Association for Computational LinguisticsISA meets Lara:An incremental word space modelfor cognitively plausible simulations of semantic learningMarco BaroniCIMeC (University of Trento)C.so Bettini 3138068 Rovereto, Italymarco.baroni@unitn.itAlessandro LenciDepartment of LinguisticsUniversity of PisaVia Santa Maria 3656126 Pisa, Italyalessandro.lenci@ilc.cnr.itLuca OnnisDepartment of PsychologyCornell UniversityIthaca, NY 14853lo35@cornell.eduAbstractWe introduce Incremental Semantic Analy-sis, a fully incremental word space model,and we test it on longitudinal child-directedspeech data.
On this task, ISA outperformsthe related Random Indexing algorithm, aswell as a SVD-based technique.
In addi-tion, the model has interesting propertiesthat might also be characteristic of the se-mantic space of children.1 IntroductionWord space models induce a semantic space fromraw textual input by keeping track of patterns ofco-occurrence of words with other words through avectorial representation.
Proponents of word spacemodels such as HAL (Burgess and Lund, 1997) andLSA (Landauer and Dumais, 1997) have argued thatsuch models can capture a variety of facts about hu-man semantic learning, processing, and representa-tion.
As such, word space methods are not onlyincreasingly useful as engineering applications, butthey are also potentially promising for modelingcognitive processes of lexical semantics.However, to the extent that current word spacemodels are largely non-incremental, they can hardlyaccommodate how young children develop a seman-tic space by moving from virtually no knowledgeof the language to reach an adult-like state.
Thefamily of models based on singular value decom-position (SVD) and similar dimensionality reduc-tion techniques (e.g., LSA) first construct a full co-occurrence matrix based on statistics extracted fromthe whole input corpus, and then build a model atonce via matrix algebra operations.
Admittedly,this is hardly a plausible simulation of how chil-dren learn word meanings incrementally by beingexposed to short sentences containing a relativelysmall number of different words.
The lack of incre-mentality of several models appears conspicuous es-pecially given their explicit claim to solve old theo-retical issues about the acquisition of language (e.g.,(Landauer and Dumais, 1997)).
Other extant modelsdisplay some degree if incrementality.
For instance,HAL and Random Indexing (Karlgren and Sahlgren,2001) can generate well-formed vector representa-tions at intermediate stages of learning.
However,they lack incrementality when they make use of stopword lists or weigthing techniques that are based onwhole corpus statistics.
For instance, consistentlywith the HAL approach, Li et al (2000) first builda word co-occurrence matrix, and then compute thevariance of each column to reduce the vector dimen-sions by discarding those with the least contextualdiversity.Farkas and Li (2000) and Li et al (2004) pro-pose an incremental version of HAL by using a a re-current neural network trained with Hebbian learn-ing.
The networks incrementally build distributionalvectors that are then used to induce word semanticclusters with a Self-Organizing Map.Farkas and Li(2000) does not contain any evaluation of the struc-ture of the semantic categories emerged in the SOM.A more precise evaluation is instead performed byLi et al (2004), revealing the model?s ability to sim-ulate interesting aspects of early vocabulary dynam-ics.
However, this is achieved by using hybrid word49representations, in which the distributional vectorsare enriched with semantic features derived fromWordNet.Borovsky and Elman (2006) also model wordlearning in a fairly incremental fashion, by using thehidden layer vectors of a Simple Recurrent Networkas word representations.
The network is probed atdifferent training epochs and its internal represen-tations are evaluated against a gold standard ontol-ogy of semantic categories to monitor the progress inword learning.
Borovsky and Elman (2006)?s claimthat their model simulates relevant aspects of childword learning should probably be moderated by thefact that they used a simplified set of artificial sen-tences as training corpus.
From their simulations itis thus difficult to evaluate whether the model wouldscale up to large naturalistic samples of language.In this paper, we introduce Incremental SemanticIndexing (ISA), a model that strives to be more de-velopmentally plausible by achieving full incremen-tality.
We test the model and some of its less incre-mental rivals on Lara, a longitudinal corpus of child-directed speech based on samples of child-adult lin-guistic interactions collected regularly from 1 to 3years of age of a single English child.
ISA achievesthe best performance on these data, and it learnsa semantic space that has interesting properties forour understanding of how children learn and struc-ture word meaning.
Thus, the desirability of incre-mentality increases as the model promises to cap-ture specific developmental trajectories in semanticlearning.The plan of the paper is as follows.
First, weintroduce ISA together with its main predecessor,Random Indexing.
Then, we present the learningexperiments in which several versions of ISA andother models are trained to induce and organize lexi-cal semantic information from child-directed speechtranscripts.
Lastly, we discuss further work in devel-opmental computational modeling using word spacemodels.2 Models2.1 Random IndexingSince the model we are proposing can be seen asa fully incremental variation on Random Indexing(RI), we start by introducing the basic features ofRI (Karlgren and Sahlgren, 2001).
Initially, eachcontext word is assigned an arbitrary vector repre-sentation of fixed dimensionality d made of a smallnumber of randomly distributed +1 and -1, with allother dimensions assigned a 0 value (d is typicallymuch smaller than the dimensionality of the full co-occurrence matrix).
This vector representation iscalled signature.
The context-dependent represen-tation for a given target word is then obtained byadding the signatures of the words it co-occurs withto its history vector.
Multiplying the history by asmall constant called impact typically improves RIperformance.
Thus, at each encounter of target wordt with a context word c, the history of t is updated asfollows:ht += i?
sc (1)where i is the impact constant, ht is the history vec-tor of t and sc is the signature vector of c. In thisway, the history of a word keeps track of the con-texts in which it occurred.
Similarity among wordsis then measured by comparing their history vectors,e.g., measuring their cosine.RI is an extremely efficient technique, since it di-rectly builds and updates a matrix of reduced di-mensionality (typically, a few thousands elements),instead of constructing a full high-dimensional co-occurrence matrix and then reducing it through SVDor similar procedures.
The model is incrementalto the extent that at each stage of corpus process-ing the vector representations are well-formed andcould be used to compute similarity among words.However, RI misses the ?second order?
effects thatare claimed to account, at least in part, for the ef-fectiveness of SVD-based techniques (Manning andSchu?tze, 1999, 15.4).
Thus, for example, since dif-ferent random signatures are assigned to the wordscat, dog and train, the model does not capture thefact that the first two words, but not the third, shouldcount as similar contexts.
Moreover, RI is not fullyincremental in several respects.
First, on each en-counter of two words, the same fixed random sig-nature of one of them is added to the history of theother, i.e., the way in which a word affects anotherdoes not evolve with the changes in the model?sknowledge about the words.
Second, RI makes useof filtering and weighting procedures that rely on50global statistics, i.e., statistics based on whole cor-pus counts.
These procedures include: a) treatingthe most frequent words as stop words; b) cuttingoff the lowest frequency words as potential contexts;and c) using mutual information or entropy mea-sures to weight the effect of a word on the other).In addition, although procedures b) and c) may havesome psychological grounding, procedure a) wouldimplausibly entail that to build semantic represen-tations the child actively filters out high frequencywords as noise from her linguistic experience.
Thus,as it stands RI has some noticeable limitations as adevelopmental model.2.2 Incremental Semantic AnalysisIncremental Semantic Analysis (ISA) differs fromRI in two main respects.
First and most importantly,when a word encounters another word, the historyvector of the former is updated with a weighted sumof the signature and the history of the latter.
Thiscorresponds to the idea that a target word is affectednot only by its context words, but also by the se-mantic information encoded by that their distribu-tional histories.
In this way, ISA can capture SVD-like second order effects: cat and dog might worklike similar contexts because they are likely to havesimilar histories.
More generally, this idea relies ontwo intuitively plausible assumptions about contex-tual effects in word learning, i.e., that the informa-tion carried by a context word will change as ourknowledge about the word increases, and that know-ing about the history of co-occurrence of a contextword is an important part of the information beingcontributed by the word to the targets it affects.Second, ISA does not rely on global statistics forfiltering and weighting purposes.
Instead, it uses aweighting scheme that changes as a function of thefrequency of the context word at each update.
Thismakes the model fully incremental and (togetherwith the previous innovation) sensitive not only tothe overall frequency of words in the corpus, but tothe order in which they appear.More explicitly, at each encounter of a target wordt with a context word c, the history vector of t isupdated as follows:ht += i?
(mchc + (1?mc)sc) (2)The constant i is the impact rate, as in the RI for-mula (1) above.
The valuemc determines how muchthe history of a word will influence the history of an-other word.
The intuition here is that frequent wordstend to co-occur with a lot of other words by chance.Thus, the more frequently a word is seen, the lessinformative its history will be, since it will reflectuninteresting co-occurrences with all sorts of words.ISA implements this by reducing the influence thatthe history of a context word c has on the target wordt as a function of the token frequency of c (noticethat the model still keeps track of the encounter withc, by adding its signature to the history of t; it is justthe history of c that is weighted down).
More pre-cisely, the m weight associated with a context wordc decreases as follows:mc =1exp(Count(c)km)where km is a parameter determining how fast thedecay will be.3 Experimental setting3.1 The Lara corpusThe input for our experiments is provided by theChild-Directed-Speech (CDS) section of the Laracorpus (Rowland et al, 2005), a longitudinal cor-pus of natural conversation transcripts of a singlechild, Lara, between the ages of 1;9 and 3;3.
Larawas the firstborn monolingual English daughter oftwo White university graduates and was born andbrought up in Nottinghamshire, England.
The cor-pus consists of transcripts from 122 separate record-ing sessions in which the child interacted with adultcaretakers in spontaneous conversations.
The totalrecording time of the corpus is of about 120 hours,representing one of the densest longitudinal corporaavailable.
The adult CDS section we used containsabout 400K tokens and about 6K types.We are aware that the use of a single-child corpusmay have a negative impact on the generalizationson semantic development that we can draw from theexperiments.
On the other hand, this choice has theimportant advantage of providing a fairly homoge-neous data environment for our computational sim-ulations.
In fact, we can abstract from the intrin-sic variability characterizing any multi-child corpus,51and stemming from differences in the conversationsettings, in the adults?
grammar and lexicon, etc.Moreover, whereas we can take our experiments toconstitute a (very rough) simulation of how a par-ticular child acquires semantic representations fromher specific linguistic input, it is not clear what simu-lations based on an ?averages?
of different linguisticexperiences would represent.The corpus was part-of-speech-tagged and lem-matized using the CLAN toolkit (MacWhinney,2000).
The automated output was subsequentlychecked and disambiguated manually, resulting invery accurate annotation.
In our experiments, weuse lemma-POS pairs as input to the word spacemodels (e.g., go-v rather than going, goes, etc.
)Thus, we make the unrealistic assumptions that thelearner already solved the problem of syntactic cate-gorization and figured out the inflectional morphol-ogy of her language.
While a multi-level bootstrap-ping process in which the morphosyntactic and lex-ical properties of words are learned in parallel isprobably cognitively more likely, it seems reason-able at the current stage of experimentation to fixmorphosyntax and focus on semantic learning.3.2 Model trainingWe experimented with three word space models:ISA, RI (our implementations in both cases) and theSVD-based technique implemented by the Infomappackage.1Parameter settings may considerably impact theperformance of word space models (Sahlgren,2006).
In a stage of preliminary investigations (notreported here, and involving also other corpora) weidentified a relatively small range of values for eachparameter of each model that produced promisingresults, and we focused on it in the subsequent, moresystematic exploration of the parameter space.For all models, we used a context window of fivewords to the left and five words to the right of thetarget.
For both RI and ISA, we set signature initial-ization parameters (determining the random assign-ment of 0s, +1s and -1s to signature vectors) similarto those described by Karlgren and Sahlgren (2001).For RI and SVD, we used two stop word filteringlists (removing all function words, and removing the1http://infomap-nlp.sourceforge.net/top 30 most frequent words), as well as simulationswith no stop word filtering.
For RI and ISA, we usedsignature and history vectors of 1,800 and 2,400 di-mensions (the first value, again, inspired by Karl-gren and Sahlgren?s work).
Preliminary experimentswith 300 and 900 dimensions produced poor results,especially with RI.
For SVD, we used 300 dimen-sions only.
This was in part due to technical lim-itations of the implementation, but 300 dimensionsis also a fairly typical choice for SVD-based mod-els such as LSA, and a value reported to produceexcellent results in the literature.
More importantly,in unrelated experiments SVD with 300 dimensionsand function word filtering achieved state-of-the-artperformance (accuracy above 90%) in the by nowstandard TOEFL synonym detection task (Landauerand Dumais, 1997).After preliminary experiments showed that bothmodels (especially ISA) benefited from a very lowimpact rate, the impact parameter i of RI and ISAwas set to 0.003 and 0.009.
Finally, km (the ISA pa-rameter determining the steepness of decay of theinfluence of history as the token frequency of thecontext word increases) was set to 20 and 100 (recallthat a higher km correspond to a less steep decay).The parameter settings we explored were system-atically crossed in a series of experiments.
More-over, for RI and ISA, given that different randominitializations will lead to (slightly) different results,each experiment was repeated 10 times.Below, we will report results for the best perform-ing models of each type: ISA with 1,800 dimen-sions, i set to 0.003 and km set to 100; RI with 2,400dimensions, i set to 0.003 and no stop words; SVDwith 300-dimensional vectors and function wordsremoved.
However, it must be stressed that 6 outof the 8 ISA models we experimented with outper-formed the best RI model (and they all outperformedthe best SVD model) in the Noun AP task discussedin section 4.1.
This suggests that the results we re-port are not overly dependent on specific parameterchoices.3.3 Evaluation methodThe test set was composed of 100 nouns and 70verbs (henceforth, Ns and Vs), selected from themost frequent words in Lara?s CDS section (wordfrequency ranges from 684 to 33 for Ns, and from523501 to 89 for Vs).
This asymmetry in the testset mirrors the different number of V and N typesthat occur in the input (2828 Ns vs. 944 Vs).
Asa further constraint, we verified that all the wordsin the test set alo appeared among the child?s pro-ductions in the corpus.
The test words were un-ambiguously assigned to semantic categories pre-viously used to model early lexical developmentand represent plausible early semantic groupings.Semantic categories for nouns and verbs were de-rived by combining two methods.
For nouns, weused the ontologies from the Macarthur-Bates Com-municative Development Inventories (CDI).2 Allthe Ns in the test set alo appear in the Tod-dler?s List in CDI.
The noun semantic categories arethe following (in parenthesis, we report the num-ber of words per class and an example): ANI-MALS REAL OR TOY (19; dog), BODY PARTS (16;nose), CLOTHING (5; hat), FOOD AND DRINK (13;pizza), FURNITURE AND ROOMS (8; table), OUT-SIDE THINGS AND PLACES TO GO (10; house),PEOPLE (10; baby), SMALL HOUSEHOLD ITEMS(13; bottle), TOYS (6; pen).
Since categories forverbs were underspecified in the CDI, we used12 broad verb semantic categories for event types,partly extending those in Borovsky and Elman(2006): ACTION (11; play), ACTION BODY (6;eat), ACTION FORCE (5; pull), ASPECTUAL (6;start), CHANGE (12; open), COMMUNICATION (4;talk), MOTION (5; run), PERCEPTION (6; hear),PSYCH (7; remember), SPACE (3; stand), TRANS-FER (6; buy).It is worth emphasizing that this experimental set-ting is much more challenging than those that areusually adopted by state-of-the-art computationalsimulations of word learning, as the ones reportedabove.
For instance, the number of words in ourtest set is larger than the one in Borovsky and Elman(2006), and so is the number of semantic categories,both for Ns and for Vs. Conversely, the Lara corpusis much smaller than the data-sets normally used totrain word space models.
For instance, the best re-sults reported by Li et al (2000) are obtained withan input corpus which is 10 times bigger than ours.As an evaluation measure of the model perfor-mance in the word learning task, we adopted Aver-2http://www.sci.sdsu.edu/cdi/age Precision (AP), recently used by Borovsky andElman (2006).
AP evaluates how close all membersof a certain category are to each other in the seman-tic space built by the model.To calculate AP, for each wi in the test set we firstextracted the corresponding distributional vector viproduced by the model.
Vectors were used to cal-culate the pair-wise cosine between each test word,as a measure of their distance in the semantic space.Then, for each target word wi, we built the list ri ofthe other test words ranked by their decreasing co-sine values with respect to wi.
The ranking ri wasused to calculate AP (wi), the Word Average Preci-sion for wi, with the following formula:AP (wi) =1|Cwi |?wj?Cwinwj (Cwi)nwjwhere Cwi is the semantic category assigned to wi,nwj is the set of words appearing in ri up to the rankoccupied bywj , and nwj (Cwi) is the subset of wordsin nwj that belong to category Cwi .AP (wi) calculates the proportion of words thatbelong to the same category of wi at each rank inri, and then divides this proportion by the numberof words that appear in the category.
AP rangesfrom 0 to 1: AP (wi) = 1 would correspond to theideal case in which all the closest words to wi in ribelonged to the same category as wi; conversely, ifall the words belonging to categories other than Cwiwere closer to wi than the words in Cwi , AP (wi)would approach 0.
We also defined the Class APfor a certain semantic category by simply averagingover the Word AP (wi) for each word in that cate-gory:AP (Ci) =?j=|Ci|j=1 AP (wj)|Ci|We adopted AP as a measure of the purity and co-hesiveness of the semantic representations producedby the model.
Words and categories for which themodel is able to converge on well-formed represen-tations should therefore have higher AP values.
Ifwe define Recall as the number of words in nwj be-longing to Cwi divided by the total number of wordsin Cwi , then all the AP scores reported in our exper-iments correspond to 100% Recall, since the neigh-bourhood we used to compute AP (wi) always in-cluded all the words in Cwi .
This represents a very53NounsTokens ISA RI SVD100k 0.321 0.317 0.243200k 0.343 0.337 0.284300k 0.374 0.367 0.292400k 0.400 0.393 0.306Verbs100k 0.242 0.247 0.183200k 0.260 0.266 0.205300k 0.261 0.266 0.218400k 0.270 0.272 0.224Table 1: Word AP scores for Nouns (top) and Verbs(bottom).
For ISA and RI, scores are averagedacross 10 iterationsstringent evaluation condition for our models, far be-yond what is commonly used in the evaluation ofclassification and clustering algorithms.4 Experiments and results4.1 Word learningSince we intended to monitor the incremental pathof word learning given increasing amounts of lin-guistic input, AP scores were computed at four?training checkpoints?
established at 100K, 200K,300K and 400K word tokens (the final point corre-sponding to the whole corpus).3 Scores were calcu-lated independently for Ns and Vs.
In Table 1, wereport the AP scores obtained by the best perform-ing models of each type , as described in section 3.2.The reported AP values refer to Word AP averagedrespectively over the number of Ns and Vs in the testset.
Moreover, for ISA and RI we report mean APvalues across 10 repetitions of the experiment.For Ns, both ISA and RI outperformed SVD at alllearning stages.
Moreover, ISA also performed sig-nificantly better than RI in the full-size input condi-tion (400k checkpoint), as well as at the 300k check-point (Welch t-test; df = 17, p < .05).One of the most striking results of these experi-ments was the strongN-V asymmetry in theWord APscores, with the Vs performing significantly worsethan the Ns.
For Vs, RI appeared to have a smalladvantage over ISA, although it was never signifi-cant at any stage.
The asymmetry is suggestive ofthe widely attested N-V asymmetry in child word3The checkpoint results for SVD were obtained by trainingdifferent models on increasing samples from the corpus, giventhe non-incremental nature of this method.learning.
A consensus has gathered in the earlyword learning literature that children from severallanguages acquire Ns earlier and more rapidly thanVs (Gentner, 1982).
An influential account explainsthis noun-bias as a product of language-external fac-tors such as the different complexity of the worldreferents for Ns and Vs.
Recently, Christiansen andMonaghan (2006) found that distributional informa-tion in English CDS was more reliable for identi-fying Ns than Vs.
This suggests that the category-bias may also be partly driven by how good cer-tain language-internal cues for Ns and Vs are in agiven language.
Likewise, distributional cues to se-mantics may be stronger for English Ns than forVs.
The noun-bias shown by ISA (and by the othermodels) could be taken to complement the resultsof Christiansen and Monaghan in showing that En-glish Ns are more easily discriminable than Vs ondistributionally-grounded semantic terms.4.2 Category learningIn Table 2, we have reported the Class AP scoresachieved by ISA, RI and SVD (best models) underthe full-corpus training regime for the nine nominalsemantic categories.
Although even in this case ISAand RI generally perform better than SVD (with theonly exceptions of FURNITURE AND ROOMSand SMALL HOUSEHOLD ITEMS), resultsshow a more complex and articulated sit-uation.
With BODY PARTS, PEOPLE, andSMALL HOUSEHOLD ITEMS, ISA significantlyoutperforms its best rival RI (Welch t-test; p < .05).For the other classes, the differences among the twomodels are not significant, except for CLOTHINGin which RI performs significantly better than ISA.For verb semantic classes (whose analytical data arenot reported here for lack of space), no significantdifferences exist among the three models.Some of the lower scores in Table 2 can be ex-plained either by the small number of class mem-bers (e.g.
TOYS has only 6 items), or by the classhighly heterogeneous composition (e.g.
in OUT-SIDE THINGS AND PLACES TO GO we find nounslike garden, flower and zoo).
The case of PEOPLE,for which the performance of all the three modelsis far below their average Class AP score (ISA =0.35; RI = 0.35; SVD = 0.27), is instead much moresurprising.
In fact, PEOPLE is one of the classes54Semantic class ISA RI SVDANIMALS REAL OR TOY 0.616 0.619 0.438BODY PARTS 0.671 0.640 0.406CLOTHING 0.301 0.349 0.328FOOD AND DRINK 0.382 0.387 0.336FURNITURE AND ROOMS 0.213 0.207 0.242OUTSIDE THINGS PLACES 0.199 0.208 0.198PEOPLE 0.221 0.213 0.201SMALL HOUSEHOLD ITEMS 0.208 0.199 0.244TOYS 0.362 0.368 0.111Table 2: Class AP scores for Nouns.
For ISA andRI, scores are averaged across 10 iterationswith the highest degree of internal coherence, be-ing composed only of nouns unambiguously denot-ing human beings, such as girl, man, grandma, etc.The token frequency of the members in this class isalso fairly high, ranging between 684 and 55 occur-rences.
Last but not least, in unrelated experimentswe found that a SVD model trained on the BritishNational Corpus with the same parameters as thoseused with Lara was able to achieve very good per-formances with human denoting nouns, similar tothe members of our PEOPLE class.These facts have prompted us to better investi-gate the reasons why with Lara none of the threemodels was able to converge on a satisfactory rep-resentation for the nouns belonging to the PEO-PLE class.
We zoomed in on this semantic classby carrying out another experiment with ISA.
Thismodel underwent 8 cycles of evaluation, in each ofwhich the 10 words originally assigned to PEOPLEhave been reclassified into one of the other nom-inal classes.
For each cycle, AP scores were re-computed for the 10 test words.
The results are re-ported in Figure 1 (where AP refers to the averageWord AP achieved by the 10 words originally be-longing to the class PEOPLE).
The highest score isreached when the PEOPLE nouns are re-labeled asANIMALS REAL OR TOY (we obtained similar re-sults in a parallel experiment with SVD).
This sug-gests that the low score for the class PEOPLE in theoriginal experiment was due to ISA mistaking peo-ple names for animals.
What prima facie appearedas an error could actually turn out to be an interestingfeature of the semantic space acquired by the model.The experiments show that ISA (as well as the othermodels) groups together animals and people Ns, asFigure 1: AP scores for Ns in PEOPLE reclassifiedin the other classesit has formed a general and more underspecified se-mantic category that we might refer to as ANIMATE.This hypothesis is also supported by qualitative ev-idence.
A detailed inspection of the CDS in theLara corpus reveals that the animal nouns in thetest set are mostly used by adults to refer either totoy-animals with which Lara plays or to charactersin stories.
In the transcripts, both types of entitiesdisplay a very human-like behavior (i.e., they talk,play, etc.
), as it happens to animal characters in mostchildren?s stories.
Therefore, the difference betweenmodel performance and the gold standard ontologycan well be taken as an interesting clue to a genuinepeculiarity in children?s semantic space with respectto adult-like categorization.
Starting from an inputin which animal and human nouns are used in sim-ilar contexts, ISA builds a semantic space in whichthese nouns belong to a common underspecified cat-egory, much like the world of a child in which catsand mice behave and feel like human beings.5 ConclusionOur main experiments show that ISA significantlyoutperforms state-of-the-art word space models ina learning task carried out under fairly challengingtraining and testing conditions.
Both the incremen-tal nature and the particular shape of the semanticrepresentations built by ISA make it a (relatively)realistic computational model to simulate the emer-55gence of a semantic space in early childhood.Of course, many issues remain open.
First of all,although the Lara corpus presents many attractivecharacteristics, it still contains data pertaining to asingle child, whose linguistic experience may be un-usual.
The evaluation of the model should be ex-tended to more CDS corpora.
It will be especiallyinteresting to run experiments in languages such asas Korean (Choi and Gopnik, 1995), where no noun-bias is attested.
There, we would predict that the dis-tributional information to semantics be less skewedin favor of nouns.
All CDS corpora we are aware ofare rather small, compared to the amount of linguis-tic input a child hears.
Thus, we also plan to test themodel on ?artificially enlarged?
corpora, composedof CDS from more than one child, plus other textsthat might be plausible sources of early linguistic in-put, such as children?s stories.In addition, the target of the model?s evaluationshould not be to produce as high a performance aspossible, but rather to produce performance match-ing that of human learners.4 In this respect, theoutput of the model should be compared to what isknown about human semantic knowledge at variousstages, either by looking at experimental results inthe acquisition literature or, more directly, by com-paring the output of the model to what we can in-fer about the semantic generalizations made by thechild from her/his linguistic production recorded inthe corpus.Finally, further studies should explore how thespace constructed by ISA depends on the order inwhich sentences are presented to it.
This could shedsome light on the issue of how different experien-tial paths might lead to different semantic general-izations.While these and many other experiments must berun to help clarifying the properties and effective-ness of ISA, we believe that the data presented hereconstitute a very promising beginning for this newline of research.ReferencesBorovsky, A. and J. Elman.
2006.
Language input andsemantic categories: a relation between cognition and4We thank an anonymous reviewer for this noteearly word learning.
Journal of Child Language, 33:759-790.Burgess, C. and K. Lund.
1997.
Modelling parsingconstraints with high-dimensional context space.
Lan-guage and Cognitive Processes, 12: 1-34.Choi, S. and A. Gopnik, A.
1995.
Early acquisition ofverbs in Korean: a cross-linguistic study.
Journal ofChild Language 22: 497-529.Christiansen, M.H.
and P. Monaghan.
2006.
Dis-covering verbs through multiple-cue integration.
InK.
Hirsh-Pasek and R.M.
Golinkoff (eds.
), Actionmeets word: How children learn verbs.
OUP, Oxford.Farkas, I. and P. Li.
2001.
A self-organizing neural net-work model of the acquisition of word meaning.
Pro-ceedings of the 4th International Conference on Cog-nitive Modeling.Gentner, D. 1982.
Why nouns are learned before verbs:Linguistic relativity versus natural partitioning.
InS.A.
Kuczaj (ed.
), Language development, vol.
2: Lan-guage, thought and culture.
Erlbaum, Hillsdale, NJ.Karlgren, J. and M. Sahlgren.
2001.
From words to un-derstanding.
In Uesaka, Y., P. Kanerva and H.
Asoh(eds.
), Foundations of real-world intelligence, CSLI,Stanford: 294-308,Landauer, T.K.
and S.T.
Dumais.
1997.
A solution toPlato?s problem: The Latent Semantic Analysis theoryof acquisition, induction and representation of knowl-edge.
Psychological Review, 104(2): 211-240.Li, P., C. Burgess and K. Lund.
2000.
The acquisition ofword meaning through global lexical co-occurrences.Proceedings of the 31st Child Language Research Fo-rum: 167-178.Li, P., I. Farkas and B. MacWhinney.
2004.
Early lexicalacquisition in a self-organizing neural network.
Neu-ral Networks, 17(8-9): 1345-1362.Manning Ch.
and H. Schu?tze.
1999.
Foundations of sta-tistical natural language processing The MIT Press,Cambridge, MASS.MacWhinney, B.
2000.
The CHILDES project: Tools foranalyzing talk (3d edition).
Erlbaum, Mahwah, NJ.Rowland, C., J. Pine, E. Lieven and A. Theakston.2005.
The incidence of error in young children?s wh-questions.
Journal of Speech, Language and HearingResearch, 48(2): 384-404.Sahlgren, M. 2006.
The Word-Space Model: Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. dissertation, Depart-ment of Linguistics, Stockholm University.56
