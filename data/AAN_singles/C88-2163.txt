Default Reasoningin Natural Language Processingl id  ZERNIKArtificial Intelligence ProgramGE, Research and Development CenterAllen BROWNSystems Sciences LaboratoryXerox, Webster Research CenterAbstractIn natural )a~gnage, as in other computational task domains it isimpox~ant tc~ operate by default assumptions.
First, m~my con-straints re(tni~'exl h)r constraint propagation are initiallytmspecified, Second, in highly ambiguous tasks such as textanalysis, ambiguity can be reduced by considering more plansi-.hie scenarios first.
Default reasoning is problematic for first-order logic when allowing non-monotonic inferences.
Whereasin monotonic logic facts can only be asserted, in non-monotoniclogk: a system must be maintained consistent even as previouslyassumed efaults are being retracted.Non~monotoniety is pervasive in natural anguage due to the seri-al nature of utterances.
When reading text left-to-fight, it hap-pens that default assumptions made early in the sentence must bewithdrawn as reading proceeds.
Truth maintenance, which ac-counts for non-monotonic inferences, ctm resolve this issue andaddress important linguistic phenomena, hi this paper wedescribe, how in NMG (Non-Monotonic Grammar), by monitor-ing a logic parser, a truth maintenance system can significantly.enhauce the parser'g capabilities.L Introduct ionIronically, atlding knowlexlge to a computational system does notalways extend its power.
When equipped with comprehensivelinguistic knowledge the same system might fare worse thanwhen equipped with impoverished linguistic knowledge, sinceadditional rules might impair simple cases.
For example consid-er the following two sentences:(l) The ,-hitd sold his parents' dearest ornmnent.
(2) "file child sold to a stranger was found alive.Linguistic systems must handle both sentences: sentence (1)which appears unambiguous~ artd requh'es only basic Englishga~ammar, and sentence (2) whose interpretation is obscured byits g~axrmaatie conslruefion.
However, it is crucial that rules re-quired for h,'tndling sentence (2) will not complicate the analysisof sentence (l) by contfibnting sptwious ambiguity and computa-tional ovedlead.This linguistic behavior i s  problematic for computer parsers.Sentence (2) involves tile garden path phenomenon, in which thereader is let to a standard interpretation which is retracted to-wards the eJJd of the sentence.
On the other hand, sentence (1)is read "linearly" without any such deviations.
For a computerparser, it is important o show (a) how the irdtial interpretation isflipped in parsing sentence (2), and (b), that for sentence (1) nosuch flipping occurs.
However, this behavior cannot be be cap-tured by PROLOG-based parsers* \[Dah177,PereiraS0\] for tworeasons: (a) representation: all linguistic rules ate viewed asequal citizens, (b) computation: PROLOG backtracking cansearch for any solution, yet it cannot retract partial solutions.Thus, this difference in parsing complexity must yet be manifest-ed in computational temas.Default reasoning \[MeCarthyS0\] and Truth Maintenance Systems(TMS's) were introduced to cope with the representational ndcomputational limitations of first-order logic \[Doyle79, Reiter87,Goodwin87, Brown87\].
First, TMS's distinguish between defaultcases, and cases presenting deviations from the norm.
Second,TMS's  ~dlow the retraction of default assumptions during tile rea-soning process itself.
In this paper we explain how existiugparsing mechanisms based on unification fail in capturing impor-tant linguistic behavior.
Furthermore, by implementing NMG(Non-Monotonic Grammar), we demonstrate how by monitoringa logic parser, a TMS can significantly enhance the parser's per-fonnance without adding auy excessive overhead.2.
The Theoretical IssuesFour theoretical issues must be considered when parsing text:ambiguity, non monotonicity, context dependency, andknowledge gaps.
(a) Ambiguity: Consider the following sentence:Mary was tired, so John shnwed her home.Even for this simple sentence which appears unambiguous to ahuman reader, a parser must span a variety of interpretationsstemming from several inguistic levels:Semantics: Who is "John": is it John Ma~erg or is it John Kepler(where both exist in the context)?Syntax: What part of speech is "her":is it an objective pronoun, i.e.
he showed Mary homeor is it a possessive, i.e.
he showed Mary's home?Lexicon: What is "to show", and what is the intended memting:he showed the house to a potential buyer,or he showed her die way to her home?Role-Binding: Does "show" (meaning make-visible) take 1 or 2 objects?he showed <her>,or he showed <her> <home>?Context: What is the purpose of the act:selling Mary's house to potential buyers,or making sure she arrives afely home?
*We allude here to PROLOG as a parsing mechanism and not to theprogramming language PROLOG which is computationally universal.80\]A parser is required to negotiate this set ofconflicting/cooperating clues and to deposit in a database a hy-pothesis about the meaning of the utterance.
This problem iseven aggravated when imposing left-to-right order on parsing.At each point, the parser must deposit a hypothesis based on apartial set of clues, a hypothesis which might be later retracted.
(b) Non-Monotonicity: Garden path sentences highlight theproblem:(3) The old man's glasses were filled with sherry.
(4) I saw the Grand Canyon flying to New York.
(5) The book sold yesterday was cheap.
(6) The horse raced past he barn fell.In each one of these sentences, the initial hypothesis i  retractedwhen an assumed efault rule is violated:(3) A semantic assumption--gl~ses stand for lookingglasses-must be retracted at the end of the sentence-sinceglasses turn out to actually mean drinking containers.
(4) A syntactic assumption dictates the default sentence struc-ture (S -> VP, NP).
Thus the Grand Canyon is taken asthe agent flying to NY.
This interpretation is in conflictwith existing world knowledge (since canyons cannot fly).
(5) A word sold is assumed to take the active voice.
Back-tracking occurs due to knowledge of the selling act(books do not normally sell days).
(6) There are two default assumptions which fail: (1) raced istaken by default to be active voice, and (2) the clauseraced past the barn is taken as the verb phrase of the sen-tence.
Both assumptions fail ~,hen the word fell is en-countered, and raced is found to be the past participle inthe unusual sense of "being driven".Among these examples, some are impossible even for humanlisteners to comprehend; in others, the flipping of the meaning ishardly noticeable.
How can a parsing mechanism reflect thesedegrees of difficulty?
(c) Context Dependency: Compare the interpretations of fol-lowing pairs of sentences:(7) John wanted anew car.
He took it up with his dad.
(8) This is his new Porsche.
He took it up with his dad.The syntactic analysis was driven by the semantic ontext esta-blished by prior discourse: in (7), up is taken as an integral partof the entire lexicai phrase to take it up with somebody, where in (8)it serves as a general adverb.
This is due to the different in-terpretation of it: in (7) John discussed "an issue", while in (8)he probably drove "the car" itself up the hill accompanied by hisdad.
How can a parser eflect such context sensitivity?
(d) Lexical Gaps: Linguistic systems cannot be assumed com-plete, and so text processing must take place even in the pres-ence of lexicai gaps \[Zeruik87, Zernik88\].
Consider the follow-ing sentence which includes a lexical unknown, the word plend.
(9) John plended Mary to come over.How can a parser make even a partial sense out of this sentence?Can a parser use default lexicon rules in parsing such unknowns?3.
Truth Maintenance: The Architecture of NMGWhat is a truth maintenance sytem and how can it be tailored foruse in the linguistic domain?
NMG is a system for employingtruth maintenance in text comprehension.
NMG's architecture issimple: it includes a parser (PAR), and a Truth Maintenance Sys-tem (TMS).
PAR is a DCG-style parser \[Pereira80\], implement-ed as a forward-chaining theorem prover.
PAR produces a parsetree in the form of a logieai dependency-net.
This dependency-net is fed to a TMS whose output is an IN/OUT labellingpresenting the currently believed interpretation.
The basic com-putationai paradigm is described by an example in parsing thesentence below:(10) The baby sold by his parents was found alive.Text interpretation can be pursued in two possible ways: eitherby propagating all possible grammatic interpretations of thatutterance, as in logic parsers.
This might introduce spurious in-terpretations for simple text.
Or alternatively, by initially tryinga default interpretation, and only when this default interpretationfails try out other interpretations.
This is the approach taken byNMG.
There are two interesting stages in the computation, ac-cording to the text already received:(10a) The baby sold(10b) by his parents was found alive.One basic default assumption was committed at stage (10a):o Unless otherwise observed, the verb sold assumes the ac-tive voice past tense.The rules related to this assumption are given in NMG as fol-lows:(rl) verb(sell,active,pas0 :- text(sold) O not-active(sold)(r2) not-active(sold) :- preceded(sold, was)(r3) not-active(sold) :.
followed(sold, by-clause)NMG extends DCG's Horu-clause notation by allowing an "un-less" term, marked by the symbol O.
Such an "unless" term ap-pears in (rl), the default case for sold.
Deviations can be esta-blished as aggregation of cases.
(R2) presents the first deviation.When sold is preceded by the particle was, it is taken as a passivevoice.
(R3) presents the second deviation, when a by-clanse fol-lows sold.
Other deviations may be added on as the linguisticsystem is enhanced.
The diagram below describes two snapshotsin the computation of sentence (10).802 (a)'( ( Ds~)Notation: ovals in this scheme stand for facts; AND gates standfor rules; dark ovals are IN; light ovals are OUT.
Consider part(a) which describes the parse of (10a): The dependency-net con-strutted by PAR is based on instantiated linguistic rules.
Thereare three new instantiated facts: NP the noun phrase, V the verb,and S the entire sentence (some short cuts were made in drawingthe parse tree due to space limitations).
The associated IN/OUTlabelling is produced by the TMS, and so far all the facts havebeen labelk~ IN.
In particular, the output of the default rule(rl) is labelled IN, since its inhibitive port (marked by an invert-er) is labeled OUT: no deviation of (rl) has yet been observed.Part (b) describes the parse after the rest of the sentence (10b)has been read.
(Notice that the dependencies of part (a) havebeen copied over for reference purposes, although in the modelitself dependencies are not recalculated or copied).
Reading(10b) causes the withdrawal of the previous interpretation andthe construction of a new one.
However, since new words wereonly added on, how in this scheme, could anything be with-drawn?
In stage (b) too there are two orthogonal ctivities:(1) PAR constructs new dependencies: The by-clause followingsold justifies the inhibitive port of (rl); the same by-clause alsojustifies an alternative role for sold 0/P), as a passive voice verb;this fact plus the by-clause itself add up to a relative clause (Re);RC joins the old noun-phrase (NP) to form a composite noun-phrase (CNP); CNP now joins a new verb-phrase O/P) in form-ing a new ~ntence (S).
Throughout this process no dependen-cies trove been modified or retracted; new dependencies wereonly added on.
(2) The TMS relabels the network.
First, the old interpreta-tion is ruled out: since the inhibitive input of (rl) is now labeledIN, the output of (rl) becomes OUT, and so does the initial in-terpretation S. The rest of the new facts are labelled IN.
Thus,the non-monotonic effect is accomplished by relabeling nodes,and not by retracting dependencies.A PROLOG-based parser, at stage (b), must undo all its priorparsing inferences, retract he fact it has deposited in the data-base, and start processing from scratch, this time ruling out theincorrect default assumption.
Using a TMS, a parser can recoverby simply relabeling the parts of the parse which depend on theabove assumptions, and proceod gracefully thereafter.Problems which are nattmdly addreued by non-~e infer-enee are pervasive in the linguiltic~ domain, wheze palmingproceeds left-to-fight, and they are epitomized by garden pathsentences.4.
NMG: A Process ModelNon-monotonic reasoning is not confined only to garden pathsentences.
We show here an example of an apparently simplesentence, for which interpretations are asserted and retraceddynamically.
This example also demonstrates the role of defaultreasoning in lexical access and in context interaction, wbem thecontext is the semantic structure yielded by prior discourse.Consider the initial interpretation constructed after reading thefollowing utterance:(lla) John needed a new battery.
He took itThis text yields an initial hypothesis: "John ptransed a battery".This hypothesis based on two default roles: (1) Lexieai access:unless otherwise observed, a generic word such as take indexesthe generic meaning ptrans (physical transfer) \[Schank771 (2)Context interaction: Unless otherwise observed, it refers to thelast physical object referred to in the discourse \[Sidner79\] (here itrefers to the battery).
However, as reading proceeds, these hy-potheses are retracted:(l Ib) John needed a new battery.
He took it up with his dadAt this point, a more specific lexical entry is accessed: "X take Yup with Z" in the sense of "raising an issue" \[Wilks75, Wilen-sky80\].
The referent for it is switched now from the battery it-self to "John's goal of getting a battery", due to selectional res-trictions in the new lexical phrase.
However, the initial interpre-tation is recovered when reading continues:(tic) John needed a new battery.He took it up with his dad from the basement.At this point, the additional clauses in the sentence are used toaugment the initial hypothesis which had been previously aban-doned.
Notice that the initial hypothesis i  recovered simply bymarlding it IN and not by computing it from scratch.5.
Logic Programming: From CFG through DCG to NMGLogic programming \[Colmerauer73, Kowalski79, Dah177,Pereira80\] has mechanized many declarative linguistic systems\[Kay79, Bresnan82, Shieber87\], and provided a new computa-tional paradigm.
Definite-Clause Grammars (DCG) in particular,have exploited the advantages of Context-Free Grammars (CFG)by a simple extension.
In DOG, a non-terminai may be anyPROLOG term, rather than simply an atom as in CFG.
The fol-lowing example demonstrates how one particular rule hasevolved from CFG to DCG.CFG: sentence --> noun-phrase, v rb-phraseDCG: sentence(s(NP,VP)) :- noun-phrase(NP,N), verb-phrase(NP,N)This extension has two features: (a) maintaining agreementrules-the argument N maintains the number in both the noun andthe verb; (b) co0structing semantic denotations-the argumentsNP and VP contain the denotations of the constituents, fromwhich the denotation of the entire sentence (s(NP, VP)) is con-structed.803Logic progrmnming has assumed a central role in language pro-cessing for two reasons: (a) It allowed the expression of declara-tive linguistic rules, and (b) Direct application of PROLOGoperationalized grammars, using unification and backtracking asthe mechanism.
PROLOG also enabled other gaTtrmnars (besideDCG) such as transformational grammar \[Chomsky81\] and casegrammm" \[Fillmore68\], to be emulated.
However, the direct ap-plication of PROLOG has presented three limitations: (a) Pars-ing was driven by syntax, and the semantic interpretation was aby-product.
(b) While PROLOG itself can be extended to ex-press default (through the notion of negation as.failure \[Clark77\]), PROLOG does not have an explicit notion ofdependency so that a parser can diagnose what went wrong withthe parse.
(c) PROLOG itself does not facilitate default reason-ing which can resolve lexical gaps.Therefore, we have introduced NMG; a logic parser whichenhances DCG's capabilities in three' ways: non-monotonic rea.-soning, refinement and retraction, and diagnostic reasoning.
(a) Non-Monotonic Reasoning: NMG enables the parser togracefully con'ect its parse tree by identifying parts of the rea-soning strncture which depend on retracted assumptions.
Non-monotonicity is pervasive in language processing due to the seri-al nature of language.
(b) Retraction and Refinement: A main objective in text pro-cessing, required for left-to-right parsing, has been parsing byrefinement.
In reading a sentence, a parser should not deposit asingle final meaning when a "full stop" is encountered.
Rather,an initial concept must be asserted as early as possible, an as~r-tion which must be refined as further words are provided at theinput, ltowever, in fulfilling this o,bjective, existing models\[Hirst86, Lytinen84, Jacobs87\] have not dealt with the possibilitythat the entire hypothesis might be retracted, and replaced by asecond hypothesis.
NMG enables a parser to both refine an ex-isting hypothesis, mad to retract the entire hypothesis from thedatabase, if contradictory evidence has been received.
(c) Diagnostic Reasoning: Consider a pair of operationalmodes, given in the diagram below:text dependencies labelin~text dependencies labellnqWhile in (1), the system operates in an "open loop", and theTMS basically monitors which hypothesis i  cmTently IN, in (2)the information produced by the TMS can be used in reasoningabout the parse itself.
This is important in learning and in pars-ing ill-fo~rned text.
We describe this mode in a later repom6.
ConelusionsWe have presented NMG, a mechanism which can potentiallyenhance all logic parsers.
NMG's advantages are emphasized inparsing complex sentences in which hypotheses are being retract-ed.
However, its main advantage is in avoiding spurious activitywhen parsing simple sentences.
Thus we have accomplished anobjective laid down by Allan Kay: "Easy things should be easy;hard things should be possible".B04\[Bresnm~82\]\[Brown87\]\[Chomsky81\]\[Clark77\]\[Colmerauer73\]\[Dah177\]\[Doyle79\]\[Fillmore68\]\[Goodwin87\]\[Hirst86\]\[Jacobs87\]\[Kay79\]\[Kowalski79\]\[Lytinen84\]Referene~sBresnan, J. and R. Kaplan, "Lexical-Functional Grammar," in The MemcqRepresentation of Grammatical Relations, ed.J.
Bresnan, MIT Press, MA (1982).Brown, A., D. Gancas, and D. Benanav, "AnAlgebraic Foundation for Truth Mainte-nance," in Proceedings The lOth Internation-al Joint Conference on Artificial Intelligence,Milano Italy (August I987).Chomsky, N., Lectures on Government amlBinding, Foris, Dordrecht (1981).Clark, K.L., "Negation as Failure," in Logicand Data Bases, ed.
H. Gallaim~ J. Mink~Plenum Press, New York and London (1977)oColmerauer, A,  It.
Kanoui, P. Roussel, andR.
Pasero, "Un Systeme de CommunicationHomme-Machine n Francais," Universited'Aix-Marseille, Marseille, France (1973).Tech Report.Dald, Veronica, "f in System Deductifd'Interrogafion de Banques de Donnees enEspgnol," Universite d'Aix-Marseille, Mar-seille, France (1977).
PhD Dissertation.Doyle, J., "A Truth Maintenance System,"Artificial Intelligence 12 (1979)oFillmore, C., "The Case for Case," pp.
1-90in Universals in Linguistic Theory, ed.
E.Bach R. Harms, Holt, Reinhart and Winston,Chicago (1968)oGoodwin, J.W., "A Theory and System forNon-Monotonic Reasoning," LinkopingUniversity, Linkoping, Sweden (1987).
PhDDissertation.Hirst, G. J., Semantic Interpretation and theResolution of Ambiguity, Cambridge, NewYork, NY (1986).Jacobs, P., "Concretion: Assumption-BasedUnderstanding," in COLING 88, Budapest,Hungary (1988).Kay, Martin, "Functional Grammar," pp.142-158 in Proceedings 5th Annual Meetingof the Berkeley Linguistic Society, Berkeley,California (1979).Kowalski, R., Logic for Problem Solving, El-sevier North Holland, New York (1979).Lyfinen, S., "The Organization of Knowledgein a Multi-lingual Integrated Parser," YaleUniversity, New Haven, CT (1984).
PhDDissertation.\[Pereira80\]\[Reiter87\]\[Schank771\[Shieber87\]\[McCarthyS0)Pereira, F. C. N. and David H. D. Warren,"Definite Clause Grammars for LanguageAnalysis- A Survey of the Formalism and aComparison with Augmented Transition Net-works," Artificial Intelligence 13, pp.231-278(1980).Reiter, R. and J. deKlegr, "Foundations ofAssumption-Based Truth Maintenance Sys-tems: Preliminary Report," in Proceedings6th National Conference on Artificial Intelli ~gence, Seattle WA (1987).Sehank, R. and R. Abelsou, Scripts PlansGoals and Understanding, Lawrence ErlbanmAssociates, Hillsdale, New Jersey (1977).Shieber, S., An Introduction to Unification-Based Approaches, Univ.
of Chicago Press,Chicago, IL (1987).McCarthy, J., "Circumseription-A Form ofNon-Monotonic Reasoning," Artificial Intelli-gence 13 (1980).\[Sidner79\]\[Wilensky80\]\[Wilks75\]\[Zernik87\]\[Zernik88\]Sidner, C., "Towards a ComputationalTheory of Definite Anaphora Comprehensionin English Discourse," MIT, Cambridge,MA (1979).
PhD Dissertation.Wilensky, R. and Y. Arens, "PHRAN: AKnowledge-Based Approach to NaturalLanguage Analysis," in Proceedings 18thAnnual Meeting of the Asosciation for Com-putational Linguistics, Philadelphia, PA(1980).Wilks, Y., "Preference Semantics," in TheFormal Semantics of Natural La,guage, ed.E.
Keenan, Cambridge, Cambridge Britain(1975).Zemik, U., "Learning Phrases in a Hierar-chy," in Proceedings lOth International JointConference on Artificial Intelligence, MilanoItaly (August 1987).Zemik, U. and M. G. Dyer, "The Self-Extending Phrasal Lexicon," The Journal ofComputational Linguistics: Special Issue onthe Lexicon (1988).
to appear.805
