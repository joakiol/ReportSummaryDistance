Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 306?309,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsCascaded Lexicalised Classifiers for Second-Person Reference ResolutionMatthew PurverDepartment of Computer ScienceQueen Mary University of LondonLondon E1 4NS, UKmpurver@dcs.qmul.ac.ukRaquel Ferna?ndezILLCUniversity of Amsterdam1098 XH Amsterdam, Netherlandsraquel.fernandez@uva.nlMatthew Frampton and Stanley PetersCSLIStanford UniversityStanford, CA 94305, USAframpton,peters@csli.stanford.eduAbstractThis paper examines the resolution of thesecond person English pronoun you inmulti-party dialogue.
Following previouswork, we attempt to classify instances asgeneric or referential, and in the latter caseidentify the singular or plural addressee.We show that accuracy and robustness canbe improved by use of simple lexical fea-tures, capturing the intuition that differentuses and addressees are associated withdifferent vocabularies; and we show thatthere is an advantage to treating referen-tiality and addressee identification as sep-arate (but connected) problems.1 IntroductionResolving second-person references in dialogue isfar from trivial.
Firstly, there is the referentialityproblem: while we generally conceive of the wordyou1 as a deictic addressee-referring pronoun, itis often used in non-referential ways, including asa discourse marker (1) and with a generic sense(2).
Secondly, there is the reference problem: inaddressee-referring cases, we need to know whothe addressee is.
In two-person dialogue, this isnot so difficult; but in multi-party dialogue, the ad-dressee could in principle be any one of the otherparticipants (3), or any group of more than one (4):(1) It?s not just, you know, noises like somethinghitting.
(2) Often, you need to know specific buttonsequences to get certain functionalities done.
(3) I think it?s good.
You?ve done a good review.
(4) I don?t know if you guys have any questions.1We include your, yours, yourself, yourselves.This paper extends previous work (Gupta et al,2007; Frampton et al, 2009) in attempting to au-tomatically treat both problems: detecting refer-ential uses, and resolving their (addressee) refer-ence.
We find that accuracy can be improved bythe use of lexical features; we also give the firstresults for treating both problems simultaneously,and find that there is an advantage to treating themas separate (but connected) problems via cascadedclassifiers, rather than as a single joint problem.2 Related WorkGupta et al (2007) examined the referentialityproblem, distinguishing generic from referentialuses in multi-party dialogue; they found that 47%of uses were generic and achieved a classificationaccuracy of 75%, using various discourse featuresand discriminative classifiers (support vector ma-chines and conditional random fields).
They at-tempted the reference-resolution problem, usingonly discourse (non-visual) features, but accuracywas low (47%).Addressee identification in general (i.e.
in-dependent of the presence of you) has been ap-proached in various ways.
Traum (2004) givesa rule-based algorithm based on discourse struc-ture; van Turnhout et al (2005) used facial ori-entation as well as utterance features; and morerecently Jovanovic (2006; 2007) combined dis-course and gaze direction features using Bayesiannetworks, achieving 77% accuracy on a portion ofthe AMI Meeting Corpus (McCowan et al, 2005)of 4-person dialogues.In recent work, therefore, Frampton et al(2009) extended Gupta et al?s method to in-clude multi-modal features including gaze direc-tion, again using Bayesian networks on the AMIcorpus.
This gave a small improvement on the ref-306erentiality problem (achieving 79% accuracy), anda large improvement on the reference-resolutiontask (77% accuracy distinguishing singular usesfrom plural, and 80% resolving singular individ-ual addressee reference).However, they treated the two tasks in isola-tion, and also broke the addressee-reference prob-lem into two separate sub-tasks (singular vs. plu-ral reference, and singular addressee reference).
Afull computational you-resolution module wouldneed to treat all tasks (either simultaneously as onejoint classification problem, or as a cascaded se-quence) ?
with inaccuracy at one task necessar-ily affecting performance at another ?
and we ex-amine this here.
In addition, we examine the ef-fect of lexical features, following a similar insightto Katzenmaier et al (2004); they used languagemodelling to help distinguish between user- androbot-directed utterances, as people use differentlanguage for the two ?
we expect that the same istrue for human participants.3 MethodWe used Frampton et al (2009)?s AMI corpusdata: 948 ?you?-containing utterances, manu-ally annotated for referentiality and accompaniedby the AMI corpus?
original addressee annota-tion.
The very small number of two-person ad-dressee cases were joined with the three-person(i.e.
all non-speaker) cases to form a single ?plu-ral?
class.
49% of cases are generic; 32% ofreferential cases are plural, and the rest are ap-proximately evenly distributed between the singu-lar participants.
While Frampton et al (2009) la-belled singular reference by physical location rel-ative to the speaker (giving a 3-way classificationproblem), our lexical features are more suited todetecting actual participant identity ?
we there-fore recast the singular reference task as a 4-wayclassification problem and re-calculate their per-formance figures (giving very similar accuracies).Discourse Features We use Frampton et al(2009)?s discourse features.
These include sim-ple durational and lexical/phrasal features (includ-ing mention of participant names); AMI dialogueact features; and features expressing the simi-larity between the current utterance and previ-ous/following utterances by other participants.
Asdialogue act features are notoriously hard to tagautomatically, and ?forward-looking?
informationabout following utterances may be unavailable inan on-line system, we examine the effect of leav-ing these out below.Visual Features Again we used Frampton et al(2009)?s features, extracted from the AMI corpusmanual focus-of-attention annotations which trackhead orientiation and eye gaze.
Features includethe target of gaze (any participant or the meet-ing whiteboard/projector screen) during each ut-terance, and information about mutual gaze be-tween participants.
These features may also notalways be available (meeting rooms may not al-ways have cameras), so we investigate the effectof their absence below.Lexical Features The AMI Corpus simulates aset of scenario-driven business meetings, with par-ticipants performing a design task (the design ofa remote control).
Participants are given specificroles to play, for example that of project manager,designer or marketing expert.
It therefore seemspossible that utterances directed towards particularindividuals will involve the use of different vocab-ularies reflecting their expertise.
Different wordsor phrases may also be associated with genericand referential discussion, and extracting these au-tomatically may give benefits over attempting tocapture them using manually-defined features.
Toexploit this, we therefore added the use of lexicalfeatures: one feature for each distinct word or n-gram seen more than once in the corpus.
Althoughsuch features may be corpus- or domain-specific,they are easy to extract given a transcript.4 Results and Discussion4.1 Individual TasksWe first examine the effect of lexical features onthe individual tasks, using 10-way cross-validationand comparing performance with Frampton et al(2009).
Table 1 shows the results for the referen-tiality task in terms of overall accuracy and per-class F1-scores; ?MC Baseline?
is the majority-class baseline; results labelled ?EACL?
are Framp-ton et al (2009)?s figures, and are presented forall features and for reduced feature sets whichmight be more realistic in various situations: ?-V?removes visual features; ?-VFD?
removes visualfeatures, forward-looking discourse features anddialogue-act tag features.As can be seen, adding lexical features(?+words?
adds single word features, ?+3grams?adds n-gram features of lengths 1-3) improves the307Features Acc Fgen FrefMC Baseline 50.9 0 67.4EACL 79.0 80.2 77.7EACL -VFD 73.7 74.1 73.2+words 85.3 85.7 84.9+3grams 87.5 87.4 87.5+3grams -VFD 87.2 86.9 87.63grams only 85.9 85.2 86.4Table 1: Generic vs. referential usesFeatures Acc Fsing FplurMC Baseline 67.9 80.9 0EACL 77.1 83.3 63.2EACL -VFD 71.4 81.5 37.1+words 83.1 87.8 72.5+3grams 85.9 90.0 76.6+3grams -VFD 87.1 91.0 77.63grams only 86.9 90.8 77.0Table 2: Singular vs. plural reference.performance significantly ?
accuracy is improvedby 8.5% absolute above the best EACL results,which is a 40% reduction in error.
Robustness toremoval of potentially problematic features is alsoimproved: removing all visual, forward-lookingand dialogue act features makes little difference.In fact, using only lexical n-gram features, whilereducing accuracy by 2.6%, still performs betterthan the best EACL classifier.Table 2 shows the equivalent results for thesingular-plural reference distinction task; in thisexperiment, we used a correlation-based fea-ture selection method, following Frampton et al(2009).
Again, performance is improved, this timegiving a 8.8% absolute accuracy improvement, or38% error reduction; robustness to removing vi-sual and dialogue act features is also very good,even improving performance.For the individual reference task (again usingfeature selection), we give a further ?NS baseline?of taking the next speaker; note that this performsrather well, but requires forward-looking informa-tion so should not be compared to ?-F?
results.Results are again improved (Table 3), but the im-provement is smaller: a 1.4% absolute accuracyimprovement (7% error reduction); we concludefrom this that visual information is most impor-tant for this part of the task.
Robustness to featureunavailability still shows some improvement: ex-Features Acc FP1 FP2 FP3 FP4MC baseline 30.7 0 0 0 47.0NS baseline 70.7 71.6 71.1 72.7 68.2EACL 80.3 82.8 79.7 75.9 81.4EACL -V 73.8 79.2 70.7 74.1 71.4EACL -VFD 56.6 58.9 55.5 64.0 47.3+words 81.4 83.9 79.7 79.3 81.8+3grams 81.7 83.9 80.3 79.3 82.5+3grams -V 74.8 81.3 71.7 75.2 71.4+3grams -VFD 60.7 66.3 55.9 66.2 53.03grams only 60.7 63.1 58.1 52.9 63.43grams +NS 74.5 76.7 73.8 75.0 72.7Table 3: Singular addressee detection.cluding all visual, forward-looking and dialogue-act features has less effect than on the EACL sys-tem (60.7% vs. 56.6% accuracy), and a systemusing only n-grams and the next speaker identitygives a respectable 74.5%.Feature Analysis We examined the contribu-tion of particular lexical features using Informa-tion Gain methods.
For the referentiality task, wefound that generic uses of you were more likelyto appear in utterances containing words related tothe main meeting topic, such as button, channel,or volume (properties of the to-be-designed remotecontrol).
In contrast, words related to meetingmanagement, such as presentation, email, projectand meeting itself, were predictive of referentialuses.
The presence of first person pronouns anddiscourse and politeness markers such as okay,please and thank you was also indicative of refer-entiality, as were n-grams capturing interrogativestructures (e.g.
do you).For the plural/singular distinction, we foundthat the plural first person pronoun we correlatedwith plural references of you.
Other predictive n-grams for this task were you mean and you know,which were indicative of singular and plural refer-ences, respectively.
Finally, for the individual ref-erence task, useful lexical features included par-ticipant names, and items related to their roles.For instance, the n-grams sales, to sell and makemoney correlated with utterances addressed to the?marketing expert?, while utterances containingspeech recognition and technical were addressedto the ?industrial designer?.Discussion The best F-score of the three sub-tasks is for the generic/referential distinction; the308Features Acc Fgen Fplur FP1 FP2 FP3 FP4MC baseline 49.1 65.9 0 0 0 0 0EACL 58.3 73.3 24.3 57.6 57.0 36.0 51.1+3grams 60.9 74.8 42.0 57.7 52.2 35.6 50.23grams only 67.5 84.8 61.6 39.1 39.3 30.6 38.6Cascade +3grams 78.1 87.4 59.1 64.1 76.4 75.0 82.6Table 4: Combined task: generic vs. plural vs. singular addressee.worst is for the detection of plural reference (Fplurin Table 2).
This is not surprising: humans find theformer task easy to annotate ?
Gupta et al (2007)report good inter-annotator agreement (?
= 0.84)?
but the latter hard.
In their analysis of the AMIaddressee annotations, Reidsma et al (2008) ob-serve that most confusions amongst annotators arebetween the group-addressing label and the labelsfor individuals; whereas if annotators agree that anutterance is addressed to an individual, they alsoreach high agreement on that addressee?s identity.4.2 Combined TaskWe next combined the individual tasks into onecombined task; for each you instance, a 6-wayclassification as generic, group-referring or refer-ring to one of the 4 participants.
This was at-tempted both as a single classification exercise us-ing a single Bayesian network; and as a cascadedpipeline of the three individual tasks; see Table 4.Both used correlation-based feature selection.For the single joint classifier, n-grams again im-prove performance over the EACL features.
Usingonly n-grams gives a significant improvement, per-haps due to the reduction in the size of the featurespace on this larger problem.
Accuracy is reason-able (67.5%), but while F-scores are good for thegeneric class (above 80%), others are low.However, use of three cascaded classifiersimproves performance to 78% and gives largeper-class F-score improvements, exploitingthe higher accuracy of the first two stages(generic/referential, singular/plural), and the factthat different features are good for different tasks.5 ConclusionsWe have shown that the use of simple lexical fea-tures can improve performance and robustness forall aspects of second-person pronoun resolution:referentiality detection and reference identifica-tion.
An overall 6-way classifier is feasible, andcascading individual classifiers can help.
Futureplans include testing on ASR transcripts, and in-vestigating different classification techniques forthe joint task.ReferencesM.
Frampton, R. Ferna?ndez, P. Ehlen, M. Christoudias,T.
Darrell, and S. Peters.
2009. Who is ?you??
com-bining linguistic and gaze features to resolve second-person references in dialogue.
In Proceedings of the12th Conference of the EACL.S.
Gupta, J. Niekrasz, M. Purver, and D. Jurafsky.2007.
Resolving ?you?
in multi-party dialog.
InProceedings of the 8th SIGdial Workshop on Dis-course and Dialogue.N.
Jovanovic, R. op den Akker, and A. Nijholt.
2006.Addressee identification in face-to-face meetings.
InProceedings of the 11th Conference of the EACL.N.
Jovanovic.
2007.
To Whom It May Concern -Addressee Identification in Face-to-Face Meetings.Ph.D.
thesis, University of Twente, The Netherlands.M.
Katzenmaier, R. Stiefelhagen, and T. Schultz.
2004.Identifying the addressee in human-human-robot in-teractions based on head pose and speech.
In Pro-ceedings of the 6th International Conference onMultimodal Interfaces.I.
McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,V.
Karaiskos, M. Kronenthal, G. Lathoud, M. Lin-coln, A. Lisowska, W. Post, D. Reidsma, andP.
Wellner.
2005.
The AMI Meeting Corpus.
InProceedings of the 5th International Conference onMethods and Techniques in Behavioral Research.D.
Reidsma, D. Heylen, and R. op den Akker.
2008.On the contextual analysis of agreement scores.
InProceedings of the LREC Workshop on MultimodalCorpora.D.
Traum.
2004.
Issues in multi-party dialogues.
InF.
Dignum, editor, Advances in Agent Communica-tion, pages 201?211.
Springer-Verlag.K.
van Turnhout, J. Terken, I. Bakx, and B. Eggen.2005.
Identifying the intended addressee in mixedhuman-humand and human-computer interactionfrom non-verbal features.
In Proceedings of ICMI.309
