Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 58?62,Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational LinguisticsModernizing Historical Slovene Words with Character-Based SMTYves ScherrerALPAGEUniversit?
Paris 7 Diderot & INRIA5 Rue Thomas Mann, Paris, Franceyves.scherrer@inria.frToma?
ErjavecDept.
of Knowledge TechnologiesJo?ef Stefan InstituteJamova cesta 39, Ljubljana, Sloveniatomaz.erjavec@ijs.siAbstractWe propose a language-independent wordnormalization method exemplified onmodernizing historical Slovene words.Our method relies on character-based sta-tistical machine translation and uses onlyshallow knowledge.
We present the rel-evant lexicons and two experiments.
Inone, we use a lexicon of historical word?contemporary word pairs and a list of con-temporary words; in the other, we onlyuse a list of historical words and one ofcontemporary ones.
We show that bothmethods produce significantly better re-sults than the baseline.1 IntroductionA lot of recent work deals with detecting andmatching cognate words in corpora of closely re-lated language varieties.
This approach is also use-ful for processing historical language (Piotrowski,2012), where historical word forms are matchedagainst contemporary forms, thus normalizing thevaried and changing spelling of words over time.Such normalization has a number of applications:it enables better full-text search in cultural heritagedigital libraries, makes old texts more understand-able to today?s readers and significantly improvesfurther text processing by allowing PoS tagging,lemmatization and parsing models trained on con-temporary language to be used on historical texts.In this paper, we try to match word pairs of dif-ferent historical stages of the Slovene language.
Inone experiment we use character-based machinetranslation to learn the character correspondencesfrom pairs of words.
In the second experiment, westart by extracting noisy word pairs from monolin-gual1 lexicons; this experiment simulates a situa-1For lack of a better term, we use ?monolingual?
to referto a single diachronic state of the language, and ?bilingual?to refer to two diachronic states of the language.tion where bilingual data is not available.The rest of this paper is structured as follows:Section 2 presents related work, Section 3 detailsthe dataset used, Section 4 shows the experimentsand results, and Section 5 concludes.2 Related WorkThe most common approach to modernizing his-torical words uses (semi-) hand-constructed tran-scription rules, which are then applied to historicalwords, and the results filtered against a contempo-rary lexicon (Baron and Rayson, 2008; Scheible etal., 2010; Scheible et al 2011); such rules are of-ten encoded and used as (extended) finite state au-tomata (Reffle, 2011).
An alternative to such de-ductive approaches is the automatic induction ofmappings.
For example, Kestemont et al(2010)use machine learning to convert 12th century Mid-dle Dutch word forms to contemporary lemmas.Word modernization can be viewed as a specialcase of transforming cognate words from one lan-guage to a closely related one.
This task has tradi-tionally been performed with stochastic transduc-ers or HMMs trained on a set of cognate wordpairs (Mann and Yarowsky, 2001).
More re-cently, character-based statistical machine trans-lation (C-SMT) (Vilar et al 2007; Tiedemann,2009) has been proposed as an alternative ap-proach to translating words between closely re-lated languages and has been shown to outperformstochastic transducers on the task of name translit-eration (Tiedemann and Nabende, 2009).For the related task of matching cognate pairs inbilingual non-parallel corpora, various language-independent similarity measures have been pro-posed on the basis of string edit distance (Kon-drak and Dorr, 2004).
Cognate word matching hasbeen shown to facilitate the extraction of trans-lation lexicons from comparable corpora (Koehnand Knight, 2002; Kondrak et al 2003; Fi?er andLjube?ic?, 2011).58For using SMT for modernizing historicalwords, the only work so far is, to the best of ourknowledge, S?nchez-Mart?nez et al(2013).3 The DatasetIn this section we detail the dataset that was usedin the subsequent experiments, which consistsof a frequency lexicon of contemporary Sloveneand training and testing lexicons of historicalSlovene.23.1 The Lexicon of Contemporary SloveneSloleks is a large inflectional lexicon of contem-porary Slovene.3 The lexicon contains lemmaswith their full inflectional paradigms and withthe word forms annotated with frequency of oc-currence in a large reference corpus of Slovene.For the purposes of this experiment, we extractedfrom Sloleks the list of its lower-cased word forms(930,000) together with their frequency.3.2 Corpora of Historical SloveneThe lexicons used in the experiments are con-structed from two corpora of historical Slovene.4The texts in the corpora are, inter alia marked upwith the year of publication and their IANA lan-guage subtag (sl for contemporary Slovene al-phabet and sl-bohoric for the old, pre-1850Bohoric?
alphabet).
The word tokens are anno-tated with the attributes nform, mform, lemma, tag,gloss, where only the first two are used in the pre-sented experiments.The nform attribute contains the result of a sim-ple normalization step, consisting of lower-casing,removal of vowel diacritics (which are not used incontemporary Slovene), and conversion of the Bo-horic?
alphabet to the contemporary one.
Thus, wedo not rely on the C-SMT model presented belowto perform these pervasive, yet deterministic andfairly trivial transformations.The modernized form of the word, mform is theword as it is (or would be, for extinct words) writ-ten today: the task of the experiments is to predictthe correct mform given an nform.2The dataset used in this paper is available under theCC-BY-NC-SA license from http://nl.ijs.si/imp/experiments/bsnlp-2013/.3Sloleks is encoded in LMF and available under the CC-BY-NC-SA license from http://www.slovenscina.eu/.4The data for historical Slovene comes from the IMP re-sources, see http://nl.ijs.si/imp/.Period Texts Words Verified18B 8 21,129 21,12919A 9 83,270 83,27019B 59 146,100 146,100?
75 250,499 250,499Table 1: Size of goo300k corpus.Period Texts Words Verified18B 11 139,649 15,46619A 13 457,291 17,61619B 270 2,273,959 65,769?
293 2,870,899 98,851Table 2: Size of foo3M corpus.The two corpora were constructed by samplingindividual pages from a collection of books andeditions of one newspaper, where the pages (butnot necessarily the publications) of the two cor-pora are disjoint:5?
goo300k is the smaller, but fully manuallyannotated corpus, in which the annotations ofeach word have been verified;6?
foo3M is the larger, and only partially manu-ally annotated corpus, in which only the morefrequent word forms that do not already ap-pear in goo300k have verified annotations.The texts have been marked up with the timeperiod in which they were published, e.g., 18Bmeaning the second half of the 18th century.
Thisallows us to observe the changes to the vocabularyin 50-year time slices.
The sizes of the corpora aregiven in Table 1 and Table 2.3.3 Lexicons of Historical SloveneFrom the two corpora we have extracted thetraining and testing lexicons, keeping only words(e.g., discarding digits) that have been manuallyverified.
The training lexicon, Lgoo is derivedfrom the goo300k corpus, while the test lexicon,Lfoo is derived from the foo3M corpus and, as5The corpora used in our experiments are slightly smallerthan the originals: the text from two books and one newspa-per issue has been removed, as the former contain highly id-iosyncratic ways of spelling words, not seen elsewhere, andthe latter contains a mixture of the Bohoric?
and contempo-rary alphabet, causing problems for word form normaliza-tion.
The texts older than 1750 have also been removed fromgoo300k, as such texts do not occur in foo3M, which is usedfor testing our approach.6A previous version of this corpus is described in (Er-javec, 2012).59Period Pairs Ident Diff OOV18B 6,305 2,635 3,670 70319A 18,733 12,223 6,510 2,11719B 30,874 24,597 6,277 4,759?
45,810 31,160 14,650 7,369Table 3: Size of Lgoo lexicon.Period OOV Pairs Ident Diff18B 660 3,199 493 2,70619A 886 3,638 1,708 1,93019B 1,983 10,033 8,281 1,752?
3,480 16,029 9,834 6,195Table 4: Size of Lfoo lexicon.mentioned, contains no ?nform, mform?
pairs al-ready appearing in Lgoo.
This setting simulatesthe task of an existing system receiving a new textto modernize.The lexicons used in the experiment contain en-tries with nform, mform, and the per-slice frequen-cies of the pair in the corpus from which the lexi-con was derived, as illustrated in the example be-low:benetkah benetkah 19A:1 19B:1aposteljnov apostolov 19A:1 19B:1ar?ati ar?etu* 18B:2The first example is a word that has not changedits spelling (and was observed twice in the 19thcentury texts), while the second and third havechanged their spelling.
The asterisk on the thirdexample indicates that the mform is not present inSloleks.
We exclude such pairs from the test lexi-con (but not from the training lexicon) since theywill most likely not be correctly modernized byour model, which relies on Sloleks.
The sizes ofthe two lexicons are given in Table 3 and Table 4.For Lgoo we give the number of pairs including theOOV words, while for Lfoo we exclude them; thetables also show the numbers of pairs with iden-tical and different words.
Note that the summaryrow has smaller numbers than the sum of the in-dividual rows, as different slices can contain thesame pairs.4 Experiments and ResultsWe conducted two experiments with the data de-scribed above.
In both cases, the goal is to cre-ate C-SMT models for automatically modernizinghistorical Slovene words.
In each experiment, wecreate three different models for the three time pe-riods of old Slovene (18B, 19A, 19B).The first experiment follows a supervised setup:we train a C-SMT model on ?historical word,contemporary word?
pairs from Lgoo and test themodel on the word pairs of Lfoo.
The second ex-periment is unsupervised and relies on monolin-gual data only: we match the old Slovene wordsfrom Lgoo with modern Slovene word candidatesfrom Sloleks; this noisy list of word pairs thenserves to train the C-SMT model.
We test againon Lfoo.4.1 Supervised LearningSMT models consist of two main components: thetranslation model, which is trained on bilingualdata, and the language model, which is trainedon monolingual data of the target language.
Weuse the word pairs from Lgoo to train the transla-tion model, and the modern Slovene words fromLgoo to train the language model.7 As said above,we test the model on the word pairs of Lfoo.The experiments have been carried out with thetools of the standard SMT pipeline: GIZA++ (Ochand Ney, 2003) for alignment, Moses (Koehn etal., 2007) for phrase extraction and decoding, andIRSTLM (Federico et al 2008) for language mod-elling.
After preliminary experimentation, we set-tled on the following parameter settings:?
We have obtained the best results with a 5-gram language model.
The beginning andthe end of each word were marked by specialsymbols.?
The alignments produced by GIZA++ arecombined with the grow-diag-final method.?
We chose to disable distortion, which ac-counts for the possibility of swapping ele-ments; there is not much evidence of this phe-nomenon in the evolution of Slovene.?
We use Good Turing discounting to adjust theweights of rare alignments.?
We set 20% of Lgoo aside for Minimum ErrorRate Training.The candidates proposed by the C-SMT sys-tem are not necessarily existing modern Slovenewords.
Following Vilar et al(2007), we added a7It is customary to use a larger dataset for the languagemodel than for the translation model.
However, adding theSloleks data to the language model did not improve perfor-mances.60Supervised UnsupervisedPeriod Total Baseline No lex filter With lex filter No lex filter With lex filter18B 3199 493 (15.4%) 2024 (63.3%) 2316 (72.4%) 1289 (40.3%) 1563 (48.9%)19A 3638 1708 (46.9%) 2611 (71.8%) 2941 (80.0%) 2327 (64.0%) 2644 (72.7%)19B 10033 8281 (82.5%) 8707 (86.8%) 9298 (92.7%) 8384 (83.6%) 8766 (87.4%)Table 5: Results of the supervised and the unsupervised experiments on Lfoo.lexicon filter, which selects the first candidate pro-posed by the C-SMT that also occurs in Sloleks.8The results of these experiments, with and with-out lexicon filter, are shown in Table 5.
As a base-line, we consider the words that are identical inboth language varieties.
Without lexicon filter, weobtain significant improvements over the baselinefor the first two time spans, but as the language va-rieties become closer and the proportion of identi-cal words increases, the SMT model becomes lessefficient.
In contrast to Vilar et al(2007), we havefound the lexicon filter to be very useful: it im-proves the results by nearly 10% absolute in 18Band 19A, and by 5% in 19B.4.2 Unsupervised LearningThe supervised approach requires a bilingualtraining lexicon which associates old words withmodern words.
Such lexicons may not be availablefor a given language variety.
In the second exper-iment we investigate what can be achieved withpurely monolingual data.
Concretely, we proposea bootstrapping step to collect potential cognatepairs from two monolingual word lists (the histor-ical words of Lgoo, and Sloleks).
We then train theC-SMT system on these hypothesized pairs.The bootstrapping step consists of searching,for each historical word of Lgoo, its most similarmodern words in Sloleks.9 The similarity betweentwo words is computed with the BI-SIM measure(Kondrak and Dorr, 2004).
BI-SIM is a measureof graphemic similarity which uses character bi-grams as basic units.
It does not allow crossingalignments, and it is normalized by the length ofthe longer string.
As a result, this measure cap-tures a certain degree of context sensitivity, avoids8In practice, we generated 50-best candidate lists withMoses, and applied the lexicon filter on that lists.
In casenone of the 50 candidates occurs in Sloleks, the filter returnsthe candidate with the best Moses score.9In order to speed up the process and remove some noise,we excluded hapaxes from Lgoo and all but the 20,000 mostfrequent words from Sloleks.
We also excluded words thatcontain less than four characters from both corpora, since thesimilarity measures proved unreliable on them.counterintuitive alignments and favours associa-tions between words of similar lengths.
BI-SIMis a language-independent measure and thereforewell-suited for this bootstrapping step.For each old Slovene word, we keep the corre-spondences that maximize the BI-SIM value, butonly if this value is greater than 0.8.10 For the18B slice, this means that 812 out of 1333 histori-cal words (60.9%) have been matched with at leastone modern word; 565 of the matches (69.6%, or42.4% of the total) were correct.These word correspondences are then used totrain a C-SMT model, analogously to the super-vised approach.
As for the language model, it istrained on Sloleks, since the modernized formsof Lgoo are not supposed to be known.
Due tothe smaller training set size, MERT yielded un-satisfactory results; we used the default weights ofMoses instead.
The other settings are the same asreported in Section 4.1.
Again, we conducted ex-periments for the three time slices.
We tested thesystem on the word pairs of the Lfoo lexicon, asabove.
Results are shown in Table 5.While the unsupervised approach performs sig-nificantly less well on the 18B period, the differ-ences gradually diminish for the subsequent timeslices; the model always performs better than thebaseline.
Again, the lexicon filter proves useful inall cases.5 ConclusionWe have successfully applied the C-SMT ap-proach to modernize historical words, obtainingup to 57.0% (absolute) accuracy improvementswith the supervised approach and up to 33.5% (ab-solute) with the unsupervised approach.
In the fu-ture, we plan to extend our model to modernizeentire texts in order to take into account possibletokenization changes.10This threshold has been chosen empirically on the basisof earlier experiments, and allows us to eliminate correspon-dences that are likely to be wrong.
If several modern wordscorrespond to the same old word, we keep all of them.61AcknowledgementsThe authors thank the anonymous reviewers fortheir comments ?
all errors, of course, remainour own.
This work has been partially fundedby the LabEx EFL (ANR/CGI), operation LR2.2,by the EU IMPACT project ?Improving Access toText?
and the Google Digital Humanities ResearchAward ?Language models for historical Slove-nian?.ReferencesAlistair Baron and Paul Rayson.
2008.
VARD 2: Atool for dealing with spelling variation in historicalcorpora.
In Proceedings of the Postgraduate Confer-ence in Corpus Linguistics, Birmingham, UK.
AstonUniversity.Toma?
Erjavec.
2012.
The goo300k corpus of his-torical Slovene.
In Proceedings of the Eighth In-ternational Conference on Language Resources andEvaluation, LREC?12, Paris.
ELRA.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: an open source toolkit forhandling large scale language models.
In Proceed-ings of Interspeech 2008, Brisbane.Darja Fi?er and Nikola Ljube?ic?.
2011.
Bilingual lexi-con extraction from comparable corpora for closelyrelated languages.
In Proceedings of the Interna-tional Conference on Recent Advances in NaturalLanguage Processing (RANLP?11), pages 125?131.Mike Kestemont, Walter Daelemans, and Guy DePauw.
2010.
Weigh your words ?
memory-basedlemmatization for Middle Dutch.
Literary and Lin-guistic Computing, 25:287?301.Philipp Koehn and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
InProceedings of the ACL 2002 Workshop on Unsu-pervised Lexical Acquisition (SIGLEX 2002), pages9?16, Philadelphia.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the As-sociation for Computational Linguistics (ACL?07),demonstration session, Prague.Grzegorz Kondrak and Bonnie Dorr.
2004.
Identifi-cation of confusable drug names: A new approachand evaluation methodology.
In In Proceedings ofCOLING 2004, pages 952?958.Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.2003.
Cognates can improve statistical translationmodels.
In Proceedings of NAACL-HLT 2003.Gideon S. Mann and David Yarowsky.
2001.
Mul-tipath translation lexicon induction via bridge lan-guages.
In Proceedings of the Second Meetingof the North American Chapter of the Associationfor Computational Linguistics (NAACL 2001), pages151?158, Pittsburgh.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Michael Piotrowski.
2012.
Natural Language Pro-cessing for Historical Texts.
Synthesis Lectures onHuman Language Technologies.
Morgan & Clay-pool.Ulrich Reffle.
2011.
Efficiently generating correc-tion suggestions for garbled tokens of historical lan-guage.
Natural Language Engineering, 17:265?282.Silke Scheible, Richard J. Whitt, Martin Durrell, andPaul Bennett.
2010.
Annotating a Historical Corpusof German: A Case Study.
In Proceedings of theLREC 2010 Workshop on Language Resources andLanguage Technology Standards, Paris.
ELRA.Silke Scheible, Richard J. Whitt, Martin Durrell, andPaul Bennett.
2011.
A Gold Standard Corpus ofEarly Modern German.
In Proceedings of the 5thLinguistic Annotation Workshop, pages 124?128,Portland, Oregon, USA, June.
Association for Com-putational Linguistics.Felipe S?nchez-Mart?nez, Isabel Mart?nez-Sempere,Xavier Ivars-Ribes, and Rafael C. Carrasco.
2013.An open diachronic corpus of historical Span-ish: annotation criteria and automatic modernisa-tion of spelling.
Research report, Departamentde Llenguatges i Sistemes Inform?tics, Universi-tat d?Alacant, Alicante.
http://arxiv.org/abs/1306.3692.J?rg Tiedemann and Peter Nabende.
2009.
Translatingtransliterations.
International Journal of Computingand ICT Research, 3(1):33?41.
Special Issue of Se-lected Papers from the fifth international conferenceon computing and ICT Research (ICCIR 09), Kam-pala, Uganda.J?rg Tiedemann.
2009.
Character-based PSMT forclosely related languages.
In Proceedings of the13th Conference of the European Association forMachine Translation (EAMT 2009), pages 12 ?
19,Barcelona.David Vilar, Jan-Thorsten Peter, and Hermann Ney.2007.
Can we translate letters?
In Proceedings ofthe Second Workshop on Statistical Machine Trans-lation, pages 33?39, Prague.62
