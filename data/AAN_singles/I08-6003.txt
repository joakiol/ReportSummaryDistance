Finding parallel texts on the web usingcross-language information retrievalAchim RuoppUniversity of Washington,Seattle, WA 98195, USAachimr@u.washington.eduFei XiaUniversity of WashingtonSeattle, WA 98195, USAfxia@u.washington.eduAbstractDiscovering parallel corpora on the web isa challenging task.
In this paper, we usecross-language information retrieval tech-niques in combination with structural fea-tures to retrieve candidate page pairs froma commercial search engine.
The candidatepage pairs are then filtered using tech-niques described by Resnik and Smith(2003) to determine if they are translations.The results allow the comparison of effi-ciency of different parameter settings andprovide an estimate for the percentage ofpages that are parallel for a certain lan-guage pair.1 IntroductionParallel corpora are invaluable resources in manyareas of natural language processing (NLP).
Theyare used in multilingual NLP as a basis for the cre-ation of translation models (Brown et.
al., 1990),lexical acquisition (Gale and Church, 1991) as wellas for cross-language information retrieval (Chenand Nie, 2000).
Parallel corpora can also benefitmonolingual NLP via the induction of monolingualanalysis tools for new languages or the improve-ment of tools for languages where tools alreadyexist (Hwa et.
al., 2005; Pad?
and Lapata, 2005;Yarowsky and Ngai, 2001).For most of the mentioned work, large parallelcorpora are required.
Often these corpora have li-mited availability due to licensing restrictions(Tiedemann and Nygaard, 2004) and/or are domainspecific (Koehn, 2005).
Also parallel corpora areonly available for a limited set of language pairs.As a result, researchers look to the World WideWeb as a source for parallel corpora (Resnik andSmith, 2003; Ma and Liberman, 1999; Chen andNie, 2000).
Because of the web?s world-wide reachand audience, many websites are bilingual, if notmultilingual.
The web is therefore a prime candi-date as a source for such corpora especially forlanguage pairs including resource-poor languages.Resnik and Smith (2003) outlined the followingthree steps for identifying parallel text on the web:(1) Locating pages that might have paralleltranslations(2) Generating candidate page pairs that mightbe translations(3) Structural filtering out of non-translationcandidate pairsIn most of the previous work, Step (1) is performedin an ad-hoc manner using structural features thatwere observed in a limited set of samples of paral-lel pages.
For example a language name in anHTML link is considered a strong indication thatthe page is also available translated to the languageindicated by the link.
The reason for this ad-hocapproach is that there aren?t any standards as tohow web developers structure multilingual webpages on a server.
Often developers use languagenames or identifiers in uniform resource locators(URLs) to distinguish different language versionsof a page on a server.When Step (1) is performed using a commercialsearch engine, another obstacle to finding candi-dates for parallel pages comes into play: the resultsare always relevance-ranked for the end user.
Inthis paper, instead of searching exclusively forstructural features of parallel pages, we are addinga dictionary-based sampling technique, based oncross-language information retrieval for Step (1).We compare the URL results from each of our ex-periments with three different matching methodsfor Step (2).
Finally, for Step (3), we adapted afiltering method from Resnik and Smith (2003) todetermine whether or not a page pair is a truetranslation pair.To estimate the percentage of parallel pages thatare available for a certain language pair in relationto the total number of pages available in each ofthe two languages, we modified a technique thatBharat and Broder (1998) used to estimate over-laps of search engine indices.We conducted our experiments on the English-German pair, but the described techniques arelargely language-independent.
The results of theexperiments in this paper would allow researchersto choose the most efficient technique when tryingto build parallel corpora from the web and guideresearch into further optimizing the retrieval ofparallel texts from the web.2 MethodologyThe first step in finding parallel text on the webhas two parts.
The first part, the sampling proce-dure, retrieves a set S1 of pages in the source lan-guage L1 by sending sampling queries to the searchengine.
These sampling queries are structured insuch a way that they retrieve pages that are likelyto have translations.
The second part, a checkingprocedure,  retrieves a set S2 of pages in target lan-guage L2 that are likely to contain the translationsof pages in S1.
The two procedures are described inSections 2.1 and 2.2, respectively.Step (2) matches up elements of S1 and S2 togenerate a set of candidates for page pairs thatcould be translations of each other.
This is ex-plained in Section 2.3.Step (3), a final filtering step, uses features ofthe pages to eliminate page pairs that are not trans-lations of each other.
The detail of the step is de-scribed in Sections 2.4.
Figure 1 illustrates the dif-ferent sets of pages and page pairs created by thethree steps.2.1 Sampling2.1 SamplingFor the baseline case the sampling should selectpages randomly from the search space.
To get arandom sample of pages from a search engine thatwe can check for translational equivalents inanother language, we select terms at random froma bilingual dictionary.Instead of using a manually crafted bilingualdictionary, we chose to use a translation lexiconautomatically created from parallel data, becausethe translation probabilities are useful for our expe-riments.
In this study, the translation lexicon wascreated by aligning part of the German-Englishportion of the Europarl corpus (Koehn, 2005) usingthe Giza++ package (Och and Ney, 2003).The drawback of using this translation lexicon isthat the lexicon is domain-specific to parliamentaryproceedings.
We alleviated this domain-specificityby selecting mainly terms with medium frequencyin the lexicon.We sorted the terms by frequency.
According toZipf?s law (Zipf, 1949), the frequency of the termsis roughly inversely proportional to their rank inthis list.
We choose terms according to a normaldistribution whose mean is the midpoint of allranks.
We tuned the deviation to ?
of the mean, soas to avoid getting very frequent terms into thesample which would just return a large set of unre-lated pages, as well as very infrequent terms whichwould return few or no results.A single word selected with this normal distribu-tion, together with the lang: parameter set tolanguage L1, is submitted to the search engine toSamplingLanguage L1CheckingLanguage L2MatchFilterWebFigure 1.
Pages and page pairs involved in thethree steps of the algorithmretrieve a sample (in our experiments 100 pages).The search engine automatically performs stem-ming on the term.2.1.1 Source Language ExpansionTo obtain a sample yielding more translation can-didates, it is valuable to use semantically relatedmulti-word queries for the sampling procedure.To obtain semantically related words, we usedthe standard information retrieval (IR) technique ofquery expansion.
Part of the sampling result ofsingle-word queries are summaries, delivered backby the search engine.
To come up with a rankedlist of terms that are semantically related to theoriginal one-word term, we extract and count allunigrams from a concatenation of the summaries.Stopwords are ignored.
After the count, the uni-grams are ranked by frequency.For an n-term query, the original single-wordquery is combined with the first (n-1) terms of thisranked list to form an expanded query that is sub-mitted to the search engine.The advantage of this form of expansion is thatit is largely language independent and often leadsto highly relevant terms, due to the ranking algo-rithms employed by the search engines.2.1.2 Language Identifiers in URLsOnce the baseline is established with single andmulti-word sampling queries, an additional struc-tural inurl: search parameter, which allows que-rying for substrings in URLs, can be added to in-crease the likelihood of finding pages that do havetranslations.For this paper we limited our experiments to usestandard (RFC 3066) two-letter language identifi-ers for this search parameter: ?en?
for English and?de?
for German.2.2 CheckingThe purpose of the checking procedure is to gener-ate a set of web pages in language L2 that are po-tentially translations of pages in the sample ob-tained in the previous section.2.2.1 Translating the Sampling QueryThe natural way to do this is to translate the sam-pling query from language L1 into the target lan-guage L2.
The sampling query does not necessarilyhave a unique one-to-one translation in languageL2.
This is where the translation lexicon createdfrom the Europarl corpus comes in.
Because thelexicon contains probabilities, we can obtain them-best translations for a single term from the sam-pling query.Given a query in L1 with n terms and each termhas up to m translations, the checking procedurewill form up to mn queries in L2 and sends each ofthem to the search engine.
Because most currentcommercial search engines set a limit on the max-imum number of queries allowed per day, longersampling queries (i.e., larger n) mean that feweroverall samples can be retrieved per day.
The ef-fect of this trade-off on the number of parallel pagepairs is evaluated in our experiments.Source language expansion can lead to sampleterms that are not part of the translation lexicon.These are removed during translation.If the inurl: search parameter was used in thesampling query, the corresponding inurl: para-meter for language L2 will be used in the checkingquery.2.2.2 Target Language ExpansionAn alternative to translating all terms in an ex-panded, multi-word sampling query (see Section2.1.1) is to translate only the original single sam-pling word to obtain top m translations in L2, andthen for each translation do a term expansion onthe target language side with (n-1) expansionterms.
The benefit of target language expansion isthat it only requires m checking queries, wheresource language expansion requires mn checkingqueries.
The performance of this different ap-proach will be evaluated in Section 3.2.2.3 Site ParameterAnother structural search parameter appropriate forchecking is the site: parameter, which manysearch engines provide.
It allows limiting the queryresults to a set of pre-defined sites.
In our experi-ments we use the sites of the top-30 results of thesampling set, which is the maximum allowed bythe Yahoo!
search engine.2.3 Matching MethodsTo obtain page pairs that might be translations ofeach other, pages in sampling set S1 are matchedup based on URL similarity with pages in corres-ponding checking set S2.
We experimented withthree methods.2.3.1 Fixed Language ListIn the fixed language list matching method, URLsdiffering only in the language names and languageidentifiers (as listed in Table 1) are considered amatch and added to the set of page pair candidates.en deen-us de-deen geenu deuenu gerenglish germanenglisch deutschTable 1.
Language identifiers and language namesfor Fixed Language List and URL Part SubstitutionAn example for a match in this category ishttp://ec.europa.eu/education/policies/rec_qual/recognition/diploma_en.html andhttp://ec.europa.eu/education/policies/rec_qual/recognition/diploma_de.html.2.3.2 Levenshtein DistanceIn the Levenshtein distance matching method, ifthe Levenshtein distance (also known as edit dis-tance) between a pair of URLs from S1 and S2 islarger than zero1 and is below a threshold, the URLpair is considered a match.
In our experiments, weset the threshold to four, because for most standard(RFC 3066) language identifiers the maximumLevenshtein distance would be four (e.g.
?en-US?vs.
?de-DE?
as part of a URL).2.3.3 URL Part SubstitutionThe third method that we tried does not requirequerying a search engine for a checking set.
In-stead, each URL U1 in the sampling set S1 is parsedto determine if it contains a language name oridentifier at a word boundary.
If so, the languagename or identifier is substituted with the corres-ponding language name or identifier for the targetlanguage to form a target language URL U2 ac-cording to the substitutions listed in Table 1.For each resulting U2, an HTTP HEAD requestis issued to verify whether the page with that URL1 We don?t want to match identical URLs.exists on the server.
If the request is successful, thepair (U1,U2) is added to the set of page pair candi-dates.
If multiple substitutions are possible for a U1all the resulting U2 will be tested.2.4 Page FilteringThe goal of this step is to filter out all the pagepairs that are not true translations.2.4.1 Structural FilteringOne method for filtering is a purely structural,language-independent method described in Resnikand Smith (2003).
In this method, the HTML struc-ture in each page is linearized and the resultingsequences are aligned to determine the structuraldifferences between the two files.
Their paper dis-cussed four scalar values that can be calculatedfrom the alignment.
We used two of the values inour experiments, as described below.The first one is called the difference percentage(dp), which indicates the percentage of nonsharedmaterial in the page pair.
Given the two linearizedsequences for a page pair (p1, p2), we used  Eq (1)to calculate dp, where length1 is the length of thefirst sequence, and diff 1  is the number of lines inthe first sequence that do not align to anything inthe second sequence; length2  and diff 2 are definedsimilarly.212121 ),( lengthlengthdiffdiffppdp ???
(1)The second value measures the correlation be-tween the lengths of aligned nonmarkup chunks.The idea is that the lengths of corresponding trans-lated sentences or paragraphs usually correlate.The longer a sentence in one language is, the long-er its translation in another language should be.
Forthe sake of simplicity, we assume there is a linearcorrelation between the lengths of the two files,and use the Pearson correlation coefficient as alength correlation metric.
From the two linearizedsequences, the lengths of nonmarkup chunks arerecorded into two arrays.
The Pearson correlationcoefficient can be directly calculated on these twoarrays.This metric is denoted as r(p1,p2), and its valueis in the range of [-1,1]: 1 indicates a perfect posi-tive linear relationship, 0 indicates there is no li-near relationship, and -1 indicates a perfect nega-tive linear relationship between the chunk lengthsin the two files.2.4.2 Content Translation MetricAs shown in Resnik and Smith (2003), the struc-tural filtering to judge whether pages are transla-tions of each other leads to very good precisionand satisfactory recall.However, when using the URL part substitutionmethod described in 2.3, many web sites, if theyreceive a request for a URL that does not exist,respond by returning the most likely page forwhich there is an existing URL on the server.
Thisis often the page content of the original URL be-fore substitution.
Identical pages in the candidatepage pair2 would be judged as translations by thepurely structural method and precision would benegatively impacted.
There are several solutionsfor this, one of them is to use a content-based me-tric to complement the structural metric.Ma and Liberman (1999) define the followingsimilarity metric between two pages in a page pair(p1, p2):121 ),( pinTokensOfNumPairsTokennTranslatioOfNumppc ?
(2)To calculate this content-based metric, the trans-lation lexicon created in Step (1) comes in handy.For the first 500 words of each page in the pagepair candidate, we calculate the similarity metric inEq (2), using the top two translations of the wordsin the translation lexicon.2.4.3 Linear CombinationWe combine the structural metrics (dp and r) andthe content-based metric c by linear combination:33),(*),(*)),(1(*),( 21212121 ppcapprappdpappt crdpdprc ????
(3)2 The two identical pages could have different URLs.3 We use 1-dp(p1,p2) to turn a dissimilarity measure intoa similarity measure.If tdprc is larger than a predefined threshold, thepage pair is judged to be a translation.2.5 Estimating the Percentage of ParallelPages for a Language PairStatistics on what share of web pages in one lan-guage have translated equivalents in another lan-guage are, to our knowledge, not available.
Obtain-ing these statistics is useful from a web metricsperspective.
The statistics allow the calculation ofrelative language web page counts and serve as abaseline to evaluate methods that try to find paral-lel pages.Fortunately there is a statistical method (Bharatand Broder, 1998) that can be adapted to obtainthese numbers.
Bharat and Broder introduce a me-thod to estimate overlaps in the coverage of a pairof search indices and to calculate the relative sizeratio of search indices.
They achieve this by ran-domly sampling pages in one index and then checkwhether the pages are also present in the other in-dex.Instead of calculating the overlap of pages intwo search engines, we adapted the method tomeasure the overlap of languages in one searchengine.
Let P(E) represent the probability that aweb page belongs to a set E. Let P(FE|E) representthe conditional probability that there exist transla-tional equivalents F of E given E. Then?
?
?
??
?ESizeFSizeEFP EE ?|(4)?
?
?
??
?FSizeESizeFEP FF ?|(5)Size(FE) and Size(E) can be determined with anexperiment using one term samples and checkingwith a site: parameter.
Size (FE) equals thenumber of page pairs that are determined to betranslations by the filtering step.
Size(E) is thenumber of checked sites per sample (30 in the caseof Yahoo!)
times the number of samples.
Size(EF)and Size(F) are calculated similarly.3 ExperimentsTo evaluate the effectiveness of various methodsdescribed in Section 2, we ran a range of experi-ments and the results are shown in Table 2.The first column is the experiment ID; the secondcolumn indicates whether source or target expan-sion is used in Step (1); the third column shows thelength of the queries after the expansion (if any).For instance, in Experiment #3, source expansionis applied, and after the expansion, the new queriesalways have three words: one is the original queryterm, and the other two are terms that are most re-levant to the original query term according to thedocuments retrieved by the search engine (see Sec-tion 2.1.2).The fourth and fifth columns indicate whether theinurl: and site: parameters are used duringthe search.
A blank cell means that the search pa-rameter is not used in that experiment.
For eachquery, the search engine returns the top 100 docu-ments.The next three columns show the numbers ofpage pairs produced by each of the three matchingmethods describe in Section 2.3.
The last threecolumns show the numbers of page pairs after thefiltering step.
Here, we used the linear combination(see Section 2.4.3).
All the numbers are the sum ofthe results from 100 sampling queries.
Let us ex-amine the experimental results in detail.3.1 Sampling and CheckingThe evaluation of the sampling and checking pro-cedures are difficult, because the number of trans-lation page pairs existing on the web is unknown.In this study, we evaluated the module indirectlyby looking at the translation pairs found by the fol-lowing steps: the matching step and the filteringstep.A few observations are worth noting.
First, queryexpansion increases the number of page pairscreated in Steps (2) and (3), and source and targetquery expansion lead to similar results.
However,the difference between n=2 and n=3 is not signifi-cant.
One possible explanation is that the semanticdivergence between queries on the source side andon the target side could become more problematicfor longer queries.Second, using the site: and inurl: search pa-rameters (described in 2.1.2 and 2.2.3) increasesthe number of discovered page pairs.
The potentiallimitation is that inurl: narrows the set of dis-coverable pages to the ones that contain languageidentifiers in the URL.Expe-rimentIDExpan-siontypeQuerylength(n)inurl:Paramsite:ParamNumber of page pairs(before filtering)Number of page pairs(after filtering)List Leven-shteinSub-stitutionList Leven-shteinSub-stitution1 none 1   5 13 1108 1 1 972 Source 2   8 28 1889 3 4 1573 Source 3   10 42 1975 1 10 1244 none 1 en/de  58 84 5083 17 22 2855 Source 2 en/de  72 132 9279 27 31 4336 Source 3 en/de  100 160 9200 25 31 3477 none 1   6 18 1099 1 3 928 Target 2   4 24 1771 2 3 1439 Target 3   4 12 1761 0 0 14910 none 1 en/de  56 93 5041 24 34 28111 Target 2 en/de  107 161 9131 27 33 42612 Target 3 en/de  45 72 8395 12 15 33513 none 1  30 10 258 n/a 6 9 n/a14 Source 2  30 22 743 n/a 9 32 n/a15 Source 3  30 46 1074 n/a 12 41 n/a16 none 1 en/de 30 59 164 n/a 13 15 n/a17 Source 2 en/de 30 118 442 n/a 28 50 n/a18 Source 3 en/de 30 171 693 n/a 46 49 n/aTable 2.
Experiment configurations and results3.2 Matching MethodsTable 2 shows that among the three matching me-thods, the URL part substitution method leads tomany more translation page pairs than the othertwo methods.Notice that although the fixed language list me-thod uses the same language name pair table (i.e.,Table 1) as the URL part substitution method, itworks much worse than the latter.
This is largelydue to the different rankings of documents in dif-ferent languages.
For instance, suppose a page p1 inL1 is retrieved by a sampling query q1, and p1 has atranslation page p2 in L2 , it is possible that p2 willnot be retrieved by the query q2, a query made upof the translation of the terms in q1 .4Another observation is that the Levenshtein dis-tance matching method outperforms the fixed lan-guage list method.
In addition, it has a unique ad-vantage: the results allow the automatic learning oflanguage identifiers that web developers use inURLs to distinguish parallel pages for certain lan-guage pairs.3.3 Parameter Tuning for Linear Combina-tion of Filtering MetricsBefore the combined metrics in Eq (3) can be usedto filter page pairs, the combination parametersneed to be tuned on a development set.
The para-meters are adp, ar, and ac as well as the thresholdabove which the combined metrics indicate a trans-lated page pair vs. an unrelated page pair.To tune the parameters, we used data from anindependent test run for the en?de language direc-tion.
We randomly chose 50 candidate pairs from aset created with the URL part substitution methodand manually judged whether or not the pages aretranslations of each other.We varied the parameters adp, ar, ac and tdprc overa range of empirical values and compared howwell the combined metrics judgment correlatedwith the human judgment for page translation (wecalculated the Pearson correlation coefficient).
Theresults of tuning are shown in Table 3.adp ar ac tdprc0.5 1.5 1 > 0.8Table 3: Parameter and threshold valueschosen for linear combination4 The search engine returns 100 or fewer documents foreach query.3.4 Evaluation of the Filtering StepTo evaluate the combined filtering method de-scribed in Section 2.4.3, we chose 110 page pairsat random from the 433 candidate page pairs inexperiment #5 (Language direction en?de, Pairsgenerated with the URL part substitution methoddescribed in 2.3.3).
Each of the page pairs was eva-luated manually to assess whether it is a true trans-lation pair.On this set, the combined filter had a precisionof 88.9% and a recall of 36.4%.
The high precisionis encouraging on the noisy test set.
The recall islow but is acceptable since one can always submitmore sampling queries to the search engine.
Resnikand Smith (2003) reported higher precision andrecall in their experiments.
However, their num-bers and ours are not directly comparable becausetheir approach required the existence of parent orsibling pages and consequently their test sets wereless noisy.From the numbers of translation pairs, we canmake an estimate of available parallel pages for alanguage pair, as explained in Section 2.5.
For in-stance, by using the results of experiment #13, theestimate is P(DE|E)=0.03% and P(ED|D)=0.27% (Efor English, and D for German).
This indicates thatthe number of English-German parallel pages issmall comparing to the total number of English andGerman web pages.4 ConclusionIn this paper we show that despite the fact thatthere are no standardized features to identify paral-lel web pages and despite the relevance ranking ofcommercial search engine results, it is possible tocome up with reliable methods to gather parallelpages using commercial search engines.
It is alsopossible to calculate an estimate of how many pag-es are available parallel in relation to the overallnumber of pages in a certain language.The number of translation pages retrieved by thecurrent methods is relatively small.
In the future,we plan to learn URL patterns from the Levensh-tein matching method and add them to the patternsused in the URL part substitution method.
Oncemore translation pages are retrieved, we plan to usethese pages as parallel data in a statistical machinetranslation (MT) system to evaluate the usefulnessof this approach to MT.Instead of using narrow query interfaces to apublic search engine interface, it also might be ad-vantageous to have access to raw indices or crawldata of the engines.
Such access will enable us totake advantage of certain page features that couldbe good indicators of parallel pages.ReferencesKrishna Bharat and Andrei Broder , 1998, A techniquefor measuring the relative size and overlap of publicWeb search engines, Computer Networks and ISDNSystems 30(1-7).Peter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Fredrick Jelinek, John D.Lafferty, Robert L. Mercer, and Paul S. Roossin,1990, A statistical approach to machine translation,Computational Linguist.
16(2), pp 79-85.Jiang Chen and Jian-Yun Nie, 2000, Parallel Web TextMining for Cross-Language IR, in Proceedings ofRIAO-2000: Content-Based Multimedia InformationAccess.William A. Gale and Kenneth W. Church, 1991, Identi-fying word correspondence in parallel texts, in 'HLT'91: Proceedings of the workshop on Speech andNatural Language',  NJ, USA, pp.
152-157.J.W.
Hunt and M.D.
McIlroy, M.D., 1976, An Algo-rithm for Differential File Comparison, Bell Labora-tories, Computer Science Technical Report 41.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak, 2005, BootstrappingParsers via Syntactic Projection across ParallelTexts, Special Issue of the Journal of Natural Lan-guage Engineering on Parallel Texts 11(3),  pp 311-325.Philipp Koehn, 2005, Europarl: A Parallel Corpus forStatistical Machine Translation, in Proceedings ofthe 2005 MT Summit.Xiaoyi Ma and Mark Y. Liberman, 1999, BITS: A Me-thod for Bilingual Text Search over the Web, in Pro-ceedings of the 1999 MT Summit, Singapore.Franz Josef Och and Hermann Ney, 2003, A SystematicComparison of Various Statistical Alignment Models,Computational Linguistics 29(1), pp 19-51.Sebastian Pad?
and Mirella Lapata, 2005, Cross-linguistic Projection of Role-Semantic Information,in  Proceedings of HLT/EMNLP 2005.Philip Resnik and Noah A. Smith, 2003, The Web as aParallel Corpus, Computational Linguistics 29(3), pp349-380.J?rg Tiedemann and Lars Nygaard, 2004, The OPUScorpus - parallel & free, in 'Proceedings of theFourth International Conference on Language Re-sources and Evaluation (LREC'04)'.David Yarowsky and Grace Ngai, 2001, Inducing multi-lingual POS taggers and NP bracketers via robustprojection across aligned corpora, in ?Proceedings ofthe Second meeting of the North American Chapterof ACL (NAACL 2001)?,  NJ, USA, pp.
1-8.George K. Zipf, 1949, Human Behavior and the Prin-ciple of Least-Effort, Addison-Wesley
