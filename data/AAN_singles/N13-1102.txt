Proceedings of NAACL-HLT 2013, pages 820?825,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsDisfluency Detection Using Multi-step Stacked LearningXian Qian and Yang LiuComputer Science DepartmentThe University of Texas at Dallas{qx,yangl}@hlt.utdallas.eduAbstractIn this paper, we propose a multi-step stackedlearning model for disfluency detection.
Ourmethod incorporates refined n-gram featuresstep by step from different word sequences.First, we detect filler words.
Second, editedwords are detected using n-gram features ex-tracted from both the original text and filler fil-tered text.
In the third step, additional n-gramfeatures are extracted from edit removed textstogether with our newly induced in-betweenfeatures to improve edited word detection.
WeuseMax-MarginMarkov Networks (M3Ns) asthe classifier with the weighted hamming lossto balance precision and recall.
Experimentson the Switchboard corpus show that the re-fined n-gram features from multiple steps andM3Ns with weighted hamming loss can signif-icantly improve the performance.
Our methodfor disfluency detection achieves the best re-ported F-score 0.841 without the use of addi-tional resources.11 IntroductionDetecting disfluencies in spontaneous speech canbe used to clean up speech transcripts, which help-s improve readability of the transcripts and make iteasy for downstream language processing modules.There are two types of disfluencies: filler words in-cluding filled pauses (e.g., ?uh?, ?um?)
and discoursemarkers (e.g., ?I mean?, ?you know?
), and editedwords that are repeated, discarded, or corrected by1Our source code is available athttp://code.google.com/p/disfluency-detection/downloads/listthe following words.
An example is shown belowthat includes edited words and filler words.I want a flight to Boston?
??
?editeduh I mean?
??
?fillerto DenverAutomatic filler word detection is much more ac-curate than edit detection as they are often fixedphrases (e.g., ?uh?, ?you know?, ?I mean?
), henceour work focuses on edited word detection.Many models have been evaluated for this task.Liu et al(2006) used Conditional Random Fields(CRFs) for sentence boundary and edited word de-tection.
They showed that CRFs significantly out-performed Maximum Entropy models and HMM-s. Johnson and Charniak (2004) proposed a TAG-based noisy channel model which showed great im-provement over boosting based classifier (Charniakand Johnson, 2001).
Zwarts and Johnson (2011)extended this model using minimal expected F-lossoriented n-best reranking.
They obtained the best re-ported F-score of 83.8% on the Switchboard corpus.Georgila (2009) presented a post-processing methodduring testing based on Integer Linear Programming(ILP) to incorporate local and global constraints.From the view of features, in addition to tex-tual information, prosodic features extracted fromspeech have been incorporated to detect editedwords in some previous work (Kahn et al 2005;Zhang et al 2006; Liu et al 2006).
Zwarts andJohnson (2011) trained an extra language model onadditional corpora, and used output log probabili-ties of language models as features in the rerankingstage.
They reported that the language model gainedabout absolute 3% F-score for edited word detectionon the Switchboard development dataset.820In this paper, we propose a multi-step stackedlearning approach for disfluency detection.
In ourmethod, we first perform filler word detection, thenedited word detection.
In every step, we generatenew refined n-gram features based on the processedtext (remove the detected filler or edited words fromthe previous step), and use these in the next step.We also include a new type of features, called in-between features, and incorporate them into the laststep.
For edited word detection, we use Max-MarginMarkov Networks (M3Ns) with weighted hammingloss as the classifier, as it can well balance the pre-cision and recall to achieve high performance.
Onthe commonly used Switchboard corpus, we demon-strate that our proposed method outperforms otherstate-of-the-art systems for edit disfluency detection.2 Balancing Precision and Recall UsingWeighted M3NsWe use a sequence labeling model for edit detection.Each word is assigned one of the five labels: BE (be-ginning of the multi-word edited region), IE (in theedited region), EE (end of the edited region), SE (s-ingle word edited region), O (other).
For example,the previous sentence is represented as:I/O want/O a/O flight/O to/BE Boston/EE uh/OI/O mean/O to/O Denver/OWe use the F-score as the evaluation metrics(Zwarts and Johnson, 2011; Johnson and Charniak,2004), which is defined as the harmonic mean of theprecision and recall of the edited words:P = #correctly predicted edited words#predicted edited wordsR = #correctly predicted edited words#gold standard edited wordsF = 2?
P ?RP + RThere are many methods to train the sequence mod-el, such as CRFs (Lafferty et al 2001), averagedstructured perceptrons (Collins, 2002), structuredSVM (Altun et al 2003), online passive aggressivelearning (Crammer et al 2006).
Previous work hasshown that minimizing F-loss is more effective thanminimizing log-loss (Zwarts and Johnson, 2011),because edited words are much fewer than normalwords.In this paper, we use Max-margin Markov Net-works (Taskar et al 2004) because our preliminaryresults showed that they outperform other classifier-s, and using weighted hamming loss is simple in thisapproach (whereas for perceptron or CRFs, the mod-ification of the objective function is not straightfor-ward).The learning task for M3Ns can be represented asfollows:min?12C?
?x,y?x,y?f(x, y)?22 +?x,y?x,yL(x, y)s.t.
?y?x,y = 1 ?x?x,y ?
0, ?x, yThe above shows the dual form for trainingM3Ns,where x is the observation of a training sample,y ?
Y is a label.
?
is the parameter neededto be optimized, C > 0 is the regularization pa-rameter.
?f(x, y) is the residual feature vector:f(x, y?)
?
f(x, y), where y?
is the true label of x.L(x, y) is the loss function.
Taskar et al(2004) usedun-weighted hamming loss, which is the numberof incorrect components: L(x, y) =?t ?
(yt, y?t),where ?
(a, b) is the binary indicator function (it is 0if a = b).
In our work, we use the weighted ham-ming loss:L(x, y) =?tv(yt, y?t)?
(yt, y?t)where v(yt, y?t) is the weighted loss for the errorwhen y?t is mislabeled as yt.
Such a weighted lossfunction allows us to balance the model?s precisionand recall rates.
For example, if we assign a largevalue to v(O, ?E) (?E denotes SE, BE, IE, EE), thenthe classifier is more sensitive to false negative er-rors (edited word misclassified as non-edited word),thus we can improve the recall rate.
In our work,we tune the weight matrix v using the developmentdataset.3 Multi-step Stacked Learning for EditDisfluency DetectionRather than just using the above M3Ns with somefeatures, in this paper we propose to use stackedlearning to incorporate gradually refined n-gram fea-tures.
Stacked learning is a meta-learning approach(Cohen and de Carvalho, 2005).
Its idea is to use two821(or more) levels of predictors, where the outputs ofthe low level predictors are incorporated as featuresinto the next level predictors.
It has the advantageof incorporating non-local features as well as non-linear classifiers.
In our task, we do not just use theclassifier?s output (a word is an edited word or not)as a feature, rather we use such output to remove thedisfluencies and extract new n-gram features for thesubsequent stacked classifiers.
We use 10 fold crossvalidation to train the low level predictors.
The fol-lowing describes the three steps in our approach.3.1 Step 1: Filler Word DetectionIn the first step, we automatically detect filler word-s.
Since filler words often occur immediately afteredited words (before the corrected words), we ex-pect that removing them will make rough copy de-tection easy.
For example, in the previous exampleshown in Section 1, if ?uh I mean?
is removed, thenthe reparandum ?to Boston?
and repair ?to Denver?will be adjacent and we can use word/POS based n-gram features to detect that disfluency.
Otherwise,the classifier needs to skip possible filler words tofind the rough copy of the reparandum.For filler word detection, similar to edited worddetection, we define 5 labels: BP , IP , EP , SP , O.We use un-weighted hamming loss to learn M3Nsfor this task.
Since for filler word detection, our per-formance metric is not F-measure, but just the over-all accuracy in order to generate cleaned text for sub-sequent n-gram features, we did not use the weight-ed hamming hoss for this.
The features we used arelisted in Table 1.
All n-grams are extracted from theoriginal text.3.2 Step 2: Edited Word DetectionIn the second step, edited words are detected usingM3Ns with the weighted-hamming loss.
The fea-tures we used are listed in Table 2.
All n-grams inthe first step are also used here.
Besides that, wordn-grams, POS n-grams and logic n-grams extractedfrom filler word removed text are included.
Featuretemplates I(w0, w?i) is to generate features detectingrough copies separated by filler words.3.3 Step 3: Refined Edited Word DetectionIn this step, we use n-gram features extracted fromthe text after removing edit disfluencies based onunigrams w0, w?1, w1, w?2, w2p0, p?1, p1, p?2, p2, w0p0bigrams w?1w0, w0w1, p?1p0, p0p1trigrams p?2p?1p0, p?1p0p1, p0p1p2logic unigrams I(wi, w0), I(pi, p0), ?4 ?
i ?
4logic bigrams I(wi?1wi, w?1, w0)I(pi?1pi, p?1p0)I(wiwi+1, w0w1)I(pipi+1, p0p1), ?4 ?
i ?
4transitions y?1y0Table 1: Feature templates for filler word detection.w0, p0 denote the current word and POS tag respective-ly.
w?i denotes the ith word to the left, wi denotes theith word to the right.
The logic function I(a, b) indicateswhether a and b are identical (eigher unigrams or bigram-s).All templates in Table 1unigrams w?1, w?2, w?3, w?4bigrams p0p?1, p0p?2, p0p?3, p0p?4w0p?1, w0p?2, w0p?3, w0p?4w0p1, w0p2, w0p3, w0p4logic unigrams I(w0, w?i), 1 ?
i ?
4transitions p0y?1y0Table 2: Feature templates for edit detection (step 2).w?i, p?i denote the ith word/POS tag to the right in the fillerwords removed text.
If current word w0 is removed instep 1, we use its original n-gram features rather than therefined n-gram features.the previous step.
According to our analysis of theerrors produced by step 2, we observed that manyerrors occurred at the boundaries of the disfluen-cies, and the word bigrams after removing the editedwords are unnatural.
The following is an example:?
Ref: The new type is prettier than whattheir/SE they used to look like.?
Sys: The new type is prettier than what/BEtheir/EE they used to look like.Using the system?s prediction, we would have bi-gram than they, which is odd.
Usually, the pronounfollowing than is accusative case.
We expect addingn-gram features derived from the cleaned-up sen-tences would allow the new classifier to fix such hy-pothesis.
This kind of n-gram features is similar tothe language models used in (Zwarts and Johnson,8222011).
They have the benefit of measuring the flu-ency of the cleaned text.Another common error we noticed is caused bythe ambiguities of coordinates, because the coordi-nates have similar patterns as rough copies.
For ex-ample,?
Coordinates: they ca n?t decide which are thegood aspects and which are the bad aspects?
Rough Copies: it/BE ?s/IE a/IE pleasure/IEto/EE it s good to get outsideTo distinguish the rough copies and the coordinateexamples shown above, we analyze the training datastatistically.
We extract all the pieces lying betweenidentical word bigrams AB .
.
.
AB.
The observationis that coordinates are often longer than edited se-quences.
Hence we introduce the in-between fea-tures for each word.
If a word lies between identicalword bigrams, then its in-between feature is the loglength of the subsequence lying between the two bi-grams; otherwise, it is zero (we use log length toavoid sparsity).
We also used other patterns such asA .
.
.
A and ABC .
.
.
ABC, but they are too noisy orinfrequent and do not yield much performance gain.Table 3 lists the feature templates used in this laststep.All templates in Table 1, Table 2word n-grams w?
?1 , w0w?
?1in-between LAB , w0bAB , bABTable 3: Feature templates for refined edit detection (step3).
w?
?i denotes the ith word tag to the right in the edit-ed word removed text.
LAB denotes the log length ofthe sub-sequence in the pattern AB.
.
.
AB, bAB indicateswhether the current word lies between two identical bi-grams.4 Experiments4.1 Experimental SetupWe use the Switchboard corpus in our experimen-t, with the same train/develop/test split as the pre-vious work (Johnson and Charniak, 2004).
We al-so remove the partial words and punctuation fromthe training and test data for the reason to simulatethe situation when speech recognizers are used andsuch kind of information is not available (Johnsonand Charniak, 2004).We tuned the weight matrix for hamming loss onthe development dataset using simple grid search.The diagonal elements are fixed at 0; for false pos-itive errors, O ?
?E (non-edited word mis-labeledas edited word), their weights are fixed at 1; for falsenegative errors, ?E ?
O, we tried the weight from1 to 3, and increased the weight 0.5 each time.
Theoptimal weight matrix is shown in Table 4.
Notethat we use five labels in the sequence labeling task;however, for edited word detection evaluation, it isonly a binary task, that is, all of the words labeledwith ?E will be mapped to the class of edited words.PPPPPPPtruthpredict BE IE EE SE OBE 0 1 1 1 2IE 1 0 1 1 2EE 1 1 0 1 2SE 1 1 1 0 2O 1 1 1 1 0Table 4: Weighted hamming loss for M3Ns.4.2 ResultsWe compare several sequence labeling models:CRFs, structured averaged perceptron (AP), M3Nswith un-weighted/weighted loss, and online passive-aggressive (PA) learning.
For each model, we tunedthe parameters on the development data: Gaussianprior for CRFs is 1.0, iteration number for AP is 10,iteration number and regularization penalty for PAare 10 and 1.
For M3Ns, we use Structured Sequen-tial Minimal Optimization (Taskar, 2004) for modeltraining.
Regularization penalty is C = 0.1 and iter-ation number is 30.Table 5 shows the results using different modelsand features.
The baseline models use only the n-grams features extracted from the original text.
Wecan see that M3Ns with the weighted hamming lossachieve the best performance, outperforming all theother models.
Regarding the features, the graduallyadded n-gram features have consistent improvemen-t for all models.
Using the weighted hamming lossin M3Ns, we observe a gain of 2.2% after deletingfiller words, and 1.8% after deleting edited words.
Inour analysis, we also noticed that the in-between fea-823CRF AP PA M3N w. M3NBaseline 78.8 79.0 78.9 79.4 80.1Step 2 81.0 81.1 81.1 81.5 82.3Step 3 82.9 83.0 82.8 83.3 84.1Table 5: Effect of training strategy and recovered featuresfor stacked learning.
F scores are reported.
AP = Aver-aged Perceptron, PA = online Passive Aggresive, M3N =un-weighted M3Ns, w. M3N = weighted M3Ns.tures yield about 1% improvement in F-score for allmodels (the gain of step 3 over step 2 is because ofthe in-between features and the new n-gram featuresextracted from the text after removing previouslydetected edited words).
We performed McNemar?stest to evaluate the significance of the difference a-mong various methods, and found that when usingthe same features, weighted M3Ns significantly out-performs all the other models (p value < 0.001).There are no significant differences among CRFs,AP and PA.
Using recovered n-gram features and in-between features significantly improves all sequencelabeling models (p value < 0.001).We also list the state-of-the-art systems evaluat-ed on the same dataset, as shown in Table 6.
Weachieved the best F-score.
The most competitivesystem is (Zwarts and Johnson, 2011), which usesextra resources to train language models.System F score(Johnson and Charniak, 2004) 79.7(Kahn et al 2005) 78.2(Zhang et al 2006)?
81.2(Georgila, 2009)?
80.1(Zwarts and Johnson, 2011)+ 83.8This paper 84.1Table 6: Comparison with other systems.
?
they usedthe re-segmented Switchboard corpus, which is not ex-actly the same as ours.
?
they reported the F-score ofBE tag (beginning of the edited sequences).
+ they usedlanguage model learned from 3 additional corpora.5 ConclusionIn this paper, we proposed multi-step stacked learn-ing to extract n-gram features step by step.
The firstlevel removes the filler words providing new ngram-s for the second level to remove edited words.
Thethird level uses the n-grams from the original tex-t and the cleaned text generated by the previous t-wo steps for accurate edit detection.
To minimizethe F-loss approximately, we modified the hammingloss in M3Ns.
Experimental results show that ourmethod is effective, and achieved the best reportedperformance on the Switchboard corpus without theuse of any additional resources.AcknowledgmentsWe thank three anonymous reviewers for their valu-able comments.
This work is partly supported byDARPA under Contract No.
HR0011-12-C-0016and FA8750-13-2-0041.
Any opinions expressed inthis material are those of the authors and do not nec-essarily reflect the views of DARPA.ReferencesYasemin Altun, Ioannis Tsochantaridis, and ThomasHofmann.
2003.
Hidden markov support vector ma-chines.
In Proc.
of ICML.Eugene Charniak and Mark Johnson.
2001.
Edit detec-tion and parsing for transcribed speech.
In Proc.
ofNAACL.William W. Cohen and Vitor Rocha de Carvalho.
2005.Stacked sequential learning.
In Proc.
of IJCAI.Michael Collins.
2002.
Discriminative training methodsfor hidden markov models: Theory and experimentswith perceptron algorithms.
In Proc.
of EMNLP.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, Yoram Singer, and Yoram Singer.
2006.
On-line passive-aggressive algorithms.
Journal of Ma-chine Learning Research.Kallirroi Georgila.
2009.
Using integer linear program-ming for detecting speech disfluencies.
In Proc.
ofNAACL.Mark Johnson and Eugene Charniak.
2004.
A TAG-based noisy-channel model of speech repairs.
In Proc.of ACL.Jeremy G. Kahn, Matthew Lease, Eugene Charniak,Mark Johnson, and Mari Ostendorf.
2005.
Effectiveuse of prosody in parsing conversational speech.
InProc.
of HLT-EMNLP.John D. Lafferty, AndrewMcCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proc.
of ICML.Yang Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Osten-dorf, and M. Harper.
2006.
Enriching speech recog-nition with automatic detection of sentence bound-824aries and disfluencies.
IEEE Transactions on Audio,Speech, and Language Processing, 14(5).Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004.Max-margin markov networks.
In Proc.
of NIPS.Ben Taskar.
2004.
Learning Structured Prediction Mod-els: A Large Margin Approach.
Ph.D. thesis, StanfordUniversity.Qi Zhang, Fuliang Weng, and Zhe Feng.
2006.
A pro-gressive feature selection algorithm for ultra large fea-ture spaces.
In Proc.
of ACL.Simon Zwarts and Mark Johnson.
2011.
The impact oflanguage models and loss functions on repair disfluen-cy detection.
In Proc.
of ACL-HLT.825
