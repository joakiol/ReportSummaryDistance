Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 68?74,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsEmploying Word Representations and Regularization forDomain Adaptation of Relation ExtractionThien Huu NguyenComputer Science DepartmentNew York UniversityNew York, NY 10003 USAthien@cs.nyu.eduRalph GrishmanComputer Science DepartmentNew York UniversityNew York, NY 10003 USAgrishman@cs.nyu.eduAbstractRelation extraction suffers from a perfor-mance loss when a model is applied toout-of-domain data.
This has fostered thedevelopment of domain adaptation tech-niques for relation extraction.
This paperevaluates word embeddings and clusteringon adapting feature-based relation extrac-tion systems.
We systematically explorevarious ways to apply word embeddingsand show the best adaptation improvementby combining word cluster and word em-bedding information.
Finally, we demon-strate the effectiveness of regularizationfor the adaptability of relation extractors.1 IntroductionThe goal of Relation Extraction (RE) is to detectand classify relation mentions between entity pairsinto predefined relation types such as Employ-ment or Citizenship relationships.
Recent researchin this area, whether feature-based (Kambhatla,2004; Boschee et al, 2005; Zhou et al, 2005;Grishman et al, 2005; Jiang and Zhai, 2007a;Chan and Roth, 2010; Sun et al, 2011) or kernel-based (Zelenko et al, 2003; Bunescu and Mooney,2005a; Bunescu and Mooney, 2005b; Zhang et al,2006; Qian et al, 2008; Nguyen et al, 2009), at-tempts to improve the RE performance by enrich-ing the feature sets from multiple sentence anal-yses and knowledge resources.
The fundamentalassumption of these supervised systems is that thetraining data and the data to which the systems areapplied are sampled independently and identicallyfrom the same distribution.
When there is a mis-match between data distributions, the RE perfor-mance of these systems tends to degrade dramat-ically (Plank and Moschitti, 2013).
This is wherewe need to resort to domain adaptation techniques(DA) to adapt a model trained on one domain (thesource domain) into a new model which can per-form well on new domains (the target domains).The consequences of linguistic variation be-tween training and testing data on NLP tools havebeen studied extensively in the last couple of yearsfor various NLP tasks such as Part-of-Speech tag-ging (Blitzer et al, 2006; Huang and Yates, 2010;Schnabel and Sch?utze, 2014), named entity recog-nition (Daum?e III, 2007) and sentiment analysis(Blitzer et al, 2007; Daum?e III, 2007; Daum?eIII et al, 2010; Blitzer et al, 2011), etc.
Un-fortunately, there is very little work on domainadaptation for RE.
The only study explicitly tar-geting this problem so far is by Plank and Mos-chitti (2013) who find that the out-of-domain per-formance of kernel-based relation extractors canbe improved by embedding semantic similarity in-formation generated from word clustering and la-tent semantic analysis (LSA) into syntactic treekernels.
Although this idea is interesting, it suf-fers from two major limitations:+ It does not incorporate word cluster informa-tion at different levels of granularity.
In fact, Plankand Moschitti (2013) only use the 10-bit clusterprefix in their study.
We will demonstrate laterthat the adaptability of relation extractors can ben-efit significantly from the addition of word clusterfeatures at various granularities.+ It is unclear if this approach can encode real-valued features of words (such as word embed-dings (Mnih and Hinton, 2007; Collobert and We-ston, 2008)) effectively.
As the real-valued fea-tures are able to capture latent yet useful proper-ties of words, the augmentation of lexical termswith these features is desirable to provide a moregeneral representation, potentially helping relationextractors perform more robustly across domains.In this work, we propose to avoid these limita-tions by applying a feature-based approach for REwhich allows us to integrate various word featuresof generalization into a single system more natu-68rally and effectively.The application of word representations suchas word clusters in domain adaptation of RE(Plank and Moschitti, 2013) is motivated by itssuccesses in semi-supervised methods (Chan andRoth, 2010; Sun et al, 2011) where word repre-sentations help to reduce data-sparseness of lexi-cal information in the training data.
In DA terms,since the vocabularies of the source and target do-mains are usually different, word representationswould mitigate the lexical sparsity by providinggeneral features of words that are shared acrossdomains, hence bridge the gap between domains.The underlying hypothesis here is that the absenceof lexical target-domain features in the source do-main can be compensated by these general fea-tures to improve RE performance on the target do-mains.We extend this motivation by further evaluat-ing word embeddings (Bengio et al, 2001; Ben-gio et al, 2003; Mnih and Hinton, 2007; Col-lobert and Weston, 2008; Turian et al, 2010) onfeature-based methods to adapt RE systems to newdomains.
We explore the embedding-based fea-tures in a principled way and demonstrate thatword embedding itself is also an effective repre-sentation for domain adaptation of RE.
More im-portantly, we show empirically that word embed-dings and word clusters capture different informa-tion and their combination would further improvethe adaptability of relation extractors.2 RegularizationGiven the more general representations providedby word representations above, how can we learn arelation extractor from the labeled source domaindata that generalizes well to new domains?
In tra-ditional machine learning where the challenge isto utilize the training data to make predictions onunseen data points (generated from the same dis-tribution as the training data), the classifier witha good generalization performance is the one thatnot only fits the training data, but also avoids ov-efitting over it.
This is often obtained via regular-ization methods to penalize complexity of classi-fiers.
Exploiting the shared interest in generaliza-tion performance with traditional machine learn-ing, in domain adaptation for RE, we would preferthe relation extractor that fits the source domaindata, but also circumvents the overfitting problemover this source domain1so that it could general-ize well on new domains.
Eventually, regulariza-tion methods can be considered naturally as a sim-ple yet general technique to cope with DA prob-lems.Following Plank and Moschitti (2013), we as-sume that we only have labeled data in a singlesource domain but no labeled as well as unlabeledtarget data.
Moreover, we consider the single-system DA setting where we construct a singlesystem able to work robustly with different butrelated domains (multiple target domains).
Thissetting differs from most previous studies (Blitzeret al, 2006) on DA which have attempted to de-sign a specialized system for every specific tar-get domain.
In our view, although this setting ismore challenging, it is more practical for RE.
Infact, this setting can benefit considerably from ourgeneral approach of applying word representationsand regularization.
Finally, due to this setting, thebest way to set up the regularization parameter isto impose the same regularization parameter onevery feature rather than a skewed regularization(Jiang and Zhai, 2007b).3 Related WorkAlthough word embeddings have been success-fully employed in many NLP tasks (Collobert andWeston, 2008; Turian et al, 2010; Maas andNg, 2010), the application of word embeddingsin RE is very recent.
Kuksa et al (2010) pro-pose an abstraction-augmented string kernel forbio-relation extraction via word embeddings.
Inthe surge of deep learning, Socher et al (2012)and Khashabi (2013) use pre-trained word embed-dings as input for Matrix-Vector Recursive Neu-ral Networks (MV-RNN) to learn compositionalstructures for RE.
However, none of these worksevaluate word embeddings for domain adaptationof RE which is our main focus in this paper.Regarding domain adaptation, in representationlearning, Blitzer et al (2006) propose structuralcorrespondence learning (SCL) while Huang andYates (2010) attempt to learn a multi-dimensionalfeature representation.
Unfortunately, these meth-ods require unlabeled target domain data whichare unavailable in our single-system setting of DA.Daum?e III (2007) proposes an easy adaptationframework (EA) which is later extended to a semi-supervised version (EA++) to incorporate unla-1domain overfitting (Jiang and Zhai, 2007b)69beled data (Daum?e III et al, 2010).
In terms ofword embeddings for DA, recently, Xiao and Guo(2013) present a log-bilinear language adaptationframework for sequential labeling tasks.
However,these methods assume some labeled data in targetdomains and are thus not applicable in our settingof unsupervised DA.
Above all, we move one stepfurther by evaluating the effectiveness of word em-beddings on domain adaptation for RE which isvery different from the principal topic of sequencelabeling in the previous research.4 Word RepresentationsWe consider two types of word representations anduse them as additional features in our DA sys-tem, namely Brown word clustering (Brown etal., 1992) and word embeddings (Bengio et al,2001).
While word clusters can be recognizedas an one-hot vector representation over a smallvocabulary, word embeddings are dense, low-dimensional, and real-valued vectors (distributedrepresentations).
Each dimension of the word em-beddings expresses a latent feature of the words,hopefully reflecting useful semantic and syntacticregularities (Turian et al, 2010).
We investigateword embeddings induced by two typical languagemodels: Collobert and Weston (2008) embeddings(C&W) (Collobert and Weston, 2008; Turian etal., 2010) and Hierarchical log-bilinear embed-dings (HLBL) (Mnih and Hinton, 2007; Mnih andHinton, 2009; Turian et al, 2010).5 Feature Set5.1 Baseline Feature SetSun et al (2011) utilize the full feature set from(Zhou et al, 2005) plus some additional featuresand achieve the state-of-the-art feature-based REsystem.
Unfortunately, this feature set includesthe human-annotated (gold-standard) informationon entity and mention types which is often miss-ing or noisy in reality (Plank and Moschitti, 2013).This issue becomes more serious in our setting ofsingle-system DA where we have a single sourcedomain with multiple dissimilar target domainsand an automatic system able to recognize entityand mention types very well in different domainsmay not be available.
Therefore, following the set-tings of Plank and Moschitti (2013), we will onlyassume entity boundaries and not rely on the goldstandard information in the experiments.
We ap-ply the same feature set as Sun et al (2011) butremove the entity and mention type information2.5.2 Lexical Feature AugmentationWhile Sun et al (2011) show that adding wordclusters to the heads of the two mentions is themost effective way to improve the generaliza-tion accuracy, the right lexical features into whichword embeddings should be introduced to obtainthe best adaptability improvement are unexplored.Also, which dimensionality of which word embed-ding should we use with which lexical features?In order to answer these questions, following Sunet al (2011), we first group lexical features into 4groups and rank their importance based on linguis-tic intuition and illustrations of the contributionsof different lexical features from various feature-based RE systems.
After that, we evaluate the ef-fectiveness of these lexical feature groups for wordembedding augmentation individually and incre-mentally according to the rank of importance.
Foreach of these group combinations, we assess thesystem performance with different numbers of di-mensions for both C&W and HLBL word embed-dings.
Let M1 and M2 be the first and second men-tions in the relation.
Table 1 describes the lexicalfeature groups.Rank Group Lexical Features1 HM HM1 (head of M1)HM2 (head of M2)2 BagWM WM1 (words in M1)WM2 (words in M2)3 HC heads of chunks in context4 BagWC words of contextTable 1: Lexical feature groups ordered by importance.6 Experiments6.1 Tools and DataOur relation extraction system is hierarchical(Bunescu and Mooney, 2005b; Sun et al, 2011)and apply maximum entropy (MaxEnt) in theMALLET3toolkit as the machine learning tool.For Brown word clusters, we directly apply theclustering trained by Plank and Moschitti (2013)2We have the same observation as Plank and Moschitti(2013) that when the gold-standard labels are used, theimpact of word representations is limited since the gold-standard information seems to dominate.
However, wheneverthe gold labels are not available or inaccurate, the word rep-resentations would be useful for improving adaptability per-formance.
Moreover, in all the cases, regularization methodsare still effective for domain adaptation of RE.3http://mallet.cs.umass.edu/70In-domain (bn+nw) Out-of-domain (bc development set)System C&W,25 C&W,50 C&W,100 HLBL,50 HLBL,100 C&W,25 C&W,50 C&W,100 HLBL,50 HLBL,1001Baseline 51.4 51.4 51.4 51.4 51.4 49.0 49.0 49.0 49.0 49.021+HM ED 54.0(+2.6) 54.1(+2.7) 55.7(+4.3) 53.7(+2.3) 55.2(+3.8) 51.5(+2.5) 52.7(+3.7) 52.5(+3.5) 50.2(+1.2) 50.6(+1.6)31+BagWM ED 52.3(+0.9) 50.9(-0.5) 51.5(+0.1) 51.8(+0.4) 52.5(+1.1) 48.5(-0.5) 48.9(-0.1) 48.6(-0.4) 48.7(-0.3) 49.0(+0.0)41+HC ED 51.3(-0.1) 50.9(-0.5) 48.3(-3.1) 50.8(-0.6) 49.8(-1.6) 44.9(-4.1) 45.8(-3.2) 45.8(-3.2) 48.7(-0.3) 47.3(-1.7)51+BagWC ED 51.5(+0.1) 50.8(-0.6) 49.5(-1.9) 51.4(+0.0) 50.3(-1.1) 48.3(-0.7) 46.3(-2.7) 44.0(-5.0) 46.6(-2.4) 44.8(-4.2)62+BagWM ED 54.3(+2.9) 53.2(+1.8) 53.2(+1.8) 54.0(+2.6) 53.8(+2.4) 52.5(+3.5) 51.4(+2.4) 50.6(+1.6) 50.0(+1.0) 48.6(-0.4)76+HC ED 53.4(+2.0) 52.3(+0.9) 52.7(+1.3) 54.2(+2.8) 53.1(+1.7) 50.5(+1.5) 50.9(+1.9) 48.4(-0.6) 50.0(+1.0) 48.9(-0.1)87+BagWC ED 53.4(+2.0) 52.2(+0.8) 50.8(-0.6) 53.5(+2.1) 53.6(+2.2) 49.2(+0.2) 50.7(+1.7) 49.2(+0.2) 47.9(-1.1) 49.5(+0.5)Table 2: In-domain and Out-of-domain performance for different embedding features.
The cells in bold are the best results.to facilitate system comparison later.
We evalu-ate C&W word embeddings with 25, 50 and 100dimensions as well as HLBL word embeddingswith 50 and 100 dimensions that are introducedin Turian et al (2010) and can be downloadedhere4.
The fact that we utilize the large, generaland unbiased resources generated from the previ-ous works for evaluation not only helps to verifythe effectiveness of the resources across differenttasks and settings but also supports our setting ofsingle-system DA.We use the ACE 2005 corpus for DA experi-ments (as in Plank and Moschitti (2013)).
It in-volves 6 relation types and 6 domains: broadcastnews (bn), newswire (nw), broadcast conversation(bc), telephone conversation (cts), weblogs (wl)and usenet (un).
We follow the standard prac-tices on ACE (Plank and Moschitti, 2013) and usenews (the union of bn and nw) as the source do-main and bc, cts and wl as our target domains.
Wetake half of bc as the only target development set,and use the remaining data and domains for testingpurposes (as they are small already).
As noted inPlank and Moschitti (2013), the distributions of re-lations as well as the vocabularies of the domainsare quite different.6.2 Evaluation of Word Embedding FeaturesWe investigate the effectiveness of word embed-dings on lexical features by following the proce-dure described in Section 5.2.
We test our systemon two scenarios: In-domain: the system is trainedand evaluated on the source domain (bn+nw, 5-fold cross validation); Out-of-domain: the systemis trained on the source domain and evaluated onthe target development set of bc (bc dev).
Table2 presents the F measures of this experiment5(the4http://metaoptimize.com/projects/wordreprs/5All the in-domain improvement in rows 2, 6, 7 of Table2 are significant at confidence levels ?
95%.suffix ED in lexical group names is to indicate theembedding features).From the tables, we find that for C&W andHLBL embeddings of 50 and 100 dimensions, themost effective way to introduce word embeddingsis to add embeddings to the heads of the two men-tions (row 2; both in-domain and out-of-domain)although it is less pronounced for HLBL embed-ding with 50 dimensions.
Interestingly, for C&Wembedding with 25 dimensions, adding the em-bedding to both heads and words of the two men-tions (row 6) performs the best for both in-domainand out-of-domain scenarios.
This is new com-pared to the word cluster features where the headsof the two mentions are always the best places foraugmentation (Sun et al, 2011).
It suggests thata suitable amount of embeddings for words in thementions might be useful for the augmentation ofthe heads and inspires further exploration.
Intro-ducing embeddings to words of mentions alonehas mild impact while it is generally a bad idea toaugment chunk heads and words in the contexts.Comparing C&W and HLBL embeddings issomehow more complicated.
For both in-domainand out-of-domain settings with different num-bers of dimensions, C&W embedding outperformsHLBL embedding when only the heads of thementions are augmented while the degree of neg-ative impact of HLBL embedding on chunk headsas well as context words seems less serious thanC&W?s.
Regarding the incremental addition offeatures (rows 6, 7, 8), C&W is better for the out-of-domain performance when 50 dimensions areused, whereas HLBL (with both 50 and 100 di-mensions) is more effective for the in-domain set-ting.
For the next experiments, we will apply theC&W embedding of 50 dimensions to the headsof the mentions for its best out-of-domain perfor-mance.716.3 Domain Adaptation with WordEmbeddingsThis section examines the effectiveness of wordrepresentations for RE across domains.
We evalu-ate word cluster and embedding (denoted by ED)features by adding them individually as well assimultaneously into the baseline feature set.
Forword clusters, we experiment with two possibil-ities: (i) only using a single prefix length of 10(as Plank and Moschitti (2013) did) (denoted byWC10) and (ii) applying multiple prefix lengths of4, 6, 8, 10 together with the full string (denoted byWC).
Table 3 presents the system performance (Fmeasures) for both in-domain and out-of-domainsettings.SystemIn-domain bc cts wlBaseline(B) 51.4 49.7 41.5 36.6B+WC10 52.3(+0.9) 50.8(+1.1) 45.7(+4.2) 39.6(+3)B+WC 53.7(+2.3) 52.8(+3.1) 46.8(+5.3) 41.7(+5.1)B+ED 54.1(+2.7) 52.4(+2.7) 46.2(+4.7) 42.5(+5.9)B+WC+ED 55.5(+4.1) 53.8(+4.1) 47.4(+5.9) 44.7(+8.1)Table 3: Domain Adaptation Results with Word Represen-tations.
All the improvements over the baseline in Table 3 aresignificant at confidence level ?
95%.The key observations from the table are:(i): The baseline system achieves a performanceof 51.4% within its own domain while the per-formance on target domains bc, cts, wl drops to49.7%, 41.5% and 36.6% respectively.
Our base-line performance is worse than that of Plank andMoschitti (2013) only on the target domain cts andbetter in the other cases.
This might be explainedby the difference between our baseline feature setand the feature set underlying their kernel-basedsystem.
However, the performance order acrossdomains of the two baselines are the same.
Be-sides, the baseline performance is improved overall target domains when the system is enrichedwith word cluster features of the 10 prefix lengthonly (row 2).
(ii): Over all the target domains, the perfor-mance of the system augmented with word clusterfeatures of various granularities (row 3) is supe-rior to that when only cluster features for the pre-fix length 10 are added (row 2).
This is significant(at confidence level ?
95%) for domains bc andwl and verifies our assumption that various granu-larities for word cluster features are more effectivethan a single granularity for domain adaptation ofRE.
(iii): Row 4 shows that word embedding itself isalso very useful for domain adaptation in RE sinceit improves the baseline system for all the targetdomains.
(iv): In row 5, we see that the addition of bothword cluster and word embedding features im-proves the system further and results in the bestperformance over all target domains (this is sig-nificant with confidence level ?
95% in domainsbc and wl).
The result suggests that word embed-dings seem to capture different information fromword clusters and their combination would be ef-fective to generalize relation extractors across do-mains.
However, in domain cts, the improvementthat word embeddings provide for word clusters ismodest.
This is because the RCV1 corpus used toinduce the word embeddings (Turian et al, 2010)does not cover spoken language words in cts verywell.
(v): Finally, the in-domain performance is alsoimproved consistently demonstrating the robust-ness of word representations (Plank and Moschitti,2013).6.4 Domain Adaptation with RegularizationAll the experiments we have conducted so far donot apply regularization for training.
In this sec-tion, in order to evaluate the effect of regulariza-tion on the generalization capacity of relation ex-tractors across domains, we replicate all the ex-periments in Section 6.3 but apply regularizationwhen relation extractors are trained6.
Table 4presents the results.SystemIn-domain bc cts wlBaseline(B) 56.2 55.5 48.7 42.2B+WC10 57.5(+1.3) 57.3(+1.8) 52.3(+3.6) 45.0(+2.8)B+WC 58.9(+2.7) 58.4(+2.9) 52.8(+4.1) 47.3(+5.1)B+ED 58.9(+2.7) 59.5(+4.0) 52.6(+3.9) 48.6(+6.4)B+WC+ED 59.4(+3.2) 59.8(+4.3) 52.9(+4.2) 49.7(+7.5)Table 4: Domain Adaptation Results with Regularization.All the improvements over the baseline in Table 4 are signif-icant at confidence level ?
95%.For this experiment, every statement in (ii), (iii),(iv) and (v) of Section 6.3 also holds.
More impor-tantly, the performance in every cell of Table 4 issignificantly better than the corresponding cell inTable 3 (5% or better gain in F measure, a sig-nificant improvement at confidence level ?
95%).This demonstrates the effectiveness of regulariza-tion for RE in general and for domain adaptationof RE specifically.6We use a L2 regularizer with the regularization parame-ter of 0.5 for its best experimental results.72ReferencesYoshua Bengio, R?ejean Ducharme, and Pascal Vincent.2001.
A Neural Probabilistic Language Model.
InAdvances in Neural Information Processing Systems(NIPS?13), pages 932-938, MIT Press, 2001.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A Neural Probabilistic Lan-guage Model.
In Journal of Machine Learning Re-search (JMLR), 3, pages 1137-1155, 2003.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain Adaptation with Structural Corre-spondence Learning.
In Proceedings of the 2006Conference on Empirical Methods in Natural Lan-guage Processing, Sydney, Australia.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, Bollywood, Boom-boxes, andBlenders: Domain Adaptation for Sentiment Classi-fication.
In Proceedings of the ACL, pages 440-447,Prague, Czech Republic, June 2007.John Blitzer, Dean Foster, and Sham Kakade.
2011.Domain Adaptation with Coupled Subspaces.
InProceedings of the 14th International Conference onArtificial Intelligence and Statistics, pages 173-181,Fort Lauderdale, FL, USA.Elizabeth Boschee, Ralph Weischedel, and Alex Zama-nian.
2005.
Automatic Information Extraction.
InProceedings of the International Conference on In-telligence Analysis.Peter F. Brown, Peter V. deSouza, Robert L. Mercer,Vincent J. Della Pietra, and Jenifer C. Lai.
1992.Class-Based n-gram Models of Natural Language.In Journal of Computational Linguistics, Volume 18,Issue 4, pages 467-479, December 1992.Razvan C. Bunescu and Raymond J. Mooney.
2005a.A Shortest Path Dependency Kenrel for Relation Ex-traction.
In Proceedings of HLT/EMNLP.Razvan C. Bunescu and Raymond J. Mooney.
2005b.Subsequence Kernels for Relation Extraction.
InProceedings of NIPS.Yee S. Chan and Dan Roth.
2010.
Exploiting Back-ground Knowledge for Relation Extraction.
InProceedings of the 23rd International Conferenceon Computational Linguistics (Coling 2010), pages152-160, Beijing, China, August.Ronan Collobert and Jason Weston.
2008.
A Unied Ar-chitecture for Natural Language Processing: DeepNeural Networks with Multitask Learning.
In Inter-national Conference on Machine Learning, ICML,2008.Hal Daum?e III.
2007.
Frustratingly Easy DomainAdaptation.
In Proceedings of the ACL, pages 256-263, Prague, Czech Republic, June 2007.Hal Daum?e III, Abhishek Kumar and Avishek Saha.2010.
Co-regularization Based Semi-supervisedDomain Adaptation.
In Advances in Neural Infor-mation Processing Systems 23 (2010).Ralph Grishman, David Westbrook and Adam Meyers.2005.
NYU?s English ACE 2005 System Descrip-tion.
ACE 2005 Evaluation Workshop.Fei Huang and Alexander Yates.
2010.
Explor-ing Representation-Learning Approaches to DomainAdaptation.
In Proceedings of the 2010 Workshopon Domain Adaptation for Natural Language Pro-cessing, pages 23-30, Uppsala, Sweden, July 2010.Jing Jiang and ChengXiang Zhai.
2007a.
A Sys-tematic Exploration of the Feature Space for Re-lation Extraction.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (NAACL-HLT?07), pages 113-120, 2007.Jing Jiang and ChengXiang Zhai.
2007b.
A Two-stageApproach to Domain Adaptation for Statistical Clas-sifiers.
In Proceedings of the ACM 16th Confer-ence on Information and Knowledge Management(CIKM?07), pages 401-410, 2007.Nanda Kambhatla.
2004.
Combining Lexical, Syntac-tic, and Semantic Features with Maximum EntropyModels for Information Extraction.
In Proceedingsof ACL-04.Daniel Khashabi.
2013.
On the Recursive Neural Net-works for Relation Extraction and Entity Recogni-tion.
Technical Report (May, 2013), UIUC.Pavel Kuksa, Yanjun Qi, Bing Bai, Ronan Collobert,Jason Weston, Vladimir Pavlovic, and Xia Ning.2010.
Semi-Supervised Abstraction-AugmentedString Kernel for Multi-Level Bio-Relation Extrac-tion.
In Proceedings of the 2010 European Confer-ence on Machine Learning and Knowledge Discov-ery in Databases, Part II (ECML PKDD?10), pages128-144, 2010.Andrew L. Maas and Andrew Y. Ng.
2010.
A Proba-bilistic Model for Semantic Word Vectors.
In NIPSWorkshop on Deep Learning and Unsupervised Fea-ture Learning, 2010.Andriy Mnih and Geoffrey Hinton.
2007.
Three newGraphical Models for Statistical Language Mod-elling.
In Proceedings of ICML?07, pages 641-648,Corvallis, OR, 2007.Andriy Mnih and Geoffrey Hinton.
2009.
A ScalableHierarchical Distributed Language Model.
In NIPS,page 1081-1088.Truc-Vien T. Nguyen, Alessandro Moschitti, andGiuseppe Riccardi.
2009.
Convolution Kernels onConstituent, Dependency and Sequential Structuresfor Relation Extraction.
In Proceedings of EMNLP09, pages 1378-1387, Stroudsburg, PA, USA.73Barbara Plank and Alessandro Moschitti.
2013.
Em-bedding Semantic Similarity in Tree Kernels for Do-main Adaptation of Relation Extraction.
In Proceed-ings of the ACL 2013, pages 1498-1507, Sofia, Bul-garia.Longhua Qian, Guodong Zhou, Qiaoming Zhu andPeide Qian.
2008.
Exploiting Constituent Dde-pendencies for Tree Kernel-based Semantic RelationExtraction.
In Proceedings of COLING, pages 697-704, Manchester.Tobias Schnabel and Hinrich Sch?utze.
2014.
FLORS:Fast and Simple Domain Adaptation for Part-of-Speech Tagging.
In Transactions of the Associa-tion for Computational Linguistics, 2 (2014), pages1526.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic Compo-sitionality through Recursive Matrix-Vector Spaces.In Proceedings EMNLP-CoNLL?12, pages 1201-1211, Jeju Island, Korea, July 2012.Ang Sun, Ralph Grishman, and Satoshi Sekine.
2011.Semi-supervised Relation Extraction with Large-scale Word Clustering.
In Proceedings of ACL-HLT, pages 521-529, Portland, Oregon, USA.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics (ACL?10), pages 384-394, Upp-sala, Sweden, July, 2010.Min Xiao and Yuhong Guo.
2013.
Domain Adaptationfor Sequence Labeling Tasks with a ProbabilisticLanguage Adaptation Model.
In Proceedings of the30th International Conference on Machine Learning(ICML-13), pages 293-301, 2013.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel Methods for RelationExtraction.
Journal of Machine Learning Research,3:10831106.Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou.2006.
A Composite Kernel to Extract Relations be-tween Entities with both Flat and Structured Fea-tures.
In Proceedings of COLING-ACL-06, pages825-832, Sydney.Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.2005.
Exploring various Knowledge in Relation Ex-traction.
In Proceedings of ACL?05, pages 427-434,Ann Arbor, USA, 2005.74
