Unsupervised Learning of Word-Category Guessing RulesAndre i  MikheevHCRC Language Techno logy GroupUn ivers i ty  of Ed inburgh2 Bucc leuch P laceEd inburgh  EH8 9LW, Scot land,  UK: Andrei.Mikheev~ed.ac.ukAbstractWords unknown to the lexicon present asubstantial problem to part-of-speech tag-ging.
In this paper we present a techniquefor fully unsupervised statistical acquisi-tion of rules which guess possible parts-of-speech for unknown words.
Three com-plementary sets of word-guessing rules areinduced from the lexicon and a raw cor-pus: prefix morphological rules, suffix mor-phological rules and ending-guessing rules.The learning was performed on the BrownCorpus data and rule-sets, with a highlycompetitive performance, were producedand compared with the state-of-the-art.1 IntroductionWords unknown to the lexicon present a substan-tial problem to part-of-speech (POS) tagging of real-world texts.
Taggers assign a single POS-tag to aword-token, provided that it is known what parts-of-speech this word can take on in principle.
So, firstwords are looked up in the lexicon.
However, 3 to5% of word tokens are usually missing in the lex-icon when tagging real-world texts.
This is whereword-Pos guessers take their place - -  they employthe analysis of word features, e.g.
word leading andtrailing characters, to figure out its possible POS cat-egories.
A set of rules which on the basis of endingcharacters of unknown words, assign them with setsof possible POS-tags is supplied with the Xerox tag-ger (Kupiec, 1992).
A similar approach was takenin (Weischedel t al., 1993) where an unknown wordwas guessed given the probabilities for an unknownword to be of a particular POS, its capitalisation fea-ture and its ending.
In (Brill, 1995) a system of ruleswhich uses both ending-guessing and more morpho-logically motivated rules is described.
The best ofthese methods are reported to achieve 82-85% oftagging accuracy on unknown words, e.g.
(Brill,1995; Weischedel et al, 1993).The major topic in the development of word-Posguessers is the strategy which is to be used for theacquisition of the guessing rules.
A rule-based tag-ger described in (Voutilainen, 1995) is equipped witha set of guessing rules which has been hand-craftedusing knowledge of English morphology and intu-ition.
A more appealing approach is an empiri-cal automatic acquisition of such rules using avail-able lexical resources.
In (Zhang&Kim, 1990) asystem for the automated learning of morphologi-cal word-formation rules is described.
This systemdivides a string into three regions and from train-ing examples infers their correspondence to under-lying morphological features.
Brill (Brill, 1995) out-lines a transformation-based l arner which learnsguessing rules from a pre-tagged training corpus.A statistical-based suffix learner is presented in(Schmid, 1994).
From a pre-tagged training cor-pus it constructs the suffix tree where every suf-fix is associated with its information measure.
Al-though the learning process in these and some othersystems is fully unsupervised and the accuracy ofobtained rules reaches current state-of-the-art, theyrequire specially prepared training data - -  a pre-tagged training corpus, training examples, etc.In this paper we describe a new fully automatictechnique for learning part-of-speech guessing rules.This technique does not require specially preparedtraining data and employs fully unsupervised statis-tical learning using the lexicon supplied with the tag-ger and word-frequencies obtained from a raw cor-pus.
The learning is implemented as a two-stagedprocess with feedback.
First, setting certain param-eters a set of guessing rules is acquired, then it isevaluated and the results of evaluation are used forre-acquisition of a better tuned rule-set.2 Guessing Rules AcquisitionAs was pointed out above, one of the requirements inmany techniques for automatic learning of part-of-speech guessing rules is specially prepared trainingdata - -  a pre-tagged training corpus, training ex-amples, etc.
In our approach we decided to reusethe data which come naturally with a tagger, viz.the lexicon.
Another source of information whichis used and which is not prepared specially for thetask is a text corpus.
Unlike other approaches wedon't require the corpus to be pre-annotated butuse it in its raw form.
In our experiments we usedthe lexicon and word-frequencies derived from the327Brown Corpus (Francis&Kucera, 1982).
There area number of reasons for choosing the Brown Cor-pus data for training.
The most important ones arethat the Brown Corpus provides a model of generalmulti-domain language use, so general language reg-ularities can be induced from it, and second, manytaggers come with data trained on the Brown Cor-pus which is useful for comparison and evaluation.This, however, by no means restricts the describedtechnique to that or any other tag-set, lexicon orcorpus.
Moreover, despite the fact that the train-ing is performed on a particular lexicon and a par-ticular corpus, the obtained guessing rules supposeto be domain and corpus independent and the onlytraining-dependent feature is the tag-set in use.The acquisition of word-Pos guessing rules is athree-step rocedure which includes the rule extrac-tion, rule scoring and rule merging phases.
At therule extraction phase, three sets of word-guessingrules (morphological prefix guessing rules, morpho-logical suffix guessing rules and ending-guessingrules) are extracted from the lexicon and cleanedfrom coincidental cases.
At the scoring phase, eachrule is scored in accordance with its accuracy ofguessing and the best scored rules are included intothe final rule-sets.
At the merging phase, rules whichhave not scored high enough to be included into thefinal rule-sets are merged into more general rules,then re-scored and depending on their score addedto the final rule-sets.2 .1 Ru le  Ext ract ion  Phase2.1.1 Extraction of Morphological Rules.Morphological word-guessing rules describe howone word can be guessed given that another word isknown.
For example, the rule: \[un (VBD VBN) (JJ)\]says that prefixing the string "un" to a word, whichcan act as past form of verb (VBD) and participle(VBN), produces an adjective (J J).
For instance, byapplying this rule to the word "undeveloped", wefirst segment he prefix "un" and if the remainingpart "developed" is found in the lexicon as (VBDVBN), we conclude that the word "undeveloped" isan adjective (JJ).
The first POS-set in a guessingrule is called the initial class (/-class) and the POS-set of the guessed word is called the resulting class(R-class).
In the example above (VBD VBN) is the/-class of the rule and (J~) is the R-class.In English, as in many other languages, morpho-logical word formation is realised by affixation: pre-fixation and suffixation.
Although sometimes the af-fixation is not just a straightforward concatenationof the affix with the stem 1, the majority of casesclearly obey simple concatenative r gularities.
So,we decided first to concentrate only on simple con-catenative cases.
There are two kinds of morpho-logical rules to be learned: suffix rules (A') - -  ruleswhich are applied to the tail of a word, and prefixrules (A p) - -  rules which are applied to the begin-ning of a word.
For example:1consider an example: try - tried.A s : \[ed (NN VB) ( J J  VBD VBN)\]says that if by stripping the suffix "ed" from anunknown word we produce a word with the POS-class(NN VB), the unknown word is of the class - (JJVBD VBN).
This rule works, for instance, for \[book--*booked\], [water---*watered\], etc.
To extract such rulesa special operator V is applied to every pair of wordsfrom the lexicon.
It tries to segment an affix by left-most string subtraction for suffixes and rightmoststring subtraction for prefixes.
If the subtractionresults in an non-empty string it creates a morpho-logical rule by storing the POS-class of the shorterword as the /-class and the POs-class of the longerword as the R-class.
For example:\[booked (J J  VBD VBN)I ~7 \[book (NN VB)\] --~A '  : \[ed (NN VB) (:IJ VBD VBN)\]\[undeveloped ( J J ) l  ~7 \[developed (VBD VBN)\] --+A p : \[un (VBD VBN) (JJ)lThe ~7 operator is applied to all possible lexicon-entry pairs and if a rule produced by such an applica-tion has already been extracted from another pair,its frequency count (f)  is incremented.
Thus twodifferent sets of guessing rules - -  prefix and suffixmorphological rules together with their frequencies- -  are produced.
Next, from these sets of guess-ing rules we need to cut out infrequent rules whichmight bias the further learning process.
To do thatwe eliminate all the rules with the frequency f lessthan a certain threshold 82.
Such filtering reducesthe rule-sets more than tenfold and does not leaveclearly coincidental cases among the rules.2.1.2 Ext ract ion  o f  End ing  Guess ing  Rules.Unlike morphological guessing rules, ending-guessing rules do not require the main form of anunknown word to be listed in the lexicon.
Theserules guess a POs-class for a word just on the ba-sis of its ending characters and without looking upits stem in the lexicon.
Such rules are able to covermore unknown words than morphological guessingrules but their accuracy will not be as high.
Forexample, an ending-guessing ruleA~: \[in s - (JJ NN VBG) \ ]says that if a word ends with "ing" it can be anadjective, a noun or a gerund.
Unlike a morphologi-cal rule, this rule does not ask to check whether thesubstring preceeding the "ing"-ending is a word witha particular POS-tag.
Thus an ending-guessing rulelooks exactly like a morphological rule apart fromthe/-class which is always void.To collect such rules we set the upper limit onthe ending length equal to five characters and thuscollect from the lexicon all possible word-endings oflength 1, 2, 3, 4 and 5, together with the POs-classesof the words where these endings were detected toappear.
This is done by the operator /X.
For ex-ample, from the word \[different (JJ)\] the/% operatorwill produce five ending-guessing rules: It - (ss)\]~ \[nt" (J:\])l; \ [ent-  (Ji\])\]; \ [ rent -  (J:I)\]; \ [erent-  (:l J)\].
The Aoperator is applied to each entry in the lexicon in the2usua/ly we set this threshold quite low: 2-4.328way described for the ~7 operator of the morpholog-ical rules and then infrequent rules with f < 0 arefiltered out.2.2 Rule Scoring PhaseOf course, not all acquired rules are equally goodas plausible guesses about word-classes: some rulesare more accurate in their guessings and some rulesare more frequent in their application.
So, for everyacquired rule we need to estimate whether it is aneffective rule which is worth retaining in the finalrule-set.
For such estimation we perform a statisticalexperiment as follows: for every rule we calculatethe number of times this rule was applied to a wordtoken from a raw corpus and the number of times itgave the right answer.
Note that the task of the ruleis not to disambiguate a word's POS but to provideall and only possible POSs it can take on.
If the ruleis correct in the majority of times it was applied itis obviously a good rule.
If the rule is wrong mostof the times it is a bad rule which should not beincluded into the final rule-set.To perform this experiment we take one-by-oneeach rule from the rule-sets produced at the rule ex-traction phase, take each word token from the cor-pus and guess its POS-set using the rule if the ruleis applicable to the word.
For example, if a guess-ing rule strips a particular suffix and a current wordfrom the corpus does not have this suffix, we classifythese word and rule as incompatible and the rule asnot applicable to that word.
If the rule is applicableto the word we perform look-up in the lexicon forthis word and then compare the result of the guesswith the information listed in the lexicon.
If theguessed POS-set is the same as the POS-set stated inthe lexicon, we count it as success, otherwise it isfailure.
The value of a guessing rule, thus, closelycorrelates with its estimated proportion of success/p~l which is the proportion of all positive outcomesof the rule application to the total number of thetrials (n), which are, in fact, attempts to apply therule to all the compatible words in the corpus.
Wealso smooth/3 so as not to have zeros in.positive ornegative outcome probabilities: 15 = ~.~i~/3 estimate is a good indicator of rule accuracy.However, it frequently suffers from large estimationerror due to insufficient raining data.
For example,i fa rule was detected to work just twice and the totalnumber of observations was also two, its estimate/3 isvery high (1, or 0.83 for the smoothed version) butclearly this is not a very reliable estimate becauseof the tiny size of the sample.
Several smoothingmethods have been proposed to reduce the estima-tion error.
For different reasons all these smoothingmethods are not very suitable in our case.
In ourapproach we tackle this problem by calculating thelower confidence l imit 7r L for the rule estimate.
Thiscan be seen as the minimal expected value of/3 forthe rule if we were to draw a large number of sam-ples.
Thus with certain confidence ~ we can assumethat if we used more training data, the rule estimate/3 would be no worse than the ~L limit.
The lowerconfidence limit 7r L is calculated as:~rL =/3 -- Z(l-~)/2 * sp =/3 -- z(t-~)/2 *This function favours the rules with higher esti-mates obtained over larger samples.
Even if onerule has a high estimate but that estimate was ob-tained over a small sample, another ule with a lowerestimate but over a large sample might be valuedhigher.
Note also that since/3 itself is smoothed wewill not have zeros in positive (/3) or negative (1 -/3)outcome probabilities.
This estimation of the rulevalue in fact resembles that used by (Tzoukermannet al, 1995) for scoring pos-disambiguation rules forthe French tagger.
The main difference between thetwo functions is that there the z value was implic-itly assumed to be 1 which corresponds to the con-fidence of 68%.
A more standard approach is toadopt a rather high confidence value in the rangeof 90-95%.
We adopted 90% confidence for whichz(1-0.90)/2 = z0.05 = 1.65.
Thus we can calculatethe score for the ith rule as: /3i - 1.65 * ~/P~(QP')Another important consideration for scoring aword-guessing rule is that the longer the affix or end-ing of the rule the more confident we are that it isnot a coincidental one, even on small samples.
Forexample, if the estimate for the word-ending "o" wasobtained over a sample of 5 words and the estimatefor the word-ending "fulness" was also obtained overa sample of 5 words, the later case is more represen-tative even though the sample size is the same.
Thuswe need to adjust the estimation error in accordancewith the length of the affix or ending.
A good wayto do that is to divide it by a value which increasesalong with the increase of the length.
After severalexperiments we obtained:scorei =/3i - 1.65 * ~ / ( 1  + log(IS, I))When the length of the affix or ending is 1 theestimation error is not changed since log(l) is 0.
Forthe rules with the affix or ending length of 2 the es-timation error is reduced by 1 + log(2) = 1.3, forthe length 3 this will be 1 + log(3) -- 1.48, etc.
Thelonger the length the smaller the sample which wiltbe considered representative enough for a confidentrule estimation.
Setting the threshold 0~ at a cer-tain level lets only the rules whose score is higherthan the threshold to be included into the final rule-sets.
The method for setting up this threshold isbased on empirical evaluations of the rule-sets andis described in Section 3.2 .3  Ru le  Merg ing  PhaseRules which have scored lower than the threshold0s can be merged into more general rules which ifscored above the threshold are also included into thefinal rule-sets.
We can merge two rules which havescored below the threshold and have the same affix329(or ending) and the initial class (/)3.
The score ofthe resulting rule will be higher than the scores ofthe merged rules since the number of positive ob-servations increases and the number of the trials re-mains the same.
After a successful application ofthe merging, the resulting rule substitutes the twomerged ones.
To perform such rule-merging overa rule-set, first, the rules which have not been in-cluded into the final set are sorted by their scoreand best-scored rules are merged first.
This is donerecursively until the score of the resulting rule doesnot exceed the threshold in which case it is addedto the final rule-set.
This process is applied until nomerges can be done to the rules which have scoredbelow the threshold.3 Direct Evaluation StageThere are two important questions which arise atthe rule acquisition stage - how to choose the scoringthreshold 0, and what is the performance of the rule-sets produced with different hresholds.
The task ofassigning a set of Pos-tags to a word is actually quitesimilar to the task of document categorisation wherea document should be assigned with a set of descrip-tors which represent its contents.
The performanceof such assignment can be measured in:reca l l -  the percentage of BOSs which were assignedcorrectly by the guesser to a word;prec is ion -  the percentage of BOSs the guesser as-signed correctly over the total number of BOSs itassigned to the word;coverage  - the proportion of words which theguesser was able to classify, but not necessarily cor-rectly;In our experiments we measured word precisionand word recall (micro-average).
There were twotypes of data in use at this stage.
First, we eval-uated the guessing rules against he actual lexicon:every word from the lexicon, except for closed-classwords and words shorter than five characters 4, wasguessed by the different guessing strategies and theresults were compared with the information the wordhad in the lexicon.
In the other evaluation experi-ment we measured the performance of the guessingrules against he training corpus.
For every word wecomputed its metrics exactly as in the previous ex-periment.
Then we multiplied these results by thecorpus frequency of this particular word and aver-aged them.
Thus the most frequent words had thegreatest influence on the aggreagte measures.First, we concentrated on finding the best thresh-olds 08 for the rule-sets.
To do that for each rule-setproduced using different hresholds we recorded thethree metrics and chose the set with the best aggre-gate.
In Table 1 some results of that experimentare shown.
The best thresholds were detected: forending rules - 75 points, for suffix rules - 60, and for3For ending-guessing rules this is always true, so onlythe ending itself counts.4the actual size of the filtered lexicon was 47,659 en-tries out of 53,015 entries of the original exicon.prefix rules - 80.
One can notice a slight differencein the results obtained over the lexicon and the cor-pus.
The corpus results are better because the train-ing technique xplicitly targeted the rule-sets to themost frequent cases of the corpus rather than thelexicon.
In average ending-guessing rules were de-tected to cover over 96% of the unknown words.
Theprecision of 74% roughly can be interpreted as thatfor words which take on three different BOSs in theirBOs-class, the ending-guessing rules will assign four,but in 95% of the times (recall) the three requiredBOSs will be among the four assigned by the guess.In comparison with the Xerox word-ending uessertaken as the base-line model we detect a substantialincrease in the precision by about 22% and a cheerfulincrease in coverage by about 6%.
This means thatthe Xerox guesser creates more ambiguity for thedisambiguator, assigning five instead of three BOSsin the example above.
It can also handle 6% lessunknown words which, in fact, might decrease itsperformance ven lower.
In comparison with theending-guessing rules, the morphological rules havemuch better precision and hence better accuracy ofguessing.
Virtually almost every word which can beguessed by the morphological rules is guessed ex-actly correct (97% recall and 97% precision).
Notsurprisingly, the coverage of morphological rules ismuch lower than that of the ending-guessing ones -for the suffix rules it is less than 40% and for theprefix rules about 5-6%.After obtaining the optimal rule-sets we per-formed the same experiment on a word-sample whichwas not included into the training lexicon and cor-pus.
We gathered about three thousand words fromthe lexicon developed for the Wall Street Journalcorpus 5 and collected frequencies of these words inthis corpus.
At this experiment we obtained simi-lar metrics apart from the coverage which droppedabout 0.5% for Ending 75 and Xerox rule-sets and7% for the Suffix 60 rule-set.
This, actually, did notcome as a surprise, since many main forms requiredby the suffix rules were missing in the lexicon.In the next experiment we evaluated whether themorphological rules add any improvement if they areused in conjunction with the ending-guessing rules.We also evaluated in detail whether a conjunctiveapplication with the Xerox guesser would boost theperformance.
As in the previous experiment we mea-sured the precision, recall and coverage both on thelexicon and on the corpus.
Table 2 demonstratessome results of this experiment.
The first part ofthe table shows that when the Xerox guesser is ap-plied before the E75 guesser we measure a drop inthe performance.
When the Xerox guesser is appliedafter the E75 guesser no sufficient changes to the per-formance are noticed.
This actually proves that theE75 rule-set fully supercedes the Xerox rule-set.
Thesecond part of the table shows that the cascadingapplication of the morphological rule-sets togetherwith the ending-guessing rules increases the over-5these words were not listed in the training lexicon330all precision of the guessing by a further 5%.
Thismakes the improvements against he base-line Xeroxguesser 28% in precision and 7% in coverage.4 Tagging Unknown WordsThe direct evaluation of the rule-sets gave us thegrounds for the comparison and selection of the bestperforming uessing rule-sets.
The task of unknownword guessing is, however, a subtask of the overallpart-of-speech tagging process.
Thus we are mostlyinterested in how the advantage of one rule-set overanother will affect the tagging performance.
So, weperformed an independent evaluation of the impactof the word guessers on tagging accuracy.
In thisevaluation we tried two different taggers.
First, weused a tagger which was a c++ re-implementationof the LISP implemented HMM Xerox tagger de-scribed in (Kupiec, 1992).
The other tagger was therule-based tagger of Brill (Brill, 1995).
Both of thetaggers come with data and word-guessing compo-nents pre-trained on the Brown Corpus 6.
This, ac-tually gave us the search-space of four combinations:the Xerox tagger equipped with the original Xe-rox guesser, Brill's tagger with its original guesser,the Xerox tagger with our cascading Ps0+S60+E75guesser and Brill's tagger with the cascading uesser.For words which failed to be guessed by the guess-ing rules we applied the standard method of classi-fying them as common nouns (NN) if they are notcapitalised inside a sentence and proper nouns (NP)otherwise.
As the base-line result we measured theperformance of the taggers with all known words onthe same word sample.In the evaluation of tagging accuracy on unknownwords we pay attention to two metrics.
First wemeasure the accuracy of tagging solely on unknownwords:UnkownScore  = Correctl~Ta$\[ ledUnkownWordsTota lUnknown WordsThis metric gives us the exact measure of how thetagger has done on unknown words.
In this case,however, we do not account for the known wordswhich were mis-tagged because of the guessers.
Toput a perspective on that aspect we measure theoverall tagging performance:TotalScore = Correct lyTagsedWordsTotaIWordsSince the Brown Corpus model is a general an-guage model, it, in principle, does not put restric-tions on the type of text it can be used for, althoughits performance might be slightly lower than that ofa model specialised for this particular sublanguage.Here we want to stress that our primary task was notto evaluate the taggers themselves but rather theirperformance with the word-guessing modules.
So wedid not worry too much about tuning the taggers forthe texts and used the Brown Corpus model instead.We tagged several texts of different origins, exceptfrom the Brown Corpus.
These texts were not seenat the training phase which means that neither the6Since Brill's tagger was trained on the Penn tag-set(Marcus et al, 1993) we provided an additional mapping.331taggers nor the guessers had been trained on thesetexts and they naturally had words unknown to thelexicon.
For each text we performed two taggingexperiments.
In the first experiment we tagged thetext with the Brown Corpus lexicon supplied withthe taggers and hence had only those unknown wordswhich naturally occur in this text.
In the second ex-periment we tagged the same text with the lexiconwhich contained only closed-class 7 and short 8 words.This small lexicon contained only 5,456 entries outof 53,015 entries of the original Brown Corpus lex-icon.
All other words were considered as unknownand had to be guessed by the guessers.We obtained quite stable results in these experi-ments.
Here is a typical example of tagging a text of5970 words.
This text was detected to have 347 un-known words.
First, we tagged the text by the fourdifferent combinations of the taggers with the word-guessers using the full-fledged lexicon.
The resultsof this tagging are summarised in Table 3.
When us-ing the Xerox tagger with its original guesser, 63 un-known words were incorrectly tagged and the accu-racy on the unknown words was measured at 81.8%.When the Xerox tagger was equipped with our cas-cading guesser its accuracy on unknown words in-creased by almost 9% upto 90.5%.
The same situa-tion was detected with Brill's tagger which in generalwas slightly more accurate than the Xerox one 9.
Thecascading uesser performed better than Brill's orig-inal guesser by about 8% boosting the performanceon the unknown words from 84.5% 1?
to 92.2%.
Theaccuracy of the taggers on the set of 347 unknownwords when they were made known to the lexiconwas detected at 98.5% for both taggers.In the second experiment we tagged the same textin the same way but with the small lexicon.
Out of5,970 words of the text, 2,215 were unknown to thesmall lexicon.
The results of this tagging are sum-marised in Table 4.
The accuracy of the taggerson the 2,215 unknown words when they were madeknown to the lexicon was much lower than in theprevious experiment - -  90.3% for the Xerox taggerand 91.5% for Brill's tagger.
Naturally, the perfor-mance of the guessers was also lower than in theprevious experiment plus the fact that many "semi-closed" class adverbs like "however", "instead", etc.,were missing in the small lexicon.
The accuracy ofthe tagging on unknown words dropped by about5% in general.
The best results on unknown wordswere again obtained on the cascading uesser (86%-87.45%) and Brill's tagger again did better then theXerox one by 1.5%.Two types of mis-taggings caused by the guessers7articles, prepositions, conjunctions, etc.Sshorter than 5 characters9This, however, was not an entirely fair comparisonbecause of the differences in the tag-sets in use by thetaggers.
The Xerox tagger was trained on the originalBrown Corpus tag-set which makes more distinctions be-tween categories than the Penn Brown Corpus tag-set.1?This figure agrees with the 85% quoted by Brill(Brin, 1994).occured.
The first type is when guessers providedbroader POS-classes for unknown words and the tag-ger had difficulties with the disambiguation of suchbroader classes.
This is especially the case with the"ing" words which, in general, can act as nouns, ad-jectives and gerunds and only direct lexicalizationcan restrict the search space, as in the case withthe word "going" which cannot be an adjective butonly a noun and a gerund.
The second type of mis-tagging was caused by wrong assignments ofBOSS bythe guesser.
Usually this is the case with irregularwords like, for example, "cattle" which was wronglyguessed as a singular noun (NN) but in fact is aplural noun (NNS).5 Discuss ion and Conclus ionWe presented a technique for fully unsupervisedstatistical acquisition of rules which guess possibleparts-of-speech for words unknown to the lexicon.This technique does not require specially preparedtraining data and uses for training the lexicon andword frequencies collected from a raw corpus.
Us-ing these training data three types of guessing rulesare learned: prefix morphological rules, suffix mor-phological rules and ending-guessing rules.
To selectbest performing uessing rule-sets we suggested anevaluation methodology, which is solely dedicated tothe performance of part-of-speech guessers.Evaluation of tagging accuracy on unknown wordsusing texts unseen by the guessers and the taggersat the training phase showed that tagging with theautomatically induced cascading uesser was consis-tently more accurate than previously quoted resultsknown to the author (85%).
The cascading uesseroutperformed the guesser supplied with the Xeroxtagger by about 8-9% and the guesser supplied withBrill's tagger by about 6-7%.
Tagging accuracy onunknown words using the cascading uesser was de-tected at 90-92% when tagging with the full-fledgedlexicon and 86-88% when tagging with the closed-class and short word lexicon.
When the unknownwords were made known to the lexicon the accu-racy of tagging was detected at 96-98% and 90-92%respectively.
This makes the accuracy drop causedby the cascading uesser to be less than 6% in gen-eral.
Another important conclusion from the evalua-tion experiments i  that the morphological guessingrules do improve the guessing performance.
Sincethey are more accurate than ending-guessing rulesthey are applied before ending-guessing rules andimprove the precision of the guessings by about 5%.This, actually, results in about 2% higher accuracyof tagging on unknown words.The acquired guessing rules employed in our cas-cading guesser are, in fact, of a standard natureand in that form or another are used in other POS-guessers.
There are, however, a few points whichmake the rule-sets acquired by the presented heretechnique more accurate:?
the learning of such rules is done from the lex-icon rather than tagged corpus, because theguesser's task is akin to the lexicon lookup;?
there is a well-tuned statistical scoring proce-dure which accounts for rule features and fre-quency distribution;?
there is an empirical way to determine an opti-mum collection of rules, since acquired rules aresubject o rigorous direct evaluation in terms ofprecision, recall and coverage;?
rules are applied cascadingly using the most ac-curate rules first.One of the most important issues in the inductionof guessing rule-sets is the choice right data for train-ing.
In our approach, guessing rules are extractedfrom the lexicon and the actual corpus frequenciesof word-usage then allow for discrimination betweenrules which are no longer productive (but have lefttheir imprint on the basic lexicon) and rules that areproductive in real-life texts.
Thus the major factorin the learning process is the lexicon.
Since guessingrules are meant o capture generM language regular-ities the lexicon should be as general as possible (listall possible POSs for a word) and as large as possi-ble.
The corresponding corpus should include mostof the words from the lexicon and be large enoughto obtain reliable estimates of word-frequency distri-bution.
Our experiments with the lexicon and wordfrequencies derived from the Brown Corpus, whichcan be considered as a generM model of English, re-sulted in guessing rule-sets which proved to be do-main and corpus independent 11, producing similarresults on test texts of different origin.Although in general the performance of the cas-cading guesser is only 6% worse than the lookup of ageneral anguage lexicon there is room for improve-ment.
First, in the extraction of the morphologicalrules we did not attempt to model non-concatenativecases.
In English, however, since most of letter mu-tations occur in the last letter of the main word it ispossible to account for it.
So our next goal is to ex-tract morphological rules with one letter mutationsat the end.
This would account for cases like "try -tries", "reduce - reducing", "advise - advisable".
Weexpect it to increase the coverage of thesuffix mor-phological rules and hence contribute to the overallguessing accuracy.
Another avenue for improvementis to provide the guessing rules with the probabilitiesof emission of POSs from their resulting POS-classes.This information can be compiled automatically andalso might improve the accuracy of tagging unknownwords.The described rule acquisition and evaluationmethods are implemented as a modular set of c++and AWK tools, and the guesser is easily extendableto sub-language specific regularities and retrainableto new tag-sets and other languages, provided thatthese languages have affixational morphology.
Boththe software and the produced guessing rule-sets areavailable by contacting the author.11but tag-set dependent3326 AcknowledgementsSome of the research reported here was fundedas part of EPSRC project IED4/1/5808 "IntegratedLanguage Database".
I would also like to thankChris Brew for helpful discussions on the issues re-lated to this paper.
"ReferencesE.
Brill 1994.
Some Advances in Transformation-Based Part of Speech Tagging.
In Proceedings ofthe Twelfth National Conference on Arlificial In-telligence (AAAAL94), Seattle, WA.E.
Brill 1995.
Transformation-based error-drivenlearning and Natural Language processing: a casestudy in part-of-speech tagging.
In ComputationalLinguistics 21(4) pp.
543-565.W.
Francis and H. Kucera 1982.
Frequency Analysisof English Usage.
Houghton Mifflin, Boston 1982.J.
Kupiec 1992.
Robust Part-of-Speech Tagging Us-ing a Hidden Markov Model.
In Computer Speechand LanguageM.
Marcus, M.A.
Marcinkiewicz, and B. Santorini1993.
Building a Large Annotated Corpus of En-glish: The Penn Treebank.
In Computational Lin-guistics, vol 19/2 pp.313-329H.
Schmid 1994.
Part of Speech Tagging with Neu-ral Networks.
In Proceedings of the InternationalConference on Computational Linguistics, pp.172-176, Kyoto, Japan.E.
Tzoukermann, D.R.
Radev, and W.A.
Gale 1995.Combining Linguistic Knowledge and StatisticalLearning in French Part of Speech Tagging.
InEACL SIGDAT Workshop, pp.51-59, Dublin, Ire-landA.
Voutilainen 1995.
A Syntax-Based Part-of-Speech Analyser In Proceedings of the Sev-enth Conference of European Chapter of the As-sociation for Computational Linguistics (EACL)pp.157-164, Dublin, IrelandR.
Weischedel, M. Meteer, R. Schwartz, L. Ramshawand J. Palmucci 1993.
Coping with ambiguity andunknown words through probabilistic models.
InComputational Linguistics, vol 19/2 pp.359-382Byoung-Tak Zhang and Yung-Taek Kim 1990.
Mor-phological Analysis and Synthesis by AutomatedDiscovery and Acquisition of Linguistic Rules.In Proceedings of the 13th International Confer-ence on Computational Linguistics, pp.431-435,Helsinki, Finland.333MeasureRecallPrecisionCoverageTestLexiconCorpusLexiconCorpusLexiconCorpusXerox0.9563130.9445260.4607610.5239650.9176980.893275Endin G 750.9457260.9520160.6751220.7453390.9770890.96104Suffix 600.957610.973520.9197960.9793510.375970.320996Prefix 800.9557480.9785150.9225340.9776330.0495580.058372Table 1: Results obtained at the evaluation of the acquired rule-sets over the training lexicon and thetraining corpus.
Guessing rule-sets produced using different confidence thresholds were compared.
Best-scored rule-sets detected: Prefix 80 - prefix morphological rules which scored over 80 points, Suffix 60 - suffixmorphological rules which scored over 60 points and Ending 75 - ending-guessing rules which scored over 75points.
As the base-line model was taken the ending guesser developed by Xerox (X).GuessingStrate~;yXerox (X)Ending 75 (E75)XWE75E75+XPs0+E75$6o+E75Ps0+S60+E75Lexicon"Precision Recall Coverage0.460761 0.956331 0.9176980.675122 0.945726 0.9770890.470249 0.95783 0.9898430.670741 0.943319 0.9898430.687126 0.946208 0.9774880.734143 0.945015 0.9796860.745504 0.945445 0.980086CorpusPrecision Recall Coverage0.523965 0.944526 0.8932750.745339 0.952016 0.961040.519715 0.949789 0.9690230.743932 0.951541 0.9690230.748922 0.951563 0.961040.792901 0.951015 0.9632890.796252 0.950562 0.963289Table 2: Results of the cascading application of the rule-sets over the training lexicon and training corpus.Ps0 - prefix rule-set scored over 80 points, $60 - suffix rule-set scored over 60 points, E75 - ending-guessingrule-set scored over 75 points.
As the base-line model was taken the ending guesser developed by Xerox (X).The first part of the table shows that the E75 rule-set outperforms and fully supercedes the Xerox rule-set.The second part of the table shows that the cascading application of the morphological rule-sets togetherwith the ending-guessing rules increases the performance by about 5% in precision.Tagger Guessing 'lbtal Unkn.
rlbtalstrategy words words mistag.Xerox Xerox 5,970 347 324Xerox Ps0 +S60 +E75 5,970 347 292Brill Brill 5,970 347 246Brill Ps0 +$60 +E75 5,970 347 219Unkn.
'lbtal Unkn.mistag.
Score Score63 94.3% 81.8%33 95.1% 90.5%54 95.9% 84.5%27 96.3% 92.2%Table 3: This table shows the results of tagging atext with 347 unknown words by four different combinationsof two taggers and three word-guessing modules using the Brown Corpus model.
The accuracy of taggingthe unknown words when they were made known to the lexicon was detected at 98.5% for both taggers.Tagger \] Guessing Total J Unkn.strategy words wordsXerox Xerox 5,970 2215Xerox Ps0 +$60 +E75 5,970 2215Brill Brill 5,970 2215Brill Ps0 +S60 +E7~ 5,970 2215Total Unkn.mistag, mistag.556 516332 309464 410327 287I Total Unkn.Score Score90.7% 76.7%94.44% 86.05%93.1% 81.5%94.52% 87.45%Table 4: This table shows the results of tagging the same as in Table 3 text by four different combinations oftwo taggers and three word-guessing modules using the Brown Corpus model and the lexicon which containedonly closed-class and short words.
The accuracy of tagging the unknown words when they were made knownto the lexicon was detected at 90.3% for the Xerox tagger and at 91.5% for Brill's tagger.334
