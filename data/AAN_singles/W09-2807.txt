Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 31?38,Suntec, Singapore, 6 August 2009.c?2009 ACL and AFNLPA Classification Algorithm for Predicting the Structure of SummariesHoracio SaggionUniversity of Sheffield211 Portobello StreetSheffield - S1 4DPUnited Kingdomhttp://www.dcs.shef.ac.uk/~saggionH.Saggion@dcs.shef.ac.ukAbstractWe investigate the problem of generatingthe structure of short domain independentabstracts.
We apply a supervised machinelearning approach trained over a set of ab-stracts collected from abstracting servicesand automatically annotated with a textanalysis tool.
We design a set of featuresfor learning inspired from past researchin content selection, information order-ing, and rhetorical analysis for trainingan algorithm which then predicts the dis-course structure of unseen abstracts.
Theproposed approach to the problem whichcombines local and contextual features isable to predict the local structure of the ab-stracts in just over 60% of the cases.1 IntroductionMani (2001) defines an abstract as ?a summaryat least some of whose material is not present inthe input?.
In a study of professional abstracting,Endres-Niggemeyer (2000) concluded that profes-sional abstractors produce abstracts by ?cut-and-paste?
operations, and that standard sentence pat-terns are used in their production.
Examples ofabstracts produced by a professional abstractor areshown in Figures 1 and 2.
They contain fragments?copied?
from the input documents together withphrases (underlined in the figures) inserted by theprofessional abstractors.
In a recent study in hu-man abstracting (restricted to the amendment ofauthors abstracts) Montesi and Owen (2007) notedthat professional abstractors prepend third personsingular verbs in present tense and without subjectto the author abstract, a phenomenon related ?
yetdifferent ?
from the problem we are investigatingin this paper.Note that the phrases or predicates prepended tothe selected sentence fragments copied from theinput document have a communicative function:Presents a model instructional session that was prepared andtaught by librarians to introduce college students, faculty,and staff to the Internet by teaching them how to join list-servs and topic- centered discussion groups.
Describes thesessions?
audience, learning objectives, facility, and coursedesign.
Presents a checklist for preparing an Internet instruc-tion session.Figure 1: Professional Abstracts with InsertedPredicates from LISA Abstracting ServiceTalks about Newsblaster, an experimental software tool thatscans and summarizes electronic news stories, developed byColumbia University?s Natural Language Processing Group.Reports that Newsblaster is a cross between a search en-gine and ...
Explains that Newsblaster publishes the sum-maries in a Web page that divides the day summaries into ....Mentions that Newsblaster is considered an aid to those whohave to quickly canvas large amounts of information frommany sources.Figure 2: Professional Abstract with InsertedPredicates from Internet & Personal ComputingAbstractsthey inform or alert the reader about the contentof the abstracted document by explicitly mark-ing what the author says or mentions, presents orintroduces, concludes, or includes, in her paper.Montesi and Owen (2007) observe that the revi-sion of abstracts is carried out to improve com-prehensibility and style and to make the abstractobjective.We investigate how to create the discoursestructure of the abstracts: more specifically we areinterested in predicting the inserted predicates orphrases and at which positions in the abstract theyshould be prepended.Abstractive techniques in text summarizationinclude sentence compression (Cohn and Lapata,2008), headline generation (Soricut and Marcu,2007), and canned-based generation (Oakes andPaice, 2001).
Close to the problem studied hereis Jing and McKeown?s (Jing and McKeown,2000) cut-and-paste method founded on Endres-Niggemeyer?s observations.
The cut-and-paste31method includes such operations as sentence trun-cation, aggregation, specialization/generalization,reference adjustment and rewording.
None ofthese operations account for the transformationsobserved in the abstracts of Figures 1 and 2.
Theformulaic expressions or predicates inserted in theabstract ?glue?
together the extracted fragments,thus creating the abstract?s discourse structure.To the best of our knowledge, and with the ex-ception of Saggion and Lapalme (2002) indicativegeneration approach which included operations toadd extra linguistic material to generate an indica-tive abstract, the work presented here is the firstto investigate this relevant operation in the field oftext abstracting and to propose a robust computa-tional method for its simulation.In this paper we are interested in the processof generating the structure of the abstract by au-tomatic means.
In order to study this problem, wehave collected a corpus of abstracts written by ab-stractors; we have designed an algorithm for pre-dicting the structure; implemented the algorithm;and evaluated the structure predicted by the auto-matic system against the true structure.2 Problem Specification, Data Collection,and AnnotationThe abstracts we study in this research follow thepattern:Abstract??ni=1Predi?
?iwhere Prediis a phrase used to introduce the ?con-tent?
?iof sentence i, n is the number of sentencesin the abstract,?indicates multiple concatena-tion, and X ?Y indicates the concatenation of Xand Y .
In this paper we concentrate only on this?linear?
structure, we plan to study more complex(e.g., tree-like representations) in future work.The problem we are interested in solving is thefollowing: given sentence fragments ?iextractedfrom the document, how to create the Abstract.Note that if N is the number of different phrases(Predi) used in the model, then a priori there areNnpossible discourse structures to select from forthe abstract, generating all possibilities and select-ing the most appropriate would be impractical.
Wepresent an algorithm that decides which predicateor phrase is most suitable for each sentence, do-ing this by considering the sentence content andthe abstract generated so far.
For the experimentsto be reported in this paper, the discourse structureof the abstracts is created using predicates or ex-pressions learned from a corpus a subset of whichis shown in Table 1.We have collected abstracts from variousdatabases including LISA, ERIC, and Internet& Personal Computing Abstracts, using our in-stitutional library?s facilities and the abstracts?providers?
keyword search facilities.
Electroniccopies of the abstracted documents can also beaccessed through our institution following a link,thus allowing us to check abstracts against ab-stracted document (additional information on theabstracts is given in the Appendix).2.1 Document Processing and AnnotationEach electronic version of the abstract was pro-cessed using the freely available GATE text analy-sis software (Cunningham et al, 2002).
First eachabstract was analyzed by a text structure analy-sis program to identify meta-data such as title, au-thor, source document, the text of the abstract, etc.Each sentence in the abstract was stripped fromthe predicate or phrase inserted by the abstractor(e.g., ?Mentions that?, ?Concludes with?)
and anormalised version of the expression was used toannotate the sentence, in a way similar to the ab-stracts in Figures 1 and 2.
After this each abstractand document title was tokenised, sentence split-ted, part-of-speech tagged, and morphologicallyanalyzed.
A rule-based system was used to carryout partial, robust syntactic and semantic analy-sis of the abstracts (Gaizauskas et al, 2005) pro-ducing predicate-argument representations wherepredicates which are used to represent entities arecreated from the morphological roots of nouns orverbs in the text (unary predicates) and predicateswith are used to represent binary relations are aclosed set of names representing grammatical re-lations such as the verb logical object, or the verblogical subject or a prepositional attachment, etc.This predicate-argument structure representationwas further analysed in order to extract ?seman-tic?
triples which are used in the experiments re-ported here.
Output of this analysis is shown inFigure 3.
Note that the representation also con-tains the tokens of the text, their parts of speech,lemmas, noun phrases, verb phrases, etc.3 Proposed SolutionOur algorithm (see Algorithm 1) takes as in-put an ordered list of sentence fragments obtainedfrom the source document and decides how to?paste?
the fragments together into an abstract;32to address; to add; to advise; to assert; to claim; to comment; to compare; to conclude; to define; todescribe; to discuss; to evaluate; to examine; to explain; to focus; to give; to highlight; to include;to indicate; to note; to observe; to overview; to point out; to present; to recommend; to report; tosay; to show; to suggest; ...to report + to indicate + to note + to declare + to include; to provide + to explain + to indicate +to mention; to point out + to report + to mention + to include; to discuss + to list + to suggest +to conclude; to present + to say + to add + to conclude + to contain; to discuss + to explain + torecommend; to discuss + to cite + to say; ...Table 1: Subset of predicates or expressions used by professional abstractors and some of the discoursestructures used.Sentence: Features a listing of ten family-oriented pro-grams, including vendor information, registration fees, anda short review of each.Representation: listing-det-a; listing-of-program; family-oriented-adj-program; fee-qual-registration; information-qual-vendor; listing-apposed-information; ...Figure 3: Sentence Representation (partial)Algorithm 1 Discourse Structure Prediction Al-gorithmGiven: a list of n sorted text fragments ?ibeginAbstract?
??;Context?
START;for all i : 0?
i?
n?1; doPred?
PredictPredicate(Context,?i);Abstract?
Abstract?Pred??i?
?.?;Context?
ExtractContext(Abstract);end forreturn Abstractendat each iteration the algorithm selects the ?best?available phrase or predicate to prepend to the cur-rent fragment from a finite vocabulary (inducedfrom the analysed corpus) based on local andcontextual information.
One could rely on ex-isting trainable sentence selection (Kupiec et al,1995) or even phrase selection (Banko et al, 2000)strategies to pick up appropriate ?i?s from the doc-ument to be abstracted and rely on recent informa-tion ordering techniques to sort the ?ifragments(Lapata, 2003).
This is the reason why we only ad-dress here the discourse structure generation prob-lem.3.1 Predicting Discourse Structure asClassificationThere are various possible ways of predicting whatexpression to insert at each point in the genera-tion process (i.e., the PredictPredicate functionin Algorithm 1).
In the experiments reported herewe use a classification algorithm based on lexical,syntactic, and discursive features, which decideswhich of the N possible available phrases is mostsuitable.
The algorithm is trained over the anno-tated abstracts and used to predict the structure ofunseen test abstracts.Where the classification algorithm is concerned,we have decided to use Support Vector Machineswhich have recently been used in different tasksin natural language processing, they have beenshown particularly suitable for text categorization(Joachims, 1998).
We have tried other machinelearning algorithms such as Decision Trees, NaiveBayes Classification, and Nearest Neighbor fromthe Weka toolkit (Witten and Frank, 1999), but thesupport vector machines gave us the best classifi-cation accuracy (a comparison with Naive Bayeswill be presented in Section 4).The features used for the experiments reportedhere are inspired by previous work in text summa-rization on content selection (Kupiec et al, 1995),rhetorical classification (Teufel and Moens, 2002),and information ordering (Lapata, 2003).
Thefeatures are extracted from the analyzed abstractswith specialized programs.
In particular we usepositional features (position of the predicate to begenerated in the structure), length features (num-ber of words in the sentence), title features (e.g.,presence of title words in sentence), content fea-tures computed as the syntactic head of noun andverb phrases, semantic features computed as the33to add; to conclude; to contain; to describe; todiscuss; to explain; to feature; to include; to indi-cate; to mention; to note; to point out; to present;to provide; to report; to sayTable 2: Predicates in the reduced corpusarguments of ?semantic?
triples (Section 2.1) ex-tracted from the parsed abstracts.
Features occur-ring less than 4 times in the corpus were removedfor the experiments.
For each sentence, a cohe-sion feature is also computed as the number ofnouns in common with the previous sentence frag-ment (or title if first sentence).
Cohesion infor-mation has been used in rhetorical-based parsingfor summarization (Marcu, 1997) in order to de-cide between ?list?
or ?elaboration?
relations andalso in content selection for summarization (Barzi-lay and Elhadad, 1997).
For some experimentswe also use word-level information (lemmas) andpart-of-speech tags.
For some of the experimentsreported here the variable Context at iteration i inAlgorithm 1 is instantiated with the predicates pre-dicted at iterations i?1 and i?2.4 Experiments and ResultsThe experiments reported here correspond to theuse of different features as input for the classifier.In these experiments we have used a subset of thecollected abstracts, they contain predicates whichappeared at least 5 times in the corpus.
With thisrestriction in place the original set of predicatesused to create the discourse structure is reduced tosixteen (See Table 2), however, the number of pos-sible structures in the reduced corpus is still con-siderable with a total of 179 different structures.In the experiments we compare several classi-fiers:?
Random Generation selects a predicate atrandom at each iteration of the algorithm;?
Predicate-based Generation is a SVM classi-fier which uses the two previous predicates togenerate the current predicate ignoring sen-tence content;?
Position-based Generation is a SVM classi-fier which also ignores sentence content butuses as features for classification the absoluteposition of the sentence to be generated;Configuration Avg.AccRandom Generation 10%Predicate-based Generation 35%Position-based Generation 38%tf*idf-based Generation 55%Summarization-based Generation 60%Table 3: Average accuracy of different classifica-tion configurations.?
tf*idf-based Generation is a SVM classifierwhich uses lemmas of the sentence fragmentto be generated to pick up one predicate (notethat position features and predicates wereadded to the mix without improving the clas-sifier);?
Summarization-based Generation is a SVMwhich uses the summarization and discoursefeatures discussed in the previous section in-cluding contextual information (Predi?2andPredi?1?
with special values when i = 0 andi = 1).We measure the performance of each instanceof the algorithm by comparing the predicted struc-ture against the true structure.
We compute twometrics: (i) accuracy at the sentence level (as inclassification), which is the proportion of predi-cates which were correctly generated; and (ii) ac-curacy at the textual level, which is the proportionof abstracts correctly generated.
For the latter wecompute the proportion of abstracts with zero er-rors, less than two errors, and less than three er-rors.For every instance of the algorithm we performa cross-validation experiment, selecting for eachexperiment 20 abstracts for testing and the rest ofthe abstracts for training.
Accuracy measures atsentence and text levels are averages of the cross-validation experiments.Results of the algorithms are presented in Ta-bles 3 and 4.
Random generation has very poorperformance with only 10% local accuracy andless than 1% of full correct structures.
Knowledgeof the predicates selected for previous sentencesimproves performance over the random system(35% local accuracy and 5% of full correct struc-tures predicted).
As in previous summarizationstudies, position proved to contribute to the task:the positional classifier predicts individual predi-cates with a 38% accuracy; however only 8% of34the structures are recalled.
Differences betweenthe accuracies of the two algorithms (predicate-based and position-based) are significant at 95%confidence level (a t-test was used).
As it is usu-ally the case in text classification experiments,the use of word level information (lemmas in ourcase) achieves good performance: 55% classifica-tion accuracy at sentence level, and 18% of fullstructures correctly predicted.
The use of lex-ical (noun and verb heads, arguments), syntac-tic (parts of speech information), and discourse(predicted predicates, position, cohesion) featureshas the better performance with 60% classifica-tion accuracy at sentence level predicting 21%of all structures with 73% of the structures con-taining less than 3 errors.
The differences inaccuracy between the word-based classifier andthe summarization-based classifier are statisticallysignificant at 95% confidence level (a t-test wasused).
A Naive Bayes classifier which uses thesummarization features achieves 50% classifica-tion accuracy.Conf.
0 errs < 2 errrs < 3 errsRandom 0.3% 4% 20%Predicate-based 5% 24% 48%Position-based 8% 33% 50%tf*idf-based 18% 42% 67%Summ-based 21% 55% 73%Table 4: Percent of correct and partially correctstructures predicted.
Averaged over all runs.Table 5 shows a partial confusion table for pred-icates ?to add?, ?to conclude?, ?to explain?, and?to present?
while and Table 6 reports individualclassification accuracy.
All these results are basedon averages of the summarization-based classifier.5 DiscussionWe have presented here a problem which has notbeen investigated before in the field of text sum-marization: the addition of extra linguistic mate-rial (i.e., not present in the source document) to theabstract ?informational content?
in order to createthe structure of the abstract.
We have proposed analgorithm which uses a classification componentat each iteration to predict predicates or phrases tobe prepended to fragments extracted from a doc-ument.
We have shown that this classifier basedon summarization features including linguistic, se-mantic, positional, cohesive, and discursive infor-mation can predict the local discourse structures inover 60% of the cases.
There is a mixed picture onthe prediction of individual predicates, with mostpredicates correctly classified in most of the casesexcept for predicates such as ?to describe?, ?tonote?, and ?to report?
which are confused withother phrases.
Predicates such as ?to present?
and?to include?
have the tendency of appearing to-wards the very beginning or the very end of the ab-stract been therefore predicted by position-basedfeatures (Edmundson, 1969; Lin and Hovy, 1997).Note that in this work we have decided to evaluatethe predicted structure against the true structure (ahard evaluation measure), in future work we willassess the abstracts with a set of quality questionssimilar to those put forward by the Document Un-derstanding Conference Evaluations (also in a waysimilar to (Kan and McKeown, 2002) who eval-uated their abstracts in a retrieval environment).We expect to obtain a reasonable evaluation resultgiven that it appears that some of the predicates orphrases are ?interchangeable?
(e.g., ?to contain?and ?to include?
).Actual Pred.
Predicted Pred.
Conf.Freq.to add to add 32%to explain 16%to say 10%to conclude to conclude 35%to say 29%to add 7%to explain to explain 35%to say 15%to add 11%to present to present 86%to discuss 7%to provide 1%Table 5: Classification Confusion Table for a Sub-set of Predicates in the Corpus (Average Fre-quency).6 Related WorkLiddy (1991) produced a formal model of the in-formational or conceptual structure of abstractsof empirical research.
This structure was elicitedfrom abstractors of two organizations ERIC andPsycINFO through a series of tasks.
Lexical clueswhich predict the components of the structurewere latter induced by corpus analysis.
In the do-main of indicative summarization, Kan and McK-35Predicate Avg.
Accuracyto add 31.40to conclude 34.78to contain 10.96to describe 15.69to discuss 54.55to explain 35.63to feature 34.38to include 85.86to indicate 20.69to mention 26.47to note 6.78to point out 91.67to present 86.19to provide 40.94to report 1.59to say 75.86Table 6: Predicate Classification Accuracyeown (2002) studied the problem of generating ab-stracts for bibliographical data which although in arestricted domain has some contact points with thework described here.
As in their work we use theabstracts in our corpus to induce the model.
Theyrely on a more or less fixed discourse structure toaccommodate the generation process.
In our ap-proach the discourse structure is not fixed but pre-dicted for each particular abstract.
Related to ourclassification experiments is work on semantic orrhetorical classification of ?structured?
abstracts(Saggion, 2008) from the MEDLINE abstractingdatabase where similar features to those presentedhere were used to identify in abstracts semanticcategories such as objective, method, results, andconclusions.
Related to this is the work by Teufeland Moens (2002) on rhetorical classification forcontent selection.
In cut-and-paste summarization(Jing and McKeown, 2000), sentence combina-tion operations were implemented manually fol-lowing the study of a set of professionally writtenabstracts; however the particular ?pasting?
oper-ation presented here was not implemented.
Pre-vious studies on text-to-text abstracting (Banko etal., 2000; Knight and Marcu, 2000) have studiedproblems such as sentence compression and sen-tence combination but not the ?pasting?
procedurepresented here.
The insertion in the abstract oflinguistic material not present in the input docu-ment has been addressed in paraphrase generation(Barzilay and Lee, 2004) and canned-based sum-marization (Oakes and Paice, 2001) in limited do-mains.
Saggion and Lapalme (2002) have studiedand implemented a rule-based ?verb selection?
op-eration in their SumUM system which has beenapplied to introduce document topics during in-dicative summary generation.Our discourse structure generation procedure isin principle generic but depends on the availabilityof a corpus for training.7 ConclusionsIn text summarization research, most attentionhas been paid to the problem of what informationto select for a summary.
Here, we have focusedon the problem of how to combine the selectedcontent with extra linguistic information in orderto create the structure of the summary.There are several contributions of this work:?
First, we have presented the problem of gen-erating the discourse structures of an abstractand proposed a meta algorithm for predictingit.
This problem has not been investigated be-fore.?
Second, we have proposed ?
based on pre-vious summarization research ?
a number offeatures to be used for solving this problem;and?
Finally, we have propose several instantia-tions of the algorithm to solve the problemand achieved a reasonable accuracy using thedesigned features;There is however much space for improvementeven though the algorithm recalls some ?partialstructures?, many ?full structures?
can not be gen-erated.
We are currently investigating the useof induced rules to address the problem and willcompare a rule-based approach with our classi-fier.
Less superficial cohesion features are beinginvestigated and will be tested in this classificationframework.AcknowledgementsWe would like to thank three anonymous review-ers for their suggestions and comments.
We thankAdam Funk who helped us improve the quality ofour paper.
Part of this research was carried outwhile the author was working for the EU-fundedMUSING project (IST-2004-027097).36ReferencesMichele Banko, Vibhu O. Mittal, and Michael J. Wit-brock.
2000.
Headline generation based on statisti-cal translation.
In ACL ?00: Proceedings of the 38thAnnual Meeting on Association for ComputationalLinguistics, pages 318?325, Morristown, NJ, USA.Association for Computational Linguistics.Regina Barzilay and Michael Elhadad.
1997.
UsingLexical Chains for Text Summarization.
In Proceed-ings of the ACL/EACL?97 Workshop on IntelligentScalable Text Summarization, pages 10?17, Madrid,Spain, July.R.
Barzilay and L. Lee.
2004.
Catching the Drift:Probabilistic Content Models, with Applications toGeneration and Summarization.
In Proceedings ofHLT-NAACL 2004.T.
Cohn and M. Lapata.
2008.
Sentence compressionbeyond word deletion.
In Proceedings of COLING2008, Manchester.H.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
GATE: A framework and graph-ical development environment for robust NLP toolsand applications.
In ACL 2002.H.P.
Edmundson.
1969.
New Methods in AutomaticExtracting.
Journal of the Association for Comput-ing Machinery, 16(2):264?285, April.Brigitte Endres-Niggemeyer.
2000.
SimSum: an em-pirically founded simulation of summarizing.
Infor-mation Processing & Management, 36:659?682.R.
Gaizauskas, M. Hepple, H. Saggion, and M. Green-wood.
2005.
SUPPLE: A Practical Parser for Natu-ral Language Engineering Applications.Hongyan Jing and Kathleen McKeown.
2000.
Cutand Paste Based Text Summarization.
In Proceed-ings of the 1st Meeting of the North American Chap-ter of the Association for Computational Linguistics,pages 178?185, Seattle, Washington, USA, April 29- May 4.T.
Joachims.
1998.
Text categorization with supportvector machines: Learning with many relevant fea-tures.
In European Conference on Machine Learn-ing (ECML), pages 137?142, Berlin.
Springer.Min-Yen Kan and Kathleen R.. McKeown.
2002.Corpus-trained text generation for summarization.In Proceedings of the Second International Natu-ral Language Generation Conference (INLG 2002),pages 1?8, Harriman, New York, USA.Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization - step one: Sentence compres-sion.
In Proceedings of the 17th National Confer-ence of the American Association for Artificial In-telligence.
AAAI, July 30 - August 3.Julian Kupiec, Jan Pedersen, and Francine Chen.
1995.A Trainable Document Summarizer.
In Proc.
of the18th ACM-SIGIR Conference, pages 68?73, Seattle,Washington, United States.M.
Lapata.
2003.
Probabilistic Text Structuring: Ex-periments with Sentence Ordering.
In Proceedingsof the 41st Meeting of the Association of Computa-tional Linguistics, pages 545?552, Sapporo, Japan.Elizabeth D. Liddy.
1991.
The Discourse-Level Struc-ture of Empirical Abstracts: An Exploratory Study.Information Processing & Management, 27(1):55?81.C.
Lin and E. Hovy.
1997.
Identifying Topics by Po-sition.
In Fifth Conference on Applied Natural Lan-guage Processing, pages 283?290.
Association forComputational Linguistics, 31 March-3 April.Inderjeet Mani.
2001.
Automatic Text Summarization.John Benjamins Publishing Company.D.
Marcu.
1997.
The Rhetorical Parsing, Summa-rization, and Generation of Natural Language Texts.Ph.D.
thesis, Department of Computer Science, Uni-versity of Toronto.M.
Montesi and J. M. Owen.
2007.
Revision of au-thor abstracts: how it is carried out by LISA editors.Aslib Proceedings, 59(1):26?45.M.P.
Oakes and C.D.
Paice.
2001.
Term extrac-tion for automatic abstracting.
In D. Bourigault,C.
Jacquemin, and M-C. L?Homme, editors, RecentAdvances in Computational Terminology, volume 2of Natural Language Processing, chapter 17, pages353?370.
John Benjamins Publishing Company.H.
Saggion and G. Lapalme.
2002.
GeneratingIndicative-Informative Summaries with SumUM.Computational Linguistics, 28(4):497?526.H.
Saggion.
2008.
Automatic Summarization: AnOverview.
Revue Franc?aise de Linguistique Ap-pliqu?ee , XIII(1), Juin.R.
Soricut and D. Marcu.
2007.
Abstractive headlinegeneration using WIDL-expressions.
Inf.
Process.Manage., 43(6):1536?1548.S.
Teufel and M. Moens.
2002.
SummarizingScientific Articles: Experiments with Relevanceand Rhetorical Status.
Computational Linguistics,28(4):409?445.Ian H. Witten and Eibe Frank.
1999.
Data Mining:Practical Machine Learning Tools and Techniqueswith Java Implementations.
Morgan Kaufmann, Oc-tober.Appendix I: Corpus Statistics andExamplesThe corpus of abstracts following the specificationgiven in Section 2 contains 693 abstracts, 10,42337sentences, and 305,105 tokens.
The reducedcorpus used for the experiments contains 300abstracts.ExamplesHere we list one example of the use of each of thepredicates in the reduced set of 300 abstracts usedfor the experiments.Adds that it uses search commands andfeatures that are similar to those oftraditional online commercial databaseservices, has the ability to do nestedBoolean queries as well as truncationwhen needed, and provides detailed doc-umentation that offers plenty of exam-ples.Concludes CNET is a network of sites,each dealing with a specialized aspect ofcomputers that are accessible from thehome page and elsewhere around the site.Contains a step-by-step guide to usingPGP.Describes smallbizNet, the LEXIS-NEXIS Small Business Service, SmallBusiness Administration, Small BusinessAdvancement National Center, and othersmall business-related sites.Discusses connections and links betweendiffering electronic mail systems.Explains DataStar was one of the first on-line hosts to offer a Web interface, andwas upgraded in 1997.Features tables showing the number ofrelevant, non-relevant, and use retrievalson both LEXIS and WIN for federal andfor state court queries.Includes an electronic organizer, an er-gonomically correct keyboard, an on-line idle-disconnect, a video capture de-vice, a color photo scanner, a real-timeWeb audio player, laptop speakers, apersonal information manager (PIM), amouse with built-in scrolling, and a voicefax-modem.Indicates that the University of Califor-nia, Berkeley, has the School of Informa-tion Management and Systems, the Uni-versity of Washington has the Informa-tion School, and the University of Mary-land has the College of Information Stud-ies.Mentions that overall, the interface iseffective because the menus and searscreens permit very precise searches withno knowledge of searching or Dialogdatabases.Notes that Magazine Index was origi-nally offered on Lyle Priest?s invention,a unique microfilm reader.Points out the strong competition that theInternet has created for the traditional on-line information services, and the moveof these services to the Internet.Presents searching tips and techniques.Provides summaries of African art; AllenMemorial Art Museum of Oberlin Col-lege; Art crimes; Asian arts; Da Vinci,Leonardo; Gallery Walk; and NativeAmerican Art Gallery.Reports that Dialog has announced ma-jor enhancements to its alerting system onthe DialogClassic, DialogClassic Web,and DialogWeb services.Says that dads from all over the countryshare advice on raising children, educa-tional resources, kids?
software, and otherrelated topics using their favorite onlineservice provider.38
