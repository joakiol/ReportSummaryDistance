49Statistics Learning and Universal Grammar:Modeling Word SegmentationTimothy Gambell59 Bishop StreetNew Haven, CT 06511USAtimothy.gambell@aya.yale.eduCharles YangDepartment of Linguistics, Yale UniversityNew Haven, CT 06511USAcharles.yale.edu@yale.eduAbstractThis paper describes a computational model of wordsegmentation and presents simulation results on re-alistic acquisition In particular, we explore the ca-pacity and limitations of statistical learning mecha-nisms that have recently gained prominence in cog-nitive psychology and linguistics.1 IntroductionTwo facts about language learning are indisputable.First, only a human baby, but not her pet kitten, canlearn a language.
It is clear, then, that there mustbe some element in our biology that accounts forthis unique ability.
Chomsky?s Universal Grammar(UG), an innate form of knowledge specific to lan-guage, is an account of what this ability is.
This po-sition gains support from formal learning theory [1-3], which sharpens the logical conclusion [4,5] thatno (realistically efficient) learning is possible with-out priori restrictions on the learning space.
Sec-ond, it is also clear that no matter how much of ahead start the child has through UG, language islearned.
Phonology, lexicon, and grammar, whilegoverned by universal principles and constraints, dovary from language to language, and they must belearned on the basis of linguistic experience.
Inother words?indeed a truism?both endowment andlearning contribute to language acquisition, the re-sult of which is extremely sophisticated body oflinguistic knowledge.
Consequently, both must betaken in account, explicitly, in a theory of languageacquisition [6,7].Controversies arise when it comes to the relativecontributions by innate knowledge and experience-based learning.
Some researchers, in particular lin-guists, approach language acquisition by charac-terizing the scope and limits of innate principlesof Universal Grammar that govern the world?s lan-guage.
Others, in particular psychologists, tend toemphasize the role of experience and the child?sdomain-general learning ability.
Such division ofresearch agenda understandably stems from the di-vision of labor between endowment and learning?plainly, things that are built in needn?t be learned,and things that can be garnered from experienceneedn?t be built in.The important paper of Saffran, Aslin, & New-port [8] on statistical learning (SL), suggests thatchildren may be powerful learners after all.
Veryyoung infants can exploit transitional probabilitiesbetween syllables for the task of word segmenta-tion, with only minimum exposure to an artificiallanguage.
Subsequent work has demonstrated SLin other domains including artificial grammar learn-ing [9], music [10], vision [11], as well as in otherspecies [12].
This then raises the possibility oflearning as an alternative to the innate endowmentof linguistic knowledge [13].We believe that the computational modeling ofpsychological processes, with special attention toconcrete mechanisms and quantitative evaluations,can play an important role in the endowment vs.learning debate.
Linguists?
investigations of UG arerarely developmental, even less so corpus-oriented.Developmental psychologists, by contrast, oftenstop at identifying components in a cognitive task[14], without an account of how such componentswork together in an algorithmic manner.
On theother hand, if computation is to be of relevanceto linguistics, psychology, and cognitive science ingeneral, being merely computational will not suf-fice.
A model must be psychological plausible, andready to face its implications in the broad empiricalcontexts [7].
For example, how does it generalizeto typologically different languages?
How does themodel?s behavior compare with that of human lan-guage learners and processors?In this article, we will present a simple compu-tational model of word segmentation and some ofits formal and developmental issues in child lan-guage acquisition.
Specifically we show that SLusing transitional probabilities cannot reliably seg-ment words when scaled to a realistic setting (e.g.,child-directed English).
To be successful, it mustbe constrained by the knowledge of phonological50structure.
Indeed, the model reveals that SL maywell be an artifact?an impressive one, nonetheless?that plays no role in actual word segmentation inhuman children.2 Statistics does not Refute UGIt has been suggested [15, 8] that word segmenta-tion from continuous speech may be achieved byusing transitional probabilities (TP) between ad-jacent syllables A and B, where , TP(A?B) =P(AB)/P(A), with P(AB) being the frequency of Bfollowing A, and P(A) the total frequency of A.Word boundaries are postulated at local minima,where the TP is lower than its neighbors.
For ex-ample, given sufficient amount of exposure to En-glish, the learner may establish that, in the four-syllable sequence ?prettybaby?, TP(pre?tty) andTP(ba?by) are both higher than TP(tty?ba): aword boundary can be (correctly) postulated.
Itis remarkable that 8-month-old infants can extractthree-syllable words in the continuous speech of anartificial language from only two minutes of expo-sure [8].To be effective, a learning algorithm?indeed anyalgorithm?must have an appropriate representationof the relevant learning data.
We thus need to becautious about the interpretation of the success ofSL, as the authors themselves note [16].
If any-thing, it seems that the findings strengthen, ratherthan weaken, the case for (innate) linguistic knowl-edge.
A classic argument for innateness [4, 5,17] comes from the fact that syntactic operationsare defined over specific types of data structures?constituents and phrases?but not over, say, linearstrings of words, or numerous other logical possibil-ities.
While infants seem to keep track of statisticalinformation, any conclusion drawn from such find-ings must presuppose children knowing what kindof statistical information to keep track of.
After all,an infinite range of statistical correlations exists inthe acoustic input: e.g., What is the probability of asyllable rhyming with the next?
What is the proba-bility of two adjacent vowels being both nasal?
Thefact that infants can use SL to segment syllable se-quences at all entails that, at the minimum, theyknow the relevant unit of information over whichcorrelative statistics is gathered: in this case, it isthe syllables, rather than segments, or front vowels.A host of questions then arises.
First, How dothey know so?
It is quite possible that the primacyof syllables as the basic unit of speech is innatelyavailable, as suggested in neonate speech perceptionstudies [18]?
Second, where do the syllables comefrom?
While the experiments in [8] used uniformlyCV syllables, many languages, including English,make use of a far more diverse range of syllabictypes.
And then, syllabification of speech is farfrom trivial, which (most likely) involve both in-nate knowledge of phonological structures as wellas discovering language-specific instantiations [14].All these problems have to be solved before SL forword segmentation can take place.3 The ModelTo give a precise evaluation of SL in a realis-tic setting, we constructed a series of (embarrass-ingly simple) computational models tested on child-directed English.The learning data consists of a random sam-ple of child-directed English sentences from theCHILDES database [19] The words were then pho-netically transcribed using the Carnegie Mellon Pro-nunciation Dictionary, and were then grouped intosyllables.
Spaces between words are removed; how-ever, utterance breaks are available to the modeledlearner.
Altogether, there are 226,178 words, con-sisting of 263,660 syllables.Implementing SL-based segmentation is straight-forward.
One first gathers pair-wise TPs from thetraining data, which are used to identify local min-ima and postulate word boundaries in the on-lineprocessing of syllable sequences.
Scoring is donefor each utterance and then averaged.
Viewed as aninformation retrieval problem, it is customary [20]to report both precision and recall of the perfor-mance.The segmentation results using TP local minimaare remarkably poor, even under the assumptionthat the learner has already syllabified the input per-fectly.
Precision is 41.6%, and recall is 23.3%; overhalf of the words extracted by the model are not ac-tual English words, while close to 80% of actualwords fail to be extracted.
And it is straightfor-ward why this is the case.
In order for SL to beeffective, a TP at an actual word boundary mustbe lower than its neighbors.
Obviously, this con-dition cannot be met if the input is a sequence ofmonosyllabic words, for which a space must be pos-tulated for every syllable; there are no local min-ima to speak of.
While the pseudowords in [8]are uniformly three-syllables long, much of child-directed English consists of sequences of monosyl-labic words: corpus statistics reveals that on aver-age, a monosyllabic word is followed by anothermonosyllabic word 85% of time.
As long as thisis the case, SL cannot, in principle, work.514 Statistics Needs UGThis is not to say that SL cannot be effectivefor word segmentation.
Its application, must beconstrained?like that of any learning algorithmhowever powerful?as suggested by formal learningtheories [1-3].
The performance improves dramat-ically, in fact, if the learner is equipped with evena small amount of prior knowledge about phono-logical structures.
Specifically, we assume, uncon-troversially, that each word can have only one pri-mary stress.
(This would not work for functionalwords, however.)
If the learner knows this, thenit may limit the search for local minima only inthe window between two syllables that both bearprimary stress, e.g., between the two a?s in thesequence ?languageacquisition?.
This assumptionis plausible given that 7.5-month-old infants aresensitive to strong/weak prosodic distinctions [14].When stress information suffices, no SL is em-ployed, so ?bigbadwolf?
breaks into three wordsfor free.
Once this simple principle is built in, thestress-delimited SL algorithm can achieve the pre-cision of 73.5% and 71.2%, which compare favor-ably to the best performance reported in the litera-ture [20].
(That work, however, uses an computa-tionally prohibitive and psychological implausiblealgorithm that iteratively optimizes the entire lexi-con.
)The computational models complement the ex-perimental study that prosodic information takespriority over statistical information when both areavailable [21].
Yet again one needs to be cautiousabout the improved performance, and a number ofunresolved issues need to be addressed by futurework.
It remains possible that SL is not used atall in actual word segmentation.
Once the one-word-one-stress principle is built in, we may con-sider a model that does not use any statistics, henceavoiding the computational cost that is likely tobe considerable.
(While we don?t know how in-fants keep track of TPs, there are clearly quite somework to do.
Syllables in English number in thethousands; now take the quadratic for the potentialnumber of pair-wise TPs.)
It simply stores previ-ously extracted words in the memory to bootstrapnew words.
Young children?s familiar segmenta-tion errors?
?I was have?
from be-have, ?hiccing up?from hicc-up, ?two dults?, from a-dult?suggest thatthis process does take place.
Moreover, there is ev-idence that 8-month-old infants can store familiarsounds in the memory [22].
And finally, there areplenty of single-word utterances?up to 10% [23]?that give many words for free.
The implementationof a purely symbolic learner that recycles knownwords yields even better performance: a precisionof 81.5% and recall of 90.1%.5 ConclusionFurther work, both experimental and computational,will need to address a few pressing questions, in or-der to gain a better assessment of the relative contri-bution of SL and UG to language acquisition.
Theseinclude, more pertinent to the problem of word seg-mentation:?
Can statistical learning be used in the acquisi-tion of language-specific phonotactics, a pre-requisite to syllabification and a prelude toword segmentation??
Given that prosodic constraints are critical forthe success of SL in word segmentation, futurework needs to quantify the availability of stressinformation in spoken corpora.?
Can further experiments, carried over realisticlinguistic input, further tease apart the multi-ple strategies used in word segmentation [14]?What are the psychological mechanisms (algo-rithms) that integrate these strategies??
How does word segmentation, statistical orotherwise, work for agglutinative (e.g., Turk-ish) and polysynthetic languages (e.g.
Mo-hawk), where the division between words,morphology, and syntax is quite different frommore clear-cut cases like English?Computational modeling can make explicit thebalance between statistics and UG, and are in thesame vein as the recent findings [24] on when/whereSL is effective/possible.
UG can help SL byproviding specific constraints on its application,and modeling may raise new questions for fur-ther experimental studies.
In related work [6,7],we have augmented traditional theories of UG?derivational phonology, and the Principles and Pa-rameters framework?with a component of statisti-cal learning, with novel and desirable consequences.Yet in all cases, statistical learning, while perhapsdomain-general, is constrained by what appears tobe innate and domain-specific knowledge of linguis-tic structures, such that learning can operate on spe-cific aspects of the input evidenceReferences1.
Gold, E. M. (1967).
Language identification inthe limit.
Information and Control, 10:447-74.2.
Valiant, L. (1984).
A theory of the learnable.Communication of the ACM.
1134-1142.523.
Vapnik, V. (1995).
The Nature of StatisticalLearning Theory.
Berlin: Springer.4.
Chomsky, N. (1959).
Review of Verbal Behav-ior by B.F. Skinner.
Language, 35, 26-57.5.
Chomsky, N. (1975).
Reflections on Language.New York: Pantheon.6.
Yang, C. D. (1999).
A selectionist theory oflanguage development.
In Proceedings of 37thMeeting of the Association for ComputationalLinguistics.
Maryland, MD.
431-5.7.
Yang, C. D. (2002).
Knowledge and Learningin Natural Language.
Oxford: Oxford Univer-sity Press.8.
Saffran, J.R., Aslin, R.N., & Newport, E.L.(1996).
Statistical learning by 8-month old in-fants.
Science, 274, 1926-1928.9.
Gomez, R.L., & Gerken, L.A. (1999).
Artifi-cial grammar learning by one-year-olds leadsto specific and abstract knowledge.
Cognition,70, 109-135.10.
Saffran, J.R., Johnson, E.K., Aslin R.N.
&Newport, E.L. (1999).
Statistical learning oftone sequences by human infants and adults.Cognition, 70, 27-52.11.
Fiser, J., & Aslin, R.N.
(2002).
Statisticallearning of new visual feature combinations byinfants.
PNAS, 99, 15822-6.12.
Hauser, M., Newport, E.L., & Aslin, R.N.(2001).
Segmentation of the speech stream ina non-human primate: Statistical learning incotton-top tamarins.
Cognition, 78, B41-B52.13.
Bates, E., & Elman, J.
(1996).
Learning redis-covered.
Science, 274, 1849-50.14.
Jusczyk, P.W.
(1999).
How infants begin to ex-tract words from speech.
Trends in CognitiveSciences, 3, 323-8.15.
Chomsky, N. (1955/1975).
The Logical Struc-ture of Linguistic Theory.
Manuscript, Har-vard University and Massachusetts Institute ofTechnology.
Published in 1975 by New York:Plenum.16.
Saffran, J.R., Aslin, R.N., & Newport, E.L.(1997).
Letters.
Science, 276, 1177-118117.
Crain, S., & Nakayama, M. (1987).
Structuredependency in grammar formation.
Language,63:522-543.18.
Bijeljiac-Babic, R., Bertoncini, J., & Mehler,J.
(1993).
How do four-day-old infants cate-gorize multisyllabic utterances.
Developmen-tal psychology, 29, 711-21.19.
MacWhinney, B.
(1995).
The CHILDESProject: Tools for Analyzing Talk.
Hillsdale:Lawrence Erlbaum.20.
Brent, M. (1999).
Speech segmentation andword discovery: a computational perspective.Trends in Cognitive Science, 3, 294-301.21.
Johnson, E.K.
& Jusczyk, P.W.
(2001) Wordsegmentation by 8-month-olds: When speechcues count more than statistics.
Journal ofMemory and Language, 44, 1-20.22.
Jusczyk, P. W., & Hohne, E. A.
(1997).
In-fants?
memory for spoken words.
Science, 277,1984-6.23.
Brent, M.R., & Siskind, J.M.
(2001).
The roleof exposure to isolated words in early vocabu-lary development.
Cognition, 81, B33-44.24.
Newport, E.L., & Aslin, R.N.
(2004).
Learn-ing at a distance: I.
Statistical learning of non-adjacent dependencies.
Cognitive Psychology,48, 127-62.
