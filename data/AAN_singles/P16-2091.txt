Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 561?566,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsJoint part-of-speech and dependency projection from multiple sourcesAnders Johannsen?Zeljko Agi?c Anders S?gaardCenter for Language Technology, University of Copenhagen, Denmarkanders@johannsen.comAbstractMost previous work on annotation projec-tion has been limited to a subset of Indo-European languages, using only a singlesource language, and projecting annota-tion for one task at a time.
In contrast,we present an Integer Linear Program-ming (ILP) algorithm that simultaneouslyprojects annotation for multiple tasks frommultiple source languages, relying on par-allel corpora available for hundreds of lan-guages.
When training POS taggers anddependency parsers on jointly projectedPOS tags and syntactic dependencies us-ing our algorithm, we obtain better perfor-mance than a standard approach on 20/23languages using one parallel corpus; and18/27 languages using another.1 IntroductionCross-language annotation projection for unsuper-vised POS tagging and syntactic parsing was in-troduced fifteen years ago (Yarowsky et al, 2001;Hwa et al, 2005), and the best unsupervised de-pendency parsers today rely on annotation projec-tion (Rasooli and Collins, 2015).Despite the maturity of the field, there is an in-herent language bias in previous work on cross-language annotation projection.
Cross-languageannotation projection experiments require trainingdata in m source languages, a parallel corpus oftranslations from the m source languages into thetarget language of interest, as well as evaluationdata for the target language.1Since the canonicalresource for parallel text is the Europarl Corpus(Koehn, 2005), which covers languages spoken inthe European parliament, annotation projection is1All previous work that we are aware of?with the possi-ble exception of McDonald et al (2011); but see Sections 2and 5?uses only a single source (m = 1), but in our experi-ments, we use multiple source languages.typically limited to the subset of Indo-Europeanlanguages that have treebanks.Previous work is also limited in another respect.While treebanks typically contain multiple layersof annotation, previous work has focused on pro-jecting data for a single task.We go significantly beyond previous work intwo ways: 1) by considering multi-source pro-jection across languages in parallel corpora thatare available for hundreds of languages, includ-ing many non-Indo-European languages; and 2)by jointly projecting annotation for two mutuallydependent tasks, namely POS tagging and depen-dency parsing.
Using multiple source languagesmakes our projections denser.
In single sourceprojection, the source language may not containall syntactic phenomena of the target language; wecombat this by transferring syntactic informationfrom multiple source languages.
Our work alsodiffers from previous work on annotation projec-tion in projecting soft rather than hard constraints,i.e., scores rather than labels and edges.Contributions We present a novel ILP-based al-gorithm for jointly projecting POS labels and de-pendency annotations across word-aligned parallelcorpora.
The performance of our algorithm com-pares favorably to that of a state-of-the-art projec-tion algorithm, as well as to multi-source delex-icalized transfer.
Our experiments include be-tween 23 and 27 languages using two parallel cor-pora that are available for hundreds of languages,namely a collection of Bibles and Watchtower pe-riodicals.
Finally, we make both the parallel cor-pora and the code publicly available.22 Projection algorithmThe projection algorithm is divided into two dis-tinct steps.
First, we project potential syntactic2https://bitbucket.org/lowlands/release561edges and POS tags from all source languages intoan intermediate target graph, which is left deliber-ately ambiguous.
In the second step, we decodethe target graph by solving a constrained optimi-sation problem, which simultaneously resolves allambiguities and produces a single dependency treewith a fixed set of POS tags.
Below we describeboth steps in more detail.2.1 Cross-language sentenceThe input to our projection algorithm is a cross-language sentence, a data structure that ties to-gether a collection of aligned sentences from a par-allel corpus, i.e., sentences in many different lan-guages that are determined to be translation equiv-alents.
One sentence of the set is designated as thetarget while the rest are sources.
We project syn-tactic information from the sources to the target.All source sentences are automatically parsedwith a graph-based dependency parser and labeledwith parts of speech.
Instead of using the sin-gle best dependency tree output by the parser, weextract its scoring matrix, an ambiguous structurethat assigns a numeric score to each potential de-pendency edge.
The target sentence is not parsedor POS-tagged.
In fact, our approach is explicitlydesigned to work for target languages where nosuch resources are available.
Only unsupervisedword alignments couple the target sentence witheach source sentence.More formally, a cross-language sentence maybe represented as a graph G = (V,E), whereeach vertex is a POS-tagged token of a sentencein some language.
With one target and n sourcelanguages, the total set of tagged word vertices Vcan be written as the union of sentence vertices:V = V0?
.
.
.?Vn.
The target sentence is Vt= V0,while source sentences are Vs= V1?
.
.
.
?
Vn.Two kinds of weighted edges connect the graph.Edges that go between tagged tokens of a sentenceVirepresent potential dependency edges.
Thus,for the sentence i, the induced subgraph G[Vi] isthe (ambiguous) dependency graph.
Edges con-necting a source vertex to target vertex representword alignments.
The set of alignment edges isA ?
Vs?
Vt.To account for POS we introduce a vertex label-ing function l : V 7?
?, where ?
is the POS vo-cabulary.
The source sentences are automaticallytagged, and for any source vertex the label func-tion simply returns this tag.
For the target sentencethe POS labels are unknown, which is to say thatevery target token is ambiguous between |?| POStags.
We represent this ambiguity in the graph bycreating a vertex for each possible combination oftarget word and POS.
Concretely, if a source sen-tence i has n tokens, and the target sentence hasmtokens, then |Vi| = n, and |Vs| = m|?|.Alignments are constrained such that an align-ment (u, v) ?
Vs?Vtonly exists if the source andtarget token were linked by the automatic alignerand l(u) = l(v), i.e., the POS tags match.
This fil-ters out potential source relations with dissimilarsyntax, a luxury that we are allowed in a multiplesource language setup.2.2 Projecting to ambiguous target graphThe target graphG[Vt] starts out empty and is pop-ulated with edges in the following way.
We gothrough the source sentences, looking for poten-tial dependency edges where both endpoints arealigned to the target sentence, and transferring theedge whenever we find one.
Technically, for everysource sentence i and for each edge in the sourcegraph (us, vs) ?
G[Vi], we create an edge (ut, vt)in the target iff both (us, ut) and (vs, vt) ?
A.The edge weight is the source edge score (as de-termined by an automatic parser) weighted by thejoint alignment probability of (us, ut) and (vs, vt):d(ut, vt) = maxus,vsa(us, vs) a(us, ut) d(vs, vt).For clarity, d refers to weights of dependencyedges, and a to alignment edge weights.
Multi-ple source sentences may project the same edgeto the target graph.
When this happens we updatethe target edge weight only if the new weight islarger than the existing.
The weight then reflectsthe strongest evidence found for a given syntacticrelation across all source languages.2.3 Decoding the target graphWe are now ready to decode the target graph.
Theresult of decoding is a dependency tree as well asa labeling of the target sentences with POS tags.Labeling with POS corresponds to selecting a sub-set of the vertices?V ?
Vt, such that exactly onevertex is chosen for each token.
Similarly the de-coded dependency tree is a subset of the projectedtarget edges with the constraint that it must forma tree over the vertices of?V .
The joint optimiza-tion objective is to simultaneously select a set ofvertices?V and edges?E to maximize the score of562the decoded tree.
We solve this constrained opti-mization problem by casting it as an integer linearprogramming (ILP) problem.The full specification of the ILP model is dis-played as Figure 1.
The model is optimized overtwo types of binary decision variables mapping di-rectly to the target graph representation discussedin the previous section, plus additional flow vari-ables that enforce tree structure.
An edge vari-able ei,k,j,lrepresents a target edge (i, j) wherethe POS of i is k and the POS of j is l. For in-stance, the variable e2,V,1,Nrepresents a directededge from the second token (a verb) to the first (anoun).
An active vertex variable vi,kindicates thatthe POS of token i is chosen as k.Following Martins (2012), we constrain thesearch space to spanning trees by using a single-commodity-flow construction.
In the commodity-flow analogy, we imagine the root as a factory thatproduces n commodities (for an n token sentence)which are distributed along the edges of the tree.Each token is a consumer that must receive andpass on all except one commodity to its depen-dents, i.e., the difference between incoming andoutgoing flow should be 1.
Since all commoditiesmust be consumed, the outgoing flow for a leafnode will be zero.
Together with the requirementthat each token must have exactly one head, thisensures all tokens are connected to the root in thetree structure.The last two constraint groups enforce edge andPOS consistency, and the selection of single POSper token.
Both are new to this work.3 Data sourcesOur projection requires parallel text, ideally span-ning a large number of languages, and dependencytreebanks for the sources.Treebanks To train the source-side taggers anddependency parsers, and to evaluate the cross-lingual taggers and parsers, we use the UniversalDependencies (UD) version 1.2 treebanks with thecorresponding test sets.3Parallel texts We exploit two sources of par-allel text: the Edinburgh Multilingual Bible cor-pus (EBC) (Christodouloupoulos and Steedman,2014), and our own collection of online texts pub-lished by the Wathctower Society (WTC).4While3http://hdl.handle.net/11234/1-15484https://www.jw.org/ILP modelEdges ei,k,j,l?
{0, 1}Vertices vi,k?
{0, 1}Flow ?i,k,j,l?
R+Maximize?i,k,j,lei,k,j,lwi,k,j,lOne parent per token?i,k,lei,k,j,l= 1 ?j 6= 0The root token (index 0) sends n flow?j,l?0,0,j,l= nEach token consumes one unit of flow?i,k,l?i,k,x,l?
?k,j,l?x,k,j,l= 1 ?x 6= 0One POS per token?kvi,k= 1 ?i 6= 0Active edges choose token POSvi,k?
ei,k,j,l?i 6= 0, j, k, lvi,l?
ei,k,j,l?i, j, k, lAbove, i, j, and x are token indices, while k and l referto POS.
Quantification over these symbols in the equa-tions are always with respect to a given target graph.Figure 1: Specification of the ILP model.
We list,in order, the decision variables, the objective, andthe five groups of constraint templates.the two collections span more than 100 languages,we focus on the subsets that overlap with the UDlanguages to facilitate evaluation.
For EBC, thatamounts to 27 languages, and 23 for WTC.Preprocessing We use simple sentence splittingand tokenization models to segment the parallelcorpora.5To sentence- and word-align the indi-vidual language pairs, we use a Gibbs sampling-based IBM1 alignment model called efmaral(?Ostling, 2015).
IBM1 has been shown to lead tomore robust alignments across typologically dis-tant language pairs (?Ostling, 2015).
We modify5https://github.com/bplank/multilingualtokenizer563the aligner to output alignment probabilities.
Allthe source-side texts are POS-tagged and depen-dency parsed using TnT (Brants, 2000) and Tur-boParser (Martins et al, 2013).
We use our ownfork of the arc-factored TurboParser to output theedge weight matrices.64 Experiments4.1 SetupIn our experiments, as in the preprocessing, we usethe TnT tagger and the arc-factored TurboParser,which we train on the EBC and WTC texts withprojected and decoded annotations.
We randomlysample up to 20k sentences per training file in bothtagging and parsing.
This 20k sampling limit ap-plies to all systems.We compare two cross-lingual projection-basedparsing systems, and one baseline system.ILP The ILP-based joint projection algorithmwe presented in Section 2.DCA Our implementation of the de facto stan-dard annotation projection algorithm of Hwa etal.
(2005), as refined by Tiedemann (2014).
Incontrast to our ILP approach, it uses heuristics toensure dependency tree constraints on a source-target sentence pair basis.
We gather all the pair-wise projections into a target sentence graph andthen perform maximum spanning tree decodingfollowing Sagae and Lavie (2006).DELEX The multi-source direct delexicalizedtransfer baseline of McDonald et al (2011).
Eachsource is represented by an approximately equalnumber of sentences.4.2 ResultsTable 1 provides a summary of dependency pars-ing scores.
We report UAS scores over predictedand gold POS.
The predicted tags come from ourcross-lingual taggers.
Our ILP approach consis-tently outperforms DCA on both by a large marginof 3-5 points UAS using predicted POS, and 5-10points on gold POS.
Note that DELEX is trainedon gold POS and therefore has an advantage in this6https://github.com/andersjo/TurboParser8We do not include DELEX in the comparison for the goldPOS scenario only.
In this particular scenario, DELEX is alsotrained on gold POS, and thus biased: the cross-lingual tag-gers do not have gold POS available for training, and the sameholds for DELEX and projected POS.ApproachPredicted POS ILP DCA DELEXEBC 51.62 (18) 48.39 (8) 42.44 (1)WTC 53.58 (20) 48.40 (0) 47.35 (3)Gold POSEBC 65.43 (25) 59.94 (2) 64.13 (?
)WTC 66.51 (23) 55.73 (0) 66.68 (?
)Table 1: Macro-averaged UAS scores summariz-ing our evaluation.
EBC: Edinburgh Bible corpus,WTC: Watchtower corpus.
Numbers of languageswith top performance per system are reported inbrackets.
All parsers use their respective EBC orWTC taggers.8setting.
Relying on predicted POS and WTC data,our ILP approach beats DCA for all the test lan-guages.
With EBC, we outperform DCA on 19out of 27 languages.In Table 2, we split the scores across the testlanguages and parallel data sources, and we alsoreport the POS tagging accuracies.
Our WTC tag-gers are on average 3.5 points better than EBC tag-gers, yielding the top score for 16/23 languagesfrom the overlap.
Notably, on several non-Indo-European languages, we observe significant im-provements.
For example, on Indonesian, DCAimproves over DELEX by 12 points UAS, whileILP adds 6 more points on top.
We observe a sim-ilar pattern for Arabic and Estonian.
We note thatDELEX tops ILP and DCA on only 1 EBC and 3WTC languages, and by a narrow margin.Analysis A projected parse is allowed to be acomposite of edges from many source languages.To find out to what degree this actually happens,we analyze all projections into English and Ger-man on the WTC corpus.For German the top four source languages areCzech, Norwegian, French, and English, con-tributing between 16% and 7% of all edges.
ForEnglish the top languages are Norwegian, Ital-ian, Indonesian, and Swedish.
Here, the top lan-guage Norwegian is responsible for 42% of theedges, while Swedish accounts for 13%.
Onlythe language projecting the highest scoring edgeis counted.
On average, a German sentence hasedges from 4.1 source languages.
The same num-ber for English is slightly higher, at 4.5.Manually annotated data We annotate a smallnumber of sentences in English from EBC and564Dependency parsingPOS tagging EBC WTCLanguage EBC WTC ILP DCA DELEX ILP DCA DELEXArabic 39.54 53.91 36.59 37.70 13.17 37.41 32.14 21.15Basque 43.43 ?
22.77 17.38 27.85 ?
?
?Bulgarian 76.45 68.27 50.6 60.03 57.83 49.68 37.18 48.37Croatian 72.83 76.18 54.19 45.08 42.34 55.16 50.56 45.49Czech 70.81 78.49 52.67 41.44 40.99 53.09 44.36 47.99Danish 76.43 86.36 61.14 53.22 49.65 61.78 58.64 55.96English 71.90 79.2 55.76 50.64 48.04 58.70 57.12 53.87* Estonian 75.55 73.98 62.9 56.95 49.32 63.85 58.41 48.48Farsi 64.94 25.67 23.53 42.37 28.93 20.34 12.26 19.48Finnish 70.41 67.44 43.66 44.51 41.18 42.59 35.6 41.52French 74.25 79.23 53.52 53.11 48.97 55.69 51.47 51.53German 74.36 68.36 45.02 50.21 49.36 43.99 36.7 45.79* Greek 56.52 75.75 62.59 37.73 37.11 62.43 52.95 54.90Hebrew 43.65 ?
30.25 39.76 19.06 ?
?
?Hindi 59.99 48.86 18.26 35.59 21.03 15.95 10.77 21.04* Hungarian 71.57 71.42 49.74 44.97 43.07 44.17 42.33 46.66Indonesian 63.30 75.61 51.99 23.53 31.18 58.01 52.29 39.67Italian 79.28 83.82 63.13 58.66 53.94 64.88 63.57 58.06* Latin 83.41 ?
68.65 68.45 41.42 ?
?
?Norwegian 77.00 85.31 65.04 58.32 53.46 66.54 64.37 60.11Polish 73.36 73.68 62.94 59.27 53.33 63.74 55.4 54.87Portuguese 78.41 83.67 63.75 60.45 52.91 64.62 63.16 56.99* Romanian 71.56 76.34 57.74 56.73 45.73 58.76 54.78 51.23* Serbian 74.07 ?
49.15 49.38 47.06 ?
?
?Slovene 75.68 78.11 59.17 53.66 50.55 59.79 54.8 52.53Spanish 76.72 85.69 63.63 52.20 47.6 64.93 61.90 55.87Swedish 78.26 84.80 65.24 55.21 50.85 66.15 62.45 57.48Average 69.40 73.05 51.62 48.39 42.44 53.58 48.40 47.35Best for 7 16 18 8 1 20 0 3Table 2: Tagging and parsing (UAS) accuracy.Scores are macro-averaged, and all parsers usepredicted POS from respective EBC or WTC tag-gers.
*: True target languages, not used as sources.WTC, which gives us a way to directly evaluatethe projections without training parsers.
On thissmall test set of 2?
50 sentences, we obtain UASscores of 68% (WTC) and 62% (EBC).
The POSaccuracies are 79% and 80%.
All figures are com-parable to the results from the indirect projectionevaluation.5 Related workIn recent years, we note an increased interest forwork in cross-lingual processing, and particularlyin POS tagging and dependency parsing of low-resource languages.Yarowsky et al (2001) proposed the idea of in-ducing NLP tools via parallel corpora.
Their con-tribution started a line of work in annotation pro-jection.
Das and Petrov (2011) used graph-basedlabel propagation to yield competitive POS tag-gers, while Hwa et al (2005) introduced the pro-jection of dependency trees.
Tiedemann (2014)further improved this approach to single-sourceprojection in the context of synthesizing depen-dency treebanks (Tiedemann and Agi?c, 2016).The current state of the art in cross-lingual de-pendency parsing also involves exploiting largeparallel corpora (Ma and Xia, 2014; Rasooli andCollins, 2015).Transferring models by training parsers with-out lexical features was first introduced by Zemanand Resnik (2008).
McDonald et al (2011) andS?gaard (2011) coupled delexicalization with con-tributions from multiple sources, while McDonaldet al (2013) were the first to leverage uniform rep-resentations of POS and syntactic dependencies incross-lingual parsing.Even more recently, Agi?c et al (2015) exposeda bias towards closely related Indo-European lan-guages shared by most previous work on anno-tation projection, while introducing a bias-freeprojection algorithm for learning 100 POS tag-gers from multiple sources.
Their line of work isnon-trivially extended to multilingual dependencyparsing by Agi?c et al (2016).The work in annotation projection for cross-lingual NLP invariably treats mutually dependentlayers of annotation separately.
Our contributionis distinct from these works by implementing thefirst approach to joint projection of POS and de-pendencies, while maintaining the outlook on pro-cessing truly low-resource languages.6 ConclusionIn our contribution, we addressed tagging andparsing for low-resource languages through jointcross-lingual projection of POS tags and syntac-tic dependencies from multiple source languages.Our novel approach to transferring the annotationsvia word alignments is based on integer linearprogramming, more specifically on a commodity-flow formalization for spanning trees.In our experiments with 27 treebanks from theUniversal Dependencies (UD) project, our ap-proach compared very favorably to two competi-tive cross-lingual systems: we provided the bestcross-lingual taggers and parsers for 18/27 and20/23 languages, depending on the parallel cor-pora used.
We made no unrealistic assumptionsas to the availability of parallel texts and prepro-cessing tools for the target languages.
Our codeand data is freely available.9Acknowledgements This research is partiallyfunded by the ERC Starting Grant LOWLANDS(#313695).9https://bitbucket.org/lowlands/release565References?Zeljko Agi?c, Dirk Hovy, and Anders S?gaard.
2015.If All You Have is a Bit of the Bible: LearningPOS Taggers for Truly Low-Resource Languages.In ACL.
?Zeljko Agi?c, Anders Johannsen, Barbara Plank, H?ectorMart?
?nez Alonso, Natalie Schluter, and AndersS?gaard.
2016.
Multilingual Projection for ParsingTruly Low-Resource Languages.
TACL, 4.Thorsten Brants.
2000.
TnT ?
A Statistical Part-of-Speech Tagger.
In ANLP.Christos Christodouloupoulos and Mark Steedman.2014.
A Massively Parallel Corpus: The Bible in100 Languages.
Language Resources and Evalua-tion, 49(2).Dipanjan Das and Slav Petrov.
2011.
UnsupervisedPart-of-Speech Tagging with Bilingual Graph-BasedProjections.
In ACL.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrap-ping Parsers via Syntactic Projection Across ParallelTexts.
Natural Language Engineering, 11(3).Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In MT Summit.Xuezhe Ma and Fei Xia.
2014.
Unsupervised De-pendency Parsing with Transferring Distribution viaParallel Guidance and Entropy Regularization.
InACL.Andr?e F. T. Martins, Miguel Almeida, and Noah A.Smith.
2013.
Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers.
In ACL.Andr?e F. T. Martins.
2012.
The Geometry of Con-strained Structured Prediction: Applications to In-ference and Learning of Natural Language Syntax.Ph.D.
thesis, Carnegie Mellon University and Insti-tuto Superior Tecnico.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.Multi-Source Transfer of Delexicalized DependencyParsers.
In EMNLP.Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuz-man Ganchev, Keith Hall, Slav Petrov, HaoZhang, Oscar T?ackstr?om, Claudia Bedini, N?uriaBertomeu Castell?o, and Jungmee Lee.
2013.Universal Dependency Annotation for MultilingualParsing.
In ACL.Robert?Ostling.
2015.
Word Order Typology throughMultilingual Word Alignment.
In ACL.Mohammad Sadegh Rasooli and Michael Collins.2015.
Density-Driven Cross-Lingual Transfer ofDependency Parsers.
In EMNLP.Kenji Sagae and Alon Lavie.
2006.
Parser Combina-tion by Reparsing.
In NAACL.Anders S?gaard.
2011.
Data Point Selection for Cross-Language Adaptation of Dependency Parsers.
InACL.J?org Tiedemann and?Zeljko Agi?c.
2016.
SyntheticTreebanking for Cross-Lingual Dependency Pars-ing.
Journal of Artificial Intelligence Research, 55.J?org Tiedemann.
2014.
Rediscovering AnnotationProjection for Cross-Lingual Parser Induction.
InCOLING.David Yarowsky, Grace Ngai, and Richard Wicen-towski.
2001.
Inducing Multilingual Text Analy-sis Tools via Robust Projection Across Aligned Cor-pora.
In NAACL.Daniel Zeman and Philip Resnik.
2008.
Cross-Language Parser Adaptation between Related Lan-guages.
In IJCNLP Workshop on NLP for Less Priv-ileged Languages.566
