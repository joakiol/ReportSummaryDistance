Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 920?929,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsMinimum Imputed Risk: Unsupervised Discriminative Training forMachine TranslationZhifei Li?Google ResearchMountain View, CA 94043, USAzhifei.work@gmail.comZiyuan Wang, Sanjeev KhudanpurJohns Hopkins UniversityBaltimore, MD 21218, USAzwang40,khudanpur@jhu.eduJason EisnerJohns Hopkins UniversityBaltimore, MD 21218, USAeisner@jhu.eduBrian RoarkOregon Health & Science UniversityBeaverton, Oregon 97006, USAroark@cslu.ogi.eduAbstractDiscriminative training for machine transla-tion has been well studied in the recent past.A limitation of the work to date is that it relieson the availability of high-quality in-domainbilingual text for supervised training.
Wepresent an unsupervised discriminative train-ing framework to incorporate the usually plen-tiful target-language monolingual data by us-ing a rough ?reverse?
translation system.
Intu-itively, our method strives to ensure that prob-abilistic ?round-trip?
translation from a target-language sentence to the source-language andback will have low expected loss.
Theoret-ically, this may be justified as (discrimina-tively) minimizing an imputed empirical risk.Empirically, we demonstrate that augment-ing supervised training with unsupervised dataimproves translation performance over the su-pervised case for both IWSLT and NIST tasks.1 IntroductionMissing data is a common problem in statistics whenfitting the parameters ?
of a model.
A common strat-egy is to attempt to impute, or ?fill in,?
the missingdata (Little and Rubin, 1987), as typified by the EMalgorithm.
In this paper we develop imputation tech-niques when ?
is to be trained discriminatively.We focus on machine translation (MT) as our ex-ample application.
A Chinese-to-English machinetranslation system is given a Chinese sentence x and?
Zhifei Li is currently working at Google Research, andthis work was done while he was a PHD student at Johns Hop-kins University.asked to predict its English translation y.
This sys-tem employs statistical models p?
(y | x) whose pa-rameters ?
are discriminatively trained using bilin-gual sentence pairs (x, y).
But bilingual data forsuch supervised training may be relatively scarce fora particular language pair (e.g., Urdu-English), es-pecially for some topics (e.g., technical manuals) orgenres (e.g., blogs).
So systems seek to exploit ad-ditional monolingual data, i.e., a corpus of Englishsentences y with no corresponding source-languagesentences x, to improve estimation of ?.
This is ourmissing data scenario.1Discriminative training of the parameters ?
ofp?
(y | x) using monolingual English data is a cu-rious idea, since there is no Chinese input x to trans-late.
We propose an unsupervised training approach,called minimum imputed risk training, which is con-ceptually straightforward: First guess x (probabilis-tically) from the observed y using a reverse English-to-Chinese translation model p?
(x | y).
Then trainthe discriminative Chinese-to-English model p?
(y |x) to do a good job at translating this imputed xback to y, as measured by a given performance met-ric.
Intuitively, our method strives to ensure thatprobabilistic ?round-trip?
translation from a target-language sentence to the source-language and backagain will have low expected loss.Our approach can be applied in an applicationscenario where we have (1) enough out-of-domainbilingual data to build two baseline translation sys-tems, with parameters ?
for the forward direction,and ?
for the reverse direction; (2) a small amount1Contrast this with traditional semi-supervised training thatlooks to exploit ?unlabeled?
inputs x, with missing outputs y.920of in-domain bilingual development data to discrim-inatively tune a small number of parameters in ?
;and (3) a large amount of in-domain English mono-lingual data.The novelty here is to exploit (3) to discrimina-tively tune the parameters ?
of all translation modelcomponents,2 p?
(y|x) and p?
(y), not merely train agenerative language model p?
(y), as is the norm.Following the theoretical development below, theempirical effectiveness of our approach is demon-strated by replacing a key supervised discriminativetraining step in the development of large MT sys-tems ?
learning the log-linear combination of sev-eral component model scores (viewed as features) tooptimize a performance metric (e.g.
BLEU) on a setof (x, y) pairs ?
with our unsupervised discrimina-tive training using only y.
One may hence contrastour approach with the traditional supervised meth-ods applied to the MT task such as minimum errorrate training (Och, 2003; Macherey et al, 2008), theaveraged Perceptron (Liang et al, 2006), maximumconditional likelihood (Blunsom et al, 2008), min-imum risk (Smith and Eisner, 2006; Li and Eisner,2009), and MIRA (Watanabe et al, 2007; Chiang etal., 2009).We perform experiments using the open-sourceMT toolkit Joshua (Li et al, 2009a), and show thatadding unsupervised data to the traditional super-vised training setup improves performance.2 Supervised Discriminative Training viaMinimization of Empirical RiskLet us first review discriminative training in the su-pervised setting?as used in MERT (Och, 2003) andsubsequent work.One wishes to tune the parameters ?
of somecomplex translation system ??(x).
The function ?
?,which translates Chinese x to English y = ??
(x)need not be probabilistic.
For example, ?
may bethe parameters of a scoring function used by ?, alongwith pruning and decoding heuristics, for extractinga high-scoring translation of x.The goal of discriminative training is to mini-mize the expected loss of ??(?
), under a given task-specific loss function L(y?, y) that measures how2Note that the extra monolingual data is used only for tuningthe model weights, but not for inducing new phrases or rules.bad it would be to output y?
when the correct outputis y.
For an MT system that is judged by the BLEUmetric (Papineni et al, 2001), for instance, L(y?, y)may be the negated BLEU score of y?
w.r.t.
y.
To beprecise, the goal3 is to find ?
with low Bayes risk,??
= argmin?
?x,yp(x, y)L(??
(x), y) (1)where p(x, y) is the joint distribution of the input-output pairs.4The true p(x, y) is, of course, not known and,in practice, one typically minimizes empirical riskby replacing p(x, y) above with the empirical dis-tribution p?
(x, y) given by a supervised training set{(xi, yi), i = 1, .
.
.
, N}.
Therefore,??
= argmin??x,yp?
(x, y)L(??
(x), y)= argmin?1NN?i=1L(??
(xi), yi).
(2)The search for ??
typically requires the use of nu-merical methods and some regularization.53 Unsupervised Discriminative Trainingwith Missing Inputs3.1 Minimization of Imputed RiskWe now turn to the unsupervised case, where wehave training examples {yi} but not their corre-sponding inputs {xi}.
We cannot compute the sum-mand L(??
(xi), yi) for such i in (2), since ??
(xi)requires to know xi.
So we propose to replace3This goal is different from the minimum risk training ofLi and Eisner (2009) in a subtle but important way.
In bothcases, ??
minimizes risk or expected loss, but the expectationis w.r.t.
different distributions: the expectation in Li and Eisner(2009) is under the conditional distribution p(y |x), while theexpectation in (1) is under the joint distribution p(x, y).4In the terminology of statistical decision theory, p(x, y) isa distribution over states of nature.
We seek a decision rule??
(x) that will incur low expected loss on observations x thatare generated from unseen states of nature.5To compensate for the shortcut of using the unsmoothedempirical distribution rather than a posterior estimate of p(x, y)(Minka, 2000), it is common to add a regularization term ||?||22in the objective of (2).
The regularization term can prevent over-fitting to a training set that is not large enough to learn all pa-rameters.921L(??
(xi), yi) with the expectation?xp?
(x | yi)L(??
(x), yi), (3)where p?(?
| ?)
is a ?reverse prediction model?
thatattempts to impute the missing xi data.
We call theresulting variant of (2) the minimization of imputedempirical risk, and say that??
= argmin?1NN?i=1?xp?
(x | yi)L(??
(x), yi) (4)is the estimate with the minimum imputed risk6.The minimum imputed risk objective of (4) couldbe evaluated by brute force as follows.1.
For each unsupervised example yi, use the re-verse prediction model p?(?
| yi) to impute pos-sible reverse translations Xi = {xi1, xi2, .
.
.
},and add each (xij , yi) pair, weighted byp?
(xij | yi) ?
1, to an imputed training set .2.
Perform the supervised training of (2) on theimputed and weighted training data.The second step means that we must use ??
toforward-translate each imputed xij , evaluate the lossof the translations y?ij against the corresponding truetranslation yi, and choose the ?
that minimizes theweighted sum of these losses (i.e., the empirical riskwhen the empirical distribution p?
(x, y) is derivedfrom the imputed training set).
Specific to our MTtask, this tries to ensure that probabilistic ?round-trip?
translation, from the target-language sentenceyi to the source-language and back again, will havea low expected loss.7The trouble with this method is that the reversemodel p?
generates a weighted lattice or hyper-graph Xi encoding exponentially many translationsof yi, and it is computationally infeasible to forward-translate each xij ?
Xi.
We therefore investigateseveral approximations to (4) in Section 3.4.6One may exploit both supervised data {(xi, yi)} and unsu-pervised data {yj} to perform semi-supervised training via aninterpolation of (2) and (4).
We will do so in our experiments.7Our approach may be applied to other tasks as well.
Forexample, in a speech recognition task, ??
is a speech recognizerthat produces text, whereas p?
is a speech synthesizer that mustproduce a distribution over audio (or at least over acoustic fea-tures or phone sequences) (Huang et al, 2010).3.2 The Reverse Prediction Model p?A crucial ingredient in (4) is the reverse predictionmodel p?(?|?)
that attempts to impute the missing xi.We will train this model in advance, doing the bestjob we can from available data, including any out-of-domain bilingual data as well as any in-domainmonolingual data8 x.In the MT setting, ??
and p?
may have similar pa-rameterization.
One translates Chinese to English;the other translates English to Chinese.Yet the setup is not quite symmetric.
Whereas ?
?is a translation system that aims to produce a single,low-loss translation, the reverse version p?
is rathera probabilistic model.
It is supposed to give an accu-rate probability distribution over possible values xijof the missing input sentence xi.
All of these val-ues are taken into account in (4), regardless of theloss that they would incur if they were evaluated fortranslation quality relative to the missing xi.Thus, ?
does not need to be trained to minimizethe risk itself (so there is no circularity).
Ideally,it should be trained to match the underlying condi-tional distribution of x given y, by achieving a lowconditional cross-entropyH(X |Y ) = ?
?x,yp(x, y) log p?
(x | y).
(5)In practice, ?
is trained by (empirically) minimiz-ing ?
1M?Nj=1 log p?
(xj | yj) + 12?2 ??
?22 on somebilingual data, with the regularization coefficient ?2tuned on held out data.It may be tolerable for p?
to impute mediocretranslations xij .
All that is necessary is that the (for-ward) translations generated from the imputed xij?simulate?
the competing hypotheses that we wouldsee when translating the correct Chinese input xi.3.3 The Forward Translation System ??
andThe Loss Function L(??
(xi), yi)The minimum empirical risk objective of (2) isquite general and various popular supervised train-ing methods (Lafferty et al, 2001; Collins, 2002;Och, 2003; Crammer et al, 2006; Smith and Eisner,8In a translation task from x to y, one usually does not makeuse of in-domain monolingual data x.
But we can exploit x totrain a language model p?
(x) for the reverse translation system,which will make the imputed xij look like true Chinese inputs.9222006) can be formalized in this framework by choos-ing different functions for ??
and L(??
(xi), yi).
Thegenerality of (2) extends to our minimum imputedrisk objective of (4).
Below, we specify the ??
andL(??
(xi), yi) we considered in our investigation.3.3.1 Deterministic DecodingA simple translation rule would define??
(x) = argmaxyp?
(y |x) (6)If this ??
(x) is used together with a loss functionL(??
(xi), yi) that is the negated BLEU score9, ourminimum imputed risk objective of (4) is equivalentto MERT (Och, 2003) on the imputed training data.However, this would not yield a differentiable ob-jective function.
Infinitesimal changes to ?
could re-sult in discrete changes to the winning output string??
(x) in (6), and hence to the loss L(??
(x), yi).
Och(2003) developed a specialized line search to per-form the optimization, which is not scalable whenthe number of model parameters ?
is large.3.3.2 Randomized DecodingInstead of using the argmax of (6), we assumeduring training that ??
(x) is itself random, i.e.
theMT system randomly outputs a translation y withprobability p?
(y |x).
As a result, we will modifyour objective function of (4) to take yet another ex-pectation over the unknown y.
Specifically, we willreplace L(??
(x), yi) in (4) with?yp?
(y |x)L(y, yi).
(7)Now, the minimum imputed empirical risk objectiveof (4) becomes??
= argmin?1NN?i=1?x,yp?
(x | yi) p?
(y |x)L(y, yi)(8)If the loss function L(y, yi) is a negated BLEU, thisis equivalent to performing minimum-risk trainingdescribed by (Smith and Eisner, 2006; Li and Eisner,2009) on the imputed data.109One can manipulate the loss function to support othermethods that use deterministic decoding, such as Perceptron(Collins, 2002) and MIRA (Crammer et al, 2006).10Again, one may manipulate the loss function to supportother probabilistic methods that use randomized decoding, suchas CRFs (Lafferty et al, 2001).The objective function in (8) is now differentiable,since each coefficient p?
(y |x) is a differentiablefunction of ?, and thus amenable to optimizationby gradient-based methods; we use the L-BFGS al-gorithm (Liu et al, 1989) in our experiments.
Weperform experiments with the syntax-based MT sys-tem Joshua (Li et al, 2009a), which implementsdynamic programming algorithms for second-orderexpectation semirings (Li and Eisner, 2009) to effi-ciently compute the gradients needed for optimizing(8).3.4 Approximating p?
(x | yi)As mentioned at the end of Section 3.1, it is com-putationally infeasible to forward-translate each ofthe imputed reverse translations xij .
We proposefour approximations that are computationally feasi-ble.
Each may be regarded as a different approxima-tion of p?
(x | yi) in equations (4) or (8).k-best.
For each yi, add to the imputed training setonly the k most probable translations {xi1, .
.
.
xik}according to p?
(x | yi).
(These can be extractedfrom Xi using standard algorithms (Huang and Chi-ang, 2005).)
Rescale their probabilities to sum to 1.Sampling.
For each yi, add to the training set k in-dependent samples {xi1, .
.
.
xik} from the distribu-tion p?
(x | yi), each with weight 1/k.
(These can besampled from Xi using standard algorithms (John-son et al, 2007).)
This method is known in the liter-ature as multiple imputation (Rubin, 1987).Lattice.
11 Under certain special cases it is be pos-sible to compute the expected loss in (3) exactlyvia dynamic programming.
Although Xi does con-tain exponentially many translations, it may use a?packed?
representation in which these translationsshare structure.
This representation may further-more enable sharing work in forward-translation, soas to efficiently translate the entire set Xi and ob-tain a distribution over translations y.
Finally, theexpected loss under that distribution, as required byequation (3), may also be efficiently computable.All this turns out to be possible if (a) the poste-rior distribution p?
(x | yi) is represented by an un-11The lattice approximation is presented here as a theoreti-cal contribution, and we do not empirically evaluate it since itsimplementation requires extensive engineering effort that is be-yond the main scope of this paper.923ambiguous weighted finite-state automaton Xi, (b)the forward translation system ??
is structured in acertain way as a weighted synchronous context-freegrammar, and (c) the loss function decomposes in acertain way.
We omit the details of the constructionas beyond the scope of this paper.In our experimental setting described below, (b) istrue (using Joshua), and (c) is true (since we use aloss function presented by Tromble et al (2008) thatis an approximation to BLEU and is decomposable).While (a) is not true in our setting because Xi is ahypergraph (which is ambiguous), Li et al (2009b)show how to approximate a hypergraph representa-tion of p?
(x | yi) by an unambiguous WFSA.
Onecould then apply the construction to this WFSA12,obtaining an approximation to (3).Rule-level Composition.
Intuitively, the reasonwhy the structure-sharing in the hypergraphXi (gen-erated by the reverse system) cannot be exploitedduring forward translating is that when the forwardHiero system translates a string xi ?
Xi, it mustparse it into recursive phrases.But the structure-sharing within the hypergraph ofXi has already parsed xi into recursive phrases, in away determined by the reverse Hiero system; eachtranslation phrase (or rule) corresponding to a hy-peredge.
To exploit structure-sharing, we can usea forward translation system that decomposes ac-cording to that existing parse of xi.
We can do thatby considering only forward translations that respectthe hypergraph structure of Xi.
The simplest way todo this is to require complete isomorphism of theSCFG trees used for the reverse and forward trans-lations.
In other words, this does round-trip impu-tation (i.e., from y to x, and then to y?)
at the rulelevel.
This is essentially the approach taken by Li etal.
(2010).3.5 The Log-Linear Model p?We have not yet specified the form of p?.
Followingmuch work in MT, we begin with a linear modelscore(x, y) = ?
?
f(x, y) =?k?kfk(x, y) (9)where f(x, y) is a feature vector indexed by k. Ourdeterministic test-time translation system ??
simply12Note that the forward translation of a WFSA is tractable byusing a lattice-based decoder such as that by Dyer et al (2008).outputs the highest-scoring y for fixed x.
At trainingtime, our randomized decoder (Section 3.3.2) usesthe Boltzmann distribution (here a log-linear model)p?
(y |x) =e?
?score(x,y)Z(x) =e??score(x,y)?y?
e??score(x,y?)
(10)The scaling factor ?
controls the sharpness of thetraining-time distribution, i.e., the degree to whichthe randomized decoder favors the highest-scoringy.
For large ?, our training objective approachesthe imputed risk of the deterministic test-time sys-tem while remaining differentiable.In a task like MT, in addition to the input x andoutput y, we often need to introduce a latent variabled to represent the hidden derivation that relates x toy.
A derivation d represents a particular phrase seg-mentation in a phrase-based MT system (Koehn etal., 2003) and a derivation tree in a typical syntax-based system (Galley et al, 2006; Chiang, 2007).We change our model to assign scores not to an(x, y) pair but to the detailed derivation d; in partic-ular, now the function f that extracts a feature vectorcan look at all of d. We replace y by d in (9)?
(10),and finally define p?
(y|x) by marginalizing out d,p?
(y |x) =?d?D(x,y)p?
(d |x) (11)where D(x, y) represents the set of derivations thatyield x and y.4 Minimum Imputed Risk vs. EMThe notion of imputing missing data is familiarfrom other settings (Little and Rubin, 1987), particu-larly the expectation maximization (EM) algorithm,a widely used generative approach.
So it is instruc-tive to compare EM with minimum imputed risk.One can estimate ?
by maximizing the log-likelihood of the data {(xi, yi), i = 1, .
.
.
, N} asargmax?1NN?i=1log p?
(xi, yi).
(12)If the xi?s are missing, EM tries to iteratively maxi-mize the marginal probability:argmax?1NN?i=1log?xp?
(x, yi).
(13)924The E-step of each iteration comprises comput-ing ?x p?t(x | yi) log p?
(x, yi), the expected log-likelihood of the complete data, where p?t(x | yi) isthe conditional part of p?t(x, yi) under the currentiterate ?t, and the M-step comprises maximizing it:?t+1 = argmax?1NN?i=1?xp?t(x | yi) log p?
(x, yi).
(14)Notice that if we replace p?t(x|yi) with p?
(x | yi)in the equation above, and admit negated log-likelihood as a loss function, then the EM update(14) becomes identical to (4).
In other words, theminimum imputed risk approach of Section 3.1 dif-fers from EM in (i) using an externally-provided andstatic p?, instead of refining it at each iteration basedon the current p?t , and (ii) using a specific loss func-tion, namely negated log-likelihood.So why not simply use the maximum-likelihood(EM) training procedure for MT?
One reason isthat it is not discriminative: the loss function (e.g.negated BLEU) is ignored during training.A second reason is that training good joint modelsp?
(x, y) is computationally expensive.
Contempo-rary MT makes heavy use of log-linear probabilitymodels, which allow the system designer to injectphrase tables, linguistic intuitions, or prior knowl-edge through a careful choice of features.
Comput-ing the objective function of (14) in closed form isdifficult if p?
is an arbitrary log-linear model, be-cause the joint probability p?
(xi, yi) is then definedas a ratio whose denominatorZ?
involves a sum overall possible sentence pairs (x, y) of any length.By contrast, our discriminative framework willonly require us to work with conditional models.While conditional probabilities such as p?
(x | y) andp?
(y |x) are also ratios, computing their denomina-tors only requires us to sum over a packed forest ofpossible translations of a given y or x.13In summary, EM would impute missing data us-ing p?
(x | y) and predict outputs using p?
(y |x),both being conditional forms of the same jointmodel p?
(x, y).
Our minimum imputed risk train-ing method is similar, but it instead uses a pair of13Analogously, discriminative CRFs have become more pop-ular than generative HMMs because they permit efficient train-ing even with a wide variety of log-linear features (Lafferty etal., 2001).separately parameterized, separately trained mod-els p?
(x | y) and p?
(y |x).
By sticking to condi-tional models, we can efficiently use more sophis-ticated model features, and we can incorporate theloss function when we train ?, which should improveboth efficiency and accuracy at test time.5 Experimental ResultsWe report results on Chinese-to-English translationtasks using Joshua (Li et al, 2009a), an open-sourceimplementation of Hiero (Chiang, 2007).5.1 Baseline Systems5.1.1 IWSLT TaskWe train both reverse and forward baseline sys-tems.
The translation models are built using the cor-pus for the IWSLT 2005 Chinese to English trans-lation task (Eck and Hori, 2005), which comprises40,000 pairs of transcribed utterances in the traveldomain.
We use a 5-gram language model withmodified Kneser-Ney smoothing (Chen and Good-man, 1998), trained on the English (resp.
Chi-nese) side of the bitext.
We use a standard train-ing pipeline and pruning settings recommended by(Chiang, 2007).5.1.2 NIST TaskFor the NIST task, the TM is trained on about 1Mparallel sentence pairs (about 28M words in eachlanguage), which are sub-sampled from corpora dis-tributed by LDC for the NIST MT evaluation using asampling method implemented in Joshua.
We alsoused a 5-gram language model, trained on a data setconsisting of a 130M words in English Gigaword(LDC2007T07) and the bitext?s English side.5.2 Feature FunctionsWe use two classes of features fk for discriminativetraining of p?
as defined in (9).5.2.1 Regular Hiero FeaturesWe include ten features that are standard in Hi-ero (Chiang, 2007).
In particular, these includeone baseline language model feature, three baselinetranslation models, one word penalty feature, threefeatures to count how many rules with an arity of925zero/one/two are used in a derivation, and two fea-tures to count how many times the unary and binaryglue rules in Hiero are used in a derivation.5.2.2 Target-rule Bigram FeaturesIn this paper, we do not attempt to discrimina-tively tune a separate parameter for each bilingualrule in the Hiero grammar.
Instead, we train severalhundred features that generalize across these rules.For each bilingual rule, we extract bigram fea-tures over the target-side symbols (including non-terminals and terminals).
For example, if a bilingualrule?s target-side is ?on the X1 issue of X2?
whereX1 and X2 are non-terminals (with a position in-dex), we extract the bigram features on the, the X ,X issue, issue of, and of X .
(Note that the posi-tion index of a non-terminal is ignored in the fea-ture.)
Moreover, for the terminal symbols, we willuse their dominant POS tags (instead of the sym-bol itself).
For example, the feature the X becomesDTX .
We use 541 such bigram features for IWSLTtask (and 1023 such features for NIST task) that firefrequently.5.3 Data Sets for Discriminative Training5.3.1 IWSLT TaskIn addition to the 40,000 sentence pairs used totrain the baseline generative models (which are usedto compute the features fk), we use three bilingualdata sets listed in Table 1, also from IWSLT, for dis-criminative training: one to train the reverse modelp?
(which uses only the 10 standard Hiero featuresas described in Section 5.2.1),14 one to train the for-ward model ??
(which uses both classes of featuresdescribed in Section 5.2, i.e., 551 features in total),and one for test.Note that the reverse model ?
is always trained us-ing the supervised data of Dev ?, while the forwardmodel ?
may be trained in a supervised or semi-supervised manner, as we will show below.In all three data sets, each Chinese sentence xihas 16 English reference translations, so each yi isactually a set of 16 translations.
When we imputedata from yi (in the semi-supervised scenario), we14Ideally, we should train ?
to minimize the conditionalcross-entropy (5) as suggested in section 3.2.
In the presentresults, we trained ?
discriminatively to minimize risk, purelyfor ease of implementation using well versed steps.Data set Purpose # of sentencesChinese EnglishDev ?
training ?
503 503?16Dev ?
training ?
503?
503?16Eval ?
testing 506 506?16Table 1: IWSLT Data sets used for discriminativetraining/test.
Dev ?
is used for discriminatively trainingof the reverse model ?, Dev ?
is for the forward model,and Eval ?
is for testing.
The star ?
for Dev ?
empha-sizes that some of its Chinese side will not be used in thetraining (see Table 2 for details).actually impute 16 different values of xi, by usingp?
to separately reverse translate each sentence inyi.
This effectively adds 16 pairs of the form (xi, yi)to the training set (see section 3.4), where each xiis a different input sentence (imputed) in each case,but yi is always the original set of 16 references.5.3.2 NIST TaskFor the NIST task, we use MT03 set (having 919sentences) to tune the component parameters in boththe forward and reverse baseline systems.
Addition-ally, we use the English side of MT04 (having 1788sentences) to perform semi-supervised tuning of theforward model.
The test sets are MT05 and MT06(having 1082 and 1099 sentences, respectively).
Inall the data sets, each source sentence has four refer-ence translations.5.4 Main ResultsWe compare two training scenarios: supervised andsemi-supervised.
The supervised system (?Sup?
)carries out discriminative training on a bilingual dataset.
The semi-supervised system (?+Unsup?)
addi-tionally uses some monolingual English text for dis-criminative training (where we impute one Chinesetranslation per English sentence).Tables 2 and 3 report the results for the two tasksunder two training scenarios.
Clearly, adding unsu-pervised data improves over the supervised case, byat least 1.3 BLEU points in IWSLT and 0.5 BLEU inNIST.5.5 Results for Analysis PurposesBelow, we will present more results on the IWSLTdata set to help us understand the behavior of the926Training scenario Test BLEUSup, (200, 200?16) 47.6+Unsup, 101?16 Eng sentences 49.0+Unsup, 202?16 Eng sentences 48.9+Unsup, 303?16 Eng sentences 49.7?Table 2: BLEU scores for semi-supervised training forIWSLT task.
The supervised system (?Sup?)
is trainedon a subset of Dev ?
containing 200 Chinese sentencesand 200?16 English translations.
?+Unsup?
means thatwe include additional (monolingual) English sentencesfrom Dev ?
for semi-supervised training; for each En-glish sentence, we impute the 1-best Chinese translation.A star ?
indicates a result that is signicantly better thanthe ?Sup?
baseline (paired permutation test, p < 0.05).Training scenario Test BLEUMT05 MT06Sup, (919, 919?4) 32.4 30.6+Unsup, 1788 Eng sentences 33.0?
31.1?Table 3: BLEU scores for semi-supervised training forNIST task.
The ?Sup?
system is trained on MT03, whilethe ?+Unsup?
system is trained with additional 1788 En-glish sentences from MT04.
(Note that while MT04 has1788?4 English sentences as it has four sets of refer-ences, we only use one such set, for computational ef-ficiency of discriminative training.)
A star ?
indicates aresult that is signicantly better than the ?Sup?
baseline(paired permutation test, p < 0.05).methods proposed in this paper.5.5.1 Imputation with Different ReverseModelsA critical component of our unsupervised methodis the reverse translation model p?
(x | y).
Wewonder how the performance of our unsupervisedmethod changes when the quality of the reverse sys-tem varies.
To study this question, we used two dif-ferent reverse translation systems, one with a lan-guage model trained on the Chinese side of the bi-text (?WLM?
), and the other one without using sucha Chinese LM (?NLM?).
Table 4 (in the fully unsu-pervised case) shows that the imputed Chinese trans-lations have a far lower BLEU score without the lan-guage model,15 and that this costs us about 1 English15The BLEU scores are low even with the language modelbecause only one Chinese reference is available for scoring.Data size Imputed-CN BLEU Test-EN BLEUWLM NLM WLM NLM101 11.8 3.0 48.5 46.7202 11.7 3.2 48.9 47.6303 13.4 3.5 48.8 47.9Table 4: BLEU scores for unsupervised trainingwith/without using a language model in the reversesystem.
A data size of 101 means that we use onlythe English sentences from a subset of Dev ?
containing101 Chinese sentences and 101?16 English translations;for each English sentence we impute the 1-best Chinesetranslation.
?WLM?
means a Chinese language modelis used in the reverse system, while ?NLM?
means noChinese language model is used.
In addition to reportingthe BLEU score on Eval ?, we also report ?Imputed-CNBLEU?, the BLEU score of the imputed Chinese sentencesagainst their corresponding Chinese reference sentences.BLEU point in the forward translations.
Still, evenwith the worse imputation (in the case of ?NLM?
),our forward translations improve as we add moremonolingual data.5.5.2 Imputation with Different k-best SizesIn all the experiments so far, we used the reversetranslation system to impute only a single Chinesetranslation for each English monolingual sentence.This is the 1-best approximation of section 3.4.Table 5 shows (in the fully unsupervised case)that the performance does not change much as k in-creases.16 This may be because that the 5-best sen-tences are likely to be quite similar to one another(May and Knight, 2006).
Imputing a longer k-bestlist, a sample, or a lattice for xi (see section 3.4)might achieve more diversity in the training inputs,which might make the system more robust.6 ConclusionsIn this paper, we present an unsupervised discrimi-native training method that works with missing in-puts.
The key idea in our method is to use a re-verse model to impute the missing input from the ob-served output.
The training will then forward trans-late the imputed input, and choose the parameters ofthe forward model such that the imputed risk (i.e.,16In the present experiments, however, we simply weightedall k imputed translations equally, rather than in proportion totheir posterior probabilities as suggested in Section 3.4.927Training scenario Test BLEUUnsup, k=1 48.5Unsup, k=2 48.4Unsup, k=3 48.9Unsup, k=4 48.5Unsup, k=5 48.4Table 5: BLEU scores for unsupervised training withdifferent k-best sizes.
We use 101?16 monolingual En-glish sentences, and for each English sentence we imputethe k-best Chinese translations using the reverse system.the expected loss of the forward translations withrespect to the observed output) is minimized.
Thismatches the intuition that the probabilistic ?round-trip?
translation from the target-language sentenceto the source-language and back should have low ex-pected loss.We applied our method to two Chinese to Englishmachine translation tasks (i.e.
IWSLT and NIST).We showed that augmenting supervised data withunsupervised data improved performance over thesupervised case (for both tasks).Our discriminative model used only a smallamount of training data and relatively few features.In future work, we plan to test our method in settingswhere there are large amounts of monolingual train-ing data (enabling many discriminative features).Also, our experiments here were performed on a lan-guage pair (i.e., Chinese to English) that has quiterich bilingual resources in the domain of the testdata.
In future work, we plan to consider low-resource test domains and language pairs like Urdu-English, where bilingual data for novel domains issparse.AcknowledgementsThis work was partially supported by NSF GrantsNo IIS-0963898 and No IIS-0964102 and theDARPA GALE Program.
The authors thank MarkusDreyer, Damianos Karakos and Jason Smith for in-sightful discussions.ReferencesPhil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In ACL, pages 200?208.Stanley F. Chen and Joshua Goodman.
1998.
An empir-ical study of smoothing techniques for language mod-eling.
Technical report.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine translation.In NAACL, pages 218?226.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Michael Collins.
2002.
Discriminative training methodsfor hidden markov models: theory and experimentswith perceptron algorithms.
In EMNLP, pages 1?8.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
J. Mach.
Learn.
Res., 7:551?585.Christopher Dyer, Smaranda Muresan, and Philip Resnik.2008.
Generalizing word lattice translation.
In ACL,pages 1012?1020.Matthias Eck and Chiori Hori.
2005.
Overview of theiwslt 2005 evaluation campaign.
In In IWSLT.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In ACL,pages 961?968.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In IWPT, pages 53?64.Jui-Ting Huang, Xiao Li, and Alex Acero.
2010.
Dis-criminative training methods for language models us-ing conditional entropy criteria.
In ICASSP.Mark Johnson, Thomas Griffiths, and Sharon Goldwa-ter.
2007.
Bayesian inference for PCFGs via Markovchain Monte Carlo.
In NAACL, pages 139?146.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In NAACL,pages 48?54.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In ICML.Zhifei Li and Jason Eisner.
2009.
First- and second-orderexpectation semirings with applications to minimum-risk training on translation forests.
In EMNLP, pages40?51.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenThornton, Jonathan Weese, and Omar.
Zaidan.
2009a.Joshua: An open source toolkit for parsing-based ma-chine translation.
In WMT09, pages 26?30.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b.Variational decoding for statistical machine transla-tion.
In ACL, pages 593?601.Zhifei Li, Ziyuan Wang, Sanjeev Khudanpur, and JasonEisner.
2010.
Unsupervised discriminative language928model training for machine translation using simulatedconfusion sets.
In COLING, pages 556?664.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminative ap-proach to machine translation.
In ACL, pages 761?768.R.
J.
A.
Little and D. B. Rubin.
1987.
Statistical Analysiswith Missing Data.
J. Wiley & Sons, New York.Dong C. Liu, Jorge Nocedal, and Dong C. 1989.
On thelimited memory bfgs method for large scale optimiza-tion.
Mathematical Programming, 45:503?528.Wolfgang Macherey, Franz Och, Ignacio Thayer, andJakob Uszkoreit.
2008.
Lattice-based minimum er-ror rate training for statistical machine translation.
InEMNLP, pages 725?734.Jonathan May and Kevin Knight.
2006.
A better n-bestlist: practical determinization of weighted finite treeautomata.
In NAACL, pages 351?358.Thomas Minka.
2000.
Empirical risk minimization isan incomplete inductive principle.
In MIT Media Labnote.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In ACL, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: A method for automatic eval-uation of machine translation.
In ACL, pages 311?318.D.
B. Rubin.
1987.
Multiple Imputation for Nonresponsein Surveys.
J. Wiley & Sons, New York.David A. Smith and Jason Eisner.
2006.
Minimumrisk annealing for training log-linear models.
In ACL,pages 787?794.Roy Tromble, Shankar Kumar, Franz Och, and WolfgangMacherey.
2008.
Lattice minimum-Bayes-risk de-coding for statistical machine translation.
In EMNLP,pages 620?629.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online large-margin training for statis-tical machine translation.
In EMNLP-CoNLL, pages764?773.929
