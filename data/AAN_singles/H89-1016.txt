RECENT PROGRESSIN THE SPHINX SPEECH RECOGNITION SYSTEMKai-Fu Lee, Hsiao-Wuen Hon, Mei-Yuh HwangComputer Science DepartmentCarnegie Mellon UniversityPittsburgh, PA 15213AbstractThis paper describes recent improvements in the SPHINXSpeech Recognition System.
These enhancements includefunction-phrase modeling, between-word coarticulationmodeling, and corrective training.
On the DARPA resourcemanagement task, SPHINX attained a speaker-independentword accuracy of 96% with a grammar (perplexity 60), and82% without grammar (perplexity 997).1.
IntroductionSPHINX is a large-vocabulary, speaker-independent,continuous speech recognition system based on discretehidden Markov models (HMMs) with LPC-derivedparameters.
In order to deal with the problem ofspeaker independence, we added knowledge to theseHMMs in several ways.
We represented additionalknowledge through the use of multiple codebooks.
Wealso enhanced the recognizer with word durationmodeling.
In order to model co-articulation i con-tinuous speech, we introduced the use of function-word-dependent phone models, and generalizedtriphone models.More recently, we have made considerable progresswith the SPHINX System.
We reformulated the general-ized triphone clustering algorithm as a maximum-likelihood procedure, and carried out some experimentswith generalized triphones.
We also implemented andevaluated the modeling of function phrases, andbetween-word coarticulation modeling rising general-ized triphones.
The latter experiment reduced SPHINX'serror rate by 24-44%.
We modified the corrective train-ing algorithm \[1\] for speaker-independent, continuousspeech recognition.
Corrective training reducedSPHINX's error rate by 20-24%.In this paper, we will describe all components of theSPHINX System, with emphasis on the recent improve-ments.
The SPHaNX System has been described in\[2\] and \[3\].
Publications on the recent improvementswill be forthcoming.On the 991-word DARPA resource managementtask, SPHINX achieved speaker-independent word recog-nition accuracies of 82% and 96%, with grammars ofperplexity 991 and 60, respectively.
Results with the1988 and 1989 test data resulted in 78 and 76% withoutgrammar, and 96% and 94% with the word pair gram-mar.2.
Speech RepresentationThe speech is sampled at 16 KHz,  and pre-emphasized with a filter of 1 - 0.97z -1.
Then, a Ham-ming window with a width of 20 msec is applied every10 msec.
Autocorrelation a alysis with order 14 is fol-lowed by LPC analysis with order 14.
Finally, 12 LPC-derived cepstral coefficients are computed from theLPC coefficients, and these LPC cepstral coefficientsare transformed to a mel-scale using a bilinear trans-form.These 12 coefficients are vector quantized into acodebook of 256 prototype vectors.
In order to incor-porate additional speech parameters, we created two ad-ditional codebooks.
One codebook is vector quantizedfrom differential coefficients.
The differential coef-ficient of frame n is the difference between the coef-ficient of frame n+2 and frame n-2.
This 40 msec.difference captures the slope of the spectral envelope.The other codebook is vector quantized from energyand differential energy values.3.
Context-Independent HMM TrainingSPHINX is based on phonetic hidden Markov models.We identified a set of 48 phones, and a hidden Markovmodel is trained for each phone.
Each phonetic HMMcontains three discrete output distributions of VQ sym-bols.
Each distribution is the joint density of the threecodebook pdf's, which are assumed to be independent.The use of multiple codebooks was introduced byGupta, et al \[4\].We initialize our training procedure with the TIMIT125phonetically labeled database.
With this initialization,we use the forward-backward algorithm to train theparameters of the 48 phonetic HMMs.
The trainingcorpus consists of 4200 task-domain sentences spokenby 105 speakers.
For each sentence, word HMMs areconstructed by concatenating phone HMMs.
Theseword HMMs are then concatenated into a large sen-tence HMM, and trained on the corresponding speech.Because the initial estimates are quite good, only twoiterations of the forward-backward algorithm are run.This training phase produces 48 context-independentphone models.
In the next two sections, we will discussthe second Iraining phase for context-dependent phonemodels.4.
Function Word/Phrase DependentModelsOne problem with continuous peech is the uncleararticulation of function words, such as a, the, in, of, etc.Since the set of function words in English is limited andfunction words occur frequently, it is possible to modeleach phone in each function word separately.
By ex-plicitly modeling the most difficult sub-vocabulary,recognition rate can be increased substantially.
Weselected a set of 42 function words, which contained105 phones.
We modeled each of these phonesseparately.We have found that function words are hardest orecognize when they occur in clusters, such as that arein the.
The words are even less clearly articulated, andhave strong inter-word eoarticulatory effects.
In viewof this, we created a set of phone models specific tofunction phrases, which are phrases that consist of onlyfunction words.
We identified 12 such phrases,modified the pronunciations of these phrases accordingto phonological rules, and modeled the phones in themseparately.
A few examples of these phrases are: is the,that are, and of the.5.
Generalized Triphone ModelsThe function-word and function-phrase dependentphone models provide better representations of thefunction words.
However, simple phone models for thenon-function words are inadequate, because the realiza-tion of a phone crucially depends on context.
In orderto model the most prominent contextual effect,Schwartz, et al \[5\] proposed the use of triphonemodels.
A different triphone model is used for each leftand right context.
While triphone models are sensitiveto neighboring phonetic ontexts, and have led to goodresults, there are a very large number of them, whichcan only be sparsely trained.
Moreover, they do nottake into account he similarity of certain phones intheir affect on other phones (such as /\]a/ and /p /  onvowels).In view of this, we introduce the generalizedtriphone model.
Generalized triphones are created fromtriphone models using a clustering procedure:1.
An HMM is generated for every triphone con-text.2.
Clusters of triphones are created; initially, eachclusters consists of one triphone.3.
Find the most similar pair of clusters whichrepresent the same phone, and merge them.4.
For each pair of same-phone clusters, considermoving every element from one to the other.1.
Move the element if the resultingconfiguration is an improvement.2.
Repeat until no such moves are left.5.
Until some convergence riterion is met, go tostep 2.To determine the distance between two models, weuse the following distance metric:(H  (Pa( i ) )Na(O) ' (H  (Pb (i))Nb(O)D(a,b) = i i (1)\ [ I  (Pm (i)) Nm ~i)iwhere D (a, b) is the distance between two models ofthe same phone in context a and b. Pa (/) is the outputprobability of codeword i in model a, and N a (i) is thecount of codeword i in model a. m is the merged modelby adding N a and N b.
In measuring the distance be-tween the two models, we only consider the outputprobabilities, and ignore the transition probabilities,which are of secondary importance.Equation 1 measures the ratio between the probabil-ity that the individual distributions generated the train-ing data and the probability that the combined istribu-tion generated the training data.
Thus, it is consistentwith the maximum-likelihood criterion used in theforward-backward algorithm.
This distance metric isequivalent to, and was motivated by, entropy clusteringused in \[6\] and \[7\].This context generalization algorithm provides theideal means for finding the equilibrium betweentrainability and sensitivity.
Given a fixed amount of126training data, it is possible to find the largest number oftrainable detailed models.
Armed with this technique,we could attack any problem and find the "right" num-ber of models that are as sensitive and trainable as pos-sible.
This is illustrated in Figure 1, which shows thatthe optimal number of models increases as the trainingdata is increased.20.0 \[\] ~--?
105 speakers~--"w 80 speakers \]1~ I ~ ' ' "  55 speakers I1S.8 I \[?
: 30speakers \[12.6, .
o7.9 I.
~ ~'-'~:.T" ~- * *%I I6"30 200 400 600 800 ' ' 1400 1000 1200Number of generalized triphone modelsFigure 1: Error rate as a function of theamount of training and the number of models.6.
Between-Word Coarticulation ModelingTriphone and generalized triphone models are power-ful subword modeling techniques because they accountfor the left and right phonetic ontexts, which are theprincipal causes of phonetic variability.
However,triphone-based models consider only intra-word con-text.
For example, in the word speech (/s p iych/), both left and right contexts for /p /  and / i y /are known, while the left context for / s /and  the rightcontext for / ch /  are a special symbol for "wordboundary".
However, in continuous peech, a word-boundary phone is strongly affected by the phonebeyond the word boundary.
This is especially true forshort function words like the or a.A simple extension of triphones to model between-word coarticulation is problematic because the numberof triphone models grows sharply when between-wordtriphones are considered.
For example, there are 2381within-word triphones in our 991-word task.
But thereare 7057 triphones when between-word triphones arealso considered.Therefore, generalized triphones are particularlysuitable for modeling between-word coarticulation.
Wefirst generaated 7057 triphone models that accounted forboth intra-word and inter-word triphones.
These 7057models were then clustered into 1000 generalizedtriphone models.
The membership ofeach generalizedtriphone is retained, so that inter-word contextual con-straints can be applied uring training and recognition.The main change in the training algorithm is in theconstruction of the sentence model.
Two connectionsare now needed to link two words together.
The firstuses the known context o connect he appropriatetriphones, and the second allows for the possibility of abetween-word silence.
In that case, a silence context isused.
Figure 2 illuslxates the word boundary network oftwo words, where word w 1 consists of phones A, B, andC, and word w 2 consists of D, E, and F.CCB,D) D(C,E)C(B,SIL) SIL D(SIL,E)Figure 2: Sentence network connectionduring training.
Here word w 1 consists ofphones A, B, and C, and word w z consists ofD, E, and F. P(L,R) represents a phone Pwith left-context phone L and right-contextphone R.For words with only one or two phones, sentencemodel concatenation is more complex.
If w 2 ispronounced (D E), then both D(C,E) and D(SIL,E)must be further forked into E(D,X) and E(D,SIL),where X is the first phone of the next word.
This iseven more complicated when several one-phone andtwo-phone words are concatenated.
To reduce the com-plexity of the pronunciation graph of a sentence, weintroduce dummy states to merge transitions whose ex-pected contexts are the same.The recognition algorithm must be modified becausewords may now have multiple begining and endingphones.
Figure 3 illustrates the connection betweentwo words during recognition.
Like the training phase,the two words are connected both directly and througha silence.
If one or both of the triphones has not oc-curred in the training data, we use the context-independent phone (or monophone) instead.
Therefore,the direct connection between two words could be em-bodied in one of four forms:127A E(D,SIL) V(SIL,W) ZE(D,V) V(E,W)Figure 3: Transitioning from one word (A BC D E) to another (V W X Y Z) in recog-nition.?
triphone to triphone.?
triphone to monophone.?
monophone totriphone.?
monophone tomonophone.The modeling of between-word coarticulationreduced SPHINX's error rate by 24-44%, for differenttest sets and grammars.
More details about our im-plementation a d results can be found in \[8\].7.
Corrective TrainingBahl et al \[1\] introduced the corrective training algo-rithm for HMMs as an alternative to the forward-backward algorithm.
While the forward-backward al-gorithm attempts to increase the probability that themodels generated the training data, corrective trainingattempts to maximize the recognition rate on the train-ing data.
This algorithm has two components: (1)error-correction learning--which improves correctwords and suppresses misrecognized words, (2)reinforcement learning - -which improves correctwords and suppresses near-misses.
Applied to the IBMspeaker-dependent isolated-word office correspondencetask, this algorithm reduced the error rate by 16% ontest data and 88% on training data.
This improvement,while significant, suggests that corrective training be-comes overly specialized for the training data.In this study, we extend the corrective and reinforce-ment learning algorithm to speaker-independent,continuous peech recognition.
Speaker independencemay present some problems, because corrections ap-propriate for one speaker may be inappropriate foranother.
However, with a speaker-independent task, itis possible to collect and use a large training set.
Moretraining provides not only improved generalization butalso a greater coverage of the vocabulary.
We also usecross-validation to increase the effective training datasize.
Cross-validation partitions the training data anddetermines misrecognitions using models trained ondifferent partitions.
This simulation of actual recog-nition leads to more realistic misrecognitions for errorcorrection.Extension to continuous speech is more problematic.With isolated-word input, both error-correcting andreinforcement training are relatively straighforward,since all errors are simple substitutions.
Bahl, et al\[1\] determined both misrecognized words and near-misses by matching the utterance against he entirevocabulary.
However, with continuous speech, the er-rors include insertions and deletions.
Moreover, manysubstitutions appear as phrase-substitutions, such ashome any for how many.
These problems make rein-forcement learning difficult.
We propose an algorithmthat hypothesizes near-miss entences for any givensentence.
First, a dynamic programming algorithm isused to align each correct sentence with the correspond-ing misrecognized sentence in the cross-recognizedtraining set to produce an ordered list of likely phrasesubstitutions.
Since simple text-to-text alignmentwould not be sensitive to sub-word and sub-phonesimilarities, we used a frame-level distance metric.This list of phrase substitutions are then used to ran-domly hypothesize near-miss entences for reinforce-ment learning.Our experiments with corrective and reinforcementlearning showed that our modifications led to a 20%error-rate reduction without grammar (72% on trainingset), and a 23% reduction with grammar (63% on train-ing set).
This demonstrated that increased training,both through speaker-independent data collection andthrough cross-validation, narrowed the gap between theresults from training and testing data.
Furthermore, thisshowed that our extension of the IBM corrective train-ing algorithm to continuous peech was successful.More details about this work are described in \[9\] and\[10\].8.
Summary of Training ProcedureThe SPHINX training procedure operates in threestages.
In the first stage, 48 context-independentphonetic models are trained.
In the second stage, themodels from the first stage are used to initialize thetraining of context-dependent phone models, whichcould be generalized triphone models and/or the func-tion word/phrase dependent models.
Since manyparameters in the context-dependent models were neverobserved, we interpolate the context-dependent modelparameters with the corresponding context-independentones.
We use deleted interpolation \[11\] to derive ap-propriate weights in the interpolation.
The third andfinal stage uses corrective training to refine the dis-128cnminatory ability of the models.
The SPHINX Ixainingprocedure is shown in Figure 4.Initial C-indPhone ModelsForw~ard.-Ind.
IPhone Models ,5\[ Forward-I c-in .l le-DDeletedInterp~lation~4 Trained C-Dep PhoneodelsI Corrective & \[ Reinforcementl/,5CorrectedC-Dep PhoneModelsTask-DomainmTrainingSpeechFigure 4: The SPHINX Training Procedure.9.
HMM Recognition with Word DurationFor recognition, we use a Viterbi search that finds theoptimal state sequence in a large HMM network.
Atthe highest level, this HMM is a network of wordHMMs, arranged according to the grammar.
Each wordis instantiated with its phonetic pronunciation network,and each phone is instantiated with the correspondingphone model.
Beam search is used to reduce theamount of computation.One problem with HMMs is that they do not providevery good duration models.
We incorporated wordduration into SPHINX as a part of the Viterbi search.The duration of a word is modeled by a univariateGaussian distribution, with the mean and variance s-timated from a supervised Viterbi segmentation f thetraining set.
By precomputing the duration score forvarious durations, this duration model has essentially nooverhead.10.
ResultsThe SPHINX System was tested on 150 sentencesfrom 15 speakers.
These sentences were the officialDARPA test data for evaluations in March and October1987.
The word accuracies for various versions ofSPHINX with the word-pair grammar (perplexity 60) andthe null grammar (perplexity 991) are shown in Table 1.Word accuracy is defined as the percent of words cor-rect minus the percent of insertions.Version No Grammar Word Pair1 Codebook 25.8% 58.1%3 Codebooks 45.3% 84.4%+Duration 49.6% 83.8%+Fn-word 57.0% 87.9%+Fn-phrase 59.2% 88.4%+Gen-triphone !
72.8% 94.2%i-Between-word I 77.9% 95.5% I+Corrective !
81.9% 96.2%Table 1: Results of various versions of SPHINX.The first improvement was obtained by adding ad-ditonal feature sets and codebooks.
Next, we foundduration modeling to be helpful when no grammar wasused.
Modeling function words and generalizedtriphones both led to substantial improvements.
Wealso found that generalized triphones outperformedtriphones, while saving 60% memory*.
The improve-ments from function-phrase dependent modeling en-couraged us to implement between-word triphonemodels.
This led to substantial improvements with noincrease in the number of models.
Finally, we showedthe effectiveness of our extension of the correctivetraining algorithm to speaker-independent continuousspeech.Since the above experiments were repeatedly run onthe same set of test data, it is important to verify thatSPHINX is capable of achieving comparable l vels ofperformance on new test data.
Recently, SPHINX wasevaluated on two new sets of test data (June 1988evaluation and February 1989 evaluation).
With nogrammar, ecognition accuracies of 78.1% and 76.4%were obtained on these two test sets.
With the word-pair grammar, the accuracies were 95.7% and 93.9%.
*More detailed descriptions and results on contextual modeling canbe found in \[2\] or \[3\].129II.
ConclusionThis paper has presented an up-to-date description ofthe SPHINX Speech Recognition System.
We havedescribed a number of recent improvements, includingfunction-phrase modeling, between-word coarticulationmodeling, and corrective and reinforcement training.Through these techniques we demonstrated that ac-curate large-vocabulary speaker-independent con-tinuous speech recognition is feasible.
We reportrecognition accuracies of 82% and 96% with grammarsof perplexity 997 and 60.
The results degraded some-what on new test data, but remain highly accurate.These results were made possible by three importantfactors: (I) ample training data, (2) a powerful learningparadigm, and (3) knowledge-guided detailed models.Encouraged by these results, we will continue in thecurrent SPHINX framework, and direct our future effortsto improving each of these three areas.
We feel thatwork in each of the three directions will lead to substan-tial progress, and hope that our future work will con-tribute to the next generation of accurate, robust, andversatile speech recognition systems.AcknowledgmentsThe authors wish to thank the CMU Speech Groupfor their support and contributions.
This research waspartly sponsored by Defense Advanced ResearchProjects Agency Contract N00039-85-C-0163, andpartly by a National Science Foundation graduate fel-lowship.References1.
Bahl.
L.R., Brown, P.F., De Souza, P.V., Mer-cer, R.L., "A New Algorithm for the Estimationof Hidden Markov Model Parameters", IEEEInternational Conference on Acoustics, Speech,and Signal Processing, April 1988.2.
Lee, K.F., Large-Vocabulary Speaker-Independent Continuous Speech Recognition:The SPHINX System, PhD dissertation, Com-puter Science Department, Carnegie MellonUniversity, April 1988.3.
Lee, K.F., Automatic Speech Recognition: TheDevelopment of the SPHINX System, KluwerAcademic Publishers, Boston, 1989.4.
Gupta, V.N., Lennig, M., Mermelstein, P.,"Integration of Acoustic Information i a LargeVocabulary Word Recognizer", IEEE Inter-national Conference on Acoustics, Speech, andSignal Processing, April 1987, pp.
697-700......10.11.Schwartz, R., Chow, Y., Kimball, O., Roucos,S., Krasner, M., Makhoul, J., "Context-Dependent Modeling for Acoustic-PhoneticRecognition of Continuous Speech", IEEE In-ternational Conference on Acoustics, Speech,and Signal Processing, April 1985.Lucassen, J.M., "Discovering PhonemicBaseforms: an Information TheoreticApproach", Research Report RC 9833, IBM,February 1983.Brown, P., The Acoustic-Modeling Problem inAutomatic Speech Recognition, PhD disser-tation, Computer Science Department, CarnegieMellon University, May 1987.Hwang, M.Y., Hon, H.W., Lee, K.F.,"Between-Word Coarticulation Modeling forContinuous Speech Recognition", TechnicalReport, Carnegie Mellon University, March1989.Lee, K.F., Mahajan, S., "Corrective and Rein-forcement Learning for Speaker-IndependentContinuous Speech Recognition", TechnicalReport CMU-CS-89-100, Carnegie MellonUniversity, January 1989.Lee, K.F., Mahajan, S., "Corrective and Rein-forcement Learning for Speaker-IndependentContinuous Speech Recognition", Submitted toComputer Speech and Language.Jelinek, F., Mercer, R.L., "Interpolated Estima-tion of Markov Source Parameters from SparseData", in Pattern Recognition in Practice, E.S.Gelsema and L.N.
Kanal, ed., North-HollandPublishing Company, Amsterdam, the Nether-lands, 1980, pp.
381-397.130
