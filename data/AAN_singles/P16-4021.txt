Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics?System Demonstrations, pages 121?126,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsMy Science Tutor: Learning Science with a Conversational Virtual TutorSameer Pradhan Ron Cole Wayne WardBoulder Learning, Inc.Boulder, CO{pradhan,rcole,wward}@boulderlearning.comAbstractThis paper presents a conversational, mul-timedia, virtual science tutor for elemen-tary school students.
It is built usingstate of the art speech recognition and spo-ken language understanding technology.This virtual science tutor is unique in thatit elicits self-explanations from studentsfor various science phenomena by engag-ing them in spoken dialogs and guidedby illustrations, animations and interactivesimulations.
There is a lot of evidence thatself-explanation works well as a tutorialparadigm, Summative evaluations indicatethat students are highly engaged in the tu-toring sessions, and achieve learning out-comes equivalent to expert human tutors.Tutorials are developed through a processof recording and annotating data from ses-sions with students, and then updating tu-tor models.
It enthusiastically supportedby students and teachers.
Teachers reportthat it is feasible to integrate into their cur-riculum.1 IntroductionAccording to the 2009 National Assessment of Ed-ucational Progress (NAEP, 2009), only 34 percentof fourth-graders, 30 percent of eighth-graders,and 21 percent of twelfth-graders tested as profi-cient in science.
Thus, over two thirds of U.S. stu-dents are not proficient in science.
The vast major-ity of these students are in low-performing schoolsthat include a high percentage of disadvantagedstudents from families with low socioeconomicstatus, which often include English learners withlow English language proficiency.
Analysis of theNAEP scores in reading, math and science overthe past twenty years indicate that this situationis getting worse.
For example, the gap betweenEnglish learners and English-only students, whichis over one standard deviation lower for Englishlearners, has increased rather than decreased overthe past 20 years.
Moreover, science instruction isoften underemphasized in U.S. schools, with read-ing and math being stressed.The Program for International Student As-sessment (PISA), coordinated by the Organiza-tion for Economic Cooperation and Development(OECD), is administered every three years in 65countries across the world.
According to theirfindings in 2012, the U.S. average science scorewas not measurably different from the OECD av-erage.Our approach to address this problem is a con-versational multimedia virtual tutor for elemen-tary school science.
The operating principles forthe tutor are grounded on research from educationand cognitive science where it has been shown thateliciting self-explanations plays an important role(Chi et al, 1989; Chi et al, 1994; Chi et al, 2001;Hausmann and VanLehn, 2007a; Hausmann andVanLehn, 2007b).
Speech, language and charac-ter animation technologies play a central role be-cause the focus of the system is on engagementand spoken explanations by students during spo-ken dialogs with the virtual tutor.
Summative eval-uations indicate that students are highly engagedin the tutoring sessions, and achieve learning out-comes equivalent to expert human tutors (Ward etal., 2011; Ward et al, 2013).
Surveys of partici-pating teachers indicate that it is feasible to incor-porate the intervention into their curriculum.
Also,importantly, most student surveys indicate enthu-siastic support for the system.Tutorials are developed through an iterative pro-cess of recording, annotating and analyzing logsfrom sessions with students, and then updating tu-tor models.
This approach has been used to de-121velop over 100 tutorial dialog sessions, of about15 minutes each, in 8 areas of elementary schoolscience.My Science Tutor (MyST) provides a supple-ment to normal classroom science instruction thatimmerses students in a multimedia environmentwith a virtual science tutor that models an en-gaging and effective human tutor.
The focus ofthe program is to improve each student?s engage-ment, motivation and learning by helping themlearn to visualize, reason about and explain sci-ence during conversations with the virtual tutor.The learning principles embedded in MyST areconsistent with conclusions and recommendationsof the National Research Council Report, ?TakingScience to School: Learning and Teaching Sciencein Grades K-8?
(NRC, 2007), which emphasizesthe critical importance of scientific discourse in K-12 science education.
The report identifies the fol-lowing crucial principles of scientific proficiency:Students who are proficient in science:1.
Know, use, and interpret scientific explana-tions of the natural world;2.
Generate and evaluate scientific evidenceand explanations;3.
Understand the nature and development ofscientific knowledge; and4.
Participate productively in scientific prac-tices and discourse.The report also emphasizes that scientific in-quiry and discourse is a learned skill, so studentsneed to be involved in activities in which theylearn appropriate norms and language for produc-tive participation in scientific discourse and argu-mentation.2 The MyST ApplicationMyST provides students with the scaffolding,modeling and practice they need to learn to rea-son and talk about science.
Students learn sciencethrough natural spoken dialogs with the virtual tu-tor Marni, a 3-D computer character.
Marni asksstudents open-ended questions related to illustra-tions, silent animations or interactive simulationsdisplayed on the computer screen.Figure 1 shows the student?s screen with Marniasking questions about media displayed in a tuto-rial.
The student?s computer shows a full screenwindow that contains Marni, a display area forpresenting media and a display button that indi-cates the listening status of the system.
Marni pro-duces accurate visual speech, with head and facemovements that are synchronized with her speech.The media facilitate dialogs with Marni by help-ing students visualize the science they are dis-cussing.
The primary focus of dialogs is to elicitexplanations from students.
MyST compares thestudent?s spoken explanations to reference expla-nations for the lesson by matching the extractedsemantic roles using the Phoenix parser (Ward,1991), then presents follow-on questions and me-dia, to help the student construct a correct explana-tion of the phenomena being studied.
The virtualtutor Marni, who speaks with a recorded humanvoice, is designed to model an effective human tu-tor that the student can relate to and work with tolearn science.
MyST provides a non-threateningand supportive environment for students to expresstheir ideas.
The dialogs scaffold learning by pro-viding students with support when needed untilthey can apply new skills and knowledge indepen-dently.MyST is intended to be used as an interventionfor struggling students, with intended users beingK-12 science students.
While it should prove abenefit to all students, struggling students shouldbenefit most.
Depending on the recording condi-tions and ambient noise, as well as the character-istics of the student and session, the recognitionword error rate ranges from low 20s to mid-40s.MyST will contain tutorials for 3 topics per grade,with content aligned with NGSS.
For each topic,students engage in an average of 10 spoken dia-log sessions with the tutor, lasting approximately20 minutes each.
oThe MyST tutorial sessions arein addition to the normal classroom instruction forthe module.
Tutoring sessions can be assigned ashomework or during regular school hours, at theteacher?s discretion.
In the initial studies, tutor-ing was always done during regular school hours.Teachers specify the space in the school to be used,generally any relatively quiet room.
Students aresent to use the system a few at a time, dependingon how many computers are available (5 comput-ers per classroom were used in the efficacy study).All students are given a demo at the beginning ofthe school year and given a chance to ask ques-tions.
Teachers schedule time for students, but stu-dents log on and use the system without supervi-122Figure 1: A snapshot of the screen as seen by a student.sion, so it has minimal impact on teacher time orother human resources.
In studies thus far, surveysreport that teachers did not have problems usingthe system and it did not interfere with their otheractivities.The application will eventually be deployed us-ing a Software as a Service (SaaS) model.
It willrun on a server and students will access it throughtheir browser.
If internet service is not available orreliable, it can be run stand-alone and the data up-loaded when service is available.
Both content anduser populations will evolve and system modelsneed to incorporate dynamic adaptation in an effi-cient way.
Data from all user sessions is logged ina database and is available for continuous evalua-tion and re-training of system models.
The systemis designed to work well even if it doesn?t under-stand the user, but becomes more engaging and ef-ficient as it understands the user better.
As trainingdata grows model parameters become more accu-rate and more explicit models are trained, such asacoustic models for ELL students.
Unsupervisedtraining is combined with active learning to op-timize use of the data for tuning system models.Teachers in the initial studies did not feel that theywould have a problem implementing the system.3 Theoretical FrameworkThe theory of change, and theoretical and empir-ical support Science curricula are structured withnew concepts building on those already encoun-tered.
Struggling students fall further and fur-ther behind if they don?t understand the contentof each topic.
Research has demonstrated that hu-man tutors are effective (Bloom, 1984; Maddenand Slavin, 1989), media presentations are effec-tive (Mayer, 2001) and QtA dialog strategies areeffective (Murphy and Edwards, 2005).
A systemthat emulates a human tutor using media presen-tations to focus a student?s attention and conduct-ing a QtA-style dialog with the student should alsobe effective.
This additional time spent thinkingand talking about the science concepts covered inclass will enable students who would have fallenbehind to understand the content of the currentinvestigation so they will be prepared to partic-123ipate in and understand subsequent topics.
Stu-dent learning will increase because they are ex-cited about and engaged by interesting and infor-mative presentations that help them visualize andunderstand the science and because they will learnto engage in conversations in which they construct,reflect on and revise mental models and explana-tions about the science they are seeing and tryingto explain.
MyST dialogs are designed to providestudents with understandable multimedia scenar-ios, explanations and challenges and a support-ive social context for communication and learn-ing.
Science is introduced through scenarios thatstudents can relate to and make sense of, and pro-vide a context for introducing and using sciencevocabulary and making connections between vo-cabulary, objects, concepts and their prior knowl-edge.
Multimedia learning tools show and explainscience, and then enable students to revisit the me-dia and explain the science in their own words.Research has demonstrated that having studentsproduce explanations improves learning (Chi etal., 1989; Chi et al, 2001; King, 1994; King etal., 1988; Palincsar and Brown, 1984).
In a seriesof studies, Chi et al (1989; 2001) found that hav-ing college students generate self-explanations oftheir understanding of physics problems improvedlearning.
Self-explanation also improved learn-ing about the circulatory system by eighth gradestudents in a controlled experiment (Chi et al,1994).
Hausmann and Van Lehn (2007a; 2007b)note that: ?self-explaining has consistently beenshown to be effective in producing robust learn-ing gains in the laboratory and in the classroom.
?Experiments by Hausmann and Van Lehn (Haus-mann and VanLehn, 2007a) indicate that it is theprocess of actively producing explanations, ratherthan the accuracy of the explanations, that makesthe biggest contribution to learning.4 Semantic UnderpinningsThe patterns used in MyST to extract frames fromstudent responses are trained from annotated data.The specification of tutorial semantics begins withcreating a narrative.
A tutorial narrative is a set ofnatural language statements that express the con-cepts to be discussed in as simple a form as possi-ble.
These do not represent the questions that thesystem asks, but are the set of points that the stu-dent should express.The narrative represents what an ideal explana-tion from a student would look like.
The narra-tive statements are manually annotated to reflectthe desired semantic parses.
These parsed state-ments define the domain of the tutorial.
The initialgrammar patterns are extracted from the narrativesand have all of the roles and entities that will bediscussed, but only a few (or one) ways of express-ing them.
As the system is used, the grammar isexpanded to cover the various ways students artic-ulate their understandings of the science concepts.This is done by annotating recordings of studentresponses generated in real use.
So the life cy-cle of the natural language processing model for amodule is:1.
Create and annotate a narrative to define thedomain of the tutorial2.
Field the system to collect data from realusers3.
Sample incoming data and annotate4.
Evaluate current model and re-train5.
Repeat step 3-4 as long as the module is usedAs the system is used, it logs all transac-tions and records student speech.
When tutori-als are deployed for live use, incoming data areprocessed automatically to assess system confi-dence in the interpretation of student responses.High-confidence items are added to the trainingdatabase, and low confidence sessions are selectedfor transcription and annotation.
The system alsoprovides a text input mode that students can use tointeract with the Avatar.
Once annotated, the dataare added to the training set and system models(acoustic models, language models and extractionpatterns) are retrained.
Periodically, data are sam-pled for test sets and a learning curve is plottedfor each module.
All elements of this process areautomatic except for transcription and annotation.The semantics of each domain are constrained,but student responses can vary greatly in the waysthey choose to express concepts and terms.
It takestime, effort and data to get good coverage of stu-dent responses.
Semantic annotation for the sys-tem consists of annotating:Entities?The basic concepts talked about inthe session and the phrases that would be consid-ered synonyms.
Electricity could be expressed aselectricity, energy, power, current or electrical en-ergy.
Coverage of term synonyms from annotateddata is generally achieved fairly quickly.
Roles?How the entities in an event or concept are related124to each other.
The larger problem is to attain cov-erage of the patterns discriminating between pos-sible role assignments.
Not only is there more dis-fluency and variability here, annotating them is amore difficult task for someone not trained to doit.
Currently, it takes about one hour for a highly-trained annotator to mark up the data collected ina single 20-minute tutorial session.5 Extrinsic EvaluationAn assessment was conducted in schools to com-pare learning gains from human tutoring andMyST tutoring to business-as-usual classrooms.Learning gain was measured using standardizedassessments given to students in each conditionbefore and after each science module.
Both tu-toring conditions had significantly higher learninggains than the control group.
While the effect sizefor human tutors vs. control (d=0.68) was largerthan for MyST vs. control (d=0.53), statisticaltests supported the hypothesis of no significant dif-ference between the two.A simple two-group comparison using a Re-peated Measures ANOVA shows a statistically sig-nificant effect at F=46.4, df 1,759, p<.0001 favor-ing the treatment group.
The interaction betweengroup and module was also significant at F=9.5, p< .001.
We also used an Analysis of Covariance(ANCOVA) to compare post-test scores.
This pro-cedure adjusts for pre-test differences while com-paring the post-test average scores.
The two-groupcomparison was significant at F=7.4, df 1,768,p=.018.
We also saw a significant interaction be-tween treatment group and module with F=12.4,df 3,768.
Testing the main effects with a hierarchi-cal mixed model with students nested within class-rooms we found a significant effect for the treat-ment group at F=6.2, df 1,2l7,662, p=0.013.
Nosignificant interaction effect was found for mod-ule by group.A written survey was given to the students whoparticipated in the gas.
Measures were taken toavoid bias wherein students give overly positiveanswers to questionnaires.
The survey includedquestions that asked for ratings of student experi-ence and impressions of the program and its us-ability.
Across schools, 47% of students said theywould like to talk with Marni after every scienceinvestigation, 62% said they enjoyed working withMarni ?a lot,?
and 53% selected ?I am more ex-cited about science?
after using the program.
Only4% felt that the tutoring did not help.
Teacherswere asked for anonymous feedback to help as-sess the feasibility of an intervention using the sys-tem and their perceptions of the impact of the sys-tem.
A teacher survey was given to all participat-ing teachers directly after their students completedtutoring.
The survey asked teachers about the per-ceived impact of using Marni for student learn-ing and engagement, impacts on instruction andscheduling, willingness to potentially adopt Marnias part of classroom instruction, and overall favor-ability toward participating in the research project.Teachers answered items related to potential barri-ers in implementing new technology in the class-room.
100% of responding teachers said that theyfelt it had a positive impact on their students, theywould be interested in the program if it were avail-able and they would recommend it to other teach-ers.
93% said that they would like to participate inthe project again.
74% indicated that they wouldlike to have all of their students use the system (notjust struggling students).
Following these studies,Boulder Learning combined the best elements ofthe initial systems into the current MyST system,and with continued funding from IES (Cognitionand Student Learning Goal 3), is conducting anefficacy study.
We are currently in the 3rd year ofa 4 year study.
While data collection will continuefor another year, preliminary results support thelearning gain performance from the initial studies.6 MyST Conversations Corpus ofStudent Speech (MCCSC)We are making a cleaned up version of the corpusavailable to the research community1for free andfor commercial use at a pre-determined cost.
Thefirst release of the corpus v0.1.0 comprises 298hours of speech out of which 198 hours are man-ually transcribed.
This covers roughly 1.4 millionwords of text.
We are in the process of cleaning upabout the same amount of collected data for futuredistribution.7 Future WorkIn the near future we plan to evaluate applying astatistical labeler trained on existing corpora to thetask of Role assignment.
This approach shouldprovide increased robustness to novel input andsubstantially reduce the human annotation effortrequired to attain a given level of coverage.
The1http://corpora.boulderlearning.com/myst125Proposition Bank (PropBank) provides a corpus ofsentences annotated with domain-independent se-mantic roles (Palmer et al, 2005).
PropBank hasbeen widely used for the development of machinelearning based Semantic Role Labeling (SRL) sys-tems.
Pradhan et al (2005) used a rich set ofsyntactic and semantic features to obtain a perfor-mance with F-score in the low-80s.
It has beenan integral component of most question answer-ing systems for the past decade.
Since its firstapplication to the newswire text, PropBank hasbeen extended to cover many more predicates anddiverse genres in the DARPA OntoNotes project(Weischedel et al, 2011; Pradhan et al, 2013) andthe DARPA BOLT program.
We plan to map Prop-Bank SRL output onto MyST frames.
Domainspecific entity patterns will still need to be appliedto produce the canonical extracted form, but that isa much simpler task than role assignment and onemore suited to non-linguists.ReferencesB.
Bloom.
1984.
The 2 sigma problem: The search formethods of group instruction as effective as one-to-one tutoring.
Educational Researcher, 13(6):4?16.M.
Chi, M. Bassok, M. Lewis, P. Reimann, R. Glaser,and Alexander.
1989.
Self-explanations: How stu-dents study and use examples in learning to solveproblems.
Cognitive Science, 13(2).M.
Chi, N. De Leeuw, M. Chiu, and C. LaVancher.1994.
Eliciting self-explanations improves under-standing.
Cognitive Science, 18(3):439?477.M.
T. H. Chi, S. A. Siler, H. Jeong, T. Yamauchi, andR.
G. Hausmann.
2001.
Learning from human tu-toring.
Cognitive Science, 25(4):471?533.R.
G. M. Hausmann and K. VanLehn.
2007a.
Ex-plaining self-explaining: A contrast between contentand generation.
Artificial Intelligence in Education,pages 417?424.R.
G. M. Hausmann and K. VanLehn.
2007b.
Self-explaining in the classroom: Learning curve evi-dence.
In 29th Annual Conference of the CognitiveScience Society, Mahwah, NJ.A.
King, A. Staffieri, and A. Adelgais.
1988.
Mu-tual peer tutoring: Effects of structuring tutorial in-teraction to scaffold peer learning.
Journal of Edu-cational Psychology, 90(1):134?152.A.
King.
1994.
Guiding knowledge construction inthe classroom: Effects of teaching children how toquestion and how to explain.
American EducationalResearch Journal, 31(2).N.
A. Madden and R. E. Slavin.
1989.
Effective pro-grams for students at risk.
In R. E. Slavin, N. L.Karweit, and N. A. Madden, editors, Effective pull-out programs for students at risk.
Allyn and Bacon.R.
Mayer.
2001.
Multimedia Learning.
CambridgeUniversity Press., Cambridge, U.K.P.
K. Murphy and M. N.b Edwards.
2005.
What thestudies tell us: A meta-analysis of discussion ap-proaches.
In American Educational Research As-sociation, Montreal, Canada.National Research Council.
NRC.
2007.
Taking sci-ence to school: Learning and teaching science ingrades k-8.
In R. A. Duschl, H. A. Schweingru-ber, and A. W. Shouse, editors, Committee on Sci-ence Learning Kindergarten through Eighth Grade.Washington D.C.
The National Academies Press.A.
Palincsar and A.
Brown.
1984.
Reciprocal teach-ing of comprehension-fostering and comprehension-monitoring activities.
Cognition and Instruction,1(2).Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James Martin, and Dan Jurafsky.2005.
Support vector learning for semantic argu-ment classification.
Machine Learning, 60(1):11?39.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Hwee Tou Ng, Anders Bj?orkelund, Olga Uryupina,Yuchen Zhang, and Zhi Zhong.
2013.
Towards ro-bust linguistic analysis using OntoNotes.
In Pro-ceedings of the Seventeenth Conference on Com-putational Natural Language Learning, pages 143?152, Sofia, Bulgaria, August.W.
Ward, R. Cole, D. Bolanos, C. Buchenroth-Martin,E.
Svirsky, S. V. Vuuren, and L. Becker.
2011.
Myscience tutor: A conversational multimedia virtualtutor for elementary school science.
ACM Trans.Speech Lang.
Process., 7(4).Wayne Ward, Ron Cole, Daniel Bolanos,C.
Buchenroth-Martin, E. Svirsky, and TimWeston.
2013.
My science tutor: A conversationalmultimedia virtual tutor.
Journal of EducationalPsychology, 105(4):1115?1125.W Ward.
1991.
Understanding spontaneous speech:the phoenix system.
In Acoustics, Speech, and Sig-nal Processing, 1991.
ICASSP-91., 1991 Interna-tional Conference on, pages 365?367 vol.1, April.Ralph Weischedel, Eduard Hovy, Mitchell Marcus,Martha Palmer, Robert Belvin, Sameer Pradhan,Lance Ramshaw, and Nianwen Xue.
2011.OntoNotes: A large training corpus for enhancedprocessing.
In Joseph Olive, Caitlin Christian-son, and John McCary, editors, Handbook of Natu-ral Language Processing and Machine Translation:DARPA Global Autonomous Language Exploitation.Springer.126
