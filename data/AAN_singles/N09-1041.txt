Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 362?370,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsExploring Content Models for Multi-Document SummarizationAria HaghighiUC Berkeley, CS Divisionaria42@cs.berkeley.eduLucy VanderwendeMicrosoft ResearchLucy.Vanderwende@microsoft.comAbstractWe present an exploration of generative prob-abilistic models for multi-document summa-rization.
Beginning with a simple word fre-quency based model (Nenkova and Vander-wende, 2005), we construct a sequence ofmodels each injecting more structure into therepresentation of document set content and ex-hibiting ROUGE gains along the way.
Ourfinal model, HIERSUM, utilizes a hierarchi-cal LDA-style model (Blei et al, 2004) torepresent content specificity as a hierarchy oftopic vocabulary distributions.
At the taskof producing generic DUC-style summaries,HIERSUM yields state-of-the-art ROUGE per-formance and in pairwise user evaluationstrongly outperforms Toutanova et al (2007)?sstate-of-the-art discriminative system.
Wealso explore HIERSUM?s capacity to producemultiple ?topical summaries?
in order to facil-itate content discovery and navigation.1 IntroductionOver the past several years, there has been much in-terest in the task of multi-document summarization.In the common Document Understanding Confer-ence (DUC) formulation of the task, a system takesas input a document set as well as a short descrip-tion of desired summary focus and outputs a wordlength limited summary.1 To avoid the problem ofgenerating cogent sentences, many systems opt foran extractive approach, selecting sentences from thedocument set which best reflect its core content.21In this work, we ignore the summary focus.
Here, the wordtopic will refer to elements of our statistical model rather thansummary focus.2Note that sentence extraction does not solve the problem ofselecting and ordering summary sentences to form a coherentThere are several approaches to modeling docu-ment content: simple word frequency-based meth-ods (Luhn, 1958; Nenkova and Vanderwende,2005), graph-based approaches (Radev, 2004; Wanand Yang, 2006), as well as more linguistically mo-tivated techniques (Mckeown et al, 1999; Leskovecet al, 2005; Harabagiu et al, 2007).
Another strandof work (Barzilay and Lee, 2004; Daume?
III andMarcu, 2006; Eisenstein and Barzilay, 2008), hasexplored the use of structured probabilistic topicmodels to represent document content.
However, lit-tle has been done to directly compare the benefit ofcomplex content models to simpler surface ones forgeneric multi-document summarization.In this work we examine a series of contentmodels for multi-document summarization and ar-gue that LDA-style probabilistic topic models (Bleiet al, 2003) can offer state-of-the-art summariza-tion quality as measured by automatic metrics (seesection 5.1) and manual user evaluation (see sec-tion 5.2).
We also contend that they provide con-venient building blocks for adding more structureto a summarization model.
In particular, we uti-lize a variation of the hierarchical LDA topic model(Blei et al, 2004) to discover multiple specific ?sub-topics?
within a document set.
The resulting model,HIERSUM (see section 3.4), can produce generalsummaries as well as summaries for any of thelearned sub-topics.2 Experimental SetupThe task we will consider is extractive multi-document summarization.
In this task we assumea document collection D consisting of documentsD1, .
.
.
, Dn describing the same (or closely related)narrative (Lapata, 2003).362set of events.
Our task will be to propose a sum-mary S consisting of sentences in D totaling at mostL words.3 Here as in much extractive summariza-tion, we will view each sentence as a bag-of-wordsor more generally a bag-of-ngrams (see section 5.1).The most prevalent example of this data setting isdocument clusters found on news aggregator sites.2.1 Automated EvaluationFor model development we will utilize the DUC2006 evaluation set4 consisting of 50 document setseach with 25 documents; final evaluation will utilizethe DUC 2007 evaluation set (section 5).Automated evaluation will utilize the standardDUC evaluation metric ROUGE (Lin, 2004) whichrepresents recall over various n-grams statistics froma system-generated summary against a set of human-generated peer summaries.5 We compute ROUGEscores with and without stop words removed frompeer and proposed summaries.
In particular, weutilize R-1 (recall against unigrams), R-2 (recallagainst bigrams), and R-SU4 (recall against skip-4bigrams)6.
We present R-2 without stop words in therunning text, but full development results are pre-sented in table 1.
Official DUC scoring utilizes thejackknife procedure and assesses significance usingbootstrapping resampling (Lin, 2004).
In addition topresenting automated results, we also present a userevaluation in section 5.2.3 Summarization ModelsWe present a progression of models for multi-document summarization.
Inference details aregiven in section 4.3.1 SumBasicThe SUMBASIC algorithm, introduced in Nenkovaand Vanderwende (2005), is a simple effective pro-cedure for multi-document extractive summariza-tion.
Its design is motivated by the observation thatthe relative frequency of a non-stop word in a doc-ument set is a good predictor of a word appearingin a human summary.
In SUMBASIC, each sentence3For DUC summarization tasks, L is typically 250.4http://www-nlpir.nist.gov/projects/duc/data.html5All words from peer and proposed summaries are lower-cased and stemmed.6Bigrams formed by skipping at most two words.S is assigned a score reflecting how many high-frequency words it contains,Score(S) = ?w?S1|S|PD(w) (1)where PD(?)
initially reflects the observed unigramprobabilities obtained from the document collectionD.
A summary S is progressively built by addingthe highest scoring sentence according to (1).7In order to discourage redundancy, the wordsin the selected sentence are updated PnewD (w) ?P oldD (w)2.
Sentences are selected in this manner un-til the summary word limit has been reached.Despite its simplicity, SUMBASIC yields 5.3 R-2without stop words on DUC 2006 (see table 1).8 Bycomparison, the highest-performing ROUGE sys-tem at the DUC 2006 evaluation, SUMFOCUS, wasbuilt on top of SUMBASIC and yielded a 6.0, whichis not a statistically significant improvement (Van-derwende et al, 2007).9Intuitively, SUMBASIC is trying to select a sum-mary which has sentences where most words havehigh likelihood under the document set unigram dis-tribution.
One conceptual problem with this objec-tive is that it inherently favors repetition of frequentnon-stop words despite the ?squaring?
update.
Ide-ally, a summarization criterion should be more recalloriented, penalizing summaries which omit moder-ately frequent document set words and quickly di-minishing the reward for repeated use of word.Another more subtle shortcoming is the use of theraw empirical unigram distribution to represent con-tent significance.
For instance, there is no distinc-tion between a word which occurs many times in thesame document or the same number of times acrossseveral documents.
Intuitively, the latter word ismore indicative of significant document set content.3.2 KLSumThe KLSUM algorithm introduces a criterion for se-lecting a summary S given document collection D,S?
= minS:words(S)?LKL(PD?PS) (2)7Note that sentence order is determined by the order inwhich sentences are selected according to (1).8This result is presented as 0.053 with the official ROUGEscorer (Lin, 2004).
Results here are scaled by 1,000.9To be fair obtaining statistical significance in ROUGEscores is quite difficult.363?B ZW?C?D?tDocument SetDocumentSentenceWordFigure 1: Graphical model depiction of TOPIC-SUM model (see section 3.3).
Note that many hyper-parameter dependencies are omitted for compactness.where PS is the empirical unigram distribution ofthe candidate summary S and KL(P?Q) repre-sents the Kullback-Lieber (KL) divergence given by?w P (w) log P (w)Q(w) .10 This quantity represents thedivergence between the true distribution P (here thedocument set unigram distribution) and the approx-imating distribution Q (the summary distribution).This criterion casts summarization as finding a setof summary sentences which closely match the doc-ument set unigram distribution.
Lin et al (2006)propose a related criterion for robust summarizationevaluation, but to our knowledge this criteria hasbeen unexplored in summarization systems.
We ad-dress optimizing equation (2) as well as summarysentence ordering in section 4.KLSUM yields 6.0 R-2 without stop words, beat-ing SUMBASIC but not with statistical significance.
Itis worth noting however that KLSUM?s performancematches SUMFOCUS (Vanderwende et al, 2007), thehighest R-2 performing system at DUC 2006.3.3 TopicSumAs mentioned in section 3.2, the raw unigram dis-tribution PD(?)
may not best reflect the content ofD for the purpose of summary extraction.
Wepropose TOPICSUM, which uses a simple LDA-liketopic model (Blei et al, 2003) similar to Daume?III and Marcu (2006) to estimate a content distribu-10In order to ensure finite values of KL-divergence wesmoothe PS(?)
so that it has a small amount of mass on all doc-ument set words.System ROUGE -stop ROUGE allR-1 R-2 R-SU4 R-1 R-2 R-SU4SUMBASIC 29.6 5.3 8.6 36.1 7.1 12.3KLSUM 30.6 6.0 8.9 38.9 8.3 13.7TOPICSUM 31.7 6.3 9.1 39.2 8.4 13.6HIERSUM 30.5 6.4 9.2 40.1 8.6 14.3Table 1: ROUGE results on DUC2006 for models pre-sented in section 3.
Results in bold represent results sta-tistically significantly different from SUMBASIC in theappropriate metric.tion for summary extraction.11 We extract summarysentences as before using the KLSUM criterion (seeequation (2)), plugging in a learned content distribu-tion in place of the raw unigram distribution.First, we describe our topic model (see figure 1)which generates a collection of document sets.
Weassume a fixed vocabulary V :121.
Draw a background vocabulary distribution ?Bfrom DIRICHLET(V ,?B) shared across docu-ment collections13 representing the backgrounddistribution over vocabulary words.
This distri-bution is meant to flexibly model stop wordswhich do not contribute content.
We will referto this topic as BACKGROUND.2.
For each document set D, we draw a contentdistribution ?C from DIRICHLET(V ,?C) repre-senting the significant content of D that wewish to summarize.
We will refer to this topicas CONTENT.3.
For each document D in D, we draw adocument-specific vocabulary distribution ?Dfrom DIRICHLET(V ,?D) representing wordswhich are local to a single document, but donot appear across several documents.
We willrefer to this topic as DOCSPECIFIC.11A topic model is a probabilistic generative process that gen-erates a collection of documents using a mixture of topic vo-cabulary distributions (Steyvers and Griffiths, 2007).
Note thisusage of topic is unrelated to the summary focus given for doc-ument collections; this information is ignored by our models.12In contrast to previous models, stop words are not removedin pre-processing.13DIRICHLET(V ,?)
represents the symmetric Dirichletprior distribution over V each with a pseudo-count of ?.
Con-crete pseudo-count values will be given in section 4.364{ star: 0.21, wars: 0.15, phantom: 0.10, ... }General Content Topic?C1{ $: 0.39, million: 0.15, record: 0.8, ... }Specific Content Topic               "Financial"?C1{ toys: 0.22, spend: 0.18, sell: 0.10, ... }{ fans: 0.16, line: 0.12, film: 0.09, ... }Specific Content Topic               "Merchandise"Specific Content Topic               "Fans"?C2?C3Document Set?C0?CK?C1ZS?DDocumentSentenceWordZW?T?G.........?B(a) Content Distributions (b) HIERSUM Graphical ModelFigure 2: (a): Examples of general versus specific content distributions utilized by HIERSUM (see section 3.4).
Thegeneral content distribution ?C0 will be used throughout a document collection and represents core concepts in astory.
The specific content distributions represent topical ?sub-stories?
with vocabulary tightly clustered together butconsistently used across documents.
Quoted names of specific topics are given manually to facilitate interpretation.
(b)Graphical model depiction of the HIERSUM model (see section 3.4).
Similar to the TOPICSUM model (see section 3.3)except for adding complexity in the content hierarchy as well as sentence-specific prior distributions between generaland specific content topics (early sentences should have more general content words).
Several dependencies aremissing from this depiction; crucially, each sentence?s specific topic ZS depends on the last sentence?s ZS .4.
For each sentence S of each documentD, draw a distribution ?T over topics(CONTENT,DOCSPECIFIC, BACKGROUND)from a Dirichlet prior with pseudo-counts(1.0, 5.0, 10.0).14 For each word position inthe sentence, we draw a topic Z from ?T ,and a word W from the topic distribution Zindicates.Our intent is that ?C represents the core con-tent of a document set.
Intuitively, ?C doesnot include words which are common amongstseveral document collections (modeled with theBACKGROUND topic), or words which don?t appearacross many documents (modeled with the DOCSPE-CIFIC topic).
Also, because topics are tied togetherat the sentence level, words which frequently occurwith other content words are more likely to be con-sidered content words.We ran our topic model over the DUC 2006document collections and estimated the distribution?C(?)
for each document set.15 Then we extracted14The different pseudo-counts reflect the intuition that mostof the words in a document come from the BACKGROUND andDOCSPECIFIC topics.15While possible to obtain the predictive posterior CON-a summary using the KLSUM criterion with our es-timated ?C in place of the the raw unigram distribu-tion.
Doing so yielded 6.3 R-2 without stop words(see TOPICSUM in table 1); while not a statisticallysignificant improvement over KLSUM, it is our firstmodel which outperforms SUMBASIC with statisticalsignificance.Daume?
III and Marcu (2006) explore a topicmodel similar to ours for query-focused multi-document summarization.16 Crucially however,Daume?
III andMarcu (2006) selected sentences withthe highest expected number of CONTENT words.17We found that in our model using this extractioncriterion yielded 5.3 R-2 without stop words, sig-nificantly underperforming our TOPICSUM model.One reason for this may be that Daume?
III andMarcu (2006)?s criterion encourages selecting sen-tences which have words that are confidently gener-ated by the CONTENT distribution, but not necessar-ily sentences which contain a plurality of it?s mass.TENT distribution by analytically integrating over ?C (Blei etal., 2003), doing so gave no benefit.16Daume?
III and Marcu (2006) note their model could beused outside of query-focused summarization.17This is phrased as selecting the sentence which has thehighest posterior probability of emitting CONTENT topicwords, but this is equivalent.365(a) HIERSUM outputThe French governmentSaturday announced sev-eral emergency measuresto support the joblesspeople, including sendingan additional 500 millionfranc (84 million U.S. dol-lars) unemployment aidpackage.
The unem-ployment rate in Francedropped by 0.3 percentto stand at 12.4 percentin November, said theMinistry of EmploymentTuesday.
(b) PYTHY outputSeveral hundred peopletook part in the demon-stration here today againstthe policies of the world?smost developed nations.The 12.5 percent unem-ployment rate is haunt-ing the Christmas sea-son in France as militantsand unionists staged sev-eral protests over the pastweek against unemploy-ment.
(c) Ref outputHigh unemployment isFrance?s main economicproblem, despite recentimprovements.
A topworry of French people,it is a factor affectingFrance?s high suicide rate.Long-term unemploymentcauses social exclusionand threatens France?ssocial cohesion.
(d) Reference Unigram Coverageword Ref PYTHY HIERSUMunemployment 8 9 10france?s 6 1 4francs 4 0 1high 4 1 2economic 2 0 1french 2 1 3problem 2 0 1benefits 2 0 0social 2 0 2jobless 2 1 2Table 2: Example summarization output for systems compared in section 5.2.
(a), (b), and (c) represent the first twosentences output from PYTHY, HIERSUM, and reference summary respectively.
In (d), we present the most frequentnon-stop unigrams appearing in the reference summary and their counts in the PYTHY and HIERSUM summaries.Note that many content words in the reference summary absent from PYTHY?s proposal are present in HIERSUM?s.3.4 HIERSUMPrevious sections have treated the content of a doc-ument set as a single (perhaps learned) unigram dis-tribution.
However, as Barzilay and Lee (2004) ob-serve, the content of document collections is highlystructured, consisting of several topical themes, eachwith its own vocabulary and ordering preferences.For concreteness consider the DUC 2006 docu-ment collection describing the opening of Star Wars:Episode 1 (see figure 2(a)).While there are words which indicate the generalcontent of this document collection (e.g.
star, wars),there are several sub-stories with their own specificvocabulary.
For instance, several documents in thiscollection spend a paragraph or two talking aboutthe financial aspect of the film?s opening and use aspecific vocabulary there (e.g.
$, million, record).
Auser may be interested in general content of a docu-ment collection or, depending on his or her interests,one or more of the sub-stories.
We choose to adaptour topic modeling approach to allow modeling thisaspect of document set content.Rather than drawing a single CONTENT distribu-tion ?C for a document collection, we now drawa general content distribution ?C0 from DIRICH-LET(V ,?G) as well as specific content distribu-tions ?Ci for i = 1, .
.
.
,K each from DIRICH-LET(V ,?S).18 Our intent is that ?C0 represents the18We choose K=3 in our experiments, but one could flexiblygeneral content of the document collection and each?Ci represents specific sub-stories.As with TOPICSUM, each sentencehas a distribution ?T over topics(BACKGROUND,DOCSPECIFIC, CONTENT).
WhenBACKGROUND or DOCSPECIFIC topics are chosen,the model works exactly as in TOPICSUM.
Howeverwhen the CONTENT topic is drawn, we must decidewhether to emit a general content word (from ?C0)or from one of the specific content distributions(from one of ?Ci for i = 1, .
.
.
,K).
The generativestory of TOPICSUM is altered as follows in this case:?
General or Specific?
We must first decidewhether to use a general or specific contentword.
Each sentence draws a binomial distribu-tion ?G determining whether a CONTENT wordin the sentence will be drawn from the generalor a specific topic distribution.
Reflecting theintuition that the earlier sentences in a docu-ment19 describe the general content of a story,we bias ?G to be drawn from BETA(5,2), pre-ferring general content words, and every latersentence from BETA(1,2).20?
What Specific Topic?
If ?G decides we arechoose K as Blei et al (2004) does.19In our experiments, the first 5 sentences.20BETA(a,b) represents the beta prior over binomial randomvariables with a and b being pseudo-counts for the first and sec-ond outcomes respectively.366emitting a topic specific content word, we mustdecide which of ?C1 , .
.
.
, ?CK to use.
In or-der to ensure tight lexical cohesion amongst thespecific topics, we assume that each sentencedraws a single specific topic ZS used for everyspecific content word in that sentence.
Reflect-ing intuition that adjacent sentences are likelyto share specific content vocabulary, we uti-lize a ?sticky?
HMM as in Barzilay and Lee(2004) over the each sentences?
ZS .
Con-cretely, ZS for the first sentence in a docu-ment is drawn uniformly from 1, .
.
.
,K, andeach subsequent sentence?s ZS will be identi-cal to the previous sentence with probability ?,and with probability 1 ?
?
we select a succes-sor topic from a learned transition distributionamongst 1, .
.
.
,K.21Our intent is that the general content distribution?C0 now prefers words which not only appear inmany documents, but also words which appear con-sistently throughout a document rather than beingconcentrated in a small number of sentences.
Eachspecific content distribution ?Ci is meant to modeltopics which are used in several documents but tendto be used in concentrated locations.HIERSUM can be used to extract several kindsof summaries.
It can extract a general summaryby plugging ?C0 into the KLSUM criterion.
It canalso produce topical summaries for the learned spe-cific topics by extracting a summary over each ?Cidistribution; this might be appropriate for a userwho wants to know more about a particular sub-story.
While we found the general content distribu-tion (from ?C0) to produce the best single summary,we experimented with utilizing topical summariesfor other summarization tasks (see section 6.1).
Theresulting system, HIERSUM yielded 6.4 R-2 withoutstop words.
While not a statistically significant im-provement in ROUGE over TOPICSUM, we found thesummaries to be noticeably improved.4 Inference and Model DetailsSince globally optimizing the KLSUM criterion inequation (equation (2)) is exponential in the totalnumber of sentences in a document collection, we21We choose ?
= 0.75 in our experiments.opted instead for a simple approximation where sen-tences are greedily added to a summary so long asthey decrease KL-divergence.
We attempted morecomplex inference procedures such as McDonald(2007), but these attempts only yielded negligibleperformance gains.
All summary sentence order-ing was determined as follows: each sentence in theproposed summary was assigned a number in [0, 1]reflecting its relative sentence position in its sourcedocument, and sorted by this quantity.All topic models utilize Gibbs sampling for in-ference (Griffiths, 2002; Blei et al, 2004).
In gen-eral for concentration parameters, the more specifica distribution is meant to be, the smaller its con-centration parameter.
Accordingly for TOPICSUM,?G = ?D = 1 and ?C = 0.1.
For HIERSUM weused ?G = 0.1 and ?S = 0.01.
These parameterswere minimally tuned (without reference to ROUGEresults) in order to ensure that all topic distributionbehaved as intended.5 Formal ExperimentsWe present formal experiments on the DUC 2007data main summarization task, proposing a generalsummary of at most 250 words22 which will be eval-uated automatically and manually in order to simu-late as much as possible the DUC evaluation envi-ronment.23 DUC 2007 consists of 45 document sets,each consisting of 25 documents and 4 human refer-ence summaries.We primarily evaluate the HIERSUM model, ex-tracting a single summary from the general con-tent distribution using the KLSUM criterion (see sec-tion 3.2).
Although the differences in ROUGE be-tween HIERSUM and TOPICSUM were minimal, wefound HIERSUM summary quality to be stronger.In order to provide a reference for ROUGEand manual evaluation results, we compare againstPYTHY, a state-of-the-art supervised sentence ex-traction summarization system.
PYTHY uses human-generated summaries in order to train a sentenceranking system which discriminatively maximizes22Since the ROUGE evaluation metric is recall-oriented, it isalways advantageous - with respect to ROUGE - to use all 250words.23Although the DUC 2007 main summarization task providesan indication of user intent through topic focus queries, we ig-nore this aspect of the data.367System ROUGE w/o stop ROUGE w/ stopR-1 R-2 R-SU4 R-1 R-2 R-SU4HIERSUM unigram 34.6 7.3 10.4 43.1 9.7 15.3HIERSUM bigram 33.8 9.3 11.6 42.4 11.8 16.7PYTHY w/o simp 34.7 8.7 11.8 42.7 11.4 16.5PYTHY w/ simp 35.7 8.9 12.1 42.6 11.9 16.8Table 3: Formal ROUGE experiment results on DUC 2007 document set collection (see section 5.1).
While HIER-SUM unigram underperforms both PYTHY systems in statistical significance (for R-2 and RU-4 with and without stopwords), HIERSUM bigram?s performance is comparable and statistically no worse.ROUGE scores.
PYTHY uses several features torank sentences including several variations of theSUMBASIC score (see section 3.1).
At DUC 2007,PYTHY was ranked first overall in automatic ROUGEevaluation and fifth in manual content judgments.As PYTHY utilizes a sentence simplification com-ponent, which we do not, we also compare againstPYTHY without sentence simplification.5.1 ROUGE EvaluationROUGE results comparing variants of HIERSUM andPYTHY are given in table 3.
The HIERSUM systemas described in section 3.4 yields 7.3 R-2 withoutstop words, falling significantly short of the 8.7 thatPYTHY without simplification yields.
Note that R-2is a measure of bigram recall and HIERSUM does notrepresent bigrams whereas PYTHY includes severalbigram and higher order n-gram statistics.In order to put HIERSUM and PYTHY on equal-footing with respect to R-2, we instead ran HIER-SUM with each sentence consisting of a bag of bi-grams instead of unigrams.24 All the details of themodel remain the same.
Once a general contentdistribution over bigrams has been determined byhierarchical topic modeling, the KLSUM criterionis used as before to extract a summary.
This sys-tem, labeled HIERSUM bigram in table 3, yields 9.3R-2 without stop words, significantly outperform-ing HIERSUM unigram.
This model outperformsPYTHY with and without sentence simplification, butnot with statistical significance.
We conclude thatboth PYTHY variants and HIERSUM bigram are com-parable with respect to ROUGE performance.24Note that by doing topic modeling in this way over bi-grams, our model becomes degenerate as it can generate incon-sistent bags of bigrams.
Future work may look at topic modelsover n-grams as suggested by Wang et al (2007).Question PYTHY HIERSUMOverall 20 49Non-Redundancy 21 48Coherence 15 54Focus 28 41Table 4: Results of manual user evaluation (see sec-tion 5.2).
15 participants expressed 69 pairwise prefer-ences between HIERSUM and PYTHY.
For all attributes,HIERSUM outperforms PYTHY; all results are statisti-cally significant as determined by pairwise t-test.5.2 Manual EvaluationIn order to obtain a more accurate measure of sum-mary quality, we performed a simple user study.
Foreach document set in the DUC 2007 collection, auser was given a reference summary, a PYTHY sum-mary, and a HIERSUM summary;25 note that the orig-inal documents in the set were not provided to theuser, only a reference summary.
For this experimentwe use the bigram variant of HIERSUM and compareit to PYTHY without simplification so both systemshave the same set of possible output summaries.The reference summary for each document setwas selected according to highest R-2 without stopwords against the remaining peer summaries.
Userswere presented with 4 questions drawn from theDUC manual evaluation guidelines:26 (1) Overallquality: Which summary was better overall?
(2)Non-Redundancy: Which summary was less redun-dant?
(3) Coherence: Which summary was morecoherent?
(4) Focus: Which summary was more25The system identifier was of course not visible to the user.The order of automatic summaries was determined randomly.26http://www-nlpir.nist.gov/projects/duc/duc2007/quality-questions.txt368Figure 3: Using HIERSUM to organize content of document set into topics (see section 6.1).
The sidebar gives keyphrases salient in each of the specific content topics in HIERSUM (see section 3.4).
When a topic is clicked in the rightsidebar, the main frame displays an extractive ?topical summary?
with links into document set articles.
Ideally, a usercould use this interface to quickly find content in a document collection that matches their interest.focused in its content, not conveying irrelevant de-tails?
The study had 16 users and each was askedto compare five summary pairs, although some didfewer.
A total of 69 preferences were solicited.
Doc-ument collections presented to users were randomlyselected from those evaluated fewest.As seen in table 5.2, HIERSUM outperformsPYTHY under all questions.
All results are statis-tically significant as judged by a simple pairwiset-test with 95% confidence.
It is safe to concludethat users in this study strongly preferred the HIER-SUM summaries over the PYTHY summaries.6 DiscussionWhile it is difficult to qualitatively compare onesummarization system over another, we can broadlycharacterize HIERSUM summaries compared to someof the other systems discussed.
For example out-put from HIERSUM and PYTHY see table 2.
On thewhole, HIERSUM summaries appear to be signifi-cantly less redundant than PYTHY and moderatelyless redundant than SUMBASIC.
The reason for thismight be that PYTHY is discriminatively trained tomaximize ROUGE which does not directly penalizeredundancy.
Another tendency is for HIERSUM to se-lect longer sentences typically chosen from an earlysentence in a document.
As discussed in section 3.4,HIERSUM is biased to consider early sentences indocuments have a higher proportion of general con-tent words and so this tendency is to be expected.6.1 Content NavigationA common concern in multi-document summariza-tion is that without any indication of user interest orintent providing a single satisfactory summary to auser may not be feasible.
While many variants ofthe general summarization task have been proposedwhich utilize such information (Vanderwende et al,2007; Nastase, 2008), this presupposes that a userknows enough of the content of a document collec-tion in order to propose a query.As Leuski et al (2003) and Branavan et al (2007)suggest, a document summarization system shouldfacilitate content discovery and yield summaries rel-evant to a user?s interests.
We may use HIERSUM inorder to facilitate content discovery via presentinga user with salient words or phrases from the spe-cific content topics parametrized by ?C1 , .
.
.
, ?CK(for an example see figure 3).
While these topics arenot adaptive to user interest, they typically reflectlexically coherent vocabularies.ConclusionIn this paper we have presented an exploration ofcontent models for multi-document summarizationand demonstrated that the use of structured topicmodels can benefit summarization quality as mea-sured by automatic and manual metrics.Acknowledgements The authors would like tothank Bob Moore, Chris Brockett, Chris Quirk, andKristina Toutanova for their useful discussions aswell as the reviewers for their helpful comments.369ReferencesRegina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In HLT-NAACL.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
JMLR.David M. Blei, Thomas L. Griffiths, Michael I. Jordan,and Joshua B. Tenenbaum.
2004.
Hierarchical topicmodels and the nested chinese restaurant process.
InNIPS.S.R.K.
Branavan, Pawan Deshpande, and Regina Barzi-lay.
2007.
Generating a table-of-contents.
In ACL.Hal Daume?
III and Daniel Marcu.
2006.
Bayesian query-focused summarization.
In Proceedings of the Confer-ence of the Association for Computational Linguistics(ACL).Jacob Eisenstein and Regina Barzilay.
2008.
Bayesianunsupervised topic segmentation.
In EMNLP-SIGDAT.Thomas Griffiths.
2002.
Gibbs sampling in the genera-tive model of latent dirichlet alocation.Sanda Harabagiu, Andrew Hickl, and Finley Laca-tusu.
2007.
Satisfying information needs with multi-document summaries.
Inf.
Process.
Manage., 43(6).Mirella Lapata.
2003.
Probabilistic text structuring: Ex-periments with sentence ordering.
In ACL.Jurij Leskovec, Natasa Milic-frayling, and Marko Gro-belnik.
2005.
Impact of linguistic analysis on the se-mantic graph coverage and learning of document ex-tracts.
In In AAAI 05.Anton Leuski, Chin-Yew Lin, and Eduard Hovy.
2003.ineats: Interactive multi-document summarization.
InACL.Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-YunNie.
2006.
An information-theoretic approach to au-tomatic evaluation of summaries.
In HLT-NAACL.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Proc.
ACL workshop onText Summarization Branches Out.H.P.
Luhn.
1958.
The automatic creation of literatureabstracts.
IBM Journal.Ryan McDonald.
2007.
A study of global inference al-gorithms in multi-document summarization.
In ECIR.Kathleen R. Mckeown, Judith L. Klavans, VasileiosHatzivassiloglou, Regina Barzilay, and Eleazar Eskin.1999.
Towards multidocument summarization by re-formulation: Progress and prospects.
In In Proceed-ings of AAAI-99.Vivi Nastase.
2008.
Topic-driven multi-document sum-marization with encyclopedic knowledge and spread-ing activation.
In Proceedings of the 2008 Conferenceon Empirical Methods in Natural Language Process-ing.A.
Nenkova and L. Vanderwende.
2005.
The impact offrequency on summarization.
Technical report, Mi-crosoft Research.Dragomir R. Radev.
2004.
Lexrank: graph-based cen-trality as salience in text summarization.
Journal ofArtificial Intelligence Research (JAIR.M.
Steyvers and T. Griffiths, 2007.
Probabilistic TopicModels.Kristina Toutanova, Chris Brockett, Michael Gamon Ja-gadeesh Jagarlamudi, Hisami Suzuki, and Lucy Van-derwende.
2007.
The pythy summarization system:Microsoft research at duc 2007.
In DUC.Lucy Vanderwende, Hisami Suzuki, Chris Brockett, andAni Nenkova.
2007.
Beyond sumbasic: Task-focusedsummarization with sentence simplification and lexi-cal expansion.
volume 43.Xiaojun Wan and Jianwu Yang.
2006.
Improved affinitygraph based multi-document summarization.
In HLT-NAACL.Xuerui Wang, Andrew McCallum, and Xing Wei.
2007.Topical n-grams: Phrase and topic discovery, with anapplication to information retrieval.
In ICDM.370
