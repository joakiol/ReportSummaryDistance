Automated Tutoring Dialogues for Training in ShipboardDamage ControlJohn Fry, Matt Ginzton, Stanley Peters, Brady Clark & Heather Pon-BarryStanford UniversityCenter for the Study of Language InformationStanford CA 94305-4115 USA{fry,mginzton,peters,bzack,ponbarry}@csli.stanford.eduAbstractThis paper describes an applicationof state-of-the-art spoken languagetechnology (OAA/Gemini/Nuance)to a new problem domain: engagingstudents in automated tutorial dia-logues in order to evaluate and im-prove their performance in a train-ing simulator.1 IntroductionShipboard damage control refers to the task ofcontaining the effects of fire, explosions, hullbreaches, flooding, and other critical eventsthat can occur aboard Naval vessels.
Thehigh-stakes, high-stress nature of this task, to-gether with limited opportunities for real-lifetraining, make damage control an ideal targetfor AI-enabled educational technologies liketraining simulators and tutoring systems.This paper describes the spoken dialoguesystem we developed for automated critiquingof student performance on a damage controltraining simulator.
The simulator is DC-Train (Bulitko and Wilkins, 1999), an im-mersive, multimedia training environment fordamage control.
DC-Train?s training sce-narios simulate a mixture of physical phenom-ena (e.g., fire, flooding) and personnel issues(e.g., casualties, communications, standard-ized procedures).
Our current tutoring sys-tem is restricted fire damage scenarios only,and in particular to the twelve fire scenar-ios available in DC-Train version 2.5, butin future versions we plan to support post-session critiques for all of the damage phe-nomena that will be modeled by DC-Train4.0: fire, flooding, missile damage, and wallor firemain ruptures.2 Previous WorkEliciting self-explanation from a student hasbeen shown to be a highly effective tutoringmethod (Chi et al, 1994).
For this reason,a number of automated tutoring systems cur-rently use NLP techniques to engage studentsin reflective dialogues.
Three notable exam-ples are the medical Circsim tutor (Zhou etal., 1999); the Basic Electricity and Electron-ics (BE&E) tutor (Rose?
et al, 1999); andthe computer literacy AutoTutor (Wiemer-Hastings et al, 1999).Our system shares several features withthese three tutoring systems:A knowledge base Our system encodesall domain knowledge relevant to supportingintelligent tutoring feedback into a structurecalled an Expert Session Summary (Section4).
These expert summaries encode causalrelationships between events on the ship aswell as the proper and improper responses toshipboard crises.Tutoring strategies In our system, as inthose above, the flow of dialogue is controlledby (essentially) a finite-state transition net-work (Fig.
1).An interpretation component In oursystem, the student?s speech is recognized andparsed into logical forms (Section 3).
A dia-logue manager inspects the current dialogueinformation state to determine how best toincorporate each new utterance into the dia-logue (Lemon et al, 2001).Promptstudent reviewof actionsCorrectstudent?sreport Prompt forreflection onSTARTENDcontinue""OK, let?sevent N...Summaryof damagemain pointsReviewperformancestudent?sEvaluatereflectionsCorrectstudent?s"You handledthis one well"event 1of damageSummaryBriefsummary ofsessionerrorsFigure 1: Post-session dialogue move graph (simplified)However, an important difference is thatthe three systems above are entirely text-based, whereas ours is a spoken dialogue sys-tem.
Our speech interface offers greater natu-ralness than keyboard-based input.
In this re-spect, our system is similar to cove (Roberts,2000), a training simulator for conning Navyships that uses speech to interact with thestudent.
But whereas cove uses short conver-sational exchanges to coach the student dur-ing the simulation, our system engages in ex-tended tutorial dialogues after the simulationhas ended.
Besides being more natural, spo-ken language systems are also better suited tomultimodal interactions (viz., one can pointand click while talking but not while typing).An additional significant difference betweenour system and a number of other automatedtutoring systems is our use of ?deep?
process-ing techniques.
While other systems utilize?shallow?
statistical approaches like Latent Se-mantic Analysis (e.g.
AutoTutor), our systemutilizes Gemini, a symbolic grammar.
Thisapproach enables us to provide precise andreliable meaning representations.3 ImplementationTo facilitate the implementation of multi-modal, mixed-initiative tutoring interactions,we decided to implement our system withinthe Open Agent Architecture (OAA) (Martinet al, 1999).
OAA is a framework for coor-dinating multiple asynchronous communicat-ing processes.
The core of OAA is a ?facilita-tor?
which manages message passing betweena number of software agents that specializein certain tasks (e.g., speech recognition ordatabase queries).
Our system uses OAA tocoordinate the following five agents:1.
The Gemini NLP system (Dowding etal., 1993).
Gemini uses a single unifi-cation grammar both for parsing stringsof words into logical forms (LFs) and forgenerating sentences from LF inputs.2.
A Nuance speech recognition server,which converts spoken utterances tostrings of words.
The Nuance server re-lies on a language model, which is com-piled directly from the Gemini grammar,ensuring that every recognized utteranceis assigned an LF.3.
The Festival text-to-speech system,which ?speaks?
word strings generated byGemini.4.
A Dialogue Manager which coordi-nates inputs from the user, interprets theuser?s dialogue moves, updates the dia-logue context, and delivers speech andgraphical outputs to the user.5.
A Critique Planner, described belowin Section 4.Agents 1-3 are reusable, ?off-the-shelf?
dia-logue system components (apart from theGemini grammar, which must be modified foreach application).
We implemented agents 4and 5 in Java specifically for this application.Variants of this OAA/Gemini/Nuance ar-chitecture have been deployed successfully inother dialogue systems, notably SRI?s Com-mandTalk (Stent et al, 1999) and an un-Figure 2: Screen shot of post-session tutorial dialogue systemmanned helicopter interface developed in ourlaboratory (Lemon et al, 2001).4 Planning the dialogueEach student session with DC-Train pro-duces a session transcript, i.e.
a time-stampedrecord of every event (both computer- andstudent-initiated) that occurred during thesimulation.
These transcripts serve as theinput to our post-session Critique Planner(CP).The CP plans a post-session tutorial di-alogue in two steps.
In the first step, anExpert Session Summary (ESS) is cre-ated from the session transcript.
The ESSis a tree whose parent nodes represent dam-age events and whose leaves represent actionstaken in response to those damage events.Each student-initiated action in the ESS isevaluated as to its timeliness and conformanceto damage control doctrine.
Actions that thestudent should have taken but did not are alsoinserted into the ESS and flagged as such.Each action node in the ESS therefore fallsinto one of three classes: (i) correct actions;(ii) errors of commission (e.g., the studentsets fire containment boundaries incorrectly);and (iii) errors of omission (e.g., the studentfails to secure permission from the captain be-fore flooding certain compartments).Our current tutoring system covers scenar-ios generated by DC-Train 2.5, which coversfire scenarios only.
Future versions will usescenarios generated by DC-Train 4.0, whichcovers damage control scenarios involving fire,smoke, flooding, pipe and hull ruptures, andequipment deactivation.
Our current tutor-ing system is based on an ESS graph that isgenerated by an expert model that consistsof an ad-hoc set of firefighting rules.
Futureversions will be based on an ESS graph thatis generated by an successor to the Minerva-DCA expert model (Bulitko and Wilkins,1999), an extended Petri Net envisionment-based reasoning system.
The new expertmodel is designed to produce an ESS graphduring the course of problem solving that con-tains nodes for all successful and unsuccessfulplan and goal achievement events, along withan explanation structure for each graph node.The second step in planning the post-session tutorial dialogue is to produce a di-alogue move graph (Fig.
1).
This is a di-rected graph that encodes all possible configu-rations of dialogue structure and content thatcan be handled by the system.Generating an appropriate dialogue movegraph from an ESS requires pedagogicalknowledge, and in particular a tutoring strat-egy.
The tutoring strategy we adopted isbased on our analysis of videotapes of fifteenactual DC-Train post-session critiques con-ducted by instructors at the Navy?s SurfaceWarfare Officer?s School in Newport, RI.
Thestrategy we observed in these critiques, andimplemented in our system, can be outlinedas follows:1.
Summarize the results of the simulation(e.g., the final condition of the ship).2.
For each major damage event in the ESS:(a) Ask the student to review his ac-tions, correcting his recollections asnecessary.
(b) Evaluate the correctness of each stu-dent action.
(c) If the student committed errors,ask him how these could have beenavoided, and evaluate the correct-ness of his responses.3.
Finally, review each type of error thatarose in step (2c).A screen shot of the tutoring system inaction is shown in Fig.
2.
As soon as aDC-Train simulation ends, the dialogue sys-tem starts up and the dialogue manager be-gins traversing the dialogue move graph.
Asthe dialogue unfolds, a graphical representa-tion of the ESS is revealed to the student inpiecemeal fashion as depicted in the top rightframe of Fig.
2.AcknowledgmentsThis work is supported by the Depart-ment of the Navy under research grantN000140010660, a multidisciplinary univer-sity research initiative on natural language in-teraction with intelligent tutoring systems.ReferencesV V. Bulitko and D C. Wilkins.
1999.
Automatedinstructor assistant for ship damage control.
InProceedings of AAAI-99, Orlando, FL, July.M.
T. H. Chi, N. de Leeuw, M. Chiu, and C.LaVancher.
1994.
Eliciting self-explanationsimproves understanding.
Cognitive Science,18(3):439?477.J.
Dowding, J. Gawron, D. Appelt, J.
Bear, L.Cherny, R. C. Moore, and D. Moran.
1993.Gemini: A natural language system for spoken-language understanding.
In Proceedings of theARPA Workshop on Human Language Technol-ogy.O.
Lemon, A. Bracy, A. Gruenstein, and S. Pe-ters.
2001.
A multi-modal dialogue system forhuman-robot conversation.
In Proceedings ofNAACL 2001.D.
Martin, A. Cheyer, and D. Moran.
1999.The Open Agent Architecture: a framework forbuilding distributed software systems.
AppliedArtificial Intelligence, 13(1-2).B.
Roberts.
2000.
Coaching driving skills ina shiphandling trainer.
In Proceedings of theAAAI Fall Symposium on Building DialogueSystems for Tutorial Applications.C.
P.
Rose?, B.
Di Eugenio, and J. D. Moore.
1999.A dialogue based tutoring system for basic elec-tricity and electronics.
In S. P. Lajoie andM.
Vivet, editors, Artificial Intelligence in Ed-ucation (Proceedings of AIED?99), pages 759?761.
IOS Press, Amsterdam.A.
Stent, J. Dowding, J. Gawron, E. O. Bratt, andR.
C. Moore.
1999.
The CommandTalk spokendialogue system.
In Proceedings of ACL ?99,pages 183?190, College Park, MD.P.
Wiemer-Hastings, K. Wiemer-Hastings, andA.
Graesser.
1999.
Improving an intelli-gent tutor?s comprehension of students with la-tent semantic analysis.
In S. P. Lajoie andM.
Vivet, editors, Artificial Intelligence in Ed-ucation (Proceedings of AIED?99), pages 535?542.
IOS Press, Amsterdam.Y.
Zhou, R. Freedman, M. Glass, J.
A. Michael,A.
A. Rovick, and M. W. Evens.
1999.
Deliver-ing hints in a dialogue-based intelligent tutoringsystem.
In Proceedings of AAAI-99, Orlando,FL, July.
