Text Generation from KeywordsKiyotaka Uchimoto?
Satoshi Sekine?
Hitoshi Isahara?
?Communications Research Laboratory2-2-2, Hikari-dai, Seika-cho, Soraku-gun,Kyoto, 619-0289, Japan{uchimoto,isahara}@crl.go.jp?New York University715 Broadway, 7th floorNew York, NY 10003, USAsekine@cs.nyu.eduAbstractWe describe a method for generating sentencesfrom ?keywords?
or ?headwords?.
This methodconsists of two main parts, candidate-text con-struction and evaluation.
The construction partgenerates text sentences in the form of depen-dency trees by using complementary informa-tion to replace information that is missing be-cause of a ?knowledge gap?
and other missingfunction words to generate natural text sen-tences based on a particular monolingual cor-pus.
The evaluation part consists of a modelfor generating an appropriate text when givenkeywords.
This model considers not only wordn-gram information, but also dependency infor-mation between words.
Furthermore, it consid-ers both string information and morphologicalinformation.1 IntroductionText generation is an important technique usedfor applications like machine translation, sum-marization, and human/computer dialogue.
Inrecent years, many corpora have become avail-able, and have been used to generate naturalsurface sentences.
For example, corpora havebeen used to generate sentences for languagemodel estimation in statistical machine trans-lation.
In such translation, given a source lan-guage text, S, the translated text, T , in thetarget language that maximizes the probabil-ity P (T |S) is selected as the most appropri-ate translation, Tbest, which is represented as(Brown et al, 1990)Tbest = argmaxTP (T |S)= argmaxT (P (S|T ) ?
P (T )) .
(1)In this equation, P (S|T ) represents the modelused to replace words or phrases in a source lan-guage with those in the target language.
It iscalled a translation model.
P (T ) represents alanguage model that is used to reorder trans-lated words or phrases into a natural order inthe target language.
The input of the languagemodel is a ?bag of words,?
and the goal of themodel is basically to reorder the words.
At thispoint, there is an assumption that natural sen-tences can be generated by merely reorderingthe words given by a translation model.
To givesuch a complete set of words, however, a trans-lation model needs a large number of bilingualcorpora.
If we could automatically complementthe words needed to generate natural sentences,we would not have to collect the large numberof bilingual corpora required by a translationmodel.
In this paper, we assume that the role ofthe translation model is not to give a completeset of words that can be used to generate nat-ural sentences, but to give a set of headwordsor center words that a speaker might want toexpress, and describe a model that can providethe complementary information needed to gen-erate natural sentences by using a target lan-guage corpus when given a set of headwords.If we denote a set of headwords in a targetlanguage as K, we can express Eq.
(1) asP (T |S) = P (K|S) ?
P (T |K).
(2)P (K|S) in this equation represents a modelthat gives a set of headwords in the target lan-guage when given a source-language text sen-tence.
P (T |K) represents a model that gener-ates text sentence T when given a set of head-words, K. We call the model represented byP (T |K) a text-generation model.
In this paper,we describe a text-generation model and a gen-eration system that uses the model.
Given a setof headwords or keywords, our system outputsthe text sentence that maximizes P (T |K) as anappropriate text sentence, Tbest:Tbest = argmaxTP (T |K)= argmaxT (P (K|T )?
P (T )) .
(3)In this equation, we call the model representedby P (K|T ) a keyword-production model.
Thisequation is equal to Eq.
(1) when a source-text sentence is replaced with a set of key-words.
Therefore, this model can be regardedas a model that translates keywords into textsentences.
The model represented by P (T ) inEq.
(3) is a language model used in statisticalmachine translation.
The n-gram model is themost popular one used as a language model.We assume that there is one extremely proba-ble ordered set of morphemes and dependenciesbetween words that produce keywords, and weexpress P (K|T ) asP (K|T ) ?
P (K,M,D|T )= P (K|M,D, T ) ?
P (D|M,T )?
P (M |T ).
(4)In this equation, M denotes an ordered set ofmorphemes and D denotes an ordered set of de-pendencies in a sentence.
P (K|M,D,T ) rep-resents a keyword-production model.
To es-timate the models represented by P (D|M,T )and P (M |T ), we use a dependency model anda morpheme model, respectively, for the depen-dency analysis and morphological analysis.Statistical machine translation and example-based machine translation require numeroushigh-quality bilingual corpora.
Interlingual ma-chine translation and transfer-based machinetranslation require a parser with high precision.Therefore, these approaches to translation arenot practical if we do not have enough bilingualcorpora or a good parser.
This is especially so ifthe source text-sentences are incomplete or haveerrors like those often found in OCR and speech-recognition output.
In these cases, however, ifwe translate headwords into words in the targetlanguage and generate sentences from the trans-lated words by using our method, we should beable to generate natural sentences from whichwe can grasp the meaning of the source-text sen-tences.The text-generation model represented byP (T |K) in Eq.
(2) can be applied to varioustasks besides machine translation.?
Sentence-generation support systemfor people with aphasia: About 300,000people are reported to suffer from aphasiain Japan, and 40% of them can select onlya few words to describe a picture.
If candi-date sentences can be generated from thesefew words, it would help these people com-municate with their families and friends.?
Support system for second languagewriting: Beginners writing in second lan-guage usually fined it easy to produce cen-ter words or headwords, but often have dif-ficulty generating complete sentences.
Ifseveral possible sentences could be gener-ated from those words, it would help begin-ners communicate with foreigners or studysecond-language writing.These are just two examples.
We believe thatthere are many other possible applications.2 Overview of the Text-GenerationSystemIn this section, we give an overview of our sys-tem for generating text sentences from givenkeywords.
As shown in Fig.
1, this system con-sists of three parts: generation-rule acquisition,candidate-text sentence construction, and eval-uation.Figure 1: Overview of the text-generation sys-tem.Given keywords, text sentences are generatedas follows.1.
During generation-rule acquisition, genera-tion rules for each keyword are automati-cally acquired.2.
Candidate-text sentences are constructedduring candidate-text construction by ap-plying the rules acquired in the firststep.
Each candidate-text sentence is rep-resented by a graph or dependency tree.3.
Candidate-text sentences are ranked ac-cording to their scores assigned during eval-uation.
The scores are calculated as aprobability estimated by using a keyword-production model and a language modelthat are trained with a corpus.4.
The candidate-text sentence that maxi-mizes the score or the candidate-text sen-tences whose scores are over a thresholdare selected as output.
The system canalso output candidate-text sentences thatare ranked within the top N sentences.In this paper, we assume that the target lan-guage is Japanese.
We define a keyword as theheadword of a bunsetsu.
A bunsetsu is a phrasalunit that usually consists of several content andfunction words.
We define the headword of abunsetsu as the rightmost content word in thebunsetsu, and we define a content word as aword whose part-of-speech is a verb, adjective,noun, demonstrative, adverb, conjunction, at-tribute, interjection, or undefined word.
Wedefine the other words as function words.
Wedefine formal nouns and auxiliary verbs ?SURU(do)?
and ?NARU (become)?
as function words,except when there are no other content wordsin the same bunsetsu.
Part-of-speech categoriesfollow those in the Kyoto University text corpus(Version 3.0) (Kurohashi and Nagao, 1997), atagged corpus of the Mainichi newspaper.Figure 2: Example of text generated from key-words.For example, given the set of keywords?kanojo (she),?
?ie (house),?
and ?iku (go),?
asshown in Fig.
2, our system retrieves sentencesincluding each word, and extracts each bunsetsuthat includes each word as a headword of thebunsetsu.
If there is no tagged corpus suchas the Kyoto University text corpus, each bun-setsu can be extracted by using a morphological-analysis system and a dependency-analysis sys-tem such as JUMAN (Kurohashi and Nagao,1999) and KNP (Kurohashi, 1998).
Our systemthen acquires generation rules as follows.?
?kanojo (she)?kanojo (she) no (of)??
?kanojo (she)?kanojo (she) ga??
?ie (house)?ie (house) ni (to)??
?iku (go)?iku (go)??
?iku (go)?itta (went)?The system next generates candidate bunsetsusfor each keyword and candidate-text sentencesin the form of dependency trees, such as ?Can-didate 1?
and ?Candidate 2?
in Fig.
2, withthe assumption that there are dependencies be-tween keywords.
Finally, the candidate-textsentences are ranked by their scores, calculatedby a text-generation model, and transformedinto surface sentences.In this paper, we focus on the keyword-production model represented by Eq.
(4) andassume that our system outputs sentences in theform of dependency trees.3 Candidate-Text ConstructionWe automatically acquire generation rules froma monolingual target corpus at the time of gen-erating candidate-text sentences.
Generationrules are restricted to those that generate bun-setsus, and the generated bunsetsus must in-clude each input keyword as a headword in thebunsetsu.
We then generate candidate-text sen-tences in the form of dependency trees by simplycombining the bunsetsus generated by the rules.The simple combination of generated bunsetsusmay produce semantically or grammatically in-appropriate candidate-text sentences, but ourgoal in this work was to generate a variety oftext sentences rather than a few fixed expres-sions with high precision 1.3.1 Generation-Rule AcquisitionLet us denote a set of keywords as KS and aset of rules, each of which generates a bunsetsuwhen given keyword k(?KS), as Rk.
We thenrestrict rk(?Rk) to those represented ask ?
hkm?.
(5)In this rule, hkrepresents the head morphemewhose word is equal to keyword k; m?
repre-sents zero, one, or a series of morphemes thatare connected to hkin the same bunsetsu.
Here,we define a morpheme as consisting of a wordand its morphological information or grammat-ical attribute, such as part-of-speech, and wedefine a head morpheme as consisting of a head-word and its grammatical attribute.
By apply-ing these rules, we generate bunsetsus from in-put keywords.3.2 Construction of Dependency TreesGiven keywords K = k1k2 .
.
.
kn, candidate bun-setsus are generated by applying the generationrules described in Section 3.1.
Next, by as-suming dependency relationships between thebunsetsus, candidate dependency trees are con-structed.
Dependencies between the bunsetsusare restricted in that they must have the follow-ing characteristics of Japanese dependencies:1Note that 83.33% (3,973/4,768) of the headwords inthe newspaper articles appearing on January 17, 1995were found in those appearing from January 1st to 16th.However, only 21.82% (2,295/10,517) of the headworddependencies in the newspaper articles appearing onJanuary 17th were found in those appearing from Jan-uary 1st to 16th.
(i) Dependencies are directed from left toright.
(ii) Dependencies do not cross.
(iii) All bunsetsus except the rightmost one de-pend on only one other bunsetsu.For example, when three keywords are givenand candidate bunsetsus including each keywordare generated as b1, b2, and b3, the candidate de-pendency trees are (b1 (b2 b3)) and ((b1 b2) b3)if we do not reorder keywords, but 16 trees re-sult if we consider the order of keywords to bearbitrary.4 Text-Generation ModelWe next describe the model represented by Eq.
(4); that is, a keyword-production model, amorpheme model that estimates how likely astring is to be a morpheme, and a dependencymodel.
The goal of this model is to selectoptimal sets of morphemes and dependenciesthat can generate natural sentences.
We imple-mented these models within an maximum en-tropy framework (Berger et al, 1996; Ristad,1997; Ristad, 1998).4.1 Keyword-Production ModelsThis section describes five keyword-productionmodels which are represented by P (K|M,D,T )in Eq.
(4).
In these models, we define the set ofheadwords whose frequency in the corpus is overa certain threshold as a set of keywords, KS,and we restrict the bunsetsus to those generatedby the generation rules represented in form (5).We assume that all keywords are independentand that kicorresponds to word wj(1 ?
j ?
m)when text is given as a series of words w1 .
.
.
wm.1.
trigram modelWe assume that kidepends only on the twoanterior words wj?1 and wj?2.P (K|M,D, T ) =n?i=1P (ki|wj?1, wj?2).(6)2.
posterior trigram modelWe assume that kidepends only on the twoposterior words wj+1 and wj+2.P (K|M,D, T ) =n?i=1P (ki|wj+1, wj+2).(7)3.
dependency bigram modelWe assume that kidepends only on the tworightmost words wland wl?1 in the right-most bunsetsu that modifies the bunsetsuincluding ki(see Fig.
3).P (K|M,D, T ) =n?i=1P (ki|wl, wl?1).
(8)Figure 3: Relationship between keywords andwords in bunsetsus.4.
posterior dependency bigram modelWe assume that kidepends only on theheadword, ws, and the word on its right,ws+1, in the bunsetsu that is modified bythe bunsetsu including ki(see Fig.
3).P (K|M,D, T ) =n?i=1P (ki|ws, ws+1).
(9)5. dependency trigram modelWe assume that kidepends only on the tworightmost words wland wl?1 in the right-most bunsetsu that modifies the bunsetsu,and on the two rightmost words whandwh?1 in the leftmost bunsetsu that modi-fies the bunsetsu including ki(see Fig.
3).P (K|M,D, T ) =n?i=1P (ki|wl, wl?1, wh, wh?1).
(10)4.2 Morpheme ModelLet us assume that there are l grammaticalattributes assigned to morphemes.
We call amodel that estimates the likelihood that a givenstring is a morpheme and has the grammaticalattribute j(1 ?
j ?
l) a morpheme model.Let us also assume that morphemes in the or-dered set of morphemes M depend on the pre-ceding morphemes.
We can then represent theprobability of M , given text T ; namely, P (M |T )in Eq.
(4):P (M |T ) =n?i=1P (mi|mi?11, T ), (11)where mican be one of the grammatical at-tributes assigned to each morpheme.4.3 Dependency ModelLet us assume that dependencies di(1 ?
i ?
n)in the ordered set of dependencies D are inde-pendent.
We can then represent P (D|M,T ) inEq.
(4) asP (D|M,T ) =n?i=1P (di|M,T ).
(12)5 EvaluationTo evaluate our system we made 30 sets ofkeywords, with three keywords in each set, asshown in Table 1.
A human subject selectedthe sets from headwords that were found tenTable 1: Input keywords and examples of sys-tem output.Input (Keywords) Ex.
of system output???
??
??
(????
(???
????))??
??
???
((???
???)
???)??
??
??
((???
???)
??)???
???
??
(????
(???
???))??
??
?????
??
???
((???
???)
???)???
??
??
((????
???)
??)???
??
??
(????
(???
?????))??
??
???
??
??
((??
???)
????)??
??
???
((???
???)
???)??
??
??
((???
?????)
??)??
??
??
(???
(???
????))??
???
??
((???
????)
??)??
??
???
(???
(???
????))??
???
??
(???
(????
?????))??
??
????
??
??
((???
???)
??????)???
??
?
((????
??????)
?)??
??
???
((???
???)
??????)??
??
????
((???
???)
??????)??
??
???
((???
???)
???)??
??
?????
??
??
((????
????)
??????)??
???
??
((???
????)
????)??
??
???
(???
(???
???))??
???
????
(???
(????
???????))??
??
???
((???
???)
???)??
???
?????
??
??
?times or more in the newspaper articles on Jan-uary 1st in the Kyoto University text corpus(Version 3.0) without looking at the articles.We evaluated each model by the percentageof outputs that were subjectively judged as ap-propriate by one of the authors.
We used twoevaluation standards.?
Standard 1: If the dependency tree rankedfirst is semantically and grammatically ap-propriate, it is judged as appropriate.?
Standard 2: If there is at least one depen-dency tree that is ranked within the topten and is semantically and grammaticallyappropriate, it is judged as appropriate.We used headwords that were found five timesor more in the newspaper articles appearingfrom January 1st to 16th in the Kyoto Univer-sity text corpus and also found in those appear-ing on January 1st as the set of headwords, KS.For headwords that were not in KS, we addedtheir major part-of-speech categories to the set.We trained our keyword-production models byusing 1,129 sentences (containing 10,201 head-words) from newspaper articles appearing onJanuary 1st.
We used a morpheme model and adependency model identical to those proposedby Uchimoto et al (Uchimoto et al, 2001; Uchi-moto et al, 1999; Uchimoto et al, 2000b).
Totrain the models, we used 8,835 sentences fromnewspaper articles appearing from January 1stto 9th in 1995.
Generation rules were acquiredfrom newspaper articles appearing from Jan-uary 1st to 16th.
The total number of sentenceswas 18,435.First, we evaluated the outputs generatedwhen the rightmost two keywords, such as ???
and??,?
on each line of Table 1 were input.Table 2 shows the results.
KM1 through KM5stand for the five keyword-production modelsdescribed in Section 4.1, and MM and DM standfor the morpheme and the dependency models,respectively.
The symbol + indicates a combi-nation of models.
In the models without MM,DM, or both, P (M |T ) and P (D|M,T ) were as-sumed to be 1.
We carried out additional ex-periments with models that considered both theanterior and posterior words, such as the com-bination of KM1 and KM2 or KM3 and KM4.The results were at most 16/30 by standard 1and 24/30 by standard 1.Table 2: Results of subjective evaluation.Model Standard 1 Standard 2KM1 (trigram) 13/30 28/30KM1 + MM 21/30 28/30KM1 + DM 12/30 28/30KM1 + MM + DM 26/30 28/30KM2 (posterior trigram) 6/30 15/30KM2 + MM 8/30 20/30KM2 + DM 10/30 20/30KM2 + MM + DM 9/30 25/30KM3 (dependency bigram) 13/30 29/30KM3 + MM 26/30 29/30KM3 + DM 14/30 28/30KM3 + MM + DM 27/30 29/30KM4 (posterior dependency bigram) 10/30 18/30KM4 + MM 9/30 26/30KM4 + DM 9/30 22/30KM4 + MM + DM 13/30 27/30KM5 (dependency trigram) 12/30 26/30KM5 + MM 17/30 28/30KM5 + DM 12/30 27/30KM5 + MM + DM 26/30 28/30The models KM1+MM+DM,KM3+MM+DM, and KM5+MM+DMachieved the best results, as shown in Ta-ble 2.
For models KM1, KM3, and KM5, theresults with MM and DM were significantlybetter than those without MM and DM inthe evaluation by standard 1.
We believe thiswas because cases are more tightly connectedwith verbs than with nouns, so models KM1,KM3, and KM5, which learn the connectionbetween cases and verbs, can better rank thecandidate-text sentences that have a naturalconnection between cases and verbs than othercandidates.Next, we conducted experiments using the30 sets of keywords shown in Table 1 as in-puts.
We used two keyword-production mod-els: model KM3+MM+DM, which achievedthe best results in the first experiment, andmodel KM5+MM+DM, which considers therichest information.
We assumed that the in-put keyword order was appropriate and did notreorder the keywords.
The results for bothmodels were the same: 19/30 in the evalu-ation by standard 1 and 24/30 in the eval-uation by standard 2.
The right column ofTable 1 shows examples of the system out-put.
For example, for the input ???
(syourai,in the future), ???
(shin-shin-tou, the NewFrontier Party), and ????
(umareru, tobe born)?, the dependency tree ?(???
[syourai wa] (????
[shin-shin-tou ga] ???????
[umareru darou]))?
(?The NewFrontier Party will be born in the future.?
)was generated.
This output was automati-cally complemented by the appropriate modal-ity ?????
(darou, will), which agrees withthe word ????
(syourai, in the future), aswell as by post-positional particles such as ???
(wa, case marker) and ???
(ga).
Forthe input ????
(gaikoku-jin, a foreigner), ??
(kanyuu, to join), and ??
(zouka, to in-crease)?, the dependency tree ?
(( ????
[gaikokujin no] ????
[kanyuu sya ga]) ??????
[zouka shite iru] )?
(?Foreignermembers are increasing in number.?)
wasgenerated.
This output was complementednot only by the modality expression ??????
(shite iru, the progressive form) andpost-positional particles such as ???
(no, of)and ???
(ga), but also by the suffix ???
(sya, person), and a compound noun ?????
(kanyuu sya, member) was generated naturally.In six cases, though, we did not obtain appro-priate outputs because the candidate-text sen-tences were not appropriately ranked.
Improv-ing the back-off ability of the model by usingclassified words or synonyms as features shouldenable us to rank sentences more appropriately.6 Related WorkMany statistical generation methods have beenproposed.
In this section, we describe the differ-ences between our method and several previousmethods.Japanese words are often followed by post-positional particles, such as ?ga?
and ?wo?,to indicate the subject and object of a sen-tence.
There are no corresponding words inEnglish.
Instead, English words are precededby articles, ?the?
and ?a,?
to distinguish def-inite and indefinite nouns, and so on, and inthis case there are no corresponding words inJapanese.
Knight et al proposed a way tocompensate for missing information caused bya lack of language-dependent knowledge, or a?knowledge gap?
(Knight and Hatzivassiloglou,1995; Langkilde and Knight, 1998a; Langkildeand Knight, 1998b).
They use semantic expres-sions as input, whereas we use keywords.
Also,they construct candidate-text sentences or wordlattices by applying rules, and apply their lan-guage model, an n-gram model, to select themost appropriate surface text.
While we can-not use their rules to generate candidate-textsentences when given keywords, we can applytheir language model to our system to generatesurface-text sentences from candidate-text sen-tences in the form of dependency trees.
We canalso apply the formalism proposed by Langkilde(Langkilde, 2000) to express the candidate-textsentences.Bangalore and Rambow proposed a methodto generate candidate-text sentences in the formof trees (Bangalore and Rambow, 2000).
Theyconsider dependency information when derivingtrees by using XTAG grammar, but they as-sume that the input contains dependency infor-mation.
Our system generates candidate-textsentences without relying on dependency infor-mation in the input, and our model estimatesthe dependencies between keywords.Ratnaparkhi proposed models to generatetext from semantic attributes (Ratnaparkhi,2000).
The input of these models is semanticattributes.
His models are similar to ours if thesemantic attributes are replaced with keywords.However, his models need a training corpus inwhich certain words are replaced with seman-tic attributes.
Although our model also needsa training corpus, the corpus can be automati-cally created by using a morphological analyzerand a dependency analyzer, both of which arereadily available.Humphreys et al proposed using mod-els developed for sentence-structure analysis torank candidate-text sentences (Humphreys etal., 2001).
As well as models developed forsentence-structure analysis, we also use thosedeveloped for morphological analysis and foundthat these models contribute to the generationof appropriate text.Berger and Lafferty proposed a languagemodel for information retrieval (Berger and Laf-ferty, 1999).
Their concept is similar to that ofour model, which can be regarded as a modelthat translates keywords into text, while theirmodel can be regarded as one that translatesquery words into documents.
However, the pur-pose of their model is different: their goal is toretrieve text that already exists while ours is togenerate new text.7 ConclusionWe have described a method for generating sen-tences from ?keywords?
or ?headwords?.
Thismethod consists of two main parts, candidate-text construction and evaluation.1.
The construction part generates text sen-tences in the form of dependency trees byproviding complementary information toreplace that missing due to a ?knowledgegap?
and other missing function words, andthus generates natural text sentences basedon a particular monolingual corpus.2.
The evaluation part consists of a modelfor generating an appropriate text sentencewhen given keywords.
This model consid-ers the dependency information betweenwords as well as word n-gram informa-tion.
Furthermore, the model considersboth string and morphological information.If a language model, such as a word n-grammodel, is applied to the generated-text sen-tences in the form of dependency trees, anappropriate surface-text sentence is generated.The word-order model proposed by Uchimoto etal.
can also generate surface text in a naturalorder (Uchimoto et al, 2000a).There are several possible directions for ourfuture research.
In particular,?
We would like to expand the generationrules.
We restricted the generation rulesautomatically acquired from a corpus tothose that generate a bunsetsu.
To gener-ate a greater variety of candidate-text sen-tences, we would like to expand the rulesthat can generate a dependency tree.
Ex-pansion would lead to complementing withcontent words as well as function words.We also would like to prepare default rulesor to classify words into several classeswhen no sentences including the keywordsare found in the target corpus.?
Some of the N-best text sentences gener-ated by our system are semantically andgrammatically unnatural.
To remove suchsentences from among the candidate-textsentences, we must enhance our model sothat it can consider more information, suchas classified words or those in a thesaurus.?
We restricted keywords to the headwords orrightmost content words in the bunsetsus.We would like to expand the definition ofkeywords to other content words and tosynonyms of the keywords.AcknowledgmentsWe thank the Mainichi Newspapers for permis-sion to use their data.
We also thank KimikoOhta, Hiroko Inui, Takehito Utsuro, Man-abu Okumura, Akira Ushioda, Jun?ichi Tsujii,Kiyosi Yasuda, and Masahisa Ohta for theirbeneficial comments during the progress of thiswork.ReferencesS.
Bangalore and O. Rambow.
2000.
Exploiting a Probabilis-tic Hierarchical Model for Generation.
In Proceedings ofthe COLING, pages 42?48.A.
Berger and J. Lafferty.
1999.
Information Retrieval asStatistical Translation.
In Proceedings of the ACM SIGIR,pages 222?229.A.
L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996.A Maximum Entropy Approach to Natural Language Pro-cessing.
Computational Linguistics, 22(1):39?71.P.
F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra,F.
Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.1990.
A Statistical Approach to Machine Translation.Computational Linguistics, 16(2):79?85.K.
Humphreys, M. Calcagno, and D. Weise.
2001.
Reusing aStatistical Language Model for Generation.
In Proceedingsof the EWNLG.K.
Knight and V. Hatzivassiloglou.
1995.
Two-Level, Many-Paths Generation.
In Proceedings of the ACL, pages 252?260.S.
Kurohashi and M. Nagao.
1997.
Building a JapaneseParsed Corpus while Improving the Parsing System.
InProceedings of the NLPRS, pages 451?456.S.
Kurohashi and M. Nagao, 1999.
Japanese MorphologicalAnalysis System JUMAN Version 3.61.
Department ofInformatics, Kyoto University.S.
Kurohashi, 1998.
Japanese Dependency/Case StructureAnalyzer KNP Version 2.0b6.
Department of Informatics,Kyoto University.I.
Langkilde and K. Knight.
1998a.
Generation that ExploitsCorpus-Based Statistical Knowledge.
In Proceedings of theCOLING-ACL, pages 704?710.I.
Langkilde and K. Knight.
1998b.
The Practical Value ofN-grams in Generation.
In Proceedings of the INLG.I.
Langkilde.
2000.
Forest-Based Statistical Sentence Gener-ation.
In Proceedings of the NAACL, pages 170?177.A.
Ratnaparkhi.
2000.
Trainable Methods for Surface Natu-ral Language Generation.
In Proceedings of the NAACL,pages 194?201.E.
S. Ristad.
1997.
Maximum Entropy Modeling for NaturalLanguage.
ACL/EACL Tutorial Program, Madrid.E.
S. Ristad.
1998.
Maximum Entropy Modeling Toolkit,Release 1.6 beta.
http://www.mnemonic.com/software/memt.K.
Uchimoto, S. Sekine, and H. Isahara.
1999.
Japanese De-pendency Structure Analysis Based on Maximum EntropyModels.
In Proceedings of the EACL, pages 196?203.K.
Uchimoto, M. Murata, Q. Ma, S. Sekine, and H. Isahara.2000a.
Word Order Acquisition from Corpora.
In Proceed-ings of the COLING, pages 871?877.K.
Uchimoto, M. Murata, S. Sekine, and H. Isahara.
2000b.Dependency Model Using Posterior Context.
In Proceed-ings of the IWPT, pages 321?322.K.
Uchimoto, S. Sekine, and H. Isahara.
2001.
The UnknownWord Problem: a Morphological Analysis of Japanese Us-ing Maximum Entropy Aided by a Dictionary.
In Proceed-ings of the EMNLP, pages 91?99.
