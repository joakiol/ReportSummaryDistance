Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 716?723, Vancouver, October 2005. c?2005 Association for Computational LinguisticsContext and Learning in Novelty DetectionBarry Schiffman and Kathleen R. McKeownDepartment of Computer ScienceColumbia UniversityNew York, N.Y.{bschiff,kathy}@cs.columbia.eduAbstractWe demonstrate the value of using con-text in a new-information detection sys-tem that achieved the highest precisionscores at the Text Retrieval Conference?sNovelty Track in 2004.
In order to de-termine whether information within a sen-tence has been seen in material read pre-viously, our system integrates informationabout the context of the sentence withnovel words and named entities within thesentence, and uses a specialized learningalgorithm to tune the system parameters.1 IntroductionNew-information detection addresses two importantproblems in a society awash in more digital infor-mation than people can exploit.
A novelty detectionsystem could help people who are tracking an eventin the news, where numerous sources present simi-lar material.
It could also provide a way to organizesummaries by focusing on the most recent informa-tion, much like an automated bulletin service.We envision that many types of users would findsuch a system valuable.
Certainly analysts, busi-ness people, and anyone interested in current events,would benefit from being able to track news storiesautomatically, without repetition.
Different news or-ganizations report on the same event, often workinghard to make their reports look different from oneanother, whether or not they have new material toreport.
Our system would help readers to zero inon new information.
In addition, a focus on newinformation provides a way of organizing a generalsummary.Our approach is unique in representing and main-taining the focus in discourse.
The idea stems fromthe fact that novelty often comes in bursts, whichis not surprising since the articles are composed ofsome number of smaller, coherent segments.
Eachsegment is started by some kind of introductory pas-sage, and that is where we expect to find the novelwords.
Novel words are identified by comparingthe current sentence?s words against a table of allwords seen in the inputs to that point.
They let usknow whether the entire segment is likely to con-tain more novel material.
Subsequent passages arelikely to continue the novel discussion whether ornot they contain novel words.
They may containpronomial references or other anaphoric referencesto the novel entity.
Our long-term goal is to inte-grate the approach described in this paper into ourlarger new-information detector, a system that per-forms a more complicated syntactic analysis of theinput texts and employs machine learning to classifypassages as new or old.Meanwhile, we tested our focus-based approachat the Novelty Track at the Text Retrieval Confer-ence (TREC) in 2004.
The Novelty Tracks in 2003and in 2004 were divided into four tasks; Task 1and Task 3 incorporate retrieval, requiring submis-sions to locate the relevant sentences before filter-ing them for novelty.
Tasks 2 and 4 are novelty de-tection alone, using the relevant sentences selectedby humans as input.
Since our interest is in nov-716elty detection, we chose to concentrate on Task 21Our TREC submission was also designed to test aspecialized learning mechanism we implemented totarget either high precision or high recall.In all, the problem of novelty detection is decep-tively difficult.
We were struck by the difficulty thatall groups in the Novelty Track in 2002 and 2003had in obtaining high precision scores.
Submissionsthat classify a very large proportion of the inputsentences as novel reached the highest F-measurescores by getting high recall scores, but failed toachieve any substantial compression of material forusers.
Given that our goal is to generate an up-date summary, we focused on improving precisionand increasing compression, removing as many falsepositives as possible.The next section discusses the Novelty Track andthe approaches others have tried; Section 3 detailsour system, and Section 4 presents the experiments.2 Novelty TrackMuch of the work in new-information detection hasbeen done for the TREC Novelty Track.
The taskis related to first story detection, which is definedon whole documents rather than on passages withindocuments.
In Task 1 of the Novelty Track, a systemis given about 25 documents on a topic and asked tofind all sentences relevant to the topic.
In Task 2,the inputs are the set of relevant sentences, so thatthe program does not see the entire documents.
Theprogram must scan the sentences in order and outputall that contain new information, that is informationnot seen in the previous input sentences.2.1 Related WorkAt the recent TREC, Dublin City University did wellby comparing the words in a sentence against theaccumulated words in all previous sentences (Blottet al, 2004).
Their runs varied the way in whichthe words were weighted with frequency and inversedocument frequency.
Like our system, theirs followsfrom the intuition that words that are new to a dis-cussion are evidence of novelty.
But our system dis-1Task 4 was similar to Task 2, in that both have the humanannotations as input.
For Task 2, participants only get the anno-tations, but in Task 4, they also receive the novel sentences fromthe first five documents as input.
We felt that we would learn asmuch from the one task as from both.tinguishes between several kinds of words, includ-ing common nouns, named persons, named organi-zation, etc.
Our system also incorporates a mecha-nism for looking at the context of the sentence.Both the Dublin system and ours are preceded bythe University of Iowa?s approach at TREC 2003.
Itbased novelty decisions on a straightforward countof new named entities and noun phrases in a sen-tence (Eichmann et al, 2003).
In 2004, the Iowa sys-tem (Eichmann et al, 2004) tried several embellish-ments, one using synonyms in addition to the wordsfor novelty comparisons, and one using word-sensedisambiguation.
These two runs were above averagein F-measure and about average in precision.The University of Massachusetts system (Abdul-Jaleel et al, 2004) mixed a vector-space model withcosine similarity and a count of previously unseennamed entities.
Their system resembled one of twobaseline methods that we submitted without our fo-cus feature.
Their submission used a similaritythreshold that was tuned experimentally, while ourswas learned automatically.
In earlier work with theTREC 2002 data, UMass (Allan et al, 2003) com-pared a number of sentence-based models rangingin complexity from a count of new words and cosinedistance, to a variety of sophisticated models basedon KL divergence with different smoothing strate-gies and a ?core mixture model?
that considered thedistribution of the words in the sentence with thedistributions in a topic model and a general Englishmodel.A number of groups have experimented withmatrix-based methods.
In 2003, a group from theUniversity of Maryland and the Center for Com-puting Sciences (Conroy et al, 2003) used threetechniques that used QR decomposition and sin-gular value decomposition.
The University ofMaryland, Baltimore County, worked with cluster-ing algorithms and singular value decompositionin sentence-sentence similarity matrices (Kallurkaret al, 2003).
In 2004, Conroy (Conroy, 2004)tested Maximal Marginal Relevance (Goldstein etal., 2000) as well as QR decomposition.The information retrieval group at Tsinghua Uni-versity used a pooling technique, grouping similarsentences into clusters in order to capture sentencesthat partially match two or more other sentences(Ruet al, 2004).
They said they had found difficulties717with sentence-by-sentence comparisons.2.2 PrecisionAt all three Novelty Track evaluations, from 2002 to2004, it is clear that high precision is much harderto obtain than high recall.
Trivial baselines ?
suchas accept all sentences as novel ?
have proved to bedifficult to beat by very much.
This one-line algo-rithm automatically obtains 100% recall and preci-sion equal to the proportion of novel sentences inthe input.
In 2003, when 66% of the relevant sen-tences were novel, the mean precision score was0.6352 and the median was 0.7.
In 2004, 41% of therelevant sentences were novel, and the average pre-cision dropped to 0.46.
The median precision wasalso 0.46.
Meanwhile, average recall scores acrossall submissions actually rose to 0.861 in 2004, com-pared with 0.795 in 2003.
In terms of a real worldsystem, this means that as the number of target sen-tences shrank, the number of sentences in the aver-age program output rose.
Likewise, a trivial systemcould guarantee no errors by returning nothing, butthis would have no value.2.3 SentencesNormally, in Information Retrieval tasks, stricterthresholds result in higher precision, and looserthresholds, higher recall.
In that way, a system cantarget its results to a user?s needs.
But in new-information detection, this rule of thumb fails atsome point as thresholds become stricter.
Recalldoes fall, but precision does not rise.
In other words,there seems to be a ceiling for precision.Several participants noted that their simplerstrategies produced the best results.
For example,in 2003, the Chinese Academy of Sciences (Sun etal., 2003), noted that word overlap was surprisinglystrong as a similarity measure.
As we have seenabove, the Iowa approach of counting nouns was in-corporated by a few others for 2004, including us.This strategy compares words in a sentence againstall previous seen words and thus, avoids comput-ing pairwise similarity between all sentences.
Al-2One group appeared to have submitted a large number of ir-relevant sentences in its submission, since it obtained relativelyhigh recall scores, but very low precision scores, causing theaverage to drop below 0.66.
The average precision of all othergroups is about 0.7.most all participants performed such pairwise com-parisons of systems.A sentence-by-sentence comparison is clearly notthe optimal operation for establishing novelty.
Sen-tences with a large amount of overlap can expressvery different thoughts.
In the extreme, a singleword change can reverse the meaning of two sen-tences: accept and reject.
This phenomenon led theTsinghua University group to remark, ?many sen-tences with an overlap of nearly 1 are real novelones.?
(Ru et al, 2004).On the other hand, it?s not hard to find cases whererealizations of equivalent statements take many dif-ferent surface forms ?
with different choices ofwords and different syntactic structures.
The data inthe Novelty Task is drawn from three news servicesand clustered into fairly cohesive sets.
The newswriters consciously try to avoid echoing each other,and over time, echoing themselves.
Sentences suchas these have low word overlap, but are not novel.For this reason, we turned to a strategy of classifyingeach sentence Si against the cumulative backgroundof all the words in all preceding sentences S1...i?1.3 SystemThe system described in this paper was built with theNovelty Track in mind.
The goal was to look at waysto consider longer spans of text than a sentence, andto avoid sentence by sentence comparisons.In the Novelty track, the relevant sentences arepresented in natural order, i.e.
by the date of thedocument they came from, and then by their loca-tion in the document.
The key characteristics of ourprogram include:?
For each relevant sentence, our program cal-culates a sum of novel terms, which are termsthat have not been previously seen.
The termsare weighted according to their category, likeperson, location, common noun or verb.
Theweights are learned automatically.?
For the entire set, the program maintains a fo-cus variable, which indicates whether the pre-vious sentence is novel or old.
Thresholds de-termine whether to continue or shift the focus.These are also learned automatically.718All input documents are fed in parallel into anamed-entity recognizer, which marks persons, or-ganizations, locations, part-of-speech tags for com-mon nouns, and into a finite-state parser, which isused only to identify sentences beginning with sub-ject pronouns.
The output from the two preprocess-ing modules are merged and sent to the classifier.The classifier reads a configuration file that con-tains a set of weights that were learned on the 2003Novelty Track data to apply to different classes ofwords that have not been previously seen.For each sentence, the system adds up the amountof novelty from the weighted terms in a sentence andcompares that to a learned threshold; it classifies thesentence as novel if it exceeds the threshold.
It alsostores the classification in a focus variable.
If thenovelty threshold is not met, the system performs aseries of tests described below, and possibly classi-fies some sentences with few content words as novel,depending on the status of the focus variable.
Ouralgorithm enumerates all cases of changes in focus,and tests these in the order that allows the systemto make the decision it can be most confident aboutfirst.
Thus, when we find a named entity new to thediscussion, we can be pretty sure that we have founda novel sentence.
We can classify that sentence asnew without regard to what preceded it.
But, whenwe find a sentence devoid of high-content words,like ?She said the idea sounded good,?
the systemuses the classification of the previous sentence.
Ifthe antecedents to she or idea are novel, then thissentence must also be novel.
The series of learnedthresholds are imposed in a cascade to maximize thenumber of correct decisions over the training cases,in hopes the values will also cover unseen cases.Thus, the classifier puts each sentence through thetests below, using the learned thresholds and weightsdescribed in Section 3.1.
If any test succeeds, thesystem goes on to the next sentence.1.
If there is a sufficient concentration of novelwords, classify the sentence as novel A suffi-cient concentration occurs when the sum of theweights of the novel content words (includingnamed entities) exceeds a threshold, Tnovel.
Ifthe previous focus was old, this indicates thefocus has shifted to a novel segment.2.
If there is a lack of novel words, classify thesentence as old This is computed by compar-ing the sum of the weights of the already-seencontent words to a separate threshold, Told.
Ifthe previous focus was novel, this means thefocus has shifted to an old segment.3.
For any remaining sentences, the classificationis based on context:(a) If the sentence does not have a sufficientnumber of content words, use the classifi-cation in the focus variable.
This adds thesums of both new and old content wordsand compares that to a threshold, Tkeep.
(b) If the first noun phrase is a third personpersonal pronoun, use the classification inthe focus variable.
Pronouns are knownto signal that the same focus continues(Grosz and Sidner, 1986).
(c) If the sentence has not met any of theabove tests but has a minimum number ofcontent words, shift the focus.
If all testsabove fail and there are a minimum num-ber of content words, with a sum of Tshiftshift the focus.4.
Default This rarely occurs but the default is tocontinue the focus, whether novel or old.We examined the 2003 Novelty Track data andfound that more than half the novel sentences ap-pear in sequences of consecutive sentences (See Ta-ble 1).
This circumstance creates an opportunity tomake principled classifications on some sentencesthat have few, if any, clearly novel words, but con-tinue a new segment.
The use of a focus variablehandles these cases.3.1 LearningIn all, the system uses 11 real-valued parameters,weights and thresholds, and we wanted to learn op-timal values for these.
In particular, we wanted to beable to target either high recall or high precision, Aswe noted above, precision was much more difficult,and for a summarization task, much more important.To learn the optimal values for the parameters, weopted to use an ad hoc algorithm.
The main advan-tage in doing so was when considering instance i,the program can reference the classifications made719Length of Run Count1 13382 4213 1324 725 436 227 118 29 310 311 212 215 217 1Table 1: Novelty often comes in bursts.
This tableshows that 1,338 of the novel sentences in the 2003evaluation were singletons, and not a part of a run ofnovel sentences.
Meanwhile, 1,526 of the sentenceswere part of runs of 2, 3 or 4 sentences.for instance i ?
1, i ?
2, and possibly all the wayback to instance 1, because the classification for in-stance i partly depends on the classification of pre-vious instances.
Not only do many standard super-vised learning methods assume conditional indepen-dence, but they also do not provide access to the on-line classifications during learning.
We constructeda randomized hill-climbing.
The learner is struc-tured like a neural net, but the weight adjustmentsare chosen at random as they are in genetic algo-rithms (See Figure 1).
The evaluation, or fitnessfunction, is the Novelty Track score itself, and thetraining data was the 2003 Novelty Track data.Changes to the hypothesis are selected at randomand evaluated.
If the change does not hurt results, itis accepted.
Otherwise the program backtracks andchooses another weight to update.
We required thenew configuration to produce a score greater than orequal to the previous one before we accepted it.
Thechoice of which weight to update is made at ran-dom, in an effort to avoid local minima in the searchspace, but with an important restriction: the previousn choices are kept in a history list, which is checkedto avoid re-use.
This list is updated at each iteration.The configurations usually converge well within 100iterations.1.
Initialize weights, historyWeights take random values2.
Run the system using current weight set3.
If current score >= previous bestUpdate previous best4.
OtherwiseUndo move5.
Update history6.
Choose next weight to change7.
Go to step 2Figure 1: The learning algorithm uses a randomizedhill climbing approach with backtracking3.2 Bias AdjustmentIn training on the 2003 data, the biggest problemwas to find a way to deal with the large percentageof novel sentences.
About 65% of the instances arepositive, so that a random system achieves a rela-tively high F-measure by increasing the number ofsentences it calls novel ?
until recall reaches 1.0.Another strategy would be to choose only the sen-tences in the first document, achieving a high pre-cision ?
more than 90% of the relevant sentences inthe first document for each topic were called novel.In the Novelty Track the F-measure was set togive equal weight to precision and recall, but wewanted to be able to coax the learner to give greaterweight to either precision or by adjusting the F-measure computation:F = 1?prec +(1??)recall?
is a number between 0 and 1.
The closer it getsto 1, the more the formula favors precision.We chose whether to emphasize precision or re-call by altering the value of ?.
At the most extreme,we set ?
at 0.9 for the largest emphasis on precision.When emphasizing recall, we left ?
at 0.5.The design was motivated by the need to explorethe problem more fully and inform the algorithm fordeciding novelty as much as to find optimal param-eters for the values.
Thus, we wanted to be ableto record all the steps the learner made through thesearch space, and to save the intermediate states.
Attimes, the learner would settle into a configuration720that produced a trivial solution, and we could chooseone of the intermediate configurations that produceda more reasonable score.3.3 Vector-Space ModuleIn addition to the system which integrates novelword features with focus tracking, we also imple-mented a vector-space approach as a baseline ?
theCosine run.
We tested the vector-space system aloneto contrast it with the focus system, but we alsotested a version which integrated the vector-spacesystem with the focus system.Our vector-space module assigns all non-stop-words a value of 1, and uses the cosine distance met-ric to compute similarity.Cos(u, v) = u ?
v?u?
?
?v?andNovel(si)????
?True if Cos(si, sj) < T,for j = 1 .
.
.
i ?
1False otherwiseAs each sentence is scanned, its similarity is com-puted with all previous sentences and the maximumsimilarity is compared to a threshold T .
If that max-imum exceeds T , it is considered novel.
We chosethe value of T after trials on the 2003 Novelty Trackdata.
It was set at 0.385, resulting in a balanced sys-tem that matched the results of one of the strongestperformers at the TREC evaluations that year.On the 2003 data, when we set T at .9, we foundthat we had a precision of .71 and a recall of 0.98,indicating that about 6% of the sentences were quitesimilar to some preceding sentence (See Figure 2).After that, each point of precision was very costly interms of recall.
Our experience was mirrored by theparticipants at TREC 2003 and again at TREC 2004.We considered this vector-space model to be ourbaseline.
We also tried it in combination with theRecall run explained above.
Because both the Re-call and Cosine runs produced a relatively large out-put and because they used different methods, wethought the intersection would result in higher pre-cision, though with some loss of recall.In practice, the range of recall was much greaterthan precision.
Judging from the experiences of the00.20.40.60.810  0.2  0.4  0.6  0.8  1RecallPrecisionPrecision, recall tradeoffCos <= .10Cos <= .20Cos <= .30Cos <= .40Cos > .40(Novelty = 1 - Cos)Figure 2: The precision and recall scores of avector-space model with cosine similarity at differ-ent thresholds, on the TREC 2003 data.
Making thetest for novelty stricter fails to improve precision buthas a drastic effect on recall.participants at TREC and our own exploratory ex-periments, it was difficult to push precision above0.80 with the TREC 2003 data, and above 0.50 withthe TREC 2004 data.4 Experiments4.1 Results from TREC 2004Our results are encouraging, especially since theconfigurations that were oriented toward higher pre-cision, indeed, achieved the best precision scoresin the evaluation, with our best precision run about20% higher in precision than the best of all the runsby other groups (See Figure 3.)
Meanwhile, ourrecall-oriented run was one of eight runs that were ina virtual tie for achieving the top f-measure.
Theseeight runs were within 0.01 of one another in themeasure.Our five submitted runs were:Prec1 aimed at moderately high precision, with rea-sonable recall.Prec2 aimed at high precision, with little attentionto recall.Recall weighted precision and recall equally.Cosine a baseline of a standard vector-space modelwith a cosine similarity metric.721Figure 3: The graph shows all 54 submission inTask 2 for the Novelty Track, with our five submis-sions labeled.
Our precision-oriented runs were wellahead of all others in precision, while our recall-oriented run was in a large group that reached about0.5 precision with relatively high recall.Combo a composite submission using the intersec-tion of Recall and Cosine.Table 2 shows the numbers of our performance ofour five submissions.
Prec1 had an F-score closeto the average of 0.577 for all systems, while Prec2was 50% ahead of random selection in accuracy.Both our Combo system and our baseline Cosinewere above average in F-measure.
Our emphasis onprecision is justified in a number of ways, althoughthe official yardstick was the F-measure.An analysis of the system?s behavior under thedifferent parameters showed that the precision-oriented runs, in particular Prec1, valued verbs andcommon nouns more than named entities in decid-ing novelty.
The precision-oriented runs also bene-fited more from the focus variable, with their scoresabout 5% higher in terms of F-measure than theywere without it.
The pronoun test, however, wasrarely used, firing less than 1% of the time.We note that we are developing novelty detectionfor summarization, where compression of the reportis valuable.
Table 2 shows the lengths of our re-turns.
It is impossible to compare these preciselywith other systems, because the averages given byNIST are averages of the scores for each of the 50sets, and we do not have the breakdown of the num-bers by set for any submissions but our own.
How-ever, we can estimate the size of the other output byconsidering average precision and recall as if theywere computed over the total number of sentences inall 50 sets.
This computation shows an average out-put for all participants of about 6,500 sentences anda median of 6,981 ?
out of a total of 8,343 sentences.However, this total includes some amount of headermaterial, not only the headline, but the document IDand other identifiers, the date and some shorthandmessages from the wire services to its clients.
Inaddition, a number of the sets had near perfect du-plicate articles.
This is in sharp contrast with typi-cal summaries.
At the 2004 Document Understand-ing Conference, the typical input cluster containedmore than 4,000 words, and the task required thatthis be reduced to 100 words.
We contend there islittle value in a system that does no more than weedout very few sentences, even though they might haveachieved high F-measures.Second, our experience, and the results of othergroups, shows that high precision is harder than highrecall.
In all three years of the Novelty Track, pre-cision scores tended to hover in a narrow band justabove what one would get by mechanically labelingall sentences as novel.5 ConclusionThe success of our use of context in the TRECNovelty Track led us to incorporate the idea into alarger system.
This system identifies clauses withinsentences that express new information and tries toidentify semantic equivalents.
It is being developedas part of a multi-document summarizer that pro-duces topical updates for users.In addition, the work here suggests three direc-tions for future work:?
Adapt the features used here to some of thenewer probabilistic formalisms, like condi-tional random fields.?
Try full segmentation of the input documentsrather than treat the sentences as a sequence.?
Try to identify all nominal references to canon-ical forms.With this experimental system, we obtained thethe top precision scores in the Novelty Track, and722Run-Id Precision Recall F-meas Output lengthPrec1 0.57 0.58 0.562 3276Prec2 0.61 0.45 0.506 2372Recall 0.51 0.82 0.611 5603Cosine 0.49 0.81 0.599 5537Combo 0.53 0.73 0.598 4578Choose All 0.41 1.000 0.581 8343Average All Runs 0.46 0.86 0.577 6500Table 2: Comparison of results of our five runs, compared to a random selection of sentences, and the overallaverage F-scores by all 55 submissions.we obtained the program settings to do this auto-matically.
High precision is very difficult to obtain,and every point in precision costs too much in recall.Further exploration is needed to determine whetherlinguistic knowledge will help, and whether state-of-the-art tools are powerful enough to improve per-formance.Beyond new-information detection, the idea oftracking context with a surface means like the focusvariable is worth exploring in other tasks, includingsummarization and question-answering.ReferencesNasreen Abdul-Jaleel, James Allan, W. Bruce Croft, Fer-nando Diaz, Leah Larkey, Xiaoyan Li, Donald Met-zler, Mark D. Smucker, Trevor Strohman, Howard Tur-tle, and Courtney Wade.
2004.
Umass at trec 2004:Notebook.
In The Thirteenth Text Retrieval Confer-ence (TREC 2004) Notebook.James Allan, Courtney Wade, and Alvaro Bolivar.
2003.Retrieval and novelty detection at the sentence level.In Proceedings of the ACM SIGIR conference on re-search and development in information retrieval.Stephen Blott, Oisin Boydell, Fabrice Camous, Paul Fer-guson, Georgina Gaughan, Cathal Gurrin, Noel Mur-phy, Noel O?Connor, Alan F. Smeaton, Barry Smyth,and Peter Wilkins.
2004.
Experiments in terabytesearching, genomic retrieval and novelty detection fortrec-2004.
In The Thirteenth Text Retrieval Confer-ence (TREC 2004) Notebook.John M Conroy, Daniel M. Dunlavy, and Dianne P.O?Leary.
2003.
From trec to duc to trec again.
InTREC Notebook Proceedings.John M. Conroy.
2004.
A hidden markov model fortrec?s novelty task.
In The Thirteenth Text RetrievalConference (TREC 2004) Notebook.David Eichmann, Padmini Srinivasan, Marc Light,Hudong Wang, Xin Ying Qiu, Robert J. Arens, andAditya Sehgal.
2003.
Experiments in novelty, genesand questions at the university of iowa.
In TREC Note-book Proceedings.David Eichmann, Yi Zhang, Shannon Bradshaw,Xin Ying Qiu, Li Zhou, Padmini Srinivasan,Aditya Kumar Sehgal, and Hudon Wong.
2004.
Nov-elty, question answering and genomics: The univer-sity of iowa response.
In The Thirteenth Text RetrievalConference (TREC 2004) Notebook.Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and MarkKantrowitz.
2000.
Multi-document summarization bysentence extraction.
In Proceedings of ANLP/NAACL-2000 Workshop on Automatic Summarization.Barbara J. Grosz and Candace L. Sidner.
1986.
Atten-tion, intention, and the structure of discourse.
Compu-tational Linguistics, 12(3):175?204.Srikanth Kallurkar, Yongmei Shi, R. Scott Cost, CharlesNicholas, Akshay Java, Christopher James, SowjanyaRajavaram, Vishal Shanbhag, Sachin Bhatkar, andDrew Ogle.
2003.
Umbc at trec 12.
In TREC Note-book Proceedings.R.
Ohgaya, A. Shimmura, and T. Takagi.
2003.
Meijiuniversity web and novelty track experiments at trec2003.
In TREC Notebook Proceedings.Liyun Ru, Le Zhao, Min Zhang, and Shaoping Ma.
2004.Improved feature selection and redundancy computing?
thuir at trec 2004 novelty track.
In The ThirteenthText Retrieval Conference (TREC 2004) Notebook.Ian Soboroff.
2004.
Draft overview of the trec 2004 nov-elty track.
In The Thirteenth Text Retrieval Conference(TREC 2004) Notebook.Jian Sun, Wenfeng pan, Huaping Zhang, Zhe Yang, BinWang, Gang Zhang, and Xueqi Cheng.
2003.
Trec-2003 novelty and web track at ict.
In TREC NotebookProceedings.723
