Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 681?684,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUsing Entity-Based Features to Model Coherence in Student EssaysJill BursteinEducational Testing ServicePrinceton, NJ 08541Joel TetreaultEducational Testing ServicePrinceton, NJ 08541Slava AndreyevEducational Testing ServicePrinceton, NJ 08541jburstein@ets.org  jtetreault@ets.org     sandreyev@ets.orgAbstractWe show how the Barzilay and Lapata entity-based coherence algorithm (2008) can beapplied to a new, noisy data domain ?
studentessays.
We demonstrate that by combiningBarzilay and Lapata?s entity-based featureswith novel features related to grammar errorsand word usage, one can greatly improve theperformance of automated coherence predictionfor student essays for different populations.1 IntroductionThere is a small body of work that has investigatedusing NLP for the problem of identifyingcoherence in student essays.
For example, Foltz,Kintsch & Landauer (1998), and Higgins, Burstein,Marcu & Gentile (2004) have developed systemsthat examine coherence in student writing.
Foltz,et al (1998) systems measure lexical relatednessbetween text segments by using vector-basedsimilarity between adjacent sentences; Higgins etal?s (2004) system computes similarity across textsegments.
Foltz et al?s (1998) approach is in linewith the earlier TextTiling method that identifiessubtopic structure in text (Hearst, 1997).Miltsakaki and Kukich (2000) addressed essaycoherence using Centering Theory (Grosz, Joshi &Weinstein, 1995).
More recently, Barzilay andLapata?s (2008) approach (henceforth, BL08) usedan entity-based representation to evaluatecoherence.
In BL08, entities (nouns and pronouns)are represented by their sentence roles in a text.The algorithm keeps track of the distribution ofentity transitions between adjacent sentences, andcomputes a value for all transition types based ontheir proportion of occurrence in a text.
BL08apply their algorithm to three tasks, using well-formed newspaper corpora: text ordering, summarycoherence evaluation, and readability assessment.For each task, their system outperforms a LatentSemantic Analysis baseline.
In addition, bestperformance on each task is achieved usingdifferent system and feature configurations.
Pitler& Nenkova (2008) applied BL08 to detect textcoherence in well-formed texts.Coherence quality is typically present in scoringcriteria for evaluating a student?s essay.
This paperfocuses on the development of models to predictlow-and high-coherence ratings for essays.Student essay data, unlike newspaper text, istypically noisy, especially when students are non-native English speakers (NNES).
Here, weevaluate how BL08 algorithm features can be usedto model coherence in a new, noisy data domain --student essays.
We found that coherence can bebest modeled by combining BL08 entity-basedfeatures with novel writing quality features.Further, our use of data sets from three differenttest-taker populations also shows that coherencemodels will differ across populations.
Differentpopulations might use language differently whichcould affect how coherence is presented.
Weexpect to incorporate coherence ratings into e-rater?, ETS?s automated essay scoring system(Attali & Burstein, 2006).2 Corpus and AnnotationWe collected approximately 800 essays (in total)across three data sets1: 1) adult, NNES test essays(TOEFL); 2) adult, native and NNES test essays;(GRE) 3) U.S. middle- and high-school native andNNES student essay submissions to Criterion?,ETS?s instructional writing application.Two annotators were trained to rate coherencequality based on how easily they could read anessay without stumbling on a coherence barrier(i.e., a confusing sentence(s)).
Annotators rated1 TOEFL?
is the Test of English as a Foreign Language,and GRE?
is the Graduate Record Admissions Test.681essays on a 3-point scale: 1) low coherence, 2)somewhat coherent, and 3) high coherence.
Theywere instructed to ignore grammar and spellingerrors, unless they affected essay comprehension.During training, Kappa agreement statisticsindicated that annotators had difficulty agreeing onthe middle, somewhat coherent category.
Theannotation scale was therefore collapsed into a 2-point scale: somewhat coherent and highcoherence categories were collapsed into the highcoherence class (H), and low-coherence (L)remained unchanged.
Two annotators labeled anoverlapping set of about 100 essays to calculateinter-rater agreement; weighted Kappa was 0.677.3 System3.1 BL08 AlgorithmWe implemented BL08?s entity-based algorithm tobuild and evaluate coherence models for the essaydata.
In short, the algorithm generates a vector ofentity transition probabilities for documents(essays, here).
Vectors are used to build coherencemodels.
The first step in the algorithm is toconstruct an entity grid in which all entities (nounsand pronouns) are represented by their roles (i.e.,Subject (S), Object (O), Other (X)).
Entity rolesare then used to generate entity transitions ?
therole transitions across adjacent sentences (e.g.,Subject-to-Object, Object-to-Object).
Entitytransition probabilities are the proportions ofdifferent entity transition types within a text.
Theprobability values are used then used as features tobuild a coherence model.Entity roles can be represented in the followingways.
In this study, consistent with BL08, differentcombinations are applied and reported (see Tables2-4).
Entities can be represented in grids withspecified roles (Syntax+) (S,O,X).
Alternatively,roles can be reduced to show only the presence andabsence of an entity (Syntax-) (i.e., Entity Present(P) or Not (N).
Co-referential entities can beresolved (Coreference+) or not (Coreference-).Finally, the Salience option reflects the frequencywith which an entity appears in the discourse: ifthe entity is mentioned two or more times, it issalient (Salient+), otherwise, not (Salient-).Consistent with BL08, we systematicallycompleted runs using various configurations ofentity representations (see Section 4).Given the combination, the entity transitionprobabilities were computed for all labeled essaysin each data set.
We used n-fold cross-validationfor evaluation.
Feature vectors were input to C5.0,a decision-tree machine learning application.3.2 Additional FeaturesIn BL08, augmenting the core coherence featureswith additional features improved the power of thealgorithm.
We extended the feature set withwriting quality features (Table 1).
GUMS featuresdescribe the technical quality of the essay.
Themotivation for type/token features (*_TT) is tomeasure word variety.
For example, a highprobability for a ?Subject-to-Subject?
transitionindicates that the writer is repeating an entity inSubject position across adjacent sentences.However, this does not take into account whetherthe same word is repeated or a variety of words areused.
The {S,O,X,SOX}_TT (type/token) featuresuncover the actual words collapsed into the entitytransition probabilities.
Shell nouns (Atkas &Cortes, 2008), common in essay writing, mightalso affect coherence.NNES essays can contain many spelling errors.We evaluated the impact of a context-sensitivespell checker (SPCR+), as spelling variation willaffect the transition probabilities in the entity grid.Finally, we experimented with a majority votemethod that combined the best performing featurecombinations.4 EvaluationFor all experiments, we used a series of n-foldcross-validation runs with C5.0 to evaluateperformance for numerous feature configurations.In Tables 2, 3 and 4, we report: baselines, resultson our data with BL08?s best system configurationfrom the summary coherence evaluation task(closest to our task), and our best systems.
In theTables, ?best systems?
combined feature sets andoutperformed baselines.
Rows in bold indicatefinal independent best systems that contribute tobest performance in the majority vote method.Agreement is reported as Weighted Kappa (WK),Precision (P), Recall (R) and F-measure (F).Baselines.
We implemented three non-trivialbaseline systems.
E-rater indicates use of the full682feature set from e-rater.
The GUMS (GUMS+)feature baseline, uses the Grammar (G+), UsageFeature Descriptor Feature DescriptionGUMS  Grammar, usage, andmechanics errors, and stylefeatures from an AES systemS_TTO_TTX_TTSOX_TT2P_TTType/token ratios for actualwords recovered from theentity grid, using the entityroles.S_TT_ShellnounsO_TT_ShellnounsX_TT_ShellnounsType/token ratio of non-topiccontent, shell nouns (e.g.,approach, aspect, challenge)Table 1: New feature category description(U+), Mechanics (M+), and Style (ST+) flags(subset of e-rater features) to evaluate a coherencemodel.
The third baseline represents the best runusing type/token features ({S,O,X,SOX}_TT), and{S,O,X}_TT_Shellnouns feature sets (Table 1).The baseline majority voting system includes e-rater, GUMS, and the best performing type/tokenbaseline (see Tables 2-4).Extended System.
We combined our writingquality features with the core BL08 feature set.The combination improved performance over thethree baselines, and over the best performing BL08feature.
Type/token features added to BL08 entitytransitions probabilities improved performance ofall single systems.
This supports the need torecover actual word use.
In Table 2, for TOEFLdata, spell correction improved performance withthe Mechanics error feature (where Spelling isevaluated).
This would suggest that annotatorswere trying to ignore spelling errors when labelingcoherence.
In Table 3, for GRE data, spellcorrection improved performance with theGrammar error feature.
Spell correction didchange grammar errors detected: annotators mayhave self-corrected for grammar.
Finally, themajority vote outperformed all systems.
In Tables3 and 4, Kappa was comparable to humanagreement (K=0.677).5 Conclusions and Future WorkWe have evaluated how the BL08 algorithmfeatures can be used to model coherence for2 Indicates an aggregate feature that computes the type/tokenratio for entities that appear in any of S,O,X role.student essays across three different populations.We found that the best coherence models foressays are built by combining BL08 entity-basedfeatures with writing quality features.
BL08?soutcomes showed that optimal performance wasobtained by using different feature sets fordifferent tasks.
Our task was most similar toBL08?s summary coherence task, but we usednoisy essay data.
The difference in the data typesmight also explain the need for our systems toinclude additional writing quality features.Our majority vote method outperformed threebaselines (and a baseline majority vote).
For two ofthe populations, Weighted Kappa between systemand human agreement was comparable.
Theseresults show promise toward development of anentity-based method that produces reliablecoherence ratings for noisy essay data.
We plan toevaluate this method on additional data sets, and inthe context of automated essay scoring.ReferencesAktas, R. N., & Cortes, V. (2008).
Shell nouns ascohesive devices in published and ESL  studentwriting.
Journal of English for Academic Purposes,7(1), 3?14.Attali, Y., & Burstein, J.
(2006).
Automated essayscoring with e-rater v.2.0 .
Journal of Technology,Learning, and Assessment, 4(3).Barzilay, R. and Lapata, M. (2008).
Modeling localcoherence: An entity-based approach.Computational Linguistics, 34(1), 1-34.Foltz, P., Kintsch, W., and Landauer, T. K. (1998).
Themeasurement of textual coherence with LatentSemantic Analysis.
Discourse Processes,25(2&3):285?307.Higgins, D., Burstein, J., Marcu, D., & Gentile, C.(2004).
Evaluating multiple aspects of coherence instudent essays .
In Proceedings of HLT-NAACL2004, Boston, MA.Grosz, B., Joshi, A., and Weinstein, S.  1995, Centering:A framework for modeling the local coherence ofdiscourse.
Computational Linguistics, 21(2): 203-226.Hearst, M. A.
(1997).
TextTiling: Segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23(1):33?6Miltsakaki, E. and Kukich, K. (2000).
Automatedevaluation of coherence in student essays.
InProceedings of LREC 2000, Athens, GreecePitler, E.,and Nenkova, A (2008).
RevisitingReadability: A Unified Framework for Predicting683Text Quality.
In Proceedings of EMNLP 2008,Honolulu, Hawaii.L (n=64) H  (n=196) L+H (n=260)BASELINES: NO BL08  FEATURES WK P R F P R F P R F(a) E-rater 0.472 56 69 62 89 82 86 79 79 79(b) GUMS 0.455 55 66 60 88 83 85 79 79 79(c)  SOX_TT3  0.484 66 55 60 86 91 88 82 82 82SYSTEMS: Includes BL08  FEATURESCoreference-Syntax+Salient+ (B&L08summary task configuration)0.253 49 34 40 81 88 84 75 75 75(d) Coreference-Syntax-Salient-SPCR+M+ 0.472 76 45 57 84 95 90 83 83 83(e) Coreference+Syntax+Salient-GUMS+ 0.590 68 70 69 90 89 90 85 85 85(f) Coreference+Syntax+Salient-GUMS+O_TT_Shellnouns+0.595 68 72 70 91 89 90 85 85 85Baseline Majority vote: (a),(b), (c) 0.450 55 64 59 88 83 85 79 79 79Majority vote:  (d), (e), (f) 0.598 69 70 70 90 90 90 85 85 85Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System AgreementL (n=48) H (n=210) L+H (n=258)BASELINES: NO BL08  FEATURES WK P R F P R F P R F(a) E-rater 0.383 79 31 45 86 98 92 86 86 86(b) GUMS 0.316 68 27 39 85 97 91 84 84 84(c)  e-rater+SOX_TT4  0.359 78 29 42 86 98 92 85 85 85SYSTEMS: INCLUDES BL08  FEATURESCoreference-Syntax+Salient+ (BL08 summarytask configuration)0.120 35 17 23 83 93 88 79 79 79(d) Coreference+Syntax+Salient-SPCR+G+ 0.547 1.0 43 60 89 1.0 94 90 90 90(e) Coreference+Syntax-Salient-P_TT+ 0.462 70 44 54 88 96 92 86 86 86(f) Coreference+Syntax+Salient+GUMS+SOX_TT+0.580 71 60 65 91 94 93 88 88 88Baseline Majority vote: (a),(b), (c) 0.383 79 31 45 86 98 92 86 86 86Majority vote: (d), (e), (f) 0.610 1.0 49 66 90 1.0 95 91 91 91Table 3: Native and Non-Native English Speaker Test-taker Data (GRE): Annotator/System AgreementL (n=37) H  (n=226) L+H (n=263)BASELINES: NO BL08  FEATURES WK P R F P R F P R F(a) E-rater 0.315 39 46 42 91 88 89 82 82 82(b) GUMS 0.350 47 41 43 90 92 91 85 85 85(c)  SOX_TT 0.263 78 19 30 88 99 93 88 88 88SYSTEMS: INCLUDES BL08  FEATURES(d) Coreference-Syntax+Salient+ (BL08summary task configuration)0.383 79 30 43 90 99 94 89 89 89(e) Coreference-Syntax-Salient-SPCR+ 0.424 67 38 48 90 97 94 89 89 89(f) Coreference+Syntax+Salient+S_TT+ 0.439 65 41 50 91 96 94 89 89 89Baseline Majority vote: (a),(b), (c) 0.324 43 41 42 90 91 91 84 84 84Majority vote: (d), (e), (f) 0.471 82 38 52 91 99 94 90 90 90Table 4:  Criterion Essay Data: Annotator/System Agreement3 Type/token ratios from all roles using a Coreference+Syntax+Salient+ grid.4 Type/token ratios from all roles using Coreference+Syntax+Salient- grid.684
