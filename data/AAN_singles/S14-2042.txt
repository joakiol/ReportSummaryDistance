Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 259?264,Dublin, Ireland, August 23-24, 2014.ECNU: Expression- and Message-level Sentiment OrientationClassification in Twitter Using Multiple Effective FeaturesJiang Zhao?, Man Lan?, Tian Tian Zhu?Department of Computer Science and TechnologyEast China Normal University?51121201042,51111201046@ecnu.cn;?mlan@cs.ecnu.edu.cnAbstractMicroblogging websites (such as Twitter,Facebook) are rich sources of data foropinion mining and sentiment analysis.
Inthis paper, we describe our approachesused for sentiment analysis in twitter (task9) organized in SemEval 2014.
This tasktries to determine whether the sentimentorientations conveyed by the whole tweetsor pieces of tweets are positive, negativeor neutral.
To solve this problem, we ex-tracted several simple and basic featuresconsidering the following aspects: surfacetext, syntax, sentiment score and twittercharacteristic.
Then we exploited thesefeatures to build a classifier using SVMalgorithm.
Despite the simplicity of fea-tures, our systems rank above the average.1 IntroductionMicroblogging services such as Twitter1, Face-book2today play an important role in expressingopinions on a variety of topics, discussing currentissues or sharing one?s feelings about different ob-jects in our daily life (Agarwal and Sabharwal,2012).
Therefore, Twitter (and other platforms)has become a valuable source of users?
sentimentsand opinions and with the continuous and rapidgrowth of the number of tweets, analyzing the sen-timents expressed in twitter has attracted more andmore researchers and communities, for example,the sentiment analysis task in twitter was held inThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/1http://twitter.com2http://facebook.com/SemEval 2013 (Nakov et al., 2013).
It will bene-fit lots of real applications such as simultaneouslybusinesses, media outlets, and help investors todiscover product trends, identify customer pref-erences and categorize users by analyzing thesetweets (Becker et al., 2013).The task of sentiment analysis in twitter in Se-mEval 2014 (Sara et al., 2014) aims to classifywhether a tweet?s sentiment is positive, negative orneutral at expression level or message level.
Theexpression-level subtask (i.e., subtask A) is to de-termine the sentiment of a marked instance of aword or phrase in the context of a given message,while the message-level subtask (i.e., subtask B)aims to determine the sentiment of a whole mes-sage.
Previous work (Nakov et al., 2013) showedthat message-level sentiment classification is moredifficult than that of expression-level (i.e., 0.690 vs0.889 in terms of F-measure) since a message maybe composed of inconsistent sentiments.To date, lots of approaches have been proposedfor conventional blogging sentiment analysis anda very broad overview is presented in (Pang andLee, 2008).
Inspired by that, many features usedin microblogging mining are adopted from tradi-tional blogging sentiment analysis task.
For ex-ample, n-grams at the character or word level,part-of-speech tags, negations, sentiment lexiconswere used in most of current work (Agarwal etal., 2011; Barbosa and Feng, 2010; Zhu et al.,2013; Mohammad et al., 2013; K?okciyan et al.,2013).
They found that n-grams are still effectivein spite of the short length nature of microblog-ging and the distributions of different POS tagsin tweets with different polarities are highly dif-ferent (Pak and Paroubek, 2010).
Compared withformal blog texts, tweets often contain many in-formal writings including slangs, emoticons, cre-259ative spellings, abbreviations and special marks(i.e., mentions @ and hashtags #), and thus manytwitter-specific features are proposed to character-ize this phenomena.
For example, features recordthe number of emoticons, elongated words andhashtags were used in (Mohammad et al., 2013;Zhu et al., 2013; K?okciyan et al., 2013).
In thiswork, we adopted many features from previouswork and then these features were fed to SVM toperform classification.The remainder of this paper is organized as fol-lows.
Section 2 describes our systems includingpreprocessing, feature representations, data sets,etc.
Results of two subtasks and discussions arereported in Section 3.
Finally, we conclude thispaper in Section 4.2 Our SystemsWe extracted eight types of features and the firstsix types were used in subtask A and all featureswere used in subtask B.
Then, several classifica-tion algorithms were examined on the develop-ment data set and the algorithm with the best per-formance was chosen in our final submitted sys-tems.2.1 PreprocessingIn order to remedy as many informal texts aspossible, we recovered the elongated words totheir normal forms, e.g., ?goooooood?
to ?good?and collected about five thousand slangs or ab-breviations from Internet to convert each slangto its complete form, e.g., ?1dering?
to ?won-dering?, ?2g2b4g?
to ?to good to be forgotten?.Then these preprocessed texts were used to extractnon twitter-specific features (i.e., POS, lexicon, n-grams, word cluster and indicator feature).2.2 Feature Representations2.2.1 POS Features(Pak and Paroubek, 2010) found that POS tagshelp to identify the sentiments of tweets and theypointed out that objective tweets often containmore nouns than subjective tweets and subjec-tive tweets may carry more adjectives and adverbsthan objective tweets.
Therefore, we used Stan-ford POS Tagger3and recorded the number offour different tags for each tweet: noun (the cor-responding POS tags are ?NN?, ?NNP?, ?NNS?and ?NNPS?
), verb (the corresponding POS tags3http://nlp.stanford.edu/software/tagger.shtmlare ?VB?, ?VBD?, ?VBG?, ?VBN?, ?VBP?
and?VBZ?
), adjective (the corresponding POS tagsare ?JJ?, ?JJR?
and ?JJS?)
and adverb (the corre-sponding POS tags are ?RB?, ?RBR?
and ?RBS?
).Then we normalized them by the length of giveninstance or message.2.2.2 Sentiment Lexicon-based FeaturesSentiment lexicons are widely used to calculatethe sentiment scores of phrases or messages in pre-vious work (Nakov et al., 2013; Mohammad et al.,2013) and they are proved to be very helpful todetect the sentiment orientation.
Given a phraseor message, we calculated the following six fea-ture values: (1) the ratio of positive words to allwords, i.e., the number of positive words dividedby the number of total words; (2) the ratio of neg-ative words to all words; (3) the ratio of objectivewords to all words; (4) the ratio of positive senti-ment score to the total score (i.e., the sum of thepositive and negative score); (5) the ratio of nega-tive sentiment score to the total score; (6) the ratioof positive score to negative score, if the negativescore is zero, which means this phrase or messagehas a very strong positive sentiment orientation,we set ten times of positive score as its value.During the calculation, we also considered theeffects of negation words since they may reversethe sentiment orientation in most cases.
To do so,we defined the negation context as a snippet of atweet that starts with a negation word and endswith punctuation marks.
If a non-negation wordis in a negation context and also in the sentimentlexicon, we reverse its polarity.
For example, theword ?bad?
in phrase ?not bad?
originally has anegative score of 0.625, after reversal, this phrasehas a positive score of 0.625.
A manually madelist containing 29 negation words (e.g., no, hardly,never, etc) was used in our experiment.Four sentiment lexicons were used to decidewhether a word is subjective or objective and ob-tain its sentiment score.MPQA (Wilson et al., 2009).
This subjectiv-ity lexicon contains about 8000 subjective wordsand each word has two types of sentiment strength:strong subjective and weak subjective, and fourkinds of sentiment polarities: positive, negative,both (positive and negative) and neutral.
We usedthis lexicon to determine whether a word is posi-tive, negative or objective and assign a value of 0.5or 1 if it is weak or strong subjective (i.e., positiveor negative) respectively.260SentiWordNet(SWN) (Baccianella et al.,2010).
This sentiment lexicon contains about117 thousand items and each item correspondsto a synset of WordNet.
Three sentiment scores:positivity, negativity, objectivity are provided andthe sum of these three scores is always 1, forexample, living#a#3, positivity: 0.5, negativity:0.125, objectivity: 0.375.
In experiment we usedthe most common sense of a word.NRC (Mohammad et al., 2013).
Mohammad etal.
collected two sets of tweets and each tweet con-tains the seed hashtags or emoticons and then theylabeled the sentiment orientation for each tweetaccording to its hashtags or emoticons.
They usedpointwise mutual information (PMI) to calculatethe sentiment score for each word and obtainedtwo sentiment lexicons (i.e., hashtag lexicon andemoticon lexicon).IMDB.
We generated an unigram lexicon byourselves from a large movie review data set fromIMDB website (Maas et al., 2011) which con-tains 25,000 positive and 25,000 negative moviereviews by calculating their PMI scores.2.2.3 Word n-GramWords in themselves in tweets usually carry outthe original sentiment orientation, so we con-sider word n-grams as one feature.
We removedURLs, mentions, hashtags, stopwords from tweetand then all words were stemmed using the nltk4toolkit.
For subtask A, only unigram was used andwe used word frequency as feature values.
Forsubtask B, both unigram and bigram were used.Besides, weighted unigram was also used wherewe replaced word frequency with their sentimentscores using the hashtag lexicon and emoticon lex-icon in NRC.2.2.4 Twitter-specific FeaturesPunctuation Generally, punctuation may expressusers?
sentiment in a certain extent.
Therefore werecorded the frequency of the following four typesof punctuation: exclamation (!
), question (?
), dou-ble (?)
and single marks (?).
In addition, we alsorecorded the number of contiguous sequences ofexclamation marks, question marks, and both ofthemwhich appeared at the end of a phrase or mes-sage.Emoticon Emoticons are widely used to directlyexpress the sentiment of users and thus we counted4http://nltk.org/the number of positive emoticons, negative emoti-cons and the sum of positive and negative emoti-cons.
To identify the polarities of emoticons, wecollected 36 positive emoticons and 33 negativeemoticons from the Internet.Hashtag A hashtag is a short phrase that con-catenates more than one words together withoutwhite spaces and users usually use hashtags tolabel the subject topic of a tweet, e.g., #toobad,#ihateschool, #NewGlee.
Since a hashtag maycontain a strong sentiment orientation, we firstused the Viterbi algorithm (Berardi et al., 2011)to split hashtags and then calculated the sentimentscores of hashtags using the hashtag and emoticonlexicon in NRC.2.2.5 Word ClusterApart from n-gram, we presented another wordrepresentations based on word clusters to exploreshallow semantic meanings and reduced the spar-sity of the word space.
1000 word clusters pro-vided by CMU pos-tagging tool5were used to rep-resent tweet contents.
For each tweet we recordedthe number of words from each cluster, resultingin 1000 features.2.2.6 Indicator FeaturesWe observed that the polarity of a message some-times is revealed by some special individual posi-tive or negative words in a certain degree.
How-ever the sentiment lexicon based features wherea synthetical sentiment score of a message is cal-culated may hide this information.
Therefore, wedirectly used several individual sentiment scoresas features.
Specifically, we created the followingsixteen features for each message where the hash-tag and emoticon lexicons were used to obtain sen-timent scores: the sentiment scores of the first andlast sentiment-bearing words, the three highest andlowest sentiment scores.2.3 Data sets and Evaluation MetricThe organizers provide tweet ids and a script forall participants to collect data.
Table 1 shows thestatistics of the data set used in our experiments.To examine the generalization of models trainedon tweets, the test data provided by the organiz-ers consists of instances from different domainsfor both subtasks.
Specifically, five corpora are in-cluded: LiveJournal(2014) is a collection of com-ments from LiveJournal blogs, SMS2013 is a SMS5http://www.ark.cs.cmu.edu/TweetNLP/261data set directly from last year, Twitter2013 is atwitter data set directly from last year, Twitter2014is a new twitter data set and Twitter2014Sarcasmis a collection of tweets that contain sarcasm.
No-tice that the data set SMS2013 and Twitter2013were also used as our development set.
Form Ta-ble 1, we find that (1) the class distributions of testdata sets almost agree with training data sets forboth subtasks, (2) the percentages of class neutralin two subtasks are significantly different (4.7%vs 45.5%), which reflects that a sentence which iscomposed of different sentiment expressions mayact neutrality, (3) Twitter2014Sarcasm data set isvery small.
According to the guideline, we did notuse any development data for training in the eval-uation period.data set Positive Negative Neutral Totalsubtask A:train 3,609(61%) 2,023(34%) 265(5%) 5,897dev 2,734(62%) 1,541(35%) 160(3%) 4,435testLiveJournal 660(50%) 511(39%) 144(11%) 1,315SMS2013 1,071(46%) 1,104(47%) 159( 7%) 2,334Twitter2013 2,734(62%) 1,541(35%) 160(3%) 4,435Twitter2014 1,807(73%) 578(23%) 88( 4%) 2,473Twitter2014S 82(66%) 37(30%) 5(4%) 124all 6,354(59%) 3,771(35%) 556(6%) 10,681subtask B:train 3,069(36%) 1,313(15%) 4,089(49%) 8,471dev 1,572(41%) 601(16%) 1,640(43%) 3,813testLiveJournal 427(37%) 304(27%) 411(36%) 1,142SMS2013 492(24%) 394(19%) 1,207(57%) 2,093Twitter2013 1,572(41%) 601(16%) 1,640(43%) 3,813Twitter2014 982(53%) 202(11%) 669(36%) 1,853Twitter2014S 33(38%) 40(47%) 13(15%) 86all 3,506(39%) 1,541(17%) 3,940(44%) 8,987Table 1: Statistics of data sets in training (train),development (dev), test (test) set.
Twitter2014Sstands for Twitter2014Sarcasm.We used macro-averaged F-measure of positiveand negative classes (without neutral since it ismargin in training data) to evaluate the perfor-mance of our systems and the averaged F-measureof five corpora was used to rank the final results.2.4 Submitted System ConfigurationsFor each subtask, each team can submit two runs:(1) constrained: only the provided data set can beused for training and no additional annotated datais allowed for training, however other resourcessuch as lexicons are allowed; (2) unconstrained:any additional data can be used for training.
Weexplored several classification algorithms on thedevelopment set and configured our final systemsas follows.
For constrained system, we used SVMand logistic regression algorithm implemented inscikit-learn toolkit (Pedregosa et al., 2011) to ad-dress two subtasks respectively and used self-training strategy to conduct unconstrained system.Self-training is a semi-supervised learning methodwhere a classifier is first trained with a smallamount of labeled data and then we repeat the fol-lowing procedure: the most confident predictionsby the current classifier are added to training pooland then the classifier is retrained(Zhu, 2005).
Theparameters in constrained models and the growthsize k and iteration number T in self-training arelisted in Table 2 according to the results of prelim-inary experiments.task constrained unconstrainedsubtask A SVM, kernel=rbf, c=500 k=100, T=40subtask B LogisticRegression, c=1 k=90, T=40Table 2: System configurations for the constrainedand unconstrained runs in two subtasks.3 Results and Discussion3.1 ResultsWe submitted four systems as described above andtheir final results are shown in Table 3, as well asthe top-ranked systems released by the organizers.From the table, we observe the following findings.Firstly, we find that the results of message-levelpolarity classification are much worse than the re-sults of expression-level polarity disambiguation(82.93 vs 61.22) on both constrained and uncon-strained systems, which is consistent with the pre-vious work (Nakov et al., 2013).
The low per-formance of message-level task may result fromtwo possible reasons: (1) a message may con-tain mixed sentiments and (2) the strength ofsentiments is different.
In contrast, the texts inexpression-level task are usually short and containa single sentiment orientation, which leads to bet-ter performance.Secondly, whether on constrained or uncon-strained systems, the performance on Twit-ter2014Sarcasm data set is much worse than theperformance on the other four data sets.
This isbecause that sarcasm often expresses the oppositemeaning of what it seems to say, that means theactual sentiment orientation of a word is oppositeto its original orientation.
Moreover, even for ourhuman it is a challenge to identify whether it is asarcasm or not.Thirdly, the results on LiveJournal and SMSare comparable to the results on Twitter2013 andTwitter2014 in both subtasks, which indicates that262online comments and SMS share some commoncharacteristics with tweets (e.g., emoticons andpunctuation).
Therefore, in case of lack of labeledonline comments or SMS data, we can use the ex-isting tweets as training data instead.Fourthly, our unconstrained systems exploit thetest data of year 2014 in training stage and performa worse result in subtask B.
We speculate that thefailure of using self-training on message-level dataset is because that the performance of initial clas-sifier was low and thus in the following iterationsmore and more noisy instances were selected toadd the training pool, which eventually resulted ina final weak classifier.In summary, we adopted some simple and ba-sic features to classify the polarities of expressionsand messages and they were promising.
For sub-task A, our systems rank 5th out of 19 submissionsunder the constrained setting and rank 2nd out of 6submissions under the unconstrained setting.
Forsubtask B, our systems rank 16th out of 42 submis-sions under the constrained setting and rank 5thout of 8 submissions under the unconstrained set-ting.3.2 Feature Combination ExperimentsTo explore the effectiveness of different featuretypes, we conducted a series of feature combina-tion experiments using the constrained setting asshown in Table 2 for both subtasks.
For each timewe repeated to add one feature type to current fea-ture set and then selected the best one until all thefeature types were processed.
Table 4 shows theresults of different feature combinations and thebest results are shown in bold font.From Table 4, we find that (1) MPQA, n-gramand Word cluster are the most effective featuretypes to identify the polarities; (2) The POS tagsmake margin contribution to improve the perfor-mance since Stanford parser is designed for for-mal texts and in the future we may use specificparser instead; (3) The lexicon IMDB extractedfrom movie reviews has negative effects to clas-sify twitter data, which indicates that there existdifferences in the way of expressing sentimentsbetween these two domains; (4) Twitter-specificfeatures, i.e., hashtag and emoticon, are not as ef-fective as expected.
This is because they are sparsein the data sets.
In subtask Awith 16578 instances,only 292 instances (1.76%) have hashtags and 419instances (2.52%) have emoticons.
In subtask Bwith 17458 messages, more instances have hash-tags (16.72%) and emoticons (26.70%).
(5) Forsubtask A MPQA, n-gram, NRC and punctuationfeatures achieve the best performance and for sub-task B the best performance is achieved by usingalmost all features.In summary, we find that n-gram and some lex-icons such as MPQA are the most effective whiletwitter-specific features (i.e., hashtag and emoti-con) are not as discriminating as expected and themain reason for this is that they are sparse in thedata sets.Feature Subtask A Feature Subtask BMPQA 77.49 Word cluster 53.50.+n-gram 80.08(2.59) .+MPQA 58.35(4.85).+NRC 82.42(2.34) .+W1Gram 60.22(1.87).+Pun.
83.83(1.41) .+Pun.
60.99(0.77).+POS 83.83(0) .+Indicator 61.38(0.39).+Emoticon 83.49(-0.34) .+SWN 61.51(0.13).+Hashtag 83.54(0.05) .+Hashtag 61.54(0.03).+IMDB 83.51(-0.03) .+n-gram 61.56(0.02).+SWN 82.92(-0.59) .+Emoticon 61.69(0.13)- - .+POS 61.71(0.02)- - .+IMDB 61.11(-0.6)- - .+NRC 61.23(0.12)Table 4: The results of feature combination exper-iments.
The numbers in the brackets are the per-formance increments compared with the previousresults.
?.+?
means to add current feature to theprevious feature set.4 ConclusionIn this paper we used several basic feature types toidentify the sentiment polarity at expression levelor message level and these feature types includen-gram, sentiment lexicon and twitter-specific fea-tures, etc.
Although they are simple, our systemsare still promising and rank above average (e.g.,rank 5th out of 19 and 16th out of 42 in subtask Aand B respectively under the constrained setting).For the future work, we would like to analyze thedistributions of different sentiments in sentences.AcknowledgmentsThis research is supported by grants from Na-tional Natural Science Foundation of China(No.60903093) and Shanghai Knowledge ServicePlatform Project (No.
ZF1213).ReferencesApoorv Agarwal and Jasneet Sabharwal.
2012.
End-to-end sentiment analysis of twitter data.
In Pro-263Systems LiveJournal SMS2013 Twitter2013 Twitter2014 Twitter2014S AverageA-constrained (expression-level) 81.67 89.31 87.28 82.67 73.71 82.93A-unconstrained 81.69 89.26 87.29 82.93 73.71 82.98NRC-Canada-A-constrained?85.49 88.03 90.14 86.63 77.13 85.48Think Positive-A-unconstrained?80.90 87.65 88.06 82.05 76.74 83.08B-constrained(message-level) 69.44 59.75 62.31 63.17 51.43 61.22B-unconstrained 64.08 56.73 63.72 63.04 49.33 59.38NRC-Canada-B-constrained?74.84 70.28 70.75 69.85 58.16 68.78Think Positive-B-unconstrained?66.96 63.20 68.15 67.04 47.85 62.64Table 3: Performance of our systems and the top-ranked systems (marked with asterisk).ceedings of the Workshop on Information Extractionand Entity Analytics on Social Media Data, pages39?44, Mumbai, India, December.
The COLING2012 Organizing Committee.Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-bow, and Rebecca Passonneau.
2011.
Sentimentanalysis of twitter data.
In Proceedings of the Work-shop on Languages in Social Media, LSM ?11, pages30?38.
Association for Computational Linguistics.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In LREC, volume 10, pages 2200?2204.Luciano Barbosa and Junlan Feng.
2010.
Robust sen-timent detection on twitter from biased and noisydata.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,pages 36?44.
Association for Computational Lin-guistics.Lee Becker, George Erhart, David Skiba, and Valen-tine Matula.
2013.
Avaya: Sentiment analysis ontwitter with self-training and polarity lexicon expan-sion.
In Proceedings of the Seventh InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 333?340.
Association for Computational Lin-guistics, June.Giacomo Berardi, Andrea Esuli, Diego Marcheggiani,and Fabrizio Sebastiani.
2011.
Isti@ trec microblogtrack 2011: Exploring the use of hashtag segmenta-tion and text quality ranking.
In TREC.Nadin K?okciyan, Arda C?elebi, Arzucan?Ozg?ur, andSuzan?Usk?udarli.
2013.
Bounce: Sentiment classifi-cation in twitter using rich feature sets.
In Proceed-ings of the Seventh International Workshop on Se-mantic Evaluation (SemEval 2013), pages 554?561.Association for Computational Linguistics, June.Andrew L Maas, Raymond E Daly, Peter T Pham, DanHuang, Andrew Y Ng, and Christopher Potts.
2011.Learning word vectors for sentiment analysis.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies-Volume 1, pages 142?150.
As-sociation for Computational Linguistics.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets.
In Proceedingsof the Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 321?327.
Asso-ciation for Computational Linguistics, June.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
Semeval-2013 task 2: Sentiment analysis intwitter.
In Proceedings of the Seventh InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 312?320.
Association for Computational Lin-guistics, June.Alexander Pak and Patrick Paroubek.
2010.
Twitter asa corpus for sentiment analysis and opinion mining.In Proceedings of the International Conference onLanguage Resources and Evaluation, LREC.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Fabian Pedregosa, Ga?el.
Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and?Edouard Duchesnay.
2011.Scikit-learn: Machine learning in Python.
Journalof Machine Learning Research, 12:2825?2830.Rosenthal Sara, Ritter Alan, Veselin Stoyanov, andNakov Preslav.
2014.
Semeval-2014 task 9: Sen-timent analysis in twitter.
In Proceedings of theEighth International Workshop on Semantic Evalu-ation (SemEval?14).
Association for ComputationalLinguistics, August.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analy-sis.
Computational linguistics, pages 399?433.Tian Tian Zhu, Fang Xi Zhang, and Man Lan.
2013.Ecnucs: A surface information based system de-scription of sentiment analysis in twitter in thesemeval-2013 (task 2).
In Proceedings of the Sev-enth International Workshop on Semantic Evalua-tion (SemEval 2013), page 408.Xiaojin Zhu.
2005.
Semi-supervised learning litera-ture survey.
Technical Report 1530, Computer Sci-ences, University of Wisconsin-Madison.264
