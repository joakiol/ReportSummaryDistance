Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 230?238,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsImproving Distributional Semantic Vectors through Context Selection andNormalisationTamara PolajnarUniversity of CambridgeComputer Laboratorytp366@cam.ac.ukStephen ClarkUniversity of CambridgeComputer Laboratorysc609@cam.ac.ukAbstractDistributional semantic models (DSMs)have been effective at representing seman-tics at the word level, and research has re-cently moved on to building distributionalrepresentations for larger segments of text.In this paper, we introduce novel ways ofapplying context selection and normalisa-tion to vary model sparsity and the rangeof values of the DSM vectors.
We showhow these methods enhance the quality ofthe vectors and thus result in improvedlow dimensional and composed represen-tations.
We demonstrate these effects onstandard word and phrase datasets, and ona new definition retrieval task and dataset.1 IntroductionDistributional semantic models (DSMs) (Turneyand Pantel, 2010; Clarke, 2012) encode wordmeaning by counting co-occurrences with otherwords within a context window and recordingthese counts in a vector.
Various IR and NLPtasks, such as word sense disambiguation, queryexpansion, and paraphrasing, take advantage ofDSMs at a word level.
More recently, researchershave been exploring methods that combine wordvectors to represent phrases (Mitchell and Lapata,2010; Baroni and Zamparelli, 2010) and sentences(Coecke et al., 2010; Socher et al., 2012).
In thispaper, we introduce two techniques that improvethe quality of word vectors and can be easily tunedto adapt the vectors to particular lexical and com-positional tasks.The quality of the word vectors is generally as-sessed on standard datasets that consist of a list ofword pairs and a corresponding list of gold stan-dard scores.
These scores are gathered through anannotation task and reflect the similarity betweenthe words as perceived by human judges (Bruni etal., 2012).
Evaluation is conducted by comparingthe word similarity predicted by the model withthe gold standard using a correlation test such asSpearman?s ?.While words, and perhaps some frequentshorter phrases, can be represented by distri-butional vectors learned through co-occurrencestatistics, infrequent phrases and novel construc-tions are impossible to represent in that way.
Thegoal of compositional DSMs is to find methods ofcombining word vectors, or perhaps higher-ordertensors, into a single vector that represents themeaning of the whole segment of text.
Elemen-tary approaches to composition employ simple op-erations, such as addition and elementwise prod-uct, directly on the word vectors.
These have beenshown to be effective for phrase similarity evalua-tion (Mitchell and Lapata, 2010) and detection ofanomalous phrases (Kochmar and Briscoe, 2013).The methods that will be introduced in this pa-per can be applied to co-occurrence vectors to pro-duce improvements on word similarity and com-positional tasks with simple operators.
We choseto examine the use of sum, elementwise prod-uct, and circular convolution (Jones and Mewhort,2007), because they are often used due to theirsimplicity, or as components of more complexmodels (Zanzotto and Dell?Arciprete, 2011).The first method is context selection (CS), inwhich the top N highest weighted context wordsper vector are selected, and the rest of the valuesare discarded (by setting to zero).
This techniqueis similar to the way that Explicit Semantic Analy-sis (ESA) (Gabrilovich and Markovitch, 2007) se-lects the number of topics that represent a word,and the word filtering approach in Gamallo andBordag (2011).
It has the advantage of improv-ing word representations and vector sum represen-tations (for compositional tasks) while using vec-tors with fewer non-zero elements.
Programminglanguages often have efficient strategies for stor-230ing these sparse vectors, leading to lower memoryusage.
As an example of the resulting accuracyimprovements, when vectors with up to 10,000non-zero elements are reduced to a maximum ofN  240 non-zero elements, the Spearman ?
im-proves from 0.61 to 0.76 on a standard word sim-ilarity task.
We also see an improvement whenused in conjunction with further, standard dimen-sionality reduction techniques: the CS sparse vec-tors lead to reduced-dimensional representationsthat produce higher correlations with human simi-larity judgements than the original full vectors.The second method is a weighted l2-normalisation of the vectors prior to application ofsingular value decomposition (SVD) (Deerwesteret al., 1990) or compositional vector operators.
Ithas the effect of drastically improving SVD with100 or fewer dimensions.
For example, we findthat applying normalisation before SVD improvescorrelation from ?
 0.48 to ?
 0.70 for 20dimensions, on the word similarity task.
Thisis an essential finding as many more complexmodels of compositional semantics (Coecke et al.,2010; Baroni and Zamparelli, 2010; Andreas andGhahramani, 2013) work with tensor objects andrequire good quality low-dimensional represen-tations of words in order to lower computationalcosts.
This technique also improves the perfor-mance of vector addition on texts of any lengthand vector elementwise product on shorter texts,on both the similarity and definitions tasks.The definition task and dataset are an additionalcontribution.
We produced a new dataset of wordsand their definitions, which is separated into nineparts, each consisting of definitions of a particularlength.
This allows us to examine how composi-tional operators interact with CS and normalisa-tion as the number of vector operations increases.This paper is divided into three main sections.Section 2 describes the construction of the wordvectors that underlie all of our experiments and thetwo methods for adaptation of the vectors to spe-cific tasks.
In Section 3 we assess the effects ofCS and normalisation on standard word similar-ity datasets.
In Section 4 we present the compo-sitional experiments on phrase data and our newdefinitions dataset.2 Word Vector ConstructionThe distributional hypothesis assumes that wordsthat occur within similar contexts share similarmeanings; hence semantic vector construction firstrequires a defintition of context.
Here we usea window method, where the context is definedas a particular sequence of words either side ofthe target word.
The vectors are then populatedthrough traversal of a large corpus, by recordingthe number of times each of the target words co-occurs with a context word within the window,which gives the raw target-context co-occurrencefrequency vectors (Freq).The rest of this section contains a descriptionof the particular settings used to construct the rawword vectors and the weighting schemes (tTest,PPMI) that we considered in our experiments.This is followed by a detailed description of thecontext selection (CS) and normalisation tech-niques.
Finally, dimensionality reduction (SVD) isproposed as a way of combating sparsity and ran-dom indexing (RI) as an essential step of encodingvectors for use with the convolution operator.Raw Vectors We used a cleaned-up corpusof 1.7 billion lemmatised tokens (Minnen etal., 2001) from the October, 2013 snapshot ofWikipedia, and constructed context vectors by us-ing sentence boundaries to provide the window.The set of context wordsC consisted of the 10,000most frequent words occurring in this dataset, withthe exception of stopwords from a standard stop-word list.
Therefore, a frequency vector for a tar-get word wiPW is represented as ~wi tfwicjuj,where cjP C (|C|  10, 000), W is a set of targetwords in a particular evaluation dataset, and fwicjis the co-occurrence frequency between the targetword, wiand context word, cj.Vector Weighting We used the tTest and PPMIweighting schemes, since they both performedwell on the development data.
The vectors result-ing from the application of the weighting schemesare as follows, where the tTest and PPMI functionsgive weighted values for the basis vector corre-sponding to context word cjfor target word wi:tTestp ~wi, cjq ppwi, cjq  ppwiqppcjqappwiqppcjq(1)PPMIp ~wi, cjq  ppwi, cjq logppwi, cjqppwiqppcjq(2)where ppwiq ?jfwicj?k?lfwkcl, ppcjq ?ifwicj?k?lfwkcl, andppwi, cjq fwicj?k?lfwkcl.231Original Normalised Normalised*10?1?0.500.511.522.533.5Figure 1: The range of context weights on tTestweighted vectors before and after normalisation.Context Ranking and Selection The weight-ing schemes change the importance of individ-ual target-context raw co-occurrence counts byconsidering the frequency with which each con-text word occurs with other target words.
Thisis similar to term-weighting in IR and many re-trieval functions are also used as weighting func-tions in DSMs.
In the retrieval-based model ESA(Gabrilovich and Markovitch, 2007), only the Nhighest-weighted contexts are kept as a represen-tative set of ?topics?
for a particular target word,and the rest are set to zero.
Here we use a sim-ilar technique and, for each target word, retainonly the N -highest weighted context words, usinga word-similarity development set to choose theN that maximises correlation across all words inthat dataset.
Throughout the paper, we will referto this technique as context selection (CS) and useN to indicate the maximum number of contextsper word.
Hence all word vectors have at most Nnon-zero elements, effectively adjusting the spar-sity of the vectors, which may have an effect onthe sum and elementwise product operations whencomposing vectors.Normalisation PPMI has only positive valuesthat span the range r0,8s, while tTest spansr1, 1s, but generally produces values tightly con-centrated around zero.
We found that these rangescan produce poor performance due to numericalproblems, so we corrected this through weightedrow normalisation: ~w : ?~w||~w||2.
With ?
 10 thishas the effect of restricting the values to r10, 10sfor tTest and r0, 10s for PPMI.
Figure 1 shows therange of values for tTest.
In general we use ?
 1,but for some experiments we use ?
 10 to pushthe highest weights above 1, as a way of combat-ing the numerical errors that are likely to arise dueto repeated multiplications of small numbers.
Thisnormalisation has no effect on the ordering of con-text weights or cosine similarity calculations be-tween single-word vectors.
We apply normalisa-tion prior to dimensionality reduction and RI.SVD SVD transforms vectors from their target-context representation into a target-topic space.The resulting space is dense, in that the vectorsno longer contain any zero elements.
If M is a|w|  |C| matrix whose rows are made of wordvectors ~wi, then the lower dimensional representa-tion of those vectors is encoded in the |W |  Kmatrix?MK UKSKwhere SVDpM,Kq UKSKVK(Deerwester et al., 1990).
We alsotried non-negative matrix factorisation (NNMF)(Seung and Lee, 2001), but found that it did notperform as well as SVD.
We used the standardMatlab implementation of SVD.Random Indexing There are two ways of creat-ing RI-based DSMs, the most popular being to ini-tialise all target word vectors to zero and to gener-ate a random vector for each context word.
Then,while traversing through the corpus, each time atarget word and a context word co-occur, the con-text word vector is added to the vector represent-ing the target word.
This method allows the RIvectors to be created in one step through a singletraversal of the corpus.
The other method, follow-ing Jones and Mewhort (2007), is to create the RIvectors through matrix multiplication rather thansequentially.
We employ this method and assigneach context word a random vector ~ecj trkukwhere rkare drawn from the normal distributionN p0,1Dq and | ~ecj|  D  4096.
The RI repre-sentation of a target word RIp ~wiq  ~wiR is con-structed by multiplying the word vector ~wi, ob-tained as before, by the |C|  D matrix R whereeach column represents the vectors ~ecj.
Weightingis performed prior to random indexing.3 Word Similarity ExperimentsIn this section we investigate the effects of contextselection and normalisation on the quality of wordvectors using standard word similarity datasets.The datasets consist of word pairs and a gold stan-dard score that indicates the human judgement ofthe similarity between the words within each pair.We calculated the similarity between word vectorsfor each pair and compared our results with thegold standard using Spearman correlation.232tTest PPMI FreqData Max ?
Full ?
Max ?
Full ?
Max ?
Full ?MENdev: 0.75 0.73 0.76 0.61 0.66 0.57MENtest 0.76 0.73 0.76 0.61 0.66 0.56WS353 0.70 0.63 0.70 0.41 0.57 0.41Table 1: ValuesN learned on dev (:) also improveperformance on the test data.
Max ?
indicates cor-relation at the values of N that lead to the high-est Spearman correlation on the development data.For each weighting scheme these are: 140 (tTest),240 (PPMI), and 20 (Freq).
Full ?
indicates thecorrelation when using full vectors without CS.The cosine, Jaccard, and Lin similarity mea-sures (Curran, 2004) were all used to ensure theresults reflect genuine effects of context selection,and not an artefact of any particular similaritymeasure.
The similarity measure and value of Nwere chosen, given a particular weighting scheme,to maximise correlation on the development partof the MEN data (Bruni et al., 2012) (MENdev).Testing was performed on the remaining sectionof MEN and the entire WS353 dataset (Finkelsteinet al., 2002).
The MEN dataset consists of 3,000word pairs rated for similarity, which is dividedinto a 2,000-pair development set and a 1,000-pairtest set.
WS353 consists only of 353 pairs, but hasbeen consistently used as a benchmark word simi-larity dataset throughout the past decade.Results Figure 2 shows how correlation varieswith N for the MEN development data.
Thepeak performance for tTest is achieved when usingaround 140 top-ranked contexts per word, whilefor PPMI it is at N  240, and for Freq N  20.The dramatic drop in performance is demonstratedwhen using all three similarity measures, althoughJaccard seems particularly sensitive to the nega-tive tTest weights that are introduced when lower-ranked contexts are added to the vectors.
The re-maining experiments only consider cosine similar-ity.
We also find that context selection improvescorrelation for tTest, PPMI, and the unweightedFreq vectors on the test data (Table 1).
Moreover,the lower the correlation from the full vectors, thelarger the improvement when using CS.3.1 Dimensionality ReductionFigure 3 shows the effects of dimensionality re-duction described in the following experiments.0 1000 2000 3000 4000 5000 6000 7000 8000 9000 100000.580.60.620.640.660.680.70.720.740.76Maximum nonzero elements per vectorSpearmanttestppmifreqmaxmaxmax0 1000 2000 3000 4000 5000 6000 7000 8000 9000 100000.10.20.30.40.50.60.70.8Maximum nonzero elements per vectorSpearmanttestppmifreqmaxmaxmax0 1000 2000 3000 4000 5000 6000 7000 8000 9000 1000000.10.20.30.40.50.60.70.8Maximum nonzero elements per vectorSpearmanttestppmifreqmaxmaxmaxFigure 2: Correlation decreases as more lower-ranked context words are introduced (MENdev),with cosine (top), Lin (bottom left), and Jaccard(bottom right) simialrity measures.3.1.1 SVD and CSTo check whether CS improves the correlationthrough increased sparsity or whether it improvesthe contextual representation of the words, we in-vestigated the behaviour of SVD on three differ-ent levels of vector sparsity.
To construct the mostsparse vectors, we chose the best performing Nfor each weighting scheme (from Table 1).
Thussparse tTest vectors had14010000 0.0140, or 1.4%,non-zero elements.
We also chose a mid-rangeof N  3300 for up to 33% of non-zero ele-ments per vector, and finally the full vectors withN  10000.Results In general the CS-tuned vectors leadto better lower-dimensional representations.
Themid-range contexts in the tTest weighting schemeseem to hold information that hinders SVD, whilethe lowest-ranked negative weights appear to help(when the mid-range contexts are present as well).For the PPMI weighting, fewer contexts consis-tently lead to better representations, while the un-weighted vectors seem to mainly hold informationin the top 20 most frequent contexts for each word.3.1.2 SVD, CS, and NormalisationWe also consider the combination of normalisationand context selection followed by SVD.2330 100 200 300 400 500 600 700 8000.30.40.50.60.70.8Number of dimensions (K)SpearmantTest140N=3300N=10000norm 140norm N=3300norm N=10000all 140all N=3300all N=10000 0 100 200 300 400 500 600 700 8000.10.20.30.40.50.60.70.8Number of dimensions (K)SpearmanPPMI240N=3300N=10000norm 240norm N=3300norm N=10000all 240all N=3300all N=10000 0 100 200 300 400 500 600 700 8000.20.30.40.50.60.7Number of dimensions (K)SpearmanFreq20N=3300N=10000norm 20norm N=3300norm N=10000all 20all N=3300all N=10000Figure 3: Vectors tuned for sparseness (blue) consistently produce equal or better dimensionality reduc-tions (results on MENdev).
The solid lines show improvement in lower dimensional representations ofSVD when dimensionality reduction is applied after normalisation.Results Normalisation leads to more stable SVDrepresentations, with a large improvement forsmall numbers of dimensions (K) as demonstratedby the solid lines in Figure 3.
At K  20 theSpearman correlation increases from 0.61 to 0.71.In addition, for tTest there is an improvement inthe mid-range vectors, and a knock-on effect forthe full vectors.
As the tTest values effectivelyrange from 0.1 to 0.1, the mid-range values arevery small numbers closely grouped around zero.Normalisation spreads and increases these num-bers, perhaps making them more relevant to theSVD algorithm.
The effect is also visible forPPMI weighting where at K  20 the correlationincreases from 0.48 to 0.70.
For PPMI and Freqwe also see that, for the full and mid-range vec-tors, the SVD representations have slightly highercorrelations than the unreduced vectors.3.2 Random IndexingWe use random indexing primarily to produce avector representation for convolution (Section 4).While this produces a lower-dimensional repre-sentation, it may not use less memory since the re-sulting vectors, although smaller, are fully dense.In summary, the RI encoded vectors with di-mensions of D  4096 lead to only slightly re-duced correlation values compared to their unen-coded counterparts.
We find that for tTest weget similar performance with or without CS atany level, while for PPMI CS helps especially forD ?
512.
On Freq we find that CS with N  60leads to higher correlation, but mid-range and fullvectors have equivalent performance.
For Freq,the correlation is equivalent to full vectors fromD  128, while for the weighted vectors 512 di-mensions appear to be sufficient.
Unlike for SVD,normalisation slightly reduces the performance formid-range dimensions.4 Compositional ExperimentsWe examine the performance of vectors aug-mented by CS and normalisation in two compo-sitional tasks.
The first is an extension of the wordsimilarity task to phrase pairs, using the datasetof Mitchell and Lapata (2010).
Each entry in thedataset consists of two phrases, each consisting oftwo words (in various syntactic relations, such asverb-object and adjective noun), and a gold stan-dard score.
We combine the two word vectors intoa single phrase vector using various operators de-scribed below.
We then calculate the similaritybetween the phrase vectors using cosine and com-pare the resulting scores against the gold standardusing Spearman correlation.
The second task isour new definitions task where, again, word vec-tors from each definition are composed to form asingle vector, which can then be compared for sim-ilarity with the target term.We use PPMI- and tTest-weighted vectors atthree CS cutoff points: the best chosen N fromSection 3, the top third of the ranked contexts atN  3300, and the full vectors without CS atN  10000.
This gives us a range of values toexamine, without directly tuning on this dataset.For dimensionality reduction we consider vectorsreduced with SVD to 100 and 700 dimensions.
Insome cases we exclude the results for SVD700be-cause they are very close to the scores for unre-duced vectors.
We experiment with 3 values of Dfrom t512, 1024, 4096u for the RI vectors.Operators To combine distributional vectorsinto a single-vector sentence representation, weuse a representative set of methods from Mitchelland Lapata (2010).
In particular, we use vectoraddition, elementwise (Hadamard) product, Kro-necker product, and circular convolution (Plate,1991; Jones and Mewhort, 2007), which are de-234fined as follows for two word vectors ~x, ~y:Sum ~x  ~y  t~xi  ~yiuiProd ~xd ~y  t~xi ~yiuiKron ~xb ~y  t~xi ~yjuijConv ~xg ~y !
?nj0p~xqj%n p~yqpijq%n)iRepeated application of the Sum operation addscontexts for each of the words that occur in aphrase, which maintains (and mixes) any noisyparts of the component word vectors.
Our inten-tion was that use of the CS vectors would leadto less noisy word vectors and hence less noisyphrase and sentence vectors.
The Prod operator,on the other hand, provides a phrase or sentencerepresentation consisting only of the contexts thatare common to all of the words in the sentence(since zeros in any of the word vectors lead tozeros in the same position in the sentence vec-tor).
This effect is particularly problematic for rarewords which may have sparse vectors, leading toa sparse vector for the sentence.1We address thesparsity problem through the use of dimensional-ity reduction, which produces more dense vectors.Kron, the Kronecker (or tensor) product of twovectors, produces a matrix (second order tensor)whose diagonal matches the result of the Prodoperation, but whose off-diagonal entries are allthe other products of elements of the two vectors.We only apply Kron to SVD-reduced vectors, andto compare two matrices we turn them into vec-tors by concatenating matrix rows, and use co-sine similarity on the resulting vectors.
While inthe more complex, type-driven methods (Baroniand Zamparelli, 2010; Coecke et al., 2010) ten-sors represent functions, and off-diagonal entrieshave a particular transformational interpretation aspart of a linear map, the significance of the off-diagonal elements is difficult to interpret in oursetting, apart from their role as encoders of the or-der of operands.
We only examine Kron as the un-encoded version of the Conv operator to see howthe performance is affected by the random index-ing and the modular summation by which Convdiffers from Kron.2We cannot use Kron for com-bining more than two words as the size of the re-sulting tensor grows exponentially with the num-1Sparsity is a problem that may be addressable throughsmoothing (Zhai and Lafferty, 2001), although we do not in-vestigate that avenue in this paper.2Conv also differs from Kron in that it is commutative,unless one of the operands is permuted.
In this paper we donot permute the operands.Oper N=140 N=3300 N=10000sumttest 0.40 (0.41) 0.40 (0.40) 0.40 (0.40)SVD1000.37 (0.42) 0.35 (0.41) 0.37 (0.40)prodttest 0.32 (0.32) 0.40 (0.40) 0.32 (0.32)SVD1000.25 (0.23) 0.23 (0.23) 0.21 (0.23)kronSVD1000.31 (0.34) 0.34 (0.38) 0.29 (0.32)SVD7000.39 (0.39) 0.37 (0.37) 0.30 (0.30)convRI5120.10 (0.12) 0.26 (0.21) 0.25 (0.25)RI10240.22 (0.15) 0.29 (0.27) 0.25 (0.26)RI40960.16 (0.19) 0.33 (0.34) 0.28 (0.30)Table 2: Behaviour of vector operators with tTestvectors on ML2010 (Spearman correlation).
Val-ues for normalised vectors in parentheses.Oper N=240 N=3300 N=10000sumppmi 0.40 (0.39) 0.40 (0.39) 0.29 (0.29)SVD1000.40 (0.40) 0.38 (0.40) 0.29 (0.30)prodppmi 0.28 (0.28) 0.40 (0.40) 0.30 (0.30)SVD1000.23 (0.17) 0.18 (0.22) 0.14 (0.12)kronSVD1000.37 (0.30) 0.36 (0.38) 0.27 (0.27)SVD7000.38 (0.37) 0.37 (0.37) 0.26 (0.26)convRI5120.09 (0.09) 0.27 (0.30) 0.25 (0.24)RI10240.08 (0.14) 0.33 (0.37) 0.25 (0.27)RI40960.18 (0.19) 0.37 (0.38) 0.27 (0.27)Table 3: Behaviour of vector operators with PPMIvectors on ML2010 (Spearman correlation).
Val-ues for normalised vectors in parentheses.ber of vector operations, but we can use Conv asan encoded alternative as it results in a vector ofthe same dimension as the two operands.4.1 Phrase SimilarityTo test how CS, normalisation, and dimensional-ity reduction affect simple compositional vectoroperations we use the test portion of the phrasalsimilarity dataset from Mitchell and Lapata (2010)(ML2010).
This dataset consists of pairs of two-word phrases and a human similarity judgementon the scale of 1-7.
There are three types ofphrases: noun-noun, adjective-noun, and verb-object.
In the original paper, and some subse-quent works, these were treated as three differentdatasets; however, here we combine the datasetsinto one single phrase pair dataset.
This allows usto summarise the effects of different types of vec-tors on phrasal composition in general.Results Our results (Tables 2 and 3) are compa-rable to those in Mitchell and Lapata (2010) av-eraged across the phrase-types (?
 0.44), butare achieved with much smaller vectors.
We findthat with normalisation, and the optimal choiceof N , there is little difference between Prod andSum.
Sum and Kron benefit from normalisa-tion, especially in combination with SVD, but forProd it either makes no difference or reduces per-formance.
Product-based methods (Prod, Kron,235Conv) have a preference for context selection thatincludes the mid-rank contexts (N  3300), butnot the full vector (N  10000).
On tTest vec-tors Sum is relatively stable across different CSand SVD settings, but with PPMI weighting, thereis a preference for lower N .
SVD reduces perfor-mance for Prod, but not for Kron.
Finally, Convgets higher correlation with higher-dimensional RIvectors and with PPMI weights.4.2 Definition RetrievalIn this task, which is formulated as a retrieval task,we investigate the behaviour of different vectoroperators as multiple operations are chained to-gether.
We first encode each definition into a sin-gle vector through repeated application of one ofthe operators on the distributional vectors of thecontent words in the definition.
Then, for eachhead (defined) word, we rank all the different defi-nition vectors in decreasing order according to in-ner product (unnormalised cosine) similarity withthe head word?s distributional vector.Performance is measured using precision andMean Reciprocal Rank (MRR).
If the correct defi-nition is ranked first, the precision (P@1) is 1, oth-erwise 0.
Since there is only one definition perhead word, the reciprocal rank (RR) is the inverseof the rank of the correct definition.
So if the cor-rect definition is ranked fourth, for example, thenRR is14.
MRR is the average of the RR across allhead words.The difficulty of the task depends on how manywords there are in the dataset and how similar theirdefinitions are.
In addition, if a head word oc-curs in the definition of another word in the samedataset, it may cause the incorrect definition to beranked higher than the correct one.
These prob-lems are more likely to occur with higher fre-quency words and in a larger dataset.
In orderto counter these effects, we average our resultsover ten repeated random samplings of 100 word-definition pairs.
The sampling also gives us a ran-dom baseline for P@1 of 0.01300.0106 and forMRR 0.0576  0.0170, which can be interpretedas there is a chance of slightly more than 1 in 100of ranking the correct definition first, and on aver-age the correct definition is ranked around the 20mark.For this task all experiments were performedusing the tTest-weighted vectors.
When applyingnormalisation we use ?
 1 (Norm) and ?
 10DD2 DD3 DD4 DD5 DD6 DD7 DD8 DD9 DD10346 547 594 537 409 300 216 150 287Table 4: Number of definitions per dataset.(Norm10).
In addition, we examine the effect ofcontinually applying Norm after every operation(CNorm).Dataset We developed a new dataset (DD) con-sisting of 3,386 definitions from the WiktionaryBNC spoken-word frequency list.3Most of thewords have several definitions, but we consideredonly the first definition with at least two non-stopwords.
The word-definition pairs were di-vided into nine separate datasets according to thenumber of non-stopwords in the definition.
For ex-ample, all of the definitions that have five contentwords are in DD5.
The exception is DD10, whichcontains all the definitions of ten or more words.Table 4 shows the number of definitions in eachdataset.Results Figure 4 shows how the MRR varieswith different DD datasets for Sum, Prod, andConv.
The CS, SVD, and RI settings for each op-erator correspond to the best average settings fromTable 5.
In some cases other settings had simi-lar performance, but we chose these for illustrativepurposes.
We can see that all operators have rel-atively higher MRR on smaller datasets (DD6-9).Compensating for that effect, we can hypothesisethat Sum has a steady performance across differ-ent definition sizes, while the performance of bothProd and Conv declines as the number of oper-ations increases.
Normalisation helps with Sumthroughout, with little difference in performancebetween Norm and Norm10, but with a slight de-crease when CNorm is used.
On the other hand,only CNorm improves the ranking of Prod-basedvectors.
Normalisation makes no difference for RIvectors combined with convolution and the resultsin Table 5 show that, on average, Conv performsworse than the random baseline.In Figure 5 we can see that, although dimen-sionality reduction leads to lower MRR, for Sum,normalisation prior to SVD counteracts this effect,while, for Prod, dimensionality reduction, in gen-eral, reduces the performance.3http://simple.wiktionary.org/wiki/Wiktionary:BNC spoken freq236DD2 DD3 DD4 DD5 DD6 DD7 DD8 DD9 DD1000.10.20.30.40.50.60.7MRRSumSum+NormProdProd+NormConvConv+NormDDsize/1000Figure 4: Per-dataset breakdown of best nor-malised and unnormalised vectors for each vectoroperator.
Stars indicate the dataset size from Ta-ble 4 divided by 1000.Sum Prod ConvNorm No Yes No CN No YesCS (N ) 140 140 3300 10000 140 3300SVD(K)/RI(D) 700 700 None None 2048 512mean P@1 0.18 0.23 0.01 0.11 0.00 0.00mean MRR 0.28 0.35 0.06 0.17 0.02 0.02Table 5: Best settings for operators calculatedfrom the highest average MRR across all thedatasets, with and without normalisation.
Theresults for vectors with no normalisation or CSare: Sum - P@1=0.1567, MRR=0.2624; Prod -P@1=0.0147, MRR=0.0542; Conv P@1=0.0027,MRR=0.0192.5 DiscussionIn this paper we introduced context selection andnormalisation as techniques for improving the se-mantic vector space representations of words.
Wefound that, although our untuned vectors performbetter on WS353 data (?
 0.63) than vectors usedby Mitchell and Lapata (2010) (?
 0.42), ourbest phrase composition model (Sum, ?
 0.40)produces a lower performance than an estimate oftheir best model (Prod, ?
 0.44).4This indicatesthat better performance on word-similarity datadoes not directly translate into better performanceon compositional tasks; however, CS and normal-isation are both effective in increasing the qual-ity of the composed representation (?
 0.42).Since CS and normalisation are computationallyinexpensive, they are an excellent way to improvemodel quality compared to the alternative, which4The estimate is computed as an average across the threephrase-type results.DD2 DD3 DD4 DD5 DD6 DD7 DD8 DD9 DD1000.10.20.30.40.50.60.7MRRSum BestSum+SVDSum+SVD+Norm10Prod BestProd+SVDProd+SVD+CNormFigure 5: Per-dataset breakdown of best nor-malised and unnormalised SVD vectors for Sumand Prod.
For both operators the best CS and SVDsettings for normalised vectors were N  140,K  700, and for unnormalised wereN  10000,K  700.is building several models with various contexttypes, in order to find which one suits the data best.Furthermore, we show that, as the number ofvector operations increases, Sum is the most sta-ble operator and that it benefits from sparser rep-resentations (low N ) and normalisation.
Employ-ing both of these methods, we are able to build anSVD-based representation that performs as wellas full-dimensional vectors which, together withSum, give the best results on both phrase and def-inition tasks.
In fact, normalisation and CS bothimprove the SVD representations of the vectorsacross different weighting schemes.
This is a keyresult, as many of the more complex composi-tional methods require low dimensional represen-tations for computational reasons.Future work will include application of CSand normalised lower-dimensional vectors to morecomplex compositional methods, and investiga-tions into whether these strategies apply to othercontext types and other dimensionality reductionmethods such as LDA (Blei et al., 2003).AcknowledgementsTamara Polajnar is supported by ERC StartingGrant DisCoTex (306920).
Stephen Clark is sup-ported by ERC Starting Grant DisCoTex (306920)and EPSRC grant EP/I037512/1.
We would liketo thank Laura Rimell for helpful discussion, andLaura and the anonymous reviewers for helpfulcomments on the paper.237ReferencesJacob Andreas and Zoubin Ghahramani.
2013.
A gen-erative model of vector space semantics.
In Pro-ceedings of the ACL 2013 Workshop on Continu-ous Vector Space Models and their Compositional-ity, Sofia, Bulgaria.M.
Baroni and R. Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InConference on Empirical Methods in Natural Lan-guage Processing (EMNLP-10), Cambridge, MA.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet allocation.
J. Mach.
Learn.Res., 3:993?1022.Elia Bruni, Gemma Boleda, Marco Baroni, andNam Khanh Tran.
2012.
Distributional semanticsin technicolor.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 136?145,Jeju Island, Korea, July.
Association for Computa-tional Linguistics.Daoud Clarke.
2012.
A context-theoretic frame-work for compositionality in distributional seman-tics.
Comput.
Linguist., 38(1):41?71, March.B.
Coecke, M. Sadrzadeh, and S. Clark.
2010.
Math-ematical foundations for a compositional distribu-tional model of meaning.
In J. van Bentham,M.
Moortgat, and W. Buszkowski, editors, Linguis-tic Analysis (Lambek Festschrift), volume 36, pages345?384.James R. Curran.
2004.
From Distributional to Seman-tic Similarity.
Ph.D. thesis, University of Edinburgh.Scott Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Journalof the Society for Information Science, 41(6):391?407.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: Theconcept revisited.
ACM Transactions on Informa-tion Systems, 20:116?131.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using Wikipedia-based explicit semantic analysis.
In Proceedings ofthe 20th international joint conference on Artificalintelligence, IJCAI?07, pages 1606?1611, San Fran-cisco, CA, USA.
Morgan Kaufmann Publishers Inc.Pablo Gamallo and Stefan Bordag.
2011.
Is singu-lar value decomposition useful for word similarityextraction?
Language Resources and Evaluation,45(2):95?119.Michael N. Jones and Douglas J. K. Mewhort.
2007.Representing word meaning and order informationin a composite holographic lexicon.
PsychologicalReview, 114:1?37.Ekaterina Kochmar and Ted Briscoe.
2013.
Capturinganomalies in the choice of content words in compo-sitional distributional semantic space.
In Proceed-ings of the Recent Advances in Natural LanguageProcessing (RANLP-2013), Hissar, Bulgaria.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Nat-ural Language Engineering, 7(3):207?223.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.T.
A.
Plate.
1991.
Holographic reduced Repre-sentations: Convolution algebra for compositionaldistributed representations.
In J. Mylopoulos andR.
Reiter, editors, Proceedings of the 12th Inter-national Joint Conference on Artificial Intelligence,Sydney, Australia, August 1991, pages 30?35, SanMateo, CA.
Morgan Kauffman.D Seung and L Lee.
2001.
Algorithms for non-negative matrix factorization.
Advances in neuralinformation processing systems, 13:556?562.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic Composi-tionality Through Recursive Matrix-Vector Spaces.In Proceedings of the 2012 Conference on Em-pirical Methods in Natural Language Processing(EMNLP), Jeju Island, Korea.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Fabio Massimo Zanzotto and Lorenzo Dell?Arciprete.2011.
Distributed structures and distributionalmeaning.
In Proceedings of the Workshop on Dis-tributional Semantics and Compositionality, DiSCo-11, pages 10?15, Portland, Oregon.
Association forComputational Linguistics.Chengxiang Zhai and John Lafferty.
2001.
A studyof smoothing methods for language models appliedto ad hoc information retrieval.
In Proceedings ofthe 24th annual international ACM SIGIR confer-ence on Research and development in informationretrieval, SIGIR ?01, pages 334?342, New York,NY, USA.
ACM.238
