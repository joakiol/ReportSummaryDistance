Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 32?35,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPNamed Entity Transcription with Pair n-Gram ModelsMartin JanscheGoogle Inc.mjansche@google.comRichard SproatGoogle Inc. and OHSUrws@google.comAbstractWe submitted results for each of the eightshared tasks.
Except for Japanese namekanji restoration, which uses a noisy channelmodel, our Standard Run submissions wereproduced by generative long-range pair n-gram models, which we mostly augmentedwith publicly available data (either fromLDC datasets or mined from Wikipedia) forthe Non-Standard Runs.1 IntroductionThis paper describes the work that we did at Google,Inc.
for the NEWS 2009 Machine TransliterationShared Task (Li et al, 2009b; Li et al, 2009a).
Exceptfor the Japanese kanji task (which we describe be-low), all models were pair n-gram language models.Briefly, we took the training data, and ran an iterativealignment algorithm using a single-state weightedfinite-state transducer (WFST).We then trained a lan-guage model on the input-output pairs of the align-ment, which was then converted into a WFST encod-ing a joint model.
For the Non-Standard runs, we useadditional data fromWikipedia or from the LDC, ex-cept where noted below.
In the few instances wherewe used data not available from Wikipedia or LDC,wewill be happy to share themwith other participantsof this competition.2 KoreanFor Korean, we created a mapping between eachHangul glyph and its phonetic transcription inWorld-Bet (Hieronymus, 1993) based on the tables fromUnitran (Yoon et al, 2007).
Vowel-initial syllableswere augmented with a ?0?
at the beginning of thesyllable, to avoid spurious resyllabifications: Abbottshould be ??
?, never ???.
We also filtered theset of possible Hangul syllable combinations, sincecertain syllables are never used in transliterations, e.g.any with two consonants in the coda.
The mappingbetween Hangul syllables and phonetic transcriptionwas handled with a simple FST.The main transliteration model for the StandardRun was a 10-gram pair language model trained onan alignment of English letters to Korean phonemes.All transliteration pairs observed in the training/development data were cached, and made availableif those names should recur in the test data.
Wealso submitted a Non-Standard Run with English/Korean pairs mined from Wikipedia.
These were de-rived from the titles of corresponding interlinked En-glish and Korean articles.
Obviously not all suchpairs are transliterations, so we filtered the raw listby predicting, for each English word, and using thetrained transliteration model, what the ten most likelytransliterations were in Korean; and then acceptingany pair in Wikipedia where the string in Korean alsooccurred in the set of predicted transliterations.
Thisresulted in 11,169 transliteration pairs.
In addition adictionary of 9,047 English and Korean translitera-tion pairs that we had obtained from another sourcewas added.
These pairs were added to the cache, andwere also used to retrain the transliteration model,along with the provided data.3 Indian LanguagesFor the Indian languages Hindi, Tamil and Kannada,the same basic approach as for Korean was used.
Wecreated a reversible map between Devanagari, Tamilor Kannada symbols and their phonemic values, us-ing a modified version of Unitran.
However, sinceBrahmi-derived scripts distinguish between diacriticand full vowel forms, in order to map back fromphonemic transcription into the script form, it is nec-essary to know whether a vowel comes after a conso-nant or not, in order to select the correct form.
Theseand other constraints were implementedwith a simplehand-constructed WFST for each script.The main transliteration model for the StandardRun was a 6-gram pair language model trained onan alignment of English letters to Hindi, Kannada32or Tamil phonemes in the training and developmentsets.
At test time, this WFST was composed with thephoneme to letter WFST just described to produce aWFST that maps directly between English letters andIndian script forms.
As with Korean, all observedtransliteration pairs from the training/developmentdata were cached, and made available if those namesshould recur in the test data.
For each Indian lan-guage we also submitted a Non-Standard Run whichincluded English/Devanagari, English/Tamil and En-glish/Kannada pairs mined from Wikipedia, and fil-tered as described above for Korean.
This resultedin 11,674 pairs for English/Hindi, 10,957 pairs forEnglish/Tamil and 2,436 pairs for English/Kannada.These pairs were then added to the cache, and werealso used to retrain the transliteration model, alongwith the provided data.4 RussianFor Russian, we computed a direct letter/letter cor-respondences between the Latin representation ofEnglish and the Cyrillic representation of Russianwords.
This seemed to be a reasonable choice sinceRussian orthography is fairly phonemic, at least at anabstract level, and it was doubtful that any gain wouldbe had from trying to model the pronunciation better.We note that many of the examples were, in fact, notEnglish to begin with, but a variety of languages, in-cluding Polish and others, that happen to be writtenin the Latin script.We used a 6-gram pair language model for theStandard Run.
For the Non-Standard Runs we in-cluded: (for NSR1) a list of 3,687 English/Russianpairs mined from the Web; and (for NSR2), those,plus a set of 1,826 mined fromWikipedia and filteredas described above.
In each case, the found pairs wereput in the cache, and were used to retrain the languagemodel.5 ChineseFor Chinese, we built a direct stochastic model be-tween strings of Latin characters representing the En-glish names and strings of hanzi representing theirChinese transcription.
It is well known (Zhang et al,2004) that the direct approach produces significantlybetter transcription quality than indirect approachesbased on intermediate pinyin or phoneme represen-tations.
This observation is consistent with our ownexperience during system development.In our version of the direct approach, we firstaligned the English letter strings with their corre-sponding Chinese hanzi strings using the same mem-oryless monotonic alignment model as before.
Wethen built standard n-gram models over the align-ments, which were then turned, for use at runtime,into weighted FSTs computing a mapping from En-glish to Chinese.The transcription model we chose for the Stan-dard Run is a 6-gram language model over align-ments, built with Kneser-Ney smoothing and a mini-mal amount of Seymore-Rosenfeld shrinking.We submitted two Non-Standard Runs with addi-tional names taken from the LDC Chinese/EnglishName Entity Lists v 1.0 (LDC2005T34).
The only listfrom this collection we used was Propernames Peo-ple EC, which contains 572,213 ?English?
names (infact, names from many languages, all represented inthe Latin alphabet) with one or more Chinese tran-scriptions for each name.
Data of similar quality canbe easily extracted from theWeb as well.
For the sakeof reproducible results, we deliberately chose to workwith a standard corpus.
The LDC name lists haveall of the problems that are usually associated withdata extracted from the Web, including improbableentries, genuine mistakes, character substitutions, avariety of unspecified source languages, etc.We removed names with symbols other than let-ters ?a?
through ?z?
from the list and divided it intoa held-out portion, consisting of names that occur inthe development or test data of the Shared Task, anda training portion, consisting of everything else, for atotal of 622,187 unique English/Chinese name pairs.We then used the model from the Standard Run topredict multiple pronunciations for each of the namesin the training portion of the LDC list and retainedup to 5 pronunciations for each English name wherethe prediction from the Standard model agreed witha pronunciation found in the LDC list.For our first Non-Standard Run, we trained a 7-gram language model based on the Shared Task train-ing data (31,961 name pairs) plus an additional 95,576name pairs from the intersection of the LDC list andthe Standard model predictions.
Since the selectionof additional training data was, by design, very con-servative, we got a small improvement over the Stan-dard Run.The reason for this cautious approach was that theadditional LDC data did not match the provided train-ing and development data very well, partly due tonoise, partly due to different transcription conven-tions.
For example, the Pinyin syllable bo?
is predom-inantly written as?
in the LDC data, but?
does not33occur at all in the Shared Task training data:Character OccurrencesTrain LDC?
0 13,110?
1,547 3,709We normalized the LDC data (towards the tran-scription conventions implicit in the Shared Taskdata) by replacing hanzi for frequent Pinyin syllableswith the predominant homophonous hanzi from theShared Task data.
This resembles a related approachto pronunciation extraction from the web (Ghoshal etal., 2009), where extraction validation and pronunci-ation normalization steps were found to be tremen-dously helpful, even necessary, when using web-derived pronunciations.
One of the conclusions therewas that extracted pronunciations should be used di-rectly when available.This is what we did in our second Non-StandardRun.
We used the filtered and normalized LDC dataas a static dictionary in which to look up the transcrip-tion of names in the test data.
This is how the sharedtask problem would be solved in practice and it re-sulted in a huge gain in quality.
Notice, however, thatdoing so is non-trivial, because of the data quality anddata mismatch problems described above.6 Japanese KatakanaThe ?English?
to Japanese katakana task sufferedfrom the usual problem that the Latin alphabet sidecovered many languages besides English.
It thus be-came an exercise in guessing which one of many validways of pronouncing the Latin letter string would bechosen as the basis for the Japanese transcription.
Wetoyed with the idea of building mixture models beforedeciding that this issue is more appropriate for a pro-nunciation modeling shared task.
In the end, we builtthe same kinds of straightforward pair n-grammodelsas in the tasks described earlier.For Japanese katakana we performed a similarkind of preprocessing as for the Indian languages:since it is possible (under minimal assumptions)to construct an isomorphism between katakana andJapanese phonemes, we chose to use phonemes asthe main level of representation in our model.
Thisis because Latin letters encode phonemes as opposedto syllables or morae (to a first approximation) andone pays a penalty (a loss of about 4% in accuracy onthe development data) for constructingmodels that gofrom Latin letters directly to katakana.For the Standard Run, we built a 5-grammodel thatmaps from Latin letter strings to Japanese phonemestrings.
The model used the same kind of Kneser-Ney smoothing and Seymore-Rosenfeld shrinking asbefore.
In addition, we restrict the model to only pro-duce well-formed Japanese phoneme strings, by com-posing it with an unweighted Japanese phonotacticmodel that enforces the basic syllable structure.7 Japanese Name KanjiIt is important to note that the Japanese name kanjitask is conceptually completely different from all ofthe other tasks.
We argue that this conceptual dif-ference must translate into a different modeling andsystem building approach.The conceptual difference is this: In all other tasks,we?re given well-formed ?English?
names.
For thesake of argument, let?s say that they are indeed justEnglish names.
These names have an English pro-nunciation which is then mapped to a correspond-ing Hindi or Korean pronunciation, and the resultingHindi or Korean ?words?
(which do not look like or-dinary Hindi or Korean words at all, except for su-perficially following the phonology of the target lan-guage) can be written down in Devanagari or Hangul.Information is lost when distinct English sounds getmapped to the same phonemes in the target languageandwhen semantic information (such as the gender ofthe bearer of a name) is simply not transmitted acrossthe phonetic channel that produces the approximationin the target language (transcription into Chinese is anexception in this regard).
We call this forward tran-scription because we?re projecting the original repre-sentation of a name onto an impoverished approxima-tion.In name kanji restoration, we?re moving in the op-posite direction.
The most natural, information-richform of a Japanese name is its kanji representation(ja-Hani).
When this gets transcribed into ro?maji (ja-Latn), only the sound of the name is preserved.
Inthis task, we?re asked to recover the richer kanji formfrom the impoverished ro?maji form.
This is the op-posite of the forward transcription tasks and just begsto be described by a noisy channel model, which isexactly what we did.The noisy channel model is a factored generativemodel that can be thought of as operating by drawingan item (kanji string) from a source model over theuniverse of Japanese names, and then, conditional onthe kanji, generating the observation (ro?maji string)in a noisy, nondeterministic fashion, by drawing it atrandom from a channel model (in this case, basicallya model of kanji readings).To simplify things, we make the natural assump-34tion that there is a latent segmentation of the ro?majistring into segments of one or more syllables andthat each individual kanji in a name generates exactlyone segment.
For illustration, consider the exampleabukawa ?
?, which has three possible segmenta-tions: a+bukawa, abu+kawa, and abuka+wa.
Notethat boundaries can fall into the middle of ambisyl-labic long consonants, as in matto?
?.Complicating this simple picture are several kindsof noise in the training data: First, Chinese pinyinmixed in with Japanese ro?maji, which we removedmostly automatically from the training and develop-ment data and for which we deliberately chose not toproduce guesses in the submitted runs on the test data.Second, the seemingly arbitrary coalescence of cer-tain vowel sequences.
For example, o?numa ??
andonuma??
appear as onuma, and kouda???
andko?da??
appear as koda in the training data.
Severespace limitations prevent us from going into furtherdetails here: we will however discuss the issues dur-ing our presentation at the workshop.For the Standard Run, we built a trigram characterlanguage model on the kanji names (16,182 from thetraining data plus 3,539 from the development data,discarding pinyin names).
We assume a zero-orderchannel model, where each kanji generates its portionof the ro?maji observation independent of its kanji orro?maji context.
We applied an EM algorithm to theparallel ro?maji/kanji data (19,684 items) in order tosegment the ro?maji under the stated assumptions andtrain the channel model.
We pruned the model by re-placing the last EM step with a Viterbi step, result-ing in faster runtime with no loss in quality.
NSR 1uses more than 100k additional names (kanji only,no additional parallel data) extracted from biograph-ical articles in Wikipedia, as well as a list, found onthe Web, of the 10,000 most common Japanese sur-names.
A total of 117,782 names were used to train atrigram source model.
Everything else is identical tothe Standard Run.
NSR 2 is like NSR 1 but adds dic-tionary lookup.
If we find the ro?maji name in a dictio-nary of 27,358 names extracted from Wikipedia andif a corresponding kanji name from the dictionary isamong the top 10 hypotheses produced by the model,that hypothesis is promoted to the top (again, this per-forms better than using the extracted names blindly).NSR 3 is like NSR 1 but the channel model is trainedon a total of 108,172 ro?maji/kanji pairs consisting ofthe training and development data plus data extractedfrom biographies in Wikipedia.
Finally NSR 4 is likeNSR 3 but adds the same kind of dictionary lookup asin NSR 2.
Note that the biggest gains are due first tothe richer source model in NSR 1 and second to thericher channel model in NSR 3.
The improvementsdue to dictionary lookups in NSR 2 and 4 are smallby comparison.8 ResultsResults for the runs are summarized below.
?Rank?is rank in SR/NSR as appropriate:Run ACC F Ranken/ta SR 0.436 0.894 2NSR1 0.437 0.894 5ja-Latn/ SR 0.606 0.749 2ja-Hani NSR1 0.681 0.790 4NSR2 0.703 0.805 3NSR3 0.698 0.805 2NSR4 0.717 0.818 1en/ru SR 0.597 0.925 3NSR1 0.609 0.928 2NSR2 0.955 0.989 1en/zh SR 0.646 0.867 6NSR1 0.658 0.865 10NSR2 0.909 0.960 1en/hi SR 0.415 0.858 9NSR1 0.424 0.862 8en/ko SR 0.476 0.742 1NSR1 0.794 0.894 1en/kn SR 0.370 0.867 2NSR1 0.374 0.868 4en/ja-Kana SR 0.503 0.843 3NSR1 0.564 0.862 n/aAcknowledgmentsThe authors acknowledge the use of the English-Chinese(EnCh) (Li et al, 2004), English-Japanese Katakana (EnJa),English-Korean Hangul (EnKo), Japanese Name (in English)-Japanese Kanji (JnJk) (http://www.cjk.org), and English-Hindi (EnHi), English-Tamil (EnTa), English-Kannada (EnKa),English-Russian (EnRu) (Kumaran and Kellner, 2007) corpora.ReferencesArnab Ghoshal, Martin Jansche, Sanjeev Khudanpur, MichaelRiley, and Morgan E. Ulinksi.
2009.
Web-derived pronunci-ations.
In ICASSP.James L. Hieronymus.
1993.
ASCII phonetic symbols for theworld?s languages: Worldbet.
AT&T Bell Laboratories, tech-nical memorandum.A.
Kumaran and Tobias Kellner.
2007.
A generic framework formachine transliteration.
In SIGIR--30.Haizhou Li, Min Zhang, and Jian Su.
2004.
A joint source chan-nel model for machine transliteration.
In ACL-42.Haizhou Li, A. Kumaran, Vladimir Pervouchine, andMin Zhang.2009a.
Report on NEWS 2009 machine transliteration sharedtask.
In ACL-IJCNLP 2009 Named Entities Workshop, Singa-pore.Haizhou Li, A. Kumaran, Min Zhang, andVladimir Pervouchine.2009b.
Whitepaper of NEWS 2009 machine transliterationshared task.
In ACL-IJCNLP 2009 Named Entities Workshop,Singapore.Su-Youn Yoon, Kyoung-Young Kim, and Richard Sproat.
2007.Multilingual transliteration using feature based phoneticmethod.
In ACL.Min Zhang, Haizhou Li, and Jian Su.
2004.
Direct orthographi-cal mapping for machine transliteration.
In COLING.35
