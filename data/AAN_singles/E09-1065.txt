Proceedings of the 12th Conference of the European Chapter of the ACL, pages 567?575,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsText-to-text Semantic Similarity for Automatic Short Answer GradingMichael Mohler and Rada MihalceaDepartment of Computer ScienceUniversity of North Texasmgm0038@unt.edu, rada@cs.unt.eduAbstractIn this paper, we explore unsupervisedtechniques for the task of automatic shortanswer grading.
We compare a number ofknowledge-based and corpus-based mea-sures of text similarity, evaluate the effectof domain and size on the corpus-basedmeasures, and also introduce a novel tech-nique to improve the performance of thesystem by integrating automatic feedbackfrom the student answers.
Overall, oursystem significantly and consistently out-performs other unsupervised methods forshort answer grading that have been pro-posed in the past.1 IntroductionOne of the most important aspects of the learn-ing process is the assessment of the knowledgeacquired by the learner.
In a typical examinationsetting (e.g., an exam, assignment or quiz), thisassessment implies an instructor or a grader whoprovides students with feedback on their answersto questions that are related to the subject mat-ter.
There are, however, certain scenarios, suchas the large number of worldwide sites with lim-ited teacher availability, or the individual or groupstudy sessions done outside of class, in which aninstructor is not available and yet students need anassessment of their knowledge of the subject.
Inthese instances, we often have to turn to computer-assisted assessment.While some forms of computer-assisted assess-ment do not require sophisticated text understand-ing (e.g., multiple choice or true/false questionscan be easily graded by a system if the correct so-lution is available), there are also student answersthat consist of free text which require an analy-sis of the text in the answer.
Research to date hasconcentrated on two main subtasks of computer-assisted assessment: the grading of essays, whichis done mainly by checking the style, grammati-cality, and coherence of the essay (cf.
(Higginset al, 2004)), and the assessment of short studentanswers (e.g., (Leacock and Chodorow, 2003; Pul-man and Sukkarieh, 2005)), which is the focus ofthis paper.An automatic short answer grading system isone which automatically assigns a grade to an an-swer provided by a student through a comparisonwith one or more correct answers.
It is importantto note that this is different from the related task ofparaphrase detection, since a requirement in stu-dent answer grading is to provide a grade on a cer-tain scale rather than a binary yes/no decision.In this paper, we explore and evaluate a set ofunsupervised techniques for automatic short an-swer grading.
Unlike previous work, which haseither required the availability of manually craftedpatterns (Sukkarieh et al, 2004; Mitchell et al,2002), or large training data sets to bootstrap suchpatterns (Pulman and Sukkarieh, 2005), we at-tempt to devise an unsupervised method that re-quires no human intervention.
We address thegrading problem from a text similarity perspec-tive and examine the usefulness of various text-to-text semantic similarity measures for automati-cally grading short student answers.Specifically, in this paper we seek answers tothe following questions.
First, given a numberof corpus-based and knowledge-based methods aspreviously proposed in the past for word and textsemantic similarity, what are the measures thatwork best for the task of short answer grading?Second, given a corpus-based measure of similar-ity, what is the impact of the domain and the sizeof the corpus on the accuracy of the measure?
Fi-nally, can we use the student answers themselvesto improve the quality of the grading system?2 Related WorkThere are a number of approaches that have beenproposed in the past for automatic short answergrading.
Several state-of-the-art short answergraders (Sukkarieh et al, 2004; Mitchell et al,2002) require manually crafted patterns which, ifmatched, indicate that a question has been an-swered correctly.
If an annotated corpus is avail-567able, these patterns can be supplemented by learn-ing additional patterns semi-automatically.
TheOxford-UCLES system (Sukkarieh et al, 2004)bootstraps patterns by starting with a set of key-words and synonyms and searching through win-dows of a text for new patterns.
A later implemen-tation of the Oxford-UCLES system (Pulman andSukkarieh, 2005) compares several machine learn-ing techniques, including inductive logic program-ming, decision tree learning, and Bayesian learn-ing, to the earlier pattern matching approach withencouraging results.C-Rater (Leacock and Chodorow, 2003)matches the syntactical features of a studentresponse (subject, object, and verb) to that of aset of correct responses.
The method specificallydisregards the bag-of-words approach to takeinto account the difference between ?dog bitesman?
and ?man bites dog?
while trying to detectchanges in voice (?the man was bitten by a dog?
).Another short answer grading system, AutoTu-tor (Wiemer-Hastings et al, 1999), has been de-signed as an immersive tutoring environment witha graphical ?talking head?
and speech recogni-tion to improve the overall experience for students.AutoTutor eschews the pattern-based approach en-tirely in favor of a bag-of-words LSA approach(Landauer and Dumais, 1997).
Later work on Au-toTutor (Wiemer-Hastings et al, 2005; Malatestaet al, 2002) seeks to expand upon the original bag-of-words approach which becomes less useful ascausality and word order become more important.These methods are often supplemented withsome light preprocessing, e.g., spelling correc-tion, punctuation correction, pronoun resolution,lemmatization and tagging.
Likewise, in order tofacilitate their goals of providing feedback to thestudent more robust than a simple ?correct?
or ?in-correct,?
several systems break the gold-standardanswers into constituent concepts that must indi-vidually be matched for the answer to be consid-ered fully correct (Callear et al, 2001).
In this waythe system can determine which parts of an answera student understands and which parts he or she isstruggling with.Automatic short answer grading is closely re-lated to the task of text similarity.
While moregeneral than short answer grading, text similarityis essentially the problem of detecting and com-paring the features of two texts.
One of the earli-est approaches to text similarity is the vector-spacemodel (Salton et al, 1997) with a term frequency/ inverse document frequency (tf.idf) weighting.This model, along with the more sophisticatedLSA semantic alternative (Landauer and Dumais,1997), has been found to work well for tasks suchas information retrieval and text classification.Another approach (Hatzivassiloglou et al,1999) has been to use a machine learning algo-rithm in which features are based on combina-tions of simple features (e.g., a pair of nouns ap-pear within 5 words from one another in bothtexts).
This method also attempts to account forsynonymy, word ordering, text length, and wordclasses.Another line of work attempts to extrapolatetext similarity from the arguably simpler prob-lem of word similarity.
(Mihalcea et al, 2006)explores the efficacy of applying WordNet-basedword-to-word similarity measures (Pedersen et al,2004) to the comparison of texts and found themgenerally comparable to corpus-based measuressuch as LSA.An interesting study has been performed at theUniversity of Adelaide (Lee et al, 2005), compar-ing simpler word and n-gram feature vectors toLSA and exploring the types of vector similaritymetrics (e.g., binary vs. count vectors, Jaccardvs.
cosine vs. overlap distance measure, etc.
).In this case, LSA was shown to perform betterthan the word and n-gram vectors and performedbest at around 100 dimensions with binary vectorsweighted according to an entropy measure, thoughthe difference in measures was often subtle.SELSA (Kanejiya et al, 2003) is a system thatattempts to add context to LSA by supplementingthe feature vectors with some simple syntacticalfeatures, namely the part-of-speech of the previousword.
Their results indicate that SELSA does notperform as well as LSA in the best case, but it hasa wider threshold window than LSA in which thesystem can be used advantageously.Finally, explicit semantic analysis (ESA)(Gabrilovich and Markovitch, 2007) usesWikipedia as a source of knowledge for textsimilarity.
It creates for each text a feature vectorwhere each feature maps to a Wikipedia article.Their preliminary experiments indicated that ESAwas able to significantly outperform LSA on sometext similarity tasks.3 Data SetIn order to evaluate the methods for short answergrading, we have created a data set of questionsfrom introductory computer science assignmentswith answers provided by a class of undergradu-ate students.
The assignments were administeredas part of a Data Structures course at the Univer-sity of North Texas.
For each assignment, the stu-dent answers were collected via the WebCT onlinelearning environment.568The evaluations reported in this paper are car-ried out on the answers submitted for three of theassignments in this class.
Each assignment con-sisted of seven short-answer questions.1 Thirtystudents were enrolled in the class and submittedanswers to these assignments.
Thus, the data setwe work with consists of a total of 630 student an-swers (3 assignments x 7 questions/assignment x30 student answers/question).The answers were independently graded by twohuman judges, using an integer scale from 0 (com-pletely incorrect) to 5 (perfect answer).
Both hu-man judges were graduate computer science stu-dents; one was the teaching assistant in the DataStructures class, while the other is one of the au-thors of this paper.
Table 1 shows two question-answer pairs with three sample student answerseach.
The grades assigned by the two humanjudges are also included.The evaluations are run using Pearson?s corre-lation coefficient measured against the average ofthe human-assigned grades on a per-question anda per-assignment basis.
In the per-question set-ting, every question and the corresponding studentanswer is considered as an independent data pointin the correlation, and thus the emphasis is placedon the correctness of the grade assigned to eachanswer.
In the per-assignment setting, each datapoint is an assignment-student pair created by to-taling the scores given to the student for each ques-tion in the assignment.
In this setting, the em-phasis is placed on the overall grade a student re-ceives for the assignment rather than on the gradereceived for each independent question.The correlation between the two human judgesis measured using both settings.
In the per-question setting, the two annotators correlated at(r=0.6443).
For the per-assignment setting, thecorrelation was (r=0.7228).A deeper look into the scores given by thetwo annotators indicates the underlying subjectiv-ity in grading short answer assignments.
Of the630 grades given, only 358 (56.8%) were exactlyagreed upon by the annotators.
Even more strik-ing, a full 107 grades (17.0%) differed by morethan one point on the five point scale, and 19grades (3.0%) differed by 4 points or more.
21In addition, the assignments had several programmingexercises which have not been considered in any of our ex-periments.2An example should suffice to explain this discrepancy inannotator scoring: Question: What does a function signatureinclude?
Answer: The name of the function and the types ofthe parameters.
Student: input parameters and return type.Scores: 1, 5.
This example suggests that the graders werenot always consistent in comparing student answers to the in-structor answer.
Additionally, the instructor answer may beinsufficient to account for correct student answers, as ?returnFurthermore, on the occasions when the annota-tors disagreed, the same annotator gave the highergrade 79.8% of the time.Over the course of this work, much attentionwas given to our choice of correlation metric.Previous work in text similarity and short-answergrading seems split on the use of Pearson?s andSpearman?s metric.
It was not initially clearthat the underlying assumptions necessary for theproper use of Pearson?s metric (e.g.
normal dis-tribution, interval measurement level, linear cor-relation model) would be met in our experimentalsetup.
We considered both Spearman?s and sev-eral less often used metrics (e.g.
Kendall?s tau,Goodman-Kruskal?s gamma), but in the end, wehave decided to follow previous work using Pear-son?s so that our scores can be more easily com-pared.34 Automatic Short Answer GradingOur experiments are centered around the use ofmeasures of similarity for automatic short answergrading.
In particular, we carry out three setsof experiments, seeking answers to the followingthree research questions.First, what are the measures of semantic sim-ilarity that work best for the task of short an-swer grading?
To answer this question, we runseveral comparative evaluations covering a num-ber of knowledge-based and corpus-based mea-sures of semantic similarity.
While previous workhas considered such comparisons for the relatedtask of paraphrase identification (Mihalcea et al,2006), to our knowledge no comprehensive eval-uation has been carried out for the task of shortanswer grading which includes all the similaritymeasures proposed to date.Second, to what extent do the domain and thesize of the data used to train the corpus-basedmeasures of similarity influence the accuracy ofthe measures?
To address this question, we runa set of experiments which vary the size and do-main of the corpus used to train the LSA and theESA metrics, and we measure their effect on theaccuracy of short answer grading.Finally, given a measure of similarity, can weintegrate the answers with the highest scores andimprove the accuracy of the measure?
We usea technique similar to the pseudo-relevance feed-back method used in information retrieval (Roc-chio, 1971) and augment the correct answer withtype?
does seem to be a valid component of a ?function sig-nature?
according to some literature on the web.3Consider this an open call for discussion in the NLPcommunity regarding the proper usage of correlation metricswith the ultimate goal of consistency within the community.569Sample questions, correct answers, and student answers GradeQuestion: What is the role of a prototype program in problem solving?Correct answer: To simulate the behavior of portions of the desired software product.Student answer 1: A prototype program is used in problem solving to collect data for the problem.
1, 2Student answer 2: It simulates the behavior of portions of the desired software product.
5, 5Student answer 3: To find problem and errors in a program before it is finalized.
2, 2Question: What are the main advantages associated with object-oriented programming?Correct answer: Abstraction and reusability.Student answer 1: They make it easier to reuse and adapt previously written code and they separate complexprograms into smaller, easier to understand classes.
5, 4Student answer 2: Object oriented programming allows programmers to use an object with classes that can bechanged and manipulated while not affecting the entire object at once.
1, 1Student answer 3: Reusable components, Extensibility, Maintainability, it reduces large problems into smallermore manageable problems.
4, 4Table 1: Two sample questions with short answers provided by students and the grades assigned by thetwo human judgesthe student answers receiving the best score ac-cording to a similarity measure.In all the experiments, the evaluations are runon the data set described in the previous section.The results are compared against a simple baselinethat assigns a grade based on a measurement ofthe cosine similarity between the weighted vector-space representations of the correct answer and thecandidate student answer.
The Pearson correla-tion for this model, using an inverse document fre-quency derived from the British National Corpus(BNC), is r=0.3647 for the per-question evaluationand r=0.4897 for the per-assignment evaluation.5 Text-to-text Semantic SimilarityWe run our comparative evaluations using eightknowledge-based measures of semantic similarity(shortest path, Leacock & Chodorow, Lesk, Wu& Palmer, Resnik, Lin, Jiang & Conrath, Hirst &St. Onge), and two corpus-based measures (LSAand ESA).
For the knowledge-based measures, wederive a text-to-text similarity metric by using themethodology proposed in (Mihalcea et al, 2006):for each open-class word in one of the input texts,we use the maximum semantic similarity that canbe obtained by pairing it up with individual open-class words in the second input text.
More for-mally, for each word W of part-of-speech class Cin the instructor answer, we find maxsim(W,C):maxsim(W,C) = maxSIMx(W,wi)where wi is a word in the student answer of classC and the SIMx function is one of the functionsdescribed below.
All the word-to-word similarityscores obtained in this way are summed up andnormalized with the length of the two input texts.We provide below a short description for each ofthese similarity metrics.5.1 Knowledge-Based MeasuresThe shortest path similarity is determined as:Simpath =1length (1)where length is the length of the shortest path be-tween two concepts using node-counting (includ-ing the end nodes).The Leacock & Chodorow (Leacock andChodorow, 1998) similarity is determined as:Simlch = ?
loglength2 ?D (2)where length is the length of the shortest path be-tween two concepts using node-counting, and Dis the maximum depth of the taxonomy.The Lesk similarity of two concepts is defined asa function of the overlap between the correspond-ing definitions, as provided by a dictionary.
It isbased on an algorithm proposed by Lesk (1986) asa solution for word sense disambiguation.The Wu & Palmer (Wu and Palmer, 1994) simi-larity metric measures the depth of two given con-cepts in the WordNet taxonomy, and the depth ofthe least common subsumer (LCS), and combinesthese figures into a similarity score:Simwup =2 ?
depth(LCS)depth(concept1) + depth(concept2)(3)The measure introduced by Resnik (Resnik, 1995)returns the information content (IC) of the LCS oftwo concepts:Simres = IC(LCS) (4)where IC is defined as:IC(c) = ?
logP (c) (5)and P (c) is the probability of encountering an in-stance of concept c in a large corpus.570The measure introduced by Lin (Lin, 1998) buildson Resnik?s measure of similarity, and adds anormalization factor consisting of the informationcontent of the two input concepts:Simlin =2 ?
IC(LCS)IC(concept1) + IC(concept2)(6)We also consider the Jiang & Conrath (Jiang andConrath, 1997) measure of similarity:Simjnc =1IC(concept1) + IC(concept2)?
2 ?
IC(LCS)(7)Finally, we consider the Hirst & St. Onge (Hirstand St-Onge, 1998) measure of similarity, whichdetermines the similarity strength of a pair ofsynsets by detecting lexical chains between thepair in a text using the WordNet hierarchy.5.2 Corpus-Based MeasuresCorpus-based measures differ from knowledge-based methods in that they do not require any en-coded understanding of either the vocabulary orthe grammar of a text?s language.
In many ofthe scenarios where CAA would be advantageous,robust language-specific resources (e.g.
Word-Net) may not be available.
Thus, state-of-the-artcorpus-based measures may be the only availableapproach to CAA in languages with scarce re-sources.One corpus-based measure of semantic similar-ity is latent semantic analysis (LSA) proposed byLandauer (Landauer and Dumais, 1997).
In LSA,term co-occurrences in a corpus are captured bymeans of a dimensionality reduction operated by asingular value decomposition (SVD) on the term-by-document matrix T representing the corpus.For the experiments reported in this section, werun the SVD operation on several corpora includ-ing the BNC (LSA BNC) and the entire EnglishWikipedia (LSA Wikipedia).4Explicit semantic analysis (ESA) (Gabrilovichand Markovitch, 2007) is a variation on the stan-dard vectorial model in which the dimensions ofthe vector are directly equivalent to abstract con-cepts.
Each article in Wikipedia represents a con-cept in the ESA vector.
The relatedness of a termto a concept is defined as the tf*idf score for theterm within the Wikipedia article, and the related-ness between two words is the cosine of the twoconcept vectors in a high-dimensional space.
Werefer to this method as ESA Wikipedia.4Throughout this paper, the references to the Wikipediacorpus refer to a version downloaded in September 2007.5.3 ImplementationFor the knowledge-based measures, we use theWordNet-based implementation of the word-to-word similarity metrics, as available in the Word-Net::Similarity package (Patwardhan et al, 2003).For latent semantic analysis, we use the InfoMappackage.5 For ESA, we use our own imple-mentation of the ESA algorithm as described in(Gabrilovich and Markovitch, 2006).
Note thatall the word similarity measures are normalized sothat they fall within a 0?1 range.
The normaliza-tion is done by dividing the similarity score pro-vided by a given measure with the maximum pos-sible score for that measure.Table 2 shows the results obtained with each ofthese measures on our evaluation data set.Measure CorrelationKnowledge-based measuresShortest path 0.4413Leacock & Chodorow 0.2231Lesk 0.3630Wu & Palmer 0.3366Resnik 0.2520Lin 0.3916Jiang & Conrath 0.4499Hirst & St-Onge 0.1961Corpus-based measuresLSA BNC 0.4071LSA Wikipedia 0.4286ESA Wikipedia 0.4681Baselinetf*idf 0.3647Table 2: Comparison of knowledge-based andcorpus-based measures of similarity for short an-swer grading6 The Role of Domain and SizeOne of the key considerations when applyingcorpus-based techniques is the extent to which sizeand subject matter affect the overall performanceof the system.
In particular, based on the underly-ing processes involved, the LSA and ESA corpus-based methods are expected to be especially sen-sitive to changes in domain and size.
Building thelanguage models depends on the relatedness of thewords in the training data which suggests that, forinstance, in a computer science domain the terms?object?
and ?oriented?
will be more closely re-lated than in a more general text.
Similarly, a largeamount of training data will lead to less sparse5http://infomap-nlp.sourceforge.net/571vector spaces, which in turn is expected to affectthe performance of the corpus-based methods.With this in mind, we developed two trainingcorpora for use with the corpus-based measuresthat covered the computer science domain.
Thefirst corpus (LSA slides) consists of several onlinelecture notes associated with the class textbook,specifically covering topics that are used as ques-tions in our sample.
The second domain-specificcorpus is a subset of Wikipedia (LSA WikipediaCS) consisting of articles that contain any of thefollowing words: computer, computing, computa-tion, algorithm, recursive, or recursion.The performance on the domain-specific cor-pora is compared with the one observed on theopen-domain corpora mentioned in the previ-ous section, namely LSA Wikipedia and ESAWikipedia.
In addition, for the purpose of runninga comparison with the LSA slides corpus, we alsocreated a random subset of the LSA Wikipediacorpus approximately matching the size of theLSA slides corpus.
We refer to this corpus as LSAWikipedia (small).Table 3 shows an overview of the various cor-pora used in the experiments, along with the Pear-son correlation observed on our data set.Measure - Corpus Size CorrelationTraining on generic corporaLSA BNC 566.7MB 0.4071LSA Wikipedia 1.8GB 0.4286LSA Wikipedia (small) 0.3MB 0.3518ESA Wikipedia 1.8GB 0.4681Training on domain-specific corporaLSA Wikipedia CS 77.1MB 0.4628LSA slides 0.3MB 0.4146ESA Wikipedia CS 77.1MB 0.4385Table 3: Corpus-based measures trained on cor-pora from different domains and of different sizesAssuming a corpus of comparable size, we ex-pect a measure trained on a domain-specific cor-pus to outperform one that relies on a generic one.Indeed, by comparing the results obtained withLSA slides to those obtained with LSA Wikipedia(small), we see that by using the in-domain com-puter science slides we obtain a correlation ofr=0.4146, which is higher than the correlationof r=0.3518 obtained with a corpus of the samesize but open-domain.
The effect of the domainis even more pronounced when we compare theperformance obtained with LSA Wikipedia CS(r=0.4628) with the one obtained with the full LSAWikipedia (r=0.4286).6 The smaller, domain-6The difference was found significant using a paired t-testspecific corpus performs better, despite the factthat the generic corpus is 23 times larger and is asuperset of the smaller corpus.
This suggests thatfor LSA the quality of the texts is vastly more im-portant than their quantity.When using the domain-specific subset ofWikipedia, we observe decreased performancewith ESA compared to the full Wikipedia space.We suggest that for ESA the high-dimensionalityof the concept space7 is paramount, since many re-lations between generic words may be lost to ESAthat can be detected latently using LSA.In tandem with our exploration of the effectsof domain-specific data, we also look at the effectof size on the overall performance.
The main in-tuitive trends are there, i.e., the performance ob-tained with the large LSA-Wikipedia is better thanthe one that can be obtained with LSA Wikipedia(small).
Similarly, in the domain-specific space,the LSA Wikipedia CS corpus leads to better per-formance than the smaller LSA slides data set.However, an analysis carried out at a finer grainedscale, in which we calculate the performance ob-tained with LSA when trained on 5%, 10%, ...,100% fractions of the full LSA Wikipedia corpus,does not reveal a close correlation between sizeand performance, which suggests that further anal-ysis is needed to determine the exact effect of cor-pus size on performance.7 Relevance Feedback based on StudentAnswersThe automatic grading of student answers im-plies a measure of similarity between the answersprovided by the students and the correct answerprovided by the instructor.
Since we only haveone correct answer, some student answers may bewrongly graded because of little or no similaritywith the correct answer that we have.To address this problem, we introduce a noveltechnique that feeds back from the student an-swers themselves in a way similar to the pseudo-relevance feedback used in information retrieval(Rocchio, 1971).
In this way, the paraphrasing thatis usually observed across student answers will en-hance the vocabulary of the correct answer, whileat the same time maintaining the correctness of thegold-standard answer.Briefly, given a metric that provides similarityscores between the student answers and the cor-rect answer, scores are ranked from most similar(p<0.001).7In ESA, all the articles in Wikipedia are used as dimen-sions, which leads to about 1.75 million dimensions in theESA Wikipedia corpus, compared to only 55,000 dimensionsin the ESA Wikipedia CS corpus.572to least.
The words of the top N ranked answersare then added to the gold standard answer.
Theremaining answers are then rescored according thethe new gold standard vector.
In practice, we holdthe scores from the first run (i.e., with no feed-back) constant for the top N highest-scoring an-swers, and the second-run scores for the remaininganswers are multiplied by the first-run score of theNth highest-scoring answer.
In this way, we keepthe original scores for the top N highest-scoringanswers (and thus prevent them from becoming ar-tificially high), and at the same time, we guaranteethat none of the lower-scored answers will get anew score higher than the best answers.The effects of relevance feedback are shown inFigure 9, which plots the Pearson correlation be-tween automatic and human grading (Y axis) ver-sus the number of student answers that are usedfor relevance feedback (X axis).Overall, an improvement of up to 0.047 onthe 0-1 Pearson scale can be obtained by usingthis technique, with a maximum improvement ob-served after about 4-6 iterations on average.
Af-ter an initial number of high-scored answers, it islikely that the correctness of the answers degrades,and thus the decrease in performance observed af-ter an initial number of iterations.
Our results in-dicate that the LSA and WordNet similarity met-rics respond more favorably to feedback than theESA metric.
It is possible that supplementing thebag-of-words in ESA (with e.g.
synonyms andphrasal differences) does not drastically alter theresultant concept vector, and thus the overall ef-fect is smaller.8 DiscussionOur experiments show that several knowledge-based and corpus-based measures of similarityperform comparably when used for the task ofshort answer grading.
However, since the corpus-based measures can be improved by account-ing for domain and corpus size, the highest per-formance can be obtained with a corpus-basedmeasure (LSA) trained on a domain-specific cor-pus.
Further improvements were also obtainedby integrating the highest-scored student answersthrough a relevance feedback technique.Table 4 summarizes the results of our experi-ments.
In addition to the per-question evaluationsthat were reported throughout the paper, we alsoreport the per-assignment evaluation, which re-flects a cumulative score for a student on a singleassignment, as described in Section 3.Overall, in both the per-question and per-assignment evaluations, we obtained the best per-formance by using an LSA measure trained onCorrelationMeasure per-quest.
per-assign.Baselinestf*idf 0.3647 0.4897LSA BNC 0.4071 0.6465Relevance Feedback based on Student AnswersWordNet shortest path 0.4887 0.6344LSA Wikipedia CS 0.5099 0.6735ESA Wikipedia full 0.4893 0.6498Annotator agreement 0.6443 0.7228Table 4: Summary of results obtained with vari-ous similarity measures, with relevance feedbackbased on six student answers.
We also list thetf*idf and the LSA trained on BNC baselines (nofeedback), as well as the annotator agreement up-per bound.a medium size domain-specific corpus obtainedfrom Wikipedia, with relevance feedback fromthe four highest-scoring student answers.
Thismethod improves significantly over the tf*idfbaseline and also over the LSA trained on BNCmodel, which has been used extensively in previ-ous work.
The differences were found to be sig-nificant using a paired t-test (p<0.001).To gain further insights, we made an additionalanalysis where we determined the ability of oursystem to make a binary accept/reject decision.
Inthis evaluation, we map the 0-5 human grading ofthe data set to an accept/reject annotation by us-ing a threshold of 2.5.
Every answer with a gradehigher than 2.5 is labeled as ?accept,?
while ev-ery answer below 2.5 is labeled as ?reject.?
Next,we use our best system (LSA trained on domain-specific data with relevance feedback), and run aten-fold cross-validation on the data set.
Specif-ically, for each fold, the system uses the remain-ing nine folds to automatically identify a thresh-old to maximize the matching with the gold stan-dard.
The threshold identified in this way is usedto automatically annotate the test fold with ?ac-cept?/?reject?
labels.
The ten-fold cross validationresulted in an accuracy of 92%, indicating the abil-ity of the system to automatically make a binaryaccept/reject decision.9 ConclusionsIn this paper, we explored unsupervised tech-niques for automatic short answer grading.We believe the paper made three important con-tributions.
First, while there are a number of wordand text similarity measures that have been pro-posed in the past, to our knowledge no previ-ous work has considered a comprehensive evalu-5730.350.40.450.50.550  5  10  15  20CorrelationNumber of student answers used for feedbackLSA-Wiki-fullLSA-Wiki-CSLSA-slides-CSESA-Wiki-fullESA-Wiki-CSWN-JCNWN-PATHTF*IDFLSA-BNCFigure 1: Effect of relevance feedback on performanceation of all the measures for the task of short an-swer grading.
We filled this gap by running com-parative evaluations of several knowledge-basedand corpus-based measures on a data set of shortstudent answers.
Our results indicate that whenused in their original form, the results obtainedwith the best knowledge-based (WordNet short-est path and Jiang & Conrath) and corpus-basedmeasures (LSA and ESA) have comparable per-formance.
The benefit of the corpus-based ap-proaches over knowledge-based approaches lies intheir language independence and the relative easein creating a large domain-sensitive corpus versusa language knowledge base (e.g., WordNet).Second, we analysed the effect of domain andcorpus size on the effectiveness of the corpus-based measures.
We found that significant im-provements can be obtained for the LSA measurewhen using a medium size domain-specific corpusbuilt from Wikipedia.
In fact, when using LSA,our results indicate that the corpus domain may besignificantly more important than corpus size oncea certain threshold size has been reached.Finally, we introduced a novel technique for in-tegrating feedback from the student answers them-selves into the grading system.
Using a methodsimilar to the pseudo-relevance feedback tech-nique used in information retrieval, we were ableto improve the quality of our system by a few per-centage points.Overall, our best system consists of an LSAmeasure trained on a domain-specific corpus builton Wikipedia with feedback from student answers,which was found to bring a significant absoluteimprovement on the 0-1 Pearson scale of 0.14 overthe tf*idf baseline and 0.10 over the LSA BNCmodel that has been used in the past.In future work, we intend to expand our analy-sis of both the gold-standard answer and the stu-dent answers beyond the bag-of-words paradigmby considering basic logical features in the text(i.e., AND, OR, NOT) as well as the existenceof shallow grammatical features such as predicate-argument structure(Moschitti et al, 2007) as wellas semantic classes for words.
Furthermore, it maybe advantageous to expand upon the existing mea-sures by applying machine learning techniques tocreate a hybrid decision system that would exploitthe advantages of each measure.The data set introduced in this paper, along withthe human-assigned grades, can be downloadedfrom http://lit.csci.unt.edu/index.php/Downloads.AcknowledgmentsThis work was partially supported by a NationalScience Foundation CAREER award #0747340.The authors are grateful to Samer Hassan for mak-ing available his implementation of the ESA algo-rithm.ReferencesD.
Callear, J. Jerrams-Smith, and V. Soh.
2001.CAA of Short Non-MCQ Answers.
Proceedings of574the 5th International Computer Assisted Assessmentconference.E.
Gabrilovich and S. Markovitch.
2006.
Overcomingthe brittleness bottleneck using Wikipedia: Enhanc-ing text categorization with encyclopedic knowl-edge.
In Proceedings of the National Conference onArtificial Intelligence (AAAI), Boston.E.
Gabrilovich and S. Markovitch.
2007.
ComputingSemantic Relatedness using Wikipedia-based Ex-plicit Semantic Analysis.
Proceedings of the 20thInternational Joint Conference on Artificial Intelli-gence, pages 6?12.V.
Hatzivassiloglou, J. Klavans, and E. Eskin.
1999.Detecting text similarity over short passages: Ex-ploring linguistic feature combinations via machinelearning.
Proceedings of the Joint SIGDAT Con-ference on Empirical Methods in Natural LanguageProcessing and Very Large Corpora.D.
Higgins, J. Burstein, D. Marcu, and C. Gentile.2004.
Evaluating multiple aspects of coherence instudent essays.
In Proceedings of the annual meet-ing of the North American Chapter of the Associa-tion for Computational Linguistics, Boston, MA.G.
Hirst and D. St-Onge, 1998.
Lexical chains as rep-resentations of contexts for the detection and correc-tion of malaproprisms.
The MIT Press.J.
Jiang and D. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
InProceedings of the International Conference on Re-search in Computational Linguistics, Taiwan.D.
Kanejiya, A. Kumar, and S. Prasad.
2003.
Au-tomatic evaluation of students?
answers using syn-tactically enhanced LSA.
Proceedings of the HLT-NAACL 03 workshop on Building educational appli-cations using natural language processing-Volume2, pages 53?60.T.K.
Landauer and S.T.
Dumais.
1997.
A solution toplato?s problem: The latent semantic analysis the-ory of acquisition, induction, and representation ofknowledge.
Psychological Review, 104.C.
Leacock and M. Chodorow.
1998.
Combining lo-cal context and WordNet sense similarity for wordsense identification.
In WordNet, An Electronic Lex-ical Database.
The MIT Press.C.
Leacock and M. Chodorow.
2003.
C-rater: Au-tomated Scoring of Short-Answer Questions.
Com-puters and the Humanities, 37(4):389?405.M.D.
Lee, B. Pincombe, and M. Welsh.
2005.
An em-pirical evaluation of models of text document simi-larity.
Proceedings of the 27th Annual Conferenceof the Cognitive Science Society, pages 1254?1259.M.E.
Lesk.
1986.
Automatic sense disambiguation us-ing machine readable dictionaries: How to tell a pinecone from an ice cream cone.
In Proceedings of theSIGDOC Conference 1986, Toronto, June.D.
Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the 15th InternationalConference on Machine Learning, Madison, WI.K.I.
Malatesta, P. Wiemer-Hastings, and J. Robertson.2002.
Beyond the Short Answer Question with Re-search Methods Tutor.
In Proceedings of the Intelli-gent Tutoring Systems Conference.R.
Mihalcea, C. Corley, and C. Strapparava.
2006.Corpus-based and knowledge-based approaches totext semantic similarity.
In Proceedings of theAmerican Association for Artificial Intelligence(AAAI 2006), Boston.T.
Mitchell, T. Russell, P. Broomhead, and N. Aldridge.2002.
Towards robust computerised marking offree-text responses.
Proceedings of the 6th Interna-tional Computer Assisted Assessment (CAA) Confer-ence.Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
Exploitingsyntactic and shallow semantic kernels for ques-tion/answer classification.
In Proceedings of the45th Conference of the Association for Computa-tional Linguistics.S.
Patwardhan, S. Banerjee, and T. Pedersen.
2003.Using measures of semantic relatedness for wordsense disambiguation.
In Proceedings of the FourthInternational Conference on Intelligent Text Pro-cessing and Computational Linguistics, MexicoCity, February.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.WordNet:: Similarity-Measuring the Relatedness ofConcepts.
Proceedings of the National Conferenceon Artificial Intelligence, pages 1024?1025.S.G.
Pulman and J.Z.
Sukkarieh.
2005.
AutomaticShort Answer Marking.
ACL WS Bldg Ed Apps us-ing NLP.P.
Resnik.
1995.
Using information content to evalu-ate semantic similarity.
In Proceedings of the 14thInternational Joint Conference on Artificial Intelli-gence, Montreal, Canada.J.
Rocchio, 1971.
Relevance feedback in informationretrieval.
Prentice Hall, Ing.
Englewood Cliffs, NewJersey.G.
Salton, A. Wong, and C.S.
Yang.
1997.
A vec-tor space model for automatic indexing.
In Read-ings in Information Retrieval, pages 273?280.
Mor-gan Kaufmann Publishers, San Francisco, CA.J.Z.
Sukkarieh, S.G. Pulman, and N. Raikes.
2004.Auto-Marking 2: An Update on the UCLES-OxfordUniversity research into using Computational Lin-guistics to Score Short, Free Text Responses.
In-ternational Association of Educational Assessment,Philadephia.P.
Wiemer-Hastings, K. Wiemer-Hastings, andA.
Graesser.
1999.
Improving an intelligent tutor?scomprehension of students with Latent SemanticAnalysis.
Artificial Intelligence in Education, pages535?542.P.
Wiemer-Hastings, E. Arnott, and D. Allbritton.2005.
Initial results and mixed directions for re-search methods tutor.
In AIED2005 - SupplementaryProceedings of the 12th International Conference onArtificial Intelligence in Education, Amsterdam.Z.
Wu and M. Palmer.
1994.
Verb semantics and lex-ical selection.
In Proceedings of the 32nd AnnualMeeting of the Association for Computational Lin-guistics, Las Cruces, New Mexico.575
