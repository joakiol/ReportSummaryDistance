Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 204?209,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsNoise-aware Character Alignment for Bootstrapping Statistical MachineTransliteration from Bilingual CorporaKatsuhito Sudoh??
Shinsuke Mori?
Masaaki Nagata?
?NTT Communication Science Laboratories?Graduate School of Informatics, Kyoto University?Academic Center for Computing and Media Studies, Kyoto Universitysudoh.katsuhito@lab.ntt.co.jpAbstractThis paper proposes a novel noise-aware char-acter alignment method for bootstrapping sta-tistical machine transliteration from automat-ically extracted phrase pairs.
The model isan extension of a Bayesian many-to-manyalignment method for distinguishing non-transliteration (noise) parts in phrase pairs.
Itworked effectively in the experiments of boot-strapping Japanese-to-English statistical ma-chine transliteration in patent domain usingpatent bilingual corpora.1 IntroductionTransliteration is used for providing translations forsource language words that have no appropriatecounterparts in target language, such as some tech-nical terms and named entities.
Statistical machinetransliteration (Knight and Graehl, 1998) is a tech-nology to solve it in a statistical manner.
Bilin-gual dictionaries can be used to train its model, butmany of their entries are actually translation but nottransliteration.
Such non-transliteration pairs hurtthe transliteration model and should be eliminatedbeforehand.Sajjad et al(2012) proposed a method to iden-tify such non-transliteration pairs, and applied itsuccessfully to noisy word pairs obtained from au-tomatic word alignment on bilingual corpora.
Itenables the statistical machine transliteration to bebootstrapped from bilingual corpora.
This approachis beneficial because it does not require carefully-developed bilingual transliteration dictionaries andit can learn domain-specific transliteration patternsfrom bilingual corpora in the target domain.
How-ever, their transliteration mining approach is sample-wise; that is, it makes a decision whether a bilingualphrase pair is transliteration or not.
Suppose thata compound word in a language A is transliteratedinto two words in another language B.
Their corre-spondence may not be fully identified by automaticword alignment and a wrong alignment between thecompound word in A and only one component wordin B is found.
The sample-wise mining cannot makea correct decision of partial transliteration on thealigned candidate, and may introduces noise to thestatistical transliteration model.This paper proposes a novel transliteration miningmethod for such partial transliterations.
The methoduses a noise-aware character alignment model thatdistinguish non-transliteration (noise) parts fromtransliteration (signal) parts.
The model is an ex-tension of a Bayesian alignment model (Finch andSumita, 2010) and can be trained by a sampling al-gorithm extended for a constraint on noise.
Ourexperiments of Japanese-to-English transliterationachieved 16% relative error reduction in transliter-ation accuracy from the sample-wise method.
Themain contribution of this paper is two-fold:?
we formulate alignment over string pairs withpartial noise and present a solution with anoise-aware alignment model;?
we proved its effectiveness by experimentswith frequent unknown words in actualJapanese-to-English patent translation data.2042 Bayesian many-to-many alignmentWe briefly review a Bayesian many-to-many charac-ter alignment proposed by Finch and Sumita (2010)on which our model is based.
The model is basedon a generative process of bilingual substring pairs?s?, t??
by the following Dirichlet process (DP):G|?,G0 ?
DP(?,G0)?s?, t?
?|G ?
G,where G is a probability distribution over substringpairs according to a DP prior with base measure G0and hyperparameter ?.
G0 is modeled as a jointspelling model as follows:G0 (?s?, t??)
=?|s?|s|s?|!
e?
?sv?|s?|s ??|t?|t|?t|!
e?
?tv?|t?|t .
(1)This is a simple joint probability of the spellingmodels, in which each alphabet appears based ona uniform distribution over the vocabulary (of sizevs and vt) and each string length follows a Poissondistribution (with the average length ?s and ?t).The model handles infinite number of substringpairs according to the Chinese Restaurant Process(CRP).
The probability of a substring pair ?s?k, t?k?is based on the counts of all other substring pairs asfollows:p(?s?k, t?k?| {?s?, t??
}?k)= N (?s?k, t?k?)
+ ?G0 (?s?k, t?k?
)?i N (?s?i, t?i?)
+ ?.
(2)Here {?s?, t??
}?k means a set of substring pairs ex-cluding ?s?k, t?k?, and N (?s?k, t?k?)
is the number of?s?k, t?k?
in the current sample space.
This align-ment model is suitable for representing very sparsedistribution over arbitrary substring pairs, thanks toreasonable CRP-based smoothing for unseen pairsbased on the spelling model.3 Proposed methodWe propose an extended many-to-many alignmentmodel that can handle partial noise.
We extend themodel in the previous section by introducing a noisesymbol and state-based probability calculation.?
?k e y(a) no noise?
?f l ynoisenoise(b) noise?
?g i v?
?
?noisee(c) partial noise: Englishside should be ?give up??
?k e y?
?g i v?
?
?noiseer e c?
?
?noiseo v e r(d) partial noise: Japanese sideshould be ?????
?Figure 1: Three types of noise in transliteration data.Solid lines are correct many-to-many alignment links.3.1 Partial noise in transliteration dataFigure 1 shows transliteration examples with ?nonoise,?
?noise,?
and ?partial noise.?
Solid lines in thefigure show correct many-to-many alignment links.The examples (a) and (b) can be distinguished ef-fectively by Sajjad et al(2012).
We aim to do align-ment as in the examples (c) and (d) by distinguishingits non-transliteration (noise) part, which cannot behandled by the existing methods.3.2 Noise-aware alignment modelWe introduce a noise symbol to handle partial noisein the many-to-many alignment model.
Htun et al(2012) extended the many-to-many alignment forthe sample-wise transliteration mining, but its noisemodel only handles the sample-wise noise and can-not distinguish partial noise.
We model partial noisein the CRP-based joint substring model.Partial noise in transliteration data typically ap-pears in compound words as mentioned earlier, be-cause their counterparts consisting of two or morewords may not be fully covered in automatically ex-tracted words and phrases as shown in Figure 1(c).Another type of partial noise is derived from mor-phological differences due to inflection, which usu-ally appear in the sub-word level as prefixes and suf-fixes as shown in Figure 1(d).
According to thisintuition, we assume that partial noise appears inthe beginning and/or end of transliteration data (incase of sample-wise noise, we assume the noise is inthe beginning).
This assumption derives a constraintbetween signal and noise parts that helps to avoida welter of transliteration and non-transliterationparts.
It also has a shortcoming that it is generally205?
?t h e?
?
?sp?
?
?spe t c h i n g sp m a s k snoise noise noiseFigure 2: Example of many-to-many alignment with par-tial noise in the beginning and end.
?noise?
stands for thenoise symbol and ?sp?
stands for a white space.not appropriate for noise in the middle, but handlingarbitrary number of noise parts increases computa-tional complexity and sparseness.
We rely on thissimple assumption in this paper and consider a morecomplex mid-noise problem as future work.Figure 2 shows a partial noise example in boththe beginning and end.
This example is actuallycorrect translation but includes noise in a sense oftransliteration; an article ?the?
is wrongly includedin the phrase pair (no articles are used in Japanese)and a plural noun ?masks?
is transliterated into?????(mask).
These non-transliteration parts arealigned to noise symbols in the proposed model.
Thenoise symbols are treated as zero-length substringsin the model, same as other substrings.3.3 Constrained Gibbs samplingFinch and Sumita (2010) used a blocked Gibbs sam-pling algorithm with forward-filtering backward-sampling (FFBS) (Mochihashi et al 2009).
We ex-tend their algorithm for our noise-aware model us-ing a state-based calculation over the three states:non-transliteration part in the beginning (noiseB),transliteration part (signal), non-transliteration partin the end (noiseE).Figure 3 illustrates our FFBS steps.
At first inthe forward filtering, we begin with transition tonoiseB and signal.
The calculation of forwardprobabilities itself is almost the same as Finch andSumita (2010) except for state transition constraints:from noiseB to signal, from signal to noiseE.
Thebackward-sampling traverses a path by probability-based sampling with true posteriors, starting fromthe choice of the ending state among noiseB (meansfull noise), signal, and noiseE.
This algorithm in-creases the computational cost by three times to con-sider three different states, compared to that of Finchand Sumita (2010).noiseBsignalnoiseEnoiseBsignalnoiseEsssssstttttt(a) Forward filteringnoiseBsignalnoiseEnoiseBsignalnoiseEsssssstttttt(b) Backward samplingFigure 3: State-based FFBS for the proposed model.4 ExperimentsWe conducted experiments comparing the pro-posed method with the conventional sample-wisemethod for the use in bootstrapping statisticalmachine transliteration using Japanese-to-Englishpatent translation dataset (Goto et al 2013).4.1 Training data setupFirst, we trained a phrase table on the 3.2M paral-lel sentences by a standard training procedure usingMoses, with Japanese tokenization using MeCab1.We obtained 591,840 phrase table entries whoseJapanese side was written in katakana (Japanesephonogram) only2.
Then, we iteratively ran themethod of Sajjad et al(2012) on these entries andeliminate non-transliteration pairs, until the num-ber of pairs converged.
Finally we obtain 104,563katakana-English pairs after 10 iterations; they wereour baseline training set mined by sample-wisemethod.
We used Sajjad et als method as pre-processing for filtering sample-wise noise while theproposed method could also do that, because theproposed method took much more training time forall phrase table entries.4.2 Transliteration experimentsThe transliteration experiment used a translation-based implementation with Moses, using a1http://code.google.com/p/mecab/2This katakana-based filtering is a language dependentheuristic for choosing potential transliteration candidate, be-cause transliterations in Japanese are usually written inkatakana.206character-based 7-gram language model trained on300M English patent sentences.
We compared threetransliteration models below.The test set was top-1000 unknown (in theJapanese-to-English translation model) katakanawords appeared in 400M Japanese patent sentences.They covered 15.5% of all unknown katakanawordsand 8.8% of all unknown words (excluding num-bers); that is, more than a half of unknown wordswere katakana words.4.2.1 Sample-wise method (BASELINE)We used the baseline training set to train sta-tistical machine transliteration model for our base-line.
The training procedure was based on Moses:MGIZA++ word alignment, grow-diag-final-andalignment symmetrization and phrase extractionwith the maximum phrase length of 7.4.2.2 Proposed method (PROPOSED)We applied the proposed method to the baselinetraining set with 30 sampling iterations and elimi-nated partial noise.
The transliteration model wastrained in the same manner as BASELINE after elim-inating noise.The hyperparameters, ?, ?s, and ?t, were op-timized using a held-out set of 2,000 katakana-English pairs that were randomly chosen from ageneral-domain bilingual dictionary.
The hyperpa-rameter optimization was based on F-score valueson the held-out set with varying ?
among 0.01, 0.02,0.05, 0.1, 1.0, and ?s among 1, 2, 3, 5.Table 1 compares the statistics on the training setsof BASELINE and PROPOSED.
Note that we ap-plied the proposed method to BASELINE data (thesample-wise method was already applied until con-vergence).
The proposed method eliminated onlytwo transliteration candidates in sample-wise butalso eliminated 5,714 (0.64%) katakana and 55,737(4.1%) English characters3.4.2.3 Proposed method using aligned jointsubstrings as phrases (PROPOSED-JOINT)The many-to-many character alignment actuallyinduces substring pairs, which can be used as3The reason of larger number of partial noise in English sidewould be a syntactic difference as shown in Figure 2 and thekatakana-based filtering heuristics.Table 1: Statistics of the training sets.Method #pairs #Ja chars.
#En chars.BASELINE 104,563 899,080 1,372,993PROPOSED 104,561 893,366 1,317,256phrases in statistical machine transliteration andimproved transliteration performance (Finch andSumita, 2010).
We extracted them by: 1) generatemany-to-many word alignment, in which all possi-ble word alignment links in many-to-many corre-spondences (e.g., 0-0 0-1 0-2 1-0 1-1 1-2 for ??
?,c o m?
), 2) run phrase extraction and scoring same asa standard Moses training.
This procedure extractslonger phrases satisfying the many-to-many align-ment constraints than the simple use of extractedjoint substring pairs as phrases.4.3 ResultsTable 2 shows the results.
We used three evalua-tion metrics: ACC, F-score, and BLEUc.
ACC isa sample-wise accuracy and F-score is a character-wise F-measure-like score (Li et al 2010).
BLEUcis BLEU (Papineni et al 2002) in the character levelwith n=4.PROPOSED achieved 63% in ACC (16% rela-tive error reduction from BASELINE), and 94.6% inF-score (25% relative error reduction from BASE-LINE).
These improvements clearly showed an ad-vantage of the proposed method over the sample-wise mining.
BLEUc showed a similar improve-ments.
Recall that BASELINE and PROPOSED hada small difference in their training data, actually0.64% (katakana) and 4.1% (English) in the num-ber of characters.
The results suggest that the partialnoise can hurt transliteration models.PROPOSED-JOINT showed similar performanceas PROPOSED with a slight drop in BLEUc, al-though many-to-many substring alignment was ex-pected to improve transliteration as reported byFinch and Sumita (2010).
The difference may bedue to the difference in coverage of the phrasetables; PROPOSED-JOINT retained relatively longsubstrings by the many-to-many alignment con-straints in contrast to the less-constrained grow-diag-final-and alignments in PROPOSED.
Since thetraining data in our bootstrapping experiments con-207Table 2: Japanese-to-English transliteration results fortop-1000 unknown katakana words.
ACC and F-scorestand for the ones used in NEWS workshop, BLEUc ischaracter-wise BLEU.Method ACC F-score BLEUcBASELINE 0.56 0.929 0.864PROPOSED 0.63 0.946 0.897PROPOSED-JOINT 0.63 0.943 0.888tained many similar phrases unlike dictionary-baseddata in Finch and Sumita (2010), the phrase table ofPROPOSED-JOINT may have a small coverage dueto long and sparse substring pairs with large prob-abilities even if the many-to-many alignment wasgood.
This sparseness problem is beyond the scopeof this paper and worth further study.4.4 Alignment ExamplesFigure 4 shows examples of the alignment results inthe training data.
As expected, partial noise both inJapanese and English was identified correctly in (a),(b), and (c).
There were some alignment errors in thesignal part in (b), in which characters in boundarypositions were aligned incorrectly to adjacent sub-strings.
These alignment errors did not directly de-grade the partial noise identification but may causea negative effect on overall alignment performancein the sampling-based optimization.
(d) is a nega-tive example in which partial noise was incorrectlyaligned.
(c) and (d) have similar partial noise in theirEnglish word endings, but it could not be identifiedin (d).
One possible reason for that is the sparse-ness problem mentioned above, as shown in erro-neous long character alignments in (d).5 ConclusionThis paper proposed a noise-aware many-to-manyalignment model that can distinguish partial noise intransliteration pairs for bootstrapping statistical ma-chine transliteration model from automatically ex-tracted phrase pairs.
The model and training al-gorithm are straightforward extension of those byFinch and Sumita (2010).
The proposed methodwas proved to be effective in Japanese-to-Englishtransliteration experiments in patent domain.Future work will investigate the proposed method?
?a n sp?
sp ?a r c sp t a n g e n t?
?
?
?
?noise noise(a) Correctly aligned?
?d o p?
?
?i n g sp e n e r g y?
?
?
?
?
?
?
?
?noise noise(b) Some alignment errors in transliteration part?
?f o r?
?m e dnoise(c) Correctly aligned?
?c u s?
?
?t o m i z e d?
noise(d) Errors in partial noiseFigure 4: Examples of noise-aware many-to-many align-ment in the training data.
?
stands for a zero-length sub-string.
Dashed lines show incorrect alignments, and boldgrey lines mean their corrections.in other domains and language pairs.
The partialnoise would appear in other language pairs, typ-ically between agglutinative and non-agglutinativelanguages.
It is also worth extending the approachinto word alignment in statistical machine transla-tion.AcknowledgmentsWe would like to thank anonymous reviewers fortheir valuable comments and suggestions.ReferencesAndrew Finch and Eiichiro Sumita.
2010.
A BayesianModel of Bilingual Segmentation for Transliteration.In Proceedings of the seventh International Workshopon Spoken Language Translation (IWSLT), pages 259?266.Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, andBenjamin K. Tsou.
2013.
Overview of the Patent Ma-chine Translation Task at the NTCIR-10 Workshop.
InThe 10th NTCIR Conference, June.Ohnmar Htun, Andrew Finch, Eiichiro Sumita, andYoshiki Mikami.
2012.
Improving TransliterationMining by Integrating Expert Knowledge with Statis-tical Approaches.
International Journal of ComputerApplications, 58(17):12?22, November.Kevin Knight and Jonathan Graehl.
1998.
Machinetransliteration.
Computational Linguistics, 24(4):599?612.208Haizhou Li, A Kumaran, Min Zhang, and Vladimir Per-vouchine.
2010.
Whitepaper of NEWS 2010 SharedTask on Transliteration Generation.
In Proceedingsof the 2010 Named Entities Workshop, pages 12?20,Uppsala, Sweden, July.
Association for ComputationalLinguistics.Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.2009.
Bayesian Unsupervised Word Segmentationwith Nested Pitman-Yor Language Modeling.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP, pages 100?108, Suntec, Singapore, August.Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for Automatic Eval-uation of Machine Translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Hassan Sajjad, Alexander Fraser, and Helmut Schmid.2012.
A statistical model for unsupervised and semi-supervised transliteration mining.
In Proceedings ofthe 50th Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers), pages469?477, Jeju Island, Korea, July.
Association forComputational Linguistics.209
