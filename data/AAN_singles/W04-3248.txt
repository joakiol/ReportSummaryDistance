A New Approach for English-Chinese Named Entity AlignmentDonghui Feng?Information Sciences InstituteUniversity of Southern California4676 Admiralty Way, Suite 1001Marina Del Rey, CA, U.S.A, 90292donghui@isi.eduYajuan Lv?
Ming Zhou?
?Microsoft Research Asia5F Sigma Center, No.49 Zhichun Road, HaidianBeijing, China, 100080{t-yjlv, mingzhou}@microsoft.comAbstract?Traditional word alignment approaches cannotcome up with satisfactory results for NamedEntities.
In this paper, we propose a novelapproach using a maximum entropy model fornamed entity alignment.
To ease the trainingof the maximum entropy model, bootstrappingis used to help supervised learning.
Unlikeprevious work reported in the literature, ourwork conducts bilingual Named Entityalignment without word segmentation forChinese and its performance is much betterthan that with word segmentation.
Whencompared with IBM and HMM alignmentmodels, experimental results show that ourapproach outperforms IBM Model 4 andHMM significantly.1 IntroductionThis paper addresses the Named Entity (NE)alignment of a bilingual corpus, which meansbuilding an alignment between each source NE andits translation NE in the target language.
Researchhas shown that Named Entities (NE) carryessential information in human language (Hobbs etal., 1996).
Aligning bilingual Named Entities is aneffective way to extract an NE translation list andtranslation templates.
For example, in thefollowing sentence pair, aligning the NEs, [ZhiChun road] and [???]
can produce a translationtemplate correctly.?
Can I get to [LN Zhi Chun road] by eighto?clock??
?????
[LN ???]?
?In addition, NE alignment can be very useful forStatistical Machine Translation (SMT) and Cross-Language Information Retrieval (CLIR).A Named Entity alignment, however, is not easyto obtain.
It requires both Named EntityRecognition (NER) and alignment be handledcorrectly.
NEs may not be well recognized, or only?
The work was done while the first author wasvisiting Microsoft Research Asia.parts of them may be recognized during NER.When aligning bilingual NEs in differentlanguages, we need to handle many-to-manyalignments.
And the inconsistency of NEtranslation and NER in different languages is also abig problem.
Specifically, in Chinese NEprocessing, since Chinese is not a tokenizedlanguage, previous work (Huang et al, 2003)normally conducts word segmentation andidentifies Named Entities in turn.
This involvesseveral problems for Chinese NEs, such as wordsegmentation error, the identification of ChineseNE boundaries, and the mis-tagging of ChineseNEs.
For example, ??????
in Chinese is reallyone unit and should not be segmented as [ON ???]/?
.
The errors from word segmentation andNER will propagate into NE alignment.In this paper, we propose a novel approach usinga maximum entropy model to carry out English-Chinese Named Entity1 alignment.
NEs in Englishare first recognized by NER tools.
We theninvestigate NE translation features to identify NEsin Chinese and determine the most probablealignment.
To ease the training of the maximumentropy model, bootstrapping is used to helpsupervised learning.On the other hand, to avoid error propagationsfrom word segmentation and NER, we directlyextract Chinese NEs and make the alignment fromplain text without word segmentation.
It is unlikeprevious work reported in the literature.
Althoughthis makes the task more difficult, it greatlyreduces the chance of errors introduced byprevious steps and therefore produces much betterperformance on our task.To justify our approach, we adopt traditionalalignment approaches, in particular IBM Model 4(Brown et al, 1993) and HMM (Vogel et al,1996), to carry out NE alignment as our baselinesystems.
Experimental results show that in this taskour approach outperforms IBM Model 4 and HMMsignificantly.
Furthermore, the performance1 We only discuss NEs of three categories: PersonName (PN), Location Name (LN), and OrganizationName (ON).without word segmentation is much better than thatwith word segmentation.The rest of this paper is organized as follows: Insection 2, we discuss related work on NEalignment.
Section 3 gives the overall frameworkof NE alignment with our maximum entropymodel.
Feature functions and bootstrappingprocedures are also explained in this section.
Weshow experimental results and compare them withbaseline systems in Section 4.
Section 5 concludesthe paper and discusses ongoing future work.2 Related WorkTranslation knowledge can be acquired via wordand phrase alignment.
So far a lot of research hasbeen conducted in the field of machine translationand knowledge acquisition, including bothstatistical approaches (Cherry and Lin, 2003;Probst and Brown, 2002; Wang et al, 2002; Ochand Ney, 2000; Melamed, 2000; Vogel et al, 1996)and symbolic approaches (Huang and Choi, 2000;Ker and Chang, 1997).However, these approaches do not work well onthe task of NE alignment.
Traditional approachesfollowing IBM Models (Brown et al, 1993) are notable to produce satisfactory results due to theirinherent inability to handle many-to-manyalignments.
They only carry out the alignmentbetween words and do not consider the case ofcomplex phrases like some multi-word NEs.
Onthe other hand, IBM Models allow at most oneword in the source language to correspond to aword in the target language (Koehn et al, 2003;Marcu, 2001).
Therefore they can not handlemany-to-many word alignments within NEs well.Another well-known word alignment approach,HMM (Vogel et al, 1996), makes the alignmentprobabilities depend on the alignment position ofthe previous word.
It does not explicitly considermany-to-many alignment either.Huang et al (2003) proposed to extract NamedEntity translingual equivalences based on theminimization of a linearly combined multi-featurecost.
But they require Named Entity Recognitionon both the source side and the target side.Moore?s (2003) approach is based on a sequence ofcost models.
However, this approach greatly relieson linguistic information, such as a string repeatedon both sides, and clues from capital letters that arenot suitable for language pairs not belonging to thesame family.
Also, there are already completelexical compounds identified on the target side,which represent a big part of the final results.During the alignment, Moore does not hypothesizethat translations of phrases would require splittingpredetermined lexical compounds on the target set.These methods are not suitable for our task,since we only have NEs identified on the sourceside, and there is no extra knowledge from thetarget side.
Considering the inherent characteristicsof NE translation, we can find several features thatcan help NE alignment; therefore, we use amaximum entropy model to integrate these featuresand carry out NE alignment.3 NE Alignment with a Maximum EntropyModelWithout relying on syntactic knowledge fromeither the English side or the Chinese side, we findthere are several valuable features that can be usedfor Named Entity alignment.
Considering theadvantages of the maximum entropy model(Berger et al, 1996) to integrate different kinds offeatures, we use this framework to handle ourproblem.Suppose the source English NE,ene },...,{ 21 ne eeene = consists of n Englishwords and the candidate Chinese NE,cne },...,{ 21 mc cccne = is composed of mChinese characters.
Suppose also that we have Mfeature functions .,...,1),,( Mmneneh ecm = Foreach feature function, we have a model parameter.,...,1, Mmm =?
The alignment probability canbe defined as follows (Och and Ney, 2002):?
??===='1]),(exp[]),(exp[)|()|(1'1cMneMmecmmMmecmmececnenehnenehnenepneneP???
(3.1)The decision rule to choose the most probablealigned target NE of the English NE is (Och andNey, 2002):{ }??????==?=MmecmmneecnecnenehnenePencc1),(maxarg)|(maxarg??
(3.2)In our approach, considering the characteristicsof NE translation, we adopt 4 features: translationscore, transliteration score, the source NE andtarget NE?s co-occurrence score, and distortionscore for distinguishing identical NEs in the samesentence.
Next, we discuss these four features indetail.3.1 Feature Functions3.1.1 Translation ScoreIt is important to consider the translationprobability between words in English NE andcharacters in Chinese NE.
When processingChinese sentence without segmentation, word hererefers to single Chinese character.The translation score here is used to representhow close an NE pair is based on translationprobabilities.
Supposing the source English NEene consists of n English words,}...,{ 21 ne eeene = and the candidate Chinese NEcne is composed of m Chinesecharacters, }...,{ 21 mc cccne = , we can get thetranslation score of these two bilingual NEs basedon the translation probability between ei and cj:?
?= ==mjniijce ecpneneS1 1)|(),(      (3.3)Given a parallel corpus aligned at the sentencelevel, we can achieve the translation probabilitybetween each English word and each Chinesecharacter )|( ij ecp via word alignments with IBMModel 1 (Brown et al, 1993).
Without wordsegmentation, we have to calculate every possiblecandidate to determine the most probablealignment, which will make the search space verylarge.
Therefore, we conduct pruning upon thewhole search space.
If there is a score jumpbetween two adjacent characters, the candidate willbe discarded.
The scores between the candidateChinese NEs and the source English NE arecalculated via this formula as the value of thisfeature.3.1.2 Transliteration ScoreAlthough in theory, translation scores can buildup relations within correct NE alignments, inpractice this is not always the case, due to thecharacteristics of the corpus.
This is more obviouswhen we have sparse data.
For example, most ofthe person names in Named Entities are sparselydistributed in the corpus and not repeated regularly.Besides that, some English NEs are translated viatransliteration (Lee and Chang, 2003; Al-Onaizanand Knight, 2002; Knight and Graehl, 1997)instead of semantic translation.
Therefore, it isfairly important to make transliteration models.Given an English Named Entity e,}...,{ 21 neeee = , the procedure of transliterating einto a Chinese Named Entity c, }...,{ 21 mcccc = ,can be described with Formula (3.4) (Forsimplicity of denotation, we here use e and c torepresent English NE and Chinese NE instead ofene and cne ).
)|(maxarg ecPcc=)        (3.4)According to Bayes?
Rule, it can be transformedto:)|(*)(maxarg cePcPcc=)    (3.5)Since there are more than 6k common-usedChinese characters, we need a very large trainingcorpus to build the mapping directly betweenEnglish words and Chinese characters.
We adopt aromanization system, Chinese PinYin, to ease thetransformation.
Each Chinese charactercorresponds to a Chinese PinYin string.
And theprobability from a Chinese character to PinYinstring is 1)|( ?crP , except for polyphonouscharacters.
Thus we have:)|(*)|(*)(maxarg rePcrPcPcc=)   (3.6)Our problem is: Given both English NE andcandidate Chinese NEs, finding the most probablealignment, instead of finding the most probableChinese translation of the English NE.
Thereforeunlike previous work (Lee and Chang, 2003;Huang et al, 2003) in English-Chinesetransliteration models, we transform eachcandidate Chinese NE to Chinese PinYin stringsand directly train a PinYin-based language modelwith a separate English-Chinese name listconsisting of 1258 name pairs to decode the mostprobable PinYin string from English NE.To find the most probable PinYin string fromEnglish NE, we rewrite Formula (3.5) as thefollowing:)|(*)(maxarg rePrPrr=)      (3.7)where r represents the romanization (PinYinstring), }...,{ 21 mrrrr = .
For each of the factor, wehave)|()|(1?==miii rePreP      (3.8))|()|()()( 132121 ?=?
?= imiii rrrPrrPrPrP   (3.9)where ie  is an English syllable and ir  is aChinese PinYin substring.For example, we have English NE ?Richard?
andits candidate Chinese NE ?????.
Since both thechannel model and language model are PinYinbased, the result of Viterbi decoding is from ?Richar d?
to ?Li Cha De?.
We transform ?????
tothe PinYin string ?Li Cha De?.
Then we comparethe similarity based on the PinYin string instead ofwith Chinese characters directly.
This is becausewhen transliterating English NEs into Chinese, it isvery flexible to choose which character to simulatethe pronunciation, but the PinYin string isrelatively fixed.For every English word, there exist several waysto partition it into syllables, so here we adopt adynamic programming algorithm to decode theEnglish word into a Chinese PinYin sequence.Based on the transliteration string of the EnglishNE and the PinYin string of the original candidateChinese NE, we can calculate their similarity withthe XDice coefficient (Brew and McKelvie, 1996).This is a variant of Dice coefficient which allows?extended bigrams?.
An extended bigram (xbig) isformed by deleting the middle letter from anythree-letter substring of the word in addition to theoriginal bigrams.Suppose the transliteration string of the EnglishNE and the PinYin string of the candidate ChineseNE are tle  and pyc , respectively.
The XDicecoefficient is calculated via the following formula:)()()()(2),(pytlpytlpytlcxbigsexbigscxbigsexbigsceXDice+?=I   (3.10)Another point to note is that foreign personnames and Chinese person names have differenttranslation strategies.
The transliterationframework above is only applied on foreign names.For Chinese person name translation, the surfaceEnglish strings are exactly Chinese person names?PinYin strings.
To deal with the two situations, letsure  denote the surface English string, the finaltransliteration score is defined by taking themaximum value of the two XDice coefficients:)),(),,(max(),(surpytlpy ecXDiceecXDiceecTl =(3.11)This formula does not differentiate foreignperson names and Chinese person names, andforeign person names?
transliteration strings orChinese person names?
PinYin strings can behandled appropriately.
Besides this, since theEnglish string and the PinYin string share the samecharacter set, our approach can also work as analternative if the transliteration decoding fails.For example, for the English name ?Cuba?, thealignment to a Chinese NE should be ????.
Ifthe transliteration decoding fails, its PinYin string,?Guba?, still has a very strong relation with thesurface string ?Cuba?
via the XDice coefficient.This can make the system more powerful.3.1.3 Co-occurrence ScoreAnother approach is to find the co-occurrencesof source and target NEs in the whole corpus.
Ifboth NEs co-occur very often, there exists a bigchance that they align to each other.
Theknowledge acquired from the whole corpus is anextra and valuable feature for NE alignment.
Wecalculate the co-occurrence score of the sourceEnglish NE and the candidate Chinese NE with thefollowing formula:?= )(*,),()|(eececco necountnenecountneneP     (3.12)where ),( ec nenecount  is the number of timescne  and ene  appear together and )(*, enecountis the number of times that ene  appears.
Thisprobability is a good indication for determiningbilingual NE alignment.3.1.4 Distortion ScoreWhen translating NEs across languages, wenotice that the difference of their positions is also agood indication for determining their relation, andthis is a must when there are identical candidates inthe target language.
The bigger the difference is,the less probable they can be translations of eachother.
Therefore, we define the distortion scorebetween the source English NE and the candidateChinese NE as another feature.Suppose the index of the start position of theEnglish NE is i, and the length of the Englishsentence is m. We then have the relative position ofthe source English NEmipose = , and thecandidate Chinese NE?s relativeposition ,cpos 1,0 ??
ce pospos .
The distortionscore is defined with the following formula:)(1),( ceec posposABSneneDist ?
?= (3.13)where ABS means the absolute value.
If thereare multiple identical candidate Chinese NEs atdifferent positions in the target language, the onewith the largest distortion score will win.3.2 Bootstrapping with the MaxEnt ModelTo apply the maximum entropy model for NEalignment, we process in two steps: selecting theNE candidates and training the maximum entropymodel parameters.3.2.1 NE Candidate SelectionTo get an NE alignment with our maximumentropy model, we first use NLPWIN (Heidorn,2000) to identify Named Entities in English.
Foreach word in the recognized NE, we find all thepossible translation characters in Chinese throughthe translation table acquired from IBM Model 1.Finally, we have all the selected characters as the?seed?
data.
With an open-ended window for eachseed, all the possible sequences located within thewindow are considered as possible candidates forNE alignment.
Their lengths range from 1 to theempirically determined length of the window.During the candidate selection, the pruningstrategy discussed above is applied to reduce thesearch space.For example, in Figure 1, if ?China?
only has atranslation probability over the threshold valuewith ??
?, the two seed data are located with theindex of 0 and 4.
Supposing the length of thewindow to be 3, all the candidates around the seeddata including ???
?, with the length rangingfrom 1 to 3, are selected.Figure 1.
Example of Seed Data3.2.2 MaxEnt Parameter TrainingWith the four feature functions defined inSection 3.1, for each identified NE in English, wecalculate the feature scores of all the selectedChinese NE candidates.To achieve the most probable aligned ChineseNE, we use the published package YASMET2 toconduct parameter training and re-ranking of allthe NE candidates.
YASMET requires supervisedlearning for the training of the maximum entropymodel.
However, it is not easy to acquire a largeannotated training set.
Here bootstrapping is usedto help the process.
Figure 2 gives the wholeprocedure for parameter training.Figure 2.
Parameter Training4 Experimental Results4.1 Experimental SetupWe perform experiments to investigate theperformance of the above framework.
We take theLDC Xinhua News with aligned English-Chinesesentence pairs as our corpus.The incremental testing strategy is to investigatethe system?s performance as more and more dataare added into the data set.
Initially, we take 3002 http://www.isi.edu/~och/YASMET.htmlsentences as the standard testing set, and werepeatedly add 5k more sentences into the data setand process the new data.
After iterative re-ranking,the performance of alignment models over the 300sentence pairs is calculated.
The learning curvesare drawn from 5k through 30k sentences with thestep as 5k every time.4.2 Baseline SystemA translated Chinese NE may appear at adifferent position from the corresponding EnglishNE in the sentence.
IBM Model 4 (Brown et al,1993) integrates a distortion probability, which iscomplete enough to account for this tendency.
TheHMM model (Vogel et al, 1996) conducts wordalignment with a strong tendency to preservelocalization from one language to another.Therefore we extract NE alignments based on theresults of these two models as our baseline systems.For the alignments of IBM Model 4 and HMM, weuse the published software package, GIZA++ 3(Och and Ney, 2003) for processing.Some recent research has proposed to extractphrase translations based on the results from IBMModel (Koehn et al, 2003).
We extract English-Chinese NE alignments based on the results fromIBM Model 4 and HMM.
The extraction strategytakes each of the continuous aligned segments asone possible candidate, and finally the one with thehighest frequency in the whole corpus wins.Figure 3.
Example of Extraction StrategyFigure 3 gives an example of the extractionstrategy.
?China?
here is aligned to either ???
?or ???.
Finally the one with a higher frequency inthe whole corpus, say, ???
?, will be viewed asthe final alignment for ?China?.4.3 Results AnalysisOur approach first uses NLPWIN to conductNER.
Suppose S?
is the set of identified NE withNLPWIN.
S is the alignment set we compute withour models based on S?, and T is the set consistingof all the true alignments based on S?.
We definethe evaluation metrics of precision, recall, and F-score as follows:3 http://www.isi.edu/~och/GIZA++.html[China] hopes to further economic ?
[EU].?
?
?
?
?
?
?
?
?
??1.
Set the coefficients i?
as uniformdistribution;2.
Calculate all the feature scores to get theN-best list of the Chinese NE candidates;3.
Candidates with their values over a giventhreshold are considered to be correct andput into the re-ranking training set;4.
Retrain the parameters i?
with YASMET;5.
Repeat from Step 2 until i?
converge, andtake the current ranking as the final result.
[China] hopes to further economic ?
[EU].?
?
?
?
?
?
?
?
?
?
?Aligned Candidates:  China ??
?China ?
?STSprecisionI=           (4.1)TTSrecallI=           (4.2)recallprecisionrecallprecisionscoreF+?
?=?2  (4.3)4.3.1 Results without Word SegmentationBased on the testing strategies discussed inSection 4.1, we perform all the experiments ondata without word segmentation and get theperformance for NE alignment with IBM Model 4,the HMM model, and the maximum entropy model.Figure 4, 5, and 6 give the learning curves forprecision, recall, and F-score, respectively, withthese experiments.Precision Without Word Segmentation00.20.40.60.815k 10k 15k 20k 25k 30k data sizeprecision IBM ModelHMMMaxEntUpper BoundFigure 4.
Learning Curve with PrecisionRecall Without Word Segmentation00.20.40.60.815k 10k 15k 20k 25k 30k data sizerecall IBM ModelHMMMaxEntFigure 5.
Learning Curve with RecallF-score Without Word Segmentation00.10.20.30.40.50.60.70.85k 10k 15k 20k 25k 30k data sizeF-score IBM ModelHMMMaxEntFigure 6.
Learning Curve with F-scoreFrom these curves, we see that HMM generallyworks a little better than IBM Model 4, both forprecision and for recall.
NE alignment with themaximum entropy model greatly outperforms IBMModel 4 and HMM in precision, recall, and F-Score.
Since with this framework, we first useNLPWIN to recognize NEs in English, we haveNE identification error.
The precision of NLPWINon our task is about 77%.
Taking this into account,we know our precision score has actually beenreduced by this rate.
In Figure 4, this causes theupper bound of precision to be 77%.4.3.2 Comparison with Results with WordSegmentationTo justify that our approach of NE alignmentwithout word segmentation really reduces the errorpropagations from word segmentation andthereafter NER, we also perform all theexperiments upon the data set with wordsegmentation.
The segmented data is directly takenfrom published LDC Xinhua News corpus.precision recall F-scoreMaxEnt(Seg)0.56705 0.734491 0.64MaxEnt(Unseg)0.636015 0.823821 0.717838HMM(Seg)0.281955 0.372208 0.320856HMM(Unseg)0.291859 0.471464 0.360531IBM 4(Seg)0.223062 0.292804 0.253219IBM 4(Unseg)0.251185 0.394541 0.30695Table 1.
Results ComparisonTable 1 gives the comparison of precision, recall,and F-score for the experiments with wordsegmentation and without word segmentationwhen the size of the data set is 30k sentences.For HMM and IBM Model 4, performancewithout word segmentation is always better thanwith word segmentation.
For maximum entropymodel, the scores without word segmentation arealways 6 to 9 percent better than those with wordsegmentation.
This owes to the reduction of errorpropagation from word segmentation and NER.For example, in the following sentence pair withword segmentation, the English NE ?UnitedStates?
can no longer be correctly aligned to ????.
Since in the Chinese sentence, the incorrectsegmentation takes ??????
as one unit.
But ifwe conduct alignment without word segmentation,????
can be correctly aligned.?
Greek Prime Minister Costas Simitis visits[United States] .?
??
??
??
?
?
????
.Similar situations exist when HMM and IBMModel 4 are used for NE alignment.
Whencompared with IBM Model 4 and HMM with wordsegmentation, our approach with wordsegmentation also has a much better performancethan them.
This demonstrates that in any case ourapproach outperforms IBM Model 4 and HMMsignificantly.4.3.3 DiscussionHuang et al?s (2003) approach investigatedtransliteration cost and translation cost, based onIBM Model 1, and NE tagging cost by an NEidentifier.
In our approach, we do not have an NEtagging cost.
We use a different type of translationand transliteration score, and add a distortion scorethat is important to distinguish identical NEs in thesame sentence.Experimental results prove that in our approachthe selected features that characterize NEtranslations from English to Chinese help much forNE alignment.
The co-occurrence score uses theknowledge from the whole corpus to help NEalignment.
And the transliteration score addressesthe problem of data sparseness.
For example,English person name ?Mostafizur Rahman?
onlyappears once in the data set.
But with thetransliteration score, we get it aligned to theChinese NE ??????????
correctly.Since in ME training we use iterativebootstrapping to help supervised learning, thetraining data is not completely clean and bringssome errors into the final results.
But it avoids theacquisition of large annotated training set and theperformance is still much better than traditionalalignment models.
The performance is alsoimpaired by the English NER tool.
Anotherpossible reason for alignment errors is theinconsistency of NE translation in English andChinese.
For example, usually only the last nameof foreigners is translated into Chinese and the firstname is ignored.
This brings some trouble for thealignment of person names.5 ConclusionsTraditional word alignment approaches cannotcome up with satisfactory results for Named Entityalignment.
In this paper, we propose a novelapproach using a maximum entropy model for NEalignment.
To ease the training of the MaxEntmodel, bootstrapping is used to help supervisedlearning.
Unlike previous work reported in theliterature, our work conducts bilingual NamedEntity alignment without word segmentation forChinese, and its performance is much better thanwith word segmentation.
When compared withIBM and HMM alignment models, experimentalresults show that our approach outperforms IBMModel 4 and HMM significantly.Due to the inconsistency of NE translation, someNE pairs can not be aligned correctly.
We mayneed some manually-generated rules to fix this.
Wealso notice that NER performance over the sourcelanguage can be improved using bilingualknowledge.
These problems will be investigated inthe future.6 AcknowledgementsThanks to Hang Li, Changning Huang, YunboCao, and John Chen for their valuable commentson this work.
Also thank Kevin Knight for hischecking of the English of this paper.
Specialthanks go to Eduard Hovy for his continuoussupport and encouragement while the first authorwas visiting MSRA.ReferencesAl-Onaizan, Y. and Knight, K. 2002.
TranslatingNamed Entities Using Monolingual andBilingual Resources.
ACL 2002, pp.
400-408.Philadelphia.Berger, A. L.; Della Pietra, S. A.; and Della Pietra,V.
J.
1996.
A Maximum Entropy Approach toNatural Language Processing.
ComputationalLinguistics, vol.
22, no.
1, pp.
39-68.Brew, C. and McKelvie, D. 1996.
Word-pairextraction for lexicography.
The 2ndInternational Conference on New Methods inLanguage Processing, pp.
45?55.
Ankara.Brown, P. F.; Della Pietra, S. A.; Della Pietra, V. J.;and Mercer, R. L. 1993.
The Mathematics ofStatistical Machine Translation: ParameterEstimation.
Computational Linguistics,19(2):263-311.Cherry, C. and Lin, D. 2003.
A Probability Modelto Improve Word Alignment.
ACL 2003.Sapporo, Japan.Darroch, J. N. and Ratcliff, D. 1972.
GeneralizedIterative Scaling for Log-linear Models.
Annalsof Mathematical Statistics, 43:1470-1480.Heidorn, G. 2000.
Intelligent Writing Assistant.
AHandbook of Natural Language Processing:Techniques and Applications for the Processingof Language as Text.
Marcel Dekker.Hobbs, J. et al 1996.
FASTUS: A Cascaded Finite-State Transducer for Extracting Informationfrom Natural Language Text, MIT Press.Cambridge, MA.Huang, F.; Vogel, S. and Waibel, A.
2003.Automatic Extraction of Named EntityTranslingual Equivalence Based on Multi-Feature Cost Minimization.
ACL 2003 Workshopon Multilingual and Mixed-language NER.Sapporo, Japan.Huang, J. and Choi, K. 2000.
Chinese-KoreanWord Alignment Based on LinguisticComparison.
ACL-2000.
Hongkong.Ker, S. J. and Chang, J. S. 1997.
A Class-basedApproach to Word Alignment.
ComputationalLinguistics, 23(2):313-343.Knight, K. and Graehl, J.
1997.
MachineTransliteration.
ACL 1997, pp.
128-135.Koehn, P.; Och, F. J. and Marcu, D. 2003.Statistical Phrase-Based Translation.HLT/NAACL 2003.
Edmonton, Canada.Lee, C. and Chang, J. S. 2003.
Acquisition ofEnglish-Chinese Transliterated Word Pairs fromParallel-Aligned Texts, HLT-NAACL 2003Workshop on Data Driven MT, pp.
96-103.Marcu, D. 2001.
Towards a Unified Approach toMemory- and Statistical-Based MachineTranslation.
ACL 2001, pp.
378-385.
Toulouse,France.Melamed, I. D. 2000.
Models of TranslationEquivalence among Words.
ComputationalLinguistics, 26(2): 221-249.Moore, R. C. 2003.
Learning Translations ofNamed-Entity Phrases from Parallel Corpora.EACL-2003.
Budapest, Hungary.Och, F. J. and Ney, H. 2003.
A SystematicComparison of Various Statistical AlignmentModels, Computational Linguistics, volume 29,number 1, pp.
19-51.Och, F. J. and Ney, H. 2002.
DiscriminativeTraining and Maximum Entropy Models forStatistical Machine Translation.
ACL 2002, pp.295-302.Och, F. J. and Ney, H. 2000.
Improved StatisticalAlignment Models.
ACL 2000, pp: 440-447.Probst, K. and Brown, R. 2002.
Using SimilarityScoring to Improve the Bilingual Dictionary forWord Alignment.
ACL-2002, pp: 409-416.Vogel, S.; Ney, H. and Tillmann, C. 1996.
HMM-Based Word Alignment in Statistical Translation.COLING?96, pp.
836-841.Wang, W.; Zhou, M.; Huang, J. and Huang, C.2002.
Structural Alignment using BilingualChunking.
COLING-2002.
