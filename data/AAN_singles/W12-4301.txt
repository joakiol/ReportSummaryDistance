Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?9,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsIdentifying Comparative Claim Sentences in Full-Text Scientific ArticlesDae Hoon Parka Catherine Blakea,baDepartment of Computer Science bCenter for Informatics Research in Scienceand Scholarship at the Graduate School ofLibrary and Information ScienceUniversity of Illinois at Urbana-Champaign University of Illinois at Urbana-ChampaignUrbana, IL 61801, USA Champaign, IL 61820-6211, USAdpark34@illinois.edu clblake@illinois.eduAbstractComparisons play a critical role in scientificcommunication by allowing an author to situatetheir work in the context of earlier researchproblems, experimental approaches, and results.Our goal is to identify comparison claimsautomatically from full-text scientific articles.In this paper, we introduce a set of semanticand syntactic features that characterize asentence and then demonstrate how thosefeatures can be used in three differentclassifiers: Na?ve Bayes (NB), a Support VectorMachine (SVM) and a Bayesian network (BN).Experiments were conducted on 122 full-texttoxicology articles containing 14,157 sentences,of which 1,735 (12.25%) were comparisons.Experiments show an F1 score of 0.71, 0.69,and 0.74 on the development set and 0.76, 0.65,and 0.74 on a validation set for the NB, SVMand BN, respectively.1 IntroductionComparisons provide a fundamental building blockin human communication.
We continually compareproducts, strategies, and political candidates in ourdaily life, but comparisons also play a central rolein scientific discourse and it is not a surprise thatcomparisons appear in several models of scientificrhetoric.
The Create a Research Space (CARS)model includes counter-claiming and establishing agap during the ?establishing a niche?
phase(Swales, 1990), and the Rhetorical StructureTheory includes a contrast schema and antithesisrelation that is used between different nucleus andsatellite clauses (Mann & Thompson, 1988).However, neither of these models identify wherescientists make these comparisons.
In contrast,Kircz?s (1991) study of physics articles onlymentions comparisons with respect to the use ofdata to compare with other experimental results(sections 4.3 and 8.1, respectively) with earlierwork.
Similarly, Teufel and Moen?s contrastcategory (which includes the action lexicon sbetter_solution, comparison and contrast) is alsorestricted to contrasts with other work (Teufel &Moens, 2002).
Lastly the Claim Framework  (CF)includes a comparison category, but in contrast tothe earlier comparisons that reflect how science issituated within earlier work, the CF capturescomparisons between entities (Blake, 2010).Identifying comparisons automatically isdifficult from a computational perspective(Friedman, 1989).
For example, the followingsentence is not a comparison even though itcontains two words (more than) which areindicative of comparisons.
More than five methodswere used.
Bresnan claimed that ?comparativeclause construction in English is almost notoriousfor its syntactic complexity?
(Bresnan, 1973),p275.
Perhaps due to this complexity, severalinstructional books have been written to teach suchconstructs to non-native speakers.Our goal in this paper is to automaticallyidentify comparison claims from full-text scientificarticles, which were first defined in Blake?s ClaimFramework (Blake, 2010).
Comparisons capture abinary relationship between two concepts within asentence and the aspect on which the comparison ismade.
For example, ?patients with AML?
(a type of1leukemia) and ?normal controls?
are beingcompared in the following sentence, and the aspecton which the comparison is made is ?the plasmaconcentration of nm23-H1?.
The plasmaconcentration of nm23-H1 was higher in patientswith AML than in normal controls (P = .0001).
Inthis paper, we focus on identifying comparisonsentences and leave extraction of the two conceptsand the aspect on which the comparison is made asfuture work.
Similar to earlier comparisonsentences in biomedicine, we consider the sentenceas the unit of analysis (Fiszman, et al 2007).To achieve this goal, we cast the problem as aclassification activity and defined both semanticand syntactic features that are indicative ofcomparisons based on comparison sentences thatwere kindly provided by Fiszman (2007) andBlake (2010).
With the features in place, weconducted experiments using the Na?ve Bayes(NB) and Support Vector Machine (SVM)classifiers, which both work well on text.
We thenintroduce a Bayesian Network (BN) that removessome of the independence assumptions made inNB model.
The subsequent evaluation considersmore than 1,735 comparison claim sentences thatwere identified in 122 full text toxicology articles.Although automatically detecting comparisonsentences in full-text articles is challenging, webelieve that the information conveyed from suchsentences will provide a powerful new way toorganize scientific findings.
For example, a studentor researcher could enter a concept of interest andthe system would provide all the comparisons thathad been made.
Such a system would advance ourgeneral knowledge of information organization byrevealing what concepts can be compared.
Such astrategy could also be used for query expansion ininformation retrieval, and comparisons havealready been used for question answering (Ballard,1989).2 Related WorkComparisons play an important role in models ofscientific discourse (see Introduction), becauseauthors can compare research hypotheses, datacollection methods, subject groups, and findings.Comparisons are similar to the antithesis in theCARS model (Swales, 1990), the contrast schemain RST (Mann & Thompson, 1988) and in (Teufel& Moens, 2002) and the comparisons category ofthe CF model (Blake, 2010).From a computational linguistic perspective,Bresnan (1973) described the comparative clauseconstruction in English as ?almost notorious for itssyntactic complexity?.
Friedman (1989) alsopointed out that comparative structure is verydifficult to process by computer since comparisoncan occur in a variety of forms pervasivelythroughout the grammar and can occur almostanywhere in a sentence.
In contrast to the syntaxdescription of comparison sentences, Staab andHahn (1997) provided a description logicrepresentation of comparative sentences.
Each ofthese linguists studied the construction ofcomparative sentence, but did not distinguishcomparatives from non-comparative sentences.Beyond the linguistic community, Jindal andLiu (2006) have explored comparisons betweenproducts and proposed a comparative sentencemining method based on sequential rule miningwith words and the neighboring words?
Part-of-Speech tags.
The sequential rules are then usedas features in machine learning algorithms.
Theyreport that their method achieved a precision of79% and a recall of 81% on their data set.
Wetoo frame the problem as a classificationactivity, but Jindal and Liu use Part-of-Speechtags and indicator words as features while weuse a dependency tree representation to capturesentence features.
We also constructed aBayesian Network to remove the independenceassumption of Na?ve Bayes classifier.
Thecomparison definition used here also reflects thework of Jindal and Liu (2006).The work on product review comparisons wassubsequently extended to identify the preferredproduct; for example, camera X would beextracted from the sentence ?the picture qualityof Camera X is better than that of Camera Y.?
(Ganapathibhotla and Liu, 2008).
Features usedfor this subsequent work included a comparativeword, compared features, compared entities, anda comparison type.
Most recently, Xu et al(2011) explored comparative opinion miningusing Conditional Random Fields (CRF) toidentify different types of comparison relationswhere two product names must be present in asentence.
They report that their approachachieved a higher F1 score than the Jindal andLiu?s method on mobile phone review data.2Yang and Ko (2011) used maximum entropymethod and Support Vector Machines (SVM) toidentify comparison sentences from the webbased on keywords and Part-of-Speech tags oftheir neighboring words.
They achieved an F1-score of 90% on a data set written in Korean.The experiments reported here considerarticles in biomedicine and toxicology which aresimilar to those used by Fiszman et al whoidentified comparisons between drugs reportedin published clinical trial abstracts (Fiszman etal., 2007).
However, their definition ofcomparative sentence is narrower than ours inthat non-gradable comparative sentences are notconsidered.
Also, the goal is to classify type ofcomparative sentences which is different fromidentifying comparative sentences from a full-text article that contains non-comparativesentences as well.From a methodological standpoint, Na?veBayes (NB), Support Vector Machines (SVM),and Bayesian Network (BN) have been exploredfor variety of text classification problems(Sebastiani, 2002).
However, we are not awareof any studies that have explored these methodsto identify comparison sentences in full-textscientific articles.3 MethodOur goal is to automatically identify comparisonsentences from full text articles, which can beframed as a classification problem.
This sectionprovides the definitions used in this paper, adescription of the semantic and syntacticfeatures, and the classifiers used to achieve thegoal.
Stated formally: Let S = {S1, S2, ?, SN} bea set of sentences in a collection D. The featuresextracted automatically from those sentences willbe X = {X1, X2, ?, XM}.
Each feature Xj is adiscrete random variable and has a value Xij foreach sentence Si.
Let Ci be a class variable thatindicates whether a sentence Si is a comparative.Thus, the classifier will predict Ci based on thefeature values Xi1, Xi2, ?, XiM of Si.3.1 DefinitionsA comparative sentence describes at least onesimilarity or difference relation between twoentities.
The definition is similar to that in (Jindal& Liu, 2006).
A sentence may include more thanone comparison relation and may also include anaspect on which the comparison is made.
Werequire that the entities participating in thecomparison relation should be non-numeric andexist in the same sentence.A comparison word expresses comparativerelation between entities.
Common comparisonwords include ?similar?, ?different?, and adjectiveswith an ?-er?
suffix.
A compared entity is an objectin a sentence that is being compared with anotherobject.
Objects are typically noun phrases, such asa chemical name or biological entity.
Other thanbeing non-numeric, no other constraints apply tothe compared entities.
A compared aspect capturesthe aspect on which two comparison entities arecompared.
The definition is similar to a feature in(Jindal & Liu, 2006).
For example: the level ofsignificance differed greatly between the first andsecond studies.
A compared aspect is optional incomparative sentence.There are two comparative relation types:gradable and non-gradable (Jindal & Liu, 2006),and we further partition the latter into non-gradable similarity comparison and non-gradable difference comparison.
Also, weconsider equative comparison (Jindal & Liu,2006) as non-gradable.
Gradable comparisonsexpress an ordering of entities with regard to acertain aspect.
For example, sentences withphrases such as ?greater than?, ?decreasedcompared with?, or ?shorter length than?
aretypically categorized into this type.
Thesentence ?The number of deaths was higher forrats treated with the Emulphor vehicle than withcorn oil and increased with dose for bothvehicles?
is a gradable difference comparisonwhere ?higher?
is a comparison word, ?ratstreated with the Emulphor vehicle?
and ?ratstreated with corn oil?
are compared entities, and?the number of deaths?
is a compared aspect.Non-gradable similarity comparisons statethe similarity between entities.
Due to nature ofsimilarity, it has a non-gradable property.Phrases such as ?similar to?, ?the same as?, ?as ~as?, and ?similarly?
can indicate similaritycomparison in the sentence.
The sentence ?Meanmaternal body weight was similar betweencontrols and treated groups just prior to thebeginning of dosing.?
is an example ofsimilarity comparison where ?similar?
is acomparison word, ?controls?
and ?treated3groups?
are compared entities, and ?Meanmaternal body weight?
is a compared aspect.Non-gradable difference comparisonsexpress the difference between entities withoutstating the order of the entities.
For example,comparison phrases such as ?different from?
and?difference between?
are present in non-gradabledifference comparison sentences.
In thesentence ?Body weight gain and foodconsumption were not significantly differentbetween groups?
there is a single term entity?groups?, and a comparison word ?different?.With the entity and comparison word, thissentence has two comparative relations: onewith a compared aspect ?body weight gain?
andanother with ?food consumption?.3.2 Feature representationsFeature selection can have significant impact onclassification performance (Mitchell, 1997).
Weidentified candidate features in a pilot study thatconsidered 274 comparison sentences inabstracts (Fiszman et al, 2007) and 164comparison claim sentences in full text articles(Blake, 2010).
Thirty-five features weredeveloped that reflect both lexical and syntacticcharacteristics of a sentence.
Lexical featuresexplored in these experiments include:L1: The first lexical feature uses terms from theSPECIALIST lexicon (Browne, McCray, &Srinivasan, 2000), a component of the UnifiedMedical Language System (UMLS1, 2011AB)and is set to true when the sentence containsany inflections that are marked ascomparisons.
We modified the lexicon byadding terms in {?better?, ?more?, ?less?,?worse?, ?fewer?, ?lesser?}
and removingterms in {?few?, ?good?, ?ill?, ?later?, ?long-term?, ?low-dose?, ?number?, ?well?, ?well-defined?
}, resulting in 968 terms in total.L2: The second lexical feature capturesdirection.
A lexicon of 104 words was createdusing 82 of 174 direction verbs in (Blake,2010) and an additional 22 manually compiledwords.
Selections of direction words werebased on how well the individual wordpredicted a comparison sentence in thedevelopment set.
This feature is set to truewhen a sentence contains any words in thelexicon.1 http://www.nlm.nih.gov/research/umls/quickstart.htmlL3: Set to true when a sentence includes any ofthe following words: from, over or above.L4: Set to true when the sentence includeseither versus or vs.L5: Set to true when the sentence includes thephrase twice the.L6: Set to true when the sentence includes anyof the following phrases times that of, halfthat of, third that of, fourth that ofThe 27 syntactic features use a combination ofsemantics (words) and syntax.
Figure 1 shows adependency tree that was generated using theStanford Parser (version 1.6.9) (Klein &Manning, 2003).
The tree shown in Figure 1would be represented as:ROOT [root orders [nsubj DBP, cop is, amodseveral, prep of [pobj magnitude [amodmutagenic/carcinogenic [advmod more], prepthan [pobj BP]], punct .
]]where dependencies are shown in italics and thetree hierarchy is captured using [].
The wordROOT depicts the parent node of the tree.Figure 1.
Dependency tree for the sentence?DBP is several orders of magnitude moremutagenic/carcinogenic than BP.
?We compiled a similarity and differencelexicon (SIMDIF), which includes 31 wordssuch as similar, different, and same.
Words wereselected in the same way as the direction words(see L2).
Each term in the SIMDIF lexicon has acorresponding set of prepositions that were4collected from dictionaries.
For example, theword different in the SIMDIF lexicon has twocorresponding prepositions: ?from?
and ?than?.The first four syntactic rules capturecomparisons containing words in SIMDIF, andrules 5 through 24 capture comparisons relatedto the features L1, L2, or both.
Each of the rules25 and 26 consists of a comparative phrase andits syntactic dependency.
Each rule is reflectedas a Boolean feature that is set to true when therule applies and false otherwise.
For example,rule S1 would be true for the sentence ?X issimilar to Y?.Subscripts in the templates below depict theword identifier and constraints applied to a word.For example W2_than means that word 2 is drawnfrom the domain of (than), where numericvalues such as 2 are used to distinguish betweenwords.
Similarly, W4_SIMDIF means that the word4 is drawn from terms in the SIMDIF lexicon.The symbols |, ?, ?, and * depict disjunctions,negations, optional, and wildcard operatorsrespectively.S1: [root W1_SIMDIF [nsubj|cop W2, (prepW3)?
]]S2: [?root W1_SIMDIF [nsubj|cop W2, (prepW3)?
]]Syntactic rules 3 and 4 capture other forms ofnon-gradable comparisons with connectedprepositions.S3: [(prep W1)?, (* W2)?
[ (prep W3)?,(acomp|nsubjpass|nsubj|dobj|conj) W4_SIMDIF[(prep W5)?
]]]S4: [(prep W1)?, (* W2)?
[ (prep W3)?,?
(acomp|nsubjpass|nsubj|dobj|conj)W4_SIMDIF [(prep W5)?
]]]The following syntactic rules capture othernon-gradable comparisons and gradablecomparisons.
For example, the comparativesentence example in Figure 1 has the component[prep than], which is satisfied by rule S5.
Oneadditional rule (rule S27) uses a construct of ?as?
as?, but it?s not included here due to spacelimitations.S5: [ prep W1_than ]S6: [ advmod W1_than ]S7: [ quantmod|mwe W1_than ]S8: [ mark W1_than ]S9: [ dep W1_than ]S10: [ ?
(prep|advmod|quantmod|mwe|mark|dep) W1_than ]S11: [ advcl|prep W1_compared ]S12: [ dep W1_compared ]S13: [ ?
(advcl|prep|dep) W1_compared ]S14: [ advcl W1_comparing ]S15: [ partmod|xcomp W1_comparing ]S16: [ pcomp W1_comparing ]S17: [ nsubj W1_comparison ]S18: [ pobj W1_comparison ]S19: [ ?
(nsubj|pobj) W1_comparison ]S20: [ dep W1_contrast ]S21: [ pobj W1_contrast ]S22: [ advmod W1_relative ]S23: [ amod W1_relative ]S24: [ ?
(advmod|amod) W1_relative ]S25: W1_compare [ advmod W2_(well|favorably)]S26: W1_% [ nsubj W2 [prep W3_of]]Two additional general features were used.The preposition feature (PREP) captures themost indicative preposition among connectedprepositions in the rules 1 through 4.
It is anominal variable with six possible values, and thevalue assignment is shown in Table 1.
When morethan two values are satisfied, the lowest value isassigned.
The plural feature (PLURAL) for therules 1 through 4 is set to true when the subjectof a comparison is in the plural form and falseotherwise.
These two features provideinformation on if the sentence containscompared entities which are required in acomparison sentence.Value Preposition connected to SIMDIF word1 between, among, or across2 proper preposition provided in SIMDIF3 between, among, or across, but may not beconnected to SIMDIF word4 in or for5 any other prepositions or no preposition6 no SIMDIF word is foundTable 1: PREP value assignment3.3 ClassifiersThe Na?ve Bayes (NB), Support Vector Machine(SVM) and Bayesian Network (BN) classifierswere used in these experiments because theywork well with text (Sebastiani, 2002).5Figure 2: Bayesian Network for comparative sentences.
Multiple features having the sameconnections are placed in a big circle node for the purpose of simple representation.
C is a classvariable (comparative).The Bayesian Network model was developedto remove the independence assumption in theNB model.
BN is a directed acyclic graph thatcan compactly represent a probabilitydistribution because only the conditionalprobabilities (rather than the joint probabilities)need to be maintained.
Each node in the BNrepresents a random variable Xi and eachdirected edge reflects influence from the parentnode to the child node.In order to improve Na?ve Bayes classifier,we designed our Bayesian Network model bycapturing proper conditional dependenciesamong features.
Figure 2 shows the BN modelused in our experiments.
The relationshipsbetween features in BN were determinedheuristically.
Based on our observation, mostgradable comparisons contain both comparisonwords and corresponding prepositions, so weconnected such pairs.
Also, most non-gradablecomparisons contained comparison words anddifferent kinds of prepositions depending onsyntactic structure and plurality of subjects, andthese relations are captured in the network.
Forexample, features S5 through S10 depend on L1because a preposition ?than?
can be a goodindicative word only if there is a comparisonword of L1 in the same sentence.
Parameters forthe BN were estimated using maximumlikelihood estimation (MLE) with additivesmoothing.
Exact inference is feasible becauseall nodes except for the class node are observed.4 Results and DiscussionA pilot study was conducted using 297 and 165sentences provided by (Fiszman et al, 2007)and (Blake, 2010) respectively to identify aninitial set of features.
Features were then refinedbased on the development set described below(section 3 reports the revised features).
The BNmodel was also created based on results in thedevelopment set.Sentence Type Develop-mentValid-ationComparativeSentences1659(12.15%)76(15.2%)Non-comparativesentences11998(87.85%)424(84.8%)Total 13657(100%)500(100%)Table 2: Distribution of comparative and non-comparative sentences.Experiments reported in this paper consider122 full text articles on toxicology.
Figures,tables, citations, and references were removedfrom the corpus, and a development setcomprising 83 articles were drawn at randomwhich included 13,657 headings and sentences(the development set).
Articles in thedevelopment set were manually inspected bythree annotators to identify comparison claimsentences.
Annotators met weekly to discussproblematic sentences and all comparisonsentences were subsequently reviewed by thefirst author and updated where required toensure consistency.
Once the featurerefinements and BN were complete, a random6sample of 500 sentences was drawn from theremaining 39 articles (the validation set) whichwere then annotated by the first author.
Table 2shows that the number of comparison and non-comparison sentences are similar between thedevelopment and validation sets.The NB, SVM (LIBSVM package), and BNimplementations from WEKA were used withtheir default settings (Hall et al, 2009; Changand Lin, 2011).
Classifier performance wasmeasured using stratified 10-fold crossvalidation and a paired t-test was performed(using two-tail p-values 0.05 and 0.01) todetermine if the performance of the BN modelwas significantly different from the NB andSVM.We measured accuracy, the proportion ofcorrect predictions, and the area under a ROCcurve (ROC AUC), which is a plot of truepositive rate vs. false positive rate.
Given theskewed dataset (only 12% of the developmentsentences are comparisons), we recordedprecision, recall, and F1 score of each class,where F1 score is a harmonic mean of precisionand recall.NB SVM BNAccuracy 0.923 0.933 0.940++++ROC AUC 0.928 0.904 0.933++++Comp.
Precision 0.653 0.780 0.782++Comp.
Recall 0.778 0.621 0.706--++Comp.
F1 score 0.710 0.691 0.742++++Non-comp.
Precision 0.968 0.949 0.960--++Non-comp.
Recall 0.943 0.976 0.973++-Non-comp.
F1 score 0.955 0.962 0.966++++Table 3: Development set results.
Superscriptsand subscripts depict statistical significance forBN vs. NB and BN vs. SVM respectively.
+/- issignificant at p=0.05 and ++/-- is significant atp=0.01.
Bold depicts the best performance foreach metric.Table 3 shows the development set results.The accuracy and area under the ROC curve wassignificantly higher in BN compared to the NBand SVM models.
For comparative sentences,recall was the highest with NB, but F1 scorewas significantly higher with BN.
Although thedifference was small, the F1 score for non-comparative sentences was significantly highestin the BN model.Table 4 shows the validation set results,which are similar to the development set in thatthe BN model also achieved the highestaccuracy and area under the ROC curve.
TheBN model had the highest non-comparative F1score, but NB had a higher F1 score oncomparatives.NB SVM BNAccuracy 0.924 0.916 0.932ROC AUC 0.948 0.883 0.958Comp.
Precision 0.726 0.886 0.875Comp.
Recall 0.803 0.513 0.645Comp.
F1 score 0.763 0.650 0.742Non-comp.
Precision 0.964 0.919 0.939Non-comp.
Recall 0.946 0.988 0.983Non-comp.
F1 score 0.955 0.952 0.961Table 4: Validation set results.The results suggest that capturingdependencies between features helped toimprove the BN performance in some cases.
Forexample, unlike the BN, the NB and SVMmodels incorrectly classified the followingsentence as comparative: ?The method offorward difference was selected for calculationof sensitivity coefficients.?
The words ?forward?and ?difference?
would activate features L2 andS4, respectively, and 5 would be assigned forPREP.
Since the BN model capturesdependencies between L and S features andbetween S and the PREP feature, the probabilityin the BN model would not increase as much asin the NB model.
To better understand thefeatures, we conducted an error analysis of theBN classifier on validation set (see Table 5).PredictedClass 0 1ActualNon-comparative (0)  417 7Comparative (1) 27 49Table 5.
Validation confusion matrix for BN.We conducted a closer inspection of the sevenfalse positives (i.e.
the non-comparativesentences that were predicted comparative).
Infour cases, sentences were predicted ascomparative because two or more independent7weak features were true.
For example, in thesentence below, the features related to?compared?
(rule S11) and ?different?
(rule S4)were true and produced an incorrectclassification.
?Although these data cannot becompared directly to those in the current studybecause they are in a different strain of rat(Charles River CD), they clearly illustrate thevariability in the incidence of glial cell tumorsin rats.?
This sentence is not comparative forcompared since there is no comparison wordbetween these data and current study.
Similarly,this sentence is not comparative for differentsince only one compared entity is present for it.Two of the remaining false positive sentenceswere misclassified because the sentence had acomparison word and comparison entities, butthe sentence was not a claim.
The last incorrectsentence included a comparison with a numericvalue.Reason of misclassification # errorsProbability is estimated poorly 10Comparison is partially covered bydependency features7Comparison word is not in lexicon 7Dependency parse error 3Total 27Table 6.
Summary of false negative errors.We also investigated false negatives (i.e.comparative sentences that were predicted asnon-comparative by the BN).
The reasons oferrors are summarized in Table 6.
Out of 27errors, poor estimation was responsible for tenerrors.
These errors mostly come from thesparse feature space.
For example, in thesentence below, the features related to?increased?
(rule L2) and ?comparison?
(ruleS18) were active, but the probability ofcomparison is 0.424 since the feature space of?comparison?
feature is sparse, and the feature isnot indicative enough.
?Mesotheliomas of thetesticular tunic were statistically ( p < 0.001)increased in the high-dose male group incomparison to the combined control groups.
?Seven of the false negative errors werecaused by poor dependency features.
In this case,the comparison was covered by either the parentor the child feature node, not by both.
Otherseven errors were caused by missing terms inthe lexicons, and the last three were caused by adependency parse error.5 ConclusionComparison sentences play a critical role inscientific discourse as they enable an author tofully engage the reader by relating work to earlierresearch hypotheses, data collection methods,subject groups, and findings.
A review scientificdiscourse models reveals that comparisons havebeen reported as the thesis/antithesis in CARS(Swales, 1990), the contrast category in RST(Mann & Thompson, 1988) in Teufel & Moens(2002) and as a comparisons category in CF(Blake, 2010).In this paper, we introduce 35 features thatcapture both semantic and syntactic characteristicsof a sentence.
We then use those features withthree different classifiers, Na?ve Bayes, SupportVector Machines, and Bayesian Networks topredict comparison sentences.
Experimentsconsider 122 full text documents and 14,157sentences, of which 1,735 express at least onecomparison.
To our knowledge, this is the largestexperiment on comparison sentences expressed infull-text scientific articles.Results show that the accuracy and F1 scoresof the BN were statistically (p<=0.05) higherthan those of both the NB and SVM classifiers.Results also suggest that scientists report claimsusing a comparison sentence in 12.24% of thefull-text sentences, which is consistent with, butmore prevalent than in an earlier ClaimFramework study which reported a rate of5.11%.
Further work is required to understandthe source of this variation and the degree towhich the comparison features and classifiersused in this paper can also be used to capturecomparisons of scientific papers in otherdomains.AcknowledgementThis material is based upon work supported by theNational Science Foundation under Grant No.(1115774).
Any opinions, findings, andconclusions or recommendations expressed in thismaterial are those of the author(s) and do notnecessarily reflect the views of the NationalScience Foundation.8ReferencesBallard, B.W.
(1989).
A General ComputationalTreatment of Comparatives for Natural LanguageQuestion Answering, Association ofComputational Linguistics.
Vancouver, BritishColumbia, Canada.Blake, C. (2010).
Beyond genes, proteins, andabstracts: Identifying scientific claims from full-text biomedical articles.
Journal of BiomedicalInformatics, 43, 173-189.Bresnan, J.W.
(1973).
Syntax of the ComparativeClause Construction in English.
Linguistic Inquiry ,4(3), 275-343.Chih-Chung Chang and Chih-Jen Lin, LIBSVM : alibrary for support vector machines.
ACMTransactions on Intelligent Systems andTechnology, 2:27:1--27:27, 2011.Browne, A.C., McCray, A.T., & Srinivasan, S. (2000).The SPECIALIST LEXICON.
Bethesda, Maryland.Fiszman, M., Demner-Fushman, D., Lang, F.M.,Goetz, P., & Rindflesch, T.C.
(2007).
InInterpreting Comparative Constructions inBiomedical Text.
(pp.
37-144).Friedman, C. (1989).
A General ComputationalTreatment Of The Comparative, Association ofComputational Linguistics (pp.
161-168).Stroudsburg, PA.Ganapathibhotla, M., & Liu, B.
(2008).
MiningOpinions in Comparative Sentences.
InternationalConference on Computational Linguistics (Coling).Manchester, UK.Hall, M., Frank, E., Holmes, G., Pfahringer, B.,Reutemann, P., & Witten, I.H.
(2009).
The WEKAData Mining Software: An Update.
SIGKDDExplorations, 11(1).Jindal, N., & Liu, B.
(2006).
IdentifyingComparative Sentences in Text Documents,Special Interest Group in Information Retrieval(SIGIR) Seattle Washington USA, 244-251.Jindal, N., & Liu, B.
(2006).
Mining ComparativeSentences and Relations, American Association forArtificial Intelligence Boston, MA.Kircz, J.G.
(1991).
Rhetorical structure of scientificarticles: the case for argumentation analysis ininformation retrieval.
Journal of Documentation ,47(4), 354-372.Klein, D., & Manning, C.D.
(2003).
In Fast ExactInference with a Factored Model for NaturalLanguage Parsing.
Advances in NeuralInformation Processing Systems, 3-10.Mann, W.C., & Thompson, S.A. (1988).
RhetoricalStructure Theory: Toward a Functional Theory ofText Organization.
Text, 8(3), 243-281.Mitchell, T.M.
(1997).
Machine Learning: McGraw-Hill.Sebastiani, F. (2002).
Machine learning in automatedtext categorization.
ACM Computing Surveys,34(1), 1 - 47.Staab, S., & Hahn, U.
(1997).
Comparatives inContext.
National Conference on AI.
NationalConference on Artificial Intelligence 616-621.Swales, J.
(1990).
Genre Analysis: English inAcademic and Research Settings: CambridgeApplied Linguistics.Teufel, S., & Moens, M. (2002).
SummarizingScientific Articles -- Experiments with Relevanceand Rhetorical Status.
Computational Linguistics ,28(4), 409-445.Xu, K., Liao, S., Li, J., & Song, Y.
(2011).
MiningComparative Opinions from Customer Reviews forCompetitive Intelligence.
Decision SupportSystems, 50(4), 743-754.Yang, S., & Ko, Y.
(2011).
Extracting ComparativeEntities and Predicates from Texts UsingComparative Type Classification, Association ofComputation Linguistics.
Portland, OR.9
