c?
2003 Association for Computational LinguisticsDisambiguating Nouns, Verbs, andAdjectives Using Automatically AcquiredSelectional PreferencesDiana McCarthy?
John Carroll?University of Sussex University of SussexSelectional preferences have been used by word sense disambiguation (WSD) systems as onesource of disambiguating information.
We evaluate WSD using selectional preferences acquiredfor English adjective?noun, subject, and direct object grammatical relationships with respect toa standard test corpus.
The selectional preferences are specific to verb or adjective classes, ratherthan individual word forms, so they can be used to disambiguate the co-occurring adjectives andverbs, rather than just the nominal argument heads.
We also investigate use of the one-sense-per-discourse heuristic to propagate a sense tag for a word to other occurrences of the same wordwithin the current document in order to increase coverage.
Although the preferences performwell in comparison with other unsupervised WSD systems on the same corpus, the results showthat for many applications, further knowledge sources would be required to achieve an adequatelevel of accuracy and coverage.
In addition to quantifying performance, we analyze the results toinvestigate the situations in which the selectional preferences achieve the best precision and inwhich the one-sense-per-discourse heuristic increases performance.1.
IntroductionAlthough selectional preferences are a possible knowledge source in an automaticword sense disambiguation (WDS) system, they are not a panacea.
One problem iscoverage: Most previous work has focused on acquiring selectional preferences forverbs and applying them to disambiguate nouns occurring at subject and direct ob-ject slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson2000; Stevenson and Wilks 2001).
In normal running text, however, a large proportionof word tokens do not fall at these slots.
There has been some work looking at otherslots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Fed-erici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem ofcoverage remains.
Selectional preferences can be used for WSD in combination withother knowledge sources (Stevenson and Wilks 2001), but there is a need to ascer-tain when they work well so that they can be utilized to their full advantage.
Thisarticle is aimed at quantifying the disambiguation performance of automatically ac-quired selectional preferences in regard to nouns, verbs, and adjectives with respect toa standard test corpus and evaluation setup (SENSEVAL-2) and to identify strengthsand weaknesses.
Although there is clearly a limit to coverage using preferences alone,because preferences are acquired only with respect to specific grammatical roles, weshow that when dealing with running text, rather than isolated examples, coverage canbe increased at little cost in accuracy by using the one-sense-per-discourse heuristic.?
Department of Informatics, University of Sussex, Brighton BN1 9QH, UK.
E-mail: {dianam,johnca}@sussex.ac.uk640Computational Linguistics Volume 29, Number 4We acquire selectional preferences as probability distributions over the Word-Net (Fellbaum 1998) noun hyponym hierarchy.
The probability distributions are con-ditioned on a verb or adjective class and a grammatical relationship.
A noun is disam-biguated by using the preferences to give probability estimates for each of its sensesin WordNet, that is, for WordNet synsets.
Verbs and adjectives are disambiguated byusing the probability distributions and Bayes?
rule to obtain an estimate of the proba-bility of the adjective or verb class, given the noun and the grammatical relationship.Previously, we evaluated noun and verb disambiguation on the English all-words taskin the SENSEVAL-2 exercise (Cotton et al 2001).
We now present results also usingpreferences for adjectives, again evaluated on the SENSEVAL-2 test corpus (but carriedout after the formal evaluation deadline).
The results are encouraging, given that thismethod does not rely for training on any hand-tagged data or frequency distributionsderived from such data.
Although a modest amount of English sense-tagged data isavailable, we nevertheless believe it is important to investigate methods that do notrequire such data, because there will be languages or texts for which sense-taggeddata for a given word is not available or relevant.2.
MotivationThe goal of this article is to assess the WSD performance of selectional preferencemodels for adjectives, verbs, and nouns on the SENSEVAL-2 test corpus.
There are twoapplications for WSD that we have in mind and are directing our research.
The firstapplication is text simplification, as outlined by Carroll, Minnen, Pearce et al (1999).One subtask in this application involves substituting words with thier more frequentsynonyms, for example, substituting letter for missive.
Our motivation for using WSDis to filter out inappropriate senses of a word token, so that the substituting synonymis appropriate given the context.
For example, in the following sentence we wouldlike to use strategy, rather than dodge, as a substitute for scheme:A recent government study singled out the scheme as an example to others.We are also investigating the disambiguation of verb senses in running text beforesubcategorization information for the verbs is acquired, in order to produce a sub-categorization lexicon specific to sense (Preiss and Korhonen 2002).
For example, ifsubcategorization were acquired specific to sense, rather than verb form, then distinctsenses of fire could have different subcategorization entries:fire(1) - sack: NP V NPfire(2) - shoot: NP V NP, NP VSelectional preferences could also then be acquired automatically from sense-taggeddata in an iterative approach (McCarthy 2001).3.
MethodologyWe acquire selectional preferences from automatically preprocessed and parsed textduring a training phase.
The parser is applied to the test data as well in the run-time phase to identify grammatical relations among nouns, verbs, and adjectives.
Theacquired selectional preferences are then applied to the noun-verb and noun-adjectivepairs in these grammatical constructions for disambiguation.641McCarthy and Carroll Disambiguating Using Selectional Preferences0.0005 0.09<time>0.080.7<entity> <measure>drink suck sipmodel for verbclassSense Tagged output<attribute>DisambiguatorParserPreprocessorAcquisitionPreferenceSelectionala single glass with...where you can drinkTraining datad04.s12.t09    drink%2:34:00::d06.s13.t03    man%1:18:00::instance ID   WordNet sense tagncmod  glass_NN1 single_JJdobj   drink_VV0    glass_NN1ncsubj drink_VV0   you_PPYGrammatical RelationsTest dataThe men drink here ...WordNetSelectional PreferencesFigure 1System overview.
Solid lines indicate flow of data during training, and broken lines show thatat run time.The overall structure of the system is illustrated in Figure 1.
We describe theindividual components in sections 3.1?3.3 and 4.3.1 PreprocessingThe preprocessor consists of three modules applied in sequence: a tokenizer, a part-of-speech (POS) tagger, and a lemmatizer.The tokenizer comprises a small set of manually developed finite-state rules foridentifying word and sentence boundaries.
The tagger (Elworthy 1994) uses a bigramhidden Markov model augmented with a statistical unknown word guesser.
Whenapplied to the training data for selectional preference acquisition, it produces the sin-gle highest-ranked POS tag for each word.
In the run-time phase, it returns multipletag hypotheses, each with an associated forward-backward probability to reduce theimpact of tagging errors.
The lemmatizer (Minnen, Carroll, and Pearce 2001) reducesinflected verbs and nouns to their base forms.
It uses a set of finite-state rules express-ing morphological regularities and subregularities, together with a list of exceptionsfor specific (irregular) word forms.3.2 ParsingThe parser uses a wide-coverage unification-based shallow grammar of English POStags and punctuation (Briscoe and Carroll 1995) and performs disambiguation usinga context-sensitive probabilistic model (Briscoe and Carroll 1993), recovering fromextra-grammaticality by returning partial parses.
The output of the parser is a set ofgrammatical relations (Carroll, Briscoe, and Sanfilippo 1998) specifying the syntacticdependency between each head and its dependent(s), taken from the phrase structuretree that is returned from the disambiguation phase.For selectional preference acquisition we applied the analysis system to the 90million words of the written portion of the British National Corpus (BNC); the parserproduced complete analyses for around 60% of the sentences and partial analysesfor over 95% of the remainder.
Both in the acquisition phase and at run time, weextract from the analyser output subject?verb, verb?direct object, and noun?adjective642Computational Linguistics Volume 29, Number 4modifier dependencies.1 We did not use the SENSEVAL-2 Penn Treebank?style brack-etings supplied for the test data.3.3 Selectional Preference AcquisitionThe preferences are acquired for grammatical relations (subject, direct object, andadjective?noun) involving nouns and grammatically related adjectives or verbs.
Weuse WordNet synsets to define our sense inventory.
Our method exploits the hyponymlinks given for nouns (e.g., cheese is a hyponym of food), the troponym links for verbs 2(e.g., limp is a troponym of walk), and the ?similar-to?
relationship given for adjectives(e.g., one sense of cheap is similar to flimsy).The preference models are modifications of the tree cut models (TCMs) originallyproposed by Li and Abe (1995, 1998).
The main differences between that work andours are that we acquire adjective as well as verb models, and also that our modelsare with respect to verb and adjective classes, rather than forms.
We acquire modelsfor classes because we are using the models for WSD, whereas Li and Abe used themfor structural disambiguation.We define a TCM as follows.
Let NC be the set of noun synsets (noun classes)in WordNet: NC = {nc ?
WordNet}, and NS be the set of noun senses 3 in Wordnet:NS = {ns ?
WordNet}.
A TCM is a set of noun classes that partition NS disjointly.We use ?
to refer to such a set of classes in a TCM.
A TCM is defined by ?
and aprobability distribution:?nc?
?p(nc) = 1 (8)The probability distribution is conditioned by the grammatical context.
In thiswork, the probability distribution associated with a TCM is conditioned on a verbclass (vc) and either the subject or direct-object relation, or an adjective class (ac)and the adjective?noun relation.
Let VC be the set of verb synsets (verb classes) inWordNet: VC = {vc ?
WordNet}.
Let AC be the set of adjective classes (which subsumeWordNet synsets; we elaborate further on this subsequently).
Thus, the TCMs definea probability distribution over NS that is conditioned on a verb class (vc) or adjectiveclass (ac) and a particular grammatical relation (gr):?nc?
?p(nc|vc, gr) = 1 (9)Acquisition of a TCM for a given vc and gr proceeds as follows.
The data foracquiring the preference are obtained from a subset of the tuples involving verbsin the synset or troponym (subordinate) synsets.
Not all verbs that are troponymsor direct members of the synset are used in training.
We take the noun argumentheads occurring with verbs that have no more than 10 senses in WordNet and afrequency of 20 or more occurrences in the BNC data in the specified grammaticalrelationship.
The threshold of 10 senses removes some highly polysemous verbs havingmany sense distinctions that are rather subtle.
Verbs that have more than 10 sensesinclude very frequent verbs such as be and do that do not select strongly for their1 In a previous evaluation of grammatical-relation accuracy with in-coverage text, the analyzer returnedsubject?verb and verb?direct object dependencies with 84?88% recall and precision (Carroll, Minnen,and Briscoe 1999).2 In WordNet, verbs are organized by the troponymy relation, but this is represented with the samehyponym pointer as is used in the noun hierarchy.3 We refer to nouns attached to synsets as noun senses.643McCarthy and Carroll Disambiguating Using Selectional Preferences0204060801000 50 100 150 200%notinWordNetfrequency rankFigure 2Verbs not in WordNet by BNC frequency.arguments.
The frequency threshold of 20 is intended to remove noisy data.
We setthe threshold by examining a plot of BNC frequency and the percentage of verbs atparticular frequencies that are not listed in WordNet (Figure 2).
Using 20 as a thresholdfor the subject slot results in only 5% verbs that are not found in WordNet, whereas73% of verbs with fewer than 20 BNC occurrences are not present in WordNet.4The selectional-preference models for adjective?noun relations are conditioned onan ac.
Each ac comprises a group of adjective WordNet synsets linked by the ?similar-to?
relation.
These groups are formed such that they partition all adjective synsets.Thus AC = {ac ?
WordNet adjective synsets linked by similar-to}.
For example, Figure 3shows the adjective classes that include the adjective fundamental and that are formedin this way.5 For selectional-preference models conditioned on adjective classes, weuse only those adjectives that have 10 synsets or less in WordNet and have 20 or moreoccurrences in the BNC.The set of ncs in ?
are selected from all the possibilities in the hyponym hierarchyaccording to the minimum description length (MDL) principle (Rissanen 1978) as usedby Li and Abe (1995, 1998).
MDL finds the best TCM by considering the cost (in bits)of describing both the model and the argument head data encoded in the model.
Thecost (or description length) for a TCM is calculated according to equation (10).
Thenumber of parameters of the model is given by k, which is the number of ncs in ?minus one.
N is the sample of the argument head data.
The cost of describing eachnoun argument head (n) is calculated by the log of the probability estimate for thatnoun:description length = model description length + data description length= k2 ?
log |N| +?
?n?Nlog p(n) (10)4 These threshold values are somewhat arbitrary, but it turns out that our results are not sensitive to theexact values.5 For the sake of brevity, not all adjectives are included in this diagram.644Computational Linguistics Volume 29, Number 4basicroot radicalgrassrootsunderlying fundamentalrudimentaryprimal elementarybasal baseof import importantprimal keycentral fundamental cardinalcrucial essentialvaluable usefulhistoricchief principal primary mainmeasurableweighty grevious gravesignificant importantmonumentalprofound fundamentalepochalearthshakingheadportentous prodigiousevidentiary evidentialnoteworthy remarkablelargeFigure 3Adjective classes that include fundamental.The probability estimate for each n is obtained using the estimates for all the nssthat n has.
Let Cn be the set of ncs that include n as a direct member: Cn = {nc ?
NC|n ?nc}.
Let nc?
be a hypernym of nc on ?
(i.e.
nc?
?
{?|nc ?
nc?})
and let nsnc?
= {ns ?
nc?
}(i.e., the set of nouns senses at and beneath nc?
in the hyponym hierarchy).
Then theestimate p(n) is obtained using the estimates for the hypernym classes on ?
for all theCn that n belongs to:p(n) =?nc?Cnp(nc?)|nsnc?
|(11)The probability at any particular nc?
is divided by nsnc?
to give the estimate for eachp(ns) under that nc?.The probability estimates for the {nc ?
?}
( p(nc|vc, gr) or p(nc|ac, gr)) are obtainedfrom the tuples from the data of nouns co-occurring with verbs (or adjectives) belong-ing to the conditioning vc (or ac) in the specified grammatical relationship (< n, v, gr >).The frequency credit for a tuple is divided by |Cn| for any n, and by the number ofsynsets of v, Cv (or Ca if the gr is adjective-noun):freq(nc|vc, gr) =?v?vc?n?ncfreq(n|v, gr)|Cn||Cv|(12)A hypernym nc?
includes the frequency credit attributed to all its hyponyms ({nc ?nc?
}).freq(nc?|vc, gr) =?nc?nc?freq(nc|vc, gr) (13)This ensures that the total frequency credit at any ?
across the hyponym hierarchyequals the credit for the conditioning vc.
This will be the sum of the frequency creditfor all verbs that are direct members or troponyms of the vc, divided by the numberof other senses of each of these verbs:freq(vc|gr) =?verb?vcfreq(verb|gr)|Cv|(14)645McCarthy and Carroll Disambiguating Using Selectional Preferencesgroup eventRoothuman_actionTCM forTCM forcontrol monarchythrone partypossessionstrawmoneyhandle0.010.020.140.010.02 0.530.060.050.070.26entityseize assume usurpseize clutchFigure 4TCMs for the direct-object slot of two verb classes that include the verb seize.To ensure that the TCM covers all NS in WordNet, we modify Li and Abe?s originalscheme by creating hyponym leaf classes below all WordNet?s internal classes in thehyponym hierarchy.
Each leaf holds the ns previously held at the internal class.Figure 4 shows portions of two TCMs.
The TCMs are similar, as they both containthe verb seize, but the TCM for the class that includes clutch has a higher probabilityfor the entity noun class compared to the class that also includes assume and usurp.This example includes only top-level WordNet classes, although the TCM may usemore specific noun classes.4.
DisambiguationNouns, adjectives and verbs are disambiguated by finding the sense (nc, vc, or ac) withthe maximum probability estimate in the given context.
The method disambiguatesnouns and verbs to the WordNet synset level and adjectives to a coarse-grained levelof WordNet synsets linked by the similar-to relation, as described previously.4.1 Disambiguating NounsNouns are disambiguated when they occur as subjects or direct objects and whenmodified by adjectives.
We obtain a probability estimate for each nc to which the targetnoun belongs, using the distribution of the TCM associated with the co-occurring verbor adjective and the grammatical relationship.Li and Abe used TCMs for the task of structural disambiguation.
To obtain proba-bility estimates for noun senses occurring at classes beneath hypernyms on the cut, Liand Abe used the probability estimate at the nc?
on the cut divided by the number ofns descendants, as we do when finding ?
during training, so the probability estimateis shared equally among all nouns in the nc?, as in equation (15).p(ns ?
nsnc?)
=p(nc?)|nsnc?
|(15)One problem with doing this is that in cases in which the TCM is quite high in thehierarchy, for example, at the entity class, the probability of any ns?s occurring underthis nc?
on the TCM will be the same and does not allow us to discriminate amongsenses beneath this level.For the WSD task, we compare the probability estimates at each nc ?
Cn, so ifa noun belongs to several synsets, we compare the probability estimates, given thecontext, of these synsets.
We obtain estimates for each nc by using the probability ofthe hypernym nc?
on ?.
Rather than assume that all synsets under a given nc?
on ?646Computational Linguistics Volume 29, Number 4have the same likelihood of occurrence, we multiply the probability estimate for thehypernym nc?
by the ratio of the prior frequency of the nc, that is, p(nc|gr), for whichwe seek the estimate divided by the prior frequency of the hypernym nc?
(p(nc?|gr)):p(nc ?
hyponyms of nc?|vc, gr) = p(nc?|vc, gr)?
p(nc|gr)p(nc?|gr) (16)These prior estimates are taken from populating the noun hyponym hierarchy with theprior frequency data for the gr irrespective of the co-occurring verbs.
The probabilityat the hypernym nc?
will necessarily total the probability at all hyponyms, since thefrequency credit of hyponyms is propagated to hypernyms.Thus, to disambiguate a noun occurring in a given relationship with a given verb,the nc ?
Cn that gives the largest estimate for p(nc|vc, gr) is taken, where the verb class(vc) is that which maximizes this estimate from Cv.
The TCM acquired for each vc ofthe verb in the given gr provides an estimate for p(nc?|vc, gr), and the estimate for ncis obtained as in equation (16).For example, one target noun was letter, which occurred as the direct object ofsign in our parses of the SENSEVAL-2 data.
The TCM that maximized the probabilityestimate for p(nc|vc, direct object) is shown in Figure 5.
The noun letter is disambiguatedby comparing the probability estimates on the TCM above the five senses of letter mul-tiplied by the proportion of that probability mass attributed to that synset.
Althoughentity has a higher probability on the TCM, compared to matter, which is above thecorrect sense of letter,6 the ratio of prior probabilities for the synset containing letter7under entity is 0.001, whereas that for the synset under matter is 0.226.
This gives aprobability of 0.009?0.226 = 0.002 for the noun class probability given the verb classapprovalrootentityhumancapitalistinterpretationmessagecommunicationabstractionrelationhuman_actsymbol0.160.0120.03290.1140.0060.009draftlettermatterwritingexplanationletteralphabetic_letterletterletterlettersignFigure 5TCM for the direct-object slot of the verb class including sign and ratify.6 The gloss is ?a written message addressed to a person or organization; e.g.
wrote an indignant letter tothe editor.
?7 The gloss is ?owner who lets another person use something (housing usually) for hire.
?647McCarthy and Carroll Disambiguating Using Selectional Preferences(with maximum probability) and grammatical context.
This is the highest probabilityfor any of the synsets of letter, and so in this case the correct sense is selected.4.2 Disambiguating Verbs and AdjectivesVerbs and adjectives are disambiguated using TCMs to give estimates for p(nc|vc, gr)and p(nc|ac, gr), respectively.
These are combined with prior estimates for p(nc|gr) andp(vc|gr) (or p(ac|gr)) using Bayes?
rule to give:p(vc|nc, gr) = p(nc|vc, gr)?
p(vc|gr)p(nc|gr) (17)and for adjective?noun relations:p(ac|nc, adjnoun) = p(nc|ac, adjnoun)?
p(ac|adjnoun)p(nc|adjnoun) (18)The prior distributions for p(nc|gr), p(vc|gr) and p(ac|adjnoun) are obtained dur-ing the training phase.
For the prior distribution over NC, the frequency credit ofeach noun in the specified gr in the training data is divided by |Cn|.
The frequencycredit attached to a hyponym is propagated to the superordinate hypernyms, and thefrequency of a hypernym (nc?)
totals the frequency at its hyponyms:freq(nc?|gr) =?nc?nc?freq(nc|gr) (19)The distribution over VC is obtained similarly using the troponym relation.
Forthe distribution over AC, the frequency credit for each adjective is divided by thenumber of synsets to which the adjective belongs, and the credit for an ac is the sumover all the synsets that are members by virtue of the similar-to WordNet link.To disambiguate a verb occurring with a given noun, the vc from Cv that givesthe largest estimate for p(vc|nc, gr) is taken.
The nc for the co-occurring noun is thenc from Cn that maximizes this estimate.
The estimate for p(nc|vc, gr) is taken as inequation (16) but selecting the vc to maximize the estimate for p(vc|nc, gr) rather thanp(nc|vc, gr).
An adjective is likewise disambiguated to the ac from all those to which theadjective belongs, using the estimate for p(nc|ac, gr) and selecting the nc that maximizesthe p(ac|nc, gr) estimate.4.3 Increasing Coverage: One Sense per DiscourseThere is a significant limitation to the word tokens that can be disambiguated usingselectional preferences, in that they are restricted to those that occur in the specifiedgrammatical relations and in argument head position.
Moreover, we have TCMs onlyfor adjective and verb classes in which there was at least one adjective or verb mem-ber that met our criteria for training (having no more than a threshold of 10 senses inWordNet and a frequency of 20 or more occurrences in the BNC data in the specifiedgrammatical relationship).
We chose not to apply TCMs for disambiguation where wedid not have TCMs for one or more classes for the verb or adjective.
To increase cov-erage, we experimented with applying the one-sense-per-discourse (OSPD) heuristic(Gale, Church, and Yarowsky 1992).
With this heuristic, a sense tag for a given wordis propagated to other occurrences of the same word within the current document inorder to increase coverage.
When applying the OSPD heuristic, we simply applied atag for a noun, verb, or adjective to all the other instances of the same word type withthe same part of speech in the discourse, provided that only one possible tag for thatword was supplied by the selectional preferences for that discourse.648Computational Linguistics Volume 29, Number 40204060801000 20 40 60 80 100RecallPrecisionsel-ospd-ana sel-ospdselsupervisedother unsupervisedfirst sense heuristicselsel-ospdsel-ospd-anaFigure 6SENSEVAL-2 English all-words task results.5.
EvaluationWe evaluated our system using the SENSEVAL-2 test corpus on the English all-words task (Cotton et al, 2001).
We entered a previous version of this system forthe SENSEVAL-2 exercise, in three variants, under the names ?sussex-sel?
(selectionalpreferences), ?sussex-sel-ospd?
(with the OSPD heuristic), and ?sussex-sel-ospd-ana?
(with anaphora resolution).
8 For SENSEVAL-2 we used only the direct object andsubject slots, since we had not yet dealt with adjectives.
In Figure 6 we show how oursystem fared at the time of SENSEVAL-2 compared to other unsupervised systems.9We have also plotted the results of the supervised systems and the precision and recallachieved by using the most frequent sense (as listed in WordNet).10In the work reported here, we attempted disambiguation for head nouns and verbsin subject and direct object relationships, and for adjectives and nouns in adjective-noun relationships.
For each test instance, we applied subject preferences before directobject preferences, and direct object preferences before adjective?noun preferences.
Wealso propagated sense tags to test instances not in these relationships by applying theone-sense-per-discourse heuristic.We did not use the SENSEVAL-2 coarse-grained classification, as this was notavailable at the time when we were acquiring the selectional preferences.
We therefore8 The motivation for using anaphora resolution was increased coverage, but anaphora resolution turnedout not actually to improve performance.9 We use unsupervised to refer to systems that do not use manually sense-tagged training data, such asSemCor.
Our systems, marked in the figure as sel, sel-ospd, and sel-ospd-ana are unsupervised.10 We are indebted to Judita Preiss for the most-frequent-sense result.
This was obtained using thefrequency data supplied with the WordNet 1.7 version prereleased for SENSEVAL-2.649McCarthy and Carroll Disambiguating Using Selectional PreferencesTable 1Overall results.With OSPD Without OSPDPrecision 51.1% Precision 52.3%Recall 23.2% Recall 20.0%Attempted 45.5% Attempted 38.3%Table 2Precision results by part of speech.Precision (%) Baseline precision (%)Nouns 58.5 51.7Polysemous nouns 36.8 25.8Verbs 40.9 29.7Polysemous verbs 38.1 25.3Adjectives 49.8 48.6Polysemous adjectives 35.5 24.0Nouns, verbs, and adjectives 51.1 44.9Polysemous nouns, verbs, and adjectives 36.8 27.3do not include in the following the coarse-grained results; they are just slightly betterthan the fine-grained results, which seems to be typical of other systems.Our latest overall results are shown in Table 1.
In this table we show the resultsboth with and without the OSPD heuristic.
The results for the English SENSEVAL-2tasks were generally much lower than those for the original SENSEVAL competition.At the time of the SENSEVAL-2 workshop, this was assumed to be due largely to theuse of WordNet as the inventory, as opposed to HECTOR (Atkins 1993), but Palmer,Trang Dang, and Fellbaum (forthcoming) have subsequently shown that, at least for thelexical sample tasks, this was due to a harder selection of words, with a higher averagelevel of polysemy.
For three of the most polysemous verbs that overlapped betweenthe English lexical sample for SENSEVAL and SENSEVAL-2, the performance wascomparable.
Table 2 shows our precision results including use of the OSPD heuristic,broken down by part of speech.
Although the precision for nouns is greater than thatfor verbs, the difference is much less when we remove the trivial monosemous cases.Nouns, verbs, and adjectives all outperform their random baseline for precision, andthe difference is more marked when monosemous instances are dropped.Table 3 shows the precision results for polysemous words given the slot and thedisambiguation source.
Overall, once at least one word token has been disambiguatedby the preferences, the OSPD heuristic seems to perform better than the selectionalpreferences.
We can see, however, that although this is certainly true for the nouns,the difference for the adjectives (1.3%) is less marked, and the preferences outperformOSPD for the verbs.
It seems that verbs obey the OSPD principle much less than nouns.Also, verbs are best disambiguated by their direct objects, whereas nouns appear tobe better disambiguated as subjects and when modified by adjectives.6.
Discussion6.1 Selectional PreferencesThe precision of our system compares well with that of other unsupervised systems onthe SENSEVAL-2 English all-words task, despite the fact that these other systems use a650Computational Linguistics Volume 29, Number 4Table 3Precision results for polysemous words by part of speech and slot or disambiguation source.Subject (%) Dobj (%) Adjm (%) OSPD (%)Polysemous nouns 33.7 26.8 31.0 49.0Polysemous verbs 33.8 47.3 ?
29.8Polysemous adjectives ?
?
35.1 36.4Polysemous nouns, verbs, and adjectives 33.4 36.0 31.6 44.8number of different sources of information for disambiguation, rather than selectionalpreferences in isolation.
Light and Greiff (2002) summarize some earlier WSD resultsfor automatically acquired selectional preferences.
These results were obtained forthree systems (Resnik 1997; Abney and Light 1999; Ciaramita and Johnson 2000) on atraining and test data set constructed by Resnik containing nouns occurring as directobjects of 100 verbs that select strongly for their objects.Both the test and training sets were extracted from the section of the Brown cor-pus within the Penn Treebank and used the treebank parses.
The test set comprisedthe portion of this data within SemCor containing these 100 verbs, and the trainingset comprised 800,000 words from the Penn Treebank parses of the Brown corpus notwithin SemCor.
All three systems obtained higher precision than the results we reporthere, with Ciaramita and Johnson?s Bayesian belief networks achieving the best accu-racy at 51.4%.
These results are not comparable with ours, however, for three reasons.First, our results for the direct-object slot are for all verbs in the English all-words task,as opposed to just those selecting strongly for their direct objects.
We would expectthat WSD results using selectional preferences would be better for the latter class ofverbs.
Second, we do not use manually produced parses, but the output from our fullyautomatic shallow parser.
Third and finally, the baselines reported for Resnik?s test setwere higher than those for the all-words task.
For Resnik?s test data, the random base-line was 28.5%, whereas for the polysemous nouns in the direct-object relation on theall-words task, it was 23.9%.
The distribution of senses was also perhaps more skewedfor Resnik?s test set, since the first sense heuristic was 82.8% (Abney and Light 1999),whereas it was 53.6% for the polysemous direct objects in the all-words task.
Althoughour results do show that the precision for the TCMs compares favorably with that ofother unsupervised systems on the English all-words task, it would be worthwhile tocompare other selectional preference models on the same data.Although the accuracy of our system is encouraging given that it does not usehand-tagged data, the results are below the level of state-of-the-art supervised systems.Indeed, a system just assigning to each word its most frequent sense as listed inWordNet (the ?first-sense heuristic?)
would do better than our preference models(and in fact better than the majority of the SENSEVAL-2 English all-words supervisedsystems).
The first-sense heuristic, however, assumes the existence of sense-tagged datathat are able to give a definitive first sense.
We do not use any first-sense information.Although a modest amount of sense-tagged data is available for English (Miller et al1993, Ng and Lee 1996), for other languages with minimal sense-tagged resources, theheuristic is not applicable.
Moreover, for some words the predominant sense variesdepending on the domain and text type.To quantify this, we carried out an analysis of the polysemous nouns, verbs,and adjectives in SemCor occurring in more than one SemCor file and found thata large proportion of words have a different first sense in different files and alsoin different genres (see Table 4).
For adjectives there seems to be a lot less ambi-651McCarthy and Carroll Disambiguating Using Selectional PreferencesTable 4Percentages of words with a different predominant sense in SemCor, across files and genres.File GenreNouns 70 66Verbs 79 74Adjectives 25 21guity (this has also been noted by Krovetz [1998]; the data in SENSEVAL-2 bearthis out, with many adjectives occurring only in their first sense.
For nouns andverbs, for which the predominant sense is more likely to vary among texts, it wouldbe worthwhile to try to detect words for which using the predominant sense isnot a reliable strategy, for example, because the word shows ?bursty?
topic-relatedbehavior.We therefore examined our disambiguation results to see if there was any patternin the predicates or arguments that were easily disambiguated themselves or weregood disambiguators of the co-occurring word.
No particular patterns were evidentin this respect, perhaps because of the small size of the test data.
There were nounssuch as team (precision= 22 ) and cancer (810 ) that did better than average, but whetheror not they did better than the first-sense heuristic depends of course on the sensein which they are used.
For example, all 10 occurrences of cancer are in the firstsense, so the first sense heuristic is impossible to beat in this case.
For the test itemsthat are not in their first sense, we beat the first-sense heuristic, but on the otherhand, we failed to beat the random baseline.
(The random baseline is 21.8% andwe obtained 21.4% for these items overall.)
Our performance on these items is lowprobably because they are lower-frequency senses for which there is less evidencein the untagged training corpus (the BNC).
We believe that selectional preferenceswould perform best if they were acquired from similar training data to that for whichdisambiguation is required.
In the future, we plan to investigate our models for WSDin specific domains, such as sport and finance.
The senses and frequency distributionof senses for a given domain will in general be quite different from those in a balancedcorpus.There are individual words that are not used in the first sense on which our TCMpreferences do well, for example sound (precision = 22 ), but there are not enough datato isolate predicates or arguments that are good disambiguators from those that arenot.
We intend to investigate this issue further with the SENSEVAL-2 lexical sampledata, which contains more instances of a smaller number of words.Performance of selectional preferences depends not just on the actual word beingdisambiguated, but the cohesiveness of the tuple <pred, arg, gr>.
We have thereforeinvestigated applying a threshold on the probability of the class (nc, vc, or ac) beforedisambiguation.
Figure 7 presents a graph of precision against threshold applied to theprobability estimate for the highest-scoring class.
We show alongside this the randombaseline and the first-sense heuristic for these items.
Selectional preferences appear todo better on items for which the probability predicted by our model is higher, but thefirst-sense heuristic does even better on these.
The first sense heuristic, with respect toSemCor, outperforms the selectional preferences when it is averaged over a given text.That seems to be the case overall, but there will be some words and texts for whichthe first sense from SemCor is not relevant, and use of a threshold on probability, andperhaps a differential between probability of the top-ranked senses suggested by themodel, should increase precision.652Computational Linguistics Volume 29, Number 400.20.40.60.810 0.002 0.004 0.006 0.008 0.01precisionthreshold"first sense""TCMs""random"Figure 7Thresholding the probability estimate for the highest-scoring class.Table 5Lemma/file combinations in SemCor with more than one sense evident.Nouns 23%Verbs 19%Adjectives 1.6%6.2 The OSPD HeuristicIn these experiments we applied the OSPD heuristic to increase coverage.
One problemin doing this when using a fine-grained classification like WordNet is that althoughthe OSPD heuristic works well for homonyms, it is less accurate for related senses(Krovetz 1998), and this distinction is not made in WordNet.
We did, however, findthat in SemCor, for the majority of polysemous11 lemma and file combinations, therewas only one sense exhibited (see Table 5).
We refrained from using the OSPD insituations in which there was conflicting evidence regarding the appropriate sense fora word type occurring more than once in an individual file.
In our experiments theOSPD heuristic increased coverage by 7% and recall by 3%, at a cost of only a 1%decrease in precision.7.
ConclusionWe quantified coverage and accuracy of sense disambiguation of verbs, adjectives,and nouns in the SENSEVAL-2 English all-words test corpus, using automaticallyacquired selectional preferences.
We improved coverage and recall by applying theone-sense-per-discourse heuristic.
The results show that disambiguation models usingonly selectional preferences can perform with accuracy well above the random base-line, although accuracy would not be high enough for applications in the absence of11 Krovetz just looked at ?actual ambiguity,?
that is, words with more than one sense in SemCor.
Wedefine polysemy as those words having more than one sense in WordNet, since we are usingSENSEVAL-2 data, and not SemCor.653McCarthy and Carroll Disambiguating Using Selectional Preferencesother knowledge sources (Stevenson and Wilks 2001).
The results compare well withthose for other systems that do not use sense-tagged training data.Selectional preferences work well for some word combinations and grammaticalrelationships, but not well for others.
We hope in future work to identify the situationsin which selectional preferences have high precision and to focus on these at theexpense of coverage, on the assumption that other knowledge sources can be usedwhere there is not strong evidence from the preferences.
The first-sense heuristic, basedon sense-tagged data such as that available in SemCor, seems to beat unsupervisedmodels such as ours.
For many words, however, the predominant sense varies acrossdomains, and so we contend that it is worth concentrating on detecting when thefirst sense is not relevant, and where the selectional-preference models provide a highprobability for a secondary sense.
In these cases evidence for a sense can be taken frommultiple occurrences of the word in the document, using the one-sense-per-discourseheuristic.AcknowledgmentsThis work was supported by UK EPSRCproject GR/N36493 ?Robust AccurateStatistical Parsing (RASP)?
and EU FW5project IST-2001-34460 ?MEANING.?
We aregrateful to Rob Koeling and threeanonymous reviewers for their helpfulcomments on earlier drafts.
We would alsolike to thank David Weir and MarkMclauchlan for useful discussions.ReferencesAbney, Steven and Marc Light.
1999.
Hidinga semantic class hierarchy in a Markovmodel.
In Proceedings of the ACL Workshopon Unsupervised Learning in NaturalLanguage Processing, pages 1?8.Agirre, Eneko and David Martinez.
2001.Learning class-to-class selectionalpreferences.
In Proceedings of the FifthWorkshop on Computational LanguageLearning (CoNLL-2001), pages 15?22.Atkins, Sue.
1993.
Tools for computer-aidedlexicography: The Hector project.
InPapers in Computational Lexicography:COMPLEX 93, Budapest.Briscoe, Ted and John Carroll.
1993.Generalised probabilistic LR parsing ofnatural language (corpora) withunification-based grammars.Computational Linguistics, 19(1):25?59.Briscoe, Ted and John Carroll.
1995.Developing and evaluating a probabilisticLR parser of part-of-speech andpunctuation labels.
In fourthACL/SIGPARSE International Workshop onParsing Technologies, pages 48?58, Prague,Czech Republic.Carroll, John, Ted Briscoe, and AntonioSanfilippo.
1998.
Parser evaluation: Asurvey and a new proposal.
In Proceedingsof the International Conference on LanguageResources and Evaluation, pages 447?454.Carroll, John, Guido Minnen, and TedBriscoe.
1999.
Corpus annotation forparser evaluation.
In EACL-99Post-conference Workshop on LinguisticallyInterpreted Corpora, pages 35?41, Bergen,Norway.Carroll, John, Guido Minnen, Darren Pearce,Yvonne Canning, Siobhan Devlin, andJohn Tait.
1999.
Simplifying English textfor language impaired readers.
InProceedings of the Ninth Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 269?270,Bergen, Norway.Ciaramita, Massimiliano and Mark Johnson.2000.
Explaining away ambiguity:Learning verb selectional preference withBayesian networks.
In Proceedings of the18th International Conference ofComputational Linguistics (COLING-00),pages 187?193.Cotton, Scott, Phil Edmonds, AdamKilgarriff, and Martha Palmer.
2001.SENSEVAL-2.
Available at http://www.sle.sharp.co.uk/senseval2/.Elworthy, David.
1994.
Does Baum-Welchre-estimation help taggers?
In Proceedingsof the fourth ACL Conference on AppliedNatural Language Processing, pages 53?58,Stuttgart, Germany.Federici, Stefano, Simonetta Montemagni,and Vito Pirrelli.
1999.
SENSE: Ananalogy-based word sensedisambiguation system.
Natural LanguageEngineering, 5(2):207?218.Fellbaum, Christiane, editor.
1998.
WordNet,An Electronic Lexical Database.
MIT Press,Cambridge, MA.Gale, William, Kenneth Church, and DavidYarowsky.
1992.
A method fordisambiguating word senses in a largecorpus.
Computers and the Humanities,654Computational Linguistics Volume 29, Number 426:415?439.Krovetz, Robert.
1998.
More than one senseper discourse.
In Proceedings of theACL-SIGLEX SENSEVAL Workshop.Available at http://www.itri.bton.ac.uk/events/senseval/ARCHIVE/PROCEEDINGS/.Li, Hang and Naoki Abe.
1995.
Generalizingcase frames using a thesaurus and theMDL principle.
In Proceedings of theInternational Conference on Recent Advancesin Natural Language Processing, pages239?248, Bulgaria.Li, Hang and Naoki Abe.
1998.
Generalizingcase frames using a thesaurus and theMDL principle.
Computational Linguistics,24(2):217?244.Light, Marc and Warren Greiff.
2002.Statistical models for the induction anduse of selectional preferences.
CognitiveScience, 26(3):269?281.McCarthy, Diana.
1997.
Word sensedisambiguation for acquisition ofselectional preferences.
In Proceedings ofthe ACL/EACL 97 Workshop AutomaticInformation Extraction and Building of LexicalSemantic Resources for NLP Applications,pages 52?61.McCarthy, Diana.
2001.
Lexical Acquisition atthe Syntax-Semantics Interface: DiathesisAlternations, Subcategorization Frames andSelectional Preferences.
Ph.D. thesis,University of Sussex.Miller, George, A., Claudia Leacock, RandeeTengi, and Ross T. Bunker.
1993.
Asemantic concordance.
In Proceedings of theARPA Workshop on Human LanguageTechnology, pages 303?308.
MorganKaufmann.Minnen, Guido, John Carroll, and DarrenPearce.
2001.
Applied morphologicalprocessing of English.
Natural LanguageEngineering, 7(3):207?223.Ng, Hwee Tou and Hian Beng Lee.
1996.Integrating multiple knowledge sourcesto disambiguate word sense: Anexemplar-based approach.
In Proceedingsof the 34th Annual Meeting of the Associationfor Computational Linguistics, pages 40?47.Palmer, Martha, Hoa Trang Dang, andChristiane Fellbaum.
2003.
Makingfine-grained and coarse-grained sensedistinctions, both manually andautomatically.
Forthcoming, NaturalLanguage Engineering.Preiss, Judita and Anna Korhonen.
2002.Improving subcategorization acquisitionwith WSD.
In Proceedings of the ACLWorkshop on Word Sense Disambiguation:Recent Successes and Future Directions,Philadelphia, PA.Resnik, Philip.
1997.
Selectional preferenceand sense disambiguation.
In Proceedingsof the SIGLEX Workshop on Tagging Text withLexical Semantics: Why, What, and How?pages 52?57, Washington, DC.Ribas, Francesc.
1995.
On learning moreappropriate selectional restrictions.
InProceedings of the Seventh Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 112?118.Rissanen, Jorma.
1978.
Modelling byshortest data description.
Automatica,14:465?471.Stevenson, Mark and Yorick Wilks.
2001.The interaction of knowledge sources inword sense disambiguation.
ComputationalLinguistics, 17(3):321?349.
