Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 25?32,Rochester, New York, April 2007. c?2007 Association for Computational LinguisticsFactorization of Synchronous Context-Free Grammars in Linear TimeHao Zhang and Daniel GildeaComputer Science DepartmentUniversity of RochesterRochester, NY 14627AbstractFactoring a Synchronous Context-FreeGrammar into an equivalent grammar witha smaller number of nonterminals in eachrule enables synchronous parsing algo-rithms of lower complexity.
The prob-lem can be formalized as searching for thetree-decomposition of a given permutationwith the minimal branching factor.
In thispaper, by modifying the algorithm of Unoand Yagiura (2000) for the closely relatedproblem of finding all common intervalsof two permutations, we achieve a lineartime algorithm for the permutation factor-ization problem.
We also use the algo-rithm to analyze the maximum SCFG rulelength needed to cover hand-aligned datafrom various language pairs.1 IntroductionA number of recent syntax-based approaches tostatistical machine translation make use of Syn-chronous Context Free Grammar (SCFG) as the un-derlying model of translational equivalence.
Wu(1997)?s Inversion Transduction Grammar, as wellas tree-transformation models of translation such asYamada and Knight (2001), Galley et al (2004), andChiang (2005) all fall into this category.A crucial question for efficient computation in ap-proaches based on SCFG is the length of the gram-mar rules.
Grammars with longer rules can representa larger set of reorderings between languages (Ahoand Ullman, 1972), but also require greater compu-tational complexity for word alignment algorithmsbased on synchronous parsing (Satta and Peserico,2005).
Grammar rules extracted from large paral-lel corpora by systems such as Galley et al (2004)can be quite large, and Wellington et al (2006) ar-gue that complex rules are necessary by analyzingthe coverage of gold-standard word alignments fromdifferent language pairs by various grammars.However, parsing complexity depends not onlyon rule length, but also on the specific permutationsrepresented by the individual rules.
It may be possi-ble to factor an SCFG with maximum rule lengthn into a simpler grammar with a maximum of knonterminals in any one rule, if not all n!
permuta-tions appear in the rules.
Zhang et al (2006) discussmethods for binarizing SCFGs, ignoring the non-binarizable grammars; in Section 2 we discuss thegeneralized problem of factoring to k-ary grammarsfor any k and formalize the problem as permutationfactorization in Section 3.In Section 4, we describe an O(k ?
n) left-to-right shift-reduce algorithm for analyzing permuta-tions that can be k-arized.
Its time complexity be-comes O(n2) when k is not specified beforehandand the minimal k is to be discovered.
Instead oflinearly shifting in one number at a time, Gildeaet al (2006) employ a balanced binary tree as thecontrol structure, producing an algorithm similar inspirit to merge-sort with a reduced time complex-ity of O(n logn).
However, both algorithms relyon reduction tests on emerging spans which involveredundancies with the spans that have already beentested.25Uno and Yagiura (2000) describe a clever algo-rithm for the problem of finding all common inter-vals of two permutations in time O(n + K), whereK is the number of common intervals, which canitself be ?(n2).
In Section 5, we adapt their ap-proach to the problem of factoring SCFGs, and showthat, given this problem definition, running time canbe improved to O(n), the optimum given the timeneeded to read the input permutation.The methodology in Wellington et al (2006) mea-sures the complexity of word alignment using thenumber of gaps that are necessary for their syn-chronous parser which allows discontinuous spansto succeed in parsing.
In Section 6, we provide amore direct measurement using the minimal branch-ing factor yielded by the permutation factorizationalgorithm.2 Synchronous CFG and SynchronousParsingWe begin by describing the synchronous CFG for-malism, which is more rigorously defined by Ahoand Ullman (1972) and Satta and Peserico (2005).We adopt the SCFG notation of Satta and Peserico(2005).
Superscript indices in the right-hand side ofgrammar rules:X ?
X(1)1 ...X(n)n , X(pi(1))pi(1) ...X(pi(n))pi(n)indicate that the nonterminals with the same indexare linked across the two languages, and will eventu-ally be rewritten by the same rule application.
EachXi is a variable which can take the value of any non-terminal in the grammar.We say an SCFG is n-ary if and only if the max-imum number of co-indexed nonterminals, i.e.
thelongest permutation contained in the set of rules, isof size n.Given a synchronous CFG and a pair of inputstrings, we can apply a generalized CYK-style bot-tom up chart parser to build synchronous parsetrees over the string pair.
Wu (1997) demonstratesthe case of binary SCFG parsing, where six stringboundary variables, three for each language as inmonolingual CFG parsing, interact with each other,yielding an O(N6) dynamic programming algo-rithm, where N is the string length, assuming thetwo paired strings are comparable in length.
For ann-ary SCFG, the parsing complexity can be as highas O(Nn+4).
The reason is even if we binarize onone side to maintain 3 indices, for many unfriendlypermutations, at most n + 1 boundary variables inthe other language are necessary.The fact that this bound is exponential in the rulelength n suggests that it is advantageous to reducethe length of grammar rules as much as possible.This paper focuses on converting an SCFG to theequivalent grammar with smallest possible maxi-mum rule size.
The algorithm processes each rulein the input grammar independently, and determineswhether the rule can be factored into smaller SCFGrules by analyzing the rule?s permutation pi.As an example, given the input rule:[X ?
A(1)B(2)C(3)D(4)E(5)F (6)G(7),X ?
E(5)G(7)D(4)F (6)C(3)A(1)B(2) ] (1)we consider the associated permutation:(5, 7, 4, 6, 3, 1, 2)We determine that this permutation can be fac-tored into the following permutation tree:(2,1)(2,1)(2,4,1,3)5 7 4 63(1,2)1 2We define permutation trees formally in the nextsection, but note here that nodes in the tree corre-spond to subsets of nonterminals that form a sin-gle continuous span in both languages, as shown bythe shaded regions in the permutation matrix above.This tree can be converted into a set of output rulesthat are generatively equivalent to the original rule:[X ?
X(1)1 X(2)2 , X ?
X(2)2 X(1)1 ][X1 ?
A(1)B(2), X1 ?
A(1)B(2) ][X2 ?
C(1)X(2)3 , X2 ?
X(2)3 C(1) ][X3 ?
D(1)E(2)F (3)G(4),X3 ?
E(2)G(4)D(1)F (3) ]where X1, X2 and X3 are new nonterminals used torepresent the intermediate states in which the syn-chronous nodes are combined.
The factorized gram-mar is only larger than the original grammar by aconstant factor.263 Permutation TreesWe define the notion of permutation structure in thissection.
We define a permuted sequence as a per-mutation of n (n ?
1) consecutive natural numbers.A permuted sequence is said to be k-ary parsableif either of the following conditions holds:1.
The permuted sequence only has one number.2.
It has more than one number and can be seg-mented into k?
(k ?
k?
?
2) permuted se-quences each of which is k-ary parsable, andthe k?
subsequences are arranged in an orderidentified by one of the k?!
permutations of k?.This is a recursive definition, and we call the cor-responding recursive structure over the entire se-quence a k-ary permutation tree.Our goal is to find out the k-ary permutation treefor a given permutation, where k is minimized.4 Shift-reduce on PermutationsIn this section, we present an O(n ?
k) algorithmwhich can be viewed as a need-to-be-optimized ver-sion of the linear time algorithm to be presented inthe next section.The algorithm is based on a shift-reduce parser,which maintains a stack for subsequences that havebeen discovered so far and loops over shift and re-duce steps:1.
Shift the next number in the input permutationonto the stack.2.
Go down the stack from the top to the bottom.Whenever the top m subsequences satisfy thepartition property, which says the total lengthof the m (k ?
m ?
2) subsequences minus 1is equal to the difference between the smallestnumber and the largest number contained in them segments, make a reduction by gluing them segments into one subsequence and restartreducing from the top of the new stack.
Stopwhen no reduction is possible.3.
If there are remaining numbers in the input per-mutation, go to 1.When we exit from the loop, if the height of the stackis 1, the input permutation of n has been reduced toStack Input Operation5, 7, 4, 6, 3, 1, 2 shift5 7, 4, 6, 3, 1, 2 shift5, 7 4, 6, 3, 1, 2 shift5, 7, 4 6, 3, 1, 2 shift5, 7, 4, 6 3, 1, 2 reduce by (2,4,1,3)[4...7] 3, 1, 2 shift[4...7], 3 1, 2 reduce by (2,1)[3...7] 1, 2 shift[3...7], 1 2 shift[3...7], 1, 2 reduce by (1,2)[3...7], [1...2] reduce by (2,1)[1...7]Table 1: The execution trace of the shift-reduceparser on the input permutation 5, 7, 4, 6, 3, 1, 2.a linear sequence of 1 to n, and parsing is success-ful.
Otherwise, the input permutation of n cannot beparsed into a k-ary permutation tree.An example execution trace of the algorithm isshown in Table 1.The partition property is a sufficient and neces-sary condition for the top m subsequences to be re-ducible.
In order to check if the property holds, weneed to compute the sum of the lengths of subse-quences under consideration and the difference be-tween the largest and smallest number in the cov-ered region.
We can incrementally compute bothalong with each step going down the stack.
If mis bounded by k, we need O(k) operations for eachitem shifted onto the stack.
So, the algorithm runs inO(n ?
k).We might also wish to compute the minimum kfor which k-arization can be successful on an inputpermutation of n. We can simply keep doing reduc-tion tests for every possible top region of the stackwhile going deeper in the stack to find the minimalreduction.
In the worst case, each time we go downto the bottom of the increasingly higher stack with-out a successful reduction.
Thus, in O(n2), we canfind the minimum k-arization.5 Linear Time FactorizationIn this section, we show a linear time algorithmwhich shares the left-to-right and bottom-up controlstructure but uses more book-keeping operations toreduce unnecessary reduction attempts.
The reasonthat our previous algorithm is asymptotically O(n2)27is that whenever a new number is shifted in, we haveto try out every possible new span ending at the newnumber.
Do we need to try every possible span?
Letus start with a motivating example.
The permutedsequence (5, 7, 4, 6) in Table 1 can only be reducedas a whole block.
However, in the last algorithm,when 4 is shifted in, we make an unsuccessful at-tempt for the span on (7, 4), knowing we are miss-ing 5, which will not appear when we expand thespan no matter how much further to the right.
Yetwe repeat the same mistake to try on 7 when 6 isscanned in by attempting on (7, 4, 6).
Such wastefulchecks result in the quadratic behavior of the algo-rithm.
The way the following algorithm differs fromand outperforms the previous algorithm is exactlythat it crosses out impossible candidates for reduc-tions such as 7 in the example as early as possible.Now we state our problem mathematically.
Wedefine a function whose value indicates the re-ducibility of each pair of positions (x, y) (1 ?
x ?y ?
n):f(x, y) = u(x, y)?
l(x, y)?
(y ?
x)wherel(x, y) = mini?
[x,y]pi(i)u(x, y) = maxi?
[x,y]pi(i)l records the minimum of the numbers that arepermuted to from the positions in the region [x, y].u records the maximum.
Figure 1 provides the vi-sualization of u, l, and f for the example permuta-tion (5, 7, 4, 6, 3, 1, 2).
u and l can be visualized asstairs.
u goes up from the right end to the left.
lgoes down.
f is non-negative, but not monotonicin general.
We can make a reduction on (x, y) ifand only if f(x, y) = 0.
This is the mathemati-cal statement of the partition property in step 2 ofthe shift-reduce algorithm.
u and l can be computedincrementally from smaller spans to larger spans toguarantee O(1) operations for computing f on eachnew span of [x, y] as long as we go bottom up.
In thenew algorithm, we will reduce the size of the searchspace of candidate position pairs (x, y) to be linearin n so that the whole algorithm is O(n).The algorithm has two main ideas:?
We filter x?s to maintain the invariant thatf(x, y) (x ?
y) is monotonically decreasingwith respect to x, over iterations on y (from 1to n), so that any remaining values of x corre-sponding to valid reductions are clustered at thepoint where f tails off to zero.
To put it anotherway, we never have to test invalid reductions,because the valid reductions have been sortedtogether for us.?
We make greedy reductions as in the shift-reduce algorithm.In the new algorithm, we use a doubly linked list,instead of a stack, as the data structure that storesthe candidate x?s to allow for more flexible main-taining operations.
The steps of the algorithm are asfollows:1.
Increase the left-to-right index y by one and ap-pend it to the right end of the list.2.
Find the pivot x?
in the list which is minimum(leftmost) among x satisfying either u(x, y ?1) < u(x, y) (exclusively) or l(x, y ?
1) >l(x, y).3.
Remove those x?s that yield even smalleru(x, y ?
1) than u(x?, y ?
1) or even largerl(x, y ?
1) than l(x?, y ?
1).
Those x?s mustbe on the right of x?
if they exist.
They mustform a sub-list extending to the right end of theoriginal x list.4.
Denote the x which is immediately to the leftof x?
as x?.
Repeatedly remove all x?s such thatf(x, y) > f(x?, y) where x is at the left end ofthe sub-list of x?s starting from x?
extending tothe right.5.
Go down the pruned list from the right end, out-put (x, y) until f(x, y) > 0.
Remove x?s suchthat f(x, y) = 0, sparing the smallest x whichis the leftmost among all such x?s on the list.6.
If there are remaining numbers in the input per-mutation, go to 1.The tricks lie in step 3 and step 4, where bad can-didate x?s are filtered out.
We use the following di-agram to help readers understand the parts of x-listthat the two steps are filtering on.28x1, ..., x?,step 4?
??
?x?, ..., xi, ..., xj , ..., xk?
??
?step 3, yThe steps from 2 to 4 are the operations that main-tain the monotonic invariant which makes the reduc-tions in step 5 as trivial as performing output.
Thestack-based shift-reduce algorithm has the same top-level structure, but lacks steps 2 to 4 so that in step 5we have to winnow the entire list.
Both algorithmsscan left to right and examine potential reductionspans by extending the left endpoint from right toleft given a right endpoint.5.1 Example Execution TraceAn example of the algorithm?s execution is shownin Figure 1.
The evolution of u(x, y), l(x, y), andf(x, y) is displayed for increasing y?s (from 2 to 7).To identify reducible spans, we can check the plot off(x, y) to locate the (x, y) pairs that yield zero.
Thepivots found by step 2 of the algorithm are markedwith ?
?s on the x-axis in the plot for u and l. The x?sthat are filtered out by step 3 or 4 are marked withhorizontal bars across.
We want to point out the in-teresting steps.
When y = 3, x?
= 1, x = 2 needsto be crossed out by step 3 in the algorithm.
Wheny = 4, x?
= 3, x = 3 itself is to be deleted by step 4in the algorithm.
x = 4 is removed at step 5 becauseit is the right end in the first reduction.
On the otherhand, x = 4 is also a bad starting point for futurereductions.
Notice that we also remove x = 5 atstep 6, which can be a good starting point for reduc-tions.
But we exclude it from further considerations,because we want left-most reductions.5.2 CorrectnessNow we explain why the algorithm works.
Both al-gorithms are greedy in the sense that at each scanpoint we exhaustively reduce all candidate spans tothe leftmost possible point.
It can be shown thatgreediness is safe for parsing permutations.What we need to show is how the monotonic in-variant holds and is valid.
Now we sketch the proof.We want to show for all xi remaining on the list,f(xi, y) ?
f(xi+1, y).
When y = 1, it is triviallytrue.
Now we do the induction on y step by caseanalysis:Case 1: If xi < xi+1 < x?, then f(xi, y) ?f(xi, y ?
1) = ?1.
The reason is if xi is on theleft of x?, both u(xi, y) and l(xi, y) are not changedfrom the y ?
1-th step, so the only difference is thaty?xi has increased by one.
Graphically, the f curveextending to the left of x?
shifts down a unit of 1.
So,the monotonic property still holds to the left of x?.Case 2: If x?
?
xi < xi+1, then f(xi, y) ?f(xi, y ?
1) = c (c ?
0).
The reason is that afterexecuting step 3 in the algorithm, the remaining xi?shave either their u(xi, y) shifted up uniformly withl(xi, y) being unchanged, or the symmetric case thatl(xi, y) is shifted down uniformly without changingu(xi, y).
In both cases, the difference between u andl increases by at least one unit to offset the one unitincrease of y ?
xi.
The result is that the f curve ex-tending from x?
to the right shifts up or remains thesame.Case 3: So the half curve of f on the left of x?
isshifting down and the half right curve on the right isshifting up, making it necessary to consider the casethat xi is on the left and xi+1 on the right.
Fortu-nately, step 4 in the algorithm deals with this caseexplicitly by cutting down the head of the right halfcurve to smooth the whole curve into a monotoni-cally decreasing one.We still need one last piece for the proof, i.e., thevalidity of pruning.
Is it possible we winnow offgood x?s that will become useful in later stages ofy?
The answer is no.
The values we remove in step3 and 4 are similar to the points indexing into thesecond and third numbers in the permuted sequence(5, 7, 4, 6).
Any span starting from these two pointswill not be reducible because the element 5 is miss-ing.1To summarize, we remove impossible left bound-aries and keep good ones, resulting in the mono-tonicity of f function which in turn makes safegreedy reductions fast.5.3 Implementation and Time AnalysisWe use a doubly linked list to implement both the uand l functions, where list element includes a spanof x values (shaded rectangles in Figure 1).
Bothlists can be doubly linked with the list of x?s so that1Uno and Yagiura (2000) prove the validity of step 3 andstep 4 rigorously.29we can access the u function and l function at O(1)time for each x.
At the same time, if we search forx based on u or l, we can follow the stair functions,skipping many intermediate x?s.The total number of operations that occur at step4 and step 5 is O(n) since these steps just involveremoving nodes on the x list, and only n nodes arecreated in total over the entire algorithm.
To findx?, we scan back from the right end of u list or llist.
Due to step 3, each u (and l) element that wescan over is removed at this iteration.
So the totalnumber of operations accountable to step 2 and step3 is bounded by the maximum number of nodes evercreated on the u and l lists, which is also n.5.4 Related WorkOur algorithm is based on an algorithm for findingall common intervals of two permutations (Uno andYagiura, 2000).
The difference2 is in step 5, wherewe remove the embedded reducible x?s and keeponly the leftmost one; their algorithm will keep all ofthe reducible x?s for future considerations so that inthe example the number 3 will be able to involve inboth the reduction ([4?7], 3) and (3, [1?2]).
In theworst case, their algorithm will output a quadraticnumber of reducible spans, making the whole algo-rithm O(n2).
Our algorithm is O(n) in the worstcase.
We can also generate all common intervals bytransforming the permutation tree output by our al-gorithm.However, we are not the first to specialize the Unoand Yagiura algorithm to produce tree structures forpermutations.
Bui-Xuan et al (2005) reached a lin-ear time algorithm in the definition framework ofPQ trees.
PQ trees represent families of permuta-tions that can be created by composing operationsof scrambling subsequences according to any per-mutation (P nodes) and concatenating subsequencesin order (Q nodes).
Our definition of permutationtree can be thought of as a more specific version of aPQ tree, where the nodes are all labeled with a spe-cific permutation which is not decomposable.2The original Uno and Yagiura algorithm also has the minordifference that the scan point goes from right to left.6 Experiments on Analyzing WordAlignmentsWe apply the factorization algorithm to analyzingword alignments in this section.
Wellington et al(2006) indicate the necessity of introducing discon-tinuous spans for synchronous parsing to match upwith human-annotated word alignment data.
Thenumber of discontinuous spans reflects the struc-tural complexity of the synchronous rules that areinvolved in building the synchronous trees for thegiven alignments.
However, the more direct and de-tailed analysis would be on the branching factors ofthe synchronous trees for the aligned data.Since human-aligned data has many-to-one wordlinks, it is necessary to modify the alignments intoone-to-one.
Wellington et al (2006) treat many-to-one word links disjunctively in their synchronousparser.
We also commit to one of the many-one linksby extracting a maximum match (Cormen et al,1990) from the bipartite graph of the alignment.
Inother words, we abstract away the alternative linksin the given alignment while capturing the backboneusing the maximum number of word links.We use the same alignment data for the fivelanguage pairs Chinese/English, Romanian/English,Hindi/English, Spanish/English, and French/English(Wellington et al, 2006).
In Table 2, we report thenumber of sentences that are k-ary parsable but notk ?
1-ary parsable for increasing k?s.
Our analysisreveals that the permutations that are accountable fornon-ITG alignments include higher order permuta-tions such as (3, 1, 5, 2, 4), albeit sparsely seen.We also look at the number of terminals the non-binary synchronous nodes can cover.
We are in-terested in doing so, because this can tell us howgeneral these unfriendly rules are.
Wellington et al(2006) did a similar analysis on the English-Englishbitext.
They found out the majority of non-ITGparsable cases are not local in the sense that phrasesof length up to 10 are not helpful in covering thegaps.
We analyzed the translation data for the fivelanguage pairs instead.
Our result differs.
The right-most column in Table 2 shows that only a tiny per-cent of the non-ITG cases are significant in the sensethat we can not deal with them through phrases ortree-flattening within windows of size 10.30y = 2:1*1223344556677u, lx10213243546576fxy = 3:1*12?23344556677u, lx10213243546576fxy = 4:(112?23?
*34)4556677u, lx10213243546576fxy = 5:((1*12?23?34?
)45)56677u, lx10213243546576fxy = 6:((1*12?23?34?
)45)56677u, lx10213243546576fxy = 7:(((112?23?34?
)45)5(6*67))7u, lx10213243546576fxFigure 1: Evolution of u(x, y), l(x, y), and f(x, y) as y goes from 2 to 7 for the permutation(5, 7, 4, 6, 3, 1, 2).
We use ?
under the x-axis to indicate the x?
?s that are pivots in the algorithm.
Use-less x?s are crossed out.
x?s that contribute to reductions are marked with either ( on its left or ) on its right.For the f function, we use solid boxes to plot the values of remaining x?s on the list but also show the otherf values for completeness.31Branching Factor1 2 4 5 6 7 10 ?
4 (and covering > 10 words)Chinese/English 451 30 4 5 1 7(1.4%)Romanian/English 195 4 0Hindi/English 3 85 1 1 0Spanish/English 195 4 1(0.5%)French/English 425 9 9 3 1 6(1.3%)Table 2: Distribution of branching factors for synchronous trees on various language pairs.7 ConclusionWe present a linear time algorithm for factorizingany n-ary SCFG rule into a set of k-ary rules wherek is minimized.
The algorithm speeds up an easy-to-understand shift-reduce algorithm, by avoidingunnecessary reduction attempts while maintainingthe left-to-right bottom-up control structure.
Em-pirically, we provide a complexity analysis of wordalignments based on the concept of minimal branch-ing factor.ReferencesAlbert V. Aho and Jeffery D. Ullman.
1972.
The The-ory of Parsing, Translation, and Compiling, volume 1.Prentice-Hall, Englewood Cliffs, NJ.Binh Minh Bui-Xuan, Michel Habib, and ChristophePaul.
2005.
Revisiting T. Uno and M. Yagiura?s algo-rithm.
In The 16th Annual International Symposiumon Algorithms and Computation (ISAAC?05), pages146?155.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Conference of the Association forComputational Linguistics (ACL-05), pages 263?270,Ann Arbor, Michigan.Thomas H. Cormen, Charles E. Leiserson, and Ronald L.Rivest.
1990.
Introduction to algorithms.
MIT Press,Cambridge, MA.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of the Human Language Technology Confer-ence/North American Chapter of the Association forComputational Linguistics (HLT/NAACL).Daniel Gildea, Giorgio Satta, and Hao Zhang.
2006.
Fac-toring synchronous grammars by sorting.
In Proceed-ings of the International Conference on ComputationalLinguistics/Association for Computational Linguistics(COLING/ACL-06) Poster Session, Sydney.Giorgio Satta and Enoch Peserico.
2005.
Some com-putational complexity results for synchronous context-free grammars.
In Proceedings of Human Lan-guage Technology Conference and Conference onEmpirical Methods in Natural Language Processing(HLT/EMNLP), pages 803?810, Vancouver, Canada,October.Takeaki Uno and Mutsunori Yagiura.
2000.
Fast algo-rithms to enumerate all common intervals of two per-mutations.
Algorithmica, 26(2):290?309.Benjamin Wellington, Sonjia Waxmonsky, and I. DanMelamed.
2006.
Empirical lower bounds on thecomplexity of translational equivalence.
In Proceed-ings of the International Conference on ComputationalLinguistics/Association for Computational Linguistics(COLING/ACL-06).Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of the39th Annual Conference of the Association for Com-putational Linguistics (ACL-01), Toulouse, France.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for ma-chine translation.
In Proceedings of the Human Lan-guage Technology Conference/North American Chap-ter of the Association for Computational Linguistics(HLT/NAACL).32
