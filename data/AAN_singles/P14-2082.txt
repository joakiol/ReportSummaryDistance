Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 501?506,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsAn Annotation Framework for Dense Event OrderingTaylor CassidyIBM Researchtaylor.cassidy.ctr@mail.milBill McDowellCarnegie Mellon Universityforkunited@gmail.comNathanael ChambersUS Naval Academynchamber@usna.eduSteven BethardUniv.
of Alabama at Birminghambethard@cis.uab.eduAbstractToday?s event ordering research is heav-ily dependent on annotated corpora.
Cur-rent corpora influence shared evaluationsand drive algorithm development.
Partlydue to this dependence, most research fo-cuses on partial orderings of a document?sevents.
For instance, the TempEval com-petitions and the TimeBank only annotatesmall portions of the event graph, focusingon the most salient events or on specifictypes of event pairs (e.g., only events in thesame sentence).
Deeper temporal reason-ers struggle with this sparsity because theentire temporal picture is not represented.This paper proposes a new annotation pro-cess with a mechanism to force annotatorsto label connected graphs.
It generates 10times more relations per document than theTimeBank, and our TimeBank-Dense cor-pus is larger than all current corpora.
Wehope this process and its dense corpus en-courages research on new global modelswith deeper reasoning.1 IntroductionThe TimeBank Corpus (Pustejovsky et al, 2003)ushered in a wave of data-driven event orderingresearch.
It provided for a common dataset of re-lations between events and time expressions thatallowed the community to compare approaches.Later corpora and competitions have based theirtasks on the TimeBank setup.
This paper ad-dresses one of its shortcomings: sparse annotation.We describe a new annotation framework (and aTimeBank-Dense corpus) that we believe is neededto fulfill the data needs of deeper reasoners.The TimeBank includes a small subset of allpossible relations in its documents.
The annota-tors were instructed to label relations critical to thedocument?s understanding.
The result is a sparse la-beling that leaves much of the document unlabeled.The TempEval contests have largely followed suitand focused on specific types of event pairs.
Forinstance, TempEval (Verhagen et al, 2007) onlylabeled relations between events that syntacticallydominated each other.
This paper is the first attemptto annotate a document?s entire temporal graph.A consequence of focusing on all relations is ashift from the traditional classification task, wherethe system is given a pair of events and asked onlyto label the type of relation, to an identification task,where the system must determine for itself whichevents in the document to pair up.
For example, inTempEval-1 and 2 (Verhagen et al, 2007; Verha-gen et al, 2010), systems were given event pairsin specific syntactic positions: events and times inthe same noun phrase, main events in consecutivesentences, etc.
We now aim for a shift in the com-munity wherein all pairs are considered candidatesfor temporal ordering, allowing researchers to askquestions such as: how must algorithms adapt tolabel the complete graph of pairs, and if the moredifficult and ambiguous event pairs are included,how must feature-based learners change?We are not the first to propose these questions,but this paper is the first to directly propose themeans by which they can be addressed.
The statedgoal of TempEval-3 (UzZaman et al, 2013) was tofocus on relation identification instead of classifica-tion, but the training and evaluation data followedthe TimeBank approach where only a subset ofevent pairs were labeled.
As a result, many systemsfocused on classification, with the top system clas-sifying pairs in only three syntactic constructions501There were four or five people inside,and they just started firingMs.
Sanders was hit several times andwas  pronounced dead at the scene.The other customers fled, and thepolice said it did not appear that anyoneelse was injured.There were four or five people inside,and they just started firingMs.
Sanders was hit several times andwas pronounced dead at the scene.The other customers fled, and thepolice said it did not appear that anyoneelse was injured.Current Systems & Evaluations This ProposalFigure 1: A TimeBank annotated document is on the left, and this paper?s TimeBank-Dense annotation ison the right.
Solid arrows indicate BEFORE relations and dotted arrows indicate INCLUDED IN relations.
(Bethard, 2013).
We describe the first annotationframework that forces annotators to annotate allpairs1.
With this new process, we created a denseordering of document events that can properly eval-uate both relation identification and relation anno-tation.
Figure 1 illustrates one document beforeand after our new annotations.2 Previous Annotation WorkThe majority of corpora and competitions for eventordering contain sparse annotations.
Annotators forthe original TimeBank (Pustejovsky et al, 2003)only annotated relations judged to be salient bythe annotator.
Subsequent TempEval competitions(Verhagen et al, 2007; Verhagen et al, 2010; Uz-Zaman et al, 2013) mostly relied on the TimeBank,but also aimed to improve coverage by annotatingrelations between all events and times in the samesentence.
However, event tokens that were men-tioned fewer than 20 times were excluded and onlyone TempEval task considered relations betweenevents in different sentences.
In practical terms, theresulting evaluations remained sparse.A major dilemma underlying these sparse tasksis that the unlabeled event/time pairs are ambigu-ous.
Each unlabeled pair holds 3 possibilities:1.
The annotator looked at the pair of events anddecided that no temporal relation exists.2.
The annotator did not look at the pair ofevents, so a relation may or may not exist.3.
The annotator failed to look at the pair ofevents, so a single relation may exist.Training and evaluation of temporal reasoners ishampered by this ambiguity.
To combat this, our1As discussed below, all pairs in a given window size.Events Times Rels RTimeBank 7935 1414 6418 0.7Bramsen 2006 627 ?
615 1.0TempEval-07 6832 1249 5790 0.7TempEval-10 5688 2117 4907 0.6TempEval-13 11145 2078 11098 0.8Kolomiyets-12 1233 ?
1139 0.9Do 20122324 232 3132 5.6This work 1729 289 12715 6.3Table 1: Events, times, relations and the ratio ofrelations to events + times (R) in various corpora.annotation adopts the VAGUE relation introducedby TempEval 2007, and our approach forces anno-tators to use it.
This is the only work that includessuch a mechanism.This paper is not the first to look into more denseannotations.
Bramsen et al (2006) annotated multi-sentence segments of text to build directed acyclicgraphs.
Kolomiyets et al (2012) annotated ?tem-poral dependency structures?, though they onlyfocused on relations between pairs of events.
Doet al (2012) produced the densest annotation, but?the annotator was not required to annotate all pairsof event mentions, but as many as possible?.
Thecurrent paper takes a different tack to annotationby requiring annotators to label every possible pairof events/times in a given window.
Thus this workis the first annotation effort that can guarantee itsevent/time graph to be strongly connected.Table 1 compares the size and density of ourcorpus to others.
Ours is the densest and it containsthe largest number of temporal relations.2Do et al (2012) reports 6264 relations, but this includesboth the relations and their inverses.
We thus halve the count5023 A Framework for Dense AnnotationFrameworks for annotating text typically have twoindependent facets: (1) the practical means of howto label the text, and (2) the higher-level rules aboutwhen something should be labeled.
The first isoften accomplished through a markup language,and we follow prior work in adopting TimeML here.The second facet is the focus of this paper: whenshould an annotator label an ordering relation?Our proposal starts with documents that have al-ready been annotated with events, time expressions,and document creation times (DCT).
The followingsentence serves as our motivating example:Police confirmed Friday that the bodyfound along a highway in San Juan be-longed to Jorge Hernandez.This sentence is represented by a 4 node graph (3events and 1 time).
In a completely annotated graphit would have 6 edges (relations) connecting thenodes.
In the TimeBank, from which this sentenceis drawn, only 3 of the 6 edges are labeled.The impact of these annotation decisions (i.e.,when to annotate a relation) can be significant.
Inthis example, a learner must somehow deal withthe 3 unlabeled edges.
One option is to assume thatthey are vague or ambiguous.
However, all 6 edgeshave clear well-defined ordering relations:belonged BEFORE confirmedbelonged BEFORE foundfound BEFORE confirmedbelonged BEFORE Fridayconfirmed IS INCLUDED IN Fridayfound IS INCLUDED IN Friday3Learning algorithms handle these unlabelededges by making incorrect assumptions, or by ig-noring large parts of the temporal graph.
Sev-eral models with rich temporal reasoners havebeen published, but since they require more con-nected graphs, improvement over pairwise classi-fiers have been minimal (Chambers and Jurafsky,2008; Yoshikawa et al, 2009).
This paper thusproposes an annotation process that builds densergraphs with formal properties that learners can relyon, such as locally complete subgraphs.3.1 Ensuring Dense GraphsWhile the ideal goal is to create a complete graph,the time it would take to hand-label n(n ?
1)/2for accurate comparison to other corpora.3Revealed by the previous sentence (not shown here).edges is prohibitive.
We approximate completenessby creating locally complete graphs over neigh-boring sentences.
The resulting event graph for adocument is strongly connected, but not complete.Specifically, the following edge types are included:1.
Event-Event, Event-Time, and Time-Timepairs in the same sentence2.
Event-Event, Event-Time, and Time-Timepairs between the current and next sentence3.
Event-DCT pairs for every event in the text4.
Time-DCT pairs for every time expression inthe textOur process requires annotators to annotate theabove edge types, enforced via an annotation tool.We describe the relation set and this tool next.3.1.1 Temporal RelationsThe TimeBank corpus uses 14 relations based onthe Allen interval relations.
The TempEval contestshave used a small set of relations (TempEval-1) andthe larger set of 14 relations (TempEval-3).
Pub-lished work has mirrored this trend, and differentgroups focus on different aspects of the semantics.We chose a middle ground between coarse andfine-grained distinctions for annotation, settling on6 relations: before, after, includes, is included, si-multaneous, and vague.
We do not adopt a morefine-grained set because we annotate pairs that arefar more ambiguous than those considered in previ-ous efforts.
Decisions between relations like beforeand immediately before can complicate an alreadydifficult task.
The added benefit of a corpus (orworking system) that makes fine-grained distinc-tions is also not clear.
We lean toward higher an-notator agreement with relations that have greaterseparation between their semantics4.3.1.2 Enforcing AnnotationImposing the above rules on annotators requiresautomated assistance.
We built a new tool thatreads TimeML formatted text, and computes theset of required edges.
Annotators are prompted toassign a label for each edge, and skipping edges isprohibited.5The tool is unique in that it includesa transitive reasoner that infers relations based onthe annotator?s latest annotations.
For example,4For instance, a relation like starts is a special case of in-cludes if events are viewed as open intervals, and immediatelybefore is a special case of before.
We avoid this overlap andonly use includes and before5Note that annotators are presented with pairs in order fromdocument start to finish, starting with the first two events.503if event e1IS INCLUDED in t1, and t1BEFOREe2, the tool automatically labels e1BEFORE e2.The transitivity inference is run after each inputlabel, and the human annotator cannot overridethe inferences.
This prohibits the annotator fromentering edges that break transitivity.
As a result,several properties are ensured through this process:the graph (1) is a strongly connected graph, (2) isconsistent with no contradictions, and (3) has allrequired edges labeled.
These 3 properties are newto all current ordering corpora.3.2 Annotation GuidelinesSince the annotation tool frees the annotators fromthe decision of when to label an edge, the focus isnow what to label each edge.
This section describesthe guidelines for dense annotation.The 80% confidence rule: The decision to labelan edge as VAGUE instead of a defined temporalrelation is critical.
We adopted an 80% rule that in-structed annotators to choose a specific non-vaguerelation if they are 80% confident that it was thewriter?s intent that a reader infer that relation.
Bynot requiring 100% confidence, we allow for alter-native interpretations that conflict with the chosenedge label as long as that alternative is sufficientlyunlikely.
In practice, annotators had different inter-pretations of what constitutes 80% certainty, andthis generated much discussion.
We mitigated thesedisagreements with the following rule.Majority annotator agreement: An edge?s la-bel is the relation that received a majority of an-notator votes, otherwise it is marked VAGUE.
If adocument has 2 annotators, both have to agree onthe relation or it is labeled VAGUE.
A documentwith 3 annotators requires 2 to agree.
This agree-ment rule acts as a check to our 80% confidencerule, backing off to VAGUE when decisions are un-certain (arguably, this is the definition of VAGUE).We also encouraged consistent labelings withguidelines inspired by Bethard and Martin (2008).Modal and conditional events: interpreted witha possible worlds analysis.
The core event wastreated as having occurred, whether or not the textimplied that it had occurred.
For example,They [EVENT expect] him to [EVENTcut] costs throughout the organization.This event pair is ordered (expect before cut) sincethe expectation occurs before the cutting (in thepossible world where the cutting occurs).
Negatedevents and hypotheticals are treated similarly.
Oneassumes the event does occur, and all other eventsare ordered accordingly.
Negated states like ?is notanticipating?
are interpreted as though the antici-pation occurs, and surrounding events are orderedwith regard to its presumed temporal span.Aspectual Events: annotated as IS INCLUDEDin their event arguments.
For instance, events thatdescribe the manner in which another event is per-formed are considered encompassed by the broaderevent.
Consider the following example:The move may [EVENT help] [EVENTprevent] Martin Ackerman from makinga run at the computer-services concern.This event pair is assigned the relation (help IS IN-CLUDED in prevent) because the help event is notmeaningful on its own.
It describes the proportionof the preventing accounted for by the move.
InTimeBank, the intentional action class is used in-stead of the aspectual class in this case, but we stillconsider it covered by this guideline.Events that attribute a property: to a personor event are interpreted to end when the entity ends.For instance, ?the talk is nonsense?
evokes a non-sense event with an end point that coincides withthe end of the talk.Time Expressions: the words now and todaywere given ?long now?
interpretations if the wordscould be replaced with nowadays and not changethe meaning of their sentences.
The time?s dura-tion starts sometime in the past and INCLUDES theDCT.
If nowadays is not suitable, then the now wasINCLUDED IN the DCT.Generic Events: can be ordered with respect toeach other, but must be VAGUE with respect tonearby non-generic events.4 TimeBank-Dense: corpus statisticsWe chose a subset of TimeBank documents for ournew corpus: TimeBank-Dense.
This provided aninitial labeling of events and time expressions.
Us-ing the tool described above, we annotated 36 ran-dom documents with at least two annotators each.These 36 were annotated with 4 times as manyrelations as the entire 183 document TimeBank.The four authors of this paper were the four an-notators.
All four annotated the same initial docu-ment, conflicts and disagreements were discussed,504Annotated Relation CountBEFORE 2590 INCLUDES 836AFTER 2104 INCLUDED IN 1060SIMULTAN.
215 VAGUE 5910Total Relations: 12715Table 2: Relation counts in TimeBank-Dense.and guidelines were updated accordingly.
The restof the documents were then annotated indepen-dently.
Document annotation was not random, butwe mixed pairs of authors where time constraints al-lowed.
Table 2 shows the relation counts in the finalcorpus, and Table 3 gives the annotator agreement.We show precision (holding one annotation as gold)and kappa computed on the 4 types of pairs fromsection 3.1.
Micro-averaged precision was 65.1%,compared to TimeBank?s 77%.
Kappa ranged from.56-.64, a slight drop from TimeBank?s .71.The vague relation makes up 46% of the rela-tions.
This is the first empirical count of how manytemporal relations in news articles are truly vague.Our lower agreement is likely due to the moredifficult task.
Table 5 breaks down the individualdisagreements.
The most frequent pertained to theVAGUE relation.
Practically speaking, VAGUE wasapplied to the final graph if either annotator choseit.
This seems appropriate since a disagreement be-tween annotators implies that the relation is vague.The following example illustrates the difficultyof labeling edges with a VAGUE relation:No one was hurt, but firefighters or-dered the evacuation of nearby homesand said they?ll monitor the ground.Both annotators chose VAGUE to label ordered andsaid because the order is unclear.
However, theydisagreed on evacuation with monitor.
One choseVAGUE, but the other chose IS INCLUDED.
There isa valid interpretation where a monitoring processhas already begun, and continues after the evacua-tion.
This interpretation reached 80% confidencefor one annotator, but not the other.
In the face ofsuch a disagreement, the pair is labeled VAGUE.How often do these disagreements occur?
Ta-ble 4 shows the 3 sources: (1) mutual vague: anno-tators agree it is vague, (2) partial vague: one anno-tator chooses vague, but the other does not, and (3)no vague: annotators choose conflicting non-vaguerelations.
Only 17% of these disagreements are dueto hard conflicts (no vague).
The released corpusincludes these 3 fine-grained VAGUE relations.Annotators # Links Prec KappaA and B 9282 .65 .56A and D 1605 .72 .63B and D 279 .70 .64C and D 1549 .65 .57Table 3: Agreement between different annotators.# VagueMutual VAGUE 1657 (28%)Partial VAGUE 3234 (55%)No VAGUE 1019 (17%)Table 4: VAGUE relation origins.
Partial vague:one annotator does not choose vague.
No vague:neither annotator chooses vague.b a i ii s vb 1776 22 88 37 21 192a 17 1444 32 102 9 155i 71 34 642 45 23 191ii 81 76 40 826 31 230s 12 8 25 28 147 29v 500 441 289 356 64 1197Table 5: Relation agreement between the two mainannotators.
Most disagreements involved VAGUE.5 ConclusionWe described our annotation framework that pro-duces corpora with formal guarantees about the an-notated graph?s structure.
Both the annotation tooland the new TimeBank-Dense corpus are publiclyavailable.6This is the first corpus with guaranteesof connectedness, consistency, and a semantics forunlabeled edges.
We hope to encourage a shift inthe temporal ordering community to consider theentire document when making local decisions.
Fur-ther work is needed to handle difficult pairs withthe VAGUE relation.
We look forward to evaluatingnew algorithms on this dense corpus.AcknowledgmentsThis work was supported, in part, by the JohnsHopkins Human Language Technology Center ofExcellence.
Any opinions, findings, and conclu-sions or recommendations expressed in this mate-rial are those of the authors.
We also give thanksto Benjamin Van Durme for assistance and insight.6http://www.usna.edu/Users/cs/nchamber/caevo/505ReferencesSteven Bethard, William J Corvey, Sara Klingenstein,and James H Martin.
2008.
Building a corpus oftemporal-causal structure.
In LREC.Steven Bethard.
2013.
Cleartk-timeml: A minimal-ist approach to tempeval 2013.
In Second JointConference on Lexical and Computational Seman-tics (*SEM), Volume 2: Proceedings of the SeventhInternational Workshop on Semantic Evaluation (Se-mEval 2013), pages 10?14, Atlanta, Georgia, USA,June.
Association for Computational Linguistics.P.
Bramsen, P. Deshpande, Y.K.
Lee, and R. Barzilay.2006.
Inducing temporal graphs.
In Proceedings ofthe 2006 Conference on Empirical Methods in Natu-ral Language Processing, pages 189?198.
ACL.N.
Chambers and D. Jurafsky.
2008.
Jointly com-bining implicit constraints improves temporal order-ing.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages698?706.
ACL.Quang Do, Wei Lu, and Dan Roth.
2012.
Joint infer-ence for event timeline construction.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 677?687, JejuIsland, Korea, July.
Association for ComputationalLinguistics.Oleksandr Kolomiyets, Steven Bethard, and Marie-Francine Moens.
2012.
Extracting narrative time-lines as temporal dependency structures.
In Proceed-ings of the 50th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 88?97, Jeju Island, Korea, July.
Associ-ation for Computational Linguistics.James Pustejovsky, Patrick Hanks, Roser Sauri, An-drew See, Robert Gaizauskas, Andrea Setzer,Dragomir Radev, Beth Sundheim, David Day, LisaFerro, et al 2003.
The timebank corpus.
In Corpuslinguistics, volume 2003, page 40.Naushad UzZaman, Hector Llorens, Leon Derczyn-ski, James Allen, Marc Verhagen, and James Puste-jovsky.
2013.
Semeval-2013 task 1: Tempeval-3:Evaluating time expressions, events, and temporalrelations.
In Second Joint Conference on Lexicaland Computational Semantics (*SEM), Volume 2:Proceedings of the Seventh International Workshopon Semantic Evaluation (SemEval 2013), pages 1?9,Atlanta, Georgia, USA, June.
Association for Com-putational Linguistics.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
Semeval-2007 task 15: Tempeval temporal re-lation identification.
In Proceedings of the 4th Inter-national Workshop on Semantic Evaluations, pages75?80.
Association for Computational Linguistics.Marc Verhagen, Roser Sauri, Tommaso Caselli, andJames Pustejovsky.
2010.
Semeval-2010 task 13:Tempeval-2.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, pages 57?62.
As-sociation for Computational Linguistics.K.
Yoshikawa, S. Riedel, M. Asahara, and Y. Mat-sumoto.
2009.
Jointly identifying temporal rela-tions with Markov Logic.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages405?413.
ACL.506
