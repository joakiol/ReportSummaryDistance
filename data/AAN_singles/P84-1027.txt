The Semant ics  of  Grammar  Formal i smsSeen  as Computer  LanguagesFernando C. N. Pereira and Stuart M. ShieberArtificial Intelligence CenterSRI InternationalandCenter for the Study of Language and InformationStanford UniversityAbst rac tThe design, implementation, and use of grammar for-ma\]isms for natural language have constituted a majorbranch of coml)utational linguistics throughout its devel-opment.
By viewing grammar formalisms as just a spe-cial ease of computer languages, we can take advantage ofthe machinery of denotational semantics to provide a pre-cise specification of their meaning.
Using Dana Scott's do-main theory, we elucidate the nature of the feature systemsused in augmented phrase-structure grammar formalisms,in particular those of recent versions of generalized phrasestructure grammar, lexical functional grammar and PATR-I1, and provide a (lcnotational semantics for a simple gram-mar formalism.
We find that the mathematical structuresdeveloped for this purpose contain an operation of featuregeneralization, ot available in those grammar formalisms,that can be used to give a partial account of the effect ofcoordination on syntactic features.1.
In t roduct ion  IThe design, implementation, and use of grammar for-malisms for natural lang,age have constituted a majorbranch of computational linguistics throughout its devel-opment.
Itowever, notwithstanding the obvious superfi-cial similarily between designing a grammar formalism anddesigning a programming language, the design techniquesused for grammar formalisms have almost always fallenshort with respect o those now available for programminglanguage design.Formal and computational linguists most often explainthe effect of a grammar formalism construct either by ex-ample or through its actual operation in a particular im-plementation.
Such practices are frowned upon by mostprogramming-language designers; they become even moredubious if one considers that most grammar formalismsin use are based either on a context-free skeleton withaugmentations or on some closely related device (such asATNs), consequently making them obvious candidates forIThe research reported in this paper has been made possible by a giftfrom the System Development Foundation.a declarative semantics z extended in the natural way fromthe declarative semantics of context-free grammars.The last point deserves amplification.
Context-freegrammars possess an obvious declarative semantics inwhich nonterminals represent sets of strings and rules rep-resent n-ary relations over strings.
This is brought out bythe reinterpretation familiar from formal language theoryof context-free grammars as polynomials over concatena-tion and set union.
The grammar formalisms developedfrom the definite-clause ubset of first order logic are theonly others used in natural-language analysis that havebeen accorded a rigorous declarative semantics--in thiscase derived from the declarative semantics of logic pro-grams \[3,12,1 I\].Much confusion, wasted effort, and dissension have re-sulted from this state of affairs.
In the absence of a rigoroussemantics for a given grammar formalism, the user, critic,or implementer of the formalism risks misunderstanding theintended interpretation f a construct, and is in a poor posi-tion to compare it to alternatives.
Likewise, the inventor ofa new formalism can never be sure of how it compares withexisting ones.
As an example of these dillqculties, two sim-ple changes in the implementation of the ATN formalism,the addition of a well-formed substring table and the useof a bottom-up parsing strategy, required a rather subtleand unanticipated reinterpretation f the register-testingand -setting actions, thereby imparting a different meaningto grammars that had been developed for initial top-downbacktrack implementation \[22\].Rigorous definitions of grammar formalisms can andshould be made available.
Looking at grammar formalismsas just a special case of computer languages, we can takeadvantage of the machinery of denotational semantics \[20 ito provide a precise specification of their meaning.
Thisapproach can elucidate the structure of the data objectsmanipulated by a formalism and the mathematical rela-tionships among various formalisms, suggest new possibil-ities for linguistic analysis (the subject matter of the for-malisms), and establish connections between grammar for-malisms and such other fields of research as programming-2This use of the term "semantics" should not be confused with themore common usage denoting that portion of a grammar concernedwith the meaning of object sentences.
Here we are concerned with themeaning of the metalanguage.123language design and theories of abstract data types.
Thislast point is particularly interesting because it opens upseveral possibilities--among them that of imposing a typediscipline on the use of a formalism, with all the attendantadvantages ofcompile-time error checking, modularity, andoptimized compilation techniques for grammar ules, andthat of relating grammar formalisms to other knowledgerepresentation languages \[l\].As a specific contribution of this study, we elucidatethe nature of the feature systems used in augmented phrase-structure grammar formalisms, in particular those of recentversions of generalized phrase structure grammar (GPSG)\[5,15\], lexical functional grammar (LFG) \[2\] and PATR-II\[ 18,17\]; we find that the mathematical structures developedfor this purpose contain an operation of feature generaliza-tion, not available in those grammar formalisms, that canbe used to give a partial account of the effect of coordina-tion on syntactic features.Just as studies in the semantics of programming lan-guages tart by giving semantics for simple languages, sowe will start with simple grammar formalisms that capturethe essence of the method without an excess of obscuringdetail.
The present enterprise should be contrasted withstudies of the generative capacity of formalisms using thetechniques of formal anguage theory.
First, a precise defini-!
;ion of the semantics of a formalism is a prerequisite for suchgenerative-capacity s udies, and this is precisely what weare trying to provide.
Second, generative capacity is a verycoarse gauge: in particular, it does not distinguish amongdifferent formalisms with the same generative capacity thatmay, however, have very different semantic accounts.
Fi-nally, the tools of formal language theory are inadequate todescribe at a sufficiently abstract level formalisms that arebased on the simultaneous solution of sets of constraints\[9,10\].
An abstract analysis of those formalisms requires anotion of partial information that is precisely captured bythe constructs of denotationai semantics.2.
Denotat iona l  Semant icsIn broad terms, denotational semantics i  the study ofthe connection between programs and mathematical enti-ties that represent their input-output relations.
For suchan account to be useful, it must be compositional, in thesense that the meaning of a program is developed from themeanings of its parts by a fixed set of mathematical oper-ations that correspond irectly to the ways in which theparts participate in the whole.For the purposes of the present work, denotational se-mantics will mean the semantic domain theory initiatedby Scott and Strachey \[20\].
In accordance with this ap-proach, the meanings of programming language constructsare certain partial mappings between objects that representpartially specified ata objects or partially defined states ofcomputation.
The essential idea is that the meaning of aconstruct describes what information it adds to a partialdescription of a data object or of a state of computation.Partial descriptions are used because computations in gen-eral may not terminate and may therefore never produce afully defined output, although each individual step may beadding more and more information to a partial descriptionof the undeliverable output.Domain theory is a mathematical theory of consider-able complexity.
Potential nontermination and the use offunctions as "first-class citizens" in computer languages ac-count for a substantial fraction of that complexity.
If, as isthe case in the present work, neither of those two aspectscomes into play, one may be justified in asking why sucha complex apparatus is used.
Indeed, both the semanticsof context-free grammars mentioned earlier and the seman-tics of logic grammars in general can be formulated usingelementary set theory \[7,21\].However, using the more complex machinery may bebeneficial for the following reasons:?
Inherent partiality:, many grammar formalisms oper-ate in terms of constraints between elements that donot fully specify all the possible features of an ele-ment.?
Technical economy, results that require laboriousconstructions without utilizing domain theory can bereached trivially by using standard results of the the-ory.?
Suggestiveness: domain theory brings with it a richmathematical structure that suggests useful opera-tions one might add to a grammar formalism.?
Eztensibilit~.
unlike a domain-theoretic a count, aspecialized semantic account, say in terms of sets,may not be easily extended as new constructs areadded to the formalism.3.
The Domain  of Feature Struc-turesWe will start with an abstract denotational descriptionof a simple feature system which bears a close resemblanceto the feature systems of GPSG, LFG and PATR-II, al-though this similarity, because of its abstractness, may notbe apparent at first glance.
Such feature systems tend touse data structures or mathematical objects that are moreor less isomorphic to directed graphs of one sort or an-other, or, as they are sometimes described, partial func-tions.
Just what the relation is between these two waysof viewing things will be explained later.
In general, thesegraph structures are used to encode linguistic informationin the form of attribute-vahm pairs.
Most importantly, par-tial information is critical to the use of such systems--forinstance, in the variables of definite clause grammars \[12\]and in the GPSG analysis of coordination \[15\].
That is, theelements of the feature systems, called fealure struclures(alternatively, feature bundles, f-structures \[2\], or terms}can be partial in some sense.
The partial descriptions, be-ing in a domain of attributes and complex values, tend to beequational in nature: some feature's value is equated withsome other value.
Partial descriptions can be understood124in one of two w:ays: either the descriptions represent setsof fully specilied elements of an underlying domain or theyare regarded as participating in a relationship of partialitywith respect o each other.
We will hold to the latter viewhere.What are feature structures from this perspective?They are repositories of information about linguistic enti-ties.
In domain-theoretic erms, the underlying domain offeature structures F is a recursive domain of partial func-tions from a set of labels L (features, attribute names, at-tributes) to complex values or primitive atomic values takenfrom a set C of constants.
Expressed formally, we have thedomain equationF=IL~F\ ]+GThe solution of this domain equation can be understood asa set of trees (finite or infinite} with branches labeled byelements of L, and with other trees or constants as nodes.The branches la .
.
.
.
, Im from a node n point to the valuesn{lt ) , .
.
.
,  n(Im) for which the node, as a partial function, isdefined.4.
The  Domain  o f  Descr ip t ionsWhat the grammar formalism does is to talk about F,not in F. That is, the grammar formalism uses a domain ofdescriptions ofelements ofF.
From an intuitive standpoint,this is because, for any given phrase, we may know factsabout it that cannot be encoded in the partial functionassociated with it..A partial description of an element n of F will be a setof equations that constrain the values of n on certain labels.In general, to describe an element z E F we have equationsof the following forms:(... (xII.
})-..)ll;.)
= (..-(z(li,))...)(l;.
)(" .
(x{l i , ) )" .
)( l i ,~) = ck ,which we prefer to write as(t~,...I;.)
= (Ij,..-i;.
)( l i ," ' l i=) = ckwith x implicit.
The terms of such equations are constantsc E C' or paths {ll, " .
It=), which we identify in what followswith strings in L*.
Taken together, constants and pathscomprise the descriptors.Using Scott's information systems approach to domainconstruction \[16\], we can now build directly a characteriza-tion of feature structures in terms of information-bearingelements, equations, that engender a system complete withnotions of compatibility and partiality of information.The information system D describing the elements ofF is defined, following Scott, as the tupleD = (/9, A, Con, ~-) ,where 19 is a set of propositions, Con is a set of finite subsetsof P, the consistent subsets, I- is an entailment relationbetween elements of Con and elements of D and A is aspecial least informative lement hat gives no informationat all.
We say that a subset S of D is deductively closedif every proposition entailed by a consistent subset of S isin S. The deductive closure -S of S ___ /9 is the smallestdeductively closed subset of/9 that contains S.The descriptor equations discussed earlier are thepropositions of the information system for feature structuredescriptions.
Equations express constraints among featurevalues in a feature structure and the entailment relationencodes the reflexivity, symmetry, transitivity and substi-tutivity of equality.
More precisely, we say that a finite setof equations E entails an equation e if?
Membership: e E E?
Reflezivit~.
e is A or d = d for some descriptor d?
Symmetry.
e is dl = d2 and dz = dl is in E?
Transitivity.
e is da = dz and there is a descriptor dsuch that dl = d and d = dz are in E?
Substitutivit~r.
e is dl = Pl ?
d2 and both pl = Pz anddl = P2 ?
d.~ are in E?
Iteration: there is E' C E such that E' b e and for alle 'E~ EF-e'With this notion of entailment, the most natural definitionof the set Con is that a finite subset E of 19 is consistent ifand only if it does not entail an inconsistent equation, whichhas the form e~ = cz, with et and Cz as distinct constants.An arbitrary subset of/9 is consistent if and only if allits finite subsets are consistent in the way defined above.The consistent and deductively closed subsets of D orderedby inclusion form a complete partial order or domain D,our domain of descriptions of feature structures.Deductive closure is used to define the elements of Dso that elements defined by equivalent sets of equations arethe same.
In the rest of this paper, we will specify elementsof D by convenient sets of equations, leaving the equationsin the closure implicit.The inclusion order K in D provides the notion ofa description being more or less specific than another.The least-upper-bound operation 12 combines two descrip-tions into the least instantiated description that satisfiesthe equations in both descriptions, their unification.
Thegreatest-lower-bound operation gives the most instanti-ated description containing all the equations common totwo descriptions, their generalization.The foregoing definition of consistency may seem verynatural, but it has the technical disadvantage that, in gen-eral, the union of two consistent sets is not itself a consistentset; therefore, the corresponding operation of unificationmay not be defined on certain pairs of inputs.
Althoughthis does not cause problems at this stage, it fails to dealwith the fact that failure to unify is not the same as lack ofdefinition and causes technical difficulties when providingrule denotations.
We therefore need a slightly less naturaldefinition.First we add another statement to the specification ofthe entailment relation:125?
Falsitv.
if e is inconsistent, {e} entails every elementof P.- That is, falsity entails anything.
Next we define Con to besimply the set of all finite subsets of P. The set Con nolonger corresponds to sets of equations that are consistentin the usual equational sense.With the new definitions of Con and I-, the deductiveclosure of a set containing an inconsistent equation is thewhole of P. The partial order D is now a lattice with topelement T = P, and the unification operation t_l is alwaysdefined and returns T on unification failure.We can now define the description mapping 6 : D --* Fthat relates descriptions to the described feature structures.The idea is that, in proceeding from a description d 6 D toa feature structure f 6 F, we keep only definite informa-tion about values and discard information that only statesvalue constraints, but does not specify the values them-selves.
More precisely, seeing d as a set of equations, weconsider only the subset LdJ of d with elements of the form( l~-.. lm)=c~ .
.Each e 6 \[d\] defines an element f(e) of F by the equationsf(e)(l,) = f,fi-,(li) ---- flf,._,(l,.)
= ek ,with each of the f~ undefined for all other labels.
Then, wecan define 6(d) as6(d) = L\] f(e)~eL~lThis description mapping can be shown to be continu-ous in the sense of domain theory, that is, it has the prop-erties that increasing information in a description leadsto nendecreasing information in the described structures{monotonieity) and that if a sequence of descriptions ap-proximates another description, the same condition holdsfor the described structures.Note that 6 may map several elements of D on to oneelement of F. For example, the elements given by the twosets of equations( fh )  = c (gi) = edescribe the same structure, because the description map-ping ignores the link between (f h) and (g i) in the firstdescription.
Such links are useful only when unifying withfurther descriptive lements, not in the completed featurestructure, which merely provides feature-value assignments.Informally, we can think of elements of D as directedrooted graphs and of elements of F as their unfoldings astrees, the unfolding being given by the mapping 6.
It isworth noting that if a description is cyclic---that is, if it hascycles when viewed as a directed graph--then the resultingfeature tree will be infinite2Stated more precisely, an element f of a domain is fi-nite, if for any ascending sequence {d~} such that f E_ U~ d~,there is an i such that f C_ d~.
Then the cyclic elementsof D are those finite elements that are mapped by 6 intononfinite lements of F.5.
P rov id ing  a Denotat ion  fo r  aGrammarWe now move on to the question of how the domain Dis used to provide a denotational semantics for a grammarformalism.We take a simple grammar formalism with rules con-sisting of a context-free part over a nonterminal vocabu-lary .
t /= {Nt, .
.
.
,  Ark} and a set of equations over paths in(\[0..c~\]- L*)0C.
A sample rule might beS ~ NP VP(o s,,bj) = (I)(o predicate) = (2)(1 agr) = (2 agr)This is a simplification of the rule format used in the PATR-II formalism \[18,17\].
The rule can be read as "an S is anNP followed by a VP, where the subject of the S is theNP, its predicate the VP, and the agreement of the NPthe same as the agreement of tile VP'.More formally, a grammar is a quintuple G =(//, S, L, C, R), where?
,t/is a finite, nonempty set of nonterminals Nt , .
.
.
,  Nk?
S is the set of strings over some alphabet (a fiat do-main with an ancillary continuous function concate-nation, notated with the symbol .).?
R is a set of pairs r = (/~0 ~ N,, .. .
N,.
,  E~),where E. is a set of equations between elements of(\[0..m\] - L ' )  0 C.As with context-free grammars, local ambiguity of agrammar means that in general there are several ways ofassembling the same subphrases into phra.ses.
Thus, thesemantics of context-free grammars is given in terms ofsets of strings.
The situation is somewhat more compli-cated in our sample formalism.
The objects specified bythe grammar are pairs of a string and a partial description.Because of partiality, the appropriate construction cannotbe given in terms of sets of string-description pairs, butrather in terms of the related domain construction of pow-erdomains \[14,19,16\].
We will use the Hoare powerdomainP = PM(S x D) of the domain S x D of string-descriptionpairs.
Each element of P is an approximation f a transdue-tion relation, which is an association between strings andtheir possible descriptions.We can get a feeling for what the domain P is doingby examinin~ our notion of lexicon.
A lexicon will be anSMote precisely a rational tree, that is, a tree with a finite number ofdistinct subtrees.126element of the domain pk, associating with each of the knonterminals N;, I < i < k a transduction relation from thecorresponding coordinate of pk.
Thus, for each nontermi-nal, the lexicon tells us what phrases are under that non-terminal and what possible descriptions each such phrasehas.
llere is a sample lexicon:NP :{"Uther", }{(agr n,tm) = sg, (agr per) = 3})("many knights",{ <agr num} = pl, (agr per) = 3})VP :("slorms Cornwall", }{( ,~,"  n,,.,) = sg})("sit at the Round Table",{(agr hum} = pl})s :  {}By decomposing the effect of a rule into appropriatesteps, we can associate with each rule r a denotationIr~ :P~ - - .
pkthat combines string-description pairs by concatenationand unification to build new string-description pairs for thenonterminal on the left-hand side of the rule, leaving allother nonterminals untouched?
By taking the union of thedenotations of the rules in a grammar, (which is a well-defined and continuous powerdomain operation,) we get amappingTG(e) d~j U H(e)reRfrom pk to pk that represents a one-step application of allthe rules of G "in parallel.
"We can now provide a denotation for the entire gram-mar as a mapping that completes a lexicon with all thederived phrases and their descriptions.
The denotation ofa grammar is the fimetion that maps each lexicon ~ into thesmallest fixed point of To containing e. The fixed point isdefined byi=Oas Tc is contimmus.It remains to describe the decomposition of a rule's ef-fect into elementary steps.
The main technicality to keep inmind is that rules stale constraints among several descrip-tions (associated with the parent and each child), whereasa set of equations in D constrains but a single descrip-tion.
This nfismateh is solved by embedding the tuple(do,..., d,,) of descriptions in a single larger description,as expressed by( i)  = di, 0 < i < r .and only then applying the rule constraints--now viewed asconstraining parts of a single description.
This is done bythe indexing and combination steps described below.
Therest of the work of applying a rule, extracting the result, isdone by the projection and deindcxing steps?The four steps for applying a ruler = (N , ,  --* U , ,  .
.
.
N , .
.
,  E , )to string-description pairs (s,,d,} .
.
.
.
.
(sk,dk} are as fol-lows.
First, we index each d,, into d~ by replacing every?
.
.
?
.
$ ?path p m any of tts equatmns with the path I " P. Wethen combine these indexed descriptions with the rule byunifying the deductive closure of E, with all the indexeddescriptions:d= u Ud{,j= lWe can now project d by removing from it all equationswith paths that do not start with O.
It is clearly evidentthat the result d o is still deductively closed.
Finally, d o isdeindexed into deo by removing 0 from the front of all pathsO.
p in its equations.
The pair associated with N,o is then( s , ,  .
.
.
s , , ,  d ,o ) .It is not difficult to show that the above operationscan be lifted into operations over elements of pk that leave.untouched the coordinates not mentioned in the rule andthat the lifted operations are continuous mappings?
Witha slight abuse of notation, we can summarize the foregoingdiscussion with the equation\[r\] = deindex o projecl o combine, o index,In the case of tile sample lexicon and one rule grammarpresented earlier, \[G~(e) would beNP  :VP :S :{... as before.- .}{--.
as before-..}("Uther storms Cornwall",{(subj agr nnm} = sg .
.
.
.
})("many knights sit at the Round Table",{(sub 1 agr hum) = pl .
.
.
.
})("many knights storms Cornwall", T)6.
App l i ca t ionsWe have used the techniques discussed here to analyzethe feature systems of GPSG \[15\], LFG \[2\] and PATR-II\[17\].
All of them turn out to be specializations of our do-main D of descriptions.
Figure 1 provides a summary of twoof the most critical formal properties of context-free-basedgrammar formalisms, the domains of their feature systems(full F~ finite elements of F, or elements of F based onnonrecursive domain equations) and whether the context-free skeletons of grammars are constrained to be off-lineparaeable \[13\] thereby guaranteeing decidability.127DCG-II a PATR-II LFG GPSG bFEATURE SYSTEM full finite finite nonrec.CF SKELETON full full off-line fullaDCGs based on Prolog-lI which allows cyclic terms.bHPSG, the current Hewlett-Packard implementation derivedfrom GPSG, would come more accurately under the PATR-IIclassification.Figure 1: Summary of Grammar System PropertiesThough notational differences and some grammaticaldevices are glossed over here, the comparison is useful asa first step in unifying the various formalisms under onesemantic umbrella.
Furthermore, this analysis elicits theneed to distinguish carefully between the domain of fea-ture structures F and that of descriptions.
This distinctionis not clear in the published accounts of GPSG and LFG,which imprecision is responsible for a number of uncertain-ties in the interpretation of operators and conventions inthose formalisms.In addition to formal insights, linguistic insights havealso been gleaned from this work.
First of all, we note'that while the systems make crucial use of unification, gen-eralization is also a well-defined notion therein and mightindeed be quite useful.
In fact, it was this availability of thegeneralization peration that suggested a simplified accountof coordination facts in English now being used in GPSG\[15\] and in an extension of PATR-II \[8\].
Though the issuesof coordination and agreement are discussed in greater de-tail in these two works, we present here a simplified view ofthe use of generalization i  a GPSG coordination analysis.Circa 1982 GPSG \[6\] analyzed coordination by using aspecial principle, the conjunct realization principle (CRP),to achieve partial instantiation of head features {includingagreement} on the parent category.
This principle, togetherwith the head feature convention (HFC) and control agree-ment principle {CAP), guaranteed agreement between thehead noun of a subject and the head verb of a predicate inEnglish sentences.
The HFC, in particular, can be statedin our notation as (0 head) = (n head) for n the head of 0.A more recent analysis \[4,15\] replaced the conjunct re-alization principle with a modified head feature conven-tion that required a head to be more instantiated than theparent, that is: (0 head) E (n head) for all constituentsn which are heads of 0.
Making coordinates heads oftheir parent achieved the effect of the CRP.
Unfortunately,since the HFC no longer forced identity of agreement, anew principle--the nominal completeness principle (NCP),which required that NP's be fully instantiated--was re-quired to guarantee that the appropriate agreements weremaintained.Making use of the order structure of the domains wehave just built, we can achieve straightforwardly the effectof the CRP and the old HFC without any notion of theNCP.
Our final version of the HFC merely requires thatthe parent's head features be the generalization f the headfeatures of the head children.
Formally, we have:(0 head) ---- \[7 (i head)i~heads of 0In the case of parents with one head child, this final HFCreduces to the old HFC requiring identity; it reduces to thenewer one, however, in cases {like coordinate structures}where there are several head constituents.Furthermore, by utilizing an order structure on the do-main of constants C, it may be possible to model that trou-blesome coordination phenomenon, number agreement incoordinated noun phrases \[8,15\].7.
ConclusionWe have approached the problem of analyzing themeaning of grammar formalisms by applying the techniquesof denotational semantics taken from work on the semanticsof computer languages.
This has enabled us to?
account rigorously for intrinsically partial descrip-tions,?
derive directly notions of unification, instantiationand generalization,?
relate feature systems in linguistics with type systemsin computer science,?
show that feature systems in GPSG, I, FG and PATR-II are special cases of a single construction,?
give semantics to a variety of mechanisms in grammarformalisms, and?
introduce operations for modeling linguistic phenom-ena that have not previously been considered.We plan to develop the approach further to give ac-counts of negative and disjunctive constraints \[8\], besidesthe simple equational constraints discussed here.On the basis of these insights alone, it should be clearthat the view of grammar formalisms as programming lan-guages offers considerable potential for investigation.
But,even more importantly, the linguistic discipline enforcedby a rigorous approach to the design and analysis of gram-mar formalisms may make possible a hitherto unachievablestandard of research in this area.References\[1\] Ait-Kaci, H. "A New Model of Computation Basedon a Calculus of Type Subsumption."
Dept.
of Com-puter and Information Science, Univer:ity of Penn-sylvania (November 1983).\[2\] Bresnan, J. and R. Kaplan.
"Lexical-FunctionalGrammar: A Formal System for Granmlatical Repre-sentation."
In J. Bresnan, Ed., The ,%Icntal Represen-tation of Grammatical Relations, MIT Press, Cam-bridge, Massachusetts (1982), pp.
173-281.128\[3\] Colmera,er, A.
"Metamorphosis Grammars."
In L.Bolc, Ed., Natural Language Communication withComputers, Springer-Verlag, Berlin (1978).
Firstappeared as "Les Grammaires de M~tamorphose,"Groupe d'lnt611igence Artificielle, Universit~ de Mar-seille II (November 1975).\[4\] Farkaz, D., D.P.
Flickinger, G. Gazdar, W.A.
Ladu-saw, A. Ojeda, J. Pinkham, G.K. Pullum, and P.Sells.
"Some Revisions to the Theory of Featuresand Feature lnstantiation."
Unpublished manuscript{August 1983).\[5\] Gazdar, Gerald and G. Pullum.
"Generalized PhraseStructure Grammar: A Theoretical Synopsis."
Indi-ana University Linguistics Club, Bloomington, Indi-ana (1982).\[6\] Gazdar, G., E. Klein, G.K. Pullum, and I.A.
Sag.
"Coordinate Structure and Unbounded Dependen-cies."
In 1M.
Barlow, D. P. Flickinger and I. A.Sag, eds., Developments in Generalized Phrase Struc-ture Grammar.
Indiana University Linguistics Club,Bloomington, Indiana (1982).\[7\] Itarrison, M. Introduction to Formal Language The-ory.
Addison-Wesley, Reading, Massachussets (1978).\[8\] Kaitunnen, Lauri.
"Features and Values."
Proceed-ings of the Tenth International Conference on Com-putational Linguistics, Stanford University, Stanford,California (4-7 July, 1984).\[9\] Kay, M. "Functional Grammar."
Proceedings of theFifth Annual Meeting of the Berkeley Linguistic Soci-ety, Berkeley Linguistic Society, Berkeley, California(February 17-19, 1979), pp.
142-158.\[10\] Marcus, M., D. Hindle and M. Fleck.
"D-Theory:Talking about Talking about Trees."
Proceedings ofthe 21st Annual Meeting of the Association for Com-putational Linguistics, Boston, Massachusetts (15-17June, 1982).\[11\] Pereira, F. "Extraposition Grammars."
AmericanJournal of Computational Linguistics 7, 4 (October-December 198 I}, 243-256.\[12\] Pereira, F. and D. H. D. Warren.
"Definite ClauseGrammars for Language Analysis--a Survey of theFormalism and a Comparison with Augmented Tran-sition Networks."
Artificial Intelligence 18 {1980),231-278.\[13\] Pereira, F. C. N., and David H. D. Warren "Parsingas Deduction."
Proceedings of the ~Ist Annual Meet-ing of the Association for Computational Linguistics,Boston, Massachusetts, (15-17 June, 1983), pp.
137-144.\[14\] Plotkin, G. "A Powerdomain Construction."
SIAMJournal of Computing 5 (1976), 452-487.\[15\] Sag, I., G. Gazdar, T. Wasow and S. Weisler.
"Coor-dination and How to Distinguish Categories."
ReportNo.
CSLI-84-3, Center for the Study of Languageand Information, Stanford University, Stanford, Cal-ifornia (June, 1982).\[16\].
\ [17\]Scott, D. "Domains for Denotational Semantics."
InICALP 82, Springer-Verlag, Heidelberg (1982).Shieber, Stuart.
"The Design of a Computer Lan-guage for Linguistic Information."
Proceedings ofthe Tenth International Conference on ComputationalLinguistics \[4-7 July, 1984)\[18\] Shieber, S., H. Uszkoreit, F. Pereira, J. Robinson andM.
Tyson.
"The Formalism and Implementation fPATR-II."
In Research on Interactive Acquisition andUse of Knowledge, SRI Final Report 1894.
SRI In-ternational, Menlo Park, Califi)rnia (1983).\[19\] Smyth, M. "Power Domains."
Journal of Computerand System Sciences 16 (1978), 23-36.\[20\] Stoy, J. Denotational Semantics: The Seott-StracheyApproach to Programming Language Theory.
MITPress, Cambridge, Ma.ssachusetts (1977).\[21\] van Erodes, M. and R. A. Kowalski.
"The Seman-tics of Predicate Logic as a Programming Language.
"Journal of the ACM 23, 4 {October 1976), 733-742.\[22\] Woods, W. et al "Speech Understanding Systems:Final Report."
BBN Report 3438, Bolt Beranek andNewman, Cambridge, Massachusetts (1976).129
