Proceedings of the ACL 2010 Conference Short Papers, pages 353?358,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsUsing Parse Features for Preposition Selection and Error DetectionJoel TetreaultEducational Testing ServicePrincetonNJ, USAJTetreault@ets.orgJennifer FosterNCLTDublin City UniversityIrelandjfoster@computing.dcu.ieMartin ChodorowHunter College of CUNYNew York, NY, USAmartin.chodorow@hunter.cuny.eduAbstractWe evaluate the effect of adding parse fea-tures to a leading model of preposition us-age.
Results show a significant improve-ment in the preposition selection task onnative speaker text and a modest incrementin precision and recall in an ESL error de-tection task.
Analysis of the parser outputindicates that it is robust enough in the faceof noisy non-native writing to extract use-ful information.1 IntroductionThe task of preposition error detection has re-ceived a considerable amount of attention in re-cent years because selecting an appropriate prepo-sition poses a particularly difficult challenge tolearners of English as a second language (ESL).It is not only ESL learners that struggle with En-glish preposition usage ?
automatically detectingpreposition errors made by ESL speakers is a chal-lenging task for NLP systems.
Recent state-of-the-art systems have precision ranging from 50% to80% and recall as low as 10% to 20%.To date, the conventional wisdom in the errordetection community has been to avoid the useof statistical parsers under the belief that a WSJ-trained parser?s performance would degrade toomuch on noisy learner texts and that the tradi-tionally hard problem of prepositional phrase at-tachment would be even harder when parsing ESLwriting.
However, there has been little substantialresearch to support or challenge this view.
In thispaper, we investigate the following research ques-tion: Are parser output features helpful in mod-eling preposition usage in well-formed text andlearner text?We recreate a state-of-the-art preposition usagesystem (Tetreault and Chodorow (2008), hence-forth T&C08) originally trained with lexical fea-tures and augment it with parser output features.We employ the Stanford parser in our experimentsbecause it consists of a competitive phrase struc-ture parser and a constituent-to-dependency con-version tool (Klein and Manning, 2003a; Kleinand Manning, 2003b; de Marneffe et al, 2006;de Marneffe and Manning, 2008).
We com-pare the original model with the parser-augmentedmodel on the tasks of preposition selection in well-formed text (fluent writers) and preposition errordetection in learner texts (ESL writers).This paper makes the following contributions:?
We demonstrate that parse features have asignificant impact on preposition selection inwell-formed text.
We also show which fea-tures have the greatest effect on performance.?
We show that, despite the noisiness of learnertext, parse features can actually make small,albeit non-significant, improvements to theperformance of a state-of-the-art prepositionerror detection system.?
We evaluate the accuracy of parsing andespecially preposition attachment in learnertexts.2 Related WorkT&C08, De Felice and Pulman (2008) and Ga-mon et al (2008) describe very similar preposi-tion error detection systems in which a model ofcorrect prepositional usage is trained from well-formed text and a writer?s preposition is com-pared with the predictions of this model.
It isdifficult to directly compare these systems sincethey are trained and tested on different data sets353but they achieve accuracy in a similar range.
Ofthese systems, only the DAPPER system (De Fe-lice and Pulman, 2008; De Felice and Pulman,2009; De Felice, 2009) uses a parser, the C&Cparser (Clark and Curran, 2007)), to determinethe head and complement of the preposition.
DeFelice and Pulman (2009) remark that the parsertends to be misled more by spelling errors thanby grammatical errors.
The parser is fundamentalto their system and they do not carry out a com-parison of the use of a parser to determine thepreposition?s attachments versus the use of shal-lower techniques.
T&C08, on the other hand, re-ject the use of a parser because of the difficultiesthey foresee in applying one to learner data.
Her-met et al (2008) make only limited use of theXerox Incremental Parser in their preposition er-ror detection system.
They split the input sentenceinto the chunks before and after the preposition,and parse both chunks separately.
Only very shal-low analyses are extracted from the parser outputbecause they do not trust the full analyses.Lee and Knutsson (2008) show that knowl-edge of the PP attachment site helps in the taskof preposition selection by comparing a classifiertrained on lexical features (the verb before thepreposition, the noun between the verb and thepreposition, if any, and the noun after the preposi-tion) to a classifier trained on attachment featureswhich explicitly state whether the preposition isattached to the preceding noun or verb.
They alsoargue that a parser which is capable of distinguish-ing between arguments and adjuncts is useful forgenerating the correct preposition.3 Augmenting a Preposition Model withParse FeaturesTo test the effects of adding parse features toa model of preposition usage, we replicated thelexical and combination feature model used inT&C08, training on 2M events extracted from acorpus of news and high school level reading ma-terials.
Next, we added the parse features to thismodel to create a new model ?+Parse?.
In 3.1 wedescribe the T&C08 system and features, and in3.2 we describe the parser output features used toaugment the model.
We illustrate our features us-ing the example phrase many local groups aroundthe country.
Fig.
1 shows the phrase structure treeand dependency triples returned by the Stanfordparser for this phrase.3.1 Baseline SystemThe work of Chodorow et al (2007) and T&C08treat the tasks of preposition selection and er-ror detection as a classification problem.
Thatis, given the context around a preposition and amodel of correct usage, a classifier determineswhich of the 34 prepositions covered by the modelis most appropriate for the context.
A model ofcorrect preposition usage is constructed by train-ing a Maximum Entropy classifier (Ratnaparkhi,1998) on millions of preposition contexts fromwell-formed text.A context is represented by 25 lexical featuresand 4 combination features:Lexical Token and POS n-grams in a 2 wordwindow around the preposition, plus the head verbin the preceding verb phrase (PV), the head nounin the preceding noun phrase (PN) and the headnoun in the following noun phrase (FN) whenavailable (Chodorow et al, 2007).
Note that theseare determined not through full syntactic parsingbut rather through the use of a heuristic chun-ker.
So, for the phrase many local groups aroundthe country, examples of lexical features for thepreposition around include: FN = country, PN =groups, left-2-word-sequence = local-groups, andleft-2-POS-sequence = JJ-NNS.Combination T&C08 expand on the lexical fea-ture set by combining the PV, PN and FN fea-tures, resulting in features such as PN-FN andPV-PN-FN.
POS and token versions of these fea-tures are employed.
The intuition behind creat-ing combination features is that the Maximum En-tropy classifier does not automatically model theinteractions between individual features.
An ex-ample of the PN-FN feature is groups-country.3.2 Parse FeaturesTo augment the above model we experimentedwith 14 features divided among five main classes.Table 1 shows the features and their values forour around example.
The Preposition Head andComplement feature represents the two basic at-tachment relations of the preposition, i.e.
its head(what it is attached to) and its complement (whatis attached to it).
Relation specifies the relationbetween the head and complement.
The Preposi-tion Head and Complement Combined featuresare similar to the T&C08 Combination featuresexcept that they are extracted from parser output.354NPNPDTmanyJJlocalNNSgroupsPPINaroundNPDTtheNNcountryamod(groups-3, many-1)amod(groups-3, local-2)prep(groups-3, around-4)det(country-6, the-5)pobj(around-4, country-6)Figure 1: Phrase structure tree and dependencytriples produced by the Stanford parser for thephrase many local groups around the countryPrep.
Head & Complement1.
head of the preposition: groups2.
POS of the head: NNS3.
complement of the preposition: country4.
POS of the complement: NNPrep.
Head & Complement Relation5.
Prep-Head relation name: prep6.
Prep-Comp relation name: pobjPrep.
Head & Complement Combined7.
Head-Complement tokens: groups-country8.
Head-Complement tags: NNS-NNPrep.
Head & Complement Mixed9.
Head Tag and Comp Token: NNS-country10.
Head Token and Comp Tag: groups-NNPhrase Structure11.
Preposition Parent: PP12.
Preposition Grandparent: NP13.
Left context of preposition parent: NP14.
Right context of preposition parent: -Table 1: Parse FeaturesModel Accuracycombination only 35.2parse only 60.6combination+parse 61.9lexical only 64.4combination+lexical (T&C08) 65.2lexical+parse 68.1all features (+Parse) 68.5Table 2: Accuracy on preposition selection taskfor various feature combinationsThe Preposition Head and Complement Mixedfeatures are created by taking the first feature inthe previous set and backing-off either the heador the complement to its POS tag.
This mix oftags and tokens in a word-word dependency hasproven to be an effective feature in sentiment anal-ysis (Joshi and Penstein-Rose?, 2009).
All the fea-tures described so far are extracted from the set ofdependency triples output by the Stanford parser.The final set of features (Phrase Structure), how-ever, is extracted directly from the phrase structuretrees themselves.4 EvaluationIn Section 4.1, we compare the T&C08 and +Parsemodels on the task of preposition selection onwell-formed texts written by native speakers.
Forevery preposition in the test set, we compare thesystem?s top preposition for that context to thewriter?s preposition, and report accuracy rates.
InSection 4.2, we evaluate the two models on ESLdata.
The task here is slightly different - if themost likely preposition according to the model dif-fers from the likelihood of the writer?s prepositionby a certain threshold amount, a preposition erroris flagged.4.1 Native Speaker Test DataOur test set consists of 259K preposition eventsfrom the same source as the original training data.The T&C08 model performs at 65.2% and whenthe parse features are added, the +Parse model im-proves performance by more than 3% to 68.5%.1The improvement is statistically significant.1Prior research has shown preposition selection perfor-mance accuracy ranging from 65% to nearly 80%.
The dif-ferences are largely due to different test sets and also trainingsizes.
Given the time required to train large models, we reporthere experiments with a relatively small model.355Model AccuracyT&C08 65.2+Phrase Structure Only 67.1+Dependency Only 68.2+Parse 68.5+head-tag+comp-tag 66.9+left 66.8+grandparent 66.6+head-token+comp-tag 66.6+head-tag 66.5+head-token 66.4+head-tag+comp-token 66.1Table 3: Which parse features are important?
Fea-ture Addition ExperimentTable 2 shows the effect of various feature classcombinations on prediction accuracy.
The resultsare clear: a significant performance improvementis obtained on the preposition selection task whenfeatures from parser output are added.
The twobest models in Table 2 contain parse features.
Thetable also shows that the non-parser-based featureclasses are not entirely subsumed by the parse fea-tures but rather provide, to varying degrees, com-plementary information.Having established the effectiveness of parsefeatures, we investigate which parse featureclasses contribute the most.
To test each contri-bution, we perform a feature addition experiment,separately adding features to the T&C08 model(see Table 3).
We make three observations.
First,while there is overlapping information betweenthe dependency features and the phrase structurefeatures, the phrase structure features are mak-ing a contribution.
This is interesting becauseit suggests that a pure dependency parser mightbe less useful than a parser which explicitly pro-duces both constituent and dependency informa-tion.
Second, using a parser to identify the prepo-sition head seems to be more useful than using it toidentify the preposition complement.2 Finally, aswas the case for the T&C08 features, the combina-tion parse features are also important (particularlythe tag-tag or tag/token pairs).4.2 ESL Test DataOur test data consists of 5,183 preposition eventsextracted from a set of essays written by non-2De Felice (2009) observes the same for the DAPPER sys-tem.Method Precision RecallT&C08 0.461 0.215+Parse 0.486 0.225Table 4: ESL Error Detection Resultsnative speakers for the Test of English as a ForeignLanguage (TOEFL R?).
The prepositions werejudged by two trained annotators and checkedby the authors using the preposition annotationscheme described in Tetreault and Chodorow(2008b).
4,881 of the prepositions were judged tobe correct and the remaining 302 were judged tobe incorrect.The writer?s preposition is flagged as an error bythe system if its likelihood according to the modelsatisfied a set of criteria (e.g., the difference be-tween the probability of the system?s choice andthe writer?s preposition is 0.8 or higher).
Un-like the selection task where we use accuracy asthe metric, we use precision and recall with re-spect to error detection.
To date, performancefigures that have been reported in the literaturehave been quite low, reflecting the difficulty of thetask.
Table 4 shows the performance figures forthe T&C08 and +Parse models.
Both precisionand recall are higher for the +Parse model, how-ever, given the low number of errors in our an-notated test set, the difference is not statisticallysignificant.5 Parser Accuracy on ESL DataTo evaluate parser performance on ESL data,we manually inspected the phrase structure treesand dependency graphs produced by the Stanfordparser for 210 ESL sentences, split into 3 groups:the sentences in the first group are fluent and con-tain no obvious grammatical errors, those in thesecond contain at least one preposition error andthe sentences in the third are clearly ungrammati-cal with a variety of error types.
For each preposi-tion we note whether the parser was successful indetermining its head and complement.
The resultsfor the three groups are shown in Table 5.
Thefigures in the first row are for correct prepositionsand those in the second are for incorrect ones.The parser tends to do a better job of de-termining the preposition?s complement than itshead which is not surprising given the well-knownproblem of PP attachment ambiguity.
Given thepreposition, the preceding noun, the preceding356OKHead CompPrep Correct 86.7% (104/120) 95.0% (114/120)Prep Incorrect - -Preposition ErrorHead CompPrep Correct 89.0% (65/73) 97.3% (71/73)Prep Incorrect 87.1% (54/62) 96.8% (60/62)UngrammaticalHead CompPrep Correct 87.8% (115/131) 89.3% (117/131)Prep Incorrect 70.8% (17/24) 87.5% (21/24)Table 5: Parser Accuracy on Prepositions in aSample of ESL Sentencesverb and the following noun, Collins (1999) re-ports an accuracy rate of 84.5% for a PP attach-ment classifier.
When confronted with the sameinformation, the accuracy of three trained annota-tors is 88.2%.
Assuming 88.2% as an approximatePP-attachment upper bound, the Stanford parserappears to be doing a good job.
Comparing theresults over the three sentence groups, its abilityto identify the preposition?s head is quite robust togrammatical noise.Preposition errors in isolation do not tend tomislead the parser: in the second group which con-tains sentences which are largely fluent apart frompreposition errors, there is little difference be-tween the parser?s accuracy on the correctly usedprepositions and the incorrectly used ones.
Exam-ples are(S (NP I)(VP had(NP (NP a trip)(PP for (NP Italy)))))in which the erroneous preposition for is correctlyattached to the noun trip, and(S (NP A scientist)(VP devotes(NP (NP his prime part)(PP of (NP his life)))(PP in (NP research))))in which the erroneous preposition in is correctlyattached to the verb devotes.6 ConclusionWe have shown that the use of a parser can boostthe accuracy of a preposition selection modeltested on well-formed text.
In the error detectiontask, the improvement is less marked.
Neverthe-less, examination of parser output shows the parsefeatures can be extracted reliably from ESL data.For our immediate future work, we plan to carryout the ESL evaluation on a larger test set to bet-ter gauge the usefulness of a parser in this context,to carry out a detailed error analysis to understandwhy certain parse features are effective and to ex-plore a larger set of features.In the longer term, we hope to compare differenttypes of parsers in both the preposition selectionand error detection tasks, i.e.
a task-based parserevaluation in the spirit of that carried out by Miyaoet al (2008) on the task of protein pair interactionextraction.
We would like to further investigatethe role of parsing in error detection by looking atother error types and other text types, e.g.
machinetranslation output.AcknowledgmentsWe would like to thank Rachele De Felice and thereviewers for their very helpful comments.ReferencesMartin Chodorow, Joel Tetreault, and Na-Rae Han.2007.
Detection of grammatical errors involv-ing prepositions.
In Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, Prague, CzechRepublic, June.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.Michael Collins.
1999.
Head-driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Rachele De Felice and Stephen G. Pulman.
2008.
Aclassifier-based approach to preposition and deter-miner error correction in L2 english.
In Proceedingsof the 22nd COLING, Manchester, United Kingdom.Rachele De Felice and Stephen Pulman.
2009.
Au-tomatic detection of preposition errors in learningwriting.
CALICO Journal, 26(3):512?528.Rachele De Felice.
2009.
Automatic Error Detectionin Non-native English.
Ph.D. thesis, Oxford Univer-sity.357Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The stanford typed dependencies repre-sentation.
In Proceedings of the COLING08 Work-shop on Cross-framework and Cross-domain ParserEvaluation, Manchester, United Kingdom.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of LREC, Genoa, Italy.Michael Gamon, Jianfeng Gao, Chris Brockett,Alexandre Klementiev, William B. Dolan, DmitriyBelenko, and Lucy Vanderwende.
2008.
Using con-textual speller techniques and language modellingfor ESL error correction.
In Proceedings of the In-ternational Joint Conference on Natural LanguageProcessing, Hyderabad, India.Matthieu Hermet, Alain De?silets, and Stan Szpakow-icz.
2008.
Using the web as a linguistic resourceto automatically correct lexico-syntactic errors.
InProceedings of LREC, Marrekech, Morocco.Mahesh Joshi and Carolyn Penstein-Rose?.
2009.
Gen-eralizing dependency features for opinion mining.In Proceedings of the ACL-IJCNLP 2009 Confer-ence Short Papers, pages 313?316, Singapore.Dan Klein and Christopher D. Manning.
2003a.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the ACL, pages 423?430,Sapporo, Japan.Dan Klein and Christopher D. Manning.
2003b.
Fastexact inference with a factored model for exact pars-ing.
In Advances in Neural Information ProcessingSystems, pages 3?10.
MIT Press, Cambridge, MA.John Lee and Ola Knutsson.
2008.
The role of PP at-tachment in preposition generation.
In Proceedingsof CICling.
Springer-Verlag Berlin Heidelberg.Yusuke Miyao, Rune Saetre, Kenji Sagae, Takuya Mat-suzaki, and Jun?ichi Tsujii.
2008.
Task-orientedevaluation of syntactic parsers and their representa-tions.
In Proceedings of the 46th Annual Meeting ofthe ACL, pages 46?54, Columbus, Ohio.Adwait Ratnaparkhi.
1998.
Maximum Entropy Mod-els for natural language ambiguity resolution.
Ph.D.thesis, University of Pennsylvania.Joel Tetreault and Martin Chodorow.
2008.
The upsand downs of preposition error detection in ESLwriting.
In Proceedings of the 22nd COLING,Manchester, United Kingdom.Joel Tetreault and Martin Chodorow.
2008b.
Na-tive Judgments of non-native usage: Experiments inpreposition error detection.
In COLING Workshopon Human Judgments in Computational Linguistics,Manchester, United Kingdom.358
