Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 424?432,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSemi-Supervised Lexicon Mining from Parenthetical Expressionsin Monolingual Web PagesXianchao Wu?
Naoaki Okazaki?
Jun?ichi Tsujii??
?Computer Science, Graduate School of Information Science and Technology, University of Tokyo7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan?School of Computer Science, University of ManchesterNational Centre for Text Mining (NaCTeM)Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester M1 7DN, UK{wxc, okazaki, tsujii}@is.s.u-tokyo.ac.jpAbstractThis paper presents a semi-supervised learn-ing framework for mining Chinese-Englishlexicons from large amount of Chinese Webpages.
The issue is motivated by the ob-servation that many Chinese neologisms areaccompanied by their English translations inthe form of parenthesis.
We classify par-enthetical translations into bilingual abbrevi-ations, transliterations, and translations.
Afrequency-based term recognition approach isapplied for extracting bilingual abbreviations.A self-training algorithm is proposed for min-ing transliteration and translation lexicons.
Inwhich, we employ available lexicons in termsof morpheme levels, i.e., phoneme correspon-dences in transliteration and grapheme (e.g.,suffix, stem, and prefix) correspondences intranslation.
The experimental results verifiedthe effectiveness of our approaches.1 IntroductionBilingual lexicons, as lexical or phrasal parallelcorpora, are widely used in applications of multi-lingual language processing, such as statistical ma-chine translation (SMT) and cross-lingual informa-tion retrieval.
However, it is a time-consuming taskfor constructing large-scale bilingual lexicons byhand.
There are many facts cumber the manual de-velopment of bilingual lexicons, such as the contin-uous emergence of neologisms (e.g., new technicalterms, personal names, abbreviations, etc.
), the dif-ficulty of keeping up with the neologisms for lexi-cographers, etc.
In order to turn the facts to a betterway, one of the simplest strategies is to automati-cally mine large-scale lexicons from corpora such asthe daily updated Web.Generally, there are two kinds of corpora usedfor automatic lexicon mining.
One is the purelymonolingual corpora, wherein frequency-basedexpectation-maximization (EM, refer to (Dempsteret al, 1977)) algorithms and cognate clues play acentral role (Koehn and Knight, 2002).
Haghighiet al (2008) presented a generative model basedon canonical correlation analysis, in which monolin-gual features such as the context and orthographicsubstrings of words were taken into account.
Theother is multilingual parallel and comparable cor-pora (e.g., Wikipedia1), wherein features such as co-occurrence frequency and context are popularly em-ployed (Cheng et al, 2004; Shao and Ng, 2004; Caoet al, 2007; Lin et al, 2008).In this paper, we focus on a special type of com-parable corpus, parenthetical translations.
The issueis motivated by the observation that Web pages andtechnical papers written in Asian languages (e.g.,Chinese, Japanese) sometimes annotate named enti-ties or technical terms with their translations in En-glish inside a pair of parentheses.
This is consideredto be a traditional way to annotate new terms, per-sonal names or other named entities with their En-glish translations expressed in brackets.
Formally,a parenthetical translation can be expressed by thefollowing pattern,f1 f2 ... fJ (e1 e2 ... eI).
(1)Here, f1 f2 ... fJ (fJ1 ), the pre-parenthesis text, de-notes the word sequence of some language otherthan English; and e1 e2 ... eI (eI1), the in-parenthesistext, denotes the word sequence of English.
We sep-arate parenthetical translations into three categories:1http://en.wikipedia.org/wiki/Main Page424Type Examples with translations in italic??
??
??
??
??
(GCOS)to Global Climate Observing System (GCOS)??
?
??
??
?- ???
(Shipton-Tilman)brand will be among Shipton-Tilman (Shipton-Tilman)??????
???
(Cancelbots)time bomb, Cancelbots (Cancelbots)?
??
??
?
???
????
??
(Bradford University)the English Bradford University (Bradford University)that holds lessons in HongkongAbbreviationTransliterationTranslationMixtureTable 1: Parenthetical translation categories and exam-ples extracted from Chinese Web pages.
Mixture standsfor the mixture of translation (University) and translitera-tion (Bradford).
???
denotes the left boundary of fJ1 .bilingual abbreviation, transliteration, and transla-tion.
Table 1 illustrates examples of these categories.We address several characteristics of parentheticaltranslations that differ from traditional comparablecorpora.
The first is that they only appear in mono-lingual Web pages or documents, and the contextinformation of eI1 is unknown.
Second, frequencyand word number of eI1 are frequently small.
Thisis because parenthetical translations are only usedwhen the authors thought that fJ1 contained someneologism(s) which deserved further explanation inanother popular language (e.g., English).
Thus, tra-ditional context based approaches are not applicableand frequency based approaches may yield low re-call while with high precision.
Furthermore, cog-nate clues such as orthographic features are not ap-plicable between language pairs such as English andChinese.Parenthetical translation mining faces the follow-ing issues.
First, we need to distinguish paren-thetical translations from parenthetical expressions,since parenthesis has many functions (e.g., definingabbreviations, elaborations, ellipsis, citations, anno-tations, etc.)
other than translation.
Second, theleft boundary (denoted as ?
in Table 1) of the pre-parenthesis text need to be determined to get rid ofthe unrelated words.
Third, we need further distin-guish different translation types, such as bilingualabbreviation, the mixture of translation and translit-eration, as shown in Table 1.In order to deal with these problems, supervised(Cao et al, 2007) and unsupervised (Li et al, 2008)methods have been proposed.
However, supervisedapproaches are restricted by the quality and quantityof manually constructed training data, and unsuper-vised approaches are totally frequency-based with-out using any semantic clues.
In contrast, we pro-pose a semi-supervised framework for mining par-enthetical translations.
We apply a monolingual ab-breviation extraction approach to bilingual abbrevia-tion extraction.
We construct an English-syllable toChinese-pinyin transliteration model which is self-trained using phonemic similarity measurements.We further employ our cascaded translation model(Wu et al, 2008) which is self-trained based onmorpheme-level translation similarity.This paper is organized as follows.
We brieflyreview the related work in the next section.
Oursystem framework and self-training algorithm is de-scribed in Section 3.
Bilingual abbreviation ex-traction, self-trained transliteration models and cas-caded translation models are described in Section 4,5, and 6, respectively.
In Section 7, we evaluate ourmined lexicons by Wikipedia.
We conclude in Sec-tion 8 finally.2 Related WorkNumerous researchers have proposed a variety ofautomatic approaches to mine lexicons from theWeb pages or other large-scale corpora.
Shao andNg (2004) presented a method to mine new transla-tions from Chinese and English news documents ofthe same period from different news agencies, com-bining both transliteration and context information.Kuo et al (2006) used active learning and unsu-pervised learning for mining transliteration lexiconfrom the Web pages, in which an EM process wasused for estimating the phonetic similarities betweenEnglish syllables and Chinese characters.Cao et al (2007) split parenthetical translationmining task into two parts, transliteration detectionand translation detection.
They employed a translit-eration lexicon for constructing a grapheme-basedtransliteration model and annotated boundaries man-ually to train a classifier.
Lin et al (2008) applieda frequency-based word alignment approach, Com-petitive Link (Melanmed, 2000), to determine theouter boundary (Section 7).On the other hand, there have been many semi-supervised approaches in numerous applications425Parenthetical expression extraction{C(E)}Chinese word segmentation{c?(e?)}
S-MSRSegHeuristic filtering{c?(e?
)}Chinese Web pagesBilingual abbreviation miningSection 4Transliteration lexicon miningSection 5Translation lexicon miningSection 6(Lin et al, 2008)Figure 1: The system framework of mining lexicons fromChinese Web pages.
(Zhu, 2007), such as self-training in word sensedisambiguation (Yarowsky, 2005) and parsing (Mc-Closky et al, 2008).
In this paper, we apply self-training to a new topic, lexicon mining.3 System Framework and Self-TrainingAlgorithmFigure 1 illustrates our system framework for min-ing lexicons from Chinese Web pages.
First, par-enthetical expressions matching Pattern 1 are ex-tracted.
Then, pre-parenthetical Chinese sequencesare segmented into word sequences by S-MSRSeg2(Gao et al, 2006).
The initial parenthetical transla-tion corpus is constructed by applying the heuristicrules defined in (Lin et al, 2008)3.
Based on thiscorpus, we mine three lexicons step by step, a bilin-gual abbreviation lexicon, a transliteration lexicon,and a translation lexicon.
The abbreviation candi-dates are extracted firstly by using a heuristic rule(Section 4.1).
Then, the transliteration candidatesare selected by employing a transliteration model(Section 5.1).
Specially, fJ1 (eI1) is taken as a translit-eration candidate only if a word ei in eI1 can betransliterated.
In addition, a transliteration candidatewill also be considered as a translation candidate ifnot all ei can be transliterated (refer to the mixtureexample in Table1).
Finally, after abbreviation filter-ing and transliteration filtering, the remaining candi-2http://research.microsoft.com/research/downloads/details/7a2bb7ee-35e6-40d7-a3f1-0b743a56b424/details.aspx3e.g., fJ1 is predominantly in Chinese and eI1 is predomi-nantly in EnglishAlgorithm 1 self-training algorithmRequire: L, U = {fJ1 (eI1)}, T , M ?L, (labeled) train-ing set; U , (unlabeled) candidate set; T , test set; M, thetransliteration or translation model.1: Lexicon = {} ?
new mined lexicon2: repeat3: N = {} ?
new mined lexicon during one iteration4: train M on L5: evaluate M on T6: for fJ1 (eI1) ?
U do7: topN = {C?|decode eI1 by M}8: N = N ?
{(c, eI1)|c ?
fJ1 ??C?
?
topN s.t.
similarity{c, C?}
?
?
}9: end for10: U = U ?N11: L = unified(L ?N)12: Lexicon = unified(Lexicon ?N)13: until |N | ?
?14: return Lexicon ?
the outputdates are used for translation lexicon mining.Algorithm 1 addresses the self-training algorithmfor lexicon mining.
The main part is a loop fromLine 2 to Line 13.
A given seed lexicon is takenas labeled data and is split into training and testingsets (L and T ).
U={fJ1 (eI1)}, stands for the (unla-beled) parenthetical expression set.
Initially, a trans-lation/transliteration model (M) is trained on L andevaluated on T (Line 4 and 5).
Then, the Englishphrase eI1 of each unlabeled entry is decoded by M,and the top-N outputs are stored in set topN (Line7?8).
A similarity function on c (a word substringof fJ1 ) and a top-N output C ?
is employed to makethe decision of classification: the pair (c, eI1) will beselected as a new entry if the similarity between cand C ?
is no smaller than a threshold value ?
(Line8).
After processing each entry in U , the new minedlexicon N is deleted from U and unified with thecurrent training set L as the new training set (Line10 and 11).
Also, N is added to the final lexicon(Line 12).
When |N | is lower than a threshold, theloop stops.
Finally, the algorithm returns the minedlexicon.One of the open problems in Algorithm 1 is howto append new mined entries into the existing seedlexicon, considering they have different distribu-tions.
One way is to design and estimate a weightfunction on the frequency of new mined entries.
Forsimplicity, we use a deficient strategy that takes theweights of all new mined entries to be one.4264 Bilingual Abbreviation Extraction4.1 MethodologyThe method that we use for extracting a bilingualabbreviation lexicon from parenthetical expressionsis inspired by (Okzaki and Ananiadou, 2006).
Theyused a term recognition approach to build a monolin-gual abbreviation dictionary from the Medical Liter-ature Analysis and Retrieval System Online (MED-LINE) abstracts, wherein acronym definitions (e.g.,ADM is short for adriamycin, adrenomedullin, etc.
)are abundant.
They reported 99% precision and 82-95% recall.
Through locating a textual fragmentwith an acronym and its expanded form in patternlong form (short form), (2)they defined a heuristic formula to compute the long-form likelihood LH(c) for a candidate c:LH(c) = freq(c)?
?t?Tcfreq(t)?
freq(t)?t?Tc freq(t).
(3)Here, c is a long-form candidate; freq(c) denotes thefrequency of co-occurrence of c with a short-form;and Tc is a set of nested long-form candidates, eachof which consists of a preceding word followed bythe candidate c. Obviously, for t ?
Tc, Equation 3can be explained as:LH(c) = freq(c)?
E[freq(t)].
(4)In this paper, we apply their method on the taskof bilingual abbreviation lexicon extraction.
Now,the long-form is a Chinese word sequence and theshort-form is an English acronym.
We filter the par-enthetical expressions in the Web pages with severalheuristic rules to meet the form of pattern 2 and tosave the computing time:?
the short-form (eI1) should contain only one En-glish word (I = 1), and all letters in whichshould be capital;?
similar with (Lin et al, 2008), the pre-parenthesis text is trimmed with: |c| ?
10 ?|eI1|+ 6 when |eI1| ?
6, and |c| ?
2?
|eI1|+ 6,otherwise.
|c| and |eI1| are measured in bytes.We further trim the remaining pre-parenthesistext by punctuations other than hyphens anddots, i.e., the right most punctuation and its leftsubsequence are discarded.o.
Chinese long-form candidates LH T/F1 ??
??
??
172.5 TTumor-Associated Antigen2 ?
?
??
?
79.9 Tthioacetamide3 ?
33.8 Famine4 ??
24.5 Fantigen5 ??
??
21.2 Fassociated antigen6 ?
??
??
??
16.5 F's Tumor-Associated Antigen7 ?
???
16.2 Ttotal amino acidTable 2: Top-7 Chinese long-form candidates for the En-glish acronym TAA, according to the LH score.4.2 ExperimentWe used SogouT Internet Corpus Version 2.04,which contains about 13 billion original Web pages(mainly Chinese) in the form of 252 gigabyte .txtfiles.
In addition, we used 55 gigabyte (.txt for-mat) Peking University Chinese Paper Corpus.
Weconstructed a partially parallel corpus in the formof Pattern 1 from the union of the two corpora us-ing the heuristic rules defined in (Lin et al, 2008).We gained a partially parallel corpus which contains12,444,264 entries.We extracted 107,856 distinct English acronyms.Limiting LH score ?
1.0 in Equation 3, we gained2,020,012 Chinese long-form candidates for the107,856 English acronyms.
Table 2 illustrates thetop-7 Chinese long-form candidates of the Englishacronym TAA.
Three candidates are correct (T) long-forms while the other 4 are wrong (F).
Wrong can-didates from No.
3 to 5 are all subsequences of thecorrect candidate No.
1.
No.
6 includes No.
1 whilewith a Chinese functional word de in the left mostside.
These error types can be easily tackled withsome filtering patterns, such as ?remove the left mostfunctional word in the long-form candidates?, ?onlykeep the relatively longer candidates with larger LHscore?, etc.Since there does not yet exists a common eval-uation data set for the bilingual abbreviation lexi-con, we manually evaluated a small sample of it.4http://www.sogou.com/labs/dl/t.html427Of the 107,856 English acronyms, we randomly se-lected 200 English acronyms and their top-1 Chi-nese long-form candidates for manually evaluating.We found, 92 candidates were correct including 3transliteration examples.
Of the 108 wrong candi-dates, 96 candidates included the correct long-formwith some redundant words on the left side (i.e., c =(word)+ correct long-form), the other 12 candidatesmissed some words of the correct long-form or hadsome redundant words right before the left paren-thesis (i.e., c = (word)?
correct long-form (word)+or c = (word)?
subsequence of correct long-formword)?).
We classified the redundant word right be-fore the correct long-form of each of the 96 candi-dates, de occupied 32, noun occupied 7, verb occu-pied 18, prepositions and conjunctions occupied theremaining ones.In total, the abbreviation translation accuracy is44.5%.
We improved the accuracy to 60.5% withan additional de filtering pattern.
According to for-mer mentioned error analysis, the accuracy may fur-ther be improved if a Chinese part-of-speech taggeris employed and the non-nominal words in the long-form are removed beforehand.5 Self-Training for Transliteration ModelsIn this section, we first describe and compare threetransliteration models.
Then, we select and train thebest model following Algorithm 1 for lexicon min-ing.
We investigate two things, the scalability of theself-trained model given different amount of initialtraining data, and the performance of several strate-gies for selecting new training samples.5.1 Model descriptionWe construct and compare three forward translit-eration models, a phoneme-based model (Englishphonemes to Chinese pinyins), a grapheme-basedmodel (English syllables to Chinese characters)and a hybrid model (English syllables to Chinesepinyins).
Similar models have been compared in(Oh et al, 2006) for English-to-Korean and English-to-Japanese transliteration.
All the three models arephrase-based, i.e., adjacent phonemes or graphemesare allowable to form phrase-level transliterationunits.
Building the correspondences on phraselevel can effectively tackle the missing or redundantphoneme/grapheme problem during transliteration.For example, when Aamodt is transliterated into amo?
te`5, a and d are missing.
The problem can beeasily solved when taking Aa and dt as single unitsfor transliterating.Making use of Moses (Koehn et al, 2007), aphrase-based SMT system, Matthews (2007) hasshown that the performance was comparable to re-cent state-of-the-art work (Jiang et al, 2007) inEnglish-to-Chinese personal name transliteration.Matthews (2007) took transliteration as translationat the surface level.
Inspired by his idea, we alsoimplemented our transliteration models employingMoses.
The main difference is that, while Matthews(2007) tokenized the English names into individualletters before training in Moses, we split them intosyllables using the heuristic rules described in (Jianget al, 2007), such that one syllable only contains onevowel letter or a combination of a consonant and avowel letter.English syllable sequences are used in thegrapheme-based and hybrid models.
In thephoneme-based model, we transfer English namesinto phonemes and Chinese characters into Pinyinsin virtue of the CMU pronunciation dictionary6 andthe LDC Chinese character-to-pinyin list7.In the mass, the grapheme-based model is themost robust model, since no additional resources areneeded.
However, it suffers from the Chinese homo-phonic character problem.
For instance, pinyin aicorresponds to numerous Chinese characters whichare applicable to personal names.
The phoneme-based model is the most suitable model that reflectsthe essence of transliteration, while restricted by ad-ditional grapheme to phoneme dictionaries.
In or-der to eliminate the confusion of Chinese homo-phonic characters and alleviate the dependency onadditional resources, we implement a hybrid modelthat accepts English syllables and Chinese pinyinsas formats of the training data.
This model is calledhybrid, since English syllables are graphemes andChinese pinyins are phonemes.5The tones of Chinese pinyins are ignored in our translitera-tion models for simplicity.6http://www.speech.cs.cmu.edu/cgi-bin/cmudict7http://projects.ldc.upenn.edu/Chinese/docs/char2pinyin.txt428grapheme-based0.00.20.40.60.81.01 2 3 4 5 6 7 8max_phrase_lengthBLEU WER PER EMatchphoneme-based0.00.20.40.60.81.01 2 3 4 5 6 7 8max_phrase_lengthBLEU WER PER EMatchComparison on EMatch0.00.10.20.30.40.51 2 3 4 5 6 7 8max_phrase_lengthgrapheme phoneme hybridhybrid-based0.00.20.40.60.81.01 2 3 4 5 6 7 8max_phrase_lengthBLEU WER PER EMatchFigure 2: The performances of the transliteration modelsand their comparison on EMatch.5.2 Experimental model selectionSimilar to (Jiang et al, 2007), the transliterationmodels were trained and tested on the LDC Chinese-English Named Entity Lists Version 1.08.
The origi-nal list contains 572,213 English people names withChinese transliterations.
We extracted 74,725 en-tries in which the English names also appeared inthe CMU pronunciation dictionary.
We randomlyselected 3,736 entries as an open testing set and theremaining entries as a training set9.
The results wereevaluated using the character/pinyin-based 4-gramBLEU score (Papineni et al, 2002), word error rate(WER), position independent word error rate (PER),and exact match (EMatch).Figure 2 reports the performances of the threemodels and the comparison based on EMatch.
Fromthe results, we can easily draw the conclusion thatthe hybrid model performs the best under the maxi-mal phrase length (mpl, the maximal phrase lengthallowed in Moses) from 1 to 8.
The performancesof the models converge at or right after mpl =4.
The pinyin-based WER of the hybrid model is39.13%, comparable to the pinyin error rate 39.6%,reported in (Jiang et al, 2007)10.
Thus, our further8Linguistic Data Consortium catalog number:LDC2005T34 (former catalog number: LDC2003E01)9Jiang et al (2007) selected 25,718 personal name pairsfrom LDC2003E01 as the experiment data: 200 as developmentset, 200 as test set, and the remaining entries as training set.10It should be notified that we achieved this result by usinglarger training set (70,989 vs. 25,718) and larger test set (3,736vs.
200) comparing with (Jiang et al, 2007), and we did not use% 0t 1t 2t 3t 4t 5t Strategy5 .3879 .3937 .3971 .3958 .3972 .3971 top1 em.3911 .3979 .3954 .3974 .3965 top1 am.4062 .4182 .4208 .4218 .4201 top5 em.3987 .4177 .4190 .4192 .4189 top5 am10 .4092 .4282 .4258 .4202 .4203 .4205 top1 em.4121 .4190 .4180 .4174 .4200 top1 am.4305 .4386 .4399 .4438 .4403 top5 em.4289 .4263 .4292 .4291 .4288 top5 am20 .4561 .4538 .4562 .4550 .4543 .4551 top1 em.4532 .4578 .4544 .4545 .4541 top1 am.4624 .4762 .4754 .4748 .4746 top5 em.4605 .4677 .4677 .4674 .4679 top5 am40 .4779 .4791 .4793 .4799 .4794 .4808 top1 em.4774 .4794 .4779 .4789 .4784 top1 am.4808 .4811 .4791 .4795 .4790 top5 em.4775 .4778 .4781 .4785 .4779 top5 am60 .5032 .4939 .5004 .5012 .5012 .5016 top1 em.4919 .4988 .4990 .4994 .4990 top1 am.5013 .5063 .5059 .5066 .5065 top5 em.4919 .4960 .4970 .4977 .4962 top5 am80 .5038 .4984 .4984 .5004 .5006 .4995 top1 em.4916 .4916 .4914 .4915 .4916 top1 am.5039 .5037 .5053 .5054 .5042 top5 em.4950 .5028 .5027 .5032 .5032 top5 am100 .5045 .5077 .5053 .5067 .5063 .5066 top1 em.5045 .5054 .5046 .5050 .5055 top1 am.5108 .5102 .5111 .5108 .5115 top5 em.5105 .5106 .5100 .5094 .5109 top5 amTable 3: The BLEU score of self-trained h4 translitera-tion models under four selection strategies.
nt (n=1..5)stands for the n-th iteration.self-training experiments are pursued on the hybridmodel taking mpl to be 4 (short for h4, hereafter).5.3 Experiments on the self-trained hybridmodelAs former mentioned, we investigate the scalabilityof the self-trained h4 model by respectively using 5,10, 20, 40, 60, 80, and 100 percent of initial trainingdata, and the performances of using exact matching(em) or approximate matching (am, line 8 in Algo-rithm 1) on the top-1 and top-5 outputs (line 7 in Al-gorithm 1) for selecting new training samples.
Weused edit distance (ed) to measure the em and amsimilarities:ed(c, C ?)
= 0 or < syllable number(C ?)/2.
(5)When applying Algorithm 1 for transliteration lexi-con mining, we decode each word in eI1 respectively.The algorithm terminated in five iterations when weset the terminal threshold ?
(Line 13 in Algorithm 1)to be 100.For simplicity, Table 3 only illustrates the BLEUscore of h4 models under four selection strategies.From this table, we can draw the following conclu-sions.
First, with fewer initial training data, the im-provement is better.
The best relative improvementsadditional Web resources as Jiang et al (2007) did.429are 8.74%, 8.46%, 4.41%, 0.67%, 0.68%, 0.32%,and 1.39%, respectively.
Second, using top-5 andem for new training data selection performs the bestamong the four strategies.
Compared under each it-eration, using top-5 is better than using top-1; emis better than am; and top-5 with am is a little bet-ter than top-1 with em.
We mined 39,424, 42,466,46,116, 47,057, 49,551, 49,622, and 50,313 distinctentries under the six types of initial data with top-5plus em strategy.
The 50,313 entries are taken as thefinal transliteration lexicon for further comparison.6 Self-Training for a Cascaded TranslationModelWe classify the parenthetical translation candidatesby employing a translation model.
In contrast to(Lin et al, 2008), wherein the lengthes of prefixesand suffixes of English words were assumed to bethree bytes, we segment words into morphemes (se-quences of prefixes, stems, and suffixes) by Morfes-sor 0.9.211, an unsupervised language-independentmorphological analyzer (Creutz and Lagus, 2007).We use the morpheme-level translation similarityexplicitly in our cascaded translation model (Wu etal., 2008), which makes use of morpheme, word,and phrase level translation units.
We train Mosesto gain a phrase-level translation table.
To gain amorpheme-level translation table, we run GIZA++(Och and Ney, 2003) on both directions between En-glish morphemes and Chinese characters, and takethe intersection of Viterbi alignments.
The English-to-Chinese translation probabilities computed byGIZA++ are attached to each morpheme-characterelement in the intersection set.6.1 ExperimentThe Wanfang Chinese-English technical term dictio-nary12, which contains 525,259 entries in total, wasused for training and testing.
10,000 entries wererandomly selected as the test set and the remainingas the training set.
Again, we investigated the scala-bility of the self-trained cascaded translation modelby respectively using 20, 40, 60, 80, and 100 per-cent of initial training data.
An aggressive similar-11http://www.cis.hut.fi/projects/morpho/12http://www.wanfangdata.com.cn/Search/ResourceBrowse.aspx% 0t 1t 2t 3t 4t 5t20 .1406 .1196 .1243 .1239 .1176 .117940 .1091 .1224 .1386 .1345 .1479 .146660 .1630 .1624 .1429 .1714 .1309 .139880 .1944 .1783 .1886 .1870 .1884 .1873100 .1810 .1814 .1539 .1981 .1542 .1944Table 4: The BLEU score of self-trained cascaded trans-lation model under five initial training sets.ity measurement was used for selecting new trainingsamples:first char(c) = first char(C ?)
?
min{ed(c, C ?)}.
(6)Here, we judge if the first characters of c and C ?are similar or not.
c was gained by deleting zeroor more characters from the left side of fJ1 .
Whenmore than one c satisfied this condition, the c thathad the smallest edit distance with C ?
was selected.When applying Algorithm 1 for translation lexiconmining, we took eI1 as one input for decoding insteadof decoding each word respectively.
Only the top-1output (C ?)
was used for comparing.
The algorithmstopped in five iterations when we set the terminalthreshold ?
to be 2000.For simplicity, Table 4 only illustrates the BLEUscore of the cascaded translation model under fiveinitial training sets.
For the reason that there are fi-nite phonemes in English and Chinese while the se-mantic correspondences between the two languagestend to be infinite, Table 4 is harder to be analyzedthan Table 3.
When initially using 40%, 60%, and100% training data for self-training, the results tendto be better at some iterations.
We gain 35.6%,5.2%, and 9.4% relative improvements, respectively.However, the results tend to be worse when 20% and80% training data were used initially, with 11.6%and 3.0% minimal relative loss.
The best BLEUscores tend to be better when more initial trainingdata are available.
We mined 1,038,617, 1,025,606,1,048,761, 1,056,311, and 1,060,936 distinct entriesunder the five types of initial training data.
The1,060,936 entries are taken as the final translationlexicon for further comparison.7 Wikipedia EvaluationWe have mined three kinds of lexicons till now,an abbreviation lexicon containing 107,856 dis-430En.
to Ch.
Ch.
to En.Cov EMatch Cov EMatchOur Lexicon 22.8% 5.2% 23.2% 5.5%Unsupervised 23.5% 5.4% 24.0% 5.4%Table 5: The results of our lexicon and an unsupervised-mined lexicon (Lin et al, 2008) evaluated underWikipedia title dictionary.
Cov is short for coverage.similar English acronyms with 2,020,012 Chineselong-form candidates; a transliteration lexicon with50,313 distinct entries; and a translation lexiconwith 1,060,936 distinct entries.
The three lexiconsare combined together as our final lexicon.Similar with (Lin et al, 2008), we compare ourfinal mined lexicon with a dictionary extracted fromWikipedia, the biggest multilingual free-content en-cyclopedia on the Web.
We extracted the titles ofChinese and English Wikipedia articles13 that arelinked to each other.
Since most titles contain lessthan five words, we take a linked title pair as a trans-lation entry without considering the word alignmentrelation between the words inside the titles.
The re-sult lexicon contains 105,320 translation pairs be-tween 103,823 Chinese titles and 103,227 Englishtitles.
Obviously, only a small percentage of titleshave more than one translation.
Whenever there ismore than one translation, we take the candidate en-try as correct if and only if it matches one of thetranslations.Moreover, we compare our semi-supervised ap-proach with an unsupervised approach (Lin et al,2008).
Lin et al (2008) took ?2(fj , ei) score14(Gale and Church, 1991) with threshold 0.001 asthe word alignment probability in a word alignmentalgorithm, Competitive Link.
Competitive Link triesto align an unlinked ei with an unlinked fj by thecondition that ?2(fj , ei) is the biggest.
Lin et al(2008) relaxed the unlinked constraints to allow con-secutive sequence of words on one side to be linkedto the same word on the other side15.
The left13English and Chinese Wikipedia pages due to 2008.09.23are used here.14?2(fj , ei) = (ad?bc)2(a+b)(a+c)(b+d)(c+d) , where a is the numberof fJ1 (eI1) containing both ei and fj ; (a + b) is the number offJ1 (eI1) containing ei; (a+ c) is the number of fJ1 (eI1) contain-ing fj ; and d is the number of fJ1 (eI1) containing neither ei norfj .15Instead of requiring both ei and fj to have no previous link-boundary inside fJ1 is determined when each ei ineI1 is aligned.
After applying the modified Compet-itive Link on the partially parallel corpus which in-cludes 12,444,264 entries (Section 4.2), we obtained2,628,366 distinct pairs.Table 5 shows the results of the two lexicons eval-uated under Wikipedia title dictionary.
The coverageis measured by the percentage of titles which ap-pears in the mined lexicon.
We then check whetherthe translation in the mined lexicon is an exact matchof one of the translations in the Wikipedia lexicon.Through comparing the results, our mined lexicon iscomparable with the lexicon mined in an unsuper-vised way.
Since the selection is based on phone-mic and semantic clues instead of frequency, a par-enthetical translation candidate will not be selectedif the in-parenthetical English text is failed to betransliterated or translated.
This is one reason thatexplains why we earned a little lower coverage.
An-other reason comes from the low coverage rate ofseed lexicons used for self-training, only 8.65% En-glish words in the partially parallel corpus are cov-ered by the Wanfang dictionary.8 ConclusionWe have proposed a semi-supervised learningframework for mining bilingual lexicons from par-enthetical expressions in monolingual Web pages.We classified the parenthesis expressions into threecategories: abbreviation, transliteration, and transla-tion.
A set of heuristic rules, a self-trained hybridtransliteration model, and a self-trained cascadedtranslation model were proposed for each category,respectively.We investigated the scalability of the self-trainedtransliteration and translation models by trainingthem with different amount of data.
The results shewthe stability (transliteration) and feasibility (transla-tion) of our proposals.
Through employing the par-allel Wikipedia article titles as a gold standard lex-icon, we gained the comparable results comparingour semi-supervised framework with our implemen-tation of Lin et al (2008)?s unsupervised miningapproach.ages, they only require that at least one of them be unlinked andthat (suppose ei is unlinked and fj is linked to ek) none of thewords between ei and ek be linked to any word other than fj .431AcknowledgmentsThis work was partially supported by Grant-in-Aidfor Specially Promoted Research (MEXT, Japan)and Japanese/Chinese Machine Translation Projectin Special Coordination Funds for Promoting Sci-ence and Technology (MEXT, Japan).
We thankthe anonymous reviewers for their constructive com-ments.ReferencesCao, Guihong, Jianfeng Gao, and Jian-Yun Nie.
2007.A system to Mine Large-Scale Bilingual Dictionar-ies from Monolingual Web Pages.
In MT Summit XI.pages 57?64, Copenhagen, Denmark.Cheng, Pu-Jen, Yi-Cheng Pan, Wen-Hsiang Lu, and Lee-Feng Chien.
2004.
Creating Multilingual TranslationLexicons with Regional Variations Using Web Cor-pora.
In ACL 2004, pages 534?541, Barcelona,Spain.Creutz, Mathias and Krista Lagus.
2007.
UnsupervisedModels for Morpheme Segmentation and MorphologyLearning.
ACM Transactions on Speech and Lan-guage Processing, 4(1):Article 3.Dempster, A. P., N. M. Laird and D. B. Rubin.
1977.Maximum Likelihood from Incomplete Data via theEM Algorithm.
Journal of the Royal Statistical Soci-ety, 39:1?38.Gale, W. and K. Church.
1991.
Identifying word corre-spondence in parallel text.
In DARPA NLP Workshop.Gao, Jianfeng, Mu Li, Andi Wu, and Chang-Ning Huang.2006.
Chinese Word Segmentation and Named EntityRecognition: A Pragmatic Approach.
ComputationalLinguistics, 31(4):531?574.Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein 2008.
Learning Bilingual Lexiconsfrom Monolingual Corpora.
In ACL-08:HLT.
pages771?779, Columbus, Ohio.Jiang, Long, Ming Zhou, Lee-Feng Chien, and ChengNiu.
2007.
Named Entity Translation with Web Min-ing and Transliteration.
In IJCAI 2007. pages 1629?1634, Hyderabad, India.Koehn, Philipp, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In ACL2007 Poster Session, pages 177?180.Koehn, Philipp and Kevin Knight.
2002.
Learninga translation lexicon from monolingual corpora.
InSIGLEX 2002, pages 9?16.Kuo, Jin-Shea, Haizhou Li, and Ying-Kuei Yang.
2006.Learning Transliteration Lexicons from the Web.
InCOLING-ACL 2006. pages 1129?1136.Lin, Dekang, Shaojun Zhao, Benjamin Van Durme, andMarius Pas?ca.
2008.
Mining Parenthetical Transla-tions from the Web by Word Alignment.
In ACL-08:HLT, pages 994?1002, Columbus, Ohio.Matthews, David.
2007.
Machine Transliteration ofProper Names.
A Thesis of Master.
University of Ed-inburgh.McClosky, David, Eugene Charniak, and Mark Johnson2008.
When is Self-Training Effective for Parsing?
InProceedings of the 22nd International Conference onComputational Linguistics (Coling 2008), pages 561?568, manchester, UK.Melamed, I. Dan.
2000.
Models of Translational Equiv-alence among Words.
Computational Linguistics,26(2):221?249.Och, Franz Josef and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51.Oh, Jong-Hoon, Key-Sun Choi, and Hitoshi Isahara.2006.
A Comparison of Different Machine Translit-eration Models.
Journal of Artifical Intelligence Re-search, 27:119?151.Okazaki, Naoaki and Sophia Ananiadou.
2006.
Buildingan Abbreviation Dictionary Using a Term RecognitionApproach.
Bioinformatics, 22(22):3089?3095.Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for Automatic Eval-uation of Machine Translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics (ACL).
pages 311?318, Philadel-phia.Shao, Li and Hwee Tou Ng.
2004.
Mining New WordTranslations from Comparable Corpora.
In Proceed-ings of the 20th International Conference on Com-putational Linguistics (COLING), pages 618?624,Geneva, Switzerland.Wu, Xianchao, Naoaki Okazaki, Takashi Tsunakawa, andJun?ichi Tsujii.
2008.
Improving English-to-ChineseTranslation for Technical Terms Using MorphologicalInformation.
In Proceedings of the 8th Conference ofthe Association for Machine Translation in the Ameri-cas (AMTA), pages 202?211, Waikiki, Hawai?i.Yarowsky, David.
1995.
Unsupervised Word Sense Dis-ambiguation Rivaling Supervised Methods.
In Pro-ceedings of the 33rd annual meeting on Associationfor Computational Linguistics, pages 189?196, Cam-bridge, Massachusetts.Zhu, Xiaojin.
2007.
Semi-Supervised Learning Litera-ture Survery.
University of Wisconsin - Madison.432
