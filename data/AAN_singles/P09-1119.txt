Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1057?1065,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPA Generative Blog Post Retrieval Model that UsesQuery Expansion based on External CollectionsWouter Weerkampw.weerkamp@uva.nlKrisztian Balogk.balog@uva.nlISLA, University of AmsterdamMaarten de Rijkemdr@science.uva.nlAbstractUser generated content is characterizedby short, noisy documents, with manyspelling errors and unexpected languageusage.
To bridge the vocabulary gap be-tween the user?s information need anddocuments in a specific user generatedcontent environment, the blogosphere, weapply a form of query expansion, i.e.,adding and reweighing query terms.
Sincethe blogosphere is noisy, query expansionon the collection itself is rarely effectivebut external, edited collections are moresuitable.
We propose a generative modelfor expanding queries using external col-lections in which dependencies betweenqueries, documents, and expansion doc-uments are explicitly modeled.
Differ-ent instantiations of our model are dis-cussed and make different (in)dependenceassumptions.
Results using two exter-nal collections (news andWikipedia) showthat external expansion for retrieval of usergenerated content is effective; besides,conditioning the external collection on thequery is very beneficial, and making can-didate expansion terms dependent on justthe document seems sufficient.1 IntroductionOne of the grand challenges in information re-trieval is to bridge the vocabulary gap between auser and her information need on the one hand andthe relevant documents on the other (Baeza-Yatesand Ribeiro-Neto, 1999).
In the setting of blogsor other types of user generated content, bridgingthis gap becomes even more challenging.
This hasseveral causes: (i) the spelling errors, unusual, cre-ative or unfocused language usage resulting fromthe lack of top-down rules and editors in the con-tent creation process, and (ii) the (often) limitedlength of user generated documents.Query expansion, i.e., modifying the query byadding and reweighing terms, is an often usedtechnique to bridge the vocabulary gap.
In gen-eral, query expansion helps more queries thanit hurts (Balog et al, 2008b; Manning et al,2008).
However, when working with user gener-ated content, expanding a query with terms takenfrom the very corpus in which one is searchingtends to be less effective (Arguello et al, 2008a;Weerkamp and de Rijke, 2008b)?topic drift isa frequent phenomenon here.
To be able to ar-rive at a richer representation of the user?s infor-mation need, while avoiding topic drift resultingfrom query expansion against user generated con-tent, various authors have proposed to expand thequery against an external corpus, i.e., a corpus dif-ferent from the target (user generated) corpus fromwhich documents need to be retrieved.Our aim in this paper is to define and evaluategenerative models for expanding queries using ex-ternal collections.
We propose a retrieval frame-work in which dependencies between queries,documents, and expansion documents are explic-itly modeled.
We instantiate the framework inmultiple ways by making different (in)dependenceassumptions.
As one of the instantiations we ob-tain the mixture of relevance models originallyproposed by Diaz and Metzler (2006).We address the following research questions:(i) Can we effectively apply external expansion inthe retrieval of user generated content?
(ii) Doesconditioning the external collection on the queryhelp improve retrieval performance?
(iii) Can weobtain a good estimate of this query-dependentcollection probability?
(iv) Which of the collec-tion, the query, or the document should the selec-tion of an expansion term be dependent on?
Inother words, what are the strongest simplificationsin terms of conditional independencies betweenvariables that can be assumed, without hurting per-formance?
(v) Do our models show similar behav-ior across topics or do we observe strong per-topic1057differences between models?The remainder of this paper is organized as fol-lows.
We discuss previous work related to queryexpansion and external sources in ?2.
Next, weintroduce our retrieval framework (?3) and con-tinue with our main contribution, external expan-sion models, in ?4.
?5 details how the componentsof the model can be estimated.
We put our modelsto the test, using the experimental setup discussedin ?6, and report on results in ?7.
We discuss ourresults (?8) and conclude in ?9.2 Related WorkRelated work comes in two main flavors: (i) querymodeling in general, and (ii) query expansion us-ing external sources (external expansion).
Westart by shortly introducing the general ideas be-hind query modeling, and continue with a quickoverview of work related to external expansion.2.1 Query ModelingQuery modeling, i.e., transformations of simplekeyword queries into more detailed representa-tions of the user?s information need (e.g., by as-signing (different) weights to terms, expanding thequery, or using phrases), is often used to bridge thevocabulary gap between the query and the doc-ument collection.
Many query expansion tech-niques have been proposed, and they mostly fallinto two categories, i.e., global analysis and localanalysis.
The idea of global analysis is to expandthe query using global collection statistics based,for instance, on a co-occurrence analysis of the en-tire collection.
Thesaurus- and dictionary-basedexpansion as, e.g., in Qiu and Frei (1993), alsoprovide examples of the global approach.Our focus in this paper is on local approachesto query expansion, that use the top retrieved doc-uments as examples from which to select termsto improve the retrieval performance (Rocchio,1971).
In the setting of language modeling ap-proaches to query expansion, the local analysisidea has been instantiated by estimating addi-tional query language models (Lafferty and Zhai,2003; Tao and Zhai, 2006) or relevance mod-els (Lavrenko and Croft, 2001) from a set of feed-back documents.
Yan and Hauptmann (2007) ex-plore query expansion in a multimedia setting.Balog et al (2008b) compare methods for sam-pling expansion terms to support query-dependentand query-independent query expansion; the lat-ter is motivated by the wish to increase ?aspectrecall?
and attempts to uncover aspects of the in-formation need not captured by the query.
Kur-land et al (2005) also try to uncover multiple as-pects of a query, and to that they provide an iter-ative ?pseudo-query?
generation technique, usingcluster-based language models.
The notion of ?as-pect recall?
is mentioned in (Buckley, 2004; Har-man and Buckley, 2004) and identified as one ofthe main reasons of failure of the current informa-tion retrieval systems.
Even though we acknowl-edge the possibilities of our approach in improvingaspect recall, by introducing aspects mainly cov-ered by the external collection being used, we arecurrently unable to test this assumption.2.2 External ExpansionThe use of external collections for query expan-sion has a long history, see, e.g., (Kwok et al,2001; Sakai, 2002).
Diaz and Metzler (2006) werethe first to give a systematic account of query ex-pansion using an external corpus in a languagemodeling setting, to improve the estimation of rel-evance models.
As will become clear in ?4, Diazand Metzler?s approach is an instantiation of ourgeneral model for external expansion.Typical query expansion techniques, such aspseudo-relevance feedback, using a blog or blogpost corpus do not provide significant perfor-mance improvements and often dramatically hurtperformance.
For this reason, query expansionusing external corpora has been a popular tech-nique at the TREC Blog track (Ounis et al, 2007).For blog post retrieval, several TREC participantshave experimented with expansion against exter-nal corpora, usually a news corpus, Wikipedia, theweb, or a mixture of these (Zhang and Yu, 2007;Java et al, 2007; Ernsting et al, 2008).
For theblog finding task introduced in 2007, TREC par-ticipants again used expansion against an exter-nal corpus, usually Wikipedia (Elsas et al, 2008a;Ernsting et al, 2008; Balog et al, 2008a; Fautschand Savoy, 2008; Arguello et al, 2008b).
The mo-tivation underlying most of these approaches is toimprove the estimation of the query representa-tion, often trying to make up for the unedited na-ture of the corpus from which posts or blogs needto be retrieved.
Elsas et al (2008b) go a step fur-ther and develop a query expansion technique us-ing the links in Wikipedia.Finally, Weerkamp and de Rijke (2008b) study1058external expansion in the setting of blog retrievalto uncover additional perspectives of a given topic.We are driven by the same motivation, but wherethey considered rank-based result combinationsand simple mixtures of query models, we takea more principled and structured approach, anddevelop four versions of a generative model forquery expansion using external collections.3 Retrieval FrameworkWe work in the setting of generative languagemodels.
Here, one usually assumes that a doc-ument?s relevance is correlated with query likeli-hood (Ponte and Croft, 1998; Miller et al, 1999;Hiemstra, 2001).
Within the language model-ing approach, one builds a language model fromeach document, and ranks documents based on theprobability of the document model generating thequery.
The particulars of the language modelingapproach have been discussed extensively in theliterature (see, e.g., Balog et al (2008b)) and willnot be repeated here.
Our final formula for rankingdocuments given a query is based on Eq.
1:logP (D|Q) ?logP (D) +?t?QP (t|?Q) logP (t|?D) (1)Here, we see the prior probability of a documentbeing relevant, P (D) (which is independent of thequery Q), the probability of a term t for a givenquery model, ?Q, and the probability of observ-ing the term t given the document model, ?D.Our main interest lies in in obtaining a better es-timate of P (t|?Q).
To this end, we take the querymodel to be a linear combination of the maximum-likelihood query estimate P (t|Q) and an expandedquery model P (t|Q?
):P (t|?Q) = ?Q ?P (t|Q)+ (1?
?Q) ?P (t|Q?)
(2)In the next section we introduce our models for es-timating p(t|Q?
), i.e., query expansion using (mul-tiple) external collections.4 Query Modeling ApproachOur goal is to build an expanded query model thatcombines evidence from multiple external collec-tions.
We estimate the probability of a term t in theexpanded query Q?
using a mixture of collection-specific query expansion models.P (t|Q?)
=?c?C P (t|Q, c) ?
P (c|Q), (3)where C is the set of document collections.To estimate the probability of a term given thequery and the collection, P (t|Q, c), we computethe expectation over the documents in the collec-tion c:P (t|Q, c) =?D?cP (t|Q, c,D) ?
P (D|Q, c).
(4)Substituting Eq.
4 back into Eq.
3 we getP (t|Q?)
= (5)?c?CP (c|Q) ?
?D?cP (t|Q, c,D) ?
P (D|Q, c).This, then, is our query model for combining evi-dence from multiple sources.The following subsections introduce four in-stances of the general external expansion model(EEM) we proposed in this section; each of the in-stances differ in independence assumptions:?
EEM1 (?4.1) assumes collection c to be inde-pendent of query Q and document D jointly,and document D individually, but keeps thedependence on Q and of t and Q on D.?
EEM2 (?4.2) assumes that term t and collec-tion c are conditionally independent, givendocument D and query Q; moreover, D andQ are independent given c but the depen-dence of t and Q on D is kept.?
EEM3 (?4.3) assumes that expansion term tand original query Q are independent givendocument D.?
On top of EEM3, EEM4 (?4.4) makes onemore assumption, viz.
the dependence of col-lection c on query Q.4.1 External Expansion Model 1 (EEM1)Under this model we assume collection c to beindependent of query Q and document D jointly,and document D individually, but keep the depen-dence on Q.
We rewrite P (t|Q, c) as follows:P (t|Q, c)=?D?cP (t|Q,D) ?
P (t|c) ?
P (D|Q)=?D?cP (t, Q|D)P (Q|D)?
P (t|c) ?P (Q|D)P (D)P (Q)?
?D?cP (t, Q|D) ?
P (t|c) ?
P (D) (6)Note that we drop P (Q) from the equation as itdoes not influence the ranking of terms for a given1059query Q.
Further, P (D) is the prior probabilityof a document, regardless of the collection it ap-pears in (as we assumed D to be independent ofc).
We assume P (D) to be uniform, leading to thefollowing equation for ranking expansion terms:P (t|Q?)
?
?c?CP (t|c) ?
P (c|Q) ?
?D?cP (t, Q|D).
(7)In this model we capture the probability of the ex-pansion term given the collection (P (t|c)).
Thisallows us to assign less weight to terms that areless meaningful in the external collection.4.2 External Expansion Model 2 (EEM2)Here, we assume that term t and collection c areconditionally independent, given document D andquery Q: P (t|Q, c,D) = P (t|Q,D).
This leavesus with the following:P (t|Q,D) =P (t, Q,D)P (Q,D)=P (t, Q|D) ?
P (D)P (Q|D) ?
P (D)=P (t, Q|D)P (Q|D)(8)Next, we assume document D and query Q tobe independent given collection c: P (D|Q, c) =P (D|c).
Substituting our choices into Eq.
4 givesus our second way of estimating P (t|Q, c):P (t|Q, c) =?D?cP (t, Q|D)P (Q|D)?
P (D|c) (9)Finally, we put our choices so far together, andimplement Eq.
9 in Eq.
3, yielding our final termranking equation:P (t|Q?)
?
(10)?c?CP (c|Q) ?
?D?cP (t, Q|D)P (Q|D)?
P (D|c).4.3 External Expansion Model 3 (EEM3)Here we assume that expansion term t and bothcollection c and original query Q are independentgiven document D. Hence, we set P (t|Q, c,D) =P (t|D).
ThenP (t|Q, c)=?D?cP (t|D) ?
P (D|Q, c)=?D?cP (t|D) ?P (Q|D, c) ?
P (D|c)P (Q|c)?
?D?cP (t|D) ?
P (Q|D, c) ?
P (D|c)We dropped P (Q|c) as it does not influence theranking of terms for a given query Q. Assumingindependence of Q and c given D, we obtainP (t|Q, c) ?
?D?cP (D|c) ?
P (t|D) ?
P (Q|D)soP (t|Q?)
?
?c?CP (c|Q) ?
?D?cP (D|c) ?
P (t|D) ?
P (Q|D).We follow Lavrenko and Croft (2001) and assumethat P (D|c) = 1|Rc| , the size of the set of topranked documents in c (denoted by Rc), finally ar-riving atP (t|Q?)
?
?c?CP (c|Q)|Rc|?
?D?RcP (t|D) ?
P (Q|D).
(11)4.4 External Expansion Model 4 (EEM4)In this fourth model we start from EEM3 and dropthe assumption that c depends on the query Q, i.e.,P (c|Q) = P (c), obtainingP (t|Q?)
?
?c?CP (c)|Rc|?
?D?RcP (t|D) ?
P (Q|D).
(12)Eq.
12 is in fact the ?mixture of relevance models?external expansion model proposed by Diaz andMetzler (2006).
The fundamental difference be-tween EEM1, EEM2, EEM3 on the one hand andEEM4 on the other is that EEM4 assumes inde-pendence between c and Q (thus P (c|Q) is set toP (c)).
That is, the importance of the external col-lection is independent of the query.
How reason-able is this choice?
Mishne and de Rijke (2006)examined queries submitted to a blog search en-gine and found many to be either news-relatedcontext queries (that aim to track mentions of anamed entity) or concept queries (that seek postsabout a general topic).
For context queries such ascheney hunting (TREC topic 867) a news collec-tion is likely to offer different (relevant) aspectsof the topic, whereas for a concept query such asjihad (TREC topic 878) a knowledge source suchas Wikipedia seems an appropriate source of termsthat capture aspects of the topic.
These observa-tions suggest the collection should depend on thequery.1060EEM3 and EEM4 assume that expansion term tand original query Q are independent given doc-ument D. This may or may not be too strong anassumption.
Models EEM1 and EEM2 also makeindependence assumptions, but weaker ones.5 Estimating ComponentsThe models introduced above offer us severalchoices in estimating the main components.
Be-low we detail how we estimate (i) P (c|Q), theimportance of a collection for a given query,(ii) P (t|c), the unimportance of a term for an ex-ternal collection, (iii) P (Q|D), the relevance ofa document in the external collection for a givenquery, and (iv) P (t, Q|D), the likelihood of a termco-occurring with the query, given a document.5.1 Importance of a CollectionRepresented as P (c|Q) in our models, the im-portance of an external collection depends on thequery; how we can estimate this term?
We con-sider three alternatives, in terms of (i) query clar-ity, (ii) coherence and (iii) query-likelihood, usingdocuments in that collection.First, query clarity measures the structure of aset of documents based on the assumption that asmall number of topical terms will have unusu-ally large probabilities (Cronen-Townsend et al,2002).
We compute the query clarity of the topranked documents in a given collection c:clarity(Q, c) =?tP (t|Q) ?
logP (t|Q)P (t|Rc)Finally, we normalize clarity(Q, c) over all col-lections, and set P (c|Q) ?
clarity(Q,c)Pc?
?C clarity(Q,c?)
.Second, a measure called ?coherence score?
isdefined by He et al (2008).
It is the fraction of?coherent?
pairs of documents in a given set ofdocuments, where a coherent document pair is onewhose similarity exceeds a threshold.
The coher-ence of the top ranked documents Rc is:Co(Rc) =?i6=j?
{1,...,|Rc|} ?
(di, dj)|Rc|(|Rc| ?
1),where ?
(di, dj) is 1 in case of a similar pair (com-puted using cosine similarity), and 0 otherwise.Finally, we set P (c|Q) ?
Co(Rc)Pc?
?C Co(Rc?
).Third, we compute the conditional probabilityof the collection using Bayes?
theorem.
We ob-serve that P (c|Q) ?
P (Q|c) (omitting P (Q) as itwill not influence the ranking and P (c) which wetake to be uniform).
Further, for the sake of sim-plicity, we assume that all documents within c areequally important.
Then, P (Q|c) is estimated asP (Q|c) =1|c|?
?D?cP (Q|D) (13)where P (Q|D) is estimated as described in ?5.3,and |c| is the number of documents in c.5.2 Unimportance of a TermRather than simply estimating the importance ofa term for a given query, we also estimate theunimportance of a term for a collection; i.e., weassign lower probability to terms that are com-mon in that collection.
Here, we take a straight-forward approach in estimating this, and defineP (t|c) = 1 ?
n(t,c)Pt?
n(t?,c) .5.3 Likelihood of a QueryWe need an estimate of the probability of a querygiven a document, P (Q|D).
We do so by usingHauff et al (2008)?s refinement of term dependen-cies in the query as proposed by Metzler and Croft(2005).5.4 Likelihood of a TermEstimating the likelihood of observing both thequery and a term for a given document P (t, Q|D)is done in a similar way to estimating P (Q|D), butnow for t, Q in stead of Q.6 Experimental SetupIn his section we detail our experimental setup:the (external) collections we use, the topic setsand relevance judgements available, and the sig-nificance testing we perform.6.1 Collections and TopicsWe make use of three collections: (i) a collec-tion of user generated documents (blog posts),(ii) a news collection, and (iii) an online knowl-edge source.
The blog post collection is the TRECBlog06 collection (Ounis et al, 2007), which con-tains 3.2 million blog posts from 100,000 blogsmonitored for a period of 11 weeks, from Decem-ber 2005 to March 2006; all posts from this periodhave been stored as HTML files.
Our news col-lection is the AQUAINT-2 collection (AQUAINT-2, 2007), from which we selected news articlesthat appeared in the period covered by the blog1061collection, leaving us with about 150,000 newsarticles.
Finally, we use a dump of the EnglishWikipedia from August 2007 as our online knowl-edge source; this dump contains just over 3.8 mil-lion encyclopedia articles.During 2006?2008, the TRECBlog06 collec-tion has been used for the topical blog post re-trieval task (Weerkamp and de Rijke, 2008a) at theTREC Blog track (Ounis et al, 2007): to retrieveposts about a given topic.
For every year, 50 topicswere developed, consisting of a title field, descrip-tion, and narrative; we use only the title field, andignore the other available information.
For all 150topics relevance judgements are available.6.2 Metrics and SignificanceWe report on the standard IR metrics Mean Aver-age Precision (MAP), precision at 5 and 10 doc-uments (P5, P10), and the Mean Reciprocal Rank(MRR).
To determine whether or not differencesbetween runs are significant, we use a two-tailedpaired t-test, and report on significant differencesfor ?
= .05 (M and O) and ?
= .01 (N and H).7 ResultsWe first discuss the parameter tuning for our fourEEM models in Section 7.1.
We then report on theresults of applying these settings to obtain our re-trieval results on the blog post retrieval task.
Sec-tion 7.2 reports on these results.
We follow with acloser look in Section 8.7.1 ParametersOur model has one explicit parameter, and onemore or less implicit parameter.
The obvious pa-rameter is ?Q, used in Eq.
2, but also the num-ber of terms to include in the final query modelmakes a difference.
For training of the param-eters we use two TREC topic sets to train andtest on the held-out topic set.
From the trainingwe conclude that the following parameter settingswork best across all topics: (EEM1) ?Q = 0.6,30 terms; (EEM2) ?Q = 0.6, 40 terms; (EEM3and EEM4) ?Q = 0.5, 30 terms.
In the remainderof this section, results for our models are reportedusing these parameter settings.7.2 Retrieval ResultsAs a baseline we use an approach without exter-nal query expansion, viz.
Eq.
1.
In Table 1 welist the results on the topical blog post finding taskmodel P (c|Q) MAP P5 P10 MRRBaseline 0.3815 0.6813 0.6760 0.7643EEM1uniform 0.3976N 0.7213N 0.7080N 0.79980.8N/0.2W 0.3992 0.7227 0.7107 0.7988coherence 0.3976 0.7187 0.7060 0.7976query clarity 0.3970 0.7187 0.7093 0.7929P (Q|c) 0.3983 0.7267 0.7093 0.7951oracle 0.4126N 0.7387M 0.7320N 0.8252MEEM2uniform 0.3885N 0.7053M 0.6967M 0.77060.9N/0.1W 0.3895 0.7133 0.6953 0.7736coherence 0.3890 0.7093 0.7020 0.7740query clarity 0.3872 0.7067 0.6953 0.7745P (Q|c) 0.3883 0.7107 0.6967 0.7717oracle 0.3995N 0.7253N 0.7167N 0.7856EEM3uniform 0.4048N 0.7187M 0.7207N 0.8261Ncoherence 0.4058 0.7253 0.7187 0.8306query clarity 0.4033 0.7253 0.7173 0.8228P (Q|c) 0.3998 0.7253 0.7100 0.8133oracle 0.4194N 0.7493N 0.7353N 0.8413EEM4 0.5N/0.5W 0.4048N 0.7187M 0.7207N 0.8261NTable 1: Results for all model instances on all top-ics (i.e., 2006, 2007, and 2008); aN/bW standsfor the weights assigned to the news (a) andWikipedia corpora (b).
Significance is tested be-tween (i) each uniform run and the baseline, and(ii) each other setting and its uniform counterpart.of (i) our baseline, and (ii) our model (instanti-ated by EEM1, EEM2, EEM3, and EEM4).
Forall models that contain the query-dependent col-lection probability (P (c|Q)) we report on multi-ple ways of estimating this: (i) uniform, (ii) bestglobal mixture (independent of the query, obtainedby a sweep over collection probabilities), (iii) co-herence, (iv) query clarity, (v) P (Q|c), and (vi) us-ing an oracle for which optimal settings were ob-tained by the same sweep as (ii).
Note that meth-ods (i) and (ii) are not query dependent; for EEM3we do not mention (ii) since it equals (i).
Finally,for EEM4 we only have a query-independent com-ponent, P (c): the best performance here is ob-tained using equal weights for both collections.A few observations.
First, our baseline per-forms well above the median for all three years(2006?2008).
Second, in each of its four instancesour model for query expansion against externalcorpora improves over the baseline.
Third, wesee that it is safe to assume that a term is depen-dent only on the document from which it is sam-pled (EEM1 vs. EEM2 vs. EEM3).
EEM3 makesthe strongest assumptions about terms in this re-spect, yet it performs best.
Fourth, capturing thedependence of the collection on the query helps,as we can see from the significant improvementsof the ?oracle?
runs over their ?uniform?
counter-parts.
However, we do not have a good methodyet for automatically estimating this dependence,1062as is clear from the insignificant differences be-tween the runs labeled ?coherence,?
?query clar-ity,?
?P (Q|c)?
and the run labeled ?uniform.
?8 DiscussionRather than providing a pairwise comparison of allruns listed in the previous section, we consider twopairwise comparisons?between (an instantion of)our model and the baseline, and between two in-stantiations of our model?and highlight phenom-ena that we also observed in other pairwise com-parisons.
Based on this discussion, we also con-sider a combination of approaches.8.1 EEM1 vs. the BaselineWe zoom in on EEM1 and make a per-topic com-parison against the baseline.
First of all, weobserve behavior typical for all query expansionmethods: some topics are helped, some are not af-fected, and some are hurt by the use of EEM1; seeFigure 1, top row.
Specifically, 27 topics show aslight drop in AP (maximum drop is 0.043 AP), 3topics do not change (as no expansion terms areidentified) and the remainder of the topics (120)improve in AP.
The maximum increase in AP is0.5231 (+304%) for topic 949 (ford bell); Top-ics 887 (world trade organization, +87%), 1032(I walk the line, +63%), 865 (basque, +53%), and1014 (tax break for hybrid automobiles, +50%)also show large improvements.
The largest drop (-20% AP) is for topic 1043 (a million little pieces,a controversial memoir that was in the news dur-ing the time coverd by the blog crawl); because wedo not do phrase or entity recognition in the query,but apply stopword removal, it is reduced to mil-lion pieces which introduced a lot of topic drift.Let us examine the ?collection preference?
oftopics: 35 had a clear preference for Wikipedia, 32topics for news, and the remainder (83 topics) re-quired a mixture of both collections.
First, we lookat topics that require equal weights for both collec-tions; topic 880 (natalie portman, +21% AP) con-cerns a celebrity with a largeWikipedia biography,as well as news coverage due to new movie re-leases during the period covered by the blog crawl.Topic 923 (challenger, +7% AP) asks for infor-mation on the space shuttle that exploded dur-ing its launch; the 20th anniversary of this eventwas commemorated during the period covered bythe crawl and therefore it is newsworthy as wellas present in Wikipedia (due to its historic im-pact).
Finally, topic 869 (muhammad cartoon,+20% AP) deals with the controversy surroundingthe publication of cartoons featuring Muhammad:besides its obvious news impact, this event is ex-tensively discussed in multiple Wikipedia articles.As to topics that have a preference forWikipedia, we see some very general ones (as is tobe expected): Topic 942 (lawful access, +30%AP)on the government accessing personal files; Topic1011 (chipotle restaurant, +13% AP) on infor-mation concerning the Chipotle restaurants; Topic938 (plug awards, +21% AP) talks about an awardshow.
Although this last topic could be expected tohave a clear preference for expansion terms fromthe news corpus, the awards were not handed outduring the period covered by the news collectionand, hence, full weight is given to Wikipedia.At the other end of the scale, topics that show apreference for the news collection are topic 1042(david irving, +28% AP), who was on trial dur-ing the period of the crawl for denying the Holo-caust and received a lot of media attention.
Furtherexamples include Topic 906 (davos, +20% AP),which asks for information on the annual worldeconomic forum meeting in Davos in January,something typically related to news, and topic 949(ford bell, +304% AP), which seeks informationon Ford Bell, Senate candidate at the start of 2006.8.2 EEM1 vs. EEM3Next we turn to a comparison between EEM1and EEM3.
Theoretically, the main differencebetween these two instantiations of our generalmodel is that EEM3 makes much stronger sim-plifying indepence assumptions than EEM1.
InFigure 1 we compare the two, not only againstthe baseline, but, more interestingly, also in termsof the difference in performance brought about byswitching from uniform estimation of P (c|Q) tooracle estimation.
Most topics gain in AP whengoing from the uniform distribution to the oraclesetting.
This happens for both models, EEM1 andEEM3, leading to less topics decreasing in APover the baseline (the right part of the plots) andmore topics increasing (the left part).
A secondobservation is that both gains and losses are higherfor EEM3 than for EEM1.Zooming in on the differences between EEM1and EEM3, we compare the two in the same way,now using EEM3 as ?baseline?
(Figure 2).
We ob-serve that EEM3 performs better than EEM1 in 871063-0.4-0.200.20.4AP differencetopics-0.4-0.200.20.4AP differencetopics-0.4-0.200.20.4AP differencetopics-0.4-0.200.20.4AP differencetopicsFigure 1: Per-topic AP differences between thebaseline and (Top): EEM1 and (Bottom): EEM3,for (Left): uniform P (c|Q) and (Right): oracle.-0.4-0.200.20.4APdifferencetopicsFigure 2: Per-topic AP differences between EEM3and EEM1 in the oracle setting.cases, while EEM1 performs better for 60 topics.Topics 1041 (federal shield law, 47% AP), 1028(oregon death with dignity act, 32%AP), and 1032(I walk the line, 32% AP) have the highest differ-ence in favor of EEM3; Topics 877 (sonic food in-dustry, 139% AP), 1013 (iceland european union,25% AP), and 1002 (wikipedia primary source,23% AP) are helped most by EEM1.
Overall,EEM3 performs significantly better than EEM1 interms of MAP (for ?
= .05), but not in terms ofthe early precision metrics (P5, P10, and MRR).8.3 Combining Our ApproachesOne observation to come out of ?8.1 and 8.2 is thatdifferent topics prefer not only different externalexpansion corpora but also different external ex-pansion methods.
To examine this phenomemon,we created an articificial run by taking, for ev-ery topic, the best performing model (with settingsoptimized for the topic).
Twelve topics preferredthe baseline, 37 EEM1, 20 EEM2, and 81 EEM3.The articifical run produced the following results:MAP 0.4280, P5 0.7600, P10 0.7480, and MRR0.8452; the differences in MAP and P10 betweenthis run and EEM3 are significant for ?
= .01.We leave it as future work to (learn to) predict fora given topic, which approach to use, thus refiningongoing work on query difficulty prediction.9 ConclusionsWe explored the use of external corpora for queryexpansion in a user generated content setting.
Weintroduced a general external expansion model,which offers various modeling choices, and in-stantiated it based on different (in)dependence as-sumptions, leaving us with four instances.Query expansion using external collection iseffective for retrieval in a user generated con-tent setting.
Furthermore, conditioning the collec-tion on the query is beneficial for retrieval perfor-mance, but estimating this component remains dif-ficult.
Dropping the dependencies between termsand collection and terms and query leads to bet-ter performance.
Finally, the best model is topic-dependent: constructing an artificial run based onthe best model per topic achieves significant betterresults than any of the individual models.Future work focuses on two themes: (i) topic-dependent model selection and (ii) improved es-timates of components.
As to (i), we first wantto determine whether a query should be expanded,and next select the appropriate expansion model.For (ii), we need better estimates of P (Q|c);one aspect that could be included is taking P (c)into account in the query-likelihood estimate ofP (Q|c).
One can make this dependent on the taskat hand (blog post retrieval vs. blog feed search).Another possibility is to look at solutions used indistributed IR.
Finally, we can also include the es-timation of P (D|c), the importance of a documentin the collection.AcknowledgementsWe thank our reviewers for their valuable feed-back.
This research is supported by the DuOMAnproject carried out within the STEVIN programmewhich is funded by the Dutch and Flemish Gov-ernments (http://www.stevin-tst.org) under projectnumber STE-09-12, and by the Netherlands Or-ganisation for Scientific Research (NWO) underproject numbers 017.001.190, 640.001.501, 640.-002.501, 612.066.512, 612.061.814, 612.061.815,640.004.802.1064ReferencesAQUAINT-2 (2007).
URL: http://trec.nist.gov/data/qa/2007 qadata/qa.07.guidelines.html#documents.Arguello, J., Elsas, J., Callan, J., and Carbonell, J.
(2008a).Document representation and query expansion models forblog recommendation.
In Proceedings of ICWSM 2008.Arguello, J., Elsas, J. L., Callan, J., and Carbonell, J. G.(2008b).
Document representation and query expansionmodels for blog recommendation.
In Proc.
of the 2nd Intl.Conf.
on Weblogs and Social Media (ICWSM).Baeza-Yates, R. and Ribeiro-Neto, B.
(1999).
Modern Infor-mation Retrieval.
ACM.Balog, K., Meij, E., Weerkamp, W., He, J., and de Rijke, M.(2008a).
The University of Amsterdam at TREC 2008:Blog, Enterprise, and Relevance Feedback.
In TREC 2008Working Notes.Balog, K., Weerkamp, W., and de Rijke, M. (2008b).
A fewexamples go a long way: constructing query models fromelaborate query formulations.
In SIGIR ?08: Proceedingsof the 31st annual international ACM SIGIR conference onResearch and development in information retrieval, pages371?378, New York, NY, USA.
ACM.Buckley, C. (2004).
Why current IR engines fail.
In SIGIR?04, pages 584?585.Cronen-Townsend, S., Zhou, Y., and Croft, W. B.
(2002).
Pre-dicting query performance.
In SIGIR02, pages 299?306.Diaz, F. and Metzler, D. (2006).
Improving the estimation ofrelevance models using large external corpora.
In SIGIR?06: Proceedings of the 29th annual international ACMSIGIR conference on Research and development in infor-mation retrieval, pages 154?161, New York, NY, USA.ACM.Elsas, J., Arguello, J., Callan, J., and Carbonell, J.
(2008a).Retrieval and feedback models for blog distillation.
In TheSixteenth Text REtrieval Conference (TREC 2007) Pro-ceedings.Elsas, J. L., Arguello, J., Callan, J., and Carbonell, J. G.(2008b).
Retrieval and feedback models for blog feedsearch.
In SIGIR ?08: Proceedings of the 31st annual in-ternational ACM SIGIR conference on Research and de-velopment in information retrieval, pages 347?354, NewYork, NY, USA.
ACM.Ernsting, B., Weerkamp, W., and de Rijke, M. (2008).
Lan-guage modeling approaches to blog post and feed finding.In The Sixteenth Text REtrieval Conference (TREC 2007)Proceedings.Fautsch, C. and Savoy, J.
(2008).
UniNE at TREC 2008: Factand Opinion Retrieval in the Blogsphere.
In TREC 2008Working Notes.Harman, D. and Buckley, C. (2004).
The NRRC reliable in-formation access (RIA) workshop.
In SIGIR ?04, pages528?529.Hauff, C., Murdock, V., and Baeza-Yates, R. (2008).
Im-proved query difficulty prediction for the web.
In CIKM?08: Proceedings of the seventeenth ACM conference onConference on information and knowledge management,pages 439?448.He, J., Larson, M., and de Rijke, M. (2008).
Usingcoherence-based measures to predict query difficulty.In 30th European Conference on Information Retrieval(ECIR 2008), page 689694.
Springer, Springer.Hiemstra, D. (2001).
Using Language Models for Informa-tion Retrieval.
PhD thesis, University of Twente.Java, A., Kolari, P., Finin, T., Joshi, A., and Martineau, J.(2007).
The blogvox opinion retrieval system.
In The Fif-teenth Text REtrieval Conference (TREC 2006) Proceed-ings.Kurland, O., Lee, L., and Domshlak, C. (2005).
Better thanthe real thing?
: Iterative pseudo-query processing usingcluster-based language models.
In SIGIR ?05, pages 19?26.Kwok, K. L., Grunfeld, L., Dinstl, N., and Chan, M. (2001).TREC-9 cross language, web and question-answeringtrack experiments using PIRCS.
In TREC-9 Proceedings.Lafferty, J. and Zhai, C. (2003).
Probabilistic relevance mod-els based on document and query generation.
In LanguageModeling for Information Retrieval, Kluwer InternationalSeries on Information Retrieval.
Springer.Lavrenko, V. and Croft, W. B.
(2001).
Relevance based lan-guage models.
In SIGIR ?01, pages 120?127.Manning, C. D., Raghavan, P., and Schu?tze, H. (2008).
Intro-duction to Information Retrieval.
Cambridge UniversityPress.Metzler, D. and Croft, W. B.
(2005).
A markov random fieldmodel for term dependencies.
In SIGIR ?05, pages 472?479, New York, NY, USA.
ACM.Miller, D., Leek, T., and Schwartz, R. (1999).
A hiddenMarkov model information retrieval system.
In SIGIR ?99,pages 214?221.Mishne, G. and de Rijke, M. (2006).
A study of blog search.In Lalmas, M., MacFarlane, A., Ru?ger, S., Tombros, A.,Tsikrika, T., and Yavlinsky, A., editors, Advances in In-formation Retrieval: Proceedings 28th European Confer-ence on IR Research (ECIR 2006), volume 3936 of LNCS,pages 289?301.
Springer.Ounis, I., Macdonald, C., de Rijke, M., Mishne, G., andSoboroff, I.
(2007).
Overview of the TREC 2006 BlogTrack.
In The Fifteenth Text Retrieval Conference (TREC2006).
NIST.Ponte, J. M. and Croft, W. B.
(1998).
A language modelingapproach to information retrieval.
In SIGIR ?98, pages275?281.Qiu, Y. and Frei, H.-P. (1993).
Concept based query expan-sion.
In SIGIR ?93, pages 160?169.Rocchio, J.
(1971).
Relevance feedback in information re-trieval.
In The SMART Retrieval System: Experiments inAutomatic Document Processing.
Prentice Hall.Sakai, T. (2002).
The use of external text data in cross-language information retrieval based on machine transla-tion.
In Proceedings IEEE SMC 2002.Tao, T. and Zhai, C. (2006).
Regularized estimation of mix-ture models for robust pseudo-relevance feedback.
In SI-GIR ?06: Proceedings of the 29th annual internationalACM SIGIR conference on Research and development ininformation retrieval, pages 162?169, New York, NY,USA.
ACM.Weerkamp, W. and de Rijke, M. (2008a).
Credibility im-proves topical blog post retrieval.
In ACL-08: HLT, pages923?931.Weerkamp, W. and de Rijke, M. (2008b).
Looking at thingsdifferently: Exploring perspective recall for informal textretrieval.
In 8th Dutch-Belgian Information RetrievalWorkshop (DIR 2008), pages 93?100.Yan, R. and Hauptmann, A.
(2007).
Query expansion us-ing probabilistic local feedback with application to mul-timedia retrieval.
In CIKM ?07: Proceedings of the six-teenth ACM conference on Conference on information andknowledge management, pages 361?370, New York, NY,USA.
ACM.Zhang, W. and Yu, C. (2007).
UIC at TREC 2006 Blog Track.In The Fifteenth Text REtrieval Conference (TREC 2006)Proceedings.1065
