Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 224?231,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsSVM Model Tampering and Anchored Learning: A Case Study in HebrewNP ChunkingYoav Goldberg and Michael ElhadadComputer Science DepartmentBen Gurion University of the NegevP.O.B 653 Be?er Sheva 84105, Israelyoavg,elhadad@cs.bgu.ac.ilAbstractWe study the issue of porting a known NLPmethod to a language with little existing NLPresources, specifically Hebrew SVM-basedchunking.
We introduce two SVM-basedmethods ?
Model Tampering and AnchoredLearning.
These allow fine grained analysisof the learned SVM models, which providesguidance to identify errors in the training cor-pus, distinguish the role and interaction oflexical features and eventually construct amodel with ?10% error reduction.
The re-sulting chunker is shown to be robust in thepresence of noise in the training corpus, relieson less lexical features than was previouslyunderstood and achieves an F-measure perfor-mance of 92.2 on automatically PoS-taggedtext.
The SVM analysis methods also providegeneral insight on SVM-based chunking.1 IntroductionWhile high-quality NLP corpora and tools are avail-able in English, such resources are difficult to obtainin most other languages.
Three challenges must bemet when adapting results established in English toanother language: (1) acquiring high quality anno-tated data; (2) adapting the English task definitionto the nature of a different language, and (3) adapt-ing the algorithm to the new language.
This paperpresents a case study in the adaptation of a wellknown task to a language with few NLP resourcesavailable.
Specifically, we deal with SVM based He-brew NP chunking.
In (Goldberg et al, 2006), weestablished that the task is not trivially transferableto Hebrew, but reported that SVM based chunking(Kudo and Matsumoto, 2000) performs well.
Weextend that work and study the problem from 3 an-gles: (1) how to deal with a corpus that is smallerand with a higher level of noise than is available inEnglish; we propose techniques that help identify?suspicious?
data points in the corpus, and identifyhow robust the model is in the presence of noise;(2) we compare the task definition in English and inHebrew through quantitative evaluation of the differ-ences between the two languages by analyzing therelative importance of features in the learned SVMmodels; and (3) we analyze the structure of learnedSVM models to better understand the characteristicsof the chunking problem in Hebrew.While most work on chunking with machinelearning techniques tend to treat the classificationengine as a black-box, we try to investigate the re-sulting classification model in order to understandits inner working, strengths and weaknesses.
We in-troduce two SVM-based methods ?
Model Tamper-ing and Anchored Learning ?
and demonstrate howa fine-grained analysis of SVM models provides in-sights on all three accounts.
The understanding ofthe relative contribution of each feature in the modelhelps us construct a better model, which achieves?10% error reduction in Hebrew chunking, as wellas identify corpus errors.
The methods also providegeneral insight on SVM-based chunking.2 Previous WorkNP chunking is the task of marking the bound-aries of simple noun-phrases in text.
It is a wellstudied problem in English, and was the focus ofCoNLL2000?s Shared Task (Sang and Buchholz,2242000).
Early attempts at NP Chunking were rulelearning systems, such as the Error Driven Prun-ing method of Pierce and Cardie (1998).
Follow-ing Ramshaw and Marcus (1995), the current dom-inant approach is formulating chunking as a clas-sification task, in which each word is classified asthe (B)eginning, (I)nside or (O)outside of a chunk.Features for this classification usually involve localcontext features.
Kudo and Matsumoto (2000) usedSVM as a classification engine and achieved an F-Score of 93.79 on the shared task NPs.
Since SVMis a binary classifier, to use it for the 3-class classi-fication of the chunking task, 3 different classifiers{B/I, B/O, I/O} were trained and their majority votewas taken.NP chunks in the shared task data are BaseNPs,which are non-recursive NPs, a definition first pro-posed by Ramshaw and Marcus (1995).
This defini-tion yields good NP chunks for English.
In (Gold-berg et al, 2006) we argued that it is not applica-ble to Hebrew, mainly because of the prevalenceof the Hebrew?s construct state (smixut).
Smixutis similar to a noun-compound construct, but onethat can join a noun (with a special morphologi-cal marking) with a full NP.
It appears in about40% of Hebrew NPs.
We proposed an alterna-tive definition (termed SimpleNP) for Hebrew NPchunks.
A SimpleNP cannot contain embedded rel-atives, prepositions, VPs and NP-conjunctions (ex-cept when they are licensed by smixut).
It cancontain smixut, possessives (even when they areattached by the ???/of?
preposition) and partitives(and, therefore, allows for a limited amount of re-cursion).
We applied this definition to the HebrewTree Bank (Sima?an et al, 2001), and constructeda moderate size corpus (about 5,000 sentences) forHebrew SimpleNP chunking.
SimpleNPs are differ-ent than English BaseNPs, and indeed some meth-ods that work well for English performed poorlyon Hebrew data.
However, we found that chunk-ing with SVM provides good result for Hebrew Sim-pleNPs.
We analyzed that this success comes fromSVM?s ability to use lexical features, as well as twoHebrew morphological features, namely ?number?and ?construct-state?.One of the main issues when dealing with Hebrewchunking is that the available tree bank is rathersmall, and since it is quite new, and has not beenused intensively, it contains a certain amount of in-consistencies and tagging errors.
In addition, theidentification of SimpleNPs from the tree bank alsointroduces some errors.
Finally, we want to investi-gate chunking in a scenario where PoS tags are as-signed automatically and chunks are then computed.The Hebrew PoS tagger we use introduces about 8%errors (compared with about 4% in English).
Weare, therefore, interested in identifying errors in thechunking corpus, and investigating how the chunkeroperates in the presence of noise in the PoS tag se-quence.3 Model Tampering3.1 Notation and Technical ReviewThis section presents notation as well as a technicalreview of SVM chunking details relevant to the cur-rent study.
Further details can be found in Kudo andMatsumoto (2000; 2003).SVM (Vapnik, 1995) is a supervised binary clas-sifier.
The input to the learner is a set of l train-ing samples (x1, y1), .
.
.
, (xl, yl), x ?
Rn, y ?{+1,?1}.
xi is an n dimensional feature vec-tor representing the ith sample, and yi is the la-bel for that sample.
The result of the learning pro-cess is the set SV of Support Vectors, the asso-ciated weights ?i, and a constant b.
The SupportVectors are a subset of the training vectors, and to-gether with the weights and b they define a hyper-plane that optimally separates the training samples.The basic SVM formulation is of a linear classifier,but by introducing a kernel function K that non-linearly transforms the data from Rn into a higherdimensional space, SVM can be used to performnon-linear classification.
SVM?s decision functionis: y(x) = sgn(?j?SV yj?jK(xj , x) + b)wherex is an n dimensional feature vector to be classi-fied.
In the linear case, K is a dot product oper-ation and the sum w = ?
yj?jxj is an n dimen-sional weight vector assigning weight for each ofthe n features.
The other kernel function we con-sider in this paper is a polynomial kernel of degree2: K(xi, xj) = (xi ?
xj + 1)2.
When using binaryvalued features, this kernel function essentially im-plies that the classifier considers not only the explic-itly specified features, but also all available pairs offeatures.
In order to cope with inseparable data, thelearning process of SVM allows for some misclas-sification, the amount of which is determined by a225parameter C, which can be thought of as a penaltyfor each misclassified training sample.In SVM based chunking, each word and its con-text is considered a learning sample.
We refer tothe word being classified as w0, and to its part-of-speech (PoS) tag, morphology, and B/I/O tag as p0,m0 and t0 respectively.
The information consid-ered for classification is w?cw .
.
.
wcw, p?cp .
.
.
pcp,m?cm .
.
.mcm and t?ct .
.
.
t?1.
The feature vectorF is an indexed list of all the features present inthe corpus.
A feature fi of the form w+1 = dogmeans that the word following the one being clas-sified is ?dog?.
Every learning sample is repre-sented by an n = |F | dimensional binary vector x.xi = 1 iff the feature fi is active in the given sample,and 0 otherwise.
This encoding leads to extremelyhigh dimensional vectors, due to the lexical featuresw?cw .
.
.
wcw.3.2 Introducing Model TamperingAn important observation about SVM classifiers isthat features which are not active in any of the Sup-port Vectors have no effect on the classifier deci-sion.
We introduce Model Tampering, a procedurein which we change the Support Vectors in a modelby forcing some values in the vectors to 0.The result of this procedure is a new Model inwhich the deleted features never take part in the clas-sification.Model tampering is different than feature selec-tion: on the one hand, it is a method that helps usidentify irrelevant features in a model after training;on the other hand, and this is the key insight, re-moving features after training is not the same as re-moving them before training.
The presence of thelow-relevance features during training has an impacton the generalization performed by the learner asshown below.3.3 The Role of Lexical FeaturesIn Goldberg et al (2006), we have established thatusing lexical features increases the chunking F-measure from 78 to over 92 on the Hebrew Tree-bank.
We refine this observation by using ModelTampering, in order to assess the importance of lex-ical features in NP Chunking.
We are interested inidentifying which specific lexical items and contextsimpact the chunking decision, and quantifying theireffect.
Our method is to train a chunking modelon a given training corpus, tamper with the result-ing model in various ways and measure the perfor-mance1 of the tampered models on a test corpus.3.4 Experimental SettingWe conducted experiments both for English and He-brew chunking.
For the Hebrew experiments, we usethe corpora of (Goldberg et al, 2006).
The first oneis derived from the original Treebank by projectingthe full syntactic tree, constructed manually, onto aset of NP chunks according to the SimpleNP rules.We refer to the resulting corpus as HEBGold sincePoS tags are fully reliable.
The HEBErr versionof the corpus is obtained by projecting the chunkboundaries on the sequence of PoS and morphologytags obtained by the automatic PoS tagger of Adler& Elhadad (2006).
This corpus includes an errorrate of about 8% on PoS tags.
The first 500 sen-tences are used for testing, and the rest for training.The corpus contains 27K NP chunks.
For the En-glish experiments, we use the now-standard trainingand test sets that were introduced in (Marcus andRamshaw, 1995)2.
Training was done using Kudo?sYAMCHA toolkit3.
Both Hebrew and English mod-els were trained using a polynomial kernel of de-gree 2, with C = 1.
For English, the features usedwere: w?2 .
.
.
w2, p?2 .
.
.
p2, t?2 .
.
.
t?1.
The samefeatures were used for Hebrew, with the addition ofm?2 .
.
.m2.
These are the same settings as in (Kudoand Matsumoto, 2000; Goldberg et al, 2006).3.5 TamperingsWe experimented with the following tamperings:TopN ?
We define model feature count to be thenumber of Support Vectors in which a feature is ac-tive in a given classifier.
This tampering leaves in themodel only the top N lexical features in each classi-fier, according to their count.NoPOS ?
all the lexical features corresponding toa given part-of-speech are removed from the model.For example, in a NoJJ tampering, all the features ofthe form wi = X are removed from all the supportvectors in which pi = JJ is active.Loc6=i ?
all the lexical features with index i areremoved from the model e.g., in a Loc6=+2 tamper-1The performance metric we use is the standard Preci-sion/Recall/F measures, as computed by the conlleval program:http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt2ftp://ftp.cis.upenn.edu/pub/chunker3http://chasen.org/?taku/software/yamcha/226ing, features of the form w+2 = X are removed).Loc=i ?
all the lexical features with an index otherthan i are removed from the model.3.6 Results and DiscussionHighlights of the results are presented in Tables (1-3).
The numbers reported are F measures.TopN HEBGold HEBErr ENGALL 93.58 92.48 93.79N=0 78.32 76.27 90.10N=10 90.21 88.68 90.24N=50 91.78 90.85 91.22N=100 92.25 91.62 91.72N=500 93.60 92.23 93.12N=1000 93.56 92.41 93.30Table 1: Results of TopN Tampering.The results of the TopN tamperings show that forboth languages, most of the lexical features are irrel-evant for the classification ?
the numbers achievedby using all the lexical features (about 30,000 in He-brew and 75,000 in English) are very close to thoseobtained using only a few lexical features.
Thisfinding is very encouraging, and suggests that SVMbased chunking is robust to corpus variations.Another conclusion is that lexical features helpbalance the fact that PoS tags can be noisy: weknow both HEBErr and ENG include PoS tag-ging errors (about 8% in Hebrew and 4% in En-glish).
While in the case of ?perfect?
PoS tagging(HEBGold), a very small amount of lexical featuresis sufficient to reach the best F-result (500 out of30,264), in the presence of PoS errors, more thanthe top 1000 lexical features are needed to reach theresult obtained with all lexical features.More striking is the fact that in Hebrew, thetop 10 lexical features are responsible for an im-provement of 12.4 in F-score.
The words cov-ered by these 10 features are the following: Startof Sentence marker and comma, quote,?of/??
?, ?and/?
?, ?the/??
and ?in/?
?.This finding suggests that the Hebrew PoS tagsetmight not be informative enough for the chunkingtask, especially where punctuation 4 and preposi-tions are concerned.
The results in Table 2 give fur-ther support for this claim.4Unlike the WSJ PoS tagset in which most punctuations getunique tags, our tagset treat punctuation marks as one group.NoPOS HEBG HEBE NoPOS HEBG HEBEPrep 85.25 84.40 Pronoun 92.97 92.14Punct 88.90 87.66 Conjunction 92.31 91.67Adverb 92.02 90.72 Determiner 92.55 91.39Table 2: Results of Hebrew NoPOS Tampering.Other scores are ?
93.3(HEBG), ?
92.2(HEBE).When removing lexical features of a specificPoS, the most dramatic loss of F-score is reachedfor Prepositions and Punctuation marks, followedby Adverbs, and Conjunctions.
Strikingly, lexi-cal information for most open-class PoS (includingProper Names and Nouns) has very little impact onHebrew chunking performance.From this observation, one could conclude thatenriching a model based only on PoS with lexicalfeatures for only a few closed-class PoS (prepo-sitions and punctuation) could provide appropri-ate results even with a simpler learning method,one that cannot deal with a large number of fea-tures.
We tested this hypothesis by training theError-Driven Pruning (EDP) method of (Cardie andPierce, 1998) with an extended set of features.
EDPwith PoS features only produced an F-result of 76.3on HEBGold.
By adding lexical features only forprepositions {?
?
?
?
??
}, one conjunction {?}
andpunctuation, the F-score on HEBGold indeed jumpsto 85.4.
However, when applied on HEBErr, EDPfalls down again to 59.4.
This striking disparity, bycomparison, lets us appreciate the resilience of theSVM model to PoS tagging errors, and its gener-alization capability even with a reduced number oflexical features.Another implication of this data is that commasand quotation marks play a major role in deter-mining NP boundaries in Hebrew.
In Goldberget al (2006), we noted the Hebrew Treebank is notconsistent in its treatment of punctuation, and thuswe evaluated the chunker only after performing nor-malization of chunk boundaries for punctuations.We now hypothesize that, since commas and quo-tation marks play such an important role in the clas-sification, performing such normalization before thetraining stage might be beneficial.
Indeed results onthe normalized corpus show improvement of about1.0 in F score on both HEBErr and HEBGold.
A10-fold cross validation experiment on punctuationnormalized HEBErr resulted in an F-Score of 92.2,improving the results reported by (Goldberg et al,2272006) on the same setting (91.4).Loc=I HEBE ENG Loc6=I HEBE ENG-2 78.26 89.79 -2 91.62 93.87-1 76.96 90.90 -1 91.86 93.030 90.33 92.37 0 79.44 91.161 76.90 90.47 1 92.33 93.302 76.55 90.06 2 92.18 93.65Table 3: Results of Loc Tamperings.We now turn to analyzing the importance of con-text positions (Table 3).
For both languages, themost important lexical feature (by far) is at position0, that is, the word currently being classified.
ForEnglish, it is followed by positions 1 and -1, andthen positions 2 and -2.
For Hebrew, back contextseems to have more effect than front context.
InHebrew, all the positions positively contribute to thedecision, while in English removing w2/?2 slightlyimproves the results (note also that including onlyfeature w2/?2 performs worse than with no lexicalinformation in English).3.7 The Real Role of Lexical FeaturesModel tampering (i.e., removing features after thelearning stage) is not the same as learning withoutthese features.
This claim is verified empirically:training on the English corpus without the lexicalfeatures at position ?2 yields worse results than withthem (93.73 vs. 93.79) ?
while removing the w?2features via tampering on a model trained with w?2yields better results (93.87).
Similarly, for all cor-pora, training using only the top 1,000 features (asdefined in the Top1000 tampering) results in loss ofabout 2 in F-Score (ENG 92.02, HEBErr 90.30,HEBGold 91.67), while tampering Top1000 yieldsa result very close to the best obtained (93.56, 92.41or 93.3F).This observation leads us to an interesting conclu-sion about the real role of lexical features in SVMbased chunking: rare events (features) are used tomemorize hard examples.
Intuitively, by giving aheavy weight to rare events, the classifier learns spe-cific rules such as ?if the word at position -2 is X andthe PoS at position 2 is Y, then the current word isInside a noun-phrase?.
Most of these rules are acci-dental ?
there is no real relation between the partic-ular word-pos combination and the class of the cur-rent word, it just happens to be this way in the train-ing samples.
Marking the rare occurrences helps thelearner achieve better generalization on the other,more common cases, which are similar to the outlieron most features, except the ?irrelevant ones?.
Asthe events are rare, such rules usually have no effecton chunking accuracy: they simply never occur inthe test data.
This observation refines the commonconception that SVM chunking does not suffer fromirrelevant features: in chunking, SVM indeed gener-alizes well for the common cases but also over-fitsthe model on outliers.Model tampering helps us design a model in twoways: (1) it is a way to ?open the black box?
ob-tained when training an SVM and to analyze the re-spective importance of features.
In our case, thisanalysis allowed us to identify the importance ofpunctuation and prepositions and improve the modelby defining more focused features (improving over-all result by ?1.0 F-point).
(2) The analysis also ledus to the conclusion that ?feature selection?
is com-plex in the case of SVM ?
irrelevant features helpprevent over-generalization by forcing over-fittingon outliers.We have also confirmed that the model learned re-mains robust in the presence of noise in the PoS tagsand relies on only few lexical features.
This veri-fication is critical in the context of languages withfew computational resources, as we expect the sizeof corpora and the quality of taggers to keep laggingbehind that achieved in English.4 Anchored LearningWe pursue the observation of how SVM dealswith outliers by developing the Anchored Learningmethod.
The idea behind Anchored Learning is toadd a unique feature ai (an anchor) to each trainingsample (we add as many new features to the modelas there are training samples).
These new featuresmake our data linearly separable.
The SVM learnercan then use these anchors (which will never occuron the test data) to memorize the hard cases, de-creasing this burden from ?real?
features.We present two uses for Anchored Learning.
Thefirst is the identification of hard cases and corpus er-rors, and the second is a preliminary feature selec-tion approach for SVM to improve chunking accu-racy.4.1 Mining for Errors and Hard CasesFollowing the intuition that SVM gives more weightto anchor features of hard-to-classify cases, we can228actively look for such cases by training an SVMchunker on anchored data (as the anchored data isguaranteed to be linearly separable, we can set a veryhigh value to the C parameter, preventing any mis-classification), and then investigating either the an-chors whose weights5 are above some threshold t orthe top N heaviest anchors, and their correspondingcorpus locations.
These locations are those thatthe learner considers hard to classify.
They canbe either corpus errors, or genuinely hard cases.This method is similar to the corpus error detec-tion method presented by Nakagawa and Matsumoto(2002).
They constructed an SVM model for PoStagging, and considered Support Vectors with high?
values to be indicative of suspicious corpus loca-tions.
These locations can be either outliers, or cor-rectly labeled locations similar to an outlier.
Theythen looked for similar corpus locations with a dif-ferent label, to point out right-wrong pairs with highprecision.Using anchors improves their method in three as-pects: (1) without anchors, similar examples are of-ten indistinguishable to the SVM learner, and in casethey have conflicting labels both examples will begiven high weights.
That is, both the regular caseand the hard case will be considered as hard exam-ples.
Moreover, similar corpus errors might resultin only one support vector that cover all the group ofsimilar errors.
Anchors mitigate these effects, result-ing in better precision and recall.
(2) The more er-rors there are in the corpus, the less linearly separa-ble it is.
Un-anchored learning on erroneous corpuscan take unreasonable amount of time.
(3) Anchorsallow learning while removing some of the impor-tant features but still allow the process to convergein reasonable time.
This lets us analyze which casesbecome hard to learn if we don?t use certain features,or in other words: what problematic cases are solvedby specific features.The hard cases analysis achieved by anchoredlearning is different from the usual error analysiscarried out on observed classification errors.
Thetraditional methods give us intuitions about wherethe classifier fails to generalize, while the methodwe present here gives us intuition about what theclassifier considers hard to learn, based on thetraining examples alone.5As each anchor appear in only one support vector, we cantreat the vector?s ?
value as the anchor weightThe intuition that ?hard to learn?
examples aresuspect corpus errors is not new, and appears alsoin Abney et al (1999) , who consider the ?heaviest?samples in the final distribution of the AdaBoost al-gorithm to be the hardest to classify and thus likelycorpus errors.
While AdaBoost models are easy tointerpret, this is not the case with SVM.
Anchoredlearning allows us to extract the hard to learn casesfrom an SVM model.
Interestingly, while both Ad-aBoost and SVM are ?large margin?
based classi-fiers, there is less than 50% overlap in the hard casesfor the two methods (in terms of mistakes on the testdata, there were 234 mistakes shared by AdaBoostand SVM, 69 errors unique to SVM and 126 errorsunique to AdaBoost)6.
Analyzing the difference inwhat the two classifiers consider hard is interesting,and we will address it in future work.
In the currentwork, we note that for finding corpus errors the twomethods are complementary.Experiment 1 ?
Locating Hard CasesA linear SVM model (Mfull) was trained onthe training subset of the anchored, punctuation-normalized, HEBGold corpus, with the same fea-tures as in the previous experiments, and a C valueof 9,999.
Corpus locations corresponding to anchorswith weights >1 were inspected.
There were about120 such locations out of 4,500 sentences used in thetraining set.
Decreasing the threshold t would resultin more cases.
We analyzed these locations into 3categories: corpus errors, cases that challenge theSimpleNP definition, and cases where the chunkingdecision is genuinely difficult to make in the absenceof global syntactic context or world knowledge.Corpus Errors: The analysis revealed the fol-lowing corpus errors: we identified 29 hard casesrelated to conjunction and apposition (is the comma,colon or slash inside an NP or separating two distinctNPs).
14 of these hard cases were indeed mistakesin the corpus.
This was anticipated, as we distin-guished appositions and conjunctive commas usingheuristics, since the Treebank marking of conjunc-tions is somewhat inconsistent.In order to build the Chunk NP corpus, the syn-tactic trees of the Treebank were processed to derivechunks according to the SimpleNP definition.
Thehard cases analysis identified 18 instances where this6These numbers are for pairwise Linear SVM and AdaBoostclassifiers trained on the same features.229transformation results in erroneous chunks.
For ex-ample, null elements result in improper chunks, suchas chunks containing only adverbs or only adjec-tives.We also found 3 invalid sentences, 6 inconsisten-cies in the tagging of interrogatives with respect tochunk boundaries, as well as 34 other specific mis-takes.
Overall, more than half of the locations iden-tified by the anchors were corpus errors.
Looking forcases similar to the errors identified by anchors, wefound 99 more locations, 77 of which were errors.Refining the SimpleNP Definition: The hardcases analysis identified examples that challengethe SimpleNP definition proposed in Goldberget al (2006).
The most notable cases are:The ?et?
marker : ?et?
is a syntactic marker of defi-nite direct objects in Hebrew.
It was regarded as apart of SimpleNPs in their definition.
In some cases,this forces the resulting SimpleNP to be too inclu-sive:[????????
?????
???
?????
,??????
??][?et?
(the government, the parliament and the media)]Because in the Treebank the conjunction depends on?et?
as a single constituent, it is fully embedded inthe chunk.
Such a conjunction should not be consid-ered simple.The ??
preposition (?of?)
marks generalized posses-sion and was considered unambiguous and includedin SimpleNPs.
We found cases where ????
causesPP attachment ambiguity:[??????]
??
[?????]
?
[????
???
????
][president-cons house-cons the-law] for [discipline] of [thepolice] / The Police Disciplinary Court PresidentBecause 2 prepositions are involved in this NP, ????
(of) and ???
(for), the ????
part cannot be attachedunambiguously to its head (?court?).
It is unclearwhether the ???
preposition should be given specialtreatment to allow it to enter simple NPs in certaincontexts, or whether the inconsistent handling ofthe ????
that results from the ???
inter-position ispreferable.Complex determiners and quantifiers: In manycases, complex determiners in Hebrew are multi-word expressions that include nouns.
The inclusionof such determiners inside the SimpleNPs is notconsistent.Genuinely hard cases were also identified.These include prepositions, conjunctions and multi-word idioms (most of them are adjectives and prepo-sitions which are made up of nouns and determin-ers, e.g., as the word unanimously is expressed inHebrew as the multi-word expression ?one mouth?
).Also, some adverbials and adjectives are impossibleto distinguish using only local context.The anchors analysis helped us improve thechunking method on two accounts: (1) it identifiedcorpus errors with high precision; (2) it made us fo-cus on hard cases that challenge the linguistic defi-nition of chunks we have adopted.
Following thesefindings, we intend to refine the Hebrew SimpleNPdefinition, and create a new version of the Hebrewchunking corpus.Experiment 2 ?
determining the role ofcontextual lexical featuresThe intent of this experiment is to understand therole of the contextual lexical features (wi, i 6= 0).This is done by training 2 additional anchored lin-ear SVM models, Mno?cont and Mnear.
These arethe same as Mfull except for the lexical featuresused during training.
Mno?cont uses only w0, whileMnear uses w0,w?1,w+1.Anchors are again used to locate the hard exam-ples for each classifier, and the differences are ex-amined.
The examples that are hard for Mnear butnot for Mfull are those solved by w?2,w+2.
Sim-ilarly, the examples that are hard for Mno?cont butnot for Mnear are those solved by w?1,w+1.
Table 4indicates the number of hard cases identified by theanchor method for each model.
One way to inter-pret these figures, is that the introduction of featuresw?1,+1 solves 5 times more hard cases than w?2,+2.Model Number of hardcases (t = 1)Hard cases forclassifier B-IMfull 120 2Mnear 320 (+ 200) 12Mno?cont 1360 (+ 1040) 164Table 4: Number of hard cases per model type.Qualitative analysis of the hard cases solved bythe contextual lexical features shows that they con-tribute mostly to the identification of chunk bound-aries in cases of conjunction, apposition, attachmentof adverbs and adjectives, and some multi-word ex-pressions.The number of hard cases specific to the B-I clas-sifier indicates how the features contribute to the de-cision of splitting or continuing back-to-back NPs.Back-to-back NPs amount to 6% of the NPs inHEBGold and 8% of the NPs in ENG.
However,230while in English most of these cases are easily re-solved, Hebrew phenomena such as null-equativesand free word order make them harder.
To quantifythe difference: 79% of the first words of the secondNP in English belong to one of the closed classesPOS, DT, WDT, PRP, WP ?
categories which mostlycannot appear in the middle of base NPs.
In con-trast, in Hebrew, 59% are Nouns, Numbers or ProperNames.
Moreover, in English the ratio of unique firstwords to number of adjacent NPs is 0.068, while inHebrew it is 0.47.
That is, in Hebrew, almost everysecond such NP starts with a different word.These figures explain why surrounding lexical in-formation is needed by the learner in order to clas-sify such cases.
They also suggest that this learningis mostly superficial, that is, the learner just mem-orizes some examples, but these will not generalizewell on test data.
Indeed, the most common class oferrors reported in Goldberg et al , 2006 are of thesplit/merge type.
These are followed by conjunctionrelated errors, which suffer from the same problem.Morphological features of smixut and agreement canhelp to some extent, but this is still a limited solu-tion.
It seems that deciding the [NP][NP] case isbeyond the capabilities of chunking with local con-text features alone, and more global features shouldbe sought.4.2 Facilitating Better LearningThis section presents preliminary results using An-chored Learning for better NP chunking.
We presenta setting (English Base NP chunking) in whichselected features coupled together with anchoredlearning show an improvement over previous results.Section 3.6 hinted that SVM based chunkingmight be hurt by using too many lexical features.Specifically, the features w?2,w+2 were shown tocause the chunker to overfit in English chunking.Learning without these features, however, yieldslower results.
This can be overcome by introduc-ing anchors as a substitute.
Anchors play the samerole as rare features when learning, while loweringthe chance of misleading the classifier on test data.The results of the experiment using 5-fold crossvalidation on ENG indicate that the F-score im-proves on average from 93.95 to 94.10 when usinganchors instead of w?2 (+0.15), while just ignoringthe w?2 features drops the F-score by 0.10.
The im-provement is minor but consistent.
Its implicationis that anchors can substitute for ?irrelevant?
lexicalfeatures for better learning results.
In future work,we will experiment with better informed sets of lex-ical features mixed with anchors.5 ConclusionWe have introduced two novel methods to under-stand the inner structure of SVM-learned models.We have applied these techniques to Hebrew NPchunking, and demonstrated that the learned modelis robust in the presence of noise in the PoS tags, andrelies on only a few lexical features.
We have iden-tified corpus errors, better understood the nature ofthe task in Hebrew ?
and compared it quantitativelyto the task in English.The methods provide general insight in the waySVM classification works for chunking.ReferencesS.
Abney, R. Schapire, and Y.
Singer.
1999.
Boostingapplied to tagging and PP attachment.
EMNLP-1999.M.
Adler and M. Elhadad.
2006.
An unsupervisedmorpheme-based hmm for hebrew morphological dis-ambiguation.
In COLING/ACL2006.C.
Cardie and D. Pierce.
1998.
Error-driven pruning oftreebank grammars for base noun phrase identification.In ACL-1998.Y.
Goldberg, M. Adler, and M. Elhadad.
2006.
Nounphrase chunking in hebrew: Influence of lexical andmorphological features.
In COLING/ACL2006.T.
Kudo and Y. Matsumoto.
2000.
Use of support vectorlearning for chunk identification.
In CoNLL-2000.T.
Kudo and Y. Matsumoto.
2003.
Fast methods forkernel-based text analysis.
In ACL-2003.M.
Marcus and L. Ramshaw.
1995.
Text Chunking Us-ing Transformation-Based Learning.
In Proc.
of the3rd ACL Workshop on Very Large Corpora.T.
Nakagawa and Y. Matsumoto.
2002.
Detecting er-rors in corpora using support vector machines.
InCOLING-2002.Erik F. Tjong Kim Sang and S. Buchholz.
2000.
Intro-duction to the conll-2000 shared task: chunking.
InCoNLL-2000.K.
Sima?an, A. Itai, Y.
Winter, A. Altman, and N. Nativ.2001.
Building a tree-bank of modern hebrew text.Traitement Automatique des Langues, 42(2).V.
Vapnik.
1995.
The nature of statistical learning the-ory.
Springer-Verlag New York, Inc.231
