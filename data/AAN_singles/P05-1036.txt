Proceedings of the 43rd Annual Meeting of the ACL, pages 290?297,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSupervised and Unsupervised Learning for Sentence CompressionJenine Turner and Eugene CharniakDepartment of Computer ScienceBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{jenine|ec}@cs.brown.eduAbstractIn Statistics-Based Summarization - StepOne: Sentence Compression, Knight andMarcu (Knight and Marcu, 2000) (K&M)present a noisy-channel model for sen-tence compression.
The main difficultyin using this method is the lack of data;Knight and Marcu use a corpus of 1035training sentences.
More data is not easilyavailable, so in addition to improving theoriginal K&M noisy-channel model, wecreate unsupervised and semi-supervisedmodels of the task.
Finally, we point outproblems with modeling the task in thisway.
They suggest areas for future re-search.1 IntroductionSummarization in general, and sentence compres-sion in particular, are popular topics.
Knight andMarcu (henceforth K&M) introduce the task ofstatistical sentence compression in Statistics-BasedSummarization - Step One: Sentence Compression(Knight and Marcu, 2000).
The appeal of this prob-lem is that it produces summarizations on a smallscale.
It simplifies general compression problems,such as text-to-abstract conversion, by eliminatingthe need for coherency between sentences.
Themodel is further simplified by being constrainedto word deletion: no rearranging of words takesplace.
Others have performed the sentence compres-sion task using syntactic approaches to this problem(Mani et al, 1999) (Zajic et al, 2004), but we fo-cus exclusively on the K&M formulation.
Thoughthe problem is simpler, it is still pertinent to cur-rent needs; generation of captions for television andaudio scanning services for the blind (Grefenstette,1998), as well as compressing chosen sentences forheadline generation (Angheluta et al, 2004) are ex-amples of uses for sentence compression.
In addi-tion to simplifying the task, K&M?s noisy-channelformulation is also appealing.In the following sections, we discuss the K&Mnoisy-channel model.
We then present our cleanedup, and slightly improved noisy-channel model.
Wealso develop unsupervised and semi-supervised (ourterm for a combination of supervised and unsuper-vised) methods of sentence compression with inspi-ration from the K&M model, and create additionalconstraints to improve the compressions.
We con-clude with the problems inherent in both models.2 The Noisy-Channel Model2.1 The K&M ModelThe K&M probabilistic model, adapted from ma-chine translation to this task, is the noisy-channelmodel.
In machine translation, one imagines that astring was originally in English, but that someoneadds some noise to make it a foreign string.
Analo-gously, in the sentence compression model, the shortstring is the original sentence and someone addsnoise, resulting in the longer sentence.
Using thisframework, the end goal is, given a long sentencel, to determine the short sentence s that maximizes290P (s | l).
By Bayes Rule,P (s | l) = P (l | s)P (s)P (l) (1)The probability of the long sentence, P (l) can be ig-nored when finding the maximum, because the longsentence is the same in every case.P (s) is the source model: the probability that sis the original sentence.
P (l | s) is the channelmodel: the probability the long sentence is the ex-panded version of the short.
This framework in-dependently models the grammaticality of s (withP (s)) and whether s is a good compression of l(P (l | s)).The K&M model uses parse trees for the sen-tences.
These allow it to better determine the proba-bility of the short sentence and to obtain alignmentsfrom the training data.
In the K&M model, thesentence probability is determined by combining aprobabilistic context free grammar (PCFG) with aword-bigram score.
The joint rules used to create thecompressions are generated by aligning the nodes ofthe short and long trees in the training data to deter-mine expansion probabilities (P (l | s)).Recall that the channel model tries to find theprobability of the long string with respect to theshort string.
It obtains these probabilities by align-ing nodes in the parsed parallel training corpus, andcounting the nodes that align as ?joint events.?
Forexample, there might be S ?
NP VP PP in the longsentence and S ?
NP VP in the short sentence; wecount this as one joint event.
Non-compressions,where the long version is the same as the short, arealso counted.
The expansion probability, as used inthe channel model, is given byPexpand(l | s) =count(joint(l, s))count(s) (2)where count(joint(l, s)) is the count of alignmentsof the long rule and the short.
Many compressionsdo not align exactly.
Sometimes the parses do notmatch, and sometimes there are deletions that are toocomplex to be modeled in this way.
In these casessentence pairs, or sections of them, are ignored.The K&M model creates a packed parse forest ofall possible compressions that are grammatical withrespect to the Penn Treebank (Marcus et al, 1993).Any compression given a zero expansion probabilityaccording to the training data is instead assigned avery small probability.
A tree extractor (Langkilde,2000) collects the short sentences with the highestscore for P (s | l).2.2 Our Noisy-Channel ModelOur starting implementation is intended to followthe K&M model fairly closely.
We use the same1067 pairs of sentences from the Ziff-Davis cor-pus, with 32 used as testing and the rest as train-ing.
The main difference between their model andours is that instead of using the rather ad-hoc K&Mlanguage model, we substitute the syntax-based lan-guage model described in (Charniak, 2001).We slightly modify the channel model equation tobe P (l | s) = Pexpand(l | s)Pdeleted, where Pdeletedis the probability of adding the deleted subtrees backinto s to get l. We determine this probability alsousing the Charniak language model.We require an extra parameter to encourage com-pression.
We create a development corpus of 25 sen-tences from the training data in order to adjust thisparameter.
That we require a parameter to encouragecompression is odd as K&M required a parameter todiscourage compression, but we address this point inthe penultimate section.Another difference is that we only generate shortversions for which we have rules.
If we have neverbefore seen the long version, we leave it alone, andin the rare case when we never see the long versionas an expansion of itself, we allow only the shortversion.
We do not use a packed tree structure, be-cause we make far fewer sentences.
Additionally,as we are traversing the list of rules to compress thesentences, we keep the list capped at the 100 com-pressions with the highest Pexpand(l | s).
We even-tually truncate the list to the best 25, still based uponPexpand(l | s).2.3 Special RulesOne difficulty in the use of training data is that somany compressions cannot be modeled by our sim-ple method.
The rules it does model, immediateconstituent deletion, as in taking out the ADVP , ofS ?
ADVP , NP VP ., are certainly common, butmany good deletions are more structurally compli-cated.
One particular type of rule, such as NP(1) ?291NP(2) CC NP(3), where the parent has at least onechild with the same label as itself, and the resultingcompression is one of the matching children, suchas, here, NP(2).
There are several hundred rules ofthis type, and it is very simple to incorporate into ourmodel.There are other structures that may be commonenough to merit adding, but we limit this experimentto the original rules and our new ?special rules.
?3 Unsupervised CompressionOne of the biggest problems with this model of sen-tence compression is the lack of appropriate train-ing data.
Typically, abstracts do not seem to con-tain short sentences matching long ones elsewherein a paper, and we would prefer a much larger cor-pus.
Despite this lack of training data, very goodresults were obtained both by the K&M model andby our variant.
We create a way to compress sen-tences without parallel training data, while stickingas closely to the K&M model as possible.The source model stays the same, and we stillpay a probability cost in the channel model for ev-ery subtree deleted.
However, the way we determinePexpand(l | s) changes because we no longer have aparallel text.
We create joint rules using only the firstsection (0.mrg) of the Penn Treebank.
We count allprobabilistic context free grammar (PCFG) expan-sions, and then match up similar rules as unsuper-vised joint events.We change Equation 2 to calculate Pexpand(s | l)without parallel data.
First, let us define svo (shorterversion of) to be: r1 svo r2 iff the righthand side ofr1 is a subsequence of the righthand side of r2.
ThendefinePexpand(l | s) =count(l)?l?s.t.
s svo l?
count(l?
)(3)This is best illustrated by a toy example.
Considera corpus with just 7 rules: 3 instances of NP ?
DTJJ NN and 4 instances of NP ?
DT NN.P(NP ?
DT JJ NN | NP ?
DT JJ NN) = 1.
Todetermine this, you divide the count of NP ?
DT JJNN = 3 by all the possible long versions of NP ?DT JJ NN = 3.P(NP ?
DT JJ NN | NP ?
DT NN) = 3/7.
Thecount of NP ?
DT JJ NN = 3, and the possible longversions of NP ?
DT NN are itself (with count of 3)and NP ?
DT JJ NN (with count of 4), yielding asum of 7.Finally, P(NP ?
DT NN | NP ?
DT NN) = 4/7.The count of NP ?
DT NN = 4, and since the short(NP ?
DT NN) is the same as above, the count ofthe possible long versions is again 7.In this way, we approximate Pexpand(l | s) with-out parallel data.Since some of these ?training?
pairs are likelyto be fairly poor compressions, due to the artifi-ciality of the construction, we restrict generation ofshort sentences to not allow deletion of the headof any subtree.
None of the special rules are ap-plied.
Other than the above changes, the unsuper-vised model matches our supervised version.
As willbe shown, this rule is not constraining enough andallows some poor compressions, but it is remarkablethat any sort of compression can be achieved with-out training data.
Later, we will describe additionalconstraints that help even more.4 Semi-Supervised CompressionBecause the supervised version tends to do quitewell, and its main problem is that the model tendsto pick longer compressions than a human would,it seems reasonable to incorporate the unsupervisedversion into our supervised model, in the hope ofgetting more rules to use.
In generating new shortsentences, if we have compression probabilities inthe supervised version, we use those, including thespecial rules.
The only time we use an unsupervisedcompression probability is when there is no super-vised version of the unsupervised rule.5 Additional ConstraintsEven with the unsupervised constraint from section3, the fact that we have artificially created our jointrules gives us some fairly ungrammatical compres-sions.
Adding extra constraints improves our unsu-pervised compressions, and gives us better perfor-mance on the supervised version as well.
We use aprogram to label syntactic arguments with the rolesthey are playing (Blaheta and Charniak, 2000), andthe rules for complement/adjunct distinction givenby (Collins, 1997) to never allow deletion of thecomplement.
Since many nodes that should not292be deleted are not labeled with their syntactic role,we add another constraint that disallows deletion ofNPs.6 EvaluationAs with Knight and Marcu?s (2000) original work,we use the same 32 sentence pairs as our Test Cor-pus, leaving us with 1035 training pairs.
After ad-justing the supervised weighting parameter, we foldthe development set back into the training data.We presented four judges with nine compressedversions of each of the 32 long sentences: A human-generated short version, the K&M version, our firstsupervised version, our supervised version with ourspecial rules, our supervised version with specialrules and additional constraints, our unsupervisedversion, our supervised version with additional con-straints, our semi-supervised version, and our semi-supervised version with additional constraints.
Thejudges were asked to rate the sentences in two ways:the grammaticality of the short sentences on a scalefrom 1 to 5, and the importance of the short sen-tence, or how well the compressed version retainedthe important words from the original, also on ascale from 1 to 5.
The short sentences were ran-domly shuffled across test cases.The results in Table 1 show compression rates,as well as average grammar and importance scoresacross judges.There are two main ideas to take away from theseresults.
First, we can get good compressions withoutpaired training data.
Second, we achieved a goodboost by adding our additional constraints in two ofthe three versions.Note that importance is a somewhat arbitrary dis-tinction, since according to our judges, all of thecomputer-generated versions do as well in impor-tance as the human-generated versions.6.1 Examples of ResultsIn Figure 1, we give four examples of most compres-sion techniques in order to show the range of perfor-mance that each technique spans.
In the first two ex-amples, we give only the versions with constraints,because there is little or no difference between theversions with and without constraints.Example 1 shows the additional compression ob-tained by using our special rules.
Figure 2 showsthe parse trees of the original pair of short and longversions.
The relevant expansion is NP ?
NP1 ,PP in the long version and simply NP1 in the shortversion.
The supervised version that includes thespecial rules learned this particular common specialjoint rule from the training data and could apply itto the example case.
This supervised version com-presses better than either version of the supervisednoisy-channel model that lacks these rules.
The un-supervised version does not compress at all, whereasthe semi-supervised version is identical with the bet-ter supervised version.Example 2 shows how unsupervised and semi-supervised techniques can be used to improve com-pression.
Although the final length of the sentencesis roughly the same, the unsupervised and semi-supervised versions are able to take the action ofdeleting the parenthetical.
Deleting parentheses wasnever seen in the training data, so it would be ex-tremely unlikely to occur in this case.
The unsuper-vised version, on the other hand, sees both PRN ?lrb NP rrb and PRN ?
NP in its training data, andthe semi-supervised version capitalizes on this par-ticular unsupervised rule.Example 3 shows an instance of our initial super-vised versions performing far worse than the K&Mmodel.
The reason is that currently our supervisedmodel only generates compressions that it has seenbefore, unlike the K&M model, which generates allpossible compressions.
S ?
S , NP VP .
never occursin the training data, and so a good compression doesnot exist.
The unsupervised and semi-supervisedversions do better in this case, and the supervisedversion with the added constraints does even better.Example 4 gives an example of the K&M modelbeing outperformed by all of our other models.7 Problems with Noisy Channel Models ofSentence CompressionTo this point our presentation has been rather nor-mal; we draw inspiration from a previous paper, andwork at improving on it in various ways.
We nowdeviate from the usual by claiming that while theK&M model works very well, there is a technicalproblem with formulating the task in this way.We start by making our noisy channel notation a293original: Many debugging features, including user-defined break points andvariable-watching and message-watching windows, have been added.human: Many debugging features have been added.K&M: Many debugging features, including user-defined points andvariable-watching and message-watching windows, have been added.supervised: Many features, including user-defined break points and variable-watchingand windows, have been added.super (+ extra rules, constraints): Many debugging features have been added.unsuper (+ constraints): Many debugging features, including user-defined breakpoints and variable-watching and message-watching windows, have been added.semi-supervised (+ constraints): Many debugging features have been added.original: Also, Trackstar supports only the critical path method (CPM) of projectscheduling.human: Trackstar supports the critical path method of project scheduling.K&M: Trackstar supports only the critical path method (CPM) of scheduling.supervised: Trackstar supports only the critical path method (CPM) of scheduling.super (+ extra rules, constraints): Trackstar supports only the critical path method (CPM) of scheduling.unsuper (+ constraints): Trackstar supports only the critical path method of project scheduling.semi-supervised (+ constraints): Trackstar supports only the critical path method of project scheduling.original: The faster transfer rate is made possible by an MTI-proprietary databuffering algorithm that off-loads lock-manager functions from the Q-bushost, Raimondi said.human: The algorithm off-loads lock-manager functions from the Q-bus host.K&M: The faster rate is made possible by a MTI-proprietary data buffering algorithmthat off-loads lock-manager functions from the Q-bus host, Raimondi said.supervised: Raimondi said.super (+ extra rules): Raimondi said.super (+ extra rules, constraints): The faster transfer rate is made possible by an MTI-proprietary data bufferingalgorithm, Raimondi said.unsuper (+ constraints): The faster transfer rate is made possible, Raimondi said.semi-supervised (+ constraints): The faster transfer rate is made possible, Raimondi said.original: The SAS screen is divided into three sections: one for writing programs, one forthe system?s response as it executes the program, and a third for output tablesand charts.human: The SAS screen is divided into three sections.K&M: The screen is divided into onesuper (+ extra rules): SAS screen is divided into three sections: one for writing programs, and a thirdfor output tables and charts.super (+ extra rules, constraints): The SAS screen is divided into three sections.unsupervised: The screen is divided into sections: one for writing programs, one for the system?sresponse as it executes program, and third for output tables and charts.unsupervised (+ constraints): Screen is divided into three sections: one for writing programs, one for thesystem?s response as it executes program, and a third for output tables and charts.semi-supervised: The SAS screen is divided into three sections: one for writing programs, one forthe system?s response as it executes the program, and a third for output tablesand charts.semi-super (+ constraints): The screen is divided into three sections: one for writing programs, one for thesystem?s response as it executes the program, and a third for output tablesand charts.Figure 1: Compression Examples294compression rate grammar importancehumans 53.33% 4.96 3.73K&M 70.37% 4.57 3.85supervised 79.85% 4.64 3.97supervised with extra rules 67.41% 4.57 3.66supervised with extra rules and constraints 68.44% 4.77 3.76unsupervised 79.11% 4.38 3.93unsupervised with constraints 77.93% 4.51 3.88semi-supervised 81.19% 4.79 4.18semi-supervised with constraints 79.56% 4.75 4.16Table 1: Experimental Resultsshort: (S (NP (JJ Many) (JJ debugging) (NNS features))(VP (VBP have) (VP (VBN been) (VP (VBN added))))(.
.
))long: (S (NP (NP (JJ Many) (JJ debugging) (NNS features))(, ,)(PP (VBG including) (NP (NP (JJ user-defined)(NN break)(NNS points)(CC and)(NN variable-watching))(CC and)(NP (JJ message-watching) (NNS windows))))(, ,))(VP (VBP have) (VP (VBN been) (VP (VBN added))))(.
.
))Figure 2: Joint Trees for special rulesbit more explicit:argmaxsp(s, L = s | l, L = l) = (4)argmaxsp(s, L = s)p(l, L = l | s, L = s)Here we have introduced explicit conditioningevents L = l and L = s to state that that the sen-tence in question is either the long version or theshort version.
We do this because in order to get theequation that K&M (and ourselves) start with, it isnecessary to assume the followingp(s, L = s) = p(s) (5)p(l, L = l | s, L = s) = p(l | s) (6)This means we assume that the probability of, say, sas a short (compressed) sentence is simply its prob-ability as a sentence.
This will be, in general, false.One would hope that real compressed sentences aremore probable as a member of the set of compressedsentences than they are as simply a member of allEnglish sentences.
However, neither K&M, nor we,have a large enough body of compressed and origi-nal sentences from which to create useful languagemodels, so we both make this simplifying assump-tion.
At this point it seems like a reasonable choicerootvpvbbuynpnnstoysrootvpvbbuynpjjlargennstoysFigure 3: A compression example ?
trees A and Brespectivelyto make.
In fact, it compromises the entire enter-prise.
To see this, however, we must descend intomore details.Let us consider a simplified version of a K&Mexample, but as reinterpreted for our model: howthe noisy channel model assigns a probability of thecompressed tree (A) in Figure 3 given the originaltree B.We compute the probabilities p(A) and p(B | A)as follows (Figure 4): We have divided the probabil-ities up according to whether they are contributed bythe source or channel models.
Those from the source295p(A) p(B | A)p(s ?
vp | H(s)) p(s ?
vp | s ?
vp)p(vp ?
vb np | H(vp)) p(vp ?
vb np | vp ?
vb np)p(np ?
nns | H(np)) p(np ?
jj nns | np ?
nns)p(vb ?
buy | H(vb)) p(vb ?
buy | vb ?
buy)p(nns ?
toys | H(nns)) p(nns ?
toys | nns ?
toys)p(jj ?
large | H(jj))Figure 4: Source and channel probabilities for com-pressing B into Ap(B) p(B | B)p(s ?
vp | H(s)) p(s ?
vp | s ?
vp)p(vp ?
vb np | H(vp)) p(vp ?
vb np | vp ?
vb np)p(np ?
jj nns | H(np)) p(np ?
jj nns | np ?
jj nns)p(vb ?
buy | H(vb)) p(vb ?
buy | vb ?
buy)p(nns ?
toys | H(nns)) p(nns ?
toys | nns ?
toys)p(jj ?
large | H(jj)) p(jj ?
large | jj ?
large)Figure 5: Source and channel probabilities for leav-ing B as Bmodel are conditioned on, e.g.
H(np) the history interms of the tree structure around the noun-phrase.In a pure PCFG this would only include the label ofthe node.
In our language model it includes muchmore, such as parent and grandparent heads.Again, following K&M, contrast this with theprobabilities assigned when the compressed tree isidentical to the original (Figure 5).Expressed like this it is somewhat daunting, butnotice that if all we want is to see which probabilityis higher (the compressed being the same as the orig-inal or truly compressed) then most of these termscancel, and we get the rule, prefer the truly com-pressed if and only if the following ratio is greaterthan one.p(np ?
nns | H(np))p(np ?
jj nns | H(np))p(np ?
jj nns | np ?
nns)p(np ?
jj nns | np ?
jj nns) (7)1p(jj ?
large | jj ?
large)In the numerator are the unmatched probabilitiesthat go into the compressed sentence noisy chan-nel probability, and in the denominator are those forwhen the sentence does not undergo any change.
Wecan make this even simpler by noting that becausetree-bank pre-terminals can only expand into wordsp(jj ?
large | jj ?
large) = 1.
Thus the last fractionin Equation 7 is equal to one and can be ignored.For a compression to occur, it needs to be less de-sirable to add an adjective in the channel model thanin the source model.
In fact, the opposite occurs.The likelihood of almost any constituent deletion isfar lower than the probability of the constituents allbeing left in.
This seems surprising, considering thatthe model we are using has had some success, butit makes intuitive sense.
There are far fewer com-pression alignments than total alignments: identicalparts of sentences are almost sure to align.
So themost probable short sentence should be very barelycompressed.
Thus we add a weighting factor tocompress our supervised version further.K&M also, in effect, weight shorter sentencesmore strongly than longer ones based upon their lan-guage model.
In their papers on sentence compres-sion, they give an example similar to our ?buy largetoys?
example.
The equation they get for the channelprobabilities in their example is similar to the chan-nel probabilities we give in Figures 3 and 4.
How-ever their source probabilities are different.
K&Mdid not have a true syntax-based language modelto use as we have.
Thus they divided the languagemodel into two parts.
Part one assigns probabilitiesto the grammar rules using a probabilistic context-free grammar, while part two assigns probabilitiesto the words using a bi-gram model.
As they ac-knowledge in (Knight and Marcu, 2002), the wordbigram probabilities are also included in the PCFGprobabilities.
So in their versions of Figures 3 and4 they have both p(toys | nns) (from the PCFG)and p(toys | buy) for the bigram probability.
Inthis model, the probabilities do not sum to one, be-cause they pay the probabilistic price for guessingthe word ?toys?
twice, based upon two different con-ditioning events.
Based upon this language model,they prefer shorter sentences.To reiterate this section?s argument: A noisychannel model is not by itself an appropriate modelfor sentence compression.
In fact, the most likelyshort sentence will, in general, be the same lengthas the long sentence.
We achieve compression byweighting to give shorter sentences more likelihood.In fact, what is really required is some model thattakes ?utility?
into account, using a utility model296in which shorter sentences are more useful.
Ourterm giving preference to shorter sentences can bethought of as a crude approximation to such a utility.However, this is clearly an area for future research.8 ConclusionWe have created a supervised version of the noisy-channel model with some improvements over theK&M model.
In particular, we learned that addingan additional rule type improved compression, andthat enforcing some deletion constraints improvesgrammaticality.
We also show that it is possible toperform an unsupervised version of the compressiontask, which performs remarkably well.
Our semi-supervised version, which we hoped would havegood compression rates and grammaticality, hadgood grammaticality but lower compression than de-sired.We would like to come up with a better utilityfunction than a simple weighting parameter for oursupervised version.
The unsupervised version prob-ably can also be further improved.
We achievedmuch success using syntactic labels to constraincompressions, and there are surely other constraintsthat can be added.However, more training data is always the easi-est cure to statistical problems.
If we can find muchlarger quantities of training data we could allow formuch richer rule paradigms that relate compressedto original sentences.
One example of a rule wewould like to automatically discover would allow usto compress all of our design goals or(NP (NP (DT all))(PP (IN of)(NP (PRP$ our) (NN design) (NNS goals))))}to all design goals or(NP (DT all) (NN design) (NNS goals))In the limit such rules blur the distinction betweencompression and paraphrase.9 AcknowledgementsThis work was supported by NSF grant IIS-0112435.
We would like to thank Kevin Knightand Daniel Marcu for their clarification and test sen-tences, and Mark Johnson for his comments.ReferencesRoxana Angheluta, Rudradeb Mitra, Xiuli Jing, andFrancine-Marie Moens.
2004.
K.U.Leuven summa-rization system at DUC 2004.
In Document Under-standing Conference.Don Blaheta and Eugene Charniak.
2000.
Assigningfunction tags to parsed text.
In The Proceedings of theNorth American Chapter of the Association for Com-putational Linguistics, pages 234?240.Eugene Charniak.
2001.
Immediate-head parsing forlanguage models.
In Proceedings of the 39th AnnualMeeting of the Association for Computational Linguis-tics.
The Association for Computational Linguistics.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In The Proceedings ofthe 35th Annual Meeting of the Association for Com-putational Linguistics, San Francisco.
Morgan Kauf-mann.Gregory Grefenstette.
1998.
Producing intelligent tele-graphic text reduction to provide an audio scanningservice for the blind.
In Working Notes of the AAAISpring Symposium on Intelligent Text Summarization,pages 111?118.Kevin Knight and Daniel Marcu.
2000.
Statistics-basedsummarization - step one: sentence compression.
InProceedings of the 17th National Conference on Arti-ficial Intelligence, pages 703?71.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: A probabilistic ap-proach to sentence compression.
In Artificial Intelli-gence, 139(1): 91-107.Irene Langkilde.
2000.
Forest-based statistical sentencegeneration.
In Proceedings of the 1st Annual Meetingof the North American Chapter of the Association forComputationl Linguistics.Inderjeet Mani, Barbara Gates, and Eric Bloedorn.
1999.Improving summaries by revising them.
In The Pro-ceedings of the 38th Annual Meeting of the Associa-tion for Computational Linguistics.
The Associationfor Computational Linguistics.Michell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.David Zajic, Bonnie Dorr, and Richard Schwartz.
2004.BBN/UMD at DUC 2004: Topiary.
In Document Un-derstanding Conference.297
