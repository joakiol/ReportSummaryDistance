Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 241?247, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsA Dataset of Syntactic-Ngrams over Timefrom a Very Large Corpus of English BooksYoav GoldbergBar Ilan University?yoav.goldberg@gmail.comJon OrwantGoogle Inc.orwant@google.comAbstractWe created a dataset of syntactic-ngrams(counted dependency-tree fragments) basedon a corpus of 3.5 million English books.
Thedataset includes over 10 billion distinct itemscovering a wide range of syntactic configura-tions.
It also includes temporal information,facilitating new kinds of research into lexicalsemantics over time.
This paper describes thedataset, the syntactic representation, and thekinds of information provided.1 IntroductionThe distributional hypothesis of Harris (1954) statesthat properties of words can be captured based ontheir contexts.
The consequences of this hypoth-esis have been leveraged to a great effect by theNLP community, resulting in algorithms for in-ferring syntactic as well as semantic properties ofwords (see e.g.
(Turney and Pantel, 2010; Baroniand Lenci, 2010) and the references therein).In this paper, we describe a very large datasetof syntactic-ngrams, that is, structures in which thecontexts of words are based on their respective po-sition in a syntactic parse tree, and not on their se-quential order in the sentence: the different words inthe ngram may be far apart from each other in thesentence, yet close to each other syntactically.
SeeFigure 1 for an example of a syntactic-ngram.The utility of syntactic contexts of words for con-structing vector-space models of word meanings iswell established (Lin, 1998; Lin and Pantel, 2001;Pado?
and Lapata, 2007; Baroni and Lenci, 2010).Syntactic relations are successfully used for mod-eling selectional preferences (Erk and Pado?, 2008;?Work performed while at Google.Erk et al 2010; Ritter et al 2010; Se?aghdha,2010), and dependency paths are also used to in-fer binary relations between words (Lin and Pantel,2001; Wu and Weld, 2010).
The use of syntactic-ngrams holds promise also for improving the accu-racy of core NLP tasks such as syntactic language-modeling (Shen et al 2008) and syntactic-parsing(Chen et al 2009; Sagae and Gordon, 2009; Co-hen et al 2012), though most successful attemptsto improve syntactic parsing by using counts fromlarge corpora are based on sequential rather thansyntactic information (Koo et al 2008; Bansal andKlein, 2011; Pitler, 2012), we believe this is be-cause large-scale datasets of syntactic counts are notreadily available.
Unfortunately, most work utiliz-ing counts from large textual corpora does not use astandardized corpora for constructing their models,making it very hard to reproduce results and chal-lenging to compare results across different studies.Our aim in this work is not to present new meth-ods or results, but rather to provide a new kind of alarge-scale (based on corpora about 100 times largerthan previous efforts) high-quality and standard re-source for researchers to build upon.
Instead of fo-cusing on a specific task, we aim to provide a flexi-ble resource that could be adapted to many possibletasks.Specifically, the contribution of this work is increating a dataset of syntactic-ngrams which is:?
Derived from a very large (345 billion words)corpus spanning a long time period.?
Covers a wide range of syntactic phenomenaand is adaptable to many use cases.?
Based on state-of-the-art syntactic processingin a modern syntactic representation.?
Broken down by year of occurrence, as well241Figure 1: A syntactic ngram appearing 112 times inthe extended-biarcs set, which include structures contain-ing three content words (see Section 4).
Grayed itemsare non-content words and are not included in the wordcount.
The dashed auxiliary ?have?
is a functional marker(see Section 3), appearing only in the extended-* sets.as some coarse-grained regional and genre dis-tinctions (British, American, Fiction).?
Freely available for non-commercial use.
1After describing the underlying syntactic represen-tation, we will present our definition of a syntactic-ngram, and detail the kinds of syntactic-ngrams wechose to include in the dataset.
Then, we present de-tails of the corpus and the syntactic processing weperformed.With respect to previous efforts, the dataset has thefollowing distinguishing characteristics:Temporal Dimension A unique aspect of ourdataset is the temporal dimension, allowing inspec-tion of how the contexts of different words varyover time.
For example, one could examine how themeaning of a word evolves over time by looking atthe contexts it appears in within different time peri-ods.
Figure 2 shows the cosine similarity betweenthe word ?rock?
and the words ?stone?
and ?jazz?from year 1930 to 2000, showing that rock acquireda new meaning around 1968.Large syntactic contexts Previous efforts of provid-ing syntactic counts from large scale corpora (Ba-roni and Lenci, 2010) focus on relations betweentwo content words.
Our dataset include structurescovering much larger tree fragments, some of themincluding 5 or more content words.
By includingsuch structures we hope to encourage research ex-ploring higher orders of interactions, for examplemodeling the relation between adjectives of two con-joined nouns, the interactions between subjects andobjects of verbs, or fine-grained selectional prefer-ences of verbs and nouns.1The dataset is made publicly available under the Cre-ative Commons Attribution-Non Commercial ShareAlike 3.0Unported License: http://creativecommons.org/licenses/by-nc-sa/3.0/legalcode.Figure 2: Word-similarity over time: The word ?rock?
startsto become similar to ?jazz?
around 1968.
The plot shows thecosine similarity between the immediate syntactic contexts ofof the word ?rock?
in each year, to the immediate syntactic con-texts of the words ?jazz?
(in red) and ?stone?
(in blue) aggre-gated over all years.A closely related effort to add syntactic anno-tation to the books corpus is described in Lin etal.
(2012).
That effort emphasize an interactivequery interface covering several languages, in whichthe underlying syntactic representations are linear-ngrams enriched with universal part-of-speech tags,as well as first order unlabeled dependencies.
Incontrast, our emphasis is not on an easy-to-use queryinterface but instead a useful and flexible resourcefor computational-minded researchers.
We focuson English and use finer-grained English-specificPOS-tags.
The syntactic analysis is done using amore accurate parser, and we provide counts over la-beled tree fragments, covering a diverse set of tree-fragments many of which include more than twocontent words.Counted Fragments instead of complete treesWhile some efforts provide complete parse treesfrom large corpora (Charniak, 2000; Baroni et al2009; Napoles et al 2012), we instead providecounted tree fragments.
We believe that our form ofaggregate information is of more immediate use thanthe raw parse trees.
While access to the parse treesmay allow for somewhat greater flexibility in thekinds of questions one could ask, it also comes witha very hefty price tag in terms of the required com-putational resources: while counting seems trivial,it is, in fact, quite demanding computationally whendone on such a scale, and requires a massive infras-tructure.
By lifting this burden of NLP researchers,we hope to free them to tackle interesting researchquestions.2422 Underlying Syntactic RepresentationWe assume the part-of-speech tagset of the PennTreebank (Marcus et al 1993).
The syntactic rep-resentation we work with is based on dependency-grammar.
Specifically, we use labeled dependencytrees following the ?basic?
variant of the Stanford-dependencies scheme (de Marneffe and Manning,2008b; de Marneffe and Manning, 2008a).Dependency grammar is a natural choice, as itemphasizes individual words and explicitly mod-els the connections between them.
Stanford de-pendencies are appealing because they model rela-tions between content words directly, without in-tervening functional markers (so in a constructionsuch as ?wanted to know?
there is a direct rela-tion (wanted, know) instead of two relation(wanted, to) and (to, know).
This facil-itates focusing on meaning-bearing content wordsand including the maximal amount of informationin an ngram.3 Syntactic-ngramsWe define a syntactic-ngram to be a rooted con-nected dependency tree over k words, which is asubtree of a dependency tree over an entire sentence.For each of the k words in the ngram, we provide in-formation about the word-form, its part-of-speech,and its dependency relation to its head.
The ngramalso indicates the relative ordering between the dif-ferent words (the order of the words in the syntactic-ngram is the same as the order in which the wordsappear in the underlying sentence) but not the dis-tance between them, nor an indication whether thereis a missing material between the nodes.
Examplesof syntactic-ngrams are provided in Figures 1 and 3.Content-words and Functional-markers We dis-tinguish between content-words which are mean-ing bearing elements and functional-markers, whichserve to add polarity, modality or definiteness in-formation to the meaning bearing elements, but donot carry semantic meaning of their own, such asthe auxiliary verb ?have?
in Figure 1.
Specifi-cally, we treat words with a dependency-label ofdet, poss, neg, aux, auxpass, ps, mark,complm and prt as functional-markers.
With theexception of poss, these are all closed-class cat-egories.
All other words except for prepositionsand conjunctions are treated as content-words.
Asyntactic-ngram of order n includes exactly n con-tent words.
It may optionally include all of thefunctional-markers that modify the content-words.Conjunctions and Prepositions Conjunctions andPrepositions receive a special treatment.
When a co-ordinating word (?and?, ?or?, ?but?)
appears as partof a conjunctive structure (e.g.
?X, Y, and Z?
), itis treated as a non-content word.
Instead, it is al-ways included in the syntactic-ngrams that includethe conjunctive relation it is a part of, allowing todifferentiate between the various kinds of conjunc-tions.
An example is seen in Figure 3d, in whichthe relation conj(efficient, effective)is enriched with the coordinating word ?or?.
Whena coordinating word does not explicitly take part ina conjunction relation (e.g.
?But, .
.
.
?)
it is treatedas a content word.When a preposition is part of a prepositional mod-ification (i.e.
in the middle of the pair (prep,pcomp) or (prep, pobj)), such as the word?of?
in Figures 1 and 3h and the word ?as?
in Figure3e, it is treated as a non-content word, and is alwaysincluded in a syntactic-ngram whenever the words itconnects are included.
In cases of ellipsis or othercases where there is no overt pobj or pcomp (?heis hard to deal with?)
the preposition is treated as acontent word.2Multiword Expressions Some multiword expres-sions are recognized by the parser.
Whenever a con-tent word in an ngram has modifiers with the mwerelation, they are included in the ngram.4 The Provided Ngram TypesWe aimed to include a diverse set of relations, withmaximal emphasis on relations between content-bearing words, while still retaining access to defi-2This treatment of prepositions and conjunction is similar tothe ?collapsed?
variant of Stanford Dependencies (de Marneffeand Manning, 2008a), in which preposition- and conjunction-words do not appear as nodes in the tree but are instead anno-tated on the dependency label between the content words theyconnect, e.g.
prep with(saw, telescope).
However,we chose to represent the preposition or conjunction as a nodein the tree rather than moving it to the dependency label as itretains the information about the location of the function wordwith respect to the other words in the structure, is consistentwith cases in which one of the content words is not present, anddoes not blow up the label-set size.243niteness, modality and polarity if they are desired.The dataset includes the following types of syntacticstructures:nodes (47M items) consist of a single content word,and capture the syntactic role of that word (as in Fig-ure 3a).
For example, we can learn that the pro-noun ?he?
is predominantly used as a subject, andthat ?help?
as a noun is over 4 times more likely toappear in object than in subject position.arcs (919M items) consist of two content words, andcapture direct dependency relations such as ?subjectof?, ?adverbial modifier of?
and so on (see Figure3c,3d for examples).
These correspond to ?depen-dency triplets?
as used in Lin (1998) and most otherwork on syntax-based semantic similarity.biarcs (1.78B items) consist of three content words(either a word and its two daughters, or a child-parent-grandparent chain) and capture relations suchas ?subject verb object?, ?a noun and two adjectivialmodifiers?, ?verb, object and adjectivial modifier ofthe object?
and many others.triarcs (1.87B items) consist of four content words(example in Figure 3f).
The locality of the depen-dency representation causes this set of three-arcsstructures to be large, sparse and noisy ?
many ofthe relations may appear random because some arcsare in many cases almost independent given the oth-ers.
However, some of the relations are known to beof interest, and we hope more of them will prove tobe of interest in the future.
Some of the interestingrelations include:- modifiers of the head noun of the subject or objectin an SVO construction: ((small,boy), ate, cookies),(boy, ate, (tasty, cookies)), and with abstraction: ad-jectives that a boy likes to eat: (boy, ate, (tasty, *) )- arguments of an embeded verb (said, (boy, ate,cookie) ), (said, ((small, boy), ate) )- modifiers of conjoined elements ( (small, boy)(young, girl) ) , ( (small, *) (young, *) )- relative clause constructions ( boy, (girl, with-cookies, saw) )quadarcs (187M items) consist of 5 content words(example in Figure 3h).
In contrast to the previousdatasets, this set includes only a subset of the pos-sible relations involving 5 content words.
We choseto focus on relations which are attested in the liter-ature (Pado?
and Lapata 2007; Appendix A), namelystructures consisting of two chains of length 2 with asingle head, e.g.
( (small, boy), ate, (tasty, cookie) ).extended-nodes, extended-arcs, extended-biarcs,extended-triarcs, extended-quadarcs (80M,1.08B, 1.62B, 1.71B, and 180M items) Like theabove, but the functional markers of each contentwords are included as well (see examples in Figures3b, 3e, 3g).
These structures retain informationregarding aspects such as modality, polarity anddefiniteness, distinguishing, e.g.
?his red car?
from?her red car?, ?will go?
from ?should go?
and ?abest time?
from ?the best time?.verbargs (130M items) This set of ngrams consistof verbs with all their immediate arguments, andcan be used to study interactions between modi-fiers of a verb, as well as subcategorization frames.These structures are also useful for syntactic lan-guage modeling, as all the daughters of a verb areguaranteed to be present.nounargs (275M items) This set of ngrams consistof nouns with all their immediate arguments.verbargs-unlex, nounargs-unlex (114M, 195Mitems) Like the above, but only the head word andthe top-1000 occurring words in the English-1Msubcorpus are lexicalized ?
other words are replacedwith a *W* symbol.
By abstracting away from non-frequent words, we include many of the larger syn-tactic configurations that will otherwise be prunedaway by our frequency threshold.
These could beuseful for inspecting fine-grained syntactic subcate-gorization frames.5 Corpora and Syntactic ProcessingThe dataset is based on the English Google Bookscorpus.
This is the same corpus used to derive theGoogle Books Ngrams, and is described in detail inMichel et al(2011).
The corpus consists of the textof 3,473,595 English books which were publishedbetween 1520 and 2008, with the majority of thecontent published after 1800.
We provide countsbased on the entire corpus, as well as on several sub-sets of it:English 1M Uniformly sampled 1 million books.Fiction Works of Fiction.American English Books published in the US.British English Books published in Britain.The sizes of the different corpora are detailed in Ta-ble 1.244Figure 3: Syntactic-ngram examples.
Non-content words aregrayed, functional markers appearing only in the extended-*collections are dashed.
(a) node (b) extended-node (c) arcs (d)arcs, including the coordinating word (e) extended-arcs, includ-ing a preposition (f) triarcs (g) extended-triarcs (h) quadarcs,including a preposition.Counts Each syntactic ngram in each of the sub-corpora is coupled with a corpus-level count as wellas counts from each individual year.
To keep theCorpus # Books # Pages # Sentences # TokensAll 3.5M 925.7M 17.6B 345.1B1M 1M 291.1M 5.1B 101.3BFiction 817K 231.3M 4.7B 86.1BAmerican 1.4M 387.6M 7.9B 146.2BBritish 423K 124.9M 2.4B 46.1BTable 1: Corpora sizes.data manageable, we employ a frequency thresholdof 10 on the corpus-level count.Data Processing We ignored pages with over 600white-spaces (which are indicative of OCR errors ornon-textual content), as well as sentences of over 60tokens.
Table 1 details the sizes of the various cor-pora.After OCR, sentence splitting and tokenization,the corpus went through several stages of syntacticprocessing: part-of-speech tagging, syntactic pars-ing, and syntactic-ngrams extraction.Part-of-speech tagging was performed using afirst order CRF tagger, which was trained on a unionof the Penn WSJ Corpus (Marcus et al 1993), theBrown corpus (Kucera and Francis, 1967) and theQuestions Treebank (Judge et al 2006).
In additionto the diverse training material, the tagger makes useof features based on word-clusters derived from tri-grams of the Books corpus.
These cluster-featuresmake the tagger more robust on the books domain.For further details regarding the tagger, see Lin et al(2012).Syntactic parsing was performed using a re-implementation of a beam-search shift-reduce de-pendency parser (Zhang and Clark, 2008) with abeam of size 8 and the feature-set described inZhang and Nivre (2011).
The parser was trainedon the same training data as the tagger after 4-wayjack-knifing so that the parser is trained on data withpredicted part-of-speech tags.
The parser providesstate-of-the-art syntactic annotations for English.33Evaluating the quality of syntactic annotation on such a var-ied dataset is a challenging task on its own right ?
the underly-ing corpus includes many different genres spanning differenttime periods, as well as varying levels of digitization and OCRquality.
It is extremely difficult to choose a representative sam-ple to manually annotate and evaluate on, and we believe nosingle number will do justice to describing the annotation qual-ity across the entire dataset.
On top of that, we then aggregatefragments and filter based on counts, further changing the datadistribution.
We feel that it is better not to provide any numbersthan to provide inaccurate, misleading or uninformative num-2456 ConclusionWe created a dataset of syntactic-ngrams based ona very large literary corpus.
The dataset containsover 10 billion unique items covering a wide rangeof syntactic structures, and includes a temporal di-mension.The dataset is available for download athttp://storage.googleapis.com/books/syntactic-ngrams/index.htmlAcknowledgmentsWe would like to thank the members of Google?sextended syntactic-parsing team (Ryan McDonald,Keith Hall, Slav Petrov, Dipanjan Das, Hao Zhang,Kuzman Ganchev, Terry Koo, Michael Ringgaardand, at the time, Joakim Nivre) for many discus-sions, support, and of course the creation and main-tenance of an extremely robust parsing infrastruc-ture.
We further thank Fernando Pereira for sup-porting the project, and Andrea Held and SupreetChinnan for their hard work in making this possible.Sebastian Pado?, Marco Baroni, Alessandro Lenci,Jonathan Berant and Dan Klein provided valuableinput that helped shape the final form of this re-source.ReferencesMohit Bansal and Dan Klein.
2011.
Web-scale featuresfor full-scale parsing.
In ACL, pages 693?702.Marco Baroni and Alessandro Lenci.
2010.
Distribu-tional memory: A general framework for corpus-basedsemantics.
Computational Linguistics, 36(4):673?721.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The wacky wide web: acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Eugene Charniak.
2000.
Bllip 1987-89 wsj corpus re-lease 1.
In Linguistic Data Consortium, Philadelphia.Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,and Kentaro Torisawa.
2009.
Improving depen-dency parsing with subtrees from auto-parsed data.
InEMNLP, pages 570?579.bers.
We therefore chose not to provide a numeric estimationof syntactic-annotation quality, but note that we used a state-of-the-art parser, and believe most of its output to be correct,although we do expect a fair share of annotation errors as well.Raphael Cohen, Yoav Goldberg, and Michael Elhadad.2012.
Domain adaptation of a dependency parser witha class-class selectional preference model.
In Pro-ceedings of ACL 2012 Student Research Workshop,pages 43?48, Jeju Island, Korea, July.
Association forComputational Linguistics.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008a.
Stanford dependencies manual.
Techni-cal report, Stanford University.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008b.
The stanford typed dependencies repre-sentation.
In Coling 2008: Proceedings of the work-shop on Cross-Framework and Cross-Domain ParserEvaluation, CrossParser ?08, pages 1?8, Stroudsburg,PA, USA.
Association for Computational Linguistics.Katrin Erk and Sebastian Pado?.
2008.
A structured vec-tor space model for word meaning in context.
In Pro-ceedings of EMNLP, Honolulu, HI.
To appear.Katrin Erk, Sebastian Pado?, and Ulrike Pado?.
2010.
Aflexible, corpus-driven model of regular and inverseselectional preferences.
Computational Linguistics,36(4):723?763.Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.John Judge, Aoife Cahill, and Josef van Genabith.
2006.Questionbank: Creating a corpus of parse-annotatedquestions.
In Proc.
of ACL, pages 497?504.
Associa-tion for Computational Linguistics.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Proc.of ACL, pages 595?603.Henry Kucera and W. Nelson Francis.
1967.
Compu-tational Analysis of Present-Day American English.Brown University Press.Dekang Lin and Patrick Pantel.
2001.
Dirt: discovery ofinference rules from text.
In KDD, pages 323?328.Yuri Lin, Jean-Baptiste Michel, Erez Aiden Lieberman,Jon Orwant, Will Brockman, and Slav Petrov.
2012.Syntactic annotations for the google books ngram cor-pus.
In ACL (System Demonstrations), pages 169?174.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Linguis-tics and 17th International Conference on Computa-tional Linguistics - Volume 2, ACL ?98, pages 768?774, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19:313?330.246Jean-Baptiste Michel, Yuan Kui Shen, Aviva PresserAiden, Adrian Veres, Matthew K. Gray, TheGoogle Books Team, Joseph P. Pickett, Dale Hoiberg,Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker,Martin A. Nowak, and Erez Lieberman Aiden.
2011.Quantitative analysis of culture using millions of digi-tized books.
Science, 331(6014):176?182.Courtney Napoles, Matthew Gormley, and Benjamin VanDurme.
2012.
Annotated gigaword.
In AKBC-WEKEX Workshop at NAACL 2012, June.Sebastian Pado?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2):161?199.Emily Pitler.
2012.
Attacking parsing bottlenecks withunlabeled data and relevant factorizations.
In ACL,pages 768?776.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A latentdirichlet alcation method for selectional preferences.In ACL, pages 424?434.Kenji Sagae and Andrew S. Gordon.
2009.
Clusteringwords by syntactic similarity improves dependencyparsing of predicate-argument structures.
In IWPT,pages 192?201.Diarmuid O?
Se?aghdha.
2010.
Latent variable models ofselectional preference.
In ACL, pages 435?444.Libin Shen, Jinxi Xu, and Ralph M. Weischedel.
2008.A new string-to-dependency machine translation algo-rithm with a target dependency language model.
InACL, pages 577?585.P.D.
Turney and P. Pantel.
2010.
From frequency tomeaning: Vector space models of semantics.
Journalof Artificial Intelligence Research, 37(1):141?188.Fei Wu and Daniel S. Weld.
2010.
Open informationextraction using wikipedia.
In ACL, pages 118?127.Yue Zhang and Stephen Clark.
2008.
A tale of twoparsers: Investigating and combining graph-based andtransition-based dependency parsing.
In Proc.
ofEMNLP, pages 562?571.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 188?193.247
