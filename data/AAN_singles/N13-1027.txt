Proceedings of NAACL-HLT 2013, pages 270?279,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsUnsupervised Learning Summarization Templates from Concise SummariesHoracio Saggion?Universitat Pompeu FabraDepartment of Information and Communication TechnologiesTALN GroupC/Tanger 122 - Campus de la Comunicacio?nBarcelona - 08018Spainhttp://www.dtic.upf.edu/?hsaggion/AbstractWe here present and compare two unsuper-vised approaches for inducing the main con-ceptual information in rather stereotypicalsummaries in two different languages.
Weevaluate the two approaches in two differ-ent information extraction settings: mono-lingual and cross-lingual information extrac-tion.
The extraction systems are trained onauto-annotated summaries (containing the in-duced concepts) and evaluated on human-annotated documents.
Extraction results arepromising, being close in performance tothose achieved when the system is trained onhuman-annotated summaries.1 IntroductionInformation Extraction (Piskorski and Yangarber,2013) and Automatic Text Summarization (Saggionand Poibeau, 2013) are two Natural Language Pro-cessing tasks which require domain and languageadaptation.
For over two decades (Riloff, 1993;Riloff, 1996) the natural language processing com-munity has been interested in automatic or semi-automatic methods which could be used to port sys-tems from one domain or task to another, aiming atreducing at least in part the cost associated with thecreation of human annotated datasets.
Automaticsystem adaptation can take different forms: if high?This work is partially supported by Ministerio de Econom?
?ay Competitividad, Secretar?
?a de Estado de Investigacio?n, De-sarrollo e Innovacio?n, Spain under project number TIN2012-38584-C06-03 and Advanced Research Fellowship RYC-2009-04291.
We thank Biljana Drndarevic?
for proofreading the paper.quality human annotated data is available, then rule-based or statistical systems can be trained on thisdata (Brill, 1994), reducing the efforts of writingrules and handcrafting dictionaries.
If high qualityhuman annotated data is unavailable, a large non-annotated corpus and a bootstrapping procedure canbe used to produce annotated data (Ciravegna andWilks, 2003; Yangarber, 2003).
Here, we concen-trate on developing and evaluating automatic proce-dures to learn the main concepts of a domain andat the same time auto-annotate texts so that they be-come available for training information extraction ortext summarization applications.
However, it wouldbe naive to think that in the current state of the art wewould be able to learn all knowledge from text au-tomatically (Poon and Domingos, 2010; Biemann,2005; Buitelaar and Magnini, 2005).
We thereforehere concentrate on learning template-like represen-tations from concise event summaries which shouldcontain the key information of an event.18 de julio de 1994DateOfAttack .
Un atentadocontra la sede de la Asociacio?n Mutual IsraelitaArgentinaTarget de Buenos AiresPlaceOfAttack causa lamuerte de 86NumberOfVictims personas.
(18th July 1994.
An attack against the headquartersof the Jewish Mutual Association in Buenos Aires, Ar-gentina, kills 86 people.
)Figure 1: Sample of Human Annotated Summary inSpanishAn example of the summaries we want to learnfrom is presented in Figure 1.
It is a summary inthe terrorist attack domain in Spanish.
It has been270manually annotated with concepts such as DateO-fAttack, Target, PlaceOfAttack, and NumberOfVic-tims, which are key in the domain.
Our task is todiscover from this kind of summary what the con-cepts are and how to recognise them automatically.As will be shown in this paper and unlike currentapproaches (Chambers and Jurafsky, 2011; Leung etal., 2011), the methods to be presented here do notrequire parsing or semantic dictionaries to work orspecification of the underlying number of conceptsin the domain to be learn.
The approach we takelearns concepts in the set of domain summaries, re-lying on noun phrase contextual information.
Theyare able to generate reasonable domain conceptual-izations from relatively small datasets and in differ-ent languages.The rest of the paper is structured as follows: InSection 2 we overview related work in the area ofconcept induction from text.
Next, in Section 3 wedescribe the dataset used and how we have processedit while in Section 4 we outline the two unsuper-vised learning algorithms we compare in this paperfor template induction from text.
Then, in Section 5,we describe the experiments on template inductionindicating how we have instantiated the algorithmsand in Section 6 we explain how we have extrinsi-cally evaluated the induction process.
In Section 7we discuss the obtained results and in Section 8 wesummarize our findings and close the paper.2 Related WorkA long standing issue in natural language process-ing is how to learn conceptualizations from text inautomatic or semi-automatic ways.
The availabil-ity of redundant data has been used, for example,to discover template-like representations (Barzilayand Lee, 2003) or sentence-level paraphrases whichcould be used for extraction or generation.
Vari-ous approaches to concept learning use clusteringtechniques.
(Leung et al 2011) apply various clus-tering procedures to learn a small number of slotsin three typical information extraction domains, us-ing manually annotated data and fixing the num-ber of concepts to be learnt.
(Li et al 2010) gen-erate templates and extraction patterns for specificentity types (actors, companies, etc.).
(Chambersand Jurafsky, 2011) learn the structure of MUC tem-plates from raw data in English, an approach thatneeds both full parsing and semantic interpretationusing WordNet (Fellbaum, 1998) in order to extractverb arguments and measure the similarity betweernverbs.
In (Saggion, 2012) an iterative learning pro-cedure is used to discover core domain conceptualinformation from short summaries in two languages.However, the obtained results were not assessed in areal information extraction scenario.
There are ap-proaches which do not need any human interven-tion or sophisticated text processing, but learn basedon redundancy of the input dataset and some wellgrounded linguistic intuitions (Banko and Etzioni,2008; Etzioni et al 2004).
Related to the work pre-sented here are approaches that aim at generatingshort stereotypical summaries (DeJong, 1982; Paiceand Jones, 1993; Ratnaparkhi, 2000; Saggion andLapalme, 2002; Konstas and Lapata, 2012).3 Dataset and Text Processing StepsFor the experiments reported here we rely on theCONCISUS corpus1 (Saggion and Szasz, 2012)which is distributed free of charge.
It is a corpusof Web summaries in Spanish and English in fourdifferent application domains: Aviation Accidents(32 English, 32 Spanish), Earthquakes (44 English,56 Spanish), Train Accidents (36 English, 43 Span-ish), and Terrorist Attacks (42 English, 53 Spanish).The dataset contains original and comparable sum-mary pairs, automatic translations of Spanish sum-maries into English, automatic translation of Englishsummaries into Spanish, and associated original fulldocuments in Spanish and English for two of the do-mains (Aviation Accidents and Earthquakes).
Thedataset comes with human annotations representingthe key information in each domain.
In Table 1we detail the concepts used in each of the domains.Note that not all concepts are represented in eachof the summaries.
Creation of such a dataset cantake up to 500 hours for a human annotator, con-sidering data collection, cleansing, and annotationproper.
Only one human annotator and one curatorwere responsible for the annotation process.1http://www.taln.upf.edu/pages/concisus/.271Aviation Accident Airline, Cause, DateOfAccident, Destination, FlightNumber, NumberOfVictims, Origin, Passengers, Place, Sur-vivors, Crew, TypeOfAccident, TypeOfAircraft, YearEarthquake City, Country, DateOfEarthquake, Depth, Duration, Epicentre, Fatalities, Homeless, Injured, Magnitude, Other-PlacesAffected, Province, Region, Survivors, TimeOfEarthquakeTerrorist Attack City, Country, DateOfAccident, Fatalities, Injured, Target, Perpetrator, Place, NumberOfVictims, TypeOfAttackTrain Accident Cause, DateOfAccident, Destination, NumberOfVictims, Origin, Passenger, Place, Survivors, TypeOfAccident,TypeOfTrainTable 1: Conceptual Information in Summaries3.1 Text ProcessingIn order to carry out experimentation we adopt theGATE infrastructure for document representationand annotation (Maynard et al 2002).
All doc-uments in the dataset are processed with availablenatural language processors to compute shallow lin-gustic information.
Documents in English are pro-cessed with the ANNIE system, a morphological an-alyzer, and a noun chunker, all three from GATE.The documents in Spanish are analyzed with Tree-Tagger (Schmid, 1995), a rule-base noun chunker,and an SVM-based named entity recognition andclassification system.4 Concept Induction AlgorithmsTwo algorithms are used to induce conceptual in-formation in a domain from a set of textual sum-maries.
The algorithms form concepts based on tar-get strings (or chunks) in the set of summaries us-ing token-level linguistic information.
The chunksare represented with different features which are ex-plained later in Section 5.1.
One algorithm we useis based on clustering, while the other is based oniterative learning.4.1 Clustering-based InductionThe procedure for learning conceptual informationby clustering is straithforward: the chunks in the setof summaries are represented as instances consider-ing both internal and sourrounding linguistic infor-mation.
These instances are the input to a clusteringprocedure which returns a list of clusters each con-taining a set of chunks.
We consider each cluster asa key concept in the set of domain summaries andthe chunks in each cluster as the concept extension.4.2 Iterative InductionWe use the iterative learning algorithm described in(Saggion, 2012) which learns from a set of sum-maries S, also annotated with target strings (e.g.chunks) and shallow linguistic information.
In a nut-shell the algorithm is as follows:(1) Choose a document D from the set of summaries Sand add it to a training set TRAIN.
Set REST toS ?
TRAIN.
(2) Choose an available target concept T from D, i.e.
atarget concept not tried before by the algorithm.
(3) Train a classifier on TRAIN to learn instances ofthe target concept using the available linguistic fea-tures; the classifier uses the linguistic informationprovided.
(4) Apply the classifier to REST (all summaries minusthose in TRAIN) to annotate all instances of the tar-get concept T .
(5) Select a document BEST in REST, where there is aninstance of the concept recognised with the highestprobability in the REST set.
(6) Remove BEST from REST and add BEST to thetraining set, remove all identified instances of Tfrom REST, and go to step 3.The algorithm is executed a number of times (seeSection 5.1 for parametrization of the algorithms)to learn all concepts in the set of summaries, andat each iteration a single concept is formed.
Thereare two circumstances when a concept being formedis discarded and their associated initial target con-cept removed from the learning process: one case iswhen there are not enough occurrences of the con-cept across a set of summaries; another case is whentoo many identical strings are proposed as instancesfor the concept in the set of summaries.
This latterrestriction is only valid if we consider sets of non-redundant documents, which is the case to which werestrict our experiments.4.3 Text ChunksGiven that the algorithms presented above try to in-duce a concept from the chunks in the summaries,272we are interested in assessing how the type of chunkinfluences the learning process.
Also, given that ourobjective is to test methods which learn with mini-mal human intervention, we are interested in inves-tigating differences between the use of manual andautomatic chunks.
We therefore use the followingchunk types in this work: gold chunks (gold) are thehuman produced annotations (as in Figure 1); namedentity chunks (ne) are named entities computed by anoff-the-shelf named entity recognizer; noun chunks(nc) are text chunks identified by rule-based off-the-shelf NP chunkers and finally, wiki chunks (wiki) arestrings of text in the summaries which happen to beWikipedia titles.In order to automatically compute these chunktypes, different levels of knowledge are needed.For example, NP chunks require syntactic infor-mation, while named entities and wiki chunks re-quire some external form of knowledge, such asprecompiled gazetteer lists or access to an ency-clop?dia or a semantic dictionary.
Named enti-ties and noun chunks are computed as described inSection 3, while wiki chunks are computed as fol-lows: string n-grams w1w2...wn are computed ineach summary and strings w1 w2 ... wn are checkedagainst the Wikipedia on-line encyclop?dia, if ahit occurs (i.e.
if for an English n-gram the pageen.wikipedia.org/wiki/w1... wn exists or for a Span-ish n-gram the page es.wikipedia.org/wiki/w1... wnexists), the n-gram is annotated in the summary as awiki chunk.
Wiki chunks are cached to speed up theautomatic annotation process.SpanishP R FTerrorist Attack 0.47 0.10 0.17Aviation Accident 0.52 0.08 0.14Earthquake 0.24 0.06 0.10Train Accident 0.59 0.15 0.24EnglishP R FTerrorist Attack 0.46 0.39 0.42Aviation Accident 0.40 0.27 0.32Earthquake 0.27 0.22 0.24Train Accident 0.57 0.27 0.36Table 2: Baseline Induction Performance4.4 Mapping the Induced Concepts ontoHuman ConceptsFor evaluation purposes, each induced concept ismapped onto one human concept applying the fol-lowing procedure: let HCi be the set of summaryoffsets where human concept i occurs, and let ICi bethe set of summary offsets where automatic concepti occurs, then the induced concept j is mapped ontoconcept k such that: k = argmaxi(|HCi ?
ICj |),where |X| is the size of set X .
That is, the in-duced concept is mapped onto the label it gives ita best match.
As an example, one induced conceptin the terrorist attack domain containing the follow-ing string instances: two bombs, car bomb, pair ofbombs, 10 coordinated shooting and bombing, twocar bombs, suicide bomb, the attack, guerrilla war-fare, the coca-growing regions, etc.
This inducedconcept is mapped onto the TypeOfAttack humanconcept in that domain.4.5 Baseline Concept InductionA baseline induction mechanism is designed forcomparison with the two learning procedures pro-posed here.
It is based on the mapping of named en-tity chunks onto concepts in a straightforward way:each named entity type is considered a different con-cept and therefore mapped onto human concepts asin Section 4.4.
For example, in the terrorist attackdomain, Organization named entity type is mappedby this procedure onto the human concept Target(i.e.
churches, government buildings, etc., are com-mon targets in terrorist attacks) while in the Avia-tion Accident domain the Organization named en-tity type is mapped onto TypeOfAircraft (i.e.
Boe-ing, Airbus, etc.
are names of organizations).5 Experimental Setting and Results of theInduction ProcessIn this section we detail the different parametersused by the algorithms and report the performanceof the induction process with different inputs.5.1 SettingsThe features used by the induction procedure are ex-tracted from the text tokens.
We extract the POS tag,root, and string of each token.
The clustering-basedalgorithm uses a standard Expectation Maximization273SpanishIterative ClusteringP R F P R FTerrorist Attack 0.25 0.59 0.35 0.59 0.59 0.59?Aviation Accident 0.50 0.62 0.55 0.66 0.66 0.66?Earthquake 0.34 0.51 0.41 0.56 0.53 0.55?Train Accident 0.41 0.69 0.52 0.58 0.58 0.58EnglishIterative ClusteringP R F P R FTerrorist Attack 0.23 0.39 0.29 0.50 0.50 0.50?Aviation Accident 0.57 0.68 0.62 0.79 0.79 0.79?Earthquake 0.26 0.53 0.34 0.39 0.39 0.39Train Accident 0.50 0.59 0.54 0.61 0.61 0.61?Table 3: Conceptual induction (Spanish and English) Using GoldChunks for Learningimplementation from the Weka machine learning li-brary (Witten and Frank, 1999).
We instruct the al-gorithm to decide on the number of clusters based onthe data, instead of setting the number of clusters byhand.
The instances to cluster are representations ofthe input chunks; these representations contain theinternal features of the chunks, as well as the infor-mation of 5 tokens to the left of the beginning ofthe chunk and 5 tokens to the right of the end of thechunk.
The transformation from GATE documentsinto arff Weka files and the mapping from Weka ontothe GATE documents, is carried out using specificprograms.
The classification algorithm used for theiterative learning process is an SVM classifier dis-tributed with the GATE system and tuned to per-form chunk learning using the same features as theclustering procedure (Li et al 2004).
This classifieroutputs a probability which we use for selecting thebest document at step (5) of the iterative procedure.The document selected to start the process is the onewith more target strings, and the target string chosenis the next available in textual order.
The iterativelearning procedure is set to stop when the numberof concepts induced reaches the average number ofchunks in the corpus.
Induced concepts not coveringat least 10% of the number of documents are dis-carded, as are concepts with strings repeated at least10% of the concept extension.5.2 Experiments and ResultsWe carry out a number of experiments per domainwhere we run the algorithms using as input the sum-maries annotated with a different chunk type eachtime.
After each experiment all concepts induced areTerrorist AttacksIterative ClusteringP R F P R Fnc 0.22 0.53 0.31?
0.15 0.51 0.23ne 0.27 0.14 0.18 0.12 0.42 0.18wiki 0.15 0.26 0.19 0.22 0.18 0.20all 0.25 0.53 0.34?
0.12 0.51 0.20Aviation AccidentsIterative ClusteringP R F P R Fnc 0.30 0.50 0.38?
0.21 0.51 0.30ne 0.84 0.07 0.14 0.57 0.07 0.13wiki 0.29 0.28 0.28?
0.27 0.17 0.21all 0.39 0.62 0.48?
0.16 0.31 0.21EarthquakesIterative ClusteringP R F P R Fnc 0.29 0.42 0.34?
0.14 0.42 0.21ne 0.20 0.19 0.20?
0.38 0.02 0.05wiki 0.16 0.16 0.16 0.24 0.11 0.15all 0.28 0.50 0.36?
0.12 0.46 0.19Train AccidentsIterative ClusteringP R F P R Fnc 0.36 0.66 0.47?
0.23 0.51 0.32ne 0.33 0.66 0.44?
0.65 0.12 0.20wiki 0.25 0.25 0.25 0.51 0.13 0.21all 0.33 0.62 0.44?
0.16 0.50 0.24Table 4: Comparison of conceptual induction in Spanishmapped onto the human concepts (see Section 4.4)producing auto-annotated summaries.
The auto-matic annotations are then compared with the goldannotations, and precision, recall, and f-score fig-ures are computed to observe the performance of thetwo algorithms, the baseline, and the effect of typeof chunk on the learning process.In Table 2 we report baseline performance on theentire dataset.
As can be appreciated by the obtainednumbers, directly mapping named entity types ontoconcepts does not provide a very good performance,especially for Spanish; we expected the learningprocedures to produce better results.
In Table 3 wepresent the results of inducing concepts from thegold chunks by the two algorithms.
In almost allcases, using gold chunks improves over the baselineprocedure, except for the Terrorist Attack domainin English, where the iterative learning procedureunderperforms the baseline.
In all tested domains,the clustering-based induction procedure has a verycompetitive performance.
A t-test is run to verifydifferences in performance between the two systemsin terms of f-score.
In all tested domains in Span-ish, except the Train Accident domain, there are sta-274Terrorist AttacksIterative ClusteringP R F P R Fnc 0.43 0.50 0.46?
0.23 0.42 0.30ne 0.28 0.44 0.34 0.42 0.29 0.34wiki 0.24 0.33 0.28?
0.15 0.25 0.19all 0.31 0.49 0.38?
0.09 0.39 0.15Aviation AccidentsIterative ClusteringP R F P R Fnc 0.48 0.31 0.38 0.33 0.34 0.34ne 0.53 0.38 0.44?
0.63 0.27 0.38wiki 0.31 0.44 0.36?
0.28 0.37 0.32all 0.50 0.67 0.58?
0.15 0.47 0.23EarthquakesIterative ClusteringP R F P R Fnc 0.29 0.48 0.36?
0.06 0.40 0.10ne 0.28 0.34 0.30 0.30 0.25 0.28wiki 0.21 0.30 0.25?
0.16 0.23 0.19all 0.31 0.44 0.37?
0.08 0.40 0.13Train AccidentsIterative ClusteringP R F P R Fnc 0.45 0.54 0.49?
0.32 0.50 0.39ne 0.47 0.29 0.36 0.58 0.27 0.36wiki 0.51 0.32 0.39?
0.30 0.29 0.29all 0.50 0.58 0.53?
0.16 0.49 0.24Table 5: Comparison of conceptual induction in EnglishSpanishP R FAviation Accident 0.83 0.60 0.70Earthquake 0.61 0.48 0.53Train Accident 0.77 0.54 0.64EnglishP R FAviation Accident 0.88 0.38 0.53Earthquake 0.86 0.56 0.68Train Accident 0.84 0.43 0.57Table 6: Cross-lingual Information Extraction.
System Trained withGold Summaries.tistically significant differences between the cluster-ing procedure and the iterative learning procedure(p = 0.01).
In all tested domains in English, exceptfor the Earthquake domain, there are statisticallysignificant differences between the performance ofclustering and iterative learning (p = 0.01).Now we turn to the results of both algorithmswhen automatic chunks are used, that is, when nohuman annotation is provided to the learners.
Re-sults are reported in Tables 4 (Spanish) and 5 (En-glish).
The results are presented by the chunk typeused during the learning procedure.
In additionto the chunk types specified above, we include atype all, which represents the use of all automat-Aviation AccidentsIterative ClusteringP R F P R Fgold 0.85 0.52 0.65?
0.84 0.41 0.55all 0.88 0.49 0.63?
0.87 0.19 0.32nc 0.87 0.46 0.60 0.88 0.46 0.60EarthquakesIterative ClusteringP R F P R Fgold 0.65 0.41 0.50?
0.66 0.31 0.43all 0.64 0.36 0.46 0.62 0.40 0.49nc 0.63 0.33 0.43 0.67 0.38 0.49Train AccidentsIterative ClusteringP R F P R Fgold 0.81 0.54 0.65 0.82 0.52 0.64all 0.81 0.52 0.64?
0.72 0.31 0.43nc 0.79 0.54 0.64?
0.79 0.42 0.55Table 7: Cross-lingual Information Extraction Results in SpanishTranslations.
System trained with auto-annotated summaries in Span-ish.Aviation AccidentsIterative ClusteringP R F P R Fgold 0.87 0.35 0.50 0.87 0.37 0.52all 0.87 0.37 0.52?
0.82 0.18 0.29nc 0.90 0.21 0.34?
0.90 0.17 0.29EarthquakesIterative ClusteringP R F P R Fgold 0.87 0.53 0.66?
0.87 0.36 0.51all 0.88 0.51 0.64?
0.87 0.30 0.45nc 0.88 0.51 0.65?
0.93 0.43 0.59Train AccidentsIterative ClusteringP R F P R Fgold 0.82 0.30 0.44 0.87 0.32 0.47all 0.84 0.39 0.53?
0.91 0.24 0.38nc 0.89 0.36 0.51?
0.46 0.25 0.32Table 8: Cross-lingual Information Extraction Results in EnglishTranslations.
System trained with auto-annotated summaries in English.ically computed chunks (i.e.
nc, ne, wiki).
Weobserve that, in general, when presented with au-tomatic chuks, the iterative learning procedure isable to induce concepts with a better f-score thanthe clustering-based algorithm.
A t-test is run toverify differences between the two induction pro-cedures within each chunk condition (differencesshown with a ?
in the tables).
In 11 out of 16 casesin Spanish and in 12 out of 16 cases in English,statistically significant differences are observed.
Inthree out of four domains the combination of au-tomatic chunks outperforms the use of individualchunk types.
Generally, named entity chunks andwiki chunks have the lowest performance.
This is275SpanishP R FAviation Accident 0.56 0.47 0.51Earthquake 0.64 0.41 0.50EnglishP R FAviation Accident 0.61 0.35 0.44Earthquake 0.78 0.41 0.54Table 9: Extraction from Full Documents.
System Trained on GoldSummaries.Aviation AccidentsIterative ClusteringP R F P R Fgold 0.55 0.37 0.44 0.54 0.31 0.39all 0.55 0.36 0.43?
0.69 0.17 0.27nc 0.45 0.22 0.30?
0.52 0.26 0.35EarthquakeIterative ClusteringP R F P R Fgold 0.62 0.31 0.41?
0.63 0.22 0.33all 0.61 0.26 0.37 0.63 0.31 0.41?nc 0.60 0.24 0.35 0.70 0.28 0.40?Table 10: Full-text Information Extraction Results in Spanish.
Sys-tem trained with auto-annotated summaries in Spanish.not an unexpected result since named entities, forexample, cover much fewer strings which may formpart of a concept extension.
Additionally, off-the-shelf entity recogizers only identify a limited num-ber of entity types.6 Information Extraction EvaluationFrameworkThe numbers above are interesting because they pro-vide intrinsic evaluation of the concept inductionprocedure, but they do not tell us much about theirusability.
Therefore, and in order to better assessthe value of the discovered concepts, we decided tocarry out two extrinsic evaluations using an informa-tion extraction task.
Once the conceps are inducedand, as a result, the summaries are auto-annotatedwith domain specific concepts, we decide to trainan off-the-shelf SVM token classification procedureand apply it to unseen human annotated documents.The SVM classifier uses the same linguistic infor-mation as the induction procedures: token level in-formation and a window size of 5 around each tokento be classified.Aviation AccidentsIterative ClusteringP R F P R Fgold 0.60 0.28 0.39 0.62 0.31 0.41?all 0.62 0.30 0.41?
0.54 0.14 0.23nc 0.53 0.15 0.23?
0.46 0.10 0.16EarthquakeIterative ClusteringP R F P R Fgold 0.70 0.35 0.47?
0.72 0.32 0.44all 0.74 0.37 0.49 0.70 0.22 0.34nc 0.73 0.36 0.48?
0.73 0.30 0.42Table 11: Full-text Information Extraction Results in English.
Sys-tem trained with auto-annotated summaries in English.6.1 Extraction from Automatic TranslationsThe first task we carry out is cross-lingual informa-tion extraction where the input documents are auto-matic translations of summaries in Spanish and En-glish2.
Note that the expriment is performed in threedomains for which such translations are manuallyannotated.
We first run an experiment to assess theextraction performance of the SVM when trained onhuman annotated data.
Results of the experimentare reported in Table 6 and they should be takenas an upperbound of the performance of a systemtrained on auto-annotated summaries.
We then trainthe SVM on the different auto-annotated datasets,but note that due to space restrictions, we here onlyreport the three most revealing experiments per lan-guage: concepts induced with gold chunks, nounchunks, and all automatic chunks.
Results are re-ported in Table 7 (Spanish) and in Table 8 (English).In most cases the SVM trained with auto-annotatedsummaries produced by the iterative learning proce-dure outperforms the clustering-based method withstatistically significant differences (?
shown in thetables) (p = 0.01).6.2 Extraction from Full DocumentsThe second and the last evaluation consists in the ap-plication of the SVM extraction system to full doc-uments.
In this case, the experiment can be run onlyin two domains for which full documents have beenprovided and manually annotated.
We first test theperformance of the system when trained on humanannotated summaries and present the results in Ta-ble 9.
Results of the experiments when the systemis trained on auto-annotated datasets are shown in2The translations were produced by Google translator.276Tables 10 (Spanish) and 11 (English).
Results arelower than when training on clean human annotatedsummaries.
It is unclear which approach is morecompetitive when training with auto-annotated sum-maries.
What is clear is that the performance ofthe iterative learning algorithm when training withconcepts induced from gold chunks is not statisti-cally different (according to a t-test and p = 0.01)from the performance of the algorithm when trainingwith concepts induced from automatically computedchunks.
We consider this to be a positive outcome ofthe experiments.7 DiscussionThe two methods presented here are able to producepartial domain conceptualizations from a relativelysmall set of domain summaries3.
We have foundthat the clustering-based procedure is very competi-tive when presented with gold chunks.
On the otherhand, the iterative learning procedure performs verywell when presented with automatic chunks in alltested domains and the two languages.
We have alsofound that the performance of the iterative inductionsystem is not much affected by the use of automati-cally computed chunks.
We have run a t-test to ver-ify the differences in induction performance whenlearning with gold and automatic chunks (all con-dition) and have found statistically significant dif-ferences in only one domain out of four in Spanish(Terrorist Attack) and in two domains out of fourin English (Aviation Accident and Train Accident)(p = 0.01).
The applicability of the induction pro-cess, that is, if the auto-annotated data could be usedfor specific tasks, has been tested in two informationextraction experiments.
In a cross-lingual informa-tion extraction setting (Riloff et al 2002; Saggionand Szasz, 2011) we have observed that a systemtrained on automatically computed chunks has a per-formance close to one trained on concepts inducedfrom gold chunks.
No statistically significant differ-ences exist (p = 0.01) between the use of automaticchunks and gold chunks, except for the Train Acci-dent domain in English, where the system trainedon fully automatically annotated summaries has abetter performance.
In a full document information3Depending on the language and domain, between 50% and77% of all concepts are generated.extraction task, although the best system trained onauto-annotated summaries in Spanish has a big dif-ference with respect to a system trained on human-annotated summaries, in English the differences areslight.
We belive that this is due to the differencesin performance between the underlying text process-ing components.
Our methods work by grouping to-gether sets of chunks, unlike (Chambers and Juraf-sky, 2011), whose approach is centered around verbarguments and clustering, and relies on the avail-ability of considerable amounts of data.
Ontologylearning approaches such as OntoUSP (Poon andDomingos, 2010) are also clustering-based but fo-cus on learning is-a relations only.
Unlike (Leung etal., 2011) whose approach is based on gold-standardhumman annotations, we here test the performanceof the induction process using automatically com-puted candidate strings, and we additionally learnthe number of concepts automatically.8 Conclusions and Future WorkIn this paper we have concentrated on the prob-lem of knowledge induction from text summaries.The approaches we have presented are fully unsu-pervised and are able to produce reasonable con-ceptualizations (close to human concepts) withoutrelying on annotated data.
Unlike previous work,our approach does not require full syntactic parsingor a semantic dictionary.
In fact, it only requiresa process of text chunking and named entity recog-nition, which we have carefully assessed here.
Webelieve our work contributes with a viable method-ology to induce conceptual information from texts,and at the same time with an auto-annotation mech-anism which could be used to train information ex-traction systems.
Since our procedure requires verylittle linguistic information, we believe it can be suc-cessfully applied to a number of languages.
We alsobelieve that there is much work to be carried out andthat induction from summaries should be comple-mented with a process that explores full event re-ports, in order to reinforce some induced concepts,discard others, and discover additional ones.ReferencesMichele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional relation extraction.
In277Proceedings of ACL-08, pages 28?36.
Association forComputational Linguistics, June.R.
Barzilay and L. Lee.
2003.
Learning to paraphrase: anunsupervised approach using multiple-sequence align-ment.
In Proceedings of the 2003 Conference of theNorth American Chapter of the Association for Com-putational Linguistics on Human Language Technol-ogy - Volume 1, NAACL ?03, pages 16?23, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Chris Biemann.
2005.
Ontology Learning from Text: ASurvey of Methods.
LDV Forum, 20(2):75?93.E.
Brill.
1994.
Some Advances in Transformation-BasedPart of Speech Tagging.
In Proceedings of the TwelfthNational Conference on AI (AAAI-94), Seattle, Wash-ington.P.
Buitelaar and B. Magnini.
2005.
Ontology learningfrom text: An overview.
In In Paul Buitelaar, P., Cimi-ano, P., Magnini B.
(Eds.
), Ontology Learning fromText: Methods, Applications and Evaluation, pages 3?12.
IOS Press.N.
Chambers and D. Jurafsky.
2011.
Template-Based In-formation Extraction without the Templates.
In ACL,pages 976?986.Fabio Ciravegna and Yorick Wilks.
2003.
Designingadaptive information extraction for the semantic webin amilcare.
In Annotation for the Semantic Web,Frontiers in Artificial Intelligence and Applications.IOS.
Press.Gerald DeJong.
1982.
An Overview of the FRUMP Sys-tem.
In W.G.
Lehnert and M.H.
Ringle, editors, Strate-gies for Natural Language Processing, pages 149?176.
Lawrence Erlbaum Associates, Publishers.O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. S. Weld, and A. Yates.2004.
Methods for Domain-Indepedent InformationExtraction from the Web: An Experimental Compari-son.
In Proceedings of AAAI-2004.Christiane Fellbaum, editor.
1998.
WordNet - An Elec-tronic Lexical Database.
MIT Press.I.
Konstas and M. Lapata.
2012.
Concept-to-text gener-ation via discriminative reranking.
In Proceedings ofthe 50th Annual Meeting of the Association for Com-putational Linguistics, pages 369?378, Jeju Island,Korea, July.
Association for Computational Linguis-tics.Cane Wing-ki Leung, Jing Jiang, Kian Ming A. Chai,Hai Leong Chieu, and Loo-Nin Teow.
2011.
Unsuper-vised information extraction with distributional priorknowledge.
In Proceedings of the 2011 Conferenceon Empirical Methods in Natural Language Process-ing, pages 814?824, Edinburgh, Scotland, UK., July.Association for Computational Linguistics.Y.
Li, K. Bontcheva, and H. Cunningham.
2004.
AnSVM Based Learning Algorithm for Information Ex-traction.
Machine Learning Workshop, Sheffield.P.
Li, J. Jiang, and Y. Wang.
2010.
Generating Templatesof Entity Summaries with an Entity-Aspect Model andPattern Mining.
In Proceedings of ACL, Uppsala.ACL.D.
Maynard, V. Tablan, H. Cunningham, C. Ursu, H. Sag-gion, K. Bontcheva, and Y. Wilks.
2002.
ArchitecturalElements of Language Engineering Robustness.
Jour-nal of Natural Language Engineering ?
Special Issueon Robust Methods in Analysis of Natural LanguageData, 8(2/3):257?274.Chris D. Paice and Paul A. Jones.
1993.
The Identi-fication of Important Concepts in Highly StructuredTechnical Papers.
In R. Korfhage, E. Rasmussen, andP.
Willett, editors, Proc.
of the 16th ACM-SIGIR Con-ference, pages 69?78.J.
Piskorski and R. Yangarber.
2013.
Information ex-traction: Past, present and future.
In Thierry Poibeau,Horacio Saggion, Jakub Piskorski, and Roman Yan-garber, editors, Multi-source, Multilingual Informa-tion Extraction and Summarization, Theory and Ap-plications of Natural Language Processing, pages 23?49.
Springer Berlin Heidelberg.H.
Poon and P. Domingos.
2010.
Unsupervised ontologyinduction from text.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, ACL ?10, pages 296?305, Stroudsburg, PA, USA.Association for Computational Linguistics.Adwait Ratnaparkhi.
2000.
Trainable methods for sur-face natural language generation.
In Proceedings ofthe 1st North American chapter of the Association forComputational Linguistics conference, NAACL 2000,pages 194?201, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.E.
Riloff, C. Schafer, and D. Yarowsky.
2002.
Induc-ing information extraction systems for new languagesvia cross-language projection.
In Proceedings of the19th international conference on Computational lin-guistics, pages 1?7, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.E.
Riloff.
1993.
Automatically constructing a dictionaryfor information extraction tasks.
Proceedings of theEleventh Annual Conference on Artificial Intelligence,pages 811?816.E.
Riloff.
1996.
Automatically generating extraction pat-terns from untagged text.
Proceedings of the Thir-teenth Annual Conference on Artificial Intelligence,pages 1044?1049.H.
Saggion and G. Lapalme.
2002.
Generat-ing Indicative-Informative Summaries with SumUM.Computational Linguistics.278H.
Saggion and T. Poibeau.
2013.
Automatic textsummarization: Past, present and future.
In ThierryPoibeau, Horacio Saggion, Jakub Piskorski, and Ro-man Yangarber, editors, Multi-source, Multilingual In-formation Extraction and Summarization, Theory andApplications of Natural Language Processing, pages3?21.
Springer Berlin Heidelberg.H.
Saggion and S. Szasz.
2011.
Multi-domain Cross-lingual Information Extraction from Clean and NoisyTexts.
In Proceedings of the 8th Brazilian Sympo-sium in Information and Human Language Technol-ogy, Cuiaba?, Brazil.
BCS.H.
Saggion and S. Szasz.
2012.
The CONCISUS Corpusof Event Summaries.
In Proceedings of the 8th Lan-guage Resources and Evaluation Conference (LREC),Istanbul, Turkey.
ELDA.H.
Saggion.
2012.
Unsupervised content discoveryfrom concise summaries.
In Proceedings of the JointWorkshop on Automatic Knowledge Base Construc-tion and Web-scale Knowledge Extraction, AKBC-WEKEX ?12, pages 13?18, Stroudsburg, PA, USA.Association for Computational Linguistics.H.
Schmid.
1995.
Improvements In Part-of-Speech Tag-ging With an Application To German.
In In Proceed-ings of the ACL SIGDAT-Workshop, pages 47?50.I.
H. Witten and E. Frank.
1999.
Data Mining: Practi-cal Machine Learning Tools and Techniques with JavaImplementations.
Morgan Kaufmann.R.
Yangarber.
2003.
Counter-Training in Discovery ofSemantic Patterns.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics (ACL?03).279
