I )YNAMICS,  DEPENI )ENCY GRAMMAR AND INCREMENTAL INTERPRETAT ION*DAVID MILWARI)Centre for Cognitive Science, University of Edinburgh2, lhn:cleuch Place, Edinburgh, ElI8 9LW, ScotlanddavidmC~cogsci.ed, ac.ukAbst rac tThe paper describes two equiwtlent grammatical for-malisnLs.
The first is a lexicalised version of dependency grammar, and this can be nsed to providetree-structured analyses of sentences (though some-what tlatter than those usually provided by phra.sestructure grammars).
The second is a new forrnalism, 'Dynamic Dependency Gramniar', which usesaxioms and deduction rules to provide analyses ofsentences in terms of transitioos between states.A reformulation of dependency grammar usiugstate transitions is of interest on several grounds.Firstly, it can be used to show that incremental in-terpretation is possible without requiring notions ofoverlapping, or flexible constituency (as ill some ver-sions of categorial grammar), and without destroying a trasmparent link between syntax and seman-tics.
Secondly, the reformulation provides a level ofdescription which can act as an intermediate stagebetween the original grammar and a parsing algo-rithm.
Thirdly, it is possible to extend the relbrnmlated grammars with further axioii~s and deductionrules to provide coverage of syntactic onstructionssuch as coortlination which are tlitficult to encodelexically.1 DynamicsDynantics can roughly be described ms the study (ffsystems which consist of a set of states (cognitive,physical etc.)
and a family of binary lmnsil~on re-lalionships between states, corresponding to actionswhich can be performed to change from one state toanother (van Benthem, 1990).This paper introduces a notion of dynamic yram..Slat, where each word ill a sentence is treated a~s anaction which ha.s the potential to produce a change instate, and each state encodes (in some form) the syn-tactic or semantic dependencies of the words which|lave been absorbed so far.
There is no requirementfor tile number of states to he finite.
(ln fact, sincedependency grammar allows centre mbedding of ar-bitrary depth, the corresponding dynamic grammarprovides an unlimited number of states).Dynamic grammars are specified using very sim-ple logics, and a sentence is accepted ,~s grammaticalif and only if there is some proof that it perforn~sa transition between some suitable initial and final*This resen.rch w~.s nUplmrted by ml SERC researchfellowship.states, It is worth noting at this early stage that dy-namic grammars are not lexicalised rehashes of Aug-mented Transition Networks (Woods, 1973).
A'I'Nsuse a finite number of states combined with a re-cursion mechanism, and act ea'~entially in the sameway ms a top down parser.
They are not particularlysuited to increment,'d interpretation.To get an idea of how logics (instead of the moreusual algebra.s) can be used to specify dynamic sys-tenLs in general, it is worth considering a reformula-tion of the {bllowing finite state machine (FSM):h0 l 2 :1This accepts .
'~ grammatical ny string which mapsfrom the initial state, 0, to the final state, 3 (i.c.strings of the form: nb*eb).
The FSM cart be refor-mulated using a logic where the notation,StateO Str Statelis used to state that the string, Str, perfornm atransition from StateO to S ta te l .
The axioms (oratomic proofs) in tile logic are provided by the tran-sitions peribrnted by the individual letters.
Thus thefollowing are mssumed, t0 "a" t 1 "b" I 2 "b" 3 1 "c" 2The transitions given by the single letter strings areput together using a deduction rule, Sequencing, 2which states that, provided there is a proof thatStr ing,  takes us from some state, So, to a stateS t and a proof that Stringb takes us from St to $2,then there is a proof that the concatenation of thestrings takes us from S0 to $2.
The rule puts toaether strings of letters if the final state reached bytile first string matches an initial state for tile secontlstring.
For example, the rule may be instantiated as:A string is grammatical ccording to the logic if andonly if it is possible to construct a proof of the state-ment 0 Str  3 using the axioms and the SequencingRule.
For example, the string "abbcb" performs thetransitions,l "a" is a ~trin~ coxudsting of the single letter, a.2Notation: capital ettet~ will be used to denote variablesthroughout this paper.
'a' will be used to denote oncatena~tion.
For example, if Stringa = "kl" mad String b = "atilt",then StrlngaeStrlngb = ukltllllll.ACRES DE C()LING.
92, NAN-rES.
23-28 AOt'rr 1992 l 0 9 5 l)Roe, ov COLING-92, NA~I'ES AUG. 23-28, 1992and has the following proof, amongst others:1 "b" l 1 "b" 11 "bb" 1 1 "c" 20 "a" 1 1 "bbc" 20 "abbc" 2 2 "b" 30 "abbcb" 3Each leaf of the tree is an axiom, and the subproofsare put together using instantiations of the Sequenc-ing Rule.2 Lex iea l i sed  Dependency  Grammar"lYaditional dependency grammar is not concernedwith constituent structure, but with links betweenindividual words.
For example, an analysis of thesentence John thought Mary showed Ben ~o Suemight be represented as follows:John thought Mary showed Ben to SueThe word thought is the head of the whole sentence,and it has two dependents, John and showed, showedis the head of the embedded sentence, with three de-pendents, Mary, Ben and to.
A dependency graph issaid to respect adjacency if no word is separated fromits head except by its own dependents, or by anotherdependent of the same head and its dependents (i.e.there are no crossed links).
Adjacency is a reason-ably standard restriction, and has been proposed asa universal principle e.g.
by Hudson (1988).Given adjacency, it is possible to extract bracketedstrings (mid hence a notion of constituent structure)by grouping together each head with its dependents(and the dependents of its dependents).
For exam-ple, the sentence above gets the bracketing:\[John thought \[Mary showed Ben \[to Sue\]\]\]A noun phrase can be thought of as a noun plus allits dependents, a sentence as a verb plus all its de-pendents.In this paper we will assume adjacency, and, forsimplicity, that dependents are fixed in their orderrelative to the head and to each other.
Depen-dency grammars adopting these assumptions wereformalised by Gaifman (Hays, 1964).
Lexicalisationis relatively trivial given this formalisation, and thework on embedded ependency grammar within cat-egorial grammar (Barry and Picketing, 1990).Lexiealised Dependency Grammar (LDG) treatseach head as af lmct ion.
For example, the headlhought is treated as a function with two arguments,a noun phrase and a sentence.
Constituents areformed by combining functions with all their argu-ments.
The example above gets the following brack-eting and tree structure:\[John thought \[Mary showed Ben \[to Sue\]\]\]s. .....
I ~ ' -~llnp) / /L ~ls~ J ,ilnpl \Lr(np, pp)PP 1 np 10Lrtnp)The tree structure is particularly flat, with all ar-guments of a function appearing at tile same level(this contrasts with standard phrase structure anal-yses where the subject of showed would appear at adifferent level from its objects).Lexical categories are feature structures with threemain features, a base type (the type of the con-stituent formed by combining the lexical item withits arguments), a list of arguments which must ap-pear to the left, and a list of arguments which mustappear to the right.
The arguments at the top of thelists must appear closest o the functor.
For example,showed has the lexical entry,I 1 showed : llnp)L=(-p, ppl Jand can combine with an np on its left and with annp and then a pp on its right to form a sentence.When left and right argument lists are empty, cat-egories are said to be saturated, and may be writtenas their base type i.e.
\ [X ) \ ]  i s ident ica l toX.L~cljA requirement inherited from dependency grammaris for arguments to be saturated categories, a LDGswill be specified more formally in Section 4.It is worth outlining the differences between thecategories in LDG and those in a directed categorialgrammar.
Firstly, in LDG there is no specificationof whether arguments to the right or to the left ofthe functor should be combined with first.
Thus,the category, I(Y) , maps to both (X \Y ) /Z  andLrlZ~ J(X/Z)\Y, 4 Seeondly, arguments in LDG must besaturated, so there can be no functions of functions.
5aIn dependency granunar it is not ponaible to specify thata head requirea dependent with only some, but not all of itsdependenta.4So called 'Steedrn~n' otation.5An extended form of LDG which allows ungaturated argu-ACRES DE COLING-92.
NANTES, 23-28 AOITI" 1992 1 096 PROC.
OF COLING-92, NANTES, AUO.
23-28, 19923 Dynamic Dependency GrammarLexical i~d Dependency Grammar  can be reformu-lated as the dynamic grammar ,  Dynamic Depen-dency Grammar  (DDG).
Each state in DDG encodesthe syntactic context, and is labelled by the typc ofthe string absorbed so far.
For example, a possibleset of transitions for the string of words "Sue sawBen" is as follows:' I i  1 S Sue  \]J'etl L~lsl j ~l ~,up, , L~-InpljThe state after absorbing "Sue saw" is of type sen-tence mi~ing  a noun phrase, and that after absorb-lug "Sue saw Ben" is of type sentence.States are labelled by complex categories which arcsimilar to tile lexical categories of LDG, but withoutthe restriction that arguments must be saturated (forexample, the state after absorbing "Sue" has an un-saturated argument on its right list).
A string ofwords, Str ,  is grammatica l  provided tile followingstatement can be proven:i 0 Str s\[r(s}The initial state is labelled with an identity type i.e.a sentence missing a sentence.
This cat, be thoughtof as a context in whic}~ a sentence is expected, or asa suitable type for a string of words of length zero.The final state is just of type sentence.DDG is specified nsing a logic consisting of a set ofaxioms and a deduction rule.
The logic is similar, butmore general, than that used in Axiomatic Grammar(Milward, 1990)f The deduction rule is again calledSequencing.
The rule is identical in form to the Se-quencing Rule used in the reformulation of the FSM,though here it puts together strings of words rathertiian strings of letters.
The rule is as follows, ~Co String~ C,~ Ct String, C~Co String.oString b C~and is restricted to non-empty strings, smenks h&s been developed, and this also e2tn be reformulatedas a dynamic ~,m*mma" (Milward, 1992).6Axiomatic Grmnm~r is a particular dynamic grammar de-signed for English, which take~ relationslfips between statestm a primary phenomenon, to be justified solely by linguisticdata (rather thmt by an existing formalism such as dependencygranmlar).ZHere 'o' concatenates strings of words e.g.
"John"o"~leepa" = "John sleeps".SThis re~trictlon is not actually necessary as far as theequivalence between LDGs and DDGa is concerned.
Howeverits inclusion makes it trivial to show certain fontud propertiesof DDGs, such a.~ termination of proofs.The set of axioms is infinite since we need toconsider transitions between an arbitrary number ofcategories.
9 The set can be described using just twoaxiom schemata, Prediction and Application.
Pre-diction is given below, but is best understood byconsidering various instantiat ions) ?i0 iOI t( IL,L' ),It' rR,( IIX),L' ,R' L~O J L ~oLrRjPrediction is usually used when the category of tileword encomitered oes not match the category ex-pected by the current state.
Consider the followinginstantiation:i 0 "Sue" s where Sue:np\[rls) r( llnp) )LrOThe current state expects a seutence and encountersa noun phrase with \]exical entry Sue:r ip.
The result-ing state expects a sentence missing a noun phraseon its left e.g.
a verb phrase.Application gets its name from its similarity tofunction application (though it actually plays therole of both application and composition).
Theschema is ~ follows:I '\[1 I 1 I:,l i0 s "W" 10 where W: r( XIL ).R' \[rlt.R'J krR\] rOJAn example instantiation is when a noun phrase i~both expected and encountered e.g.
\[sll 0 "Ben" s where Ben:npL rl up, jGiven a word and a particular current state, theonly non-determinism in forming a resnlting state isdue to lexical ambiguity or from a choice between us-ing Prediction or Application (Prediction is possiblewhenever Application is).
Non-determinism is gen-?An infinite nmnber of distinguishable stat~ is required todeal with centre mbedding.X?L, L ~, R dad R' are lists of categories.
'#' COhCatermteslists e.g.
(k,l} ?
(,non} = (k,l,m,n).AC1ES DE COTING-92, Nnl, rn~.s, 2.3~28 Ao~' 1992 1 0 9 7 P~toc.
OF COL1NG-92, N^NaXS, AUO.
23-28, 1992erally kept low due to states being labelled by typesas opposed to explicit tree structures.
This is easiestto illustrate using a verb final language.
Consider apseudo English where the strings, "Ben Sue saw" and"Ben Sue sleeps believes" are acceptable sentences,and have the following LDG analyses:l l(np, L r ?
np}s/ \l(np)Lr(I JDespite the differences in the LDG tree structures,the initial fragment of each sentence, "Ben Sue", can..be treated identically by the corresponding DDG.The proof of the transition performed by the string"Ben Sue" involves two applications of Predictionput together using the Sequencing Rule.
The transi-tions are as follows:11) ~Z'~n s ~t  sLr(s)j r l  l(np) ) r(  l(np, np) )L r(l L L~The transitions for the two sentences diverge whenwe consider the words saw and sleeps.
In the formercase, Application is used, in the latter, Predictionthen Application.Efficient parsing algorithms can be based uponDDGs due to this relative lack of non-determinism inchoosing between states.
H The simplest algorithmis merely to non-deterministically apply Predictionand Application to the initial category.
Derivationsof algorithms from more complex logics, and the useof normalised proofs and equivalence classes of proofsare described in Milward (1991).4 LDGs --+ DDGsAn LDG can he specified more formally as follows:1.
A finite set of base types To, .. "In (such as s,up, and pp)llDetermlnism can also be increased by restricting the ax-ioms according to the properties of a particular lexicon.
Forexample, there is no point predicting categories missing twonoun phrases to the left when pansing English.2.
An infinite set of lexical categories of the form,XL1 where X is a base type, and L and I t  arerR\]lists of base types.
When L and R are empty, acategory is identical to its base type, X3.
A finite lexicon, L, which assigns lexical cate-gories to words4.
A distinguished base type, To.
A string is gram-matieal iff it has the category, To5.
A combination rule stating that,F \] if W has category, 1( Ti,.., TI)J r (  7,+a ..... Ti+i)and String1 has category T1, String2 has cate-gory 7~ etc.then the string formed by concatenatingString1, .. ,String~, "W' ,  Stringi+l, .. ,String/+jhas category Xcorresponding DDG is as follows:.
.
.
.L~Rjwhere X is a base type, and L and Yt are lists ofcategories2.
Two axiom schemata, Application and Predic-tion3.
The lexicon, L (as above)4.
One deduction rule, SequencingI \] 5.
A distinguished pair of categories, ll) , 7~L~:(nlJwhere To is as above.
A string, Str, is grammat-ical iff it is possible to prove: \[ 1'7(~?)
\] Str 7~r(To) jA proof that any DDG is strongly equivalent to itscorresponding LDG is given by Milward (1992).
Theproof is split into a soundness proof (that a DDGaccepts the same strings of words and assigns themcorresponding analysesl2), and a completeness proof(that a DDG accepts whatever strings are acceptedby the corresponding LDG).The1.5 Incrementa l  In terpretat ionIt is possible to augment each state with a semantictype and a semantic value.
Adopting a 'standard'  A-calculus emantics (c.f.
Dowty et al 1981) we obtainthe following transitions for the string "Sue saw":12For this purpose, it is convenient to treat an analysis in aDDG as the traasition~ performed by each word.
Each anal-ysis is a label for all equivalence Class of proofs.Acr~ DE COLING-92, NANTES, 23-28 AOI~'T 1992 1 0 9 8 PROC.
OF COLING-92.
NA~CrEs, AUG. 23-28.
1992I'I1 Is \] iO Sue 10 ~ S L~(s) j ~(il,,v)Lr(I Jl~t  (e--~t)-~tAQ.Q AP.P(sue').....
I s \] I 0) L ~-I up) je--~ tAY.saw'(sue',Y)The semantic types can generally be extracted fronlthe syntactic types.
The base types s and ap map tothe semantic types l and e, stmtding for trutb-valueand enl, ity respectively.
Categories with argmnentsmap to corresponding flmctional types.Provided a close mapping between syntactic andsemantic types is assumed, the addition of semanticvalues to the axiom schemata is relatively trivial, asis the addition of semantic vahtes to the lexicon.
Forexample, the semantic value given to the verb saw isAYAX.saw'(X,Y),  which has type e~(e-~t).It is worth contrasting the approach taken herewith two otller al)proaches to incremental interpre-tation.
Tim first is that of Pnlman (19851.
Pulman'sapproach separates syntactic and senmntie analysis,driving semantic ombinations off the actions of aparser for a phrase structnre grammar.
The ap-proach was important ill showing that hierarchicalsyntactic analysis and word by word incremental in-terpretation are not incompatihle.
The second ap-proach is that of Ades and Steedman (19821 whoincorporate conlposition rules directly into a catego-rim granlmar.
This allows a certain amount of incre-mental interpretation dim to tile possibility of form-ing constitnents for some initial substrings, flow-ew:r, the incorporation of composition i to the gram-mar itself does haw: sonic unwanted side effects whennfixed with a use of functions of fimctions.
For exam-pie, if the two types, N/N and N/N are composed togive tile type N/N, then this can be modified by anadjectival modifier of type (N/N)/(N/N).
Thus, thephrase the very old green car" can get the bracketing,\[the \[very \[old green\]\] car\].
Although tile Applica-tion schema used in DDGs does compose functionstogether, DI)Gs have identical strong generative ca-pacity to the LDGs they are based upon (the cov-erage of the grammars i  identical, and tile analysesare ill a one-to-one correspondence).
136 App l i ca t ionsSo far, l)ynamic l)ependeney Grammars can be seensolely as a way to provide incremental parsing and ill-terpretation for Lexicalised l)ependency Grammars.As such, they are not of particular linguistic signifi-cance, ltowever, it is possible to use DDGs as subsetsof ntore expressive dynamic gramnrars, where extraaxioms and deduction rules are used to provide cov-erage of syntactic phenomena which are difficult tola'I'his is also true for dyn~anic refomlulations of extendedversions of LDG which allow functionn of functions.encode lexically (e.g.
coordination, topicalisation andextrapcsition).
For example, tile following deductionrule (again restricted to non-empty strings),Co String, C,, C ~ C ,Co String,, "and '~C~provides all account of the syntax of non-constituentcoordination (Milward, 1991).
The sentences Johngave Mary a book and Peter a paper" and Sue soldand Peter thinks Bert bought a painting are acceptedsince "Mary a book" and "Peter a paper" performtile same transitions between syntactic states, ms do"Sue sold" and "Peter thinks Ben botlght"The gr,'mtmars described in this paper have beenimplemeuted in Prolog.
A dynamic gramnlar basedupon the extended version of l,I)Gs is being developed to provide incremental interpretation for thenatural hmguage interface to a grapllics package.Re ferencesAdes, A. and Steedman, M. (19721 On the Order ofWords.
Linguistics and Philosophy 4, 517-558.Aho, A. and Ulhnan, J.
(19721 The Theory of Pars-ing, Traaslatton and Compiling, Volume 1:Parsing.Prentice-Ilall nc, New Jersey.Barry, G. and Pickering, M. (19901 l)et)endcncy andConstituency ill Categorial Gramnlar.
Ill Barry, G.and Morrill, G. (eds), Studies in UategoTial Gram-mar.
Centre for Cognitive Science, University of E(I-inburgh.van Benthem, 3.
(19901 General )ynamics.
ITL1 re-port, Anlsterdaln (to appear in Theoretical Linguis-tics).I)owty, I).R., Wall, R.F.
and Peters, S. (19811 ht-troduction to Montague Semantics.
1).l~idel, Dor-drecht.Hays, II.G.
(19641 Dependency Theory: A Formal-ism and Some Observations.
Language 40,511-525.lhldson, IL (1988) Coordination and GramnlaticalRelations.
Journal of Linguistics 24, 303-342.Milward, D. (19901 Coordination in an AxiomaticGrammar.
hi Coling-90, llelsmki, vol 3,207-212.Milward, I).
(1991) Axiomatic Grammar, Non-Constituent Coordination, and lnerenmntal Interpre-tation.
PhD thesis, University of Cambridge.Milward, 11.
(1992) Dynamic Grammars.
TechnicalReport, Centre for Cognitive Science, University ofEdinburgh.
In preparation.Puhuan, S. (1985) A Parser Ttlat Doesn't.
In 2ndEuropean ACL, Geneva.Woods, W. (1973) An Experimental Parsing Sys-tem for Transition Network Grammars.
In Rustin,R.
(ed.
), Natural Language Processing, AlgorithmicsPress, New York.Aorl!s DE COLlNG-92, NANTES, 23-28 AOUq' 1992 I 0 9 9 PROC.
OF COLING-92, NAN'rI~.S, Auo.
23-28, 1992
