JEP-TALN-RECITAL 2012, Atelier DEFT 2012: D?fi Fouille de Textes, pages 61?68,Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCPParticipation du LINA ?
DEFT 2012Florian Boudin Amir Hazem Nicolas Hernandez Prajol ShresthaUniversit?
de Nantespr?nom.nom@univ-nantes.frR?SUM?Cet article pr?sente la participation de l?
?quipe TALN du LINA au d?fi fouille de textes (DEFT)2012.
D?velopp?
sp?cifiquement pour la seconde piste du d?fi, notre syst?me combine les sortiesde trois diff?rentes m?thodes d?extraction de mots cl?s.
Notre syst?me s?est class?
?
la 2i e`me placesur un total de 9 syst?mes avec une f-mesure de 21,3%.ABSTRACTLINA at DEFT 2012This article presents the participation of the TALN group at LINA to the d?fi fouille de textes(DEFT) 2012.
Developed specifically for the second task, our system combines the outputs ofthree different keyword extraction methods.
Our system ranked 2nd out of 9 systems with af-measure of 21,3%.MOTS-CL?S : extraction de mots cl?s, deft 2012, combinaison de m?thodes.KEYWORDS: keyword extraction, deft 2012, combining methods.1 IntroductionL?indexation automatique consiste ?
identifier un ensemble de mots cl?s (e.g.
mots, termes) quid?crit le contenu d?un document.
Les mots cl?s peuvent ensuite ?tre utilis?s, entre autres, pourfaciliter la recherche d?information ou la navigation dans les collections de documents.
L?
?dition2012 du d?fi fouille de textes (DEFT) porte sur l?extraction automatique de mots cl?s ?
partird?articles scientifiques parus dans le domaine des Sciences Humaines et Sociales (SHS).L?objectif du d?fi est de retrouver, ?
partir du contenu des documents (i.e.
articles scientifiques),les mots cl?s qui ont pu ?tre choisis par les auteurs.
Deux diff?rentes pistes ont ?t?
propos?es.La premi?re piste consiste ?
identifier dans une terminologie, les mots cl?s qui ont ?t?
assign?saux documents.
Cette terminologie regroupe l?ensemble des mots cl?s utilis?s dans la collection.La seconde piste, de prime abord plus complexe, consiste ?
extraire les mots cl?s directement ?partir du contenu des documents.
Cet article d?crit notre participation ?
la seconde piste du d?fi.Le reste de cet article est organis?
comme suit.
La section 2 d?crit l?ensemble de donn?es utilis?pour la campagne d??valuation.
La section 3 pr?sente les diff?rentes m?thodes que nous avonsd?velopp?es sp?cifiquement pour la seconde piste du d?fi.
Nous d?crivons ensuite en section4 nos r?sultats exp?rimentaux avant de pr?senter les m?thodes que nous avons test?es et qui61qui ont eu un impact nul ou n?gatif sur les r?sultats.
La section 6 conclut cet article et donnequelques perspectives de travaux futurs.2 Description de la campagne DEFT 2012L?ensemble de documents utilis?
pour le d?fi 2012 est constitu?
de 234 articles scientifiques parusdans le domaine des SHS.
Ces articles ont ?t?
publi?s entre 2001 et 2008 dans quatre revuesdiff?rentes.
L?ensemble d?apprentissage contient 60% des documents (soit 141 articles), et celuide test contient les 40% restants (soit 93 articles).
La r?partition des quatre diff?rentes revuesdans les deux ensembles est uniforme.Du point de vue technique, les articles sont au format XML.
Ils sont structur?s en deux parties : ler?sum?
et le corps de l?article.
Chaque article contient ?galement le nombre de mots cl?s indexantson contenu.
Les mots cl?s assign?s ?
chaque article sont disponibles pour chacun des articles del?ensemble d?entra?nement.Les syst?mes participant au d?fi sont ?valu?s ?
l?aide des mesures classiques de pr?cision, rappelet f-mesure.
Pour chaque article, les mots cl?s g?n?r?s par les syst?mes sont compar?s aux motscl?s de r?f?rence (assign?s par les auteurs).
Afin de limiter les probl?mes li?s aux diff?rentesvariations orthographiques, plusieurs traitements de normalisation (i.e.
normalisation de la casseet lemmatisation) sont appliqu?s au pr?alable aux mots cl?s.
Chaque participant peut soumettrejusqu??
trois ex?cutions par piste.La liste ci-dessous pr?sente quelques unes des difficult?s que nous avons identifi?es dans lesarticles de l?ensemble d?entra?nement.?
Articles diff?rents ayant le m?me r?sum?, e.g.
les articles as_2002_007048ar et as_2002_007053ar.?
Contenu des articles dans des langues diff?rentes et/ou m?lang?es, e.g.
fran?ais et anglaisdans ttr_2008_037494ar, espagnol dans meta_2005_019927ar.?
Contenu des articles tr?s bruit?
avec des probl?mes de ponctuation, de caract?res unicodes etde segmentation en paragraphes, e.g.
ci-dessous un extrait de l?article meta_2005_019840ar.<p>Ce langage est au c.ur des pr?occupations des juristes , quinous rappellent r?guli?rement </p><p>que le droit est affaire de mots.
Et cela dans tout l.universdu droit , vers quelque c?t?
que l.on se</p><p>tourne , dans le monde juridique anglophone - o?, pourMellinkoff (1963& amp;#x00A0 ;: vii), &amp;#x00A0;The law is a</p><p></p><p>dans son ensemble , la technique juridique aboutit , pour laplus grande part , ?
une question de</p><p>terminologie&amp;# x00A0;.
Chacun pourra le v?rifier par laconsultation d.ouvrages parmi les plus r?cents et</p>623 ApprochesLes diff?rentes m?thodes que nous avons d?velopp?es utilisent le mot comme unit?
principale.Nous avons donc appliqu?
un ensemble commun de pr?-traitements aux documents : segmenta-tion en phrases, d?coupage en mots et ?tiquetage morpho-syntaxique.
L?information structurellepr?sente dans chacun des documents (i.e.
r?sum?, corps de l?article et paragraphes) est pr?serv?e.Chaque paragraphe est segment?
en phrases en utilisant la m?thode PUNKT de d?tection dechangement de phrases (Kiss et Strunk, 2006) mise en ?uvre dans la bo?te ?
outils NLTK (Bird etLoper, 2004).
La tokenisation des phrases est effectu?e avec un outil d?velopp?
en interne utili-sant le lexique des formes fl?chies du fran?ais (lefff)1 pour l?indentification des unit?s lexicalescomplexes (e.g.
mots compos?s).
L?
?tiquetage morpho-syntaxique est obtenu ?
l?aide du StanfordPOS Tagger (Toutanova et al, 2003)2 entrain?e sur le French Treebank (Abeill?
et al, 2003).3.1 Syst?me 1Ce syst?me est bas?
sur du T F?
IDF et trois r?gles issues du corpus d?apprentissage.
La principalequestion qui se pose ici est : qu?est ce qu?un mot cl?
?
ou autrement dit, qu?est ce qui fait qu?unterme a plus de chances d?
?tre un mot cl?
qu?un autre ?En analysant les documents du corpus d?apprentissage, nous avons relev?
trois particularit?s li?esaux mots cl?s.
La premi?re concerne leur localisation dans les documents.
Chaque document ?tantdivis?
en deux parties qui sont : le r?sum?
(ABSTRACT) et le corps du document (BODY), nousnous sommes donc int?ress?s ?
la position des mots cl?s par rapport ?
ce d?coupage.
Nous avonspu constater qu?un terme apparaissant ?
la fois dans le r?sum?
et dans le corps du document avaitplus de chances d?
?tre un mot cl?.
Ainsi, nous avons utilis?
cette information comme premi?rer?gle de notre syst?me (nous appellerons cette r?gle : R1).
Deux strat?gies utilisant cette r?gleont ?t?
adopt?es, la premi?re consiste ?
ne s?lectionner que des termes qui apparaissent ?
lafois dans le r?sum?
et dans le corps du document (nous appellerons cette strat?gie : S1), ladeuxi?me consiste ?
donner la priorit?
aux termes respectant la strat?gie S1 en utilisant unepond?ration par un param?tre ?
fix?
empiriquement (nous appellerons cette strat?gie : S2).Les diff?rents tests conduits ont montr?
que l?utilisation de la strat?gie S1 donnait de meilleursr?sultats que l?utilisation de la strat?gie S2.
Intuitivement, nous aurions tendance ?
penser lecontraire (la strat?gie S2 devrait ?tre meilleur que S1), car ?liminer des termes n?apparaissantque dans le r?sum?
ou que dans le corps du document nous ferait sans doute perdre des motscl?s.
L?explication et que la strat?gie S1 corrige sans doute les faiblesses de notre syst?me quirenverrait plus de faux positifs que de vrais n?gatifs.La deuxi?me particularit?
relative aux n-grammes, d?coule de la question suivante : est ce qu?unterme simple (1-gramme) a plus de chances d?
?tre un mot cl?
qu?un terme compos?
(n-grammesavec n > 1) ?
De part le corpus d?apprentissage nous avons pu constater qu?il y avait 70% determes simples et 30% de termes compos?s.
Ainsi, nous avons voulu donner une plus grandeimportance aux termes simples extraits par notre syst?me.
De la m?me mani?re que pour lastrat?gie S2, nous avons introduit un param?tre de pond?ration ?
afin de prioriser les termessimples (nous appellerons cette r?gle R2).1http://www.labri.fr/perso/clement/lefff/2Nous utilison la version 3.1.0 avec les param?tres par d?faut.63La troisi?me particularit?
rel?ve de la simple observation que la quasi totalit?
des mots cl?s?taient soit des noms, soit des adjectifs.
?
partir de cette constatation, nous avons introduit unetroisi?me r?gle (R3) qui filtre les verbes.3.2 Syst?me 2Ce syst?me repose sur l?exploitation d?un existant ?
savoir l?approche KEA (Keyphrase ExtractionAlgorithm) de (Witten et al, 1999).
KEA permet d?une part de mod?liser les expressions significa-tives (compos?es d?un ou plusieurs mots) du contenu de textes ?
l?aide de textes et d?expressionscl?s associ?es et d?autre part d?extraire les expressions cl?s d?une collection de textes ?
l?aided?une mod?lisation construite a priori.
L?approche utilise un classifieur bay?sien na?f pour calculerun score de probabilit?
de chaque expression cl?
candidate.
La construction requiert un ensembled?expressions cl?s class?es positivement pour chaque texte du corpus d?apprentissage.
L?extractionse r?alise sur un corpus de domaine similaire au domaine du corpus d?apprentissage.Les phases de mod?lisation ou d?extraction des expressions cl?s fonctionnent toutes deux ?
lasuite de deux phases ?l?mentaires : l?extraction de candidats et le calcul de traits descriptifsdes candidats.
Les candidats s?obtiennent par extraction de n-grammes de taille pr?d?finie ned?butant pas et ne finissant pas par un mot outil.Les traits utilis?s pour d?crire chaque candidat au sein d?un document sont les suivants : leT F ?
IDF (mesure de sp?cificit?
du candidat pour le document), la position de la premi?reoccurrence (pourcentage du texte pr?c?dent l?occurrence), le nombre de mots qui compose lecandidat.
Les candidats ayant un haut T F ?
IDF , apparaissant au d?but d?un texte et comptantle plus de mots sont ainsi consid?r?s comme ?tant de bons descripteurs du contenu d?un texte.Les derni?res ?volutions de KEA permettent d?exploiter des lexiques contr?l?s de type th?saurusdans la construction de la mod?lisation (Medelyan et Witten, 2006).Nous n?avons pas exploit?
de ressources ext?rieures de type lexiques contr?l?s dans la constructionde notre mod?lisation.
Nous avons utilis?
la version 5.0 de l?impl?mentation de KEA3 disponiblesous licence GNU ; en pratique nous avons utilis?
les fonctionnalit?s d?extraction ?libre?
pr?sentesd?s la version 3.0.
Les fonctionnalit?s d?velopp?es ult?rieurement concernent l?exploitation delexiques contr?l?s.
Nos candidats ?taient au maximum de taille 5.
Nous avons exploit?
le corpusd?apprentissage fourni pour la seconde piste pour construire notre mod?lisation.
Chaque texte(r?sum?
et corps) a ?t?
consid?r?
comme une unit?
documentaire.L?approche KEA est facilement portable ?
diff?rentes langues du fait qu?elle n?cessite peu deressources.
En particulier elle ne requiert pas une pr?-analyse syntaxique pour s?lectionner descandidats.
Nous avons n?anmoins port?
une certaine attention ?
nos traitements pr?liminaireset nous avons constat?
qu?une pr?-segmentation en token mots ainsi que l?utilisation d?une listemultilingue de mots outils augmentaient la qualit?
de l?extraction des expressions cl?s lorsquenous ?valuions l?approche par validation crois?e sur le corpus d?apprentissage.
Concernant laliste des mots outils, nous avons fusionn?
les listes fournies par KEA pour le fran?ais, l?anglais etl?espagnol.
Nous l?avons compl?t?e des formes des mots outils pouvant subir une ?lision du e finalen fran?ais (e.g.
?le?
s?est vu compl?t?
de la forme ?l?
?, de m?me pour ?lorsque?
avec ?lorsqu??.
.
.
).Ces formes ?taient en effet reconnues par notre segmenteur en mots.3http://www.nzdl.org/Kea643.3 Syst?me 3Ce syst?me est bas?
sur une approche par classification supervis?e.
La t?che d?extraction de motscl?s est ici consid?r?e comme une t?che de classification binaire.
La premi?re ?tape consiste ?g?n?rer tous les mots cl?s candidats ?
partir du document.
Pour ce faire, nous commen?ons parextraire tous les n-grammes de mots jusqu??
n = 4.
Des contraintes syntaxiques sont ensuiteutilis?es pour filtrer les candidats.
Ainsi, seuls les n-grammes compos?s uniquement de noms,d?adjectifs et de mots outils (except?
en premier/dernier mot du n-gramme) sont gard?s.Pour chaque candidat, nous calculons les traits suivants :?
Poids T F ?
IDF?
Nombre de mots du n-gramme?
Patron syntaxique du n-gramme (e.g.
?Nom Adjectif?)?
Position relative de la premi?re occurence dans le document?
Section(s) o?
apparait le n-gramme (r?sum?, corps ou les deux)?
Nombre de documents de la collection dans lesquels le n-gramme apparait?
Score de saillance dans l?arbre de d?pendances de coh?sion lexicale du texte (voir ci-dessous)Nous construisons ce que nous appelons un ?arbre de d?pendances de coh?sion lexicale?
selonune approche d?crite par Choi ?
la section 6.3.1. de sa th?se (Choi, 2002).
Une d?pendanceest pr?suppos?e exister entre deux phrases cons?cutives si celles-ci ont des mots en commun ;l?hypoth?se est de consid?rer la seconde phrase comme une ?laboration de la premi?re.
Enpratique, notre algorithme ne reconna?t pas syst?matiquement une relation de d?pendanceentre deux phrases cons?cutives qui partagent des mots en commun.
En effet notre algorithmerecherche, pour chaque phrase du texte, la phrase la plus haute dans la cha?ne de d?pendancede la phrase pr?c?dente avec laquelle elle partage des mots en commun.
L?arbre est construiten prenant le texte dans son ensemble (r?sum?
et corps) pr?alablement lemmatis?.
Un score desaillance est calcul?
pour chaque phrase en fonction du nombre de ses d?pendances (directeset transitives) normalis?
par le nombre de d?pendances maximal qu?une phrase peut avoir surle texte donn?.
Chaque expression candidate h?rite alors du score de la phrase o?
apparait sapremi?re occurence.Nous utilisons la combinaison par vote de trois algorithmes de classification disponibles dans laboite ?
outils Weka (Hall et al, 2009) : NaiveBayes, J48 et RandomForest.
Les mots-cl?s candidatssont ensuite tri?s selon leurs scores de pr?diction.3.4 Combinaison des syst?mesLes trois syst?mes que nous avons d?velopp?s utilisent diff?rentes m?thodes pour capturerl?importance d?un mot cl?
par rapport ?
un document.
Une combinaison des sorties de cesderniers est donc pertinente.Nous disposons pour chaque document, de trois listes pond?r?es de mots cl?s.
La m?thode laplus simple consisterait ?
utiliser la somme des scores des trois syst?mes.
Cependant, les scorescalcul?s par chacun des syst?mes ne sont pas directement comparables.
?
la place du score, nousutilisons pour chaque mot cl?
candidat, l?inverse de son rang dans la liste ordonn?e.Deux strat?gies de combinaison ont ?t?
utilis?es.
La premi?re, COMBI1 consiste ?
assigner la65somme de l?inverse des rangs d?un mot cl?
dans les listes ordonn?es des trois syst?mes.
Pour laseconde strat?gie, COMBI2, nous ne consid?rons que les mots cl?s apparaissant dans les sortiesdes trois syst?mes.
L?id?e est de filtrer les mots cl?s consid?r?s comme important par seulementun ou deux des trois syst?mes.4 R?sultatsNous pr?sentons dans cette section les r?sultats officiels de la campagne DEFT 2012.
Nousavons soumis trois ex?cutions pour chacune des deux pistes.
Pour la premi?re piste, nous avonssimplement utilis?
le Syst?me 1 (d?crit dans la section 3.1) et filtr?
les mots cl?s candidats ?
l?aidede la terminologie.
Le nombre de mots cl?s retourn?s est fix?
?
7 pour la premi?re ex?cution et ?6 pour les deux autres.
Les trois configurations utilisent la r?gle R3.La premi?re ex?cution utilise la r?gle R2 (?
= 0,6).
La seconde ex?cution utilise la r?gle R2(?
= 0, 65).
La troisi?me ex?cution utilise la r?gle R1 avec la strat?gie S2 (?= 0, 65) et la r?gleR2 (?
= 0,65).
Pour la seconde piste, nous avons soumis les ex?cutions de deux combinaisons(COMBI1 et COMBI2) ainsi que du syst?me 3 (d?crit dans la section 3.3).
Le nombre de mots cl?sretourn?s est fix?
?
130% du nombre de mots cl?s de r?f?rence pour COMBI1 et COMBI2 et ?110% pour le syst?me 3.
Ces nombres permettent d?obtenir les meilleurs r?sultats sur l?ensembled?entra?nement.La table 1 pr?sente les r?sultats de nos trois ex?cutions pour la premi?re piste.
Les r?sultatsobtenus par les trois ex?cutions sont moins bons que ceux obtenus sur l?ensemble d?entrainement(f-mesure=0,44 pour la premi?re ex?cution).
Nous constatons que la variation du rappel sur lestrois ex?cutions est faible.
La chute de la pr?cision pour la troisi?me ex?cution s?explique parl?application de la r?gle R1 qui limite le nombre de candidats possibles.Syst?me Pr?cision Rappel f-mesure1 0,3812 0,4004 0,39062 0,3759 0,3948 0,38513 0,3343 0,4097 0,3682TAB.
1 ?
R?sultats de nos trois ex?cutions pour la premi?re piste.La table 2 montre les r?sultats de nos trois ex?cutions pour la seconde piste.
Nous pouvons voirque la performance de COMBI2 est largement en dessous de COMBI1.
Nous avions constat?
leph?nom?ne inverse sur les donn?es d?entra?nement.
Ceci est du au fait que le nombre de motscl?s retourn?s par COMBI2 peut dans certains cas ?tre inf?rieur au seuil que nous avons fix?.
Eneffet, l?intersection des listes des 100 meilleurs mots cl?s candidats de chaque syst?me est tr?srestrainte pour quelque uns des documents de l?ensemble de test.
Nous constatons que les scoresdu syst?me 3, ayant obtenu les meilleurs r?sultats sur l?ensemble d?entra?nement parmis nos troissyst?mes, sont faibles en comparaison des deux combinaisons.
Ce r?sultat semble indiquer unprobl?me de sur-entra?nement et illustre bien l?utilit?
de la combinaison.La table 3 pr?sente, pour chacune des deux pistes, le classement des diff?rentes ?quipes sur labase de la meilleure soumission.
Notre soumission est class?e au rang 5 sur 10 pour la premi?re66Syst?me Pr?cision Rappel f-mesureCOMBI1 0,1949 0,2355 0,2133COMBI2 0,1788 0,2128 0,1943Syst?me 3 0,1643 0,1880 0,1753TAB.
2 ?
R?sultats de nos trois ex?cutions pour la seconde piste.piste et au rang 2 sur 9 pour la seconde piste.
Les r?sultats obtenus par l?
?quipe 16 sont bienau dessus de toutes les autres ?quipes et montrent qu?une marge de progression importante estpossible pour notre syst?me.Rang Piste 1 Piste 21 ?quipe 16 (0,9488) ?quipe 16 (0,5874)2 ?quipe 05 (0,7475) ?quipe 06 (0,2133)3 ?quipe 04 (0,4417) ?quipe 05 (0,2087)4 ?quipe 02 (0,3985) ?quipe 02 (0,1921)5 ?quipe 06 (0,3906) ?quipe 01 (0,1901)6 ?quipe 01 (0,2737) ?quipe 13 (0,1632)7 ?quipe 13 (0,1378) ?quipe 04 (0,1270)8 ?quipe 17 (0,1079) ?quipe 17 (0,0895)9 ?quipe 03 (0,0857) ?quipe 03 (0,0785)10 ?quipe 18 (0,0428) -TAB.
3 ?
Classement de DEFT 2012 sur la base de la meilleure soumission de chaque ?quipe pourchacune des deux pistes.
Notre classement est indiqu?
en gras (?quipe 06).5 Ce qui n?a pas march?Nous d?crivons ici les m?thodes qui ont eu un impact nul ou n?gatif sur les r?sultats.Traits ayant un impact n?gatif sur la performance du syst?me 3 : la dispersion d?un mot cl?dans le document, mots appartenant ?
des phrases contenant des citations, noms des auteurs lesplus cit?s dans le document (sp?cifique aux articles commen?ant par ?as?
).Suppression de la redondance : nous avons constat?
un niveau de redondance importantdes mots cl?s dans les sorties de nos syst?mes.
Par exemple, les mots cl?s ?jardins collectifs?,?jardins?
et ?collectifs?
sont tous les trois pr?sents dans le top 10, ce qui fait baisser le rappel.Plusieurs strat?gies ont ?t?
exp?riment?es pour supprimer cette redondance (e.g.
suppressiond?un n-gramme si tous les mots qui le composent sont ?galement pr?sents parmi les 10 meilleurscandidats).
Une d?gradation des r?sultats est cependant observ?e indiquant que la strat?gie ?adopter est d?pendante des documents.Mod?le de pond?ration ?
base de graphe : nous avons impl?ment?
l?approche propos?edans (Mihalcea et Tarau, 2004).
Il s?agit de repr?senter chaque document sous la forme d?un67graphe de mots connect?s par des relations de co-occurrences.
Des algorithmes de centralit?
sontensuite appliqu?s pour extraire les mots les plus caract?ristiques.
Les r?sultats obtenus par cettem?thode sont inf?rieurs ?
ceux obtenus ?
l?aide d?une pond?ration par la mesure T F ?
IDF .6 ConclusionsNous avons d?crit la participation du LINA ?
DEFT 2012.
Notre syst?me est le r?sultat de lacombinaison des sorties de trois diff?rentes m?thodes d?extraction de mots cl?s.
Les r?sultatsobtenus par ce dernier sont toujours meilleurs que ceux obtenus par chacune des trois m?thodesindividuellement.
Pour la seconde piste, notre syst?me s?est class?
?
la 2i e`me place sur un total de9 syst?mes avec une f-mesure de 21,3%.La strat?gie que nous avons employ?e pour combiner les sorties des diff?rentes m?thodes n?estcependant pas optimale.
Nous envisageons d?
?tendre ce travail en proposant d?autres strat?giescomme par exemple l?utilisation d?un meta-classifieur.R?f?rencesABEILL?, A., CL?MENT, L. et TOUSSENEL, F. (2003).
Building a treebank for French.
Treebanks :building and using parsed corpora, pages 165?188.BIRD, S. et LOPER, E. (2004).
NLTK : The natural language toolkit.
In ACL, Barcelone, Espagne.CHOI, F. Y. Y.
(2002).
Content-based Text Navigation.
Th?se de doctorat, Department of ComputerScience, University of Manchester.HALL, M., FRANK, E., HOLMES, G., PFAHRINGER, B., REUTEMANN, P. et WITTEN, I.
(2009).
The wekadata mining software : an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?18.KISS, T. et STRUNK, J.
(2006).
Unsupervised multilingual sentence boundary detection.
Compu-tational Linguistics, 32(4):485?525.MEDELYAN, O. et WITTEN, I. H. (2006).
Thesaurus based automatic keyphrase indexing.
InProceedings of the 6th ACM/IEEE-CS joint conference on Digital libraries, JCDL ?06, pages 296?297,New York, NY, USA.
ACM.MIHALCEA, R. et TARAU, P. (2004).
Textrank : Bringing order into texts.
In LIN, D. et WU,D., ?diteurs : Proceedings of EMNLP 2004, pages 404?411, Barcelona, Spain.
Association forComputational Linguistics.TOUTANOVA, K., KLEIN, D., MANNING, C. et SINGER, Y.
(2003).
Feature-rich part-of-speech taggingwith a cyclic dependency network.
In Proceedings of the 3rd Conference of the North AmericanChapter of the ACL (NAACL 2003), pages 173?180.
Association for Computational Linguistics.WITTEN, I. H., PAYNTER, G. W., FRANK, E., GUTWIN, C. et NEVILL-MANNING, C. G. (1999).
Kea :Practical automatic keyphrase extraction.
CoRR, cs.DL/9902007.68
