Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 43?48,Baltimore, Maryland, 26-27 July 2014. c?2014 Association for Computational LinguisticsRACAI GEC ?
A hybrid approach to Grammatical Error CorrectionTiberiu Boro?Calea 13 Septembrie, 13BucharestRACAItibi@racai.roStefan Daniel DumitrescuCalea 13 Septembrie,13BucharestRACAIsdumitrescu@racai.roAdrian ZafiuStr.
Targul din Vale, 1PitestiUPIT - FECCadrian.zafiu@comin.roDan Tufi?Calea 13 Septembrie, 13BucharestRACAItufis@racai.roVerginica Mititelu BarbuCalea 13 Septembrie, 13BucharestRACAIvergi@racai.roPaul Ionu?
V?duvaCalea 13 Septembrie, 13BucharestRACAIionut@racai.roAbstractThis paper describes RACAI?s (ResearchInstitute for Artificial Intelligence) hy-brid grammatical error correction system.This system was validated during the par-ticipation into the CONLL?14 SharedTask on Grammatical Error Correction.We offer an analysis of the types of er-rors detected and corrected by our sys-tem, we present the necessary steps to re-produce our experiment and also the re-sults we obtained.1 IntroductionGrammatical error correction (GEC) is a com-plex task mainly because of the natural depend-encies between the words of a sentence both atthe lexical and the semantic levels, leave it asidethe morphologic and syntactic levels, an intrinsicand complex attribute specific to the human lan-guage.
Grammatical error detection and correc-tion received a significant level of interest fromvarious research groups both from the academicand commercial environments.
A testament tothe importance of this task is the long history ofchallenges (e.g.
Microsoft Speller Challenge andCONLL Shared Task) (Hwee et al., 2014) thathad the primary objective of proving a commontesting ground (i.e.
resources, tools and goldstandards) in order to assess the performance ofvarious methods and tools for GEC, when ap-plied to identical input data.In the task of GEC, one can easily distinguishtwo separate tasks: grammatical error detectionand grammatical error correction.
Typically,there are three types of approaches: statistical,rule-based and hybrid.
The difficulty of detectingand correcting an error depends on its class.
(a) Statistical approaches rely on buildingstatistical models (using surface forms orsyntactic labels) that are used for detectingand correcting local errors.
The typicalstatistical approach is to model how likelythe occurrence of an event is, given a his-tory of preceding events.
Thus, statisticalapproaches easily adaptable to any lan-guage (requiring only training data in theform of raw or syntactically labeled text)are very good guessers when it comes todetecting and correcting collocations, idi-oms, typos and small grammatical inad-vertences such as the local gender andcase agreements.
The main impedimentsof such systems are two-fold: (1) they areresource consuming techniques(memory/storage) and they are highly de-pendent on data ?
large and domainadapted datasets are required in order toavoid the data-scarceness specific issueand currently they rely only on a limitedhorizon of events; (2) they usually lacksemantic information and favoring high-occurring events is not always the bestway of detecting and correcting grammat-ical errors.
(b) Rule-based approaches embed linguisticknowledge in the form of machine parsa-ble rules that are used to detect errors and43describe via transformations how variouserror types should be corrected.
Thedrawbacks to rule-based system are (1) theextensive effort required to build the rule-set, (2) regardless of the size of the rule-set, given the variability of the human lan-guage it is virtually impossible to captureall possible errors and (3) the large num-ber of exceptions to rules.
(c) Hybrid systems that combine both rule-based and statistical approaches are plau-sible to overcome the weaknesses of thetwo methodologies if the mixture of thetwo components is done properly.
Detec-tion of errors can be achieved statisticallyand rule-based, the task of the hybrid ap-proach being to resolve any conflicts thatarise between the outputs of the two ap-proaches.However, even the most advanced systems areonly able to distinguish between a limited num-ber of error types and the task of correcting anerror is even more difficult.
Along the typical setof errors that are handled by typical correctionsystems (punctuation, capitalization, spelling,typos, verb tense, missing verb, etc.
), CONLL?sGEC task introduces some hard cases which re-quire a level of semantic analysis: local redun-dancy, unclear meaning, parallelism, etc.2 External tools and resourcesOne step in the preparation phase was theanalysis of the types of errors.
The training setwas automatically processed with our Bermudatool (Boro?
et al., 2013): it underwent sentencesplitting, tokenization, part of speech tagging,lemmatization and also chunking.
Comparing theoriginal and the corrected sentences, we couldrank the types of mistakes.The most frequent ones, i.e.
occurring morethan 1000 times, are presented in the followingtable:Type of error Occurrences Percentuse of articles and de-terminers6647 14.98wrong collocations oridioms5300 11.94local redundancies 4668 10.52noun number 3770 8.49tenses 3200 7.21punctuation and orthog-raphy3054 6.88use of prepositions 2412 5.43word form 2160 4.87subject-verb agreement 1527 3.44Verb form 1444 3.25Link word/phrases 1349 3.04Table 1.
The most frequent types of mistakesin the training dataThere are also some less frequent errors: pro-noun form, noun possessive form, word order ofadjectives and adverbs, etc.
Some of these can besolved by means of rules, others by accessinglexical resources and others are extremely diffi-cult to deal with.As far as the test data are concerned, the errordistribution according to their types is the fol-lowing:Type of error Occurrencesin official-2014.0.m2Occurrencesin official-2014.1.m2use of articles anddeterminers 332 437wrong collocationsor idioms 339 462local redundancies 94 194noun number 214 222tenses 133 146punctuation andorthography 227 474use of prepositions 95 152word form 76 104subject-verbagreement 107 148Verb form 132 88Link word/phrases 93 78Table 2.
The most frequent types of mistakesin the test dataRoughly, the same types of mistakes are morefrequent in the test set, just like as in the trainingset.For collocations and idioms, as well as for cor-rect prepositions use, we consider that only lexi-cal resources can be of help.
They can take theform of a corpus or lists of words that subcatego-rize for prepositional phrases obligatorily headedby a certain preposition (see section 3.2).
Weadopted the former solution: we used Google 1Tn-grams corpus (see section 2.2) from which theselectional restrictions can be learned quite suc-cessfully.
However, dealing with collocations isdifficult, as correction does not involve only syn-tax, but also semantics.
Changing a word in asentence usually implies changing the meaningof the sentence as a whole.
Nevertheless, a solu-tion can be found: as mistakes in collocationsinvolve the use of a related word (synonyms), a44resource such as the WordNet can be of help.When the word used in the sentence and the oneoccurring in the corpus can be found in the samesynset (or even in synsets in direct relation), thecorrection could be made.
Otherwise, it is riskyto try.
In any scenario, this remains as futurework for us.2.1 RACAI NLP ToolsWe have used our in-house Bermuda softwaresuite (Boro?
et al., 2013), (Boro?
andDumitrescu, 2013) to perform text pre-processing.
As the tool is well documented in thecited papers above, we summarize its main func-tionalities concerning the task at hand and thealgorithms behind them.Tokenization.
A basic necessary prepro-cessing step that needs to be applied from thebeginning as most tools work on a certain to-kenization format.
Bermuda uses a custom-built,language dependent tokenizer.
Based on regularexpressions, it detects and splits words such as[haven?t] into [have] and [n?t]; [boy?s] into [boy]and [?s], while leaving abbreviations like [dr.] or[N.Y.] as a single token.Part-of-speech (POS) tagger.
Tagging is es-sential to determine each word?s part of speechand thus its role in the sentence.
Each word istagged with a morpho-syntactic descriptor, calledMSD.
The English language has around 100MSDs defined, while more inflected languages,like Romanian ?
a Latin-derived language, usesover 600.
An MSD completely characterizes theword morphologically and syntactically 1 .
Forexample, ?Np?
refers to a proper noun while?Ncns?
refers to a common (c) noun (N) that hasa neuter (n) gender and is in singular form (ex:zoo, zone).
Our tagger is based on a neural net-work, introduced in (Boro?
et al., 2013).
Overall,the Bermuda POS Tagger obtains very high ac-curacy rates (>98%) even on the more difficult,highly inflected languages.Lemmatization.
The Bermuda Lemmatizer isbased on the MIRA algorithm (Margin InfusedRelaxed Algorithm) (Crammer and Singer,2003).
We treat lemmatization as a tagging task,in which each individual letter of the surfaceword is tagged as either remaining unchanged,being removed or transformed to another letter.The lemmatizer was trained and tested on anEnglish lexicon containing a number of around120K surface-lemma-MSD entries.1 Full description of MSDs can be found at :http://nl.ijs.si/ME/V4/msd/html/msd-en.html2.2 Google 1T corpusA good performing language model is a very im-portant resource for the current task, as it allowsdiscriminating between similar phrases by com-paring their perplexities.Although we had several corpora available toextract surface-based language models from, wepreferred to use a significantly larger model thanwe could create: Google 1T n-gram corpus(Brants and Franz, 2006).
This 4 billion n-gramcorpus should provide high-quality perplexityestimations.
However, loading 4*10^9 n-gramswithout any compression scheme would require,even by today?s standards, a large amount ofmemory.
For example, using SRILM (Stolcke,2002) which uses 33 bytes per n-gram, wouldrequire a total of ~116GB of RAM.
The articleby Adam Pauls and Dan Klein (2011) describesan ingenious way to create a data structure thatreduces the amount of RAM needed to load the1T corpus.
However, the system they propose iswritten in Java, a language that is object-oriented, and which, for any object, introduces anadditional overhead.
Furthermore, they do notimplement any smoothing method for the 1Tcorpus, defaulting to the +1 ?stupid smoothing?as they themselves named it, relying on the factthat smoothing is less relevant with a very largecorpus.
For these reasons, coupled with the dif-ficulty to understand and modify other persons?code, we wrote our language model software.We based our implementation around Pauls andKlein?s sorted array idea, with a few modifica-tions.
Firstly, we encoded the unigrams in a sim-ple HashMap instead of a value-rank array.
Sec-ondly, we wrote a multi-step n-gram reader andloader.
Thirdly, we implemented the Jelinek-Mercer smoothing method instead of the simple+1 smoothing.
Using deleted interpolation wecomputed the lambda parameters for the JMsmoothing; we further built a stand-alone serverthat would load the smoothed n-gram probabili-ties and could be queried over TCP-IP either foran n-gram (max 5-gram ?
direct probability) orfor an entire sentence (compute its perplexity).The entire software was written in C++ to avoidJava?s overhead problems.
Overall, the simpli-fied ranked array encoding allowed us to obtainvery fast response times (under a millisecond perquery) with a moderate memory usage: the entire1T corpus was loaded in around 60GB of RAM,well below our development server memory lim-it.45We are aware of the limitations of this corpus:as data was collected from the web, mistakes willoccur in it.We also need a language model that can esti-mate the probability of parts of speech.
By learn-ing a model from the parts of speech we canlearn to discriminate between words differentforms.
Grantedly, a part of speech languagemodel can promote a grammatically ?more?
cor-rect but semantically inferior sentence over asemantically sound one, due to assigning a high-er probability  to a more common part of speechsequence in the sentence.
Our experiments showthat, generally, a part of speech language modelhelps text quality overall.Our initial idea for this POS language modelwas to use the same 1T corpus that we could an-notate using our tagging tools.
However, giventhe limited context, performance would havebeen acceptable at the 5-gram level, decreasingto the point of simply picking the most commonpart of speech for the unigrams, as no contextexists for them.
As such, we used the followingavailable monolingual resources for English: theNews CRAWL corpus (2007-2012 editions),Europarl, UN French-English Corpus, the NewsCommentary, our own cleaned English Wikipe-dia dump.
The total size of the raw text wasaround 20GB.
We joined and annotated the filesand extracted all the 1-5 grams, using the sameformat as the 1T corpus.
We then used anotherinstance of the language model software to loadthis POS LM and await the main system perplex-ity estimation requests.
Overall, the part ofspeech language model turned out to be rathersmall (a hard-disk footprint of only 315MB ofbinary part of speech LM compared to the 57GBof surface model compressed data).
This is nor-mal, as the entire part of speech MSD vocabularyfor English is around 100 tags, compared to themore than 13 million surface forms (unigrams) inthe 1T corpus.3 RACAI?s Hybrid Grammatical ErrorCorrection System3.1 An overview of the systemIn many cases, statistical methods are preferableover rule-based systems since they only rely onlarge available raw corpora instead of hand-crafted rules that are difficult to design and arelimited by the effort invested by human expertsin their endeavor.However, a purely statistical method is notalways able to validate rarely used expressionsand always favors frequency over fine grainedcompositions.As a rule of thumb, hybrid systems are alwaysa good choice in tasks where the complexity ex-ceeds the capacity of converting knowledge intoformal rules and large scale training data isavailable for developing statistical models.Our GEC system has three cascaded phasesdivided between two modules: (a) in the firstphase, a statistical surface based and a POS LMare used to solve orthographic errors inside theinput sentences, thus enhancing the quality of theNLP processing for the second stage; (b) a rule-based system is used to detect typical grammati-cal errors, which are labeled and then (c) correct-ed using a statistical method to validate betweenautomatically generated candidates.3.2 The statistical componentTypos are a distinctive class of errors found intexts written by both native and non-native Eng-lish speakers which do not violate any explicit(local agreement related) grammatical con-straints.
Most POS tagging systems handle pre-viously unseen words through suffix analysis andare able (using the local context) to assign a tagwhich is conformant with the tags of the sur-rounding words.
Such errors cannot be detectedby applying rules, since it is impossible to havelexicons that cover the entire possible vocabularyof a language.The typical approach is to generate spelling al-ternatives for words that are outside the vocabu-lary and to use a LM to determine the most likelycorrect word form.
However, when relying onsimple distance functions such as the unmodifiedLevenstein it is extremely difficult to differenti-ate between spelling alternatives even with thehelp of contextual information.
There are multi-ple causes for this type of errors, starting fromthe lack of language knowledge (typically non-native speakers rely on phonetic similarity whenspelling words) to speed (usually results in miss-ing letters) or keyboard related (multiple keystouched at once).
The distance function we usedfor scoring alternatives uses a weighted Leven-stein algorithm, which was tuned on the TRECdataset.3.3 The rule based error detection and cor-rectionAs previously mentioned, not all grammaticalerrors are automatically detectable by pure statis-tical methods.
In our experiments we noticedfrequent cases where the LM does not provide46sufficient support to distinguish between truegrammatical errors and simply unusual butgrammatically correct expressions.For the present shared task we concentrated ona subset of potential errors.
Our rules aimed thecorrection of the verb tense especially in timeclauses, the use of the short infinitive aftermodals, the position of frequency adverbs in asentence, subject-verb agreement, word order ininterrogative sentences, punctuation accompany-ing certain lexical elements, the use of articles, ofcorrelatives, etc.For the sake of an easier understanding of ourrule-based component of the GEC system, wewill start by introducing some technical detailsabout how the rule interpreter works, emphasiz-ing on the structure of the configuration file, theinput modality and the general pointers on writ-ing rules.
In our approach we treat error detec-tion and error correction separately, in a two-stage system.
The configuration file contains aset of language dependent rules, each rule beinguniquely identified by the label and its body.The role of using labels is two-fold: (1) they pro-vide guidance and assistance to the user in navi-gating through the structure of the configurationfile (when editing or creating new rules); (2) theyplay a crucial role in the error correction processand serve as common denominators for differentclasses of errors.Our rule description system is inspired afterthe time-independent logic function (combina-tional logic) paradigm, which stipulates that afixed input size logical function, describedthrough a stochastic list of input/output depend-ence sequence, through a process of logical min-imization, this function can be implemented asan array of ?AND?
gates, followed by an array of?OR?
gates.
Thus, in our configuration file, eachrule is described by a set of string pairs (i0 r0, i1r1?
in rn) which act as ?AND?
gates ?
we referto this as a sub-instance of a rule.
At this point, asub-instance is activated only if all constraintsare met.
The ?OR?
gate array is simulated byadding rules with the same label.
This way, ifany sub-instance is active then the rule is consid-ered active and we proceed to the errror correc-tion step.Every pair (ik rk) is a single Boolean input of asub-instance.
A rule is checked against everytoken inside an utterance, from left to right.
rk isa regular expression which, depending on thevalue of ik, is applied to the word?s surface form(s), the word?s lemma (l) or the word?s MSD(m).
ik can also select if the regular expressionshould be applied to a neighboring token.
To ex-emplify, we have extracted two sections from ourconfiguration file: (a) the modal infinitive com-mon error for non-native English speakers (alsofound in the development set of CONLL) (lines 1to 7) and (b) the possible missing comma case(line 8):1) modal_infinitive: s must   s+1 to s-1 ^!a2) modal_infinitive: s could  s+1 to3) modal_infinitive: s can    s+1 to4) modal_infinitive: s might  s+1 to5) modal_infinitive: s may    s+1 to6) modal_infinitive: s would  s+1 to7) modal_infinitive: s should s+1 to8) pmc: s which m-1 ^((?!COMMA).
)*$Table 3: a sample of error detection rulesThe ?modal_infinitive?
rule is complex and itis described using 7 sub-instances, which sharean identical label.
Line 1 of the configurationexcerpt contains three pairs as opposed to theother sub-instances.
This does not contradict thecombinational logic paradigm, since we can con-sider this rule as having a fixed input size ofthree and, as a result of logic minimization, thethird parameter for 6 of the seven instances fallsinto the ?DON?T CARE?
special input class.
Thefirst ikrk pair (?s must?)
is used to check if thesurface form (?s?)
of the current word is ?must?.The second pair (?s+1 to?)
checks if the wordform of the next token is ?to?.
The third pair(?s-1 ^!a?)
verifies that the collocation ?a mustto?
does not accidentally trigger this rule.
Thisrule will detect the error in ?I must to go...?, butwill licence a sequence like ?This book is a mustto read...?.The error detection rules that we designed forthe CONLL shared task are created, as an exter-nal resource for the program, on the basis of themistakes observed in the training set and can beupdated/extended any time .In the error correction phase, for every errortype we encompass, we provide the necessarytransformations (at token level) through whichthe initial word sequence that generated this errorshould be corrected.
The configuration file ofthis module is straightforward: rule-labels aremarked as strings at the beginning of a new line;for each label, we provide a set of transformationrules, that are contained in the following tab-indented lines; once a new line does not startwith a TAB character, it should either be emptyor contain the label for a different error type.
thecorrection phase, multiple sentence candidatesare automatically generated (based on the trans-formation rules) and they are checked against the47language model to see which one yields the low-est perplexity.
That is, once an error is found, itscorrection way tends to be applied provided thatthe language model offers another solution.As an example, suppose that the rule for de-tecting a possible missing comma (pmc in Table3, line 8) was fired.
The corresponding correc-tion rule is described as below:pmc:$w-1 , $w$w-1 $wThe "pmc" rule is activated if the word"which" is not preceded by a comma.
Since it isnot always the case that the wordform "which"should be preceded by this punctuation mark, inour error correction system step we generate twocandidates: (a) one in which we insert a commabefore "which" and (b) one in which we keep theword sequence untouched.4 Results and ConclusionsThe RACAI hybrid GEC system obtained a pre-cision of 31.31%, a recall of 14.23% and an F0.5score of 25.25% on the test set provided by theCONLL shared task on Grammatical Error Cor-rection.We presented our system and the resources weused in the development process.
All the dataand tools required to run a similar experiment areavailable online and we are currently working ondeveloping a self-contained GEC system thatwill be made publicly available.Future development plans include the en-hancement of the lexicons we use for Englishand the extension of this system for Romanian.Furthermore, we plan to include an extendedmethod for solving collocations errors based onthe synsets of Princeton WordNet (PWN) (Fell-baum, 1989).ReferencesAndreas Stolcke.
2002.
SRILM: An extensible lan-guage modeling toolkit.
In Proceedings of Inter-speechBoro?, T., Radu, I., & Tufi?, D. (2013).
Large tagsetlabeling with Feed Forward Neural Networks.
Casestudy on Romanian Language.
In Proceedings ofACLBoro?, T., & Dumitrescu, S. D. (2013).
Improving theRACAI Neural Network MSD Tagger.
In Engi-neering Applications of Neural Networks (pp.
42-51).
Springer Berlin HeidelbergCrammer, K., & Singer, Y.
(2003).
Ultraconservativeonline algorithms for multiclass problems.
TheJournal of Machine Learning Research, 3, 951-991.Fellbaum, Ch.
(1998, ed.)
WordNet: An ElectronicLexical Database.
Cambridge, MA: MIT PressHwee Tou Ng, Siew Mei Wu, Ted Briscoe, ChristianHadiwinoto, Raymond Hendy Susanto, and Chris-topher Bryant (2014).
The CoNLL-2014 SharedTask on Grammatical Error Correction.
Proceed-ings of the Eighteenth Conference on Computa-tional Natural Language Learning: Shared Task(CoNLL-2014 Shared Task).
Baltimore, Maryland,USA.Thorsten Brants and Alex Franz.
2006.
GoogleWeb1T 5-gram corpus, version 1.
In Linguistic Da-ta Consortium, Philadelphia, Catalog NumberLDC2006T13Pauls, Adam, and Dan Klein.
"Faster and smaller n-gram language models."
Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies-Volume 1.
Association for Computational Linguis-tics, 2011.48
