LINGUISTIC COHERENCE: A PLAN-BASED ALTERNATIVEDiane J. L i tmanAT&T Bell Laborator ies3C-408A600 Mountain AvenueMurray Hill, NJ 079741ABSTRACTTo fully understand a sequence of utterances, onemust be able to infer implicit relationships betweenthe utterances.
Although the identif ication of sets ofutterance relationships forms the basis for manytheories of discourse, the formalization and recogni-tion of such relationships has proven to be anextremely difficult computational task.This paper presents a plan-based approach to therepresentation and recognition of implicit relation-ships between utterances.
Relationships are formu-lated as discourse plans, which allows their representa-tion in terms of planning operators and their computa-tion via a plan recognition process.
By incorporatingcomplex inferential processes relating utterances intoa plan-based framework,  a formalization and computa-bility not available in the earl ier works is provided.INTRODUCTIONIn order to interpret a sequence of utterancesfully, one must know how the utterances cohere; thatis, one must be able to infer implicit relationships aswell as non-relationships between the utterances.
Con-sider the following fragment, taken from a terminaltranscript between a user and a computer operator(Mann \[12\]):Could you mount a magtape for me?It's tape 1.Such a fragment appears coherent because it is easy toinfer how the second utterance is related to the first.Contrast this with the following fragment:Could you mount a magtape for me?It's snowing like crazy.This sequence appears much less coherent since nowthere is no obvious connection between the two utter-ances.
While one could postulate some connection(e.g., the speaker's magtape contains a database ofplaces to go skiing), more likely one would say thatthere is no relationship between the utterances.
Furth-IThis work was done at the Department of Computer Sci-ence.
University of Rochester.
Rochester NY 14627. and support-ed in part by DARPA under Grant N00014-82-K-0193.
NSF underGrant DCR8351665.
and ONR under Grant N0014-80-C-0197.ermore, because the second utterance violates anexpectation of discourse coherence (Reichman \[16\].Hobbs \[8\], Grosz, Joshi, and Weinstein \[6\]), the utter-ance seems inappropriate since there are no linguisticclues (for example, prefacing the utterance with"incidentally") marking it as a topic change.The identif ication and specif ication of sets oflinguistic relationships between utterances 2 forms thebasis for many computational models of discourse(Reichman \[17\], McKeown \[14\], Mann \[13\], Hobbs \[8\],Cohen \[3\]).
By limiting the relationships al lowed in asystem and the ways in which relationships coherentlyinteract, eff icient mechanisms for understanding andgenerating well organized discourse can be developed.Furthermore,  the approach provides a f ramework forexplaining the use of surface linguistic phenomenasuch as clue words, words like "incidentally" that oftencorrespond to part icular relationships between utter-ances.
Unfortunately.
while these theories proposerelationships that seem intuitive (e.g.
"elaboration," asmight be used in the first fragment above), there hasbeen little agreement on what the set of possible rela-tionships should be, or even if such a set can bedefined.
Furthermore,  since the formalization of therelationships has proven to be an extremely difficulttask, such theories typically have to depend onunrealistic computational processes.
For example.Cohen \[3\] uses an oracle to recognize her "evidence"relationships.
Reichman's \[17\] use of a set of conver-sational moves depends on the future development ofextremely sophisticated semantics modules.
Hobbs \[8\]acknowledges that his theory of coherence relations"may seem to be appealing to magic," since there areseveral places where he appeals to as yet incompletesubtheories.
Finally, Mann \[13\] notes that his theory ofrhetorical predicates is currently descriptive ratherthan constructive.
McKeown's \[14\] implemented sys-tem of rhetorical predicates is a notable exception, butsince her predicates have associated semanticsexpressed in terms of a specific data base system theapproach is not part icularly general.-'Although in some theories relationships hold between groupof utterances, in others between clauses of an utterance, thesedistinctions will not be crucial for the purposes of this paper.215This paper presents a new model for representingand recognizing implicit relationships between utter-ances.
Underlying linguistic relationships are formu-lated as discourse plans in a plan-based theory ofdialogue understanding.
This allows the specificationand formalization of the relationships within a compu-tational framework, and enables a plan recognitionalgorithm to provide the link from the processing ofactual input to the recognition of underlying discourseplans.
Moreover, once a plan recognition systemincorporates knowledge of linguistic relationships, itcan then use the correlations between linguistic rela-tionships and surface linguistic phenomena to guide itsprocessing.
By incorporating domain independentlinguistic results into a plan recognition framework, aformalization and computability generally not avail-able in the earlier works is provided.The next section illustrates the discourse planrepresentation of domain independent knowledgeabout communication as knowledge about the planningprocess itself.
A plan recognition process is thendeveloped to recognize such plans, using linguisticclues, coherence preferences, and constraint satisfac-tion.
Finally, a detailed example of the processing ofa dialogue fragment is presented, illustrating therecognition of various types of relationships betweenutterances.REPRESENTING COHERENCE USING DISCOURSEPLANSIn a plan-based approach to language understand-ing, an utterance is considered understoo~ when it hasbeen related to some underlying plan of the speaker.While previous works have explicitly represented andrecognized the underlying task plans of a givendomain (e.g., mount a tape) (Grosz \[5\], Allen and Per-rault \[1\], Sidner and Israel \[21\].
Carberry \[2\], Sidner\[24\]), the ways that utterances could be related to suchplans were limited and not of particular concern.
As aresult, only dialogues exhibiting a very limited set ofutterance relationships could be understood.In this work, a set of domain-independent plansabout plans (i.e.
meta-plans) called discourse plans areintroduced to explicitly represent, reason about, andgeneralize such relationships.
Discourse plans arerecognized from every utterance and represent planintroduction, plan execution, plan specification, plandebugging, plan abandonment, and so on.
indepen-dently of any domain.
Although discourse plans canrefer to both domain plans or other discourse plans.domain plans can only be accessed and manipulatedvia discourse plans.
For example, in the tape excerptabove "Could you mount a magtape for me?"
achievesa discourse plan to introd,we a domain plan to mount atape.
"It's tape 1" then further specifies this domainplan.Except for the fact that they refer to other plans(i.e.
they take other plans as arguments), the represen-tation of discourse plans is identical to the usualrepresentation of domain plans (Fikes and Nilsson \[4\],Sacerdoti \[18\]).
Every plan has a header, a parameter-ized action description that names the plan.
Actiondescriptions are represented as operators on aplanner's world model and defined in terms of prere-quisites, decompositions, and effects.
Prerequisites areconditions that need to hold (or to be made to hold) inthe world model before the action operator can beapplied.
Effects are statements that are asserted intothe world model after the action has been successfullyexecuted.
Decompositions enable hierarchical plan-ning.
Although the action description of.
the headermay be usefully thought of at one level of abstractionas a single action achieving a goal, such an actionmight not be executable, i.e.
it might be an abstract asopposed to primitive action.
Abstract actions are inactuality composed of primitive actions and possiblyother abstract action descriptions (i.e.
other plans).Finally, associated with each plan is a set of applica-bility conditions called constraintsJ These are similarto prerequisites, except that the planner neverattempts to achieve a constraint if it is false.
The planrecognizer will use such general plan descriptions torecognize the particular plan instantiations underlyingan utterance.HEADER:< "7DECOMPOSITION:EFFECTS:CONSTRAINTS:INTRODUCE-PLAN(speaker.
heareraction, plan)REQUEST(speaker.
hearer, action)WANT(hearer.
plan)NEXT(action.
plan)STEP(action, plan)AGENT(action.
hearer)Figure 1.
INTRODUCE-PLAN.Figures 1, 2, and 3 present examples of discourseplans (see Litman \[10\] for the complete set).
The firstdiscourse plan, INTRODUCE-PLAN,  takes a plan ofthe speaker that involves the hearer and presents it tothe hearer (who is assumed cooperative).
The decom-position specifies a typical way to do this, via execu-tion of the speech act (Searle \[19\]) REQUEST.
Theconstraints use a vocabulary for referring to anddescribing plans and actions to specify that the onlyactions requested will be those that are in the plan andhave the hearer as agent.
Since the hearer is assumedcooperative, he or she will then adopt as a goal the3These constraints hould not be confused with the con-straints of Stefik \[25\].
which are dynamical b formulated uringhierarchical plan generation and represent the interactionsbetween subprobiems.216joint plan containing the action (i.e.
the first effect).The second effect states that the action requested willbe the next action performed in the introduced plan.Note that since INTRODUCE-PLAN has no prere-quisites it can occur in any discourse context, i.e.
itdoes not need to be related to previous plans.INTRODUCE-PLAN thus allows the recognition oftopic changes when a previous topic is completed aswell as recognition of interrupting topic changes (andwhen not linguistically marked as such, ofincoherency) at any point in the dialogue.
It also cap-tures previously implicit knowledge that at the begin-ning of a dialogue an underlying plan needs to berecognized.HEADER:PREREQUISITES:DECOMPOSITION:EFFECT:CONSTRAINTS:CONTINUE-PLAN(speaker, hearer, stepnextstep, lan)LAST(step.
plan)WANT(hearer.
plan)REQUEST(speaker.
hearer, nextstep)NEXT(nextstep.
lan)STEP(step.
plan)STEP(nextstep.
lan)AFTER(step.
nextstep, lan)AGENT(nextstep.
hearer)CANDO(hearer, nextstep)Figure 2.
CONTINUE-PLAN.The discourse plan in Figure 2, CONTINUE-PLAN, takes an already introduced plan as defined bythe WANT prerequisite and moves execution to thenext step, where the previously executed step ismarked by the predicate LAST.
One way of doingthis is to request the hearer to perform the step thatshould occur after the previously executed step,assuming of course that the step is something thehearer actually can perform.
This is captured by thedecomposition together with the constraints.
Asabove, the NEXT effect then updates the portion ofthe plan to be executed.
This discourse plan capturesthe previously implicit relationship of coherent opiccontinuation in task-oriented dialogues (withoutinterruptions), i.e.
the fact that the discourse structurefollows the task structure (Grosz \[5\]).Figure 3 presents CORRECT-PLAN, the lastdiscourse plan to be discussed.
CORRECT-PLANinserts a repair step into a pre-existing plan that wouldotherwise fail.
More specifically, CORRECT-PLANtakes a pre-existing plan having subparts that do notinteract as expected uring execution, and debugs theplan by adding a new goal to restore the expectedinteractions.
The pre-existing plan has subpartslaststep and nextstep, where laststep was supposed toenable the performance of nextstep, but in reality didnot.
The plan is corrected by adding newstep, whichHEADER:PREREQUISITES:DECOMPOSITION-l:DECOMPOSITION-2:EFFECTS:CONSTRAINTS:CORRECT-PLAN(speaker.
hearer,laststep, newstep, nextstep, lan)WANT(hearer, plan)LAST(laststep.
lan)REQUEST(speaker, hearer, newstep)REQUEST(speaker, hearer, nextstep)STEP(newstep.
lan)AFTER(laststep.
newstep, lan)AFTER(newstep.
nextstep, lan)NEXT(newstep.
lan)STEP(laststep.
lan)STEP(nextstep+ lan)AFTER(laststep, nextstep, lan)AGENT(newstep.
hearer)"CANDO(speaker.
nextstep)MODIFIES(newstep, laststep)ENABLES(newstep.
nextstep)Figure 3.
CORRECT-PLAN.enables the performance of nextstep and thus of therest of plan.
The correction can be introduced by aREQUEST for either nextstep or newstep.
Whennextstep is requested, the hearer has to use theknowledge that ne.rtstep cannot currently be per-formed to infer that a correction must be added to theplan.
When newstep is requested, the speaker expli-citly provides the correction.
The effects and con-straints capture the plan situation described above andshould be self-explanatory with the exception of twonew terms.
MODIFIES(action2, actionl) means thataction2 is a variant of action1, for example, the sameaction with different parameters or a new actionachieving the still required effects.ENABLES(action1, action2) means that false prere-quisites of action2 are in the effects of action1.CORRECT-PLAN is an example of a topic interrup-tion that relates to a previous topic,To illustrate how these discourse plans representthe relationships between utterances, consider anaturally-occurring protocol (Sidner \[22\]) in which auser interacts with a person simulating an editing sys-tem to manipulate network structures in a knowledgerepresentation language:1) User: Hi.
Please show the concept Person.2) System: Drawing...OK.3) User: Add a role called hobby.4) System: OK.5) User: Make the vr be Pastime.Assume a typical task plan in this domain is to edit astructure by accessing the structure then performing asequence of editing actions.
The user's first requestthus introduces a plan to edit the concept person.Each successive user utterance continues through theplan by requesting the system to perform the variousediting actions.
More specifically, the first utterancewould correspond to INTRODUCE-PLAN (User, Sys-tem, show the concept Person, edit plan).
Since one of217the effects of INTRODUCE-PLAN is that the systemadopts the plan, the system responds by executing thenext action in the plan, i.e.
by showing the conceptPerson.
The user's next utterance can then be recog-nized as CONTINUE-PLAN (User, System, show theconcept Person, add hobby role to Person.
edit plan),and so on.Now consider two variations of the above dialo-gue.
For example, imagine replacing utterance (5)with the User's "No, leave more room please."
In thiscase, since the system has anticipated the require-ments of future editing actions incorrectly, the usermust interrupt execution of the editing task to correctthe system, i.e.
CORRECT-PLAN(User.
System, addhobby role to Person, compress the concept Person,next edit step, edit plan).
Finally.
imagine that utter-ance (5) is again replaced, this time with "Do youknow if it's time for lunch yet?"
Since eating lunchcannot be related to the previous editing plan topic,the system recognizes the utterance as a total changeof topic, i.e.
INTRODUCE-PLAN(User, System, Sys-tem tell User if time for lunch, eat lunch plan).RECOGNIZING DISCOURSE PLANSThis section presents a computational algorithmfor the recognition of discourse plans.
Recall that theprevious lack of such an algorithm was in fact a majorforce behind the last section's plan-based formaliza-tion of the linguistic relationships.
Previous work inthe area of domain plan recognition (Allen and Per-rault \[1\], Sidner and Israel \[21\].
Carberry \[2\], Sidner\[24\]) provides a partial solution to the recognitionproblem.
For example, since discourse plans arerepresented i entically to domain plans, the same pro-cess of plan recognition can apply to both.
In particu-lar, every plan is recognized by an incremental processof heuristic search.
From an input, the plan recognizertries to find a plan for which the input is a step, 4 andthen tries to find more abstract plans for which thepostulated plan is a step, and so on.
After every stepof this chaining process, a set of heuristics prune thecandidate plan set based on assumptions regardingrational planning behavior.
For example, as in Allenand Perrault \[1\] candidates whose effects are alreadytrue are eliminated, since achieving these plans wouldproduce no change in the state of the world.
As inCarberry \[2\] and Sidner and Israel \[21\] the plan recog-nition process is also incremental; if the heuristicscannot uniquely determine an underlying plan, chain-ing stops.As mentioned above, however, this is not a fullsolution.
Since the plan recognizer is now recognizingdiscourse as well as domain plans from a single utter-ance, the set of recognition processes must be coordi-aPlan chaining can also be done ~ia effects and prerequisites.To keep the example in the next section simple, plans have beennated.
5 An algorithm for coordinating the recognitionof domain and discourse plans from a single utterancehas been presented in Litman and Alien \[9,11\].
Inbrief, the plan recognizer ecognizes a discourse planfrom every utterance, then uses a process of constraintsatisfaction to initiate recognition of the domain andany other discourse plans related to the utterance.Furthermore, to record and monitor execution of thediscourse and domain plans active at any point in adialogue, a dialogue context in the form of a planstack is built and maintained by the plan recognizer.Various models of discourse have argued that an idealinterrupting topic structure follows a stack-like discip-line (Reichman \[17\], Polanyi and Scha \[15\], Grosz andSidner \[7\]).
The plan recognition algorithm will bereviewed when tracing through the example of thenext section.Since discourse plans reflect linguistic relation-ships between utterances, the earlier work on domainplan recognition can also be augmented in severalother ways.
For example, the search process can beconstrained by adding heuristics that prefer discourseplans corresponding to the most linguistically coherentcontinuations of the dialogue.
More specifically, inthe absence of any linguistic clues (as will bedescribed below), the plan recognizer will prefer rela-tionships that, in the following order:(1) continue a previous topic (e.g.
CONTINUE-PLAN)(2) interrupt a topic for a semantically related topic(e.g.
CORRECT-PLAN, other corrections andclarifications as in Litman \[10\])('3) interrupt a topic for a totally unrelated topic (e.g.INTRODUCE-PLAN).Thus, while interruptions are not generally predicted,they can be handled when they do occur.
The heuris-tics also follow the principle of Occam's razor, sincethey are ordered to introduce as few new plans as pos-sible.
If within one of these preferences there are stillcompeting interpretations, the interpretation that mostcorresponds to a stack discipline is preferred. '
Forexample, a continuation resuming a recently inter-rupted topic is preferred to continuation of a topicinterrupted earlier in the conversation.Finally, since the plan recognizer now recognizesimplicit relationships between utterances, linguisticclues signaling such relationships (Grosz \[5\], Reich-man \[17\], Polanyi and Scha \[15\], Sidner \[24\], Cohen\[3\], Grosz and Sidner \[7\]) should be exploitable by theplan recognition algorithm.
In other words, the planrecognizer should be aware of correlations betweenexpressed so that chaining via decompositions is ufficient.5Although Wilensky \[26\] introduced meta-plans into a natur-al language system to handle a totally different issue, that of con-current goal interaction, he does not address details of coordina-tion.218specific words and the discourse plans they typicallysignal.
Clues can then be used both to reinforce aswell as to overrule the preference ordering givenabove.
In fact, in the latter case clues ease the recog-nition of topic relationships that would otherwise bedifficult (if not impossible (Cohen \[3\], Grosz andSidner \[7\], Sidner \[24\])) to understand.
For example,consider recognizing the topic change in the tape vari-ation earlier, repeated below for convenience:Could you mount a magtape for me?It's snowing like crazy.Using the coherence preferences the plan recognizerfirst tries to interpret the second utterance as a con-tinuation of the plan to mount a tape, then as arelated interruption of this plan.
and only when theseefforts fail as an unrelated change of topic.
This isbecause a topic change is least expected in .theunmarked case.
Now, imagine the speaker prefacingthe second utterance with a clue such as "incidentally,"a word typically used to signal topic interruption.Since the plan recognizer knows that "incidentally" isa signal for an interruption, the search will not evenattempt o satisfy the first preference heuristic since asignal for the second or third is explicitly present.EXAMPLEThis section uses the discourse plan representa-tions and plan recognition algorithm of the previoussections to illustrate the processing of the followingdialogue, a slightly modified portion of a scenario(Sidner and Bates \[23\]) developed from the set of pro-tocols described above:User: Show me the generic concept called "employee."System:OK.
<system displays network>User: No, move the concept up.System:OK. <system redisplays network>User: Now, make an individual employee conceptwhose first name is "Sam" and whose lastname is "Jones.
"Although the behavior to be described is fully speci-fied by the theory, the implementation correspondsonly to the new model of plan recognition.
All simu-lated computational processes have been implementedelsewhere, however.
Litman \[10\] contains a full discus-sion of the implementation.Figure 4 presents the relevant domain plans forthis domain, taken from Sidner and Israel \[21\] withminor modifications.
ADD-DATA is a plan to addnew data into a network, while EXAMINE is a planto examine parts of a network.
Both plans involve thesubplan CONSIDER-ASPECT,  in which the user con-siders some aspect of a network, for example by look-ing at it (the decomposition shown), listening to adescription, or thinking about it.The processing begins with a speech act analysisof "Show me the generic concept called 'employee'"HEADER: ADD-DATA(user.
netpiece, data,screenLocation)DECOMPOSITION: CONSIDER-ASPECT(user.
netpiece)PUT(system, data, screenLocation)HEADER: EXAMINE(user.
netpiece)DECOMPOSITION: CONSIDER-ASPECT(user, netpiece)HEADER: CONSIDER-ASPECT(user, netpiece)DECOMPOSITION: DISPLAY(system.
user.
netpiece)Figure 4.
Graphic Editor Domain Plans.REQUEST (user.
system.
DI :DISPLAY (sys-tem, user, El))where E1 stands for "the generic concept called'employee.'"
As in Allen and Perrault \[1\], determina-tion of such a literal 6 speech act is fairly straightfor-ward.
Imperatives indicate REQUESTS and the pro-positional content (e.g.
DISPLAY) is determined viathe standard syntactic and semantic analysis of mostparsers.Since at the beginning of a dialogue there is nodiscourse context, the plan recognizer tries to intro-duce a plan (or plans) according to coherence prefer-ence (3).
Using the plan schemas of the second sec-tion, the REQUEST above, and the process of for-ward chaining via plan decomposition, the system pos-tulates that the utterance is the decomposition ofINTRODUCE-PLAN(  user, system.
Dr, ?plan), whereSTEP(D1, ?plan) and AGENT(D1,  system).
Thehypothesis is then evaluated using the set of planheuristics, e.g.
the effects of the plan must notalready be true and the constraints of every recog-nized plan must be satisfiable.
To "satisfy the STEPconstraint a plan containing D1 will be created.
Noth-ing more needs to be done with respect to the secondconstraint since it is already satisfied.
Finally, sinceINTRODUCE-PLAN is not a step in any other plan,further chaining stops.The system then expands the introduced plan con-taining D1, using an analogous plan recognition pro-cess.
Since the display action could be a step of theCONSIDER-ASPECT plan, which itself could be astep of either the ADD-DATA or EXAMINE plans,the domain plan is ambiguous.
Note that heuristicscan not eliminate either possibility, since at the begin-ning of the dialogue any domain plan is a reasonableexpectation.
Chaining halts at this branch point andsince no more plans are introduced the process of planrecognition also ends.
The final hypothesis is that the6See Litman \[10\] for a discussion of the treatment of indirectspeech acts (Searle \[20\]).219user executed a discourse plan to introduce either thedomain plan ADD-DATA or EXAMINE.Once the plan structures are recognized, theireffects are asserted and the postulated plans areexpanded top down to include any other steps (usingthe information in the plan descriptions).
The planrecognizer then constructs a stack representing eachhypothesis, as shown in Figure 5.
The first stack hasPLAN1 at the top, PLAN2 at the bottom, and encodesthe information that PLAN1 was executed whilePLAN2 will be executed upon completion of PLAN1.The second stack is analogous.
Solid lines representplan recognition inferences due to forward chaining,while dotted lines represent inferences due to laterplan expansion.
As desired, the plan recognizer hasconstructed a plan-based interpretation of the utter-ance in terms of expected discourse and domain plans,an interpretation which can then be used to constructand generate a response.
For example, in eitherhypothesis the system can pop the completed planintroduction and execute D1, the next action in bothdomain plans.
Since the higher level plan containingDI is still ambiguous, deciding exactly what to do is aninteresting plan generation issue.Unfortunately, the system chooses a display thatdoes not allow room for the insertion of a new con-cept, leading to the user's response "No, move the con-cept up."
The utterance is parsed and input to the planrecognizer as the clue word "no" (using the planrecognizer's list of standard linguistic clues) followedby the REQUEST(user,  system, Ml:MOVE(system,El,  up)) (assuming the resolution of "the concept" toEl).
The plan recognition algorithm then proceeds inboth contexts postulated above.
Using the knowledgethat "no" typically does not signal a topic continuation,the plan recognizer first modifies its default mode ofprocessing, i.e.
the assumption that the REQUEST isa CONTINUE-PLAN (preference 1) is overruled.Note, however, that even without such a linguistic cluerecognition of a plan continuation would have ulti-mately failed, since in both stacks CONTINUE-PLAN's constraint STEP(M1, PLAN2/PLAN3) wouldhave failed.
The clue thus allows the system to reachreasonable hypotheses more efficiently, since unlikelyinferences are avoided.Proceeding with preference (2), the system postu-lates that either PLAN2 or PLAN3 is being corrected,i.e., a discourse plan correcting one of the stackedplans is hypothesized.
Since the REQUEST matchesboth decompositions of CORRECT-PLAN,  there aretwo possibilities: CORRECT-PLAN(user ,  system,?laststep, M1, ?nextstep, ?plan), and CORRECT-PLAN(user, system, ?laststep, ?newstep, M1, ?plan),where the variables in each will be bound as a resultof constraint and prerequisite satisfaction from appli-cation of the heuristics.
For example, candidate plansare only reasonable if their prerequisites were true,i.e.
(in both stacks and corrections) WANT(system,'?plan) and LAST(?laststep, ?plan).
Assuming the planwas executed in the context of PLAN2 or PLAN3(after PLAN1 or PLANIa  was popped and theDISPLAY performed), ?plan could only have beenbound to PLAN2 or PLAN3.
and ?laststep bound toDI.
Satisfaction of the constraints eliminates thePLAN3 binding, since the constraints indicate at leasttwo steps in the plan, while PLAN3 contains a singlestep described at different levels of abstraction.
Satis-faction of the constraints also eliminates the secondCORRECT-PLAN interpretation, since STEP( M1.PLAN2) is not true.
Thus only the first correction onthe first stack remains plausible, and in fact, usingPLAN2 and the first correction the rest of the con-straints can be satisfied.
In particular, the bindingsyieldPLAN1 \[completed\]INTRODUCE-PLAN(user ,system ,D1 ,PLAN2)REQUEST(u!er,system.D1)\[LAST\]PLAN2ADD-DATA(user, El, '?data, ?loc)CONSIDER-~EI i '  PUTis';siem.
?d at a,?locDl:DISPLA~(system.user.E 1)\[NEXT\]PLANla \[completed\]\[NTRODUCE-PLAN(user,system.DI.PLAN3)REQUEST(us!r.system.D1)\[LAST\]PLAN3EXAMINE(user,E 1)CONSIDER-AS~ECT(user.E 1)D l:DISPLAY(sys!em.user.E 1)\[NEXT\]Figure 5.
The Two Plan Stacks after the First Utterance.220(1) STEP(D1, PLAN2)(2) STEP(P1, PLAN2)(3) AFTER(D1, P1, PLAN2)(4) AGENT(M1,  system)(5)-CANDO(user,  P1)(6) MODIFIES(M1, D1)(7) ENABLES(M l, Pl)where Pl stands for PUT(system, ?data, ?loc).resulting in the hypothesis CORRECT-PLAN(user .system, D1, M1, Pl, PLAN2).
Note that a final possi-ble hypothesis for the REQUEST,  e.g.
introduction ofa new plan.
is discarded since it does not tie in withany of the expectations (i.e.
a preference (2) choice ispreferred over a preference (3) choice).The effects of CORRECT-PLAN are asserted(M1 is inserted into PLAN2 and marked as NEXT)and CORRECT-PLAN is pushed on to the stacksuspending the plan corrected, as shown in Figure 6.The system has thus recognized not only that aninterruption of ADD-DATA has occurred, but alsothat the relationship of interruption is one of plancorrection.
Note that unlike the first utterance, theplan referred to by the second utterance is found inthe stack rather than constructed.
Using the updatedstack, the system can then pop the completed correc-tion and resume PLAN2 with the new (next) step M1.The system parses the user's next utterance("Now, make an individual employee concept whosefirst name is 'Sam' and whose last names is 'Jones'")and again picks up an initial clue word, this time onethat explicitly marks the utterance as a continuationand thus reinforces coherence preference (1).
Theutterance can indeed be recognized as a continuationof PLAN2, e.g.
CONTINUE-PLAN(  user, system,M1, MAKE1, PLAN2), analogously to the abovedetailed explanations.
M1 and PLAN2 are bound dueto prerequisite satisfaction, and MAKE1 chainedthrough P1 due to constraint satisfaction.
The updatedstack is shown in Figure 7.
At this stage, it would thenbe appropriate for the system to pop the completedCONTINUE plan and resume execution of PLAN2 byperforming MAKEI .PLAN4 \[completed\]C l:CORRECT-PLAN(user,syste rn.D1.M1,P1.PLAN2)REQUEST(user!systern.M 1)\[LAST\]PLAN2CONSIDER- S~CT(user,E1)Dl:DISPLAY/system,user,E 1)\[LAST\]ADD-DATA(user.E 1,?dat a,?loc)\[NEXT\]P l:PUT(sys-Tgm.
?dat a.?ioc)Figure 6.
The Plan Stack after the User's Second Utterance.\[completed\]CONTINUE-PLAN(user,system,M 1, AKE 1.PLAN2)REQUEST(user,sy!tem,MAKE 1)\[LAST\]PLAN2C ON SI DE R-~'P-'E-CT ( u s e r,E 1)Dl:DISPLAYtsystem,user,E 1 )ADD-DATA(user,E 1.SamJones,?loc)~ P )  Pl:PUT(system,SamJones,?loc)\[LAST\] IMAKE1 MAKE \[ , :, (system.user.Sam Jones)\[NEXT\]Figure 7.
Continuation of the Domain Plan.221CONCLUSIONSThis paper has presented a framework for bothrepresenting as well as recognizing relationshipsbetween utterances.
The framework, based on theassumption that people's utterances reflect underlyingplans, reformulates the complex inferential processesrelating utterances within a plan-based theory ofdialogue understanding.
A set of meta-plans calleddiscourse plans were introduced to explicitly formalizeutterance relationships in terms of a small set ofunderlying plan manipulations.
Unlike previousmodels of coherence, the representation was accom-panied by a fully specified model of computationbased on a process of plan recognition.
Constraintsatisfaction is used to coordinate the recognition ofdiscourse plans, domain plans, and their relationships.Linguistic phenomena ssociated with coherence rela-tionships are used to guide the discourse plan recogni-tion process.Although not the focus of this paper, the incor-poration of topic relationships into a plan-basedframework can also be seen as an extension of work inplan recognition.
For example, Sidner \[21,24\]analyzed debuggings (as in the dialogue above) interms of multiple plans underlying a single utterance.As discussed fully in Litman and Allen \[11\], therepresentation and recognition of discourse plans is asystemization and generalization of this approach.Use of even a small set of discourse plans enables theprincipled understanding of previously problematicclasses of dialogues in several task-oriented omains.Ultimately the generality of any plan-based approachdepends on the ability to represent any domain ofdiscourse in terms of a set of underlying plans.Recent work by Grosz and Sidner \[7\] argues for thevalidity of this assumption.ACKNOWLEDGEMENTSI would like to thank Julia Hirschberg, MarciaDerr, Mark Jones, Mark Kahrs, and Henry Kautz fortheir helpful comments on drafts of this paper.REFERENCES1.
J. F. Allen and C. R. Perrault, AnalyzingIntention in Utterances, Artificial Intelligence 15,3 (1980), 143-178.2.
S. Carberry, Tracking User Goals in anInformation-Seeking Environment, AAAI,Washington, D.C., August 1983.59-63.3.
R. Cohen, A Computational Model for theAnalysis of Arguments, Ph.D. Thesis and Tech.Rep.
151, University of Toronto.
October 1983.4.
R .E .
Fikes and N. J. Nilsson, STRIPS: A newApproach to the Application of TheoremProving to Problem Solving, Artificial Intelligence2, 3/4 (1971), 189-208.5.
B .
J .
Grosz, The Representation and Use ofFocus in Dialogue Understanding, TechnicalNote 151, SRI, July 1977.6.
B .
J .
Grosz, A. K. Joshi and S. Weinstein,Providing a Unified Account of Definite NounPhrases in  Discourse.
ACL.
MIT, June 1983, 44-50.7.
B.J .
Grosz and C. L. Sidner, Discourse Structureand the Proper Treatment of Interruptions,IJCAI, Los Angeles, August 1985, 832-839.8.
J .R .
Hobbs, On the Coherence and Structure ofDiscourse, in The Structure of Discourse, L.Polanyi (ed.
), Ablex Publishing Corporation,Forthcoming.
Also CSLI (Stanford) Report No.CSLI-85-37, October 1985.9.
D.J .
Litman and J. F. Allen, A Plan RecognitionModel for Clarification Subdialogues, Coling84,Stanford, July 1984, 302-311.10.
D. J. Litman, Plan Recognition and DiscourseAnalysis: An Integrated Approach forUnderstanding Dialogues, PhD Thesis andTechnical Report 170, University of Rochester,1985.11.
D. J .
Litman and J. F. Allen.
A Plan RecognitionModel for Subdialogues in Conversation,Cognitive Science, , to appear.
, Also Universityof Rochester Tech.
Rep. 141, November 1984.12.
W. Mann, Corpus of Computer OperatorTranscripts, Unpublished Manuscript, ISI, 1970's.13.
W. C. Mann, Discourse Structures for TextGeneration, Coling84, Stanford, July 1984, 367-375.14.
K. R. McKeown, Generating Natural LanguageText in Response to Questions about DatabaseStructure, PhD Thesis, University ofPennsylvania, Philadelphia, 1982.15.
L. Polanyi and R. J. H. Scha, The Syntax ofDiscourse, Text (Special Issue: Formal Methodsof Discourse Analysis) 3, 3 (1983), 261-270.16.
R. Reichman, Conversational Coherency,Cognitive Science 2, 4 (1978), 283-328.17.
R. Reichman-Adar, Extended Person-MachineInterfaces, Artificial Intelligence 22, 2 (1984),157-218.18.
E. D. Sacerdoti, A Structure for Plans andBehavior.
Elsevier, New York, 1977.19.
J. R. Searle, in Speech Acts, an Essay in thePhilosophy of Language, Cambridge UniversityPress, New York, 1969.20.
J .R.
Searle, Indirect Speech Acts, in Speech Acts,vol.
3, P. Cole and Morgan (ed.
), AcademicPress.
New York, NY, 1975.22221.
C. L. Sidner and D. J. Israel.
RecognizingIntended Meaning and Speakers' Plans, IJCAI.Vancouver, 1981, 203-208.22.
C. L. Sidner, Protocols of Users ManipulatingVisually Presented Information with NaturalLanguage, Report 5128.
Bolt Beranek andNewman , September 1982.23.
C. L. Sidner and M. Bates.
Requirements ofNatural Language Understanding in a Systemwith Graphic Displays.
Report Number 5242,Bolt Beranek and Newman Inc.. March 1983.24.
C.L.
Sidner.
Plan Parsing for Intended ResponseRecognition in Discourse, ComputationalIntelligence 1, 1 (February 1985).
1-10.25.
M. Stefik, Planning with Constraints (MOLGEN:Part 1), Artificial Intelligence 16, (1981), 111-140.26.
R. Wilensky, Planning and Understanding.Addison-Wesley Publishing company, Reading,Massachusetts, 1983.223t
