Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1434?1439,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsRandom Walks and Neural Network Language Modelson Knowledge BasesJosu Goikoetxea, Aitor Soroa and Eneko AgirreIXA NLP GroupUniversity of the Basque CountryDonostia, Basque Country{josu.goikoetxea,a.soroa,e.agirre}@ehu.eusAbstractRandom walks over large knowledge baseslike WordNet have been successfully used inword similarity, relatedness and disambigua-tion tasks.
Unfortunately, those algorithmsare relatively slow for large repositories, withsignificant memory footprints.
In this pa-per we present a novel algorithm which en-codes the structure of a knowledge base in acontinuous vector space, combining randomwalks and neural net language models in or-der to produce novel word representations.Evaluation in word relatedness and similar-ity datasets yields equal or better results thanthose of a random walk algorithm, using adense representation (300 dimensions insteadof 117K).
Furthermore, the word representa-tions are complementary to those of the ran-dom walk algorithm and to corpus-based con-tinuous representations, improving the state-of-the-art in the similarity dataset.
Our tech-nique opens up exciting opportunities to com-bine distributional and knowledge-based wordrepresentations.1 IntroductionGraph-based techniques over Knowledge Bases(KB) like WordNet (Fellbaum, 1998) have beenwidely used in NLP tasks, including word sensedisambiguation (Agirre et al, 2014; Moro et al,2014), semantic similarity and semantic relatednessbetween terms (Agirre et al, 2009; Agirre et al,2010; Pilehvar et al, 2013).
For instance, Agirreet al (2009; 2010) apply a random walk algorithmbased on Personalized PageRank to WordNet, pre-Figure 1: Main architecture for generating KB wordembeddings.
A random walk algorithm over theKB produces a synthetic corpus, which is fed into aNNLM to produce continuous word representations.senting the best results to date among WordNet-based methods for the well-known WS353 word-similarity dataset (Finkelstein et al, 2001).
Foreach target word, the method performs a personal-ized random walk on the WordNet graph.
At con-vergence, the target word is represented as a vectorin a multi-dimensional conceptual space, with onedimension for each concept in the KB.
The goodresults of the algorithm contrast with the large di-mensionality of the vectors that it needs to produce,117K dimensions (one per synset) for WordNet.In recent years a wide variety of Neural NetworkLanguage Models (NNLM) have been successfullyemployed in several tasks, including word similarity(Collobert and Weston, 2008; Socher et al, 2011;1434Turian et al, 2010).
NNLM extract meaning fromunlabeled corpora following the distributional hy-pothesis (Harris, 1954), where semantic featuresof a word are related to its co-occurrence patterns.NNLM learn word representations in the form ofdense scalar vectors in n-dimensional spaces (e.g.300 dimensions), in which each dimension is a la-tent semantic feature.
The representations are ob-tained by optimizing the likelihood of existing un-labeled text.
More recently, Mikolov et al havedeveloped simpler NNLM architectures (Mikolovet al, 2013a; Mikolov et al, 2013b; Mikolov etal., 2013c), which drastically reduced computationalcomplexity by deleting the hidden layer, enablingto compute accurate word representations from verylarge corpora.
The representations obtained by thesemethods are compact, taking 1.5G for 3M words on300-dimensional space, and have been shown to out-perform other distributional corpus-based methodson several tasks, including the WS353 word similar-ity dataset (Baroni et al, 2014).In this work we propose to encode the meaningof words using the structural information in knowl-edge bases.
That is, instead of modeling the mean-ing based on the co-occurrences of words in corpora,we model the meaning based on random walks overthe knowledge base.
Each random walk is seen as acontext for words in the vocabulary, and fed into theNNLM architecture, which optimizes the likelihoodof those contexts (cf.
Fig.
1).
The resulting wordrepresentations are more compact than those pro-duced by regular random walk algorithms (300 vs.tens of thousands), and produce very good resultson two well-known benchmarks on word related-ness and similarity: WS353 (Finkelstein et al, 2001)and SL999 (Hill et al, 2014b), respectively.
Wealso show that the obtained representations are com-plementary to those of random walks alone and todistributional representations obtained by the sameNNLM algorithm, improving the results.Some recent work has explored embedding KBsin low-dimensional continous vector spaces, repre-senting each entity in a k-dimensional vector andcharacterizing typed relations between entities in theKB (e.g.
born-in-city in Freebase or part-of in Word-Net) as operations in the k-dimensional space (Wanget al, 2014).
The model estimates the parameterswhich maximize the likelihood of the triples, whichcan then be used to infer new typed relations whichare missing in the KB.
In contrast, we use the rela-tions to explicitly model the context of words, in twocomplementary approaches to embed information inKBs into continuous spaces.2 NNLMNeural Network Language Models have become auseful tool in NLP on the last years, specially in se-mantics.
We have used the two models proposedin (Mikolov et al, 2013c) due to their simplicityand effectiveness in word similarity and related-ness tasks (Baroni et al, 2014): Continuous Bagof Words (CBOW) and Skip-gram.
The first oneis quite similar to the feedforward Neural NetworkLanguage Model, but instead of a hidden layer it hasa projection layer, and thus all the words are pro-jected in the same position.
Word order has thusno influence in the projection.
The training crite-rion is as follows: knowing previous and subsequentwords in context, the model maximizes the proba-bility of the predicting the word in the middle.
TheSkip-gram model uses each current word as an inputto a log-linear classifier with a continuous projec-tion layer, and predicts the previous and subsequentwords in a context window.Although the Skip-gram model seems to be moreaccurate in most of the semantic tasks, we have usedboth variants in our experiments.
We used a publiclyavailable implementation1.3 Random Walks and NNLMOur method performs random walks over KB graphsto create synthetic contexts which are fed into theNNLM architecture, creating novel word represen-tations.
The algorithm used for creating the contextsis a Monte Carlo method for computing the PageR-ank algorithm (Avrachenkov et al, 2007).We consider a KB as undirected graph G =(V,E), where V is the set of concepts and E rep-resents links among concepts.
We also need a dic-tionary, an association from words to KB concepts.We construct an inverse dictionary that maps graphvertices with the words than can be linked to it.The inputs of the algorithm are: 1) the graph G =(V,E), 2) the inverse dictionary and 3) the damp-1https://code.google.com/p/word2vec/1435Figure 2: Spearman results on relatedness (WS353)for different corpus sizes (in sentences).ing factor ?2.
In our experiments we used Word-Net 3.0 with gloss relations3, which has 117.522nodes (synsets) and 525.356 edges (semantic rela-tions).
Regarding the dictionary, WordNet aleadycontains links from words to concepts.
The dictio-nary includes the probability of a concept being lexi-calized by a specific word, as estimated by the Word-Net team from their hand-annotated corpora.
Bothdictionary and graph are freely available4.The method first chooses a vertex at random fromthe vertex set V , and performs a random walk start-ing from it.
At each step, the random walk mightterminate with probability (1??)
or choose a neigh-bor vertex at random with probability ?.
Each timethe random walk reaches a vertex, a word is emittedat random using the probabilities in the inverse dic-tionary.
When the random walk terminates, the se-quence of emitted words forms the pseudo sentencewhich is fed to the NNLM architecture, and the pro-cess starts again choosing a vertex at random untila maximum number of pseudo sentences have beengenerated.Our method creates pseudo sentences like the fol-lowing:2The damping factor is the only parameter of PageRank.3http://wordnet.princeton.edu/glosstag.shtml4http://ixa2.si.ehu.eus/ukbFigure 3: Spearman results on similarity (SL999)for different corpus sizes (in sentences).
(1) amphora wine nebuchadnezzar bear retainlong(2) graphology writer write scribble scrawlerheedlessly in haste jot note notebookThese examples give us clues of the kind of theimplicit semantic information that is encoded in thegenerated pseudo-corpus.
Example 1 starts with am-phora following with wine (with which amphorasare usually filled with), nebuchadnezzar (a partic-ular bottle size) and finishing with words that arerelated to wine storage, like bear,retain and long.Example 2 shows a similar phenomenom; it startswith graphology, follows with the closely relatedwriter, then writer, finishing with names and adjec-tives of different variants of writing, such as scrib-ble, scrawler, heedlessly, in haste and jot; finally,the context ends with note and notebook.
Notethat our method also produces multiword terms likein haste.4 ExperimentsWe have trained two Neural Network models,CBOW and Skip-gram, with several iterations ofrandom walks over WordNet.
We trained both mod-els with default parameters (Mikolov et al, 2013a):vector size 300, 3 iterations, 5 negative samples, and1436SL999 WS353Skip-gram 0.442 0.686RWSGRAM 0.520 0.683RWCBOW 0.486 0.591PPV 0.493 0.683Table 1: Spearman correlation results for our meth-ods (RWSGRAM, RWCBOW) on WordNet randomwalks, compared to just random walks (PPV), andSkip-gram on text corpora.window size 5.
In order to check how many iter-ations of the random walk algorithm are needed tolearn good word representations, we produced up to70 ?
106contexts.
The the damping factor (?)
of therandom walk algorithm was set to 0.85, a usual value(Agirre et al, 2010).
All parameters were thus set todefault, and we only explored different corpus sizes.The word representations were evaluated onWS353 (Finkelstein et al, 2001) and SL999 (Hillet al, 2014b), two datasets on word relatedness andword similarity, respectively.
In order to computethe similarity of two words, it suffices to calculatethe cosine between the respective word representa-tions.
The evaluation measure computes the rankcorrelation (Spearman) between the human judg-ments and the system values.In order to contrast our results with the two relatedtechniques, we used UKB5, a publicly available im-plementation of Personalized PageRank (Agirre etal., 2014), and ran it over the same graph as ourproposed methods.
We used it out-of-the-box witha damping value of 0.85.
We also downloaded theembeddings learnt by (Mikolov et al, 2013a) usingSkip-gram over a large text corpus6.
We used thesame cosine algorithm to compute similarity withall word representations.
To distinguish one wordrepresentation from the other, we will call our mod-els RWCBOW and RWSGRAM respectively (RW forrandom-walk), in contrast to the original Person-alized PageRank algorithm (PPV) and the corpus-based embeddings learned using Skip-grams (Skip-gram).Figures 2 and 3 show the learning curves on theWS353 and SL999 datasets relative to the number5http://ixa2.si.ehu.eus6https://code.google.com/p/word2vec/SL999 WS353(a) RWSGRAM 0.518 0.683(b) PPV 0.493 0.683(c) Skip-gram 0.442 0.686(a+b) 0.535 0.700(a+c) 0.533 0.748(a+b+c) 0.552 0.759Best 0.520 0.800Table 2: Combinations and best published results:SL999 (Hill et al, 2014a), WS353 (Radinsky et al,2011).of contexts produced by the random walks on Word-Net.
The results show that WordNet representa-tions grow quickly (around 7 million contexts), con-verging around 70M, obtaining practically the sameresults as PPV for WS353, and better results forSL9997.The results at convergence are shown in Table 1,together with those of PPV and Skip-gram.
Regard-ing SL999, we can see that the best results are ob-tained with RWSGRAM, improving over PPV andSkip-gram.
Regarding WS353, all methods exceptRWSGRAM obtain similar results.
The results showthat our methods are able to effectively capture theinformation in WordNet, performing on par to theoriginal PPV algorithm, and better than the corpus-based Skip-gram on the SL999 dataset.
Note thatthe best published results for WS353 using WordNetare those of (Agirre et al, 2010) using PPV, whichreport 0.685.In order to see if the word representations that welearn are complementary to those of PPV and Skip-gram, we combined the scores produced by eachword representation.
Given the potentially differentscales of the similarity values, we assigned to eachitem the average of the ranks of the pair in each out-put.
The top part of Table 2 repeats the three rel-evant systems.
The (a+b) row reports an improve-ment in both datasets, showing that RWSGRAM onWordNet is complementary to PPV in WordNet, andis thus a different representation, even if both use thesame knowledge base.
The (a+b) and (a+b+c) showthat corpus-based Skip-grams are also complemen-7We tried larger context sizes, up to 700M confirming thatconvergence was around 70M.1437tary, yielding incremental improvements.
In fact, thecombination of all three improves over the best pub-lished results on SL999, and approaches the best re-sults for WS353, as shown in the last row of the Ta-ble.
The state of the art on SL999 corresponds to(Hill et al, 2014a), who training a Recurrent Neu-ral Net model on bilingual text.
The best resultson WS353 correspond to (Radinsky et al, 2011),who combine a Wikipedia-based algorithm with acorpus-based method which uses date-related infor-mation from news to learn word representations.Note that we have only performed some simplecombination to show the complementarity of eachinformation source.
More sophisticated combina-tions (e.g.
learning a regression model) could furtherimprove results.We have performed some qualitative analysis,which indicates that there is a slight tendency forcorpus embeddings (with the window size usedin the experiments) to group related words (e.g.physics - proton), and not so much similar words(e.g.
vodka - gin), while our KB embeddings in-clude both.
This analysis agrees with the results inTable 1, where all KB results are better than corpus-based Skip-gram for the semantic similarity dataset(SL999).
In passing, note that the best published re-sults to date on similarity (Hill et al, 2014a) use em-beddings learnt from bilingual text which suggeststhat bilingual corpora are better suited to learn em-beddings capturing semantic similarity.5 ConclusionsWe have presented a novel algorithm which encodesthe structure of a knowledge base in a continuousvector space, combining random walks and neuralnet language models to produce new word represen-tations.
Our evaluation in word relatedness and sim-ilarity datasets has shown that these new word repre-sentations attain similar results to those of the orig-inal random walk algorithm, using 300 dimensionsinstead of tens of thousands.
Furthermore, the wordrepresentations are complementary to those of therandom walk algorithm and to corpus-based contin-uous representations, producing better results whencombined, and improving the state-of-the-art in thesimilarity dataset.
Hand inspection reinforces theobservation that WordNet-basedA promising direction of this research is to lever-age multilingual Wordnets to produce cross-lingualembeddings.On another direction, one of the main limitationsof KB approaches is that they produce a relativelysmall number of embeddings, limited by the size ofthe dictionary.
In the future we want to overcomethis sparsity problem by combining both textual andKB based embeddings into a unified model.
In fact,we think that our technique opens up exciting oppor-tunities to combine distributional and knowledge-based word representations.It would also be interesting to investigate the in-fluence of the different semantic relations in Word-Net, either by removing certain relations or by as-signing different weights to them.
This investiga-tion could give us deeper insights about the way ourknowledge-based approach codes meaning in vectorspaces.AcknowledgementsThis work was partially funded by MINECO(CHIST-ERA READERS project ?
PCIN-2013-002-C02-01, and SKaTeR project ?
TIN2012-38584-C06-02), and the European Commission(QTLEAP ?
FP7-ICT-2013.4.1-610516).
The IXAgroup is funded by the Basque Government (A typeResearch Group).ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and WordNet-based approaches.
In Proceedingsof Human Language Technologies: The 2009 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 19?27.Association for Computational Linguistics.Eneko Agirre, Montse Cuadros, German Rigau, and AitorSoroa.
2010.
Exploring Knowledge Bases for Simi-larity.
In LREC.Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.2014.
Random walks for knowledge-based wordsense disambiguation.
Computational Linguistics,40(1):57?84.K.
Avrachenkov, N. Litvak, D. Nemirovsky, and N. Os-ipova.
2007.
Monte carlo methods in pagerank com-putation: When one iteration is sufficient.
SIAM J.Numer.
Anal., 45(2):890?904.1438Marco Baroni, Georgiana Dinu, and Germ?an Kruszewski.2014.
Dont count, predict!
a systematic comparison ofcontext-counting vs. context-predicting semantic vec-tors.
In Proceedings of the 52nd Annual Meeting of theAssociation for Computational Linguistics, volume 1.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof the 25th international conference onMachine learn-ing, pages 160?167.
ACM.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2001.
Placing search in context: The con-cept revisited.
In Proceedings of the 10th internationalconference on World Wide Web, pages 406?414.
ACM.Zellig S Harris.
1954.
Distributional structure.
Word.Felix Hill, KyungHyun Cho, S?ebastien Jean, ColineDevin, and Yoshua Bengio.
2014a.
Not all neural em-beddings are born equal.
CoRR, abs/1410.0718.Felix Hill, Roi Reichart, and Anna Korhonen.
2014b.Simlex-999: Evaluating semantic models with(genuine) similarity estimation.
arXiv preprintarXiv:1408.3456.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word representa-tions in vector space.
arXiv preprint arXiv:1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed represen-tations of words and phrases and their composition-ality.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013c.
Linguistic regularities in continuous spaceword representations.
In Proceedings of NAACL-HLT,pages 746?751.Andrea Moro, Alessandro Raganato, and Roberto Nav-igli.
2014.
Entity linking meets word sense dis-ambiguation: a unied approach.
Transactions of theAssociation of Computational Linguistics, 2:231?244,May.Mohammad Taher Pilehvar, David Jurgens, and RobertoNavigli.
2013.
Align, Disambiguate and Walk: aUnified Approach for Measuring Semantic Similarity.In Proceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics, pages 1341?1351, Sofia, Bulgaria.Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich,and Shaul Markovitch.
2011.
A word at a time: com-puting word relatedness using temporal semantic anal-ysis.
In Proceedings of the 20th international confer-ence on World wide web, WWW ?11, pages 337?346,New York, NY, USA.
ACM.Richard Socher, Jeffrey Pennington, Eric H Huang, An-drew Y Ng, and Christopher D Manning.
2011.
Semi-supervised recursive autoencoders for predicting sen-timent distributions.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, pages 151?161.
Association for ComputationalLinguistics.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 384?394.
Association for Computa-tional Linguistics.Zhen Wang, Jianwen Zhang, Jianlin Feng, and ZhengChen.
2014.
Knowledge graph and text jointly em-bedding.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 1591?1601, Doha, Qatar, October.Association for Computational Linguistics.1439
