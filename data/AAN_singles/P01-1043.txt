A language?independent shallow?parser CompilerAlexandra KinyonCIS Dpt.
.University of Pennsylvaniakinyon@linc.cis.upenn.eduhttp://www.cis.upenn.edu/?kinyonAbstractWe present a rule?based shallow?parser compiler, which allows togenerate a robust shallow?parser forany language, even in the absence oftraining data, by resorting to a verylimited number of rules which aim atidentifying constituent boundaries.We contrast our approach to otherapproaches used for shallow?parsing(i.e.
finite?state and probabilisticmethods).
We present an evaluationof our tool for English (PennTreebank) and for French (newspapercorpus "LeMonde") for several tasks(NP?chunking & "deeper" parsing) .1 IntroductionFull syntactic parsers of unrestricted text arecostly to develop, costly to run and often yielderrors, because of lack of robustness of wide?coverage grammars and problems of attachment.This has led, as early as 1958 (Joshi & Hopely97), to the development of shallow?parsers,which aim at identifying as quickly andaccurately as possible, main constituents  (andpossibly syntactic functions) in an input,without dealing with the most difficult problemsencountered with "full?parsing".
Hence,shallow?parsers are very practical tools.
Thereare two main techniques used to developshallow?parsers:1?
Probabilistic techniques (e.g.
Magerman94, Ratnaparkhi 97, Daelmans & al.
99)2?
Finite?state techniques (e.g.
Grefenstette 96)Probabilistic techniques require large amountsof syntactically?annotated training data1, whichmakes them very unsuitable for languages for1 We are leaving aside unsupervised learning techniqueshere, since to our knowledge they have not proved asuccessful  for  developing practical shallow?parsers.which no such data is available (i.e.
mostlanguages except English) and also, they are notdomain?independent nor "style?independent"(e.g.
they do not allow to successfully shallow?parse speech, if no annotated data is availablefor that ?style?).
Finally, a shallow?parserdeveloped using these techniques will have tomirror the information contained in the trainingdata.
For instance, if one trains such a tool ondata were only non recursive NP chunks aremarked2, then one will not be able to obtainricher information such as chunks of othercategories, embeddings, syntactic functions...On the other hand, finite?state techniques relyon the development of a large set of rules (oftenbased on regular expressions) to capture all theways a constituent can expend.
So for examplefor detecting English NPs, one could write thefollowing rules :NP ?
Det adj* noun adj*NP ?
Det adj         (for noun ellipsis)NP ?
ProperNoun etc ....But this is time consuming and difficult sinceone needs to foresee all possible rewriting cases,and if some rule is forgotten, or if too manyPOS errors are left, robustness and/or accuracywill suffer.Then these regular expressions have to bemanipulated i.e.
transformed into automata,which will be determinized and minimized(both being costly operations).
And even thoughdeterminization and minimization must be doneonly once (in theory) for a given set of rules, itis still costly to port such tools to a new set ofrules (e.g.
for a new language, a new domain) orto change some existing rules.In this paper, we argue that in order toaccomplish the same task, it is unnecessary todevelop full sets of regular expression : instead2See (Abney 91) for the definition of a chunk.of specifying all the ways a constituent can berewritten, it is sufficient to express how itbegins and/or ends.
This allows to achievesimilar results but with far fewer rules, andwithout a need for determinization orminimization because rules which are writtenthat way are de?facto deterministic.
So in asense, our approach bears some similarities withthe constraint?based formalism because weresort to ?local rules?
(Karlsson & al.
95), butwe focus on identifying constituent boundaries(and not syntactic functions), and allow anylevel of embedding thanks to the use of a stack.In the first part of this paper, we present ourtool: a shallow?parser compiler.
In a secondpart, we present output samples as well asseveral evaluations  for French and for English,where the tool has been used to develop both anNP?chunker and a richer shallow?parser.
Wealso explain why our approach is more tolerantto POS?tagging errors.
Finally, we discusssome other practical uses which are made of thisshallow?parser compiler.2   Presentation of the compilerOur tool has been developed using JavaCC (acompiler compiler similar to Lex & Yacc, butfor java).
The program takes as input a filecontaining rules.
These rules aim at identifyingconstituent boundaries for a given language.
(For example for English, one such rule couldsay  "When encountering a preposition, start aPP"), either by relying on function words, or onmorphological information (e.g.
gender) if it isappropriate for the language which is beingconsidered.These rule files specify :?
A mapping between the "abstract" morpho?syntactic tags, used in the rules, and "real"morpho?syntactic tags as they will appearin the input.?
A declaration of the syntactic constituentswhich will be detected (e.g.
NP, VP, PP ...)?
A set of unordered rulesFrom this rule file, the compiler generates a javaprogram, which is a shallow?parser based onthe rule file.
One can then run this shallow?parser on an input to obtain a shallow?parsedtext3.The compiler itself is quite simple, but we havedecided to compile the rules rather than interpretthem essentially for efficiency reasons.
Also, it3 The input is generally POS?tagged, although this is notan intrinsic requirement of the compiler.is language independent since a rule file may bewritten for any given language, and compiledinto a shallow?parser for that language.Each rule is of the form:{Preamble} disjunction of patterns then actions2.1 A concrete example : compiling asimple NP?chunker for EnglishIn this section we present a very simple "toy"example which aims at identifying some NPs inthe Penn Treebank4 (Marcus & al 93).In order to do so, we write a rule file, shown onfigure 1.
The top of the file declares a mappingbetween the abstract tagset we use in our rules,and the tagset of the PennTreebank.
Forexample commonN corresponds to the 3 tags NN,NNS, NNPS in the PennTreebank.
It thendeclares the labels of the constituents which willbe detected (here there is only one: NP).
Finally,it declares 3 rules.%% A small NP?chunker for the Penn?treebanktagmap <QuantityAdv:any,some,many>;tagmap<ProperN:NNP>;tagmap<det:DT,PDT>;tagmap<commonN:NN,NNS,NNPS>;tagmap<DemPro:D*>;tagmap<Adj:JJ*>;tagmap<OtherTags:V*,P,C*,RB*.,:,,>;label NP;%% rule 1{} (:$det) | ($QuantityAdv:) | (:$DemPro) thenclose(),open(NP);%% rule 2{!NP} (:$commonN) | (:$Adj) | (:$ProperN) thenclose(),open(NP);%% rule 3{} (:$OtherTags) then close();FIGURE 1 : An example of a  rule?fileRule 1 says that when a determiner, a quantityadverb or a demonstrative pronoun isencountered, the current constituent must beclosed, and an NP must be opened.
Rule 2 saysthat, when not inside an NP, if a common noun,an adjective or a proper noun is encountered,then the current constituent should be closed andan NP should be opened.
Finally, Rule 3 saysthat when some other tag is encountered (i.e.
averb, a preposition, a punctuation, a conjunction4 This example is kept very simple for sake of clarity.
Itdoes not aim at yielding a very accurate result.or  an adverb) then the current constituentshould be closed.This rule file is then compiled into an NP?chunker.
If one inputs  (a) to the NP?chunker, itwill then output (b)(a) The/DT cat/NNS eats/VBZ the/DTmouse/NNS ./.
(b) <NP> The/DT cat/NNS </NP> eats/VBZ <NP>the/DT mouse/NNS </NP> ./.In our compiler, rules access  a limited context :?
The constituent(s) being built?
The previous form and POS?
The current form and POS?
The next form and POSSo contrary to standard finite?state techniques,only constituent boundaries are explicited, andit is not necessary (or even possible) to specifyall the possible ways a constituent may berealized .As shown in section 3, this reduces greatly thenumber of rules in the system (from severaldozens to less than 60 for a wide?coverageshallow?parser).
Also, focussing only onconstituent boundaries  ensures determinism :there is no need for determinizing norminimizing the automata we obtain from ourrules.Our tool is robust : it never fails to provide anoutput and can be used to create a parser for anytext from any domain in any language.It is also important to note that the parsing isdone incrementally : the input is scanned strictlyfrom left to right, in one single pass.
And foreach pattern matched, the associated actions aretaken (i.e.
constituent boundaries are added).Since there is no backtracking, this allows anoutput in linear time.
If several patterns match,the longest one is applied.
Hence our rules aredeclarative and unordered.
Although in theoryconflicts could appear between 2 patterns ofsame length (as shown in (c1) and (c2)), this hasnever happened in practice.
Of course the case isnonetheless dealt with in the implementation,and a warning is then issued to the user.
(c1)  {} (:a) (:b) then close();(c2) {} (:a) (:b) then open(X);As is seen on figure 1, one can writedisjunctions of patterns for a given rule.In this very simple example, only non recursiveNP?chunks are marked, by choice.
But it is notan intrinsic limitation of the tool, since anyamount of embedding can be obtained (asshown in section 3 below), through the use of aStack.
From a formal point of view, our tool hasthe power of a deterministic push?downautomaton.When there is a match between the input and thepattern in a rule, the following actions may betaken :?
close(): closes the constituent last opened byinserting </X> in the output, were X is thesyntactic label at the top of the Stack.?
open(X): opens a new constituent byinserting label <X> in the output?
closeWhenOpen(X,Y):  delays the closingof constituent labeled X, until constituentlabeled Y is opened.?
closeWhenClose(X,Y): delays the closing ofconstituent labeled X, until constituentlabeled Y is closed.?
doNothing(): used to "neutralize" a shortermatch.Examples for the actions open() and close() wereprovided on figure 1.
The actionscloseWhenOpen(X,Y) and closeWhenClose(X,Y)allow to perform some attachments.
Forexample a rule for English could say :{NP} (:$conjCoord) then close(), open(NPcoord),closeWhenClose(NPcoord,NP);This rule says that  when inside an NP, acoordinating conjunction is encountered, aNPcoord should be opened, and should beclosed only when the next NP to the right willbe closed.
This allows, for example, to obtainoutput (d) for a coordination of NPs5.
(d) John likes<NP>Apples</NP><NPcoord> and<NP> green beans </NP></NPcoord>An example of the action doNothing() forEnglish could be:{} (:$prep) then open(PP);{} P(:$prep) (:$prep) then doNothing()  ;The first rule says that when a preposition isencountered, a PP should be opened.
The secondrule says that when a preposition is encountered,if the previous tag was also a preposition,nothing should be done.
Since the pattern for5This is shown as an example as to how this action can beused, it does not aim at imposing this structure tocoordinations, which could be dealt with differently usingother rules.rule 2 is longer than the pattern for rule 1, it willapply when the second preposition in a row isencountered, hence "neutralizing" rule 1.
Thisallows to obtain "flatter" structures for PPs,such as the one in (e1).
Without this rule, onewould obtain the structure in (e2) for the sameinput.
(e1) This costs <PP> up  to 1000 $ </PP>(e2) This costs<PP> up<PP> to 1000 $ </PP></PP>3.
Some "real world" applicationsIn this section, we present some uses whichhave been made of this Shallow?Parsercompiler.
First we explain how the tool has beenused to develop a 1 million word Treebank forFrench, along with an evaluation.
Then wepresent an evaluation for English.It is well known that evaluating a Parser is adifficult task, and this is even more true forShallow?Parsers, because there is no realstandard task (some Shallow?parsers haveembedded constituents, some encode syntacticfunctions, some encode constituent information,some others dependencies or even a mixture ofthe 2) There also isn?t standard evaluationmeasures for such tools.
To perform evaluation,one can compare the output of the parser to awell?established Treebank developedindependently (assuming one is available for thelanguage considered), but the result is unfair tothe parser because generally in Treebanks allconstituents are attached.
One can also comparethe output of the parser to a piece of text whichhas been manually annotated just for thepurpose of the evaluation.
But then it is difficultto ensure an objective measure (esp.
if theperson developing the parser and the persondoing the annotation are the same).
Finally, onecan automatically extract, from a well?established Treebank, information that isrelevant to a given , widely agreed on, nonambiguous task such as identifying bare non?recursive NP?chunks, and compare the outputof the parser for that task to the extractedinformation.
But this yields an evaluation that isvalid only for this particular task and may notwell reflect the overall performance of theparser.
In what follows, in order to be asobjective as possible, we use these 3 types ofevaluation, both for French and for English6, anduse standard measures of recall and precision.Please bear in mind though that these metricmeasures, although very fashionable, have theirlimits7.
Our goal is not to show that our tool isthe one which provides the best results whencompared to other shallow?parsers, but rather toshow that it obtains similar results, although in amuch simpler way, with a limited number ofrules compared to finite?state techniques andmore tolerance to POS errors,  and even in theabsence of available training data (i.e.
caseswere probabilistic techniques could not beused).
To achieve this goal, we also presentsamples of parsed outputs we obtain, so that thereader may judge for himself/herself.3.1.
A shallow?parser for FrenchWe used our compiler to create a shallow?parserfor French.
Contrary to English, very fewshallow?parsers exist for French, and noTreebank actually exist to train a probabilisticparser (although one is currently being builtusing our tool c.f.
(Abeill?
& al.
00)).Concerning shallow?parsers, one can mention(Bourigault 92) who aims at isolating NPsrepresenting technical terms, whereas we wishto have information on other constituents aswell, and (Ait?Moktar & Chanod 97)  whosetool is not publicly available.
One can alsomention (Vergne 99), who developed a parserfor French which also successfully relies onfunction words to identify constituentboundaries.
But contrary to us, his tool does notembed constituents8.
And it is also not publiclyavailable.In order to develop a set of rules for French, wehad to examine the linguistic characteristics ofthis language.
It turns out that although Frenchhas a richer morphology than English (e.g.gender for nouns, marked tense for verbs), mostconstituents are nonetheless triggered by theoccurrence of a function word.
Following thelinguistic tradition, we consider as functionwords all words associated to a POS whichlabels a closed?class i.e.
: determiners,prepositions, clitics, auxiliaries, pronouns(relative, demonstrative), conjunctions6 Of course, manual annotation was done by a differentperson than the one who developed the rules.7 For instance in a rule?based system, performance mayoften be increased by adding more rules.8 Instead, it identifies chunks and then assigns somesyntactic functions to these chunks.
(subordinating, coordinating), auxiliaries,punctutation marks and adverbs belonging to aclosed class (e.g.
negation adverbs "ne" "pas")9.The presence of function words makes thedetection of the beginning of a constituent rathereasy.
For instance, contrary to English,subordinating conjunctions (que/that) are neveromitted when a subordinating clause starts.Similarly,  determiners are rarely omitted at thebeginning of an NP.Our aim was to develop a shallow?parser whichdealt with some embedding, but did not committo attach potentially ambiguous phrases such asPPs and verb complements.
We wanted toidentify the following constituents : NP, PP, VN(verbal nucleus), VNinf (infinitivals introducedby a preposition), COORD (for coordination),SUB (sentential complements), REL (relativeclauses), SENT (sentence boundaries), INC (forconstituents of unknown category), AdvP(adverbial phrases).We wanted NPs to include all adjectives but notother postnominal  modifiers (i.e.
postposedrelative clauses and PPs), in order to obtain astructure similar to (f).
(f) <NP> Le beau livre bleu </NP><PP> de  <NP>ma cousine</NP> </PP> ?
(my cousin?s beautiful blue book)Relative clauses also proved easy to identifysince they begin when a relative pronoun isencountered.
The ending of clauses occursessentially when a punctuation mark or aconjunction of coordination is encountered orwhen another clause begins, or when a sentenceends (g1) .
These rules for closing clauses workfairly well in practice (see evaluation below) butcould be further refined, since they will yield awrong closing boundary for the relative in asentence like  (g2)(g1) <SENT> <NP> Jean </NP><VN> voit</VN><NP>la femme </NP><REL> qui<VN> pense </VN><SUB> que<NP> Paul </NP><VN> viendra </VN></SUB></REL>  .
</SENT>(John sees the woman who thinks that Paul willcome)9 Considering punctuation marks as function words maybe "extending" the linguistic tradition.
Nonetheless, it is aclosed class, since there is a small finite number ofpunctuation marks.
(g2) * <SENT> <NP> Jean </NP><VN> pense</VN><SUB> que<NP> la femme </NP><REL> que<NP> Pierre </NP><VN> voit</VN><VN> aime </VN><NP>  Donald </NP></REL></SUB>  .
</SENT>(*John thinks that the woman [REL that Peter seeslikes Donald])Concerning clitics, we have decided to groupthem with the verb (h1) even when dealing withsubject clitics (h2).
One motivation is thepossible inversion of the subject clitic (h3).
(h1)  <SENT><NP> JEAN </NP><VN> le lui donne</VN> .
</SENT>(J. gives it to him).
(h2) <SENT> <VN> Il le voit </VN>  .
</SENT>(He sees it)(h3) <SENT><VN> L?as tu vu </VN>  ?
</SENT>(Him did you see ?
).Sentences are given a flat structure, that iscomplements are not included in a verbal phrase10(i).
From a practical point of view this eases ourtask.
From a theoretical point of view,  thetraditional VP (with complements) is subject tomuch linguistic debate and is oftendiscontinuous in French as is shown in (j1) and(j2): In (j1) the NP subject (IBM) is postverbaland precedes the locative complement (sur lemarch?).
In (j2), the adverb certainement is alsopostverbal and precedes the NP object  (uneaugmentation de capital).
(i) <SENT><NP> JEAN </NP><VN> donne</VN><NP>une pomme</NP><PP> ?
<NP> Marie </NP> </PP> .
</SENT>(John gives an apple to Mary)(j1) les actions qu?a mises IBM sur le march?
(the shares that IBM put on the market)(j2) Les actionnaires d?cideront certainement uneaugmentation de capital (the stock holders willcertainly decide on a raise of capital)3.1.1 Evaluation for French10Hence the use of VN(for verbal nucleus) instead of VP.When we began our task, we had at our disposala 1 million word POS tagged and hand?corrected corpus (Abeill?
& Cl?ment 98).
Thecorpus was meant to be syntactically annotatedfor constituency.
To achieve this, preciseannotation guidelines for constituency had beenwritten and a portion of the corpus had beenhand?annotated (independently of thedevelopment of the shallow?parser) to test theguidelines (approx.
25 000 words) .To evaluate the shallow parser, weperformed as described at the beginning ofsection 3 : We parsed the 1 million words.
Weset aside 500 sentences (approx.
15 000 words)for quickly tuning our rules.
We also set asidethe 25 000 words that had been independentlyannotated in order to compare the output of theparser to a portion of the final Treebank.
Inaddition, an annotator hand?corrected the outputof the shallow?parser on 1000 new randomlychosen sentences (approx.
30 000 words).Contrary to the 25 000 words which constitutedthe beginning of the Treebank, for these 30 000words verb arguments, PPs and modifiers werenot attached.
Finally, we extracted bare non?recursive NPs from the 25 000 words, in orderto evaluate how the parser did on this particulartask.When compared to the hand?correctedparser?s output, for opening brackets we  obtaina recall of 94.3 % and a precision of 95.2%.
Forclosing brackets, we obtain a precision of 92.2% and a recall of 91.4 %.
Moreover, 95.6 % ofthe correctly placed brackets are labeledcorrectly, the remaining 4.4% are not strictlyspeaking labeled incorrectly, since they arelabeled INC (i.e.
unknown) These unknownconstituents, rather then errors, constitute amechanism of underspecification (the idea beingto assign as little wrong information aspossible)11.When compared to the 25 000 words of theTreebank,  For opening brackets, the recall is92.9% and the precision is 94%.
For closingbrackets, the recall is 62,8% and the precision is65%.
These lower results are normal, since theTreebank contains attachments that the parser isnot supposed to make.Finally, on the specific task of identifying non?recursive NP?chunks, we obtain a recall of 96.6% and a precision of 95.8 %.
for opening11 These underspecified label can be removed at a deeperparsing stage, or one can add a guesser  .<SENT>  <NP> La:Dfs proportion:NC </NP><PP> d?
:P <NP> ?tudiants:NC </NP> </PP><PP> par_rapport_?
:P<NP> la:Ddef population:NC</NP> </PP><PONCT> ,:PONCT </PONCT><PP> dans:P <NP> notre:Dposspays:NC</NP> </PP><PONCT> ,:PONCT</PONCT><VN> est:VP inf?rieure:Aqual </VN><PP> ?
:P <NP> ce:PROdem</NP> </PP><REL> qu:PROR3ms<VN> elle:CL est:VP </VN><COORD> <PP> ?
:P <NP> les:Ddef Etats?Unis:NP </NP> </PP>   ou:CC<PP> ?
:P <NP> le:DdefJapon:NP</NP></PP> </COORD></REL> <PONCT> .
:PONCT</PONCT></SENT><SENT>  <NP> Les:Dmp pays:NC</NP><NP> les:Ddef plus:ADV efficaces:Aqual?conomiquement:ADV</NP><VN> sont:VP</VN><NP> ceux:PROdem</NP><REL> qui:PROR<VN> ont:VP</VN><NP> les:Ddef travailleurs:NC les:Ddefmieux:ADV</NP><VN> form?s:VK</VN></REL> <PONCT> .
:PONCT</PONCT></SENT><SENT>  <ADVP> D?autre_part:ADV</ADVP><PONCT> ,:PONCT </PONCT><SUB> si:CS<VN> nous:CL voulons:VP demeurer:VW</VN><NP> une:Dind grande_puissance:NC</NP></SUB> <PONCT> ,:PONCT</PONCT><VN> nous:CL devons:VP rester:VW</VN><NP> un:Dind pays:NC</NP><REL> qui:PROR<VN> cr?e:VP</VN><NP> le:Ddef savoir:NC</NP></REL><PONCT> .
:PONCT</PONCT></SENT><SENT>  <COORD> Et:CC<PP> pour:P <NP> cela:PROdem</NP></PP> </COORD><PONCT> ,:PONCT </PONCT><VN> il:CL faut:VP</VN><NP> un:Dindenseignement_sup?rieur:NC fort:Aqual</NP><PONCT> .
:PONCT</PONCT> </SENT><SENT>  <COORD> Mais:CC<PP> en_dehors_de:P<NP> ces:Ddem raisons:NC?conomiques:Aqual ou:CCphilosophiques:Aqual </NP> </PP></COORD><PONCT> ,:PONCT </PONCT><VN> il:CL y:CL a:VP </VN><NP> la:Ddef r?alit?
:NC </NP><NP> les:Ddef ?tudiants:NC</NP><VN> sont:VP</VN><PP> ?
:P <NP> notre:Dposs porte:NC</NP></PP>  <PONCT> .
:PONCT</PONCT> </SENT>FIGURE 2 : Sample output for Frenchbrackets, and a recall and precision of resp.94.3% and 92.9 % for closing brackets.To give an idea about the coverage of theparser, sentences are on average 30 words longand comprise 20.6 opening brackets (and thus asmany closing brackets).
Errors difficult tocorrect with access to a limited context involvemainly "missing" brackets (e.g.
"comptez vous* ne pas le traiter" (do you expect not to treathim) appears as single constituent, while thereshould be 2) , while "spurious" brackets canoften be eliminated by adding more rules (e.g.for multiple prepositions  : "de chez").
Mosterrors for closing brackets are due to clauseboundaries(i.e.
SUB, COORD and REL).To obtain these results, we had to write only48 rules.Concerning speed, as argued in (Tapanainen& J?rvinen, 94), we found that rule?basedsystems are not necessarily slow, since the 1million words are parsed in 3mn 8 seconds.One can compare this to (Ait?Moktar &Chanod 97), who, in order to shallow?parseFrench resort to 14 networks and parse150words /sec (Which amounts to approx.
111minutes for one million words)12.
It is difficult tocompare our result to other results, since  mostShallow?parsers pursue different tasks, and usedifferent evaluation metrics.
However to give anidea, standard techniques typically produce anoutput for one million words in 20 mn andreport a precision and a recall ranging from 70%to 95% depending on the language, kind of textand task.
Again, we are not saying that ourtechnique obtains best results, but simply that itis fast and  easy to use for unrestricted text forany language.
To give a better idea to the reader,we provide an output of the Shallow?parser forFrench on figure 2.In order to improve our tool and our rules, ademo is available online on the author?shomepage.3.2 A Shallow?Parser for EnglishWe wanted to evaluate our compiler on morethan one language, to make sure that our resultswere easily replicable.
So we wrote a new set ofrules for English using the PennTreebank tagset,both for POS and for constituent labels.12 They report a recall ranging from 82.6% and 92.6%depending on the type of texts, and a precision of 98% forsubject recognition, but their results are not directlycomparable to ours, since the task is different.We sat aside sections 00 and 01 of the WSJ forevaluation (i.e.
approx.
3900 sentences), andused other sections of the WSJ for tuning ourrules.Contrary to the French Treebank, the PennTreebank contains non?surfastic constructionssuch as empty nodes, and constituents that arenot triggered by a lexical items.Therefore, before evaluating our new shallow?parser, we automatically removed from the testsentences all opening brackets that were notimmediately followed by a lexical item, withtheir corresponding closing brackets, as well asall the constituents which contained an emptyelement.
We also removed all information onpseudo?attachment.
We then evaluated theoutput of the shallow?parser to the testsentences.
For bare NPs, we compared ouroutput to the POS tagged version of the testsentences (since bare?NPs are marked there).For the shallow?parsing task, we obtain aprecision of 90.8% and a recall of 91% foropening brackets, a precision of 65.7% andrecall of 66.1% for closing brackets.
For theNP?chunking task, we obtain a precision of91% and recall of 93.2%, using an ?exactmatch?
measure (i.e.
both the opening andclosing boundaries of an NP must match to becounted as correct).The results, were as satisfactory as for French.Concerning linguistic choices when writing therules, we didn?t really make any, and simplyfollowed closely those of the Penn Treebanksyntactic annotation guidelines (modulo theembeddings, the empty categories and pseudo?attachments mentioned above).Concerning the number of rules, we used 54 ofthem in order to detect all constituents, and 27rules for NP?chunks identification.
.
In sections00 and 01 of the wsj there were 24553 NPs,realized as 1200 different POS patterns (ex : CDNN,  DT $ JJ NN, DT NN?).
Even thoughthese 1200 patterns corresponded to a lowernumber of regular expressions, a standardfinite?state approach would have to resort tomore than 27 rules.
One can also compare thisresult to the one reported in (Ramshaw &Marcus 95) who, obtain up to 93.5% recall and93.1% precision on the same task, but usingbetween 500 and 2000 rules.3.3 Tolerance to POS errorsTo test the tolerance to POS tagging errors,we have extracted the raw text from the Englishcorpus from section 3.2., and retagged it usingthe publicly available tagger TreeTagger(Schmid, 94).
without retraining it.
The authorsof the tagger advertise an error?rate between 3and 4%.
We then ran the NP?chunker on theoutput of the tagger, and still obtain a precisionof 90.2% and a recall of 92% on the ?exactmatch?
NP identification task: the fact that ourtool does not rely on regular expressionsdescribing "full constituent patterns" allows toignore some POS errors since mistagged wordswhich do not appear at constituent boundaries(i.e.
essentially lexical words)  have noinfluence on the output.
This improves accuracyand robustness.
For example, if "first" has beenmistagged noun instead of adjective in [NP thefirst man ] on the moon ..., it won?t preventdetecting the NP, as long as the determiner hasbeen tagged correctly.ConclusionWe have presented a tool which allows togenerate a shallow?parser for unrestricted textin any language.
This tool is based on the use ofa imited number of rules which aim atidentifying constituent boundaries.
We thenpresented evaluations on French and on English,and concluded that our tools obtains resultssimilar to other shallow?parsing techniques, butin a much simpler and economical way.We are interested in developing new sets ofrules for new languages (e.g.
Portuguese andGerman) and new style (e.g.
French oral texts).It would also be interesting to test the tool oninflectional languages.The shallow?parser for French is also beingused in the SynSem project which aims atsyntactically and semantically annotatingseveral millions words of French textsdistributed by ELRA13.
Future improvements ofthe tool will consist in adding a module toannotate syntactic functions, and completevalency information for verbs, with the help of alexicon (Kinyon, 00).Finally, from a theoretical point of view, it maybe interesting to see if our rules could beacquired automatically from raw text (althoughthis might not be worth it in practice,considering the small number of rules we use,and the fact that acquiring the rules in such away would most likely introduce errors).13European Language Ressources AssociationAcknowledgement We especially thank F.Toussenel, who has performed most of theevaluation for French presented in section 3.1.1.ReferencesAbeill?
A. Cl?ment L. 1999 : A tagged reference corpusfor French.
Proc.
LINC?EACL?99.
BergenAbeill?
A., Cl?ment L., Kinyon A., Toussenel F. 2001Building a Treebank for French.
In Treebanks (AAbeill?
ed.).
Kluwer academic publishers.Abney S. 1991.
Parsing by chunks.
In Principle?basedParsing.
(R. Berwick, S. Abney and C. Tenny eds),Kluwer academic publishers.A?t?Mokhtar S. & Chanod J.P. 1997.
Incremental Finite?State Parsing.
Proc.
ANLP?97, Washington,Bourigault 1992 : Surface Grammatical analysis for theextraction of terminological noun phrases.
Proc.COLING?92.
Vol 3,  pp.
977?981Brants T., Skut W.,  Uszkoreit H., 1999.
SyntacticAnnotation of a German Newspaper Corpus.
Proc.ATALA Treebank Workshop.
Paris, France.Daelemans W., Buchholz S., Veenstra J.. Memory?BasedShallow Parsing.Proc.CoNLL?EACL?99Grefenstette G.. 1996.
Light Parsing as Finite?StateFiltering.
Proc.
ECAI ?96 workshop on "Extendedfinite state models of  language".Joshi A.K.
Hopely P. 1997.
A parser from antiquity.
InExtended Finite State Models of Language.
(A.
Kornaied.).
University Press.Karlsson F., Voutilainen A., Heikkil J., Antilla A.
(eds.)1995.
Constraint Grammar: a language?independentsystem for parsing unrestricted text.
Mouton deGruyer.Kinyon A.
2000.
Hypertags.
Proc.
COLING?00.Sarrebrucken.Magerman D.M., 1994  Natural language parsing asstatistical pattern recognition.
PhD Dissertation,Stanford University.Marcus M., Santorini B., and Marcinkiewicz M.A.
1993.Building a large annotated corpus of english: The penntreebank.
Computational Linguistics, 19:313?
?330.Ramshaw, L.A. & Marcus, M.P., 1995.
Text Chunkingusing Transformation?Based Learning, ACL ThirdWorkshop on Very Large Corpora, pp.82?94, 1995.Ratnaparkhi A.
1997. linear observed time statisticalparser based on maximum entropy models.
TechnicalReport cmp-lg/9706014.Tapanainen P. and J?rvinen T., 1994, Syntactic Analysisof a Natural Language Using Linguistic Rules andCorpus?Based Patterns.
Proc.
COLING?94.
Vol I, pp629?634.
Kyoto.Schmid H. 1994 Probabilistic Part?Of?Speech TaggingUsing Decision Trees.
Proc.
NEMLAP?94.Vergne J.
1999.
Etude et mod?lisation de la syntaxe deslangues ?
l?aide de l?ordinateur.
Analyse syntaxiqueautomatique non combinatoire.
Dossier d?habilitation?
diriger des recherches.
Univ.
de Caen.
