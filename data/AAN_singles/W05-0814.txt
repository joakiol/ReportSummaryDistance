Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 91?94,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005ISI?s Participation in the Romanian-English Alignment TaskAlexander FraserInformation Sciences InstituteUniversity of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292-6601fraser@isi.eduDaniel MarcuInformation Sciences InstituteUniversity of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292-6601marcu@isi.eduAbstractWe discuss results on the shared task ofRomanian-English word alignment.
Thebaseline technique is that of symmetrizingtwo word alignments automatically gener-ated using IBM Model 4.
A simple vo-cabulary reduction technique results in animprovement in performance.
We alsoreport on a new alignment model and anew training algorithm based on alternat-ing maximization of likelihood with mini-mization of error rate.1 IntroductionISI participated in the WPT05 Romanian-Englishword alignment task.
The system used for baselineexperiments is two runs of IBM Model 4 (Brown etal., 1993) in the GIZA++ (Och and Ney, 2003) im-plementation, which includes smoothing extensionsto Model 4.
For symmetrization, we found that Ochand Ney?s ?refined?
technique described in (Och andNey, 2003) produced the best AER for this data setunder all experimental conditions.We experimented with a statistical model for in-ducing a stemmer cross-lingually, but found that thebest performance was obtained by simply lower-casing both the English and Romanian text and re-moving all but the first four characters of each word.We also tried a new model and a new trainingcriterion based on alternating the maximization oflikelihood and minimization of the alignment errorrate.
For these experiments, we have implementedan alignment package for IBM Model 4 using a hill-climbing search and Viterbi training as described in(Brown et al, 1993), and extended this to use newsubmodels.
The starting point is the final alignmentgenerated using GIZA++?s implementation of IBMModel 1 and the Aachen HMM model (Vogel et al,1996).Paper organization: Section 2 is on the baseline,Section 3 discusses vocabulary reduction, Section 4introduces our new model and training method, Sec-tion 5 describes experiments, Section 6 concludes.We use the following notation: e refers to an En-glish sentence composed of English words labeledei.
f refers to a Romanian sentence composed ofRomanian words labeled fj .
a is an alignment of eto f .
We use the term ?Viterbi alignment?
to denotethe most probable alignment we can find, rather thanthe true Viterbi alignment.2 BaselineTo train our systems, Model 4 was trained two times,first using Romanian as the source language andthen using English as the source language.
For eachtraining, we ran 5 iterations of Model 1, 5 iterationsof the HMM model and 3 iterations of Model 4.For the distortion calculations of Model 4, we re-moved the dependencies on Romanian and Englishword classes.
We applied the ?union?, ?intersection?and ?refined?
symmetrization metrics (Och and Ney,2003) to the final alignments output from training, aswell as evaluating the two final alignments directly.We tried to have a strong baseline.
GIZA++ hasmany free parameters which can not be estimated us-ing Maximum Likelihood training.
We did not use91the defaults, but instead used settings which producegood AER results on French/English bitext.
Wealso optimized p0 on the 2003 test set (using AER),rather than using likelihood training.
Turning off theextensions to GIZA++ and training p0 as in (Brownet al, 1993) produces a substantial increase in AER.3 Vocabulary Size ReductionRomanian is a Romance language which has a sys-tem of suffixes for inflection which is richer thanEnglish.
Given the small amount of training data,we decided that vocabulary size reduction was de-sirable.
As a baseline for vocabulary reduction, wetried reducing words to prefixes of varying sizesfor both English and Romanian after lowercasingthe corpora.
We also tried Porter stemming (Porter,1997) for English.
(Rogati et al, 2003) extended Model 1 with an ad-ditional hidden variable to represent the split pointsin Arabic between the prefix, the stem and the suf-fix to generate a stemming for use in Cross-LingualInformation Retrieval.
As in (Rogati et al, 2003),we can find the most probable stemming given themodel, apply this stemming, and retrain our wordalignment system.
However, we can also use themodified model directly to find the best word align-ment without converting the text to its stemmedform.We introduce a variable rj for the Romanian stemand a variable sj for the Romanian suffix (whichwhen concatenated together give us the Romanianword fj) into the formula for the probability of gen-erating a Romanian word fj using an alignment ajgiven only an English sentence e. We use the indexz to denote a particular stemming possibility.
For agiven Romanian word the stemming possibilities aresimply every possible split point where the stem is atleast one character (this includes the null suffix).p(fj , aj |e) =?zp(rj,z, sj,z, aj |e) (1)If the assumption is made that the stem and thesuffix are generated independently from e, we canassume conditional independence.p(fj , aj |e) =?zp(rj,z, aj |e)p(sj,z, aj |e) (2)We performed two sets of experiments, one setwhere the English was stemmed using the Porterstemmer and one set where each English word wasstemmed down to its first four characters.
Wetried the best performing scoring heuristic for Ara-bic from (Rogati et al, 2003) where p(sj,z, aj |e) ismodeled using the heuristic p(sj,z|lj) where sj,z isthe Romanian suffix, and lj is the last letter of theRomanian word fj ; these adjustments are updatedduring EM training.
We also tried several other ap-proximations of p(sj,z, aj |e) with and without up-dates in EM training.
We were unable to producebetter results and elected to use the baseline vocab-ulary reduction technique for the shared task.4 New Model and Training AlgorithmOur motivation for a new model and a new trainingapproach which combines likelihood maximizationwith error rate minimization is threefold:?
Maximum Likelihood training of Model 4 isnot sufficient to find good alignments?
We would like to model factors not captured byIBM Model 4?
Using labeled data could help us produce betteralignments, but we have very few labelsWe create a new model and train it using an al-gorithm which has a step which increases likelihood(like one iteration in the EM algorithm), alternatingwith a step which decreases error.
We accomplishthis by:?
grouping the parameters of Model 4 into 5 sub-models?
implementing 6 new submodels?
combining these into a single log-linear modelwith 11 weights, ?1 to ?11, which we groupinto the vector ??
defining a search algorithm for finding thealignment of highest probability given the sub-models and ??
devising a method for finding a ?
which min-imizes alignment error given fixed submodelsand a set of gold standard alignments?
inventing a training method for alternatingsteps which estimate the submodels by increas-ing likelihood with steps which set ?
to de-crease alignment errorThe submodels in our new alignment model arelisted in table 1, where for ease of exposition we92Table 1: Submodels used for alignment1 t(fj |ei) TRANSLATION PROBABILITIES2 n(?i|ei) FERTILITY PROBABILITIES, ?i IS THE NUMBER OF WORDS GENERATED BY THE ENGLISH WORD ei3 null PARAMETERS USED IN GENERATING ROMANIAN WORDS FROM ENGLISH NULL WORD (INCLUDING p0, p1)4 d1(4j) MOVEMENT (DISTORTION) PROBABILITIES OF FIRST ROMANIAN WORD GENERATED FROM ENGLISH WORD5 d>1(4j) MOVEMENT (DISTORTION) PROBABILITIES OF OTHER ROMANIAN WORDS GENERATED FROM ENGLISH WORD6 TTABLE ESTIMATED FROM INTERSECTION OF TWO STARTING ALIGNMENTS FOR THIS ITERATION7 TRANSLATION TABLE FROM ENGLISH TO ROMANIAN MODEL 1 ITERATION 58 TRANSLATION TABLE FROM ROMANIAN TO ENGLISH MODEL 1 ITERATION 59 BACKOFF FERTILITY (FERTILITY ESTIMATED OVER ALL ENGLISH WORDS)10 ZERO FERTILITY ENGLISH WORD PENALTY11 NON-ZERO FERTILITY ENGLISH WORD PENALTYconsider English to be the source language and Ro-manian the target language.The log-linear alignment model is specified byequation 3.
The model assigns non-zero proba-bilities only to 1-to-many alignments, like Model4.
(Cettolo and Federico, 2004) used a log-linearmodel trained using error minimization for the trans-lation task, 3 of the submodels were taken fromModel 4 in a similar way to our first 5 submodels.p?
(a, f |e) =exp(?m ?mhm(f, a, e))?f,e,a exp(?m ?mhm(f, a, e))(3)Given ?, the alignment search problem is to findthe alignment a of highest probability according toequation 3.
We solve this using the local search de-fined in (Brown et al, 1993).We set ?
as follows.
Given a sequence A of align-ments we can calculate an error function, E(A).
Forthese experiments average sentence AER was used.We wish to minimize this error function, so we se-lect ?
accordingly:argmin??a?E(a?)?
(a?, (argmaxap?
(a, f |e))) (4)Maximizing performance for all of the weightsat once is not computationally tractable, but (Och,2003) has described an efficient one-dimensionalsearch for a similar problem.
We search over each?m (holding the others constant) using this tech-nique to find the best ?m to update and the best valueto update it to.
We repeat the process until no furthergain can be found.Our new training method is:REPEAT?
Start with submodels and lambda from previ-ous iteration?
Find Viterbi alignments on entire training cor-pus using new model (similar to E-step ofModel 4 training)?
Reestimate submodel parameters from Viterbialignments (similar to M-step of Model 4Viterbi training)?
Find a setting for ?
that reduces AER on dis-criminative training set (new D-step)We use the first 148 sentences of the 2003 test setfor the discriminative training set.
10 settings for ?are found, the hypothesis list is augmented using theresults of 10 searches using these settings, and thenanother 10 settings for ?
are found.
We then selectthe best ?.
The discriminative training regimen isotherwise similar to (Och, 2003).5 ExperimentsTable 2 provides a comparison of our baseline sys-tems using the ?refined?
symmetrization metric withthe best limited resources track system from WPT03(Dejean et al, 2003) on the 2003 test set.
The bestresults are obtained by stemming both English andRomanian words to the first four letters, as describedin section 2.Table 3 provides details on our shared task sub-mission.
RUN1 is the word-based baseline system.RUN2 is the stem-based baseline system.
RUN4uses only the first 6 submodels, while RUN5 usesall 11 submodels.
RUN3 had errors in processing,so we omit it.Results:?
Our new 1-to-many alignment model and train-ing method are successful, producing decreasesof 0.03 AER when the source is Romanian, and0.01 AER when the source is English.93Table 2: Summary of results for 2003 test setSYSTEM STEM SIZES AERXEROX ?NOLEM-ER-56K?
0.289BASELINE NO PROCESSING 0.284BASELINE ENG PORTER / ROM 4 0.251BASELINE ENG 4 / ROM 4 0.248Table 3: Full results on shared task submissions (blind test 2005)RUN NAMES STEM SIZES SOURCE ROM SOURCE ENG UNION INTERSECTION REFINEDISI.RUN1 NO PROCESSING 0.3834 0.3589 0.3590 0.3891 0.3165ISI.RUN2 ENG 4 / ROM 4 0.3056 0.2880 0.2912 0.3041 0.2675ISI.RUN4 ENG 4 / ROM 4 0.2798 0.2833 0.2773 0.2862 0.2663ISI.RUN5 ENG 4 / ROM 4 0.2761 0.2778 0.2736 0.2807 0.2655?
These decreases do not translate to a large im-provement in the end-to-end task of producingmany-to-many alignments with a balanced pre-cision and recall.
We had a very small decreaseof 0.002 AER using the ?refined?
heuristic.?
The many-to-many alignments produced using?union?
and the 1-to-1 alignments produced us-ing ?intersection?
were also improved.?
It may be a problem that we trained p0 usinglikelihood (it is in submodel 3) rather than op-timizing p0 discriminatively as we did for thebaseline.6 Conclusion?
Considering multiple stemming possibilitiesfor each word seems important.?
Alternating between increasing likelihood anddecreasing error rate is a useful training ap-proach which can be used for many problems.?
Our model and training method improve upon astrong baseline for producing 1-to-many align-ments.?
Our model and training method can be usedwith the ?intersection?
heuristic to producehigher quality 1-to-1 alignments?
Models which can directly model many-to-many alignments and do not require heuristicsymmetrization are needed to produce higherquality many-to-many alignments.
Our train-ing method can be used to train them.7 AcknowledgmentsThis work was supported by DARPA-ITO grantNN66001-00-1-9814 and NSF grant IIS-0326276.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,and R. L. Mercer.
1993.
The mathematics of statistical ma-chine translation: Parameter estimation.
Computational Lin-guistics, 19(2):263?311.Mauro Cettolo and Marcello Federico.
2004.
Minimum er-ror training of log-linear translation models.
In Proc.
of theInternational Workshop on Spoken Language Translation,pages 103?106, Kyoto, Japan.Herve Dejean, Eric Gaussier, Cyril Goutte, and Kenji Yamada.2003.
Reducing parameter space for word alignment.
InHLT-NAACL 2003 Workshop on Building and Using Paral-lel Texts: Data Driven Machine Translation and Beyond, Ed-monton, Alberta, July.Franz Josef Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proc.
of the 41st Annual Meet-ing of the Association for Computational Linguistics (ACL),pages 160?167, Sapporo, Japan, July.M.
F. Porter.
1997.
An algorithm for suffix stripping.
In Read-ings in information retrieval, pages 313?316, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Monica Rogati, Scott McCarley, and Yiming Yang.
2003.
Un-supervised learning of arabic stemming using a parallel cor-pus.
In Proc.
of the 41st Annual Meeting of the Associationfor Computational Linguistics (ACL), Sapporo, Japan, July.Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996.HMM-based word alignment in statistical translation.
InCOLING ?96: The 16th Int.
Conf.
on Computational Lin-guistics, pages 836?841, Copenhagen, Denmark, August.94
