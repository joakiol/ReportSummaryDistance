Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 1?8,Athens, Greece, 31 March 2009. c?2009 Association for Computational LinguisticsOne distributional memory, many semantic spacesMarco BaroniUniversity of TrentoTrento, Italymarco.baroni@unitn.itAlessandro LenciUniversity of PisaPisa, Italyalessandro.lenci@ilc.cnr.itAbstractWe propose an approach to corpus-basedsemantics, inspired by cognitive science,in which different semantic tasks are tack-led using the same underlying reposi-tory of distributional information, col-lected once and for all from the sourcecorpus.
Task-specific semantic spaces arethen built on demand from the repository.A straightforward implementation of ourproposal achieves state-of-the-art perfor-mance on a number of unrelated tasks.1 IntroductionCorpus-derived distributional semantic spaceshave proved valuable in tackling a variety of tasks,ranging from concept categorization to relation ex-traction to many others (Sahlgren, 2006; Turney,2006; Pado?
and Lapata, 2007).
The typical ap-proach in the field has been a ?local?
one, in whicheach semantic task (or set of closely related tasks)is treated as a separate problem, that requires itsown corpus-derived model and algorithms.
Itssuccesses notwithstanding, the ?one task ?
onemodel?
approach has also some drawbacks.From a cognitive angle, corpus-based modelshold promise as simulations of how humans ac-quire and use conceptual and linguistic informa-tion from their environment (Landauer and Du-mais, 1997).
However, the common view in cog-nitive (neuro)science is that humans resort to amultipurpose semantic memory, i.e., a databaseof interconnected concepts and properties (Rogersand McClelland, 2004), adapting the informationstored there to the task at hand.
From an engineer-ing perspective, going back to the corpus to train adifferent model for each application is inefficientand it runs the risk of overfitting the model to aspecific task, while losing sight of its adaptivity ?
ahighly desirable feature for any intelligent system.Think, by contrast, of WordNet, a single networkof semantic information that has been adapted toall sorts of tasks, many of them certainly not en-visaged by the resource creators.In this paper, we explore a different approachto corpus-based semantics.
Our model consistsof a distributional semantic memory ?
a graph ofweighted links between concepts - built once andfor all from our source corpus.
Starting from thetuples that can be extracted from this graph, wederive multiple semantic spaces to solve a widerange of tasks that exemplify various strands ofcorpus-based semantic research: measuring se-mantic similarity between concepts, concept cate-gorization, selectional preferences, analogy of re-lations between concept pairs, finding pairs thatinstantiate a target relation and spotting an alterna-tion in verb argument structure.
Given a graph likethe one in Figure 1 below, adaptation to all thesetasks (and many others) can be reduced to two ba-sic operations: 1) building semantic spaces, as co-occurrence matrices defined by choosing differentunits of the graph as row and column elements;2) measuring similarity in the resulting matrix ei-ther between specific rows or between a row andan average of rows whose elements share a certainproperty.After reviewing some of the most closely re-lated work (Section 2), we introduce our approach(Section 3) and, in Section 4, we proceed to testit in various tasks, showing that its performance isalways comparable to that of task-specific meth-ods.
Section 5 draws the current conclusions anddiscusses future directions.2 Related workTurney (2008) recently advocated the need for auniform approach to corpus-based semantic tasks.Turney recasts a number of semantic challenges interms of relational or analogical similarity.
Thus,if an algorithm is able to tackle the latter, it can1also be used to address the former.
Turney tests hissystem in a variety of tasks, obtaining good resultsacross the board.
His approach amounts to pick-ing a task (analogy recognition) and reinterpretingother tasks as its particular instances.
Conversely,we assume that each task may keep its speci-ficity, and unification is achieved by designing asufficiently general distributional structure, fromwhich semantic spaces can be generated on de-mand.
Currently, the only task we share with Tur-ney is finding SAT analogies, where his methodoutperforms ours by a large margin (cf.
Section4.2.1).
However, Turney uses a corpus that is25 times larger than ours, and introduces nega-tive training examples, whereas we dependency-parse our corpus ?
thus, performance is not di-rectly comparable.
Besides the fact that our ap-proach does not require labeled training data likeTurney?s one, it provides, we believe, a more intu-itive measure of taxonomic similarity (taxonomicneighbours are concepts that share similar con-texts, rather than concepts that co-occur with pat-terns indicating a taxonomic relation), and it isbetter suited to model productive semantic phe-nomena, such as the selectional preferences ofverbs with respect to unseen arguments (eatingtopinambur vs. eating ideas).
Such tasks will re-quire an extension of the current framework ofTurney (2008) beyond evidence from the direct co-occurrence of target word pairs.While our unified framework is, as far as weknow, novel, the specific ways in which we tacklethe different tasks are standard.
Concept similar-ity is often measured by vectors of co-occurrencewith context words that are typed with dependencyinformation (Lin, 1998; Curran and Moens, 2002).Our approach to selectional preference is nearlyidentical to the one of Pado?
et al (2007).
Wesolve SAT analogies with a simplified version ofthe method of Turney (2006).
Detecting whethera pair expresses a target relation by looking atshared connector patterns with model pairs is acommon strategy in relation extraction (Pantel andPennacchiotti, 2008).
Finally, our method to de-tect verb slot similarity is analogous to the ?slotoverlap?
of Joanis et al (2008) and others.
Sincewe aim at a unified approach, the lack of origi-nality of our task-specific methods should be re-garded as a positive fact: our general frameworkcan naturally reproduce, locally, well-tried ad-hocsolutions.3 Distributional semantic memoryMany different, apparently unrelated, semantictasks resort to the same underlying information,a ?distributional semantic memory?
consisting ofweighted concept+link+concept tuples extractedfrom the corpus.
The concepts in the tuples aretypically content words.
The link contains corpus-derived information about how the two words areconnected in context: it could be for example adependency path or a shallow lexico-syntactic pat-tern.
Finally, the weight typically derives from co-occurrence counts for the elements in a tuple, re-scaled via entropy, mutual information or similarmeasures.
The way in which the tuples are iden-tified and weighted when populating the memoryis, of course, of fundamental importance to thequality of the resulting models.
However, oncethe memory has been populated, it can be used totackle many different tasks, without ever having togo back to the source corpus.Our approach can be compared with the typicalorganization of databases, in which multiple alter-native ?views?
can be obtained from the same un-derlying data structure, to answer different infor-mation needs.
The data structure is virtually inde-pendent from the way in which it is accessed.
Sim-ilarly, the structure of our repository only obeysto the distributional constraints extracted from thecorpus, and it is independent from the ways it willbe ?queried?
to address a specific semantic task.Different tasks can simply be defined by how wesplit the tuples from the repository into row andcolumn elements of a matrix whose cells are filledby the corresponding weights.
Each of these de-rived matrices represents a particular view of dis-tributional memory: we will discuss some of theseviews, and the tasks they are appropriate for, inSection 4.Concretely, we used here the web-derived, 2-billion word ukWaC corpus,1 dependency-parsedwith MINIPAR.2 Focusing for now on modelingnoun-to-noun and noun-to-verb connections, weselected the 20,000 most frequent nouns and 5,000most frequent verbs as target concepts (minus stoplists of very frequent items).
We selected as tar-get links the top 30 most frequent direct verb-noun dependency paths (e.g., kill+obj+victim),the top 30 preposition-mediated noun-to-noun or1http://wacky.sslmit.unibo.it2http://www.cs.ualberta.ca/?lindek/minipar.htm2dievictimsubj_in1335.2teachersubj_tr109.4soldiersubj_in4547.5policemansubj_in68.6schoolin2.5killsubj_tr22.4obj915.4     obj9.9subj_tr1306.9obj8948.3subj_tr38.2obj538.1at7020.1with28.9in11894.4handbookwith3.2use10.1gunwith105.9use41.0in2.8at10.3in2.5with30.5use7.4Figure 1: A fragment of distributional memoryverb-to-noun paths (e.g., soldier+with+gun) andthe top 50 transitive-verb-mediated noun-to-nounpaths (e.g., soldier+use+gun).
We extracted alltuples in which a target link connected two targetconcepts.
We computed the weight (strength ofassociation) for all the tuples extracted in this wayusing the local MI measure (Evert, 2005), that istheoretically justified, easy to compute for triplesand robust against overestimation of rare events.Tuples with local MI ?
0 were discarded.
Foreach preserved tuple c1+ l+c2, we added a same-weight c1 + l?1 + c2 tuple.
In graph-theoreticalterms (treating concepts as nodes and labeling theweighted edges with links), this means that, foreach edge directed from c1 to c2, there is an edgefrom c2 to c1 with the same weight and inverselabel, and that such inverse edges constitute thefull set of links directed from c2 to c1.
The re-sulting database (DM, for Distributional Memory)contains about 69 million tuples.
Figure 1 de-picts a fragment of DM represented as a graph (as-sume, for what we just said, that for each edgefrom x to y there is a same-weight edge from yto x with inverse label: e.g., the obj link fromkill to victim stands for the tuples kill+obj+victimand victim+obj?1+kill, both with weight 915.4;subj in identifies the subjects of intransitive con-structions, as in The victim died; subj tr refers tothe subjects of transitive sentences, as in The po-liceman killed the victim).We also trained 3 closely comparable modelsthat use the same source corpus, the same tar-get concepts (in one case, also the same targetlinks) and local MI as weighting method, with thesame filtering threshold.
The myPlain model im-plements a classic ?flat?
co-occurrence approach(Sahlgren, 2006) in which we keep track of verb-to-noun co-occurrence within a window that caninclude, maximally, one intervening noun, andnoun-to-noun co-occurrence with no more than2 intervening nouns.
The myHAL model usesthe same co-occurrence window, but, like HAL(Lund and Burgess, 1996), treats left and right co-occurrences as distinct features.
Finally, myDVuses the same dependency-based target links ofDM as filters.
Like in the DV model of Pado?and Lapata (2007), only pairs connected by targetlinks are preserved, but the links themselves arenot part of the model.
Since none of these alter-native models stores information about the links,they are only appropriate for the concept similar-ity tasks, where links are not necessary.4 Semantic views and experimentsWe now look at three views of the DMgraph: concept-by-link+concept (CxLC),concept+concept-by-link (CCxL), andconcept+link-by-concept (CLxC).
Each viewwill be tested on one or more semantic tasks andcompared with alternative models.
There is afourth possible view, links-by-concept+concept(LxCC), that is not explored here, but would leadto meaningful semantic tasks (finding links thatexpress similar semantic relations).4.1 The CxLC semantic spaceMuch work in computational linguistics and re-lated fields relies on measuring similarity amongwords/concepts in terms of their patterns of co-occurrence with other words/concepts (Sahlgren,2006).
For this purpose, we arrange the informa-tion from the graph in a matrix where the concepts(nodes) of interest are rows, and the nodes theyare connected to by outgoing edges are columns,typed with the corresponding edge label.
We re-fer to this view as the concept-by-link+concept3(CxLC) semantic space.
From the graph in Fig-ure 1, we can for example construct the matrixin Table 1 (here and below, showing only somerows and columns of interest).
By comparing therow vectors of such matrix using standard geo-metrical techniques (e.g., measuring the normal-ized cosine distance), we can find out about con-cepts that tend to share similar properties, i.e., aretaxonomically similar (synonyms, antonyms, co-hyponyms), e.g., soldiers and policemen, that bothkill, are killed and use guns.subj in?1subj tr?1 obj?1 with usedie kill kill gun gunteacher 109.4 0.0 9.9 0.0 0.0victim 1335.2 22.4 915.4 0.0 0.0soldier 4547.5 1306.9 8948.3 105.9 41.0policeman 68.6 38.2 538.1 30.5 7.4Table 1: A fragment of the CxLC spaceWe use the CxLC space in three taxonomic sim-ilarity tasks: modeling semantic similarity judg-ments, noun categorization and verb selectionalrestrictions.4.1.1 Human similarity ratingsWe use the dataset of Rubenstein and Goode-nough (1965), consisting of 65 noun pairs ratedby 51 subjects on a 0-4 similarity scale (e.g.
car-automobile 3.9, cord-smile 0.0).
The average rat-ing for each pair is taken as an estimate of theperceived similarity between the two words.
Fol-lowing Pado?
and Lapata (2007), we use Pearson?sr to evaluate how the distances (cosines) in theCxLC space between the nouns in each pair cor-relate with the ratings.
Percentage correlations forDM, our other models and the best absolute re-sult obtained by Pado?
and Lapata (DV+), as wellas their best cosine-based performance (cosDV+),are reported in Table 2.model r model rmyDV 70 DV+ 62DM 64 myHAL 61myPlain 63 cosDV+ 47Table 2: Correlation with similarity ratingsDM is the second-best model, outperformedonly by DV when the latter is trained on compara-ble data (myDV in Table 2).
Notice that, here andbelow, we did not try any parameter tuning (e.g.,using a similarity measure different than cosine,feature selection, etc.)
to improve the performanceof DM.4.1.2 Noun categorizationWe use the concrete noun dataset of the ESSLLI2008 Distributional Semantics shared task,3 in-cluding 44 concrete nouns to be clustered into cog-nitively justified categories of increasing general-ity: 6-way (birds, ground animals, fruits, greens,tools and vehicles), 3-way (animals, plants andartifacts) and 2-way (natural and artificial enti-ties).
Following the task guidelines, we clusteredthe target row vectors in the CxLX matrix withCLUTO,4 using its default settings, and evalu-ated the resulting clusters in terms of cluster-size-weighted averages of purity and entropy (see theCLUTO documentation).
An ideal solution wouldhave 100% purity and 0% entropy.
Table 3 pro-vides percentage results for our models as well asfor the ESSLLI systems that reported all the rel-evant performance measures, indexed by first au-thor.
Models are ranked by a global score given bysumming the 3 purity values and subtracting the 3entropies.model 6-way 3-way 2-way globalP E P E P EKatrenko 89 13 100 0 80 59 197Peirsman+ 82 23 84 34 86 55 140DM 77 24 79 38 59 97 56myDV 80 28 75 51 61 95 42myHAL 75 27 68 51 68 89 44Peirsman?
73 28 71 54 61 96 27myPlain 70 31 68 60 59 97 9Shaoul 41 77 52 84 55 93 -106Table 3: Concrete noun categorizationDM outperforms our models trained on com-parable resources.
Katrenko?s system queriesGoogle for patterns that cue the category of a con-cept, and thus its performance should rather beseen as an upper bound for distributional models.Peirsman and colleagues report results based ondifferent parameter settings: DM?s performance?
not tuned to the task ?
is worse than their topmodel, but better than their worse.4.1.3 Selectional restrictionsIn this task we test the ability of the CxLC space topredict verbal selectional restrictions.
We use theCxLC matrix to compare a concept to a ?proto-type?
constructed by averaging a set of other con-cepts, that in this case represent typical fillers of3http://wordspace.collocations.de/doku.php/esslli:start4http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview4a verbal slot ?
for example, by averaging the vec-tors of the nouns that are, according to the underly-ing graph, objects of killing, we can build a vectorfor the typical ?killee?, and model selectional re-strictions by measuring the similarity of other con-cepts (including concepts that have not been seenas objects of killing in the corpus) to this proto-type.
Note that the DM graph is used both to findthe concepts to enter in the prototype (the set ofnouns that are connected to a verb by the relevantedge) and to compute similarity.
Thus, the methodis fully unsupervised.We test on the two datasets of human judgmentsabout the plausibility of nouns as arguments (ei-ther subjects or objects) of verbs used in Pado?
etal.
(2007), one (McRae) consisting of 100 noun-verb pairs rated by 36 subjects, the second (Pado?
)with 211 pairs rated by 20 subjects.
For each verbin these datasets, we built its prototypical sub-ject/object argument vector by summing the nor-malized vectors of the 50 nouns with the highestweight on the appropriate dependency link to theverb (e.g., the top 50 nouns connected to kill by anobj link).
The cosine distance of a noun to a proto-type is taken as the model ?plausibility judgment?about the noun occurring as the relevant verb ar-gument.
Since we are interested in generalization,if the target noun is in the prototype set we sub-tract its vector from the prototype before calculat-ing the cosine.
For our comparison models, thereis no way to determine which nouns would formthe prototype, and thus we train them using thesame top noun lists we employ for DM.
FollowingPado?
and colleagues, performance is measured bythe Spearman ?
correlation coefficient between theaverage human ratings and the model predictions.Table 4 reports percentage coverage and correla-tions for our models as well as those in Pado?
etal.
(2007) (ParCos is the best among their purelycorpus-based systems).model McRae Pado?coverage ?
coverage ?Pado?
56 41 97 51DM 96 28 98 50ParCos 91 21 98 48myDV 96 21 98 39myHAL 96 12 98 29myPlain 96 12 98 27Resnik 94 3 98 24Table 4: Correlation with verb-argument plausibil-ity judgmentsDM does very well on this task: its performanceon the Pado?
dataset is comparable to that of thePado?
system, that relies on FrameNet.
DM hasnearly identical performance to the latter on thePado?
dataset.
On the McRae data, DM has a lowercorrelation, but much higher coverage.
Since weare using a larger corpus than Pado?
et al (2007),who train on the BNC, a fairer comparison mightbe the one with our alternative models, that are alloutperformed by DM by a large margin.4.2 The CCxL semantic spaceAnother view of the DM graph is exemplified inTable 5, where concept pairs are represented interms of the edge labels (links) connecting them.Importantly, this matrix contains the same infor-mation that was used to build the CxLC spaceof Table 1, with a different arrangement of whatgoes in the rows and in the columns, but the sameweights in the cells ?
compare, for example, thesoldier+gun-by-with cell in Table 5 to the soldier-by-with+gun cell in Table 1.in at with useteacher school 11894.47020.1 28.9 0.0teacher handbook 2.5 0.0 3.2 10.1soldier gun 2.8 10.3 105.9 41.0Table 5: A fragment of the CCxL spaceWe use this space to measure ?relational?
sim-ilarity (Turney, 2006) of concept pairs, e.g., find-ing that the relation between teachers and hand-books is more similar to the one between soldiersand guns, than to the one between teachers andschools.
We also extend relational similarity toprototypes.
Given some example pairs instantiat-ing a relation, we can harvest new pairs linked bythe same relation by computing the average CCxLvector of the examples, and finding the nearestneighbours to this average.
In the case at hand,the link profile of pairs such as soldier+gun andteacher+handbook could be used to build an ?in-strument relation?
prototype.We test the CCxL semantic space on recogniz-ing SAT analogies (relational similarity betweenpairs) and semantic relation classification (rela-tional similarity to prototypes).4.2.1 Recognizing SAT analogiesWe used the set of 374 multiple-choice ques-tions from the SAT college entrance exam.
Eachquestion includes one target pair, usually called5the stem (ostrich-bird) , and 5 other pairs (lion-cat, goose-flock, ewe-sheep, cub-bear, primate-monkey).
The task is to choose the pair most anal-ogous to the stem.
Each SAT pair can be rep-resented by the corresponding row vector in theCCxL matrix, and we select the pair with the high-est cosine to the stem.
In Table 6 we report ourresults, together with the state-of-the-art from theACL wiki5 and the scores of Turney (2008) (Pair-Class) and from Amac?
Herdag?delen?s PairSpacesystem, that was trained on ukWaC.
The Attr cellssummarize the performance of the 6 models on thewiki table that are based on ?attributional similar-ity?
only (Turney, 2006).
For the other systems,see the references on the wiki.
Since our coverageis very low (44% of the stems), in order to make ameaningful comparison with the other models, wecalculated a corrected score (DM?).
Having fullaccess to the results of the ukWaC-trained, simi-larly performing PairSpace system, we calculatedthe adjusted score by assuming that the DM-to-PairSpace error ratio (estimated on the items wecover) is constant on the whole dataset, and thusthe DM hit count on the unseen items is approx-imated by multiplying the PairSpace hit count onthe same items by the error ratio (DM+ is DM?saccuracy on the covered test items only).model % correct model % correctLRA 56.1 KnowBest 43.0PERT 53.3 DM?
42.3PairClass 52.1 LSA 42.0VSM 47.1 AttrMax 35.0DM+ 45.3 AttrAvg 31.0PairSpace 44.9 AttrMin 27.3k-means 44.0 Random 20.0Table 6: Accuracy with SAT analogiesDM does not excel in this task, but its correctedperformance is well above chance and that of allthe attributional models, and comparable to that ofa WordNet-based system (KnowBest) and a sys-tem that uses manually crafted information aboutanalogy domains (LSA).
All systems with perfor-mance above DM+ (and k-means) use corpora thatare orders of magnitude larger than ukWaC.4.2.2 Classifying semantic relationsWe also tested the CCxL space on the 7semantic relations between nominals adoptedin Task 4 of SEMEVAL 2007 (Girju et5http://www.aclweb.org/aclwiki/index.php?title=SAT_Analogy_Questionsal., 2007): Cause-Effect, Instrument-Agency,Product-Producer, Origin-Entity, Theme-Tool,Part-Whole, Content-Container.
For each rela-tion, the dataset includes 140 training examplesand about 80 test cases.
Each example consistsof a small context retrieved from the Web, con-taining word pairs connected by a certain pattern(e..g., ?
* contains *?).
The retrieved contexts weremanually classified by the SEMEVAL organizersas positive (e.g., wrist-arm) or negative (e.g., ef-fectiveness-magnesium) instances of a certain re-lation (e.g., Part-Whole).
About 50% training andtest cases are positive instances.
For each rela-tion, we built ?hit?
and ?miss?
prototype vectors,by averaging across the vectors of the positive andnegative training pairs attested in our CCxL model(we use only the word pairs, not the surround-ing contexts).
A test pair is classified as a hitfor a certain relation if it is closer to the hit pro-totype vector for that relation than to the corre-sponding miss prototype.
We used the SEMEVAL2007 evaluation method, i.e., precision, recall, F-measure and accuracy, macroaveraged over all re-lations, as reported in Table 7.
The DM+ scoresignore the 32% pairs not in our CCxL space; theDM?
scores assume random performance on suchpairs.
These scores give the range within whichour performance will lie once we introduce tech-niques to deal with unseen pairs.
We also reportresults of the SEMEVAL systems that did not usethe organizer-provided WordNet sense labels norinformation about the query used to retrieve theexamples, as well as performance of several trivialclassifiers, also from the SEMEVAL task descrip-tion.model precision recall F accuracyUCD-FC 66.1 66.7 64.8 66.0UCB 62.7 63.0 62.7 65.4ILK 60.5 69.5 63.8 63.5DM+ 60.3 62.6 61.1 63.3UMELB-B 61.5 55.7 57.8 62.7SemeEval avg 59.2 58.7 58.0 61.1DM?
56.7 58.2 57.1 59.0UTH 56.1 57.1 55.9 58.8majority 81.3 42.9 30.8 57.0probmatch 48.5 48.5 48.5 51.7UC3M 48.2 40.3 43.1 49.9alltrue 48.5 100.0 64.8 48.5Table 7: SEMEVAL relation classificationThe DM accuracy is higher than the three SE-MEVAL baselines (majority, probmatch and all-true), DM+ is above the average performance of6the comparable SEMEVAL models.
Differentlyfrom DM, the models that outperform it use fea-tures extracted from the training contexts and/orspecific additional resources: an annotated com-pound database for UCD-FC, machine learningalgorithms to train the relation classifiers (ILK,UCD-FC), Web counts (UCB), etc.
The less thanoptimal performance by DM is thus counterbal-anced by its higher ?parsimony?
and generality.4.3 The CLxC semantic spaceA third view of the information in the DM graphis the concept+link-by-concept (CLxC) semanticspace exemplified by the matrix in Table 8.teacher victim soldier policemankill subj tr 0.0 22.4 1306.9 38.2kill obj 9.9 915.4 8948.3 538.1die subj in 109.4 1335.2 4547.5 68.6Table 8: A fragment of the CLxC spaceThis view captures patterns of similarity be-tween (surface approximations to) argument slotsof predicative words.
We can thus use the CLxCspace to extract generalizations about the innerstructure of lexico-semantic representations of thesort formal semanticists have traditionally beinginterested in.
In the example, the patterns ofco-occurrence suggest that objects of killing arerather similar to subjects of dying, hinting at theclassic cause(subj,die(obj)) analysis of killing byDowty (1977) and many others.
Again, no new in-formation has been introduced ?
the matrix in Ta-ble 8 is yet another re-organization of the data inour graph (compare, for example, the die+subj in-by-teacher cell of this matrix with the teacher-by-subj in+die cell in Table 1).4.3.1 The causative/inchoative alternationSyntactic alterations (Levin, 1993) representa key aspect of the complex constraints thatshape the syntax-semantics interface.
One ofthe most important cases of alternation is thecausative/inchoative, in which the object argu-ment (e.g., John broke the vase) can also be re-alized as an intransitive subject (e.g., The vasebroke).
Verbs differ with respect to the possi-ble syntactic alternations they can participate in,and this variation is strongly dependent on theirsemantic properties (e.g.
semantic roles, eventtype, etc.).
For instance, while break can undergothe causative/inchoative alternation, mince cannot:cf.
John minced the meat and *The meat minced.We test our CLxC semantic space on thediscrimination between transitive verbs un-dergoing the causative-inchoative alterna-tions and non-alternating ones.
We took232 causative/inchoative verbs and 170 non-alternating transitive verbs from Levin (1993).For each verb vi, we extracted from the CLxCmatrix the row vectors corresponding to its tran-sitive subject (vi + subj tr), intransitive subject(vi + subj in), and direct object (vi + obj) slots.Given the definition of the causative/inchoativealternation, we predict that with alternating verbsvi + subj in should be similar to vi + obj(the things that are broken also break), whilethis should not hold for non-alternating verbs(mincees are very different from mincers).Our model is completely successful in detect-ing the distinction.
The cosine similarity betweentransitive subject and object slots is fairly low forboth classes, as one would expect (medians of 0.16for alternating verbs and 0.11 for non-alternatingverbs).
On the other hand, while for the non-alternating verbs the median cosine similarity be-tween the intransitive subject and object slots isa similarly low 0.09, for the alternating verbs themedian similarity between these slots jump upto 0.31.
Paired t-tests confirm that the per-verbdifference between transitive subject vs. objectcosines and intransitive subject vs. object cosinesis highly statistically significant for the alternatingverbs, but not for the non-alternating ones.5 ConclusionWe proposed an approach to semantic tasks wherestatistics are collected only once from the sourcecorpus and stored as a set of weighted con-cept+link+concept tuples (naturally representedas a graph).
Different semantic spaces are con-structed on demand from this underlying ?distri-butional memory?, to tackle different tasks with-out going back to the corpus.
We have shown thata straightforward implementation of this approachleads to excellent performance in various taxo-nomic similarity tasks, and to performance that,while not outstanding, is at least reasonable on re-lational similarity.
We also obtained good resultsin a task (detecting the causative/inchoative alter-nation) that goes beyond classic NLP applicationsand more in the direction of theoretical semantics.The most pressing issue we plan to address ishow to improve performance in the relational sim-7ilarity tasks.
Fortunately, some shortcomings ofour current model are obvious and easy to fix.The low coverage is in part due to the fact thatour set of target concepts does not contain, by de-sign, some words present in the task sets.
More-over, while our framework does not allow ad-hocoptimization of corpus-collection methods for dif-ferent tasks, the way in which the information inthe memory graph is adapted to tasks should ofcourse go beyond the nearly baseline approacheswe adopted here.
In particular, we need to de-velop a backoff strategy for unseen pairs in therelational similarity tasks, that, following Turney(2006), could be based on constructing surrogatepairs of taxonomically similar words found in theCxLC space.Other tasks should also be explored.
Here, weviewed our distributional memory in line with howcognitive scientists look at the semantic memoryof healthy adults, i.e., as an essentially stable longterm knowledge repository.
However, much in-teresting semantic action takes place when under-lying knowledge is adapted to context.
We planto explore how contextual effects can be modeledin our framework, focusing in particular on howcomposition affects word meaning (Erk and Pado?,2008).
Similarity could be measured directly onthe underlying graph, by relying on graph-basedsimilarity algorithms ?
an elegant approach thatwould lead us to an even more unitary view ofwhat distributional semantic memory is and whatit does.
Alternatively, DM could be represented asa three-mode tensor in the framework of Turney(2007), enabling smoothing operations analogousto singular value decomposition.AcknowledgmentsWe thank Ken McRae and Peter Turney for pro-viding data-sets, Amac?
Herdag?delen for access tohis results, Katrin Erk for making us look at DM asa graph, and the reviewers for helpful comments.ReferencesJ.
Curran and M. Moens.
2002.
Improvements in auto-matic thesaurus extraction.
Proceedings of the ACLWorkshop on Unsupervised Lexical Acquisition, 59?66.D.
Dowty.
1977.
Word meaning and Montague Gram-mar.
Kluwer, Dordrecht.K.
Erk and S. Pado?.
2008.
A structured vector spacemodel for word meaning in context.
Proceedings ofEMNLP 2008.S.
Evert.
2005.
The statistics of word cooccurrences.Ph.D.
dissertation, Stuttgart University, Stuttgart.R.
Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-ney and Y. Deniz.
2007.
SemEval-2007 task 04:Classification of semantic relations between nomi-nals.
Proceedings of SemEval-2007, 13?18.E.
Joanis, S. Stevenson and D. James.
2008.
A gen-eral feature space for automatic verb classification.Natural Language Engineering, 14(3): 337?367.T.K.
Landauer and S.T.
Dumais.
1997.
A solutionto Plato?s problem: The Latent Semantic Analysistheory of acquisition, induction and representationof knowledge.
Psychological Review, 104(2): 211?240.B.
Levin.
1993.
English Verb Classes and Alterna-tions.
A Preliminary Investigation.
Chicago, Uni-versity of Chicago Press.D.
Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
Proceedings of ACL 1998, 768?774.K.
Lund and C. Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behaviour Research Methods, 28: 203?208.S.
Pado?
and M. Lapata.
2007.
Dependency-based con-struction of semantic space models.
ComputationalLinguistics, 33(2): 161?199.S.
Pado?, S. Pado?
and K. Erk.
2007.
Flexible, corpus-based modelling of human plausibility judgements.Proceedings EMNLP 2007, 400?409.P.
Pantel and M. Pennacchiotti.
2008.
Automaticallyharvesting and ontologizing semantic relations.
InP.
Buitelaar and Ph.
Cimiano (eds.
), Ontology learn-ing and population.
IOS Press, Amsterdam.T.
Rogers and J. McClelland.
2004.
Semantic cog-nition: A parallel distributed processing approach.The MIT Press, Cambridge.H.
Rubenstein and J.B. Goodenough.
1965.
?Contex-tual correlates of synonymy?.
Communications ofthe ACM, 8(10):627-633.M.
Sahlgren.
2006.
The Word-space model.
Ph.D. dis-sertation, Stockholm University, Stockholm.P.
Turney.
2006.
Similarity of semantic relations.Computational Linguistics, 32(3): 379?416.P.
Turney.
2007.
Empirical evaluation of four ten-sor decomposition algorithms.
IIT Technical ReportERB-1152, National Research Council of Canada,Ottawa.P.
Turney.
2008.
A uniform approach to analogies,synonyms, antonyms and associations.
Proceedingsof COLING 2008, 905?912.8
