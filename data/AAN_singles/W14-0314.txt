Workshop on Humans and Computer-assisted Translation, pages 93?98,Gothenburg, Sweden, 26 April 2014.c?2014 Association for Computational LinguisticsQuantifying the Influence of MT Output in the Translators?
Performance:A Case Study in Technical TranslationMarcos ZampieriSaarland UniversitySaarbr?ucken, Germanymzampier@uni-koeln.deMihaela VelaSaarland UniversitySaarbr?ucken, Germanym.vela@mx.uni-saarland.deAbstractThis paper presents experiments on the useof machine translation output for technicaltranslation.
MT output was used to pro-duced translation memories that were usedwith a commercial CAT tool.
Our exper-iments investigate the impact of the useof different translation memories contain-ing MT output in translations?
quality andspeed compared to the same task withoutthe use of translation memory.
We evalu-ated the performance of 15 novice transla-tors translating technical English texts intoGerman.
Results suggest that translatorsare on average over 28% faster when us-ing TM.1 IntroductionProfessional translators use a number of tools toincrease the consistency, quality and speed of theirwork.
Some of these tools include spell checkers,text processing software, terminological databasesand others.
Among all tools used by professionaltranslators the most important of them nowadaysare translation memory (TM) software.
TM soft-ware use parallel corpora of previously translatedexamples to serve as models for new transla-tions.
Translators then validate or correct previ-ously translated segments and translate new onesincreasing the size of the memory after each newtranslated segment.One of the great issues in working with TMs isto produce the TM itself.
This can be time con-suming and the memory should ideally contain agood amount of translated segments to be consid-ered useful and accurate.
For this reason, manynovice translators do not see the benefits of theuse of TM right at the beginning, although it isconsensual that on the long run the use of TMs in-crease the quality and speed of their work.
To copewith this limitation, more TM software have pro-vided interface to machine translation (MT) soft-ware.
MT output can be used to suggest new seg-ments that were not previously translated by a hu-man translator but generated automatically froman MT software.
But how helpful are these trans-lations?To answer this question, the experiments pro-posed in this paper focus on the translator?s per-formance when using TMs produced by MT out-put within a commercial CAT tool interface.
Weevaluate the quality of the translation output aswell as the time and effort taken to accomplisheach task.
The impact of MT and TM in trans-lators?
performance has been explored and quan-tified in different settings (Bowker, 2005; Guer-berof, 2009; Guerberof, 2012; Morado Vazquezet al., 2013).
We believe this paper constitutesanother interesting contribution to the interfacebetween the study of the performance of humantranslators, CAT tools and machine translation.2 Related WorkCAT tools have become very popular in the last20 years.
They are used by freelance transla-tors as well as by companies and language ser-vice providers to increase translation?s quality andspeed (Somers and Diaz, 2004; Lagoudaki, 2008).The use of CAT tools is part of the core curricu-lum of most translation studies degrees and a rea-sonable level of proficiency in the use of thesetools is expected from all graduates.
With the im-provement of state-of-the-art MT software, a re-cent trend in CAT research is its integration withmachine translation tools as for example the Mate-Cat1project (Cettolo et al., 2013).There is considerable amount of studies on MTpost-editing published in the last years (Specia,2011; Green et al., 2013).
Due to the scope of our1www.matecat.com93paper (and space limitation) we will deliberatelynot discuss the findings of these experiments andinstead focus on those that involve the use of trans-lation memories.
Post-editing tools are substan-tially different than commercial CAT tools (suchas the one used here) and even though the TMsused in our experiments were produced using MToutput, we believe that our experiment setting hasmore in common with similar studies that investi-gate TMs than MT post-editing.The study by Bowker (2005) was one of thefirst to quantify the influence of TM in transla-tors work.
The experiment divided translators inthree groups: A, B and C. Translators in GroupA did not use a TM, translators in Group B usedan unmodified TM and finally translators in groupC used a TM that had been deliberately mod-ified with a number of translation errors.
Thestudy concluded that when faced with time pres-sure, translators using TMs tend not to be criti-cal enough about the suggestions presented by thesoftware.Another similar experiment (Guerberof, 2009)compared productivity and quality of human trans-lations using MT and TM output.
The experimentwas conducted starting with the hypothesis that thetime invested in post-editing one string of machinetranslated text will correspond to the same time in-vested in editing a fuzzy matched string located inthe 80-90 percent range.
This study quantified theperformance of 8 translators using a post-editingtool.
According to the author, the results indicatethat using a TM with 80 to 90 fuzzy matches pro-duces more errors than using MT segments or hu-man translation.The aforementioned recent work by MoradoVazquez et al.
(2013) investigates the performanceof twelve human translators (students) using theACCEPT post-editing tool.
Researchers providedMT and TM output and compared time, qualityand keystroke effort.
Findings of this study indi-cate that the use of a specific MT has a great im-pact in the translation activity in all three aspects.In the context of software localization, productiv-ity was also tested by Plitt and Masselot (2010)combining MT output and a post-editing tool.
An-other study compared the performance of humantranslators in a scenario using TMs and a com-mercial CAT tool (Across) with a second scenariousing post-editing (L?aubli et al., 2013).As to our study, we used instead of a post-editing tool, a commercial CAT tool, the SDL Tra-dos Studio 2014 version.
A similar setting to ourswas explored by Federico et al.
(2012) using SDLTrados Studio integrating a commercial MT soft-ware.
We took the decision of working a commer-cial CAT tool for two reasons: first, because thisis the real-world scenario faced by translators inmost companies and language service providers2and second, because it allows us to explore a dif-ferent variable that the aforementioned studies didnot substantially explore, namely: MT output asTM segments.3 Setting the ExperimentIn our experiments we provided short texts fromthe domain of software development containing upto 343 tokens each to 15 beginner translators.
Theaverage length of these texts ranges between 210tokens in experiment 1 to 264 tokens in experi-ment 3 divided in 15 to 17 segments (average) (seetable 2).
Translators were given English texts andwere asked to translate them into German, theirmother tongue.
One important remark is that all15 participants were not aware that the TMs wemade available were produced using MT output.The 15 translators who participated in theseexperiments are all 3rdsemester master degreestudents who have completed a bachelors degreein translation studies and are familiar with CATtools.
All of them attended at least 20 classhours about TM software and related technologies.Translators who participated in this study were allproficient in English and they have studied it as aforeign language at bachelor level.As previously mentioned, the CAT tool used inthese experiments is the most recent version ofSDL Trados, the Studio 20143version.
Transla-tors were given three different short texts to betranslated in three different scenarios:1.
Using no translation memory.2.
Using a translation memory collected withmodified MT examples.3.
Using translation memory collected with un-modified MT examples.In experiment number two we performed anumber of modifications in the TM segments.
As2Although the use of MT and post-editing software hasbeen growing, commercial TM software is still the most pop-ular alternative.3http://www.sdl.com/campaign/lt/sdl-trados-studio-2014/94can be seen in table 1, these modifications weresufficient to alter the coverage of the TM, but didnot introduce translation errors to the memory.4The alterations we performed along with an exam-ple of each of them can be summarized as follows:?
Deletion: ?To paste the text currently in theclipboard, use the Edit Paste menu item.?
-?To paste the text, use the Edit Paste menuitem.??
Modification: ?Persistent Selection is dis-abled by default.?
- ?Persistent Selection isenabled by default.??
Substitution: ?The editor is composed of thefollowing components:?
- ?The editor is com-posed of the following elements:?Three texts were available per scenario, each ofthem with different TM coverage scores (see table1).
Students were asked to translate the texts attheir own pace without time limitation and wereallowed to use external linguistic resources suchas dictionaries, lexica, parallel concordancers, etc.3.1 Corpus and TMThe corpus used for these experiments is the KDEcorpus obtained from the Opus5repository (Tiede-mann, 2012).
The corpus contains texts from thedomain of software engineering, hence the title: ?acase study in technical translation?.
We are con-vinced that technical translation contains a sub-stantial amount of fixed expressions and techni-cal terms different from, for example, news texts.This makes technical translation, to our under-standing, an interesting domain for the use of TMby professional translators and for experiments ofthis kind.In scenarios 1, 2 and 3 we measured differentaspects of translation such as time and edited seg-ments.
One known shortcoming of our experimentdesign is that unlike most post-editing softwarethe reports available in CAT tools are quite poor(e.g.
no information about keystrokes is provided).Even so, we stick to our decision of using a TMsoftware and tried to compensate this shortcomingby a careful qualitative and quantitative data anal-ysis after the experiments.4Modifications were carried out in the source and targetlanguages5http://opus.lingfil.uu.se/Table number 1 presents the coverage scores forthe different TMs and texts used in the experi-ments.
Coverage scores were calculated based onthe information provided by SDL Trados Studio.We provided 9 different texts to be translated toGerman (3 for each scenario), the 6 texts providedfor experiments 2 and 3 are presented next.Text Experiment TM CoverageText D 2 61.23%Text E 2 78.16%Text F 2 59.15%Average 2 66,18%Text G 3 88.27%Text H 3 59.92%Text I 3 65.16%Average 3 71,12%Table 1: TM CoverageWe provided different texts and levels of coverageto investigate the impact of this variable.
We as-sured an equal distribution of texts among trans-lators: each text was translated by 5 translators.This allowed us to calculate average results andto consider the average TM coverage difference of4,93% between experiment 2 and 3.4 ResultsWe observed performance gain when using any ofthe two TMs, which was expectable.
The resultsvaried according to the coverage of the TM.
Inexperiment number 3, texts contained on averageover 7 segments with 100% matches6and exper-iment number 2 only 2.68.
This allowed transla-tors to finish the task faster in experiment number3.
The average results obtained in the different ex-periments are presented in table number 2.7Criteria Exp.
1 Exp.
2 Exp.
3Number of Segments 15.85 15.47 17.29Number of Tokens 209.86 202.89 264.53Context Matches 6.58 6.06Repetitions 0.18100% 2.68 7.1895% to 99% 0.42 0.1285% to 94% 0.2175% to 84% 2.11 0.1850% to 75% 0.19New Segments 15.86 5.89 3.24Time Elapsed (mins.)
37m45s 26m3s 19m21sTable 2: Average Scores6Translators were allowed to modify 100% and contextmatches.7According to the Trados Studio documentation, a repeti-tion occurs every time the tool finds the exact same segmentin another (or the same) file the user is translating95As to the time spent per segment, experimentsindicate a performance gain of over 52% in ex-periment number 3 and over 28% in experimentnumber 2.Criteria Exp.1 Exp.
2 Exp.
3Time Segment (mins.)
2m22s 1m41s 1m07sAverage gain to 1 +28.87% +52.82%Average gain to 2 +33.77%Table 3: Time per SegmentApart from the expectable performance gain whenusing TM, we also found a considerable differencebetween the use of the modified and unmodifiedTM.
Translators completed segments in experi-ment number 3, on average, 33.77% faster thanexperiment two.
The difference of coverage be-tween the two TMs was 4,93%, which suggeststhat a few percentage points of TM coverage re-sults on a greater performance boost.We also have to acknowledge that the experi-ments were carried out by translators in the sameorder in which they are presented in this paper.This may, of course, influence performance in allthree experiments as translators were more usedto the task towards the end of the experiment.
Onehypothesis is that the poor performance in exper-iment 1, could be improved if this task was donefor last and conversely, the performance boost ob-served in experiment 3, could be a bit lower ifthis experiment was done first.
This variable wasnot explored in similar productivity studies suchas those presented in section two and, to our un-derstanding, inverting the order of tasks could bean interesting variable to be tested in future exper-iments.As a general remark, although all translatorshad experience with the 2014 version of TradosStudio, we observed a great difficulty in perform-ing simple tasks with Windows for at least half ofthe group.
Simple operations such as copying, re-naming and moving files or creating folders in thefile system were very time consuming.
Trados in-terface also posed difficulties to translators.
Forexample, the generation of reports through batchtasks in a different window was for most transla-tors confusing.
These operations could be simpli-fied as it is in other CAT tools such as memoQ.88http://kilgray.com/products/memoq4.1 A Glance at Quality EstimationOne of the future directions that this work will takeis to investigate the quality of human translations.Our initial hypothesis is that it is possible to applystate-of-the-art metrics such as BLEU (Papineniet al., 2002) or METEOR (Denkowski and Lavie,2011) to estimate the quality of these translationsregardless of how they are produced.For machine translation output, quality nowa-days is measured by automatic evaluation met-rics such as the aforementioned IBM BLEU (Pap-ineni et al., 2002), NIST (Doddington, 2002), ME-TEOR (Denkowski and Lavie, 2011), the Leven-sthein (1966) distance based WER (word error-rate) metric, the position-independent error ratemetric PER (Tillmann et al., 1997) and the trans-lation error rate metric TER (Snover et al., 2006)with its newer version TERp (Snover et al., 2009).The most frequently used one is IBMBLEU (Papineni et al., 2002).
It is easy touse, language-independent, fast and requiresonly the candidate and reference translation.IBM BLEU is based on the n-gram precision bymatching the machine translation output againstone or more reference translations.
It accountsfor adequacy and fluency through word precision,respectively the n-gram precision, by calculatingthe geometric mean.
Instead of recall, in IBMBLEU the brevity penalty (BP) was introduced.Different from IBM BLEU, METEOR evalu-ates a candidate translation by calculating the pre-cision and recall on unigram level and combiningthem in a parametrized harmonic mean.
The resultfrom the harmonic mean is than scaled by a frag-mentation penalty which penalizes gaps and dif-ferences in word order.For our investigation we applied METEOR onthe human translated text.
Our intention is to testwhether we can reproduce the observations fromthe experiments: is the experiment setting 3 bet-ter than the setting of experiment 2?
Therefore,METEOR is used here to investigate whether wecan correlate it with our experiments and not toevaluate the produced translations.
Table number4 presents the scores obtained with METEOR.Exp.
2 Exp.
3Average Score (mean) 0.14 0.41Best Result 0.35 0.58Worst Result 0.11 0.25Table 4: METEOR Scores96In experiment number 3 we have previously ob-served that the translators?
performance was sig-nificantly better and that translators could translateeach segment on average 33.77% faster than ex-periment 2 and 52.82% faster than experiment 1.By applying METEOR scores we can also observethat experiment 3 achieved higher scores whichseems to indicate more suitable translations thanexperiment number 2.
Quality estimation is oneof the aspects we would like to explore in futurework.5 ConclusionThis paper is a first step towards the comparisonof different TMs produced with MT output andtheir direct impact in human translation.
Our studyshows a substantial improvement in performancewith the use of translation memories containingMT output used trough commercial CAT software.To our knowledge this experiment setting was nottested in similar studies, which makes our paper anew contribution in the study of translators?
per-formance.
Although the performance gain seemsintuitive, the quantification of these aspects withina controlled experiment was not substantially ex-plored.We opted for the use of a state-of-the-art com-mercial CAT tool as this is the real-world scenariothat most translators face everyday.
In compari-son to translating without TM, translators were onaverage 28.87% faster using a modified TM and52.82% using an unmodified one.
Between thetwo TMs we observed that translators were on av-erage 33.77% faster when using the unmodifiedTM.
As previously mentioned, the order in whichthis tasks were carried out should be also takeninto account.
The performance boost of 33.77%when using a TM that is only 4,93% better is alsoan interesting outcome of our experiments thatshould be looked at in more detail.Finally, in this paper we used METEOR scoresto assess whether it is possible to correlate trans-lations?
speed, quality and TM coverage.
The av-erage score for experiment number 2 was 0.14 andfor experiment number 3 was 0.41.
Our initialanalysis suggests that a relation between the twovariables exists for our dataset.
Whether this rela-tion can be found in other scenarios is still an openquestion and we wish to investigate this variablemore carefully in future work.5.1 Future WorkWe consider these experiments as a pilot study thatwas carried out to provide us a set of variables thatwe wish to investigate further.
There are a numberof aspects that we wish to look in more detail infuture work.Future experiments include the aforementionedquality estimation analysis by applying state-of-the-art metrics used in machine translation.
Usingthese metrics we would like to explore the extentto which it is possible to use automatic methodsto study the interplay between quality and perfor-mance in computer assisted translation.
Further-more, we would like to perform a qualitative anal-ysis of the produced translations using human an-notators and inter annotator agreement (Carletta,1996).The performance boost observed between sce-narios 2 and 3 should be looked in more detailin future experiments.
We would like to replicatethese experiments using other different TMs andexplore this variable more carefully.
Another as-pect that we would like to explore in the future isthe direct impact of the use of different CAT tools.Does the same TM combined with different CATtools produce different results?
When conductingthese experiments, we observed that a simplifiedinterface may speed up translators?
work consid-erably.Other directions that our work will take includecontrolling other variables not taken into accountin this pilot study such as: the use of termino-logical databases, spelling correctors, etc.
Howand to which extent do they influence performanceand quality?
Finally, we would also like to useeye-tracking to analyse the focus of attention oftranslators as it was done in previous experiments(O?brien, 2006).AcknowledgmentsWe thank the students who participated in theseexperiments for their time.
We would also liketo thank the detailed feedback provided by theanonymous reviewers who helped us to increasethe quality of this paper.ReferencesLynne Bowker.
2005.
Productivity vs quality?
a pilotstudy on the impact of translation memory systems.Localisation Reader, pages 133?140.97Jean Carletta.
1996.
Assessing agreement on classi-fication tasks: The kappa statistic.
ComputationalLinguistics, 22(2):249?254.Mauro Cettolo, Christophe Servan, Nicola Bertoldi,Marcello Federico, Loic Barrault, and HolgerSchwenk.
2013.
Issues in incremental adaptation ofstatistical mt from human post-edits.
In Proceedingsof the MT Summit XIV Workshop on Post-editingTechnology and Practice (WPTP-2), Nice, France.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic Metric for Reliable Optimizationand Evaluation of Machine Translation Systems.
InProceedings of the EMNLP 2011 Workshop on Sta-tistical Machine Translation.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the Sec-ond International Conference on Human LanguageTechnology Research, HLT 2002, pages 138?145,San Francisco, CA, USA.
Morgan Kaufmann Pub-lishers Inc.Marcello Federico, Alessandro Cattelan, and MarcoTrombetti.
2012.
Measuring user productivityin machine translation enhanced computer assistedtranslation.
In Proceedings of Conference of the As-sociation for Machine Translation in the Americas(AMTA).Spence Green, Jeffrey Heer, and Christopher D Man-ning.
2013.
The efficacy of human post-editing forlanguage translation.
In Proceedings of the SIGCHIConference on Human Factors in Computing Sys-tems.Ana Guerberof.
2009.
Productivity and quality inthe post-editing of outputs from translation memo-ries and machine translation.
Localisation Focus,7(1):133?140.Ana Guerberof.
2012.
Productivity and Quality in thePost-Edition of Outputs from Translation Memoriesand Machine Translation.
Ph.D. thesis, Rovira andVirgili University Tarragona.Elina Lagoudaki.
2008.
The value of machine transla-tion for the professional translator.
In Proceedingsof the 8th Conference of the Association for Ma-chine Translation in the Americas, pages 262?269,Waikiki, Hawaii.Samuel L?aubli, Mark Fishel, Gary Massey, MaureenEhrensberger-Dow, and Martin Volk.
2013.
Assess-ing post-editing efficiency in a realistic translationenvironment.
In Proceedings of MT Summit XIVWorkshop on Post-editing Technology and Practice.Vladimir Iosifovich Levenshtein.
1966.
Binary codescapable of correcting deletions, insertions and rever-sals.
Soviet Physics Doklady, (8):707?710, Febru-ary.Lucia Morado Vazquez, Silvia Rodriguez Vazquez, andPierrette Bouillon.
2013.
Comparing forum datapost-editing performance using translation memoryand machine translation output: a pilot study.
InProceedings of the Machine Translation SummitXIV), Nice, France.Sharon O?brien.
2006.
Eye-tracking and translationmemory matches.
Perspectives: Studies in Transla-tology, 14:185?204.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Mirko Plitt and Franc?ois Masselot.
2010.
A productiv-ity test of statistical machine translation post-editingin a typical localisation context.
The Prague Bul-letin of Mathematical Linguistics, 93:7?16.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Transla-tion in the Americas, AMTA.Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Fluency, adequacy, orhter?
exploring different human judgments with atunable mt metric.
In Proceedings of the FourthWorkshop on Statistical Machine Translation at the12th Meeting of the European Chapter of the Asso-ciation for Computational Linguistics, EACL 2009.Harold Somers and Gabriela Fernandez Diaz.
2004.Translation memory vs. example-based mt: What isthe difference?
International Journal of Transla-tion, 16(2):5?33.Lucia Specia.
2011.
Exploiting objective annotationsfor measuring translation post-editing effort.
In Pro-ceedings of the 15th Conference of the EuropeanAssociation for Machine Translation, pages 73?80,Leuven, Belgium.J?org Tiedemann.
2012.
Parallel data, tools and inter-faces in opus.
In Proceedings of the Eight Interna-tional Conference on Language Resources and Eval-uation (LREC?12), Istanbul, Turkey.
European Lan-guage Resources Association (ELRA).Christoph Tillmann, Stephan Vogel, Hermann Ney,Alexander Zubiaga, and Hassan Sawaf.
1997.
Ac-celerated dp based search for statistical translation.In European Conference on Speech Communicationand Technology, EUROSPEECH 1977, pages 2667?2670.98
