Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 155?163,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsStructured and Unstructured Cache Models for SMT Domain AdaptationAnnie LouisSchool of InformaticsUniversity of Edinburgh10 Crichton StreetEdinburgh EH8 9ABalouis@inf.ed.ac.ukBonnie WebberSchool of InformaticsUniversity of Edinburgh10 Crichton StreetEdinburgh EH8 9ABbonnie@inf.ed.ac.ukAbstractWe present a French to English transla-tion system for Wikipedia biography ar-ticles.
We use training data from out-of-domain corpora and adapt the systemfor biographies.
We propose two formsof domain adaptation.
The first biasesthe system towards words likely in biogra-phies and encourages repetition of wordsacross the document.
Since biographies inWikipedia follow a regular structure, oursecond model exploits this structure as asequence of topic segments, where eachsegment discusses a narrower subtopic ofthe biography domain.
In this structuredmodel, the system is encouraged to usewords likely in the current segment?s topicrather than in biographies as a whole.We implement both systems using cache-based translation techniques.
We showthat a system trained on Europarl and newscan be adapted for biographies with 0.5BLEU score improvement using our mod-els.
Further the structure-aware model out-performs the system which treats the entiredocument as a single segment.1 IntroductionThis paper explores domain adaptation of statisti-cal machine translation (SMT) systems to contextswhere the target documents have predictable reg-ularity in topic and document structure.
Regular-ities can take the form of high rates of word rep-etition across documents, similarities in sentencesyntax, similar subtopics and discourse organiza-tion.
Domain adaptation for such documents canexploit these similarities.
In this paper we focuson topic (lexical) regularities in a domain.
Wepresent a system that translates Wikipedia biogra-phies from French to English by adapting a systemtrained on Europarl and news commentaries.
Thistask is interesting for the following two reasons.Many techniques for SMT domain adaptionhave focused on rather diverse domains such as us-ing systems trained on Europarl or news to trans-late medical articles (Tiedemann, 2010a), blogs(Su et al., 2012) and transcribed lectures (Federicoet al., 2012).
The main challenge for such systemsis translating out-of-vocabulary words (Carpuat etal., 2012).
In contrast, words in biographies arecloser to a training corpus of news commentariesand parlimentary proceedings and allow us to ex-amine how well domain adaptation techniques candisambiguate lexical choices.
Such an analysis isharder to do on very divergent domains.In addition, biographies have a fairly regu-lar discourse structure: a central entity (personwho is the topic of the biography), recurringsubtopics such as ?childhood?, ?schooling?, ?ca-reer?
and ?later life?, and a likely chronologicalorder to these topics.
These regularities becomemore predictable in documents from sources suchas Wikipedia.
This setting allows us to explore theutility of models which make translation decisionsdepending on the discourse structure.
Translationmethods for structured documents have only re-cently been explored in Foster et al.
(2010).
How-ever, their system was developed for parlimentaryproceedings and translations were adapted usingseparate language models based upon the identityof the speaker, text type (questions, debate, etc.
)and the year when the proceedings took place.Biographies constitute a more realistic discoursecontext to develop structured models.This paper introduces a new corpus consistingof paired French-English translations of biographyarticles from Wikipedia.1We translate this cor-pus by developing cache-based domain adaptationmethods, a technique recently proposed by Tiede-1Corpus available at http://homepages.inf.ed.ac.uk/alouis/wikiBio.html.155mann (2010a).
In such methods, cache(s) can befilled with relevant items for translation and trans-lation hypotheses that match a greater number ofcache items are scored higher.
These cache scoresare used as additional features during decoding.We use two types of cache?one which encour-ages the use of words more indicative of the biog-raphy domain and another which encourages wordrepetition in the same document.We also show how cache models allowfor straightforward implementation of structuredtranslation by refreshing the cache in response totopic segment boundaries.
We fill caches withwords relevant to the topic of the current segmentwhich is being translated.
The cache contents areobtained from an unsupervised topic model whichinduces clusters of words that are likely to ap-pear in the same topic segment.
Evaluation re-sults show that cache-based models give upto 0.5BLEU score improvements over an out-of-domainsystem.
In addition, models that take topical struc-ture into account score 0.3 BLEU points higherthan those which ignore discourse structure.2 Related workThe study that is closest to our work is that ofTiedemann (2010a), which proposed cache mod-els to adapt a Europarl-trained system to medicaldocuments.
The system used caching in two ways:a cache-based language model (stores target lan-guage words from translations of preceding sen-tences in the same document) and a cache-basedtranslation model (stores phrase pairs from pre-ceding sentence translations).
These caches en-couraged the system to imitate the ?consistency?aspect of domain-specific texts i.e., the propertythat words or phrases are likely to be repeated in adomain and within the same document.Cache models developed in later work, Tiede-mann (2010b) and Gong et al.
(2011), were ap-plied for translating in-domain documents.
Gonget al.
(2011) introduced additional caches to store(i) words and phrase pairs from training docu-ments most similar to a current source article,and (ii) words from topical clusters created on thetraining set.
However, a central issue in these sys-tems is that caches become noisy over time, sincethey ignore topic shifts in the documents.
This pa-per presents cache models which not only take ad-vantage of likely words in the domain and consis-tency, but which also adapt to topic shifts.A different line of work very relevant to ourstudy is the creation of topic-specific translationsby either inferring a topic for the source documentas a whole, or at the other extreme, finer topics forindividual sentences (Su et al., 2012; Eidelman etal., 2012).
Neither of these granularities seem in-tuitive in natural discourse.
In this work, we pro-pose that tailoring translations to topics associatedwith discourse segments in the article is likely tobe beneficial for two reasons: a) subtopics of suchgranularity can be assumed with reasonable con-fidence to re-occur in documents from the samedomain and b) we can hypothesize that a domainwill have a small number of segment-level topics.3 System adaptation for biographiesWe introduce two types of translation systemsadapted for biographies:General domain models (domain-) that use in-formation about biographies but treat the docu-ment as a whole.Structured models (struct-) that are sensitive totopic segment boundaries and the specific topic ofthe segment currently being translated.We implement both models using caches.
Sincewe do not have parallel corpora for the biographydomain, our caches contain items in the target lan-guage only.
We use two types of caches:Topic cache stores target language words (uni-grams) likely in a particular topic.
Each unigramhas an associated score.Consistency cache favours repetition of words inthe sentences from the same document.
It storestarget language words (unigrams) from the 1-besttranslations of previous sentences in the same doc-ument.
Each word is associated with an age valueand a score.
Age indicates when a word enteredthe cache and introduces a ?decay effect?.
Wordsused in immediately previous sentences have alow age value while higher age values indicatewords from sentences much prior in the document.Scores are inversely proportional to age.Both the types of caches are present in boththe general domain and structured models, but thecache words and scores are computed differently.3.1 A general domain modelThis system seeks to bias translations towardswords which occur often in biography articles.The topic cache is filled with word unigramsthat are more likely to occur in biographies com-156pared to general news documents.
We comparethe words from 1,475 English Wikipedia biogra-phies articles to those in a large collection (64,875articles) of New York Times (NYT) news articles(taken from the NYT Annotated Corpus (Sand-haus, 2008)).
We use a log-likelihood ratio test(Lin and Hovy, 2000) to identify words which oc-cur with significantly higher probability in biogra-phies compared to NYT.
We collect only wordsindicated with 0.0001 significance by the test tobe more likely in biographies.
We rank this set of18,597 words in decreasing order of frequency inthe biography article set and assign to each worda score equal to 1/rank of the word.
These wordswith their associated scores form the contents ofthe topic cache.
In the general domain model,these same words are assumed to be useful for thefull document and so the cache contents remainconstant during translation of the full document.The consistency cache stores words from thetranslations of preceding sentences of the samedocument.
After each sentence is translated, wecollect the words from the 1-best translation andfilter out punctuation marks and out of vocabu-lary words.
The remaining words are assigned anage of 1.
Words already present in the cache havetheir age incremented by one.
The new words withage 1 are added to the cache2and the scores forall cache words are recomputed as e1/age.
Theage therefore gets incremented as each sentence?swords are inserted into the cache creating a decay.The cache is cleared at the end of each document.During decoding, a candidate phrase is split intounigrams and checked against each cache.
Scoresfor matching unigrams are summed up to obtain ascore for the phrase.
Separate scores are computedfor matches with the topic and consistency caches.3.2 A structured modelHere we consider topic and consistency at a nar-rower level?within topic segments of the article.The topic cache is filled with words likely inindividual topic segments of an article.
To do this,we need to identify the topic of smaller segmentsof the article and also store a set of most probablewords for each topic.
The topics should also havebilingual mappings which will allow us to infer forevery French document segment, words that arelikely in such a segment in the English language.We designed and implemented an unsupervised2If the word already exists in the cache, it is first removed.topic model based on Latent Dirichlet Allocation(LDA) (Blei et al., 2003) to induce such word clus-ters.
In a first step, we induce subtopics frommonolingual articles in English and French sep-arately.
The topics are subsequently aligned be-tween the languages as explained below.In the first step, we learn a topic model whichincorporates two main ideas a) adds sensitivityto topic boundaries by assigning a single topicper topic segment b) allows for additional flex-ibility by not only drawing the words of a seg-ment from the segment-level topic, but also al-lows some words to be either specific to the doc-ument (such as named entities) or stop words.
Toaddress idea b), we have a ?switching variable?to switch between document-specific word, stop-word or domain-words.The generative story to create a monolingualdataset of biographies is as follows:?
Draw a distribution ?
for the proportion of thethree word types in the full corpus (domainsubtopic, document-specific, stopwords) ?Dirichlet(?)?
For each domain subtopic ?l, 1 ?
l ?
T ,draw a distribution over word vocabulary ?Dirichlet(?)?
Draw a distribution ?
over word vocabularyfor stopwords ?
Dirichlet()?
For each document Di:?
Draw a distribution piiover vocab-ulary for document-specific words ?Dirichlet(?)?
Draw a distribution ?igiving the mix-ture of domain subtopics for this docu-ment ?
Dirichlet(?)?
For each topic segment Mijin Di:?
Draw a domain subtopic zij?Multinomial(?i)?
For each word wijkin segment Mij:?
Draw a word type sijk?Multinomial(?)?
Depending on the chosen switchvalue sijk, draw the word fromthe subtopic of the segment ?zijor document-specific vocabularypii, or stopwords ?We use the section markings in the Wikipediaarticles as topic segment boundaries while learn-ing the model.
We use symmetric Dirichlet priors157for the vocabulary distributions associated withdomain subtopics, document-specific words andstopwords.
The concentration parameters are setto 0.001 to encourage sparsity.
The distribution?ifor per-document subtopics is also drawn froma symmetric Dirichlet distribution with concentra-tion parameter 0.01.
We use asymmetric Dirich-let priors for ?
set to (5, 3, 2) for (domain topic,document-specific, stopwords).
The hyperparam-eter values were minimally tuned so that the differ-ent vocabulary distributions behaved as intended.We perform inference using collapsed Gibbssampling where we integrate out many multinomi-als.
The sampler chooses a topic zijfor every seg-ment and then samples a word type sijkfor eachword in the segment.
We initialize these variablesrandomly and the assignment after 1000 Gibbs it-erations are taken as the final ones.
We createthese models separately for English and French,in each case obtaining T domain subtopics.The second step creates an alignment betweenthe source and target topics using a bilingual dic-tionary3.
For each French topic, we find the topmatching English topic by scoring the numberof dictionary matches.
It is unlikely for everyFrench topic to have a closely corresponding En-glish topic.
Based on observations about the qual-ity of topic alignment, we select the top 60% (outof T ) pairs of French-English aligned topics only.Note that our method uses two steps to learnbilingual topics in contrast to some multilingualtopic models which learn aligned topics directlyfrom parallel or comparable corpora (Zhao andXing, 2006; Boyd-Graber and Blei, 2009; Jagar-lamudi and Daum?e III, 2010).
These methods in-duce topic-specific translations of words.
Ratherwe choose a less restrictive pairing of word clus-ters by topic since (i) we have monolingual bi-ographies in the two languages which could bequite heterogenous in the types of personalitiesdiscussed, (ii) we seek to identify words likely in atopic segment for example ?career-related?
wordsrather than specific translations for source words.During translation, for each topic segment in thesource document, we infer the French topic mostlikely to have produced the segment and find thecorresponding English-side topic.
The most prob-able words for that English topic are then loadedinto the topic cache.
The score for a word is itsprobability in that topic.
When a topic segment3A filtered set of 13,400 entries from www.dict.ccboundary is reached, the topic cache is cleared andthe topic words for the new segment are filled.The consistency cache?s contents are computedsimilarly to the general domain case.
However, thecache gets cleared at segment boundaries.4 Training and test dataWe distinguish two resources for data.
The out-of-domain system is trained using the WMT?12datasets comprising Europarl and news commen-tary texts.
It has 2,144,820 parallel French-English sentence pairs.
The language model istrained using the English side of the training cor-pus.
The tuning set has 2,489 sentence pairs.Our test set is a corpus of French to En-glish translations of biographies compiled fromWikipedia.
To create the biography corpus, wecollect articles which are marked with a ?Trans-lation template?
in Wikipedia metadata.
Thesemarkings indicate a page which is translated froma corresponding page in a different language andalso contains a link to the source article.
(Notethat these article pairs are not those written onthe same topic separately in the two languages.
)We collect pairs of French-English pages with thistemplate and filter those which do not belong tothe Biography topic (using Wikipedia metadata).Note, however, that these article pairs are notvery close translations.
During translation an edi-tor may omit or add information and also reorga-nize parts of the article.
So we filter out the paireddocuments which differ significantly in length.
Weuse LFAligner4to create sentence alignments forthe remaining document pairs.
We constrain thealignments to be within documents but since sec-tion headings were not maintained in translations,we did not further constrain alignments within sec-tions.
We manually corrected the resulting align-ments and keep only documents which have goodalignments and have manually marked topic seg-ments (Wikipedia section headings).
Unalignedsentences were filtered out.
Table 1 shows a sum-mary of this data and the split for tuning and test.The articles are 12 to 87 sentences long and con-tain 5 topic segments on average.We also collect a larger set of monolingualFrench and English Wikipedia biographies to cre-ate the domain subtopics.
We select only articlesthat have at least 10 segments (sections) to ensure4http://sourceforge.net/projects/aligner/158Tuning TestNo.
of article pairs 15 30Total sentences pairs 430 1008Min.
article size (in sentences) 13 12Max.
article size (in sentences) 59 85Average no.
of segments per article 4.7 5.3Table 1: Summary of Wikipedia biographies datathat they are comprehensive ones.
This collectioncontains 1000 French and 1000 English articles.5 Experimental settingsWe use the Moses phrase-based translation system(Koehn et al., 2007) to implement our models.5.1 Out-of-domain modelThis baseline model is trained on the WMT 2012training sets described in the previous section anduses the six standard features from Koehn et al.(2003).
We build a 5-gram language model us-ing SRILM.
The features were tuned using MERT(Och, 2003) on the WMT 2012 tuning sets.
Thissystem does not use any data about biographies.5.2 Biography-adapted modelsFirst we perform experiments using the manuallymarked sections in Wikipedia as topic segments.We also report results with automatic segmenta-tion in Section 7.The domain and structured models have two ex-tra features ?topic cache?
and ?consistency cache?.For the structured model, topic segment bound-aries and inferred topic is passed as XML markupon the source documents.
For the consistencycache, we use a wrapper which passes the 1-best translation (also using XML markup) of thepreceding sentence and updates the cache beforetranslating every next sentence.We tune the weights for these new cache fea-tures as follows.
The weights for the baseline fea-tures from the out-of-domain model are kept con-stant.
The weights for the new cache features areset using a grid search.
This tuning uses the bi-ographies documents listed in Table 1 as tuningdata.
We run the decoding using the baseline fea-ture weights and a weight for a cache feature andcompute the (case-insensitive) BLEU (Papineni etal., 2002) scores of each tuning document.
Theweight for the cache feature which maximizes theaverage BLEU value over the tuning documentsis chosen.
We have not tuned the features us-ing MERT in this study since a grid search al-lowed us to quantify the influence of increasingFigure 1: Effect of feature weights and number oftopics on accuracy for structured topic cacheweights on the new features directly.
Previouswork has noted that MERT fails to find good set-tings for cache models (Tiedemann, 2010b).
Infuture work, we will explore how successful op-timization of baseline and cache feature weightscould be done jointly.
We present the findingsfrom our grid search below.The struct-topic cache has two parameters, thenumber of topics T and the number of most prob-able words from each topic which get loaded intothe cache.
We ran the tuning for T = 25, 50,100 and 200 topics (note that 60% of the topicswill be kept after bilingual alignment, see Section3.2).
We also varied the number of topic wordschosen?50, 100, 250 and 500.The performance did not vary with the numberof topic words used and 50 words gave the sameperformance as 500 words for topic models withany number of topics.
This interesting result sug-gests that only the most likely and basic wordsfrom each topic are useful.
The top 50 words fromtwo topics (one capturing early life and the otheran academic career) taken from the 50-topic modelon English biographies are shown in Table 2.In Figure 1, we show the performance of sys-tems using different number of topics.
In eachcase, the same number of topic words (50) wasadded to the cache.
We find that 50 topics modelperforms best confirming our hypothesis that onlya small number of domain subtopics is plausible.We choose the 50 topic model with top 50 wordsfor each topic for the structured topic cache.The best weights and average document levelBLEU scores on the tuning set are given in Table3.
The scores were computed using the mteval-v13a.pl script in Moses.
BLEU scores for the159his a s family on life She child St motherof in married children They death became whom friends attendedand had He that daughter son marriage lived later workto was born died wife years met couple I agehe her at she father home moved about husband houseof is He received has included National original Academy Frenchand The by its used works study list book Collegehis work Award Medal award His Institute life contributed Yearin he are awarded also title Arts Royal edition awardss University Prize Society A honorary Library include Sciences recognitionTable 2: Top 50 words from 2 topics of the T = 50 topic modelCache type weight BLEU-docDomain-topic 0.075 19.79Domain-consistency 0.05 19.70Domain-topic + consis.
0.05, 0.05 19.80Struct-topic (50 topics) 1.75 19.94Struct-consistency 0.125 19.70Struct-topic + consis.
0.4, 0.1 19.84Domain-consis.
+ struct-topic 0.1, 0.25 19.86Out-of-domain 19.33Table 3: Best weights for cache features andBLEU scores (averaged for tuning documents).out-of-domain model are shown on the last line.Note that these scores are overall on a lower scalefor a French-English system due to out-of-domaindifferences and because the reference translationsfrom Wikipedia are not very close ones.These numbers show that cache models have thepotential to provide better translations comparedto an out-of-domain baseline.
The structured topicmodel system is the best system outperforming theout-of-domain system and also the domain-topicsystem.
Hence, treating documents as composedof topical segments is a useful setting for auto-matic translation.The domain and structured versions of the con-sistency cache however, show no difference.
Thisresult could arise due to the decay factor incor-porated in the consistency cache.
Higher scoresare given to words from immediately previoussentences compared to those far off.
This decayimplicitly gives lower scores to words from ear-lier topic segments than those from recent ones.Explicitly refreshing the cache in the structuredmodel does not give additional benefits.When consistency and topic caches are used to-gether in both general domain and structured set-tings, the combination is not better than individualcaches.
We also tried a setting where the consis-tency cache is document-range and the topic cacheworks at segment level (domain-consis.
+ struct-topic).
This combination also does not outperformusing the structured topic cache alone.Model BLEU-doc BLEU-sentDomain-topic 17.63 17.61Domain-consistency 17.70 17.75Domain-topic + consis.
17.63 17.63Struct-topic (50 topics) 17.76 17.84Struct-consistency 17.33 17.34Struct-topic + consis.
17.47 17.51Struct-topic + dom-consis.
17.29 17.25Out-of-domain 17.37 17.43Table 4: BLEU scores on the test set.
?doc?
in-dicates BLEU scores averaged over documents,?sent?
indicates sentence-level BLEU6 Results on the test corpusThe best weights chosen on the tuning corpus areused to decode the biographies test corpus (sum-marized in Table 1).
Table 4 reports the av-erage BLEU of documents as well as sentencelevel BLEU scores of the corpus.
We used thepaired bootstrap resampling method (Koehn 2004)to compute significance.The struct-topic model gives the highest im-provement of 0.4 sentence level BLEU over theout-of-domain model.
Struct-topic is also 0.23BLEU points better compared to the domain-topic model confirming the usefulness of model-ing structure regularities.
These improvements atsignificant at 95% confidence level.The second best model is the domain-consistency model (significantly better than out-of-domain model at 90% confidence level).
Butthe performance of this cache decreases in thestructured setting.
Moreover, combinations ofcaches fail to improve over individual caches.One hypothesis for this result is that biogra-phy subtopic words which give good performancein the topic cache differ from the words whichprovide benefits in the consistency cache.
Forexample, words related to named entities andother document-specific content words could beones that are more consistent within the docu-ment.
Then clearing the consistency cache at topicboundaries would remove such words from the160cache leading to low performance of the ?struc-tured?
version.
In our current model, we do notdistinguish between words making up the consis-tency cache.
In future, we plan to experimentwith consistency caches of different ranges andwhich hold different types of words.
This ap-proach would require identifying named entitiesand parts of speech on the automatic translationsof previous sentences, which is likely to be error-prone and so require methods for associating aconfidence measure with the cache words.7 Understanding factors that influencestructured cache modelsThe documents in our test corpus have varyinglengths, number of segments and segment sizes.This section explores the behavior of structuredmodels on these different document types.
Forthis analysis, we compare the BLEU scores fromthe domain and the structured versions of the twocaches.
We do not consider the out-of-domain sys-tem here since we are interested in quantifyinggains from using document structure.For each document in our test corpus, we com-pute (i) the difference between the BLEU scoresof struct-topic and domain-topic systems (BLEU-gain-topic), and (ii) the difference in BLEUscores between the struct-consistency and domain-consistency systems (BLEU-gain-consis).
Table 5reports the average BLEU gains binned by a) thedocument length (in sentences) b) number of topicsegments in the document and c) the average sizeof topic segments in a document (in sentences).The numbers clearly indicate that performanceis not uniform across different types of docu-ments.
The struct-topic cache performs much bet-ter on longer documents of over 30 sentences giv-ing 0.3 to 0.4 BLEU points increase compared tothe general domain model.
On the other hand, theperformance worsens when the structured cacheis applied on documents with less than 20 sen-tences.
Similarly, the struct-topic cache is benefi-cial for documents where the average segment sizeis larger than 5 sentences and when the number oftopic segments is around 5 to 7.The struct-consistency cache generally per-forms worse than the unstructured version andthere does not appear to be a niche set accordingto any of the properties?document length, num-ber of segments and segment size.Given these findings, it is possible that thestruct-topic cache can benefit by modifying the(a) Average BLEU gains and document lengthdoc.
length no.
docs gain-topic gain-consis12 to 19 7 -0.41 -0.2020 to 29 10 0.17 -0.6330 to 49 8 0.44 -0.1650 to 85 5 0.34 -0.45(b) Average BLEU gains and no.
of topic segmentsno.
segments no.
docs gain-topic gain-consis3 to 4 9 -0.09 -0.215 13 0.24 -0.376 to 7 5 0.34 -0.749 3 -0.03 -0.26(c) Average BLEU gains and topic segment sizeavg.
segment size no.
docs gain-topic gain-consis< 5 10 -0.23 -0.415 to 10 18 0.33 -0.3711 to 17 2 0.39 -0.24Table 5: Average BLEU score gains from a struc-tured cache (compared to domain caches) split bydifferent properties of documents in the test setdocument structure to match that handled betterby the structured model.
We test this hypothe-sis by segmenting all test documents with an idealsegment size.
The model seems to perform betterwhen each segment has around 5 to 10 sentences(longer segments are also preferred but we havefew very long documents in our corpus), so wetry to re-segment the articles to contain approxi-mately 7 sentences in each segment.
We use anautomatic topic segmentation method (Eisensteinand Barzilay, 2008) to segment the source arti-cles in our test corpus.
For each article we request(document length)/7 segments to be created.5We then run the structured topic and consis-tency models on the automatically segmented cor-pus using the same feature weights as before.
Theresults are shown in Table 6.Model BLEU (doc) BLEU (sent)Struct-topic 17.94 17.94Struct-consistency 17.51 17.46Table 6: Translation performance on automati-cally segmented test corpusThe struct-topic cache now reaches our best re-sult of 0.5 BLEU improvement over the out-of-domain model and 0.3 improvement over the un-structured domain model.
The consistency cacheis also slightly better using the automatic segmen-tation than the manual sections.
Choosing theright granularity appears to be important for struc-tured caches and coarse section headers may notbe ideal.
This result also shows automatic segmen-5Note that we only specify the number of segments, butthe system could create long or short segments.161of (42) he (36) his (36) the (22) to (11) in (9) was (7) one (6) a (3) at (3)head (3) that (3) construction (3) empire office french bases reconstruction only suchall ban marseille main charged have well researchers openness retreatan two mechanical events army iron class surrender order thirtyand black objectives factory disciple largest close budget part timeas who ceremony figure majority level even sentence project trainedon seat diplomatic wheat working winner life archaeological 9 duringTable 7: Impact words computed on the test corpus.
The number of times each word was found in theimpact list is indicated within parentheses.
Words listed without parentheses appeared once in the list.
(1) (S) Pendant la Premi`ere Guerre mondiale, mobilis?e dans les troupes de marine, il combat dans les Balkans et lesDardanelles.
(R) During the First World War, conscripted into the navy, he fought in the Balkans and the Dardanelles.
(B) During World War I, mobilized in troops navy, it fight in the Balkans and Dardanelles.
(C) During World War I, mobilized troops in the navy, he fight in the Balkans and the Dardanelles.
(2) (S)`A l?
?age de 15 ans, elle a ?et?e choisie par la troupe d?op?era de l?arm?ee chinoise pour ?etre form?ee au chant.
(R) At the age of 15, she was selected by the Chinese Armys Operatic troupe to be trained as a singer.
(B) In the age of 15 years, she was chosen by the pool of opera of the Chinese military to be formed the call.
(C) In the age of 15 years, she was chosen by the pool of opera of the Chinese military to be trained to call.
(3) (S) La figure de la Corriveau n?a cess?e, depuis, d?inspirer romans, chansons et pi`eces de th?e?atre et d?alimenter lescontroverses.
(R) The figure of Corriveau still inspires novels, songs and plays and is the subject of argument.
(B) The perceived the Corriveau has stopped, since, inspire novels, songs and parts of theater and fuel controversies.
(C) The figure of the Corriveau has stopped, since, inspire novels, songs and parts of theater and fuel controversies.Table 8: Three examples of impact words in test translations.
Abbreviations: S - source sentence, R -reference translation, B - baseline translation, C - structured topic cache translationtation can be successfully used in these models.8 Changes made by the cache modelsHere we examine the kinds of changes made bythe cache models which have lead to the im-proved BLEU scores.
We focus on the the topiccache since its changes are straightforward tocompute compared to consistency.
We analyzethe struct-topic cache translations on automati-cally segmented documents as that provided thebest performance overall.To do this analysis, we define the notion of animpact word.
An impact word is one which satis-fies three conditions: (i) the word is not present inthe out-of-domain translation of a sentence, (ii) itis present in the translation produced by the topiccache model (iii) the word matches the referencetranslation for the sentence.These impact words provide a simple (albeit ap-proximate) way to analyze useful changes madeby the topic cache over the out-of-domain system.On the test corpus (30 documents), 231 impactword tokens were found and they come from 70unique word types.
So topic cache model signif-icantly affects translation decisions and over 200useful word changes were made in the 30 doc-uments.
The impact word types and counts areshown in Table 7.
Several of these changes relateto function words and pronouns.
For example, thepronoun ?he?
and the past tense verb ?was?
werecorrectly introduced in several sentences such asExample (1) in Table 8.
A content word change isindicated in examples (2) and (3).
These changesappear to be appropriate for biographies.9 ConclusionsWe have introduced a new corpus of biographytranslations which we propose as suitable for ex-amining discourse-motivated SMT methods.
Weshowed that cache-based techniques which alsotake the topic organization into account, makemore appropriate lexical choices for the domain.In future work, we plan to explore how other do-main similarities such as sentence syntax and en-tity reference, for example biographies have a cen-tral entity (person), can be used to improve transla-tion performance.
We also plan to take advantageof recent methods to do document level decoding(Hardmeier et al., 2012).AcknowledgementsThe first author was supported by a Newton Inter-national Fellowship (NF120479) from the RoyalSociety and The British Academy.
We also thankthe NLP group at Edinburgh for their commentsand feedback on this work.162ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet allocation.
the Journal of ma-chine Learning research, 3:993?1022.Jordan Boyd-Graber and David M. Blei.
2009.
Multi-lingual topic models for unaligned text.
In Proceed-ings of UAI, pages 75?82.Marine Carpuat, Hal Daum?e III, Alexander Fraser,Chris Quirk, Fabienne Braune, Ann Clifton, AnnIrvine, Jagadeesh Jagarlamudi, John Morgan, Ma-jid Razmara, Ale?s Tamchyna, Katharine Henry, andRachel Rudinger.
2012.
Domain adaptation in ma-chine translation: Final report.
In 2012 Johns Hop-kins Summer Workshop Final Report.Vladimir Eidelman, Jordan Boyd-Graber, and PhilipResnik.
2012.
Topic models for dynamic translationmodel adaptation.
In Proceedings of ACL, pages115?119.Jacob Eisenstein and Regina Barzilay.
2008.
Bayesianunsupervised topic segmentation.
In Proceedings ofEMNLP, pages 334?343.Marcello Federico, Mauro Cettolo, Luisa Bentivogli,Michael Paul, and Sebastian Stueker.
2012.Overview of the IWSLT 2012 evaluation campaign.Proceedings of IWSLT.George Foster, Pierre Isabelle, and Roland Kuhn.2010.
Translating structured documents.
In Pro-ceedings of AMTA.Zhengxian Gong, Min Zhang, and Guodong Zhou.2011.
Cache-based document-level statistical ma-chine translation.
In Proceedings of EMNLP, pages909?919.Christian Hardmeier, Joakim Nivre, and J?org Tiede-mann.
2012.
Document-wide decoding for phrase-based statistical machine translation.
In Proceed-ings of the EMNLP-CoNLL, pages 1179?1190.Jagadeesh Jagarlamudi and Hal Daum?e III.
2010.
Ex-tracting multilingual topics from unaligned compa-rable corpora.
In Advances in Information Retrieval,Lecture Notes in Computer Science, pages 444?456.Springer Berlin Heidelberg.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of NAACL-HLT, pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:open source toolkit for statistical machine transla-tion.
In Proceedings of the ACL meeting on Interac-tive Poster and Demonstration Sessions, pages 177?180.Chin-Yew Lin and Eduard Hovy.
2000.
The automatedacquisition of topic signatures for text summariza-tion.
In Proceedings of COLING, pages 495?501.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofACL, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automaticevaluation of machine translation.
In Proceedingsof ACL, pages 311?318.Evan Sandhaus.
2008.
The New York Times Anno-tated Corpus.
Corpus number LDC2008T19, Lin-guistic Data Consortium, Philadelphia.Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,Xiaodong Shi, Huailin Dong, and Qun Liu.
2012.Translation model adaptation for statistical machinetranslation with monolingual topic information.
InProceedings of ACL, pages 459?468.J?org Tiedemann.
2010a.
Context adaptation in statisti-cal machine translation using models with exponen-tially decaying cache.
In Proceedings of the 2010Workshop on Domain Adaptation for Natural Lan-guage Processing.J?org Tiedemann.
2010b.
To cache or not to cache?
:experiments with adaptive models in statistical ma-chine translation.
In Proceedings of the Joint FifthWorkshop on Statistical Machine Translation andMetricsMATR, pages 189?194.Bing Zhao and Eric P. Xing.
2006.
Bitam: bilingualtopic admixture models for word alignment.
In Pro-ceedings of the COLING-ACL, pages 969?976.163
