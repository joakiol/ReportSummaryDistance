Proceedings of the 12th Conference of the European Chapter of the ACL, pages 86?93,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsSyntactic Phrase Reordering for English-to-Arabic Statistical MachineTranslationIbrahim Badr Rabih ZbibComputer Science and Artificial Intelligence LabMassachusetts Institute of TechnologyCambridge, MA 02139, USA{iab02, rabih, glass}@csail.mit.eduJames GlassAbstractSyntactic Reordering of the source lan-guage to better match the phrase struc-ture of the target language has beenshown to improve the performance ofphrase-based Statistical Machine Transla-tion.
This paper applies syntactic reorder-ing to English-to-Arabic translation.
It in-troduces reordering rules, and motivatesthem linguistically.
It also studies the ef-fect of combining reordering with Ara-bic morphological segmentation, a pre-processing technique that has been shownto improve Arabic-English and English-Arabic translation.
We report on results inthe news text domain, the UN text domainand in the spoken travel domain.1 IntroductionPhrase-based Statistical Machine Translation hasproven to be a robust and effective approach tomachine translation, providing good performancewithout the need for explicit linguistic informa-tion.
Phrase-based SMT systems, however, havelimited capabilities in dealing with long distancephenomena, since they rely on local alignments.Automatically learned reordering models, whichcan be conditioned on lexical items from both thesource and the target, provide some limited re-ordering capability when added to SMT systems.One approach that explicitly deals with longdistance reordering is to reorder the source sideto better match the target side, using predefinedrules.
The reordered source is then used as inputto the phrase-based SMT system.
This approachindirectly incorporates structure information sincethe reordering rules are applied on the parse treesof the source sentence.
Obviously, the same re-ordering has to be applied to both training data andtest data.
Despite the added complexity of parsingthe data, this technique has shown improvements,especially when good parses of the source side ex-ist.
It has been successfully applied to German-to-English and Chinese-to-English SMT (Collins etal., 2005; Wang et al, 2007).In this paper, we propose the use of a similarapproach for English-to-Arabic SMT.
Unlike mostother work on Arabic translation, our work is inthe direction of the more morphologically com-plex language, which poses unique challenges.
Wepropose a set of syntactic reordering rules on theEnglish source to align it better to the Arabic tar-get.
The reordering rules exploit systematic differ-ences between the syntax of Arabic and the syntaxof English; they specifically address two syntac-tic constructs.
The first is the Subject-Verb orderin independent sentences, where the preferred or-der in written Arabic is Verb-Subject.
The sec-ond is the noun phrase structure, where many dif-ferences exist between the two languages, amongthem the order of adjectives, compound nounsand genitive constructs, as well as the way defi-niteness is marked.
The implementation of theserules is fairly straightforward since they are ap-plied to the parse tree.
It has been noted in previ-ous work (Habash, 2007) that syntactic reorderingdoes not improve translation if the parse quality isnot good enough.
Since in this paper our sourcelanguage is English, the parses are more reliable,and result in more correct reorderings.
We showthat using the reordering rules results in gains inthe translation scores and study the effect of thetraining data size on those gains.This paper also investigates the effect of usingmorphological segmentation of the Arabic target86in combination with the reordering rules.
Mor-phological segmentation has been shown to benefitArabic-to-English (Habash and Sadat, 2006) andEnglish-to-Arabic (Badr et al, 2008) translation,although the gains tend to decrease with increas-ing training data size.Section 2 provides linguistic motivation for thepaper.
It describes the rich morphology of Arabic,and its implications on SMT.
It also describes thesyntax of the verb phrase and noun phrase in Ara-bic, and how they differ from their English coun-terparts.
In Section 3, we describe some of the rel-evant previous work.
In Section 4, we present thepreprocessing techniques used in the experiments.Section 5 describes the translation system, the dataused, and then presents and discusses the experi-mental results from three domains: news text, UNdata and spoken dialogue from the travel domain.The final section provides a brief summary andconclusion.2 Arabic Linguistic Issues2.1 Arabic MorphologyArabic has a complex morphology compared toEnglish.
The Arabic noun and adjective are in-flected for gender and number; the verb is inflectedin addition for tense, voice, mood and person.Various clitics can attach to words as well: Con-junctions, prepositions and possessive pronounsattach to nouns, and object pronouns attach toverbs.
The example below shows the decompo-sition into stems and clitics of the Arabic verbphrase wsyqAblhm1 and noun phrase wbydh, bothof which are written as one word:(1) a. w+ands+willyqAblmeet-3SM+hmthemand he will meet themb.
w+andb+withydhand+hhisand with his handAn Arabic corpus will, therefore, have moresurface forms than an equivalent English corpus,and will also be sparser.
In the LDC news corporaused in this paper (see Section 5.2), the averageEnglish sentence length is 33 words compared tothe Arabic 25 words.1All examples in this paper are writ-ten in the Buckwalter Transliteration System(http://www.qamus.org/transliteration.htm)Although the Arabic language family consistsof many dialects, none of them has a standardorthography.
This affects the consistency of theorthography of Modern Standard Arabic (MSA),the only written variety of Arabic.
Certain char-acters are written inconsistently in different datasources: Final ?y?
is sometimes written as ?Y?
(AlifmqSwrp), and initial Alif hamza (The Buckwal-ter characters ?<?
and ?{?)
are written as bare alif(A).
Arabic is usually written without the diacriticsthat denote short vowels.
This creates an ambigu-ity at the word level, since a word can have morethan one reading.
These factors adversely affectthe performance of Arabic-to-English SMT, espe-cially in the English-to-Arabic direction.Simple pattern matching is not enough to per-form morphological analysis and decomposition,since a certain string of characters can, in princi-ple, be either an affixed morpheme or part of thebase word itself.
Word-level linguistic informationas well as context analysis are needed.
For exam-ple the written form wly can mean either ruler orand for me, depending on the context.
Only in thelatter case should it be decomposed.2.2 Arabic SyntaxIn this section, we describe a number of syntacticfacts about Arabic which are relevant to thereordering rules described in Section 4.2.Clause StructureIn Arabic, the main sentence usually hasthe order Verb-Subject-Object (VSO).
The orderSubject-Verb-Object (SVO) also occurs, but is lessfrequent than VSO.
The verb agrees with the sub-ject in gender and number in the SVO order, butonly in gender in the VSO order (Examples 2c and2d).
(2) a. Aklate-3SMAlwldthe-boyAltfAHpthe-applethe boy ate the appleb.
Alwldthe-boyAklate-3SMAltfAHpthe-applethe boy ate the applec.
Aklate-3SMAlAwlAdthe-boysAltfAHAtthe-applesthe boys ate the applesd.
AlAwlAdthe-boysAklwAate-3PMAltfAHAtthe-applesthe boys ate the apples87In a dependent clause, the order must be SVO,as illustrated by the ungrammaticality of Exam-ple 3b below.
As we discuss in more detail later,this distinction between dependent and indepen-dent clauses has to be taken into account when thesyntactic reordering rules are applied.
(3) a. qAlsaid-3SMAnthatAlwldthe-boyAklateAltfAHpthe-applehe said that the boy ate the appleb.
*qAlsaid-3SMAnthatAklateAlwldthe-boyAltfAHpthe-applehe said that the boy ate the appleAnother pertinent fact is that the negation parti-cle has to always preceed the verb:(4) lmnotyAkleat-3SMAlwldthe-boyAltfAHpthe-applethe boy did not eat the appleNoun PhraseThe Arabic noun phrase can have constructsthat are quite different from English.
The adjectivein Arabic follows the noun that it modifies, and itis marked with the definite article, if the head nounis definite:(5) AlbAbthe-doorAlkbyrthe-bigthe big doorThe Arabic equivalent of the English posses-sive, compound nouns and the of -relationship isthe Arabic idafa construct, which compounds twoor more nouns.
Therefore, N1?s N2 and N2 of N1are both translated as N2 N1 in Arabic.
As Exam-ple 6b shows, this construct can also be chainedrecursively.
(6) a. bAbdoorAlbytthe-housethe house?s doorb.
mftAHkeybAbdoorAlbytthe-houseThe key to the door of the houseExample 6 also shows that an idafa construct ismade definite by adding the definite article Al- tothe last noun in the noun phrase.
Adjectives followthe idafa noun phrase, regardless of which noun inthe chain they modify.
Thus, Example 7 is am-biguous in that the adjective kbyr (big) can modifyany of the preceding three nouns.
The same is truefor relative clauses that modify a noun.
(7) mftAHkeybAbdoorAlbytthe-houseAlkbyrthe-bigThese and other differences between the Arabicand English syntax are likely to affect the qual-ity of automatic alignments, since correspondingwords will occupy positions in the sentence thatare far apart, especially when the relevant words(e.g.
the verb and its subject) are separated by sub-ordinate clauses.
In such cases, the lexicalized dis-tortion models used in phrase-based SMT do nothave the capability of performing reorderings cor-rectly.
This limitation adversely affects the trans-lation quality.3 Previous WorkMost of the work in Arabic machine translationis done in the Arabic-to-English direction.
Theother direction, however, is also important, sinceit opens the wealth of information in different do-mains that is available in English to the Arabicspeaking world.
Also, since Arabic is a morpho-logically richer language, translating into Arabicposes unique issues that are not present in theopposite direction.
The only works on English-to-Arabic SMT that we are aware of are Badr etal.
(2008), and Sarikaya and Deng (2007).
Badret al show that using segmentation and recom-bination as pre- and post- processing steps leadsto significant gains especially for smaller train-ing data corpora.
Sarikaya and Deng use JointMorphological-Lexical Language Models to re-rank the output of an English-to-Arabic MT sys-tem.
They use regular expression-based segmen-tation of the Arabic so as not to run into recombi-nation issues on the output side.Similarly, for Arabic-to-English, Lee (2004),and Habash and Sadat (2006) show that vari-ous segmentation schemes lead to improvementsthat decrease with increasing parallel corpus size.They use a trigram language model and the Ara-bic morphological analyzer MADA (Habash andRambow, 2005) respectively, to segment the Ara-bic side of their corpora.
Other work on Arabic-to-English SMT tries to address the word reorder-ing problem.
Habash (2007) automatically learnssyntactic reordering rules that are then applied tothe Arabic side of the parallel corpora.
The wordsare aligned in a sentence pair, then the Arabic sen-tence is parsed to extract reordering rules based onhow the constituents in the parse tree are reorderedon the English side.
No significant improvement is88shown with reordering when compared to a base-line that uses a non-lexicalized distance reorderingmodel.
This is attributed in the paper to the poorquality of parsing.Syntax-based reordering as a preprocessing stephas been applied to many language pairs otherthan English-Arabic.
Most relevant to the ap-proach in this paper are Collins et al (2005)and Wang et al (2007).
Both parse the sourceside and then reorder the sentence based on pre-defined, linguistically motivated rules.
Signifi-cant gain is reported for German-to-English andChinese-to-English translation.
Both suggest thatreordering as a preprocessing step results in bet-ter alignment, and reduces the reliance on the dis-tortion model.
Popovic and Ney (2006) use sim-ilar methods to reorder German by looking at thePOS tags for German-to-English and German-to-Spanish.
They show significant improvements ontest set sentences that do get reordered as wellas those that don?t, which is attributed to the im-provement of the extracted phrases.
(Xia andMcCord, 2004) present a similar approach, witha notable difference: the re-ordering rules are au-tomatically learned from aligning parse trees forboth the source and target sentences.
They reporta 10% relative gain for English-to-French trans-lation.
Although target-side parsing is optionalin this approach, it is needed to take full advan-tage of the approach.
This is a bigger issue whenno reliable parses are available for the target lan-guage, as is the case in this paper.
More generally,the use of automatically-learned rules has the ad-vantage of readily applicable to different languagepairs.
The use of deterministic, pre-defined rules,however, has the advantage of being linguisticallymotivated, since differences between the two lan-guages are addressed explicitly.
Moreover, the im-plementation of pre-defined transfer rules basedon target-side parses is relatively easy and cheapto implement in different language pairs.Generic approaches for translating from En-glish to more morphologically complex languageshave been proposed.
Koehn and Hoang (2007)propose Factored Translation Models, which ex-tend phrase-based statistical machine translationby allowing the integration of additional morpho-logical features at the word level.
They demon-strate improvements for English-to-German andEnglish-to-Czech.
Tighter integration of fea-tures is claimed to allow for better modeling ofthe morphology and hence is better than usingpre-processing and post-processing techniques.Avramidis and Koehn (2008) enrich the Englishside by adding a feature to the Factored Model thatmodels noun case agreement and verb person con-jugation, and show that it leads to a more gram-matically correct output for English-to-Greek andEnglish-to-Czech translation.
Although FactoredModels are well equipped for handling languagesthat differ in terms of morphology, they still usethe same distortion reordering model as a phrase-based MT system.4 Preprocessing Techniques4.1 Arabic Segmentation and RecombinationIt has been shown previously work (Badr et al,2008; Habash and Sadat, 2006) that morphologi-cal segmentation of Arabic improves the transla-tion performance for both Arabic-to-English andEnglish-to-Arabic by addressing the problem ofsparsity of the Arabic side.
In this paper, we usesegmented and non-segmented Arabic on the tar-get side, and study the effect of the combination ofsegmentation with reordering.As mentioned in Section 2.1, simple patternmatching is not enough to decompose Arabicwords into stems and affixes.
Lexical informationand context are needed to perform the decompo-sition correctly.
We use the Morphological Ana-lyzer MADA (Habash and Rambow, 2005) to de-compose the Arabic source.
MADA uses SVM-based classifiers of features (such as POS, num-ber, gender, etc.)
to score the different analysesof a given word in context.
We apply morpho-logical decomposition before aligning the trainingdata.
We split the conjunction and preposition pre-fixes, as well as possessive and object pronoun suf-fixes.
We then glue the split morphemes into oneprefix and one suffix, such that any given word issplit into at most three parts: prefix+ stem +suffix.Note that plural markers and subject pronouns arenot split.
For example, the word wlAwlAdh (?andfor his children?)
is segmented into wl+ AwlAd+P:3MS.Since training is done on segmented Arabic, theoutput of the decoder must be recombined into itsoriginal surface form.
We follow the approach ofBadr et.
al (2008) in combining the Arabic out-put, which is a non-trivial task for several reasons.First, the ending of a stem sometimes changeswhen a suffix is attached to it.
Second, word end-89ings are normalized to remove orthographic incon-sistency between different sources (Section 2.1).Finally, some words can recombine into more thanone grammatically correct form.
To address theseissues, a lookup table is derived from the trainingdata that maps the segmented form of the word toits original form.
The table is also useful in re-combining words that are erroneously segmented.If a certain word does not occur in the table, weback off to a set of manually defined recombina-tion rules.
Word ambiguity is resolved by pickingthe more frequent surface form.4.2 Arabic Reordering RulesThis section presents the syntax-based rules usedfor re-ordering the English source to better matchthe syntax of the Arabic target.
These rules aremotivated by the Arabic syntactic facts describedin Section 2.2.Much like Wang et al (2007), we parse the En-glish side of our corpora and reorder using prede-fined rules.
Reordering the English can be donemore reliably than other source languages, suchas Arabic, Chinese and German, since the state-of-the-art English parsers are considerably betterthan parsers of other languages.
The followingrules for reordering at the sentence level and thenoun phrase level are applied to the English parsetree:1.
NP: All nouns, adjectives and adverbs in thenoun phrase are inverted.
This rule is moti-vated by the order of the adjective with re-spect to its head noun, as well as the idafaconstruct (see Examples 6 and 7 in Section2.2.
As a result of applying this rule, thephrase the blank computer screen becomesthe screen computer blank .2.
PP: All prepositional phrases of the formN1ofN2 ...ofNn are transformed toN1N2 ...Nn .
All N i are also made indefi-nite, and the definite article is added to Nn ,the last noun in the chain.
For example, thephrase the general chief of staff of the armedforces becomes general chief staff the armedforces.
We also move all adjectives in thetop noun phrase to the end of the construct.So the real value of the Egyptian poundbecomes value the Egyptian pound real.
Thisrule is motivated by the idafa construct andits properties (see Example 6).3. the: The definite article the is replicated be-fore adjectives (see Example 5 above).
So theblank computer screen becomes the blank thecomputer the screen.
This rule is applied af-ter NP rule abote.
Note that we do not repli-cate the before proper names.4.
VP: This rule transforms SVO sentences toVSO.
All verbs are reordered on the condi-tion that they have their own subject nounphrase and are not in the participle form,since in these cases the Arabic subject occursbefore the verb participle.
We also check thatthe verb is not in a relative clause with a thatcomplementizer (Example 3 above).
The fol-lowing example illustrates all these cases: thehealth minister stated that 11 police officerswere wounded in clashes with the demonstra-tors?
stated the health minister that 11 po-lice officers were wounded in clashes with thedemonstrators.
If the verb is negated, thenegative particle is moved with the verb (Ex-ample 4.
Finally, if the object of the reorderedverb is a pronoun, it is reordered with theverb.
Example: the authorities gave us allthe necessary help becomes gave us the au-thorities all the necessary help.The transformation rules 1, 2 and 3 are appliedin this order, since they interact although they donot conflict.
So, the real value of the Egyptianpound ?
value the Egyptian the pound the realThe VP reordering rule is independent.5 Experiments5.1 System descriptionFor the English source, we first tokenize us-ing the Stanford Log-linear Part-of-Speech Tag-ger (Toutanova et al, 2003).
We then proceedto split the data into smaller sentences and tagthem using Ratnaparkhi?s Maximum Entropy Tag-ger (Ratnaparkhi, 1996).
We parse the data us-ing the Collins Parser (Collins, 1997), and thentag person, location and organization names us-ing the Stanford Named Entity Recognizer (Finkelet al, 2005).
On the Arabic side, we normalizethe data by changing final ?Y?
to ?y?, and chang-ing the various forms of Alif hamza to bare Alif,since these characters are written inconsistently insome Arabic sources.
We then segment the datausing MADA according to the scheme explainedin Section 4.1.90The English source is aligned to the seg-mented Arabic target using the standardMOSES (MOSES, 2007) configuration ofGIZA++ (Och and Ney, 2000), which is IBMModel 4, and decoding is done using the phrase-based SMT system MOSES.
We use a maximumphrase length of 15 to account for the increasein length of the segmented Arabic.
We alsouse a lexicalized bidirectional reordering modelconditioned on both the source and target sides,with a distortion limit set to 6.
We tune usingOch?s algorithm (Och, 2003) to optimize weightsfor the distortion model, language model, phrasetranslation model and word penalty over theBLEU metric (Papineni et al, 2001).
For thesegmented Arabic experiments, we experimentwith tuning using non-segmented Arabic as areference.
This is done by recombining the outputbefore each tuning iteration is scored and has beenshown by Badr et.
al (2008) to perform better thanusing segmented Arabic as reference.5.2 Data UsedWe report results on three domains: newswire text,UN data and spoken dialogue from the travel do-main.
It is important to note that the sentencesin the travel domain are much shorter than in thenews domain, which simplifies the alignment aswell as reordering during decoding.
Also, sincethe travel domain contains spoken Arabic, it ismore biased towards the Subject-Verb-Object sen-tence order than the Verb-Subject-Object ordermore common in the news domain.
Also notethat since most of our data was originally intendedfor Arabic-to-English translation, our test and tun-ing sets have only one reference, and therefore,the BLEU scores we report are lower than typi-cal scores reported in the literature on Arabic-to-English.The news training data consists of several LDCcorpora2.
We construct a test set by randomlypicking 2000 sentences.
We pick another 2000sentences randomly for tuning.
Our final trainingset consists of 3 million English words.
We alsotest on the NIST MT 05 ?test set while tuning onboth the NIST MT 03 and 04 test sets.
We use thefirst English reference of the NIST test sets as thesource, and the Arabic source as our reference.
For2LDC2003E05 LDC2003E09 LDC2003T18LDC2004E07 LDC2004E08 LDC2004E11 LDC2004E72LDC2004T18 LDC2004T17 LDC2005E46 LDC2005T05LDC2007T24SchemeRandT MT 05S NoS S NoSBaseline 21.6 21.3 23.88 23.44VP 21.9 21.5 23.98 23.58NP 21.9 21.8NP+PP 21.8 21.5 23.72 23.68NP+PP+VP 22.2 21.8 23.74 23.16NP+PP+VP+The 21.3 21.0Table 1: Translation Results for the News Domainin terms of the BLEU Metric.the language model, we use 35 million words fromthe LDC Arabic Gigaword corpus, plus the Arabicside of the 3 million word training corpus.
Exper-imentation with different language model ordersshows that the optimal model orders are 4-gramsfor the baseline system and 6-grams for the seg-mented Arabic.
The average sentence length is 33for English, 25 for non-segmented Arabic and 36for segmented Arabic.To study the effect of syntactic reordering onlarger training data sizes, we use the UN English-Arabic parallel text (LDC2003T05).
We experi-ment with two training data sizes: 30 million and3 million words.
The test and tuning sets arecomprised of 1500 and 500 sentences respectively,chosen at random.For the spoken domain, we use the BTEC 2007Arabic-English corpus.
The training set consistsof 200K words, the test set has 500 sentences andthe tuning set has 500 sentences.
The languagemodel consists of the Arabic side of the trainingdata.
Because of the significantly smaller datasize, we use a trigram LM for the baseline, anda 4-gram for segmented Arabic.
In this case, theaverage sentence length is 9 for English, 8 for Ara-bic, and 10 for segmented Arabic.5.3 Translation ResultsThe translation scores for the News domain areshown in Table 1.
The notation used in the table isas follows:?
S: Segmented Arabic?
NoS: Non-Segmented Arabic?
RandT: Scores for test set where sentenceswere picked at random from NEWS data?
MT 05: Scores for the NIST MT 05 test setThe reordering notation is explained in Section4.2.
All results are in terms of the BLEU met-91S NoSShort Long Short LongBaseline 22.57 25.22 22.40 24.33VP 22.95 25.05 22.95 24.02NP+PP 22.71 24.76 23.16 24.067NP+PP+VP 22.84 24.62 22.53 24.56Table 2: Translation Results depending on sen-tence length for NIST test set.Scheme Score % Oracle reordVP 25.76 59%NP+PP 26.07 58%NP+PP+VP 26.17 53%Table 3: Oracle scores for combining baseline sys-tem with other reordered systems.ric.
It is important to note that the gain that wereport in terms of BLEU are more significant thatcomparable gains on test sets that have multiplereferences, since our test sets have only one refer-ence.
Any amount of gain is a result of additionaln-gram precision with one reference.
We note thatthe gain achieved from the reordering of the non-segmented and segmented systems are compara-ble.
Replicating the before adjectives hurts thescores, possibly because it increases the sentencelength noticeably, and thus deteriorates the align-ments?
quality.
We note that the gains achieved byreordering on the NIST test set are smaller thanthe improvements on the random test set.
This isdue to the fact that the sentences in the NIST testset are longer, which adversely affects the parsingquality.
The average English sentence length is 33words in the NIST test set, while the random testset has an average sentence length of 29 words.Table 2 shows the reordering gains of the non-segmented Arabic by sentence length.
Short sen-tences are sentences that have less that 40 words ofEnglish, while long sentences have more than 40words.
Out of the 1055 sentence in the NIST testset 719 are short and 336 are long.
We also reportoracle scores in Table 3 for combining the base-line system with the reordering systems, as wellas the percentage of oracle sentences produced bythe reordered system.
The oracle score is com-puted by starting with the reordered system?s can-didate translations and iterating over all the sen-tences one by one: we replace each sentence withits corresponding baseline system translation thenScheme 30M 3MBaseline 32.17 28.42VP 32.46 28.60NP+PP 31.73 28.80Table 4: Translation Results on segmentd UN datain terms of the BLEU Metric.compute the total BLEU score of the entire set.
Ifthe score improves, then the sentence in questionis replaced with the baseline system?s translation,otherwise it remains unchanged and we move onto the next one.In Table 4, we report results on the UN corpusfor different training data sizes.
It is important tonote that although gains from VP reordering stayconstant when scaled to larger training sets, gainsfrom NP+PP reordering diminish.
This is due tothe fact that NP reordering tend to be more local-ized then VP reorderings.
Hence with more train-ing data the lexicalized reordering model becomesmore effective in reordering NPs.In Table 5, we report results on the BTECcorpus for different segmentation and reorderingscheme combinations.
We should first point outthat all sentences in the BTEC corpus are short,simple and easy to align.
Hence, the gain intro-duced by reordering might not be enough to offsetthe errors introduced by the parsing.
We also notethat spoken Arabic usually prefers the Subject-Verb-Object sentence order, rather than the Verb-Subject-Object sentence order of written Arabic.This explains the fact that no gain is observedwhen the verb phrase is reordered.
Noun phrasereordering produces a significant gain with non-segmented Arabic.
Replicating the definite arti-cle the in the noun phrase does not create align-ment problems as is the case with the newswiredata, since the sentences are considerably shorter,and hence the 0.74 point gain observed on the seg-mented Arabic system.
That gain does not trans-late to the non-segmented Arabic system since inthat case the definite article Al remains attached toits head word.6 ConclusionThis paper presented linguistically motivated rulesthat reorder English to look like Arabic.
Weshowed that these rules produce significant gains.We also studied the effect of the interaction be-tween Arabic morphological segmentation and92Scheme S NoSBaseline 29.06 25.4VP 26.92 23.49NP 27.94 26.83NP+PP 28.59 26.42The 29.8 25.1Table 5: Translation Results for the Spoken Lan-guage Domain in the BLEU Metric.syntactic reordering on translation results, as wellas how they scale to bigger training data sizes.AcknowledgmentsWe would like to thank Michael Collins, Ali Mo-hammad and Stephanie Seneff for their valuablecomments.ReferencesEleftherios Avramidis, and Philipp Koehn 2008.
En-riching Morphologically Poor Languages for Statis-tical Machine Translation.
In Proc.
of ACL/HLT.Ibrahim Badr, Rabih Zbib, and James Glass 2008.
Seg-mentation for English-to-Arabic Statistical MachineTranslation.
In Proc.
of ACL/HLT.Michael Collins 1997.
Three Generative, LexicalizedModels for Statistical Parsing.
In Proc.
of ACL.Michael Collins, Philipp Koehn, and Ivona Kucerova2005.
Clause Restructuring for Statistical MachineTranslation.
In Proc.
of ACL.Jenny Rose Finkel, Trond Grenager, and ChristopherManning 2005.
Incorporating Non-local Informa-tion into Information Extraction Systems by GibbsSampling.
In Proc.
of ACL.Nizar Habash, 2007.
Syntactic Preprocessing for Sta-tistical Machine Translation.
In Proc.
of the Ma-chine Translation Summit (MT-Summit).Nizar Habash and Owen Rambow, 2005.
Arabic Tok-enization, Part-of-Speech Tagging and Morphologi-cal Disambiguation in One Fell Swoop.
In Proc.
ofACL.Nizar Habash and Fatiha Sadat, 2006.
Arabic Pre-processing Schemes for Statistical Machine Trans-lation.
In Proc.
of HLT.Philipp Koehn and Hieu Hoang, 2007.
FactoredTranslation Models.
In Proc.
of EMNLP/CNLL.Young-Suk Lee, 2004.
Morphological Analysisfor Statistical Machine Translation.
In Proc.
ofEMNLP.MOSES, 2007.
A Factored Phrase-based Beam-search Decoder for Machine Translation.
URL:http://www.statmt.org/moses/.Franz Och 2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proc.
of ACL.Franz Och and Hermann Ney 2000.
Improved Statisti-cal Alignment Models.
In Proc.
of ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu 2001.
BLUE: a Method for AutomaticEvaluation of Machine Translation.
In Proc.
ofACL.Maja Popovic and Hermann Ney 2006.
POS-basedWord Reordering for Statistical Machine Transla-tion.
In Proc.
of NAACL LREC.Adwait Ratnaparkhi 1996.
A Maximum EntropyModel for Part-of-Speech Tagging.
In Proc.
ofEMNLP.Ruhi Sarikaya and Yonggang Deng 2007.
JointMorphological-Lexical Language Modeling for Ma-chine Translation.
In Proc.
of NAACL HLT.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.In Proc.
of NAACL HLT.Chao Wang, Michael Collins, and Philipp Koehn 2007.Chinese Syntactic Reordering for Statistical Ma-chine Translation.
In Proc.
of EMNLP.Fei Xia and Michael McCord 2004.
Improving aStatistical MT System with Automatically LearnedRewrite Patterns.
In COLING.93
