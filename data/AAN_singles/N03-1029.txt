Comma Restoration Using Constituency InformationStuart M. ShieberHarvard Universityshieber@deas.harvard.eduXiaopeng TaoHarvard Universityxptao@deas.harvard.eduAbstractAutomatic restoration of punctuation from un-punctuated text has application in improvingthe fluency and applicability of speech recog-nition systems.
We explore the possibility thatsyntactic information can be used to improvethe performance of an HMM-based system forrestoring punctuation (specifically, commas) intext.
Our best methods reduce sentence errorrate substantially ?
by some 20%, with an ad-ditional 8% reduction possible given improve-ments in extraction of the requisite syntactic in-formation.1 MotivationThe move from isolated word to connected speech recog-nition engendered a qualitative improvement in the nat-uralness of users?
interactions with speech transcriptionsystems, sufficient even to make up in user satisfactionfor some modest increase in error rate.
Nonetheless, suchsystems still retain an important source of unnaturalnessin dictation, the requirement to utter all punctuation ex-plicitly.
In order to free the user from this burden, a tran-scription system would have to reconstruct the punctua-tion from the word sequence.
For certain applications ?for instance, transcription of naturally occurring speechnot originally targeted to a speech recognizer (as broad-cast audio) ?
there is no alternative to performing recon-struction of punctuation.Reconstruction of different punctuation marks is likelyto respond to different techniques.
Reconstruction of pe-riods, question marks, and exclamation marks, for in-stance, is in large part the problem of sentence bound-ary detection.
In this paper, we address the problem ofcomma restoration.
The published literature on intrasen-tence punctuation restoration is quite limited, the state ofthe art represented by Beeferman, Berger, and Lafferty?sCYBERPUNC system, which we review in Section 2, andreimplement as a baseline for our own experiments.
(SeeSection 5 for discussion of related work.
)The CYBERPUNC system uses a simple HMM with tri-gram probabilities to model the comma restoration prob-lem.
It is trained on fully punctuated text, and thentested for precision and recall in reconstructing commasin text that has had them removed.
Our replication of thetrigram-based method yields a sentence accuracy of 47%.However, the role of the comma in text is closelyrelated to syntactic constituency.
Nunberg (1990) de-scribes two main classes of comma: the delimiter comma,which is used to mark off a constituent, and the sepa-rator comma, which is inserted between conjoined ele-ments with or without a conjunction.
In both cases, oneexpects to see commas at the beginning or end of con-stituents, rather than in the middle.
But this type of cor-relation is difficult to model with a flat model such as anHMM.
For this reason, we explore here the use of syn-tactic constituency information for the purpose of commarestoration.
We show that even very rarefied amounts ofsyntactic information can dramatically improve commarestoration performance; our best method accurately re-stores 58% of sentences.
Furthermore, even approximatesyntactic information provides significant improvement.There is, of course, great variation in appropriate punc-tuation of a single word stream.1 For this reason, inde-pendent human annotators consider only about 86% ofthe sentences in the test set to be correct with respectto comma placement (Beeferman et al, 1998).
Thus, amove from 47% to 58% is a quite substantial improve-ment, essentially a reduction in sentence error rate ofsome 30%.1In an old unattributed joke, an English professor asks somestudents to punctuate the word sequence ?Woman without herman is nothing?.
The male students preferred ?Woman, withouther man, is nothing.?
whereas the female proposed ?Woman!Without her, man is nothing.?
No, it?s not funny, but it doesmake the point.Edmonton, May-June 2003Main Papers , pp.
142-148Proceedings of HLT-NAACL 2003Digression: What?s Statistical Parsing Good For?There has been a tremendous amount of research sincethe early 1990?s on the problem of parsing using statisti-cal models and evaluated by statistical measures such ascrossing brackets rate.
Statistical parsing, like languagemodeling, is presumably of interest not in and of itself butrather by virtue of its contribution to some end-user appli-cation.
In the case of language modeling, speech recog-nition is the leading exemplar among a large set of end-user natural-language-processing applications that bene-fit from the technology.
Further, the statistical figures ofmerit are appropriate just insofar as they vary more or lesscontinuously and monotonically with the performance ofthe end-user application.
Again, in the case of languagemodeling, speech recognition error rate is generally ac-knowledged to improve in direct relation to reduction incross-entropy of the language model employed.For statistical parsing, it is much more difficult to saywhat applications actually benefit from this componenttechnology in the sense that incremental improvementsto the technology as measured by the statistical figuresof merit provide incremental benefit to the application.The leading argument for parsing a sentence is that thisestablishes the structure upon which semantic interpreta-tion can be performed.
But it is hard to imagine in whatsense an 85% correct parse is better than an 80% correctparse, as the semantic interpretation generated off of eachis likely to be wrong.
Barring a sensible notion of ?par-tially correct interpretation?
and an end-user applicationin which a partially correct interpretation is partially asgood as a fully correct one, we would not expect statisti-cal parsers to be useful for end user applications based onsentence interpretation.2 In fact, to the authors?
knowl-edge, the comma restoration results presented here are thefirst instance of an end-user application that bears on itsface this crucial property, that incremental improvementon statistical parsing provides incremental improvementin the application.2 The Trigram BaselineAs a baseline, we replicated the three-state HMM methodof Beeferman et al (1998).
In this section, we describethat method, which we use as the basis for our extensions.The input to comma restoration is a sentence x =x1 .
.
.
xn of words and punctuation but no commas.
Wewould like to generate a restored string y = y1 .
.
.
yn+c,which is the string x with c commas inserted.
The se-lected y should maximize conformance with a simple tri-2We might expect nonstatistical parsers also not to be use-ful, but for a different reason, their fragility.
Rather than de-livering partially correct results, they partially deliver correctresults.
But that is a different issue.w?w,w,w_w_w_xi ,xi ,xi ,xixixip(xi | xi-2 xi-1)p(xi | , xi-1)p(xi | xi-2 xi-1) p(, | xi-1 xi)p(xi | xi-1 ,) p(, | , xi)p(xi | , xi-1) p(, | xi-1 xi)p(xi | xi-1 ,)Figure 1: Three-state HMM for decoding a comma-reduced string x1 ?
?
?xn to its comma-restored form.Transitions are labeled with their position-dependentprobabilities.gram model:y?
= argmaxyn+c?i=1p(yi | yi?2yi?1)We take the string x to be the observed output of anHMM with three states and transition probabilities de-pendent on output; the states encode the position of com-mas in a reconstructed string.
Figure 1 depicts the au-tomaton.
The start state (1) corresponds to having seen aword with no prior or following comma, state (2) a wordwith a following comma, and state (3) a word with a priorbut no following comma.
It is easy to see that a paththrough the automaton traverses a string y with probabil-ity?n+ci=1 p(yi | yi?2yi?1).
The decoded string y?
cantherefore be computed by Viterbi decoding.3This method requires a trigram language model p().We train this language model on sections 02?22 of thePenn Treebank Wall Street Journal data (WSJ)4, com-prising about 36,000 sentences.
The CMU StatisticalLanguage Modeling Toolkit (Clarkson and Rosenfeld,1997) was used to generate the model.
Katz smoothingwas used to incorporate lower-order models.
The model3As it turns out, the same computation can be done using atwo-state model.
This automaton does not, however, lend itselfas easily to extensions.4For consistency, we use the version of the Wall Street Jour-nal data that was used by Beeferman et al (1998) for their CY-BERPUNC experiments.
This comprises sections 02?23 of theWall Street Journal (the last of these being used as test data)with minor variations from the Treebank version, for instance,a small number of missing sentences and some variation in thetags.
Runs of the experiments below using the Treebank ver-sions of the data yield essentially identical results.was then tested on the approximately 2300 sentences ofWSJ Section 23.
Precision of the comma restorationwas 71.1% and recall 55.2%.
F-measure, calculated as2PR/(P + R), where P is precision and R recall, is62.2%.
Overall 96.3% of all comma placement decisionswere made correctly, a metric we refer to as token accu-racy.
Sentence accuracy, the percentage of sentences cor-rectly restored, was 47.0%.
(These results are presentedas model 1 in Table 1.)
This is the baseline against whichwe evaluate our alternative comma restoration models.Beeferman et al present an alternative trigram model,which computes the following:y?
= argmaxyn+c?i=1p(yi | yi?2yi?1)(1 ?
p(, | yi?2yi?1))?(yi)where?
(yi) ={0 yi =,1 otherwiseThat is, an additional penalty is assessed for not placinga comma at a given position.
By penalizing omission ofa comma between two words, the model implicitly re-wards commas; we would therefore expect higher recalland correspondingly lower precision.5 In fact, the methodwith the omission penalty (model 2 in Table 1), does havehigher recall and lower precision, essentially identical F-measure, but lower sentence accuracy.
Henceforth, themodels described here do not use an omission penalty.3 Commas and ConstituencyInsofar as commas are used as separators or delimiters,we should see correlation of comma position with con-stituent structure of sentences.
A simple test reveals thatthis is so.
We define the start count sci of a string posi-tion i as the number of constituents whose left boundaryis at i.
The end count eci is defined analogously.
For ex-ample, in Figure 2, sc0 is 4, as the constituents labeledJJ, NPB, S, and S start there; ec0 is 0.
We compute theend count for positions that have a comma by first drop-ping the comma from the tree.
Thus, at position 5, sc5is 2 (constituents DT, NPB) and ec5 is 4 (constituents JJ,ADJP, VP, S).We expect to find that the distributions of sc and ecfor positions in which a comma is inserted should differfrom those in which no comma appears.
Figure 3 revealsthat this intuition is correct.
The charts show the per-centage of string positions with each possible value ofsc (resp.
ec) for those positions with commas and those5Counterintuitively, Beeferman et al (1998) come to theopposite expectation, and their reported results bear out theirintuition.
We have no explanation for this disparity with ourresults.SSNPB VPJJ NN NNS VBP JJ PUNC, DT NN VBD PUNC.Further   staff   cuts     are   likely       ,       the   spokesman   indicated     .0              1        2         3      4                5            6                   7                8      9ADJP NPBVPFigure 2: Sample tree, showing computation of sc andec values.
The four constituents leading to ec5 = 4 areshown circled, and the two leading to sc5 = 2 are showncircled and shaded.without.
We draw the data again from sections 02?22 ofthe Wall Street Journal, using as the specification for theconstituency of sentences the parses for these sentencesfrom the Penn Treebank.
The distributions are quite dif-ferent, hinting at an opportunity for improved commarestoration.The ec distribution is especially well differentiated,with a cross-over point at about 2 constituents.
We canadd this kind of information, a single bit specifying an ecvalue of k or greater (call it e?ci), to the language model,as follows.
We replace p(yi | yi?2yi?1) with the proba-bility p(yi | yi?2yi?1e?ci).
We smooth the model usinglower order models p(yi | yi?1e?ci), p(yi | e?ci), p(yi).6These distributions can be estimated from the trainingdata directly, and smoothed appropriately.Adding just this one bit of information provides signifi-cant improvement to comma restoration performance.
Asit turns out, a k value of 3 turns out to maximize perfor-mance.7 Compared to the baseline, F-measure increasesto 63.2% and sentence accuracy to 52.3%.
This exper-iment shows that constituency information, even in rar-efied form, can provide significant performance improve-ment in comma restoration.
(Figure 1 lists performancefigures as model 3.
)Of course, this result does not provide a practical al-gorithm for comma restoration, as it is based on a prob-abilistic model that requires data from a manually con-structed parse for the sentence to be restored.
To make themethod practical, we might replace the Treebank parsewith a statistically generated parse.
In the sequel, we useCollins?s statistical parser (Collins, 1997) as our canon-ical automated approximation of the Treebank.
We cantrain a similar model, but using ec values extracted from6Alternative backoff paths, for instance backing off first top(yi | yi?2yi?1), exhibit inferior performance.7With k = 2 (model 4), precision drops precipitously to60.4%, recall stays roughly the same at 66.4%.InfosourcesTrainingTestingtrigraminsertionpenaltywordclassec,threshold=2ec,threshold=3ec,nothresholdstemmerTreebankCollinsparse,commasCollinsparse,nocommasTreebankCollinsparse,commasCollinsparse,nocommasModelnumberprecisionrecallF-measuretokenaccuracysentenceaccuracyreductioninsentence?error??1.711.552.621.963.470.000??2.684.576.625.962.457-.033????.834.511.634.967.514.113????.709.559.625.963.464-.014????3.752.627.683.968.523.135????4.604.664.633.958.435-.091?????.863.563.681.971.555.217????8.671.780.721.967.508.097?????10.796.704.748.974.579.280??????11.791.711.749.974.576.271????5.714.588.645.964.489.049????.733.557.633.964.489.048?????.851.532.655.969.534.164????9.657.674.666.963.476.015?????12.797.626.701.970.549.204??????.791.631.702.970.544.190????6.714.581.641.964.486.041????7.738.609.668.967.507.095????.746.602.666.967.503.085?????.859.556.675.970.550.205????.681.728.704.966.501.080?????.811.672.735.973.571.260?????
?.805.677.735.973.570.256Table 1: Performance of the various comma restoration models described in this paper.00.10.20.30.40.50.60.70 1 2 3 4 5 6 7 8without commawith comma(a)00.10.20.30.40.50.60.70 1 2 3 4 5 6 7 8 9 10 11 12 13without commawith comma(b)Figure 3: Differential pattern of constituent starts andends for string positions with and without commas.
Chart(a) shows the percentage of constituents with various val-ues of sc (number of constituents starting at the posi-tion) for string positions with commas (square points)and without (diamond points).
Chart (b) shows the corre-sponding pattern for values of ec (number of constituentsending).Collins parses of the training data, and use the model torestore commas on a test sentence again using ec valuesfrom the Collins parse of the test datum.
This model,listed as model 5 in Table 1, has an F-measure of 64.5%,better than the pure trigram model (62.2%), but not asgood as the oracular Treebank-trained model (68.4%).The other metrics show similar relative orderings.In this model, since the test sentence has no commasinitially, we want to train the model on the parses of sen-tences that have had commas removed, so that the modelis being applied to data as similar as possible to that onwhich it was trained.
We would expect, and experimentsverify (model 6), that training on the parses with com-mas retained yields inferior performance (in particular,F-measure of 64.1% and sentence accuracy of 48.6%).Again consistent with expectations, if we could clairvoy-antly know the value of e?ci based on a Collins parse ofthe test sentence with the commas that we are trying torestore (model 7), performance is improved over model5; F-measure rises to 66.8%.The steady rise in performance from models 6 to 5 to7 to 3 exactly tracks the improved nature of the syntac-tic information available to the system.
As the qualityof the syntactic information better approximates groundtruth, our ability to restore commas gradually and mono-tonically improves.4 Using More Detailed SyntacticInformation4.1 Using full end count informationThe examples above show that even a tiny amount of syn-tactic information can have a substantive advantage forcomma restoration.
In order to use more information,we might imagine using values of ec directly, rather thanthresholding.
However, this quickly leads to data spar-sity problems.
To remedy this, we assume independencebetween the bigram in the conditioning context and thesyntactic information, that is, we takep(yi | yi?2yi?1eci) ?p(yi | yi?2yi?1)p(yi | yi?1eci)p(yi)This model8 (model 8) has an F-measure of 72.1% dueto a substantial increase in recall, demonstrating that theincreased articulation in the syntactic information avail-able provides a concomitant benefit.
Although the sen-tence accuracy is slightly less than that with thresholdedec, we will show in a later section that this model com-bines well with other modifications to generate further8We back off the first term in the approximation as before,and the second to p(yi | yi?1).improvements.94.2 Using part of speechAdditional information from the parse can be useful inpredicting comma location.
In this section, we incorpo-rate part of speech information into the model, generatingmodel 10.
We estimate the joint probability of each wordxi and its part of speech Xi as follows:p(xi, Xi | xi?2, Xi?2, xi?1, Xi?1, ec) ?p(xi | xi?2xi?1ec)p(Xi | Xi?2Xi?1)The first term is computed as in model 8, the second back-ing off to bigram and unigram models.
Adding a part ofspeech model in this way provides a further improvementin performance.
F-measure improves to 74.8%, sentenceaccuracy to 57.9%, a 28% improvement over the base-line.These models (8 and 10), like model 3, assumedavailability of the Treebank parse and part of speechtags.
Using the Collins-parse-generated parses still showsimprovement over the corresponding model 5: an F-measure of 70.1% and sentence accuracy of 54.9%, twicethe improvement over the baseline as exhibited by model5.5 Related WorkWe compare our comma restoration methods to those ofBeeferman et al (1998), as their results use only textualinformation to predict punctuation.
Several researchershave shown prosodic information to be useful in predict-ing punctuation (Christensen et al, 2001; Kim and Wood-land, 2001) (along with related phenomena such as dis-fluencies and overlapping speech (Shriberg et al, 2001)).These studies, typically based on augmenting a Marko-vian language model with duration or other prosodic cuesas conditioning features, show that prosody informationis orthogonal to language model information; combinedmodels outperform models based on each type of infor-mation separately.
We would expect therefore, that ourtechniques would similarly benefit from the addition ofprosodic information.In the introduction, we mentioned the problem of sen-tence boundary detection, which is related to the punc-tuation reconstruction problem especially with regard topredicting sentence boundary punctuation such as peri-ods, question marks, and exclamation marks.
(This prob-lem is distinct from the problem of sentence boundarydisambiguation, where punctuation is provided, but thecategorization of the punctuation as to whether or not9An alternative method of resolving the data sparsity issuesis to back off the model p(yi | yi?2yi?1eci), for instance top(yi | yi?2yi?1) or to p(yi | yi?1eci).
Both of these performless well than the approximation in model 8.it marks a sentence boundary is at issue (Palmer andHearst, 1994; Reynar and Ratnaparkhi, 1997).)
Stolckeand Shriberg (1996) used HMMs for the related problemof linguistic segmentation of text, where the segmentscorresponded to sentences and other self-contained unitssuch as disfluencies and interjections.
They argue that alinguistic segmentation is useful for improving the per-formance and utility of language models and speech rec-ognizers.
Like the present work, they segment clean textrather than automatically transcribed speech.
Stevensonand Gaizauskas (Stevenson and Gaizauskas, 2000) andGoto and Renals (Gotoh and Renals, 2000) address thesentence boundary detection problem directly, again us-ing lexical and, in the latter, prosodic cues.6 Future Work and ConclusionThe experiments reported here ?
like much of the previ-ous work in comma restoration (Beeferman et al, 1998)and sentence boundary disambiguation and restoration(Stolcke and Shriberg, 1996; Shriberg et al, 2001; Go-toh and Renals, 2000; Stevenson and Gaizauskas, 2000)(though not all (Christensen et al, 2001; Stolcke et al,1998; Kim and Woodland, 2001)) ?
assume an ideal ref-erence transcription of the text.
The performance of themethod on automatically transcribed speech with its con-comitant error remains to be determined.
A hopeful signis the work of Kim and Woodland (Kim and Woodland,2001) on punctuation reconstruction using prosodic in-formation.
The performance of their system drops froman F-measure of 78% on reference transcriptions to 44%on automatically transcribed speech at a word error rateof some 20%.
Nonetheless, prosodic features were stilluseful in improving the reconstructed punctuation evenin the automatically transcribed case.The simple HMM model that we inherit from earlierwork dramatically limits the features of the parse that wecan easily appeal to in predicting comma locations.
Manyalternatives suggest themselves to expand the options,including maximum entropy models, which have beenpreviously successfully applied to, inter alia, sentenceboundary detection (Reynar and Ratnaparkhi, 1997), andtransformation-based learning, as used in part-of-speechtagging and statistical parsing applications (Brill, 1995).In addition, all of the methods above are essentiallynonhierarchical, based as they are on HMMs.
An alter-native approach would use the statistical parsing modelitself as a model of comma placement, that is, to selectthe comma placement for a string such that the resultingreconstructed string has maximum likelihood under thestatistical parsing model.
This approach has the benefitthat the ramifications of comma placement on all aspectsof the syntactic structure are explored, but the disadvan-tage that the longer distance lexical relationships foundin a trigram model are eliminated.Nonetheless, even under these severe constraints andusing quite simple features distilled from the parse, wecan reduce sentence error by 20%, with the potential ofanother 8% if statistical parsers were to approach Tree-bank quality.
As such, comma restoration may stand asthe first end-user application that benefits from statisti-cal parsing technology smoothly and incrementally.
Fi-nally, our methods use features that are orthogonal to theprosodic features that other researchers have explored.They therefore have the potential to combine well withprosodic methods to achieve further improvements.AcknowledgmentsPartial support for the work reported in this paper wasprovided by the National Science Foundation under grantnumber IRI 9712068.We are indebted to Douglas Beeferman for makingavailable his expertise and large portions of the code anddata for replicating the CYBERPUNC experiments.The first author would like to express his appreciationto Ivan Sag and the Center for the Study of Language andInformation, Stanford, California and to Oliviero Stockand the Centro per la Ricerca Scientifica e Tecnologica,Trento, Italy, for space and support for this work duringspring and summer of 2002.ReferencesDoug Beeferman, Adam Berger, and John Lafferty.1998.
CYBERPUNC: A lightweight punctuation an-notation system for speech.
In Proceeding as of theIEEE International Conference on Acoustics, Speechand Signal Processing, pages 689?692, Seattle, WA.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A case studyin part of speech tagging.
Computational Linguistics,21(4):543?565.Heidi Christensen, Yoshihiko Gotoh, and Steve Renals.2001.
Punctuation annotation using statistical prosodymodels.
In Proceedings of the 2001 ISCA Tutorial andResearch Workshop on Prosody in Speech Recognitionand Understanding, Red Bank, NJ, October 22?24.
In-ternational Speech Communication Association.Philip Clarkson and Ronald Rosenfeld.
1997.
Statisticallanguage modeling using the CMU-Cambridge toolkit.In Proceedings of Eurospeech ?97, pages 2707?2710,Rhodes, Greece, 22?25 September.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the35th Annual Meeting of the Association for Compu-tational Linguistics and Eighth Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics, pages 16?23, Madrid, Spain, 7?11 July.Yoshihiko Gotoh and Steve Renals.
2000.
Sentenceboundary detection in broadcast speech transcripts.In Proceedings of the ISCA Workshop on AutomaticSpeech Recognition: Challenges for the New Millen-nium (ASR-2000), Paris, France, 18?20 September.
In-ternational Speech Communication Association.Ji-Hwan Kim and P. C. Woodland.
2001.
The use ofprosody in a combined system for punctuation gener-ation and speech recognition.
In Proceedings of Eu-rospeech ?01, pages 2757?2760, Aalborg, Denmark,September 3?7.Geoffrey Nunberg.
1990.
The Linguistics of Punctua-tion.
CSLI Publications, Stanford, CA.David D. Palmer and Marti A. Hearst.
1994.
Adaptivesentence boundary disambiguation.
In Proceedings ofthe Fourth ACL Conference on Applied Natural Lan-guage Processing, pages 78?83, Stuttgart, Germany,13?15 October.
Morgan Kaufmann.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.
Amaximum entropy approach to identifying sentenceboundaries.
In Proceedings of the Fifth Conference onApplied Natural Language Processing, pages 16?19,Washington, DC, 31 March?3 April.Elizabeth Shriberg, Andreas Stolcke, and Don Baron.2001.
Can prosody aid the automatic processing ofmulti-party meetings?
Evidence from predicting punc-tuation, disfluencies, and overlapping speech.
In Pro-ceedings of the 2001 ISCA Tutorial and ResearchWorkshop on Prosody in Speech Recognition and Un-derstanding, Red Bank, NJ, October 22?24.
Interna-tional Speech Communication Association.Mark Stevenson and Robert Gaizauskas.
2000.
Ex-periments on sentence boundary detection.
In Pro-ceedings of the Sixth Conferernce on Applied Natu-ral Language Processing and the First Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 24?30, Seattle, WA,April.Andreas Stolcke and Elizabeth Shriberg.
1996.
Au-tomatic linguistic segmentation of conversationalspeech.
In H. T. Bunnell and W. Idsardi, editors, Pro-ceedings of the International Conference on SpokenLanguage Processing, volume 2, pages 1005?1008,Philadelphia, PA, 3?6 October.Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates,Mari Ostendorf, Dilek Hakkani, Madelaine Plauche,Go?khan Tu?r, and Yu Lu.
1998.
Automatic detection ofsentence boundaries and disfluencies based on recog-nized words.
In Proceedings of the International Con-ference on Spoken Language Processing, volume 5,pages 2247?2250, Sydney, Australia, 30 November?4December.
