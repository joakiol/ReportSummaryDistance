Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 39?47,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPSyntax-Driven Sentence Revision for Broadcast News SummarizationHideki Tanaka, Akinori Kinoshita, Takeshi Kobayakawa,Tadashi Kumano and Naoto KatoNHK Science and Technology Research Labs.1-10-11, Kinuta, Setagaya-ku, Tokyo, Japan{tanaka.h-ja,kinoshita.a-ek,kobayakawa-t.ko,kumano.t-eq,kato.n-ga}@nhk.or.jpAbstractWe propose a method of revising lead sentences ina news broadcast.
Unlike many other methods pro-posed so far, this method does not use the corefer-ence relation of noun phrases (NPs) but rather,insertion and substitution of the phrases modifyingthe same head chunk in lead and other sentences.The method borrows an idea from the sentencefusion methods and is more general than thoseusing NP coreferencing as ours includes them.
Weshow in experiments the method was able to findsemantically appropriate revisions thus demon-strating its basic feasibility.
We also show that thatparsing errors mainly degraded the sentential com-pleteness such as grammaticality and redundancy.1 IntroductionWe address the problem of revising the lead sen-tence in a broadcast news text to increase theamount of background information in the lead.This is one of the draft and revision approachesto summarization, which has received keen atten-tion in the research community.
Unlike manyother methods that directly utilize noun phrase(NP) coreference (Nenkova 2008; Mani et al1999), we propose a method that employs inser-tion and substitution of phrases that modify thesame chunk in the lead and other sentences.
Wealso show its effectiveness in a revision experi-ment.As is well known, the extractive summary thathas been extensively studied from the early daysof summarization history (Luhn, 1958) suffersfrom various drawbacks.
These include the prob-lems of a break in cohesion in the summary textsuch as dangling anaphora and a sudden shift intopic.To ameliorate these problems, the idea of revis-ing the extracted sentences was proposed in asingle document summarization study.
Jing andMcKeown (1999; 2000) found that human sum-marization can be traced back to six cut-and-paste operations of a text and proposed a revisionmethod consisting of sentence reduction andcombination modules with a sentence extractionpart.
Mani and colleagues (1999) proposed asummarization system based on ?draft and revi-sion?
together with sentence extraction.
The re-vision part is achieved with the sentence aggre-gation and smoothing modules.The cohesion break problem becomes particu-larly conspicuous in multi-document summariza-tion.
To ameliorate this, revision of the extractedsentences is also thought to be effective, andmany ideas and methods have been proposed sofar.
For example, Otterbacher and colleagues(2002) analyzed manually revised extracts andfactored out cohesion problems.
Nenkova (2008)proposed a revision idea that utilizes nouncoreference with linguistic quality improvementsin mind.Other than the break in cohesion, multi-document summarization faces the problem ofinformation overlap particularly when the docu-ment set consists of similar sentences.
Barzilayand McKeown (2005) proposed an idea calledsentence fusion that integrates information inoverlapping sentences to produce a non-overlapping summary sentence.
Their algorithmfirstly analyzes the sentences to obtain the de-pendency trees and sets a basis tree by findingthe centroid of the dependency trees.
It nextaugments the basis tree with the sub-trees in oth-er sentences and finally prunes the predefinedconstituents.
Their algorithm was further modi-fied and applied to the German biographies byFilippova and Strube (2008).Like the work of Jing and McKeown (2000) andMani et al (1999), our work was inspired by thesummarization method used by human abstrac-tors.
Actually, our abstractors first extract impor-tant sentences, which is called lead identification,and then revise them, which is referred to asphrase elaboration or specification.
In this paper,we concentrate on the revision part.Our work can be viewed as an application of thesentence fusion method to the draft and revision39approach to a single Japanese news documentsummarization.
Actually, our dependency struc-ture alignment is almost the same as that ofFilippova and Strube (2008), and our lead sen-tence plays the role of a basis tree in the Barzilayand McKeown approach (2005).
Though the ideaof sentence fusion was developed mainly forsuppressing the overlap in multi-document sum-marization, we consider this effective in aug-menting the extracts in a single-document sum-marization task where we face less overlapamong sentences.Before explaining the method in detail, we willbriefly introduce the Japanese dependency 1structure on which our idea is based.
The de-pendency structure is constructed based on thebunsetsu chunk, which we call ?chunk?
for sim-plicity.
The chunk usually consists of one con-tent-bearing word and a series of function words.All the chunks in a sentence except for the lastone modify a chunk in the right direction.
Wecall the modifying chunk the modifier and themodified chunk the head.
We usually span a di-rected edge from a modifier chunk to the headchunk 2 .
Our dependency tree has no syntacticinformation such as subject or object.2 Broadcast news summarizationTanaka et al (2005) showed that most Japanesebroadcast news texts are written with a three-partstructure, i.e., the lead, body, and supplement.The most important information is succinctlymentioned in the lead, which is the opening sen-tence(s) of a news story, referred to as an ?arti-cle?
here.
Proper names and details are some-times avoided in favor of more abstract expres-sions such as ?big insurance company.?
The leadis then detailed in the body by answering who,what, when, where, why, and how, and propernames only alluded to in the lead appear here.Necessary information that was not covered inthe lead or the body is placed in the supplement.The research also reports that professional newsabstractors who are hired for digital text servicessummarize articles in a two-step approach.
First,they identify the lead sentences and set it (them)as the starting point of the summary.
As the av-erage lead length is 95 characters and the al-1 This is the kakari-uke (modifier-modifiee) relation ofJapanese, which differs from the conventional dependencyrelation.
We use the term dependency for convenience inthis paper.2 This is the other way around compared to the English de-pendency such as in Barzilay and McKeown (2005).lowed summary length is about 115 characters(or 150 characters depending on the screen de-sign), they revise the lead sentences using ex-pressions from the remainder of the story.We see here that the extraction and revisionstrategy that has been extensively studied bymany researchers for various reasons was actu-ally applied by human abstractors, and therefore,the strategy can be used as a real summarizationmodel.
Inspired by this, we decided to study anews summarization system based on the aboveapproach.
To develop a complete summarizationsystem, we have to solve three problems: 1)identifying the lead, body, and supplement struc-ture in each article, 2) finding the lead revisioncandidates, and 3) generating a final summary byselecting and combining the candidates.We have already studied problem 1) and showedthat automatic recognition of three tags with adecision tree algorithm reached a precision over92% (Tanaka et al 2007).
We then moved toproblem 2), which we discuss extensively in therest of this paper.3 Manual lead revision experimentTo see how problem 2) in the previous sectioncould be solved, we conducted a manual lead-revision experiment.
We asked a native Japanesespeaker to revise the lead sentences of 15 newsarticles using expressions from the body sectionof each article with cut-and-paste operations (in-sertion and substitution) of bunsetsu chunk se-quences.
We refer to chunk sequences as phrases.We also asked the reviser to find as many revi-sions as possible.In the interview with her, we found that she tookadvantage of the syntactic structure to revise thelead sentences.
Actually, she first searched forthe ?same?
chunks in the lead and the body andchecked whether the modifier phrases to thesechunks could be used for revision.
To see whatmakes these chunks the ?same,?
we comparedthe syntactic head chunk of the lead and bodyphrases used for substitution and insertion.Table 1 summarizes the results of the compari-son in three categories: perfect match, partialmatch (content word match), and different.The table indicates that nearly half of the headchunks were exactly the same, and the rest con-tained some differences.
The second row showsthe number where the syntactic heads had thesame content words but not the same functionwords.
The pair ???
kaidan-shi ?talked?
and??????
kaidan-shi-mashi-ta ?talked?
is an40Ins.
Sub.
Total1) Perfect 9 6 152) Partial 6 6 123) Different 1 6 7Total 16 18 34LeadIAEA?of the IAEA???
?the team??
?at Korea?????
?arrivedTable 1.
Degree of syntactic head agreementexample.
These are the  syntactic and aspectualvariants of the same verb ????
kaidan-suru?talk.
?The third row represents cases where the syntac-tic heads had no common surface words.
Wefound that even in this case, though, the syntacticheads were close in some way.
In one example,there was accordance in the distant heads, forinstance, in the pair ?????
mitsuka-tta?found?
and ???
ichibu-no ?part of.?
In thiscase, we can find the chunk ?????
mit-suka-tta ?found?
at a short edge distance from ???
ichibu-no ?part of.?
Based on the findings,we devised a lead sentence revision algorithm.4 Revision algorithm4.1 ConceptWe explain here the concept of our algorithmand show an example in Figure 1.
We have alead sentence and a body sentence, both of whichhave the ?same?
syntactic head chunk, ?????
?, touchaku-shima-shi-ta, ?arrived.
?The head chunk of the lead has two phrases (un-derlined with thick lines in Figure 1) that directlymodify the head.
We call such a phrase a maxi-mum phrase of a head3.
Like the lead sentence,the body sentence also has two maximum phras-es.
In the following part, we use the term phraseto refer to a maximum phrase for simplicity.By comparing the phrases in Figure 1, we noticethat the following operations can add useful in-formation to the lead sentence; 1) inserting thefirst phrase of the body will supply the fact thevisit was on the 4th, 2) substituting the firstphrase of the lead with the second one in thebody adds the detail of the IAEA team.
This re-vision strategy was employed by the human re-viser mentioned in section 2, and we considerthis to be effective because our target documenthas a so-called inverse pyramid structure (Robinand McKeown 1996), in which the first sentenceis elaborated by the following sentences.3 To be more precise, a maximum phrase is defined as themaximum chunk sequence on a dependency path of a head.Figure 1.
Concept of revision algorithmFurther analyzing the above fact, we devised thelead sentence revision algorithm below.
We pre-sent the outline here and discuss the details in thenext section.
We suppose an input pair of a leadand a body sentence that are syntactically ana-lyzed.1) Trigger searchWe search for the ?same?
chunks in the leadand body sentences.
We call the ?same?chunks triggers as they give the starting pointto the revision.2) Phrase alignmentWe identify the maximum phrases of eachtrigger, and these phrases are aligned accordingto a similarity metric.3) SubstitutionIf a body phrase has a corresponding phrase inthe lead, and the body phrase is richer in in-formation, we substitute the body phrase forthe lead phrase.4) InsertionIf a body phrase has no counterpart in the lead,that is, the phrase is floating, we insert it intothe lead sentence.Our method inserts and substitutes any type ofphrase that modifies the trigger and therefore hasno limitation in syntactic type.
Although NPelaboration such as in (Nenkova 2008) is of greatimportance, there are other useful syntactic typesfor revision.
An example is the adverbial phraseinsertion of time and location.
The insertion ofthe phrase 4?
yokka ?on the 4th?
in figure 1 in-deed adds useful information to the lead sentence.4.2 AlgorithmThe overall flow of the revision algorithm isshown in Algorithm 1.
The inputs are a lead anda body sentence that are syntactically parsed,which are denoted by L and B respectively.The whole algorithm starts with the all-triggersearch in step 1.
Revision candidates are thenfound for each trigger pair in the main loop fromsteps 2 to 6.
The revision for each trigger pair isIAEA?of the IAEA???inspectors??????arrived5??fiveBody4?
?on the 4thinsertion substitutionmaximum phrase41Algorithm 14 (Left figures are the step numbers.
)1: find all trigger pairs between L and B andstore them in T.T={(l, b) ; l b, l?L and b?B } ?2: for all (l, b) ?
T dofind l?s max phrases and store in Pl.Pl={pl ; pl ?
max phrase of l}3:  do the same for trigger bPb={pb ; pb ?
max phrase of b}4:  align phrases in Pl and Pb and storeresult in AA={( pl, pb) ; pl  pb ,  ?pl ?
Pl, pb ?
Pb }5:  for all (pl, pb) ?
A dofollow Table 2end for6: end forBodypb =?
pb?
?pl =?
4: no op.
1: insertion Leadpl  ?
?
3: no op.
2: substitutionTable 2.
Operations for step 5found based on the idea in the previous section insteps 4 and 5.
Now we explain the main parts.?
Step 1: trigger chunk pair searchWe first detect the trigger pairs in step 1 that arethe base of the revision process.
What then canbe a trigger pair that yields correct revisions?
Weroughly define trigger pairs as the ?coreferential?chunk pairs of all parts of speech, i.e., the partsof speech that point to the same entity, event,action, change, and so on.Notice that the term coreferential is used in anextended way as it is usually used to describe thephenomena in noun group pairs (Mitkov, 2002).The chunk ??????
touchaku-shimashita?arrived?
and IAEA?
IAEA-no ?of the IAEA?in Figure 1 are examples.Identifying our coreferential chunks is evenharder than the conventional coreference resolu-tion, and we made a simplifying assumption as inNenkova (2008) with some additional conditionsthat were obtained through our preliminary ex-periments.
(1) Assumption: Two chunks having the samesurface forms are coreferential.
(2) Conditions for light verb (noun) chunks:Agreement of modifying verbal nous is fur-?
?4 The sign a b means the chunk ?a?
and ?b?
are triggers.The sign p q means the phrases ?p?
and ?q?
are aligned.ther required for chunks whose contentwords consist only of light verbs such as ??
aru ?be?
and ??
naru ?become?
: thesechunks themselves have little lexical mean-ing.
The agreement is checked with thehand-crafted rules.
Similar checks are ap-plied to chunks whose content words consistonly of light nouns such as ??
koto (?koto?makes the previous verb a noun) .
(3) Conditions for verb inflections: a chunk thatcontains a verb usually ends with a functionword series that indicates a variety of infor-mation such as inflection type, dependencytype, tense, aspect, and modality.
Some in-formation such as tense and aspect is vital todecide the coreference relation (exchangingthe modifier phrases ?arrive?
and ?will ar-rive?
will likely bring about inconsistency inmeaning), although some is not.
We are inthe process of categorizing function wordsthat do not affect the coreference relation andtemporally adopted the empirically obtainedrule: the difference in verb inflection be-tween the te-form (predicate modifyingform) and dictionary form (sentence endform) can be ignored.?
Step 4: phrase alignmentWe used the surface form agreement for similar-ity evaluation.
We applied several metrics andexplain them one by one.1) Chunk similarity t, st, s : x, y?
chunk [0, 1].
?Function t is the Dice coefficient between theset of content words in x and those in y. Thesame coefficient calculated with all words(function and content words) is denoted as s.2) Phrase absorption ratioa : px, py?
phrases  [0, 1] ?This is the function that indicates how manychunks in phrase px is represented in py and iscalculated with t as in,??
?=xypxpyxyx yxtpppa )),((max1:),( .3) Alignment  qualityWith the above two functions, the alignment qual-lity is evaluated by the functiong : px, py ?
phrases ?
[0, 1]],1,0[),,()1(),(:),(??+=???
yxsppappg yxyxwhere the shorter phrase is set to px so thatyx pp < .
The variables x and y are the last42chunks in px and py, respectively.
Intuitively,the function evaluates how many chunks in theshorter phrase px are represented in py and howsimilar the last chunks are.
The last chunk in aphrase, especially the function words in thechunk, determines the syntactic character ofthe phrase, and we measured this value withthe second term of the alignment quality.
Theparameter ?
is decided empirically, which wasset at 0.375 in this paper.In alignment, we calculated the score for allpossible phrase combinations and then greed-ily selected the pair with the highest score.
Weset the minimum alignment score at 0.185;those pairs with scores lower than this valuewere not aligned.?
Step 5 (Table 2, case 1): insertionStep 5 starts either an insertion or substitutionprocess, as in Table 2.
If pb  (body phrase isnot null) and pl =  (lead phrase is null) in Table2, the insertion process starts.?
?
?In this process, we check the following.1) Redundancy checkInsertion may cause redundancy in informa-tion.
As a matter of fact, redundancy oftenhappens when there is an error in syntacticanalysis.
Suppose there are the same lead andbody phrases that modify the same chunks inthe lead and body sentences.
If the lead phrasefails to modify the correct chunk because of anerror, the body phrase loses the chance to bealigned to the lead phrase since they belong todifferent trigger chunks.
As a result, the bodyphrase becomes a floating phrase and is in-serted into the lead chunk, which duplicatesthe same phrase.To prevent this, we evaluate the degree of du-plication with the phrase absorption ratio aand allow phrase insertion when the score isbelow a predefined threshold ?
: we allow in-sertion when?
),( bb pLpa < ,?
phrase, L : lead sentence,is satisfied.2) Discourse coherence checkBlind phrase insertion may invite a break incohesion in a lead sentence.
This frequentlyhappens when the inserted phrase has wordsthat require an antecedent.
We then prepared alist of words that contain such context-requiring words and forbid phrase insertionsthat contain words that are on the list.
This listcontains the pronoun family such as ??
ko-kono ?this?
and special adjectives such as ?
?chigau ?different.
?3) Insertion point decisionThe body phrase should be inserted at theproper position in the lead sentence to main-tain the syntactic consistency.
Because wedealt with single-phrase insertion here, weemployed a simple heuristics.Since the Japanese dependency edge spansfrom left to right as we mentioned in section 1,we considered that the right phrase of the in-serted phrase is important to keep the new de-pendency from the inserted phrase to the trig-ger chunk.
Because we already know thephrase alignment status at this stage, we fol-low the next steps to determine the insertionposition in the lead of the insertion phrase.A) In the body sentence, find the nearest rightsubstitution phrase pr of the insertionphrase.B) Find the pr?s aligned phrase in the lead prL.C) Insert the phrase to the left of the prL.D) If there is no pr, insert the phrase to the leftto the trigger.?
Step 5 (Table 2, case 2): substitutionIf pb ?
?
and pl ?
?
in Table 2, the substitu-tion process starts.
This process first checks ifeach aligned phrase pair contains the same chunkother than the present trigger.
If there is such achunk, the substitution phrase is reduced to thesubtree from the present trigger to the identicalchunk.
The newly found identical chunks are intrigger table T, and the remaining part will beevaluated later in the main loop.
Owing to thephrase partitioning, we can avoid phrase substi-tutions which are in an inclusive relation.The substitution candidate goes through threechecks: information increase, redundancy, anddiscourse cohesion.
As the latter two are almostthe same as those in the insertion, we explainhere the information increase.
This involveschecking whether the number of chunks in thebody phrase is greater than that in the alignedlead phrase.
This is based on the simple assump-tion that elaboration requires more words.5 Revision experiments5.1 Data and evaluation steps?
PurposeWe conducted a lead revision experiment withthree purposes.
The first one was to empiricallyevaluate the validity of our simplified assump-43tions: trigger identification and concreteness in-crease evaluation.
For trigger identification, webasically viewed the identical chunks as triggersand added some amendments for light verbs(nouns) and verb inflections.
For the check of anincrease in concreteness, we assumed thatphrases with more chunks were more concrete.However, these simplifications should be veri-fied in experiments.The second purpose was to check the validity ofusing the revision phrases only in body sentencesand not in the supplemental sentences.The last one was to determine how ineffectivethe result is if the syntactic parsing fails.
Withthese purposes in mind, we designed our experi-ment as follows.?
DataA total of 257 articles from news programsbroadcast on 20 Jan., 20 Apr., and 20 July in2004 were tagged with lead, body, and supple-ment tags by a native Japanese evaluator.
Thearticles were morphologically analyzed by Me-cab (Kudo et al, 2003) and syntactically parsedby Cabocha (Kudo and Matsumoto, 2002).?
Evaluator and evaluation detailWe prepared an evaluation interface that presentsa lead with one revision point (insertion or sub-stitution) that was obtained using the body andsupplemental sentences to an evaluator.A Japanese native speaker evaluated the resultsone by one with the above interface.
We planneda linguistic evaluation like DUC2005 (Hoa Trang,2005).
Since their five-type evaluation is in-tended for multi-document summarization,whereas our task is single-document summariza-tion, and we are interested in evaluating ourquestions mentioned above, we carried out theevaluation as follows.
In future, we plan to in-crease the number of evaluation items and thenumber of evaluators.Concreteness ScoreDecreased 0Unchanged 1Increased 2Table 3.
Evaluation of increased concretenessCompleteness Required operations ScorePoor More than 2 0Acceptable One 1Perfect None 2Table 4.
Sentential completenessE1) The evaluator judged if the revision was ob-tained from the lead and body sentences withor without parsing errors.
Here, errors that didnot affect the revision were not considered.E2) Second, she checked whether the revisionwas semantically correct or revised informa-tion matching the fact described in the leadsentence.
Here, she did not care about thegrammaticality or the improvements in con-creteness of the revision; if the revision wasproblematic but manually correctable, it wasjudged as OK.
This step evaluated the correct-ness of the trigger selection; wrong triggers,i.e., those referring to different facts producesemantically inconsistent revisions as they mixup different facts.The following evaluation was done for thosejudged correct in evaluation step E2, as we foundthat revisions that were semantically inconsistentwith the lead?s facts were often too difficult toevaluate further.E3) Third, she evaluated the change in concrete-ness after revision with the revisions thatpassed evaluation E2.
She judged whether ornot the revision increased the concreteness ofthe lead in three categories (Table 3).Notice that original lead sentences are sup-posed to have an average score of 1.E4) Last, she checked the sentential complete-ness of the revision result that passed evalua-tion E2.
They still contained problems such asgrammatical errors and improper insertion po-sition.
Rather than evaluating these items sepa-rately, we measured them together for senten-tial completeness.
At this time, we measured interms of the number of operations (insertion,deletion, substitution) needed to make the sen-tence complete5.As shown in Table 4, revisions requiring morethan two operations are categorized as ?poor,?those requiring one operation are ?acceptable,?and those requiring no operations are ?perfect.
?We employed this measure because we foundthat grading detailed items such as grammatical-ity and insertion positions at fine levels wasrather difficult.
We also found that native Japa-nese speakers can correct errors easily.
Noticethe lead sentences are perfect and are supposed5 This was not an automatic process and may not be perfect.The evaluator simulated the correction in mind and judgedwhether it was done with one action.44to have an average score of 2 in sentential com-pleteness.
Since the revision does not improvethe completeness further but elicits defects suchas grammatical errors, it usually produces a scorebelow 2.
Some examples of the results with theirscores are shown below.
The underlined parts arethe inserted body chunk phrases, and the paren-thesized parts are the deleted lead chunks.1) Concreteness 2, Completeness 2??????????????????????????????????????????????
?minkan-dantai-no ?privateorganization?, korea-society-nado-ga ?Korea Soci-ety and others?, shusai-suru?sponsored?, chousen-hantou-heiwa-forumu-ni  ?Peace Fo-rum in Korean Peninsula?,(moyooshi-ni ?event?
),shusseki-suru ?attend?2) Concreteness 1, Completeness 2????????????????????
?buhin-ni ?to the parts?
ki-retsu-ga ?cracks?, haitte-iru-no-ga ?being there?
(),mitsuka-tta ?found?3) Concreteness 2, Completeness 0????????????????????????????????
?Herikoputa-kara ?from a hel-icopter?, chijou-niju-metoru-no-takasa-kara ?from20 meters high?
(), rakka-shi ?fell and?, shibou-shima-shita ?killed?Example 1 is the perfect substitution and hadscores of 2 for both concreteness increase andcompleteness.
Actually, the originally vaguelymentioned term ?event?
was replaced by a moreconcrete phrase with proper names, ?Korean Pen-insula Peace Forum sponsored by Korea Societyand others.?
Notice that this can be achieved byNP coreference based methods if they can iden-tify that these two different phrases are corefer-ential.
Our method does this through the depend-ency on the same trigger ????
shusseki-suru?attend.
?Example 2 is a perfect sentence, but its concrete-ness stayed at the same level.
As a result, thescores were 1 for concreteness increase and 2 forcompleteness.Incorrect Correct Cor.
RatioSucc.
70 353 0.83ParseFail.
31 149 0.83Body 50 464 0.90Sent.Supp.
51 38 0.43Table 5.
Results of semantic correctnessScore 0 1 2 Ave.Succ.
0 55 298 1.84ParseFail.
1 19 129 1.86Body 1 61 402 1.86Sent.Supp.
0 13 25 1.66Table 6.
Results of concreteness increaseScore 0 1 2 Ave.Succ.
78 60 215 1.39ParseFail.
66 55 28 0.74Body 120 110 234 1.25Sent.Supp.
24 5 9 0.61Table 7.
Results of sentential completenessActually, the original sentence that meant ?Theyfound a crack in the parts?
was revised to ?Theyfound there was a crack in the parts,?
which didnot add useful information.
Example 3 has agrammatical problem although the revision sup-plied useful information.As a result, it had scoresof 2 for concreteness increase and 0 for com-pleteness.
The added kara-case phrase (fromphrase) ?????????????
chijou-niju-metoru-no-takasa-kara ?from 20 metershigh?
is useful, but since the original sentencealready has the kara-case ???????
?herikoputa-kara ?from helicopter,?
the insertioninvited a double kara-case, which is forbidden inJapanese.
To correct the error, we need at leasttwo operations, and thus, a completeness score of0 was assigned.5.2 Results of experimentsTable 5 presents the results of evaluation E2, thesemantic correctness with the parsing status ofevaluation E1 and the source sentence categoryfrom which the phrases for revision were ob-tained.
Columns 2 and 3 list the number of revi-sions (insertions and substitutions) that were cor-rect and incorrect and column 4 shows the cor-rectness ratio.
We obtained a total of 603 revi-sions and found that 30% (180/603) of themwere derived with syntactic errors.The semantic correctness ratio was unchangedregardless of the parsing success.
On the contrary,it was affected by the source sentence type.
Thecorrectness ratio with the supplemental sentence45was significantly6 lower than that with the bodysentence.
Table 6 lists the results of the con-creteness improvements with the parsing statusand the source sentence type.
Columns 2, 3 and 4list the number of revisions that fell in the scores(0-2) listed in the first row.
The average score inthis table again was not affected by the parsingfailure but was significantly affected by thesource sentence category.
The result with thesupplement sentences was significantly worsethan that with body sentences.Table 7 lists the results of the sentential com-pleteness in the same fashion as Table 6.
Thesentential completeness was significantly wors-ened by both the parsing failure and source sen-tence category.These results indicate that the answers to thequestions posed at the beginning of this sectionare as follows.
From the semantic correctnessevaluation, we infer that our trigger selectionstrategy worked well especially when the sourcesentence category was limited to the body.From the concreteness-increase evaluation, theassumption that we made also worked reasonablywell when the source sentence category was lim-ited to the body.The effect of parsing was much more limitedthan we had anticipated in that it did not degradeeither the semantic correctness or the concrete-ness improvements.
Parsing failure, however,degraded the sentential completeness of the re-vised sentences.
This seems quite reasonable:parsing errors elicit problems such as wrongphrase attachment and wrong maximum phraseidentification.
The revisions with these errorsinvite incomplete sentences that need corrections.It is worth noting that cases sometimes occurredwhere a parsing error did not cause any problemin the revision.
We found that the phrases gov-erned by a trigger pair in many cases were quitesimilar, and therefore, the parser makes the sameerror.
In that case, the errors are often offset andcause no problems superficially.We consider that the sentential completenessneeds further improvements to make an auto-matic summarization system, although the se-mantic correctness and concreteness increase areat an almost satisfactory level.
Our dependency-based revision is expected to be potentially use-ful to develop a summarization system.6 In this section, the ?significance?
was tested with theMann-Whitney U test with Fisher?s exact probability.
Weset the significance level at 5%.6 Future workSeveral problems remain to be solved, which willbe addressed in future work.
Obviously, we needto improve the parsing accuracy that degradedthe sentential completeness in our experiments.Although we did not quantitatively evaluate theerrors in phrase insertion position and redun-dancy, we could see these happening in the re-vised sentences because of the inaccurate parsing.Apart from this, we need to further refine thefollowing problems.Regarding the trigger selection, one particularproblem we faced was the mixture of statementsof different politicians in a news article.
Thestatements were often included as direct quota-tions that end with the chunk ?????
nobe-mashi-ta ?said.?
Our system takes the chunk asthe trigger and does not care whose statementsthey are; thus, it ended up mixing them up.
Asimilar problem happened when we had two dif-ferent female victims of an incident in an article.Since our system has no means to distinguishthem, the modifier phrases about these womenwere mixed up.We think that we can improve our method byapplying more general language generation tech-niques.
An example is the kara-case collision thatwe explained in example 3 in section 5.1.
Theessence of the problem is that the added contentis useful, but there is a grammatical problem.
Inother words, ?what to say?
is ok but ?how tosay?
needs refinement.
This particular problemcan be solved by doing the case-collision check,and by synthesizing the colliding phrases intoone.
These can be better treated in the generationframework.7 ConclusionWe proposed a lead sentence revision methodbased on the operations of phrases that have thesame head in the lead and other sentences.
Thismethod is a type of sentence fusion and is moregeneral than methods that use noun phrasecoreferencing in that it can add phrases of anysyntactic type.
We described the algorithm andthe rules extensively, conducted a lead revisionexperiment, and showed that the algorithm wasable to find semantically appropriate revisions.We also showed that parsing errors mainly de-grade the sentential completeness such as gram-maticality and repetition.46ReferenceRegina Barzilay and Kathleen R. McKeown.
2005.Sentence Fusion for Multidocument News Summa-rization.
Computational Linguistics.
31(3): 298-327.Katja Filippova and Michael Strube.
2008.
SentenceFusion via Dependency Graph Compression.
proc.of the EMNLP 2008: 177-185Hongyan Jing and Kathleen R. McKeown.
1999.
TheDecomposition of Human-Written Summary Sen-tences.
proc.
of the 22nd International Conferenceon Research and Development in Information Re-trieval  SIGIR 99: 129-136.Hongyan Jing and Kathleen R. McKeown.
2000.
Cutand Paste Based Text Summarization, proc.
of the1st meeting of the North American Chapter of theAssociation for Computational Linguistics: 178-185.Taku Kudo and Yuji Matsumoto.
2002.
Japanese De-pendency Analysis using Cascaded Chunking.
Proc.of the 6th Conference on Natural Language Learn-ing 2002: 63-69.Taku Kudo, Kaoru Yamamoto and Yuji Matsumoto.2004.
Applying Conditional Random Fields to Jap-anese Morphological Analysis, proc.
of theEMNLP 2004: 230-237.H.
P. Luhn.
1958.
The Automatic Creation of Litera-ture Abstracts.
Advances in Automatic Text Sum-marization.
The MIT Press: 15-21.Inderjeet Mani, Barbara Gates, and Eric Bloedorn.1999.
Improving Summaries by Revising Them.Proc.
of the 37th Annual Meeting of the Associa-tion for Computational Linguistics.
: 558-565.Ruslan Mitkov 2002, Anaphora Resolution, PearsonEducation.Ani Nenkova.
2008.
Entity-driven Rewrite for Multi-document Summarization, proc.
of the 3rd Interna-tional Joint Conference on Natural Language Gen-eration: 118-125.Jahna C. Otterbacher, Dragomir R. Radev, and AirongLuo 2002, Revisions that Improve Cohesion inMulti-document Summaries: A Preliminary Study.Proc.
of the ACL-02 Workshop on AutomaticSummarization: 27-36.Jacques Robin and Kathleen McKeown.
1996.
Em-pirically designing and evaluating a new revision-based model for summary generation.
Artificial In-telligence.
85: 135-179.47
