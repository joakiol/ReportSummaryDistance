Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 260?265,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsCan Document Selection Help Semi-supervised Learning?A Case Study On Event ExtractionShasha Liao Ralph GrishmanComputer Science DepartmentNew York Universityliaoss@cs.nyu.edu grishman@cs.nyu.eduAbstractAnnotating training data for eventextraction is tedious and labor-intensive.Most current event extraction tasks relyon hundreds of annotated documents, butthis is often not enough.
In this paper, wepresent a novel self-training strategy,which uses Information Retrieval (IR) tocollect a cluster of related documents asthe resource for bootstrapping.
Also,based on the particular characteristics ofthis corpus, global inference is applied toprovide more confident and informativedata selection.
We compare this approachto self-training on a normal newswirecorpus and show that IR can provide abetter corpus for bootstrapping and thatglobal inference can further improveinstance selection.
We obtain gains of1.7% in trigger labeling and 2.3% in rolelabeling through IR and an additional1.1% in trigger labeling and 1.3% in rolelabeling by applying global inference.1 IntroductionThe goal of event extraction is to identifyinstances of a class of events in text.
In additionto identifying the event itself, it also identifiesall of the participants and attributes of eachevent; these are the entities that are involved inthat event.
The same event might be presentedin various expressions, and an expression mightrepresent different events in different contexts.Moreover, for each event type, the eventparticipants and attributes may also appear inmultiple forms and exemplars of the differentforms may be required.
Thus, event extraction isa difficult task and requires substantial trainingdata.
However, annotating events for training isa tedious task.
Annotators need to read thewhole sentence, possibly several sentences, todecide whether there is a specific event or not,and then need to identify the event participants(like Agent and Patient), and attributes (likeplace and time) to complete an event annotation.As a result, for event extraction tasks likeMUC4, MUC6 (MUC 1995) and ACE2005,from one to several hundred annotateddocuments were needed.In this paper, we apply a novel self-trainingprocess on an existing state-of-the-art baselinesystem.
Although traditional self-training onnormal newswire does not work well for thisspecific task, we managed to use informationretrieval (IR) to select a better corpus forbootstrapping.
Also, taking advantage ofproperties of this corpus, cross-documentinference is applied to obtain more?informative?
probabilities.
To the best of ourknowledge, we are the first to apply informationretrieval and global inference to semi-supervisedlearning for event extraction.2 Task DescriptionAutomatic Content Extraction (ACE) defines anevent as a specific occurrence involving260participants 1 ; it annotates 8 types and 33subtypes of events.2 We first present some ACEterminology to understand this task more easily:?
Event mention3: a phrase or sentence withinwhich an event is described, including onetrigger and an arbitrary number of arguments.?
Event trigger: the main word that mostclearly expresses an event occurrence.?
Event mention arguments (roles): the entitymentions that are involved in an eventmention, and their relation to the event.Here is an example:(1) Bob Cole was killed in France today;he was attacked?Table 1 shows the results of the preprocessing,including name identification, entity mentionclassification and coreference, and timestamping.
Table 2 shows the results for eventextraction.MentionIDHead  Ent.ID TypeE1-1 France E-1 GPET1-1 today T1 TimexE2-1 Bob Cole E-2 PERE2-2 He E-2 PERTable 1.
An example of entities and entitymentions and their typesEventtypeTrigger RolePlace Victim TimeDie killed E1-1 E2-1 T1-1Place Target TimeAttack attacked E1-1 E2-2 T1-1Table 2.
An example of event triggers and roles1http://projects.ldc.upenn.edu/ace/docs/English-Events-Guidelines_v5.4.3.pdf2  In this paper, we treat the event subtypesseparately, and no type hierarchy is considered.3  Note that we do not deal with event mentioncoreference in this paper, so each event mention istreated separately.3 Related WorkSelf-training has been applied to several naturallanguage processing tasks.
For event extraction,there are several studies on bootstrapping from aseed pattern set.
Riloff (1996) initiated the idea ofusing document relevance for extracting newpatterns, and Yangarber et al (2000, 2003)incorporated this into a bootstrapping approach,extended by Surdeanu et al (2006) to co-training.Stevenson and Greenwood (2005) suggested analternative method for ranking the candidatepatterns by lexical similarities.
Liao andGrishman (2010b) combined these twoapproaches to build a filtered ranking algorithm.However, these approaches were focused onfinding instances of a scenario/event type ratherthan on argument role labeling.
Starting from aset of documents classified for relevance,Patwardhan and Riloff (2007) created aself-trained relevant sentence classifier andautomatically learned domain-relevant extractionpatterns.
Liu (2009) proposed the BEAR system,which tagged both the events and their roles.However, the new patterns were boostrappedbased on the frequencies of sub-pattern mutationsor on rules from linguistic contexts, and not onstatistical models.The idea of sense consistency was firstintroduced and extended to operate across relateddocuments by (Yarowsky, 1995).
Yangarber etal.
(Yangarber and Jokipii, 2005; Yangarber,2006; Yangarber et al, 2007) appliedcross-document inference to correct localextraction results for disease name, location andstart/end time.
Mann (2007) encoded specificinference rules to improve extraction ofinformation about CEOs (name, start year, endyear).
Later, Ji and Grishman (2008) employed arule-based approach to propagate consistenttriggers and arguments across topic-relateddocuments.
Gupta and Ji (2009) used a similarapproach to recover implicit time information forevents.
Liao and Grishman (2010a) use astatistical model to infer the cross-eventinformation within a document to improve eventextraction.2614 Event Extraction Baseline SystemWe use a state-of-the-art English IE system asour baseline (Grishman et al 2005).
This systemextracts events independently for each sentence,because the definition of event mentionarguments in ACE constrains them to appear inthe same sentence.
The system combines patternmatching with statistical models.
In the trainingprocess, for every event mention in the ACEtraining corpus, patterns are constructed based onthe sequences of constituent heads separating thetrigger and arguments.
A set of MaximumEntropy based classifiers are also trained:?
Argument Classifier: to distinguisharguments of a potential trigger fromnon-arguments.?
Role Classifier: to classify arguments byargument role.
We use the same features asthe argument classifier.?
Reportable-Event Classifier (TriggerClassifier): Given a potential trigger, anevent type, and a set of arguments, todetermine whether there is a reportableevent mention.In the test procedure, each document isscanned for instances of triggers from thetraining corpus.
When an instance is found, thesystem tries to match the environment of thetrigger against the set of patterns associated withthat trigger.
If this pattern-matching processsucceeds, the argument classifier is applied to theentity mentions in the sentence to assign thepossible arguments; for any argument passingthat classifier, the role classifier is used to assigna role to it.
Finally, once all arguments have beenassigned, the reportable-event classifier isapplied to the potential event mention; if theresult is successful, this event mention isreported.5 Our ApproachIn self-training, a classifier is first trained with asmall amount of labeled data.
The classifier isthen used to classify the unlabeled data.Typically the most confident unlabeled points,together with their predicted labels, are added tothe training set.
The classifier is re-trained andthe procedure repeated.
As a result, the criterionfor selecting the most confident examples iscritical to the effectiveness of self-training.To acquire confident samples, we need to firstdecide how to evaluate the confidence for eachevent.
However, as an event contains one triggerand an arbitrary number of roles, a confidentevent might contain unconfident arguments.Thus, instead of taking the whole event, we selecta partial event, containing one confident triggerand its most confident argument, to feed back tothe training system.For each mention mi, its probability of filling arole r in a reportable event whose trigger is t iscomputed by:?PRoleOfTrigger(mi,r,t) = PArg(mi) ?
PRole(mi,r) ?
PEvent (t)where PArg(mi) is the probability from theargument classifier, PRole(mi,r) is that from therole classifier, and PEvent(t) is that from thetrigger classifier.
In each iteration, we added themost confident <role, trigger> pairs to thetraining data, and re-trained the system.5.1 Problems of Traditional Self-training(ST)However, traditional self-training does notperform very well (see our results in Table 3).The newly added samples do not improve thesystem performance; instead, its performancestays stable, and even gets worse after severaliterations.We analyzed the data, and found that this iscaused by two common problems of traditionalself-training.
First, the classifier uses its ownpredictions to train itself, and so a classificationmistake can reinforce itself.
This is particularlytrue for event extraction, due to its relatively poorperformance, compared to other NLP tasks, likeNamed Entity Recognition, parsing, orpart-of-speech tagging, where self-training hasbeen more successful.
Figure 1 shows that theprecision using the original training data is notvery good: while precision improves withincreasing classifier threshold, about 1/3 of theroles are still incorrectly tagged at a threshold of0.90.2620.350.40.450.50.550.60.650.70.750.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Trigger LabelingArgument LabelingRole LabelingFigure 1.
Precision on the original training datawith different thresholds (from 0.0 to 0.9)Another problem of self-training is thatnothing ?novel?
is added because the mostconfident examples are those frequently seen inthe training data and might not provide ?new?information.
Co-training is a form ofself-training which can address this problem tosome extent.
However, it requires two views ofthe data, where each example is described usingtwo different feature sets that provide different,complementary information.
Ideally, the twoviews are conditionally independent  and eachview is sufficient (Zhu, 2008).
Co-training hashad some success in training (binary) semanticrelation extractors for some relations, where thetwo views correspond to the arguments of therelation and the context of these arguments(Agichtein and Gravano 2000).
However, it hashad less success for event extraction becauseevent arguments may participate in multipleevents in a corpus and individual event instancesmay omit some arguments.5.2 Self-training on Information RetrievalSelected Corpus (ST_IR)To address the first problem (low precision ofextracted events), we tried to select a corpuswhere the baseline system can tag the instanceswith greater confidence.
(Ji and Grishman 2008)have observed that the events in a cluster ofdocuments on the same topics as documents inthe training corpus can be tagged moreconfidently.
Thus, we believe that bootstrappingon a corpus of topic-related documents shouldperform better than a regular newswire corpus.We followed Ji and Grishman (2008)?sapproach and used the INDRI retrieval system4(Strohman et al, 2005) to obtain the top N4 http://www.lemurproject.org/indri/related documents for each annotated documentin the training corpus.
The query is event-basedto insure that related documents contain the sameevents.
For each training document, we constructan INDRI query from the triggers and arguments.For example, for sentence (1) in section 2, we usethe keywords ?killed?, ?attacked?, ?France?,?Bob Cole?, and ?today?
to extract relateddocuments.
Only names and nominal argumentswill be used; pronouns appearing as argumentsare not included.
For each argument we also addother names coreferential with the argument.5.3 Self-training using Global Inference(ST_GI)Although bootstrapping on related documentscan solve the problem of ?confidence?
to someextent, the ?novelty?
problem still remains:  thetop-ranked extracted events will be too similar tothose in the training corpus.
To address thisproblem, we propose to use a simple form ofglobal inference based on the specialcharacteristics of related-topic documents.Previous studies pointed out that informationfrom wider scope, at the document orcross-document level, could provide non-localinformation to aid event extraction (Ji andGrishman 2008, Liao and Grishman 2010a).There are two common assumptions within acluster of related documents (Ji and Grishman2008):?
Trigger Consistency Per Cluster: if oneinstance of a word triggers an event, otherinstances of the same word will trigger eventsof the same type.?
Role Consistency Per Cluster: if one entityappears as an argument of multiple events ofthe same type in a cluster of relateddocuments, it should be assigned the samerole each time.Based on these assumptions, if a trigger/rolehas a low probability from the baseline system,but a high one from global inference, it meansthat the local context of this trigger/role tag is notfrequently seen in the training data, but the tag isstill confident.
Thus, we can confidently add it tothe training data and it can provide novelinformation which the samples confidentlytagged by the baseline system cannot provide.263To start, the baseline system extracts a set ofevents and estimates the probability that aparticular instance of a word triggers an event ofthat type, and the probability that it takes aparticular argument.
The global inferenceprocess then begins by collecting all theconfident triggers and arguments from a clusterof related documents.5 For each trigger word andevent type, it records the highest probability(over all instances of that word in the cluster) thatthe word triggers an event of that type.
For eachargument, within-document and cross-documentcoreference6 are used to collect all instances ofthat entity; we then compute the maximumprobability (over all instances) of that argumentplaying a particular role in a particular eventtype.
These maxima will then be used in place ofthe locally-computed probabilities in computingthe probability of each trigger-argument pair inthe formula for PRoleOfTrigger given above.7  Forexample, if the entity ?Iraq?
is tagged confidently(probability > 0.9) as the ?Attacker?
rolesomewhere in a cluster, and there is anotherinstance where from local information it is onlytagged with 0.1 probability to be an ?Attacker?role, we use probability of 0.9 for both instances.In this way, a trigger pair containing thisargument is more likely to be added into thetraining data through bootstrapping, because wehave global evidence that this role probability ishigh, although its local confidence is low.
In thisway, some novel trigger-argument pairs will bechosen, thus improving the baseline system.6 ResultsWe randomly chose 20 newswire texts from theACE 2005 training corpora (from March to Mayof 2003) as our evaluation set, and used the5 In our experiment, only triggers and roles withprobability higher than 0.9 will be extracted.6 We use a statistical within-document coreferencesystem (Grishman et al 2005), and a simplerule-based cross-document coreference system,where entities sharing the same names will be treatedas coreferential across documents.7 If a word or argument has multiple tags (differentevent types or roles) in a cluster, and the differencein the probabilities of the two tags is less than somethreshold, we treat this as a ?conflict?
and do not usethe conflicting information for global inference.remaining newswire texts as the original trainingdata (83 documents).
For self-training, we picked10,000 consecutive newswire texts from theTDT5 corpus from 20038 for the ST experiment.For ST_IR and ST_GI, we retrieved the best N(using N = 25, which (Ji and Grishman 2008)found to work best) related texts for each trainingdocument from the English TDT5 corpusconsisting of 278,108 news texts (from April toSeptember of 2003).
In total we retrieved 1650texts; the IR system returned no texts or fewerthan 25 texts for some training documents.
Ineach iteration, we extract 500 trigger andargument pairs to add to the training data.Results (Table 3) show that bootstrapping onan event-based IR corpus can produceimprovements on all three evaluations, whileglobal inference can yield further gains.TriggerlabelingArgumentlabelingRolelabelingBaseline 54.1 39.2 35.4ST 54.2 40.0 34.6ST_IR 55.8 42.1 37.7ST_GI 56.9 43.8 39.0Table 3.
Performance (F score) with differentself-training strategies after 10 iterations7 Conclusions and Future WorkWe proposed a novel self-training process forevent extraction that involves informationretrieval (IR) and global inference to providemore accurate and informative instances.Experiments show that using an IR-selectedcorpus improves trigger labeling F score 1.7%,and role labeling 2.3%.
Global inference canachieve further improvement of 1.1% for triggerlabeling, and 1.3% for role labeling.
Also, thisbootstrapping involves processing a much8  We selected all bootstrapping data from 2003newswire, with the same genre and time period asACE 2005 data to avoid possible influences ofvariations in the genre or time period on thebootstrapping.
Also, we selected 10,000 documentsbecause this size of corpus yielded a set ofconfidently-extracted events (probability > 0.9)roughly comparable in size to those extracted fromthe IR-selected corpus; a larger corpus would haveslowed the bootstrapping.264smaller but more closely related corpus, which ismore efficient.
Such pre-selection of documentsmay benefit bootstrapping for other NLP tasks aswell, such as name and relation extraction.AcknowledgmentsWe would like to thank Prof. Heng Ji for her kindhelp in providing IR data and useful suggestions.ReferencesEugene Agichtein and Luis Gravano.
2000.Snowball:  Extracting relations from largeplain-text collections.
In Proceedings of 5th ACMInternational Conference on Digital Libraries.Ralph Grishman, David Westbrook and AdamMeyers.
2005.
NYU?s English ACE 2005 SystemDescription.
In Proc.
ACE 2005 EvaluationWorkshop, Gaithersburg, MD.Prashant Gupta and Heng Ji.
2009.
PredictingUnknown Time Arguments based on Cross-EventPropagation.
In Proceedings of ACL-IJCNLP2009.Heng Ji and Ralph Grishman.
2008.
Refining EventExtraction through Cross-Document Inference.
InProceedings of ACL-08: HLT, pages 254?262,Columbus, OH, June.Shasha Liao and Ralph Grishman.
2010a.
UsingDocument Level Cross-Event Inference toImprove Event Extraction.
In Proceedings of ACL2010.Shasha Liao and Ralph Grishman.
2010b.
FilteredRanking for Bootstrapping in Event Extraction.
InProceedings of COLING 2010.Ting Liu.
2009.
Bootstrapping events and relationsfrom text.
Ph.D. thesis, State University of NewYork at Albany.Gideon Mann.
2007.
Multi-document RelationshipFusion via Constraints on Probabilistic Databases.In Proceedings of HLT/NAACL 2007.
Rochester,NY, US.MUC.
1995.
Proceedings of the Sixth MessageUnderstanding Conference (MUC-6), San Mateo,CA.
Morgan Kaufmann.S.
Patwardhan and E. Riloff.
2007.
EffectiveInformation Extraction with Semantic AffinityPatterns and Relevant Regions.
In Proceedings ofthe 2007 Conference on Empirical Methods inNatural Language Processing (EMNLP-07).Ellen Riloff.
1996.
Automatically GeneratingExtraction Patterns from Untagged Text.
InProceedings of Thirteenth National Conference onArtificial Intelligence (AAAI-96), pp.
1044-1049.M.
Stevenson and M. Greenwood.
2005.
A SemanticApproach to IE Pattern Induction.
In Proceedingsof ACL 2005.Trevor Strohman, Donald Metzler, Howard Turtleand W. Bruce Croft.
2005.
Indri: ALanguage-model based Search Engine forComplex Queries (extended version).
TechnicalReport IR-407, CIIR, UMass Amherst, US.Mihai Surdeanu, Jordi Turmo, and Alicia Ageno.2006.
A Hybrid Approach for the Acquisition ofInformation Extraction Patterns.
In Proceedings ofthe EACL 2006 Workshop on Adaptive TextExtraction and Mining (ATEM 2006).Roman Yangarber, Ralph Grishman, PasiTapanainen, and Silja Huttunen.
2000.
AutomaticAcquisition of Domain Knowledge forInformation Extraction.
In Proceedings ofCOLING 2000.Roman Yangarber.
2003.
Counter-Training inDiscovery of Semantic Patterns.
In Proceedings ofACL2003.Roman Yangarber and Lauri Jokipii.
2005.Redundancy-based Correction of AutomaticallyExtracted Facts.
In Proceedings of HLT/EMNLP2005.
Vancouver, Canada.Roman Yangarber.
2006.
Verification of Facts acrossDocument Boundaries.
In Proceedings ofInternational Workshop on Intelligent InformationAccess.
Helsinki, Finland.Roman Yangarber, Clive Best, Peter von Etter, FlavioFuart, David Horby and Ralf Steinberger.
2007.Combining Information about Epidemic Threatsfrom Multiple Sources.
In Proceedings of RANLP2007 workshop on Multi-source, MultilingualInformation Extraction and Summarization.Borovets, Bulgaria.David Yarowsky.
1995.
Unsupervised Word SenseDisambiguation Rivaling Supervised Methods.
InProceedings of ACL 1995.
Cambridge, MA.Xiaojin Zhu.
2008 Semi-Supervised LearningLiterature Survey.
http:// pages.cs.wisc.edu/~jerryzhu/research/ssl/semireview.html265
