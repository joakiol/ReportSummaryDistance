Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 631?642, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsEnlarging Paraphrase Collections through Generalization and InstantiationAtsushi FujitaFuture University Hakodate116-2 Kameda-nakano-cho,Hakodate, Hokkaido, 041-8655, Japanfujita@fun.ac.jpPierre Isabelle Roland KuhnNational Research Council Canada283 Alexandre-Tache?
Boulevard,Gatineau, QC, J8X 3X7, Canada{Pierre.Isabelle, Roland.Kuhn}@nrc.caAbstractThis paper presents a paraphrase acquisitionmethod that uncovers and exploits generali-ties underlying paraphrases: paraphrase pat-terns are first induced and then used to col-lect novel instances.
Unlike existing methods,ours uses both bilingual parallel and monolin-gual corpora.
While the former are regarded asa source of high-quality seed paraphrases, thelatter are searched for paraphrases that matchpatterns learned from the seed paraphrases.We show how one can use monolingual cor-pora, which are far more numerous and largerthan bilingual corpora, to obtain paraphrasesthat rival in quality those derived directly frombilingual corpora.
In our experiments, thenumber of paraphrase pairs obtained in thisway from monolingual corpora was a largemultiple of the number of seed paraphrases.Human evaluation through a paraphrase sub-stitution test demonstrated that the newly ac-quired paraphrase pairs are of reasonable qual-ity.
Remaining noise can be further reducedby filtering seed paraphrases.1 IntroductionParaphrases are semantically equivalent expressionsin the same language.
Because ?equivalence?
is themost fundamental semantic relationship, techniquesfor generating and recognizing paraphrases play animportant role in a wide range of natural languageprocessing tasks (Madnani and Dorr, 2010).In the last decade, automatic acquisition of knowl-edge about paraphrases from corpora has been draw-ing the attention of many researchers.
Typically, theacquired knowledge is simply represented as pairs ofsemantically equivalent sub-sentential expressionsas in (1).
(1) a. look like ?
resembleb.
control system ?
controllerThe challenge in acquiring paraphrases is to ensuregood coverage of the targeted classes of paraphrasesalong with a low proportion of incorrect pairs.
How-ever, no matter what type of resource has been used,it has proven difficult to acquire paraphrase pairswith both high recall and high precision.Among various types of corpora, monolingualcorpora can be considered the best source for high-coverage paraphrase acquisition, because there isfar more monolingual than bilingual text avail-able.
Most methods that exploit monolingual cor-pora rely on the Distributional Hypothesis (Harris,1968): expressions that appear in similar contextsare expected to have similar meaning.
However,if one uses purely distributional criteria, it is dif-ficult to distinguish real paraphrases from pairs ofexpressions that are related in other ways, such asantonyms and cousin words.In contrast, since the work in (Bannard andCallison-Burch, 2005), bilingual parallel corporahave been acknowledged as a good source of high-quality paraphrases: paraphrases are obtained byputting together expressions that receive the sametranslation in the other language (pivot language).Because translation expresses a specific meaningmore directly than context in the aforementioned ap-proach, pairs of expressions acquired in this mannertend to be correct paraphrases.
However, the cov-erage problem remains: there is much less bilingualparallel than monolingual text available.Our objective in this paper is to obtain para-phrases that have high quality (like those extractedfrom bilingual parallel corpora via pivoting) but canbe generated in large quantity (like those extracted631from monolingual corpora via contextual similar-ity).
To achieve this, we propose a method that ex-ploits general patterns underlying paraphrases anduses both bilingual parallel and monolingual sourcesof information.
Given a relatively high-quality set ofparaphrases obtained from a bilingual parallel cor-pus, a set of paraphrase patterns is first induced.Then, appropriate instances of such patterns, i.e.,potential paraphrases, are harvested from a mono-lingual corpus.After reviewing existing methods in Section 2,our method is presented in Section 3.
Section 4describes our experiments in acquiring paraphrasesand presents statistics summarizing the coverage ofour method.
Section 5 describes a human evaluationof the quality of the acquired paraphrases.
Finally,Section 6 concludes this paper.2 Literature on Paraphrase AcquisitionThis section summarizes existing corpus-basedmethods for paraphrase acquisition, following theclassification in (Hashimoto et al2011): similarity-based and alignment-based methods.2.1 Similarity-based MethodsTechniques that use monolingual (non-parallel) cor-pora mostly rely on the Distributional Hypothesis(Harris, 1968).
Because a large quantity of mono-lingual data is available for many languages, a largenumber of paraphrase candidates can be acquired(Lin and Pantel, 2001; Pas?ca and Dienes, 2005; Bha-gat and Ravichandran, 2008, etc.).
The recipes pro-posed so far are based on three main ingredients, i.e.,features used for representing context of target ex-pression (contextual features), criteria for weightingand filtering features, and aggregation functions.A drawback of relying only on contextual simi-larity is that it tends to give high scores to semanti-cally related but non-equivalent expressions, such asantonyms and cousin words.
To enhance the preci-sion of the results, filtering mechanisms need to beintroduced (Marton et al2011).2.2 Alignment-based MethodsPairs of expressions that get translated to the sameexpression in a different language can be regarded asparaphrases.
On the basis of this hypothesis, Barzi-lay and McKeown (2001) and Pang et al2003)created monolingual parallel corpora from multiplehuman translations of the same source.
Then, theyextracted corresponding parts of such parallel sen-tences as sub-sentential paraphrases.Leveraging recent advances in statistical ma-chine translation (SMT), Bannard and Callison-Burch (2005) proposed a method for acquiring sub-sentential paraphrases from bilingual parallel cor-pora.
As in SMT, a translation table is first built onthe basis of alignments between expressions, such aswords, phrases, and subtrees, across a parallel sen-tence pair.
Then, pairs of expressions (e1, e2) in thesame language that are aligned with the same ex-pressions in the other language (pivot language) areextracted as paraphrases.
The likelihood of e2 beinga paraphrase of e1 is given byp(e2|e1) =?f?Tr(e1,e2)p(e2|f)p(f |e1), (1)where Tr(e1, e2) stands for the set of shared trans-lations of e1 and e2.
Each factor p(e|f) and p(f |e)is estimated from the number of times e and f arealigned and the number of occurrences of each ex-pression in each language.
Kok and Brockett (2010)showed how one can discover paraphrases that donot share any translation in one language by travers-ing a graph created from multiple translation tables,each corresponding to a bilingual parallel corpus.This approach, however, suffers from a cover-age problem, because both monolingual parallel andbilingual parallel corpora tend to be significantlysmaller than monolingual non-parallel corpora.
Theacquired pairs of expressions include some non-paraphrases as well.
Many of these come from er-roneous alignments, which are particularly frequentwhen the given corpus is small.Monolingual comparable corpora have also beenexploited as sources of paraphrases using alignment-based methods.
For instance, multiple news arti-cles covering the same event (Shinyama et al2002;Barzilay and Lee, 2003; Dolan et al2004; Wubbenet al2009) have been used.
Such corpora havealso been created manually through crowdsourcing(Chen and Dolan, 2011).
However, the availabil-ity of monolingual comparable corpora is very lim-ited for most languages; thus, approaches relyingon these corpora have typically produced only very632small collections of paraphrases.
Hashimoto et al(2011) found a way around this limitation by collect-ing sentences that constitute explicit definitions ofparticular words or phrases from monolingual non-parallel Web documents, pairing sentences that de-fine the same noun phrase, and then finding corre-sponding phrases in each sentence pair.
One limita-tion of this approach is that it requires a considerableamount of labeled data for both the corpus construc-tion and the paraphrase extraction steps.2.3 SummaryExisting methods have investigated one of the fol-lowing four types of corpora as their principal re-source1: monolingual non-parallel corpora, mono-lingual parallel corpora, monolingual comparablecorpora, and bilingual parallel corpora.
No matterwhat type of resource has been used, however, ithas proven difficult to acquire paraphrases with bothhigh recall and precision, with the possible excep-tion of the method in (Hashimoto et al2011) whichrequires large amounts of labeled data.3 Proposed MethodWhile most existing methods deal with expressionsonly at the surface level, ours exploits generalitiesunderlying paraphrases to achieve better coveragewhile retaining high precision.
Furthermore, unlikeexisting methods, ours uses both bilingual paralleland monolingual non-parallel corpora as sources foracquiring paraphrases.The process is illustrated in Figure 1.
First, aset of high-quality seed paraphrases, PSeed , is ac-quired from bilingual parallel corpora by using analignment-based method.
Then, our method collectsfurther paraphrases through the following two steps.Generalization (Step 2): Paraphrase patterns arelearned from the seed paraphrases, PSeed .Instantiation (Step 3): A novel set of paraphrasepairs, PHvst , is finally harvested from mono-lingual non-parallel corpora using the learnedpatterns; each newly acquired paraphrase pairis assessed by contextual similarity.1Chan et al2011) used monolingual corpora only for re-ranking paraphrases obtained from bilingual parallel corpora.To the best of our knowledge, bilingual comparable corporahave never been used as sources for acquiring paraphrases.Monolingual Non-parallel CorpusStep 1.
Seed Paraphrase AcquisitionStep 2.
Paraphrase Pattern InductionStep 3.
Paraphrase Instance Acquisition?health issue?
?
?health problem?
?look like?
?
?resemble?
?regional issue?
?
?regional problem?
?health issue?
?
?probl?me de sant??
?health problem?
?
?probl?me de sant??
?look like?
?
?ressemble?
?regional issue?
?
?probl?me r?gional?
?regional problem?
?
?probl?me r?gional?
?resemble?
?
?ressemble?
?X issue?
?
?X problem?
;         {food, regional, ...}?backlog issue?
?
?backlog problem?
?communal issue?
?
?communal problem?
?phishing issue?
?
?phishing problem?
?spatial issue?
?
?spatial problem?Translation TablePSeed: Seed ParaphrasesParaphrase PatternsPHvst: Novel ParaphrasesBilingual Parallel CorpusFigure 1: Process of paraphrase acquisition.The set PSeed acquired early in the process can bepooled with the set PHvst harvested in the last stageof the process.3.1 Step 1.
Seed Paraphrase AcquisitionThe goal of the first step is to obtain a set of high-quality paraphrase pairs, PSeed .For this purpose, alignment-based methods withbilingual or monolingual parallel corpora are prefer-able to similarity-based methods applied to non-parallel corpora.
Among various options, in this pa-per, we start from the standard technique proposedby Bannard and Callison-Burch (2005) with bilin-gual parallel corpora (see also Section 2.2).
In par-ticular, we assume the phrase-based SMT frame-work (Koehn et al2003).
Then, we purify the re-sults with several filtering methods.The phrase pair extraction process of phrase-based SMT systems aims at high recall for increasedrobustness of the translation process.
As a result,a naive application of the paraphrase acquisitionmethod produces pairs of expressions that are notexact paraphrases.
For instance, the algorithm ex-plained in Koehn (2009, p.134) extracts both ?dass?and ?, dass?
as counterparts of ?that?
from the sen-tence pair.
To reduce that kind of noise, we applysome filtering techniques to the candidate translationpairs.
First, statistically unreliable translation pairs(Johnson et al2007) are filtered out.
Then, we alsofilter out phrases made up entirely of stop words (in-cluding punctuation marks), both in the language ofinterest and in the pivot language.Let PRaw be the initial set of paraphrase pairs ex-tracted from the sanitized translation table.
We first633lp: control apparatusrp: control devicep(rp|lp).172rp: control system.032rp: the control device.015rp: control device of the.005rp: controlling device.004rp: control system of.003rp: a control system for an.001rp: a controlling device.001Figure 2: RHS-filtering for ?control apparatus?.rp: control devicelp: controller p(lp|rp).153lp: control apparatus.135lp: the control apparatus.010lp: control apparatus of .008lp: controlling unit .004lp: control equipment.002lp: controller for a.001lp: to the control apparatus.001Figure 3: LHS-filtering for ?control device?.discard pairs whose difference comprises only stopwords, such as ?the schools?
?
?schools and?.
Wealso remove pairs containing only singular-pluraldifferences, such as ?family unit?
?
?family units?.Depending on the language of interest, other types ofmorphological variants, such as those shown in (2),may also be ignored.
(2) a.
?europe?enne?
?
?europe?en?
(Gender in French)b.
?guten Lo?sungen?
?
?gute Lo?sungen?
(Case in German)We further filter out less reliable pairs, such asthose shown with dotted lines in Figures 2 and 3.This is carried out by comparing the right-hand side(RHS) phrases of each left-hand side (LHS) phrase,and vice versa2.
Given a set of paraphrase pairs,RHS phrases corresponding to the same LHS phraselp are compared.
A RHS phrase rp is not licensed ifflp has another RHS phrase rp?
(?= rp) which satis-fies the following two conditions (see also Figure 2).?
rp?
is a word sub-sequence of rp?
rp?
is a more likely paraphrase than rp,i.e., p(rp ?|lp) > p(rp|lp)LHS phrases for each RHS phrase rp are also com-pared in a similar manner, i.e., a LHS phrase lp isnot qualified as a legitimate source of rp iff rp hasanother LHS phrase lp?
(?= lp) which satisfies thefollowing conditions (see also Figure 3).?
lp?
is a word sub-sequence of lp?
lp?
is a more likely source than lp,i.e., p(lp ?|rp) > p(lp|rp)The two directions of filtering are separately appliedand the intersection of their results is retained.2cf.
Denkowski and Lavie (2011); they only compared eachRHS phrase to its corresponding LHS phrase.Candidate pairs are finally filtered on the basisof their reliability score.
Traditionally, a threshold(thp) on the conditional probability given by Eq.
(1)is used (Du et al2010; Max, 2010; Denkowskiand Lavie, 2011, etc.).
Furthermore, we also re-quire that LHS and RHS phrases exceed a thresh-old (ths ) on their contextual similarity in a mono-lingual corpus.
This paper neither proposes a spe-cific recipe nor makes a comprehensive comparisonof existing recipes for computing contextual simi-larity, although one particular recipe is used in ourexperiments (see Section 4.1).3.2 Step 2.
Paraphrase Pattern InductionFrom a set of seed paraphrases, PSeed , paraphrasepatterns are induced.
For instance, from paraphrasesin (3), we induce paraphrase patterns in (4).
(3) a.
?restraint system?
?
?restraint apparatus?b.
?movement against racism??
?anti-racism movement?c.
?middle eastern countries??
?countries in the middle east?
(4) a.
?X system?
?
?X apparatus?b.
?X against Y ?
?
?anti-Y X?c.
?X eastern Y ?
?
?Y in the X east?Word pairs of LHS and RHS phrases will be re-placed with variable slots iff they are fully identi-cal or singular-plural variants.
Note that stop wordsare retained.
While a deeper level of lexical cor-respondences, such as ?eastern?
and ?east?
in (3c)and ?system?
and ?apparatus?
in (3a), could be cap-tured, this would require the use of rich languageresources, thereby making the method less portableto resource-poor languages.634Note that our aim is to automatically capture gen-eral paraphrase patterns of the kind that have some-times been manually described (Jacquemin, 1999;Fujita et al2007).
This is different from ap-proaches that attach variable slots to paraphrases forcalculating their similarity (Lin and Pantel, 2001;Szpektor and Dagan, 2008) or for constrainingthe context in which they are regarded legitimate(Callison-Burch, 2008; Zhao et al2009).3.3 Step 3.
Paraphrase Instance AcquisitionGiven a set of paraphrase patterns, such as thoseshown in (4), a set of novel instances, i.e., novelparaphrases, PHvst , will now be harvested frommonolingual non-parallel corpora.
In other words,a set of appropriate slot-fillers will be extracted.First, expressions that match both elements ofthe pattern, except stop words, are collected froma given monolingual corpus.
Pattern matching alonemay generate inappropriate pairs, so we then assessthe legitimacy of each collected slot-filler.Let LHS (w) and RHS (w) be the expressionsgenerated by instantiating the k variable slots inLHS and RHS phrases of the pattern with a k-tupleof slot-fillers w (= w1, .
.
.
, wk), respectively.
Weestimate how likelyRHS (w) is to be a paraphrase ofLHS (w) based on the contextual similarity betweenthem using a monolingual corpus; a pair of phrasesis discarded if they are used in substantially dissim-ilar contexts.
We use the same recipe and thresholdvalue for ths with Step 1 in our experiments.Contextual similarity of antonyms and cousinwords can also be high, as they are often used in sim-ilar contexts.
However, this is not a problem in ourframework, because semantic equivalence betweenLHS (w) and RHS (w) is almost entirely guaran-teed as a result of the way the corresponding patternswere learned from a bilingual parallel corpus.3.4 CharacteristicsIn terms of coverage, PHvst is expected to be greatlylarger than PSeed , although it will not cover to-tally different pairs of paraphrases, such as thoseshown in (1).
On the other hand, the quality ofPHvst depends on that of PSeed .
Unlike in the puresimilarity-based method, PHvst is constrained by theparaphrase patterns derived from the set of high-quality paraphrases, PSeed , and will therefore gen-erally exclude the kind of semantically similar butnon-equivalent pairs that contextual similarity alonetends to extract alongside real paraphrases.As mentioned in Section 3.1, other types of meth-ods can be used for obtaining high-quality seedparaphrases, PSeed .
For instance, the supervisedmethod proposed by Hashimoto et al2011) usesthe existence of shared words as a feature to deter-mine whether the given pair of expressions are para-phrases, and thereby extracts many pairs sharing thesame words.
Thus, their output has a high potentialto be used as an alternative seed for our method.Another advantage of our method is that it doesnot require any labeled data, unlike the super-vised methods proposed by Zhao et al2009) andHashimoto et al2011).4 Quantitative Impact4.1 Experimental SettingsTwo different sets of corpora were used as datasources; in both settings, we acquired English para-phrases.Europarl: The English-French version of the Eu-roparl Parallel Corpus3 consisting of 1.8M sen-tence pairs (51M words in English and 56Mwords in French) was used as a bilingual par-allel corpus, while its English side and the En-glish side of the 109 French-English corpus4consisting of 23.8M sentences (649M words)were used as monolingual data.Patent: The Japanese-English Patent Translationdata (Fujii et al2010) consisting of 3.2M sen-tence pairs (122M morphemes in Japanese and106Mwords in English) was used as a bilingualparallel corpus, while its English side and the30.0M sentences (626M words) from the 2007chapter of NTCIR unaligned patent documentswere used as monolingual data.To study the behavior of our method for differentamounts of bilingual parallel data, we carried outlearning curve experiments.We used our in-house tokenizer for segmentationof English and French sentences and MeCab5 forJapanese sentences.3http://statmt.org/europarl/, release 64http://statmt.org/wmt10/training-giga-fren.tar5http://mecab.sourceforge.net/, version 0.98635103104105106107108106 107 108# of paraphrasepairs# of words in the English side of bilingual corpusPRawPRaw (thp=0.01)PSeed (thp=?, ths=?
)PSeed (thp=0.01, ths=?
)103104105106107108106 107 108# of paraphrasepairs# of words in the English side of bilingual corpusPRawPRaw (thp=0.01)PSeed (thp=?, ths=?
)PSeed (thp=0.01, ths=?
)Figure 4: # of paraphrase pairs in PSeed (left: Europarl, right: Patent).Stop word lists for sanitizing translation pairs andparaphrase pairs were manually compiled: we enu-merated 442 English words, 193 French words, and149 Japanese morphemes, respectively.From a bilingual parallel corpus, a translation ta-ble was created by our in-house phrase-based SMTsystem, PORTAGE (Sadat et al2005).
Phrasealignments of each sentence pair were identified bythe heuristic ?grow-diag-final?6 with a maximumphrase length 8.
The resulting translation pairs werethen filtered with the significance pruning techniqueof (Johnson et al2007), using ?
+ ?
as threshold.As contextual features for computing similarityof each paraphrase pair, all of the 1- to 4-grams ofwords adjacent to each occurrence of a phrase werecounted.
This is a compromise between less expen-sive but noisier approaches, such as bag-of-words,and more accurate but more expensive approachesthat incorporate syntactic features (Lin and Pantel,2001; Shinyama et al2002; Pang et al2003;Szpektor and Dagan, 2008).
Contextual similarity isfinally measured by taking cosine between two fea-ture vectors.4.2 Statistics on Acquired ParaphrasesSeed Paraphrases (PSeed )Figure 4 shows the number of paraphrase pairsPSeed obtained from the bilingual parallel corpora.The general trend is simply that the larger the cor-pus is, the more paraphrases are acquired.Given the initial set of paraphrases, PRaw (???
),our filtering techniques (?2?)
discarded a large por-tion (63-75% in Europarl and 43-64% in Patent) ofthem.
Pairs with zero similarity were also filteredout, i.e., ths = ?.
This suggests that many incorrect6http://statmt.org/moses/?n=FactoredTraining.AlignWordsand/or relatively useless pairs, such as those shownin Figures 2 and 3, had originally been acquired.Lines with ???
show the results based on awidely-used threshold value on the conditional prob-ability in Eq.
(1), i.e., thp = 0.01 (Du et al2010;Max, 2010; Denkowski and Lavie, 2011, etc.).
Thepercentage of paraphrase pairs thereby discardedvaried greatly depending on the corpus size (17-78%in Europarl and 31-82% in Patent), suggesting thatthe threshold value should be determined dependingon the given corpus.
In the following experiment,however, we conform to the convention thp = 0.01(???)
to ensure the quality of PSeed that we will beusing for inducing paraphrase patterns, even thoughthis results in discarding some less frequent but cor-rect paraphrase pairs, such as ?control apparatus??
?controlling device?
in Figure 2.Paraphrase PatternsFigures 5 and 6 show the number of paraphrasepatterns that our method induced and their cover-age against PSeed , respectively.
Due to their ratherrigid form, the patterns covered no more than 15%of PSeed in Europarl.
In contrast, a higher propor-tion of PSeed in Patent was generalized into patterns.We speculate it is because the patent domain con-tains many expressions, including technical terms,that have similar variations of constructions.The acquired patterns were mostly one-variablepatterns: 88-93% and 80-91% of total patterns fordifferent variants of the Europarl and Patent set-tings, respectively.
Given that there are far moreone-variable patterns than other types, and that one-variable patterns are the simplest type, we hence-forth focus on them.
More complex patterns, includ-ing two-variable patterns (7-11% and 8-17% in eachsetting), will be investigated in our future work.636102103104105106106 107 108# of paraphrasepatterns# of words in the English side of bilingual corpusAll (Patent)1-var (Patent)All (Europarl)1-var (Europarl)Figure 5: # of paraphrase patterns.010203040106 107 108Coverage[%]# of words in the English side of bilingual corpusAll (Patent)1-var (Patent)All (Europarl)1-var (Europarl)Figure 6: Coverage of the paraphrase patterns.103104105106107108106 107 108# of paraphrasepairs# of unique LHSphrases# of words in the English side of bilingual corpusPair in PHvstLHS in PHvstPair in PSeedLHS in PSeed18.1M1.22M103104105106107108106 107 108# of paraphrasepairs# of unique LHSphrases# of words in the English side of bilingual corpusPair in PHvstLHS in PHvstPair in PSeedLHS in PSeed28.7M1.41MFigure 7: # of paraphrase pairs and unique LHS phrases in PSeed and PHvst (left: Europarl, right: Patent).Novel Paraphrases (PHvst )Using the paraphrase patterns, novel paraphrasepairs, PHvst , were harvested from the monolingualnon-parallel corpora.
In this experiment, we onlyretained one-variable patterns and regarded only sin-gle words as slot-fillers for them.
Nevertheless, wemanaged to acquire a large number of paraphrasepairs as depicted in Figure 7, where pairs havingzero similarity were excluded.
For instance, whenthe full size of bilingual parallel corpus in Patent wasused, we acquired 1.41M pairs of seed paraphrases,PSeed , and 28.7M pairs of novel paraphrases, PHvst .In other words, our method expanded PSeed by about21 times.
The number of unique LHS phrases thatPHvst covers was also significantly larger than thatof PSeed .Figure 8 highlights the remarkably large ratio ofPHvst to PSeed in terms of the number of paraphrasepairs and the number of unique LHS phrases.
Thesmaller the bilingual corpus is, the higher the ratiois, except when there is only a very small amount ofEuroparl data.
This demonstrates that our method isquite powerful, given a minimum amount of data.Another striking difference between PSeed andPHvst is the average number of RHS phrases perunique LHS phrase, i.e., their relative yield.
Asdisplayed in Figure 9, the yield for PHvst increasedrapidly with the scaling up of the bilingual cor-pus, while that of PSeed only grew slowly.
Thealignment-based method with bilingual corpora can-not produce very many RHS phrases per uniqueLHS phrase due to its reliance on conditional prob-ability and the surface level processing.
In con-trast, our method does not limit the number of RHSphrases: each RHS phrase is separately assessed byits similarity to the corresponding LHS phrase.
Onelimitation of our method is that it cannot achievehigh yield for PHvst whenever only a small num-ber of paraphrase patterns can be extracted from thebilingual corpus (see also Figure 5).Both the ratio of PHvst to PSeed and the relativeyield could probably be increased by scaling up themonolingual corpus.
For instance, in the patent do-main, monolingual documents 10 times larger thanthe one used in the above experiments are avail-able at the NTCIR project7.
It would be interestingto compare the relative gains brought by in-domainversus general-purpose corpora.7http://ntcir.nii.ac.jp/PatentMT-2/637020406080106 107 108Ratioof PHvst to PSeed# of words in the English side of bilingual corpusLHS (Patent)Pair (Patent)LHS (Europarl)Pair (Europarl)Figure 8: Ratio of PHvst to PSeed .12345106 107 108Avg.
#ofRHS phrases# of words in the English side of bilingual corpusPHvst (Patent)PSeed (Patent)PHvst (Europarl)PSeed (Europarl)Figure 9: Average # of RHS phrases per LHS phrase.1031041051061071080  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1# of paraphrasepairsProbability threshold thpPHvst (Patent)PSeed (Patent)PHvst (Europarl)PSeed (Europarl) 1031041051061071080  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1# of paraphrasepairsSimilarity threshold thsPHvst (Patent)PSeed (Patent)PHvst (Europarl)PSeed (Europarl)Figure 10: # of acquired paraphrase pairs against threshold values.
(left: probability-based (0.01 ?
thp ?
0.9, ths = ?
), right: similarity-based (?
?
ths ?
0.9, thp = 0.01))Finally, we investigated how the number of para-phrase pairs varies depending on the values for thetwo thresholds, i.e., thp on the conditional probabil-ity and ths on the contextual similarity, respectively.Figure 10 shows the results when the full sizes ofbilingual corpora are used.
When the pairs were fil-tered only with thp , the number of paraphrase pairsin PHvst decreased more slowly than that of PSeedaccording to the increase of the threshold value.
Thisis a benefit from our generalization and instantiationmethod.
The same paraphrase pattern is often in-duced from more than one paraphrase pair in PSeed .Thus, as long as at least one of them has a proba-bility higher than the given threshold value, corre-sponding novel paraphrases can be harvested.On the other hand, as a results of assessing eachindividual paraphrase pair by the contextual similar-ity, many pairs in PHvst , which are supposed to beincorrect instances of their corresponding pattern,are filtered out by a larger threshold value for ths .In contrast, many pairs in PSeed have a relativelyhigh similarity, e.g., 40% of all pairs have similarityhigher than 0.4.
This indicates the quality of PSeedis highly guaranteed by the shared translations.5 Human Evaluation of QualityTo confirm that the quality of PHvst is sufficientlyhigh, we carried out a substitution test.First, by substituting sub-sentential paraphrasesto existing sentences in a given test corpus, pairsof slightly different sentences were automaticallygenerated.
For instance, by applying ?looks like??
?resembles?
to (5), (6) was generated.
(5) The roof looks like a prehistoric lizard?s spine.
(6) The roof resembles a prehistoric lizard?s spine.Human evaluators were then asked to score eachpair of an original sentence and a paraphrased sen-tence with the following two 5-point scale gradesproposed by Callison-Burch (2008):Grammaticality: whether the paraphrased sen-tence is grammatical (1: horrible, 5: perfect)Meaning: whether the meaning of the original sen-tence is properly retained by the paraphrasedsentence (1: totally different, 5: equivalent)To make results more consistent and reduce thehuman labor, evaluators were asked to rate at thesame time several paraphrases for the same sourcephrase.
For instance, given a source sentence (5), the638evaluators might be given the following sentences inaddition to a paraphrased sentence (6).
(7) The roof seems like a prehistoric lizard?s spine.
(8) The roofwould look like a prehistoric lizard?s spine.In this experiment, we showed five paraphrasesper source phrase, assuming that evaluators wouldget confused if too large a number of paraphrasecandidates were presented at the same time.5.1 Data for EvaluationAs in previous work (Callison-Burch, 2008; Chanet al2011), we evaluated paraphrases acquiredfrom the Europarl corpus on news sentences.
Para-phrase examples were automatically generated fromthe English part ofWMT 2008-2011 ?newstest?
data(10,050 unique sentences) by applying the union ofPSeed and PHvst of the Europarl setting (19.3M para-phrases for 5.95M phrases).On the other hand, paraphrases acquired frompatent documents are much more difficult to eval-uate due to the following reasons.
First, they maybe too domain-specific to be of any use in generalareas such as news sentences.
However, conduct-ing an in-domain evaluation would be difficult with-out enrolling domain experts.
We expect that para-phrases from a domain can be used safely in thatdomain.
Nevertheless, deciding under what circum-stances they can be used safely in another domain isan interesting research question.To reduce the human labor for the evaluation, sen-tences were restricted to those with moderate length:10-30 words, which are expected to provide suf-ficient but succinct context.
To propose multipleparaphrase candidates at the same time, we also re-stricted phrases to be paraphrased (LHS phrases) tothose having at least five paraphrases including onesfrom PHvst .
This resulted in 60,421 paraphrases for988 phrase tokens (353 unique phrases).Finally, we randomly sampled 80 unique phrasetokens and five unique paraphrases for each phrasetoken (400 examples in total), and asked six peoplehaving a high level of English proficiency to evalu-ate them.
Inter-evaluator agreement was calculatedfrom five different pairs of evaluators, each judgingthe same 10 examples.
The remaining 350 exam-ples were divided into six chunks of slightly unequallength, with each chunk being judged by one of thesix evaluators.5-point Binaryn G M G M BothPSeed 55 4.60 4.35 0.85 0.93 0.78PHvst 295 4.22 3.35 0.74 0.67 0.55Total 350 4.28 3.50 0.76 0.71 0.58Table 1: Avg.
score and precision of binary classification.5.2 ResultsTable 1 shows the average of the original 5-pointscale scores and the percentage of examples thatare judged correct based on a binary judgment(Callison-Burch, 2008): an example is considered tobe correct iff the grammaticality score is 4 or aboveand/or the meaning score is 3 or above.
Paraphrasesbased on PSeed achieved a quite high performancein both grammaticality (?G?)
and meaning (?M?)
inpart because of the effectiveness of our filtering tech-niques.
The performance of paraphrases drawn fromPHvst was reasonably high and similar to the scores0.68 for grammaticality, 0.61 for meaning, and 0.55for both, of the best model reported in (Callison-Burch, 2008), although it was inferior to PSeed .Despite the fact that all of our evaluators had ahigh-level command of English, the agreement wasnot very high.
This was true even when the col-lected scores were mapped into binary classes.
Inthis case, the ?
values (Cohen, 1960) for each crite-rion were 0.45 and 0.45, respectively, which indicatethe agreement was ?fair?.
To obtain a better ?
value,the criteria for grading will need to be improved.However, we think that was not too low either8.The most promising way for improving the qual-ity of PHvst is to ensure that paraphrase patternscover only legitimate paraphrases.
We investigatedthis by filtering the manually scored paraphrase ex-amples with two thresholds for cleaning seed para-phrases PSeed : thp on the conditional probability es-timated using the bilingual parallel corpus and thson the contextual similarity in the monolingual non-parallel corpus.
Figure 11 shows the average scoreof the examples whose corresponding paraphrase isobtainable with the given threshold values.
Note thatthe points in the figure with higher threshold valuesare less reliable than the others, because filtering re-duces the number of the manually scored examples8Note that Callison-Burch (2008) might possibly underesti-mate the chance agreement and overestimate the ?
values, be-cause the distribution of human scores would not be uniform.63933.544.550  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Avg.
scoreProbability threshold thpGrammaticality (PSeed)Grammaticality (PHvst)Meaning (PSeed)Meaning (PHvst)33.544.550  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Avg.
scoreSimilarity threshold thsGrammaticality (PSeed)Grammaticality (PHvst)Meaning (PSeed)Meaning (PHvst)Figure 11: Average score of paraphrase examples against threshold values.
(left: probability-based (0.01 ?
thp ?
0.9, ths = ?
), right: similarity-based (?
?
ths ?
0.9, thp = 0.01))The points with higher threshold values are less reliable than the others,because filtering reduces the number of the manually scored examples used to calculate scores.used to calculate scores.
Nevertheless, it indicatesthat better filtering of PSeed with higher thresholdvalues is likely to produce a better-quality set ofparaphrases PHvst .
For instance, an inappropriateparaphrase pattern (9a) was excluded with thp = 0.1or ths = 0.1, while correct ones (9b) and (9c) re-mained even when a large threshold value is used.
(9) a.
?X years?
?
?turn X?b.
?X supplied?
?
?X provided?c.
?main X?
?
?most significant X?Kendall?s correlation coefficient ?B (Kendall,1938) between the contextual similarity and each ofthe human scores were 0.24 for grammaticality and0.21 for meaning, respectively.
Although they are ri-valing the best results reported in (Chan et al2011),i.e., 0.24 and 0.21, similarity metrics should be fur-ther investigated to realize a more accurate filtering.6 ConclusionIn this paper, we exploited general patterns under-lying paraphrases to acquire automatically a largenumber of high-quality paraphrase pairs using bothbilingual parallel and monolingual non-parallel cor-pora.
Experiments using two sets of corpora demon-strated that our method is able to leverage informa-tion in a relatively small bilingual parallel corpusto exploit large amounts of information in a rela-tively large monolingual non-parallel corpus.
Hu-man evaluation through a paraphrase substitutiontest revealed that the acquired paraphrases are gen-erally of reasonable quality.
Our original objectivewas to extract from monolingual corpora a largequantity of paraphrases whose quality is as high asthat of paraphrases from bilingual parallel corpora.We have met the quantity part of the objective, andhave come close to meeting the quality part.There are three main directions for our futurework.
First, we intend to carry out in-depth anal-yses of the proposed method.
For instance, whilewe showed that the performance of phrase substi-tution could be improved by removing noisy seedparaphrases, this also strongly affected the quan-tity.
We will therefore investigate similarity metricsin our future work.
Other interesting questions re-lated to the work presented here are, as mentioned inSection 4.2, exploitation of patterns with more thanone variable, learning curve experiments with dif-ferent amounts of monolingual data, and compari-son of in-domain and general-purpose monolingualcorpora.
Second, we have an interest in exploitingsophisticated paraphrase patterns; for instance, byinducing patterns hierarchically (recursively) and in-corporating lexical resources such as those exempli-fied in (4).
Finally, the developed paraphrase col-lection will be attested through applications, suchas sentence compression (Cohn and Lapata, 2008;Ganitkevitch et al2011) and machine translation(Callison-Burch et al2006; Marton et al2009).AcknowledgmentsWe are deeply grateful to our colleagues at NationalResearch Council Canada, especially George Foster,Eric Joanis, and Samuel Larkin, for their technicalsupport.
The first author is currently a JSPS (theJapan Society for the Promotion of Science) Post-doctoral Fellow for Research Abroad.640ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics (ACL), pages 597?604.Regina Barzilay and Kathleen R. McKeown.
2001.
Ex-tracting paraphrases from a parallel corpus.
In Pro-ceedings of the 39th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 50?57.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of the 2003 Hu-man Language Technology Conference and the NorthAmerican Chapter of the Association for Computa-tional Linguistics (HLT-NAACL), pages 16?23.Rahul Bhagat and Deepak Ravichandran.
2008.
Largescale acquisition of paraphrases for learning surfacepatterns.
In Proceedings of the 46th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 161?170.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine translationusing paraphrases.
In Proceedings of the Human Lan-guage Technology Conference of the North AmericanChapter of the Association for Computational Linguis-tics (HLT-NAACL), pages 17?24.Chris Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), pages196?205.Tsz Ping Chan, Chris Callison-Burch, and Benjamin Van-Durme.
2011.
Reranking bilingually extracted para-phrases using monolingual distributional similarity.
InProceedings of the Workshop on Geometrical Modelsof Natual Language Semantics (GEMS), pages 33?42.David L. Chen and William B. Dolan.
2011.
Collectinghighly parallel data for paraphrase evaluation.
In Pro-ceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 190?200.Jacob Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Measure-ment, 20(1):37?46.Trevor Cohn and Mirella Lapata.
2008.
Sentence com-pression beyond word deletion.
In Proceedings of the22nd International Conference on Computational Lin-guistics (COLING), pages 137?144.Michael Denkowski and Alon Lavie.
2011.
Meteor 1.3:Automatic metric for reliable optimization and evalua-tion of machine translation systems.
In Proceedings ofthe 6th Workshop on Statistical Machine Translation(WMT), pages 85?91.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.In Proceedings of the 20th International Conferenceon Computational Linguistics (COLING), pages 350?356.Jinhua Du, Jie Jiang, and Andy Way.
2010.
Facilitatingtranslation using source language paraphrase lattices.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 420?429.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Take-hito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, andSayori Shimohata.
2010.
Overview of the patenttranslation task at the NTCIR-8 workshop.
In Pro-ceedings of NTCIR-8 Workshop Meeting, pages 371?376.Atsushi Fujita, Shuhei Kato, Naoki Kato, and SatoshiSato.
2007.
A compositional approach toward dy-namic phrasal thesaurus.
In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Para-phrasing (WTEP), pages 151?158.Juri Ganitkevitch, Chris Callison-Burch, CourtneyNapoles, and Benjamin Van Durme.
2011.
Learn-ing sentential paraphrases from bilingual parallel cor-pora for text-to-text generation.
In Proceedings ofthe 2011 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 1168?1179.Zellig Harris.
1968.
Mathematical Structures of Lan-guage.
John Wiley & Sons.Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,Jun?ichi Kazama, and Sadao Kurohashi.
2011.
Ex-tracting paraphrases from definition sentences on theWeb.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 1087?1097.Christian Jacquemin.
1999.
Syntagmatic and paradig-matic representations of term variation.
In Proceed-ings of the 37th Annual Meeting of the Association forComputational Linguistics (ACL), pages 341?348.Howard Johnson, Joel Martin, George Foster, and RolandKuhn.
2007.
Improving translation quality by dis-carding most of the phrasetable.
In Proceedings ofthe 2007 Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 967?975.Maurice Kendall.
1938.
A new measure of rank correla-tion.
Biometrika, 30(1-2):81?93.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the 2003 Human Language Technology Con-ference and the North American Chapter of the Asso-641ciation for Computational Linguistics (HLT-NAACL),pages 48?54.Philipp Koehn.
2009.
Statistical Machine Translation.Cambridge University Press.Stanley Kok and Chris Brockett.
2010.
Hitting the rightparaphrases in good time.
In Proceedings of HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics (NAACL-HLT), pages 145?153.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for question answering.
Natural LanguageEngineering, 7(4):343?360.Nitin Madnani and Bonnie J. Dorr.
2010.
Gener-ating phrasal and sentential paraphrases: A surveyof data-driven methods.
Computational Linguistics,36(3):341?387.Yuval Marton, Chris Callison-Burch, and Philip Resnik.2009.
Improved statistical machine translation usingmonolingually-derived paraphrases.
In Proceedings ofthe 2009 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 381?390.Yuval Marton, Ahmed El Kholy, and Nizar Habash.2011.
Filtering antonymous, trend-contrasting, andpolarity-dissimilar distributional paraphrases for im-proving statistical machine translation.
In Proceedingsof the 6th Workshop on Statistical Machine Translation(WMT), pages 237?249.Aure?lien Max.
2010.
Example-based paraphrasing forimproved phrase-based statistical machine translation.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 656?666.Marius Pas?ca and Pe?ter Dienes.
2005.
Aligning needlesin a haystack: Paraphrase acquisition across the Web.In Proceedings of the 2nd International Joint Con-ference on Natural Language Processing (IJCNLP),pages 119?130.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations: Ex-tracting paraphrases and generating new sentences.
InProceedings of the 2003 Human Language Technol-ogy Conference and the North American Chapter ofthe Association for Computational Linguistics (HLT-NAACL), pages 102?109.Fatiha Sadat, Howard Johnson, Akakpo Agbago, GeorgeFoster, Roland Kuhn, Joel Martin, and Aaron Tikuisis.2005.
PORTAGE: A phrase-based machine transla-tion system.
In Proceedings of the ACL Workshop onBuilding and Using Parallel Texts, pages 129?132.Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, andRalph Grishman.
2002.
Automatic paraphrase acqui-sition from news articles.
In Proceedings of the 2002Human Language Technology Conference (HLT).Idan Szpektor and Ido Dagan.
2008.
Learning entail-ment rules for unary templates.
In Proceedings of the22nd International Conference on Computational Lin-guistics (COLING).
849-856.Sander Wubben, Antal van den Bosch, Emiel Krahmer,and Erwin Marsi.
2009.
Clustering and matchingheadlines for automatic paraphrase acquisition.
InProceedings of the 12th European Workshop on Nat-ural Language Generation, pages 122?125.Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.2009.
Extracting paraphrase patterns from bilin-gual parallel corpora.
Natural Language Engineering,15(4):503?526.642
