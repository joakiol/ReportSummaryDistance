Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 64?71,Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational LinguisticsLearning Subregular Classes of Languages with Factored DeterministicAutomataJeffrey HeinzDept.
of Linguistics and Cognitive ScienceUniversity of Delawareheinz@udel.eduJames RogersDept.
of Computer ScienceEarlham Collegejrogers@cs.earlham.eduAbstractThis paper shows how factored finite-state representations of subregular lan-guage classes are identifiable in the limitfrom positive data by learners which arepolytime iterative and optimal.
These rep-resentations are motivated in two ways.First, the size of this representation fora given regular language can be expo-nentially smaller than the size of theminimal deterministic acceptor recogniz-ing the language.
Second, these rep-resentations (including the exponentiallysmaller ones) describe actual formal lan-guages which successfully model naturallanguage phenomenon, notably in the sub-field of phonology.1 IntroductionIn this paper we show how to define certain sub-regular classes of languages which are identifiablein the limit from positive data (ILPD) by efficient,well-behaved learners with a lattice-structured hy-pothesis space (Heinz et al 2012).
It is shownthat every finite set of DFAs defines such an ILPDclass.
In this case, each DFA can be viewed asone factor in the description of every language inthe class.
This factoring of language classes intomultiple DFA can provide a compact, canonicalrepresentation of the grammars for every languagein the class.
Additionally, many subregular classesof languages can be learned by the above methodsincluding the Locally k-Testable, Strictly k-Local,Piecewise k-Testable, and Strictly k-Piecewiselanguages (McNaughton and Papert, 1971; Rogersand Pullum, 2011; Rogers et al 2010).
From alinguistic (and cognitive) perspective, these sub-regular classes are interesting because they appearto be sufficient for modeling phonotactic patternsin human language (Heinz, 2010; Heinz et al2011; Rogers et al to appear).2 PreliminariesFor any function f and element a in the domain off , we write f(a)?
if f(a) is defined, f(a)?= x ifit is defined for a and its value is x, and f(a) ?otherwise.
The range of f , the set of values ftakes at elements for which it is defined, is denotedrange(f).??
and ?k denote all sequences of any finitelength, and of length k, over a finite alphabet ?.The empty string is denoted ?.
A language L is asubset of ?
?.For all x, y belonging to a partially-ordered set(S,?
), if x ?
z and y ?
z then z is an upperbound of x and y.
For all x, y ?
S, the least upperbound (lub) x?
y = z iff x ?
z, y ?
z, and for allz?
which upper bound x and y, it is the case thatz ?
z?.
An upper semi-lattice is a partially orderedset (S,?)
such that every subset of S has a lub.
IfS is finite, this is equivalent to the existence ofx ?
y for all x, y ?
S.A deterministic finite-state automaton (DFA) isa tuple (Q,?, Q0, F, ?).
The states of the DFA areQ; the input alphabet is ?
; the set of initial statesis Q0; the final states are F ; and ?
: Q?
?
?
Qis the transition function.We admit a set of initial states solely to accom-modate the empty DFA, which has none.
Deter-ministic automata never have more than one ini-tial state.
We will assume that, if the automaton isnon-empty, then Q0 = {q0};The transition function?s domain is extended toQ?
??
in the usual way.The language of a DFA A isL(A) def= {w ?
??
| ?
(q0, w)??
F}.A DFA is trim iff it has no useless states:(?q ?
Q)[ ?w, v ?
??
|?
(q0, w)?= q and ?
(q, v)??
F ].64Every DFA can be trimmed by eliminating uselessstates from Q and restricting the remaining com-ponents accordingly.The empty DFA isA?
= (?,?,?,?,?).
Thisis the minimal trim DFA such that L(A?)
= ?.The DFA product of A1 = (Q1,?, Q01, F1, ?1)and A2 = (Q2,?, Q02, F2, ?2) is?
(A1,A2) = (Q,?, Q0, F, ?
)where Q = Q1 ?
Q2, Q0 = Q01 ?
Q02,F = F1 ?
F2 and(?q ?
Q)(??
?
?)[?
((q1, q2), ?)
def= (?1(q1, ?
), ?2(q2, ?
))]The DFA product of two DFA is also a DFA.
Itis not necessarily trim, but we will generally as-sume that in taking the product the result has beentrimmed, as well.The product operation is associative and com-mutative (up to isomorphism), and so it can be ap-plied to a finite set S of DFA, in which case wewrite?S = ?A?S A (letting?
{A} = A).
Inthis paper, grammars are finite sequences of DFAs~A = ?A1 ?
?
?
An?
and we also use the?nota-tion for the product of a finite sequence of DFAs:?
~A def= ?A?
~AA and L( ~A)def= L(?
~A).
Se-quences are used instead of sets in order to matchfactors in two grammars.
Let DFA denote thecollection of finite sequences of DFAs.Theorem 1 is well-known.Theorem 1 Consider a finite set S of DFA.
ThenL(?A?S A)= ?A?S L(A).An important consequence of Theorem 1 is thatsome languages are exponentially more com-pactly represented by their factors.
The grammar~A = ?A1 ?
?
?
An?
has?1?i?n card(Qi) states,whereas the trimmed?
~A can have as many as?1?i?n card(Qi) ?
?(max1?i?n(card(Qi))n)states.
An example of such a language is givenin Section 4, Figures 1 and 2.2.1 Identification in the limitA positive text T for a language L is a totalfunction T : N ?
L ?
{#} (# is a ?pause?
)such that range(T ) = L (i.e., for every w ?
Lthere is at least one n ?
N for which w =T (n)).
Let T [i] denote the initial finite sequenceT (0), T (1) .
.
.
T (i ?
1).
Let SEQ denote the setof all finite initial portions of all positive texts forall possible languages.
The content of an elementT [i] of SEQ iscontent(T [i]) def={w ?
??
| (?j ?
i?
1)[T (j) = w]}.In this paper, learning algorithms are programs:?
: SEQ ?
DFA.
A learner ?
identifies in thelimit from positive texts a collection of languagesL if and only if for all L ?
L, for all positive textsT for L, there exists an n ?
N such that(?m ?
n)[?
(T [m]) = ?
(T [n])] and L(T [n]) = L(see Gold (1967) and Jain et al(1999)).
A class oflanguages is ILPD iff it is identifiable in the limitby such a learner.3 Classes of factorable-DFA languagesIn this section, classes of factorable-DFA lan-guages are introduced.
The notion of sub-DFA iscentral to this concept.
Pictorially, a sub-DFA isobtained from a DFA by removing zero or morestates, transitions, and/or revoking the final statusof zero or more final states.Definition 1 For any DFA A = (Q,?, Q0, F, ?
),a DFA A?
= (Q?,?
?, Q?0, F ?, ??)
is sub-DFA of A,written A?
?
A, if and only if Q?
?
Q, ?
?
?
?,Q?0 ?
Q0, F ?
?
F , ??
?
?.The sub-DFA relation is extended to grammars(sequences of DFA).
Let ~A = ?A1 ?
?
?
An?
and~A?
= ?A?1 ?
?
?
A?n?.Then ~A?
?
~A ?
(?0 ?
i ?
n)[A?i ?
Ai].Clearly, if A?
?
A then L(A?)
?
L(A).Every grammar ~A determines a class of lan-guages: those recognized by a sub-grammar of ~A.Our interest is not in L( ~A), itself.
Indeed, this willgenerally be ??.
Rather, our interest is in identi-fying languages relative to the class of languagesrecognizable by sub-grammars of ~A.Definition 2 Let G( ~A) def= {~B | ~B ?
~A}, the classof grammars that are sub-grammars of ~A.Let L( ~A) def= {L( ~B) | ~B ?
~A}, the class of lan-guages recognized by sub-grammars of ~A.A class of languages is a factorable-DFA classiff it is L( ~A) for some ~A.The set G( ~A) is necessarily finite, since ~A is, soevery class L( ~A) is trivially ILPD by a learningalgorithm that systematically rules out grammarsthat are incompatible with the text, but this na?
?vealgorithm is prohibitively inefficient.
Our goal is65to establish that the efficient general learning algo-rithm given by Heinz et al(2012) can be appliedto every class of factorable-DFA languages, andthat this class includes many of the well-knownsub-regular language classes as well as classes thatare, in a particular sense, mixtures of these.4 A motivating exampleThis section describes the Strictly 2-Piecewise lan-guages, which motivate the factorization that isat the heart of this analysis.
Strictly Piecewise(SP) languages are characterized in Rogers et al(2010) and are a special subclass of the PiecewiseTestable languages (Simon, 1975).Every SP language is the intersection of a finiteset of complements of principal shuffle ideals:L ?
SP def??
L =?w?S[SI(w)], S finitewhereSI(w) def= {v ?
??
| w = ?1 ?
?
?
?k and(?v0, .
.
.
, vk ?
??
)[v = v0 ?
?1 ?
v1 ?
?
?
?k ?
vk]}So v ?
SI(w) iff w occurs as a subsequence of vand L ?
SP iff there is a finite set of strings forwhich L includes all and only those strings thatdo not include those strings as subsequences.
Wesay that L is generated by S. It turns out that SP isexactly the class of languages that are closed undersubsequence.A language is SPk iff it is generated by a set ofstrings each of which is of length less than or equalto k. Clearly, every SP language is SPk for somek and SP = ?1?k?N[SPk].If w ?
??
and |w| = k, then SI(w) = L(Aw)for a DFA Aw with no more than k states.
Forexample, if k = 2 and ?
= {a, b, c} and, hence,w ?
{a, b, c}2, then the minimal trim DFA recog-nizing SI(w) will be a sub-DFA (in which one ofthe transitions from the ?1 state has been removed)of one of the three DFA of Figure 1.Figure 1 shows ~A = ?Aa, Ab, Ac?, where ?
={a, b, c} and each A?
is a DFA accepting ?
?whose states distinguish whether ?
has yet oc-curred.
Figure 2 shows?
~A.Note that every SP2 language over {a, b, c} isL( ~B) for some ~B ?
~A.
The class of grammarsof G( ~A) recognize a slight extension of SP2 over{a, b, c} (which includes 1-Reverse Definite lan-guages as well).Observe that 6 states are required to describe ~Abut 8 states are required to describe?
~A.
Let ~A?be the sequence of DFA with one DFA for eachletter in ?, as in Figure 1.
As card(?)
increasesthe number of states of ~A?
is 2 ?
card(?)
butthe number of states in?
~A?
is 2card(?).
Thenumber of states in the product, in this case, is ex-ponential in the number of its factors.The Strictly 2-Piecewise languages are cur-rently the strongest computational characteriza-tion1 of long-distance phonotactic patterns in hu-man languages (Heinz, 2010).
The size of thephonemic inventories2 in the world?s languagesranges from 11 to 140 (Maddieson, 1984).
Englishhas about 40, depending on the dialect.
With an al-phabet of that size ~A?
would have 80 states, while?
~A?
would have 240 ?
1 ?
1012 states.
Thefact that there are about 1011 neurons in humanbrains (Williams and Herrup, 1988) helps moti-vate interest in the more compact, parallel repre-sentation given by ~A?
as opposed to the singularrepresentation of the DFA?
~A?.5 Learning factorable classes oflanguagesIn this section, classes of factorable-DFA lan-guages are shown to be analyzable as finite latticespaces.
By Theorem 6 of Heinz et al(2012), ev-ery such class of languages can be identified in thelimit from positive texts.Definition 3 (Joins) LetA = (Q,?, Q0, F, ?
),A1 = (Q1,?, Q01, F1, ?1) ?
AandA2 = (Q2,?, Q02, F2, ?2) ?
A.The join of A1 and A2 isA1?A2def= (Q1?Q2,?, Q01?Q02, F1?F2, ?1?
?2).Similarly, for all ~A = ?A1 ?
?
?
An?
and ~B =?B1 ?
?
?
Bn?
?
~A, ~C2 = ?C1 ?
?
?
Cn?
?
~A, the joinof and ~B and ~C is ~B ?
~C def= ?B1 ?
C1 ?
?
?
Bn ?
Cn?.Note that the join of two sub-DFA of A is also asub-DFA of A.
Since G( ~A) is finite, binary joinsuffices to define join of any set of sub-DFA of agiven DFA (as iterated binary joins).
Let?
[S] bethe join of S, a set of sub-DFAs of some A (or ~A).1See Heinz et al(2011) for competing characterizations.2The mental representations of speech sounds are calledphonemes, and the phonemic inventory is the set of these rep-resentations (Hayes, 2009).66a0 a1 b0 b1 c0 c1b, caa, b, c a, cba, b, c a, bca, b, cFigure 1: The sequence of DFA ~A = ?Aa, Ab, Ac?, where ?
= {a, b, c} and each A?
accepts ??
andwhose states distinguish whether ?
has yet occurred.a0b0c0a1b0c0a0b1c0a0b0c1a1b1c0a0b1c1a1b0c1a1b1c1abcbcaacbabcca, bab, cba, ca, b, cFigure 2: The product?
?Aa, Ab, Ac?.67Lemma 1 The set of sub-DFA of a DFA A, or-dered by ?, ({B | B ?
A},?
), is an upper semi-lattice with the least upper bound of a set of S sub-DFA of A being their join.Similarly the set of sub-grammars of a grammar~A, ordered again by?, ({~B ?
~A},?
), is an uppersemi-lattice with the least upper bound of a set ofsub-grammars of ~A being their join.3This follows from the fact that Q1 ?Q2 (similarlyF1 ?F2 and ?1 ?
?2) is the lub of Q1 and Q2 (etc.
)in the lattice of sets ordered by subset.5.1 Paths and ChiselsDefinition 4 LetA = (Q,?, {q0}, F, ?)
be a non-empty DFA and w = ?0?1 ?
?
?
?n ?
?
?.If ?
(q0, w)?, the path of w in A is the sequence?
(A, w) def=?
(q0, ?0), .
.
.
, (qn, ?n), (qn+1, ?
)?where (?0 ?
i ?
n)[qi+1 = ?
(qi, ?i)].If ?
(q0, w)?
then ?
(A, w)?.If ?
(A, w)?, let Q?
(A,w) denote set of states ittraverses, ??
(A,w) denote the the transitions it tra-verses, and let F?
(A,w) = {qn+1}.Next, for any DFA A, and any w ?
L(A), wedefine the chisel of w given A to be the sub-DFAof A that exactly encompasses the path etched outin A by w.Definition 5 For any non-empty DFA A =(Q,?, {q0}, F, ?)
and all w ?
?
?, if w ?
L(A),then the chisel of w given A is the sub-DFACA(w) = (Q?
(A,w),?, {q0}, F?
(A,w), ??
(A,w)).If w 6?
L(A), then CA(w) = A?.Consider any ~A = ?A1 ?
?
?
An?
and any wordw ?
??.
The chisel of w given ~A is C ~A(w) =?CA1(w) ?
?
?CAn(w)?.Observe that CA(w) ?
A for all words w and allA, and that CA(w) is trim.Using the join, the domain of the chisel is ex-tended to sets of words: C ~A(S) =?w?S C ~A(w).Note that {C ~A(w) | w ?
??}
is finite, since{~B | ~B ?
~A} is.Theorem 2 For any grammar ~A, let C( ~A) ={C ~A(S) | S ?
??}.
Then (C( ~A),?)
is an up-per semi-lattice with the lub of two elements givenby the join ?.3These are actually complete finite lattices, but we are in-terested primarily in the joins.Proof This follows immediately from the finite-ness of {C ~A(w) | w ?
??}
and Lemma 1.
Lemma 2 For all A = (Q,?, Q0, F, ?
), there isa finite set S ?
??
such that?w?S CA(w) = A.Similarly, for all ~A = ?A1 ?
?
?
An?, there is a finiteset S ?
??
such that C ~A(S) = ~A.Proof If A is empty, then clearly S = ?
suffices.Henceforth consider only nonempty A.For the first statement, let S be the set of u?vwhere, for each q ?
Q and for each ?
?
?,?
(q0, u) ?= q and ?(?
(q, ?
), v) ??
F such thatu?v has minimal length.
By construction, S is fi-nite.
Furthermore, for every state and every transi-tion in A, there is a word in S whose path touchesthat state and transition.
By definition of ?
it fol-lows that CA(S) = A.For proof of the second statement, for each Aiin ~A, construct Si as stated and take their union.
Heinz et al(2012) define lattice spaces.
For anupper semi-lattice V and a function f : ??
?
Vsuch that f and ?
are (total) computable, (V, f) iscalled a Lattice Space (LS) iff, for each v ?
V ,there exists a finite D ?
range(f) with?D = v.Theorem 3 For all grammars ~A = ?A1 ?
?
?
An?,(C( ~A), C ~A) is a lattice space.Proof For all ~A?
?
C( ~A), by Lemma 2, there is afinite S ?
??
such that ?w?S C ~A(w) = ~A?.
For Heinz et al(2012), elements of the lat-tice are grammars.
Likewise, here, each grammar~A = ?A1 ?
?
?
An?
defines a lattice whose elementsare its sub-grammars.
Heinz et al(2012) associatethe languages of a grammar v in a lattice space(V, f) with {w ?
??
| f(w) ?
v}.
This definitioncoincides with ours: for any element ~A?
of C( ~A)(note ~A?
?
~A), a word w belongs to L( ~A?)
if andonly if C ~A(w) is a sub-DFA of ~A?.
The class oflanguages of a LS is the collection of languagesobtained by every element in the lattice.
For ev-ery LS (C( ~A), C ~A), we now define a learner ?
ac-cording to the construction in Heinz et al(2012):?T ?
SEQ, ?
(T ) = ?w?content(T ) C ~A(w).Let L(C( ~A),C ~A) denote the class of languagesassociated with the LS in Theorem 3.
Accord-ing to Heinz et al(2012, Theorem 6), the learner?
identifies L(C( ~A),CvA) in the limit from posi-tive data.
Furthermore, ?
is polytime iterative,68i.e can compute the next hypothesis in polytimefrom the previous hypothesis alone, and opti-mal in the sense that no other learner convergesmore quickly on languages in L(C( ~A),CG).
In ad-dition, this learner is globally-consistent (everyhypothesis covers the data seen so far), locally-conservative (the hypothesis never changes unlessthe current datum is not consistent with the cur-rent hypothesis), strongly-monotone (the currenthypothesis is a superset of all prior hypotheses),and prudent (it never hypothesizes a language thatis not in the target class).
Formal definitions ofthese terms are given in Heinz et al(2012) and canalso be found elsewhere, e.g.
Jain et al(1999).6 Complexity considerationsThe space of sub-grammars of a given sequence ofDFAs is necessarily finite and, thus, identifiable inthe limit from positive data by a na?
?ve learner thatsimply enumerates the space of grammars.
Thelattice learning algorithm has better efficiency be-cause it works bottom-up, extending the grammarminimally, at each step, with the chisel of the cur-rent string of the text.
The lattice learner neverexplores any part of the space of grammars thatis not a sub-grammar of the correct one and, as itnever moves down in the lattice, it will skip muchof the space of grammars that are sub-grammars ofthe correct one.
The space it explores will be mini-mal, given the text it is running on.
Generalizationis a result of the fact that in extending the gram-mar for a string the learner adds its entire Nerodeequivalence class to the language.The time complexity of either learning or recog-nition with the factored automata may actually besomewhat worse than the complexity of doing sowith its product.
Computing the chisel of a stringw in the product machine of Figure 2 is ?
(|w|),while in the factored machine of Figure 1 one mustcompute the chisel in each factor and its complex-ity is, thus, ?
(|w| card(?)k?1).
But ?
and k arefixed for a given factorization, so this works out tobe a constant factor.Where the factorization makes a substantial dif-ference is in the number of features that mustbe learned.
In the factored grammar of theexample, the total number of states plus edgesis ?(kcard(?
)k?1), while in its product it is?(2(card(?)k?1)).
This represents an exponentialimprovement in the space complexity of the fac-tored grammar.Every DFA can be factored in many ways, butthe factorizations do not necessarily provide anasymptotically significant improvement in spacecomplexity.
The canonical contrast is betweensequences of automata ?A1, .
.
.
,An?
that countmodulo some sequence of mi ?
N. If themi are pairwise prime, the product will require?1?i?n[mi] = ?
((maxi[mi])n) states.
If on theother hand, they are all multiples of each other itwill require just ?
(maxi[mi]).7 ExamplesThe fact that the class of SP2 languages is effi-ciently identifiable in the limit from positive datais neither surprising or new.
The obvious ap-proach to learning these languages simply accu-mulates the set of pairs of symbols that occur assubsequences of the strings in the text and builds amachine that accepts all and only those strings inwhich no other such pairs occur.
This, in fact, isessentially what the lattice learner is doing.What is significant is that the lattice learner pro-vides a general approach to learning any languageclass that can be captured by a factored grammarand, more importantly, any class of languages thatare intersections of languages that are in classesthat can be captured this way.Factored grammars in which each factor recog-nizes ?
?, as in the case of Figure 1, are of par-ticular interest.
Every sub-Star-Free class of lan-guages in which the parameters of the class (k, forexample) are fixed can be factored in this way.4 Ifthe parameters are not fixed and the class of lan-guages is not finite, none of these classes can beidentified in the limit from positive data at all.5 Sothis approach is potentially useful at least for allsub-Star-Free classes.
The learners for non-strictclasses are practical, however, only for small val-ues of the parameters.
So that leaves the StrictlyLocal SLk and Strictly Piecewise SPk languagesas the obvious targets.The SLk languages are those that are deter-mined by the substrings of length no greater thank that occur within the string (including endmark-4We conjecture that there is a parameterized class of lan-guages that is equivalent to the Star-Free languages, whichwould make that class learnable in this way as well.5For most of these classes, including the Definite,Reverse-Definite and Strictly Local classes and their superclasses, this is immediate from the fact that they are super-finite.
SP, on the other hand, is not super-finite (since itdoes not include all finite languages) but nevertheless, it isnot IPLD.69ers).
These can be factored on the basis of thosesubstrings, just as the SPk languages can, althoughthe construction is somewhat more complex.
(Seethe Knuth-Morris-Pratt algorithm (Knuth et al1977) for a way of doing this.)
But SLk is a case inwhich there is no complexity advantage in factor-ing the DFA.
This is because every SLk languageis recognized by a DFA that is a Myhill graph:with a state for each string of ?<k (i.e., of lengthless than k).
Such a graph has ?(card(?
)k?1)states, asymptotically the same as the number ofstates in the factored grammar, which is actuallymarginally worse.Therefore, factored SLk grammars are not, inthemselves, interesting.
But they are interesting asfactors of other grammars.
Let (SL+ SP)k,l (resp.
(LT + SP)k,l, (SL + PT)k,l) be the class of lan-guages that are intersections of SLk and SPl (resp.LTk and SPl, SLk and PTl) languages.
WhereLT (PT) languages are determined by the set ofsubstrings (subsequences) that occur in the string(see Rogers and Pullum (2011) and Rogers et al(2010)).These classes capture co-occurrence of lo-cal constraints (based on adjacency) and long-distance constraints (based on precedence).
Theseare of particular interest in phonotactics, as theyare linguistically well-motivated approaches tomodeling phonotactics and they are sufficientlypowerful to model most phonotactic patterns.
Theresults of Heinz (2007) and Heinz (2010) stronglysuggest that nearly all segmental patterns are(SL+ SP)k,l for small k and l. Moreover, roughly72% of the stress patterns that are included inHeinz?s database (Heinz, 2009; Phonology Lab,2012) of patterns that have been attested in nat-ural language can be modeled with SLk grammarswith k ?
6.
Of the rest, all but four are LT1 + SP4and all but two are LT2 + SP4.
Both of these lasttwo are properly regular (Wibel et al in prep).8 ConclusionWe have shown how subregular classes of lan-guages can be learned over factored representa-tions, which can be exponentially more compactthan representations with a single DFA.
Essen-tially, words in the data presentation are passedthrough each factor, ?activating?
the parts touched.This approach immediately allows one to natu-rally ?mix?
well-characterized learnable subreg-ular classes in such a way that the resulting lan-guage class is also learnable.
While this mixing ispartly motivated by the different kinds of phono-tactic patterns in natural language, it also suggestsa very interesting theoretical possibility.
Specifi-cally, we anticipate that the right parameterizationof these well-studied subregular classes will coverthe class of star-free languages.
Future work couldalso include extending the current analysis to fac-toring stochastic languages, perhaps in a way thatconnects with earlier research on factored HMMs(Ghahramani and Jordan, 1997).AcknowledgmentsThis paper has benefited from the insightful com-ments of three anonymous reviewers, for whichthe authors are grateful.
The authors also thankJie Fu and Herbert G. Tanner for useful discus-sion.
This research was supported by NSF grant1035577 to the first author, and the work was com-pleted while the second author was on sabbatical atthe Department of Linguistics and Cognitive Sci-ence at the University of Delaware.ReferencesZoubin Ghahramani and Michael I. Jordan.
1997.
Fac-torial hidden markov models.
Machine Learning,29(2):245?273.E.M.
Gold.
1967.
Language identification in the limit.Information and Control, 10:447?474.Bruce Hayes.
2009.
Introductory Phonology.
Wiley-Blackwell.Jeffrey Heinz, Chetan Rawal, and Herbert G. Tan-ner.
2011.
Tier-based strictly local constraints forphonology.
In Proceedings of the 49th AnnualMeet-ing of the Association for Computational Linguis-tics, pages 58?64, Portland, Oregon, USA, June.
As-sociation for Computational Linguistics.Jeffrey Heinz, Anna Kasprzik, and Timo Ko?tzing.2012.
Learning with lattice-structured hypothesisspaces.
Theoretical Computer Science, 457:111?127, October.Jeffrey Heinz.
2007.
The Inductive Learning ofPhonotactic Patterns.
Ph.D. thesis, University ofCalifornia, Los Angeles.Jeffrey Heinz.
2009.
On the role of locality in learningstress patterns.
Phonology, 26(2):303?351.Jeffrey Heinz.
2010.
Learning long-distance phono-tactics.
Linguistic Inquiry, 41(4):623?661.70Sanjay Jain, Daniel Osherson, James S. Royer, andArun Sharma.
1999.
Systems That Learn: An In-troduction to Learning Theory (Learning, Develop-ment and Conceptual Change).
The MIT Press, 2ndedition.Donald Knuth, James H Morris, and Vaughn Pratt.1977.
Fast pattern matching in strings.
SIAM Jour-nal on Computing, 6(2):323?350.Ian Maddieson.
1984.
Patterns of Sounds.
CambridgeUniversity Press, Cambridge, UK.Robert McNaughton and Seymour Papert.
1971.Counter-Free Automata.
MIT Press.UD Phonology Lab.
2012.
UD phonology labstress pattern database.
http://phonology.cogsci.udel.edu/dbs/stress.
AccessedDecember 2012.James Rogers and Geoffrey Pullum.
2011.
Aural pat-tern recognition experiments and the subregular hi-erarchy.
Journal of Logic, Language and Informa-tion, 20:329?342.James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlef-sen, Molly Visscher, David Wellcome, and SeanWibel.
2010.
On languages piecewise testable in thestrict sense.
In Christian Ebert, Gerhard Ja?ger, andJens Michaelis, editors, The Mathematics of Lan-guage, volume 6149 of Lecture Notes in Artifical In-telligence, pages 255?265.
Springer.James Rogers, Jeffrey Heinz, Margaret Fero, JeremyHurst, Dakotah Lambert, and Sean Wibel.
to appear.Cognitive and sub-regular complexity.
In Proceed-ings of the 17th Conference on Formal Grammar.Imre Simon.
1975.
Piecewise testable events.
InAutomata Theory and Formal Languages: 2ndGrammatical Inference conference, pages 214?222,Berlin.
Springer-Verlag.Sean Wibel, James Rogers, and Jeffery Heinz.
Factor-ing of stress patterns.
In preparation.R.W.
Williams and K. Herrup.
1988.
The control ofneuron number.
Annual Review of Neuroscience,11:423?453.71
