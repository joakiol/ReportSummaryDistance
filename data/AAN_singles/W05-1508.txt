Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 74?82,Vancouver, October 2005. c?2005 Association for Computational LinguisticsTreebank TransferMartin JanscheCenter for Computational Learning SystemsColumbia UniversityNew York, NY 10027, USAjansche@acm.orgAbstractWe introduce a method for transferringannotation from a syntactically annotatedcorpus in a source language to a target lan-guage.
Our approach assumes only thatan (unannotated) text corpus exists for thetarget language, and does not require thatthe parameters of the mapping betweenthe two languages are known.
We outlinea general probabilistic approach based onData Augmentation, discuss the algorith-mic challenges, and present a novel algo-rithm for sampling from a posterior distri-bution over trees.1 IntroductionAnnotated corpora are valuable resources for Natu-ral Language Processing (NLP) which often requiresignificant effort to create.
Syntactically annotatedcorpora ?
treebanks, for short ?
currently exist for asmall number of languages; but for the vast majorityof the world?s languages, treebanks are unavailableand unlikely to be created any time soon.The situation is especially difficult for dialectalvariants of many languages.
A prominent exam-ple is Arabic: syntactically annotated corpora ex-ist for the common written variety (Modern Stan-dard Arabic or MSA), but the spoken regional di-alects have a lower status in written communicationand lack annotated resources.
This lack of dialecttreebanks hampers the development of syntax-basedNLP tools, such as parsers, for Arabic dialects.On the bright side, there exist very large anno-tated (Maamouri et al, 2003, 2004a,b) corpora forModern Standard Arabic.
Furthermore, unannotatedtext corpora for the various Arabic dialects can alsobe assembled from various sources on the Internet.Finally, the syntactic differences between the Ara-bic dialects and Modern Standard Arabic are rela-tively minor (compared with the lexical, phonologi-cal, and morphological differences).
The overall re-search question is then how to combine and exploitthese resources and properties to facilitate, and per-haps even automate, the creation of syntactically an-notated corpora for the Arabic dialects.We describe a general approach to this problem,which we call treebank transfer: the goal is toproject an existing treebank, which exists in a sourcelanguage, to a target language which lacks annotatedresources.
The approach we describe is not tied inany way to Arabic, though for the sake of concrete-ness one may equate the source language with Mod-ern Standard Arabic and the target language with adialect such as Egyptian Colloquial Arabic.We link the two kinds of resources that are avail-able ?
a treebank for the source language and anunannotated text corpus for the target language ?in a generative probability model.
Specifically, weconstruct a joint distribution over source-languagetrees, target-language trees, as well as parameters,and draw inferences by iterative simulation.
This al-lows us to impute target-language trees, which canthen be used to train target-language parsers andother NLP components.Our approach does not require aligned data,unlike related proposals for transferring annota-tions from one language to another.
For exam-ple, Yarowksy and Ngai (2001) consider the transferof word-level annotation (part-of-speech labels andbracketed NPs).
Their approach is based on aligned74corpora and only transfers annotation, as opposed togenerating the raw data plus annotation as in our ap-proach.We describe the underlying probability model ofour approach in Section 2 and discuss issues per-taining to simulation and inference in Section 3.Sampling from the posterior distribution of target-language trees is one of the key problems in iterativesimulation for this model.
We present a novel sam-pling algorithm in Section 4.
Finally in Section 5 wesummarize our approach in its full generality.2 The Probability ModelOur approach assumes that two kinds of resourcesare available: a source-language treebank, and atarget-language text corpus.
This is a realisticassumption, which is applicable to many source-language/target-language pairs.
Furthermore, someknowledge of the mapping between source-languagesyntax and target-language syntax needs to be incor-porated into the model.
Parallel corpora are not re-quired, but may help when constructing this map-ping.We view the source-language treebank as a se-quence of trees S1, .
.
.
,Sn, and assume that thesetrees are generated by a common process from acorresponding sequence of latent target-languagetrees T1, .
.
.
,Tn.
The parameter vector of the pro-cess which maps target-language trees to source-language trees will be denoted by ?.
The mappingitself is expressed as a conditional probability distri-bution p(Si | Ti,?)
over source-language trees.
Theparameter vector ?
is assumed to be generated ac-cording to a prior distribution p(?
| ? )
with hyper-parameter ?
, assumed to be fixed and known.We further assume that each target-language treeTi is generated from a common language model ?for the target language, p(Ti | ?).
For expository rea-sons we assume that ?
is a bigram language modelover the terminal yield (also known as the fringe) ofTi.
Generalizations to higher-order n-gram modelsare completely straightforward; more general mod-els that can be expressed as stochastic finite au-tomata are also possible, as discussed in Section 5.Let t1, .
.
.
, tk be the terminal yield of tree T .
Thenp(T | ?)
= ?
(t1 | #)(k?j=2?
(t j | t j?1))?
($ | tk),where # marks the beginning of the string and $marks the end of the string.There are two options for incorporating the lan-guage model ?
into the overall probability model.In the first case ?
which we call the full model ??
is generated by an informative prior distributionp(?
| ? )
with hyper-parameter ?
.
In the second case?
the reduced model ?
the language model ?
is fixed.The structure of the full model is specified graph-ically in Figure 1.
In a directed acyclic graphicalmodel such as this one, we equate vertices with ran-dom variables.
Directed edges are said to go from aparent to a child node.
Each vertex depends directlyon all of its parents.
Any particular vertex is condi-tionally independent from all other vertices given itsparents, children, and the parents of its children.The portion of the full model we are interested inis the following factored distribution, as specified byFigure 1:p(S1, .
.
.
,Sn,T1, .
.
.
,Tn,?,?
| ?
,?
)= p(?
| ? )
p(?
| ?
)n?i=1p(Ti | ?)
p(Si | Ti,?)
(1)In the reduced model, we drop the leftmost term/vertex, corresponding to the prior for ?
with hyper-parameter ?
, and condition on ?
instead:p(S1, .
.
.
,Sn,T1, .
.
.
,Tn,?
| ?,?
)= p(?
| ?
)n?i=1p(Ti | ?)
p(Si | Ti,?)
(2)The difference between the full model (1) and thereduced model (2) is that the reduced model assumesthat the language model ?
is fixed and will not beinformed by the latent target-language trees Ti.
Thisis an entirely reasonable assumption in a situationwhere the target-language text corpus is much largerthan the source-language treebank.
This will typ-ically be the case, since it is usually very easy tocollect large corpora of unannotated text which ex-ceed the largest existing annotated corpora by sev-eral orders of magnitude.
When a sufficiently largetarget-language text corpus is available, ?
is simplya smoothed bigram model which is estimated oncefrom the target-language corpus.If the target-language corpus is relatively small,then the bigram model ?
can be refined on the ba-sis of the imputed target-language trees.
A bigram75?
?
?S1T1?T2 S2Tn Sn...
...Figure 1: The graphical structure of the full probability model.
Bold circles indicate observed variables,dotted circles indicate parameters.model is simply a discrete collection of multinomialdistributions.
A simple prior for ?
takes the formof a product of Dirichlet distributions, so that thehyper-parameter ?
is a vector of bigram counts.
Inthe full model (1), we assume ?
is fixed and set it tothe observed bigram counts (plus a constant) in thetarget-language text corpus.
This gives us an infor-mative prior for ?.
If the bigram counts are suffi-ciently large, ?
will be fully determined by this in-formative prior distribution, and the reduced model(2) can be used instead.By contrast, usually very little is known a pri-ori about the syntactic transfer model ?.
Instead ?needs to be estimated from data.
We assume that ?too is a discrete collection of multinomial distribu-tions, governed by Dirichlet priors.
However, unlikein the case of ?, the priors for ?
are noninforma-tive.
This is not a problem, since a lot of informa-tion about the target language is provided by the lan-guage model ?.As one can see in Figure 1 and equation (1),the overall probability model constrains the latenttarget-language trees Ti in two ways: From the left,the language model ?
serves as a prior distributionover target-language trees.
On the one hand, ?
isan informative prior, based on large bigram countsobtained from the target-language text corpus; onthe other hand, it only informs us about the fringeof the target-language trees and has very little di-rectly to say about their syntactic structure.
From theright, the observed source-language trees constrainthe latent target-language trees in a complementaryfashion.
Each target-language tree Ti gives rise to acorresponding source-language tree Si according tothe syntactic transfer mapping ?.
This mapping isinitially known only qualitatively, and comes with anoninformative prior distribution.Our goal is now to simultaneously estimate thetransfer parameter ?
and impute the latent trees Ti.This is simplified by the following observation: ifT1, .
.
.
,Tn are known, then finding ?
is easy; viceversa, if ?
is known, then finding Ti is easy.
Si-multaneous inference for ?
and T1, .
.
.
,Tn is possiblevia Data Augmentation (Tanner and Wong, 1987),or, more generally, Gibbs sampling (Geman and Ge-man, 1984).3 Simulation of the Joint PosteriorDistributionWe now discuss the simulation of the joint poste-rior distribution over the latent trees T1, .
.
.
,Tn, thetransfer model parameter ?, and the language modelparameter ?.
This joint posterior is derived from theoverall full probability model (1).
Using the reducedmodel (2) instead of the full model amounts to sim-ply omitting ?
from the joint posterior.
We will dealprimarily with the more general full model in thissection, since the simplification which results in thereduced model will be straightforward.The posterior distribution we focus on isp(T1, .
.
.
,Tn,?,?
| S1, .
.
.
,Sn,?
,?
), which providesus with information about all the variables of inter-est, including the latent target-language trees Ti, thesyntactic transfer model ?, and the target-language76language model ?.
It is possible to simulate thisjoint posterior distribution using simple sampling-based approaches (Gelfand and Smith, 1990), whichare instances of the general Markov-chain MonteCarlo method (see, for example, Liu, 2001).Posterior simulation proceeds iteratively, as fol-lows.
In each iteration we draw the three kinds ofrandom variables ?
latent trees, language model pa-rameters, and transfer model parameters ?
from theirconditional distributions while holding the values ofall other variables fixed.
Specifically:?
Initialize ?
and ?
by drawing each from itsprior distribution.?
Iterate the following three steps:1.
Draw each Ti from its posterior distribu-tion given Si, ?, and ?.2.
Draw ?
from its posterior distributiongiven T1, .
.
.
,Tn and ?
.3.
Draw ?
from its posterior distributiongiven S1, .
.
.
,Sn, T1, .
.
.
,Tn, and ?
.This simulation converges in the sense that the drawsof T1, .
.
.
,Tn, ?, and ?
converge in distribution tothe joint posterior distribution over those variables.Further details can be found, for example, in Liu,2001, as well as the references cited above.We assume that the bigram model ?
is a family ofmultinomial distributions, and we write ?
(t j | t j?1)for the probability of the word t j following t j?1.Using creative notation, ?
( ?
| t j?1) can be seen asa multinomial distribution.
Its conjugate prior isa Dirichlet distribution whose parameter vector ?ware the counts of words types occurring immediatelyafter the word type w of t j?1.
Under the conven-tional assumptions of exchangeability and indepen-dence, the prior distribution for ?
is just a product ofDirichlet priors.
Since we employ a conjugate prior,the posterior distribution of ?p(?
| S1, .
.
.
,Sn,T1, .
.
.
,Tn,?,?
,?
)= p(?
| T1, .
.
.
,Tn,? )
(3)has the same form as the prior ?
it is likewise a prod-uct of Dirichlet distributions.
In fact, for each wordtype w the posterior Dirichlet density has parameter?w+cw, where ?w is the parameter of the prior distri-bution and cw is a vector of counts for all word formsappearing immediately after w along the fringe ofthe imputed trees.We make similar assumptions about the syntactictransfer model ?
and its posterior distribution, whichisp(?
| S1, .
.
.
,Sn,T1, .
.
.
,Tn,?,?
,?
)= p(?
| S1, .
.
.
,Sn,T1, .
.
.
,Tn,?
).
(4)In particular, we assume that syntactic transfer in-volves only multinomial distributions, so that theprior and posterior for ?
are products of Dirichletdistributions.
This means that sampling ?
and ?from their posterior distributions is straightforward.The difficult part is the first step in each scan ofthe Gibbs sampler, which involves sampling eachtarget-language latent tree from the correspondingposterior distribution.
For a particular tree Tj, theposterior takes the following form:p(Tj | S1, .
.
.
,Sn,T1, .
.
.
,Tj?1,Tj+1, .
.
.
,Tn,?,?,?
,?
)= p(Tj | S j,?,?)
=p(Tj,S j | ?,?
)?Tj p(Tj,S j | ?,?)?
p(Tj | ?)
p(S j | Tj,?)
(5)The next section discusses sampling from this poste-rior distribution in the context of a concrete exampleand presents an algorithmic solution.4 Sampling from the Latent Tree PosteriorWe are faced with the problem of sampling Tj fromits posterior distribution, which is proportional to theproduct of its language model prior p(Tj | ?)
andtransfer model likelihood p(S j | Tj,?).
Rejectionsampling using the prior as the proposal distributionwill not work, for two reasons: first, the prior is onlydefined on the yield of a tree and there are poten-tially very many tree structures with the same fringe;second, even if the first problem could be overcome,it is unlikely that a random draw from an n-gramprior would result in a target-language tree that cor-responds to a particular source-language tree, as theprior has no knowledge of the source-language tree.Fortunately, efficient direct sampling from the la-tent tree posterior is possible, under one very rea-sonable assumption: the set of all target-languagetrees which map to a given source-language tree S j77CS1v2O3a11n12a21n22Figure 2: Syntax tree illustrating SVO constituentorder within a sentence, and prenominal adjectiveswithin noun phrases.should be finite and representable as a packed for-est.
More specifically, we assume that there is acompact (polynomial space) representation of po-tentially exponentially many trees.
Moreover, eachtree in the packed forest has an associated weight,corresponding to its likelihood under the syntactictransfer model.If we rescale the weights of the packed forest sothat it becomes a normalized probabilistic context-free grammar (PCFG), we can sample from this newdistribution (corresponding to the normalized likeli-hood) efficiently.
For example, it is then possible touse the PCFG as a proposal distribution for rejectionsampling.However, we can go even further and samplefrom the latent tree posterior directly.
The keyidea is to intersect the packed forest with the n-gram language model and then to normalize the re-sulting augmented forest.
The intersection opera-tion is a special case of the intersection constructionfor context-free grammars and finite automata (Bar-Hillel et al, 1961, pp.
171?172).
We illustrate it herefor a bigram language model.Consider the tree in Figure 2 and assume it isa source-language tree, whose root is a clause (C)which consists of a subject (S), verb (v) and object(O).
The subject and object are noun phrases consist-ing of an adjective (a) and a noun (n).
For simplicity,we treat the part-of-speech labels (a, n, v) as termi-nal symbols and add numbers to distinguish multipleoccurrences.
The syntactic transfer model is statedas a conditional probability distribution over source-language trees conditional on target language trees.Syntactic transfer amounts to independently chang-ing the order of the subject, verb, and object, andchanging the order of adjectives and nouns, for ex-ample as follows:p(SvO | SvO) = ?1p(SOv | SvO) = (1?
?1)?2p(vSO | SvO) = (1??1)(1?
?2)p(SvO | SOv) = ?3p(SOv | SOv) = (1?
?3)?4p(vSO | SOv) = (1??3)(1?
?4)p(SvO | vSO) = ?5p(SOv | vSO) = (1?
?5)?6p(vSO | vSO) = (1??5)(1?
?6)p(an | an) = ?7p(na | an) = 1?
?7p(an | na) = ?8p(na | na) = 1?
?8Under this transfer model, the likelihood of a target-language tree [A v [S a1 n1][O n2 a2]] corresponding tothe source-language tree shown in Figure 2 is ?5 ?
?7 ??8.
It is easy to construct a packed forest of alltarget-language trees with non-zero likelihood thatgive rise to the source-language tree in Figure 2.Such a forest is shown in Figure 3.
Forest nodes areshown as ellipses, choice points as rectangles con-nected by dashed lines.
A forest node is to be un-derstood as an (unordered) disjunction of the choicepoints directly underneath it, and a choice point asan (ordered, as indicated by numbers) conjunctionof the forest nodes directly underneath it.
In otherwords, a packed forest can be viewed as an acyclicand-or graph, where choice points represent and-nodes (whose children are ordered).
As a simpli-fying convention, for nodes that dominate a singlechoice node, that choice node is not shown.
The for-est in Figure 3 represents SvO, SOv, and vSO permu-tations at the sentence level and an, na permutationsbelow the two noun phrases.
The twelve overall per-mutations are represented compactly in terms of twochoices for the subject, two choices for the object,and three choices for the root clause.78CC_1C_2 C_3S1 VO2S_1 S_2a11n12 2 1v1O2O_1 O_2a21n22 2 11 OV2211SO21 2Figure 3: Plain forest of target-language trees that can correspond to the source-language tree in Figure 2.We intersect/compose the packed forest with thebigram language model ?
by augmenting each nodein the forest with a left context word and a right pe-ripheral word: a node N is transformed into a triple(a,N,b) that dominates those trees which N domi-nates in the original forest and which can occur aftera word a and end with a word b.
The algorithm isroughly1 as shown in Figure 5 for binary branchingforests; it requires memoization (not shown) to beefficient.
The generalization to forests with arbitrarybranching factors is straightforward, but the presen-tation of that algorithm less so.
At the root level, wecall forest_composition with a left context of #(indicating the start of the string) and add dummynodes of the form (a,$,$) (indicating the end of thestring).
Further details can be found in the prototypeimplementation.
Each node in the original forest isaugmented with two words; if there are n leaf nodesin the original forest, the total number of nodes inthe augmented forest will be at most n2 times largerthan in the original forest.
This means that the com-pact encoding property of the packed forest (expo-nentially many trees can be represented in polyno-mial space) is preserved by the composition algo-rithm.
An example of composing a packed forest1A detailed implementation is available from http://www.cs.columbia.edu/?jansche/transfer/.with a bigram language model appears in Figure 4,which shows the forest that results from composingthe forest in Figure 3 with a bigram language model.The result of the composition is an augmentedforest from which sampling is almost trivial.
Thefirst thing we have to do is to recursively propagateweights from the leaves upwards to the root of theforest and associate them with nodes.
In the non-recursive case of leaf nodes, their weights are pro-vided by the bigram score of the augmented forest:observe that leaves in the augmented forest have la-bels of the form (a,b,b), where a and b are terminalsymbols, and a represents the immediately preced-ing left context.
The score of such a leaf is sim-ply ?
(b | a).
There are two recursive cases: Forchoice nodes (and-nodes), their weight is the prod-uct of the weights of the node?s children times a lo-cal likelihood score.
For example, the node (v,O,n)in Figure 4 dominates a single choice node (notshown, per the earlier conventions), whose weightis ?
(a | v) ?
(n | a) ?7.
For other forest nodes (or-nodes), their weight is the sum of the weights of thenode?s children (choice nodes).Given this very natural weight-propagation algo-rithm (and-nodes correspond to multiplication, or-nodes to summation), it is clear that the weight of theroot node is the sum total of the weights of all treesin the forest, where the weight of a tree is the prod-79(#,root,$)(#,root,$)_1(#,root,$)_2(#,root,$)_3(#,C,n)1 (n,$,$)2(#,C,n)_1(#,C,n)_2(#,C,n)_3(#,S,n)1(n,VO,n)2(#,a1,a)1(a,n1,n)2(n,v,v)1(v,O,n)2 (v,a2,a)1(a,n2,n)2(#,S,a)1(a,VO,n)2(#,n1,n)1(n,a1,a)22(a,v,v)1(#,v,v)1(v,SO,n)2(v,SO,n)_1(v,SO,n)_2(v,S,n)1(n,O,n)22(v,a1,a)12 (n,a2,a)1(v,S,a)1(a,O,n)22(v,n1,n)12(a,a2,a)1(#,C,a)1(a,$,$)2(#,C,a)_1(#,C,a)_2(#,C,a)_31(a,VO,a)2 1 (v,O,a)2(v,n2,n)1(n,a2,a)21(n,VO,a)2121(v,SO,a)2(v,SO,a)_1(v,SO,a)_21(a,O,a)22 (a,n2,n)11(n,O,a)22(n,n2,n)1(#,C,v)1(v,$,$)2(#,C,v)_1(#,C,v)_21(n,OV,v)2(n,OV,v)_1(n,OV,v)_221211(a,OV,v)2(a,OV,v)_1(a,OV,v)_22121Figure 4: Augmented forest obtained by intersecting the forest in Figure 3 with a bigram language model.80forest_composition(N, a):if N is a terminal:return { (a,N,N) }else:nodes = {}for each (L,R) in N.choices:left_nodes <- forest_composition(L, a)for each (a,L,b) in left_nodes:right_nodes <- forest_composition(R, b)for each (b,R,c) in right_nodes:new_n = (a,N,c)nodes <- nodes + { new_n }new_n.choices <- new_n.choices + [((a,L,b), (b,R,c))]return nodesFigure 5: Algorithm for computing the intersection of a binary forest with a bigram language model.uct of the local likelihood scores times the languagemodel score of the tree?s terminal yield.
We canthen associate outgoing normalized weights with thechildren (choice points) of each or-node, where theprobability of going to a particular choice node froma given or-node is equal to the weight of the choicenode divided by the weight of the or-node.This means we have managed to calculate thenormalizing constant of the latent tree posterior (5)without enumerating the individual trees in the for-est.
Normalization ensures that we can sample fromthe augmented and normalized forest efficiently, byproceeding recursively in a top-down fashion, pick-ing a child of an or-node at random with probabilityproportional to the outgoing weight of that choice.It is easy to see (by a telescoping product argument)that by multiplying together the probabilities of eachsuch choice we obtain the posterior probability of alatent tree.
We thus have a method for sampling la-tent trees efficiently from their posterior distribution.The sampling procedure described here is verysimilar to the lattice-based generation procedurewith n-gram rescoring developed by Langkilde(2000), and is in fact based on the same intersectionconstruction (Langkilde seems to be unaware thatthe CFG-intersection construction from (Bar-Hillelet al, 1961) is involved).
However, Langkilde is in-terested in optimization (finding the best tree in theforest), which allows her to prune away less prob-able trees from the composed forest in a procedurethat combines composition, rescoring, and pruning.Alternatively, for a somewhat different but relatedformulation of the probability model, the samplingmethod developed by Mark et al (1992) can be used.However, its efficiency is not well understood.5 ConclusionsThe approach described in this paper was illustratedusing very simple examples.
The simplicity of theexposition should not obscure the full generality ofour approach: it is applicable in the following situa-tions:?
A prior over latent trees is defined in terms ofstochastic finite automata.We have described the special case of bigrammodels, and pointed out how our approachwill generalize to higher-order n-gram models.However, priors are not generally constrainedto be n-gram models; in fact, any stochasticfinite automaton can be employed as a prior,since the intersection of context-free grammarsand finite automata is well-defined.
However,the intersection construction that appears to benecessary for sampling from the posterior dis-tribution over latent trees may be rather cum-bersome when higher-order n-gram models ormore complex finite automata are used as pri-ors.81?
The inverse image of an observed tree under themapping from latent trees to observed trees canbe expressed in terms of a finite context-freelanguage, or equivalently, a packed forest.The purpose of Gibbs sampling is to simulate theposterior distribution of the unobserved variables inthe model.
As the sampling procedure converges,knowledge contained in the informative but struc-turally weak prior ?
is effectively passed to the syn-tactic transfer model ?.
Once the sampling proce-dure has converged to a stationary distribution, wecan run it for as many additional iterations as wewant and sample the imputed target-language trees.Those trees can then be collected in a treebank, thuscreating novel syntactically annotated data in the tar-get language, which can be used for further process-ing in syntax-based NLP tasks.AcknowledgementsI would like to thank Steven Abney, the participantsof the 2005 Johns Hopkins workshop on Arabic di-alect parsing, and the anonymous reviewers for help-ful discussions.
The usual disclaimers apply.ReferencesY.
Bar-Hillel, M. Perles, and E. Shamir.
1961.
Onformal properties of simple phrase structuregrammars.
Zeitschrift fu?r Phonetik, Sprach-wissenschaft und Kommunikationsforschung,14(2):143?172.Alan E. Gelfand and Adrian F. M. Smith.
1990.Sampling-based approaches to calculatingmarginal densities.
Journal of the AmericanStatistical Association, 85(410):398?409.Stuart Geman and Donald Geman.
1984.
Stochasticrelaxation, Gibbs distributions, and the Bayesianrestoration of images.
IEEE Transactions onPattern Matching and Machine Intelligence,6(6):721?741.Irene Langkilde.
2000.
Forest-based statisticalsentence generation.
In Proceedings of the FirstMeeting of the North American Chapter of theAssociation for Computational Linguistics, pages170?177.
ACL Anthology A00-2023.Jun S. Liu.
2001.
Monte Carlo Strategies in Scien-tific Computing.
Springer.Mohamed Maamouri, Ann Bies, Tim Buckwalter,and Hubert Jin.
2004a.
Arabic Treebank: Part 2v 2.0.
Electronic resource, available from LDC.Mohamed Maamouri, Ann Bies, Tim Buckwalter,and Hubert Jin.
2004b.
Arabic Treebank: Part 3v 1.0.
Electronic resource, available from LDC.Mohamed Maamouri, Ann Bies, Hubert Jin, andTim Buckwalter.
2003.
Arabic Treebank: Part 1v 2.0.
Electronic resource, available from LDC.Kevin Mark, Michael Miller, Ulf Grenander, andSteve Abney.
1992.
Parameter estimation forconstrained context-free language models.
InSpeech and Natural Language: Proceedings of aWorkshop Held at Harriman, New York, Febru-ary 23?26, 1992, pages 146?149.
ACL Anthol-ogy H92-1028.Martin A. Tanner and Wing Hung Wong.
1987.
Thecalculation of posterior distributions by data aug-mentation.
Journal of the American StatisticalAssociation, 82(398):528?540.David Yarowksy and Grace Ngai.
2001.
Induc-ing multilingual POS taggers and NP bracketersvia robust projection across aligned corpora.
InProceedings of the Second Meeting of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 200?207.82
