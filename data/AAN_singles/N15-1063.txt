Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 609?618,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsModel Invertibility Regularization:Sequence Alignment With or Without Parallel DataTomer Levinboim?levinboim.1@nd.eduAshish Vaswani?avaswani@isi.eduDavid Chiang?dchiang@nd.edu?Dept.
of Computer Science and EngineeringUniversity of Notre Dame?Information Sciences InstituteUniversity of Southern CaliforniaAbstractWe present Model Invertibility Regularization(MIR), a method that jointly trains two direc-tional sequence alignment models, one in eachdirection, and takes into account the invertibil-ity of the alignment task.
By coupling the twomodels through their parameters (as opposedto through their inferences, as in Liang et al?sAlignment by Agreement (ABA), and Ganchevet al?s Posterior Regularization (PostCAT)),our method seamlessly extends to all IBM-style word alignment models as well as toalignment without parallel data.
Our proposedalgorithm is mathematically sound and inher-its convergence guarantees from EM.
We eval-uate MIR on two tasks: (1) On word align-ment, applying MIR on fertility based mod-els we attain higher F-scores than ABA andPostCAT.
(2) On Japanese-to-English back-transliteration without parallel data, applied tothe decipherment model of Ravi and Knight,MIR learns sparser models that close the gapin whole-name error rate by 33% relative toa model trained on parallel data, and further,beats a previous approach by Mylonakis et al1 IntroductionThe transfer of information between languages is acommon natural language phenomenon that is in-tuitively invertible.
For example, in transliteration,a source-language word is mapped to a target lan-guage?s writing system under a sound preservingmapping (for example, ?computer?
to Japanese Ro-maji, ?konpyutaa?).
The original word should thenbe recoverable from its transliterated version.
Simi-larly, in translation, the back-translation of the trans-lation of a word is likely to be that same word itself.In NLP, however, commonly-used generativemodels describing such phenomena are directional,only concerned with the transfer of source-languagesymbols to target-language symbols or vice versa,but not both directions.
Left unchecked, indepen-dently training two such directional models (source-to-target and target-to-source) often yields two mod-els that diverge from this invertibility intuition.In word alignment, this can lead to disagreementsbetween alignments inferred by a model trained inone direction and those inferred by a model trainedin the reverse direction.
To remedy this disparity(and other shortcomings), it is common to turn toalignment symmetrization techniques such as grow-diag-final-and (Koehn et al, 2003) which heuristi-cally combines alignments from both directions.Liang et al (2006) suggest a more fundamentalapproach they call Alignment by Agreement (ABA),which jointly trains two word alignment models bymaximizing their data-likelihoods along with a reg-ularizer that rewards agreement between their align-ment posteriors (computed over each parallel sen-tence pair).
Although their EM-like optimizationprocedure is heuristic, it proves effective at jointlytraining bidirectional models.
Ganchev et al (2008)propose another approach for agreement betweenthe directed models by adding constraints on thealignment posteriors.
Unlike ABA, their optimizationis exact, but it can be computationally expensive,requiring multiple forward-backward inferences ineach E-step.In this paper we develop a different approach forjointly training general bidirectional sequence align-ment models called Model Invertibility Regulariza-tion, or MIR (Section 3).
Our approach has twokey benefits over ABA and PostCAT: First, MIR can609be applied to sequence alignment without paralleldata.
Second, a single implementation seamlesslyextends to all IBM models, including the fertilitybased models.
Furthermore, since MIR follows theMAP-EM framework, it inherits its desirable con-vergence guarantees.The key idea facilitating the easy extension tocomplex models and to non-parallel data settings isin our regularizer, which operates on the model pa-rameters as opposed to their inferences.
Specifically,MIR was designed to reward model pairs whosetranslation tables respect the invertibility intuition.We tested MIR against competitive baselines ontwo sequence alignment tasks: word alignment(with parallel data) and back-transliteration deci-pherment (without parallel data).On Czech-English and Chinese-English wordalignment (Section 5), restricted to the HMM model,MIR attains F- and Bleu score improvements that arecomparable to those of ABA and PostCAT.
We fur-ther apply MIR beyond HMM, on the fertility-basedIBM Models, showing further gains in F-score com-pared to the baseline, ABAandPostCAT.
Interest-ingly, the HMM alignments obtained by ABA andMIR are qualitatively different, so that combining thetwo yields additive gains over each method by itself.On English-Japanese back-transliteration deci-pherment (Section 6), we apply MIR to the cascadeof wFSTs approach proposed by Ravi and Knight(2009).
Using MIR, we are able to reduce the whole-name error-rate relative to a model trained on paral-lel data by 33%, as well as significantly outperformthe joint model proposed by Mylonakis et al (2007).2 BackgroundWe are concerned with learning generative modelsthat describe transformations of a source-languagesequence e = (e1, .
.
.
, eI) to a target-language se-quence f = ( f1, .
.
.
, fJ).
We consider two differentdata scenarios.In the parallel data setting, each sample in the ob-served data consists of a pair (e, f).
The generativestory assigns the following probability to the eventthat f arises from e:p(f | e; ?)
=?ap(a, f | e; ?)
(1)where ?
denotes the model parameters and a de-notes a hidden variable that corresponds to unknownchoices taken in the generative process.In the non-parallel data setting, only the target se-quence f is observed and the source sequence e ishidden.
The model assigns the following probabil-ity to the observed data:p(f; ?)
=?ep(e)?ap(a, f | e).
(2)That is, the sequence f can arise from any sequencee by first selecting e ?
p(e) and then proceeding ac-cording to the parallel-data generative story (Eq.
1).Unsupervised training of such models entailsmaximizing the data log-likelihood L(?
):arg max?L(?)
= arg max?
?x?Xlog p(x; ?
)where X = {(en, fn)}nin the parallel data setting andX = {(fn)}nin the non-parallel data setting.Although the structure of ?
is unspecified, inpractice, most models that follow these generativestories contain a word translation table (t-table) de-noted t, with each parameter t( f | e) representing theconditional probability of mapping a given sourcesymbol e to a target symbol f .3 Model Invertibility RegularizationIn this section we propose a method for jointly train-ing two word alignment models, a source-to-targetmodel ?1and a target-to-source model ?2, by reg-ularizing their parameters to respect the invertibil-ity of the alignment task.
We therefore name ourmethod Model Invertibility Regularization (MIR).3.1 RegularizerOur regularizer operates on the t-table parameterst1, t2of the two models, as follows: Let matricesT1,T2denote the t-tables t1, t2in matrix form andconsider their multiplication T = T1T2.
The re-sulting matrix T is a stochastic square matrix ofdimension |V1| ?
|V1| where |V1| denotes the sizeof the source-language vocabulary.
Each entry Ti jrepresents the total probability mass mapped fromsource word eito source word ejby first applyingthe source-to-target mapping T1and then the target-to-source mapping T2.610In particular, each diagonal entry Tiiholds theprobability of mapping a source symbol back ontoitself, a quantity we intuitively believe should behigh.
We therefore (initially) consider maximizingthe trace of T :Tr[T ] =?iTii=?e?ft1( f | e) t2(e | f ).We further note that Tr[T ] = Tr[T1T2] = Tr[T2T1],so that the trace captures equally well how much thetarget symbols map onto themselves.Since T is stochastic, setting it to the identity ma-trix I maximizes its trace.
In other words, the moreT1and T2behave as (pseudo-)inverses of each other,the higher the trace is.
This exactly fits with our in-tuition regarding invertibility.Unfortunately, the trace is not concave in both T1and T2, a property which will become desirable inoptimization.
We therefore modify the trace regular-izer by applying the entrywise square root operatoron T1, T2and denote the new term R:R(t1, t2) = Tr[?T1?T2]=?e?f?t1( f | e) t2(e | f ).
(3)Note that R is maximized when?T1?T2= I.Concavity of R in both t1, t2(or equivalentlyT1,T2) follows by observing that it is a sum of con-cave functions ?
each term in the summation is ageometric mean, which is concave in its parameters.3.2 Joint Objective FunctionWe apply MIR in two data scenarios: In the paralleldata setting, we observe N sequence pairs {xn1}n={(en, fn)}nor, equivalently, {xn2}n= {(fn, en)}n.In the non-parallel setting, two monolingualdatasets are observed: N1source sequences {xn1}n={en}nand N2target sequences {xn2}n= {fn}n.The probability of the nth sample under the kthmodel ?k(for k ?
{1, 2}) is denoted pk(xnk; ?k).Specifically, in the parallel data setting, the proba-bility of xnkunder its model is:1p1(xn1; ?1) = p(fn| en; ?1)p2(xn2; ?2) = p(en| fn; ?2)1This slight notational abuse helps represent both data sce-narios succinctly.whereas in the non-parallel data setting, the proba-bility is defined as:p1(xn1; ?1) = p(fn; ?1)p2(xn2; ?2) = p(en; ?2).Using the above definitions and the MIR regular-izer R (Eq.
3), we formulate an optimization pro-gram for maximizing the regularized log-likelihoodsof the observed data:max?1,?2?R(t1, t2) +?k?
{1,2}Nk?n=1log pk(xnk; ?k) (4)where ?
?
0 is a tunable hyperparameter (note that,in the parallel case, N = N1= N2).We defer discussion on the relationship and mer-its of our approach with respect to ABA (Liang et al,2006) and PostCAT (Ganchev et al, 2008) to Sec-tion 4.3.3 Optimization ProcedureUsing our concave regularizer, MIR optimization(Eq.
4) neatly falls under the MAP-EM framework(Dempster et al, 1977) and inherits the convergenceproperties of the underlying algorithms.
MAP-EM follows the same structure as standard EM:The E-step remains identical to the standard E-step,while the M-step maximizes the complete-data log-likelihood plus the regularization term.
In the caseof MIR, the E-step can be carried out independentlyfor each model.
The only extra work is in the M-step, which optimizes a single (concave) objectivefunction.Specifically, let zndenote the missing data, where,in the parallel data setting, only the alignment ismissing (znk= ank) and in the non-parallel data set-ting, both alignment and source symbol are missing(zn1= (an1, en), zn2= (an2, fn)).In the E-step, each model ?k(for k ?
{1, 2})is held fixed and its posterior distribution over themissing data znkis computed per each observa-tion, xnk:qk(znk, xnk) := pk(znk| xnk; ?k).In the M-step, the computed posteriors are usedto define a convex optimization program that max-611imizes the regularized sum of expected complete-data log-likelihoods:max?1,?2?R(t1, t2) +?k?
{1,2}Nk?n=1qk(znk, xnk) log pk(xnk, znk)where n ranges over the appropriate sample set.Operationally, for models ?kthat can be encodedas wFSTs (such as the IBM1, IBM2 and HMMword alignment models), the E-step can be carriedout efficiently and exactly using dynamic program-ming (Eisner, 2002).
Other models resort to ap-proximation techniques ?
for example, the fertility-based word alignment models apply hill-climbingand sampling heuristics in order to efficiently esti-mate the posteriors (Brown et al, 1993)From the computed posteriors qkwe collect ex-pected counts for each event, used to construct theM-step optimization objective.
Since the MIR regu-larizer couples only the t-table parameters, the up-date rule for any remaining parameter is left un-changed (that is, one can use the usual closed-formcount-and-divide solution).Now, let Ce, f1and Ce, f2denote the expected countsfor the t-table parameters.
That is, Ce, fkdenotes theexpected number of times a source-symbol type e isseen aligned to a target-symbol type f according tothe posterior qk.
In the M-step, we maximize thefollowing objective with respect to t1and t2:arg maxt1,t2?e, fCe, f1log t1( f | e) +?e, fCe, f2log t2(e | f ) + ?R(t1, t2) (5)which can be efficiently solved using convex pro-gramming techniques due to the concavity of R andthe complete-data log-likelihoods in both t1and t2.In our implementation, we applied ProjectedGradient Descent (Bertsekas, 1999; Schoenemann,2011), where at each step, the parameters are up-dated in the direction of the M-step objective gradi-ent at (t1, t2) and then projected back onto the prob-ability simplex.
We used simple stopping conditionsbased on objective function value convergence and abounded number of iterations.4 Baselines4.1 Parallel Data Baseline: ABA and PostCATOur approach is most similar to Alignment byAgreement (Liang et al, 2006) which uses a singlejoint objective for two word alignment models.
Thedifference between our objective (Eq.
4) and theirslies in their proposed regularizer, which rewards theper-sample agreement of the two models?
alignmentposteriors:?nlog?zp1(z | xn) ?
p2(z | xn)where xn= (en, fn) and where z ranges over the pos-sible alignments between enand fn(practically, onlyover 1-to-1 alignments, since each model is only ca-pable of producing one-to-many alignments).Liang et al (2006) note that proper EM opti-mization of their regularized joint objective leads toan intractable E-step.
Unable to exactly and effi-ciently compute alignment posteriors, they resort toa product-of-marginals heuristic which breaks EM?sconvergence guarantees, but has a closed-form solu-tion and works well in practice.MIR regularization has both theoretical and prac-tical advantages compared to ABA, which make ourmethod more convenient and broadly applicable:1.
By regularizing for posterior agreement, ABA isrestricted to a parallel data setting, whereas MIRcan be applied even without parallel data.2.
The posteriors of more advanced word align-ment models (such as fertility-based models) donot correspond to alignments, and furthermore,are already estimated with approximation tech-niques.
Thus, even if we somehow adapt ABA?sproduct-of-marginals heuristic to such models,we run the risk of estimating highly inaccurateposteriors (specifically, zero-valued posteriors).In contrast, MIR extends to all IBM-style wordalignment models and does not add heuristics.The M-step computation can be done exactly andefficiently with convex optimization.3.
MIR provides the same theoretical convergenceguarantees as the underlying algorithms.Ganchev et al (2008) propose PostCAT whichuses Posterior Regularization (Ganchev et al, 2010)612to enforce posterior agreement between the twomodels.
Specifically, they add a KL-projection stepafter the E-step of the EM algorithm which returnsthe posterior q(z | x) closest in KL-Divergence toan E-step posterior, but which also upholds certainconstraints.
The particular constraints they suggestencode alignment agreement in expectation betweenthe two models?
posteriors.
For details, the readercan refer to (Ganchev et al, 2008).Similarly to ABA, with their suggested alignmentagreement constraints PostCAT cannot be appliedwithout parallel data and it is unclear how to extendit to fertility based models (however, it does seemspossible to apply other constraints using the generalposterior regularization framework).We compare MIR against ABA and PostCAT inSection 5.4.2 Non-Parallel Data Baseline: bi-EMMylonakis et al (2007) cast the two directionalmodels as a single joint model by reparameterizationand normalization.
That is, both directional models,consisting of a t-table only, are reparameterized as:t1( f | e) =?e, f?f?e, ft2(e | f ) =?e, f?e?e, f(6)They then maximize the likelihood of observedmonolingual sequences from both languages:max?L1({fn}; ?)
+ L2({en}; ?)
(7)where, for example:L1({fn}; ?)
= log?np(fn)= log?n?ep(fn| e)p(e)= log?n?ep(e)?mt1( fnm| e)Here, p(e) denotes the probability of e according toa fixed source language model.Once training of ?
is complete, we can decodean observed target sequence f, by casting ?
back interms of t1and apply the Viterbi decoding algorithm.To solve for ?
in Eq.
7, Mylonakis et al (2007)propose bi-EM, an iterative EM-style algorithm.
Theobjective function in their M-step is not concave,hinting that a closed-form solution for the maxi-mizer is unlikely.
The probability estimate that theyuse in the M-step appears to maximize an approx-imation of their M-step objective which omits thenormalization factors in Eq.
7.Nevertheless, bi-EM attains improved resultscompared to standard EM on both POS-tagging andmonotone noun sequence translation without paral-lel data.
We compare MIR against bi-EM in Sec.
6.5 Experiments with Parallel DataIn this section, we compare MIR against stan-dard EM training and ABA on Czech-English andChinese-English word alignment and translation.5.1 Implementation and CodeFor ABA2and PostCAT3training we used the au-thors?
implementation, which supports the HMMmodel.
Vanilla EM training was done usingGIZA++,4which supports all IBM models as wellas HMM.
Our method MIR was implemented on topof GIZA++.55.2 DataWe used the following parallel data to train the wordalignment models:Chinese-English: 287K sentence pairs from theNIST 2009 Open MT Evaluation constrained taskconsisting of 5.3M and 6.6M tokens, respectively.Czech-English: 85K sentence pairs from the NewsCommentary corpus, consisting of 1.6M and 1.8Mtokens, respectively.Sentence length was restricted to at most 40 tokens.5.3 Word Alignment ExperimentsWe obtained HMM alignments by running either 5or 10 iterations (optimized on a held-out validationset) of both IBM Model 1 and HMM.
We obtainedIBM Model 4 alignments by continuing with 5 it-erations of IBM Model 3 and 10 iterations of IBM2http://cs.stanford.edu/?pliang/software/cross-em-aligner-1.3.zip3http://www.seas.upenn.edu/?strctlrn/CAT/CAT.html4http://code.google.com/p/giza-pp/5https://github.com/vaswani/MIR_ALIGNMENT613Chi-Eng Cze-EngMethod Align F1 Align F1EM-HMM 64.6 65.0PostCAT-HMM 69.8 69.6ABA-HMM 70.8 70.4MIR-HMM 70.9 69.6EM-IBM4 68.4 67.3MIR-IBM4 72.9 70.7Table 1: Word alignment F1 scores.Model 4.
We then extracted symmetrized align-ments in the following manner: For all HMM mod-els, we used the posterior decoding technique de-scribed in Liang et al (2006) as implemented byeach package.
For IBM Model 4, we used thestandard grow-diag-final-and (gdfa) symmetrizationheuristic (Koehn et al, 2003).We tuned MIR?s ?
parameter to maximize align-ment F-score on a validation set of 460 hand-alignedCzech-English and 1102 Chinese-English sentences.Alignment F-scores are reported in Table 1.
Inparticular, the best results were obtained by MIR,when applied to the fertility based IBM4 model -we obtained gains of +2.1% (Chinese-English) and+0.3% (Czech-English) compared to the best com-petitor.5.4 MT ExperimentsWe ran MT experiments using the Moses (Koehnet al, 2007) phrase-based translation system.6Thefeature weights were trained discriminatively usingMIRA (Chiang et al, 2008), and we used a 5-gramlanguage model trained on the Xinhua portion of En-glish Gigaword (LDC2007T07).
All other parame-ters remained with their default settings.
The devel-opment data used for discriminative training were:for Chinese-English, data from the NIST 2004 andNIST 2006 test sets; for Czech-English, 2051 sen-tences from the WMT 2010 shared task.
We usedcase-insensitive IBM Bleu (closest reference length)as our metric.On both language pairs, ABA, PostCAT and MIRoutperform their respective EM baseline with com-parable gains overall.
However, we noticed thatABA and MIR are not producing the same alignments.6http://www.statmt.org/moses/Chi-Eng Cze-EngMethod NIST08 WMT09 WMT10EM-HMM 23.6 16.7 17.1PostCAT-HMM 24.6 16.9 17.4MIR-HMM 24.0 17.1 17.6ABA-HMM 24.4 17.1 17.7EM-IBM4 24.2 16.8 17.2MIR-IBM4 24.6 17.2 17.5ABA + MIR-HMM 25.1 17.4 17.9Table 2: Bleu scores.
Combining ABA and MIR HMMalignments improves Bleu score significantly over allother methods.For example, by combining their HMM alignments(simply concatenating aligned bitexts) the total im-provement reaches +1.5 Bleu on the Chinese-to-English task, a statistically significant improvement(p < 0.05) according to a bootstrap resampling sig-nificance test (Koehn, 2004)).
Table 5.4 summarizesour MT results.6 Experiments without Parallel DataRavi and Knight (2009) consider the challeng-ing task of learning a Japanese-English back-transliteration model without parallel data.
The goalis to correctly decode a list of 100 US senator nameswritten in katakana script, without having access toparallel data.
In this section, we reproduce their de-cipherment experiment and show that applying MIRto their baseline model significantly outperformsboth the baseline and the bi-EM method.6.1 Phonetic-Based Japanese DeciphermentRavi and Knight (2009) construct a English-to-Japanese transliteration model as a cascade of wF-STs (depicted in Figure 1, top).
According totheir generative story, any word in katakana is gen-erated by re-writing an English word in its En-glish phonetic representation, which is then trans-formed to a Japanese phonetic representation andfinally re-written in katakana script.
For example,the word ?computer?
is mapped to a sequence of8 English phonemes (k, ah,m, p, y, uw, t, er), whichis mapped to a sequence of 9 Japanese phonemes(K,O,N,P,Y,U,T,A,A) and finally to Katakana.They apply their trained transliteration model todecode a list of 100 US senator names and report a614whole-name error-rate (WNER)7of 40% with paral-lel data (trained over 3.3k word pairs), compared to73% WNER without parallel data (trained over 9.5kJapanese words only), demonstrating the weaknessof methods that do not use parallel data.6.2 Forward PipelineWe reproduced the English-to-Japanese translitera-tion pipeline of Ravi and Knight (2009) by con-structing each of the cascade wFSTs as follows:1.
A unigram language model (LM) of Englishterms, estimated over the top 40K most frequentcapitalized words found in the Gigaword corpus(without smoothing).2.
An English pronunciation wFST from the CMUpronunciation dictionary.83.
An English-to-Japanese phoneme mappingwFST that encodes a phoneme t-table t1whichwas designed according to the best setting re-ported by Ravi and Knight (2009).
Specifically,t1is restricted to either 1-to-1 or 1-to-2 phonememappings and maintains consonant parity.
Seefurther details in their paper.4.
A hand-built Japanese pronunciation toKatakana wFST (Ravi and Knight, 2009).6.3 Backward PipelineMIR requires a pipeline in the reverse direction,transliteration of Japanese to English.
We con-structed a unigram LM of Katakana terms over thetop 25K most frequent Katakana words found inthe Japanese 2005-2008-news dictionary from theLeipzig corpora.9The remaining required wFSTs were obtained byinverting the forward model wFSTs (that is, wFSTs2,3,4 above), and the cascade was composed in thereverse direction.
In particular, by inverting t1, weobtained the Japanese-to-English t-table t2that al-lows only 2-to-1 or 1-to-1 phoneme mappings.7The percentage of names where any error occurs anywherein either the first or last name.8http://www.speech.cs.cmu.edu/cgi-bin/cmudict9http://corpora.uni-leipzig.de/6.4 Training DataFor training data, we used the top 50% most frequentterms from the monolingual data over which we con-structed the LM wFSTs.
This resulted in a set of20K English terms (denoted ENG) and a set of 13KJapanese terms in Katakana (denoted KTKN).Taking the entire set of monolingual terms led topoor baseline results, probably since uncommon En-glish terms are not transliterated, and uncommonKatakana terms may be borrowed from languagesother than English.In any case, it is important to note that ENG andKTKN are unrelated, since both were collected overnon-parallel corpora.6.5 Training and TuningWe train and tune 4 models:baseline: the model proposed by Ravi and Knight(2009), which maximizes the likelihood (Eq.
2) ofthe observed Japanese terms KTKN.MIR: Our bidirectional, regularized model, whichmaximizes the regularized likelihoods (Eq.
4) ofboth monolingual corpora ENG, KTKN.bi-EM: The joint model proposed by Mylonakis etal.
(2007), which maximizes the likelihoods (Eq.
7)of both monolingual corpora ENG, KTKN.Oracle: As an upper bound, we train the model ofRavi and Knight (2009) as if it was given the correctEnglish origin for each Japanese term.
(over 4.2Kparallel English-Japanese phoneme sequences).We train each method for 15 EM iterations, whilekeeping the LM and pronunciation wFSTs fixed.Training was done using the Carmel finite-statetoolkit.10Specifically, baseline and oracle relyon Carmel exclusively, while for MIR and bi-EM, wemanipulated Carmel to output the E-step posteriors,which we then used to construct and solve the M-step objective using our own implementation.The different models were tuned over a develop-ment set consisting of 50 frequent Japanese termsand their English origin.
For each method, we chosethe so-called stretch-factor ?
?
{1, 2, 3} used to ex-ponentiate the model parameters before decoding10http://www.isi.edu/licensed-sw/carmel/615Figure 1: The transliteration generative story as a cascade of wFSTs.
Each box represents a transducer.
Top: transliter-ation of the word ?computer?
to Japanese Katakana.
Bottom: the reverse process.
MIR jointly trains the two cascadesby maximizing the regularized data log-likelihood with respect to the two (shaded) phoneme mapping models t1, t2.A B CHD E G H I J K M N O P R S SH T TS U W Y Zaaahawbdeheygihjhlnowpstuhvyzh0.000.080.160.240.320.400.480.560.64A B CHD E G H I J K M N O P R S SH T TS U W Y Zaaahawbdeheygihjhlnowpstuhvyzh0.000.080.160.240.320.400.480.560.64Figure 2: The 1-to-1 mapping submatrix of the t1transliteration table for independent training (left) and MIR (right).MIR learns sparser, peaked models compared to those learned by independent training.
(see Ravi and Knight (2009)), our model?s hyper-parameter ?
?
{1, 2, 3, 4}, and the number of itera-tions (up to 15) to minimize WNER on the develop-ment set.We decoded Japanese terms using the Viterbi al-gorithm, applied on the selected t1model (usingEq.
6 to convert the bi-EM model ?
back to to t1).Finally, note that ABA training and symmetrizationdecoding heuristics are inapplicable, since they relyon parallel data.6.6 Senator Name Decoding ResultsWe compiled our own test set, consisting of 100 USsenator names (first and last), and compared the per-formance of the four algorithms.
Table 3 reportsWNER, average normalized edit distance (NED)and the number of model parameters (t1) with valuegreater than 0.01 as an indication of sparsity.
Figure2 further compares the 1-to-1 portions of the bestmodel learned by the baseline method with thebest model learned by MIR, showing the differencein parameter sparsity.WNER NED t1> 0.01baseline 67% 23.2 649bi-EM 66% 21.8 600MIR 59% 17.3 421Oracle 43% 10.8 152Table 3: MIR reduces error rates (WNER, NED) andlearns sparser models (number of t1parameters greaterthan 0.01) compared to the other models.Using MIR, we obtained significant reduction inerror rates, closing the gap between the baselinemethod and Oracle, which was trained on paralleldata, by 33% in WNER and nearly 50% in NED.This error reduction clearly demonstrates the effi-cacy of MIR in the non-parallel data setting.6167 ConclusionWe presented Model Invertibility Regularization(MIR), an unsupervised method for jointly train-ing bidirectional sequence alignment models withor without parallel data.
Our formulation is basedon the simple observation that the alignment tasksat hand are inherently invertible and encouragesthe translation tables in both models to behave likepseudo-inverses of each other.We derived an efficient MAP-EM algorithm anddemonstrated our method?s effectiveness on two dif-ferent alignment tasks.
On word alignment, apply-ing MIR on the IBM4 model yielded the highest Fscores and the resulting Bleu scores were compara-ble to that of Alignment by Agreement (Liang et al,2006) and PostCAT (Ganchev et al, 2008).
Our bestMT results (up to +1.5 Bleu improvement) wereobtained by combining alignments from both MIRand ABA, indicating that the two methods learn com-plementary alignments.
On Japanese-English back-transliteration with no parallel data, we obtained asignificant error reduction over two baseline meth-ods (Ravi and Knight, 2009; Mylonakis et al, 2007).As future work, we plan to apply MIR on large-scale MT decipherment (Ravi and Knight, 2011;Dou and Knight, 2013), where, so far, only a singledirectional model has been used.
Another promis-ing direction is to encourage invertibility not onlybetween words, but between their senses and syn-onyms.AcknowledgementsWe would like to thank Markos Mylonakis andKhalil Sima?an for their help in understanding thederivation of their bi-EM method.
This work waspartially supported by DARPA grants DOI/NBCD12AP00225 and HR0011-12-C-0014 and a GoogleFaculty Research Award to Chiang.ReferencesDimitri P. Bertsekas.
1999.
Nonlinear Programming.Athena Scientific.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19:263?311.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of EMNLP.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Computational Linguistics, 39(4):1?38.Qing Dou and Kevin Knight.
2013.
Dependency-baseddecipherment for resource-limited machine transla-tion.
In EMNLP, pages 1668?1676.
ACL.Jason Eisner.
2002.
Parameter estimation for proba-bilistic finite-state transducers.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 1?8, Stroudsburg,PA, USA.
Association for Computational Linguistics.Kuzman Ganchev, Jo?ao V. Grac?a, and Ben Taskar.
2008.Better alignments = better translations?
In Proceed-ings of ACL-08: HLT, pages 986?993, Columbus,Ohio, June.
Association for Computational Linguis-tics.Kuzman Ganchev, Joao Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
The Journal of MachineLearning Research, 11:2001?2049.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of NAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, et al 2007.
Moses: Open source toolkit for sta-tistical machine translation.
In Proceedings of the 45thAnnual Meeting of the ACL on Interactive Poster andDemonstration Sessions, pages 177?180.
Associationfor Computational Linguistics.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of HLT-NAACL.Markos Mylonakis, Khalil Sima?an, and Rebecca Hwa.2007.
Unsupervised estimation for noisy-channelmodels.
In Proceedings of the 24th International Con-ference on Machine Learning, ICML ?07, pages 665?672, New York, NY, USA.
ACM.Sujith Ravi and Kevin Knight.
2009.
Learning phonememappings for transliteration without parallel data.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,NAACL ?09, pages 37?45, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Sujith Ravi and Kevin Knight.
2011.
Deciphering for-eign language.
In Proceedings of the 49th Annual617Meeting of the Association for Computational Linguis-tics: Human Language Technologies - Volume 1, HLT?11, pages 12?21, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Thomas Schoenemann.
2011.
Probabilistic word align-ment under the L0-norm.
In Proceedings of CoNLL.618
