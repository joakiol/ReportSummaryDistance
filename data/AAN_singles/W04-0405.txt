MWEs as Non-propositional Content IndicatorsKosho Shudo, Toshifumi Tanabe, Masahito Takahashi?
and Kenji YoshimuraFukuoka University                      ?Kurume Institute of Technology8-19-1, Nanakuma, Fukuoka,                   66-2228, Kamitsu, Kurume,814-0180 JAPAN                                    830-0052 JAPAN{shudo,tanabe,yosimura}@tl.fukuoka-u.ac.jp taka@cc.kurume-it.ac.jpAbstractWe report that a proper employment of MWEsconcerned enables us to put forth a tractableframework, which is based on a multiplenesting of semantic operations, for theprocessing of non-inferential, Non-propositional Contents (NPCs) of naturalJapanese sentences.
Our framework ischaracterized by its broad syntactic andsemantic coverage, enabling us to deal withmultiply composite modalities and theirsemantic/pragmatic similarity.
Also, therelationship between indirect (Searle, 1975)and direct speech, and equations peculiar tomodal logic and its family (Mally, 1926; Prior,1967) are treated in the similarity paradigm.1 IntroductionWhile proper treatment of the PropositionalContent (PC) of a sentence is undoubtedlyimportant in natural language processing (NLP),the Non-propositional Content (NPC) also plays acritical role in tasks such as discourseunderstanding, dialogue modeling, detectingspeaker?s intension.
We refer generically to theinformation which is provided by auxiliaries,adverbs, sentence-final particles or specificpredicative forms in Japanese sentences as NPC.
Itis concerned with notions such as polarity, tense,aspect, voice, modality, and illocutionary act,which incorporate temporal, contingent, subjective,epistemic or attitudinal information into the PC.Though the inferential NPC e.g., implicature(Grice, 1975), has been discussed in semantics orpragmatics, it lies beyond the state-of-the-arttechnology of NLP.
Besides, no systematic attemptto connect linguistic forms in the sentence with thenon-inferential NPCs has been reported in NLPcommunity.
In this paper, we present a frameworkfor the treatment of NPC of a sentence on the basisof the extensive, proper employment of multiwordexpressions (MWEs) indicating the NPCs inJapanese.
In Japanese, which is a so-called SOVlanguage, NPCs are typically indicated in the V-final position by auxiliaries, particles and theirvarious alternative multiword expressions.
Wehave extracted extensively these expressions fromlarge-scale Japanese linguistic data.
We refer tothese, including auxiliaries and ending-particles, asNPC indicators (NPCIs).
The number of NPCIsamounts to 1,500, whereas that of auxiliaries andending-particles is about 50, which is apparentlyinsufficient for practical NLP tasks.Our model leads to dealing not only with some ofillocutionary acts (Austin, 1962) but also with thelogical operations peculiar to the family of modallogic, i.e., deontic (Mally, 1926) and temporallogic (Prior, 1967).We also present, in this paper, the idea of thesimilarity among NPCs within our framework.This is essential for text retrieval, paraphrasing,document summarization, example-based MT, etc.Some of the indirect speech acts (Searle, 1975) andaxioms proper to the family of modal logic aretreated formally in the similarity paradigm.In Section 2, we introduce an overview of ourongoing MWE resource development for generalJapanese language processing.
In Section 3, weintroduce a framework for the treatment of NPC.
Aset of primitive functions to compose NPC isexplained in Section 4.
In Section 5, first, therelationship between the framework and Japanesesyntax, and second, methods to identify NPCs ofJapanese sentences and to apply them to atranslation task are described.
In Section 6, weformalize the similarity among NPCs within theframework.
In Section 7, we present conclusionsand comment on future work.2 Background MWE ResourcesThe authors have been concerned with how toselect atomic expressions of the sentenceconstruction in NLP based on the semanticcompositionality.
Morphosyntactically, thisproblem is also serious for the processing of theagglutinative, space-free language like Japanese.Our research on this subject started in ?70s byextracting manually multiword expressions asMWEs from large-scale Japanese linguistic data inthe general domain.
We estimate that the amount ofdata examined is 200,000 sentences.Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp.
32-39In this Section, we present an overview of ourongoing development of Japanese MWE resources.We have extracted multiword expressions that takeat least one of the following three features;f1: idiomaticity (semantic non-decomposability),f2: lexical rigidity (non-separability),f3: statistical boundness.The expression which causes the difficulty incomposing its overall meaning from normalmeanings of component words has f1.1  f2 includesthe feature to allow other words to cut in betweenthe component words.
The expression whosecomponents are bound each other with highconditional probability has f3.
Each multiwordexpression selected as a MWE was endowed with abinary-valued basic triplet (f1, f2, f3).
For example,an idiomatic, separable and not-statistically-boundexpression, ???????
hone?wo?oru?
?make aneffort (lit.
break bone)?
is endowed with (1,0,0) andcompositional, separable and statistically-boundexpression, ?????
???
gussuri?nemuru?
?sleep soundly?, with (0,0,1).
A dot ???
denotes aconventional word-boundary, hereafter.Fixed expressions, decomposable idioms,institutionalized phrases, syntactically-idiomaticphrases, light verb constructions discussed in (Saget al, 2002) and proverbs might correspondroughly to the triplets, (1,1,0), (1,0,0), (0,0,1),(0,x,1), (0,x,1) and (1,1,1), respectively.MWEs, whose number amounts to 64,800 atpresent, are classified by their overall, grammaticalfunctions as follows.
Examples with a triplet andthe current number of expressions are also given inthe following.
Compound nouns and proper nounsare excluded in the present study.Conceptual MWEs:nominal<10,000>:??
??
???
aka?no?tanin?
(1,1,0) ?complete stranger (lit.
red stranger)?;???????
turu?no?hitokoe?
(1,1,0) ?the voiceof authority (lit.
one note of crane)?
;  etc.verbal-nominal<1,700>:?
?
?
?
?
?
?morai?naki?
(1,1,0) ?weeping in sympathy (lit.received crying)?
; ?
?
?
?
?
?
?rappa?nomi?
(1,1,0) ?drinking direct from thebottle (lit.
trumpet drink)?
;  etc.verbal<34,000>: ?
?
?
?
?
?
?kami?simeru ?
(1,1,0) ?chew well (lit.
bite andfasten)?
; ??
????
ni?tumeru?
(1,1,0) ?boildown (lit.
boil and pack in)?
;  etc.adjectival<4,300>: ?
?
?
?
?
?
?okorip?poi ?
(0,1,0) ?irritable (lit.
anger-ish)?
;1 At present f1 and presumably f2 will not be decided by any statistical method.???
???
chuui?bukai?
(1,1,0) ?careful (lit.deep in caution)?
;  etc.adjectival-nominal<2,000>:???
??
????ikkan?no?owari?
(1,1,0) ?the very end (lit.
theend of a roll)?
; ?
?
?
?
?
?
?sujigaki?doori?
(0,1,0) ?as just planned (lit.
justas a plot)?
;  etc.adverbial<5,200>: ?
?
?
?
?
?
?
?waruku?suru?to?
(1,1,0) ?if the worst happens (lit.if it worsens)?
;  ?
?
?
?
?
?
?uttori?to?(0,1,0)?abstractedly?
;  etc.adnominal<2,600>: ???
??
???
taai?no?nai?
(1,0,1)?inconsiderable (lit.
with no altruism)?;?
?????
danko?taru?
(0,1,0) ?firm?
; etc.connective<300>: ???
???
sono?kekka?
(1,1,0) ?consequently (lit.
the result)?
;  ???????????
sore?ha?sate?oki?
(1,1,1) ?by the way(lit.
setting it aside)?
; etc.proverb-sentential<1,300>:?
?
?
?
?
?
?
?isoga?ba?maware?
(1,1,1) ?Make haste slowly.(lit.
go round if it is in a hurry.)?
; ????????????
shunmin?akatuki?wo?oboe?zu?
(1,1,1) ?Inspring one sleeps a sleep that knows no dawn.?
;etc.proverb-sentential-incomplete<900>: ?????????
yamai?ha?ki?kara?
(1,1,0) ?Fancy may killor more.
(lit.
Illness is brought from one?sfeeling.)?
; ?
?
?
?
?
?
?
?
?
?
?uma?no?mimi?ni?nenbutu?
(1,1,1) ?A nod is asgood as a wink to blind horse.
(lit.
buddhist?sinvocation to the ear of a horse)?
; etc.Functional MWEs:relation-indicator(RI)<1,000>:?
?
?
?
?
?
?ni?tui?te?
(1,1,0) ?about (lit.
in touch with)?
;  ???????
ni?yot?te?
(1,1,0) ?by (lit.
dependingon)?
; ???????
to?tomo?ni?
(1,1,0) ?with (lit.accompanied with)?
; ??
????
ni?okeru?
(1,1,0) ?in?
,?on  (lit.
placed in)?
;  etc.NPCI<1,500>: See Section 4.Nominals listed above are those marked with atriplet (1,1,x).
We exclude compound nouns with(0,0,x) and proper nouns, whose number amountsto quite large, in this study.
They should be treatedin some other way in NLP.
A treatment of thosecompound nouns for Japanese language processingis reported in (Miyazaki et al, 1993).Formally, the triplet is expanded in the lexicon to apartly multi-valued 7-tuple (f1, f2, f3, f4, f5, f6, f7).The augmented features are as follows;f4: grammatical class (shown above)f5: syntactical, original internal-structuref6: morphosyntactical variation: (m1, m2, ... , m9)m1: possibility to be modified by adnominalm2: possibility to be modified by appredicativem3: auxiliaries insertable in between its wordsm4: particles insertable in between its wordsm5: deletable particlesm6: particles by which those in it are replacedm7: constituents which can be reorderedm8: possibility to be nominalized by inversionm9: possibility to be passivizedf7: estimated relative frequencyf6 was adopted to ensure the flexibility of MWEs,while controlling the number of headings.Thus, our lexicon is not simply a list of MWEs butdesigned as a resource proliferous to a total varietyof idiosyncratic expressions.
(Shudo et al, 1980,1988; Shudo, 1989; Yasutake et al, 1997).The present study focuses on a set of NPCIs and itsrelationship to the non-propositional structure ofnatural sentences.
Some of our multiword NPCIsare treated in the general, rewriting framework forMT in (Shirai et al, 1993).3 Non-propositional Structures (NPSs)Let us consider the meaning of a sentence;(1) ??
??
???
??
???
????
????
??kare?ha?soko?ni?iru?bekide?nakat?ta?
?He shouldnot have been there?,where a verb ???
iru?
?be?
is followed bythree auxiliaries, ????
bekida?
?should?, ???nai?
?not?
and ??
ta?
?-ed?
which meanobligation, negation and past-tense, respectively, inthe sentence-final position 2 .
According to theoccurrences of them, the solely literal paraphraseof (1) would be something like;(2) ????????????????????????????????kare?ha?soko?ni?iru?bekida?to?iu?koto?ha?nakat?ta?
?It was not necessary for him to be there?,However, this reading is not correct for (1).
Rather,in contrast, its regular reading should be somethinglike;(3) ??
??
???
??
??
??
??
??
????kare?ga?soko?ni?i?ta?no?ha?mazui?
?It is evaluatedin the negative that he was there?,By the way, it will be reasonable to think sentences2 ????
bekida?
and ???
nai ?
are inflected as ????
bekide?
and ????
nakat?, respectively, in (1).
(2) and (3) share a kernel sentence ?????????
???
kare?ga?soko?ni?iru?
?He is there?, intowhich NPCs are incorporated successively, i.e.,first - obligation, second - negation, third - past-tense, in the case of (2), and first - past-tense,second - speaker?s-negative-evaluation, in the caseof (3).
Moreover, each stage of this incorporationwould be regarded as mapping the utterance?smeaning from one to another, in parallel with asyntactic form being mapped from one to another.Hence, by introducing Non-propositional PrimitiveFunctions (NPFs), e.g., OBLIGATION2,NEGATION1, PAST-TENSE, and NEG-EVAL, wecan explain the Non-propositional Structure (NPS)of (2) as;(4)PAST-TENSE [NEGATION1[OBLIGATION2[?
?
?
?
?
?
?
?
?
?
?
?kare?ga?soko?ni?iru?
?He is there?]
] ]and NPS of (3), hence, of (1) as,(5)NEG-EVAL[PAST-TENSE[????????????
kare?ga?soko?ni?iru?
?He is there?]
].3Here, a problem is that (4) is wrong for (1).
Inorder to cope with this, while adopting a MWE,??????????
bekide?nakat?ta?
as a NPCIwith a triplet (1,0,0) which has a composite NPF,NEG-EVAL[PAST-TENSE[x]]4, we have designedour segmenter to prefer a longer segment by theleast-cost evaluation.It should be noted that a composite of NPFs likethis could be associated with a single NPCI.
5 Thisis caused by its idiomaticity, i.e., by the difficultyin decomposing it into semantically consistent sub-forms.Investigating a reasonably sized set of Japaneselinguistic data, keeping the strategy exemplifiedabove in mind, revealed that NPS of a naturalJapanese sentence can be generally formulated as anested functional form;(6) Mn[Mn-1?[M2[M1[S]]]?
],where S is a propositional, kernel sentence; Mi(1?i?n), a NPF.
In the following, we use the3 We use lower-suffixes to distinguish NPFs by the subtle differences inmeaning, degree, etc.4 Another choice could be, first, to adopt a shorter MWE, ???????
bekide?nai?
?should not?
as a NPCI indicating PROHIBITION2, second, to build a NPS,PAST-TENSE[PROHIBITION2[????????????
kare?ga?soko?ni?iru?
?Heis there?
]], and last, to apply the following similarity rule in order to obtain (5),unless it yields the overgeneralization;PAST-TENSE[PROHIBITION2[x]] ?NEG-EVAL[PAST-TENSE[x]].The similarity rules are discussed in Section 6.5 Another typical example is???
mai?
which is a single auxiliary but has themeaning of ?will not?, i.e., GUESS2[NEGATION1[x]].notation for a composite function,Mn?Mn-1?
?M2?M1, where Mn?Mn-1?
?M2?M1[S] =Mn[Mn-1?[M2[M1[S]]]?
].4 NPCIs, NPFsWe have settled a set of 150 basic NPFs byclassifying 1,500 NPCIs which had been extractedfrom the large-scale data.
After manuallyextracting them, the data has been continuouslychecked and updated by comparing with variousdictionaries and linguistic literature such as(Morita et al,1989).They are subclassified as follows, though theboundaries between subclasses are partly subtle.
Itshould be noted that some NPCIs are semanticallyambiguous, being included in different subclassesbelow.
Examples of NPCIs and the number ofNPFs are given in brackets, in the following list.F1:polarity <3>:NEGATION1(???
nai?
?not?
; ?????????no?de?ha?nai?
(1,0,0) ?not?
; etc.),NEGATION2(?
?
?
?
?
?
?
?
?
?
?
?
?
?to?iu?wake?de?ha?nai?
(1,0,0) ?not?
; etc.
),etc.F2:tense <1>:PAST-TENSE(??
ta?
V-ed ; ??
da?
V-ed)F3:aspect-observational <9>:IMMEDI-AFT-TERMINATING (????????ta?tokoro?da?
(1,1,0) ?have just V-en?
;  ??????????????
ta?bakari?no?tokoro?da?
(1,1,0)?have just V-en?
; etc.),IMMEDI-BEF-BEGINING(??????
?????u?to?si?te?iru?
(1,0,0)  ?be about to?
; ????????????
you?to?si?te?iru?
(1,0,0) ?be about to?
;etc.),PROGRESSING(?????
te?iru?
(1,0,0)  ?be V-ing?
;  ??????
tutu?aru?
(1,1,0) ?be V-ing?;etc.
), etc.F4:aspect-action <8>:INCHOATIVE(?????
hajimeru?
?begin to?;?
?
?
dasu?
?begin to?
; etc.),TERMINATIVE(????
owaru?
?finish V-ing?
; ????
oeru?
?finish V-ing?
; etc.),CONTINUATIVE(????
tuzukeru?
?continueto?
;  ?????
nagaraeru?
?continue to?
;  etc),etc.F5:voice <10>:PASSIVE(???
reru?
?be V-en?
;  ????rareru?
?be V-en?),CAUSATIVE(???
seru?
?make?V??
;  ????
saseru?
?make?V??),PAS-SUFFERING(???
reru?
?have?V-en?
;????
rareru?
?have?V-en?
;  etc.
),PAS-BENE-TAKING1 (?
?
?
?
?
?te?morau?
(1,0,0)?ask ?V?
; ?
?
?
?
?
?
?te?itadaku?
(1,0,0) ?ask?
V?
;  etc.),BENE-TAKING(??????
te?kureru?
(1,0,0)?V... for (someone)...?
; etc.
), etc.F6:politeness-operator <3>:POLITENESS1 (???
masu?
; etc.)
,etc.F7:predicate-suffix <30>:TRIAL(??
???
te?miru?
(1,0,0) ?try to?
;etc.
),etc.F8:modality <60>:NEG-EVAL(????????
beki?de?nai?
(1,0,0)?should not?
; ??????????
no?ha?yoku?nai?
(1,0,0) ?should not?
; etc.),OBLIGATION2(????????
hituyou?ga?aru ?
(1,0,0) ?need?, ????
bekida?
?should?, etc.
),OBLIGATION1(?
?
?
?
?
?
?
?
?
?
?
?nakere?ba?nara?nai?
(1,1,1) ?have to?
;  etc.),PROHIBITION(?
?
?
?
?
?
?
?
?
?te?ha?nara?nai?
(1,0,1) ?should not?, etc.),CAPABILITY(???
uru?
; ?????????koto?ga?dekiru?
(1,0,0) ?be able to?
; etc.),GUESS1(??
u?
?will?
), etc.F9:illocutionary-act <28>:IMPERATIVE(imperative-form of verb?imperative form?
),INTERROGATIVE (?
?
ka?
?interrogativeform?
;  ??
??
no?ka?
(1,1,0) ?interrogativeform?
;  etc.
),  PROHIBITIVE(?
?
na??Don?t...
?
), PERMISSIVE(??
?
?
?
(1,1,0)te?yoi?
?You may...?
; ??
??
????
???te?mo?kamawa?nai?
(1,0,0) ?You may...?
; etc.),REQUESTING(?
?
?
?
?
te?kure?(1,1,0)?Please...?
; ??????
te?hosii?
(1,1,0) ?I wantyou to...?
;  etc.
), etc.5 Treatment of NPSs5.1 Sentence-final Structure in JapaneseEmploying MWEs as NPCIs enabled us todescribe the outermost structure of a Japanesesentence by the following production rules;(7) S0?BP*?PRED,(8) Si?Si-1?mi, (1?i?n),where S0 denotes a kernel sentence; BP, a basicphrase called bunsetsu; PRED, a predicate of thekernel sentence; Si, a sentence, mi, a NPCI and asymbol ?
*?, closure operator on the concatenation,???.
In the following, we use predicative parts,PRED?
m1?
m2 ?
???
?
mn instead of full sentences, forsimplicity.Our morphology model was developed so as to fitfor the general semantic processing, adoptingMWEs.
It is a probabilistic finite automaton with150 states that prescribes minutely the internalstructure of each BP and the predicative part.
Weleave its detail to (Shudo et al, 1980).5.2 Identifying NPSBased on our morphological analyzer, we havedeveloped a segmenter (SEG) that segments theinput predicative part into a PRED and each NPCI,and a NPS-constructor (NPSC) that constructsNPSs.
For example, an input;(9) ?
?
?
?
?
?
?
?
?
?
?
?
?
?yomanakerebanaranaidarou?
?will have toread?is first segmented into(10) ???
/????
??
???
???
/???
?
?yoma/?nakere?ba?nara?nai/?daro?u ?by SEG.
Here, a slash ?/?
denotes a segment-boundary identified by SEG.
Then, NPSCevaluates a function nps defined below.
(11) nps(S0)=S0,nps(S0/m1/m2.?/mi)=Mik[?Mi2[Mi1[nps(S0/m1/m2.
?/mi-1)]]],(1?i?n),where Mik[?Mi2[Mi1[x]]] is a NPF (if k =1) or acomposite of NPFs (if k?2) associated with mi.Hence, the computation of nps for (10) is;(12) nps(???/????????????/?????yoma/?nakere?ba?nara?nai/?daro?u?)=GUESS2[nps(???
/????
??
???
???yoma/?nakere?ba?nara?nai?
?have to read?)]=GUESS2[OBLIGATION1[nps(???
yomu??read?)]]=GUESS2[OBLIGATION1[?
?
?
yomu??read?
]],where GUESS2 and OBLIGATION1 are associatedwith ?????
daro?u?
?will?
and ????????????
nakere?ba?nara?nai ?
?have to?, respectively.In order to examine the adequacy and sufficiencyof NPFs, we evaluated outputs of NPSC for 4,083input predicative parts, which had been takenrandomly as a test set from newspaper articles andsegmented by SEG.
It produced a recall of 97.4%and a precision, 41.8%.
The score of the recallseems to imply the sufficiency of the set of NPFsand NPCIs.
Relatively low score of the precision isdue to the system?s over-generation caused by thesemantic ambiguities of NPCIs.
Among variousmeasures to be taken, firstly, semantic constraintsto control the composition operation ???
may beeffective to produce a better precision.
Thecomplete disambiguation measure is left to futurework.5.3 Application to J/E Machine TranslationWe introduce here another experimental system,referred to as ENGL, whose input is the NPS of asentence and whose output is its English forms, todemonstrate the usefulness of our formalism.ENGL simply realizes NPFs within English syntax.We assumed each NPF for English could beaccomplished by applying rewriting rules of twotypes; i) V ?
x ?
Vv ?
y and ii) S?
x ?
Sv ?
y , whereV is a verb or an auxiliary; Vv is V, a null string, ora variant of V; S, a sentence; Sv, a variant of S; andx, y, a null string or a string of specific words.Basically, a single rewriting rule is applied for asingle NPF.
However, occasionally, a NPF requiresseveral rules to be applied successively.
Also wemay have no NPCI corresponding to a given NPFwithin the target language.
For example,POLITENESS, which is common in colloquialJapanese, has mostly no NPCI in English.For example, the computation for (12) isGUESS2 [OBLIGATION1 [???
yomu?
]]= GUESS2 [OBLIGATION1 [read]]= GUESS2 [have to ?
read]=will ?
have to ?
read,where the rewriting rules associated withNECESSITY1 and GUESS2 are V?have to ?
Vrootand V?will ?
Vroot , respectively.We give four more I/O examples  In (14), theinstantaneous aspect of aruki ?
hajimeru ; beginwalk-ing excludes the possibility of theinterpretations, PROGRESSING1,PROGRESSING2 and STATE-OF-THINGS ofteiru, which remain in (13) or (15).
This is becausethe system deals with concatenatability rules basedon aspect features of the predicate.
(ENGL simplydenotes the verb?s inflected form by -ed, -en, etc.
)(13) nps(???/?????
; manan/?de?iru?
)=1 PROGRESSING1[study]= be study-ing,=2 PROGRESSING2[study]= have be-en study-ing,=3 COMPLETED1[study]=have study-en(14)nps(?
?
?
/?
?
?
?
?
?
?
?
;aruki/?hajime?te?iru?
)=COMPLETED1[INCHOATIVE[walk]]= havebegin-en walk-ing(15)nps(???/?????
; aisi/?te?iru?
)= STATE-OF-THINGS[love] = love(16)nps(????/????/???????/??????/??
/?
?
;ugokasi/?te?mi/?te?mo?yoi/?no?desho/?u/?ka?
)=NTERROGATIVE[GUESS1[DECLARATION[PERMISSIVE[TRIAL[move]]]]]= Will it be allowed that...try to move....?A small-scale experiment, for 300 NPSs extractedfrom sentences in technical papers has shown thatENGL produced a precision of 86% and a recall,80%.
While these relatively high scores implies thefundamental validity of the NPF framework, moreextensive tests will be required to make morereliable evaluation for the general domain, sincetechnical papers tend to have less-complicatedNPFs.
In addition, further correction andrefinement of synthesis rules for English will benecessary to obtain higher scores.6 Similarity between NPSsIn this section, we show that our framework for theNPS description can be used properly to formalizesome semantic or pragmatic relationship betweennon-propositionalized sentences.6.1 Logical RulesFirst, we discuss, here, the logical similarityrelation, ?
?
((?Fi)*)2, (1?i?8), which seemscrutial for NLP tasks such as text retrieval orparaphrasing.6 We prefer the term, ?similarity?
to?equivalence?
here since it should be based on truthvalues taken in ?most situations?, or in some?similar?
worlds.
7There are basic rules such as;(17) NEG-EVAL?
NEGATION1?OBLIGATION2(18) NEGATION1?PERMISSION?PROHIBITION,(19) NEGATION1,2?NEGATION1,2?
?
(identity function),(20) N???
?
?N ?N for?N?
(?Fi)* , (1?i?8),(21) POLITENESS ?
?,(17) asserts that, for example, an utterance, ?Hehas to go there.?
is similar to ?It is evaluated inthe negative that he does not go there.?.
Besidesthese basic rules, there is a set of logically notablerules.
For example, from the observation that ???
/?
?
?
?
?
?
?
?
?
/?
?
?
?
?
?
?
?
?6 While the NPF in Fi, (1?i?7) produces a truth conditional sentence, the NPFin F9 does not.
The NPF in F8 produces a truth conditional sentence, unless it isused for the speaker?s epistemic judgment.7 But we do not enter further theoretical arguments here.hatarai/?te?bakari?iru/?wake?de?ha?nai?
?do notalways work?
is similar to ??????/???????
hataraka?nai/?toki?ga?aru?
?It happensoccasionally that?do not work?
the following rulewill be induced;(22) NEGATION2?HIGHEST-FREQUENCY?LOW-FREQUENCY?NEGATION1.Also, ?
?
?
/?
?
?
/?
?
?
?
?
?
?hataraka/?naku/?te?mo?yoi?
?need not work?
; ?It isallowed that...do not work??
and ???/??????
?
?
?
?
?
?
/?
?
?
?
?
?
?hataraka/?nakere?ba?nara?nai/?koto?ha?nai?
?It isnot obligatory that ?work?
?, will induce a rule;(23) PERMISSION?NEGATION1?NEGATION1?OBLIGATION.These rules can be generalized as (24), (24?)
byintroducing a ?duality?
function, d defined below;M, d(N)                          d(M), N-----------------------------------------------------------POSSIBILITY                     NECESSITY,HIGHEST-PROBABILITY,HIGHEST-CERTAINTYLOW-FREQUENCY   HIGHEST-FREQUENCY,HIGHEST-USUALITYPERMISSION                    OBLIGATION,HIGHEST-INEVITABILITY-----------------------------------------------------------(24) NEGATION1,2?M ?
d(M) ?NEGATION1,2,(24?)
M ?NEGATION1,2?
d(M) ?NEGATION1,2.We show two more examples;(22?)
HIGHEST-FREQUENCY?NEGATION1?NEGATION2?LOW-FREQUENCY.nps(?
?
?
/?
?
?
?
?
?
?
?
?
?
?
?hataraka/?nai?de?bakari?iru?
?It is alwaysthat?do not work?? )
?nps(???/????????
/?
?
?
?
?
?
?
?
?
?hataraku/?koto?ga?aru/?to?ha?ie?nai?
?It does nothappen that?sometimes work??
).(23?)
OBLIGATION?NEGATION1?NEGATION1?PERMISSION.nps(?
?
?
/?
?
?
?
?
?
?
?
?
?hatarai/?te?ha?nara?nai?
?must not work?
)?nps(???
/??
???
/??
???
??
??
???hatarai/?te?yoi/?to?iu?koto?ha?nai?
?It is notpermissible that?work??
).Rule (24) corresponds to the axiom, ??????
?, in modal logic and its variants, e.g., deontic(Mally, 1926) or temporal (Prior, 1967) logic,where ?
and ?
are the necessity and possibilityoperator, respectively.6.2 Pragmatic RulesThe similarity relation among the speaker?sattitude or intention toward the hearer is defined asa set, ??
{ (a,b) | a,b ?
(F1?F2?
?F9)* ?
((?i,1?i?l?fi?F9)?
(?j, 1?j?m?gj?F9)), wherea=f1?f2?
?fl, b=g1?g2?
?gm}.Some of the indirect speech acts (Searle, 1975) canbe formulated as the similarity within ourframework.
Examples of the rules and theirinstances are;(25) REQUESTING?INTERROGATIVE?NEGATION1?CAPABILITY,?INTERROGATIVE?CAPABILITY,?POLITENESS?IMPERATIVE,?INTERROGATIVE?NEGATION1?BENE-TAKING,?INTERROGATIVE?NEGATION1?CAPABILITY?PASS-BENE-TAKING,?DESIRING?PASS-BENE-TAKING,?DESIRING?PASSIVE.nps(??/?????
mi/?te?kure?
?Look at ??
),?
nps(???
/???
??
???
/???
/??miru/?koto?ga?deki/?nai/?ka?
?Can?t you lookat ???
),?nps(???/?????????/??miru/?koto?ga?dekiru/?ka?
?Can you look at???
),?nps(??/????
mi/?nasai?
?Please look at??
),?
nps(?
?
/?
?
?
?
?
/?
?
?
/?
?mi/?te?kure/?nai/?ka?
?Don?t you look at ?
forme ???
),?
nps(?
?
/?
?
?
?
?
/?
?
/?
?
?
/?
?mi/?te?mora/?e/?nai/?ka?
?Can?t I have youlook at?
for me???
),?nps(??/??????/???
mi/?te?morai/?tai ?
?I want you to look at ?
for me??
),?nps(??/???/???
mi/?rare/?tai?
?I wantyou to look at ??
).With respect to prohibition, invitation,permission and assertion, we have;(26) PROHIBITIVE?PROHIBITION,?NEGATION1?CAPABILITY.nps(???/??
hairu/?na?
?Do not enter??
)?
nps(?
?
?
/?
?
?
?
?
?
?
?
?
?hait/?te?ha?nara?nai?
?You must not enter?? )
,?
nps(?
?
?
/?
?
?
?
?
?
?
/?
?
?hairu/?koto?ga?deki/?nai?
?You can notenter??
),(27)INVITING?INTERROGATIVE?
INVITING,?INTERROGATIVE?NEGATIVE1.nps(????/??
tabeyo/?u?
?Let?s eat??
)?nps(????/??/??
tabeyo/?u/?ka?
?Will you eat???
),?nps(???/???/??
tabe/?nai/?ka?
?Don?t you eat???
).(28)PERMISSIVE?POSSIBILITY.nps(??/?????
ki/?te?yoi?
?You may wear??
)?
nps(?
?
?
/?
?
?
?
?
?
?
?
?kiru/?koto?ga?dekiru?
?You can wear??
).(29)ASSERTING?PAST-TENSE?
NEGATION1?INTERROGATIVE?
PAST-TENSEnps(???/????/??/??
tabe/?nakat/?ta/?yo?
;?I did not eat... ?
),?nps(???
/??
/???
tabe/?ta/?kai?
; ?Did Ieat ...?
?
).We have obtained approximately 30 pragmaticrules concerned with the NPCIs in Japanese.
In therealistic tasks of NLP, application of these rulesshould be controlled by rather complicatedconditions settled for each of them.
For example,conditions for rules (25) ~ (28) will include thatthe agent of their complement sentence should bethe second person, and for (29), the first.
Althoughthe principle underlying these rules were discussedin a lot of literature, e.g., felicity condition in(Searle, 1975), etc., the whole picture has not beenclarified for computational usage.7 ConclusionsWe have shown that as far as the non-inferential,Non-Propositional Content (NPC) in Japanesesentence is concerned, its semanticcompositionality can be secured, providedsentence-final MWEs are adopted properly asNPCIs.
Although the functional treatment of NPCsis not particularly new in the theoretical domain,our model is characterized by its broadsyntactic/semantic coverage and its tractability inNLP.
It connects syntax with semantics by actuallydefining 150 non-propositional functions (NPFs)for 1500 NPC indicators through a large-scaleempirical study.
The similarity equations presentedhere might lead to some formal system of?calculations?
on the set of NPFs, which might beavailable for NLP in future.The syntactic coverage of our semantic/pragmaticmodel will surely be further broadened byinvestigating non-final parts of Japanese sentences.This research should focus on the sentenceembedment whose main verb is epistemic orperformative (Austin, 1962), and adverbs that takepart in indicating NPCs.While the list of NPFs introduced in this paper willprovide, we believe, a basis for analyzing the NPCof natural sentences, it might be possible, or rathernecessary for particular task, to refine NPFs byenriching them with case-elements, more detaileddegrees or subtle differences in meaning, etc.We have not solved the problem of semanticallydisambiguating each NPCI.
Further, we know littleabout the language-dependency, consistency of thesimilarity rules.
The language-dependency of NPSis interesting from the viewpoint of machinetranslation or comparative pragmatics.
Theframeworks presented here could hopefullyprovide tools for those comparative studies.ReferencesJohn L. Austin.
1962.
How to Do Things withWords.
Oxford U.P.H.
Paul Grice.
1975.
Logic and Conversation.
In P.Cole and J. L. Morgan, editors, Syntax andSemantics Vol.
3, Speech Acts: 41-57.
AcademicPress.Ernst Mally.
1926.
Grundgesetze des Sollens:Elemente der Logik des Willens.
Universit?ts-Buchhandlung: viii+85.Masahiro Miyazaki, Satoru Ikehara and AkioYokoo.
1993.
Combined Word Retrieval forBilingual Dictionary Based on the Analysis ofCompound Words.
Trans.
IPSJ 34-4: 743-754.
(in Japanese)Yoshiyuki Morita and Masae Matsuki.
1989.Expression Pattern of Japanese.
ALC Press.
(inJapanese)Arthur Prior.
1967.
Past, Present and Future.Clarendon press, Oxford.Iwan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake and Dan Flickinger.
2002.
MultiwordExpressions: A Pain in the Neck for NLP.
TheProc.
of the 3rd CICLING: 1-15.John R. Searle.
1975.
Indirect Speech Acts.
In P.Cole and J. L. Morgan, editors, Syntax andSemantics Vol.
3, Speech Acts: 59-82, AcademicPress.Satoshi Shirai, Satoru Ikehara and TsukasaKawaoka.
1993.
Effects of Automatic Rewritingof Source Language within a Japanese toEnglish MT System.
Fifth InternationalConference on Theoretical and MethodologicalIssues in Machine Translation: TMI-93: 226-239Kosho Shudo, Toshiko Narahara and Sho Yoshida.1980.
Morphological Aspect of JapaneseLanguage Processing.
The Proc.
of the 8thCOLING: 1-8.Kosho Shudo, Kenji Yoshimura, Mitsuno Takeutiand Kenzo Tsuda.
1988.
On the IdiomaticExpressions in Japanese Language ?
AnApproach through the Close and ExtensiveInvestigation of Non-standard Usage of Words ?IPSJ SIG Notes, NL-66-1: 1-7.
(in Japanese)Kosho Shudo.
1989.
Fixed Collocations.
Ministryof Education, Science, Sports and Culture,Grant-in-Aid for Scientific Research, 63101005.
(in Japanese)Masako Yasutake, Yasuo Koyama, KenjiYoshimura, Kosho Shudo.
1997.
Fixed-collocations and Their Permissible Variants.
TheProc.
of the 3rd Annual Meeting of ANLP: 449-452.
(in Japanese)
