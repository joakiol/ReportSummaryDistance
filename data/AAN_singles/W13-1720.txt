Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 152?156,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsSimple Yet Powerful Native Language Identification on TOEFL11Ching-Yi Wu Po-Hsiang Lai Yang Liu     Vincent NgUniversity of Texas at Dallas Emerging Technology Lab Samsung R&D - Dallas University of Texas at Dallas800 W Campbell Rd 1301 Lookout Drive 800 W Campbell RdRichardson, TX, USA Plano, TX, USA Richardson, TX, USAcxw120631@utdallas.edu s.lai@samsung.com yangl@hlt.utdallas.eduvince@hlt.utdallas.eduAbstractNative language identification (NLI) is thetask to determine the native language of theauthor based on an essay written in a secondlanguage.
NLI is often treated as a classifica-tion problem.
In this paper, we use theTOEFL11 data set which consists of moredata, in terms of the amount of essays andlanguages, and less biased across prompts, i.e.,topics, of essays.
We demonstrate that evenusing word level n-grams as features, and sup-port vector machine (SVM) as a classifier canyield nearly 80% accuracy.
We observe thatthe accuracy of a binary-based word level n-gram representation (~80%) is much betterthan the performance of a frequency-basedword level n-gram representation (~20%).Notably, comparable results can be achievedwithout removing punctuation marks, suggest-ing a very simple baseline system for NLI.1 IntroductionNative language identification (NLI) is an emerg-ing field in the natural language processing com-munity and machine learning community (Koppelet al 2005; Blanchard et al 2013).
It is a task toidentify the native language (L1) of an authorbased on his/her texts written in a second language.The application of NLI can bring many benefits,such as providing a learner adaptive feedback oftheir writing errors based on the native languagefor educational purposes (Koppel et al 2005;Blanchard et al 2013).NLI can be viewed as a classification problem.In a classification problem, a classifier is firsttrained using a set of training examples.
Eachtraining example is represented as a set of features,along with a class label.
After a classifier istrained, the classifier is evaluated using a testingset (Murphy, 2012).
Good data representation oftenyields a better classification performance (Murphy,2012).
Often time, the simpler representationsmight produce better performance.
In this work,we demonstrate that a binary-based word level n-gram representation yields much better perform-ance than a frequency-based word level n-gramrepresentation.
In addition, we observed that re-moving punctuation marks in an essay does notmake too much difference in a classification per-formance.The contributions of this paper are to demon-strate the usefulness of a binary-based word leveln-gram representation, and a very simple baselinesystem without the need of removing punctuationmarks and stop words.This paper is organized as the following.
InSection 2, we present related literatures.TOEFL11 data set is introduced in Section 3.
InSection 4, our features and system design are de-scribed.
The results are presented in Section 5,followed by conclusion in Section 6.1522 Related WorkThe work by Koppel et al(2005) is the first studyto investigate native language identification.
Theyuse the International Corpus of Learner English(ICLE).
They set up this task as a classificationproblem studied in machine learning community.They use three types of features: function words,character n-gram, errors and idiosyncrasies, e.g.spelling and grammatical errors.
For errors andidiosyncrasies, they used Microsoft Office Word todetect those errors.
Their features were evaluatedon a subset of the ICLE corpus, including essayssampled from five native languages (Russian,Czech, Bulgarian, French and Spanish) with 10-fold cross validation.
They achieve an accuracy of80.2% by combining all of the features and using asupport vector machine as the classification algo-rithm.
In addition, Tsur and Rappoport (2007)show that using character n-gram only on the ICLEcan yield an accuracy of 66%.The work from Kochmar (2011) identifies anauthor?s native language using error analysis.
Shesuggests that writers with different native lan-guages generate different grammatical error pat-terns.
Instead of using ICLE, this work uses adifferent corpus, English learner essays from theCambridge Learner Corpus.
She uses SVM onmanually annotated spelling and grammatical er-rors along with lexical features.Most of the systems described in NLI literaturereach good performance in predicting an author?snative language, using character n-gram and part ofspeech n-gram as features (Blanchard et al 2013).In recent years, various studies have started to lookinto complex features in order to improve the per-formance.
Wong and Dras (2009) use contrastiveanalysis, a systematic analysis of structural simi-larities and differences in a pair of languages.
Awriter?s native language influences the target lan-guage they aim to learn.
They explore the impactof three English as Second Language (ESL) errortypes, subject-verb disagreement, noun-numberdisagreement and determiner errors, and use a sub-set of ICLE with 7 languages.
However, althoughthe determiner error feature seems useful, when itis combined with a baseline model of lexical fea-tures, the classification performance is not signifi-cantly improved (Wong and Dras, 2009).Wong and Dras (2011) use complex featuressuch as production rules from two parsers andreranking features into the classification frame-work, incorporating lexical features of Koppel et al(2005).
They achieve a classification performanceof 81.71% on the 7-native-languages NLI, slightlybetter than 80.2% accuracy of the original Koppelet al(2005).Note that although the International Corpus ofLearner English (ICLE) is used in most of the NLIstudies, ICLE has been known to have fewer es-says, and a skewed distribution toward topics ofessays (Blanchard et al 2013).
In addition, eventhough there are 16 native languages in ICLE, aseach language has different numbers of essays,most work often uses different subsets of 7 nativelanguages, which makes comparison harder acrossdifferent studies (Blanchard et al 2013).
The NLIshared task 2013 provides a new data set, namelythe TOEFL11 (Blanchard et al 2013), which ad-dresses these issues.
As previously discussed,complex features do not necessarily improve clas-sification accuracy.
In this work, we useTOEFL11 to investigate the classification per-formance using simple word n-gram based features.3 DataIn this work, we use TOEFL11 as our corpus.TOEFL11 is a new data set for NLI (Blanchard etal., 2013).
There are 11 native languages, includingArabic (ARA), Chinese (CHI), French (French),German (GER), Hindi (HIN), Italian (ITA), Japa-nese (JPN), Korean (KOR), Spanish (SPA), Telugu(TEL), and Turkish (TUR).
Authors write essaysbased on 8 different topics in English.
There are1,100 essays for each language, and sampled from8 different topics, i.e., prompts.
Each essay isalso annotated with an English proficiency level(low/medium/high) determined by assessment spe-cialists.
Among 12,100 essays, there are 9,900essays in the training set, 1,100 essays in the de-velopment set, i.e., validation set in machine learn-ing, and 1,100 essays in the testing set.
In thetraining set and the development set, there areequal numbers of essays from each of the 11 nativelanguages.
By using TOEFL11, it makes ouranalysis less biased toward a specific topic of es-says (Blanchard et al 2013).1534 NIL System DesignIn this section, we describe our NLI system, thefeatures, and the classifier we use.4.1 Data PreprocessingEach essay is tokenized, and then capitalizationsare removed.
Note that we did not remove Englishstop words, which might be useful to discriminatethe native language for a writer.
For example,function words, which belong to stop words, suchas ?the?, ?at?, ?which?, have been proven to be ef-fective to distinguish native language for writers(Koppel et al 2005).
There are two settings: ei-ther punctuation marks are removed or kept.When punctuation marks are kept, they are viewedthe same as word in constructing n-grams.
Forexample, in the sentence ?NLI is fun.
?, ?fun .?
isviewed as a bigram.4.2 FeaturesIn our system, word level n-grams are used to rep-resent an essay.
Previous studies have shown thatword level n-grams are useful in determining thenative language of a writer (Bykh and Meurers,2012).
One reasonable hypothesis is that non-native English writers with the same native lan-guages tend to choose more similar words to ex-press the same or similar concepts.
In addition, thecombination of a sequence of words might also beaffected by the different native language of writers.Therefore, word n-gram is useful to distinguish thenative language of a writer.
Even though someprevious studies have looked into using word leveln-grams as features, how to use word level n-grams has not been explored too much yet onTOEFL11 corpus.
To our knowledge, the mostrecent study by Blanchard et al(2013) started toresearch the effect of different forms of word leveln-gram representations.There could be many ways to represent an essayby word level n-grams.
One possible representa-tion of an essay is to use the frequency of a spe-cific word n-gram, i.e., the number of times aspecific word n-gram appears in an essay dividedby the number of times all word n-grams appear inan essay.
In this representation, an essay is a vec-tor whose elements are the frequency of differentword n-grams in the essay.
Another possible rep-resentation is to use binary representation, i.e., 1indicates this word n-gram is in this essay, 0 indi-cates this word n-gram is not in this essay.
Oneinteresting question to ask is:Which representation can be more informativeto distinguish the native language of writers of es-says?Here we compare the performance of a fre-quency-based word level n-gram representationand a binary-based word level n-gram representa-tion.
We included all word level n-grams in thetraining set, without any frequency cutoff.
Forboth binary-based and frequency-based representa-tions, we run the experiments on the two settings:punctuation marks are either removed or kept.In addition to word level n-grams, sinceTOEFL11 also consists of English proficiency lev-els evaluated by assessment experts, we also in-cluded it to test whether this feature might improvethe classification performance.
All of the featuresused in our system are summarized in Table 1.Besides each feature described above, we have alsocombined different features to test whether variouscombinations of features might improve the accu-racy performance.
Here, we simply aggregateddifferent features, for example, all word level uni-grams, combined with all word level bigrams.4.3 ClassifierPrevious literatures have used various methodssuch as Na?ve Bayse, logistic regression and sup-port vector machine on NLI problem.
As it hasbeen shown that when representing an essay inorder to perform a classification task, it often re-sults in an essay being represented in a very highdimensional space.
Since support vector machine(SVM) is known to be adaptive when the featuredimension is high, we chose SVM as our classifi-cation algorithm.
We also compared the resultsfrom Na?ve Bayse for an experimental purpose andfound that SVM is better.
We use SVM-Light forour system (Joachims, 1999).
We then train ourSVM classifier on the training set (n=9900), andtest the trained classifier on the testing set(n=1100).1545 Results and Discussions5.1 ResultsTable 1 and Table 2 show the accuracies on thetesting set for the different feature sets, when punc-tuation marks are removed or kept respectively.As the results demonstrated, the accuracies of wordlevel bigram are better than unigram using a bi-nary-based representation.
When combining wordlevel unigram and bigram, the accuracy is im-proved in a binary-based representation.
This isconsistent when punctuations are either removed orkept.
This observation is consistent with the exist-ing NLI literatures: when combining word n-grams,it seems to improve the accuracy of the classifier,compared with a word n-gram alone.
But we donot observe too much difference when punctuationmarks are removed or kept, using both unigramand bigram.
In fact, including punctuation markslead to high accuracies in many scenarios, espe-cially in unigram in a frequency-based representa-tion, suggesting the usage of punctuation marksvaries across native languages.FeaturesPerformance ofBinary Word n-gram Representa-tionPerformance ofFreq.
Word n-gram Representa-tionword unigram 70.91% 25.36%word bigram 76.00% 17.64%word unigramandword bigram79.73% 23.36%Table 1 Accuracy of Different Feature Sets, withoutPunctuation MarksFeaturesPerformance ofBinary Word n-gram Representa-tionPerformance ofFreq.
Word n-gram Representa-tionword unigram 70.18% 30.00%word bigram 77.09% 18.73%word unigramandword bigram79.45% 28.73%Table 2 Accuracy of Different Feature Sets, withPunctuation MarksTable 3 shows the confusion matrix of classifi-cation performance, using unigram and bigram, ina binary-based representation when punctuationmarks are removed.
We observe that some of na-tive languages, such as German, Italian, and Chi-nese, lead to better classification accuracy than forKorean, Spanish, and Arabic.ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Preci-sionRe-callF-measureARA 75 1 5 3 1 3 1 1 3 4 3 78.9 75.0 76.9CHI 3 86 0 0 1 0 5 4 0 0 1 81.9 86.0 83.9FRE 1 1 79 7 3 4 2 0 1 0 2 77.5 79.0 78.2GER 3 1 2 87 1 1 1 0 2 0 2 79.8 87.0 83.3HIN 1 2 1 2 77 0 0 0 5 10 2 74.0 77.0 75.5ITA 0 0 6 4 0 85 0 0 3 0 2 83.3 85.0 84.2JPN 2 2 1 0 0 1 86 3 2 0 3 77.5 86.0 81.5KOR 0 8 2 1 1 0 14 72 1 1 0 82.8 72.0 77.0SPA 4 0 6 3 4 6 1 1 70 1 4 78.7 70.0 74.1TEL 1 0 0 1 15 0 0 0 0 82 1 83.7 82.0 82.8TUR 5 4 0 1 1 2 1 6 2 0 78 79.6 78.0 78.8Average Performance: 79.7%.
Precision, Recall, F-measures are in %.Table 3 Confusion Matrix on Testing Set5.2 Binary Based of Word N-Gram Repre-sentationWe observe that the accuracy of a binary-basedword level n-gram representation in our system issignificantly better than a frequency-based repre-sentation.
This is similar to the result reported byBlanchard et al (2013) in TOEFL11 corpus.
Thedifferences between their system and ours are thatthe system developed by Blanchard et al (2013)used logistic regression with L1-regularzation, in-stead of SVM and they did not remove all punctua-tion marks and special characters.This might imply that a frequency-based wordn-gram representation do not capture the character-istics of the data.
This might be because the dataresides in a high dimension space, and the frequen-cies of word level n-grams would be skewed.
In afuture study, one might investigate a better repre-sentation form and other complex features thathave a stronger interpretative power of the data.5.3 Effects of Proficiency LevelIn our results, we have included English profi-ciency level (low/medium/high) as a feature pro-vided by assessment experts.
However, we did notfind a strong improvement in accuracies, for ex-ample, 79.13% using a binary-based word level n-grams when punctuation marks removed.
Wethink this might be because only one feature will155not dramatically change the accuracies.
This maybe due to the fact word n-grams have already con-tributed a large amount of features.6 ConclusionIn this paper, we used a new data set, TOEFL11 toinvestigate NLI.
In the most existing literatures,ICLE corpus was used.
However, ICLE has fewerdata and is known to be biased to topics of essays.The newly released corpus, TOEFL11 addressesthese two drawbacks, which is useful for NLIcommunity.
Support vector machine (SVM) wasused as a classifier in our system.
We have dem-onstrated that a binary-based word level n-gramrepresentation has resulted in a significantly betterperformance compared to a frequency-based n-gram representation.
We observed that there is notmuch difference in classification accuracies whenpunctuation removed or kept, when combiningboth unigram and bigram.
Interestingly, a fre-quency-based word unigram with punctuationmarks outperforms than the case without punctua-tion marks, suggesting the potential of utilizingpunctuation marks in NLI.
In addition, Englishproficiency level has also been included in our fea-ture set, but did not yield a significant improve-ment in accuracy.
As most of the essays arerepresented in a high dimension space using wordlevel n-grams, we are looking into feature selectionto reduce dimensionality and how to representthose features in order to improve accuracy, aswell as other features.ReferencesBlanchard, D., Tetreault, J., Higgins, D., Cahill, A., andChodorow, M. 2013.
TOEFL11: A Corpus of Non-Native English.
Educational Testing Service.Bykh, S. and Meurers, D. 2012.
Native Language Iden-tification using Recurring n-grams - InvestigatingAbstraction and Domain Dependence.
In Proceed-ings of COLING 2012, 425-440, Mumbai, India.
TheCOLING 2012 Organizing Committee.Joachims, T. 1999.
Making large-Scale SVM LearningPractical.
Advances in Kernel Methods - SupportVector Learning, B. Sch?lkopf and C. Burges and A.Smola (ed.
), MIT-Press.Kochmar, E. 2011.
Identification of a writer?s nativelanguage by error analysis.
Master?s thesis, Univer-sity of Cambridge.Koppel, M., Schler, J., and Zigdon, K. 2005.
Automati-cally determining an anonymous author?s native lan-guage.
In ISI, 209?217.Murphy, K. P. 2012.
Machine learning: a probabilisticperspective.
MIT Press.Tsur, O. and Rappoport, A.
2007.
Using classifier fea-tures for studying the effect of native language on thechoice of written second language words.
In Pro-ceedings of the Workshop on Cognitive Aspects ofComputational Language Acquisition, 9?16, Prague,Czech Republic.
Association for Computational Lin-guistics.Wong, S.-M. J. and Dras, M. 2009.
Contrastive analysisand native language identification.
In Proceedings ofthe Australasian Language Technology AssociationWorkshop 2009, 53?61, Sydney, Australia.Wong, S.-M. J. and Dras, M. 2011.
Exploiting parsestructures for native language identification.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, 1600?1610,Edinburgh, Scotland, UK.
Association for Computa-tional Linguistics.156
