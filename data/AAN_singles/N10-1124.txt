Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 822?830,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsClinical Information Retrieval using Document and PICO StructureFlorian Boudin and Jian-Yun NieDIRO, Universite?
de Montre?alCP.
6128, succursale Centre-villeMontre?al, H3C 3J7 Que?bec, Canada{boudinfl,nie}@iro.umontreal.caMartin DawesDepartment of Family MedicineMcGill University, 515 Pine Ave WMontre?al, H2W 1S4 Que?bec, Canadamartin.dawes@mcgill.caAbstractIn evidence-based medicine, clinical questionsinvolve four aspects: Patient/Problem (P), In-tervention (I), Comparison (C) and Outcome(O), known as PICO elements.
In this pa-per we present a method that extends the lan-guage modeling approach to incorporate bothdocument structure and PICO query formu-lation.
We present an analysis of the distri-bution of PICO elements in medical abstractsthat motivates the use of a location-basedweighting strategy.
In experiments carried outon a collection of 1.5 million abstracts, themethod was found to lead to an improvementof roughly 60% in MAP and 70% in P@10 ascompared to state-of-the-art methods.1 IntroductionAs the volume of published medical literature con-tinues to grow exponentially, there is more and moreresearch for physicians to assess and evaluate andless time to do so.
Evidence-based medicine (EBM)(Sackett et al, 1996) is a widely accepted paradigmin medical practice that relies on evidence frompatient-centered clinical research to make decisions.Taking an evidence-based approach to searchingmeans doing a systematic search of all the availableliterature, individually critically appraising each re-search study and then applying the findings in clini-cal practice.
However, this is a time consuming ac-tivity.
One way to facilitate searching for a preciseanswer is to formulate a well-focused and structuredquestion (Schardt et al, 2007).Physicians are educated to formulate their clinicalquestions according to several well defined aspectsin EBM: Patient/Problem (P), Intervention (I),Comparison (C) and Outcome (O), which are calledPICO elements.
In many documents in medical lit-erature (e.g.
MEDLINE), one can find the elementsof the PICO structure, but rarely explicitly anno-tated (Dawes et al, 2007).
To identify documentscorresponding to a patient?s state, physicians alsoconstruct their queries according to the PICO struc-ture.
For example, in the question ?In children withpain and fever how does paracetamol comparedwith ibuprofen affect levels of pain and fever??
onecan identify the following PICO elements:Patient/Problem: children/pain and feverIntervention: paracetamolComparison: ibuprofenOutcome: levels of pain and feverVery little work, if any, has been carried out on theuse of these elements in the Information Retrieval(IR) process.
There are several reasons for that.
Itis not easy to identify PICO elements in documents,as well as in the question if these are not explicitlyseparated in it.
Several studies have been performedon identifying PICO elements in abstracts (Demner-Fushman and Lin, 2007; Hansen et al, 2008; Chung,2009).
However, all of them are reporting coarse-grain (sentence-level) tagging methods that have notyet been shown to be sufficient for the purpose ofIR.
Moreover, there is currently no standard test col-lection of questions in PICO structure available forevaluation.
On the other hand, the most critical as-pect in IR is term weighting.
One of the purposeof tagging PICO elements is to assign appropriateweights to these elements during the retrieval pro-cess.
From this perspective, a semantic tagging ofPICO elements may be a task that goes well beyond822that is required for IR.
It may be sufficient to havea method that assigns appropriate weights to ele-ments rather than recognizing their semantic roles.In this paper, we will propose an approach to deter-mine term weights according to document structure.This method will be compared to that using taggingof PICO elements.In this paper, we first report an attempt to manu-ally annotate the PICO elements in documents byphysicians and use them as training data to buildan automatic tagging tool.
It turns out that thereis a high disagreement rate between human anno-tators.
The utilization of the automatic tagging toolin an IR experiment shows only a small gain in re-trieval effectiveness.
We therefore propose an alter-native to PICO element detection that uses the struc-tural information of documents.
This solution turnsout to be robust and effective.
The alternative ap-proach is motivated by a strong trend that we ob-serve in the distribution of PICO elements in docu-ments.
We then make use of both PICO query anddocument structure to extend the classical languagemodeling approach to IR.
Specifically, we investi-gate how each element of a PICO query should beweighted and how a location-based weighting strat-egy can be used to emphasize the most informativeparts (i.e.
containing the most PICO elements) ofdocuments.The paper is organized as follows.
We first brieflyreview the previous work, followed by a descriptionof the method we propose.
Next, we present ourexperiments and results.
Lastly, we conclude with adiscussion and directions for future work.2 Related workThere have been only a few studies trying to usePICO elements in the retrieval process.
(Demner-Fushman and Lin, 2007) is one of the few such stud-ies.
The method they describe consists in re-rankingan initial list of retrieved citations.
To this end, therelevance of a document is scored by the use of de-tected PICO elements, among other things.
Severalother studies aimed to build a Question-Answeringsystem for clinical questions (Demner-Fushman andLin, 2006; Andrenucci, 2008).
But again, the focushas been set on the post-retrieval step, while the doc-ument retrieval step only uses a standard approach.In this paper, we argue that IR has much to gain byusing PICO elements.The task of identifying PICO elements has how-ever gain more attention.
In their paper, (Demner-Fushman and Lin, 2007) presented a method thatuses either manually crafted pattern-matching rulesor a combination of basic classifiers to detect PICOelements in medical abstracts.
Prior to that, biomed-ical concepts are labelled by Metamap (Aronson,2001) while relations between these concepts areextracted with SemRep (Rindflesch and Fiszman,2003).
Recently, supervised classification usingSupport Vector Machines (SVM) was proposed by(Hansen et al, 2008) to extract the number of trialparticipants.
In a later study, (Chung, 2009) ex-tended this work to other elements using ConditionalRandom Fields.
Although these studies are report-ing interesting results, they are limited in several as-pects.
First, many are restricted to some segmentsof the medical documents (e.g.
Method section)(Chung, 2009), and in most cases, the test collectionis very small (a few hundreds abstracts).
Second, theprecision and granularity of these methods have notyet been shown to be sufficient for the purpose of IR.The structural information provided by markuplanguages (e.g.
XML) has been successfully usedto improve the IR effectiveness (INEX, 2002 2009).For such documents, the structure information canbe used to emphasize some particular parts of thedocument.
Thereby, a given word should not havethe same importance depending on its position in thedocument structure.Taking into account the structure can be done ei-ther at the step of querying or at the step of index-ing.
One way to integrate the structure at queryingis to adapt query languages (Fuhr and Gro?johann,2001).
These approaches follow the assumption thatthe user knows where the most relevant informationis located.
However, (Kamps et al, 2005) showedthat it is preferable to use structure as a search hint,and not as a strict search requirementThe second approach consists in integrating thedocument structure at the indexing step by introduc-ing a structure weighting scheme (Wilkinson, 1994).In such a scheme, the weight assigned to a word isnot only based on its frequency but also on its posi-tion in the document.
The structure of a documentcan be defined in terms of tags (e.g.
title, section),823each of those having a weight chosen either empiri-cally or automatically by the use of optimizing tech-niques such as genetic algorithms (Trotman, 2005).3 Using PICO elements in retrievalIn this section, we present an experiment on themanual annotation of PICO elements.
We then de-scribe an approach to detect these elements in doc-uments and give some results on the use of thesetagged elements in the retrieval process.3.1 Manual annotation of PICO elementsWe asked medical professionals to manually anno-tate the PICO elements in a small collection of ab-stracts from PubMed1.
The instructions given tothe annotators were fairly simple.
They were askedto precisely annotate all PICO elements in abstractswith no restriction about the size of the elements (i.e.they could be words, phrases or sentences).
Morethan 50 abstracts were manually annotated this wayby at least two different annotators.
Two annotationsby two annotators are considered to agree if theyshare some words (i.e.
they overlap).
We computedthe well known Cohen?s kappa measure as well as anad-hoc measure called loose.
The latter uses PICOelements as units and estimates the proportion of el-ements that have been annotated by both raters.Measure P-element I/C-element O-elementkappa 0.687 0.539 0.523loose 0.363 0.136 0.140Table 1: Agreement measures computed for each ele-ment.
Cohen?s kappa and loose agreement are presented.We can observe that there is a very low agree-ment rate between human annotators.
The loosemeasure indicates that less than 15% of the I, C andO elements have been marked by both annotators.This fact shows that such human annotations can behardly used to develop an automatic tagging tool forPICO elements, which requires consistent trainingdata.
We therefore try to develop a coarser-grainedtagging method.1www.pubmed.gov, PubMed is a service of the US Na-tional Library of Medicine that includes over 19 million cita-tions from MEDLINE and other life science journals.3.2 Automatic detection of PICO elementsSimilarly to previous work, we propose a sentence-level detection method.
The identification of PICOelements can be seen as a classification task.
Evenfor a coarser-grain classification task, we are stilllack of annotated data.
One solution is to use thestructural information embedded in some medicalabstracts for which the authors have clearly stateddistinctive sentence headings.
Some recent ab-stracts in PubMed do contain explicit headings suchas ?PATIENTS?, ?SAMPLE?
or ?OUTCOMES?,that can be used to locate sentences correspond-ing to PICO elements.
Using that information, weextracted three sets of abstracts: Patient/Problem(14 279 abstracts), Intervention/Comparison (9 095)and Outcome (2 394).Tagging each document goes through a three stepsprocess.
First, the document is segmented into plainsentences.
Then each sentence is converted into afeature vector using statistical (e.g.
position, length)and knowledge-based features (e.g.
MeSH semantictype).
Knowledge-based features were derived ei-ther from manually crafted cue-words/verbs lists orsemantic types within the MeSH ontology2.
Finally,each vector is submitted to multiple classifiers, onefor each element, allowing to label the correspond-ing sentence.
We use several algorithms imple-mented in the Weka toolkit3: decision trees, SVM,multi-layer perceptron and Naive Bayes.
Combin-ing multiple classifiers using a weighted linear com-bination of their prediction scores achieves the bestresults with a f-measure score of 86.3% for P, 67%for I/C and 56.6% for O in 10-fold cross-validation.3.3 Use of detected elements in IRWe use language modeling approach to IR in thiswork.
The idea is that a document is a good match toa query if its language model is likely to generate thequery (Ponte and Croft, 1998).
It is one of the state-of-the-art approaches in current IR research.
Mostlanguage modeling work in IR use unigram lan-guage models ?also called bags-of-words models?assuming that there is no structure in queries or doc-uments.
A typical way to score a document d asrelevant to a query q is to use the Kullback-Leibler2www.nlm.nih.gov/mesh/3www.cs.waikato.ac.nz/ml/index.html824divergence between their respective LMs:score(q, d) =?w?qP(w | Mq) ?
log P(w | Md) (1)?
?KL(Mq || Md)whereMq is the LM of the query andMd the LM ofthe document.
P(w | M?)
estimates the probabilityof the word w given the language model M?.
Themost direct way to estimate these models is to useMaximum Likelihood estimation over the words:P(w | M?)
=count(w, ?
)| ?
|where ?
is the observed document, count(w, ?)
thenumber of times the wordw occurs in ?
and | ?
| thelength of the document.
Bayesian smoothing usingDirichlet priors is then applied to the maximum like-lihood estimator to compensate for data sparseness.We propose an approach that extend the basicLM approach to take into consideration the PICOelement annotation.
We assume that each ele-ment in the document has a different importanceweight.
Four more LMs are created, one for eachelements.
Given?e the weight of the PICO elemente, P(w | Md) in equation 1 is re-defined as:P1(w | Md) ?
P(w | Md) +?e?
[P,I,C,O]?e ?
P(w | Me)The right hand of the above equation is not a prob-ability function.
We could use a normalization totransform it.
However, for the purpose of documentranking, this will not make any difference.
There-fore, we will keep the un-normalized value.We performed an extensive series of experimentsusing this model on the test collection described inSection 5.
The results are shown in Table 2.
It turnsout that the best improvement we were able to obtainis very small (0.5% of MAP increase).
There maybe several reasons for that.
First, the accuracy ofthe automatic document tagging may be insufficient.Second, even if elements are correctly identified indocuments, if queries are treated as bags-of-wordsthen any PICO element can match with any identi-cal word in the query, whether it describe the sameelement or not.
However, we also tested a na?
?ve ap-proach that matches the PICO elements in querieswith the corresponding elements in documents.
Butthis approach quickly turns out to be too restrictiveand leads to bad results.MeasureWeighted elementsP I / C O Best?MAP increase 0.0% ?0.2% ?0.1% +0.5%Table 2: Results using the PICO elements automaticallydetected in documents (?
: wP = 0.5, wI = 0.2).As we can see, this approach only brings limitedimprovement in retrieval effectiveness.
This risesthe question of the usability of such tagging methodin its current performance state.
We will see in thenext section an alternative solution to this problemthat relies on the distribution of PICO elements indocuments.4 Method4.1 Distribution of PICO elementsPICO elements are not evenly distributed in medicaldocuments, which often follow some implicit writ-ing convention.
An intuitive method is to weighthigher a segment that is more probable to con-tain PICO elements.
The distribution of PICO el-ements is likely to correlate to the position withinthe document.
This intuition has been used in mostof the supervised PICO detection methods whichuse location-based features.
There has been sev-eral studies that cover the PICO extraction problem.However, as far as we know, none of them analysesand uses the positional ditribution of these elementswithin the documents for the purpose of IR.
Biomed-ical abstracts can be typically represented by four or-dered rhetorical categories which are Introduction,Methods, Results and Discussion (IMRAD) (Sollaciand Pereira, 2004).
The reason is found in the needfor speed when reviewing literature, as this formatallows readers to pick those parts of particular in-terest.
Besides, many scientific journals explicitlyrecommended this ordered structure.The PICO dispersion is highly correlated to theserhetorical categories as some elements are morelikely to occur in certain categories.
For example,outcomes are more likely to appear in Results and/orDiscussion parts.
One could also expect to infer the825role played by PICO elements in a clinical study.
Forexample, the drug pioglitazone has not the same rolein a clinical study if it appears as the main interven-tion (likely to occur in all parts) or as a comparativetreatment (Methods and/or Results parts).Instead of analysing the dispersion of PICO ele-ments into the four IMRAD categories, we choose tothe use automatically splitted parts.
There are sev-eral reasons for that.
First, the IMRAD categoriesare not explicitely marked in abstracts.
An auto-matic tagging of these would surely result in someerrors.
Second, using a low granularity approachwould provide more precise statistics.
Furthermore,if one would use the dispersion of elements as a cri-terion to estimate how important each part is, an au-tomatic partition would be a good choice because ofits repeatability and ease to implement.We divided each manually annotated abstract into10 parts of equal length (P1 being the begining andP10 the ending) and computed statistics on the num-ber of elements than occur in each of these parts.The Figure 1 shows the proportion of elements foreach part.
We can observe that PICO elements arenot evenly distributed throughout the abstracts.
Uni-versally accepted rules that govern medical writingstyles would be the first reason for that.
It is clearthat the beginning and ending parts of abstracts docontain most of the PICO elements.
This gives us aclear indication on which parts should be enhancedwhen searching for these elements.8 9 10 11 12 13% of PICO elementsP10P9P8P7P6P5P4P3P2P1Parts of the abstractsFigure 1: Proportion of PICO elements computed foreach different part of abstracts.Therefore, there may be several levels of granu-larity when using the PICO framework in IR.
Onecan identify each PICO element in the document,whether it is described by a word, a phrase or a com-plete sentence.
One can also use a coarser-grainapproach, estimating from the distribution acrossdocuments the probability that each part contains aPICO element.
As attempts to precisely locate PICOelements have shown that this task is particularlydifficult, we propose to get rid this issue by usingthe second method.4.2 Model definitionsWe propose three different models that extend theclassical language modeling approach.
The first usesthe structural information of documents, the secondtakes advantage of the PICO query structure whilethe third simply combine the first two models.Model-1Attempts to precisely locate PICO elements in doc-uments have shown that this task is particularly dif-ficult.
We propose to get around this issue by intro-ducing structural markers to convey document struc-ture and use them as a means of providing locationinformation.
Accordingly, each document is repre-sented as a series of successive parts.
To integratedocument structure into the ranking function, we es-timate a series of probabilities that constraints theword counts to a specific part instead of the entiredocument.
Each document d is then ranked by aweighted linear interpolation.
Intuitively, the weightof a part should depend on how much information isconveyed by its words.
Given ?p the weight of thepart p ?
[TITLE, P1 ?
?
?
P10], P(w | Md) in equation1 is re-defined as:P2(w |Md) ?
P(w |Md)+?p?d?p ?P(w ?
p |Md)Model-2The PICO formulation of queries provides informa-tion about the role of each query word.
One ideais to use this structural decomposition to thoroughlybalance elements in the ranking function.
For exam-ple, the weight given to the drug fluoxetine should bedifferent depending on whether it refers to the inter-vention or comparison concept.
The same goes forobesity which can be a problem or an outcome.
To826integrate this in the ranking function, we define a pa-rameter ?e that represents the weight given to querywords belonging to the element e ?
[P, I, C, O].f(w, e) = 1 if w ?
e, 0 otherwise.
We re-definedP(w | Md) in equation 1 as:P3(w |Mq) ?
P(w |Mq)+?e?
[P,I,C,O]?e ?f(w, e)?P(w |Mq)Model-1+2This is the combination of the two previously de-scribed models.
We re-defined the scoring functionas:score(q, d) =?w?qP3(w | Mq) ?
log P2(w | Md)5 ExperimentsIn this section, we describe the details of our exper-imental protocol.
We then present the results ob-tained with the three proposed models.Experimental settingsWe gathered a collection of nearly 1.5 million ab-stracts from PubMed with the following require-ments: with abstract, humans subjects, in englishand selecting the following publication types: RCT,reviews, clinical trials, letters, practice guidelines,editorials and meta-analysis.
Prior to the index con-struction, each abstract is automatically divided into10 parts of equal length, abstracts containing lessthan 10 words are discarded.
The following fieldsare then marked: TITLE, P1, P2, ... P10 with P1 be-ing the begining of the document and P10 the end-ing.Unfortunately, there is no standard test collectionappropriate for testing the use of PICO in IR andwe had to manually create one.
For queries, we usethe Cochrane systematic reviews4 on 10 clinicalquestions about different aspects of ?diabetes?.These reviews contain the best available infor-mation about an healthcare intervention and aredesigned to facilitate the choices that doctors facein health care.
All the documents in the ?Includedstudies?
section are judged to be relevant for the4www.cochrane.org/reviews/question.
These included studies are selected bythe reviewers (authors of the review article) andjudged to be highly related to the clinical question.In our experiments, we consider these documentsas relevant ones.
From the 10 selected questions,professors in family medicine have formulated a setof 52 queries, each of which was manually anno-tated according to the PICO structure.
The resultingtesting corpus is composed of 52 queries (averagelength of 14.7 words) and 378 relevant documents.Below are some of the alternative formulations ofqueries for the question ?Pioglitazone for type 2diabetes mellitus?
:In patients with type 2 diabetes (P) | does pioglita-zone (I) | compared to placebo (C) | reduce strokeand myocardial infarction (O)In patients with type 2 diabetes who have a high riskof macrovascular events (P) | does pioglitazone (I) |compared to placebo (C) | reduce mortality (O)We use cross-validation to determine reasonableweights and avoid over-fitting.
We have divided thequeries into two groups of 26 queries: Qa and Qb.The best parameters found for Qa are used to teston Qb, and vice versa.
In our experiments, we usethe KL divergence ranking (equation 1) as baseline.The following evaluation measures are consideredrelevant:Precision at n (P@n).
Precision computed on onlythe n topmost retrieved documents.Mean Average Precision (MAP).
Average of preci-sions computed at the point of each relevant docu-ment in the ranked list of retrieved documents.MAP is a popular measure that gives a globalquality score of the entire ranked list of retrieveddocuments.
In the case of clinical searches, onecould also imagine this scenario: a search performedby a physician who does not have the time to lookinto large sets of results, but for whom it is impor-tant to have relevant results in the top 10.
In suchcase, P@10 is also an appropriate measure.Student?s t-test is performed to determine statis-tical significance.
The Lemur Toolkit5 was used for5www.lemurproject.org827all retrieval tasks.
Experiments were performed withan ?out-of-the-box?
version of Lemur, using its tok-enization algorithm and porter stemmer.
The Dirich-let prior smoothing parameter was set to its defaultvalue ?
= 2500.Experiments with model-1We first investigated whether assigning a weight toeach part of the document can improve the retrievalaccuracy.
It is however difficult to determine a setof reasonable values for all the parts together, as thevalue of one part will affect those of the others.
Inthis study, we perform a two pass tuning.
First, weconsider the ?p weights to be independent.
By doingso, searching for the optimal weight distribution canbe seen as tuning the weight of each part separately.When searching the optimal weight of a part, theweight for other parts is assigned 0.
Second, theseapproximations of the optimum values are used asinitial weights prior to the second pass.
The finalweight distribution is obtained by searching for thebest weight combination around the initial values.The Figure 2 shows the optimal weight distri-butions along with the best relative MAP increasefor each part.
A noticeable improvement is ob-tained by increasing the weights associated to the ti-tle/introduction and conclusion of documents.
Thisis consistent with the results observed on the dis-tribution of PICO elements in abstracts.
Boostingmiddle parts of documents seems to have no impactat all.
We can see that the two ?p weight distribu-tions (1-pass and 2-pass) are very close.Performance measures obtained by model-1 arepresented in Table 3.
With 1-pass tuning, we ob-serve a MAP score increase of 37.5% and a P@10increase of 64.1%.
After the second pass, scores arelower with 35% and 60.5% for MAP and P@10 re-spectively.
This result indicates that there is possiblyoverfitting when we perform the two pass parametertuning.
It could also be caused by the limited num-ber of query in our test collection.
However, we candetermine reasonable weights by tuning each partweight separately.Experiments with model-2We have seen that a large improvement could comefrom weighting each part accordingly.
In a secondseries of experiments, we try to assign a different1020300.20.40.60.81.0Weight parameter ?pQ26A1-pass2-passTitle P1 P2 P3 P4 P5 P6 P7 P8 P9 P100102030MAPincrease (%)Different part of the documents0.00.20.40.60.81.0Q26B1-pass2-passFigure 2: Best MAP increase for each part p (bar charts),corresponding 1 and 2-pass ?p weights are also given.weight to each PICO element in queries.
A gridsearch was used to find the optimal ?e weights com-bination.
The results are shown in Table 3.We observe a MAP score increase of 22.5% andan increase of 11% in P@10.
Though the use ofa PICO weighting scheme increases the retrievalaccuracy, there is clearly much to gain by usingthe document structure.
The optimal [?p, ?i, ?c, ?o]weights distribution is [0.3, 1.2, 0, 0.1] for Qa and[0.2, 1, 0, 0.2] for Qb.
That means that the most im-portant words in queries belong to the Interventionelement.
This supports the manual search strategyproposed by (Weinfeld and Finkelstein, 2005), inwhich they suggested that I and P elements shouldbe used first to construct queries, and only if toomany results are obtained that other elements shouldbe considered.It is interesting to see that query words belongingto the Comparison element have to be consideredas the least important part of a query.
Even moreso because they are in the same semantic group asthe Intervention words.
A reason for that could bethe use of vague words such as ?no-intervention?
or?placebo?.
The methodology employed to constructthe queries is also responsible.
Indeed, physicianshave focused on producing alternative formulationsof 10 general clinical questions by predominantlymodifying the one of the PICO elements.
As a re-sult, some of them do share the same vague Com-parison words.828ExperimentsMAP P@10Qb?Qa Qa?Qb % Avg.
Qb?Qa Qa?Qb % Avg.Baseline 0.118 0.131 0.219 0.239Model-1 / 1pass 0.165 0.176 +37.5%?
0.377 0.373 +64.1%?Model-1 / 2pass 0.165 0.170 +35.0%?
0.354 0.381 +60.5%?Model-2 0.149 0.168 +22.5%?
0.250 0.258 +11.0%Model-1+2 0.198 0.202 +61.5%?
0.385 0.392 +70.0%?Table 3: Cross-validation (train?test) scores for the baseline (Kullback-Leibler divergence), model-1 with 1 and 2-pass tuning, model-2 and their combination (model-1+2).
Relative increase over the baseline is also given (averagedbetween Qa and Qb).
(?
: t.test < 0.01)Experiments with model-1+2We have seen that both the use of a location-basedweighting and a PICO-structure weighting schemeincrease the retrieval accuracy.
In this last series ofexperiments, we analyse the results of their com-bination.
We can observe that fusing model-1 andmodel-2 allows us to obtain the best retrieval ac-curacy with a MAP score increase of 61.5% and aP@10 increase of 70.0%.
It is a large improvementover the baseline as it means that instead of abouttwo relevant documents in the top 10, our systemcan retrieve nearly four.
These results confirm thatboth PICO framework and document structure canbe very helpful for the IR process.6 ConclusionWe presented a language modeling approach that in-tegrates document and PICO structure for the pur-pose of clinical IR.
A straightforward idea is to de-tect PICO elements in documents and use the ele-ments in the retrieval process.
However, this ap-proach does not work well because of the diffi-culty to arrive at a consistent tagging of these ele-ments.
Instead, we propose a less demanding ap-proach which assigns different weights to differentparts of a document.We first analysed the distribution of PICO el-ements in a manually annotated abstracts collec-tion.
The observed results led us to believe that alocation-based weighting scheme can be used in-stead of a PICO detection approach.
We then ex-plored whether this strategy can be used as an in-dicator to refine document relevance.
We also pro-posed a model to integrate the PICO informationprovided in queries and investigated how each el-ement should be balanced in the ranking function.On a data set composed of 1.5 million abstracts ex-tracted from PubMed, our method obtains an in-crease of 61.5% for MAP and 70% for P@10 overthe classical language modeling approach.This work can be much improved in the future.For example, the location-based weighting methodcan be improved in order to model a different weightdistribution for each PICO element.
As the distri-bution in abstracts is not the same among PICO el-ements, it is expected that differentiated weightingschemes could result in better retrieval effectiveness.In a similar perspective, we are continuing our ef-forts to construct a larger manually annotated col-lection of abstracts.
It will be thereafter conceiv-able to use this data to infer the structural weightingschemes or to train a more precise PICO detectionmethod.
The focused evaluation described in thispaper is a first step.
Although the queries are limitedto diabetes, this does not affect the general PICOstructure in queries.
We plan to extend the coverageof queries to other topics in the future.AcknowledgementsThe work described in this paper was funded bythe Social Sciences and Humanities Research Coun-cil (SSHRC).
The authors would like to thank Dr.Ann McKibbon, Dr. Dina Demner-Fushman, LorieKloda, Laura Shea, Lucas Baire and Lixin Shi fortheir contribution in the project.829ReferencesA.
Andrenucci.
2008.
Automated Question-AnsweringTechniques and the Medical Domain.
In InternationalConference on Health Informatics, volume 2, pages207?212.A.R.
Aronson.
2001.
Effective Mapping of BiomedicalText to the UMLS Metathesaurus: The MetaMap Pro-gram.
In AMIA Symposium.G.
Chung.
2009.
Sentence retrieval for abstracts of ran-domized controlled trials.
BMC Medical Informaticsand Decision Making, 9(1):10.M.
Dawes, P. Pluye, L. Shea, R. Grad, A. Green-berg, and J.Y.
Nie.
2007.
The identification ofclinically important elements within medical jour-nal abstracts: Patient-Population-Problem, Exposure-Intervention, Comparison, Outcome, Duration andResults (PECODR).
Informatics in Primary care,15(1):9?16.D.
Demner-Fushman and J. Lin.
2006.
Answer extrac-tion, semantic clustering, and extractive summariza-tion for clinical question answering.
In ACL.D.
Demner-Fushman and J. Lin.
2007.
Answeringclinical questions with knowledge-based and statisticaltechniques.
Computational Linguistics, 33(1):63?103.N.
Fuhr and K. Gro?johann.
2001.
XIRQL: A querylanguage for information retrieval in XML documents.In SIGIR, pages 172?180.M.J.
Hansen, N.O.
Rasmussen, and G. Chung.
2008.
Amethod of extracting the number of trial participantsfrom abstracts describing randomized controlled trials.Journal of Telemedicine and Telecare, 14(7):354?358.INEX.
2002-2009.
Proceedings of the INitiative for theEvaluation of XML Retrieval (INEX) workshop.J.
Kamps, M. Marx, M. de Rijke, and B. Sigurbjo?rnsson.2005.
Structured queries in XML retrieval.
In CIKM,pages 4?11.J.M.
Ponte and W.B.
Croft.
1998.
A language model-ing approach to information retrieval.
In SIGIR, pages275?281.T.C.
Rindflesch and M. Fiszman.
2003.
The interac-tion of domain knowledge and linguistic structure innatural language processing: interpreting hypernymicpropositions in biomedical text.
Journal of BiomedicalInformatics, 36(6):462?477.D.L.
Sackett, W. Rosenberg, J.A.
Gray, R.B.
Haynes, andW.S.
Richardson.
1996.
Evidence based medicine:what it is and what it isn?t.
British medical journal,312(7023):71.C.
Schardt, M. Adams, T. Owens, S. Keitz, andP.
Fontelo.
2007.
Utilization of the PICO frame-work to improve searching PubMed for clinical ques-tions.
BMC Medical Informatics and Decision Mak-ing, 7(1):16.L.B.
Sollaci and M.G.
Pereira.
2004.
The introduction,methods, results, and discussion (IMRAD) structure:a fifty-year survey.
Journal of the Medical LibraryAssociation, 92(3):364.A.
Trotman.
2005.
Choosing document structureweights.
Information Processing and Management,41(2):243?264.J.M.
Weinfeld and K. Finkelstein.
2005.
How to answeryour clinical questions more efficiently.
Family prac-tice management, 12(7):37.R.
Wilkinson.
1994.
Effective retrieval of structured doc-uments.
In SIGIR, pages 311?317.830
