Mapping Collocational Properties into Machine Learning FeaturesJ anyce  M.  Wiebet  and Kenneth  J .
McKeever t  and Rebecca  F.  Bruce$Department of Computer  Science and the Computing Research LaboratoryNew Mexico State UniversityLas Cruces, NM 88003e-mail: wiebe, kmckeeve@cs.nmsu.eduSDepartment of Computer  ScienceUniversity of North Carolina at AshevilleAsheville, NC 28804-3299e-mail: bruce@cs.unca.eduAbst rac tThis paper investigates interactions betweencollocational properties and methods for orga-nizing them into features for machine learn-ing.
In experiments performing an event cat-egorization task, Wiebe et al (1997a) foundthat different organizations are best for dif-ferent properties.
This paper presents a sta-tistical analysis of the results across differentmachine learning algorithms.
In the experi-ments, the relationship between property andorganization was strikingly consistent across al-gorithms.
This prompted further analysis ofthis relationship, and an investigation of cri-teria for recognizing beneficial ways to includecollocational properties in machine learning ex-periments.
While many types of collocationalproperties and methods of organizing them intofeatures have been used in NLP, systematic in-vestigations of their interaction are rare.1 In t roduct ionProperties can be mapped to features in amachine learning algorithm in different ways,potentially ielding different results (see, e.g.,Hu and Kibler 1996 and Pagallo and Haussler1990).
This paper investigates interactions be-tween collocational properties and methods fororganizing them into features.
Collocations,conceived broadly as words meeting certain con-straints that are correlated with the targetedclassification, are used in a wide range of NLPapplications, from word-sense disambiguationto discourse processing.
They must be selectedand represented in some way.
Thus, this workis widely applicable to experimental design inNLP.In experiments performing an event catego-rization task, Wiebe et al (1997a) co-variedfour types of organization and three types ofcollocational property.
They found that differ-ent organizations are best for different proper-ties, and that the best results are obtained withthe most constrained properties and an orga-nization that is not common in NLP (but seeGoldberg 1995 and Cohen 1996).
However, theyexperimented with only one machine learningalgorithm, and did not offer any insight into theresults.This paper presents a statistical analysis ofthe results across different machine learning al-gorithms.
In the experiments, the relationshipbetween property and organization is strikinglyconsistent across algorithms.
This promptedfurther analysis of this relationship, and a studyof criteria for recognizing beneficial ways to in-clude collocations in machine learning experi-ments.
While many types of collocational prop-erties and methods for representing them as fea-tures have been used in NLP, systematic inves-tigations of their interaction are rare.The paper is organized as follows.
The eventcategorization task is described in second 2.The collocational properties, methods for se-lecting collocations, and methods for organizingthem into features are presented in sections 3,4.1, and 4.2, respectively.
The machine learn-ing algorithms are identified in section 5, andthe results and statistical analysis of them arepresented in section 6.
The study of interactionbetween property and organization is presentedin section 7.2252 The  Event  Categor i za t ion  TaskThis work is part of a larger project on pro-cessing newspaper a ticles to support automaticsegmentation and summarization.
A funda-mental component of reporting is evidentiality(Chafe 1986, van Dijk 1988): What source doesthe reporter give for his information?
Is the in-formation being presented as fact, opinion, orspeculation?
Our end application is a segmen-tation of the text into factual and non-factualsegments, to include in a document profile forsummarization a d retrieval.
A prerequisite toanswering such questions i recognizing where inthe text speech events and private states (belief,opinions, perception) are presented.
That is theproblem addressed here.Specifically, the main state or event of eachsentence is classified into one of the followingevent categories:1. ps: clauses about private states.
"PhilipMorris hopes that by taking its Bill.
ofRights theme to the airwaves it will reachthe broadest possible audience."2.
se.ds: clauses presenting speech events inthe form of direct speech.
"I'm hopeful thatwe'll have further discussions," Mr. Hahnsaid.3.
se.ms: speech-event clauses that are mix-tures of direct and indirect speech.
"Thecompany s?id the fastener business 'hasbeen under severe cost pressures for sometime.'
"4. se.o: clauses presenting speech events inthe form of indirect speech, together withclauses about speech events that do notfall in the other speech-event categories.
"Stelco Inc. said it plans to shut downthree Toronto-area plants."5.
ps \[ event: private state and either a speechevent or other action.
"They were at oddsover the price."6.
other: clauses that are not included in anyof the other categories.
"The-fasteners,nuts and bolts, are sold to the North Amer-ican auto market.
"Speech events and private states are very :fre-quent in newspaper articles: 48% of the sen-tences in our corpus.
Note that the speechevent category isbroken into subcategories, co~:-responding to different styles.
The styles varyin the amount of paraphrase they admit, whichin turn strongly affects how the sentence can beintegrated into the surrounding discourse.
Weanticipate these distinctions to be important forfuture discourse segmentation tasks.This event categorization task is very chal-lenging.
The language used for speech eventsand private states is rich and varied.
Metaphorand idiom are widely used (Barnden 1992) andthere is a great deal of syntactic and partof speech variation.
The classification is alsohighly context dependent.
For example, a wordlike agree may simply refer to a belief, as in Heagrees that interest rates may go down, but mayalso refer to a specific speech event, as in Shesaid they should begin, and he agreed.
For an-other example, many words normally associatedwith non-verbal actions may refer directly tospeech events, if they appear in a strong speechcontext: e.g., attack, estimate, explore, guide,analyze, rise above, measure, etc.We developed etailed coding instructions formanual annotation of the data, and performedan inter-coder reliability study, including twoexpert and one naive annotator.
The results ofthe study, which will be reported elsewhere, arevery good.
The coding instructions, the anno-tations of the data, and the results of the studywill be made available on the project web site.The event categorization task is a challeng-ing test for the issues concerning collocationsaddressed in this paper.
However, it is impor-tant to note that these issues are relevant forany NLP task for which collocational informa-tion may be useful, including wordsense disam-biguation.3 Co l locat iona l  P roper t iesCollocations have been used extensively inwordsense disambiguation research.
In thatcontext, collocations are words that co-occurwith senses of the target word more often thanexpected by chance.
Collocations also usuallyinvolve some constraint(s).
For example, theconstraint might be that the word must appearimmediately to the right of the target word (see,for example, Ng & Lee 1996 and Bruce & Wiebe1994); the actual collocations would be wordsthat occur there.226We need to untie the notion of collocationfrom wordsense disambiguation, and considercollocations to be words that co-occur (morethan chance) with whatever classes are beingtargeted (such as the event categories presentedabove).
Viewed in this way, collocations are alsoimportant for many event categorization anddiscourse processing tasks.
Examples are open-class words that suggest dialog acts; words thathelp disambiguate cue words (e.g., is now be-ing used temporally, or as a discourse marker?
(Hirschberg & Litman 1993)); and words thatsuggest states versus events (Siegel 1997).The work reported here is relevant when thereare man) potential collocations to choose from,and we are automatically sifting through thevarious possibilities for good ones.
For word-sense disambiguation, many different words co-occur in the corpus with the target word; wewant to choose a subset that are good indica-tors of the sense of the target word.
For di-alog act recognition, we could search throughthe adjectives in the corpus, for example, forsome that suggest a rejection dialog act (e.g.,busy, occupied, committed, tied up, ...) in thescheduling domain (Wiebe et.
al 1997b)).
Fordisambiguating the cue phrase now, we couldsearch for words that prefer the temporal ver-sus the discourse interpretation (perhaps tem-poral adverbs and verbs with temporal aspectsof their meaning).
For event categorization, wecould sift through the main verbs to find thosethat are good indicators of speech, for example(say, demand, attack, concede,...).To aid discussion, we use the following for-mal definitions.
A collocational property is a setof constraints, P I  Pp.
In wordsense disam-biguation, for example, we might have an adja-cent collocational property, defined by four con-straints:P1 = being one word left of target word,P2 = being two words left of target word,P3 = being one word right of target word,P4 = being two words right of target word.words.
Finally, a collocation word is a poten-tial collocation word that is judged to be cor-related with the classification, according to ametric such as conditional probability, an infor-mation theoretic riterion, or a goodness-of-fittest.We allow properties to be divided into sub-properties.
That is, the set of constraintsdefining a property are divided into subsets,S1.. .Ss.
In our example, if s = 1, there isjust one undivided property, defined by the set(P1, P~, P3, P4}.
If s = p = 4, then there arefour subproperties, each defined by one of theconstraints.
Or, there might be two subproper-ties, 81 = (P1, P2}, corresponding to adjacentwords on the left, and $2 = (Pa, P4), corre-sponding to adjacent words on the right.
Be-cause these definitions cover many variations ina uniform framework, they facilitate compara-tive evaluation of systems implementing differ-ent schemes.The experiments performed here use colloca-tional properties defined in Wiebe et al 1997ato perform the event categorization task de-scribed in section 2.
For this and other appli-cations in which event type is important, suchas many information extraction, text catego-rization, and discourse processing tasks, highlydefinitive properties, i.e., properties that pin-point only the more relevant parts of the sen-tence, can lead to better performance.
We de-fine such a highly definitive collocational prop-erty.
Specifically, it is defined by a set of syn-tactic patterns that are regular expressions com-posed of parts of speech and root forms of words.The property is referred to as the SP colloca-tional property; it yields the best overall resultson our event categorization task, as shown laterin table 1.
A partial description of the SP prop-erty is the following (where NPapprox approx-imates a noun phrase):baseAdjPat = {a \] a is in the pattern(main_verb adv* a), where the main verb iscopular}.
E.g., "She is/seems happy"A potential collocation word is a word that sat-isfies one of the constraints.
Continuing the ex-ample, all of the words that appear in the cor-pus one or two words to the left or right of thetarget word are potential adjacent collocationcomplexAdjPat = {a I a is in the pattern(main_verb adv* \[ YPapprox \] \ ["to" \] adv* vadv* a), where v is copular} E.g., "It surprisedhim to actually be so happy.
"227Our SP property is organized into two subprop-erties (i.e., s is 2).
Recall that a subpropertyis defined by a set of constraints.
Our first SPsubproperty is defined by baseAdjPat and Com-plexAdjPat.
The potential collocation wordscorresponding to this subproperty are all adjec-tives that are used in either pattern in the cor-pus, and the actual collocation words are wordschosen from this set.
Our second SP subprop-erty is defined by two verb patterns not shownabove.
Given a clause, our system can applythe syntactic patterns fully automatically, us-ing regular expression matching techniques.The other collocational property, CO, was de-fined to contrast with the SP property becauseit is not highly definitive.
That is, it is definedby very loose constraints that do not inw~lvesyntactic patterns.
The two CO constraints weuse are simply adjective and verb, so that thepotential collocation words are all the adjec-tives and verbs appearing in the corpus (ignor-ing where they appear in the sentence).
In ourexperiments, each of these constraints i treatedas a subproperty (so, again, s is 2).4 Se lec t ing  Co l locat ions  andRepresent ing  them as FeaturesThe context of this work is automatic classifica-tion.
Suppose there is a training sample, whereeach tagged sentence is represented by a vector(F1,...,Fn_l,C).
The Fi's are input featuresand C is the targeted classification.
Our task isto induce a classifier that will predict he valueof C given an untagged sentence represented bythe Fi's.
This section addresses selecting collo-cations and representing them as such features.4.1 Selecting CollocationsFollowing are two methods for selecting collo-cation words of a given collocational property(Wiebe et al 1997a).
Assume there are cclasses, C1 ... Cc, and s subproperties, $1... Ss.4.1.1 Per-Class MethodIn the per-class method (also used by Ng andLee 1996), a set of words, WordsCiSj, is se-lected for each combination of Class Ci and sub-property Sj.
They are selected to be words that,when they satisfy a constraint in Sj, are corre-lated with class Ci.
Specifically: WordsCiS) ={W\[ P(Cilw satisfies a constraint in Sj) > k}.We use k = 0.5.
We experimented with someother values of k and other criteria, but did notfind any that consistently yield better results.A more thorough investigation is planned.4.1.2 Over-Range MethodIn the over-range method, a set of words,l?ordsSj, is selected for each subproperty Sj,such that, when they satisfy a constraint in Sj,they are correlated with the classification vari-able across the range of its values.Specifically, the model of independence b -tween each word w (when satisfying aconstraintin Sj) and the classification variable is assessed,using the likelihood ratio statistic, G 2 (Bishopet al 1975).
Those with the top N 6 2 val-ues, i.e., for which independence is a poor fit,are chosen 1.
For the purposes of comparison,we limit the number of words to the maximumnumber of features permitted by one of the MLpackages, 20 for ORe and 19 for ORb (ORe andORb are defined below.
)4.2 Organizat ionsFinally, the collocation words must be organizedinto features.
Following are two organizationsfor each selection method (Wiebe et al 1997a).4.2.1 Over -Range B inary  (ORb)This organization is commonly used in NLP, forexample by Gale et al 1992.
A binary featureis defined for each word in each set WordsSj,l< j<s .4.2.2 Over-Range Enumerated  (ORe)This organization is used by, for example, Ng &Lee 1996.
One feature is defined per subprop-erty Sj.
It has I WordsSj I + l  values, one valuefor each word in IVordsSj, corresponding to thepresence of that word.
Each feature also has avalue for the absence of any word in WordsSj.E.g., for both CO and SP collocations, thereis one feature for adjectives and one for verbs.The adjective feature has a value for each se-lected adjective, and a value for none of themoccurring.
(The verb feature is analogous.
)4.2.3 Per-Class B inary  (PCb)There is one binary feature for each class Ci,whose value is 1 if any member of any of the setsWordsCiSj appears in the sentence, 1 _< j < s.1Because all models have the same degrees of free-dom, ranking values based on the raw G 2 value is equiv-alent to rank based on a significance test.2284.2.4 Per-Class Enumerated  (PCe)For each subproperty Sj, a feature is definedwith c + 1 values as follows.
There is one valuefor each class Ci, corresponding to the presenceof a word in WordsCiSj.
Each feature also hasa value for the absence of any of those words.E.g., for both CO and SP collocations, there isone feature for adjectives and one for verbs.
Theadjective feature has one value for each class,corresponding to the presence of any of the ad-jectives chosen for that class; there is also avalue for the absence of any of them.
(The verbfeature is analogous.
)Note that, in the over-range organizations, in-creasing the number of words increases the com-plexity of the event space, in ORe by increas-ing the number of feature values and in ORbby increasing the number of features.
These in-creases in complexity can worsen accuracy andcomputation time (Goldberg 1995, Bruce etal.1996, Cohen 1996).
The per-class organizationsallow the number of collocation words to beincreased without a corresponding increase incomplexity.5 The  Mach ine  Learn ing  A lgor i thmsThe algorithms included in this study are rep-resentative of the major types suggested byMichie et al (1994) of the StatLog projectcomparing machine learning algorithms.
(1)PEBLS, a K-Nearest Neighbor algorithm (Costand Salzberg 1993); (2) C4.5, a decision tree al-gorithm (Quinlan 1994); (3) Ripper, an induc-tive rule based classifier (Cohen 1996); (4) theNaive Bayes classifier; and (5), a probabilisticmodel search procedure (Bruce & Wiebe 1994)using the public domain software CoCo (Bads-berg 1995).
Linear discriminant classifiers areomitted because they are not appropriate forcategorical data.
Neural network classifiers areomitted as well.6 Resu l tsFigure 1 presents the accuracy of~ach of themachine learning algorithms on each combina-tion of collocational property and feature or-ganization.
Table 1 shows the mean accuracyacross algorithms.
In addition to collocationalfeatures, all experiments included seven other(automatically determined) features, such asposition in the paragraph.
Two main modi-ORe ORb PCb PCeCO .690 .719 .584 .607SP .698 .710 .737 .746Table 1: Mean Accuracy Across Algorithmsfications of Wiebe et al (1997a) were made tofacilitate the comparisons at issue here.
First,nouns were originally included in the CO butnot the SP collocational property.
Here, theyare not included in either.
Second, a weaknessin the method for selecting the collocation sets ischanged so that, for each collocational property,the words in the sets WordsCiSj are identicalfor both per-class experiments.The data consists of 2,544 main clauses fromthe Wall Street Journal Treebank corpus (Mar-cus et al, 1993).
2 There are six classes, and thelower bound for the classification problem--thefrequency in the data set of the most frequentclass--is 52%.10-fold cross-validation was performed.
Allexperiments were independent, sothat, for eachfold, the collocations were determined and ruleinduction or model search, etc., was performedanew on the training set.We performed an analysis of variance to de-tect significant differences in accuracy consider-ing algorithm, collocational property, and fea-ture organization.
When there are, we per-formed post-hoc analyses (using Tukey's HSD,to control for multiple comparison error rates(SAS Institute 1989)) to identify the differences.The algorithms differ in accuracy, i.e., theanalysis hows there is a significant main effectof algorithm on accuracy (p < 0.0001).
Post-hoc analysis hows that there is only one signif-icant difference: the lower performance of PE-BLS relative to the others.However, the pattern of interaction betweenalgorithm and features is extremely consistentacross algorithms.
The analysis shows thatthere is no higher level interaction between algo-rithm, on the one hand, and collocational prop-~The Treebank syntax trees are used only to identifythe main clause.
This must be done only because theproblem is defined as classifying the main clause.229u 0.7" t~0.6-U0.5-0.4 ,  I ; , ; ;Bayes CoCo c4.5 PEBLS Ripper\ [ \ ]  CO-ORe\ [ \ ]  CO-ORb\ [ \ ]  CO-PCb\ [ \ ]  CO-PCe\ [ \ ]  SP-ORe\[\] SP-ORb\ [ \ ]  SP-PCb\ [ \ ]  SP-PCeFigure 1: Accuracy of Machine Learning Algorithms (means across folds)erty and organization, on the other (p > 0.996).That is, the relative effects of property andorganization on accuracy do not significantlychange from one algorithm to another.No attempt was made to tune the algorithmsfor performance (e.g., varying the number ofneighbors in the PEBLS experiments).
Thus,we do not take the results to be indicative of thequality of the algorithms.
Rather, the consistentpattern off results indicates that per-class orga-nization is beneficial or not depending mainlyon the collocational property.Further analysis, controlling for differencesacross algorithms, reveals a highly significantinteraction (P < 0.0001) between collocationalproperty and feature organization.
Post-hoccomparisons show that the best per-class exper-iment, SP-PCe, is significantly better than anyover-range xperiment, but is not significantlybetter than the other syntactic pattern/per-class experiment, SP-PCb.
In fact, we experi-mented (using the CoCo search algorithm) withper-class variations not presented in Wiebe etal.
(1997a), specifically with different sets ofsubproperties (e.g., PCe with s= 1).
There isno statistically significant difference among anyof the syntactic pattern/per-class experiments.In contrast, the co-occurrence/per-class ex-periments (CO-PCe and CO-PCb) are signifi-cantly worse than all the other experiments.Among the four over-range xperiments, theonly significant difference is between CO-ORband CO-ORe. As seen in table 2, a large numberof per-class collocation words appear only once(a consequence of the basic conditional proba-bility test we use).
We reran the per-class ex-periments (10-fold cross validation using CoCosearch), excluding collocation words that ap-pear only once in the training set.
There wereminiscule increases in the SP results (less than0.3%).
For the CO collocations, the PCb ex-periment increased by 3.15% and the PCe byless than 1%.
With these new results, the per-class/co-occurrence results are still much worsethan all the other experiments.7 Ana lys i sIn the previous section, we established thatthere is a highly significant interaction in theexperiments between collocational property andfeature organization, and that the pattern ofthis interaction is extremely consistent acrossthe algorithms.
In this section, the propertiesand organizations are analyzed in order to gaininsight into the pattern of results and developsome diagnostics for recognizing when the per-class organizations may be beneficial.
We con-sider a number of factors, including conflictingclass indicators, entropy, conditional probabil-ity, and event space complexity.As table 2 illustrates, the SP collocations areof much lower frequency, since they are moreconstrained.
Specifically, table 2 shows thenumber of occurrences in one training set of thecollocation words selected per-class.7.1 Confl icts in Per -C lass  Exper imentsThe main differences between CO and SP col-locations occur under the per-class organiza-tions.
These organizations appear to be vul-nerable to collocations that indicate conflictingclasses, since the collocation words are selectedto be those highly indicative of a particularclass.
Two words in the same sentence indicateconflicting classes if one is in a set WordsCjSiand the other is in a set WordsCkSt, and j ?
k.230"Frequency: > 50 41-50 31-40 21-30 11-20 6-10 3-5 2 1CO 3 5 6 25 57 130 396 213 1293SP 3 0 0 2 15 50 91 96 409Table 2: Frequency of Collocation Words Selected with the Per-Class MethodTable 3 shows that the CO collocations oftenconflict, while the SP collocations rarely do.This is true whether or not the collocations ap-pearing only once are included (shown on theleft versus the right side of the table).PCbPCeAll > 1CO SP" CO SP.4227 .1111 .3865 .0941.1852 .0139 .1495 .0039Table 3: Percentage of Sentences with Conflict-ing Collocations7.2 Measures o f  Feature QualityWe argue that, for the per-class organizations tobe beneficial, the individual collocation wordsmust strongly select a single majority class.Suppose that two words wl and w2 in the setWordsCc4Ss: select different classes as the sec-ond most probable class, with, say, conditionalprobabilities of .24 and .22, respectively.
In-formation concerning the second most probableclass is lost under the per-class grouping, eventhough the words are associated with anotherclass over 20% of the time.
If the conditionalprobability of the most strongly associated classwere higher for both words, the frequency of thesecondary association would be reduced, result-ing in fewer erroneous classifications.Two measures that can be used to assess howstrongly collocation words select single majorityclasses are entropy and conditional probabilityof class given feature.Quality of low frequency collocations i diffi-cult to measure.
For example, entropy tends tobe unreliable for low frequency features.
There-fore, table 4 shows statistics calculated for themore frequent words selected in common un-CO SPConditional Probability .6494 .7967Entropy .9362 .5541Table 4: Means for Collocations in Commonwith Frequency > 10der the SP and CO constraints in the trainingset of one fold of a per-class experiment.
The17 selected words all occur at least 10 timesunder each constraint in the training set used.Since an identical set of words is measured un-der both kinds of collocational property, the re-sults strongly reflect the quality of the proper-ties.The entropy of the conditional distribution ofthe class C given value f of feature F is:H ~ - ~ p(c \[ F = f )?log(p(c I F = f ) )ce{c~ ..... co}The first line of table 4 shows shows that, onaverage, the SP collocation words are morestrongly indicative of a single class.
The sec-ond line shows that, on average, SP collocationshave much lower entropy than the others.7.3 The Potent ia l  of Per-ClassOrganizations: more  in format ionwi thout  added complexi tyAs shown above in tables 2, 3, and 4, collocationwords of the more constrained SP property areof lower frequency and higher quality than theCO collocations.
Because the SP collocationsare low frequency, using them requires includinga larger number of collocations words.To assess the influence of the per-class orga-nizations when the number of collocation wordsis not increased, the following exercise was per-formed.
We took the collocation words that231were included in the original ORe experimentand organized them as PCe and similarly forORb and PCb, and reran the experiments (10-fold cross validation using CoCo search).
Whenthe features are so transformed, the accuracy isvirtually unchanged, as shown in table 5.CO SPOriginal ORe .6980 .7110ORe ~ PCe .7004 .7079Original ORb .7267 .7223ORb ---> PCb .7322 .7228Table 5: Accuracy with OR CollocationsMapped to PC CollocationsThe results suggest hat simply applying theper-class organizations to existing collocationswill not result in significant improvement.
Theimprovement we see when moving from theover-range to the per-class organizations of theSP collocations i largely due to inclusion of ad-ditional high quality collocations; the PC or-ganizations allow them to be included withoutadding complexity.Various methods have been proposed for re-ducing the complex feature space associatedwith large numbers of low frequency properties.For example, one can ignore infrequent collo-cations entirely (e.g., Ng & Lee), consider onlythe single best property (e.g., Yarowsky 1993),or ignore negative evidence, i.e., the absenceof a property (e.g., Hearst 1992).
Another isto retain the high quality collocations, group-ing them per-class.
Cohen (1996) and Goldberg(1995) propose similar methods for text catego-rization tasks, although they do not address thecomparative issues investigated here.8 Conc lus ionsWe performed extensive xperimentation inves-tigating the interactions among collocationalproperty, feature organization, and machinelearning algorithm.
We found a highly signifi-cant interaction between collocational propertyand feature organization, which is extremelyconsistent across the machine learning algo-rithms experimented with.
The results obtainedwith the per-class organization and the highly232definitive collocations (i.e., the SP collzcations)are significantly better than any experiment us-ing either" the lower quality collocations or theover-range organization.The per-class organizations allow us to takeadvantage of the lower frequency, higher qual-ity collocations; with the over-range organiza-tions, the results are no better than with thelower quality ones.
Our analysis shows, how-ever, that merely using a per-class organizationwith high-quality collocations i not sufficient orealize the potential benefits: a larger numberof collocations are needed for increased results.Very importantly, using the per-class orga-nizations with the lower quality collocationsproved costly--the results decreased by over10%.
Choices must be made in how colloca-tions are selected and organized in any event.
Amain lesson from these experiments i that in-appropriate organizations must be avoided forthe particular type of property at hand.In continuing work, we are investigating in-teractions with additional experimental param-eters.
The goals of this paper were to investi-gate issues relevant for many NLP applicationsin a uniform framework, and to shed some lighton interactions between collocational propertiesand how they are represented asfeatures in ma-chine learning algorithms.9 AcknowledgementsThis research was supported in part by theOffice of Naval Research under grant numberN00014-95-1-0776.
We thank Julie Maples forher work developing the annotation instructionsand manually annotating the data, and LeiDuan for his work implementing the original ex-periments.10 ReferencesBadsberg, J.
1995.
An Environment for Graph-ical Models.
Ph.D.
diss., Aalborg University.Bishop, Y. M.; Fienberg, S.; and Holland, P.1975.
Discrete Multivariate Analysis: Theoryand Practice.
(Cambridge: The MIT Press).Bruce, R.; Wiebe, J., and Pedersen, T. 1996.The measure of a model.
Proc.
EMNLP-1, pp.101-112.Bruce, R. and Wiebe, J.
1994.
Word-SenseDisambiguation Using Decomposable Models.Proc.
32nd Annual Meeting of the Assoc.
forComp.
Linguistics (ACL-94), pp.
139-146.Chafe, Wallace.
1986 Evidentiality in En-glish Conversation and Academic Writing.
In:Chafe, Wallace and Nichols, Johanna, Eds., Ev-identiality: The Linguistic Coding of Epistemol-ogy.
Ablex, ~Norwood, NJ: 261-272.Cohen, W. 1996.
Learning Trees and Rules withSet-Valued Features.
Proc.
AAAI-96, pp.
709--717..~.
-Cost ,  S. and Salzberg, S. 1993.
A WeightedNearest Neighbor Algorithm for Learning withSymbolic Features, Machine Learning 10 (1):57-78.van Dijk, T.A.
(1988).
News as Discourse.
(Hillsdale, N J: Lawrence Erlbaum).Gale, W.; Church, K.; and Yarowsky, D. 1992.A Method for Disambiguating Word Senses ina Large Corpus.
AT&T Bell Laboratories Sta-tistical Research Report No.
104.Goldberg, J. H. 1995.
CDM: An Approach toLearning in Text Categorization.
Proc.
IEEEInternational Conference on Tools with AI, pp.258-265.Hearst, M. (1992).
Automatic acquisition ofhyponyms from large text corpora.
Proc.COLING-92.Hirschberg, J. and Litman, D. (1993).
Em-pirical studies on the disambiguation of cuephrases.
Computational Linguistics 19, 3, 501-530.Hu, Y.J.
and Kibler, D. 1996.
Generation of At-tributes for Learning Algorithms.
Proc.
AAAI-96, pp.
806-811.Marcus, M.; Santorini, B.; and Marcinkiewicz,M.
1993.
Building a Large Annotated Corpusof English: The Penn Treebank.
ComputationalLinguistics I9 (2): 313-330.Michie, D.; Spiegelhalter, D.J.
;, and Taylor,C.C.
1994.
Machine Learning, Neural and Sta-233tistical Classification (NY: Ellis Horwood).Ng, H., and Lee, H. 1996.
Integrating Multi-ple Knowledge Sources to Disambiguate WordSenses: An Exemplar-Based Approach.
Proc.ACL-96, pp.
40--47.Pagallo, G. and Haussler, D. 1990.
Boolean Fea-ture Discovery in Empirical Learning.
MachineLearning, 5: 71-99.Quinian, J. R. 1994.
C4.5: Programs for Ma-chine Learning (San Mateo: Morgan Kaufman).SAS Institute Inc. 1989.
SAS/STAT User'sGuide, Version 6, Fourth Edition, Volume 2.Cary, NC: SAS Institute Inc).Siegel, E. (1997).
Learning methods for combin-ing linguistic indicators to classify verbs.
Proc.2nd Conference on Empirical Methods in Natu-ral Language Processing (EMNLP-2), pp.
156-162.Wiebe, J.; Bruce, R.; and Duan, L. 1997a.Probabilistic Event Categorization.
Proc.
Con-ference on Recent Advances in Natural Lan-guage Processing (RANLP-97), pp.
163-170.European Commission, DG XIII.Wiebe, Janyc%.
O'Hara, Tom, McKeever,Kenneth, and OhrstrSm-Sandgren, Thorsten.1997b.
An empirical approach to temporal ref-erence resolution.
In Proc.
2nd Conference onEmpirical Methods in Natural Language Pro-cessing (EMNLP-2), pp.
174-186.Yarowsky, D. 1993.
One Sense Per Colloca-tion.
Proc.
1993 Speech and Natural LanguageARPA Workshop.
