First Joint Conference on Lexical and Computational Semantics (*SEM), pages 237?245,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsA Probabilistic Lexical Model for Ranking Textual InferencesEyal Shnarch and Ido DaganComputer Science DepartmentBar-Ilan UniversityRamat-Gan 52900, Israel{shey,dagan}@cs.biu.ac.ilJacob GoldbergerFaculty of EngineeringBar-Ilan UniversityRamat-Gan 52900, Israelgoldbej@eng.biu.ac.ilAbstractIdentifying textual inferences, where themeaning of one text follows from another, isa general underlying task within many natu-ral language applications.
Commonly, it is ap-proached either by generative syntactic-basedmethods or by ?lightweight?
heuristic lexicalmodels.
We suggest a model which is confinedto simple lexical information, but is formu-lated as a principled generative probabilisticmodel.
We focus our attention on the task ofranking textual inferences and show substan-tially improved results on a recently investi-gated question answering data set.1 IntroductionThe task of identifying texts which share semanticcontent arises as a general need in many natural lan-guage processing applications.
For instance, a para-phrasing application has to recognize texts whichconvey roughly the same content, and a summariza-tion application needs to single out texts which con-tain the content stated by other texts.
We refer to thisgeneral task as textual inference similar to prior useof this term (Raina et al, 2005; Schoenmackers etal., 2008; Haghighi et al, 2005).In many textual inference scenarios the setting re-quires a classification decision of whether the infer-ence relation holds or not.
But in other scenariosranking according to inference likelihood would bethe natural task.
In this work we focus on rankingtextual inferences; given a sentence and a corpus,the task is to rank the corpus passages by their plau-sibility to imply as much of the sentence meaning aspossible.
Most naturally, this is the case in questionanswering (QA), where systems search for passagesthat cover the semantic components of the question.A recent line of research was dedicated to this task(Wang et al, 2007; Heilman and Smith, 2010; Wangand Manning, 2010).A related scenario is the task of Recognizing Tex-tual Entailment (RTE) within a corpus (Bentivogliet al, 2010)1.
In this task, inference systems shouldidentify, for a given hypothesis, the sentences whichentail it in a given corpus.
Even though RTE waspresented as a classification task, it has an appeal-ing potential as a ranking task as well.
For instance,one may want to find texts that validate a claim suchas cellular radiation is dangerous for children, or tolearn more about it from a newswire corpus.
To thatend, one should look for additional mentions of thisclaim such as extensive usage of cell phones may beharmful for youngsters.
This can be done by rank-ing the corpus passages by their likelihood to entailthe claim, where the top ranked passages are likelyto contain additional relevant information.Two main approaches have been used to addresstextual inference (for either ranking or classifica-tion).
One is based on transformations over syntac-tic parse trees (Echihabi and Marcu, 2003; Heilmanand Smith, 2010).
Some works in this line describea probabilistic generative process in which the parsetree of the question is generated from the passage(Wang et al, 2007; Wang and Manning, 2010).In the second approach, lexical models have beenemployed for textual inference (MacKinlay andBaldwin, 2009; Clark and Harrison, 2010).
Typi-1http://www.nist.gov/tac/2010/RTE/index.html237cally, lexical models consider a text fragment as abag of terms and split the inference decision intotwo steps.
The first is a term-level estimation of theinference likelihood for each term independently,based on direct lexical match and on lexical knowl-edge resources.
Some commonly used resources areWordNet (Fellbaum, 1998), distributional-similaritythesauri (Lin, 1998), and web knowledge resourcessuch as (Suchanek et al, 2007).
The second stepis making a final sentence-level decision based onthese estimations for the component terms.
Lex-ical models have the advantage of being fast andeasy to utilize (e.g.
no dependency on parsing tools)while being highly competitive with top performingsystems, e.g.
the system of Majumdar and Bhat-tacharyya (2010).In this work, we investigate how well such lexi-cal models can perform in textual inference rankingscenarios.
However, while lexical models usuallyapply heuristic methods, we would like to pursue aprincipled learning-based generative framework, inanalogy to the approaches for syntactic-based infer-ence.
An attractive work in this spirit is presented in(Shnarch et al, 2011a), that propose a model whichis both lexical and probabilistic.
Later, Shnarch etal.
(2011b) improved this model and reported re-sults that outperformed previous lexical models andwere on par with state-of-the-art RTE models.Whereas their term-level model provides meansto integrate lexical knowledge in a probabilisticmanner, their sentence-level model depends to agreat extent on heuristic normalizations which wereintroduced to incorporate prominent aspects of thesentence-level decision.
This deviates their modelfrom a pure probabilistic methodology.Our work aims at amending this deficiency andproposes a new probabilistic sentence-level modelbased on a Markovian process.
In that model, allparameters are estimated by an EM algorithm.
Weevaluate this model on the tasks of ranking passagesfor QA and ranking textual entailments within a cor-pus, and show that eliminating the need for heuris-tic normalizations greatly improves state-of-the-artperformance.
The full implementation of our modelis available for download2 and can be used as aneasy-to-install and highly competitive inference en-2http://www.cs.biu.ac.il/?nlp/downloads/probLexModel.htmlgine that operates only on lexical knowledge, or as alexical component integrated within a more complexinference system.2 BackgroundWang et al (2007) provided an annotated data set,based on the Text REtrieval Conference (TREC) QAtracks3, specifically for the task of ranking candidateanswer passages.
We adopt their experimental setupand next review the line of syntactic-based workswhich reported results on this data set.2.1 Syntactic generative modelsWang et al (2007) propose a quasi-synchronousgrammar formulation which specifies the generationof the question parse tree, loosely conditioned on theparse tree of the candidate answer passage.
Theirmodel showed improvement over previous syntac-tic models for QA: Punyakanok et al (2004), whocomputed similarity between question-answer pairswith a generalized tree-edit distance, and Cui et al(2005), who developed an information measure forsentence similarity based on dependency paths ofaligned words.
Wang et al (2007) reproduced thesemethods and extended them to utilize WordNet.More recently, Heilman and Smith (2010) im-proved Wang et al (2007) results with a classifica-tion based approach.
Feature for the classifier wereextracted from a greedy algorithm which searchesfor tree-edit sequences which transform the parsetree of the candidate answer into the one of the ques-tion.
Unlike other works reviewed here, this onedoes not utilize lexical knowledge resources.Similarly, Wang and Manning (2010) present anextended tree-edit operations set and search for editsequences to generate the question from the answercandidate.
Their CRF-based classifier models thesesequences as latent variables.An important merit of these methods is that theyoffer principled, often probabilistic, generative mod-els for the task of ranking candidate answers.
Theirdrawback is the need for syntactic analysis whichmakes them slower to run, dependent on parsing per-formance, which is often mediocre in many text gen-res, and inadequate for languages which lack properparsing tools.3http://trec.nist.gov/data/qamain.html2382.2 Lexical modelsLexical models, on the other hand, are faster, eas-ier to implement and are more practical for vari-ous genres and languages.
Such models derive fromknowledge resources lexical inference rules whichindicate that the meaning of a lexical term can beinferred from the meaning of another term (e.g.youngsters?
children and harmful?
dangerous).They are common in the Recognizing Textual En-tailment (RTE) systems and we present some rep-resentative methods for that task.
We adopt textualentailment terminology and henceforth use Hypoth-esis (denoted H) for the inferred text fragment andText (denoted T ) for the text from which it is beinginferred4.Majumdar and Bhattacharyya (2010) utilized asimple union of lexical rules derived from vari-ous lexical resources for the term-level step.
Theyderived their sentence-level decision based on thenumber of matched hypothesis terms.
The resultsof this simple model were only slightly worse thanthe best results of the RTE-6 challenge which wereachieved by a syntactic-based system (Jia et al,2010).
Clark and Harrison (2010), on the other hand,considered the number of mismatched terms in es-tablishing their sentence-level decision.
MacKinlayand Baldwin (2009) represented text and hypothe-sis as word vectors augmented with lexical knowl-edge.
For sentence-level similarity they used a vari-ant of the cosine similarity score.
Common to mostof these lexical models is the application of heuris-tic methods in both the term and the sentence levelsteps.Targeted to replace heuristic methods with princi-pled ones, Shnarch et al (2011a) present a modelwhich aims at combining the advantages of a proba-bilistic generative model with the simplicity of lex-ical methods.
In some analogy to generative parse-tree based models, they propose a generative processfor the creation of the hypothesis from the text.At the term-level, their model combines knowl-edge from various input resources and has the ad-vantages of considering the effect of transitive ruleapplication (e.g.
mobile phone?
cell phone?
cel-lular) as well as the integration of multiple pieces4In the task of passage ranking for QA, the hypothesis is thequestion and the text is the candidate passage.of evidence for the inference of a term (e.g.
boththe appearance of harmful and risky in T provideevidence for the inference of dangerous in H).
Wedenote this term-level Probabilistic Lexical Modelas PLMTL, and have reproduced it in our work aspresented in Section 4.1.
For the sentence-level de-cision they describe an AND gate mechanism, i.e.deducing a positive inference decision for H as awhole only if all its terms were inferred from T .In an extension to that work, Shnarch et al(2011b) modified PLMTL to improve the sentence-level step.
They pointed out some prominent aspectsfor the sentence-level decision.
First, they suggestthat a hypothesis as a whole can be inferred fromthe text even if some of its terms are not inferred.To model this, they introduced a noisy-AND mech-anism (Pearl, 1988).
Additionally, they emphasizedthe effect of hypothesis length and the dependencybetween terms on the sentence-level decision.
How-ever, they did not fully achieve their target of pre-senting a fully coherent probabilistic model, as theirmodel included heuristic normalization formulae.On the contrary, the model we present is the firstalong this line to be fully specified in terms of agenerative setting and formulated in pure probabilis-tic terms.
We introduce a Markovian-style proba-bilistic model for the sentence-level decision.
Thismodel receives as input term-level probabilistic es-timates, which may be provided by any term-levelmodel.
In our implementation we embed PLMTL asthe term-level model and present a complete coher-ent Markovian-based Probabilistic Lexical Model,which we term M-PLM.3 Markovian sentence-level modelThe goal of a sentence-level model is to integrateterm-level inputs into an inference decision for thehypothesis as a whole.
For a hypothesis H =h1, .
.
.
, hn and a text T , term-level models first esti-mate independently for each term ht its probabilityto be inferred from T .
Let xt be a binary randomvariable representing the event that ht is indeed in-ferred from T (i.e., xt = 1 if ht is inferred and 0otherwise).Given these term-level probabilities, a sentence-level model is employed to estimate the probabilitythat H as a whole is inferred from T .
This step is239term-levelsentence-levelText: Hypo:t 1t m?h 1h 2h n?x 1x 2x n?y 1y 2y n?Figure 1: A probabilistic lexical model: the upper part is theterm-level input to the sentence-level Markovian process, de-picted in the lower part.
xi is a binary variable representing theinference of hi and yj is a variable for the accumulative infer-ence decision for the first j terms of Hypo.
The final sentence-level decision is given by yn.the focus of our work.
We assume that the term-level probabilities are given as input.
Section 4.1describes PLMTL, as a concrete method for derivingthese probabilities.Our sentence-level model is based on a Marko-vian process and is described in Section 3.1.
In par-ticular, it takes into account, in probabilistic terms,the prominent factors in lexical entailment, men-tioned in Section 2.
An efficient inference algorithmfor our model is given in Section 3.2 and EM-basedlearning is specified in Section 3.3.3.1 Markovian sentence-level decisionThe motivation for proposing a Markovian processfor the sentence-level is to establish an intermedi-ate model, lying between two extremes: assumingfull independence between hypothesis terms versusassuming that every term is dependent on all otherterms.
The former alternative is too weak, whilethe latter alternative is computationally hard andnot very informative, and thus hard to capture ina model.
Our model specifies a Markovian depen-dence structure, which limits the dependence scopeto adjacent terms, as follows.We define a binary variable yt to be the accumu-lated sentence-level inference decision up to ht.
Inother words, yt=1 if the subset {h1, .
.
.
, ht} of H?sterms is inferred as a whole from T .Note that this means that yt can be 1 even if someterms amongst h1, .
.
.
, ht are not inferred.
As yn isthe decision for the complete hypothesis, our modeladdresses this way the prominent aspect that the hy-pothesis as a whole may be inferred even if some ofits terms are not inferred.
The reason for allowingthis is that such un-inferred terms may be inferredfrom the global context of T , or alternatively, are ac-tually inferred from T but the knowledge resourcesin use do not contain the proper lexical rule to makesuch inference.Figure 1 describes both steps of a full lexical in-ference model.
Its lower part depicts our Markovianprocess.
In the proposed model the inference deci-sion at each position t is a combination of xt, thevariable for the event of ht being inferred, and yt?1,the accumulated decision at the previous position.Therefore, the transition parameters of M-PLM canbe modeled as:qij(k)=P (yt=k|yt?1 = i, xt=j) ?k, i, j?
{0, 1}where y1=x1.
For instance, q01(1) is the probabilitythat yt=1, given that yt?1 =0 and xt=1.Applying the Markovian process on the entirehypothesis we get yn, which represents the finalsentence-level decision, where a soft decision is ob-tained by computing the probability of yn=1:P (yn=1) =?x1, ..., xny2, ..., yn?1, yn=1P (x1)n?t=2P (xt)P (yt|yt?1, xt)The summation is done over all possible binaryvalues of the term-level variables x1, ..., xn and theaccumulated sentence-level variables y2, ..., yn?1where yn=1.
Note that for clarity, in this formula xtand yt denote the binary values at the correspondingvariable positions.
A tractable form for computingP (yn=1) is presented in Section 3.2.Overall, the prominent factors in lexical entail-ment, raised by prior works, are incorporated withinthe core structure of this probabilistic model, with-out the need to resort to heuristic normalizations.Reducing the negative affect of hypothesis length onthe entailment probability is achieved by having yt,at each position, being directly dependent only on xtand yt?1 as opposed to being affected by all hypoth-esis terms.
The second factor, modeling the depen-dency between hypothesis terms, is addressed by the240indirect dependency of yn on all preceding hypothe-sis terms.
This dependency arises from the recursivenature of the Markovian model, as can be seen in thenext section.Our proposed Markovian process presents a lineardependency between terms which, to some extent,poses an anomaly with respect to the structure of theentailment phenomenon.
Yet, as we do want to limitthe dependence structure, following the natural or-der of the sentence words seems the most reasonablechoice, as common in many other types of sequentialmodels.
We also tried randomizing the word orderwhich, on average, did not improve performance.3.2 InferenceThe accumulated sentence-level inference can beefficiently computed using a typical forward algo-rithm.
We denote the probability of xt=j, j?
{0, 1}by ht(j) = P (xt = j).
The forward step is given inEq.
(1) and its initialization is defined in Eq.
(2).
?t(k) = P (yt=k)=?i,j?
{0,1}?t?1(i)ht(j)qij(k) (1)?1(k) = P (x1 =k) (2)where k?
{0, 1} and t = 2, ..., n.?t(k) is the probability that the accumulated de-cision at position t is k. It is calculated by sum-ming over the probabilities of all four combinationsof ?t?1(i) and ht(j), multiplied by the correspond-ing transition probability, qij(k).The soft sentence-level decision can be efficientlycalculated by:P (yn=a) = ?n(a) a?
{0, 1} (3)3.3 LearningTypically, natural language applications work at thesentence-level.
The training data for such applica-tions is, therefore, available as annotations at thesentence-level.
Term-level alignments between pas-sage terms and question terms are rarely available.Hence, we learn our term-level parameters fromavailable sentence-level annotations, using the gen-erative process described above to bridge the gap be-tween these two levels.For learning we use the typical backwards algo-rithm which is described by Eq.
(4) and Eq.
(5),where ?t(a|i) is the probability that the full hypoth-esis inference value is a given that yt= i.?n(a|i) = P (yn=a|yn= i) = 1{a=i} (4)?t(a|i) = P (yn=a|yt= i) ==?j,k?
{0,1}ht+1(j)qij(k)?t+1(a|k) (5)where t = n?1, .., 1, a ?
{0, 1} and 1{condition} isthe indicator function which returns 1 if conditionholds and 0 otherwise.To estimate qij(k), the parameters of the Marko-vian process, we employ the EM algorithm:E-step: For each (T,H) pair in the trainingdata set, annotated with a ?
{0, 1} as its sentence-level inference value, we evaluate the expectedprobability of every transition given the annotationvalue a:wtijk(T,H) = P (yt?1 = i, xt=j, yt=k|yn=a)=?t?1(i)ht(j)qij(k)?t(a|k)P (yn=a)(6)?i, j, k?
{0, 1} and t = 2, ..., |H|.M-step: Given the values of wtijk(T,H) wecan estimate each qij(1), i, j?
{0, 1}, by taking theproportion of transitions in which yt?1 = i, xt = jand yt = 1, out of the total transitions in whichyt?1 = i and xt=j:qij(1)??(T,H)?|H|t=2wtij1(T,H)?(T,H)?|H|t=2?k?
{0,1}wtijk(T,H)(7)qij(0) = 1?qij(1)4 Complete model implementationWe next describe the end-to-end probabilistic lexicalinference model we used in our evaluations.
We im-plemented PLMTL as our term-level model to pro-vide us with ht(j), the term-level probabilities.
Wechose this model since it is fully lexical, has the ad-vantages of lexical knowledge integration describedin Section 2 and achieved top results on RTE datasets.
Next, we summarize PLMTL, and in AppendixA we show how to adjust the learning schema to fitinto our sentence-level model.2414.1 PLMTLShnarch et al (2011a) provide a term-level modelwhich integrates lexical rules from various knowl-edge resources.
As described below it also consid-ers transitive chains of rule applications as well asthe impact of parallel chains which provide multipleevidence that h?H is inferred from T .Their model assumes a parameter ?R for eachknowledge resource R in use.
?R specifies the re-source?s reliability, i.e.
the prior probability that ap-plying a rule from R to an arbitrary text-hypothesispair would yield a valid inference.Next, transitive chains may connect a text term toa hypothesis term via intermediate term(s).
For in-stance, starting from the text term T-Mobile, a chainthat utilizes the lexical rules T-Mobile?
telecomand telecom?
cell phone enables the inference ofthe term cell phone from T .
They compute, for eachstep in a chain, the probability that this step is validbased on the ?R values.
Denoting the resource whichprovided a rule r by R(r), Eq.
(8) specifies that thevalidity probability of the inference step correspond-ing to the application of the rule r within the chain cpointing at ht (as represented by xtcr) is ?R(r).Next, for a chain c pointing at ht (represented byxtc) to be valid, all its rule steps should be valid forthis pair.
Eq.
(9) estimates this probability by thejoint probability that the applications of all rules r?c are valid, assuming independence of rules.Several chains may connect terms in T to ht, thusproviding multiple pieces of evidence that ht is in-ferred from T .
For instance, both youngsters andkids in T may indicate the inference of children inH .
For a term ht to be inferred from the entire sen-tence T it is enough that at least one of the chainsfrom T to ht is valid.
This is the complement eventof ht not being inferred from T which happens whenall chains which suggest the inference of ht, denotedby C(ht), are invalid.
Eq.
(10) specifies this proba-bility (again assuming independence of chains).P (xtcr = 1) = ?R(r) (8)P (xtc = 1) =?r?cP (xtcr = 1) (9)ht(1) = P (xt = 1) = 1?P (xt = 0) (10)= 1?
?c?C(ht)P (xtc = 0)With respect to the contributions of our work, wenote that previous works resorted to applying someheuristic amendments on these equations to achievevaluable results.
In contrast, our work is the firstto present a purely generative model.
This achieve-ment shows that it is possible to shift from ad-hocheuristic methods, which are common practice, tomore solid mathematically-based methods.Finally, for ranking text passages from a corpusfor a given hypothesis (question in the QA scenario),our Markovian sentence-level model takes as its in-put the outcome of Eq.
(10) for each ht ?
H .
ForPLMTL we need to estimate the model parameters,that is the various ?R values.
In our Markovianmodel this is done by the scheme detailed in Ap-pendix A.
Given these term-level probabilities, ourmodel computes for each hypothesis its probabil-ity to be inferred from each of the corpus passages,namely P (yn = 1) in Eq (3).
Passages are thenranked according to this probability.5 Evaluations and ResultsTo evaluate the performance of M-PLM for rankingtextual inferences we focused on the task of rankingcandidate answer passages for question answering(QA) as presented in Section 5.1.
Additionally, wedemonstrate the added value of our sentence-levelmodel in another ranking experiment based on RTEdata sets, described in Section 5.2.5.1 Answer ranking for question answeringData set We adopted the experimental setup ofWang et al (2007) who also provided an annotateddata set for answer passage ranking in QA5.In their data set an instance is a pair of a factoidquestion and a candidate answer passage (a singlesentence in this data set).
It was constructed from thedata of the QA tracks at TREC 8?13.
The question-candidate pairs were manually judged and a pair wasannotated as positive if the candidate passage indi-cates the correct answer for the question.
The train-ing and test sets roughly contain 5700 and 1500 pairscorrespondingly.5The data set was kindly provided to us byMengqiu Wang and is available for download athttp://www.cs.stanford.edu/?mengqiu/data/qg-emnlp07-data.tgz.242Method PLMTL utilizes WordNet and the Catvar(Categorial Variation) derivations database (Habashand Dorr, 2003) as generic and publicly availablelexical knowledge resources, when question andanswer terms are restricted to the first WordNetsense.
In order to be consistent with (Shnarch et al,2011b), the best performing model of prior work,we restricted our model to utilize only these two re-sources which they used.
However, additional lexi-cal resources can be provided as input to our model(e.g.
a distributional similarity-base thesaurus).We report Mean Average Precision (MAP) andMean Reciprocal Rank (MRR), the standard mea-sures for ranked lists.
In the cases of tie we tooka conservative approach and ranked positive anno-tated instances below the negative instances scoredwith the same probability.
Hence, the reported fig-ures are lower-bounds for any tie-breaking methodthat could have been applied.Results We compared our model to all 5 mod-els evaluated for this data set, described in Sec-tion 2, and to our own implementation of (Shnarchet al, 2011b).
We term this model Heuristically-Normalized Probabilistic Lexical Model, HN-PLM,since it modifies PLMTL by introducing heuristicnormalization formulae.
As explained earlier, bothM-PLM and HN-PLM embed PLMTL in their im-plementation but they differ in their sentence-levelmodel.
In our implementation of both models,PLMTL applies chains of transitive rule applicationswhose maximal length is 3.As seen in Table 1, M-PLM outperforms all priormodels by a large margin.
A comparison of M-PLMand HN-PLM reveals the major positive effect ofchoosing the Markovian process for the sentence-level decision.
By avoiding heuristically-normalizedformulae and having all our parameters being part ofthe Markovian model, we managed to increases bothMAP and MRR by nearly 2.5%6.Ablation Test As an additional examination ofthe impact of the Markovian process components,we evaluated the contribution of having 4 transitionparameters.
The AND-logic applied by (Shnarch et6The difference is not significant according to the Wilcoxontest, however we note that given the data set size it is hard to geta significant difference and that both Heilman and Smith (2010)and Wang and Manning (2010) improvements over the resultsof Wang et al (2007) were not statistically significant.System MAP MRRPunyakanok et al 41.89 49.39Cui et al 43.50 55.69Wang & Manning 59.51 69.51Wang et al 60.29 68.52Heilman & Smith 60.91 69.17Shnarch et al HN-PLM 61.89 70.24M-PLM 64.38 72.69Table 1: Results (in %) for the task of answer ranking forquestion answering (sorted by MAP).al., 2011a) to their sentence-level decision roughlycorresponds to 2 of the Markovian parameters.
Abinary AND outputs 1 if both its inputs are 1.
Thiscorresponds to q11(1) which is indeed estimate to benear 1.
In any other case an AND gate outputs 0.This corresponds to q00(1) which was estimated tobe near zero.The two parameters q01 and q10 are novel to theMarkovian process and do not have counterparts in(Shnarch et al, 2011a).
These parameters are thecases in which the sentence-level decision accumu-lated so far and the term-level decision do not agree.Introducing these 2 parameters enables our model toprovide a positive decision for the hypothesis as awhole (or for a part of it) even if some of its termswere not inferred.
We performed an ablation test oneach of these two parameters by forcing the value ofthe ablated parameter to be zero.
The notable perfor-mance drop presented in Table 2 indicates the crucialcontribution of these parameters to our model.Ablated parameter ?
MAP ?
MRRq01(1) = 0 -2.61 -4.91q10(1) = 0 -2.12 -2.86Table 2: Ablation test for the novel parameters of the Marko-vian process.
Results (in %) indicate performance drop whenforcing a parameter to be zero.5.2 RTE evaluationsTo assess the added value of our model on an addi-tional ranking evaluation, we utilize the search taskdata sets of the recent Recognizing Textual Entail-ment (RTE) benchmarks (Bentivogli et al, 2009;Bentivogli et al, 2010), which were originally con-243structed for the task of entailment classification.
Inthat task a hypothesis is given with a corpus and thegoal is to identify which sentences of the corpus en-tail the hypothesis.
This setting naturally lends itselfto a ranking scenario, in which the desired output isa list of the corpus sentences ranked by their proba-bility to entail the given hypothesis.To that end, we employed the same method-ology as described in the previous section.
Ta-ble 3 presents the improvement of our model overHN-PLM, whose classification performance was re-ported to be on par with best-performing systems onthese data sets7.
As can be seen, the improvementis substantial for both measures on both data sets.These results further assess the contribution of ourMarkovian sentence-level model.RTE-5 RTE-6MAP MRR MAP MRRHN-PLM 58.0 82.9 54.0 71.9M-PLM 61.6 84.8 60.0 79.2?
+3.6 +1.9 +6.0 +7.3Table 3: Improvements of our sentence-level model overHN-PLM.
Results (in %) are shown for the last RTE andfor the search task in RTE-5.6 DiscussionThis paper investigated probabilistic lexical mod-els for ranking textual inferences focusing on pas-sage ranking for QA.
We showed that our coher-ent probabilistic model, whose sentence-level modelis based on a Markovian process, considerably im-proves five prior syntactic-based models as well asa heuristically-normalized lexical model.
Therefore,it raises the baseline for future methods.In future work we would like to further explorea broader range of related probabilistic models.
Es-pecially, as our Markovian process is dependent onterm order, it would be interesting to investigatemodels which are not order dependent.Initial experiments on the classification task showthat M-PLM performs well above the average sys-tem but below HN-PLM, since it does not normalize7RTE data sets were only used for the classification taskso far, therefore there are no state-of-the-art results to comparewith, when utilizing them for the ranking task.the estimated probability well across hypothesis.
Wetherefore suggest a future work on better classifica-tion models.Finally, we view this work as joining a line of re-search which develops principled probabilistic mod-els for the task of textual inference and demonstratestheir superiority over heuristic methods.A Appendix: Adaptation of PLMTLlearningM-PLM embeds PLMTL as its term-level model.PLMTL introduces ?R values as additional parame-ters for the complete model.
We show how we mod-ify (Shnarch et al, 2011a) E-step formula to fit ourMarkovian modeling, described in Section 3.1.
TheM-step formula remains exactly the same.Eq.
(11) estimates the a-posteriori validity prob-ability of a single application of the rule r in thetransitive chain c pointing at ht, given that the an-notation of the pair is a.wtcr(T,H) = P (xtcr = 1|yn = a) =(11)?i,j,k?
{0,1} ?t?1(i)P (xt=j|xtcr =1)?R(r)qij(k)?t(a|k)P (yn = a)where t=2 .
.
.
n and P (xt =j|xtcr =1) is the prob-ability that the inference value of xt is j, given thatthe application of r provides a valid inference step.As appeared in (Shnarch et al, 2011b) this probabil-ity can be evaluated as follows:P (xt=1|xtcr =1)=1?P (xt = 0)P (xtc = 0)(1?P (xtc = 1)?R(r))For t = 1 there is no accumulated sentence-leveldecision at the previous position (i.e.
no ?t?1) there-fore Eq.
(11) becomes:w1cr(T,H) =?j?
{0,1}P (x1 =j|x1cr =1)?R(r)?1(a|j)P (yn = a)AcknowledgmentsThis work was partially supported by the IsraelScience Foundation grant 1112/08, the PASCAL-2 Network of Excellence of the European Com-munity FP7-ICT-2007-1-216886, and the Euro-pean Community?s Seventh Framework Programme(FP7/2007-2013) under grant agreement no.
287923(EXCITEMENT).244ReferencesLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, and Bernardo Magnini.
2009.
The fifthPASCAL recognizing textual entailment challenge.
InProceedings of the Text Analysis Conference.Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa TrangDang, and Danilo Giampiccolo.
2010.
The sixthPASCAL recognizing textual entailment challenge.
InProceedings of the Text Analysis Conference.Peter Clark and Phil Harrison.
2010.
BLUE-Lite: aknowledge-based lexical entailment system for RTE6.In Proceedings of the Text Analysis Conference.Hang Cui, Renxu Sun, Keya Li, Min yen Kan, and Tatseng Chua.
2005.
Question answering passage re-trieval using dependency relations.
In Proceedings ofSIGIR.Abdessamad Echihabi and Daniel Marcu.
2003.
Anoisy-channel approach to question answering.
InProceedings of ACL.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database (Language, Speech, and Com-munication).
The MIT Press.Nizar Habash and Bonnie Dorr.
2003.
A categorial vari-ation database for english.
In Proceedings of the Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics.Aria Haghighi, Andrew Ng, and Christopher Manning.2005.
Robust textual inference via graph matching.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing.Michael Heilman and Noah A. Smith.
2010.
Treeedit models for recognizing textual entailments, para-phrases, and answers to questions.
In Proceedings ofthe Conference of the North American Chapter of theAssociation for Computational Linguistics.Houping Jia, Xiaojiang Huang, Tengfei Ma, XiaojunWan, and Jianguo Xiao.
2010.
PKUTM participationat the Text Analysis Conference 2010 RTE and sum-marization track.
In Proceedings of the Text AnalysisConference.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING.Andrew MacKinlay and Timothy Baldwin.
2009.
Abaseline approach to the RTE5 search pilot.
In Pro-ceedings of the Text Analysis Conference.Debarghya Majumdar and Pushpak Bhattacharyya.2010.
Lexical based text entailment system for maintask of RTE6.
In Proceedings of the Text AnalysisConference.Judea Pearl.
1988.
Probabilistic reasoning in intelli-gent systems: networks of plausible inference.
MorganKaufmann.Vasin Punyakanok, Dan Roth, and Wen tau Yih.
2004.Mapping dependencies trees: An application to ques-tion answering.
In Proceedings of the InternationalSymposium on Artificial Intelligence and Mathemat-ics.Rajat Raina, Andrew Y. Ng, and Christopher D. Man-ning.
2005.
Robust textual inference via learning andabductive reasoning.
In Proceedings of AAAI.Stefan Schoenmackers, Oren Etzioni, and Daniel Weld.2008.
Scaling textual inference to the web.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing.Eyal Shnarch, Jacob Goldberger, and Ido Dagan.
2011a.A probabilistic modeling framework for lexical entail-ment.
In Proceedings of ACL.Eyal Shnarch, Jacob Goldberger, and Ido Dagan.
2011b.Towards a probabilistic model for lexical entailment.In Proceedings of the TextInfer Workshop on TextualEntailment.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: A Core of Semantic Knowl-edge.
In Proceedings of WWW.Mengqiu Wang and Christopher Manning.
2010.
Proba-bilistic tree-edit models with structured latent variablesfor textual entailment and question answering.
In Pro-ceedings of Coling.Mengqiu Wang, Noah A. Smith, and Teruko Mita-mura.
2007.
What is the Jeopardy model?
a quasi-synchronous grammar for QA.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing.245
