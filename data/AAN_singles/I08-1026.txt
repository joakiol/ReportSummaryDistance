A Study on Effectiveness of Syntactic Relationship in Dependence Re-trieval ModelFan Ding1,21: Graduate University,Chinese Academy of SciencesBeijing, 100080, Chinadingfan@ict.ac.cnBin Wang22: Institute of Computing Technology,Chinese Academy of SciencesBeijing, 100080, Chinawangbin@ict.ac.cnAbstractTo relax the Term Independence Assump-tion, Term Dependency is introduced and ithas improved retrieval precision dramati-cally.
There are two kinds of term depend-encies, one is defined by term proximity,and the other is defined by linguistic de-pendencies.
In this paper, we take a com-parative study to re-examine these twokinds of term dependencies in dependencelanguage model framework.
Syntactic rela-tionships, derived from a dependencyparser, Minipar, are used as linguistic termdependencies.
Our study shows: 1) Lin-guistic dependencies get a better result thanterm proximity.
2) Dependence retrievalmodel achieves more improvement in sen-tence-based verbose queries than keyword-based short queries.1 IntroductionFor the sake of computational simplicity, TermIndependence Assumption (TIA) is widely used inmost retrieval models.
It states that terms are statis-tically independent from each other.
Though un-reasonable, TIA did not cause very bad perform-ance.
However, relaxing the assumption by addingterm dependencies into the retrieval model is still abasic IR problem.
Relaxing TIA is not easy be-cause improperly relaxing may introduce muchnoisy information which will hurt the final per-formance.
Defining the term dependency is thefirst step in dependence retrieval model.
Two re-search directions are taken to define the term de-pendency.
The first is to treat term dependencies asterm proximity, for example, the Bi-gram Model(F. Song and W. B. Croft, 1999) and Markov Ran-dom Field Model (D. Metzler and W. B. Croft,2005) in language model.
The second direction isto derive term dependencies by using some linguis-tic structures, such as POS block (Lioma C. andOunis I., 2007) or Noun/Verb Phrase (Mitra et al,1997), Maximum Spanning Tree (C. J. vanRijsbergen, 1979) and Linkage Model (Gao et al,2004) etc.Though linguistic information is intensivelyused in QA (Question Answering) and IE (Infor-mation Extraction) task, it is seldom used in docu-ment retrieval (T. Brants, 2004).
In document re-trieval, how effective linguistic dependencieswould be compared with term proximity still needsto be explored thoroughly.In this paper, we use syntactic relationships de-rived by a popular dependency parser, Minipar (D.Lin, 1998), as linguistic dependencies.
Minipar is abroad-coverage parser for the English language.
Itrepresents the grammar as a network of nodes andlinks, where the nodes represent grammatical cate-gories and the links represent types of dependency.We extract the dependencies between contentwords as term dependencies.To systematically compare term proximity withsyntactic dependencies, we study the dependenceretrieval models in language model framework andpresent a smooth-based dependence languagemodel (SDLM).
It can incorporate these two kindsof term dependencies.
The experiments in TRECcollections show that SDLM with syntactic rela-tionships achieves better result than with the termproximity.The rest of this paper is organized as follows.Section 2 reviews some previous relevant work,197Section 3 presents the definition of term depend-ency using syntactic relationships derived byMinipar.
Section 4 presents in detail the smooth-based dependence language model.
A series of ex-periments on TREC collections are presented inSection 5.
Some conclusions are summarized inSection 6.2 Related WorkGenerally speaking, when using term dependenciesin language modeling framework, two problemsshould be considered: The first is to define andidentify term dependencies; the second is tointegrate term dependencies into a weightingschema.
Accordingly, this section briefly reviewssome recent relevant work, which is summarizedinto two parts: the definition of term dependenciesand weight of term dependencies.2.1 Definition of Term DependenciesIn definition of term dependencies, there are twomain methods: shallow parsing by some linguistictools and term proximity with co-occurrence in-formation.
Both queries and documents are repre-sented as a set of terms and term dependenciesamong terms.
Table 1 summarizes some recentrelated work according to the method they use toidentify term dependencies in queries and docu-ments.Methods DocumentParsingDocument ProximityQueryParsingI: DM,LDM,etc.II: CULM, RP, etc.QueryProximityIII: NIL IV: BG ,WPLM,MRF, etc.Table 1.
Methods in identifying dependenciesIn the part I of table 1, DM is Dependence Lan-guage Model (Gao et al, 2004).
It introduces a de-pendency structure, called linkage model.
Thelinkage structure assumes that term dependenciesin a sentence form an acyclic, planar graph, wheretwo related terms are linked.
LDM (Gao et al,2005) represents the related terms as linguisticconcepts, which can be semantic chunks (e.g.named entities like person name, location name,etc.)
and syntactic chunks (e.g.
noun phrases, verbphrases, etc.
).In the part II of table 1, CULM (M. Srikanth andR.
Srihari, 2003) is a concept unigram languagemodel.
The parser tree of a user query is used toidentify the concepts in the query.
Term sequencein a concept is treated as bi-grams in the documentmodel.
RP (Recognized Phrase, S. Liu et al, 2004)uses some linguistic tools and statistical tools torecognize four types of phrase in the query, includ-ing proper names, dictionary phrase, simple phraseand complex phrase.
A phrase is in a document ifall its content words appear in the document withina certain window size.
The four kinds of phrasecorrespond to variant window size.In the part IV of table 1, BG (bi-gram languagemodel) is the simplest model which assumes termdependencies exist only between adjacent wordsboth in queries and documents.
WPLM (word pairsin language model, Alvarez et al, 2004) relax theco-occurrence window size in documents to 5 andrelax the order constraint in bi-gram model.
MRF(Markov Random Field) classify the term depend-encies in queries into sequential dependence andfull dependence, which respectively corresponds toordered and unordered co-occurrence within a pre-define-sized window in documents.From above discussion we can see that when thequery is sentence-based, parsing method is pre-ferred to proximity method.
When the query iskeyword-based, proximity method is preferred toparsing method.
Thorsten (T. Brants, 2004) note:the longer the queries, the bigger the benefit ofNLP.
This conclusion also holds for the definitionof query term dependencies.2.2 Weight of Term DependenciesIn dependence retrieval model, the final relevancescore of a query and a document consists of boththe independence score and dependence score,such as Bahadur Lazarsfeld expansion (R. M.Losee, 1994) in classical probabilistic IR models.However, Spark Jones et al point out that withouta theoretically motivated integration model, docu-ments containing dependencies (e.g.
phrases) maybe over-scored if they are weighted in the sameway as single words (Jones et al, 1998).
Smooth-ing strategy in language modeling framework pro-vide such an elegant solution to incorporate termdependencies.In the simplest bi-gram model, the probability ofbi-gram (qi-1,qi) in document D is smoothed by itsunigram:198)|()|(),|(,),|()1()|(),|(11111DqPDqqPDqqPwhereDqqPDqPDqqPiiiiiiiiiismoothed???????
?+?= ??
(1)Further, the probability of bi-gram (qi-1,qi) indocument P(qi|qi-1,D) can be smoothed by its prob-ability in collection P(qi|qi-1,C).
If P(qi|qi-1,D) issmoothed as Equation (1), the relevance score ofquery Q={q1q2?qm} and document D is:)|()|()|(log)|,(,)|,()|(log))|()|()|(11log()|(log))|(),|()1(log()|(log)),|()1()|(log()|(log),|(log)|(log)|(log111...21...1...2 11...1...21...1...211...211DqPDqPDqqPDqqMIusuallyDqqMIDqPDqPDqPDqqPDqPDqPDqqPDqPDqqPDqPDqPDqqPDqPDQPiiiiiimiiismoothedmiimi iiiimiimi iiimiimiiiimiiismoothed??
?=?== ??==?==?=???+=???++???++=??+?+=+=??????????????
(2)In Equation (2), the first score term is independ-ence unigram score and the second score term issmoothed dependence score.
Usually ?
is set to 0.9,i.e., the dependence score is given a less weightcompared with the independence score.DM (Gao et al, 2004), which can be regarded asthe generalization of the bi-gram model, gives therelevance score of a document as:??
?=++=LjijimiiDLqqMIDLPDqPDQP),(...1),|,()|(log)|(log)|(log(3)In Equation (3),L is the set of term dependenciesin query Q.
The score function consists of threeparts: a unigram score, a smoothing factorlogP(L|D), and a dependence score MI(qi,qj|L,D).MRF (D. Metzler and W. B. Croft, 2005) com-bines the score of full independence, sequentialdependence and full dependence in an interpolatedway with the weight (0.8, 0.1, 0.1).Though these above models are derived fromdifferent theories, smoothing is an important partwhen incorporating term dependencies.3 Syntactic Parsing of Queries andDocumentsTerm dependencies defined as term proximity maycontain many ?noisy?
dependencies.
It?s our beliefthat parsing technique can filter out some of thesenoises and syntactic relationship is a clue to defineparser, Minipar, to extract the syntactic depend-ency between words.
In this section we will dis-cuss the extraction of syntactic dependencies andthe indexing schemes of term dependencies.3.1 Extraction of Syntactic Dependencieterm dependencies.
We use a popular dependencysaryandes in the parsing result are singlewA dependency relationship is an asymmetric binrelationship between a word called head (orgovernor, parent), and another word calledmodifier (or dependent, daughter).
Dependencygrammars represent sentence structures as a set ofdependency relationships.
For example, Figure 1takes the description field of TREC topic 651 as anexample and shows part of the parsing result ofMinipar.In Figure 1, Cat is the lexical category of word,d Rel is a label assigned to the syntactic depend-encies, such as subject (sub), object (obj), adjunct(mod:A), prepositional attachment (Prep:pcomp-n),etc.
Since function words have no meaning, thedependency relationships including function words,such as N:det:Det, are ignored.
Only the depend-ency relationships between content words are ex-tracted.
However, prepositional attachment is anexception.
A prepositional noun phrase containstwo parts: (N:mod:Prep) and (Prep:pcomp-n:N).We combine these two parts and get a relationshipbetween nouns.Mostly, the noords.
When the nodes are proper names, diction-ary phrases, or compound words connected by hy-phen, there are more than one word in the node.For example, the 5th and 6th relationship in Figure 1describes a compound word ?make up?.
We di-vide these nodes into bi-grams, which assume de-pendencies exist between adjacent words inside theFigure 1.
Parsing Result of MiniparNode2 Node1 Cat1:Rel:Cat2TREC Topic 651: ?How is the ethnic make-up of the U.S. population changing??
?3   makeup N:det:Det the4   makeup N:mod:A ethnic5   makeup N:lex-mod:U make6   makeup N:lex-mod:U -8   makeup N:mod:Prep of11 of  Prep:pcomp-n:N population9   population N:det:Det the10 population N:nn:N  U.S.?199nodes.
If the compound-word node has a relation-ship with other nodes, each word in the compound-word node is assumed to have a relationship withthe other nodes.
Finally, the term dependencies arerepresented as word pairs.
The direction of syntac-tic dependencies is ignored.3.2 Indexing of Term DependenciesAnd theofe thattrieval status value (RSV) has the form:Parsing is a time-consuming process.documents parsing should be an off-line process.The parsing results, recognized as term dependen-cies, should be organized efficiently to support thecomputation of relevance score at the retrieval step.As a supplement of regular documents?wordsinverted index, the indexing of term dependenciesis organized as documents?dependencies lists.For example, Document A has n unique words;each of these n words has relationships with atleast one other word.
Then the term dependenciesinside these n words can be represented as a half-angle matrix as Figure 2 shows.The (i,j)-th element of the matrix is the numbertimes that tidi and tidj have a dependency indocument A.
The matrix has the size of (n-1)*n/2and it is stored as list of size (n-1)*n/2.
Eachdocument corresponds to such a matrix.
When ac-cessing the term dependencies index, the globalword id in the regular index is firstly converted tothe internal id according to the word?s appearanceorder in the document.
The internal id is the indexof the half-angle matrix.
Using the internal id pair,we can get its position in the matrix.4 Smooth-based Dependence ModelFrom the discussion in section 2.2, we can sesmoothing is very important not only in unigramlanguage model, but also in dependence languagemodel.
Taking the smoothed unigram model (C.Zhai and J. Lafferty, 2001) as the example, the re-DDQw DDMLUG QCwpDwpQwcDQRSV ??
log||)|()|(log),(),( += ??
?In Equation (4), c(w,Q) is the frequency of wQ.
The equation has three parts: PDML(w|D), ?D andP((4)inw|C).
PDML(w|D) is the discounted maximumlikelihood estimation of unigram P(w|D), ?D is thesmoothing coefficient of document D, and P(w|C)is collection language model.
If we use a smooth-ing strategy as the smoothed MI in Equation (2),and replace term w with term pair (wi,wj), we canget the smoothed dependence model as:???
?+=DLww jismoothjjijiCwwpDQwwc),(0 ))|,()|1log(),,( ?
(5)In Equation (5), ?0 is the smoothing coefficient.Psm (w ,w |D) and Psm (w ,w |C) is the smoothedwi je the Psmooth(wi,wj|D):pair with relation-shismoothDEPwwpDQRSV,(),(ooth i j ooth i jeight of term pair (wi,wj) in document D and col-lection C.4.1 Smoothing P(w ,w |D)We use two parts to estimatone is the weight of the termips in D, P(wi,wj|R,D), the other is the weight ofthe term co-occurrence in D, Pco(wi,wj|D).
Thesetwo parts are defined as below:|D|)/(wCD)|P(wD)|P(wD)|P(wD)|w,(wP|D|R)/,w,(wCD)R,|w,P(w jiDji?=tid1 tid2 ?
tidn-1 tidntid1tid2?tidn-1tidn??????????????????0****10...**03...
**45..0*20...10iDijijiCO==(6)|D| is the document length, CD(wi,wj,R) denotesthe count of the dependency (wi,w ) in the docu-mjico1jent D, and CD(wi) is the frequency of word wi inD.
Psmooth(wi,wj|D) is defined as a combination ofthe two parts:P )?-(1D)R,|w,P(w?
D)|w,(wP ji1jismooth?+ D)|w,(w?=(7)Figure 2.
Half-angle matrix of term dependencies4.2 Smoothing P(wi,wj|C)bability of term pair.
We use docu-mTo directly estimate the pro(w ,w ) in the collection is not easyi jent frequency of term pair (wi,wj) as its approxi-mation.
Same as Psmooth(wi,wj|D), Psmooth(wi,wj|C)consists of two parts: one is the document fre-quency of term pair (wi,wj), DF(wi,wj), the other isthe averaged document frequency of wi and wj.Then, Psmooth(wi,wj|C) is defined as:DjiDjijismoothCwDFwDFCwwDFCwwP||)()()1(||),()|,( 2??
?+ 2?=??
(8)200In Equation (8), |C|D is the count of Document inCollection C.Finally, if substituting Equation (7) and (8) intoEq).
The final retrieval status value ofthsTo answer the question whether the syntactic de-term proximity,,w ,R) in Equa-tioparameterisns.
Some statistics of the col-lecrameters (?
,?
,?
),SD(MB) Doc.uation (5), there are three parameters (?0,?1,?2)in RSVDEP(Q,De smooth-based dependence model, RSVSDLM, isthe sum of RSVDEP and RSVUG:),(),(),( DQRSVDQRSVDQRSV UGDEPSDLM +=   (9)5 Experiments and Resultpendencies is more effective thanwe systematically compared their performance ontwo kinds of queries.
One is verbose queries (thedescription field of TREC topics), the other is shortqueries (the title field of TREC topics).
Since theverbose queries are sentence-level, they are parsedby Minipar to get the syntactic dependencies.
Inshort queries, term proximity is used to define thedependencies, which assume every two words inthe queries have a dependency.Our smooth-based dependence language model(SDLM) is used as dependence retrieval model inthe experiments.
If defining CD(wi jn (6) to different meanings, we can get a de-pendence model with syntactic dependencie,SDLM_Syn, or a dependence model with termproximity, SDLM_Prox.
In SDLM_Syn,CD(wi,wj,R) is the count of syntactic dependenciesbetween wi and wj in D. In SDLM_Prox,CD(wi,wj,R) is the number of times the terms wiand wj appear within a window N terms.We use Dirichlet-Prior smoothed KL-Divergence model as the unigram model in Equa-tion (9).
The Dirichlet-Prior smoothingset to 2000.
This unigram model, UG, is also thebaseline in the experiments.
The main evaluationmetric in this study is the non-interpolated averageprecision (AvgPr.
)We evaluated the smooth-based dependencelanguage model in two document collections andfour query collectiotions are shown in Table 2.Three retrieval models are evaluated in theTREC collections: UG, SDLM_Syn andSDLM_Prox.
Besides the pa 0 1 2LM_Prox has one more parameter thanSDLM_Syn.
It is the window size N ofCD(wi,wj,R).
In the experiments, we tried the win-dow size N of 5, 10, 20 and 40 to find the optimalsetting.
We find the optimal N is 10.
This size isclose to sentence length and it is used in the fol-lowing experiments.Coll.
Queries Documents Size #AP 51-200 Associated Press(1489 164,597988,1989) inDisk2TR -8EC7 351-450RoHard queries inbust04 35 hard351-450Robust04New651-700ex.672Disk 4&5(no CR)3,120 528,155Table 2.
TREC collectionseter ,?2) were trained on three queryse 700.
Each query setwas divided into two halves, and we applied two-foParam s (?0,?1ts: 51-200, 351-450 and 651-ld cross validation to get the final result.
Wetrained (?0,?1,?2) by directly maximizing MAP(mean average precision).
Since the parameterrange was limited, we used a linear search methodat step 0.1 to find the optimal setting of (?0,?1,?2).Topic Index0 20 40 60 80 100 120 140 160AvgPr0.0.2.4.6.81.0UGSDLM_SynTopic Index0 20 40 60 80 100AvgPr0.0.2.4.6.81.0UGSDLM_SynTopic Index0 10 20 30 40AvgPr0.00.05.10.15.20.25.30.35UGSDLM_SynTopic Index0 10 20 30 40 50AvgPr0.0.2.4.6.81.0UGSDLM_SynFigure 3 UG vs. SDLM_Syn in verbose queries:Top Left (51-200), Top Right (351-450), BottomLeft (hard topics in 351-450), and Bottom RightTable 3 and Table 4 respectively.
Thesettings of (?0,?1,?2) used in the experiments areal(651-700)The results on verbose queries and short queriesare listed inso listed.
A star mark after the change percentvalue indicates a statistical significant difference atthe 0.05 level(one-sided Wilcoxon test).
In verbosequeries, we can see that SDLM has distinct201UG SDLM_Prox SDLM_Syn collectionsAvgPr.
AvgPr.
%ch over UG (?0,?1,?2) AvgPr.
%ch over UG (?0,?1,?2)AP 0.2159 0.2360 9.31* (1.8,0.6,0.9) 0.2393 10.84* (1.9,0.7,0.9)TREC7-8 0.1893 0.2049 8.24* (1.2,0.1,0.2) 0.2061 8.87* (0.4,0.1,0.9)Robust04_hard 0.0909 0.1049 15.40* (1.2,0.1,0.2) 0.1064 17.05* (0.4,0.1,0.9)Robust04_new 0.2754 0.3022 9.73* (0.7,0.1,0.3) 0.3023 9.77* (0.7,0.1,0.3)Table 3.
Comparison results on verbose queriesUG SDLM_Prox SDLM_Syn collectionsAvgPr.
AvgPr.
%ch over UG (?0,?1,?2) AvgPr.
%ch over UG (?0,?1,?2)AP 0.2643 0.2644 0 (1.3,0.6,0.1) 0.2647 0.15 (1.1,0.5,0.2)TREC7-8 0.2069 0.2076 0.34 (1.2,0.3,0.2) 0.2070 0 (1,0.1,0.2)Robust04_hard 0.1037 0.1044 0.68 (1.2,0.3,0.2) 0.1045 0.77 (1,0.1,0.2)Robust04_new 0.2771 0.2888 4.22* (1.3,0.3,0.4) 0.2869 3.54* (1.3,0.1,0.4)Table 4.
Comparison results on short queriesTopic Index0 20 40 60 80 100 120 140 160AvgPr0.0.2.4.6.81.0SDLM_ProxSDLM_SynTopic Index0 20 40 60 80 100AvgPr0.0.2.4.6.81.0SDLM_ProxSDLM_SynTopic Index0 10 20 30 40AvgPr0.00.05.10.15.20.25.30.35SDLM_ProxSDLM_SynTopic Index0 10 20 30 40 50AvgPr0.0.2.4.6.81.0SDLM_ProxSDLM_SynFigure 4.
SDLM_Prox vs. SDLM_Syn in verbosequeries: Top Left (51-200), Top Right (351-450),Bottom Left (hard topics in 351-450), BottomRight (651-700)improvement over UG and SDLM_Syn has robustimprovement over SDLM_Prox.
In short queries,SDLM has slight improvement over UG andSDLM_Syn is comparative with SDLM_Prox.To study the effectiveness of syntactic depend-encies in detail, Figure 3 and 4 compareSDLM_Syn and UG, SDLM_Syn andSDLM_Prox topic by topic in verbose queries.As shown in Figure 3 and Figure 4, SDLM_Synachieves substantial improvements over UG in themajority of queries.
While SDLM_Syn is com-parative with SDLM_Prox in most of the queries,SDLM_Syn still get some noticeable improve-ments over SDLM_Prox.From Table 3 and 4, we can see while the pa-rameters (?0,?1,?2) change a lot in two differentdocument collections, there is little change in thesame document collection.
This shows the robust-ness of our smooth-based dependence languagemodel.6 ConclusionIn this paper we have systematically studied theeffectiveness of syntactic dependencies comparedwith term proximity in dependence retrievalmodel.
To compare the effectiveness of syntacticdependencies and term proximity, we develop asmooth-based dependence language model thatcan incorporate different term dependencies.Experiments on four TREC collections indicatethe effectiveness of syntactic dependencies: Inverbose queries, the improvement of syntacticdependencies over term proximity is noticeable;In short queries, the improvement is not notice-able.
For keywords-based short queries with av-erage length of 2-3 words, the term dependenciesin the queries are very few.
So the improvementof dependence retrieval model over independenceunigram model is very limited.
Meanwhile, thedifference between syntactic dependencies andterm proximity is not noticeable.
For dependenceretrieval model, we can get the same conclusion asThorsten Brants: the longer the queries are, thebigger the benefit of NLP is.202Referencesijsber Rworths, 1979.varez, s, Ji Niee g fo tions  200 686-s for l e m  aformation re ,ges 334?342b aluati MINIthe E ion ondomfield model for term dependencies, In Proceedings ofSIGIR?05, Pages 472-479, 2005Fei Song and W. Bruce Croft.
A general languagemodel for information retrieval.
In Proceedings ofSIGIR?99, pages 279-280, 1999.Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu and Gu -hong Cao, Dependence Language Model for Info -mation Retrieval, In Proceedings of SIGIR?04,Pages:170-177, 2004Jianfeng Gao, Haoliang Qi, Xinsong Xia and Jian-YunNie.
Linear Discriminant Model for Information Re-trieval.
In Proceedings of SIGIR?05, Pages:290-297,2005y Computer Laboratory.
1998Shuang Liu, Fang Liu, Clement Yu and Weiyi Meng,An Effective Approach to Document Retrieval viaUtilizing WordNet an ses, In Pro-in ITer ence heu feld e n. Inf ess-d men 3?3atur uage  In-formation Retrieval .
In Proceedings of 20th Interna-tional Conference o nal Linguistics,eC.
J. van R g foen.
In rmation etrieval.
Butter-Carmen Al  Philippe Langlai an-Yun , BahadWord Pairs inRetrieval, In PrLanguagoceedingModelinof RIAOr Informa4, Pagesing an705, 2004Chengxianging methodZhai an n Lafferty.
A stud Johanguady of smooth-lied to ad hoc gtrieval.
InodelsProceedippngs of SIGIR?01inpa, 2001Dekang Lin, DepPAR, Proceedinendency-gs of Woased Evrkshop onon ofvaluat-fParsing Systems, Granada, Spain, May,Donald Metzler and W. Bruce Croft, A Markov ra1998.irK.
Sparck Jones, S. Walker, and S. E. Robertson, Aprobabilistic model of information retrieval: devel-opment and status.
Technical Report TR-446, Cam-bridge UniversitLioma C. and Ounis I., A Syntactically-Based QueryReformulation Technique for Information Retrieval,Information Processing and Management (IPM), El-sevier Science, 2007M.
Mitra, C. Buckley, A. Singhal, and C. Cardie.
AnAnalysis of Statistical and Syntactic Phrases.
InProceedings of RIAO-97, 5th International Confer-ence ?Recherche d?Information Assistee par Ordi-nateur?, pages 200-214, Montreal, CA, 1997.Munirathnam Srikanth and Rohini Srihari, ExploitingSyntactic Structure of Queries in a Language Mod-eling Approach to IR, In Proceedings of CIKM?03,Pages: 476-483, 2003d Recognizing Phra-2ceed gs of SIG R?04, Pages: 266 72, 2004Robert M. Losee.
m depend : Truncating tr Lazars xpansio ormation ProcManage t, 30(2):29 03, 1994.Thorsten Brants.
N al Lang Processing inn ComputatioAntw rp, Belgium, 2004:1-13.203
