2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 386?395,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsA Comparative Investigation of Morphological Language Modeling for theLanguages of the European UnionThomas Mu?ller, Hinrich Schu?tze and Helmut SchmidInstitute for Natural Language ProcessingUniversity of Stuttgart, Germany{muellets,schmid}@ims.uni-stuttgart.deAbstractWe investigate a language model that com-bines morphological and shape features witha Kneser-Ney model and test it in a largecrosslingual study of European languages.Even though the model is generic and we usethe same architecture and features for all lan-guages, the model achieves reductions in per-plexity for all 21 languages represented in theEuroparl corpus, ranging from 3% to 11%.
Weshow that almost all of this perplexity reduc-tion can be achieved by identifying suffixes byfrequency.1 IntroductionLanguage models are fundamental to many naturallanguage processing applications.
In the most com-mon approach, language models estimate the proba-bility of the next word based on one or more equiv-alence classes that the history of preceding words isa member of.
The inherent productivity of naturallanguage poses a problem in this regard because thehistory may be rare or unseen or have unusual prop-erties that make assignment to a predictive equiva-lence class difficult.In many languages, morphology is a key sourceof productivity that gives rise to rare and unseenhistories.
For example, even if a model can learnthat words like ?large?, ?dangerous?
and ?serious?are likely to occur after the relatively frequent his-tory ?potentially?, this knowledge cannot be trans-ferred to the rare history ?hypothetically?
withoutsome generalization mechanism like morphologicalanalysis.Our primary goal in this paper is not to de-velop optimized language models for individual lan-guages.
Instead, we investigate whether a simplegeneric language model that uses shape and mor-phological features can be made to work well acrossa large number of languages.
We find that this isthe case: we achieve considerable perplexity reduc-tions for all 21 languages in the Europarl corpus.We see this as evidence that morphological languagemodeling should be considered as a standard part ofany language model, even for languages like Englishthat are often not viewed as a good application ofmorphological modeling due to their morphologicalsimplicity.To understand which factors are important forgood performance of the morphological compo-nent of a language model, we perform an exten-sive crosslingual analysis of our experimental re-sults.
We look at three parameters of the morpho-logical model we propose: the frequency threshold?
that divides words subject to morphological clus-tering from those that are not; the number of suffixesused ?
; and three different morphological segmen-tation algorithms.
We also investigate the differen-tial effect of morphological language modeling ondifferent word shapes: alphabetical words, punctua-tion, numbers and other shapes.Some prior work has used morphological modelsthat require careful linguistic analysis and language-dependent adaptation.
In this paper we show thatsimple frequency analysis performs only slightlyworse than more sophisticated morphological anal-ysis.
This potentially removes a hurdle to usingmorphological models in cases where sufficient re-sources to do the extra work required for sophisti-cated morphological analysis are not available.The motivation for using morphology in lan-guage modeling is similar to distributional clustering386(Brown et al, 1992).
In both cases, we form equiv-alence classes of words with similar distributionalbehavior.
In a preliminary experiment, we find thatmorphological equivalence classes reduce perplex-ity as much as traditional distributional classes ?
asurprising result we intend to investigate in futurework.The main contributions of this paper are as fol-lows.
We present a language model design and aset of morphological and shape features that achievereductions in perplexity for all 21 languages rep-resented in the Europarl corpus, ranging from 3%to 11%, compared to a Kneser-Ney model.
Weshow that identifying suffixes by frequency is suf-ficient for getting almost all of this perplexity reduc-tion.
More sophisticated morphological segmenta-tion methods do not further increase perplexity orjust slightly.
Finally, we show that there is one pa-rameter that must be tuned for good performance formost languages: the frequency threshold ?
abovewhich a word is not subject to morphological gen-eralization because it occurs frequently enough forstandard word n-gram language models to use it ef-fectively for prediction.The paper is organized as follows.
In Section 2we discuss related work.
In Section 3 we describethe morphological and shape features we use.
Sec-tion 4 introduces language model and experimentalsetup.
Section 5 discusses our results.
Section 6summarizes the contributions of this paper.2 Related WorkWhittaker and Woodland (2000) apply languagemodeling to morpheme sequences and investigatedata-driven segmentation methods.
Creutz et al(2007) propose a similar method that improvesspeech recognition for highly inflecting languages.They use Morfessor (Creutz and Lagus, 2007) tosplit words into morphemes.
Both approaches areessentially a simple form of a factored languagemodel (FLM) (Bilmes and Kirchhoff, 2003).
In ageneral FLM a number of different back-off pathsare combined by a back-off function to improve theprediction after rare or unseen histories.
Vergyri etal.
(2004) apply FLMs and morphological featuresto Arabic speech recognition.These papers and other prior work on using mor-phology in language modeling have been language-specific and have paid less attention to the ques-tion as to how morphology can be useful acrosslanguages and what generic methods are appropri-ate for this goal.
Previous work also has concen-trated on traditional linguistic morphology whereaswe compare linguistically motivated morphologi-cal segmentation with frequency-based segmenta-tion and include shape features in our study.Our initial plan for this paper was to use com-plex language modeling frameworks that allow ex-perimenters to include arbitrary features (includingmorphological and shape features) in the model.
Inparticular, we looked at publicly available imple-mentations of maximum entropy models (Rosen-feld, 1996; Berger et al, 1996) and random forests(Xu and Jelinek, 2004).
However, we found thatthese methods do not currently scale to running alarge set of experiments on a multi-gigabyte parallelcorpus of 21 languages.
Similar considerations ap-ply to other sophisticated language modeling tech-niques like Pitman-Yor processes (Teh, 2006), re-current neural networks (Mikolov et al, 2010) andFLMs in their general, more powerful form.
In ad-dition, perplexity reductions of these complex mod-els compared to simpler state-of-the-art models aregenerally not large.We therefore decided to conduct our study in theframework of smoothed n-gram models, which cur-rently are an order of magnitude faster and morescalable.
More specifically, we adopt a class-basedapproach, where words are clustered based on mor-phological and shape features.
This approach has thenice property that the number of features used to es-timate the classes does not influence the time neededto train the class language model, once the classeshave been found.
This is an important considerationin the context of the questions asked in this paper asit allows us to use large numbers of features in ourexperiments.3 Modeling of morphology and shapeOur basic approach is to define a number of morpho-logical and shape features and then assign all wordswith identical feature values to one class.
For themorphological features, we investigate three differ-ent automatic suffix identification algorithms: Re-387s, e, d, ed, n, g, ng, ing, y, t, es, r, a, l, on, er, ion,ted, ly, tion, rs, al, o, ts, ns, le, i, ation, an, ers, m, nt,ting, h, c, te, sed, ated, en, ty, ic, k, ent, st, ss, ons, se,ity, ble, ne, ce, ess, ions, us, ry, re, ies, ve, p, ate, in,tions, ia, red, able, is, ive, ness, lly, ring, ment, led,ned, tes, as, ls, ding, ling, sing, ds, ded, ian, nce, ar,ating, sm, ally, nts, de, nd, ism, or, ge, ist, ses, ning,u, king, na, elFigure 1: The 100 most frequent English suffixes in Eu-roparl, ordered by frequencyports (Keshava and Pitler, 2006), Morfessor (Creutzand Lagus, 2007) and Frequency, where Frequencysimply selects the most frequent word-final letter se-quences as suffixes.
The 100 most frequent suffixesfound by Frequency for English are given in Fig-ure 1.We use the ?
most frequent suffixes for all threealgorithms, where ?
is a parameter.
The focus of ourwork is to evaluate the utility of these algorithms forlanguage modeling; we do not directly evaluate thequality of the suffixes.A word is segmented by identifying the longest ofthe ?
suffixes that it ends with.
Thus, each word hasone suffix feature if it ends with one of the ?
suffixesand none otherwise.In addition to suffix features, we define featuresthat capture shape properties: capitalization, specialcharacters and word length.
If a word in the test sethas a combination of feature values that does not oc-cur in the training set, then it is assigned to the classwhose features are most similar.
We described thesimilarity measure and details of the shape featuresin prior work (Mu?ller and Schu?tze, 2011).
The shapefeatures are listed in Table 1.4 Experimental SetupExperiments are performed using srilm (Stolcke,2002), in particular the Kneser-Ney (KN) andgeneric class model implementations.
Estimation ofoptimal interpolation parameters is based on (Bahlet al, 1991).4.1 BaselineOur baseline is a modified KN model (Chen andGoodman, 1999).4.2 Morphological class language modelWe use a variation of the model proposed by Brownet al (1992) that we developed in prior work on En-glish (Mu?ller and Schu?tze, 2011).
This model is aclass-based language model that groups words intoclasses and replaces the word transition probabilityby a class transition probability and a word emissionprobability:PC(wi|wi?1i?N+1) =P (g(wi)|g(wi?1i?N+1)) ?
P (wi|g(wi))where g(w) is the class of word w and we writeg(wi .
.
.
wj) for g(wi) .
.
.
g(wj).Our approach targets rare and unseen histories.We therefore exclude all frequent words from clus-tering on the assumption that enough training datais available for them.
Thus, clustering of words isrestricted to those below a certain token frequencythreshold ?.
As described above, we simply groupall words with identical feature values into one class.Words with a training set frequency above ?
areadded as singletons.
The class transition probabil-ity P (g(wi)|g(wi?1i?N+1)) is estimated using Witten-Bell smoothing.1The word emission probability is defined as fol-lows:P (w|c) =????
?1 , N(w) > ?N(w)Pw?c N(w) ??
(c)|c|?1 , ??N(w)>0?
(c) , N(w) = 0where c = g(w) is w?s class and N(w) is the fre-quency of w in the training set.
The class-dependentout-of-vocabulary (OOV) rate ?
(c) is estimated onheld-out data.
Our final model PM interpolates PCwith a modified KN model:PM (wi|wi?N+1i?1 ) =?
(g(wi?1)) ?
PC(wi|wi?N+1i?1 )+(1?
?
(g(wi?1))) ?
PKN(wi|wi?N+1i?1 ) (1)This model can be viewed as a generalization ofthe simple interpolation ?PC + (1?
?
)PW used byBrown et al (1992) (where PW is a word n-gram1Witten-Bell smoothing outperformed modified Kneser-Ney(KN) and Good-Turing (GT).388is capital(w) first character of w is an uppercase letteris all capital(w) ?
c ?
w : c is an uppercase lettercapital character(w) ?
c ?
w : c is an uppercase letterappears in lowercase(w) ?capital character(w) ?
w?
?
?Tspecial character(w) ?
c ?
w : c is not a letter or digitdigit(w) ?
c ?
w : c is a digitis number(w) w ?
L([+?
?][0?
9] (([., ][0?
9])|[0?
9]) ?
)Table 1: Shape features as defined by Mu?ller and Schu?tze (2011).
?T is the vocabulary of the training corpus T , w?
isobtained from w by changing all uppercase letters to lowercase and L(expr) is the language generated by the regularexpression expr.model and PC a class n-gram model).
For the set-ting ?
= ?
(clustering of all words), our model isessentially a simple interpolation of a word n-gramand a class n-gram model except that the interpola-tion parameters are optimized for each class insteadof using the same interpolation parameter ?
for allclasses.
We have found that ?
= ?
is never optimal;it is always beneficial to assign the most frequentwords to their own singleton classes.Following Yuret and Bic?ici (2009), we evaluatemodels on the task of predicting the next word froma vocabulary that consists of all words that occurmore than once in the training corpus and the un-known word UNK.
Performing this evaluation forKN is straightforward: we map all words with fre-quency one in the training set to UNK and then com-pute PKN(UNK |h) in testing.In contrast, computing probability estimates forPC is more complicated.
We define the vocabularyof the morphological model as the set of all wordsfound in the training corpus, including frequency-1words, and one unknown word for each class.
Wedo this because ?
as we argued above ?
morpholog-ical generalization is only expected to be useful forrare words, so we are likely to get optimal perfor-mance for PC if we include all words in clusteringand probability estimation, including hapax legom-ena.
Since our testing setup only evaluates on wordsthat occur more than once in the training set, we ide-ally would want to compute the following estimatewhen predicting the unknown word:PC(UNKKN |h) =?
{w:N(w)=1}PC(w|h) +?cPC(UNKc |h) (2)where we distinguish the unknown words of themorphological classes from the unknown word usedin evaluation and by the KN model by giving the lat-ter the subscript KN.However, Eq.
2 cannot be computed efficientlyand we would not be able to compute it in practicalapplications that require fast language models.
Forthis reason, we use the modified class model P ?C inEq.
1 that is defined as follows:P ?C(w|h) ={ PC(w|h) , N(w) ?
1PC(UNKg(w) |h), N(w) = 0P ?C and ?
by extension ?
PM are deficient.
Thismeans that the evaluation of PM we present belowis pessimistic in the sense that the perplexity reduc-tions would probably be higher if we were willing tospend additional computational resources and com-pute Eq.
2 in its full form.4.3 Distributional class language modelThe most frequently used type of class-based lan-guage model is the distributional model introducedby Brown et al (1992).
To understand the dif-ferences between distributional and morphologicalclass language models, we compare our morpholog-ical model PM with a distributional model PD thathas exactly the same form as PM; in particular, itis defined by Equations (1) and (2).
The only dif-ference is that the classes are morphological for PMand distributional for PD.The exchange algorithm that was used by Brownet al (1992) has very long running times for largecorpora in standard implementations like srilm.
Itis difficult to conduct the large number of cluster-ings necessary for an extensive study like ours usingstandard implementations.389Language T/T ?
#SentencesS bg Bulgarian .0183 .0094 181,415S cs Czech .0185 .0097 369,881S pl Polish .0189 .0096 358,747S sk Slovak .0187 .0088 368,624S sl Slovene .0156 .0090 365,455G da Danish .0086 .0077 1,428,620G de German .0091 .0073 1,391,324G en English .0028 .0023 1,460,062G nl Dutch .0061 .0048 1,457,629G sv Swedish .0090 .0095 1,342,667E el Greek .0081 .0079 851,636R es Spanish .0040 .0031 1,429,276R fr French .0029 .0024 1,460,062R it Italian .0040 .0030 1,389,665R pt Portuguese .0042 .0032 1,426,750R ro Romanian .0142 .0079 178,284U et Estonian .0329 .0198 375,698U fi Finnish .0231 .0183 1,394,043U hu Hungarian .0312 .0163 364,216B lt Lithuanian .0265 .0147 365,437B lv Latvian .0182 .0086 363,104Table 2: Statistics for the 21 languages.
S = Slavic, G= Germanic, E = Greek, R = Romance, U = Uralic, B= Baltic.
Type/token ratio (T/T) and # sentences for thetraining set and OOV rate ?
for the validation set.
Thetwo smallest and largest values in each column are bold.We therefore induce the distributional classesas clusters in a whole-context distributional vectorspace model (Schu?tze and Walsh, 2011), a modelsimilar to the ones described by Schu?tze (1992)and Turney and Pantel (2010) except that dimensionwords are immediate left and right neighbors (as op-posed to neighbors within a window or specific typesof governors or dependents).
Schu?tze and Walsh(2011) present experimental evidence that suggeststhat the resulting classes are competitive with Brownclasses.4.4 CorpusOur experiments are performed on the Europarl cor-pus (Koehn, 2005), a parallel corpus of proceed-ings of the European Parliament in 21 languages.The languages are members of the following fam-ilies: Baltic languages (Latvian, Lithuanian), Ger-manic languages (Danish, Dutch, English, Ger-man, Swedish), Romance languages (French, Ital-ian, Portuguese, Romanian, Spanish), Slavic lan-guages (Bulgarian, Czech, Polish, Slovak, Slovene),Uralic languages (Estonian, Finnish, Hungarian)and Greek.
We only use the part of the corpus thatcan be aligned to English sentences.
All 21 corporaare divided into training set (80%), validation set(10%) and test set (10%).
The training set is used formorphological and distributional clustering and esti-mation of class and KN models.
The validation setis used to estimate the OOV rates ?
and the optimalparameters ?, ?
and ?.
Table 2 gives basic statisticsabout the corpus.
The sizes of the corpora of lan-guages whose countries have joined the Europeancommunity more recently are smaller than for coun-tries who have been members for several decades.We see that English and French have the lowesttype/token ratios and OOV rates; and the Uralic lan-guages (Estonian, Finnish, Hungarian) and Lithua-nian the highest.
The Slavic languages have highervalues than the Germanic languages, which in turnhave higher values than the Romance languages ex-cept for Romanian.
Type/token ratio and OOVrate are one indicator of how much improvementwe would expect from a language model witha morphological component compared to a non-morphological language model.25 Results and DiscussionWe performed all our experiments with an n-gramorder of 4; this was the order for which the KNmodel performs best for all languages on the vali-dation set.5.1 Morphological modelUsing grid search, we first determined on the vali-dation set the optimal combination of three param-eters: (i) ?
?
{100, 200, 500, 1000, 2000, 5000},(ii) ?
?
{50, 100, 200, 500} and (iii) segmentationmethod.
Recall that we only cluster words whosefrequency is below ?
and only consider the ?
most2The tokenization of the Europarl corpus has a preferencefor splitting tokens in unclear cases.
OOV rates would be higherfor more conservative tokenization strategies.4A two-tailed paired t-test on the improvements by languageshows that the morphological model significantly outperformsthe distributional model with p=0.0027.
A test on the Germanic,Romance and Greek languages yields p=0.19.390PPKN ?
?M ??
M?
PPC PPM ?M ?
?D PPWC PPD ?DS bg 74 200 50 f 103 69 0.07 500 141 71 0.04S cs 141 500 100 f 217 129 0.08 1000 298 134 0.04S pl 148 500 100 m 241 134 0.09 1000 349 141 0.05S sk 123 500 200 f 186 111 0.10 1000 261 116 0.06S sl 118 500 100 m 177 107 0.09 1000 232 111 0.06G da 69 1000 100 r 89 65 0.05 2000 103 65 0.05G de 100 2000 50 m 146 94 0.06 2000 150 94 0.06G en 55 2000 50 f 73 53 0.03 5000 87 53 0.04G nl 70 2000 50 r 100 67 0.04 5000 114 67 0.05G sv 98 1000 50 m 132 92 0.06 2000 154 92 0.06E el 80 1000 100 f 108 73 0.08 2000 134 74 0.07R es 57 2000 100 m 77 54 0.05 5000 93 54 0.05R fr 45 1000 50 f 56 43 0.04 5000 71 42 0.05R it 69 2000 100 m 101 66 0.04 2000 100 66 0.05R pt 62 2000 50 m 88 59 0.05 2000 87 59 0.05R ro 76 500 100 m 121 70 0.07 1000 147 71 0.07U et 256 500 100 m 422 230 0.10 1000 668 248 0.03U fi 271 1000 500 f 410 240 0.11 2000 706 261 0.04U hu 151 200 200 m 222 136 0.09 1000 360 145 0.03B lt 175 500 200 m 278 161 0.08 1000 426 169 0.03B lv 154 500 200 f 237 142 0.08 1000 322 147 0.05Table 3: Perplexities on the test set for N = 4.
S = Slavic, G = Germanic, E = Greek, R = Romance, U =Uralic, B = Baltic.
?
?x, ??
and M?
denote frequency threshold, suffix count and segmentation method optimal on thevalidation set.
The letters f, m and r stand for the frequency-based method, Morfessor and Reports.
PPKN, PPC,PPM, PPWC, PPD are the perplexities of KN, morphological class model, interpolated morphological class model,distributional class model and interpolated distributional class model, respectively.
?x denotes relative improvement:(PPKN?PPx)/PPKN.
Bold numbers denote maxima and minima in the respective column.4frequent suffixes.
An experiment with the optimalconfiguration was then run on the test set.
The re-sults are shown in Table 3.
The KN perplexities varybetween 45 for French and 271 for Finnish.The main result is that the morphological modelPM consistently achieves better performance thanKN (columns PPM and ?M), in particular forSlavic, Uralic and Baltic languages and Greek.
Im-provements range from 0.03 for English to 0.11 forFinnish.Column ?
?M gives the threshold that is optimal forthe validation set.
Values range from 200 to 2000.Column ??
gives the optimal number of suffixes.
Itranges from 50 to 500.
The morphologically com-plex language Finnish seems to benefit from moresuffixes than morphologically simple languages likeDutch, English and German, but there are a few lan-guages that do not fit this generalization, e.g., Esto-nian for which 100 suffixes are optimal.The optimal morphological segmenter is given incolumn M?
: f = Frequency, r = Reports, m = Mor-fessor.
The most sophisticated segmenter, Morfes-sor is optimal for about half of the 21 languages, butFrequency does surprisingly well.
Reports is opti-mal for two languages, Danish and Dutch.
In gen-eral, Morfessor seems to have an advantage for com-plex morphologies, but is beaten by Frequency forFinnish and Latvian.5.2 Distributional modelColumns PPD and ?D show the performance of thedistributional class language model.
As one wouldperhaps expect, the morphological model is superiorto the distributional model for morphologically com-plex languages like Estonian, Finnish and Hungar-ian.
These languages have many suffixes that have391?
?+ ????
?+ ??
?
?+ ????
?+ ??
?M+ ??M?
M+ M?S bg 0.03 200 5000 0.01 50 500 f mS cs 0.03 500 5000 100 500 f rS pl 0.03 500 5000 0.01 100 500 m rS sk 0.02 500 5000 200 500 0.01 f rS sl 0.03 500 5000 0.01 100 500 m rG da 0.02 1000 100 100 50 r fG de 0.02 2000 100 50 500 m fG en 0.01 2000 100 50 500 f rG nl 0.01 2000 100 50 500 r fG sv 0.02 1000 100 50 500 m fE el 0.02 1000 100 100 500 0.01 f rR es 0.02 2000 100 100 500 m rR fr 0.01 1000 100 50 500 f rR it 0.01 2000 100 100 500 m rR pt 0.02 2000 100 50 500 m rR ro 0.03 500 5000 100 500 m rU et 0.02 500 5000 0.01 100 50 0.01 m rU fi 0.03 1000 100 0.03 500 50 0.02 f rU hu 0.03 200 5000 0.01 200 50 m rB lt 0.02 500 5000 200 50 m rB lv 0.02 500 5000 200 500 f rTable 4: Sensitivity of perplexity values to the parameters (on the validation set).
S = Slavic, G = Germanic, E =Greek, R = Romance, U = Uralic, B = Baltic.
?x+ and ?x?
denote the relative improvement of PM over the KNmodel when parameter x is set to the best (x+) and worst value (x?
), respectively.
The remaining parameters are setto the optimal values of Table 3.
Cells with differences of relative improvements that are smaller than 0.01 are leftempty.high predictive power for the distributional contextsin which a word can occur.
A morphological modelcan exploit this information even if a word with aninformative suffix did not occur in one of the lin-guistically licensed contexts in the training set.
Fora distributional model it is harder to learn this typeof generalization.What is surprising about the comparative perfor-mance of morphological and distributional models isthat there is no language for which the distributionalmodel outperforms the morphological model by awide margin.
Perplexity reductions are lower thanor the same as those of the morphological modelin most cases, with only four exceptions ?
English,French, Italian, and Dutch ?
where the distributionalmodel is better by one percentage point than themorphological model (0.05 vs. 0.04 and 0.04 vs.0.03).Column ?
?D gives the frequency threshold for thedistributional model.
The optimal threshold rangesfrom 500 to 5000.
This means that the distributionalmodel benefits from restricting clustering to less fre-quent words ?
and behaves similarly to the morpho-logical class model in that respect.
We know of noprevious work that has conducted experiments onfrequency thresholds for distributional class modelsand shown that they increase perplexity reductions.5.3 Sensitivity analysis of parametersTable 3 shows results for parameters that were opti-mized on the validation set.
We now want to analyzehow sensitive performance is to the three parame-ters ?, ?
and segmentation method.
To this end, wepresent in Table 4 the best and worst values of eachparameter and the difference in perplexity improve-ment between the two.Differences of perplexity improvement betweenbest and worst values of ?M range between 0.01392and 0.03.
The four languages with the smallestdifference 0.01 are morphologically simple (Dutch,English, French, Italian).
The languages with thelargest difference (0.03) are morphologically morecomplex languages.
In summary, the frequencythreshold ?M has a comparatively strong influenceon perplexity reduction.
The strength of the effect iscorrelated with the morphological complexity of thelanguage.In contrast to ?, the number of suffixes ?
andthe segmentation method have negligible effect onmost languages.
The perplexity reductions for dif-ferent values of ?
are 0.03 for Finnish, 0.01 for Bul-garian, Estonian, Hungarian, Polish and Slovenian,and smaller than 0.01 for the other languages.
Thismeans that, with the exception of Finnish, we canuse a value of ?
= 100 for all languages and be veryclose to the optimal perplexity reduction ?
either be-cause 100 is optimal or because perplexity reductionis not sensitive to choice of ?.
Finnish is the onlylanguage that clearly benefits from a large numberof suffixes.Surprisingly, the performance of the morphologi-cal segmentation methods is very close for 17 of the21 languages.
For three of the four where there isa difference in improvement of ?
0.01, Frequency(f) performs best.
This means that Frequency is agood segmentation method for all languages, exceptperhaps for Estonian.5.4 Impact of shapeThe basic question we are asking in this paper isto what extent the sequence of characters a wordis composed of can be exploited for better predic-tion in language modeling.
In the final analysis inTable 5 we look at four different types of charactersequences and their contributions to perplexity re-duction.
The four groups are alphabetic charactersequences (W), numbers (N), single special charac-ters (P = punctuation), and other (O).
Examples forO would be ?751st?
and words containing specialcharacters like ?O?Neill?.
The parameters used arethe optimal ones of Table 3.
Table 5 shows that theimpact of special characters on perplexity is similaracross languages: 0.04 ?
?P ?
0.06.
The same istrue for numbers: 0.23 ?
?N ?
0.33, with two out-liers that show a stronger effect of this class: Finnish?N = 0.38 and German ?N = 0.40.?W ?P ?N ?OS bg 0.07 0.04 0.28 0.16S cs 0.09 0.04 0.26 0.33S pl 0.10 0.05 0.23 0.22S sk 0.10 0.05 0.25 0.28S sl 0.10 0.04 0.28 0.28G da 0.05 0.05 0.31 0.18G de 0.06 0.05 0.40 0.18G en 0.03 0.04 0.33 0.14G nl 0.04 0.05 0.31 0.26G sv 0.06 0.05 0.31 0.35E el 0.08 0.05 0.33 0.14R es 0.05 0.04 0.26 0.14R fr 0.04 0.04 0.29 0.01R it 0.04 0.05 0.33 0.02R pt 0.05 0.05 0.28 0.39R ro 0.08 0.04 0.25 0.17U et 0.11 0.05 0.26 0.26U fi 0.12 0.06 0.38 0.36U hu 0.10 0.04 0.32 0.23B lt 0.08 0.06 0.27 0.05B lv 0.08 0.05 0.26 0.19Table 5: Relative improvements of PM on the valida-tion set compared to KN for histories wi?1i?N+1 groupedby the type of wi?1.
The possible types are alphabeticword (W), punctuation (P), number (N) and other (O).The fact that special characters and numbers be-have similarly across languages is encouraging asone would expect less crosslinguistic variation forthese two classes of words.In contrast, ?true?
words (those exclusively com-posed of alphabetic characters) show more variationfrom language to language: 0.03 ?
?W ?
0.12.The range of variation is not necessarily larger thanfor numbers, but since most words are alphabeticalwords, class W is responsible for most of the differ-ence in perplexity reduction between different lan-guages.
As before we observe a negative correlationbetween morphological complexity and perplexityreduction; e.g., Dutch and English have small ?Wand Estonian and Finnish large values.We provide the values of ?O for completeness.The composition of this catch-all group varies con-siderably from language to language.
For exam-ple, many words in this class are numbers with al-phabetic suffixes like ?2012-ben?
in Hungarian and393words with apostrophes in French.6 SummaryWe have investigated an interpolation of a KN modelwith a class language model whose classes are de-fined by morphology and shape features.
We testedthis model in a large crosslingual study of Europeanlanguages.Even though the model is generic and we usethe same architecture and features for all languages,the model achieves reductions in perplexity for all21 languages represented in the Europarl corpus,ranging from 3% to 11%, when compared to a KNmodel.
We found perplexity reductions across all21 languages for histories ending with four differenttypes of word shapes: alphabetical words, specialcharacters, and numbers.We looked at the sensitivity of perplexity reduc-tions to three parameters of the model: ?, a thresh-old that determines for which frequencies words aregiven their own class; ?, the number of suffixes usedto determine class membership; and morphologicalsegmentation.
We found that ?
has a considerableinfluence on the performance of the model and thatoptimal values vary from language to language.
Thisparameter should be tuned when the model is usedin practice.In contrast, the number of suffixes and the mor-phological segmentation method only had a smalleffect on perplexity reductions.
This is a surprisingresult since it means that simple identification of suf-fixes by frequency and choosing a fixed number ofsuffixes ?
across languages is sufficient for gettingmost of the perplexity reduction that is possible.7 Future WorkA surprising result of our experiments was that theperplexity reductions due to morphological classeswere generally better than those due to distributionalclasses even though distributional classes are formeddirectly based on the type of information that a lan-guage model is evaluated on ?
the distribution ofwords or which words are likely to occur in se-quence.
An intriguing question is to what extent theeffect of morphological and distributional classes isadditive.
We ran an exploratory experiment witha model that interpolates KN, morphological classmodel and distributional class model.
This modelonly slightly outperformed the interpolation of KNand morphological class model (column PPM in Ta-ble 3).
We would like to investigate in future work ifthe information provided by the two types of classesis indeed largely redundant or if a more sophisticatedcombination would perform better than the simplelinear interpolation we have used here.Acknowledgments.
This research was funded byDFG (grant SFB 732).
We would like to thank theanonymous reviewers for their valuable comments.ReferencesLalit R. Bahl, Peter F. Brown, Peter V. de Souza,Robert L. Mercer, and David Nahamoo.
1991.
A fastalgorithm for deleted interpolation.
In Eurospeech.Adam L. Berger, Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Comput.
Linguist.Jeff A. Bilmes and Katrin Kirchhoff.
2003.
Factoredlanguage models and generalized parallel backoff.
InNAACL-HLT.Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Comput.Linguist.Stanley F. Chen and Joshua Goodman.
1999.
An empir-ical study of smoothing techniques for language mod-eling.
Computer Speech & Language.Mathias Creutz and Krista Lagus.
2007.
Unsupervisedmodels for morpheme segmentation and morphologylearning.
ACM TSLP.Mathias Creutz, Teemu Hirsima?ki, Mikko Kurimo, AnttiPuurula, Janne Pylkko?nen, Vesa Siivola, Matti Var-jokallio, Ebru Arisoy, Murat Sarac?lar, and AndreasStolcke.
2007.
Morph-based speech recognitionand modeling of out-of-vocabulary words across lan-guages.
ACM TSLP.Samarth Keshava and Emily Pitler.
2006.
A simpler,intuitive approach to morpheme induction.
In PASCALMorpho Challenge.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT summit.Toma?s?
Mikolov, Martin Karafia?t, Luka?s?
Burget, JanC?ernocky?, and Sanjeev Khudanpur.
2010.
Recurrentneural network based language model.
In ICSLP.Thomas Mu?ller and Hinrich Schu?tze.
2011.
Improvedmodeling of out-of-vocabulary words using morpho-logical classes.
In ACL.394Ronald Rosenfeld.
1996.
A maximum entropy approachto adaptive statistical language modelling.
ComputerSpeech & Language.Hinrich Schu?tze and Michael Walsh.
2011.
Half-contextlanguage models.
Comput.
Linguist.Hinrich Schu?tze.
1992.
Dimensions of meaning.In ACM/IEEE Conference on Supercomputing, pages787?796.Andreas Stolcke.
2002.
SRILM - An extensible lan-guage modeling toolkit.
In Interspeech.Yee Whye Teh.
2006.
A hierarchical bayesian languagemodel based on Pitman-Yor processes.
In ACL.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.JAIR.Dimitra Vergyri, Katrin Kirchhoff, Kevin Duh, and An-dreas Stolcke.
2004.
Morphology-based languagemodeling for Arabic speech recognition.
In ICSLP.E.W.D.
Whittaker and P.C.
Woodland.
2000.
Particle-based language modelling.
In ICSLP.Peng Xu and Frederick Jelinek.
2004.
Random forests inlanguage modeling.
In EMNLP.Deniz Yuret and Ergun Bic?ici.
2009.
Modeling morpho-logically rich languages using split words and unstruc-tured dependencies.
In ACL-IJCNLP.395
