Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 195?200,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsCUNI in WMT14: Chimera Still Awaits BellerophonAle?s Tamchyna, Martin Popel, Rudolf Rosa, Ond?rej BojarCharles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied LinguisticsMalostransk?e n?am?est??
25, Prague, Czech Republicsurname@ufal.mff.cuni.czAbstractWe present our English?Czech andEnglish?Hindi submissions for thisyear?s WMT translation task.
ForEnglish?Czech, we build upon last year?sCHIMERA and evaluate several setups.English?Hindi is a new language pair forthis year.
We experimented with reverseself-training to acquire more (synthetic)parallel data and with modeling target-sidemorphology.1 IntroductionIn this paper, we describe translation systems sub-mitted by Charles University (CU or CUNI) to theTranslation task of the Ninth Workshop on Statis-tical Machine Translation (WMT) 2014.In ?2, we present our English?Czech systems,CU-TECTOMT, CU-BOJAR, CU-DEPFIX and CU-FUNKY.
The systems are very similar to our sub-missions (Bojar et al., 2013) from last year, themain novelty being our experiments with domain-specific and document-specific language models.In ?3, we describe our experiments withEnglish?Hindi translation, which is a translationpair new both to us and to WMT.
We unsuccess-fully experimented with reverse self-training and amorphological-tags-based language model, and soour final submission, CU-MOSES, is only a basicinstance of Moses.2 English?CzechOur submissions for English?Czech build uponlast year?s successful CHIMERA system (Bojaret al., 2013).
We combine several different ap-proaches:?
factored phrase-based Moses model (?2.1),?
domain-adapted language model (?2.2),?
document-specific language models (?2.3),?
deep-syntactic MT system TectoMT (?2.4),?
automatic post-editing system Depfix (?2.5).We combined the approaches in several waysinto our four submissions, as made clear by Ta-ble 1.
CU-TECTOMT is the stand-alone TectoMTtranslation system, while the other submissionsare Moses-based, using TectoMT indirectly to pro-vide an additional phrase-table.
CU-BOJAR usesa factored model and a domain-adapted languagemodel; in CU-DEPFIX, Depfix post-processing isadded; and CU-FUNKY also employs document-specific language models.CU-TECTOMTCU-BOJARCU-DEPFIXCU-FUNKYTectoMT (?2.4) D D D DFactored Moses (?2.1) D D DAdapted LM (?2.2) D D DDocument-specific LMs (?2.3) DDepfix (?2.5) D DTable 1: EN?CS systems submitted to WMT.2.1 Our Baseline Factored Moses SystemOur baseline translation system (denoted ?Base-line?
in the following) is similar to last year ?
wetrained a factored Moses model on the concatena-tion of CzEng (Bojar et al., 2012) and Europarl(Koehn, 2005), see Table 2.
We use two fac-tors: tag, which is the part-of-speech tag, and stc,which is ?supervised truecasing?, i.e.
the surfaceform with letter case set according to the lemma;see (Bojar et al., 2013).
Our factored Moses sys-tem translates from English stc to Czech stc | tagin one translation step.Our basic language models are identical to lastyear?s submission.
We added an adapted language195Tokens [M]Corpus Sents [M] English CzechCzEng 1.0 14.83 235.67 205.17Europarl 0.65 17.61 15.00Table 2: English?Czech parallel data.Corpus Sents [M] Tokens [M]CzEng 1.0 14.83 205.17CWC Articles 36.72 626.86CNC News 28.08 483.88CNA 47.00 830.32Newspapers 64.39 1040.80News Crawl 24.91 444.84Total 215.93 3631.87Table 3: Czech monolingual data.model which we describe in the following section.Tables 3 and 4 show basic data about the languagemodels.
Aside from modeling surface forms, ourlanguage models also capture morphological co-herence to some degree.2.2 Adapted Language ModelWe used the 2013 News Crawl to create a languagemodel adapted to the domain of the test set (i.e.news domain) using data selection based on infor-mation retrieval (Tamchyna et al., 2012).
We usethe Baseline system to translate the source sides ofWMT test sets 2012?2014.
The translations thenconstitute a ?query corpus?
for Lucene.1For eachsentence in the query corpus, we use Lucene toretrieve 20 most similar sentences from the 2013News Crawl.
After de-duplication, we obtained amonolingual corpus of roughly 250 thousand sen-tences and trained an additional 6-gram languagemodel on this data.Domain Factor Order Sents Tokens ARPA.gz Trie[M] [M] [GB] [GB]General stc 4 201.31 3430.92 28.2 11.8General stc 7 24.91 444.84 13.1 8.1General tag 10 14.83 205.17 7.2 3.0News stc 6 0.25 4.73 0.2 ?Table 4: Czech LMs used in CU-BOJAR.
The lastsmall model is described in ?2.2.1http://lucene.apache.org2.3 Document-Specific Language ModelsCU-FUNKY further extends the idea described in?2.2.
Taking advantage of document IDs whichare included in WMT development and test data,we split our dev- (WMT 13) and test-set (WMT14) into documents.
We translate each documentwith the Baseline system and use Lucene to re-trieve 10,000 most similar target-side sentencesfrom News Crawl 2013 for each document sen-tence.Using this procedure, we obtain a corpus foreach document.
On average, the corpora con-tain roughly 208 thousand sentences after de-duplication.
Each corpus then serves as thetraining data for the document-specific languagemodel.We implemented an alternative tomoses-parallel.perl which splits theinput corpus based on document IDs and runs aseparate Moses instance/job for each document.Moreover, it allows to modify the Moses config-uration file according to document ID.
We usethis feature to plant the correct document-specificlanguage model to each job.In tuning, our technique only adds one weight.In each split, the weight corresponds to a differ-ent language model.
The optimizer then hope-fully averages the utility of this document-specificLM across all documents.
The same weight is ap-plied also in the test set translation, exchanging thedocument-specific LM file.2.4 TectoMT Deep-Syntactic MT SystemTectoMT2was one of the three key componentsin last year?s CHIMERA.
It is a linguistically-motivated tree-to-tree deep-syntactic translationsystem with transfer based on Maximum Entropycontext-sensitive translation models (Mare?cek etal., 2010) and Hidden Tree Markov Models(?Zabokrtsk?y and Popel, 2009).
It is trained onthe WMT-provided data: CzEng 1.0 (parallel data)and News Crawl (2007?2012 Czech monolingualsets).We maintain the same approach to combiningTectoMT with Moses as last year ?
we translateWMT test sets from years 2007?2014 and usethem as additional synthetic parallel training data ?a corpus consisting of the test set source side (En-glish) and TectoMT output (synthetic Czech).
Wethen use the standard extraction pipeline to create2http://ufal.mff.cuni.cz/tectomt/196an additional phrase table from this corpus.
Thetranslated data overlap completely both with ourdevelopment and test data for Moses so that tuningcan assign an appropriate weight to the syntheticphrase table.2.5 Depfix Automatic Post-EditingAs in the previous years, we used Depfix (Rosa,2013) to post-process the translations.
Depfix isan automatic post-editing system which is mainlyrule-based and uses various linguistic tools (tag-gers, parsers, morphological generators, etc.)
todetect and correct errors, especially grammaticalones.
The system was slightly improved since lastyear, and a new fixing rule was added for correct-ing word order in noun clusters translated as geni-tive constructions.In English, a noun can behave as an adjective,as in ?according to the house owners?, while inCzech, this is not possible, and a genitive construc-tion has to be used instead, similarly to ?accordingto the owners of the house?
?
the modifier is in thegenitive morphological case and follows the noun.However, SMT systems translating into Czech donot usually focus much on word reordering, whichleads to non-fluent or incomprehensible construc-tions, such as ?podle domugenvlastn??k?ugen?
(ac-cording to-the-house of-the-owners).
Fortunately,such cases are easy to distinguish with the helpof a dependency parser and a morphological tag-ger ?
genitive modifiers usually do not precede thehead but follow it (unless they are parts of namedentities), so we can safely switch the word orderto the correct one: ?podle vlastn??k?ugendomugen?
(according to-the-owners of-the-house).2.6 ResultsWe report scores of automatic metrics as shown inthe submission system,3namely (case-sensitive)BLEU (Papineni et al., 2002) and TER (Snoveret al., 2006).
The results, summarized in Ta-ble 5, show that CU-FUNKY is the most success-ful of our systems according to BLEU, whilethe simpler CU-DEPFIX wins in TER.
The re-sults of manual evaluation suggest that CU-DEPFIX(dubbed CHIMERA) remains the best performingEnglish?Czech system.In comparison to other English?Czech sys-tems submitted to WMT 2014, CU-FUNKY rankedas the second in BLEU, and CU-DEPFIX ranked3http://matrix.statmt.org/as the second in TER; the winning system, ac-cording to both of these metrics, was UEDIN-UNCONSTRAINED.System BLEU TER ManualCU-DEPFIX 21.1 0.670 0.373UEDIN-UNCONSTRAINED 21.6 0.667 0.357CU-BOJAR 20.9 0.674 0.333CU-FUNKY 21.2 0.675 0.287GOOGLE TRANSLATE 20.2 0.687 0.168CU-TECTOMT 15.2 0.716 -0.177CU-BOJAR +full 2013 news 20.7 0.677 ?Table 5: Scores of automatic metrics and results ofmanual evaluation for our systems.
The table alsolists the best system according to automatic met-rics and Google Translate as the best-performingcommercial system.Our analysis of CU-FUNKY suggests that it isnot the best performing system on average (de-spite achieving the highest BLEU scores from oursubmissions), but that it is rather the most volatilesystem.
Some sentences were obviously improvedcompared to CU-BOJAR but most got degraded es-pecially in adequacy.
We are well aware of themany shortcomings our current implementationhas, the most severe of which lie in the sentenceselection by Lucene.
For instance, we do not useany stopwords or keyword detection methods, andalso pretending that each sentence in our monolin-gual corpus is a ?document?
for the informationretrieval system is far from ideal.We also evaluated a version of CU-BOJAR whichuses not only the adapted LM but also an addi-tional LM trained on the full 2013 News Crawldata (see ?CU-BOJAR +full 2013 news?
in Table 5)but found no improvement compared to using justthe adapted model (trained on a subset of the data).3 English?HindiEnglish-Hindi is a new language pair thisyear.
We submitted an unconstrained system forEnglish?Hindi translation.We used HindEnCorp (Bojar et al., 2014) as thesole source of parallel data (nearly 276 thousandsentence pairs, around 3.95 million English tokensand 4.09 million Hindi tokens).Given that no test set from previous years wasavailable and that the size of the development setprovided by WMT organizers was only 500 sen-tence pairs, we held out the first 5000 sentencepairs of HindEnCorp for this purpose.
Our de-velopment set then consisted of the 500 provided197Corpus Sents [M] Tokens [M]NewsCrawl 1.27 27.27HindEnCorp 0.28 4.09HindMonoCorp 43.38 945.43Total 44.93 976.80Table 6: Hindi monolingual data.sentences plus 1500 sentence pairs from HindEn-Corp.
The remaining 3500 sentence pairs takenfrom HindEnCorp constituted our test set.As for monolingual data, we used the NewsCrawl corpora provided for the task and the newmonolingual HindMonoCorp, which makes oursubmission unconstrained.
Table 6 shows statis-tics of our monolingual data.We tagged and lemmatized the English data us-ing Mor?ce (Spoustov?a et al., 2007) and the Hindidata using Siva Reddy?s POS tagger.43.1 Baseline SystemThe baseline system was eventually our best-performing one.
Its design is completely straight-forward ?
it uses one phrase table trained onall parallel data (we translate from ?supervised-truecased?
English into Hindi forms) and one 5-gram language model trained on all monolingualdata.
We used KenLM (Heafield et al., 2013) forestimating the model as the data was rather large(see Table 6).We used GIZA++ (Och and Ney, 2000) asour word alignment tool.
We experimented withseveral coarser representations to make the finalalignment more reliable.
Table 7 shows the re-sults.
The factor ?stem4?
refers to simply takingthe first four characters of each word.
For lem-mas, we used the outputs of the tools mentionedabove.
However, lemmas as output by the Hinditagger were not much coarser than surface forms?
the ratio between the number of types is merely1.11 ?
so we also tried ?stemming?
the lemmas(lemma4).
Of these variants, stem4-stem4 align-ment worked best and we used it for the rest of ourexperiments.3.2 Reverse Self-TrainingBojar and Tamchyna (2011) showed a simple tech-nique for improving translation quality in situa-tions where there is only a small amount of par-4http://sivareddy.in/downloads#hindi_toolsEnglish Hindi BLEUstem4 stem4 22.96?1.17lemma lemma4 22.59?1.17lemma lemma 22.41?1.20Table 7: Comparison of different factor combina-tions for word alignment.allel data available but where there is a sufficientquantity of target-side monolingual texts.
The so-called ?reverse self-training?
uses a factored sys-tem trained in the opposite direction to translatethe large monolingual data into the source lan-guage.
The translation (in the source language,i.e.
English in our case) and the original target-side data (Hindi) can be used as additional syn-thetic parallel data.
The authors recommend creat-ing a separate phrase table from it and combiningthe two translation models as alternatives in thelog-linear model (letting tuning weigh their impor-tance).The factored setup of the reverse system(Hindi?English) is essential ?
alternative decod-ing paths with a back-off to a coarser representa-tion (e.g.
stems) on the source side (Hindi) givethe system the ability to generalize beyond surfaceforms observed in the training data.
The main aimof this technique is to learn new forms of knownwords.The technique is thus aimed at translating into amorphologically richer language than the source.Indeed, the authors showed that if the target lan-guage has considerably more word types than thesource, the gains achieved by reverse self-trainingare higher.
In this respect, English?Hindi is notan ideal candidate given that the ratio we observedis only 1.2.The choice of back-off representation is impor-tant.
We measure the vocabulary reduction ofseveral options and summarize the results in Ta-ble 8.
E.g.
for stem4, the vocabulary size isroughly 30% compared to the number of surfaceword forms.Bojar and Tamchyna (2011) achieved the bestresults using ?nosuf3?
(?suffix trimming?, i.e.
cut-ting of the last 3 characters of each word); how-ever, they experimented with European languagesand the highest reduction of vocabulary reportedin the paper is to roughly one half.
In our case, thevocabulary is reduced much more, so we opted fora more conservative back-off, namely ?nosuf2?.198Back-off % of vocab.
sizestem4 30.21lemma4 32.36nosuf3 36.36nosuf2 50.76stem5 53.48lemma5 57.47lemma 90.09Table 8: Options for back-off factors in reverseself-training and the percentage of their vocabu-lary size compared to surface forms.We translated roughly 2 million sentences fromthe Hindi monolingual data, focusing on newsto maintain a domain match with the WMT testset.
However, adding the synthetic phrase tabledid not bring any improvement and in fact, theBLEU score dropped to 22.37?1.17 (baseline is22.96?1.17).We can attribute the failure of reverse self-training to the nature of the language pair at hand.While Hindi has some synthetic properties (e.g.future tense of verbs or inflection of adjectives aremarked by suffixes), its inflectional morphemesare realized mainly by post-positions which areseparated from their head-words.
Overlooking thisessential property, we attempted to use reverseself-training but our technique could contributeonly very little.3.3 Target-Side MorphologyWe also experimented with a setup that tradition-ally works very well for English?Czech trans-lation: using a high-order language model onmorphological tags to explicitly model target-sidemorphological coherence in translation.
We usedthe same monolingual data as for the baseline lan-guage model; however, the order of our morpho-logical language model was set to 10.This setup also brought no improvement overthe baseline ?
in fact, the BLEU score droppedeven further to 22.27?1.14.4 ConclusionWe presented our contributions to the Translationtask of WMT 2014.As we have focused on English?Czech trans-lation for many years, we have developed sev-eral complex and well-performing systems for it?
an adaptation of the phrase-based Moses sys-tem, a linguistically-motivated syntax-based Tec-toMT system, and an automatic post-editing Dep-fix system.
We combine the individual systemsusing a very simple yet effective method and thecombined system called CHIMERA confirmed itsstate-of-the-art performance.For English?Hindi translation, which was anew task for us, we managed to get competitiveresults by using a baseline Moses setup, but wereunable to improve upon those by employing ad-vanced techniques that had proven to be effectivefor other translation directions.AcknowledgmentsThis research was supported by the grants FP7-ICT-2013-10-610516 (QTLeap), FP7-ICT-2011-7-288487 (MosesCore), SVV 260 104. andGAUK 1572314.
This work has been using lan-guage resources developed, stored and distributedby the LINDAT/CLARIN project of the Ministryof Education, Youth and Sports of the Czech Re-public (project LM2010013).ReferencesOnd?rej Bojar and Ale?s Tamchyna.
2011.
ImprovingTranslation Model by Monolingual Data.
In Proc.of WMT, pages 330?336.
ACL.Ond?rej Bojar, Zden?ek?Zabokrtsk?y, Ond?rej Du?sek, Pe-tra Galu?s?c?akov?a, Martin Majli?s, David Mare?cek, Ji?r??Mar?s?
?k, Michal Nov?ak, Martin Popel, and Ale?s Tam-chyna.
2012.
The Joy of Parallelism with CzEng1.0.
In Proc.
of LREC, pages 3921?3928.
ELRA.Ondrej Bojar, Rudolf Rosa, and Ale?s Tamchyna.
2013.Chimera ?
Three Heads for English-to-Czech Trans-lation.
In Proceedings of the Eighth Workshop onStatistical Machine Translation, pages 90?96.Ond?rej Bojar, Vojt?ech Diatka, Pavel Rychl?y, PavelStra?n?ak, V?
?t Suchomel, Ale?s Tamchyna, and DanielZeman.
2014.
HindEnCorp ?
Hindi-English andHindi-only Corpus for Machine Translation.
Reyk-jav?
?k, Iceland.
European Language Resources Asso-ciation.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable ModifiedKneser-Ney Language Model Estimation.
In Proc.of ACL.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Machine Trans-lation Summit X, pages 79?86.David Mare?cek, Martin Popel, and Zden?ek?Zabokrtsk?y.2010.
Maximum entropy translation model in199dependency-based MT framework.
In Proc.
of WMTand MetricsMATR, pages 201?206.
ACL.Franz Josef Och and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
In Proc.
of ACL,pages 440?447, Hong Kong.
ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
of ACL,pages 311?318, Stroudsburg, PA, USA.
ACL.Rudolf Rosa.
2013.
Automatic post-editing of phrase-based machine translation outputs.
Master?s thesis,Charles University in Prague, Faculty of Mathemat-ics and Physics, Praha, Czechia.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A studyof translation edit rate with targeted human annota-tion.
In In Proceedings of Association for MachineTranslation in the Americas, pages 223?231.Drahom?
?ra Spoustov?a, Jan Haji?c, Jan Votrubec, PavelKrbec, and Pavel Kv?eto?n.
2007.
The best of twoworlds: Cooperation of statistical and rule-basedtaggers for Czech.
In Proceedings of the Work-shop on Balto-Slavonic Natural Language Process-ing, ACL 2007, pages 67?74, Praha.Ale?s Tamchyna, Petra Galu?s?c?akov?a, Amir Kamran,Milo?s Stanojevi?c, and Ond?rej Bojar.
2012.
Select-ing Data for English-to-Czech Machine Translation.In Proceedings of the Seventh Workshop on Statis-tical Machine Translation, WMT ?12, pages 374?381, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Zden?ek?Zabokrtsk?y and Martin Popel.
2009.
HiddenMarkov Tree Model in Dependency-based MachineTranslation.
In Proc.
of ACL-IJCNLP Short Papers,pages 145?148.200
