Evaluating an Ontology-Driven WYSIWYM InterfaceFeikje Hielkema Chris MellishComputing ScienceSchool of Natural & Computing SciencesUniversity of AberdeenAberdeen, AB24 3FX, UK{f.hielkema, c.mellish, p.edwards}@abdn.ac.ukPeter EdwardsAbstractThis paper describes an evaluation study ofan ontology-driven WYSIWYM interface formetadata creation.
Although the results areencouraging, they are not as positive as thoseof a similar tool developed for the medicaldomain.
We believe this may be due, not tothe WYSIWYM interface, but to the complex-ity of the underlying ontologies and the factthat subjects were unfamiliar with them.
Wediscuss the ways in which ontology develop-ment might be influenced by issues stemmingfrom using an NLG approach for user accessto data, and the effect these factors have ongeneral usability.1 IntroductionIn the PolicyGrid1 project we are investigating howbest to support social science researchers throughthe use of Semantic Grid (De Roure et al, 2005)technologies.
The Semantic Grid is often describedas an ?extension of the current Grid in which infor-mation and services are given well-defined mean-ing, better enabling computers and people to workin cooperation?.
Semantic Grids thus not only sharedata and compute resources, but also share and pro-cess metadata and knowledge, e.g.
through the useof RDF2 (Resource Description Framework, a meta-data model for making statements about resources)1Funded under the UK Economic and Social ResearchCouncil e-Social Science programme; grant reference RES-149-25-1027 (http://www.policygrid.org)2http://www.w3.org/RDF/or OWL3 (knowledge representation language forauthoring ontologies).Numerous e-science applications rely on meta-data descriptions of resources.
But how does meta-data come into existence?
Ideally the user shouldcreate it.
However, metadata creation is a complextask, and few users know how to create them in RDF.To enable our users to describe their resources, weneed to provide a tool that facilitates creation, query-ing and browsing of metadata by users with no priorexperience of such technologies.Existing tools that provide access to RDF meta-data are often graphical, e.g.
(Handschuh et al,2001; Catarci et al, 2004).
However, we believethat, for social scientists, natural language is thebest medium to use, as the way they conduct theirresearch and the structure of their documents anddata indicate that they are more oriented towards textthan graphics.
Natural language approaches includeGINO (Bernstein and Kaufmann, 2006), an ontol-ogy editor with an approach reminiscent of Natu-ral Language Menus (Tennant et al, 1983), and us-ing Controlled languages such as PENG-D (Schwit-ter and Tilbrook, 2004).
Such natural language ap-proaches tend to restrict expressivity to ensure thatevery entry can be parsed, limiting the language andoften making it stilted, so that there is a small learn-ing curve before the user knows which structures areallowed.
In order to maintain full expressivity andto shorten the learning curve, we have elected to useWYSIWYM (What You See Is What You Meant)(Power et al, 1998).
This is a natural language gen-eration approach where the system generates a feed-3http://www.w3.org/TR/owl-features/138back text for the user that is based on a semantic rep-resentation.
This representation is edited directly bythe user by manipulating the feedback text.
WYSI-WYM has been used by a number of other projects,such as MILE (Piwek et al, 2000) and CLEF (Hal-lett, 2006).
As evaluation results in both of theseprojects were very positive (Piwek, 2002; Hallett etal., 2007), we felt that WYSIWYM would be a suit-able approach to use in our work.We have developed a metadata elicitation tool thatenables users to create metadata in the shape of on-tology instance data; the tool is driven by the on-tologies that define those instances.
We are currentlyimplementing a WYSIWYM tool for querying, thatuses the same interface as the metadata creation tool.We also aim to develop a tool for presenting theresults of the query, and for browsing the descrip-tions in the database.
These three tools will be inte-grated into one consistent interface, so that users canswitch effortlessly between querying, browsing andediting ontology instance data.
This aim is similarto the support that the graphical tool SHAKEN pro-vides for ontology editing and browsing (Thome?re?et al, 2002).
We want to ensure that these tools aregeneric, so that if the ontologies change over timeor are replaced, the tools will still function.
Thatmeans that all domain specific information (as muchas is possible) should be contained in the ontologies.In this paper we explore the ways in which Natu-ral Language Generation issues influence ontologybuilding and vice versa.This paper is structured as follows: section 2 de-scribes the tool for metadata creation that we haveimplemented; section 3 discusses issues in ontol-ogy development and Natural Language Generation;and section 4 presents an evaluation study of themetadata creation tool.
In section 5 the results ofthis study are discussed and compared to those ofthe CLEF project; we argue that different domainsand ontologies affect the usability and complexityof metadata access interfaces.2 The Metadata Creation ToolWe have developed a WYSIWYM tool that enablesusers to upload resources (e.g.
acadamic papers,statistical datasets, interview transcripts) and createmetadata descriptions for them, even if these usersFigure 1: The Metadata Creation Tool.are unfamiliar with ontologies.
First, the user selectsthe type of resource he is depositing (e.g.
a Tran-script).
The tool then generates a brief feedback textthat presents the information specified by the user.The feedback text contains anchors, phrases in redboldface and blue italics that signal where new in-formation can be added.
When the user clicks on ananchor, a menu pops up listing the kinds of infor-mation that can be added here (see Figure 1).
Afterselecting a menu item, the user is prompted to enteran appropriate value; this may be a date, a free-textstring, or another object that may or may not be inthe text already.
The feedback text is regeneratedwhenever the user has added some information.The tool is driven by one or more ontologies.Their class hierarchies are presented when users areselecting a resource type, or creating a new objectas range for a property.
The anchors correspond toindividuals in the ontology; the menu items to theproperties of those individuals.
The feedback text isdivided into paragraphs which correspond to the in-dividuals; each property of an individual is realisedas (part of) a sentence in its paragraph.
Each prop-erty in the ontology is associated with a linguisticspecification, a Dependency Tree (Mel?cuk, 1988)that corresponds to a sentence.
The specificationhas slots where the source and target of the prop-erty should be inserted, and is sufficiently detailedto support processes such as aggregation, throughwhich the feedback text is made more fluent.
Fora more extensive description of the metadata cre-139ation tool and its implementation, see Hielkema etal.
(2007b).In August 2007 we ran a pilot evaluation study(Hielkema et al, 2007a) on this tool.
This study washeuristic in nature, with subjects discussing the in-terface with the experimenter while performing settasks.
It highlighted a number of aspects whichwe felt it was necessary to improve before embark-ing on the formal evaluation.
Apart from there be-ing standard usability considerations such as a needfor better undo and help functions, it became evi-dent that the underlying ontology was neither ex-tensive enough nor sufficiently well-structured: sub-jects struggled to find the options they needed, andwere often not satisfied with the options?
names ortheir location in the sub-menus.
We therefore de-cided that, as well as improving the basic usabilityof the interface, we needed to redevelop the ontol-ogy that was driving the interface.
Users, we felt,would find it easier to navigate the menus whenthis ontology matched their mental model of the do-main.
Throughout the development of this new on-tology, user requirements and feedback were gath-ered through a number of focus group sessions.
Thenext section describes the ways in which this ontol-ogy development was affected by the demands of themetadata interface.3 Ontologies in NLGPortability has always been a major issue in NLG.Language generation involves the use of much infor-mation that is domain-specific, and cannot be gener-alised without a cost in the expressivity of the result-ing text.
If we want to create an application that isdomain-independent, we have to find a way to storeall domain-specific information in a structure that iseasily extended or replaced.We have decided to use an ontology, a com-mon structure whose use has become widespread inknowledge representation.
Ideally, we would like tocreate a generator that can be applied to any domain,provided there is an appropriate domain-specific on-tology.
But what information should such an ontol-ogy contain?
How should it be structured?
In thissection we explore issues that occur when devel-oping or adapting ontologies for use in the WYSI-WYM tool; we believe that this can at least in partbe generalised to NLG.
The ontologies we have usedso far were developed at the same time as the WYSI-WYM tool, so that both tool and ontology influencedeach other?s development.
We are currently adaptingan ontology from another e-science project for usein our WYSIWYM interface, to further investigatesuch issues (see section 5).There are a number of existing tools that generatelanguage from ontologies, using various approaches.Wilcock (2003) describes an ontology verbaliser us-ing XML-based generation.
As Wilcock states, hisapproach is domain-specific, and therefore probablyincompatible with more general ontologies (and pre-sumably with ontologies from a different domain).MIAKT (Bontcheva and Wills, 2004) is a sys-tem that generates textual medical reports from anRDF description.
It uses a medical domain ontol-ogy and an NLG lexicon that contains lexicalisa-tions for the concepts and instances in the ontol-ogy.
In order to verbalise properties, MIAKT?s sur-face realiser needs lexical specifications for them.Four basic property types are distinguished whosesub-properties can mostly be realised automaticallythrough the grammar rules in the realiser.
This tech-nique increases the portability of the system, butdoes affect the variability and expressivity of thegenerator.We do not aim to generate from any ontology ina domain, but to generate texts with high expressiv-ity and clarity from ontologies that are designed inan ?NLG-aware?
way.
We are investigating what re-quirements an ontology has to meet in order to beusable for our application, so that for any domainan ontology can be built or adapted which we canuse to produce a usable NL-interface.
As many on-tology developers are not linguists, ideally we wantto support the adaptation to ?NLG-aware?
ontologieswithout requiring linguistic expertise, for instancethrough a supporting software tool.
Ontologies areprimarily built to model domain-specific knowledge,making domain assumptions explicit, and to facili-tate reasoning with this knowledge.
These aims maysometimes conflict with the requirements of NLGapplications, but they do frequently coincide (e.g.the need for clear, unambiguous resource names.1403.1 Domain Ontologies for WYSIWYMWhat information does our WYSIWYM applicationneed its ontologies to provide?
First of all, the partsof it that will be shown to the user need to be eas-ily mapped to natural language.
The purpose ofthe tool is to support creation of ontology instancedata by users unfamiliar with ontologies, so the partsthey see should be comprehensible to novices.
Thenames of properties are used to populate the pop-up menus, while the class names are shown in theclass hierarchy.
These names are mapped to natu-ral language by replacing capitals and underscoreswith whitespace, and if necessary adding a deter-miner.
Therefore, they need to correspond to phrasesin natural language in order to be understood by theuser, with individual words separated by capitals orunderscores.
If there is no intuitive NL-phrase torepresent a class, it probably does not correspond toa concept in the domain either and might confusethe user, so it should be removed from the hierarchy.Classes whose instances are best presented by somedistinctive name (e.g.
Person or Paper) should havea name or title property whose value can be used(e.g.
?John?).
For other classes (e.g.
Interview), theclass name can be used (e.g.
?some interview?
).We need a linguistic specification for each prop-erty, sufficiently detailed to support aggregation andpronominalisation, but also to produce more thanone surface form: a query is presented differentlythan a description, even if it contains the same infor-mation (compare the texts in Figure 1 and 2).
Thelinguistic specification should be sufficiently richto support the generation of these different surfaceforms.
For this purpose we are using DependencyTrees, whose richness in both syntactic and semanticinformation provides ample support for such trans-formations.
These trees can be associated with thedomain ontology4.
This specification also containsthe header of the submenu in which the propertyshould appear.Some peculiarities in natural language aredomain-independent.
For instance, an address ispresented in a very specific way and cannot be re-alised in the standard manner without sacrificing4For an example of how this is done, seehttp://www.csd.abdn.ac.uk/research/policygrid/ontologies/Lexicon/Lexicon.owlFigure 2: The Query Tool.clarity (e.g.
?The address?
street is Union Street.Its place is Aberdeen?).
Such ?utility?
classes areused across domains.
In PolicyGrid we have cre-ated a utility ontology that contains classes such as?Person?, ?Address?
and ?Date?
5.
Instances of theseclasses are generated to a special surface form.
In or-der to get the best realisation from the WYSIWYMtool, domain ontologies should use the classes fromthis utility ontology.
As the properties of the utilityclasses are already furnished with linguistic specifi-cations, they are already NLG-aware.
Another wayto hasten the process is to use, where possible, prop-erties from this ontology instead of those from thedomain ontology.3.2 WYSIWYM for OntologiesWhat should the WYSIWYM application do in or-der to provide access to ontologies?
For metadatacreation it is essential that users can only produce?correct?
metadata, which does not violate the con-straints in the ontology.
The feedback text shouldbe presented coherently, while the Text Planner onlyuses information that is either domain independentor present in the ontology.
Perhaps most impor-tantly, the application should support easy creationof the linguistic information that the ontology mustcontain, as we cannot expect ontology developers tohave the linguistic expertise to create DependencyTrees.
We are devising a way for users to cre-ate a specification by manipulating the surface formof a ?template?
specification.
We currently have12 templates which represent commonly used sen-5http://www.policygrid.org/utility.owl141tences to present ontology properties in text.
Theuser can fine-tune the surface form by adding adjec-tives, changing morphological information and theroot of individual words; actions for which only abasic linguistic knowledge is needed.
This approachis outlined in more detail in (Hielkema et al, 2007b).The main challenge with this approach is that thespecification is used to generate two surface forms;it remains to be seen whether a specification that isfine-tuned through one surface form will accommo-date the accurate generation of another.The Penman Upper Model (Bateman, 1990) sup-ports the specification of linguistic informationthrough a different approach.
The Upper Model is adomain-independent ontology that supports sophis-ticated NLP.
To make a domain ontology availablefor NLP, its resources have to be placed in the hier-archy of the Upper Model; their place there deter-mines their surface realisation.
This task appears torequire considerable linguistic expertise, but like thecreation of our Dependency Trees could probably bemade easier for non-linguists through some special-purpose interface.4 Usability EvaluationThe best evaluation of our tool would be to let usersdeposit their resources in real-life contexts, but ourtool is not ready for a full deployment.
Another waywould be to compare its usability to another meta-data creation tool in an experiment where users com-pleted the same tasks with both tools.
Unfortunately,most metadata tools focus on providing support forontology editing (e.g.
Prote?ge?6 or GINO (Bern-stein and Kaufmann, 2006)), or query formulation(e.g.
SEWASIE (Catarci et al, 2004)).
A numberof tools for metadata creation use formal (RDF) orcontrolled languages, which are difficult to use forthose wholly unfamiliar with formal logic.
Othertools were developed for one specific purpose, e.g.CREAM (Handschuh et al, 2001) which was de-veloped for the annotation of web pages, and couldnot easily be adapted to our purposes.
We were notaware of any tool that we could adapt to the e-socialscience ontologies and thus use in an experiment.Alternatively, we could have compared our inter-face to direct authoring of RDF; but in an environ-6http://protege.stanford.edu/ment where most users have no experience of on-tologies or metadata this seemed spurious.
Instead,we adopted an approach similar to that used in theCLEF project (Hallett et al, 2007).
They evaluatedtheir WYSIWYM system (which enabled users tocreate SQL queries for a database in a medical do-main) by measuring the performance of fifteen sub-jects on four consecutive tasks, after a brief intro-duction.
These subjects were all knowledgeable inthe domain, and all but two knew the representationlanguage of the repository and how the data con-tained in it was structured.
These subjects achievedperfect results from the second task onwards, andbecame faster with each task, especially after thefirst.
We also expected users to become faster andmore accurate with each completed task, and indeedhoped for perfect scores on their last task.Subjects Sixteen researchers and PhD studentsfrom various social science-related disciplines par-ticipated in the experiment.
None of them had priorexperience of the metadata elicitation interface, andonly two of the subjects had any previous experi-ence of using ontologies.
The ontology driving thesystem models the description of social science re-sources and was based on requirements gatheringsessions, in which a few subjects had participated.None of the subjects knew its precise structure.Methodology After providing some informationabout their background, subjects viewed a video in-troduction7 of six minutes.
This video showed theconstruction of a simple resource description, high-lighting the main functionalities of the interface,while a voice-over explained what was happeningon the screen.Subjects were then handed four short resource de-scriptions expressed as paragraphs of English (see?Materials?)
and asked to reproduce these descrip-tions as closely as possible using the tool.
To avoidmaking the choice of the correct options too obvi-ous, we tried to avoid phrases that corresponded lit-erally to those in the menus.
Each subject receivedthe descriptions in a different order, in case therewere differences in the complexity of the tasks.
Sub-jects were allowed as much time as they needed to7This video can be viewed at http://www.csd.abdn.ac.uk/ re-search/policygrid/demos/WysiwymIntroduction1.mov142Task order Completion time Operations Total errors Avoidable errors?
?
?
?
?
?
?
?First 512.81 366.132 48.38 24.527 3.31 1.922 1.56 .727Second 329.50 95.716 37.75 12.228 2.69 2.442 1.38 .957Third 260.06 90.542 35.13 9.749 2.75 2.720 1.63 1.310Fourth 309.81 106.049 39.38 10.844 2.00 1.966 1.44 1.504Table 1: Mean completion times, operations and errors per completed task.complete each task.For each task, the tool recorded the completiontime, the produced description, the number of op-erations used to produce it, and the frequency withwhich various operation types were used, such as?undo?
or the ?help?
functions.
After the subjectshad completed all four tasks, they were asked to ratethe usability (very difficult - difficult - OK - easy -very easy) and usefulness (useless - not much use- adequate - useful - very useful) of the tool on afive-point Likert scale, and to note any feedback theymight have.
The entire experiment took on average50 min.
per subject.Materials We used four resource descriptions,one of which was:You are depositing the transcript of aninterview that was held by Dr. Riversin 1907, at Eddystone.
The interviewmainly discussed ?male-female relation-ships?, ?burial practices?
and ?the socialimpact of the interdiction on head hunt-ing?.
Access to this transcript should re-main private.Figure 1 shows the corresponding description thatcould be produced with the tool.
The separation ofthe transcript from the interview is an example of theclear distinctions necessary for knowledge represen-tation.
In natural language, this distinction wouldnot necessarily be made, and indeed this step wasmissed by a number of users.To ensure that tasks did not repeat identical sub-tasks, we tried to use different parts of the ontol-ogy in each task.
Every task described a differ-ent resource type (conference paper, transcript, aca-demic paper, report), which corresponded to a dif-ferent class in the ontology.
We were also careful tochoose varying menu items (corresponding to prop-erties in the ontology), although some repetition wasunavoidable (e.g.
specifying names).
In fact, a real-life use of the tool would involve rather more taskrepetition (specifying titles, authors and dates wouldbe necessary for practically any resource) than theartificial descriptions in this study.Results To analyse the accuracy of the produceddescriptions, we divided each description task into 8to 10 subtasks.
For the task shown in the previousparagraph, these subtasks were:?
Specify that you are depositing a ?Transcript??
Specify that access is private?
Specify that it is a transcript of an ?Interview?
(creating an ?interview?
object)?
Specify the three main topics?
Add an interviewer (creating a ?Person?object)?
Call this person ?Dr.
Rivers??
Specify the location of the interview?
Specify the date of the interviewAs some subtasks are more complicated than oth-ers and take longer, we did not try to give each taskexactly the same number of subtasks, but instead en-sured that all tasks needed the same number of op-erations (e.g.
menu item selections, button clicks,etc.)
in order to be completed.
Each subtask that wasmissing or completed differently than in the descrip-tion shown in ?Materials?
was counted as one er-ror.
Erroneous ways to complete subtasks includedchoosing a different menu item and adding informa-tion to the wrong object.
For instance, a number ofsubjects, instead of specifying an interviewer for theinterview, added a creator for the transcript; this was143counted as one erroneously completed subtask, andtherefore one error.The list of subtasks above shows that some sub-tasks depend on the successful completion of othertasks; for instance, you cannot add an interviewerunless you have created an ?interview?
object.
Wetherefore analysed two error counts: the total num-ber of errors, and the ?avoidable?
errors.
The ?avoid-able?
errors were the total number of errors minusthose subtasks that depended on another subtask thatwas missing or had been completed incorrectly.We analysed the mean completion times, numberof operations used and the two error counts of thetasks that were completed first, second, third andlast, using a repeated measures ANOVA (see Table 1for the means and standard deviations).
Mean com-pletion times went down significantly (Huynh-Feldtp-value < 0.01).
Tukey?s HSD post-hoc (applied toa univariate ANOVA, with task order as the indepen-dent variable) test shows that both the third (p-value< 0.01) and the fourth (p-value 0.030) were com-pleted significantly faster than the first task.
How-ever, no significant differences were found for thenumber of operations (Huynh-Feldt p-value 0.062),the total number of errors (Huynh-Feldt p-value.322) or the number of avoidable errors (Huynh-Feldt p-value .931).Subject feedback on the tool was positive: it wasperceived as useful (?
3.94; 1=?useless?, 5=?veryuseful?
), and OK or easy to use (?
2.69; 1=?veryeasy?, 5=?very difficult?).
Five subjects expressed apreference for a form-based interface, and five oth-ers for a NL-interface such as the one tested.
Infeedback, subjects indicated a desire for more form-based elements in the interface, to speed up thecreation of the standard description elements (e.g.name/title, author), and complained that the envi-ronment was initially unfamiliar, with some menuitems overlapping.
This unfamiliarity meant thatitems that were necessary to complete the descrip-tion were often overlooked; subjects often solvedthis by choosing the closest approximation theycould find, e.g.
?creator?
instead of ?interviewer?.5 Discussion and Future WorkAlthough users quickly gained speed using the tool,and were positive in their feedback, the evaluationresults are not nearly as positive as those found forCLEF (see section 4).
The mean number of errorsdecreased, but this effect was not significant andonly five out of sixteen subjects received a perfectscore on the last task (four other subjects performedsome earlier task(s) perfectly).
Evidently there isa difference in usability of both tools - but whatcauses it?
No doubt the difference can partly be as-cribed to differences in the implementation of theinterface.
However, the most common feedback wereceived from the subjects was that they were over-whelmed by the large number of options available tothem.
Each class in the social science ontology hason average 30 properties, which means a descrip-tion with three objects provides 90 options.
In con-trast, the number of available options in the CLEFsystem was deliberately kept small (max.
three) for?non-terminal anchors?.
Especially in the first task,users had trouble finding the option they wanted,and although it became easier in the later tasks asthey familiarised themselves with the system, theresults indicate that it remained a problem.
Thiswas likely aggravated by our deliberate avoidanceof subtask-repetition; more standard descriptions,which always involve titles and authors, might haveproduced a greater learning effect.
CLEF was de-veloped for a medical domain, which is well definedand understood by the experimental subjects.
Thesocial science domain encompasses many differenttheories and concepts, not just about what subjectsare investigated, but also about how the researchshould be conducted.
PolicyGrid has tried to de-velop an ontology that the different disciplines insocial science could be satisfied with.
As a result,it is quite large and complex, and most users willonly recognise parts of it.
Thus the number of avail-able options in the tool driven by this ontology islarge, and users have to explore the ontology andlearn to navigate it where their domain knowledgedoes not suffice.
This flattens the learning curve anddecreases the usability of the tool.Half the users preferred a form-based interfaceover an NLG interface.
Although forms are an eas-ily understood mechanism which are just as familiarto users as natural language, we have three reasonsfor preferring the WYSIWYM approach.
First, thelarge number of options in the ontology means thata form would reach truly daunting proportions.
Sec-144ond, we want our resource descriptions to be con-nected through shared people, projects, institutions,etc; using the expressivity that RDF offers us.
Thiswould be more difficult to achieve in an interfacewhere the user completes a form by providing eachproperty with a free-text description.
Thirdly, formscan be confusing for the user as well; the brief de-scriptions provided for each element are frequentlyambiguous and therefore misunderstood.
An NLG-interface, which provides feedback by presenting theproperty in a complete sentence, should help to clar-ify the meaning of the property name for the user.As we discussed earlier, there are many con-straints on the development of domain ontologiesthat can be accessed through NLG, and the evalua-tion indicates that the structure of the ontology is es-sential for the tool?s usability.
Still, the evaluation issufficiently positive that we believe the WYSIWYMapproach suitable for providing access to ontologies,especially for users who are unfamiliar with ontolo-gies or their graphical representations.
Navigationcould be made easier by providing users with anoverview of the underlying ontology, possibly pre-sented as an index of objects, and the informationthat can be specified about each object.
An onlinemanual with some worked examples and screenshotsmight also help users get started on the more obvi-ous parts of a description.
We are currently attempt-ing to adapt an ontology developed in another UK e-science project8 for use in theWYSIWYMmetadataelication tool.
Instead of assuming the depositing ofa resource, this ontology was developed to captureuser-elicited metadata for video annotation.
Part ofthis metadata is captured automatically, part of it iselicited from the user.
We hope that the adaptationof an ontology that was originally developed for adifferent purpose for use in an NLG application willhighlight other issues involved in the use of ontolo-gies in NLG.One way in which subjects did tasks erroneouslywas by using the ?hasComment?
property when theycould not find the option they wanted.
This is notprecisely wrong: the metadata it produces is correctand any human readers will understand the descrip-tion.
But it is not the best description for query-ing purposes.
We think some subjects may have8http://www.ncess.ac.uk/research/digital records/had trouble grasping the exact purpose of the pro-duced descriptions.
We hope that users who haveused the query tool to find (the descriptions of) re-sources, will have a better understanding of what aneffective metadata description is.We intend to run more evaluation experiments, toassess the usability but also the usefulness of thecombined toolset.
Rather than asking subjects tocopy descriptions or queries, we may ask them tofind a particular resource, or to try to deposit and de-scribe one of their own papers.
If possible, it wouldalso be interesting to see how they perform the sametasks using a different interface for metadata ac-cess, e.g.
a graphical interface such as SHAKEN(Thome?re?
et al, 2002).6 ConclusionWe have presented a WYSIWYM interface for thecreation of RDF metadata, which will be extendedby the addition of querying and browsing tools.This tool is driven by an ontology that contains alldomain-specific information needed to present it innatural language.
We have highlighted a number ofissues in ontology development for access throughNLG.
We have evaluated the tool?s usability throughan experiment with potential users.
The results wereencouraging, but indicate that the structure and fa-miliarity of the underlying ontology strongly influ-ence the usability of the interface.AcknowledgmentsMany thanks to Catalina Hallett and Richard Powerfrom the CLEF project, for their help in comparingthe two tools and their different evaluation results.ReferencesJ.A.
Bateman.
1990.
Upper Modelling: A General Or-ganisation of Knowledge for Natural Language Pro-cessing.
In Proceedings of the International LanguageGeneration Workshop, Pittsburgh, USA.A.
Bernstein and E. Kaufmann.
2006.
GINO - A GuidedInput Natural Language Ontology Editor.
In Inter-national Semantic Web Conference 2006, pages 144?157.K.
Bontcheva and Y. Wills.
2004.
Automatic Re-port Generation from Ontologies: the MIAKT ap-proach.
In Nineth International Conference on Appli-145cations of Natural Language to Information Systems(NLDB?2004), Manchester, UK.T.
Catarci, P. Dongilli, T. Di Mascio, E. Franconi, G. San-tucci, and S. Tessaris.
2004.
An Ontology-based Vi-sual Tool for Query Formulation Support.
In Proceed-ings of the Sixteenth European Conference on Artifi-cial Intelligence (ECAI 2004).D.
De Roure, N.R.
Jennings, and N.R.
Shadbolt.
2005.The Semantic Grid: Past, Present and Future.
In Pro-ceedings of the IEEE 93(3), pages 669?681.C.
Hallett, D. Scott, and R. Power.
2007.
ComposingQuestions through Conceptual Authoring.
Computa-tional Linguistics, 33(1):105?133.C.
Hallett.
2006.
Generic Querying of RelationalDatabases using Natural Language Generation Tech-niques.
In Proceedings of the Fourth InternationalNatural Language Generation Conference, pages 88?95, Nottingham, UK.S.
Handschuh, S. Staab, and A. Maedche.
2001.CREAM: creating relational metadata with acomponent-based, ontology-driven annotationframework.
In K-CAP ?01: Proceedings of the 1stinternational conference on Knowledge capture, pages76?83, New York, NY, USA.
ACM Press.F.
Hielkema, P. Edwards, C. Mellish, and J. Farrington.2007a.
A Flexible Interface to Community-DrivenMetadata.
In Proceedings of the Third InternationalConference on eSocial Science.F.
Hielkema, C. Mellish, and P. Edwards.
2007b.
UsingWYSIWYM to Create an Open-ended Interface for theSemantic Grid.
In S. Busemann, editor, Proceedingsof the 11th European Workshop on Natural LanguageGeneration.I.A.
Mel?cuk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York.P.
Piwek, R. Evans, L. Cahil, and N. Tipper.
2000.
Natu-ral Language Generation in the MILE System.
In Pro-ceedings of IMPACTS in NLG workshop, pages 33?42,Schloss Dagstuhl, Germany.P.
Piwek.
2002.
Requirements Definition, Validation,Verification and Evaluation of the CLIME Interfaceand Language Processing Technology.
Technical Re-port ITRI-02-03, ITRI, University of Brighton.R.
Power, D. Scott, and R. Evans.
1998.
What You See IsWhat YouMeant: Direct Knowledge Editing with Nat-ural Language Feedback.
In Proceedings of the Thir-teenth European Conference on Artificial Intelligence,Brighton, UK.R.
Schwitter and M. Tilbrook.
2004.
Controlled Natu-ral Language meets the Semantic Web.
In Proceed-ings of the Australasian Language Technology Work-shop 2004.H.R.
Tennant, K.M.
Ross, R.M.
Saenz, C.W.Thompson,and J.R. Miller.
1983.
Menu-based Natural LanguageUnderstanding.
In Proceedings of the Twenty-first An-nual Meetings on Association for Computational Lin-guistics, pages 151?158, Cambridge, Massachusetts.J.
Thome?re?, K. Barker, V. Chaudhri, P. Clark, M. Erik-sen, S. Mishra, B. Porter, and A. Rodriguez.
2002.A Web-based Ontology Browsing and Editing System.In Eighteenth National Conference on Artificial Intelli-gence, pages 927?934, Menlo Park, CA, USA.
Amer-ican Association for Artificial Intelligence.G.
Wilcock.
2003.
Talking OWLs: Towards an Ontol-ogy Verbalizer.
In Human Language Technology forthe Semantic Web and Web Services (ISWC?03), pages109?112, Sanibel Island, Florida.146
