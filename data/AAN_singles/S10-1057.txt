Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 256?259,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsUTD: Classifying Semantic Relations by CombiningLexical and Semantic ResourcesBryan Rink and Sanda HarabagiuHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, Texas{bryan,sanda}@hlt.utdallas.eduAbstractThis paper describes our system forSemEval-2010 Task 8 on multi-way clas-sification of semantic relations betweennominals.
First, the type of semantic re-lation is classified.
Then a relation type-specific classifier determines the relationdirection.
Classification is performed us-ing SVM classifiers and a number of fea-tures that capture the context, semanticrole affiliation, and possible pre-existingrelations of the nominals.
This approachachieved an F1 score of 82.19% and an ac-curacy of 77.92%.1 IntroductionSemEval-2010 Task 8 evaluated the multi-wayclassification of semantic relations between nom-inals in a sentence (Hendrickx et al, 2010).Given two nominals embedded in a sentence,the task requires identifying which of the fol-lowing nine semantic relations holds betweenthe nominals: Cause-Effect, Instrument-Agency,Product-Producer, Content-Container, Entity-Origin, Entity-Destination, Component-Whole,Member-Collection, Message-Topic, or Other ifno other relation is appropriate.
For instance, thefollowing sentence provides an example of theEntity-Destination relation:?A small [piece]E1of rock landed into the[trunk]E2.
?The two nominals given for this sentence areE1(piece) and E2(trunk).
This is an Entity-Destination relation because the piece of rockoriginated from outside of the trunk, but endedup there.
Finally, the direction of the relation is(E1,E2) because E1, the piece, is the Entity and E2,the trunk, is the Destination.Analysis of the training data revealed three ma-jor classes of knowledge required for recognizingsemantic relations: (i) examples that require back-ground knowledge of an existing relation betweenthe nominals (e.g., example 5884 below), (ii) ex-amples using background knowledge regardingthe typical role of one of the nominals (e.g., ex-ample 3402), and (iii) examples that require con-textual cues to disambiguate the role between thenominals (e.g., example 5710).Example 5884 ?The Ca content in the [corn]E1[flour]E2has also a strong dependence onthe pericarp thickness.
?Example 3402 ?The [rootball]E1was in a[crate]E2the size of a refrigerator, and someof the arms were over 12 feet tall.
?Example 5710 ?The seniors poured [flour]E1into wax [paper]E2and threw the items asprojectiles on freshmen during a morning peprally.
?In example 5884, the background knowledgethat flour is often made or derived from corn candirectly lead to the classification of the exampleas containing an Entity-Origin relation.
Likewise,knowing that crates often act as containers is astrong reason for believing that example 3402 isa Content-Container relation.
However, in exam-ple 5710, neither the combination of the nominalsnor their individual affiliations lead to an obvioussemantic relation.
After taking the context intoaccount, it becomes clear that this is an Entity-Destination relation because E1is going into E2.2 ApproachWe cast the task of determining a semantic re-lation and its direction as a classification task.Rather than classifying both pieces of informa-tion (relation and direction) simultaneously, oneclassifier is used to determine the relation type,and then, for each relation type, a separate clas-sifier determines the direction.
We used a totalof 45 feature types (henceforth: features), which256were shared among all of the direction classi-fiers and the one relation classifier.
These fea-ture types can be partitioned into 8 groups: lexicalfeatures, hypernyms from WordNet1, dependencyparse, PropBank parse, FrameNet parse, nominal-ization, predicates from TextRunner, and nomi-nal similarity derived from the Google N-Gramdata set.
All features were treated as FEATURE-TYPE:VALUE pairs which were then presented tothe SVM2classifier as a boolean feature (0 or 1).We further group our features into the threeclasses described above: Contextual, Nominal af-filiation, and Pre-existing relations.
Table 1 illus-trates sample feature values from example 117 ofthe training set.3 Contextual and Lexical FeaturesThe contextual features consist of lexical featuresand features based on dependency, PropBank, andFrameNet parses.
For lexical features, we extractthe words and parts of speech for E1and E2, thewords, parts of speech, and prefixes of length 5 fortokens between the nominals, and the words be-fore and single word after E1and E2respectively.The words between the nominals can be strongindicators for the type of relation.
For examplethe words into, produced, and caused are likelyto occur in Entity-Destination, Product-Producer,and Cause-Effect relations, respectively.
Usingthe prefixes of length 5 for the words between thenominals provides a kind of stemming (produced?
produ, caused ?
cause).Inspired by a feature from (Beamer et al, 2007),we extract a coarse-grained part of speech se-quence for the words between the nominals.
Thisis accomplished by building a string using the firstletter of each token?s Treebank POS tag.
This fea-ture is motivated by the fact that relations such asMember-Collection usually invoke prepositionalphrases such as: of, in the, and of various.
Thecorresponding POS sequences we extract are: ?I?,?I D?, and ?I J?.
Finally, we also use the num-ber of words between the nominals as a featurebecause relations such as Product-Producer andEntity-Origin often have no intervening tokens(e.g., organ builder or Coconut oil).Syntactic and semantic parses capture long dis-tance relationships between phrases in a sentence.Instead of a traditional syntactic parser, we chose1http://wordnet.princeton.edu/2We used Weka?s SMO classifierhttp://www.cs.waikato.ac.nz/ml/weka/the Stanford dependency parser3for the simplersyntactic structure it produces.
Our dependencyfeatures are based on paths in the dependency treeof length 1 and length 2.
The paths encode the de-pendencies and words those dependencies attachto.
To generalize the paths, some of the featuresreplace verbs in the path with their top-level Levinclass, as determined by running a word sense dis-ambiguation system (Mihalcea and Csomai, 2005)followed by a lookup in VerbNet4.
One of the fea-tures for length 2 paths generalizes further by re-placing all words with their location relative to thenominals, either BEFORE, BETWEEN, or AFTER.Consider example 117 from Table 1.
The length2 dependency path (feature depPathLen2VerbNet)neatly captures the fact that E1is the subject of averb falling into Levin class 27, and E2is the di-rect object.
Levin class 27 is the class of engenderverbs, such as cause, spawn, and generate.
Thispath is indicative of a Cause-Effect relation.Semantic parses such as ASSERT?s PropBankparse5and LTH?s FrameNet parse6identify predi-cates in text and their semantic roles.
These parsesgo beyond the dependency parse and identify thespecific role each nominal assumes for the pred-icates in the sentence, so the parses should be amore reliable indicator for the relation type be-tween nominals.
We have features for the iden-tified predicates and for the roles assigned to eachnominal.
Several of the features are only triggeredif both nominals are arguments for the same pred-icate.
The values from Table 1 show that the fea-tures correctly determined that E1and E2are gov-erned by a verb of Levin class 27, and that the lex-ical unit is cause.v.4 Nominal Role Affiliation FeaturesAlthough context can be critical to identifying thesemantic relation present in some examples, inothers we must bring some background knowledgeto bear regarding the types of nominals involved.Knowing that a writer is a person provides sup-porting evidence for that nominal taking part ina PRODUCER role.
Additionally, writer nominal-izes the verb write which is classified by Levin(Levin, 1993) as an ?Image creation?
or ?Creationand Transformation?
verb.
This provides furthersupport for assigning writer to a PRODUCER role.3http://nlp.stanford.edu/software/lex-parser.shtml4http://verbs.colorado.edu/ mpalmer/projects/verbnet.html5http://cemantix.org/assert.html6http://nlp.cs.lth.se/software/semantic parsing: framenet frames/257Example 117: Forward [motion]E1of the vehicle through the air caused a [suction]E2on the road draft tube.Feature Set Feature ValuesLexical e1Word=motion, e2Word=suction, e1OrE2Word={motion,suction}, wordsBetween={of, the, vehicle,through, the, air, caused, a}, posE1=NN, posE2=NN, posE1orE2=NN posBetween=I D N I D N V D,distance=8, wordsOutside={Forward, on}, prefix5Between={air, cause, a, of, the, vehic, throu, the}DependencydepPathLen1={caused?nsubj?<E1>, caused?dobj?<E2>,...}depPathLen1VerbNet={vn:27?nsubj?<E1>, vn:27?dobj?<E2>,...}depPathLen2VerbNet={<E1>?nsubj?vn:27?dobj?<E2>},depPathLen2Location={<E1>?nsubj?BETWEEN?dobj?<E2>}PropBankpbPredStem=caus, pbVerbNet=27, pbE1CoarseRole=ARG0, pbE2CoarseRole=ARG1,pbE1orE2CoarseRole={ARG1,ARG2}, pbNumPredToks=1,pbE1orE2PredHyper = {cause#v#1, create#v#1}FrameNet fnAnyLU={cause.v, vehicle.n, road.n}, fnAnyTarget={cause,vehicle,road}, fnE2LU=cause.v,fnE1OrE2LU=cause.vHypernym hyperE1={gesture#n#2, communication#n#2, entity#n#1, ...}, hyperE2={suction#n#1, phe-nomenon#n#1, entity#n#1,...}, hyperE1orE2={gesture#n#2, communication#n#2, entity#n#1, suc-tion#n#1, phenomenon#n#1, ...}, hyperBetween={quality#n#1, cause#v#1, create#v#1, ...}NomLex-Plus Features did not fireNGrams knnE1={motion, amendment, action, appeal, decision}, knnE2={suction, hose, pump, vacuum, nozzle},knnE1Role=Message, knnE2Role=ComponentTextRunner trE1 E2={may result from, to contact, created, moves, applies, causes, falls below, corresponds to which},trE2 E1={including, are moved under, will cause, according to, are effected by, repeats, can match},trE1 E2Hyper={be#v#6, agree#v#3, cause#v#1, ensue#v#1, contact#v#1, apply#v#1, ...}Table 1: All of the feature types and values for example 117 from the training data.
Despite the errors indisambiguation the system still correctly classifies this as Cause-Effect(E1,E2)We capture this background knowledge by lever-aging four sources of lexical and semantic knowl-edge: WordNet, NomLex-Plus7, VerbNet, and theGoogle N-Gram data8.We utilize a word sense disambiguation sys-tem (Mihalcea and Csomai, 2005) to determine thebest sense for each nominal and use all of the hy-pernyms as a feature.
Hypernyms are also deter-mined for the words between the nominals, how-ever only the top three levels are used as a feature.Following (Beamer et al, 2007), we also incor-porate a nominalization feature for each nominalbased on NomLex-Plus.
Rather than use the agen-tial information as they did, we determine the verbbeing nominalized and retrieve the verb?s top-levelLevin class from VerbNet.
This reduces the spar-sity problem for nominalizations while still cap-turing their semantics.Our final role-affiliation features make use ofthe Google N-Gram data.
Using the 5-grams wedetermined the top 1,000 words that occur mostoften in the context of each nominal.
Nominalswere then compared to each other using Jaccardsimilarity of their contexts and the 4 closest neigh-bors were retained.
For each nominal, we have afeature containing the nominal itself and its 4 near-est neighbors from the training set.
Additional fea-tures determine the most frequent role assigned tothe neighbors.
Examples of all these features can7http://nlp.cs.nyu.edu/meyers/NomBank.html8Available from LDC as LDC2006T13be seen in Table 1 in the row for NGrams.
Theneighbors for motion in the table show the diffi-culty this feature has with ambiguity, incorrectlypicking up words similar to the sense meaning aproposal for action.5 Pre-existing Relation FeaturesFor some examples the context and the individ-ual nominal affiliations provide little help in de-termining the semantic relation, such as example5884 from before (i.e., corn flour).
These ex-amples require knowledge of the interaction be-tween the nominals and we cannot rely solelyon determining the role of one nominal or theother.
We turned to TextRunner (Yates et al,2007) as a large source of background knowl-edge about pre-existing relations between nom-inals.
TextRunner is a queryable database ofNOUN-VERB-NOUN triples extracted from a largecorpus of webpages.
For example, the phrases re-trieved from TextRunner for ?corn flour?include: ?is ground into?, ?to make?, ?to ob-tain?, and ?makes?.
Querying in the reverse direc-tion, for ?flour corn?
returns phrases suchas: ?contain?, ?filled with?, ?comprises?, and ?ismade from?.
We use the top ten phrases for the?<E1> <E2>?
query results, and also forthe ?<E2> <E1>?
results, forming two fea-tures.
In addition, we include a feature that has allof the hypernyms for the content words in the verbphrases from the queries for the E1-E2direction.258Relation P R F1Cause-Effect 89.63 89.63 89.63Component-Whole 74.34 81.73 77.86Content-Container 84.62 85.94 85.27Entity-Destination 88.22 89.73 88.96Entity-Origin 83.87 80.62 82.21Instrument-Agency 71.83 65.38 68.46Member-Collection 84.30 87.55 85.89Message-Topic 81.02 85.06 82.99Product-Producer 82.38 74.89 78.46Other 52.97 51.10 52.02Overall 82.25 82.28 82.19Table 2: Overall and individual relation scores onthe test set, along with precision and recall6 ResultsOur system achieved the best overall score as mea-sured by macro-averaged F1 (for scoring detailssee (Hendrickx et al, 2010)) among the ten teamsthat participated in the semantic relation task atSemEval-2010.
The results in Table 2 show theperformance of the system on the test set for eachrelation type and the overall score.The training data consisted of 8,000 annotatedinstances, including the numbered examples intro-duced earlier, and the test set contained 2,717 ex-amples.
To assess the learning curve for this taskwe trained on sets of size 1000, 2000, 4000, and8000, obtaining test scores of 73.08, 77.02, 79.93,and 82.19, respectively.
These results indicate thatmore training data does help, but going from 1,000training instances to 8,000 only boosts the score byabout 9 points of F-measure.Because our approach makes use of many dif-ferent features, we ran ablation tests on the 8 setsof features from Table 1 to determine which typesof features contributed the most to classifying se-mantic relations.
We evaluated all 256 (28) combi-nations of the feature sets on the training data us-ing 10-fold cross validation.
The results are shownin Table 3.
The last lines of Tables 2 and 3 corre-spond to the system submitted for SemEval-2010Task 8.
The score on the training data is lower be-cause the data includes examples from SemEval-2007, which has more of the harder to classifyOther relations9.These tests have shown that the NomLex-Plusfeature likely did not help.
Further, the depen-dency parse feature added little beyond PropBankand FrameNet.
Given the high score for the lexicalfeature set we split it into smaller sets to see theircontributions in the top portion of Table 3.
This9To confirm this we performed a 10 fold cross validationof examples 1-7109, adding examples 7110-8000 (the 2007data) to each training set.
This resulted in an F1 of 82.18Feature Sets F1E1and E2only 48.7Words between only 64.0E1, E2, and words between 72.5All word features (incl.
before and after) 73.11 Lexical 73.82 +Hypernym 77.83 +FrameNet 78.94 +NGrams 79.75 -FrameNet +PropBank +TextRunner 80.56 +FrameNet 81.17 +Dependency 81.38 +NomLex-Plus 81.3Table 3: Scores obtained for various sets of fea-tures on the training set.
The bottom portion ofthe table shows the best combination containing 1to 8 feature setsreveals the best individual feature is for the wordsbetween the two nominals.7 ConclusionBy combining various linguistic resources wewere able to build a state of the art system forrecognizing semantic relations in text.
While thelarge training size available in SemEval-2010 Task8 enables achieving high scores using only word-based features, richer linguistic and background-knowledge resources still provide additional aid inidentifying semantic relations.AcknowledgmentsThe authors would like to thank Kirk Roberts forproviding code and insightful comments.ReferencesB.
Beamer, S. Bhat, B. Chee, A. Fister, A. Rozovskaya,and R. Girju.
2007.
UIUC: a knowledge-richapproach to identifying semantic relations betweennominals.
In ACL SemEval07 Workshop.I.
Hendrickx, S.N.
Kim, Z. Kozareva, P. Nakov, D.?OS?eaghdha, S. Pad?o, M. Pennacchiotti, L. Romano,and S. Szpakowicz.
2010.
Semeval-2010 task 8:Multi-way classification of semantic relations be-tween pairs of nominals.
In Proceedings of the 5thSIGLEX Workshop on Semantic Evaluation, Upp-sala, Sweden.B.
Levin.
1993.
English verb classes and alternations:A preliminary investigation.
Chicago, Il.R.
Mihalcea and A. Csomai.
2005.
SenseLearner:word sense disambiguation for all words in unre-stricted text.
In Proceedings of the ACL 2005 onInteractive poster and demonstration sessions.
ACL.A.
Yates, M. Cafarella, M. Banko, O. Etzioni,M.
Broadhead, and S. Soderland.
2007.
Text-Runner: open information extraction on the web.
InProceedings of HLT: NAACL: Demonstrations.259
