Statistical Sense Disambiguation with Relatively Small CorporaUsing Dictionary DefinitionsMicrosoft InstituteNorth Ryde, NSW 2113, Australiat-alphal@microsoft.comAlpha K. LukDepartment ofComputingMacquarie UniversityNSW 2109, AustraliaAbstractCorpus-based sense disambiguation methods, likemost other statistical NLP approaches, uffer fromthe problem of data sparseness.
In this paper, wedescribe an approach which overcomes this problemusing dictionary definitions.
Using the definition-based conceptual co-occurrence data collected fromthe relatively small Brown corpus, our sensedisambiguation system achieves an average accuracycomparable to human performance given the samecontextual information.1 IntroductionPrevious corpus-based sense disambiguation methodsrequire substantial mounts of sense-tagged trainingdata (Kelly and Stone, 1975; Black, 1988 andHearst, 1991) or aligned bilingual corpora (Brown etal., 1991; Dagan, 1991 and Gale et al 1992).Yarowsky (1992) introduces a thesaurus-basedapproach to statistical sense disambiguation whichworks on monolingual corpora without he need forsense-tagged training data.
By collecting statisticaldata of word occurrences in the context of differentthesaurus categories from a relatively large corpus(10 million words), the system can identify salientwords for each category.
Using these salient words,the system is able to disambiguate polysemous wordswith respect to thesaurus categories.Statistical approaches like these generally sufferfrom the problem of data sparseness.
To estimate thesalience of a word with reasonable accuracy, thesystem needs the word to have a significant numberof occurrences in the corpus.
Having large corporawill help but some words are simply too infrequentto make a significant statistical contribution even ina rather large corpus.
Moreover, huge corpora arenot generally available in all domains and storageand processing of very huge corpora can beproblematic n some cases.ZIn this paper, we describe an approach whichattacks the problem of.
data sparseness in automaticstatistical sense disambiguation.
Using definitionsfrom LDOCE (Longman Dictionary ofContemporary English; Procter, 1978), co-occurrence data of concepts, rather than words, iscollected from a relatively small corpus, the onemillion word Brown corpus.
Since all the definitionsin LDOCE are written using words from the 2000word controlled vocabulary (or in our terminology,defining concepts), even our small corpus is found tobe capable of providing statistically significant co-occurrence data at the level of the defining concepts.This data is then used in a sense disambiguationsystem.
The system is tested on twelve wordspreviously discussed in the sense disambiguationliterature.
The results are found to be comparable tohuman performance given the same contextualinformation.2 Statistical Sense Disambiguation UsingDictionary DefinitionsIt is well known that some words tend to co-occurwith some words more often than with others.Similarly, looking at the meaning of the words, oneshould find that some concepts co-occur more oftenwith some concepts than with others.
For example,the concept crime is found to co-occur frequentlywith the concept punishment.
This kind ofconceptual relationship s not always reflected at thelexical level.
For instance, in legal reports, theStatistical data is domain dependent.
Dataextracted from a corpus of one particular domain isusually not very useful for processing text of anotherdomain.181concept crime will usually be expressed by wordslike offence or felony, etc., and punishment will beexpressed by words such as sentence, fine or penalty,etc.
The large number of different words of similarmeaning is the major cause of the data sparsenessproblem.The meaning or underlying concepts of a wordare very difficult o capture accurately but dictionarydefinitions provide a reasonable representation a dare readily available.
2 For instance, the LDOCEdefinitions of both offence and felony contain theword crime, and all of the definitions of sentence,fine and penalty contain the word punishment.
Todisambiguate a polysemous word, a system can selectthe sense with a dictionary definition containingdefining concepts that co-occur most frequently withthe defining concepts in the definitions of the otherwords in the context.
In the current experiment, thisconceptual co-occurrence data is collected from theBrown corpus.2.1 Collecting Conceptual Co-occurrence DataOur system constructs a two-dimensional tablewhich records the frequency of co-occurrence ofeachpair of defining concepts.
The controlled vocabularyprovided by Longman is a list of all the words usedin the definitions but, in its crude form, it does notsuit our purpose.
From the controlled vocabulary, wemanually constructed a list of 1792 definingconcepts.
To minimise the size of the table and theprocessing time, all the closed class words and wordswhich are rarely used in definitions (e.g., the days ofthe week, the months) are excluded from the list.
Tostrengthen the signals, words which have the samesemantic root are combined as one element in the list(e.g., habit and habitual are combined as {habit,habitual}).The whole LDOCE is pre-processed first.
Foreach entry in LDOCE, we construct itscorresponding conceptual expansion.
The conceptualexpansion of an entry whose headword is not adefining concept is a set of conceptual sets.
Eachconceptual set corresponds to a sense in the entryand contains all the defining concepts which occurin the definition of the sense.
The entry of the nounsentence and its corresponding conceptual expansion2 Manually constructed semantic frames could bemore useful computationally but building semanticframes for a huge lexicon is an extremely expensiveexercise.are shown in Figure 1.
If the headword of an entry isa defining concept DC, the conceptual expansion isgiven as {{DC}}.The corpus is pre-segrnented into sentences butnot pre-processed in any other way (sense-tagged orpart-of-speech-tagged).
The context of a word isdefined to be the current sentence) The systemprocesses the corpus sentence by sentence andcollects conceptual co-occurrence data for eachdefining concept which occurs in the sentence.
Thisallows the whole table to be constructed in a singlerun through the corpus.Since the training data is not sense tagged, thedata collected will contain noise due to spurioussenses of polysemous words.
Like the thesaurus-based approach of Yarowsky (1992), our approachrelies on the dilution of this noise by theirdistribution through all the 1792 defining concepts.Different words in the corpus have differentnumbers of senses and different senses havedefinitions of varying lengths.
The principle adoptedin collecting co-occurrence data is that every pair ofcontent words which co-occur in a sentence shouldhave equal contribution to the conceptual co-occurrence data regardless of the number ofdefinitions (senses) of the words and the lengths ofthe definitions.
In addition, the contribution of aword should be evenly distributed between all thesenses of a word and the contribution of a senseshould be evenly distributed between all the conceptsin a sense.
The algorithm for conceptual co-occurrence data collection is shown in Figure 2.2.2 Using the Conceptual Co-occurrence Datafor Sense DisambiguationTo disambiguate a polysemous word W in a contextC, which is taken to be the sentence containing W,the system scores each sense S of W, as defined inLDOCE, with respect to C using the followingequations.score(S, C) = score(CS, C') - score(CS, GlobalCS) \[1\]where CS is the corresponding conceptual set of S,C' is the set of conceptual expansions of all contentwords (which are defined in LDOCE) in C andGlobalCS is the conceptual set containing all the1792 defining concepts.3 The average sentence l ngth of the Brown corpus is19.4 words.182Entry in LDOCE1.
(an order given by a judge which fixes) a punishment for a criminalfound guilty in court2.
a group of words that forms a statement, command, exclamation, orquestion, usu.
contains a subject and a verb, and (in writing) beginswith a capital letter and ends with one of the marks.
!
?conceptual expansion{ {order, judge, punish, crime, criminal,fred, guilt, court},{group, word, form, statement,command, question, contain, subject,verb, write, begin, capital, letter, end,mark} }Figure 1.
The entry of sentence (n.) in LDOCE and its corresponding conceptual expansion1.
Initialise the Conceptual Co-occurrence Data Table (CCDT) with initial value of 0 for2.
For each sentence S in the corpus, doa.
Construct S', the set of conceptual expansions of all content words (which aredefined in LDOCE) in S.b.
For each unique pair of conceptual expansions (CE~, CEj) in S', doFor each defining concept DC~mp in each conceptual set CS~m in CE~, doFor each defining concept DCjnq in each conceptual set CSj, in CEj, doincrease the values of the cells CCDT(DCimp, DCjnq)and CCDT(DCjnq, Dcirnp) by the product of w(DCimp) and w(DCjnq)where w(DCxyz) is the weight of DCxyz given by!w(DC~ ) =ICE, I, IC%Ieach cell.Figure 2.
The algorithm for collecting conceptual co-occurrence datascore< CS, C'> = ve~S, core< CS, CE'> /I C'\]for any concp, set CS and concp, exp.
set C' \[2\]score(CS, CE') = max score(CS,CS')C8'~C?
'for any concp, set CSand concp, exp.
CE' \[31score( CS, CS') = voe'.es' ~ sc?re( eS'DC') /ICS'\[for any concp, sets CS and CS' \[4\]score(CS, DC')= ~f~ score(DC, DC') /\[CS\[for any concp, set CS and def.
concept DC' \[5\]score( DC, DC' ) = max(0, I ( DC, DC' ))for any def.
concepts DC and DC' \[6\]I(DC, DC') is the mutual information 4 (Fano, 1961)between the 2 defining concepts DC and DC' givenby:I(x,y) --- log s P(x,y)P(x).
P(y)f (x ,y ) .NI?g2 f (x ) .
f(y)(using the Maximum Likelihood Estimator).f(x,y) is looked up directly from the conceptual co-occurrence data table, fix) and f(y) are looked upfrom a pre-constructed list off(DC) values, for eachdefining concept DC:f(OC) = ~_,f(DC, DC')VDC'4 Church and Hanks (1989) use Mutual Informationto measure word association norms.183N is taken to be the total number of pairs of wordsprocessed, given by~ f ( DC)/2since for each pair of surface words processed,LI( c)V/~Cis increased by 2.Our scoring method is based on a probabilisticmodel at the conceptual level.
In a standard model,the logarlthm of the probability of occurrence of aconceptual set {x,, x~ ..... xm} in the context of theconceptual set {y~, y~.....y,} is given bylog2 P(xl,x2 ..... x,,lyl,y2 ..... y,)"~ ~=l ( "j~.__ll(x,,Yj)+l?g2 P(xi))assuming that each P(x~) is independent of eachother given y~, y2...., y, and each P(Y.i) is independentof each other given x~, for all x~.SOur scoring method deviates from the standardmodel in a number of aspects:1. log 2 P(x~), the term of the occurrence Probabilityof each of the defining concepts in the sense, isexcluded in our scoring method.
Since the trainingdata is not sense-tagged, the occurrence probabilityis highly unreliable.
Moreover, the magnitude ofmutual information is decreased ue to the noise ofthe spurious senses while the average magnitude ofthe occurrence probability is unaffected, e Inclusionof the occurrence probability term will lead to thedominance of this term over the mutual informationterm, resulting in the system flavouring the sensewith the more frequently occurring defining conceptsmost of the time.2.
The score of a sense with respect o the currentcontext is normalised by subtracting the score of thesense calculated with respect to the GlobalCS (whichcontains all defining concepts) from it (see formula5 The occurrence probabilities of some definingconcepts will not be independent in some contexts.However, modelling the dependency betweendifferent concepts in different contexts will lead toan explosion of the complexity of the model.6 The noise only leads to incorrect distribution of theoccurrence probability.\[1\]).
In effect, we are comparing the score betweenthe sense with the current context and the scorebetween the sense and an artificially constructed"average" context.
This is needed to rectify the biastowards the sense(s) with defining concepts of higheraverage mutual information (over the set of alldefining concepts), 'which is intensified by theambiguity of the context words.3.
Negative mutual information score is taken to be 0(\[6\]).
Negative mutual information is unreliable dueto the smaller number of data points.4.
The evidence (mutual information score) frommultiple defining concepts/words is averaged ratherthan summed (\[2\], \[4\] & \[5\]).
This is to compensatefor the different lengths of definitions of differentsenses and different lengths of the context.
Theevidence from a polysemous context word is taken tobe the evidence from its sense with the highestmutual information score (\[3\]).
This is due to thefact that only one of the senses is used in the givensentence.3 EvaluationOur system is tested on the twelve words discussedin Yarowsky (1992) and previous publications onsense disambiguation.
Results are shown in Table 1.Our system achieves an average accuracy of 77% ona mean 3-way sense distinction over the twelvewords.
Numerically, the result is not as good as the92% as reported in Yarowsky (1992).
However,direct comparison between the numerical results canbe misleading since the experiments are carried outon two very different corpora both in size and genre.Firstly, Yarowsky's system is trained with the 10million word Grolier's Encyclopedia, which is amagnitude larger than the Brown corpus used by oursystem.
Secondly, and more importantly, the twocorpora, which are also the test corpora, are verydifferent in genre.
Semantic oherence of text, onwhich both systems rely, is generally stronger intechnical writing than in most other kinds of text.Statistical disambiguation systems which rely onsemantic oherence will generally perform better ontechnical writing, which encyclopedia entry can beregarded as one kind of, than on most other kinds oftext.
On the other hand, the Brown corpus is acollection of text with all kinds of genre.People make use of syntactic, semantic andpragmatic knowledge in sense disambiguation.
It isnot very realistic to expect any system which onlypossesses semantic oherence knowledge (including184ours as well as Yarowsky's) to achieve a very highlevel of accuracy for all words in general text.
Toprovide a better evaluation of our approach, we haveconducted an informal experiment aiming atestablishing a more reasonable upper bound of theperformance of such systems.
In the experiment, ahuman subject is asked to perform the samedisambiguation task as our system, given the samecontextual information, 7 Since our system only usessemantic oherence information and has no deeperunderstanding of the meaning of the text, the humansubject is asked to disambiguate the target word,given a list of all the content words in the context(sentence) of the target word in random order.
Thewords are put in random order because the systemdoes not make use of syntactic information of thesentence ither.
The human subject is also allowedaccess to a copy of LDOCE which the system alsouses.
The results are listed in Table 1.
The actualupper bound of the performance of statisticalmethods using semantic oherence information onlyshould be slightly better than the performance ofhuman since the human is disadvantaged by anumber of factors, including but not limited to: 1. itis unnatural for human to disambiguate in thedescribed manner; 2. the semantic coherenceknowledge used by the human is not complete orspecific to the current corpusS; 3. human error.However, the results provide a rough approximationof the upper bound of performance of such systems,The human subject achieves an average accuracyof 71% over the twelve words, which is 6% lowerthan our system.
More interestingly, the results ofthe human subject are found to exhibit a similarpattern to the results of our system - the humansubject performs better on words and senses forwhich our system achieve higher accuracy and lesswell on words and senses for which our system has alower accuracy.4 The Use of Sentence as Local ContextAnother significant point our experiments haveshown is that the sentence can also provide enoughcontextual information for semantic oherence based7 The result is less than conclusive since only onehuman subject is tested.
In order to acquire morereliable results, we are currently seeking a few moresubjects to repeat he experiment.s The subject has not read through the whole corpus.approaches in a large proportion of cases.
9 Theaverage sentence length in the Brown corpus is19.41?
words which is 5 times smaller than the 100word window used in Gale et al (1992) andYarowsky (1992).
Our approach works well evenwith a small "window" because it is based on theidentification of salient concepts rather than salientwords.
In salient word based approaches, due to theproblem of data sparseness, many less frequentlyoccurring words which are intuitively salient to aparticular word sense will not be identified inpractice unless an extremely large corpus is used.Therefore the sentence usually does not containenough identified salient words to provide enoughcontextual information.
Using conceptual co-occurrence data, contextual information from thesalient but less frequently used words in the sentencewill also be utilised through the salient concepts inthe conceptual expansions of these words.
Obviously,there are still cases where the sentence does notprovide enough contextual information even usingconceptual co-occurrence data, such as when thesentence is too short, and contextual informationfrom a larger context has to be used.
However, theability to make use of information in a smallercontext is very important because the smaller contextalways overrules the larger context if their sensepreferences are different.
For example, in a legaltrial context, the correct sense of sentence in theclause she was asked to repeat he last word of herprevious entence will be its word sense rather thanits legal sense which would have been selected if alarger context is used instead.9 Analysis of the test samples which our system failsto correctly disambiguate also shows that increasingthe window size will benefit the disambiguationprocess only in a very small proportion of thesesamples.
The main cause of errors is the polysemouswords in dictionary definitions which we will discussin Section 6.1o Based on 1004998 words and 51763 sentences.185Table 1.
Results of ExperimentsSense N i DBCC HumanBASSFishMusical sensesBOWbending forwardweaponviolin partknotfront of shipbend in object *CONEshaped objectfruit of a plantpart of eye *DUTYobligationtaxGALLEYancient shipship's kitchenprinter's trayINTERESTcuriosityadvantagesharemoney paidISSUEbringing outimportant pointstock *MOLEskin blemishanimalstone wall **quantity *machine *SENTENCEpunishmentgroup of words1151610242o .5054256040187598483023687123112031i 100% 100%i 93% 100%Thes.100%99%i 94% 100% 99%!
0% 100%i - - 92%i 100% 100% 100%i 100% 100% 25%i 50% 100% 94%- -- 50%i 78% 100% 91%i 100% 100% 61%i .
.
.
.
99%- - 69%i 100% 100% 77%i 57%i 100%j 59%i -i lOO%i --i 100%i 43%i 42%i 25%88%i 49%i 64%i 56%59%72%100%73%50%50%41%47%38%75%47%75%40%50%50%100%67%100%45%65%2 i 50%0 i1 i 100%3 i  67%i 91%i 80%i 84%96%96%96%97%50%100%95%88%34%38%90%72%89%94%100%94%100%100%98%100%99%99%98%98%Sense N i DBCC HumanSLUGanimalfake cointype stripbulletmass unit *metallurgy *STARspace objectshaped objectcelebrityTASTEflavourpreference1 i 0%0 i --0 i --4 i 100%5 i  ao%4 i 75%0!
--11 j 45%15i 53%21 i 100%261 96%47 i 98%Thes.0% 100%-- 50%-- 100%50% 100%-- 100%- 100%40% 97%75% 96%- 95%64% 82%67% 96%95% 93%85% 93%89% 93%Notes:1.
N marks the column with the number of tcst samples foreach sense.
DBCC (Defmition-Bascd Conceptual Co-occurrence) and Human mark the columns with the resultsof our system and the human subject in disambiguating theoccurrences of the 12 words in the Brown corpus,respectively.
Thes.
(thesaurus) marks the column with theresults of Yarowsky (1992) tested on the Grolier'sEncyclopedia.2.
The "correct" sense of each test sample is chosen byhand disambiguation carried out by the author using thesentence as the context.
A small proportion of test samplescannot be disambiguated within the given context and areexcluded from the experiment.3.
The senses marked with * are used in Yarowsky (1992)but no corresponding sense is found in LDOCE.4.
The sense marked with ** is defined in LDOCE but notused in Yarowsky (1992).6.
In our experiment, the words are disambiguatedbetween all the senses listed except he ones marked with7.
The rare senses listed in LDOCE are not listed here.For some of the words, more than one sense listed inLDOCE corresponds to a sense as used in Yarowsky(1992).
In these cases, the senses used by Yarowsky areadopted for easier comparison.8.
All results are based on 100% recall.1865 Related WorkPrevious attempts to tackle the data sparsenessproblem in general corpus-based work include theclass-based approaches and similarity-basedapproaches.
In these approaches, relationshipsbetween a given pair of words are modelled byanalogy with other words that resemble the givenpair in some way.
The class-based approaches(Brown et al, 1992; Resnik, 1992; Pereira et al,1993) calculate co-occurrence data of wordsbelonging to different classes,~ rather thanindividual words, to enhance the co-occurrence datacollected and to cover words which have lowoccurrence frequencies.
Dagan et al (1993) arguethat using a relatively small number of classes tomodel the similarity between words may lead tosubstantial loss of information.
In the similarity-based approaches (Dagan et al, 1993 & 1994;Grishman et al, 1993), rather than a class, eachword is modelled by its own set of similar wordsderived from statistical data collected from corpora.However, deriving these sets of similar wordsrequires a substantial mount of statistical data andthus these approaches require relatively largecorpora to start with.~ 2Our definition-based approach to statistical sensedisambiguation is similar in spirit to the similarity-based approaches, with respect to the "specificity" ofmodelling individual words.
However, usingdefinitions from existing dictionaries rather thanderived sets of similar words allows our method towork on corpora of much smaller sizes.
In ourapproach, each word is modelled by its own set ofdefining concepts.
Although only 1792 definingconcepts are used, the set of all possiblecombinations (a power set of the defining concepts)is so huge that it is very unlikely two word senseswill have the same combination of defining conceptsunless they are almost identical in meaning.
On theother hand, the thesaurus-based method of Yarowsky(1992) may suffer from loss of information (since itis semi-class-based) as well as data sparseness ( inceH Classes used in Resnik (1992) are based on theWordNet taxonomy while classes of Brown et al(1992) and Pereira et al (1993) are derived fromstatistical data collected from corpora.~2 The corpus used in Dagan et al (1994) contains40.5 million words.it is based on salient words) and may not perform aswell on general text as our approach.6 Limitation and Further workBeing a dictionary-based method, the naturallimitation of our approach is the dictionary.
Themost serious problem is that many of the words inthe controlled vocabulary of LDOCE are polysemousthemselves.
The result is that many of our list of1792 defining concepts actually stand for a numberof distinct concepts.
For example, the definingconcept point is used in its place sense, idea senseand sharp end sense in different definitions.
Thisaffects the accuracy of disambiguating senses whichhave definitions containing these polysemous wordsand is found to be the main cause of errors for mostof the senses with below-average r sults.We are currently working on ways todisambiguate the words in the dictionary definitions.One possible way is to apply the current method ofdisambiguation on the defining text of dictionaryitself.
The LDOCE defining text has roughly half amillion words in its 41000 entries, which is half thesize of the Brown corpus used in the currentexperiment.
Although the result on the dictionarycannot be expected to be as good as the result on theBrown corpus due to the smaller size of thedictionary, the reliability of further co-occurrencedata collected and, thus, the performance of thedisambiguation system can be improved significantlyas long as the disambiguation of the dictionary isconsiderably more accurate than by chance.Our success in using definitions of word senses toovercome the data sparseness problem may also leadto further improvement of sense disambiguationtechnologies.
In many cases, semantic coherenceinformation is not adequate to select the correctsense, and knowledge about local constraints isneeded.
~3 For disambiguation f polysemous nouns,these constraints include the modifiers of thesenouns and the verbs which take these nouns asobjects, etc.
This knowledge has been successfullyacquired from corpora in manual or semi-automaticapproaches such as that described in Hearst (1991).However, fully automatic lexically based approaches3 Hatzivassiloglou (1994) shows that theintroduction of linguistic cues improves theperformance of a statistical semantic knowledgeacquisition system in the context of word grouping.187such as that described in Yarowsky (1992) are veryunlikely to be capable of acquiring this finerknowledge because the problem of data sparsenessbecomes even more serious with the introduction ofsyntactic onstraints.
Our approach has overcomethe data sparseness problem by using the definingconcepts of words.
It is found to be effective inacquiring semantic coherence knowledge from arelatively small corpus.
It is possible that a similarapproach based on dictionary definitions will besuccessful in acquiring knowledge of localconstraints from a reasonably sized corpus.7 ConclusionWe have shown that using definition-basedconceptual co-occurrence data collected from arelatively small corpus, our sense disambiguationsystem has achieved accuracy comparable to humanperformance given the same amount of contextualinformation.
By overcoming the data sparsenessproblem, contextual information from a smaller localcontext becomes ufficient for disambiguation i alarge proportion of cases.AcknowledgmentstI would like to thank Robert Dale and VanceGledhill for their helpful comments on earlier draftsof this paper, and Richard Buckland and Mark Drasfor their help with the statistics.ReferencesBlack, E., 1988.
An Experiment In ComputationalDiscrimination of English Word Senses.
IBMJournal of research and development, vol.
32,pp.
185-194.Brown, P., et al, 1991.
Word-sense Disambiguationusing Statistical Methods.
In Proceedings of 29thannual meeting of ACL, pp.264-270.Brown, P. et al, 1992.
Class-based n-gram Modelsof Natural Language.
Computational Linguistics,18(4):467-479.Church, K. and P. Hanks, 1989.
Word AssociationNorms, Mutual Information, and Lexicography.
InProceedings of the 27th Annual Meeting of theAssociation for Computational Linguistics, pp.76-83.Dagan, I. et al, 1991.
Two Languages Are MoreInformative Than One.
In Proceedings ofthe 29thAnnual Meeting of the ACL, pp130-137.Dagan, I. et al, 1993.
Contextual Word Similarityand Estimation From Sparse Data.
In Proceedings ofthe 31st Annual Meeting of the ACL.Dagan, I. et al, 1994.
Similarity-Based Estimationof Word Cooccurrence Probabilities.
In Proceedingsof the 32nd Annual Meeting of the ACL, Las Cruces,pp272-278.Fano, R., 1961.
Transmission of Information.
MITPress, Cambridge, Mass.Gale, W., et al, 1992.
A Method for DisambiguatingWord Senses in a Large Corpus.
Computer andHumanities, vol.
26 pp.415-439.Grishman, R. and J.
Sterling, 1993.
Smoothing ofautomatically generated selectional constraints.
InHuman Language Technology, pp.254-259, SanFrancisco, California.
Advanced Research ProjectsAgency, Software and Intelligent SystemsTechnology Office, Morgan Kanfmann.Hatzivassiloglou, V., 1994.
Do We Need LinguisticsWhen We Have Statistics?
A Comparative Analysisof the Contributions of Linguistic Cues to aStatistical Word Grouping System.
In Proceedingsof Workshop The Balancing Act: CombiningSymbolic and Statistical Approaches toLanguage,Las Cruces, New Mexico.
Association ofComputational Linguistics.Hearst, M., J991.
Noun Homograph DisambiguationUsing Local Context in Large Text Corpora, UsingCorpora, University of Waterloo, Waterloo, Ontario.Kelly, E. and P. Stone, 1975.
Computer Recognitionof English Word Senses, North-Holland, Amsterdam.Pereira F., et al, 1993.
Distributional Clustering ofEnglish words.
In Proceedings of the 31st AnnualMeeting of the ACL.
pp183-190.Procter, P., et al (eds.
), 1978.
Longman Dictionaryof Contemporary English, Longman Group.Resnik, P., 1992.
WordNet and distributionalanalysis: A class-based approach to lexicaldiscovery.
In Proceedings of AAAI Workshop onStatistically-based NLP Techniques, San Jose,California.Yarowsky, D., 1992.
Word-sense Disambiguationusing Statistical Models of Roget's CategoriesTrained on Large Corpora.
In Proceedings ofCOLING9 2, pp.454-460.188
