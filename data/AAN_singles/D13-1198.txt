Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1926?1932,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsLearning to rank lexical substitutionsGyo?rgy Szarvas1Amazon Inc.szarvasg@amazon.comRo?bert Busa-Fekete2 Eyke Hu?llermeierUniversity of MarburgHans-Meerwein-Str., 35032 Marburg, Germanybusarobi@mathematik.uni-marburg.deeyke@mathematik.uni-marburg.deAbstractThe problem to replace a word with a syn-onym that fits well in its sentential contextis known as the lexical substitution task.
Inthis paper, we tackle this task as a supervisedranking problem.
Given a dataset of targetwords, their sentential contexts and the poten-tial substitutions for the target words, the goalis to train a model that accurately ranks thecandidate substitutions based on their contex-tual fitness.
As a key contribution, we cus-tomize and evaluate several learning-to-rankmodels to the lexical substitution task, includ-ing classification-based and regression-basedapproaches.
On two datasets widely used forlexical substitution, our best models signifi-cantly advance the state-of-the-art.1 IntroductionThe task to generate lexical substitutions in context(McCarthy and Navigli, 2007), i.e., to replace wordsin a sentence without changing its meaning, has be-come an increasingly popular research topic.
Thistask is used, e.g.
to evaluate semantic models withregard to their accuracy in modeling word meaningin context (Erk and Pado?, 2010).
Moreover, it pro-vides a basis of NLP applications in many fields,including linguistic steganography (Topkara et al2006; Chang and Clark, 2010), semantic text simi-larity (Agirre et al 2012) and plagiarism detection(Gipp et al 2011).
While closely related to WSD,1Work was done while working at RGAI of the HungarianAcad.
Sci.
and University of Szeged.2 R. Busa-Fekete is on leave from the Research Group onArtificial Intelligence of the Hungarian Academy of Sciencesand University of Szeged.lexical substitution does not rely on explicitly de-fined sense inventories (Dagan et al 2006): the pos-sible substitutions reflect all conceivable senses ofthe word, and the correct sense has to be ascertainedto provide an accurate substitution.While a few lexical sample datasets (McCarthyand Navigli, 2007; Biemann, 2012) with human-provided substitutions exist and can be used toevaluate different lexical paraphrasing approaches,a practically useful system must also be able torephrase unseen words, i.e., any word for whicha list of synonyms is provided.
Correspondingly,unsupervised and knowledge-based approaches thatare not directly dependent on any training material,prevailed in the SemEval 2007 shared task on En-glish Lexical Substitution and dominated follow-upwork.
The only supervised approach is limited tothe combination of several knowledge-based lexi-cal substitution models based on different underly-ing lexicons (Sinha and Mihalcea, 2009).3A recent work by Szarvas et al(2013) de-scribes a tailor-made supervised system based ondelexicalized features that ?
unlike earlier super-vised approaches, and similar to unsupervised andknowledge-based methods proposed for this task ?is able to generalize to an open vocabulary.
Foreach target word to paraphrase, they first computea set of substitution candidates using WordNet: allsynonyms from all of the target word?s WordNetsynsets, together with the words from synsets insimilar to, entailment and also see relation to thesesynsets are considered as potential substitutions.Each candidate then constitutes a training (or test)3Another notable example for supervised lexical substitu-tion is Biemann (2012), but this is a lexical sample system ap-plicable only to the target words of the training datasets.1926example, and these instances are characterized usingnon-lexical features from heterogeneous evidencesuch as lexical-semantic resources and distributionalsimilarity, n-gram counts and shallow syntactic fea-tures computed on large, unannotated backgroundcorpora.
The goal is then i) to predict how wella particular candidate fits in the original context,and ii) given these predictions for each of the can-didates, to correctly order the elements of the candi-date set according to their contextual fitness.
That is,a model is successful if it prioritizes plausible substi-tutions ahead of less likely synonyms (given the con-text).
This model is able to generate paraphrases fortarget words not contained in the training material.This favorable property is achieved using only suchfeatures (e.g.
local n-gram frequencies in context)that are meaningfully comparable across the differ-ent target words and candidate substitutions they arecomputed from.
More importantly, their model alsoprovides superior ranking results compared to stateof the art unsupervised and knowledge based ap-proaches and therefore it defines the current state ofthe art for open vocabulary lexical substitution.Motivated by the findings of Szarvas et al(2013),we address lexical substitution as a supervised learn-ing problem, and go beyond their approach froma methodological point of view.
Our experimentsshow that the performance on the lexical substitutiontask is strongly influenced by the way in which thistask is formalized as a machine learning problem(i.e., as binary or multi-class classification or regres-sion) and by the learning method used to solve thisproblem.
As a result, we are able to report the bestperformances on this task for two standard datasets.2 Related workPrevious approaches to lexical substitution oftenseek to automatically generate a set of candidatesubstitutions for each target word first, and to rankthe elements of this set of candidates afterward (Has-san et al 2007; Giuliano et al 2007; Martinez etal., 2007; Yuret, 2007).
Alternatively, the candidateset can be defined by all human-suggested substi-tutions for the given target word in all of its con-texts; then, the focus is just on the ranking problem(Erk and Pado?, 2010; Thater et al 2010; Dinu andLapata, 2010; Thater et al 2011).
While only theformer approach qualifies as a full-fledged substitu-tion system for arbitrary, previously unseen targetwords, the latter simplifies the comparison of se-mantic ranking models, as the ranking step is notburdened with the shortcomings of automaticallygenerated substitution candidates.As mentioned before, Szarvas et al(2013) re-cently formalized the lexical substitution problem asa supervised learning task, using delexicalized fea-tures.
This non-lexical feature representation makesdifferent target word/substitution pairs in differentcontexts4 directly comparable.
Thus, it becomespossible to learn an all-words system that is appli-cable to unseen words, using supervised methods,which provides superior ranking accuracy to unsu-pervised and knowledge based models.In this work, we build on the problem formu-lation and the features proposed by Szarvas etal.
(2013) while largely extending their machinelearning methodology.
We customize and experi-ment with several different learning-to-rank models,which are better tailored for this task.
As our experi-ments show, this contribution leads to further signif-icant improvements in modeling the semantics of atext and in end-system accuracy.3 Datasets and experimental setupHere we introduce the datasets, experimental setupand evaluation measures used in our experiments.Since space restrictions prohibit a comprehensiveexposition, we only provide the most essential in-formation and refer to Szarvas et al(2013), whoseexperimental setup we adopted, for further details.Datasets.
We use two prominent datasets for lex-ical substitution.
The LexSub dataset introducedin the Lexical Substitution task at Semeval 2007(McCarthy and Navigli, 2007)5 contains 2002 sen-tences for a total of 201 target words (from allparts of speech), and lexical substitutions assigned(to each target word and sentence pair) by 5 na-tive speaker annotators.
The second dataset, TWSI(Biemann, 2012)6, consists of 24,647 sentences fora total of 1,012 target nouns, and lexical substitu-4E.g., bright substituted with intelligent in ?He was brightand independent and proud?
and side for part in ?Find someonewho can compose the biblical side?.5http://nlp.cs.swarthmore.edu/semeval/tasks/task10/data.shtml6http://www.ukp.tu-darmstadt.de/data/lexical-resources/twsi-lexical-substitutions/1927tions for each target word in context resulting froma crowdsourced annotation process.For each sentence in each dataset, the annotatorsprovided as many substitutions for the target wordas they found appropriate in the context.
Each sub-stitution is then labeled by the number of annotatorswho listed that word as a good lexical substitution.Experimental setup and Evaluation.
On bothdatasets, we conduct experiments using a 10-foldcross validation process, and evaluate all learning al-gorithms on the same train/test splits.
The datasetsare randomly split into 10 equal-sized folds on thetarget word level, such that all examples for a par-ticular target word fall into either the training orthe test set, but never both.
This way, we makesure to evaluate the models on target words not seenduring training, thereby mimicking an open vocab-ulary paraphrasing system: at testing time, para-phrases are ranked for unseen target words, simi-larly as the models would rank paraphrases for anywords (not necessarily contained in the dataset).
Foralgorithms with tunable parameters, we further di-vide the training sets into a training and a validationpart to find the best parameter settings.
For evalua-tion, we use Generalized Average Precision (GAP)(Kishida, 2005) and Precision at 1 (P@1), i.e., thepercentage of correct paraphrases at rank 1.Features.
In all experiments, we used the featuresdescribed in Szarvas et al(2013), implemented pre-cisely as proposed by the original work.Each (sentence, targetword, substitution)triplet represents an instance, and the feature valuesare computed from the sentence context, the targetword and the substitution word.
The features usedfall into four major categories.The most important features describe the syntag-matic coherence of the substitution in context, mea-sured as local n-gram frequencies obtained fromweb data.
The frequency for a 1-5gram context withthe substitution word is computed and normalizedwith respect to either 1) the frequency of the origi-nal context (with the target word) or 2) the sum offrequencies observed for all possible substitutions.A third feature computes similar frequencies for thesubstitution and the target word observed in the lo-cal context (as part of a conjunctive phrase).A second group of features describe the (non-positional, i.e.
non-local) distributional similarity ofthe target and its candidate substitution in termsof sentence level co-occurrence statistics collectedfrom newspaper texts: 1) How many words from thesentence appear in the top 1000 salient words listedfor the candidate substitution in a distributional the-saurus, 2) how similar the top K salient words listsare for the candidate and the target word, 3) howsimilar the 2nd order distributional profiles are forcandidate and target, etc.
All these features are care-fully normalized so that values compare well accrossdifferent words and contexts.Another set of features capture the properties ofthe target and candidate word in WordNet, such astheir 1) number of senses, 2) how frequent sensesare synonymous and 3) the lowest common ancestor(and all synsets up) for the candidate and target wordin the WordNet hierarchy (represented as a nominalfeature, by the ID of these synsets).Lastly a group of features capture shallow syntac-tic patterns of the target word and its local context inthe form of 1) part of speech patterns (trigrams) ina sliding window around the target word using mainPOS categories, i.e.
only the first letter of the PennTreebank codes, and 2) the detailed POS code of thecandidate word assigned by a POS tagger.We omit a mathematically precise description ofthese features for space reasons and refer the readerto Szarvas et al(2013) for a more formal anddetailed description of the feature functions.
Im-portantly, these delexicalized features are numeri-cally comparable across the different target wordsand candidate substitutions they are computed from.This property enables the models to generalize overthe words in the datasets and thus enables a super-vised, all-words lexical substitution system.4 Learning-to-Rank methodsMachine learning methods for ranking are tradition-ally classified into three categories.
In the point-wise approach, a model is trained that maps in-stances (in this case candidate substitutions in a con-text) to scores indicating their relevance or fitness;to this end, one typically applies standard regressiontechniques, which essentially look at individual in-stances in isolation (i.e., independent of any otherinstances in the training or test set).
To predict aranking of a set of query instances, these are sim-ply sorted by their predicted scores (Li et al 2007).1928The pairwise approach trains models that are ableto compare pairs of instances.
By marking such apair as positive if the first instance is preferred to thesecond one, and as negative otherwise, the problemcan formally be reduced to a binary classificationtask (Freund et al 2003).
Finally, in the listwise ap-proach, tailor-made learning methods are used thatdirectly optimize the ranking performance with re-spect to a global evaluation metric, i.e., a measurethat evaluates the ranking of a complete set of queryinstances (Valizadegan et al 2009).Below we give a brief overview of the methods in-cluded in our experiments.
We used the implementa-tions provided by the MultiBoost (Benbouzid et al2012), RankSVM and RankLib packages.7 For a de-tailed description, we refer to the original literature.4.1 MAXENTThe ranking model proposed by Szarvas et al(2013)was used as a baseline.
This is a pointwise approachbased on a maximum entropy classifier, in whichthe ranking task is cast as a binary classificationproblem, namely to discriminate good (label > 0)from bad substitutions.
The actual label values forgood substitutions were used for weighting the train-ing examples.
The underlying MaxEnt model wastrained until convergence, i.e., there was no hyper-parameter to be tuned.
For a new target/substitutionpair, the classifier delivers an estimation of the pos-terior probability for being a good substitution.
Theranking is then produced by sorting the candidatesin decreasing order according to this probability.4.2 EXPENSEXPENS (Busa-Fekete et al 2013) is a point-wise method with listwise meta-learning stepthat exploits an ensemble of multi-class classi-fiers.
It consists of three steps.
First, AD-ABOOST.MH (Schapire and Singer, 1999) classi-fiers with several different weak learners (Busa-Fekete et al 2011; Ke?gl and Busa-Fekete, 2009)are trained to predict the level of relevance (qual-ity) of a substitution (i.e., the number of annotatorswho proposed the candidate for that particular con-text).
Second, the classifiers are calibrated to obtain7RankLib is available at http://people.cs.umass.edu/?vdang/ranklib.html.
We extended the imple-mentation of the LAMBDAMART algorithm in this package tocompute the gradients of and optimize for the GAP measure.an accurate posterior distribution; to this end, severalcalibration techniques, such as Platt scaling (Platt,2000), are used to obtain a diverse pool of calibratedclassifiers.
Note that this step takes advantage ofthe ordinal structure of the underlying scale of rel-evance levels, which is an important difference toMAXENT.
Third, the posteriors of these calibratedclassifiers are additively combined, with the weightof each model being exponentially proportional toits GAP score (on the validation set).
This methodhas two hyperparameters: the number of boosting it-erations T and the scaling factor in the exponentialweighting scheme c. We select T and c from the in-tervals [100, 2000] and [0, 100], with step sizes 100and 10, respectively.4.3 RANKBOOSTRANKBOOST (Freund et al 2003) is a pairwiseboosting approach.
The objective function is therank loss (as opposed to ADABOOST, which opti-mizes the exponential loss).
In each boosting it-eration, the weak classifier is chosen by maximiz-ing the weighted rank loss.
For the weak learner,we used the decision stump described in (Freund etal., 2003), which is able to optimize the rank lossin an efficient way.
The only hyperparameter ofRANKBOOST to be tuned is the number of iterationsthat we selected from the interval [1, 1000].4.4 RANKSVMRANKSVM (Joachims, 2006) is a pairwise methodbased on support vector machines, which formulatesthe ranking task as binary classification of pairs ofinstances.
We used a linear kernel, because the opti-mization using non-linear kernels cannot be done ina reasonable time.
The tolerance level of the op-timization was set to 0.001 and the regularizationparameter was validated in the interval [10?6, 104]with a logarithmically increasing step size.4.5 LAMBDAMARTLAMBDAMART (Wu et al 2010) is a listwisemethod based on the gradient boosted regressiontrees by Friedman (1999).
The ordinal labels arelearned directly by the boosted regression treeswhose parameters are tuned by using a gradient-based optimization method.
The gradient of parame-ters is calculated based on the evaluation metric used(in this case GAP).
We tuned the number of boosting1929Database LexSub TWSICandidates WN Gold WN GoldGAPMaxEnt 43.8 52.4 36.6 47.2ExpEns 44.3 53.5 37.8 49.7RankBoost 44.0 51.4 37.0 47.8RankSVM 43.3 51.8 35.5 45.2LambdaMART 45.5 55.0 37.8 50.1P@1MaxEnt 40.2 57.7 32.4 49.5ExpEns 39.8 58.5 33.8 53.2RankBoost 40.7 55.2 33.1 50.8RankSVM 40.3 51.7 33.2 45.1LambdaMART 40.8 60.2 33.1 53.6Table 1: GAP and p@1 values, with significant improve-ments over the performance of MaxEnt marked in bold.System GAPErk and Pado?
(2010) 38.6Dinu and Lapata (2010) 42.9Thater et al(2010) 46.0Thater et al(2011) 51.7Szarvas et al(2013) 52.4EXPENS 53.5LAMBDAMART 55.0Table 2: Comparison to previous studies (dataset LexSub,candidates Gold).iterations in the interval [10, 1000] and the numberof tree leaves in {8, 16, 32}.5 Results and discussionOur results using the above learning methods aresummarized in Table 1.
As can be seen, the twomethods that exploit the cardinal structure of thelabel set (relevance degrees), namely EXPENS andLAMBDAMART, consistently outperform the base-line taken from Szarvas et al(2013) ?
the only ex-ception is the p@1 score for EXPENS on the SemevalLexical Substitution dataset and the candidate sub-stitutions extracted from WordNet.
The improve-ments are significant (using paired t-test, p < 0.01)for 3 out of 4 settings for EXPENS and in all settingsfor LAMBDAMART.
In particular, the results ofLAMBDAMART are so far the best scores that havebeen reported for the best studied setting, i.e.
theLexSub dataset using substitution candidates takenfrom the gold standard (see Table 2).We suppose that the relatively good resultsachieved by the LAMBDAMART and EXPENSmethods are due to that, first, it seems crucial toproperly model and exploit the ordinal nature ofthe annotations (number of annotators who sug-gested a given word as a good paraphrase) pro-vided by the datasets.
Second, the RANKBOOST andRANKSVM are less complex methods than the EX-PENS and LAMBDAMART.
The RANKSVM is theleast complex method from the pool of learning-to-rank methods we applied, since it is a simple lin-ear model.
The RANKBOOST is a boosted decisionstump where, in each boosting iteration, the stumpis found by maximizing the weighted exponentialrank loss.
On the other hand, both the EXPENSand LAMBDAMART make use of tree learners in theensemble classifier they produce.
We believe thatoverfitting is not an issue in a learning task like theLexSub task: most features are relatively weak pre-dictors on their own, and we can learn from a largenumber of data points (2000 sentences with an av-erage set size of 20, about 40K data points for thesmallest dataset and setting).
Rather, as our resultsshow, less complex models tend to underfit the data.Therefore we believe that more complex models canachieve a better performance, of course with an in-creased computational cost.6 Conclusion and future workIn this paper, we customized and applied some rela-tively novel algorithms from the field of learning-to-rank for ranking lexical substitutions in context.
Inturn, we achieved significant improvements on thetwo prominent datasets for lexical substitution.Our results indicate that an exploitation of the or-dinal structure of the labels in the datasets can lead toconsiderable gains in terms of both ranking quality(GAP) and precision at 1 (p@1).
This observationis supported both for the theoretically simpler point-wise learning approach and for the most powerfullistwise approach.
On the other hand, the pairwisemethods that cannot naturally exploit this property,did not provide a consistent improvement over thebaseline.
In the future, we plan to investigate thisfinding in the context of other, similar ranking prob-lems in Natural Language Processing.AcknowledgmentThis work was supported by the German ResearchFoundation (DFG) as part of the Priority Programme1527.1930ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In *SEM 2012:The First Joint Conference on Lexical and Computa-tional Semantics ?
Volume 1: Proceedings of the mainconference and the shared task, and Volume 2: Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 385?393,Montre?al, Canada.D.
Benbouzid, R. Busa-Fekete, N. Casagrande, F.-D.Collin, and B. Ke?gl.
2012.
MultiBoost: a multi-purpose boosting package.
Journal of Machine Learn-ing Research, 13:549?553.Chris Biemann.
2012.
Creating a System for Lexi-cal Substitutions from Scratch using Crowdsourcing.Language Resources and Evaluation: Special Issueon Collaboratively Constructed Language Resources,46(2).R.
Busa-Fekete, B. Ke?gl, T.
E?lteto?, and Gy.
Szarvas.2011.
Ranking by calibrated AdaBoost.
In (JMLRW&CP), volume 14, pages 37?48.R.
Busa-Fekete, B. Ke?gl, T.
E?lteto?, and Gy.
Szarvas.2013.
Tune and mix: learning to rank using ensemblesof calibrated multi-class classifiers.
Machine Learn-ing, 93(2?3):261?292.Ching-Yun Chang and Stephen Clark.
2010.
Practi-cal linguistic steganography using contextual synonymsubstitution and vertex colour coding.
In Proceedingsof the 2010 Conference on Empirical Methods in Nat-ural Language Processing, pages 1194?1203, Cam-bridge, MA.Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-morshtein, and Carlo Strapparava.
2006.
Direct wordsense matching for lexical substitution.
In Proceed-ings of the 21st International Conference on Compu-tational Linguistics and the 44th annual meeting of theAssociation for Computational Linguistics, ACL-44,pages 449?456, Sydney, Australia.Georgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1162?1172, Cambridge,MA.Katrin Erk and Sebastian Pado?.
2010.
Exemplar-basedmodels for word meaning in context.
In Proceedingsof the ACL 2010 Conference Short Papers, pages 92?97, Uppsala, Sweden.Y.
Freund, R. Iyer, R. E. Schapire, and Y.
Singer.
2003.An efficient boosting algorithm for combining prefer-ences.
Journal of Machine Learning Research, 4:933?969.J.
Friedman.
1999.
Greedy function approximation: agradient boosting machine.
Technical report, Dept.
ofStatistics, Stanford University.Bela Gipp, Norman Meuschke, and Joeran Beel.
2011.Comparative Evaluation of Text- and Citation-basedPlagiarism Detection Approaches using GuttenPlag.In Proceedings of 11th ACM/IEEE-CS Joint Confer-ence on Digital Libraries (JCDL?11), pages 255?258,Ottawa, Canada.
ACM New York, NY, USA.
Avail-able at http://sciplore.org/pub/.Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.2007.
FBK-irst: Lexical substitution task exploit-ing domain and syntagmatic coherence.
In Proceed-ings of the Fourth International Workshop on SemanticEvaluations (SemEval-2007), pages 145?148, Prague,Czech Republic.Samer Hassan, Andras Csomai, Carmen Banea, RaviSinha, and Rada Mihalcea.
2007.
UNT: SubFinder:Combining knowledge sources for automatic lexicalsubstitution.
In Proceedings of the Fourth Interna-tional Workshop on Semantic Evaluations (SemEval-2007), pages 410?413, Prague, Czech Republic.T.
Joachims.
2006.
Training linear svms in linear time.In Proceedings of the ACM Conference on KnowledgeDiscovery and Data Mining (KDD).B.
Ke?gl and R. Busa-Fekete.
2009.
Boosting products ofbase classifiers.
In International Conference on Ma-chine Learning, volume 26, pages 497?504, Montreal,Canada.Kazuaki Kishida.
2005.
Property of Average Precisionand Its Generalization: An Examination of EvaluationIndicator for Information Retrieval Experiments.
NIItechnical report.
National Institute of Informatics.P.
Li, C. Burges, and Q. Wu.
2007.
McRank: Learning torank using multiple classification and gradient boost-ing.
In Advances in Neural Information ProcessingSystems, volume 19, pages 897?904.
The MIT Press.David Martinez, Su Nam Kim, and Timothy Bald-win.
2007.
MELB-MKB: Lexical substitution systembased on relatives in context.
In Proceedings of theFourth International Workshop on Semantic Evalua-tions (SemEval-2007), pages 237?240, Prague, CzechRepublic.Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
InProceedings of the Fourth International Workshop onSemantic Evaluations (SemEval-2007), pages 48?53,Prague, Czech Republic.J.
Platt.
2000.
Probabilistic outputs for support vec-tor machines and comparison to regularized likelihoodmethods.
In A.J.
Smola, P. Bartlett, B. Schoelkopf,and D. Schuurmans, editors, Advances in Large Mar-gin Classifiers, pages 61?74.
MIT Press.1931R.
E. Schapire and Y.
Singer.
1999.
Improved boostingalgorithms using confidence-rated predictions.
Ma-chine Learning, 37(3):297?336.Ravi Sinha and Rada Mihalcea.
2009.
Combining lex-ical resources for contextual synonym expansion.
InProceedings of the International Conference RANLP-2009, pages 404?410, Borovets, Bulgaria.Gyo?rgy Szarvas, Chris Biemann, and Iryna Gurevych.2013.
Supervised all-words lexical substitution us-ing delexicalized features.
In Proceedings of the 2013Conference of the North American Chapter of the As-sociation for Computational Linguistics: Human Lan-guage Technologies (NAACL-HLT 2013), June.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations us-ing syntactically enriched vector models.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 948?957, Uppsala,Sweden.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2011.
Word meaning in context: A simple and effec-tive vector model.
In Proceedings of the Fifth Interna-tional Joint Conference on Natural Language Process-ing : IJCNLP 2011, pages 1134?1143, Chiang Mai,Thailand.
MP, ISSN 978-974-466-564-5.Umut Topkara, Mercan Topkara, and Mikhail J. Atal-lah.
2006.
The hiding virtues of ambiguity: quan-tifiably resilient watermarking of natural language textthrough synonym substitutions.
In Proceedings of the8th workshop on Multimedia and security, pages 164?174, New York, NY, USA.
ACM.H.
Valizadegan, R. Jin, R. Zhang, and J. Mao.
2009.Learning to rank by optimizing NDCG measure.
InAdvances in Neural Information Processing Systems22, pages 1883?1891.Q.
Wu, C. J. C. Burges, K. M. Svore, and J. Gao.
2010.Adapting boosting for information retrieval measures.Inf.
Retr., 13(3):254?270.Deniz Yuret.
2007.
Ku: Word sense disambiguation bysubstitution.
In Proceedings of the Fourth Interna-tional Workshop on Semantic Evaluations (SemEval-2007), pages 207?214, Prague, Czech Republic, June.Association for Computational Linguistics.1932
