Workshop on Humans and Computer-assisted Translation, pages 99?103,Gothenburg, Sweden, 26 April 2014. c?2014 Association for Computational LinguisticsSpeech-Enabled Computer-Aided Translation: A Satisfaction Surveywith Post-Editor TraineesBartolom?
Mesa-LaoCenter for Research and Innovation in Translation and Translation TechnologyDepartment of International Business CommunicationCopenhagen Business School, Denmarkbm.ibc@cbs.dkAbstractThe present study has surveyed post-editortrainees?
views and attitudes before and after theintroduction of speech technology as a front end toa computer-aided translation workbench.
The aimof the survey was (i) to identify attitudes andperceptions among post-editor trainees beforeperforming a post-editing task using automaticspeech recognition (ASR); and (ii) to assess thedegree to which post-editors?
attitudes andexpectations to the use of speech technologychanged after actually using it.
The survey wasbased on two questionnaires: the first oneadministered before the participants performedwith the ASR system and the second one at the endof the session, once they have actually used ASRwhile post-editing machine translation outputs.Overall, the results suggest that the surveyed post-editor trainees tended to report a positive view ofASR in the context of post-editing and they wouldconsider adopting ASR as an input method forfuture post-editing tasks.1 IntroductionIn recent years, significant progress has beenmade in advancing automatic speech recognition(ASR) technology.
Nowadays it can be found atthe other end of customer-support hotlines, it isbuilt into operating systems and it is offered asan alternative text-input method in many mobiledevices.
This technology is not only improving ata steady pace, but is also becoming increasinglyusable and useful.At the same time, the translation industry isgoing through a societal and technologicalchange in its evolution.
In less than ten years, theindustry is considering new tools, workflows andsolutions to service a steadily growing market.Given the significant improvements in machinetranslation (MT) quality and the increasingdemand for translations, post-editing of MT isbecoming a well-accepted practice in thetranslation industry, since it has been shown toallow for larger volumes of translations to beproduced saving time and costs.Against this background, it seems reasonableto envisage an era of converge in the future yearswhere speech technology can make a differencein the field of translation technologies.
As post-editing services are becoming a common practiceamong language service providers and ASR isgaining momentum, it seems reasonable toexplore the interplay between both fields tocreate new business solutions and workflows.In the context of machine-aided humantranslation and human-aided machine translation,different scenarios have been investigated wherehuman translators are brought into the loopinteracting with a computer through a variety ofinput modalities to improve the efficiency andaccuracy of the translation process (e.g.,Dragsted et al.
2011, Toselli et al.
2011, Vidal2006).
ASR systems have the potential toimprove the productivity and comfort ofperforming computer-based tasks for a widevariety of users, allowing them to enter both textand commands into the computer using just theirvoice.
However, further studies need to beconducted to build up new knowledge about theway in which state-of-the-art ASR software canbe applied to one of the most common taskstranslators face nowadays, i.e.
post-editing ofMT outputs.The present study has two related objectives:First, to report on a satisfaction survey with post-editor trainees after showing them how to useASR in post-editing tasks.
Second, based on thefeedback provided by the participants, to assessthe change in users?
expectations and acceptanceof ASR technology as an alternative inputmethod for their daily work.992 MethodIn this study, we explore the potential ofcombining one of the most popular computer-aided translation workbenches in the market (i.e.memoQ) with one of the most well-known ASRpackages (i.e.
Dragon Naturally Speaking fromNuance).2.1 OverviewTwo questionnaires were developed anddeployed as a survey.
The survey was dividedinto two phases, a prospective phase in which wesurveyed post-editor trainees?
views andexpectations toward ASR and a subsequentretrospective phase in which actual post-editor?sexperiences and satisfaction with the technologywere surveyed.
Participants had to answer a 10-item questionnaire in the prospective phase and a7-item questionnaire in the retrospective phase.These two questionnaires partially overlapped,allowing us to compare, for each participant, theanswers given before and after the introductionand use of the target technology.2.2 Participants profileParticipants were recruited through theUniversitat Aut?noma de Barcelona (Spain).
Thegroup included 11 females and 4 males, rangingin age from 22 to 35.
All 15 participants had afull degree in Translation and InterpretingStudies and were regular users of computer-aidedtranslation software (mainly memoQ and SDLTrados Studio).
All of them had alreadyperformed MT post-editing tasks as part of theirprevious training as translators and, at themoment of the data collection, they were alsotaking a 12-hour course on post-editing as part oftheir master?s degree in Translation.
None of theparticipants had ever user Dragon NaturallySpeaking, but four participants declared to havetried the speech input options in their mobilephones to dictate text messages.2.3 ProcedureIndividual sessions occurred at a universityoffice.
In the first part of the session, eachparticipant had to complete an on-linequestionnaire.
This initial survey covered thefollowing topics:1.
General information about their profileas translators; including education, yearsof experience and employment status.2.
Background in computer-aided trans-lation software in their daily life asprofessional translators.3.
Experience in the field of post-editingMT outputs and training received.4.
Information about their usage of ASR ascompared to other input methods and, ifapplicable, likes and dislike about it.In the second part of the session, after theinitial questionnaire was completed, allparticipants performed two post-editing tasksunder the following two input conditions (oneeach):?
Condition 1: non-ASR input modality, i.e.keyboard and mouse.?
Condition 2: ASR input modality com-bined with other non-ASR modalities, i.e.keyboard and mouse.The language pair involved in the tasks wasSpanish to English1.
Two different texts from thedomain of mobile phone marketing were used toperform the post-editing tasks under condition 1and 2.
These two texts were imported to amemoQ project and then fully pre-translatedusing MT coming from the Google API plug-inin memoQ.
The order of the two input conditionsand the two texts in each condition werecounterbalanced across participants.In an attempt to unify post-editing criteriaamong participants, all of them were instructedto follow the same post-editing guidelines aimingat a final high-quality target text2.
In the ASRinput condition, participants also read in hardcopy the most frequent commands in DragonNaturally Speaking v.10 that they could use topost-edit using ASR (Select <w>, Scratch that,Cut that, etc.).
All of them had to do the basictraining tutorial included in the software (5minutes training on average per participant) inorder to improve the recognition accuracy.Following the training, participants also had thechance to practice the dictation of text andcommands before actually performing the twopost-editing tasks.1 Participants performed from L1 to L2.2 The post-editing guidelines distributed in hard copywere: i) Retain as much raw MT as possible; ii) Donot introduce stylistic changes; iii) Make correctionsonly where absolutely necessary, i.e.
correct wordsand phrases that are clearly wrong, inadequate orambiguous according to English grammar; iv) Makesure there are no mistranslations with regard to theSpanish source text; v) Publishable quality is expected.100In the third part of the session, participantscompleted a 7-item post-session questionnaireregarding their opinions about ASR while post-editing.2.4 Data collection and analysisSurvey dataFor questionnaires?
data, responses toquantitative items were entered into aspreadsheet and mean responses were calculatedacross participants.
For a comparison ofresponses to different survey items, pairedstatistics were used: paired t-test for items codedas ordinal variables, and chi-square test for itemscoded as categorical variables.
Thequestionnaires did not include open-endedquestions or comments.Task log filesFor task performance data (which is not going tobe elaborated in this paper), computer screenincluding audio was recorded using BBFlashBack Recorder Pro v. 2.8 from BlueberrySoftware.
With the use of the video recordings, atime-stamped log of user actions and ASRsystem responses was produced for eachparticipant.
Each user action was coded for thefollowing: (i) input method involved; (ii) for thepost-editing task involving ASR, text entry ratein the form of text or commands, and (iii), for thesame task, which method of error correction wasused.Satisfaction dataResponses to the post-session questionnaire wereentered and averaged.
We computed an overallASR ?satisfaction score?
for each participant bysumming the responses to the seven items thatrelated to satisfaction with ASR.
We computed a95 percent confidence interval (CI) for the meanof the satisfaction score to create boundedestimated for the satisfaction score.3 Survey results3.1 Usage of speech input methodTo determine why participants would decide touse ASR in the future to post-edit, we askedthem to rate the importance of eight differentreasons, on a scale of 1 to 7, with 7 being thehighest in importance.
The top reason fordeciding to use ASR was that it would involveless fatigue (Table 1).Reasons for using speechinput method Mean  95% CILess fatigue 5.6* 4.9, 6.4Speed  5.5* 4.8, 6.3Ease of use 4.9* 4.7, 5.3Cool technology  4.7* 4.0, 4.8Limited alternatives 3.1 2.9, 3.3Accuracy 2.9 2.1, 3.2Personal preference 2.7 2.3, 2.9Others 1 1, 1.2* Reasons with importance significantly greater thanneutral rating of 4.0 (p < 0.05)Table 1: Importance of reasons for using automaticspeech recognition (ASR), rated on a scale from 1 to 7.3.2 Usage of non-speech input methodsSince none of the participants had ever used ASRto perform any of their translation or post-editingassignments before, and in order to understandthe relative usage data, we also askedparticipants about their reasons for choosing non-speech input methods (i.e.
keyboard and mouse).For this end, they rated the importance of sixreasons on a scale of 1 to 7, with 7 being mostimportant.
In the introductory questionnaire,most participants believed that keyboard short-cuts would be quicker and easier than usingspoken commands (Table 2).Reasons for using non-speech input methods Mean  95% CIThey are easier 6.5* 5.7, 6.8Less setup involved 6.1* 5.5, 6.3Frustration with speech 5.9* 5.2, 6.1They are faster 3.1 2.7, 3.8Just for variety 2.0 1.3, 2.8To rest my voice 1.3 1.1, 2.3* Reasons with importance significantly greater thanneutral rating of 4.0 (p < 0.05)Table 2: Importance of reasons for choosing non-speech input methods instead of automatic speechrecognition, rated on a scale from 1 to 7.Having to train the system (setup involved) inorder to improve recognition accuracy ordonning a headset for dictating was initiallyperceived as a barrier for using ASR as thepreferred input method.
According to the survey,participants would also choose other inputmethods when ASR performed poorly or not atall, either in general or for dictating particular101commands (e.g., for some participants thecommand Cut that was consistently recognizedas Cap that).
Less important reasons were theneed to rest one?s voice or to switch methods justfor variety.3.3 Opinions about speech and non-speechinput methodsParticipants rated their satisfaction with 10usability indicators for both ASR and non-ASRalternatives (Tables 3 and 4).Likes % responding yes ASR Non-ASREase  85.3 91.9Speed 74.9 88.6Less effort 73.9 75.3Fun 62.3 23.6Accuracy 52.7 85.3Trendy 39.5 23.1Table 3: Percentage of participants who likedparticular aspects of the automatic speech recognition(ASR) system and non-speech input methods.Dislikes % responding yes ASR Non-ASRFixing recognition mistakes 74.5 ?Disturbs colleagues  45.9 ?Setup involved 36.8 ?Fatigue 17.3 12.7Table 4: Percentage of participants who dislikedparticular aspects of the automatic speech recognition(ASR) system and non-speech input methods.ASR for translator-computer interactionsucceeds at easing the task (its most-likedbenefit).
Almost 75% liked the speed theyarchived with ASR, despite being slower whencompared against non-ASR input methods.Almost 74% liked the effort required to use ASR,and only 17.3% found it fatiguing.
Participant?slargest complaint with ASR was related torecognition accuracy.
Only 52.7% liked therecognition accuracy they achieved and fixingrecognition mistakes ranked as the top dislike at74.5%.
The second most frequent dislike waspotential work environment dissonance or loss ofprivacy during use of ASR at 45.9% ofparticipants.Ratings show significant differences betweenASR and non-speech input methods, particularlywith regard to accuracy and amusement involved(Fun item in the questionnaire).3.4 Post-session questionnaire resultsTo further examine subjective opinions of ASRin post-editing compared to non-speech inputmethods, we asked participants to rate theiragreement to several statements regardinglearnability, ease of use, reliability and fun afterperforming the post-editing tasks under the twoconditions.
Agreement was rated on a scale of 1to 7, from ?strongly disagree?
to ?stronglyagree?.
Table 5 shows participants?
level ofagreement with the seven statements in the post-session questionnaire.StatementLevel ofagreementMean 95% CI1.
I expected using ASR in post-editing to be more difficult than itactually is.6.6* 6.5, 6.82.
My performance with theselection of ASR commandsimproved by the end of the session.6.5* 5.4, 6.93.
The system correctly recognizesalmost every command I dictate.
5.9* 5.5, 6.44.
It is difficult to correct errorsmade by the ASR software.
2.9 2.3, 4.15.
Using ASR in the context ofpost-editing can be a frustratingexperience.2.4 1.9, 3.86.
I can enter text more accuratelywith ASR than with any othermethod.2.1 1.7, 2.97.
I was tired by the end of thesession.
1.7 1.2, 2.9* Agreement significantly greater than neutral ratingof 4.0 (p < 0.05)Table 5: Participants?
level of agreement to statementsabout ASR input method in post-editing tasks.Ratings are on scale 1 to 7, from ?strong disagree?
to?strongly agree?, with 4.0 representing neutral rating.The results of the post-session questionnaireshow that participants had significantly greaterthan neutral agreement (positively) about ASR inthe context of post-editing.
Overall they agreedthat it is easier to use ASR for post-editingpurposes than they actually thought.
They alsopositively agreed that the ASR software was ableto recognize almost every command theydictated (i.e.
Select <w>, Scratch that, etc.)
andacknowledged that their performance whendictating commands was better as they becamemore familiar with the task.When scores were combined for the sevenstatements into an overall satisfaction score, theaverage was 73.5 [66.3, 87.4], on a scale of 0 to1021003 .
Thus, this average is significantly morepositive than neutral.
12 out of the 15 surveyedparticipants stated that they will definitelyconsider adopting ASR in combination with non-speech input modalities in their daily practice asprofessional translators.4 DiscussionThe results of the present study show that thesurveyed post-editor trainees tended to report avery positive view on the use of ASR in thecontext of post-editing.
In general, findingssuggest that human translators would not regretthe integration of ASR as one of the possibleinput methods for performing post-editing tasks.While many questions regarding effective useof ASR remain, this study provides some basisfor further efforts to better integrate ASR in thecontext of computer-aided translation.
Somespecific insights supported by the collected dataare:?
Expectations about ASR were definitelymore positive after having performed withspeech as an input method.
Participantspositively agreed that it is easier and moreeffective than previously thought.?
Most of the challenges (dislikes) of ASRwhen compared to other non-inputmethods can be tacked if the user isprovided with both ASR and non-ASRinput methods for them to be used at theirconvenience.
Participants?
views seem toindicate that they would use ASR as acomplement rather than a substitute fornon-speech input methods.5 ConclusionsPost-editor trainees have a positive view of ASRwhen combining traditional non-speech inputmethods (i.e.
keyboard and mouse) with the useof speech.
Acknowledging this up front, aninteresting field for future work is to introduceproper training on correction strategies.
Studiesin this direction could help to investigate howtraining post-editors to apply optimal correctionstrategies can help them to increase performanceand, consequently, user satisfaction.3 A score of 100 represents a strong agreement withall positive statements and a strong disagreement withall negative statements, while a score of 50 representsa neutral response to all statements.AcknowledgmentsWe would like to thank all the participants in thisstudy for their generous contributions of time,effort and insights.ReferencesDragsted, B., Mees, I. M., Gorm Hansen, I.
2011.Speaking your translation: students?
first encounterwith speech recognition technology, Translation &Interpreting, Vol 3(1).Dymetman,M., Brousseau, J., Foster, G., Isabelle, P.,Normandin, Y., & Plamondon, P. 1994.
Towardsan automatic dictation system for translators: theTransTalk project.
Proceedings of the internationalconference on spoken language processing (ICSLP94), 691?694.Koester, HH.
2004.
Usage, performance, andsatisfaction outcomes for experienced users ofautomatic speech recognition.
Journal ofRehabilitation Research & Development.
Vol 41(5):739-754.O?Brien, S. 2012.
Translation as human-computerinteraction.
Translation Spaces, 1(1), 101-122.Toselli, A., Vidal, E., Casacuberta, F. 2011.Multimodal Interactive Pattern Recognition andApplications.
Springer.Vidal, E., Casacuberta, F., Rodr?guez, L., Civera, J.,Mart?nez-Hinarejos.
C.D.
2006.
Computer-AssistedTranslation Using Speech Recognition.
IEEETransactions on Audio, Speech, and LanguageProcessing, 14(3): 941-951.103
