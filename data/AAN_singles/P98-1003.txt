Towards a single proposal in spelling correctionEneko Agirre, Koldo Gojenola, Kepa SarasolaDept.
of Computer Languages and SystemsUniversity of the Basque Country, 649 P. K.,E-20080 Donostia, Basque Countryeneko@si.ehu.esAbstractThe study presented here relies on theintegrated use of different kinds ofknowledge in order to improve first-guessaccuracy in non-word context-sensitivecorrection for general unrestricted texts.
Stateof the art spelling correction systems, e.g.ispell, apart from detecting spelling errors,also assist the user by offering a set ofcandidate corrections that are close to themisspelled word.
Based on the correctionproposals of ispell, we built several guessers,which were combined in different ways.Firstly, we evaluated all possibilities andselected the best ones in a corpus withartificially generated typing errors.
Secondly,the best combinations were tested on textswith genuine spelling errors.
The results forthe latter suggest that we can expectautomatic non-word correction for all theerrors in a free running text with 80%precision and a single proposal 98% of thetimes (1.02 proposals on average).IntroductionThe problem of devising algorithms andtechniques for automatically correcting words intext remains a research challenge.
Existingspelling correction techniques are limited in theirscope and accuracy.
Apart from detectingspelling errors, many programs assist users byoffering a set of candidate corrections that areclose to the misspelled word.
This is true for mostcommercial word-processors a  well as the Unix-based spelling-corrector ispelP (1993).
Theseprograms tolerate lower first guess accuracy byreturning multiple guesses, allowing the user tomake the final choice of the intended word.
Ini lspell was used for the spell-checking and correctioncandidate generation.
Its assets include broad-coverageand excellent reliability.Atro VoutilainenDepartment ofGeneral LinguisticsUniversity of Helsinki, P.O.
Box 4FIN-00014 Helsinki, Finlandavoutila@ling.helsinki.ficontrast, some applications will require fullyautomatic correction for general-purpose t xts(Kukich 1992).It is clear that context-sensitive spelling correctionoffers better results than isolated-word errorcorrection.
The underlying task is to determine therelative degree of well formedness amongalternative sentences (Mays et al 1991).
Thequestion is what kind of knowledge (lexical,syntactic, semantic .
.
.
.  )
should be represented,utilised and combined to aid in this determination.This study relies on the integrated use of threekinds of knowledge (syntagmatic, paradigmaticand statistical) in order to improve first guessaccuracy in non-word context-sensitive correctionfor general unrestricted texts.
Our techniques wereapplied to the corrections posed by ispell.Constraint Grammar (Karlsson et al 1995) waschosen to represent syntagmatic knowledge.
Itsuse as a part of speech tagger for English has beenhighly successful.
Conceptual Density (Agirre andRigau 1996) is the paradigmatic omponentchosen to discriminate semantically amongpotential noun corrections.
This techniquemeasures "affinity distance" between ouns usingWordnet (Miller 1990).
Finally, general anddocument word-occurrence frequency-ratescomplete the set of knowledge sources combined.We knowingly did not use any model of commonmisspellings, the main reason being that we didnot want to use knowledge about he error source.This work focuses on language models, not errormodels (typing errors, common misspellings, OCRmistakes, peech recognition mistakes, etc.
).The system was evaluated against two sets oftexts: artificially generated errors from the Browncorpus (Francis and Kucera 1967) and genuinespelling errors from the Bank of EnglishLThe remainder of this paper is organised as2 http://titania.cobuild.collins.co.uk/boe_info.html22follows.
Firstly, we present he techniques thatwill be evaluated and the way to combine them.Section 2 describes the experiments and showsthe results, which are evaluated in section 3.Section 4 compares other relevant work incontext sensitive correction.1 The basic techniques1.1 Constraint Grammar (CG)Constraint Grammar was designed with the aimof being a language-independent and robust toolto disambiguate and analyse unrestricted texts.CG grammar statements are close to real textsentences and directly address parsing problemssuch as ambiguity.
Its application to English(ENGCG 3) resulted a very successful part ofspeech tagger for English.
CG works on a textwhere all possible morphological interpretationshave been assigned to each word-form by theENGTWOL morphological nalyser (Voutilainenand Heikkil~i 1995).
The role of CG is to apply aset of linguistic constraints that discard as manyalternatives as possible, leaving at the end almostfully disambiguated sentences, with onemorphological or syntactic interpretation for eachword-form.
The fact that CG tries to leave aunique interpretation for each word-form makesthe formalism adequate to achieve our objective.Application of Constraint GrammarThe text data was input to the morphologicalanalyser.
For each unrecognised word, ispell wasapplied, placing the morphological analyses ofthe correction proposals as alternativeinterpretations of the erroneous word (seeexample 1).
EngCG-2 morphologicaldisambiguation was applied to the resulting texts,ruling out the correction proposals with anincompatible POS (cf.
example 2).
We must notethat the broad coverage lexicons of ispell andENGTWOL are independent.
This caused thecorrespondence between unknown words andispell's proposals not to be one to one with thoseof the EngCG-2 morphological analyser,especially in compound words.
Such problemswere solved considering that a word was correctif it was covered by any of the lexicons.1.2 Conceptual  Density (CD)3 A recent version of ENGCG, known as EngCG-2,can be tested at http://www.conexor.fi/analysers.htmlThe discrimination of the correct category isunable to distinguish among readings belonging tothe same category, so we also applied a word-sense disambiguator based on Wordnet, that hadalready been tried for nouns on free-running text.In our case it would choose the correction proposalsemantically closer to the surrounding context.
Ithas to be noticed that Conceptual Density can onlybe applied when all the proposals are categorisedas nouns, due to the structure of Wordnet.<our>"our" PRON PL ...<bos> ; INCORRECT OR SPELLING ERROR"boss" N S"boys" N P"bop" V S"Bose" <Proper>Example 1.
Proposals and morphological analysisfor the misspelling bos<our>"our" PRON PL ...<bos> ; INCORRECT OR SPELLING ERROR"boss" N S"boys" N P ,,t.t.nj~,__,, ~i"Bose" <Proper><are> ...Example 2.
CG leaves only nominal proposals1.3 Frequency statistics (DF & BF)Frequency data was calculated as word-formfrequencies obtained from the document where theerror was obtained (Document frequency, DF) orfrom the rest of the documents in the whole BrownCorpus (Brown frequency, BF).
The experimentsproved that word-forms were better suited for thetask, compared to frequencies on lemmas.1.4 Other  interest ing heuristics (HI,  H2)We eliminated proposals beginning with anuppercase character when the erroneous word didnot begin with uppercase and there werealternative proposals beginning with lowercase.
Inexample 1, the fourth reading for the misspelling"bos" was eliminated, as "Bose" would be at anediting distance of two from the misspelling(heuristic HI).
This heuristic proved very reliable,and it was used in all experiments.
After obtainingthe first results, we also noticed that words withless than 4 characters like "si", "teh", ...(misspellings for "is" and "the") produced toomany proposals, difficult to disambiguate.
As theywere one of the main error sources for our method,we also evaluated the results excluding them23(heuristic H2).1.5 Combination of the basic techniquesusing votesWe considered all the possible combinationsamong the different techniques, e.g.
CG+BF,BF+DF, and CG+DF.
The weight of the vote canbe varied for each technique, e.g.
CG could havea weight of 2 and BF a weight of 1 (we willrepresent his combination as CG2+BF1).
Thiswould mean that the BF candidate(s) will only bechosen if CG does not select another option or ifCG selects more than one proposal.
Severalcombinations of weights were tried.
This simplemethod to combine the techniques can beimproved using optimization algorithms tochoose the best weights among fractional values.Nevertheless, we did some trials weighting eachtechnique with its expected precision, and noimprovement was observed.
As the bestcombination of techniques and weights for agiven set of texts can vary, we separated the errorcorpora in two, trying all the possibilities on thefirst half, and testing the best ones on the secondhalf (c.f.
section 2.1).2 The experimentsBased on each kind of knowledge, we builtsimple guessers and combined them in differentways.
In the first phase, we evaluated all thepossibilities and selected the best ones on part ofthe corpus with artificially generated errors.Finally, the best combinations were tested againstthe texts with genuine spelling errors.2.1 The er ror  corporaWe chose two different corpora for theexperiment.
The first one was obtained bysystematically generating misspellings from asample of the Brown Corpus, and the second onewas a raw text with genuine errors.
While thefirst one was ideal for experimenting, allowingfor automatic verification, the second one offereda realistic setting.
As we said before, we aretesting language models, so that both kinds ofdata are appropriate.
The corpora with artificialerrors, artificial corpora for short, have thefollowing features: a sample was extracted fromSemCor (a subset of the Brown Corpus) selecting150 paragraphs at random.
This yielded a seedcorpus of 505 sentences and 12659 tokens.
Tosimulate spelling errors, a program namedantispell, which applies Damerau's rules atrandom, was run, giving an average of onespelling error for each 20 words (non-words wereleft untouched).
Antispell was run 8 times on theseed corpus, creating 8 different corpora with thesame text but different errors.
Nothing was done toprevent wo errors in the same sentence, and someparagraphs did not have any error.The corpus of genuine spelling errors, which wealso call the "real" corpus for short, was magazinetext from the Bank of English Corpus, whichprobably was not previously spell-checked (itcontained many misspellings), so it was a goodsource of errors.
Added to the difficulty ofobtaining texts with real misspellings, there is theproblem of marking the text and selecting thecorrect proposal for automatic evaluation.As mentioned above, the artificial-error corporawere divided in two subsets.
The first one wasused for training purposes 4.
Both the second halfand the "real" texts were used for testing.2.2 Data for each corporaThe two corpora were passed trough ispell, and foreach unknown word, all its correction proposalswere inserted.
Table 1 shows how, if themisspellings are generated at random, 23.5% ofthem are real words, and fall out of the scope ofthis work.
Although we did not make a similarcounting in the real texts, we observed that asimilar percentage can be expected.words~rrorsaon real-word errorsispell proposals~vords with multiple proposalsLong word errors (H2)proposals for long words (H2)long word errors (H2) withmultiple proposalsl~'half 2 ~ half"real"47584 47584397321772 18111354 1403 3657242 8083 1257810 852 15~968 98C 332245 2313 80~430 425 124Table 1.
Number of errors and proposalsFor the texts with genuine errors, the method usedin the selection of the misspellings was thefollowing: after applying ispell, no correction wasfound for 150 words (mainly proper nouns andforeign words), and there were about 300 which4 In fact, there is no training in the statistical sense.
Itjust involves choosing the best alternatives for voting.5 As we focused on non-word words, there is not acount of real-word errors.24Basic techniquesrandom baselinerandom+H2CGCG+H2BFBF+H2DFDF+H2CDCombinationsCG 1 +DF2CGI+DF2+H2CGI+DFI+BF1100.00 54.36 1.0071.49 71.59 1.0099.85 86.91 2.3371.42 95.86 1.7096.23 86.57 1.0068.69 92.15 1.0090.55 89.97 1.0262.92 96.13 1.016.06 79.27 1.01CGI+DFI+BFI+H2CGI+DFI+BFI+CD 1CGI+DFI+BFI+CDI+H2Table 2.
Results for several combinations (I"99.93 90.39 1.1771.49 96.38 1.1299.93 89.14 1.0371.49 94.73 1.0399.93 89.14 1.0271.49 94.63 1.02half)Basic techniquesrandom baselinerandom+H2CGCG+H2BFBF+H2DFDF+H2CDCombinationsCGI+DF2CGI+DF2+H2CGI+DFI+BF1CGI+DFI+BFI+H2CGI+DFI+BFI+CD1CGI+DFI+BFI+CD+H2100.00 23.70 1.0052.70 36.05 1.0099.75 78.09 3.2352.57 90.68 2.5893.70 76.94 1.0048.04 81.38 1.0084.20 81.96 1.0338.48 89.49 1.038.27 75.28 1.0199.88 83.93 1.2852.70 91.86 1.4399.88 81.83 1.0452.70 88.14 1.0699.88 81.83 1.0452.70 87.91 1.05multiple Table 3.
Results on errors withproposals (1" half)were formed by joining two consecutive words orby special affixation rules (ispell recognised themcorrectly).
This left 369 erroneous word-forms.After examining them we found that the correctword-form was among ispell's proposals, withvery few exceptions.
Regarding the selectionamong the different alternatives for an erroneousword-form, we can see that around half of themhas a single proposal.
This gives a measure of thework to be done.
For example, in the real errorcorpora, there were 158 word-forms with 1046different proposals.
This means an average of6.62 proposals per word.
If words of length lessBasic techniauesrandom baseline 100.00 53.67 1.00random+H2 69.85 71.53 1.00DF 90.31 89.50 !.02DF+H2 61.51 95.60 1.01CombinationsCGI+DF2 99.64 90.06 1.19CGI+DF2+H2 69.85 95.71 1.22CGI+DFI+BF1 99.64 87.77 1.03CGI+DFI+BFI+H2 69.85 93.16 1.03CGI+DFI+BFI+CD1 99.64 87.91 1.03CGI+DFI+BFI+CD+H2 69.85 93.27 1.02Table 4.
Validation of the best combinations(2 *J half)l B+~$ie teehniouesrandom baseline 100.00 23.71 1.00random+H2 50.12 34.35 1.00DF 84.04 81.42 1.03DF+H2 36.32 87.66 1.04CombinationsCGI+DF2 99.41 83.59 1.31CGI+DF2+H2 50.12 90.12 1.50CGI+DFI+BF1 99.41 79.81 1.05CGI+DFI+BFI+H2 50.12 84.24 1.06CGI+DFI+BFI+CD1 99.41 80.05 1.05CGI+DFI+BFI+CDI+H2 50.12 84.47 1.06Table 5.
Results on errors with multipleproposals (2 "d half)than 4 are not taken into account, there are 807proposals, that is, 4.84 alternatives per word.2.3 ResultsWe mainly considered three measures:?
coverage: the number of errors for which thetechnique yields an answer.?
precision: the number of errors with thecorrect proposal among the selected ones?
remaining proposals: the average number ofselected proposals.2.3.1 Search for the best combinationsTable 2 shows the results on the training corpora.We omit many combinations that we tried, for thesake of brevity.
As a baseline, we show the resultswhen the selection is done at random.
HeuristicH1 is applied in all the cases, while tests areperformed with and without heuristic H2.
If wefocus on the errors for which ispell generates more25Cover.
% I Prec.
% \[#prop.Basic techniquesrandom baseline 100.00 69.9275.4784.1590.30random+H2 89.70CG 99.19CG+H2 89.43DF 70.19 93.05DF+H2 61.52 97.80BF 98.37 80.99BF+H2 88.08 85.54CombinationsCG 1 +DF2 100.00CG 1 +DF2+H2 89.70CGI+DFI+BF1 100.00CGI+DFI+BFI+H2 89.701.001.001.611.571.021.001.001.00Table 6.
Best combinationsCover.
% I Prec.
%Basic techniquesrandom baseline 100.00random+H2 76.54CG 98.
I0CG+H2 75.93DF 30.38DF+H2 12.35BF 96.20BF+H2 72.8487.26 1.4290.94 1.4380.76 1.0284.89 1.02"real" corpus)\[ #prop29.75 1.0034.52 1.0062.58 2.4573.98 2.5262.50 1.1375.00 1.0554.61 1.0060.17 1.00CombinationsCG 1 +DF2 100.00 70.25 1.99CGI+DF2+H2 76.24 75.81 2.15CGI+DFI+BF1 100.00 55.06 1.04CGI+DFI+BFI+H2 76.54 59.68 1.05Table 7.
Results on errors with multipleproposals ("real" corpus)than one correction proposal (cf.
table 3), we geta better estimate of the contribution of eachguesser.
There were 8.26 proposals per word inthe general case, and 3.96 when H2 is applied.The results for all the techniques are well abovethe random baseline.
The single best techniquesare DF and CG.
CG shows good results onprecision, but fails to choose a single proposal.H2 raises the precision of all techniques at thecost of losing coverage.
CD is the weakest of alltechniques, and we did not test it with the othercorpora.
Regarding the combinations,CGI+DF2+H2 gets the best precision overall, butit only gets 52% coverage, with 1.43 remainingproposals.
Nearly 100% coverage is attained bythe H2 combinations, with highest precision forCGI+DF2 (83% precision, 1.28 proposals).2.3.2 Validation of the best combinationsIn the second phase, we evaluated the bestcombinations on another corpus with artificialerrors.
Tables 4 and 5 show the results, whichagree with those obtained in 2.3.1.
They showslightly lower percentages but always in parallel.2.3.3 Corpus of genuine rrorsAs a final step we evaluated the best combinationson the corpus with genuine typing errors.
Table 6shows the overall results obtained, and table 7 theresults for errors with multiple proposals.
For thelatter there were 6.62 proposals per word in thegeneral case (2 less than in the artificial corpus),and 4.84 when heuristic H2 is applied (one morethat in the artificial corpus).
These tables arefurther commented in the following section.3 Evaluation of resultsThis section reviews the results obtained.
Theresults for the "real" corpus are evaluated first, andthe comparison with the other corpora comes later.Concerning the application of each of the simpletechniques separately6:?
Any of the guessers performs much better thanrandom.?
DF has a high precision (75%) at the cost of alow coverage (12%).
The difference incoverage compared to the artificial errorcorpora (84%) is mainly due to the smaller sizeof the documents in the real error corpus(around 50 words per document).
For medium-sized documents we expect a coverage similarto that of the artificial error corpora.?
BF offers lower precision (54%) with the gainsof a broad coverage (96%).?
CG presents 62% precision with nearly 100%coverage, but at the cost of leaving manyproposals (2.45)?
The use of CD works only with a small fractionof the errors giving modest results.
The factthat it was only applied a few times prevents usfrom making further conclusions.Combining the techniques, the results improve:?
The CGI+DF2 combination offers the bestresults in coverage (100%) and precision (70%)for all tests.
As can be seen, CG raises the6 If not explicitly noted, the figures and comments referto the "real" corpus, table 7.26coverage of the DF method, at the cost of alsoincreasing the number of proposals (1.9) pererroneous word.
Had the coverage of DFincreased, so would also the number ofproposals decrease for this combination, forinstance, close to that of the artificial errorcorpora (1.28).?
The CGI+DFI+BF1 combination provides thesame coverage with nearly one interpretationper word, but decreasing precision to a 55%.?
If full coverage is not necessary, the use of theH2 heuristic raises the precision at least 4%for all combinations.When comparing these results with those of theartificial errors, the precisions in tables 2, 4 and 6can be misleading.
The reason is that thecoverage of some techniques varies and theprecision varies accordingly.
For instance,coverage of DF is around 70% for real errors and90% for artificial errors, while precisions are93% and 89% respectively (cf.
tables 6 and 2).This increase in precision is not due to the betterperformance of DF 7, but can be explainedbecause the lower the coverage, the higher theproportion of errors with a single proposal, andtherefore the higher the precision.The comparison between tables 3 and 7 is moreclarifying.
The performance of all techniquesdrops in table 7.
Precision of CG and BF drops 15and 20 points.
DF goes down 20 points inprecision and 50 points in coverage.
This latterdegradation is not surprising, as the length of thedocuments in this corpus is only of 50 words onaverage.
Had we had access to medium sizeddocuments, we would expect a coverage similarto that of the artificial error corpora.The best combinations hold for the "real" texts, asbefore.
The highest precision is for CGI+DF2(with and without H2).
The number of proposalsleft is higher in the "real" texts than in theartificial ones (1.99 to 1.28).
It can be explainedbecause DF does not manage to cover all errors,and that leaves many CG proposals untouched.We think that the drop in performance for the"real" texts was caused by different factors.
Firstof all, we already mentioned that the size of thedocuments trongly affected DF.
Secondly, thenature of the errors changes: the algorithm to7 In fact the contrary is deduced from tables 3 and 7.produce spelling errors was biased in favour offrequent words, mostly short ones.
We will have toanalyse this question further, specially regardingthe origin of the natural errors.
Lastly, BF wastrained on the Brown corpus on American English,while the "real" texts come from the Bank ofEnglish.
Presumably, this could have also affectednegatively the performance of these algorithms.Back to table 6, the figures reveal which would bethe output of the correction system.
Either we get asingle proposal 98% of the times (1.02 proposalsleft on average) with 80% precision for all non-word errors in the text (CGI+DFI+BF1) or wecan get a higher precision of 90% with 89%coverage and an average of 1.43 proposals(CGI+DF2+H2).4 Compar ison  with other  context-sensitive correct ion systemsThere is not much literature about automaticspelling correction with a single proposal.
Menezoet al (1996) present a spelling/grammar checkerthat adjusts its strategy dynamically taking intoaccount different lexical agents (dictionaries .. .
.
),the user and the kind of text.
Although noquantitative r sults are given, this is in accord withusing document and general frequencies.Mays et al (1991) present he initial success ofapplying word trigram conditional probabilities tothe problem of context based detection andcorrection of real-word errors.Yarowsky (1994) experiments with the use ofdecision lists for lexical ambiguity resolution,using context features like local syntactic patternsand collocational information, so that multipletypes of evidence are considered in the context ofan ambiguous word.
In addition to word-forms,the patterns involve POS tags and lemmas.
Thealgorithm is evaluated in missing accentrestoration task for Spanish and French text,against a predefined set of a few words giving anaccuracy over 99%.Golding and Schabes (1996) propose a hybridmethod that combines part-of-speech trigrams andcontext features in order to detect and correct real-word errors.
They present an experiment wheretheir system has substantially higher performancethan the grammar checker in MS Word, but itscoverage is limited to eighteen particularconfusion sets composed by two or three similarwords (e.g.
: weather, whether).27The last three systems rely on a previouslycollected set of confusion sets (sets of similarwords or accentuation ambiguities).
On thecontrary, our system has to choose a singleproposal for any possible spelling error, and it istherefore impossible to collect the confusion sets(i.e.
sets of proposals for each spelling error)beforehand.
We also need to correct as manyerrors as possible, even if the amount of data for aparticular case is scarce.ConclusionThis work presents a study of different methodsthat build on the correction proposals of ispell,aiming at giving a single correction proposal formisspellings.
One of the difficult aspects of theproblem is that of testing the results.
For thatreason, we used both a corpus with artificiallygenerated errors for training and testing, and acorpus with genuine rrors for testing.Examining the results, we observe that the resultsimprove as more context is taken into account.The word-form frequencies serve as a crude buthelpful criterion for choosing the correctproposal.
The precision increases as closercontexts, like document frequencies andConstraint Grammar are incorporated.
From theresults on the corpus of genuine errors we canconclude the following.
Firstly, the correct wordis among ispell's proposals 100% of the times,which means that all errors can be recovered.Secondly, the expected output from our presentsystem is that it will correct automatically thespelling errors with either 80% precision with fullcoverage or 90% precision with 89% coverageand leaving an average of 1.43 proposals.Two of the techniques proposed, BrownFrequencies and Conceptual Density, did notyield useful results.
CD only works for a verysmall fraction of the errors, which prevents usfrom making further conclusions.There are reasons to expect better results in thefuture.
First of all, the corpus with genuine rrorscontained very short documents, which causedthe performance of DF to degrade substantially.Further tests with longer documents should yieldbetter esults.
Secondly, we collected frequenciesfrom an American English corpus to correctBritish English texts.
Once this languagemismatch is solved, better performance should beobtained.
Lastly, there is room for improvementin the techniques themselves.
We knowingly didnot use any model of common misspellings.Although we expect limited improvement,stronger methods to combine the techniques canalso be tried.Continuing with our goal of attaining a singleproposal as reliably as possible, we will focus onshort words and we plan to also include moresyntactic and semantic ontext in the process bymeans of collocational information.
This stepopens different questions about the size of thecorpora needed for accessing the data and thespace needed to store the information.AcknowledgementsThis research was supported by the BasqueGovernment, the University of the BasqueCountry and the CICYT (ComisirnInterministerial de Ciencia y Tecnologfa).ReferencesAgirre E. and Rigau G. (1996) Word sensedisambiguation using conceptual density.
In Proc.
ofCOLING-96, Copenhagen, Denmark.Golding A. and Schabes.
Y.
(1996) Combining trigram-based and feature-based methods for context-sensitivespelling correction.
In Proc.
of the 34th ACLMeeting, Santa Cruz, CA.Ispell (1993) International Ispell Version 3.1.00,10/08/93.Francis S.and Kucera H. (1967) Computing Analysis ofPresent-Day American English.
Brown Univ.
Press.Karlsson F., Voutilainen A., Heikkil~i J. and Anttila A.
(1995) Constraint Grammar: a LanguageIndependent System for Parsing Unrestricted Text.Ed.Mouton de Gruyter.Koskenniemi K. (1983) Two-level Morphology: Ageneral Computational Model for Word-FormRecognition and Production.
University of Helsinki.Kukich K. (1992) Techniques for automaticallycorrecting words in text.
In ACM ComputingSurveys, Vol.
24, N. 4, December, pp.
377-439.Mays E., Damerau F. and Mercer.
R. (1991) Contextbased spelling correction.
Information Processing &Management, Vol.
27, N. 5, pp.
517-522.Miller G. (1990) Five papers on WordNet.
Special Issueof the Int.
Journal of Lexicography, Vol.
3, N. 4.Menezo J., Genthial D. and Courtin J.
(1996)Reconnaisances pluri-lexicales dans CELINE, unsyst~me multi-agents de drtection et correction deserreurs.
NLP + IA 96, Moncton, N. B., Canada.Yarowsky D. (1994) Decision lists for lexical ambiguityresolution.
In Proceedings of the 32nd ACL Meeting,Las Cruces, NM, pp.88-95.28
