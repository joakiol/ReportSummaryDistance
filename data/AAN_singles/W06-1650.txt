Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 423?430,Sydney, July 2006. c?2006 Association for Computational LinguisticsAutomatically Assessing Review HelpfulnessSoo-Min Kim?, Patrick Pantel?, Tim Chklovski?, Marco Pennacchiotti?
?Information Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA  90292{skim,pantel,timc}@isi.edu?ART Group - DISPUniversity of Rome ?Tor Vergata?Viale del Politecnico 1Rome, Italypennacchiotti@info.uniroma2.itAbstractUser-supplied reviews are widely andincreasingly used to enhance e-commerce and other websites.
Becausereviews can be numerous and varying inquality, it is important to assess howhelpful each review is.
While reviewhelpfulness is currently assessed manu-ally, in this paper we consider the taskof automatically assessing it.
Experi-ments using SVM regression on a vari-ety of features over Amazon.comproduct reviews show promising results,with rank correlations of up to 0.66.
Wefound that the most useful features in-clude the length of the review, its uni-grams, and its product rating.1 IntroductionUnbiased user-supplied reviews are solicitedubiquitously by online retailers like Ama-zon.com, Overstock.com, Apple.com and Epin-ions.com, movie sites like imdb.com, travelingsites like citysearch.com, open source softwaredistributors like cpanratings.perl.org, and count-less others.
Because reviews can be numerousand varying in quality, it is important to rankthem to enhance customer experience.In contrast with ranking search results, assess-ing relevance when ranking reviews is of littleimportance because reviews are directly associ-ated with the relevant product or service.
Instead,a key challenge when ranking reviews is to de-termine which reviews the customers will findhelpful.Most websites currently rank reviews by theirrecency or product rating (e.g., number of starsin Amazon.com reviews).
Recently, more sophis-ticated ranking schemes measure reviews by theirhelpfulness, which is typically estimated by hav-ing users manually assess it.
For example, onAmazon.com, an interface allows customers tovote whether a particular review is helpful or not.Unfortunately, newly written reviews and re-views with few votes cannot be ranked as severalassessments are required in order to properly es-timate helpfulness.
For example, for all MP3player products on Amazon.com, 38% of the20,919 reviews received three or fewer helpful-ness votes.
Another problem is that low-trafficitems may never gather enough votes.
Among theMP3 player reviews that were authored at leastthree months ago on Amazon.com, still only 31%had three or fewer helpfulness votes.It would be useful to assess review helpfulnessautomatically, as soon as the review is written.This would accelerate determining a review?sranking and allow a website to provide rapidfeedback to review authors.In this paper, we investigate the task of auto-matically predicting review helpfulness using amachine learning approach.
Our main contribu-tions are:?
A system for automatically ranking reviewsaccording to helpfulness; using state of the artSVM regression, we empirically evaluate oursystem on a real world dataset collected fromAmazon.com on the task of reconstructing thehelpfulness ranking; and?
An analysis of different classes of featuresmost important to capture review helpful-ness; including structural (e.g., html tags,punctuation, review length), lexical (e.g., n-grams), syntactic (e.g., percentage of verbs andnouns), semantic (e.g., product feature men-tions), and meta-data (e.g., star rating).2 Relevant WorkThe task of automatically assessing product re-view helpfulness is related to these broader areas423of research: automatic analysis of product re-views, opinion and sentiment analysis, and textclassification.In the thriving area of research on automaticanalysis and processing of product reviews (Huand Liu 2004; Turney 2002; Pang and Lee 2005),little attention has been paid to the important taskstudied here ?
assessing review helpfulness.
Pangand Lee (2005) have studied prediction of prod-uct ratings, which may be particularly relevantdue to the correlation we find between productrating and the helpfulness of the review (dis-cussed in Section 5).
However, a user?s overallrating for the product is often already available.Helpfulness, on the other hand, is valuable toassess because it is not explicitly known in cur-rent approaches until many users vote on thehelpfulness of a review.In opinion and sentiment analysis, the focus ison distinguishing between statements of fact vs.opinion, and on detecting the polarity of senti-ments being expressed.
Many researchers haveworked in various facets of opinion analysis.Pang et al (2002) and Turney (2002) classifiedsentiment polarity of reviews at the documentlevel.
Wiebe et al (1999) classified sentencelevel subjectivity using syntactic classes such asadjectives, pronouns and modal verbs as features.Riloff and Wiebe (2003) extracted subjectiveexpressions from sentences using a bootstrappingpattern learning process.
Yu and Hatzivassi-loglou (2003) identified the polarity of opinionsentences using semantically oriented words.These techniques were applied and examined indifferent domains, such as customer reviews (Huand Liu 2004) and news articles (TREC noveltytrack 2003 and 2004).In text classification, systems typically usebag-of-words models, although there is someevidence of benefits when introducing relevantsemantic knowledge (Gabrilovich and Mark-ovitch, 2005).
In this paper, we explore the use ofsome semantic features for review helpfulnessranking.
Another potential relevant classificationtask is academic and commercial efforts on de-tecting email spam messages1, which aim to cap-ture a much broader notion of helpfulness.
For anSVM-based approach, see (Drucker et al 1999).Finally, a related area is work on automatic es-say scoring, which seeks to rate the quality of anessay (Attali and Burstein 2006; Burstein et al2004).
The task is important for reducing thehuman effort required in scoring large numbers1 See http://www.ceas.cc/, http://spamconference.org/of student essays regularly written for standardtests such as the GRE.
The exact scoring ap-proaches developed in commercial systems areoften not disclosed.
However, more recent workon one of the major systems, e-rater 2.0, has fo-cused on systematizing and simplifying the set offeatures used (Attali and Burstein 2006).
Ourchoice of features to test was partially influencedby the features discussed by Attali and Burstein.At the same time, due to differences in the tasks,we did not use features aimed at assessing essaystructure such as discourse structure analysis fea-tures.
Our observations suggest that even helpfulreviews vary widely in their discourse structure.We present the features which we have used be-low, in Section 3.2.3 Modeling Review HelpfulnessIn this section, we formally define the learningtask and we investigate several features for as-sessing review helpfulness.3.1 Task DefinitionFormally, given a set of reviews R for a particu-lar product, our task is to rank the reviews ac-cording to their helpfulness.
We define a reviewhelpfulness function, h, as:( ) ( )( ) ( )rratingrratingrratingRrh?+++=?
(1)where rating+(r) is the number of people that willfind a review helpful and rating-(r) is the numberof people that will find the review unhelpful.
Forevaluation, we resort to estimates of h from man-ual review assessments on websites like Ama-zon.com, as described in Section 4.3.2 FeaturesOne aim of this paper is to investigate how welldifferent classes of features capture the helpful-ness of a review.
We experimented with variousfeatures organized in five classes: Structural,Lexical, Syntactic, Semantic, and Meta-data.
Be-low we describe each feature class in turn.Structural FeaturesStructural features are observations of the docu-ment structure and formatting.
Properties such asreview length and average sentence length arehypothesized to relate structural complexity tohelpfulness.
Also, HTML formatting tags couldhelp in making a review more readable, and con-sequently more helpful.
We experimented withthe following features:424?
Length (LEN): The total number of tokens in asyntactic analysis2 of the review.?
Sentential (SEN): Observations of the sen-tences, including the number of sentences, theaverage sentence length, the percentage ofquestion sentences, and the number of excla-mation marks.?
HTML (HTM): Two features for the number ofbold tags <b> and line breaks <br>.Lexical FeaturesLexical features capture the words observed inthe reviews.
We experimented with two sets offeatures:?
Unigram (UGR): The tf-idf statistic of eachword occurring in a review.?
Bigram (BGR): The tf-idf statistic of each bi-gram occurring in a review.For both unigrams and bigrams, we used lemma-tized words from a syntactic analysis of the re-views and computed the tf-idf statistic (Saltonand McGill 1983) using the following formula:( )Nidftfidftflog?=where N is the number of tokens in the review.Syntactic FeaturesSyntactic features aim to capture the linguisticproperties of the review.
We grouped them intothe following feature set:?
Syntax (SYN): Includes the percentage ofparsed tokens that are open-class (i.e., nouns,verbs, adjectives and adverbs), the percentageof tokens that are nouns, the percentage of to-kens that are verbs, the percentage of tokensthat are verbs conjugated in the first person,and the percentage of tokens that are adjectivesor adverbs.Semantic FeaturesMost online reviews are fairly short; their spar-sity suggests that bigram features will not per-form well (which is supported by ourexperiments described in Section 5.3).
Althoughsemantic features have rarely been effective inmany text classification problems (Moschitti andBasili 2004), there is reason here to hypothesizethat a specialized vocabulary of important wordsmight help with the sparsity.
We hypothesized2  Reviews are analyzed using the Minipar dependencyparser (Lin 1994).that good reviews will often contain: i) refer-ences to the features of a product (e.g., the LCDand resolution of a digital camera), and ii) men-tions of sentiment words (i.e., words that expressan opinion such as ?great screen?).
Below wedescribe two families of features that capturethese semantic observations within the reviews:?
Product-Feature (PRF): The features of prod-ucts that occur in the review, e.g., capacity ofMP3 players and zoom of a digital camera.This feature counts the number of lexicalmatches that occur in the review for each prod-uct feature.
There is no trivial way of obtaininga list of all the features of a product.
In Section5.1 we describe a method for automatically ex-tracting product features from Pro/Con listingsfrom Epinions.com.
Our assumption is thatpro/cons are the features that are important forcustomers (and hence should be part of a help-ful review).?
General-Inquirer (GIW): Positive and negativesentiment words describing products or prod-uct features (e.g., ?amazing sound quality?
and?weak zoom?).
The intuition is that reviewsthat analyze product features are more helpfulthan those that do not.
We try to capture thisanalysis by extracting sentiment words usingthe publicly available list of positive and nega-tive sentiment words from the General InquirerDictionaries3.Meta-Data FeaturesUnlike the previous four feature classes, meta-data features capture observations which are in-dependent of the text (i.e., unrelated with linguis-tic features).
We consider the following feature:?
Stars (STR): Most websites require reviewersto include an overall rating for the productsthat they review (e.g., star ratings in Ama-zon.com).
This feature set includes the ratingscore (STR1) as well as the absolute value ofthe difference between the rating score and theaverage rating score given by all reviewers(STR2).We differentiate meta-data features from seman-tic features since they require external knowl-edge that may not be available from certainreview sites.
Nowadays, however, most sites thatcollect user reviews also collect some form ofproduct rating (e.g., Amazon.com, Over-stock.com, and Apple.com).3 http://www.wjh.harvard.edu/~inquirer/homecat.htm4254 Ranking SystemIn this paper, we estimate the helpfulness func-tion in Equation 1 using user ratings extractedfrom Amazon.com, where rating+(r) is the num-ber of unique users that rated the review r ashelpful and rating-(r) is the number of uniqueusers that rated r as unhelpful.Reviews from Amazon.com form a gold stan-dard labeled dataset of {review, h(review)} pairsthat can be used to train a supervised machinelearning algorithm.
In this paper, we applied anSVM (Vapnik 1995) package on the features ex-tracted from reviews to learn the function h.Two natural options for learning helpfulnessaccording to Equation 1 are SVM Regression andSVM Ranking (Joachims 2002).
Though learningto rank according to helpfulness requires onlySVM Ranking, the helpfulness function providesnon-uniform differences between ranks in thetraining set.
Also, in practice, many productshave only one review, which can serve as train-ing data for SVM Regression but not SVM Rank-ing.
Furthermore, in large sites such asAmazon.com, when new reviews are written it isinefficient to re-rank all previously ranked re-views.
We therefore choose SVM Regression inthis paper.
We describe the exact implementationin Section 5.1.After the SVM is trained, for a given productand its set of reviews R, we rank the reviews of Rin decreasing order of h(r), r ?
R.Table 1 shows four sample reviews for theiPod Photo 20GB product from Amazon.com,their total number of helpful and unhelpful votes,as well as their rank according to the helpfulnessscore h from both the gold standard from Ama-zon.com and using the SVM prediction of ourbest performing system described in Section 5.2.5 Experimental ResultsWe empirically evaluate our review model andranking system, described in Section 3 and Sec-tion 4, by comparing the performance of variousfeature combinations on products mined fromAmazon.com.
Below, we describe our experi-mental setup, present our results, and analyzesystem performance.5.1 Experimental SetupWe describe below the datasets that we extractedfrom Amazon.com, the implementation of ourSVM system, and the method we used for ex-tracting features of reviews.Extraction and Preprocessing of DatasetsWe focused our experiments on two productsfrom Amazon.com: MP3 Players and DigitalCameras.Using Amazon Web Services API, we col-lected reviews associated with all products in theMP3 Players and Digital Cameras categories.For MP3 Players, we collected 821 products and33,016 reviews; for Digital Cameras, we col-lected 1,104 products and 26,189 reviews.In most retailer websites like Amazon.com,duplicate reviews, which are quite frequent, skewstatistics and can greatly affect a learning algo-rithm.
Looking for exact string matches betweenreviews is not a sufficient filter since authors ofduplicated reviews often make small changes tothe reviews to avoid detection.
We built a simplefilter that compares the distribution of word bi-grams across each pair of reviews.
A pair isdeemed a duplicate if more than 80% of theirbigrams match.Also, whole products can be duplicated.
Fordifferent product versions, such as iPods that cancome in black or white models, reviews on Ama-zon.com are duplicated between them.
We filterTable 1.
Sample of 4 out of 43 reviews for the iPod Photo 20GB product from Ama-zon.com along with their ratings as well as their helpfulness ranks (from both the goldstandard from Amazon.com and the SVM prediction of our best performing system de-scribed in Section 5.2).RANK(h)REVIEW TITLEHELPFULVOTESUNHELPFULVOTES GOLDSTANDARDSVMPREDICTION?iPod Moves to All-color Line-up?
215 11 7 1?iPod: It's NOT Music to My Ears?
11 13 25 30?The best thing I ever bought?
22 32 26 27?VERY disappointing?
1 18 40 40426out complete products where each of its reviewsis detected as a duplicate of another product (i.e.,only one iPod version is retained).The filtering of duplicate products and dupli-cate reviews discarded 85 products and 12,097reviews for MP3 Players and 38 products and3,692 reviews for Digital Cameras.In order to have accurate estimates for thehelpfulness function in Equation 1, we filteredout any review that did not receive at least fiveuser ratings (i.e., reviews where less than fiveusers voted it as helpful or unhelpful are filteredout).
This filtering was performed before dupli-cate detection and discarded 45.7% of the MP3Players reviews and 32.7% of the Digital Cam-eras reviews.Table 2 describes statistics for the final data-sets after the filtering steps.
10% of products forboth datasets were withheld as development cor-pora and the remaining 90% were randomlysorted into 10 sets for 10-fold cross validation.SVM RegressionFor our regression model, we deployed the stateof the art SVM regression tool SVMlight(Joachims 1999).
We tested on the developmentsets various kernels including linear, polynomial(degrees 2, 3, and 4), and radial basis function(RBF).
The best performing kernel was RBF andwe report only these results in this paper (per-formance was measured using Spearman?s corre-lation coefficient, described in Section 5.2).We tuned the RBF kernel parameters C (thepenalty parameter) and ?
(the kernel width hy-perparameter) performing full grid search overthe 110 combinations of exponentially spacedparameter pairs (C,?)
following (Hsu et al 2003).Feature ExtractionTo extract the features described in Section 3.2,we preprocessed each review using the Minipardependency parser (Lin 1994).
We used theparser tokenization, sentence breaker, and syn-tactic categorizations to generate the Length,Sentential, Unigram, Bigram, and Syntax featuresets.In order to count the occurrences of productfeatures for the Product-Feature set, we devel-oped an automatic way of mining references toproduct features from Epinions.com.
On thiswebsite, user-generated product reviews includeexplicit lists of pros and cons, describing the bestand worst aspects of a product.
For example, forMP3 players, we found the pro ?belt clip?
andthe con ?Useless FM tuner?.
Our assumption isthat the pro/con lists tend to contain references tothe product features that are important to cus-tomers, and hence their occurrence in a reviewmay correlate with review helpfulness.
We fil-tered out all single-word entries which were in-frequently seen (e.g., hold, ever).
After splittingand filtering the pro/con lists, we were left with atotal of 9,110 unique features for MP3 Playersand 13,991 unique features for Digital Cameras.The Stars feature set was created directly fromthe star ratings given by each author of an Ama-zon.com review.For each feature measurement f, we appliedthe following standard transformation:( )1ln +fand then scaled each feature between [0, 1] assuggested in (Hsu et al 2003).We experimented with various combinationsof feature sets.
Our results tables use the abbre-viations presented in Section 3.2.
For brevity, wereport the combinations which contributed to ourbest performing system and those that help assessthe power of the different feature classes in cap-turing helpfulness.5.2 Ranking PerformanceEvaluating the quality of a particular ranking isdifficult since certain ranking intervals can bemore important than others (e.g., top-10 versusbottom-10) We adopt the Spearman correlationcoefficient ?
(Spearman 1904) since it is themost commonly used measure of correlation be-tween two sets of ranked data points4.For each fold in our 10-fold cross-validationexperiments, we trained our SVM system using 9folds.
For the remaining test fold, we ranked eachproduct?s reviews according to the SVM predic-tion (described in Section 4) and computed the ?4 We used the version of Spearman?s correlation coeffi-cient that allows for ties in rankings.
See Siegel and Cas-tellan (1988) for more on alternate rank statistics such asKendall?s tau.Table 2.
Overview of filtered datasets extractedfrom Amazon.com.MP3 PLAYERSDIGITALCAMERASTotal Products 736 1066Total Reviews 11,374 14,467Average Reviews/Product 15.4 13.6Min/MaxReviews/Product 1 / 375 1 / 168427correlation between the ranking and the goldstandard ranking from the test fold5.Although our task definition is to learn reviewrankings according to helpfulness, as an interme-diate step the SVM system learns to predict theabsolute helpfulness score for each review.
Totest the correlation of this score against the goldstandard, we computed the standard Pearson cor-relation coefficient.Results show that the highest performing fea-ture combination consisted of the Length, theUnigram, and the Stars feature sets.
Table 3 re-ports the evaluation results for every combinationof these features with 95% confidence bounds.Of the three features alone, neither was statisti-cally more significant than the others.
Examiningeach pair combination, only the combination oflength with stars outperformed the others.
Sur-prisingly, adding unigram features to this combi-nation had little effect for the MP3 Players.Given our list of features defined in Section3.2, helpfulness of reviews is best captured witha combination of the Length and Stars features.Training an RBF-kernel SVM regression modeldoes not necessarily make clear the exact rela-tionship between input and output variables.
Toinvestigate this relationship between length andhelpfulness, we inspected their Pearson correla-tion coefficient, which was 0.45.
Users indeedtend to find short reviews less helpful than longerones: out of the 5,247 reviews for MP3 Playersthat contained more than 1000 characters, theaverage gold standard helpfulness score was82%; the 204 reviews with fewer than 100 char-acters had on average a score of 23%.
The ex-plicit product rating, such as Stars is also an5 Recall that the gold standard is extracted directly fromuser helpfulness votes on Amazon.com (see Section 4).indicator of review helpfulness, with a Pearsoncorrelation coefficient of 0.48.The low Pearson correlations of Table 3 com-pared to the Spearman correlations suggest thatwe can learn the ranking without perfectly learn-ing the function itself.
To investigate this, wetested the ability of SVM regression to recoverthe target helpfulness score, given the score itselfas the only feature.
The Spearman correlation forthis test was a perfect 1.0.
Interestingly, the Pear-son correlation was only 0.798, suggesting thatthe RBF kernel does learn the helpfulness rank-ing without learning the function exactly.5.3 Results AnalysisTable 3 shows only the feature combinations ofour highest performing system.
In Table 4, wereport several other feature combinations to showwhy we selected certain features and what wasthe effect of our five feature classes presented inSection 3.2.In the first block of six feature combinations inTable 4, we show that the unigram features out-perform the bigram features, which seem to besuffering from the data sparsity of the short re-views.
Also, unigram features seem to subsumethe information carried in our semantic featuresProduct-Feature (PRF) and General-Inquirer(GIW).
Although both PRF and GIW performwell as standalone features, when combined withunigrams there is little performance difference(for MP3 Players we see a small but insignificantdecrease in performance whereas for DigitalCameras we see a small but insignificant im-provement).
Recall that PRF and GIW are simplysubsets of review words that are found to beproduct features or sentiment words.
The learn-ing algorithm seems to discover on its own whichTable 3.
Evaluation of the feature combinations that make up our best performing system(in bold), for ranking reviews of Amazon.com MP3 Players and Digital Cameras accord-ing to helpfulness.MP3 PLAYERS DIGITAL CAMERASFEATURE COMBINATIONSSPEARMAN?
PEARSON?
SPEARMAN?
PEARSON?LEN 0.575 ?
0.037 0.391 ?
0.038 0.521 ?
0.029 0.357 ?
0.029UGR 0.593 ?
0.036 0.398 ?
0.038 0.499 ?
0.025 0.328 ?
0.029STR1 0.589 ?
0.034 0.326 ?
0.038 0.507 ?
0.029 0.266 ?
0.030UGR+STR1 0.644 ?
0.033 0.436 ?
0.038 0.490 ?
0.032 0.324 ?
0.032LEN+UGR 0.582 ?
0.036 0.401 ?
0.038 0.553 ?
0.028 0.394 ?
0.029LEN+STR1 0.652 ?
0.033 0.470 ?
0.038 0.577 ?
0.029 0.423 ?
0.031LEN+UGR+STR1 0.656 ?
0.033 0.476 ?
0.038 0.595 ?
0.028 0.442 ?
0.031LEN=Length; UGR=Unigram; STR=Stars?95% confidence bounds are calculated using 10-fold cross-validation.428words are most important in a review and doesnot use additional knowledge about the meaningof the words (at least not the semantics containedin PRF and GIW).We tested two different versions of the Starsfeature: i) the number of star ratings, STR1; andii) the difference between the star rating and theaverage rating of the review, STR2.
The secondblock of feature combinations in Table 4 showsthat neither is significantly better than the otherso we chose STR1 for our best performing sys-tem.Our experiments also revealed that our struc-tural features Sentential and HTML, as well asour syntactic features, Syntax, did not show anysignificant improvement in system performance.In the last block of feature combinations in Table4, we report the performance of our best per-forming features (Length, Unigram, and Stars)along with these other features.
Though none ofthe features cause a performance deterioration,neither of them significantly improves perform-ance.5.4 DiscussionIn this section, we discuss the broader implica-tions and potential impacts of our work, and pos-sible connections with other research directions.The usefulness of the Stars feature for deter-mining review helpfulness suggests the need fordeveloping automatic methods for assessing pro-duct ratings, e.g., (Pang and Lee 2005).Our findings focus on predictors of helpful-ness of reviews of tangible consumer products(consumer electronics).
Helpfulness is also solic-ited and tracked for reviews of many other typesof entities: restaurants (citysearch.com), films(imdb.com), reviews of open-source softwaremodules (cpanratings.perl.org), and countlessothers.
Our findings of the importance of Length,Unigrams, and Stars may provide the basis ofcomparison for assessing helpfulness of reviewsof other entity types.Our work represents an initial step in assessinghelpfulness.
In the future, we plan to investigateother possible indicators of helpfulness such as areviewer?s reputation, the use of comparatives(e.g., more and better than), and references toother products.Taken further, this work may have interestingconnections to work on personalization, socialnetworks, and recommender systems, for in-stance by identifying the reviews that a particularuser would find helpful.Our work on helpfulness of reviews also haspotential applications to work on automatic gen-Table 4.
Performance evaluation of various feature combinations for ranking reviews of MP3 Playersand Digital Cameras on Amazon.com according to helpfulness.
The first six lines suggest that uni-grams subsume the semantic features; the next two support the use of the raw counts of product ratings(stars) rather than the distance of this count from the average rating; the final six investigate the impor-tance of auxiliary feature sets.MP3 PLAYERS DIGITAL CAMERASFEATURE COMBINATIONSSPEARMAN?
PEARSON?
SPEARMAN?
PEARSON?UGR 0.593 ?
0.036 0.398 ?
0.038 0.499 ?
0.025 0.328 ?
0.029BGR 0.499 ?
0.040 0.293 ?
0.038 0.434 ?
0.032 0.242 ?
0.029PRF 0.591?
0.037 0.400 ?
0.039 0.527 ?
0.030 0.316 ?
0.028GIW 0.571 ?
0.036 0.381 ?
0.038 0.524 ?
0.030 0.333 ?
0.028UGR+PRF 0.570 ?
0.037 0.375 ?
0.038 0.546 ?
0.029 0.348 ?
0.028UGR+GIW 0.554 ?
0.037 0.358 ?
0.038 0.568 ?
0.031 0.324 ?
0.029STR1 0.589 ?
0.034 0.326 ?
0.038 0.507 ?
0.029 0.266 ?
0.030STR2 0.556 ?
0.032 0.303 ?
0.038 0.504 ?
0.027 0.229 ?
0.027LEN+UGR+STR1 0.656 ?
0.033 0.476 ?
0.038 0.595 ?
0.028 0.442 ?
0.031LEN+UGR+STR1+SEN 0.653 ?
0.033 0.470 ?
0.038 0.599 ?
0.028 0.448 ?
0.030LEN+UGR+STR1+HTM 0.640 ?
0.035 0.459 ?
0.039 0.594 ?
0.028 0.442 ?
0.031LEN+UGR+STR1+SYN 0.645 ?
0.034 0.469 ?
0.039 0.595 ?
0.028 0.447 ?
0.030LEN+UGR+STR1+SEN+HTM+SYN 0.631 ?
0.035 0.453 ?
0.039 0.600 ?
0.028 0.452 ?
0.030LEN+UGR+STR1+SEN+HTM+SYN+PRF+GIW 0.601 ?
0.035 0.396 ?
0.038 0.604 ?
0.027 0.460 ?
0.030LEN=Length; SEN=Sentential; HTM=HTML; UGR=Unigram; BGR=Bigram;SYN=Syntax; PRF=Product-Feature; GIW=General-Inquirer; STR=Stars?95% confidence bounds are calculated using 10-fold cross-validation.429eration of review information, by providing away to assess helpfulness of automatically gener-ated reviews.
Work on generation of reviews in-cludes review summarization and extraction ofuseful reviews from blogs and other mixed texts.6 ConclusionsRanking reviews according to user helpfulness isan important problem for many online sites suchas Amazon.com and Ebay.com.
To date, mostwebsites measure helpfulness by having usersmanually assess how helpful each review is tothem.
In this paper, we proposed an algorithm forautomatically assessing helpfulness and rankingreviews according to it.
Exploiting the multitudeof user-rated reviews on Amazon.com, wetrained an SVM regression system to learn ahelpfulness function and then applied it to rankunlabeled reviews.
Our best system achievedSpearman correlation coefficient scores of 0.656and 0.604 against a gold standard for MP3 play-ers and digital cameras.We also performed a detailed analysis of dif-ferent features to study the importance of severalfeature classes in capturing helpfulness.
Wefound that the most useful features were thelength of the review, its unigrams, and its productrating.
Semantic features like mentions of prod-uct features and sentiment words seemed to besubsumed by the simple unigram features.
Struc-tural features (other than length) and syntacticfeatures had no significant impact.It is our hope through this work to shed somelight onto what people find helpful in user-supplied reviews and, by automatically rankingthem, to ultimately enhance user experience.ReferencesAttali, Y. and Burstein, J.
2006.
Automated EssayScoring With e-rater?
V.2.
Journal of Technology,Learning, and Assessment, 4(3).Burstein, J., Chodorow, M., and Leacock, C. 2004.Automated essay evaluation: the criterion onlinewriting service.
AI Magazine.
25(3), pp 27?36.Drucker,H., Wu,D.
and Vapnik,V.
1999.
Support vectormachines for spam categorization.
IEEE Trans.Neural Netw., 10, 1048?1054.Gabrilovich, E. and Markovitch, S. 2005.Feature Generation for Text Categorization UsingWorld Knowledge.
In Proceedings of IJCAI-2005.Hsu, C.-W.; Chang, C.-C.; and Lin, C.-J.
2003.
Apractical guide to SVM classification.
Technicalreport, Department of Computer Science andInformation Technology, National Taiwan University.Hu, M. and Liu, B.
2004.
Mining and summarizingcustomer reviews.
KDD?04.
pp.168 ?
177Kim, S. and Hovy, E. 2004.
Determining the Sentimentof Opinions.
Proceedings of COLING-04.Joachims, T. 1999.
Making Large-Scale SVM LearningPractical.
In B. Sch?lkopf, C. Burges, and A. Smola(eds), Advances in Kernel Methods: Support VectorLearning.
MIT Press.
Cambridge, MA.Joachims, T. 2002.
Optimizing Search Engines UsingClickthrough Data.
In Proceedings of ACM KDD-02.Moschitti, A. and Basili R. 2004.
Complex LinguisticFeatures for Text Classification: A ComprehensiveStudy.
In Proceedings of ECIR 2004.
Sunderland,U.K.Pang, B, L. Lee, and S. Vaithyanathan.
2001.
Thumbsup?
Sentiment Classification using Machine LearningTechniques.
Proceedings of EMNLP 2002.Pang, B. and Lee, L. 2005.
Seeing stars: Exploiting classrelationships for sentiment categorization with respectto rating scales.
In Proceedings of the ACL, 2005.Riloff , E. and J. Wiebe.
2003.
Learning ExtractionPatterns for Subjective Expressions.
In Proc.
ofEMNLP-03.Riloff, E., J. Wiebe, and T. Wilson.
2003.
LearningSubjective Nouns Using Extraction PatternBootstrapping.
Proceedings of CoNLL-03Rose, C., Roque, A., Bhembe, D., and Vanlehn, K. 2003.A Hybrid Text Classification Approach for Analysisof Student Essays.
In Proc.
of the HLT-NAACL, 2003.Salton, G. and McGill, M. J.
1983.
Introduction toModern Information Retrieval.
McGraw Hill.Siegel, S. and Castellan, N.J. Jr. 1988.
NonparametricStatistics for the Behavioral Sciences.
McGraw-Hill.Spearman C. 1904.
The Proof and Measurement ofAssociation Between Two Things.
American Journalof Psychology, 15:72?101.Turney, P. 2002.
Thumbs Up or Thumbs Down?Semantic Orientation Applied to UnsupervisedClassification of Reviews.
Proceedings of the 40thAnnual Meeting of the ACL, Philadelphia, 417?424.Vapnik, V.N.
1995.
The Nature of Statistical LearningTheory.
Springer.Wiebe, J, R. Bruce, and T. O?Hara.
1999.
Developmentand use of a gold standard data set for subjectivityclassifications.
Proc.
of the 37th Annual Meeting of theAssociation for Computational Linguistics(ACL-99),246?253.Yu, H. and Hatzivassiloglou, V. 2003.
TowardsAnswering Opinion Questions: Separating Facts fromOpinions and Identifying the Polarity of OpinionSentences.
Proceedings of EMNLP 2003.430
