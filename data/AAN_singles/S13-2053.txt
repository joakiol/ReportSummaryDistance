Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 321?327, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsNRC-Canada: Building the State-of-the-Art inSentiment Analysis of TweetsSaif M. Mohammad, Svetlana Kiritchenko, and Xiaodan ZhuNational Research Council CanadaOttawa, Ontario, Canada K1A 0R6{saif.mohammad,svetlana.kiritchenko,xiaodan.zhu}@nrc-cnrc.gc.caAbstractIn this paper, we describe how we created twostate-of-the-art SVM classifiers, one to de-tect the sentiment of messages such as tweetsand SMS (message-level task) and one to de-tect the sentiment of a term within a message(term-level task).
Among submissions from44 teams in a competition, our submissionsstood first in both tasks on tweets, obtainingan F-score of 69.02 in the message-level taskand 88.93 in the term-level task.
We imple-mented a variety of surface-form, semantic,and sentiment features.
We also generatedtwo large word?sentiment association lexi-cons, one from tweets with sentiment-wordhashtags, and one from tweets with emoticons.In the message-level task, the lexicon-basedfeatures provided a gain of 5 F-score pointsover all others.
Both of our systems can bereplicated using freely available resources.11 IntroductionHundreds of millions of people around the world ac-tively use microblogging websites such as Twitter.Thus there is tremendous interest in sentiment anal-ysis of tweets across a variety of domains such ascommerce (Jansen et al 2009), health (Chew andEysenbach, 2010; Salathe?
and Khandelwal, 2011),and disaster management (Verma et al 2011; Man-del et al 2012).1The three authors contributed equally to this paper.
Svet-lana Kiritchenko developed the system for the message-leveltask, Xiaodan Zhu developed the system for the term-level task,and Saif Mohammad led the overall effort, co-ordinated bothtasks, and contributed to feature development.In this paper, we describe how we created twostate-of-the-art SVM classifiers, one to detect thesentiment of messages such as tweets and SMS(message-level task) and one to detect the sentimentof a term within a message (term-level task).
Thesentiment can be one out of three possibilities: posi-tive, negative, or neutral.
We developed these classi-fiers to participate in an international competition or-ganized by the Conference on Semantic EvaluationExercises (SemEval-2013) (Wilson et al 2013).2The organizers created and shared sentiment-labeledtweets for training, development, and testing.
Thedistributions of the labels in the different datasets isshown in Table 1.
The competition, officially re-ferred to as Task 2: Sentiment Analysis in Twitter,had 44 teams (34 for the message-level task and 23for the term-level task).
Our submissions stood firstin both tasks, obtaining a macro-averaged F-scoreof 69.02 in the message-level task and 88.93 in theterm-level task.The task organizers also provided a second testdataset, composed of Short Message Service (SMS)messages (no training data of SMS messages wasprovided).
We applied our classifiers on the SMStest set without any further tuning.
Nonetheless, theclassifiers still obtained the first position in identify-ing sentiment of SMS messages (F-score of 68.46)and second position in detecting the sentiment ofterms within SMS messages (F-score of 88.00, only0.39 points behind the first ranked system).We implemented a number of surface-form, se-mantic, and sentiment features.
We also gener-ated two large word?sentiment association lexicons,2http://www.cs.york.ac.uk/semeval-2013/task2321Table 1: Class distributions in the training set (Train), de-velopment set (Dev) and testing set (Test).
The Train setwas accessed through tweet ids and a download script.However, not all tweets were accessible.
Below is thenumber of Train examples we were able to download.The Dev and Test sets were provided by FTP.Dataset Positive Negative Neutral TotalTweetsMessage-level task:Train 3,045 (37%) 1,209 (15%) 4,004 (48%) 8,258Dev 575 (35%) 340 (20%) 739 (45%) 1,654Test 1,572 (41%) 601 (16%) 1,640 (43%) 3,813Term-level task:Train 4,831 (62%) 2,540 (33%) 385 (5%) 7,756Dev 648 (57%) 430 (38%) 57 (5%) 1,135Test 2,734 (62%) 1,541 (35%) 160 (3%) 4,435SMSMessage-level task:Test 492 (23%) 394 (19%) 1,208 (58%) 2,094Term-level task:Test 1,071 (46%) 1,104 (47%) 159 (7%) 2,334one from tweets with sentiment-word hashtags, andone from tweets with emoticons.
The automaticallygenerated lexicons were particularly useful.
In themessage-level task for tweets, they alone provided again of more than 5 F-score points over and abovethat obtained using all other features.
The lexiconsare made freely available.32 Sentiment LexiconsSentiment lexicons are lists of words with associa-tions to positive and negative sentiments.2.1 Existing, Automatically Created SentimentLexiconsThe manually created lexicons we used include theNRC Emotion Lexicon (Mohammad and Turney,2010; Mohammad and Yang, 2011) (about 14,000words), the MPQA Lexicon (Wilson et al 2005)(about 8,000 words), and the Bing Liu Lexicon (Huand Liu, 2004) (about 6,800 words).2.2 New, Tweet-Specific, AutomaticallyGenerated Sentiment Lexicons2.2.1 NRC Hashtag Sentiment LexiconCertain words in tweets are specially marked witha hashtag (#) to indicate the topic or sentiment.
Mo-3www.purl.com/net/sentimentoftweetshammad (2012) showed that hashtagged emotionwords such as joy, sadness, angry, and surprised aregood indicators that the tweet as a whole (even with-out the hashtagged emotion word) is expressing thesame emotion.
We adapted that idea to create a largecorpus of positive and negative tweets.We polled the Twitter API every four hours fromApril to December 2012 in search of tweets with ei-ther a positive word hashtag or a negative word hash-tag.
A collection of 78 seed words closely relatedto positive and negative such as #good, #excellent,#bad, and #terrible were used (32 positive and 36negative).
These terms were chosen from entries forpositive and negative in the Roget?s Thesaurus.A set of 775,000 tweets were used to generate alarge word?sentiment association lexicon.
A tweetwas considered positive if it had one of the 32 pos-itive hashtagged seed words, and negative if it hadone of the 36 negative hashtagged seed words.
Theassociation score for a term w was calculated fromthese pseudo-labeled tweets as shown below:score(w) = PMI(w, positive)?
PMI(w, negative)(1)where PMI stands for pointwise mutual informa-tion.
A positive score indicates association with pos-itive sentiment, whereas a negative score indicatesassociation with negative sentiment.
The magni-tude is indicative of the degree of association.
Thefinal lexicon, which we will refer to as the NRCHashtag Sentiment Lexicon has entries for 54,129unigrams and 316,531 bigrams.
Entries were alsogenerated for unigram?unigram, unigram?bigram,and bigram?bigram pairs that were not necessarilycontiguous in the tweets corpus.
Pairs with cer-tain punctuations, ?@?
symbols, and some functionwords were removed.
The lexicon has entries for308,808 non-contiguous pairs.2.2.2 Sentiment140 LexiconThe sentiment140 corpus (Go et al 2009) is acollection of 1.6 million tweets that contain pos-itive and negative emoticons.
The tweets are la-beled positive or negative according to the emoti-con.
We generated a sentiment lexicon from thiscorpus in the same manner as described above (Sec-tion 2.2.1).
This lexicon has entries for 62,468unigrams, 677,698 bigrams, and 480,010 non-contiguous pairs.3223 Task: Automatically Detecting theSentiment of a MessageThe objective of this task is to determine whether agiven message is positive, negative, or neutral.3.1 Classifier and featuresWe trained a Support Vector Machine (SVM) (Fanet al 2008) on the training data provided.
SVMis a state-of-the-art learning algorithm proved to beeffective on text categorization tasks and robust onlarge feature spaces.
The linear kernel and the valuefor the parameter C=0.005 were chosen by cross-validation on the training data.We normalized all URLs to http://someurl and alluserids to @someuser.
We tokenized and part-of-speech tagged the tweets with the Carnegie MellonUniversity (CMU) Twitter NLP tool (Gimpel et al2011).
Each tweet was represented as a feature vec-tor made up of the following groups of features:?
word ngrams: presence or absence of contigu-ous sequences of 1, 2, 3, and 4 tokens; non-contiguous ngrams (ngrams with one token re-placed by *);?
character ngrams: presence or absence of con-tiguous sequences of 3, 4, and 5 characters;?
all-caps: the number of words with all charac-ters in upper case;?
POS: the number of occurrences of each part-of-speech tag;?
hashtags: the number of hashtags;?
lexicons: the following sets of features weregenerated for each of the three manually con-structed sentiment lexicons (NRC EmotionLexicon, MPQA, Bing Liu Lexicon) and foreach of the two automatically constructed lex-icons (Hashtag Sentiment Lexicon and Senti-ment140 Lexicon).
Separate feature sets wereproduced for unigrams, bigrams, and non-contiguous pairs.
The lexicon features werecreated for all tokens in the tweet, for each part-of-speech tag, for hashtags, and for all-caps to-kens.
For each token w and emotion or po-larity p, we used the sentiment/emotion scorescore(w, p) to determine:?
total count of tokens in the tweet withscore(w, p) > 0;?
total score =?w?tweet score(w, p);?
the maximal score =maxw?tweetscore(w, p);?
the score of the last token in the tweet withscore(w, p) > 0;?
punctuation:?
the number of contiguous sequences ofexclamation marks, question marks, andboth exclamation and question marks;?
whether the last token contains an excla-mation or question mark;?
emoticons: The polarity of an emoticon wasdetermined with a regular expression adoptedfrom Christopher Potts?
tokenizing script:4?
presence or absence of positive and nega-tive emoticons at any position in the tweet;?
whether the last token is a positive or neg-ative emoticon;?
elongated words: the number of words with onecharacter repeated more than two times, for ex-ample, ?soooo?;?
clusters: The CMU pos-tagging tool providesthe token clusters produced with the Brownclustering algorithm on 56 million English-language tweets.
These 1,000 clusters serve asalternative representation of tweet content, re-ducing the sparcity of the token space.?
the presence or absence of tokens fromeach of the 1000 clusters;?
negation: the number of negated contexts.
Fol-lowing (Pang et al 2002), we defined a negatedcontext as a segment of a tweet that startswith a negation word (e.g., no, shouldn?t) andends with one of the punctuation marks: ?,?,?.
?, ?
:?, ?
;?, ?!
?, ???.
A negated context af-fects the ngram and lexicon features: we add?
NEG?
suffix to each word following the nega-tion word (?perfect?
becomes ?perfect NEG?
).The ?
NEG?
suffix is also added to polarity andemotion features (?POLARITY positive?
be-comes ?POLARITY positive NEG?).
The listof negation words was adopted from Christo-pher Potts?
sentiment tutorial.54http://sentiment.christopherpotts.net/tokenizing.html5http://sentiment.christopherpotts.net/lingstruc.html3233.2 ExperimentsWe trained the SVM classifier on the set of 9,912annotated tweets (8,258 in the training set and 1,654in the development set).
We applied the model to thetest set of 3,813 unseen tweets.
The same model wasapplied unchanged to the other test set of 2,094 SMSmessages as well.
The bottom-line score used by thetask organizers was the macro-averaged F-score ofthe positive and negative classes.
The results ob-tained by our system on the training set (ten-foldcross-validation), development set (when trained onthe training set), and test sets (when trained on thecombined set of tweets in the training and devel-opment sets) are shown in Table 2.
The table alsoshows baseline results obtained by a majority clas-sifier that always predicts the most frequent class asoutput.
Since the bottom-line F-score is based onlyon the F-scores of positive and negative classes (andnot on neutral), the majority baseline chose the mostfrequent class among positive and negative, whichin this case was the positive class.
We also showbaseline results obtained using an SVM and unigramfeatures alone.
Our system (SVM and all features)obtained a macro-averaged F-score of 69.02 on thetweet set and 68.46 on the SMS set.
In the SemEval-2013 competition, our submission ranked first onboth datasets.
There were 48 submissions from 34teams for this task.Table 3 shows the results of the ablation experi-ments where we repeat the same classification pro-cess but remove one feature group at a time.
Themost influential features for both datasets turned outto be the sentiment lexicon features: they providedgains of more than 8.5%.
It is interesting to notethat tweets benefited mostly from the automatic sen-timent lexicons (NRC Hashtag Lexicon and the Sen-timent140 Lexicon) whereas the SMS set benefitedmore from the manual lexicons (MPQA, NRC Emo-tion Lexicon, Bing Liu Lexicon).
Among the au-tomatic lexicons, both the Hashtag Sentiment Lex-icon and the Sentiment140 Lexicon contributed toroughly the same amount of improvement in perfor-mance on the tweet set.The second most important feature group forthe message-level task was that of ngrams (wordand character ngrams).
Expectedly, the impact ofngrams on the SMS dataset was less extensive sinceTable 2: Message-level Task: The macro-averaged F-scores on different datasets.Classifier Tweets SMSTraining set: Majority 26.94 -SVM-all 67.20 -Development set: Majority 26.85 -SVM-all 68.72 -Test set: Majority 29.19 19.03SVM-unigrams 39.61 39.29SVM-all 69.02 68.46Table 3: Message-level Task: The macro-averaged F-scores obtained on the test sets with one of the featuregroups removed.
The number in the brackets is the dif-ference with the all features score.
The biggest drops areshown in bold.Experiment Tweets SMSall features 69.02 68.46all - lexicons 60.42 (-8.60) 59.73 (-8.73)all - manual lex.
67.45 (-1.57) 65.64 (-2.82)all - auto.
lex.
63.78 (-5.24) 67.12 (-1.34)all - Senti140 lex.
65.25 (-3.77) 67.33 (-1.13)all - Hashtag lex.
65.22 (-3.80) 70.28 (1.82)all - ngrams 61.77 (-7.25) 67.27 (-1.19)all - word ngrams 64.64 (-4.38) 66.56 (-1.9)all - char.
ngrams 67.10 (-1.92) 68.94 (0.48)all - negation 67.20 (-1.82) 66.22 (-2.24)all - POS 68.38 (-0.64) 67.07 (-1.39)all - clusters 69.01 (-0.01) 68.10 (-0.36)all - encodings (elongated, emoticons, punctuations,all-caps, hashtags) 69.16 (0.14) 68.28 (-0.18)the classifier model was trained only on tweets.Attention to negations improved performance onboth datasets.
Removing the sentiment encodingfeatures like hashtags, emoticons, and elongatedwords, had almost no impact on performance, butthis is probably because the discriminating informa-tion in them was also captured by some other fea-tures such as character and word ngrams.4 Task: Automatically Detecting theSentiment of a Term in a MessageThe objective of this task is to detect whether a term(a word or phrase) within a message conveys a pos-itive, negative, or neutral sentiment.
Note that thesame term may express different sentiments in dif-ferent contexts.3244.1 Classifier and featuresWe trained an SVM using the LibSVM package(Chang and Lin, 2011) and a linear kernel.
In ten-fold cross-validation over the training data, the lin-ear kernel outperformed other kernels implementedin LibSVM as well as a maximum-entropy classi-fier.
Our model leverages a variety of features, asdescribed below:?
word ngrams:?
presence or absence of unigrams, bigrams,and the full word string of a target term;?
leading and ending unigrams and bigrams;?
character ngrams: presence or absence of two-and three-character prefixes and suffixes of allthe words in a target term (note that the targetterm may be a multi-word sequence);?
elongated words: presence or absence of elon-gated words (e.g., ?sooo?);?
emoticons: the numbers and categories ofemoticons that a term contains6;?
punctuation: presence or absence of punctua-tion sequences such as ??!?
and ?!!!?;?
upper case:?
whether all the words in the target startwith an upper case letter followed bylower case letters;?
whether the target words are all in upper-case (to capture a potential named entity);?
stopwords: whether a term contains only stop-words.
If so, separate features indicate whetherthere are 1, 2, 3, or more stop-words;?
lengths:?
the length of a target term (number ofwords);?
the average length of words (number ofcharacters) in a term;?
a binary feature indicating whether a termcontains long words;6http://en.wikipedia.org/wiki/List of emoticons?
negation: similar to those described for themessage-level task.
Whenever a negation wordwas found immediately before the target orwithin the target, the polarities of all tokens af-ter the negation term were flipped;?
position: whether a term is at the beginning,end, or another position;?
sentiment lexicons: we used automatically cre-ated lexicons (NRC Hashtag Sentiment Lexi-con, Sentiment140 Lexicon) as well as manu-ally created lexicons (NRC Emotion Lexicon,MPQA, Bing Liu Lexicon).?
total count of tokens in the target termwith sentiment score greater than 0;?
the sum of the sentiment scores for all to-kens in the target;?
the maximal sentiment score;?
the non-zero sentiment score of the last to-ken in the target;?
term splitting: when a term contains a hash-tag made of multiple words (e.g., #biggest-daythisyear), we split the hashtag into compo-nent words;?
others:?
whether a term contains a Twitter username;?
whether a term contains a URL.The above features were extracted from targetterms as well as from the rest of the message (thecontext).
For unigrams and bigrams, we used fourwords on either side of the target as the context.
Thewindow size was chosen through experiments on thedevelopment set.4.2 ExperimentsWe trained an SVM classifier on the 8,891 annotatedterms in tweets (7,756 terms in the training set and1,135 terms in the development set).
We applied themodel to 4,435 terms in the tweets test set.
The samemodel was applied unchanged to the other test set of2,334 terms in unseen SMS messages as well.
Thebottom-line score used by the task organizers wasthe macro-averaged F-score of the positive and neg-ative classes.325The results on the training set (ten-fold cross-validation), the development set (trained on thetraining set), and the test sets (trained on the com-bined set of tweets in the training and developmentsets) are shown in Table 4.
The table also showsbaseline results obtained by a majority classifier thatalways predicts the most frequent class as output,and an additional baseline result obtained using anSVM and unigram features alone.
Our submissionobtained a macro-averaged F-score of 88.93 on thetweet set and was ranked first among 29 submissionsfrom 23 participating teams.
Even with no tuningspecific to SMS data, our SMS submission still ob-tained second rank with an F-score of 88.00.
Thescore of the first ranking system on the SMS set was88.39.
A post-competition bug-fix in the bigram fea-tures resulted in a small improvement: F-score of89.10 on the tweets set and 88.34 on the SMS set.Note that the performance is significantly higherin the term-level task than in the message-level task.This is largely because of the ngram features (seeunigram baselines in Tables 2 and 4).
We analyzedthe labeled data provided to determine why ngramsperformed so strongly in this task.
We found that thepercentage of test tokens already seen within train-ing data targets was 85.1%.
Further, the average ra-tio of instances pertaining to the most dominant po-larity of a target term to the total number of instancesof that target term was 0.808.Table 5 presents the ablation F-scores.
Observethat the ngram features were the most useful.
Notealso that removing just the word ngram features orjust the character ngram features results in only asmall drop in performance.
This indicates that thetwo feature groups capture similar information.The sentiment lexicon features are the next mostuseful group?removing them leads to a drop in F-score of 3.95 points for the tweets set and 4.64 forthe SMS set.
Modeling negation improves the F-score by 0.72 points on the tweets set and 1.57 pointson the SMS set.The last two rows in Table 5 show the results ob-tained when the features are extracted only from thetarget (and not from its context) and when they areextracted only from the context of the target (andnot from the target itself).
Observe that even thoughthe context may influence the polarity of the tar-get, using target features alone is substantially moreTable 4: Term-level Task: The macro-averaged F-scoreson the datasets.
The official scores of our submission areshown in bold.
SVM-all* shows results after a bug fix.Classifier Tweets SMSTraining set: Majority 38.38 -SVM-all 86.80 -Development set: Majority 36.34 -SVM-all 86.49 -Test set: Majority 38.13 32.11SVM-unigrams 80.28 78.71official SVM-all 88.93 88.00SVM-all* 89.10 88.34Table 5: Term-level Task: The F-scores obtained on thetest sets with one of the feature groups removed.
Thenumber in brackets is the difference with the all featuresscore.
The biggest drops are shown in bold.Experiment Tweets SMSall features 89.10 88.34all - ngrams 83.86 (-5.24) 80.49 (-7.85)all - word ngrams 88.38 (-0.72) 87.37 (-0.97)all - char.
ngrams 89.01 (-0.09) 87.31 (-1.03)all - lexicons 85.15 (-3.95) 83.70 (-4.64)all - manual lex.
87.69 (-1.41) 86.84 (-1.5)all - auto lex.
88.24 (-0.86) 86.65 (-1.69)all - negation 88.38 (-0.72) 86.77 (-1.57)all - stopwords 89.17 (0.07) 88.30 (-0.04)all - encodings (elongated words, emoticons, punctns.,uppercase) 89.16 (0.06) 88.39 (0.05)all - target 72.97 (-16.13) 68.96 (-19.38)all - context 85.02 (-4.08) 85.93 (-2.41)useful than using context features alone.
Nonethe-less, adding context features improves the F-scoresby roughly 2 to 4 points.5 ConclusionsWe created two state-of-the-art SVM classifiers, oneto detect the sentiment of messages and one to de-tect the sentiment of a term within a message.
Oursubmissions on tweet data stood first in both thesesubtasks of the SemEval-2013 competition ?Detect-ing Sentiment in Twitter?.
We implemented a varietyof features based on surface form and lexical cate-gories.
The sentiment lexicon features (both manu-ally created and automatically generated) along withngram features (both word and character ngrams)led to the most gain in performance.326AcknowledgmentsWe thank Colin Cherry for providing his SVM codeand for helpful discussions.ReferencesChih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2(3):27:1?27:27.Cynthia Chew and Gunther Eysenbach.
2010.
Pan-demics in the Age of Twitter: Content Analysis ofTweets during the 2009 H1N1 Outbreak.
PLoS ONE,5(11):e14118+, November.R.-E.
Fan, K.-W. Chang, C.-J.
Hsieh, X.-R. Wang, andLin C.-J.
2008.
LIBLINEAR: A Library for LargeLinear Classification.
Journal of Machine LearningResearch, 9:1871?1874.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-Speech Tagging forTwitter: Annotation, Features, and Experiments.
InProceedings of the Annual Meeting of the Associationfor Computational Linguistics.Alec Go, Richa Bhayani, and Lei Huang.
2009.
TwitterSentiment Classification using Distant Supervision.
InFinal Projects from CS224N for Spring 2008/2009 atThe Stanford Natural Language Processing Group.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, KDD ?04, pages 168?177, New York, NY, USA.
ACM.Bernard J. Jansen, Mimi Zhang, Kate Sobel, and AbdurChowdury.
2009.
Twitter power: Tweets as electronicword of mouth.
Journal of the American Society forInformation Science and Technology, 60(11):2169?2188.Benjamin Mandel, Aron Culotta, John Boulahanis,Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.2012.
A demographic analysis of online sentimentduring hurricane irene.
In Proceedings of the SecondWorkshop on Language in Social Media, LSM ?12,pages 27?36, Stroudsburg, PA, USA.
Association forComputational Linguistics.Saif M. Mohammad and Peter D. Turney.
2010.
Emo-tions Evoked by Common Words and Phrases: UsingMechanical Turk to Create an Emotion Lexicon.
InProceedings of the NAACL-HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, LA, California.Saif Mohammad and Tony Yang.
2011.
Tracking Sen-timent in Mail: How Genders Differ on EmotionalAxes.
In Proceedings of the 2nd Workshop on Com-putational Approaches to Subjectivity and SentimentAnalysis (WASSA 2.011), pages 70?79, Portland, Ore-gon.
Association for Computational Linguistics.Saif Mohammad.
2012.
#Emotional Tweets.
In Pro-ceedings of the First Joint Conference on Lexicaland Computational Semantics (*SEM), pages 246?255, Montre?al, Canada.
Association for Computa-tional Linguistics.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: Sentiment Classification UsingMachine Learning Techniques.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 79?86, Philadelphia, PA.Marcel Salathe?
and Shashank Khandelwal.
2011.
As-sessing vaccination sentiments with online social me-dia: Implications for infectious disease dynamics andcontrol.
PLoS Computational Biology, 7(10).Sudha Verma, Sarah Vieweg, William Corvey, LeysiaPalen, James Martin, Martha Palmer, Aaron Schram,and Kenneth Anderson.
2011.
Natural language pro-cessing to the rescue?
extracting ?situational aware-ness?
tweets during mass emergency.
In InternationalAAAI Conference on Weblogs and Social Media.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the confer-ence on Human Language Technology and EmpiricalMethods in Natural Language Processing, HLT ?05,pages 347?354, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, SaraRosenthal, Veselin Stoyanov, and Alan Ritter.
2013.SemEval-2013 task 2: Sentiment analysis in twitter.
InProceedings of the International Workshop on Seman-tic Evaluation, SemEval ?13, Atlanta, Georgia, USA,June.327
