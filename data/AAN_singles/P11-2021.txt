Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 118?124,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsQuestion Detection in Spoken Conversations Using Textual ConversationsAnna Margolis and Mari OstendorfDepartment of Electrical EngineeringUniversity of WashingtonSeattle, WA, USA{amargoli,mo}@ee.washington.eduAbstractWe investigate the use of textual Internet con-versations for detecting questions in spokenconversations.
We compare the text-trainedmodel with models trained on manually-labeled, domain-matched spoken utteranceswith and without prosodic features.
Over-all, the text-trained model achieves over 90%of the performance (measured in Area Underthe Curve) of the domain-matched model in-cluding prosodic features, but does especiallypoorly on declarative questions.
We describeefforts to utilize unlabeled spoken utterancesand prosodic features via domain adaptation.1 IntroductionAutomatic speech recognition systems, which tran-scribe words, are often augmented by subsequentprocessing for inserting punctuation or labelingspeech acts.
Both prosodic features (extracted fromthe acoustic signal) and lexical features (extractedfrom the word sequence) have been shown to beuseful for these tasks (Shriberg et al, 1998; Kimand Woodland, 2003; Ang et al, 2005).
However,access to labeled speech training data is generallyrequired in order to use prosodic features.
On theother hand, the Internet contains large quantities oftextual data that is already labeled with punctua-tion, and which can be used to train a system us-ing lexical features.
In this work, we focus on ques-tion detection in the Meeting Recorder Dialog Actcorpus (MRDA) (Shriberg et al, 2004), using textsentences with question marks in Wikipedia ?talk?pages.
We compare the performance of a ques-tion detector trained on the text domain using lex-ical features with one trained on MRDA using lex-ical features and/or prosodic features.
In addition,we experiment with two unsupervised domain adap-tation methods to incorporate unlabeled MRDA ut-terances into the text-based question detector.
Thegoal is to use the unlabeled domain-matched data tobridge stylistic differences as well as to incorporatethe prosodic features, which are unavailable in thelabeled text data.2 Related WorkQuestion detection can be viewed as a subtask ofspeech act or dialogue act tagging, which aimsto label functions of utterances in conversations,with categories as question/statement/backchannel,or more specific categories such as request or com-mand (e.g., Core and Allen (1997)).
Previous workhas investigated the utility of various feature types;Boakye et al (2009), Shriberg et al (1998) and Stol-cke et al (2000) showed that prosodic features wereuseful for question detection in English conversa-tional speech, but (at least in the absence of recog-nition errors) most of the performance was achievedwith words alone.
There has been some previousinvestigation of domain adaptation for dialogue actclassification, including adaptation between: differ-ent speech corpora (MRDA and Switchboard) (Guzet al, 2010), speech corpora in different languages(Margolis et al, 2010), and from a speech domain(MRDA/Switchboard) to text domains (emails andforums) (Jeong et al, 2009).
These works didnot use prosodic features, although Venkataraman118et al (2003) included prosodic features in a semi-supervised learning approach for dialogue act la-beling within a single spoken domain.
Also rele-vant is the work of Moniz et al (2011), who com-pared question types in different Portuguese cor-pora, including text and speech.
For question de-tection on speech, they compared performance of alexical model trained with newspaper text to modelstrained with speech including acoustic and prosodicfeatures, where the speech-trained model also uti-lized the text-based model predictions as a feature.They reported that the lexical model mainly iden-tified wh questions, while the speech data helpedidentify yes-no and tag questions, although resultsfor specific categories were not included.Question detection is related to the task of auto-matic punctuation annotation, for which the contri-butions of lexical and prosodic features have beenexplored in other works, e.g.
Christensen et al(2001) and Huang and Zweig (2002).
Kim andWoodland (2003) and Liu et al (2006) used auxil-iary text corpora to train lexical models for punc-tuation annotation or sentence segmentation, whichwere used along with speech-trained prosodic mod-els; the text corpora consisted of broadcast news ortelephone conversation transcripts.
More recently,Gravano et al (2009) used lexical models built fromweb news articles on broadcast news speech, andcompared their performance on written news; Shenet al (2009) trained models on an online encyclo-pedia, for punctuation annotation of news podcasts.Web text was also used in a domain adaptationstrategy for prosodic phrase prediction in news text(Chen et al, 2010).In our work, we focus on spontaneous conversa-tional speech, and utilize a web text source that issomewhat matched in style: both domains consist ofgoal-directed multi-party conversations.
We focusspecifically on question detection in pre-segmentedutterances.
This differs from punctuation annota-tion or segmentation, which is usually seen as a se-quence tagging or classification task at word bound-aries, and uses mostly local features.
Our focus alsoallows us to clearly analyze the performance on dif-ferent question types, in isolation from segmenta-tion issues.
We compare performance of textual-and speech-trained lexical models, and examine thedetection accuracy of each question type.
Finally,we compare two domain adaptation approaches toutilize unlabeled speech data: bootstrapping, andBlitzer et al?s Structural Correspondence Learning(SCL) (Blitzer et al, 2006).
SCL is a feature-learning method that uses unlabeled data from bothdomains.
Although it has been applied to severalNLP tasks, to our knowledge we are the first to applySCL to both lexical and prosodic features in order toadapt from text to speech.3 Experiments3.1 DataThe Wiki talk pages consist of threaded posts bydifferent authors about a particular Wikipedia entry.While these lack certain properties of spontaneousspeech (such as backchannels, disfluencies, and in-terruptions), they are more conversational than newsarticles, containing utterances such as: ?Are you se-rious??
or ?Hey, that?s a really good point.?
Wefirst cleaned the posts (to remove URLs, images,signatures, Wiki markup, and duplicate posts) andthen performed automatic segmentation of the postsinto sentences using MXTERMINATOR (Reynarand Ratnaparkhi, 1997).
We labeled each sentenceending in a question mark (followed optionally byother punctuation) as a question; we also includedparentheticals ending in question marks.
All othersentences were labeled as non-questions.
We thenremoved all punctuation and capitalization from theresulting sentences and performed some additionaltext normalization to match the MRDA transcripts,such as number and date expansion.For the MRDA corpus, we use the manually-transcribed sentences with utterance time align-ments.
The corpus has been hand-annotated withdetailed dialogue act tags, using a hierarchical la-beling scheme in which each utterance receives one?general?
label plus a variable number of ?specific?labels (Dhillon et al, 2004).
In this work we areonly looking at the problem of discriminating ques-tions from non-questions; we consider as questionsall complete utterances labeled with one of the gen-eral labels wh, yes-no, open-ended, or, or-after-yes-no, or rhetorical question.
(To derive the questioncategories below, we also consider the specific la-bels tag and declarative, which are appended to oneof the general labels.)
All remaining utterances, in-119cluding backchannels and incomplete questions, areconsidered as non-questions, although we removedutterances that are very short (less than 200ms), haveno transcribed words, or are missing segmentationtimes or dialogue act label.
We performed minor textnormalization on the transcriptions, such as mappingall word fragments to a single token.The Wiki training set consists of close to 46kutterances, with 8.0% questions.
We derived anMRDA training set of the same size from the train-ing division of the original corpus; it consists of6.6% questions.
For the adaptation experiments, weused the full MRDA training set of 72k utterancesas unlabeled adaptation data.
We used two meet-ings (3k utterances) from the original MRDA devel-opment set for model selection and parameter tun-ing.
The remaining meetings (in the original devel-opment and test divisions; 26k utterances) were usedas our test set.3.2 Features and ClassifierLexical features consisted of unigrams through tri-grams including start- and end-utterance tags, repre-sented as binary features (presence/absence), plus atotal-number-of-words feature.
All ngram featureswere required to occur at least twice in the trainingset.
The MRDA training set contained on the orderof 65k ngram features while the Wiki training setcontained over 205k.
Although some previous workhas used part-of-speech or parse features in relatedtasks, Boakye et al (2009) showed no clear benefitof these features for question detection on MRDAbeyond the ngram features.We extracted 16 prosody features from the speechwaveforms defined by the given utterance times, us-ing stylized F0 contours computed based on So?nmezet al (1998) and Lei (2006).
The features are de-signed to be useful for detecting questions and aresimilar or identical to some of those in Boakye etal.
(2009) or Shriberg et al (1998).
They include:F0 statistics (mean, stdev, max, min) computed overthe whole utterance and over the last 200ms; slopescomputed from a linear regression to the F0 contour(over the whole utterance and last 200ms); initialand final slope values output from the stylizer; ini-tial intercept value from the whole utterance linearregression; ratio of mean F0 in the last 400-200msto that in the last 200ms; number of voiced frames;and number of words per frame.
All 16 featureswere z-normalized using speaker-level parameters,or gender-level parameters if the speaker had lessthan 10 utterances.For all experiments we used logistic regressionmodels trained with the LIBLINEAR package (Fanet al, 2008).
Prosodic and lexical features werecombined by concatenation into a single feature vec-tor; prosodic features and the number-of-words werez-normalized to place them roughly on the samescale as the binary ngram features.
(We substituted 0for missing prosody features due to, e.g., no voicedframes detected, segmentation errors, utterance tooshort.)
Our setup is similar to (Surendran andLevow, 2006), who combined ngram and prosodicfeatures for dialogue act classification using a lin-ear SVM.
Since ours is a detection problem, withquestions much less frequent than non-questions,we present results in terms of ROC curves, whichwere computed from the probability scores of theclassifier.
The cost parameter C was tuned to opti-mize Area Under the Curve (AUC) on the develop-ment set (C = 0.01 for prosodic features only andC = 0.1 in all other cases.
)3.3 Baseline ResultsFigure 1 shows the ROC curves for the baselineWiki-trained lexical system and the MRDA-trainedsystems with different feature sets.
Table 2 com-pares performance across different question cate-gories at a fixed false positive rate (16.7%) near theequal error rate of the MRDA (lex) case.
For analy-sis purposes we defined the categories in Table 2 asfollows: tag includes any yes-no question given theadditional tag label; declarative includes any ques-tion category given the declarative label that is nota tag question; the remaining categories (yes-no, or,etc.)
include utterances in those categories but notincluded in declarative or tag.
Table 1 gives exam-ple sentences for each category.As expected, the Wiki-trained system does worston declarative, which have the syntactic form ofstatements.
For the MRDA-trained system, prosodyalone does best on yes-no and declarative.
Alongwith lexical features, prosody is more useful fordeclarative, while it appears to be somewhat re-dundant with lexical features for yes-no.
Ideally,such redundancy can be used together with unla-120yes-no did did you do that?declarative you?re not going to be aroundthis afternoon?wh what do you mean um referenceframes?tag you know?rhetorical why why don?t we do that?open-ended do we have anything else to sayabout transcription?or and @frag@ did they use sig-moid or a softmax type thing?or-after-YN or should i collect it all?Table 1: Examples for each MRDA question category asdefined in this paper, based on Dhillon et al (2004).beled spoken utterances to incorporate prosodic fea-tures into the Wiki system, which may improve de-tection of some kinds of questions.0 0.2 0.4 0.6 0.8 100.20.40.60.810.9250.9120.6960.833false pos ratedetectionratetrain meetings (lex+pros)train meetings (lex only)train meetings (pros only)train wiki (lex only)Figure 1: ROC curves with AUC values for question de-tection on MRDA; comparison between systems trainedon MRDA using lexical and/or prosodic features, andWiki talk pages using lexical features.3.4 Adaptation ResultsFor bootstrapping, we first train an initial baselineclassifier using the Wiki training data, then use it tolabel MRDA data from the unlabeled adaptation set.We select the k most confident examples for eachof the two classes and add them to the training setusing the guessed labels, then retrain the classifierusing the new training set.
This is repeated for rrounds.
In order to use prosodic features, which aretype (count) MRDA(L+P)MRDA(L)MRDA(P)Wiki(L)yes-no (526) 89.4 86.1 59.3 77.2declar.
(417) 69.8 59.2 49.4 25.9wh (415) 95.4 93.0 42.2 92.8tag (358) 89.7 90.5 26.0 79.1rhetorical (75) 88.0 90.7 25.3 93.3open-ended (50) 88.0 92.0 16.0 80.0or (38) 97.4 100 29.0 89.5or-after-YN (32) 96.9 96.9 25.0 90.6Table 2: Question detection rates (%) by question type foreach system (L=lexical features, P=prosodic features.
)Detection rates are given at a false positive rate of 16.7%(starred points in Figure 1), which is the equal error ratepoint for the MRDA (L) system.
Boldface gives best re-sult for each type.type (count) baseline bootstrap SCLyes-no (526) 77.2 81.4 83.5declar.
(417) 25.9 30.5 32.1wh (415) 92.8 92.8 93.5tag (358) 79.1 79.3 80.7rhetorical (75) 93.3 88.0 92.0open-ended (50) 80.0 76.0 80.0or (38) 89.5 89.5 89.5or-after-YN (32) 90.6 90.6 90.6Table 3: Adaptation performance by question type, atfalse positive rate of 16.7% (starred points in Figure 2.
)Boldface indicates adaptation results better than baseline;italics indicate worse than baseline.available only in the bootstrapped MRDA data, wesimply add 16 zeros onto the Wiki examples in placeof the missing prosodic features.
The values k = 20and r = 6 were selected on the dev set.In contrast with bootstrapping, SCL (Blitzer et al,2006) uses the unlabeled target data to learn domain-independent features.
SCL has generated much in-terest lately because of the ability to incorporate fea-tures not seen in the training data.
The main idea isto use unlabeled data in both domains to learn linearpredictors for many ?auxiliary?
tasks, which shouldbe somewhat related to the task of interest.
In par-ticular, if x is a row vector representing the originalfeature vector and yi represents the label for auxil-iary task i, the linear predictor wi is learned to pre-dict y?i = wi ?
x?
(where x?
is a modified version of121x that excludes any features completely predictiveof yi.)
The learned predictors for all tasks {wi} arethen collected into the columns of a matrix W, onwhich singular value decomposition USVT = Wis performed.
Ideally, features that behave simi-larly across many yi will be represented in the samesingular vector; thus, the auxiliary tasks can tie to-gether features which may never occur together inthe same example.
Projection of the original featurevector onto the top h left singular vectors gives anh?dimensional feature vector z ?
UT1:h ?
x?.
Themodel is then trained on the concatenated featurerepresentation [x, z] using the labeled source data.As auxiliary tasks yi, we identify all initial wordsthat begin an utterance at least 5 times in each do-main?s training set, and predict the presence of eachinitial word (yi = 0 or 1).
The idea of using theinitial words is that they may be related to the inter-rogative status of an utterance?
utterances startingwith ?do?
or ?what?
are more often questions, whilethose starting with ?i?
are usually not.
There wereabout 250 auxiliary tasks.
The prediction features x?used in SCL include all ngrams occuring at least 5times in the unlabeled Wiki or MRDA data, exceptthose over the first word, as well as prosody features(which are zero in the Wiki data.)
We tuned h = 100and the scale factor of z (to 1) on the dev set.Figure 2 compares the results using the boot-strapping and SCL approaches, and the baseline un-adapted Wiki system.
Table 3 shows results by ques-tion type at the fixed false positive point chosenfor analysis.
At this point, both adaptation meth-ods improved detection of declarative and yes-noquestions, although they decreased detection of sev-eral other types.
Note that we also experimentedwith other adaptation approaches on the dev set:bootstrapping without the prosodic features did notlead to an improvement, nor did training on Wikiusing ?fake?
prosody features predicted based onMRDA examples.
We also tried a co-training ap-proach using separate prosodic and lexical classi-fiers, inspired by the work of Guz et al (2007) onsemi-supervised sentence segmentation; this led toa smaller improvement than bootstrapping.
Sincewe tuned and selected adaptation methods on theMRDA dev set, we compare to training with the la-beled MRDA dev (with prosodic features) and Wikidata together.
This gives superior results comparedto adaptation; but note that the adaptation processdid not use labeled MRDA data to train, but merelyfor model selection.
Analysis of the adapted sys-tems suggests prosody features are being utilized toimprove performance in both methods, but clearlythe effect is small, and the need to tune parame-ters would present a challenge if no labeled speechdata were available.
Finally, while the benefit from3k labeled MRDA utterances added to the Wiki ut-terances is encouraging, we found that most of theMRDA training utterances (with prosodic features)had to be added to match the MRDA-only result inFigure 1, although perhaps training separate lexicaland prosodic models would be useful in this respect.4 ConclusionThis work explored the use of conversational webtext to detect questions in conversational speech.We found that the web text does especially poorlyon declarative questions, which can potentially beimproved using prosodic features.
Unsupervisedadaptation methods utilizing unlabeled speech anda small labeled development set are shown to im-prove performance slightly, although training withthe small development set leads to bigger gains.Our work suggests approaches for combining largeamounts of ?naturally?
annotated web text withunannotated speech data, which could be useful inother spoken language processing tasks, e.g.
sen-tence segmentation or emphasis detection.0 0.2 0.4 0.6 0.8 100.20.40.60.810.8590.8500.8330.884false pos ratedetectionrateSCLbootstrapbaseline (no adapt)include MRDA devFigure 2: ROC curves and AUC values for adaptation,baseline Wiki, and Wiki + MRDA dev.122ReferencesJeremy Ang, Yang Liu, and Elizabeth Shriberg.
2005.Automatic dialog act segmentation and classificationin multiparty meetings.
In Proc.
Int.
Conference onAcoustics, Speech, and Signal Processing.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of the 2006 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 120?128, Sydney, Australia, July.
As-sociation for Computational Linguistics.Kofi Boakye, Benoit Favre, and Dilek Hakkini-tu?r.
2009.Any questions?
Automatic question detection in meet-ings.
In Proc.
IEEE Workshop on Automatic SpeechRecognition and Understanding.Zhigang Chen, Guoping Hu, and Wei Jiang.
2010.
Im-proving prosodic phrase prediction by unsupervisedadaptation and syntactic features extraction.
In Proc.Interspeech.Heidi Christensen, Yoshihiko Gotoh, and Steve Renals.2001.
Punctuation annotation using statistical prosodymodels.
In in Proc.
ISCA Workshop on Prosody inSpeech Recognition and Understanding, pages 35?40.Mark G. Core and James F. Allen.
1997.
Coding dialogswith the DAMSL annotation scheme.
In Proc.
of theWorking Notes of the AAAI Fall Symposium on Com-municative Action in Humans and Machines, Cam-bridge, MA, November.Rajdip Dhillon, Sonali Bhagat, Hannah Carvey, and Eliz-abeth Shriberg.
2004.
Meeting recorder project: Di-alog act labeling guide.
Technical report, ICSI Tech.Report.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874, August.Agustin Gravano, Martin Jansche, and Michiel Bacchi-ani.
2009.
Restoring punctuation and capitalization intranscribed speech.
In Proc.
Int.
Conference on Acous-tics, Speech, and Signal Processing.Umit Guz, Se?bastien Cuendet, Dilek Hakkani-Tu?r, andGokhan Tur.
2007.
Co-training using prosodic andlexical information for sentence segmentation.
InProc.
Interspeech.Umit Guz, Gokhan Tur, Dilek Hakkani-Tu?r, andSe?bastien Cuendet.
2010.
Cascaded model adaptationfor dialog act segmentation and tagging.
ComputerSpeech & Language, 24(2):289?306, April.Jing Huang and Geoffrey Zweig.
2002.
Maximum en-tropy model for punctuation annotation from speech.In Proc.
Int.
Conference on Spoken Language Process-ing, pages 917?920.Minwoo Jeong, Chin-Yew Lin, and Gary G. Lee.
2009.Semi-supervised speech act recognition in emails andforums.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,pages 1250?1259, Singapore, August.
Association forComputational Linguistics.Ji-Hwan Kim and Philip C. Woodland.
2003.
Acombined punctuation generation and speech recog-nition system and its performance enhancement us-ing prosody.
Speech Communication, 41(4):563?577,November.Xin Lei.
2006.
Modeling lexical tones for Man-darin large vocabulary continuous speech recognition.Ph.D.
thesis, Department of Electrical Engineering,University of Washington.Yang Liu, Elizabeth Shriberg, Andreas Stolcke, DustinHillard, Mari Ostendorf, and Mary Harper.
2006.Enriching speech recognition with automatic detec-tion of sentence boundaries and disfluencies.
IEEETrans.
Audio, Speech, and Language Processing,14(5):1526?1540, September.Anna Margolis, Karen Livescu, and Mari Ostendorf.2010.
Domain adaptation with unlabeled data for dia-log act tagging.
In Proceedings of the 2010 Workshopon Domain Adaptation for Natural Language Process-ing, pages 45?52, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Helena Moniz, Fernando Batista, Isabel Trancoso, andAna Mata.
2011.
Analysis of interrogatives in dif-ferent domains.
In Toward Autonomous, Adaptive,and Context-Aware Multimodal Interfaces.
Theoret-ical and Practical Issues, volume 6456 of LectureNotes in Computer Science, chapter 12, pages 134?146.
Springer Berlin / Heidelberg.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.
Amaximum entropy approach to identifying sentenceboundaries.
In Proc.
5th Conf.
on Applied NaturalLanguage Processing, April.Wenzhu Shen, Roger P. Yu, Frank Seide, and Ji Wu.2009.
Automatic punctuation generation for speech.In Proc.
IEEE Workshop on Automatic Speech Recog-nition and Understanding, pages 586?589, December.Elizabeth Shriberg, Rebecca Bates, Andreas Stolcke,Paul Taylor, Daniel Jurafsky, Klaus Ries, Noah Coc-caro, Rachel Martin, Marie Meteer, and Carol Van Ess-Dykema.
1998.
Can prosody aid the automatic classi-fication of dialog acts in conversational speech?
Lan-guage and Speech (Special Double Issue on Prosodyand Conversation), 41(3-4):439?487.Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, JeremyAng, and Hannah Carvey.
2004.
The ICSI meet-ing recorder dialog act (MRDA) corpus.
In Proc.
ofthe 5th SIGdial Workshop on Discourse and Dialogue,pages 97?100.123Kemal So?nmez, Elizabeth Shriberg, Larry Heck, andMitchel Weintraub.
1998.
Modeling dynamicprosodic variation for speaker verification.
In Proc.Int.
Conference on Spoken Language Processing,pages 3189?3192.Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-beth Shriberg, Rebecca Bates, Daniel Jurafsky, PaulTaylor, Rachel Martin, Carol Van Ess-Dykema, andMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational Linguistics, 26:339?373.Dinoj Surendran and Gina-Anne Levow.
2006.
Dialogact tagging with support vector machines and hiddenMarkov models.
In Proc.
Interspeech, pages 1950?1953.Anand Venkataraman, Luciana Ferrer, Andreas Stolcke,and Elizabeth Shriberg.
2003.
Training a prosody-based dialog act tagger from unlabeled data.
In Proc.Int.
Conference on Acoustics, Speech, and Signal Pro-cessing, volume 1, pages 272?275, April.124
