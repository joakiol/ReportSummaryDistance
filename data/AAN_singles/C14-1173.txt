Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1834?1843, Dublin, Ireland, August 23-29 2014.Converting Phrase Structures to Dependency Structures in SanskritPawan GoyalDepartment of CSEIndian Institute of TechnologyKharagpur, India ?
721302pawang@cse.iitkgp.ernet.inAmba KulkarniDepartment of Sanskrit StudiesUniversity of HyderabadHyderabad, India ?
500046apksh@uohyd.ernet.inAbstractTwo annotations schemes for presenting the parsed structures are prevalent viz.
the constituencystructure and the dependency structure.
While the constituency trees mark the relations due topositions, the dependency relations mark the semantic dependencies.
Free word order languageslike Sanskrit pose more problems for constituency parses since the elements within a phrase aredislocated.
In this work, we show how the enriched constituency tree with the information ofdisplacement can help construct the unlabelled dependency tree automatically.1 IntroductionSanskrit has a rich tradition of linguistic analysis with intense discussions and argumentations on variousaspects of language analysis ranging from phonetics (?siks.
?a), grammar (vy?akaran.a), logic (ny?aya), ritualexegesis (karmam?
?m?am.s?a), and literary theory (alam.k?ara?s?astra) which is not only useful for analysingSanskrit but it also has much to offer computational linguistics in these areas.
The series of symposiain Sanskrit Computational Linguistics (Huet et al., 2009; Kulkarni and Huet, 2009), the consortiumproject sponsored by the Technology Development for Indian Languages (TDIL) and the research ofindividual scholars and the collaborations (Goyal et al., 2012) among them resulted into a) developmentof several tools ranging from segmenters (Huet, 2009), morphological analysers (Kulkarni and Shukl,2009), parsers (Goyal et al., 2009; Hellwig, 2009; Kumar, 2012; Kulkarni, 2013) to discourse annotators,b) lexical resources ranging from dictionaries, WordNet (Kulkarni et al., 2010) to Knowledge-Nets (Nair,2011), and c) annotated corpora [http://sanskrit.uohyd.ernet.in/scl].P?an.inian grammar, the oldest dependency grammar, provides a formalism for annotation of thesentences.
While the Sanskrit consortium has annotated a few thousand sentences following thedependency grammar, we also came across a very valuable source of annotation of Sanskrit sentencesfollowing the constituency structure (Gillon, 1996).
The constituency structure was enriched to suitethe requirements of Sanskrit.
This aroused our curiosity to study the equivalence of the two annotationschemes.The importance of dependency structure has been well recognised by several computational linguists(Culotta and Sorensen, 2004; Haghighi et al., 2005; Quirk et al., 2005) in the recent past.
The dependencyformat is preferred over the constituency not only from evaluation point of view (Lin, 1998) but alsobecause of its suitability (Marneffe et al., 2006) for a wide range of NLP tasks such as MachineTranslation (MT), information extraction, question answering etc..
This has upsurged several workson converting a constituency structure into dependency.
The parsers for English now produce thedependency parse as well.
Xia and Palmer discuss three different algorithms to convert dependencystructures to phrase structures for English (Xia and Palmer, 2001).
Magerman gave a set of prioritylists, in the form of a head percolation table to find heads of constituents (Magerman, 1994).
Yamadaand Matsumoto modified these head percolation rules further (Yamada and Matsumoto, 2003).
Theirmethod was reimplemented by Nivre, who also defined certain heuristics to infer the arc labels in thedependency tree produced (Nivre, 2006).
Johansson and Nugues used a richer set of edge labels andThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1834introduced links to handle long-distance phenomena such as wh-movement, topicalization, expletivesand gapping (Johansson and Nugues, 2007).
Their conversion procedure made use of this extendedstructure in Penn Treebank.
De et al.
described a system for generating typed dependency parsed fromthe phrase structure parses (De Marneffe et al., 2006).
(Palmer et al., 2009; Xia et al., 2009; Bhatt et al.,2009) discuss a multi-layered representation framework for Hindi and Urdu, where the information fromsyntactic as well as dependency parse is presented together.In this work, we explore the relationship between enriched constituency structures and dependencystructures for Sanskrit language, with main emphasis on the conversion from constituency to dependencystructures.
This work aims not only at designing an algorithm to convert Treebanks from one type ofrepresentation to the other, but also to judge the adequacy of the enriched constituency structure fromparsing point of view.
This paper has been organized as follows.
Section 2 discusses the history andorigin of this work.
Section 3 describes the background of the constituency and dependency structures,utilized for Sanskrit language.
Section 4 discusses the algorithm we used for converting constituencystructure into dependency structure.
Section 5 describes the results obtained by our approach, with someexamples.
Section 6 concludes this paper with the directions for future work.2 Origin of the workThe dataset we are using in this work has its origin in the remarkable treatise on Sanskrit Syntax by Apte(Apte, 1885) which is the most authentic book on Sanskrit Syntax even after 125 years.
The work wasinitiated in 1986 by Brendan Gillon, then a senior fellow at the American Institute of Indian Studies,at Deccan College, put all the prose exercise sentences from Apte?s Student Guide onto 5 ?
7 cards,assigning a syntactic parse to each sentence, giving each sentence an English translation and annotatingeach sentence for miscellaneous syntactic and semantic facts.
On the basis of these sentences, BrendanGillon published the grammar underlying his syntactic parse of these sentences (Gillon, 1996).In 1991, Brendan Gillon transferred the material from a paper format to an electronic format, makingrevisions.
An example sentence in this dataset is given below:Example{3}Source{1.1.3 (P) <U 4.5.3>} % Apte{7,3}Parse[S [INJ haa ] [ADV katham ][NP1s [NP6 (mahaaraaja<Dasharathasya) ] (dharma<daaraa.h) ][VP 0 [NP1 (priya<sakhii) [NP6 me ] [NP1 Kaushalyaa ] ] ] ]Gloss{Oh, how is it that the legal wife of King Dasharatha is my dearfriend Kaushalyaa}Comment{copula: covert: predicational: NP1s VP }Each example is given a serial number, its source - the corresponding reference in Apte?s book.
Then,its constituency parse is provided in a tree structure.
The Sanskrit text is transliterated into Romanusing the Velthuis notation1.
Finally, the gloss (translation) of the prose is provided along with someobservations regarding syntax in the field ?comment?.
The proper nouns are transliterated following theEnglish convention of capitalisation.
The constituency structure is enriched reflecting the morphologicalinformation such as the case marker.
The underlying constituency structure of the compounds is alsoshown clearly marking the head of the compound.
The requirement that constituency tree be a binary isalso done away with, resulting into a more flat structure than the normal hierarchical phrase structure.In 2004, G?erard Huet re-engineered the document in order to parse it mechanically, and he verified itscorrect syntactic structure after typographical corrections.
He devised an abstract syntax to formalize thisconstituency structure.
In the abstract syntax, the above constituency structure is represented as below:list Tag_tree.syntax =[S1Originally developed in 1991 by Frans Velthuis for use with his devnag Devanagari font, designed for the TeX typesettingsystem.1835[INJ ("haa", 1); ADV [("katham", 2)];NP([Case 1; Role Subject],[NP ([Case 6], [N (Compound (Stem <mahaaraaja>, Stem <Dasharathasya>),3)]);N (Compound (Stem <dharma>, Stem <daaraa.h>), 4)]);VP0[NP([Case 1],[N (Compound (Stem <priya>, Stem <sakhii>), 5);NP ([Case 6], [N (Stem <me>, 6)]);NP ([Case 1], [N (Stem <Kaushalyaa>, 7)])]);NIL 8]]]Each stem is given a unique index.
The syntax, while preserving the original structure of the text, givesadditional structuring with the word numbers, explicit case markers and stems for the compounds.
Whilethese constituent trees preserve much of the tagging related information, they still do not have the genderand number information for the substantives, for instance.
This information can enhance the constituencyrepresentation further.The same set of sentences were also parsed manually by Sheetal Pokar, a research scholar at theUniversity of Hyderabad, showing the dependency structure.
Sheetal followed the annotation guidelinesdeveloped by the Consortium of Institutes working on the Development of Sanskrit ComputationalTools2.
This tagset has a little above 40 tags marking various relations.
The dependency tree for theexample 3, discussed above, is shown in Figure 1.
It is a directed tree with nodes corresponding tothe words in the sentence and edges corresponding to the relation between the head and the modifier.Each node has a number indicating the word index.
A generic relation sambandhah.
(R) is used if therelation does not fall under any of the given tags.
As one may notice, both the constituency as well asthe dependency structures posit a NULL verb ?to be?
asti.
Among the Indian schools dealing with verbalcognition, not all schools accept the insertion of missing copula.
We follow the grammarian school whoaccept this insertion.
(a) (b)Figure 1: Example 3: (a).
Constituency Parse and (b).
Dependency ParseWhile the two structures in Figure 1 mark different kind of information, we notice that the dominancerelation in the constituency structure under each phrase corresponds to the the modifier-modified relationin the dependency tree.
This was the main motivation to develop the converter to convert a phrasestructure into an unlabelled dependency structure.2http://sanskrit.uohyd.ernet.in/scl18363 Dependency and Constituency StructuresVerbal understanding of any utterance requires the knowledge of how words in that utterance are relatedto each other.
There are two major representational frameworks for representing this knowledge as aparse tree viz.
constituency and dependency parse trees.
Constituency trees show how the individual unitsin a sentence are grouped together leading to semantically richer phrases in the constituency structures.The dependency structure, on the other hand, shows how each word is related to other words in thesentence either directly or indirectly.The constituency structure derives from the subject-predicate division of Latin and Greek grammarsthat is based on term logic.
Basic clause structure is understood in terms of a binary division of the clauseinto subject (noun phrase NP) and predicate (verb phrase VP).
These ideas originated with LeonardBloomfield (Bloomfield, 1962) and were further developed by a number of American structuralistlinguists, including Harris (Harris, 1955) and Wells (Wells, 1947).
Though these grammars wereinitially conceived as applying only to phrases, it was shown that such rules could be used for analyzingcompounds as well as derivational morphology for Sanskrit.
Gillon showed that the same extensionworks for classical Sanskrit as well (Gillon, 1995).The dependency analysis dates back to P?an.ini who uses the syntactico-semantic relations called k?arakarelations for the linguistic analysis of a sentence.
In modern times, the seminal work of Tesni`ere(Tesni`ere, 1959) became the basis for the work on dependency grammar.
Meaning-Text Theory (Mel?cuk,1988), Word grammar (Hudson, 1984), Functional Generative Description (Segall et al., 1986) are someof the flavours of the dependency grammar.
A dependency parse is generally modelled as a directed treewith nodes representing the words and edges representing the possible relations between them.
A typeddependency parse also labels the relations.
For every element (word or morph) in a sentence, there is justone node in the syntactic structure.Xia and Palmer (Xia and Palmer, 2001) discuss three different algorithms to convert dependencystructures to phrase structures for English.
They also attempt to clarify the differences in representationalcoverage of the two approaches.
In the first stage, they identify the head of each constituent of thesentence, which is further modified to retrieve the semantic head.
In the second phase, they label eachof the dependency extracted with a grammatical relation using patterns defined over the phrase structuretree.
(Palmer et al., 2009; Xia et al., 2009; Bhatt et al., 2009) discuss a multi-layered representationframework for Hindi and Urdu, where the information from syntactic as well as dependency parse ispresented together.
They first construct a dependency parse and then convert it into the constituencyparse tree using conversion rules.
A conversion rule is a (DS pattern, PS pattern) pair, where DS and PScorrespond to dependency and phrase structure respectively.The constituency parse we are dealing with being enriched with linguistic information pertaining to themorphology of the simple as well as compound words, it was much simpler to convert this structure intoa dependency structure.
In the next section, we will discuss our algorithm for converting a constituencystructure to a dependency structure.4 Conversion from Constituency to Dependency Structure in SanskritThe notion of ?head?
is very important for both the constituency and dependency structures.
In theconstituency structure, the head determines the main properties of the phrase.
Head may have severallevels of projection.
In the dependency structure, on the other hand, the head is linked to its dependents.The core of the algorithm is to identify the head of each phrase in the constituency tree and establish itsrelation with the head of its parent node.
And also to establish the relation between the head with itsdependents.
The head for each XP is the node X within that XP.
Thus the head for an NP is the noun,head for a VP is the verb, and so on.
The head for the S is the head of a VP, in case it is a simple sentence,and head of the VP of the main clause in case it is a complex sentence.
In case of complex sentences,we identified the main clause taking clues from the connectives.
Each relation is named after the XP ofthe modifier.
Our algorithm for finding the head node is implemented on the abstract syntax discussed insection 2 before.
A rough outline of the algorithm is:1)The head of VP is the ROOT node in the dependency tree.1837a) In the case of sentences with sub-ordinate clauses, identify the main clause taking clues from theconnectives.b) Head of a clause is an auxiliary, if present, otherwise the main verb is the head.c) In the case of sentences with quotative markers, the verb of the main clause is the head.
(The later rule is stronger than the previous.
)2) All the XPs within VP are dependent on the ROOT.3) If S is the parent of VP, then all the XPs which are children of S are also dependent on this ROOT.Finding the head for each node was not trivial though, as many of the parses involve dislocated phrases,which were not fully marked.
We had to enrich the constituency trees by incorporating the dislocationinformation, which was provided in comments and was missing from the tree notations.
We used ?!?
and?$?
to indicate the dislocation.
?!?
indicates the position from where a component is dislocated, while?$?
indicates the dislocated component.
An example of a constituency parse, enriched with dislocationinformation is given below.Example{2}Source{1.1.2 (P) <V 3.28; V 3.6.3>} % Apte{7,2}Parse[S [ADV sarvatra ] [NP6 audarikasya $1][VP 0 [NP1 abhyavahaaryam [PRT eva ] ] ][NP1s !1 vi.saya.h ] ]Gloss{In every case, a glutton?s object is only food.
}Comment{copula: covert: predicational: VP NP1s"eva" in predicate NPleft extraposition from NP1s of NP6 within MC, modulo adverbial ADV.
}This constituency tree involves one dislocated phrases.
This information is marked with ?
!1?for theplace from which it is dislocated and with ?$1?
for the phrase that has been dislocated.
This dislocationinformation is used by our algorithm to find the right relata for the dislocated words.
In case of morethan one dislocated phrases, they are numbered sequentially.5 Results and DiscussionsWe implemented our algorithm on a dataset of 232 sentences and matched the output of our algorithmwith the Gold dataset, the dependency graph constructed manually for the sentences by Sheetal.
Figure2 shows the dependency structure for Example 3, produced by our conversion algorithm.Figure 2: Dependency graph constructed by conversion from constituency structureThe conversion algorithm captures the relations between various constituents and produces adependency graph.
Labels of the dependency graph correspond to the intermediate nodes in the phrasestructure.
On comparing the graph in Figure 2 with the graph in Figure 1, we find that most of theoriginal connections were captured.
However, there was a mismatch in the relation between wordskau?saly?a, priyasakh??
and me.
This discrepancy is because of the non-agreement between the annotatorsas to which one is the head.
In case of sentences with apposition, in Sanskrit, the two annotators havedifference of opinion as to which among the two is the head.
This resulted in the mismatch between thetwo graphs.
The relations in this graph are labelled by the dominating XP.1838P?an.ini?s grammar provides rules for assigning case markers given the syntactico-semantic relationbetween the relata.
Inverting these rules it should be possible to get the relation labels.
These relationlabels, however, will not be obtained deterministically.
The non-determinism will lead to multiplelabelled dependency structures.
Hence we could not assign the labels from the tagset, and resortedto the names of the phrases which the word belongs to.Below we give an example where we found an exact match between the manual dependency graphand the dependency graph, produced by our conversion algorithm, using the constituency parse of thesentence.Example{29}[S [VP [NP7 tatra ] [CNJ ca ][NP5 [NP6 [AP6 (((nikhila<(dhara.nii<tala))<parya.tana)<khinnasya) ](nija<balasya) ](vizraama<heto.h) ][NP2 [AP2 katipayaan ] divasaan ]ati.s.that ] ]Gloss{And he remained there for a few days in order to rest his armyexhausted from roaming the entire surface of the earth.
}(a) (b)Figure 3: Example 29: (a).
Dependency graph constructed manually and (b).
Dependency graphconstructed by conversion from constituency structureOut of 232 cases, we found 97 such cases with exact match.
For the rest of the cases,1) In 40 cases, number of words in dependency and phrase-converted graph are different.
For example,the words such as kad?acit ?probably?, yadyapi ?even if?, tath?api ?even then?, athav?a ?or?
etc.
were treatedin one structure as a single word while in the other as two words.
These words at morphological levelconsist of two morphemes which have independent existence.
So it was natural to treat these as twowords, in the constituency trees.
However, at the semantic level, these two words indicate a singlemeaning which at times is non-compositional.
The annotator of dependency graph has treated these as asingle word.2) In 95 cases, one or more relations do not match.
These were due to various reasons such as1.
differences in the treatment and identification of adjectives,2.
disagreement in the attachment, and3.
cases of ellipsis, null head, and cases where the treatment of conjunct ?ca?
(and) differs.1839These differences are very much important from linguistic analysis point of view.
However due to spaceconstraint we illustrate here only two cases where the treatment of the two annotators differ.Example{72}[S [VOC sakhi [VOC Vaasanti ] ][VP 0 [NP4 du.hkhaaya [PRT eva ] [NP6 su-h.rdaam ] ] ][NP1s [ADV idaaniim ] [NP6 Raamasya ] darshanam ] ]Gloss{Oh my friend Vaasanti, seeing Raama now leads only tothe unhappiness of his friends.
}(a) (b)Figure 4: Example 72: (a).
Dependency graph constructed manually and (b).
Dependency graphconstructed by conversion from constituency structureIn this example, there are two places where the annotators disagree.a) The attachment of the word id?an?
?m (now).b) The decision of head in case of vocative with a modifier.The null copula (VP 0) in the constituency structure is replaced by the Sanskrit verbal form astiuniformly.
The annotator of dependency graph has provided a more appropriate verb bhavis.yati.However, we ignore this difference.Let us look at another example, where the ambiguity in morphological analysis has led to the ambiguityin the readings resulting in the disagreement in the parse.Example{46}[S [NP1s aarya.h ][VP daapayatu [NP2 [NP6 me ][NP4 (Vaisha.mpaayana<aanayanaaya) ](gamana<abhyanuj?naam) ][NP3 taatena ] ] ]Gloss{May you, sir, make my father give me permission to go to bringVaisha.mpaayana.
}In this example, the pronominal form me is ambiguous between two readings.
It can be either a genitiveor a dative of the first person pronoun asmad ?I?.
The sub-ordinate clause is analysed by Gillon as ?thepermission for going to bring Vai?samp?ayan by me?.
In Sanskrit the first person pronoun in such casestakes genitive case marker.
Sheetal on the other hand has analysed it as ?the permission to me for going tobring Vai?samp?ayan?, where me is analysed with dative case.
It is clear that ?to bring Vai?samp?ayan?
is thepurpose for going.
But since ?permission for going?
is a compound in Sanskrit3, and the ?permission?being the head of this compound, Sheetal avoided linking ?to bring Vai?samp?ayana?
with ?permissionto go?
as it results into an ?asamartha sam?asa (incompatible compound formation, where the externalmodifier connects the modifier component of a compound and not the head).
This has resulted into thedifferences in annotation.
Such compounds are not rare.
Thus, in order to provide a correct parse, incase of dependency structure, it is necessary to show the internal structure of the compounds as well,3In Sanskrit a compound is always written as a single word without any space in between.1840(a) (b)Figure 5: Example 46: (a).
Dependency graph constructed manually and (b).
Dependency graphconstructed by conversion from constituency structurerather than treating a compound unanalysed.
This will allow one to connect the elements to the part of acompound other than the head.
Similar treatment is necessary in the phrase structure annotation as well.We end with example 2 from section 4, where the dislocation information in the constituency treehelps in retrieving the correct dependency structure.
In this example, even though the word audarikasyahas been displaced, the displacement information in the parse tree positions it at the correct place in thedependency tree constructed from the constituency structure.
(a) (b)Figure 6: Example 2: (a).
Constituency structure and (b).
Dependency graph constructed by conversionfrom constituency structure6 Conclusions and Future WorkThis work focussed mainly on conversion from constituency to dependency structure.
The sentences inour dataset are chosen from (Apte, 1885), which is an authentic book for higher learning of Sanskrit,covering a wide range of grammatical constructions.
The tool was tested on a dataset of 232 sentencesand the initial results were encouraging.
Specifically, most of the cases of mismatch were linguisticissues and need further discussion.
The phrase labels indicating the case labels is an important extensionof the constituency trees to accommodate morphologically rich languages.
The enriched constituencystructure has an advantage of recording the word order, and at the same time marking the dislocationinformation.
In this work, we have shown that such enriched constituency tree can help construct theunlabelled dependency tree automatically.
Further, one may also try inferring the dependency relationnames, and use statistical parsing to resolve non-determinism favouring popular usages.1841Another interesting aspect would be to try the other way conversion, that is, from dependency tophrase structure.
The main challenge for this conversion is to find out the projection table correspondingto each lexical item.
This work will also be a first step towards an abstract syntax, which can inheritthe properties of both the constituency and dependency structures.
If so, this would be an alternativeformalism for tagging the Sanskrit corpus.AcknowledgementsThe authors would like to acknowledge the discussions with G?erard Huet, INRIA Paris Rocquencourt,towards enriching the abstract syntax.
The work was also supported by Emilie Aussant and SylvieArchaimbault, laboratoire HTL, Universit?e Paris Diderot.ReferencesV?aman Shivar?am Apte.
1885.
The Student?s Guide to Sanskrit Composition.
A Treatise on Sanskrit Syntax for Useof Schools and Colleges.
Lokasamgraha Press, Poona, India.Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer, Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009.A multi-representational and multi-layered treebank for hindi/urdu.
In Proceedings of the Third LinguisticAnnotation Workshop, pages 186?189.
Association for Computational Linguistics.Leonard Bloomfield.
1962.
Language.
New York: Holt.A.
Culotta and J. Sorensen.
2004.
Dependency tree kernels for relation extraction.
In 42nd Annual Meeting of theAssociation for Computational Linguistics (ACL), pages 423?429, Barcelona, Spain.Monali Das and Amba Kulkarni.
2013.
Discourse level tagger for mah?abh?as.ya - a Sanskrit commentary onp?an.ini?s grammar.
In Proceedings of the 10th International Conference on NLP, Delhi, India.Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al.
2006.
Generating typeddependency parses from phrase structure parses.
In Proceedings of LREC, volume 6, pages 449?454.Brendan S. Gillon.
1995.
Autonomy of word formation: evidence from Classical Sanskrit.
Indian Linguistics, 56(1-4), pages 15?52.Brendan S Gillon.
1996.
Word order in classical sanskrit.
Indian Linguistics, 57(1-4):1?35.Brendan S. Gillon.
2009.
Tagging classical Sanskrit compounds.
In Amba Kulkarni and G?erard Huet, editors,Sanskrit Computational Linguistics 3, pages 98?105.
Springer-Verlag LNAI 5406.Pawan Goyal, Vipul Arora, and Laxmidhar Behera.
2009.
Analysis of Sanskrit text: Parsing and semanticrelations.
In G?erard Huet, Amba Kulkarni, and Peter Scharf, editors, Sanskrit Computational Linguistics 1& 2, pages 200?218.
Springer-Verlag LNAI 5402.Pawan Goyal, G?erard Huet, Amba Kulkarni, Peter Scharf, and Ralph Bunker.
2012.
A distributed platform forsanskrit processing.
In Proceedings of 24th COLING, Mumbai, India.A.D.
Haghighi, A.Y.
Ng, and C.D.
Manning.
2005.
Robust textual inference via graph matching.
InHuman Language Technology Conference (HLT) and Conference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 387?394, Vancouver, Canada.Zellig S Harris.
1955.
From phoneme to morpheme.
Language, 31(2):190?222.Oliver Hellwig.
2009.
Extracting dependency trees from Sanskrit texts.
In Amba Kulkarni and G?erard Huet,editors, Sanskrit Computational Linguistics 3, pages 106?115.
Springer-Verlag LNAI 5406.R.
Hudson.
1984.
Word Grammar.
Basil Blackwell, Oxford.G?erard Huet, Amba Kulkarni, and Peter Scharf, editors.
2009.
Sanskrit Computational Linguistics 1 & 2.Springer-Verlag LNAI 5402.G?erard Huet.
2009.
Formal structure of Sanskrit text: Requirements analysis for a mechanical Sanskritprocessor.
In G?erard Huet, Amba Kulkarni, and Peter Scharf, editors, Sanskrit Computational Linguistics 1& 2.
Springer-Verlag LNAI 5402.1842Richard Johansson and Pierre Nugues.
2007.
Extended constituent-to-dependency conversion for english.
InProceedings of the 16th Nordic Conference on Computational Linguistics (NODALIDA), pages 105?112.Amba Kulkarni and Monali Das.
2012.
Discourse analysis of sanskrit texts.
In Proceedings of the workshop onAdvances in Discourse Analysis and its Computational Aspects, 24th COLING, Mumbai, India.Amba Kulkarni and G?erard Huet, editors.
2009.
Sanskrit Computational Linguistics 3.
Springer-Verlag LNAI5406.Amba Kulkarni and K. V. Ramakrishnamacharyulu.
2013.
Parsing Sanskrit texts: Some relation specific issues.
InMalhar Kulkarni, editor, Proceedings of the 5th International Sanskrit Computational Linguistics Symposium.D.
K. Printworld(P) Ltd.Amba Kulkarni and Devanand Shukl.
2009.
Sanskrit morphological analyser: Some issues.
Indian Linguistics,70(1-4):169?177.Malhar Kulkarni, Chaitali Dangarikar, Irawati Kulkarni, Abhishek Nanda, and Pushpak Bhattacharyya.
2010.Introducing sanskrit wordnet.
In Christiane Felbaum Pushpak Bhattacharyya and Piek Vossen, editors,Principles, Construction and Application of Multilingual Wordnets, Proceedings of the Global WordnetConference, 2010.
Narosa Publishing House, New Delhi.Amba Kulkarni.
2013.
A deterministic dependency parser with dynamic programming for Sanskrit.
InProceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages157?166, Prague, Czech Republic, August.
Charles University in Prague, Matfyzpress, Prague, Czech Republic.Anil Kumar.
2012.
An automatic Sanskrit Compound Processing.
Ph.D. thesis, University of Hyderabad,Hyderabad.Dekang Lin.
1998.
Dependency-based evaluation of minipar.
In Workshop on the evaluation of Parsing Systems,Granada, Spain.David Mitchell Magerman.
1994.
Natural Language Parsing As Statistical Pattern Recognition.
Ph.D. thesis,Stanford, CA, USA.
UMI Order No.
GAX94-22102.Marie-Catherine De Marneffe, Bill MacCartney, and Christopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
In Proceedings of LREC-06.I.
Mel?cuk.
1988.
Dependency Syntax: Theory and Practice.
State University of New York Press, Albany.Sivaja Nair.
2011.
The Knowledge Structure in Amarako?sa.
Ph.D. thesis, University of Hyderabad, Hyderabad.Joakim Nivre.
2006.
Inductive dependency parsing.
Springer.Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan, Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009.Hindi syntax: Annotating dependency, lexical predicate-argument structure, and phrase structure.
In The 7thInternational Conference on Natural Language Processing, pages 14?17.C.
Quirk, A. Menezes, and C. Cherry.
2005.
Dependency treelet translation: Syntactically informed phrasal smt.In 43rd Annual Meeting of the Association for, Computational Linguistics (ACL), pages 271?279, Ann Arbor,USA.Peter Scharf and Malcolm Hyman.
2009.
Linguistic Issues in Encoding Sanskrit.
Motilal Banarsidass, Delhi.P.
Segall, E. Hajiov, and J. Panevov.
1986.
The Meaning of the Sentence in its Semantic and Pragmatic Aspects.Springer, Heidelberg.L Tesni`ere.
1959.?El?ements de syntaxe structurale.
Klinksieck, Paris.Rulon S Wells.
1947.
Immediate constituents.
Language, 23(2):81?117.Fei Xia and Martha Palmer.
2001.
Converting dependency structures to phrase structures.
In Proceedingsof the first international conference on Human language technology research, pages 1?5.
Association forComputational Linguistics.Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer, and Dipti Misra Sharma.
2009.
Towards amulti-representational treebank.
In The 7th International Workshop on Treebanks and Linguistic Theories.Groningen, Netherlands.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statistical dependency analysis with support vector machines.Proceedings of IWPT, 3.1843
