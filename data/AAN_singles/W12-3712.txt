Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 79?83,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsCross-discourse Development of Supervised Sentiment Analysis in theClinical DomainPhillip SmithSchool of Computer ScienceUniversity of Birminghampxs697@cs.bham.ac.ukMark LeeSchool of Computer ScienceUniversity of BirminghamM.G.Lee@cs.bham.ac.ukAbstractCurrent approaches to sentiment analysis as-sume that the sole discourse function ofsentiment-bearing texts is expressivity.
How-ever, the persuasive discourse function alsoutilises expressive language.
In this work,we present the results of training supervisedclassifiers on a new corpus of clinical textsthat contain documents with an expressive dis-course function, and we test the learned mod-els on a subset of the same corpus containingpersuasive texts.
The results of this indicatethat despite the difference in discourse func-tion, the learned models perform favourably.1 IntroductionExamining the role that discourse function holds isa critical part of an in-depth analysis into the capa-bilities of supervised sentiment classification tech-niques.
However, it is a field that has not been com-prehensively examined within the domain of sen-timent analysis due to the lack of suitable cross-discourse corpora to train and test various machinelearning methods upon.In order to carry out such an investigation, thisstudy will focus on the relationship between senti-ment classification and two types of discourse func-tion: Expressive and Persuasive.
The expressivefunction denotes the feelings or attitudes of the au-thor of a document.
This is demonstrated in the fol-lowing examples:1.
?I didn?t like the attitude of the nursing staff.?2.
?The doctors treated me with such care.
?Intuitively, the associated polarity of each exam-ple is trivial to determine in these explicit examples.However, expressive statements do not operate inisolation of other respective discourse functions.
AsBiber (1988) notes, a persuasive statement incorpo-rates elements of the expressive function in order toadvise an external party of a proposed action thatshould be taken.
The following example shows howpersuasive statements make use of expressive func-tions:1.
?The clumsy nurse who wrongly diagnosed meshould be fired.
?The role of a persuasive statement is to incitean action in the target, dependent upon the inten-tion that the author communicates.
By using plain,sentiment-neutral language, the reader may misin-terpret why the request for action is being given, andin the worst-case scenario not carry it out.
Throughthe incorporation of expressive language, the weightof the persuasive statement is increased.
This en-ables the speaker to emphasise the underlying senti-ment of their statement, thereby increasing the like-lihood of the intended action being undertaken, andtheir goals being accomplished.
In the above ex-ample, the intention communicated by the authoris the firing of the nurse.
This in itself holds neg-ative connotations, but through the use of the word?clumsy?, the negative sentiment of the statement be-comes clearer to understand.The inclusion of expressive aspects in the lan-guage of the persuasive discourse function, enablesus to identify the sentiment of a persuasive com-ment.
As there is this cross-over in the language ofthe two discourse functions, we can hypothesise that79if we train a supervised classifier on an expressivecorpus, a learned model will be created that whenapplied to a corpus of persuasive documents, willclassify these texts to an adequate standard.As the corpus that we developed is in the clin-ical domain, it is worth noting the important rolethat sentiment analysis can play for health practi-tioners, which unfortunately has not received a greatdeal of attention.
In assessing the effectiveness oftreatments given by the health service for a conditionwhich is curable, the results themselves indicate theeffectiveness of such a process.
However, for pallia-tive treatments which merely alleviate the symptomsof an illness or relieve pain, it is vital to discover theextent to which these are effective.
Feedback hasprogressed from the filling in of paper forms to theability to give feedback through web pages and mo-bile phones.
Text is stored in a highly accessibleway, and is now able to be efficiently processed bysentiment classification algorithms to determine theopinions that patients are expressing.
This in turnshould enable health services to make informed de-cisions about the palliative care which they provide.2 Patient Feedback CorpusNHS Choices1 is a website run by the NationalHealth Service (NHS), which acts as an extensiveknowledge base for any health-related queries.
Thiswebsite not only provides comprehensive articlesabout various ailments, but also gives the users ofthe site the option to rate and comment on the ser-vices that are provided to them at hospitals and GPsurgeries.
This user feedback provides an excellentbasis for the sentiment classification experiments ofthis work.The reviews that are submitted are typically pro-vided by a patient or close relative who has experi-enced the healthcare system within a hospital.
Whensubmitting feedback, the user is asked to split theirfeedback into various fields, as opposed to submit-ting a single documents detailing all the commentsof the user.
During corpus compilation, each com-ment was extracted verbatim, so spelling mistakesremain in the developed corpus.
All punctuationalso remains in order to enable future experiments tobe carried out on either the sentence or phrase level1http://nhs.ukCorpus D W Davglength VExpressivePositive 1152 75052 65.15 6107Negative 1108 76062 68.65 6791PersuasivePositive 768 46642 60.73 4679Negative 864 113632 131.52 7943Table 1: Persuasive & expressive corpus statistics.within each comment.In developing the corpus, we leverage the fact thatthe data was separated into subfields, as opposedto one long review, where the all data is mergedinto a single document.
We extracted commentswhich came under three categories in the NHS Pa-tient Feedback dataset: Likes, Dislikes and Advice.The Likes were assumed to express positive senti-ment and highlight elements of the health servicethat patients appreciated.
Conversely, the documentsgiven under the Dislikes header were assumed toconvey a negative sentiment.
These two subsetsmake up the Expressive subset of the compiled cor-pus.
The Advice documents did not have an initialsentiment associated with them, so each commentwas labelled by two independent annotators at thedocument level as being either a positive or nega-tive comment.
These Advice comments contributedto the Persuasive subcorpus.
In compiling the per-suasive document sets, we automatically discardedthose comments that contained the term ?N/A ?
orany of its derivative forms.3 MethodThe aim in this work was to examine the effect oftraining a supervised classifier on a corpus whosediscourse function differs to that of the trainingset.
We experimented with three standard super-vised machine learning algorithms: standard Na?
?veBayes (NB), multinomial Na?
?ve Bayes (MN NB)and Support Vector Machines (SVM) classification.Each has proven to be effective in previous senti-ment analysis studies (Pang et al , 2002), so asthis experiment is rooted in sentiment classification,these methods were also assumed to perform well inthis cross-discourse setting.For the cross-discourse sentiment classification80experiments, two variants of the Na?
?ve Bayes algo-rithm are used.
The difference between the stan-dard NB and MN NB is the way in which the fea-tures for classification, the words, are modelled.
Inthe standard NB learning method, a binary presenceapproached is taken in modelling the words of thetraining documents.
This differs to the MN NB clas-sifier, which takes into account term frequency whenmodelling the documents.
Each has proven to be ahigh performing classifier across various sentimentanalysis domains, but no distinction has been givenas to which is the preferable method to use.
There-fore in this paper, both were implemented.In the literature, results from the use of SVMs inclassification based experiments have outperformedother algorithms (Joachims, 1998; Pang et al ,2002).
For these cross-discourse experiments we usethe Sequential Minimal Optimization training algo-rithm (Platt, 1998), in order to achieve the maximalhyperplane, and maximise the potential of the cre-ated classifier.
Traditionally SVMs have performedwell in text classification, but across discourse do-mains the results of such classification has not beenexamined.Each document in the corpus was modelled asa bag of words.
Features used within this repre-sentation were unigrams, bigrams and bigrams aug-mented with part-of-speech information.
Due tothis, and observing the results of preliminary experi-mentation that included rare features, it was decidedto remove any feature that did not occur more than5 times throughout the training set.
A stopword listand stemmer were also used.Each supervised classification technique was thentrained using a random sample of 1,100 documentsfrom both the positive and negative subsections ofthe expressive corpus.
Following this we tested theclassifiers on a set of 1,500 randomly selected per-suasive documents, using 750 documents from eachof the positive and negative subcorpora.The results of cross-validation (Table 2) sug-gested that unigram features may outperform bothbigram and part-of-speech augmented bigrams forall learning methods.
In particular, the accuracyresults produced by the NB algorithm surpassedthe results of other classifiers in the tenfold cross-validation.
This suggests that within a single dis-course domain, presence based features are prefer-Features NB Multinomial NB SVMUnigrams 79.65 78.14 76.11Bigrams 57.79 60.84 63.36Bigrams + POS 74.25 75.71 72.83Table 2: Average tenfold cross-validation accuracies ononly the expressive corpus.
Boldface: best performancefor a given classifier.able to considering the frequency of a term whengenerating a machine learning model.4 ResultsTable 3 shows the classification accuracies achievedin all experiments.
For each classifier, with each fea-ture set, if we take the most basic baseline for thetwo-class (positive/negative) problem to be the ran-dom baseline of 50% classification accuracy, thenthis is clearly exceeded.
However if we take the re-sults of the tenfold cross-validation as a baseline foreach classifier in the experiments, then only the re-sults given by the MN NB classifier with unigramand bigram features are able to surpass this.The results given from the NB and the MN NBclassifier imply that using frequency based fea-tures are preferable to using presence based featureswhen performing cross-discourse sentiment classi-fication.
The MN NB is one of the few classifierstested that exceeds the results of the cross-validatedmodel.
These results support experiments carriedout for topic based classification using Bayesianclassifiers by McCallum and Nigam (1998), but dif-fers from sentiment classification results from Panget al (2002) that suggest that term-based modelsperform better than the frequency-based alternative.This also differs to the results that were returnedduring the cross-validation of the classifiers, wherepresence based features produced the greatest clas-sification accuracy.In our tests, the feature set which yielded the high-est degree of classification accuracy across all clas-sifiers is the unigram bag of words model.
Tan etal.
(2002) suggest that using bigrams enhances textclassification, but as sentiment classification goesbeyond this task, the assumption does not hold, asthe results here show.
The difference in discoursefunction could also contribute to bigrams yielding81Positive NegativeAccuracy Precision Recall F1 Precision Recall F1NB Uni 76.07 78.29 72.13 75.09 74.17 80.00 76.97NB Bi 58.93 55.19 94.93 69.80 81.90 22.93 35.83NB Bi + POS 65.00 71.84 49.33 58.50 61.42 80.67 69.74MN NB Uni 83.53 82.04 85.87 83.91 85.17 81.20 83.14MN NB Bi 57.00 63.78 32.40 42.97 54.69 81.60 65.49MN NB Bi + POS 69.97 69.59 69.87 69.73 69.75 69.47 69.61SVM Uni 69.00 68.43 70.53 69.47 69.60 67.47 68.52SVM Bi 55.40 60.98 30.00 40.21 53.58 80.80 64.43SVM Bi + POS 63.27 63.11 63.87 63.49 63.43 62.67 63.04Table 3: Results of experimentation, with the expressive corpus as the training set, and the persuasive corpus as thetest set.
Boldface indicates the best performance for each metric.the lowest accuracy results.
Bigrams model quitespecific language patterns, but as the expressive andpersuasive language differs in structure and content,then the patterns learnt in one domain do not accu-rately map to another domain.
Bigrams contributethe least to sentiment classification in this cross-discourse scenario, and only when they are aug-mented with part of speech information does the ac-curacy sufficiently pass the random baseline.
How-ever for good recall, using bigram based featuresproduces excellent results, at the sacrifice of ade-quate precision, which suggests that bigram mod-els overfit when they are used as features in such alearned model.The SVM classifier with a variety of features doesnot perform as well as the multinomial Na?
?ve Bayesclassifier.
Joachims (1998) suggests that for textcategorization, the SVM algorithm regularly outper-forms other classifiers, but unfortunately the out-come of our experiments do not correlate with theseresults.
This suggests that SVMs struggle with textclassification when the discourse function betweenthe training and test domains differ.5 DiscussionThe results produced through training supervisedmachine learning methods on an expressive corpus,and testing on a corpus which contains documentswith a persuasive discourse function indicate thatcross-discourse sentiment classification is feasible.The best performance occurred when the classi-fier took frequency based features into account, asopposed to solely presence based features.
The rea-soning for this could be attributed to the way that pa-tients were asked to submit their feedback.
Insteadof asking a patient to submit a single comment ontheir experience with the health service, they wereasked to submit three distinct comments on whatthey liked, disliked and any advice that they had.This gave the user the opportunity to separate theirsentiments, and clearly communicate their thoughts.It is of interest to note that the cross-discourse ac-curacy should surpass the cross-validation accuracyon the training set.
This was not to be expected, dueto the differences in discourse function, and there-fore features used.
However, where just the presenceof a particular word may have made the differencein a single domain, across domains, taking into ac-count the frequency of a word in the learned modelis effective in correctly classifying a comment byits sentiment.
Unigram features outperform both thebigram and bigrams augmented with part-of-speechfeatures in our experiments.
By using single tokensas features, each word is taken out of the contextthat its neighbours provide.
In doing so the languagecontributing to the relative sentiment is generalisedenough to form a robust model which can then beapplied across discourse domains.6 Related WorkA number of studies (Cambria at al.
, 2011; Xia etal.
, 2009) have used patient feedback as the domainfor their sentiment classification experiments.
How-ever our work differs to these studies as we consider82the effect that cross-discourse evaluation has on theclassification outcome.
Other work that has consid-ered different discourse functions in sentiment anal-ysis, have experimented on detecting arguments (So-masundaran et al , 2007) and the stance of politicaldebates (Thomas et al , 2006).Machine learning approaches to text classificationhave typically performed well when using a Sup-port Vector Machine (Joachims, 1998) classifier ora Na?
?ve Bayes (McCallum and Nigam, 1998) basedclassifier.
Pang et al (2002) applied these classi-fiers to the movie review domain, which producedgood results.
However the difference in domain,and singularity of discourse function differentiatesthe scope of this work from theirs.7 Conclusion & Future WorkIn this study we focused on the cross-discoursedevelopment of supervised machine learning algo-rithms in the clinical domain, that trained and testedacross the expressive and persuasive discourse func-tions.
We demonstrated that despite the differencesin function of a corpus of patient feedback, thegreatest classification accuracy was achieved whenconsidering word frequency in the features of thelearned model.This study centred on the expressive and persua-sive discourse functions, but it would be interestingto examine other such functions that convey a sen-timent, such as argumentation.
Another interestingavenue of investigation for this work would be to ex-plore the lexical semantics of the different discoursefunctions, that could be used in sentiment classifica-tion, and factor this into the evaluation of the overallsentiment of persuasive documents within a corpus.ReferencesDouglas Biber.
1988.
Variation Across Speech andWriting.
Cambridge University Press.John Blitzer, Mark Dredze and Fernando Pereira.
2007.Biographies, Bollywood, Boom-boxes, and Blenders:Domain Adaptation for Sentiment Classification.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pp.
440?447.Erik Cambria, Amir Hussain and Chris Eckl.
2011.Bridging the Gap between Structured and Unstruc-tured Health-Care Data through Semantics andSentics.
In Proceedings of ACM WebSci, Koblenz.Andrea Esuli and Fabrizio Sebastiani.
2006.
Senti-WordNet: A Publicly Available Lexical Resourcefor Opinion Mining In Proceedings of LanguageResources and Evaluation (LREC), pp 417?422.Thorsten Joachims.
1998.
Text categorization withsupport vector machines: learning with many relevantfeatures.
In Proceedings of ECML-98, 10th EuropeanConference on Machine Learning, pp.
137?142.Andrew McCallum and Kamal Nigam.
1998.
AComparison of Event Models for Naive Bayes TextClassification.
In Proceedings of the AAAI/ICML-98Workshop on Learning for Text Categorization, pp.41?48.Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment Classification usingMachine Learning Techniques.
In Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pp.
79?87.John Platt.
Sequential Minimal Optimization: A FastAlgorithm for Training Support Vector Machines.In Advances in Kernel Methods - Support VectorLearning.Swapna Somasudaran and Josef Ruppenhofer and JanyceWiebe.
2007.
Detecting Arguing and Sentiment inMeetings.
In Proceedings of the SIGdial Workshop onDiscourse and Dialogue, pp.26?34.Chade-Meng Tan, Yuan-Fang Wang and Chan-Do Lee.2002.
The use of bigrams to enhance text catego-rization.
In Information Processing & Management,38(4) pp.
529?546.Matt Thomas, Bo Pang and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition fromCongressional floor-debate transcripts.
In Proceedingof the 2006 Conference on Emperical Methods inNatural Language Processing (EMNLP), pp.327?335.Lei Xia, Anna Lisa Gentile, James Munro and Jose?
Iria.2009.
Improving Patient Opinion Mining throughMulti-step Classification.
In Proceedings of the 12thInternational Conference on Text, Speech and Dia-logue (TSD?09), pp.
70?76.83
