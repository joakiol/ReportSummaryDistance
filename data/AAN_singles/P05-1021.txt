Proceedings of the 43rd Annual Meeting of the ACL, pages 165?172,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsImproving Pronoun Resolution Using Statistics-BasedSemantic Compatibility InformationXiaofeng Yang??
Jian Su?
Chew Lim Tan?
?Institute for Infocomm Research21 Heng Mui Keng Terrace,Singapore, 119613{xiaofengy,sujian}@i2r.a-star.edu.sg?
Department of Computer ScienceNational University of Singapore,Singapore, 117543{yangxiao,tancl}@comp.nus.edu.sgAbstractIn this paper we focus on how to improvepronoun resolution using the statistics-based semantic compatibility information.We investigate two unexplored issues thatinfluence the effectiveness of such in-formation: statistics source and learningframework.
Specifically, we for the firsttime propose to utilize the web and thetwin-candidate model, in addition to theprevious combination of the corpus andthe single-candidate model, to computeand apply the semantic information.
Ourstudy shows that the semantic compatibil-ity obtained from the web can be effec-tively incorporated in the twin-candidatelearning model and significantly improvethe resolution of neutral pronouns.1 IntroductionSemantic compatibility is an important factor forpronoun resolution.
Since pronouns, especially neu-tral pronouns, carry little semantics of their own,the compatibility between an anaphor and its an-tecedent candidate is commonly evaluated by ex-amining the relationships between the candidate andthe anaphor?s context, based on the statistics that thecorresponding predicate-argument tuples occur in aparticular large corpus.
Consider the example givenin the work of Dagan and Itai (1990):(1) They know full well that companies held taxmoney aside for collection later on the basisthat the government said it1 was going to col-lect it2.For anaphor it1, the candidate government shouldhave higher semantic compatibility than money be-cause government collect is supposed to occur morefrequently than money collect in a large corpus.
Asimilar pattern could also be observed for it2.So far, the corpus-based semantic knowledge hasbeen successfully employed in several anaphora res-olution systems.
Dagan and Itai (1990) proposeda heuristics-based approach to pronoun resolu-tion.
It determined the preference of candidatesbased on predicate-argument frequencies.
Recently,Bean and Riloff (2004) presented an unsupervisedapproach to coreference resolution, which minedthe co-referring NP pairs with similar predicate-arguments from a large corpus using a bootstrappingmethod.However, the utility of the corpus-based se-mantics for pronoun resolution is often argued.Kehler et al (2004), for example, explored theusage of the corpus-based statistics in supervisedlearning based systems, and found that such infor-mation did not produce apparent improvement forthe overall pronoun resolution.
Indeed, existinglearning-based approaches to anaphor resolutionhave performed reasonably well using limitedand shallow knowledge (e.g., Mitkov (1998),Soon et al (2001), Strube and Muller (2003)).Could the relatively noisy semantic knowledge giveus further system improvement?In this paper we focus on improving pronominalanaphora resolution using automatically computedsemantic compatibility information.
We propose toenhance the utility of the statistics-based knowledgefrom two aspects:Statistics source.
Corpus-based knowledge usu-ally suffers from data sparseness problem.
That is,many predicate-argument tuples would be unseeneven in a large corpus.
A possible solution is the165web.
It is believed that the size of the web is thou-sands of times larger than normal large corpora, andthe counts obtained from the web are highly corre-lated with the counts from large balanced corporafor predicate-argument bi-grams (Keller and Lapata,2003).
So far the web has been utilized in nominalanaphora resolution (Modjeska et al, 2003; Poesioet al, 2004) to determine the semantic relation be-tween an anaphor and candidate pair.
However, toour knowledge, using the web to help pronoun reso-lution still remains unexplored.Learning framework.
Commonly, the predicate-argument statistics is incorporated into anaphora res-olution systems as a feature.
What kind of learn-ing framework is suitable for this feature?
Previousapproaches to anaphora resolution adopt the single-candidate model, in which the resolution is done onan anaphor and one candidate at a time (Soon et al,2001; Ng and Cardie, 2002).
However, as the pur-pose of the predicate-argument statistics is to eval-uate the preference of the candidates in semantics,it is possible that the statistics-based semantic fea-ture could be more effectively applied in the twin-candidate (Yang et al, 2003) that focusses on thepreference relationships among candidates.In our work we explore the acquisition of the se-mantic compatibility information from the corpusand the web, and the incorporation of such semanticinformation in the single-candidate model and thetwin-candidate model.
We systematically evaluatethe combinations of different statistics sources andlearning frameworks in terms of their effectivenessin helping the resolution.
Results on the MUC dataset show that for neutral pronoun resolution in whichan anaphor has no specific semantic category, theweb-based semantic information would be the mosteffective when applied in the twin-candidate model:Not only could such a system significantly improvethe baseline without the semantic feature, it also out-performs the system with the combination of the cor-pus and the single-candidate model (by 11.5% suc-cess).The rest of this paper is organized as follows.
Sec-tion 2 describes the acquisition of the semantic com-patibility information from the corpus and the web.Section 3 discusses the application of the statisticsin the single-candidate and twin-candidate learningmodels.
Section 4 gives the experimental results,and finally, Section 5 gives the conclusion.2 Computing the Statistics-based SemanticCompatibilityIn this section, we introduce in detail how to com-pute the semantic compatibility, using the predicate-argument statistics obtained from the corpus or theweb.2.1 Corpus-Based Semantic CompatibilityThree relationships, possessive-noun, subject-verband verb-object, are considered in our work.
Be-fore resolution a large corpus is prepared.
Doc-uments in the corpus are processed by a shallowparser that could generate predicate-argument tuplesof the above three relationships1.To reduce data sparseness, the following steps areapplied in each resulting tuple, automatically:?
Only the nominal or verbal heads are retained.?
Each Named-Entity (NE) is replaced by a com-mon noun which corresponds to the seman-tic category of the NE (e.g.
?IBM?
?
?com-pany?)
2.?
All words are changed to their base morpho-logic forms (e.g.
?companies ?
company?
).During resolution, for an encountered anaphor,each of its antecedent candidates is substituted withthe anaphor .
According to the role and type of theanaphor in its context, a predicate-argument tuple isextracted and the above three steps for data-sparsereduction are applied.
Consider the sentence (1),for example.
The anaphors ?it1?
and ?it2?
indicatea subject verb and verb object relationship, respec-tively.
Thus, the predicate-argument tuples for thetwo candidates ?government?
and ?money?
wouldbe (collect (subject government)) and (collect (sub-ject money)) for ?it1?, and (collect (object govern-ment)) and (collect (object money)) for ?it2?.Each extracted tuple is searched in the preparedtuples set of the corpus, and the times the tuple oc-curs are calculated.
For each candidate, its semantic1The possessive-noun relationship involves the forms like?NP2 of NP1?
and ?NP1?s NP2?.2In our study, the semantic category of a NE is identifiedautomatically by the pre-processing NE recognition component.166compatibility with the anaphor could be representedsimply in terms of frequencyStatSem(candi, ana) = count(candi, ana) (1)where count(candi, ana) is the count of the tupleformed by candi and ana, or alternatively, in termsof conditional probability (P (candi, ana|candi)),where the count of the tuple is divided by the countof the single candidate in the corpus.
That isStatSem(candi, ana) = count(candi, ana)count(candi) (2)In this way, the statistics would not bias candidatesthat occur frequently in isolation.2.2 Web-Based Semantic CompatibilityUnlike documents in normal corpora, web pagescould not be preprocessed to generate the predicate-argument reserve.
Instead, the predicate-argumentstatistics has to be obtained via a web search enginelike Google and Altavista.
For the three types ofpredicate-argument relationships, queries are con-structed in the forms of ?NPcandi VP?
(for subject-verb), ?VP NPcandi?
(for verb-object), and ?NPcandi?s NP?
or ?NP of NPcandi?
(for possessive-noun).Consider the following sentence:(2) Several experts suggested that IBM?s account-ing grew much more liberal since the mid 1980sas its business turned sour.For the pronoun ?its?
and the candidate ?IBM?, thetwo generated queries are ?business of IBM?
and?IBM?s business?.To reduce data sparseness, in an initial query onlythe nominal or verbal heads are retained.
Also, eachNE is replaced by the corresponding common noun.
(e.g, ?IBM?s business?
?
?company?s business?
and?business of IBM?
?
?business of company?
).A set of inflected queries is generated by ex-panding a term into all its possible morphologi-cal forms.
For example, in Sentence (1), ?collectmoney?
becomes ?collected|collecting|... money?,and in (2) ?business of company?
becomes ?businessof company|companies?).
Besides, determiners areinserted for every noun.
If the noun is the candidateunder consideration, only the definite article the isinserted.
For other nouns, instead, a/an, the and theempty determiners (for bare plurals) would be added(e.g., ?the|a business of the company|companies?
).Queries are submitted to a particular web searchengine (Google in our study).
All queries are per-formed as exact matching.
Similar to the corpus-based statistics, the compatibility for each candidateand anaphor pair could be represented using eitherfrequency (Eq.
1) or probability (Eq.
2) metric.
Insuch a situation, count(candi, ana) is the hit num-ber of the inflected queries returned by the searchengine, while count(candi) is the hit number of thequery formed with only the head of the candidate(i.e.,?the + candi?
).3 Applying the Semantic CompatibilityIn this section, we discuss how to incorporate thestatistics-based semantic compatibility for pronounresolution, in a machine learning framework.3.1 The Single-Candidate ModelOne way to utilize the semantic compatibility is totake it as a feature under the single-candidate learn-ing model as employed by Ng and Cardie (2002).In such a learning model, each training or testinginstance takes the form of i{C, ana}, where ana isthe possible anaphor and C is its antecedent candi-date.
An instance is associated with a feature vectorto describe their relationships.During training, for each anaphor in a given text,a positive instance is created by pairing the anaphorand its closest antecedent.
Also a set of negative in-stances is formed by pairing the anaphor and eachof the intervening candidates.
Based on the train-ing instances, a binary classifier is generated using acertain learning algorithm, like C5 (Quinlan, 1993)in our work.During resolution, given a new anaphor, a test in-stance is created for each candidate.
This instance ispresented to the classifier, which then returns a pos-itive or negative result with a confidence value indi-cating the likelihood that they are co-referent.
Thecandidate with the highest confidence value wouldbe selected as the antecedent.3.2 FeaturesIn our study we only consider those domain-independent features that could be obtained with low167Feature DescriptionDefNp 1 if the candidate is a definite NP; else 0Pron 1 if the candidate is a pronoun; else 0NE 1 if the candidate is a named entity; else 0SameSent 1 if the candidate and the anaphor is in the same sentence; else 0NearestNP 1 if the candidate is nearest to the anaphor; else 0ParalStuct 1 if the candidate has an parallel structure with ana; else 0FirstNP 1 if the candidate is the first NP in a sentence; else 0Reflexive 1 if the anaphor is a reflexive pronoun; else 0Type Type of the anaphor (0: Single neuter pronoun; 1: Plural neuter pronoun; 2:Male personal pronoun; 3: Female personal pronoun)StatSem?
the statistics-base semantic compatibility of the candidateSemMag??
the semantic compatibility difference between two competing candidatesTable 1: Feature set for our pronoun resolution system(*ed feature is only for the single-candidate modelwhile **ed feature is only for the twin-candidate mode)computational cost but with high reliability.
Table 1summarizes the features with their respective possi-ble values.
The first three features represent the lex-ical properties of a candidate.
The POS propertiescould indicate whether a candidate refers to a hearer-old entity that would have a higher preference to beselected as the antecedent (Strube, 1998).
SameSentand NearestNP mark the distance relationships be-tween an anaphor and the candidate, which wouldsignificantly affect the candidate selection (Hobbs,1978).
FirstNP aims to capture the salience of thecandidate in the local discourse segment.
ParalStuctmarks whether a candidate and an anaphor have sim-ilar surrounding words, which is also a salience fac-tor for the candidate evaluation (Mitkov, 1998).Feature StatSem records the statistics-based se-mantic compatibility computed, from the corpus orthe web, by either frequency or probability metric,as described in the previous section.
If a candidateis a pronoun, this feature value would be set to thatof its closest nominal antecedent.As described, the semantic compatibility of a can-didate is computed under the context of the cur-rent anaphor.
Consider two occurrences of anaphors?.
.
.
it1 collected .
.
.
?
and ?.
.
.
it2 said .
.
.
?.
As ?NPcollected?
should occur less frequently than ?NPsaid?, the candidates of it1 would generally havepredicate-argument statistics lower than those of it2.That is, a positive instance for it1 might bear a lowersemantic feature value than a negative instance forit2.
The consequence is that the learning algorithmwould think such a feature is not that ?indicative?and reduce its salience in the resulting classifier.One way to tackle this problem is to normalize thefeature by the frequencies of the anaphor?s context,e.g., ?count(collected)?
and ?count(said)?.
This,however, would require extra calculation.
In fact,as candidates of a specific anaphor share the sameanaphor context, we can just normalize the semanticfeature of a candidate by that of its competitor:StatSemN (C, ana) = StatSem(C, ana)maxci?candi set(ana)StatSem(ci, ana)The value (0 ?
1) represents the rank of thesemantic compatibility of the candidate C amongcandi set(ana), the current candidates of ana.3.3 The Twin-Candidate ModelYang et al (2003) proposed an alternative twin-candidate model for anaphora resolution task.
Thestrength of such a model is that unlike the single-candidate model, it could capture the preference re-lationships between competing candidates.
In themodel, candidates for an anaphor are paired andfeatures from two competing candidates are put to-gether for consideration.
This property could nicelydeal with the above mentioned training problem ofdifferent anaphor contexts, because the semanticfeature would be considered under the current can-didate set only.
In fact, as semantic compatibility is168a preference-based factor for anaphor resolution, itwould be incorporated in the twin-candidate modelmore naturally.In the twin-candidate model, an instance takes aform like i{C1, C2, ana}, where C1 and C2 are twocandidates.
We stipulate that C2 should be closer toana than C1 in distance.
The instance is labelled as?10?
if C1 the antecedent, or ?01?
if C2 is.During training, for each anaphor, we find itsclosest antecedent, Cante.
A set of ?10?
instances,i{Cante, C, ana}, is generated by pairing Cante andeach of the interning candidates C. Also a set of ?01?instances, i{C, Cante, ana}, is created by pairingCante with each candidate before Cante until anotherantecedent, if any, is reached.The resulting pairwise classifier would return?10?
or ?01?
indicating which candidate is preferredto the other.
During resolution, candidates are pairedone by one.
The score of a candidate is the totalnumber of the competitors that the candidate winsover.
The candidate with the highest score would beselected as the antecedent.Features The features for the twin-candidatemodel are similar to those for the single-candidatemodel except that a duplicate set of features has tobe prepared for the additional candidate.
Besides,a new feature, SemMag, is used in place of Stat-Sem to represent the difference magnitude betweenthe semantic compatibility of two candidates.
Letmag = StatSem(C1, ana)/StatSem(C2, ana), featureSemMag is defined as follows,SemMag(C1, C2, ana) ={mag ?
1 : mag >= 11?mag?1 : mag < 1The positive or negative value marks the times thatthe statistics of C1 is larger or smaller than C2.4 Evaluation and Discussion4.1 Experiment SetupIn our study we were only concerned about the third-person pronoun resolution.
With an attempt to ex-amine the effectiveness of the semantic feature ondifferent types of pronouns, the whole resolutionwas divided into neutral pronoun (it & they) reso-lution and personal pronoun (he & she) resolution.The experiments were done on the newswire do-main, using MUC corpus (Wall Street Journal ar-ticles).
The training was done on 150 documentsfrom MUC-6 coreference data set, while the testingwas on the 50 formal-test documents of MUC-6 (30)and MUC-7 (20).
Throughout the experiments, de-fault learning parameters were applied to the C5 al-gorithm.
The performance was evaluated based onsuccess, the ratio of the number of correctly resolvedanaphors over the total number of anaphors.An input raw text was preprocessed automati-cally by a pipeline of NLP components.
The nounphrase identification and the predicate-argument ex-traction were done based on the results of a chunktagger, which was trained for the shared task ofCoNLL-2000 and achieved 92% accuracy (Zhou etal., 2000).
The recognition of NEs as well as theirsemantic categories was done by a HMM basedNER, which was trained for the MUC NE taskand obtained high F-scores of 96.9% (MUC-6) and94.3% (MUC-7) (Zhou and Su, 2002).For each anaphor, the markables occurring withinthe current and previous two sentences were takenas the initial candidates.
Those with mismatchednumber and gender agreements were filtered fromthe candidate set.
Also, pronouns or NEs that dis-agreed in person with the anaphor were removed inadvance.
For the training set, there are totally 645neutral pronouns and 385 personal pronouns withnon-empty candidate set, while for the testing set,the number is 245 and 197.4.2 The Corpus and the WebThe corpus for the predicate-argument statisticscomputation was from the TIPSTER?s Text Re-search Collection (v1994).
Consisting of 173,252Wall Street Journal articles from the year 1988 to1992, the data set contained about 76 million words.The documents were preprocessed using the samePOS tagging and NE-recognition components as inthe pronoun resolution task.
Cass (Abney, 1996), arobust chunker parser was then applied to generatethe shallow parse trees, which resulted in 353,085possessive-noun tuples, 759,997 verb-object tuplesand 1,090,121 subject-verb tuples.We examined the capacity of the web and thecorpus in terms of zero-count ratio and count num-ber.
On average, among the predicate-argument tu-ples that have non-zero corpus-counts, above 93%have also non-zero web-counts.
But the ratio is onlyaround 40% contrariwise.
And for the predicate-169Neutral Pron Personal Pron OverallLearning Model System Corpus Web Corpus Web Corpus Webbaseline 65.7 86.8 75.1+frequency 67.3 69.9 86.8 86.8 76.0 76.9Single-Candidate +normalized frequency 66.9 67.8 86.8 86.8 75.8 76.2+probability 65.7 65.7 86.8 86.8 75.1 75.1+normalized probability 67.7 70.6 86.8 86.8 76.2 77.8baseline 73.9 91.9 81.9Twin-Candidate +frequency 76.7 79.2 91.4 91.9 83.3 84.8+probability 75.9 78.0 91.4 92.4 82.8 84.4Table 2: The performance of different resolution systemsRelationship N-Pron P-PronPossessive-Noun 0.508 0.517Verb-Object 0.503 0.526Subject-Verb 0.619 0.676Table 3: Correlation between web and corpus countson the seen predicate-argument tuplesargument tuples that could be seen in both datasources, the count from the web is above 2000 timeslarger than that from the corpus.Although much less sparse, the web counts aresignificantly noisier than the corpus count since notagging, chunking and parsing could be carried outon the web pages.
However, previous study (Kellerand Lapata, 2003) reveals that the large amount ofdata available for the web counts could outweigh thenoisy problems.
In our study we also carried out acorrelation analysis3 to examine whether the countsfrom the web and the corpus are linearly related,on the predicate-argument tuples that can be seenin both data sources.
From the results listed in Ta-ble 3, we observe moderately high correlation, withcoefficients ranging from 0.5 to 0.7 around, betweenthe counts from the web and the corpus, for bothneutral pronoun (N-Pron) and personal pronoun (P-Pron) resolution tasks.4.3 System EvaluationTable 2 summarizes the performance of the systemswith different combinations of statistics sources andlearning frameworks.
The systems without the se-3All the counts were log-transformed and the correlation co-efficients were evaluated based on Pearsons?
r.mantic feature were used as the baseline.
Under thesingle-candidate (SC) model, the baseline systemobtains a success of 65.7% and 86.8% for neutralpronoun and personal pronoun resolution, respec-tively.
By contrast, the twin-candidate (TC) modelachieves a significantly (p ?
0.05, by two-tailed t-test) higher success of 73.9% and 91.9%, respec-tively.
Overall, for the whole pronoun resolution,the baseline system under the TC model yields asuccess 81.9%, 6.8% higher than SC does4.
Theperformance is comparable to most state-of-the-artpronoun resolution systems on the same data set.Web-based feature vs. Corpus-based featureThe third column of the table lists the results us-ing the web-based compatibility feature for neutralpronouns.
Under both SC and TC models, incorpo-ration of the web-based feature significantly booststhe performance of the baseline: For the best sys-tem in the SC model and the TC model, the successrate is improved significantly by around 4.9% and5.3%, respectively.
A similar pattern of improve-ment could be seen for the corpus-based semanticfeature.
However, the increase is not as large asusing the web-based feature: Under the two learn-ing models, the success rate of the best system withthe corpus-based feature rises by up to 2.0% and2.8% respectively, about 2.9% and 2.5% less thanthat of the counterpart systems with the web-basedfeature.
The larger size and the better counts of theweb against the corpus, as reported in Section 4.2,4The improvement against SC is higher than that reportedin (Yang et al, 2003).
It should be because we now used 150training documents rather than 30 ones as in the previous work.The TC model would benefit from larger training data set as ituses more features (more than double) than SC.170should contribute to the better performance.Single-candidate model vs. Twin-Candidatemodel The difference between the SC and the TCmodel is obvious from the table.
For the N-Pronand P-Pron resolution, the systems under TC couldoutperform the counterpart systems under SC byabove 5% and 8% success, respectively.
In addition,the utility of the statistics-based semantic feature ismore salient under TC than under SC for N-Pron res-olution: the best gains using the corpus-based andthe web-based semantic features under TC are 2.9%and 5.3% respectively, higher than those under theSC model using either un-normalized semantic fea-tures (1.6% and 3.3%), or normalized semantic fea-tures (2.0% and 4.9%).
Although under SC, the nor-malized semantic feature could result in a gain closeto under TC, its utility is not stable: with metric fre-quency, using the normalized feature performs evenworse than using the un-normalized one.
These re-sults not only affirm the claim by Yang et al (2003)that the TC model is superior to the SC model forpronoun resolution, but also indicate that TC is morereliable than SC in applying the statistics-based se-mantic feature, for N-Pron resolution.Web+TC vs. Other combinations The aboveanalysis has exhibited the superiority of the webover the corpus, and the TC model over theSC model.
The experimental results also re-veal that using the the web-based semantic fea-ture together with the TC model is able to furtherboost the resolution performance for neutral pro-nouns.
The system with such a Web+TC combi-nation could achieve a high success of 79.2%, de-feating all the other possible combinations.
Es-pecially, it considerably outperforms (up to 11.5%success) the system with the Corpus+SC combina-tion, which is commonly adopted in previous work(e.g., Kehler et al (2004)).Personal pronoun resolution vs.
Neutral pro-noun resolution Interestingly, the statistics-basedsemantic feature has no effect on the resolution ofpersonal pronouns, as shown in the table 2.
Wefound in the learned decision trees such a featuredid not occur (SC) or only occurred in bottom nodes(TC).
This should be because personal pronounshave strong restriction on the semantic category (i.e.,human) of the candidates.
A non-human candidate,even with a high predicate-argument statistics, couldFeature Group Isolated CombinedSemMag (Web-based) 61.2 61.2Type+Reflexive 53.1 61.2ParaStruct 53.1 61.2Pron+DefNP+InDefNP+NE 57.1 67.8NearestNP+SameSent 53.1 70.2FirstNP 65.3 79.2Table 4: Results of different feature groups underthe TC model for N-pron resolutionSameSent_1 = 0::..SemMag > 0:: :..Pron_2 = 0: 10 (200/23): : Pron_2 = 1: ...: SemMag <= 0:: :..Pron_2 = 1: 01 (75/1): Pron_2 = 0:: :..SemMag <= -28: 01 (110/19): SemMag > -28: ...SameSent_1 = 1::..SameSent_2 = 0: 01 (1655/49)SameSent_2 = 1::..FirstNP_2 = 1: 01 (104/1)FirstNP_2 = 0::..ParaStruct_2 = 1: 01 (3)ParaStruct_2 = 0::..SemMag <= -151: 01 (27/2)SemMag > -151:...Figure 1: Top portion of the decision tree learnedunder TC model for N-pron resolution (features endedwith ?
1?
are for the first candidate C1 and those with ?
2?
arefor C2.
)not be used as the antecedent (e.g.
company said inthe sentence ?.
.
.
the company .
.
.
he said .
.
.
?).
Infact, our analysis of the current data set reveals thatmost P-Prons refer back to a P-Pron or NE candidatewhose semantic category (human) has been deter-mined.
That is, simply using features NE and Pronis sufficient to guarantee a high success, and thus therelatively weak semantic feature would not be takenin the learned decision tree for resolution.4.4 Feature AnalysisIn our experiment we were also concerned about theimportance of the web-based compatibility feature(using frequency metric) among the feature set.
Forthis purpose, we divided the features into groups,and then trained and tested on one group at a time.Table 4 lists the feature groups and their respectiveresults for N-Pron resolution under the TC model.171The second column is for the systems with only thecurrent feature group, while the third column is withthe features combined with the existing feature set.We see that used in isolation, the semantic compati-bility feature is able to achieve a success up to 61%around, just 4% lower than the best indicative fea-ture FirstNP.
In combination with other features, theperformance could be improved by as large as 18%as opposed to being used alone.Figure 1 shows the top portion of the pruned deci-sion tree for N-Pron resolution under the TC model.We could find that: (i) When comparing two can-didates which occur in the same sentence as theanaphor, the web-based semantic feature would beexamined in the first place, followed by the lexi-cal property of the candidates.
(ii) When two non-pronominal candidates are both in previous sen-tences before the anaphor, the web-based semanticfeature is still required to be examined after FirstNPand ParaStruct.
The decision tree further indicatesthat the web-based feature plays an important role inN-Pron resolution.5 ConclusionOur research focussed on improving pronoun reso-lution using the statistics-based semantic compati-bility information.
We explored two issues that af-fect the utility of the semantic information: statis-tics source and learning framework.
Specifically, weproposed to utilize the web and the twin-candidatemodel, in addition to the common combination ofthe corpus and single-candidate model, to computeand apply the semantic information.Our experiments systematically evaluated differ-ent combinations of statistics sources and learn-ing models.
The results on the newswire domainshowed that the web-based semantic compatibilitycould be the most effectively incorporated in thetwin-candidate model for the neutral pronoun res-olution.
While the utility is not obvious for per-sonal pronoun resolution, we can still see the im-provement on the overall performance.
We believethat the semantic information under such a config-uration would be even more effective on technicaldomains where neutral pronouns take the majorityin the pronominal anaphors.
Our future work wouldhave a deep exploration on such domains.ReferencesS.
Abney.
1996.
Partial parsing via finite-state cascades.
InWorkshop on Robust Parsing, 8th European Summer Schoolin Logic, Language and Information, pages 8?15.D.
Bean and E. Riloff.
2004.
Unsupervised learning of contex-tual role knowledge for coreference resolution.
In Proceed-ings of 2004 North American chapter of the Association forComputational Linguistics annual meeting.I.
Dagan and A. Itai.
1990.
Automatic processing of large cor-pora for the resolution of anahora references.
In Proceedingsof the 13th International Conference on Computational Lin-guistics, pages 330?332.J.
Hobbs.
1978.
Resolving pronoun references.
Lingua,44:339?352.A.
Kehler, D. Appelt, L. Taylor, and A. Simma.
2004.
The(non)utility of predicate-argument frequencies for pronouninterpretation.
In Proceedings of 2004 North Americanchapter of the Association for Computational Linguistics an-nual meeting.F.
Keller and M. Lapata.
2003.
Using the web to obtainfreqencies for unseen bigrams.
Computational Linguistics,29(3):459?484.R.
Mitkov.
1998.
Robust pronoun resolution with limitedknowledge.
In Proceedings of the 17th Int.
Conference onComputational Linguistics, pages 869?875.N.
Modjeska, K. Markert, and M. Nissim.
2003.
Using the webin machine learning for other-anaphora resolution.
In Pro-ceedings of the Conference on Empirical Methods in NaturalLanguage Processing, pages 176?183.V.
Ng and C. Cardie.
2002.
Improving machine learning ap-proaches to coreference resolution.
In Proceedings of the40th Annual Meeting of the Association for ComputationalLinguistics, pages 104?111, Philadelphia.M.
Poesio, R. Mehta, A. Maroudas, and J. Hitzeman.
2004.Learning to resolve bridging references.
In Proceedings of42th Annual Meeting of the Association for ComputationalLinguistics.J.
R. Quinlan.
1993.
C4.5: Programs for machine learning.Morgan Kaufmann Publishers, San Francisco, CA.W.
Soon, H. Ng, and D. Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.
Computa-tional Linguistics, 27(4):521?544.M.
Strube and C. Muller.
2003.
A machine learning approachto pronoun resolution in spoken dialogue.
In Proceedingsof the 41st Annual Meeting of the Association for Computa-tional Linguistics, pages 168?175, Japan.M.
Strube.
1998.
Never look back: An alternative to centering.In Proceedings of the 17th Int.
Conference on ComputationalLinguistics and 36th Annual Meeting of ACL, pages 1251?1257.X.
Yang, G. Zhou, J. Su, and C. Tan.
2003.
Coreference reso-lution using competition learning approach.
In Proceedingsof the 41st Annual Meeting of the Association for Computa-tional Linguistics, Japan.G.
Zhou and J. Su.
2002.
Named Entity recognition using aHMM-based chunk tagger.
In Proceedings of the 40th An-nual Meeting of the Association for Computational Linguis-tics, Philadelphia.G.
Zhou, J. Su, and T. Tey.
2000.
Hybrid text chunking.
InProceedings of the 4th Conference on Computational Natu-ral Language Learning, pages 163?166, Lisbon, Portugal.172
