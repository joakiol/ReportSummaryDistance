Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1521?1532,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsA Hierarchical Entity-based Approach to Structuralize User GeneratedContent in Social Media: A Case of Yahoo!
AnswersBaichuan Li1,2?, Jing Liu3?, Chin-Yew Lin4, Irwin King1,2, and Michael R. Lyu1,21Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China2Department of Computer Science and EngineeringThe Chinese University of Hong Kong, Shatin, N.T., Hong Kong3Harbin Institute of Technology, Harbin 150001, P.R.
China4Microsoft Research Asia, Beijing 100080, P.R.
Chinabcli@cse.cuhk.edu.hk jliu@ir.hit.edu.cn cyl@microsoft.com{king,lyu}@cse.cuhk.edu.hkAbstractSocial media like forums and microblogs haveaccumulated a huge amount of user generatedcontent (UGC) containing human knowledge.Currently, most of UGC is listed as a wholeor in pre-defined categories.
This ?list-based?approach is simple, but hinders users frombrowsing and learning knowledge of certaintopics effectively.
To address this problem, wepropose a hierarchical entity-based approachfor structuralizing UGC in social media.
Byusing a large-scale entity repository, we designa three-step framework to organize UGC ina novel hierarchical structure called ?clusterentity tree (CET)?.
With Yahoo!
Answers asa test case, we conduct experiments and theresults show the effectiveness of our frame-work in constructing CET.
We further evaluatethe performance of CET on UGC organiza-tion in both user and system aspects.
Froma user aspect, our user study demonstratesthat, with CET-based structure, users performsignificantly better in knowledge learning thanusing traditional list-based approach.
Froma system aspect, CET substantially booststhe performance of two information retrievalmodels (i.e., vector space model and querylikelihood language model).1 IntroductionWith the development of Web 2.0, socialmedia websites?such as online forums, blogs,microblogs, social networks, and community?This work was done when the first two authors were oninternship at MSRA.Table 1: Sample questions about Edinburgh1.
Where can i buy a hamburger in Edinburgh?2.
Where can I get a shawarma in Edinburgh?3.
How long does it take to drive between Glasgowand Edinburgh?4.
Whats the difference between Glasgow and Edinburgh?5.
Good hotels in London and Edinburgh?6.
Looking for nice , clean cheap hotel in Edinburgh?7.
Does anyone know of a reasonably cheap hotel inEdinburgh that is near to Niddry Street South ?8.
Who can recommend a affordable hotel inEdinburgh City Center?question answering (CQA) portals?have becomethe mainstream of web, where users create, share,and exchange information with each other.
As aresult, more and more UGC is accumulated, withsocial media websites retaining a huge amount ofhuman knowledge and user experience.
At present,most of UGC is organized in a list structure withextra information (e.g., category hierarchies inonline forums), or without any other information.This ?list-of-content?
(list-based approach) issimple and straightforward, but ineffective forbrowsing and knowledge learning.
Considerthe following case: a user wants to spend hisvacation in Edinburgh.
He visits a CQA websiteto explore which aspects are mostly asked.
In thisscenario, he may browse some relevant categorieslike ?Travel:United Kingdom:Edinburgh?
to getuseful information.
He may also issue a query like?travel in Edinburgh?
to search relevant questions.However, both the browsing and the searching givethe user a list of relevant contents (e.g., questionsshown in Table 1), not the direct knowledge.
Thus,the user has to read these contents, understand them,classify them into various topics, and gain valuable1521HGLQEXUJK^`VKDZDUPD^`KDPEXUJHU^`KRWHO^`ORQGRQ^`KRWHO^`KRWHO^`JODVJRZ^`ORQGRQ^`QLGU\VWUHHWVRXWK^`FLW\FHQWHU^`KRWHO^`FLW\FHQWHU^`QLGU\VWUHHWVRXWK^`&OXVWHU)RRG&OXVWHU/RFDWLRQ&OXVWHU&LW\ &OXVWHU/RFDWLRQ/D\HU/D\HU/D\HUFigure 1: An CET constructed from questions about Edinburghknowledge himself.
Obviously, it is ineffective andtime-consuming.The above problem calls for a new approach tostructuralize UGC in social media, which facilitatesusers to seek knowledge (e.g., travel informationabout Edinburgh) more effectively.
Traditionally,we can utilize topic models (Blei et al 2003) orsocial tagging to structuralize UGC.
However, fortopic models, it is not easy to control the granularityof topics, and it is hard for users to interpret a topiconly based on the multinomial distribution (Mei etal., 2007).
For social tagging, it is not applicablein many sites and has sparsity problem (Shepitsenet al 2008).
Thus, both topic models and socialtagging are not suitable for structuralizing UGC insocial media.In this paper, we propose a novel hierarchicalentity-based approach, i.e., ?cluster entity tree?
orCET, to structuralize UGC in social media by lever-aging an existing large-scale entity repository.
Fig-ure 1 shows how CET structuralizes UGC in Table 1.In this CET, each node contains one (named) entityand a set of question IDs.
With ?edinburgh?
as theroot entity, layer 1 includes all entities that co-occurwith ?edinburgh?.
Similarly, entities on layer 2 co-occur with their parent entities on layer 1 and theroot entity ?edinburgh?.
For example, ?city center?co-occurs with ?hotel?
and ?edinburgh?
in Question8.
Deeper layers provide more specific topics aboutdifferent aspects of Edinburgh (e.g., Edinburgh?shotels).
As shown in Fig.
1, the corresponding ques-tion IDs are also attached in each node.
In addition,entities which share the same parent are clustered todifferent groups (see dashed rectangles in Fig.
1)1.1Single-node clusters are not surrounded by rectangles, likeThrough this hierarchical structure, we can easilybrowse corresponding questions and answers, andlearn knowledge about Edinburgh more effectively,such as food and location.
Moreover, since similarentities (and corresponding questions) are groupedin each layer, it also helps the system manage similarcontents.By utilizing a large-scale entity repository, CETavoids the granularity, interpretation, and sparsityproblems.
Entity repositories like Freebase2 providea large number of named entities across variouspre-defined topics, which avoid the granularity andsparsity problems.
In addition, they usually givedescriptions of entities, which prevent the interpre-tation problem.
Therefore, CET is more suitable forstructuralizing UGC.In this paper, we propose a three-step frameworkto construct CETs.1.
Entity extraction.
In this step, we extract en-tities from documents using an existing entityrepository.2.
Tree construction.
we build the co-occurrencerelationship between any two entities and con-struct hierarchical ?entity trees (ETs)?.3.
Hierarchical entity clustering.
In an ET, someentities are more similar than other entitieswhich share the same parents.
Therefore, oneach layer of the ET we cluster entities withthe same parents (e.g., ?london?, ?nidry streetsouth?, and ?city center?
on layer 2 of Fig.
1)and finally construct a CET.We select Yahoo!
Answers as a test case to evalu-ate 1) the performance of our framework for con-?hotel{5}?
on layer 2.2http://www.freebase.com/1522structing CET and 2) the effectiveness of CET inUGC structuralization.
Yahoo!
Answers is a popularCQA portal, where users post questions and provideanswers in different categories.
Experimental resultsdemonstrate the good performance of our frame-work for constructing CET.
We further evaluatethe effectiveness of CET in structuralizing UGCfrom both user and system aspects.
From a useraspect, our user study show that, with CET-basedorganization, users perform significantly better inknowledge learning than using list-based approach.From a system aspect, CET boosts systems?
infor-mation retrieval substantially: the mean reciprocalrank (MRR) of vector space model (VSM) and querylikelihood language model (QLLM) are improvedby 9.3% and 8.2%, respectively.To summarize, our contributions are three-fold:1.
We propose a novel hierarchical entity-basedapproach to structuralize UGC in social media.To our knowledge, we are the first to utilize en-tities to structuralize UGC for content browsingand knowledge learning at a large scale;2.
We present a three-step framework to constructCETs and show its effectiveness from empiricalresults;3.
We demonstrate the significant advantages ofour approach for both users and systems inknowledge learning and retrieval.The paper proceeds as follows.
We present relatedwork in Section 2.
We detail our framework to con-struct CETs and show empirical results in Section3.
Section 4 and Section 5 evaluate the effectivenessof CET on knowledge organization from user andsystem aspects respectively.
We conclude the paperin Section 6.2 Related WorkUGC organization in social media.
Most UGCin social media is unstructured, or organized ina predefined category hierarchy.
These categoriesgive shallow semantics of UGC, and boosts theperformance of information retrieval (Cao et al2008; Duan et al 2008; Cao et al 2012) andrecommendation (Guo et al 2008; Li et al 2011).With new content kept adding into a hierarchy, weneed to maintain category hierarchy (Yuan et al2012) to make content within the same categorymore topically cohesive.Apart from category hierarchy, UGC can also beorganized by topic models and tags.
Topic models,such as latent Dirichlet alcation (LDA) (Blei etal., 2003), are widely applied in document cluster-ing and classification, However, it is not trivial tocontrol the granularity of topics (Chen et al 2011).What?s more, it is generally difficult to understanda topic only from the multinomial distribution (Meiet al 2007).
Social tagging provides an alternativeapproach in document organization (Gupta et al2010).
In some UGC websites like stackoverflow3,users usually add tags to describe their contents.However, tags are not widely applicable and tagsare usually sparse (Shepitsen et al 2008).
Ourhierarchical entity-based approach prevents theseproblems by employing a large-scale entity repos-itory.
As the entity repository provides a unifiedset of entities across various of pre-defined topics,and gives descriptions of entities, CET avoids thegranularity, interpretation and sparsity issues.The work in (Zhu et al 2013), which automati-cally generates and updates topic terms to organizeUGC, is mostly related to our work.
In this paper,given a root topic, subtopics and lower-level topicsare extracted from UGC, which form a hierarchicalstructure to organize corresponding UGC.
However,in (Zhu et al 2013) more external sources areutilized to identify subtopics.
In addition, relation-ships among subtopics which under the same parentare not investigated.
The metro maps proposedin (Shahaf et al 2013) are also related to our work.Different from (Shahaf et al 2013), we employ alarge-scale entity repository to extract more mean-ingful and interpretable key terms (entities), whichmake each subtopic much easier to understand.Entity extraction.
In our framework, we lever-age an entity repository to extract named entitiesfrom UGC.
A common approach is to utilize aNamed Entity Recognition (NER) system like Stan-ford NER (Finkel et al 2005), which recognizes thenames of things (e.g., person and product names)from texts.
For cross-domain NER, (Ru?d et al2011) employed search engines.
For short-textNER, (Liu et al 2012) proposed a graphical model.However, most of above systems are restricted from3http://stackoverflow.com/1523producing labels for a few entity classes.
To ad-dress the problem, (Ling and Weld, 2012) defineda fine-grained set of 112 tags based on Freebasefor entity extraction.
However, this approach stillfaces the ?low-recall?
problem in the real world.Our approach, which leverages a large-scale entityrepository, addresses this issue.Entity-based document classification andretrieval.
Entity repository has been employed inother research areas, like document classificationand retrieval.
(Schonhofen, 2006) utilizedWikipedia to classify documents.
(Yerva etal., 2012a) proposed an entity-based classificationfor tweets.
In addition, entity-based retrievalmodels were proposed and applied in both QAarchives (Singh, 2012a) and tweets (Yerva et al2012b).
Besides, (Singh, 2012b) proposed an entity-based translation language model and demonstratedthat it outperformed classical translation languagemodel in question retrieval.
However, to the best ofour knowledge, no previous study leverages entitiesto organize UGC in social media.3 CET ConstructionIn this section, we formulate the framework to con-struct CET and show the empirical results.
Firstly,we provide the definitions of the entity repositoryand CET.Definition 3.1 (Entity Repository) LetER = {R, g} be an entity repository, whereR is a set of named entities and g : R ?
R is amapping function that defines the similarity of anytwo entities.Note that we do not require a hierarchical structurein an ER (like Freebase); only a similarity functionis needed.Definition 3.2 (Cluster Entity Tree) LetD be a setof documents, ER = {R, g} be an entity reposi-tory, e be an entity, a cluster entity tree CETe =(ve, V,E,C) is defined as a tree structure, with theroot node ve, node set V , edge set E, and clusterset C .
Each node vs ?
V on CETe includes anentity extracted from the set of documents De ?D containing e, and a list L(s) which stores theindexes of documents containing entity s and itssuperior entities.
If vs is vt?s parent node, entity tmust co-occur with s and s?s all superior entities atleast once in the same document.
Each element of C(one cluster) includes a set of nodes which share thesame parent node, and the entities within a clusterare more similar to each other than the entities inother clusters.3.1 FrameworkThis section shows our three-step framework forconstructing CET: entity extraction (Section 3.2.1),tree construction (Section 3.2.2), and hierarchicalentity clustering (Section 3.2.3).3.1.1 Entity ExtractionWe adopt a simple entity repository based ap-proach to extract entities, which address the ?low-recall?
problem for traditional NERmethods (detailsare given in Section 3.3.2).
This approach involvestwo phases: candidate entity extraction and entropy-based filtering.Candidate entity extraction.
We employ theStanford Parser4 to parse each document to a parsetree.
Then, we extract all noun phrases, preprocessthem (including stemming), and extract the nounphrases which are included in our entity repository.In our experiments, we adopt a large-scale enterpriseentity repository (anonymized for blind reviews).Entropy-based filtering.
The candidate entitiesgenerated from the last step may contain manyfalse examples, which are not relevant to the mainsemantics of documents, like ?we?, ?how do i?, etc.To filter them, we propose an entropy-based method.Given a document with a category label (or tags,which are available in most UGC sites), we get thedistributions of each candidate entity over all topcategories.
The entropy of a candidate entity ei iscalculated as follows:Entropy(ei) = ?|C|?c=1Pc(ei)logPc(ei), (1)where |C| is the number of top categories and Pc(ei)is the number of ei in category c divided by allnumber of candidate entities in that category.Top-ranked entities are general terms among cat-egories.
We set a threshold ?
and remove all candi-date entities with entropy larger than ?.
The settingof ?
is a tradeoff: higher values will introduce morenoise, while smaller values will lead to decreased4http://nlp.stanford.edu/software/lex-parser.shtml1524recall.
In our experiments, we empirically set ?
as1.5 since it provides the most satisfying results.3.1.2 Tree ConstructionGiven an entity (e.g., ?edinburgh?
), we first searchdocuments containing this entity and make the entitytogether with document ids as the root node.
Then,from searched documents we find all entities that co-occur with the root entity.
These entities and cor-responding document ids form layer-1 nodes of theentity tree (see the example in Fig.
1).
Afterwards,for each entity in layer-1 nodes, we search entitiesthat co-occur with it and its superiors, combine themand corresponding document ids as new nodes, andput these new nodes under current node, which formlayer-2 nodes.
Iteratively, we construct the entitytree with the given entity as the root.3.1.3 Hierarchical Entity ClusteringUnder the same parent, some entities5 may sharesimilar topics.
Therefore, the final step is to hier-archically cluster entities with the same parents atdifferent layers of entity trees.
This step not onlyfacilitates knowledge learning but also reduces thewidth of a tree.
In this paper, we follow the workin (Hu et al 2012) and employ an agglomerativeclustering algorithm for two reasons: 1) it is easyto implement and its time complexity is O(N2);2) there is no need to set the number of clusters.Although the clustering results may be influencedby instance order, our empirical results demonstrateits effectiveness.
Any other advanced algorithmslike spectral clustering (Ng et al 2002) can also beapplied, but that is not the emphasis of this paper.Algorithm.
For a set of entities with the sameparent, the agglomerative clustering algorithmworks as follows:1.
Select one entity and create a new cluster whichcontains the entity;2.
Select the next entity ei, create an empty can-didate list, calculate the similarity between theentity and all existing clusters.
Three strategiesare employed6:?
AC-MAX: If the similarity between entityei and entity ej in one of the clusters (the5Here we use the entity to represent the node.6We modify the clustering algorithm in (Hu et al 2012)slightly to assign a unique cluster for each entity.first one) is larger than threshold ?max, weput the cluster index and correspondingsimilarity in the candidate list.?
AC-MIN: If the similarity between entityei and any entity ej in one of the clustersis larger than threshold ?min, we put thecluster index and corresponding similarityin the candidate list.?
AC-AVG: If the mean similarity betweenentity ei and any entity ej in one of theclusters is larger than threshold ?avg , weput the cluster index and correspondingsimilarity in the candidate list.3.
If the candidate list is not empty, put ei in thecluster with highest similarly.4.
If the candidate list is empty, a new cluster withei as the element will be created.5.
Stop when all entities are clustered.Similarity Function.
In our entity repository, thesimilarity between two entities is computed usingthe approach in (Shi et al 2010), which estimatesthe similarity of two terms according to their first-order and second-order co-occurrences.
For exam-ple, ?such as NP, NP?
is a good pattern for detectingsimilar entities using first-order co-occurrences.
Inaddition, if two entities usually co-occur with athird entity (second-order co-occurrence), these twoentities are likely to be similar.
To construct simi-larity functions, pattern-based approaches (Ohshimaet al 2006; Zhang et al 2009) utilize first-orderco-occurrences while distributional similarity ap-proaches (Pasca et al 2006; Pennacchiotti andPantel, 2009) employ second-order co-occurrences.In the following, we briefly introduce the pattern-based approach (PB) and the distributional similarityapproach (DS) in (Shi et al 2010).PB.
Some well-designed patterns are leveragedto extract similar entities from a huge repository ofwebpages.
The set of terms extracted by applyinga pattern one time is called a raw semantic class(RASC).
Given two entities ta and tb, PB calculatestheir similarity based on the number of RASCscontaining both of them (Zhang et al 2009):Sim(ta, tb) = log(1 +rab?i=1Pabi )) ?
?idf(ta) ?
idf(tb), (2)where idf(ta) = log(1 + NC(ta)), Pabi is a patternwhich can generate RASC(s) containing both term1525ta and term tb, rab is the total number of suchpatterns, N is the total number of RASCs, andC(ta) is the number of RASCs containing ta.
Theabove similarity is normalized using the followingfunction:SimPB(ta, tb) ==logSim(ta, tb)2 logSim(ta, ta)+logSim(ta, tb)2 logSim(tb, tb).
(3)DS.
DS approach assumes that terms appearingin similar contexts tend to be similar.
In thisapproach, a term is represented by a feature vector,with each feature corresponding to a context inwhich the term appears.
The similarity between twoterms is computed as the similarity between theircorresponding feature vectors.
Jaccard similarityis employed to estimate the similarity between twoterms.
Suppose the feature vector of ta and tb are xand y respectively,SimDS(ta, tb) =?imin(xi, yi)?i(xi) +?i(yi) ?
?imin(xi, yi).
(4)Shi et al(Shi et al 2010) found that PB performedbetter when dealing with proper nouns; while DSwas relatively good at estimating similarity of othertypes of entities.
The similarity function in our ERfollows the suggestion of (Shi et al 2010): if atleast one entity is proper noun, PB is employed;otherwise DS is used.3.2 Experiments3.2.1 SetupWe evaluate the performance of our frameworkemploying questions from Yahoo!
Answers.
54.7million questions are crawled from all 26 top cate-gories in Yahoo!
Answers, which consist of questiontitles and corresponding categories.
From thesequestions, we construct the following two test setsfor evaluating entity extraction and entity clustering:Set EE.
This set is employed to evaluate theperformance of entity extraction.
It contains 520randomly sampled questions, 20 from each top cat-egory.
One author is asked to label entities for eachquestion.Set EC.
This set is constructed to automaticallyevaluate hierarchical entity clustering and select thebest clustering strategy.
The construction process isas follows.
First, we map the four top categoriesof Yahoo!
Answers to some categories of Freebasemanually, as shown in Table 2.
Second, fromquestions at each top category of Yahoo!
Answers,Table 2: Category mapping between Yahoo!
Answersand FreeBaseYahoo!
Answers FreeBaseCars & Transportation Aviation, Transportation, BoatsSpaceflight, Automotive, Bicycles, RailComputers & Internet Computer, InternetSoccer, Olympics,Sports, American football,Sports Baseball,Basketball,Ice Hockey,Martial Arts,Cricket,Tennis,Boxing,SkiingTravel Travel, Location, TransportationTable 3: Number of questions and entities in Set ECCategory Number of Questions Number of EntitiesCars & Transportation 1,220,427 3,267,596Computers & Internet 2,912,280 7,324,655Sports 2,363,758 6,230,868Travel 1,347,801 3,728,286we extract entities which appear exactly once in thecorresponding Freebase categories.
For instance, ifan entity is extracted from questions in the cate-gory Computers & Internet, and it appears two ormore times in Computer and Internet categories inFreebase, it will be filtered.
Therefore, each entityis attached with a unique Freebase category label(i.e., the ground truth for clustering).
Questionscontaining at least two entities are selected for SetEC.
Table 3 reports the statistics.
Intuitively, entitieswith a same Freebase category label should be in onecluster.Note that Set EC only covers a small set of realentities and clustering on Set EC is partial clus-tering.
However, it leverages Freebase labels andavoids manual labeling, which is time-consuming.Furthermore, partial clustering results are enoughfor evaluating different strategies?
performance andchoosing the best strategy.Following the common practice, we evaluate en-tity extraction using precision, recall, and F1 score.For evaluating entity clustering results, we adopt B-cubed metrics.
As reported in (Amigo?
et al 2009),B-cubed metrics are more suitable than traditionalmetrics, such as NMI and purity.Table 4: Entity extraction for various methodsMethod Precision Recall F1Standord NER 0.750 0.155 0.257FIGER 0.763 0.154 0.256Freebase 0.644 0.595 0.619Our 0.647 0.809 0.7191526Table 5: Clustering results using AC-MAX (?max=0.1)Level Travel Cars & Transportation Computer & Internet SportsCount P R F1 Count P R F1 Count P R F1 Count P R F11 748 0.972 0.653 0.743 1281 0.948 0.868 0.897 3064 0.913 0.664 0.743 890 0.941 0.883 0.9012 200 0.974 0.730 0.798 1202 0.989 0.956 0.965 11344 0.961 0.842 0.879 636 0.978 0.964 0.9633 120 1.000 0.833 0.890 858 1.000 0.981 0.988 8184 0.978 0.899 0.920 492 0.965 0.882 0.8994 NA NA NA NA 1776 1.000 0.980 0.986 3648 0.990 0.908 0.934 1080 0.978 0.844 0.8815 NA NA NA NA NA NA NA NA 2520 1.000 0.952 0.968 NA NA NA NATotal 1068 0.976 0.688 0.770 5117 0.984 0.946 0.959 28760 0.968 0.857 0.891 3098 0.965 0.886 0.9073.2.2 Results: Entity ExtractionTwo ERs (i.e., ours and Freebase) are employedin entity extraction for comparison.
In addition, wecompare our approach with Stanford NER (Finkelet al 2005) and fine-grained entity recognition(FIGER) (Ling and Weld, 2012).
Table 4 reportsthe results of different methods.
We can find thatStanford NER and FIGER get a relatively highprecision in extracting entities.
However, theirrecalls are very low and only about 15% of entitiesare recognized.
With the help of entity reposito-ries, recall is significantly improved with a smalldecrease of precision.
Therefore, the F1s of entity-based approaches are much higher.
This observationshows the great advantage of utilizing an entityrepositories in entity extraction and the effectivenessof our approach.
As our ER performs better thanFreebase, we adopt it as our entity repository in thefollowing evaluations.3.2.3 Results: Hierarchical Entity ClusteringTable 5 reports the count of clusters, B-CubedPrecision, Recall, and F1 for different layers ofclustering across four categories using AC-MAX.
Inour experiments, AC-MAX performed better thanAC-MIN and AC-AVG.
Due to space limitations,we only report the results of AC-MAX here.
ForAC-MAX, we changed the settings of ?
from 0.01to 0.9, and the best performance was achieved when?max was set at around 0.1.
From Table 5 wecan find that, although AC-MAX?s accuracy variesacross categories (e.g., the F1 of Transportation ismuch higher than that of Travel), it performs well ingeneral.
Thus, we adopt AC-MAX with ?
= 0.1 forhierarchical entity clustering.4 User StudyIn this section, we investigate the influence of CETon users?
content browsing and knowledge learningfrom a user study.
In the study, we design 24 tasks infour popular Yahoo!
Answers categories (see Table2).
For each category, we design three knowledge-learning tasks and three question-search tasks, asshown in Table A.1 in the supplementary material.A knowledge-learning task asks for some knowl-edge about a main entity from question texts.
Forinstance, ?find the games running on macbook pro?requires game names as the answer, where the mainentity is ?macbook pro?.
A question-search task,however, asks users to find similar questions tothe question in the task.
For example, ?questionsabout who will win the MVP in NBA this year?asks for finding similar questions, and filling theirquestion IDs as the answer.
For each task, wegive some suggested keywords (entities) to facilitateinformation gathering.To evaluate user experience, we ask participantsto fill out a questionnaire after each task.
Fol-lowing the work in (Kato et al 2012), we collectinformation from 5 aspects: familiarity, easiness,satisfaction, adequate time, and helpfulness.
A 5-point Likert scale is designed for each questionnaire.?5?
means the participant totally agrees while ?1?means the participant totally disagrees.4.1 SetupPrograms.
We develop two programs in our userstudy.
One is CET-based, and the other is traditionallist-based7 .
The list-based program searches ques-tions by utilizing Apache Lucene8.
The standard an-alyzer and the default search algorithm are adopted.For each query, top 200 most relevant questions areretrieved.Data.
We extract 70,195 questions which containat least one of the 24 main entities (see Table A.1)7The interface of CET-based program is provided in thesupplementary material (see Fig.
A.1).
The interface of list-based program is similar, but the CET display area is replacedby a flat-ranked list.8http://lucene.apache.org/core/1527Table 6: User study resultsKnowledge-learning Tasks Question-search TasksCET-based List-based CET-based List-based# Queries 2.99 4.47 2.56 3.38# Answers 8.32 6.06 10.60 10.92Precision 0.38 0.19 0.40 0.44Time 136.44 121.87 103.71 87.75Table 7: Questionnaire resultsKnowledge-learning Tasks Question-search TasksCET-based List-based CET-based List-basedFamiliarity 3.18 3.22 3.07 3.28Easiness 3.64 3.66 4.10 4.06Satisfaction 3.70 2.94 3.86 3.44Enough Time 3.87 3.83 4.44 4.54Helpfulness 4.16 3.03 4.31 3.71in the four categories.
For each question, we extractthe entities with the help of our entity repository.
Foreach main entity, we build the corresponding CETfrom all extracted questions.Participants.
Sixteen volunteers are invited inthe user study.
They are first briefly informed of theresearch design and taught how to use two programs.To familiarize the participants with our programspromptly, we provide demonstrations using sampleentities.
Each volunteer is asked to finish 12 tasks(6 knowledge-learning tasks and 6 question-searchtasks) using the CET-based program and 12 othertasks using the list-based program in random order.Thus, each task is finished by exactly 8 differentparticipants using each program.4.2 Results and DiscussionsTable 6 reports the user study results, where we givethe statistics for users?
performance with the twoprograms.
We evaluate from the number of queriesissued, number of answers found, the precisionof answers, and query time for each task.
Asour 24 tasks contain both knowledge-learning tasksand question-search tasks, we report their resultsseparately.
Z-tests are employed for significancetests.From Table 6, we observe that more queries areissued in the knowledge-learning tasks than in thequestion-search tasks using both programs.
How-ever, the CET-based program reduces the numberof queries substantially in both tasks.
Because theCET-based program provides a series of clusteredentities, it helpes users further refine queries throughclicking on entities rather than reconstructing a newquery.
However, the list-based program only listsrelevant questions, and users have to issue newqueries according to returned questions.By using the CET-based program, volunteers findmore answers in knowledge-learning tasks (z =1.69, p < 0.05).
The reason is that the CET-based program clusters similar results in the samegroup, and if the user finds one answer she caneasily get more answers.
On the contrary, thelist-based program returns a list of questions, andusers need to find answers question-by-question.For question-search tasks, users of the list-basedprogram find more answers, but the difference is notsignificant (z = 0.19).
As the list-based programreturns similar questions as top-ranked results, usersare able to fill in answers easily.
For CET-basedprogram users, they have to find corresponding keyentities in the CETs first.
Therefore, they spendmore time (the fourth row in Table 6) finding entitiesand less time filling answers.
It is worth notingthat our GUI prototype for CET is non-optimal, andusers?
searching time on CET-based program can befurther reduced with better user interface.The precision of answers from CET-based pro-gram users is twice of that from list-based programusers (z = 4.15, p < 0.0001) in knowledge-learning tasks, which demonstrates the advantage ofCET in helping knowledge-learning.
For question-search tasks, CET-based program users performslightly worse than list-based program users, but thedifference is not significant (z = 0.48).
Since usersof the CET-based program spend more time findingentities, they have limited time to check the answers.In both tasks, users spend more time on theCET-based program.
According to users?
post-user-study feedbacks, a few volunteers reported that theysometimes spent a considerable amount of time onfinding entities from CETs; however, one positiveobservation is that most users find ?the entity-basedinterface?
very interesting, which stimulates them tospend more time on exploring answers.The questionnaires reveal more about user expe-rience on these two programs (see Table 7).
Users?responses to task familiarity and easiness are similar.However, users of entity-based interface are moresatisfied in both knowledge-learning tasks (z =3.98, p < 0.0001) and question-search tasks (z =15281.38), and they feel that entity-based interface ismore helpful in finding answers for both knowledge-learning tasks (z = 6.47, p < 0.0001) and question-search tasks (z = 2.55, p < 0.01).
These promisingobservations show that CET helps knowledge learn-ing greatly through structuralizing content.5 CET-based Question Re-RankingIn this section, we show that CET also helps systemsto better retrieve information through re-ranking.
Inthe following, we continue to use Yahoo!
Answersas a test case.Algorithm 1 CET-based search results re-rankingInput: query q, question collection Q, a ranked list ofk relevant questions Qq = {q1, q2, ..., qk} to q, anentity repository ER, an empty list ?.Output: A new ranked list of questions.1: Extract entities from each question of Q andconstruct a entity co-occurrence graph;2: Get the PageRank score of each entity;3: if There is no entity e in q then4: return Qq;5: else6: Identify the key entity e from q which has thehighest PageRank score;7: Construct the CET cete from Q based on ER;8: for each question qi do9: For all entities in qi, build a entity chain C indescending order of PageRank scores;10: From C extract the first entity e?
that is notsimilar to e;11: if e?
exists then12: Put qi in the corresponding cluster of nodes;13: else14: Put qi in ?
;15: end if16: end for17: end if18: Rank all clusters on cete according to their firstelements?
original ranking;19: Output the final ranking cluster by cluster and appendthe questions in ?
at the last.Intuitively, questions sharing similar topicsshould be ranked similarly.
However, traditionalquestion retrieval models (Cao et al 2010) such asQLLM and VSM do not capture key semantics andgive more weights for entity terms.
CET providesa feasible way to address this issue.
By utilizingCET, entities are given more weight while trivialTable 8: Re-ranking results for VSM and QLLM (*means that p < 0.05 in students?
t-test)VSM Re-ranking QLLM Re-rankingMRR 0.3838 0.4195* (9.30%) 0.3593 0.3889* (8.24%)MAP 0.3376 0.3558* (5.39%) 0.3326 0.3479* (4.60%)Prec@1 0.2500 0.3125* (25.00%) 0.2438 0.2688* (10.25%)words are not.
In addition, through clusteringquestions with similar topics, those questions whichare ranked lower will be brought higher by theirtop-ranked neighbors.Algorithm 1 illustrates the re-ranking algorithm indetail.
We first extract entities from each question ofthe whole question collection, and construct a entityco-occurrence graph (Line 1).
Then, we calculatethe PageRank score of each entity (Line 2).
Line 3-5 check whether q contains at least one entity.
Ifthe answer is no, we return the original ranking.Otherwise, we identify the key entity in q (Line6) and construct the CET cete whose root entityis e (Line 7).
Line 8-16 iteratively put questionsin corresponding clusters of cete.
In Line 8, wefirst build an entity chain for question qi, in whichentities of qi are ranked according to their PageRankscores.
Afterwards, the first entity e?, which is notsimilar to e (the similarity is calculated in Section4.2.1 and the threshold of similarity is set to 0.1), ispicked up as the main aspect of e, and qi is groupedinto the corresponding cluster on cete (Line 7-8).
Ife?
does not exist, we put qi in a new cluster (Line13-14).
Then, we rank all clusters according totheir first elements?
original rankings (Line 18) andoutput the final re-ranked list (Line 19).We perform our re-ranking on 160 randomlyselected questions from Computers & Internet andTravel categories of our data set9.
Each categorycontains 80 questions.
All other questions in thesetwo categories constitute the question collection Q.For each question, we utilize the VSM and QLLM10respectively to get the top 15 most relevant questions(excluding itself).
The correct ranking is manuallylabeled and checked by two annotators.
We firstlyemployed the VSM and QLLM respectively to re-trieve the top 15 results and then obtained manualjudgments.
Given a retrieved question by VSM9These 160 questions are not used for constructing the entityco-occurrence graph.10Following (Zhai and Lafferty, 2004), we set ?
to 0.2.1529or QLLM, two assessors are asked to label it with?relevant?
or ?irrelevant?.
If their annotations areopposite, the third assessor is involved to determinethe final label.We re-rank these questions using Algorithm 1.Table 8 shows the results of MRR, mean averageprecision (MAP), and Precision@1.
We can see thatCET-based re-ranking improves the performance ofstandard retrieval models substantially.
For VSM,our re-ranking boosts the MRR and MAP by 9.3%and 5.4%, respectively.
It is worth noting that our re-ranking improves Prec@1 significantly: from 0.25to 0.31.
The reason is that traditional methodsmay give relatively low weights to the key terms(entities), while CET-based re-ranking addressesthe problem.
QLLM and re-ranking report similarresults.
Figures 2 and 3 illustrate the performanceof various approaches across categories.
We findthat our re-ranking is neither category-biased noralgorithm-biased, yet it performs better than origi-nal models on both categories.
The above resultsdemonstrate that, by utilizing the hierarchical entity-based approach, CET greatly improves the retrievalperformance of these two standard models.6 Conclusion and Future WorkTraditional list-based organization of UGC in socialmedia is not effective for content browsing andknowledge learning due to large volume of doc-uments.
To address this problem, we propose anovel hierarchical entity-based approach to struc-turalize UGC in social media.
By using a large-scale entity repository, we construct a three-stepframework to organize knowledge in ?cluster entitytrees?.
Experimental results show the effectivenessof the framework in constructing CET.
We furtherevaluate the performance of CET on knowledgeorganization from both user and system aspects.Our user study demonstrates that, with CET-basedorganization, users perform significantly better inknowledge learning than using list-based approach.In addition, CET boosts systems?
content searchperformance substantially through re-ranking.To our best knowledge, this work is the firstattempt to utilize entities to structuralize UGC insocial media, and there are some limitations to beimproved in our future work.
First, we employFigure 2: Re-ranking results of Computer & InternetFigure 3: Re-ranking results of TravelYahoo!
Answers as our test data, in which questions(documents) are usually short.
We observe thatnearly 92% of all 54.7 million questions contain1-4 entities, which means the depth of CETs areusually not so deep.
However, long documents,such as Blog posts, will lead to deep CETs andhinder users?
knowledge learning.
Second, ourcurrent entity extraction focuses on named entitiesinstead of canonical entities.
In the future, weplan to employ document summarization techniquesto shorten the depth of CETs.
We also aim toincorporate semantic analysis and normalize namedentities to canonical entities, which make CET moresuitable for practical use.AcknowledgmentsThe work described in this paper was fully supportedby the Shenzhen Major Basic Research Program(Project No.
JC201104220300A) and the ResearchGrants Council of the Hong Kong Special Adminis-trative Region, China (Project Nos.
CUHK413212,CUHK415212).1530ReferencesEnrique Amigo?, Julio Gonzalo, Javier Artiles, and FelisaVerdejo.
2009.
A comparison of extrinsic clusteringevaluation metrics based on formal constraints.
Inf.Retr., 12(4):461?486, August.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
J. Mach.
Learn.Res., 3:993?1022, March.Yunbo Cao, Huizhong Duan, Chin-Yew Lin, YongYu, and Hsiao-Wuen Hon.
2008.
Recommendingquestions using the mdl-based tree cut model.
In Proc.of WWW, WWW ?08, pages 81?90.Xin Cao, Gao Cong, Bin Cui, and Christian S. Jensen.2010.
A generalized framework of exploring categoryinformation for question retrieval in communityquestion answer archives.
In Proceedings of the 19thinternational conference on World wide web, WWW?10, pages 201?210, New York, NY, USA.
ACM.Xin Cao, Gao Cong, Bin Cui, Christian S. Jensen, andQuan Yuan.
2012.
Approaches to exploring categoryinformation for question retrieval in communityquestion-answer archives.
ACM Trans.
Inf.
Syst.,30(2):7:1?7:38, May.Mengen Chen, Xiaoming Jin, and Dou Shen.
2011.Short text classification improved by learning multi-granularity topics.
In Proceedings of the Twenty-Second international joint conference on ArtificialIntelligence - Volume Volume Three, IJCAI?11, pages1776?1781.
AAAI Press.Huizhong Duan, Yunbo Cao, Chin yew Lin, and YongYu.
2008.
Searching questions by identifying questiontopic and question focus.
In In Proceedings of46th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Tchnologies(ACL:HLT.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 363?370, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Jinwen Guo, Shengliang Xu, Shenghua Bao, and YongYu.
2008.
Tapping on the potential of QA communityby recommending answer providers.
In Proc.
ofCIKM, CIKM ?08, pages 921?930.Manish Gupta, Rui Li, Zhijun Yin, and Jiawei Han.
2010.Survey on social tagging techniques.
SIGKDD Explor.Newsl., 12(1):58?72, November.Yunhua Hu, Yanan Qian, Hang Li, Daxin Jiang, Jian Pei,and Qinghua Zheng.
2012.
Mining query subtopicsfrom search log data.
In Proceedings of the 35thinternational ACM SIGIR conference on Researchand development in information retrieval, SIGIR ?12,pages 305?314, New York, NY, USA.
ACM.Makoto P. Kato, Tetsuya Sakai, and Katsumi Tanaka.2012.
Structured query suggestion for specializationand parallel movement: effect on search behaviors.
InProc.
of WWW, WWW ?12, pages 389?398.Baichuan Li, Irwin King, and Michael R. Lyu.
2011.Question routing in community question answering:putting category in its place.
In Proc.
of CIKM, CIKM?11, pages 2041?2044.Xiao Ling and Daniel S. Weld.
2012.
Fine-grained entityrecognition.
In AAAI.Xiaohua Liu, Ming Zhou, Furu Wei, Zhongyang Fu,and Xiangyang Zhou.
2012.
Joint inferenceof named entity recognition and normalization fortweets.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics:Long Papers - Volume 1, ACL ?12, pages 526?535,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.2007.
Automatic labeling of multinomial topicmodels.
In Proc.
of KDD, KDD ?07, pages 490?499.Andrew Y Ng, Michael I Jordan, Yair Weiss, et al2002.On spectral clustering: Analysis and an algorithm.Advances in neural information processing systems,2:849?856.Hiroaki Ohshima, Satoshi Oyama, and Katsumi Tanaka.2006.
Searching coordinate terms with their contextfrom the web.
In Proceedings of the 7th internationalconference on Web Information Systems, WISE?06,pages 40?47, Berlin, Heidelberg.
Springer-Verlag.Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lif-chits, and Alpa Jain.
2006.
Organizing and searchingthe world wide web of facts - step one: the one-millionfact extraction challenge.
In proceedings of the 21stnational conference on Artificial intelligence - Volume2, AAAI?06, pages 1400?1405.
AAAI Press.Marco Pennacchiotti and Patrick Pantel.
2009.
Entityextraction via ensemble semantics.
In Proceedingsof the 2009 Conference on Empirical Methods inNatural Language Processing: Volume 1 - Volume 1,EMNLP ?09, pages 238?247, Stroudsburg, PA, USA.Association for Computational Linguistics.Stefan Ru?d, Massimiliano Ciaramita, Jens Mu?ller, andHinrich Schu?tze.
2011.
Piggyback: using searchengines for robust cross-domain named entity recogni-tion.
In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies - Volume 1, ACL-HLT ?11,pages 965?975, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Peter Schonhofen.
2006.
Identifying document topicsusing the wikipedia category network.
In Proceedings1531of the 2006 IEEE/WIC/ACM International Conferenceon Web Intelligence, WI ?06, pages 456?462, Wash-ington, DC, USA.
IEEE Computer Society.Dafna Shahaf, Jaewon Yang, Caroline Suen, Jeff Jacobs,Heidi Wang, and Jure Leskovec.
2013.
Informationcartography: creating zoomable, large-scale mapsof information.
In Proceedings of the 19th ACMSIGKDD international conference on Knowledgediscovery and data mining, KDD ?13, pages 1097?1105, New York, NY, USA.
ACM.Andriy Shepitsen, Jonathan Gemmell, BamshadMobasher, and Robin Burke.
2008.
Personalizedrecommendation in social tagging systems usinghierarchical clustering.
In Proceedings of the 2008ACM conference on Recommender systems, RecSys?08, pages 259?266, New York, NY, USA.
ACM.Shuming Shi, Huibin Zhang, Xiaojie Yuan, and Ji-Rong Wen.
2010.
Corpus-based semantic classmining: distributional vs. pattern-based approaches.In Proceedings of the 23rd International Conferenceon Computational Linguistics, COLING ?10, pages993?1001, Stroudsburg, PA, USA.
Association forComputational Linguistics.Amit Singh.
2012a.
Entity based QA retrieval.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, EMNLP-CoNLL ?12, pages 1266?1277, Stroudsburg, PA,USA.
Association for Computational Linguistics.Amit Singh.
2012b.
Entity based translation languagemodel.
In Proc.
of WWW, WWW ?12 Companion,pages 599?600.Surender Reddy Yerva, Zolta?n Miklo?s, and Karl Aberer.2012a.
Entity-based classification of twitter messages.IJCSA, 9(1):88?115.Surender Reddy Yerva, Zoltan Miklos, Flavia Grosan,Alexandru Tandrau, and Karl Aberer.
2012b.Tweetspector: entity-based retrieval of tweets.
InProc.
of SIGIR, SIGIR ?12, pages 1016?1016.Quan Yuan, Gao Cong, Aixin Sun, Chin-Yew Lin,and Nadia Magnenat Thalmann.
2012.
Categoryhierarchy maintenance: a data-driven approach.
InProceedings of the 35th international ACM SIGIR con-ference on Research and development in informationretrieval, SIGIR ?12, pages 791?800, New York, NY,USA.
ACM.Chengxiang Zhai and John Lafferty.
2004.
A studyof smoothing methods for language models appliedto information retrieval.
ACM Trans.
Inf.
Syst.,22(2):179?214, April.Huibin Zhang, Mingjie Zhu, Shuming Shi, and Ji-RongWen.
2009.
Employing topic models for pattern-based semantic class discovery.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Volume1 - Volume 1, ACL ?09, pages 459?467, Stroudsburg,PA, USA.
Association for Computational Linguistics.Xingwei Zhu, Zhao-Yan Ming, Xiaoyan Zhu, and Tat-Seng Chua.
2013.
Topic hierarchy constructionfor the organization of multi-source user generatedcontents.
In Proceedings of the 36th internationalACM SIGIR conference on Research and developmentin information retrieval, SIGIR ?13, pages 233?242,New York, NY, USA.
ACM.1532
