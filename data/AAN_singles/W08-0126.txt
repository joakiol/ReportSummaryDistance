Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 164?171,Columbus, June 2008. c?2008 Association for Computational LinguisticsAbstractWe propose to use user simulation for testingduring the development of a sophisticated dia-log system.
While the limited behaviors of thestate-of-the-art user simulation may not coverimportant aspects in the dialog system testing,our proposed approach extends the functional-ity of the simulation so that it can be used atleast for the early stage testing before the sys-tem reaches stable performance for evaluationinvolving human users.
The proposed ap-proach includes a set of evaluation measuresthat can be computed automatically from theinteraction logs between the user simulatorand the dialog system.
We first validate thesemeasures on human user dialogs using usersatisfaction scores.
We also build a regressionmodel to estimate the user satisfaction scoresusing these evaluation measures.
Then, weapply the evaluation measures on a simulateddialog corpus trained from the real user cor-pus.
We show that the user satisfaction scoresestimated from the simulated corpus are notstatistically different from the real users?
satis-faction scores.1 IntroductionSpoken dialog systems are being widely used indaily life.
The increasing demands of such systemsrequire shorter system development cycles andbetter automatic system developing techniques.
Asa result, machine learning techniques are applied tolearn dialog strategies automatically, such as rein-forcement learning (Singh et al, 2002; Williams &Young, 2007), supervised learning (Henderson et* This study was conducted when the author was an intern atBosch RTC.al., 2005), etc.
These techniques require a signifi-cant amount of training data for the automaticlearners to sufficiently explore the vast space ofpossible dialog states and strategies.
However, it isalways hard to obtain training corpora that arelarge enough to ensure that the learned strategiesare reliable.
User simulation is an attempt to solvethis problem by generating synthetic training cor-pora using computer simulated users.
The simu-lated users are built to mimic real users' behaviorsto some extent while allowing them to be pro-grammed to explore unseen but still possible userbehaviors.
These simulated users can interact withthe dialog systems to generate large amounts oftraining data in a low-cost and time-efficient man-ner.
Many previous studies (Scheffler, 2002;Pietquin, 2004) have shown that the dialog strate-gies learned from the simulated training data out-perform the hand-crafted strategies.
There are alsostudies that use user simulation to train speech rec-ognition and understanding components (Chung,2004).While user simulation is largely used in dialogsystem training, it has only been used in limitedscope for testing specific dialog system compo-nents in the system evaluation phase (L?pez-C?zaret al, 2003; Filisko and Seneff, 2006).
This ispartly because the state-of-the-art simulated usershave quite limited abilities in mimicking humanusers' behaviors and typically over-generate possi-ble dialog behaviors.
This is not a major problemwhen using simulated dialog corpus as the trainingcorpus for dialog strategy learning because theover-generated simulation behaviors would onlyprovide the machine learners with a broader dialogstate space to explore (Ai et al, 2007).
However,realistic user behaviors are highly desired in thetesting phase because the systems are evaluatedand adjusted based on the analysis of the dialogsgenerated in this phase.
Therefore, we would ex-User Simulation as Testing for Spoken Dialog SystemsHua Ai* Fuliang WengIntelligent Systems Program Research and Technology CenterUniversity of Pittsburgh Robert Bosch LLC210 S. Bouquet St., Pittsburg, PA 15260 4009 Miranda Ave., Palo Alto, CA 94304Hua@cs.pitt.edu Fuliang.weng@us.bosch.com164pect that these user behaviors are what we will seein the final evaluation with human users.
In thiscase, any over-generated dialog behaviors maycause the system to be blamed for untargeted func-tions.
What is more, the simulated users cannotprovide subjective user satisfaction feedbackwhich is also important for improving the systems.Since it is expensive and time-consuming to testevery version of the system with a significantamount of paid subjects, the testing during the de-velopment is typically constrained to a limitednumber of users, and often, to repeated users whoare colleagues or developers themselves.
Thus, thesystem performance is not always optimized forthe intended users.Our ultimate goal is to supplement human test-ing with simulated users during the development tospeed up the system development towards desiredperformance.
This would be especially useful inthe early development stage, since it would avoidconducting tests with human users when they mayfeel extremely frustrated due to the malfunction ofthe unstable system.As a first attempt, we try to extend the state-of-the-art user simulation by incorporating a set ofnew but straightforward evaluation measures forautomatically assessing the dialog system perform-ance.
These evaluation measures focus on threebasic aspects of task-oriented dialog systems: un-derstanding ability, efficiency, and the appropri-ateness of the system actions.
They are firstapplied on a corpus generated between a dialogsystem and a group of human users to demonstratethe validity of these measures with the human us-ers' satisfaction scores.
Results show that thesemeasures are significantly correlated with the hu-man users' satisfactions.
Then, a regression modelis built to predict the user satisfaction scores usingthese evaluation measures.
We also apply the re-gression model on a simulated dialog corpustrained from the above real user corpus, and showthat the user satisfaction scores estimated from thesimulated dialogs do not differ significantly fromthe real users?
satisfaction scores.
Finally, we con-clude that these evaluation measures can be used toassess the system performance based on the esti-mated user satisfaction.2 User Simulation TechniquesMost user simulation models are trained from dia-log corpora generated by human users.
Earliermodels predict user actions based on simple rela-tions between the system actions and the followinguser responses.
(Eckert et al, 1997) first suggest abigram model to predict the next user's actionbased on the previous system's action.
(Levin et al,2000) add constraints to the bigram model to ac-cept the expected dialog acts only.
However, theirbasic assumption of making the next user's actiondependent only on the system's previous action isoversimplified.
Later, many studies model morecomprehensive user behaviors by adding user goalsto constrain the user actions (Scheffler, 2002; Piet-quin, 2004).
These simulated users mimic real userbehaviors in a statistical way, conditioning the useractions on the user goals and the dialog contexts.More recent research defines agenda for simulatedusers to complete a set of settled goals (Schatz-mann et al, 2007).
This type of simulated user up-dates the agenda and the current goal based on thechanges of the dialog states.In this study, we build a simulated user similarto (Schatzmann et al, 2007) in which the simulateduser keeps a list of its goals and another agenda ofactions to complete the goals.
In our restaurant se-lection domain, the users?
tasks are to find a de-sired restaurant based on several constraintsspecified by the task scenarios.
We consider theserestaurant constraints as the goals for the simulateduser.
At the beginning of the dialog, the simulateduser randomly generates an agenda for the list ofthe ordered goals corresponding to the three con-straints in requesting a restaurant.
An agenda con-tains multiple ordered items, each of whichconsists of the number of constraints and the spe-cific constraints to be included in each user utter-ance.
During the dialog, the simulated user updatesits list of goals by removing the constraints thathave been understood by the system.
It also re-moves from its agenda the unnecessary actions thatare related to the already filled goals while addingnew actions.
New actions are added according tothe last system?s question (such as requesting theuser to repeat the last utterance) as well as thesimulated user?s current goals.
The actions thataddress the last system?s question are given higherpriorities then other actions in the agenda.
For ex-ample, if the dialog system fails to understand thelast user utterance and thus requests a clarification,the simulated user will satisfy the system?s request165before moving on to discuss a new constraint.
Thesimulated user updated the agenda with the newactions after each user turn.The current simulated user interacts with thesystem on the word level.
It generates a string ofwords by instantiating its current action using pre-defined templates derived from previously col-lected corpora with real users.
Random lexicalerrors are added to simulate a spoken languageunderstanding performance with a word error rateof 15% and a semantic error rate of 11% based onprevious experience (Weng et al, 2006).3 System and CorpusCHAT (Conversational Helper for AutomotiveTasks) is a spoken dialog system that supports na-vigation, restaurant selection and mp3 player ap-plications.
The system is specifically designed forusers to interact with devices and receive serviceswhile performing other cognitive demanding, orprimary tasks such as driving (Weng et al, 2007).CHAT deploys a combination of off-the-shelfcomponents, components used in previous lan-guage applications, and components specificallydeveloped as part of this project.
The core compo-nents of the system include a statistical languageunderstanding (SLU) module with multiple under-standing strategies for imperfect input, an informa-tion-state-update dialog manager (DM) thathandles multiple dialog threads and mixed initia-tives (Mirkovic and Cavedon, 2005), a knowledgemanager (KM) that controls access to ontology-based domain knowledge, and a content optimizerthat connects the DM and the KM for resolvingambiguities from the users' requests, regulating theamount of information to be presented to the user,as well as providing recommendations to users.
Inaddition, we use Nuance 8.51 with dynamic gram-mars and classbased n-grams, for speech recogni-tion, and Nuance Vocalizer 3.0 for text-to-speechsynthesis (TTS).
However, the two speech compo-nents, i.e., the recognizer and TTS are not used inthe version of the system that interacts with thesimulated users.The CHAT system was tested for the navigationdomain, the restaurant selection and the MP3 mu-sic player.
In this study, we focus on the dialogcorpus collected on the restaurant domain only.
A1 See http://www.nuance.com for details.small number of human users were used as dry-runtests for the system development from November,2005 to January, 2006.
We group the adjacent dry-runs to represent system improvement stages on aweekly basis.
Table 1 shows the improvementstages, the dry-run dates which each stage in-cludes, and the number of subjects tested in eachstage.
A final evaluation was conducted duringJanuary 19-31, 2006, without any further systemmodifications.
This final evaluation involved 20paid subjects who were recruited via internet ad-vertisement.Only the users in the final evaluation completeduser satisfaction surveys after interacting with thesystem.
In the survey, users were asked to rate theconversation from 6 perspectives, each on a 5-point scale: whether the system was easy to use,whether the system understood the user well,whether the interaction pattern was natural,whether the system's actions were appropriate,whether the system acted as expected, and whetherthe user was willing to use the system on a regularbase.
A user satisfaction score was computed asthe average of the 6 ratings.Nine tasks of restaurant selections were used inboth dry-runs and the final evaluation using 12constraints in total (e.g., cuisine type, price level,location).
These 12 constraints are spread acrossthe nine tasks evenly with three constraints pertask.
In addition, each task is carefully wordedbased on the task-constrained and language-unconstrained guideline.
In other words, we wantthe users to form an intended mental context whiletrying to prevent them from copying the exactphrasing in the task description.
During the dry-runs, the users randomly pick three to four tasks toStage Dry-run Dates Users1 11/21/05, 11/22/05 22 11/30/05, 12/1/05, 12/2/05 33 12/7/05, 12/8/05 24 12/13/05, 12/14/05, 12/15/05 55 12/19/05, 12/20/05, 12/21/05 46 12/27/05, 12/28/05 27 1/4/06, 1/5/06 28 1/10/06, 1/11/06, 1/13/06 49 1/16/06, 1/17/06 3Table 1: Dry-runs166test the system, while in the final evaluation eachuser is required to complete all of the 9 tasks.
As aresult of the final evaluation in the restaurant do-main with 2500 restaurants, we reached a taskcompletion rate of 94% with a word recognitionrate of 85%, and a semantic accuracy rate of 89%.4 Evaluation MeasuresIn this section, we describe in detail the evaluationmeasures covering three basic aspects of task-oriented dialog systems: understanding ability, ef-ficiency, and the appropriateness of the systemactions.4.1 Understanding Ability MeasuresHuman-human dialog is a process to reach mutualunderstandings between the dialog partners by ex-changing information through the dialog.
This in-formation exchanging process also takes place inthe interaction between users and spoken dialogsystems.
In a task-oriented conversation, the dialogsystem's major task is to understand the users'needs in order to provide the right service.
In theinformation-state update framework, the systemcontinuously updates its information-states duringthe dialog while the users are conveying their re-quirements.
If a misunderstanding occurs, therewould be a mismatch between the users?
require-ments and the system?s understandings.
Thus, theerror recovery dialog is needed to fix the mis-matches.
The error recovery dialog can be initiatedeither by the system by asking the user to rephraseor to repeat the previous utterance, or by the userto restate the previous request.We use the percent of agreement between thesystem's and the user's understandings (under-standingAgreement) to measure how well thesystem understands the user.
The computation ofthis measure is illustrated through the example dia-log in Table 2.
In this table, the first column showsthe system utterances and the user utterances re-ceived by the system.
The correct words are shownin square brackets immediately after the misunder-stood words (E.g., in Utterance ?User1?).
The sec-ond column represents semantic content from theusers?
utterances in the form of constraint-valuepairs based on the system?s understandings.
Thisinformation can be automatically retrieved fromthe system logs.
The third column includes the ac-tion types of the current system/user utterances.Since the dialog manager is an information-updating dialog manager that manages informationin the format of slots, this column also shows thenumber of slots that are exchanged in the utteranceand the number of matched slots.
In our task do-main, the user can request information (req_info),request the same information again (rereq_info),answer an explicit confirmation (exp_confirm),and close a dialog (close_dialog).
The system canprovide information (pro_info) or explicitly con-firms (exp_confirm) the information.
AnotherUtterances Constraints Understood by the SystemAction Types & Under-standing StatusUser1: I'm looking for a five star Thai [Italian]restaurant accept credit cardsAction: req_infoSlots conveyed: 3 slotsSystem1: There are 7 restaurants that havea five star rating, accept credit cards, andserve Thai food in my database.
Here arethe first few: TeJUS, Slam Corner Cate,Setha's Restaurant.
Should I continue?service level=five starcuisine type=Thaipayment type=credit cardAction: pro_infoSlots accepted: 3 slotsCompletely matched: 2 slotsUser2: I need to look for Italian restaurant  Action: rereq_info Slot  conveyed: 1 slotSystem2: Are you looking for a restaurantthat serves Italian food?service level=five starcuisine type=Italian (unconfirmed)payment type=credit cardAction: exp_confirmSlot  updated: 1 slotCompletely matched: 2.5 slotUser3: yes  Action: exp_confirm Slot conveyed: 1 slotSystem3: I found 38 restaurants that serveItalian food, accept credit cards and have afive star rating, for example, Toto Caf?
?service level=five starcuisine type=Italian (confirmed)payment type=credit cardAction: pro_infoSlots accepted: 3 slotsCompletely matched: 3 slotsUser4: I'm done.
Action: close_dialogTable 2: A Sample Dialog with Annotations.167available system action that is not shown in thisexample is to ask the user to repeat/rephrase (re-phrase), where the user can respond by providingthe information again (repro_info).In our experiment, we measure the understand-ings between the users and the system by compar-ing the values of the constraints that are specifiedby the users with their values understood by thesystem.
In this dialog, the user specified all con-straints in the first utterance:Service level = Five starCuisine type = ItalianPayment type = Credit cardThe first system utterance shows that the systemunderstood two constraints but misunderstood thecuisine type, thus the percent agreement of mutualunderstandings is 2/3 at this time.
Then, the userrestated the cuisine type and the second systemutterance confirmed this information.
Since thesystem only asks for explicit information when itsconfidence is low, we count the system's under-standing on the cuisine type as a 50% match withthe user's.
Therefore, the total percent agreement is2.5/3.
The user then confirmed that the system hadcorrectly understood all constraints.
Therefore, thesystem provided the restaurant information in thelast utterance.
The system's understanding matches100% with the user's at this point.The percent agreement of system/user under-standings over the entire dialog is calculated byaveraging the percent agreement after each turn.
Inthis example, understandingAgreement is (2/3 +2.5/3 + 1)/3 =83.3%.
We hypothesize that thehigher the understandingAgreement is, the betterthe system performs, and thus the more the user issatisfied.
The matches of understandings can becalculated automatically from the user simulationand the system logs.
However, since we work withhuman users' dialogs in the first part of this study,we manually annotated the semantic contents (e.g.,cuisine name) in the real user corpus.Previous studies (E.g., Walker et al, 1997) use acorpus level semantic accuracy measure (semanti-cAccuracy) to capture the system?s understandingability.
SemanticAccuracy is defined in the stan-dard way as the total number of correctly under-stood constraints divided by the total number ofconstraints mentioned in the entire dialog.
The un-derstandingAgreement measure we introduce hereis essentially the averaged per-sentence semanticaccuracy, which emphasizes the utterance levelperception rather than a single corpus level aver-age.
The intuition behind this new measure is thatit is better for the system to always understandsomething to keep a conversation going than forthe system to understand really well sometimes butreally bad at other times.
We compute both meas-ures in our experiments for comparison.4.2 Efficiency MeasureEfficiency is another important measure of the sys-tem performance.
A standard efficiency measure isthe number of dialog turns.
However, we wouldlike to take into account the user's dialog strategybecause how the user specifies the restaurant selec-tion constraints has a certain impact on the dialogpace.
Comparing two situations where one userspecifies the three constraints of selecting a restau-rant in three separate utterances, while another userspecifies all the constraints in one utterance, wewill find that the total number of dialog turns in thesecond situation is smaller assuming perfect under-standings.
Thus, we propose to use the ratio be-tween the number of turns in the perfectunderstanding situation and the number of turns inpractice (efficiencyRatio) to measure the systemefficiency.
The larger the efficiencyRatio is, thecloser the actual number of turns is to the perfectunderstanding situation.
In the example in Table 2,because the user chose to specify all the constraintsin one utterance, the dialog length would be 2 turnsin perfect understanding situation (excluding thelast user turn which is always "I'm done").
How-ever, the actual dialog length is 6 turns.
Thus, theefficiencyRatio is 2/6.Since our task scenarios always contain threeconstraints, we can calculate the length of the er-ror-free dialogs based on the user?s strategy.
Whenthe user specifies all constraints in the first utter-ance, the ideal dialog will have only 2 turns; whenthe user specifies two constraints in one utteranceand the other constraints in a separate utterance,the ideal dialog will have 4 turns; when the userspecifies all constraints one by one, the ideal dia-log will have 6 turns.
Thus, in the simulation envi-ronment, the length of the ideal dialog can becalculated from the simulated users?
agenda.
Then,the efficiencyRatio can be calculated automati-cally.
We manually computed this measure for thereal users?
dialogs.168Similarly, in order to compare with previousstudies, we also investigate the total number ofdialog turns (dialogTurns) proposed as the effi-ciency measure (E.g., M?ller et al, 2007).4.3 Action Appropriateness MeasureThis measure aims to evaluate the appropriatenessof the system actions.
The definition of appropri-ateness can vary on different tasks and differentsystem design requirements.
For example, somesystems always ask users to explicitly confirmtheir utterances due to high security needs.
In thiscase, an explicit confirmation after each user utter-ance is an appropriate system action.
However, inother cases, frequent explicit confirmations may beconsidered as inappropriate because they may irri-tate the users.
In our task domain, we define theonly inappropriate system action to be providinginformation based on misunderstood user require-ments.
In this situation, the system is not aware ofits misunderstanding error.
Instead of conductingan appropriate error-recovering dialog, the systemprovides wrong information to the user which wehypothesize will decrease the user?s satisfaction.We use the percentage of appropriate system ac-tions out of the total number of system actions(percentAppropriate) to measure the appropriate-ness of system actions.
In the example in Table 2,only the first system action is inappropriate in all 3system actions.
Thus, the percent system actionappropriateness is 2/3.
Since we can detect the sys-tem?s misunderstanding and the system?s action inthe simulated dialog environment, this measure canbe calculated automatically for the simulated dia-logs.
For the real user corpus, we manually codedthe inappropriate system utterances.Note that the definition of appropriate action weuse here is fairly loose.
This is partly due to thesimplicity of our task domain and the limited pos-sible system/user actions.
Nevertheless, there isalso an advantage of the loose definition: we donot bias towards one particular dialog strategysince our goal here is to find some general and eas-ily measurable system performance factors that arecorrelated with the user satisfaction.5 Investigating Evaluation Measures onthe Real User CorpusIn this section, we first validate the proposedmeasures using real users?
satisfaction scores, andthen show the differentiating power of these meas-ures through the improvement curves plotted onthe dry-run data.5.1 Validating Evaluation MeasuresTo validate the evaluation measures introduced inSection 4, we use Pearson?s correlation to examinehow well these evaluation measures can predict theuser satisfaction scores.
Here, we only look at thedialog corpus in final evaluation because onlythese users filled out the user satisfaction surveys.For each user, we compute the average value of theevaluation measures across all dialogs generatedby that user.Table 3 lists the correlation between the evalua-tion measures and the user satisfaction scores, aswell as the p-value for each correlation.
The corre-lation describes a linear relationship between thesemeasures and the user satisfaction scores.
For themeasures that describe the system?s understandingabilities and the measures that describe the sys-tem?s efficiency, our newly proposed measuresshow higher correlations with the user satisfactionscores than their counterparts.
Therefore, in therest of the study, we drop the two measures usedby the previous studies, i.e., semanticAccuracy anddialogTurns.We observe that the user satisfaction scores aresignificantly positively correlated with all the threeproposed measures.
These correlations confirmsour expectations: user satisfaction is higher whenthe system?s understanding matches better with theusers?
requirements; when the dialog efficiency iscloser to the situation of perfect understanding; orwhen the system's actions are mostly appropriate.We suggest that these measures can serve as indi-cators for user satisfaction.We further use all the measures to build a re-gression model to predict the user satisfactionscore.
The prediction model is:Evaluation Measure Correlation P-valueunderstandingAgreement 0.354 0.05semanticAccuracy 0.304 0.08efficiencyRatio 0.406 0.02dialogTurns -0.321 0.05percentAppropriate 0.454 0.01Table3: Correlations with User Satisfaction Scores.169User Satisfaction= 6.123*percentAppropriate+2.854*efficiencyRatio                         --- (1)+0.864*understandingAgreement - 4.67The R-square is 0.655, which indicates that65.5% of the user satisfaction scores can be ex-plained by this model.
While this prediction modelhas much room for improvement, we suggest thatit can be used to estimate the users?
satisfactionscores for simulated users in the early system test-ing stage to quickly assess the system's perform-ance.
Since the weights are tuned based on the datafrom this specific application, the prediction modelmay not be used directly for other domains.5.2 Assessing the Differentiating Power of theEvaluation MeasuresSince this set of evaluation measures intends toevaluate the system's performance in the develop-ment stage, we would like the measures to be ableto reflect small changes made in the system and toindicate whether these changes show the righttrend of increased user satisfaction in reality.
A setof good evaluation measures should be sensible tosubtle system changes.We assess the differentiating power of the eval-uation measures using the dialog corpus collectedduring the dry-runs.
The system was tested on aweekly basis as explained in Table 1.
For each im-provement stage, we compute the values for thethree evaluation measures averaging across all dia-logs from all users.
Figure 1 shows the three im-provement curves based on these three measures.The x-axis shows the first date of each improve-ment stage; the y-axis shows the value of the eval-uation measures.
We observe that all three curvesshow the right trends that indicate the system?simprovements over the development stages.6 Applying the Evaluation Measures onthe Simulated CorpusWe train a goal and agenda driven user simulationmodel from the final evaluation dialog corpus withthe real users.
The simulation model interacts withthe dialog system 20 times (each time the simula-tion model represents a different simulated user),generating nine dialogs on all of the nine taskseach time.
In each interaction, the simulated usersgenerate their agenda randomly based on a uniformdistribution.
The simulated corpus consists of 180dialogs from 20 simulated users, which is of thesame size as the real user corpus.
The values of theevaluation measures are computed automatically atthe end of each simulated dialog.We compute the estimated user satisfaction scoreusing Equation 1 for each simulated user.
We thencompare the user satisfaction scores of the 20 si-mulated users with the satisfaction scores of the 20real users.
The average and the standard deviationof the user satisfaction scores for real users are(3.79, 0.72), and the ones for simulated users are(3.77, 1.34).
Using two-tailed t-test at significancelevel p<0.05, we observe that there are no statisti-cally significant differences between the two poolsof scores.
Therefore, we suggest that the user satis-faction estimated from the simulated dialog corpuscan be used to assess the system performance.However, these average scores only offer us oneperspective in comparing the real with the simu-lated user satisfaction.
In the future, we would liketo look further into the differences between thedistributions of these user satisfaction scores.7 Conclusions and Future WorkUser simulation has been increasingly used in gen-erating large corpora for using machine learningtechniques to automate dialog system design.However, user simulation has not been used muchin testing dialog systems.
There are two major con-00.10.20.30.40.50.60.70.80.9111/21/05 11/30/05 12/05/05 12/13/05 12/19/05 12/27/05 01/04/06 01/10/06 01/16/06understandingAgreement eff iciencyRatio percentAppropriateFigure 1: The Improvement Curves on Dry-run Data170cerns: 1. we are not sure how well the state-of-the-art user simulation can mimic realistic user behav-iors; 2. we do not get important feedback on usersatisfaction when replacing human users withsimulated users.
In this study, we suggest thatwhile the simulated users might not be mature touse in the final system evaluation stage, they canbe used in the early testing stages of the systemdevelopment cycle to make sure that the system isfunctioning in the desired way.
We further proposea set of evaluation measures that can be extractedfrom the simulation logs to assess the system per-formance.
We validate these evaluation measureson human user dialogs and examine the differenti-ating power of these measures.
We suggest thatthese measures can be used to guide the develop-ment of the system towards improving user satis-faction.
We also apply the evaluation measures ona simulation corpus trained from the real user dia-logs.
We show that the user satisfaction scores es-timated on the simulated dialogs do notsignificantly differ statistically from the real users?satisfaction scores.
Therefore, we suggest that theestimated user satisfaction can be used to assessthe system performance while testing with simu-lated users.In the future, we would like to confirm our pro-posed evaluation measures by testing them on dia-log systems that allows more complicated dialogstructures and systems on other domains.AcknowledgmentsThe authors would like to thank ZhongchaoFei, Zhe Feng, Junkuo Cao, and Baoshi Yanfor their help during the simulation system de-velopment and the three anonymous reviewersfor their insightful suggestions.
All the remain-ing errors are ours.ReferencesH.
Ai, J. Tetreault, and D. Litman.
2007.
ComparingUser Simulation Models for Dialog Strategy Learn-ing.
In Proc.
NAACL-HLT (short paper session).G.
Chung.
2004.
Developing a Flexible Spoken DialogSystem Using Simulation.
In Proc.
of ACL 04.W.
Eckert, E. Levin, and R. Pieraccini.
1997.
UserModeling for Spoken Dialogue System Evaluation.
InProc.
of IEEE workshop on ASRU.E.
Filisko and S. Seneff.
2006.
Learning Decision Mod-els in Spoken Dialogue Systems Via User Simulation.In Proc.
of AAAI Workshop on Statistical and Em-pirical Approaches for Spoken Dialogue Systems.J.
Henderson, O.
Lemon, and K. Georgila.
2005.
HybridReinforcement/Supervised Learning for DialoguePolicies from COMMUNICATOR data.
In IJCAIworkshop on Knowledge and Reasoning in PracticalDialogue Systems.E.
Levin, R. Pieraccini, and W. Eckert.
2000.
A Stochas-tic Model of Human-Machine Interaction For learn-ing Dialogue Strategies.
IEEE Trans.
On Speech andAudio Processing, 8(1):11-23.R.
L?pez-C?zar, A.
De la Torre, J. C. Segura and A. J.Rubio.
(2003).
Assessment of dialogue systems bymeans of a new simulation technique.
Speech Com-munication (40): 387-407.D.
Mirkovic and L. Cavedon.
2005.
Practical multi-domain, multi-device dialogue management,PACLING'05: 6th Meeting of the Pacific Associationfor Computational Linguistics.Sebastian M?ller, Jan Krebber and Paula Smeele.
2006.Evaluating the speech output component of a smart-home system.
Speech Communication (48): 1-27.O.
Pietquin, O.
2004.
A Framework for UnsupervisedLearning of Dialog Strategies.
Ph.D.
diss., FacultePolytechnique de Mons.K.
Scheffler.
2002.
Automatic Design of Spoken DialogSystems.
Ph.D.
diss., Cambridge University.S.
Singh, D. Litman, M. Kearns, and M. Walker.
2002.Optimizing DialogueManagement with Reinforce-ment Learning: Experiments with the NJFun System.Journal of Artificial Intelligence Research (JAIR),vol.
16.J.
Schatzmann, B. Thomson, K. Weilhammer, H. Ye,and Young.
S. 2007.
Agenda-Based User Simulationfor Bootstrapping a POMDP Dialogue System.
InProc.
of NAACL-HLT (short paper session).F.
Weng, S. Varges, B. Raghunathan, F. Ratiu, H. Pon-Barry, B. Lathrop, Q. Zhang, H. Bratt, T. Scheideck,R.
Mishra, K. Xu, M. Purvey, A. Lien, M. Raya, S.Peters, Y. Meng, J. Russell,  L. Cavedon, E. Shri-berg, and H. Schmidt.
2006.
CHAT: A Conversa-tional Helper for Automotive Tasks.
In Proc.
ofInterspeech.F.
Weng, B. Yan, Z. Feng, F. Ratiu, M. Raya, B. Lath-rop, A. Lien, S. Varges, R. Mishra, F. Lin, M. Purver,H.
Bratt, Y. Meng, S. Peters, T. Scheideck, B. Rag-hunathan and Z. Zhang.
2007.
CHAT to your destina-tion.
In Proc.
Of 8th SIGdial workshop on Discourseand Dialogue.J.
Williams and S. Young.
2006.
Partially ObservableMarkov Decision Processes for Spoken Dialog Sys-tems.
Computer Speech and Language.M.
Walker, D. Litman, C. Kamm, and A. Abella.
1997.PARADISE: A Framework for Evaluating SpokenDialogue Agents.
In Proceedings of the 35th ACL.171
