Three Generative, Lexicalised Models for Statistical ParsingMichae l  Collins*Dept.
of Computer  and In format ion  ScienceUnivers i ty  of Pennsy lvan iaPh i ladelphia ,  PA, 19104, U.S.A.mcollins~gradient, cis.
upenn, eduAbst ractIn this paper we first propose a new sta-tistical parsing model, which is a genera-tive model of lexicalised context-free gram-mar.
We then extend the model to in-clude a probabilistic treatment of both sub-categorisation and wh-movement.
Resultson Wall Street Journal text show that theparser performs at 88.1/87.5% constituentprecision/recall, an average improvementof 2.3% over (Collins 96).1 IntroductionGenerative models of syntax have been central inlinguistics since they were introduced in (Chom-sky 57).
Each sentence-tree pair (S,T) in a lan-guage has an associated top-down derivation con-sisting of a sequence of rule applications of a gram-mar.
These models can be extended to be statisti-cal by defining probability distributions at points ofnon-determinism in the derivations, thereby assign-ing a probability 7)(S, T) to each (S, T) pair.
Proba-bilistic context free grammar (Booth and Thompson73) was an early example of a statistical grammar.A PCFG can be lexicalised by associating a head-word with each non-terminal in a parse tree; thusfar, (Magerman 95; Jelinek et al 94) and (Collins96), which both make heavy use of lexical informa-tion, have reported the best statistical parsing per-formance on Wall Street Journal text.
Neither ofthese models is generative, instead they both esti-mate 7)(T\] S) directly.This paper proposes three new parsing models.Mode l  1 is essentially a generative version of themodel described in (Collins 96).
In Mode l  2, weextend the parser to make the complement/adjunctdistinction by adding probabilities over subcategori-sation frames for head-words.
In Model  3 we givea probabilistic treatment of wh-movement, whichThis research was supported by ARPA GrantN6600194-C6043.is derived from the analysis given in GeneralizedPhrase Structure Grammar (Gazdar et al 95).
Thework makes two advances over previous models:First, Model 1 performs significantly better than(Collins 96), and Models 2 and 3 give further im-provements - - our final results are 88.1/87.5% con-stituent precision/recall, an average improvementof 2.3% over (Collins 96).
Second, the parsersin (Collins 96) and (Magerman 95; Jelinek et al94) produce trees without information about wh-movement or subcategorisation.
Most NLP applica-tions will need this information to extract predicate-argument structure from parse trees.In the remainder of this paper we describe the 3models in section 2, discuss practical issues in sec-tion 3, give results in section 4, and give conclusionsin section 5.2 The  Three  Pars ing  Mode ls2.1 Mode l  1In general, a statistical parsing model defines theconditional probability, 7)(T\] S), for each candidateparse tree T for a sentence S. The parser itself isan algorithm which searches for the tree, Tb~st, thatmaximises 7~(T I S).
A generative model uses theobservation that maximising 7V(T, S) is equivalentto maximising 7~(T \] S): 1Tbe,t = argm~xT~(TlS) = argmTax ?~(T,S) ~(s)= arg m~x 7~(T, S) (1)7~(T, S) is then estimated by attaching probabilitiesto a top-down derivation of the tree.
In a PCFG,for a tree derived by n applications of context-freere-write rules LHSi  ~ RHSi, 1 < i < n,7~(T,S) = H 7)(RHSi I LHSi)  (2)i=l .
.nThe re-write rules are either internal to the tree,where LHS is a non-terminal nd RHS is a string7~(T,S) 17~(S) is constant, hence maximising ~ is equiv-alent to maximising "P(T, S).16TOPi S(bought)N P ( w ~ o u g h t  )t VB/~Np m JJ NN NNP I I I ooks)Last  week Marks I 1bought NNPfBrooksTOP -> S(bought)S(bought) -> NP(week)NP(week) -> JJ(Last)NP (Marks) -> NNP (Marks)VP (bought) -> VB (bought)NP (Brooks) -> NNP (Brooks)NP(Marks) VP(bought)NN(week)NP(Brooks)Figure 1: A lexicalised parse tree, and a list of the rules it contains.
For brevity we omit the POS tagassociated with each word.of one or more non-terminals; or lexical, where LHSis a part of speech tag and RHS is a word.A PCFG can be lexicalised 2 by associating a wordw and a part-of-speech (POS) tag t with each non-terminal X in the tree.
Thus we write a non-terminal as X(x), where x = (w,t), and X is aconstituent label.
Each rule now has the form3:P(h) -> Ln(In)...ni(l l)H(h)Rl(rl)...Rm(rm) (3)H is the head-child of the phrase, which inheritsthe head-word h from its parent P. L1...L~ andR1...Rm are left and right modifiers of H. Eithern or m may be zero, and n = m = 0 for unaryrules.
Figure 1 shows a tree which will be used asan example throughout this paper.The addition of lexical heads leads to an enormousnumber of potential rules, making direct estimationof ?
)(RHS { LHS) infeasible because of sparse dataproblems.
We decompose the generation of the RHSof a rule such as (3), given the LHS, into three steps- -  first generating the head, then making the inde-pendence assumptions that the left and right mod-ifiers are generated by separate 0th-order markovprocesses 4:1.
Generate the head constituent label of thephrase, with probability 7)H(H I P, h).2.
Generate modifiers to the right of the headwith probability 1-Ii=1..m+1 ~n(Ri(ri) { P, h, H).R,~+l(r,~+l) is defined as STOP - -  the STOPsymbol is added to the vocabulary of non-terminals, and the model stops generating rightmodifiers when it is generated.2We find lexical heads in Penn treebank data usingrules which are similar to those used by (Magerman 95;Jelinek et al 94).SWith the exception of the top rule in the tree, whichhas the form TOP --+ H(h).4An exception is the first rule in the tree, T0P -+H (h), which has probability Prop (H, hlTOP )3.
Generate modifiers to the left of the head withprobability rL=l..n+ l ?)
L ( L~( li ) l P, h, H), whereLn+l (ln+l) = STOP.For example, the probability of the rule S(bought)-> NP(week) NP(Marks) YP(bought)would  be es-t imated as7~h(YP I S,bought) x ~l(NP(Marks) I S,YP,bought) x7~,(NP(week) { S,VP,bought) x 7~z(STOP I S,VP,bought) x~r(STOP I S, VP, bought)We have made the 0 th order markov assumptions7~,(Li(li) { H, P, h, L1 (ll)...Li-1 (/i-1)) =P~(Li(li) { H,P,h) (4)Pr (Ri (ri) { H, P, h, R1 (rl)...R~- 1 (r i -  1 )) =?~r(Ri(ri) { H, P, h) (5)but in general the probabilities could be conditionedon any of the preceding modifiers.
In fact, if thederivation order is fixed to be depth-first - -  thatis, each modifier recursively generates the sub-treebelow it before the next modifier is generated - -then the model can also condition on any structurebelow the preceding modifiers.
For the moment weexploit this by making the approximations7~l( Li(li ) { H, P, h, Ll ( ll )...Li_l (l~_l ) ) =?
)l(ni(li) l H, P,h, distancez(i - 1)) (6)?
)r( ai(ri) \] H, P, h, R1 (rl)...Ri-1 (ri-l ) ) =?~T(Ri(ri) \[ H,P.h,  distancer(i - 1)) (7)where distancez and distancer are functions of thesurface string from the head word to the edge of theconstituent (see figure 2).
The distance measure isthe same as in (Collins 96), a vector with the fol-lowing 3 elements: (1) is the string of zero length?
(Allowing the model to learn a preference for right-branching structures); (2) does the string contain a17verb?
(Allowing the model to learn a preference formodification of the most recent verb).
(3) Does thestring contain 0, 1, 2 or > 2 commas?
(where acomma is anything tagged as "," or ":").P(h)d is tance -IFigure 2: The next child, Ra(r3), is generated withprobability 7~(R3(r3) \[ P,H, h, distancer(2)).
Thedistance is a function of the surface string from theword after h to the last word of R2, inclusive.
Inprinciple the model could condition on any struc-ture dominated by H, R1 or R2.2.2 Mode l  2: The  complement /ad junctdistinction and subcategor i sat ionThe tree in figure 1 is an example of the importanceof the complement/adjunct distinction.
It would beuseful to identify "Marks" as a subject, and "Lastweek" as an adjunct (temporal modifier), but thisdistinction is not made in the tree, as both NPs arein the same position 5 (sisters to a VP under an Snode).
From here on we will identify complementsby attaching a "-C" suffix to non-terminals - -  fig-ure 3 gives an example tree.TOP1 S(bought)N P ( w ~ o u g h t )Last week Marks VBD NP-C(Brooks)I l bought BrooksFigure 3: A tree with the "-C" suffix used to identifycomplements.
"Marks" and "Brooks" are in subjectand object position respectively.
"Last week" is anadjunct.A post-processing stage could add this detail tothe parser output, but we give two reasons for mak-ing the distinction while parsing: First, identifyingcomplements i  complex enough to warrant a prob-abilistic treatment.
Lexical information is needed5Except "Marks" is closer to the VP, but note that"Marks" is also the subject in "Marks last week boughtBrooks".- -  for example, knowledge that "week '' is likely tobe a temporal modifier.
Knowledge about subcat-egorisation preferences - -  for example that a verbtakes exactly one subject - -  is also required.
Theseproblems are not restricted to NPs, compare "Thespokeswoman said (SBAR that the asbestos wasdangerous)" vs. "Bonds beat short-term invest-ments (SBAR because the market is down)", wherean SBAR headed by "that" is a complement, but anSBAI:t headed by "because" is an adjunct.The second reason for making the comple-ment/adjunct distinction while parsing is that itmay help parsing accuracy.
The assumption thatcomplements are generated independently of eachother often leads to incorrect parses - -  see figure 4for further explanation.2.2.1 Ident i fy ing  Complements  andAd juncts  in the Penn TreebankWe add the "-C" suffix to all non-terminals intraining data which satisfy the following conditions:1.
The non-terminal must be: (1) an NP, SBAR,or S whose parent is an S; (2) an NP, SBAR, S,or VP whose parent is a VP; or (3) an S whoseparent is an SBAR.2.
The non-terminal must not have one of the fol-lowing semantic tags: ADV, VOC, BNF, DIR,EXT, LOC, MNR, TMP, CLR or PRP.
See(Marcus et al 94) for an explanation of whatthese tags signify.
For example, the NP "Lastweek" in figure 1 would have the TMP (tempo-ral) tag; and the SBAR in "(SBAR because themarket is down)", would have the ADV (adver-bial) tag.In addition, the first child following the head of aprepositional phrase is marked as a complement.2.2.2 Probab i l i t ies  over Subcategor i sa t ionF ramesThe model could be retrained on training datawith the enhanced set of non-terminals, and itmight learn the lexical properties which distinguishcomplements and adjuncts ("Marks" vs "week", or"that" vs. "because").
However, it would still sufferfrom the bad independence assumptions illustratedin figure 4.
To solve these kinds of problems, the gen-erative process is extended to include a probabilisticchoice of left and right subcategorisation frames:1.
Choose a head H with probability ~H(H\[P, h).2.
Choose left and right subcat frames, LC andRC, with probabilities 7)~c(LC \[ P, H, h) and18I.
(a) Incorrect S (b) Correct SNP-C  VPNP-C  NP-C  VPI I ~ f ~.
was ADJPNP  NP  Dreyfus the best fund was ADJP \[I I I l ow low Dreyfus the best fund2.
(a) Incorrect S (b) Correct SNP-C  VPNP-C  VP  lI ~ The issue / ~The issue was NP-Cw -C NP  VPa bill a bill funding NP-C funding NP-CI I Congress CongressFigure 4: Two examples where the assumption that modifiers are generated independently of eachother leads to errors.
In (1) the probability of generating both "Dreyfus" and "fund" as sub-jects, 7~(NP-C(Dreyfus) I S,VP,was) * T'(NP-C(fund) I S,VP,was) is unreasonably high.
(2) is similar:7 ~ (NP-C (bill), VP-C (funding) I VP, VB, was) = P(NP-C (bill) I VP, VB, was) * 7~(VP-C (funding) I VP, VB, was)is a bad independence assumption.Prc(RCIP,  H,h ).
Each subcat frame is amultiset 6 specifying the complements which thehead requires in its left or right modifiers.3.
Generate the left and right modifiers with prob-abilities 7)l(Li, li I H, P, h, distancet(i - 1), LC)and 7~r (R~, ri I H, P, h, distancer(i - 1), RC) re-spectively.
Thus the subcat requirements areadded to the conditioning context.
As comple-ments are generated they are removed from theappropriate subcat multiset.
Most importantly,the probability of generating the STOP symbolwill be 0 when the subcat frame is non-empty,and the probability of generating a complementwill be 0 when it is not in the subcat frame;thus all and only the required complements willbe generated.The probability of the phrase S(bought ) ->NP(week) NP-C(Marks) VP(bought)is now:7)h(VPIS,bought) xto({NP-C} I S,VP,bought) x t S,VP,bought) ?7~/(NP-C(Marks) IS ,VP,bought, {NP-C}) x7:~I(NP(week) I S ,VP ,bought, {}) x7)l(STOe I S ,ve ,bought, {}) ?Pr(STOP I S, VP,bought, {})Here the head initially decides to take a sin-gle NP-C (subject) to its left, and no complements~A rnultiset, or bag, is a set which may contain du-plicate non-terminal labels.to its right.
NP-C(Marks) is immediately gener-ated as the required subject, and NP-C is removedfrom LC, leaving it empty when the next modi-fier, NP(week) is generated.
The incorrect struc-tures in figure 4 should now have low probabil-ity because ~Ic({NP-C,NP-C} \[ S,VP,bought) and"Prc({NP-C,VP-C} I VP,VB,was) are small.2.3 Model  3: Traces and Wh-MovementAnother obstacle to extracting predicate-argumentstructure from parse trees is wh-movement.
Thissection describes a probabilistic treatment of extrac-tion from relative clauses.
Noun phrases are most of-ten extracted from subject position, object position,or from within PPs:Example  1 The store (SBAR which TRACEbought Brooks Brothers)Example  2 The store (SBAR which Marks boughtTRACE)Example  3 The store (SBAR which Marks boughtBrooks Brothers/tom TRACE)It might be possible to write rule-based patternswhich identify traces in a parse tree.
However, weargue again that this task is best integrated intothe parser: the task is complex enough to warranta probabilistic treatment, and integration may helpparsing accuracy.
A couple of complexities are thatmodification by an SBAR does not always involveextraction (e.g., "the fact (SBAR that besoboru is19NP(store)NP(store)  SBAR(that)(+gap)The storeWHNP(that)WDTIthat(i) NP -> NP(2) SBAR(+gap) -> WHNP(3) S(+gap) -> NP-C(4) VP(+gap) -> VBS(bought )(-}-gap)N P - C ( ~ h t )  (--{-gap)I B ~ wMarksV eek)I I bought last weekSBAR(+gap)S-C(+gap)VP(+gap)TRACE NPFigure 5: A +gap feature can be added to non-terminals to describe NP extraction.
The top-level NPinitially generates an SBAR modifier, but specifies that it must contain an NP trace by adding the +gapfeature.
The gap is then passed down through the tree, until it is discharged as a TRACE complement tothe right of bought.played with a ball and a bat)"), and it is not un-common for extraction to occur through several con-stituents, (e.g., "The changes (SBAR that he saidthe government was prepared to make TRACE)").The second reason for an integrated treatmentof traces is to improve the parameterisation f themodel.
In particular, the subcategorisation proba-bilities are smeared by extraction.
In examples 1, 2and 3 above 'bought' is a transitive verb, but with-out knowledge of traces example 2 in training datawill contribute to the probability of 'bought' beingan intransitive verb.Formalisms imilar to GPSG (Gazdar et al 95)handle NP extraction by adding a gap feature toeach non-terminal in the tree, and propagating gapsthrough the tree until they are finally discharged as atrace complement (see figure 5).
In extraction casesthe Penn treebank annotation co-indexes a TRACEwith the WHNP head of the SBAR, so it is straight-forward to add this information to trees in trainingdata.Given that the LHS of the rule has a gap, thereare 3 ways  that the gap  can be passed down to theRHS:Head The gap is passed to the head of the phrase,as in rule (3) in figure 5.Left,  R ight  The gap is passed on recursively to oneof the left or right modifiers of the head, or isdischarged as a trace argument to the left/rightof the head.
In rule (2) it is passed on to a rightmodifier, the S complement.
In rule (4) a traceis generated to the right of the head VB.We specify a parameter 7~c(GIP, h, H) where Gis either Head,  Left or Right.
The generative pro-cess is extended to choose between these cases aftergenerating the head of the phrase.
The rest of thephrase is then generated in different ways depend-ing on how the gap is propagated: In the Headcase the left and right modifiers are generated asnormal.
In the Left,  R ight  cases a gap require-ment is added to either the left or right SUBCATvariable.
This requirement is fulfilled (and removedfrom the subcat list) when a trace or a modifiernon-terminal which has the +gap feature is gener-ated.
For example, Rule (2), SBAR(that) (+gap) ->WHNP(that) S-C(bought) (+gap), has probability~h (WHNP I SBAR, that) ?
7~G (Right I SBAR, WHNP, that) xT~LC({} I SBAR,WHNP,that) xT'Rc({S-C} \[ SBAR,WHNP, that) x7~R (S-C (bought) (+gap) \[ SBAR, WHNP, that, {S-C, +gap}) x7~R(STOP I SBAR,WHNP,that, {}) xPC (STOP I SBAR, WHNP, that, { })Rule (4), VP(bought) (+gap) -> VB(bought)TRACE NP (week), has probability7~h(VB IVP,bought) x PG(Right I VP,bought,VB) xPLC({} IVP,bought,VB) x ~PRc({NP-C} I vP,bought,VB) x7~R(TRACE IVP,bought,VB, {NP-C, +gap}) xPR(NP(week) I VP,bought ,VB, {}) ?7)L(STOP I VP,bought,VB, {}) x7~R (STOP I VP ,bought ,VB, {})In rule (2) Right is chosen, so the +gap requirementis added to RC.
Generation of S -C(bought) (+gap)20(a) H(+) =~ P(-)?
H(+)Prob =X Pr?b = X'X~H(HIP,.
.
.
)(b) P(-) + Ri(+) =~H R1Prob -= X Prob = YFigure 6: The life of a constituent in the chart.
(c) P(-) =~ P(+)Prob = X Prob = X X'PL(STOP I .... )xPR(STOP I .... )P(-)?
.
H R1 RiProb = X x Y x ~R(Ri(ri) I P,H,...)(+) means a constituent is complete (i.e.
it includes thestop probabilities), ( - )  means a constituent is incomplete.
(a) a new constituent is started by projecting acomplete rule upwards; (b) the constituent then takes left and right modifiers (or none if it is unary).
(c)finally, STOP probabilities are added to complete the constituent.Back-off "PH(H I"-) Pa(G I ...) PL~(Li(It,) I..-)Level PLc(LC t ...) Pm(Ri(rti) I...)7)Rc(RC I ...)1 P, w, t P, H, w, t P, H, w, t, A, LC2 P, t P, H, t P, H, t, A, LC3 P P, H P, H, &, LC4 - -PL2(lwi l ...)PR2(rwi I ...)Li, Iti, P, H, w, t, A, LCL,, lti, P, H, t, A, LCLI, ltiIt~Table 1: The conditioning variables for each level of back-off.
For example, T'H estimation interpolatesel = ~?H(H I P, w, t), e2 = 7~H(H I P, t), and e3 = PH(H I P).
A is the distance measure.
:ulfills both the S-C and +gap requirements in RC.In rule (4) R ight  is chosen again.
Note that gen-eration of trace satisfies both the NP-C and +gapsubcat requirements.3 P rac t i ca l  I ssues3.1 Smooth ing  and  Unknown WordsTable 1 shows the various levels of back-off or eachtype of parameter in the model.
Note that we de-compose "PL(Li(lwi,lti) I P, H ,w, t ,~ ,LC)  (wherelwi and Iti are the word and POS tag generatedwith non-terminal Li, A is the distance measure)into the product 79L1(Li(lti) I P, H,w,t ,  Zx,LC) x7~ L2(lwi ILi, lti, 19, H, w, t, A, LC), and then smooththese two probabilities separately (Jason Eisner,p.c.).
In each case 7 the final estimate ise----Ale1 + (1 - &l)(A2e2 + (1 - &2)ea)where ex, e2 and e3 are maximum likelihood esti-mates with the context at levels 1, 2 and 3 in thetable, and ,kl, ,k2 and )~3 are smoothing parameterswhere 0 _< ,ki _< 1.
All words occurring less than 5times in training data, and words in test data whichrExcept cases L2 and R2, which have 4 levels, so thate = ~le t  + (1 -- *X1)()~2e2 + (1 - ,~2)(&3e3 + (1 - ~3)e4) ) .have never been seen in training, are replaced withthe "UNKNOWN" token.
This allows the model torobustly handle the statistics for rare or new words.3.2 Par t  o f  Speech  Tagg ing  and  Pars ingPart of speech tags are generated along with thewords in this model.
When parsing, the POS tags al-lowed for each word are limited to those which havebeen seen in training data for that word.
For un-known words, the output from the tagger describedin (Ratnaparkhi 96) is used as the single possible tagfor that word.
A CKY style dynamic programmingchart parser is used to find the maximum probabilitytree for each sentence (see figure 6).4 Resu l t sThe parser was trained on sections 02 - 21 of the WallStreet Journal portion of the Penn Treebank (Mar-cus et al 93) (approximately 40,000 sentences), andtested on section 23 (2,416 sentences).
We use thePAR.SEVAL measures (Black et al 91) to compareperformance:Labe led  Prec i s ion  =number of  correct constituents in proposed parsenumber of  constituents in proposed parse21MODEL(Magerman 95)(Collins 96)Model 1Model 2Model 3~ c e ~ )  2 CBs84.6% 84.9% 1.26 56.6% 81.4% 84.0% 84.3% 1.46 54.0%85.8% 86.3% 1.14 59.9% 83.6% 85.3% 85.7% 1.32 57.2%87.4% 88.1% 0.96 65.7% 86.3% 86.8% 87.6% 1.11 63.1%88.1% 88.6% 0.91 66.5% 86.9% 87.5% 88.1% 1.07 63.9%88.1% 88.6% 0.91 66.4% 86.9% 87.5% 88.1% 1.07 63.9%78.8%80.8%84.1%84.6%84.6%Table 2: Results on Section 23 of the WSJ Treebank.
LR /LP  = labeled recall/precision.
CBs is the averagenumber of crossing brackets per sentence.
0 CBs, < 2 CBs are the percentage of sentences with 0 or < 2crossing brackets respectively.Labeled Recall -~number o/  correct constituents in proposed parsenumber of  constituents in treebank parseCrossing Brackets ---- number of con-stituents which violate constituent boundarieswith a constituent in the treebank parse.For a constituent to be 'correct' it must span thesame set of words (ignoring punctuation, i.e.
all to-kens tagged as commas, colons or quotes) and havethe same label s as a constituent in the treebankparse.
Table 2 shows the results for Models 1, 2 and3.
The precision/recall of the traces found by Model3 was 93.3%/90.1% (out of 436 cases in section 23of the treebank), where three criteria must be metfor a trace to be "correct": (1) it must be an argu-ment to the correct head-word; (2) it must be in thecorrect position in relation to that head word (pre-ceding or following); (3) it must be dominated by thecorrect non-terminal label.
For example, in figure 5the trace is an argument to bought,  which it fol-lows, and it is dominated by a VP.
Of the 436 cases,342 were string-vacuous extraction from subject po-sition, recovered with 97.1%/98.2% precision/recall;and 94 were longer distance cases, recovered with76%/60.6% precision/recall 94.1 Compar i son  to previous workModel 1 is similar in structure to (Collins 96) - -the major differences being that the "score" for eachbigram dependency is 7't(L{,liIH, P, h, distancet)8(Magerman 95) collapses ADVP and PRT to the samelabel, for comparison we also removed this distinctionwhen calculating scores.9We exclude infinitival relative clauses from these fig-ures, for example "I called a plumber TRACE to fix thesink" where 'plumber' is co-indexed with the trace sub-ject of the infinitival.
The algorithm scored 41%/18%precision/recall on the 60 cases in section 23 - -  but in-finitival relatives are extremely difficult even for humanannotators to distinguish from purpose clauses (in thiscase, the infinitival could be a purpose clause modifying'called') (Ann Taylor, p.c.
)rather than Pz(Li, P, H I li, h, distancel), and thatthere are the additional probabilities of generat-ing the head and the STOP symbols for each con-stituent.
However, Model 1 has some advantageswhich may account for the improved performance.The model in (Collins 96) is deficient, that is formost sentences S, Y~T 7~( T \] S) < 1, because prob-ability mass is lost to dependency structures whichviolate the hard constraint that no links may cross.For reasons we do not have space to describe here,Model 1 has advantages in its treatment of unaryrules and the distance measure.
The generativemodel can condition on any structure that has beenpreviously generated - -  we exploit this in models 2and 3 - -  whereas (Collins 96) is restricted to condi-tioning on features of the surface string alone.
(Charniak 95) also uses a lexicalised genera-tive model.
In our notation, he decomposesP(RHSi  l LHSi) as "P(R,~...R1HL1..Lm \]P,h) x1-L=I..~ 7~(r~l P, Ri, h) x I-L=l..m 7)(lil P, Li, h).
ThePenn treebank annotation style leads to a verylarge number of context-free rules, so that directlyestimating 7~(R .... R1HL1..Lm I P, h) may lead tosparse data problems, or problems with coverage(a rule which has never been seen in training maybe required for a test data sentence).
The com-plement/adjunct distinction and traces increase thenumber of rules, compounding this problem.
(Eisner 96) proposes 3 dependency models, andgives results that show that a generative model sim-ilar to Model 1 performs best of the three.
However,a pure dependency model omits non-terminal infor-mation, which is important.
For example, "hope" islikely to generate a VP(T0) modifier (e.g., I hope\[VP to sleep\]) whereas "'require" is likely to gen-erate an S(T0) modifier (e.g., I require IS Jim tosleep\]), but omitting non-terminals conflates thesetwo cases, giving high probability to incorrect struc-tures such as "I hope \[Jim to sleep\]" or "I require \[tosleep\]".
(Alshawi 96) extends a generative depen-dency model to include an additional state variablewhich is equivalent to having non-terminals - -  his22suggestions may be close to our models 1 and 2, buthe does not fully specify the details of his model, anddoesn't give results for parsing accuracy.
(Miller etal.
96) describe a model where the RHS of a rule isgenerated by a Markov process, although the pro-cess is not head-centered.
They increase the set ofnon-terminals by adding semantic labels rather thanby adding lexical head-words.
(Magerman 95; Jelinek et al 94) describe ahistory-based approach which uses decision trees toestimate 7a(T\[S).
Our models use much less sophis-ticated n-gram estimation methods, and might wellbenefit from methods uch as decision-tree estima-tion which could condition on richer history thanjust surface distance.There has recently been interest in usingdependency-based parsing models in speech recog-nition, for example (Stolcke 96).
It is interesting tonote that Models 1, 2 or 3 could be used as lan-guage models.
The probability for any sentence canbe estimated as P(S) = ~~.TP(T,S), or (makinga Viterbi approximation for efficiency reasons) as7)(S) .~ P(Tb~st, S).
We intend to perform experi-ments to compare the perplexity of the various mod-els, and a structurally similar 'pure' PCFG 1?.5 ConclusionsThis paper has proposed a generative, lexicalised,probabilistic parsing model.
We have shown that lin-guistically fundamental ideas, namely subcategori-sation and wh-movement, can be given a statisticalinterpretation.
This improves parsing performance,and, more importantly, adds useful information tothe parser's output.6 AcknowledgementsI would like to thank Mitch Marcus, Jason Eisner,Dan Melamed and Adwait Ratnaparkhi for manyuseful discussions, and comments on earlier versionsof this paper.
This work has also benefited greatlyfrom suggestions and advice from Scott Miller.ReferencesH.
Alshawi.
1996.
Head Automata and BilingualTiling: Translation with Minimal Representa-tions.
Proceedings of the 3~th Annual Meetingof the Association for Computational Linguistics,pages 167-176.E.
Black et al 1991.
A Procedure for Quantita-tively Comparing the Syntactic Coverage of En-glish Grammars.
Proceedings of the February 1991DARPA Speech and Natural Language Workshop.1?Thanks to one of the anonymous reviewers for sug-gesting these experiments.T.
L. Booth and R. A. Thompson.
1973.
ApplyingProbability Measures to Abstract Languages.
IEEETransactions on Computers, C-22(5), pages 442-450.E.
Charniak.
1995.
Parsing with Context-Free Gram-mars and Word Statistics.
Technical Report CS-95-28, Dept.
of Computer Science, Brown Univer-sity.N.
Chomsky.
1957.
Syntactic Structures, Mouton,The Hague.M.
J. Collins.
1996.
A New Statistical Parser Basedon Bigram Lexical Dependencies.
Proceedings o/the 34th Annual Meeting o/ the Association forComputational Linguistics, pages 184-191.J.
Eisner.
1996.
Three New Probabilistic Models forDependency Parsing: An Exploration.
Proceed-ings o/ COLING-96, pages 340-345.G.
Gazdar, E.H. Klein, G.K. Pullum, I.A.
Sag.
1985.Generalized Phrase Structure Grammar.
HarvardUniversity Press.F.
Jelinek, J. Lafferty, D. Magerman, R. Mercer, A.Ratnaparkhi, S. Roukos.
1994.
Decision Tree Pars-ing using a Hidden Derivation Model.
Proceedingso/ the 1994 Human Language Technology Work-shop, pages 272-277.D.
Magermaa.
1995.
Statistical Decision-Tree Mod-els for Parsing.
Proceedings o/ the 33rd AnnualMeeting o\] the Association for ComputationalLinguistics, pages 276-283.M.
Marcus, B. Santorini and M. Marcinkiewicz.1993.
Building a Large Annotated Corpus of En-glish: the Penn Treebank.
Computational Linguis-tics, 19(2):313-330.M.
Marcus, G. Kim, M. A. Marcinkiewicz, R.MacIntyre, A. Bies, M. Ferguson, K. Katz, B.Schasberger.
1994.
The Penn Treebank: Annotat-ing Predicate Argument Structure.
Proceedings ofthe 1994 Human Language Technology Workshop,pages 110~115.S.
Miller, D. Staliard and R. Schwartz.
1996.
AFully Statistical Approach to Natural LanguageInterfaces.
Proceedings o/ the 34th Annual Meetingof the Association for Computational Linguistics,pages 55-61.A.
Ratnaparkhi.
1996.
A Maximum Entropy Modelfor Part-Of-Speech Tagging.
Conference on Em-pirical Methods in Natural Language Processing.A.
Stolcke.
1996.
Linguistic Dependency Modeling.Proceedings of ICSLP 96, Fourth InternationalConference on Spoken Language Processing.23
