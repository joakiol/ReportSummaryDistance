Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 155?162,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsExploiting Morphological, Grammatical, and Semantic Correlates forImproved Text Difficulty AssessmentElizabeth Salesky, Wade Shen?MIT Lincoln Laboratory Human Language Technology Group, 244 Wood Street, Lexington MA 02420, USA{elizabeth.salesky, swade}@ll.mit.eduAbstractWe present a low-resource, language-independent system for text difficulty as-sessment.
We replicate and improve upona baseline by Shen et al.
(2013) on theInteragency Language Roundtable (ILR)scale.
Our work demonstrates that the ad-dition of morphological, information the-oretic, and language modeling features toa traditional readability baseline greatlybenefits our performance.
We use theMargin-Infused Relaxed Algorithm andSupport Vector Machines for experimentson Arabic, Dari, English, and Pashto, andprovide a detailed analysis of our results.1 IntroductionWhile there is a growing breadth of reading mate-rials available in various languages, finding perti-nent documents at suitable reading levels remainsdifficult.
Information retrieval methods can findresources with desired vocabulary, but educatorsstill need to filter these to find appropriate diffi-culty levels.
This task is often more challeng-ing than manually adapting the documents them-selves.
Reading level assessment systems can beused to automatically find documents at specificInteragency Language Roundtable (ILR) levels,aiding both instructors and learners by providingproficiency-tailored materials.While interest in readability assessment hasbeen gaining momentum in many languages, themajority of previous work is language-specific.Shen et al.
(2013) introduced a baseline forlanguage-independent text difficulty assessment,based on the ILR proficiency scale.
In this work,we replicate and extend their results.
?This work is sponsored by the Defense Language In-stitute under Air Force Contract FA8721-05-C-0002.
Opin-ions, interpretations, conclusions and recommendations arethose of the authors and are not necessarily endorsed by theUnited States Government.The ILR scale is the standard language profi-ciency measure for the U.S. federal government.It ranges from no proficiency to native proficiencyon a scale of 0-5, with half-level denotationswhere proficiency meets some but not all of thecriteria for the next level (Interagency LanguageRoundtable, 2013).
For second language learners,it is sufficient to use up to ILR level 4.
Since profi-ciency is a continuous spectrum, text difficulty as-sessment is often treated as a regression problem,as we do here.
Though the ILR levels may ap-pear to be discrete categories, documents can fallbetween levels.
The degree to which they do isimportant for us to measure.Level Description1 Elementary: can fulfill basic needs,limited to fundamental vocabulary2 Limited working: routine social demands,gist of non-technical works, elementarygrasp of grammar3 General professional: general vocabulary,good control of grammar, errors do notinterfere with understanding4 Advanced professional: fluent languageuse on all levels, only rare & minute errorsTable 1: Description of proficiency at ILR levelsThe ILR scale addresses semantic and gram-matical capabilities, and to model it appropri-ately, a system needs to reflect both.
The base-line system developed by Shen et al.
(2013)uses both term frequency log-weighted (TFLOG)word-usage features and z-normalized word, sen-tence, and document length features.
However,their results are not equally significant across itsset of test languages, which this paper addresseswith additional features.The utilization of types for TFLOG weightedvectors is not as representative for morpholog-ically rich languages, where multiple types canrepresent different word-forms within a single155paradigm.
By incorporating morphology, we canimprove our TFLOG vectors?
representation ofsemantic complexity for these languages.
Weemploy the Morfessor Categories-MAP algorithmfor segmentation (Creutz & Lagus, 2007).
Rela-tive entropy and statistical language models (LMs)can also measure semantic complexity, and class-based language models (cLMs) can give us a mea-sure of the grammatical complexity of the text.
Allof these methods are low-resource and unsuper-vised; they can be easily applied to new languages.We have compared their performance to language-specific methods where possible.The remainder of this paper is structured as fol-lows; Section 2 summarizes previous research onreadability assessment.
Section 3 introduces ourcorpus and approach, while Section 4 details ourresults and their analyses.
Section 5 provides asummary and description of future work.2 Background & Related WorkEarly work on readability assessment approxi-mated grammatical and lexical complexity us-ing shallow features like sentence length and thenumber of syllables in a word, like the promi-nent Flesch-Kincaid measure, in large part dueto their low computational cost (Kincaid et al.,1975).
Such features over-generalize what makesa text difficult; it is not always the case that longerwords and sentences are more grammatically com-plex than their shorter counterparts.
Subsequentwork such as the Dale-Chall model (Dale & Chall,1995) added representation on static word lists:in this case, one of 3,000 words familiar to 4thgraders.
Such lists, however, are not readily avail-able for many difficulty scales and languages.Ensuing approaches have employed more so-phisticated methods, such as word frequency es-timates to measure lexical complexity (Stenner,1996) and statistical language models to measuresemantic and syntactic complexity, and have seensignificant performance gains over previous work(Collins-Thompson & Callan, 2004; Schwarm &Ostendorf, 2005; Petersen & Ostendorf, 2009).
Inthe case of Heilman et al.
(2007), the combina-tion of lexical and grammatical features specifi-cally addressed the order in which vocabulary andgrammar are acquired by second language learn-ers, where grasp of grammar often trails othermarkers of proficiency.The extension of readability research to lan-guages beyond English necessitated the introduc-tion of new features such as morphology, whichhave long been proven useful in other areas.Dell?Orletta et al.
(2011) developed a two-classreadability model for Italian based on its verbalmorphology.
Franc?ois and Fairon (2012) built asix-class readability model, but for adult learnersof French, utilizing verb tense and mood-basedfeatures.
Most recently, Hancke et al.
(2012) builta two-class German reading level assessment sys-tem heavily utilizing morphology.
In addition totraditional syntactic, lexical, and language model-ing features used in English readability research,Hancke et al.
(2012) tested a broad range of fea-tures based on German inflectional and deriva-tional morphology.
While all of these systemswere very effective, they required many language-specific resources, including part-of-speech tags.Recent experiments have several noteworthycharacteristics in common.
While some systemsdiscriminate between multiple grade-level cate-gories, most are two- or three-class classifica-tion tasks between ?easy?
and ?difficult?
which donot require such fine-grained feature discrimina-tion.
Outside of English, there are few multi-levelgraded datasets; for those that do exist, they arevery small, averaging less than a hundred labeleddocuments per level.
Further, though recent workhas been increasingly motivated by second lan-guage learners, most systems have only been im-plemented for a single language (Schwarm & Os-tendorf, 2005; Petersen & Ostendorf, 2009); Va-jjala & Meurers, 2012).
The language-specificmorphological and syntactic features used bymany systems outside of English would make itdifficult to apply them to other languages.
Shen etal.
(2013) address this problem by using language-independent features and testing their work onfour languages.
In this work, we extend their sys-tem in order to improve upon their results.3 Approach3.1 CorpusWe conducted our experiments on the corpus usedby Shen et al.
(2013).
The dataset was collected bythe Defense Language Institute Foreign LanguageCenter (DLIFLC) for instructional use.
It com-prises approximately 1390 documents for each ofArabic, Dari, English, and Pashto.
The documentsare evenly distributed across seven test ILR levels:{1, 1+, 2, 2+, 3, 3+, 4}.
This equates to close to156200 documents per level per language.
We use an80/20 train test split.Lang.
Tokens Types StemsMorphs/ WordArabic 593,113 84,160 14,591 2.60Dari 761,412 43,942 13,312 2.61English 796,406 44,738 35,594 1.80Pashto 840,673 59,031 20,015 2.34Table 2: Corpus statisticsThe documents were chosen by language in-structors as representative of a particular level andrange from news articles to excerpts from philos-ophy to craigslist postings.
Three graders hand-leveled each document.
The corpus is annotateonly with the aggregate scores; we use only thisscore for comparison.
The creation of the corpustook 70 hours per language on average.
We as-sume the ILR scale is linear and measure perfor-mance by mean squared error (MSE), typical forregression.
MSE reflects the variance and bias ofour predictions, and is therefore a good measureof performance uniformity within levels.3.2 Experimental DesignWe compare our results to the best performing Su-port Vector Machine (SVM) and Margin-InfusedRelaxed Algorithm (MIRA) baselines from Shenet al.
(2013).
Both of these baselines have thesame features: TFLOG weighted word vectors,average sentence length by document, averageword length by document, and document wordcount.
We used an implementation of the MIRAalgorithm for regression (Crammer & Singer,2003).
We embedded Morfessor for unsupervisedmorphological segmentation and preprocessed ourdata as required by this algorithm (Creutz & La-gus, 2007).
To verify our results across classifiers,we compare with SVM (Chang & Lin, 2001).We also compare Morfessor to ParaMor (Mon-son 2009), an unsupervised system with a differ-ent level of segmentation aggression, as well as tolanguage-specific analyzers.Our experiments apply word-usage features,shallow length features, and language models.
Forthe first, we compare TFLOG vectors based onword types, all morphemes, and stems only.
Forthe second, we tested the three baseline shallowlength features (average word length in charactersper document, average sentence length per docu-ment, and document word count) as well as mea-sures of relative entropy, average stem fertility, av-erage morphemes per word, and the ratio of typesto tokens.
Of these, only relative entropy posi-tively impacted performance, and only its resultsare reported in this paper.
All length features werez-normalized.
We compare both word- and class-based language models.
We trained LMs for eachILR level and used the document perplexity mea-sured against each as features.Optimal settings were determined by sweepingalgorithm parameters, and Morfessor?s perplexitythreshold for each language.
We conducted a fea-ture analysis for all combinations of word, length,and LM features across all four languages.4 Results & AnalysisWe first replicate the baseline results of Shen etal.
(2013) using both the MIRA and SVM algo-rithms.
We find there is very overall little perfor-mance difference between the two algorithms, andthe difference is language-dependent.
It is incon-clusive which algorithm performs best.Algorithm AR DA EN PAMIRA 0.216 0.296 0.154 0.348SVM 0.198 0.301 0.147 0.391Table 3: Baseline results in MSE, SVM vs. MIRATable 3 shows the averageMSE across the sevenILR levels for each language.
Figure 1 depictsMSE performance on each individual ILR level.Figure 1: MSE by ILR level, baseline4.1 Morphological AnalysisReading level assessment in English does not ne-cessitate the use of morphological features, and so157they have not been researched for this task untilrecently.
Morphology has long been shown to beuseful in other areas; it is unsurprising that seg-mentation should help with this task for morpho-logically rich languages.
What we demonstrateis that unsupervised methods perform similarly tolanguage-specific methods, at a lower cost.Language TYPES MORPHS STEMSArabic 0.216 0.198 0.208Dari 0.296 0.304 0.294English 0.154 0.151 0.151Pashto 0.348 0.303 0.293Table 4: Average MSE results comparing the useof types, all morphs, and stems for TFLOG vec-tors.
Morfessor algorithm used for segmentation.Table 4 compares the performance of the base-line, which utilizes types for its TFLOG weightedword vectors, to our configurations that alterna-tively use all morphemes or stems only.
We seethat morphological information improves perfor-mance for all cases but one, all morphs for Dari,and that using stems only shows the greatest im-provement.Our greatest improvement was seen in Pashto,which has the most unique stems in our datasetboth outright and compared to types (see Table4).
Without stemming, TFLOG word vectors wereheavily biased by the frequency of alternate wordforms within a paradigm.
With stemming, whichreduced overall MSE compared to the baseline by16%, the number of word vectors in the optimizedconfiguration increased by 18%, and were muchmore diverse, reflecting the actual semantic com-plexity of the documents.
We posit that the rea-son Dari, which has a similar ratio of morphemesper word to Pashto, does not improve in this wayis due to its much smaller and more uniform vo-cabulary in our data.
Our Pashto documents have1.5 times as many unique words as our Dari, andin fact, with stemming, the number of word vec-tors utilized in our optimized configuration wasreduced by 20%, as fewer units were necessary toreflect the same content.We compare our results using Morfessor to an-other unsupervised segmentation system, ParaMor(Monson 2009).
ParaMor is built on a differ-ent mathematical framework than Morfessor, andso has a very different splitting pattern.
Morfes-sor has a tunable perplexity threshold that dic-tates how aggressively the algorithm segments.Even set at its highest, ParaMor still segmentsmuch more aggressively, sometimes isolating sin-gle characters, which can be useful for down-stream applications (Kurimo et al.
2009).
This isnot the case here, as shown in Table 5.
All furtherresults use Morfessor for stemming.Algorithm AR DA EN PAMorfessor 0.208 0.294 0.151 0.293ParaMor 0.227 0.321 0.158 0.301Table 5: Comparison of unsupervised segmentersTo our knowledge, no Pashto-specific morpho-logical analyzer yet exists for comparison.
How-ever, in lacking both a standardized writing systemand spelling conventions, one word in Pashto maybe written in many different ways (Kathol, 2005).To account for this, we normalized the data us-ing the Levenshtein distance between types.
Weswept possible cutoff thresholds up to 0.25, eval-uated by the overall MSE of the subsequent re-sults.
Using normalized data did not improve re-sults; in many cases the edit distance between al-ternate misspellings is just as high or higher as thedistance between word types.We believe that the limited change in Dari per-formance is primarily related to corpus character-istics; relatively uniform data provides low per-plexity, making it more difficult for Morfessor todiscover all morphological segmentations.
Usingthe Perstem stemmer in place of Morfessor, thenumber of word vectors in the optimized systemrose 143% and our results improved 8%.
Thisincrease affirms that Morfessor is under-splitting.Perstem is tailored to Farsi, and while the two di-alects are mutually intelligible, they have gram-matical, phonological, and loan word differences(Shah et al.
2007).We highlight that the overall MSE of all config-urations in Table 4 vary only 2% for English, withidentical results using all morphs and only stems.This is expected, as English is not morphologi-cally complex.
Given the readily available rule-based systems for English, we compared resultswith Morfessor to the traditional Porter and Paicestemmers, as well as the multi-lingual FreeLingstemmer, as seen in Table 6.Performance variance between all analyzers ofonly 3% points us to the similar and limited gram-matical rules found in the different algorithms, aswell as the relatively limited number of unique158Baseline Morf.
Porter Paice FreeLing0.154 0.151 0.149 0.148 0.153Table 6: Comparison of English segmentersstems and affixes to be found in English.
Topicalsimilarities in our data are also possible.Like Pashto, Arabic has a rich morphologi-cal structure, but in addition to affixes it con-tains templatic morphology.
It is difficult for un-supervised analyzers not specifically tailored totemplatic morphology to capture non-contiguousmorphemes.
Here, Morfessor consistently seg-ments vowelized types into sequences of two char-acter stems.
When compared with MADA, arule-based Arabic analyzer (Habash, 2010), wefound that Morfessor outperformed MADA by10%.
This is likely because the representationspresent in the dataset are what is significant; if aform is ?morphologically correct?
but perpetuatesa sparsity problem, linguistically-accurate stem-ming will not help.
Neither stemmer contributesmuch to Arabic results, however, as MIRA doesnot weight word-usage features very heavily foreither Arabic analyzer.4.2 Relative Entropy and Word LMsAs mentioned in Section 2, traditional featureslike document word count and average sentencelength overstate the importance of length to diffi-culty.
To capture the significance of the length ofthe document, rather than merely the length itself,we utilized relative entropy.
Relative entropy, alsoknown as the Kullback-Leibler divergence (KL),is a measure of the information lost by using oneprobability distribution as compared to another.Expressed as an equation, we have:D(p, q) =Xx2"p(x) logp(x)q(x).
(1)In this work, we are comparing a unigram prob-ability distribution of a document q(x) to a uni-form distribution over the same length p(x).
Thisprovides both a measure of the semantic and struc-tural complexity of a document, allowing us todifferentiate between documents of similar length.Figure 2 shows the normalized distribution of therelative entropy feature for Pashto.The separability of ILR levels suggests we willbe able to discriminate between them.
As demon-strated by the improved performance in Figure 3,where the inclusion of relative entropy is super-Figure 2: Pashto, normalized KL distributionimposed over the baseline, this feature greatly con-tributes to the separability of outlier levels of ourcorpus.
Common z-scores between levels 2 and3 explain the system?s poorer performance on theILR levels 2.0 and 2.5 (Figure 3).
Adding the rel-ative entropy feature to the baseline produced anaverage MSE reduction of 15%.Figure 3: MSE by ILR level, baseline +stems +KLThe combination of stemming for TFLOG vec-tors and relative entropy together is more effec-tive than either alone.
Further removing docu-ment word count improved performance by anaverage 1%.
As seen in Figure 3, the combi-nation of all these changes produces significantgains over the baseline, particularly in Dari andPashto.
The combination configuration reducedoverall MSE by 52% for Pashto documents andby 18% for Dari.
From Figure 3 above, we seethat the +stems+KL configuration exhibits verypoor performance in Arabic level 4, and on outly-ing levels for Dari.
While these MSE values areclear outliers in this figure, they values are lessthan 0.1 greater than their MIRA baseline coun-159terparts.
This may be due to data similarity be-tween level 3+ and 4 documents, or MIRA mayhave been overfit during training.
In contrast, thevariance for English and Pashto is much smaller;overall, the variance has been greatly reduced.Statistical language models (LMs) are a proba-bility distribution over text.
An n-gram languagepredicts a word wngiven the preceding contextw1...wn 1.
We used the SRI Language Model-ing Toolkit to train LMs on our training data foreach ILR level (Stolcke, 2002).
To account forunseen n-grams, we used Kneser-Ney smoothing.To score documents against these LMs, we calcu-late their perplexity (PP), a measure of how wella probability distribution represents data.
Perplex-ity represents the average number of bits neces-sary to encode each word.
For each document inour dataset, we use the perplexities against eachILR level LM as features in MIRA.
We comparedn-gram orders 2-5, and while we found an aver-age decrease of 3% MSE between orders 2 and3 across languages, there was a difference of lessthan 1% between 3-gram and 5-gram LMs.Features AR DA EN PAbaseline 0.216 0.296 0.154 0.348+stems +KL 0.208 0.269 0.147 0.173+LM 0.208 0.176 0.117 0.171+LM -WVs 0.567 0.314 0.338 0.355+stems +KL+LM0.168 0.167 0.096 0.137Table 7: Average MSE results comparing featuresfrom Sections 4.1 and 4.2.
LMs are order 5.As we can see from Table 7, the addition of lan-guage models alone can provide a huge measureof improvement from the baseline.
For Arabic andPashto, it is the same improvement seen by stem-ming TFLOG vectors and adding relative entropy.For Dari and English, however, the performanceimprovement is unmatched by any other featurespresented thus far.
We compare these results tothe same configuration without TFLOG vectors,in order to measure the overlap between these fea-tures; see Table 7.
Based on the relative results,it seems that word vector and LM features are or-thogonal.
The addition of all three new features(stemmed word vectors, relative entropy, and lan-guage models) provides considerable further im-provement upon any previous configuration.
It ap-pears that the interactions between these featureshave a further positive influence on our discrimi-native ability.4.3 Class-Based LMsIt is possible to group words based on similarmeaning and syntactic function.
It is reasonableto think that the probability distributions of wordsin such groups would be similar (though not thesame).
By assigning classes to words, we cancalculate the probability of a word based not onthe sequence of preceding words, but rather, wordclasses.
Doing so decreases the size of resultingmodels and also allows for better predictions ofunseen word sequences.
Sparsity is a concern withlanguage models, where we rely on the frequencyof sequences, not just words.
Using word classesassuages some of this concern.
These word classesare generated in an unsupervised manner.
We trainour class-based language models (cLMs) using c-discounting to account for data sparsity.Features AR DA EN PAbaseline 0.216 0.296 0.154 0.348+LM 0.208 0.176 0.117 0.171+cLM 0.130 0.286 0.144 0.211+LM +cLM 0.094 0.155 0.051 0.084+stems +KL+LM +cLM0.092 0.152 0.049 0.079Table 8: Average MSE results comparing all fea-tures.
LMs and cLMs are order 5.Class-based and word-based LMs each helpdifferent languages in our test set.
The twotypes of LMs model different information, withword-based LMs providing a measure of semanticcomplexity and class-based modeling grammati-cal complexity.
As seen in Table 8, the combina-tion of this complementary information is highlybeneficial and strongly correlated to ILR level.
Wesee average MSE reductions of 56%, 48%, 67%,and 77% in Arabic, Dari, English, and Pashto, re-spectively, using both types of language model.Algorithm AR DA EN PAMIRA 0.091 0.156 0.049 0.079SVM 0.089 0.159 0.069 0.070Table 9: Final system results, comparing avg.MSE with the MIRA and SVM algorithms160Figure 4: Comparison of final configuration withall features to baseline by MSE, MIRA algorithmThe further inclusion of TFLOG stemming andrelative entropy reduces average MSE an addi-tional 1%.
Figure 4 reflects this configuration?sperformance across the seven ILR levels.Figure 4 superimposes our final error resultsover those of the baseline.
It is clear that error hasbecome much less language-specific; performanceon all seven ILR levels has become considerablymore consistent across the four languages, as hasthe accuracy at each individual ILR level.
It seemslikely that our error measures would be similar toinner-annotator disagreement, a measure that wewould like to quantify in the future.We find that our results are significant acrossclassifiers.
Table 9 shows the performance of ourfinal feature set with both MIRA and SVM.
TheMSE exhibits the same trends across ILR levelsand languages with both algorithms.
The averagedifference in error between the algorithms remainsthe same as it was with the baseline features.5 Conclusions and Future WorkOur experiments demonstrate that language-independent methods can improve text difficultyassessment performance on the ILR scale forfour languages.
Morphological segmentation forTFLOG word vectors improves our measure ofsemantic complexity and allows us to do topicanalysis better.
Unsupervised methods performsimilarly to language-specific and linguistically-accurate analyzers on this task; we are not sac-rificing performance for a language-independentsystem.
Relative entropy gives structural con-text to more traditional shallow length features,and with word-based LM features provide anotherway to measure semantic complexity.
Class-basedLM features measure grammatical complexity andto some degree account for data sparsity issues.All of these features are low-cost and require nolanguage-specific resources to be applied to newlanguages.
The combination of all these featuressignificantly improves our performance as mea-sured by mean square error across a diverse set oflanguages.We would like to expand our work to more di-verse languages and datasets in future work.
Thereis room to improve upon features described inthis paper, such as new frequency-based measuresfor word vectors and unsupervised morphologicalsegmentation methods.
In the future, we wouldlike to directly compare inner-annotator error andwell-known formulas with our results.
It wouldalso be interesting to look at performance on sub-sets of the corpus to test dependence on datasetsize.
We would also like to investigate the ILRscale; while we assume that it is linear, this is notlikely to be the case.AcknowledgmentsThis paper benefited from valuable discussionwith Jennifer Williams.ReferencesJ.
Chall, E. Dale.
1995.
Readability revisited: The newDale-Chall readability formula.
Brookline Books,Cambridge, MA.C-C. Chang, C-J.
Lin.
2001.
LIBSVM: a libraryfor support vector machines.
Software available athttp://www.csie.ntu.edu.tw/ cjlin/libsvm.K.
Collins-Thompson, J. Callan.
2005.
Predictingreading difficulty with statistical language models.Journal of the American Society for Information Sci-ence and Technology 56(13), 1448-1462.K.
Crammer, Y.
Singer.
2003.
Ultraconservative On-line Algorithms for Multiclass Problems.
Journal ofMachine Learning Research, 3(2003):951-991.M.
Creutz, K. Lagus.
2007.
Unsupervised models formorpheme segmentation and morphology learning.Association for Computing Machinery Transactionson Speech and Language Processing (ACM TSLP),4(1):1-34.F.
Dell?Orletta, S. Montemagni, G. Venturi.
2011.Read-it: Assessing readability of italian texts witha view to text simplification.
Proceedings of the 2ndWorkshop on Speech and Language Processing forAssistive Technologies (SLPAT) 73-83.161T.
Franc?ois, C. Fairon.
2012.
An AI readabilityformula for French as a foreign language.
Pro-ceedings of the 2012 Joint Conference on Em-pirical Methods in Natural Language Processing(EMNLP) and Computational Natural LanguageLearning (CoNLL).
466-477.N.
Habash, O. Rambow, R. Roth.
2010.
Mada+Tokan:A toolkit for arabic tokenization, diacritization, mor-phological disambiguation, pos tagging, stemmingand lemmatization.
Proceedings of the 2nd Inter-national Conference on Arabic Language Resourcesand Tools (MEDAR).J.
Hancke, S. Vajjala, D. Meurers 2012.
ReadabilityClassification for German using lexical, syntactic,and morphological features.
Proceedings of CoL-ING 2012: Technical Papers, 1063-1080.K.S.
Hasan, M.A.
ur Rahman, V. Ng.
2009.Learning-Based Named Entity Recognition forMorphologically-Rich, Resource-Scarce Lan-guages.
Proceedings of the 12th Conferenceof the European Chapter of the Association forComputational Linguistics (EACL), 354-362.M.
Heilman, K. Collins-Thompson, J. Callan, M. Es-kenazi.
2007.
Combining Lexical and Grammat-ical Features to Improve Readability Measures forFirst and Second Language Texts.
Proceedings ofNAACL HLT, 460-467.Interagency Language Roundtable.
ILR Skill Scale.http://www.govtilr.org/Skills/ILRscale4.htm.
2013.A.
Jadidinejad, F. Mahmoudi, J. Dehdari.
2010.
Eval-uation of perstem: a simple and efficient stemmingalgorithm for Persian.
Multilingual Information Ac-cess Evaluation Text Retrieval Experiments.A.
Kathol, K. Precoda, D. Vergyri, W. Wang, S. Riehe-mann.
2005.
Speech translation for low-resourcelanguages: The case of pashto.
Proceedings of IN-TERSPEECH, 2273-2276.J.P.
Kincaid, R.P Fishburne Jr., R.L.
Rodgers, and B.S.Chisson 1975.
Derivation of new readability formu-las for Navy enlisted personnel.
Research BranchReport, U.S.
Naval Air Station, Memphis, 8-75.M.
Kurimo, V. Turunen, M. Varjokallio.
2009.Overview of Morpho Challenge 2008.
EvaluatingSystems for Multilingual and Multimodal Informa-tion Access, Springer Berlin Heidelberg, 951-966.C.
Monson.
2009.
ParaMor: From Paradigm Structureto Natural Language Morphology Induction.
PhDthesis.
Carnegie Mellon University.R.
Munro, C.D.
Manning.
2010.
Subword Variation inText Message Classification.
The 2010 Annual Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics, 510-518.M.
Padr.
2004.
FreeLing: An Open-Source Suite ofLanguage Analyzers.
Proceedings of the 4th In-ternational Conference on Language Resources andEvaluation (LREC?04).C.D.
Paice.
1990.
Another Stemmer.
SIGIR Forum,24:56-61.S.
E. Petersen and M. Ostendorf.
2009.
A ma-chine learning approach to reading level assessment.Computer Speech and Language, 23(2009):89-106.M.F.
Porter.
1980.
An algorithm for suffix stripping.Program, 14(3): 130-137.A.
Ratnaparkhi.
1997.
A simple introduction to max-imum entropy models for natural language process-ing.
IRCS Technical Reports Series, 81.S.
E. Schwarm and M. Ostendorf.
2005.
ReadingLevel Assessment Using Support Vector Machinesand Statistical Language Models.
In Proceedingsof the 43rd Annual Meeting of the Association forComputational Linguistics (ACL).M.I.
Shah, J. Sadri, C.Y.
Suen, N. Nobile.
2007.A New Multipurpose Comprehensive Database forHandwritten Dari Recognition.
11th InternationalConference on Frontiers in Handwriting Recogni-tion, Montreal, 635-40.W.
Shen, J. Williams, T. Marius, E. Salesky.
2013.A language-independent approach to automatic textdifficulty assessment for second-language learners.Proceedings of the 2nd Workshop on Predicting andImproving Text Readability for Target Reader Popu-lations (PITR) 2013.A.
Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
Proceedings of the ICSLP, vol.
2,901-4.S.
Vajjala, D. Meurers.
2012.
On improving the accu-racy of readability classification using insights fromsecond language acquisition.
Proceedings of theSeventh Workshop on Building Educational Appli-cations Using NLP.
Association for ComputationalLinguistics, 2012.
163-173.162
