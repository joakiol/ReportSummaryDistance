Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 211?215, New York City, June 2006. c?2006 Association for Computational LinguisticsDependency Parsing Based on Dynamic Local OptimizationTing Liu Jinshan Ma Huijia Zhu Sheng LiInformation Retrieval LabHarbin Institute of TechnologyHarbin, 150001, China{tliu,mjs,hjzhu,ls}@ir.hit.edu.cnAbstractThis paper presents a deterministic pars-ing algorithm for projective dependencygrammar.
In a bottom-up way the al-gorithm finds the local optimum dynam-ically.
A constraint procedure is madeto use more structure information.
Thealgorithm parses sentences in linear timeand labeling is integrated with the parsing.This parser achieves 63.29% labeled at-tachment score on the average in CoNLL-X Shared Task.1 IntroductionRecently, dependency grammar has gained renewedattention in the parsing community.
Good resultshave been achieved in some dependency parsers(Yamada and Matsumoto, 2003; Nivre et al, 2004).With the availability of many dependency treebanks(van der Beek et al, 2002; Hajic?
et al, 2004;Bo?hmova?
et al, 2003; Kromann, 2003; Dz?eroski etal., 2006) and more other treebanks which can beconverted to dependency annotation (Brants et al,2002; Nilsson et al, 2005; Chen et al, 2003; Kawataand Bartels, 2000), multi-lingual dependency pars-ing is proposed in CoNLL shared task (Buchholz etal., 2006).Many previous works focus on unlabeled parsing,in which exhaustive methods are often used (Eis-ner, 1996).
Their global searching performs wellin the unlabeled dependency parsing.
But with theincrease of parameters, efficiency has to be consid-ered in labeled dependency parsing.
Thus determin-istic parsing was proposed as a robust and efficientmethod in recent years.
Such method breaks theconstruction of dependency tree into a series of ac-tions.
A classifier is often used to choose the mostprobable action to assemble the dependency tree.
(Yamada and Matsumoto, 2003) defined three ac-tions and used a SVM classifier to choose one ofthem in a bottom-up way.
The algorithm in (Nivreet al, 2004) is a blend of bottom-up and top-downprocessing.
Its classifier is trained by memory-basedlearning.Deterministic parsing derives an analysis withoutredundancy or backtracking, and linear time can beachieved.
But when searching the local optimum inthe order of left-to-right, some wrong reduce mayprevent next analysis with more possibility.
(Jin etal., 2005) used a two-phase shift-reduce to decreasesuch errors, and improved the accuracy of long dis-tance dependencies.In this paper a deterministic parsing based on dy-namic local optimization is proposed.
According tothe probabilities of dependency arcs, the algorithmdynamically finds the one with the highest probabil-ities instead of dealing with the sentence in order.A procedure of constraint which can integrate morestructure information is made to check the rational-ity of the reduce.
Finally our results and error anal-ysis are presented.2 Dependency ProbabilitiesAn example of Chinese dependency tree is showedin Figure1.
The tree can be represented as a directedgraph with nodes representing word tokens and arcs211Figure 1: A Chinese dependency treerepresenting dependency relations.
The assumptionthat the arcs are independent on each other often ismade so that parsing can be handled easily.
On theother side the independence assumption will resultin the loss of information because dependencies areinterrelated on each other actually.
Therefore, twokinds of probabilities are used in our parser.
One isarc probabilities which are the possibility that twonodes form an arc, and the other is structure proba-bilities which are used to describe some specific syn-tactic structures.2.1 Arc ProbabilitiesA dependency arc Aican be expressed as a 4-tupleAi= <Nodei, Nodej, D, R>.
Nodeiand Nodejarenodes that constitute the directed arc.
D is the direc-tion of the arc, which can be left or right.
R is rela-tion type labeled on the arc.
Under the independenceassumption that an arc depends on its two nodes wecan calculate arc probability given two nodes.
In ourpaper the arc probabilities are calculated as follows:P1= P(R,D|CTagi, CTagj, Dist)P2= P(R,D|FTagi, FTagj)P3= P(R,D|CTagi, Wordj)P4= P(R,D|Wordi, CTagj)P5= P(R,D|Wordi,CTagi, Wordj,CTagj)P6= P(R,D|CTagi?1, CTagi, CTagj, CTagj+1)Where CTag is coarse-grained part of speech tagand FTag is fine-grained tag.
As to Word we chooseits lemma if it exists.
Dist is the distance betweenNodeiand Nodej.
It is divided into four parts:Dist = 1 if j-i = 1Dist = 2 if j-i = 2Dist = 3 if 3?j-i?6Dist = 4 if j-i > 6All the probabilities are obtained by maximumlikelihood estimation from the training data.
Theninterpolation smoothing is made to get the final arcprobabilities.2.2 Structure ProbabilitiesStructure information plays the critical role in syn-tactic analysis.
Nevertheless the flexibility of syn-tactic structures and data sparseness pose obstaclesto us.
Especially some structures are related to spe-cific language and cannot be employed in multi-lingual parsing.
We have to find those language-independent features.In valency theory ?valence?
represents the num-ber of arguments that a verb is able to govern.
Inthis paper we extend the range of verbs and argu-ments to all the words.
We call the new ?valence?Governing Degree (GD), which means the ability ofone node governing other nodes.
In Figure1, the GDof node ????
is 2 and the GDs of two other nodesare 0.
The governing degree of nodes in dependencytree often shows directionality.
For example, Chi-nese token ???
always governs one left node.
Fur-thermore, we subdivide the GD into Left GoverningDegree (LGD) and Right Governing Degree (RGD),which are the ability of words governing their leftchildren or right children.
In Figure 1 the LGD andRGD of verb ????
are both 1.In the paper we use the probabilities of GDover the fine-grained tags.
The probabilities ofP(LDG|FTag) and P(RGD|FTag) are calculatedfrom training data.
Then we only reserve the FTagswith large probability because their GDs are stableand helpful to syntactic analysis.
Other FTags withsmall probabilities are unstable in GDs and cannotprovide efficient information for syntactic analysis.If their probabilities are less than 0.65 they will beignored in our dependency parsing.3 Dynamic local optimizationMany previous methods are based on history-basedmodels.
Despite many obvious advantages, thesemethods can be awkward to encode some constrainswithin their framework (Collins, 2000).
Classifiersare good at encoding more features in the determin-istic parsing (Yamada and Matsumoto, 2003; Nivreet al, 2004).
However, such algorithm often makemore probable dependencies be prevented by pre-ceding errors.
An example is showed in Figure 2.Arc a is a frequent dependency and b is an arc withmore probability.
Arc b will be prevented by a if thereduce is carried out in order.212Figure 2: A common error in deterministic parsing3.1 Our algorithmOur deterministic parsing is based on dynamic localoptimization.
The algorithm calculates the arc prob-abilities of two continuous nodes, and then reducesthe most probable arc.
The construction of depen-dency tree includes four actions: Check, Reduce,Delete, and Insert.
Before a node is reduced, theCheck procedure is made to validate its correctness.Only if the arc passes the Check procedure it canbe reduced.
Otherwise the Reduce will be delayed.Delete and Insert are then carried out to adjust thechanged arcs.
The complete algorithm is depictedas follows:Input Sentence: S = (w1, w2,l, wn)Initialize:for i = 1 to nRi= GetArcProb(wi,wi+1);Push(Ri) onto Stack;Sort(Stack);Start:i = 0;While Stack.empty = falseR = Stack.top+i;if Check(R) = trueReduce(R);Delete(R?);Insert(R?
);i = 0;elsei++;The algorithm has following advantages:?
Projectivity can be guaranteed.
The node isonly reduced with its neighboring node.
If anode is reduced as a leaf it will be removedfrom the sentence and doesn?t take part in nextReduce.
So no cross arc will occur.?
After n-1 pass a projective dependency tree iscomplete.
Algorithm is finished in linear time.?
The algorithm always reduces the node with theFigure 3: Adjustmenthighest probability if it passes the Check.
Noany limitation on order thus the spread of errorscan be mitigated effectively.?
Check is an open process.
Various constrainscan be encoded in this process.
Structural con-strains, partial parsed information or language-dependent knowledge can be added.Adjustment is illustrated in Figure 3, where ????
is reduced and arc R?
is deleted.
Then the algo-rithm computes the arc probability of R?
and insertsit to the Stack.3.2 CheckingThe information in parsing falls into two kinds:static and dynamic.
The arc probabilities in 2.1 de-scribe the static information which is not changed inparsing.
They are obtained from the training data inadvance.
The structure probabilities in 2.2 describethe dynamic information which varies in the processof parsing.
The use of dynamic information oftendepends on what current dependency tree is.Besides the governing degree, Check procedurealso uses another dynamic information?SequentialDependency.
Whether current arc can be reduced isrelating to previous arc.
In Figure 3 the reduce of thearc R depends on the arc R?.
If R?
has been delayedor its probability is little less than that of R, arc Rwill be delayed.If the arc doesn?t pass the Check it will be de-layed.
The delayed time ranges from 1 to Lengthwhich is the length of sentence.
If the arc is delayedLength times it will be blocked.
The Reduce will bedelayed in the following cases:?
?GD(Nodei) > 0 and its probability is P. IfGD(Nodei) = 0 and Nodeiis made as childin the Reduce, the Nodeiwill be delayedLength*P times.?
?GD(Nodei) ?
m (m > 0) and its probabilityis P. If GD(Nodei) = m and Nodeiis made asparent in the Reduce, the Nodeiwill be delayedLength*P times.213Figure 4: Token score with size of training dataFigure 5: Token score with sentence length?
P(R?)
> ?P(R), the current arc R will be de-layed Length*(P(R?
)/P(R)) times.
R?
is the pre-ceding arc and ?
= 0.60.?
If arc R?
is blocking, the arc R will be delayed.
?GD is empirical value and GD is current value.4 Experiments and analysisOur parsing results and average results are listedin the Table 1.
It can be seen that the attachmentscores vary greatly with different languages.
A gen-eral analysis and a specific analysis are made respec-tively in this section.4.1 General analysisWe try to find the properties that make the differ-ence to parsing results in multi-lingual parsing.
Theproperties of all the training data can be found in(Buchholz et al, 2006).
Intuitively the size of train-ing data and average length of per sentence wouldbe influential on dependency parsing.
The relationof these properties and scores are showed in the Fig-ure 4 and 5.From the charts we cannot assuredly find theproperties that are proportional to score.
WhetherCzech language with the largest size of training dataor Chinese with the shortest sentence length, don?tachieve the best results.
It seems that no any factor isdetermining to parsing results but all the propertiesexert influence on the dependency parsing together.Another factor that maybe explain the differenceof scores in multi-lingual parsing is the characteris-tics of language.
For example, the number of tokenswith HEAD=0 in a sentence is not one for some lan-guages.
Table 1 shows the range of governing de-gree of head.
This statistics is somewhat differentwith that from organizers because we don?t distin-guish the scoring tokens and non-scoring tokens.Another characteristic is the directionality of de-pendency relations.
As Table 1 showed, many rela-tions in treebanks are bi-directional, which increasesthe number of the relation actually.
Furthermore, theflexibility of some grammatical structures poses dif-ficulties to language model.
For instance, subjectcan appear in both sides of the predicates in sometreebanks which tends to cause the confusion withthe object (Kromann, 2003; Afonso et al, 2002;Civit Torruella and Mart??
Anton?
?n, 2002; Oflazer etal., 2003; Atalay et al, 2003).As to our parsing results, which are lower than allthe average results except for Danish.
That can beexplained from the following aspects:(1) Our parser uses a projective parsing algorithmand cannot deal with the non-projective tokens,which exist in all the languages except for Chinese.
(2) The information provided by training data is notfully employed.
Only POS and lemma are used.
Themorphological and syntactic features may be helpfulto parsing.
(3) We haven?t explored syntactic structures in depthfor multi-lingual parsing and more structural fea-tures need to be used in the Check procedure.4.2 Specific analysisSpecifically we make error analysis to Chinese andTurkish.
In Chinese result we found many errorsoccurred near the auxiliary word ???(DE).
We callthe noun phrases with ???
DE Structure.
The word???
appears 355 times in the all 4970 dependenciesof the test data.
In Table 2 the second row shows thefrequencies of ?DE?
as the parent of dependencies.The third row shows the frequencies while it is aschild.
Its error rate is 33.1% and 43.4% in our re-sults respectively.
Furthermore, each head error willresult in more than one errors, so the errors from DEStructures are nearly 9% in our results.214Ar Ch Cz Da Du Ge Ja Po Sl Sp Sw Tuour 50.74 75.29 58.52 77.70 59.36 68.11 70.84 71.13 57.21 65.08 63.83 41.72ave 59.94 78.32 67.17 76.16 70.73 78.58 85.86 80.63 65.16 73.52 76.44 55.95NH 17 1 28 4 9 1 14 1 11 1 1 5BD 27/24 78/55 82/72 54/24 26/17 46/40 7/2 55/40 26/23 21/19 64/54 26/23Table 1: The second and third rows are our scores and average scores.
The fourth row lists the maximalnumber of tokens with HEAD=0 in a sentence.
The last row lists the number of relations/the number ofbi-directional relations of them (Our statistics are slightly different from that of organizers).gold system error headerrparent 320 354 106 106child 355 355 154 74Table 2: Chinese DE Structure ErrorsThe high error rate is due to the flexibility of DEStructure.
The children of DE can be nouns andverbs, thus the ambiguities will occur.
For example,the sequence ?V N1 DE N2?
is a common ambigu-ious structure in Chinese.
It needs to be solved withsemantic knowledge to some extent.
The errors ofDE being child are mostly from noun compounds.For example, the string ?????????
resultsin the error: ?DE?
as the child of ????.
It will bebetter that noun compounds are processed specially.Our results and average results achieve the low-est score on Turkish.
We try to find some reasonsthrough the following analysis.
Turkish is a typi-cal head-final language and 81.1% of dependenciesare right-headed.
The monotone of directionality in-creases the difficulties of identification.
Another dif-ficulty is the diversity of the same pair.
Taking nounand pronoun as example, which only achieve the ac-curacy of 25% and 28% in our results, there are 14relations in the noun-verb pairs and 11 relations inthe pronoun-verb pairs.
Table 3 illustrates the distri-bution of some common relations in the test data.The similarity of these dependencies makes ourparser only recognize 23.3% noun-verb structuresand 21.8% pronoun-verb structures.
The syntacticor semantic knowledge maybe helpful to distinguishthese similar structures.5 ConclusionThis paper has applied a deterministic algorithmbased on dynamic local optimization to multi-total obj sub mod D.A L.ANoun-V 1300 494 319 156 102 78Pron-V 215 91 60 9 37 3Table 3: The distribution of some common relationslingual dependency parsing.
Through the erroranalysis for some languages, we think that the dif-ference between languages is a main obstacle posedon multi-lingual dependency parsing.
Adoptingdifferent learners according to the type of languagesmay be helpful to multi-lingual dependency parsing.Acknowledgement This work was supportedby the National Natural Science Foundation ofChina under Grant No.
60435020?60575042 and60503072.ReferencesM.
Collins.
2000.
Discriminative reranking for naturallanguage parsing.
In Proc.
of ICML.M.X.
Jin, M.Y.
Kim, and J.H.
Lee.
2005.
Two-phaseshift-reduce deterministic dependency parser of chi-nese.
In Proc.
of IJCNLP: Companion Volume includ-ing Posters/Demos and tutorial abstracts.J.
Nivre, J.
Hall, and J. Nilsson.
2004.
Memory-baseddependency parsing.
In Proc.
of the Eighth Conf.
onComputational Natural Language Learning (CoNLL),pages 49?56.J.
Eisner.
1996.
Three new probabilistic models fordependency parsing: An exploration.
In Proc.
ofthe 16th Intern.
Conf.
on Computational Linguistics(COLING), pages 340?345.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In Proc.of the 8th Intern.
Workshop on Parsing Technologies(IWPT).215
