Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 97?100,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPCorrelating Human and Automatic Evaluation of a German SurfaceRealiserAoife CahillInstitut f?ur Maschinelle Sprachverarbeitung (IMS)University of Stuttgart70174 Stuttgart, Germanyaoife.cahill@ims.uni-stuttgart.deAbstractWe examine correlations between nativespeaker judgements on automatically gen-erated German text against automatic eval-uation metrics.
We look at a number ofmetrics from the MT and Summarisationcommunities and find that for a relativeranking task, most automatic metrics per-form equally well and have fairly strongcorrelations to the human judgements.In contrast, on a naturalness judgementtask, the General Text Matcher (GTM) toolcorrelates best overall, although in gen-eral, correlation between the human judge-ments and the automatic metrics was quiteweak.1 IntroductionDuring the development of a surface realisationsystem, it is important to be able to quickly and au-tomatically evaluate its performance.
The evalua-tion of a string realisation system usually involvesstring comparisons between the output of the sys-tem and some gold standard set of strings.
Typi-cally automatic metrics from the fields of MachineTranslation (e.g.
BLEU) or Summarisation (e.g.ROUGE) are used, but it is not clear how success-ful or even appropriate these are.
Belz and Reiter(2006) and Reiter and Belz (2009) describe com-parison experiments between the automatic eval-uation of system output and human (expert andnon-expert) evaluation of the same data (Englishweather forecasts).
Their findings show that theNIST metric correlates best with the human judge-ments, and all automatic metrics favour systemsthat generate based on frequency.
They concludethat automatic evaluations should be accompaniedby human evaluations where possible.
Stent et al(2005) investigate a number of automatic evalua-tion methods for generation in terms of adequacyand fluency on automatically generated Englishparaphrases.
They find that the automatic metricsare reasonably good at measuring adequacy, butnot good measures of fluency, i.e.
syntactic cor-rectness.In this paper, we carry out experiments to corre-late automatic evaluation of the output of a surfacerealisation ranking system for German against hu-man judgements.
We particularly look at correla-tions at the individual sentence level.2 Human Evaluation ExperimentsThe data used in our experiments is the output ofthe Cahill et al (2007) German realisation rank-ing system.
That system is couched within theLexical Functional Grammar (LFG) grammaticalframework.
LFG has two levels of representa-tion, C(onstituent)-Structure which is a context-free tree representation and F(unctional)-Structurewhich is a recursive attribute-value matrix captur-ing basic predicate-argument-adjunct relations.Cahill et al (2007) use a large-scale hand-crafted grammar (Rohrer and Forst, 2006) to gen-erate a number of (almost always) grammaticalsentences given an input F-Structure.
They showthat a linguistically-inspired log-linear rankingmodel outperforms a simple baseline tri-gram lan-guage model trained on the Huge German Corpus(HGC), a corpus of 200 million words of newspa-per and other text.Cahill and Forst (2009) describe a number ofexperiments where they collect judgements fromnative speakers about the three systems com-pared in Cahill et al (2007): (i) the originalcorpus string, (ii) the string chosen by the lan-guage model, and (iii) the string chosen by thelinguistically-inspired log-linear model.1We onlytake the data from 2 of those experiments sincethe remaining experiments would not provide any1In all cases, the three strings were different.97informative correlations.
In the first experimentthat we consider (A), subjects are asked to rankon a scale from 1?3 (1 being the best, 3 beingthe worst) the output of the three systems (jointrankings were not permitted).
In the second ex-periment (B), subjects were asked to rank on ascale from 1?5 (1 being the worst, 5 being thebest) how natural sounding the string chosen bythe log-linear model was.
The goal of experimentB was to determine whether the log-linear modelwas choosing good or bad alternatives to the orig-inal string.
Judgements on the data were collectedfrom 24 native German speakers.
There were 44items in Experiment A with an average sentencelength of 14.4, and there were 52 items in Exper-iment B with an average sentence length of 12.1.Each item was judged by each native speaker atleast once.3 Correlation with Automatic MetricsWe examine the correlation between the humanjudgements and a number of automatic metrics:BLEU (Papineni et al, 2001) calculates the number of n-grams a solution shares with a reference, adjusted by abrevity penalty.
Usually the geometric mean for scoresup to 4-gram are reported.ROUGE (Lin, 2004) is an evaluation metric designed to eval-uate automatically generated summaries.
It comprisesa number of string comparison methods including n-gram matching and skip-ngrams.
We use the defaultROUGE-L longest common subsequence f-score mea-sure.2GTM General Text Matching (Melamed et al, 2003) calcu-lates word overlap between a reference and a solution,without double counting duplicate words.
It places lessimportance on word order than BLEU.SED Levenshtein (String Edit) distanceWER Word Error RateTER Translation Error Rate (Snover et al, 2006) computesthe number of insertions, deletions, substitutions andshifts needed to match a solution to a reference.Most of these metrics come from the MachineTranslation field, where the task is arguably sig-nificantly different.
In the evaluation of a surfacerealisation system (as opposed to a complete gen-eration system), typically the choice of vocabularyis limited and often the task is closer to word re-ordering.
Many of the MT metrics have methods2Preliminary experiments with the skip n-grams per-formed worse than the default parameters.Experiment A Experiment BGOLD LM LL LLhuman A (rank 1?3) 1.4 2.55 2.05human B (scale 1?5) 3.92BLEU 1.0 0.67 0.72 0.79ROUGE-L 1.0 0.85 0.78 0.85GTM 1.0 0.55 0.60 0.74SED 1.0 0.54 0.61 0.71WER 0.0 48.04 39.88 28.83TER 0.0 0.16 0.14 0.11DEP 100 82.60 87.50 93.11WDEP 1.0 0.70 0.82 0.90Table 1: Average scores of each metric for Exper-iment A dataSentence Corpuscorr p-value corr p-valueBLEU -0.615 <0.001 -1 0.3333ROUGE-L -0.644 <0.001 -0.5 1GTM -0.643 <0.001 -1 0.3333SED -0.628 <0.001 -1 0.3333WER 0.623 <0.001 1 0.3333TER 0.608 <0.001 1 0.3333Table 2: Correlation between human judgementsfor experiment A (rank 1?3) and automatic metricsfor attempting to account for different but equiva-lent translations of a given source word, typicallyby integrating a lexical resource such as WordNet.Also, these metrics were mostly designed to eval-uate English output, so it is not clear that they willbe equally appropriate for other languages, espe-cially freer word order ones, such as German.The scores given by each metric for the dataused in both experiments are presented in Table 1.For the Experiment A data, we use the Spearmanrank correlation coefficient to measure the corre-lation between the human judgements and the au-tomatic scorers.
The results are presented in Table2 for both the sentence and the corpus level corre-lations, we also present p-values for statistical sig-nificance.
Since we only have judgements on threesystems, the corpus correlation is not that informa-tive.
Interestingly, the ROUGE-L metric is the onlyone that does not rank the output of the three sys-tems in the same order as the judges.
It ranks thestrings chosen by the language model higher thanthe strings chosen by the log-linear model.
How-ever, at the level of the individual sentence, theROUGE-L metric correlates best with the humanjudgements.
The GTM metric correlates at aboutthe same level, but in general there seems to belittle difference between the metrics.For the Experiment B data we use the Pearsoncorrelation coefficient to measure the correlationbetween the human judgements and the automatic98SentenceCorrelation P-ValueBLEU 0.095 0.5048ROUGE-L 0.207 0.1417GTM 0.424 0.0017SED 0.168 0.2344WER -0.188 0.1817TER -0.024 0.8646Table 3: Correlation between human judgementsfor experiment B (naturalness scale 1?5) and au-tomatic metricsmetrics.
The results are given in Table 3.
Herewe only look at the correlation at the individualsentence level, since we are looking at data fromonly one system.
For this data, the GTM met-ric clearly correlates most closely with the humanjudgements, and it is the only metric that has a sta-tistically significant correlation.
BLEU and TERcorrelate particularly poorly, with correlation co-efficients very close to zero.3.1 Syntactic MetricsRecently, there has been a move towards moresyntactic, rather than purely string based, evalu-ation of MT output and summarisation (Hovy etal., 2005; Owczarzak et al, 2008).
The idea is togo beyond simple string comparisons and evaluateat a deeper linguistic level.
Since most of the workin this direction has only been carried out for En-glish so far, we apply the idea rather than a specifictool to the data.
We parse the data from both ex-periments with a German dependency parser (Halland Nivre, 2008) trained on the TIGER Treebank(with sentences 8000-10000 heldout for testing).This parser achieves 91.23% labelled accuracy onthe 2000-sentence test set.To calculate the correlation between the humanjudgements and the dependency parser, we parsethe original strings as well as the strings chosenby the log-linear and language models.
The stan-dard evaluation procedure relies on both stringsbeing identical to calculate (un-)labelled depen-dency accuracy, and so we map the dependen-cies produced by the parser into sets of triplesas used in the evaluation software of Crouch etal.
(2002) where each dependency is representedas deprel(head,word) and each word is in-dexed with its position in the original string.3Wecompare the parses for both experiments against3This is a 1-1 mapping, and the indexing ensures that du-plicate words in a sentence are not confused.Experiment A Experiment Bcorr p-value corr p-valueDependencies -0.640 <0.001 0.186 0.1860Unweighted Deps -0.657 <0.001 0.290 0.03686Table 4: Correlation between dependency-basedevaluation and human judgementsthe parses of the original strings.
We calculateboth a weighted and unweighted dependency f-score, as given in Table 1.
The unweighted f-scoreis calculated by taking the average of the scoresfor each dependency type, while the weighted f-score weighs each average score by its frequencyin the test corpus.
We calculate the Spearmanand Pearson correlation coefficients as before; theresults are given in Table 4.
The results showthat the unweighted dependencies correlate moreclosely (and statistically significantly) with the hu-man judgements than the weighted ones.
This sug-gests that the frequency of a dependency type doesnot matter as much as its overall correctness.4 DiscussionThe large discrepancy between the absolute corre-lation coefficients for Experiment A and B can beexplained by the fact that they are different tasks.Experiment A ranks 3 strings relative to one an-other, while Experiment B measures the natural-ness of the string.
We would expect automaticmetrics to be better at the first task than the sec-ond, as it is easier to rank systems relative to eachother than to give a system an absolute score.Disappointingly, the correlation between the de-pendency parsing metric and the human judge-ments was no higher than the simple GTM string-based metric (although it did outperform all otherautomatic metrics).
This does not correspond torelated work on English Summarisation evalua-tion (Owczarzak, 2009) which shows that a met-ric based on an automatically induced LFG parserfor English achieves comparable or higher correla-tion with human judgements than ROUGE and Ba-sic Elements (BE).4Parsers of German typicallydo not achieve as high performance as their En-glish counterparts, and further experiments includ-ing alternative parsers are needed to see if we canimprove performance of this metric.The data used in our experiments was almostalways grammatically correct.
Therefore the task4The GTM metric was not compared in that paper99of an evaluation system is to score more naturalsounding strings higher than marked or unnaturalones.
In this respect, our findings mirror those ofStent et al (2005) for English data, that the au-tomatic metrics do not correlate well with humanjudges on syntactic correctness.5 ConclusionsWe presented data that examined the correla-tion between native speaker judgements and au-tomatic evaluation metrics on automatically gen-erated German text.
We found that for our firstexperiment, all metrics were correlated to roughlythe same degree (with ROUGE-L achieving thehighest correlation at an individual sentence leveland the GTM tool not far behind).
At a corpuslevel all except ROUGE were in agreement withthe human judgements.
In the second experiment,the General Text Matcher Tool had the strongestcorrelation.
We carried out an experiment to testwhether a more sophisticated syntax-based evalua-tion metric performed better than the more simplestring-based ones.
We found that while the un-weighted dependency evaluation metric correlatedwith the human judgements more strongly than al-most all metrics, it did not outperform the GTMtool.
The correlation between the human judge-ments and the automatic evaluation metrics wasmuch higher for the relative ranking task than forthe naturalness task.AcknowledgmentsThis work was funded by the Collaborative Re-search Centre (SFB 732) at the University ofStuttgart.
We would like to thank Martin Forst,Alex Fraser and the anonymous reviewers for theirhelpful feedback.
Furthermore, we would liketo thank Johan Hall, Joakim Nivre and YannickVersely for their help in retraining the MALT de-pendency parser with our data set.ReferencesAnja Belz and Ehud Reiter.
2006.
Comparing auto-matic and human evaluation of NLG systems.
InProceedings of EACL 2006, pages 313?320, Trento,Italy.Aoife Cahill and Martin Forst.
2009.
Human Eval-uation of a German Surface Realisation Ranker.
InProceedings of EACL 2009, pages 112?120, Athens,Greece, March.Aoife Cahill, Martin Forst, and Christian Rohrer.
2007.Stochastic Realisation Ranking for a Free Word Or-der Language.
In Proceedings of ENLG-07, pages17?24, Saarbr?ucken, Germany, June.Richard Crouch, Ron Kaplan, Tracy Holloway King,and Stefan Riezler.
2002.
A comparison of evalu-ation metrics for a broad coverage parser.
In Pro-ceedings of the LREC Workshop: Beyond PARSE-VAL, pages 67?74, Las Palmas, Spain.Johan Hall and Joakim Nivre.
2008.
A dependency-driven parser for German dependency and con-stituency representations.
In Proceedings ofthe Workshop on Parsing German, pages 47?54,Columbus, Ohio, June.Eduard Hovy, Chin yew Lin, and Liang Zhou.
2005.Evaluating duc 2005 using basic elements.
In Pro-ceedings of DUC-2005.Chin-Yew Lin.
2004.
Rouge: A package for auto-matic evaluation of summaries.
In Stan SzpakowiczMarie-Francine Moens, editor, Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81, Barcelona, Spain, July.I.
Dan Melamed, Ryan Green, and Joseph P. Turian.2003.
Precision and recall of machine translation.In Proceedings of NAACL-03, pages 61?63, NJ,USA.Karolina Owczarzak, Josef van Genabith, and AndyWay.
2008.
Evaluating machine translation withLFG dependencies.
Machine Translation, 21:95?119.Karolina Owczarzak.
2009.
DEPEVAL(summ):Dependency-based Evaluation for Automatic Sum-maries.
In Proceedings of ACL-IJCNLP 2009, Sin-gapore.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof ACL-02, pages 311?318, NJ, USA.Ehud Reiter and Anja Belz.
2009.
An Investigationinto the Validity of Some Metrics for AutomaticallyEvaluating Natural Language Generation Systems.Computational Linguistics, 35.Christian Rohrer and Martin Forst.
2006.
ImprovingCoverage and Parsing Quality of a Large-Scale LFGfor German.
In Proceedings of LREC 2006, Genoa,Italy.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and Ralph Weischedel.
2006.
Astudy of translation error rate with targeted humanannotation.
In Proceedings of AMTA 2006, pages223?231.Amanda Stent, Matthew Marge, and Mohit Singhai.2005.
Evaluating evaluation methods for generationin the presense of variation.
In Proceedings of CI-CLING, pages 341?351.100
