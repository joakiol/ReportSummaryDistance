Workshop on Monolingual Text-To-Text Generation, pages 74?83,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 74?83,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsCreating Disjunctive Logical Forms from Aligned Sentences forGrammar-Based Paraphrase GenerationScott Martin and Michael WhiteDepartment of LinguisticsThe Ohio State UniversityColumbus, Ohio, USA{scott,mwhite}@ling.ohio-state.eduAbstractWe present a method of creating disjunctivelogical forms (DLFs) from aligned sentencesfor grammar-based paraphrase generation us-ing the OpenCCG broad coverage surface re-alizer.
The method takes as input word-levelalignments of two sentences that are para-phrases and projects these alignments onto thelogical forms that result from automaticallyparsing these sentences.
The projected align-ments are then converted into phrasal editsfor producing DLFs in both directions, wherethe disjunctions represent alternative choicesat the level of semantic dependencies.
The re-sulting DLFs are fed into the OpenCCG re-alizer for n-best realization, using a pruningstrategy that encourages lexical diversity.
Af-ter merging, the approach yields an n-best listof paraphrases that contain grammatical alter-natives to each original sentence, as well asparaphrases that mix and match content fromthe pair.
A preliminary error analysis suggeststhat the approach could benefit from taking theword order in the original sentences into ac-count.
We conclude with a discussion of plansfor future work, highlighting the method?s po-tential use in enhancing automatic MT evalu-ation.1 IntroductionIn this paper, we present our initial steps towardsmerging the grammar-based and data-driven para-phrasing traditions, highlighting the potential ofour approach to enhance the automatic evaluationof machine translation (MT).
Kauchak and Barzi-lay (2006) have shown that creating synthetic ref-erence sentences by substituting synonyms fromWordnet into the original reference sentences canincrease the number of exact word matches withan MT system?s output and yield significant im-provements in correlations of BLEU (Papineni etal., 2002) scores with human judgments of trans-lation adequacy.
Madnani (2010) has also shownthat statistical machine translation technique can beemployed in a monolingual setting, together withparaphrases acquired using Bannard and Callison-Burch?s (2005) pivot method, in order to enhancethe tuning phase of training an MT system by aug-menting a reference translation with automatic para-phrases.
Earlier, Barzilay and Lee (2003) and Panget al (2003) developed approaches to aligning mul-tiple reference translations in order to extract para-phrases and generate new sentences.
By startingwith reference sentences from multiple human trans-lators, these data-driven methods are able to capturesubtle, highly-context sensitive word and phrase al-ternatives.
However, the methods are not particu-larly adept at capturing variation in word order orthe use of function words that follow from generalprinciples of grammar.
By contrast, grammar-basedparaphrasing methods in the natural language gen-eration tradition (Iordanskaja et al, 1991; Elhadadet al, 1997; Langkilde and Knight, 1998; Stede,1999; Langkilde-Geary, 2002; Velldal et al, 2004;Gardent and Kow, 2005; Hogan et al, 2008) havethe potential to produce many such grammatical al-ternatives: in particular, by parsing a reference sen-tence to a representation that can be used as the in-put to a surface realizer, grammar-based paraphrasescan be generated if the realizer supports n-best out-put.
To our knowledge though, methods of using agrammar-based surface realizer together with multi-ple aligned reference sentences to produce synthetic74Source Liu Lefei says that [in the long term] , in terms of asset alocation, overseas in-vestment should occupy a certain proportion of [an insurance company?s overallallocation] .Reference Liu Lefei said that in terms of capital allocation , outbound investment should makeup a certain ratio of [overall allocations for insurance companies] [in the long run].Paraphrase Liu Lefei says that [in the long run], in terms of capital allocation, overseas invest-ment should occupy the certain ratio of an [insurance company?s overall allocation]Table 1: Zhao et al?s (2009) similarity example, with italics added to show word-level substitutions, and squarebrackets added to show phrase location or construction mismatches.
Here, the source sentence (itself a referencetranslation) has been paraphrased to be more like the reference sentence.references have not been investigated.1As an illustration of the need to combine gram-matical paraphrasing with data-driven paraphrasing,consider the example that Zhao et al (2009) useto illustrate the application of their paraphrasingmethod to similarity detection, shown in Table 1.Zhao et al make use of a large paraphrase table,similar to the phrase tables used in statistical MT, inorder to construct paraphrase candidates.
(Like the-sauri or WordNet, such resources are complemen-tary to the ones we make use of here.)
To test theirsystem?s ability to paraphrase reference sentences inservice of MT evaluation, they attempt to paraphraseone reference translation to make it more similar toanother reference translation; thus, in Table 1, thesource sentence (itself a reference translation) hasbeen paraphrased to be more like the (other) refer-ence sentence.
As indicated by italics, their sys-tem has successfully paraphrased term, asset andproportion as run, capital and ratio, respectively(though the certain seems to have been mistakenlysubstituted for a certain).
However, their systemis not capable of generating a paraphrase with inthe long run at the end of the sentence, nor can itrephrase insurance company?s overall allocation asoverall allocations for insurance companies, whichwould seem to require access to more general gram-matical knowledge.To combine grammar-based paraphrasing withlexical and phrasal alternatives gleaned from mul-tiple reference sentences, our approach takes advan-1The task is not unrelated to sentence fusion in multidoc-ument summarization (Barzilay and McKeown, 2005), exceptthere the goal is to produce a single, shorter sentence from mul-tiple related input sentences.tage of the OpenCCG realizer?s ability to generatefrom disjunctive logical forms (DLFs), i.e.
packedsemantic dependency graphs (White, 2004; White,2006a; White, 2006b; Nakatsu and White, 2006; Es-pinosa et al, 2008; White and Rajkumar, 2009).
Inprinciple, semantic dependency graphs offer a betterstarting point for paraphrasing than the syntax treesemployed by Pang et.
al, as paraphrases can gener-ally be expected to be more similar at the level ofunordered semantic dependencies than at the levelof syntax trees.
Our method starts with word-levelalignments of two sentences that are paraphrases,since the approach can be used with any alignmentmethod from the MT (Och and Ney, 2003; Haghighiet al, 2009, for example) or textual inference (Mac-Cartney et al, 2008, inter alia) literature in princi-ple.
The alignments are projected onto the logicalforms that result from automatically parsing thesesentences.
The projected alignments are then con-verted into phrasal edits for producing DLFs in bothdirections, where the disjunctions represent alterna-tive choices at the level of semantic dependencies.The resulting DLFs are fed into the OpenCCG re-alizer for n-best realization.
In order to enhancethe variety of word and phrase choices in the n-bestlists, a pruning strategy is used that encourages lex-ical diversity.
After merging, the approach yieldsan n-best list of paraphrases that contain grammat-ical alternatives to each original sentence, as wellas paraphrases that mix and match content from thepair.The rest of the paper is organized as follows.
Sec-tion 2 provides background on surface realizationwith OpenCCG and DLFs.
Section 3 describes our75method of creating DLFs from aligned paraphrases.Finally, Section 4 characterizes the recurring errorsand concludes with a discussion of related and futurework.2 Surface Realization with OpenCCGOpenCCG is an open source Java library for pars-ing and realization using Baldridge?s multimodalextensions to CCG (Steedman, 2000; Baldridge,2002).
In the chart realization tradition (Kay, 1996),the OpenCCG realizer takes logical forms as inputand produces strings by combining signs for lexicalitems.
Alternative realizations are scored using in-tegrated n-gram and perceptron models (White andRajkumar, 2009), where the latter includes syntac-tic features from Clark and Curran?s (2007) normalform model as well as discriminative n-gram fea-tures (Roark et al, 2004).
Hypertagging (Espinosaet al, 2008), or supertagging for surface realiza-tion, makes it practical to work with broad coveragegrammars.
For parsing, an implementation of Hock-enmaier and Steedman?s (2002) generative model isused to select the best parse.
The grammar is auto-matically extracted from a version of the CCGbank(Hockenmaier and Steedman, 2007) with Propbank(Palmer et al, 2005) roles projected onto it (Boxwelland White, 2008).A distinctive feature of OpenCCG is the abilityto generate from disjunctive logical forms (White,2006a).
This capability has many benefits, suchas enabling the selection of realizations accordingto predicted synthesis quality (Nakatsu and White,2006), and avoiding repetition in the output of a dia-logue system (Foster and White, 2007).
Disjunctiveinputs make it possible to exert fine-grained controlover the specified paraphrase space.
In the chart re-alization tradition, previous work has not generallysupported disjunctive logical forms, with Shemtov?s(Shemtov, 1997) more complex approach as the onlypublished exception.An example disjunctive input from the COMICsystem appears in Figure 1(c).2 Semantic de-pendency graphs such as these?represented in-ternally in Hybrid Logic Dependency Semantics2To simplify the exposition, the features specifying informa-tion structure and deictic gestures have been omitted, as havethe semantic sorts of the discourse referents.be<TENSE>pres,<MOOD>dcl e<ARG> <PROP>based_on <DET>the,<NUM>sgdesign d p<SOURCE> <ARTIFACT> collection<DET>the,<NUM>sg c<HASPROP> <CREATOR>Funny_Day f v Villeroy_and_Boch(a) Semantic dependency graph for The design (is|?s)based on the Funny Day collection by Villeroy andBoch.be<TENSE>pres,<MOOD>dcl e<ARG> <PROP>based_on <DET>the,<NUM>sgdesign d p<SOURCE> <ARTIFACT> series<NUM>sg c<HASPROP> <GENOWNER >Funny_Day f v Villeroy_and_Boch(b) Semantic dependency graph for The design (is|?s)based on Villeroy and Boch?s Funny Day series.be<TENSE>pres,<MOOD>dcl e<ARG> <PROP>based_on <DET>the,<NUM>sgdesign d p<SOURCE> <ARTIFACT> collection|series (< DET>the) ?
,<NUM>sg c<HASPROP> <GENOWNER >Funny_Day f v Villeroy_and_Boch<CREATOR>(c) Disjunctive semantic dependency graph covering (a)-(b), i.e.
The design (is|?s) based on (the Funny Day(collection|series) by Villeroy and Boch | Villeroy andBoch?s Funny Day (collection|series)).Figure 1: Example semantic dependency graphsfrom the COMIC dialogue system.
@e(be ?
?TENSE?pres ?
?MOOD?dcl ??ARG?
(d ?
design ?
?DET?the ?
?NUM?sg) ??PROP?
(p ?
based on ?
?ARTIFACT?d ??SOURCE?
(c ?
collection ?
?DET?the ?
?NUM?sg ??HASPROP?
(f ?
Funny Day) ??CREATOR?
(v ?
V&B))))(a)...@e(be ?
?TENSE?pres ?
?MOOD?dcl ??ARG?
(d ?
design ?
?DET?the ?
?NUM?sg) ??PROP?
(p ?
based on ?
?ARTIFACT?d ??SOURCE?
(c ?
?NUM?sg ?
(?DET?the)?
?
(collection ?
series) ??HASPROP?
(f ?
Funny Day) ?
(?CREATOR?v ?
?GENOWNER?v ))))?
@v(Villeroy and Boch)(c)Figure 2: HLDS for examples in Figure 1.2 Disjunctive Logical FormsAs an illustration of disjunctive logical forms,consider the semantic dependency graphs in Fig-ure 1, which are taken from the COMIC1 mul-timodal dialogue system.2 Graphs such as theseconstitute the input to the OpenCCG realizer.Each node has a lexical predication (e.g.
design)and a set of semantic features (e.g.
?NUM?sg);nodes are connected via dependency relations (e.g.?ARTIFACT?
).Given the lexical categories in the COMICgrammar, the graphs in Figure 1(a) and (b) fullyspecify their respective realizations, with the ex-ception of the choice of the full or contractedform of the copula.
To generalize over these al-ternatives, the disjunctive graph in (c) may beemployed.
This graph allows a free choice be-tween the domain synonyms collection and se-ries, as indicated by the vertical bar betweentheir respective predications.
The graph also al-lows a free choice between the ?CREATOR?
and?GENOWNER?
relations?lexicalized via by andthe possessive, respectively?connecting the headc (collection or series) with the dependent v (for1http://www.hcrc.ed.ac.uk/comic/2To simplify the exposition, the features specifying infor-mation structure and deictic gestures have been omitted, ashave the semantic sorts of the discourse referents.be<TENSE>pres,<MOOD>dcl e<ARG> <PROP>based_on <DET>the,<NUM>sgdesign d p<SOURCE> <ARTIFACT> collection<DET>the,<NUM>sg c<HASPROP> <CREATOR>Funny_Day f v Villeroy_and_Boch(a) Semantic dependency graph for The design (is|?s)based on the Funny Day collection by Villeroy andBoch.be<TENSE>pres,<MOOD>dcl e<ARG> <PROP>based_on <DET>the,<NUM>sgdesign d p<SOURCE> <ARTIFACT> series<NUM>sg c<HASPROP> <GENOWNER >Funny_Day f v Villeroy_and_Boch(b) Semantic dependency graph for The design (is|?s)based on Villeroy and Boch?s Funny Day series.be<TENSE>pres,<MOOD>dcl e<ARG> <PROP>based_on <DET>the,<NUM>sgdesign d p<SOURCE> <ARTIFACT> collection|series (< DET>the) ?
,<NUM>sg c<HASPROP> <GENOWNER >Funny_Day f v Villeroy_and_Boch<CREATOR>(c) Disjunctive semantic dependency graph covering (a)-(b), i.e.
The design (is|?s) based on (the Funny Day(collection|series) by Villeroy and Boch | Villeroy andBoch?s Funny Day (collection|series)).Figure 1: Example semantic dependency graphsfrom the COMIC dialogue system.
@e(be ?
?TENSE?pres ?
?MOOD?dcl ??ARG?
(d ?
design ?
?DET?the ?
?NUM?sg) ??PROP?
(p ?
based on ?
?ARTIFACT?d ??SOURCE?
(c ?
collection ?
?DET?the ?
?NUM?sg ??HASPROP?
(f ?
Funny Day) ??CREATOR?
(v ?
V&B))))(a)...@e(be ?
?TENSE?pres ?
?MOOD?dcl ??ARG?
(d ?
design ?
?DET?the ?
?NUM?sg) ??PROP?
(p ?
based on ?
?ARTIFACT?d ??SOURCE?
(c ?
?NUM?sg ?
(?DET?the)?
?
(collection ?
series) ??HASPROP?
(f ?
Funny Day) ?
(?CREATOR?v ?
?GENOWNER?v ))))?
@v(Villeroy and Boch)(c)Figure 2: HLDS for examples in Figure 1.2 Disjunctive Logical FormsAs an illustration f disjunctive logical forms,consider the semantic dependency graphs in Fig-ure 1, which are taken from the COMIC1 mul-timodal dialogue system.2 Graphs such as theseconstitute the input to the OpenCCG realizer.Each node has a lexical predication (e.g.
design)and a set of semantic features (e.g.
?NUM?sg);nodes are connected via dependency relations (e.g.?ARTIFACT?
).Given the lexical categories in the COMICgrammar, the graphs in Figure 1(a) and (b) fullyspecify their respective realizations, with the ex-ception of the choice of the full or contractedform of the copula.
To generalize over these al-ternatives, the disjunctive graph in (c) may beemployed.
This graph allows a free choice be-tween the domain synonyms collection and se-ries, as indicated by the vertical bar betweentheir respective predications.
The graph also al-lows a free choice between the ?CREATOR?
and?GENOWNER?
relations?lexicalized via by andthe possessive, respectively?connecting the headc (collection or series) with the dependent v (for1http://www.hcrc.ed.ac.uk/comic/2To simplify the exposition, the features specifying infor-mation structure and deictic gestures have been omitted, ashave the semantic sorts of the discourse referents.be<TENSE>pres,<MOOD>dcl e<ARG> <PROP>based_on <DET>the,<NUM>sgdesign d p<SOURCE> <ARTIFACT> collection<DET>the,<NUM>sg c<HASPROP> <CREATORFunny_Day f v Villeroy_and_Boch(a) Semantic dependency graph for The design (is|?s)based on the Funny Day collection by Villeroy andBoch.be<TENSE>pres,<MOOD>dcl e<ARG> <PROP>based_on <DET>the,<NUM>sgdesign d p<SOURCE> <ARTIFACT> series<NUM>sg c<HASPROP> <GE OWNER >Funny_Day f v Villeroy_and_Boch(b) Semantic dependency graph for The design (is|?s)based on Villeroy and Boch?s Funny Day series.be<TENSE>pres,<MOOD>dcl e<ARG> <PROP>based_on <DET>the,<NUM>sgdesign d p<SOURCE> <ARTIFACT> collection|series (< DET>the) ?
,<NUM>sg c<HASPROP> <GENOWNER >Funny_Day f v Villeroy_and_Boch<CREATOR>(c) Disjunctive semantic dependency graph covering (a)-(b), i.e.
The design (is|?s) based on (the Funny Day(collection|series) by Vill roy and Boch | Villeroy andBoch?s Funny Day (collection| ries)).Figure 1: Example semantic dependency graphsfrom the COMIC dialogue system.
@e(be ?
?TENSE?pres ?
?MOOD?dcl ??ARG?
(d ?
design ?
?DET?the ?
?NUM?sg) ??PROP?
(p ?
based on ?
?ARTIFACT?d ??SOURCE?
(c ?
collection ?
?DET?the ?
?NUM?sg ??HASPROP?
(f ?
Funny Day) ??CREATOR?
(v ?
V&B))))(a)...@e(be ?
?TENSE?pres ?
?MOOD?dcl ??ARG?
(d ?
design ?
?DET?the ?
?NUM?sg) ??PROP?
(p ?
based on ?
?ARTIFACT?d ??SOURCE?
(c ?
?NUM?sg ?
(?DET?the)?
?
(collection ?
series) ??HASPROP?
(f ?
Funny Day) ?
(?CREATOR?v ?
?GENOWNER?v ))))?
@v(Villeroy and Boch)(c)Figure 2: HLDS for examples in Figure 1.2 Disjunctive Logical FormsAs an illustration of disjunctive logical forms,consider the semantic dependency graphs in Fig-ure 1, which are taken from the COMIC1 mul-timodal dialogue system.2 Graphs such as thesetitute the input to the OpenCCG realizer.Each node has a lexical predication (e.g.
design)and a set of semantic features (e.g.
?NUM?sg);n des ar connected via dep ndency relations (e.g.?ARTIFACT?
).Given the l xical at gori in the COMICgrammar, the graphs n Figure 1(a) and (b) fullyspecify their respective realizations, with the ex-ception of the choice of the full or contractedfor of the copula.
To generalize over these al-ternatives, the disjunctive graph in (c) may beempl yed.
This graph allows a free choice be-tween the domain synonyms collection and se-ries, as indicated by the vertical bar betweentheir respective predications.
The graph also al-lows a free choice between the ?CREATOR?
and?GENOWNER?
relations?lexicalized via by andthe possessive, respectively?connecting the headc (collection or series) with the dependent v (for1http://www.hcrc.ed.ac.uk/comic/2To simplify the exposition, the features specifying infor-mati n structure and deictic gestures hav been omitted, ashave the semantic sorts of the discourse referents.Figure 1: Two imi ar logical forms from the COMICsystem as semantic ependency graphs, together with adisjunctive logical form representing their combinationas a packed semantic dependency graph.76(Baldridge and Kruijff, 2002; White, 2006b), orHLDS?constitute the input to the OpenCCG re-alizer.3 This graph allows a free choice betweenthe domain synonyms collection and series, as in-dicated by the vertical bar between their respec-tive predications.
The graph also allows a freechoice between the ?CREATOR?
and ?GENOWNER?relations?lexicalized via by and the possessive,respectively?connecting the head c (collection orseries) with the dependent v (for Villeroy and Boch);this choice is indicated by an arc between the twodependency relations.
Finally, the determiner fea-ture (?DET?the) on c is indicated as optional, via thequestion mark.
Note that as an alternative, the deter-miner feature could have been included in the dis-junction with the ?CREATOR?
relation (though thiswould have been harder to show graphically); how-ever, it is not necessary to do so, as constraints in thelexicalized grammar will ensure that the determineris not generated together with the possessive.3 Constructing DLFs from AlignedParaphrasesTo develop our approach, we use the gold-standardalignments in Cohn et al?s (2008) paraphrase cor-pus.
This corpus is constructed from three monolin-gual sentence-aligned paraphrase subcorpora fromdiffering text genres, with word-level alignmentsprovided by two human annotators.
We parse eachcorpus sentence pair using the OpenCCG parser toyield a logical form (LF) as a semantic dependencygraph with the gold-standard alignments projectedonto the LF pair.
Disjunctive LFs are then con-structed by inspecting the graph structure of each LFin comparison with the other.
Here, an alignment isrepresented simply as a pair ?n1, n2?
where n1 is anode in the first LF and n2 a node in the second LF.As Cohn et al?s corpus contains some block align-ments, there are cases where a single node is aligned3To be precise, the HLDS logical forms are descriptions ofsemantic dependency graphs, which in turn can be interpretedmodel theoretically via translation to Discourse RepresentationTheory (Kamp and Reyle, 1993), as White (2006b) explains.
Adisjunctive logical form is thus a description of a set of seman-tic dependency graphs.
(As the LFs derived using CCGbankgrammars do not represent quantifier scope properly, it wouldbe more accurate to call them quasi-LFs; as this issue does notappear to impact the realization or DLF creation algorithms,however, we have employed the simpler term.
)to multiple nodes in the other sentence of the para-phrase.A semantic dependency is represented as graphG = ?N,E?, where N = nodes(G) is the set ofnodes in G and E = edges(G) is the set of edgesin G. An edge e is a labeled dependency betweennodes, with source(e) denoting the source node,target(e) the target node, and label(e) the relatione represents.
For n, n?
?
nodes(G) members of theset of nodes for some graph G, n?
?
parents(n) ifand only if there is an edge e ?
edges(G) with n?
=source(e) and n = target(e).
The set ancestors(n)models the transitive closure of the ?parent-of?
re-lation: a ?
ancestors(n) if and only if there issome p ?
parents(n) such that either a = p ora ?
ancestors(p).
Nodes in a graph additionallybear associated predicates and semantic features thatare derived during the parsing process.3.1 The AlgorithmAs a preprocessing step, we first characterize the dif-ference between two LFs as a set of edit operationsvia MAKEEDITS(g1, g2, alignments), as detailedin Algorithm 1.
An insert results when the secondgraph contains an unaligned subgraph.
Similarly, anunaligned subgraph in the first LF is characterizedby a delete operation.
For both inserts and deletes,only the head of the inserted or deleted subgraph isrepresented as an edit in order to reflect the fact thatthese operations can encompass entire subgraphs.
Asubstitution occurs when a subgraph in the first LFis aligned to one or more subgraphs in the second LF.The case where subgraphs are block aligned corre-sponds to a multi-word phrasal substitution (for ex-ample, the substitution of Goldman for The US in-vestment bank in paraphrase (2), below).
The DLFgeneration process is then driven by these edit oper-ations.DLFs are created for each sentence by DIS-JUNCTIVIZE(g1, g2, alignments) and DISJUNC-TIVIZE(g2, g1, alignments), respectively, whereg1 is the first sentence?s LF and g2 the LF of thesecond (see Algorithm 2).
The DLF constructionprocess takes as inputs a pair of dependency graphs?g1, g2?
and a set of word-level alignments fromCohn et al?s (2008) paraphrase corpus projectedonto the graphs.
This process creates a DLF bymerging or making optional material from the sec-77Algorithm 1 Preprocesses a pair of aligned LFs representing a paraphrase into edit operations.1: procedure MAKEEDITS(g1, g2, alignments)2: for all i ?
{n ?
nodes(g2) | ??x.
?x, n?
?
alignments} do .
inserts3: if ?
?p.p ?
parents(i) ?
??x.
?x, p?
?
alignments then4: insert(i)5: for all d ?
{n ?
nodes(g1) | ??y.
?n, y?
?
alignments} do .
deletes6: if ?
?p.p ?
parents(d) ?
??y.
?p, y?
?
alignments then7: delete(d)8: for all s ?
nodes(g1) do .
substitutions9: if ?y.
?s, y?
?
alignments ?
?
?z.z ?
parents(y) ?
?s, z?
?
alignments then10: substitution(s, y)Algorithm 2 Constructs a disjunctive LF from an aligned paraphrase.1: procedure DISJUNCTIVIZE(g1, g2, alignments)2: MAKEEDITS(g1, g2, alignments)3: for all i ?
{n ?
nodes(g2) | insert(n)} do4: for all p ?
{e ?
edges(g2) | i = target(e)} do5: for all ?n1, n2?
?
{?x, y?
?
alignments | y = source(p)} do6: option(n1, p)7: for all d ?
{n ?
nodes(g1) | delete(n)} do8: for all p ?
{e ?
edges(g1) | d = target(e)} do9: option(source(p), p)10: for all s ?
{n ?
nodes(g1) | ?y.substitution(n, y)} do11: for all p ?
parents(s) do12: choice(p, {e ?
edges(g2) | substitution(s, target(e)) ?
?p, source(e)?
?
alignments})78ond LF into the first LF.As Algorithm 2 describes, first the inserts (line 3)and deletes (line 7) are handled.
In the case of in-serts, for each node i in the second LF that is thehead of an inserted subgraph, we find every n2 thatis the source of an edge p whose target is i. Theedge p is added as an option for each node n1 in thefirst LF that is aligned to n2.
The process for deletesis similar, modulo direction reversal.
We find everyedge p whose target is d, where d is the head of anunaligned subgraph in the first sentence, and make pan option for the parent node source(p).
With bothinserts and deletes, the intuitive idea is that an un-aligned subgraph should be treated as an optionaldependency from its parent.The following corpus sentence pair demonstratesthe handling of inserts/deletes:(1) a.
Justices said that the constitution allowsthe government to administer drugs onlyin limited circumstances.b.
In a 6-3 ruling, the justices said suchanti-psychotic drugs can be used only inlimited circumstances.In the DLF constructed for (1a), the node represent-ing the word drugs has two alternate children thatare not present in the first sentence itself (i.e., are in-serted), such and anti-psychotic, both of which are inthe modifier relation to drugs.
This happens becausedrugs is aligned to the word drugs in (1b), which hasthe modifier child nodes.
The second sentence alsocontains the insertion In a 6-3 ruling.
This entiresubgraph is represented as an optional modifier ofsaid.
Finally, the determiner the is inserted beforejustices in the second sentence.
This determiner isalso represented as an optional edge from justices.Figure 2 shows the portion of the DLF reflecting theoptional modifier In a 6-3 ruling and optional deter-miner the.For substitutions (line 10), we consider eachsubgraph-heading node s in the first LF that is sub-stituted for some node y in the second LF that isalso a subgraph head.
Then for each parent p of s,the choices for p are contained in the set of edgeswhose source is aligned to p and whose target is asubstitution for s. The intuition is that for each nodep in the first LF with an aligned subgraph c, there is adisjunction between c and the child subgraphs of thee say?TENSE?pastiinrrulingaa x 6-3j justicest the?MOD??ARG1??DET??MOD??ARG0?
?DET?Figure 2: Disjunctive LF subgraph for the alternation (Ina 6-3 ruling)?
(the)?
justices said .
.
.
in paraphrase (1).The dotted lines represent optional edges, and some se-mantic features are suppressed for readability.node that p is aligned to in the second LF.
For effi-ciency, in the special case of substitutions involvingsingle nodes rather than entire subgraphs, only thesemantic predicates are disjoined.4To demonstrate, consider the following corpussentence pair involving a phrasal substitution:(2) a.
The US investment bank said: we be-lieve the long-term prospects for the en-ergy sector in the UK remain attractive.b.
We believe the long-term prospects forthe energy sector in the UK remain at-tractive, Goldman said.In this paraphrase, the subtree The US investmentbank in (2a) is aligned to the single word Gold-man in (2b), but their predicates are obviously differ-ent.
The constructed DLF contains a choice betweenGoldman and The US investment bank as the subjectof said.
Figure 3 illustrates the relevant subgraph ofthe DLF constructed from the Goldman paraphrasewith a choice between subjects (?ARG0?).
This dis-junction arises because said in the first sentence isaligned to said in the second, and The US investmentbank is the subject of said in the first while Gold-man is its subject in the second.
Note that, sincethe substitution is a phrasal (block-aligned) one, theconstructed DLF forces a choice between Goldmanand the entire subgraph headed by bank, not between4We leave certain more complex cases, e.g.
multiple nodeswith aligned children, for future work.79e say?TENSE?pastggoldman b bankttheu usi inv.?ARG0?
?ARG0??DET??MOD?
?MOD?Figure 3: Disjunctive LF subgraph for the alternation(Goldman | The US investment bank) said .
.
.
in para-phrase (2).
The arc represents the two choice edges forthe ?ARG0?
relation from say.
Certain semantic dependen-cies are omitted, and the word investment is abbreviatedto save space.Goldman and each of bank?s dependents (the, US,and investment).4 Discussion and Future WorkWith a broad coverage grammar, we have found thatmost of the realization alternatives in an n-best listtend to reuse the same lexical choices, with the dif-ferences mostly consisting of alternate word ordersor use of function words or punctuation.
Accord-ingly, in order to enhance the variety of word andphrase choices in the n-best lists, we have takenadvantage of the API-level support for plugging incustom pruning strategies and developed a customstrategy that encourages lexical diversity.
This strat-egy groups realizations that share the same openclass stems into equivalence classes, where newequivalence classes are favored over new alterna-tives within the same equivalence class in filling upthe n-best list.Using this lexical diversity pruning strategy, anexample of the paraphrases generated after DLF cre-ation appears in Table 2.
In the example, the girland brianna are successfully alternated, as are hermother?s and the (bedroom).
The example also in-cludes a reasonable Heavy-NP shift, with into thebedroom appearing before the NP list.
Without thelexical diversity pruning strategy, the phrase hermother?s does not find its way into the n-best list.The paraphrases also include a mistaken change intense from had to has and a mysterious inclusion ofincluding.
Interestingly though, these mistakes fol-low in the n-best list alternatives that are otherwisethe same, suggesting that a final pruning of the listmay make it possible to keep only generally goodparaphrases.
(Note that the appositive 33 in the sec-ond reference sentence also has been dropped, mostlikely since the pruning strategy does not includenumbers in the set of content words at present.
)Although we have not yet formally evaluated theparaphrases, we can already characterize some re-curring errors.
Named entities are an issue sincewe have not incorporated a named entity recognizer;thus, the realizer is apt to generate O. Charles Princeinstead of Charles O.
Prince, for example.
Worse,medical examiner ?s spokeswoman ellen borakoveis realized both correctly and as medical examiner?s ellen spokeswoman borakove.
Naturally, thereare also paraphrasing errors that stem from parsererrors.
Certainly with named entities, though per-haps also with parser errors, we plan to investigatewhether we can take advantage of the word order inthe reference sentence in order to reduce the numberof mistakes.
Here, we plan to investigate whethera feature measuring similarity in word order to theoriginal can be balanced against the averaged per-ceptron model score in a way that allows new para-phrases to be generated while sticking to the orig-inal order in cases of uncertainty.
Initial experi-ments with adding to the perceptron model scorean n-gram precision score (approximating BLEU)with an appropriate weight indicate that realizationsincluding the correct word order in names such asCharles O.
Prince can be pushed to the top of then-best list, though it remains to be verified that theweight for the similarity score can be adequatelytuned with held-out data.
Incorporating a measure ofsimilarity to the original reference sentences into re-alization ranking is a form of what Madnani (2010)calls a self-paraphrase bias, though a different onethan his method of adjusting the probability mass as-signed to the original.In future work, we plan to evaluate the gener-ated paraphrases both intrinsically and extrinsicallyin combination with MT evaluation metrics.
Withthe intrinsic evaluation, we expect to examine theimpact of parser and alignment errors on the para-phrases, and the extent to which these can be miti-gated by a self-paraphrase bias, along with the im-pact of the lexical diversity pruning strategy on the80Reference 1 lee said brianna had dragged food , toys and other things into the bedroom .Realizations lee said the girl had dragged food , toys and other things into the bedroom .lee said brianna had dragged food , toys and other things into the bedroom .lee said , the girl had dragged [into the bedroom] food , toys and other things .lee said the girl has dragged into the bedroom food , toys and other things .lee said , brianna had dragged into the bedroom food , toys and other things .lee said the girl had dragged food , toys and other things into her mother ?s bedroom .lee said , the girl had dragged into her mother ?s bedroom food , toys and other things .lee said brianna had dragged food , toys and other things into her mother ?s bedroom .lee said the girl had dragged food , toys and other things into including the bedroom .lee said , brianna had dragged into her mother ?s bedroom food , toys and other things .Reference 2 lee , 33 , said the girl had dragged the food , toys and other things into her mother ?s bedroom .Realizations lee said the girl had dragged [into the bedroom] the food , toys and other things .lee said , the girl had dragged into the bedroom the food , toys and other things .lee said the girl has dragged into the bedroom the food , toys and other things .lee said brianna had dragged the food , toys and other things into the bedroom .lee said , brianna had dragged into the bedroom the food , toys and other things .lee said the girl had dragged the food , toys and other things into her mother ?s bedroom .lee said brianna had dragged into her mother ?s bedroom the food , toys and other things .lee said , the girl had dragged into her mother ?s bedroom the food , toys and other things .lee said brianna had dragged the food , toys and other things into her mother ?s bedroom .lee said the girl had dragged the food , toys and other things into including the bedroom .Table 2: Example n-best realizations starting from each reference sentence.
Alternative phrasings from the othermember of the pair are shown in italics the first time, and alternative phrase locations are shown in square brackets.Mistakes are underlined, and suppressed after the first occurrence in the list.number of acceptable paraphrases in the n-best list.With the extrinsic evaluation, we plan to investi-gate whether n-best paraphrase generation using themethods described here can be used to augment aset of reference translations in such a way as to in-crease the correlation of automatic metrics with hu-man judgments.
As Madnani observes, generatedparaphrases of reference translations may be eitheruntargeted or targeted to specific MT hypotheses.In the case of targeted paraphrases, the generatedparaphrases then approximate the process by whichautomatic translations are evaluated using HTER(Snover et al, 2006), with a human in the loop, asthe closest acceptable paraphrase of a reference sen-tence should correspond to the version of the MThypothesis with minimal changes to make it accept-able.
While in principle we might similarly acquireparaphrase rules using the pivot method, as in Mad-nani?s approach, such rules would be quite noisy, asit is a difficult problem to characterize the contextsin which words or phrases can be acceptably substi-tuted.
Thus, our immediate focus will be on gen-erating synthetic references with high precision, re-lying on grammatical alternations plus contextuallyacceptable alternatives present in multiple referencetranslations, given that metrics such as METEOR(Banerjee and Lavie, 2005) and TERp (Snover et al,2010) can now employ paraphrase matching as partof their scoring, complementing what can be donewith our methods.
To the extent that we can main-tain high precision in generating synthetic referencesentences, we may expect the correlations betweenautomatic metric scores and human judgments toimprove as the task of the metrics becomes simpler.AcknowledgementsThis work was supported in part by NSF grant num-ber IIS-0812297.
We are also grateful to TrevorCohn for help with the paraphrase data.ReferencesJason Baldridge and Geert-Jan Kruijff.
2002.
CouplingCCG and Hybrid Logic Dependency Semantics.
InProc.
ACL-02.Jason Baldridge.
2002.
Lexically Specified Derivational81Control in Combinatory Categorial Grammar.
Ph.D.thesis, School of Informatics, University of Edinburgh.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Proc.of the ACL-05 Workshop on Intrinsic and ExtrinsicEvaluation Measures for Machine Translation and/orSummarization.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proc.ACL-05, pages 597?604.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proc.
of NAACL-HLT.Regina Barzilay and Kathleen McKeown.
2005.
Sen-tence fusion for multidocument news summarization.Computational Linguistics, 31(3):297?327.Stephen Boxwell and Michael White.
2008.
ProjectingPropbank roles onto the CCGbank.
In Proc.
LREC-08.Stephen Clark and James R. Curran.
2007.
Wide-Coverage Efficient Statistical Parsing with CCG andLog-Linear Models.
Computational Linguistics,33(4):493?552.Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.2008.
Constructing corpora for the development andevaluation of paraphrase systems.
Computational Lin-guistics, 34(4):597?614.M.
Elhadad, J. Robin, and K. McKeown.
1997.
Floatingconstraints in lexical choice.
Computational Linguis-tics, 23(2):195?239.Dominic Espinosa, Michael White, and Dennis Mehay.2008.
Hypertagging: Supertagging for surface real-ization with CCG.
In Proceedings of ACL-08: HLT,pages 183?191, Columbus, Ohio, June.
Associationfor Computational Linguistics.Mary Ellen Foster and Michael White.
2007.
Avoidingrepetition in generated text.
In Proceedings of the 11thEuropean Workshop on Natural Language Generation(ENLG 2007).Claire Gardent and Eric Kow.
2005.
Generating and se-lecting grammatical paraphrases.
In Proc.
ENLG-05.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with supervisedITG models.
In Proceedings of ACL, pages 923?931,Suntec, Singapore, August.
Association for Computa-tional Linguistics.Julia Hockenmaier and Mark Steedman.
2002.
Gener-ative models for statistical parsing with CombinatoryCategorial Grammar.
In Proc.
ACL-02.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and DependencyStructures Extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.Deirdre Hogan, Jennifer Foster, Joachim Wagner, andJosef van Genabith.
2008.
Parser-based retrainingfor domain adaptation of probabilistic generators.
InProc.
INLG-08.Lidija Iordanskaja, Richard Kittredge, and AlainPolgu?ere.
1991.
Lexical selection and paraphrasein a meaning-text generation model.
In Ce?cile L.Paris, William R. Swartout, and William C. Mann, edi-tors, Natural Language Generation in Artificial Intelli-gence and Computational Linguistics, pages 293?312.Kluwer.Hans Kamp and Uwe Reyle.
1993.
From Discourse toLogic.
Kluwer.David Kauchak and Regina Barzilay.
2006.
Paraphras-ing for automatic evaluation.
In Proceedings of HLT-NAACL.Martin Kay.
1996.
Chart generation.
In Proc.
ACL-96.Irene Langkilde and Kevin Knight.
1998.
The practicalvalue of n-grams in generation.
In Proc.
INLG-98.Irene Langkilde-Geary.
2002.
An empirical verificationof coverage and correctness for a general-purpose sen-tence generator.
In Proc.
INLG-02.Bill MacCartney, Michel Galley, and Christopher D.Manning.
2008.
A phrase-based alignment model fornatural language inference.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 802?811, Honolulu, Hawaii,October.
Association for Computational Linguistics.Nitin Madnani.
2010.
The Circle of Meaning: FromTranslation to Paraphrasing and Back.
Ph.D. thesis,Department of Computer Science, University of Mary-land College Park.Crystal Nakatsu and Michael White.
2006.
Learning tosay it well: Reranking realizations by predicted syn-thesis quality.
In Proc.
COLING-ACL-06.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 1(29):19?52.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?106.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations: Ex-tracting paraphrases and generating new sentences.
InProc.
HLT/NAACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proc.
ACL-02.Brian Roark, Murat Saraclar, Michael Collins, and MarkJohnson.
2004.
Discriminative language modelingwith conditional random fields and the perceptron al-gorithm.
In Proc.
ACL-04.82Hadar Shemtov.
1997.
Ambiguity Management in Natu-ral Language Generation.
Ph.D. thesis, Stanford Uni-versity.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proc.
of the Association for Machine Translation inthe Americas (AMTA-06).Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2010.
TER-plus: Paraphrase,semantic, and alignment enhancements to translationedit rate.
Machine Translation, 23:117?127.M.
Stede.
1999.
Lexical Semantics and Knowledge Rep-resentation in Multilingual Text Generation.
KluwerAcademic Publishers.Mark Steedman.
2000.
The Syntactic Process.
MITPress.Erik Velldal, Stephan Oepen, and Dan Flickinger.
2004.Paraphrasing treebanks for stochastic realization rank-ing.
In Proceedings of the 3rd Workshop on Treebanksand Linguistic Theories.Michael White and Rajakrishnan Rajkumar.
2009.
Per-ceptron reranking for CCG realization.
In Proceedingsof the 2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 410?419, Singapore,August.
Association for Computational Linguistics.Michael White.
2004.
Reining in CCG Chart Realiza-tion.
In Proc.
INLG-04.Michael White.
2006a.
CCG chart realization from dis-junctive logical forms.
In Proc.
INLG-06.Michael White.
2006b.
Efficient Realization of Coordi-nate Structures in Combinatory Categorial Grammar.Research on Language & Computation, 4(1):39?75.Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li.
2009.Application-driven statistical paraphrase generation.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, pages 834?842, Suntec, Singapore, Au-gust.
Association for Computational Linguistics.83
