Proceedings of the Second Workshop on Statistical Machine Translation, pages 207?211,Prague, June 2007. c?2007 Association for Computational LinguisticsThe ?noisier channel?
: translation from morphologically complex languagesChristopher J. DyerDepartment of LinguisticsUniversity of MarylandCollege Park, MD 20742redpony@umd.eduAbstractThis paper presents a new paradigm fortranslation from inflectionally rich lan-guages that was used in the Universityof Maryland statistical machine transla-tion system for the WMT07 Shared Task.The system is based on a hierarchicalphrase-based decoder that has been aug-mented to translate ambiguous input givenin the form of a confusion network (CN),a weighted finite state representation of aset of strings.
By treating morphologi-cally derived forms of the input sequenceas possible, albeit more ?costly?
paths thatthe decoder may select, we find that sig-nificant gains (10% BLEU relative) canbe attained when translating from Czech,a language with considerable inflectionalcomplexity, into English.1 IntroductionMorphological analysis occupies a tenuous positionstatistical machine translation systems.
Conven-tional translation models are constructed with noconsideration of the relationships between lexicalitems and instead treat different inflected (observed)forms of identical underlying lemmas as completelyindependent of one another.
While the variouslyinflected forms of one lemma may express differ-ences in meaning that are crucial to correct transla-tion, the strict independence assumptions normallymade exacerbate data sparseness and lead to poorlyestimated models and suboptimal translations.
A va-riety of solutions have been proposed: Niessen andNey (2001) use of morphological information to im-prove word reordering before training and after de-coding.
Goldwater and McClosky (2005) show im-provements in a Czech to English word-based trans-lation system when inflectional endings are simpli-fied or removed entirely.
Their method can, how-ever, actually harm performance since the discardedmorphemes carry some information that may havebearing on the translation (cf.
Section 3.3).
To avoidthis pitfall, Talbot and Osborne (2006) use a data-driven approach to cluster source-language morpho-logical variants that are meaningless in the targetlanguage, and Yang and Kirchhoff (2006) proposethe use of a backoff model that uses morphologicallyreduced forms only when the translation of the sur-face form is unavailable.
All of these approacheshave in common that the decisions about whether touse morphological information are made in either apre- or post-processing step.Recent work in spoken language translation sug-gests that allowing decisions about the use of mor-phological information to be made along side othertranslation decisions (i.e., inside the decoder), willyield better results.
At least as early as Ney (1999),it has been shown that when translating the out-put from automatic speech regonition (ASR) sys-tems, the quality can be improved by consideringmultiple (rather than only a single best) transcrip-tion hypothesis.
Although state-of-the-art statisticalmachine translation systems have conventionally as-sumed unambiguous input; recent work has demon-strated the possibility of efficient decoding of am-207biguous input (represented as confusion networks orword lattices) within standard phrase-based models(Bertoldi et al, to appear 2007) as well as hierarchi-cal phrase-based models (Dyer and Resnik, 2007).These hybrid decoders search for the target languagesentence e?
that maximizes the following probability,where G(o) represents the set of weighted transcrip-tion hypotheses produced by an ASR decoder:e?
= argmaxemaxf ?
?G(o)P (e, f ?|o) (1)The conditional probability p(e, f |o) that is maxi-mized is modeled directly using a log-linear model(Och and Ney, 2002), whose parameters can betuned to optimize either the probability of a devel-opment set or some other objective (such as max-imizing BLEU).
In addition to the standard trans-lation model features, the ASR system?s posteriorprobability is another feature.
The decoder thusfinds a translation hypothesis e?
that maximizes thejoint translation/transcription probability, which isnot necessarily the one that corresponds to the bestsingle transcription hypothesis.2 Noisier channel translationWe extend the concept of translating from an am-biguous set of source hypotheses to the domain oftext translation by redefining G(?)
to be a set ofweighted sentences derived by applying morpholog-ical transformations (such as stemming, compoundsplitting, clitic splitting, etc.)
to a given source sen-tence f .
This model for translation extends the usualnoisy channel metaphor by suggesting that an ?En-glish?
source signal is first distorted into a morpho-logically neutral ?French?
and then morphologicalprocesses represent a further distortion of the signal,which can be modeled independently.
Whereas inthe context of an ASR transcription hypothesis, G(?
)assigns a posterior probability to each sentence, weredefine of this value to be a backoff penalty.
Thiscan be intuitively thought of as a measure of the?distance?
that a given morphological alternative isfrom the observed input sentence.The remainder of the paper is structured as fol-lows.
In Section 2, we describe the basic hierarchi-cal translation model.
In Section 3, we describe thedata and tools used and present experimental resultsfor Czech-English.
Section 4 concludes.3 Hierarchical phrase-based decodingChiang (2005; to appear 2007) introduced hierar-chical phrase-based translation models, which areformally based on synchronous context-free gram-mars.
These generalize phrase-based translationmodels by allowing phrase pairs to contain vari-ables.
Like phrase correspondences, the correspond-ing synchronous grammar rules can be learned auto-matically from aligned, but otherwise unannotated,training bitext.
For details about the extraction algo-rithm, refer to Chiang (to appear 2007).The rules of the induced grammar consist of pairsof strings of terminals and non-terminals in thesource and target languages, as well one-to-one cor-respondences between non-terminals on the sourceand target side of each pair (shown as indexes inthe examples below).
Thus they encapsulate notonly meaning translation (of possibly discontinuousspans), but also typical reordering patterns.
For ex-ample, the following two rules were extracted fromthe Spanish ?
English segment of the Europarl cor-pus (Koehn, 2003):X ?
?la X1de X2,X2?s X1?
(2)X ?
?el X1verde, the green X1?
(3)Rule (2) expresses the fact that possessors canbe expressed prior to the possessed object in En-glish but must follow in Spanish.
Rule (3) showsthat the adjective verde follows the modified expres-sion in Spanish whereas the corresponding Englishlexical item green precedes what it modifies.
Al-though the rules given here correspond to syntacticconstituents, this is accidental.
The grammars ex-tracted make use of only a single non-terminal cate-gory and variables are posited that may or may notcorrespond to linguistically meaningful spans.Given a synchronous grammar G, the translationprocess is equivalent to parsing an input sentencewith the source side of G and thereby inducing atarget sentence.
The decoder we used is based onthe CKY+ algorithm, which permits the parsing ofrules that are not in Chomsky normal form (Chep-palier and Rajman, 1998) and that has been adaptedto admit input that is in the form of a confusion net-work (Dyer and Resnik, 2007).
To incorporate target208Language Tokens Types SingletonsCzech surface 1.2M 88037 42341Czech lemmas 1.2M 34227 13129Czech truncated 1.2M 37263 13093English 1.4M 31221 10508Spanish 1.4M 47852 20740French 1.2M 38241 15264German 1.4M 75885 39222Table 1: Corpus statistics, by language, for theWMT07 training subset of the News Commentarycorpus.language model probabilities into the model, whichis important for translation quality, the grammar isintersected during decoding with an m-gram lan-guage model.
This process significantly increasesthe effective size of the grammar, and so a beam-search heuristic called cube pruning is used, whichhas been experimentally determined to be nearly aseffective as an exhaustive search but far more effi-cient.4 ExperimentsWe carried out a series of experiments using differ-ent strategies for making use of morphological in-formation on the News Commentary Czech-Englishdata set provided for the WMT07 Shared Task.Czech was selected because it exhibits a rich inflec-tional morphology, but its other morphological pro-cesses (such as compounding and cliticization) thataffect multiple lemmas are relatively limited.
Thishas the advantage that a morphologically simpli-fied (i.e., lemmatized) form of a Czech sentence hasthe same number of tokens as the surface form haswords, which makes representing G(f) as a confu-sion network relatively straightforward.
The relativemorphological complexity of Czech, as well as thepotential benefits that can be realized by stemming,can be inferred from the corpus statistics given inTable 1.4.1 Technical detailsA trigram English language model with modifiedKneser-Ney smoothing (Kneser and Ney, 1995) wastrained using the SRI Language Modeling Toolkit(Stolcke, 2002) on the English side of the NewsCommentary corpus as well as portions of theGigaWord v2 English Corpus and was used forall experiments.
Recasing was carried out usingSRI?s disambig tool using a trigram languagemodel.
The feature set used included bidirectionaltranslation probabilities for rules, lexical transla-tion probabilities, a target language model proba-bility, and count features for target words, num-ber of non-terminal symbols used, and finally thenumber of morphologically simplified forms se-lected in the CN.
Feature weight tuning was carriedout using minimum error rate training, maximizingBLEU scores on a held-out development set (Och,2003).
Translation scores are reported using case-insensitive BLEU (Papineni et al, 2002) with a sin-gle reference translation.
Significance testing wasdone using bootstrap resampling (Koehn, 2004).4.2 Data preparation and trainingWe used a Czech morphological analyzer by Hajic?and Hladka?
(1998) to extract the lemmas from theCzech portions of the training, development, andtest data (the Czech-English portion of the NewsCommentary corpus distributed as as part of theWMT07 Shared Task).
Data sets consisting of trun-cated forms were also generated; using a length limitof 6, which Goldwater and McClosky (2005) exper-imentally determined to be optimal for translationperformance.
We refer to the three data sets and themodels derived from them as SURFACE, LEMMA,and TRUNC.
Czech?English grammars were ex-tracted from the three training sets using the meth-ods described in Chiang (to appear 2007).
Two ad-ditional grammars were created by combining therules from the SURFACE grammar and the LEMMAor TRUNC grammar and renormalizing the condi-tional probabilities, yielding the combined modelsSURFACE+LEMMA and SURFACE+TRUNC.Confusion networks for the development and testsets were constructed by providing a single back-off form at each position in the sentence where thelemmatizer or truncation process yielded a differentword form.
The backoff form was assigned a cost of1 and the surface form a cost of 0.
Numbers andpunctuation were not truncated.
A ?backoff?
set,corresponding approximately to the method of Yangand Kirchhoff (2006) was generated by lemmatiz-ing only unknown words.
Figure 1 shows a samplesurface+lemma CN from the test set.2091 2 3 4 5 6 7 8 9 10 11 12z americke?ho br?ehu atlantiku se veskera?
takova?
odu?vodne?n??
jev??
jako naprosto bizarn??americky?
br?eh atlantik s takovy?
jevitFigure 1: Example confusion network generated by lemmatizing the source sentence to generate alternates ateach position in the sentence.
The upper element in each column is the surface form and the lower element,when present, is the lemma.Input BLEU Sample translationSURFACE 22.74 From the US side of the Atlantic all such odu?vodne?n??
appears to be a totally bizarre.LEMMA 22.50 From the side of the Atlantic with any such justification seem completely bizarre.TRUNC (l=6) 22.07 From the bank of the Atlantic, all such justification appears to be totally bizarre.backoff (SURFACE+LEMMA) 23.94 From the US bank of the Atlantic, all such justification appears to be totally bizarre.CN (SURFACE+LEMMA) 25.01 From the US side of the Atlantic all such justification appears to be a totally bizarre.CN (SURFACE+TRUNC) 23.57 From the US Atlantic any such justification appears to be a totally bizarre.Table 2: Czech-English results on WMT07 Shared Task DEVTEST set.
The sample translations are transla-tions of the sentence shown in Figure 1.4.3 Experimental resultsTable 2 summarizes the performance of the sixCzech?English models on the WMT07 SharedTask development set.
The basic SURFACE modeltends to outperform both the LEMMA and TRUNCmodels, although the difference is only marginallysignificant.
This suggests that the Goldwater andMcClosky (2005) results are highly dependent onthe kind of translation model and quantity of data.The backoff model, a slightly modified versionof the method proposed by Yang and Kirchhoff(2006),1 does significantly better than the baseline(p < .05).
However, the joint (SURFACE+LEMMA)model outperforms both surface and backoff base-lines (p < .01 and p < .05, respectively).
The SUR-FACE+TRUNC model is an improvement over theSURFACE model, but it performances significantlyworse than the SURFACE+LEMMA model.5 ConclusionWe presented a novel model-driven method for us-ing morphologically reduced forms when translat-ing from a language with complex inflectional mor-1Our backoff model has two primary differences frommodeldescribed by Y&K.
The first is that our model effectively cre-ates backoff forms for every surface string, whereas Y&K dothis only for forms that are not found in the surface string.
Thismeans that in our model, the probabilities of a larger numberof surface rules have been altered by backoff discounting thanwould be the case in the more conservative model.
Second, thejoint model we used has the benefit of using morphologicallysimpler forms to improve alignment.phology.
By allowing the decoder to select amongthe surface form of a word or phrase and variantsof morphological alternatives on the source side,we outperform baselines where hard decisions aboutwhat form to use are made in advance of decod-ing, as has typically been done in systems that makeuse of morphological information.
This ?decoder-guided?
incorporation of morphology was enabledby adopting techniques for translating from ambigu-ous sources that were developed to address problemsspecific to spoken language translation.
Althoughthe results presented here were obtained using a hi-erarchical phrase-based system, the model general-izes to any system where the decoder can accept aweighted word graph as its input.AcknowledgementsThe author would like to thank David Chiang formaking the Hiero decoder sources available to usand Daniel Zeman for his assistance in the prepara-tion of the Czech data.
This work was generouslysupported by the GALE program of the DefenseAdvanced Research Projects Agency, Contract No.HR0011-06-2-0001.ReferencesN.
Bertoldi, R. Zens, and M. Federico.
to appear 2007.
Speechtranslation by confusion network decoding.
In 32nd Inter-national Conference on Acoustics, Speech, and Signal Pro-cessing (ICASSP), Honolulu, Hawaii, April.210J.
Cheppalier and M. Rajman.
1998.
A generalized CYKalgorithm for parsing stochastic CFG.
In Proceedingsof the Workshop on Tabulation in Parsing and Deduction(TAPD98), pages 133?137, Paris, France.D.
Chiang.
to appear 2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2).C.
Dyer and P. Resnik.
2007.
Word Lattice Parsing for Sta-tistical Machine Translation.
Technical report, University ofMaryland, College Park, April.S.
Goldwater and D. McClosky.
2005.
Improving statisticalmt through morphological analysis.
In Proceedings of Hu-man Language Technology Conference and Conference onEmpirical Methods in Natural Language Processing, pages676?683, Vancouver, British Columbia.J.
Hajic?
and B. Hladka?.
1998.
Tagging inflective languages:Prediction of morphological categories for a rich, structuredtagset.
In Proceedings of the COLING-ACL Conference,pages 483?490.R.
Kneser and H. Ney.
1995.
Improved backing-off for m-gram language modeling.
In Proceedings of IEEE Interna-tion Conference on Acoustics, Speech, and Signal Process-ing, pages 181?184.P.
Koehn.
2003.
Europarl: A multilingual corpus for evaluationof machine translation.
Draft, unpublished.P.
Koehn.
2004.
Statistical signficiance tests for machinetranslation evluation.
In Proceedings of the 2004 Confer-ence on Empirical Methods in Natural Language Processing(EMNLP), pages 388?395.H.
Ney.
1999.
Speech translation: Coupling of recognitionand translation.
In IEEE International Conference on Acous-tic, Speech and Signal Processing, pages 517?520, Phoenix,AR, March.S.
Niessen and H. Ney.
2001.
Morpho-syntactic analysis forreordering in statistical machine translation.
In Proceedingsof MT Summit VIII, Santiago de Compostela, Galicia, Spain.F.
Och and H. Ney.
2002.
Discriminitive training and maxi-mum entropy models for statistical machine translation.
InProceedings of the 40th Annual Meeting of the ACL, pages295?302.F.
Och.
2003.
Minimum error rate training in statistical ma-chine translation.
In Proceedings of the 41st Annual Meet-ing of the Association for Computational Linguistics (ACL),pages 160?167, Sapporo, Japan, July.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.
Bleu: amethod for automatic evaluation of machine translation.
InProceedings of the 40th Annual Meeting of the ACL, pages311?318.A.
Stolcke.
2002.
SRILM ?
an extensible language modelingtoolkit.
In Intl.
Conf.
on Spoken Language Processing.D.
Talbot and M. Osborne.
2006.
Modelling lexical redun-dancy for machine translation.
In Proceedings of ACL 2006,Sydney, Australia.M.
Yang and K. Kirchhoff.
2006.
Phrase-based backoff mod-els for machine translation of highly inflected languages.
InProceedings of the EACL 2006, pages 41?48.211
