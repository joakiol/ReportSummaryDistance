Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1073?1080Manchester, August 2008Automatic Seed Word Selection for Unsupervised SentimentClassification of Chinese TextTaras Zagibalov    John CarrollUniversity of SussexDepartment of InformaticsBrighton  BN1 9QH, UK{T.Zagibalov,J.A.Carroll}@sussex.ac.ukAbstractWe describe and evaluate a new methodof automatic seed word selection for un-supervised  sentiment  classification  ofproduct  reviews  in  Chinese.
The  wholemethod is unsupervised and does not re-quire any annotated training data; it onlyrequires information about commonly oc-curring negations  and adverbials.
Unsu-pervised  techniques  are  promising  forthis task since they avoid problems of do-main-dependency  typically  associatedwith supervised methods.
The results ob-tained  are  close  to  those  of  supervisedclassifiers and sometimes better, up to anF1 of 92%.1 IntroductionAutomatic classification of document  sentiment(and more generally extraction of opinion fromtext) has recently attracted a lot of interest.
Oneof the main reasons for this is the importance ofsuch  information  to  companies,  otherorganizations,  and  individuals.
Applicationsinclude  marketing  research  tools  that  help  acompany see market or media reaction towardstheir  brands,  products  or  services,  or  searchengines  that  help potential  purchasers  make  aninformed choice of a product they want to buy.Sentiment  classification  research  has  drawn onand contributed to research in text classification,unsupervised  machine  learning,  and  cross-domain adaptation.This paper presents a new, automatic approachto automatic seed word selection as part of senti-ment classification of product reviews written inChinese,  which  addresses  the  problem  of  do-?
2008.
Licensed under the Creative Commons Attribu-tion-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Somerights reserved.main-dependency of sentiment classification thathas been observed in previous work.
It may alsofacilitate  building  sentiment  classification  sys-tems in other languages since the approach as-sumes a very small amount of linguistic knowl-edge: the only language-specific information re-quired is a basic description of the most frequentnegated adverbial constructions in the language.The paper is structured as follows.
Section 2surveys related work in sentiment classification,unsupervised  machine  learning  and  Chineselanguage  processing.
Section  3  motivates  ourapproach,  which  is  described  in  detail  inSection 4.
The  data  used  for  experiments  andbaselines,  as well  as the results of  experimentsare covered in Section 5.
Section 6 discusses thelessons learned and proposes directions for futurework.2 Related Work2.1 Sentiment ClassificationMost work on sentiment classification has usedapproaches based on supervised machine learn-ing.
For  example,  Pang  et  al.
(2002)  collectedmovie reviews that had been annotated with re-spect to sentiment by the authors of the reviews,and used this data to train supervised classifiers.A number of studies have investigated the impacton classification accuracy of different factors, in-cluding choice of  feature set,  machine learningalgorithm,  and pre-selection of the segments oftext  to  be  classified.
For  example,  Dave  et  al.
(2003) experiment with the use of linguistic, sta-tistical and n-gram features and measures for fea-ture  selection  and  weighting.
Pang  and  Lee(2004)  use  a  graph-based technique to  identifyand  analyze  only subjective  parts  of  texts.
Yuand  Hatzivassiloglou  (2003)  use  semantically-oriented  words  for  identification  of  polarity  atthe sentence level.
Most of this work assumes bi-nary classification (positive and negative), some-1073times  with  the  addition  of  a  neutral  class  (interms of polarity, representing lack of sentiment).While  supervised  systems  generally  achievereasonably high accuracy, they do so only on testdata that is similar to the training data.
To moveto another domain one would have to collect an-notated data in the new domain and retrain theclassifier.
Engstr?m (2004) reports decreased ac-curacy in cross-domain classification since senti-ment in different domains is often expressed indifferent ways.
However, it is impossible in prac-tice to have annotated data for all  possible do-mains  of  interest.
Aue  and  Gamon  (2005)  at-tempt  to  solve  the  problem of  the  absence  oflarge  amounts  of  labeled  data  by  customizingsentiment classifiers to new domains using train-ing data from other domains.
Blitzer et al (2007)investigate domain adaptation for sentiment clas-sifiers using structural correspondence learning.Read  (2005)  also  observed  significant  differ-ences between the accuracy of classification ofreviews in the same domain but published in dif-ferent time periods.Recently, there has been a shift of interest to-wards more fine-grained approaches to process-ing of sentiment, in which opinion is extracted atthe sentence level, sometimes including informa-tion about different features of a product that arecommented on and/or the opinion holder (Hu andLiu, 2004; Ku et al, 2006).
But even in such ap-proaches, McDonald et al (2007) note that infor-mation about the overall sentiment orientation ofa document  facilitates more  accurate extractionof more specific information from the text.2.2 Unsupervised ApproachOne way of tackling the problem of domain de-pendency could be to use an approach that doesnot  rely  on  annotated  data.
Turney  (2002)  de-scribes a method of sentiment classification  us-ing two human-selected seed words (the wordspoor and  excellent)  in conjunction with a verylarge  text  corpus;  the  semantic  orientation  ofphrases is computed as their association with theseed words (as measured by pointwise mutual in-formation).
The sentiment of a document is cal-culated as the average semantic orientation of allsuch phrases.Yarowsky  (1995)  describes  a  'semi-unsuper-vised' approach to the problem of sense disam-biguation  of  words,  also  using  a  set  of  initialseeds, in this case a few high quality sense anno-tations.
These annotations are used to start an it-erative process of learning information about thecontexts  in  which  senses  of  words  appear,  ineach iteration labeling senses of previously unla-beled  word  tokens  using  information  from theprevious iteration.2.3 Chinese Language ProcessingA major issue in processing Chinese text is thefact that words are not delimited in the writtenlanguage.
In many cases, NLP researchers work-ing  with  Chinese  use  an  initial  segmentationmodule  that  is  intended  to  break  a  text  intowords.
Although  this  can  facilitate  the  use  ofsubsequent computational techniques, there is noa clear definition of what a 'word' is in the mod-ern Chinese  language,  so the  use  of  such  seg-menters is of dubious theoretical status; indeed,good  results  have  been  reported  from systemswhich do not assume such pre-processing (Fooand Li, 2004; Xu et al, 2004).2.4 Seed Word SelectionWe are not aware of any sentiment analysis sys-tem that uses unsupervised seed word selection.However, Pang et al (2002) showed that it is dif-ficult  to  get  good coverage of  a  target  domainfrom manually selected words, and even simplecorpus  frequency counts  may  produce  a  betterlist of features for supervised classification: hu-man-created lists resulted in 64% accuracy on amovie  review  corpus,  while  a  list  of  frequentwords scored 69%.
Pang et al also observed thatsome  words  without  any  significant  emotionalorientation were quite good features: for exam-ple, the word ?still?
turned out to be a good indi-cator of positive reviews as it was often used insentences  such  as  ?Still,  though,  it  was  worthseeing''.3 Our ApproachOur main goal is to overcome the problem of do-main-dependency  in  sentiment  classification.Unsupervised approaches seem promising in thisregard, since they do not require annotated train-ing data, just access to sufficient raw text in eachdomain.
We base our approach on a previouslydescribed,  'almost-unsupervised'  system  thatstarts with only a single, human-selected seed ?
(good) and uses an iterative method to extract atraining sub-corpus (Zagibalov & Carroll, 2008).The approach does not use a word segmentationmodule;  in  this  paper  we use  the  term 'lexicalitem' to denote any sequence of Chinese charac-ters that is treated by the system as a unit, what-ever it is linguistically ?
a morpheme, a word ora phrase.1074Our initial aim was to investigate ways of im-proving the classifier by automatically finding abetter seed, because Zagibalov & Carroll indicatethat in different domains they could, by manualtrial and error, find a seed other than  ?
(good)which produced better results.To find such a seed automatically,  we maketwo assumptions:1.
Attitude  is  often  expressed  through  thenegation of vocabulary items with the op-posite meaning; for example in Chinese itis  more  common  to  say  not  good thanbad  (Tan,  2002).
Zagibalov  &  Carroll'ssystem uses this observation to find nega-tive lexical items while nevertheless start-ing only from a positive seed.
This leadsus  to  believe  that  it  is  possible  to  findcandidate  seeds  themselves  by  lookingfor  sequences  of  characters  which  areused with negation.2.
The polarity of a candidate seed needs tobe determined.
To do this we assume wecan use the lexical item   ?
(good) as agold  standard  for  positive  lexical  itemsand  compare  the  pattern  of  contexts  acandidate seed occurs in to the pattern ex-hibited by the gold standard.Looking at product review corpora, we observedthat  good is  always  more  often  used  withoutnegation in positive texts, while in negative textsit  is  more  often  used  with  negation  (e.g.
notgood).
Also,  good occurs more often in positivetexts than negative, and more frequently withoutnegation than with it.
We use the latter observa-tion  as  the  basis  for  identifying  seed  lexicalitems,  finding those which occur with negationbut more frequently occur without it.As well as detecting negation1 we also use ad-verbials2 to  avoid  hypothesizing  non-contentfulseeds: the characters following the sequence of anegation and an adverbial are in general content-ful units, as opposed to parts of words, functionwords, etc.
In what follows we refer to such con-structions as negated adverbial constructions.1We use only six frequently occurring negations: ?
(bu), ??
(buhui), ??
(meiyou), ??
(baituo), ??
(mianqu),and ??
(bimian).
We are trying to be as language-inde-pendent as possible so we take a simplistic approach to de-tecting negation.2We use five frequently occurring adverbials: ?
(hen), ??
(feichang), ?
(tai), ?
(zui), and ??
(bijiao).
Similarly tonegation, we deliberately take a simplistic approach.4 MethodWe use a similar  sentiment  classifier and itera-tive retraining technique to the almost-unsuper-vised  system  of  Zagibalov  &  Carroll  (2008),summarized below in Sections 4.2 and 4.3.
Themain  new contributions  of  this  paper  are  tech-niques for automatically finding the seeds fromraw text in a particular domain (Section 4.1), andfor detecting when the process should stop (Sec-tion 4.4).
This new system therefore differs fromthat of Zagibalov & Carroll (2008) in being com-pletely unsupervised and not depending on arbi-trary iteration limits.
(The evaluation also differssince we focus in this paper on the effects of do-main on sentiment classification accuracy).4.1 Seed Lexical Item IdentificationThe first step is to identify suitable positive seedsfor  the  given  corpus.
The  intuition  behind  theway this is done is outlined above in Section 3.The algorithm is as follows:1. find all sequences of characters betweennon-character  symbols  (i.e.
punctuationmarks,  digits  and  so  on)  that  containnegation  and  an  adverbial,  split  the  se-quence at the negation, and store the char-acter  sequence  that  follows  the  negatedadverbial construction;2. count the number of occurrences of eachdistinct  sequence that  follows a negatedadverbial construction (X);3. count the number of occurrences of eachdistinct sequence without the construction(Y);4. find all sequences with Y ?
X > 0.4.2 Sentiment ClassificationThis  approach  to  Chinese  language  processingdoes not use pre-segmentation (in the sense dis-cussed in Section 2.3) or grammatical analysis:the basic unit of processing is the 'lexical item',each of which is a sequence of one or more Chi-nese characters excluding punctuation marks (soa lexical item may actually form part of a word, awhole word or a sequence of words), and 'zones',each of which is a sequence of characters delim-ited by punctuation marks.Each  zone  is  classified  as  either  positive  ornegative based whether positive or negative vo-cabulary  items  predominate.
As  there  are  twoparts of the vocabulary (positive and negative),we  correspondingly  calculate  two  scores  (Si ,1075where  i is  either  positive or  negative)  usingEquation (1), where Ld is the length in charactersof a matching lexical item (raised to the power oftwo to increase the significance of longer itemswhich capture more context),  Lphrase is the lengthof the current zone in characters, Sd is the currentsentiment score of the matching lexical item (ini-tially 1.0), and Nd is a negation check coefficient.S i= Ld2L phrase S d N d(1)The negation check is a regular expression whichdetermines  if  the lexical  item is  preceded by anegation within its enclosing zone.
If a negationis found then Nd is set to ?1.The sentiment  score of a zone is the sum ofsentiment of all the items found in it.To determine the sentiment orientation of thewhole document, the classifier computes the dif-ference between the number of positive and neg-ative zones.
If the result is greater than zero thedocument is classified as positive, and vice ver-sa.4.3 Iterative RetrainingIterative retraining is used to enlarge the initialseed  vocabulary into  a  comprehensive  vocabu-lary  list  of  sentiment-bearing  lexical  items.
Ineach iteration, the current version of the classifieris run on the input corpus to classify each docu-ment,  resulting in a training subcorpus of posi-tive and a negative documents.
The subcorpus isused to adjust the scores of existing positive andnegative vocabulary items and to find new itemsto be included in the vocabulary.Each lexical item that occurs at least twice inthe corpus is a candidate for inclusion in the vo-cabulary list.
After candidate items are found, thesystem  calculates  their  relative  frequencies  inboth the positive and negative parts of the currenttraining subcorpus.
The system also checks fornegation while counting occurrences: if a lexicalitem is preceded by a negation, its count is re-duced by one.For all candidate items we compare their rela-tive frequencies in the positive and negative doc-uments in the subcorpus using Equation (2).difference= ?F p?
F n?
?F p?Fn?/2(2)If difference < 1, then the frequencies are similarand the item does not have enough distinguishingpower,  so it  is  not  included in  the vocabulary.Otherwise  the  sentiment  score  of  the  item  is(re-)calculated  ?
according  to  Equation  (3)  forpositive  items,  and  analogously  for  negativeitems.F p?Fn         (3)Finally, the adjusted vocabulary list with the newscores is ready for the next iteration3.4.4 Iteration ControlTo maximize the number of productive iterationswhile avoiding unnecessary processing and arbi-trary  iteration  limits,  iterative  retraining  isstopped when there is no change to the classifica-tion of any document over the previous two itera-tions.5 Experiments5.1 DataAs our approach is unsupervised, we do not usean annotated training corpus, but run our iterativeprocedure on the raw data extracted from an an-notated test corpus, and evaluate the final accura-cy of the system with respect to the annotationsin that corpus.Our  test  corpus  is  derived  from product  re-views harvested from the website IT1684.
All thereviews  were  tagged by their  authors  as  eitherpositive or negative overall.
Most reviews con-sist of two or three distinct parts: positive opin-ions, negative opinions, and comments ('other') ?although some reviews have only one part.
Weremoved  duplicate  reviews  automatically  usingapproximate matching, giving a corpus of 29531reviews of which 23122 are positive (78%) and6409 are  negative  (22%).
The  total  number  ofdifferent  products  in  the  corpus  is  10631,  thenumber of product categories is 255, and most ofthe reviewed products are either software prod-ucts  or  consumer  electronics.
Unfortunately,  itappears  that  some  users  misuse  the  sentiment3An alternative approach might be to use point-wise mutualinformation instead of relative frequencies of newly foundfeatures in a subcorpus produced in the previous iteration.However, in preliminary experiments, SO-PMI did not pro-duce good corpora from the first iteration.
Also, it is notclear how to manage subsequent iterations since PMI wouldhave to be calculated between thousands of new vocabularyitems and every newly found sequence of characters, whichwould be computationally intractable.4http://product.it168.com1076tagging facility on the website so quite a lot ofreviews have incorrect tags.
However, the partsof the reviews are much more reliably identifiedas being positive or negative so we used these asthe items of the test corpus.
In the experimentsdescribed below we use 10 subcorpora contain-ing a total of 7982 reviews, distributed betweenproduct types as shown in Table 1.Corpus/product type ReviewsMonitors 683Mobile phones 2317Digital cameras 1705MP3 players 779Computer  parts  (CD-drives,  mother-boards)308Video cameras and lenses 361Networking (routers, network cards) 350Office equipment (copiers,multifunction devices, scanners)611Printers (laser, inkjet) 569Computer peripherals (mice, keyboards,speakers)457Table 1.
Product types and sizes of the testcorpora.We constructed five of the corpora by combin-ing smaller ones of 100?250 reviews each (as in-dicated  in  parentheses  in  Table  1)  in  order  tohave reasonable amounts of data.Each corpus has equal numbers of positive andnegative reviews so we can derive upper boundsfrom the corpora (Section 5.2)  by applying su-pervised  classifiers.
We  balance  the  corporasince (at least on this data) these classifiers per-form less well with skewed class distributions5.5.2 Baseline and Upper BoundSince the  corpora  are  balanced with respect  tosentiment  orientation  the  na?ve  (unsupervised)baseline  is  50%.
We  also  produced  an  upperbound  using  Naive  Bayes  multinomial  (NBm)and Support Vector Machine (SVM)6 classifierswith the NTU Sentiment  Dictionary (Ku  et al,2006)  vocabulary items  as  the  feature  set.
Thedictionary contains  2809 items  in  the  'positive'part  and  8273  items  in  the  'negative'.
We  ran5We have made this corpus publicly available at http://www.informatics.sussex.ac.uk/users/tz21/coling08.zip6We used WEKA 3.4.11 (http://www.cs.waikato.ac.nz/?ml/weka )both classifiers in 10-fold stratified cross-valida-tion mode, resulting in the accuracies shown inTable 2.
The macroaveraged accuracies across all10  corpora  are  82.78%  (NBm)  and  80.89%(SVM).Corpus Nbm(%)SVM(%)Monitors 86.21 83.87Mobile phones 86.52 84.49Digital cameras 82.27 82.04MP3 players 82.64 79.43Computer parts 81.10 79.47Video cameras and lenses 83.05 84.16Networking 77.65 75.35Office equipment 82.13 80.00Printers 81.33 79.57Computer peripherals 84.86 80.48Table 2.
Upper bound accuracies.We also tried adding the negations and adver-bials specified in Section 3 to the feature set, andthis resulted in slightly improved accuracies, of83.90% (Nbm) and 82.49% (SVM).An alternative  approach would have been toautomatically segment the reviews and then de-rive a feature set of a manageable size by settinga threshold on word frequencies; however the ex-tra processing means that this is a less valid up-per bound.Another possible comparison could be with aversion of Turney's (2002) sentiment  classifica-tion method applied to Chinese.
However, the re-sults  would  not  be  comparable  since  Turney'smethod would require the additional use of verylarge  text  corpus  and  the  manual  selection  ofpositive and negative seed words.5.3 Experiment 1To be able to compare to the accuracy of the al-most-unsupervised  approach  of  Zagibalov  &Carroll (2008), we ran our system using the seed?
(good) for each corpus.
The results are shownin Table 3.
We compute precision, recall and F1measure rather than just accuracy, since our clas-sifier can omit some reviews whereas the super-vised classifiers attempt to classify all  reviews.The macroaveraged F1 measure is 80.55, whichbeats the na?ve baseline by over 30 percentagepoints, and approaches the two upper bounds.1077Corpus Iter P R F1Monitors 12 86.62 86.24 86.43Mobile phones 11 90.15 89.68 89.91Digital cameras 13 81.33 80.23 80.78MP3 players 13 86.10 85.10 85.60Computer parts 10 69.10 67.53 68.31Video cameras andlenses10 82.81 81.44 82.12Networking 11 69.28 68.29 68.78Office equipment 12 81.83 80.36 81.09Printers 12 81.04 79.61 80.32Computer peripherals 10 82.20 81.84 82.02Macroaverage 81.05 80.03 80.54Table 3.
Results with the single, manuallychosen seed ?
(good) for each corpus.5.4 Experiment 2We then ran our full system, including the seedidentifier.
Appendix A shows that for most of thecorpora the algorithm found different (highly do-main-salient)  seeds.
Table  4  shows  the  resultsachieved.Corpus Iter P R F1Monitors 11 85.57 85.07 85.32Mobile phones 10 92.63 92.19 92.41Digital cameras 13 84.92 83.58 84.24MP3 players 13 88.69 87.55 88.11Computer parts 12 77.78 77.27 77.52Video cameras andlenses11 83.62 81.99 82.8Networking 13 72.83 72.00 72.41Office equipment 10 82.42 81.34 81.88Printers 12 81.04 79.61 80.32Computer peripherals 10 82.24 82.06 82.15Macroaverage 83.17 82.27 82.72Table 4.
Results with the seeds automaticallyidentified for each corpus.Across all 10 subcorpora, the improvement us-ing  automatically  identified  seed  words  com-pared with just using the seed good is significant(paired t-test, P<0.0001), and the F1 measure liesbetween the two upper bounds.6 Conclusions and Future WorkThe unsupervised approach to seed words selec-tion for sentiment classification presented in thispaper produces results which in most  cases areclose to the results of supervised classifiers andto  the  previous  almost-unsupervised  approach:eight  out  of  ten  results  showed  improvementover the human selected seed word and three re-sults  outperformed  the  supervised  approach,while three other results were less than 1% infe-rior to the supervised ones.How does  it  happen that  the  chosen seed isusually (in our  dataset  ?
always)  positive?
Wethink that this happens due to the socially accept-ed norm of behaviour: as a rule one needs to befriendly to communicate with others.
This in turndefines  linguistic  means  of  expressing  ideas  ?they will be at least slightly positive overall.
Thehigher  prevalence of positive  reviews has  beenobserved previously: for example, in our corpusbefore  we  balanced  it  almost  80% of  reviewswere  positive;  Pang  et  al.
(2002)  constructedtheir move  review  corpus  from  an  originaldataset  of  1301  positive  and  752  negative  re-views (63% positive).
Ghose et al (2007) quotetypical  examples  of  highly  positive  languageused in the online marketplace.
We can make apreliminary conclusion that a relatively high fre-quency of positive  words  is  determined  by theusage of language that reflects the social  beha-viour of people.In future work we intend to explore these is-sues of positivity of language use.
We will alsoapply  our  approach  to  other  genres  containingsome quantity of evaluative language (for exam-ple newspaper articles), and see if it works equal-ly well  for  languages  other  than Chinese.
It  isalso likely we can use a smaller set of negationwords and adverbials to produce the seed lists.AcknowledgementsThe first author is supported by the Ford Founda-tion International Fellowships Program.ReferencesAue, Anthony, and Michael Gamon.
2005.
Customiz-ing Sentiment Classifiers to New Domains: a CaseStudy.
In Proceedings of the International Confer-ence  RANLP-2005  Recent  Advances  in  NaturalLanguage Processing.1078Blitzer,  John,  Mark  Dredze,  and  Fernando  Pereira.2007.
Biographies,  Bollywood,  Boom-boxes  andBlenders: Domain Adaptation for Sentiment Clas-sification.
In Proceedings of the 45th Annual Meet-ing of  the Association of  Computational Linguis-tics.
440?447.Dave,  Kushal,  Steve Lawrence,  and David M. Pen-nock.
2003.
Mining  the  Peanut  Gallery:  OpinionExtraction and Semantic Classification of ProductReviews.
In  Proceedings  of  the  Twelfth  Interna-tional World Wide Web Conference.
519?528.Engstr?m, Charlotte.
2004.
Topic Dependence in Sen-timent Classification.
Unpublished MPhil Disserta-tion.
University of Cambridge.Foo, Schubert, and Hui Li.
2004.
Chinese Word Seg-mentation and Its Effects on Information Retrieval.Information  Processing  and  Management,  40(1).161?190.Ghose, Anindya, Panagiotis Ipeirotis, and Arun Sun-dararajan.
2007.
Opinion Mining using Economet-rics: A Case Study on Reputation Systems.
In Pro-ceedings of the 45th Annual Meeting of the Associ-ation of Computational Linguistics.
416?423.Hu, Minqing, and Bing Liu.
2004.
Mining and Sum-marizing Customer Reviews.
In Proceedings of the10th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining.
168?177.Ku,  Lun-Wei,  Yu-Ting  Liang,  and  Hsin-Hsi  Chen.2006.
Opinion  Extraction,  Summarization  andTracking in News and Blog Corpora.
In  Proceed-ings of the AAAI-2006 Spring Symposium on Com-putational  Approaches  to  Analyzing  Weblogs.AAAI Technical Report.McDonald, Ryan, Kerry Hannan, Tyler Neylon, MikeWells,  and Jeff Reynar.
2007.
Structured  Modelsfor  Fine-to-Coarse  Sentiment  Analysis.
In  Pro-ceedings of the 45th Annual Meeting of the Associ-ation of Computational Linguistics.
432?439.Pang, Bo, and Lillian Lee.
2004.
A Sentimental Edu-cation:  Sentiment  Analysis  Using  SubjectivitySummarization Based on Minimum Cuts.
In  Pro-ceedings of the 42nd Annual Meeting of the Associ-ation for Computational Linguistics.
271?278.Pang,  Bo,  Lillian  Lee,  and  ShivakumarVaithyanathan.
2002.
Thumbs up?
Sentiment Clas-sification using Machine Learning Techniques.
InProceedings of the 2002 Conference on EmpiricalMethods in Natural Language Processing.
79?86.Read,  Jonathon.
2005.
Using  Emoticons  to  ReduceDependency in Machine Learning Techniques forSentiment  Classification.
In  Proceedings  of  theACL Student Research Workshop at ACL-05.
43?48.Tan, Aoshuang.
2002.
Problemy skrytoj grammatiki.Sintaksis,  semantika  i  pragmatika  jazyka  izoliru-ju?
?ego stroja na primere kitajskogo jazyka  [Prob-lems of a hidden grammar.
Syntax, semantics andpragmatics of a language of the isolating type, tak-ing the Chinese language  as an example].
JazykiSlavjanskoj Kultury.Turney,  Peter  D.  2002.
Thumbs  Up  or  ThumbsDown?
Semantic Orientation Applied to Unsuper-vised Classification of Reviews.
In Proceedings ofthe  40th  Annual  Meeting  of  the  Association  forComputational Linguistics.
417?424.Xu, Jia, Richard Zens, and Hermann Ney.
2004.
DoWe Need Chinese Word Segmentation for Statisti-cal  Machine  Translation?
In  Proceedings  of  theThird  SIGHAN  Workshop  on  Chinese  LanguageLearning.
122?128.Yarowsky,  David.
1995.
Unsupervised  Word  SenseDisambiguation  Rivaling Supervised  Methods.
InProceedings of the 33rd Annual Meeting of the As-sociation for Computational Linguistics.
189?196.Yu, Hong, and Vasileios Hatzivassiloglou.
2003.
To-wards  Answering  Opinion  Questions:  SeparatingFacts from Opinions and Identifying the Polarity ofOpinion  Sentences.
In  Proceedings  of  the  2003Conference on Empirical Methods in Natural Lan-guage Processing.
129?136.Zagibalov,  Taras,  and John Carroll.
2008.
Unsuper-vised  Classification of  Sentiment  and  Objectivityin Chinese Text.
In Proceedings of the Third Inter-national  Joint  Conference  on  Natural  LanguageProcessing.
304?311.1079Appendix A.
Seeds Automatically Identified for each Corpus.Corpus Seed Corpus SeedMonitors ?
(good)?
(convenient; cheap)??
(clear)?
(straight)??
(comfortable)?
(fill, fulfill)??
(sharp)??
(comfortable)?
(cool)Videocamerasand lenses??
(clear ?
of  sound  or  image)??
(comfortable)??
(practical)??
(perfect)?
(cool)Mobilephones?
(good)??
(support)?
(convenient; cheap)??
(comfortable)??
(clear ?of sound or image)?
(sufficient)??
(easy to use)??
(comfortable)???
(user friendly)??
(smooth and easy)??
(distinct)?
(cool)??
(has become better)??
(durable)???
(comfortable)???
(satisfied)??
(fit, suit)???
(has become comfortable)??
(applicable)??
(handy)??
(science, scientific)Digitalcameras?
(good)?
(convenient; cheap)??
(comfortable)??
(clear?of sound or image)??
(special)?
(cool)??
(satisfied)??
(durable)??
(comfortable)??
(perfect)??
(straight)??
(stable)???
(has become comfortable)??
(polite)??
(detailed)Networking ??
(stable) Printers ?
(good)MP3 players ?
(good)?
(convenient; cheap)??
(comfortable)??
(practical)??
(sensitive)??
(comfortable)?
(cool)???
(has become comfortable)Computerperipherals?
(good)?
(convenient;cheap)??
(comfortable)?
(precise)??
(comfortable)??
(habitual)??
(smooth and easy)??
(stable)Computerparts?
(good)??
(stable)Officeequipment?
(good)??
(comfortable)??
(stable)??
(practical)1080
