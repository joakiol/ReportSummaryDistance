A COMMON FACTS DATA BASEWilliam CrowtherBBN Systems and Technologies Corporation10 Moulton StreetCambridge, MA 02138INTRODUCTIONThis note is about a project whose goal has been to develop methods for converting large amounts of naturallanguage text into machine-accessible data base relations.
The easiest way to explain the project is to describe itshistorical development, pointing out along the way key ideas relating to the project's goal, its version of"meaning," its data representation, a d how it treats errors.PHASE 1 - THE GOALThe origins of the project go back about en years.
At that time, I wanted to make a game that required a lot ofinformation about animals - which ones were dangerous, which ones were good for eating, how big they were,etc.
So I manually constructed a relational data base listing all the common animals and all their importantproperties.
There were several thousand animals, and each had about 100 properties, but because of inheritance,the total number or relations was only about 20,000.
I used a relational data base because it seemed, for example,that a structure such as (aardvark weight-in-pounds 140) captured the information i  a natural way.
Structuringthe information as pairs - e.g., (rattlesnake - poisonous) was sometimes more natural, but it was easy to encodepairs as triples (rattlesnake is poisonous), and harder to encode triples as pairs.This data base was the precursor of the current project, whose goal is simply to create amuch larger version of thesame thing, with many more nouns and properties, and tens or hundreds of millions of relations.
We call theresulting data base a "common facts" data base to emphasize the notion that we are storing a large number of factsthat are part of common knowledge.
With such a goal, manual entry is prohibitive and some form of automaticacquisition is essential.
Despite the change in scale, users will still ask prototypical questions uch as "is xpoisonous?"
and "what is the weight of x?
"PHASE 2 - DICTIONARIES, AND THE MEANING OF "MEANING"A few years ago, at a fairly low and intermittent level of effort, I started working with an on-line dictionary,initially just trying to find the head noun of each noun definition.
This limitation had the virtue that it madeconstructing a parser plausible, and if it worked well I would have a complete inheritance hierarchy.Finding the head noun was too simplistic.
There were constructs like "body of water," genus of rose" and"structural member" to deal with, as well as the general problem that words have more than one meaning.
But afew general rules took care of such problems fairly well, for example "genus of x is a plant ff x is a plant.
"These rules were stored in the data base as simple templates; e.g., see x, produce y.
Surprisingly, a few hundredrules that I developed appeared to cover a large number of these constructs.
I also needed to manually override afew hundred efinitions - e.g., animal doesn't inherit much from kingdom, even though it was defined in terms of"animal kingdom."
The result was a hierarchy incorporating about 50,000 nouns.
There were errors, of course:some were due to the parser, some to the rules, and some could arguably be traced to ambiguity or omissions inthe dictionary.
Based on casual examination ofsamples of the dictionary, itwas clear that a large percentage(maybe close to 90%) of the dictionary nouns were being correctly placed in the hierarchy.The parser also found some modifiers.
All the adjectives which preceded the head noun were available, andgenerally one modifier following the head noun (e.g., a prepositional phrase like "of Africa").
Beyond that, theparser would have to deal with prepositional ttachment, which it was unable to do at the time.
I didn't worry too89much about how to organize the modifiers.
For example, the program knew the relation (rattlesnake ispoisonous), but really had no idea what it meant.
However, as I shall argue below, the goal was to acquire andretrieve the information for retrieval applications, and meaning might not play a major role.In retrospect, the idea that the program did not need to know the meaning of something in order to store andretrieve it has become acornerstone of the system.
As the system grew, it learned more and more about herelation "is poisonous."
There were dozens of examples of poisonous things, a definition of poisonouscontaining a link to poison, which also has a definition, etc.
There was a description of what poisons do, andwhat antidotes do, and how they are linked to poisons.
Eventually, one is tempted to say that the systemsomehow "knows" the meaning of poisonous, but it really doesn't.
From a pragmatic point of view, the systemknows about definitions, modifiers, inheritance, and the like - all else is just interrelationships n the data.
Fromthis point on, I decided that dealing with meaning may not be necessary for the goals of this project.
The systemha s no underlying model of the world, nor does it really try to acquire one from the data it reads.
All it does isstore and retrieve data, sometimes following relational links to do so.The system was set up to accept adictionary as input and produce as set of relations as output.
Processing a 600-page dictionary through the system at this time produced 0.5 million relations.
Very roughly, I estimated that thisconstituted about one-third the number of relations in the dictionary.
Bearing in mind the difficulty of deciding onthe correctness of relations, it was my judgement that as many as 80% of the 0.5 million relations were correct.
(Note that storing this number of relations requires about 5-10 MB of memory.
At this rate, one could imagineholding relations from several books in the main memory of a modem machine.
)Initially, we worried about he bootstrapping aspect of the project.
The parser needed semantics from the database to parse most things, and the data base couldn't be filled without a parser.
Now we know that bootstrappingworks well.
What we can parse on syntax and hand entered semantics i enough to get started, and if necessarythe same material can be read several times, each pass gleaning more information than the last.PHASE 3 - TRANSFORMATION AND REPRESENTAT IONAt this point, a modest BBN internal research and development effort was started to support the project; however,it was still very much a part-time ffort.
There were now quite a few interesting new things to try - none of themparticularly easy to do well, but many of the common cases could be handled with modest effort.The easiest thing to try was to process definitions other than nouns.
Here it became apparent that a dictionary wasreally not a repository of facts about he world, but rather of facts about words.
The definition of an adjective rarelysaid anything more than "this word is equivalent to this phrase," leaving the meaning still opaque.
Still, we wereable to capture these equivalences without much change to the parser.
Verb definitions were harder, because theyoften resembled clauses.
But the simple cases were tractable, and again related aword to an expression.
Inretrospect, the noun definitions which had seemed to contain real information really amounted to an assertionthat, among all the possible things one could describe, some had names.
Again, we see the concept of "meaning"becoming weakenedto equivalence between expressions.Still, the processing of these quivalences was the start of something which has become more and more importantas the project has gone on.
Going hack to the idea of a game for a moment, suppose the client wants to askwhether aparticular animal is "poisonous."
This is easy when the source text is cooperative and uses the sameword the client chose.
But suppose the text used the word "venomous."
The information to discover the relationbetween poisonous and venomous i readily available - in this case venomous has poisonous as a directsynonym, and one need not even bother to process anything beyond synonyms.
So, we implemented a synonymlink, and also incorporated actual definitions from the dictionary, which expressed equivalence between a word anda phrase instead of between aword and a word.
In this manner, we had started own a path which would eventuallybecome another cornerstone of the project: while we might not know what something "meant," we did know howto transform it into another expression which meant roughly the same thing.Meanwhile, it was important to improve the parser again to parse more than the head noun and the first phrase.Part of this involved adding new and more complex syntax - even some simple clauses.
It was tempting to try touse one of the existing well-developed parsers at BBN.
But for a number of reasons, I was unable to use any ofthese parsers in my system; for example, none of them was geared to process tens of thousands of sentences perhour.90A main thrust in parser improvement was the idea of using the data base to resolve parse ambiguities.
The waythe parser worked was to process a sentence until it discovered one of a set of ambiguities, which it tried to resolveusing information already in the data base.
The parser ecognized about eight different classes of ambiguities, ofwhich a typical example was the ambiguous conjunction: (xor y z); the parser must decide whether theexpression means ((x or y) z), or (x or (yz)).
For example, "iron or stone fence" would be grouped one way, while"gate or stone fence" the other.
For this type of ambiguity, the data base itself often has the information eeded toresolve the ambiguity.
In this example, for instance, iron and stone are both materials while gate and fence areman-made structures.
The program is able to resolve these types of ambiguities from the similarities discoveredby questioning the data base.Prepositional ttachment is another type of ambiguity that could sometimes be resolved because the program couldreadily distinguish categories like places, times, objects, and actions.
In this way, the parser was not only a frontend processor, it also used the data base explicitly in doing its work in a bootstrapping fashion.With these improvements in place, we estimate that we will be able to acquire about wo-thirds of the relationsfrom the dictionary, though we have not as yet processed the whole dictionary.Continuing the attempt to acquire more relations, we processed through the system encyclopedia articles instead ofdictionary entries.
In the encyclopedia, the problems are much harder.
Some of the difficulty comes from simplethings, like the fact that our parser is incomplete about hree-way conjunctions.
Part of the difficulty comes frommuch harder things, like figuring out what pronouns are referring to.
(We do match pronouns to a crude contextlist, which works well for the most common cases.)
The principal difficulty with encyclopedias was that theinformation we were gathering was much more complex than for dictionaries.
The representation f suchinformation became areal issue, which we discuss next.RepresentationOriginally our representation was straightforward: animals had properties like COLOR, WEIGHT, HABITAT,PREY, PREDATORS, FOOD-VALUE, and ADJECTIVAL-MODIFIER ; and our goal was simply to fill inthe values of those properties with words like BLACK, 140, GRASSLAND ....
CARNIVOROUS.
There wasa need to extend the representation to cover more complex constructions, asdetailed below.The representation was first extended to include multi-word constructs and relations, such as "automatic weaponsfactory" and "body temperature in winter."
The program uses a mapping between data base relations and sentencestructure operators.
The method used to implement the mapping, while developed independently, seems toresemble the work of Katz 1 at MIT.The next extension i cluded representations forsynonyms and equivalent expressions.
With this extension, thesystem was able to relate POISONOUS to VENOMOUS, and even "COLOR RED " to "LOOKED RED," forexample.
However, we didn't want to make these transformations oninput, because generally the system had notacquired enough information to make all of these transformations bythen.
For example, upon encountering"rattlesnake", the system had not seen "venomous."
Since it didn't matter when the transformation was made, andthe system was already making simple inheritance transformations onretrieval, we chose to make alltransformations at retrieval time.
This choice turned out to be fortuitous, even from a performance point of view,since there were many more input sentences than queries.It then became apparent that the concept of "synonym" was an extreme form of the concept "overlappinginformation."
Is a DANGEROUS snake POISONOUS ?
Probably so, but perhaps not!
In this case, wecouldn't make the transformation input; DANGEROUS is really different from POISONOUS, and we didn'twant to corrupt he data base with possibly wrong inferences, particularly when we might suddenly acquire thecorrect information.
However, in the absence of any other information, we wanted to say yes to the question or atleast to say that the statement was likely.
In this fashion, we have extended our representation to includeinferencing.
This extension resulted in a representation that was inherently ambiguous, ince the sameinformation was likely to be in the data base in different forms.
As an example, there were literally thousands of1 Katz, B.
1988.
Using English for Indexing and Retrieving.
In Proceedings of the Conference on User-orientedContent-based Text and Image Handling, RIAO "88, Vol.
1.
314 - 332.
MIT, Cambridge, MA.91ways to store the information "RATTLESNAKE IS POISONOUS ," all of which would yield "YES" to thequestion "is a rattlesnake poisonous?"
In retrospect, we saw that we had been storing information i  more thanone way from the beginning.
Simple inheritance - "birds can fly" - gives us an alternate way to store the fact that"ravens can fly."
Only now we had a situation in which things were not black and white.
Instead of answeringyes or no, we were answering "probably."
Of course, things were never eally black and white - not all birds canfly, just typical birds, yet our source of data was very prone to saying things like "birds fly," without qualifiers.Conf idence  in Retr ieva lNot all the different ways to store information have the same confidence l vel, which leads us to the next topic,confidence of retrieved information.
With an ambiguous representation, wewere forced to look for answers in avariety of places.
Usually the answer was found in several places, and often the answers did not agree.
In a longsearch, one would frequently encounter a broken entry, roughly equivalent to "true is false," and then prove justabout anything, so dealing with errors and likelihood of error bec~m very important.
Our view is that theprogram establishes a path from query to answer by finding a number of inference links, and that each link carriesits own measure of disbelief.
The goodness of a path is measured by the sum of the disbelief acquired over thelinks.
The disbelief may be as simple as the fact that inheritance sometimes fails because a relation expresses"typical" truth, while the child may not be typical.
Or the disbelief may be of a completely different kind: "theencyclopedia s ys x" can be quite different from "Himalayan tribesmen say x."
In our world, every statementhas a source, which has an associated level of disbelief.
Recognizing this, we can now choose the most believableanswer as our reply.
Or we can report conflicting answers if the belief is roughly equal.Inexact inferences can lead to errors in information re~eval from the data base.
We were originally tempted toascribe the errors to some flaw in our system - perhaps if we relied on better sources, or understood them better,we could operate without errors.
Many existing systems do assume an underlying perfect world model in whicheverything can be precisely expressed in terms of a set of fundamental properties, from which all other propertiescan be derived.
Instead, our underlying model is inherently ambiguous and incomplete.
We believe that both thenumber of underlying facts and the number of possible properties are infinite, and that we will never know morethan a small fraction of them.
In addition, we know that relations and concepts are themselves related, usually inimprecise and ambiguous ways.
Therefore, we will always be trying to infer some relation we do not know fromsome other elation we do know, and that inference itself will always be suspect.
We consider this ambiguity anduncertainty in our model to be one of the strengths of our system, because it seems to mirror the ambiguity inthe data we are acquiring.One might think we have taken an enormous performance p nalty by doing inferencing on all our queries, sincegeneral inferencing is notoriously slow.
However, the system does not do general inferencing, and there are anumber of tricks which make special types of inferences rather speedy.
For example, we have the whole of the database available, and we never add an inference unless the data base has entries which might match it.
The space ofreal ~la!~ is much smaller than the space of possible inferences.
Also, any inference which amounts to a simplesubstitution of one word or phrase for another is handled by a special and efficient algorithm.
Furthermore,because ach inference introduces disbelief, we simply refuse to follow long chains of inference; they are notbelievable.
And, of course, we rely on the fact that the data base is small enough to fit in real memory, whereaccess is relatively cheap.PHASE 4 - THE FUTUREBecause of recent DARPA interest in this project, our focus has shifted more toward possible applications of thework.
It becomes important at this point, therefore, to determine what can and cannot be done with this approach.Crucial to such determination is the development of methods to evaluate the performance of the system in arigorous manner, which will require the addition of certain capabilities to the system before such evaluation canmeaningfully take place.
Our primary focus now is on measuring what fraction of the available information theapproach can recover, trying to determine both what we can do now and what we could do with a little work (or alot of work).
We also may be able to distinguish between retrievable and non-retrievable information.We are also laying to determine possible applications for such a system.
While it is designed to retrieve specificinformation, it is just as well suited to retrieving references, and could be used to find articles about a particulartopic, or even find articles about rather specific interrelationships of topics, since that is the kind of information itmanipulates.
It might also serve as a memory component of some other system.
Surely many parsers would92benefit from having this sort of information available, and systems which model parts of the real world couldperhaps find some useful information here.We are working toward a massive processing of an encyclopedia - s much as we can fit into memory of thebiggest machine we can access.
Sheer size has alwhys been important to this project, and we need to know howthings will scale in the presence of larger amounts of data.
There is always the worry that irrelevant clutter andwrong information will choke off growth, but we have the hope that redundant information will allow eliminationof errors and promote growth.
As the information becomes more complex, issues of representation, meaning, andbelief become more and more urgent.93
