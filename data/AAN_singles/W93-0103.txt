Lexical Concept Acquisit ion From Col location Map 1Young S. Han, Young Kyoon Han, and Key-Sun ChoiComputer Science DepartmentKorea Advanced Institute of Science and TechnologyTaejon, 305-701, Koreayshan@csking.kaist.ac.kr, kschoi@csking.kaist.ac.krAbstractThis paper introduces an algorithm for automatically acquiring the conceptual struc-ture of each word from corpus.
The concept of a word is defined within the proba-bilistic framework.
A variation of Belief Net named as Collocation Map is used tocompute the probabilities.
The Belief Net captures the conditional independencesof words, which is obtained from the cooccurrence relations.
The computation ingeneral Belief Nets is known to be NP-hard, so we adopted Gibbs sampling for theapproximation of the probabilities.The use of Belief Net to model the lexical meaning is unique in that the network islarger than expected in most other applications, and this changes the attitude towardthe use of Belief Net.
The lexical concept obtained from the Collocation Map bestreflects the subdomain of language usage.
The potential application of conditionalprobabilities the Collocation Map provides may extend to cover very diverse areas oflanguage processing such as sense disambiguation, thesaurus construction, automaticindexing, and document classification.1 Introduct ionThe level of the conceptual representation of words can be very complex in certain con-texts, but in this paper we assume rather simple structure in which a concept is a setof weighted associated words.
We propose an automatic oncept acquisition frameworkbased on the conditional probabil it ies suppliedd by a network representation of lexical re-lations.
The network is in the spirit of Belief Net, but the probabil it ies are not necessarilyBayesian.
In fact this variation of Bayesian Net is discussed recently by (Neal, 1992).
Weemployed the Belief Net with non Bayesian probabil it ies as a base for representing thestatist ical relations among concepts, and implemented the details of the computat ion.Belief or Bayesian Nets have been extensively studied "in the normative xpert systems(Heckerman, 1991).
Experts provided the network with the Bayesian(subjective) proba-bilities solely based on his/her technical experiences.
Thus the net has been also knownas a Belief Net among a dozen other names that share all or some of the principles ofBayesian net.
The probabil ist ic model has been also used in the problems of integratingvarious sources of evidences within sound framework (Cho, 1992).
One of the powerfulfeatures of Belief Net is that  the conditional independences of the variables in the modelare natural ly captured, on which we can derive a form of probabilistic inference.
If weregard the occurrence of a word as a model variable and assume the variables occur withinsome conditional influences of the variables(words) that previously took place, the Beliefapproach appears to be appropriate to compute some aspects of lexical relations latent in1 This work was supported in part by a grant from Korea National Science Foundation as a basicresearch project and by a grant from Korea Ministry of Science and Technology in project "an intelligentmultimedia nformation system platform and image signal transmission i  high speed network"22the texts.
The probabilities on dependent variables are computed from the frequencies,so the probability is now of objective nature rather than Bayesian.The variation of Belief Net we use is identical to the sigmoid Belief Net by Neal (1992).In ordinary Belief Nets, 2 ~ probabilities for a parent variable with n children should bespecified.
This certainly is a burden in our context in which the net may contain evenhundred thousands of variables with heavy interconnections.
Sigmoid interpretation ofthe connections as in artificial neural networks provides a solution to the problem withoutdamaging the power of the network.
Computing a joint probability is also exponentialin an arbitrary Belief network, thus Gibbs sampling which originates from Metropolisalgorithm introduced in 50's can be used to approximate the probabilities.
To speedup the convergence of the sampling we adopted simulated annealing algorithm with thesampling.
The simulated annealing is also a descendant of metropolis algorithm, and hasbeen frequently used to compute an optimal state vector of a system of variables.From the Collocation Map we can compute an arbitrary conditional probabilities ofvariables.
This is a very powerful utility applicable to every level of language processing.To name a few automatic indexing, document classification, thesaurus construction, andambiguity resolution are promising areas.
But one big problem with the model is that itcannot be used in real time applications because the Gibbs sampling still requires an ampleamount of computation.
Some applications such as automatic indexing and lexical conceptacquisition are fortunately not real time bounded tasks.
We are currently undertakinga large scale testing of the model involving one hundred thousand words, which includesthe study on the cost of sampling versus the accuracy of probability.To reduce the computational cost in time, the multiprocessor model that is success-fully implemented for Hopfield Network(Yoon, 1992) can be considered in the context ofsampling.
Other options to make the sampling efficient should be actively pursued, andtheir success is the key to the implementation f the model to the real time problems.2 Definit ion of Lexical ConceptWhenever we think of a word, we are immediately reminded of some form of meaningof the word.
The reminded structure can be very diverse in size and the type of theinformation that the structure delivers.
Though it is not very clear at this point what thestructure is and how it is derived, we are sure that at least some type of the remindedstructure is readily converted to the verbal representation.
Then the content of vebralform must be a clue to the reminded structure.
The reminded structure is commonlyreferred to as the meaning of a word.
Still the verbal representation can be arbitrarilycomplex, yet the representation is made up of words.
Thus the words in the clue to themeaning of a word seem to be an important element of the meaning.Now define the concept of a word asDefinit ion 1 The lexical concept of a word is a set of associated words that are weightedby their associativeness.The notion of association is rather broadly defined.
A word is associated with anotherword when the one word is likely to occur in the clue of the reminded structure of the otherword in some relations.
The association by its definition can be seen as a probabilisticfunction of two words.
Some words are certainly more likely to occur in association witha particular word.
The likeliness may be deterministically explained by some formal23theories, but we believe it is more of inductive(experimental) process.
Now define theconcept a of word w as a probabilistic distribution of its associated words.= { (w,, pd},  (l)wherepi = P(Wl I w) ,andp i>T.Thus the set of associated words consists of those whose probability is above thresholdvalue T. The probabilistic distribution of words may exist independently of the influence ofrelations among words though it is true that relations in fact can affect the distribution.But in this paper we do not take other information into the model.
If we do so, themodel will have the complexity and sophistication of knowledge representation.
Such anapproach is exemplified by the work of Goldman and Charniak (1992).Equation 1 can be further elaborated in several ways.
It seems that the concept ofa word as in Equation 1 may not be sufficient.
That is, Equation 1 is about the directassociation of a given word.
Indirect association can also contribute to the meaning of aword.
Now define the expanded concept of a word asa'(w) = { (wi, Pi)} U { (vi, qi)}, (2)Orswhereqi = P( vi I o'(w)) ,andqi >T .= { (w,, pl)} u (3)If the indirect association is repeated for several depths a class of words in particularaspects can be obtained.
A potential application of Equation 3 and 4 is the automaticthesaurus construction.
Subsumption relation between words may be computed by care-fully expanding the meaning of the words.
The subsumption relation, however, may notbe based on the meaning of the words, but it rather be defined in statistical nature.The definition of lexical meaning as we defined is simple, and yet powerful in manyways.
For instance, the distance between words can be easily computed from the rep-resentation.
The probabilistic elements of the representation make the acquisition anexperimental process and base the meaning of words on more consistent foundation.
Thecomputation of Equation 1, however, is not simple.
In the next section we define Collo-cation Map and explain the algorithm to compute the conditional probabilities from theMap.24Figure 1: DG to DAGFigure 2: Word Dependency in Collocation Map3 Col location MapCollocation map is a kind of Belief Net or knowledge map that represents the dependenciesamong words(concepts).
As it does not have decision variables and utility, it is differentfrom influence diagram.
One problem with knowledge map is that it does not allow cycleswhile words can be mutually dependent.
Being DAG is a big advantage of the formalismin computing probabilistic decisions, so we cannot help but stick to it.
A cyclic relationshould be broken into linear form as shown in figure 1.
Considering the size of collocationmap and the connectivity of nodes in our context is huge it is not practical to maintainall the combination of conditional probabilities for each node.
For instance if a node hasn conditioning nodes there will be 2 n units of probability information to be stored in thenode.
We limit the scope to the direct dependencies denoted by arcs.What follows is about the dependency between two words.
In figure 2,P(bla)  = pl, (4)P(c la)  = p2.
(5)Pl denotes the probability that word b occurs provided word a occurred.
Once a textis transformed into an ordered set of words, the list should be decomposed into binaryrelations of words to be expressed in collocation map.
Here in fact we are making animplicit assumption that if a word physically occurs frequently around another word, thefirst word is likely to occur in the reminded structure of the second word.
In other words,physical occurrence order may be a cause to the formation of associativeness among words.Di = (a ,b ,c ,d ,e , f , .
.
.
, z ) .When Di is represented by a, b, c, ?
- -, z), the set of binary relations with window size3(let us call this set/~3) format is as follows.25sD i = (ab, ac, bc, ad, bd, cd, be,ce,de,c f , .
.
.
, ) .For words di and ct, P(c tldi) can be computed at least in two ways.
As mentionedearlier, we take the probability in the sense of frequency rather than belief.
In the firstmethod,P(c~ldi) ~ f (ctd i )  (6)y(d~) 'where i < j.Each node di in the map maintains two variables f(di) and f(diej),  while each arckeeps the information of P(cjldi).
From the probabilities in arcs the joint distributionover all variables in the map can be computed, then any conditional probability can becomputed.
Let S denote the state vector of the map.P(g  = ~) = H P(Si  = silS t = sj : j < i) .
(7)iComputing exact conditional probability or marginal probability requires often exponen-tial resources as the problem is know to be NP-hard.
Gibb's sampling must be one of thebest solutions for computing conditional or marginal probabilities in a network such ascollocation map.
It approximates the probabilities, and when optimal solutions are askedsimulated annealing can be incorporated.
Not only for computing probabilities, patterncompletion and pattern classification can be done through the map using Gibb's sampling.In Gibb's sampling, the system begins at an arbitrary state or a given S, and a freevariable is selected arbitrarily or by a selecting function, then the value of the variablewill be alternated.
Once the selection is done, we may want to compute P(S = g) or otherfimction of S. As the step is repeated, the set of S's form a sample.
In choosing the nextvariable, the following probability can be considered.p(s~ = xlSt = s~ : j ?
i)P (St  = xlSt = st : j < i ) .I~  p(s t  = st ISi = ~, & = ,k : k < j, k ?
i).
(8)j>iThe probability is acquired from samples by recording frequencies, and can be up-dated as the frequencies change.
The second method is inspired by the model of (Neal1992) which shares much similarity with Boltzmann Machine.
The difference is that thecollocation map has directed ares.
The probability that a node takes a particular value ismeasured by the energy difference caused by the value of the node.P(Si  = silSj = sj : j < i) = o'(si E sjwij) .j<i(9)26Hidden UnitsFigure 3: Collocation Map with Hidden Units1 where a(t) - l+e- tA node takes -1 or 1 as its value.P (S=g)  = I I P (S i=s i lS?
=s?
: j< i )i= Ha(s '  Es jw ' i ) "  (10)i i<iConditional and marginal probabilities can be approximated from Gibb's sampling.
Aselection of next node to change has the following probability distribution.P(S~ = xIS j = sj : j # i)+ (11)j<i j>i  k<j,k~iThe acquisition of probability for each arc in the second method is more complicatedthan the first one.
In principle, the general patterns of variables cannot be capturedwithout the assistance of hidden nodes.
Since in our case the pattern classification is notan absolute requirement, we may omit the hidden nodes after careful testing.
If we employhidden units, the collocation map may look as in figure 5 for instance.Learning is done by changing the weights in ares.
As in (Neal, 1992), we adopt gradientascent algorithm that maximize log-likelihood of patterns.L = logH P(V =v) = E l ?gP(V=O) '  (12)V'ET 17' E TE ZXwij = ~sls jo ' ( -s l  ~ SkWik), (13)k<iwhere N = \]T\[ .Batch learning over all the patterns is, however, unrealistic in our case consideringthe size of collocation map and the gradual nature of updating.
It is hard to vision27that whole learning is readjusted every time a new document is to be learned.
Graduallearning(non batch) may degrade the performance of pattern classification probably by asignificant degree, but what we want to do with collocation map is not a clear cut patternidentification up to each learning instance, but is a much more brute categorization.
Oneway to implement the learning is first to clamp the nodes corresponding to the input setof binary dependencies, then run Gibb's sampling for a while.
Then, add the average ofenergy changes of each arc to the existing values.So far we have discussed about computing the conditional probability from CollocationMap.
But the use of the algorithm is not limited to the acquisition of lexical concept.The areas of the application of the Collocation Map seems to reach virtually every cornerof natural language processing and other text processing such as automatic indexing.An indexing problem is to order the words appearing in a document by their relativeimportance with respect to the document.
Then the weight ?
(wi) of each word is theprobability of the word conditioned by the rest of the words.ek(wi) = P( wi l wj, j 5k i) .
(14)The application of the Collocation Map in the automatic indexing is covered in detailin Han (1993).In the following we illustrate the function of Collocation Map by way of an example.The Collocation Map is built from the first 12500 nouns in the textbook collection in PennTree Bank.
Weights are estimated using the mutual information measure.
The topics ofthe textbook used includes the subjects on planting where measuring by weight and lengthis frequently mentioned.
Consider the two probabilities as a result of the sampling on theCollocation Map.P(depthlinch ) = 0.51325,andP(weightlinch ) = 0.19969.When the sampling was loosened, the values were 0.3075 and 0 respectively.
The firstversion took about two minutes, and the second one about a minute in Sun 4 workstation.The quality of sampling can be controlled by adjusting the constant factor, the coolingspeed of temperature in simulated annealing, and the sampling density.
The simple ex-periment agrees with our intuition, and this demonstrates the potentail of CollocationMap.
It, however, should be noted that the coded information in the Map is at best local.When the Map is applied to other areas, the values will not be very meaningful.
This maysound like a limitation of Collocation Map like approach, but can be an advantage.
Nosystem in practice will be completely general, nor is it desirable in many cases.
Figure 4shows a dumped content of node tree in the Collocation Map, which is one of 4888 nodesin the Map.4 Conc lus ionWe have introduced a representation of lexical knowledge ncoding from which an arbi-trary conditional probability can be computed, thereby rendering an automatic acquisition28< h23 >f:b:tree di(36494) ctr(92)inch mi(19) rooting mi(20) resistance mi(21)period mi(22) straw mi(31) evaporation mi(32)mulch mi(29) pulling mi(34) flower mi(5)plant mi(1) root mi(13) moisture mi(28)shrub mi(24) c(4) 0.043478water mi(26) c(3) 0.032609under-watering mi(36) c(1) 0.010870fertilizer mi(59) c(3) 0.032609tree mi(58) c(5) 0.054348March mi(54) c(2) 0.021739pecan mi(102) c(2) 0.021739temperature mi(106) c(1) 0.010870plant mi(9) c(1)  0.010870fruit mi(107) c(1) 0.010870mulch mi(33) c(1) 0.010870blueberry mi(123) c(1) 0.010870shade mi(130) c(4) 0.043478planting mi(155) c(1) 0.010870bank mi(350) c(1) 0.010870branch mi(172) c(1) 0.010870landscape mi(368) c(2) 0.021739cooling mi(586) c(1) 0.010870ground mi(126) c(2) 0.021739inch mi(133) c(1) 0.010870period mi(181) c(1) 0.010870position mi(596) c(1) 0.010870pocket mi(605) c(2) 0.021739metal mi(612) c(1) 0.010870place mi(443) c(1) 0.010870grass mi(381) c(1) 0.010870command mi(1815) c(1) 0.010870bird mi(701) c(1) 0.010870building mi(307) c(1) 0.010870sprinkler mi(25) c(1) 0.010870system mi(35) c(1) 0.010870over-watering mi(37) c(1) 0.010870hole mi(60) c(1) 0.010870pound mi(61) c(3) 0.032609growing mi(105) c(2) 0.021739spring mi(42) c(2) 0.021739February mi(38) c(2) 0.021739thing mi(43) c(1) 0.010870cutting mi(114) c(1) 0.010870rabbiteye mi(122) c(1) 0.010870ajuga mi(124) c(1) 0.010870area mi(131) c(1) 0.010870slope mi(132) c(1) 0.010870trunk mi(225) c(5) 0.054348myrtle mi(194) c(2) 0.021739heating mi(585) c(1) 0.010870step mi(588) c(1) 0.010870root mi(141) c(4) 0.043478drying mi(590) c(1) 0.010870crowding mi(595) c(1) 0.010870transplanting mi(594) c(1) 0.010870evaporation mi(609) c(1) 0.010870stake mi(613) c(3) 0.032609people mi(267) c(1) 0.010870triangle mi(616) c(1) 0.010870breath mi(1334) c(1) 0.010870stone mi(1813) c(1) 0.010870Government mi(2337) c(1) 0.010870Figure 4: Dumped Content of the node tree in the Collocation Map < h23 > indicatesthe index of tree in the Map is 23. di(19) is an index to dictionary, ctr(92) says treeoccurred 92 times, mi(19) indicates the index of inch in the Map is 19. c(4) of shrub saysshrub occurred 4 times in the back list.29of lexical concept.
The representation named Collocation Map is a variation of Belief Netthat uses sigmoid function in summing the conditioning evidences.
The dependency isnot as strong as that of ordinary Belief Net, but is of event occurrence.The potential power of Collocation Map can be fully appreciated when the computa-tional overhead is further educed.
Several options to alleviate the computational burdenare also begin studied in two approaches.
The one is parallel algorithm for Gibbs samplingand the other is to localize or optimize the sampling itself.
Preliminary test on the Mapbuilt from 100 texts shows a promising outlook, and we currently having a large scaletesting on 75,000 Korean text units(two million word corpus) and Pentree Bank.
Theaims of the test include the accuracy of modified sampling, sampling cost versus accuracy,comparison with the Boltzman machine implementation f the Collocation Map, LexicalConcept Acquisition, thesaurus construction, and sense disambiguation problems uch asin PP attachment and homonym resolution.References\[1\] Baker, J. K. 1979.
Trainable grammars for speech recognition.
Proceedings of SpringConference of the Acoustical Society of America, 547-550.
Boston, MA.\[2\] Ackley, G.E.
Hinton and T.J. Sejnowski.
(1985).
A Learning Algorithm for Boltzmannmachines, Cognitive Science.
9.
147-169.\[3\] Cho, Sehyeong, Maida, Anthony S. (1992).
"Using a Bayesian Framework to Identifythe Referents of Definite Descriptions."
AAAI Fall Symposium, Cambridge, Mas-sachusetts.\[4\] Dempster, A.P.
Laird, N.M. and Rubin, D.B.
(1977).
Maximum likelihood from in-complete data via the EM algorithm, J. Roy.
Star.
Soc.
B 39, 1-38.\[5\] Gelfan, A.E.
and Smith, A.F.M.
(1990).
Sampling-based approaches to calculatingmarginal densities, J.
Am.
Star.
Assoc 85.
398-409.\[6\] Goldman, Robert P. and Charniak Eugene.
(1992).
Probabilistic Text Understanding.Statistics and Computing.
2:105-114.\[7\] aan, Young S. Choi, Key-Sun.
(1993).
Indexing Based on Formal Relevancy ofBayesian Document Semantics.
Korea/Japan Joint Conferenceon on Expert Systems,Seoul, Korea.\[8\] Lauritzen and Spiegelhalter, D.J.
(1988).
Local computation with probabilities ongraphical structures and their application to expert systems.
J Roy.
Star.
Soc.
50.157-224.\[9\] Metropolis, N. Rosenbluth, A. W. Teller, A.H. Teller and Teller, E. (1953).
Equationof state calculations by fast computing machines.
J Chem.
Phys.
21.
1087-1092.\[10\] Neal, R.M.
(1992).
Connectionist learning of belief network.
Artificial Intelligence 56.71-113.\[11\] Pearl, J.
(1988).
Probabilistic Reasoning in Intelligent System: Networks of PlausibleInference.
Morgan Kaufman, San Mateo.30\[12\] Schutze, Hinrich.
(1992).
Context Space, AAAI Fall Symposium Series, Cambridge,Massachusetts.\[13\] Spiegelhalter D.. and Lauritzen, S.L.. (1990).
sequential updating of conditionalprobabilities on directed graphical structures.
Networks 20.
579-605.\[14\] Yoon, HyunSoo.
(1992) "A Study on the Parallel Hopfield Neural Network withStable-State Convergence Property."
KAIST TR(Computer Architecture Lab).31
