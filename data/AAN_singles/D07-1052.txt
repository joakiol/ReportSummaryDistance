Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
496?505, Prague, June 2007. c?2007 Association for Computational LinguisticsAntecedent Selection Techniques for High-Recall Coreference ResolutionYannick Versleyversley@sfs.uni-tuebingen.deSFB 441 / Seminar fu?r SprachwissenschaftUniversita?t Tu?bingenAbstractWe investigate methods to improve the re-call in coreference resolution by also tryingto resolve those definite descriptions whereno earlier mention of the referent shares thesame lexical head (coreferent bridging).
Theproblem, which is notably harder than iden-tifying coreference relations among men-tions which have the same lexical head, hasbeen tackled with several rather different ap-proaches, and we attempt to provide a mean-ingful classification along with a quantita-tive comparison.
Based on the different mer-its of the methods, we discuss possibilities toimprove them and show how they can be ef-fectively combined.1 IntroductionCoreference resolution, the task of grouping men-tions in a text that refer to the same referent in thereal world, has been shown to be beneficial for anumber of higher-level tasks such as information ex-traction (McCarthy and Lehnert, 1995), question an-swering (Morton, 2000) and summarisation (Stein-berger et al, 2005).While the resolution of pronominal anaphora andtracking of named entities is possible with goodaccuracy, the resolution of definite NPs (having acommon noun as their head) is usually limited tothe cases that Vieira and Poesio (2000) call directcoreference, where both coreferent mentions havethe same head.
The other cases, called coreferentbridging by Vieira and Poesio1, are notably harderbecause the number of potential candidates is much1Because bridging (in the sense of Clark, 1975, or Asher andlarger when it is no longer possible to rely on surfacesimilarity.To overcome the limit of recall that is encoun-tered when only relying on surface features, newersystems for coreference resolutions (Daume?
III andMarcu, 2005; Ponzetto and Strube, 2006; Versley,2006; Ng, 2007, inter alia) use lexical semantic in-formation as an indication for semantic compati-bility in the absence of head equality.
Most cur-rent systems integrate the identification of discourse-new definites (i.e., cases like ?the sun?
or ?the manthat Ben met yesterday?, which are definite, butnot anaphoric) with the antecedent selection proper,which implies that the gain obtained for new featuresis dependent on the feature?s usefulness both in find-ing semantically related mentions and for the use indetecting discourse-new definites.One goal of this paper is to provide a better under-standing of these information sources by comparingproposed (and partly new) approaches for resolv-ing coreferent bridging by separately consideringthe task of antecedent selection (i.e., presupposingthat discourse-new markables have been identifiedbeforehand).
Although state of the art methods formodular discourse-new detection (Uryupina, 2003;Poesio et al, 2005) do not achieve near-perfect accu-racy for discourse-new detection, the results we givefor antecedent selection represent an upper boundon recall and precision for the full coreference task,and we think that this upper bound will be useful forLascarides, 1998) is a much broader concept, the term ?corefer-ent bridging?
is potentially confusing, as many cases are exam-ples of perfectly well-behaved anaphoric definite noun phrases.Because we want to emphasise the important difference to themore easily resolved cases of same-head coreference, we willstick with ?coreferent bridging?
as the only term that has beenestablished for this in the literature.496the design of features in both systems using a mod-ular approach, such as (Poesio et al, 2005), wherethe decision on discourse-newness is taken before-hand, and those that integrate discourse-new classifi-cation with the actual resolution of coreferent bridg-ing cases.
In contrast to earlier investigations (Mark-ert and Nissim, 2005; Garera and Yarowsky, 2006),we provide a more extensive overview on featuresand also discuss properties that influence their com-binability.Several approaches have been proposed for thetreatment of coreferent bridging.
Poesio et al (1997)use WordNet, looking for a synonymy or hypernymyrelation (additionally, for coordinate sisters in Word-Net).
The system of Cardie and Wagstaff (1999)uses the node distance in WordNet (with an upperlimit of 4) as one component in the distance measurethat guides their clustering algorithm.
Harabagiuet al (2001) use paths through Wordnet, using notonly synonym and is-a relations, but also parts, mor-phological derivations, gloss texts and polysemy,which are weighted with a measure based on the re-lation types and number of path elements.
Other ap-proaches use large corpora to get an indication forbridging relations: Poesio et al (1998) use a generalword association metric based on common terms oc-curing in a fixed-width window, Gasperin and Vieira(2004) use syntactic contexts of words in a large cor-pus to induce a semantic similarity measure (similarto the one introduced by Lin, 1998), and then uselists of the n nouns that are (globally) most sim-ilar to a given noun.
Markert and Nissim (2005)mine the World Wide Web for shallow patterns like?China and other countries?, indicating an is-a rela-tionship.
Finally, Garera and Yarowsky (2006) pro-pose an association-based approach using nouns thatoccur in a 2-sentence window before a definite de-scription that has no same-head antecedent.1.1 Lexical vs. Referential RelationsOne important property of these information sourcesis the kind of lexical relations that they detect.
Thelexical relations that we expect in coreferent bridg-ing cases are:?
instance: The antecedent is an instance of theconcept denoted by the anaphorCorsica .
.
.
the island?
synonymy: The antecedent and the anaphor aresynonymsthe automobile .
.
.
the car?
hyperonymy: The anaphor is a strict generali-sation of the antecedentthe murderer .
.
.
the man?
near-synonymy: The anaphor and antecedentare semantically related but not synonyms inthe strict sensethe CD .
.
.
the albumOf course, not all cases of coreferent bridging realisesuch a lexical relation, as sometimes the anaphortakes up information introduced elsewhere than inthe lexical noun phrase head (Peter was found deadin his flat .
.
.
the deceased), or the coreference rela-tion is forced by the discourse structure, without theitems being lexically related.As an illustrating example, in(1) John walked towards [1 the house].
(2) a.
[1 The building] was illuminated.b.
[1 The manor] was guarded by dogs.c.
[2 The door] was open.Typical cases of coreference include cases like1,2a (hypernym) or 1,2b (compatible but non-synonymous term).
The discourse in 1,2c is anexample of associative bridging between the NP?the door?
and its antecedent to ?the house?
; itis inferred that the door must be part of the housementioned earlier (since doors are typically part ofa house), which is not compatible with coreferentbridging, but is also ranked highly by associationmeasures.While hypernym relations (as found by hypernymlookup in WordNet, or patterns indicating such rela-tions in unannotated texts) are usually a strong in-dicator of coreference, they can only cover someof the cases, while the near-synonymous cases areleft undiscovered.
Similarity and association mea-sures can help for the cases of near-synonymy.
How-ever, while similarity measures (such as WordNetdistance or Lin?s similarity metric) only detect casesof semantic similarity, association measures (suchas the ones used by Poesio et al, or by Gareraand Yarowsky) also find cases of associative bridg-497Lin98 RFF TheY TheY:G2 PL03Land (country/state/land)Staat Staat Kemalismus Regierung Kontinentstate state Kemalism government continentStadt Stadt Bauernfamilie Pra?sident Regioncity city agricultural family president regionRegion Landesregierung Bankgesellschaft Dollar Stadtregion country government banking corporation dollar cityBundesrepublik Bundesregierung Baht Albanien Staatfederal republic federal government Baht Albania stateRepublik Gewerkschaft Gasag Hauptstadt Bundeslandrepublic trade union (a gas company) capital stateMedikament (medical drug)Arzneimittel Pille RU Patient Arzneimittelpharmaceutical pill (a drug?)
patient pharmaceuticalPra?parat Droge Abtreibungspille Arzt Lebensmittelpreparation drug (non-medical) abortion pill doctor foodstuffPille Pra?parat Viagra Pille Pra?paratpill preparation Viagra pill preparationHormon Pestizid Pharmakonzern Behandlung Behandlunghormone pesticide pharmaceutical company treatment treatmentLebensmittel Lebensmittel Pra?parat Abtreibungspille Arzneifoodstuff foodstuff preparation abortion pill drughighest ranked words, with very rare words removed?
: RU 486, an abortifacient drugLin98: Lin?s distributional similarity measure (Lin, 1998)RFF: Geffet and Dagan?s Relative Feature Focus measure (Geffet and Dagan, 2004)TheY: association measure introduced by Garera and Yarowsky (2006)TheY:G2: similar method using a log-likelihood-based statistic (see Dunning 1993)this statistic has a preference for higher-frequency termsPL03: semantic space association measure proposed by Pado?
and Lapata (2003)Table 1: Similarity and association measures: most similar itemsing like 1a,b; the result of this can be seen in ta-ble (2): while the similarity measures (Lin98, RFF)list substitutable terms (which behave like synonymsin many contexts), the association measures (Gareraand Yarowsky?s TheY measure, Pado?
and Lapata?sassociation measure) also find non-compatible asso-ciations such as country?capital or drug?treatment,which is why they are commonly called relation-free.
For the purpose of coreference resolution, how-ever we do not want to resolve ?the door?
to the an-tecedent ?the house?
as the two descriptions do notcorefer, and it may be useful to filter out non-similarassociations.1.2 Information SourcesDifferent resources may be differently suited forthe recognition of the various relations.
Gener-ally, it would be expected that using a wordnetis the best solution if we are interested in an isa-like relation between two words.
On the otherhand, wordnets usually have limited coverage bothin terms of lexical items and in terms of relationsencoded (as their construction is necessarily labor-intensive), and ?
as Markert and Nissim remark?
they do not (and arguably should not) containcontext-dependent relations that do not hold gener-ally but only in some rather specific context, for ex-ample steel being anaphorically described as a com-modity in a financial text.
Context-dependent rela-tions, Markert and Nissim argue, can be found usingshallow patterns (for example, steel and other com-modities), since a use in such a context would meanthat the idiosyncratic conceptual relation holds inthat context.
Wordnets also have usually have poor(or non-existant) coverage of named entities, whichare especially relevant for instance relations; thiskind of instance relations can often be found in largetext corpora.
The high-precision patterns that Mark-ert and Nissim use only occur infrequently, but theapproach using shallow patterns allows to perform498the search of the World Wide Web, which somewhatalleviates the sparse data problem.While some near-synonyms can be found by look-ing at the distance in a wordnet, they may be farapart from each other because of ontological mod-eling decisions, or lexical items not covered by thewordnet.
Similarity and association measures canprovide greater coverage for these near-synonym re-lations.The measures both of Lin (1998) and of Pado?
andLapata (2003, 2007) are distributional methods; foreach word, they create a distribution of the contextsthey occur in, and similarity between two words iscalculated as the similarity of these distributions.2The difference in these two methods is the repre-sentation of the contexts.
While Lin uses contextsthat are expected to determine semantic preferences(like being in the direct object position of one verb),Pado?
and Lapata only use the co-occuring words,weighted by syntax-based distance.
For example, in(3) Peter subj?
likes dobj?
ice-cream.Lin?s approach would yield ?subj :like for Peterand ?dobj :like for ice-cream, while Pado?
andLapata?s approach would yield the contexts like(with a weight of 1.0) and ice-cream (with aweight of 0.5) for Peter.
As a consequence, Pado?and Lapata?s measure is more robust against datasparseness but also finds related non-similar terms(which are ultimately unwanted for coreference res-olution).
Pado?
and Lapata show their dependency-based measure to perform better in a word sensedisambiguation task than the measure of Lund et al(1995), on which Poesio et al (1998) based their ex-periments and which is based on the surface distanceof words.We also reimplemented the approach of Gar-era and Yarowsky (2006), who extract potentialanaphor-antecedent pairs from unlabeled texts andrank these potentially related pairs by the mutual in-formation statistic.
As an example, in a text like(4) Peter likes ice-cream.The boy devours tons of it.2Both measures use a weighted Jaccard metric on mutualinformation vectors to calculate the similarity.
See Weeds andWeir (2005) for an overview of other measures.we would extract the pairs ?boy,(person)?
and?boy,ice-cream?, in the hope that the formerpair occurs comparatively more often and gets ahigher mutual information value.2 Experiments on Antecedent SelectionIn a setting similar to Markert and Nissim (2005),we evaluate the precision (proportion of correctcases in the resolved cases) and recall (correct casesto all cases) for the resolution of discourse-old def-inite noun phrases.
Before trying to resolve coref-erent bridging cases, we look for compatible an-tecedent candidates with the same lexical head andresolve to the nearest such candidate if there is one.For our experiments, we used the first 125 articlesof the coreferentially annotated Tu?Ba-D/Z corpus ofwritten newspaper text (Hinrichs et al, 2005), to-talling 2239 sentences with 633 discourse-old defi-nite descriptions, and the latest release of GermaNet(Kunze and Lemnitzer, 2002), which is the German-language part of EuroWordNet.Unlike Markert and Nissim, we did not limit theevaluation to discourse-old noun phrases where anantecedent is in the 4 preceding sentences, but alsoincluded cases where the antecedent is further away.As a real coreference resolution system would haveto either resolve them correctly or leave them unre-solved, we feel that this is less unrealistic and thuspreferable even when it gives less optimistic evalu-ation results.
Because overall precision is a mixtureof the precision of the same-head resolver and theprecision of the resolution for coreferent bridging,which is lower than that for same-head cases, weforcibly get less precision if we resolve more coref-erent bridging cases.
As it is always possible to im-prove overall precision by resolving fewer cases ofcoreferent bridging, we separately mention the pre-cision for coreferent bridging cases alone (i.e., num-ber of correct coreferent bridging cases by all re-solved coreferent bridging cases), which we deemmore informative.In our evaluation, we included hypernymy searchand a simple edge-based distance based on Ger-maNet, as well as a baseline using semantic classes(automatically determined by a combination of sim-ple named entity classification and GermaNet sub-sumption), as well as an evolved version of Markert499Prec Recl F?=1 Prec.NSHsame-head 0.87 0.50 0.63 ?nearest(1) (only number check) 0.57 0.55 0.56 0.12semantic class+gender check(1) 0.68 0.61 0.64 0.35semantic class+gender check(2) 0.67 0.62 0.65 0.36GermaNet, hypernymy lookup 0.83 0.58 0.68 0.67GermaNet, node distance(1) 0.71 0.61 0.65 0.39single pattern: ?Y wie X?
(1) 0.83 0.54 0.66 0.55TheY(1) (only number checking) 0.66 0.59 0.62 0.29TheY(2) (only number checking) 0.66 0.60 0.63 0.31Lin(1) (only number checking) 0.66 0.60 0.63 0.30Lin(2) (only number checking) 0.69 0.64 0.66 0.39PL03(1) (only number checking) 0.68 0.63 0.65 0.38PL03(2) (only number checking) 0.70 0.64 0.65 0.4215-most-similar(1) 0.82 0.54 0.65 0.50100-most-similar(2,3) 0.73 0.60 0.66 0.42Prec.NSH: precision for coreferent bridging cases(1): consider candidates in the 4 preceding sentences(2): consider candidates in the 16 preceding sentences(3): also try candidates such that the anaphor isin the antecedent?s similarity listTable 2: Baseline resultsand Nissim?s approach, which is presented in (Ver-sley, 2007).
For the methods based on similarityand association measures, we implemented a simpleranking by the respective similarity or relatednessvalue.
Additionally, we included an approach due toGasperin and Vieira (2004), who tackle the problemof similarity by using lists of most similar words to acertain word, based on a similarity measure closelyrelated to Lin?s.
They allow resolution if either (i)the candidate is among the words most similar to theanaphor, (ii) the anaphor is among the words mostsimilar to the candidate, (iii) the similarity lists ofanaphor and candidate share a common item.
Wetried out several variations in the length of the simi-lar words list (Gasperin and Vieira used 15, we alsotried lists with 25, 50 and 100 items).
The third pos-sibility that Gasperin and Vieira mention (a commonitem in the similarity lists of both anaphor and an-tecedent) resolves some correct cases, but leads to amuch larger number of false positives, which is whywe did not include it in our evaluation.To induce the similarity and association measurespresented earlier, we used texts from the Germannewspaper die tageszeitung, comprising about 11Msentences.
For the extraction of anaphor-antecedentcandidates, we used a chunked version of the cor-pus (Mu?ller and Ule, 2002).
The identification ofgrammatical relations, was carried out on a subsetof all sentences (those with length ?
30), with anunlexicalised PCFG parser and subsequent extrac-tion of dependency relations (Versley, 2005).
Forthe last approach, where dependency relations wereneeded but labeling accuracy was not as important,we used a deterministic shift-reduce parser that Fothand Menzel (2006) used as input source in hybriddependency parsing.3For all three approaches, we lemmatised thewords by using a combination of SMOR (Schmidet al, 2004), a derivational finite-state morphologyfor German, and lexical information derived fromthe lexicon of a German dependency parser (Fothand Menzel, 2006).
We mitigated the problem of vo-cabulary growth in the lexicon, due to German syn-thetic compounds, by using a frequency-sensitiveunsupervised compound splitting technique, and(for semantic similarity) normalised common personand location names to ?(person)?
and ?
(location)?, re-spectively.Same-head resolution (including a check formodifier compatibility) allows to correctly resolve49.8% of all cases, with a precision of 86.5%.The most simple approach for coreferent bridging,just resolving coreferent bridging cases to the near-est possible antecedent (only checking for numberagreement), yields very poor precision (12% for thecoreferent bridging cases), and as a result, the re-call gain is very limited.
If we use semantic classes(based on both GermaNet and a simple classificationfor named entities) to constrain the candidates andthen use the nearest number- and gender-compatibleantecedent4, we get a much better precision (35%for coreferent bridging cases), and a much betterrecall of 61.1%.
Hyponymy lookup in GermaNet,without a limit on sentence distance, achieves a re-call of 57.5% (with a precision of 67% for the re-solved coreferent bridging cases), whereas using thebest single pattern (Y wie X , which corresponds to3Arguably, it would have been more convenient to use a sin-gle parser for all three approaches, but differing tradeoffs be-tween speed on one hand and accuracy for relevant informationand/or fitness of representation on the other hand made the re-spective parser or chunker a compelling choice.4In German, grammatical gender is not as predictive as inEnglish as it does not reproduce ontological distinctions.
Forpersons, grammatical and natural gender almost always coin-cide, and we check gender equality iff the anaphor is a person.500the English Y s such as X), with a distance limit of4 sentences5, on the Web only improves the recallto 54.3% (with a lower precision of 55% for coref-erent bridging cases).
This is in contrast to the re-sults of Markert and Nissim, who found that Webpattern search performs better than wordnet lookup;see (Versley, 2007) for a discussion.
Ranking allcandidates that are within a distance of 4 hyper-/hyponymy edges in GermaNet by their edge dis-tance, we get a relatively good recall of 60.5%, butthe precision (for the coreferent bridging cases) isonly at 39%, which is quite poor in comparison.The results for Garera and Yarowsky?s TheY al-gorithm are quite disconcerting ?
recall and the pre-cision on coreferent bridging cases are lower thanthe respective baseline using (wordnet-based) se-mantic class information or Pado?
and Lapata?s asso-ciation measure.
The technique based on Lin?s simi-larity measure does outperform the baseline, but stillsuffers from bad precision, along with Pado?
and La-pata?s association measure.
In other words, the simi-larity and association measures seem to be too noisyto be used directly for ranking antecedents.
The ap-proach of Gasperin and Vieira performs compara-bly to the approach using Web-based pattern search(although the precision is poorer than for the best-performing pattern for German, ?X wie Y ?
?
Xsuch as Y , it is comparable to that of other patterns).2.1 Improving Distributional Similarity?While it would be na?
?ve to think that the methodspurely based on statistical similarity measures couldreach the accuracy that can be achieved with a hand-constructed lexicalised ontology, it would of coursebe nice if we could improve the quality of the se-mantic similarity measure used in ranking and themost-similar-word lists.Geffet and Dagan (2004) propose an approachto improve the quality of the feature vectors usedin distributional similarity measures: instead ofweighting features using the mutual informationvalue between the word and the feature, they pro-pose to use a measure they call Relative Feature Fo-cus: the sum of the similarities to the (globally) most5There is a degradation in precision for the pattern-basedapproach, but not for the GermaNet-based approach, which iswhy we do not use a distance limit for the GermaNet-based ap-proach.similar words that share this feature.By replacing mutual information values with RFFvalues in Lin?s association measure, Geffet and Da-gan were able to significantly improve the propor-tion of substitutable words in the list of the most sim-ilar words.
In our experiments, however, using theRFF-based similarity measure did not improve thesimilarity-list-based resolution or the simple rank-ing, to the contrary, both recall and precision are lessthan for the Weighted Jaccard measure that we usedoriginally.6We attribute this to two factors: Firstly, Geffetand Dagan?s evaluation emphasises the precision interms of types, whereas the use in resolving coref-erent bridging does not punish unrelated rare wordsbeing ranked high ?
since these are rare, the like-lihood that they occur together, changing a reso-lution decision, is quite low, whereas rare relatedwords that are ranked high can allow a correct res-olution.
Secondly, Geffet and Dagan focus on high-frequency words, which makes sense in the contextof ontology learning, but the applicability for taskslike coreference resolution (directly or in the ap-proach of Gasperin and Vieira) also depends on asensible treatment of lower-frequency words.Using the framework of Weeds et al (2004), wefound that the bias of lower frequency words forpreferring high-frequency neighbours was higher forRFF (0.58 against 0.35 for Lin?s measure).
Weedsand Weir (2005) discuss the influence of bias to-wards high- or low-frequency items for differenttasks (correlation with WordNet-derived neighboursets and pseudoword disambiguation), and it wouldnot be surprising if the different high-frequency biaswere leading to different results.2.2 Combining Information SourcesThe information sources that we presented earlierand the corpus-based methods based on similarityor association measures draw from different kinds ofevidence and thus should be rather complementary.To put it another way, it should be possible to getthe best from all methods, achieving the recall of thehigh-recall methods (like using semantic class in-6Simple ranking with RFF gives a precision of 33% forcoreferent bridging cases, against 39% for Lin?s original mea-sure; for an approach based on similarity lists, we get 39%against 44%.50115-most-similar(2,3)100-most-similar(2,3)GWN5: hypernymyWeb (combined)semclass+gend(2)Lin similarity(2)Lin(2)+sem+gendTheY+sem+gendLin+BndPL03+BndGWN5?WebGWN5?TheY+s+gGWN5?Web?25-m.s.
(2,3) ?LinBndall combinedRecall (total)Precision(Non-same-head)0.550 0.600 0.650 0.7000.300.400.500.60Prec Recl F?=1 Prec.NSHsem.
class+gender checking 0.68 0.61 0.64 0.35GermaNet, hypernymy lookup 0.83 0.57 0.68 0.67GermaNet ?
?Y wie X?
0.81 0.60 0.69 0.63GermaNet ?
all patterns 0.81 0.61 0.70 0.64TheY(2)+semclass+gender 0.76 0.60 0.67 0.47TheY+sem+gend+Bnd 0.78 0.59 0.67 0.50Lin(2)+semclass+gender 0.71 0.63 0.67 0.43Lin+sem+gend+Bnd 0.80 0.58 0.67 0.53PL03(2)+semclass+gender 0.72 0.64 0.68 0.45PL03+sem+gend+Bnd 0.80 0.59 0.68 0.57GermaNet ?
all patterns 0.81 0.62 0.70 0.64?
25-most-similar(2,3) 0.79 0.65 0.72 0.62?
LinBnd 0.79 0.68 0.73 0.63?
Lin ?
TheY+sem+gend 0.74 0.70 0.72 0.54(2): consider candidates in the 16 preceding sentences(3): also try candidates such that the anaphor isin the antecedent?s similarity listTable 3: Combination-based approachesformation, or similarity and association measures),with a precision closer to the most precise methodusing GermaNet.
In the case of web-based patterns,Versley (2007) combines several pattern searches onthe web and uses the combined positive and nega-tive evidence to compute a composite score ?
with asuitably chosen cutoff, it outperforms all single pat-terns both in terms of precision and recall.
First re-solving via hyponymy in GermaNet and then usingthe pattern-combination approach outperforms thesemantic class-based baseline in terms of recall andis reasonably close to the GermaNet-based approachin terms of precision (i.e., much better than the ap-proach based only on the semantic class).As a first step to improve the precision of thecorpus-based approaches, we added filtering basedon automatically assigned semantic classes (per-sons, organisations, events, other countable objects,and everything else).
Very surprisingly, Garera andYarowsky?s TheY approach, despite starting out at alower precision (31%, against 39% for Lin and 42%for PL03), profits much more from the semantic fil-ter and reaches the best precision (47%), whereasLin?s semantic similarity measure profits the least.Since limiting the distance to the 4 previous sen-tences had quite a devastating effect for the approachbased on Lin?s similarity measure (which achieves39% precision when all the candidates are avail-able and 30% precision if it choses the most se-mantically similar out of the candidates that are inthe last 4 sentences), we also wanted to try and ap-ply the distance-based filtering after finding seman-tically related candidates.The approach we tried was as follows: we rank allcandidates using the similarity function, and keeponly the 3 top-rated candidates.
From these 3 top-rated candidates, we keep only those within the last4 sentences.
Without filtering by semantic class, thisimproves the precision to 41% (from 30% for lim-iting the distance beforehand, or 39% without lim-iting the distance).
Adding filtering based on se-mantic classes to this (only keeping those from the3 top-rated candidates which have a compatible se-mantic class and are within the last 4 sentences), weget a much better precision of 53%, with a recallthat can still be seen as good (57.8%).
In compari-son with the similarity-list-based approach, we get amuch better precision than we would get for meth-ods with comparable recall (the version with the 100most similar items has 44% precision, the versionwith 50 most similar items and matching both wayshas 46% precision).Applying this distance-bounding method to Gar-era and Yarowsky?s association measure still leadsto an improvement over the case with only seman-tic and gender checking, but the improvement (from47% to 50%) is not as large as with the semanticsimilarity measure or Pado?
and Lapata?s associationmeasure (from 45% to 57%).For the final system, we back off from the mostprecise information sources to the less precise.
Start-ing with the combination of GermaNet and pattern-based search on the World Wide Web, we beginby adding the distance-bounded semantic similarity-based resolver (LinBnd) and resolution based onthe list of 25 most similar words (following the502approach of Gasperin and Vieira 2004).
This re-sults in visibly improved recall (from 62% to 68%),while the precision for coreferent bridging casesdoes not suffer much.
Adding resolution based onLin?s semantic similarity measure and Garera andYarowsky?s TheY value leads to a further improve-ment in recall to 69.7%, but also leads to a largerloss in precision.3 ConclusionIn this paper, we compared several approaches to re-solve cases of coreferent bridging in open-domainnewspaper text.
While none of the informationsources can match the precision of the hypernymyinformation encoded in GermaNet, or that of usinga combination of high-precision patterns with theWorld Wide Web as a very large corpus, it is possi-ble to achieve a considerable improvement in termsof recall without sacrificing too much precision bycombining these methods.Very interestingly, the distributional methodsbased on intra-sentence relations (Lin, 1998;Pado?
and Lapata, 2003) outperformed Garera andYarowsky?s (2006) association measure when usedfor ranking, which may due to sparse data problemsor simply too much noise for the latter.
For the asso-ciation measures, the fact that they are relation-freealso means that they can profit from added semanticfiltering.The novel distance-bounded semantic similaritymethod (where we use the most similar words in theprevious discourse together with a semantic class-based filter and a distance limit) comes near the pre-cision of using surface patterns, and offers better ac-curacy than Gasperin and Vieira?s method of usingthe globally most similar words.By combining existing higher-precision informa-tion sources such as hypernym search in GermaNetand the Web-based approach presented in (Vers-ley, 2007) together with similarity- and association-based resolution, it is possible to get a large im-provement in recall even compared to the combinedGermaNet+Web approach or an approach combin-ing GermaNet with a semantically filtered versionof Garera and Yarowsky?s TheY approach.In independent research, Goecke et al (2006)combined the original LSA-based method of Lundet al (1995) with wordnet relations and patternsearch on a fixed-size corpus.7 However, they eval-uate only on a small subset of discourse-old definitedescriptions (those where a wordnet-compatible se-mantic relation was identified and which were rea-sonably close to their antecedent), and they did notdistinguish coreferent from associative bridging an-tecedents.
Although the different evaluation methoddisallows a meaningful comparison, we think thatthe more evolved information sources we use (Pado?and Lapata?s association measure instead of Lundet als, combined pattern search on the World WideWeb instead of search for patterns in a fixed-sizecorpus), as well as the additional information basedon semantic similarity, lead to superior results whenevaluated in a comparable task.3.1 Ongoing and Future WorkBoth the distributional similarity statistics and theassociation measure can profit from more trainingdata, something which is bound by availability ofsimilar text (Gasperin et al, 2004 point out that us-ing texts from a different genre strongly limits theusefulness of the learned semantic similarity mea-sure), and by processing costs (which are more se-rious for distributional similarity measures than fornon-grammar-related association measures, as theformer necessitate parsed input).Based on existing results for named entity coref-erence, a hypothetical coreference resolver combin-ing our information sources with a perfect detec-tor for discourse-new mentions would be able toachieve a precision of 88% and a recall of 83% con-sidering all full noun phrases (i.e., including names,but not pronouns).
This is both much higher thanstate-of-the art results for the same data set (Versley,2006, gets 62% precision and 70% recall), but suchaccuracy may be very difficult to achieve in prac-tice, as perfect (or even near-perfect) discourse-newdetection does not seem to achievable in the near fu-ture.
Preliminary experiments show that the inte-gration of pattern-based information leads to an in-crease in recall of 0.6% for the whole system (or46% more coreferent bridging cases), but the inte-gration of distributional similarity (loosely based onthe approach by Gasperin and Vieira) does not lead7Thanks to Tonio Wandmacher for pointing this out to me atGLDV?07.503to a noticeable improvement over GermaNet alne;in isolation, the distributional similarity informationdid improve the recall, albeit less than informationfrom GermaNet did.The fact that only a small fraction of the achiev-able recall gain is currently attained seems to sug-gest that better identification of discourse-old men-tions could potentially lead to larger improvements.It also seems that firstly, it makes more sense to com-bine information sources that cover different rela-tions (e.g.
GermaNet for hypernymy and synonymyand the pattern-based approach for instance rela-tions) than those that yield independent evidence forthe same relation(s), as GermaNet and the Gasperinand Vieira approach do for (near-)synonymy; andsecondly, that good precision is especially importantin the context of integrating antecedent selection anddiscourse-new identification, which means that thefiner view that we get using antecedent selection ex-periments (compared to direct use in a coreferenceresolver) is indeed helpful.Acknowledgements I am very grateful to SabineSchulte im Walde, Piklu Gupta and Sandra Ku?blerfor useful criticism of an earlier version, and toSimone Ponzetto and Michael Strube for feedbackon a talk related to this paper.
The research re-ported in this paper was supported by the DeutscheForschungsgemeinschaft (DFG) as part of Collab-orative Research Centre (Sonderforschungsbereich)441 ?Linguistic Data Structures?.ReferencesAsher, N. and Lascarides, A.
(1998).
Bridging.
Jour-nal of Semantics, 15(1):83?113.Cardie, C. and Wagstaff, K. (1999).
Noun phrasecoreference as clustering.
In Proceedings of theJoint Conference on Empirical Methods in Natu-ral Language Processing and Very Large Corpora(EMNLP/VLC 1999), pages 82?89.Clark, H. H. (1975).
Bridging.
In Schank, R. C. andNash-Webber, B. L., editors, Proceedings of the1975 workshop on Theoretical issues in naturallanguage processing, pages 169?174, Cambridge,MA.
Association for Computing Machinery.Daume?
III, H. and Marcu, D. (2005).
A large-scale exploration of effective global features fora joint entity detection and tracking model.
InHLT/EMNLP?05, pages 97?104.Dunning, T. (1993).
Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistics, 19(1):61?74.Foth, K. and Menzel, W. (2006).
Hybrid pars-ing: Using probabilistic models as predictors fora symbolic parser.
In ACL 2006.Garera, N. and Yarowsky, D. (2006).
Resolving andgenerating definite anaphora by modeling hyper-nymy using unlabeled corpora.
In CoNLL 2006.Gasperin, C., Salmon-Alt, S., and Vieira, R. (2004).How useful are similarity word lists for indirectanaphora resolution?
In Proc.
DAARC 2004.Gasperin, C. and Vieira, R. (2004).
Using wordsimilarity lists for resolving indirect anaphora.
InACL?04 workshop on reference resolution and itsapplications.Geffet, M. and Dagan, I.
(2004).
Feature vectorquality and distributional similarity.
In CoLing2004.Goecke, D., Stu?hrenberg, M., and Wandmacher, T.(2006).
Extraction and representation of seman-tic relations for resolving definite descriptions.
InWorkshop on Ontologies in Text Technology (OTT2006).
extended abstract.Harabagiu, S., Bunescu, R., and Maiorano, S.(2001).
Text and knowledge mining for corefer-ence resolution.
In Proceedings of the 2nd Meet-ing of the North American Chapter of the Associa-tion of Computational Linguistics (NAACL-2001).Hinrichs, E., Ku?bler, S., and Naumann, K. (2005).
Aunified representation for morphological, syntac-tic, semantic and referential annotations.
In ACLWorkshop on Frontiers in Corpus Annotation II:Pie in the Sky, Ann Arbor.Kunze, C. and Lemnitzer, L. (2002).
Germanet ?representation, visualization, application.
In Pro-ceedings of LREC 2002.Lin, D. (1998).
Automatic retrieval and clusteringof similar words.
In Proc.
CoLing/ACL 1998.Lund, K., Atchley, R. A., and Burgess, C.(1995).
Semantic and associative priming in high-dimensional semantic space.
In Proc.
of the 17thAnnual Conference of the Cognitive Science Soci-ety, pages 660?665.504Markert, K. and Nissim, M. (2005).
Comparingknowledge sources for nominal anaphora resolu-tion.
Computational Linguistics, 31(3):367?402.McCarthy, J. F. and Lehnert, W. G. (1995).
Usingdecision trees for coreference resolution.
In IJCAI1995, pages 1050?1055.Morton, T. S. (2000).
Coreference for NLP applica-tions.
In ACL-2000.Mu?ller, F. H. and Ule, T. (2002).
Annotating topo-logical fields and chunks ?
and revising POS tagsat the same time.
In Proceedings of the NineteenthInternational Conference on Computational Lin-guistics (COLING 2002).Ng, V. (2007).
Shallow semantics for coreferenceresolution.
In IJCAI 2007, pages 1689?1694.Pado?, S. and Lapata, M. (2003).
Constructing se-mantic space models from parsed corpora.
In Pro-ceedings of ACL 2003.Pado?, S. and Lapata, M. (2007).
Dependency-basedconstruction of semantic space models.
Compu-tational Linguistics, to appear.Poesio, M., Alexandrov-Kabadjov, M., Vieira, R.,Goulart, R., and Uryupina, O.
(2005).
Doesdiscourse-new detection help definite descriptionresolution?
In Proceedings of the 6th Inter-national Workshop on Computational Semantics(IWCS-6).Poesio, M., Schulte im Walde, S., and Brew, C.(1998).
Lexical clustering and definite descrip-tion interpretation.
In AAAI Spring Symposiumon Learning for Discourse.Poesio, M., Vieira, R., and Teufel, S. (1997).
Re-solving bridging descriptions in unrestricted text.In ACL-97 Workshop on Operational Factors inPractical, Robust, Anaphora Resolution For Un-restricted Texts.Ponzetto, S. P. and Strube, M. (2006).
Exploitingsemantic role labeling, wordnet and wikipedia forcoreference resolution.
In HLT-NAACL 2006.Schmid, H., Fitschen, A., and Heid, U.
(2004).SMOR: A german computational morphologycovering derivation, composition and inflection.In Proceedings of LREC 2004.Steinberger, J., Kabadjov, M., Poesio, M., andSanchez-Graillet, O.
(2005).
Improving LSA-based summarization with anaphora resolution.In Proceedings of Human Language TechnologyConference and Conference on Empirical Meth-ods in Natural Language Processing, pages 1?8.Uryupina, O.
(2003).
High-precision identificationof discourse new and unique noun phrases.
InProceedings of the ACL Student Workshop.Versley, Y.
(2005).
Parser evaluation across texttypes.
In Proceedings of the Fourth Workshop onTreebanks and Linguistic Theories (TLT 2005).Versley, Y.
(2006).
A constraint-based approachto noun phrase coreference resolution in Germannewspaper text.
In Konferenz zur VerarbeitungNatu?rlicher Sprache (KONVENS 2006).Versley, Y.
(2007).
Using the Web to resolvecoreferent bridging in German newspaper text.In Proceedings of GLDV-Fru?hjahrstagung 2007,Tu?bingen.
Narr.Vieira, R. and Poesio, M. (2000).
An empiricallybased system for processing definite descriptions.Computational Linguistics, 26(4):539?593.Weeds, J. and Weir, D. (2005).
Co-occurrence re-trieval: A flexible framework for lexical distri-butional similarity.
Computational Linguistics,31(4):439?475.Weeds, J., Weir, D., and McCarthy, D. (2004).
Char-acterizing measures of lexical distributional simi-larity.
In CoLing 2004.505
