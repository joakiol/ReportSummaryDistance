77Some Tests of an Unsupervised Model of Language AcquisitionBo Pedersen and Shimon EdelmanDepartment of PsychologyCornell UniversityIthaca, NY 14853, USA{bp64,se37}@cornell.eduZach Solan, David Horn, Eytan RuppinFaculty of Exact SciencesTel Aviv UniversityTel Aviv, Israel 69978{zsolan,horn,ruppin}@post.tau.ac.ilAbstractWe outline an unsupervised language acquisitionalgorithm and offer some psycholinguistic supportfor a model based on it.
Our approach resem-bles the Construction Grammar in its general phi-losophy, and the Tree Adjoining Grammar in itscomputational characteristics.
The model is trainedon a corpus of transcribed child-directed speech(CHILDES).
The model?s ability to process novelinputs makes it capable of taking various standardtests of English that rely on forced-choice judgmentand on magnitude estimation of linguistic accept-ability.
We report encouraging results from severalsuch tests, and discuss the limitations revealed byother tests in our present method of dealing withnovel stimuli.1 The empirical problem of languageacquisitionThe largely unsupervised, amazingly fast and al-most invariably successful learning stint that is lan-guage acquisition by children has long been theenvy of computer scientists (Bod, 1998; Clark,2001; Roberts and Atwell, 2002) and a dauntingenigma for linguists (Chomsky, 1986; Elman et al,1996).
Computational models of language acqui-sition or ?
grammar induction?
are usually dividedinto two categories, depending on whether they sub-scribe to the classical generative theory of syn-tax, or invoke ?
general-purpose?
statistical learningmechanisms.
We believe that polarization betweenclassical and statistical approaches to syntax ham-pers the integration of the stronger aspects of eachmethod into a common powerful framework.
Onthe one hand, the statistical approach is geared totake advantage of the considerable progress madeto date in the areas of distributed representationand probabilistic learning, yet generic ?
connection-ist?
architectures are ill-suited to the abstractionand processing of symbolic information.
On theother hand, classical rule-based systems excel injust those tasks, yet are brittle and difficult to train.We are developing an approach to the acquisi-tion of distributional information from raw input(e.g., transcribed speech corpora) that also supportsthe distillation of structural regularities comparableto those captured by Context Sensitive Grammarsout of the accrued statistical knowledge.
In think-ing about such regularities, we adopt Langacker?snotion of grammar as ?
simply an inventory of lin-guistic units?
((Langacker, 1987), p.63).
To de-tect potentially useful units, we identify and pro-cess partially redundant sentences that share thesame word sequences.
We note that the detectionof paradigmatic variation within a slot in a set ofotherwise identical aligned sequences (syntagms) isthe basis for the classical distributional theory oflanguage (Harris, 1954), as well as for some mod-ern work (van Zaanen, 2000).
Likewise, the pat-tern ?
the syntagm and the equivalence class ofcomplementary-distribution symbols that may ap-pear in its open slot ?
is the main representationalbuilding block of our system, ADIOS (for AutomaticDIstillation Of Structure).Our goal in the present short paper is to illus-trate some of the capabilities of the representa-tions learned by our method vis a vis standard testsused by developmental psychologists, by second-language instructors, and by linguists.
Thus, themain computational principles behind the ADIOSmodel are outlined here only briefl y.
The algo-rithmic details of our approach and accounts of itslearning from CHILDES corpora appear elsewhere(Solan et al, 2003a; Solan et al, 2003b; Solan et al,2004; Edelman et al, 2004).2 The principles behind the ADIOSalgorithmThe representational power of ADIOS and its capac-ity for unsupervised learning rest on three princi-ples: (1) probabilistic inference of pattern signifi-cance, (2) context-sensitive generalization, and (3)recursive construction of complex patterns.
Each ofthese is described briefl y below.78P84 that P58 P63E63 E64 P48E64 Beth | Cindy | George | Jim | Joe | Pam | P49 | P51P48 , doesn't itP51 the E50P49 a E50E50 bird | cat | cow | dog | horse | rabbitP61 who E62E62 adores | loves | scolds | worshipsE53 Beth | Cindy | George | Jim | Joe | PamE85 annoyes | bothers | disturbes | worriesP58 E60 E64E60 flies | jumps | laughsthat BethCindyGeorgeJimJoePam abird catcowdoghorserabbitthebird catcowdoghorserabbitfliesjumpslaughsannoyesbothersdisturbsworriesBethCindyGeorgeJimJoePamwhoadoreslovesscoldsworshipsBethCindyGeorgeJimJoePam abird catcowdoghorserabbitthebird catcowdoghorserabbit ,doesn't it5049505164605885 53 62615049505164486384 0.0001BethCindyGeorgeJimJoePam35believesthat34BethCindyGeorgeJimJoePam35thinksthat3839BethCindyGeorgeJimJoePam35believesthat34BethCindyGeorgeJimJoePam35believesthat34BethCindyGeorgeJimJoePam35thinksthat383952715454Joe thinks that George thinks that Cindy believes that George thinks that Pam thinks that ...that the bird jumps disturbes Jim who adores the cat, doesn't it?that BethCindyGeorgeJimJoePam abird catcowdoghorserabbitthebird catcowdoghorserabbitfliesjumpslaughsannoyesbothersdisturbsworriesBethCindyGeorgeJimJoePamwhoadoreslovesscoldsworshipsBethCindyGeorgeJimJoePam abird catcowdoghorserabbitthebird catcowdoghorserabbit ,doesn't it5049505164605885 53 62615049505164486384 0.0001BethCindyGeorgeJimJoePam35believesthat34BethCindyGeorgeJimJoePam35thinksthat3839BethCindyGeorgeJimJoePam35believesthat34BethCindyGeorgeJimJoePam35believesthat34BethCindyGeorgeJimJoePam35thinksthat383952715454Joe thinks that George thinks that Cindy believes that George thinks that Pam thinks that ...that the bird jumps disturbes Jim who adores the cat, doesn't it?P84 "that" P58 P63E63 E64 P48E64 "Beth" | "Cindy" | "George" | "Jim" | "Joe" | "Pam" | P49 | P51P48 "," "doesn't" "it"P51 "the" E50P49 "a" E50E50 "bird" | "cat" | "cow" | "dog" | "horse" | "rabbit"P61 "who" E62E62 "adores" | "loves" | "scolds" | "worships"E53 "Beth" | "Cindy" | "George" | "Jim" | "Joe" | "Pam"E85 "annoyes" | "bothers" | "disturbes" | "worries"P58 E60 E64E60 "flies" | "jumps" | "laughs"Long Range DependencyFigure 1: Left: a pattern (presented in a tree form), capturing a long range dependency (equivalence classlabels are underscored).
This and other examples here were distilled from a 400-sentence corpus generatedby a 40-rule Context Free Grammar.
Right: the same pattern recast as a set of rewriting rules that can bese as a Context Free Grammar fragment.thinksthat55thinks that84210barksmeows73and92athe89bird catcowdoghorserabbit109fliesjumpslaughs65114,doesn'tshe ?178BethCindyGeorgeJimJoePam75BethCindyPam56BethCindyGeorgeJimJoePam75P210 P55 P84BEGIN P55 P84 BEGIN E56 "thinks" "that" P84P55 P84 P178 P55 E75 "thinks" "that" P178AgreementPam Beth and Jim think that Joe thinks that Georgethinks that Cindy believes that Jim  who adores acat meows and the bird flies  , don't they?that Pam laughs worries a dog , doesn't it?that a cow jumps disturbs Jim who loves a horse ,doesn't it?Joe and Beth think that Jim believes that  the rabb itmeows and Pam  who scolds the dog laughs  , don'tthey?that Joe is eager to please disturbs the bird.Cindy thinks that Jim believes that  to read is tough .Beth thinks that Jim believes that  Beth  who loves ahorse meows and the horse jumps  , doesn't  sh e?that Pam is tough to please worries the cat.read53istough144 0.5topleaseread53521iseasy111 1123 0.5toplease52 1thatBethCindyGeorgeJimJoePam55iseagereasytough71topleaseread53521annoyesbothersdisturbsworries67661700.27BethCindyGeorgeJim55740.027BEGINthinksthat55thinks that84210barksmeows73and92athe89bird catcowdoghorserabbit109fliesjumpslaughs65114,doesn'tshe ?178BethCindyGeorgeJimJoePam75BethCindyPam56BethCindyGeorgeJimJoePam75P210 P55 P84BEGIN P55 P84 BEGIN E56 "thinks" "that" P84P55 P84 P178 P55 E75 "thinks" "that" P178AgreementPam Beth and Jim think that Joe thinks that Georgethinks that Cindy believes that Jim  who adores acat meows and the bird flies  , don't they?that Pam laughs worries a dog , doesn't it?that a cow jumps disturbs Jim who loves a horse ,doesn't it?Joe and Beth think that Jim believes that  the rabb itmeows and Pam  who scolds the dog laughs  , don'tthey?that Joe is eager to please disturbs the bird.Cindy thinks that Jim believes that  to read is tough .Beth thinks that Jim believes that  Beth  who loves ahorse meows and the horse jumps  , doesn't  sh e?that Pam is tough to please worries the cat.read53istough144 0.5topleaseread53521iseasy111 1123 0.5toplease52 1thatBethCindyGeorgeJimJoePam55iseagereasytough71topleaseread53521annoyesbothersdisturbsworries67661700.27BethCindyGeorgeJim55740.027BEGINFigure 2: Left: because ADIOS does not rewire all the occurrences of a specific pattern, but only those thatshare t e same context, its power is comparable to that of Context Sensitiv Grammars.
In this example,equivalence class #75 is not extended to subsume the subject position, because that position appears ina different context (e.g., immediately to the right of the symbol BEGIN).
Thus, long-range agreement isenforced and over-generalization prevented.
Right: the context-sensitive ?
rules?
corresponding to pattern#210.Probabilistic inference of pattern significance.ADIOS repr ents corpus of sentences as an ini-tially highly redundant directed graph, which can beinformally visualized as a tangle of strands that arepartially segregated into bundles.
Each of these con-sists of some strands clumped together; a bundle isformed when two or more strands join together andrun in parallel and is dissolved when more strandsleave the bundle than stay in.
In a given corpus,there will be many bundles, with each strand (sen-tence) possibly participating in several.
Our algo-rithm, described in detail in (Solan et al, 2004),identifies significant bundles that balance high com-pression (small size of the bundle ?
lexicon? )
againstgood generalization (the ability to generate newgrammatical sentences by splicing together variousstrand fragments each of which belongs to a differ-ent bundle).Context sensitivity of patterns.
A pattern is anabstraction of a bundle of sentences that are identi-cal up to variation in one place, where one of severalsymbols ?
the members of the equivalence classassociated with the pattern ?
may appear (Fig-ure 1).
Because this variation is only allowed inthe context specifi d by the pat ern, the generaliza-tion afforded by a set of patterns is inherently saferthan in approaches that posit globally valid cate-gories (?
parts f speec ? )
and rules (?
grammar?
).The reliance of ADIOS on many context-sensitivepatterns rather than on traditional rules can be com-pared both to the Construction Grammar (discussedlater) and to Langacker?s concept of the grammar asa collection of ?
patterns of all intermediate degreesof generality?
((Langacker, 1987), p.46).Hierarchical structure of patterns.
The ADIOSgraph is rewired every time a new pattern is de-tected, so that a bundle of strings subsumed by itis represented by a single new edge.
Following therewiring, which is context-specific, potentially far-apart symbols that used to straddle the newly ab-stracted pattern become close neighbors.
Patternsthus become hierarchically structured in that theirelements may be either terminals (i.e., fully speci-fied strings) or other patterns.
Moreover, patternsmay refer to themselves, which opens the door forrecursion.793 Related computational and linguisticformalisms and psycholinguistic findingsUnlike ADIOS, very few existing algorithms for un-supervised language acquisition use raw, unanno-tated corpus data (as opposed, say, to sentences con-verted into sequences of POS tags).
The only workdescribed in a recent review (Roberts and Atwell,2002) as completely unsupervised ?
the GraSpmodel (Henrichsen, 2002) ?
does attempt to in-duce syntax from raw transcribed speech, yet it isnot completely data-driven in that it makes a priorcommitment to a particular theory of syntax (Cate-gorial Grammar, complete with a pre-specified setof allowed categories).
Because of the unique na-ture of our chosen challenge ?
finding structurein language rather than imposing it ?
the follow-ing brief survey of grammar induction focuses oncontrasts and comparisons to approaches that gen-erally stop short of attempting to do what our al-gorithm does.
We distinguish between approachesthat are motivated computationally (Local Grammarand Variable Order Markov models, and Tree Ad-joining Grammar, discussed elsewhere (Edelman etal., 2004), and those whose main motivation is lin-guistic and cognitive psychological (Cognitive andConstruction grammars, discussed below).Local Grammar and Markov models.
In cap-turing the regularities inherent in multiple criss-crossing paths through a corpus, ADIOS su-perficially resembles finite-state Local Grammars(Gross, 1997) and Variable Order Markov (VOM)models (Guyon and Pereira, 1995).
The VOM ap-proach starts by postulating a maximum-n struc-ture, which is then fitted to the data by maximizingthe likelihood of the training corpus.
The ADIOSphilosophy differs from the VOM approach in sev-eral key respects.
First, rather than fitting a modelto the data, we use the data to construct a (recur-sively structured) graph.
Thus, our algorithm nat-urally addresses the inference of the graph?s struc-ture, a task that is more difficult than the estima-tion of parameters for a given configuration.
Sec-ond, because ADIOS works from the bottom up in arecursive, data-driven fashion, it is less susceptibleto complexity issues.
It can be used on huge graphs,and may yield very large patterns, which in a VOMmodel would correspond to an unmanageably highorder n. Third, ADIOS transcends the idea of VOMstructure, in the following sense.
Consider a set ofpatterns of the form b1[c1]b2[c2]b3, etc.
The equiv-alence classes [?]
may include vertices of the graph(both words and word patterns turned into nodes),wild cards (i.e., any node), as well as ambivalentcards (any node or no node).
This means that theterminal-level length of the string represented bya pattern does not have to be of a fixed length.This goes conceptually beyond the variable orderMarkov structure: b2[c2]b3 do not have to appear ina Markov chain of a finite order ||b2||+ ||c2||+ ||b3||because the size of [c2] is ill-defined, as explainedabove.
Fourth, as we showed earlier (Figure 2),ADIOS incorporates both context-sensitive substitu-tion and recursion.Tree Adjoining Grammar.
The proper place inthe Chomsky hierarchy for the class of strings ac-cepted by our model is between Context Free andContext Sensitive Languages.
The pattern-basedrepresentations employed by ADIOS have counter-parts for each of the two composition operations,substitution and adjoining, that characterize a TreeAdjoining Grammar, or TAG, developed by Joshiand others (Joshi and Schabes, 1997).
Specifically,both substitution and adjoining are subsumed in therelationships that hold among ADIOS patterns, suchas the membership of one pattern in another.
Con-sider a pattern Pi and its equivalence class E(Pi);any other pattern Pj ?
E(Pi) can be seen as substi-tutable in Pi.
Likewise, if Pj ?
E(Pi), Pk ?
E(Pi)and Pk ?
E(Pj), then the pattern Pj can be seenas adjoinable to Pi.
Because of this correspon-dence between the TAG operations and the ADIOSpatterns, we believe that the latter represent regu-larities that are best described by Mildly Context-Sensitive Language formalism (Joshi and Schabes,1997).
Importantly, because the ADIOS patternsare learned from data, they already incorporate theconstraints on substitution and adjoining that in theoriginal TAG framework must be specified manu-ally.Psychological and linguistic evidence for pattern-based representations.
Recent advances in un-derstanding the psychological role of representa-tions based on what we call patterns, or construc-tions (Goldberg, 2003), focus on the use of statis-tical cues such as conditional probabilities in pat-tern learning (Saffran et al, 1996; Go?mez, 2002),and on the importance of exemplars and construc-tions in children?s language acquisition (Cameron-Faulkner et al, 2003).
Converging evidence for thecentrality of pattern-like structures is provided bycorpus-based studies of prefabs ?
sequences, con-tinuous or discontinuous, of words that appear tobe prefabricated, that is, stored and retrieved as awhole, rather than being subject to syntactic pro-cessing (Wray, 2002).
Similar ideas concerning theubiquity in syntax of structural peculiarities hithertomarginalized as ?
exceptions?
are now being voicedby linguists (Culicover, 1999; Croft, 2001).80Cognitive Grammar; Construction Grammar.The main methodological tenets of ADIOS ?
pop-ulating the lexicon with ?
units?
of varying com-plexity and degree of entrenchment, and usingcognition-general mechanisms for learning and rep-resentation ?
fit the spirit of the foundations ofCognitive Grammar (Langacker, 1987).
At thesame time, whereas the cognitive grammarians typ-ically face the chore of hand-crafting structures thatwould refl ect the logic of language as they per-ceive it, ADIOS discovers the primitives of gram-mar empirically and autonomously.
The same istrue also for the comparison between ADIOS and thevarious Construction Grammars (Goldberg, 2003;Croft, 2001), which are all hand-crafted.
A con-struction grammar consists of elements that differin their complexity and in the degree to which theyare specified: an idiom such as ?
big deal?
is a fullyspecified, immutable construction, whereas the ex-pression ?
the X, the Y?
?
as in ?
the more, the bet-ter?
(Kay and Fillmore, 1999) ?
is a partially spec-ified template.
The patterns learned by ADIOS like-wise vary along the dimensions of complexity andspecificity (e.g., not every pattern has an equiva-lence class).4 ADIOS: a psycholinguistic evaluationTo illustrate the applicability of our method to realdata, we first describe briefl y the outcome of run-ning it on a subset of the CHILDES collection(MacWhinney and Snow, 1985), consisting of tran-scribed speech directed at children.
The corpus weselected contained 300, 000 sentences (1.3 milliontokens) produced by parents.
After 14 real-timedays, the algorithm (version 7.3) identified 3400patterns and 3200 equivalence classes.
The outcomewas encouraging: the algorithm found intuitivelysignificant patterns and produced semantically ad-equate corresponding equivalence sets.
The algo-rithm?s ability to recombine and reuse the acquiredpatterns is exemplified in the legend of Figure 3,which lists some of the novel sentences it generated.The input module.
The ADIOS system?s inputmodule allows it to process a novel sentence byforming its distributed representation in terms of ac-tivities of existing patterns.
We stress that this mod-ule plays a crucial role in the tests described below,all of which require dealing with novel inputs.
Fig-ure 4 shows the activation of two patterns (#141 and#120) by a phrase that contains a word in a novelcontext (stay), as well as another word never beforeencountered in any context (5pm).Acceptability of correct and perturbed novel sen-tences.
To test the quality of the representationsdo you14380wannawant to143791504115040I'm was14378thoughtyou14818were148191554015539gonna go ing14383to14384155441554316544go to the16543 (0.25)(1) (1) (1) (1) (1)(1)(1)(1)(1)(0.33)48 976531121let ' s14335(1)14374I'm was14378thoughtyou14818were148191554015539 (1)gonna go ing14383to143841554415543 (0.33)16556change heryour1655716555 (0.14)(1) (1)(1) (1)(1)10121314 1516171819Figure 3: a typical pattern extracted from theCHILDES collection (MacWhinney and Snow,1985).
Hundreds of such patterns and equivalenceclasses (underscored) together constitute a conciserepresentation of the raw data.
Some of the phrasesthat can be described/generated by these patternsare: let?s change her.
.
.
; I thought you weregonna change her.
.
.
; I was going to changeyour.
.
.
; none of these appear in the training data,illustrating the ability of ADIOS to generalize.
Thegeneration process operates as a depth-first searchof the tree corresponding to a pattern.
For detailssee (Solan et al, 2003a; Solan et al, 2004).
(patterns and their associated equivalence classes)acquired by ADIOS, we have examined their abil-ity to support various kinds of grammaticality judg-ments.
The first experiment we report sought tomake a distinction between a set of (presumablygrammatical) CHILDES sentences not seen by thealgorithm during training, and the same sentencesin which the word order has been perturbed.
Wefirst trained the model on 10, 000 sentences fromCHILDES, then compared its performance on (1)1000 previously unseen sentences and (2) the samesentences in each of which a single random wordorder switch has been carried out.
The results,shown in Figure 5, indicate a substantial sensitiv-ity of the ADIOS input module to simple deviationsfrom grammaticality in novel data, even after a verybrief training.Learnability of nonadjacent dependenciesWithin the ADIOS framework, the ?
nonadjacentdependencies?
that characterize the artificial lan-guages used by (Go?mez, 2002) translate, simply,into patterns with embedded equivalence classes.81WednesdayBEGINBethCindyGeorgeJoeJim Pamandare livwork ing141... activation level: 0.97274C0 C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 C13W8=1.0W0=1.0C14 C15 C16 C17 C18playBethCindyGeorgeJoeJim Pam86112113W15=0.8untiltomorrowFridayMondaySaturdaySundayThursdayTuesdayWednesdaynextmonthweekwinterEND120... activation level: 0.6671009389119C0 C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 C13W13=1.0W0=1.0W2..8=?W1=?Figure 4: The two most active patterns responding to the partially novel input Joe and Beth are stayinguntil 5pm.
Leaf activation, which is proportional to the mutual information between input words and variousmembers of the equivalence classes, is propagated upward by taking the average at each junction (Solan etal., 2003a).0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 501020304050607080Figure 5: Grammaticality of perturbed sentences(CHILDES data).
The figure shows a histogramof the input module output values for two kinds ofstimuli: novel grammatical sentences (dark/blue),and sentences obtained from these by a single word-order permutation (light/red).Go?mez showed that the ability of subjects to learna language L1 of the form {aXd, bXe, cXf}1,as measured by their ability to distinguish itimplicitly from L2={aXe, bXf, cXd}, dependson the amount of variation introduced at X .
Wereplicated this experiment by training ADIOS on432 strings from L1, with |X| = 2, 6, 12, 24.
Thestimuli were the same strings as in the originalexperiment, with the individual letters serving asthe basic symbols.
A subsequent test resulted in1Symbols a?f here stand for nonce words such as pel, vot,or dak, whereas X denotes a slot in which a subset of 24 othernonce words may appear.a perfect acceptance of L1 and a perfect rejectionof L2.
Training with the original words (ratherthan letters) as the basic symbols resulted in L2rejection rates of 0%, 55%, 100%, and 100%,respectively, for |X| = 2, 6, 12, 24.
Thus, theADIOS performance both mirrors that of the humansubjects and suggests a potentially interesting neweffect (of the granularity of the input stimuli) thatmay be explored in further psycholinguistic studies.A developmental test.
The CASL test (Compre-hensive Assessment of Spoken Language) is widelyused in the USA to assess language comprehen-sion in children (Carrow-Woolfolk, 1999).
One ofits many components is a grammaticality judgmenttest, which consists of 57 sentences and is admin-istered as follows: a sentence is read to the child,who then has to decide whether or not it is correct.If not, the child has to suggest a correct version ofthe sentence.
For every incorrect sentence, the testlists 2-3 acceptable correct ones.
The present ver-sion of the ADIOS algorithm can compare sentencesbut cannot score single sentences.
We therefore ig-nored 11 out of the 57 sentences, which were correctto begin with.
The remaining 46 incorrect sentencesand their corrected versions were scored by ADIOS(which for this test had been trained on a 300,000-sentence corpus from the CHILDES database); thehighest scoring sentence in each trial was inter-preted as the model?s choice.
The model labeled17 of the test sentences correctly, yielding a scoreof 108 (100 = norm) for the age interval 7-0 through7-2.
This score is the norm for the age interval 8-3through 8-5.22ADIOS was undecided about the majority of the other sen-tences on which it did not score correctly.82Figure 6: The results of several grammaticality tests(the Go?teborg ESL test is described in the text).ESL test (forced choice).
We next used a stan-dard test developed for English as Second Lan-guage (ESL) classes, which has been administeredin Go?teborg (Sweden) to more than 10, 000 uppersecondary levels students (that is, children who typ-ically had 9 years of school, but only 6-7 years ofEnglish).
The test consists of 100 three-choice ques-tions, such as She asked me at once (choices:come, to come, coming) and The tickets havebeen paid for, so you not worry (choices: may,dare, need); the average score for the populationmentioned is 65%.
As before, the choice given thehighest score by the algorithm won; if two choicesreceived the same top score, the answer was ?
don?tknow?
.
The algorithm?s performance in this andseveral other tests is summarized in Figure 6 (thesetests have been conducted with an earlier version ofthe algorithm (Solan et al, 2003a)).
In the ESL test,ADIOS scored at just under 60%; compare this tothe 45% precision (with 20% recall) achieved by astraightforward bi-gram benchmark.3ESL test (magnitude estimation).
In this exper-iment, six subjects were asked to provide magni-tude estimates of linguistic acceptability (Gurman-Bard et al, 1996) for all the 3 ?
100 sentences inthe Go?teborg ESL test.
The test was paper basedand included the instructions from (Keller, 2000).No measures were taken to randomize the order ofthe sentences or otherwise control the experiment.The same 300 sentences were processed by ADIOS,whose responses were normalized by dividing theoutput by the sum of each triplet?s score.
The re-sults indicate a significant correlation (R2 = 6.3%,p < 0.001) between the scores produced by the sub-jects and by ADIOS.
In some cases the scores of3Chance performance in this test is 33%.
We note that thecorpus used here was too small to train an n-gram model forn > 2; thus, our algorithm effectively overcomes the problemof sparse data by putting the available data to a better use.ADIOS are equal, which usually indicates that thereare too many unfamiliar words.
Omitting these sen-tences yields a significant R2 = 9.7%, p < 0.001;removing sentences for which the choices score al-most equally (within 10%) results in R2 = 12.7%,p < 0.001.4Figure 7: Magnitude estimation study from Keller,plotted against the ADIOS score on the same sen-tences (R2 = 0.53, p < 0.05).
The sentences(ranked by increasing score) are:How many men did you destroy the picture of?How many men did you destroy a picture of?How many men did you take the picture of?How many men did you take a picture of?Which man did you destroy the picture of?Which man did you destroy a picture of?Which man did you take the picture of?Which man did you take a picture of?Modeling Keller?s data.
A manuscript by FrankKeller lists magnitude estimation data for eight sen-tences.5 We compared these to the scores pro-duced by ADIOS, and obtained a significant corre-lation (Figure 7).
The input module seems capa-ble of dealing with the substitution of a with theor of take with destroy, and it does reasonablywell on the substitution of How many men withWhich man.
We conjecture that this performancecan be improved by a more sophisticated normal-ization of the score produced by the input module,which should do a better job quantifying the cover(Edelman, 2004) of a novel sentence by the storedpatterns.
The limitations of the present version ofthe model became apparent when we tested it on the4Four of the subjects only filled out the test partially (thenumbers of responses were 300, 300, 186, 159, 96, 60), but thecorrelation was highly significant despite the missing data.5http://elib.uni-stuttgart.de/opus/volltexte/1999/81/pdf/81.pdf8352 sentences from Keller?s dissertation, using hismagnitude estimation method (Keller, 2000).6 Forthese sentences, no correlation was found betweenthe human and the model scores.
One of the morechallenging aspects of this set is the central role ofpronoun binding in many of the sentences, e.g., Thewoman/Each woman saw Peter?s photographof her/herself/him/himself.
Moreover, this test setcontains examples of context effects, where infor-mation in an earlier sentence can help resolve a laterambiguity.
Thus, many of the grammatical contraststhat appear in Keller?s test sentences are too subtlefor the present version of the ADIOS input moduleto handle.Acceptability of correct and perturbed artifi-cial sentences.
In this experiment 64 random sen-tences was produced with a CFG.
For uniformity thesentence length was kept within 15-20 words.
16 ofthe sentences had two adjacent words switched andanother 16 had two random words switched.
The 64sentences were presented to 17 subjects, who placedeach on a computer screen at a lateral position re-fl ecting the perceived acceptability.
As expected,the perturbed sentences were rated as less accept-able than the non-perturbed ones (R2 = 50.3% withp < 0.01).
We controlled for sentence number, forhow high on the screen the sentence was placed, forthe reaction time and for sentence length; only thelatter had a significant contribution to the correla-tion.
The random permutations scored significantly(p < 0.01) lower than the adjacent permutations.Furthermore, the variance in the scores of the ran-domly permuted sentences was significantly larger(p < 0.005), suggesting that this kind of permu-tation violates the sentence structure more severely,but may also sometimes create acceptable sentencesby chance.
Previous tests showed that ADIOS is verygood at recognizing perturbed CFG-generated sen-tences as such, but it remains to be seen whether ornot ADIOS also exhibits differential behavior on theadjacent and non-adjacent permutations.Acceptability of ADIOS-generated sentences.ADIOS was trained on 12,700 sentences (out of atotal of 12,966 sentences) in the ATIS (Air TravelInformation System) corpus; the remaining 226 sen-tences were used for precision/recall tests.
Because6We remark that this methodology is not without its prob-lems.
As one of our linguistically naive subjects remarked,?
The instructions were (purposefully?)
vague about what Iwas supposed to judge ?
understandability, grammar, correctuse of language, or getting the point through.
.
.
?
.
Indeed, thescores in a magnitude experiment must be composites of sev-eral factors ?
at the very least, well-formedness and meaning-fulness.
We are presently exploring various means of acquiringand dealing with such multidimensional ?
acceptability?
data.ADIOS is sensitive to the presentation order of thetraining sentences, 30 instances were trained on ran-domized versions of the training set.
Eight hu-man subjects were then asked to estimate accept-ability of 20 sentences from the original corpus, in-termixed randomly with 20 sentences generated bythe trained versions of ADIOS.
The precision, calcu-lated as the average number of sentences acceptedby the subjects divided by the total number of sen-tences in the set (20), was 0.73 ?
0.2 for sentencesfrom the original corpus and 0.67 ?
0.07 for thesentences generated by ADIOS.
Thus, the ADIOS-generated sentences are, on the average, as accept-able to human subjects as the original ones.5 Concluding remarksThe ADIOS approach to the representation oflinguistic knowledge resembles the ConstructionGrammar in its general philosophy (e.g., in its re-liance on structural generalizations rather than onsyntax projected by the lexicon), and the Tree Ad-joining Grammar in its computational capacity (e.g.,in its apparent ability to accept Mildly Context Sen-sitive Languages).
The representations learned bythe ADIOS algorithm are truly emergent from the(unannotated) corpus data.
Previous studies focusedon the algorithm that makes such learning possible(Solan et al, 2004; Edelman et al, 2004).
In thepresent paper, we concentrated on testing the inputmodule that allows the acquired patterns to be usedin processing novel stimuli.The results of the tests we described here are en-couraging, but there is clearly room for improve-ment.
We believe that the most pressing issue inthis regard is developing a conceptually and com-putationally well-founded approach to the notion ofcover (that is, a distributed representation of a novelsentence in terms of the existing patterns).
Intu-itively, the best case, which should receive the topscore, is when there is a single pattern that preciselycovers the entire input, possibly in addition to otherevoked patterns that are only partially active.
We arecurrently investigating various approaches to scor-ing distributed representations in which several pat-terns are highly active.
A crucial constraint that ap-plies to such cases is that a good cover should give aproper expression to the subtleties of long-range de-pendencies and binding, many of which are alreadycaptured by the ADIOS learning algorithm.Acknowledgments.
Supported by the US-Israel Bi-national Science Foundation and by the Thanks toScandinavia Graduate Scholarship at Cornell.84ReferencesR.
Bod.
1998.
Beyond grammar: an experience-based theory of language.
CSLI Publications,Stanford, US.T.
Cameron-Faulkner, E. Lieven, and M. Tomasello.2003.
A construction-based analysis of child di-rected speech.
Cognitive Science, 27:843?874.E.
Carrow-Woolfolk.
1999.
Comprehensive As-sessment of Spoken Language (CASL).
AGS Pub-lishing, Circle Pines, MN.N.
Chomsky.
1986.
Knowledge of language: its na-ture, origin, and use.
Praeger, New York.A.
Clark.
2001.
Unsupervised Language Acquisi-tion: Theory and Practice.
Ph.D. thesis, COGS,U.
of Sussex.W.
Croft.
2001.
Radical Construction Grammar:syntactic theory in typological perspective.
Ox-ford U.
Press, Oxford.P.
W. Culicover.
1999.
Syntactic nuts: hard cases,syntactic theory, and language acquisition.
Ox-ford U.
Press, Oxford.S.
Edelman, Z. Solan, D. Horn, and E. Ruppin.2004.
Bridging computational, formal and psy-cholinguistic approaches to language.
In Proc.
ofthe 26th Conference of the Cognitive Science So-ciety, Chicago, IL.S.
Edelman.
2004.
Bridging language with therest of cognition: computational, algorithmicand neurobiological issues and methods.
InM.
Gonzalez-Marquez, M. J. Spivey, S. Coulson,and I. Mittelberg, eds., Proc.
of the Ithaca work-shop on Empirical Methods in Cognitive Linguis-tics.
John Benjamins.J.
L. Elman, E. A. Bates, M. H. Johnson,A.
Karmiloff-Smith, D. Parisi, and K. Plunkett.1996.
Rethinking innateness: A connectionistperspective on development.
MIT Press, Cam-bridge, MA.A.
E. Goldberg.
2003.
Constructions: a new theo-retical approach to language.
Trends in CognitiveSciences, 7:219?224.R.
L. Go?mez.
2002.
Variability and detectionof invariant structure.
Psychological Science,13:431?436.M.
Gross.
1997.
The construction of local gram-mars.
In E. Roche and Y. Schabe`s, eds., Finite-State Language Processing, pages 329?354.
MITPress, Cambridge, MA.E.
Gurman-Bard, D. Robertson, and A. Sorace.1996.
Magnitude estimation of linguistic accept-ability.
Language, 72:32?68.I.
Guyon and F. Pereira.
1995.
Design of a linguis-tic postprocessor using Variable Memory LengthMarkov Models.
In Proc.
3rd Int?l Conf.
Doc-ument Analysis and Recogition, pages 454?457,Montreal, Canada.Z.
S. Harris.
1954.
Distributional structure.
Word,10:140?162.P.
J. Henrichsen.
2002.
GraSp: Grammar learningform unlabeled speech corpora.
In Proceedingsof CoNLL-2002, pages 22?28.
Taipei, Taiwan.A.
Joshi and Y. Schabes.
1997.
Tree-AdjoiningGrammars.
In G. Rozenberg and A.
Salomaa,eds., Handbook of Formal Languages, volume 3,pages 69 ?
124.
Springer, Berlin.P.
Kay and C. J. Fillmore.
1999.
Grammaticalconstructions and linguistic generalizations: theWhat?s X Doing Y?
construction.
Language,75:1?33.F.
Keller.
2000.
Gradience in Grammar: Experi-mental and Computational Aspects of Degrees ofGrammaticality.
Ph.D. thesis, U. of Edinburgh.R.
W. Langacker.
1987.
Foundations of cogni-tive grammar, volume I: theoretical prerequisites.Stanford U.
Press, Stanford, CA.B.
MacWhinney and C. Snow.
1985.
The ChildLanguage Exchange System.
Journal of Compu-tational Lingustics, 12:271?296.A.
Roberts and E. Atwell.
2002.
Unsupervisedgrammar inference systems for natural language.Technical Report 2002.20, School of Computing,U.
of Leeds.J.
R. Saffran, R. N. Aslin, and E. L. Newport.
1996.Statistical learning by 8-month-old infants.
Sci-ence, 274:1926?1928.Z.
Solan, E. Ruppin, D. Horn, and S. Edelman.2003a.
Automatic acquisition and efficient rep-resentation of syntactic structures.
In S. Thrun,editor, Advances in Neural Information Process-ing, volume 15, Cambridge, MA.
MIT Press.Z.
Solan, E. Ruppin, D. Horn, and S. Edelman.2003b.
Unsupervised efficient learning and rep-resentation of language structure.
In R. Alter-man and D. Kirsh, eds., Proc.
25th Conferenceof the Cognitive Science Society, Hillsdale, NJ.Erlbaum.Z.
Solan, D. Horn, E. Ruppin, and S. Edelman.2004.
Unsupervised context sensitive languageacquisition from a large corpus.
In L. Saul, ed-itor, Advances in Neural Information Processing,volume 16, Cambridge, MA.
MIT Press.M.
van Zaanen.
2000.
ABL: Alignment-BasedLearning.
In COLING 2000 - Proceedings of the18th International Conference on ComputationalLinguistics, pages 961?967.A.
Wray.
2002.
Formulaic language and the lexi-con.
Cambridge U.
Press, Cambridge, UK.
