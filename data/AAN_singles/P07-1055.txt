Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 432?439,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsStructured Models for Fine-to-Coarse Sentiment AnalysisRyan McDonald?
Kerry Hannan Tyler Neylon Mike Wells Jeff ReynarGoogle, Inc.76 Ninth AvenueNew York, NY 10011?Contact email: ryanmcd@google.comAbstractIn this paper we investigate a structuredmodel for jointly classifying the sentimentof text at varying levels of granularity.
Infer-ence in the model is based on standard se-quence classification techniques using con-strained Viterbi to ensure consistent solu-tions.
The primary advantage of such amodel is that it allows classification deci-sions from one level in the text to influencedecisions at another.
Experiments show thatthis method can significantly reduce classifi-cation error relative to models trained in iso-lation.1 IntroductionExtracting sentiment from text is a challenging prob-lem with applications throughout Natural LanguageProcessing and Information Retrieval.
Previouswork on sentiment analysis has covered a wide rangeof tasks, including polarity classification (Pang etal., 2002; Turney, 2002), opinion extraction (Pangand Lee, 2004), and opinion source assignment(Choi et al, 2005; Choi et al, 2006).
Furthermore,these systems have tackled the problem at differ-ent levels of granularity, from the document level(Pang et al, 2002), sentence level (Pang and Lee,2004; Mao and Lebanon, 2006), phrase level (Tur-ney, 2002; Choi et al, 2005), as well as the speakerlevel in debates (Thomas et al, 2006).
The abil-ity to classify sentiment on multiple levels is impor-tant since different applications have different needs.For example, a summarization system for productreviews might require polarity classification at thesentence or phrase level; a question answering sys-tem would most likely require the sentiment of para-graphs; and a system that determines which articlesfrom an online news source are editorial in naturewould require a document level analysis.This work focuses on models that jointly classifysentiment on multiple levels of granularity.
Considerthe following example,This is the first Mp3 player that I have used ... Ithought it sounded great ... After only a few weeks,it started having trouble with the earphone connec-tion ...
I won?t be buying another.Mp3 player review from Amazon.comThis excerpt expresses an overall negative opinion ofthe product being reviewed.
However, not all partsof the review are negative.
The first sentence merelyprovides some context on the reviewer?s experiencewith such devices and the second sentence indicatesthat, at least in one regard, the product performedwell.
We call the problem of identifying the senti-ment of the document and of all its subcomponents,whether at the paragraph, sentence, phrase or wordlevel, fine-to-coarse sentiment analysis.The simplest approach to fine-to-coarse sentimentanalysis would be to create a separate system foreach level of granularity.
There are, however, obvi-ous advantages to building a single model that clas-sifies each level in tandem.
Consider the sentence,My 11 year old daughter has also been using it andit is a lot harder than it looks.In isolation, this sentence appears to convey negativesentiment.
However, it is part of a favorable review432for a piece of fitness equipment, where hard essen-tially means good workout.
In this domain, hard?ssentiment can only be determined in context (i.e.,hard to assemble versus a hard workout).
If the clas-sifier knew the overall sentiment of a document, thendisambiguating such cases would be easier.Conversely, document level analysis can benefitfrom finer level classification by taking advantageof common discourse cues, such as the last sentencebeing a reliable indicator for overall sentiment in re-views.
Furthermore, during training, the model willnot need to modify its parameters to explain phe-nomena like the typically positive word great ap-pearing in a negative text (as is the case above).
Themodel can also avoid overfitting to features derivedfrom neutral or objective sentences.
In fact, it has al-ready been established that sentence level classifica-tion can improve document level analysis (Pang andLee, 2004).
This line of reasoning suggests that acascaded approach would also be insufficient.
Valu-able information is passed in both directions, whichmeans any model of fine-to-coarse analysis shouldaccount for this.In Section 2 we describe a simple structuredmodel that jointly learns and infers sentiment on dif-ferent levels of granularity.
In particular, we reducethe problem of joint sentence and document levelanalysis to a sequential classification problem us-ing constrained Viterbi inference.
Extensions to themodel that move beyond just two-levels of analysisare also presented.
In Section 3 an empirical eval-uation of the model is given that shows significantgains in accuracy over both single level classifiersand cascaded systems.1.1 Related WorkThe models in this work fall into the broad class ofglobal structured models, which are typically trainedwith structured learning algorithms.
Hidden Markovmodels (Rabiner, 1989) are one of the earliest struc-tured learning algorithms, which have recently beenfollowed by discriminative learning approaches suchas conditional random fields (CRFs) (Lafferty et al,2001; Sutton and McCallum, 2006), the structuredperceptron (Collins, 2002) and its large-margin vari-ants (Taskar et al, 2003; Tsochantaridis et al, 2004;McDonald et al, 2005; Daume?
III et al, 2006).These algorithms are usually applied to sequentiallabeling or chunking, but have also been applied toparsing (Taskar et al, 2004; McDonald et al, 2005),machine translation (Liang et al, 2006) and summa-rization (Daume?
III et al, 2006).Structured models have previously been used forsentiment analysis.
Choi et al (2005, 2006) useCRFs to learn a global sequence model to classifyand assign sources to opinions.
Mao and Lebanon(2006) used a sequential CRF regression model tomeasure polarity on the sentence level in order todetermine the sentiment flow of authors in reviews.Here we show that fine-to-coarse models of senti-ment can often be reduced to the sequential case.Cascaded models for fine-to-coarse sentimentanalysis were studied by Pang and Lee (2004).
Inthat work an initial model classified each sentenceas being subjective or objective using a global min-cut inference algorithm that considered local label-ing consistencies.
The top subjective sentences arethen input into a standard document level polarityclassifier with improved results.
The current workdiffers from that in Pang and Lee through the use ofa single joint structured model for both sentence anddocument level analysis.Many problems in natural language processingcan be improved by learning and/or predicting mul-tiple outputs jointly.
This includes parsing and rela-tion extraction (Miller et al, 2000), entity labelingand relation extraction (Roth and Yih, 2004), andpart-of-speech tagging and chunking (Sutton et al,2004).
One interesting work on sentiment analysisis that of Popescu and Etzioni (2005) which attemptsto classify the sentiment of phrases with respect topossible product features.
To do this an iterative al-gorithm is used that attempts to globally maximizethe classification of all phrases while satisfying localconsistency constraints.2 Structured ModelIn this section we present a structured model forfine-to-coarse sentiment analysis.
We start by exam-ining the simple case with two-levels of granularity?
the sentence and document ?
and show that theproblem can be reduced to sequential classificationwith constrained inference.
We then discuss the fea-ture space and give an algorithm for learning the pa-rameters based on large-margin structured learning.433Extensions to the model are also examined.2.1 A Sentence-Document ModelLet Y(d) be a discrete set of sentiment labels atthe document level and Y(s) be a discrete set ofsentiment labels at the sentence level.
As input asystem is given a document containing sentencess = s1, .
.
.
, sn and must produce sentiment labelsfor the document, yd ?
Y(d), and each individ-ual sentence, ys = ys1, .
.
.
, ysn, where ysi ?
Y(s) ?1 ?
i ?
n. Define y = (yd,ys) = (yd, ys1, .
.
.
, ysn)as the joint labeling of the document and sentences.For instance, in Pang and Lee (2004), yd would bethe polarity of the document and ysi would indicatewhether sentence si is subjective or objective.
Themodels presented here are compatible with arbitrarysets of discrete output labels.Figure 1 presents a model for jointly classifyingthe sentiment of both the sentences and the docu-ment.
In this undirected graphical model, the labelof each sentence is dependent on the labels of itsneighbouring sentences plus the label of the docu-ment.
The label of the document is dependent onthe label of every sentence.
Note that the edgesbetween the input (each sentence) and the outputlabels are not solid, indicating that they are givenas input and are not being modeled.
The fact thatthe sentiment of sentences is dependent not only onthe local sentiment of other sentences, but also theglobal document sentiment ?
and vice versa ?
al-lows the model to directly capture the importanceof classification decisions across levels in fine-to-coarse sentiment analysis.
The local dependenciesbetween sentiment labels on sentences is similar tothe work of Pang and Lee (2004) where soft localconsistency constraints were created between everysentence in a document and inference was solved us-ing a min-cut algorithm.
However, jointly modelingthe document label and allowing for non-binary la-bels complicates min-cut style solutions as inferencebecomes intractable.Learning and inference in undirected graphicalmodels is a well studied problem in machine learn-ing and NLP.
For example, CRFs define the prob-ability over the labels conditioned on the input us-ing the property that the joint probability distribu-tion over the labels factors over clique potentials inundirected graphical models (Lafferty et al, 2001).Figure 1: Sentence and document level model.In this work we will use structured linear classi-fiers (Collins, 2002).
We denote the score of a la-beling y for an input s as score(y, s) and define thisscore as the sum of scores over each clique,score(y, s) = score((yd,ys), s)= score((yd, ys1, .
.
.
, ysn), s)=n?i=2score(yd, ysi?1, ysi , s)where each clique score is a linear combination offeatures and their weights,score(yd, ysi?1, ysi , s) = w ?
f(yd, ysi?1, ysi , s) (1)and f is a high dimensional feature representationof the clique and w a corresponding weight vector.Note that s is included in each score since it is givenas input and can always be conditioned on.In general, inference in undirected graphical mod-els is intractable.
However, for the common case ofsequences (a.k.a.
linear-chain models) the Viterbi al-gorithm can be used (Rabiner, 1989; Lafferty et al,2001).
Fortunately there is a simple technique thatreduces inference in the above model to sequenceclassification with a constrained version of Viterbi.2.1.1 Inference as Sequential LabelingThe inference problem is to find the highest scor-ing labeling y for an input s, i.e.,argmaxyscore(y, s)If the document label yd is fixed, then inferencein the model from Figure 1 reduces to the sequen-tial case.
This is because the search space is onlyover the sentence labels ysi , whose graphical struc-ture forms a chain.
Thus the problem of finding the434Input: s = s1, .
.
.
, sn1.
y = null2.
for each yd ?
Y(d)3. ys = argmaxys score((yd,ys), s)4. y?
= (yd,ys)5. if score(y?, s) > score(y, s) or y = null6.
y = y?7.
return yFigure 2: Inference algorithm for model in Figure 1.The argmax in line 3 can be solved using Viterbi?salgorithm since yd is fixed.highest scoring sentiment labels for all sentences,given a particular document label yd, can be solvedefficiently using Viterbi?s algorithm.The general inference problem can then be solvedby iterating over each possible yd, finding ys max-imizing score((yd,ys), s) and keeping the singlebest y = (yd,ys).
This algorithm is outlined in Fig-ure 2 and has a runtime of O(|Y(d)||Y(s)|2n), dueto running Viterbi |Y(d)| times over a label space ofsize |Y(s)|.
The algorithm can be extended to pro-duce exact k-best lists.
This is achieved by usingk-best Viterbi techniques to return the k-best globallabelings for each document label in line 3.
Mergingthese sets will produce the final k-best list.It is possible to view the inference algorithm inFigure 2 as a constrained Viterbi search since it isequivalent to flattening the model in Figure 1 to asequential model with sentence labels from the setY(s) ?
Y(d).
The resulting Viterbi search wouldthen need to be constrained to ensure consistentsolutions, i.e., the label assignments agree on thedocument label over all sentences.
If viewed thisway, it is also possible to run a constrained forward-backward algorithm and learn the parameters forCRFs as well.2.1.2 Feature SpaceIn this section we define the feature representa-tion for each clique, f(yd, ysi?1, ysi , s).
Assume thateach sentence si is represented by a set of binarypredicates P(si).
This set can contain any predicateover the input s, but for the present purposes it willinclude all the unigram, bigram and trigrams inthe sentence si conjoined with their part-of-speech(obtained from an automatic classifier).
Back-offsof each predicate are also included where one ormore word is discarded.
For instance, if P(si) con-tains the predicate a:DT great:JJ product:NN,then it would also have the predicatesa:DT great:JJ *:NN, a:DT *:JJ product:NN,*:DT great:JJ product:NN, a:DT *:JJ *:NN, etc.Each predicate, p, is then conjoined with the labelinformation to construct a binary feature.
For exam-ple, if the sentence label set is Y(s) = {subj, obj}and the document set is Y(d) = {pos, neg}, thenthe system might contain the following feature,f(j)(yd, ysi?1, ysi , s) =??????????
?1 if p ?
P(si)and ysi?1 = objand ysi = subjand yd = neg0 otherwiseWhere f(j) is the jth dimension of the feature space.For each feature, a set of back-off features are in-cluded that only consider the document label yd, thecurrent sentence label ysi , the current sentence anddocument label ysi and yd, and the current and pre-vious sentence labels ysi and ysi?1.
Note that throughthese back-off features the joint models feature setwill subsume the feature set of any individual levelmodel.
Only features observed in the training datawere considered.
Depending on the data set, the di-mension of the feature vector f ranged from 350K to500K.
Though the feature vectors can be sparse, thefeature weights will be learned using large-margintechniques that are well known to be robust to largeand sparse feature representations.2.1.3 Training the ModelLet Y = Y(d) ?
Y(s)n be the set of all validsentence-document labelings for an input s. Theweights, w, are set using the MIRA learning al-gorithm, which is an inference based online large-margin learning technique (Crammer and Singer,2003; McDonald et al, 2005).
An advantage of thisalgorithm is that it relies only on inference to learnthe weight vector (see Section 2.1.1).
MIRA hasbeen shown to provide state-of-the-art accuracy formany language processing tasks including parsing,chunking and entity extraction (McDonald, 2006).The basic algorithm is outlined in Figure 3.
Thealgorithm works by considering a single training in-stance during each iteration.
The weight vector w isupdated in line 4 through a quadratic programmingproblem.
This update modifies the weight vector so435Training data: T = {(yt, st)}Tt=11.
w(0) = 0; i = 02. for n : 1..N3.
for t : 1..T4.
w(i+1) = argminw*???w*?
w(i)???s.t.
score(yt, st)?
score(y?, s) ?
L(yt,y?
)relative to w*?y?
?
C ?
Y , where |C| = k5.
i = i + 16. return w(N?T )Figure 3: MIRA learning algorithm.that the score of the correct labeling is larger thanthe score of every labeling in a constraint set C witha margin proportional to the loss.
The constraint setC can be chosen arbitrarily, but it is usually taken tobe the k labelings that have the highest score underthe old weight vector w(i) (McDonald et al, 2005).In this manner, the learning algorithm can update itsparameters relative to those labelings closest to thedecision boundary.
Of all the weight vectors that sat-isfy these constraints, MIRA chooses the one that isas close as possible to the previous weight vector inorder to retain information about previous updates.The loss function L(y,y?)
is a positive real val-ued function and is equal to zero when y = y?.
Thisfunction is task specific and is usually the hammingloss for sequence classification problems (Taskar etal., 2003).
Experiments with different loss functionsfor the joint sentence-document model on a develop-ment data set indicated that the hamming loss oversentence labels multiplied by the 0-1 loss over doc-ument labels worked best.An important modification that was made to thelearning algorithm deals with how the k constraintsare chosen for the optimization.
Typically these con-straints are the k highest scoring labelings under thecurrent weight vector.
However, early experimentsshowed that the model quickly learned to discardany labeling with an incorrect document label forthe instances in the training set.
As a result, the con-straints were dominated by labelings that only dif-fered over sentence labels.
This did not allow the al-gorithm adequate opportunity to set parameters rel-ative to incorrect document labeling decisions.
Tocombat this, k was divided by the number of doc-ument labels, to get a new value k?.
For each doc-ument label, the k?
highest scoring labelings wereFigure 4: An extension to the model from Figure 1incorporating paragraph level analysis.extracted.
Each of these sets were then combined toproduce the final constraint set.
This allowed con-straints to be equally distributed amongst differentdocument labels.Based on performance on the development dataset the number of training iterations was set to N =5 and the number of constraints to k = 10.
Weightaveraging was also employed (Collins, 2002), whichhelped improve performance.2.2 Beyond Two-Level ModelsTo this point, we have focused solely on a model fortwo-level fine-to-coarse sentiment analysis not onlyfor simplicity, but because the experiments in Sec-tion 3 deal exclusively with this scenario.
In thissection, we briefly discuss possible extensions formore complex situations.
For example, longer doc-uments might benefit from an analysis on the para-graph level as well as the sentence and documentlevels.
One possible model for this case is givenin Figure 4, which essentially inserts an additionallayer between the sentence and document level fromthe original model.
Sentence level analysis is de-pendent on neighbouring sentences as well as theparagraph level analysis, and the paragraph anal-ysis is dependent on each of the sentences withinit, the neighbouring paragraphs, and the documentlevel analysis.
This can be extended to an arbitrarylevel of fine-to-coarse sentiment analysis by simplyinserting new layers in this fashion to create morecomplex hierarchical models.The advantage of using hierarchical models ofthis form is that they are nested, which keeps in-ference tractable.
Observe that each pair of adja-cent levels in the model is equivalent to the origi-nal model from Figure 1.
As a result, the scoresof the every label at each node in the graph canbe calculated with a straight-forward bottom-up dy-namic programming algorithm.
Details are omitted436Sentence Stats Document StatsPos Neg Neu Tot Pos Neg TotCar 472 443 264 1179 98 80 178Fit 568 635 371 1574 92 97 189Mp3 485 464 214 1163 98 89 187Tot 1525 1542 849 3916 288 266 554Table 1: Data statistics for corpus.
Pos = positivepolarity, Neg = negative polarity, Neu = no polarity.for space reasons.Other models are possible where dependenciesoccur across non-neighbouring levels, e.g., by in-serting edges between the sentence level nodes andthe document level node.
In the general case, infer-ence is exponential in the size of each clique.
Boththe models in Figure 1 and Figure 4 have maximumclique sizes of three.3 Experiments3.1 DataTo test the model we compiled a corpus of 600 on-line product reviews from three domains: car seatsfor children, fitness equipment, and Mp3 players.
Ofthe original 600 reviews that were gathered, we dis-carded duplicate reviews, reviews with insufficienttext, and spam.
All reviews were labeled by on-line customers as having a positive or negative polar-ity on the document level, i.e., Y(d) = {pos, neg}.Each review was then split into sentences and ev-ery sentence annotated by a single annotator as ei-ther being positive, negative or neutral, i.e., Y(s) ={pos, neg, neu}.
Data statistics for the corpus aregiven in Table 1.All sentences were annotated based on their con-text within the document.
Sentences were anno-tated as neutral if they conveyed no sentiment or hadindeterminate sentiment from their context.
Manyneutral sentences pertain to the circumstances un-der which the product was purchased.
A commonclass of sentences were those containing productfeatures.
These sentences were annotated as havingpositive or negative polarity if the context supportedit.
This could include punctuation such as excla-mation points, smiley/frowny faces, question marks,etc.
The supporting evidence could also come fromanother sentence, e.g., ?I love it.
It has 64Mb ofmemory and comes with a set of earphones?.3.2 ResultsThree baseline systems were created,?
Document-Classifier is a classifier that learnsto predict the document label only.?
Sentence-Classifier is a classifier that learnsto predict sentence labels in isolation of oneanother, i.e., without consideration for eitherthe document or neighbouring sentences sen-timent.?
Sentence-Structured is another sentence clas-sifier, but this classifier uses a sequential chainmodel to learn and classify sentences.
Thethird baseline is essentially the model from Fig-ure 1 without the top level document node.
Thisbaseline will help to gage the empirical gains ofthe different components of the joint structuredmodel on sentence level classification.The model described in Section 2 will be calledJoint-Structured.
All models use the same ba-sic predicate space: unigram, bigram, trigram con-joined with part-of-speech, plus back-offs of these(see Section 2.1.2 for more).
However, due to thestructure of the model and its label space, the featurespace of each might be different, e.g., the documentclassifier will only conjoin predicates with the doc-ument label to create the feature set.
All models aretrained using the MIRA learning algorithm.Results for each model are given in the first fourrows of Table 2.
These results were gathered using10-fold cross validation with one fold for develop-ment and the other nine folds for evaluation.
Thistable shows that classifying sentences in isolationfrom one another is inferior to accounting for a moreglobal context.
A significant increase in perfor-mance can be obtained when labeling decisions be-tween sentences are modeled (Sentence-Structured).More interestingly, even further gains can be hadwhen document level decisions are modeled (Joint-Structured).
In many cases, these improvements arehighly statistically significant.On the document level, performance can also beimproved by incorporating sentence level decisions?
though these improvements are not consistent.This inconsistency may be a result of the modeloverfitting on the small set of training data.
We437suspect this because the document level error rateon the Mp3 training set converges to zero muchmore rapidly for the Joint-Structured model than theDocument-Classifier.
This suggests that the Joint-Structured model might be relying too much onthe sentence level sentiment features ?
in order tominimize its error rate ?
instead of distributing theweights across all features more evenly.One interesting application of sentence level sen-timent analysis is summarizing product reviews onretail websites like Amazon.com or review aggrega-tors like Yelp.com.
In this setting the correct polar-ity of a document is often known, but we wish tolabel sentiment on the sentence or phrase level toaid in generating a cohesive and informative sum-mary.
The joint model can be used to classify sen-tences in this setting by constraining inference to theknown fixed document label for a review.
If this isdone, then sentiment accuracy on the sentence levelincreases substantially from 62.6% to 70.3%.Finally we should note that experiments usingCRFs to train the structured models and logistic re-gression to train the local models yielded similar re-sults to those in Table 2.3.2.1 Cascaded ModelsAnother approach to fine-to-coarse sentimentanalysis is to use a cascaded system.
In such a sys-tem, a sentence level classifier might first be runon the data, and then the results input into a docu-ment level classifier ?
or vice-versa.1 Two cascadedsystems were built.
The first uses the Sentence-Structured classifier to classify all the sentencesfrom a review, then passes this information to thedocument classifier as input.
In particular, for ev-ery predicate in the original document classifier, anadditional predicate that specifies the polarity of thesentence in which this predicate occurred was cre-ated.
The second cascaded system uses the docu-ment classifier to determine the global polarity, thenpasses this information as input into the Sentence-Structured model, constructing predicates in a simi-lar manner.The results for these two systems can be seen inthe last two rows of Table 2.
In both cases there1Alternatively, decisions from the sentence classifier canguide which input is seen by the document level classifier (Pangand Lee, 2004).is a slight improvement in performance suggestingthat an iterative approach might be beneficial.
Thatis, a system could start by classifying documents,use the document information to classify sentences,use the sentence information to classify documents,and repeat until convergence.
However, experimentsshowed that this did not improve accuracy over a sin-gle iteration and often hurt performance.Improvements from the cascaded models are farless consistent than those given from the joint struc-ture model.
This is because decisions in the cas-caded system are passed to the next layer as the?gold?
standard at test time, which results in errorsfrom the first classifier propagating to errors in thesecond.
This could be improved by passing a latticeof possibilities from the first classifier to the secondwith corresponding confidences.
However, solutionssuch as these are really just approximations of thejoint structured model that was presented here.4 Future WorkOne important extension to this work is to augmentthe models for partially labeled data.
It is realisticto imagine a training set where many examples donot have every level of sentiment annotated.
Forexample, there are thousands of online product re-views with labeled document sentiment, but a muchsmaller amount where sentences are also labeled.Work on learning with hidden variables can be usedfor both CRFs (Quattoni et al, 2004) and for in-ference based learning algorithms like those used inthis work (Liang et al, 2006).Another area of future work is to empirically in-vestigate the use of these models on longer docu-ments that require more levels of sentiment anal-ysis than product reviews.
In particular, the rela-tive position of a phrase to a contrastive discourseconnective or a cue phrase like ?in conclusion?
or?to summarize?
may lead to improved performancesince higher level classifications can learn to weighinformation passed from these lower level compo-nents more heavily.5 DiscussionIn this paper we have investigated the use of a globalstructured model that learns to predict sentiment ondifferent levels of granularity for a text.
We de-438Sentence Accuracy Document AccuracyCar Fit Mp3 Total Car Fit Mp3 TotalDocument-Classifier - - - - 72.8 80.1 87.2 80.3Sentence-Classifier 54.8 56.8 49.4 53.1 - - - -Sentence-Structured 60.5 61.4 55.7 58.8 - - - -Joint-Structured 63.5?
65.2??
60.1??
62.6??
81.5?
81.9 85.0 82.8Cascaded Sentence ?
Document 60.5 61.4 55.7 58.8 75.9 80.7 86.1 81.1Cascaded Document ?
Sentence 59.7 61.0 58.3 59.5 72.8 80.1 87.2 80.3Table 2: Fine-to-coarse sentiment accuracy.
Significance calculated using McNemar?s test between top twoperforming systems.
?Statistically significant p < 0.05.
?
?Statistically significant p < 0.005.scribed a simple model for sentence-document anal-ysis and showed that inference in it is tractable.
Ex-periments show that this model obtains higher ac-curacy than classifiers trained in isolation as wellas cascaded systems that pass information from onelevel to another at test time.
Furthermore, extensionsto the sentence-document model were discussed andit was argued that a nested hierarchical structurewould be beneficial since it would allow for efficientinference algorithms.ReferencesY.
Choi, C. Cardie, E. Riloff, and S. Patwardhan.
2005.
Identi-fying sources of opinions with conditional random fields andextraction patterns.
In Proc.
HLT/EMNLP.Y.
Choi, E. Breck, and C. Cardie.
2006.
Joint extraction of enti-ties and relations for opinion recognition.
In Proc.
EMNLP.M.
Collins.
2002.
Discriminative training methods for hiddenMarkov models: Theory and experiments with perceptronalgorithms.
In Proc.
EMNLP.K.
Crammer and Y.
Singer.
2003.
Ultraconservative onlinealgorithms for multiclass problems.
JMLR.Hal Daume?
III, John Langford, and Daniel Marcu.
2006.Search-based structured prediction.
In Submission.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmenting and la-beling sequence data.
In Proc.
ICML.P.
Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006.
Anend-to-end discriminative approach to machine translation.In Proc.
ACL.Y.
Mao and G. Lebanon.
2006.
Isotonic conditional randomfields and local sentiment flow.
In Proc.
NIPS.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Online large-margin training of dependency parsers.
In Proc.
ACL.R.
McDonald.
2006.
Discriminative Training and SpanningTree Algorithms for Dependency Parsing.
Ph.D. thesis, Uni-versity of Pennsylvania.S.
Miller, H. Fox, L.A. Ramshaw, and R.M.
Weischedel.
2000.A novel use of statistical parsing to extract information fromtext.
In Proc NAACL, pages 226?233.B.
Pang and L. Lee.
2004.
A sentimental education: Sen-timent analysis using subjectivity summarization based onminimum cuts.
In Proc.
ACL.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbs up?Sentiment classification using machine learning techniques.In EMNLP.A.
Popescu and O. Etzioni.
2005.
Extracting product featuresand opinions from reviews.
In Proc.
HLT/EMNLP.A.
Quattoni, M. Collins, and T. Darrell.
2004.
Conditionalrandom fields for object recognition.
In Proc.
NIPS.L.
R. Rabiner.
1989.
A tutorial on hidden Markov models andselected applications in speech recognition.
Proceedings ofthe IEEE, 77(2):257?285, February.D.
Roth and W. Yih.
2004.
A linear programming formula-tion for global inference in natural language tasks.
In Proc.CoNLL.C.
Sutton and A. McCallum.
2006.
An introduction to con-ditional random fields for relational learning.
In L. Getoorand B. Taskar, editors, Introduction to Statistical RelationalLearning.
MIT Press.C.
Sutton, K. Rohanimanesh, and A. McCallum.
2004.
Dy-namic conditional random fields: Factorized probabilisticmodels for labeling and segmenting sequence data.
In Proc.ICML.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-marginMarkov networks.
In Proc.
NIPS.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.2004.
Max-margin parsing.
In Proc.
EMNLP.M.
Thomas, B. Pang, and L. Lee.
2006.
Get out the vote:Determining support or opposition from congressional floor-debate transcripts.
In Proc.
EMNLP.I.
Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004.Support vector learning for interdependent and structuredoutput spaces.
In Proc.
ICML.P.
Turney.
2002.
Thumbs up or thumbs down?
Sentiment ori-entation applied to unsupervised classification of reviews.
InEMNLP.439
