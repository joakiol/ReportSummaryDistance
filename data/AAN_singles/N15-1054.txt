Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 515?525,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsInferring Missing Entity Type Instances for Knowledge Base Completion:New Dataset and MethodsArvind Neelakantan?Department of Computer ScienceUniversity of Massachusetts, AmherstAmherst, MA, 01003arvind@cs.umass.eduMing-Wei ChangMicrosoft Research1 Microsoft WayRedmond, WA 98052, USAminchang@microsoft.comAbstractMost of previous work in knowledge base(KB) completion has focused on the problemof relation extraction.
In this work, we focuson the task of inferring missing entity type in-stances in a KB, a fundamental task for KBcompetition yet receives little attention.Due to the novelty of this task, we constructa large-scale dataset and design an automaticevaluation methodology.
Our knowledge basecompletion method uses information withinthe existing KB and external information fromWikipedia.
We show that individual methodstrained with a global objective that consid-ers unobserved cells from both the entity andthe type side gives consistently higher qual-ity predictions compared to baseline methods.We also perform manual evaluation on a smallsubset of the data to verify the effectivenessof our knowledge base completion methodsand the correctness of our proposed automaticevaluation method.1 IntroductionThere is now increasing interest in the constructionof knowledge bases like Freebase (Bollacker et al,2008) and NELL (Carlson et al, 2010) in the nat-ural language processing community.
KBs containfacts such as Tiger Woods is an athlete, and BarackObama is the president of USA.
However, one of themain drawbacks in existing KBs is that they are in-complete and are missing important facts (West et?Most of the research conducted during summer internshipat Microsoft.al., 2014), jeopardizing their usefulness in down-stream tasks such as question answering.
This hasled to the task of completing the knowledge baseentries, or Knowledge Base Completion (KBC) ex-tremely important.In this paper, we address an important subprob-lem of knowledge base completion?
inferring miss-ing entity type instances.
Most of previous workin KB completion has only focused on the problemof relation extraction (Mintz et al, 2009; Nickel etal., 2011; Bordes et al, 2013; Riedel et al, 2013).Entity type information is crucial in KBs and iswidely used in many NLP tasks such as relationextraction (Chang et al, 2014), coreference reso-lution (Ratinov and Roth, 2012; Hajishirzi et al,2013), entity linking (Fang and Chang, 2014), se-mantic parsing (Kwiatkowski et al, 2013; Berantet al, 2013) and question answering (Bordes et al,2014; Yao and Durme, 2014).
For example, addingentity type information improves relation extractionby 3% (Chang et al, 2014) and entity linking by4.2 F1 points (Guo et al, 2013).
Despite their im-portance, there is surprisingly little previous workon this problem and, there are no datasets publiclyavailable for evaluation.We construct a large-scale dataset for the task ofinferring missing entity type instances in a KB.
Mostof previous KBC datasets (Mintz et al, 2009; Riedelet al, 2013) are constructed using a single snapshotof the KB and methods are evaluated on a subsetof facts that are hidden during training.
Hence, themethods could be potentially evaluated by their abil-ity to predict easy facts that the KB already contains.Moreover, the methods are not directly evaluated515Figure 1: Freebase description of Jean Metellus can be used to infer that the entity has the type /book/author.
Thismissing fact is found by our algorithm and is still missing in the latest version of Freebase when the paper is written.on their ability to predict missing facts.
To over-come these drawbacks we construct the train andtest data using two snapshots of the KB and evaluatethe methods on predicting facts that are added to themore recent snapshot, enabling a more realistic andchallenging evaluation.Standard evaluation metrics for KBC methods aregenerally type-based (Mintz et al, 2009; Riedel etal., 2013), measuring the quality of the predictionsby aggregating scores computed within a type.
Thisis not ideal because: (1) it treats every entity typeequally not considering the distribution of types, (2)it does not measure the ability of the methods to rankpredictions across types.
Therefore, we additionallyuse a global evaluation metric, where the quality ofpredictions is measured within and across types, andalso accounts for the high variance in type distri-bution.
In our experiments, we show that modelstrained with negative examples from the entity sideperform better on type-based metrics, while whentrained with negative examples from the type sideperform better on the global metric.In order to design methods that can rank pre-dictions both within and across entity (or relation)types, we propose a global objective to train themodels.
Our proposed method combines the ad-vantages of previous approaches by using nega-tive examples from both the entity and the typeside.
When considering the same number of nega-tive examples, we find that the linear classifiers andthe low-dimensional embedding models trained withthe global objective produce better quality rankingwithin and across entity types when compared totraining with negatives examples only from entity ortype side.
Additionally compared to prior methods,the model trained on the proposed global objectivecan more reliably suggest confident entity-type paircandidates that could be added into the given knowl-edge base.Our contributions are summarized as follows:?
We develop an evaluation framework com-prising of methods for dataset constructionand evaluation metrics to evaluate KBCapproaches for missing entity type in-stances.
The dataset and evaluation scripts arepublicly available at http://research.microsoft.com/en-US/downloads/df481862-65cc-4b05-886c-acc181ad07bb/default.aspx.?
We propose a global training objective for KBCmethods.
The experimental results show thatboth linear classifiers and low-dimensional em-bedding models achieve best overall perfor-mance when trained with the global objectivefunction.?
We conduct extensive studies on models for in-ferring missing type instances studying the im-pact of using various features and models.2 Inferring Entity TypesWe consider a KB ?
containing entity type informa-tion of the form (e, t), where e ?
E (E is the set ofall entities) is an entity in the KB with type t ?
T (Tis the set of all types).
For example, e could be TigerWoods and t could be sports athlete.
As a singleentity can have multiple types, entities in Freebaseoften miss some of their types.
The aim of this workis to infer missing entity type instances in the KB.Given an unobserved fact (an entity-type pair) in thetraining data (e, t) 6?
?
where entity e ?
E and typet ?
T , the task is to infer whether the KB currentlymisses the fact, i.e., infer whether (e, t) ?
?.
Weconsider entities in the intersection of Freebase andWikipedia in our experiments.2.1 Information ResourcesNow, we describe the information sources used toconstruct the feature representation of an entity to516infer its types.
We use information in Freebase andexternal information from Wikipedia to completethe KB.?
Entity Type Features: The entity types ob-served in the training data can be a useful sig-nal to infer missing entity type instances.
Forexample, in our snapshot of Freebase, it is notuncommon to find an entity with the type /peo-ple/deceased person but missing the type /peo-ple/person.?
Freebase Description: Almost all entities inFreebase have a short one paragraph descrip-tion of the entity.
Figure 1 shows the Freebasedescription of Jean Metellus that can be usedto infer the type /book/author which Freebasedoes not contain as the date of writing this arti-cle.?
Wikipedia: As external information, we in-clude the Wikipedia full text article of an en-tity in its feature representation.
We con-sider entities in Freebase that have a link totheir Wikipedia article.
The Wikipedia full textof an entity gives several clues to predict it?sentity types.
For example, Figure 2 showsa section of the Wikipedia article of ClaireMartin which gives clues to infer the type/award/award winner that Freebase misses.3 Evaluation FrameworkIn this section, we propose an evaluation methodol-ogy for the task of inferring missing entity type in-stances in a KB.
While we focus on recovering entitytypes, the proposed framework can be easily adaptedto relation extraction as well.First, we discuss our two-snapshot dataset con-struction strategy.
Then we motivate the importanceof evaluating KBC algorithms globally and describethe evaluation metrics we employ.3.1 Two Snapshots ConstructionIn most previous work on KB completion to pre-dict missing relation facts (Mintz et al, 2009; Riedelet al, 2013), the methods are evaluated on a subsetof facts from a single KB snapshot, that are hiddenwhile training.
However, given that the missing en-tries are usually selected randomly, the distributionof the selected unknown entries could be very differ-ent from the actual missing facts distribution.
Also,since any fact could be potentially used for evalua-tion, the methods could be evaluated on their abilityto predict easy facts that are already present in theKB.To overcome this drawback, we construct ourtrain and test set by considering two snapshots of theknowledge base.
The train snapshot is taken froman earlier time without special treatment.
The testsnapshot is taken from a later period, and a KBCalgorithm is evaluated by its ability of recoveringnewly added knowledge in the test snapshot.
Thisenables the methods to be directly evaluated on factsthat are missing in a KB snapshot.
Note that thefacts that are added to the test snapshot, in general,are more subtle than the facts that they already con-tain and predicting the newly added facts could beharder.
Hence, our approach enables a more realis-tic and challenging evaluation setting than previouswork.We use manually constructed Freebase as the KBin our experiments.
Notably, Chang et al (2014) usea two-snapshot strategy for constructing a dataset forrelation extraction using automatically constructedNELL as their KB.
The new facts that are added toa KB by an automatic method may not have all thecharacteristics that make the two snapshot strategymore advantageous.We construct our train snapshot ?0by taking theFreebase snapshot on 3rdSeptember, 2013 and con-sider entities that have a link to their Wikipedia page.KBC algorithms are evaluated by their ability to pre-dict facts that were added to the 1stJune, 2014 snap-shot of Freebase ?.
To get negative data, we makea closed world assumption treating any unobservedinstance in Freebase as a negative example.
Un-observed instances in the Freebase snapshot on 3rdSeptember, 2013 and 1stJune, 2014 are used as neg-ative examples in training and testing respectively.1The positive instances in the test data (??
?0) arefacts that are newly added to the test snapshot ?.
Us-ing the entire set of negative examples in the test datais impractical due to the large number of negative ex-amples.
To avoid this we only add the negative types1Note that some of the negative instances used in trainingcould be positive instances in test but we do not remove themduring training.517Figure 2: A section of the Wikipedia article of Claire Martin which gives clues that entity has the type/award/award winner.
This currently missing fact is also found by our algorithm.of entities that have at least one new fact in the testdata.
Additionally, we add a portion of the negativeexamples for entities which do not have new fact inthe test data and that were unused during training.This makes our dataset quite challenging since thenumber of negative instances is much larger than thenumber of positive instances in the test data.It is important to note that the goal of thiswork is not to predict facts that emerged betweenthe time period of the train and test snapshot2.For example, we do not aim to predict the type/award/award winner for an entity that won anaward after 3rdSeptember, 2013.
Hence, we usethe Freebase description in the training data snap-shot and Wikipedia snapshot on 3rdSeptember, 2013to get the features for entities.One might worry that the new snapshot mightcontain a significant amount of emerging facts soit could not be an effective way to evaluate theKBC algorithms.
Therefore, we examine the differ-ence between the training snapshot and test snap-shot manually and found that this is likely notthe case.
For example, we randomly selected 25/award/award winner instances that were added tothe test snapshot and found that all of them had wonat least one award before 3rdSeptember, 2013.Note that while this automatic evaluation is closerto the real-world scenario, it is still not perfect as thenew KB snapshot is still incomplete.
Therefore, wealso perform human evaluation on a small dataset toverify the effectiveness of our approach.2In this work, we also do not aim to correct existing falsepositive errors in Freebase3.2 Global Evaluation MetricMean average precision (MAP) (Manning et al,2008) is now commonly used to evaluate KB com-pletion methods (Mintz et al, 2009; Riedel et al,2013).
MAP is defined as the mean of average pre-cision over all entity (or relation) types.
MAP treatseach entity type equally (not explicitly accountingfor their distribution).
However, some types occurmuch more frequently than others.
For example,in our large-scale experiment with 500 entity types,there are many entity types with only 5 instances inthe test set while the most frequent entity type hastens of thousands of missing instances.
Moreover,MAP only measures the ability of the methods tocorrectly rank predictions within a type.To account for the high variance in the distribu-tion of entity types and measure the ability of themethods to correctly rank predictions across typeswe use global average precision (GAP) (similarlyto micro-F1) as an additional evaluation metric forKB completion.
We convert the multi-label classi-fication problem to a binary classification problemwhere the label of an entity and type pair is true ifthe entity has that type in Freebase and false oth-erwise.
GAP is the average precision of this trans-formed problem which can measure the ability of themethods to rank predictions both within and acrossentity types.Prior to us, Bordes et al (2013) use mean recip-rocal rank as a global evaluation metric for a KBCtask.
We use average precision instead of mean re-ciprocal rank since MRR could be biased to the toppredictions of the method (West et al, 2014)While GAP captures global ordering, it would be518beneficial to measure the quality of the top k pre-dictions of the model for bootstrapping and activelearning scenarios (Lewis and Gale, 1994; Cucerzanand Yarowsky, 1999).
We report G@k, GAP mea-sured on the top k predictions (similarly to Preci-sion@k and Hits@k).
This metric can be reliablyused to measure the overall quality of the top k pre-dictions.4 Global Objective for Knowledge BaseCompletionWe describe our approach for predicting missing en-tity types in a KB in this section.
While we focuson recovering entity types in this paper, the meth-ods we develop can be easily extended to other KBcompletion tasks.4.1 Global Objective FrameworkDuring training, only positive examples are ob-served in KB completion tasks.
Similar to previouswork (Mintz et al, 2009; Bordes et al, 2013; Riedelet al, 2013), we get negative training examples bytreating the unobserved data in the KB as negativeexamples.
Because the number of unobserved ex-amples is much larger than the number of facts inthe KB, we follow previous methods and sample fewunobserved negative examples for every positive ex-ample.Previous methods largely neglect the samplingmethods on unobserved negative examples.
The pro-posed global object framework allows us to system-atically study the effect of the different samplingmethods to get negative data, as the performance ofthe model for different evaluation metrics does de-pend on the sampling method.We consider a training snapshot of the KB ?0,containing facts of the form (e, t) where e is an en-tity in the KB with type t. Given a fact (e, t) inthe KB, we consider two types of negative examplesconstructed from the following two sets: NE(e, t) isthe ?negative entity set?, and NT(e, t) is the ?nega-tive type set?.
More precisely,NE(e, t) ?
{e?|e??
E, e?6= e, (e?, t) /?
?0},andNT(e, t) ?
{t?|t??
T, t?6= t, (e, t?)
/?
?0}.Let ?
be the model parameters, m = |NE(e, t)|and n = |NT(e, t)| be the number of negative exam-ples and types considered for training respectively.For each entity-type pair (e, t), we define the scor-ing function of our model as s(e, t|?
).3We definetwo loss functions one using negative entities andthe other using negative types:LE(?0, ?)
=?(e,t)??0,e?
?NE(e,t)[s(e?, t)?
s(e, t) + 1]k+,andLT(?0, ?)
=?(e,t)??0,t?
?NT(e,t)[s(e, t?)?
s(e, t) + 1]k+,where k is the power of the loss function (k can be 1or 2), and the function [?
]+is the hinge function.The global objective function is defined asmin?Reg(?)
+ CLT(?0, ?)
+ CLE(?0, ?
), (1)where Reg(?)
is the regularization term of themodel, and C is the regularization parameter.
In-tuitively, the parameters ?
are estimated to rank theobserved facts above the negative examples with amargin.
The total number of negative examples iscontrolled by the size of the sets NEand NT.
Weexperiment by sampling only entities or only typesor both by fixing the total number of negative exam-ples in Section 5.The rest of section is organized as follows: wepropose three algorithms based on the global objec-tive in Section 4.2.
In Section 4.3, we discuss the re-lationship between the proposed algorithms and ex-isting approaches.
Let ?
(e) ?
Rdebe the featurefunction that maps an entity to its feature represen-tation, and ?
(t) ?
Rdtbe the feature function thatmaps an entity type to its feature representation.4deand dtrepresent the feature dimensionality of the en-tity features and the type features respectively.
Fea-ture representations of the entity types (?)
is onlyused in the embedding model.3We often use s(e, t) as an abbreviation of s(e, t|?)
in orderto save space.4This gives the possibility of defining features for the labelsin the output space but we use a simple one-hot representationfor types right now since richer features did not give perfor-mance gains in our initial experiments.519Algorithm 1 The training algorithm for Lin-ear.Adagrad.1: Initialize wt= 0, ?t = 1 .
.
.
|T |2: for (e, t) ?
?0do3: for e??
NE(e, t) do4: if wTt?(e)?wTt?(e?)?
1 < 0 then5: AdaGradUpdate(wt,?(e?)?
?
(e))6: end if7: end for8: for t??
NT(e, t) do9: if wTt?(e)?wTt??(e)?
1 < 0 then10: AdaGradUpdate(wt,??
(e))11: AdaGradUpdate(wt?,?
(e)).12: end if13: end for14: end for4.2 AlgorithmsWe propose three different algorithms based on theglobal objective framework for predicting missingentity types.
Two algorithms use the linear modeland the other one uses the embedding model.Linear Model The scoring function in this modelis given by s(e, t|?
= {wt}) = wTt?
(e), wherewt?
Rdeis the parameter vector for target typet.
The regularization term in Eq.
(1) is defined asfollows: R(?)
= 1/2?t=1wTtwt.
We use k = 2 inour experiments.
Our first algorithm is obtained byusing the dual coordinate descent algorithm (Hsiehet al, 2008) to optimize Eq.
(1), where we modi-fied the original algorithm to handle multiple weightvectors.
We refer to this algorithm as Linear.DCD.While DCD algorithm ensures convergence to theglobal optimum solution, its convergence can beslow in certain cases.
Therefore, we adopt an on-line algorithm, Adagrad (Duchi et al, 2011).
Weuse the hinge loss function (k = 1) with no regu-larization (Reg(?)
= ?)
since it gave best resultsin our initial experiments.
We refer to this algo-rithm as Linear.Adagrad, which is described in Al-gorithm 1.
Note that AdaGradUpdate(x, g) is a pro-cedure which updates the vector x with the respectto the gradient g.Embedding Model In this model, vector repre-sentations are constructed for entities and types us-ing linear projection matrices.
Recall ?
(t) ?
Rdtis the feature function that maps a type to its featurerepresentation.
The scoring function is given byAlgorithm 2 The training algorithm for the embed-ding model.1: Initialize V,U randomly.2: for (e, t) ?
?0do3: for e??
NE(e, t) do4: if s(e, t)?
s(e?, t)?
1 < 0 then5: ??
VT?
(t)6: ?
?
UT(?(e?)?
?
(e))7: for i ?
1 .
.
.
d do8: AdaGradUpdate(Ui, ?[i](?(e?)?
?
(e)))9: AdaGradUpdate(Vi, ?[i]?
(t))10: end for11: end if12: end for13: for t??
NT(e, t) do14: if s(e, t)?
s(e, t?)?
1 < 0 then15: ??
VT(?(t?)??
(t))16: ?
?
UT?
(e)17: for i ?
1 .
.
.
d do18: AdaGradUpdate(Ui, ?[i]?
(e))19: AdaGradUpdate(Vi, ?[i](?(t?)??
(t)))20: end for21: end if22: end for23: end fors(e, t|?= (U,V)) = ?(t)TVUT?
(e),where U ?
Rde?dand V ?
Rdt?dare projectionmatrices that embed the entities and types in a d-dimensional space.
Similarly to the linear classifiermodel, we use the l1-hinge loss function (k = 1)with no regularization (Reg(?)
= ?).
Uiand Videnote the i-th column vector of the matrix U andV, respectively.
The algorithm is described in detailin Algorithm 2.The embedding model has more expressive powerthan the linear model, but the training unlike in thelinear model, converges only to a local optimum so-lution since the objective function is non-convex.4.3 Relationship to Existing MethodsMany existing methods for relation extraction andentity type prediction can be cast as a special caseunder the global objective framework.
For exam-ple, we can consider the work in relation extrac-tion (Mintz et al, 2009; Bordes et al, 2013; Riedelet al, 2013) as models trained with NT(e, t) = ?.These models are trained only using negative entitieswhich we refer to as Negative Entity (NE) objective.The entity type prediction model in Ling and Weld(2012) is a linear model with NE(e, t) = ?
which52070 types 500 typesEntities 2.2M 2.2MTraining Data Statistics (?0)positive example 4.5M 6.2Mmax #ent for a type 1.1M 1.1Mmin #ent for a type 6732 32Test Data Statistics (??
?0)positive examples 163K 240Knegative examples 17.1M 132Mnegative/positive ratio 105.22 554.44Table 1: Statistics of our dataset.
?0is our training snap-shot and ?
is our test snapshot.
An example is an entity-type pair.we refer to as the Negative Type (NT) objective.
Theembedding model described in Weston et al (2011)developed for image retrieval is also a special caseof our model trained with the NT objective.While the NE or NT objective functions couldbe suitable for some classification tasks (Weston etal., 2011), the choice of objective functions for theKBC tasks has not been well motivated.
Often thechoice is made neither with theoretical foundationnor with empirical support.
To the best of our knowl-edge, the global objective function, which includesbothNE(e, t) andNT(e, t), has not been consideredpreviously by KBC methods.5 ExperimentsIn this section, we give details about our dataset anddiscuss our experimental results.
Finally, we per-form manual evaluation on a small subset of thedata.5.1 DataFirst, we evaluate our methods on 70 entity typeswith the most observed facts in the training data.5We also perform large-scale evaluation by testing themethods on 500 types with the most observed factsin the training data.Table 1 shows statistics of our dataset.
The num-ber of positive examples is much larger in the train-ing data compared to that in the test data since thetest set contains only facts that were added to themore recent snapshot.
An additional effect of this is5We removed few entity types that were trivial to predict inthe test data.that most of the facts in the test data are about en-tities that are not very well-known or famous.
Thehigh negative to positive examples ratio in the testdata makes this dataset very challenging.5.2 Automatic Evaluation ResultsTable 2 shows automatic evaluation results where wegive results on 70 types and 500 types.
We comparedifferent aspects of the system on 70 types empiri-cally.Adagrad Vs DCD We first study the linear mod-els by comparing Linear.DCD and Linear.AdaGrad.Table 2a shows that Linear.AdaGrad consistentlyperforms better for our task.Impact of Features We compare the effect ofdifferent features on the final performance usingLinear.AdaGrad in Table 2b.
Types are repre-sented by boolean features while Freebase descrip-tion and Wikipedia full text are represented using tf-idf weighting.
The best MAP results are obtained byusing all the information (T+D+W) while best GAPresults are obtained by using the Freebase descrip-tion and Wikipedia article of the entity.
Note thatthe features are simply concatenated when multipleresources are used.
We tried to use idf weightingon type features and on all features, but they did notyield improvements.The Importance of Global Objective Table 2cand 2d compares global training objective with NEand NT training objective.
Note that all the threemethods use the same number of negative examples.More precisely, for each (e, t) ?
?0, |NE(e, t)| +|NT(e, t)| = m + n = 2.
The results show thatthe global training objective achieves best scoreson both MAP and GAP for classifiers and low-dimensional embedding models.
Among NE andNT, NE performs better on the type-based metricwhile NT performs better on the global metric.Linear Model Vs Embedding Model Finally, wecompare the linear classifier model with the embed-ding model in Table 2e.
The linear classifier modelperforms better than the embedding model in bothMAP and GAP.We perform large-scale evaluation on 500 typeswith the description features (as experiments areexpensive) and the results are shown in Table 2f.521Features Algorithm MAP GAPDescriptionLinear.Adagrad 29.17 28.17Linear.DCD 28.40 27.76Description +WikipediaLinear.Adagrad 33.28 31.97Linear.DCD 31.92 31.36(a) Adagrad vs. Dual coordinate descent (DCD).
Results areobtained using linear models trained with global training ob-jective (m=1, n=1) on 70 types.Features MAP GAPType (T) 12.33 13.58Description (D) 29.17 28.17Wikipedia (W) 30.81 30.56D + W 33.28 31.97T + D + W 36.13 31.13(b) Feature Comparison.
Results are obtained from using Lin-ear.Adagrad with global training objective (m=1, n=1) on 70types.Features Objective MAP GAPD + WNE (m = 2) 33.01 23.97NT (n = 2) 31.61 29.09Global (m = 1, n = 1) 33.28 31.97T + D + WNE (m = 2) 34.56 21.79NT (n = 2) 34.45 31.42Global (m = 1, n = 1) 36.13 31.13(c) Global Objective vs NE and NT.
Results are obtained us-ing Linear.Adagrad on 70 types.Features Objective MAP GAPD + WNE (m = 2) 30.92 22.38NT (n = 2) 25.77 23.40Global (m = 1, n = 1) 31.60 30.13T + D + WNE (m = 2) 28.70 19.34NT (n = 2) 28.06 25.42Global (m = 1, n = 1) 30.35 28.71(d) Global Objective vs NE and NT.
Results are obtained us-ing the embedding model on 70 types.Features Model MAP GAP G@1000 G@10000D + WLinear.Adagrad 33.28 31.97 79.63 68.08Embedding 31.60 30.13 73.40 64.69T + D + WLinear.Adagrad 36.13 31.13 70.02 65.09Embedding 30.35 28.71 62.61 64.30(e) Model Comparison.
The models were trained with the global training objective (m=1, n=1) on 70 types.Model MAP GAP G@1000 G@10000Linear.Adagrad 13.28 20.49 69.23 60.14Embedding 9.82 17.67 55.31 51.29(f) Results on 500 types using Freebase description features.
We train the models with the global training objective (m=1, n=1).Table 2: Automatic Evaluation Results.
Note that m = |NE(e, t)| and n = |NT(e, t)|.One might expect that with the increased number oftypes, the embedding model would perform betterthan the classifier since they share parameters acrosstypes.
However, despite the recent popularity of em-bedding models in NLP, linear model still performsbetter in our task.5.3 Human EvaluationTo verify the effectiveness of our KBC algorithms,and the correctness of our automatic evaluationmethod, we perform manual evaluation on the top100 predictions of the output obtained from two dif-ferent experimental setting and the results are shownin Table 3.
Even though the automatic evalua-tion gives pessimistic results since the test KB isalso incomplete6, the results indicate that the auto-matic evaluation is correlated with manual evalua-tion.
More excitingly, among the 179 unique in-stances we manually evaluated, 17 of them are still7missing in Freebase which emphasizes the effective-ness of our approach.6This is true even with existing automatic evaluation meth-ods.7at submission time.522Features G@100 G@100-M Accuracy-MD + W 87.68 97.31 97T + D + W 84.91 91.47 88Table 3: Manual vs. Automatic evaluation of top 100 pre-dictions on 70 types.
Predictions are obtained by train-ing a linear classifier using Adagrad with global trainingobjective (m=1, n=1).
G@100-M and Accuracy-M arecomputed by manual evaluation.5.4 Error Analysis?
Effect of training data: We find the perfor-mance of the models on a type is highly de-pendent on the number of training instances forthat type.
For example, the linear classifiermodel when evaluated on 70 types performs24.86 % better on the most frequent 35 typescompared to the least frequent 35 types.
Thisindicates bootstrapping or active learning tech-niques can be profitably used to provide moresupervision for the methods.
In this case, G@kwould be an useful metric to compare the effec-tiveness of the different methods.?
Shallow Linguistic features: We found someof the false positive predictions are caused bythe use of shallow linguistic features.
For ex-ample, an entity who has acted in a movie andcomposes music only for television shows iswrongly tagged with the type /film/composersince words like ?movie?, ?composer?
and?music?
occur frequently in the Wikipedia arti-cle of the entity (http://en.wikipedia.org/wiki/J._J._Abrams).6 Related WorkEntity Type Prediction and Wikipedia FeaturesMuch of previous work (Pantel et al, 2012; Lingand Weld, 2012) in entity type prediction has fo-cused on the task of predicting entity types at thesentence level.
Yao et al (2013) develop a methodbased on matrix factorization for entity type predic-tion in a KB using information within the KB andNew York Times articles.
However, the method wasstill evaluated only at the sentence level.
Toral andMunoz (2006), Kazama and Torisawa (2007) use thefirst line of an entity?s Wikipedia article to performnamed entity recognition on three entity types.Knowledge Base Completion Much of preciouswork in KB completion has focused on the problemof relation extraction.
Majority of the methods infermissing relation facts using information within theKB (Nickel et al, 2011; Lao et al, 2011; Socher etal., 2013; Bordes et al, 2013) while methods suchas Mintz et al (2009) use information in text doc-uments.
Riedel et al (2013) use both informationwithin and outside the KB to complete the KB.Linear EmbeddingModel Weston et al (2011) isone of first work that developed a supervised linearembedding model and applied it to image retrieval.We apply this model to entity type prediction butwe train using a different objective function whichis more suited for our task.7 Conclusion and Future WorkWe propose an evaluation framework comprisingof methods for dataset construction and evaluationmetrics to evaluate KBC approaches for inferringmissing entity type instances.
We verified that ourautomatic evaluation is correlated with human eval-uation, and our dataset and evaluation scripts arepublicly available.8Experimental results show thatmodels trained with our proposed global training ob-jective produces higher quality ranking within andacross types when compared to baseline methods.In future work, we plan to use information fromentity linked documents to improve performanceand also explore active leaning, and other human-in-the-loop methods to get more training data.ReferencesJonathan Berant, Vivek Srikumar, Pei-Chun Chen,Abby Vander Linden, Brittany Harding, Brad Huang,and Christopher D. Manning.
2013.
Semantic parsingon freebase from question-answer pairs.
In EmpiricalMethods in Natural Language Processing.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a collabo-ratively created graph database for structuring humanknowledge.
In Proceedings of the ACM SIGMOD In-ternational Conference on Management of Data.8http://research.microsoft.com/en-US/downloads/df481862-65cc-4b05-886c-acc181ad07bb/default.aspx523Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran,Jason Weston, and Oksana Yakhnenko.
2013.
Trans-lating embeddings for modeling multi-relational data.In Advances in Neural Information Processing Sys-tems.Antoine Bordes, Sumit Chopra, and Jason Weston.
2014.Question answering with subgraph embeddings.
InEmpirical Methods in Natural Language Processing.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka, and A.
2010.
Towardan architecture for never-ending language learning.
InIn AAAI.Kai-Wei Chang, Wen tau Yih, Bishan Yang, and Christo-pher Meek.
2014.
Typed tensor decomposition ofknowledge bases for relation extraction.
In Proceed-ings of the 2014 Conference on Empirical Methods inNatural Language Processing.Silviu Cucerzan and David Yarowsky.
1999.
Lan-guage indep endent named entity recognition combin-ing morphological and contextual evidence.
In ointSIGDAT Conference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
In Journal of Machine Learn-ing Research.Yuan Fang and Ming-Wei Chang.
2014.
Entity link-ing on microblogs with spatial and temporal signals.In Transactions of the Association for ComputationalLinguistics.Stephen Guo, Ming-Wei Chang, and Emre Kiciman.2013.
To link or not to link?
a study on end-to-endtweet entity linking.
In The North American Chap-ter of the Association for Computational Linguistics.,June.Hannaneh Hajishirzi, Leila Zilles, Daniel S. Weld, andLuke Zettlemoyer.
2013.
Joint coreference resolutionand named-entity linking with multi-pass sieves.
InEmpirical Methods in Natural Language Processing.Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. SathiyaKeerthi, and S. Sundararajan.
2008.
A dual coordinatedescent method for large-scale linear svm.
In Interna-tional Conference on Machine Learning.Jun?ichi Kazama and Kentaro Torisawa.
2007.
Exploit-ing wikipedia as external knowledge for named entityrecognition.
In Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke.Zettlemoyer.
2013.
Scaling semantic parsers with on-the-fly ontology matching.
In Empirical Methods inNatural Language Processing.Ni Lao, Tom Mitchell, and William W. Cohen.
2011.Random walk inference and learning in a large scaleknowledge base.
In Conference on Empirical Methodsin Natural Language Processing.David D. Lewis and William A. Gale.
1994.
A sequentialalgorithm for training text classifiers.
In ACM SIGIRConference on Research and Development in Informa-tion Retrieval.Xiao Ling and Daniel S. Weld.
2012.
Fine-grained entityrecognition.
In Association for the Advancement ofArtificial Intelligence.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Sch?utze.
2008.
Introduction to information re-trieval.
In Cambridge University Press, Cambridge,UK.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.2009.
Distant supervision for relation extraction with-out labeled data.
In Association for ComputationalLinguistics and International Joint Conference on Nat-ural Language Processing.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2011.
A three-way model for collectivelearning on multi-relational data.
In InternationalConference on Machine Learning.Patrick Pantel, Thomas Lin, and Michael Gamon.
2012.Mining entity types from query logs via user intentmodeling.
In Association for Computational Linguis-tics.Lev Ratinov and Dan Roth.
2012.
Learning-based multi-sieve co-reference resolution with knowledge.
In JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M. Marlin.
2013.
Relation extraction withmatrix factorization and universal schemas.
In TheNorth American Chapter of the Association for Com-putational Linguistics.Richard Socher, Danqi Chen, Christopher Manning, andAndrew Y. Ng.
2013.
Reasoning with neural ten-sor networks for knowledge base completion.
In Ad-vances in Neural Information Processing Systems.Antonio Toral and Rafael Munoz.
2006.
A proposal toautomatically build and maintain gazetteers for namedentity recognition by using wikipedia.
In EuropeanChapter of the Association for Computational Linguis-tics.Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shao-hua Sun, Rahul Gupta, and Dekang Lin.
2014.Knowledge base completion via search-based questionanswering.
In Proceedings of the 23rd internationalconference on World wide web, pages 515?526.
Inter-national World Wide Web Conferences Steering Com-mittee.524Jason Weston, Samy Bengio, and Nicolas Usunier.
2011.Wsabie: Scaling up to large vocabulary image anno-tation.
In International Joint Conference on ArtificialIntelligence.Xuchen Yao and Benjamin Van Durme.
2014.
Informa-tion extraction over structured data: Question answer-ing with freebase.
In Association for ComputationalLinguistics.Limin Yao, Sebastian Riedel, and Andrew McCallum.2013.
Universal schema for entity type prediction.In Proceedings of the 2013 Workshop on AutomatedKnowledge Base Construction.525
