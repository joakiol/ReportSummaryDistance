NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 64?80,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsThe PASCAL Challenge on Grammar InductionDouwe Gelling and Trevor CohnDepartment of Computer ScienceUniversity of Sheffield, UK{d.gelling,t.cohn}@sheffield.ac.ukPhil BlunsomDepartment of Computer ScienceUniversity of Oxford, UKPhil.Blunsom@cs.ox.ac.ukJoa?o Grac?aL2F Spoken Language Systems LaboratoryINESC ID Lisboa, Portugaljoao.graca@l2f.inesc-id.ptAbstractThis paper presents the results of the PASCALChallenge on Grammar Induction, a compe-tition in which competitors sought to predictpart-of-speech and dependency syntax fromtext.
Although many previous competitionshave featured dependency grammars or parts-of-speech, these were invariably framed assupervised learning and/or domain adaption.This is the first challenge to evaluate unsuper-vised induction systems, a sub-field of syntaxwhich is rapidly becoming very popular.
Ourchallenge made use of a 10 different treebanksannotated in a range of different linguistic for-malisms and covering 9 languages.
We pro-vide an overview of the approaches taken bythe participants, and evaluate their results oneach dataset using a range of different evalua-tion metrics.1 IntroductionInducing grammatical structure from text has longbeen fundamental problem in Computational Lin-guistics and Natural Language Processing.
In re-cent years interest has grown, spurred by advancesin unsupervised statistical modelling and machinelearning.
The task has relevance to cognitive scien-tists and linguists attempting to gauge the learnabil-ity of natural language by human children, and alsonatural language processing researchers who seeksyntactic representations for languages with few lin-guistic resources.Grammar learning has been popular in previouschallenges.
For example the CoNLL shared tasks in2006 and 2007 (Buchholz and Marsi, 2006; Nivreet al, 2007) involved supervised learning of de-pendency parsers across a wide range of differentlanguages.
Our challenge has many similarities tothese, in that we focus on dependency grammars,however we seek to evaluate unsupervised algo-rithms only using syntactically annotated data forevaluation and not for training.
Additionally we alsoconsider the related task of part-of-speech (POS) in-duction, and the next logical challenge: the jointtask of POS and dependency induction.
Other re-lated challenges can be found in the formal gram-mar community (e.g., the Omphalos1 competition)in which competitors seek to learn synthetic lan-guages.
In contrast we seek to model natural lan-guage text, which entails many different challenges.Research into unsupervised grammar and POS in-duction holds considerable promise, although cur-rent approaches are still a long way from solvingthe general problem.
For example, the majority ofrecent research into dependency grammar inductionhas adopted the evaluation setting of Klein and Man-ning (2004) who learn grammars on strings of POStags, rather than on words themselves.
One aim ofthis challenge is to popularise the more difficult andambitious task of inducing grammars directly fromtext, which can be viewed as integrating the POS andgrammar induction tasks.
A second aim is to fostergrammar and POS induction research across a widervariety of languages, and improving the standard ofevaluation.We have collated data from existing treebanks ina variety of different languages, domains and lin-guistic formalisms.
This gives a diverse range of1See http://www.irisa.fr/Omphalos64data upon which to test induction algorithms, yield-ing a deeper insight into their strengths and short-comings.
One key problem in grammar inductionresearch is how to evaluate the models?
predictionsgiven that often many different analyses are linguis-tically plausible, e.g., the choice of whether deter-miners or nouns should head noun phrases, or how torepresent coordination.
Simply comparing against asingle gold standard often results in poor reportedperformance because the model has discovered adifferent analysis to that used when annotating thetreebank.
For this reason it has been popular to uselenient measures for comparing predicted trees tothe treebank gold standard trees, such as undirectedaccuracy and the neutral edge distance (Schwartz etal., 2011).
As well as evaluating using these popularmetrics, we also propose a new method of evaluationwhich is also lenient in that it rewards different typesof linguistically plausible output, but requires con-sistency in the output, something the previous meth-ods cannot do.The paper is organised as follows.
Section 2 de-scribes the tasks and our data format and section 3outlines the different treebanks used for the chal-lenge.
The baselines, our own benchmark systemsand the competitors entries are described in section5.
In section 6 we present and analyse the resultsfor the three different tracks.
Finally we conclude insection 7.2 Task DefinitionThe three tracks of the WILS challenge are de-scribed below.
First we describe the data format forthe submissions common to the three tracks (POSinduction, Dependency induction, and jointly induc-ing both), and then the three tracks are describedalong with the respective evaluation metrics.2.1 Data formatAll datasets were presented in a file format similar tothat used in the CoNLL tasks, but with slight mod-ifications.
In particular the last two columns are re-moved, as no projective head or projective depen-dency relations were used, and an extra POS columnwas inserted at column 6 to accommodate the Uni-versal POS tagset (Petrov et al, 2011).
Each line in afile then either consists of 9 columns, separated by atab character, or is an empty line.
Empty lines sepa-rate sentences, and all other lines give the annotationfor a single token in the sentence as follows:1.
ID: Token counter, gives the index of currentword in the sentence.
Indexing starts at 1.2.
FORM: Surface form of the token in the sen-tence.3.
LEMMA: Stemmed form of the word form ifavailable.4.
CPOSTAG: Coarse-grained POS tag.5.
POSTAG: Fine-grained POS tag, or CPOSTAGagain if not available.6.
UPOSTAG: Universal POS tag, based on thePOSTAG and CPOSTAG.7.
FEATS: List of syntactic / morphological fea-tures, separated by a vertical pipe (|).8.
HEAD: Syntactic head of the token, with 0 in-dicating the root node.9.
DEPREL: The general type of the dependencyrelation, e.g., subject.In this setup, the LEMMA, FEATS and DEPRELcolumns are optional, in which case an underscore( ) will be used as a placeholder.
Each treebankwas split into training, development and testing par-titions.
The HEAD and DEPREL entries were onlysupplied for the development and the final testingsets,2 but not for the training partition.
The com-petitors were encouraged to develop their unsuper-vised entries on the union of the three partitions, andmake sparse use of the development set, i.e., for san-ity checking more than model fitting in order to min-imise the extent of supervision.2.2 POS inductionIn the POS induction track, participants developedsystems to induce the Part-of-Speech (POS) classesfor each word in the testing corpus.
In order to trainthe systems, the same training and development setswere used as for the other tracks.
These corpora in-cluded manually supplied POS tags for each token,2For the initial test set these fields were omitted.65which were not to be used for training, only evalua-tion.
Participants submitted predicted tags for eachtoken, which were scored against the gold-standard.For evaluation, we used 4 different metrics.
Thefirst is the many-to-one metric (M-1) (also knownas cluster purity), which is widely used for clusterevaluation as well as evaluation of POS induction.This metric assigns each word cluster to its mostcommon tag, and then measures the proportion ofcorrectly tagged words.
The second metric is theone-to-one mapping (1-1), a constrained version ofMany-to-one mapping in which each predicted tagis associated with only one gold-standard tag andvice versa (Haghighi and Klein, 2006).
Word clus-ters are assigned greedily to tags, and in the eventof there being more word classes than tags, someword classes will be left unassigned.
Another met-ric that was used is Variation of information (VI)(Meila, 2003), which is based the conditional en-tropy of between the two different clusterings (John-son, 2007).
Lastly, we use the V-measure (VM) met-ric (Rosenberg and Hirschberg, 2007), which is an-other entropy-based measure, but defined in terms ofa F score to balance precision and recall terms (weuse equal weighting of the two factors).
Please seeChristodoulopoulos et al (2010) for further detailsabout these metrics.3 For these metrics, a higherscore is better, with the exception of VI.For all these metrics, the induced tags are eval-uated against the universal pos tags, as this meansthere are a consistent number of tags across the lan-guages.
Using these metrics, the results will vary asa result of predicting a different number of tags (inparticular, more tags will mean a higher score for M-1, and the converse is true for 1-1).
However, usingthe universal POS tags, we think will make resultsless sensitive to large differences in POS inventorybetween languages (such as for the Dutch dataset).2.3 Dependency inductionFor the Dependency induction track, the trainingdata consisted of the original treebank data, butwithout dependency annotations.
A development setwas also provided, which included the dependencyannotations, but this was meant mainly as a way to3Thanks to Christos Christodoulopoulos for sharing his im-plementation of the POS induction metrics, which we have usedin our evaluation.verify systems, as we mean to minimise the amountof supervision in the task.
The participants werelater supplied with test sets for which the systemscould generate predictions.
Only after the predic-tions were submitted were the fully annotated testsets released.The dependency inductions were evaluated on3 metrics: directed accuracy, undirected accuracyand Neutral Edge Detection (NED) (Schwartz etal., 2011).
Directed accuracy is the ratio of cor-rectly predicted dependencies (including direction)over total amount of predicted dependencies.
Undi-rected accuracy is much the same, but also considersa predicted dependency correct if the direction of thedependency is reversed (e.g.
if the predicted depen-dency is not A ?
B, but B ?
A).
Lastly, the NEDmetric is a variant of undirected accuracy that alsorewards cases where an edge-flip occurs, meaningthat the predicted parent of a token is actually thegrandparent of that same token in the gold-standarddata.
Note that before evaluating with these metricspunctuation was removed from all sentences, andany child words under a punctuation node were re-attached to their nearest ancestor that wasn?t punc-tuation.The final ?joint?
task consisted of inducing depen-dency structure from only the tokens in the corpus,without recourse to the gold POS tags.
Where POSis predicted (e.g., in a pipeline), we included thesein our general POS evaluation.
The induced depen-dency trees were evaluated with the same metrics asin the dependency induction track, but are consid-ered separately.
We expect these systems to havelower scores overall due to the lack of gold-standardPOS tags.3 TreebanksWe selected a number of different treebanks for usein the challenge, aiming to represent a wide rangeof different languages, dialects and genres of text.In total we used ten different treebanked corporain nine different languages.
For the practical rea-sons of simplifying the administration of the chal-lenge and allowing the data to be reused in future re-search, we chose corpora with licences allowing ei-ther free redistribution, or those held by the Linguis-66tic Data Consortium (LDC).4 Many of these datasetshave been used before in dependency grammar orpart-of-speech research, particularly the shared tasksat CoNLL 2006 and 2007.
For the purpose ofthe competition, we have updated these datasets toinclude any annotation updates or additional data,where available.
It is important for unsupervised ap-proaches to have sufficient amounts of data, espe-cially given the common sentence length limitationsimposed by most dependency grammar models.
Asdescribed in section 2, we have included an extrafield for the universal part-of-speech (UPOS) usingPetrov et al (2011)?s automatic conversion tool.5Below we describe the different treebanks used,and the conversion process into our data format forthe purpose of the competition.
Please see Table 1for statistics on each of the treebanks.Dependency treebanks We used the followingdependency treebanks: Arabic The Prague Ara-bic Dependency Treebank V1 (Hajic?
et al, 2004).6Basque The Basque 3lb dependency treebank(Aduriz et al, 2003).
Czech The Prague Depen-dency Treebank 2.0 (Bo?hmova?
et al, 2001).7 Dan-ish The Copenhagen Dependency Treebank ver-sion 2 (Buch-Kromann et al, 2007).
English TheCHILDES US/Brown subcorpus (Sagae et al,2007).
Slovene The jos500k Treebank (Erjavec etal., 2010).
8 Swedish The Talbanken treebank(Nivre et al, 2006).
The conversion of each of thesetreebanks was quite straightforward as they were al-ready annotated for dependencies.
Moreover, manyof these corpora had been used previously in theCoNLL 2006 and 2007 shared tasks, and thereforewe were able to reuse this data and/or their conver-sion scripts.
In the case of Arabic and Swedish weused the exact same data, simply converting fromCoNLL dependency format into our own format (re-moving redundant columns and adding a UPOS col-umn).
While many of the other corpora had also4In the following corpus descriptions, when not otherwisespecified the corpus is freely available for research purposes.5http://code.google.com/p/universal-pos-tags6LDC catalogue number LDC2004T23.7LDC catalogue number LDC2006T01.8For the shared task, the annotation was converted to englishusing the tables found at the JOS website: http://nl.ijs.si/jos/msd/html-en/index.htmlbeen used previously, our data is different, makinguse of subsequent corrections to these treebanks andadditional annotated data now available.First language acquisition provides an importantmotivation for grammar induction research, conse-quently we have included data from the CHILDESdatabase of child-directed speech.
We use theBrown sub-corpus, a longitudinal study of parent-child interactions for three children aged between 18months and 5 years old.
The corpus has been man-ually annotated with syntactic dependencies (Sagaeet al, 2007) and morphology.
From this we take allchild-directed utterances, extracting word, morphol-ogy, part-of-speech and dependency markup, anddeveloped our own conversion into UPOS.
Our test-ing and development sets were drawn from the first15 Eve files which were manually annotated for de-pendency structure.
The rest of the corpus, whichhad not been manually annotated for syntax, wasmerged to form the training set.Phrase-structure treebanks As well as depen-dency treebanks, we used three different phrase-structure treebanks: The Dutch Alpino treebank(Bouma et al, 2000), the English Penn TreebankV3 (Marcus et al, 1993),9 and the Portuguese Flo-resta Sinta?
(c)tica treebank (Afonso et al, 2002).
Asthese treebanks do not explicitly mark dependen-cies, we automatically extracted these using headfinding heuristics.
Thankfully the difficult workof creating such scripts has already been done aspart of the CoNLL shared tasks.
We have reusedtheir scripts to create dependency representations ofthese treebanks, before converting into our file for-mat and augmenting with UPOS annotation.
In thecase of Dutch, we have reused the same CoNLL2006 data; note that this dataset includes predictedpart-of-speech rather than gold standard annotation(Buchholz and Marsi, 2006).
For the Portuguese,we used the same Bosque 7.3 sub-corpus10 fromCoNLL 2006, additionally including in our trainingset the recently-annotated Selva 1.0 subcorpus.The Penn Treebank is the most common data setin parsing and grammar induction.
We have patched9LDC catalogue number LDC99T42.10An updated version of this corpus is available, howeverthe file format had changed significantly and we were unableto adapt the conversion scripts in time for the competition.67ar cs da en-childes en-ptb eu nl pt sl svannotation d d d d p d p p d dTraining dataTokens 106.6k 1.2M 68.5k 312.8k 1.1M 124.7k 192.2k 196.4k 193k 184.6kSentences 2.8k 68.5k 3.6k 57.4k 45.4k 9.1k 13k 8.7k 9.4k 10.7kTokens/sent 38.4 17.1 18.8 5.5 23.9 13.7 14.8 22.6 20.5 17.3CPOSTAG 15 12 25 31 31 16 13 16 13 41POSTAG 21 61 141 76 45 50 300 22 31 41FEATS 22 75 338 29 0 269 310 146 46 0Development dataTokens 5.1k 159k 17k 25.3k 32.9k 12.6k 2.9k 10.3k 20.2k 6.9kSentences 139 9.3k 1k 5k 1.3k 1k 386 400 1k 389Tokens/sent 36.8 17.1 17 5.1 24.4 12.5 7.4 25.8 20.2 17.6% New words 27.5 26 49.8 9.8 11.4 46.1 18.8 27.5 38.7 13.8Test dataTokens 5.1k 173.6k 14.7k 28.4k 56.7k 14.3k 5.6k 5.9k 22.6k 5.7kSentences 131 10.1k 1k 5.2k 2.4k 1.1k 386 288 1k 389Tokens/sent 39.1 17.1 14.7 5.4 23.5 12.7 14.5 20.4 22.6 14.5% New words 24.3 25.3 43.7 9 12.1 51.5 40.5 25.2 37.1 34.6Table 1: Properties of the treebanks.
We report the linguistic annotation method (dependency vs. phrase-structure),the size of each treebank, the number of types for the different granularities of part-of-speech tags and morphologicalfeatures (note that UPOS has a fixed set of 12 tags), and the proportion of word types that were not present in training.the treebank to include NP-internal structure usingVadas and Curran?s annotations (Vadas and Cur-ran, 2007), which was then converted to dependencystructures using the penn-converter11 script(Johansson and Nugues, 2007).
This tool has a num-ber of options controlling the linguistic decisionsin converting from phrase-structure to dependencytrees, e.g., the treatment of coordination.
We ex-tracted five versions of the treebank, each encodingeach different sets of linguistic assumptions (Tsar-faty et al, 2011).12 These are denoted default, old-LTH, CoNLL-2007, functional and lexical; for themain results we used the standard options, we alsoreport separately evaluations using each of the fivevariants.
The treebank was partitioned into training(sections 0-22), development (sec.
24) and testingsets (sec.
23).4 Baselines and BenchmarksA number of standard baselines and previously pub-lished benchmark systems were implemented foreach task in order to place the submitted systems incontext.11http://nlp.cs.lth.se/software/treebank_converter12Note that Tsarfaty et al (2011) also propose an evalua-tion metric for comparing dependency trees, which we have notused.
Note however that it could, in principle, be used for simi-lar evaluations.The standard baseline for grammar inductionmodels is to assume either left branching or rightbranching analyses (LB, RB).
These capture the ten-dency for languages to favour one attachment direc-tion over another.
The most frequently cited andextended model for dependency induction is DMV(Klein and Manning, 2004).
We provide results forthis model trained on each of the coarse (DMVc), fine(DMVp), and universal (DMVu) POS tag sets, all ini-tialised with the original harmonic initialiser.
As afurther baseline we also evaluated the dependencytrees resulting from directly using the harmonic ini-tialiser without any training (H).As a strong benchmark we include the results ofthe non-parametric Bayesian model previously pub-lished in Blunsom and Cohn (2010) (BC).
The statedresults are for the unlexicalised model described inthat paper where the final analysis is formed bychoosing the maximum marginal probability depen-dency links estimated from forty independent Gibbssampler runs.For part-of-speech tagging we include resultsfrom an implementation of the Brown word clus-tering algorithm (Brown et al, 1992) (Bc,p,u), andthe mkcls tool written by Franz Och (Och, 1999)(MKc,p,u).
Both of these benchmarks were trainedwith the number of classes matching the numberin the gold standard of each of the tagsets in turn:coarse (c), fine (p), and universal (u).
A notable68property of both of these word class models is thatthey enforce a one-tag-per-type restriction that en-sures there is a one-to-one mapping between wordtypes and classes.For POS tagging we also provide benchmark re-sults from two previously published models.
Thefirst of these is the Pitman-Yor HMM model de-scribed in (Blunsom and Cohn, 2011), which in-corporates ta one-tag-per-type restriction (BC).
Thismodel was trained with the same number of tags asin the gold standard fine tag set for each corpus.
Thesecond benchmark is the HMM with Sparsity Con-straints trained using Posterior Regularization (PR)described in (Grac?a et al, 2011).
In this modelthe HMM emission probabilitiy distribution are esti-mated using small Maximum Entropy models (fea-tures set described in the original paper).
The mod-els were trained for 200 iterations of PR using boththe same number of hidden states as the coarse Gcand universal Gu gold standard.
All parameters wereset to the values described in the original paper.5 SubmissionsThe shared task received submissions covering a di-verse range of approaches to the dependency andpart-of-speech induction challenges.
Encouraginglyall of these submissions made significant departuresfrom the benchmark HMM and DMV approacheswhich have dominated the published literature onthese tasks in recent years.
The submissions werecharacterised by varied choices of model structure,parameterisation, regularisation, and the degree towhich light supervision was provided through con-straints or the use of labelled tuning data.
In the fol-lowing sections we summarise the approaches takenby the systems submitted for each task.5.1 Part-of-Speech InductionThe part-of-speech induction challenge received twosubmission, (Chrupa?a, 2012; Christodoulopoulos etal., 2012).
Both of these submissions based their in-duction systems on LDA inspired models for cluster-ing word types by the contexts in which they appear.Notably, the strongest of the provided benchmarksand the two submissions modelled part-of-speechtags at the type level, thus restricting all tokens ofa given word type to share the same tag.
Thoughclearly out of step with the gold standard tagging,this one-tag-per-type restriction has previously beenshown to be a crude but effective way of regularisingmodels towards a good solution.
Below we sum-marise the approach of each submission, identifiedby the surname of the first author on the submittedsystem description.Chrupa?a (2012) employed a two stage approachto inducing part-of-speech tags.
The first stage usedan LDA style probabilistic model to induce a dis-tribution over possible tags for a given word type.These distributions were then hierarchically clus-tered and the final tags selected using the prefix ofthe path from the root node to the word type in thecluster tree.
The length of the prefixes, and thus thenumber of tags, was tuned on the labelled develop-ment data.The system of Christodoulopoulos et al (2012)was based upon an LDA type model which includedboth contexts and other conditionally independentfeatures (Christodoulopoulos et al, 2011).
This basesystem was then iterated with a DMV system andwith the resultant dependencies being repeatedly fedback into the POS model as features.
This submis-sion is notable for being one of the first to attemptjoint POS and dependency induction rather than tak-ing a pipeline approach.5.2 Dependency InductionThe dependency parsing task saw a variety of ap-proaches with only a couple based on the previouslydominant DMV system.
Two forms of light super-vision were popular, the first being the inclusion ofpre-specified constraints or rules for allowable de-pendency links, and the second being the tuning ofmodel parameters or selecting between competingmodels on the labelled development data.
Obviouslythe merits of such supervision would depend on thedesired application for the induced parser.
The di-rect comparison of models which include a form ofuniversal prior syntactic information with those thatdon?t does permit interesting development linguisticquestions to be explored in future.Bisk and Hockenmaier (2012) chose to induce arestricted form of Combinatory Categorial Grammar(CCG), the parses of which were then mapped todependency structures.
Restrictions on head-childdependencies were encoded in the allowable cate-69gories for each POS tag and the heads of sentences.Key features of their approach were a maximumlikelihood objective function and an iterative proce-dure for generating composite categories from sim-ple ones.
Such composite categories allow the pa-rameterisation of larger units than just head-childdependencies, improving over the more limited con-ditioning of DMV.Marac?ek and Z?abokrtsky?
(2012) introduced anumber of novel features in their dependency induc-tion submission.
Wikipedia articles were used toquantify the reducibility of word types, the degreeto which the word could be removed from a sen-tence and grammaticality maintained.
This metricwas then used, along with a model of child fertil-ity and dependency distance, within a probabilisticmodel.
Inference was performed by using a localGibbs sampler to approximate the marginal distribu-tion over head-child links.S?gaard (2012) presented two model-free heuris-tic algorithms.
The first was based on heuristicallyadding dependency edges based on rules such as ad-jacency, function words, and morphology.
The re-sulting structure is then run through a PageRank al-gorithm and another heuristic is used to select a treefrom the resulting ranked dependency edges.
Thesecond approach takes the universal rules of Naseemet al (2010) but rather than estimating a probabilis-tic model with these rules, a rule based heuristic isused to select a parse rather.
This second model-freeapproach in particular provides a strong baseline forprobabilistic models built upon hand-specified de-pendency rules.Tu (2012) described a system based on an ex-tended DMV model.
Their work focussed on theexploration of multiple forms of regularisation, in-cluding Dirichlet priors and posterior regularisation,to favour both sparse conditional distributions andlow ambiguity in the induced parse charts.
Whilemany previous works have included sparse priorson the conditional head-child distributions the ad-ditional regularisation of the ambiguity over parsetrees is a novel and interesting addition.
The la-belled development sets were employed to both se-lect between models employing different regularisa-tion, and to tune model parameters.5.3 POS and Dependency InductionThere was only a single submission for the task ofinducing dependencies without gold standard part-of-speech tags supplied.
Christodoulopoulos et al(2012) submitted the same joint tagging and DMVsystem used for the POS induction task to the depen-dency induction task.
Results on the developmentdata indicated that this iterated joint training had asignificant benefit for the induced tags and a smallerbenefit for the dependency structures induced.6 ResultsThe main results for the three tasks are shown in Ta-bles 2, 3, and 4, for the POS induction, dependencyinduction and joint tasks, respectively.13 We nowpresent a detailed analysis of each of the three tasks.6.1 POS inductionThe main evaluation results for the POS induc-tion task are shown in Table 2, which comparesthe induced clusters against the gold universal tags(UPOS).14 Given the diversity of scenarions used byeach system (e.g.
number of hidden states, tuningon development data) a direct comparison betweenthe systems can only be illustrative.
A first obser-vation is that depending on the particular evaluationmetric employed the ranking of the systems changessubstantially, for instance the Gu system is the bestusing the 1-1 and VI metric but is the worst of the en-tries (excepting the baselines) when using the othertwo metrics.
Focusing on the VM metric, whichwas shown empirically not to have low bias with re-spect to the word classes (Christodoulopoulos et al,2010), the best entry is the BC system which has thebest performance in 9 out of 10 entries followed bythe CGS and the C system.
Note that this rankingholds also for the comparison against fine POS tags,shown in Table 7.An interesting aspect is that almost all systemsbeat the strong Brown (B) and mkcls (MK) base-line across the different metrics when we restrictour attention to the cases where the same number13Additional tables of results are in the appendix, and fur-ther results are online at http://wiki.cs.ox.ac.uk/InducingLinguisticStructure.14See also Table 7 for the comparison against the fine POStags; we base our analysis on UPOS instead as this tag set has afixed size irrespective of the treebank.70M-1testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKuarabic 83.80 N/A 83.33 83.33 65.05 66.61 66.20 69.89 66.22 71.08 72.72 68.03basque 80.85 79.54 86.67 86.67 77.37 73.88 74.73 77.49 73.32 74.80 78.63 71.40czech 83.10 66.78 72.27 77.97 N/A N/A 60.85 75.57 60.42 65.43 79.35 57.16danish 81.44 77.76 84.13 84.92 68.16 53.78 72.12 79.77 47.09 72.26 82.59 53.07dutch 80.75 70.13 74.04 76.11 63.37 57.64 57.99 84.17 57.31 68.18 84.78 63.04en-childes 90.36 85.42 91.50 91.50 N/A N/A 82.65 89.70 70.12 86.27 91.44 75.63en-ptb 86.73 81.93 78.11 84.35 77.14 71.10 77.29 80.88 63.74 79.99 83.88 63.34portuguese 81.69 77.38 80.38 81.90 75.54 74.35 70.07 74.25 67.60 70.79 72.90 68.08slovene 70.81 65.31 75.53 75.92 67.94 59.96 61.58 68.93 58.32 58.43 65.69 50.36swedish 78.61 80.45 79.60 79.60 69.91 58.79 71.69 71.69 57.55 76.45 76.45 57.30averages 81.82 76.08 80.56 82.23 70.56 64.51 69.52 77.23 62.17 72.37 78.84 62.741-1testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKuarabic 53.67 N/A 39.44 39.44 39.83 55.52 40.55 33.57 43.31 51.54 40.24 51.58basque 36.10 36.03 47.15 47.15 47.09 54.70 32.61 20.53 40.62 34.80 27.28 37.65czech 31.82 49.30 30.49 27.20 N/A N/A 46.19 26.66 45.10 43.70 24.48 39.25danish 42.54 42.77 31.67 31.04 39.95 45.58 36.04 17.74 39.19 43.89 22.18 44.23dutch 42.79 56.15 43.10 39.62 56.45 45.37 48.18 21.36 43.12 55.99 21.32 54.09en-childes 38.79 42.57 43.76 43.76 N/A N/A 40.78 35.54 57.71 43.45 32.00 59.18en-ptb 41.55 39.57 43.86 31.56 42.07 51.70 39.79 33.90 46.50 40.55 36.22 51.17portuguese 59.66 47.45 35.90 35.50 46.50 56.08 51.15 42.68 51.58 44.28 35.38 46.31slovene 39.02 53.04 33.18 32.50 50.90 48.50 46.83 40.16 42.28 40.34 39.32 40.58swedish 42.38 32.44 26.45 26.45 34.99 54.92 27.56 27.56 51.34 35.82 35.82 43.60averages 42.83 44.37 37.50 35.42 44.72 51.55 40.97 29.97 46.07 43.44 31.42 46.76VMtestset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKuarabic 61.75 N/A 51.27 51.27 44.81 47.07 39.93 42.43 39.92 47.47 43.91 44.49basque 42.17 41.52 43.04 43.04 40.86 40.05 34.85 33.33 36.08 36.32 34.35 33.42czech 52.26 45.31 40.22 39.20 N/A N/A 38.56 42.90 37.46 41.70 46.03 37.34danish 56.57 54.63 52.46 52.32 47.26 41.96 47.89 44.37 35.13 50.52 48.17 39.96dutch 56.96 53.35 54.87 52.90 48.57 45.80 43.34 49.33 43.67 51.37 50.11 47.20en-childes 64.53 62.32 62.76 62.76 N/A N/A 58.87 60.31 57.06 62.76 60.92 60.51en-ptb 60.73 57.99 53.14 52.09 55.10 52.54 54.76 55.08 48.04 56.81 57.29 48.46portuguese 64.17 58.41 52.54 52.32 55.96 58.14 52.09 53.18 50.32 52.48 50.87 50.18slovene 51.15 51.29 46.60 46.50 50.98 45.98 44.49 45.80 38.61 36.79 43.43 36.43swedish 57.05 54.21 47.08 47.08 48.89 45.73 45.87 45.87 40.84 49.77 49.77 42.83averages 56.73 53.23 50.40 49.95 49.05 47.16 46.06 47.26 42.71 48.60 48.48 44.08VItestset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKuarabic 2.48 N/A 3.70 3.70 3.39 2.98 3.78 3.94 3.53 3.31 3.82 3.30basque 3.82 3.44 3.98 3.98 3.25 2.82 3.92 4.98 3.45 3.79 4.76 3.58czech 3.83 3.41 4.92 5.77 N/A N/A 3.70 4.76 3.69 3.63 4.53 3.83danish 3.36 3.34 4.31 4.38 3.78 3.46 3.86 5.43 3.79 3.64 4.90 3.59dutch 3.56 3.13 3.28 3.71 3.30 3.44 3.66 5.22 3.60 3.26 5.15 3.39en-childes 2.81 2.86 3.06 3.06 N/A N/A 3.13 3.34 2.59 2.84 3.33 2.50en-ptb 3.18 3.28 3.67 4.36 3.34 3.03 3.46 3.62 3.36 3.36 3.52 3.28portuguese 2.47 2.83 3.96 4.09 2.96 2.62 3.19 3.36 3.10 3.21 3.52 3.15slovene 3.62 3.14 4.80 4.86 3.16 3.30 3.61 4.09 3.73 4.15 4.33 3.99swedish 3.31 3.68 4.98 4.98 3.90 3.32 4.46 4.46 3.70 4.07 4.07 3.62averages 3.24 3.23 4.07 4.29 3.39 3.12 3.68 4.32 3.45 3.53 4.19 3.42Table 2: Results for the POS induction task, showing one-to-one, many-to-one, VM and VI scores, measured againstthe gold UPOS tags.
Each system is shown in a column, where the title is an acronym of the authors?
last names, orelse the name of a benchmark system (B is the Brown clusterer and MK is mkcls).
The superscripts c, p and u denotedifferent applications of the same method with a number of word classes set to equal the true number of coarse tags,full tags or universal tags, respectively, for each treebank.71of hidden states are used (the exception being the Gsystem which occasionally under-performed againstMK).
Interestingly the assumption of one-tag-per-word, made by all but the G system, works verywell in practice leading to consistently strong re-sults.
This suggests that dealing with word ambigu-ity is still an unresolved issue in unsupervised POSinduction.Comparing the performance of the systems fordifferent languages, as expected the languages forwhich we have a larger corpora (English CHILDESand PTB and Czech) tend to result in systems withbetter accuracies.
An interesting future question ishow do the propose methods scale when training onreally large corpora (e.g., wikipedia) both in termsof performance (accuracy) but also in the resourcesthey required.Finally, the wild divergences in the system rank-ings when considering the different evaluation met-rics calls for some sort of external evaluation usingthe induced clusters as features to other end sys-tems, for instance semi-supervised tagging.
Themain question is if there will be a definitive rankingbetween systems for a diverse set of tasks, or if onthe contrary the effectiveness of the output of eachsystem will vary according to the task at hand.6.2 Dependency inductionThe main evaluation results for the dependency taskare shown in Table 3.
From this we make severalobservations.15 Firstly, for almost all the corporathe participants systems have outperformed the sim-ple baselines, and by a significant margin.
Thereare three exceptions to this: for Arabic, Basque andDanish the left or right-branching baselines outper-forms most or all of the competitors.
This may in-dicate that these languages are inherently difficult,or may simply be a consequence of these three lan-guages having the least data of all of our corpora.Basque and Dutch proved to be the hardest of thetreebanks, with the lowest overall scores, and theCHILDES (English) and Portuguese were the eas-iest.
The reasons for this are not immediately clear,15Table 3 evaluates against the full test sets, however it istraditional to present results for short sentences mirroring thecommon training setup.
See Tables 8 and 9 for results oversentences with 10 words or fewer, excluding punctuation.
Notethat our analysis is based on the results for the full test set.although we speculate that Basque is difficult due toits dissimilarity from other European languages, andtherefore may not match the assumptions underly-ing models developed primarily on English.
Dutchis difficult as its annotation was non-projective, andit has a very large set of POS tags, while CHILDESis made easier due to its extremely short and simplesentences.In terms of declaring a ?winner?, it is clear thatTu?s system ranks best under directed accuracy andNED, and a very narrow second (to the organisers?submission, BC) for undirected accuracy.
MoreoverTu?s system was a consistent performed across allcorpora, with no single result well below the resultsof the other participants.
Note that the three differentmetrics often predict the same winner across the dif-ferent treebanks, however there are some large dis-crepancies, such as Portuguese and Dutch where thedirected and undirected accuracy metrics concur, butNED produces a very different ranking.
It is unclearwhich metric should be trusted more than another;this could only be assessed by correlating these met-rics with some form of secondary evaluation, suchas in a task based setting or obtaining human gram-maticality judgements.16The benchmark systems include DMV (Klein andManning, 2004), which has historical importancein terms of being the first research systems to out-perform simple baselines for dependency induction,and also the model upon which most recent depen-dency induction research is based, including manyof the competitors in the competition.
We ob-serve that in most cases the competitors have out-performed the DMV models, in many cases by alarge margin.
In all cases DMV improved overits initialisation condition (the harmonic initialiser),although often this improvement was only slight,underscoring the importance of good initialisation.The effect of inducing DMV grammars from var-ious different granularity of POS tags made littledifference in most cases, although for Dutch17 andthe English PTB there change was more dramatic.16It was our intention to include a task-based evaluation formachine translation, but this proved impractical for the compe-tition due to the volumes of data that we would require eachparticipant to process.17Note that for Dutch the full POS tags were not gold stan-dard, but were system predictions.72Directedtestset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RBarabic 61.1 47.2 15.7 64.8 64.8 47.2 54.6 66.7 46.3 45.4 50.0 42.6 9.3 64.8basque 56.3 50.3 28.0 30.3 27.2 33.3 22.3 58.6 46.3 43.2 31.3 21.8 34.3 24.4czech 50.0 48.5 61.3 57.5 57.7 45.5 51.2 59.0 30.1 31.2 31.8 24.7 28.9 34.3danish 46.2 49.3 60.2 51.3 61.4 56.9 60.5 60.8 47.2 50.2 35.3 36.4 18.7 49.2dutch 50.5 50.8 37.0 49.5 38.4 38.9 50.0 51.7 48.7 39.7 49.1 35.1 34.0 39.5en-childes 48.1 62.2 56.8 47.2 51.8 50.5 53.5 56.0 51.7 51.9 39.0 31.7 36.0 23.3en-ptb 72.1 73.7 58.9 67.4 52.2 44.8 61.0 74.7 31.7 44.7 30.6 35.2 40.4 19.9portuguese 54.3 76.3 63.6 59.9 44.3 47.7 71.1 55.7 27.1 37.2 26.9 31.1 28.1 37.7slovene 65.8 53.9 42.1 51.4 39.2 39.7 50.3 67.7 35.7 37.2 35.6 25.7 35.9 14.7swedish 65.8 66.7 61.4 63.7 70.8 48.2 72.0 76.5 44.2 44.2 45.8 39.1 33.2 31.3averages 57.0 57.9 48.5 54.3 50.8 45.3 54.7 62.7 40.9 42.5 37.5 32.3 29.9 33.9Undirectedtestset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RBarabic 57.3 29.7 57.6 62.0 58.7 48.0 58.4 59.3 41.8 42.0 43.7 41.2 61.7 63.9basque 58.0 47.2 43.3 45.0 43.2 47.5 24.3 53.3 48.1 47.7 40.3 37.6 53.9 53.1czech 59.0 45.0 57.8 54.3 55.5 49.3 55.8 61.4 46.2 46.7 45.3 38.5 51.5 52.3danish 60.8 50.7 60.7 56.1 60.3 56.6 60.5 61.6 55.1 54.1 51.6 46.0 58.7 59.9dutch 61.0 45.0 47.5 51.5 48.9 46.8 51.4 54.6 52.2 45.0 52.2 37.2 50.1 50.8en-childes 63.5 68.4 67.2 59.9 61.4 62.0 62.4 66.9 63.8 64.0 57.5 49.0 50.0 49.9en-ptb 66.2 58.1 49.7 57.6 48.2 49.5 58.8 62.1 43.1 53.1 43.0 36.2 51.7 51.5portuguese 56.6 72.4 61.4 61.9 49.8 52.6 66.9 61.4 44.3 48.1 43.6 41.2 55.7 56.8slovene 58.1 47.9 45.2 49.1 44.5 42.4 53.5 61.8 42.1 40.6 42.1 32.5 40.8 41.1swedish 70.0 58.5 58.8 59.3 60.4 53.5 65.2 66.9 51.1 51.1 53.3 44.5 53.0 53.2averages 61.0 52.3 54.9 55.7 53.1 50.8 55.7 60.9 48.8 49.2 47.3 40.4 52.7 53.2NEDtestset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RBarabic 63.6 37.3 59.2 67.1 63.2 56.5 65.8 64.1 48.9 48.0 48.8 47.5 62.7 69.0basque 69.6 55.8 51.5 55.6 53.4 58.8 38.0 65.8 57.6 57.1 51.5 49.3 67.2 59.1czech 71.0 55.7 70.2 65.2 67.3 63.2 69.7 71.6 53.2 52.9 54.1 47.6 56.3 68.6danish 72.0 63.1 72.9 69.5 73.5 65.9 71.8 76.4 64.8 63.5 58.9 53.5 61.6 71.5dutch 71.6 58.6 68.6 72.0 69.7 60.6 63.8 66.9 63.5 54.5 63.5 46.9 55.1 67.0en-childes 80.9 79.6 82.8 74.1 72.7 77.1 83.2 80.4 78.1 78.3 77.5 67.2 61.0 75.2en-ptb 75.2 69.8 69.4 73.8 67.2 64.1 71.6 69.8 49.8 67.0 49.6 44.8 53.9 68.1portuguese 67.5 79.8 75.6 75.7 71.7 66.9 78.2 80.4 62.1 66.6 61.3 51.8 57.3 75.4slovene 64.4 60.7 56.9 58.9 57.1 55.9 66.7 68.7 49.2 47.3 49.2 38.9 43.8 56.6swedish 80.1 70.9 73.2 73.8 72.7 66.7 77.0 77.1 64.0 64.0 62.0 56.0 56.5 71.0averages 71.6 63.1 68.0 68.6 66.9 63.6 68.6 72.1 59.1 59.9 57.6 50.3 57.6 68.1Table 3: Directed accuracy, undirected accuracy and NED results for the dependency task (using supplied POS).
Thefirst column (BC) is our benchmark system, the next seven are participants systems, and the remaining columns consistof the DMV benchmark and various simple baselines.
The superscripts c, p and u denote which type of POS was used,and S1 and S2 denote two different submissions for S?gaard (2012).Overall the full POS tagset lead to the best perfor-mance over the coarse and universal tags (consider-ing undirected accuracy or NED), which is to be ex-pected as there is considerably more syntactic infor-mation contained in the full POS.
This must be bal-anced against the additional model complexity fromexpanding its parameter space, which may explainwhy the difference in performance differences are sosmall.
The same pattern can also be seen in Marac?ekand Z?abokrtsky?
(2012)?s submission, whose systemusing full POS (Mp) outperformed their other vari-ants.6.3 Joint taskAs we had only one submission for the joint prob-lem of POS and dependency induction, there arefew conclusions we can draw for this joint task (seeTable 4 for the results, and Table 9 for the shortsentence evaluation).
Compared to the dependencyinduction task using gold standard POS, as shownin Table 3, the accuracy for the joint models arelower.
Interestingly, the DMV model performs bestwhen using the same number of word clusters asthere are POS tags, mirroring the findings reported73directedtestset CGS DMVc DMVp DMVuarabic N/A 35.3 44.4 34.2basque 24.5 27.5 25.1 28.7czech 24.7 19.9 33.2 20.0danish 21.4 23.3 31.9 10.0dutch 15.1 20.6 33.7 20.5en-childes 29.9 38.6 42.2 40.3en-ptb 21.5 22.5 23.3 17.2portuguese 19.7 28.5 28.0 17.1slovene 19.2 13.9 11.5 14.4swedish 23.6 26.4 26.4 20.5averages 22.2 25.7 30.0 22.3undirectedtestset CGS DMVc DMVp DMVuarabic N/A 45.5 52.5 45.0basque 43.5 46.4 47.3 47.0czech 38.9 37.5 50.9 38.5danish 51.4 52.2 48.8 37.3dutch 40.3 41.9 48.6 40.8en-childes 54.9 59.2 60.8 58.1en-ptb 43.4 45.4 48.8 39.4portuguese 45.5 51.8 52.7 39.8slovene 32.8 33.3 36.7 32.8swedish 45.6 48.9 48.9 40.3averages 44.0 46.2 49.6 41.9NEDtestset CGS DMVc DMVp DMVuarabic N/A 53.4 57.6 53.3basque 55.9 55.6 54.4 54.7czech 51.2 49.3 63.4 51.5danish 61.7 60.3 60.4 46.3dutch 47.2 57.5 56.8 55.2en-childes 78.2 77.7 78.1 76.5en-ptb 53.9 60.2 63.5 47.5portuguese 50.0 69.4 70.8 57.9slovene 40.7 38.7 47.5 40.3swedish 54.5 65.4 65.4 54.3averages 54.8 58.8 61.8 53.8Table 4: Directed, undirected and NED accuracy resultsfor evaluating the predicted dependency structures in thejoint task (i.e., not using supplied POS tags).
The firstcolumn is the participant?s system and the next three areDMV models trained on the Brown word clusters (seesection 6.1).above with gold standard tags.
The best joint sys-tem was the DMVp model, which only marginallyunder-performed the equivalent DMV model trainedon gold POS.
This is an encouraging finding, sug-gesting that word clusters are able to represent im-portant POS distinctions to inform deeper syntacticprocessing.6.4 AnalysisUntil now we have adopted the standard metrics independency evaluation: namely directed head at-tachment accuracy, and its more lenient counter-parts, undirected accuracy and NED.
The latter met-rics reward structures that almost match the goldstandard tree, by way of rewarding child-parentedges that are predicted in the reverse direction, i.e.,attaching the child as the parent (NED takes this fur-ther, by also rewarding the grandparent-child edgewhen this occurs).
This allows some degree of flexi-bility when considering various contentious linguis-tic decisions such as whether a preposition shouldhead a preposition phrase, or the head of the childnoun-phrase.
This added leniency comes at a price,as shown in Table 3 where the undirected accuracyand NED results are considerably higher than di-rected accuracy, and display less spread of values(look in particular at the random trees, Ra).
Is isunclear that the predicted trees are truly predictinglinguistically plausible structures, but instead thatthe differences are due largely to chance.
Moreover,systems that predict linguistic phenomena inconsis-tently between sentences or across types of relatedphenomena are rewarded under these lenient met-rics.For these reasons we also consider a different,less permissive, evaluation method, using multiplereferences of the treebank where each is annotatedwith different styles of dependency.
As describedin section 2, we processed the Penn treebank fivetimes with different options to the LTH conversiontool.
This affected the treatment of coordination,preposition phrases, subordinate clauses, infinitivalclauses etc.
Next we compare the directed accu-racy of the systems against these five different ?goldstandard?
references, which are displayed in Table 5,alongside the maximum score for each system.
Notethat most systems performed well against the stan-dard, conll2007 and functional references but poorlyagainst the lexical and oldLTH references.18 Con-sidering the latter two references, a different systemwould be selected as the highest performing, namelyBisk and Hockenmaier (2012) (BH) over Blunsomand Cohn (2010) (BC) which wins in the other cases.18The common difference here is that the latter two refer-ences do not treat prepositions as heads of PPs.74This evaluation method rewards many different lin-guistically plausible structures, but in such a waythat the predictions must be consistent between dif-ferent sentences in the testing set, and in their treat-ment of related linguistic phenomena.
One caveatis that this method can only be used when thereare many references, although in many cases differ-ent outputs can be generated automatically, e.g., byadjusting head-finding heuristics in converting be-tween phrase-structure to dependency trees.The previous analysis has rated each system interms of overall performance against treebank trees,however this doesn?t necessarily mean that the pre-dictions of the best ranked system will be the mostuseful ones in a task-based setting.
Take the ex-ample of information extraction, in which a centralproblem is to identify the arguments (subject, objectetc) of a given verb.
This setting gives rise to sometypes of dependency edges being more valuable thanothers.
We present comparative results for the Penntreebank in Table 6 showing the directed accuracyfor different types of dependency relations.
Observethat there is a wide spread of accuracies for predict-ing the head word of the sentence (ROOT), and simi-larly for verbs?
subject and object arguments.
Thesescores are similar to the scores for the local modi-fiers shown, such as NMOD which describe the ar-guments of a noun.
This is surprising as noun edgestend to be much shorter than for the arguments to averb, and thus should be easier to predict.
Also in-teresting are the spread of results for the CC edges(these link a coordinating conjunction to its head),suggesting that the systems learn to represent coor-dination in very different ways to the method usedin the reference.Figure 1 illustrates the directed accuracy over dif-ferent lengths of dependency edge.
For all systemsthe accuracy diminishes with edge length, howeversome fall at a much faster rate.
The two best systems(Tu, BC) have similar overall accuracy results, butit is clear that Tu does better on short edges whileBC does better on longer ones.
The same patternwas also observed when considering the average ac-curacy over all treebanks (not shown), although thesystems?
results were closer together.system ROOT SBJ OBJ PRD NMOD COORD CCTu 71.0 64.8 53.7 49.4 56.9 36.8 11.4LB 17.8 40.1 15.3 18.0 41.9 27.7 9.7BC 74.9 65.7 53.0 50.2 56.8 36.3 71.4DMVc 17.0 11.7 16.0 31.3 27.8 25.7 9.2DMVu 17.6 9.3 16.4 25.0 27.8 25.7 8.6BH 67.5 55.3 44.9 45.6 58.6 27.6 62.7Mu 29.3 42.4 38.8 51.8 34.5 30.5 33.0R 12.9 9.4 16.1 21.1 12.1 15.7 2.7Mc 60.7 47.4 39.9 45.8 36.5 33.9 44.3RB 17.9 12.4 26.2 36.5 15.3 25.4 1.1H 19.4 29.3 12.2 22.2 17.3 20.9 10.3DMVp 54.7 42.0 30.7 30.1 28.9 25.4 24.3S2 45.2 41.9 44.2 49.8 39.7 25.4 63.8Mp 67.8 54.3 49.6 59.4 47.7 37.7 49.7S1 43.1 47.9 36.3 46.7 27.9 23.5 7.6Table 6: Directed accuracy results on the Penn treebank,stratified by dependency relation.
For clarity, only 9 im-portant relation types are shown.
The vertical bars sepa-rate different groups of relations, from left to right, relat-ing to the main verb, general modifiers and coordination.7 ConclusionThis challenge set out to evaluate the state-of-the-art in part-of-speech and dependency grammar in-duction, promoting research in this field and, im-portantly, providing a fair means of evaluation.
Theparticipants submissions used a wide variety of dif-ferent approaches, many of which we shown tohave improved over competitive benchmark sys-tems.
While the results were overall very positive,it is fair to say that the tasks of part-of-speech andgrammar induction are still very much open chal-lenges, and that there is still considerable room forimprovement.
The data submitted to this evaluationcampaign will provide a great resource for devisingnew methods of evaluation, and we plan to pursuethis avenue in future work, in particular task-basedevaluation such as in an information extraction ormachine translation setting.8 AcknowledgementsThis challenge was funded by the PASCAL 2 (Pat-tern Analysis, Statistical Modelling and Compu-tational Intelligence) European Network of Excel-lence.
We would also like to thank the treebankproviders for allowing us to use their resources, as-sisting us in converting these into our desired for-mat, and helping to resolve various questions.
Inparticular, special thanks to Zdenek Zabokrtsky andJan (Czech and Arabic), Tomaz Erjavec (Slovene),and Eckhard Bick and Diana Santos (Portuguese).We are also indebted to the organisers of the previ-75testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RBconll2007 54.9 51.7 40.4 49.2 36.8 32.2 41.7 54.2 20.9 33.2 20.4 18.0 30.1 20.3functional 59.6 52.4 41.5 47.4 36.2 30.6 40.0 58.5 20.9 37.2 20.6 19.3 29.2 23.7lexical 40.6 41.9 28.5 37.3 24.8 27.7 35.5 39.5 23.5 23.1 23.0 14.4 33.1 10.1oldLTH 41.4 43.6 28.8 37.8 24.6 28.6 36.1 39.5 22.3 23.7 21.8 14.3 32.0 10.7standard 56.0 50.4 41.0 50.3 37.5 32.8 42.5 55.5 22.3 33.5 21.8 18.4 31.4 20.4best 59.6 52.4 41.5 50.3 37.5 32.8 42.5 58.5 23.5 37.2 23.0 19.3 33.1 23.7Table 5: Directed accuracy results measured against different conversions of the Penn Treebank into dependency trees.l lllll ll l2 4 6 810203040506070edge lengthdirected accuracy (%)l TuBCBHMZ?pS?2DMV?pFigure 1: Directed accuracy on the Penn treebank strat-ified by dependency length.
For clarity only a subset ofthe systems are shown, and edges of length 10 or morewere omitted.ous CoNLL 2006 and 2007 competitions, who con-tributed significant efforts into collating so manytreebanks and developing treebank conversion tools,making our job much easier than it would other-wise have been.
Thanks to Sebastian Reidel, JoakimNivre and Sabine Buchholz for promptly answer-ing our questions.
We would like to thank theLDC, who allowed their licenced data to be usedfree of charge by the competitors, and Ilya Ahtaridiswho administered the licencing and corpus distribu-tion.
Thanks also to Valentin Spitkovski and Chris-tos Christodoulopoulos who kindly provided us withtheir evaluation scripts, and finally, the participantsthemselves for taking part.ReferencesI.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,A.
Diaz de Ilarraza, A. Garmendia, and M. Oronoz.2003.
Construction of a Basque dependency treebank.In Proceedings of the 2nd Workshop on Treebanks andLinguistic Theories (TLT).Susana Afonso, Eckhard Bick, Renato Haber, and Di-ana Santos.
2002.
Floresta sinta?
(c)tica: a treebankfor Portuguese.
In Proceedings of the Third Interna-tional Conference on Language Resources and Evalu-ation (LREC 2002), pages 1698?1703, May.Yonatan Bisk and Julia Hockenmaier.
2012.
Induction oflinguistic structure with combinatory categorial gram-mars.
In Proceedings of the NAACL-HLT 2012 Work-shop on Inducing Linguistic Structure Shared Task,June.Phil Blunsom and Trevor Cohn.
2010.
Unsupervised in-duction of tree substitution grammars for dependencyparsing.
In Proceedings of the 2010 Conference onEmpirical Methods on Natural Language Processing(EMNLP), pages 1204?1213, Cambridge, MA, USA.Phil Blunsom and Trevor Cohn.
2011.
A hierarchicalPitman-Yor process HMM for unsupervised part ofspeech induction.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 865?874,Portland, Oregon, USA, June.Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and BarboraHladka?.
2001.
The Prague Dependency Treebank: AThree-Level Annotation Scenario.
In Anne Abeille?,editor, Treebanks: Building and Using SyntacticallyAnnotated Corpora, pages 103?127.
Kluwer Aca-demic Publishers.Gosse Bouma, Gertjan van Noord, and Robert Malouf.2000.
Alpino: Wide coverage computational analysisof Dutch.
In Proceedings of Computational Linguis-tics in the Netherlands (CLIN 2000), pages 45?59.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Comput.Linguist., 18:467?479, December.Matthias Buch-Kromann, Ju?rgen Wedekind,and Jakob Elming.
2007.
The Copen-hagen Danish-English dependency tree-bank.
http://code.google.com/p/copenhagen-dependency-treebank.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
In76Proceedings of the Tenth Conference on Computa-tional Natural Language Learning (CoNLL-X), pages149?164.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsupervisedPOS induction: how far have we come?
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing, EMNLP ?10, pages575?584.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2011.
A Bayesian mixture modelfor PoS induction using multiple features.
In Proceed-ings of the 2011 Conference on Empirical Methods inNatural Language Processing, pages 638?647, Edin-burgh, Scotland, UK., July.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2012.
Turning the pipeline intoa loop: Iterated unsupervised dependency parsingand pos induction.
In Proceedings of the NAACL-HLT 2012 Workshop on Inducing Linguistic StructureShared Task, June.Grzegorz Chrupa?a.
2012.
Hierarchical clusteringof word class distributions.
In Proceedings of theNAACL-HLT 2012 Workshop on Inducing LinguisticStructure Shared Task, June.Tomaz?
Erjavec, Darja Fis?er, Simon Krek, and NinaLedinek.
2010.
The JOS linguistically tagged corpusof Slovene.
In Proceedings of the Seventh Interna-tional Conference on Language Resources and Evalu-ation (LREC?10).Joa?o Grac?a, Kuzman Ganchev, Lu?
?sa Coheur, FernandoPereira, and Benjamin Taskar.
2011.
Controllingcomplexity in part-of-speech induction.
J. Artif.
Intell.Res.
(JAIR), 41:527?551.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe main conference on Human Language TechnologyConference of the North American Chapter of the As-sociation of Computational Linguistics (HLT-NAACL?06), pages 320?327.Jan Hajic?, Otakar Smrz?, Petr Zema?nek, Jan S?naidauf, andEmanuel Bes?ka.
2004.
Prague Arabic dependencytreebank: Development in data and tools.
In Proceed-ings of the NEMLAR International Conference on Ara-bic Language Resources and Tools, pages 110?117.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProceedings of the 16th Nordic Conference of Compu-tational Linguistics (NODALIDA 2007).Mark Johnson.
2007.
Why doesn?t EM find good hmmpos-taggers.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing, EMNLP-CoNLL ?07, pages 296?305.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: models of de-pendency and constituency.
In ACL ?04: Proceedingsof the 42nd Annual Meeting on Association for Com-putational Linguistics, page 478.David Marac?ek and Zdene?k Z?abokrtsky?.
2012.
Unsuper-vised dependency parsing using reducibility and fertil-ity features.
In Proceedings of the NAACL-HLT 2012Workshop on Inducing Linguistic Structure SharedTask, June.Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: the Penn treebank.
ComputationalLinguistics, 19(2):313?330.Marina Meila.
2003.
Comparing Clusterings by the Vari-ation of Information.
Learning Theory and KernelMachines, pages 173?187.Tahira Naseem, Harr Chen, Regina Barzilay, and MarkJohnson.
2010.
Using universal linguistic knowl-edge to guide grammar induction.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1234?1244, October.Joakim Nivre, Jens Nilsson, and Johan Hall.
2006.
Tal-banken05: A Swedish treebank with phrase structureand dependency annotation.
In Proceedings of the fifthinternational conference on Language Resources andEvaluation (LREC2006).Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on depen-dency parsing.
In Proceedings of the CoNLL SharedTask Session of EMNLP-CoNLL 2007, pages 915?932,June.Franz Josef Och.
1999.
An efficient method for deter-mining bilingual word classes.
In Proceedings of theninth conference on European chapter of the Associa-tion for Computational Linguistics, pages 71?76.Slav Petrov, Dipanjan Das, and Ryan T. McDonald.2011.
A universal part-of-speech tagset.
CoRR,abs/1104.2086.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clusterevaluation measure.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 410?420.K.
Sagae, E. Davis, A. Lavie, B. MacWhinney, andS.
Wintner.
2007.
High-accuracy annotation and pars-ing of CHILDES transcripts.
In Proceedings of theACL-2007 Workshop on Cognitive Aspects of Compu-tational Language Acquisition., June.Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-poport.
2011.
Neutralizing linguistically problematic77annotations in unsupervised dependency parsing eval-uation.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies, pages 663?672.Anders S?gaard.
2012.
Two baselines for unsuperviseddependency parsing.
In Proceedings of the NAACL-HLT 2012 Workshop on Inducing Linguistic StructureShared Task, June.Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.2011.
Evaluating dependency parsing: Robust andheuristics-free cross-annotation evaluation.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 385?396,Edinburgh, UK, July.Kewei Tu.
2012.
Combining the sparsity and unambi-guity biases for grammar induction.
In Proceedings ofthe NAACL-HLT 2012 Workshop on Inducing Linguis-tic Structure Shared Task, June.David Vadas and James R. Curran.
2007.
Adding nounphrase structure to the Penn Treebank.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics (ACL-07), pages 240?247,June.AppendixDirectedtestset CGS DMVc DMVp DMVuarabic 36.1 42.6 51.9 49.1basque 28.4 28.9 27.1 30.2czech 33.1 28.6 38.2 28.3danish 27.9 36.4 38.4 18.2dutch 31.0 39.0 41.1 40.3en-childes 31.2 40.8 44.3 42.1en-ptb 22.7 25.1 23.1 23.1portuguese 26.7 38.4 34.5 31.1slovene 26.3 20.6 19.2 22.6swedish 29.0 30.9 30.9 26.5averages 29.3 33.1 34.9 31.1Undirectedtestset CGS DMVc DMVp DMVuarabic 58.3 52.8 58.3 61.1basque 49.3 49.2 50.5 50.0czech 48.7 45.9 57.2 47.9danish 56.3 60.9 57.0 43.7dutch 47.0 53.6 57.2 53.8en-childes 56.3 61.0 62.7 59.9en-ptb 50.7 52.9 54.1 46.9portuguese 51.8 61.1 59.4 51.6slovene 40.5 41.9 45.3 41.2swedish 52.5 57.1 57.1 48.6averages 51.1 53.6 55.9 50.5NEDtestset CGS DMVc DMVp DMVuarabic 62.0 63.0 66.7 67.6basque 67.4 62.8 62.5 62.3czech 65.1 60.7 72.0 64.0danish 72.0 72.4 73.2 60.3dutch 57.7 64.9 65.0 64.7en-childes 79.9 79.6 79.9 78.4en-ptb 67.9 73.4 74.3 63.7portuguese 58.2 80.7 79.5 72.6slovene 55.7 52.3 59.4 51.9swedish 64.8 78.4 78.4 65.7averages 65.1 68.8 71.1 65.1Table 9: Evaluation of the joint task on the dependencyoutput using a maximum sentence length of 10.78M-1testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKuarabic 75.06 N/A 79.00 79.00 62.20 62.24 61.81 64.60 61.55 65.69 67.82 63.23basque 71.58 69.20 75.23 75.23 65.97 56.37 64.37 68.58 62.17 63.59 68.22 60.49czech 74.84 61.53 66.97 76.00 N/A N/A 55.60 72.51 55.01 60.38 73.38 51.96danish 56.48 55.41 70.28 71.62 49.24 35.32 50.21 66.50 33.57 49.40 61.60 35.02dutch 80.72 70.13 74.04 76.08 63.37 57.64 57.99 83.76 57.31 68.18 84.64 63.04en-childes 84.23 77.57 85.35 85.35 N/A N/A 76.34 85.11 59.75 77.55 86.39 59.55en-ptb 78.26 72.26 62.86 73.46 63.15 56.32 65.10 68.10 48.76 70.31 73.96 47.91portuguese 76.00 72.05 75.47 77.13 68.40 65.86 65.52 69.61 61.84 64.63 66.81 62.95slovene 67.29 59.78 72.18 72.71 63.95 55.23 54.85 63.68 52.15 54.08 59.31 45.40swedish 66.20 67.86 73.55 73.55 60.10 48.43 61.21 61.21 47.51 64.39 64.39 46.04averages 73.07 67.31 73.49 76.01 62.05 54.68 61.30 70.37 53.96 63.82 70.65 53.561-1testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKuarabic 50.90 N/A 39.15 39.15 41.49 53.92 39.89 34.23 40.63 50.53 41.98 50.27basque 42.45 46.25 52.38 52.38 48.91 45.55 41.29 33.49 50.80 43.27 35.73 43.43czech 31.45 48.24 32.18 31.74 N/A N/A 43.12 33.55 41.94 38.93 28.33 35.31danish 43.08 43.64 32.17 31.77 40.56 34.83 33.48 30.87 26.33 38.92 30.59 32.95dutch 43.22 55.85 43.26 39.98 56.45 45.37 48.13 21.88 43.10 55.86 22.42 54.04en-childes 64.10 63.62 64.50 64.50 N/A N/A 59.96 56.87 59.75 63.43 53.40 57.68en-ptb 57.63 56.02 45.52 41.35 48.95 53.15 55.43 49.60 47.57 54.10 51.80 45.43portuguese 59.71 50.18 36.13 35.38 54.42 60.08 49.57 45.00 48.25 46.57 38.37 45.10slovene 42.62 50.66 33.23 32.59 56.55 50.30 44.97 44.34 40.62 41.24 40.01 38.93swedish 48.76 40.54 34.07 34.07 38.21 46.32 36.12 36.12 44.57 41.90 41.90 38.05averages 48.39 50.55 41.26 40.29 48.19 48.69 45.20 38.59 44.36 47.48 38.45 44.12VMtestset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKuarabic 61.59 N/A 52.95 52.95 46.99 47.18 40.75 43.16 40.40 47.95 45.09 45.14basque 53.83 51.34 54.45 54.45 49.34 44.26 44.18 45.46 45.37 45.02 44.90 42.76czech 56.80 50.22 45.06 46.76 N/A N/A 42.50 49.93 41.51 45.15 51.38 39.62danish 61.57 59.00 63.39 63.62 53.35 43.20 51.83 58.38 33.46 52.52 58.44 39.46dutch 57.82 53.94 55.01 53.40 48.99 46.26 44.08 50.49 44.37 52.02 51.33 47.99en-childes 80.17 76.59 78.18 78.18 N/A N/A 73.67 76.47 65.44 76.14 76.87 68.25en-ptb 71.44 68.12 59.90 61.31 63.90 60.04 63.79 63.64 52.96 66.40 66.50 54.33portuguese 67.49 60.37 54.61 54.74 58.91 59.58 53.30 54.99 50.26 53.15 52.67 50.76slovene 54.80 52.13 51.85 51.88 52.99 48.55 45.33 48.33 40.13 39.25 45.73 38.68swedish 61.52 58.23 56.09 56.09 55.02 48.69 51.76 51.76 43.39 54.28 54.28 44.51averages 62.70 58.88 57.15 57.34 53.69 49.72 51.12 54.26 45.73 53.19 54.72 47.15VItestset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKuarabic 2.65 N/A 3.76 3.76 3.47 3.19 3.96 4.12 3.73 3.49 3.96 3.48basque 3.65 3.50 3.78 3.78 3.45 3.36 4.09 4.79 3.67 4.00 4.72 3.83czech 3.80 3.49 4.96 5.48 N/A N/A 3.92 4.57 3.91 3.85 4.46 4.17danish 3.76 3.85 4.07 4.08 4.29 4.53 4.54 4.91 5.24 4.45 4.78 4.85dutch 3.53 3.14 3.31 3.72 3.33 3.47 3.68 5.16 3.61 3.26 5.07 3.39en-childes 1.86 2.12 2.11 2.11 N/A N/A 2.39 2.32 2.59 2.17 2.31 2.47en-ptb 2.69 2.90 3.67 4.03 3.16 3.08 3.24 3.41 3.66 3.05 3.19 3.50portuguese 2.40 2.90 4.01 4.11 2.97 2.74 3.35 3.46 3.35 3.40 3.63 3.37slovene 3.65 3.40 4.65 4.68 3.34 3.48 3.92 4.23 4.03 4.38 4.51 4.25swedish 3.36 3.78 4.57 4.57 3.89 3.65 4.45 4.45 4.11 4.17 4.17 4.07averages 3.13 3.23 3.89 4.03 3.49 3.44 3.75 4.14 3.79 3.62 4.08 3.74Table 7: One to one, Many to one, VM and VI scores of POS induction results evaluated against fine POS tags (c.f.,Table 2 which used UPOS).79Directedtestset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RBarabic 61.1 47.2 15.7 64.8 64.8 47.2 54.6 66.7 46.3 45.4 50.0 42.6 9.3 64.8basque 56.3 50.3 28.0 30.3 27.2 33.3 22.3 58.6 46.3 43.2 31.3 21.8 34.3 24.4czech 50.0 48.5 61.3 57.5 57.7 45.5 51.2 59.0 30.1 31.2 31.8 24.7 28.9 34.3danish 46.2 49.3 60.2 51.3 61.4 56.9 60.5 60.8 47.2 50.2 35.3 36.4 18.7 49.2dutch 50.5 50.8 37.0 49.5 38.4 38.9 50.0 51.7 48.7 39.7 49.1 35.1 34.0 39.5en-childes 48.1 62.2 56.8 47.2 51.8 50.5 53.5 56.0 51.7 51.9 39.0 31.7 36.0 23.3en-ptb 72.1 73.7 58.9 67.4 52.2 44.8 61.0 74.7 31.7 44.7 30.6 35.2 40.4 19.9portuguese 54.3 76.3 63.6 59.9 44.3 47.7 71.1 55.7 27.1 37.2 26.9 31.1 28.1 37.7slovene 65.8 53.9 42.1 51.4 39.2 39.7 50.3 67.7 35.7 37.2 35.6 25.7 35.9 14.7swedish 65.8 66.7 61.4 63.7 70.8 48.2 72.0 76.5 44.2 44.2 45.8 39.1 33.2 31.3averages 57.0 57.9 48.5 54.3 50.8 45.3 54.7 62.7 40.9 42.5 37.5 32.3 29.9 33.9Undirectedtestset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RBarabic 69.4 59.3 59.3 69.4 69.4 59.3 65.7 67.6 52.8 53.7 55.6 54.6 61.1 69.4basque 65.6 59.5 49.8 50.1 48.4 54.3 32.8 66.4 60.1 58.1 48.5 42.2 56.4 53.9czech 65.9 59.9 69.2 66.6 67.6 62.3 63.5 70.1 51.6 51.2 50.5 47.2 54.2 56.9danish 67.9 63.5 70.6 64.4 71.1 67.9 70.7 70.2 65.0 64.3 60.0 56.4 59.7 63.9dutch 63.2 59.9 57.5 63.3 58.0 58.5 58.5 60.5 62.7 56.7 62.9 51.1 56.3 60.7en-childes 65.3 70.6 69.4 62.4 63.7 64.3 63.6 69.1 65.7 66.1 59.5 51.2 51.2 51.0en-ptb 79.4 79.2 65.9 72.5 62.4 62.5 75.1 78.8 53.8 65.3 53.2 52.0 58.8 54.9portuguese 66.3 81.9 71.6 70.2 62.3 65.8 78.5 72.1 54.0 60.9 54.3 53.3 56.7 63.8slovene 70.6 63.7 56.3 59.1 55.1 54.8 63.7 72.0 46.4 53.1 46.3 44.9 45.5 46.0swedish 82.3 73.5 70.1 71.1 75.4 66.5 77.3 83.7 64.5 64.5 66.1 59.2 59.2 59.5averages 69.6 67.1 64.0 64.9 63.3 61.6 64.9 71.0 57.7 59.4 55.7 51.2 55.9 58.0NEDtestset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RBarabic 78.7 68.5 66.7 75.9 75.9 68.5 72.2 71.3 61.1 63.0 63.0 63.0 64.8 75.9basque 77.9 68.6 62.9 65.2 64.2 70.1 55.0 79.5 69.3 69.0 63.8 58.2 73.2 64.0czech 79.9 72.6 81.8 78.6 79.9 76.0 78.1 81.2 62.9 61.7 63.6 60.3 63.2 73.8danish 81.7 76.3 83.2 78.4 84.3 77.3 84.4 85.0 77.9 75.8 69.5 67.8 66.2 77.5dutch 71.0 71.8 77.1 78.4 76.8 72.6 68.5 71.0 73.2 64.6 73.0 60.5 64.4 71.0en-childes 82.4 81.6 84.5 76.7 74.8 79.0 84.9 82.5 79.9 80.4 79.9 70.1 63.2 76.9en-ptb 86.7 89.4 87.5 88.4 84.0 78.7 87.1 84.3 64.0 80.7 64.2 64.3 65.2 75.0portuguese 78.2 90.7 87.8 87.8 87.5 82.9 91.9 90.2 75.6 81.7 76.5 67.7 60.9 82.4slovene 79.3 76.8 70.8 72.8 70.4 68.1 78.9 79.8 59.4 62.2 59.4 55.8 54.3 62.2swedish 91.7 85.8 83.1 85.6 87.1 80.8 87.6 92.1 76.1 76.1 76.4 75.3 67.2 79.1averages 80.8 78.2 78.5 78.8 78.5 75.4 78.9 81.7 69.9 71.5 68.9 64.3 64.3 73.8Table 8: Evaluation of the dependency task using a maximum sentence length of 10.
See also Table 3 which presentsthe same results with no length restriction.80
