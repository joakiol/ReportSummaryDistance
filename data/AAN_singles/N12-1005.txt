2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39?48,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsContinuous Space Translation Models with Neural NetworksLe Hai Son and Alexandre Allauzen and Franc?ois YvonUniv.
Paris-Sud, France and LIMSI/CNRSrue John von Neumann, 91403 Orsay cedex, FranceFirstname.Lastname@limsi.frAbstractThe use of conventional maximum likelihoodestimates hinders the performance of existingphrase-based translation models.
For lack ofsufficient training data, most models only con-sider a small amount of context.
As a par-tial remedy, we explore here several contin-uous space translation models, where transla-tion probabilities are estimated using a con-tinuous representation of translation units inlieu of standard discrete representations.
Inorder to handle a large set of translation units,these representations and the associated esti-mates are jointly computed using a multi-layerneural network with a SOUL architecture.
Insmall scale and large scale English to Frenchexperiments, we show that the resulting mod-els can effectively be trained and used on topof a n-gram translation system, delivering sig-nificant improvements in performance.1 IntroductionThe phrase-based approach to statistical machinetranslation (SMT) is based on the following infer-ence rule, which, given a source sentence s, selectsthe target sentence t and the underlying alignment amaximizing the following term:P (t,a|s) =1Z(s)exp( K?k=1?kfk(s, t,a)), (1)where K feature functions (fk) are weighted by aset of coefficients (?k), and Z is a normalizing fac-tor.
The phrase-based approach differs from otherapproaches by the hidden variables of the translationprocess: the segmentation of a parallel sentence pairinto phrase pairs and the associated phrase align-ments.This formulation was introduced in (Zens et al,2002) as an extension of the word based mod-els (Brown et al, 1993), then later motivated withina discriminative framework (Och and Ney, 2004).One motivation for integrating more feature func-tions was to improve the estimation of the translationmodel P (t|s), which was initially based on relativefrequencies, thus yielding poor estimates.This is because the units of phrase-based mod-els are phrase pairs, made of a source and a tar-get phrase; such units are viewed as the events ofdiscrete random variables.
The resulting representa-tions of phrases (or words) thus entirely ignore themorphological, syntactic and semantic relationshipsthat exist among those units in both languages.
Thislack of structure hinders the generalization power ofthe model and reduces its ability to adapt to otherdomains.
Another consequence is that phrase-basedmodels usually consider a very restricted context1.This is a general issue in statistical Natural Lan-guage Processing (NLP) and many possible reme-dies have been proposed in the literature, such as,for instance, using smoothing techniques (Chen andGoodman, 1996), or working with linguistically en-riched, or more abstract, representations.
In statisti-cal language modeling, another line of research con-siders numerical representations, trained automat-ically through the use of neural network (see eg.1typically a small number of preceding phrase pairs for then-gram based approach (Crego and Marin?o, 2006), or no con-text at all, for the standard approach of (Koehn et al, 2007).39(Collobert et al, 2011)).
An influential proposal,in this respect, is the work of (Bengio et al, 2003)on continuous space language models.
In this ap-proach, n-gram probabilities are estimated using acontinuous representation of words in lieu of stan-dard discrete representations.
Experimental results,reported for instance in (Schwenk, 2007) show sig-nificant improvements in speech recognition appli-cations.
Recently, this model has been extended inseveral promising ways (Mikolov et al, 2011; Kuoet al, 2010; Liu et al, 2011).
In the context of SMT,Schwenk et al (2007) is the first attempt to esti-mate translation probabilities in a continuous space.However, because of the proposed neural architec-ture, the authors only consider a very restricted setof translation units, and therefore report only a slightimpact on translation performance.
The recent pro-posal of (Le et al, 2011a) seems especially relevant,as it is able, through the use of class-based models,to handle arbitrarily large vocabularies and opens theway to enhanced neural translation models.In this paper, we explore various neural architec-tures for translation models and consider three dif-ferent ways to factor the joint probability P (s, t)differing by the units (respectively phrase pairs,phrases or words) that are projected in continuousspaces.
While these decompositions are theoreti-cally straightforward, they were not considered inthe past because of data sparsity issues and of theresulting weaknesses of conventional maximum like-lihood estimates.
Our main contribution is then toshow that such joint distributions can be efficientlycomputed by neural networks, even for very largecontext sizes; and that their use yields significantperformance improvements.
These models are eval-uated in a n-best rescoring step using the frameworkof n-gram based systems, within which they inte-grate easily.
Note, however that they could be usedwith any phrase-based system.The rest of this paper is organized as follows.
Wefirst recollect, in Section 2, the n-gram based ap-proach, and discuss various implementations of thisframework.
We then describe, in Section 3, the neu-ral architecture developed and explain how it can bemade to handle large vocabulary tasks as well as lan-guage models over bilingual units.
We finally re-port, in Section 4, experimental results obtained ona large-scale English to French translation task.2 Variations on the n-gram approachEven though n-gram translation models can beintegrated within standard phrase-based systems(Niehues et al, 2011), the n-gram based frame-work provides a more convenient way to introduceour work and has also been used to build the base-line systems used in our experiments.
In the n-gram based approach (Casacuberta and Vidal, 2004;Marin?o et al, 2006; Crego and Marin?o, 2006), trans-lation is divided in two steps: a source reorderingstep and a translation step.
Source reordering isbased on a set of learned rewrite rules that non-deterministically reorder the input words so as tomatch the target order thereby generating a latticeof possible reorderings.
Translation then amountsto finding the most likely path in this lattice using an-gram translation model 2 of bilingual units.2.1 The standard n-gram translation modeln-gram translation models (TMs) rely on a spe-cific decomposition of the joint probability P (s, t),where s is a sequence of I reordered source words(s1, ..., sI ) and t contains J target words (t1, ..., tJ ).This sentence pair is further assumed to be de-composed into a sequence of L bilingual unitscalled tuples defining a joint segmentation: (s, t) =u1, ..., uL.
In the approach of (Marin?o et al, 2006),this segmentation is a by-product of source reorder-ing, and ultimately derives from initial word andphrase alignments.
In this framework, the basictranslation units are tuples, which are the analogousof phrase pairs, and represent a matching u = (s, t)between a source s and a target t phrase (see Fig-ure 1).
Using the n-gram assumption, the joint prob-ability of a segmented sentence pair decomposes as:P (s, t) =L?i=1P (ui|ui?1, ..., ui?n+1) (2)A first issue with this model is that the elementaryunits are bilingual pairs, which means that the under-lying vocabulary, hence the number of parameters,can be quite large, even for small translation tasks.Due to data sparsity issues, such models are bound2Like in the standard phrase-based approach, the translationprocess also involves additional feature functions that are pre-sented below.40to face severe estimation problems.
Another prob-lem with (2) is that the source and target sides playsymmetric roles, whereas the source side is known,and the target side must be predicted.2.2 A factored n-gram translation modelTo overcome some of these issues, the n-gram prob-ability in equation (2) can be factored by decompos-ing tuples in two (source and target) parts :P (ui|ui?1, ..., ui?n+1) =P (ti|si, si?1, ti?1, ..., si?n+1, ti?n+1)?
P (si|si?1, ti?1..., si?n+1, ti?n+1)(3)Decomposition (3) involves two models: the firstterm represents a TM, the second term is best viewedas a reordering model.
In this formulation, the TMonly predicts the target phrase, given its source andtarget contexts.Another benefit of this formulation is that the el-ementary events now correspond either to source orto target phrases, but never to pairs of such phrases.The underlying vocabulary is thus obtained as theunion, rather than the cross product, of phrase in-ventories.
Finally note that the n-gram probabilityP (ui|ui?1, ..., ui?n+1) could also factor as:P (si|ti, si?1, ti?1, ..., si?n+1, ti?n+1)?
P (ti|si?1, ti?1, ..., si?n+1, ti?n+1)(4)2.3 A word factored translation modelA more radical way to address the data sparsity is-sues is to take (source and target) words as the basicunits of the n-gram TM.
This may seem to be a stepbackwards, since the transition from word (Brown etal., 1993) to phrase-based models (Zens et al, 2002)is considered as one of the main recent improvementin MT.
One important motivation for consideringphrases rather than words was to capture local con-text in translation and reordering.
It should then bestressed that the decomposition of phrases in wordsis only re-introduced here as a way to mitigate theparameter estimation problems.
Translation unitsare still pairs of phrases, derived from a bilingualsegmentation in tuples synchronizing the source andtarget n-gram streams, as defined by equation (3).In fact, the estimation policy described in section 3will actually allow us to design n-gram models withlonger contexts than is typically possible in the con-ventional n-gram approach.Let ski denote the kth word of source tuple si.Considering again the example of Figure 1, s111 isto the source word nobel, s411 is to the source wordpaix, and similarly t211 is the target word peace.
Wefinally denote hn?1(tki ) the sequence made of then ?
1 words preceding tki in the target sentence: inFigure 1, h3(t211) thus refers to the three word con-text receive the nobel associated with the target wordpeace.
Using these notations, equation (3) is rewrit-ten as:P (s, t) =L?i=1[ |ti|?k=1P(tki |hn?1(tki ), hn?1(s1i+1))?|si|?k=1P(ski |hn?1(t1i ), hn?1(ski ))] (5)This decomposition relies on the n-gram assump-tion, this time at the word level.
Therefore, thismodel estimates the joint probability of a sentencepair using two sliding windows of length n, one foreach language; however, the moves of these win-dows remain synchronized by the tuple segmenta-tion.
Moreover, the context is not limited to the cur-rent phrase, and continues to include words in ad-jacent phrases.
Using the example of Figure 1, thecontribution of the target phrase t11 = nobel, peaceto P (s, t) using a 3- gram model isP(nobel|[receive, the], [la, paix])?P(peace|[the, nobel], [la, paix]).Likewise, the contribution of the source phrases11 =nobel, de, la, paix is:P(nobel|[receive, the], [recevoir,le])?
P(de|[receive, the], [le,nobel])?
P(la|[receive, the], [nobel, de])?
P(paix|[receive, the], [de,la]).A benefit of this new formulation is that the involvedvocabularies only contain words, and are thus muchsmaller.
These models are thus less bound to be af-fected by data sparsity issues.
While the TM definedby equation (5) derives from equation (3), a variationcan be equivalently derived from equation (4).41s?8: ?t?8: tos?9: recevoirt?9: receives?10: let?10: thes?11: nobel de la paixt?11: nobel peaces?12: prixt?12: prizeu8u9u10u11u12S :   ....T :   ....?
recevoir le prix nobel de la paixorg :   ............Figure 1: Extract of a French-English sentence pair segmented in bilingual units.
The original (org) French sentenceappears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into asequence of L bilingual units (tuples) u1, ..., uL.
Each tuple ui contains a source and a target phrase: si and ti.3 The SOUL modelIn the previous section, we defined three differentn-gram translation models, based respectively onequations (2), (3) and (5).
As discussed above, amajor issue with such models is to reliably estimatetheir parameters, the numbers of which grow expo-nentially with the order of the model.
This problemis aggravated in natural language processing, due towell known data sparsity issues.
In this work, wetake advantage of the recent proposal of (Le et al,2011a): using a specific neural network architecture(the Structured OUtput Layer model), it becomespossible to handle large vocabulary language mod-eling tasks, a solution that we adapt here to MT.3.1 Language modeling in a continuous spaceLet V be a finite vocabulary, n-gram language mod-els (LMs) define distributions over finite sequencesof tokens (typically words) wL1 in V+ as follows:P (wL1 ) =L?i=1P (wi|wi?1i?n+1) (6)Modeling the joint distribution of several discreterandom variables (such as words in a sentence) isdifficult, especially in NLP applications where Vtypically contains dozens of thousands words.In spite of the simplifying n-gram assump-tion, maximum likelihood estimation remains un-reliable and tends to underestimate the proba-bility of very rare n-grams.
Smoothing tech-niques, such as Kneser-Ney and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for anempirical overview, and (Teh, 2006) for a Bayesianinterpretation), perform back-off to lower order dis-tributions, thus providing an estimate for the proba-bility of these unseen events.One of the most successful alternative to date is touse distributed word representations (Bengio et al,2003), where distributionally similar words are rep-resented as neighbors in a continuous space.
Thisturns n-grams distributions into smooth functionsof the word representations.
These representationsand the associated estimates are jointly computedin a multi-layer neural network architecture.
Fig-ure 2 provides a partial representation of this kindof model and helps figuring out their principles.
Tocompute the probability P (wi|wi?1i?n+1), the n ?
1context words are projected in the same continu-ous space using a shared matrix R; these continuousword representations are then concatenated to builda single vector that represents the context; after anon-linear transformation, the probability distribu-tion is computed using a softmax layer.The major difficulty with the neural network ap-proach remains the complexity of inference andtraining, which largely depends on the size of theoutput vocabulary (i.e.
the number of words thathave to be predicted).
One practical solution is to re-strict the output vocabulary to a short-list composedof the most frequent words (Schwenk, 2007).
How-ever, the usual size of the short-list is under 20k,which does not seem sufficient to faithfully repre-sent the translation models of section 2.3.2 Principles of SOULTo circumvent this problem, Structured OutputLayer (SOUL) LMs are introduced in (Le et al,2011a).
Following Mnih and Hinton (2008), theSOULmodel combines the neural network approachwith a class-based LM (Brown et al, 1992).
Struc-42turing the output layer and using word class informa-tion makes the estimation of distributions over theentire vocabulary computationally feasible.To meet this goal, the output vocabulary is struc-tured as a clustering tree, where each word belongsto only one class and its associated sub-classes.
Ifwi denotes the ith word in a sentence, the sequencec1:D(wi) = c1, .
.
.
, cD encodes the path for wordwiin the clustering tree, with D being the depth of thetree, cd(wi) a class or sub-class assigned to wi, andcD(wi) being the leaf associated with wi (the worditself).
The probability of wi given its history h canthen be computed as:P (wi|h) =P (c1(wi)|h)?D?d=2P (cd(wi)|h, c1:d?1).
(7)There is a softmax function at each level of the treeand each word ends up forming its own class (a leaf).The SOULmodel, represented on Figure 2, is thusthe same as for the standard model up to the outputlayer.
The main difference lies in the output struc-ture which involves several layers with a softmax ac-tivation function.
The first (class layer) estimatesthe class probability P (c1(wi)|h), while other out-put sub-class layers estimate the sub-class probabili-ties P (cd(wi)|h, c1:d?1).
Finally, theword layers es-timate the word probabilities P (cD(wi)|h, c1:D?1).Words in the short-list remain special, since each ofthem represents a (final) class.Training a SOULmodel can be achieved by maxi-mizing the log-likelihood of the parameters on sometraining corpus.
Following (Bengio et al, 2003),this optimization is performed by stochastic back-propagation.
Details of the training procedure canbe found in (Le et al, 2011b).Neural network architectures are also interestingas they can easily handle larger contexts than typicaln-grammodels.
In the SOUL architecture, enlargingthe context mainly consists in increasing the size ofthe projection layer, which corresponds to a simplelook-up operation.
Increasing the context length atthe input layer thus only causes a linear growth incomplexity in the worst case (Schwenk, 2007).0...010010...0000...0010wi-1wi-2wi-3RRRshared input spaceinput layerhidden layersshortlistsub-classlayerswordlayersclasslayerFigure 2: The architecture of a SOUL Neural Networklanguage model in the case of a 4-gram model.3.3 Translation modeling with SOULThe SOUL architecture was used successfully todeliver (monolingual) LMs probabilities for speechrecognition (Le et al, 2011a) and machine transla-tion (Allauzen et al, 2011) applications.
In fact,using this architecture, it is possible to estimate n-gram distributions for any kind of discrete randomvariables, such as a phrase or a tuple.
The SOUL ar-chitecture can thus be readily used as a replacementfor the standard n-gram TM described in section 2.1.This is because all the random variables are eventsover the same set of tuples.Adopting this architecture for the other n-gramTM respectively described by equations (3) and (5)is more tricky, as they involve two different lan-guages and thus two different vocabularies: the pre-dicted unit is a target phrase (resp.
word), whereasthe context is made of both source and target phrases(resp.
words).
A subsequent modification of theSOUL architecture was thus performed to make upfor ?mixed?
contexts: rather than projecting all thecontext words or phrases into the same continuousspace (using the matrix R, see Figure 2), we usedtwo different projection matrices, one for each lan-guage.
The input layer is thus composed of two vec-tors in two different spaces; these two representa-tions are then combined through the hidden layer,the other layers remaining unchanged.434 Experimental ResultsWe now turn to an experimental comparison of themodels introduced in Section 2.
We first describethe tasks and data that were used, before presentingour n-gram based system and baseline set-up.
Ourresults are finally presented and discussed.Let us first emphasize that the design and inte-gration of a SOUL model for large SMT tasks isfar from easy, given the computational cost of com-puting n-gram probabilities, a task that is performedrepeatedly during the search of the best translation.Our solution was to resort to a two pass approach:the first pass uses a conventional back-off n-grammodel to produce a k-best list (the k most likelytranslations); in the second pass, the probability ofa m-gram SOUL model is computed for each hy-pothesis, added as a new feature and the k-best listis accordingly reordered3.
In all the following ex-periments, we used a fixed context size for SOUL ofm = 10, and used k = 300.4.1 Tasks and corporaThe two tasks considered in our experimentsare adapted from the text translation track ofIWSLT 2011 from English to French (the ?TED?talk task): a small data scenario where the onlytraining data is a small in-domain corpus; and a largescale condition using all the available training data.In this article, we only provide a short overview ofthe task; all the necessary details regarding this eval-uation campaign are on the official website4.The in-domain training data consists of 107, 058sentence pairs, whereas for the large scale task, allthe data available for the WMT 2011 evaluation5 areadded.
For the latter task, the available parallel dataincludes a large Web corpus, referred to as the Gi-gaWord parallel corpus.
This corpus is very noisyand is accordingly filtered using a simple perplexitycriterion as explained in (Allauzen et al, 2011).
Thetotal amount of training data is approximately 11.5million sentence pairs for the bilingual part, andabout 2.5 billion of words for the monolingual part.As the provided development data was quite small,3The probability estimated with the SOULmodel is added asa new feature to the score of an hypothesis given by Equation 1.The coefficients are retuned before the reranking step.4iwslt2011.org5www.statmt.org/wmt11/Model Vocabulary sizeSmall task Large tasksrc trg src trgStandard 317k 8847kPhrase factored 96k 131k 4262k 3972kWord factored 45k 53k 505k 492kTable 1: Vocabulary sizes for the English to French tasksobtained with various SOUL translation (TM) models.For the factored models, sizes are indicated for bothsource (src) and target (trg) sides.development and test set were inverted, and we fi-nally used a development set of 1,664 sentences, anda test set of 934 sentences.
The table 1 provides thesizes of the different vocabularies.
The n-gram TMsare estimated over a training corpus composed of tu-ple sequences.
Tuples are extracted from the word-aligned parallel data (using MGIZA++6 with defaultsettings) in such a way that a unique segmentationof the bilingual corpus is achieved, allowing to di-rectly estimate bilingual n-gram models (see (Cregoand Marin?o, 2006) for details).4.2 n-gram based translation systemThe n-gram based system used here is based on anopen source implementation described in (Crego etal., 2011).
In a nutshell, the TM is implemented asa stochastic finite-state transducer trained using a n-gram model of (source, target) pairs as described insection 2.1.
Training this model requires to reordersource sentences so as to match the target word or-der.
This is performed by a non-deterministic finite-state reordering model, which uses part-of-speechinformation generated by the TreeTagger to gener-alize reordering patterns beyond lexical regularities.In addition to the TM, fourteen feature functionsare included: a target-language model; four lexi-con models; six lexicalized reordering models (Till-mann, 2004; Crego et al, 2011); a distance-baseddistortion model; and finally a word-bonus modeland a tuple-bonus model.
The four lexicon mod-els are similar to the ones used in standard phrase-based systems: two scores correspond to the rela-tive frequencies of the tuples and two lexical weightsare estimated from the automatically generated word6geek.kyloo.net/software44alignments.
The weights associated to feature func-tions are optimally combined using the MinimumError Rate Training (MERT) (Och, 2003).
All theresults in BLEU are obtained as an average of 4 op-timization runs7.For the small task, the target LM is a standard4-gram model estimated with the Kneser-Ney dis-counting scheme interpolated with lower order mod-els (Kneser and Ney, 1995; Chen and Goodman,1996), while for the large task, the target LM is ob-tained by linear interpolation of several 4-grammod-els (see (Lavergne et al, 2011) for details).
As forthe TM, all the available parallel corpora were sim-ply pooled together to train a 3-gram model.
Resultsobtained with this large-scale system were found tobe comparable to some of the best official submis-sions.4.3 Small task evaluationTable 2 summarizes the results obtained with thebaseline and different SOUL models, TMs and atarget LM.
The first comparison concerns the stan-dard n-gram TM, defined by equation (2), when es-timated conventionally or as a SOULmodel.
Addingthe latter model yields a slight BLEU improvementof 0.5 point over the baseline.
When the SOUL TMis phrased factored as defined in equation (3) thegain is of 0.9 BLEU point instead.
This differencecan be explained by the smaller vocabularies usedin the latter model, and its improved robustness todata sparsity issues.
Additional gains are obtainedwith the word factored TM defined by equation (5):a BLEU improvement of 0.8 point over the phrasefactored TM and of 1.7 point over the baseline arerespectively achieved.
We assume that the observedimprovements can be explained by the joint effect ofa better smoothing and a longer context.The comparison with the condition where we onlyuse a SOUL target LM is interesting as well.
Here,the use of the word factored TM still yields to a 0.6BLEU improvement.
This result shows that thereis an actual benefit in smoothing the TM estimates,rather than only focus on the LM estimates.Table 3 reports a comparison among the dif-ferent components and variations of the word7The standard deviations are below 0.1 and thus omitted inthe reported results.Model BLEUdev testBaseline 31.4 25.8Adding a SOUL modelStandard TM 32.0 26.3Phrase factored TM 32.7 26.7Word factored TM 33.6 27.5Target LM 32.6 26.9Table 2: Results for the small English to French task ob-tained with the baseline system and with various SOULtranslation (TM) or target language (LM) models.Model BLEUdev testAdding a SOUL model+ P(tki |hn?1(tki ), hn?1(s1i+1))32.6 26.9+ P(ski |hn?1(t1i ), hn?1(ski ))32.1 26.2+ the combination of both 33.2 27.5+ P(ski |hn?1(ski ), hn?1(t1i+1))31.7 26.1+ P(tki |hn?1(s1i ), hn?1(tki ))32.7 26.8+ the combination of both 33.4 27.2Table 3: Comparison of the different components andvariations of the word factored translation model.factored TM.
In the upper part of the table,the model defined by equation (5) is evaluatedcomponent by component: first the translationterm P(tki |hn?1(tki ), hn?1(s1i+1)), then its distor-tion counterpart P(ski |hn?1(t1i ), hn?1(ski ))and fi-nally their combination, which yields the joint prob-ability of the sentence pair.
Here, we observe thatthe best improvement is obtained with the transla-tion term, which is 0.7 BLEU point better than thelatter term.
Moreover, the use of just a translationterm only yields a BLEU score equal to the one ob-tained with the SOUL target LM, and its combina-tion with the distortion term is decisive to attain theadditional gain of 0.6 BLEU point.
The lower part ofthe table provides the same comparison, but for thevariation of the word factored TM.
Besides a similartrend, we observe that this variation delivers slightlylower results.
This can be explained by the restrictedcontext used by the translation term which no longerincludes the current source phrase or word.45Model BLEUdev testBaseline 33.7 28.2Adding a word factored SOUL TM+ in-domain TM 35.2 29.4+ out-of-domain TM 34.8 29.1+ out-of-domain adapted TM 35.5 29.8Adding a SOUL LM+ out-of-domain adapted LM 35.0 29.2Table 4: Results for the large English to French trans-lation task obtained by adding various SOUL translationand language models (see text for details).4.4 Large task evaluationFor the large-scale setting, the training material in-creases drastically with the use of the additional out-of-domain data for the baseline models.
Results aresummarized in Table 4.
The first observation is thelarge increase of BLEU (+2.4 points) for the base-line system over the small-scale baseline.
For thistask, only the word factored TM is evaluated sinceit significantly outperforms the others on the smalltask (see section 4.3).In a first scenario, we use a word factored TM,trained only on the small in-domain corpus.
Eventhough the training corpus of the baseline TM is onehundred times larger than this small in-domain data,adding the SOUL TM still yields a BLEU increaseof 1.2 point8.
In a second scenario, we increase thetraining corpus for the SOUL, and include parts ofthe out-of-domain data (the WMT part).
The result-ing BLEU score is here slightly worse than the oneobtained with just the in-domain TM, yet deliveringimproved results with the respect to the baseline.In a last attempt, we amended the training regimeof the neural network.
In a fist step, we trained con-ventionally a SOUL model using the same out-of-domain parallel data as before.
We then adapted thismodel by running five additional epochs of the back-propagation algorithm using the in-domain data.
Us-ing this adapted model yielded our best results todate with a BLEU improvement of 1.6 points overthe baseline results.
Moreover, the gains obtainedusing this simple domain adaptation strategy are re-8Note that the in-domain data was already included in thetraining corpus of the baseline TM.spectively of +0.4 and +0.8 BLEU, as comparedwith the small in-domain model and the large out-of-domain model.
These results show that the SOULTM can scale efficiently and that its structure is wellsuited for domain adaptation.5 Related workTo the best of our knowledge, the first work on ma-chine translation in continuous spaces is (Schwenket al, 2007), where the authors introduced the modelreferred here to as the the standard n-gram trans-lation model in Section 2.1.
This model is an ex-tension of the continuous space language modelof (Bengio et al, 2003), the basic unit is the tuple(or equivalently the phrase pair).
The resulting vo-cabulary being too large to be handled by neural net-works without a structured output layer, the authorshad thus to restrict the set of the predicted units to a8k short-list .
Moreover, in (Zamora-Martinez et al,2010), the authors propose a tighter integration of acontinuous space model with a n-gram approach butonly for the target LM.A different approach, described in (Sarikaya etal., 2008), divides the problem in two parts: first thecontinuous representation is obtained by an adapta-tion of the Latent Semantic Analysis; then a Gaus-sian mixture model is learned using this continu-ous representation and included in a hidden Markovmodel.
One problem with this approach is the sep-aration between the training of the continuous rep-resentation on the one hand, and the training of thetranslation model on the other hand.
In comparison,in our approach, the representation and the predic-tion are learned in a joined fashion.Other ways to address the data sparsity issuesfaced by translation model were also proposed in theliterature.
Smoothing is obviously one possibility(Foster et al, 2006).
Another is to use factored lan-guage models, introduced in (Bilmes and Kirchhoff,2003), then adapted for translation models in (Koehnand Hoang, 2007; Crego and Yvon, 2010).
Such ap-proaches require to use external linguistic analysistools which are error prone; moreover, they did notseem to bring clear improvements, even when trans-lating into morphologically rich languages.466 ConclusionIn this paper, we have presented possible ways to usea neural network architecture as a translation model.A first contribution was to produce the first large-scale neural translation model, implemented here inthe framework of the n-gram based models, tak-ing advantage of a specific hierarchical architecture(SOUL).
By considering several decompositions ofthe joint probability of a sentence pair, several bilin-gual translation models were presented and dis-cussed.
As it turned out, using a factorization whichclearly distinguishes the source and target sides, andonly involves word probabilities, proved an effec-tive remedy to data sparsity issues and provided sig-nificant improvements over the baseline.
Moreover,this approach was also experimented within the sys-tems we submitted to the shared translation task ofthe seventh workshop on statistical machine trans-lation (WMT 2012).
These experimentations in alarge scale setup and for different language pair cor-roborate the improvements reported in this article.We also investigated various training regimes forthese models in a cross domain adaptation setting.Our results show that adapting an out-of-domainSOUL TM is both an effective and very fast way toperform bilingual model adaptation.
Adding up allthese novelties finally brought us a 1.6 BLEU pointimprovement over the baseline.
Even though ourexperiments were carried out only within the frame-work of n-gram basedMT systems, using such mod-els in other systems is straightforward.
Future workwill thus aim at introducing them into conventionalphrase-based systems, such as Moses (Koehn et al,2007).
Given that Moses only implicitly uses n-gram based information, adding SOUL translationmodels is expected to be even more helpful.AcknowledgmentsThis work was partially funded by the French Stateagency for innovation (OSEO), in the Quaero Pro-gramme.ReferencesAlexandre Allauzen, Gilles Adda, He?le`ne Bonneau-Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,Guillaume Wisniewski, and Franc?ois Yvon.
2011.LIMSI @ WMT11.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation, pages 309?315, Edinburgh, Scotland.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
JMLR, 3:1137?1155.Jeff A. Bilmes and Katrin Kirchhoff.
2003.
Factoredlanguage models and generalized parallel backoff.
InNAACL ?03: Proceedings of the 2003 Conference ofthe North American Chapter of the Association forComputational Linguistics on Human Language Tech-nology, pages 4?6.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Computational Linguistics, 19(2):263?311.Francesco Casacuberta and Enrique Vidal.
2004.
Ma-chine translation with inferred stochastic finite-statetransducers.
Computational Linguistics, 30(3):205?225.Stanley F. Chen and Joshua Goodman.
1996.
An empiri-cal study of smoothing techniques for language model-ing.
In Proc.
ACL?96, pages 310?318, San Francisco.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.Journal of Machine Learning Research, 12:2493?2537.Josep M. Crego and Jose?
B. Marin?o.
2006.
Improvingstatistical MT by coupling reordering and decoding.Machine Translation, 20(3):199?215.Josep M. Crego and Franc?ois Yvon.
2010.
Factoredbilingual n-gram language models for statistical ma-chine translation.
Machine Translation, pages 1?17.Josep M. Crego, Franc?ois Yvon, and Jose?
B. Marin?o.2011.
N-code: an open-source Bilingual N-gram SMTToolkit.
Prague Bulletin of Mathematical Linguistics,96:49?58.George Foster, Roland Kuhn, and Howard Johnson.2006.
Phrase-table smoothing for statistical machinetranslation.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Processing,pages 53?61, Sydney, Australia.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the IEEE International Conference onAcoustics, Speech and Signal Processing, volume I,pages 181?184, Detroit, Michigan.47Philipp Koehn and Hieu Hoang.
2007.
Factored transla-tion models.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 868?876.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proc.ACL?07, pages 177?180, Prague, Czech Republic.Hong-Kwang Kuo, Lidia Mangu, Ahmad Emami, andImed Zitouni.
2010.
Morphological and syntactic fea-tures for Arabic speech recognition.
In Proc.
ICASSP2010.Thomas Lavergne, Alexandre Allauzen, Hai-Son Le, andFranc?ois Yvon.
2011.
LIMSI?s experiments in do-main adaptation for IWSLT11.
In Proceedings ofthe eight International Workshop on Spoken LanguageTranslation (IWSLT), San Francisco, CA.Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-LucGauvain, and Franc?ois Yvon.
2011a.
Structured out-put layer neural network language model.
In Proceed-ings of ICASSP?11, pages 5524?5527.Hai-Son Le, Ilya Oparin, Abdel Messaoudi, Alexan-dre Allauzen, Jean-Luc Gauvain, and Franc?ois Yvon.2011b.
Large vocabulary SOUL neural network lan-guage models.
In Proceedings of InterSpeech 2011.Xunying Liu, Mark J. F. Gales, and Philip C. Woodland.2011.
Improving lvcsr system combination using neu-ral network language model cross adaptation.
In IN-TERSPEECH, pages 2857?2860.Jose?
B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`de Gispert, Patrick Lambert, Jose?
A.R.
Fonollosa, andMarta R. Costa-Jussa`.
2006.
N-gram-based machinetranslation.
Computational Linguistics, 32(4):527?549.Tomas Mikolov, Stefan Kombrink, Lukas Burget, JanCernocky?, and Sanjeev Khudanpur.
2011.
Extensionsof recurrent neural network language model.
In Proc.of ICASSP?11, pages 5528?5531.Andriy Mnih and Geoffrey E Hinton.
2008.
A scalablehierarchical distributed language model.
In D. Koller,D.
Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-vances in Neural Information Processing Systems 21,volume 21, pages 1081?1088.Jan Niehues, Teresa Herrmann, Stephan Vogel, and AlexWaibel.
2011.
Wider context by using bilingual lan-guage models in machine translation.
In Proceedingsof the Sixth Workshop on Statistical Machine Trans-lation, pages 198?206, Edinburgh, Scotland.
Associa-tion for Computational Linguistics.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30:417?449, Decem-ber.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL ?03: Proc.
ofthe 41st Annual Meeting on Association for Computa-tional Linguistics, pages 160?167.Ruhi Sarikaya, Yonggang Deng, Mohamed Afify, BrianKingsbury, and Yuqing Gao.
2008.
Machine trans-lation in continuous space.
In Proceedings of Inter-speech, pages 2350?2353, Brisbane, Australia.Holger Schwenk, Marta R. Costa-Jussa`, and Jose?
A.R.Fonollosa.
2007.
Smooth bilingual n-gram transla-tion.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 430?438, Prague, Czech Re-public.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech and Language, 21(3):492?518.Yeh W. Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proc.
ofACL?06, pages 985?992, Sidney, Australia.Christoph Tillmann.
2004.
A unigram orientation modelfor statistical machine translation.
In Proceedings ofHLT-NAACL 2004, pages 101?104.Francisco Zamora-Martinez, Maria Jose?
Castro-Bleda,and Holger Schwenk.
2010.
N-gram-based MachineTranslation enhanced with Neural Networks for theFrench-English BTEC-IWSLT?10 task.
In Proceed-ings of the seventh International Workshop on SpokenLanguage Translation (IWSLT), pages 45?52.Richard Zens, Franz Josef Och, and Hermann Ney.
2002.Phrase-based statistical machine translation.
In KI?02: Proceedings of the 25th Annual German Con-ference on AI, pages 18?32, London, UK.
Springer-Verlag.48
