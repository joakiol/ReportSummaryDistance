Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ?
Student Research Workshop, pages 72?79,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsA Dataset for Joint Noun?Noun Compound Bracketing and InterpretationMurhaf FaresDepartment of InformaticsUniversity of Oslomurhaff@ifi.uio.noAbstractWe present a new, sizeable dataset of noun?noun compounds with their syntactic analysis(bracketing) and semantic relations.
Derived fromseveral established linguistic resources, such asthe Penn Treebank, our dataset enables experi-menting with new approaches towards a holisticanalysis of noun?noun compounds, such as joint-learning of noun?noun compounds bracketing andinterpretation, as well as integrating compoundanalysis with other tasks such as syntactic parsing.1 IntroductionNoun?noun compounds are abundant in many lan-guages, and English is no exception.
Accordingto?O S?eaghdha (2008), three percent of all wordsin the British National Corpus (Burnard, 2000,BNC) are part of nominal compounds.
There-fore, in addition to being an interesting linguis-tic phenomenon per se, the analysis of noun?noun compounds is important to other natural lan-guage processing (NLP) tasks such as machinetranslation and information extraction.
Indeed,there is already a nontrivial amount of researchon noun?noun compounds within the field of com-putational linguistics (Lauer, 1995; Nakov, 2007;?O S?eaghdha, 2008; Tratz, 2011, inter alios).As Lauer and Dras (1994) point out, the treat-ment of noun?noun compounds involves threetasks: identification, bracketing and semantic in-terpretation.
With a few exceptions (Girju et al,2005; Kim and Baldwin, 2013), most studies onnoun?noun compounds focus on one of the afore-mentioned tasks in isolation, but these tasks are ofcourse not fully independent and therefore mightbenefit from a joint-learning approach, especiallybracketing and semantic interpretation.Reflecting previous lines of research, most ofthe existing datasets on noun?noun compounds ei-ther include bracketing information or semanticrelations, rarely both.
In this article we presenta fairly large dataset for noun?noun compoundbracketing as well as semantic interpretation.
Fur-thermore, most of the available datasets list thecompounds out of context.
Hence they implic-itly assume that the semantics of noun?noun com-pounds is type-based; meaning that the same com-pound will always have the same semantic rela-tion.
To test this assumption of type-based vs.token-based semantic relations, we incorporate thecontext of the compounds in our dataset and treatcompounds as tokens rather than types.
Lastly, tostudy the effect of noun?noun compound brack-eting and interpretation on other NLP tasks, wederive our dataset from well-established resourcesthat annotate noun?noun compounds as part ofother linguistic structures, viz.
the Wall StreetJournal Section of the Penn Treebank (Marcus etal., 1993, PTB), PTB noun phrase annotation byVadas and Curran (2007), DeepBank (Flickingeret al, 2012), the Prague Czech?English Depen-dency Treebank 2.0 (Haji?c et al, 2012, PCEDT)and NomBank (Meyers et al, 2004).
We thereforecan quantify the effect of compound bracketing onsyntactic parsing using the PTB, for example.In the following section, we review some ofthe existing noun compound datasets.
In ?
3, wepresent the process of constructing a dataset ofnoun?noun compounds with bracketing informa-tion and semantic relations.
In ?
4, we explainhow we construct the bracketing of noun?nouncompounds from three resources and report ?inter-resource?
agreement levels.
In ?
5, we present thesemantic relations extracted from two resourcesand the correlation between the two sets of rela-tions.
In ?
6, we conclude the article and presentan outlook for future work.72Dataset Size Relations BracketingNastase & Szpakowicz 600 30 NoGirju et al 4,500 21 600?O S?eaghdha & Copestake 1,443 6 NoKim & Baldwin12,169 20 NoTratz & Hovy 17,509 43 NoTable 1: Overview of noun compound datasets.Size: type count2 BackgroundThe syntax and semantics of noun?noun com-pounds have been under focus for years, in linguis-tics and computational linguistics.
Levi (1978)presents one of the early and influential stud-ies on noun?noun compounds as a subset of so-called complex nominals.
Levi (1978) definesa set of nine ?recoverably deletable predicates?which express the ?semantic relationship betweenhead nouns and prenominal modifiers?
in complexnominals.
Finin (1980) presented one of the ear-liest studies on nominal compounds in computa-tional linguistics, but Lauer (1995) was among thefirst to study statistical methods for noun com-pound analysis.
Lauer (1995) used the Grolierencyclopedia to estimate word probabilities, andtested his models on a dataset of 244 three-wordbracketed compounds and 282 two-word com-pounds.
The compounds were annotated witheight prepositions which Lauer takes to approxi-mate the semantics of noun?noun compounds.Table 1 shows an overview of some of the exist-ing datasets for nominal compounds.
The datasetsby Nastase and Szpakowicz (2003) and Girju etal.
(2005) are not limited to noun?noun com-pounds; the former includes compounds with ad-jectival and adverbial modifiers, and the latter hasmany noun-preposition-noun constructions.
Thesemantic relations in?O S?eaghdha and Copestake(2007) and Kim and Baldwin (2008) are basedon the relations introduced by Levi (1978) andBarker and Szpakowicz (1998), respectively.
Allof the datasets in Table 1 list the compounds outof context.
In addition, the dataset by Girju et al(2005) includes three-word bracketed compounds,whereas the rest include two-word compoundsonly.
On the other hand, (Girju et al, 2005) is theonly dataset in Table 1 that is not publicly avail-able.1In Table 1 we refer to (Kim and Baldwin, 2008), the otherdataset by Kim and Baldwin (2013), which includes 1,571three-word compounds, is not publicly available.NNPhNNP0Compounds 38,917 29,666Compound types 21,016 14,632Table 2: Noun?noun compounds in WSJ Corpus3 FrameworkThis section gives an overview of our method toautomatically construct a bracketed and semanti-cally annotated dataset of noun?noun compoundsfrom four different linguistic resources.
The con-struction method consists of three steps that cor-respond to the tasks defined by Lauer and Dras(1994): identification, bracketing and semantic in-terpretation.Firstly, we identify the noun?noun compoundsin the PTB WSJ Section using two of the com-pound identification heuristics introduced by Fareset al (2015), namely the so-called syntax-basedNNPhheuristic which includes compounds thatcontain common and proper nouns but excludesthe ones headed by proper nouns, and the syntax-based NNP0heuristic which excludes all com-pounds that contain proper nouns, be it in the headposition or the modifier position.
Table 2 showsthe number of compounds and compound types weidentified using the NNPhand NNP0heuristics.Note that the number of compounds will vary inthe following sections depending on the resourceswe use.Secondly, we extract the bracketing of the iden-tified compounds from three resources: PTB nounphrase annotation by Vadas and Curran (2007),DeepBank and PCEDT.
Vadas and Curran (2007)manually annotated the internal structure of nounphrases (NPs) in PTB which were originally leftunannotated.
However, as is the case with otherresources, Vadas and Curran (2007) annotation isnot completely error-free, as shown by Fares etal.
(2015).
We therefore crosscheck their brack-eting through comparing to those of DeepBankand PCEDT.
The latter two, however, do not con-tain explicit annotation of noun?noun compoundbracketing, but we can ?reconstruct?
the bracket-ing based on the dependency relations assigned inboth resources, i.e.
the logical form meaning rep-resentation in DeepBank and the tectogrammaticallayer (t-layer) in PCEDT.
Based on the bracketingextracted from the three resources, we define thesubset of compounds that are bracketed similarlyin the three resources.
Lastly, we extract the se-73mantic relations of two-word compounds as wellas multi-word bracketed compounds from two re-sources: PCEDT and NomBank.On a more technical level, we use the so-called phrase-structure layer (p-layer) in PCEDTto identify noun?noun compounds, because it in-cludes the NP annotation by Vadas and Curran(2007), which is required to apply the noun?nouncompound identification heuristics by Fares et al(2015).
For bracketing, we also use the PCEDT p-layer, in addition to the dataset prepared by Oepenet al (2016) which includes DeepBank and thePCEDT tectogrammatical layer.
We opted for thedataset by Oepen et al (2016) because they con-verted the tectogrammatical annotation in PCEDTto dependency representation in which the ?set ofgraph nodes is equivalent to the set of surface to-kens.?
For semantic relations, we also use thedataset by Oepen et al (2016) for PCEDT rela-tions and the original NomBank files for Nom-Bank relations.Throughout the whole process we store the datain a relational database with a schema that repre-sents the different types of information, and thedifferent resources from which they are derived.As we will show in ?
4 and ?
5, this set-up allowsus to combine information in different ways andtherefore create ?different?
datasets.4 BracketingNoun?noun compound bracketing can be definedas the disambiguation of the internal structure ofcompounds with three nouns or more.
For exam-ple, we can bracket the compound noon fashionshow in two ways:1.
Left-bracketing: [[noon fashion] show]2.
Right-bracketing: [noon [fashion show]]In this example, the right-bracketing interpretation(a fashion show happening at noon) is more likelythan the left-bracketing one (a show of noon fash-ion).
However, the correct bracketing need not al-ways be as obvious, some compounds can be sub-tler to bracket, e.g.
car radio equipment (Girju etal., 2005).4.1 Data & ResultsAs explained in ?
3, we first identify noun?nouncompounds in the WSJ Corpus, then we extractand map their bracketing from three linguistic re-sources: PCEDT, DeepBank and noun phrase an-notation by Vadas and Curran (2007) (VC-PTB,henceforth).
Even though we can identify 38,917noun?noun compounds in the full WSJ Corpus(cf.
Table 2), the set of compounds that consti-tutes the basis for bracketing analysis (i.e.
the setof compounds that occur in the three resources)is smaller.
First, because DeepBank only an-notates the first 22 Sections of the WSJ Cor-pus.
Second, because not all the noun sequencesidentified as compounds in VC-PTB are treatedas such in DeepBank and PCEDT.
Hence, thenumber of compounds that occur in the three re-sources is 26,500.
Furthermore, three-quarters(76%) of these compounds consist of two nounsonly, meaning that they do not require bracket-ing, which leaves us a subset of 6,244 multi-wordcompounds?we will refer to this subset as thebracketing subset.After mapping the bracketings from the threeresources we find that they agree on the brack-eting of almost 75% of the compounds in thebracketing subset.
Such an agreement level isrelatively good compared to previously reportedagreement levels on much smaller datasets, e.g.Girju et al (2005) report a bracketing agreementof 87% on a set of 362 three-word compounds.Inspecting the disagreement among the three re-sources reveals two things.
First, noun?noun com-pounds which contain proper nouns (NNP) consti-tute 45% of the compounds that are bracketed dif-ferently.
Second, 41% of the differently bracketedcompounds are actually sub-compounds of largercompounds.
For example, the compound con-sumer food prices is left-bracketed in VC-PTB,i.e.
[[consumer food] prices], whereas in PCEDTand DeepBank it is right-bracketed.
This dif-ference in bracketing leads to two different sub-compounds, namely consumer food in VC-PTBand food prices in PCEDT and DeepBank.It is noteworthy that those two observations donot reflect the properties of compounds contain-ing proper nouns or sub-compounds; they only tellus their percentages in the set of differently brack-eted compounds.
In order to study their properties,we need to look at the number of sub-compoundsand compounds containing NNPs in the set ofcompounds where the three resources agree.
Asit turns out, 72% of the compounds containingproper nouns and 76% of the sub-compounds arebracketed similarly.
Therefore when we excludethem from the bracketing subset we do not see asignificant change in bracketing agreement among74???
???
???
????
?NNPh80% 79% 88% 75%NNP078% 75% 90% 74%NNPhw/o sub 82% 82% 86% 75%NNP0w/o sub 81% 77% 90% 74%Table 3: Bracketing agreement ?
?
: DeepBank; ?
:PCEDT; ?
: VC-PTB; NNP0: excl.
proper nouns;NNPh: incl.
proper nouns; w/o sub: excl.
sub-compoundsthe three resources, as shown in the right-most col-umn in Table 3.We report pairwise bracketing agreementamong the three resources in Table 3.
We observehigher agreement level between PCEDT and VC-PTB than the other two pairs; we speculate thatthe annotation of the t-layer in PCEDT might havebeen influenced by the so-called phrase-structurelayer (p-layer) which in turn uses VC-PTB anno-tation.
Further, PCEDT and VC-PTB seem to dis-agree more on the bracketing of noun?noun com-pounds containing NNPs; because when propernouns are excluded (NNP0), the agreement levelbetween PCEDT and VC-PTB increases, but it de-creases for the other two pairs.As we look closer at the compound instanceswhere at least two of the three resources disagree,we find that some instances are easy to classifyas annotation errors.
For example, the compoundNew York streets is bracketed as right-branchingin VC-PTB, but we can confidently say that this aleft-bracketing compound.
Not all bracketing dis-agreements are that easy to resolve though; oneexample where left- and right-bracketing can beaccepted is European Common Market approach,which is bracketed as follows in DeepBank (1) andPCEDT and VC-PTB (2):1.
[[European [Common Market]] approach]2.
[European [[Common Market] approach]]Even though this work does not aim to resolveor correct the bracketing disagreement betweenthe three resources, we will publish a tool thatallows resource creators to inspect the bracketingdisagreement and possibly correct it.5 RelationsNow that we have defined the set of compoundswhose bracketing is agreed-upon in different re-sources, we move to adding semantic relations toCompound Functor NomBank ArgNegligence penalty CAUS ARG3Death penalty RSTR ARG2Staff lawyer RSTR ARG3Government lawyer APP ARG2Table 4: Example compounds with semantic rela-tionsour dataset.
We rely on PCEDT and NomBank todefine the semantic relations in our dataset, whichincludes bracketed compounds from ?
4 as well astwo-word compounds.
However, unlike ?
4, ourset of noun?noun compounds in this section con-sists of the compounds that are bracketed simi-larly in PCEDT and VC-PTB and occur in bothresources.2This set consists of 26,709 compoundsand 14,405 types.PCEDT assigns syntactico-semantic labels, so-called functors, to all the syntactic dependency re-lations in the tectogrammatical layer (a deep syn-tactic structure).
Drawing on the valency theoryof the Functional Generative Description, PCEDTdefines 69 functors for verbs as well as nouns andadjectives (Cinkov?a et al, 2006).3NomBank, onthe other hand, is about nouns only; it assigns rolelabels (arguments) to common nouns in the PTB.In general, NomBank distinguishes between pred-icate arguments and modifiers (adjuncts) whichcorrespond to those defined in PropBank (Kings-bury and Palmer, 2002).4We take both types ofroles to be part of the semantic relations of noun?noun compounds in our dataset.Table 4 shows some examples of noun?nouncompounds annotated with PCEDT functors andNomBank arguments.
The functor CAUS ex-presses causal relationship; RSTR is an under-specified adnominal functor that is used wheneverthe semantic requirements for other functors arenot met; APP expresses appurtenance.
While thePCEDT functors have specific definitions, most ofthe NomBank arguments have to be interpreted inconnection with their predicate or frame.
For ex-2We do not use the intersection of the three resources asin ?
4, because DeepBank does not contribute to the semanticrelations of noun?noun compounds and it limits the size ofour dataset (cf.
?
4).
Nonetheless, given our technical set-upwe can readily produce the set of compounds that occur in thethree resources and are bracketed similarly, and then extracttheir semantic relations from PCEDT and NomBank.3The full inventory of functors is available onhttps://ufal.mff.cuni.cz/pcedt2.0/en/functors.html (visited on 22/04/2016).4See Table 2 in Meyers (2007, p. 90) for an overview ofadjunct roles in NomBank.75ample, ARG3 of the predicate penalty in Table 4describes crime whereas ARG3 of the predicatelawyer describes rank.
Similarly, ARG2 in penaltydescribes punishment, whereas ARG2 in lawyerdescribes beneficiary or consultant.5.1 Data & ResultsGiven 26,709 noun?noun compounds, we con-struct a dataset with two relations per compound:a PCEDT functor and a NomBank argument.
Theresulting dataset is relatively large compared to thedatasets in Table 1.
However, the largest dataset inTable 1, by Tratz and Hovy (2010), is type-basedand does not include proper nouns.
The size of ourdataset becomes 10,596 if we exclude the com-pounds containing proper nouns and only countthe types in our dataset; this is still a relativelylarge dataset and it has the important advantageof including bracketing information of multi-wordcompounds, inter alia.Overall, the compounds in our dataset are an-notated with 35 functors and 20 NomBank argu-ments, but only twelve functors and nine Nom-Bank arguments occur more than 100 times inthe dataset.
Further, the most frequent NomBankargument (ARG1) accounts for 60% of the data,and the five most frequent arguments account for95%.
We see a similar pattern in the distribu-tion of PCEDT functors, where 49% of the com-pounds are annotated with RSTR (the least spe-cific adnominal functor in PCEDT).
Further, thefive most frequent functors account for 89% of thedata (cf.
Table 5).
Such distribution of relations isnot unexpected because according to Cinkov?a etal.
(2006), the relations that cannot be expressedby ?semantically expressive?
functors usually re-ceive the functor PAT, which is the second mostfrequent functor.
Furthermore, Kim and Baldwin(2008) report that 42% of the compounds in theirdataset are annotated as TOPIC, which appearsclosely related to ARG1 in NomBank.In theory, some of the PCEDT functors andNomBank arguments express the same type ofrelations.
We therefore show the ?correlation?between PCEDT functors and NomBank argu-ments in Table 5.
The first half of the tablemaps PCEDT functors to NomBank arguments,and the second half shows the mapping from Nom-Bank to PCEDT.
Due to space limitations, thetable only includes a subset of the relations?the most frequent ones.
The underlined num-bers in Table 5 indicate the functors and Nom-Bank arguments that are semantically compara-ble; for example, the temporal and locative func-tors (TWHEN, THL, TFRWH and LOC) intuitivelycorrespond to the temporal and locative modifiersin NomBank (ARGM-TMP and ARGM-LOC), andthis correspondence is also evident in the figuresin Table 5.
The same applies to the functor AUTH(authorship) which always maps to the NomBankargument ARG0 (agent).
However, not all ?the-oretical similarities?
are necessarily reflected inpractice, e.g.
AIM vs. ARGM-PNC in Table 5 (bothexpress purpose).
NomBank and PCEDT are twodifferent resources that were created with differentannotation guidelines and by different annotators,and therefore we cannot expect perfect correspon-dence between PCEDT functors and NomBank ar-guments.PCEDT often assigns more than one functorto different instances of the same compound.
Infact, around 13% of the compound types were an-notated with more than one functor in PCEDT,whereas only 1.3% of our compound types areannotated with more than one argument in Nom-Bank.
For example, the compound takeover bid,which occurs 28 times in our dataset, is annotatedwith four different functors in PCEDT, includingAIM and RSTR, whereas in NomBank it is alwaysannotated as ARGM-PNC.
This raises the questionwhether the semantics of noun?noun compoundsvaries depending on their context, i.e.
token-basedvs.
type-based relations.
Unfortunately we can-not answer this question based on the variation inPCEDT because its documentation clearly statesthat ?
[t]he annotators tried to interpret complexnoun phrases with semantically expressive func-tors as much as they could.
This annotation is,of course, very inconsistent.
?5Nonetheless, ourdataset still opens the door to experimenting withlearning PCEDT functors, and eventually deter-mining whether the varied functors are mere in-consistencies or there is more to this than meetsthe eye.6 Conclusion & Future WorkIn this article we presented a new noun?noun com-pound dataset constructed from different linguis-tic resources, which includes bracketing informa-tion and semantic relations.
In ?
4, we explained5https://ufal.mff.cuni.cz/pcedt2.0/en/valency.html (visited on 22/04/2016).76HHHHPNARG1 ARG2 ARG0 ARG3 M-LOC M-MNR M-TMP M-PNC Count FreqRSTR 0.60 0.12 0.08 0.10 0.03 0.03 0.01 0.01 12992 48.64PAT 0.89 0.05 0.01 0.03 0.01 0.01 0.01 3867 14.48APP 0.42 0.37 0.17 0.01 0.03 0.00 0.00 0.00 3543 13.27REG 0.75 0.09 0.07 0.07 0.00 0.01 0.00 0.00 2176 8.15ACT 0.46 0.03 0.48 0.01 0.01 0.00 1286 4.81LOC 0.16 0.20 0.09 0.01 0.54 979 3.67TWHEN 0.12 0.04 0.00 0.01 0.81 367 1.37AIM 0.65 0.12 0.06 0.08 0.00 0.00 0.05 284 1.06ID 0.39 0.30 0.27 0.04 0.00 256 0.96MAT 0.86 0.09 0.01 0.02 136 0.51NE 0.32 0.46 0.13 0.02 0.06 132 0.49ORIG 0.20 0.19 0.13 0.37 0.06 0.01 0.01 114 0.43MANN 0.23 0.07 0.01 0.04 0.65 83 0.31MEANS 0.45 0.09 0.04 0.12 0.14 0.11 56 0.21EFF 0.60 0.18 0.11 0.04 0.04 55 0.21AUTH 1.00 49 0.18BEN 0.45 0.35 0.03 0.17 40 0.15THL 0.03 0.03 0.95 38 0.14ARG1 ARG2 ARG0 ARG3 M-LOC M-MNR M-TMP M-PNCRSTR 0.50 0.40 0.38 0.76 0.37 0.79 0.27 0.66PAT 0.22 0.05 0.02 0.06 0.02 0.07 0.13APP 0.09 0.34 0.22 0.02 0.09 0.00 0.01 0.01REG 0.10 0.05 0.05 0.08 0.01 0.02 0.01 0.07ACT 0.04 0.01 0.23 0.00 0.02 0.01LOC 0.01 0.05 0.03 0.00 0.47TWHEN 0.00 0.00 0.00 0.00 0.58AIM 0.01 0.01 0.01 0.01 0.00 0.00 0.09ID 0.01 0.02 0.03 0.01 0.00MAT 0.01 0.00 0.00 0.00NE 0.00 0.02 0.01 0.00 0.01ORIG 0.00 0.01 0.01 0.02 0.01 0.00 0.01MANN 0.00 0.00 0.00 0.00 0.10MEANS 0.00 0.00 0.00 0.00 0.01 0.01EFF 0.00 0.00 0.00 0.00 0.01AUTH 0.02BEN 0.00 0.00 0.00 0.00THL 0.00 0.00 0.07Count 15811 3779 2701 1767 1131 563 510 149Freq 59.20 14.15 10.11 6.62 4.23 2.11 1.91 0.56Table 5: Correlation between NomBank arguments and PCEDT functorsthe construction of a set of bracketed multi-wordnoun?noun compounds from the PTB WSJ Cor-pus, based on the NP annotation by Vadas and Cur-ran (2007), DeepBank and PCEDT.
In ?
5, we con-structed a variant of the set in ?
4 whereby eachcompound is assigned two semantic relations, aPCEDT functor and NomBank argument.
Ourdataset is the largest data set that includes bothcompound bracketing and semantic relations, andthe second largest dataset in terms of the num-ber of compound types excluding compounds thatcontain proper nouns.Our dataset has been derived from different re-sources that are licensed by the Linguistic DataConsortium (LDC).
Therefore, we are investigat-ing the possibility of making our dataset publiclyavailable in consultation with the LDC.
Otherwisethe dataset will be published through the LDC.In follow-up work, we will enrich our datasetby mapping the compounds in our dataset to thedatasets by Kim and Baldwin (2008) and Tratz andHovy (2010); all of the compounds in the formerand some of the compounds in the latter are ex-tracted from the WSJ Corpus.
Further, we will ex-periment with different classification and rankingapproaches to bracketing and semantic interpre-tation of noun?noun compounds using differentcombinations of relations.
We will also study theuse of machine learning models to jointly bracketand interpret noun?noun compounds.
Finally, weaim to study noun?noun compound identification,bracketing and interpretation in an integrated set-up, by using syntactic parsers to solve the identifi-cation and bracketing tasks, and semantic parsersto solve the interpretation task.77Acknowledgments.
The author wishes to thankStephan Oepen and Erik Velldal for their helpfulassistance and guidance, as well as Michael Rothand the three anonymous reviewers for thought-ful comments.
The creation of the new datasetwouldn?t have been possible without the efforts ofthe resource creators from which the dataset wasderived.ReferencesKen Barker and Stan Szpakowicz.
1998.
Semi-Automatic Recognition of Noun Modifier Relation-ships.
In Proceedings of the 17th International Con-ference on Computational Linguistics and the 36thMeeting of the Association for Computational Lin-guistics, page 96 ?
102, Montreal, Quebec, Canada.Lou Burnard.
2000.
Reference guide for the BritishNational Corpus version 1.0.Silvie Cinkov?a, Jan Haji?c, Marie Mikulov?a, Lu-cie Mladov?a, Anja Nedolu?zko, Petr Pajas, JarmilaPanevov?a, Ji?r??
Semeck?y, Jana?Sindlerov?a, JosefToman, Zde?nka Ure?sov?a, and Zden?ek?Zabokrtsk?y.2006.
Annotation of English on the tectogrammati-cal level: reference book.
Technical report, CharlesUniversity, Prague.
version 1.0.1.Murhaf Fares, Stephan Oepen, and Erik Velldal.
2015.Identifying Compounds: On The Role of Syntax.
InInternational Workshop on Treebanks and LinguisticTheories), page 273 ?
283, Warsaw, Poland.Timothy Wilking Finin.
1980.
The Semantic Interpre-tation of Compound Nominals.
PhD thesis, Univer-sity of Illinois at Urbana-Champaign.Dan Flickinger, Yi Zhang, and Valia Kordoni.
2012.DeepBank.
A dynamically annotated treebank of theWall Street Journal.
In Proceedings of the 11th In-ternational Workshop on Treebanks and LinguisticTheories, page 85 ?
96, Lisbon, Portugal.
Edic?
?oesColibri.Roxana Girju, Dan Moldovan, Marta Tatu, and DanielAntohe.
2005.
On the semantics of noun com-pounds.
Computer Speech & Language, 19(4):479 ?496.Jan Haji?c, Eva Haji?cov?a, Jarmila Panevov?a, PetrSgall, Ond?rej Bojar, Silvie Cinkov?a, Eva Fu?c?
?kov?a,Marie Mikulov?a, Petr Pajas, Jan Popelka, Ji?r?
?Semeck?y, Jana?Sindlerov?a, Jan?St?ep?anek, JosefToman, Zde?nka Ure?sov?a, and Zden?ek?Zabokrtsk?y.2012.
Announcing Prague Czech-English Depen-dency Treebank 2.0.
In Proceedings of the 8th In-ternational Conference on Language Resources andEvaluation, page 3153 ?
3160, Istanbul, Turkey.Su Nam Kim and Timothy Baldwin.
2008.
Standard-ised Evaluation of English Noun Compound Inter-pretation.
In Proceedings of the LREC Workshop:Towards a Shared Task for Multiword Expressions,page 39 ?
42, Marrakech, Morocco.Su Nam Kim and Timothy Baldwin.
2013.
A lexi-cal semantic approach to interpreting and bracketingEnglish noun compounds.
Natural Language Engi-neering, 19(03):385 ?
407.Paul Kingsbury and Martha Palmer.
2002.
From Tree-Bank to PropBank.
In Proceedings of the 3rd In-ternational Conference on Language Resources andEvaluation, page 1989 ?
1993, Las Palmas, Spain.Mark Lauer and Mark Dras.
1994.
A probabilisticmodel of compound nouns.
In Proceedings of the7th Australian Joint Conference on AI, page 474 ?481, Armidale, Australia.Mark Lauer.
1995.
Designing Statistical LanguageLearners.
Experiments on Noun Compounds.
Doc-toral dissertation, Macquarie University, Sydney,Australia.Judith N Levi.
1978.
The syntax and semantics of com-plex nominals.
Academic Press.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpora of English.
The Penn Treebank.
Computa-tional Linguistics, 19:313 ?
330.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
Annotating noun ar-gument structure for NomBank.
In Proceedingsof the 4th International Conference on LanguageResources and Evaluation, page 803 ?
806, Lisbon,Portugal.Adam Meyers.
2007.
Annotation guidelines forNomBank-noun argument structure for PropBank.Technical report, New York University.Preslav Ivanov Nakov.
2007.
Using the Web as an Im-plicit Training Set: Application to Noun CompoundSyntax and Semantics.
Doctoral dissertation, EECSDepartment, University of California, Berkeley.Vivi Nastase and Stan Szpakowicz.
2003.
Explor-ing Noun-Modifier Semantic Relations.
In Fifth In-ternational Workshop on Computational Semantics,page 285 ?
301.Diarmuid?O S?eaghdha and Ann Copestake.
2007.Co-occurrence Contexts for Noun Compound In-terpretation.
In Proceedings of the Workshop onA Broader Perspective on Multiword Expressions,page 57 ?
64, Prague, Czech Republic.
Associationfor Computational Linguistics.Diarmuid?O S?eaghdha.
2008.
Learning compoundnoun semantics.
Technical Report UCAM-CL-TR-735, University of Cambridge, Computer Labora-tory, Cambridge, UK.78Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,Daniel Zeman, Silvie Cinkov?a, Dan Flickinger,Jan Haji?c, Angelina Ivanova, and Zde?nka Ure?sov?a.2016.
Towards Comparability of Linguistic GraphBanks for Semantic Parsing.
In Proceedings ofthe 10th International Conference on Language Re-sources and Evaluation (LREC), page 3991 ?
3995,Portoro?z, Slovenia.
European Language ResourcesAssociation.Stephen Tratz and Eduard Hovy.
2010.
A taxonomy,dataset, and classifier for automatic noun compoundinterpretation.
In Proceedings of the 48th Meeting ofthe Association for Computational Linguistics, page678 ?
687, Uppsala, Sweden.Stephen Tratz.
2011.
Semantically-enriched parsingfor natural language understanding.
Doctoral dis-sertation, University of Southern California.David Vadas and James Curran.
2007.
Adding NounPhrase Structure to the Penn Treebank.
In Pro-ceedings of the 45th Meeting of the Association forComputational Linguistics, page 240 ?
247, Prague,Czech Republic.79
