Workshop on Humans and Computer-assisted Translation, pages 47?56,Gothenburg, Sweden, 26 April 2014.c?2014 Association for Computational LinguisticsBeyond Linguistic Equivalence.
An Empirical Study of TranslationEvaluation in a Translation Learner CorpusMihaela Vela Anne-Kathrin SchumannDepartment of Applied Linguistics, Translation and InterpretingSaarland University, Saarbr?cken, Germany{m.vela, anne.schumann, a.wurm}@mx.uni-saarland.deAndrea WurmAbstractThe realisation that fully automatic trans-lation in many settings is still far fromproducing output that is equal or superiorto human translation has lead to an in-tense interest in translation evaluation inthe MT community.
However, research inthis field, by now, has not only largely ig-nored the tremendous amount of relevantknowledge available in a closely relateddiscipline, namely translation studies, butalso failed to provide a deeper understand-ing of the nature of "translation errors" and"translation quality".
This paper presentsan empirical take on the latter concept,translation quality, by comparing humanand automatic evaluations of learner trans-lations in the KOPTE corpus.
We willshow that translation studies provide so-phisticated concepts for translation qual-ity estimation and error annotation.
More-over, by applying well-established MTevaluation scores, namely BLEU and Me-teor, to KOPTE learner translations thatwere graded by a human expert, we hopeto shed light on properties (and potentialshortcomings) of these scores.1 Translation quality assessmentIn recent years, researchers in the field of MTevaluation have proposed a large variety of meth-ods for assessing the quality of automatically pro-duced translations.
Approaches range from fullyautomatic quality scoring to efforts aimed at thedevelopment of "human" evaluation scores that tryto exploit the (often tacit) linguistic knowledge ofhuman evaluators.
The criteria according to whichquality is estimated often include adequacy, thedegree of meaning preservation, and fluency, tar-get language correctness (Callison-Burch et al.,2007).
The goals of both "human" evaluation andfully automatic quality scoring are manifold andcover system optimisation as well as benchmark-ing and comparison.In translation studies, the scientific (and pre-scientific) discussion on how to assess the qualityof human translations has been going on for cen-turies.
In recent years, the development of appro-priate concepts and tools has become even morevital to the discipline due to the pressing needsof the language industry.
However, different fromthe belief, typical to MT, that the "goodness" of atranslation can be scored on the basis of linguisticcriteria alone, the notion of "translation quality",in translation studies, has assumed a multi-facetedshape, distancing itself from a simple strive forequivalence and embracing concepts such as func-tional, stylistic and pragmatic appropriateness aswell as textual coherence.
In this section, we pro-vide an overview over approaches to translationquality assessment developed in MT and transla-tion studies to specify how "quality" is being de-fined in both fields and which methods and fea-tures are used.
Due to the amount of availableliterature, this overview is necessarily incomplete,but still insightful with respect to differences andcommonalities between MT and human transla-tion evaluation.1.1 Automatic MT quality scoresMT output is usually evaluated by automaticlanguage-independent metrics which can be ap-plied to any language produced by an MT sys-tem.
The use of automatic metrics for MT eval-uation is legitimate, since MT systems deal withlarge amounts of data, on which manual evaluationwould be very time-consuming and expensive.Automatic metrics typically compute the close-ness (adequacy) of a "hypothesis" to a "reference"translation and differ from each other by how thiscloseness is measured.
The most popular MT eval-47uation metrics are IBM BLEU (Papineni et al.,2002) and NIST (Doddington, 2002) which areused not only for tuning MT systems, but also asevaluation metrics for shared tasks, such as theWorkshop on Statistical Machine Translation (Bo-jar et al., 2013).IBM BLEU uses n-gram precision by match-ing machine translation output against one or morereference translations.
It accounts for adequacyand fluency by calculating word precision, respec-tively the n-gram precision.
In order to deal withthe over generation of common words, precisioncounts are clipped, meaning that a reference wordis exhausted after it is matched.
This is then themodified n-gram precision.
For N=4 the modifiedn-gram precision is calculated and the results arecombined by using the geometric mean.
Instead ofrecall, the brevity penalty (BP) is used.
It penal-izes candidate translations which are shorter thanthe reference translations.The NIST metric is derived from IBM BLEU.The NIST score is the arithmetic mean of modi-fied n-gram precision for N=5 scaled by BP.
Addi-tionally, NIST also considers the information gainof each n-gram, giving more weight to more infor-mative (less frequent) n-grams and less weight toless informative (more frequent) n-grams.Another often used machine translation eval-uation metric is Meteor (Denkowski and Lavie,2011).
Different from IBM BLEU and NIST, Me-teor evaluates a candidate translation by calcu-lating precision and recall on the unigram leveland combining them into a parametrized harmonicmean.
The result from the harmonic mean is thenscaled by a fragmentation penalty which penalizesgaps and differences in word order.Besides these evaluation metrics, several othermetrics are sometimes used for the evaluationof MT output.
Some of these are the WER(word error-rate) metric based on the Levens-thein distance (Levenshtein, 1966), the position-independent error rate metric PER (Tillmann etal., 1997) and the translation edit rate metricTER (Snover et al., 2006) with its newer versionTERp (Snover et al., 2009).1.2 Human MT quality evaluationHuman evaluation of MT output is performed indifferent ways.
The most frequently used evalua-tion method seems to be a simple ranking of trans-lated sentences by a "reasonable number of eval-uators" (Farr?s et al., 2010).
According to Birchet al.
(2013), this form of evaluation was used,among others, during the last STATMT workshopsand can thus be considered rather popular.
AP-PRAISE (Federmann, 2012) is a tool that can beused for such as task, since it allows for the man-ual ranking of sentences, quality estimation, errorannotation and post-editing.Other forms of evaluation, however, exist.
Forexample, Birch et al.
(2013) propose HMEANT,an evaluation score based on MEANT (Lo andWu, 2011), a semi-automatic MT quality scorethat measures the degree of meaning preservationby comparing verb frames and semantic roles ofhypothesis translations to their respective coun-terparts in the reference translation(s).
Unfor-tunately, Birch et al.
(2013) report difficulty inproducing coherent role alignments between hy-potheses and translations, a problem that affectsthe final HMEANT score calculation.
This, how-ever, seems hardly surprising given the difficultyof the annotation task (although, following the au-thors?
description, some familiarity of the anno-tators with the linguistic key concepts can be as-sumed) and the fact that guidelines and trainingare meant to be minimal.Another (indirect) human evaluation method forMT that is also employed for error analysis arereading comprehension tests (e.g.
Maney et al.
(2012), Weiss and Ahrenberg (2012)).
More-over, HTER (Snover et al., 2006) is a TER-basedrepair-oriented metric which uses human annota-tors (the only apparent qualificational requirementbeing fluency in the target language) to generate"targeted" reference translations by post-editingthe MT output or the existing reference trans-lations, following the goal to find the shortestpath between the hypothesis and a "correct" refer-ence.
Snover et al.
(2006) report a high correlationbetween evaluation with HTER and traditional hu-man adequacy and fluency judgements.
Last butnot least, Somers (2011) mentions other repair-oriented measures such as post-editing effort mea-sured by the amount of key-strokes or time spenton producing a "correct" translation on the basisof MT output.1.3 The notion of quality in translationstudiesDiscussions of translation "quality", in translationstudies, for a long time focused on equivalence48which, in its oldest and simplest form, used toecho adequacy as understood by today?s MT re-searchers: "good" translation was viewed as anoptimal compromise between meaning preserva-tion and target language correctness, which wasespecially relevant to the translation of religioustexts.
For example, Ku?maul (2000) emphaticallycites Martin Luther?s famous Bible translation intoGerman as an example of "good" translation be-cause Luther, according to his own testimony andfollowing his reformative ambition, focused onproducing fluent, easily understandable text ratherthan mimicking the linguistic structures of the He-brew, Aramaic and Greek originals (see also Win-dle and Pym (2011) for a further discussion).More recent work in translation studies hasabandoned one-dimensional views of the relationbetween source and target text and postulates that,depending on the communicative context withinand for which a translation is produced, this re-lation can vary greatly.
That is, the degree of lin-guistic or semantic "fidelity" of a good translationtowards the source text depends on functional cri-teria.
This view is echoed in the concepts of "pri-mary vs. secondary", "documentary vs. instru-mental" and "covert vs. overt" translation (H?nig,2003).
The consequence of this shift in paradigmsis that, since different translation strategies maybe appropriately adopted in different situations,evaluation criteria become essentially dependenton the function that the translation is going to playin the target language and culture.
This view ismost prominently advocated by the so-called sko-pos theory (cf.
Dizdar (2003)).
Translation errors,then, are not just simple violations of the targetlanguage system or outright failures to translatewords or segments, but violations of the transla-tion task that can manifest themselves on all levelsof text production (Nord, 2003).
It is importantto point out that, in this framework, linguistic er-rors are just one type of error covering not onlyone of the favourite MT error categories, namelyun- and mistranslated words (compare, for ex-ample, Stymne and Ahrenberg (2012), Weiss andAhrenberg (2012), Popovi?c et al.
(2013)), but alsophraseological, idiomatic, syntactic, grammatical,modal, temporal, stylistic, cohesion and otherkinds of errors.
Moreover, translation-specific er-rors occur when the translation does not fulfill itsfunction because of pragmatic (e.g.
text-type spe-cific forms of address), cultural (e.g.
text con-ventions, proper names, or other conventions) orformal (e. g. layout) defects (Nord, 2003).
De-pending on the appropriate translation strategy fora given translation task, these error types may beweighted differently.
Furthermore, the commu-nicative and functional view on translation alsodictates a change in the concept of equivalencewhich is no longer considered to be adequatelydescribed by the notions of "meaning preserva-tion" or "fidelity", but becomes dependent on aes-thetic, connotational, textual, communicative, sit-uational, functional and cognitive aspects (for adetailed discussion see Horn-Helf (1999)).
In MTevaluation, most of these aspects have not yet oronly in part been considered.Last but not least, the translation industry hasdeveloped normative standards and proofreadingschemes.
For example, the DIN EN 15038:2006-08 (Deutsches Institut f?r Normung, 2006) dis-cusses translation errors, quality management andqualificational requirements for translators andproofreaders, while the SAE J2450 standard (So-ciety of Automotive Engineers, 2005) presents aweighted "translation quality metric".
An appli-cation perspective is given by Mertin (2006) whodiscusses translation quality management proce-dures in a big automotive company and, amongother things, develops a weighted translation errorscheme for proofreading.1.4 DiscussionThe above discussion shows that, while the objectof evaluation is the same for both MT and trans-lation studies, namely translation, the differencesbetween evaluation approaches developed in bothfields are considerable.
Most importantly, in trans-lation studies, translation evaluation is consideredan expert task for which fluency in one or severallanguages is certainly not enough, but for whichtranslation-specific expert knowledge is required.Another important distinction is that evaluation,again in translation studies, is normally not car-ried out on the sentence level, since sentences areusually split up into several "units of translation"and can certainly contain more than one "trans-lation problem".
Consequently, the popular MTpractice of ranking whole sentences according tosome automatic score, by anonymous evaluatorsor even users of Amazon Turk (e.g.
in the intro-duction to Bojar et al.
(2013)), from a translationstudies point of view, is unlikely to provide reason-49able evaluations.
Last but not least, the MT com-munity?s strive for adequacy or meaning preser-vation does not match the notions of weightingtranslation errors, of adopting different translationstrategies and, consequently, does not fit the com-plicated source/target text relations that have beenacknowledged by translation studies.
Evaluationmethods that are based on simple measures of lin-guistic equality such as n-gram overlap (BLEU)or, just slightly more complicated, the preservationof syntactic frames and semantic roles (MEANT)fail to provide straightforward criteria for distin-guishing between legitimate and illegitimate vari-ation.
Moreover, semantic and pragmatic criteriaas well as the notion of "reference translation" re-main, at best, rather unclear.On the other hand, the MT community hasrecognised translation evaluation as an unresolvedresearch problem.
For example, Birch et al.
(2013)state that ranking judgements are difficult to gen-eralise, while Callison-Burch et al.
(2007) carryout extensive correlation tests of a whole rangeof automatic MT evaluation metrics in compar-ison to human judgements, showing that BLEUdoes not rank highest, but still remains in the topsegment.
It still needs to be shown how MT re-search can benefit from more sophisticated evalu-ation measures and whether all the parameters thatare considered relevant to the evaluation of humantranslations are relevant for MT usage scenarios,too.
In the remainder of this paper, we present astudy on how much and possibly for which reasonsautomatic MT evaluation scores (namely BLEUand Meteor) differ from translation expert qualityjudgements on extracts of a French-German trans-lation learner corpus.2 The KOPTE corpus2.1 General corpus designThe KOPTE project (Wurm, 2013) was designedto enable research on translation evaluation ina university training course (master?s level) fortranslators and to enlighten students?
translationproblems as well as their problem solving strate-gies.
To achieve this goal, a corpus of studenttranslations was compiled.
The corpus consists ofseveral translations of the same source texts pro-duced by student translators in a classroom set-ting.
As a whole, it covers 985 translations of77 source texts amounting to a total of 318,467tokens.
Source texts were taken from Frenchnewspapers and translated into German in classover a span of several years, the translation briefcalling for a ready-to-publish text to be printedin a German national newspaper.
Consequently,all translation tasks include the use of idiomaticlanguage, explanations of culture-specific items,changes in the explicitness of macrotextual cohe-sive elements, etc.12.2 Annotation of translation features andtranslation evaluation in KOPTEStudent translations were evaluated by one of theauthors, an experienced translation teacher, withthe aim of giving feedback to students.
All trans-lations were graded and errors as well as goodsolutions were marked in the text according to afine-grained evaluation scheme.
In this scheme,the weight of evaluated items is indicated throughnumbers ranging from plus/minus 1 (minor) toplus/minus 8 (major).
Based on these evaluations,each translation was assigned a final grade accord-ing to the German grading system on a scale rang-ing from 1 ("very good") to 6 ("highly erroneous")with in-between intervals at the levels of .0, .3 and.7.
To calculate this grade, positive and negativeevaluations were summed up separately, before thenegative score was subtracted from the positiveone.
A score of around zero corresponds to thegrade "good" (=2), to achieve "very good" (=1) thestudent needs a surplus of positive evaluations.The evaluation scheme based on which studenttranslations are graded is divided into externaland internal factors.
External characteristics de-scribe the communicative situation given by thesource text and the translation brief (author, re-cipient, medium, location, time).
Internal fac-tors, on the other hand, comprise eight categories:form, structure, cohesion, stylistics/register, gram-mar, lexis/semantics, translation-specific prob-lems, function.
These categories are containers formore fine-grained criteria which can be applied tosegments of the (source or target) text or even tothe whole text, depending on the nature of the cri-terion.
Some internal subcriteria of the scheme aresummarised in Table 1.
A quantitative analysis oferror types in KOPTE shows that semantic/lexicalerrors are by far the most common error in the stu-dent translations (Wurm, 2013).Evaluations in KOPTE were carried out by just1More information about KOPTE is available fromhttp://fr46.uni-saarland.de/index.php?id=3702&L=%2524L.50one evaluator for the reason that, in a classroomsetting, multiple evaluations are not feasible.
Al-though multiple evaluations would have been con-sidered highly valuable, the data available fromKOPTE was evaluated by an experienced trans-lation scholar with long-standing experience inteaching translation.
Moreover, the evaluationscheme is much more detailed than error annota-tion schemes that are normally described in the lit-erature and it is theoretically well-motivated.
Ananalysis of the median grades in our data sample(compare Tables 2?4) shows that grading variesonly slightly between different texts, consideringthe maximum variation potential ranging from 1to 6, and thus can be considered consistent.Criteria Examples ofsubcriteriaauthor, recipients,medium, topic, ?location, timeform paragraphs, formattingstructure thematic, progression,macrostructure, illustrationscohesion reference, connectionsstylistics style, genregrammar determiners, modality, syntaxsemantics textual semantics, idioms,numbers, terminologytranslation erroneous sourceproblems text, proper names, culture-specificitems, ideology, math.
units,pragmatics, allusionsfunction goal dependenceTable 1: Internal evaluation criteria in the KOPTE annotationscheme.3 ExperimentsThe goal of our experiments was to studywhether the human translation expert judgementsin KOPTE can be mimicked using simple au-tomatic quality metrics as used in MT, namelyBLEU and Meteor.
More specifically, we aim at:?
studying how automatic evaluation scores re-late to fine-grained human expert evaluations,?
investigating whether a higher number of ref-erences improves the automatic scores andwhy (or why not),?
examining whether a higher number of ref-erences provides more reliable evaluationscores as measured by an improved correla-tion with the human expert judgments.In order to study the behaviour of automatic MTevaluation scores, we conducted three experimentsby applying IBM BLEU (Papineni et al., 2002)and Meteor 1.4 (Denkowski and Lavie, 2011) toa sample of KOPTE translations that were pro-duced by translation students preparing for theirfinal master?s exams.
Scores were calculated onthe complete texts.
To evaluate the overall perfor-mance of the automatic evaluation scores on thesetexts, we calculated Kendall?s rank correlation co-efficient for each text following the procedure de-scribed in Sachs and Hedderich (2009).
Correla-tions were calculated for:?
the human expert grades and BLEU scoresfor each translation,?
the human expert grades and Meteor scoresfor each translation,?
BLEU and Meteor scores for each transla-tion.3.1 Experimental setup and resultsIn a first experiment, we applied the automaticevaluation scores to the source texts given in Ta-ble 2, choosing, for each text, the student transla-tion with the best human grade as reference trans-lation.
The median human grades as well as meanBLEU and Meteor and correlation scores obtainedfor each text (excluding the reference translation)are included in Table 2.
In a second experiment,we repeated this procedure, however, using a setof three reference translations.
Results are givenin Table 3.
Finally, in a last experiment we usedfive reference translations selected according totheir human expert grade (Table 4).
In both steps,source texts for which less than four hypotheseswere available were excluded from the data sets.3.2 DiscussionThe tables show that in the first experiment a set of152 translations was evaluated, whereas in the sec-ond and third experiment these numbers were re-duced to 108 and 68 respectively due to the selec-tion of more references.
The human expert eval-uations rated most of these translations at least asacceptable, as can be seen from the median gradefor each experiment which was 2.3 in the first ex-periment and consecutively decreased to 3.0 forthe third experiment, again due to the selectionof more "good" translations as references.
The51Source Human trans./ Median Mean Mean Correlation Correlation Correlationtext source text grades BLEU Meteor Human-BLEU Human-Meteor BLEU-MeteorAT001 7 2.
7 0.
15 0.
33 ?0.
39 ?0.
73 0.
24AT002 12 2.
3 0.
15 0.
35 ?0.
20 ?0.
43 0.
49AT004 12 2.
7 0.
19 0.
37 0.
14 0.
11 0.
63AT005 12 2.
3 0.
20 0.
36 0.
32 0.
45 0.
45AT008 10 2.
15 0.
23 0.
38 ?0.
43 ?0.
29 0.
78AT010 11 2.
7 0.
25 0.
41 0.
06 ?0.
10 0.
56AT012 9 2.
0 0.
22 0.
40 ?0.
30 ?0.
36 0.
50AT015 5 2.
0 0.
11 0.
28 0.
36 0.
12 0.
60AT017 7 2.
3 0.
22 0.
38 ?0.
20 0.
06 0.
71AT021 4 3.
0 0.
18 0.
39 ?0.
55 ?0.
55 1.
00AT023 6 2.
3 0.
22 0.
38 0.
50 ?0.
07 ?0.
20AT025 4 2.
15 0.
13 0.
36 0.
33 0.
0 0.
00AT026 21 3.
0 0.
12 0.
26 ?0.
19 ?0.
35 0.
67AT039 13 3.
0 0.
10 0.
29 ?0.
08 0.
03 0.
49AT052 7 2.
0 0.
17 0.
31 ?0.
32 0.
05 0.
00AT053 7 2.
3 0.
18 0.
32 0.
62 0.
39 0.
33AT059 5 2.
0 0.
24 0.
36 0.
00 0.
22 0.
80Table 2: Source texts, number of human translations per source text, median of the obtained grade per source text, mean of theBLEU and Meteor scores per source text and Kendall?s rank correlation coefficients for the first experiment.Source Human trans./ Median Mean Mean Correlation Correlation Correlationtext source text grades BLEU Meteor Human?BLEU Human-Meteor BLEU-MeteorAT001 5 3.
0 0.
17 0.
36 ?0.
12 0.
36 0.
60AT002 10 2.
3 0.
17 0.
36 ?0.
14 0.
05 0.
38AT004 10 2.
85 0.
20 0.
37 0.
39 0.
16 0.
51AT005 10 2.
3 0.
20 0.
40 ?0.
10 0.
05 0.
47AT008 8 2.
5 0.
25 0.
45 ?0.
67 ?0.
15 0.
00AT010 9 2.
7 0.
23 0.
41 ?0.
10 ?0.
50 0.
28AT012 7 2.
3 0.
23 0.
43 0.
00 0.
11 0.
52AT017 5 2.
3 0.
21 0.
43 0.
12 0.
36 0.
60AT023 4 2.
5 0.
21 0.
38 0.
41 0.
81 0.
67AT026 19 3.
3 0.
10 0.
26 ?0.
31 ?0.
41 0.
77AT039 11 3.
0 0.
11 0.
34 0.
06 0.
14 0.
74AT052 5 2.
0 0.
18 0.
40 0.
12 0.
36 0.
20AT053 5 2.
3 0.
17 0.
35 0.
36 ?0.
12 0.
40Table 3: Source texts, number of human translations per source text, median of the obtained grade per source text, mean of theBLEU and Meteor scores per source text and Kendall?s rank correlation coefficients for the second experiment.grades for the best translations selected as refer-ences range for the first and second experimentbetween 1.0 and 2.3, whereas for the third exper-iment the selected references were evaluated withgrades between 1.0 and 2.7.
Nevertheless, the me-dian grade for the references in all three exper-iments is always 1.7.
From the overall mediangrade and the median grade of the selected trans-lations as reference we can notice, that the trans-lations selected as references were indeed "better"than the remaining ones.The BLEU and Meteor scores given in the ta-bles are mean values over the individual transla-tions?
scores for each source text.
These scoresare very low, reaching a maximum of 0.25 overall three experiments for BLEU and 0.45 for Me-teor.
However, given the human expert gradesthe translations cannot be considered unreadable.In fact, the correlation coefficients show that nei-ther BLEU nor Meteor (except a few exceptionalcases) correlate with the human quality judge-ments, however, they show a (weak) tendency tocorrelate with each other.
Moreover, the datashows that the addition of reference translationsresults neither in significantly higher BLEU orMeteor scores nor in improved correlation.3.3 Qualitative analysisOur finding that human quality judgements do notcorrelate with automatic scores if the object ofevaluation is a translation produced by a human(as opposed to a machine) matches earlier resultspresented by Doddington (2002) within the con-text of evaluating NIST.
Doddington (2002) pro-poses the explanation that "differences betweenprofessional translators are far more subtle [thandifferences between machine-produced transla-tions, the authors] and thus less well characterized52Source Human trans./ Median Mean Mean Correlation Correlation Correlationtext source text grades BLEU Meteor Human-BLEU Human-Meteor BLEU-MeteorAT002 8 2.
5 0.
17 0.
36 ?0.
08 0.
00 0.
43AT004 8 3.
0 0.
20 0.
36 0.
00 0.
23 0.
71AT005 8 2.
3 0.
20 0.
42 0.
00 0.
08 0.
43AT008 6 2.
85 0.
26 0.
45 ?0.
55 ?0.
14 0.
33AT010 7 2.
7 0.
23 0.
41 0.
00 ?0.
12 0.
05AT012 5 2.
3 0.
23 0.
43 0.
22 0.
22 0.
40AT026 17 3.
3 0.
11 0.
31 ?0.
24 ?0.
34 0.
62AT039 9 3.
0 0.
10 0.
37 0.
22 0.
55 0.
22Table 4: Source texts, number of human translations per source text, median of the obtained grade per source text, mean of theBLEU and Meteor scores per source text and Kendall?s rank correlation coefficients for the third experiment.by N-gram statistics."
We conducted a qualitativeanalysis of some KOPTE translations in order tocheck whether the differences between individualtranslations are indeed as subtle as suggested byDoddington and to come up at least with hypothe-ses that could explain the poor performance of theautomatic scores.
We selected three source textsused in the second experiment, namely AT008,AT023 and AT053 and compared their respectivereference translations to selected hypothesis trans-lations.
This analysis was conducted on the lex-ical level alone, that is, most of the features ofKOPTE?s elaborated evaluation scheme were noteven considered.
The analysis, however, showsthat the amount of variation that can be found juston the lexical level is almost overwhelming.
Someexamples are listed in Appendix A.A common phenomenon is simple variation dueto synonyms or the use of phrasal variants orparaphrases.
Moreover, the listed examples showthat lexical variation can be triggered by differ-ent source text elements.
The phenomena shownin the tables are well-known translation prob-lems, e.g.
proper names, colloquial or figurativespeech or numbers.
The other categories in thetable are less clear-cut, that is, they can overlap.In our analysis, source text elements that cannotbe translated literally, but instead call for a cre-ative solution were classified as translation prob-lems.
Different translation strategies can be ap-plied to different kinds of problems, most impor-tantly to the translation of culture-specific items,proper names, underspecified source text elementsor culture-specific arguments.
The respective tableand other examples that we analysed show that forthis category some translators chose to add addi-tional information, to adapt the perspective to theGerman target audience (for example, by adapt-ing pronouns or deictic elements) or to adapt theformatting choices to the variant preferred by thetarget culture (e.g.
commas instead of fullstops,different types of quotation marks), whereas othertranslators chose to translate literally.
Both strate-gies are legitimate under certain circumstances,however, it can be assumed that adaptations re-quire a greater cognitive effort.
Source ambigu-ities, according to our preliminary typology, aresource text features that can be interpreted in dif-ferent ways - at least for a translator translatingfrom a foreign language (as opposed to a nativespeaker).
Obviously, the line between this cat-egory and outright translation errors is not veryclear.However, it needs to be stated that also for theother categories - while many variants are correctand legitimate - not all are equally good.
Bestsolutions for given problems are distributed un-equally across the translations studied.
Beyondthe purely lexical level, extensive variation can bewitnessed on the syntactic, but also the grammat-ical level.
For example, some translators chose tobreak the rather complicated syntax of the Frenchoriginal into simpler, easily readable sentences,producing, in some cases, considerable shifts inthe information structure of the text - often a legit-imate strategy.With respect to the performance of the auto-matic scores, our preliminary study - that still callsfor larger-scale and in-depth verification - suggeststhat neither BLEU nor Meteor are able to copewith the amount of variation found in the data.More specifically, they cannot distinguish betweenlegitimate and illegitimate variation or grave andslight errors respectively, but seem to fail to matchacceptable variants because of lexical and phrasalvariation or divergent grammatical structures re-sulting in different verb frames, word sequencesand text lengths, not to talk even about acceptablevariation on higher linguistic levels.
Therefore,automatic scores seem to overrate surface differ-53ences and thus assign very low scores to manytranslations that were found to be at least accept-able by a human expert.Considering the impact of these findings for MTevaluation purposes, it is not straightforward to as-sume that the differences that we have observedbetween the human translations are more "subtle"(in the sense of being unimportant) than the onesproduced by machine translation systems.
On thecontrary, our analysis suggests that "good" trans-lations are characterised by creative solutions thatare not easily reproducible but that help to achievetarget language readability and comprehensibility.This is a fundamental quality aspect of translationindependently of its production mode.
Moreover,it is difficult to see why some of the variants thatwe observed in the human translations selectedfrom KOPTE, once the context shifts from humanto machine translation, should be found valid inone situation and invalid in another, depending onthe training and test data used for developing anMT system: A high amount of the variation foundin the human translations goes back to the legiti-mate use of the creative and constructive powersof natural language, and it is, among others, thesepowers that should be mimicked by MT output.4 Conclusion and future workIn this paper, we have studied the performanceof two fully automatic MT evaluation metrics,namely BLEU and Meteor, in comparison to hu-man translation expert evaluations on a sample oflearner translations from the KOPTE corpus.
Theautomatic scores were tested in three experimentswith a varying number of reference translationsand their performance was compared to the hu-man evaluations by means of Kendall?s rank cor-relation coefficient.
The experiments suggest thatboth BLEU and Meteor systematically underesti-mate the quality of the translations tested, that is,they assign scores that, given the human expertevaluations, seem to be by far too low.
Moreover,they do not consistently correlate with the humanexpert evaluations.
Coming up with explanationsfor this failure is not straightforward, however, theresults of our qualitative and explorative analysissuggest that lexical similarity scores are not ableto cope satisfactorily neither with standard lexicalvariation (paraphrases etc.)
nor with dissimilari-ties that can be traced back to the specific natureof the translation process, leave alone linguisticlevels beyond the lexicon.
For Meteor, this short-coming may partly be alleviated by the provisionof richer sets of synonyms and paraphrases, how-ever, the amount of uncovered variation is still im-mense.
In fact, it seems that many more referencetranslations would be needed in order to cover thewhole range of legitimate variants that can be usedto translate a given source text - a scenario thatseems hardly feasible!
So how can BLEU or Me-teor scores be interpreted when they are given inMT papers?
Based on our analyses, it seems clearthat these scores are based on a data-driven no-tion of translation quality, that is, they measurethe degree of compliance of a hypothesis transla-tion with some reference set.
This is insofar prob-lematic as studies based on different reference setscannot be compared, neither can BLEU or Me-teor scores be generalised to other domains.
Evenmore importantly, BLEU or Meteor scores cannotbe used to measure a data-independent concept ofquality or even the usability of a translation fora target audience which, as we have shown, de-pends on many more factors than just lexical sur-face overlap.However, our study also leads to some openresearch questions.
One of these questions iswhether automatic evaluation scores can still beused for more coarse-grained distinctions, that is,to distinguish "really bad" translations from "re-ally good" ones.
The fine-grained distinctionsmade by the evaluator of KOPTE on generallyrather good translations do not allow us to answerthis question.
Future work will also deal with acomparison of mistakes made by MT systems asopposed to human translators as well as with thequestion how (and which) translation-specific as-pects can be applied to the evaluation of MT sys-tems.ReferencesAlexandra Birch, Barry Haddow, Ulrich Germann,Maria Nadejde, Christian Buck, and Philipp Koehn.2013.
The feasibility of HMEANT as a human MTevaluation metric.
In Proceedings of the 8th Work-shop on SMT, pages 52?61.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Barry Haddow, Philipp Koehn, Christof Monz, MattPost, Herv?
Saint-Amand, Radu Soricut, and LuciaSpecia, editors.
2013.
Proceedings of the 8th Work-shop on SMT.
ACL.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2007.54(Meta-) evaluation of machine translation.
In Pro-ceedings of the 2nd Workshop on SMT, pages 136?158.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic metric for reliable optimization andevaluation of machine translation systems.
In Pro-ceedings of the 6th Workshop on SMT, pages 85?91.Deutsches Institut f?r Normung.
2006.
DINEN 15038:2006-08: ?bersetzungsdienstleistungen-Dienstleistungsanforderungen.
Beuth.Dilek Dizdar.
2003.
Skopostheorie.
In HandbuchTranslation, pages 104?107.
Stauffenburg.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the 2nd In-ternational Conference on HLT, pages 138?145.Mireia Farr?s, Marta R.
Costa-Juss?, Jos?
B. Mar-i?o, and Jos?
A. R. Fonollosa.
2010.
Linguistic-based evaluation criteria to identify statistical ma-chine translation errors.
In Proceedings of the 14thAnnual Conference of the EAMT, pages 167?173.Christian Federmann.
2012.
Appraise: An open-source toolkit for manual evaluation of machinetranslation output.
PBML, 98:25?35, 9.Hans H?nig.
2003.
Human?bersetzung (therapeutischvs.
diagnostisch).
In Handbuch Translation, pages378?381.
Stauffenburg.Brigitte Horn-Helf.
1999.
Technisches ?bersetzen inTheorie und Praxis.
Franke.Paul Ku?maul.
2000.
Kreatives ?bersetzen.
Stauffen-burg.Vladimir Iosifovich Levenshtein.
1966.
Binary codescapable of correcting deletions, insertions and rever-sals.
Soviet Physics Doklady, 10(8):707?710.Chi-Kiu Lo and Dekai Wu.
2011.
MEANT: Aninexpensive, high-accuracy, semi-automatic metricfor evaluating translation utility based on semanticroles.
In Proceedings of the 49th Annual Meeting ofthe ACL, pages 220?229.Tucker Maney, Linda Sibert, Dennis Perzanowski,Kalyan Gupta, and Astrid Schmidt-Nielsen.
2012.Toward determining the comprehensibility of ma-chine translations.
In Proceedings of the 1st PITR,pages 1?7.Elvira Mertin.
2006.
Prozessorientiertes Qualit?ts-management im Dienstleistungsbereich ?bersetzen.Peter Lang.Christiane Nord.
2003.
Transparenz der Korrektur.In Handbuch Translation, pages 384?387.
Stauffen-burg.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the ACL, pages 311?318.Maja Popovi?c, Eleftherios Avramidis, Aljoscha Bur-chardt, Sabine Hunsicker, Sven Schmeier, CindyTscherwinka, David Vilar, and Hans Uszkoreit.2013.
Learning from human judgements of machinetranslation output.
In MT Summit, pages 231?238.Lothar Sachs and J?rgen Hedderich.
2009.
Ange-wandte Statistik.
Methodensammlung mit R.Springer.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of AMTA, pages 223?231.Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Fluency, adequacy, orHTER?
Exploring different human judgments with atunable MT metric.
In Proceedings of the 4th Work-shop on SMT, pages 259?268.Society of Automotive Engineers.
2005.
SAEJ2450:2005-08: Translation Quality Metric.
SAE.Harold Somers.
2011.
Machine translation: History,development, and limitations.
In The Oxford Hand-book of Translation Studies, pages 427?440.
OxfordUniversity Press.Sara Stymne and Lars Ahrenberg.
2012.
On the prac-tice of error analysis for machine translation evalua-tion.
In Proceedings of the 8th LREC, pages 1785?1790.Christoph Tillmann, Stephan Vogel, Hermann Ney,Alexander Zubiaga, and Hassan Sawaf.
1997.
Ac-celerated DP based search for statistical translation.In Proceedings of the EUROSPEECH, pages 2667?2670.Sandra Weiss and Lars Ahrenberg.
2012.
Error pro-filing for evaluation of machine-translated text: apolish-english case study.
In Proceedings of theEighth LREC, pages 1764?1770.Kevin Windle and Anthony Pym.
2011.
Europeanthinking on secular translation.
In The OxfordHandbook of Translation Studies, pages 7?22.
Ox-ford University Press.Andrea Wurm.
2013.
Eigennamen und Re-alia in einem Korpus studentischer ?bersetzungen(KOPTE).
trans-kom, 6(2):381?419.55A Examples of lexical variation in human translationIn the examples below, bold face indicates the French source.A.1 Proper namespr?sident gabonaisPr?sidenten von GabonPr?sidenten GabunsPr?sidenten von GabunPr?sident des afrikanischen Landes Gabongabunesischen Pr?sidentenla Commission nationale de l?informatique et des libert?s (CNIL)Commission nationale de l?informatique et des libert?s (CNIL)franz?sische Datenschutzbeh?rde (CNIL)franz?sische Datenschutzkommission CNILfranz?sische Datenschutzbeh?rde CNILfranz?sische Kommission f?r Datenschutz (CNIL)A.2 Problematic source text elements (translation problems)pivot de l?influence fran?aiseSt?tzpunkt des Einflusses Frankreichszentralen Figur des franz?sischen EinflussSt?tze f?r den Einfluss FrankreichsSchl?sselfigur f?r den Einfluss FrankreichsGarant f?r den franz?sischen Einflu?
"doyen de l?Afrique"obersten W?rdentr?gers Afrikas"Alten Herrn von Afrika""Abtes von Afrika""?ltesten von Afrika""doyen de l?Afrique"A.3 Paraphrasessera-t-elle capablees schaffenf?hig seinin der Lage seinsich als f?hig erweisense tenir ?
la bonne distanceauf angemessener Distanz zu bleibensich nicht einzumischensich herauszuhaltendie geb?hrende Neutralit?t zu wahrenA.4 Culture-specific elements and underspecified source text itemsla "Fran?afrique""Fran?afrique"Franz?sisch-Afrika ("Fran?afrique")?Franzafrika?
"Frankafrika""Fran?afrique" d.h. der franz?sisch beeinflussten Gebiete Afrikasles "voitures Google", ?quip?es de cam?ras ?
360 degr?smit 360-Grad-Kameras ausgestatteten "Google-Kamerawagen"Kamera-AutosStreet-View-Wagen mit ihren 360?-Kameras"Google-Autos", die auf dem Dach eine 360-Grad-Kamera montiert haben,mit 360-Grad-Kameras ausgestatteten "Street View-Autos"A.5 Source text ambiguities (syntactic and semantic)la France a soutenu un r?gime autoritaire et pr?dateur, sans piti?
pour les opposantsautorit?ren Systems [...], das kein Mitleid mit seinen Gegnern zeigtehat Frankreich ohne R?cksicht auf Regimekritiker ein autorit?res Gewaltregime unterst?tztautorit?re und ausbeutende Regime [...], welches keine Gnade f?r seine Gegner kannteautorit?res und angriffslustiges Regime [...], das kein Mitleid mit seinen Gegnern hattehat Frankreich dieses autorit?re und ausbeuterische System, ohne Mitleid mit dessen Gegnern, gest?tztjustes paroleshat die Wahrheit gesagthat [...] die richtigen Worte gefundenhat die richtigen Worte gefundenAussage [...] war nichts als Wortehat genau das Richtige gesagtA.6 Numbersune amende de 100 000 eurosGeldstrafe in H?he von 100 000 EuroStrafe von 100 000CGeldstrafe von 100.000,- EURGeldstrafe in H?he von 100.000 EuroBu?geld in H?he von 100 000Cphotographe Yann Arthus-Bertrand, 63 ans63j?hrigen Fotografen Yann Arthus-BetrandFotographen Yann Arthus-Bertrand (63 Jahre)Fotografen Yann Arthus-Bertrand (63)63-j?hrigen Fotografen Y.A.B.Fotografen Yann Arthus-Bertrand, 63A.7 Colloquial or figurative speechJe vais viteIch beeile michIch mache es schnellIch bewege mich schnellIch hab?s eiligIch beeile michr?sultats des petits fr?resEinnahmen der Vorg?ngerVerdienste zus?tzlicher kleiner ArtikelEinnahmen durch andere ProdukteErl?se von MerchandisingEinnahmen aus dem MerchandisingA.8 Source text element triggering correct and incorrect translations65 cha?nes de t?l?vision, dont France 2 et 23 cha?nes en Afrique65 Fernsehsendern, darunter auch France 2 und 23 afrikanische Sender65 Fernsehsendern, unter anderem France 2 und 23 Sender in Afrika65 Fernsehsender, darunter der franz?sische Sender France 2 und 23 afrikanische Sender65 Fernsehkan?len, u.a.
2 in Frankreich und 23 in Afrika65 Fernsehkan?len, darunter France 2 und 23 afrikanische Sender56
