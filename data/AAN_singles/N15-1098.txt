Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 970?976,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsDo Supervised Distributional MethodsReally Learn Lexical Inference Relations?Omer Levy?Steffen Remus?Chris Biemann?Ido Dagan??
Natural Language Processing Lab ?
Language Technology LabBar-Ilan University Technische Universit?t DarmstadtRamat-Gan, Israel Darmstadt, Germany{omerlevy,dagan}@cs.biu.ac.il {remus,biem}@cs.tu-darmstadt.deAbstractDistributional representations of words havebeen recently used in supervised settings forrecognizing lexical inference relations be-tween word pairs, such as hypernymy and en-tailment.
We investigate a collection of thesestate-of-the-art methods, and show that theydo not actually learn a relation between twowords.
Instead, they learn an independentproperty of a single word in the pair: whetherthat word is a ?prototypical hypernym?.1 IntroductionInference in language involves recognizing infer-ence relations between two words (x and y), suchas causality (flu ?
fever), hypernymy (cat ?animal), and other notions of lexical entailment.The distributional approach to automatically recog-nize these relations relies on representing each wordx as a vector ~x of contextual features: other wordsthat tend to appear in its vicinity.
Such features aretypically used in word similarity tasks, where cosinesimilarity is a standard similarity measure betweentwo word vectors: sim(x, y) = cos(~x, ~y).Many unsupervised distributional methods of rec-ognizing lexical inference replace cosine similaritywith an asymmetric similarity function (Weeds andWeir, 2003; Clarke, 2009; Kotlerman et al, 2010;Santus et al, 2014).
Supervised methods, reportedto perform better, try to learn the asymmetric opera-tor from a training set.
The various supervised meth-ods differ by the way they represent each candidatepair of words (x, y): Baroni et al (2012) use con-catenation ~x ?
~y, others (Roller et al, 2014; Weedset al, 2014; Fu et al, 2014) take the vectors?
differ-ence ~y ?
~x, and more sophisticated representations,based on contextual features, have also been tested(Turney and Mohammad, 2014; Rimell, 2014).In this paper, we argue that these supervised meth-ods do not, in fact, learn to recognize lexical infer-ence.
Our experiments reveal that much of their pre-viously perceived success stems from lexical mem-orizing.
Further experiments show that these super-vised methods learn whether y is a ?prototypical hy-pernym?
(i.e.
a category), regardless of x, ratherthan learning a concrete relation between x and y.Our mathematical analysis reveals that said meth-ods ignore the interaction between x and y, explain-ing our empirical findings.
We modify them ac-cordingly by incorporating the similarity betweenx and y.
Unfortunately, the improvement in per-formance is incremental.
We suspect that methodsbased solely on contextual features of single wordsare not learning lexical inference relations becausecontextual features might lack the necessary infor-mation to deduce how one word relates to another.2 Experiment SetupDue to various differences (e.g.
corpora, train/testsplits), we do not list previously reported results,but apply a large space of state-of-the-art supervisedmethods and review them comparatively.
We ob-serve similar trends to previously published results,and make the dataset splits available for replication.11http://u.cs.biu.ac.il/~nlp/resources/downloads/970Dataset #Instances #Positive #NegativeKotlerman 2010 2,940 880 2,060Bless 2011 14,547 1,337 13,210Baroni 2012 2,770 1,385 1,385Turney 2014 1,692 920 772Levy 2014 12,602 945 11,657Table 1: Datasets evaluated in this work.2.1 Word RepresentationsWe built 9 word representations over Wikipedia (1.5billion tokens) using the cross-product of 3 types ofcontexts and 3 representation models.2.1.1 Context TypesBag-of-Words Uses 5 tokens to each side of the tar-get word (10 context words in total).
It also employssubsampling (Mikolov et al, 2013a) to increase theimpact of content words.Positional Uses only 2 tokens to each side of thetarget word, and decorates them with their position(relative to the target word); e.g.
the?1is a commonpositional context of cat (Sch?tze, 1993).Dependency Takes all words that share a syntacticconnection with the target word (Lin, 1998; Pad?and Lapata, 2007; Baroni and Lenci, 2010).
We usedthe same parsing apparatus as in (Levy and Gold-berg, 2014).2.1.2 Representation ModelsPPMI A word-context positive pointwise mutual in-formation matrix M (Niwa and Nitta, 1994).SVD We reduced M ?s dimensionality to k = 500using Singular Value Decomposition (SVD).2SGNS Skip-grams with negative sampling (Mikolovet al, 2013b) with 500 dimensions and 5 nega-tive samples.
SGNS was trained using a modifiedversion of word2vec that allows different contexttypes (Levy and Goldberg, 2014).32.2 Labeled DatasetsWe used 5 labeled datasets for evaluation.
Eachdataset entry contains two words (x, y) and a labelwhether x entails y.
Note that each dataset was cre-ated with a slightly different goal in mind, affectingword-pair generation and annotation.
For example,2Following Caron (2001), we used the square root of theeigenvalue matrix ?kfor representing words: Mk= Uk?
?k.3http://bitbucket.org/yoavgo/word2vecfboth of Baroni?s datasets are designed to capture hy-pernyms, while other datasets try to capture broadernotions of lexical inference (e.g.
causality).
Table 1provides metadata on each dataset, and the descrip-tion below explains how each one was created.
(Kotlerman et al, 2010) Manually annotated lexi-cal entailment of distributionally similar nouns.
(Baroni and Lenci, 2011) a.k.a.
BLESS.
Createdby selecting unambiguous word pairs and their se-mantic relations from WordNet.
Following Roller etal.
(2014), we labeled noun hypernyms as positiveexamples and used meronyms, noun cohyponyms,and random noun pairs as negative.
(Baroni et al, 2012) Created in a similar fashionto BLESS.
Hypernym pairs were selected as posi-tive examples from WordNet, and then permutatedto generate negative examples.
(Turney and Mohammad, 2014) Based on acrowdsourced dataset of 79 semantic relations (Ju-rgens et al, 2012).
Each semantic relation was lin-guistically annotated as entailing or not.
(Levy et al, 2014) Based on manually anno-tated entailment graphs of subject-verb-object tuples(propositions).
Noun entailments were extractedfrom entailing tuples that were identical except forone of the arguments, thus propagating the exis-tence/absence of proposition-level entailment to thenoun level.
This dataset is the most realistic dataset,since the original entailment annotations were madein the context of a complete proposition.2.3 Supervised MethodsWe tested 4 compositions for representing (x, y) asa feature vector: concat (~x?~y) (Baroni et al, 2012),diff (~y ?
~x) (Roller et al, 2014; Weeds et al, 2014;Fu et al, 2014), only x (~x), and only y (~y).
For eachcomposition, we trained two types of classifiers, tun-ing hyperparameters with a validation set: logisticregression with L1or L2regularization, and SVMwith a linear kernel or quadratic kernel.3 Negative ResultsBased on the above setup, we present three nega-tive empirical results, which challenge the claim thatthe methods presented in ?2.3 are learning a rela-tion between x and y.
In addition to our setup, theseresults were also reproduced in preliminary exper-971Dataset Lexical +Contextual ?Kotlerman 2010 .346 .437 .091Bless 2011 .960 .960 .000Baroni 2012 .638 .802 .164Turney 2014 .644 .747 .103Levy 2014 .302 .370 .068Table 2: The performance (F1) of lexical versus contex-tual feature classifiers on a random train/test split withlexical overlap.iments by applying the JoBimText framework4forscalable distributional thesauri (Biemann and Riedl,2013) using Google?s syntactic N-grams (Goldbergand Orwant, 2013) as a corpus.Lexical Memorization is the phenomenon inwhich the classifier learns that a specific word in aspecific slot is a strong indicator of the label.
Forexample, if a classifier sees many positive exampleswhere y = animal, it may learn that anything thatappears with y = animal is likely to be positive,effectively memorizing the word animal.The following experiment shows that supervisedmethods with contextual features are indeed mem-orizing words from the training set.
We randomlysplit each dataset into 70% train, 5% validation, and25% test, and train lexical-feature classifiers, using aone-hot vector representation of y as input features.By definition, these classifiers memorize words fromthe training set.
We then add contextual-features (asdescribed in ?2.1), on top of the lexical features,and train classifiers analogously.
Table 2 comparesthe best lexical- and contextual-feature classifiers oneach dataset.
The performance difference is under10 points in the larger datasets, showing that muchof the contextual-feature classifiers?
success is dueto lexical memorization.
Similar findings were alsoreported by Roller et al (2014) and Weeds et al(2014), supporting our memorization argument.To prevent lexical memorization in our followingexperiments, we split each dataset into train and testsets with zero lexical overlap.
We do this by ran-domly splitting the vocabulary into ?train?
and ?test?words, and extract train-only and test-only subsets ofeach dataset accordingly.
About half of each originaldataset contains ?mixed?
examples (one train-wordand one test-word); these are discarded.4http://jobimtext.orgDataset Best Supervised Only ~y UnsupervisedKotlerman 2010 .408 .375 .461Bless 2011 .665 .637 .197Baroni 2012 .774 .663 .788Turney 2014 .696 .649 .642Levy 2014 .324 .324 .231Table 3: A comparison of each dataset?s best supervisedmethod with: (a) the best result using only y composi-tion; (b) unsupervised cosine similarity cos(~x, ~y).
Perfor-mance is measured by F1.
Uses lexical train/test splits.Supervised vs Unsupervised While supervisedmethods were reported to perform better than un-supervised ones, this is not always the case.
As abaseline, we measured the ?vanilla?
cosine similar-ity of x and y, tuning a threshold with the validationset.
This unsupervised symmetric method outper-forms all supervised methods in 2 out of 5 datasets(Table 3).Ignoring x?s Information We compared the per-formance of only y to that of the best configurationin each dataset (Table 3).
In 4 out of 5 datasets, thedifference in performance is less than 5 points.
Thismeans that the classifiers are ignoring most of theinformation in x.
Furthermore, they might be over-looking the compatibility (or incompatibility) of x toy.
Weeds et al (2014) reported a similar result, butdid not address the fundamental question it beckons:if the classifier cannot capture a relation between xand y, then what is it learning?4 Prototypical HypernymsWe hypothesize that the supervised methods exam-ined in this paper are learning whether y is a likely?category?
word ?
a prototypical hypernym ?
and,to a lesser extent, whether x is a likely ?instance?word.
This hypothesis is consistent with our previ-ous observations (?3).Though the terms ?instance?
and ?category?
per-tain to hypernymy, we use them here in the broadersense of entailment, i.e.
as ?tends to entail?
and?tends to be entailed?, respectively.
We later show(?4.2) that this phenomenon indeed extends to otherinference relations, such as meronymy.4.1 Testing the HypothesisTo test our hypothesis, we measure the performanceof a trained classifier on mismatched instance-972Dataset Top Positional Contexts of yKotlerman 2010 grave?1, substances+2, lend-lease?1, poor?2, bureaucratic?1, physical?1, dry?1, air?1, civil?1Bless 2011 other?1, resembling+1, such+1, assemblages+1,magical?1, species+1, any?2, invertebrate?1Baroni 2012 any?1, any?2, social?1, every?1, this?1, kinds?2, exotic?1,magical?1, institute?2, important?1Turney 2014 of+1, inner?1, including+1, such+1, considerable?1, their?1, extra?1, types?2, different?1, other?1Levy 2014 psychosomatic?1, unidentified?1, auto-immune+2, specific?1, unspecified?1, treatable?2, any?1Table 4: Top positional features learned with logistic regression over concat.
Displaying positive features of y.category pairs, e.g.
(banana, animal).
For eachdataset, we generate a set of such synthetic exam-ples S, by taking the positive examples from the testportion T+, and extracting all of its instance wordsT+xand category words T+y.T+x= {x|(x, y) ?
T+} T+y= {y|(x, y) ?
T+}We then define S as all the in-place combinations ofinstance-category word pairs that did not appear inT+, and are therefore likely to be false.S =(T+x?
T+y)\ T+Finally, we test the classifier on a sample of S (due toits size).
Since all examples are assumed to be false,we measure the false positive rate as match error?
the error of classifying a mismatching instance-category pair as positive.According to our hypothesis, the classifier can-not differentiate between matched and mismatchedexamples (T+and S, respectively).
We thereforeexpect it to classify a similar proportion of T+andS as positive.
We validate this by comparing recall(proportion of T+classified as positive) to match er-ror (proportion of S classified as positive).
Figure 1plots these two measures across all configurationsand datasets, and finds them to be extremely close(regression curve: match error = 0.935 ?
recall),thus confirming our hypothesis.4.2 Prototypical Hypernym FeaturesA qualitative way of analyzing our hypothesis is tolook at which features the classifiers tend to con-sider.
Since SVD and SGNS features are not eas-ily interpretable, we used PPMI with positional con-texts as our representation, and trained a logistic re-gression model with L1regularization using concatover the entire dataset (no splits).
We then observedthe features with the highest weights (Table 4).Figure 1: The correlation of recall (positive rate on T+)with match error (positive rate on S) compared to perfectcorrelation (green line).Many of these features describe dataset-specificcategory words.
For example, in Levy?s medical-domain dataset, many words entail ?symptom?,which is captured by the discriminative featurepsychosomatic?1.
Other features are domain-independent indicators of category, e.g.
any?1,every?1, and kinds?2.
The most striking features,though, are those that occur in Hearst (1992) pat-terns: other?1, such+1, including+1, etc.
Thesefeatures appear in all datasets, and their analoguesare often observed for x (e.g.
such?2).
Even quali-tatively, many of the dominant features capture pro-totypical or dataset-specific hypernyms.As mentioned, the datasets examined in this workalso contain inference relations other than hyper-nymy.
In Turney?s dataset, for example, 77 %of positive pairs are non-hypernyms, and y is of-ten a quality (coat ?
warmth) or a component(chair ?
legs) of x.
Qualities and componentscan often be detected via possessives, e.g.
of+1andtheir?1.
Other prominent features, such as extra?1973and exotic?1, may also indicate qualities.
These ex-amples suggest that our hypothesis extends beyondhypernymy to other inference relations as well.5 Analysis of Vector CompositionOur empirical findings show that concat and diff areclearly ignoring the relation between x and y.
To un-derstand why, we analyze these compositions in thesetting of a linear SVM.
Given a test example, (x, y)and a training example that is part of the SVM?s sup-port (xs, ys), the linear kernel function yields Equa-tions (1) for concat and (2) for diff.K (~x?
~y, ~xs?
~ys) = ~x ?
~xs+ ~y ?
~ys(1)K (~y ?
~x, ~ys?
~xs) = ~x ?
~xs+ ~y ?
~ys?
~x ?
~ys?
~y ?
~xs(2)Assuming all vectors are normalized (as in our ex-periments), the kernel function of concat is actuallythe similarity of the x-words plus the similarity ofthe y-words.
Two dis-similarity terms are added todiff?s kernel, preventing the x of one pair from beingtoo similar to the other pair?s y (and vice versa).Notice the absence of the term ~x ?
~y.
This meansthat the classifier has no way of knowing if x and yare even related, let alne entailing.
This flaw makesthe classifier believe that any instance-category pair(x, y) is in an entailment relation, even if they areunrelated, as seen in ?4.
Polynomial kernels alsolack ~x ?
~y, and thus suffer from the same flaw.6 Adding Intra-Pair SimilarityUsing an RBF kernel with diff slightly mitigates thisissue, as it factors in ~x ?
~y, among other similarities:KRBF(~y ?
~x, ~ys?
~xs) = e?1?2|(~y?~x)?
( ~ys?
~xs)|2= e?1?2(~x~y+ ~xs~ys+~x ~xs+~y ~ys?~x ~ys?~y ~xs?2)(3)A more direct approach of incorporating ~x ?
~y is tocreate a new kernel, which balances intra-pair simi-larities with inter-pair ones:KSIM((~x, ~y) , ( ~xs, ~ys)) = (~x~y ?
~xs~ys)?2(~x ~xs?
~y ~ys)1?
?2(4)While these methods reduce match error ?match error = 0.618 ?
recall versus the previousregression curve of match error = 0.935 ?
recall?
their overall performance is only incrementallybetter than that of linear methods (Table 5).
Thisimprovement is also, partially, a result of the non-linearity introduced in these kernels.Dataset LIN(concat) LIN(diff) RBF(diff) SIMKotlerman 2010 .367 .187 .407 .332Bless 2011 .634 .665 .636 .687Baroni 2012 .745 .769 .848 .859Turney 2014 .696 .694 .691 .641Levy 2014 .229 .219 .252 .244Table 5: Performance (F1) of SVM across kernels.
LINrefers to the linear kernel (equations (1) and (2)), RBF tothe Gaussian kernel (equation (3)), and SIM to our newkernel (equation (4)).
Uses lexical train/test splits.7 The Limitations of Contextual FeaturesIn this work, we showed that state-of-the-art su-pervised methods for recognizing lexical inferenceappear to be learning whether y is a prototypicalhypernym, regardless of its relation with x. Wetried to factor in the similarity between x and y,yet observed only marginal improvements.
Whilemore sophisticated methods might be able to extractthe necessary relational information from contextualfeatures alone, it is also possible that this informa-tion simply does not exist in those features.A (de)motivating example can be seen in ?4.2.
Atypical y often has such+1as a dominant feature,whereas x tends to appear with such?2.
These fea-tures are relics of the Hearst (1992) pattern ?y suchas x?.
However, contextual features of single wordscannot capture the joint occurrence of x and y in thatpattern; instead, they record only this observationas two independent features of different words.
Inthat sense, contextual features are inherently hand-icapped in capturing relational information, requir-ing supervised methods to harness complementaryinformation from more sophisticated features, suchas textual patterns that connect x with y (Snow et al,2005; Turney, 2006).AcknowledgementsThis work was supported by the Adolf MesserFoundation, the Google Research Award Program,and the German Research Foundation through theGerman-Israeli Project Cooperation (DIP, grant DA1600/1-1).
We thank Stephen Roller for his valuableinsights.974ReferencesMarco Baroni and Alessandro Lenci.
2010.
Distribu-tional memory: A general framework for corpus-basedsemantics.
Computational Linguistics, 36(4):673?721.Marco Baroni and Alessandro Lenci.
2011.
How weblessed distributional semantic evaluation.
In Pro-ceedings of the GEMS 2011Workshop on GEometricalModels of Natural Language Semantics, pages 1?10,Edinburgh, UK.Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, andChung-chieh Shan.
2012.
Entailment above the wordlevel in distributional semantics.
In Proceedings of the13th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 23?32,Avignon, France.Chris Biemann and Martin Riedl.
2013.
Text: Nowin 2D!
A framework for lexical expansion with con-textual similarity.
Journal of Language Modelling,1(1):55?95.John Caron.
2001.
Experiments with LSA scor-ing: Optimal rank and basis.
In Proceedings ofthe SIAM Computational Information Retrieval Work-shop, pages 157?169.Daoud Clarke.
2009.
Context-theoretic semantics fornatural language: an overview.
In Proceedings ofthe Workshop on Geometrical Models of Natural Lan-guage Semantics, pages 112?119, Athens, Greece.Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, HaifengWang, and Ting Liu.
2014.
Learning semantic hier-archies via word embeddings.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1199?1209, Baltimore, Maryland.Yoav Goldberg and Jon Orwant.
2013.
A dataset ofsyntactic-ngrams over time from a very large corpusof english books.
In Second Joint Conference on Lex-ical and Computational Semantics (*SEM), Volume 1:Proceedings of the Main Conference and the SharedTask: Semantic Textual Similarity, pages 241?247, At-lanta, Georgia, USA.Marti A Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In COLING 1992Volume 2: The 15th International Conference onComputational Linguistics, pages 529?545, Nantes,France.David A Jurgens, Peter D Turney, Saif M Mohammad,and Keith J Holyoak.
2012.
Semeval-2012 task 2:Measuring degrees of relational similarity.
In *SEM2012: The First Joint Conference on Lexical and Com-putational Semantics ?
Volume 1: Proceedings of themain conference and the shared task, and Volume 2:Proceedings of the Sixth International Workshop onSemantic Evaluation (SemEval 2012), pages 356?364,Montr?al, Quebec, Canada.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional distributionalsimilarity for lexical inference.
Natural Language En-gineering, 4(16):359?389.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In Proceedings of the 52ndAnnual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pages 302?308,Baltimore, Maryland.Omer Levy, Ido Dagan, and Jacob Goldberger.
2014.Focused entailment graphs for open ie propositions.In Proceedings of the Eighteenth Conference on Com-putational Natural Language Learning, pages 87?97,Baltimore, Maryland.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Linguis-tics and 17th International Conference on Computa-tional Linguistics, Volume 2, pages 768?774, Mon-tr?al, Quebec, Canada.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word repre-sentations in vector space.
In Proceedings of the In-ternational Conference on Learning Representations(ICLR).Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory SCorrado, and Jeffrey Dean.
2013b.
Distributed rep-resentations of words and phrases and their composi-tionality.
In Advances in Neural Information Process-ing Systems, pages 3111?3119.Yoshiki Niwa and Yoshihiko Nitta.
1994.
Co-occurrencevectors from corpora vs. distance vectors from dictio-naries.
In COLING 1994 Volume 1: The 15th Interna-tional Conference on Computational Linguistics, Ky-oto, Japan.Sebastian Pad?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2):161?199.Laura Rimell.
2014.
Distributional lexical entailment bytopic coherence.
In Proceedings of the 14th Confer-ence of the European Chapter of the Association forComputational Linguistics, pages 511?519, Gothen-burg, Sweden.Stephen Roller, Katrin Erk, and Gemma Boleda.
2014.Inclusive yet selective: Supervised distributional hy-pernymy detection.
In Proceedings of COLING2014, the 25th International Conference on Compu-tational Linguistics: Technical Papers, pages 1025?1036, Dublin, Ireland.Enrico Santus, Alessandro Lenci, Qin Lu, and SabineSchulte im Walde.
2014.
Chasing hypernyms in vec-tor spaces with entropy.
In Proceedings of the 14th975Conference of the European Chapter of the Associa-tion for Computational Linguistics, volume 2: ShortPapers, pages 38?42, Gothenburg, Sweden.Hinrich Sch?tze.
1993.
Part-of-speech induction fromscratch.
In Proceedings of the 31st Annual Meeting ofthe Association for Computational Linguistics, pages251?258, Columbus, Ohio, USA.Rion Snow, Daniel Jurafsky, and Andrew Y Ng.
2005.Learning syntactic patterns for automatic hypernymdiscovery.
In Advances in Neural Information Pro-cessing.Peter D Turney and Saif M Mohammad.
2014.
Experi-ments with three approaches to recognizing lexical en-tailment.
Natural Language Engineering, pages 1?40.Peter D Turney.
2006.
Similarity of semantic relations.Computational Linguistics, 32(3):379?416.Julie Weeds and David Weir.
2003.
A general frame-work for distributional similarity.
In Proceedings ofthe 2003 Conference on Empirical Methods in NaturalLanguage Processing, pages 81?88, Sapporo, Japan.Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir,and Bill Keller.
2014.
Learning to distinguish hyper-nyms and co-hyponyms.
In Proceedings of COLING2014, the 25th International Conference on Compu-tational Linguistics: Technical Papers, pages 2249?2259, Dublin, Ireland.976
