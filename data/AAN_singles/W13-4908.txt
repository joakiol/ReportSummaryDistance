Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 71?77,Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational LinguisticsExploiting the Contribution of Morphological Information to Parsing: theBASQUE TEAM system in the SPRML?2013 Shared TaskIakes Goenaga, Nerea EzeizaIXA NLP GroupFaculty of Computer ScienceUniv.
of the Basque Country UPV/EHUiakesg@gmail.com, n.ezeiza@ehu.esKoldo GojenolaIXA NLP GroupTechnical School of Engineering, BilbaoUniv.
of the Basque Country UPV/EHUkoldo.gojenola@ehu.esAbstractThis paper presents a dependency parsingsystem, presented as BASQUE TEAM atthe SPMRL?2013 Shared Task, based onthe analysis of each morphological featureof the languages.
Once the specific rel-evance of each morphological feature iscalculated, this system uses the most sig-nificant of them to create a series of ana-lyzers using two freely available and stateof the art dependency parsers, MaltParserand Mate.
Finally, the system will com-bine previously achieved parses using avoting approach.1 IntroductionMorphologically rich languages present new chal-lenges, as the use of state of the art parsers formore configurational and non-inflected languageslike English does not reach similar performancelevels in languages like Basque, Greek or Turk-ish (Nivre et al 2007).
Using morphological in-formation as features in parsing has been a com-monly used method for parsing MRLs (Tsarfaty etal., 2010).
In some cases the effect of this infor-mation is positive but in others it does not help orcauses a negative effect.In most of the work on dependency parsing, thespecific relevance of each morphological featurein the final result is unknown.
The authors includeall the morphological features1 in their systemswith the aim of taking advantage of the diversityof the used information.
This approach commonlyproduces very good results but they are not alwaysthe best ones (see table 2).On the other hand, some authors have made ex-periments to specify which is the real impact of1That is, they treat all the morphological features in thesame way in the feature specification, and let the learningalgorithms decide the weight assigned to each one.the morphological features.
Ambati et al(2010)explore ways of integrating local morphosyntacticfeatures into Hindi dependency parsing.
They ex-periment with different sets of features on a graph-based and a transition-based dependency parser.They show that using some morphological fea-tures (root, case, and suffix) outperforms a base-line using POS as the only feature, with both goldand predicted settings .Bengoetxea and Gojenola (2010) make use ofMaltParser?s feature configuration file to take ad-vantage of morphological features in parsing withgold data.
Their experiments show that case andsubordination type considerably increase parsingaccuracy.Marton et al(2013) also explore which mor-phological features could be useful in dependencyparsing of Arabic.
They observe the effect of fea-tures by adding them one at a time separately andcomparing the outcomes.
Experiments showedthat when gold morphology is provided, casemarkers help the most, whereas when the mor-phology is automatically predicted the outcomeis the opposite: using case harms the results themost.
When features are combined in a greedyheuristic, using definiteness, person, number, andgender information improves accuracy.Similarly, Seeker and Kuhn (2013) also deter-mine that the use of case is specially relevant forparsing, demonstrating that morpho-syntactic con-straints can delimit the search space of a statisticaldependency parser to outperform state-of-the-artbaselines for Czech, German and Hungarian.Following this line of research, our first stepwill be to determine which is the concrete value ofeach feature on dependency parsing, adding one ofthe morphological features at a time starting withan empty FEATS column.C?etinog?lu and Kuhn (2013) have shown thatsome parsers tend to improve the results whenswapping or replacing POS by some of the mor-71phological features.
They have made use of theMETU-Sabanc Turkish Treebank (Oflazer et al2003) for training and the ITU validation set(Eryigit, 2007) for testing.
In their work, it is ob-served that moving CASE to the POS field helpswith a 0.3% LAS absolute increase in the goldpipeline settings and using CASE instead of nom-inal POS improves the labelled accuracy by 0.3%absolute for the training set.These experiments suggest that in some waythe parser is not making an optimal use of all theavailable morpho-syntactic information, and thatthe parser algorithm (or the feature specificationfor the learning phase) is geared towards POS andCPOS, giving a lower status to other types of in-formation.
Although this strategy is good in gen-eral, it seems that, at least for some languages, spe-cific features (e.g.
CASE) are crucial in obtaininga high parsing performance.
Taking these ideasinto consideration, we will work on three differentapproaches:?
We will experiment the effect of using onlythe best three morphological features in theFEATS column (see table 1), compared toworking with the full set of morpho-syntacticfeatures.
This can have the effect of speed-ing the learning and parsing processes, as thenumber of features can be smaller.
On theother hand, the elimination of non-relevantfeatures can also help to improve the parser?sresults, because some features can even bedetrimental for parsing.?
Following C?etinog?lu and Kuhn (2013), onceour system resolves which feature is the mostsignificant, it will be used to replace the POSand CPOS fields one by one and we will testthe effect of these variants on the parsers.
Fi-nally, we will also try right-to-left versionsof those 3 variants (baseline, and replacingPOS and CPOS) completing a set of 6 differ-ent parsers.?
Finally, we will experiment the combinationof the different or parsers with a voting ap-proach (Hall et al 2010) using the Malt-Blender tool2.All of the experiments will be performed onautomatically predicted POS and morphosyntacticdata, taking the tags given in the Shared Task data,2http://w3.msi.vxu.se/users/jni/blend/that is, we will not made use of any specificallytrained morphological tagger.In the rest of this paper we will first presentthe resources we have used to carry out our ex-periments in section 2, followed by a study of thecontribution of the morphological information toparsing in section 3 and the effect of this infor-mation on the individual parsers in subsection 4.1.The final results of the best parser combinationsare showed in subsection 4.2 and the main conclu-sions of the work in section 5.2 ResourcesThis section will describe the main resources thathave been used in the experiments.
Subsection2.1 will describe the languages we have used inour experiments, subsection 2.2 will explain theparsers we use, while subsection 2.3 will presentbriefly the MaltBlender tool.2.1 Selected LanguagesAlthough the SPMRL?2013 Shared Task (Seddahet al 2013) offers the opportunity to parse ninemorphologically rich languages, to carry out ourexperiments we have selected five of them, due inpart to time constraints, but also taking into ac-count the relevance of the morpho-syntactic infor-mation (FEATS column, see table 1) .
The selectedfive languages are: Basque (Aduriz et al 2003),French (Abeille?
et al 2003), German (Seeker andKuhn, 2012), Hungarian (Vincze et al 2010) andSwedish (Nivre et al 2006).2.2 ParsersWe have made use of MaltParser (Nivre et al2007b) and Mate (Bohnet and Nivre, 2012), twostate of the art dependency parsers3 representingthe dominant approaches in data-driven depen-dency parsing, and that have been successfullyapplied to typologically different languages andtreebanks.MaltParser is a representative of local, greedy,transition-based dependency parsing models,where the parser obtains deterministically adependency tree in a single pass over the inputusing two data structures: a stack of partiallyanalyzed items and the remaining input sequence.To determine the best action at each step, the3Due to time constraints, we did not have enough time toexperiment with other options such as the MST parser or theEasyFirst parser.72parser uses history-based feature models and dis-criminative machine learning.
The specificationof the learning configuration can include anykind of information (such as word-form, lemma,category, subcategory or morphological features).We will use one of its latest versions (MaltParserversion 1.7).To fine-tune Maltparser we have used MaltOp-timizer (Ballesteros and Nivre, 2012a; Ballesterosand Nivre, 2012b).
This tool is an interactive sys-tem that first performs an analysis of the trainingset in order to select a suitable starting point foroptimization and then guides the user through theoptimization of parsing algorithm, feature model,and learning algorithm.
Empirical evaluation ondata from the CoNLL 2006 and 2007 shared taskson dependency parsing shows that MaltOptimizerconsistently improves over the baseline of defaultsettings and sometimes even surpasses the resultof manual optimization.The Mate parser (Bohnet and Nivre, 2012) is adevelopment of the algorithms described in (Car-reras, 2007; Johansson and Nugues, 2008).
It basi-cally adopts the second order maximum spanningtree dependency parsing algorithm.
In particular,this parser exploits a hash kernel, a new parallelparsing and feature extraction algorithm that im-proves accuracy as well as parsing speed (Bohnet,2010).2.3 Parser CombinationsThe MaltBlender tool makes a two-stage optimiza-tion of the result of several parser outcomes, basedon the work of Sagae and Lavie (2006), and it wasused for the first time for the ten languages in themultilingual track of the CoNLL 2007 shared taskon dependency parsing(Hall et al 2010).
The firststage consists in tuning several single-parser sys-tems.
The second stage consists in building anensemble system that will combine the differentparsers.
When this system was evaluated on theofficial test sets at the CoNLL 2007 shared task,the ensemble system significantly outperformedthe single-parser system and achieved the highestaverage labelled attachment score of all participat-ing systems.3 Contribution of MorphologicalInformation to ParsingWe examined the effect of each type of morpho-logical information, contained in the FEATS col-umn, to investigate their overall contribution toparsing.
This will help us to determine which arethe most relevant features for parsing.
To carry outthis task we have used the Mate parser, due to lackof time for testing, and also taking into consid-eration that it gives better results than MaltParserfor all the languages?s baselines.
Firstly, we willobtain the baseline for each language parsing thefiles with an empty FEATS column.
This baselinewill help us to determine the contribution of eachmorphological feature to parsing.
Next, we trainedthe parsers using one feature at a time obtaining asmany results as features for each language.
Table1 shows the effect of each information on the Mateparser.In this table we can observe that Basque is oneof the most sensitive languages regarding the influ-ence of its features.
Using case (KAS) as a uniquefeature improves the labelled attachment scoreover using an empty FEATS column by almost5.7%.
The next two better features are number(NUM) and type of subordinate sentence (ERL).They help with a 1.1% and 0.6% increase, respec-tively.
The rest of the features do not contributemuch in isolation, with a maximum of 0.2%.
Onthe other hand, including all the features results inan improvement of 6.5%.If we analyze the results for French we see that,in contrast to Basque, the influence of the featureson the parser is minimum.
The most significantfeature is gender (g), which helps with a 0.1% in-crease.
With respect to the improvement using theother features, although they do not provide big in-creases all of them contribute positively.
In clos-ing, including all the features we obtain a 84.6%labelled attachment score with a 0.4% improve-ment over not using any features.As with French, the German morphological fea-tures provide small increases.
The most two sig-nificant features are case and gender, which obtainincreases of 0.2%, 0.13%, respectively.
It is inter-esting to observe how including all the features weobtain worse results than using only the case, al-though the difference is not significant.
That couldoccur due to the weak influence of its features inthe final result and the negative influence of someof them.Hungarian is the language which offers morefeatures, 14 altogether.
This language, in line withBasque, tends to vary significantly its labelled at-tachment score depending on the used morpholog-73Basque French German Hungarian Swedishall feats 83.0 all feats 84.6 all feats 91.0 all feats 82.8 all feats 76.7no feats 76.5 no feats 84.2 no feats 90.9 no feats 75.3 no feats 76.9KAS 82.2 g 84.3 case 91.0 Cas 80.9 verbform 77.0NUM 77.7 n 84.3 gender 91.0 PerP 76.3 definiteness 76.8ERL 77.1 p 84.3 number 90.9 NumP 76.3 degree 76.8DADUDIO 76.8 c 84.2 person 90.9 SubPOS 75.9 case 76.8NORK 76.7 m 84.2 tense 90.9 Def 75.7 number 76.3MDN 76.6 s 84.2 degree 90.8 Num 75.7 perfectform 76.3NOR 76.6 t 84.2 mood 90.8 PerP 75.7 abbrv 76.3ASP 76.4 Mood 75.5 mood 76.2NORI 76.2 NumPd 75.4 pronounform 76.1ADM 76.5 Coord 75.3 gender 76.0Form 75.3Tense 75.3Type 75.3Deg 75.0Table 1: The effect of each feature sorted by language (MATE parser)ical feature.
If we focus on the three most signif-icant features, the case (Cas) helps with a 5.6%increase, person of possessor (PerP) with a 1%,while number of possessor helps with a 0.9%.
Thegrammatical subcategory within the main part ofspeech (SubPOS) improves the baseline in a 0.6%and the number and person in a 0.4%.
The remain-ing features do not contribute very appreciativelyeven obtaining negative results.
Including all thefeatures we obtain a labelled attachment score of82.83%.
That means the real contribution of allthe features is 7.5%, this improvement being themost important among all the used languages.In common with French and German, theSwedish morphological features do not seem tohelp the parsers to achieve significant improve-ments in terms of LAS.
However, we can observesome interesting phenomena.
While in the otherlanguages the case is one of the best features, inSwedish is does not help, achieving a negative re-sult.
In general, excluding the verb form (verb-form), all the features obtain negative results withrespect to not using any feature.
In this scenarioit is not surprising to verify that including all thefeatures does not help the Mate parser.
Havingsaid this, the best three features are the verb form(verbform), definiteness (definiteness) and degree(degree).4 Testing the Effect of DifferentMorphosyntactic features on parsersWe examined the effect of the most significantmorphological features, examined in the previousstep, to investigate their overall contribution toparsing.
For this task, we created three variants foreach parser, apart from the baseline using all themorphosyntactic features.
We obtain these vari-ants by: i) using the most 3 relevant features inthe FEATS column (see table 1 in previous sec-tion), ii) moving the most relevant feature for eachlanguage to the POS column and iii) moving themost relevant feature to the CPOS column.
Next,we have tested parser combinations including allthe baselines and their variants in subsection 4.2.4.1 Individual ParsersTable 2 shows the effect of each information onboth parsers, Maltparser and Mate parser.
If weanalyze the results on Basque, the difference be-tween the two parsers is noticeable, as Mate ob-tains on average a 3 point improvement with re-spect to MaltParser.
A similar difference occurson all the used languages.
The best LAS in Basqueis acquired using the 3 best features in the FEATScolumn with the Mate parser (83.4%).
On a com-parison with the LAS obtained by the Mate base-line (All-Feats), that means a 0.4 improvement.Regarding Maltparser?s results for Basque, we getthe best LAS (81.0%) moving the best feature(case) to POS in its right-to-left version, increas-ing the LAS baseline (All-Feats) by 1.0.
We no-tice that Maltparser and Mate tend to improve theirbaseline scores using some of the presented vari-ants.On the other hand, the best score for Frenchis obtained using the baseline (All-Feats and74Basque French German Hungarian SwedishBaselinesAll ?
FeatsMalt 80.0 79.9 87.6 77.3 73.4All ?
FeatsMate 83.0 84.6 91.0 82.3 76.7Left2right3?
bestMalt 79.9 79.9 87.6 75.9 73.4CPOS ?
bestMalt 80.3 79.7 87.5 76.6 72.9POS ?
bestMalt 78.7 78.7 86.6 77.2 72.83?
bestMate 83.4 84.3 90.8 82.4 76.6CPOS ?
bestMate 82.7 84.3 91.0 82.7 76.8POS ?
bestMate 82.2 83.4 90.5 82.5 76.5Right2left3?
bestMalt 80.1 78.9 86.9 75.3 69.3CPOS ?
bestMalt 80.0 79.0 86.7 76.6 69.3POS ?
bestMalt 81.0 77.8 85.4 74.9 70.23?
bestMate 83.3 84.3 90.9 82.1 76.5CPOS ?
bestMate 83.1 84.6 91.0 82.6 77.0POS ?
bestMate 81.6 83.5 90.6 82.4 76.4Table 2: Testing the effect of features on MaltParser and Matethe Mate parser, 84,6%).
Contrary to Basque,in French, although some of the used variantsachieve similar scores with respect to their base-lines (All-Feats), they do not give noticeable in-creases.
The unique variant that equals its base-line (79,9%) is 3?
bestMalt using the left-to-rightversion and the three best features (gender, num-ber and person) in the FEATS column using Malt-parser.With respect to German, the only variant thatequals the baseline is CPOS ?
bestMate with91.0% LAS.
.
If we focus on Maltparser?s (Mal-tOptimizer) scores, we get the best result amongthe variants with 3 ?
bestMalt (87.6%) using theleft-to-right version.
The variants do not improveMaltparser?s baseline.Although some of the Hungarian variant scoresare very similar to their baselines, they give someimprovements over the baseline.
The best two re-sults on the Mate parser are 82.7% and 82.6%.
Weobtain the first score moving the best feature (case)to CPOS in its left-to-right version, and the secondone using the same configuration in its right-to-leftversion.
The best two scores on Maltparser with-out taking the baseline into account are 77.2% and76.6%, obtained when moving the best feature toPOS and moving the best feature to CPOS in itsright-to-left version, respectively.The best two results for Swedish on the Mateparser are 77.0% and 76.8%.
We get the first re-sult moving the best feature (verbform) to CPOSin its right-to-left version and the second one in itsstandard version.
These two results are the onlyvariants that improve the baseline (76.7% LAS)with a 0.30 and 0.17 increase, respectively.
On theother hand, if we focus on Maltparser, the variantsdo not improve the baseline (73.4% LAS) wherethe best two results are 73.4% and 72.9% LAS.For the best result we use the three best features(verbform, definiteness and degree) in the FEATScolumn, while for the second one the best feature(verbform) has been moved to CPOS.Despite that only the Basque and Swedish vari-ants haven been able to significantly improve theirbaselines, in the next subsection we present a com-bination system expecting to take advantage on thevariety of the parsed files (Surdeanu and Manning,2010).4.2 Parser CombinationsAlthough in several cases the use of specific mor-phosyntactic information does not give noticeableincreases, we also tested the effect on parser com-binations.
Table 3 presents the result of combin-ing the extended parsers with the baselines (us-ing all the features) obtained in individual parsers.The table shows that the Basque language hasachieved the biggest increase.
Parser combinationin Basque helps with an improvement of 3.2 withrespect to the Mate baseline.
Contrary to Basque,French is the language that has obtained the small-est increases in parser combination if we compareit with the Mate (highest) parser baseline.
Thecombined system improves the Mate parser base-75Basque French German Hungarian SwedishMaltParser baseline 80.0 79.9 87.6 77.3 73.4Mate parser baseline 83.0 84.6 91.0 82.8 76.7Parser combination 86.2 85.1 91.8 84.1 78.1Table 3: Results of parser combinationsline by 0.5.
Parser combination in German gives a0.8 increase with respect to the best single parser(Mate, 91.0).
Our system achieves a 1.3 increasefor Hungarian with respect to the Mate parser?sbaseline.
Finally, if we focus on Swedish, theparser combination helps with a 1.4 increase withrespect to the Mate parser.After examining the parsers involved in parsercombinations we noticed that there are always sev-eral variants included in the best parser combina-tions, although the only variant that appears in allthe best parser combinations is CPOS?bestMatein its left-to-right version.
Taking into accountthat the most relevant feature for Basque, Germanand Hungarian is the case, it would be interest-ing to use the CPOS?caseMate variant for otherlanguages.
Finally, the presented results suggestthat the introduced variants contribute positivelyon parsing and they help to improve the scores ob-tained by the base parsers.5 Conclusion and Future WorkWe have presented a combined system that wasdesigned after analyzing the relevance of the mor-phological features in order to take advantage onthe effect of those features on some parsers.
Ingeneral the improvements have been noticeable,specially for Basque.
We can point out some in-teresting avenues for research:?
Use of new parsing algorithms for testingthe effect of different morphological fea-tures.
The results of this work show that theused techniques are specially useful for lan-guages where the FEATS column, contain-ing morpho-syntactic information, gives thebiggest increments with respect to not us-ing the features, like Basque and Hungar-ian.
We expect that similar improvementscould be obtained for languages like Turkishor Czech, which share many characteristicswith Basque and Hungarian.?
Experimenting different models for parsercombinations using new parsers.
Several ofthe parser variants we have used give onlyslight modifications over the base algorithms,even though when combined they give sig-nificant increases.
Widening the spectrum ofparsers and adding new algorithms can implyan important boost in parser combination.?
Application to the rest of the languages of theSPMRL 2013 Shared Task: Korean, Hebrew,Arabic and Polish.AcknowledgementsThis research was supported by the Department ofIndustry of the Basque Government (IT344-10, SPE11UN114), the University of the Basque Coun-try (GIU09/19) and the Spanish Ministry of Sci-ence and Innovation (MICINN, TIN2010-20218).ReferencesAnne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.2003.
Building a treebank for french.
In AnneAbeille?, editor, Treebanks.
Kluwer, Dordrecht.I.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,A.
D?
?az de Ilarraza, A. Garmendia, and M. Oronoz.2003.
Construction of a Basque dependency tree-bank.
pages 201?204.Bharat Ram Ambati, Samar Husain, Sambhav Jain,Dipti Misra Sharma, and Rajeev Sangal.
2010.
Twomethods to incorporate local morphosyntactic fea-tures in hindi dependency parsing.
In Proceedings ofthe NAACL HLT 2010 First Workshop on StatisticalParsing of Morphologically-Rich Languages, pages22?30.Miguel Ballesteros and Joakim Nivre.
2012a.
Maltop-timizer: A system for maltparser optimization.
InLREC, pages 2757?2763.Miguel Ballesteros and Joakim Nivre.
2012b.
Mal-toptimizer: an optimization tool for maltparser.
InProceedings of the Demonstrations at the 13th Con-ference of the European Chaptr of the Associationfor Computational Linguistics, pages 58?62.Kepa Bengoetxea and Koldo Gojenola.
2010.
Appli-cation of different techniques to dependency pars-ing of basque.
In Proceedings of the NAACLHLT 2010 First Workshop on Statistical Parsing ofMorphologically-Rich Languages, pages 31?39.76Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1455?1465.Bernd Bohnet.
2010.
Very high accuracy and fast de-pendency parsing is not a contradiction.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, pages 89?97.Xavier Carreras.
2007.
Experiments with a higher-order projective dependency parser.
In Proceed-ings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, Prague, Czech Republic, June.O?zlem C?etinog?lu and Jonas Kuhn.
2013.
Towardsjoint morphological analysis and dependency pars-ing of turkish.
In Proceedings of the Second In-ternational Conference on Dependency Linguistics(DepLing 2013), pages 23?32, Prague, Czech Re-public, August.
Charles University in Prague, Mat-fyzpress, Prague, Czech Republic.Gu?lsen Eryigit.
2007.
Itu validation set for metu-sabanc?
turkish treebank.
URL: http://www3.
itu.edu.
tr/ gulsenc/papers/validationset.
pdf.Johan Hall, Jens Nilsson, and Joakim Nivre.
2010.Single malt or blended?
a study in multilingualparser optimization.
In Trends in Parsing Technol-ogy, pages 19?33.
Springer.Richard Johansson and Pierre Nugues.
2008.Dependency-based syntactic-semantic analysis withpropbank and nombank.
In Proceedings of theTwelfth Conference on Computational Natural Lan-guage Learning, pages 183?187.Yuval Marton, Nizar Habash, and Owen Rambow.2013.
Dependency parsing of modern standard ara-bic with lexical and inflectional features.
Computa-tional Linguistics, 39(1):161?194.Joakim Nivre, Jens Nilsson, and Johan Hall.
2006.
Tal-banken05: A Swedish treebank with phrase struc-ture and dependency annotation.
In Proceedings ofLREC, pages 1392?1395, Genoa, Italy.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared taskon dependency parsing.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL2007, Prague, Czech Republic, June.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, Gu?lsen Eryigit, Sandra Ku?bler, SvetoslavMarinov, and Erwin Marsi.
2007b.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tu?r, and Go?khan Tu?r.
2003.
Building a turkishtreebank.
Building and Exploiting Syntactically-annotated Corpora.Kenji Sagae and Alon Lavie.
2006.
Parser com-bination by reparsing.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the ACL.Djame?
Seddah, Reut Tsarfaty, Sandra Ku?bler, MarieCandito, Jinho Choi, Richa?rd Farkas, Jennifer Fos-ter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg,Spence Green, Nizar Habash, Marco Kuhlmann,Wolfgang Maier, Joakim Nivre, Adam Przepi-orkowski, Ryan Roth, Wolfgang Seeker, YannickVersley, Veronika Vincze, Marcin Wolin?ski, AlinaWro?blewska, and Eric Villemonte de la Cle?rgerie.2013.
Overview of the spmrl 2013 shared task: Across-framework evaluation of parsing morpholog-ically rich languages.
In Proceedings of the 4thWorkshop on Statistical Parsing of MorphologicallyRich Languages: Shared Task, Seattle, WA.Wolfgang Seeker and Jonas Kuhn.
2012.
Making El-lipses Explicit in Dependency Conversion for a Ger-man Treebank.
In Proceedings of the 8th Interna-tional Conference on Language Resources and Eval-uation, pages 3132?3139, Istanbul, Turkey.
Euro-pean Language Resources Association (ELRA).Wolfgang Seeker and Jonas Kuhn.
2013.
Morphologi-cal and syntactic case in statistical dependency pars-ing.
Computational Linguistics, 39(1):23?55.Mihai Surdeanu and Christopher D. Manning.
2010.Ensemble models for dependency parsing: Cheapand good?
In Proceedings of the North Ameri-can Chapter of the Association for ComputationalLinguistics Conference (NAACL-2010), Los Ange-les, CA, June.Reut Tsarfaty, Djam Seddah, Yoav Goldberg, San-dra Ku?bler, Marie Candito, Jennifer Foster, Yan-nick Versley, Ines Rehbein, and Lamia Tounsi.2010.
Statistical parsing of morphologically richlanguages (spmrl) what, how and whither.
In In Pro-ceedings of the NAACL HLT 2010 First Workshopon Statistical Parsing of Morphologically-Rich Lan-guages.Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgyMo?ra, Zolta?n Alexin, and Ja?nos Csirik.
2010.
Hun-garian dependency treebank.
In LREC.77
