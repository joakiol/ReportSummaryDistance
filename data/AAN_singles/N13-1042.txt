Proceedings of NAACL-HLT 2013, pages 411?415,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsFocused training sets to reduce noise in NER feature modelsAmber McKenzieComputer Science and Engineering DepartmentUniversity of South Carolinamckenzie.amber@gmail.comAbstractFeature and context aggregation play alarge role in current NER systems, allowingsignificant opportunities for research into op-timizing these features to cater to differentdomains.
This work strives to reduce the noiseintroduced into aggregated features from dis-parate and generic training data in order to al-low for contextual features that more closelymodel the entities in the target data.
The pro-posed approach trains models based on only apart of the training set that is more similar tothe target domain.
To this end, models aretrained for an existing NER system using thetop documents from the training set that aresimilar to the target document in order todemonstrate that this technique can be appliedto improve any pre-built NER system.
Initialresults show an improvement over the Univer-sity of Illinois NE tagger with a weighted av-erage F1 score of 91.67 compared to theIllinois tagger?s score of 91.32.
This researchserves as a proof-of-concept for futureplanned work to cluster the training docu-ments to produce a number of more focusedmodels from a given training set, thereby re-ducing noise and extracting a more repre-sentative feature set.1 IntroductionThough research in the area of named entityrecognition (NER) is fairly extensive, current state-of-the-art solutions are generic, succeeding onlyfor domains similar to their training data, and stillfail to adequately provide functionality that isadaptable to a broad range of domains (Tkachenkoand Simanovsky, 2012).
This leaves room for im-provement in designing a system that can moreeasily adapt to previously unseen data.
In particu-lar, the increasingly popular feature set producedby feature and context aggregation provides manyopportunities for different types of optimizationgiven the strong correlation between the traininginput and the feature values that are produced.
Thisis due to the fact that aggregation looks at featuresat a document or corpus level, rather than at thetoken level, and therefore will be sensitive tochanges in the training set.
This research looks toexploit this aspect of feature and context aggrega-tion by identifying portions of a training set thatare more similar to the target data and will thusprovide feature values that are likely more repre-sentative of the entities within that data.Rather than train a model with a full trainingset, this approach extracts portions of the trainingdata that are most similar to the target data andtrains a model using only those documents.
Thisinitial work tailors a model to a given target docu-ment to demonstrate that less, but more appropri-ate, training data is preferable to a full generictraining set.Similar to that of Dalton et al(2011), in whichthey use passage retrieval to expand their featureset, cosine similarity is used to retrieve documentscontaining similar entity instances in an effort toachieve a more relevant feature set that will resultin more likely output label predictions.
However,the proposed approach conducts document similar-ity above the tagger level, without modifying theunderlying tagging system.
This allows for domainadaptation improvements using any available NERtagger.
This approach is able to be implementedwith any pre-existing NER tagger in order to im-prove the performance of the tagger for out-of-domain data.
Initial results show an improvementover the standard NE tagger from the University ofIllinois at Urbana-Champaign using a smallertraining set and no additional external data sources.2  Related workFeature aggregation refers to collecting featureinformation from across a document or documentset, rather than simply taking the information froma particular word instance.
With feature aggrega-411tion, researchers strive to expand the context usedto predict the classification of a given token.
Muchof the recent work on features for NER has beenrelated to aggregation of some sort in an effort towiden model coverage, decrease human interactionin the feature generation process, and increase de-tection and classification accuracy.
Many systemsincorporating feature aggregation have seen per-formance improvements over other nearly state-of-the-art systems.The global features discussed by Chieu and Ng(2003) represent context aggregation in that theyextract features about the word in multiple instanc-es within a document.
Krishnan and Manning(2006) introduce a two-stage approach to featureaggregation layering two CRFs in which the sec-ond uses the output of the first as features, aggre-gated over both documents and the entire corpus.Ratinov and Roth (2009) use a similar imple-mentation for their work, substituting relative fre-quencies of tags within a 1000 token window forthe majority tags used by Krishnan and Manning.They refer to the information gathered from aggre-gation as non-local features and categorize the dif-ferent approaches as context aggregation, two-stage prediction aggregation and extended predic-tion history.
In an effort not to treat all tokens in atext similarly, which they assert is the case withcontext aggregation and two-stage prediction,Ratinov and Roth developed an approach for non-local feature generation based on extended predic-tion history.
Their approach is based on the ideathat named entities are easier to spot at the begin-ning of texts where they are first introduced.
Theykeep track of all label assignments for the token inthe last 1000 words and use that probability infor-mation as a prediction history feature for the token.Huang and Yates (2009) present their featureaggregation approaches in the form of smoothingof the dataset.
Their goal for smoothing is the sameas for aggregation in that they strive to extend theusefulness of the model by sharing informationabout multiple contexts for a token in order to pro-vide more information about words that are rarely,or never, seen in training.
In experimentation, theauthors found that their smoothing approach im-proved performance on rare words, out-of-domaintext, and smaller training sets.Dalton et al(2011) take an external knowledgeapproach to context aggregation.
Using an infor-mation retrieval method called Pseudo-RelevanceFeedback (PRF), they query for relevant passagesin an external data set using the context for the tar-get token.
Given that they searched for the contextthat the entity occurs in, it is assumed that the topreturned passages all contain instances of the entitywith the same label.
They then aggregate the fea-tures for this token across a number of the top re-trieved documents and induce features based onthis information.
Their approach is compared withthe Stanford and Illinois NER systems and foundthat their aggregated features improved perfor-mance over those systems.Apart from the body of work attempting to in-corporate external data sources, such as Wikipedia,to augment training data, approaches for domainadaptation for NER focus on either adapting fea-tures to fit the domain or searching for more ab-stract features that can span multiple domains(Zhang and Johnson, 2003; Huang and Yates,2009; Lin and Wu, 2009).
This is largely due to theassumption that a domain-specific, tagged trainingset will not be available for most target domains.This research expands on previous work byproviding a more informative training set that is acloser representation of the features contained inthe target documents.
Further, the proposed systemdoes not require external knowledge sources oradditional tagged data to augment the utilizedtraining set.
The modifications that are made areimplemented above the tagger level allowing forany existing tagger to be used without need to alterthe underlying source code.3 NER approachFeature aggregation has become an integral partof building an NER prediction model.
Becauseaggregating the context of every named entityacross an entire training set can be fairly computa-tionally expensive and introduces significant noiseinto the features due to the many contexts in whichan entity may occur, many researchers have choseninstead to conduct local aggregation, such as acrossa document, or with a certain window of tokensthat may span several documents.
The NER taggerproduced by the University of Illinois at Urbana-Champaign, one of the best performing systems onthe CoNLL 2003 data set, uses a 1000 token win-dow across which to take their global context ag-gregation (Ratinov and Roth, 2009).
By choosing1000 tokens, the researchers hope to be able to412capture a large enough example set to provide arobust feature value while maintaining a reasona-ble computation time.
However, this method leavesthe choice of context to chance: determined byhow the documents are organized within the train-ing set.
A better option would be to choose thecontext that best represents the entities to betagged.
To that end, this work serves to provide amore useful and informative training set fromwhich to pull context information.The hypothesis explored in this work is that thecontext aggregation feature would prove more use-ful if the training data were more specific to thetarget entities.
For this research, documents fromthe training set were compiled based on their simi-larity to the target document.
These documentswere then used to train a model for the Illinois NEtagger.
In this way we strive to reduce the noisepresent in the context aggregation feature as a re-sult of the generic contexts found in a large, oftenheterogeneous, training set and produce featurevalues that are more representative of the targetentities, thus producing more reliable output labels.3.1 MethodologyFor an initial proof-of-concept test, vectorswere created for all test (not the development set)and training documents in the CoNLL-2003 sharedtask data.
This corpus was chosen due to the previ-ous NER research using this corpus and the resultsavailable using the LBJ tagger.
Also, it has beennoted that the test and training sets within the cor-pus are not as similar in nature as are the develop-ment and training sets (Ratinov and Roth, 2009).The training set contains 946 documents, while thetest set contains 231.
For each test document, aspecified number of the top documents from thetraining set most similar to that test document wascollected.
For this initial work, a simple cosinesimilarity measure was used.
These top similardocuments were used as a training set for the LBJtagger, and the test document was then tagged us-ing the resultant model.
The system was tested bypulling the top 20, 50, 100, and 300 similar train-ing documents to train the models.
The perfor-mance of this customized model is compared tothat of the standard, two-phase LBJ tagger trainedon the full CoNLL ?03 training set.3.2 ResultsFor this research, because each test document istagged using a different model, we chose to meas-ure our performance on a per-document basis, ra-ther than the standard overall measure for theentire test set.1 This performance is compared tothat achieved by the standard LBJ tagger on thesame document.
Figure 1 shows how many docu-ments were tagged more accurately using the pro-posed system compared to the LBJ tagger.Figure 1 ?
Results showing the number of documentsfor which each system performed better or for whichthey had equal F1 scores.Further, Figure 2 displays the average percent-age better and worse in terms of F1 score for eachtraining document size.
In contrast to Figure 1,Figure 2 demonstrates the average difference in F1scores between the LBJ tagger trained on the entiretraining set and the proposed system trained onvarying numbers of training documents.
Thesenumbers indicate that there exists an optimal bal-ance that can achieve the dual advantages of hav-ing a smaller, more relevant training set while alsomaintaining enough data to ensure enough featuresto accurately predict NER labels.The overall aggregated difference is also pro-vided as a more global view of performanceachievements.
This measurement is calculated bymultiplying the F1 score of a given document bythe number of entity tokens contained in that doc-ument, summing these calculations, and then divid-ing by the total number of entity tokens across thetest dataset.
These results reveal an improvementover the Illinois tagger for the 300 document train-1 The Illinois NE tagger only provides performance infor-mation in the form of percentages and does not give enoughinformation to calculate an overall F1 score for the test setusing the CoNLL eval script.05010015020 50 100 300#oftestdocuements# Training DocumentsLBJProposedsystemEqual413ing set with a weighted average F1 score of 91.67compared to the Illinois score of 91.32.Figure 2 ?
Average percentage points better and worsein the F1 score that the proposed system achieved com-pared to the standard LBJ tagger for models trained withthe top 20, 50, 100, and 300 similar documents.These initial results demonstrate that an availa-ble training set can be easily tailored to better servethe needs of a target data set that differs from thetraining set and showed improvements on an exist-ing competitive NER system by modifying thetraining data set used to build the prediction model.By identifying a smaller, relevant training set, thesequence tagging model is better equipped to accu-rately predict output labels for target data that doesnot closely align with the training documents.4 Future workGiven the computational expense of training amodel for each individual document to be tagged,improvements must be made to the approach totransform it into a viable long-term NER solution.The next logical step in this research will be tocluster the training documents and train modelsbased on those clusters.
Subsequently, the testdocuments can be clustered to the training set clus-ters and be tagged using the appropriate model forthat cluster set.
Alternatively, the test set could beinitially clustered, with the training set then fit tothose clusters.
Tests must be conducted to deter-mine which option produces the best predictionaccuracy levels.
Once a viable clustering method-ology has been developed, further testing will beconducted to compare it with some of the best cur-rent techniques (e.g.
the work of Dalton et.
al2011) to provide a more comprehensive evalua-tion.The results presented here were achieved usingbaseline document representation and documentsimilarity techniques.
Significant work remains forexperimentation to determine which alternativemethodologies will result in the optimal NER per-formance.
Not only could different clustering algo-rithms be employed, but an investigation intowhich type of clustering, in particular linear or hi-erarchical, is better suited for NER would be pru-dent.
Also, further work will test the validity ofthis approach for successful domain adaptation bydemonstrating that it is extensible to other datasets.5 SummaryThis research has implications in the NER do-main adaptation space as it demonstrates that fewertraining documents are required as long as they aresufficiently similar to the targeted test set.
Thismethodology could potentially allow for better uti-lization of existing, freely-available (possibly ge-neric) training sets by extracting portions of thetraining set that are more similar to the target data.It also allows for existing NER systems to be betteradapted to domain-specific data without modifica-tion for feature augmentation or the inclusion ofadditional external data sources.
The opportunitiesfor continuing this tread of research are numerous,and initial results illustrate significant promise giv-en the relative simplicity of the execution com-pared with its achievement.6 AcknowledgementsThis document was prepared by Oak Ridge Na-tional Laboratory, P.O.
Box 2008, Oak Ridge,Tennessee 37831-6285; managed by UT-Battelle,LLC, for the US Department of Energy under con-tract number DE-AC05-00OR22725.This manuscript has been authored by UT-Battelle, LLC, under contract DE-AC05-00OR22725 with the U.S. Department of Energy.The United States Government retains and the pub-lisher, by accepting the article for publication,acknowledges that the United States Governmentretains a non-exclusive, paid-up, irrevocable,world-wide license to publish or reproduce thepublished form of this manuscript, or allow othersto do so, for United States Government purposes.05101520 50 100 300Percentagepoints# of training documentsAvg.
betterAvg.
worse414ReferencesDan Wu, Wee Sun Lee, and Hai Leong Chieu.
2009.Domain adaptive bootstrapping for named entityrecognition.
In EMNLP, pp.
142 - 147.Fei Hueng, and Alexander Yates.
2009.
Distributionalrepresentations for handling sparsity in supervisedsequence-labeling.
Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP.
1, pp.
495-503.Suntec, Singapore: ACL.Hai Leong Chieu, and Hwee Tou  Ng.
2003.
Namedentity recognition with a maximum entropy approach.Proceedings CoNLL 2003, (pp.
160-163).
Edmonton,Canada.Jeffrey Dalton, James Allan, and David A. Smith.
2011.Passage retrieval for incorporating global evidence insequence labeling.
Proceedings of the 20th ACMInternational Conference on Information andKnowledge Management (pp.
355-364).
Glasgow,UK: ACM.Lev Ratinov, and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.CoNLL '09 Proceedings of the Thirteenth Conferenceon Computational Natural Language Learning (pp.147-155).
Boulder, CO: Association forComputational Linguistics.Maksim Tkachenko and Andrey Simanovsky.
2012.Named entity recognition: exploiting features.
Pro-ceedings of KONVENS 2012.
Vienna, Austria.Terry Koo, Xavier Carreras, and Michael John Collins.2008.
Simple semi-supervised dependency parsing.Proceedings of ACL, (pp.
595-603).Vijay Krishnan, and Christopher D. Manning.
2006.
Aneffective two-stage model for exploiting non-localdependencies in named entity recognition.Proceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meetingof the ACL, (pp.
1121-1128).
Sydney, Australia.Xiaohua Liu, Shaodian Zhang, Furu Wei, and MingZhou.
2011.
Recognizing named entities in tweets.Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies.
1, pp.
359-367.
Portland,OR: Association for Computational Linguistics.415
