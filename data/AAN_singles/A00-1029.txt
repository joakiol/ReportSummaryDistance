A Tool for Automated Revision of Grammars for NLP SystemsNanda Kambhatla nd Wlodek ZadroznyIBM T.J. Watson Research Center30 Saw Mill River Road,Hawthorne, NY, 10532{nanda, wlodz} @us.ibm.comAbstractWe present an algorithm and a tool forautomatically revising grammars for naturallanguage processing (NLP) systems todisallow specifically identified sentences orsets of sentences.
We also outline anapproach for automatically revising attributevalue grammars using counter-examples.Developing rammars for NLP systems thatare both general enough to accept mostsentences about a domain, but constrainedenough to disallow other sentences i verytedious.
Our approach of revising grammarsautomatically using counter-examplesgreatly simplifies the development andrevision of tightly constrained grammars.We have successfully used our tool toconstrain over-generalizing rammars ofspeech understanding systems and obtainedhigher ecognition accuracy.1 IntroductionNatural language processing systems oftenconstrain the set of "utterances" from a user(spoken, typed in, etc.)
to narrow down thepossible syntactic and semantic resolutions ofthe utterance and reduce the number ofmisrecognitions and/or misunderstandings bythe system.
Such constraints on the allowedsyntax and the inferred semantics are oftenexpressed in the form of a "grammar "l, a set ofThroughout his document, by using the word"grammar", we refer to a Context-Free Grammar thatconsists of a finite set of non-terminals, a finite set ofterminals, a unique non-terminal called the startsymbol, and a set of production rules of the form A->a, where A is a non-terminal nd a is a string ofterminal or non-terminal symbols.
The 'language'rules specifying the set of allowed utterancesand possibly also specifying the semanticsassociated with these utterances.
For instance,grammars are commonly used in speechunderstanding systems to specify both the set ofallowed sentences and to specify "tags" toextract semantic entities (e.g.
the "amount" ofmoney).Constraining the number of sentences acceptedby a grammar is essential for reducingmisinterpretations of user queries by an NLPsystem.
For instance, for speech understandingsystems, if the grammar accepts a large numberof sentences, then the likelihood of recognizinguttered sentences as random, irrelevant, orundesirable sentences is increased.
Fortransaction processing systems, misrecognizedwords can lead to unintended transactions beingprocessed.
An effective constraining rammarcan reduce transactional errors by limiting thenumber of sentence l vel errors.
The problem ofover-generalization f speech grammars andrelated issues is well discussed by Seneff (1992).Thus, speech grammars must often balance theconflicting requirements of?
accepting a wide variety of sentences toincrease flexibility, and?
accepting a small number of sentencesto increase system accuracy androbustness.Developing tight grammars which trade-offthese conflicting constraints is a tedious andaccepted by a grammar is the set of all terminalstrings that can be generated from the start symbol bysuccessive application of the production rules.
Thegrammar may optionally have semantic interpretationrules associated with each production rule (e.g.
see(Allen 95)).210difficult process.
Typically, grammarsovergeneralize and accept too many sentencesthat are irrelevant or undesirable for a givenapplication.
We call such sentences "counter-examples".
The problem is usually handled byrevising the grammar manually to disallow suchcounter-examples.
For instance, the sentence"give me my last eighteen transactions" mayneed to be excluded from a grammar for aspeech understanding system, since the words"eighteen" and "ATM" are easily confused bythe speech recogniser.
However, "five" and"ten" should remain as possible modifiers of"transactions".
Counter-examples can also besets of sentences that need to be excluded from agrammar (specified by allowing the inclusion ofnon-terminals in counter-examples).
Forexample, for a banking application thatdisallows money transfers to online accounts, wemight wish to exclude the set of sentences"transfer <AMOUNT> dollars to my onlineaccount" from the grammar, where<AMOUNT> is a non-terminal in the grammarthat maps to all possible ways of specifyingamounts.In this paper, we are proposing techniques forautomatically revising grammars using counter-examples.
The grammar developer identifiescounter-examples from among sentences (or setsof sentences) mis-recognized by the speechrecognizer or from sentences randomlygenerated by a sentence generator using theoriginal grammar.
The grammar reviser modifiesF igure  I .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Grammar  Rev iserPerser L counter.-r examplesparse  ~t reeGrammar  L in i t ia lModi f ie r  I~ grammarrev ised  grammarthe original grammar to invalidate the counter-examples.
The revised grammar can be fed backto the grammar reviser and whole process can beiterated several times until the resultinggrammar is deemed satisfactory.In the next sections, we first describe ouralgorithm for revising grammars to disallowcounter-examples.
Wealso discuss algorithms tomake the revised grammar compact usingminimum description length (MDL) basedgrammar compaction techniques and extensionsto our basic algorithm to handle grammars withrecursion.
We then present some results ofapplying our grammar reviser tool to constrainspeech grammars of speech understandingsystems.
Finally, we present an approach forrevising attribute value grammars using ourtechnique and present our conclusions.2 Automated Grammar Revision by rulemodificationIn this section, we describe an algorithm (seeFigure 1) for revising grammars that directlymodifies the rules of the grammar to disallowcounter-examples.
For each counter-example 2,we generate the parse tree (representation f allthe grammar rules needed to generate thesentence or set of sentences) and the grammarmodifier modifies the production rules of thegrammar to invalidate the counter-example.
Thisprocess is repeated for each counter-exampleusing the revised grammar from the previousiteration for generating the parse tree for thecurrent counter-example.
If a counter-examplegenerates multiple parse trees, the abovealgorithm is repeated for each parse tree in turn.2.1 Grammar modification algorithmWe present the grammar modification algorithmbelow.
For, we assume that the parse-tree(s) ofthe counter-example contain no recursion (i.e.the same production rule does not occur twice inany of the parse trees).
In section 2.4, we presentan approach for using the algorithm even whenthe parse-trees contain recursion.
Thus, thealgorithm is applicable for any context-freegrammar.
The grammar modification algorithma Note that a counter-example can be a sentence suchas "move to operator" or a set of sentences such as"transfer <AMOUNT> to online account".
The latteris specified using non-terminals interspersed withwords.~11for modifying the rules of a grammar to disallowa counter-example c (identified by a grammardeveloper) using a parse-tree for e proceeds asfollows :1.
For each non-terminal <N> in the parsetree, except he <<START>> symbol,a.
Add a rule to define a new non-terminal <N'> such that <N'>generates all phrases that <N>generates except for the phrasein the counter-example that <N>generates.b.
Add a rule to define a new non-terminal <No> such that <No>generates only the phrase(s) inthe counter-example that <N>generates.2.
Modify the rule that contains the<<START>> symbol in the parse tree,such that the <<START>> symbol nolonger generates the given counter-example.Figure 2(a) Original grammar:<<START>> : :=  <V> <N> <PP> t<V> <PP> .<PP> : := " to  m <N> .<V> : := "move"  I " t rans fer"  .<N> : := "check ing"  I "sav ings"  I"money" \[ "operator"  .
(b) Parse Tree for "move to operator".
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.<%'> : == "move"  .
!I <PP> : := " to"  <N> .
1' <N> : := "operator" .
I............................................................................................................................... iWe illustrate the algorithm with an example.Figure 2(a) shows a simple grammar.
Supposethe sentence "move to operator" is a counter-example for an application.
Figure 2(b) showsthe parse-tree for "move to operator".
Since theparse tree contains the rule: <V> ::= "move",new rules are added to define non-terminals<V'> and <Vo>, where <V'> does not generate"move" and <Vo> generates only "move".Similarly, since the parse tree contains the rule:<N>::= "operator", the new rules: <N'>::="checking" I "savings" I "money"; and <No>::="operator", are added.
For the non-terminal<PP>, the new rules: <PP'>::= "to" <N'>; and<PPo>::= "to" <No>, are added.
Note that since<No> only generates the phrase "operator"which is part of the counter-example, <PPo>only generates the phrase "to operator" which ispart of the counter-example.
Also, <PP'>generates all phrases that <PP> generates exceptfor the phrase "to operator".
Finally, the rule:<<START>>::= <V> <PP> is modified usingthe newly created non-terminals <V'>, <Vo>,<PP'> and <PPo> such that the only sentenceswhich are accepted by the grammar and beginwith the phrase "move" do not end with thephrase "to operator", and also, the onlysentences which are accepted by the grammarand end with the phrase "to operator" do notbegin with the phrase "move".
Figure 3 showsthe final modified grammar that accepts all thesentences that the grammar in Figure 2(a)accepts except for the sentence "move toFigure 3<<START>> : := <V> <N> <PP> I<V'> <PPo> \]<Vo> <PP '> l<V '> <PP '> .<PP> : :=  " tO"  <N> .<PP '> : :=  " tO"  <N'> .<PPo> : :=  " tO"  <NO> .<V> : := "move"  \[ " t rans fer"  .<V'> :: = " t rans fer "  .<Vo> : :=  "move"  .<N> : := "check ing"  I " sav ings"  \["money"  I "operator"  .<N'> : :=  "check ing"  I " sav ings"  l?
money"  .<No> : :=  "operator "  .operator".
In Figure 3, all the grammar rules thatare new or modified are shown in bold anditalics.The above algorithm for grammar modificationhas a time complexity of O(m*2 k) rule creation(or modification) steps for removing a counter-example, where m is the number of productionrules in the parse tree of the counter-exampleand k is the largest number of non-terminals onthe right hand side of any of these production212rules.
Since grammars used for real applicationsrarely have more than a handful of non-terminalson the right hand side of production rules, thiscomplexity is quite manageable.2.2 Automated grammar compaction usingMDL based grammar inductionAs seen in the example described above, the sizeof the grammar (number of production rules) canincrease greatly by applying our algorithmsuccessively for a number of counter-examples.However, we can remedy this by applyinggrammar induction algorithms based onminimum description length (MDL) (e.g.Grunwald (1996) and Zadrozny (1997)) tocombine rules and create a compact grammarthat accepts the same language.The MDL principle (Rissanen (1982)) selectsthat description (theory) of data, whichminimizes the sum of the length, in bits, of thedescription of the theory, and the length, in bits,of data when encoded using the theory.
In ourcase, the data is the set of possible wordcombinations and the theory is the grammar thatspecifies it.
We are primarily interested in usingthe MDL principle to obtain (select) a compactgrammar (the theory) from among a set ofequivalent grammars.
Since the set of possibleword combinations (data) is the same for allgrammars in consideration, we focus on thedescription length of the grammars itself, whichwe approximate by using a set of heuristicsdescribed in step 1 below.We use the following modified version ofZadrozny's (1997) algorithm to generate a morecompact grammar from the revised grammarusing the MDL principle:1.
Compute the description length of thegrammar, i.e.
the total number ofsymbols needed to specify the grammar,where each non-terminal, "::=", and "1"are counted as one symbol.2.
Modify the current grammar byconcatenating all possible pairs of non-terminals, and compute the descriptionlength of each such resultant grammar.For concatenating <NI> and <N2>,introduce the rule <N3>::= <NI> <N2>,search all other rules for consecutiveoccurrences of <NI> and <N2>, andreplace such occurrences with <N3>.Note that this change results in anequivalent grammar (that accepts thesame set of sentences as the originalgrammar).3.
Modify the current grammar by mergingall possible pairs of non-terminals, andcompute the description length of eachsuch resultant grammar.
For merging<N4> and <N5>, introduce the rule:<N6>::= <N4> \[ <N5>, search for pairsof rules which differ only in oneposition such that for one of the rules,<N4> occurs in that position and theother rule, the <N5> occurs in the sameposition.
Replace the pair of rules with anew rule that is exactly the same aseither of the pairs of rules, except for theuse of <N6> instead of <N3> or <N4>.Note that this change results in anequivalent grammar (that accepts thesame set of sentences as the originalgrammar).4.
Compute a table of description lengthsof the grammars obtained byconcatenating or merging all possiblepairs of non- terminals of the initialgrammar, as described above.
Select hepair of non-terminals (if any) togetherwith the action (concatenate or merge)that results in the least descriptionlength and execute the correspondingaction.5.
Iterate steps 2, 3, and 4 until thedescription length does not decrease.No further modification is performed ifthe base description length of thegrammar is lower than that resultingfrom merging or concatenating any pairof non- terminals.In variations of this algorithm, the selection ofthe pairs of non-terminals to concatenate ormerge, can be based on; the syntactic ategoriesof the corresponding terminals, the semanticcategories of the corresponding terminals, andthe frequency of occurrence of the non-terminals.213Using the algorithm described above inconjunction with the algorithm in section 2.1, wecan obtain a compact grammar that is guaranteedto disallow the counter-examples.2.3 Results for grammar revision for speechunderstanding systemsWe have built a graphical tool for revisinggrammars for NLP systems based on thealgorithm described in sections 2.1 and 2.2above.
The tool takes as input an existinggrammar and can randomly generate sentencesaccepted by the grammar including non-terminalstrings and strings containing terminals and non-terminals (e.g.
both "move to operator" and"transfer <AMOUNT> to online account" wouldbe generated if they were accepted by thegrammar).
A grammar developer (a human)interacts with the tool and either inputs counter-examples selected from speech recognition errorlogs or selects counter-examples ike the oneslisted above.
The grammar developer can thenrevise the grammar to disallow the counter-examples by pressing a button and then reducethe size of the resulting grammar using thealgorithm in section 2.2 by pressing anotherbutton to obtain a compact grammar that doesnot accept any of the identified counter-examples.
Typically, the grammar developerrepeats the above cycle several times to obtain atightly constrained grammar.We have successfully used the tool describedabove to greatly constrain overgeneralizinggrammars for speech understanding systems thatwe built for telephony banking, stock tradingand directory assistance (Zadrozny et al 1998).The speech recognition grammars for thesesystems accepted around fifty million sentenceseach.
We successfully used the reviser tool toconstrain these grammars by eliminatingthousands of sentences and obtained around 20-30% improvement in sentence recognitionaccuracy.
We conducted two user studies of ourtelephony banking system at different stages ofdevelopment.
The user studies were conductedeight months apart.
During these eight months,we used a multi-pronged strategy of constraininggrammars using the grammar revisionalgorithms described in this paper, improvingthe pronunciation models of some words andredesigning the prompts of the system to enablefast and easy error recovery by users.
Thecombination of all these techniques resulted inimproving the 'successful transaction in firsttry '3 from 43% to 71?/0, an improvement of65%.The average number of wrong tries (turns ofconversation) to get a successful answer wasreduced from 2.1 to 0.5 tries.
We did notconduct experiments o isolate the contributionof each factor towards this improvement insystem performance.It is important o note here that we wouldprobably have obtained this improvement inrecognition accuracy even with a manualrevision of the grammars.
However, the mainadvantage in using our tool is the tremendoussimplification of the whole process of revisionfor a grammar developer who now selectscounter-examples with an interactive toolinstead of manually revising the grammars.2.4 Handling recursion in grammarsWe now describe an extension of the algorithmin section 2.1 that can modify grammars withrecursion to disallow a finite set of counter-examples.
The example grammars shown aboveare regular grammars (i.e.
equivalent finite stateautomatons exist).
For regular grammars (andonly for regular grammars), an alternativeapproach for eliminating counter-examplesusing standard automata theory is"?
Compute the finite state automaton(FSA) G corresponding to the originalgrammar.?
Compute the FSA C corresponding tothe set of counter-examples.?
Compute C', the complement of C withrespect to the given alphabet.?
Compute G', the intersection of G andC'.
The FSA G' is equivalent to a revisedgrammar which disallows the counter-examples.3 We measured the number of times the user'stransactional intent (e.g.
checking balance, last fivetransactions etc.)
was recognized and acted uponcorrectly by the system in the first try, even when theactual utterance may not have been recognizedcorrectly word for word.914.
214The time complexity of the algorithm is O(n*m),where n and m are the number of states in thefinite state automatons G and C respectively.This is comparable to the quadratic timecomplexity of our grammar revision algorithmpresented in Section 3.1.However, the above algorithm for eliminatingcounter-examples only works for regulargrammars.
This is because context-freegrammars are not closed under complementationand intersection.
However we can use ouralgorithm for grammar modification (section2.1) to handle any context-free grammar asfollows:1) As before, generate parse tree p forcounter-example c for an initialgrammar G.2) If p contains a recursion (two ormore repetitions of any productionrule in the same parse tree), rewritethe initial grammar G as theequivalent grammar G', where therecursion is "unrolled" sufficientlymany times (at least one more timethan the number of repetitions of therecursive production rule in theparse tree).
We explain the unrollingof recursion in greater detail below.If p does not contain any recursion,go to step 4.3) Generate parse tree p' for thecounter-example c for the rewrittengrammar G'.
Note that p' will nolonger contain a recursiveapplication of any production rules,though G' itself will still haverecursion.4) Use the algorithm described insection 2.1 to modify the grammarG' to eliminate the counter-examplec using the parse tree p'.We illustrate the above algorithm with anexample.
Figure 4(a) shows a context freegrammar which accepts all strings of the forma"b", for any n greater than 0.
Note that this isnot a regular language.
Suppose we wish toeliminate the counter-example aaabbb from theinitial grammar.
The parse treep for the counter-example aaabbb is shown in Figure 4(b).
Thegrammar in 4(a) can be rewritten as theequivalent grammar 4(c), where the recursion of(S->aSb) is unrolled three times.
The parse treep '  for the counter-example aaabbb with respectto grammar in 4(c) is shown in Figure 4(d).
Notethat p '  does not contain any recursion, thoughthe rewritten grammar does.
We revised theFIGURE 4(a) ORIG INAL  GRAMMAR G<S> : := "a"  <S> "b" \[ "a n "b" .
(b) PARSE TREE p<S> : := "a  n <S> "b" .<S> : := "a"  <S> "b" .<S> : := "a  n rib" .
(c) REWRITTEN GRAMMAR G'<S> : := "a" <$1> "b" l "a" "b" .<S l> : := "a" <$2> "b" I "a" "b"  .<$2> : := "a"  <$3> "b" I "a" "b" .<$3> : := "a"  <$3> "b"  \[ "a"  "b"  .
(d) PARSE TREE p'<S> : := "a"  <S l> "b" .<$1> : := "a" <$2> "b"  .<$2> : :=  "a"  "b" .~) REVISED GRAMMAR Gr<S> : := "a"  <S l> "b" \[ "a" "b" .<S I> : :=  "a"  <$2> "b" I "a"  "b"  .<82> : := "a" <$3> "b" .<$3> : :=  "a"  <$3> "b" \[ "a" "b"  .grammar in 4(c) to eliminate the counter-example aaabbb using the parse tree in Figure4(d).
The revised grammar is shown in Figure4(e).
Note that here we are assuming that amechanism exists for rewriting the rules of agrammar with recursion to unroll the recursion(if it exists) a finite number of times.
Such anunrolling is readily accomplished by introducinga set of new non-terminars, one for each iterationof unrolling as shown in Figure 4(c).3 Automated revision of attribute-valuegrammarsIn this section, we delineate an approach forautomatically modifying attribute valuegrammars using counter-examples.
We firstconvert an attribute value grammar into anequivalent non-attributed grammar by creatingnew non-terminals and encoding the attributes inthe names of the new non-terminals (seeManaster Ramer and Zadrozny (1990) andPollard and Sag (1994)).For example, suppose the grammar in Figure2(a) is an attribute value grammar with an~l f i  215Figure 5<<START>> : := <V> <N> <PP> \[<V> <pp> .<PP> : : :  " tO"  <N> .<V> : :=  "move"  \[ " t rans fer "  .<N> : := <N_account_check ing> \[<N_accountsavings> \[<N_accountunspec i f ied><N_ .account_check ing> : :=  "check ing"  .<N_account_savings> : :=  "sav ings" .<N_account_unspecified> ::= "money" I"operator "  .attribute 'account', which encodes informationabout the type of account specified, e.g.
'account' might have the values, SAVINGS,CHECKING and UNSPECIFIED.
Figure 5shows an equivalent non-attributed grammar,where the value of the attribute 'account' hasbeen encoded in the names of the non-terminals.Note that such an encoding can potentiallycreate a very large number of non-terminals.Also, the specific coding used needs to be such<<START>> : := <V> <N> <PP> \[ <V '> <PPO>Fiaure 6 .
-vo> .cp_p,> I <v ,> <pp,> .<PP> : := " to  n <N> .<PP '> :.
'= "tO" <N'> ?<PPo> : :=  "tO" <NO> ?<V> : := "move"  ~ " t rans fer "  .<Vr> :~= " t rans fer "  .<Vo> :~= .~ve  # .<N> :~= <Naccount_check ing> I<N_account_sav ings> 1<N__account_unspec i f ied> .<N'> : : :  <N_account_check ing> \]<N_account_savings> \]<N '_acco un t _unspec  i f i ed><No> : : :  <No_account_umspec i f ied> .<N_accountcheck ing> : := "check ing"  .<N_account  sav ings> : := "sav ings" .<Naccountunspec i f ied> : := "money"  I"operator "  .<N'_account__unspec i f ied> : := "money"  .<No account_unspec i f ied> : := "operator "  .that the attributes can be easily recovered fromthe non-terminal names later on.We can now use our modification algorithms(Section 2.1 and 2.2) to eliminate counter-examples from the non-attributed grammar.
Forinstance, suppose we wish to eliminate 'move tooperator' from the attributed grammar based onFigure 2(a), as discussed above.
We apply ouralgorithm (Section 2.1) to the grammar in Figure5 and obtain the grammar shown in Figure 6.Note that we name any new non-terminalscreated uring the grammar modification i sucha way as to leave the encoding of the attributevalues in the non-terminal names intact.After applying the grammar revision algorithm,we can extract the attribute values from theencoding in the non-terminal names.
Forinstance, in the example outlined above, wemight systematically check for suffixes of acertain type and recover the attributes and theirvalues.
Also, as described earlier, we can use thealgorithm described in section 2.2 to make theresulting rammar compact again by using MDLbased grammar induction algorithms.4 ConclusionsWe have presented a set of algorithms and aninteractive tool for automatically revisinggrammars of NLP systems to disallow identifiedcounter-examples (sentences or sets of sentencesaccepted by the current grammar but deemed tobe irrelevant for a given application).
We havesuccessfully used the tool to constrainovergeneralizing grammars of speechunderstanding systems and obtained 20-30%higher recognition accuracy.
However, webelieve the primary benefit of using our tool isthe tremendously reduced effort for the grammardeveloper.
Our technique relieves the grammardeveloper f om the burden of going through thetedious and time consuming task of revisinggrammars by manually modifying productionrules one at a time.
Instead, the grammardeveloper simply identifies counter-examples toan interactive tool that revises the grammar toinvalidate the identified sentences.We also discussed an MDL based algorithm forgrammar compaction to reduce the size of therevised grammar.
Thus, using a combination ofthe algorithms presented in this paper, one canobtain a compact grammar that is guaranteed todisallow the counter-examples.Although our discussion here was focussed onspeech understanding applications, thealgorithms and the tool described here areapplicable for any domain where grammars areused.
We are currently implementing anextension of the grammar modifier to handleattribute-value grammars.
We outlined an216approach for automated modification ofattribute-value grammars in Section 3.We conclude that algorithms for automaticallyconstraining grammars based on counter-examples can be highly effective in reducing theburden on grammar developers to developconstrained, domain specific grammars.Moreover, these algorithms can be used in anyapplications, which deal with grammars.AcknowledgementsWe thank all of our colleagues in theconversation machines group at IBM T.J.Watson Research Center for several helpfulcomments and suggestions through the course ofthis work.ReferencesZadrozny W., Wolf C., Kambhatla N., and Ye Y.(1998).
Conversation machines for transactionprocessing.
In proceedings of AAAI'98/IAAI'98,AAAI Press/MIT Press, pp 1160-1166.Allen J.
(1995).
Natural Language Understanding.The Benjamin/Cummings Publishing Company,Redwood City, CA 94065.Gnmwald P. (1996).
A minimum description lengthapproach to grammar inference.
In S. Wemter etal., editors, Symbolic, Connectionist and StatisticalApproach to Learning for Natural LanguageProcessing, Springer, Berlin, p 203-216.Manaster-Ramer and Zadrozny W. (1990),Expressive Power of Grammatical Formalisms,Proceedings of Coling-90.
UniversitasHelsingiensis.
Helsinki, Finland", pp.
195-200.Pollard, C. and Sag I A.
(1994).
Head-Driven PhraseStructure Grammar.
The U. of Chicago Press.Rissanen J.
(1982).
A universal prior for integers andestimation by minimum description length.
Annalsof Statistics, 11:416-431.Seneff S. (1992).
TINA: A natural anguage systemfor spoken language applications, ComputationalLinguistics, 18:p61-86.Zadrozny W. (1997).
Minimum description lengthand compositionality.
Proceedings of SecondInternational Workshop for ComputationalSemantics, Tilburg.
Recently re-published as abook chapter in: H.Bunt and R.Muskens (eds.
)Computing Meaning.
Kluwer AcademicPublishers, Dordrecht/Boston, 1999.~17 217
