Feature Selection for Fluency RankingDanie?l de KokUniversity of Groningend.j.a.de.kok@rug.nlAbstractFluency rankers are used in modern sentencegeneration systems to pick sentences that arenot just grammatical, but also fluent.
It hasbeen shown that feature-based models, such asmaximum entropy models, work well for thistask.Since maximum entropy models allow for in-corporation of arbitrary real-valued features,it is often attractive to create very generalfeature templates, that create a huge num-ber of features.
To select the most discrim-inative features, feature selection can be ap-plied.
In this paper we compare three fea-ture selection methods: frequency-based se-lection, a generalization of maximum entropyfeature selection for ranking tasks with real-valued features, and a new selection methodbased on feature value correlation.
We showthat the often-used frequency-based selectionperforms badly compared to maximum en-tropy feature selection, and that models with afew hundred well-picked features are compet-itive to models with no feature selection ap-plied.
In the experiments described in this pa-per, we compressed a model of approximately490.000 features to 1.000 features.1 IntroductionAs shown previously, maximum entropy modelshave proven to be viable for fluency ranking (Nakan-ishi et al, 2005; Velldal and Oepen, 2006; Velldal,2008).
The basic principle of maximum entropymodels is to minimize assumptions, while impos-ing constraints such that the expected feature valueis equal to the observed feature value in the train-ing data.
In its canonical form, the probability of acertain event (y) occurring in the context (x) is a log-linear combination of features and feature weights,whereZ(x) is a normalization over all events in con-text x (Berger et al, 1996):p(y|x) =1Z(x)expn?i=1?ifi (1)The training process estimates optimal featureweights, given the constraints and the principle ofmaximum entropy.
In fluency ranking the input (e.g.a dependency structure) is a context, and a realiza-tion of that input is an event within that context.Features can be hand-crafted or generated auto-matically using very general feature templates.
Forexample, if we apply a template rule that enumer-ates the rules used to construct a derivation tree tothe partial tree in figure 1 the rule(max xp(np)) andrule(np det n) features will be created.Figure 1: Partial derivation tree for the noun phrase deadviezen (the advices).To achieve high accuracy in fluency rankingquickly, it is attractive to capture as much of the lan-guage generation process as possible.
For instance,in sentence realization, one could extract nearly ev-ery aspect of a derivation tree as a feature using verygeneral templates.
This path is followed in recentwork, such as Velldal (2008).
The advantage of thisapproach is that it requires little human labor, andgenerally gives good ranking performance.
How-ever, the generality of templates leads to huge mod-els in terms of number of features.
For instance, themodel that we will discuss contains about 490,000features when no feature selection is applied.
Suchmodels are very opaque, giving very little under-standing of good discriminators for fluency ranking,and the size of the models may also be inconvenient.To make such models more compact and transpar-ent, feature selection can be applied.In this paper we make the following contribu-tions: we modify a maximum entropy feature selec-tion method for ranking tasks; we introduce a newfeature selection method based on statistical corre-lation of features; we compare the performance ofthe preceding feature selection methods, plus a com-monly used frequency-based method; and we givean analysis of the most effective features for fluencyranking.2 Feature Selection2.1 IntroductionFeature selection is a process that tries to extractS ?
F from a set of features F , such that the modelusing S performs comparably to the model usingF .
Such a compression of a feature set can be ob-tained if there are features: that occur sporadically;that correlate strongly with other features (featuresthat show the same behavior within events and con-texts); or have values with little or no correlation tothe classification or ranking.Features that do have no correlation to the classi-fication can be removed from the model.
For a setof highly-correlating features, one feature can be se-lected to represent the whole group.Initially it may seem attractive to perform fluencyselection by training a model on all features, select-ing features with relatively high weights.
However,if features overlap, weight mass will usually be di-vided over these features.
For instance, suppose thatf1 alone has a weight of 0.5 in a given model.
Ifwe retrain the model, after adding the features f2..f5that behave identically to f1, the weight may be dis-tributed evenly between f1..f5, giving each featurethe weight 0.1.In the following sections, we will give a shortoverview of previous research in feature selection,and will then proceed to give a more detailed de-scription of three feature selection methods.2.2 BackgroundFeature selection can be seen as model selection,where the best model of all models that can beformed using a set of features should be selected.Madigan and Raftery (1994) propose an method formodel selection aptly named Occam?s window.
Thismethod excludes models that do not perform com-petitively to other models or that do not perform bet-ter than one of its submodels.
Although this methodis conceptually firm, it is nearly infeasable to applyit with the number of features used in fluency rank-ing.
Berger et al (1996) propose a selection methodthat iteratively builds a maximum entropy model,adding features that improve the model.
We modifythis method for ranking tasks in section 2.5.
Ratna-parkhi (1999) uses a simple frequency-based cutoff,where features that occur infrequently are excluded.We discuss a variant of this selection criterium insection 2.3.
Perkins et al (2003) describe an ap-proach where feature selection is applied as a partof model parameter estimation.
They rely on thefact that `1 regularizers have a tendency to force asubset of weights to zero.
However, such integratedapproaches rely on parameter tuning to get the re-quested number of features.In the fluency ranking literature, the use of a fre-quency cut-off (Velldal and Oepen, 2006) and `1regularization (Cahill et al, 2007) is prevalent.
Weare not aware of any detailed studies that comparefeature selection methods for fluency ranking.2.3 Frequency-based SelectionIn frequency-based selection we follow Malouf andVan Noord (2004), and count for each feature f thenumber of inputs where there are at least two realiza-tions y1, y2, such that f(y1) 6= f(y2).
We then usethe first N features with the most frequent changesfrom the resulting feature frequency list.Veldall (2008) also experiments with this selec-tion method, and suggests to apply frequency-basedselection to fluency ranking models that will be dis-tributed to the public (for compactness?
sake).
Inthe variant he and Malouf and Van Noord (2004)discuss, all features that change within more than ncontexts are included in the model.2.4 Correlation-based SelectionWhile frequency-based selection helps selecting fea-tures that are discriminative, it cannot account forfeature overlap.
Discriminative features that have astrong correlation to features that were selected pre-viously may still be added.To detect overlap, we calculate the correlation of acandidate feature and exclude the feature if it showsa high correlation with features selected previously.To estimate Pearson?s correlation of two features, wecalculate the sample correlation coefficient,rf1,f2 =?x?X,y?Y (f1(x, y)?
f?1)(f2(x, y)?
f?2)(n?
1)sf1sf2(2)where f?x is the average feature value of fx, andsfx is the sample standard deviation of fx.Of course, correlation can only indicate overlap,and is in itself not enough to find effective features.In our experiments with correlation-based selectionwe used frequency-based selection as described in2.3, to make an initial ranking of feature effective-ness.2.5 Maximum Entropy Feature SelectionCorrelation-based selection can detect overlap, how-ever, there is yet another spurious type of featurethat may reduce its effectiveness.
Features with rel-atively noisy values may contribute less than theirfrequency of change may seem to indicate.
For in-stance, consider a feature that returns a completelyrandom value for every context.
Not only does thisfeature change very often, its correlation with otherfeatures will also be weak.
Such a feature may seemattractive from the point of view of a frequency orcorrelation-based method, but is useless in practice.To account for both problems, we have to measurethe effectiveness of features in terms of how muchtheir addition to the model can improve predictionof the training sample.
Or in other words: does thelog-likelihood of the training data increase?We have modified the Selective Gain Com-putation (SGC) algorithm described by Zhou etal.
(2003) for ranking tasks rather than classificationtasks.
This method builds upon the maximum en-tropy feature selection method described by Bergeret al (1996).
In this method features are added iter-atively to a model that is initially uniform.
Duringeach step, the feature that provides the highest gainas a result of being added to the model, is selectedand added to the model.In maximum entropy modeling, the weights of thefeatures in a model are optimized simultaneously.However, optimizing the weights of the features inmodel pS,f for every candidate feature f is compu-tationally intractable.
As a simplification, it is as-sumed that the weights of features that are alreadyin the model are not affected by the addition of afeature f .
As a result, the optimal weight ?
of f canbe found using a simple line search method.However, as Zhou et al (2003) note, there is stillan inefficiency in that the weight of every candidatefeature is recalculated during every selection step.They observe that gains of remaining candidate fea-tures rarely increase as the result of adding a fea-ture.
If it is assumed that this never happens, a listof candidate features ordered by gain can be kept.To account for the fact that the topmost feature inthat list may have lost its effectiveness as the resultof a previous addition of a feature to the model, thegain of the topmost feature is recalculated and rein-serted into the list according to its new gain.
Whenthe topmost feature retains its position, it is selectedand added to the model.Since we use feature selection with features thatare not binary, and for a ranking task, we modifiedthe recursive forms of the model to:sum?S?f (y|x) = sumS(y|x) ?
e?f(y) (3)Z?S?f (x) = ZS(x)?
?ysumS(y|x)+?ysumS?f (y|x) (4)Another issue that needs to be dealt with is thecalculation of context and event probabilities.
In theliterature two approaches are prevalent.
The first ap-proach divides the probability mass uniformly overcontexts, and the probability of events within a con-text is proportional to the event score (Osborne,2000):p(x) =1|X|(5)p(y|x) =p(x)( score(x,y)Py score(x,y))(6)where |X| is the number of contexts.
The sec-ond approach puts more emphasis on the contextsthat contain relatively many events with high scores,by making the context probability dependent on thescores of events within that context (Malouf and vanNoord, 2004):p(x) =?y score(x, y)?y?X score(x, y)(7)In our experiments, the second definition of con-text probability outperformed the first by such awide margin, that we only used the second defini-tion in the experiments described in this paper.2.6 A Note on Overlap DetectionAlthough maximum-entropy based feature-selectionmay be worthwhile in itself, the technique can alsobe used during feature engineering to find overlap-ping features.
In the selection method of Berger etal.
(1996), the weight and gain of each candidate fea-ture is re-estimated during each selection step.
Wecan exploit the changes in gains to detect overlap be-tween a selected feature fn, and the candidates forfn+1.
If the gain of a feature changed drastically inthe selection of fn+1 compared to that of fn, thisfeature has overlap with fn.To determine which features had a drastic changein gain, we determine whether the change has a sig-nificance with a confidence interval of 99% afternormalization.
The normalized gain change is cal-culated in the following manner as described in al-gorithm 1.Algorithm 1 Calculation of the normalized gaindelta?Gf ?
Gf,n ?Gf,n?1if ?Gf ?
0.0 then?Gf,norm ?
?GfGf ,nelse?Gf,norm ?
?GfGf,n?1end if3 Experimental Setup3.1 TaskWe evaluated the feature selection methods in con-junction with a sentence realizer for Dutch.
Sen-tences are realized with a chart generator for theAlpino wide-coverage grammar and lexicon (Boumaet al, 2001).
As the input of the chart genera-tor, we use abstract dependency structures, whichare dependency structures leaving out informationsuch as word order.
During generation, we store thecompressed derivation trees and associated (HPSG-inspired) attribute-value structures for every real-ization of an abstract dependency structure.
Wethen use feature templates to extract features fromthe derivation trees.
Two classes of features (andtemplates) can be distinguished output features thatmodel the output of a process and construction fea-tures that model the process that constructs the out-put.3.1.1 Output FeaturesCurrently, there are two output features, both rep-resenting auxiliary distributions (Johnson and Rie-zler, 2000): a word trigram model and a part-of-speech trigram model.
The part-of-speech tag setconsists of the Alpino part of speech tags.
Bothmodels are trained on newspaper articles, consist-ing of 110 million words, from the Twente NieuwsCorpus1.The probability of unknown trigrams is estimatedusing linear interpolation smoothing (Brants, 2000).Unknown word probabilities are determined withLaplacian smoothing.1http://wwwhome.cs.utwente.nl/druid/TwNC/TwNC-main.html3.1.2 Construction FeaturesThe construction feature templates consist of tem-plates that are used for parse disambiguation, andtemplates that are specifically targeted at generation.The parse disambiguation features are used in theAlpino parser for Dutch, and model various linguis-tic phenomena that can indicate preferred readings.The following aspects of a realization are describedby parse disambiguation features:?
Topicalization of (non-)NPs and subjects.?
Use of long-distance/local dependencies.?
Orderings in the middle field.?
Identifiers of grammar rules used to build thederivation tree.?
Parent-daughter combinations.Output features for parse disambiguation, suchas features describing dependency triples, were notused.
Additionally, we use most of the templates de-scribed by Velldal (2008):?
Local derivation subtrees with optional grand-parenting, with a maximum of three parents.?
Local derivation subtrees with back-off andoptional grand-parenting, with a maximum ofthree parents.?
Binned word domination frequencies of thedaughters of a node.?
Binned standard deviation of word dominationof node daughters.3.2 DataThe training and evaluation data was constructed byparsing 11764 sentences of 5-25 tokens, that wererandomly selected from the (unannotated) DutchWikipedia of August 20082, with the wide-coverageAlpino parser.
For every sentence, the best parse ac-cording to the disambiguation component was ex-tracted and considered to be correct.
The Alpinosystem achieves a concept accuracy of around 90%on common Dutch corpora (Van Noord, 2007).
The2http://ilps.science.uva.nl/WikiXML/original sentence is considered to be the best realiza-tion of the abstract dependency structure of the bestparse.We then used the Alpino chart generator to con-struct derivation trees that realize the abstract de-pendency structure of the best parse.
The result-ing derivation trees, including attribute-value struc-tures associated with each node, are compressed andstored in a derivation treebank.
Training and testingdata was then obtained by extracting features fromderivation trees stored in the derivation treebank.At this time, the realizations are also scored usingthe General Text Matcher method (GTM) (Melamedet al, 2003), by comparing them to the originalsentence.
We have previously experimented withROUGE-N scores, which gave rise to similar results.However, it is shown that GTM shows the highestcorrelation with human judgments (Cahill, 2009).3.3 MethodologyTo evaluate the feature selection methods, we firsttrain models for each selection method in threesteps: 1.
For each abstract dependency structure inthe training data 100 realizations (and correspondingfeatures) are randomly selected.
2.
Feature selectionis applied, and the N -best features according to theselection method are extracted.
3.
A maximum en-tropy model is trained using the TADM3 software,with a `2 prior of 0.001, and using the N -best fea-tures.We used 5884 training instances (abstract depen-dency trees, and scored realizations) to train themodel.
The maximum entropy selection method wasused with a weight convergence threshold of 1e?6.Correlation is considered to be strong enough foroverlap in the correlation-based method when twofeatures have a correlation coefficient of rf1,f2 ?0.9Each model is then evaluated using 5880 held-out evaluation instances, where we select only in-stances with 5 or more realizations (4184 instances),to avoid trivial ranking cases.
For every instance,we select the realization that is the closest to theoriginal sentence to be the correct realization4.
Wethen calculate the fraction of instances for which the3http://tadm.sourceforge.net/4We follow this approach, because the original sentence isnot always exactly reproduced by the generator.model picked the correct sentence.
Of course, this isa fairly strict evaluation, since there may be multipleequally fluent sentences.4 Results4.1 Comparing the CandidatesSince each feature selection method that we evalu-ated gives us a ranked list of features, we can trainmodels for an increasing number of features.
Wehave followed this approach, and created models foreach method, using 100 to 5000 features with a stepsize of 100 features.
Figure 2 shows the accuracyfor all selection methods after N features.
We havealso added the line that indicates the accuracy thatis obtained when a model is trained with all features(490667 features).0.050.10.150.20.250.30.350.40.450  1000  2000  3000  4000  5000Best matchaccuracyFeaturesallmaxentcutoffcorrFigure 2: Accuracy of maximum entropy, correlation-based, and frequency-based selection methods after se-lecting N features (N ?
5000), with increments of 100features.In this graph we can see two interesting phenom-ena.
First of all, only a very small number of fea-tures is required to perform this task almost as wellas a model with all extracted features.
Secondly, themaximum entropy feature selection model is ableto select the most effective features quickly - fewerthan 1000 features are necessary to achieve a rela-tively high accuracy.As expected, the frequency-based method faredworse than maximum entropy selection.
Initiallysome very useful features, such as the n-grammodels are selected, but improvement of accuracyquickly stagnates.
We expect this to be caused byoverlap of newly selected features with features thatwere initially selected.
Even after selecting 5000features, this method does not reach the same accu-racy as the maximum entropy selection method hadafter selecting only a few hundred features.The correlation-based selection method fares bet-ter than the frequency-based method without over-lap detection.
This clearly shows that feature over-lap is a problem.
However, the correlation-basedmethod does not achieve good accuracy as quicklyas the maximum entropy selection method.
Thereare three possible explanations for this.
First, theremay be noisy features that are frequent, and sincethey show no overlap with selected features they aregood candidates according to the correlation-basedmethod.
Second, less frequent features that overlapwith a frequent feature in a subset of contexts mayshow a low correlation.
Third, some less frequentfeatures may still be very discriminative for the con-texts where they appear, while more frequent fea-tures may just be a small indicator for a sentenceto be fluent or non-fluent.
It is possible to refinethe correlation-based method to deal with the secondclass of problems.
However, the lack of performanceof the correlation-based method makes this unattrac-tive - during every selection step a candidate featureneeds to be compared with all previously selectedfeatures, rather than some abstraction of them.Table 1 shows the peak accuracies when select-ing up to 5000 features with the feature selectionmethods described.
Accuracy scores of the randomselection baseline, the n-gram models, and a modeltrained on all features are included for comparison.The random selection baseline picks a realizationsrandomly.
The n-gram models are the very samen-gram models that were used as auxiliary distribu-tions in the feature-based models.
The combinedword/tag n-gram model was created by training amodel with both n-gram models as the only fea-tures.
We also list a variation of the frequency-basedmethod often used in other work (such as Velldal(2008) and Malouf and Van Noord (2004)), wherethere is a fixed frequency threshold (here 4), ratherthan using the first N most frequently changing fea-tures.Besides confirming the observation that featureselection can compress models very well, this tableshows that the popular method of using a frequencycutoff, still gives a lot of opportunity for compress-ing the model further.
In practice, it seems best toplot a graph as shown in figure 2, choose an accept-able accuracy, and to use the (number of) featuresthat can provide that accuracy.Method Features AccuracyRandom 0 0.0778Tag n-gram 1 0.2039Word n-gram 1 0.2799Word/tag n-gram 2 0.2908All 490667 0.4220Fixed cutoff (4) 90103 0.4181Frequency 4600 0.4029Correlation 4700 0.4172Maxent 4300 0.4201Table 1: Peak accuracies for the maximum entropy,correlation-based, and frequency-based selection meth-ods when selecting up to 5000 features.
Accuracies forrandom, n-gram and full models are included for com-parison.4.2 Overlap in Frequency-based SelectionAs we argued in section 2.5, the primary disadvan-tage of the frequency-based selection is that it cannotaccount for correlation between features.
In the ex-treme case, we could have two very distinctive fea-tures f1 and f2 that behave exactly the same in anyevent.
While adding f2 after adding f1 does not im-prove the model, frequency-based selection cannotdetect this.
To support this argumentation empiri-cally, we analyzed the first 100 selected features tofind good examples of this overlap.Initially, the frequency-based selection choosesthree distinctive features that are also selected bythe maximum entropy selection method: the two n-gram language models, and a preference for topical-ized NP subjects.
After that, features that indicatewhether the vp arg v(np) rule was used change veryfrequently within a context.
However, this aspectof the parse tree is embodied in 13 successively se-lected features.
Due to the generality of the featuretemplates, there are multiple templates to capture theuse of this grammar rule: through local derivationtrees (with optional grandparenting), back-off for lo-cal derivation trees, and the features that calculatelexical node dominance.Another example of such overlap in the first100 features is in features modeling the use of thenon wh topicalization(np) rule.
Features containingthis rule identifier are used 30 times in sequence,where it occurs in local derivation subtrees (withvarying amounts of context), back-off local deriva-tion subtrees, lexical node domination, or as a grand-parent of another local derivation subtree.In the first 100 features, there were many overlap-ping features, and we expect that this also is the casefor more infrequent features.4.3 Effective FeaturesThe maximum entropy selection method shows thatonly a small number of features is necessary to per-form fluency ranking (section 4.1).
The first fea-tures that were selected in maximum entropy selec-tion can give us good insight of what features areimportant for fluency ranking.
Table 2 shows the 10topmost features as returned by the maximum en-tropy selection.
The weights shown in this table, arethose given by the selection method, and their signindicates whether the feature was characteristic of afluent sentence (+) or a non-fluent sentence (?
).As expected (see table 1) the n-gram models area very important predictor for fluency.
The onlysurprise here may be that the overlap between bothn-gram models is small enough to have both mod-els as a prominent feature.
While the tag n-grammodel is a worse predictor than the word n-grammodel, we expect that the tag n-gram model is espe-cially useful for estimating fluency of phrases withword sequences that are unknown to the word n-gram model.The next feature that was selected,r2(vp arg v(pred),2,vproj vc), indicates thatthe rule vp arg v(pred) was used with a vproj vcnode as its second daughter.
This combinationoccurs when the predicative complement is placedafter the copula, for instance as in Amsterdam is dehoofdstad van Nederland (Amsterdam is the capitalof The Netherlands), rather than De hoofdstadvan Nederland is Amsterdam (The capital of TheNetherlands is Amsterdam).The feature s1(non subj np topic) and its neg-ative weight indicates that realizations with non-topicalized NP subjects are dispreferred.
In Dutch,non-topicalized NP subjects arise in the OVS word-order, such as in de soep eet Jan (the soup eats Jan).While this is legal, SVO word-order is clearly pre-ferred (Jan eet de soep).The next selected feature (ldsb(vc vb,vb v,[vproj vc,vp arg v(pp)])) is also related to top-icalization: it usually indicates a preference forprepositional complements that are not topicalized.For instance, dit zorgde voor veel verdeeldheid(this caused lots of discord) is preferred over thePP-topicalized voor veel verdeeldheid zorgde dit(lots of discord caused this).ldsb(n n pps,pp p arg(np),[]) gives preferencePP-ordering in conjuncts where the PP modifier fol-lows the head.
For instance, the conjunct groepenvan bestaan of khandas (planes of existance or khan-das) is preferred by this feature over van bestaangroepen of khandas (of existence planes or khan-das).The next feature (lds dl(mod2,[pp p arg(np)],[1],[non wh topicalization(modifier)])) forms anexception to the dispreference of topicalization ofPPs.
If we have a PP that modifies a copula in asubject-predicate structure, topicalization of the PPcan make the realization more fluent.
For instance,volgens Williamson is dit de synthese (according toWilliamson is this the synthesis) is considered morefluent than dit is de synthese volgens Williamson(this is the synthesis according to Williamson).The final three features deal with punctuation.Since punctuation is very prevalent in Wikipediatexts due to the amount of definitions and clarifi-cations, punctuation-related features are common.Note that the last two lds dl features may seem tobe overlapping, they are not: they use different fre-quency bins for word domination.5 Conclusions and Future WorkOur conclusion after performing experiments withfeature selection is twofold.
First, fluency modelscan be compressed enormously by applying featureselection, without losing much in terms of accuracy.Second, we only need a small number of targetedfeatures to perform fluency ranking.The maximum entropy feature selection methodWeight Name0.012 ngram lm0.009 ngram tag0.087 r2(vp arg v(pred),2,vproj vc)-0.094 s1(non subj np topic)0.090 ldsb(vc vb,vb v,[vproj vc,vp arg v(pp)])0.083 ldsb(n n pps,pp p arg(np),[])0.067 lds dl(mod2,[pp p arg(np)],[1],[non wh topicalization(modifier)])0.251 lds dl(start start ligg streep,[top start xp,punct(ligg streep),top start xp],[0,0,1],[top start])0.186 lds dl(start start ligg streep,[top start xp,punct(ligg streep),top start xp],[0,0,2],[top start])0.132 r2(n n modroot(haak),5,l)Table 2: The first 10 features returned by maximum en-tropy feature selection, including the weights estimatedby this feature selection method.shows a high accuracy after selecting just a few fea-tures.
The commonly used frequency-based selec-tion method fares far worse, and requires additionof many more features to achieve the same perfor-mance as the maximum entropy method.
By exper-imenting with a correlation-based selection methodthat uses the frequency method to make an initialordering of features, but skips features that show ahigh correlation with previously selected features,we have shown that the ineffectiveness of frequency-based selection can be attributed partly to featureoverlap.
However, the maximum entropy methodwas still more effective in our experiments.In the future, we hope to evaluate the same tech-niques to parse disambiguation.
We also plan tocompare the feature selection methods described inthis paper to selection by imposing a `1 prior.The feature selection methods described in thispaper are usable for feature sets devised for rankingand classification tasks, especially when huge sets ofautomatically extracted features are used.
An opensource implementation of the methods described inthis paper is available5, and is optimized to work onlarge data and feature sets.5http://danieldk.eu/Code/FeatureSqueeze/ReferencesA.L.
Berger, V.J.D.
Pietra, and S.A.D.
Pietra.
1996.
Amaximum entropy approach to natural language pro-cessing.
Computational linguistics, 22(1):71.G.
Bouma, G. Van Noord, and R. Malouf.
2001.
Alpino:Wide-coverage computational analysis of Dutch.
InComputational Linguistics in the Netherlands 2000.Selected Papers from the 11th CLIN Meeting.T.
Brants.
2000.
TnT ?
a statistical part-of-speech tagger.In Proceedings of the Sixth Applied Natural LanguageProcessing (ANLP-2000), Seattle, WA.A.
Cahill, M. Forst, and C. Rohrer.
2007.
Stochas-tic realisation ranking for a free word order language.In ENLG ?07: Proceedings of the Eleventh EuropeanWorkshop on Natural Language Generation, pages17?24, Morristown, NJ, USA.
Association for Com-putational Linguistics.A.
Cahill.
2009.
Correlating Human and AutomaticEvaluation of a German Surface Realiser.
In Proceed-ings of the ACL-IJCNLP 2009 Conference Short Pa-pers, pages 97?100.M.
Johnson and S. Riezler.
2000.
Exploiting auxiliarydistributions in stochastic unification-based grammars.In Proceedings of the 1st North American chapter ofthe Association for Computational Linguistics confer-ence, pages 154?161, San Francisco, CA, USA.
Mor-gan Kaufmann Publishers Inc.D.
Madigan and A.E.
Raftery.
1994.
Model selection andaccounting for model uncertainty in graphical modelsusing Occam?s window.
Journal of the American Sta-tistical Association, 89(428):1535?1546.R.
Malouf and G. van Noord.
2004.
Wide cover-age parsing with stochastic attribute value grammars.In IJCNLP-04 Workshop: Beyond shallow analyses -Formalisms and statistical modeling for deep analy-ses.
JST CREST, March.I.
D. Melamed, R. Green, and J. P. Turian.
2003.
Pre-cision and recall of machine translation.
In HLT-NAACL.H.
Nakanishi, Y. Miyao, and J. Tsujii.
2005.
Probabilis-tic models for disambiguation of an hpsg-based chartgenerator.
In Parsing ?05: Proceedings of the NinthInternational Workshop on Parsing Technology, pages93?102, Morristown, NJ, USA.
Association for Com-putational Linguistics.M.
Osborne.
2000.
Estimation of stochastic attribute-value grammars using an informative sample.
In Pro-ceedings of the 18th conference on Computational lin-guistics, pages 586?592, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.S.
Perkins, K. Lacker, and J. Theiler.
2003.
Grafting:Fast, incremental feature selection by gradient descentin function space.
The Journal of Machine LearningResearch, 3:1333?1356.A.
Ratnaparkhi.
1999.
Learning to parse naturallanguage with maximum entropy models.
MachineLearning, 34(1):151?175.G.
Van Noord.
2007.
Using self-trained bilexical pref-erences to improve disambiguation accuracy.
In Pro-ceedings of the 10th International Conference on Pars-ing Technologies, pages 1?10.
Association for Compu-tational Linguistics.E.
Velldal and S. Oepen.
2006.
Statistical ranking intactical generation.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 517?525.
Association for Compu-tational Linguistics.E.
Velldal.
2008.
Empirical Realization Ranking.
Ph.D.thesis, University of Oslo, Department of Informatics.Y.
Zhou, F. Weng, L. Wu, and H. Schmidt.
2003.
Afast algorithm for feature selection in conditional max-imum entropy modeling.
