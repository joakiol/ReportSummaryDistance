Improving IBM Word-Alignment Model 1Robert C. MOOREMicrosoft ResearchOne Microsoft WayRedmond, WA 90052USAbobmoore@microsoft.comAbstractWe investigate a number of simple methods forimproving the word-alignment accuracy of IBMModel 1.
We demonstrate reduction in alignmenterror rate of approximately 30% resulting from (1)giving extra weight to the probability of alignmentto the null word, (2) smoothing probability esti-mates for rare words, and (3) using a simple heuris-tic estimation method to initialize, or replace, EMtraining of model parameters.1 IntroductionIBM Model 1 (Brown et al, 1993a) is a word-alignment model that is widely used in workingwith parallel bilingual corpora.
It was originallydeveloped to provide reasonable initial parameterestimates for more complex word-alignment mod-els, but it has subsequently found a host of ad-ditional uses.
Among the applications of Model1 are segmenting long sentences into subsententalunits for improved word alignment (Nevado et al,2003), extracting parallel sentences from compara-ble corpora (Munteanu et al, 2004), bilingual sen-tence alignment (Moore, 2002), aligning syntactic-tree fragments (Ding et al, 2003), and estimatingphrase translation probabilities (Venugopal et al,2003).
Furthermore, at the 2003 Johns Hopkinssummer workshop on statistical machine transla-tion, a large number of features were tested to dis-cover which ones could improve a state-of-the-arttranslation system, and the only feature that pro-duced a ?truly significant improvement?
was theModel 1 score (Och et al, 2004).Despite the fact that IBM Model 1 is so widelyused, essentially no attention seems to have beenpaid to whether it is possible to improve on the stan-dard Expectation-Maximization (EM) procedure forestimating its parameters.
This may be due in partto the fact that Brown et al (1993a) proved that thelog-likelihood objective function for Model 1 is astrictly concave function of the model parameters,so that it has a unique local maximum.
This, in turn,means that EM training will converge to that max-imum from any starting point in which none of theinitial parameter values is zero.
If one equates opti-mum parameter estimation with finding the globalmaximum for the likelihood of the training data,then this result would seem to show no improve-ment is possible.However, in virtually every application of statisti-cal techniques in natural-language processing, max-imizing the likelihood of the training data causesoverfitting, resulting in lower task performance thansome other estimates for the model parameters.
Thisis implicitly recognized in the widespread adoptionof early stopping in estimating the parameters ofModel 1.
Brown et al (1993a) stopped after onlyone iteration of EM in using Model 1 to initializetheir Model 2, and Och and Ney (2003) stop af-ter five iterations in using Model 1 to initialize theHMM word-alignment model.
Both of these are farshort of convergence to the maximum likelihood es-timates for the model parameters.We have identified at least two ways in whichthe standard EM training method for Model 1leads to suboptimal performance in terms of word-alignment accuracy.
In this paper we show that byaddressing these issues, substantial improvementsin word-alignment accuracy can be achieved.2 Definition of Model 1Model 1 is a probabilistic generative model withina framework that assumes a source sentence S oflength l translates as a target sentence T , accordingto the following stochastic process:?
A length m for sentence T is generated.?
For each target sentence position j ?
{1, .
.
.
,m}:?
A generating word siin S (including anull word s0) is selected, and?
The target word tjat position j is gener-ated depending on si.Model 1 is defined as a particularly simple in-stance of this framework, by assuming all possiblelengths for T (less than some arbitrary upper bound)have a uniform probability , all possible choices ofsource sentence generating words are equally likely,and the translation probability tr(tj|si) of the gen-erated target language word depends only on thegenerating source language word?which Brown etal.
(1993a) show yields the following equation:p(T |S) =(l + 1)mm?j=1l?i=0tr(tj|si) (1)Equation 1 gives the Model 1 estimate for theprobability of a target sentence, given a source sen-tence.
We may also be interested in the question ofwhat is the most likely alignment of a source sen-tence and a target sentence, given an instance ofModel 1; where, by an alignment, we mean a speci-fication of which source words generated which tar-get words according to the generative model.
SinceModel 1, like many other word-alignment models,requires each target word to be generated by exactlyone source word (including the null word), an align-ment a can be represented by a vector a1, .
.
.
, am,where each ajis the sentence position of the sourceword generating tjaccording to the alignment.
It iseasy to show that for Model 1, the most likely align-ment a?
of S and T is given by this equation:a?
= argmaxam?j=1tr(tj|saj) (2)Since in applying Model 1, there are no depen-dencies between any of the ajs, we can find themost likely aligment simply by choosing, for eachj, the value for ajthat leads to the highest value fortr(tj|saj).The parameters of Model 1 for a given pair oflanguages are normally estimated using EM, takingas training data a corpus of paired sentences of thetwo languages, such that each pair consists of sen-tence in one language and a possible translation inthe other language.
The training is normally ini-tialized by setting all translation probability distri-butions to the uniform distribution over the targetlanguage vocabulary.3 Problems with Model 1Model 1 clearly has many shortcomings as a modelof translation.
Some of these are structural limita-tions, and cannot be remedied without making themodel significantly more complicated.
Some of themajor structural limitations include:?
(Many-to-one) Each word in the target sen-tence can be generated by at most one wordin the source sentence.
Situations in which aphrase in the source sentence translates as asingle word in the target sentence are not well-modeled.?
(Distortion) The position of any word in thetarget sentence is independent of the positionof the corresponding word in the source sen-tence, or the positions of any other source lan-guage words or their translations.
The ten-dency for a contiguous phrase in one languageto be translated as a contiguous phrase in an-other language is not modeled at all.?
(Fertility) Whether a particular source word isselected to generate the target word for a givenposition is independent of which or how manyother target words the same source word is se-lected to generate.These limitations of Model 1 are all well known,they have been addressed in other word-alignmentmodels, and we will not discuss them further here.Our concern in this paper is with two other problemswith Model 1 that are not deeply structural, and canbe addressed merely by changing how the parame-ters of Model 1 are estimated.The first of these nonstructural problems withModel 1, as standardly trained, is that rare wordsin the source language tend to act as ?garbage col-lectors?
(Brown et al, 1993b; Och and Ney, 2004),aligning to too many words in the target language.This problem is not unique to Model 1, but anec-dotal examination of Model 1 alignments suggeststhat it may be worse for Model 1, perhaps becauseModel 1 lacks the fertility and distortion parametersthat may tend to mitigate the problem in more com-plex models.The cause of the problem can be easily under-stood if we consider a situation in which the sourcesentence contains a rare word that only occurs oncein our training data, plus a frequent word that has aninfrequent translation in the target sentence.
Sup-pose the frequent source word has the translationpresent in the target sentence only 10% of the timein our training data, and thus has an estimated trans-lation probability of around 0.1 for this target word.Since the rare source word has no other occurrencesin the data, EM training is free to assign whateverprobability distribution is required to maximize thejoint probability of this sentence pair.
Even if therare word also needs to be used to generate its ac-tual translation in the sentence pair, a relatively highjoint probability will be obtained by giving the rareword a probability of 0.5 of generating its true trans-lation and 0.5 of spuriously generating the transla-tion of the frequent source word.
The probability ofthis incorrect alignment will be higher than that ob-tained by assigning a probability of 1.0 to the rareword generating its true translation, and generatingthe true translation of the frequent source word witha probability of 0.1.
The usual fix for over-fittingproblems of this type in statistical NLP is to smooththe probability estimates involved in some way.The second nonstructural problem with Model 1is that it seems to align too few target words tothe null source word.
Anecdotal examination ofModel 1 alignments of English source sentenceswith French target sentences reveals that null wordalignments rarely occur in the highest probabilityalignment, despite the fact that French sentencesoften contain function words that do not corre-spond directly to anything in their English trans-lation.
For example, English phrases of the form?noun1??noun2?
are often expressed in French by aphrase of the form ?noun2?
de ?noun1?, which mayalso be expressed in English (but less often) by aphrase of the form ?noun2?
of ?noun1?.The structure of Model 1 again suggests why weshould not be surprised by this problem.
As nor-mally defined, Model 1 hypothesizes only one nullword per sentence.
A target sentence may con-tain many words that ideally should be aligned tonull, plus some other instances of the same wordthat should be aligned to an actual source languageword.
For example, we may have an English/Frenchsentence pair that contains two instances of of inthe English sentence, and five instances of de in theFrench sentence.
Even if the null word and of havethe same initial probabilty of generating de, in iter-ating EM, this sentence is going to push the modeltowards estimating a higher probabilty that of gen-erates de and a lower estimate that the null wordgenerates de.
This happens because there are aretwo instances of of in the source sentence and onlyone hypothetical null word, and Model 1 gives equalweight to each occurrence of each source word.
Ineffect, of gets two votes, but the null word gets onlyone.
We seem to need more instances of the nullword for Model 1 to assign reasonable probabilitiesto target words aligning to the null word.4 Smoothing Translation CountsWe address the nonstructural problems of Model 1discussed above by three methods.
First, to addressthe problem of rare words aligning to too manywords, at each interation of EM we smooth all thetranslation probability estimates by adding virtualcounts according to a uniform probability distribu-tion over all target words.
This prevents the modelfrom becoming too confident about the translationprobabilities for rare source words on the basis ofvery little evidence.
To estimate the smoothed prob-abilties we use the following formula:tr(t|s) =C(t, s) + nC(s) + n ?
|V |(3)where C(t, s) is the expected count of s generatingt, C(s) is the corresponding marginal count for s,|V | is the hypothesized size of the target vocabularyV , and n is the added count for each target word inV .
|V | and n are both free parameters in this equa-tion.
We could take |V | simply to be the total num-ber of distinct words observed in the target languagetraining, but we know that the target language willhave many words that we have never observed.
Wearbitrarily chose |V | to be 100,000, which is some-what more than the total number of distinct wordsin our target language training data.
The value of nis empirically optimized on annotated developmenttest data.This sort of ?add-n?
smoothing has a poor repu-tation in statistical NLP, because it has repeatedlybeen shown to perform badly compared to othermethods of smoothing higher-order n-gram mod-els for statistical language modeling (e.g., Chen andGoodman, 1996).
In those studies, however, add-nsmoothing was used to smooth bigram or trigrammodels.
Add-n smoothing is a way of smooth-ing with a uniform distribution, so it is not surpris-ing that it performs poorly in language modelingwhen it is compared to smoothing with higher or-der models; e.g, smoothing trigrams with bigramsor smoothing bigrams with unigrams.
In situationswhere smoothing with a uniform distribution is ap-propriate, it is not clear that add-n is a bad wayto do it.
Furthermore, we would argue that theword translation probabilities of Model 1 are a casewhere there is no clearly better alternative to a uni-form distribution as the smoothing distribution.
Itshould certainly be better than smoothing with a un-igram distribution, since we especially want to ben-efit from smoothing the translation probabilities forthe rarest words, and smoothing with a unigram dis-tribution would assume that rare words are morelikely to translate to frequent words than to otherrare words, which seems counterintuitive.5 Adding Null Words to the SourceSentenceWe address the lack of sufficient alignments of tar-get words to the null source word by adding extranull words to each source sentence.
Mathematically,there is no reason we have to add an integral numberof null words, so in fact we let the number of nullwords in a sentence be any positive number.
Onecan make arguments in favor of adding the samenumber of null words to every sentence, or in fa-vor of letting the number of null words be propor-tional to the length of the sentence.
We have chosento add a fixed number of null words to each sourcesentence regardless of length, and will leave for an-other time the question of whether this works betteror worse than adding a number of null words pro-portional to the sentence length.Conceptually, adding extra null words to sourcesentences is a slight modification to the structure ofModel 1, but in fact, we can implement it withoutany additional model parameters by the simple ex-pedient of multiplying all the translation probabili-ties for the null word by the number of null wordsper sentence.
This multiplication is performed dur-ing every iteration of EM, as the translation proba-bilities for the null word are re-estimated from thecorresponding expected counts.
This makes theseprobabilities look like they are not normalized, butModel 1 can be applied in such a way that the trans-lation probabilities for the null word are only everused when multiplied by the number of null wordsin the sentence, so we are simply using the null wordtranslation parameters to keep track of this prod-uct pre-computed.
In training a version of Model1 with only one null word per sentence, the param-eters have their normal interpretation, since we aremultiplying the standard probability estimates by 1.6 Initializing Model 1 with HeuristicParameter EstimatesNormally, the translation probabilities of Model 1are initialized to a uniform distribution over the tar-get language vocabulary to start iterating EM.
Theunspoken justification for this is that EM trainingof Model 1 will always converge to the same set ofparameter values from any set of initial values, sothe intial values should not matter.
But this is onlythe case if we want to obtain the parameter values atconvergence, and we have strong reasons to believethat these values do not produce the most accuratesentence alignments.
Even though EM will head to-wards those values from any initial position in theparameter space, there may be some starting pointswe can systematically find that will take us closerto the optimal parameter values for alignment accu-racy along the way.To test whether a better set of initial parame-ter estimates can improve Model 1 alignment ac-curacy, we use a heuristic model based on the log-likelihood-ratio (LLR) statistic recommended byDunning (1993).
We chose this statistic because ithas previously been found to be effective for au-tomatically constructing translation lexicons (e.g.,Melamed, 2000; Moore, 2001).
In our application,the statistic can be defined by the following formula:?t??{t,?t}?s??
{s,?s}C(t?, s?)
log p(t?|s?)p(t?
)(4)In this formula t and s mean that the correspond-ing words occur in the respective target and sourcesentences of an aligned sentence pair, ?t and ?smean that the corresponding words do not occurin the respective sentences, t?
and s?
are variablesranging over these values, and C(t?, s?)
is the ob-served joint count for the values of t?
and s?.
Allthe probabilities in the formula refer to maximumlikelihood estimates.1These LLR scores can range in value from 0 toN ?
log(2), where N is the number of sentence pairsin the training data.
The LLR score for a pair ofwords is high if the words have either a strong pos-itive association or a strong negative association.Since we expect translation pairs to be positively as-sociated, we discard any negatively associated wordpairs by requiring that p(t, s) > p(t) ?
p(s).To use LLR scores to obtain initial estimates forthe translation probabilities of Model 1, we have tosomehow transform them into numbers that rangefrom 0 to 1, and sum to no more than 1 for all thetarget words associated with each source word.
Weknow that words with high LLR scores tend to betranslations, so we want high LLR scores to cor-respond to high probabilities, and low LLR scoresto correspond to low probabilities.
The simplestapproach would be to divide each LLR score bythe sum of the scores for the source word of thepair, which would produce a normalized conditionalprobability distribution for each source word.Doing this, however, would discard one of themajor advantages of using LLR scores as a measureof word association.
All the LLR scores for rarewords tend to be small; thus we do not put too muchconfidence in any of the hypothesized word associ-ations for such words.
This is exactly the propertyneeded to prevent rare source words from becom-ing garbage collectors.
To maintain this property,for each source word we compute the sum of the1This is not the form in which the LLR statistic is usuallypresented, but it can easily be shown by basic algebra to beequivalent to ??
in Dunning?s paper.
See Moore (2004) fordetails.LLR scores over all target words, but we then di-vide every LLR score by the single largest of thesesums.
Thus the source word with the highest LLRscore sum receives a conditional probability distri-bution over target words summing to 1, but the cor-responding distribution for every other source wordsums to less than 1, reserving some probability massfor target words not seen with that word, with moreprobability mass being reserved the rarer the word.There is no guarantee, of course, that this is theoptimal way of discounting the probabilities as-signed to less frequent words.
To allow a widerrange of possibilities, we add one more parameterto the model by raising each LLR score to an empir-ically optimized exponent before summing the re-sulting scores and scaling them from 0 to 1 as de-scribed above.
Choosing an exponent less than 1.0decreases the degree to which low scores are dis-counted, and choosing an exponent greater than 1.0increases degree of discounting.We still have to define an initialization of thetranslation probabilities for the null word.
We can-not make use of LLR scores because the null wordoccurs in every source sentence, and any word oc-curing in every source sentence will have an LLRscore of 0 with every target word, since p(t|s) =p(t) in that case.
We could leave the distributionfor the null word as the uniform distribution, but weknow that a high proportion of the words that shouldalign to the null word are frequently occuring func-tion words.
Hence we initialize the distribution forthe null word to be the unigram distribution of targetwords, so that frequent function words will receivea higher probability of aligning to the null word thanrare words, which tend to be content words that dohave a translation.
Finally, we also effectively addextra null words to every sentence in this heuristicmodel, by multiplying the null word probabilities bya constant, as described in Section 5.7 Training and EvaluationWe trained and evaluated our various modificationsto Model 1 on data from the bilingual word align-ment workshop held at HLT-NAACL 2003 (Mihal-cea and Pedersen, 2003).
We used a subset of theCanadian Hansards bilingual corpus supplied forthe workshop, comprising 500,000 English-Frenchsentences pairs, including 37 sentence pairs desig-nated as ?trial?
data, and 447 sentence pairs desig-nated as test data.
The trial and test data had beenmanually aligned at the word level, noting particularpairs of words either as ?sure?
or ?possible?
align-ments, as described by Och and Ney (2003).To limit the number of translation probabilitiesthat we had to store, we first computed LLR associ-ation scores for all bilingual word pairs with a posi-tive association (p(t, s) > p(t)?p(s)), and discardedfrom further consideration those with an LLR scoreof less that 0.9, which was chosen to be just lowenough to retain all the ?sure?
word alignments inthe trial data.
This resulted in 13,285,942 possibleword-to-word translation pairs (plus 66,406 possi-ble null-word-to-word pairs).For most models, the word translation parame-ters are set automatically by EM.
We trained eachvariation of each model for 20 iterations, which wasenough in almost all cases to discern a clear mini-mum error on the 37 sentence pairs of trial data, andwe chose as the preferred iteration the one with thelowest alignment error rate on the trial data.
Theother parameters of the various versions of Model 1described in Sections 4?6 were optimized with re-spect to alignment error rate on the trial data usingsimple hill climbing.
All the results we report forthe 447 sentence pairs of test data use the parametervalues set to their optimal values for the trial data.We report results for four principal versions ofModel 1, trained using English as the source lan-guage and French as the target language:?
The standard model is initialized usinguniform distributions, and trained withoutsmoothing using EM, for a number of itera-tions optimized on the trial data.?
The smoothed model is like the standardmodel, but with optimized values of the null-word weight and add-n parameter.?
The heuristic model simply uses the initialheuristic estimates of the translation parametervalues, with an optimized LLR exponent andnull-word weight, but no EM re-estimation.?
The combined model initializes the translationparameter values with the heuristic estimates,using the LLR exponent and null-word weightfrom the optimal heuristic model, and appliesEM using optimized values of the null-wordweight and add-n parameters.
The null-wordweight used during EM is optimized separatelyfrom the null-word weight used in the initialheuristic parameter estimates.We also performed ablation experiments in whichwe ommitted each applicable modification in turnfrom each principal version of Model 1, to observethe effect on alignment error.
All non-EM-trainedparameters were re-optimized on the trial data foreach version of Model 1 tested, with the exceptionModel Trial Test Test Test LLR Init EM Add EM(Ablation) AER AER Recall Precision Exp NW NW n IterStandard 0.311 0.298 0.810 0.646 NA NA 1.0 0.0000 17Smoothed 0.261 0.271 0.646 0.798 NA NA 10.0 0.0100 15(EM NW) 0.285 0.273 0.833 0.671 NA NA 1.0 0.0100 20(Add n) 0.302 0.300 0.638 0.751 NA NA 13.0 0.0000 14Heuristic 0.234 0.255 0.655 0.844 1.3 2.4 NA NA NA(LLR Exp) 0.257 0.259 0.655 0.844 1.0 2.4 NA NA NA(Init NW) 0.300 0.308 0.740 0.657 1.5 1.0 NA NA NACombined 0.203 0.215 0.724 0.839 1.3 2.4 7.0 0.005 1(LLR Exp) 0.258 0.272 0.636 0.809 1.0 2.4 10.0 0.0035 3(Init NW) 0.197 0.209 0.722 0.854 1.5 1.0 10.0 0.0005 1(EM NW) 0.281 0.267 0.833 0.680 1.3 2.4 1.0 0.0080 8(Add n) 0.208 0.221 0.724 0.826 1.3 2.4 8.0 0.0000 1Table 1: Evaluation Results.that the value of the LLR exponent and initial null-word weight in the combined model were carriedover from the heuristic model.8 ResultsWe report the performance of our different versionsof Model 1 in terms of precision, recall, and align-ment error rate (AER) as defined by Och and Ney(2003).
These three performance statistics are de-fined asrecall =|A ?
S||S|(5)precision =|A ?
P ||A|(6)AER = 1 ?|A ?
S| + |A ?
P ||A| + |S|(7)where S denotes the annotated set of sure align-ments, P denotes the annotated set of possiblealignments, and A denotes the set of alignmentsproduced by the model under test.2 We take AER,which is derived from F-measure, as our primaryevaluation metric.The results of our evaluation are presented in Ta-ble 1.
The columns of the table present (in order) adescription of the model being tested, the AER onthe trial data, the AER on the test data, test data re-call, and test data precision, followed by the optimalvalues on the trial data for the LLR exponent, theinitial (heuristic model) null-word weight, the null-word weight used in EM re-estimation, the add-nparameter value used in EM re-estimation, and thenumber of iterations of EM.
?NA?
means a parame-ter is not applicable in a particular model.2As is customary, alignments to the null word are not ex-plicitly counted.Results for the four principal versions of Model 1are presented in bold.
For each principal version, re-sults of the corresponding ablation experiments arepresented in standard type, giving the name of eachomitted modification in parentheses.3 Probably themost striking result is that the heuristic model sub-stantially reduces the AER compared to the standardor smoothed model, even without EM re-estimation.The combined model produces an additional sub-stantial reduction in alignment error, using a singleiteration of EM.The ablation experiments show how importantthe different modifications are to the various mod-els.
It is interesting to note that the importance ofa given modification varies from model to model.For example, the re-estimation null-word weightmakes essentially no contribution to the smoothedmodel.
It can be tuned to reduce the error on the trialdata, but the improvement does not carry over to thetest data.
The smoothed model with only the null-word weight and no add-n smoothing has essen-tially the same error as the standard model; and thesmoothed model with add-n smoothing alone hasessentially the same error as the smoothed modelwith both the null-word weight and add-n smooth-ing.
On the other hand, the re-estimation null-wordweight is crucial to the combined model.
With it, thecombined model has substantially lower error thanthe heuristic model without re-estimation; withoutit, for any number of EM iterations, the combinedmodel has higher error than the heuristic model.A similar analysis shows that add-n smoothingis much less important in the combined model than3Modificiations are ?omitted?
by setting the correspondingparameter to a value that is equivalent to removing the modifi-cation from the model.the smoothed model.
The probable explanation forthis is that add-n smoothing is designed to addressover-fitting from many iterations of EM.
While thesmoothed model does require many EM iterationsto reach its minimum AER, the combined model,with or without add-n smoothing, is at its minimumAER with only one EM iteration.Finally, we note that, while the initial null-wordweight is crucial to the heuristic model without re-estimation, the combined model actually performsbetter without it.
Presumably, the re-estimationnull-word weight makes the inital null-word weightredundant.
In fact, the combined model without theinitial null word-weight has the lowest AER on boththe trial and test data of any variation tested (noteAERs in italics in Figure 1).
The relative reductionin AER for this model is 29.9% compared to thestandard model.We tested the significance of the differences inalignment error between each pair of our principalversions of Model 1 by looking at the AER for eachsentence pair in the test set using a 2-tailed pairedt test.
The differences between all these modelswere significant at a level of 10?7 or better, exceptfor the difference between the standard model andthe smoothed model, which was ?significant?
at the0.61 level?that is, not at all significant.
The rea-son for this is probably the very different balancebetween precision and recall with the standard andsmoothed models, which indicates that the modelsmake quite different sorts of errors, making statisti-cal significance hard to establish.
This conjecture issupported by considering the smoothed model omit-ting the re-estimation null-word weight, which hassubstantially the same AER as the full smoothedmodel, but with a precision/recall balance muchcloser to the standard model.
The 2-tailed pairedt test comparing this model to the standard modelshowed significance at a level of better than 10?10.We also compared the combined model with andwithout the initial null-word weight, and found thatthe improvement without the weight was significantat the 0.008 level.9 ConclusionsWe have demonstrated that it is possible to improvethe performance of Model 1 in terms of alignmenterror by about 30%, simply by changing the way itsparameters are estimated.
Almost half this improve-ment is obtained with a simple heuristic model thatdoes not require EM re-estimation.It is interesting to contrast our heuristic modelwith the heuristic models used by Och and Ney(2003) as baselines in their comparative study ofalignment models.
The major difference betweenour model and theirs is that they base theirs on theDice coefficient, which is computed by the formula42 ?
C(t, s)C(t) + C(s)(8)while we use the log-likelihood-ratio statistic de-fined in Section 6.
Och and Ney find that the stan-dard version of Model 1 produces more accuratealignments after only one iteration of EM than ei-ther of the heuristic models they consider, while wefind that our heuristic model outperforms the stan-dard version of Model 1, even with an optimal num-ber of iterations of EM.While the Dice coefficient is simple andintuitive?the value is 0 for words never found to-gether, and 1 for words always found together?itlacks the important property of the LLR statistic thatscores for rare words are discounted; thus it does notaddress the over-fitting problem for rare words.The list of applications of IBM word-alignmentModel 1 given in Section 1 should be sufficient toconvince anyone of the relevance of improving themodel.
However, it is not clear that AER as definedby Och and Ney (2003) is always the appropriateway to evaluate the quality of the model, since theViterbi word alignment that AER is based on is sel-dom used in applications of Model 1.5 Moreover, itis notable that while the versions of Model 1 havingthe lowest AER have dramatically higher precisionthan the standard version, they also have quite a bitlower recall.
If AER does not reflect the optimalbalance between precision and recall for a particu-lar application, then optimizing AER may not pro-duce the best task-based performance for that appli-cation.
Thus the next step in this research must beto test whether the improvements in AER we havedemonstrated for Model 1 lead to improvements ontask-based performance measures.ReferencesPeter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993a.4Och and Ney give a different formula in their paper, inwhich the addition in the denominator is replaced by a multi-plication.
According to Och (personal communication), how-ever, this is merely a typographical error in the publication, andthe results reported are for the standard definition of the Dicecoefficient.5A possible exception is suggested by the results of Koehnet al (2003), which show that phrase translations extractedfrom Model 1 alignments can perform almost as well in aphrase-based statistical translation system as those extractedfrom more sophisticated alignment models, provided enoughtraining data is used.The mathematics of statistical machine transla-tion: parameter estimation.
Computational Lin-guistics, 19(2):263?311.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, Meredith J. Goldsmith, Jan Hajic,Robert L. Mercer, and Surya Mohanty.
1993b.But dictionaries are data too.
In Proceedings ofthe ARPA Workshop on Human Language Tech-nology, pp.
202?205, Plainsboro, New Jersey,USA.Stanley F. Chen and Joshua Goodman.
1996.
Anempirical study of smoothing techniques for lan-guage modeling.
In Proceedings of the 34th An-nual Meeting of the Association for Computa-tional Linguistics, pp.
310?318, Santa Cruz, Cal-ifornia, USA.Yuan Ding, Daniel Gildea, and Martha Palmer.2003.
An algorithm for word-level alignment ofparallel dependency trees.
In Proceedings of theNinth Machine Translation Summit, pp.
95?101,New Orleans, Louisiana, USA.Ted Dunning.
1993.
Accurate methods for thestatistics of surprise and coincidence.
Computa-tional Linguistics, 19(1):61?74.Philipp Koehn, Franz Joseph Och, and DanielMarcu.
2003.
Statistical phrase-based transla-tion.
In Proceedings of the Human LanguageTechnology Conference of the North AmericanChapter of the Association for ComputationalLinguistics (HLT-NAACL 2003), pp.
127?133,Edmonton, Alberta, Canada.I.
Dan Melamed.
2000.
Models of Transla-tional Equivalence.
Computational Linguistics,26(2):221?249.Rada Mihalcea and Ted Pedersen.
2003.
An eval-uation exercise for word alignment.
In Proceed-ings of the HLT-NAACL 2003 Workshop, Buildingand Using Parallel Texts: Data Driven MachineTranslation and Beyond, pp.
1?6, Edmonton, Al-berta, Canada.Robert C. Moore.
2001.
Towards a simple and ac-curate statistical approach to learning translationrelationships among words.
In Proceedings ofthe Workshop Data-driven Machine Translationat the 39th Annual Meeting of the Association forComputational Linguistics, pp.
79?86, Toulouse,France.Robert C. Moore.
2002.
Fast and accurate sentencealignment of bilingual corpora.
In S. Richard-son (ed.
), Machine Translation: From Researchto Real Users (Proceedings, 5th Conference ofthe Association for Machine Translation in theAmericas, Tiburon, California), pp.
135?244,Springer-Verlag, Heidelberg, Germany.Robert C. Moore.
2004.
On log-likelihood-ratiosand the significance of rare events.
In Proceed-ings of the 2004 Conference on Empirical Meth-ods in Natural Language Processing, Barcelona,Spain.Dragos S. Munteanu, Alexander Fraser, and DanielMarcu.
2004.
Improved machine translation per-formance via parallel sentence extraction fromcomparable corpora.
In Proceedings of the Hu-man Language Technology Conference of theNorth American Chapter of the Association forComputational Linguistics (HLT-NAACL 2004),pp.
265?272, Boston, Massachusetts, USA.Francisco Nevado, Francisco Casacuberta, and En-rique Vidal.
2003.
Parallel corpora segmen-tation using anchor words.
In Proceedings ofthe 7th International EAMT workshop on MTand other language technology tools, ImprovingMT through other language technology tools, Re-sources and tools for building MT, pp.
33?40, Bu-dapest, Hungary.Franz Joseph Och and Hermann Ney.
2003.A systematic comparison of various statisticalalignment models.
Computational Linguistics,29(1):19?51.Franz Josef Och et al 2004.
A smorgasbord offeatures for statistical machine translation.
InProceedings of the Human Language Technol-ogy Conference of the North American Chapterof the Association for Computational Linguistics(HLT-NAACL 2004), pp.
161?168, Boston, Mas-sachusetts, USA.Ashish Venugopal, Stephan Vogel, and AlexWaibel.
2003.
Effective phrase translation ex-traction from alignment models.
In Proceedingsof the 41st Annual Meeting of the Associationfor Computational Linguistics, pp.
319?326, Sap-poro, Japan.
