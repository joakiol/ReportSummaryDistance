Proceedings of the ACL 2010 Conference Short Papers, pages 43?48,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsThe impact of interpretation problems on tutorial dialogueMyroslava O. Dzikovska and Johanna D. MooreSchool of Informatics, University of Edinburgh, Edinburgh, United Kingdom{m.dzikovska,j.moore}@ed.ac.ukNatalie Steinhauser and Gwendolyn CampbellNaval Air Warfare Center Training Systems Division, Orlando, FL, USA{natalie.steihauser,gwendolyn.campbell}@navy.milAbstractSupporting natural language input mayimprove learning in intelligent tutoringsystems.
However, interpretation errorsare unavoidable and require an effectiverecovery policy.
We describe an evaluationof an error recovery policy in the BEE-TLE II tutorial dialogue system and dis-cuss how different types of interpretationproblems affect learning gain and user sat-isfaction.
In particular, the problems aris-ing from student use of non-standard ter-minology appear to have negative conse-quences.
We argue that existing strategiesfor dealing with terminology problems areinsufficient and that improving such strate-gies is important in future ITS research.1 IntroductionThere is a mounting body of evidence that studentself-explanation and contentful talk in human-human tutorial dialogue are correlated with in-creased learning gain (Chi et al, 1994; Purandareand Litman, 2008; Litman et al, 2009).
Thus,computer tutors that understand student explana-tions have the potential to improve student learn-ing (Graesser et al, 1999; Jordan et al, 2006;Aleven et al, 2001; Dzikovska et al, 2008).
How-ever, understanding and correctly assessing thestudent?s contributions is a difficult problem dueto the wide range of variation observed in studentinput, and especially due to students?
sometimesvague and incorrect use of domain terminology.Many tutorial dialogue systems limit the rangeof student input by asking short-answer questions.This provides a measure of robustness, and previ-ous evaluations of ASR in spoken tutorial dialoguesystems indicate that neither word error rate norconcept error rate in such systems affect learninggain (Litman and Forbes-Riley, 2005; Pon-Barryet al, 2004).
However, limiting the range of pos-sible input limits the contentful talk that the stu-dents are expected to produce, and therefore maylimit the overall effectiveness of the system.Most of the existing tutoring systems that acceptunrestricted language input use classifiers basedon statistical text similarity measures to matchstudent answers to open-ended questions withpre-authored anticipated answers (Graesser et al,1999; Jordan et al, 2004; McCarthy et al, 2008).While such systems are robust to unexpected ter-minology, they provide only a very coarse-grainedassessment of student answers.
Recent researchaims to develop methods that produce detailedanalyses of student input, including correct, in-correct and missing parts (Nielsen et al, 2008;Dzikovska et al, 2008), because the more detailedassessments can help tailor tutoring to the needs ofindividual students.While the detailed assessments of answers toopen-ended questions are intended to improve po-tential learning, they also increase the probabil-ity of misunderstandings, which negatively impacttutoring and therefore negatively impact studentlearning (Jordan et al, 2009).
Thus, appropri-ate error recovery strategies are crucially impor-tant for tutorial dialogue applications.
We describean evaluation of an implemented tutorial dialoguesystem which aims to accept unrestricted studentinput and limit misunderstandings by rejecting lowconfidence interpretations and employing a rangeof error recovery strategies depending on the causeof interpretation failure.By comparing two different system policies, wedemonstrate that with less restricted language in-put the rate of non-understanding errors impactsboth learning gain and user satisfaction, and thatproblems arising from incorrect use of terminol-ogy have a particularly negative impact.
A moredetailed analysis of the results indicates that, eventhough we based our policy on an approach ef-43fective in task-oriented dialogue (Hockey et al,2003), many of our strategies were not success-ful in improving learning gain.
At the same time,students appear to be aware that the system doesnot fully understand them even if it accepts theirinput without indicating that it is having interpre-tation problems, and this is reflected in decreaseduser satisfaction.
We argue that this indicates thatwe need better strategies for dealing with termi-nology problems, and that accepting non-standardterminology without explicitly addressing the dif-ference in acceptable phrasing may not be suffi-cient for effective tutoring.In Section 2 we describe our tutoring system,and the two tutoring policies implemented for theexperiment.
In Section 3 we present experimen-tal results and an analysis of correlations betweendifferent types of interpretation problems, learninggain and user satisfaction.
Finally, in Section 4 wediscuss the implications of our results for error re-covery policies in tutorial dialogue systems.2 Tutorial Dialogue System and ErrorRecovery PoliciesThis work is based on evaluation of BEETLE II(Dzikovska et al, 2010), a tutorial dialogue sys-tem which provides tutoring in basic electricityand electronics.
Students read pre-authored mate-rials, experiment with a circuit simulator, and thenare asked to explain their observations.
BEETLE IIuses a deep parser together with a domain-specificdiagnoser to process student input, and a deep gen-erator to produce tutorial feedback automaticallydepending on the current tutorial policy.
It alsoimplements an error recovery policy to deal withinterpretation problems.Students currently communicate with the sys-tem via a typed chat interface.
While typingremoves the uncertainty and errors involved inspeech recognition, expected student answers areconsiderably more complex and varied than ina typical spoken dialogue system.
Therefore, asignificant number of interpretation errors arise,primarily during the semantic interpretation pro-cess.
These errors can lead to non-understandings,when the system cannot produce a syntactic parse(or a reasonable fragmentary parse), or when itdoes not know how to interpret an out-of-domainword; and misunderstandings, where a system ar-rives at an incorrect interpretation, due to eitheran incorrect attachment in the parse, an incorrectword sense assigned to an ambiguous word, or anincorrectly resolved referential expression.Our approach to selecting an error recovery pol-icy is to prefer non-understandings to misunder-standings.
There is a known trade-off in spoken di-alogue systems between allowing misunderstand-ings, i.e., cases in which a system accepts andacts on an incorrect interpretation of an utterance,and non-understandings, i.e., cases in which a sys-tem rejects an utterance as uninterpretable (Bo-hus and Rudnicky, 2005).
Since misunderstand-ings on the part of a computer tutor are knownto negatively impact student learning, and sincein human-human tutorial dialogue the majority ofstudent responses using unexpected terminologyare classified as incorrect (Jordan et al, 2009),it would be a reasonable approach for a tutorialdialogue system to deal with potential interpreta-tion problems by treating low-confidence interpre-tations as non-understandings and focusing on aneffective non-understanding recovery policy.1We implemented two different policies for com-parison.
Our baseline policy does not attempt anyremediation or error recovery.
All student utter-ances are passed through the standard interpreta-tion pipeline, so that the results can be analyzedlater.
However, the system does not attempt to ad-dress the student content.
Instead, regardless ofthe answer analysis, the system always uses a neu-tral acceptance and bottom out strategy, giving thestudent the correct answer every time, e.g., ?OK.One way to phrase the correct answer is: the openswitch creates a gap in the circuit?.
Thus, the stu-dents are never given any indication of whetherthey have been understood or not.The full policy acts differently depending on theanalysis of the student answer.
For correct an-swers, it acknowledges the answer as correct andoptionally restates it (see (Dzikovska et al, 2008)for details).
For incorrect answers, it restates thecorrect portion of the answer (if any) and providesa hint to guide the student towards the completelycorrect answer.
If the student?s utterance cannot beinterpreted, the system responds with a help mes-sage indicating the cause of the problem togetherwith a hint.
In both cases, after 3 unsuccessful at-tempts to address the problem the system uses thebottom out strategy and gives away the answer.1While there is no confidence score from a speech recog-nizer, our system uses a combination of a parse quality scoreassigned by the parser and a set of consistency checks to de-termine whether an interpretation is sufficiently reliable.44The content of the bottom out is the same as inthe baseline, except that the full system indicatesclearly that the answer was incorrect or was notunderstood, e.g., ?Not quite.
Here is the answer:the open switch creates a gap in the circuit?.The help messages are based on the Targeted-Help approach successfully used in spoken dia-logue (Hockey et al, 2003), together with the errorclassification we developed for tutorial dialogue(Dzikovska et al, 2009).
There are 9 different er-ror types, each associated with a different targetedhelp message.
The goal of the help messages is togive the student as much information as possibleas to why the system failed to understand them butwithout giving away the answer.In comparing the two policies, we would expectthat the students in both conditions would learnsomething, but that the learning gain and user sat-isfaction would be affected by the difference inpolicies.
We hypothesized that students who re-ceive feedback on their errors in the full conditionwould learn more compared to those in the base-line condition.3 EvaluationWe collected data from 76 subjects interactingwith the system.
The subjects were randomly as-signed to either the baseline (BASE) or the full(FULL) policy condition.
Each subject took a pre-test, then worked through a lesson with the system,and then took a post-test and filled in a user satis-faction survey.
Each session lasted approximately4 hours, with 232 student language turns in FULL(SD = 25.6) and 156 in BASE (SD = 2.02).
Ad-ditional time was taken by reading and interact-ing with the simulation environment.
The studentshad little prior knowledge of the domain.
The sur-vey consisted of 63 questions on the 5-point Lik-ert scale covering the lesson content, the graphicaluser interface, and tutor?s understanding and feed-back.
For purposes of this study, we are using anaveraged tutor score.The average learning gain was 0.57 (SD =0.23) in FULL, and 0.63 (SD = 0.26) in BASE.There was no significant difference in learninggain between conditions.
Students liked BASE bet-ter: the average tutor evaluation score for FULLwas 2.56 out of 5 (SD = 0.65), compared to 3.32(SD = 0.65) in BASE.
These results are signif-icantly different (t-test, p < 0.05).
In informalcomments after the session many students said thatthey were frustrated when the system said that itdid not understand them.
However, some studentsin BASE also mentioned that they sometimes werenot sure if the system?s answer was correcting aproblem with their answer, or simply phrasing itin a different way.We used mean frequency of non-interpretableutterances (out of all student utterances ineach session) to evaluate the effectiveness ofthe two different policies.
On average, 14%of utterances in both conditions resulted innon-understandings.2 The frequency of non-understandings was negatively correlated withlearning gain in FULL: r = ?0.47, p < 0.005,but not significantly correlated with learning gainin BASE: r = ?0.09, p = 0.59.
However, in bothconditions the frequency of non-understandingswas negatively correlated with user satisfaction:FULL r = ?0.36, p = 0.03, BASE r = ?0.4, p =0.01.
Thus, even though in BASE the systemdid not indicate non-understanding, students werenegatively affected.
That is, they were not satis-fied with the policy that did not directly addressthe interpretation problems.
We discuss possiblereasons for this below.We investigated the effect of different types ofinterpretation errors using two criteria.
First, wechecked whether the mean frequency of errors wasreduced between BASE and FULL for each individ-ual strategy.
The reduced frequency means thatthe recovery strategy for this particular error typeis effective in reducing the error frequency.
Sec-ond, we looked for the cases where the frequencyof a given error type is negatively correlated witheither learning gain or user satisfaction.
This isprovides evidence that such errors are negativelyimpacting the learning process, and therefore im-proving recovery strategies for those error types islikely to improve overall system effectiveness,The results, shown in Table 1, indicate that themajority of interpretation problems are not sig-nificantly correlated with learning gain.
How-ever, several types of problems appear to beparticularly significant, and are all related toimproper use of domain terminology.
Thesewere irrelevant answer, no appr terms, selec-tional restriction failure and program error.An irrelevant answer error occurs when the stu-dent makes a statement that uses domain termi-2We do not know the percentage of misunderstandings orconcept error rate as yet.
We are currently annotating the datawith the goal to evaluate interpretation correctness.45full baselineerror typemean freq.(std.
dev)satisfac-tion rgainrmean freq(std.
dev)satisfac-tion rgainrirrelevant answer 0.008 (0.01) -0.08 -0.19 0.012 (0.01) -0.07 -0.47**no appr terms 0.005 (0.01) -0.57** -0.42** 0.003 (0.01) -0.38** -0.01selectional restr failure 0.032 (0.02) -0.12 -0.55** 0.040 (0.03) 0.13 0.26*program error 0.002 (0.003) 0.02 0.26 0.003 (0.003) 0 -0.35**unknown word 0.023 (0.01) 0.05 -0.21 0.024 (0.02) -0.15 -0.09disambiguation failure 0.013 (0.01) -0.04 0.02 0.007 (0.01) -0.18 0.19no parse 0.019 (0.01) -0.14 -0.08 0.022(0.02) -0.3* 0.01partial interpretation 0.004 (0.004) -0.11 -0.01 0.004 (0.005) -0.19 0.22reference failure 0.012 (0.02) -0.31* -0.09 0.017 (0.01) -0.15 -0.23Overall 0.134 (0.05) -0.36** -0.47** 0.139 (0.04) -0.4** -0.09Table 1: Correlations between frequency of different error types and student learning gain and satisfac-tion.
** - correlation is significant with p < 0.05, * - with p <= 0.1.nology but does not appear to answer the system?squestion directly.
For example, the expected an-swer to ?In circuit 1, which components are in aclosed path??
is ?the bulb?.
Some students mis-read the question and say ?Circuit 1 is closed.?
Ifthat happens, in FULL the system says ?Sorry, thisisn?t the form of answer that I expected.
I am look-ing for a component?, pointing out to the studentthe kind of information it is looking for.
The BASEsystem for this error, and for all other errors dis-cussed below, gives away the correct answer with-out indicating that there was a problem with in-terpreting the student?s utterance, e.g., ?OK, thecorrect answer is the bulb.
?The no appr terms error happens when the stu-dent is using terminology inappropriate for the les-son in general.
Students are expected to learn toexplain everything in terms of connections and ter-minal states.
For example, the expected answer to?What is voltage??
is ?the difference in states be-tween two terminals?.
If instead the student says?Voltage is electricity?, FULL responds with ?I amsorry, I am having trouble understanding.
I see nodomain concepts in your answer.
Here?s a hint:your answer should mention a terminal.?
The mo-tivation behind this strategy is that in general, it isvery difficult to reason about vaguely used domainterminology.
We had hoped that by telling the stu-dent that the content of their utterance is outsidethe domain as understood by the system, and hint-ing at the correct terms to use, the system wouldguide students towards a better answer.Selectional restr failure errors are typically dueto incorrect terminology, when the studentsphrased answers in a way that contradicted the sys-tem?s domain knowledge.
For example, the sys-tem can reason about damaged bulbs and batter-ies, and open and closed paths.
So if the stu-dent says ?The path is damaged?, the FULL sys-tem would respond with ?I am sorry, I am havingtrouble understanding.
Paths cannot be damaged.Only bulbs and batteries can be damaged.
?Program error were caused by faults in the un-derlying network software, but usually occurredwhen the student was using extremely long andcomplicated utterances.Out of the four important error types describedabove, only the strategy for irrelevant answer waseffective: the frequency of irrelevant answer er-rors is significantly higher in BASE (t-test, p <0.05), and it is negatively correlated with learninggain in BASE.
The frequencies of other error typesdid not significantly differ between conditions.However, one other finding is particularly in-teresting: the frequency of no appr terms errorsis negatively correlated with user satisfaction inBASE.
This indicates that simply accepting the stu-dent?s answer when they are using incorrect termi-nology and exposing them to the correct answer isnot the best strategy, possibly because the studentsare noticing the unexplained lack of alignment be-tween their utterance and the system?s answer.4 Discussion and Future WorkAs discussed in Section 1, previous studies ofshort-answer tutorial dialogue systems produced acounter-intuitive result: measures of interpretationaccuracy were not correlated with learning gain.With less restricted language, misunderstandings46negatively affected learning.
Our study providesfurther evidence that interpretation quality signif-icantly affects learning gain in tutorial dialogue.Moreover, while it has long been known that usersatisfaction is negatively correlated with interpre-tation error rates in spoken dialogue, this is thefirst attempt to evaluate the impact of differenttypes of interpretation errors on task success andusability of a tutoring system.Our results demonstrate that different types oferrors may matter to a different degree.
In oursystem, all of the error types negatively correlatedwith learning gain stem from the same underlyingproblem: the use of incorrect or vague terminol-ogy by the student.
With the exception of the ir-relevant answer strategy, the targeted help strate-gies we implemented were not effective in reduc-ing error frequency or improving learning gain.Additional research is needed to understand why.One possibility is that irrelevant answer was eas-ier to remediate compared to other error types.
Itusually happened in situations where there was aclear expectation of the answer type (e.g., a list ofcomponent names, a yes/no answer).
Therefore,it was easier to design an effective prompt.
Helpmessages for other error types were more frequentwhen the expected answer was a complex sen-tence, and multiple possible ways of phrasing thecorrect answer were acceptable.
Therefore, it wasmore difficult to formulate a prompt that wouldclearly describe the problem in all contexts.One way to improve the help messages may beto have the system indicate more clearly when userterminology is a problem.
Our system apologizedeach time there was a non-understanding, leadingstudents to believe that they may be answering cor-rectly but the answer is not being understood.
Adifferent approach would be to say something like?I am sorry, you are not using the correct termi-nology in your answer.
Here?s a hint: your answershould mention a terminal?.
Together with an ap-propriate mechanism to detect paraphrases of cor-rect answers (as opposed to vague answers whosecorrectness is difficult to determine), this approachcould be more beneficial in helping students learn.We are considering implementing and evaluatingthis as part of our future work.Some of the errors, in particular instances ofno appr terms and selectional restr failure, alsostemmed from unrecognized paraphrases withnon-standard terminology.
Those answers couldconceivably be accepted by a system using seman-tic similarity as a metric (e.g., using LSA with pre-authored answers).
However, our results also indi-cate that simply accepting the incorrect terminol-ogy may not be the best strategy.
Users appear tobe sensitive when the system?s language does notalign with their terminology, as reflected in the de-creased satisfaction ratings associated with higherrates of incorrect terminology problems in BASE.Moreover, prior analysis of human-human dataindicates that tutors use different restate strate-gies depending on the ?quality?
of the student an-swers, even if they are accepting them as correct(Dzikovska et al, 2008).
Together, these point atan important unaddressed issue: existing systemsare often built on the assumption that only incor-rect and missing parts of the student answer shouldbe remediated, and a wide range of terminologyshould be accepted (Graesser et al, 1999; Jordanet al, 2006).
While it is obviously important forthe system to accept a range of different phrasings,our analysis indicates that this may not be suffi-cient by itself, and students could potentially ben-efit from addressing the terminology issues with aspecifically devised strategy.Finally, it could also be possible that somedifferences between strategy effectiveness werecaused by incorrect error type classification.
Man-ual examination of several dialogues suggests thatmost of the errors are assigned to the appropri-ate type, though in some cases incorrect syntac-tic parses resulted in unexpected interpretation er-rors, causing the system to give a confusing helpmessage.
These misclassifications appear to beevenly split between different error types, thougha more formal evaluation is planned in the fu-ture.
However from our initial examination, webelieve that the differences in strategy effective-ness that we observed are due to the actual differ-ences in the help messages.
Therefore, designingbetter prompts would be the key factor in improv-ing learning and user satisfaction.AcknowledgmentsThis work has been supported in part by US Officeof Naval Research grants N000140810043 andN0001410WX20278.
We thank Katherine Harri-son, Leanne Taylor, Charles Callaway, and ElaineFarrow for help with setting up the system andrunning the evaluation.
We would like to thankanonymous reviewers for their detailed feedback.47ReferencesV.
Aleven, O. Popescu, and K. R. Koedinger.
2001.Towards tutorial dialog to support self-explanation:Adding natural language understanding to a cogni-tive tutor.
In Proceedings of the 10th InternationalConference on Artificial Intelligence in Education(AIED ?01)?.Dan Bohus and Alexander Rudnicky.
2005.
Sorry,I didn?t catch that!
- An investigation of non-understanding errors and recovery strategies.
InProceedings of SIGdial-2005, Lisbon, Portugal.Michelene T. H. Chi, Nicholas de Leeuw, Mei-HungChiu, and Christian LaVancher.
1994.
Elicitingself-explanations improves understanding.
Cogni-tive Science, 18(3):439?477.Myroslava O. Dzikovska, Gwendolyn E. Campbell,Charles B. Callaway, Natalie B. Steinhauser, ElaineFarrow, Johanna D. Moore, Leslie A. Butler, andColin Matheson.
2008.
Diagnosing natural lan-guage answers to support adaptive tutoring.
InProceedings 21st International FLAIRS Conference,Coconut Grove, Florida, May.Myroslava O. Dzikovska, Charles B. Callaway, ElaineFarrow, Johanna D. Moore, Natalie B. Steinhauser,and Gwendolyn C. Campbell.
2009.
Dealing withinterpretation errors in tutorial dialogue.
In Pro-ceedings of SIGDIAL-09, London, UK, Sep.Myroslava O. Dzikovska, Johanna D. Moore, NatalieSteinhauser, Gwendolyn Campbell, Elaine Farrow,and Charles B. Callaway.
2010.
Beetle II: a sys-tem for tutoring and computational linguistics ex-perimentation.
In Proceedings of ACL-2010 demosession.A.
C. Graesser, P. Wiemer-Hastings, P. Wiemer-Hastings, and R. Kreuz.
1999.
Autotutor: A simula-tion of a human tutor.
Cognitive Systems Research,1:35?51.Beth Ann Hockey, Oliver Lemon, Ellen Campana,Laura Hiatt, Gregory Aist, James Hieronymus,Alexander Gruenstein, and John Dowding.
2003.Targeted help for spoken dialogue systems: intelli-gent feedback improves naive users?
performance.In Proceedings of the tenth conference on Europeanchapter of the Association for Computational Lin-guistics, pages 147?154, Morristown, NJ, USA.Pamela W. Jordan, Maxim Makatchev, and Kurt Van-Lehn.
2004.
Combining competing language under-standing approaches in an intelligent tutoring sys-tem.
In James C. Lester, Rosa Maria Vicari, andFa?bio Paraguac?u, editors, Intelligent Tutoring Sys-tems, volume 3220 of Lecture Notes in ComputerScience, pages 346?357.
Springer.Pamela Jordan, Maxim Makatchev, Umarani Pap-puswamy, Kurt VanLehn, and Patricia Albacete.2006.
A natural language tutorial dialogue systemfor physics.
In Proceedings of the 19th InternationalFLAIRS conference.Pamela Jordan, Diane Litman, Michael Lipschultz, andJoanna Drummond.
2009.
Evidence of misunder-standings in tutorial dialogue and their impact onlearning.
In Proceedings of the 14th InternationalConference on Artificial Intelligence in Education(AIED), Brighton, UK, July.Diane Litman and Kate Forbes-Riley.
2005.
Speechrecognition performance and learning in spoken di-alogue tutoring.
In Proceedings of EUROSPEECH-2005, page 1427.Diane Litman, Johanna Moore, Myroslava Dzikovska,and Elaine Farrow.
2009.
Generalizing tutorial dia-logue results.
In Proceedings of 14th InternationalConference on Artificial Intelligence in Education(AIED), Brighton, UK, July.Philip M. McCarthy, Vasile Rus, Scott Crossley,Arthur C. Graesser, and Danielle S. McNamara.2008.
Assessing forward-, reverse-, and average-entailment indeces on natural language input fromthe intelligent tutoring system, iSTART.
In Proceed-ings of the 21st International FLAIRS conference,pages 165?170.Rodney D. Nielsen, Wayne Ward, and James H. Mar-tin.
2008.
Learning to assess low-level conceptualunderstanding.
In Proceedings 21st InternationalFLAIRS Conference, Coconut Grove, Florida, May.Heather Pon-Barry, Brady Clark, Elizabeth OwenBratt, Karl Schultz, and Stanley Peters.
2004.
Eval-uating the effectiveness of SCoT: A spoken conver-sational tutor.
In J. Mostow and P. Tedesco, editors,Proceedings of the ITS 2004 Workshop on Dialog-based Intelligent Tutoring Systems, pages 23?32.Amruta Purandare and Diane Litman.
2008.
Content-learning correlations in spoken tutoring dialogs atword, turn and discourse levels.
In Proceedings 21stInternational FLAIRS Conference, Coconut Grove,Florida, May.48
