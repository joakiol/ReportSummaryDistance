Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 539?549,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsPerplexity Minimization for Translation Model Domain Adaptation inStatistical Machine TranslationRico SennrichInstitute of Computational LinguisticsUniversity of ZurichBinzm?hlestr.
14CH-8050 Z?richsennrich@cl.uzh.chAbstractWe investigate the problem of domainadaptation for parallel data in StatisticalMachine Translation (SMT).
While tech-niques for domain adaptation of monolin-gual data can be borrowed for parallel data,we explore conceptual differences betweentranslation model and language model do-main adaptation and their effect on per-formance, such as the fact that translationmodels typically consist of several featuresthat have different characteristics and canbe optimized separately.
We also exploreadapting multiple (4?10) data sets with noa priori distinction between in-domain andout-of-domain data except for an in-domaindevelopment set.1 IntroductionThe increasing availability of parallel corporafrom various sources, welcome as it may be,leads to new challenges when building a statis-tical machine translation system for a specificdomain.
The task of determining which par-allel texts should be included for training, andwhich ones hurt translation performance, is te-dious when performed through trial-and-error.Alternatively, methods for a weighted combina-tion exist, but there is conflicting evidence as towhich approach works best, and the issue of de-termining weights is not adequately resolved.The picture looks better in language mod-elling, where model interpolation through per-plexity minimization has become a widespreadmethod of domain adaptation.
We investigate theapplicability of this method for translation mod-els, and discuss possible applications.We move the focus away from a binary com-bination of in-domain and out-of-domain data.
Ifwe can scale up the number of models whose con-tributions we weight, this reduces the need for apriori knowledge about the fitness1 of each poten-tial training text, and opens new research oppor-tunities, for instance experiments with clusteredtraining data.2 Domain Adaptation for TranslationModelsTo motivate efforts in domain adaptation, let usreview why additional training data can improve,but also decrease translation quality.Adding more training data to a translation sys-tem is easy to motivate through the data sparse-ness problem.
Koehn and Knight (2001) showthat translation quality correlates strongly withhow often a word occurs in the training corpus.Rare words or phrases pose a problem in sev-eral stages of MT modelling, from word align-ment to the computation of translation probabil-ities through Maximum Likelihood Estimation.Unknown words are typically copied verbatim tothe target text, which may be a good strategy fornamed entities, but is often wrong otherwise.
Ingeneral, more data allows for a better word align-ment, a better estimation of translation probabili-ties, and for the consideration of more context (inphrase-based or syntactic SMT).A second effect of additional data is not nec-essarily positive.
Translations are inherently am-biguous, and a strong source of ambiguity is the1We borrow this term from early evolutionary biology toemphasize that the question in domain adaptation is not how?good?
or ?bad?
the data is, but how well-adapted it is to thetask at hand.539domain of a text.
The German word ?Wort?
(engl.word) is typically translated as floor in Europarl,a corpus of Parliamentary Proceedings (Koehn,2005), owing to the high frequency of phrasessuch as you have the floor, which is translated intoGerman as Sie haben das Wort.
This translationis highly idiomatic and unlikely to occur in othercontexts.
Still, adding Europarl as out-of-domaintraining data shifts the probability distribution ofp(t|?Wort?)
in favour of p(?floor?|?Wort?
), andmay thus lead to improper translations.We will refer to the two problems as the datasparseness problem and the ambiguity problem.Adding out-of-domain data typically mitigates thedata sparseness problem, but exacerbates the am-biguity problem.
The net gain (or loss) of addingmore data changes from case to case.
Becausethere are (to our knowledge) no tools that predictthis net effect, it is a matter of empirical investi-gation (or, in less suave terms, trial-and-error), todetermine which corpora to use.2From this understanding of the reasons for andagainst out-of-domain data, we formulate the fol-lowing hypotheses:1.
A weighted combination can control the con-tribution of the out-of-domain corpus on theprobability distribution, and thus limit theambiguity problem.2.
A weighted combination eliminates the needfor data selection, offering a robust baselinefor domain-specific machine translation.We will discuss three mixture modelling tech-niques for translation models.
Our aim is to adaptall four features of the standard Moses SMT trans-lation model: the phrase translation probabilitiesp(t|s) and p(s|t), and the lexical weights lex(t|s)and lex(s|t).32.1 Linear InterpolationA well-established approach in language mod-elling is the linear interpolation of several mod-els, i.e.
computing the weighted average of the in-2A frustrating side-effect is that these findings rarely gen-eralize.
For instance, we were unable to reproduce the find-ing by Ceaus?u et al(2011) that patent translation systemsare highly domain-sensitive and suffer from the inclusion ofparallel training data from other patent subdomains.3We can ignore the fifth feature, the phrase penalty,which is a constant.dividual model probabilities.
It is defined as fol-lows:p(x|y;?)
=n?i=1?ipi(x|y) (1)with ?i being the interpolation weight of eachmodel i, and with (?i ?i) = 1.For SMT, linear interpolation of translationmodels has been used in numerous systems.
Theapproaches diverge in how they set the inter-polation weights.
Some authors use uniformweights (Cohn and Lapata, 2007), others em-pirically test different interpolation coefficients(Finch and Sumita, 2008; Yasuda et al 2008;Nakov and Ng, 2009; Axelrod et al 2011), othersapply monolingual metrics to set the weights forTM interpolation (Foster and Kuhn, 2007; Koehnet al 2010).There are reasons against all these approaches.Uniform weights are easy to implement, but givelittle control.
Empirically, it has been shown thatthey often do not perform optimally (Finch andSumita, 2008; Yasuda et al 2008).
An opti-mization of BLEU scores on a development set ispromising, but slow and impractical.
There is noeasy way to integrate linear interpolation into log-linear SMT frameworks and perform optimizationthrough MERT.
Monolingual optimization objec-tives such as language model perplexity have theadvantage of being well-known and readily avail-able, but their relation to the ambiguity problemis indirect at best.Linear interpolation is seemingly well-definedin equation 1.
Still, there are a few implemen-tation details worth pointing out.
If we directlyinterpolate each feature in the translation model,and define the feature values of non-occurringphrase pairs as 0, this disregards the meaning ofeach feature.
If we estimate p(x|y) via MLE as inequation 2, and c(y) = 0, then p(x|y) is strictlyspeaking undefined.
Alternatively to a naive al-gorithm, which treats unknown phrase pairs ashaving a probability of 0, which results in a defi-cient probability distribution, we propose and im-plement the following algorithm.
For each valuepair (x, y) for which we compute p(x|y), we re-place ?i with 0 for all models i with p(y) =0, then renormalize the weight vector ?
to 1.We do this for p(t|s) and lex(t|s), but not forp(s|t) and lex(s|t), the reasoning being the con-540sequences for perplexity minimization (see sec-tion 2.4).
Namely, we do not want to penalizea small in-domain model for having a high out-of-vocabulary rate on the source side, but we dowant to penalize models that know the sourcephrase, but not its correct translation.
A sec-ond modification pertains to the lexical weightslex(s|t) and lex(t|s), which form no true proba-bility distribution, but are derived from the indi-vidual word translation probabilities of a phrasepair (see (Koehn et al 2003)).
We propose tonot interpolate the features directly, but the wordtranslation probabilities which are the basis of thelexical weight computation.
The reason for this isthat word pairs are less sparse than phrase pairs,so that we can even compute lexical weights forphrase pairs which are unknown in a model.42.2 Weighted CountsWeighting of different corpora can also be imple-mented through a modified Maximum LikelihoodEstimation.
The traditional equation for MLE is:p(x|y) =c(x, y)c(y)=c(x, y)?x?
c(x?, y)(2)where c denotes the count of an observation, andp the model probability.
If we generalize the for-mula to compute a probability from n corpora,and assign a weight ?i to each, we get5:p(x|y;?)
=?ni=1 ?ici(x, y)?ni=1?x?
?ici(x?, y)(3)The main difference to linear interpolation isthat this equation takes into account how well-evidenced a phrase pair is.
This includes the dis-tinction between lack of evidence and negative ev-idence, which is missing in a naive implementa-tion of linear interpolation.Translation models trained with weightedcounts have been discussed before, and havebeen shown to outperform uniform ones in somesettings.
However, researchers who demon-strated this fact did so with arbitrary weights (e.g.
(Koehn, 2002)), or by empirically testing differ-ent weights (e.g.
(Nakov and Ng, 2009)).
We donot know of any research on automatically deter-mining weights for this method, or which is notlimited to two corpora.4For instance if the word pairs (the,der) and (man,Mann)are known, but the phrase pair (the man, der Mann) is not.5Unlike equation 1, equation 3 does not require that(?i ?i) = 1.2.3 Alternative PathsA third method is using multiple translation mod-els as alternative decoding paths (Birch et al2007), an idea which Koehn and Schroeder (2007)first used for domain adaptation.
This approachhas the attractive theoretical property that addingnew models is guaranteed to lead to equal or bet-ter performance, given the right weights.
At best,a model is beneficial with appropriate weights.
Atworst, we can set the feature weights so that thedecoding paths of one model are never picked forthe final translation.
In practice, each translationmodel adds 5 features and thus 5 more dimensionsto the weight space, which leads to longer search,search errors, and/or overfitting.
The expectationis that, at least with MERT, using alternative de-coding paths does not scale well to a high numberof models.A suboptimal choice of weights is not the onlyweakness of alternative paths, however.
Let usassume that all models have the same weights.Note that, if a phrase pair occurs in several mod-els, combining models through alternative pathsmeans that the decoder selects the path with thehighest probability, whereas with linear interpo-lation, the probability of the phrase pair wouldbe the (weighted) average of all models.
Select-ing the highest-scoring phrase pair favours statis-tical outliers and hence is the less robust decision,prone to data noise and data sparseness.2.4 Perplexity MinimizationIn language modelling, perplexity is frequentlyused as a quality measure for language models(Chen and Goodman, 1998).
Among other appli-cations, language model perplexity has been usedfor domain adaptation (Foster and Kuhn, 2007).For translation models, perplexity is most closelyassociated with EM word alignment (Brown etal., 1993) and has been used to evaluate differentalignment algorithms (Al-Onaizan et al 1999).We investigate translation model perplexityminimization as a method to set model weightsin mixture modelling.
For the purpose of opti-mization, the cross-entropy H(p), the perplexity2H(p), and other derived measures are equivalent.The cross-entropy H(p) is defined as:66See (Chen and Goodman, 1998) for a short discussionof the equation.
In short, a lower cross-entropy indicates thatthe model is better able to predict the development set.541H(p) = ??x,yp?
(x, y) log2 p(x|y) (4)The phrase pairs (x, y) whose probability wemeasure, and their empirical probability p?
needto be extracted from a development set, whereasp is the model probability.
To obtain the phrasepairs, we process the development set with thesame word alignment and phrase extraction toolsthat we use for training, i.e.
GIZA++ and heuris-tics for phrase extraction (Och and Ney, 2003).The objective function is the minimization of thecross-entropy, with the weight vector ?
as argu-ment:??
= argmin???x,yp?
(x, y) log2 p(x|y;?)
(5)We can fill in equations 1 or 3 for p(x|y;?).
Theoptimization itself is convex and can be done withoff-the-shelf software.7 We use L-BFGS withnumerically approximated gradients (Byrd et al1995).Perplexity minimization has the advantage thatit is well-defined for both weighted counts and lin-ear interpolation, and can be quickly computed.Other than in language modelling, where p(x|y)is the probability of a word given a n-gram his-tory, conditional probabilities in translation mod-els express the probability of a target phrase givena source phrase (or vice versa), which connectsthe perplexity to the ambiguity problem.
Thehigher the probability of ?correct?
phrase pairs,the lower the perplexity, and the more likelythe model is to successfully resolve the ambigu-ity.
The question is in how far perplexity min-imization coincides with empirically good mix-ture weights.8 This depends, among others, onthe other model components in the SMT frame-work, for instance the language model.
We willnot evaluate perplexity minimization against em-pirically optimized mixture weights, but apply itin situations where the latter is infeasible, e.g.
be-cause of the number of models.7A quick demonstration of convexity: equation 1 isaffine; equation 3 linear-fractional.
Both are convex in thedomain R>0.
Consequently, equation 4 is also convex be-cause it is the weighted sum of convex functions.8There are tasks for which perplexity is known to be un-reliable, e.g.
for comparing models with different vocabular-ies.
However, such confounding factors do not affect the op-timization algorithm, which works with a fixed set of phrasepairs, and merely varies ?.Our main technical contributions are as fol-lows: Additionally to perplexity optimization forlinear interpolation, which was first applied byFoster et al(2010), we propose perplexity opti-mization for weighted counts (equation 3), and amodified implementation of linear interpolation.Also, we independently perform perplexity mini-mization for all four features of the standard SMTtranslation model: the phrase translation proba-bilities p(t|s) and p(s|t), and the lexical weightslex(t|s) and lex(s|t).3 Other Domain Adaptation TechniquesSo far, we discussed mixture modelling for trans-lation models, which is only a subset of domainadaptation techniques in SMT.Mixture-modelling for language models is wellestablished (Foster and Kuhn, 2007).
Languagemodel adaptation serves the same purpose astranslation model adaptation, i.e.
skewing theprobability distribution in favour of in-domaintranslations.
This means that LM adaptation mayhave similar effects as TM adaptation, and thatthe two are to some extent redundant.
Foster andKuhn (2007) find that ?both TM and LM adap-tation are effective?, but that ?combined LM andTM adaptation is not better than LM adaptationon its own?.A second strand of research in domain adap-tation is data selection, i.e.
choosing a subset ofthe training data that is considered more relevantfor the task at hand.
This has been done for lan-guage models using techniques from informationretrieval (Zhao et al 2004), or perplexity (Lin etal., 1997; Moore and Lewis, 2010).
Data selec-tion has also been proposed for translation mod-els (Axelrod et al 2011).
Note that for transla-tion models, data selection offers an unattractivetrade-off between the data sparseness and the am-biguity problem, and that the optimal amount ofdata to select is hard to determine.Our discussion of mixture-modelling is rela-tively coarse-grained, with 2-10 models beingcombined.
Matsoukas et al(2009) propose an ap-proach where each sentence is weighted accord-ing to a classifier, and Foster et al(2010) ex-tend this approach by weighting individual phrasepairs.
These more fine-grained methods need notbe seen as alternatives to coarse-grained ones.Foster et al(2010) combine the two, apply-ing linear interpolation to combine the instance-542weighted out-of-domain model with an in-domainmodel.4 EvaluationApart from measuring the performance of the ap-proaches introduced in section 2, we want to in-vestigate the following open research questions.1.
Does an implementation of linear interpola-tion that is more closely tailored to trans-lation modelling outperform a naive imple-mentation?2.
How do the approaches perform outside abinary setting, i.e.
when we do not workwith one in-domain and one out-of-domainmodel, but with a higher number of models?3.
Can we apply perplexity minimization toother translation model features such as thelexical weights, and if yes, does a separateoptimization of each translation model fea-ture improve performance?4.1 Data and MethodsIn terms of tools and techniques used, we mostlyadhere to the work flow described for the WMT2011 baseline system9.
The main tools are Moses(Koehn et al 2007), SRILM (Stolcke, 2002), andGIZA++ (Och and Ney, 2003), with settings asdescribed in the WMT 2011 guide.
We reporttwo translation measures: BLEU (Papineni et al2002) and METEOR 1.3 (Denkowski and Lavie,2011).
All results are lowercased and tokenized,measured with five independent runs of MERT(Och and Ney, 2003) and MultEval (Clark et al2011) for resampling and significance testing.We compare three baselines and four transla-tion model mixture techniques.
The three base-lines are a purely in-domain model, a purely out-of-domain model, and a model trained on the con-catenation of the two, which corresponds to equa-tion 3 with uniform weights.
Additionally, weevaluate perplexity optimization with weightedcounts and the two implementations of linear in-terpolation contrasted in section 2.1.
The two lin-ear interpolations that are contrasted are a naiveone, i.e.
a direct, unnormalized interpolation of9http://www.statmt.org/wmt11/baseline.htmlData set sentences words (fr)Alpine (in-domain) 220k 4 700kEuroparl 1 500k 44 000kJRC Acquis 1 100k 24 000kOpenSubtitles v2 2 300k 18 000kTotal train 5 200k 91 000kDev 1424 33 000Test 991 21 000Table 1: Parallel data sets for German ?
French trans-lation task.Data set sentences wordsAlpine (in-domain) 650k 13 000kNews-commentary 150k 4 000kEuroparl 2 000k 60 000kNews 25 000k 610 000kTotal 28 000k 690 000kTable 2: Monolingual French data sets for German ?French translation task.all translation model features, and a modified onethat normalizes ?
for each phrase pair (s, t) forp(t|s) and recomputes the lexical weights basedon interpolated word translation probabilites.
Thefourth weighted combination is using alternativedecoding paths with weights set through MERT.The four weighted combinations are evaluatedtwice: once applied to the original four or ten par-allel data sets, once in a binary setting in whichall out-of-domain data sets are first concatenated.Since we want to concentrate on translationmodel domain adaptation, we keep other modelcomponents, namely word alignment and the lex-ical reordering model, constant throughout the ex-periments.
We contrast two language models.
Anunadapted, out-of-domain language model trainedon data sets provided for the WMT 2011 transla-tion task, and an adapted language model which isthe linear interpolation of all data sets, optimizedfor minimal perplexity on the in-domain develop-ment set.While unadapted language models are becom-ing more rare in domain adaptation research, theyallow us to contrast different TM mixtures with-out the effect on performance being (partially)hidden by language model adaptation with thesame effect.The first data set is a DE?FR translation sce-nario in the domain of mountaineering.
The in-domain corpus is a collection of Alpine Club pub-543lications (Volk et al 2010).
As parallel out-of-domain dataset, we use Europarl, a collection ofparliamentary proceedings (Koehn, 2005), JRC-Acquis, a collection of legislative texts (Stein-berger et al 2006), and OpenSubtitles v2, a par-allel corpus extracted from film subtitles10 (Tiede-mann, 2009).
For language modelling, we use in-domain data and data from the 2011 Workshopon Statistical Machine Translation.
The respec-tive sizes of the data sets are listed in tables 1 and2.As the second data set, we use the Haitian Cre-ole ?
English data from the WMT 2011 featuredtranslation task.
It consists of emergency SMSsent in the wake of the 2010 Haiti earthquake.Originally, Microsoft Research and CMU oper-ated under severe time constraints to build a trans-lation system for this language pair.
This limitsthe ability to empirically verify how much eachdata set contributes to translation quality, and in-creases the importance of automated and quickdomain adaptation methods.Note that both data sets have a relatively highratio of in-domain to out-of-domain parallel train-ing data (1:20 for DE?EN and 1:5 for HT?EN)Previous research has been performed with ratiosof 1:100 (Foster et al 2010) or 1:400 (Axelrodet al 2011).
Since domain adaptation becomesmore important when the ratio of IN to OUT islow, and since such low ratios are also realistic11,we also include results for which the amount ofin-domain parallel data has been restricted to 10%of the available data set.We used the same development set for lan-guage/translation model adaptation and settingthe global model weights with MERT.
While itis theoretically possible that MERT will give toohigh weights to models that are optimized on thesame development set, we found no empirical evi-dence for this in experiments with separate devel-opment sets.4.2 ResultsThe results are shown in tables 5 and 6.
In theDE?FR translation task, results vary between 13.5and 18.9 BLEU points; in the HT?EN task, be-tween 24.3 and 33.8.
Unsurprisingly, an adapted10http://www.opensubtitles.org11We predict that the availability of parallel data willsteadily increase, most data being out-of-domain for anygiven task.Data set units words (en)SMS (in-domain) 16 500 380 000Medical 1 600 10 000Newswire 13 500 330 000Glossary 35 700 90 000Wikipedia 8 500 110 000Wikipedia NE 10 500 34 000Bible 30 000 920 000Haitisurf dict 3 700 4000Krengle dict 1 600 2 600Krengle 650 4 200Total train 120 000 1 900 000Dev 900 22 000Test 1274 25 000Table 3: Parallel data sets for Haiti Creole ?
Englishtranslation task.Data set sentences wordsSMS (in-domain) 16k 380kNews 113 000k 2 650 000kTable 4: Monolingual English data sets for Haiti Cre-ole ?
English translation task.LM performs better than an out-of-domain one,and using all available in-domain parallel data isbetter than using only part of it.
The same is nottrue for out-of-domain data, which highlights theproblem discussed in the introduction.
For theDE?FR task, adding 86 million words of out-of-domain parallel data to the 5 million in-domaindata set does not lead to consistent performancegains.
We observe a decrease of 0.3 BLEU pointswith an out-of-domain LM, and an increase of 0.4BLEU points with an adapted LM.
The out-of-domain training data has a larger positive effectif less in-domain data is available, with a gain of1.4 BLEU points.
The results in the HT?EN trans-lation task (table 6) paint a similar picture.
Aninteresting side note is that even tiny amounts ofin-domain parallel data can have strong effects onperformance.
A training set of 1600 emergencySMS (38 000 tokens) yields a comparable perfor-mance to an out-of-domain data set of 1.5 milliontokens.As to the domain adaptation experiments,weights optimized through perplexity minimiza-tion are significantly better in the majority ofcases, and never significantly worse, than uniform544Systemout-of-domain LM adapted LMfull IN TM full IN TM small IN TMBLEU METEOR BLEU METEOR BLEU METEORin-domain 16.8 35.9 17.9 37.0 15.7 33.5out-of-domain 13.5 31.3 14.8 32.3 14.8 32.3counts (concatenation) 16.5 35.7 18.3 37.3 17.1 35.4binary in/outweighted counts 17.4 36.6 18.7 37.9 17.6 36.2linear interpolation (naive) 17.4 36.7 18.8 37.9 17.6 36.1linear interpolation (modified) 17.2 36.5 18.9 38.0 17.6 36.2alternative paths 17.2 36.5 18.6 37.8 17.4 36.04 modelsweighted counts 17.3 36.6 18.8 37.8 17.4 36.0linear interpolation (naive) 17.1 36.5 18.5 37.7 17.3 35.9linear interpolation (modified) 17.2 36.5 18.7 37.9 17.3 36.0alternative paths 17.0 36.2 18.3 37.4 16.3 35.1Table 5: Domain adaptation results DE?FR.
Domain: Alpine texts.
Full IN TM: Using the full in-domain parallelcorpus; small IN TM: using 10% of available in-domain parallel data.weights.12 However, the difference is smaller forthe experiments with an adapted language modelthan for those with an out-of-domain one, whichconfirms that the benefit of language model adap-tation and translation model adaptation are notfully cumulative.
Performance-wise, there seemsto be no clear winner between weighted countsand the two alternative implementations of lin-ear interpolation.
We can still argue for weightedcounts on theoretical grounds.
A weighted MLE(equation 3) returns a true probability distribution,whereas a naive implementation of linear interpo-lation results in a deficient model.
Consequently,probabilities are typically lower in the naively in-terpolated model, which results in higher (worse)perplexities.
While the deficiency did not affectMERT or decoding negatively, it might becomeproblematic in other applications, for instance ifwe want to use an interpolated model as a compo-nent in a second perplexity-based combination ofmodels.13When moving from a binary setting withone in-domain and one out-of-domain transla-tion model (trained on all available out-of-domaindata) to 4?10 translation models, we observe aserious performance degradation for alternativepaths, while performance of the perplexity opti-12This also applies to linear interpolation with uniformweights, which is not shown in the tables.13Specifically, a deficient model would be dispreferred bythe perplexity minimization algorithm.mization methods does not change significantly.This is positive for perplexity optimization be-cause it demonstrates that it requires less a prioriinformation, and opens up new research possibil-ities, i.e.
experiments with different clusterings ofparallel data.
The performance degradation foralternative paths is partially due to optimizationproblems in MERT, but also due to a higher sus-ceptibility to statistical outliers, as discussed insection 2.3.14A pessimistic interpretation of the resultswould point out that performance gains comparedto the best baseline system are modest or eveninexistent in some settings.
However, we wantto stress two important points.
First, we oftendo not know a priori whether adding an out-of-domain data set boosts or weakens translation per-formance.
An automatic weighting of data sets re-duces the need for trial-and-error experimentationand is worthwhile even if a performance increaseis not guaranteed.
Second, the potential impactof a weighted combination depends on the trans-lation scenario and the available data sets.
Gen-erally, we expect non-uniform weighting to havea bigger impact when the models that are com-bined are more dissimilar (in terms of fitness forthe task), and if the ratio of in-domain to out-of-domain data is low.
Conversely, there are situa-14We empirically verified this weakness in a synthetic ex-periment with a randomly split training corpus and identicalweights for each path.545Systemout-of-domain LM adapted LMfull IN TM full IN TM small IN TMBLEU METEOR BLEU METEOR BLEU METEORin-domain 30.4 30.7 33.4 31.7 29.7 28.6out-of-domain 24.3 28.0 28.9 30.2 28.9 30.2counts (concatenation) 30.3 31.2 33.6 32.4 31.3 31.3binary in/outweighted counts 31.0 31.6 33.8 32.4 31.5 31.3linear interpolation (naive) 30.8 31.4 33.7 32.4 31.9 31.3linear interpolation (modified) 30.8 31.5 33.7 32.4 31.7 31.2alternative paths 30.8 31.3 33.2 32.4 29.8 30.710 modelsweighted counts 31.0 31.5 33.5 32.3 31.8 31.5linear interpolation (naive) 30.9 31.4 33.8 32.4 31.9 31.3linear interpolation (modified) 31.0 31.6 33.8 32.5 32.1 31.5alternative paths 25.9 29.2 24.3 29.1 29.8 30.9Table 6: Domain adaptation results HT?EN.
Domain: emergency SMS.
Full IN TM: Using the full in-domainparallel corpus; small IN TM: using 10% of available in-domain parallel data.tions where we actually expect a simple concate-nation to be optimal, e.g.
when the data sets havevery similar probability distributions.4.2.1 Individually Optimizing Each TMFeatureIt is hard to empirically show how translationmodel perplexity optimization compares to usingmonolingual perplexity measures for the purposeof weighting translation models, as e.g.
done by(Foster and Kuhn, 2007; Koehn et al 2010).
Oneproblem is that there are many different possibleconfigurations for the latter.
We can use sourceside or target side language models, operate withdifferent vocabularies, smoothing techniques, andn-gram orders.One of the theoretical considerations thatfavour measuring perplexity on the translationmodel rather than using monolingual measuresis that we can optimize each translation modelfeature separately.
In the default Moses transla-tion model, the four features are p(s|t), lex(s|t),p(t|s) and lex(t|s).We empirically test different optimizationschemes as follows.
We optimize perplexity oneach feature independently, obtaining 4 weightvectors.
We then compute one model with oneweight vector per feature (namely the feature thatthe vector was optimized on), and four modelsthat use one of the weight vectors for all features.A further model uses a weight vector that is theweightsperplexityBLEU1 2 3 4weighted countsuniform 5.12 7.68 4.84 13.67 30.3separate 4.68 6.62 4.24 8.57 31.01 4.68 6.84 4.50 10.86 30.32 4.78 6.62 4.48 10.54 30.33 4.86 7.31 4.24 9.15 30.84 5.33 7.87 4.52 8.57 30.9average 4.72 6.71 4.38 9.95 30.4linear interpolation (modified)uniform 19.89 82.78 4.80 10.78 30.6separate 5.45 8.56 4.28 8.85 31.01 5.45 8.79 4.40 8.89 30.82 5.71 8.56 4.54 8.91 30.93 6.46 11.88 4.28 9.07 31.04 6.12 10.86 4.47 8.85 30.9average 5.73 9.72 4.34 8.89 30.9LM 6.01 9.83 4.56 8.96 30.8Table 7: Contrast between a separate optimization ofeach feature and applying the weight vector optimizedon one feature to the whole model.
HT?EN with out-of-domain LM.546average of the other four.
For linear interpolation,we also include a model whose weights have beenoptimized through language model perplexity op-timization, with a 3-gram language model (modi-fied Knesey-Ney smoothing) trained on the targetside of each parallel data set.Table 7 shows the results.
In terms of BLEUscore, a separate optimization of each feature is awinner in our experiment in that no other schemeis better, with 8 of the 11 alternative weightingschemes (excluding uniform weights) being sig-nificantly worse than a separate optimization.
Thedifferences in BLEU score are small, however,since the alternative weighting schemes are gen-erally felicitious in that they yield both a lowerperplexity and better BLEU scores than uniformweighting.
While our general expectation is thatlower perplexities correlate with higher transla-tion performance, this relation is complicated byseveral facts.
Since the interpolated models aredeficient (i.e.
their probabilities do not sum to 1),perplexities for weighted counts and our imple-mentation of linear interpolation cannot be com-pard.
Also, note that not all features are equallyimportant for decoding.
Their weights in the log-linear model are set through MERT and vary be-tween optimization runs.5 ConclusionThis paper contributes to SMT domain adaptationresearch in several ways.
We expand on workby (Foster et al 2010) in establishing transla-tion model perplexity minimization as a robustbaseline for a weighted combination of translationmodels.15 We demonstrate perplexity optimiza-tion for weighted counts, which are a natural ex-tension of unadapted MLE training, but are of lit-tle prominence in domain adaptation research.
Wealso show that we can separately optimize the fourvariable features in the Moses translation modelthrough perplexity optimization.We break with prior domain adaptation re-search in that we do not rely on a binary clusteringof in-domain and out-of-domain training data.
Wedemonstrate that perplexity minimization scaleswell to a higher number of translation models.This is not only useful for domain adaptation, butfor various tasks that profit from mixture mod-15The source code is available in the Moses repositoryhttp://github.com/moses-smt/mosesdecoderelling.
We envision that a weighted combinationcould be useful to deal with noisy datasets, or ap-plied after a clustering of training data.AcknowledgementsThis research was funded by the Swiss NationalScience Foundation under grant 105215_126999.ReferencesYaser Al-Onaizan, Jan Curin, Michael Jahr, KevinKnight, John Lafferty, Dan Melamed, Franz-JosefOch, David Purdy, Noah A. Smith, and DavidYarowsky.
1999.
Statistical machine translation.Technical report, Final Report, JHU Summer Work-shop.Amittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domaindata selection.
In Proceedings of the EMNLP 2011Workshop on Statistical Machine Translation.Alexandra Birch, Miles Osborne, and Philipp Koehn.2007.
CCG supertags in factored statistical ma-chine translation.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages9?16, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Peter F. Brown, Vincent J. Della Pietra, Stephen A.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation:Parameter Estimation.
Computational Linguistics,19(2):263?311.Richard H. Byrd, Peihuang Lu, Jorge Nocedal, andCiyou Zhu.
1995.
A limited memory algorithmfor bound constrained optimization.
SIAM J.
Sci.Comput., 16:1190?1208, September.Alexandru Ceaus?u, John Tinsley, Jian Zhang, andAndy Way.
2011.
Experiments on domain adap-tation for patent machine translation in the PLuTOproject.
In Proceedings of the 15th conference ofthe European Association for Machine Translation,Leuven, Belgium.Stanley F. Chen and Joshua Goodman.
1998.
An em-pirical study of smoothing techniques for languagemodeling.
Computer Speech & Language, 13:359?393.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testing forstatistical machine translation: Controlling for op-timizer instability.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages176?181, Portland, Oregon, USA, June.
Associa-tion for Computational Linguistics.Trevor Cohn and Mirella Lapata.
2007.
MachineTranslation by Triangulation: Making Effective Useof Multi-Parallel Corpora.
In Proceedings of the54745th Annual Meeting of the Association of Compu-tational Linguistics, pages 728?735, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic Metric for Reliable Optimizationand Evaluation of Machine Translation Systems.
InProceedings of the EMNLP 2011 Workshop on Sta-tistical Machine Translation.Andrew Finch and Eiichiro Sumita.
2008.
Dynamicmodel interpolation for statistical machine transla-tion.
In Proceedings of the Third Workshop onStatistical Machine Translation, StatMT ?08, pages208?215, Stroudsburg, PA, USA.
Association forComputational Linguistics.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for smt.
In Proceedings of theSecond Workshop on Statistical Machine Transla-tion, StatMT ?07, pages 128?135, Stroudsburg, PA,USA.
Association for Computational Linguistics.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adap-tation in statistical machine translation.
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing, pages 451?459,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Philipp Koehn and Kevin Knight.
2001.
Knowledgesources for word-level translation models.
In Lil-lian Lee and Donna Harman, editors, Proceedingsof the 2001 Conference on Empirical Methods inNatural Language Processing, pages 27?35.Philipp Koehn and Josh Schroeder.
2007.
Experi-ments in domain adaptation for statistical machinetranslation.
In Proceedings of the Second Work-shop on Statistical Machine Translation, StatMT?07, pages 224?227, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InNAACL ?03: Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology, pages 48?54, Morristown, NJ, USA.Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondr?ej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open Source Toolkit for Statistical Ma-chine Translation.
In ACL 2007, Proceedings of the45th Annual Meeting of the Association for Com-putational Linguistics Companion Volume Proceed-ings of the Demo and Poster Sessions, pages 177?180, Prague, Czech Republic, June.
Association forComputational Linguistics.Philipp Koehn, Barry Haddow, Philip Williams, andHieu Hoang.
2010.
More linguistic annotationfor statistical machine translation.
In Proceedingsof the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, pages 115?120, Up-psala, Sweden, July.
Association for ComputationalLinguistics.Philipp Koehn.
2002.
Europarl: A Multilingual Cor-pus for Evaluation of Machine Translation.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Machine Transla-tion Summit X, pages 79?86, Phuket, Thailand.Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien,Keh-Jiann Chen, and Lin-Shan Lee.
1997.
Chineselanguage model adaptation based on document clas-sification and multiple domain-specific languagemodels.
In George Kokkinakis, Nikos Fakotakis,and Evangelos Dermatas, editors, EUROSPEECH.ISCA.Spyros Matsoukas, Antti-Veikko I. Rosti, and BingZhang.
2009.
Discriminative corpus weight esti-mation for machine translation.
In Proceedings ofthe 2009 Conference on Empirical Methods in Nat-ural Language Processing: Volume 2 - Volume 2,pages 708?717, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Robert C. Moore and William Lewis.
2010.
Intelli-gent selection of language model training data.
InProceedings of the ACL 2010 Conference Short Pa-pers, ACLShort ?10, pages 220?224, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Preslav Nakov and Hwee Tou Ng.
2009.
Improvedstatistical machine translation for resource-poorlanguages using related resource-rich languages.
InProceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing: Vol-ume 3 - Volume 3, EMNLP ?09, pages 1358?1367,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
Bleu: A method for automaticevaluation of machine translation.
In ACL ?02: Pro-ceedings of the 40th Annual Meeting on Associa-tion for Computational Linguistics, pages 311?318,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Tomaz Erjavec, Dan Tufis, andDaniel Varga.
2006.
The JRC-Acquis: A multilin-gual aligned parallel corpus with 20+ languages.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation (LREC?2006).548A.
Stolcke.
2002.
SRILM ?
An Extensible LanguageModeling Toolkit.
In Seventh International Confer-ence on Spoken Language Processing, pages 901?904, Denver, CO, USA.J?rg Tiedemann.
2009.
News from opus - a col-lection of multilingual parallel corpora with toolsand interfaces.
In N. Nicolov, K. Bontcheva,G.
Angelova, and R. Mitkov, editors, RecentAdvances in Natural Language Processing, vol-ume V, pages 237?248.
John Benjamins, Amster-dam/Philadelphia, Borovets, Bulgaria.Martin Volk, Noah Bubenhofer, Adrian Althaus, MayaBangerter, Lenz Furrer, and Beni Ruef.
2010.
Chal-lenges in building a multilingual alpine heritagecorpus.
In Proceedings of the Seventh conferenceon International Language Resources and Evalu-ation (LREC?10), Valletta, Malta.
European Lan-guage Resources Association (ELRA).Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,and Eiichiro Sumita.
2008.
Method of selectingtraining data to build a compact and efficient trans-lation model.
In Proceedings of the 3rd Interna-tional Joint Conference on Natural Language Pro-cessing (IJCNLP).Bing Zhao, Matthias Eck, and Stephan Vogel.
2004.Language model adaptation for statistical machinetranslation with structured query models.
In Pro-ceedings of the 20th international conference onComputational Linguistics, COLING ?04, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.549
