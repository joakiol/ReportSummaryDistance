Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370?1380,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsFast and Robust Neural Network Joint Models for Statistical MachineTranslationJacob Devlin, Rabih Zbib, Zhongqiang Huang,Thomas Lamar, Richard Schwartz, and John MakhoulRaytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, USA{jdevlin,rzbib,zhuang,tlamar,schwartz,makhoul}@bbn.comAbstractRecent work has shown success in us-ing neural network language models(NNLMs) as features in MT systems.Here, we present a novel formulation fora neural network joint model (NNJM),which augments the NNLM with a sourcecontext window.
Our model is purely lexi-calized and can be integrated into any MTdecoder.
We also present several varia-tions of the NNJM which provide signif-icant additive improvements.Although the model is quite simple, ityields strong empirical results.
On theNIST OpenMT12 Arabic-English condi-tion, the NNJM features produce a gain of+3.0 BLEU on top of a powerful, feature-rich baseline which already includes atarget-only NNLM.
The NNJM featuresalso produce a gain of +6.3 BLEU on topof a simpler baseline equivalent to Chi-ang?s (2007) original Hiero implementa-tion.Additionally, we describe two novel tech-niques for overcoming the historicallyhigh cost of using NNLM-style modelsin MT decoding.
These techniques speedup NNJM computation by a factor of10,000x, making the model as fast as astandard back-off LM.This work was supported by DARPA/I2O Contract No.HR0011-12-C-0014 under the BOLT program (Approved forPublic Release, Distribution Unlimited).
The views, opin-ions, and/or findings contained in this article are those of theauthor and should not be interpreted as representing the of-ficial views or policies, either expressed or implied, of theDefense Advanced Research Projects Agency or the Depart-ment of Defense.1 IntroductionIn recent years, neural network models have be-come increasingly popular in NLP.
Initially, thesemodels were primarily used to create n-gram neu-ral network language models (NNLMs) for speechrecognition and machine translation (Bengio et al,2003; Schwenk, 2010).
They have since been ex-tended to translation modeling, parsing, and manyother NLP tasks.In this paper we use a basic neural network ar-chitecture and a lexicalized probability model tocreate a powerful MT decoding feature.
Specifi-cally, we introduce a novel formulation for a neu-ral network joint model (NNJM), which augmentsan n-gram target language model with an m-wordsource window.
Unlike previous approaches tojoint modeling (Le et al, 2012), our feature can beeasily integrated into any statistical machine trans-lation (SMT) decoder, which leads to substantiallylarger improvements than k-best rescoring only.Additionally, we present several variations of thismodel which provide significant additive BLEUgains.We also present a novel technique for trainingthe neural network to be self-normalized, whichavoids the costly step of posteriorizing over theentire vocabulary in decoding.
When used in con-junction with a pre-computed hidden layer, thesetechniques speed up NNJM computation by a fac-tor of 10,000x, with only a small reduction on MTaccuracy.Although our model is quite simple, we obtainstrong empirical results.
We show primary resultson the NIST OpenMT12 Arabic-English condi-tion.
The NNJM features produce an improvementof +3.0 BLEU on top of a baseline that is alreadybetter than the 1st place MT12 result and includes1370a powerful NNLM.
Additionally, on top of a sim-pler decoder equivalent to Chiang?s (2007) origi-nal Hiero implementation, our NNJM features areable to produce an improvement of +6.3 BLEU ?as much as all of the other features in our strongbaseline system combined.We also show strong improvements on theNIST OpenMT12 Chinese-English task, as well asthe DARPA BOLT (Broad Operational LanguageTranslation) Arabic-English and Chinese-Englishconditions.2 Neural Network Joint Model (NNJM)Formally, our model approximates the probabilityof target hypothesis T conditioned on source sen-tence S. We follow the standard n-gram LM de-composition of the target, where each target wordtiis conditioned on the previous n ?
1 targetwords.
To make this a joint model, we also condi-tion on source context vector Si:P (T |S) ?
?|T |i=1P (ti|ti?1, ?
?
?
, ti?n+1, Si)Intuitively, we want to define Sias the windowthat is most relevant to ti.
To do this, we first saythat each target word tiis affiliated with exactlyone source word at index ai.
Siis then them-wordsource window centered at ai:Si= sai?m?12, ?
?
?
, sai, ?
?
?
, sai+m?12This notion of affiliation is derived from theword alignment, but unlike word alignment, eachtarget word must be affiliated with exactly onenon-NULL source word.
The affiliation heuristicis very simple:(1) If tialigns to exactly one source word, aiisthe index of the word it aligns to.
(2) If tialign to multiple source words, aiis theindex of the aligned word in the middle.1(3) If tiis unaligned, we inherit its affiliationfrom the closest aligned word, with prefer-ence given to the right.2An example of the NNJM context model for aChinese-English parallel sentence is given in Fig-ure 1.For all of our experiments we use n = 4 andm = 11.
It is clear that this model is effectivelyan (n+m)-gram LM, and a 15-gram LM would be1We arbitrarily round down.2We have found that the affiliation heuristic is robust tosmall differences, such as left vs. right preference.far too sparse for standard probability models suchas Kneser-Ney back-off (Kneser and Ney, 1995)or Maximum Entropy (Rosenfeld, 1996).
Fortu-nately, neural network language models are ableto elegantly scale up and take advantage of arbi-trarily large context sizes.2.1 Neural Network ArchitectureOur neural network architecture is almost identi-cal to the original feed-forward NNLM architec-ture described in Bengio et al (2003).The input vector is a 14-word context vector(3 target words, 11 source words), where eachword is mapped to a 192-dimensional vector us-ing a shared mapping layer.
We use two 512-dimensional hidden layers with tanh activationfunctions.
The output layer is a softmax over theentire output vocabulary.The input vocabulary contains 16,000 sourcewords and 16,000 target words, while the out-put vocabulary contains 32,000 target words.
Thevocabulary is selected by frequency-sorting thewords in the parallel training data.
Out-of-vocabulary words are mapped to their POS tag (orOOV, if POS is not available), and in this caseP (POSi|ti?1, ?
?
? )
is used directly without fur-ther normalization.
Out-of-bounds words are rep-resented with special tokens <src>, </src>,<trg>, </trg>.We chose these values for the hidden layer size,vocabulary size, and source window size becausethey seemed to work best on our data sets ?
largersizes did not improve results, while smaller sizesdegraded results.
Empirical comparisons are givenin Section 6.5.2.2 Neural Network TrainingThe training procedure is identical to that of anNNLM, except that the parallel corpus is usedinstead of a monolingual corpus.
Formally, weseek to maximize the log-likelihood of the train-ing data:L =?ilog(P (xi))where xiis the training sample, with one samplefor every target word in the parallel corpus.Optimization is performed using standard backpropagation with stochastic gradient ascent (Le-Cun et al, 1998).
Weights are randomly initial-ized in the range of [?0.05, 0.05].
We use an ini-tial learning rate of 10?3and a minibatch size of1371Figure 1: Context vector for target word ?the?, using a 3-word target history and a 5-word source window(i.e., n = 4 and m = 5).
Here, ?the?
inherits its affiliation from ?money?
because this is the first alignedword to its right.
The number in each box denotes the index of the word in the context vector.
Thisindexing must be consistent across samples, but the absolute ordering does not affect results.128.3At every epoch, which we define as 20,000minibatches, the likelihood of a validation set iscomputed.
If this likelihood is worse than the pre-vious epoch, the learning rate is multiplied by 0.5.The training is run for 40 epochs.
The trainingdata ranges from 10-30M words, depending on thecondition.
We perform a basic weight update withno L2 regularization or momentum.
However, wehave found it beneficial to clip each weight updateto the range of [-0.1, 0.1], to prevent the trainingfrom entering degenerate search spaces (Pascanuet al, 2012).Training is performed on a single Tesla K10GPU, with each epoch (128*20k = 2.6M samples)taking roughly 1100 seconds to run, resulting ina total training time of ?12 hours.
Decoding isperformed on a CPU.2.3 Self-Normalized Neural NetworkThe computational cost of NNLMs is a significantissue in decoding, and this cost is dominated bythe output softmax over the entire target vocabu-lary.
Even class-based approaches such as Le etal.
(2012) require a 2-20k shortlist vocabulary, andare therefore still quite costly.Here, our goal is to be able to use a fairlylarge vocabulary without word classes, and to sim-ply avoid computing the entire output layer at de-code time.4To do this, we present the noveltechnique of self-normalization, where the outputlayer scores are close to being probabilities with-out explicitly performing a softmax.Formally, we define the standard softmax log3We do not divide the gradient by the minibatch size.
Forthose who do, this is equivalent to using an initial learningrate of 10?3?
128 ?
10?1.4We are not concerned with speeding up training time, aswe already find GPU training time to be adequate.likelihood as:log(P (x)) = log(eUr(x)Z(x))= Ur(x)?
log(Z(x))Z(x) = ?|V |r?=1eUr?
(x)where x is the sample, U is the raw output layerscores, r is the output layer row corresponding tothe observed target word, and Z(x) is the softmaxnormalizer.If we could guarantee that log(Z(x)) were al-ways equal to 0 (i.e., Z(x) = 1) then at decodetime we would only have to compute row r of theoutput layer instead of the whole matrix.
Whilewe cannot train a neural network with this guaran-tee, we can explicitly encourage the log-softmaxnormalizer to be as close to 0 as possible by aug-menting our training objective function:L =?i[log(P (xi))?
?(log(Z(xi))?
0)2]=?i[log(P (xi))?
?
log2(Z(xi))]In this case, the output layer bias weights areinitialized to log(1/|V |), so that the initial net-work is self-normalized.
At decode time, we sim-ply use Ur(x) as the feature score, rather thanlog(P (x)).
For our NNJM architecture, self-normalization increases the lookup speed duringdecoding by a factor of ?15x.Table 1 shows the neural network training re-sults with various values of the free parameter?.
In all subsequent MT experiments, we use?
= 10?1.We should note that Vaswani et al (2013) im-plements a method called Noise Contrastive Es-timation (NCE) that is also used to train self-normalized NNLMs.
Although NCE results infaster training time, it has the downside that there1372Arabic BOLT Val?
log(P (x)) | log(Z(x))|0 ?1.82 5.0210?2?1.81 1.3510?1?1.83 0.681 ?1.91 0.28Table 1: Comparison of neural network likelihoodfor various ?
values.
log(P (x)) is the averagelog-likelihood on a held-out set.
| log(Z(x))| isthe mean error in log-likelihood when using Ur(x)directly instead of the true softmax probabilitylog(P (x)).
Note that ?
= 0 is equivalent to thestandard neural network objective function.is no mechanism to control the degree of self-normalization.
By contrast, our ?
parameter al-lows us to carefully choose the optimal trade-offbetween neural network accuracy and mean self-normalization error.
In future work, we will thor-oughly compare self-normalization vs. NCE.2.4 Pre-Computing the Hidden LayerAlthough self-normalization significantly im-proves the speed of NNJM lookups, the modelis still several orders of magnitude slower than aback-off LM.
Here, we present a ?trick?
for pre-computing the first hidden layer, which further in-creases the speed of NNJM lookups by a factor of1,000x.Note that this technique only results in a signif-icant speedup for self-normalized, feed-forward,NNLM-style networks with one hidden layer.
Wedemonstrate in Section 6.6 that using one hiddenlayer instead of two has minimal effect on BLEU.For the neural network described in Section 2.1,computing the first hidden layer requires mul-tiplying a 2689-dimensional input vector5witha 2689 ?
512 dimensional hidden layer matrix.However, note that there are only 3 possible posi-tions for each target word, and 11 for each sourceword.
Therefore, for every word in the vocabu-lary, and for each position, we can pre-computethe dot product between the word embedding andthe first hidden layer.
These are computed offlineand stored in a lookup table, which is <500MB insize.Computing the first hidden layer now only re-quires 15 scalar additions for each of the 512hidden rows ?
one for each word in the input52689 = 14 words ?
192 dimensions + 1 biasvector, plus the bias.
This can be reduced tojust 5 scalar additions by pre-summing each 11-word source window when starting a test sen-tence.
If our neural network has only one hid-den layer and is self-normalized, the only remain-ing computation is 512 calls to tanh() and a sin-gle 513-dimensional dot product for the final out-put score.6Thus, only ?3500 arithmetic opera-tions are required per n-gram lookup, comparedto ?2.8M for self-normalized NNJM without pre-computation, and ?35M for the standard NNJM.7Neural Network SpeedCondition lookups/sec sec/wordStandard 110 10.9+ Self-Norm 1500 0.8+ Pre-Computation 1,430,000 0.0008Table 2: Speed of the neural network computa-tion on a single CPU thread.
?lookups/sec?
is thenumber of unique n-gram probabilities that can becomputed per second.
?sec/word?
is the amortizedcost of unique NNJM lookups in decoding, persource word.Table 2 shows the speed of self-normalizationand pre-computation for the NNJM.
The decodingcost is based on a measurement of ?1200 uniqueNNJM lookups per source word for our Arabic-English system.8By combining self-normalization and pre-computation, we can achieve a speed of 1.4Mlookups/second, which is on par with fast back-off LM implementations (Tanaka et al, 2013).We demonstrate in Section 6.6 that using the self-normalized/pre-computed NNJM results in onlya very small BLEU degradation compared to thestandard NNJM.3 Decoding with the NNJMBecause our NNJM is fundamentally an n-gramNNLM with additional source context, it can eas-ily be integrated into any SMT decoder.
In thissection, we describe the considerations that mustbe taken when integrating the NNJM into a hierar-chical decoder.6tanh() is implemented using a lookup table.73500 ?
5?
512 + 2?
513; 2.8M ?
2?
2689?
512 +2 ?
513; 35M ?
2 ?
2689 ?
512 + 2 ?
513 ?
32000.
Forthe sake of a fair comparison, these all use one hidden layer.A second hidden layer adds 0.5M floating point operations.8This does not include the cost of duplicate lookupswithin the same test sentence, which are cached.13733.1 Hierarchical ParsingWhen performing hierarchical decoding with ann-gram LM, the leftmost and rightmost n ?
1words from each constituent must be stored in thestate space.
Here, we extend the state space toalso include the index of the affiliated source wordfor these edge words.
This does not noticeably in-crease the search space.
We also train a separatelower-order n-gram model, which is necessary tocompute estimate scores during hierarchical de-coding.3.2 Affiliation HeuristicFor aligned target words, the normal affiliationheuristic can be used, since the word alignmentis available within the rule.
For unaligned words,the normal heuristic can also be used, except whenthe word is on the edge of a rule, because then thetarget neighbor words are not necessarily known.In this case, we infer the affiliation from the rulestructure.
Specifically, if unaligned target word tis on the right edge of an arc that covers sourcespan [si, sj], we simply say that t is affiliated withsource word sj.
If t is on the left edge of the arc,we say it is affiliated with si.4 Model VariationsRecall that our NNJM feature can be describedwith the following probability:?|T |i=1P (ti|ti?1, ti?2, ?
?
?
, sai, sai?1, sai+1, ?
?
?
)This formulation lends itself to several naturalvariations.
In particular, we can reverse the trans-lation direction of the languages, as well as the di-rection of the language model.We denote our original formulation as a source-to-target, left-to-right model (S2T/L2R).
We cantrain three variations using target-to-source (T2S)and right-to-left (R2L) models:S2T/R2L?|T |i=1P (ti|ti+1, ti+2, ?
?
?
, sai, sai?1, sai+1, ?
?
?
)T2S/L2R?|S|i=1P (si|si?1, si?2, ?
?
?
, ta?i, ta?i?1, ta?i+1, ?
?
?
)T2S/R2L?|S|i=1P (si|si+1, si+2, ?
?
?
, ta?i, ta?i?1, ta?i+1, ?
?
?
)where a?iis the target-to-source affiliation, de-fined analogously to ai.The T2S variations cannot be used in decodingdue to the large target context required, and arethus only used in k-best rescoring.
The S2T/R2Lvariant could be used in decoding, but we have notfound this beneficial, so we only use it in rescor-ing.4.1 Neural Network Lexical TranslationModel (NNLTM)One issue with the S2T NNJM is that the prob-ability is computed over every target word, so itdoes not explicitly model NULL-aligned sourcewords.
In order to assign a probability to everysource word during decoding, we also train a neu-ral network lexical translation model (NNLMT).Here, the input context is the 11-word sourcewindow centered at si, and the output is the tar-get token tsiwhich sialigns to.
The probabil-ity is computed over every source word in the in-put sentence.
We treat NULL as a normal targetword, and if a source word aligns to multiple targetwords, it is treated as a single concatenated token.Formally, the probability model is:?|S|i=1P (tsi|si, si?1, si+1, ?
?
?
)This model is trained and evaluated like ourNNJM.
It is easy and computationally inexpensiveto use this model in decoding, since only one neu-ral network computation must be made for eachsource word.In rescoring, we also use a T2S NNLTM modelcomputed over every target word:?|T |i=1P (sti|ti, ti?1, ti+1, ?
?
?
)5 MT SystemIn this section, we describe the MT system used inour experiments.5.1 MT DecoderWe use a state-of-the-art string-to-dependency hi-erarchical decoder (Shen et al, 2010).
Our base-line decoder contains a large and powerful set offeatures, which include:?
Forward and backward rule probabilities?
4-gram Kneser-Ney LM?
Dependency LM (Shen et al, 2010)?
Contextual lexical smoothing (Devlin, 2009)?
Length distribution (Shen et al, 2010)?
Trait features (Devlin and Matsoukas, 2012)?
Factored source syntax (Huang et al, 2013)?
7 sparse feature types, totaling 50k features(Chiang et al, 2009)?
LM adaptation (Snover et al, 2008)1374We also perform 1000-best rescoring with thefollowing features:?
5-gram Kneser-Ney LM?
Recurrent neural network language model(RNNLM) (Mikolov et al, 2010)Although we consider the RNNLM to be partof our baseline, we give it special treatment in theresults section because we would expect it to havethe highest overlap with our NNJM.5.2 Training and OptimizationFor Arabic word tokenization, we use the MADA-ARZ tokenizer (Habash et al, 2013) for the BOLTcondition, and the Sakhr9tokenizer for the NISTcondition.
For Chinese tokenization, we use a sim-ple longest-match-first lexicon-based approach.For word alignment, we align all of the train-ing data with both GIZA++ (Och and Ney, 2003)and NILE (Riesa et al, 2011), and concatenate thecorpora together for rule extraction.For MT feature weight optimization, we useiterative k-best optimization with an Expected-BLEU objective function (Rosti et al, 2010).6 Experimental ResultsWe present MT primary results on Arabic-Englishand Chinese-English for the NIST OpenMT12 andDARPA BOLT conditions.
We also present a setof auxiliary results in order to further analyze ourfeatures.6.1 NIST OpenMT12 ResultsOur NIST system is fully compatible with theOpenMT12 constrained track, which consists of10M words of high-quality parallel training forArabic, and 25M words for Chinese.10TheKneser-Ney LM is trained on 5B words of datafrom English GigaWord.
For test, we usethe ?Arabic-To-English Original Progress Test?
(1378 segments) and ?Chinese-to-English Orig-inal Progress Test + OpenMT12 Current Test?
(2190 segments), which consists of a mix ofnewswire and web data.11All test segments have4 references.
Our tuning set contains 5000 seg-ments, and is a mix of the MT02-05 eval set aswell as held-out parallel training.9http://www.sakhr.com10We also make weak use of 30M-100M words of UN data+ ISI comparable corpora, but this data provides almost nobenefit.11http://www.nist.gov/itl/iad/mig/openmt12results.cfmNIST MT12 TestAr-En Ch-EnBLEU BLEUOpenMT12 - 1st Place 49.5 32.6OpenMT12 - 2nd Place 47.5 32.2OpenMT12 - 3rd Place 47.4 30.8?
?
?
?
?
?
?
?
?OpenMT12 - 9th Place 44.0 27.0OpenMT12 - 10th Place 41.2 25.7Baseline (w/o RNNLM) 48.9 33.0Baseline (w/ RNNLM) 49.8 33.4+ S2T/L2R NNJM (Dec) 51.2 34.2+ S2T NNLTM (Dec) 52.0 34.2+ T2S NNLTM (Resc) 51.9 34.2+ S2T/R2L NNJM (Resc) 52.2 34.3+ T2S/L2R NNJM (Resc) 52.3 34.5+ T2S/R2L NNJM (Resc) 52.8 34.7?Simple Hier.?
Baseline 43.4 30.1+ S2T/L2R NNJM (Dec) 47.2 31.5+ S2T NNLTM (Dec) 48.5 31.8+ Other NNJMs (Resc) 49.7 32.2Table 3: Primary results on Arabic-English andChinese-English NIST MT12 Test Set.
The firstsection corresponds to the top and bottom rankedsystems from the evaluation, and are taken fromthe NIST website.
The second section correspondsto results on top of our strongest baseline.
Thethird section corresponds to results on top of asimpler baseline.
Within each section, each rowincludes all of the features from previous rows.BLEU scores are mixed-case.Results are shown in the second section of Ta-ble 3.
On Arabic-English, the primary S2T/L2RNNJM gains +1.4 BLEU on top of our baseline,while the S2T NNLTM gains another +0.8, andthe directional variations gain +0.8 BLEU more.This leads to a total improvement of +3.0 BLEUfrom the NNJM and its variations.
Consideringthat our baseline is already +0.3 BLEU better thanthe 1st place result of MT12 and contains a strongRNNLM, we consider this to be quite an extraor-dinary improvement.12For the Chinese-English condition, there is animprovement of +0.8 BLEU from the primaryNNJM and +1.3 BLEU overall.
Here, the base-line system is already +0.8 BLEU better than the12Note that the official 1st place OpenMT12 result was ourown system, so we can assure that these comparisons are ac-curate.1375best MT12 system.
The smaller improvement onChinese-English compared to Arabic-English isconsistent with the behavior of our baseline fea-tures, as we show in the next section.6.2 ?Simple Hierarchical?
NIST ResultsThe baseline used in the last section is a highly-engineered research system, which uses a widearray of features that were refined over a num-ber of years, and some of which require linguis-tic resources.
Because of this, the baseline BLEUscores are much higher than a typical MT system?
especially a real-time, production engine whichmust support many language pairs.Therefore, we also present results using asimpler version of our decoder which emulatesChiang?s original Hiero implementation (Chiang,2007).
Specifically, this means that we don?tuse dependency-based rule extraction, and our de-coder only contains the following MT features: (1)rule probabilities, (2) n-gram Kneser-Ney LM, (3)lexical smoothing, (4) target word count, (5) con-cat rule penalty.Results are shown in the third section of Table 3.The ?Simple Hierarchical?
Arabic-English systemis -6.4 BLEU worse than our strong baseline, andwould have ranked 10th place out of 11 systemsin the evaluation.
When the NNJM features areadded to this system, we see an improvement of+6.3 BLEU, which would have ranked 1st place inthe evaluation.Effectively, this means that for Arabic-English,the NNJM features are equivalent to the combinedimprovements from the string-to-dependencymodel plus all of the features listed in Section 5.1.For Chinese-English, the ?Simple Hierarchical?system only degrades by -3.2 BLEU comparedto our strongest baseline, and the NNJM featuresproduce a gain of +2.1 BLEU on top of that.6.3 BOLT Web Forum ResultsDARPA BOLT is a major research project with thegoal of improving translation of informal, dialec-tical Arabic and Chinese into English.
The BOLTdomain presented here is ?web forum,?
which wascrawled from various Chinese and Egyptian Inter-net forums by LDC.
The BOLT parallel trainingconsists of all of the high-quality NIST training,plus an additional 3 million words of translatedforum data provided by LDC.
The tuning and testsets consist of roughly 5000 segments each, with2 references for Arabic and 3 for Chinese.Results are shown in Table 4.
The baseline hereuses the same feature set as the strong NIST sys-tem.
On Arabic, the total gain is +2.6 BLEU,while on Chinese, the gain is +1.3 BLEU.BOLT TestAr-En Ch-EnBLEU BLEUBaseline (w/o RNNLM) 40.2 30.6Baseline (w/ RNNLM) 41.3 30.9+ S2T/L2R NNJM (Dec) 42.9 31.9+ S2T NNLTM (Dec) 43.2 31.9+ Other NNJMs (Resc) 43.9 32.2Table 4: Primary results on Arabic-English andChinese-English BOLT Web Forum.
Each rowincludes the aggregate features from all previousrows.6.4 Effect of k-best Rescoring OnlyTable 5 shows performance when our S2T/L2RNNJM is used only in 1000-best rescoring, com-pared to decoding.
The primary purpose of this isas a comparison to Le et al (2012), whose modelcan only be used in k-best rescoring.BOLT TestAr-EnWithout WithRNNLM RNNLMBLEU BLEUBaseline 40.2 41.3S2T/L2R NNJM (Resc) 41.7 41.6S2T/L2R NNJM (Dec) 42.8 42.9Table 5: Comparison of our primary NNJM in de-coding vs. 1000-best rescoring.We can see that the rescoring-only NNJM per-forms very well when used on top of a baselinewithout an RNNLM (+1.5 BLEU), but the gain ontop of the RNNLM is very small (+0.3 BLEU).The gain from the decoding NNJM is large in bothcases (+2.6 BLEU w/o RNNLM, +1.6 BLEU w/RNNLM).
This demonstrates that the full power ofthe NNJM can only be harnessed when it is usedin decoding.
It is also interesting to see that theRNNLM is no longer beneficial when the NNJMis used.13766.5 Effect of Neural Network ConfigurationTable 6 shows results using the S2T/L2R NNJMwith various configurations.
We can see that re-ducing the source window size, layer size, or vo-cab size will all degrade results.
Increasing thesizes beyond the default NNJM has almost no ef-fect (102%).
Also note that the target-only NNLM(i.e., Source Window=0) only obtains 33% of theimprovements of the NNJM.BOLT TestAr-EnBLEU % Gain?Simple Hier.?
Baseline 33.8 -S2T/L2R NNJM (Dec) 38.4 100%Source Window=7 38.3 98%Source Window=5 38.2 96%Source Window=3 37.8 87%Source Window=0 35.3 33%Layers=384x768x768 38.5 102%Layers=192x512 38.1 93%Layers=128x128 37.1 72%Vocab=64,000 38.5 102%Vocab=16,000 38.1 93%Vocab=8,000 37.3 83%Activation=Rectified Lin.
38.5 102%Activation=Linear 37.3 76%Table 6: Results with different neural net-work architectures.
The ?default?
NNJM inthe second row uses these parameters: SW=11,L=192x512x512, V=32,000, A=tanh.
All mod-els use a 3-word target history (i.e., 4-gram LM).?Layers?
refers to the size of the word embeddingfollowed by the hidden layers.
?Vocab?
refers tothe size of the input and output vocabularies.
?%Gain?
is the BLEU gain over the baseline relativeto the default NNJM.6.6 Effect of SpeedupsAll previous results use a self-normalized neuralnetwork with two hidden layers.
In Table 7, wecompare this to using a standard network (withtwo hidden layers), as well as a pre-computed neu-ral network.13The ?Simple Hierarchical?
base-line is used here because it more closely approx-imates a real-time MT engine.
For the sake ofspeed, these experiments only use the S2T/L2RNNJM+S2T NNLTM.13The difference in score for self-normalized vs. pre-computed is entirely due to two vs. one hidden layers.Each result from Table 7 corresponds to a rowin Table 2 of Section 2.4.
We can see that go-ing from the standard model to the pre-computedmodel only reduces the BLEU improvement from+6.4 to +6.1, while increasing the NNJM lookupspeed by a factor of 10,000x.BOLT TestAr-EnBLEU Gain?Simple Hier.?
Baseline 33.8 -Standard NNJM 40.2 +6.4Self-Norm NNJM 40.1 +6.3Pre-Computed NNJM 39.9 +6.1Table 7: Results for the standard NNs vs. self-normalized NNs vs. pre-computed NNs.In Table 2 we showed that the cost of uniquelookups for the pre-computed NNJM is only?0.001 seconds per source word.
This does notinclude the cost of n-gram creation or cachedlookups, which amount to ?0.03 seconds persource word in our current implementation.14However, the n-grams created for the NNJM canbe shared with the Kneser-Ney LM, which reducesthe cost of that feature.
Thus, the total cost in-crease of using the NNJM+NNLTM features indecoding is only ?0.01 seconds per source word.In future work we will provide more detailedanalysis regarding the usability of the NNJM in alow-latency, high-throughput MT engine.7 Related WorkAlthough there has been a substantial amount ofpast work in lexicalized joint models (Marino etal., 2006; Crego and Yvon, 2010), nearly all ofthese papers have used older statistical techniquessuch as Kneser-Ney or Maximum Entropy.
How-ever, not only are these techniques intractable totrain with high-order context vectors, they alsolack the neural network?s ability to semanticallygeneralize (Mikolov et al, 2013) and learn non-linear relationships.A number of recent papers have proposed meth-ods for creating neural network translation/jointmodels, but nearly all of these works have ob-tained much smaller BLEU improvements thanours.
For each related paper, we will briefly con-14In our decoder, roughly 95% of NNJM n-gram lookupswithin the same sentence are duplicates.1377trast their methodology with our own and summa-rize their BLEU improvements using scores takendirectly from the cited paper.Auli et al (2013) use a fixed continuous-spacesource representation, obtained from LDA (Bleiet al, 2003) or a source-only NNLM.
Also, theirmodel is recurrent, so it cannot be used in decod-ing.
They obtain +0.2 BLEU improvement on topof a target-only NNLM (25.6 vs. 25.8).Schwenk (2012) predicts an entire target phraseat a time, rather than a word at a time.
He obtains+0.3 BLEU improvement (24.8 vs. 25.1).Zou et al (2013) estimate context-free bilinguallexical similarity scores, rather than using a largecontext.
They obtain an +0.5 BLEU improvementon Chinese-English (30.0 vs. 30.5).Kalchbrenner and Blunsom (2013) implementa convolutional recurrent NNJM.
They score a1000-best list using only their model and are ableto achieve the same BLEU as using all 12 standardMT features (21.8 vs 21.7).
However, additive re-sults are not presented.The most similar work that we know of is Le etal.
(2012).
Le?s basic procedure is to re-order thesource to match the linear order of the target, andthen segment the hypothesis into minimal bilin-gual phrase pairs.
Then, he predicts each targetword given the previous bilingual phrases.
How-ever, Le?s formulation could only be used in k-best rescoring, since it requires long-distance re-ordering and a large target context.Le?s model does obtain an impressive +1.7BLEU gain on top of a baseline without an NNLM(25.8 vs. 27.5).
However, when compared tothe strongest baseline which includes an NNLM,Le?s best models (S2T + T2S) only obtain an +0.6BLEU improvement (26.9 vs. 27.5).
This is con-sistent with our rescoring-only result, which indi-cates that k-best rescoring is too shallow to takeadvantage of the power of a joint model.Le?s model also uses minimal phrases ratherthan being purely lexicalized, which has two maindownsides: (a) a number of complex, hand-craftedheuristics are required to define phrase boundaries,which may not transfer well to new languages, (b)the effective vocabulary size is much larger, whichsubstantially increases data sparsity issues.We should note that our best results use six sep-arate models, whereas all previous work only usesone or two models.
However, we have demon-strated that we can obtain 50%-80% of the to-tal improvement with only one model (S2T/L2RNNJM), and 70%-90% with only two models(S2T/L2R NNJM + S2T NNLTM).
Thus, the oneand two-model conditions still significantly out-perform any past work.8 DiscussionWe have described a novel formulation for a neuralnetwork-based machine translation joint model,along with several simple variations of this model.When used as MT decoding features, these modelsare able to produce a gain of +3.0 BLEU on top ofa very strong and feature-rich baseline, as well asa +6.3 BLEU gain on top of a simpler system.Our model is remarkably simple ?
it requires nolinguistic resources, no feature engineering, andonly a handful of hyper-parameters.
It also has noreliance on potentially fragile outside algorithms,such as unsupervised word clustering.
We con-sider the simplicity to be a major advantage.
Notonly does this suggest that it will generalize well tonew language pairs and domains, but it also sug-gests that it will be straightforward for others toreplicate these results.Overall, we believe that the following factors setus apart from past work and allowed us to obtainsuch significant improvements:1.
The ability to use the NNJM in decodingrather than rescoring.2.
The use of a large bilingual context vector,which is provided to the neural network in?raw?
form, rather than as the output of someother algorithm.3.
The fact that the model is purely lexicalized,which avoids both data sparsity and imple-mentation complexity.4.
The large size of the network architecture.5.
The directional variation models.One of the biggest goals of this work is to quellany remaining doubts about the utility of neuralnetworks in machine translation.
We believe thatthere are large areas of research yet to be explored.For example, creating a new type of decoder cen-tered around a purely lexicalized neural networkmodel.
Our short term ideas include using moreinteresting types of context in our input vector(such as source syntax), or using the NNJM tomodel syntactic/semantic structure of the target.1378ReferencesMichael Auli, Michel Galley, Chris Quirk, and Geof-frey Zweig.
2013.
Joint language and translationmodeling with recurrent neural networks.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1044?1054, Seattle, Washington, USA, October.
Associa-tion for Computational Linguistics.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022, March.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine transla-tion.
In HLT-NAACL, pages 218?226.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Josep Maria Crego and Franc?ois Yvon.
2010.
Factoredbilingual n-gram language models for statistical ma-chine translation.
Machine Translation, 24(2):159?175.Jacob Devlin and Spyros Matsoukas.
2012.
Trait-based hypothesis selection for machine translation.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL HLT ?12, pages 528?532, Stroudsburg, PA,USA.
Association for Computational Linguistics.Jacob Devlin.
2009.
Lexical features for statisticalmachine translation.
Master?s thesis, University ofMaryland.Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-kander, and Nadi Tomeh.
2013.
Morphologicalanalysis and disambiguation for dialectal arabic.
InHLT-NAACL, pages 426?432.Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.2013.
Factored soft source syntactic constraints forhierarchical machine translation.
In EMNLP, pages556?566.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.Reinhard Kneser and Hermann Ney.
1995.
Im-proved backing-off for m-gram language modeling.In Acoustics, Speech, and Signal Processing, 1995.ICASSP-95., 1995 International Conference on, vol-ume 1, pages 181?184.
IEEE.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous space translation models withneural networks.
In Proceedings of the 2012 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, NAACL HLT ?12, pages 39?48, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Yann LeCun, L?eon Bottou, Genevieve B Orr, andKlaus-Robert M?uller.
1998.
Efficient backprop.
InNeural networks: Tricks of the trade, pages 9?50.Springer.Jos?e B Marino, Rafael E Banchs, Josep M Crego, Adri`aDe Gispert, Patrik Lambert, Jos?e AR Fonollosa, andMarta R Costa-Juss`a.
2006.
N-gram-based machinetranslation.
Computational Linguistics, 32(4):527?549.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock?y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH, pages 1045?1048.Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous spaceword representations.
In HLT-NAACL, pages 746?751.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.2012.
On the difficulty of training recurrent neuralnetworks.
arXiv preprint arXiv:1211.5063.Jason Riesa, Ann Irvine, and Daniel Marcu.
2011.Feature-rich language-independent syntax-basedalignment for statistical machine translation.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?11,pages 497?507, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Ronald Rosenfeld.
1996.
A maximum entropy ap-proach to adaptive statistical language modeling.Computer, Speech and Language, 10:187?228.Antti Rosti, Bing Zhang, Spyros Matsoukas, andRich Schwartz.
2010.
BBN system descrip-tion for WMT10 system combination task.
InWMT/MetricsMATR, pages 321?326.Holger Schwenk.
2010.
Continuous-space languagemodels for statistical machine translation.
PragueBull.
Math.
Linguistics, 93:137?146.Holger Schwenk.
2012.
Continuous space translationmodels for phrase-based statistical machine transla-tion.
In COLING (Posters), pages 1071?1080.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2010.String-to-dependency statistical machine transla-tion.
Computational Linguistics, 36(4):649?671,December.1379Matthew Snover, Bonnie Dorr, and Richard Schwartz.2008.
Language and translation model adaptationusing comparable corpora.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?08, pages 857?866,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Makoto Tanaka, Yasuhara Toru, Jun-ya Yamamoto, andMikio Norimatsu.
2013.
An efficient languagemodel using double-array structures.Ashish Vaswani, Yinggong Zhao, Victoria Fossum,and David Chiang.
2013.
Decoding with large-scale neural language models improves translation.In Proceedings of the 2013 Conference on Em-pirical Methods in Natural Language Processing,pages 1387?1392, Seattle, Washington, USA, Oc-tober.
Association for Computational Linguistics.Will Y Zou, Richard Socher, Daniel Cer, and Christo-pher D Manning.
2013.
Bilingual word embeddingsfor phrase-based machine translation.
In Proceed-ings of the 2013 Conference on Empirical Methodsin Natural Language Processing, pages 1393?1398.1380
