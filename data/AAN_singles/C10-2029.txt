Coling 2010: Poster Volume, pages 250?258,Beijing, August 2010Topic models for meaning similarity in contextGeorgiana DinuDept.
of Computational LinguisticsSaarland Universitydinu@coli.uni-sb.deMirella LapataSchool of InformaticsUniversity of Edinburghmlap@inf.ed.ac.ukAbstractRecent work on distributional methods forsimilarity focuses on using the contextin which a target word occurs to derivecontext-sensitive similarity computations.In this paper we present a method for com-puting similarity which builds vector rep-resentations for words in context by mod-eling senses as latent variables in a largecorpus.
We apply this to the Lexical Sub-stitution Task and we show that our modelsignificantly outperforms typical distribu-tional methods.1 IntroductionDistributional methods for word similarity ((Lan-dauer and Dumais, 1997), (Schuetze, 1998)) arebased on co-occurrence statistics extracted fromlarge amounts of text.
Typically, each word isassigned a representation as a point in a high-dimensional space, where the dimensions rep-resent contextual features such as co-occurringwords.
Following this, meaning relatednessscores are computed by using various similaritymeasures on the vector representations.One of the major issues that all distributionalmethods have to face is sense ambiguity.
Sincevector representations reflect mixtures of uses ad-ditional methods have to be employed in order tocapture specific meanings of a word in context.Consider the occurrence of verb shed in the fol-lowing SemEval 2007 Lexical Substitution Task(McCarthy and Navigli, 2007) example:Cats in the latent phase only have the virus internally ,but feel normal and do not shed the virus to other cats andthe environment .Human participants in this task provided wordssuch as transmit and spread as good substitutesfor shed in this context, however a vector spacerepresentation of shed will not capture this infre-quent sense.For these reasons, recent work on distributionalmethods for similarity such as (Mitchell and La-pata, 2008) (Erk and Pado?, 2008) (Thater et al,2009) focuses on using the context in which a tar-get word occurs to derive context-sensitive simi-larity computations.In this paper we present a method for comput-ing similarity which builds vector representationsfor words in context.
Most distributional methodsso far extract representations from large texts, andonly as a follow-on step they either 1) alter thesein order to reflect a disambiguated word (suchas (Erk and Pado?, 2008)) or 2) directly asses theappropriateness of a similarity judgment, given aspecific context (such as (Pantel et al, 2007)).
Ourapproach differs from this as we assume ambigu-ity of words at the, initial, acquisition step, by en-coding senses of words as a hidden variable in thetext we process.In this paper we focus on a particular distribu-tional representation inspired by (Lin and Pantel,2001a) and induce context-sensitive similarity be-tween phrases represented as paths in dependencygraphs.
It is inspired by recent work on topic mod-els and it deals with sense-ambiguity in a naturalmanner by modeling senses as latent variables ina large corpus.
We apply this to the Lexical Sub-stitution Task and we show that our model outper-forms the (Lin and Pantel, 2001a) method by in-ducing context-appropriate similarity judgments.2502 Related workDiscovery of Inference Rules from Text (DIRT)A popular distributional method for meaning re-latedness is the DIRT algorithm for extracting in-ference rules (Lin and Pantel, 2001a).
In this al-gorithm a pattern is a noun-ending path in a de-pendency graph and the goal is to acquire pairs ofpatterns for which entailment holds (in at least onedirection) such as (X solve Y, X find solution to Y).The method can be seen a particular instanceof a vector space.
Each pattern is represented bythe sets of its left hand side (X) and right handside (Y) noun fillers in a large corpus.
Two pat-terns are compared in the X-filler space, and cor-respondingly in the Y-filler space by using the Linsimilarity measure:simLin(v, w) =?i?I(v)?I(w)(vi + wi)?i?I(v) vi +?l?I(w)wiwhere values in v and w are point-wise mutualinformation, and I(?)
gives the indices of positivevalues in a vector.The final similarity score between two patternsis obtained by multiplying the X and Y similarityscores.
Table 1 shows a fragment of a DIRT-likevector space... case problem ..(X solve Y, Y) .. 6.1 4.4 ..(X settle Y, Y) .. 5.2 5.9 ..Table 1: DIRT-like vector representation in the Y-fillerspace.
The values represent mutual information.Further on, this similarity method is used forthe task of paraphrasing.
A total set of patternsis extracted from a large corpus and each of themcan be paraphrased by returning its most similarpatterns, according to the similarity score.
Al-though relatively accurate1, it has been noted (Linand Pantel, 2001b) that the paraphrases extractedthis way reflect, as expected, various meanings,and that a context-sensitive representation wouldbe appropriate.1Precision is estimated to lie around 50% for the mostconfident paraphrasesContext-sensitive extensions of DIRT (Pantelet al, 2007) and (Basili et al, 2007) focus on mak-ing DIRT rules context-sensitive by attaching ap-propriate semantic classes to the X and Y slots ofan inference rule.
For this purpose, the initial stepin their methods is to acquire an inference ruledatabase, using the DIRT algorithm.
Followingthis, given an inference rule, they identify seman-tic classes for the X and Y fillers which make theapplication of the rule appropriate.
For this (Pan-tel et al, 2007) build a set of semantic classes us-ing WordNet in one case and CBC clustering al-gorithm in the other; for each rule, they use theoverlap of the fillers found in the input corpus asan indicator of the correct semantic classes.
Thesame idea is used in (Basili et al, 2007) where,this time, the X and Y fillers are clustered for eachrule individually; these nouns are clustered us-ing an LSA-vector representation extracted froma large corpus.
(Connor and Roth, 2007) take a slightly differ-ent approach as they attempt to classify the con-text of a rule as appropriate or not, again usingthe overlap of fillers as an indicator.
They allshow improvement over DIRT by evaluating onoccurrences of rules in context which are anno-tated as correct/incorrect by human participants.On a common data set (Pantel et al, 2007) and(Basili et al, 2007) achieve significant improve-ments over DIRT at 95% confidence level whenemploying the clustering methods.
(Szpektor etal., 2008) propose a general framework for thesemethods and show that some of these settings ob-tain significant (level 0.01) improvements over theDIRT algorithm on data derived from the ACE2005 event detection task.Related work on topic models Topic modelshave been previously used for semantic tasks.Work such as (Cai et al, 2007) or (Boyd-Graber etal., 2007) use the document-level topics extractedwith Latent Dirichlet Allocation (LDA) as indi-cators of meanings for word sense disambigua-tion.
More related to our work are (Brody andLapata, 2009) or (Toutanova and Johnson, 2008)who use LDA-based models which induce latentvariables from task-specific data rather than fromsimple documents.251(Brody and Lapata, 2009) apply such a modelfor word sense induction on a set of 35 targetnouns.
They assume senses as latent variables andcontext features as observations; unlike our modelthey induce local senses specific to every targetword by estimating separate models with the finalgoal of explicitly inducing word senses.
(Toutanova and Johnson, 2008) use an LDA-based model for semi-supervised part-of-speechtagging.
They build a word context model inwhich each token involves: generating a distri-bution over tags, sampling a tag, and finally gen-erating context words according to a tag-specificword distribution (context words are observa-tions).
Their model achieves highest performancewhen combined with a ambiguity class compo-nent which uses a dictionary for possible tags oftarget words.Both these papers show improvements overstate-of-the-art systems for their tasks.3 Generative model for similarity incontextWe develop a method for computing similarity ofpatterns in context, i.e.
patterns with instantiatedX and Y values.
We do not enhance the repre-sentation of an inference rule with sense (context-appropriateness) information but rather focus onthe task of assigning similarity scores to such pairsof instantiated patterns.
Unlike previous work, wedo not employ any other additional resources, in-vestigating this way whether structurally richer in-formation can be learned from the same input co-occurrence matrix as the original DIRT method.Our model, as well as the DIRT algorithm,uses context information extracted from largecorpora to learn similarities between patterns;however ideally we would like to learn contex-tual preferences (or, in general, some form ofsense-disambiguation) for these patterns.
This isachieved in our model by assuming an intermedi-ate layer consisting of meanings (senses): the con-text surrounding a pattern is indicative of mean-ings, and preference for some meanings gives thecharacterization of a pattern.For this we use a generative model inspiredby Latent Dirichlet Allocation (Blei et al, 2003)(Griffiths and Steyvers, 2004) which is success-X solve Ywe-X:122, country-X:89, government-X:82,it-X:69,..., problem-Y:1088, issue-Y:134,crisis-Y:99, dispute-Y:78,...Table 2: Fragments of the document associatedto X solve Y. we-X: 122 indicates that X solve Yoccurs 122 times with we as an X filler.fully employed for modeling collections of doc-uments and the underlying topics which occur inthem.
The statistical model is characterized by thefollowing distributions:wi|zi, ?zi Discrete(?zi)?z Dirichlet(?
)zi|?p Discrete(?p)?p Dirichlet(?
)?p is the distribution over meanings associatedto a pattern p and ?z is the distribution over wordsassociated to a meaning z.
The occurrence ofeach filler word wi with a pattern p, is then gener-ated by sampling 1) a meaning conditioned on themeaning distribution associated to p: zi|?p and 2)a word conditioned on the word distribution asso-ciated to the meaning zi: wi|zi, ?zi .
?p and ?z areassumed to be Dirichlet distributions with param-eters ?
and ?.The set of context words (X and Y fillers) oc-curring with a pattern p form the document (inLDA terms) associated to a pattern p. Table 2 listsa fragment of the document associated to patternX solve Y.
These are built simply by listing foreach pattern, occurrence counts with specific fillerwords.
Since we want our model to differentiatebetween X and Y fillers, words occurring as fillersare made disjoint by adding a corresponding suf-fix.The total set of such documents extracted froma large corpus is then used for estimating themodel.
We use Gibbs sampling2 and the resultis a set of samples from P (z|w) (i.e.
mean-ing assignments for each occurring filler word)from which ?p (pattern-meaning distributions)and ?z(meaning-word distributions) can be esti-mated.Our model has the advantage that, once these2http://gibbslda.sourceforge.net/252distributions are estimated, given a pattern p and acontext wn, in-context vector representations canbe built in a straightforward manner.Meaning representation in-context Let K bethe assumed number of meanings, (z1, ..., zK).We associate to a pattern in context (p,wn), theK-dimensional vector containing for each mean-ing zi (i : 1..K), the probability of zi, conditionedon pattern p and context word wn:vec(p, wn) = (P (z1|wn, p), ..., P (zK |wn, p))(1)where,P (zi|wn, p) =P (zi, p)P (wn|zi)?Ki=1P (zi, p)P (wn|zi)(2)This is the probability that wn is generated bymeaning zi conditioned on p, therefore, the proba-bility that pattern p has meaning zi in context wn,exactly the concept we want to model.Meaning representation out-of-context Wecan also associate to pattern p an out-of-contextvector representation: the K-dimensional vectorrepresenting its distribution over meanings:vec(p) = (P (z1|p), ..., P (zK |p)) (3)This can be seen as a dimensionality reductionmethod, since we bring vector representations to alower dimensional space over (ideally) meaning-ful concepts.From the generative model we obtain the de-sired distributions P (zi|p) = ?pi and P (wn|zi) =?zin .3Computing similarity between patterns Thesimilarity between patterns occurring with X andY filler-words is computed following (Lin andPantel, 2001a) by multiplying the similarities ob-tained separately in the X and Y spaces.
:sim((wX1, p1, wY 1)(wX2, p2, wY 2)) =sim(vec(p1, wX1), vec(p2, wX2))?sim(vec(p1, wY 1), vec(p2, wY 2))(4)3For similarity in context, we use the conditional P (zi|p)instead of the joint P (zi, p) which is computationally equiv-alent for the paraphrasing setting.we subj????
make dobj???
?statementwe subj????
give dobj???
?statement goodwe subj????
prepare dobj???
?statement badTable 3: Development set: good/bad substitutesfor we subj????
make dobj???
?statementOut-of-context similarity is defined in a straight-forward manner:sim(p1, p2) = sim(vec(p1, ), vec(p2)) (5)4 Evaluation setupIn this paper we evaluate our model oncomputing similarities between pairs of thetype (X, pattern, Y ), (X, pattern?, Y ) wheretwo different patterns are compared in identicalcontexts.
For this we use the Semeval LexicalSubstitution dataset, which requires human par-ticipants to provide substitutes for a set of targetwords occurring in different contexts.
This sec-tion describes the evaluation methodology for thisdata as well as the automatically generated dataset we use for development.Development set For finding good model pa-rameters, we use the SemCor corpus providingtext in which all content words are tagged withWordNet 1.6 senses.
We used this data in the fol-lowing manner: We parse the text using Stanfordparser and extract occurrences of triples (X, pat-tern, Y).
Given these triples we generate good andbad substitutes for them: the good substitutes aregenerated by replacing the words occurring in thepatterns with sense-appropriate synonyms, whilebad ones are obtained by substitution with syn-onyms corresponding to the rest of the senses (thewrong senses).
The synonyms are extracted fromWordNet 1.6 synsets using the sense annotationpresent in the text.For evaluation we feed the models pairs of in-stantiated patterns.
One of them is the originalphrase encountered in the data, and the other oneis a good/bad substitute for it.
Table 3 shows anexample of the data.We evaluate the output of a system by requir-ing that, for each instance, every good substituteis scored more similar to the original phrase than253every bad substitute.
This leads to an accuracyscore which can be compared against a randombaseline of 50%.The data set obtained is far from being a veryreliable resource for the task of lexical substitu-tion, however this method of generating data hasthe advantage of producing a large number of in-stances which can be easily acquired from anysense-annotated data set.
In our experiments weuse the Brown2 fragment from which we extractover 3000 instances of patterns in context.Lexical substitution task The Lexical Substitu-tion Task (McCarthy and Navigli, 2007) presents5 annotators with a set of target words, each indifferent context sentences.
The task requiresthe participants to provide appropriate substitutewords for each occurrence of the target words.We use this data similarly to (Erk and Pado?,2008) and (Thater et al, 2009) and for each targetword, we pool together all the substitutes givenfor all context sentences.
Similarly to the Sem-Cor data, we do not use the entire sentence as acontext as we extract only patterns containing tar-get words together with their X and Y fillers.
Themodels assign similarity scores to each candidateby comparing them to the pattern occurring in theoriginal sentence.
A ranked list of candidates isobtained which in turn is compared with the sub-stitutes provided by the participants.
Table 4 givesan example of this data set (for each substitute welist the number of participants providing it).To evaluate the performance of a model we em-ploy two similarity measures, which capture dif-ferent aspects of the task.
Kendall ?
rank coeffi-cient measures the correlation between two ranks;since the gold ranking is usually only a partial or-der, we use ?b which makes adjustments for ties.We employ a second evaluation measure: Gener-alized Average Precision (Kishida, 2005).
This isa measure inspired from information retrieval andhas been previously used for evaluating this task(Thater et al, 2009).
It evaluates a system on itsability to retrieve correct substitutes using the goldranking together with the associated confidencescores.
The confidence scores are in turn deter-mined by the number of people providing eachsubstitute.pattern human substitutesstudy subj????
shed dobj???
?light throw 3, reveal 2,shine 1cat subj????
shed dobj???
?virus spread 2, pass 2,transmit 2, emit 1Table 4: Lexical substitution data set: target verbshed5 Experiments5.1 Model selectionThe data we use to estimate our models is ex-tracted from a GigaWord fragment containing ap-proximately 100 million tokens.
We parse thetext with Stanford dependency parser to obtain de-pendency graphs from which we extract paths to-gether with counts of their left and right fillers.We extract paths containing at most four words,including the two noun anchors.
Furthermorewe impose a frequency threshold on patterns andwords, leading us to a collection of?80 000 paths,with filler nouns over a vocabulary of ?40 000words.We estimate a total number of 20 models.
Weset ?
= 0.01 as previous work (Wang et al, 2009)reports good results with this value.
For parame-ter ?
we test 4 settings: ?1 = 2K and ?4 = 50Kwhich are reported in the literature as good ((Por-teous et al, 2008) and (Griffiths and Steyvers,2004)), as well as 2 intermediate values: ?2 = 5Kand ?3 = 10K .
We test a set of 5 K values:{800, 1000, 1200, 1400, 1600}.
These are chosento be large since they represent the global set ofmeanings shared by all the patterns in the collec-tion.As vector similarity measure we test scalarproduct (sp), which in our model is interpretedas the probability that two patterns share a com-mon meaning.
Additionally we test cosine (cos)similarity and inverse Jensen-Shannon (JS) diver-gence, which is a popular measure for comparingprobability distributions:JSD(v, w) = 12KLD(v|m) +12KLD(w|m)with m = 12(v + w) and KLD the stan-dard Kullback-Leibler divergence: KLD(v|w) =?ivilog( viwi ).254We perform both in-context (using eq.
(4))as well as out-of-context computations (eq.
(5)).Similarly to previous work (Erk and Pado?, 2008),we observe that comparing a contextualized repre-sentation against a non-contextualized one bringssignificant improvements over comparing tworepresentations in context.
We assume this is spe-cific to the type of data we work with, in whichtwo patterns are compared in an identical context,rather than across different contexts; we thereforecompute context-sensitive similarities by contex-tualizing just the target word.Number of topics Although the parameterscover relatively large ranges the models performsurprisingly similar across different ?
and K val-ues, as well as across all three similarity measures.For sp similarity, the accuracy scores we obtainare in the range [56.5-59.5] with a average devi-ation from the mean of just 0.8%; similar figuresare obtained using the other similarity measures.Figure 1 plots the average of the accuracy scoresusing sp as similarity measure, across differentnumber of topics.
A small preference for higherK values is observed, all models performing con-sistently good at 1200, 1400 and 1600 topics.Figure 1: Average accuracy across the 5 K values.Mixture models This leads us to attempting avery simple mixture model, which computes thesimilarity score between two patterns as the aver-age similarity obtained across a number of mod-els.
For each ?
setting, we mix models across thethree best topic numbers: {1200, 1400, 1600}.
InFigure 2 we plot this mixture model together withthe three single ones, at each ?
value.
It can beFigure 2: Mixture model {1200, 1400, 1600}(bold) vs. the three individual models, across the4 ?
values.noticed that the mixture model improves over allthree single models for three out of the four ?
val-ues.In-context vs. out-of-context computationsFurther on we compare in-context versus out-of-context computations.
The similarity measuresexhibit significant differences in regard to this as-pect.
In Figure 3 we plot in-context vs. out-of-context computations using scalar product (left)and JS (right) with the mixture model previouslydefined, plotted at different ?
values.
For spin-context computations significantly outperformout-of-context ones and the two intermediate al-pha values seem to be the best.
However for JSsimilarity the out-of-context computations are sig-nificantly better and a clear preference for smaller?
values can be observed.Finally, on the test data, we use the followingmodels (where GMmixt/sing,sim stands for a mix-ture or single model with similarity measure sim):?
GMmixt,sp/cosmixt({1200, 1400, 1600}x{?2, ?3})?
GMmixt,jsmixt({1200, 1400, 1600}x{?1, ?2})?
GMsing,sp: (1600, ?2)?
GMsing,cos/js: (1200, ?1)The mixture models are build based on the ob-servations previously made while the single mod-255Model In-context Out-of-contextGMmixt,sp 59.89 58.68GMmixt,cos 59.50 58.67GMmixt,js 59.73 60.68GMsing,sp 59.48 58.86GMsing,cos 59.43 57.87GMsing,js 58.65 59.36Table 5: Accuracy results on development setels are the best performing ones, for each similar-ity measure.
The accuracy scores obtained withthese models are given in Table 5.
Mixture modelsgenerally outperform single ones and in-contextcomputations outperform out-of-context ones forsp and cos.
The best results on the developmentset are however achieved by out-of-context mod-els using JS as similarity measure.Figure 3: In-context (bold) vs. out-of-contextcomputations across the 4 ?
values using scalarproduct (left) and JS (right)5.2 ResultsTable 6 shows the results for the Lexical Substitu-tion data set.
We use the subset of the data con-taining sentences in which the target word is partof a syntactic path which is present in the total col-lection of patterns.
This leads to a set containing165 instances of patterns in context, most of thesecontaining target verbs.Since sp and cos measures perform very sim-ilarly we only list results with cosine similaritymeasure.
In addition to the models with settingsdetermined on the development set, we also testa very simple mixture model: GMmixt?all,sim.This simply averages over all 20 configurationsand its purpose is to investigate the necessity of acarefully selected mixture model.It can be noticed that all GM mixture mod-els outperform DIRT, which is reflected in bothModel ?b GAPRandom 0.0 34.91DIRT 14.53 48.06GMmixt,cos 22.35 52.04GMmixt,js 18.17 50.80GMmixt?all,cos 20.42 51.13GMmixt?all,js 19.03 51.15GMsing,cos 15.10 48.20GMsing,js 14.17 47.97Table 6: Results on Lexical Substitution datasimilarity measures.
Notably the very simplemodel which averages all the configurations im-plemented is surprisingly performant.
Using ran-domized significance testing we obtained thatGMmixt,cos is significantly better than DIRT at plevel 1e-03 on both GAP and ?b.
GMmixt?all,cosoutperforms DIRT at level 0.05.In terms of similarity measures, the observa-tions made on the development set hold, as forthe in-context computations cos and sp outper-form JS.
However, unlike on the developmentdata, the single models perform much worse thanthe mixture ones which can indicate that the de-velopment set is not perfectly suited for choosingmodel parameters.Out-of-context computations for all models andall similarity measures are significantly outper-formed, leading to scores in ranges [11-14] ?b and[45-48] GAP.In Table 7 we list the rankings produced bythree models for the target word shed in con-text virus obj???
shed prep????
to pobj????
cat.
As itcan be observed, the model performing context-sensitive computations GMmixt,cos-in-context re-turns a better ranking in comparison to theDIRTand GMmixt,cos-out-of-context models.6 ConclusionWe have addressed the task of computing meaningsimilarity in context using distributional methods.The specific representation we use follows (Linand Pantel, 2001a): we extract patterns (pathsin dependency trees which connect two nouns)and we use the co-occurrence with these nounsto build high-dimensional vectors.
Using this data256virus obj???
shed prep????
to pobj????
catGMmixt,cos GMmixt,cos DIRT GOLDin-context out-of-contextlose lose drop pass 2drop drop lose spread 2transmit relinquish give transmit 2spread reveal transmitpass pass spreadrelinquish throw revealreveal spread relinquishthrow transmit throwgive give passTable 7: Ranks returned for virus obj???
shed prep????
to pobj????
catwe develop a principled method to induce context-sensitive representations by modeling the mean-ing of a pattern as a latent variable in the inputcorpus.
We apply this model to the task of Lex-ical Substitution and we show it allows the com-putation of context-sensitive similarities; it signif-icantly outperforms the original method, while us-ing the exact same input data.In future work, we plan to use our model forgenerating paraphrases for patterns occurring incontext, a scenario closer to real applications thanout-of-context paraphrasing.Finally, a formulation of our model in a typicalbag-of-words semantic space for word similaritycan be employed in a wider range of applicationsand will allow comparison with other methods forbuilding context-sensitive vector representations.7 AcknowledgmentsThis work was partially supported by DFG (IRTG715).ReferencesBasili, Roberto, Diego De Cao, Paolo Marocco, andMarco Pennacchiotti.
2007.
Learning selectionalpreferences for entailment or paraphrasing rules.
InIn Proceedings of RANLP 2007, Borovets, Bulgaria.Blei, David M., Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022.Boyd-Graber, Jordan, David M. Blei, and Xiaojin Zhu.2007.
A topic model for word sense disambigua-tion.
In Empirical Methods in Natural LanguageProcessing.Brody, Samuel and Mirella Lapata.
2009.
Bayesianword sense induction.
In EACL ?09: Proceedingsof the 12th Conference of the European Chapterof the Association for Computational Linguistics,pages 103?111, Morristown, NJ, USA.
Associationfor Computational Linguistics.Cai, Jun Fu, Wee Sun Lee, and Yee Whye Teh.
2007.Nus-ml:improving word sense disambiguation us-ing topic features.
In Proceedings of the FourthInternational Workshop on Semantic Evaluations(SemEval-2007), pages 249?252, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Connor, Michael and Dan Roth.
2007.
Context sensi-tive paraphrasing with a global unsupervised classi-fier.
In ECML ?07: Proceedings of the 18th Euro-pean conference on Machine Learning, pages 104?115, Berlin, Heidelberg.
Springer-Verlag.Erk, Katrin and Sabastian Pado?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of EMNLP 2008, Waikiki, Honolulu,Hawaii.Griffiths, T. L. and M. Steyvers.
2004.
Finding scien-tific topics.
Proceedings of the National Academyof Sciences, 101(Suppl.
1):5228?5235, April.Kishida, Kazuaki.
2005.
Property of average precisionand its generalization: An examination of evaluationindicator for information retrieval experiments.
NIITechnical Report.Landauer, Thomas K. and Susan T. Dumais.
1997.A solution to plato?s problem: The latent seman-tic analysis theory of acquisition, induction and rep-resentation of knowledge.
Psychological Review,104(2):211?240.Lin, Dekang and Patrick Pantel.
2001a.
DIRT ?
Dis-covery of Inference Rules from Text.
In Proceed-ings of the ACM Conference on Knowledge Discov-ery and Data Mining (KDD-01), San Francisco, CA.257Lin, Dekang and Patrick Pantel.
2001b.
Discovery ofinference rules for question-answering.
Nat.
Lang.Eng., 7(4):343?360.McCarthy, D. and R. Navigli.
2007.
SemEval-2007Task 10: English Lexical Substitution Task.
In Pro-ceedings of SemEval, pages 48?53, Prague.Mitchell, Jeff and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08: HLT, pages 236?244, Columbus, Ohio.Pantel, Patrick, Rahul Bhagat, Bonaventura Coppola,Timothy Chklovski, and Eduard Hovy.
2007.
ISP:Learning inferential selectional preferences.
In Hu-man Language Technologies 2007: The Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, Rochester, NewYork.Porteous, Ian, David Newman, Alexander Ihler, ArthurAsuncion, Padhraic Smyth, and Max Welling.2008.
Fast collapsed gibbs sampling for latentdirichlet alocation.
In KDD ?08: Proceeding ofthe 14th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 569?577, New York, NY, USA.
ACM.Schuetze, Hinrich.
1998.
Automatic word sense dis-crimination.
Journal of Computational Linguistics,24:97?123.Szpektor, Idan, Ido Dagan, Roy Bar-Haim, and JacobGoldberger.
2008.
Contextual preferences.
In Pro-ceedings of ACL-08: HLT, pages 683?691, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Thater, Stefan, Georgiana Dinu, and Manfred Pinkal.2009.
Ranking paraphrases in context.
In Proceed-ings of TextInfer ACL 2009.Toutanova, Kristina and Mark Johnson.
2008.
Abayesian lda-based model for semi-supervised part-of-speech tagging.
In Platt, J.C., D. Koller,Y.
Singer, and S. Roweis, editors, Advances inNeural Information Processing Systems 20, pages1521?1528.
MIT Press, Cambridge, MA.Wang, Yi, Hongjie Bai, Matt Stanton, Wen-Yen Chen,and Edward Y. Chang.
2009.
Plda: Parallel latentdirichlet alocation for large-scale applications.
InProc.
of 5th International Conference on Algorith-mic Aspects in Information and Management.258
