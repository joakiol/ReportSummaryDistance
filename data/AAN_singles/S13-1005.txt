Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 44?52, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsUMBC EBIQUITY-CORE: Semantic Textual Similarity SystemsLushan Han, Abhay Kashyap, Tim FininComputer Science andElectrical EngineeringUniversity of Maryland, Baltimore CountyBaltimore MD 21250{lushan1,abhay1,finin}@umbc.eduJames Mayfield and Jonathan WeeseHuman Language TechnologyCenter of ExcellenceJohns Hopkins UniversityBaltimore MD 21211mayfield@jhu.edu, jonny@cs.jhu.eduAbstractWe describe three semantic text similaritysystems developed for the *SEM 2013 STSshared task and the results of the correspond-ing three runs.
All of them shared a word sim-ilarity feature that combined LSA word sim-ilarity and WordNet knowledge.
The first,which achieved the best mean score of the 89submitted runs, used a simple term alignmentalgorithm augmented with penalty terms.
Theother two runs, ranked second and fourth, usedsupport vector regression models to combinelarger sets of features.1 IntroductionMeasuring semantic text similarity has been a re-search subject in natural language processing, infor-mation retrieval and artificial intelligence for manyyears.
Previous efforts have focused on compar-ing two long texts (e.g., for document classification)or a short text with a long text (e.g., Web search),but there are a growing number of tasks requiringcomputing the semantic similarity between two sen-tences or other short text sequences.
They includeparaphrase recognition (Dolan et al 2004), Twittertweets search (Sriram et al 2010), image retrievalby captions (Coelho et al 2004), query reformula-tion (Metzler et al 2007), automatic machine trans-lation evaluation (Kauchak and Barzilay, 2006) andschema matching (Han et al 2012).There are three predominant approaches to com-puting short text similarity.
The first uses informa-tion retrieval?s vector space model (Meadow, 1992)in which each text is modeled as a ?bag of words?and represented using a vector.
The similarity be-tween two texts is then computed as the cosinesimilarity of the vectors.
A variation on this ap-proach leverages web search results (e.g., snip-pets) to provide context for the short texts and en-rich their vectors using the words in the snippets(Sahami and Heilman, 2006).
The second approachis based on the assumption that if two sentences orother short text sequences are semantically equiva-lent, we should be able to align their words or ex-pressions.
The alignment quality can serve as asimilarity measure.
This technique typically pairswords from the two texts by maximizing the sum-mation of the word similarity of the resulting pairs(Mihalcea et al 2006).
The third approach com-bines different measures and features using machinelearning models.
Lexical, semantic and syntacticfeatures are computed for the texts using a varietyof resources and supplied to a classifier, which thenassigns weights to the features by fitting the modelto training data (Saric et al 2012).For evaluating different approaches, the 2013 Se-mantic Textual Similarity (STS) task asked auto-matic systems to compute sentence similarity ac-cording to a scale definition ranging from 0 to 5,with 0 meaning unrelated and 5 semantically equiv-alent (Agirre et al 2012; Agirre et al 2013).
Theexample sentence pair ?The woman is playing theviolin?
and ?The young lady enjoys listening to theguitar?
is scored as only 1 and the pair ?The bird isbathing in the sink?
and ?Birdie is washing itself inthe water basin?
is given a score of 5.The vector-space approach tends to be too shallowfor the task, since solving it well requires discrimi-nating word-level semantic differences and goes be-44yond simply comparing sentence topics or contexts.Our first run uses an align-and-penalize algorithm,which extends the second approach by giving penal-ties to the words that are poorly aligned.
Our othertwo runs use a support vector regression model tocombine a large number of general and domain spe-cific features.
An important and fundamental featureused by all three runs is a powerful semantic wordsimilarity model based on a combination of LatentSemantic Analysis (LSA) (Deerwester et al 1990;Landauer and Dumais, 1997) and knowledge fromWordNet (Miller, 1995).The remainder of the paper proceeds as follows.Section 2 presents the hybrid word similarity model.Section 3 describes the align-and-penalize approachused for the PairingWords run.
In Section 4 we de-scribe the SVM approach used for the Galactus andSaiyan runs.
Section 5 discusses the results and isfollowed by a short conclusion.2 Semantic Word Similarity ModelOur word similarity model was originally developedfor the Graph of Relations project (UMBC, 2013a)which maps informal queries with English wordsand phrases for an RDF linked data collection intoa SPARQL query.
For this, we wanted a metricin which only the semantics of a word is consid-ered and not its lexical category.
For example, theverb ?marry?
should be semantically similar to thenoun ?wife?.
Another desiderata was that the met-ric should give highest scores and lowest scores inits range to similar and non-similar words, respec-tively.
In this section, we describe how we con-structed the model by combining LSA word simi-larity and WordNet knowledge.2.1 LSA Word SimilarityLSA Word Similarity relies on the distributional hy-pothesis that words occurring in the same contextstend to have similar meanings (Harris, 1968).2.1.1 Corpus Selection and ProcessingIn order to produce a reliable word co-occurrencestatistics, a very large and balanced text corpus isrequired.
After experimenting with several cor-pus choices including Wikipedia, Project Gutenberge-Books (Hart, 1997), ukWaC (Baroni et al 2009),Reuters News stories (Rose et al 2002) and LDCgigawords, we selected the Web corpus from theStanford WebBase project (Stanford, 2001).
Weused the February 2007 crawl, which is one of thelargest collections and contains 100 million webpages from more than 50,000 websites.
The Web-Base project did an excellent job in extracting tex-tual content from HTML tags but still has abun-dant text duplications, truncated text, non-Englishtext and strange characters.
We processed the collec-tion to remove undesired sections and produce highquality English paragraphs.
We detected paragraphsusing heuristic rules and only retrained those whoselength was at least two hundred characters.
We elim-inated non-English text by checking the first twentywords of a paragraph to see if they were valid En-glish words.
We used the percentage of punctuationcharacters in a paragraph as a simple check for typi-cal text.
We removed duplicated paragraphs using ahash table.
Finally, we obtained a three billion wordscorpus of good quality English, which is available at(Han and Finin, 2013).2.1.2 Word Co-Occurrence GenerationWe performed POS tagging and lemmatization onthe WebBase corpus using the Stanford POS tagger(Toutanova et al 2000).
Word/term co-occurrencesare counted in a moving window of a fixed sizethat scans the entire corpus1.
We generated two co-occurrence models using window sizes ?1 and ?4because we observed different natures of the models.
?1 window produces a context similar to the depen-dency context used in (Lin, 1998a).
It provides amore precise context but only works for comparingwords within the same POS.
In contrast, a contextwindow of ?4 words allows us to compute semanticsimilarity between words with different POS.Our word co-occurrence models were based ona predefined vocabulary of more than 22,000 com-mon English words and noun phrases.
We alsoadded to it more than 2,000 verb phrases extractedfrom WordNet.
The final dimensions of our wordco-occurrence matrices are 29,000 ?
29,000 whenwords are POS tagged.
Our vocabulary includesonly open-class words (i.e.
nouns, verbs, adjectivesand adverbs).
There are no proper nouns in the vo-cabulary with the only exception of country names.1We used a stop-word list consisting of only the three arti-cles ?a?, ?an?
and ?the?.45Word Pair ?4 model ?1 model1.
doctor NN, physician NN 0.775 0.7262. car NN, vehicle NN 0.748 0.8023. person NN, car NN 0.038 0.0244. car NN, country NN 0.000 0.0165. person NN, country NN 0.031 0.0696. child NN, marry VB 0.098 0.0007. wife NN, marry VB 0.548 0.2748. author NN, write VB 0.364 0.1289. doctor NN, hospital NN 0.473 0.34710. car NN, driver NN 0.497 0.281Table 1: Ten examples from the LSA similarity model2.1.3 SVD TransformationSingular Value Decomposition (SVD) has beenfound to be effective in improving word similar-ity measures (Landauer and Dumais, 1997).
SVDis typically applied to a word by document ma-trix, yielding the familiar LSA technique.
Inour case we apply it to our word by word ma-trix.
In literature, this variation of LSA is some-times called HAL (Hyperspace Analog to Lan-guage) (Burgess et al 1998).Before performing SVD, we transform the rawword co-occurrence count fij to its log frequencylog(fij + 1).
We select the 300 largest singular val-ues and reduce the 29K word vectors to 300 dimen-sions.
The LSA similarity between two words is de-fined as the cosine similarity of their correspondingword vectors after the SVD transformation.2.1.4 LSA Similarity ExamplesTen examples obtained using LSA similarity aregiven in Table 1.
Examples 1 to 6 illustrate that themetric has a good property of differentiating simi-lar words from non-similar words.
Examples 7 and8 show that the ?4 model can detect semanticallysimilar words even with different POS while the ?1model yields much worse performance.
Example 9and 10 show that highly related but not substitutablewords can also have a strong similarity but the ?1model has a better performance in discriminatingthem.
We call the ?1 model and the ?4 modelas concept similarity and relation similarity respec-tively since the ?1 model has a good performanceon nouns and the ?4 model is good at computingsimilarity between relations regardless of POS ofwords, such as ?marry to?
and ?is the wife of?.2.2 Combining with WordNet KnowledgeStatistical word similarity measures have limita-tions.
Related words can have similarity scores ashigh as what similar words get, as illustrated by?doctor?
and ?hospital?
in Table 1.
Word similar-ity is typically low for synonyms having many wordsenses since information about different senses aremashed together (Han et al 2013).
By using Word-Net, we can reduce the above issues.2.2.1 Boosting LSA similarity using WordNetWe increase the similarity between two words if anyof the following relations hold.?
They are in the same WordNet synset.?
One word is the direct hypernym of the other.?
One word is the two-link indirect hypernym ofthe other.?
One adjective has a direct similar to relationwith the other.?
One adjective has a two-link indirect similar torelation with the other.?
One word is a derivationally related form of theother.?
One word is the head of the gloss of the otheror its direct hypernym or one of its direct hy-ponyms.?
One word appears frequently in the glosses ofthe other and its direct hypernym and its directhyponyms.We use the algorithm described in (Collins, 1999)to find a word gloss header.
We require a minimumLSA similarity of 0.1 between the two words to filterout noisy data when extracting WordNet relations.We define a word?s ?significant senses?
to dealwith the problem of WordNet trivial senses.
Theword ?year?, for example, has a sense ?a body ofstudents who graduate together?
which makes it asynonym of the word ?class?.
This causes problemsbecause ?year?
and ?class?
are not similar, in gen-eral.
A sense is significant, if any of the followingconditions are met: (i) it is the first sense; (ii) itsWordNet frequency count is not less than five; or(iii) its word form appears first in its synset?s word46form list and it has a WordNet sense number lessthan eight.We assign path distance of zero to the category1, path distance of one to the category 2, 4 and 6,and path distance of two to the other categories.
Thenew similarity between word x and y by combiningLSA similarity and WordNet relations is shown inthe following equationsim?
(x, y) = simLSA(x, y) + 0.5e?
?D(x,y) (1)where D(x, y) is the minimal path distance betweenx and y.
Using the e?
?D(x,y) to transform simpleshortest path length has been demonstrated to bevery effective according to (Li et al 2003).
The pa-rameter ?
is set to be 0.25, following their experi-mental results.
The ceiling of sim?
(x, y) remains1.0 and we simply cut the excess.2.2.2 Dealing with words of many sensesFor a word w with many WordNet senses (currentlyten or more), we use its synonyms with fewer senses(at most one third of that of w) as its substitutions incomputing similarity with another word.
Let Sx andSy be the sets of all such substitutions of the wordsx and y respectively.
The new similarity is obtainedusing Equation 2.sim(x, y) = max( maxsx?Sx?{x}sim?
(sx, y),maxsy?Sy?{y}sim?
(x, sy)) (2)An online demonstration of a similar modeldeveloped for the GOR project is available(UMBC, 2013b), but it lacks some of this version?sfeatures.3 Align-and-Penalize ApproachFirst we hypothesize that STS similarity betweentwo sentences can be computed usingSTS = T ?
P ?
?
P ??
(3)where T is the term alignments score, P ?
is thepenalty for bad term alignments and P ??
is thepenalty for syntactic contradictions led by the align-ments.
However P ??
had not been fully implementedand was not used in our STS submissions.
We showit here just for completeness.3.1 Aligning terms in two sentencesWe start by applying the Stanford POS tagger to tagand lemmatize the input sentences.
We use our pre-defined vocabulary, POS tagging data and simpleregular expressions to recognize multi-word termsincluding noun and verb phrases, proper nouns,numbers and time.
We ignore adverbs with fre-quency count larger than 500, 000 in our corpus andstop words with general meaning.Equation 4 shows our aligning function g whichfinds the counterpart of term t ?
S in sentence S?.g(t) = argmaxt??S?sim?
(t, t?)
(4)sim?
(t, t?)
is a wrapper function over sim(x, y) inEquation 2 that uses the relation similarity model.It compares numerical and time terms by their val-ues.
If they are equal, 1 is returned; otherwise 0.sim?
(t, t?)
provides limited comparison over pro-nouns.
It returns 1 between subject pronouns I, we,they, he, she and their corresponding object pro-nouns.
sim?
(t, t?)
also outputs 1 if one term is theacronym of the other term, or if one term is the headof the other term, or if two consecutive terms in asentence match a single term in the other sentence(e.g.
?long term?
and ?long-term?).
sim?
(t, t?)
fur-ther adds support for matching words2 not presentedin our vocabulary using a simple string similarity al-gorithm.
It computes character bigram sets for eachof the two words without using padding characters.Dice coefficient is then applied to get the degree ofoverlap between the two sets.
If it is larger than twothirds, sim?
(t, t?)
returns a score of 1; otherwise 0.g(t) is direction-dependent and does not achieveone-to-one mapping.
This property is useful in mea-suring STS similarity because two sentences are of-ten not exact paraphrases of one another.
Moreover,it is often necessary to align multiple terms in onesentence to a single term in the other sentence, suchas when dealing with repetitions and anaphora or,e.g., mapping ?people writing books?
to ?writers?.Let S1 and S2 be the sets of terms in two inputsentences.
We define term alignments score T as thefollowing equation shows.
?t?S1 sim?
(t, g(t))2 ?
|S1|+?t?S2 sim?
(t, g(t))2 ?
|S2|(5)2We use the regular expression ?[A-Za-z][A-Za-z]*?
toidentify them.473.2 Penalizing bad term alignmentsWe currently treat two kinds of alignments as ?bad?,as described in Equation 6.
For the set Bi, we havean additional restriction that neither of the sentenceshas the form of a negation.
In defining Bi, we useda collection of antonyms extracted from WordNet(Mohammad et al 2008).
Antonym pairs are a spe-cial case of disjoint sets.
The terms ?piano?
and ?vi-olin?
are also disjoint but they are not antonyms.
Inorder to broaden the set Bi we will need to developa model that can determine when two terms belongto disjoint sets.Ai ={?t, g(t)?
|t ?
Si ?
sim?
(t, g(t)) < 0.05}Bi = {?t, g(t)?
|t ?
Si ?
t is an antonymof g(t)}i ?
{1, 2} (6)We show how we compute P ?
in Equation 7.PAi =??t,g(t)?
?Ai (sim?
(t, g(t)) +wf (t) ?
wp(t))2 ?
|Si|PBi =??t,g(t)?
?Bi (sim?
(t, g(t)) + 0.5)2 ?
|Si|P ?
= PA1 + PB1 + PA2 + PB2 (7)The wf (t) and wp(t) terms are two weighting func-tions on the term t. wf (t) inversely weights the logfrequency of term t and wp(t) weights t by its part ofspeech tag, assigning 1.0 to verbs, nouns, pronounsand numbers, and 0.5 to terms with other POS tags.4 SVM approachWe used the scores from the align-and-penalize ap-proach along with several other features to learn asupport vector regression model.
We started by ap-plying the following preprocessing steps.?
The sentences were tokenized and POS-taggedusing NLTK?s (Bird, 2006) default Penn Tree-bank based tagger.?
Punctuation characters were removed from thetokens except for the decimal point in numbers.?
All numbers written as words were convertedinto numerals, e.g., ?2.2 million?
was replacedby ?2200000?
and ?fifty six?
by ?56?.?
All mentions of time were converted into mil-itary time, e.g., ?5:40pm?
was replaced by?1740?
and ?1h30am?
by ?0130?.?
Abbreviations were expanded using a compiledlist of commonly used abbreviations.?
About 80 stopwords were removed.4.1 Ngram MatchingThe sentence similarities are derived as a function ofthe similarity scores of their corresponding pairedword ngrams.
These features closely resemble theones used in (Saric et al 2012).
For our system, weused unigrams, bigrams, trigrams and skip-bigrams,a special form of bigrams which allow for arbitrarydistance between two tokens.An ngram from the first sentence is exclusivelypaired with an ngram from the second which has thehighest similarity score.
Several similarity metricsare used to generate different features.
For bigrams,trigrams and skip-bigrams, the similarity score fortwo ngrams is computed as the arithmetic mean ofthe similarity scores of the individual words theycontain.
For example, for the bigrams ?he ate?
and?she spoke?, the similarity score is the average of thesimilarity scores between the words ?he?
and ?she?and the words ?ate?
and ?spoke?.The ngram overlap of two sentences is definedas ?the harmonic mean of the degree to whichthe second sentence covers the first and the de-gree to which the first sentence covers the second?
(Saric et al 2012).
Given sets S1 and S2 containingngrams from sentences 1 and 2, and sets P1 and P2containing their paired ngrams along with their sim-ilarity scores, the ngram overlap score for a givenngram type is computed using the following equa-tion.HM(?n?P1 w(n).sim(n)?n?S1 w(n),?n?P2 w(n).sim(n)?n?S2 w(n))(8)In this formula, HM is the harmonic mean, w(n) isthe weight assigned for the given ngram and sim(n)is the similarity score of the paired word.By default, all the ngrams are assigned a uniformweight of 1.
But since different words carry differ-ent amount of information, e.g.
?acclimatize?
vs.?take?, ?cardiologist?
vs.
?person?, we also use in-formation content as weights.
The information con-tent of a word is as defined in (Saric et al 2012).ic(w) = ln(?w?
?C freq(w?
)freq(w))(9)48Here C is the set of words in the corpus and freq(w)is the frequency of a word in the corpus.
Theweight of an ngram is the sum of its constituent wordweights.
We use refined versions of Google ngramfrequencies (Michel et al 2011) from (Mem, 2008)and (Saric et al 2012) to get the information con-tent of the words.
Words not in this list are assignedthe average weight.We used several word similarity metrics forngram matching apart from the similarity metric de-scribed in section 2.
Our baseline similarity metricwas an exact string match which assigned a scoreof 1 if two tokens contained the same sequence ofcharacters and 0 otherwise.
We also used NLTK?slibrary to compute WordNet based similarity mea-sures such as Path Distance Similarity, Wu-PalmerSimilarity (Wu and Palmer, 1994) and Lin Similar-ity (Lin, 1998b).
For Lin Similarity, the Semcor cor-pus was used for the information content of words.4.2 Contrast ScoresWe computed contrast scores between two sen-tences using three different lists of antonym pairs(Mohammad et al 2008).
We used a large list con-taining 3.5 million antonym pairs, a list of about22,000 antonym pairs from Wordnet and a list of50,000 pairs of words with their degree of contrast.Contrast scores between two sentences were derivedas a function of the number of antonym pairs be-tween them similar to equation 8 but with negativevalues to indicate contrast scores.4.3 FeaturesWe constructed 52 features from different combina-tions of similarity metrics, their parameters, ngramtypes (unigram, bigram, trigram and skip-bigram)and ngram weights (equal weight vs. informationcontent) for all sentence pairs in the training data.?
We used scores from the align-and-penalize ap-proach directly as a feature.?
Using exact string match over different ngramtypes and ngram weights, we extracted eightfeatures (4 ?
4).
We also developed four addi-tional features (2 ?
2) by includin stopwords inbigrams and trigrams, motivated by the natureof MSRvid dataset.?
We used the LSA boosted similarity metric inthree modes: concept similarity, relation simi-larity and mixed mode, which used the conceptmodel for nouns and relation model for verbs,adverbs and adjectives.
A total of 24 featureswere extracted (4 ?
2 ?
3).?
For Wordnet-based similarity measures, weused uniform weights for Path and Wu-Palmersimilarity and used the information content ofwords (derived from the Semcor corpus) forLin similarity.
Skip bigrams were ignored anda total of nine features were produced (3 ?
3).?
Contrast scores used three different lists ofantonym pairs.
A total of six features were ex-tracted using different weight values (3 ?
2).4.4 Support Vector RegressionThe features described in 4.3 were used in dif-ferent combinations to train several support vec-tor regression (SVR) models.
We used LIBSVM(Chang and Lin, 2011) to learn the SVR models andran a grid search provided by (Saric et al 2012) tofind the optimal values for the parameters C , g andp.
These models were then used to predict the scoresfor the test sets.The Galactus system was trained on all of STS2012 data and used the full set of 52 features.
TheFnWN dataset was handled slightly differently fromthe others.
We observed that terms like ?frame?
and?entity?
were used frequently in the five sample sen-tence pairs and treated them as stopwords.
To ac-commodate the vast difference in sentence lengths,equation 8 was modified to compute the arithmeticmean instead of the harmonic mean.The Saiyan system employed data-specific train-ing and features.
The training sets were subsets ofthe supplied STS 2012 dataset.
More specifically,the model for headlines was trained on 3000 sen-tence pairs from MSRvid and MSRpar, SMT used1500 sentence pairs from SMT europarl and SMTnews, while OnWN used only the 750 OnWN sen-tence pairs from last year.
The FnWN scores weredirectly used from the Align-and-Penalize approach.None of the models for Saiyan used contrast fea-tures and the model for SMT also ignored similarityscores from exact string match metric.495 Results and discussionTable 2 presents the official results of our three runsin the 2013 STS task.
Each entry gives a run?s Pear-son correlation on a dataset as well as the rank of therun among all 89 runs submitted by the 35 teams.The last row shows the mean of the correlations andthe overall ranks of our three runs.We tested performance of the align-and-penalizeapproach on all of the 2012 STS datasets.
It ob-tained correlation values of 0.819 on MSRvid, 0.669on MSRpar, 0.553 on SMTeuroparl, 0.567 on SMT-news and 0.722 on OnWN for the test datasets, andcorrelation values of 0.814 on MSRvid, 0.707 onMSRpar and 0.646 on SMTeuroparl for the trainingdatasets.
The performance of the approach withoutusing the antonym penalty is also tested, producingcorrelation scores of 0.795 on MSRvid, 0.667 onMSRpar, 0.554 on SMTeuroparl, 0.566 on SMTnewand 0.727 on OnWN, for the test datasets, and 0.794on MSRvid, 0.707 on MSRpar and 0.651 on SM-Teuroparl for the training datasets.
The average ofthe correlation scores on all eight datasets with andwithout the antonym penalty is 0.6871 and 0.6826,respectively.
Since the approach?s performance wasonly slightly improved when the antonym penaltywas used, we decided to not include this penalty inour PairingWords run in the hope that its simplicitywould make it more robust.During development, our SVM approachachieved correlations of 0.875 for MSRvid, 0.699for MSRpar, 0.559 for SMTeuroparl, 0.625 forSMTnews and 0.729 for OnWN on the 2012 STStest data.
Models were trained on their respectivetraining sets while SMTnews used SMTeuroparl andOnWN used all the training sets.
We experimentedwith different features and training data to studytheir influence on the performance of the models.We found that the unigram overlap feature, based onboosted LSA similarity and weighted by informa-tion content, could independently achieve very highcorrelations.
Including more features improved theaccuracy slightly and in some cases added noise.The difficulty in selecting data specific features andtraining for novel datasets is indicated by Saiyan?scontrasting performance on headlines and OnWNdatasets.
The model used for Headlines was trainedon data from seemingly different domains (MSRvid,Dataset Pairing Galactus SaiyanHeadlines (750 pairs) 0.7642 (3) 0.7428 (7) 0.7838 (1)OnWN (561 pairs) 0.7529 (5) 0.7053 (12) 0.5593 (36)FNWN (189 pairs) 0.5818 (1) 0.5444 (3) 0.5815 (2)SMT (750 pairs) 0.3804 (8) 0.3705 (11) 0.3563 (16)Weighted mean 0.6181 (1) 0.5927 (2) 0.5683 (4)Table 2: Performance of our three systems on the fourtest sets.MSRpar) while OnWN was trained only on OnWNfrom STS 2012.
When the model for headlinesdataset was used to predict the scores for OnWN,the correlation jumped from 0.55 to 0.71 indicatingthat the earlier model suffered from overfitting.Overfitting is not evident in the performance ofPairingWords and Galactus, which have more con-sistent performance over all datasets.
The relativelysimple PairingWords system has two advantages: itis faster, since the current Galactus requires comput-ing a large number of features; and its performanceis more predictable, since training is not needed thuseliminating noise induced from diverse training sets.6 ConclusionWe described three semantic text similarity systemsdeveloped for the *SEM 2013 STS shared task andthe results of the corresponding three runs we sub-mitted.
All of the systems used a lexical similarityfeature that combined POS tagging, LSA word sim-ilarity and WordNet knowledge.The first run, which achieved the best mean scoreout of all 89 submissions, used a simple term align-ment algorithm augmented with two penalty met-rics.
The other two runs, ranked second and fourthout of all submissions, used support vector regres-sion models based on a set of more than 50 addi-tional features.
The runs differed in their featuresets, training data and procedures, and parametersettings.AcknowledgmentsThis research was supported by AFOSR awardFA9550-08-1-0265 and a gift from Microsoft.50References[Agirre et al012] Eneko Agirre, Mona Diab, Daniel Cer,and Aitor Gonzalez-Agirre.
2012.
Semeval-2012 task6: a pilot on semantic textual similarity.
In Proceed-ings of the First Joint Conference on Lexical and Com-putational Semantics, pages 385?393.
Association forComputational Linguistics.
[Agirre et al013] Eneko Agirre, Daniel Cer, Mona Diab,Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*sem2013 shared task: Semantic textual similarity, includ-ing a pilot on typed-similarity.
In *SEM 2013: TheSecond Joint Conference on Lexical and Computa-tional Semantics.
Association for Computational Lin-guistics.
[Baroni et al009] M. Baroni, S. Bernardini, A. Fer-raresi, and E. Zanchetta.
2009.
The wacky wideweb: A collection of very large linguistically pro-cessed web-crawled corpora.
Language Resourcesand Evaluation, 43(3):209?226.
[Bird2006] Steven Bird.
2006.
Nltk: the natural lan-guage toolkit.
In Proceedings of the COLING/ACL onInteractive presentation sessions, COLING-ACL ?06,pages 69?72, Stroudsburg, PA, USA.
Association forComputational Linguistics.
[Burgess et al998] C. Burgess, K. Livesay, and K. Lund.1998.
Explorations in context space: Words, sen-tences, discourse.
Discourse Processes, 25:211?257.
[Chang and Lin2011] Chih-Chung Chang and Chih-JenLin.
2011.
LIBSVM: A library for support vector ma-chines.
ACM Transactions on Intelligent Systems andTechnology, 2:27:1?27:27.
[Coelho et al004] T.A.S.
Coelho, Pa?vel Pereira Calado,Lamarque Vieira Souza, Berthier Ribeiro-Neto, andRichard Muntz.
2004.
Image retrieval using multipleevidence ranking.
IEEE Trans.
on Knowl.
and DataEng., 16(4):408?417.
[Collins1999] Michael John Collins.
1999.
Head-drivenstatistical models for natural language parsing.
Ph.D.thesis, University of Pennsylvania.
[Deerwester et al990] Scott Deerwester, Susan T. Du-mais, George W. Furnas, Thomas K. Landauer, andRichard Harshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society for Informa-tion Science, 41(6):391?407.
[Dolan et al004] Bill Dolan, Chris Quirk, and ChrisBrockett.
2004.
Unsupervised construction oflarge paraphrase corpora: exploiting massively paral-lel news sources.
In Proceedings of the 20th interna-tional conference on Computational Linguistics, COL-ING ?04.
Association for Computational Linguistics.
[Ganitkevitch et al013] Juri Ganitkevitch, Ben-jamin Van Durme, and Chris Callison-Burch.
2013.PPDB: The paraphrase database.
In HLT-NAACL2013.
[Han and Finin2013] Lushan Han and Tim Finin.
2013.UMBC webbase corpus.
http://ebiq.org/r/351.
[Han et al012] Lushan Han, Tim Finin, and AnupamJoshi.
2012.
Schema-free structured querying of db-pedia data.
In Proceedings of the 21st ACM interna-tional conference on Information and knowledge man-agement, pages 2090?2093.
ACM.
[Han et al013] Lushan Han, Tim Finin, Paul McNamee,Anupam Joshi, and Yelena Yesha.
2013.
ImprovingWord Similarity by Augmenting PMI with Estimatesof Word Polysemy.
IEEE Transactions on Knowledgeand Data Engineering, 25(6):1307?1322.
[Harris1968] Zellig Harris.
1968.
Mathematical Struc-tures of Language.
Wiley, New York, USA.
[Hart1997] M. Hart.
1997.
Project gutenberg electronicbooks.
http://www.gutenberg.org/wiki/Main Page.
[Kauchak and Barzilay2006] David Kauchak and ReginaBarzilay.
2006.
Paraphrasing for automatic evalua-tion.
In HLT-NAACL ?06, pages 455?462.
[Landauer and Dumais1997] T. Landauer and S. Dumais.1997.
A solution to plato?s problem: The latent se-mantic analysis theory of the acquisition, induction,and representation of knowledge.
In PsychologicalReview, 104, pages 211?240.
[Li et al003] Y. Li, Z.A.
Bandar, and D. McLean.2003.
An approach for measuring semantic similar-ity between words using multiple information sources.IEEE Transactions on Knowledge and Data Engineer-ing, 15(4):871?882.
[Lin1998a] Dekang Lin.
1998a.
Automatic retrieval andclustering of similar words.
In Proc.
17th Int.
Conf.
onComputational Linguistics, pages 768?774, Montreal,CN.
[Lin1998b] Dekang Lin.
1998b.
An information-theoretic definition of similarity.
In Proceedings of theFifteenth International Conference on Machine Learn-ing, ICML ?98, pages 296?304, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.[Meadow1992] Charles T. Meadow.
1992.
Text Informa-tion Retrieval Systems.
Academic Press, Inc.[Mem2008] 2008.
Google word frequency counts.http://bit.ly/10JdTRz.
[Metzler et al007] Donald Metzler, Susan Dumais, andChristopher Meek.
2007.
Similarity measures forshort segments of text.
In Proceedings of the 29thEuropean conference on IR research, pages 16?27.Springer-Verlag.
[Michel et al011] Jean-Baptiste Michel, Yuan K. Shen,Aviva P. Aiden, Adrian Veres, Matthew K. Gray, TheGoogle Books Team, Joseph P. Pickett, Dale Hoiberg,Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker,51Martin A. Nowak, and Erez L. Aiden.
2011.
Quan-titative analysis of culture using millions of digitizedbooks.
Science, 331(6014):176?182, January 14.
[Mihalcea et al006] Rada Mihalcea, Courtney Corley,and Carlo Strapparava.
2006.
Corpus-based andknowledge-based measures of text semantic similarity.In Proceedings of the 21st national conference on Ar-tificial intelligence, pages 775?780.
AAAI Press.
[Miller1995] G.A.
Miller.
1995.
WordNet: a lexicaldatabase for English.
Communications of the ACM,38(11):41.
[Mohammad et al008] Saif Mohammad, Bonnie Dorr,and Graeme Hirst.
2008.
Computing word-pairantonymy.
In Proc.
Conf.
on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-2008), October.
[Rose et al002] Tony Rose, Mark Stevenson, and MilesWhitehead.
2002.
The reuters corpus volume 1 - fromyesterdays news to tomorrows language resources.
InIn Proceedings of the Third International Conferenceon Language Resources and Evaluation, pages 29?31.
[Sahami and Heilman2006] Mehran Sahami and Timo-thy D. Heilman.
2006.
A web-based kernel functionfor measuring the similarity of short text snippets.
InProceedings of the 15th international conference onWorld Wide Web, WWW ?06, pages 377?386.
ACM.
[Saric et al012] Frane Saric, Goran Glavas, MladenKaran, Jan Snajder, and Bojana Dalbelo Basic.
2012.Takelab: systems for measuring semantic text simi-larity.
In Proceedings of the First Joint Conferenceon Lexical and Computational Semantics, pages 441?448.
Association for Computational Linguistics.
[Sriram et al010] Bharath Sriram, Dave Fuhry, EnginDemir, Hakan Ferhatosmanoglu, and Murat Demirbas.2010.
Short text classification in twitter to improveinformation filtering.
In Proceedings of the 33rd in-ternational ACM SIGIR conference on Research anddevelopment in information retrieval, pages 841?842.ACM.
[Stanford2001] Stanford.
2001.
Stanford WebBaseproject.
http://bit.ly/WebBase.
[Toutanova et al000] Kristina Toutanova, DanKlein, Christopher Manning, William Mor-gan, Anna Rafferty, and Michel Galley.
2000.Stanford log-linear part-of-speech tagger.http://nlp.stanford.edu/software/tagger.shtml.
[UMBC2013a] UMBC.
2013a.
Graph of relationsproject.
http://ebiq.org/j/95.
[UMBC2013b] UMBC.
2013b.
Semantic similaritydemonstration.
http://swoogle.umbc.edu/SimService/.
[Wu and Palmer1994] Z. Wu and M. Palmer.
1994.
Verbsemantic and lexical selection.
In Proceedings of the32nd Annual Meeting of the Association for Compu-tational Linguistics (ACL-1994), pages 133?138, LasCruces (Mexico).52
