Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1148?1157,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsPCFGs, Topic Models, Adaptor Grammars and Learning TopicalCollocations and the Structure of Proper NamesMark JohnsonDepartment of ComputingMacquarie Universitymjohnson@science.mq.edu.auAbstractThis paper establishes a connection be-tween two apparently very different kindsof probabilistic models.
Latent Dirich-let Allocation (LDA) models are usedas ?topic models?
to produce a low-dimensional representation of documents,while Probabilistic Context-Free Gram-mars (PCFGs) define distributions overtrees.
The paper begins by showing thatLDA topic models can be viewed as aspecial kind of PCFG, so Bayesian in-ference for PCFGs can be used to inferTopic Models as well.
Adaptor Grammars(AGs) are a hierarchical, non-parametericBayesian extension of PCFGs.
Exploit-ing the close relationship between LDAand PCFGs just described, we proposetwo novel probabilistic models that com-bine insights from LDA and AG models.The first replaces the unigram componentof LDA topic models with multi-word se-quences or collocations generated by anAG.
The second extension builds on thefirst one to learn aspects of the internalstructure of proper names.1 IntroductionOver the last few years there has been consider-able interest in Bayesian inference for complex hi-erarchical models both in machine learning and incomputational linguistics.
This paper establishesa theoretical connection between two very differ-ent kinds of probabilistic models: ProbabilisticContext-Free Grammars (PCFGs) and a class ofmodels known as Latent Dirichlet Allocation (Bleiet al, 2003; Griffiths and Steyvers, 2004) modelsthat have been used for a variety of tasks in ma-chine learning.
Specifically, we show that an LDAmodel can be expressed as a certain kind of PCFG,so Bayesian inference for PCFGs can be used tolearn LDA topic models as well.
The importanceof this observation is primarily theoretical, as cur-rent Bayesian inference algorithms for PCFGs areless efficient than those for LDA inference.
How-ever, once this link is established it suggests a vari-ety of extensions to the LDA topic models, two ofwhich we explore in this paper.
The first involvesextending the LDA topic model so that it generatescollocations (sequences of words) rather than indi-vidual words.
The second applies this idea to theproblem of automatically learning internal struc-ture of proper names (NPs), which is useful fordefinite NP coreference models and other applica-tions.The rest of this paper is structured as follows.The next section reviews Latent Dirichlet Alloca-tion (LDA) topic models, and the following sec-tion reviews Probabilistic Context-Free Grammars(PCFGs).
Section 4 shows how an LDA topicmodel can be expressed as a PCFG, which pro-vides the fundamental connection between LDAand PCFGs that we exploit in the rest of thepaper, and shows how it can be used to definea ?sticky topic?
version of LDA.
The follow-ing section reviews Adaptor Grammars (AGs), anon-parametric extension of PCFGs introduced byJohnson et al (2007b).
Section 6 exploits the con-nection between LDA and PCFGs to propose anAG-based topic model that extends LDA by defin-ing distributions over collocations rather than indi-vidual words, and section 7 applies this extensionto the problem of finding the structure of propernames.2 Latent Dirichlet Allocation ModelsLatent Dirichlet Allocation (LDA) was introducedas an explicit probabilistic counterpart to La-tent Semantic Indexing (LSI) (Blei et al, 2003).Like LSI, LDA is intended to produce a low-dimensional characterisation or summary of a doc-1148WZ???
?n m`Figure 1: A graphical model ?plate?
representa-tion of an LDA topic model.
Here ` is the numberof topics, m is the number of documents and n isthe number of words per document.ument in a collection of documents for informa-tion retrieval purposes.
Both LSI and LDA dothis by mapping documents to points in a rela-tively low-dimensional real-valued vector space;distance in this space is intended to correspond todocument similarity.An LDA model is an explicit generative proba-bilistic model of a collection of documents.
Wedescribe the ?smoothed?
LDA model here (seepage 1006 of Blei et al (2003)) as it correspondsprecisely to the Bayesian PCFGs described in sec-tion 4.
It generates a collection of documents byfirst generating multinomials ?i over the vocab-ulary V for each topic i ?
1, .
.
.
, `, where ` isthe number of topics and ?i,w is the probabilityof generating word w in topic i.
Then it gen-erates each document Dj , j = 1, .
.
.
,m in turnby first generating a multinomial ?j over topics,where ?j,i is the probability of topic i appearingin document j.
(?j serves as the low-dimensionalrepresentation of document Dj).
Finally it gener-ates each of the n words of document Dj by firstselecting a topic z for the word according to ?j ,and then drawing a word from ?z .
Dirichlet priorswith parameters ?
and ?
respectively are placedon the ?i and the ?j in order to avoid the zerosthat can arise from maximum likelihood estima-tion (i.e., sparse data problems).The LDA generative model can be compactlyexpressed as follows, where ???
should be readas ?is distributed according to?.
?i ?
Dir(?)
i = 1, .
.
.
, `?j ?
Dir(?)
j = 1, .
.
.
,mzj,k ?
?j j = 1, .
.
.
,m; k = 1, .
.
.
, nwj,k ?
?zj,k j = 1, .
.
.
,m; k = 1, .
.
.
, nIn inference, the parameters ?
and ?
of theDirichlet priors are either fixed (i.e., chosen bythe model designer), or else themselves inferred,e.g., by Bayesian inference.
(The adaptor gram-mar software we used in the experiments de-scribed below automatically does this kind ofhyper-parameter inference).The inference task is to find the topic probabil-ity vector ?j of each documentDj given the wordswj,k of the documents; in general this also requiresinferring the topic to word distributions ?
and thetopic assigned to each word zj,k.
Blei et al (2003)describe a Variational Bayes inference algorithmfor LDA models based on a mean-field approx-imation, while Griffiths and Steyvers (2004) de-scribe an Markov Chain Monte Carlo inference al-gorithm based on Gibbs sampling; both are quiteeffective in practice.3 Probabilistic Context-Free GrammarsContext-Free Grammars are a simple model of hi-erarchical structure often used to describe natu-ral language syntax.
A Context-Free Grammar(CFG) is a quadruple (N,W,R, S) where N andW are disjoint finite sets of nonterminal and ter-minal symbols respectively,R is a finite set of pro-ductions or rules of the formA?
?
whereA ?
Nand ?
?
(N?W )?, and S ?
N is the start symbol.In what follows, it will be useful to interpret aCFG as generating sets of finite, labelled, orderedtrees TA for each X ?
N ?
W .
Informally, TXconsists of all trees t rooted in X where for eachlocal tree (B, ?)
in t (i.e., where B is a parent?slabel and ?
is the sequence of labels of its imme-diate children) there is a rule B ?
?
?
R.Formally, the sets TX are the smallest sets oftrees that satisfy the following equations.If X ?
W (i.e., if X is a terminal) then TX ={X}, i.e., TX consists of a single tree, which inturn only consists of a single node labelled X .If X ?
N (i.e., if X is a nonterminal) thenTX =?X?B1...Bn?RXTREEX(TB1 , .
.
.
, TBn)where RA = {A ?
?
: A ?
?
?
R} for eachA ?
N , andTREEX(TB1 , .
.
.
, TBn)={ PPXt1 tn.
.
.
:ti ?
TBi ,i = 1, .
.
.
, n}That is, TREEX(TB1 , .
.
.
, TBn) consists of the setof trees with whose root node is labelled X andwhose ith child is a member of TBi .1149The set of trees generated by the CFG is TS ,where S is the start symbol, and the set of stringsgenerated by the CFG is the set of yields (i.e., ter-minal strings) of the trees in TS .A Probabilistic Context-Free Grammar (PCFG)is a pair consisting of a CFG and set of multino-mial probability vectors ?X indexed by nontermi-nals X ?
N , where ?X is a distribution over therulesRX (i.e., the rules expandingX).
Informally,?X??
is the probability ofX expanding to ?
usingthe rule X ?
?
?
RX .
More formally, a PCFGassociates each X ?
N ?
W with a distributionGX over the trees TX as follows.If X ?
W (i.e., if X is a terminal) then GXis the distribution that puts probability 1 on thesingle-node tree labelled X .If X ?
N (i.e., if X is a nonterminal) then:GX =?X?B1...Bn?RX?X?B1...BnTDX(GB1 , .
.
.
, GBn) (1)where:TDA(G1, .
.
.
, Gn)( PPXt1 tn.
.
.
)=n?i=1Gi(ti).That is, TDA(G1, .
.
.
, Gn) is a distribution overTA where each subtree ti is generated indepen-dently from Gi.
These equations have solutions(i.e., the PCFG is said to be ?consistent?)
whenthe rule probabilities ?A obey certain conditions;see e.g., Wetherell (1980) for details.The PCFG generates the distribution over treesGS , where S is the start symbol.
The distribu-tion over the strings it generates is obtained bymarginalising over the trees.In a Bayesian PCFG one puts Dirichlet priorsDir(?X) on each of the multinomial rule proba-bility vectors ?X for each nonterminal X ?
N .This means that there is one Dirichlet parameter?X??
for each rule X ?
?
?
R in the CFG.In the ?unsupervised?
inference problem for aPCFG one is given a CFG, parameters ?X for theDirichlet priors over the rule probabilities, and acorpus of strings.
The task is to infer the cor-responding posterior distribution over rule prob-abilities ?X .
Recently Bayesian inference algo-rithms for PCFGs have been described.
Kuriharaand Sato (2006) describe a Variational Bayes algo-rithm for inferring PCFGs using a mean-field ap-proximation, while Johnson et al (2007a) describea Markov Chain Monte Carlo algorithm based onGibbs sampling.4 LDA topic models as PCFGsThis section explains how to construct a PCFGthat generates the same distribution over a collec-tion of documents as an LDA model, and whereBayesian inference for the PCFG?s rule proba-bilities yields the corresponding distributions asBayesian inference of the corresponding LDAmodels.
(There are several different ways of en-coding LDA models as PCFGs; the one presentedhere is not the most succinct ?
it is possible tocollapse the Doc and Doc?
nonterminals ?
but ithas the advantage that the LDA distributions mapstraight-forwardly onto PCFG nonterminals).The terminals W of the CFG consist of the vo-cabulary V of the LDA model plus a set of special?document identifier?
terminals ?
j?
for each doc-ument j ?
1, .
.
.
,m, where m is the number ofdocuments.
In the PCFG encoding strings fromdocument j are prefixed with ?
j?
; this indicatesto the grammar which document the string comesfrom.
The nonterminals consist of the start symbolSentence, Docj and Doc?j for each j ?
1, .
.
.
,m,and Topici for each i ?
1, .
.
.
, `, where ` is thenumber of topics in the LDA model.The rules of the CFG are all instances of thefollowing schemata:Sentence?
Doc?j j ?
1, .
.
.
,mDoc?j ?
j j ?
1, .
.
.
,mDoc?j ?
Doc?j Docj j ?
1, .
.
.
,mDocj ?
Topici i ?
1, .
.
.
, `; j ?
1, .
.
.
,mTopici ?
w i ?
1, .
.
.
, `;w ?
VFigure 2 depicts a tree generated by such aCFG.
The relationship between the LDA modeland the PCFG can be understood by studying thetrees generated by the CFG.
In these trees the left-branching spine of nodes labelled Doc?j propagatethe document identifier throughout the whole tree.The nodes labelled Topici indicate the topics as-signed to particular words, and the local trees ex-panding Docj to Topici (one per word in the docu-ment) indicate the distribution of topics in the doc-ument.The corresponding Bayesian PCFG associatesprobabilities with each of the rules in the CFG.The probabilities ?Topici associated with the rulesexpanding the Topici nonterminals indicate howwords are distributed across topics; the ?Topiciprobabilities correspond exactly to to the ?i prob-abilities in the LDA model.
The probabilities1150SentenceDoc3'Doc3'Doc3'Doc3'Doc3'_3Doc3Topic4shallowDoc3Topic4circuitsDoc3Topic4computeDoc3Topic7fasterFigure 2: A tree generated by the CFG encodingan LDA topic model.
The prefix ?
3?
indicatesthat this string belongs to document 3.
The treealso indicates the assignment of words to topics.
?Docj associated with rules expanding Docj spec-ify the distribution of topics in document j; theycorrespond exactly to the probabilities ?j of theLDA model.
(The PCFG also specifies severalother distributions that are suppressed in the LDAmodel.
For example ?Sentence specifies the distri-bution of documents in the corpus.
However, it iseasy to see that these distributions do not influencethe topic distributions; indeed, the expansions ofthe Sentence nonterminal are completely deter-mined by the document distribution in the corpus,and are not affected by ?Sentence).A Bayesian PCFG places Dirichlet priorsDir(?A) on the corresponding rule probabilities?A for each A ?
N .
In the PCFG encoding anLDA model, the ?Topici parameters correspondexactly to the ?
parameters of the LDA model, andthe ?Docj parameters correspond to the ?
param-eters of the LDA model.As suggested above, each document Dj in theLDA model is mapped to a string in the corpusused to train the corresponding PCFG by prefix-ing it with a document identifier ?
j?.
Given thistraining data, the posterior distribution over ruleprobabilities ?Docj?
Topici is the same as the pos-terior distribution over topics given documents ?j,iin the original LDA model.As we will see below, this connection betweenPCFGs and LDA topic models suggests a num-ber of interesting variants of both PCFGs andtopic models.
Note that we are not suggestingthat Bayesian inference for PCFGs is necessar-ily a good way of estimating LDA topic models.Current Bayesian PCFG inference algorithms re-quire time proportional to the cube of the length ofthe longest string in the training corpus, and sincethese strings correspond to entire documents in ourembedding, blindly applying a Bayesian PCFG in-ference algorithm is likely to be impractical.A little reflection shows that the embedding stillholds if the strings in the PCFG corpus correspondto sentences or even smaller units of the originaldocument collection, so a single document wouldbe mapped to multiple strings in the PCFG infer-ence task.
In this way the cubic time complex-ity of PCFG inference can be mitigated.
Also, thetrees generated by these CFGs have a very spe-cialized left-branching structure, and it is straight-forward to modify the general-purpose CFG infer-ence procedures to avoid the cubic time complex-ity for such grammars: thus it may be practical toestimate topic models via grammatical inference.However, we believe that the primary value ofthe embedding of LDA topic models into BayesianPCFGs is theoretical: it suggests a number ofnovel extensions of both topic models and gram-mars that may be worth exploring.
Our claim hereis not that these models are the best algorithms forperforming these tasks, but that the relationshipwe described between LDA models and PCFGssuggests a variety of interesting novel models.We end this section with a simple example ofsuch a modification to LDA.
Inspired by the stan-dard embedding of HMMs into PCFGs, we pro-pose a ?sticky topic?
variant of LDA in which ad-jacent words are more likely to be assigned thesame topic.
Such an LDA extension is easy todescribe as a PCFG (see Fox et al (2008) for asimilar model presented as an extended HMM).The nonterminals Sentence and Topici for i =1, .
.
.
, ` have the same interpretation as before, butwe introduce new nonterminals Docj,i that indi-cate we have just generated a nonterminal in doc-ument j belonging to topic i.
Given a collection ofm documents and ` topics, the rule schemata areas follows:Sentence?
Docj,i i ?
1, .
.
.
, `;j ?
1, .
.
.
,mDocj,1 ?
j j ?
1, .
.
.
,mDocj,i ?
Docj,i?
Topici i, i?
?
1, .
.
.
, `;j ?
1, .
.
.
,mTopici ?
w i ?
1, .
.
.
, `;w ?
VA sample parse generated by a ?sticky topic?1151SentenceDoc3'_Doc3'TDoc3'TDoc3'TDoc3'pi34oshcTalwrrou4oshcTchmc7hta4oshcTcofs7te4oshc_watemFigure 3: A tree generated by the ?sticky topic?CFG.
Here a nonterminal Doc3, 7 indicates wehave just generated a word in document 3 belong-ing to topic 7.CFG is shown in Figure 3.
The probabilities ofthe rules Docj,i ?
Docj,i?
Topici in this PCFGencode the probability of shifting from topic i totopic i?
(this PCFG can be viewed as generatingthe string from right to left).We can use non-uniform sparse Dirichlet pri-ors on the probabilities of these rules to encour-age ?topic stickiness?.
Specifically, by settingthe Dirichlet parameters for the ?topic shift?
rulesDocj,i?
?
Docj,i Topici where i?
6= i much lowerthan the parameters for the ?topic preservation?rules Docj,i ?
Docj,i Topici, Bayesian inferencewill be biased to find distributions in which adja-cent words will tend to have the same topic.5 Adaptor GrammarsNon-parametric Bayesian inference, where the in-ference task involves learning not just the valuesof a finite vector of parameters but which parame-ters are relevant, has been the focus of intense re-search in machine learning recently.
In the topic-modelling community this has lead to work onDirichlet Processes and Chinese Restaurant Pro-cesses, which can be used to estimate the numberof topics as well as their distribution across docu-ments (Teh et al, 2006).There are two obvious non-parametric exten-sions to PCFGs.
In the first we regard the setof nonterminals N as potentially unbounded, andtry to learn the set of nonterminals required to de-scribe the training corpus.
This approach goes un-der the name of the ?infinite HMM?
or ?infinitePCFG?
(Beal et al, 2002; Liang et al, 2007; Lianget al, 2009).
Informally, we are given a set of ?ba-sic categories?, say NP,VP, etc., and a set of rulesthat use these basic categories, say S ?
NP VP.The inference task is to learn a set of refined cate-gories and rules (e.g., S7 ?
NP2 VP5) as well astheir probabilities; this approach can therefore beviewed as a Bayesian version of the ?split-merge?approach to grammar induction (Petrov and Klein,2007).In the second approach, which we adopt here,we regard the set of rules R as potentially un-bounded, and try to learn the rules required todescribe a training corpus as well as their prob-abilities.
Adaptor grammars are an example ofthis approach (Johnson et al, 2007b), where en-tire subtrees generated by a ?base grammar?
canbe viewed as distinct rules (in that we learn a sep-arate probability for each subtree).
The inferencetask is non-parametric if there are an unboundednumber of such subtrees.We review the adaptor grammar generative pro-cess below; for an informal introduction see John-son (2008) and for details of the adaptor grammarinference procedure see Johnson and Goldwater(2009).An adaptor grammar (N,W,R, S, ?, A,C) con-sists of a PCFG (N,W,R, S, ?)
in which a sub-set A ?
N of the nonterminals are adapted, andwhere each adapted nonterminal X ?
A has anassociated adaptor CX .
An adaptor CX for X is afunction that maps a distribution over trees TX toa distribution over distributions over TX (we giveexamples of adaptors below).Just as for a PCFG, an adaptor grammar de-fines distributions GX over trees TX for each X ?N ?W .
If X ?
W or X 6?
A then GX is definedjust as for a PCFG above, i.e., using (1).
How-ever, if X ?
A then GX is defined in terms of anadditional distribution HX as follows:GX ?
CX(HX)HX =?X?Y1...Ym?RX?X?Y1...YmTDX(GY1 , .
.
.
, GYm)That is, the distribution GX associated with anadapted nonterminal X ?
A is a sample fromadapting (i.e., applying CX to) its ?ordinary?PCFG distribution HX .
In general adaptors arechosen for the specific properties they have.
Forexample, with the adaptors used hereGX typicallyconcentrates mass on a smaller subset of the treesTX than HX does.Just as with the PCFG, an adaptor grammar gen-erates the distribution over treesGS , where S ?
N1152is the start symbol.
However, whileGS in a PCFGis a fixed distribution (given the rule probabili-ties ?
), in an adaptor grammar the distribution GSis itself a random variable (because each GX forX ?
A is random), i.e., an adaptor grammar gen-erates a distribution over distributions over treesTS .
However, the posterior joint distribution Pr(t)of a sequence t = (t1, .
.
.
, tn) of trees in TS iswell-defined:Pr(t) =?GS(t1) .
.
.
GS(tn) dGwhere the integral is over all of the random distri-butions GX , X ?
A.
The adaptors we use in thispaper are Dirichlet Processes or two-parameterPoisson-Dirichlet Processes, for which it is pos-sible to compute this integral.
One way to do thisuses the predictive distributions:Pr(tn+1 | t, HX)?
?GX(t1) .
.
.
GX(tn+1)CX(GX | HX) dGXwhere t = (t1, .
.
.
, tn) and each ti ?
TX .
The pre-dictive distribution for the Dirichlet Process is the(labeled) Chinese Restaurant Process (CRP), andthe predictive distribution for the two-parameterPoisson-Dirichlet process is the (labeled) Pitman-Yor Process (PYP).In the context of adaptor grammars, the CRP is:CRP(t | t, ?X , HX) ?
nt(t) + ?XHX(t)where nt(t) is the number of times t appears in tand ?X > 0 is a user-settable ?concentration pa-rameter?.
In order to generate the next tree tn+1a CRP either reuses a tree t with probability pro-portional to number of times t has been previouslygenerated, or else it ?backs off?
to the ?base distri-bution?HX and generates a fresh tree t with prob-ability proportional to ?XHX(t).The PYP is a generalization of the CRP:PYP(t | t, aX , bX , HX)?
max(0, nt(t)?mt aX) + (maX + bX)HX(t)Here aX ?
[0, 1] and bX > 0 are user-settableparameters, andmt is the number of times the PYPhas generated t in t from the base distributionHX ,and m =?t?TXmt is the number of times anytree has been generated from HX .
(In the ChineseRestaurant metaphor, mt is the number of tableslabeled with t, and m is the number of occupiedtables).
If aX = 0 then the PYP is equivalent toa CRP with ?X = bX , while if aX = 1 then thePYP generates samples from HX .Informally, the CRP has a strong preferenceto regenerate trees that have been generated fre-quently before, leading to a ?rich-get-richer?
dy-namics.
The PYP can mitigate this somewhat byreducing the effective count of previously gener-ated trees and redistributing that probability massto new trees generated from HX .
As Goldwa-ter et al (2006) explain, Bayesian inference forHX given samples from GX is effectively per-formed from types if aX = 0 and from tokensif aX = 1, so varying aX smoothly interpolatesbetween type-based and token-based inference.Adaptor grammars have previously been usedprimarily to study grammatical inference in thecontext of language acquisition.
The word seg-mentation task involves segmenting a corpusof unsegmented phonemic utterance representa-tions into words (Elman, 1990; Bernstein-Ratner,1987).
For example, the phoneme string corre-sponding to ?you want to see the book?
(with itscorrect segmentation indicated) is as follows:y Mu Nw Ma Mn Mt Nt Mu Ns Mi ND M6 Nb MU MkWe can represent any possible segmentation of anypossible sentence as a tree generated by the fol-lowing unigram adaptor grammar.Sentence?WordSentence?Word SentenceWord?
PhonemesPhonemes?
PhonemePhonemes?
Phoneme PhonemesThe trees generated by this adaptor grammar arethe same as the trees generated by the CFG rules.For example, the following skeletal parse in whichall but the Word nonterminals are suppressed (theothers are deterministically inferrable) shows theparse that corresponds to the correct segmentationof the string above.
(Word y u) (Word w a n t) (Word t u)(Word s i) (Word d 6) (Word b u k)Because the Word nonterminal is adapted (indi-cated here by underlining) the adaptor grammarlearns the probability of the entire Word subtrees(e.g., the probability that b u k is a Word); seeJohnson (2008) for further details.11536 Topic models with collocationsHere we combine ideas from the unigram wordsegmentation adaptor grammar above and thePCFG encoding of LDA topic models to presenta novel topic model that learns topical colloca-tions.
(For a non-grammar-based approach to thisproblem see Wang et al (2007)).
Specifically, wetake the PCFG encoding of the LDA topic modeldescribed above, but modify it so that the Topicinodes generate sequences of words rather than sin-gle words.
Then we adapt each of the Topici non-terminals, which means that we learn the probabil-ity of each of the sequences of words it can expandto.Sentence?
Docj j ?
1, .
.
.
,mDocj ?
j j ?
1, .
.
.
,mDocj ?
Docj Topici i ?
1, .
.
.
, `;j ?
1, .
.
.
,mTopici?Words i ?
1, .
.
.
, `Words?WordWords?Words WordWord?
w w ?
VIn order to demonstrate that this modelworks, we implemented this using the publically-available adaptor grammar inference software,1and ran it on the NIPS corpus (composed of pub-lished NIPS abstracts), which has previously beenused for studying collocation-based topic models(Griffiths et al, 2007).
Because there is no gen-erally accepted evaluation for collocation-finding,we merely present some of the sample analysesfound by our adaptor grammar.
We ran our adap-tor grammar with ` = 20 topics (i.e., 20 distinctTopici nonterminals).
Adaptor grammar inferenceon this corpus is actually relatively efficient be-cause the corpus provided by Griffiths et al (2007)is already segmented by punctuation, so the termi-nal strings are generally rather short.
Rather thanset the Dirichlet parameters by hand, we placedvague priors on them and estimated them as de-scribed in Johnson and Goldwater (2009).The following are some examples of colloca-tions found by our adaptor grammar:Topic0?
cost functionTopic0?
fixed pointTopic0?
gradient descentTopic0?
learning rates1http://web.science.mq.edu.au/ ?mjohnson/Software.htmTopic1?
associative memoryTopic1?
hamming distanceTopic1?
randomly chosenTopic1?
standard deviationTopic3?
action potentialsTopic3?
membrane potentialTopic3?
primary visual cortexTopic3?
visual systemTopic10?
nervous systemTopic10?
action potentialTopic10?
ocular dominanceTopic10?
visual fieldThe following are skeletal sample parses, wherewe have elided all but the adapted nonterminals(i.e., all we show are the Topic nonterminals, sincethe other structure can be inferred deterministi-cally).
Note that because Griffiths et al (2007)segmented the NIPS abstracts at punctuation sym-bols, the training corpus contains more than onestring from each abstract.3 (Topic5 polynomial size)(Topic15 threshold circuits)4 (Topic11 studied)(Topic19 pattern recognition algorithms)4 (Topic2 feedforward neural network)(Topic1 implementation)5 (Topic11 single)(Topic10 ocular dominance stripe)(Topic12 low) (Topic3 ocularity)(Topic12 drift rate)7 Finding the structure of proper namesGrammars offer structural and positional sensitiv-ity that is not exploited in the basic LDA topicmodels.
Here we explore the potential for us-ing Bayesian inference for learning linear order-ing constraints that hold between elements withinproper names.The Penn WSJ treebank is a widely used re-source within computational linguistics (Marcuset al, 1993), but one of its weaknesses is thatit does not indicate any structure internal to basenoun phrases (i.e., it presents ?flat?
analyses of thepre-head NP elements).
For many applications itwould be extremely useful to have a more elab-orated analysis of this kind of NP structure.
Forexample, in an NP coreference application, if wecould determine that Bill and Hillary are both first1154names then we could infer that Bill Clinton andHillary Clinton are likely to refer to distinct in-dividuals.
On the other hand, because Mr in MrClinton is not a first name, it is possible that MrClinton and Bill Clinton refer to the same individ-ual (Elsner et al, 2009).Here we present an adaptor grammar based onthe insights of the PCFG encoding of LDA topicmodels that learns some of the structure of propernames.
The key idea is that elements in propernames typically appear in a fixed order; we expecthonorifics to appear before first names, which ap-pear before middle names, which in turn appearbefore surnames, etc.
Similarly, many companynames end in fixed phrases such as Inc. Herewe think of first names as a kind of topic, albeitone with a restricted positional location.
One ofthe challenges is that some of these structural ele-ments can be filled by multiword expressions; e.g.,de Groot can be a surname.
We deal with this bypermitting multi-word collocations to fill the cor-responding positions, and use the adaptor gram-mar machinery to learn these collocations.Inspired by the grammar presented in Elsneret al (2009), our adaptor grammar is as follows,where adapted nonterminals are indicated by un-derlining as before.NP?
(A0) (A1) .
.
.
(A6)NP?
(B0) (B1) .
.
.
(B6)NP?
Unordered+A0?Word+.
.
.A6?Word+B0?Word+.
.
.B6?Word+Unordered?Word+In this grammar parentheses indicate optional-ity, and the Kleene plus indicates iteration (thesewere manually expanded into ordinary CFG rulesin our experiments).
The grammar provides threedifferent expansions for proper names.
The firstexpansion says that a proper name can consist ofsome subset of the six different collocation classesA0 through A6 in that order, while the second ex-pansion says that a proper name can consist ofsome subset of the collocation classes B0 throughB6, again in that order.
Finally, the third expan-sion says that a proper name can consist of an ar-bitrary sequence of ?unordered?
collocations (thisis intended as a ?catch-all?
expansion to provideanalyses for proper names that don?t fit either ofthe first two expansions).We extracted all of the proper names (i.e.,phrases of category NNP and NNPS) in the PennWSJ treebank and used them as the training cor-pora for the adaptor grammar just described.
Theadaptor grammar inference procedure found skele-tal sample parses such as the following:(A0 barrett) (A3 smith)(A0 albert) (A2 j.)
(A3 smith) (A4 jr.)(A0 robert) (A2 b.)
(A3 van dover)(B0 aim) (B1 prime rate) (B2 plus) (B5fund) (B6 inc.)(B0 balfour) (B1 maclaine) (B5 interna-tional) (B6 ltd.)(B0 american express) (B1 informationservices) (B6 co)(U abc) (U sports)(U sports illustrated)(U sports unlimited)While a full evaluation will have to await furtherstudy, in general it seems to distinguish personnames from company names reasonably reliably,and it seems to have discovered that person namesconsist of a first name (A0), a middle name or ini-tial (A2), a surname (A3) and an optional suffix(A4).
Similarly, it seems to have uncovered thatcompany names typically end in a phrase such asinc, ltd or co.8 ConclusionThis paper establishes a connection between twovery different kinds of probabilistic models; LDAmodels of the kind used for topic modelling, andPCFGs, which are a standard model of hierarchi-cal structure in language.
The embedding we pre-sented shows how to express an LDA model as aPCFG, and has the property that Bayesian infer-ence of the parameters of that PCFG produces anequivalent model to that produced by Bayesian in-ference of the LDA model?s parameters.The primary value of this embedding is theoret-ical rather than practical; we are not advocatingthe use of PCFG estimation procedures to inferLDA models.
Instead, we claim that the embed-ding suggests novel extensions to both the LDAtopic models and PCFG-style grammars.
We jus-tified this claim by presenting several hybrid mod-els that combine aspects of both topic models and1155grammars.
We don?t claim that these are neces-sarily the best models for performing any particu-lar tasks; rather, we present them as examples ofmodels inspired by a combination of PCFGs andLDA topic models.
We showed how the LDAto PCFG embedding suggested a ?sticky topic?model extension to LDA.
We then discussed adap-tor grammars, and inspired by the LDA topic mod-els, presented a novel topic model whose prim-itive elements are multi-word collocations ratherthan words.
We concluded with an adaptor gram-mar that learns aspects of the internal structure ofproper names.AcknowledgmentsThis research was funded by US NSF awards0544127 and 0631667, as well as by a start-upaward from Macquarie University.
I?d like tothank the organisers and audience at the TopicModeling workshop at NIPS 2009, my former col-leagues at Brown University (especially EugeneCharniak, Micha Elsner, Sharon Goldwater, TomGriffiths and Erik Sudderth), my new colleaguesat Macquarie University and the ACL reviewersfor their excellent suggestions and comments onthis work.
Naturally all errors remain my own.ReferencesM.J.
Beal, Z. Ghahramani, and C.E.
Rasmussen.
2002.The infinite Hidden Markov Model.
In T. Dietterich,S.
Becker, and Z. Ghahramani, editors, Advances inNeural Information Processing Systems, volume 14,pages 577?584.
The MIT Press.N.
Bernstein-Ratner.
1987.
The phonology of parent-child speech.
In K. Nelson and A. van Kleeck,editors, Children?s Language, volume 6.
Erlbaum,Hillsdale, NJ.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Jeffrey Elman.
1990.
Finding structure in time.
Cog-nitive Science, 14:197?211.Micha Elsner, Eugene Charniak, and Mark Johnson.2009.
Structured generative models for unsuper-vised named-entity clustering.
In Proceedings ofHuman Language Technologies: The 2009 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, pages164?172, Boulder, Colorado, June.
Association forComputational Linguistics.E.
Fox, E. Sudderth, M. Jordan, and A. Willsky.
2008.An HDP-HMM for systems with state persistence.In Andrew McCallum and Sam Roweis, editors,Proceedings of the 25th Annual International Con-ference on Machine Learning (ICML 2008), pages312?319.
Omnipress.Sharon Goldwater, Tom Griffiths, and Mark John-son.
2006.
Interpolating between types and tokensby estimating power-law generators.
In Y. Weiss,B.
Scho?lkopf, and J. Platt, editors, Advances in Neu-ral Information Processing Systems 18, pages 459?466, Cambridge, MA.
MIT Press.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101:52285235.Thomas L. Griffiths, Mark Steyvers, and Joshua B.Tenenbaum.
2007.
Topics in semantic representa-tion.
Psychological Review, 114(2):211244.Mark Johnson and Sharon Goldwater.
2009.
Im-proving nonparameteric Bayesian inference: exper-iments on unsupervised word segmentation withadaptor grammars.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associa-tion for Computational Linguistics, pages 317?325,Boulder, Colorado, June.
Association for Computa-tional Linguistics.Mark Johnson, Thomas Griffiths, and Sharon Gold-water.
2007a.
Bayesian inference for PCFGs viaMarkov chain Monte Carlo.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 139?146, Rochester, New York, April.Association for Computational Linguistics.Mark Johnson, Thomas L. Griffiths, and Sharon Gold-water.
2007b.
Adaptor Grammars: A framework forspecifying compositional nonparametric Bayesianmodels.
In B. Scho?lkopf, J. Platt, and T. Hoffman,editors, Advances in Neural Information ProcessingSystems 19, pages 641?648.
MIT Press, Cambridge,MA.Mark Johnson.
2008.
Using adaptor grammars to iden-tifying synergies in the unsupervised acquisition oflinguistic structure.
In Proceedings of the 46th An-nual Meeting of the Association of ComputationalLinguistics, Columbus, Ohio.
Association for Com-putational Linguistics.Kenichi Kurihara and Taisuke Sato.
2006.
VariationalBayesian grammar induction for natural language.In 8th International Colloquium on Grammatical In-ference.Percy Liang, Slav Petrov, Michael Jordan, and DanKlein.
2007.
The infinite PCFG using hierarchi-cal Dirichlet processes.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 688?697.1156Percy Liang, Michael Jordan, and Dan Klein.
2009.Probabilistic grammars and hierarchical Dirichletprocesses.
In The Oxford Handbook of AppliedBayesian Analysis.
Oxford University Press.Michell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 404?411, Rochester, New York.
Associ-ation for Computational Linguistics.Y.
W. Teh, M. Jordan, M. Beal, and D. Blei.
2006.
Hi-erarchical Dirichlet processes.
Journal of the Amer-ican Statistical Association, 101:1566?1581.Xuerui Wang, Andrew McCallum, and Xing Wei.2007.
Topical n-grams: Phrase and topic discovery,with an application to information retrieval.
In Pro-ceedings of the 7th IEEE International Conferenceon Data Mining (ICDM), pages 697?702.C.S.
Wetherell.
1980.
Probabilistic languages: A re-view and some open questions.
Computing Surveys,12:361?379.1157
