Proceedings of NAACL-HLT 2015, pages 86?90,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsA Concrete Chinese NLP PipelineNanyun Peng, Francis Ferraro, Mo Yu, Nicholas Andrews,Jay DeYoung, Max Thomas, Matthew R. Gormley, Travis Wolfe,Craig Harman, Benjamin Van Durme, Mark DredzeHuman Language Technology Center of ExcellenceJohns Hopkins University, Baltimore, Maryland USAAbstractNatural language processing research increas-ingly relies on the output of a variety of syn-tactic and semantic analytics.
Yet integratingoutput from multiple analytics into a singleframework can be time consuming and slowresearch progress.
We present a CONCRETEChinese NLP Pipeline: an NLP stack builtusing a series of open source systems inte-grated based on the CONCRETE data schema.Our pipeline includes data ingest, word seg-mentation, part of speech tagging, parsing,named entity recognition, relation extractionand cross document coreference resolution.Additionally, we integrate a tool for visualiz-ing these annotations as well as allowing forthe manual annotation of new data.
We releaseour pipeline to the research community to fa-cilitate work on Chinese language tasks thatrequire rich linguistic annotations.1 IntroductionOver the past few years, the natural language pro-cessing community has shifted its attention towardsthe Chinese language, with numerous papers cover-ing a range of NLP tasks for Chinese.
Last year?sEMNLP and ACL alone featured two dozen papersfocused primarily on Chinese data1, not includingmany others that considered Chinese language datawithin a broader context.
The large number of Chi-nese speakers, coupled with the unique challengesof Chinese compared to well studied Romance and1Excluding the Chinese Restaurant Process.Germanic languages, have driven these research ef-forts.
This focus has given rise to new NLP sys-tems that enable the automated processing of Chi-nese data.
While some pipelines cover multipletasks, such as Stanford CoreNLP (Manning et al,2014), other tasks such as relation extraction are notincluded.Modern NLP research, including research fo-cused on Chinese, often relies on automatically pro-duced analytics, or annotations, from multiple stagesof linguistic analysis.
Downstream systems, such assentiment analysis and question answering, assumethat data has been pre-processed by a variety of syn-tactic and semantic analytics.
Consider the task ofknowledge base population (KBP), in which infor-mation is extracted from text corpora for inclusionin a knowledge base.
Associated information ex-traction systems rely on various NLP analytics runon the data of interest, such as relation extractorsthat require the identification of named entities andsyntactically parsed text.
Similarly, entity linkingtypically assumes the presence of within documentcoreference resolution, named entity identificationand relation extraction.
These analytics themselvesrely on other core NLP systems, such as part ofspeech tagging and syntactic parsing.While each of these tasks have received exten-sive attention and have associated research softwarefor producing annotations, the output of these com-ponents must be integrated into a single cohesiveframework for use in a downstream task.
This inte-gration faces a wide variety of challenges resultingfrom the simple fact that most research systems aredesigned to produce good performance on an eval-86uation metric, but are not designed for integrationin a pipeline.
Beyond the production of integratedNLP pipelines, research groups often produce re-sources of corpora annotated by multiple systems,such as the Annotated Gigaword Corpus (Napoleset al, 2012).
Effective sharing of these corpora re-quires a common standard.These factors lead to the recent development ofCONCRETE, a data schema that represents numeroustypes of linguistic annotations produced by a varietyof NLP systems (Ferraro et al, 2014).
CONCRETEenables interoperability between NLP systems, fa-cilitates the development of large scale research sys-tems, and aids sharing of richly annotated corpora.This paper describes a Chinese NLP pipeline thatingests Chinese text to produce richly annotateddata.
The pipeline relies on existing Chinese NLPsystems that encompass a variety of syntactic andsemantic tasks.
Our pipeline is built on the CON-CRETE data schema to produce output in a struc-tured, coherent and shareable format.
To be clear,our goal is not the development of new methods orresearch systems.
Rather, our focus is the integra-tion of multiple tools into a single pipeline.
The ad-vantages of this newly integrated pipeline lie in thefact that the components of the pipeline communi-cate through a unified data schema: CONCRETE.
Bydoing so, we can?
easily switch each component of the pipeline toany state-of-the-art model;?
keep several annotations of the same type gen-erated by different tools; and?
easily share the annotated corpora.Furthermore, we integrate a visualization tool forviewing and editing the annotated corpora.
We positall the above benefits as the contributions of this pa-per and hope the efforts can facilitate ongoing Chi-nese focused research and aid in the constructionand distribution of annotated corpora.
Our code isavailable at http://hltcoe.github.io.2 The CONCRETE Data SchemaWe use CONCRETE, a recently introduced dataschema designed to capture and layer many differ-ent types of NLP output (Ferraro et al, 2014).2Aprimary purpose of CONCRETE is to ease analyticpipelining.
Based on Apache Thrift (Slee et al,2007), it captures NLP output via a number of inter-working structs, which are translated automaticallyinto in-memory representations for many commonprogramming languages, including Java, C++ andPython.
In addition to being, in practice, language-agnostic, CONCRETE and Thrift try to limit pro-grammer error: Thrift generates I/O libraries, mak-ing it easy for analytics to read and write CON-CRETE files; with this common format and I/O li-braries, developers can more easily share NLP out-put.
Unlike XML or JSON, Thrift?s automatic val-idation of strongly typed annotations help ensurelegitimate annotations: developers cannot acciden-tally populate a field with the wrong type of object,nor must they manually cast values.CONCRETE allows both within-document andcross-document annotations.
The former includesstandard tagging tasks (e.g., NER or POS), syn-tactic parses, relation extraction and entity corefer-ence, though Ferraro et al (2014) show how CON-CRETE can capture deeper semantics, such as framesemantic parses and semantic roles.
These within-document annotations, such as entity coref, can formthe basis of cross-document annotations.We chose CONCRETE as our data schema to sup-port as many NLP analytics as possible.
In thefuture, we plan to add additional analytics to ourpipeline, and we expect other research groups to in-tegrate their own tools.
A flexible and well docu-mented data schema is critical for these goals.
Fur-thermore, the release of multiple corpora in CON-CRETE (Ferraro et al, 2014) support our goal offacilitating the construction and distribution of newChinese corpora.3 Analytic PipelineWe describe each stage of our pipeline with a briefdescription of the associated tool and relevant detailsof its integration into the pipeline.2CONCRETE, language interfaces, and utility libraries areopen-source projects (https://hltcoe.github.io/).87(a) The basic visualization of a Communication.Each line is a tokenized sentence, with options to viewthe part of speech, constituency and dependency parse,and entity relation information.
(b) Multiple types of annotations can be viewed simul-taneously.
Here, entity information is laid atop a depen-dency parse.
A particular mention-of-interest is shownin yellow, with all other mentions in pink.Figure 1: CONCRETE Communication containing Chinese text displayed in Quicklime (section 3.7).3.1 Data IngestThe first stage of our pipeline requires in-gesting existing Chinese text into CONCRETECommunication objects, the core document rep-resentation of CONCRETE.
The existing CONCRETEJava and Python utilities support ingesting raw textfiles.
Part of this process requires not only ingestingthe raw text, but identifying section (paragraph) andsentence boundaries.Not all corpora contain raw text, as many corporacome with existing manual (or automatic) linguis-tic annotations.
We provide code to support twodata formats of existing Chinese corpora: the Chi-nese ACE 2005 relation extraction dataset (Walkeret al, 2006) and the new Chinese Entities, Rela-tions, and Events (ERE) dataset (Consortium, 2013).Both data sets include annotations for entities anda variety of relations (Aguilar et al, 2014).
Thelabeled entities and relations are represented byCONCRETE EntityMentions and stored in aEntityMentionSetList.
Additional annota-tions that are typically utilized by relation extractionsystems, such as syntactic parses, are provided auto-matically by the pipeline.3.2 Word SegmentationChinese text processing requires the identification ofword boundaries, which are not indicated in writ-ten Chinese as they are in most other languages.Our word segmentation is provided by the Stan-ford CoreNLP3(Manning et al, 2014) Chineseword segmentation tool, which is a conditional ran-dom field (CRF) model with character based fea-tures and lexicon features according to Chang et al(2008).
Word segmentations decisions are repre-sented by CONCRETE Token objects and stored inthe TokenList.
We follow the Chinese Penn Tree-bank segmentation standard (Xue et al, 2005).
Oursystem tracks token offsets so that segmentation isrobust to unexpected spaces or line breaks within aChinese word.3.3 SyntaxPart of speech tagging and syntactic parsing are alsoprovided by Stanford CoreNLP.
The part of speechtagger is based on Toutanova et al (2003) adaptedfor Chinese, which is a log-linear model under-neath.
Integration with CONCRETE was facilitatedby the concrete-stanford library4, though support-ing Chinese required significant modifications to the3http://nlp.stanford.edu/software/corenlp.shtml4https://github.com/hltcoe/concrete-stanford88library.
Resulting tags are stored in a CONCRETETokenTaggingList.Syntactic constituency parsing is based on themodel of Klein and Manning (2003) adaptedfor Chinese.
We obtained dependency parsesfrom the CoreNLP dependency converter.
Westore the constituency parses as a CONCRETEParse, and the dependency analyses as CON-CRETE DependencyParses.3.4 Named Entity RecognitionWe support the two most common named entity an-notation standards: the CoNLL standard (four types:person, organization, location and miscellaneous),and the ACE standard, which includes the additionaltypes of geo-political entity, facility, weapon and ve-hicle.
The ACE standard also includes support fornested entities.
We used the Stanford CoreNLP NERtoolkit which is a CRF model based on the methodin Finkel et al (2005), plus features based on Brownclustering.
For the CoNLL standard annotations, weuse one CRF model to label all the four types of en-tities.
For the ACE standard annotations, in orderto deal with the nested cases, we build one taggerfor each entity type.
Each entity is stored in a CON-CRETE EntityMention.3.5 Relation ExtractionRelations are extracted for every pair of entity men-tions.
We use a log-linear model with both tra-ditional hand-crafted features and word embeddingfeatures.
The hand-crafted features include all thebaseline features of Zhou et al (2005) (excluding theCountry gazeteer and WordNet features), plus sev-eral additional carefully-chosen features that havebeen highly tuned for ACE-style relation extrac-tion over years of research (Sun et al, 2011).
Theembedding-based features are from Yu et al (2014),which represent each word as the outer product be-tween its word embedding and a list of its asso-ciated non-lexical features.
The non-lexical fea-tures indicate the word?s relative positions compar-ing to the target entities (whether the word is thehead of any target entity, in-between the two enti-ties, or on the dependency path between entities),which improve the expressive strength of word em-beddings.
We store the extracted relations in CON-CRETE SituationMentions.
See Figure 2 forFigure 2: ACE entity relations viewed throughQuicklime (Section 3.7).an example visualization.3.6 Cross Document Coreference ResolutionCross document coreference resolution is performedvia the phylogenetic entity clustering model ofAndrews et al (2014).5Since the method isfully unsupervised we did not require a Chinesespecific model.
We use this system to clusterEntityMentions and store the clustering in toplevel CONCRETE Clustering objects.3.7 Creating Manual AnnotationsQuicklime6is a browser-based tool for viewing andediting NLP annotations stored in a CONCRETEdocument.
Quicklime supports a wide array of ana-lytics, including parse trees, token taggings, entities,mentions, and ?situations?
(e.g.
relations.)
Quick-lime uses the visualization layer of BRAT (Stenetorpet al, 2012) to display some annotations but does notuse the BRAT annotation editing layer.
BRAT anno-tations are stored in a standoff file format, whereasQuicklime reads and writes CONCRETE objects us-ing the Thrift JavaScript APIs.
Figure 1 showsQuicklime displaying annotations on Chinese data.In particular, Quicklime can combine and overlaymultiple annotations, such as entity extraction anddependency parses, as in Figure 1b.
Figure 2 showsentity relation annotations.AcknowledgmentsWe would like to thank the reviewers for their help-ful comments and perspectives.
A National ScienceFoundation Graduate Research Fellowship, under5https://bitbucket.org/noandrews/phyloinf6https://github.com/hltcoe/quicklime89Grant No.
DGE-1232825, supported the second au-thor.
Any opinions expressed in this work are thoseof the authors.ReferencesJacqueline Aguilar, Charley Beller, Paul McNamee, andBenjamin Van Durme.
2014.
A comparison ofthe events and relations across ace, ere, tac-kbp, andframenet annotation standards.
ACL 2014, page 45.Nicholas Andrews, Jason Eisner, and Mark Dredze.2014.
Robust entity clustering via phylogenetic infer-ence.
In Association for Computational Linguistics.Pi-Chuan Chang, Michel Galley, and Christopher D Man-ning.
2008.
Optimizing chinese word segmentationfor machine translation performance.
In Third Work-shop on Statistical Machine Translation.Linguistic Data Consortium.
2013.
DEFT ERE annota-tion guidelines: Events v1.1.Francis Ferraro, Max Thomas, Matthew R. Gormley,Travis Wolfe, Craig Harman, and Benjamin VanDurme.
2014.
Concretely Annotated Corpora.
InAKBC Workshop at NIPS.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In ACL.Dan Klein and Christopher D Manning.
2003.
Accurateunlexicalized parsing.
In ACL.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In ACL: Demos.Courtney Napoles, Matthew Gormley, and Benjamin VanDurme.
2012.
Annotated gigaword.
In AKBC-WEKEX Workshop at NAACL 2012.Mark Slee, Aditya Agarwal, and Marc Kwiatkowski.2007.
Thrift: Scalable cross-language services imple-mentation.
Facebook White Paper.Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c, SophiaAnaniadou, and Akiko Aizawa.
2012.
Normalisationwith the brat rapid annotation tool.
In InternationalSymposium on Semantic Mining in Biomedicine.Ang Sun, Ralph Grishman, and Satoshi Sekine.
2011.Semi-supervised relation extraction with large-scaleword clustering.
In ACL.Kristina Toutanova, Dan Klein, Christopher D Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In NAACL.Christopher Walker, Stephanie Strassel, Julie Medero,and Kazuaki Maeda.
2006.
Ace 2005 multilingualtraining corpus ldc2006t06.
Linguistic Data Consor-tium.Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta Palmer.2005.
The penn chinese treebank: Phrase structure an-notation of a large corpus.
Natural language engineer-ing, 11(02):207?238.Mo Yu, Matthew Gormley, and Mark Dredze.
2014.Factor-based compositional embedding models.
InNIPS Workshop on Learning Semantics.GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.2005.
Exploring various knowledge in relation extrac-tion.
In ACL, pages 427?434.90
