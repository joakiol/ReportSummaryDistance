Word Sense Acquisition from Bilingual Comparable CorporaHiroyuki KajiCentral Research Laboratory, Hitachi, Ltd.1-280 Higashi-Koigakubo, Kokubunji-shi, Tokyo 185-8601, Japankaji@crl.hitachi.co.jpAbstractManually constructing an inventory of wordsenses has suffered from problems includinghigh cost, arbitrary assignment of meaning towords, and mismatch to domains.
To over-come these problems, we propose a methodto assign word meaning from a bilingualcomparable corpus and a bilingual dictionary.It clusters second-language translationequivalents of a first-language target word onthe basis of their translingually aligned dis-tribution patterns.
Thus it produces a hierar-chy of corpus-relevant meanings of the targetword, each of which is defined with a set oftranslation equivalents.
The effectiveness ofthe method has been demonstrated through anexperiment using a comparable corpus con-sisting of Wall Street Journal and Nihon Kei-zai Shimbun corpora together with the EDRbilingual dictionary.1  IntroductionWord Sense Disambiguation (WSD) is an importantsubtask that is necessary for accomplishing most natu-ral language processing tasks including machinetranslation and information retrieval.
A great deal ofresearch on WSD has been done over the past decade(Ide and Veronis, 1998).
In contrast, word sense acqui-sition has been a human activity; inventories of wordsenses have been constructed by lexicographers basedon their intuition.
Manually constructing an inventoryof word senses has suffered from problems such ashigh cost, arbitrary division of word senses, and mis-match to application domains.We address the problem of word sense acquisitionalong the lines of the WSD where word senses aredefined with sets of translation equivalents in anotherlanguage.
Bilingual corpora or second-language cor-pora enable unsupervised WSD (Brown, et al, 1991;Dagan and Itai, 1994).
However, the correspondencebetween senses of a word and its translations is notone-to-one, and therefore we need to prepare an in-ventory of word senses, each of which is defined witha set of synonymous translation equivalents.
Althoughconventional bilingual dictionaries usually grouptranslations according to their senses, the groupingdiffers by dictionary.
In addition, senses specific to adomain are often missing while many senses irrelevantto the domain or rare senses are included.
To over-come these problems, we propose a method for pro-ducing a hierarchy of clusters of translation equiva-lents from a bilingual corpus and a bilingual diction-ary.To the best of our knowledge, there are two pre-ceding research papers on word sense acquisition (Fu-kumoto and Tsujii, 1994; Pantel and Lin, 2002).
Bothproposed distributional word clustering algorithms thatare characterized by their capabilities to produceoverlapping clusters.
According to their algorithms, apolysemous word is assigned to multiple clusters, eachof which represents one of its senses.
These and ourapproach differ in how to define the word sense, i.e., aset of synonyms in the same language versus a set oftranslation equivalents in another language.
Schuetze(1998) proposed a method for dividing occurrences ofa word into classes, each of which consists of contex-tually similar occurrences.
However, it does not pro-duce definitions of senses such as sets of synonymsand sets of translation equivalents.2  Basic Idea2.1  Clustering of translation equivalentsMost work on automatic extraction of synonyms fromtext corpora rests on the idea that synonyms haveEdmonton, May-June 2003Main Papers , pp.
32-39Proceedings of HLT-NAACL 2003similar distribution patterns (Hindle, 1990; Peraira, etal., 1993; Grefenstette, 1994).
This idea is also usefulfor our task, i.e., extracting sets of synonymous trans-lation equivalents, and we adopt the approach to dis-tributional word clustering.We need to mention that the singularity of our taskmakes the problem easier.
First, we do not have tocluster all words of a language, but we only have tocluster a small number of translation equivalents foreach target word, whose senses are to be extracted,separately.
As a result, the problem of computationalefficiency becomes less serious.
Second, even if atranslation equivalent itself is polysemous, it is notnecessary to consider senses that are irrelevant to thetarget word.
A translation equivalent usually representsone and only one sense of the target word, at least incase the language-pair is those with different originslike English and Japanese.
Therefore, anon-overlapping clustering algorithm, which is farsimpler than overlapping clustering algorithms, is suf-ficient.2.2  Translingual distributional word clusteringIn conventional distributional word clustering, a wordis characterized by a vector or weighted set consistingof words in the same language as that of the word it-self.
In contrast, we propose a translingual distribu-tional word clustering method, whereby a word ischaracterized by a vector or weighted set consisting ofwords in another language.
It is based on thesense-vs.-clue correlation matrix calculation methodwe originally developed for unsupervised WSD (Kajiand Morimoto, 2002).
That method presupposes thateach sense of a target word x is defined with a syno-nym set consisting of the target word itself and one ormore translation equivalents which represent the sense.It calculates correlations between the senses of x andthe words statistically related to x, which act as cluesfor determining the sense of x, on the basis oftranslingual alignment of pairs of related words.
Rowsof the resultant correlation matrix are regarded astranslingual distribution patterns characterizing trans-lation equivalents.Sense-vs.-clue correlation matrix calculationmethod *)1) Alignment of pairs of related words*) A description of the wild-card pair of related words, which playsan essential role in recovering alignment failure, has been omittedfor simplicity.Let X(x) be the set of clues for determining the sen-se of a first-language target word x.
That is,X(x)={x?|(x, x?
)?RX},where RX denotes the collection of pairs of relatedwords extracted from a corpus of the first language.Henceforth, the j-th clue for determining the sense of xwill be denoted as x?(j).
Furthermore, let Y(x, x?
(j)) bethe set consisting of all second-language counterpartsof a first-language pair of related words x and x?
(j).That is,Y(x, x?
(j)) = {(y, y?)
| (y, y?
)?RY, (x, y)?D,(x?
(j), y?
)?D},where RY denotes the collection of pairs of relatedwords extracted from a corpus of the second language,and D denotes a bilingual dictionary, i.e., a collectionof pairs consisting of a first-language word and asecond-language word that are translations of one an-other.Then, for each alignment, i.e., pair of (x, x?
(j)) and(y, y?)
(?Y(x, x?
(j))), a weighted set of common re-lated words Z((x, x?
(j)), (y, y? ))
is constructed as fol-lows:Z((x, x?
(j)), (y, y? ))
= {x?
/ w(x?)
| (x, x?)?RX,(x?
(j), x?
)?RX}.The weight of x?, denoted as w(x?
), is determined asfollows:- w(x?)
= 1+?
?MI(y, y?)
when ?y?
(x?, y?
)?D,(y, y?
)?RY, and (y?, y?
)?RY .- w(x?)
= 1 otherwise.This is where MI(y, y?)
is the mutual information of yand y?.
The coefficient ?
was set to 5 in the experimentdescribed in Section 4.2) Calculation of correlation between senses and cluesThe correlation between the i-th sense S(x, i) andthe j-th clue x?
(j) is defined as:( ) ( )( ) ( )( )( ) ( )( ).),(,,,)(,maxmax),(,,,)(,max)(,)(),,(),()),(,(Y),(),()),(,(),(???????????=???
?kxSy'yjx'xAixSy'yjx'xAjx'xMIjx'ixSCkxSyjx'xy'ykixSyjx'xYy'yThis is where MI(x, x?
(j)) is the mutual information ofx and x?
(j), and A((x, x?
(j)), (y, y?
), S(x,i)), the plausi-bility of alignment of (x, x?
(j)) with (y, y?)
suggestingS(x, i), is defined as the weighted sum of the correla-tions between the sense and the common related words,i.e.,( )( ).),,()w(),(),,()),(,()),()),(,((??
?=y'yjx'xZx"x"ixSCx"ixSy'yjx'xAThe correlations between senses and clues are cal-culated iteratively with the following initial values:C0(S(x, i), x?
(j))=MI(x, x?(j)).
The number of iterationswas set to 6 in the experiment.
Figure 1 shows how thecorrelation values converge.Advantages of using translingually aligneddistribution patternsTranslingual distributional word clustering has advan-tages over conventional monolingual distributionalword clustering, when they are used to cluster transla-tion equivalents of a target word.
First, it avoids clus-ters being degraded by polysemous translation equiva-lents.
Let ?race?
be the target word.
One of itstranslation equivalents, ???
?<REESU>?, is a poly-semous word representing ?lace?
as well as ?race?.According to monolingual distributional word cluster-ing, ????<REESU>?
is characterized by a mixture ofthe distribution pattern for ????<REESU>?
repre-senting ?race?
and that for ????<REESU>?
repre-senting ?lace?, which often results in degraded clusters.In contrast, according to translingual distributionalword clustering, ????<REESU>?
is characterized bythe distribution pattern for the sense of ?race?
thatmeans ?competition?.Second, translingual distributional word clusteringcan exclude from the clusters translation equivalentsirrelevant to the corpus.
For example, a bilingual dic-tionary renders ???<TOKUCHOU>?
(?feature?)
as atranslation of ?race?, but that sense of ?race?
is usedinfrequently.
If it is the case in a given domain, ???<TOKUCHOU>?
has low correlation with most wordsrelated to ?race?, and can therefore be excluded fromany clusters.We should also mention the data-sparseness prob-lem that hampers distributional word clustering.
Gen-erally speaking, the problem becomes more difficult intranslingual distributional word clustering, since thesparseness of data in two languages is multiplied.However, the sense-vs.-clue correlation matrix calcu-lation method overcomes this difficulty; it calculatesthe correlations between senses and clues iteratively tosmooth out the sparse data.Translingual distributional word clustering can alsobe implemented on the basis of word-for-word align-ment of a parallel corpus.
However, availability oflarge parallel corpora is extremely limited.
In contrast,the sense-vs.-clue correlation calculation method ac-cepts comparable corpora which are available in manydomains.2.3  Similarity based on subordinate distribu-tion patternNaive translingual distributional word clustering basedon the sense-vs.-clue correlation matrix calculationmethod is outlined in the following steps:1) Define the sense of a target word by using eachtranslation equivalent.2) Calculate the sense-vs.-clue correlation matrix forthe set of senses resulting from step 1).3) Calculate similarities between senses on the basisof distribution patterns shown by the sense-vs.-cluecorrelation matrix.4) Cluster senses by using a hierarchical agglomera-tive clustering method, e.g., the group-averagemethod.However, this naive method is not effective be-cause some senses usually have duplicated definitionsin step 1) despite the fact that the sense-vs.-clue corre-lation matrix calculation algorithm presupposes a setof senses without duplicated definitions.
The algo-rithm is based on the ?one sense per collocation?
hy-pothesis, and it results in each clue having a high cor-relation with one and only one sense.
A clue can neverhave high correlations with two or more senses, evenwhen they are actually the same sense.
Consequently,synonymous translation equivalents do not necessarilyhave high similarity.Figure 2(a) shows parts of distribution patterns for0.00.51.01.52.02.50 1 2 3 4 5 6 7 8 9 10IterationCorrelationC(S1, brand) C(S2, brand)C(S3, brand) C(S1, woman)C(S2, woman) C(S3, woman)S1={promotion, ??
<SENDEN>, ?
?
???
?
?<PUROMOUSHON>,  ???
?<URIKOMI>, ?
}(?an activity intended to help sell a product?
)S2={promotion, ?
?<SHOUKAKU>, ??<SHOUSHIN>,?
?<TOUYOU>, ?
}(?advancement in rank or position?
)S3={promotion, ?
?<SHOUREI>, ?
?<SHINKOU>, ??<JOCHOU>,?
}(?action to help something develop or succeed?
)Figure 1.
Convergence of correlation betweensenses and clues.
{promotion, ?
?<SENDEN>}, {promotion, ??????
?<PUROMOUSHON>}, and {promotion, ???
?<URIKOMI>} all of which define the ?sales activity?sense of ?promotion?.
We see that most clues for se-lecting that sense have higher correlation with {pro-motion, ?
?<SENDEN>} than with {promotion, ??????
?<PUROMOUSHON>} and {promotion, ????<URIKOMI>}.
This is because ??
?<SENDEN>?is the most dominant translation equivalent of ?promo-tion?
in the corpus.To resolve the above problem, we calculated thesense-vs.-clue correlation matrix not only for the fullset of senses but also for the set of senses excludingone of these senses.
Excluding a definition of the sense,which includes the most dominant translation equiva-lent, allows most clues for selecting the sense to havethe highest correlations with another definition of thesame sense, which includes the second most dominanttranslation equivalent.
Figure 2(b) shows parts of dis-tribution patterns for {promotion, ??????
?<PUROMOUSHON>} and {promotion, ?
?
?
?<URIKOMI>} shown by the sense-vs.-clue correlationmatrix for the set of senses excluding {promotion, ??<SENDEN>}.
We see that most clues for selectingthe ?sales activity?
sense have higher correlations with{promotion, ??????
?<PUROMOUSHON>} thanwith {promotion, ????<URIKOMI>}.
This is be-cause ????????<PUROMOUSHON>?
is the sec-ond most dominant translation equivalent in the corpus.We also see that the distribution pattern for {promo-tion, ??????
?<PUROMOUSHON>} in Fig.
2(b) ismore similar to that for {promotion, ?
?<SENDEN>}in Fig.
2(a) than that for {promotion, ??????
?<PUROMOUSHON>} in Fig.
2(a).We call the distribution pattern for sense S2, result-ing from the sense-vs.-clue correlation matrix for theset of senses excluding sense S1, the distribution pat-tern for S2 subordinate to S1, while we call the distri-bution pattern for sense S2, resulting from thesense-vs.-clue correlation matrix for the full set ofsenses, simply the distribution pattern for S2.
We de-fine the similarity of S2 to S1 as the similarity of thedistribution pattern for S2 subordinate to S1 to the dis-tribution pattern for S1.Calculating the sense-vs.-clue correlation matrixfor a set of senses excluding one sense is of courseinsufficient since three or more translation equivalentsmay represent the same sense of the target word.
Weshould calculate the sense-vs.-clue correlation matricesboth for the full set of senses and for the set of sensesexcluding one of these senses again, after mergingsimilar senses into one.
Repeating these proceduresenables corpus-relevant but less dominant translationequivalents to be drawn up, while corpus-irrelevantones are never drawn up.
Thus, a hierarchy of cor-pus-relevant senses or clusters of corpus-relevanttranslation equivalents is produced.3  Proposed Method3.1  OutlineAs shown in Fig.
3, our method repeats the followingthree steps:1) Calculate sense-vs.-clue correlation matrices bothfor the full set of senses and for a set of senses ex-cluding each of these senses.2) Calculate similarities between senses on the basisof distribution patterns and subordinate distributionpatterns.3) Merge each pair of senses with high similarityinto one.The initial set of senses is given as ?
(x)={{x, y1}, {x,y2}, ?, {x, yN}} where x is a target word in the firstlanguage, and y1, y2, ?, and yN are translation equiva-lents of x in the second-language.
Translationequivalents that occur less frequently in the sec-ond-language corpus can be excluded from the initial01234567Acclaimadcampaignaffirmativeanalystsay AudiBatmanbrandBurger KingcareercerealCoca-ColaConrailCoorsLightCyrkdiscriminationemployeefilmGeneralHispanicindustry job labellast yearmanagementClueCorrelation{promotion, ?
?<SENDEN>}{promotion, ??????
?<PUROMOUSHON>}{promotion, ???
?<URIKOMI>}(a) Distribution patterns01234567ClueCorrelation(b) Distribution patterns subordinate to{promotion, ?
?<SENDEN>}Figure 2.
Distribution Patterns for Some Sensesof ?promotion?.set to shorten the processing time.
The details of thesteps are described in the following sections.3.2  Calculation of sense-vs.-clue correlationmatricesFirst, a sense-vs.-clue correlation matrix is calculatedfor the full set of senses.
The resulting correlation ma-trix is denoted as C. That is, C(i, j) is the correlationbetween the i-th sense S(x,i) of a target word x and itsj-th clue x?
(j).Then a set of active senses, ?A(x), is determined.
Asense is regarded active if and only if the ratio of clueswith which it has the highest correlation exceeds apredetermined threshold ?
(In the experiment in Sec-tion 4, ?
was set to 0.05).
That is,{ }?
))(()()( >= ix,SR|ix,Sx?A ,where R(S(x, i)) denotes the ratio of clues having thehighest correlation with S(x, i), i.e.,})({)},(max),()({)),(( jx'jkCjiC|jx'ixSRk== .Thus ?A(x) consists of senses of the target word x thatare relevant to the corpus.Finally, a sense-vs.-clue correlation matrix is cal-culated for the set of senses excluding each of the ac-tive senses.
The correlation matrix calculated for theset of senses excluding the k-th sense is denoted as C-k.That is, C-k(i, j) (i?k) is the correlation between thei-th sense and the j-th clue that is calculated excludingthe k-th sense.
C-k(k, j) (j=1, 2, ...) are set to zero.
Thisredundant k-th row is included to maintain the samecorrespondence between rows and senses as in C.3.3  Calculation of sense similarity matrixSimilarity of the i-th sense S(x, i) to the j-th sense S(x,j), Sim(S(x, i), S(x, j)), is defined as the similarity ofthe distribution pattern for S(x, i) subordinate to S(x, j)to the distribution pattern of S(x, j).
Note that thissimilarity is asymmetric and reflects which sense ismore dominant in the corpus.
It is probable thatSim(S(x, i), S(x, j)) is large but Sim(S(x, j), S(x, i)) isnot when S(x, j) is more dominant than S(x, i).According to the sense-vs.-clue correlation matrix,each sense is characterized by a weighted set of clues.Therefore, we used the weighted Jaccard coefficient asthe similarity measure.
That is,{ }{ }?
?=kj-kj-kj,Cki,Ckj,Cki,CjxSixSSim )(),(max)(),(min)),(),,((when S(x, j)?
?A(x).0)),(),,(( =jxSixSSim  otherwise.It should be noted that a sense is characterized by dif-ferent weighted sets of clues depending on whichsense the similarity is calculated.
Note also that inac-tive senses are neglected because they are not reliable.3.4  Merging similar sensesThe set of senses is updated by merging every pair ofmutually most-similar senses into one.
That is,?
(x) ?
?
(x) ?
{S(x, i), S(x, j)} + {S(x, i)?S(x, j)}if {maxmax)),(),,((j'jxSixSSim =))}},(),',(()),,(),,(({ ixSjxSSimj'xSixSSim ,{maxmax)),(),,((i'jxSixSSim =))}},(),,(()),,(),,(({ i'xSjxSSimjxSi'xSSim ,and ?>)),(),,(( jxSixSSim .The ?
is a predetermined threshold for similarity,which is introduced to avoid noisy pairs of senses be-ing merged.
In the experiment in Section 4, ?
was setto 0.25.If at least one pair of senses are merged, the wholeprocedure, i.e., the calculation of sense-vs.-clue ma-trices through the merger of similar senses, is repeatedfor the updated set of senses.
Otherwise, the clusteringprocedure terminates.Agglomerative clustering methods usually sufferfrom the problem of when to terminate merging.
In ourmethod described above, the similarity of senses thatare merged into one does not necessarily decreaseInitial set of sensesSense-vs.-clue correlation matricesSense similarity matrixUpdated set of sensesComparablecorpusBilingualdictionaryCalculate similaritiesbetween distribution patternsCalculate correlationsbetween senses and cluesMerge similar sensesFigure 3.
Flow Diagram of Proposed Method.monotonically, which makes the problem more diffi-cult.
At present, we are forced to output a dendrogramthat represents the history of mergers and leave thefinal decision to humans.
The dendrogram consists oftranslation equivalents that are included in activesenses in the final cycle.
Other translation equivalentsare rejected as they are irrelevant to the corpus.4  Experimental Evaluation4.1  Experimental settingsOur method was evaluated through an experiment us-ing a Wall Street Journal corpus (189 Mbytes) and aNihon Keizai Shimbun corpus (275 Mbytes).First, collected pairs of related words, which werestricted to nouns and unknown words, were obtainedfrom each corpus by extracting pairs of wordsco-occurring in a window, calculating mutual informa-tion of each pair of words, and selecting pairs withmutual information larger than the threshold.
The sizeof the window was 25 words excluding function words,and the threshold for mutual information was set tozero.
Second, a bilingual dictionary was prepared bycollecting pairs of nouns that were translations of oneanother from the Japan Electronic Dictionary ResearchInstitute (EDR) English-to-Japanese and Japa-nese-to-English dictionaries.
The resulting dictionaryincludes 633,000 pairs of 269,000 English nouns and276,000 Japanese nouns.Evaluating the performance of word sense acquisi-tion methods is not a trivial task.
First, we do not havea gold-standard sense inventory.
Even if we have one,we have difficulty mapping acquired senses onto thosein it.
Second, there is no way to establish the completeset of senses appearing in a large corpus.
Therefore,we evaluated our method on a limited number of targetwords as follows.We prepared a standard sense inventory by select-ing 60 English target words and defining an average of3.4 senses per target word manually.
The senses wererather coarse-grained; i.e., they nearly corresponded togroups of translation equivalents within the entries ofeveryday English-Japanese dictionaries.
We then sam-pled 100 instances per target word from the WallStreet Journal corpus, and we sense-tagged themmanually.
Thus, we estimated the ratios of the sensesin the training corpus for each target word.We defined two evaluative measures, recall ofsenses and accuracy of sense definitions.
The recall ofsenses is the proportion of senses with ratios not lessthan a threshold that are successfully extracted, and itvaries with change of the threshold.
We judged that asense was extracted, when it shared at least one trans-lation equivalent with some active sense in the finalcycle.To evaluate the accuracy of sense definitions whileavoiding mapping acquired senses onto those in thestandard sense inventory, we regard a set of senses as aset of pairs of synonymous translation equivalents.
LetTS be a set consisting of pairs of translation equiva-lents belonging to the same sense in the standard senseinventory.
Likewise, let T(k) be a set consisting ofpairs of translation equivalents belonging to the sameactive sense in the k-th cycle.
Further, let U be a set ofpairs of translation equivalents that are included inactive senses in the final cycle.
Recall and precision ofpairs of synonymous translation equivalents in the k-thcycle are defined as:UTkTTkRSS?
?=)()( .
)()()( kTkTTkP S ?= .Further, F-measure of pairs of synonymous translationequivalents in the k-th cycle is defined as:)()()()(2)( kPkRkPkRkF+?
?= .The F-measure indicates how well the set of activesenses coincides with the set of sense definitions in thestandard senses inventory.
Although the currentmethod cannot determine the optimum cycle, humanscan identify the set of appropriate senses from a hier-archy of senses at a glance.
Therefore, we define theaccuracy of sense definitions as the maximumF-measure in all cycles.4.2  Experimental resultsTo simplify the evaluation procedure, we clusteredtranslation equivalents that were used to define thesenses of each target word in the standard sense in-ventory, rather than clustering translation equivalentsrendered by the EDR bilingual dictionary.
The recallof senses for totally 201 senses of the 60 target wordswas:96% for senses with ratios not less than 25%,87% for senses with ratios not less than 5%, and78% for senses with ratios not less than 1%.The accuracy of sense definitions, averaged over the60 target words, was 77%.The computational efficiency of our methodproved to be acceptable.
It took 13 minutes per targetword on a HP9000 C200 workstation (CPU clock: 200MHz, memory: 32 MB) to produce a hierarchy ofclusters of translation equivalents.Some clustering results are shown in Fig.
4.
Thesedemonstrate that our proposed method shows a greatdeal of promise.
At the same time, evaluating the re-sults revealed its deficiencies.
The first of these lies inthe crucial role of the bilingual dictionary.
It is obviousthat a sense is never extracted if the translation equiva-lents representing it are not included in it.
Anexhaustive bilingual dictionary is therefore required.From this point of view, the EDR bilingual dictionaryis fairly good.
The second deficiency lies in the factthat it performs badly for low-frequency or non-topicalsenses.
For example, the sense of ?bar?
as the ?legalprofession?
was clearly extracted, but its sense as a?piece of solid material?
was not extracted.We also compared our method with two alterna-tives: monolingual distributional clustering mentionedin Section 2.2 and naive translingual clustering men-tioned in Section 2.3.
Figures 5(a), (b), and (c) showrespective examples of clustering obtained by ourmethod, the monolingual method, and the naivetranslingual method.
Comparing (a) with (b) revealsthe superiority of the translingual approach to themonolingual approach, and comparing (a) with (c)reveals the effectiveness of the subordinate distribu-tion pattern introduced in Section 2.3.
Note that delet-ing the corpus-irrelevant translation equivalents fromthe dendrograms in both (b) and (c) would not result inappropriate ones.5  DiscussionOur method has several practical advantages.
One ofthese is that it produces a corpus-dependent inventoryof word senses.
That is, the resulting inventory coversmost senses relevant to a domain, while it excludessenses irrelevant to the domain.Second, our method unifies word sense acquisitionwith word sense disambiguation.
The sense-vs.-cluecorrelation matrix is originally used for word sensedisambiguation.
Therefore, our method guarantees thatacquired senses can be distinguished by machines, andfurther it demonstrates the possibility of automaticallyoptimizing the granularity of word senses.Some limitations of the present methods are dis-cussed in the following with possible future extensions.First, our method produces a hierarchy of clusters butcannot produce a set of disjoint clusters.
It is very im-portant to terminate merging senses autonomouslyduring an appropriate cycle.
Comparing distributionpatterns (not subordinate ones) may be useful to ter-minate merging; senses characterized by complemen-tary distribution patterns should not be merged.Second, the present method assumes that eachtranslation equivalent represents one and only one sense of the target word, but this is not always the case.
[Target word]Resulting dendrogram(English equivalentother than targetword)[association]???????<KANKEI>??????????<KOUSAI>?
????<TEIKEI>??????????<KANREN>??????????<KYOUDOU>?????????<RENGOU>????????<KUMIAI>???????<KYOUKAI>??????<KAI>?????<DANTAI>(relation)(friendship)(cooperation)(relation)(cooperation)(federation)(society)(society)(society)(organization)[bar]????????<URIBA>??????????<KAUNTAA>?
????????<BAA>?
???????<SHOUHEKI>??????????<KOUSHI>??????????<HOUSOU>?????????<BENGOSHI>???????
?<HOUTEI>(shop)(counter)(saloon)(obstacle)(lattice)(legal profession)(lawyer)(law court)[discipline]????<KUNREN>??????<GAKKA>??
????<GAKUMON>???????<KYOUKA>???????<CHITSUJO>??
????<KISEI>??????<CHOUBATSU>??????<TOUSEI>????
?<KIRITSU>(training)(subject of study)(learning)(subject of study)(order)(regulation)(punishment)(control)(order)[measure]?????<SHAKUDO>?????????<RYOU>?
?????<SHISUU>?
???????<SHUDAN>?????????<TAISAKU>??
??????<KIJUN>????????<HOUREI>??????<GIAN>????
?<HOUAN>(gauge)(quantity)(index)(means)(counter plan)(standard)(law)(bill)(bill)[promotion]????????<TOUYOU>??????????<SHOUSHIN>??????????<URIKOMI>?????????????
<PUROMOUSHON>?????????<SENDEN>(elevation)(advancement)(sale)(advertisingcampaign)(advertisement)[traffic]?????<SHOUGYOU>??????<TORIHIKI>??????<BAIBAI>??????<TSUUKOU>??????<KOUTSUU>????
?<UNYU>(commerce)(trade)(bargain)(passage)(transport)(transport)Figure 4.
Examples of Clustering.A Japanese Katakana word resulting from translitera-tion of an English word sometimes represents multiplesenses of the English word.
It is necessary to detectand split translation equivalents representing morethan one sense of the target word.Third, not only are acquired senses rathercoarse-grained but also generic senses are difficult toacquire.
One of the reasons for this may be that werely on co-occurrence in the window.
The fact thatmost distributional word clustering methods use syn-tactic co-occurrence suggests that it is the most effec-tive tool for extracting pairs of related words.6  ConclusionWe presented a translingual distributional word clus-tering method enabling word senses, exactly a hierar-chy of clusters of translation equivalents, to be ac-quired from a comparable corpus and a bilingual dic-tionary.
Its effectiveness was demonstrated through anexperiment using Wall Street Journal and Nihon Kei-zai Shimbun corpora and the EDR bilingual dictionary.The recall of senses was 87% for senses whose ratiosin the corpus were not less than 5%, and the accuracyof sense definitions was 77%.Acknowledgments: This research was supported bythe New Energy and Industrial Technology Develop-ment Organization of Japan.ReferencesBrown, Peter F., Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1991.
Word-sense disam-biguation using statistical methods.
In Proceedings ofthe 29th Annual Meeting of the Association for Com-putational Linguistics, pages 264-270.Dagan, Ido and Alon Itai.
1994.
Word sense disambigua-tion using a second language monolingual corpus.Computational Linguistics, 20(4): 563-596.Fukumoto, Fumiyo and Junichi Tsujii.
1994.
Automaticrecognition of verbal polysemy.
In Proceedings of the15th International Conference on Computational Lin-guistics, pages 762-768.Grefenstette, Gregory.
1994.
Explorations in AutomaticThesaurus Discovery.
Kluwer Academic Publishers,Boston.Hindle, Donald.
1990.
Noun classification from predi-cate-argument structures.
In Proceedings of the 28thAnnual Meeting of the Association for ComputationalLinguistics, pages 268-275.Ide, Nancy and Jean Veronis.
1998.
Introduction to thespecial issue on word sense disambiguation: The stateof the art.
Computational Linguistics, 24(1): 1-40.Kaji, Hiroyuki and Yasutsugu Morimoto.
2002.
Unsuper-vised word sense disambiguation using bilingual com-parable corpora.
In Proceedings of the 19th Interna-tional Conference on Computational Linguistics, pages411-417.Pantel, Patrick and Dekang Lin.
2002.
Discovering wordsenses from text.
In Proceedings of the 8th ACMSIGKDD International Conference on Knowledge Dis-covery and Data Mining, pages 613-619.Pereira, Fernando, Naftali Tishby, and Lillian Lee.
1993.Distributional clustering of English words.
In Proceed-ings of the 31st Annual Meeting of the Association forComputational Linguistics, pages 183-190.Schuetze, Hinrich.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1): 97-124.[race]??????<KEIRIN>???????<KEIBA>???????<REESU>???????<KOKUMIN>???????<MINZOKU>?????
?<JINSHU>(cycle race)(horse race)(competition)(nation)(ethnic)(human race)(a) Proposed method[race]???????????<HI>????????????????<KYOUSOU>???
?????<REESU>???
????????????????<KEIBA>?
?
????????????<JINSHU>?
???????
?????????<SHISSOU>?
????????????<SERIAI>?
??????????<KOKUMIN>?
??????????????<HINKAKU>????????????????????<KEIRIN>??
??????<TOKUCHOU>??
?????????????<TOKUSEI>?
???????????????<HINSHU>?
???
????????<FUUMI>???
???????????????<SUIRO>?
???????<MINZOKU>????????????????
?<YOUSUI>(match)(competition)(competition)(horse race)(human race)(scamper)(competition)(nation)(dignity)(cycle race)(feature)(character)(kind)(flavor)(waterway)(ethnic)(water forirrigation)(b) Monolingual distributional clustering[race]?????????????????<KEIBA>???????????????????<REESU>??
???????????????<JINSHU>????????????????????<MINZOKU>?
??????<SERIAI>?
???????<SHISSOU>?
???????????<HINKAKU>?
?
???????<HI>?
????
????????<SUIRO>?
?
???????????<TOKUSEI>?
???
???????????<KYOUSOU>?
?
?
??????????<TOKUCHOU>?
???
??????????????<FUUMI>?
?
?
????????????<KEIRIN>???
?
??????????????<HINSHU>?
????????????????<YOUSUI>??????????????????
?<KOKUMIN>(horse race)(competition)(human race)(ethnic)(competition)(scamper)(dignity)(match)(waterway)(character)(competition)(feature)(flavor)(cycle race)(kind)(water forirrigation)(nation)(c) Naive translingual distributional clusteringFigure 5.
Comparison with Alternatives.
