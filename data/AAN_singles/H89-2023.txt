Data Collection and Evaluation IIRalph GrishmanNew York UniversityThis session consisted of four presentations concerning data collection and the evaluationof speech and text processing systems.The first presentation, by Dave Pallett, described an evaluation by Pallett, Fiscus, andGarofolo (National Institute for Standards and Technology) of the significance of differ-ences in performance of various speech understanding systems.
A number of sites whichare developing such systems have been using the DARPA Resource Management SpeechCorpora to evaluate their systems.
As performance gradually improves, and differences- -  between systems and between successive versions of a system - -  narrow, the questionarises as to whether eported ifferences are statistically significant.
To assess this ques-tion, the authors have implemented two statistical tests and applied them to the output ofseveral speech understanding systems.
They reported that, for speaker dependent sys-tems and systems using a word-pair grammar, differences were often not significant; forspeaker independent systems and systems not using a grammar, significant differenceswere found in many cases.Turing to issues of data collection, Mitch Marcus presented a report by Marcus, San-torini, and Magerman (Univ.
of Pennsylvania) of "First Steps Towards an AnnotatedDatabase of American English."
This is an effort to collect and annotate, over severalyears, substantial samples of written and spoken English.
Work in the first year (justbeginning) will be on written text.
Marcus described the development of guidelines andtools for the text annotation.
A small set of parts of speech (34) has been defined, anannotation workbench based on the Gnu Emacs editor has been developed, and a manualhas been prepared.
Experiments with several approaches to part-of-speech tagging indi-cated that statistically-based automatic tagging (based on work by Ken Church) followedby manual review was the most efficient and reliable.
Tagging rates of 20 minutes/1000words, with error rates of 3-4%, were reported.Mark Liberman then reported on efforts of the ACL (Association for ComputationalLinguistics) Data Collection Initiative.
This initiative aims to acquire and prepare a largetext corpus, to be made available without royalties for scientific research.
The text willbe formatted using SGML (the Standard Generalized Markup Language).
To dateseveral hundred million words of text have been collected, including material from news-papers, parliamentary ecords, literary works, technical abstracts, and reference works.An initial release of a subset of this material is planned for the near future.To close the session, Beth Sundheim of the Naval Ocean Systems Center reported onMUCK-H, the second Messsage Understanding Conference, held at NOSC (San Diego)in June 1989.
This conference was the first substantial effort at measuring and compar-ing the performance of message understanding systems.
The task given to these systemswas to identify - -  from brief narratives of naval encounters - -  certain types of critical1 7 1events, and to extract he agent, instrument, time, location, etc.
of each event.
A total of130 messages were prepared for training and evaluation, along with specifications of thecorrect output for each message.
Nine groups participated in the evaluation procedure.The results from the training and on-site evaluation test sets were briefly presented at thesession; more detailed results will be included in a forthcoming NOSC report.The session highlighted two trends among the work in computational linguistics.
First,the gradual matunng of evaluation efforts - -  in particular, the shift in the text processingdomain from 'proof of concept' to quantifiable valuation.
Second, the increasinginterest (noted by Mitch Marcus at several points in this conference) in the use of muchlarger data bases (of text, speech, and lexical information) as basic tools in computationallinguistics research.172
