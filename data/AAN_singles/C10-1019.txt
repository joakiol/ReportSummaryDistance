Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161?169,Beijing, August 2010Jointly Modeling WSD and SRL with Markov LogicWanxiang Che and Ting LiuResearch Center for Information RetrievalMOE-Microsoft Key Laboratory of Natural Language Processing and SpeechSchool of Computer Science and Technology{car, tliu}@ir.hit.edu.cnAbstractSemantic role labeling (SRL) and wordsense disambiguation (WSD) are two fun-damental tasks in natural language pro-cessing to find a sentence-level seman-tic representation.
To date, they havemostly been modeled in isolation.
How-ever, this approach neglects logical con-straints between them.
We therefore ex-ploit some pipeline systems which verifythe automatic all word sense disambigua-tion could help the semantic role label-ing and vice versa.
We further propose aMarkov logic model that jointly labels se-mantic roles and disambiguates all wordsenses.
By evaluating our model on theOntoNotes 3.0 data, we show that thisjoint approach leads to a higher perfor-mance for word sense disambiguation andsemantic role labeling than those pipelineapproaches.1 IntroductionSemantic role labeling (SRL) and word sense dis-ambiguation (WSD) are two fundamental tasks innatural language processing to find a sentence-level semantic representation.
Semantic role la-beling aims at identifying the relations betweenpredicates in a sentence and their associated ar-guments.
Word sense disambiguation is the pro-cess of identifying the correct meaning, or senseof a word in a given context.
For example, forthe sentence in Figure 1, we can find out that thepredicate token ?hitting?
at position 3 has sense?cause to move by striking?
and the sense label is?hit.01?.
The argument headed by the token ?cat?at position 1 with sense ?feline mammal?
(cat.01)is referring to the player (A0), and the argumentheaded by the token ?ball?
at position 5 with senseFigure 1: A sample of word sense disambiguationand semantic role labeling.
?round object that is hit in games?
(ball.01) is re-ferring to the game object (A1) being hit.Normally, semantic role labeling and wordsense disambiguation are regarded as two inde-pendent tasks, i.e., the word sense informationis rarely used in a semantic role labeling systemand vice versa.
A few researchers have used se-mantic roles to help the verb sense disambigua-tion (Dang and Palmer, 2005).
More people usedpredicate senses in semantic role labeling (Hajic?et al, 2009; Surdeanu et al, 2008).
However, bothof the pipeline methods ignore possible dependen-cies between the word senses and semantic roles,and can result in the error propagation problem.The same problem also appears in other naturallanguage processing tasks.In order to make different natural language pro-cessing tasks be able to help each other, jointlymodeling methods become popular recently, suchas joint Chinese word segmentation and part-of-speech tagging (Kruengkrai et al, 2009; Zhangand Clark, 2008; Jiang et al, 2008), joint lemma-tization and part-of-speech prediction (Toutanovaand Cherry, 2009), joint morphological segmenta-tion and syntactic parsing (Goldberg and Tsarfaty,2008), joint text and aspect ratings for sentimentsummarization (Titov and McDonald, 2008), andjoint parsing and named entity recognition (Finkeland Manning, 2009).
For semantic role label-ing, Dahlmeier et al (2009) proposed a methodto maximize the joint probability of the seman-161tic role of preposition phrases and the prepositionsense.In order to do better joint learning, a novelstatistical relational learning framework, Markovlogic (Domingos and Lowd, 2009) was intro-duced to join semantic role labeling and predicatesenses (Meza-Ruiz and Riedel, 2009).
Markovlogic combines the first order logic and Markovnetworks, to develop a joint probability modelover all related rules.
Global constraints (intro-duced by Punyakanok et al (2008)) among se-mantic roles can be easily added into Markovlogic.
And the more important, the jointly model-ing can be realized using Markov logic naturally.Besides predicates and prepositions, other wordsenses are also important information for recog-nizing semantic roles.
For example, if we know?cat?
is an ?agent?
of the predicate ?hit?
in a sen-tence, we can guess that ?dog?
can also be an?agent?
of ?hit?, though it does not appear in thetraining data.
Similarly, the semantic role infor-mation can also help to disambiguate word senses.In addition, the predicate sense and the argumentsense can also help each other.
In the sentence?The cat is hitting a ball.
?, if we know ?hit?
herehas a game related sense, we can guess that the?ball?
should have the sense ?is a round object ingames?.
In the same way, the correct ?ball?
sensecan help to disambiguate the sense of ?hit?.
Thejoint probability, that they are disambiguated cor-rectly simultaneously will be larger than other ab-normalities.The release of OntoNotes (Hovy et al, 2006)provides us an opportunity to jointly model allword senses disambiguation and semantic role la-beling.
OntoNotes is a large corpus annotatedwith constituency trees (based on Penn Tree-bank), predicate argument structures (based onPenn PropBank), all word senses, etc.
It has beenused in some natural language processing tasks,such as joint parsing and named entity recogni-tion (Finkel and Manning, 2009), and word sensedisambiguation (Zhong et al, 2008).In this paper, we first propose some pipelinesystems which exploit automatic all word sensedisambiguation into semantic role labeling taskand vice versa.
Then we present a Markov logicmodel which can easily express useful global con-straints and jointly disambiguate all word sensesand label semantic roles.Experiments on the OntoNotes 3.0 corpus showthat (1) the automatic all word sense disambigua-tion and semantic role labeling tasks can help eachother when using pipeline approaches, and moreimportant, (2) the joint approach using Markovlogic leads to higher accuracy for word sense dis-ambiguation and performance (F1) for semanticrole labeling than pipeline approaches.2 Related WorkJoint models were often used in semantic role la-beling community.
Toutanova et al (2008) andPunyakanok et al (2008) presented a re-rankingmodel and an integer linear programming modelrespectively to jointly learn a global optimal se-mantic roles assignment.
Besides jointly learningsemantic role assignment of different constituentsfor one task (semantic role labeling), their meth-ods have been used to jointly learn for two tasks(semantic role labeling and syntactic parsing).However, it is easy for the re-ranking model toloss the optimal result, if it is not included in thetop n results.
In addition, the integer linear pro-gramming model can only use hard constraints.
Alot of engineering work is also required in bothmodels.Recently, Markov logic (Domingos and Lowd,2009) became a hot framework for joint model.It has been successfully used in temporal relationsrecognition (Yoshikawa et al, 2009), co-referenceresolution (Poon and Domingos, 2008), etc.
Itis very easy to do joint modeling using Markovlogic.
The only work is to define relevant formu-las.
Meza-Ruiz and Riedel (2009) have joined se-mantic role labeling and predicate senses disam-biguation with Markov logic.The above idea, that the predicate senses andthe semantic role labeling can help each other,may be inspired by Hajic?
et al (2009), Surdeanuet al (2008), and Dang and Palmer (2005).
Theyhave shown that semantic role features are helpfulto disambiguate verb senses and vice versa.Besides predicate senses, Dahlmeier et al(2009) proposed a joint model to maximize prob-ability of the preposition senses and the semanticrole of prepositional phrases.162Except for predicate and preposition senses,Che et al (2010) explored all word senses for se-mantic role labeling.
They showed that all wordsenses can improve the semantic role labeling per-formance significantly.
However, the golden wordsenses were used in their experiments.
The resultsare still unknown when an automatic word sensedisambiguation system is used.In this paper, we not only use all word sensesdisambiguated by an automatic system, but alsomake the semantic role labeling results to helpword sense disambiguation synchronously with ajoint model.3 Markov LogicMarkov logic can be understood as a knowledgerepresentation with a weight attached to a first-order logic formula.
Let us describe Markovlogic in the case of the semantic role labelingtask.
We can model this task by first introduc-ing a set of logical predicates such as role(p, a, r)and lemma(i, l), which means that the argumentat position a has the role r with respect to thepredicate at position p and token at position i haslemma l respectively.
Then we specify a set ofweighted first order formulas that define a distri-bution over sets of ground atoms of these predi-cates (or so-called possible worlds).Ideally, the distribution we define with theseweighted formulas assigns high probability topossible worlds where semantic role labeling iscorrect and a low probability to worlds where thisis not the case.
For instance, for the sentencein Figure 1, a suitable set of weighted formulaswould assign a high probability to the world:lemma(1, cat), lemma(3, hit), lemma(5, ball)role(3, 1, A0), role(3, 5, A1)and low probabilities to other cases.A Markov logic network (MLN) M is a setof weighted formulas, i.e., a set of pairs (?, ?
),where ?
is a first order formula and ?
is the realweight of the formula.
M defines a probabilitydistribution over possible worlds:p(y) = 1Z exp(?(?,?)?M?
?c?C?f?c (y))where each c is a binding of free variables in ?to constants.
Each f?c is a binary feature functionthat returns 1 if the possible world y includes theground formula by replacing the free variables in?
with the constants in c is true, and 0 otherwise.C?
is the set of all bindings for the variables in ?.Z is a normalization constant.4 ModelWe divide our system into two stages: word sensedisambiguation and semantic role labeling.
Forcomparison, we can process them with pipelinestrategy, i.e., the word sense disambiguation re-sults are used in semantic role labeling or the se-mantic role labeling results are used in word sensedisambiguation.
Of course, we can jointly processthem with Markov logic easily.We define two hidden predicates for the twostages respectively.
For word sense disambigua-tion, we define the predicate sense(w, s) whichindicates that the word at position w has thesense s. For semantic role labeling, the predicaterole(p, a, r) is defined as mentioned in above.Different from Meza-Ruiz and Riedel (2009),which only used sense number as word senserepresentation, we use a triple (lemma, part-of-speech, sense num) to represent the word senses.
For example, (hit, v, 01) denotes that the verb?hit?
has sense number 01.
Obviously, our rep-resentation can distinguish different word senseswhich have the identical sense number.
In ad-dition, we use one argument classification stagewith predicate role to label semantic roles as Cheet al (2009).
Similarly, no argument identifica-tion stage is used in our model.
The approach canimprove the recall of the system.In addition to the hidden predicates, we defineobservable predicates to represent the informationavailable in the corpus.
Table 1 presents thesepredicates.4.1 Local FormulaA local formula means that its groundings relateany number of observed ground atoms to exactlyone hidden ground atom.
For examplelemma(p,+l1)?lemma(a,+l2)?
role(p, a,+r)163Predicates Descriptionword(i, w) Token i has word wpos(i, t) Token i has part-of-speech tlemma(i, l) Token i has lemma lchdpos(i, t) The part-of-speech string of to-ken i?s all children is tchddep(i, d) The dependency relation stringof token i?s all children is tfirstLemma(i, l) The leftmost lemma of a sub-tree rooted by token i is llastLemma(i, l) The rightmost lemma of a sub-tree rooted by token i is lposFrame(i, fr) fr is a part-of-speech frame attoken idep(h, a, de) The dependency relation be-tween an argument a and itshead h is deisPredicate(p) Token p is a predicateposPath(p, a, pa) The part-of-speech path be-tween a predicate p and an ar-gument a is padepPath(p, a, pa) The dependency relation pathbetween a predicate p and an ar-gument a is papathLen(p, a, le) The path length between a pred-icate p and an argument a is leposition(p, a, po) The relative position between apredicate p and an argument ais pofamily(p, a, fa) The family relation between apredicate p and an argument ais fawsdCand(i, t) Token i is a word sense disam-biguation candidate, here t is?v?
or ?n?uniqe(r) For a predicate, semantic role rcan only appear onceTable 1: Observable Predicates.means that if the predicate lemma at position pis l1 and the argument lemma at position a is l2,then the semantic role between the predicate andthe argument is r with some possibility.The + notation signifies that Markov logic gen-erates a separate formula and a separate weight foreach constant of the appropriate type, such as eachpossible pair of lemmas (l1, l2, r).
This type of?template-based?
formula generation can be per-formed automatically by a Markov logic engine,such as the thebeast1 system.The local formulas are based on features em-ployed in the state-of-the-art systems.
For wordsense disambiguation, we use the basic featuresmentioned by Zhong et al (2008).
The semanticrole labeling features are from Che et al (2009),1http://code.google.com/p/thebeast/Features SRL WSDLemma ?
?POS ?
?FirstwordLemma ?HeadwordLemma ?HeadwordPOS ?LastwordLemma ?POSPath ?PathLength ?Position ?PredicateLemma ?PredicatePOS ?RelationPath ?DepRelation ?POSUpPath ?POSFrame ?FamilyShip ?BagOfWords ?Window3OrderedWords ?Window3OrderedPOSs ?Table 2: Local Features.the best system of the CoNLL 2009 shared task.The final features are listed in Table 2.What follows are some simple examples in or-der to explain how we implement each feature asa formula (or a set of formulas).Consider the ?Position?
feature.
We first intro-duce a predicate position(p, a, po) that denotesthe relative position between predicate p and ar-gument a is po.
Then we add a formulaposition(p, a,+po)?
role(p, a,+r)for all possible combinations of position and rolerelations.The ?BagOfWords?
feature means that thesense of a word w is determined by all of lemmasin a sentence.
Then, we add the following formulaset:wsdCand(w,+tw) ?
lemma(w,+lw) ?
lemma(1,+l1) ?
sense(w,+s).
.
.wsdCand(w,+tw) ?
lemma(w,+lw) ?
lemma(2,+li) ?
sense(w,+s).
.
.wsdCand(w,+tw) ?
lemma(w,+lw) ?
lemma(n,+ln) ?
sense(w,+s)where, the w is the position of current word andtw is its part-of-speech tag, lw is its lemma.
liis the lemma of token i.
There are n tokens in asentence totally.4.2 Global FormulaGlobal formulas relate more than one hiddenground atoms.
We use this type of formula fortwo purposes:1641.
To capture the global constraints among dif-ferent semantic roles;2.
To reflect the joint relation between wordsense disambiguation and semantic role labeling.Punyakanok et al (2008) proposed an integerlinear programming (ILP) model to get the globaloptimization for semantic role labeling, which sat-isfies some constraints.
This approach has beensuccessfully transferred into dependency parsetree based semantic role labeling system by Cheet al (2009).
The final results must satisfy twoconstraints which can be described with Markovlogic formulas as follows:C1: Each word should be labeled with one andonly one label.role(p, a, r1) ?
r1 6= r2 ?
?role(p, a, r2)The same unique constraint also happens on theword sense disambiguation, i.e.,sense(w, s1) ?
s1 6= s2 ?
?sense(w, s2)C2: Some roles (A0?A5) appear only once fora predicate.role(p, a1, r) ?
uniqe(r) ?
a1 6= a2 ?
?role(p, a2, r)It is also easy to express the joint relation be-tween word sense disambiguation and semanticrole labeling with Markov logic.
What we needto do is just adding some global formulas.
Therelation between them can be shown in Figure 2.Inspired by CoNLL 2008 (Surdeanu et al, 2008)and 2009 (Hajic?
et al, 2009) shared tasks, wheremost of successful participant systems used pred-icate senses for semantic role labeling, we alsomodel that the word sense disambiguation impli-cates the semantic role labeling.Here, we divide the all word sense disambigua-tion task into two subtasks: predicate sense dis-ambiguation and argument sense disambiguation.The advantages of the division method approachlie in two aspects.
First, it makes us distinguishthe contributions of predicate and argument wordsense disambiguation respectively.
Second, asprevious discussed, the predicate and argumentsense disambiguation can help each other.
There-fore, we can reflect the help with the division anduse Markov logic to represent it.Figure 2: Global model between word sense dis-ambiguation and semantic role labeling.Finally, we use three global formulas to imple-ment the three lines with direction in Figure 2.They are:sense(p,+s) ?
role(p, a,+r)sense(a,+s) ?
role(p, a,+r)sense(p,+s) ?
sense(a,+s)5 Experiments5.1 Experimental SettingIn our experiments, we use the OntoNotesRelease 3.02 corpus, the latest version ofOntoNotes (Hovy et al, 2006).
The OntoNotesproject leaders describe it as ?a large, multilingualrichly-annotated corpus constructed at 90% inter-nanotator agreement.?
The corpus has been an-notated with multiple levels of annotation, includ-ing constituency trees, predicate argument struc-ture, word senses, co-reference, and named enti-ties.
For this work, we focus on the constituencytrees, word senses, and predicate argument struc-tures.
The corpus has English, Chinese, and Ara-bic portions, and we just use the English portion,which has been split into four sections: broad-cast conversation (bc), broadcast news (bn), mag-azine (mz), and newswire (nw).
There are severaldatasets in each section, such as cnn and voa.We will do our experiments on all of theOntoNotes 3.0 English datasets.
For each dataset,we aimed for roughly a 60% train / 20% develop-ment / 20% test split.
See Table 3 for the detailedstatistics.
Here, we use the human annotated part-of-speech and parse trees provided by OntoNotes.The lemma of each word is extracted using Word-Net tool3.2http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2009T243http://wordnet.princeton.edu/165Training Developing Testingbccctv 1,042 (0000-0003) 328 (0004-0004) 333 (0005-0005)cnn 2,927 (0000-0004) 963 (0005-0006) 880 (0007-0008)msnbc 2,472 (0000-0003) 1,209 (0004-0005) 1,315 (0006-0007)phoenix 590 (0000-0001) 240 (0002-0002) 322 (0003-0003)bnabc 594 (0001-0040) 146 (0041-0054) 126 (0057-0069)cnn 1,610 (0001-0234) 835 (0235-0329) 1,068 (0330-0437)mnb 309 (0001-0015) 111 (0016-0020) 114 (0021-0025)nbc 281 (0001-0023) 128 (0024-0031) 78 (0032-0039)pri 1,104 (0001-0068) 399 (0069-0090) 366 (0091-0112)voa 1,159 (0001-0159) 315 (0160-0212) 315 (0213-0265)mz sinorama 5,051 (1001-1048) 1,262 (1049-1063) 1,456 (1064-1078)nw wsj 8,138 (0020-1446) 2,549 (1447-1705) 3,133 (1730-2454)xinhua 2,285 (0001-0195) 724 (0196-0260) 670 (0261-0325)All 27,562 9,209 10,176Table 3: Training, developing and testing set sizes for the datasets in sentences.
The file ranges (inparenthesis) refer to the numbers within the names of the original OntoNotes 3.0 files.
Here, we remove4,873 sentences without semantic role labeling annotation.Because we used semantic role labeling sys-tem which is based on dependence syntactic trees,we convert the constituency trees into dependencetrees with an Constituent-to-Dependency Conver-sion Tool4.The thebeast system is used in our experimentas Markov logic engine.
It uses cutting planes in-ference technique (Riedel, 2008) with integer lin-ear programming.
The weights are learned withMIRA (Crammer and Singer, 2003) online learn-ing algorithm.To our knowledge, this is the first word sensedisambiguation and semantic role labeling exper-iment on OntoNotes 3.0 corpus.
In order to com-pare our joint model with previous work, we buildseveral systems:Baseline: There are two independent baselinesystems: word sense disambiguation and seman-tic role labeling.
In each of baseline systems,we only use the local formulas (Section 4.1) andthe global formulas which only express the globalconstraints (Section 4.2).Pipeline: In a pipeline system, we use ad-ditional features outputted by preceded stages.Such as in semantic role labeling pipeline sys-tem, we use word sense as features, i.e., we setsense(w, s) as an observable predicate and addsense(p, s) ?
role(p, a, r) and sense(a, s) ?role(p, a, r) formulas into semantic role label-ing task.
As for word sense disambiguation4http://nlp.cs.lth.se/software/treebank converter/task, we add a set of formulas role(p, ai, r) ?sense(p, s), where ai is the ith argument ofthe predicate at position p, and a formularole(p, a, r) ?
sense(p, s) for the argument atposition a respectively.Jointly: We use all global formulas mentionedin Section 4.2.
With Markov logic, we can addglobal constraints and get the word sense disam-biguation and the semantic role labeling results si-multaneously.5.2 Results and DiscussionThe performance of these systems on test set isshown in Table 4.
All of the parameters are finetuned on the development set.Here, we only consider the noun and verb wordsense disambiguation, which cover most of multi-sense words.
Therefore, the word sense disam-biguation performance means the accuracy of allnouns and verbs in the test set.
The performanceof semantic role labeling is calculated using thesemantic evaluation metric of the CoNLL 2009shared task scorer5.
It measures the precision, re-call, and F1 score of the recovered semantic de-pendencies.
The F1 score is used as the final per-formance metric.
A semantic dependency is cre-ated for each predicate and its arguments.
The la-bel of such dependency is the role of the argument.The same with the CoNLL 2009 shared task, weassume that the predicates have been identified5http://ufal.mff.cuni.cz/conll2009-st/eval09.pl166WSD SRLMost Frequent Sense 85.58 ?Baseline 89.37 83.97PS 89.53 84.17Pipeline AS 89.41 83.94PS + AS ?
84.24JointlyPS?
SRL 89.53 84.27AS?
SRL 89.49 84.16PS?
AS 89.45 ?PS + AS?
SRL 89.54 84.34Fully 89.55 84.36Table 4: The results of different systems.
Here, PSmeans predicate senses and AS means argumentsenses.correctly.The first row of Table 4 gives the word sensedisambiguation result with the most frequentsense, i.e., the #01 sense of each candidate wordwhich normally is the most frequent one in a bal-ance corpus.The second row shows the baseline perfor-mances.
Here, we note that the 89.37 word sensedisambiguation accuracy and the 83.97 semanticrole labeling F1 we obtained are comparable tothe state-of-the-art systems, such as the 89.1 wordsense disambiguation accuracy given by Zhong etal.
(2008) and 85.48 semantic role labeling perfor-mance given by Che et al (2010) on OntoNotes2.0 respectively, although the corpus used in ourexperiments is upgraded version of theirs6.
Ad-ditionally, the performance of word sense dis-ambiguation is higher than that of the most fre-quent sense significantly (z-test7 with ?
< 0.01).Therefore, the experimental results show that theMarkov logic can achieve considerable perfor-mances for word sense disambiguation and se-mantic role labeling on the latest OntoNotes 3.0corpus.There are two kinds of pipeline systems: wordsense disambiguation (WSD) based on semanticrole labeling and semantic role labeling (SRL)based on word sense disambiguation.
For the us-ing method of word senses, we first only exploitpredicate senses (PS) as mentioned by Surdeanuet al (2008) and Hajic?
et al (2009).
Then, in or-6Compared with OntoNotes 2.0, the version 3.0 incorpo-rates more corpus.7http://www.dimensionresearch.com/resources/calculators/ztest.htmlder to examine the contribution of word senses ex-cept for predicates, we use argument senses (AS)in isolation.
Finally, all word senses (PS + AS)were considered.We can see that when the predicate senses (PS)are used to label semantic role, the performanceof semantic role labeling can be improved from83.97 to 84.17.
The conclusion, that the predi-cate sense can improve semantic role labeling per-formance, is similar with CoNLL 2008 (Surdeanuet al, 2008) and 2009 (Hajic?
et al, 2009) sharedtasks.
However, the improvement is not signifi-cant (?2-test8 with ?
< 0.1).
Additionally, thesemantic role labeling can improve the predicatesense disambiguation significantly from 89.37 to89.53 (z-test with ?
< 0.1).
The same conclusionwas obtained by Dang and Palmer (2005).However, when we only use argument senses(AS), both of the word sense disambiguation andsemantic role labeling performances are almostunchanged (from 89.37 to 89.41 and from 83.97to 83.94 respectively).
For the semantic role la-beling task, the reason is that the original lemmaand part-of-speech features have been able to de-scribe the argument related information.
This kindof sense features is just reduplicate.
On the otherhand, the argument senses cannot be determinedonly by the semantic roles.
For example, thesemantic role ?A1?
cannot predict the argumentsense of ?ball?
exactly.
The predicates must beconsidered simultaneously.Therefore, we use the last strategy (PS + AS),which combines the predicate sense and the ar-gument sense together to predict semantic roles.The results show that the performance can beimproved significantly (?2-test with ?
< 0.05)from 83.97 to 84.24.
Accordingly, the experi-ment proves that automatic all word sense disam-biguation can further improve the semantic rolelabeling performance.
Different from Che et al(2010), where the semantic role labeling can beimproved with correct word senses about F1 = 1,our improvement is much lower.
The main reasonis that the performance of our word sense disam-biguation with the most basic features is not highenough.
Another limitation of the pipeline strat-8http://graphpad.com/quickcalcs/chisquared1.cfm167egy is that it is difficult to predict the combinationbetween predicate and argument senses.
This isan obvious shortcoming of the pipeline method.With Markov logic, we can easily join differenttasks with global formulas.
As shown in Table 4,we use five joint strategies:1.
PS ?
SRL: means that we jointly disam-biguate predicate senses and label semantic roles.Compared with the pipeline PS system, wordsense disambiguation performance is unchanged.However, the semantic role labeling performanceis improved from 84.17 to 84.27.
Compared withthe baseline?s 83.97, the improvement is signifi-cant (?2-test with ?
< 0.05).2.
AS ?
SRL: means that we jointly disam-biguate argument senses and label semantic roles.Compared with the pipeline AS system, both ofword sense disambiguation and semantic role la-beling performances are improved (from 89.41 to89.49 and from 83.94 to 84.16 respectively).
Al-though, the improvement is not significant, it isobserved that the joint model has the capacity toimprove the performance, especially for semanticrole labeling, if we could have a more accurateword sense disambiguation.3.
PS ?
AS: means that we jointly dis-ambiguate predicate word senses and argumentsenses.
This kind of joint model does not influ-ence the performance of semantic role labeling.The word sense disambiguation outperforms thebaseline system from 89.37 to 89.45.
The resultverifies our assumption that the predicate and ar-gument senses can help each other.4.
PS + AS ?
SRL: means that we jointlydisambiguate all word senses and label semanticroles.
Compared with the pipeline method whichuses the PS + AS strategy, the joint method canfurther improve the semantic role labeling (from84.24 to 84.34).
Additionally, it can obtain thepredicate and argument senses together.
The allword sense disambiguation performance (89.54)is higher than the baseline (89.37) significantly (z-test with ?
< 0.1).5.
Fully: finally, we use all of the three globalformulas together, i.e., we jointly disambiguatepredicate senses, argument senses, and label se-mantic roles.
It fully joins all of the tasks.
Both ofall word sense disambiguation and semantic rolelabeling performances can be further improved.Although the improvements are not significantcompared with the best pipeline system, they sig-nificantly (z-test with ?
< 0.1 and ?2-test with?
< 0.01 respectively) outperform the baselinesystem.
Additionally, the performance of the fullyjoint system does not outperform partly joint sys-tems significantly.
The reason seems to be thatthere is some overlap among the contributions ofthe three joint systems.6 ConclusionIn this paper, we presented a Markov logic modelthat jointly models all word sense disambiguationand semantic role labeling.
We got the followingconclusions:1.
The baseline systems with Markov logic iscompetitive to the state-of-the-art word sense dis-ambiguation and semantic role labeling systemson OntoNotes 3.0 corpus.2.
The predicate sense disambiguation is ben-eficial to semantic role labeling.
However, theautomatic argument sense disambiguation itself isharmful to the task.
It must be combined with thepredicate sense disambiguation.3.
The semantic role labeling not only can helppredicate sense disambiguation, but also argumentsense disambiguation (a little).
In contrast, be-cause of the limitation of the pipeline model, itis difficult to make semantic role labeling to helppredicate and argument sense disambiguation si-multaneously.4.
It is easy to implement the joint model ofall word sense disambiguation and semantic rolelabeling with Markov logic.
More important, thejoint model can further improve the performanceof the all word sense disambiguation and semanticrole labeling than pipeline systems.AcknowledgementThis work was supported by National NaturalScience Foundation of China (NSFC) via grant60803093, 60975055, the ?863?
National High-Tech Research and Development of China viagrant 2008AA01Z144, and Natural Scientific Re-search Innovation Foundation in Harbin Instituteof Technology (HIT.NSRIF.2009069).168ReferencesChe, Wanxiang, Zhenghua Li, Yongqiang Li, YuhangGuo, Bing Qin, and Ting Liu.
2009.
Multilingualdependency-based syntactic and semantic parsing.In Proceedings of CoNLL 2009: Shared Task, pages49?54, June.Che, Wanxiang, Ting Liu, and Yongqiang Li.
2010.Improving semantic role labeling with word sense.In NAACL 2010, pages 246?249, June.Crammer, Koby and Yoram Singer.
2003.
Ultracon-servative online algorithms for multiclass problems.Journal of Machine Learning Research, 3:951?991.Dahlmeier, Daniel, Hwee Tou Ng, and Tanja Schultz.2009.
Joint learning of preposition senses and se-mantic roles of prepositional phrases.
In Proceed-ings of EMNLP 2009, pages 450?458, August.Dang, Hoa Trang and Martha Palmer.
2005.
The roleof semantic roles in disambiguating verb senses.
InProceedings of ACL 2005, pages 42?49, Morris-town, NJ, USA.Domingos, Pedro and Daniel Lowd.
2009.
MarkovLogic: An Interface Layer for Artificial Intelligence.Synthesis Lectures on Artificial Intelligence andMachine Learning.
Morgan & Claypool Publishers.Finkel, Jenny Rose and Christopher D. Manning.2009.
Joint parsing and named entity recognition.In Proceedings of NAACL 2009, pages 326?334,June.Goldberg, Yoav and Reut Tsarfaty.
2008.
A singlegenerative model for joint morphological segmenta-tion and syntactic parsing.
In Proceedings of ACL2008, pages 371?379, June.Hajic?, Jan, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The conll-2009shared task: Syntactic and semantic dependenciesin multiple languages.
In Proceedings of CoNLL2009: Shared Task, pages 1?18, June.Hovy, Eduard, Mitchell Marcus, Martha Palmer,Lance Ramshaw, and Ralph Weischedel.
2006.Ontonotes: The 90% solution.
In Proceedings ofNAACL 2006, pages 57?60, June.Jiang, Wenbin, Liang Huang, Qun Liu, and Yajuan Lu?.2008.
A cascaded linear model for joint chineseword segmentation and part-of-speech tagging.
InProceedings of ACL 2008, pages 897?904, June.Kruengkrai, Canasai, Kiyotaka Uchimoto, Jun?ichiKazama, Yiou Wang, Kentaro Torisawa, and HitoshiIsahara.
2009.
An error-driven word-character hy-brid model for joint chinese word segmentation andpos tagging.
In Proceedings of ACL-IJCNLP 2009,pages 513?521, August.Meza-Ruiz, Ivan and Sebastian Riedel.
2009.
Jointlyidentifying predicates, arguments and senses usingmarkov logic.
In Proceedings of NAACL 2009,pages 155?163, June.Poon, Hoifung and Pedro Domingos.
2008.
Jointunsupervised coreference resolution with MarkovLogic.
In Proceedings of EMNLP 2008, pages 650?659, October.Punyakanok, Vasin, Dan Roth, and Wen tau Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(2).Riedel, Sebastian.
2008.
Improving the accuracy andefficiency of map inference for markov logic.
InProceedings of UAI 2008, pages 468?475.
AUAIPress.Surdeanu, Mihai, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The conll2008 shared task on joint parsing of syntactic andsemantic dependencies.
In Proceedings of CoNLL2008, pages 159?177, August.Titov, Ivan and Ryan McDonald.
2008.
A joint modelof text and aspect ratings for sentiment summariza-tion.
In Proceedings of ACL 2008, pages 308?316,June.Toutanova, Kristina and Colin Cherry.
2009.
A globalmodel for joint lemmatization and part-of-speechprediction.
In Proceedings of ACL-IJCNLP 2009,pages 486?494, August.Toutanova, Kristina, Aria Haghighi, and Christo-pher D. Manning.
2008.
A global joint model forsemantic role labeling.
Computational Linguistics,34(2).Yoshikawa, Katsumasa, Sebastian Riedel, MasayukiAsahara, and Yuji Matsumoto.
2009.
Jointly iden-tifying temporal relations with markov logic.
InProceedings of ACL-IJCNLP 2009, pages 405?413,August.Zhang, Yue and Stephen Clark.
2008.
Joint word seg-mentation and POS tagging using a single percep-tron.
In Proceedings of ACL 2008, pages 888?896,June.Zhong, Zhi, Hwee Tou Ng, and Yee Seng Chan.
2008.Word sense disambiguation using OntoNotes: Anempirical study.
In Proceedings of EMNLP 2008,pages 1002?1010, October.169
