Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 284?293,Honolulu, October 2008. c?2008 Association for Computational LinguisticsArabic Named Entity Recognition using Optimized Feature SetsYassine Benajiba?
Mona Diab Paolo Rosso?
?Natural Language Engineering Lab.,Dept.
de Sistemas Informa?ticos y Computacio?nUniversidad Polite?cnica de Valencia{ybenajiba,prosso}@dsic.upv.esCenter of Computational Learning SystemsColumbia Universitymdiab@cs.columbia.eduAbstractThe Named Entity Recognition (NER) taskhas been garnering significant attention inNLP as it helps improve the performanceof many natural language processing applica-tions.
In this paper, we investigate the im-pact of using different sets of features in twodiscriminative machine learning frameworks,namely, Support Vector Machines and Condi-tional Random Fields using Arabic data.
Weexplore lexical, contextual and morphologicalfeatures on eight standardized data-sets of dif-ferent genres.
We measure the impact of thedifferent features in isolation, rank them ac-cording to their impact for each named entityclass and incrementally combine them in or-der to infer the optimal machine learning ap-proach and feature set.
Our system yields aperformance of F?=1-measure=83.5 on ACE2003 Broadcast News data.1 IntroductionNamed Entity Recognition (NER) is the process bywhich named entities are identified and classified inan open-domain text.
NER is one of the most im-portant sub-tasks in Information Extraction.
Thanksto standard evaluation test beds such as the Auto-matic Content Extraction (ACE)1, the task of NERhas garnered significant attention within the natu-ral language processing (NLP) community.
ACEhas facilitated evaluation for different languages cre-ating standardized test sets and evaluation metrics.NER systems are typically enabling sub-tasks within1http://www.nist.gov/speech/tests/ace/2004/doc/ace04-evalplan-v7.pdflarge NLP systems.
The quality of the NER sys-tem has a direct impact on the quality of the overallNLP system.
Evidence abound in the literature inareas such as Question Answering, Machine Trans-lation, and Information Retrieval (Babych and Hart-ley, 2003; Ferra?ndez et al, 2004; Toda and Kataoka,2005).
The most prominent NER systems approachthe problem as a classification task: identifying thenamed entities (NE) in the text and then classify-ing them according to a set of designed features intoone of a predefined set of classes (Bender et al,2003).
The number of classes differ depending onthe data set.
To our knowledge, to date, the ap-proach is always to model the problem with a sin-gle set of features for all the classes simultaneously.This research, diverges from this view.
We recog-nize that different classes are sensitive to differingfeatures.
Hence, in this study, we aspire to discoverthe optimum feature set per NE class.
We approachthe NER task from a multi-classification perspec-tive.
We create a classifier for each NE class inde-pendently based on an optimal feature set, then com-bine the different classifiers for a global NER sys-tem.
For creating the different classifiers per class,we adopt two discriminative approaches: SupportVector Machines (SVM)(Vapnik, 1995), and Condi-tional Random Fields (CRF)(Lafferty et al, 2001).We comprehensively investigate many sets of fea-tures for each class of NEs: contextual, lexical, mor-phological and shallow syntactic features.
We ex-plore the feature sets in isolation first.
Then, weemploy the Fuzzy Borda Voting Scheme (FBVS)(Garc?
?a Lapresta and Mart?
?nez Panero, 2002) in or-der to rank the features according to their perfor-284mance per class.
The incremental approach to fea-ture selection leads to an interpretable system wherewe have a better understanding of the resulting er-rors.
The paper is structured as follows: Section2 gives a general overview of the state-of-the-artNER approaches with a particular emphasis on Ara-bic NER; Section 3 describes relevant character-istics of the Arabic language illustrating the chal-lenges posed to NER; in Section 4.1 we describethe Support Vector Machines and Conditional Ran-dom Fields Modeling approaches.
We discuss de-tails about our feature-set in 4.2 and describe theFuzzy Borda Voting Scheme in Section 4.3.
Sec-tion 5 describes the experiments and shows the re-sults obtained; Withing Section 5, Section 5.1 givesdetails about the data-sets which we use; finally, wediscuss the results and some of our insights in Sec-tion 6 and draw some conclusions in 7.2 Related WorkTo date, the most successful language independentapproaches to English NER are systems that employMaximum Entropy (ME) techniques in a supervisedsetting (Bender et al, 2003).
(Tran et al, 2007) show that using a Sup-port Vector Machine (SVM) approach outperforms(F?=1=87.75) using CRF (F?=1=86.48) on the NERtask in Vietnamese.
For Arabic NER, (Benajibaet al, 2007) show that using a basic ME approachyields F?=1=55.23.
Then they followed up with fur-ther work in (Benajiba and Rosso, 2007), where theymodel the problem as a two step classification ap-proach applying ME, separating the NE boundarydetection from the NE classification.
That mod-ification showed an improvement in performanceyielding an F?=1=65.91.
None of these studies in-cluded Arabic specific features, all the features usedwere language independent.
In a later study, (Be-najiba and Rosso, 2008) report using lexical andmorphological features in a single step model us-ing CRF which resulted in significant improvementover state of the art to date for Arabic NER, yield-ing F?=1=79.21.
However, the data that was used inthese evaluation sets were not standard sets.
Mostrecently, (Farber et al, 2004) have explored usinga structured perceptron based model that employsArabic morphological features.
Their system ben-efits from the basic POS tag (15 tags) informationand the corresponding capitalization information onthe gloss corresponding to the Arabic word.
Exploit-ing this information yields a significant improve-ment in recall of 7% and an overall F?=1=69.6 onthe ACE2005 data set.
The authors note the lack ofimprovement in the system?s performance when us-ing other Arabic morphological information.3 Arabic in the context of NERThe Arabic language is a language of significant in-terest in the NLP community mainly due to its po-litical and economic significance, but also due to itsinteresting characteristics.
Arabic is a Semitic lan-guage.
It is known for its templatic morphologywhere words are made up of roots, patterns, and af-fixes.
Clitics agglutinate to words.
For instance, thesurface word ?
?EA 	J?m'.
?
wbHsnAthm2 ?and by theirvirtues[fem.
]?, can be split into the conjunction w?and?, preposition b ?by?, the stem HsnAt ?virtues[fem.
]?, and possessive pronoun hm ?their?.With respect to the NER task, Arabic poses sev-eral major challenges:Absence of capital letters in the orthography:English like many other Latin script based languageshas a specific marker in the orthography, namelycapitalization of the initial letter, indicating that aword or sequence of words is a named entity.
Arabichas no such special signal rendering the detection ofNEs more challenging.Absence of short vowels: The absence of shortvowels renders the lexical items a lot more ambigu-ous than in other languages exacerbating the homog-raphy problem.
The average polysemy for surfaceunvowelized words in Arabic is 12 possible vow-elized forms and when the inflections are removedthe average is 4 possible vowelized forms.3 For in-stance, words such as X@QK.
brAd can be read both as?refrigerator?
or ?Brad?,respectively, where the for-mer is a common noun and the latter is an NE.2We use the Buckwalter transliteration scheme to show ro-manized Arabic (Buckwalter, 2002).3It is worth noting that each vowelized form could still beambiguous as in the English homograph/homophone ?bank?case.285The Arabic language is highly inflectional: Aswe mentioned earlier, Arabic language uses an ag-glutinative strategy to form surface tokens.
As seenin the example above, a surface Arabic word may betranslated as a phrase in English.
Consequently, theArabic data in its raw surface form (from a statisticalviewpoint) is much more sparse which decreases theefficiency of training significantly.4 Our ApproachWe approach the problem of NER from a per NEclass based perspective.
The intuition is that featuresthat are discriminative for one NE class might not befor another class.
In the process, we decide on an op-timal set of features for each NE class.
Finally wecombine the different classifiers to create a globalNER system.
Hence, we identify a set of features forNER and proceed to investigate them individually.Then we use an automatic ranking system to pickthe optimal set of features per NE class.
To that end,we use the Fuzzy Borda Voting Scheme (FBVS).
Weemploy two discriminative classification techniques:Support Vector Machines (SVM) and ConditionalRandom Fields (CRF).
Even though some previousstudies seem to point to the superiority of SVM overCRF for NER (Tran et al, 2007), it is hard to drawa definitive conclusion since their assessment wasbased on comparing the average F-measure.4 More-over, the best system to date on Arabic NER reportsresults using CRF (Benajiba and Rosso, 2008).
Weadopt an IOB2 annotation scheme for classification.For each NE class, we have two types of class labels:B-Class, marking the beginning of a Class chunk,and I-Class marking the inside of a class chunk.
Fi-nally, we mark words not participating in an NE asO, meaning they are outside some NE class label.4.1 SVM and CRFSVM approach is based on Neural Networks(Vapnik, 1995).
The goal is to find, in the trainingphase, the best decision function which allows us toobtain the class c for each set of features f .
SVMare robust to noise and have powerful generalizationability, especially in the presence of a large numberof features.
Moreover, SVM have been used suc-4The authors did not report any per class comparison be-tween SVM and CRF.cessfully in many NLP areas of research in general(Diab et al, 2007), and for the NER task in partic-ular (Tran et al, 2007).
We use a sequence modelYamcha toolkit,5 which is defined over SVM.CRF are a generalization of Hidden Markov Mod-els oriented toward segmenting and labeling se-quence data (Lafferty et al, 2001).
CRF are undi-rected graphical models.
During the training phasethe conditional likelihood of the classes are maxi-mized.
The training is discriminative.
They havebeen used successfully for Arabic NER (see sec-tion 2).
We have used CRF++6 for our experiments.4.2 Our Feature SetsOne of the most challenging aspects in machinelearning approaches to NLP problems is deciding onthe optimal feature sets.
In this work, we investigatea large space of features which are characterized asfollows:Contextual (CXT): defined as a window of +/?n tokens from the NE of interestLexical (LEXi): defined as the lexical ortho-graphic nature of the tokens in the text.
Itis a representation of the character n-grams ina token.
We define the lexical features fo-cusing on the first three and last three char-acter n-grams in a token.
Accordingly, for atoken C1C2C3...Cn?1Cn, then the lexical fea-tures for this token are LEX1=C1, LEX2=C1C2,LEX3=C1C2C3, LEX4=Cn, LEX5 = Cn?1Cn,LEX6 = Cn?2Cn?1Cn.Gazetteers (GAZ): These include hand-crafteddictionaries/gazetteers listing predefined NEs.
Weuse three gazetteers for person names, locationsand organization names.7 We semi-automaticallyenriched the location gazetteer using the ArabicWikipedia8 as well as other web sources.
This en-richment consisted of: (i) taking the page labeled?Countries of the world?
(??A??
@ ?
?X, dwl AlEAlm)as a starting point to crawl into Wikipedia and re-trieve location names; (ii) we automatically filter thedata removing stop words; (iii) finally, the resulting5http://chasen.org/?taku/software/yamcha/6http://crfpp.sourceforge.net/7http://www.dsic.upv.es/?ybenajiba8http://ar.wikipedia.org286list goes through a manual validation step to ensurequality.
On the training and test data, we tag onlythe entities which exist entirely in the gazetteer, e.g.if the entity ?United States of America?
exists in ourgazetteer, we would not tag ?United States?
on thedata as a location.
Exception is made for personnames.
We augment our dictionary by convertingthe multiword names to their singleton counterpartsin addition to keeping the multiword names in thelist.
We tag them on the evaluation data separately.Accordingly, the name ?Bill Clinton?
and ?MichaelJohnson?
as two entries in our dictionary, are furtherbroken down to ?Bill?, ?Clinton?, ?Michael?, ?John-son?.
The intuition is that the system will be ableto identify names such as ?Bill Johnson?
and ?Clin-ton?
as person names.
This is always true for personnames, however this assumption does not hold forlocation or organization names.Part-Of-Speech (POS) tags and Base PhraseChunks (BPC): To derive part of speech tags(POS) and base phrase chunks (BPC) for Arabic, weemploy the AMIRA-1.0 system9 described in (Diabet al, 2007).
The POS tagger has a reported accu-racy of 96.2% (25 tags) and the BPC system per-forms at a reported F?=1=96.33%, assuming goldtokenization and POS tagging.Nationality (NAT): The input is checked againsta manually created list of nationalities.Morphological features (MORPH): This featureset is based on exploiting the characteristic rich mor-phological features of the Arabic language.
Werely on the MADA system for morphological dis-ambiguation (Habash and Rambow, 2005), to ex-tract relevant morphological information.
MADAdisambiguates words along 14 different morphologi-cal dimensions.
It typically operates on untokenizedtexts (surface words as they naturally occur), hence,several of the features indicate whether there areclitics of different types.
We use MADA for thepreprocessing step of clitic tokenization (which ad-dresses one of the challenges we note in Section 3,namely the impact different morphological surfaceforms have on sparseness).
Recognizing the varyingimportance of the different morphological featuresand heeding the reported MADA performance per9http://www1.cs.columbia.edu/?mdiab/feature, we carefully engineered the choice of therelevant morphological features and their associatedvalue representations.
We selected 5 morphologicalfeatures to include in this study.1.
Aspect (MASP ) : In Arabic, a verb maybe im-perfective, perfective or imperative.
However sincenone of the NEs is verbal, we decided to turn thisfeature into a binary feature, namely indicating if atoken is marked for Aspect (APP, for applicable) ornot (NA, for not applicable).2.
Person (MPER) : In Arabic, verbs, nouns,and pronouns typically indicate person information.The possible values are first, second or third person.Again, similar to aspect, the applicability of this fea-ture to the NEs is more relevant than the actual valueof first versus second, etc.
Hence, we converted thevalues to APP and NA, where APP applies if the per-son feature is rendered as first, second or third.3.
Definiteness (MDEF ) : MADA indicateswhether a token is definite or not.
All the NEs bydefinition are definite.
The possible values are DEF,INDEF or NA.4.
Gender (MGEN ) : All nominals in Arabic beargender information.
According to MADA, the pos-sible values for this feature are masculine (MASC),feminine (FEM), and neuter (or not applicable NA),which is the case where gender is not applicable forinstance in some of the closed class tokens such asprepositions, or in the case of verbs.
We use thethree possible values MASC, FEM and NA, for thisfeature.
The intuition is that since we are using asequence model, we are likely to see agreement ingender information in participants in the same NE.5.
Number (MNUM ) : For almost all the tokenscategories (verbs, nouns, adjectives, etc.)
MADAprovides the grammatical number.
In Arabic, thepossible values are singular (SG), dual (DU) andplural (PL).
The correlation of the SG value withmost of the NEs classes is very high.
Heeding theunderlying agreement of words in Arabic when theyare part of the same NE, the values for this featureare SG, DU, PL and NA (for cases where number isnot applicable such as closed class function words).Corresponding English Capitalization (CAP):MADA provides the English translation for the287words it morphologically disambiguates as it isbased on an underlying bilingual lexicon.
The in-tuition is that if the translation begins with a capitalletter, then it is most probably a NE.
This feature isan attempt to overcome the lack of capitalization forNEs in Arabic (see Section 3).
This is similar to theGlossCAP feature used in (Farber et al, 2004).4.3 Fuzzy Borda Voting SchemeFuzzy Borda Voting Scheme (FBVS) is useful whenseveral possible candidates (cn) are ranked by differ-ent experts (em) and we need to infer a single rank-ing (Garc?
?a Lapresta and Mart?
?nez Panero, 2002).It is based on the Borda count method which wasintroduced by Jean-Charles de Borda in 1770.
InFBVS, each expert provides the ranking of the can-didates with a weight10 (wmn ) assigned to each ofthem.
Thereafter, for each expert ei, we generatea square matrix such as ei = (ri1,1 .
.
.
rin,n) where:rij,k =wijwij + wik(1)Given each expert matrix, we calculate for eachrow r?ij =?k rij,k; rij,k > ?
where ?
is a certainthreshold.
Accordingly, for each candidate, we sumup the weights obtained from the different expertsin order to obtain a final weight for each candidate(r?
?j =?i r?ij ).
Finally, we rank them according tor?
?j .
In our experiments, the candidates we rank arethe features.
The FBVS ranking is calculated perML technique and class of NEs across all the datasets according to the features?
performances F?=1,i.e.
the weights.
The F?=1 ranges from 0?1.
We use?
= 0.5, thereby taking into consideration only thefeatures which have shown a significant differencein performance.5 Experiments and Results5.1 DataWe report the results of our experiments on the stan-dard sets of ACE 2003, ACE 2004 and ACE 2005data sets.11 The ACE data (see Table 1) is anno-tated for many tasks: Entity Detection and Track-ing (EDT), Relation Detection and Recognition10weights are not required for classical Borda count.11http://www.nist.gov/speech/tests/ace/Corpus genre Sizetrain Sizedev SizetestACE 2003BN 12.41k 4.12k 5.63kNW 23.85k 9.5k 9.1kACE 2004BN 45.68k 14.44k 14.81kNW 45.66k 15.2k 16.9kATB 19.04k 6.16k 6.08kACE 2005BN 18.54k 5k 8.4kNW 40.26k 12.5k 13.83kWL 13.7k 6.2k 6.4Table 1: Statistics of ACE 2003, 2004 and 2005 data(RDR), Event Detection and Recognition (EDR).All the data sets comprise Broadcast News (BN)and Newswire (NW) genres.
ACE 2004 includes anadditional NW data set from the Arabic TreeBank(ATB).
ACE 2005 includes a different genre of We-blogs (WL).We create a dev, test and train set for each ofthe collections.
Table 1 gives the relevant statis-tics.
It is worth noting that the standard trainingsets have 4 folds that are typically used for training.We used one of the folds as dev data for tuning pur-poses, rendering our training data less for our exper-iments.
For data preprocessing, we remove all anno-tations which are not oriented to the EDR task.
Also,we remove all the ?nominal?
and ?pronominal?
men-tions of the entities and keep only the ?named?
ones.Hence, all the listed characteristics for this corpuspertain to the portions of the data that are relevant toNER only.
The ACE 2003 data defines four differ-ent NE classes: Person (e.g.
Albert Einstein), Ge-ographical and Political Entities (GPE) (e.g.
Kaza-khistan), Organization (e.g.
Google Co.) and Facil-ity (e.g.
the White House).
Whereas in ACE 2004and 2005, two NE classes are added to the ACE2003 tag-set: Vehicles (e.g.
Rotterdam Ship) andWeapons (e.g.
Kalashnikof).
In order to overcomethe sparseness issues resulting , we clitic tokenizethe text using the MADA system.
We use the ATBstyle clitic tokenization standard.
Finally, we con-vert the data from the ACE format into the IOB2 an-notation scheme (Tjong Kim Sang and De Meudler,2003).5.2 ExperimentationOur objective is to find the optimum set of featuresper NE class and then combine the outcome in a288global NER system for Arabic.
We set the contextwindow to be of size?1/+1 for all the experiments,as it empirically yields the best performance.
We usethe CoNLL evaluation metrics of precision, recall,and F?=1 measures.
The CoNLL metrics are gearedto the chunk level yielding results as they pertainto the entire NE (Tjong Kim Sang and De Meudler,2003).
Our experiments are presented as follows:1.
Training per individual NE class: We trainfor an individual class by turning off the other an-notations for the other classes in the training set.We experimented with two settings: 1.
Setting allthe other NE classes to O, similar to non-NE words,thereby yielding a 3-way classification, namely, B-NE and I-NE for the class of interest, and O for therest including the rest of the NEs and other wordsand punctuation; 2.
The second setting discrimi-nated between the other NE classes that are not ofinterest and the rest of the words.
The intuition inthis case is that NE class words will naturally be-have differently than the rest of the words in thedata.
Thereby, this setting yields a 4-way classifi-cation: B-NE and I-NE for class of interest, NE forthe other NE classes, and O for the other words andpunctuation in the data.
In order to contrast the 3-way vs the 4-way classification, we run experimentsand evaluate using the ACE 2003 data set with nofeatures apart from ?CXT?
and ?current word?
usingSVM.
Table 2 illustrates the yielded results.
For allClass Num(classes) BN genre NW genreGPE3 76.72 79.884 76.88 80.99PER3 64.34 42.934 67.56 44.43ORG3 41.73 25.244 46.02 37.97FAC3 23.33 15.34 23.33 18.12Table 2: F?=1 Results using 3-way vs. 4-way class anno-tations using SVMthe NE classes we note that the 4-way classificationyields the best results.
Moreover, we counted thenumber of ?conflicts?
obtained for each NE classifi-cation.
A ?conflict?
arises when the same token isclassified as a different NE class by more than oneclassification system.
Our findings are summarizedas follows:(i).
3 classes: 16 conflicts (8 conflicts in BN and 8in NW).
10 of these conflicts are between GPE andPER, and 6 of them are between GPE and ORG.(ii).
4 classes: 10 conflicts (3 conflicts in BN and7 in NW).
9 of these conflicts are between GPE andORG, and only one of them is between GPE andFAC.An example of a conflict observed using the 3-way classification that disappeared when we ap-ply the 4-way classification is in the following sen-tence: @QKQ?K ??
?AK 	?
?J ?@???Jm?
HQ??n$rt SHyfpWA$nTn tAyms tqryrA, which is translated as ?TheWashington Times newspaper published a report?.When trained using a 3-way classifier, ?Washington?is assigned the tag GPE by the GPE classifier sys-tem and as an ORG by the ORG classifier system.However, when trained using the 4-way classifier,this conflict is resolved as an ORG in the ORG clas-sifier system and an NE in the GPE classifier sys-tem.
Thereby confirming our intuition that a 4-wayclassification is better suited for the individual NEclassification systems.
Accordingly, for the rest ofthe experiments in this paper reporting on individualNE classifiers systems, we use a 4-way classificationapproach.2.
Measuring the impact of Individual featuresper class : An experiment is run for each foldof the data.
We train on data annotated for oneNE class, one Machine Learning (ML) method (i.e.SVM or CRF), and one feature.
For each experimentwe use the tuning set for evaluation, i.e.
obtainingthe F?=1 performance value.3.
FBVS Ranking : After obtaining the F-measures for all the individual features on all thedata genres and using the two ML techniques, werank the features (in a decreasing order) accordingto their impact (F-measure obtained) using FBVS(see 4.3).
This results in a ranked list of features foreach ML approach and data genre per class.
Oncethe features are ranked, we incrementally experi-ment with the features in the order of the ranking, i.e.train with the first feature and measure the perfor-mance on the tuning data, then train with the secondtogether with the first feature, i.e.
the first two fea-tures and measure performance, then the first threefeatures and so on.289Feats PER GPE ORG FAC VEH/WEALEX1 16 12 12 15 4LEX2 3 15 7 12 5LEX3 10 6 15 10 6LEX4 7 16 4 8 7LEX5 15 14 16 16 8LEX6 12 4 10 9 9GAZ 14 7 9 11 3BPC 4 13 13 6 1POS 1 5 1 4 16NAT 8 3 2 3 15MASP 13 2 5 2 10MPER 11 11 3 5 14MDEF 9 9 6 7 11MGEN 5 8 11 13 12MNUM 6 10 14 14 13CAP 2 1 8 1 2Table 3: Ranked features according to FBVS using SVMfor each NE class4.
Feature set/class generalization : Finally, wepick the first n features that yield the best convergingperformance (after which additional features do notimpact performance or cause it to deteriorate).
Weuse the top n features to tag the test data and comparethe results against the system when it is trained onthe whole feature set.5.3 Individual Features ExperimentsAfter running experiments using each feature indi-vidually, each result is considered an expert (the ob-tained F-measure is the weight in this framework).Our goal is to find a general ranking of the fea-tures for each ML approach and each class.
Table 3shows the obtained rankings of the features for eachclass using SVM.
It is worth noting that the obtainedCRF rankings are very similar to those yielded byusing SVM.
We note that there are no specific fea-tures that have proven to be useless for all classesand ML approaches.5.4 Feature set/class ExperimentsWe combine the features per NE class incrementally.Since the total number of features is 16, each MLclassifier is trained and evaluated on the tuning data16 times for each genre.
A best number of featuresper class per genre per ML technique is determinedbased on the highest yielded F?=1.
Finally, the laststep is combining the outputs of the different clas-sifiers for all the classes.
In case of conflicts, wherethe same token is tagged as two different NE classes,we use a simple heuristic based on the classifier pre-cision for that specific tag, favoring the tag with thehighest precision.Table 4 illustrates the obtained results.
For eachdata set and each genre it shows the F-measure ob-tained using the best feature set and ML approach.We show results for both the dev and test data usingthe optimal number of features Best Feat-Set/MLcontrasted against the system when using all 16 fea-tures per class All Feats/ML.
The table also illus-trates three baseline results on the test data only.FreqBaseline: For this baseline, we assign a testtoken the most frequent tag observed for it in thetraining data, if a test token is not observed in thetraining data, it is assigned the most frequent tagwhich is the O tag.
MLBaseline: In this baselinesetting, we train an NER system with the full 16features for all the NE classes at once.
We use thetwo different ML approaches yielding two baselines:MLBaselineSVM and MLBaselineCRF .It is important to note the difference between the AllFeats/ML setting and the MLBaseline setting.
Inthe former, All Feats/ML, all 16 features are usedper class in a 4-way classifier system and then theclassifications are combined and the conflicts are re-solved using our simple heuristic while in the lat-ter case of MLBaseline the classes are trained to-gether with all 16 features for all classes in one sys-tem.
Since different feature-sets and different MLapproaches are used and combined for each experi-ment, it is not possible to present the number of fea-tures used in each experiment in Table 4.
However,Table 5 shows the number of features and the MLapproach used for each genre and NE class.6 Discussion and Error AnalysisAs illustrated in Table 5, SVM outperformed CRFon most of the classes.
Interestingly, CRF tends tomodel the ORG and FAC entities better than SVM.Hence, it is not possible to give a final word on thesuperiority of SVM or CRF in the NER task, and itis necessary to conduct a per class study, as the onewe present in this paper, in order to determine theright ML approach and features to use for each class.Therefore, our best global NER system combined290ACE 2003 ACE 2004 ACE 2005BN NW BN NW ATB BN NW WLFreqBaseline 73.74 67.61 62.17 51.67 62.94 70.18 57.17 27.66MLBaselineSVM 80.58 76.37 74.21 71.11 73.14 79.3 73.9 54.68MLBaselineCRF 81.02 76.18 74.67 71.8 73.04 80.13 74.75 55.32devBest Feat-set/ML 83.41 79.11 76.9 72.9 74.82 81.42 76.07 54.49All Feats.
SVM 81.79 77.99 75.49 71.8 73.71 80.87 75.69 53.73All Feats.
CRF 81.76 76.6 76.26 71.85 74.19 79.66 74.83 36.11testBest Feat-set/ML 83.5 78.9 76.7 72.4 73.5 81.31 75.3 57.3All Feats.
SVM 81.76 77.27 74.71 71.16 73.63 81.1 72.41 55.58All Feats.
CRF 81.37 75.89 75.73 72.36 74.21 80.16 74.43 27.36Table 4: Final Results obtained with selected features contrasted against all features combinedBN NW ATB WLN ML N ML N ML N MLPerson 12 SVM 14 SVM 9 SVM 11 SVMLocation 10 SVM 7 SVM 16 CRF 14 SVMOrganization 9 CRF 6 CRF 10 CRF 12 CRFFacility 10 CRF 14 CRF 14 SVM 16 CRFVehicle 3 SVM 3 SVM 3 SVM 3 SVMWeapon 3 SVM 3 SVM 3 SVM 3 SVMTable 5: Number of features and ML approach used to obtain the best resultsthe results obtained from both ML approaches.Table 4, shows that our Best Feat-set/ML set-ting outperforms the baselines and the All Feats{SVM/CRF} settings for all the data genres and setsforthe test data.
Moreover, the Best Feat-set/MLsetting outperforms both All Feats {SVM/CRF}settings for the dev data for all genres except forACE2003 NW, where the difference is very small.The results yielded from the ML baselines arecomparable across all the data genres and the twoML approaches.Comparing the global ML baseline systemsagainst the All Feature Setting, we see that the AllFeats setting consistently outperforms the MLBase-line settings except for ACE2005 NW data set.
Thissuggests that training separate systems for the differ-ent NEs has some benefit over training in one globalsystem.Comparing the performance per genre across thedifferent data sets.
We note better performanceacross the board for BN data over NW per year.The worst results are yielded for ACE 2004 data forboth BN and NW genres.
There is no definitive con-clusion that a specific ML approach is better suitedfor a specific data genre.
We observe slightly bet-ter performance for the CRF ML approach in theMLBaselineCRF condition for both BN and NW.The worst performance is yielded for the WLdata.
This may be attributed to the small amountof training data available for this genre.
Moreoverthe quality of the performance of the different fea-ture extraction tools such as AMIRA (for POS tag-ging and BPC) and MADA (for the morphologicalfeatures) are optimized for NW data genres, therebyyielding suboptimal performance on the WL genre,leading to more noise than signal for training.
How-ever, comparing relative performance on this genre,we see a significant jump from the most frequentbaseline FreqBaseline (F?=1=27.66) to the bestbaseline MLBaselineCRF (F?=1=55.32).
We see afurther significant improvement when the Best Feat-set/ML setting is applied yielding an F?=1=57.3.Interestingly, however the MLBaselineCRF yieldsa much better performance (F?=1=55.32) than AllFeats CRF with an F?=1=27.36.
This may indi-cate that a global system that trains all classes atonce using CRF for sparse data is better than train-ing separate classifiers and then combining the out-291puts.
It is worth noting the difference betweenMLBaselineSVM and All Feats SVM, F?=1=54.68and F?=1=55.58, respectively.
This result suggeststhat SVM are more robust to less training data as il-lustrated in the case of the individual classifiers inthe latter setting.Comparing dev and test performance, we note thatthe overall results on the dev data are better thanthose obtained on the test data, which is expectedgiven that the weights for the FBVS ranking are de-rived based on the dev data used as a tuning set.
Theonly counter example for this trend is with the WLdata genre, where the test data yields a significantlyhigher performance for all the conditions except forAll Feats CRF.As observed in Table 3, the ranking of the indi-vidual features could be very different for two NEclasses.
For instance, the BPC is ranked 4th forthe PER class and is ranked 13th for GPE and ORGclasses.
The disparity in ranking for the same indi-vidual features strongly suggests that using the samefeatures for all the classes cannot lead to a global op-timal classifier.
With regards to morphological fea-tures, we note in Table 3, that Definiteness, MDEF ,is helpful for all the NE classification systems, byvirtue of being included for all optimal systems forall NE classification systems.
Aspect,MASP , is use-ful for all classes except PER.
Moreover,MGEN andMNUM , corresponding to Gender and Number, re-spectively, contributed significantly to the increasein recall for PER and GPE classes.
Finally, the Per-son feature, MPER contributed mostly to improv-ing the classification of ORG and FAC classes.
Ac-cordingly, observing these results, contrary to pre-vious results by (Farber et al, 2004), our resultsstrongly suggest the significant impact morpholog-ical features have on Arabic NER, if applied at theright level of granularity.Inconsistencies in the data lead to many of the ob-served errors.
The problem is that the ACE datais annotated primarily for a mention detection taskwhich leads to the same exact words not being anno-tated consistently.
For instance, the word ?Palestini-ans?
would sometimes be annotated as a GPE classwhile in similar other contexts it is not annotated as anamed entity at all.
Since we did not manually cor-rect these cases, the classifiers are left with mixedsignals.
The VEH and WEA classes both exhibit auniform ranking for all the features and yield a verylow performance.
This is mainly attributed to thefact that they appear very rarely in the training data.For instance, in the ACE 2005, BN genre, there are1707 instances of the class PER, 1777 of GPE, 103of ORG, 106 of FAC and only 4 for WEA and 24 forVEH.7 Conclusions and Future DirectionsWe described the performance yielded usinglanguage-dependent and language independent fea-tures in SVM and CRF for the NER task on differ-ent standard Arabic data-sets comprising differentgenres.
We have measured the impact of each fea-ture individually on each class, we ranked them ac-cording to their impact using the Fuzzy Borda Vot-ing Scheme, and then performed an incremental fea-tures?
selection considering each time the N bestfeatures.We reported the importance of each feature foreach class and then the performance obtained whenthe best feature-set is used.
Our experiments yieldstate of the art performance significantly outper-forming the baseline.
Our best results achieve anF?=1 score of 83.5 for the ACE 2003 BN data.
OurACE2005 results are state of the art when comparedto the best system to date.
It is worth noting thatthese obtained results are trained on less data sincewe train only on 3 folds vs the standard 4 folds.
Ourresults show that the SVM and CRF have very sim-ilar behaviors.
However, SVM showed more robustperformance in our system using data with very ran-dom contexts, namely for the WL data, i.e.
We-blogs.
We definitively illustrate that correctly ex-ploiting morphological features for languages withrich morphological structures yields state of the artperformance.
For future work, we intend to investi-gate the use of automatic feature selection methodson the same data.Acknowledgments The authors would like to thankthe reviewers for their detailed constructive comments.We would like to thank MCyT TIN2006-15265-C06-04and PCI-AECI A/010317/07 research projects for par-tially funding this work.
Mona Diab would like to ac-knowledge DARPA GALE Grant Contract No.
HR0011-06-C-0023 for partially funding this work.292ReferencesBogdan Babych and Anthony Hartley.
2003.
ImprovingMachine Translation Quality with Automatic NamedEntity Recognition.
In Proc.
of EACL-EAMT.
Bu-dapest.Yassine Benajiba and Paolo Rosso.
2008.
Arabic NamedEntity Recognition using Conditional Random Fields.In Proc.
of Workshop on HLT & NLP within the ArabicWorld, LREC?08.Yassine Benajiba, Paolo Rosso and Jose?
Miguel Bened??.2007.
ANERsys: An Arabic Named Entity Recogni-tion system based on Maximum Entropy.
In Proc.of CICLing-2007, Springer-Verlag, LNCS(4394), pp.143-153.Yassine Benajiba and Paolo Rosso.
2007.
ANERsys 2.0 :Conquering the NER task for the Arabic language bycombining the Maximum Entropy with POS-tag infor-mation.
In Proc.
of Workshop on Natural Language-Independent Engineering, IICAI-2007.Oliver Bender, Franz Josef Och, and Hermann Ney.2003.
Maximum Entropy Models For Named EntityRecognition.
In Proc.
of CoNLL-2003.
Edmonton,Canada.Tim Buckwalter.
2002.
Buckwalter Arabic Morpho-logical Analyzer.
In Linguistic Data Consortium.
(LDC2002L49).Mona Diab, Kadri Hacioglu and Daniel Jurafsky.
2007.Arabic Computational Morphology: Knowledge-based and Empirical Methods, chapter 9, pp.
159?179.Abdelhadi Soudi, Antal van den Bosch and GunterNeumann (Eds.
), Springer.Benjamin Farber, Dayne Freitag, Nizar Habash andOwen Rambow.
2008.
Improving NER in Arabic Us-ing a Morphological Tagger.
In Proc.
of LREC?08.Sergio Ferra?ndez, O?scar Ferra?ndez, Antonio Ferra?ndezand Rafael Mun?oz.
2007.
The Importance of NamedEntities in Cross-Lingual Question Answering InProc.
of RANLP?07.Jose?
Luis Garc?
?a Lapresta and Miguel Mart?
?nez Panero2002.
Borda Count Versus Approval Voting: A FuzzyApproach.
Public Choice, 112(1-2):pp.
167?184.Nizar Habash and Owen Rambow.
2005.
Arabic Tok-enization, Part-Of-Speech Tagging and MorphologicalDisambiguation in One Fell Swoop.
In Proc.
of Work-shop of Computational Approaches to Semitic Lan-guages, ACL-2005.John Lafferty, Andrew McCallum and Fernando Pereira.2001.
Conditional Random Fields: ProbabilisticModels for Segmenting and Labeling Sequence Data.In Proc.
of ICML-2001.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CoNLL-2003 Shared Task:Language-Independent Named Entity Recognition.
InProc.
of CoNLL-2003.
pp.
142?147.Hiroyuki Toda and Ryoji Kataoka.
2005.
A SearchResult Clustering Method using Informatively NamedEntities..
In Proc.
of the 7th ACM International Work-shop on Web Information and Data Management.Q.
Tri Tran, T.X.
Thao Pham, Q.
Hung Ngo,Dien Dinh,and Nigel Collier.
2007.
Named Entity Recognition inVietnamese documents.
Progress in Informatics Jour-nal.
2007.Vladimir Vapnik.
1995.
The Nature of Statistical Learn-ing Theory.
Springer Verlag.293
