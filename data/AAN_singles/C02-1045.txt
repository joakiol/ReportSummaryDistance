A Method of Cluster-Based Indexing of Textual DataAkiko AizawaNational Institute of Informaticsakiko@nii.ac.jpAbstractThis paper presents a framework for cluster-ing in text-based information retrieval systems.The prominent feature of the proposed methodis that documents, terms, and other related el-ements of textual information are clustered si-multaneously into small overlapping clusters.
Inthe paper, the mathematical formulation andimplementation of the clustering method arebriefly introduced, together with some experi-mental results.1 IntroductionThis paper is an attempt to provide a view ofindexing as a process of generating many smallclusters overlapping with each other.
Individ-ual clusters, referred to as micro-clusters in thispaper, contain multiple subsets of associated el-ements, such as documents, terms, authors, key-words, and other related attribute sets.
For ex-ample, a cluster in Figure 1 represents ?a set ofdocuments written by a specific community ofauthors related to a subject represented by a setof terms?.Our motivations for considering such clustersare that (i) the universal properties of text-based information spaces, namely large scale,sparseness, and local redundancy (Joachims,2001), may be better manipulated by focusingon only limited sub-regions of the space; andalso that (ii) the multiple viewpoints of infor-mation contents, which a conventional retrievalsystem provides, can be better utilized by con-sidering not only the relations between ?doc-uments?
and ?terms?
but also associations be-tween other attributes such as ?authors?
withinthe same unified framework.Based on the background, this paper presentsa framework of micro-clustering, within whichwe adopt a probabilistic formulation of co-ST : a subset of termsSD: a subset of documentsSA: a subsetof authorsA ?cluster?that representsassociations betweenST, SA, and SD.Term spaceDocument spaceAuthor spaceFigure 1: Cluster-based indexing of informationspaces.occurrences of textual elements.
For simplic-ity, we focus primarily on the co-occurrencesbetween ?documents?
and ?terms?
in our expla-nation, but the presented framework is directlyapplicable to more general cases with more thantwo attributes.2 Background IssuesA view from indexingIn information retrieval research, matrixtransformation-based indexing methods such asLatent Semantic Indexing (LSI) (Deerwesteret al, 1990) have recently become quite com-mon.
These methods can be viewed as an es-tablished basis for exposing hidden associationsbetween documents and terms.
However, theirobjective is to generate a compact representa-tion of the original information space, and it islikely in consequence that the resulting orthog-onal vectors are dense with many non-zero ele-ments (Dhillon and Modha, 1999).
In addition,because the reduction process is globally op-timized, matrix transformation-based methodsbecome computationally infeasible when deal-ing with high-dimensional data.A view from clusteringThe document-clustering problem has alsobeen extensively studied in the past (Iwayamaand Tokunaga, 1995; Steinbach et al, 2000).The majority of the previous approaches to clus-tering construct either a partition or a hierarchyof target documents, where the generated clus-ters are either exclusive or nested.
However,generating mutually exclusive or tree-structuredclusters in general is a hard-constrained prob-lem and thus is likely to suffer high computa-tional costs when dealing with large-scale data.Also, such a constraint is not necessarily re-quired in actual applications, because ?topics?of documents, or rather ?indices?
in our context,are arbitrarily overlapped in nature (Zamir andEtzioni, 1998).Basic Strategy:Based on the above observations, our basicstrategy is as follows:?
Instead of generating component vectors withmany non-zero elements, produce only limitedsubsets of elements, i.e., micro-clusters, withsignificance weights.?
Instead of transforming the entire co-occurrence matrix into a different featurespace, extract tightly associated sub-structuresof the elements on the graphical representationof the matrix.
?Use entropy-based criteria for cluster evalua-tion so that the sizes of the generated clusterscan be determined independently of other ex-isting clusters.
?Allow the generated clusters to overlap witheach other.
By assuming that each elementcan be categorized into multiple clusters, wecan reduce the problem to a feasible level wherethe clusters are processed individually.Related studies:Another important aspect of the proposedmicro-clustering scheme is that the method em-ploys simultaneous clustering of its composingelements.
This not only enables us to com-bine issues in term indexing and document clus-tering, as mentioned above, but also is usefulfor connecting matrix-based and graph-basednotions of clustering; the latter is based onthe association networks of the elements ex-tracted from the original co-occurrence matri-ces.
Some recent topics dealing with this sortof duality and/or graphical views include: theInformation Bottleneck Method (Slonim andTishby, 2000), Conceptual Indexing (Dhillonand Modha, 1999; Karypis and Han, 2000), andBipartite Spectral Graph Partitioning (Dhillon,2001), although each of these follows its ownmathematical formulation.3 The Clustering Method3.1 Definition of Micro-ClustersLet D = {d1, ?
?
?
, dN} be a collection of N tar-get documents, and let SD be a subset of doc-uments such that SD ?
D. Likewise, let T ={t1, ?
?
?
, tM} be a set of M distinct terms thatappear in the target document collection, andlet ST be a subset of terms such that ST ?
T .A cluster, denoted as c, is defined as a combi-nation of ST and SD:c = (ST , SD).
(1)The co-occurrences of terms and documents canbe expressed as a matrix of size M ?N in whichthe (i, j)-th cell indicates that ti (?
T ) appearsin dj (?
D).
We make the value of the (i, j)-thcell equal to freq(ti, dj).
Although we primarilyassume the value is either ?1?
(exist) or ?0?
(notexist) in this paper, our formulation could eas-ily be extended to the cases where freq(ti, dj)represents the actual number of times that tiappears in dj .The observed total frequency of ti over all thedocuments in D is denoted as freq(ti,D).
Simi-larly, the observed total frequency of dj , i.e.
thetotal number of terms contained in dj , is de-noted as freq(T, dj).
These values correspondto summations of the columns and the rows ofthe co-occurrence matrix.
The total frequencyof all the documents is denoted as freq(T,D).Thus,freq(T,D) =?ti?Tfreq(ti ,D) =?dj?Dfreq(T, dj )=?ti?T?dj?Dfreq(ti , dj).
(2)We sometimes use freq(ti) for freq(ti,D),freq(dj) for freq(T, dj) and F for freq(T,D).DocumentsTermsA representationof a cluster thatis composed ofsubsets ofdocuments andtermsSDSTc(ST,SD)freq(ti, dj)=1 freq(ST, SD)=7001 11111|ST|=3|SD|=3DocumentsTerms71Before agglomeration After agglomeration|SD|=3|ST|=3STSDSTSDFigure 2: Example of a cluster defined on a co-occurrence matrix.When a cluster c is being considered, T and Din the above definitions are changed to ST andSD.
In this case, freq(ti, SD) and freq(ST , dj)represent the frequencies of ti and dj withinc = (ST , SD), respectively.
In the co-occurrencematrix, a cluster is expressed as a ?rectangular?region if terms and documents are so permuted(Figure 2).3.2 Probabilistic FormulationThe view of the co-occurrence matrix can befurther extended by assigning probabilities toeach cell.
With the probabilistic formulation,ti and dj are considered as independently ob-served events, and their combination as a sin-gle co-occurrence event (ti, dj).
Then, a clusterc = (ST , SD) is also considered as a single co-occurrence event of observing one of ti ?
STwithin one of dj ?
SD.In estimating the probability of each event,we use a simple discounting method similar tothe absolute discounting in probabilistic lan-guage modeling studies (Baayen, 2001).
Themethod subtracts a constant value ?, called adiscounting coefficient, from all the observedterm frequencies and estimates the probabilityof ti as:P (ti) =freq(ti) ?
?F.
(3)Note that the discounting effect is stronger forlow-frequency terms.
For high-frequency terms,P (ti) ?
freq(ti)/F .
In the original definition,the value of ?
was uniquely determined, for ex-ample as ?
= m(1)M with m(1) being the numberof terms that appear exactly once in the text.However, we experimentally vary the value of ?in our study, because it is an essential factor forcontrolling the size and quality of the generatedclusters.Assuming that the probabilities assigned todocuments are not affected by the discounting,P (dj |ti) = freq(ti, dj) / freq(ti).
Then, apply-ing P (ti, dj) = P (dj |ti)P (ti), the co-occurrenceprobability of ti and dj is given as:P (ti, dj) =freq(ti) ?
?freq(ti)?
freq(ti , dj)F. (4)Similarly, the co-occurence probability of STand SD is given as:P (ST , SD) =freq(ST ) ?
?freq(ST )?
freq(ST , SD)F. (5)3.3 Criteria for Cluster EvaluationThe evaluation is based on the information the-oretic view of the retrieval systems (Aizawa,2000).
Let T and D be two random vari-ables corresponding to the events of observ-ing a term and a document, respectively.
De-note their occurrence probabilities as P (T ) andP (D), and their co-occurrence probability as ajoint distribution P (T ,D).
By the general defi-nition of traditional information theory, the mu-tual information between T and D, denoted asI(T ,D), is calculated as:I(T ,D) =?ti?T?dj?DP (ti, dj)logP (ti, dj)P (ti)P (dj), (6)where the values of P (ti, dj) and P (ti) are cal-culated using Eqs.
(3) and (4).
P (dj) is deter-mined by P (dj) =?ti?T P (ti, dj), or approx-imated simply by P (dj) = freq(dj)/F .
Next,the mutual information after agglomerating STand SD into a single cluster (Figure 2) is calcu-lated as:I ?
(T ,D) =?ti /?ST?dj /?SDP (ti, dj)logP (ti, dj)P (ti)P (dj)+P (ST , SD)logP (ST , SD)P (ST )P (SD), (7)where P (ST ) =?ti?ST P (ti) and P (SD) =?dj?SD P (dj).The fitness of a cluster, denoted as?I(ST , SD), is defined as the difference of thetwo information values given by Eqs.
(6) and (7):?I(ST , SD) = I ?
(T ,D) ?
I(T ,D)= P (ST , SD)logP (ST , SD)P (ST )P (SD)?
?ti?ST?dj?SDP (ti, dj)logP (ti, dj)P (ti)P (dj).
(8)Without discounting, the value of ?I(ST , SD) inthe above equation is always negative or zero.However, with discounting, the value becomespositive for uniformly dense clusters, becausethe frequencies of individual cells are alwayssmaller than their agglomeration and so the dis-counting effect is stronger for the former.Using the same formula, we calculated thesignificance weights ti in c = (ST , SD) as:?I(ti, SD) =?dj?SDP (ti, dj)logP (ti, dj)P (ti)P (dj), (9)and the significance weights of dj as:?I(ST , dj) =?ti?STP (ti, dj)logP (ti, dj)P (ti)P (dj).
(10)In other words, all the terms and documents in acluster can be jointly ordered according to theircontribution in the entropy calculation given byEq.
(7).To summarize, the proposed probabilisticformulation has the following two major fea-tures.
First, clustering is generally defined asan operation of agglomerating a group of cellsin the contingency table.
Such an interpreta-tion is unique because existing probabilistic ap-proaches, including those with a duality view,agglomerate entire rows or columns of the con-tingency table all at once.
Second, the estima-tion of the occurrence probability is not simplyin proportion to the observed frequency.
Thediscounting scheme enables us to trade off (i)the loss of averaging probabilities in the ag-glomerated clusters, and (ii) the improvementof probability estimations by using larger sam-ples sizes after agglomeration.It should be noted that although we have re-stricted our focus to one-to-one correspondencesbetween terms and documents, the proposedframework can be directly applicable to moregeneral cases with k(?
2) attributes.
Namely,given k random variables X1, ?
?
?
,Xk, Eq.
(8)can be extended as:?I(SX1, ?
?
?
, SXk)= P (SX1, ?
?
?
, SXk )logP (SX1, ?
?
?
, SXk )P (SX1) ?
?
?P (SXk )(11)??x1?SX1?
?
?
?xk?SXkP (x1, ?
?
?
, xk)logP (x1, ?
?
?
, xk)P (x1) ?
?
?P (xk).3.4 Cluster Generation ProcedureThe cluster generation process is defined as therepeated iterations of cluster initiation and clus-ter improvement steps (Aizawa, 2002).First, in the cluster initiation step, a singleterm ti is selected, and an initial cluster is thenformulated by collecting documents that con-tain ti and terms that co-occur with ti withinthe same document.
The collected subsets,respectively, become SD and ST of the initi-ated cluster.
On the bipartite graph of termsand documents (Figure 2), the process can beviewed as a two-step expansion starting from ti.Next, in the cluster improvement step, all theterms and documents in the initial cluster aretested for elimination in the order of increas-ing significance weights given by Eqs.
(9) and(10).
If the performance of the target cluster isimproved after the elimination, then the corre-sponding term or document is removed.
Whenfinished with all the terms and documents in thecluster, the newly generated cluster is tested tosee whether the evaluation value given by Eq.
(8) is positive.
Clusters that do not satisfy thiscondition are discarded.
Note that the resultingcluster is only locally optimized, as the improve-ment depends on the order of examining termsand documents for elimination.At the initiation step, instead of randomlyselecting an initiating term, our current im-plementation enumerates all the existing termsti ?
T .
We also limit the sizes of ST and SDto kmax = 50 to avoid explosive computationcaused by high frequency terms.
Except forkmax, the discounting coefficient ?
is the onlyparameter that controls the sizes of the gener-ated clusters.
The effect of ?
is examined indetail in the following experiments.4 Experimental Results4.1 The Data SetIn our experiments, we used NTCIR-J11, aJapanese text collection for retrieval tasks thatis composed of abstracts of conference papersorganized by Japanese academic societies.
Inpreparing the data for the experiments, we firstselected 52,867 papers from five different so-cieties: 23,105 from the Society of PolymerScience, Japan (SPSJ), 20,482 from the JapanSociety of Civil Engineers (JSCE), 4,832 fromthe Japan Society for Precision Engineering(JSPE), 2,434 from the Ecological Society ofJapan (ESJ), and 2,014 from the Japanese So-ciety for Artificial Intelligence (JSAI).The papers were then analyzed by the mor-phological analyzer ChaSen Ver.2.02 (Mat-sumoto et al, 1999) to extract nouns and com-pound nouns using the Part-Of-Speech tags.Next, the co-occurrence frequencies betweendocuments and terms were collected.
After pre-processing, the number of distinctive terms was772,852 for the 52,867 documents.4.2 Clustering ResultsIn our first experiments, we used a frameworkof unsupervised text categorization, where thequality of the generated clusters was evaluated1http://research.nii.ac.jp/ntcir/by the goodness of the separation between dif-ferent societies.
To investigate the effect of thediscounting parameter, it was given the values?
= 0.1,0.3,0.5,0.7, 0.9, 0.95.Table 1 compares the total number of gener-ated clusters (c), the average number of docu-ments per cluster (sd), and the average numberof terms per cluster (st), for different values of?.
We also examined the ratio of unique clus-ters that consist only of documents from a sin-gle society (rs), and an inside-cluster ratio thatis defined as the average relative weight of thedominant society for each cluster (ri).
Here, theweight of each society within a cluster was cal-culated as the sum of the significance weights ofits component documents given by Eq.
(10).The results shown in Table 1 indicate that re-ducing the value of ?
improves the quality of thegenerated clusters: with smaller ?, the single so-ciety ratio and the inside-cluster ratio becomeshigher, while the number of generated clustersbecomes smaller.Table 1: Summary of clustering results.?
c sd st rs ri0.10 136,832 3.25 9.3 0.953 0.9830.30 187,079 3.94 29.4 0.896 0.9600.50 196,208 4.81 39.7 0.866 0.9510.70 196,911 5.39 44.4 0.851 0.9480.90 197,164 5.81 46.3 0.841 0.9450.95 197,193 5.89 46.6 0.839 0.9444.3 Categorization ResultsIn our second experiment, we used a frame-work of supervised text categorization, wherethe generated clusters were used as indices forclassifying documents between the existing so-cieties, and the categorization performance wasexamined.For this purpose, the documents were first di-vided into a training set of 50,182 documentsand a test set of 2,641 documents.
Then, assum-ing that the originating societies of the trainingdocuments are known, the significance weightsof the five societies were calculated for eachcluster generated in the previous experiments.Next, the test documents were assigned to oneof the five societies based on the membershipof the multiple clusters to which they belong.For comparison, two supervised text categoriza-tion methods, naive Bayes and Support VectorMachine (SVM), were also applied to the sametraining and test sets.The results are shown in Table 2.
In thiscase, the performance was better for larger ?,indicating that the major factor determiningthe categorization performance was the num-ber of clusters rather than their quality.
For?
= 0.5 ?
0.95, each tested document appearedin at least one of the generated clusters, and theperformance was almost comparable to the per-formance of standard text categorization meth-ods: slightly better than naive Bayes, but notso good as SVM.
We also compared the perfor-mance for varied sizes of training sets and alsousing different combination of societies, but thetendency remained the same.Table 2: Summary of categorization results.?
correct judge F-value0.10 2,370 2,446 0.9320.30 2,520 2,623 0.9570.50 2,575 2,641 0.9750.70 2,583 2,641 0.9780.90 2,584 2,641 0.9780.95 2,583 2,641 0.978naive Bayes 2,579 2,641 0.977SVM 2,602 2,641 0.9854.4 Further AnalysisAnalysis of categorization errorsTable 3 compares the patterns of misclassi-fication, where the columns and rows repre-sent the classified and the real categories, re-spectively.
It can be seen that as far as mi-nor categories such as ESJ and JSAI are con-cerned, the proposed micro-clustering methodperformed slightly better than SVM.
The rea-son may be that the former method is based onlocally conformed clusters and less affected bythe skew of the distribution of category sizes.However, the details are left for further investi-gation.In addition, by manually analyzing the indi-vidual misclassified documents, it can be con-firmed that most of them dealt with inter-domain topics.
For example, nine out of the tenJSCE documents misclassified as ESJ were re-lated to environmental issues; six out of the 14JSPE documents misclassified as JSCE, as well asall seven JSPE documents misclassified as JSAI,were related to the application of artificial intel-ligence techniques.
These were the major causesof the performance difference of the two meth-ods.Table 3: Analysis of miss-classification.
(a) Micro-clustering resultsj u d g eSPSJ JSCE JSPE ESJ JSAIr SPSJ 1146 7 2 0 0e JSCE 5 1007 1 10 1a JSPE 3 14 216 1 7l ESJ 0 1 0 120 0JSAI 0 3 1 1 95(b) Text categorization resultsj u d g eSPSJ JSCE JSPE ESJ JSAIr SPSJ 1150 2 3 0 0e JSCE 2 1017 1 2 2a JSPE 5 9 226 1 0l ESJ 0 2 0 119 0JSAI 1 3 6 0 90Effect of local improvement:We also tested the categorization perfor-mance without local improvement where the top50 terms at most survive unconditionally afterforming the initial clusters.
In this case, theclustering works similarly to the automatic rel-evance feedback in information retrieval.
Us-ing the same data set, the result was 2,564 cor-rect judgments (F-value 0.971), which shows theeffectiveness of local improvement in reducingnoise in automatic relevance feedback.Effect of cluster duplication check:Because we do not apply any duplicationcheck in our generation step, the same clustermay appear repeatedly in the resulting clusterset.
We have also tested the other case whereclusters with terms or document sets identi-cal to existing better-performing clusters wereeliminated.
The obtained categorization per-formance was slightly worse than the one with-out elimination.
For example, the best perfor-mance obtained for ?
= 0.9 was 2,582 correctjudgments (F-value 0.978) with 137,867 (30%reduced) clusters.The results indicate that the system does notnecessarily require expensive redundancy checksfor the generated clusters as a whole.
Such con-sideration becomes necessary when the formu-lated clusters are presented to users, in whichcase, the duplication check can be applied onlylocally.5 DiscussionIn this paper, we reported a method of gener-ating overlapping micro-clusters in which doc-uments, terms, and other related elements oftext-based information are grouped together.Comparing the proposed micro-clusteringmethod with existing text categorization meth-ods, the distinctive feature of the former is thatthe documents on borders are readily viewedand examined.
In addition, the terms in thecluster can be further utilized in digesting thedescriptions of the clustered documents.
Suchproperties of micro-clustering may be particu-larly important when the system actually inter-acts with its users.For comparison purposes, we have used onlythe conventional documents-and- terms featurespace in our experiments.
However, the pro-posed micro-clustering framework can be ap-plied more flexibly to other cases as well.
Forexample, we have also generated clusters usingthe co-occurrences of the triple of documents,terms, and authors.
Although the performancewas not much different in terms of text catego-rization (2,584 correct judgments out of 2,639judgments, the precision slightly improved), wecan confirm that many of the highly ranked clus-ters contain documents produced by the samegroup of authors, emphasizing the characteris-tics of such generated clusters.Future issues include: (i) enhancing the prob-abilistic models considering other discountingtechniques in linguistic studies; (ii) developinga strategy for initiating clusters by combiningdifferent attribute sets, such as documents orauthors; and also (iii) establishing a methodof evaluating overlapping clusters.
We are alsolooking into the possibility of applying the pro-posed framework to Web document clusteringproblems.ReferencesA.
Aizawa.
2000.
The feature quantity: An informa-tion theoretic perspective of tfidf-like measures.In Proc.
of ACM SIGIR 2000, pages 104?111.A.
Aizawa.
2002.
An approach to microscopic clus-tering of terms and documents.
In Proc.
of the 7thPacific Rim Conference on Artificial Intelligence(to appear).R.
H. Baayen.
2001.
Word frequency distributions.Kluwer Academic Publishers.S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K.Landauer, and R. Harshman.
1990.
Indexing bylatent semantic analysis.
Journal of American So-ciety of Information Science, 41:391?407.I.
S. Dhillon and D. S. Modha.
1999.
Conceptdecomposition for large sparse text data usingclustering.
Technical Report Research Report RJ10147, IBM Almaden Research Center.I.
S. Dhillon.
2001.
Co-clustering documents andwords using bipartite spectral graph partitioning.Technical Report 2001-05, UT Austin CS Dept.M.
Iwayama and T. Tokunaga.
1995.
Cluster-basedtext categorization: a comparison of categorysearch strategies.
In Proc.
of ACM SIGIR?95,pages 273?281.T.
Joachims.
2001.
A statistical learning model oftext classification for support vector machines.
InProc.
of ACM SIGIR 2001, pages 128?136.G.
Karypis and E.-H. Han.
2000.
Fast superviseddimensionality reduction algorithm with applica-tions to document categorization and retrieval.
InProc.
of the 9th ACM International Conference onInformation and Knowledge Management, pages12?19.Y.
Matsumoto, A. Kitauchi, T. Yamashita, Y. Hi-rano, K. Matsuda, and M. Asahara.
1999.
Mor-phological analysis system chasen 2.0.2 usersmanual.
NAIST Technical Report NAIST-IS-TR99012, Nara Institute of Science and Technol-ogy.N.
Slonim and N. Tishby.
2000.
Document cluster-ing using word clusters via the information bot-tleneck method.
In Proc.
of ACM SIGIR 2000,pages 2008?2015.M.
Steinbach, G. Karypis, and V. Kumar.
2000.
Acomparison of document clustering techniques.
InKDD Workshop on Text Mining.O.
Zamir and O. Etzioni.
1998.
Web document clus-tering: A feasibility demonstration.
In Proc.
ofACM SIGIR?98, pages 46?54.
