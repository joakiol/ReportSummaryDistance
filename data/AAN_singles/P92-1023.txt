GPSM: A GENERALIZED PROBABILISTICSEMANTIC MODEL FOR AMBIGUITY RESOLUTIONtJing-Shin Chang, *Yih-Fen Luo and tKeh-Yih SutDepartment of Electrical EngineeringNational Tsing Hua UniversityHsinchu, TAIWAN 30043, R.O.C.tEmail: shin@ee.nthu.edu.tw, kysu@ee.nthu.edu.tw*Behavior Design CorporationNo.
28, 2F, R&D Road II, Science-Based Industrial ParkHsinchu, TAIWAN 30077, R.O.C.ABSTRACTIn natural anguage processing, ambiguity res-olution is a central issue, and can be regardedas a preference assignment problem.
In thispaper, a Generalized Probabilistic SemanticModel (GPSM) is proposed for preferencecomputation.
An effective semantic taggingprocedure is proposed for tagging semanticfeatures.
A semantic score function is de-rived based on a score function, which inte-grates lexical, syntactic and semantic prefer-ence under a uniform formulation.
The se-mantic score measure shows substantial im-provement in structural disambiguation overa syntax-based approach.1.
IntroductionIn a large natural language processing system,such as a machine translation system (MTS), am-biguity resolution is a critical problem.
Variousrule-based and probabilistic approaches had beenproposed to resolve various kinds of ambiguityproblems on a case-by-case basis.In rule-based systems, alarge number of rulesare used to specify linguistic constraints for re-solving ambiguity.
Any parse that violates the se-mantic constraints i  regarded as ungrammaticaland rejected.
Unfortunately, because very "rule"tends to have exception and uncertainty, and ill-formedness has significant contribution to the er-ror rate of a large practical system, such "hardrejection" approaches fail to deal with these situa-tions.
A better way is to find all possible interpre-tations and place emphases on preference, ratherthan weU-formedness (e.g., \[Wilks 83\].)
However,most of the known approaches for giving prefer-ence depend heavily on heuristics such as countingthe number of constraint satisfactions.
Therefore,most such preference measures can not be objec-tively justified.
Moreover, it is hard and cosilyto acquire, verify and maintain the consistency ofthe large fine-grained rule base by hand.Probabilistic approaches greatly relieve theknowledge acquisition problem because they areusually trainable, consistent and easy to meet cer-tain optimum criteria.
They can also providemore objective preference measures for "soft re-jection."
Hence, they are attractive for a large sys-tem.
The current probabilistic approaches have awide coverage including lexical analysis \[DeRose88, Church 88\], syntactic analysis \[Garside 87,Fujisaki 89, Su 88, 89, 91b\], restricted semanticanalysis \[Church 89, Liu 89, 90\], and experimentaltranslation systems \[Brown 90\].
However, thereis still no integrated approach for modeling thejoint effects of lexical, syntactic and semantic in-formation on preference valuation.A generalized probabilistic semantic model(GPSM) will be proposed in this paper to over-come the above problems.
In particular, an in-tegrated formulation for lexical, syntactic and se-mantic knowledge will be used to derive the se-mantic score for semantic preference valuation.Application of the model to structural disam-177biguation is investigated.
Preliminary experimentsshow about 10%-14% improvement of the seman-tic score measure over a model that uses syntacticinformation only.2.
Preference Assignment UsingScore FunctionIn general, a particular semantic interpretation ofa sentence can be characterized by a set of lexicalcategories (or parts of speech), a syntactic struc-ture, and the semantic annotations associated withit.
Among the-various interpretations of a sen-tence, the best choice should be the most probablesemantic interpretation for the given input words.In other words, the interpretation that maximizesthe following score function \[Su 88, 89, 91b\] oranalysis score \[Chen 91\] is preferred:Score (Semi, Sgnj, Lexk, Words)-- P (Semi, Synj, LezklWords)= P (SemilSynj, Lexk, Words)?
P (Syn I ILexk, Words)x P (LexklWords)(1)(semantic score)(syntactic score)(lexical score)where (Lex,, Synj, Semi) refers to the kth set oflexical categories, the jth syntactic structure andthe ith set of semantic annotations for the inputWords.
The three component functions are re-ferred to as semantic score (Ssem), syntactic score(Ssyn) and lexical score (Stex), respectively.
Theglobal preference measure will be referred to ascompositional score or simply as score.
In partic-ular, the semantic score accounts for the semanticpreference on a given set of lexical categories anda particular syntactic structure for the sentence.Various formulation for the lexical score and syn-tactic score had been studied extensively in ourprevious works \[Su 88, 89, 91b, Chiang 92\] andother literatures.
Hence, we will concentrate onthe formulation for semantic score.3.
Semantic TaggingCanonical Form of SemanticRepresentationGiven the formulation in Eqn.
(1), first we willshow how to extract he abstract objects (Semi,Synj, LexD from a semantic representation.
Ingeneral, a particular interpretation of a sentencecan be represented by an annotated syntax tree(AST), which is a syntax tree annotated with fea-ture structures in the tree nodes.
Figure 1 showsan example of AST.
The annotated version of anode A is denoted as A = A \[fa\] in the figure,where fA is the feature structure associated withnode A.
Because an AST preserves both syntacticand semantic information, it can be converted toother deep structure representations easily.
There-fore, without lose of generality, the AST represen-tation will be used as the canonical form of seman-tic representation for preference valuation.
Thetechniques used here, of course, can be applied toother deep structure representations as well.A\[~\] //-..<B\[fB\] C\[fc\]D\[fD\] E\[fE\] F\[fF\] G\[fc\]C I C2 C 3 C4(wl) (w2) (w3) (w4)Ls={A }L7={B, C }L~={B, F,  G }Ls={B, F,c4}L4={B, c3, c4}L3={D,E ,c3,ca}L2={D, c2, c3, c4}L1 ={Cl, C2, C3, C4 }Figure 1.
Annotated Syntax Tree(AST) and Phrase Levels (PL).The hierarchical AST can be represented bya set of phrase levels, such as L\] through L8 inFigure 1.
Formally, a phrase level (PL) is a setof symbols corresponding to a sententialform ofthe sentence.
The phrase levels in Figure 1 arederived from a sequence of rightmost derivations,which is commonly used in an LR parsing mech-anism.
For example, 1-,5 and L4 correspond to therightmost derivation B F Ca ~+ B c3 c4.
Note rmthat the first phrase level L\] consists of all lexicalcategories cl ... cn of the terminal words (wl ...w,,).
A phrase level with each symbol annotatedwith its feature structure is called an annotatedphrase level (APL).
The i-th APL is denoted asFi.
For example, L5 in Figure 1 has an annotatedphrase level F5 = {B \[fB\], F \[fF\], c4 \[fc,\]} as its178counterpart, where fc, is the atomic feature of thelexical category c4, which comes from the lexicalitem of the 4th word w4.
With the above nota-tions, the score function can be re-formulated asfollows:Score (Semi, Synj , Lexk, Words)- P (FT, L 7, c~ I,o7)n n = P (r~ nIL~ n , c 1 , wl )x P(LT'Ic , wDx P (c, \[w 1 )(2)(semantic score)(syntactic score)(lexical score)where c\]" (a short form for {cl ... c,,}) is thekth set of lexical categories (Lexk), /-,1" ({L\] ...Lr,,}) is the jth syntactic structure (Synj), and r l  m({F1 ... Fro}) is the ith set of semantic annotations(Semi) for the input words wl" ({wl ... wn}).
Agood encoding scheme for the Fi's will allow usto take semantic information into account with-out using redundant information.
Hence, we willshow how to annotate a syntax tree so that variousinterpretations can be characterized differently.Semantic TaggingA popular linguistic approach to annotate a treeis to use a unification-based mechanism.
How-ever, many information irrelevant to disambigua-tion might be included.
An effective encod-ing scheme should be simple yet can preservemost discrimination information for disambigua-tion.
Such an encoding scheme can be ac-complished by associating each phrase struc-ture rule A --+ X1X2.
.
.
XM with a head list(X i , ,X i , .
.
.X iM) .
The head list is formed byarranging the children nodes (X1 ,X2 , .
.
.
,XM)in descending order of importance to the compo-sitional semantics of their mother node A.
For thisreason, Xi~, Xi~ and Xi, are called the primary,secondary and the j-th heads of A, respectively.The compositional semantic features of the mothernode A can be represented asan ordered list of thefeature structures of its children, where the orderis the same as in the head list.
For example, forS ~ NP VP, we have a head list (VP, NP), be-cause VP is the (primary) head of the sentence.When composing the compositional semantics ofS, the features of VP and NP will be placed inthe first and second slots of the feature structureof S, respectively.Because not all children and all features ina feature structure am equally significant for dis-ambiguation, it is not really necessary to annotatea node with the feature structures of all its chil-dren.
Instead, only the most important N chil-dren of a node is needed in characterizing thenode, and only the most discriminative f ature ofa child is needed to be passed to its mother node.In other words, an N-dimensional feature vector,called a semantic N-tuple, could be used to char-acterize a node without losing much informationfor disambiguation.
The first feature in the se-mantic N-tuple comes from the primary head, andis thus called the head feature of the semantic N-tuple.
The other features come from the otherchildren in the order of the head list.
(Comparethese notions with the linguistic sense of head andhead feature.)
An annotated node can thus beapproximated as A ,~ A( f l , f2 , .
.
.
, fN), wheref j  = HeadFeature X~7~,~) is the (primary) headfeature of its j-th head (i.e., Xij) in the head list.Non-head features of a child node Xij will not bepercolated up to its mother node.
The head fea-ture of ~ itself, in this case, is fx.
For a terminalnode, the head feature will be the semantic tag ofthe corresponding lexical item; other features inthe N-tuple will be tagged as ~b (NULL).Figure 2 shows two possible annotated syn-tax trees for the sentence "... saw the boy inthe park."
For instance, the "loc(ation)" featureof "park" is percolated to its mother NP nodeas the head feature; it then serves as the sec-ondary head feature of its grandmother node PP,because the NP node is the secondary head ofPP.
Similarly, the VP node in the left tree is an-notated as VP(sta,anim) according to its primaryhead saw(sta,q~) and secondary head NP(anim,in).The VP(sta,in) node in the fight tree is tagged if-ferently, which reflects different attachment pref-erence of the prepositional phrase.By this simple mechanism, the major charac-teristics of the children, namely the head features,can be percolated to higher syntactic levels, and179sta: stative verbS S def."
definite article.
~ .
~ ~ ~ .
loc: locationanim: animate?t(a-hlLz-h2) ~~(~-h l ,~-h2)  ot(?X-hl~t~sta, in~.._ ~(~--h~,~-h~)N P ~  f ~ -  ~)saw( : ta : (~d)e~: ,~) in ( in~,def )the(def'~,?)
boy(-y~(~m,?
)in(in,~) ~ the(def#)/#)~p~par~Nk(loc,?
)the(def,t~) park(loc#)Figure 2.
Ambiguous PP attachment patterns annotated with semantic 2-tuples.their correlation and dependency an be taken intoaccount in preference evaluation even if they arefar apart.
In this way, different interpretations willbe tagged ifferently.
The preference on a partic-ular interpretation can thus be evaluated from thedistribution of the annotated syntax trees.
Basedon the above semantic tagging scheme, a seman-tic score will be proposed to evaluate the seman-tic preference on various interpretations for a sen-tence.
Its performance improvement over syntac-tic score \[Su 88, 89, 91b\] will be investigated.Consequently, a brief review of the syntactic scoreevaluation method is given before going into de-tails of the semantic score model.
(See the citedreferences for details.)4.
Syntactic ScoreAccording to Eqn.
(2), the syntactic score can beformulated as follows \[Su 88, 89, 91b\]:S,y,, =_ P(SynilLeZk,W'~) = P(L'~lc'~,w~) (3)f t i= HP(LtlL~-' ,c~,w~)1=21-I P (L, IL',-')~" I I  P(L' IL ' - ' )= HP({o~t ,  A,, /3,} I{o,,, ~',})180where at, fit are the left context and right contextunder which the derivation At =~ X1X2.
.
.
XMoccurs.
(Assume that Lt = {at, At,fit} andLI-1 = {at,X1,"" ,XM,fil}.)
If L left contextsymbols in al and R right context symbols in fitare consulted to evaluate the syntactic score, it issaid to operate in LLRR mode of operation.
Whenthe context is ignored, such an LoRo mode of oper-ation reduces to a stochastic context-free grammar.To avoid the normalization problem \[Su 91b\]arisen from different number of transition prob-abilities for different syntax trees, an alternativeformulation of the syntactic score is to evaluatethe transition probabilities between configurationchanges of the parser.
For instance, the config-uration of an LR parser is defined by its stackcontents and input buffer.
For the AST in Figure1, the parser configurations after the read of cl,c2, c3, c4 and $ (end-of-sentence) areequivalentto L1, L2, L4, 1-.5 and Ls, respectively.
Therefore,the syntactic score can be approximated as \[Su89, 91b\]:S, vn ~ P(Ls, LT'"  L2IL,) (4)P(LslL~) x P(LsIL4) x P(L41L2) x P(L21L1)In this way, the number of transition probabilitiesin the syntactic scores of all AST's will be keptthe same as the sentence l ngth.5.
Semantic ScoreSemantic score evaluation is similar to syntacticscore evaluation.
From Eqn.
(2), we have thefollowing semantic model for semantic score:S, em (Semi,  Synj  , Lex~:, Words)= p (p~n ILT, c~, w~)mI - -1  ra  n n = I"\[P(F,  IF1 ,L1 ,Cl,Wl)(5)1=21"I P(r, lr,_l)=where 3~j am the semantic tags from the chil-dren of A1.
For example, we have termslike e (VP(s ta ,  anim) \[ a, VP ~- v NP,fl) andP (VP(s ta ,  in) la ,  Ve~v NP PP,fl),respec-fively, for the left and right trees in Figure 2.
Theannotations of the context am ignored in evalu-ating Eqn.
(6) due to the assumption of seman-tics compositionality.
The operation mode will becalled LLRR+Alv, where N is the dimension of theN-tuple, and the subscript L (or R) refers to thesize of the context window.
With an appropriateN, the score will provide sufficient discriminationpower for general disambiguation problem with-out resorting to full-blown semantic analysis.where At = At ( f t , l , f ln , .
.
.
, fuv )  is the anno-tated version of At, whose semantic N-tuple is(fl,1, fl,2,-", ft,N), and 57, fit are the annotatedcontext symbols.
Only Ft.1 is assumed to be sig-nificant for the transition to Ft in the last equa-tion, because all required information is assumedto have been percolated to Ft-j through semanticscomposition.Each term in Eqn.
(5) can be interpreted asthe probability thatAt is annotated with the partic-ular set of head features (fs,1, f t ,2 , .
.
.
,  fI,N), giventhat X1 ... XM are reduced to At in the context ofa7 and fit.
So it can be interpreted informally asP(At (fl,1, ft,2, .
.
.
, fz ~v) I Ai ~ X1.
.
.
XM ,in the context of ~-7, fit ).
It corresponds tothe se-mantic preference assigned to the annotated nodeA t" Since (11,1, f l ,~,""  ft,N) are the head featuresfrom various heads of the substructures of A, eachterm reflects the feature co-occurrence preferenceamong these heads.
Furthermore, the heads couldbe very far apart.
This is different from mostsimple Markov models, which can deal with localconstraints only.
Hence, such a formulation wellcharacterizes long distance dependency among theheads, and provides a simple mechanism to incor-porate the feature co-occurrence preference amongthem.
For the semantic N-tuple model, the seman-tic score can thus be expressed as follows:S~.~ (6)m"~ I-\[ P ( A* (ft,,, f,,2 " " " ft,N) la, ,A,  , - -  X l  " " gM,/~l)l=21816.
Major Categories andSemantic FeaturesAs mentioned before, not all constituents areequally important for disambiguation.
For in-stance, head words are usually more importantthan modifiers in determining the compositionalsemantic features of their mother node.
There isalso lots of redundancy in a sentence.
For in-stance, "saw boy in park" is equally recogniz-able as "saw the boy in the park."
Therefore,only a few categories, including verbs, nouns, ad-jectives, prepositions and adverbs and their pro-jections (NP, VP, AP, PP, ADVP), are used tocarry semantic features for disambiguation.
Thesecategories are roughly equivalent to the major cat-egories in linguistic theory \[Sells 85\] with the in-clusion of adverbs as the only difference.The semantic feature of each major categoryis encoded with a set of semantic tags that welldescribes each category.
A few rules of thumbare used to select he semantic tags.
In particular,semantic features that can discriminate differentlinguistic behavior from different possible seman-tic N-tuples are preferred as the semantic tags.With these heuristics in mind, the verbs, nouns,adjectives, adverbs and prepositions are dividedinto 22, 30, 14, 10 and 28 classes, respectively.For example, the nouns are divided into "human,""plant," time," space," and so on.
These seman-tic classes come from a number of sources andthe semantic attribute hierarchy of the ArchTranMTS \[Su 90, Chen 91\].Table 1.
Close Test of Semantic Score7.
Test and AnalysisThe semantic N-tuple model is used to test theimprovement of the semantic score over syntacticscore in structure disambiguation.
Eqn.
(3) isadopted to evaluate the syntactic score in L2RImode of operation.
The semantic score is derivedfrom Eqn.
(6) in L2R~ +AN mode, for N = 1, 2,3, 4, where N is the dimension of the semanticS-tuple.A total of 1000 sentences (including 3 un-ambiguous ones) are randomly selected from 14computer manuals for training or testing.
Theyare divided into 10 parts; each part contains 100sentences.
In close tests, 9 parts are used bothas the training set and the testing set.
In opentests, the rotation estimation approach \[Devijver82\] is adopted to estimate the open test perfor-mance.
This means to iteratively test one part ofthe sentences while using the remaining parts asthe training set.
The overall performance is thenestimated as the average performance of the 10iterations.The performance is evaluated in terms of Top-N recognition rate (TNRR), which is defined asthe fraction of the test sentences whose preferredinterpretation is successfully ranked in the firstN candidates.
Table 1 shows the simulation re-suits of close tests.
Table 2 shows partial resultsfor open tests (up to rank 5.)
The recognitionrates achieved by considering syntactic score onlyand semantic score only are shown in the tables.
(L2RI+A3 and L2RI+A4 performance are the sameas L2R~+A2 in the present est environment.
Sothey are not shown in the tables.)
Since each sen-tence has about 70-75 ambiguous constructs onthe average, the task perplexity of the current dis-ambiguation task is high.ScoreRank123451318Syntax Semantics Semantics(L2R1) (L2RI+A1) (L2RI+A2)Count TNRR(%)78110195Count TNRR(%)87.07 87298.33 2099.33 599.89100.0097.21 86699.44 24100.00 421Count TNRR(%)96.5499.2299.6799.89100.00DataBase: 900 SentencesTest Set: 897 SentencesTotal Number of Ambiguous Trees = 63233(*) TNRR: Top-N Recognition RateTable 2.
Open Test of Semantic ScoreScore Syntax(L2R1)Rank Count TNRR(%)1 430 43.132 232 66A03 94 75.834 80 83.855 35 87.36Semantics(L2RI+A1)Count TNRR!
(%)569 57.07163 73.4290 82.4550 87.4622 89.67Semantics(L2RI+A2)Count TNRR(%)578 57.97167 74.7275 82.2549 87.1628 89.97DataBase: 900 Sentences (+)Test Set: 997 Sentences (++)Total Number of Ambiguous Trees = 75339(+) DataBase : effective database size for rotationestimation(++) Test Set : all test sentences participating therotation estimation test182The close test Top-1 performance (Table 1)for syntactic score (87%) is quite satisfactory.When semantic score is taken into account, sub-stantial improvement in recognition rate can beobserved further (97%).
This shows that the se-mantic model does provide an effective mecha-nism for disambiguation.
The recognition ratesin open tests, however, are less satisfactory underthe present est environment.
The open test per-formance can be attributed to the small databasesize and the estimation error of the parametersthus introduced.
Because the training database issmall with respect to the complexity of the model,a significant fraction of the probability entries inthe testing set can not be found in the training set.As a result, the parameters are somewhat "over-tuned" to the training database, and their valuesare less favorable for open tests.
Nevertheless,in both close tests and open tests, the semanticscore model shows substantial improvement oversyntactic score (and hence stochastic ontext-freegrammar).
The improvement is about 10% forclose tests and 14% for open tests.In general, by using a larger database and bet-ter robust estimation techniques \[Su 91a, Chiang92\], the baseline model can be improved further.As we had observed from other experiments forspoken language processing \[Su 91a\], lexical tag-ging, and structure disambiguation \[chiang 92\],the performance under sparse data condition canbe improved significantly if robust adaptive leam-ing techniques are used to adjust he initial param-eters.
Interested readers are referred to \[Su 91a,Chiang 92\] for more details.8.
Concluding RemarksIn this paper, a generalized probabilistic seman-tic model (GPSM) is proposed to assign semanticpreference to ambiguous interpretations.
The se-mantic model for measuring preference is basedon a score function, which takes lexical, syntacticand semantic information into consideration andoptimizes the joint preference.
A simple yet effec-tive encoding scheme and semantic tagging proce-dure is proposed to characterize various interpreta-183tions in an N dimensional feature space.
With thisencoding scheme, one can encode the interpre-tations with discriminative f atures, and take thefeature co-occurrence preference among variousconstituents into account.
Unlike simple Markovmodels, long distance dependency can be man-aged easily in the proposed model.
Preliminarytests show substantial improvement of the seman-tic score measure over syntactic score measure.Hence, it shows the possibility to overcome theambiguity resolution problem without resorting tofull-blown semantic analysis.With such a simple, objective and trainableformulation, it is possible to take high level se-mantic knowledge into consideration i statisticsense.
It also provides a systematic way to con-struct a disambiguation module for large practicalmachine translation systems without much humanintervention; the heavy burden for the linguists towrite fine-grained "rules" can thus be relieved.REFERENCES\[Brown 90\] Brown, P. et al, "A Statistical Ap-proach to Machine Translation," ComputationalLinguistics, vol.
16, no.
2, pp.
79-85, June1990.\[Chen 91\] Chen, S.-C., J.-S. Chang, J.-N. Wangand K.-Y.
Su, "ArchTran: A Corpus-BasedStatistics-Oriented English-Chinese MachineTranslation System," Proceedings of MachineTranslation Summit 11I, pp.
33-40, Washing-ton, D.C., USA, July 1-4, 1991.\[Chiang 92\] Chiang, T.-H., Y.-C. Lin and K.-Y.Su, "Syntactic Ambiguity Resolution Using ADiscrimination and Robustness Oriented Adap-tive Leaming Algorithm", to appear in Pro-ceedings of COLING-92, 14th Int.
Conferenceon Computational Linguistics, Nantes, France,20-28 July, 1992.\[Church 88\] Church, K., "A Stochastic Parts Pro-gram and Noun Phrase Parser for UnrestrictedText," ACL Proc.
2nd Conf.
on Applied Natu-ral Language Processing, pp.
136-143, Austin,Texas, USA, 9-12 Feb. 1988.\[Church 89\] Church, K. and P. Hanks, "Word As-sociation Norms, Mutual Information, and Lex-icography," Proc.
27th Annual Meeting of theACL, pp.
76-83, University of British Colum-bia, Vancouver, British Columbia, Canada, 26-29 June 1989.\[DeRose 88\] DeRose, SteverL J., "GrammaticalCategory Disambiguation by Statistical Opti-mization," Computational Linguistics, vol.
14,no.
1, pp.
31-39, 1988.\[Devijver 82\] Devijver, P.A., and J. Kittler,Pattern Recognition: A Statistical Approach,Prentice-Hall, London, 1982.\[Fujisaki 89\] Fujisaki, T., F. Jelinek, J. Cocke, E.Black and T. Nishino, "A Probabilistic ParsingMethod for Sentence Disambiguation," Proc.
ofInt.
Workshop on Parsing Technologies (IWPT-89), pp.
85-94, CMU, Pittsburgh, PA, U.S.A.,28-31 August 1989.\[Garside 87\] Garside, Roger, Geoffrey Leech andGeoffrey Sampson (eds.
), The ComputationalAnalysis of English: A Corpus-Based Approach,Longman Inc., New York, 1987.\[Liu 89\] Liu, C.-L., On the Resolution of EnglishPP Attachment Problem with a Probabilistic Se-mantic Model, Master Thesis, National TsingHua University, Hsinchu, TAIWAN, R.O.C.,1989.\[Liu 90\] Liu, C.-L, J.-S. Chang and K.-Y.
Su,"The Semantic Score Approach to the Disam-biguation of PP Attachment Problem," Proc.
of?
ROCLING-III, pp.
253-270, Taipei, R.O.C.,September 1990.\[Sells 85\] Sells, Peter, Lectures On Con-temporary Syntactic Theories: An Introduc-tion to Government-Binding Theory, General-ized Phrase Structure Grammar, and Lexical-Functional Grammar, CSLI Lecture NotesNumber 3, Center for the Study of Languageand Information, Leland Stanford Junior Uni-versity., 1985.\[Su 88\] Su, K.-Y.
and J.-S. Chang, "Semantic andSyntactic Aspects of Score Function," Proc.
ofCOLING-88, vol.
2, pp.
642-644, 12th Int.Conf.
on Computational Linguistics, Budapest,Hungary, 22-27 August 1988.\[Su 89\] Su, K.-Y., J.-N. Wang, M.-H. Su and J.-S.Chang, "A Sequential Truncation Parsing Algo-rithm Based on the Score Function," Proc.
ofInt.
Workshop on Parsing Technologies (IWPT-89), pp.
95-104, CMU, Pittsburgh, PA, U.S.A.,28-31 August 1989.\[Su 90\] Su, K.-Y.
and J.-S. Chang, "Some KeyIssues in Designing MT Systems," MachineTranslation, vol.
5, no.
4, pp.
265-300, 1990.\[Su 91a\] Su, K.-Y., and C.-H. Lee, "Robusmessand Discrimination Oriented Speech Recog-nition Using Weighted HMM and SubspaceProjection Approach," Proceedings of IEEEICASSP-91, vol.
1, pp.
541-544, Toronto, On-tario, Canada.
May 14-17, 1991.\[Su 91b\] Su, K.-Y., J.-N. Wang, M.-H. Su, and J.-S. Chang, "GLR Parsing with Scoring".
In M.Tomita (ed.
), Generalized LR Parsing, Chapter7, pp.
93-112, Kluwer Academic Publishers,1991.\[Wilks 83\] Wilks, Y.
A., "Preference Semantics,Ul-Formedness, and Metaphor," AJCL, vol.
9,no.
3-4, pp.
178 - 187, July - Dec. 1983.184
