Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 96?100,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsDisambiguation of Preposition Sense Using Linguistically MotivatedFeaturesStephen Tratz and Dirk HovyInformation Sciences InstituteUniversity of Southern California4676 Admiralty Way, Marina del Rey, CA 90292{stratz,dirkh}@isi.eduAbstractIn this paper, we present a supervised classifi-cation approach for disambiguation of prepo-sition senses.
We use the SemEval 2007Preposition Sense Disambiguation datasets toevaluate our system and compare its results tothose of the systems participating in the work-shop.
We derived linguistically motivated fea-tures from both sides of the preposition.
In-stead of restricting these to a fixed windowsize, we utilized the phrase structure.
Testingwith five different classifiers, we can report anincreased accuracy that outperforms the bestsystem in the SemEval task.1 IntroductionClassifying instances of polysemous words intotheir proper sense classes (aka sense disambigua-tion) is potentially useful to any NLP applicationthat needs to extract information from text or builda semantic representation of the textual information.However, to date, disambiguation between preposi-tion senses has not been an object of great study.
In-stead, most word sense disambiguation work has fo-cused upon classifying noun and verb instances intotheir appropriate WordNet (Fellbaum, 1998) senses.Prepositions have mostly been studied in the con-text of verb complements (Litkowski and Hargraves,2007).
Like instances of other word classes, manyprepositions are ambiguous, carrying different se-mantic meanings (including notions of instrumental,accompaniment, location, etc.)
as in ?He ran withdetermination?, ?He ran with a broken leg?, or ?Heran with Jane?.
As NLP systems take more and moresemantic content into account, disambiguating be-tween preposition senses becomes increasingly im-portant for text processing tasks.In order to disambiguate different senses, mostsystems to date use a fixed window size to deriveclassification features.
These may or may not besyntactically related to the preposition in question,resulting?in the worst case?in an arbitrary bag ofwords.
In our approach, we make use of the phrasestructure to extract words that have a certain syn-tactic relation with the preposition.
From the wordscollected that way, we derive higher level features.In 2007, the SemEval workshop presented par-ticipants with a formal preposition sense dis-ambiguation task to encourage the developmentof systems for the disambiguation of prepositionsenses (Litkowski and Hargraves, 2007).
The train-ing and test data sets used for SemEval have been re-leased to the general public, and we used these datato train and test our system.
The SemEval work-shop data consists of instances of 34 prepositionsin natural text that have been tagged with the ap-propriate sense from the list of the common Eng-lish preposition senses compiled by The PrepositionProject, cf.
Litkowski (2005).
The SemEval dataprovides a natural method for comparing the per-formance of preposition sense disambiguation sys-tems.
In our paper, we follow the task requirementsand can thus directly compare our results to the onesfrom the study.
For evaluation, we compared our re-sults to those of the three systems that participatedin the task (MELB: Ye and Baldwin (2007); KU:Yuret (2007); IRST: Popescu et al (2007)).
We alsoused the ?first sense?
and the ?most frequent sense?96baselines (see section 3 and table 1).
These baselinesare determined by the TPP listing and the frequencyin the training data, respectively.
Our system beatthe baselines and outperformed the three participat-ing systems.2 Methodology2.1 Data PreparationWe downloaded the test and training data providedby the SemEval-2007 website for the prepositionsense disambiguation task.
These are 34 separateXML files?one for each preposition?, comprising16557 training and 8096 test example sentences,each sentence containing one example of the respec-tive preposition.What are your beliefs<head>about</head> these emotions ?The preposition is annotated by a head tag, and themeaning of the preposition in question is given asdefined by TPP.Each preposition had between 2 and 25 differentsenses (on average 9.76).
For the case of ?about?these would be1.
on the subject of; concerning2.
so as to affect3.
used to indicate movement within a particulararea4.
around5.
used to express location in a particular place6.
used to describe a quality apparent in a personWe parsed the sentences using the Charniakparser (Charniak, 2000).
Note that the Charniakparser?even though among the best availbale Eng-lish parsers?occasionally fails to parse a sentencecorrectly.
This might result in an erroneous extrac-tion, such as an incorrect or no word.
However,these cases are fairly rare, and we did not manuallycorrect this, but rather relied on the size of the datato compensate for such an error.After this preprocessing step, we were able to ex-tract the features.2.2 Feature ExtractionFollowing O?Hara and Wiebe (2003) andAlam (2004), we assumed that there is a meaningfulconnection between syntactically related words onboth sides of the preposition.
We thus focused onspecific words that are syntactically related to thepreposition via the phrase structure.
This has theadvantage that it is not limited to a certain windowsize; phrases might stretch over dozens of words,so the extracted word may occur far away from theactual preposition.
These words were chosen basedon a manual analysis of training data.
Using Tregex(Levy and Andrew, 2006), a utility for expressing?regular expressions over trees?, we created a setof rules to extract the words in question.
Each rulematched words that exhibited a specific relationshipwith the preposition or were within a two wordwindow to cover collocations.
An example rule isgiven below.IN > (PP < (V P < # = x& <#!AUX))This particular rule finds the head (denoted by x) ofa verb phrase that governs the prepositional phrasecontaining the preposition, unless x is an auxiliaryverb.
Tregex rules were used to identify the follow-ing words for feature generation:?
the head verb/noun that immediately dominatesthe preposition along with all of its modifyingdeterminers, quantifiers, numbers, and adjec-tives?
the head verb/noun immediately dominated bythe preposition along with all of its modifyingdeterminers, quantifiers, numbers, and adjec-tives?
the subject, negator, and object(s) of the imme-diately dominating verb?
neighboring prepositional phrases dominatedby the same verb/noun (?sister?
prepositionalphrases)?
words within 2 positions to the left or right ofthe prepositionFor each word extracted using these rules, we col-lected the following items:97?
the word itself?
lemma?
part-of-speech (both exact and conflated, e.g.both ?VBD?
and ?verb?
for ?VBD?)?
all synonyms of the first WordNet sense?
all hypernyms of the first WordNet sense?
boolean indicator for capitalizationEach feature is a combination of the extractionrule and the extracted item.
The values the featurecan take on are binary: present or absent.
For someprepositions, this resulted in several thousand fea-tures.
In order to reduce computation time, we usedthe following steps: For each preposition classifier,we ranked the features using information gain (For-man, 2003).
From the resulting lists,we included atmost 4000 features.
Thus not all classifiers used thesame features.2.3 Classifier TrainingWe chose maximum entropy (Berger et al, 1996) asour primary classifier, since it had been successfullyapplied by the highest performing systems in boththe SemEval-2007 preposition sense disambiguationtask (Ye and Baldwin, 2007) and the general wordsense disambiguation task (Tratz et al, 2007).
Weused the implementation provided by the Mallet ma-chine learning toolkit (McCallum, 2002).
For thesake of comparison, we also built several other clas-sifiers, including multinomial na?
?ve Bayes, SVMs,kNN, and decision trees (J48) using the WEKAtoolkit (Witten, 1999).
We chose the radial basisfunction (RBF) kernel for the SVMs and left allother parameters at their default values.3 ResultsWe measured the accuracy of the classifiers overthe test set provided by SemEval-2007 and providedthese results in Table 1.
It is notable that our systemproduced good results with all classifiers: For threeof the classifiers, the accuracy is higher than MELB,the winning system of the task.
As expected, thehighest accuracy was achieved using the maximumentropy classifier.
Overall, our system outperformedthe winning system by 0.058, an 8 percent improve-ment.
A simple proportion test shows this to be sta-tistically significant at 0.001.
????????????????????
?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
?Table 1: Accuracy results on SemEval data (with 4000features)Since our initial cutoff of 4000 features was ar-bitrary, we reran our Maximum Entropy experimentmultiple times with different cutoffs.
Accuracy con-sistently increased as the feature limit was relaxed,resulting in 0.764 accuracy at the 10k feature limit.These results are displayed in Figure 1.its modifying determiners, quantifiers,numbers, and adjectives?
the head verb/noun immediately domi-nated by the preposition along with all ofits modifying determiners, quantifiers,numbers, and adjectives?
the subject, negator, and object(s) of theimmediately dominating verb?
neighboring prepositional phrases domi-nated by the same verb/noun (?sister?prepositional phrases)?
words within 2 positions to the left  or rightof the prepositionFor words extracted using these rules, we col-lected the following features:?
the word itself?
lemma?
part-of-speech (both exact  and conflated(e.g.
both 'VBD' and 'verb' for 'VBD'))?
synonyms of the first WordNet sense?
hypernyms of the first WordNet s nse?
boolean indicator for capitalizationThis resulted in several thousand features for theprepositions.
We used information gain (Foreman,2003) in order to find the highest ranking featuresof each class and limited our classifiers to the top4000 features in order to reduce computation time.2.3 Classifier TrainingWe chose maximum entropy (Berger, 1996) as ourprimary classifier because the highest performingsystems in both the SemEval-2007 prepositionsense disambiguation task (Ye and Baldwin, 2007)and the general word sense dis mbiguation t sk(Tratz et al, 2007) used it.
We used the implemen-tation provided by the Mallet machine learningtoolkit (McCallum, 2002).
Then, for the sake ofcomparison, we also built several other classifiersincluding multinomial na?ve Bayes, SVMs, kNN,and decision trees (J48) using the WEKA toolkit(Witten, 1999).
We chose the radial basis function(RBF) kernel for the SVMs and left all other pa-rame ers at their default values.3 ResultsWe measured the accuracy of the classifiers overthe test  et provided by SemEval-2007 and pro-vided these results in Table 1.
It  is notable that  oursystem produced good results with all classifiers:For three of the classifiers, the accuracy is higherthan MELB, the winning system of the task.
Asexpected, the highest  accuracy was achieved usingthe maximum entropy classifier.Overall, our system outperformed the winningsystem by 0.058, an 8 percent improvement.
Asimple proportion test  shows this to be statisticallysignificant at 0.001.System AccuracykNN 684SVM (RBF Kernel) 692J48 decision trees 712Multinomial Na?ve Bayes 731Maximum Entropy 751Most Frequent Sense 396IRST (Popescu et al, 2007) 496KU (Yuret, 2007) 547MELB (Ye and Baldwin, 2007) 693Table 1.
Accuracy results on SemEval-2007 data.Since our initial cutoff of 4000 features was arbi-trary, we reran our Maximum Entropy experimentmultiple times with different cutoffs.
Accuracyconsistently increased as the feature limit was re-laxed, resulting in 0.764 accuracy at  the 10k fea-ture limit.
These results re displayed in Figure 1.Figure 1.
Relationship between maximum feature limitand accuracy for the Maximum Entropy classifiers.4 Related WorkThe linguistic literature on prepositions and theiruse is copious and diverse.
We restrict ourselves tothe works that deal with preposition sense disam-biguation in computational linguistics.O'Hara and Wiebe (2003) make use of PennTreebank (Marcus et  al., 1993) and FrameNet(Baker et  al., 1998) to classify prepositions.
Theyshow that  using high level features from the con-text, such as semantic roles, significantly aids dis-Figure 1: Maximum feature limit vs. accuracy for maxi-mum entropy classifier4 Related WorkThe linguistic literature on prepositions and their useis copious and div rse.
We restrict our lves to thesystems that compet d in the SemEval 2007 Prepo-sition Sense Disambiguation task.
All three of thesystems within the framework of the SemEval taskused supervised learning algorithms, yet they dif-fered widely in the data collection and model prepa-ration.98Ye and Baldwin (2007) participated in the Sem-Eval task using a maximum entropy classifier andachieved the highest accuracy of the participatingsystems.
The features they extracted were similarto the ones we used, including POS and WordNetfeatures, but they used a substantially larger wordwindow, taking seven words from each side of thepreposition.
While they included many higher levelfeatures, they state that the direct lexical context(i.e., bag-of-words) features were the most effectiveand account for the majority of features, while syn-tactic and semantic features had relatively little im-pact.Yuret (2007) used a n-gram model based on wordsubstitution by synonyms or antonyms.
While thisproved to be quite successful with content words, ithad considerable problems with prepositions, sincethe number of synonyms and/or antonyms is fairlylimited.Popescu et al (2007) take an interesting approachwhich they call Chain Clarifying Relationship.
Theyare using a supervised algorithm to learn a regu-lar language.
They used the Charniak parser andFrameNet information on the head, yet the featuresthey extract are generally not linguistically moti-vated.5 DiscussionUsing the phrase structure allows for more freedomin the choice of words for feature selection, yet stillguarantees to find words for which some syntacticrelation with the preposition holds.
Extracting se-mantic features from these words (hypernyms, syn-onyms, etc.)
allows for a certain degree of abstrac-tion, and thus a high level comparison.
O?Hara andWiebe (2003) also make use of high level features,in their case the Penn Treebank (Marcus et al, 1993)and FrameNet (Baker et al, 1998) to classify prepo-sitions.
They show that using high level features?such as semantic roles?of words in the context sub-stantially aids disambiguation efforts.
They cau-tion, however, that indiscriminately using colloca-tions and neighboring words may yield high accu-racy, but has the risk of overfitting.
In order to mit-igate this, they classify the features by their part ofspeech.
While we made use of collocation features,we also took into account higher order aspects of thecontext, such as the governing phrase, part of speechtype, and semantic class according to WordNet.
Allother things being equal, this seems to increase per-formance substantially.As for the classifiers used, our results seem toconfirm that Maximum Entropy classifiers are verywell suited for disambiguation tasks.
Other thanna?
?ve Bayes, they do not presuppose a conditionalindependence between the features, which clearlynot always holds (quite contrary, the underlying syn-tactic structure creates strong interdependencies be-tween words and features).
This, however, does notsatisfactory explain the ranking of the other classi-fiers.
One possible explanation could be the sensi-tivity of for example decision trees to random noise.Though we made use of information gain beforeclassification, there still seems to be a certain ten-dency to split on features that are not optimal.6 ConclusionWe showed that using a number of simple linguis-tically motivated features can improve the accu-racy of preposition sense disambiguation.
Utilizingwidely used and freely available standard tools forlanguage processing and a set of simple rules, wewere able to extract these features easily and withvery limited preprocessing.
Instead of taking a ?bagof words?
approach that focuses primarily upon thewords within a fixed window size, we focused on el-ements that are related via the phrase structure.
Wealso included semantic information gathered fromWordNet about the extracted words.
We comparedfive different classifiers and demonstrated that theyall perform very well, using our selected feature set.Several of them even outperformed the top systemat SemEval.
Our best result was obtained using amaximum entropy classifier, just as the best partici-pating system, leading us to believe that our primaryadvantage was our feature set.
While the contribu-tion of the direct context (+/-7 words) might havea stronger effect than higher level features (Ye andBaldwin, 2007), we conclude from our findings thathigher level features do make an important contribu-tion.
These results are very encouraging on severallevels, and demonstrate the close interaction of syn-tax and semantics.
Leveraging these types of fea-tures effectively is a promising prospect for future99machine learning research in preposition sense dis-ambiguation.AcknowledgementsThe authors would like to thank Eduard Hovy andGully Burns for invaluable comments and helpfuldiscussions.ReferencesY.S.
Alam.
2004.
Decision Trees for Sense Disambigua-tion of Prepositions: Case of Over.
In HLT-NAACL2004: Workshop on Computational Lexical Semantics,pages 52?59.C.F.
Baker, C.J.
Fillmore, and J.B. Lowe.
1998.The Berkeley FrameNet Project.
In Proceedings ofthe 17th international conference on Computationallinguistics-Volume 1, pages 86?90.
Association forComputational Linguistics Morristown, NJ, USA.A.L.
Berger, V.J.
Della Pietra, and S.A. Della Pietra.1996.
A maximum entropy approach to naturallanguage processing.
Computational Linguistics,22(1):39?71.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In ACM International Conference Proceeding Series,volume 4, pages 132?139.C.
Fellbaum.
1998.
WordNet: an electronic lexicaldatabase.
MIT Press USA.G.
Forman.
2003.
An extensive empirical study of fea-ture selection metrics for text classification.
The Jour-nal of Machine Learning Research, 3:1289?1305.R.
Levy and G. Andrew.
2006.
Tregex and Tsurgeon:tools for querying and manipulating tree data struc-tures.
In LREC 2006.Ken Litkowski and Orin Hargraves.
2007.
SemEval-2007 Task 06: Word-Sense Disambiguation of Prepo-sitions.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations (SemEval-2007),Prague, Czech Republic.Ken Litkowski.
2005.
The preposition project.http://www.clres.com/prepositions.html.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of Eng-lish: the Penn TreeBank.
Computational Linguistics,19(2):313?330.A.K.
McCallum.
2002.
MALLET: A Machine Learningfor Language Toolkit.
2002. http://mallet.
cs.
umass.edu.T.
O?Hara and J. Wiebe.
2003.
Preposition semanticclassification via Penn Treebank and FrameNet.
InProceedings of CoNLL, pages 79?86.Octavian Popescu, Sara Tonelli, and Emanuele Pianta.2007.
IRST-BP: Preposition Disambiguation based onChain Clarifying Relationships Contexts.
In MELB-YB: Preposition Sense Disambiguation Using Rich Se-mantic Features, Prague, Czech Republic.S.
Tratz, A. Sanfilippo, M. Gregory, A. Chappell,C.
Posse, and P. Whitney.
2007.
PNNL: A SupervisedMaximum Entropy Approach to Word Sense Disam-biguation.
In Proceedings of the Fourth InternationalWorkshop on Semantic Evaluations (SemEval-2007).I.H.
Witten.
1999.
Weka: Practical Machine Learn-ing Tools and Techniques with Java Implementations.Dept.
of Computer Science, University of Waikato,University of Waikato, Dept.
of Computer Science.Patrick Ye and Timothy Baldwin.
2007.
MELB-YB:Preposition Sense Disambiguation Using Rich Seman-tic Features.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations (SemEval-2007),Prague, Czech Republic.Deniz Yuret.
2007.
Ku: Word sense disambiguation bysubstitution.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations (SemEval-2007),Prague, Czech Republic.100
