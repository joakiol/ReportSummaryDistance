Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 732?742,Dublin, Ireland, August 23-24, 2014.UNAL-NLP: Combining Soft Cardinality Features for SemanticTextual Similarity, Relatedness and EntailmentSergio Jimenez, George Due?nas,and Julia BaqueroUniversidad Nacional de ColombiaCiudad Universitaria, edificio 453,oficina 114, Bogot?a, Colombia[sgjimenezv,geduenasl,jmbaquerov]@unal.edu.coAlexander GelbukhCenter for Computing Research (CIC),Instituto Polit?ecnico Nacional (IPN),Av.
Juan Dios B?atiz, Av.
Mendiz?abal,Col.
Nueva Industrial Vallejo,Mexico City, Mexicowww.gelbukh.comAbstractThis paper describes our participation inthe SemEval-2014 tasks 1, 3 and 10.
Weused an uniform approach for addressingall the tasks using the soft cardinality forextracting features from text pairs, andmachine learning for predicting the goldstandards.
Our submitted systems rankedamong the top systems in all the task andsub-tasks in which we participated.
Theseresults confirm the results obtained in pre-vious SemEval campaigns suggesting thatthe soft cardinality is a simple and usefultool for addressing a wide range of naturallanguage processing problems.1 IntroductionThe semantic textual similarity is a core prob-lem in the computational linguistic field.
Con-sequently, the previous evaluation campaigns ofthis task in SemEval have attracted the attentionof many research groups worldwide (Agirre et al.,2012; Agirre et al., 2013).This year, 3 tasks relatedto this problem have been proposed exploring dif-ferent facets such as semantic relatedness, entail-ment , multilingualism, lack of training data andimbalance in the amount of information.The soft cardinality (Jimenez et al., 2010) is asimple concept that generalizes the classical setcardinality by considering the similarities amongthe elements in a collection for a more intuitivequantification of the number of elements in thatcollection.
This approach can be applied to textapplications representing texts as collections ofwords and providing a similarity function thatcompares two words.
Varying this word-to-wordsimilarity function the soft cardinality can reflectThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/notions of syntactic similarity, semantic related-ness, among others.
We (and others) have usedthis approach to address with success the semantictextual similarity and other tasks in previous Se-mEval editions (Jimenez et al., 2012b; Jimenez etal., 2012a; Jimenez et al., 2013a; Jimenez et al.,2013b; Jimenez et al., 2013c; Croce et al., 2013).In this paper we describe our participating sys-tems in the SemEval-2014 tasks 1, 3, and 10,which used the soft cardinality as core approach.2 Features from Soft CardinalitiesThe cardinality of a collection of elements is thecounting of non-repeated elements in it.
This def-inition is intrinsically associated with the notionof set, which is a collection of non-repeated ele-ments.Thus, the cardinality of a collection or setA is denoted as |A|.
Clearly, the cardinality of acollection with repeated elements treats groups ofidentical elements as a single instance contribut-ing only with a unit (1) to the element counting.Jimenez et al.
(2010) proposed the soft cardinal-ity that uses a notion of similarity among elementsfor grouping not only identical elements but simi-lar too.
That notion of similarity among elementsis provided by a similarity function that comparestwo elements aiand ajand returns a score in [0,1]interval, having sim(ai, ai) = 1.
Although, itis not necessary that sim fulfills another metricproperties aside of identity, symmetry is also de-sirable.
Thus, the soft cardinality of a collectionA, whose elements a1, a2, .
.
.
, a|A|are compara-ble with a similarity function sim(ai, aj), is de-noted as |A|sim.
This soft cardinality is given bythe following expression:|A|sim=|A|?i=1wai?|A|j=1sim(ai, aj)p(1)It is trivial to see that |A| = |A|simeither ifp ?
?
or when the function sim is a crisp com-732Basic Derived|A| |A ?B| = |A|+ |B| ?
|A ?B||B| |A4B| = |A ?B| ?
|A ?B|||A ?B| |A \B| = |A| ?
|A ?B||B \A| = |B| ?
|A ?B|Table 1: The 7 basic and derived cardinalities fortwo sets comparison.parator, i.e.
one that returns 1 for identical ele-ments and 0 otherwise.
This property shows thatthe soft cardinality generalizes the classical cardi-nality and that the parameter p controls its degreeof ?softness?, whose default value is 1.
The valueswaiare optional ?importance?
weights associatedwith each element ai, by default those weights canbe assigned to 1.For the tasks at hand, we represent each shorttext (lets say A) as a collection of words aiandthe sim function can be any operator that com-pares pairs of words.
The motivation for using thesoft cardinality is that the sim function can reflectany dimension of word similarity (e.g.
syntactic,semantic) and the soft cardinality projects that no-tion at sentence level.
For instance, if sim pro-vides the degree of semantic relatedness betweentwo words using WordNet, two texts A and Bcould be compared by computing |A|sim, |B|simand |A?B|sim.
Given that A?B could be empty,the soft cardinality of the intersection must be ap-proximated by |A ?
B|sim?
|A|sim+ |B|sim?|A ?
B|siminstead of being computed directlyfrom A ?
B using equation 1.
Using that approx-imation, the commonality (intersection) betweenA and B is induced by the pair-wise similaritiesprovided by sim among the words in A and B.Since more than a century when Jaccard (1901)proposed his well-known index, the classical setcardinality has been used to build similarity func-tions for set comparison.
Any binary-cardinality-based similarity function is an algebraic combina-tion of |A|, |B| and either |A ?
B| or |A ?
B|(e.g.
Jaccard, Dice, Tversky, overlap and cosineindexes).
These three cardinalities describes un-ambiguously all the regions in the Venn?s diagramwhen comparing two sets.
Thus, in this scenario 4possible cardinalities can be derived from these 3basic cardinalities, see Table 1.
Clearly, the sameset of cardinalities can be obtained for the soft car-dinality.When training data is available, which is the# Feature expression1|A|/|A?B|2|A|?|A?B|/|A|3|A|?|A?B|/|A?B|4|B|/|A?B|5|B|?|A?B|/|B|6|B|?|A?B|/|A?B|7|A?B|/|A|8|A?B|/|B|9|A?B|/|A?B|10|A?B|?|A?B|/|A?B|Table 2: Extended set of 10 rational features.case for tasks 1, 3 and 10 in SemEval 2014, itis possible to think that instead of using an ad-hoc expression (e.g.
Jaccard, Dice) the similar-ity function can be obtained using the cardinalitiesin Table 1 as features for a machine-learning re-gression algorithm.
Our hypothesis is that suchlearnt function should predict in a more accurateway the gold standard variable than any other ad-hoc function.
However, these cardinality featuresare intrinsically correlated with the length of thetexts where they were obtained.
This correlationmakes that the performance of the learnt similar-ity function could be dependent of the length ofthe texts.
For instance, if the function was trainedusing long texts it is plausible to think that thisfunction would be more effective when tested withlong texts than with shorter ones.
Having this inmind, an extended set of rational features is pro-posed, whose values are standardized in [0,1] in-terval aiming to reduce the effect of the length ofthe texts.
These features are presented in Table 2.The soft cardinality has proven to overcomethe classic cardinality in the semantic textualsimilarity (STS) task in previous SemEval cam-paigns (Jimenez et al., 2012b; Jimenez et al.,2013a).
Even using a simplistic function simbased on q-grams of characters, the soft cardinal-ity method ranked third among 89 participatingsystems (Agirre et al., 2012).
Thus, our participat-ing systems in the SemEval 2014 campaign werebased on the previously described set of 17 fea-tures, obtained from the soft cardinality with dif-ferent sim functions for comparing pairs of words.Each sim function produced a different set of fea-tures, which were combined with a regression al-gorithm for similarity and relatedness tasks.
Sim-ilarly, a classification algorithm was used for the733entailment task.3 Systems DescriptionIn this section the different feature sets used foreach submitted system to the different task andsubtask are described.
Besides, the data used fortraining, parameters and other preprocessing de-tails are described for each system.3.1 Task 1: Textual Relatedness andEntailmentThe task 1 is based on the SICK (SentencesInvolving Compositional Knowledge) data set(Marelli et al., 2014), which contains nearly10,000 pairs of sentences manually labeled by re-latedness and entailment.
The relatedness gold la-bels range from 1 to 5, having 1 the minimum levelof relatedness between the texts and 5 for the max-imum.
The entailment labels have three categori-cal values: neutral, contradiction and entailment.The two sub tasks consist of predicting the related-ness and entailment gold standards using approxi-mately the 50% of the text pairs as training and theother part as test bed.Our overall approach consists in extracting 4different sets of features using the method pre-sented in section 2 and training a machine learn-ing algorithm for predicting the gold standard la-bels in the test data.
Each feature set is describedin the following 4 subsections and the subsection3.1.6 provides details of the used combination offeatures, machine learning algorithm and prepro-cessing details.3.1.1 String-Matching FeaturesFirst, all texts in the SICK data set where prepro-cessed by lower casing, tokenizing and stop-wordremoval (using the NLTK1).
Then each word wasreduced to its stem using the Porter?s algorithm(Porter, 1980) and a idf weight (Jones, 2004) wasassociated to each stem (waiweights in eq.
1) us-ing the very SICK data set as document collec-tion.
Next, for each instance in the data, whichis composed of two texts A and B, the 17 fea-tures listed in Tables 1 and 2 where extracted usingeq.1.
The used word-to-word similarity functionsim decomposes each word in bags of 3-gramsof characters, which are compared using the sym-metrical Tversky?s index (Tversky, 1977; Jimenezet al., 2013a).
Thus, the similarity between two1http://www.nltk.org/pairs of words w1and w2, represented each one asa collection of 3-grams of characters, is given bythe following expression:sim(w1, w2) =|c|?
(?|wmin|+ (1?
?
)|wmax|) + |c|(2)|c| = |w1?
w2|+ biassim,|wmin| = min[|w1\ w2|, |w2\ w1|],|wmax| = max[|w1\ w2|, |w2\ w1|].The values used for the parameters were ?
=1.9, ?
= 2.36, bias = ?0.97, and p = 0.39(where p corresponds to eq.1).
The motivation andjustification for these parameters can be found in(Jimenez et al., 2013a).
These values were ob-tained by building a text similarity function us-ing the Dice?s coefficient and the soft cardinali-ties plugging eq.2 in eq.1.
Next, this text similar-ity function is evaluated in the 5,000 training textpairs and the obtained scores are compared againstthe relatedness gold-standard using the Pearson?scorrelation.waiare not training parameters, but they areweights associated with the words.
These weightscould have been obtained from a larger corpus,but we use the training texts to obtain them.
Thisprocess is repeated iteratively exploring the searchspace defined by these 4 parameters using a hill-climbing approach until a maximum correlation isreached.
We observe that the optimal values of theparameters p, ?, ?, and bias vary considerably be-tween the data sets and for the different sim func-tions of word-to-word similarity.
We do not yetunderstand from which factors of the data and thesim functions depend on these parameters.
Thisissue will be the objective of further research.Henceforth, the set of 17 string-based featuresdescribed in this subsection will be referred asSM.3.1.2 ESA FeaturesFor this set of features we used the idea proposedby Gabrilovich and Markovitch (2007) of enrich-ing the representation of a text by representingeach word by its textual definition in a knowl-edge base, i.e.
explicit semantic analysis (ESA).For that, we used as knowledge base the synset?stextual definitions provided by WordNet.
First,in order to determine the textual definition asso-ciated to each word, the texts were tagged using734the maximum entropy POS tagger included in theNLTK.
Next, the adapted Lesk algorithm (Baner-jee and Pedersen, 2002) for word sense disam-biguation was applied in the texts disambiguatingone word at the time.
The software package usedfor this disambiguation process was pywsd2.
Thearguments needed for the disambiguation of eachword are the POS tag of the target word and theentire sentence as context.
Once all the words aredisambiguated with their corresponding WordNetsynsets, each word is replaced by all the words intheir textual definition jointly with the same wordand its lemma.
The final result of this stage is thateach text in the data set is replaced by a longertext including the original text and some relatedwords.
The motivation of this procedure is that theextended versions of each pair of texts have morechance of sharing common words that the originaltexts.The extended versions of these texts were usedto obtain another 17 features with the same proce-dure described in the previous subsection (3.1.1).This feature subset will henceforth be referred asESA.3.1.3 Features for each part-of-speechcategoryThis set of features is motivated by the idea pro-posed by Corley and Mihalcea (2005) of group-ing words by their POS category before beingcompared for semantic textual similarity.
Our ap-proach consist in provide a version of each textpair in the data set for each POS category in-cluding only the words belonging to that cate-gory.
For instance, the pair of texts {?A beauti-ful girl is playing tennis?, ?A nice and handsomeboy is playing football?}
produce new pairs suchas: {?beautiful?, ?nice handsome?}
for the ADJtag, {?girl tennis?, ?boy football?}
for NOUN and{?is playing?, ?is playing?}
for VERB.Again, the POS tags were provided by theNLTK?s max entropy tagger.
The 28 POS cate-gories were simplified to 9 categories in order toavoid an excessive number of features and hencesparseness; the used mapping is shown in Table 3.Next, for each one of the 9 new POS categories aset of 17 features (SM) is extracted reusing againthe method proposed in subsection 3.1.1.
The onlydifference with the method described in that sub-section is that the stop-words were not removed2https://github.com/alvations/pywsdReduced tag set NLTK?s POS tag setADJ JJ,JJR,JJSNOUN NN,NNP,NNPS,NNSADV RB,RBR,RBS,WRBVERB VB,VBD,VBG,VBN,VBP,VBZPRO WP,WP$,PRP,PRP$PREP RP,INDET PDT,DT,WDTEX EXCC CCTable 3: Mapping reduction of the POS tag set.and the stemming process was not performed.
Themotivation for generating this feature sets by POScategory is that the machine learning algorithmscould weight differently each category.
The intu-ition behind this is that it is reasonable that cat-egories such as VERB and NOUN could play amore important role for the task at hand than oth-ers such as ADJ or PREP.
Using these categorizedfeatures, such discrimination among POS cate-gories can be discovered from the training data.Finally, the total number of features in this set is153 (17 features?
9 POS categories).
This featureset will be referred as POS.3.1.4 Features From DependenciesThe syntactic soft cardinality (Croce et al., 2012;Croce et al., 2013) extend the soft cardinalityapproach by representing texts as bags of de-pendencies instead of bags of words.
Each de-pendency is a 3-tuple composed of two syntac-tically related words and the type of their rela-tionship.
For instance, the sentence ?The boyplays football?
can be represented with 3 depen-dencies: [det,?boy?,?The?
], [subj,?plays?,?boy?
]and [obj,?plays?,?football?].
Clearly, this repre-sentation distinguish pairs of texts such as {?Thedog bites a boy?,?The boy bites a dog?
}, whichare indistinguishable when they are represented asbags of words.
This representation can be obtainedautomatically using the Stanford Parser (De Marn-effe et al., 2006), which in addition provides a de-pendency identifying the root word in a sentence.We used the version 3.3.13of that parser to obtainsuch representation.Once the texts are represented as bags of de-pendencies, it is necessary to provide a similar-ity function between two dependency tuples in or-3http://nlp.stanford.edu/software/lex-parser.shtml735der to use the soft cardinality (eq.
1) and henceto obtain the 17 cardinality features in Tables 1and 2.
Such function can be obtained using thesim function (eq.
2) for comparing the first andsecond words between the dependencies and eventhe labels of the dependency types.
Let?s considertwo dependencies tuples d = [ddep, dw1, dw2] andp = [pdep, pw1, pw2] where ddepand pdepare thelabels of the dependency type; dw1and pw1arethe first words on each dependency tuple; and dw2and pw2are the second words.
The similarity func-tion for comparing two dependency tuples can be alinear combination of the sim scores between thecorresponding elements of the dependency tuplesby the following expression:simdep(d, p) =?sim(ddep, pdep) + ?sim(dw1, pw2) + ?sim(dw2, pw2)Although, it is unusual to compare the depen-dencies?
type labels ddepand pdepwith a similar-ity function designed for words, we observed ex-perimentally that this approach yield better overallperformance in the relatedness task in comparisonwith a simple crisp comparison.
The optimal val-ues for the parameters ?
= ?3, ?
= 10 and ?
= 3were determined with the same methodology usedin subsection 3.1.1 for determining ?, ?
and bias.Clearly, the fact that ?
> ?
means that the firstwords in the dependency tuples plays a more im-portant role than the second ones for the task athand.
However, the fact that ?
< 0 is counter intu-itive because it means that the lower the similaritybetween the dependency type labels is, the largerthe similarity between the two dependencies.
Upto date we have been unable to find a plausible ex-planation for this phenomenon.
This set of 17 fea-tures will be referred hereinafter as DEP.3.1.5 Additional FeaturesIn addition to the feature sets based in soft car-dinality, we designed some features aimed to ad-dress linguistic phenomena such as antonymy, hy-pernymy and negation.Antonymy: Consider the following text pairfrom the test data {?A man is emptying a containermade of plastic?,?A man is filling a containermade of plastic?
}, which is labeled as a contra-diction with a relatedness score of 3.91.
Clearly,these labels are explained by the antonymy rela-tion between ?emptying?
and ?filling?.
Given thatnone of the features presented above address thisissue, a list of 11,028 pairs of antonym words wasgathered from several web sites (see Table 4) andfrom the antonymy relationships in WordNet, inorder to detect these cases.
That list was used tocount the number of occurrences of pairs antonymwords between pairs of texts and in each one ofthe texts.
Thus, for any pair of texts A and B (rep-resented as sets of words), three features (referredhenceforth as ANT) were extracted:antonym AB Counts the number of occurrencesof pairs of antonyms in A ?
B (Cartesianproduct) or in B ?A .antonym AA Counts the number of occurrencesof pairs of antonyms in A?A.antonym BB Counts the number of occurrencesof pairs of antonyms in B ?B.Hypernymy: Consider the following text pairfrom the test data {?A man is sitting comfortablyat a table?,?A person is sitting comfortably at thetable?
}, which is labeled as an entailment witha relatedness score of 3.96.
In this case, the en-tailment is based on the hypernymy between ?per-son?
and ?man?.
In order to capture this linguis-tic factor 3 features similar to the previously de-scribed antonym features were proposed.
First,word sense disambiguation was performed (as de-scribed in subsection 3.1.2) for obtaining a synsetlabel for each word.
Secondly, we build a bi-nary function hyp(ss1, ss2) that takes two Word-Net synsets as arguments and returns 1 if ss1isa hypernym of ss2with a maximum depth in theWordNet?s is-a hierarchy of 6 steps, and 0 oth-erwise.
This hypernymy function was build us-ing the WordNet interface provided by the NLTK.Next, based on that synset-to-synset function, atext-to-text function that captures the degree or hy-pernymy in a text or in a pair of texts was build us-ing the Monge-Elkan measure (Monge and Elkan,1996).
Thus, for two texts A and B representedas sets of synset labels, the following expressionmeasures their degree of hypernymy:HY P (A,B) =1|A||A|?i=1|B|maxj=1hyp(ai, bj)Using the function HY P (?, ?
), 3 features areextracted from each pair of text (referred hence-forth as HYP):hypernym AB from HY P (A,B)736http://www.myenglishpages.com/site php files/vocabulary-lesson-opposites-adjectives.phphttp://www.allaboutspace.com/wordlist/opposites.shtmlhttp://www.michigan-proficiency-exams.com/antonym-list.htmlhttp://examples.yourdictionary.com/examples-of-antonyms.htmlhttp://www.synonyms-antonyms.com/antonyms.htmlhttp://englishwilleasy.com/word-must-know/vocabulary/vocabulary-list-by-opposites-or-antonyms/http://www.meridianschools.org/staff/districtcurriculum/moreresources/languagearts/all grades/antonyms.dochttp://mrsbrower.weebly.com/uploads/1/3/2/4/13243672/antonymlist.pdfhttps://foxhugh.wordpress.com/word-lists/list-of-antonyms/http://www.paulnoll.com/Books/Clear-English/English-antonyms-1.htmlhttp://wordnet.princeton.edu/wordnet/download/Table 4: URLs used for the list of 11,028 antonym pairs (accessed on March 20, 2014).hypernym AA from HY P (A,A)hypernym BB from HY P (B,B)Negation: Negations play an important role inthe task at hand.
For instance, consider this pairof texts {?A person is rinsing a steak with wa-ter?,?A man is not rinsing a large steak?}
labeledas a contradiction.
In that example the negation ofthe verb ?rising?
is the main factor of contradic-tion.
In order to capture this linguistic feature webuild a simple function that detects the occurrenceof a verb negation if the text contains one of thefollowing words: ?not?, ?n?t?, ?nor?, ?null?, ?nei-ther?, ?either?, ?barely?, ?scarcely?
and ?hardly?.Similarly, noun negation is detected looking forthe words: ?no?, ?none?, ?nobody?, ?nowhere?,?nothing?
and ?never?.
Thus, for two texts A andB, 4 features are extracted (referred henceforth asNEG):verb neg A if verb negation is detected in Averb neg B if verb negation is detected in Bnoun neg A if noun negation is detected in Anoun neg B if noun negation is detected in B3.1.6 Submitted Runs and ResultsRUN1 (PRIMARY) This system produced pre-dictions by extracting all the features describedpreviously (SM, ESA, POS, DEP, ANT,HYP and NEG) from all the texts in the SICKdata set.
Next, two machine learning models wereobtained (WEKA (Hall et al., 2009) was usedfor that) using the training part of SICK, one forregression (relatedness) and another for classifi-cation (entailment).
The regression model wasa reduced-error pruning tree (REPtree) (Quin-lan, 1987) boosted with 20 iterations of bagging(Breiman, 1996).
The classification model was aJ48Graft tree also boosted with 20 bagging itera-tions.
These two models produced the predictionsfor the test part of SICK.RUN2 This system is similar to the one usedin RUN1, but it used only the feature sets SM andNEG.
Another difference is that a linear regres-sion was used instead of the REPtree and no bag-ging was performed.RUN3 The same as RUN1, but again, linearregression was used instead of the REPtree and nobagging was performed.RUN4 The same as RUN2, but the modelswere boosted with 20 iterations of bagging.RUN5 The same as RUN3, but 30 iterations ofbagging were used instead of 20.The official results obtained by these systems(prefixed UNAL-NLP) are shown in Table 5jointly with those obtained by other 3 top sys-tems among the 18 participating systems.
Ourprimary run (RUN1) obtained pretty competitiveresults ranking 3th and 4th in the entailment andrelatedness tasks.
The RUN4 obtained a remark-able performance (it would be ranked 6th for en-tailment and 8th for relatedness) in spite of thefact that is a system purely based on string match-ing.
The comparison of our runs 1, 3 and 5, whichmainly differs by the use of bagging, shows thatthis boosting method provides considerable im-provements.
In fact, comparing RUN3 (all fea-tures, no bagging) and RUN4 (SM and NEG fea-ture sets boosted with bagging), they performedsimilarly in spite of the considerable larger num-ber of features used in RUN3.
Besides, the RUN5slightly outperformed our primary run (RUN1) us-737Entailment Relatednesssystem accuracy official rank Pearson Spearman MSE official rankUNAL-NLP run1 (primary) 83.05% 3rd/18 0.8043 0.7458 0.3593 4th/17UNAL-NLP run2 79.81% - 0.7482 0.7033 0.4487 -UNAL-NLP run3 80.15% - 0.7747 0.7286 0.4081 -UNAL-NLP run4 80.21% - 0.7662 0.7142 0.4210 -UNAL-NLP run5 83.24% - 0.8070 0.7489 0.3550 -ECNU run1 83.64% 2nd/18 0.8280 0.7689 0.3250 1st/17Stanford run5 74.49% 12th/18 0.8272 0.7559 0.3230 2nd/17Illinois-LH run1 84.58% 1st/18 0.7993 0.7538 0.3692 5th/17Table 5: Results for task 1.ing 10 additional iterations of bagging.3.1.7 Error AnalysisOur primary run for the task 1 failed in 835 pairs ofsentences out of 4,927 in the entailment subtask.We wanted to understand in why our system failedin these 835 instances, so we classified manuallythese instances in 4 error categories (each instancecould be assigned to several categories).Paraphrase not detected (NP): exam-ple={?Two groups of people are playing football?,?Two teams are competing in a football match?
},gold standard=entailment, prediction=neutral,number of occurrences= 420 (50.3%).
The systemfailed to detect the paraphrase between ?groups ofpeople?
and ?teams?.Negation not detected (NN) : exam-ple={?There is no one playing the guitar?,?Someone is playing the guitar?
}, gold stan-dard=contradiction, prediction=neutral, numberof occurrences=94 (11.3%).
The system failed todetect that the contradiction is due to the negationin the first text.False similarity between words (NSS) : ex-ample={?Two dogs are playing by a tree?,?Two dogs are sleeping by a tree?
}, gold stan-dard=neutral, prediction=entailment, number ofoccurrences=413 (49.5%).
The only differencebetween these 2 sentences is the gerund ?playing?vs.
?sleeping?, which the system erroneously con-sidered as similar.Antonym not detected (NA): exam-ple={?Three children are running down hill?,?Three children are running up hill?
}, goldstandard=contradiction, prediction=entailment,number of occurrences=40 (4.8%).
The onlydifference between these 2 sentences is thewords ?down?
vs. ?up?.
In spite that this pairof antonyms was included in the antonym list,Error category NP NN NSS NANP 420 5 125 0NN - 94 1 0NSS - - 413 22NA - - - 40Table 6: Co-ocurrences of types of errors in RUN1(task1).the system failed to distinguish the contradictionbetween the texts.The matrix in Table 6 reports the number ofco-occurrences of error categories in the 835 in-stances erroneously classified.3.2 Task 3: Cross-level Semantic SimilarityThe SemEval 2014 task 3 (cross-level semanticsimilarity) (Jurgens et al., 2014) proposed the se-mantic textual similarity task but across differ-ent textual levels, namely paragraph-to-sentence,sentence-to-phrase, phrase-to-word and word-to-sense.
As usual, the goal is to predict the gold sim-ilarity scores for each pair of texts.
For each oneof these cross-level comparison types there wereproposed a separated training and test data sets.Basically, we addressed this task using the set offeatures SM presented in subsection 3.1.1 in com-bination with a text expansion approach similar tothe method presented in subsection 3.1.2.3.2.1 Paragraph-to-sentence andSentence-to-phraseFor these two cross-level comparison types weextracted the SM feature set using the pro-vided texts.
The model parameters obtained forparagraph-to-sentence were ?
= 0.1, ?
= 1.75,bias = ?1.35, p = 1.55; and for sentence-to-phrase were ?
= 0.68, ?
= 0.92, bias = ?0.92,p = 2.49.738The system for the RUN2 used the SM fea-ture set and a machine learning model build withthe provided training data for generating the simi-larity score predictions for the test data.
For theparagraph-to-sentence data set the model was aREPtree for regression boosted with 40 bagging it-erations.
Similarly, the model for the sentence-to-phrase data set was a linear regressor also boostedwith 40 bagging iterations.Unlike RUN2, RUN1 does not make use of anymachine learning algorithm.
Instead, we used theonly the basic cardinalities (see Table 1) from theSM feature set in combination with an ad-hoc re-semblance coefficient, i.e.
the Dice?s coefficient2|A?B|/|A|+|B| for the paragraph-to-sentence dataset.
In turn, for sentence-to-phrase the overlap co-efficient, i.e.|A?B|/min[|A|,|B|], was used.3.2.2 Phrase-to-word and Word-to-senseBefore applying the same procedure used in theprevious subsection, the texts in the phrase-to-word and word-to-sense data sets were expandedwith a similar approach to that was used in subsec-tion 3.1.2.Phrase-to-word expansion: First, the ?word?was expanded finding its corresponding WordNetsynset using the adapted Lesk?s algorithm provid-ing as context the ?phrase?.
Then, once the word?ssynset is obtained, the ?word?
text is extendedwith the textual definition of the synset.
Simi-larity, this procedure is repeated for each word inthe ?phase?
obtaining and extended version of thephrase.
Finally, these two texts are used for ex-tracting the SM feature set.
The model param-eters were ?
= 0.8, ?
= 1.9, bias = ?0.8,p = 1.5.Word-to-sense expansion: First, the ?sense?(i.e.
synset) is replaced by its textual definitionand its lemma.
At this point the pair word-sensebecomes a pair word-sentence.
Then, the synsetof the ?word?
is obtained performing the adaptedLesk?s algorithm.
Next, the ?word?
is extendedwith textual definition of the synset.
Finally, thesetwo texts are used for extracting the SM featureset obtaining the following model parameters were?
= 0.59, ?
= 0.9, bias = ?0.89, p = 3.91.3.2.3 ResultsThe official results obtained by the two submittedruns jointly with other 3 top systems are shown inTable 7.
Our submissions (prefixed with UNAL-NLP) ranked 3rd and 5th among 38 participatingtest data train dataOnWN (en) OnWN 2012/2013 testheadlines (en) headlines 2013 testimages (en) MSRvid 2012 train and testdeft-news (en) MSRpar 2013 train and testdeft-forum (en)MSRvid 2012 train and testOnWN 2012/2013 testtweet-news (en)SMTeuroparl 2012 testSMTnews 2012 testWikipedia (es) SMTeuroparl 2012 trainnews (es) SMTeuroparl 2012 trainTable 8: Training data used for the STS-2014 datasets (task 10).systems, showing that the SM (string-matching)feature set is effective for the prediction of sim-ilarity scores.
Particularly, in the paragraph-to-sentence data set, which has the longest text,RUN2 obtained the best official score.
In contrast,the scores obtained for the phrase-to-word andword-to-sense data sets were considerably lowerin comparison with the top system, but still com-petitive against most of the other participating sys-tems.3.3 Task 10: Multilingual SemanticSimilarityThe SemEval-2014 task 10 (multilingual seman-tic similarity) (Agirre et al., 2014) is the sequel ofthe semantic textual similarity (STS) evaluationsat SemEval in the past two years (Agirre et al.,2012; Agirre et al., 2013).
This year 6 test datasets were proposed in English and 2 data sets inSpanish.
Similarly to the 2013 campaign, there isnot explicit training data for each data set.
Conse-quently, different data sets from the previous STSevaluations were selected to be used as trainingdata for the new data sets.
The selection criterionwas the average character length and type of thetexts.
The Table 8 shows the training data used foreach test data set.3.3.1 English SubtaskThe RUN1 for the English data sets was producedwith a parameterized similarity function based onthe SM feature set and the symmetrized Tversky?sindex (Tversky, 1977; Jimenez et al., 2013a).
Fora detailed description of this function and its pa-rameters, please refer to the STSsimfeature inthe system description paper of the NTNU team(Lynum et al., 2014).
The parameters used in that739System Para-2-Sent Sent-2-Phr Phr-2-Word Word-2-Sense Official RankSimCompass run1 0.811 0.742 0.415 0.356 1st/38ECNU run1 0.834 0.771 0.315 0.269 2nd/38UNAL-NLP run2 0.837 0.738 0.274 0.256 3rd/38SemantiKLUE run1 0.817 0.754 0.215 0.314 4th/38UNAL-NLP run1 0.817 0.739 0.252 0.249 5th/38Table 7: Official results for task 3 (Pearson?s correlation).Data ?
?
bias p ???
?bias?OnWN 0.53 -0.53 1.01 1.00 -4.89 0.52 0.46headlines 0.36 -0.29 4.17 0.85 -4.50 0.43 0.19images 1.12 -1.11 0.93 0.64 -0.98 0.50 0.11deft-news 3.36 -0.64 1.37 0.44 2.36 0.72 0.02deft-forum 1.01 -1.01 0.24 0.93 -2.71 0.42 1.63tweet-news 0.13 0.14 2.80 0.01 2.66 1.74 0.45Table 9: Optimal parameters used for task 10 inEnglish.function are reported in Table 9.
Unlike subsec-tion 3.1.1 where the Dice?s coefficient was used asthe text similarity function, here the symmetricalTversky?s index (eq.
2) was reused generating thethree additional parameters marked with apostro-phe (?
?, ?
?and bias?
).For the RUN2 the SM feature set was extractedfrom all the data sets in English (en) listed in Table8.
Then, a REPtree (Quinlan, 1987) boosted with50 bagging iterations (Breiman, 1996) was trainedusing the training data sets selected for each testdata set.
Finally, these machine learning modelsproduced the similarity score predictions for eachtest data set.The RUN3 was identical to the RUN2 but in-cluded additional feature sets apart from SM,namely: ESA, POS and WN.
The WN featureset is the same as SM, but replacing the word-to-word similarity function in eq.
2 by the path mea-sure from the WordNet::Similarity package (Ped-ersen et al., 2004).3.3.2 Spanish SubtaskThe Spanish system was based entirely in the SMfeature set with some small changes for adapt-ing the system to Spanish.
Basically, the list ofEnglish stop-words was replaced by the Spanishstop-words provided by the NLTK.
In addition,the Porter stemmer was replaced by its Spanishequivalent, i.e.
the Snowball stemmer for Span-ish.
The RUN1 is equivalent to the RUN1 for thedata set run1 run2 run3deft-forum 0.5043 0.3826 0.4607deft-news 0.7205 0.7305 0.7216headlines 0.7616 0.7645 0.7605images 0.8071 0.7706 0.7782OnWN 0.7823 0.8268 0.8426tweet-news 0.6145 0.4028 0.6583mean (en) 0.7113 0.6573 0.7209official rank (en) 12th/38 22th/38 9th/38Wikipedia 0.7804 0.7566 0.6894news 0.8154 0.7829 0.7965mean (es) 0.8013 0.7723 0.7533official rank (es) 3rd/22 9th/22 12th/22Table 10: Official results for the task 10 (Pearson?scorrelation).English subtask described in the previous subsec-tion.
The parameters used for the text similarityfunction were ?
= 1.16, ?
= 1.08, bias = 0.02,p = 1.02, ?
?= 1.54, ?
?= 0.08 and bias?= 1.37.The description and meaning of these parameterscan be found in (Lynum et al., 2014) associated tothe STSsimfeature.The RUN2 was obtained using the SM featureset and a linear regressor for generating the simi-larity score predictions.
Similarity, RUN3 used thesame feature set SM in combination with a REP-tree boosted with 30 bagging iterations.3.3.3 ResultsThe results for the 3 submitted runs correspond-ing to the 2 sub tasks (English and Spanish) areshown in Table 10.
It is important to note thatthe RUN1 for the Wikipedia data set in Spanishwas the top system among 22 participating sys-tems.
This result is remarkable given that this sys-tem was trained with a data set in English showingthe domain adaptation ability of the soft cardinal-ity approach.7404 ConclusionsWe participated in the SemEval-2014 task 1, 3 and10 with an uniform approach based on soft cardi-nality features, obtaining pretty satisfactory resultsin all data sets, tasks and sub tasks.
This approachhas been used since SemEval-2012 in all versionsof the following tasks: semantic textual similar-ity (Jimenez et al., 2012b; Jimenez et al., 2013a),typed similarity (Croce et al., 2013), cross-lingualtextual entailment (Jimenez et al., 2012a; Jimenezet al., 2013c), student response analysis (Jimenezet al., 2013b), and multilingual semantic textualsimilarity (Lynum et al., 2014).
In the majorityof the cases, the systems based on soft cardinality,built by us and other teams, have been among thetop systems.
Given the uniformity of the approach,the consistency of the results, the few computa-tional resources required and the overall concep-tual simplicity, the soft cardinality is establishedas a useful tool for a wide spectrum of applicationsin natural language processing.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-Agirre Aitor.
2012.
SemEval-2012 task 6: A piloton semantic textual similarity.
In Proceedings of the6th International Workshop on Semantic Evaluation(SemEval@*SEM 2012), Montreal,Canada.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 sharedtask: Semantic textual similarity, including a piloton typed-similarity.
Atlanta, Georgia, USA.Eneko Agirre, Carmen Banea, Claire Cardie, DanielCer, Mona Diab, Aitor Gonzalez-Agirre, WeiweiGuo, Rada Mihalcea, German Rigau, and JanyceWeibe.
2014.
SemEval-2014 task 10: Multilingualsemantic textual similarity.
In Proceedings of the8th International Workshop on Semantic Evaluation(SemEval-2014), Dublin, Ireland, August.Satanjeev Banerjee and Ted Pedersen.
2002.
Anadapted lesk algorithm for word sense disambigua-tion using WordNet.
In Computational linguis-tics and intelligent text processing, page 136?145.Springer.Leo Breiman.
1996.
Bagging predictors.
MachineLearning, 24(2):123?140.Courtney Corley and Rada Mihalcea.
2005.
Measur-ing the semantic similarity of texts.
In Proceedingsof the ACL Workshop on Empirical Modeling of Se-mantic Equivalence and Entailment, EMSEE ?05,page 13?18, Stroudsburg, PA, USA.Danilo Croce, Valerio Storch, P. Annesi, and RobertoBasili.
2012.
Distributional compositional seman-tics and text similarity.
In 2012 IEEE Sixth Interna-tional Conference on Semantic Computing (ICSC),pages 242?249, September.Danilo Croce, Valerio Storch, and Roberto Basili.2013.
UNITOR-CORE TYPED: Combining textsimilarity and semantic filters through SV regres-sion.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 1: Pro-ceedings of the Main Conference and the SharedTask: SemanticTextual Similarity, page 59, Atlanta,Georgia, USA.Marie-Catherine De Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of LREC, volume 6, page 449?454,Genoa, Italy, May.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In Proceedings ofthe 20th International Joint Conference on ArtificalIntelligence, IJCAI?07, page 1606?1611, San Fran-cisco, CA, USA.
Morgan Kaufmann Publishers Inc.Mark Hall, Frank Eibe, Geoffrey Holmes, and Bern-hard Pfahringer.
2009.
The WEKA data min-ing software: An update.
SIGKDD Explorations,11(1):10?18.Paul Jaccard.
1901.
Etude comparative de la distribu-tion florare dans une portion des alpes et des jura.Bulletin de la Soci?et?e Vaudoise des Sciences Na-turelles, pages 547?579.Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-bukh.
2010.
Text comparison using soft cardi-nality.
In Edgar Chavez and Stefano Lonardi, ed-itors, String Processing and Information Retrieval,volume 6393 of LNCS, pages 297?302.
Springer,Berlin, Heidelberg.Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2012a.
Soft cardinality: A parameterizedsimilarity function for text comparison.
In SemEval2012, Montreal, Canada.Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2012b.
Soft cardinality+ ML: Learning adap-tive similarity functions for cross-lingual textual en-tailment.
In SemEval 2012, Montreal, Canada.Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2013a.
SOFTCARDINALITY-CORE: Im-proving text overlap with distributional measures forsemantic textual similarity.
In *SEM 2013, Atlanta,Georgia, USA, June.Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2013b.
SOFTCARDINALITY: Hierarchicaltext overlap for student response analysis.
In Se-mEval 2013, Atlanta, Georgia, USA, June.741Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2013c.
SOFTCARDINALITY: Learningto identify directional cross-lingual entailment fromcardinalities and SMT.
In SemEval 2013, Atlanta,Georgia, USA, June.Karen Sp?arck Jones.
2004.
A statistical interpretationof term specificity and its application in retrieval.Journal of Documentation, 60(5):493?502, October.David Jurgens, Mohammad T. Pilehvar, and RobertoNavigli.
2014.
SemEval-2014 task 3: Cross-level semantic similarity.
In Proceedings of the8th International Workshop on Semantic Evaluation(SemEval-2014), Dublin, Ireland, August.Andr?e Lynum, Partha Pakray, Bj?orn Gamb?ack, andSergio Jimenez.
2014.
NTNU: Measuring se-mantic similarity with sublexical feature represen-tations and soft cardinalty.
In Proceedings of the8th International Workshop on Semantic Evaluation(SemEval-2014), Dublin, Ireland, August.Marco Marelli, Stefano Menini, Marco Baroni, LuciaBentivogli, Raffaella Bernardi, and Roberto Zam-parelli.
2014.
A SICK cure for the evaluation ofcompositional distributional semantic models.
InProceedings of LREC, Reykjavik, Iceland, May.Alvaro E. Monge and Charles Elkan.
1996.
The fieldmatching problem: Algorithms and applications.
InProceeding of the 2nd International Conference onKnowledge Discovery and Data Mining (KDD-96),pages 267?270, Portland, OR.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
WordNet::similarity: measuring therelatedness of concepts.
In Proceedings HLT-NAACL?Demonstration Papers, Stroudsburg, PA,USA.Martin Porter.
1980.
An algorithm for suffix stripping.Program, 3(14):130?137, October.J.
Ross Quinlan.
1987.
Simplifying decisiontrees.
International journal of man-machine studies,27(3):221?234.Amos Tversky.
1977.
Features of similarity.
Psycho-logical Review, 84(4):327?352, July.742
