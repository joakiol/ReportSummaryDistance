Towards a Model of Face-to-Face GroundingYukiko I.
Nakano?/??
Gabe Reinstein?
Tom Stocky?
Justine Cassell?
?MIT Media LaboratoryE15-31520 Ames StreetCambridge, MA 02139 USA{yukiko, gabe, tstocky, justine}@media.mit.edu?
?Research Institute of Science andTechnology for Society (RISTEX)2-5-1 Atago Minato-ku,Tokyo 105-6218, Japannakano@kc.t.u-tokyo.ac.jpAbstractWe investigate the verbal and nonverbalmeans for grounding, and propose a designfor embodied conversational agents that re-lies on both kinds of signals to establishcommon ground in human-computer inter-action.
We analyzed eye gaze, head nodsand attentional focus in the context of a di-rection-giving task.
The distribution ofnonverbal behaviors differed depending onthe type of dialogue move being grounded,and the overall pattern reflected a monitor-ing of lack of negative feedback.
Based onthese results, we present an ECA that usesverbal and nonverbal grounding acts to up-date dialogue state.1 IntroductionAn essential part of conversation is to ensure thatthe other participants share an understanding ofwhat has been said, and what is meant.
The proc-ess of ensuring that understanding ?
adding whathas been said to the common ground ?
is calledgrounding [1].
In face-to-face interaction, nonver-bal signals as well as verbal participate in thegrounding process, to indicate that an utterance isgrounded, or that further work is needed to ground.Figure 1 shows an example of human face-to-faceconversation.
Even though no verbal feedback isprovided, the speaker (S) continues to add to thedirections.
Intriguingly, the listener gives no ex-plicit nonverbal feedback ?
no nods or gaze to-wards S. S, however, is clearly monitoring thelistener?s behavior, as we see by the fact that Slooks at her twice (continuous lines above thewords).
In fact, our analyses show that maintainingfocus of attention on the task (dash-dot lines un-derneath the words) is the listener?s public signalof understanding S?s utterance sufficiently for thetask at hand.
Because S is manifestly attending tothis signal, the signal allows the two jointly to rec-ognize S?s contribution as grounded.
This paperprovides empirical support for an essential role fornonverbal behaviors in grounding, motivating anarchitecture for an embodied conversational agentthat can establish common ground using eye gaze,head nods, and attentional focus.Although grounding has received significant at-tention in the literature, previous work has not ad-dressed the following questions: (1) whatpredictive factors account for how people use non-verbal signals to ground information, (2) how can amodel of the face-to-face grounding process beused to adapt dialogue management to face-to-faceconversation with an embodied conversationalagent.
This paper addresses these issues, with thegoal of contributing to the literature on discoursephenomena, and of building more advanced con-versational humanoids that can engage in humanconversational protocols.In the next section, we discuss relevant previouswork, report results from our own empirical studyand, based on our analysis of conversational data,propose a model of grounding using both verbaland nonverbal information, and present our im-plementation of that model into an embodied con-versational agent.
As a preliminary evaluation, wecompare a user interacting with the embodied con-versational agent with and without grounding.Figure 1: Human face-to-face conversation[580] S: Go    to    the    fourth    floor,[590] S: hang    a    left,[600] S: hang    another    left.look at map gaze at listenergaze at listenerlook at maplook at maplook at maplook at mapspeaker?s behaviorlistener?s behavior2 Related WorkConversation can be seen as a collaborative activ-ity to accomplish information-sharing and to pur-sue joint goals and tasks.
Under this view,agreeing on what has been said, and what is meant,is crucial to conversation.
The part of what hasbeen said that the interlocutors understand to bemutually shared is called the common ground, andthe process of establishing parts of the conversa-tion as shared is called grounding [1].
As [2] pointout, participants in a conversation attempt tominimize the effort expended in grounding.
Thus,interlocutors do not always convey all the informa-tion at their disposal; sometimes it takes less effortto produce an incomplete utterance that can be re-paired if needs be.
[3] has proposed a computational approach togrounding where the status of contributions asprovisional or shared is part of the dialoguesystem?s representation of the ?information state?of the conversation.
Conversational actions cantrigger updates that register provisionalinformation as shared.
These actions achievegrounding.
Acknowledgment acts are directly as-sociated with grounding updates while other utter-ances effect grounding updates indirectly, becausethey proceed with the task in a way that presup-poses that prior utterances are uncontroversial.
[4], on the other hand, suggest that actions inconversation give probabilistic evidence of under-standing, which is represented on a par with otheruncertainties in the dialogue system (e.g., speechrecognizer unreliability).
The dialogue managerassumes that content is grounded as long as itjudges the risk of misunderstanding as acceptable.
[1, 5] mention that eye gaze is the most basicform of positive evidence that the addressee is at-tending to the speaker, and that head nods have asimilar function to verbal acknowledgements.
Theysuggest that nonverbal behaviors mainly contributeto lower levels of grounding, to signify that inter-locutors have access to each other?s communica-tive actions, and are attending.
With a similar goalof broadening the notion of communicative actionbeyond the spoken word, [6] examine other kindsof multimodal grounding behaviors, such as post-ing information on a whiteboard.
Although theseand other researchers have suggested that nonver-bal behaviors undoubtedly play a role in grounding,previous literature does not characterize their pre-cise role with respect to dialogue state.On the other hand, a number of studies on theseparticular nonverbal behaviors do exist.
An earlystudy, [7], reported that conversation involves eyegaze about 60% of the time.
Speakers look up atgrammatical pauses for feedback on how utter-ances are being received, and also look at the task.Listeners look at speakers to follow their directionof gaze.
In fact, [8] claimed speakers will pauseand restart until they obtain the listener?s gaze.
[9]found that during conversational difficulties, mu-tual gaze was held longer at turn boundaries.Previous work on embodied conversationalagents (ECAs) has demonstrated that it is possibleto implement face-to-face conversational protocolsin human-computer interaction, and that correctrelationships among verbal and nonverbal signalsenhances the naturalness and effectiveness of em-bodied dialogue systems [10], [11].
[12] reportedthat users felt the agent to be more helpful, lifelike,and smooth in its interaction style when it demon-strated nonverbal conversational behaviors.3 Empirical StudyIn order to get an empirical basis for modelingface-to-face grounding, and implementing an ECA,we analyzed conversational data in two conditions.3.1 Experiment DesignBased on previous direction-giving tasks, studentsfrom two different universities gave directions tocampus locations to one another.
Each pair had aconversation in a (1) Face-to-face condition(F2F): where two subjects sat with a map drawnby the direction-giver sitting between them, and ina (2) Shared Reference condition (SR): where anL-shaped screen between the subjects let themshare a map drawn by the direction-giver, but notto see the other?s face or body.Interactions between the subjects were video-recorded from four different angles, and combinedby a video mixer into synchronized video clips.3.2 Data Coding10 experiment sessions resulted in 10 dialogues percondition (20 in total), transcribed as follows.Coding verbal behaviors: As grounding oc-curs within a turn, which consists of consecutiveutterances by a speaker, following [13] we token-ized a turn into utterance units (UU), correspond-ing to a single intonational phrase [14].
Each UUwas categorized using the DAMSL coding scheme[15].
In the statistical analysis, we concentrated onthe following four categories with regular occur-rence in our data: Acknowledgement, Answer, In-formation request (Info-req), and Assertion.Coding nonverbal behaviors: Based on previ-ous studies, four types of behaviors were coded:Gaze At Partner (gP): Looking at the partner?seyes, eye region, or face.Gaze At Map (gM): Looking at the mapGaze Elsewhere (gE): Looking away elsewhereHead nod (Nod): Head moves up and down in asingle continuous movement on a vertical axis,but eyes do not go above the horizontal axis.By combining Gaze and Nod, six complex catego-ries (ex.
gP with nod, gP without nod, etc) are gen-erated.
In what follows, however, we analyze onlycategories with more than 10 instances.
In order toanalyze dyadic behavior, 16 combinations of thenonverbal behaviors are defined, as shown in Table1.
Thus, gP/gM stands for a combination ofspeaker gaze at partner and listener gaze at map.ResultsWe examine differences between the F2F and SRconditions, correlate verbal and nonverbal behav-iors within those conditions, and finally look atcorrelations between speaker and listener behavior.Basic Statistics: The analyzed corpus consistsof 1088 UUs for F2F, and 1145 UUs for SR. Themean length of conversations in F2F is 3.24 min-utes, and in SR is 3.78 minutes  (t(7)=-1.667 p<.07(one-tail)).
The mean length of utterances in F2F(5.26 words per UU) is significantly longer than inSR (4.43 words per UU) (t(7)=3.389 p< .01 (one-tail)).
For the nonverbal behaviors, the number ofshifts between the statuses in Table 1 was com-pared (eg.
NV status shifts from gP/gP to gM/gMis counted as one shift).
There were 887 NV statusshifts for F2F, and 425 shifts for SR.
The numberof NV status shifts in SR is less than half of that inF2F (t(7)=3.377 p< .01 (one-tail)).These results indicate that visual access to theinterlocutor?s body affects the conversation, sug-gesting that these nonverbal behaviors are used ascommunicative signals.
In SR, where the meanlength of UU is shorter, speakers present informa-tion in smaller chunks than in F2F, leading to morechunks and a slightly longer conversation.
In F2F,on the other hand, conversational participants con-vey more information in each UU.Correlation between verbal and nonverbalbehaviors: We analyzed NV status shifts with re-spect to the type of verbal communicative actionand the experimental condition (F2F/SR).
To lookat the continuity of NV status, we also analyzed theamount of time spent in each NV status.
For gaze,transition and time spent gave similar results; sincehead nods are so brief, however, we discuss thedata in terms of transitions.
Table 2 shows the mostfrequent target NV status (shift to these statuses fromothers) for each speech act type in F2F.
Numbers inparentheses indicates the proportion to the total num-ber of transitions.<Acknowledgement> Within an UU, thedyad?s NV status most frequently shifts togMwN/gM (eg.
speaker utters ?OK?
while nodding,and listener looks at the map).
At pauses, a shift togMgM is most frequent.
The same results werefound in SR where the listener could not see thespeaker?s nod.
These findings suggest that Ac-knowledgement is likely to be accompanied by ahead nod, and this behavior may function intro-spectively, as well as communicatively.<Answer> In F2F, the most frequent shiftwithin a UU is to gP/gP.
This suggests that speak-ers and listeners rely on mutual gaze (gP/gP) toensure an answer is grounded, whereas they cannotuse this strategy in SR.
In addition, we found thatTable 1: NV statusesListener?s behavior Combinations ofNVs gP gM gMwN gEgP gP/gP gP/gM gP/gMwN gP/gEgM gM/gP gM/gM gM/gMwN gM/gEgMwN gMwN/gP gMwN/gM gMwN/gMwN gMwN/gESpeaker?sbehaviorgE gE/gP gE/gM gE/gMwN gE/gEShift towithin UU pauseAcknowledgement gMwN/gM (0.495) gM/gM (0.888)Answer gP/gP (0.436) gM/gM (0.667)Info-req gP/gM (0.38) gP/gP (0.5)Assertion gP/gM (0.317) gM/gM (0.418)Table 2: Salient transitionsspeakers frequently look away at the beginning ofan answer, as they plan their reply [7].<Info-req> In F2F, the most frequent shiftwithin a UU is to gP/gM, while at pauses betweenUUs shift to gP/gP is the most frequent.
This sug-gests that speakers obtain mutual gaze after askinga question to ensure that the question is clear, be-fore the turn is transferred to the listener to reply.In SR, however, rarely is there any NV status shift,and participants continue looking at the map.<Assertion> In both conditions, listeners lookat the map most of the time, and sometimes nod.However, speakers?
nonverbal behavior is verydifferent across conditions.
In SR, speakers eitherlook at the map or elsewhere.
By contrast, in F2F,they frequently look at the listener, so that a shiftto gP/gM is the most frequent within an UU.
Thissuggests that, in F2F, speakers check whether thelistener is paying attention to the referent men-tioned in the Assertion.
This implies that not onlylistener?s gazing at the speaker, but also payingattention to a referent works as positive evidenceof understanding in F2F.In summary, it is already known that eye gazecan signal a turn-taking request [16], but turn-taking cannot account for all our results.
Gaze di-rection changes within as well as between UUs,and the usage of these nonverbal behaviors differsdepending on the type of conversational action.Note that subjects rarely demonstrated communica-tion failures, implying that these nonverbal behaviorsrepresent positive evidence of grounding.Correlation between speaker and listenerbehavior: Thus far we have demonstrated a differ-ence in distribution among nonverbal behaviors,with respect to conversational action, and visibilityof interlocutor.
But, to uncover the function ofthese nonverbal signals, we must examine howlistener?s nonverbal behavior affects the speaker?sfollowing action.
Thus, we looked at two consecu-tive Assertion UUs by a direction-giver, and ana-lyzed the relationship between the NV status of thefirst UU and the direction-giving strategy in thesecond UU.
The giver?s second UU is classified asgo-ahead if it gives the next leg of the directions,or as elaboration if it gives additional informationabout the first UU, as in the following example:[U1]S: And then, you?ll godown this little corridor.
[U2]S: It?s not very long.Results are shown in Figure 2.
When the listenerbegins to gaze at the speaker somewhere within anUU, and maintains gaze until the pause after theUU, the speaker?s next UU is an elaboration of theprevious UU 73% of the time.
On the other hand,when the listener keeps looking at the map duringan UU, only 30% of the next UU is an elaboration(z = 3.678, p<.01).
Moreover, when a listenerkeeps looking at the speaker, the speaker?s nextUU is go-ahead only 27% of the time.
In contrast,when a listener keeps looking at the map, thespeaker?s next UU is go-ahead 52% of the time (z= -2.049, p<.05)1.
These results suggest that speak-ers interpret listeners?
continuous gaze as evidenceof not-understanding, and they therefore add moreinformation about the previous UU.
Similar find-ings were reported for a map task by [17] whosuggested that, at times of communicative diffi-culty, interlocutors are more likely to utilize all thechannels available to them.
In terms of floor man-agement, gazing at the partner is a signal of givingup a turn, and here this indicates that listeners aretrying to elicit more information from the speaker.In addition, listeners?
continuous attention to themap is interpreted as evidence of understanding,and speakers go ahead to the next leg of the direc-tion2.3.3 A Model of Face-to-Face GroundingAnalyzing spoken dialogues, [18] reported thatgrounding behavior is more likely to occur at an1 The percentage for map does not sum to 100% because someof the UUs are cue phrases or tag questions which are part ofthe next leg of the direction, but do not convey content.2 We also analyzed two consecutive Answer UUs from a giver,and found that when the listener looks at the speaker at apause, the speaker elaborates the Answer 78% of the time.When the listener looks at the speaker during the UU and atthe map after the UU (positive evidence), the speaker  elabo-rates only 17% of the time.00.10.20.30.40.50.60.70.8gaze mapelaborationgo-aheadFigure 2: Relationship between receiver?s NV andgiver?s next verbal behaviorintonational boundary, which we use to identifyUUs.
This implies that multiple grounding behav-iors can occur within a turn if it consists of multi-ple UUs.
However, in previous models,information is grounded only when a listener re-turns verbal feedback, and acknowledgementmarks the smallest scope of grounding.
If we ap-ply this model to the example in Figure 1, none ofthe UU have been grounded because the listenerhas not returned any spoken grounding clues.In contrast, our results suggest that consideringthe role of nonverbal behavior, especially eye-gaze,allows a more fine-grained model of grounding,employing the UU as a unit of grounding.Our results also suggest that speakers are ac-tively monitoring positive evidence of understand-ing, and also the absence of negative evidence ofunderstanding (that is, signs of miscommunication).When listeners continue to gaze at the task, speak-ers continue on to the next leg of directions.Because of the incremental nature of grounding,we implement nonverbal grounding functionalityinto an embodied conversational agent using aprocess model that describes steps for a system tojudge whether a user understands system contribu-tion: (1) Preparing for the next UU: according tothe speech act type of the next UU, nonverbal posi-tive or negative evidence that the agent expects toreceive are specified.
(2) Monitoring: monitors andchecks the user?s nonverbal status and signals dur-ing the UU.
After speaking, the agent continuesmonitoring until s/he gets enough evidence of un-derstanding or not-understanding represented byuser?s nonverbal status and signals.
(3) Judging:once the agent gets enough evidence, s/he tries tojudge groundedness as soon as possible.
Accordingto some previous studies, length of pause betweenUUs is in between 0.4 to 1 sec [18, 19].
Thus, timeout for judgment is 1 sec after the end of the UU.
Ifthe agent does not have evidence then, the UU re-mains ungrounded.This model is based on the information stateapproach [3], with update rules that revise the stateof the conversation based on the inputs the systemreceives.
In our case, however, the inputs are sam-pled continuously, include the nonverbal state, andonly some require updates.
Other inputs indicatethat the last utterance is still pending, and allow theagent to wait further.
In particular, task attentionover an interval following the utterance triggersgrounding.
Gaze in the interval means that thecontribution stays provisional, and triggers an ob-ligation to elaborate.
Likewise, if the systemtimes-out without recognizing any user feedback,the segment remains ungrounded.
This processallows the system to keep talking across multipleutterance units without getting verbal feedbackfrom the user.
From the user?s perspective, explicitacknowledgement is not necessary, and minimalcost is involved in eliciting elaboration.4 Face-to-face Grounding with ECAsBased on our empirical results, we propose a dia-logue manager that can handle nonverbal input tothe grounding process, and we implement themechanism in an embodied conversational agent.4.1 SystemMACK is an interactive public information ECAkiosk.
His current knowledgebase concerns theactivities of the MIT Media Lab; he can answerquestions about the lab?s research groups, projects,and demos, and give directions to each.On the input side, MACK recognizes three mo-dalities: (1) speech, using IBM?s ViaVoice, (2) pengesture via a paper map atop a table with an em-bedded Wacom tablet, and (3) head nod and eyegaze via a stereo-camera-based 6-degree-of-freedom head-pose tracker (based on [20]).
Theseinputs operate as parallel threads, allowing the Un-derstanding Module (UM) to interpret the multiplemodalities both individually and in combination.MACK produces multimodal output as well: (1)speech synthesis using the Microsoft WhistlerText-to-Speech (TTS) API, (2) a graphical figurewith synchronized hand and arm gestures, andhead and eye movements, and (3) LCD projectorhighlighting on the paper map, allowing MACK toreference it.The system architecture is shown in Figure 3.The UM interprets the input modalities and con-verts them to dialogue moves which it then passeson to the Dialogue Manager (DM).
The DM con-sists of two primary sub-modules, the ResponsePlanner, which determines MACK?s next action(s)and creates a sequence of utterance units, and theGrounding Module (GrM), which updates the Dis-course Model and decides when the ResponsePlanner?s next UU should be passed on to the Gen-eration module (GM).
The GM converts the UUinto speech, gesture, and projector output, sendingthese synchronized modalities to the TTS engine,Animation Module (AM), and Projector Module.The Discourse Model maintains informationabout the state and history of the discourse.
Thisincludes a list of grounded beliefs and ungroundedUUs; a history of previous UUs with timing infor-mation; a history of nonverbal information (di-vided into gaze states and head nods) organized bytimestamp; and information about the state of thedialogue, such as the current UU under considera-tion, and when it started and ended.4.2 Nonverbal InputsEye gaze and head nod inputs are recognized by ahead tracker, which calculates rotations and trans-lations in three dimensions based on visual anddepth information taken from two cameras [20].The calculated head pose is translated into ?look atMACK,?
?look at map,?
or ?look elsewhere.?
Therotation of the head is translated into head nods,using a modified version of [21].
Head nod andeye gaze events are timestamped and logged withinthe nonverbal component of the Discourse History.The Grounding Module can thus look up the ap-propriate nonverbal information to judge a UU.4.3 The Dialogue ManagerIn a kiosk ECA, the system needs to ensure that theuser understands the information provided by theagent.
For this reason, we concentrated on imple-menting a grounding mechanism for Assertion,when the agent gives the user directions, and Answer, when the agent answers the user?s questionsGenerating the ResponseThe first job of the DM is to plan the response to auser?s query.
When a user asks for directions, theDM receives an event from the UM stating thisintention.
The Response Planner in the DM, rec-ognizing the user?s direction-request, calculates thedirections, broken up into segments.
These seg-ments are added to the DM?s Agenda, the stack ofUUs to be processed.At this point, the GrM sends the first UU (a di-rection segment) on the Agenda to the GM to beprocessed.
The GM converts the UU into speechand animation commands.
For MACK?s own non-verbal grounding acts, the GM determinesMACK?s gaze behavior according to the type ofUU.
For example, when MACK generates a direc-tion segment (an Assertion), 66% of the time hekeeps looking at the map.
When elaborating aprevious UU, 47% of the time he gazes at the user.When the GM begins to process the UU, it logsthe start time in the Discourse Model, and when itfinishes processing (as it sends the final commandto the animation module), it logs the end time.
TheGrM waits for this speech and animation to end(by polling the Discourse Model until the end timeis available), at which point it retrieves the timingdata for the UU, in the form of timestamps for theUU start and finish.
This timing data is used tolook up the nonverbal behavior co-occurring withthe utterance in order to judge whether or not theUU was grounded.Judgment of groundingWhen MACK finishes uttering a UU, the Ground-ing Module judges whether or not the UU isgrounded, based on the user?s verbal and nonverbalbehaviors during and after the UU.Using verbal evidence:  If the user returns anacknowledgement, such as ?OK?, the GrM judgesthe UU grounded.
If the user explicitly reportsfailure in perceiving MACK?s speech (ex.?what??
), or not-understanding (ex.
?I don?t un-derstand?
), the UU remains ungrounded.
Notethat, for the moment, verbal evidence is consideredstronger than nonverbal evidence.Using nonverbal evidence:  The GrM looks upthe nonverbal behavior occurring during the utter-ance, and compares it to the model shown in Table3.
For each type of speech act, this model specifiesthe nonverbal behaviors that signal positive or ex-plicit negative evidence.
First, the GrM comparesthe within-UU nonverbal behavior to the model.Then, it looks at the first nonverbal behavior oc-curring during the pause after the UU.
If these twobehaviors (?within?
and ?pause?)
match a patternthat signals positive evidence, the UU is grounded.If they match a pattern for negative evidence, theUU is not grounded.
If no pattern has yet beenFigure 3: MACK system architecturematched, the GrM waits for a tenth of a second andchecks again.
If the required behavior has oc-curred during this time, the UU is judged.
If not,the GrM continues looping in this manner until theUU is either grounded or ungrounded explicitly, ora 1 second threshold has been reached.
If thethreshold is reached without a decision, the GrMtimes out and judges the UU ungrounded.Updating the Dialogue StateAfter judging grounding, the GrM updates theDiscourse Model.
The Discourse State maintainedin the Discourse Model is similar to TRINDI kit[3], except that we store nonverbal information.There are three key fields: (1) a list of groundedUUs, (2) a list of pending (ungrounded) UUs, and(3) the current UU.
If the current UU is judgedgrounded, its belief is added to (1).
If ungrounded,the UU is stored in (2).
If an UU has subsequentcontributions such as elaboration, these are storedin a single discourse unit, and grounded togetherwhen the last UU is grounded.Determining the Next ActionAfter judging the UU?s grounding, the GrM de-cides what MACK does next.
(1) MACK can con-tinue giving the directions as normal, by sendingon the next segment in the Agenda to the GM.
Asshown in Table 3, this happens 70% of the timewhen the UU is grounded, and only 27% of thetime when it is not grounded.
Note, this happens100% of the time if verbal acknowledgement (e.g.
?Uh huh?)
is received for the UU.
(2) MACK can elaborate on the most recentstage of the directions.
Elaborations are generated73% of the time when an Assertion is judged un-grounded, and 78% of the time for an ungroundedAnswer.
MACK elaborates by describing the mostrecent landmark in more detail.
For example, ifthe directions were ?Go down the hall and make aright at the door,?
he might elaborate by saying?The big blue door.?
In this case, the GrM asksthe Response Planner (RP) to provide an elabora-tion for the current UU; the RP generates thiselaboration (looking up the landmark in the data-base) and adds it to the front of the Agenda; andthe GrM sends this new UU on to the GM.Finally, if the user gives MACK explicit verbalevience of not understanding, MACK will simplyrepeat the last thing he said, by sending the UUback to the GM.4.4 ExampleFigure 4 shows an example of a user's interactionwith MACK.
The user asks MACK for directions,and MACK replies using speech and pointing (us-ing a projector) to the shared map.When the GrM sends the first segment in theAgenda to the GM,the starting time of the UU isnoted and it is sent to the AM to be spoken andanimated.
During this time, the user?s nonverbalsignals are logged in the Discourse Model.
Whenthe UU has finished, the GrM evaluates the log ofthe UU and of the very beginning of the pause (bywaiting a tenth of a second and then checking thenonverbal history).
In this case, MACK noted thatthe user looked at the map during the UU, and con-tinued to do so just afterwards.
This patternmatches the model for Assertion.
The UU isjudged as grounded, and the grounded belief isadded to the Discourse Model.MACK then utters the second segment as be-fore, but this time the GrM, finds that the user waslooking up at MACK during most of the UU aswell as after it, which signals that the UU is notgrounded.
Therefore, the RP generates an elabora-tion (line 4).
This utterance is judged to beTable 3: Grounding Model for MACKTarget UUTypeEvidenceTypeNVPatternJudgment ofground Suggested next actionpositive within: mappause: map /nodgroundedgo-ahead: 0.7elaboration: 0.30 Assertionnegative within: gazepause: gazeungroundedgo-ahead: 0.27elaboration:0.73positive within: gazepause: mapgroundedgo-ahead: 0.83elaboration: 0.17 Answernegative pause: gaze ungroundedgo-ahead: 0.22elaboration: 0.78[1] U: How do I get to Room 309?
[2] M: To get to Room 309, go to that door and make a right.
[3] M: Walk down the hall and make a left at the door[4] M: It?s the glass door with red couches right outside.
[5] M: And that?s Room 309.look at maplook at maplook at maplook at mapgaze at MACKnodFigure 4: Example of user (U) interacting withMACK (M).
User gives negative evidence ofgrounding in [3], so MACK elaborates [4].grounded both because the user continues lookingat the map, and because the user nods, and so thefinal stage of the directions is spoken.
This is alsogrounded, leaving MACK ready for a new inquiry.5 Preliminary EvaluationAlthough we have shown an empirical basis forour implementation, it is important to ensure boththat human users interact with MACK as we ex-pect, and that their interaction is more effectivethan without nonverbal grounding.
The issue ofeffectiveness merits a full-scale study and thus wehave chosen to concentrate here on whetherMACK elicits the same behaviors from users asdoes interaction with other humans.Two subjects were therefore assigned to one of thefollowing two conditions, both of which were runas Wizard of Oz (that is, ?speech recognition?
wascarried out by an experimenter):(a) MACK-with-grounding: MACK recognizeduser?s nonverbal signals for grounding, and dis-played his nonverbal signals as a speaker.
(b) MACK-without-grounding: MACK paid noattention to the user?s nonverbal behavior, and didnot display nonverbal signals as a speaker.
He gavethe directions in one single turn.Subjects were instructed to ask for directions totwo places, and were told that they would have tolead the experimenters to those locations to testtheir comprehension.
We analyzed the second di-rection-giving interaction, after subjects becameaccustomed to the system.Results: In neither condition, did users return ver-bal feedback during MACK?s direction giving.
Asshown in Table 4, in MACK-with-grounding 7nonverbal status transitions were observed duringhis direction giving, which consisted of 5 AssertionUUs, one of them an elaboration.
The transitionpatterns between MACK and the user whenMACK used nonverbal grounding are strikinglysimilar to those in our empirical study of human-to-human communication.
There were three transi-tions to gM/gM (both look at the map), which is anormal status in map task conversation, and twotransitions to gP/gM (MACK looks at the user, andthe user looks at the map), which is the most fre-quent transition in Assertion as reported in Section3.
Moreover, in MACK?s third UU, the user beganlooking at MACK at the middle of the UU andkept looking at him after the UU ended.
This be-havior successfully elicited MACK?s elaborationin the next UU.On the other hand, in the MACK-without-grounding condition, the user never looked atMACK, and nodded only once, early on.
As shownin Table 4, only three transitions were observed(shift to gMgM at the beginning of the interaction,shift to gMgMwN, then back to gMgM).While a larger scale evaluation with quantita-tive data is one of the most important issues forfuture work, the results of this preliminary studystrongly support our model, and show MACK?spotential for interacting with a human user usinghuman-human conversational protocols.6 Discussion and Future WorkWe have reported how people use nonverbal sig-nals in the process of grounding.
We found thatnonverbal signals that are recognized as positiveevidence of understanding are different dependingon the type of speech act.
We also found that main-taining gaze on the speaker is interpreted as evi-dence of not-understanding, evoking an additionalexplanation from the speaker.
Based on these em-pirical results, we proposed a model of nonverbalgrounding and implemented it in an embodiedconversational agent.One of the most important future directions isto establish a more comprehensive model of face-to-face grounding.
Our study focused on eye gazeFigure 5: MACK with userTable 4: Preliminary evaluationwith-grounding w/o-groundingnum of UUs 5 4gMgM 3 2gPgM 2 0gMgP 1 0gPgP 1 0gMgMwN 0 1Shift tototal 7 3and head nods, which directly contribute togrounding.
It is also important to analyze othertypes of nonverbal behaviors and investigate howthey interact with eye gaze and head nods toachieve common ground, as well as contradictionsbetween verbal and nonverbal evidence (eg.
aninterlocutor says, ?OK?, but looks at the partner).Finally, the implementation proposed here is asimple one, and it is clear that a more sophisticateddialogue management strategy is warranted, andwill allow us to deal with back-grounding, andother aspects of miscommunication.
For example,it would be useful to distinguish different levels ofmiscommunication: a sound that may or may notbe speech, an out-of-grammar utterance, or an ut-terance whose meaning is ambiguous.
In order todeal with such uncertainty in grounding, incorpo-rating a probabilistic approach [4] into our modelof face-to-face grounding is an elegant possibility.AcknowledgementThanks to Candy Sidner, Matthew Stone, and 3anonymous reviewers for comments that improvedthe paper.
Thanks to Prof. Nishida at Univ.
of To-kyo for his support of the research.References1.Clark, H.H.
and E.F. Schaefer, Contributing to dis-course.
Cognitive Science, 1989.
13,: p. 259-294.2.Clark, H.H.
and D. Wilkes-Gibbs, Referring as a col-laborative process.
Cognition, 1986.
22: p. 1-39.3.Matheson, C., M. Poesio, and D. Traum.
ModellingGrounding and Discourse Obligations Using UpdateRules.
in 1st Annual Meeting of the North AmericanAssociation for Computational Linguistics(NAACL2000).
2000.4.Paek, T. and E. Horvitz, Uncertainty, Utility, andMisunderstanding, in Working Papers of the AAAI FallSymposium on Psychological Models of Communicationin Collaborative Systems, S.E.
Brennan, A. Giboin, andD.
Traum, Editors.
1999, AAAI: Menlo Park, California.p.
85-92.5.Clark, H.H., Using Language.
1996, Cambridge:Cambridge University Press.6.Traum, D.R.
and P. Dillenbourg.
Miscommunicationin Multimodal Collaboration.
in AAAI Workshop onDetecting, Repairing, and Preventing Human-MachineMiscommunication.
1996.
Portland, OR.7.Argyle, M. and M. Cook, Gaze and Mutual Gaze.1976, Cambridge: Cambridge University Press.8.Goodwin, C., Achieving Mutual Orientation at TurnBeginning, in Conversational Organization: Interactionbetween speakers and hearers.
1981, Academic Press:New York.
p. 55-89.9.Novick, D.G., B. Hansen, and K. Ward.
Coordinatingturn-taking with gaze.
in ICSLP-96.
1996.
Philadelphia,PA.10.Cassell, J., et al More Than Just a Pretty Face: Af-fordances of Embodiment.
in IUI 2000.
2000.
New Or-leans, Louisiana.11.Traum, D. and J. Rickel.
Embodied Agents for Multi-party Dialogue in Immersive Virtual Worlds.
inAutonomous Agents and Multi-Agent Systems.
2002.12.Cassell, J. and K.R.
Thorisson, The Power of a Nodand a Glance: Envelope vs.
Emotional Feedback inAnimated Conversational Agents.
Applied ArtificialIntelligence, 1999.
13: p. 519-538.13.Nakatani, C. and D. Traum, Coding discourse struc-ture in dialogue (version 1.0).
1999, University ofMaryland.14.Pierrehumbert, J.B., The phonology and phonetics ofenglish intonation.
1980, Massachusetts Institute ofTechnology.15.Allen, J. and M. Core, Draft of DMSL: Dialogue ActMarkup in Several Layers.
1997,http://www.cs.rochester.edu/research/cisd/resources/damsl/RevisedManual/RevisedManual.html.16.Duncan, S., On the structure of speaker-auditor in-teraction during speaking turns.
Language in Society,1974.
3: p. 161-180.17.Boyle, E., A. Anderson, and A. Newlands, The Ef-fects of Visibility in a Cooperative Problem SolvingTask.
Language and Speech, 1994.
37(1): p. 1-20.18.Traum, D. and P. Heeman.
Utterance Units andGrounding in Spoken Dialogue.
in ICSLP.
1996.19.Nakajima, S.y.
and J.F.
Allen.
Prosody as a cue fordiscourse structure.
in ICSLP.
1992.20.Morency, L.P., A. Rahimi, and T. Darrell.
A View-Based Appearance Model for 6 DOF Tracking," Pro-ceed-ings of.
in IEEE conference on Computer Visionand Pattern Recognition.
2003.
Madison, Wisconsin.21.Kapoor, A. and R.W.
Picard.
A Real-Time Head Nodand Shake Detector.
in Workshop on Perceptive UserInterfaces.
2001.
Orlando FL.
