Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1235?1245,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsA Semantically Enhanced Approach to Determine Textual SimilarityEduardo Blanco and Dan MoldovanLymba CorporationRichardson, TX 75080 USA{eduardo,moldovan}@lymba.comAbstractThis paper presents a novel approach to deter-mine textual similarity.
A layered methodol-ogy to transform text into logic forms is pro-posed, and semantic features are derived froma logic prover.
Experimental results show thatincorporating the semantic structure of sen-tences is beneficial.
When training data isunavailable, scores obtained from the logicprover in an unsupervised manner outperformsupervised methods.1 IntroductionThe task of Semantic Textual Similarity (Agirre etal., 2012) measures the degree of semantic equiv-alence between two sentences.
Unlike textual en-tailment (Giampiccolo et al 2007), textual similar-ity is symmetric, and unlike both textual entailmentand paraphrasing (Dolan and Brockett, 2005), tex-tual similarity is modeled using a graded score ratherthan a binary decision.
For example, sentence pair(1) below is very similar [5 out of 5], (2) is some-what similar [3 out of 5] and (3) is not similar at all[0 out of 5]:1.
Someone is removing the scales from the fish.A person is descaling a fish.2.
A woman is chopping an herb.A man is finely chopping a green substance.3.
A cat is playing with a watermelon on a floor.A man is pouring oil into a pan.State-of-the-art systems to determine textual sim-ilarity (Ba?r et al 2012; S?aric?
et al 2012; Baneaet al 2012) do not account for the semantic struc-ture of sentences, and mostly rely on word pair-ings and knowledge derived from large corpora, e.g.,man holdingAGENTaaTHEME==leaf(a)monkey fightingAGENTddTHEME==man(b)Figure 1: Semantic representation of 1(a) A man is hold-ing a leaf and 1(b) A monkey is fighting a man.Wikipedia.
Regardless of details, each word in sent1is paired with the word in sent2 that is most simi-lar according to some similarity measure.
Then, allsimilarities are added and normalized by the lengthof sent1 to obtain the similarity score from sent1 tosent2.
The process is repeated to obtain the simi-larity score from sent2 to sent1, and both scores arethen averaged to determine the overall textual sim-ilarity.
Several word-to-word similarity measuresare often combined with other shallow features, e.g.,n-gram overlap, syntactic dependencies, to obtainthe final similarity score.Consider sentences 1(a) A man is holding a leafand 1(b) A monkey is fighting a man.
These twosentences are very dissimilar, the only commonal-ity is the concept ?man?.
Any approach that blindlysearches for the word in 1(b) that is the most similarto word ?man?
in 1(a) will find ?man?
from 1(b) tobe a perfect match.
One of three content words is amatch and thus the estimated similarity will be muchhigher than it actually is.Consider now the semantic representations forsentences 1(a) and 1(b) in Figure 1.
?man?
plays therole of AGENT in 1(a), and THEME in 1(b).
Whilein both sentences the word ?man?
encodes the sameconcept, their semantic functions with respect toother concepts are different.
Intuitively, it seems rea-sonable to penalize the similarity score based on therole discrepancy.1235man usedAGENT[[THEMEAAPURPOSE&&sword sliceTHEME88INSTRUMENT]]AGENT}} plastic bottleVALUE(a)man slicedAGENT\\THEME77INSTRUMENT%%plastic bottleVALUEsword(b)womanPART%%applyingAGENT``THEME;;LOCATION((cosmetics face(c)woman puttingAGENT__THEME==makeup(d)woman dancingAGENT__LOCATION@@rain(e)woman dancesAGENT__LOCATIONAArainLOCATION@@outside(f)Figure 2: Semantic representations of 2(a) The man used a sword to slice a plastic bottle, 2(b) A man sliced a plasticbottle with a sword, 2(c) A woman is applying cosmetics to her face, 2(d) A woman is putting on makeup, 2(e) Awoman is dancing in the rain, and 2(f) A woman dances in the rain outside.
Pairs (a, b), (c, d) and (e, f) are highlysimilar even though concepts and relations only match partially.This paper proposes a novel approach to deter-mine textual similarity.
Semantic representationsof sentences are exploited, syntactic features omit-ted and the only external resource used in WordNet(Miller, 1995).
The main novelties of our approachare: it (1) derives semantic features from a logicprover to be used in a machine learning framework;(2) uses three logic form transformations capturingdifferent levels of knowledge; and (3) incorporatessemantic representations extracted automatically.1.1 Matching Semantic Representations andDetermining Textual SimilarityThroughout this paper, the semantic representationof a sentence comprises the concepts in it, semanticrelations linking those concepts and named entitiesqualifying them.
First, we note that existing tools toextract semantic relations and named entities are notperfect, thus any system relying on them will sufferfrom incomplete and incorrect representations.
Sec-ond, even if flawless representations were readilyavailable, the problem of determining textual simi-larity cannot be reduced to matching semantic repre-sentations: partial matches may correspond to com-pletely similar sentences.
The rest of this sectionillustrates this point with the examples in Figure 2.Our approach (Section 3) copes with the inherent er-rors made by tools used to obtain semantic represen-tations and learns which parts of a representation areimportant to determine textual similarity.Consider sentences 2(a) The man used a sword toslice a plastic bottle and 2(b) A man sliced a plasticbottle with a sword.
Both sentences have high simi-larity [5 out of 5], and yet their semantic representa-tions only match partially.
In this example, the verb?used ?
in 2(a) and its semantic links are somewhatsemantically superfluous.
Note that in other cases,missing a semantic relation signals lower similarity,e.g., I had fun [at the party]LOCATION and I had fun,while similar, do not convey the same meaning.Sentence 2(c) A woman is applying cosmetics toher face and 2(d) A woman is putting on makeup arehighly similar even though the latter specifies neitherthe LOCATION where the ?makeup?
is applied northe fact that a PART of the ?woman?
is her ?face?.Similarly, sentences 2(e) A woman is dancing in therain and 2(f) A woman dances in the rain outsideare semantically equivalent since ?rain?
always hasLOCATION ?outside?
: missing this information doesnot carry loss of meaning.2 Related WorkDetermining similarity between text snippets is rele-vant to information retrieval (Hatzivassiloglou et al1999), paraphrase recognition (Madnani and Dorr,2010), grading answers to questions (Mohler et al2011) and many others.
We focus on recent workand emphasize the differences from our approach.The SemEval 2012 Task 6: A Pilot on SemanticTextual Similarity (Agirre et al 2012) brought to-gether 35 teams that competed against each other.The top 3 performers (Ba?r et al 2012; S?aric?
etal., 2012; Banea et al 2012), followed a ma-1236SentencesLogic FormTransfor-mationLogicProverMachineLearningPairwiseSimilarityMeasurespairwise word similarity scoreslogic formsLFT-based scoresfeaturesscoreFigure 3: Main components of our system to determine textual similarity.chine learning approach with features that do nottake into account the semantic structure of sen-tences, e.g., n-grams, word overlap, evaluation mea-sures for machine translation, pairwise word similar-ities, syntactic dependencies.
All three used Word-Net, Wikipedia and other large corpora.
In partic-ular, Banea et al(2012) obtained models from 6million Wikipedia articles and more than 9.5 mil-lion hyperlinks; Ba?r et al(2012) used Wiktionary1,which contains over 3 million entries; and S?aric?
etal.
(2012) used The New York Times AnnotatedCorpus (Sandhaus, 2008), which contains over 1.8million news articles, and Google n-grams (Lin etal., 2012), which consists of approximately 24GBof compressed text files.
Our approach only usesWordNet, by far the smallest external resource withless than 120,000 synsets.Participants that incorporated information aboutthe semantic structure of sentences (Glinos, 2012;Rios et al 2012)2 did not perform at the top.
Outof 88 runs, they were ranked 16, 36 and 64.
We be-lieve this is because they use semantic relations tocalculate some ad-hoc similarity score.
In contrast,our approach derives features from semantic repre-sentations encoded using logic, and combine thesefeatures using machine learning.
Moreover, we usethree logic form transformations capturing differentlevels of knowledge, from only content words to se-mantic structure.
In turn, this allows us to boostperformance by relying on semantics when simplershallow methods fail.A few logic-based approaches to recognize tex-tual entailment are similar to the work presentedhere.
Bos and Markert (2006) extract semantic rep-resentations with Boxer (Bos et al 2004) and in-corporate background knowledge from external re-1http://www.wiktionary.org/2A third team, spirin2, submitted results but a descriptionpaper could not be found in the ACL anthology.sources.
They use a standard theorem prover andextract 8 features that are later combined using ma-chine learning.
Raina et al(2005) use a logic formtransformation derived from dependency parses andnamed entities.
They use abductive reasoning anddefine an assumption cost model to account for par-tial entailments.
Unlike them, we define three logicfrom transformations, use a modified resolution stepand extract hundreds of features from the proofs.Tatu and Moldovan (2005) use a modified logicprover that drops predicates when a proof cannotbe found.
Unlike us, they do not drop unboundpredicates and use a single logic form transforma-tion.
Another key difference is that they assign fixedweights to predicates a priori instead of using ma-chine learning to determine them.3 ApproachOur approach to determine textual similarity (Fig-ure 3) is grounded on using semantic features de-rived from a logic prover that are later combinedin a standard supervised machine learning frame-work.
First, sentences are transformed into logicforms (lft1, lft2).
Then, a modified logic prover isused to find a proof in both directions (lft1 to lft2and lft2 to lft1).
The prover yields similarity scoresbased on the number of predicates dropped and fea-tures characterizing the proofs.
Additional similar-ity scores are obtained using standard pairwise wordsimilarity measures.
Finally, all scores and featuresare combined using machine learning to yield the fi-nal textual similarity score.If training data is unavailable, only the LFT-basedand individual pairwise word similarity scores ap-ply, the machine learning component is the only onesupervised.
The rest of this section details eachcomponent and exemplifies it with 2(e) A woman isdancing in the rain and 2(f) A woman dances in therain outside.1237sent1: A woman is dancing in the rain.semantic relations extracted: AGENT(dancing, woman), LOCATION(dancing, rain)Basic woman N(x1) & dance V(x2) & rain N(x3)SemRels woman N(x1) & dance V(x2) & AGENT SR(x2, x1) & rain N(x3) & LOCATION SR(x2, x3)Full woman N(x1) & dance V(x2) & AGENT SR(x2, x1) & rain N(x3) & LOCATION SR(x2, x3)sent2: A woman dances in the rain outside.semantic relations extracted: AGENT(dances, woman), LOCATION(dances, rain)Basic woman N(x1) & dance V(x2) & rain N(x3) & outside M(x4)SemRels woman N(x1) & dance V(x2) & AGENT SR(x2, x1) & rain N(x3) & LOCATION SR(x2, x3)Full woman N(x1) & dance V(x2) & AGENT SR(x2, x1) & rain N(x3) & LOCATION SR(x2, x3) &outside M(x4)Table 1: Examples of logic from transformation using modes Basic, SemRels and Full.3.1 Logic Form TransformationThe logic form transformation (LFT) of a sentenceis derived from the concepts in it, the semanticrelations linking them and named entities.
Un-like other LFT proposals (Zettlemoyer and Collins,2005; Poon and Domingos, 2009), transformingsentences into logic forms is a straightforward step,the quality of the logic forms is determined by theoutput of standard NLP tools.We distinguish six types of predicates:?
N for nouns, e.g., woman: woman N(x1).?
V for verbs, e.g., dances: dance V(x2).?
M for adjectives and adverbs, e.g., outside:outside M(x3).?
O for concepts encoded by other POS tags.?
NE for named entities, e.g., guitar:guitar N(x4) & instrument NE(x4).?
SR for semantic relations, e.g., A womandances: woman N(x1) & dance V(x2) &AGENT SR(x2, x1).In order to overcome semantic relation extractionerrors, we have experimented with three logic formtransformation modes.
Each mode captures differentlevels of knowledge:Basic generates predicates for all nouns, verbs,modifiers and named entities.
This logic formis parallel to accounting for content words,their POS tags and named entity types.SemRels generates predicates for all semantic rela-tions, concepts that are arguments of relationsand named entities qualifying those concepts.This mode ignores concepts not linked to otherconcepts through a relation and might miss keyconcepts if some relations are missing.
If nosemantic relations are found, this mode backsoff to Basic to avoid empty logic forms.Full generates predicates for all concepts, all se-mantic relations and all named entities.
It isequivalent to SemRels after adding predicatesfor concepts that are not arguments of a seman-tic relation.Table 1 exemplifies the three logic form modes.If perfect semantic relations were always available,SemRels would be the preferred mode.
However,this is often not the case and combining the threelogic forms yields better performance (Section 4).Note that since relation LOCATION(rain, outside) isnot extracted from sent2, predicate outside M(x4)is not present in mode SemRels.3.2 Modified Logic ProverTextual similarity is symmetric and therefore wefind proofs in both directions (from lft1 to lft2 andfrom lft2 to lft1).
The logic prover uses a modifiedresolution procedure to calculate a similarity scoreand features derived from the proof.
The rest of thissection exemplifies one direction, lft1 to lft2.
Thelogic prover is a modification of OTTER3 (McCuneand Wos, 1997), an automated theorem prover forfirst-order logic.
For the textual similarity task, weload lft1 and ?lft2 to the set of support and lexicalchain axioms to the usable list.
Then, the logicprover begins its search for a proof.
Two scenar-ios are possible: (1) a contradiction is found, i.e.,a proof is found; or (2) a contradiction cannot befound.
The modifications to the standard resolution3http://www.cs.unm.edu/?mccune/otter/1238sent1: A woman plays an electric guitar sent2: A man is cutting a potatolft1: woman N(x1) & play V(x2) & AGENT SR(x2, x1) & electric M(x3) & guitar N(x4) &instrument NE(x4) & VALUE SR(x4, x3) & THEME SR(x2, x4)?lft2: ?man N(x1) ?
?cut V(x2) ?
?AGENT SR(x2, x1) ?
?potato N(x3) ?
?THEME SR(x2, x3)Step Predicate dropped (regular) Score Predicate dropped (unbound) Score1 woman N(x1) 0.875 n/a 0.8752 play V(x2) 0.750 AGENT SR(x2, x1) 0.6253 electric M(x3) 0.500 n/a 0.5004 guitar N(x4) 0.375 instrument NE(x4), VALUE SR(x4, x3),THEME SR(x2, x4)0.000Table 2: Example of predicate dropping step by step.
Predicates AGENT SR(x2, x1) and THEME SR(x2, x4) would notbe dropped if unbound predicates were not dropped, yielding a score of 0.250 instead of 0.000.procedure are used in scenario (2), when a proofcannot be found.
In this case, predicates from lft1 aredropped until a proof is found.
The worst case oc-curs when all predicates in lft1 are dropped.
The goalof the dropping mechanism is to force the prover toalways find a proof, and penalize partial proofs ac-cordingly.Lexical chain axioms are extracted from WordNet.Assuming each word w in sent1 has the first sense,axioms w ?
c, where c is at most distance 2 inthe WordNet hierarchy are generated.
For exam-ple, axioms derived from woman include woman?female, woman ?
mistress, woman ?
widow andwoman?
madam.
Although simple, this WordNetexpansion proved useful in our experiments.3.2.1 Predicate Dropping CriteriaWhen a proof cannot be found, individual predi-cates from lft1 not present in lft2 are dropped.
Agreedy algorithm was implemented for this step: outof all predicates from lft1 not present in lft2, dropwhichever occurs first.Dropping a predicate is not done in isolation.
Af-ter dropping a predicate, all predicates that becomeunbound are dropped as well.
With our current logicform transformation, dropping a noun, verb or modi-fier may make a semantic relation ( SR) or named en-tity ( NE) predicate unbound.
To avoid determininghigh similarity between sentences with a commonsemantic structure but unrelated concepts instantiat-ing this structure, predicates encoding semantic rela-tions and named entities are automatically droppedwhen they become unbound.3.2.2 Proof Scoring CriterionThe score assigned to the proof from lft1 to lft2is calculated as the ratio of number of predicates inlft1 not dropped to find the proof over the originalnumber of predicates in lft1.Note that the dropping mechanism, and in par-ticular whether predicates that become unboundare automatically dropped, greatly impact the proofobtained and its score (Table 2).
If predi-cates that become unbound were not automati-cally dropped in each step, instrument NE(x4) andVALUE SR(x4, x3) would be dropped in steps 5 and6, AGENT SR(x2, x1) and THEME SR(x2, x4) would notbe dropped, and the final score would be 0.250 in-stead of 0.000.
In plain English, dropping unboundpredicates avoids matching semantic structures in-stantiated by unrelated concepts.3.2.3 Feature SelectionWhile the proof score can be used directly as an es-timator of the similarity between lft1 and lft2, ad-ditional features are extracted from the proof itself.Namely, for each predicate type (N, V, M, O, SR,NE), we count the number of predicates present inlft1, the number of predicates dropped to find a prooffor lft2 and the ratio of the two counts.
These threecounts are also calculated for each specific seman-tic relation predicate (AGENT SR, LOCATION SR, etc.
).An example of score and feature calculation in bothdirections is shown in Table 3.The LFT-based scores and features are fed to amachine learning algorithm.
Specifically, there are477 features derived from the logic prover:?
9 LFT-based scores (3 ?
3; three scores (2 di-rections and average), three LFT modes)1239lft1: woman N(x1) & dance V(x2) & AGENT SR(x2, x1) & rain N(x3) & LOCATION SR(x2, x3)lft2: woman N(x1)&dance V(x2)&AGENT SR(x2, x1)&rain N(x3)&LOCATION SR(x2, x3)&outside M(x4)lft1 to lft2pred.
dropped nonescore 1featuresnt nd nr vt vd vr mt md mr net ned ner srt srd srr2 0 0 1 0 0 0 0 0 0 0 0 2 0 0lft2 to lft1pred.
dropped outside M(x4)score 5/6 = 0.833featuresnt nd nr vt vd vr mt md mr net ned ner srt srd srr2 0 0 1 0 0 1 1 1 0 0 0 2 0 0Table 3: Two logic forms and output of logic prover in both directions.
For each predicate type (n, v, m, o, ne, sr)and semantic relation type (AGENT, LOCATION, etc.)
features indicate the total number of predicates, the number ofpredicates dropped until a proof is found and ratio of the two counts (t, d and r respectively).
We omit the features forpredicate O and individual semantic relations because of space constraints.?
108 features for predicates (3 ?
6 ?
3 ?
2 =108; three features for each of the six predicatetypes, three LFT modes, two directions)?
360 features specific to a semantic relation (3?20?3?2 = 360; three features for each of the20 semantic relations types, three LFT modes,two directions)3.3 Pairwise Word SimilaritiesPairwise word similarity measures between con-cepts have been long studied, and they have beenused for the task of textual similarity before (Mihal-cea et al 2006).
We incorporate scores derived us-ing these measures for comparison purposes and toimprove robustness in our approach.Basically, each open-class word in sent1 is pairedwith the open-class word in sent2 that is most sim-ilar according to some similarity measure.
All theseindividual similarities are summed and normalizedby the length of sent1 to find the similarity be-tween sent1 and sent2.
The process is repeatedfrom sent2 to sent1 to obtain the similarity betweensent2 and sent1, and both overall similarities are av-eraged to determine the final similarity score.We have experimented with measures Path(distance in a taxonomy), LCH (Leacock andChodorow, 1998), Lesk (Lesk, 1986), WUP (Wuand Palmer, 1994), Resnik (Resnik, 1995), Lin (Lin,1998) and JCN (Jiang and Conrath, 1997), and usethe WordNet::Similarity package4.4http://wn-similarity.sourceforge.net/3.4 Machine Learning AlgorithmWe follow a standard supervised machine learningframework.
Instances from the training split areused to create a model that is later tested with testinstances not seen during training.
The model wastuned using 10-fold cross-validation over the train-ing instances.
As a learning algorithm, we use bag-ging with M5P decision trees (Quinlan, 1992; Wangand Witten, 1997) as implemented in the Weka soft-ware package (Hall et al 2009).4 Experiments and ResultsLogic forms are derived from the output of state-of-the-art NLP tools developed previously and nottuned in any way to the current task or corpora.
Ourapproach is not tied to any tool, set of named enti-ties or relations.
Any other semantic representationcould be used; the only required modification wouldbe the LFT component (Figure 3) so that it accountsfor the subtleties of the representation of choice.The named entity recognizer extracts 35 fine-grained types organized in a taxonomy (date, lan-guage, city, instrument, etc.)
and was first developedfor a question answering system (Moldovan et al2002).
The implementation uses publicly availablegazetteers as well as machine learning.Semantic relations are extracted with Polaris(Moldovan and Blanco, 2012), a semantic parserthat given text extracts semantic relations.
Polarisis trained using FrameNet (Baker et al 1998), Prop-Bank (Palmer et al 2005), NomBank (Meyers et al2004), several SemEval corpora (Girju et al 2007;1240Score Sentence Pair NotesMSRpar(36/35) [750/750]2.600 The unions also staged a five-day strike in Marchthat forced all but one of Yale?s dining halls to close.Long sentences, difficult toparse; often several detailsare missing in one sentencebut the pair is similarThe unions also staged a five-day strike in March;strikes have preceded eight of the last 10 contracts.MSRvid 0.000 A woman is swimming underwater.
Short sentences, easy toparse(13/13), [750/750] A man is slicing some carrots.SMTeuroparl 4.250 Then perhaps we could have avoided a catastrophe.
One sentence oftenungrammatical (SMT)(56/21), [734/459] We would perhaps then able prevent a disaster.surprise.OnWN 1.500 the alleviation of distress WN glosses, difficult toparse with standard tools(?/16), [?/750] a change for the better.surprise.SMTnews 3.000 He did, but the initiative did not get very far One sentence oftenungrammatical (SMT)(?/24), [?/399] What he has done without the initiative goes too far.Table 4: Examples of sentence pairs belonging to the five sources.
The numbers between round (square) parenthesisindicate the average number of tokens per sentence pair (number of instances) in the train and test splits.Pustejovsky and Verhagen, 2009; Hendrickx et al2010) and in-house annotations.4.1 CorporaWe use the corpora released by SemEval 2012Task 06: A Pilot on Semantic Textual Similarity5(Agirre et al 2012).
These corpora consist of pairsof sentences labeled with their semantic similar-ity score, ranging from 0.0 to 5.0.
Sentence pairscome from five sources: (1) MSRpar, a corpus ofparaphrases; (2) MSRvid, short video descriptions;(3) SMTeuroparl, output of machine translation sys-tems and reference translations; (4) surprise.OnWN,OntoNotes (Hovy et al 2006) and WordNet (Miller,1995) glosses; and (5) surprise.SMTnews, output ofmachine translation systems in the news domain andgold translations.
Examples can be found in Table 4,for more details refer to the aforementioned citation.4.2 Results and Error AnalysisResults are reported using the same train and testsplits provided by the organization of SemEval 2012Task 6.
For surprise.OnWn and surprise.SMTnews,only test data is available and supervised machinelearning is not an option.Table 5 shows results obtained with the test splitnot dropping and dropping unbound predicates.
Forcomparison purposes, results of the top-3 perform-ers and participants using the semantic structure ofsentences are also shown.
LFT-score systems output5http://www.cs.york.ac.uk/semeval-2012/task6/the score (average of both directions) obtained withthe corresponding logic form transformation (Basic,SemRels or Full) and are unsupervised: training datawith textual similarity scores is not used.
The otherthree systems presented are supervised.
LFT-scores+ features combines the 9 LFT-scores and 468 fea-tures derived from the logic proof.
WN-scores usesas features the 7 scores derived using pairwise wordsimilarity measures.
Finally, All combines the fullset of 484 features.
We indicate that the performanceof one of our systems with respect to LFT score Ba-sic not dropping unbound predicates is significantwith ?
(confidence 99%) and ?
(confidence 95%).Overall, systems that drop unbound predicatesperform better than systems that do not drop them.The only noticeable exception is LFT-score withsentences from SMTeuroparl.
However, best resultsfor SMTeuroparl are obtained dropping unboundpredicates and using All features.
Henceforth, wecomment on results dropping unbound predicates asthey are higher.Regarding logic form transformations, one cansee a trend depending on the source of sen-tences.
Polaris, the semantic parser, and the syn-tactic parser Polaris relies on are mostly trained inthe news domain, and thus semantic representationshave higher quality in that domain.
For SMTeu-roparl and SMTnews, the two corpora closest to thenews domain, Full obtains better results than Ba-sic and SemRels.
The difference is most noticeablein SMTnews, where Basic yields 0.4616, SemRels1241System MSRpar MSRvid SMTeuroparl OnWN SMTnewsnotdroppingunboundpredicatesnoMLLFT scoreBasic 0.4963 0.8198 0.5101 0.6103 0.4588SemRels ?0.3952 ?0.6753 0.4920 ?0.5055 0.4477Full 0.4525 ?0.7024 0.5183 0.5895 0.4956MLLFT scores + features ?0.5750 0.8466 0.4725 n/a n/aWN scores 0.4978 0.8495 0.5217 n/a n/aAll ?0.5992 ?0.8660 0.5194 n/a n/adroppingunboundpredicatesnoMLLFT scoreBasic ?0.5552 0.8234 0.4994 0.6120 0.4616SemRels 0.4556 ?0.7388 0.4871 ?0.5113 0.4796Full 0.5250 ?0.7672 0.5130 0.5895 ?0.5291MLLFT scores + features ?0.5770 0.8440 0.5277 n/a n/aWN scores 0.4977 0.8495 0.5217 n/a n/aAll ?0.6157 ?0.8709 ?0.5745 n/a n/aTopperformer(Ba?r et al 2012) 0.6830 0.8739 0.5280 0.6641 0.4937(S?aric?
et al 2012) 0.6985 0.8620 0.3612 0.7049 0.4683(Banea et al 2012) 0.5353 0.8750 0.4203 0.6715 0.4033Team w/semanticstructurespirin2 0.5769 0.8203 0.4667 0.5835 0.4945(Rios et al 2012) 0.3628 0.6426 0.3074 0.2806 0.2082(Glinos, 2012) 0.2312 0.6595 0.1504 0.2735 0.1426Table 5: Correlations obtained with the test split using our approach (not dropping and dropping unbound predicates),and results obtained by the top-3 performers and teams that included in their models features derived from the semanticstructure of sentences.
Statistically significant differences in performance between our systems and LFT score Basicnot dropping unbound predicates are indicated with ?
(confidence 99%) and ?
(confidence 95%).0.4796 (+0.0180) and Full 0.5291 (+0.0675 and+0.0495 respectively).Outside the news domain (MSRpar, MSRvid,OnWN), Basic performs better than SemRels andFull, and Full performs better than SemRels.
Thisleads to the conclusion that several semantic rela-tions are often missing, and thus considering con-cepts even if they are not linked to other conceptsvia a semantic relation (Full) is more sound than ig-noring them (SemRels).When training data is available (MSRpar,MSRvid, SMTeuroparl), LFT-scores + features al-ways outperforms the scores obtained with a singlelogic form transformation in an unsupervised man-ner.
In other words, combining the scores obtainedwith the three logic form transformations and in-corporating the additional features derived from theproofs improves performance.
These results demon-strate that while a shallow logic form transforma-tion (Basic) offers a strong baseline, it can be suc-cessfully complemented with logic form transfor-mations that consider the semantic structure of sen-tences (SemRels, Full) and additional features char-acterizing the proofs.
The improvements LFT-scores+ features brings over the LFT-score obtained withBasic are substantial: 0.0218 (3.9%) for MSRpar,0.0206 (2.5%) for MSRvid and 0.0283 (5.7%) forSMTeuroparl.WN scores, which only uses as features thescores derived from pairwise word similarity mea-sures, performs astonishingly well for some cor-pora.
Namely, the differences in performance be-tween LFT scores + features and WN scores inMSRvid and SMTeuroparl are minimal (?0.0055and +0.0060).
We believe this is due to the charac-teristics of these two corpora.
Sentence pairs fromMSRvid are very short with 13 tokens on average(Table 4), i.e., 6.5 tokens per sentence, and SMTeu-roparl pairs are hard to parse: at least one comesfrom a machine translation system and is often un-grammatical.Finally, dropping unbound predicates and usingAll features outperforms any other system.
Whileboth LFT scores + features and WN scores yield1242good performance, the combination of the two out-performs them.
Features extracted successfullycomplement each other for all corpora.4.2.1 A Look at the ML ModelA benefit of decision trees is that one can inspectthem.
This section briefly gives insight about themost predictive features for All system.The best features, i.e., features used in decisionscloser to the root, are the LFT-scores calculated us-ing Basic and Full.
The LFT-score obtained us-ing SemRels is used only when the other two can-not discriminate.
Sorted by impact, the features ex-tracted for verbs, nouns, semantic relations, namedentities and modifiers follow.
Towards the bottomof the tree, features for specific semantic relations(AGENT SR, LOCATION SR, etc.)
are used.
All threesources (MSRpar, MSRvid and SMTeuroparl) usefeatures for THEME, LOCATION, AGENT and QUAN-TIFICATION.
MSRpar also benefits from features forTIME and only SMTeuroparl benefits from TOPICand MANNER.4.2.2 Comparison with Previous WorkThe semantic logic-based approach presented in thispaper either outperforms other systems or performsin the top-3 (Table 5).
Moreover, it clearly outper-forms any other proposal that takes into account thesemantic structure of sentences.
These results leadto the conclusion that the semantic structure of sen-tences is worth considering and more effort shouldbe devoted to deeper approaches.When using sentences in the news domain (SM-Teuroparl and SMTnews), i.e., when text is closerto the domain in which the NLP tools are trained,our semantic approach yields the best results knownto date.
For MSRvid, the system presented hereperforms as well as systems that use externalknowledge (Section 2), the differences are mini-mal (+0.0030, ?0.0089, +0.0041) and not statis-tically significant (confidence 99%).
For MSRpar,the system performs amongst the top-3 even thoughtwo of these systems clearly obtained better results(+0.0673, +0.0828); both differences are statisti-cally significant (confidence 99%).Performance using surprise.OnWN deserves spe-cial comment.
This corpus contains definitions, notsentences (Table 4).
Lin?s similarity measure aloneyields a correlation of 0.6787, beating all systems inTable 5 except one of the top-3 performers (S?aric?
etal., 2012).
Our semantic approach is not success-ful because we cannot extract valid representations,glosses are rarely a full sentence and are hard toparse with generic NLP tools like the ones we use.5 ConclusionsThis paper presents a novel approach to determinetextual similarity that employs a logic prover to ex-tract semantic features.
A layered methodology totransform text into logic forms using three logicform transformations modes is presented.
Eachmode captures different levels of knowledge, fromonly content words to semantic representations auto-matically extracted.
Best results are obtained whenfeatures derived from the logic prover are comple-mented with simpler pairwise word similarity mea-sures.
Features that account for the semantic struc-ture of sentences are incorporated when needed, asthe results obtained with systems All, LFT scoresand WN scores show.Our approach is heavily dependent on the qual-ity of semantic representations, and unlike currenttop performers, does not require knowledge derivedfrom Wikipedia or other large corpora.
State-of-the-art NLP tools to extract semantic representationsfrom text, which are far from perfect, yield promis-ing results.
Indeed, the approach outperforms previ-ous work when the source text is relatively familiarto the tools, i.e., within the news domain, and per-forms in the top-3 otherwise.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: A piloton semantic textual similarity.
In Proceedings of theSixth International Workshop on Semantic Evaluation(SemEval 2012), pages 385?393, Montre?al, Canada,7-8 June.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In Proceed-ings of the 17th international conference on Computa-tional Linguistics, Montreal, Canada.Carmen Banea, Samer Hassan, Michael Mohler, andRada Mihalcea.
2012.
Unt: A supervised syner-gistic approach to semantic text similarity.
In Pro-ceedings of the Sixth International Workshop on Se-1243mantic Evaluation (SemEval 2012), pages 635?642,Montre?al, Canada, 7-8 June.Daniel Ba?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
Ukp: Computing semantic textual sim-ilarity by combining multiple content similarity mea-sures.
In Proceedings of the Sixth International Work-shop on Semantic Evaluation (SemEval 2012), pages435?440, Montre?al, Canada, 7-8 June.Johan Bos and Katja Markert.
2006.
Recognising tex-tual entailment with robust logical inference.
In Pro-ceedings of the First international conference on Ma-chine Learning Challenges: evaluating Predictive Un-certainty Visual Object Classification, and Recogniz-ing Textual Entailment, MLCW?05, pages 404?426,Berlin, Heidelberg.
Springer-Verlag.Johan Bos, Stephen Clark, Mark Steedman, James R.Curran, and Julia Hockenmaier.
2004.
Wide-coveragesemantic representations from a ccg parser.
In Pro-ceedings of Coling 2004, pages 1240?1246, Geneva,Switzerland, Aug 23?Aug 27.
COLING.William B. Dolan and Chris Brockett.
2005.
Automati-cally constructing a corpus of sentential paraphrases.In Proceedings of the Third International Workshopon Paraphrasing (IWP2005).
Association for Compu-tational Linguistics.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, andBill Dolan.
2007.
The third pascal recognizing tex-tual entailment challenge.
In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Para-phrasing, pages 1?9, Prague, June.
Association forComputational Linguistics.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.SemEval-2007 Task 04: Classification of SemanticRelations between Nominals.
In Proceedings of theFourth International Workshop on Semantic Evalua-tions (SemEval-2007), pages 13?18, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Demetrios Glinos.
2012.
Ata-sem: Chunk-based deter-mination of semantic text similarity.
In Proceedingsof the Sixth International Workshop on Semantic Eval-uation (SemEval 2012), pages 547?551, Montre?al,Canada, 7-8 June.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18.Vasileios Hatzivassiloglou, Judith L. Klavans, andEleazar Eskin.
1999.
Detecting text similarity overshort passages: exploring linguistic feature combina-tions via machine learning.
In In Proceedings of the1999 Joint SIGDAT Conference on Empirical Meth-ods in Natural Language Processing and Very LargeCorpora, pages 203?212.Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, PreslavNakov, Diarmuid O?
Se?aghdha, Sebastian Pado?, MarcoPennacchiotti, Lorenza Romano, and Stan Szpakow-icz.
2010.
Semeval-2010 task 8: Multi-way classifica-tion of semantic relations between pairs of nominals.In Proceedings of the 5th International Workshop onSemantic Evaluation, pages 33?38, Uppsala, Sweden,July.
Association for Computational Linguistics.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:the 90% Solution.
In NAACL ?06: Proceedings ofthe Human Language Technology Conference of theNAACL, Companion Volume: Short Papers on XX,pages 57?60, Morristown, NJ, USA.
Association forComputational Linguistics.J.J.
Jiang and D.W. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
InProc.
of the Int?l.
Conf.
on Research in ComputationalLinguistics.C.
Leacock and M. Chodorow, 1998.
Combining localcontext and WordNet similarity for word sense identi-fication, pages 305?332.
In C. Fellbaum (Ed.
), MITPress.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: how to tell a pinecone from an ice cream cone.
In Proceedings of the5th annual international conference on Systems docu-mentation, SIGDOC ?86, pages 24?26, New York, NY,USA.
ACM.Yuri Lin, Jean-Baptiste Michel, Erez Aiden Lieberman,Jon Orwant, Will Brockman, and Slav Petrov.
2012.Syntactic annotations for the google books ngram cor-pus.
In Proceedings of the ACL 2012 System Demon-strations, pages 169?174, Jeju Island, Korea, July.
As-sociation for Computational Linguistics.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the Fifteenth In-ternational Conference on Machine Learning, ICML?98, pages 296?304, San Francisco, CA, USA.
Mor-gan Kaufmann Publishers Inc.Nitin Madnani and Bonnie J. Dorr.
2010.
Generatingphrasal and sentential paraphrases: A survey of data-driven methods.
Comput.
Linguist., 36(3):341?387,September.William McCune and Larry Wos.
1997.
Otter: The cade-13 competition incarnations.
Journal of AutomatedReasoning, 18:211?220.Adam Meyers, Ruth Reeves, Catherine Macleod, RachelSzekely, Veronika Zielinska, Brian Young, and RalphGrishman.
2004.
Annotating noun argument struc-ture for nombank.
In LREC.
European Language Re-sources Association.1244Rada Mihalcea, Courtney Corley, and Carlo Strappar-ava.
2006.
Corpus-based and knowledge-based mea-sures of text semantic similarity.
In Proceedings ofthe 21st national conference on Artificial intelligence,AAAI?06, pages 775?780.
AAAI Press.George A. Miller.
1995.
WordNet: A Lexical Databasefor English.
In Communications of the ACM, vol-ume 38, pages 39?41.Michael Mohler, Razvan Bunescu, and Rada Mihalcea.2011.
Learning to grade short answer questions usingsemantic similarity measures and dependency graphalignments.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 752?762, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Dan Moldovan and Eduardo Blanco.
2012.
Po-laris: Lymba?s semantic parser.
In Nicoletta Calzo-lari, Khalid Choukri, Thierry Declerck, Mehmet Ug?urDog?an, Bente Maegaard, Joseph Mariani, andJan Odijk a nd Stelios Piperidis, editors, Proceedingsof the Eighth International Conference on LanguageResources and Evaluation (LREC-2012), pages 66?72,Istanbul, Turkey, May.
European Language ResourcesAssociation (ELRA).
ACL Anthology Identifier: L12-1040.D.
Moldovan, S. Harabagiu, R. Girju, P. Morarescu,F.
Lacatusu, A. Novischi, A. Badulescu, and O. Bolo-han.
2002.
Lcc tools for question answering.
InVoorhees and Buckland, editors, Proceedings of the11th Text REtrieval Conference (TREC-2002), NIST,Gaithersburg.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?106.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised Semantic Parsing.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 1?10, Singapore, August.
As-sociation for Computational Linguistics.James Pustejovsky and Marc Verhagen.
2009.
SemEval-2010 Task 13: Evaluating Events, Time Expressions,and Temporal Relations (TempEval-2).
In Proceed-ings of the Workshop on Semantic Evaluations: Re-cent Achievements and Future Directions (SEW-2009),pages 112?116, Boulder, Colorado, June.
Associationfor Computational Linguistics.Ross J. Quinlan.
1992.
Learning with continuousclasses.
In 5th Australian Joint Conference on Arti-ficial Intelligence, pages 343?348, Singapore.
WorldScientific.Rajat Raina, Andrew Y. Ng, and Christopher D. Man-ning.
2005.
Robust textual inference via learning andabductive reasoning.
In Proceedings of the 20th na-tional conference on Artificial intelligence - Volume 3,AAAI?05, pages 1099?1105.
AAAI Press.Philip Resnik.
1995.
Using information content to evalu-ate semantic similarity in a taxonomy.
In Proceedingsof the 14th international joint conference on Artificialintelligence - Volume 1, IJCAI?95, pages 448?453, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.Miguel Rios, Wilker Aziz, and Lucia Specia.
2012.Uow: Semantically informed text similarity.
In Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 673?678,Montre?al, Canada, 7-8 June.Evan Sandhaus.
2008.
The new york times annotatedcorpus.
In Linguistic Data Consortium, Philadelphia,PA.Marta Tatu and Dan Moldovan.
2005.
A semantic ap-proach to recognizing textual entailment.
In Proceed-ings of the conference on Human Language Technol-ogy and Empirical Methods in Natural Language Pro-cessing, HLT ?05, pages 371?378, Stroudsburg, PA,USA.
Association for Computational Linguistics.Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,and Bojana Dalbelo Bas?ic?.
2012.
Takelab: Sys-tems for measuring semantic text similarity.
In Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 441?448,Montre?al, Canada, 7-8 June.Y.
Wang and I. H. Witten.
1997.
Induction of model treesfor predicting continuous classes.
In Poster papers ofthe 9th European Conference on Machine Learning.Springer.Zhibiao Wu and Martha Palmer.
1994.
Verbs semanticsand lexical selection.
In Proceedings of the 32nd an-nual meeting on Association for Computational Lin-guistics, ACL ?94, pages 133?138, Stroudsburg, PA,USA.
Association for Computational Linguistics.Luke Zettlemoyer and Michael Collins.
2005.
Learn-ing to Map Sentences to Logical Form: StructuredClassification with Probabilistic Categorial Grammars.In Proceedings of the Proceedings of the Twenty-FirstConference Annual Conference on Uncertainty in Ar-tificial Intelligence (UAI-05), pages 658?666, Arling-ton, Virginia.
AUAI Press.1245
