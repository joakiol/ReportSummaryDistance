Coling 2010: Poster Volume, pages 1399?1407,Beijing, August 2010Jointly Identifying Entities and Extracting Relations in EncyclopediaText via A Graphical Model Approach?Xiaofeng YU Wai LAMInformation Systems LaboratoryDepartment of Systems Engineering & Engineering ManagementThe Chinese University of Hong Kong{xfyu,wlam}@se.cuhk.edu.hkAbstractIn this paper, we investigate the problem of en-tity identification and relation extraction from en-cyclopedia articles, and we propose a joint discrim-inative probabilistic model with arbitrary graphicalstructure to optimize all relevant subtasks simulta-neously.
This modeling offers a natural formalismfor exploiting rich dependencies and interactionsbetween relevant subtasks to capture mutual ben-efits, as well as a great flexibility to incorporate alarge collection of arbitrary, overlapping and non-independent features.
We show the parameter es-timation algorithm of this model.
Moreover, wepropose a new inference method, namely collec-tive iterative classification (CIC), to find the mostlikely assignments for both entities and relations.We evaluate our model on real-world data fromWikipedia for this task, and compare with currentstate-of-the-art pipeline and joint models, demon-strating the effectiveness and feasibility of our ap-proach.1 IntroductionWe investigate a compound information extrac-tion (IE) problem from encyclopedia articles,which consists of two subtasks ?
recognizingstructured information about entities and extract-ing the relationships between entities.
The mostcommon approach to this problem is a pipelinearchitecture: attempting to perform different sub-tasks, namely, named entity recognition and rela-tion extraction between recognized entities in sev-eral separate, and independent stages.
Such kindof design is widely adopted in NLP.
?The work described in this paper is substantially sup-ported by grants from the Research Grant Council of theHong Kong Special Administrative Region, China (ProjectNo: CUHK4128/07) and the Direct Grant of the Fac-ulty of Engineering, CUHK (Project Codes: 2050442 and2050476).
This work is also affiliated with the Microsoft-CUHK Joint Laboratory for Human-centric Computing andInterface Technologies.The most common and simplest approach toperforming compound NLP tasks is the 1-bestpipeline architecture, which only takes the 1-besthypothesis of each stage and pass it to the nextone.
Although it is comparatively easy to buildand efficient to run, this pipeline approach ishighly ineffective and suffers from serious prob-lems such as error propagation (Finkel et al,2006; Yu, 2007; Yu et al, 2008).
It is not sur-prising that, the end-to-end performance will berestricted and upper-bounded.Usually, one can pass N-best lists between dif-ferent stages in pipeline architectures, and this of-ten gives useful improvements (Hollingshead andRoark, 2007).
However, effectively making use ofN-best lists often requires lots of engineering andhuman effort (Toutanova, 2005).
On the otherhand, one can record the complete distribution ateach stage in a pipeline, to compute or approxi-mate the complete distribution at the next stage.Doing this is generally infeasible, and this solu-tion is rarely adopted in practice.One promising way to tackle the problem of er-ror propagation is to explore joint learning whichintegrates evidences from multiple sources andcaptures mutual benefits across multiple compo-nents of a pipeline for all relevant subtasks simul-taneously (e.g., (Toutanova et al, 2005), (Poonand Domingos, 2007), (Singh et al, 2009)).
Jointlearning aims to handle multiple hypotheses anduncertainty information and predict many vari-ables at once such that subtasks can aid each otherto boost the performance, and thus usually leadsto complex model structure.
However, it is typ-ically intractable to run a joint model and theysometimes can hurt the performance, since they1399increase the number of paths to propagate errors.Due to these difficulties, research on building jointapproaches is still in the beginning stage.A significant amount of recent work has shownthe power of discriminatively-trained probabilisticgraphical models for NLP tasks (Lafferty et al,2001; Sutton and McCallum, 2007; Wainwrightand Jordan, 2008).
The superiority of graphicalmodel is its ability to represent a large number ofrandom variables as a family of probability dis-tributions that factorize according to an underly-ing graph, and capture complex dependencies be-tween variables.
And this progress has begun tomake the joint learning approach possible.In this paper we study and formally define thejoint problem of entity identification and relationextraction from encyclopedia text, and we proposea joint paradigm in a single coherent frameworkto perform both subtasks simultaneously.
Thisframework is based on undirected probabilisticgraphical models with arbitrary graphical struc-ture.
We show how the parameters in this modelcan be estimated efficiently.
More importantly, wepropose a new inference method ?
collective it-erative classification (CIC), to find the maximuma posteriori (MAP) assignments for both entitiesand relations.
We perform extensive experimentson real-world data from Wikipedia for this task,and substantial gains are obtained over state-of-the-art probabilistic pipeline and joint models, il-lustrating the promise of our approach.2 Problem Formulation2.1 Problem DescriptionThis problem involves identifying entities and dis-covering semantic relationships between entitypairs from English encyclopedic articles.
The ba-sic document is an article, which mainly definesand describes an entity (known as principal en-tity).
This document mentions some other entitiesas secondary entities related to the principal en-tity.
Clearly, our task consists of two subtasks ?first, for entity identification, we need to recog-nize the secondary entities (both the boundariesand types of them) in the document 1.
Second,1Since the topic/title of an article usually defines a princi-pal entity (e.g., a famous person) and it is easy to identify, inafter all the secondary entities are identified, ourgoal for relation extraction is to predict what rela-tion, if any, each secondary entity has to the prin-cipal entity.
We assume that there is no relation-ship between any two secondary entities in onedocument.As an illustrative example, Figure 1 shows thetask of entity identification and relationship ex-traction from encyclopedic documents.
Here,Abraham Lincoln is the principal entity.
Ourtask consists of assigning a set of pre-defined en-tity types (e.g., PER, DATE, YEAR, and ORG)to segmentations in encyclopedic documents andassigning a set of pre-defined relations (e.g.,birth day, birth year, and member of) for eachidentified secondary entity to the principal entity.2.2 Problem FormulationLet x be an observation sequence of tokens inencyclopedic text and x = {x1, ?
?
?
, xN}.
Letsp be the principal entity (we assume that it isknown or can be easily recognized), and let s ={s1, ?
?
?
, sL} be a segmentation assignment of ob-servation sequence x.
Each segment si is a triplesi = {?i, ?i, yi}, where ?i is a start position, ?iis an end position, and yi is the label assigned toall tokens of this segment.
The segment si satis-fies 0 ?
?i < ?i ?
|x| and ?i+1 = ?i + 1.
Letrpn be the relation assignment between principalentity sp and secondary entity candidate sn fromthe segmentation s, and r be the set of relation as-signments for sequence x.Let y = {r, s} be the pair of segmentation s andsegment relations r for an observation sequencex.
A valid assignment y must satisfy the condi-tion that the assignments of the segments and theassignments of the relations of segments are max-imized simultaneously.
We now formally definethis joint optimization problem as follows:Definition 1 (Joint Optimization of Entity Iden-tification and Relation Extraction): Given an ob-servation sequence x, the goal of joint optimiza-tion of entity identification and relation extractionis to find the assignment y?
= {r?, s?}
that has themaximum a posteriori (MAP) probabilityy?
= argmaxyP (y|x), (1)this paper we only focus on secondary entity identification.1400      !"# $# %&	' 	''Figure 1: An example of entity identification and relation extraction excerpted from our dataset.
Thesecondary entities are in pink color and labeled.
The semantic relation of each secondary entity tothe principal entity Abraham Lincoln (in green color and we assume that it is known or can be easilyrecognized) is also shown.where r?
and s?
denote the most likely relationassignment and segmentation assignment, respec-tively.Note that this problem is usually very challeng-ing and offers new opportunities for informationextraction, since complex dependencies betweensegmentations and relations should be exploited.3 Our Proposed Model3.1 PreliminariesConditional random fields (CRFs) (Lafferty et al,2001) are undirected graphical models trained tomaximize the conditional probability of the de-sired outputs given the corresponding inputs.
LetG be a factor graph (Kschischang et al, 2001)defining a probability distribution over a set ofoutput variables o conditioned on observation se-quences x.
C = {?c(oc, xc)} is a set of factors inG, then the probability distribution over G can bewritten asP (o|x) = 1Z(x)?c?C?c(oc, xc) (2)where ?c is a potential function and Z(x) =?o?c?C ?c(oc, xc) is a normalization factor.We assume the potentials factorize according toa set of features {fk(oc, xc)} as ?c(oc, xc) =exp(?k ?kfk(oc, xc)) so that the family of dis-tributions is an exponential family.
The modelparameters are a set of real-valued weights ?
={?k}, one weight for each feature.
Practical mod-els rely extensively on parameter tying to use thesame parameters for several factors.However, the traditional fashion of CRFs canonly deal with single task, they lack the capabil-ity to represent more complex interaction betweenmultiple subtasks.
In the following we will de-scribe our joint model in detail for this problem.3.2 A Joint Model for Entity Identificationand Relation ExtractionFollowing the notations in Section 2.2, let L andM be the number of segments and number ofrelations for sequence x, respectively.
We de-fine a joint conditional distribution for segmen-tation s in observation sequence x and segmentrelation r in undirected, probabilistic graphicalmodels.
The nature of our modeling enables usto partition the factors C of G into three groups{CS , CR, CO}={{?S}, {?R}, {?O}}, namely thesegmentation potential ?S , the relation potential?R, and the segmentation-relation joint poten-tial ?O, and each potential is a clique templatewhose parameters are tied.
The potential function?S(i, s, x) models segmentation s in x, the poten-tial function ?R(rpm, rpn, r) (m 6= n) representdependencies (e.g., long-distance dependencies,relation transitivity, etc) between any two rela-tions in the relation set r, where rpm is the relationassignment between the principal entity sp and thesecondary entity candidate sm from s, and simi-larly for rpn.
And the joint potential ?O(sp, sj , r)captures rich and complex interactions betweensegmentation s for secondary entity identificationand relation r between each secondary entity can-didate sj to the principal entity sp.
Accordingto the celebrated Hammersley-Clifford theorem(Besag, 1974), the joint conditional distributionP (y|x) = P ({r, s}|x) is factorized as a productof potential functions over cliques in the graph Gas the form of an exponential family:P (y|x) = 1Z(x)(?CS?S(i, s, x))(?CR?R(rpm, rpn, r))(?CO?O(sp, sj , r)) (3)1401where Z(x) = ?y?CS ?S(i, s, x)?CR ?R(rpm, rpn, r)?CO ?O(sp, sj , r) is the normalizationfactor of the joint model.We assume the potential functions ?S , ?Rand ?O factorize according to a set of fea-tures and a corresponding set of real-valuedweights.
More specifically, ?S(i, s, x) =exp(?|s|i=1?Kk=1 ?kgk(i, s, x)).
To effectivelycapture properties of segmentation, we relax thefirst-order Markov assumption to semi-Markovsuch that each segment feature function gk(?
)depends on the current segment si, the previ-ous segment si?1, and the whole observation se-quence x, that is, gk(i, s, x) = gk(si?1, si, x) =gk(yi?1, yi, ?i, ?i, x).
And transitions within asegment can be non-Markovian.Similarly, the potential ?R(rpm, rpn, r) =exp(?Mm,n?Ww=1 ?wqw(rpm, rpn, r)) and ?O(sp,sj , r) = exp(?Lj=1?Tt=1 ?tht(sp, sj , r)), whereW and T are number of feature functions, qw(?
)and ht(?)
are feature functions, ?w and ?t arecorresponding weights for them.
The potential?R(rpm, rpn, r) allows long-range dependencyrepresentation between different relations rpm andrpn.
For example, if the same secondary en-tity is mentioned more than once in an obser-vation sequence, all mentions probably have thesame relation to the principal entity.
Using poten-tial ?R(rpm, rpn, r), evidences for the same entitysegments to the principal entity are shared amongall their occurrences within the document.
Thejoint factor ?O(sp, sj , r) exploits tight dependen-cies between segmentations and relations.
For ex-ample, if a segment is labeled as a location andthe principal entity is person, the semantic rela-tion between them can be birth place or visited,but cannot be employment.
Such dependenciesare essential and modeling them often leads to im-proved performance.
In summary, the probabilitydistribution of the joint model can be rewritten as:P (y|x) = 1Z(x) exp{ |s|?i=1K?k=1?kgk(i, s, x) +M?m,nW?w=1?wqw(rpm, rpn, r) +L?j=1T?t=1?tht(sp, sj , r)}(4)              Figure 2: Graphical representation of the proba-bilistic joint model.
The gray nodes represent se-quence tokens {x1, ?
?
?
, xN}.
Each ellipse repre-sents a segment consisting of several consecutivesequence tokens.
The pink nodes represent seg-mentation assignment {s1, ?
?
?
, sL} of sequence.The yellow nodes represent relation assignment{rp1, ?
?
?
, rpL} between the principal entity sp (ingreen color) and secondary entity segments.As illustrated in Figure 2, our model consistsof three sub-structures: a semi-Markov chain onthe segmentations s conditioned on the observa-tion sequences x, represented by ?S ; potential ?Rmeasuring dependencies between different rela-tions rpm and rpn; and a fully-connected graphon the principal entity sp and each segment sj fortheir relations, represented by ?O.While several special cases of CRFs are of par-ticular interest, and we emphasize on the differ-ences and advantages of our model against oth-ers.
Linear-chain CRFs (Lafferty et al, 2001) canonly perform single sequence labeling, they lackthe ability to capture long-distance dependencyand represent complex interactions between mul-tiple subtasks.
Skip-chain CRFs (Sutton and Mc-callum, 2004) introduce skip edges to model long-distance dependencies to handle the label consis-tency problem in single sequence labeling and ex-traction.
2D CRFs (Zhu et al, 2005) are two-dimensional conditional random fields incorporat-ing the two-dimensional neighborhood dependen-cies in Web pages, and the graphical representa-tion of this model is a 2D grid.
Hierarchical CRFs(Liao et al, 2007) are a class of CRFs with hi-erarchical tree structure.
Our probabilistic modelfor joint entity identification and relation extrac-tion has distinct graphical structure from 2D andhierarchical CRFs.
And this modeling has sev-1402eral advantages over previous probabilistic graph-ical models by using semi-Markov chains for effi-cient segmentation and labeling, by representinglong-range dependencies between relations, andby capturing rich and complex interactions be-tween relevant subtasks to exploit mutual benefits.4 Learning the ParametersGiven independent and identically distributed(IID) training data D = {xi, yi}Ni=1, where xi isthe i-th sequence instance, yi = {ri, si} is thecorresponding segmentation and relation assign-ments.
The objective of learning is to estimate?
= {?k, ?w, ?t} which is the vector of model?sparameters.
Under the IID assumption, we ig-nore the summation operator ?Ni=1 in the log-likelihood during the following derivations.
Toreduce over-fitting, we use regularization and acommon choice is a spherical Gaussian prior withzero mean and covariance ?2I .
Then the regular-ized log-likelihood function L for the data isL = log[?
(r, s, x)]?
log [Z(x)]?K?k=1?2k2?2??W?w=1?2w2?2??T?t=1?2t2?2?
(5)where ?
(r, s, x) = exp{?|s|i=1?Kk=1 ?kgk(i, s, x)+?Mm,n?Ww=1 ?wqw(rpm, rpn, r)+?Lj=1?Tt=1?tht(sp, sj , r)}, Z(x) = ?y??
(r, s, x), and1/2?2?, 1/2?2?, 1/2?2?
are regularization parame-ters.Taking derivatives of the function L over theparameter ?k yields:?L?
?k=|s|?i=1gk(i, s, x)?|s|?i=1gk(i, s, x)P (y|x)?K?k=1?k?2?
(6)Similarly, the partial derivatives of the log-likelihood with respect to parameters ?w and nutare as follows:?L?
?w=M?m,nqw(rpm, rpn, r)?M?m,nqw(rpm, rpn, r)?
P (y|x)?W?w=1?w?2?(7)?L?
?t=L?j=1ht(sp, sj , r)?L?j=1ht(sp, sj , r)P (y|x)?T?t=1?t?2?
(8)The function L is concave, and can be effi-ciently maximized by standard techniques suchas stochastic gradient and limited memory quasi-Newton (L-BFGS) algorithms.
The parameters ?k?w and ?t are optimized iteratively until converge.5 Finding the Most Likely AssignmentsThe objective of inference is to find y?
={r?, s?}
= argmax{r,s} P (r, s|x) such that boths?
and r?
are optimized simultaneously.
Unfortu-nately, exact inference to this problem is generallyprohibitive, since it requires enumerating all pos-sible segmentation and corresponding relation as-signments.
Consequently, approximate inferencebecomes an alternative.We propose a new algorithm: collective it-erative classification (CIC) to perform approxi-mate inference to find the maximum a posteriori(MAP) segmentation and relation assignments ofour model in an iterative fashion.
The basic ideaof CIC is to decode every target hidden variablebased on the assigning labels of its sampled vari-ables, where the labels might be dynamically up-dated throughout the iterative process.
Collectiveclassification refers to the classification of rela-tional objects described as nodes in a graphicalstructure, as in our model.The CIC algorithm performs inference in twosteps, as shown in Algorithm 1.
The first step,bootstrapping, predicts an initial labelingassignment for a unlabeled sequence xi, giventhe trained model P (y|x).
The second stepis the iterative classification processwhich re-estimates the labeling assignment of xiseveral times, picking them in a sample set Sbased on initial assignment for xi.
Here we exploitthe sampling technique (Andrieu et al, 2003).The advantages of sampling are summarized asfollows.
Sampling stochastically enables us togenerate a wide range of inference situations, andthe samples are likely to be in high probability ar-eas, increasing our chances of finding the max-1403imum, thus leading to more robust and accurateperformance.
The CIC algorithm may convergeif none of the labeling assignments change dur-ing an iteration or a given number of iterations isreached.Noticeably, this inference algorithm is alsoused to efficiently compute the marginal probabil-ity P (y|x) during parameter estimation (the nor-malization constant Z(x) can also be calculatedvia approximation techniques).
As can be seen,this algorithm is simple to design, efficient andscales well w.r.t.
the size of data.6 Experiments6.1 DataOur data comes from Wikipedia2, the world?slargest free online encyclopedia.
This dataset con-sists of 1127 paragraphs from 441 pages from theonline encyclopedia Wikipedia.
We labeled 7740entities into 8 categories, yielding 1243 person,1085 location, 875 organization, 641 date, 1495year, 38 time, 59 number, and 2304miscellaneousnames.
This dataset alo contains 4701 relationinstances and 53 labeled relation types.
The 10most frequent relation types are job title, visited,birth place, associate, birth year, member of,birth day, opus, death year, and death day.
Notethat this compound IE task involving entity iden-tification and relation extraction is very challeng-ing, and modeling tight interactions between enti-ties and their relations is highly attractive.6.2 Feature SetAccurate entities enable features that are naturallyexpected to be useful to boost relation extraction.And a wide range of rich, overlapping featurescan be exploited in our model.
These featuresinclude contextual features, part-of-speech (POS)tags, morphological features, entity-level dictio-nary features, clue word features.
Feature con-junctions are also used.
In leveraging relation ex-traction to improve entity identification, we use acombination of syntactic, entity, keyword, seman-tic, and Wikipedia characteristic features.
Moreimportantly, our model can incorporate multiplemention features qw(?
), which are used to collect2http://www.wikipedia.org/Algorithm 1: Collective Iterative Classifica-tion InferenceInput: A unlabeled sequence xi and a trainedmodel P (y|x)Output: The set of predicted assignmentyi = {ri, si}// Bootstrappingforeach yi ?
Y doyi ?
argmaxyi P (yi|xi);end// Iterative ClassificationrepeatGenerate a sample set S based on initiallabel assignment yi for sequence xi;foreach si ?
S doAssign new label assignment tosample si;enduntil all labels have stabilized or a thresholdnumber of iterations have elapsed ;return yi = {ri, si}evidences from other occurrences of the same sec-ondary entities for consistent segmentation and re-lation labeling to the principal entity.
The featuresht(?)
capture deep dependencies between segmen-tations and relations, and they are natural and use-ful to enhance the performance.6.3 MethodologyWe perform four-fold cross-validation on thisdataset, and take the average performance.
Forperformance evaluation, we use the standard mea-sures of Precision (P), Recall (R), and F-measure(the harmonic mean of P and R: 2PRP+R ) for bothentity identification and relation extraction.
Weconduct holdout methodology for parameter tun-ing and optimization of our model.
We compareour approach with a series of linear-chain CRFs:CRF+CRF and a joint model DCRF (Sutton etal., 2007): dynamic probabilistic models com-bined with factored approach to multiple sequencelabeling.
CRF+CRF perform entity identificationand relation extraction separately.
Relation ex-traction is viewed as a sequence labeling problemin the second CRF.
All these models exploit stan-dard parameter learning and inference algorithms1404Table 1: Comparative performance of our model, CRF+CRF, and DCRF models for entity identifica-tion.Entities CRF+CRF DCRF Our modelP R F1 P R F1 P R F1person 75.33 83.22 79.08 75.96 83.82 79.70 82.91 84.26 83.58location 77.03 69.45 73.04 77.68 70.13 73.71 82.94 80.52 81.71organization 53.78 47.76 50.59 54.55 46.98 50.48 61.63 62.61 62.12date 98.54 97.53 98.03 97.98 95.22 96.58 98.90 96.24 97.55year 97.14 99.10 98.11 98.12 99.09 98.60 97.36 99.55 98.44time 60.00 20.33 30.37 50.00 25.33 33.63 100.0 25.00 40.00number 98.88 60.33 74.94 100.0 66.00 79.52 100.0 65.52 79.17miscellaneous 77.42 80.56 78.96 79.81 83.14 81.44 82.69 85.16 83.91Overall 89.55 88.70 89.12 90.98 90.37 90.67 93.35 93.37 93.36in our experiments.
To avoid over-fitting, penal-ization techniques on likelihood are performed.We also use the same set of features for all thesemodels.6.4 Experimental ResultsTable 1 shows the performance of entity identifi-cation and Table 2 shows the overall performanceof relation extraction 3, respectively.
Our modelsubstantially outperforms all baseline models onthe overall F-measure for entity identification, re-sulting in an relative error reduction of up to38.97% and 28.83% compared to CRF+CRF andDCRF, respectively.
For relation extraction, theimprovements on the F-measure over CRF+CRFand DCRF are 4.68% and 3.75%.
McNemar?spaired tests show that all improvements of ourmodel over baseline models are statistically sig-nificant.
These results demonstrate the meritsof our approach by capturing tight interactionsbetween entities and relations to explore mutualbenefits.
The pipeline model CRF+CRF per-forms entity identification and relation extractionindependently, and suffers from problems suchas error accumulation.
For example, CRF+CRFcannot extract the member of relation betweenthe secondary entity Republican and the princi-pal entity George W. Bush, since the organiza-tion name Republican is incorrectly labeled as amiscellaneous.
By modeling interactions betweentwo subtasks, enhanced performance is achieved,as illustrated by DCRF.
Unfortunately, traininga DCRF model with unobserved nodes (hiddenvariables) makes this approach difficult to opti-3Due to space limitation, we only present the overall per-formance and omit the performance for 53 relation types.Table 2: Comparative performance of our model,CRF+CRF, and DCRF models for relation extrac-tion.Model Precision Recall F-measureCRF+CRF 70.40 57.85 63.51DCRF 69.30 60.22 64.44Our model 72.57 64.30 68.19mize, as we will show below.The efficiency of different models is summa-rized in Table 3.
Compared to the pipeline modelCRF+CRF, the learning time of our model isonly a small constant factor slower.
Notably,our model is over orders of magnitude (approx-imately 15.7 times) faster than the joint modelDCRF.
The DCRF model uses loopy belief prop-agation (LBP) for approximate learning and infer-ence.
When the graph has large tree-width as inour case, the LBP algorithm in DCRF is ineffi-cient, and is slow to converge.
Using L-BFGSand the CIC approximate inference algorithms,both learning and decoding can be carried out ef-ficiently.Table 3: Efficiency comparison of different mod-els on learning time (sec.)
and inference time(sec.
).Model Learning time Inference timeCRF+CRF 2822.55 6.20DCRF 105993.00 127.50Our model 6733.69 62.75Table 4 compares our CIC inference with twostate-of-the-art inference approaches: Gibbs sam-pling (GS) (Geman and Geman, 1984) and theiterative classification algorithm (ICA) (Nevilleand Jensen, 2000) for our model.
The CIC infer-ence is shown empirically to help improve classi-1405Table 4: Comparative performance of different in-ference algorithms for our model on entity identi-fication and relation extraction.Entity Precision Recall F-measureGS 92.45 92.15 92.30ICA 92.19 91.98 92.08CIC 93.35 93.37 93.36Relation Precision Recall F-measureGS 71.22 63.29 67.02ICA 71.58 63.68 67.40CIC 72.57 64.30 68.19fication accuracy and robustness over these two al-gorithms.
When probability distributions are verycomplex or even unknown, the GS algorithm can-not be applied.
ICA iteratively infers the statesof variables given the current predicted labelingassignments of neighboring variables as observedinformation.
Prediction errors on labels may thenpropagate during the iterations and the algorithmwill then have difficulties to generalize correctly.We mention some recently published results re-lated to Wikipedia datasets (Note that it is difficultto compare with them strictly, since these resultscan be based on different experimental settings).Culotta et al (2006) used a data set with a 70/30split for training/testing and Nguyen et al (2007)used 5930 articles for training and 45 for testing,to perform relatione extraction from Wikipedia.And the obtained F-measures were 67.91 and37.76, respectively.
Yu et al (2009) proposedan integrated approach incorporating probabilis-tic graphical models with first-order logic to per-form relation extraction from encyclopedia arti-cles, with a F-measure of 65.66.
All these sys-tems assume that the golden-standard entities arealready known and they only perform relation ex-traction.
However, such assumption is not validin practice.
Notably, our approach deals with afairly more challenging problem involving bothentity identification and relation extraction, and itis more applicable to real-world IE tasks.7 Related WorkA number of previous researchers have takensteps toward joint models in NLP and informa-tion extraction, and we mention some recentlyproposed, closely related approaches here.
Rothand Yih (2007) considered multiple constraintsbetween variables from tasks such as named en-tities and relations, and developed a integer lin-ear programming formulation to seek an optimalglobal assignment to these variables.
Zhangand Clark (2008) employed the generalized per-ceptron algorithm to train a statistical model forjoint segmentation and POS tagging, and appliedmultiple-beam search algorithm for fast decoding.Toutanova et al (2008) presented a model captur-ing the linguistic intuition that a semantic argu-ment frame is a joint structure, with strong depen-dencies among the arguments.
Finkel and Man-ning (2009) proposed a discriminative feature-based constituency parser for joint named entityrecognition and parsing.
And Dahlmeier et al(2009) proposed a joint model for word sense dis-ambiguation of prepositions and semantic role la-beling of prepositional phrases.
However, most ofthe mentioned approaches are task-specific (e.g.,(Toutanova et al, 2008) for semantic role label-ing, and (Finkel and Manning, 2009) for parsingand NER), and they can hardly be applicable toother NLP tasks.
Since we capture rich and com-plex dependencies between subtasks via potentialfunctions in probabilistic graphical models, ourapproach is general and can be easily applied toa variety of NLP and IE tasks.8 Conclusion and Future WorkIn this paper, we investigate the compound IE taskof identifying entities and extracting relations be-tween entities in encyclopedia text.
And we pro-pose a unified framework based on undirected,conditionally-trained probabilistic graphical mod-els to perform all relevant subtasks jointly.
Moreimportantly, we propose a new algorithm: CIC,to enable approximate inference to find the MAPassignments for both segmentations and relations.As we shown, our modeling offers several advan-tages over previous models and provides a naturalformalism for this compound task.
Experimentalstudy exhibits that our model significantly outper-forms state-of-the-art models while also runningmuch faster than the joint models.
In addition, thesuperiority of the CIC algorithm is also discussedand compared.
We plan to improve the scalabilityof our approach and apply it to other real-worldproblems in the future.1406ReferencesChristophe Andrieu, Nando de Freitas, Arnaud Doucet, andMichael I. Jordan.
An introduction toMCMC for machinelearning.
Machine Learning, 50(1):5?43, 2003.Julian Besag.
Spatial interaction and the statistical analysisof lattice systems.
Journal of the Royal Statistical Society,36:192?236, 1974.Aron Culotta, Andrew McCallum, and Jonathan Betz.
Inte-grating probabilistic extraction models and data mining todiscover relations and patterns in text.
In Proceedings ofHLT/NAACL-06, pages 296?303, New York, 2006.Daniel Dahlmeier, Hwee Tou Ng, and Tanja Schultz.
Jointlearning of preposition senses and semantic roles ofprepositional phrases.
In Proceedings of EMNLP-09,pages 450?458, Singapore, 2009.Jenny Rose Finkel and Christopher D. Manning.
Jointparsing and named entity recognition.
In Proceedingsof HLT/NAACL-09, pages 326?334, Boulder, Colorado,2009.Jenny Rose Finkel, Christopher D. Manning, and Andrew Y.Ng.
Solving the problem of cascading errors: Ap-proximate Bayesian inference for linguistic annotationpipelines.
In Proceedings of EMNLP-06, pages 618?626,Sydney, Australia, 2006.Stuart Geman and Donald Geman.
Stochastic relaxation,Gibbs distributions, and the Bayesian restoration of im-ages.
IEEE Transitions on Pattern Analysis and MachineIntelligence, 6:721?741, 1984.Kristy Hollingshead and Brian Roark.
Pipeline iteration.
InProceedings of ACL-07, pages 952?959, Prague, CzechRepublic, 2007.Frank R. Kschischang, Brendan J. Frey, and Hans-AndreaLoeliger.
Factor graphs and the sum-product algorithm.IEEE Transactions on Information Theory, 47:498?519,2001.John Lafferty, Andrew McCallum, and Fernando Pereira.Conditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedings ofICML-01, pages 282?289, 2001.Lin Liao, Dieter Fox, and Henry Kautz.
Extracting placesand activities from GPS traces using hierarchical condi-tional random fields.
International Journal of RoboticsResearch, 26:119?134, 2007.Jennifer Neville and David Jensen.
Iterative classificationin relational data.
In Proceedings of the AAAI-2000Workshop on Learning Statistical Models from RelationalData, pages 42?49, 2000.Dat P. T. Nguyen, Yutaka Matsuo, and Mitsuru Ishizuka.
Re-lation extraction from Wikipedia using subtree mining.
InProceedings of AAAI-07, pages 1414?1420, Vancouver,British Columbia, Canada, 2007.Hoifung Poon and Pedro Domingos.
Joint inference in in-formation extraction.
In Proceedings of AAAI-07, pages913?918, Vancouver, British Columbia, Canada, 2007.Dan Roth and Wentau Yih.
Global inference for entity andrelation identification via a linear programming formula-tion.
In Lise Getoor and Ben Taskar, editors, Introductionto Statistical Relational Learning.
MIT Press, 2007.Sameer Singh, Karl Schultz, and Andrew Mccallum.
Bi-directional joint inference for entity resolution and seg-mentation using imperatively-defined factor graphs.
InProceedings of ECML/PKDD-09, pages 414?429, Bled,Slovenia, 2009.Charles Sutton and Andrew Mccallum.
Collective segmen-tation and labeling of distant entities in information ex-traction.
In Proceedings of ICML Workshop on StatisticalRelational Learning and Its Connections to Other Fields,2004.Charles Sutton and Andrew McCallum.
An introduction toconditional random fields for relational learning.
In LiseGetoor and Ben Taskar, editors, Introduction to StatisticalRelational Learning.
MIT Press, 2007.Charles Sutton, Andrew McCallum, and Khashayar Rohan-imanesh.
Dynamic conditional random fields: Factor-ized probabilistic models for labeling and segmenting se-quence data.
Journal of Machine Learning Research,8:693?723, 2007.Kristina Toutanova, Aria Haghighi, and Christopher D. Man-ning.
Joint learning improves semantic role labeling.
InProceedings of ACL-05, pages 589?596, 2005.Kristina Toutanova, Aria Haghighi, and Christopher D. Man-ning.
A global joint model for semantic role labeling.Computational Linguistics, 34:161?191, 2008.Kristina Toutanova.
Effective statistical models for syntacticand semantic disambiguation.
PhD thesis, Stanford Uni-versity, 2005.Martin J. Wainwright and Michael I. Jordan.
Graphical mod-els, exponential families, and variational inference.
Foun-dations and Trends in Machine Learning, 1:1?305, 2008.Xiaofeng Yu, Wai Lam, and Shing-Kit Chan.
A frameworkbased on graphical models with logic for chinese namedentity recognition.
In Proceedings of the Third Interna-tional Joint Conference on Natural Language Processing(IJCNLP-08), pages 335?342, Hyderabad, India, 2008.Xiaofeng Yu, Wai Lam, and Bo Chen.
An integrated dis-criminative probabilistic approach to information extrac-tion.
In Proceedings of CIKM-09, pages 325?334, HongKong, China, 2009.Xiaofeng Yu.
Chinese named entity recognition with cas-caded hybrid model.
In Proceedings of HLT/NAACL-07,pages 197?200, Rochester, New York, 2007.Yue Zhang and Stephen Clark.
Joint word segmentation andPOS tagging using a single perceptron.
In Proceedings ofACL-08, pages 888?896, Ohio, USA, 2008.Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, and Wei-Ying Ma.
2D conditional random fields for Web informa-tion extraction.
In Proceedings of ICML-05, pages 1044?1051, Bonn, Germany, 2005.1407
