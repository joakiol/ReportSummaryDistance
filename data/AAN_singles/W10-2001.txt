Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 1?8,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsUsing Sentence Type Information for Syntactic Category AcquisitionStella Frank (s.c.frank@sms.ed.ac.uk)Sharon Goldwater (sgwater@inf.ed.ac.uk)Frank Keller (keller@inf.ed.ac.uk)School of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB, UKAbstractIn this paper we investigate a new source ofinformation for syntactic category acquisition:sentence type (question, declarative, impera-tive).
Sentence type correlates strongly withintonation patterns in most languages; we hy-pothesize that these intonation patterns are avaluable signal to a language learner, indicat-ing different syntactic patterns.
To test this hy-pothesis, we train a Bayesian Hidden MarkovModel (and variants) on child-directed speech.We first show that simply training a separatemodel for each sentence type decreases perfor-mance due to sparse data.
As an alternative, wepropose two new models based on the BHMMin which sentence type is an observed variablewhich influences either emission or transitionprobabilities.
Both models outperform a stan-dard BHMM on data from English, Cantonese,and Dutch.
This suggests that sentence typeinformation available from intonational cuesmay be helpful for syntactic acquisition cross-linguistically.1 IntroductionChildren acquiring the syntax of their native languagehave access to a large amount of contextual informa-tion.
Acquisition happens on the basis of speech, andthe acoustic signal carries rich prosodic and intona-tional information that children can exploit.
A key taskis to separate the acoustic properties of a word from theunderlying sentence intonation.
Infants become attunedto the pragmatic and discourse functions of utterancesas signalled by intonation extremely early; in this theyare helped by the fact that intonation contours of childand infant directed speech are especially well differen-tiated between sentence types (Stern et al, 1982; Fer-nald, 1989).
Children learn to use appropriate intona-tional melodies to communicate their own intentions atthe one word stage, before overt syntax develops (Snowand Balog, 2002).It follows that sentence type information (whether asentence is declarative, imperative, or a question), assignaled by intonation, is readily available to childrenby the time they start to acquire syntactic categories.Sentence type also has an effect on sentence structurein many languages (most notably on word order), sowe hypothesise that sentence type is a useful cue forsyntactic category learning.
We test this hypothesis byincorporating sentence type information into an unsu-pervised model of part of speech tagging.We are unaware of previous work investigating theusefulness of this kind of information for syntacticcategory acquisition.
In other domains, intonation hasbeen used to identify sentence types as a means of im-proving speech recognition language models.
Specifi-cally, (Taylor et al, 1998) found that using intonationto recognize dialogue acts (which to a significant extentcorrespond to sentence types) and then using a special-ized language model for each type of dialogue act ledto a significant decrease in word error rate.In the remainder of this paper, we first present theBayesian Hidden Markov Model (BHMM; Goldwaterand Griffiths (2007)) that is used as the baseline modelof category acquisition, as well as our extensions tothe model, which incorporate sentence type informa-tion.
We then discuss the distinctions in sentence typethat we used and our evaluation measures, and finallyour experimental results.
We perform experiments oncorpora in four different languages: English, Spanish,Cantonese, and Dutch.
Our results on Spanish show nodifference between the baseline and the models incor-porating sentence type, possibly due to the small size ofthe Spanish corpus.
Results on all other corpora showa small improvement in performance when sentencetype is included as a cue to the learner.
These cross-linguistic results suggest that sentence type may be auseful source of information to children acquiring syn-tactic categories.2 BHMM Models2.1 Standard BHMMWe use a Bayesian HMM (Goldwater and Griffiths,2007) as our baseline model.
Like a standard trigramHMM, the BHMM assumes that the probability of tagti depends only on the previous two tags, and the proba-bility of word wi depends only on ti.
This can be writtenasti|ti?1 = t, ti?2 = t?,?(t,t?)
?
Mult(?(t,t?))
(1)wi|ti = t,?
(t) ?
Mult(?
(t)) (2)where ?(t,t?)
are the parameters of the multinomial dis-tribution over following tags given previous tags (t, t ?
)1and ?
(t) are the parameters of the distribution over out-puts given tag t. The BHMM assumes that these param-eters are in turn drawn from symmetric Dirichlet priorswith parameters ?
and ?, respectively:?(t,t?)|?
?
Dirichlet(?)
(3)?(t)|?
?
Dirichlet(?)
(4)Using these Dirichlet priors allows the multinomial dis-tributions to be integrated out, leading to the followingpredictive distributions:P(ti|t?i,?)
=C(ti?2, ti?1, ti)+?C(ti?2, ti?1)+T?
(5)P(wi|ti, t?i,w?i,?)
=C(ti,wi)+?C(ti)+Wti?
(6)where t?i = t1 .
.
.
ti?1, w?i = w1 .
.
.wi?1, C(ti?2, ti?1, ti)and C(ti,wi) are the counts of the trigram (ti?2, ti?1, ti)and the tag-word pair (ti,wi) in t?i and w?i, T is thesize of the tagset, and Wti is the number of word typesemitted by ti.Based on these predictive distributions, (Goldwa-ter and Griffiths, 2007) develop a Gibbs sampler forthe model, which samples from the posterior distri-bution over tag sequences t given word sequences w,i.e., P(t|w,?,?)?
P(w|t,?)P(t|?).
This is done by us-ing Equations 5 and 6 to iteratively resample each tagti given the current values of all other tags.1 The re-sults show that the BHMM with Gibbs sampling per-forms better than the standard HMM using expectation-maximization.
In particular, the Dirichlet priors in theBHMM constrain the model towards sparse solutions,i.e., solutions in which each tag emits a relatively smallnumber of words, and in which a tag transitions to fewfollowing tags.
This type of model constraint allowsthe model to find solutions which correspond to truesyntactic parts of speech (which follow such a sparse,Zipfian distribution), unlike the uniformly-sized clus-ters found by standard maximum likelihood estimationusing EM.In the experiments reported below, we use the Gibbssampler described by (Goldwater and Griffiths, 2007)for the BHMM, and modify it as necessary for our ownextended models.
We also follow (Goldwater and Grif-fiths, 2007) in using Metropolis-Hastings sampling forthe hyperparameters, which are inferred automaticallyin all experiments.
A separate ?
parameter is inferredfor each tag.2.2 BHMM with Sentence TypesWe wish to add a sentence type feature to each time-step in the HMM, signalling the current sentence type.We treat sentence type (s) as an observed variable, onthe assumption that it is observed via intonation or1Slight corrections need to be made to Equation 5 to ac-count for sampling tags from the middle of the sequencerather than from the end; these are given in (Goldwater andGriffiths, 2007) and are followed in our own samplers.ti?2 ti?1 tiwisi?
?NFigure 1: Graphical model representation of theBHMM-T, which includes sentence type as an ob-served variable on tag transitions (but not emissions).punctuation features (not part of our model), and thesefeatures are informative enough to reliably distinguishsentence types (as speech recognition tasks have foundto be the case, see Section 1).In the BHMM, there are two obvious ways that sen-tence type could be incorporated into the generativemodel: either by affecting the transition probabilitiesor by affecting the emission probabilities.
The first casecan be modeled by adding si as a conditioning variablewhen choosing ti, replacing line 1 from the BHMMdefinition with the following:ti|si = s, ti?1 = t, ti?2 = t?,?(t,t?)
?
Mult(?(s,t,t?))
(7)We will refer to this model, illustrated graphically inFigure 1, as the BHMM-T.
It assumes that the distribu-tion over ti depends not only on the previous two tags,but also on the sentence type, i.e., that different sen-tence types tend to have different sequences of tags.In contrast, we can add si as a conditioning variablefor wi by replacing line 2 from the BHMM withwi|si = s, ti = t,?
(t) ?
Mult(?
(s,t)) (8)This model, the BHMM-E, assumes that different sen-tence types tend to have different words emitted fromthe same tag.The predictive distributions for these models aregiven in Equations 9 (BHMM-T) and 10 (BHMM-E):P(ti|t?i,si,?)
=C(ti?2, ti?1, ti,si)+?C(ti?2, ti?1,si)+T?(9)P(wi|ti,si,?)
=C(ti,wi,si)+?C(ti,si)+Wti?
(10)Of course, we can also create a third new model,the BHMM-B, in which sentence type is used to con-dition both transition and emission probabilities.
Thismodel is equivalent to training a separate BHMM oneach type of sentence (with shared hyperparameters).Note that introducing the extra conditioning variablein these models has the consequence of splitting thecounts for transitions, emissions, or both.
The split dis-tributions will therefore be estimated using less data,which could actually degrade performance if sentencetype is not a useful variable.2Our prediction is that sentence type is more likelyto be useful as a conditioning variable for transitionprobabilities (BHMM-T) than for emission probabili-ties (BHMM-E).
For example, the auxiliary inversionin questions is likely to increase the probability ofthe AUX?
PRO transition, compared to declaratives.Knowing that the sentence is a question may also af-fect emission probabilities, e.g., it might increase theprobability the word you given a PRO and decrease theprobability of I; one would certainly expect wh-wordsto have much higher probability in wh-questions thanin declaratives.
However, many other variables also af-fect the particular words used in a sentence (princi-pally, the current semantic and pragmatic context).
Weexpect that sentence type plays a relatively small rolecompared to these other factors.
The ordering of tagswithin an utterance, on the other hand, is principallyconstrained by sentences type (especially in the shortand grammatically simple utterances found in child-directed speech).3 Sentence TypesWe experiment with a number of sentence-type cate-gories, leading to increasingly fine grained distinctions.The primary distinction is between questions (Q) anddeclaratives (D).
Questions are marked by punctuation(in writing) or by intonation (in speech), as well as byword order or other morpho-syntactic markers in manylanguages.Questions may be separated into categories, mostnotably wh-questions and yes/no-questions.
Many lan-guages (including several English dialects) have dis-tinct intonation patterns for wh- and yes/no-questions(Hirst and Cristo, 1998).Imperatives are a separate type from declaratives,with distinct word order and intonation patterns.Declaratives may be further subdivided into frag-ments and full sentences.
We define fragments as ut-terances without a verb (including auxiliary verbs).As an alternate sentence-level feature to sentencetype, we use length.
Utterances are classified accord-ing to their length, as either shorter or longer than av-erage.
Shorter utterances are more likely to be frag-ments and may have distinct syntactic patterns.
How-ever these patterns are likely to be less strong than inthe above type-based types.
In effect this condition isa pseudo-baseline, testing the effects of less- or non-informative sentence features on our proposed models.4 Evaluation MeasuresEvaluation of fully unsupervised part of speech taggingis known to be problematic, due to the fact that thepart of speech clusters found by the model are unla-beled, and do not automatically correspond to any ofthe gold standard part of speech categories.
We reportthree evaluation measures in our experiments, in orderto avoid the weaknesses inherent in any single measureand in an effort to be comparable to previous work.Matched accuracy (MA), also called many-to-oneaccuracy, is a commonly used measure for evaluatingunlabeled clusterings in part of speech tagging.
Eachunlabeled cluster is given the label of the gold categorywith which it shares the most members.
Given these la-bels, accuracy can be measured as usual, as the percent-age of tokens correctly labeled.
Note that multiple clus-ters may have the same label if several clusters matchthe same gold standard category.
This can lead to a de-generate solution if the model is allowed an unboundednumber of categories, in which each word is in a sepa-rate cluster.
In less extreme cases, it makes comparingMA across clustering results with different numbers ofclusters difficult.
Another serious issue with MA is the?problem of matching?
(Meila, 2007): matched accu-racy only evaluates whether or not the items in the clus-ter match the majority class label.
The non-matchingitems within a cluster might all be from a second goldclass, or they might be from many different classes.
In-tuitively, the former clustering should be evaluated asbetter, but matched accuracy is the same for both clus-terings.Variation of Information (VI) (Meila, 2007) is aclustering evaluation measure that avoids the match-ing problem.
It measures the amount of informationlost and gained when moving between two clusterings.More precisely:V I(C,K) = H(C)+H(K)?2I(C,K)= H(C|K)+H(K|C)A lower score implies closer clusterings, since eachclustering has less information not shared with theother: two identical clusterings have a VI of zero.
How-ever, VI?s upper bound is dependent on the maximumnumber of clusters in C or K, making it difficult to com-pare clustering results with different numbers of clus-ters.As a third, and, in our view, most informativemeasure, we use V-measure (VM; Rosenberg andHirschberg (2007)).
Like VI, VM uses the conditionalentropy of clusters and categories to evaluate cluster-ings.
However, it also has the useful characteristic ofbeing analogous to the precision and recall measurescommonly used in NLP.
Homogeneity, the precisionanalogue, is defined asV H = 1?H(C|K)H(C).VH is highest when the distribution of categorieswithin each cluster is highly skewed towards a smallnumber of categories, such that the conditional entropyis low.
Completeness (recall) is defined symmetricallyto VH as:VC = 1?H(K|C)H(K).VC measures the conditional entropy of the clusterswithin each gold standard category, and is highest ifeach category maps to a single cluster so that each3Eve ManchesterSentence type Counts |w| Counts |w|Total 13494 4.39 13216 4.23DTotal 8994 4.48 8315 3.52I 623 4.87 757 4.22F 2996 1.73 4146 1.51QTotal 4500 4.22 4901 5.44wh 2105 4.02 1578 4.64Short utts 5684 1.89 6486 1.74Long utts 7810 6.21 6730 6.64Table 1: Counts of sentence types in the Eve andManchester training set.
(Test and dev sets are approx-imately 10% of the size of training.)
|w| is the averagelength in words of utterances of this type.
D: declar-atives, I: imperatives, F: fragments, Q: questions, wh:wh-questions.model cluster completely contains a category.
The V-measure VM is simply the harmonic mean of VH andVC, analogous to traditional F-score.
Unlike MA andVI, VM is invariant with regards to both the number ofitems in the dataset and to the number of clusters used,and consequently it is best suited for comparing resultsacross different corpora.5 English experiments5.1 CorporaWe use the Eve corpus (Brown, 1973) and theManchester corpus (Theakston et al, 2001) from theCHILDES collection (MacWhinney, 2000).
The Evecorpus is a longitudinal study of a single US Ameri-can child from the age of 1.5 to 2.25 years, whereasthe Manchester corpus follows a cohort of 12 Britishchildren from the ages of 2 to 3.
Using both corporaensures that any effect is not due to a particular child,and is not specific to a type of English.From both corpora we remove all utterances spokenby a child; the remaining utterances are nearly exclu-sively child-directed speech (CDS).
We use the full Evecorpus and a similarly sized subset of the Manchestercorpus, consisting of the first 70 CDS utterances fromeach file.
Files from the chronological middle of eachcorpus are set aside for development and testing (Eve:file 10 for testing, 11 for dev; Manchester: file 17 fromeach child for testing, file 16 for dev).Both corpora have been tagged using the relativelyrich CHILDES tagset, which we collapse to a smallerset of thirteen tags: adjectives, adverbs, auxiliaries,conjunctions, determiners, infinitival-to, nouns, nega-tion, participles, prepositions, pronouns, verbs andother (communicators, interjections, fillers and thelike).
wh-words are tagged as adverbs (why,where,when and how, or pronouns (who and the rest).Table 1 show the sizes of the training sets, andthe breakdown of sentence types within them.
Eachsentence type can be identified using a distinguish-ing characteristic.
Sentence-final punctuation is used todifferentiate between questions and declaratives; wh-questions are then further differentiated by the pres-ence of a wh-word.
Imperatives are separated from thedeclaratives by a heuristic (since CHILDES does nothave an imperative verb tag): if an utterance includesa base verb within the first two words, without a pro-noun proceeding it (with the exception of you, as inyou sit down right now), the utterance is coded as animperative.
Fragments are also identified using the tagannotations, namely by the lack of a verb or auxiliarytag in an utterance.The CHILDES annotation guide specifies that thequestion mark is to be used with any utterance with ?fi-nal rising contour?, even if syntactically the utterancemight appear to be a declarative or exclamation.
Thequestion category consequently includes echo ques-tions (Finger stuck?)
and non-inverted questions (Youwant me to have it?
).5.2 Inference and Evaluation ProcedureUnsupervised models do not suffer from overfitting,so generally it is thought unnecessary to use separatetraining and testing data, with results being reportedon the entire set of input data.
However, there is stilla danger, in the course of developing a model, of over-fitting in the sense of becoming too finely attuned to aparticular set of input data.
To avoid this, we use sep-arate test and development sets.
The BHMM is trainedon (train+dev) or (train+test), but evaluation scores arecomputed based on the dev or test portions of the dataonly.
2We run the Gibbs sampler for 2000 iterations, withhyperparameter resampling and simulated annealing.Each iteration produces an assignment of tags to thetokens in the corpus; the final iteration is used for eval-uation purposes.
Since Gibbs sampling is a stochas-tic algorithm, we run all models multiple times (three,except where stated otherwise) and report average val-ues for all evaluation measures, as well as confidenceintervals.
We run our experiments using a variety ofsentence type features, ranging from the coarse ques-tion/declarative (Q/D) distinction to the full five types.For reasons of space we do not report all results here,instead confining ourselves to representative samples.5.3 BHMM-B: Type-specific Sub-ModelsWhen separate sub-models are used for each sen-tence type, as in the BHMM-B, where both transitionand emission probabilities are conditioned on sentencetype, the hidden states (tags) in each sub-model donot correspond to each other, e.g., a hidden state 9 inone sub-model is not the same state 9 in another sub-model.
Consequently, when evaluating the tagged out-put, each sentence type must be evaluated separately(otherwise the evaluation would equate declaratives-tag-9 with questions-tag-9).2The results presented in this paper are all evaluated onthe dev set; preliminary test set results on the Eve corpusshow the same patterns.4Model VM VC VH VI MAwh-questionsBHMM: 63.0 (1.0) 59.8 (0.4) 66.6 (1.8) 1.63 (0.03) 70.7 (2.7)BHMM-B: 58.7 (2.0) 58.2 (2.1) 59.2 (2.0) 1.74 (0.09) 59.7 (2.0)Other QuestionsBHMM: 65.6 (1.4) 62.7 (1.3) 68.7 (1.5) 1.62 (0.06) 74.5 (0.5)BHMM-B: 64.4 (3.6) 62.6 (4.4) 66.2 (2.8) 1.65 (0.19) 70.8 (2.5)DeclarativesBHMM: 60.9 (1.3) 58.7 (1.1) 63.3 (1.6) 1.84 (0.06) 73.5 (0.8)BHMM-B: 58.0 (1.2) 55.5 (1.1) 60.7 (1.5) 1.99 (0.06) 69.0 (1.5)Table 2: Results for BHMM-B on W/Q/D sentence types (dev set evaluation) in the Manchester corpus, comparedto the standard BHMM.
The confidence interval is indicated in parentheses.
Note that lower VI is better.Model VM VC VH VI MABHMM: 59.4 (0.2) 56.9 (0.2) 62.3 (0.2) 1.96 (0.01) 72.2 (0.2)Q/D: 61.2 (1.2) 58.6 (1.2) 64.0 (1.4) 1.88 (0.06) 72.1 (1.5)W/Q/D: 61.0 (1.7) 59.0 (1.5) 63.0 (2.0) 1.86 (0.08) 69.6 (2.2)F/I/D/Q/W: 61.7 (1.7) 58.9 (1.8) 64.8 (1.6) 1.80 (0.09) 70.5 (1.3)Table 3: Results for BHMM-E on the Eve corpus (dev set evaluation), compared to the standard BHMM.
Theconfidence interval is indicated in parentheses.Table 2 shows representative results for the W/Q/Dcondition on the Manchester corpus, separated into wh-questions, other questions, and declaratives.
For eachsentence type, the BHMM-B performs significantlyworse than the BHMM.
The wh-questions sub-model,which is trained on the smallest subset of the input cor-pus, performs the worst across all measures except VI.This suggests that lack of data is why these sub-modelsperform worse than the standard model.5.4 BHMM-E: Type-specific EmissionsHaving demonstrated that using entirely separate sub-models does not improve tagging performance, we turnto the BHMM-E, in which emission probability distri-butions are sentence-type specific, but transition prob-abilities are shared between all sentence types.The results in Table 3 show that BHMM-E does re-sult in slightly better tagging performance as evaluatedby VI (lower VI is better) and VM and its components.Matched accuracy does not capture this same trend.
In-specting the clusters found by the model, we find thatclusters for the most part do match gold categories.
Thetokens that do not fall into the highest matching goldcategories are not distributed randomly, however; forinstance, nouns and pronouns often end up in the samecluster.
VI and VM capture these secondary matcheswhile MA does not.
Some small gold categories (e.g.the single word infinitival-to and negation-not cate-gories) are often merged by the model into a singlecluster, with the result that MA considers nearly halfthe cluster as misclassified.The largest increase in performance with regards tothe standard BHMM is obtained by adding the distinc-tion between declaratives and questions.
Thereafter,adding the wh-question, fragment and imperative sen-tence types does not worsen performance, but also doesnot significantly improve performance on any measure.5.5 BHMM-T: Type-specific TransitionsLastly, the BHMM-T shares emission probabilitiesamong sentence types and uses sentence type specifictransition probabilities.Results comparing the standard BHMM with theBHMM-T with sentence-type-specific transition prob-abilities are presented in Table 4.
Once again, VMand VI show a clear trend: the models using sen-tence type information outperform both the standardBHMM and models splitting according to utterancelength (shorter/longer than average).
MA shows no sig-nificant difference in performance between the differ-ent models (aside from clearly showing that utterancelength is an unhelpful feature).
The fact that the MAmeasure shows no clear change in performance is likelya fault of the measure itself; as explained above, VI andVM take into account the distribution of words withina category, which MA does not.As with the BHMM-E, the improvements to VM andVI are obtained by distinguishing between questionsand declaratives, and then between wh- and other ques-tions.
Both of these distinctions are marked by intona-tion in English.
In contrast, distinguishing fragmentsand imperatives, which are less easily detected by in-tonation, provides no obvious benefit in any case.
Us-ing sentence length as a feature degrades performanceconsiderably, confirming that improvements in perfor-mance are due to sentence types capturing useful infor-mation about the tagging task, and not simply due tosplitting the input in some arbitrary way.One reason for the improvement when adding thewh-question type is that the models are learning toidentify and cluster the wh-words in particular.
If weevaluate the wh-words separately, VM rises from 32.35Model VM VC VH VI MAEveBHMM: 59.4 (0.2) 56.9 (0.2) 62.3 (0.2) 1.96 (0.01) 72.2 (0.2)Q/D: 60.9 (0.5) 58.3 (0.4) 63.7 (0.6) 1.89 (0.02) 72.7 (0.3)W/Q/D: 62.5 (1.2) 60.0 (1.3) 65.2 (1.0) 1.81 (0.06) 72.9 (0.8)F/I/D/Q/W: 62.2 (1.5) 59.5 (1.6) 65.2 (1.3) 1.77 (0.08) 71.5 (1.4)Length: 57.9 (1.2) 55.3 (1.1) 60.7 (1.3) 2.04 (0.06) 69.7 (2.0)ManchesterBHMM: 60.2 (0.9) 57.6 (0.9) 63.1 (1.0) 1.92 (0.05) 72.1 (0.7)Q/D: 61.5 (0.7) 59.2 (0.6) 63.9 (0.9) 1.84 (0.03) 71.6 (1.5)W/Q/D: 62.7 (0.2) 60.6 (0.2) 65.0 (0.3) 1.78 (0.01) 71.2 (0.6)F/I/D/Q/W: 62.5 (0.4) 60.3 (0.5) 64.9 (0.4) 1.79 (0.02) 71.3 (0.9)Length: 58.1 (0.7) 55.6 (0.8) 60.8 (0.6) 2.02 (0.04) 71.0 (0.6)Table 4: Results on the Eve and Manchester corpora for the various sentence types in the BHMM and BHMM-Tmodels.
The confidence interval is indicated in parentheses.in the baseline BHMM to 41.5 in the W/Q/D conditionwith the BHMM-T model and 46.8 with the BHMM-E model.
Performance for the non-wh-words also im-proves in the W/Q/D condition, albeit less dramati-cally: from 61.1 in the baseline BHMM to 63.6 withBHMM-T and 62.0 with BHMM-E.
The wh-questiontype enables the models to pick up on the defining char-acteristics of these sentences, namely wh-words.We predicted the sentence-type specific transitionprobabilities in the BHMM-T to be more useful thanthe sentence-type specific emission probabilities in theBHMM-E.
The BHMM-T does perform slightly betterthan the BHMM-E, however, the effect is small.
Wordor tag order may be the most overt difference betweenquestions and declaratives in English, but word choice,especially the use of wh-words varies sufficiently be-tween sentence types for sentence-type specific emis-sion probabilities to be equally useful.6 Crosslinguistic ExperimentsIn the previous section we found that sentence typeinformation improved syntactic categorisation in En-glish.
In this section, we evaluate the BHMM?s perfor-mance on a range of languages other than English, andinvestigate whether sentence type information is use-ful across languages.
To our knowledge this is the firstapplication of the BHMM to non-English data.Nearly all human languages distinguish betweenyes/no-questions and declaratives in intonation; ques-tions are most commonly marked by rising intonation(Hirst and Cristo, 1998).
wh-questions do not alwayshave a distinct intonation type, but they are signalledby the presence of members of the small class of wh-words.The CHILDES collection includes tagged corporafor Spanish and Cantonese: the Ornat corpus (Ornat,1994) and the Lee Wong Leung (LWL) corpus (Leeet al, 1994) respectively.
To cover a greater variety ofword order patterns, a Dutch corpus of adult dialogue(not CDS) is also tested.
We describe each corpus inturn below; Table 5 describes their relative sizes.Total Ds all Qs wh-QsSpanish 8759 4825 3934 1507|w| 4.29 4.41 4.14 3.72Cantonese 12544 6689 5855 2287|w| 4.16 3.85 4.52 4.80Dutch 8967 7812 1155 363|w| 6.16 6.19 6.00 7.08Table 5: Counts of sentence types in the training setsfor Spanish.
Cantonese and Dutch.
(Test and dev setsare approximately 10% of the size of training.)
|w| isthe average length in words of utterances of this type.D: declaratives, Qs: questions, wh-Qs: wh-questions.6.1 SpanishThe Ornat corpus is a longitudinal study of a singlechild between the ages of one and a half and nearlyfour years, consisting of 17 files.
Files 08/09 are usedtesting/development.We collapse the Spanish tagset used in the Ornat cor-pus in a similar fashion to the English corpora.
Thereare 11 tags in the final set: adjectives, adverbs, con-juncts, determiners, nouns, prepositions, pronouns, rel-ative pronouns, auxiliaries, verbs, and other.Spanish wh-questions are formed by fronting thewh-word (but without the auxiliary verbs added inEnglish); yes/no-questions involve raising the mainverb (again without the auxiliary inversion in English).Spanish word order in declaratives is generally freerthan English word order.
Verb- and object-fronting ismore common, and pronouns may be dropped (sinceverbs are marked for gender and number).6.2 CantoneseThe LWL corpus consists of transcripts from a set ofchildren followed over the course of a year, totalling128 files.
The ages of the children are not matched, butthey range between one and three years old.
Our train-ing set consists of the first 500 utterances of all train-ing files, in order to create a data set of similar size asthe other corpora used.
Files from children aged two6years and five months are used as the test set; files fromtwo years and six months are the development set files(again, the first 500 utterances from each of these makeup the test/dev corpus).The tagset used in the LWL is larger than the En-glish corpus.
It consists of 20 tags: adjective, ad-verb, aspectual marker, auxiliary or modal verb, clas-sifier, communicator, connective, determiners, genitivemarker, preposition or locative, noun, negation, pro-nouns, quantifiers, sentence final particle, verbs, wh-words, foreign word, and other.
We remove all sen-tences that are encoded as being entirely in English butleave single foreign, mainly English, words (generallynouns) in a Cantonese context.Cantonese follows the same basic SVO word orderas English, but with a much higher frequency of topic-raising.
Questions are not marked by different word or-der.
Instead, particles are inserted to signal questioning.These particles can signal either a yes/no-question or awh-question; in the case of wh-questions they replacethe item being questioned (e.g., playing-you what?
),without wh-raising as in English or Spanish.
Despitethe use of tones in Cantonese, questions are markedwith rising final intonation.6.3 DutchThe Corpus of Spoken Dutch (CGN) contains Dutchspoken in a variety of settings.
We use the ?spontaneousconversation?
component, consisting of 925 files, sinceit is the most similar to CDS.
However, the utterancesare longer, and there are far fewer questions, especiallywh-questions (see Table 5).The corpus does not have any meaningful timeline,so we designated all files with numbers ending in 0 astest files and files ending in 9 as dev files.
The first60 utterances from each file were used, to create train-ing/test/dev sets similar in size to the other corpora.The coarse CGN tagset consists of 11 tags, whichwe used directly: adjective, adverb, conjunction, deter-miner, interjection, noun, number, pronoun/determiner,preposition, and verb.Dutch follows verb-second word order in mainclauses and SOV word order in embedded clauses.Yes/no-questions are created by verb-fronting, as inSpanish.
wh-questions involve a wh-word at the begin-ning of the utterance followed by the verb in secondposition.6.4 ResultsWe trained standard BHMM, BHMM-T and BHMM-Emodels in the same manner as with the English corpora.Given the poor performance of the BHMM-B, we didnot test it here.Due to inconsistent annotation and lack of familiar-ity with the languages we tested only two sentence typedistinctions, Q/D and W/Q/D.
Punctuation was usedto distinguish between questions and declaratives.
wh-questions were identified by using a list of wh-wordsfor Spanish and Dutch; for Cantonese we relied on thewh-word tag annotation.Results are shown in Table 6.
Since the corporaare different sizes and use tagsets of varying sizes, VIand MA results are not comparable between corpora.VM (and VC and VH) are more robust, but even socross-corpora comparisons should be made carefully.The English corpora VM scores are significantly higher(around 10 points higher) than the non-English corporascores.In Cantonese and Dutch, the W/Q/D BHMM-Tmodel performs best; in both cases significantly bet-ter than the BHMM.
In Cantonese, the separation ofwh-questions improves tagging significantly in both theBHMM-T and BHMM-E models, whereas simply sep-arating questions and declaratives helps far less.
Inthe Dutch corpus, wh-questions improved only in theBHMM-T, not in the BHMM-E.The Spanish models have higher variance, due to thesmall size of the corpus.
Due to the high variance, thereare no significant differences between any of the con-ditions; it is also difficult to spot a trend.7 Future WorkWe have shown sentence type information to be use-ful for syntactic tagging.
However, the BHMM-E andBHMM-T models are successful in part however be-cause they also share information as well as split it;the completely split BHMM-B does not perform well.Many aspects of tagging do not change significantlybetween sentence types.
Within a noun phrase, the or-dering of determiners and nouns is the same whetherit is in a question or an imperative, and to a large ex-tent the determiners and nouns used will be the sameas well.
Learning these patterns over as much input aspossible is essential.
Therefore, the next step in this lineof work will be to add a general (corpus-level) modelalongside type-specific models.
Ideally, the model willlearn when to use the type-specific model (when tag-ging the beginning of questions, for instance) and whento use the general model (when tagging NPs).
Such amodel would make use of sentence-type information ina better way, hopefully leading to further improvementsin performance.
A further, more sophisticated modelcould learn the useful sentence types distinctions auto-matically, perhaps forgoing the poorly performing im-perative or fragment types we tested here in favor of amore useful type we did not identify.8 ConclusionsWe set out to investigate a hitherto unused source ofinformation for models of syntactic category learning,namely intonation and its correlate, sentence type.
Weshowed that this information is in fact useful, and in-cluding it in a Bayesian Hidden Markov Model im-proved unsupervised tagging performance.We found tagging performance increases if sentencetype information is used to generate either transitionprobabilities or emission probabilities in the BHMM.However, we found that performance decreases if sen-tence type information is used to generate both transi-7Model VM VC VH VI MASpanishBHMM: 49.4 (1.8) 47.2 (1.9) 51.8 (1.8) 2.27 (0.09) 61.5 (2.1)BHMM-E Q/D: 49.4 (1.5) 47.0 (1.4) 52.1 (1.7) 2.28 (0.06) 60.9 (2.6)BHMM-E W/Q/D: 48.7 (2.5) 46.4 (2.4) 51.2 (2.6) 2.31 (0.11) 60.2 (3.0)BHMM-T Q/D: 49.0 (1.7) 46.7 (1.6) 51.6 (1.7) 2.30 (0.07) 60.9 (2.5)BHMM-T W/Q/D: 49.5 (2.5) 47.2 (2.3) 52.1 (2.8) 2.27 (0.11) 61.0 (3.0)CantoneseBHMM: 49.4 (0.8) 44.5 (0.7) 55.4 (1.0) 2.60 (0.04) 67.2 (1.0)BHMM-E Q/D: 50.7 (1.6) 45.4 (1.5) 57.5 (1.7) 2.55 (0.09) 69.0 (1.0)BHMM-E W/Q/D: 52.3 (0.3) 46.9 (0.3) 59.3 (0.4) 2.46 (0.02) 69.4 (0.9)BHMM-T Q/D: 50.3 (0.9) 45.0 (0.9) 57.0 (1.0) 2.57 (0.05) 68.4 (0.8)BHMM-T W/Q/D: 52.2 (0.8) 46.8 (0.9) 59.1 (0.7) 2.47 (0.05) 69.9 (1.9)DutchBHMM: 48.4 (0.7) 47.1 (0.8) 49.7 (0.7) 2.38 (0.04) 62.3 (0.3)BHMM-E Q/D: 48.4 (0.4) 47.3 (0.4) 49.7 (0.5) 2.37 (0.02) 62.2 (0.3)BHMM-E W/Q/D 47.6 (0.3) 46.3 (0.4) 48.8 (0.2) 2.41 (0.02) 61.2 (1.1)BHMM-T Q/D: 47.9 (0.5) 46.7 (0.4) 49.1 (0.5) 2.40 (0.02) 61.5 (0.4)BHMM-T W/Q/D: 49.6 (0.2) 48.5 (0.2) 50.8 (0.2) 2.31 (0.10) 64.1 (0.2)Table 6: Results for BHMM, BHMM-E, and BHMM-T on non-English corpora.tion and emission probabilities (which is equivalent totraining a separate BHMM for each sentence type).To test the generality of our findings, we carried out aseries of cross-linguistic experiments, integrating sen-tence type information in unsupervised tagging mod-els for Spanish, Cantonese, and Dutch.
The results forCantonese and Dutch mirrored those for English, show-ing a small increase in tagging performance for modelsthat included sentence type information.
For Spanish,no improvement was observed.ReferencesRoger Brown.
1973.
A first language: The earlystages.
Harvard University Press.Anne Fernald.
1989.
Intonation and communicativeintent in mothers?
speech to infants: Is the melodythe message?
Child Development, 60(6):1497?1510.Sharon Goldwater and Thomas L. Griffiths.
2007.A fully Bayesian approach to unsupervised part-of-speech tagging.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Lin-guistics.Daniel Hirst and Albert Di Cristo, editors.
1998.
Into-nation systems: a survey of twenty languages.
Cam-bridge University Press.Thomas H.T.
Lee, Colleen H Wong, Samuel Leung,Patricia Man, Alice Cheung, Kitty Szeto, and CathyS P Wong.
1994.
The development of grammaticalcompetence in cantonese-speaking children.
Techni-cal report, Report of RGC earmarked grant 1991-94.Brian MacWhinney.
2000.
The CHILDES Project:Tools for Analyzing Talk.
Lawrence Erlbaum Asso-ciates, Mahwah, NJ.Marina Meila.
2007.
Comparing clusterings ?
an in-formation based distance.
Journal of MultivariateAnalysis, 98:873?895.Susana Lopez Ornat.
1994.
La adquisicion de lalengua espagnola.
Siglo XXI, Madrid.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clus-ter evaluation measure.
In EMNLP.David Snow and Heather Balog.
2002.
Do chil-dren produce the melody before the words?
A re-view of developmental intonation research.
Lingua,112:1025?1058.Daniel N. Stern, Susan Spieker, and Kristine MacK-ain.
1982.
Intonation contours as signals in mater-nal speech to prelinguistic infants.
DevelopmentalPsychology, 18(5):727?735.Paul A. Taylor, S. King, S. D. Isard, and H. Wright.1998.
Intonation and dialogue context as constraintsfor speech recognition.
Language and Speech,41(3):493?512.Anna Theakston, Elena Lieven, Julian M. Pine, andCaroline F. Rowland.
2001.
The role of per-formance limitations in the acquisition of verb-argument structure: an alternative account.
Journalof Child Language, 28:127?152.8
