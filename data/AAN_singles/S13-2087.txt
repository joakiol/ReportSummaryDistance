Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 525?529, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguisticssielers : Feature Analysis and Polarity Classification of Expressions fromTwitter and SMS DataHarshit Jain, Aditya Mogadala and Vasudeva VarmaSearch and Information Extraction Lab, IIIT-HHyderabadIndiaharshit.jain@research.iiit.ac.in, aditya.m@research.iiit.ac.in,vv@iiit.ac.inAbstractIn this paper, we describe our system for theSemEval-2013 Task 2, Sentiment Analysis inTwitter.
We formed features that take into ac-count the context of the expression and take asupervised approach towards subjectivity andpolarity classification.
Experiments were per-formed on the features to find out whetherthey were more suited for subjectivity or po-larity Classification.
We tested our model forsentiment polarity classification on Twitter aswell as SMS chat expressions, analyzed theirF-measure scores and drew some interestingconclusions from them.1 IntroductionIn recent years there has been a huge growth in pop-ularity of vaious social media microblogging plat-forms like Twitter.
Users freely share their personalopinions on various events and entities on these plat-forms.
However, while character constraints makesure the opinions are short and to the point, they alsocontribute to the noisy nature of Twitter data.The contextual polarity of the phrase in which aparticular instance of a word appears may be quitedifferent from the word?s prior polarity.
Positivewords are used in phrases expressing negative sen-timents, or vice versa.
Also, quite often words thatare positive or negative out of context are neutral incontext, meaning they are not even being used to ex-press a sentiment.
This is evident from the exampleof underlined phrase in the following tweet:Lana Del Rey at Hammersmith Apollo inMay...Very badly want ticketsIn a technique with large lexicon of words markedwith their prior polarity, badly would have a negativescore making the whole sentence with negative sen-timent.
Even if we perform phrase-level analysis forthe phrase ?Very badly?, Very only acts as an intensi-fier for badly and the whole sentence is still markednegative.
It?s only when we look further from theunderlined phrase that we realize that ?Very badly?in the context of wanting something shows positivesentiment.Early work on sentiment analysis is based ondocument-level analysis of reviews (Pang, B., andLee, L., 2004).
This approach isn?t feasible for mi-croblogging data due to the extremely small size ofindividual documents.
The results on the effective-ness of part-of-speech features are mixed.
Whilemost regard POS features helpful in subjectivityclassification (Barbosa, L. and Feng, J., 2010), somereport very insignificant improvement on using them(Kouloumpis, E., Wilson, T. and Moore, J., 2011).However, most phrase-level approaches began witha large lexicon of words marked with their prior po-larity (Kim, S. M., and Hovy, E., 2004; Hu, M., andLiu, B, 2004).
Wilson, Wiebe and Hoffman (2005)sought to include contextual polarity in the foray byusing various dependency relation based features forsubjectivity and polarity classification.
Our goal isto perform contextual sentiment polarity classifica-tion in the domain of noisy expressions from tweetsand SMS messages.2 DataWe use the annotated Twitter expressions providedby SemEval-2013 Task 2 (Wilson et al 2013) or-525ganizers for training our model.
Each instance ofthe data contains an expression and its parent tweet.There are a total of 24939 tweet expressions in thetraining dataset and they are annotated into fourclasses:?
Objective: Expressions carrying no opinion bythemselves or even in the context of their parenttweet.?
Positive: Expressions carrying positive senti-ment in the context of the parent tweet.?
Negative: Expressions carrying negative senti-ment in the context of the parent tweet.?
Neutral: Expressions carrying prior subjectiv-ity but are rendered objective in the context oftheir parent tweet.Two separate lexicons for emoticons and interjec-tions having non-zero prior polarities were created.47 Subjective emoticons were extracted from train-ing data as well as from various popular chat ser-vices.
212 Subjective interjections were extractedfrom training data as well from Wiktionary1.We test our trained model on two separate testdatasets provided by SemEval-2013 Task 2 organiz-ers, 1) Twitter expressions and 2) SMS expressions.2.1 PreprocessingData preprocessing consists of three steps: 1) To-kenization, 2) Part-of-Speech (POS) tagging, and3) Normalization.
For the first two steps we useTwitter NLP and Part-of-Speech Tagging system(Gimpel, K., et al 2011).
It is a Tokenizer andPOS Tagger made for Twitter dataset and thuscontains separate POS tags for hash-tags(#), at-mention(@), URLs and E-Mail addresses(U) andemoticons(E).
The POS Tagger identifies commonabbreviations and tags them accordingly.
We useTwitter NLP and Part-of-Speech Tagging system forthe SMS expressions too due to similar noisy na-ture of SMS data.
For the normalization process,all upper case letters are converted to lower case,and instances of repeated characters are replacedby a repetition of two characters.
This is done1http://en.wiktionary.org/wiki/Category:English_interjectionsso that existing legal words having characters re-peating two times aren?t harmed.
#hash-tags arestripped of the # character and then treated as a nor-mal word/phrase, at-mention(@) denote the name ofa person/organization and thus they are treated asproper noun and since URLs don?t carry any senti-ment, they are ignored in the expression.
We expectthe normalization process to aid in forming betterfeatures and in turn improving the performance ofthe system as a whole.3 FeaturesWe use three types of features for our classificationexperiments,?
Phrase Prior Polarity Features?
POS Tag Pattern Features?
Noisy data specific FeaturesBoth Phrase Prior Polarity and POS Tag features arecomputed for the expression to be analyzed as wellas, if available, two words 2 before and after the ex-pression.3.1 Phrase Prior Polarity FeatureEvery expression in the dataset is represented byits aggregate positive and negative polarity score.Senti-Wordnet (Baccianella, S., Esuli, A., and Se-bastiani, F., 2010), Emoticon Lexicon and an Inter-jection Lexicon are used to calculate these prior po-larities.
Bigrams and trigrams are identified by theirpresence in Senti-Wordnet.
For each identified un-igram, bigram or trigram, we compute the mean ofall its subjective wordnet sense scores under the POStag assigned to it.
If a unigram word isn?t present inSenti-Wordnet, its stemmed3 form is searched keep-ing the original POS Tag.
We perform negation de-tection by enabling a flag whenever a word occur-ring in negation list appears.
The negation list con-sists of words like no, not, never, etc, as well allwords ending with -n?t.
Negation words act as po-larity reversers, for e.g., consider the following ex-pression:?not so sure?.
In a simple bag of words ap-proach, ?not so sure?
wouldn?t be classified as neg-ative due to the presence of sure.
To overcome this,2The figure of two words was reached empirically upon try-ing various lengths.3The stemmer used is Snowball Stemmer for English.526prior polarities of all words are reversed on the oc-currence of a negation word.
Some negation wordssuch as no, not, never, also carry their own negativescore (-1), in case no subjective word is found in theexpression, their individual negative score is addedto the aggregate prior polarity of the expression.
Ad-jectives and adverbs are treated as polarity shifters.They either shift the prior polarities of nouns andverbs, or in case of objective nouns and verbs, con-tribute their own prior polarities to the expression,e.g., ?exceedingly slow?, ?little truth?, ?amazingcar?, etc.On encountering any emoticon or interjection inthe expression that is present in our lexicon, its cor-responding score is added to the aggregate prior po-larity of the expression.Finally, both positive and negative prior polaritiesof the expression are normalized by the number ofwords in the expression after tokenization.3.2 POS Tag Pattern FeatureBoth Tweets and SMS messages are extremely short.Twitter is a social microblogging platform havingjust 140 character space for a tweet while SMS mes-sages have little word length due to typing con-straints on a mobile device.
All the above factorscontribute to the noisiness of data.
Hence, it isn?tenough to find prior polarities of n-grams occurringin the expression.
We thus formed a heuristic tech-nique of using POS tag patterns as features.
POS tagpatterns carry information regarding POS tags com-bined with the location of their occurrence in the ex-pression as a feature.
For e.g., the POS tag patternfor the expression ?not so sure?
in the tweet@thehuwdavies you think the Boro willbeat Swansea?
I?m not so sure, Decem-ber/January is when we implodewill be RRA, where R = Adverb and A = Adjective.3.3 Noisy data specific FeaturesInterjections and emoticons are useful indicators ofsubjectivity in a sentence.
Even if many interjectionsor emoticons don?t carry a defininte sentiment polar-ity, they do indicate that some sort of opinion fromthe user is available in the tweet or sms.
Some ex-amples of interjections and emoticons with no fixedprior polarity are, ?wow?, ?oh my god?, ?
:-o?, etc.4 Experiments and ResultsOur goal for these experiments is two-fold.
First,we want to evaluate the effectiveness of our featureswhen using them for subjectivity classification ascompared to sentiment polarity classification.
Sec-ond, we want to evaluate and compare the perfor-mance of our learnt model when tested upon Twitterand SMS expression data.
We use Naive Bayes clas-sifier in Weka (Hall, M., et al 2009) as the learningalgorithm.Feature Analysis between Subjectivity and Polar-ity Classification For our first set of experiments,we re-label all positive, negative and neutral expres-sions as subjective for subjectivity classification inthe training dataset.
For polarity classification weremove all objective expressions from the trainingdataset and perform 3-way classification betweenpositive, negative and neutral expressions.
In bothcases we perform 10-fold cross validation on thetraining dataset.
For subjectivity classification wehave 24939 tweet expressions with 15565 objectiveand 9374 subjective expressions.
Subjective expres-sions contain 5787 positive, 3131 negative and 456neutral expressions.
Table 1 shows the accuracy ofsubjectivity and sentiment polarity classification re-sults and improvement due to each feature.It is fairly evident from Table 1 that phrase priorpolarity features are equally important for both sub-jectivity and sentiment polarity classification.
Thesame however, doesn?t completely hold true for theother two feature types.
While POS Tag patternfeatures provide an improvement of 1.89% in sub-jectivity classification accuracy, they only provide a0.64% increase in accuracy in polarity classification.Many inferences can be drawn from this result anda deeper analysis is required on POS tag patterns toprove that this wasn?t a mere aberration.
Emoticonand interjection feature too give lower improvementin accuracies during sentiment polarity classifica-tion (0.44%) as compared to subjectivity classifica-tion (0.83%).
This, however, is expected since mostcommon emoticons and interjections with prior po-larities are already covered in the total score of theexpression.
Thus, the noisy data based binary fea-tures have significant contribution only when theemoticons and interjections aren?t present in the lex-icon.
This implies that these binary features only527Features Subjectivity Polarityf1 86.58 72.93f1 + f2 88.47 73.57f1 + f2 + f3 89.3 74.01f1 + f2 + f3 - context 84.38 72.25f1 : Phrase Prior Polarity Featuresf2 : POS Tag Pattern Featuresf3 : Noisy Data Specific Featurescontext : Phrase Prior Polarity and POS Tagpattern features defined for 2 wordsbefore and after the expressionTable 1: Accuracies for all three features used forSubjectivity and Sentiment Polarity Classification.hint towards the expression being subjective.
Thecontext features, i.e., phrase prior polarity and POStag pattern features defined for 2 words before andafter the expression also carry more significance dur-ing subjectivity classification than in sentiment po-larity classification.Polarity Classification comparison for Twitterand SMS expression data For the second set ofexperiments comparing the performance of polarityclassification in Twitter expressions and SMS ex-pressions, we use the polarity classification modellearnt in the above experiment.
Tables 2(a) and 2(b)shows the precision, recall and F-measure scores forboth Twitter and SMS expressions.The polarity classification accuracies for Twitterand SMS expressions are 74.76% and 70.82%, re-spectively.
Closer inspection of test data shows thatSMS expressions exhibit more aggressive usage ofabbreviations and slangs and are in general noisierthan Twitter expressions.
This is probably due to thefact that typing on a cellphone is more cumbersomethan on a keyboard.
The quantitative distribution ofpositive, negative and neutral classes in both datasetsaffects the F-measure scores of individual classes.This is evident from the difference in positive andnegative F-measures of Twitter and SMS expres-sions data.
In both datasets, neutral class F-measureis extremely low.
This is partially expected due tothe low quantity of neutral class expressions in Twit-ter (160/4435) and SMS (159/2334) data.
Still, itClass Precision Recall F-measurepositive 0.8120 0.8120 0.8120negative 0.6477 0.7073 0.6762neutral 0.3333 0.0375 0.0674(a) Twitter expression dataClass Precision Recall F-measurepositive 0.6823 0.8263 0.7475negative 0.7520 0.6947 0.7222neutral 0.0588 0.0063 0.0114(b) SMS expression dataTable 2: Precision, Recall and F-measure scores forpositive, negative and neutral classes computed onTwitter and SMS expressions data.seems that more fine-grained analysis of neutral ex-pressions is required for better polarity classificationaccuracy.Our method ranks 16th (F-measure: 0.7441) outof 28 participating systems for Twitter data and 12th(F-measure: 0.7348) out of 26 participating systemsfor SMS data.
The best performing system have0.8893(NRC-Canada) and 0.8837(GUMLTLT) av-eraged(positive, negative) F-measure score for Twit-ter and SMS data, respectively.5 ConclusionsOur experiments on features show that phrase priorpolarity features give good results for both subjec-tivity and polarity classification.
POS tag patternfeatures, emoticon and interjection features, on theother hand, are better suited for subjectivity classi-fication.
A deeper analysis is required and variousrelational and dependency features should be iden-tified and used to improve the performance of po-larity classification.
SMS expressions are noisier ingeneral than Twitter expressions and thus the polar-ity classifier gives less accurate results for it.
How-ever, both of these datasets face problems commonto the polarity classifier.
More research is neededwith a balanced dataset to understand various under-lying relational causes for an expression to becomeneutral and to further confirm the conclusions of thispaper.528ReferencesBaccianella, Stefano, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.Proceedings of LREC.
Malta.Barbosa, Luciano, and Junlan Feng.
2010.
Robust senti-ment detection on twitter from biased and noisy data.Proceedings of Coling.
Beijing.Gimpel, Kevin, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, and NoahA.
Smith.
2011.
Part-of-Speech Tagging for Twitter:Annotation, Features, and Experiments.
Proceedingsof ACL 2011.Hall, Mark, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.ACM SIGKDD Explorations Newsletter, 11(1), 10?18.Hu, Minqing, and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
KDD-2004.Kim, Soo-Min, and Eduard Hovy.
2004.
Determiningthe sentiment of opinions.
Coling-2004.Kouloumpis, Efthymios, Theresa Wilson, and JohannaMoore.
2011.
Twitter Sentiment Analysis: The Goodthe Bad and the OMG!.
Proceedings of ICWSM.Barcelona.Pak, Alexander, and Patrick Paroubek.
2010.
Twitter asa corpus for sentiment analysis and opinion mining.Proceedings of LREC.
Malta.Pang, Bo, and Lillian Lee.
2004.
A sentimental edu-cation: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
Proceedings of theACL.Theresa Wilson and Zornitsa Kozareva and PreslavNakov and Sara Rosenthal and Veselin Stoyanov andAlan Ritter.
SemEval-2013 Task 2: Sentiment Analysisin Twitter.
Proceedings of the International Workshopon Semantic Evaluation.
SemEval ?13.
June 2013.Atlanta, Georgia.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
Proceedings of HumanLanguage Technology Conference and Conference onEmpirical Methods in Natural Language Processing(HLT/EMNLP).
Vancouver.529
