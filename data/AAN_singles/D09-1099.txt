Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 948?957,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPToward Completeness in Concept Extraction and ClassificationEduard Hovy and Zornitsa KozarevaUSC Information Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292hovy@isi.edu, zkozareva@gmail.comEllen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112riloff@cs.utah.eduAbstractMany algorithms extract terms from text to-gether with some kind of taxonomic clas-sification (is-a) link.
However, the generalapproaches used today, and specifically themethods of evaluating results, exhibit seriousshortcomings.
Harvesting without focusing ona specific conceptual area may deliver largenumbers of terms, but they are scattered overan immense concept space, making Recalljudgments impossible.
Regarding Precision,simply judging the correctness of terms andtheir individual classification links may pro-vide high scores, but this doesn?t help with theeventual assembly of terms into a single coher-ent taxonomy.
Furthermore, since there is nocorrect and complete gold standard to measureagainst, most work invents some ad hoc evalu-ation measure.
We present an algorithm that ismore precise and complete than previous onesfor identifying from web text just those con-cepts ?below?
a given seed term.
Comparingthe results to WordNet, we find that the algo-rithm misses terms, but also that it learns manynew terms not in WordNet, and that it clas-sifies them in ways acceptable to humans butdifferent from WordNet.1 Collecting Information with CareOver the past few years, many algorithms have beenpublished on automatically harvesting terms andtheir conceptual types from the web and/or otherlarge corpora (Etzioni et al, 2005; Pasca, 2007;Banko et al, 2007; Yi and Niblack, 2005; Snow etal., 2005).
But several basic problems limit the even-tual utility of the results.First, there is no standard collection of factsagainst which results can be measured.
As we showin this paper, WordNet (Fellbaum, 1998), the mostobvious contender because of its size and popularity,is deficient in various ways: it is neither completenor is its taxonomic structure inarguably perfect.
Asa result, alternative ad hoc measures are inventedthat are not comparable.
Second, simply harvestingfacts about an entity without regard to its actual sub-sequent organization inflates Recall and Precisionevaluation scores: while it is correct that a jaguaris a animal, mammal, toy, sports-team, car-make,and operating-system, this information doesn?t helpto create a taxonomy that, for example, places mam-mal and animal closer to one another than to someof the others.
((Snow et al, 2005) is an exceptionto this.)
As a result, this work may give a mislead-ing sense of progress.
Third, entities are of differ-ent formal types, and their taxonomic treatment isconsequently different: some are at the level of in-stances (e.g., Michelangelo was a painter) and someat the level of concepts (e.g., a painter is a human).The goal of our research is to learn terms for en-tities (objects) and their taxonomic organization si-multaneously, from text.
Our method is to use asingle surface-level pattern with several open posi-tions.
Filling them in different ways harvests differ-ent kinds of information, and/or confirms this infor-mation.
We evaluate in two ways: against WordNet,since that is a commonly available and popular re-source, and also by asking humans to judge the re-sults since WordNet is neither complete nor exhaus-tively taxonomized.In this paper, we describe experiments with tworich and common portions of an entity taxonomy:Animals and People.
The claim of this paper is: It ispossible to learn terms automatically to populate atargeted portion of a taxonomy (such as below An-948imals or People) both at high precision comparedto WordNet and including additional correct ones aswell.
We would like to also report on Recall rela-tive to WordNet, but given the problems describedin Section 4, this turns out to be much harder thanwould seem.First, we need to define some basic terminology:term: An English word (for our current purposes, anoun or a proper name).seed term: A word we use to initiate the algorithm.concept: An item in the classification taxonomy weare building.
A concept may correspond to severalterms (singular form, plural form, the term?s syn-onyms, etc.
).root concept: A concept at a fairly general (high)level in the taxonomy, to which many others areeventually learned to be subtypes/instances of.basic-level concept: A concept at the ?basic level?,corresponding approximately to the Basic Level cat-egories defined in Prototype Theory in Psychology(Rosch, 1978).
For our purposes, a concept corre-sponding to the (proto)typical level of generality ofits type; that is, a dog, not a mammal or a dachshund;a singer, not a human or an opera diva.instance: An item in the classification taxonomythat is more specific than a concept; only one exam-ple of the instance exists in ?the real world?
at anytime.
For example, Michelangelo is an instance, aswell as Mazda Miata with license plate 3HCY687,while Mazda Miata is not.classification link: We use a single relation, that,depending on its arguments, is either is a type of(when both arguments are concepts), or is an in-stance of or is an example of (when the first argu-ment is an instance/example of the second).Section 2 describes our method for harvesting;Section 3 discusses related work; and Section 4 de-scribes the experiments and the results.2 Term and Relation Extraction using theDoubly-Anchored PatternOur goal is to develop a technique that automatically?fills in?
the concept space in the taxonomy belowany root concept, by harvesting terms through re-peated web queries.
We perform this in two alter-nating stages.Stage 1: Basic-level/Instance concept collec-tion: We use the Doubly-Anchored Pattern DAP de-veloped in (Kozareva et al, 2008):DAP: [SeedTerm1] such as [SeedTerm2] and <X>which learns a list of basic-level concepts or in-stances (depending on whether SeedTerm2 ex-presses a basic-level concept or an instance).1 DAPis very reliable because it is instantiated with ex-amples at both ?ends?
of the space to be filled (thehigher-level (root) concept SeedTerm1 and a basic-level term or instance (SeedTerm2)), which mutu-ally disambiguate each other.
For example, ?pres-idents?
for SeedTerm1 can refer to the leader of acountry, corporation, or university, and ?Ford?
forSeedTerm2 can refer to a car company, an automo-bile pioneer, or a U.S. president.
But when the twoterms co-occur in a text that matches the pattern?Presidents such as Ford and <X>?, the text willalmost certainly refer to country presidents.The first stage involves a series of repeated re-placements of SeedTerm2 by newly-learned termsin order to generate even more seed terms.
That is,each new basic-level concept or instance is rotatedinto the pattern (becoming a new SeedTerm2) in abootstrapping cycle that Kozareva et al called reck-less bootstrapping.
This procedure is implementedas exhaustive breadth-first search, and iterates untilno new terms are harvested.
The harvested terms areincorporated in a Hyponym Pattern Linkage Graph(HPLG) G = (V,E), where each vertex v ?
V isa candidate term and each edge (u, v) ?
E indi-cates that term v was generated by term u.
A termu is ranked by Out-Degree(u) =P?
(u,v)?Ew(u,v)|V |?1,which represents the weighted sum of u?s outgoingedges normalized by the total number of other nodesin the graph.
Intuitively, a term ranks highly if itis frequently discovering many different terms dur-ing the reckless bootstrapping cycle.
This method isvery productive, harvesting a constant stream of newterms for basic-level concepts or instances when thetaxonomy below the initial root concept SeedTerm1is extensive (such as for Animals or People).1Strictly speaking, our lowest-level concepts can be in-stances, basic-level concepts, or concepts below the basic level(e.g., dachsund).
But for the sake of simplicity we will refer toour lowest-level terms as basic-level concepts and instances.949Stage 2: Intermediate level concept collection:Going beyond (Kozareva et al, 2008), we next applythe Doubly-Anchored Pattern in the ?backward?
di-rection (DAP?1), for any two seed terms represent-ing basic-level concepts or instances:DAP?1: <X> such as [SeedTerm1] and [SeedTerm2]which harvests a set of concepts, most of them inter-mediate between the basic level or instance and theinitial higher-level seed.This second stage (DAP?1) has not yet been de-scribed in the literature.
It proceeds analogously.For pairs of basic-level concepts or instances be-low the root concept that were found during the firststage, we instantiate DAP?1 and issue a new webquery.
For example, if the term ?cats?
was harvestedby DAP in ?Animals such as dogs and <X>?, thenthe pair < dogs, cats > forms the new Web query?<X> such as dogs and cats?.
We extract up to 2consecutive nouns from the <X> position.This procedure yields a large number of discov-ered concepts, but they cannot all be used for fur-ther bootstrapping.
In addition to practical limita-tions (such as limits on web querying), many of themare too general?more general than the initial rootconcept?and could derail the bootstrapping processby introducing terms that stray every further awayfrom the initial root concept.
We therefore rank theharvested terms based on the likelihood that theywill be productive if they are expanded in the nextcycle.
Ranking is based on two criteria: (1) the con-cept should be prolific (i.e., produce many lower-level concepts) in order to keep the bootstrappingprocess energized, and (2) the concept should besubordinate to the root concept, so that the processstays within the targeted part of the search space.To perform ranking, we incorporate both the har-vested concepts and the basic-level/instance pairsinto a Hypernym Relation Graph (HRG), which wedefine as a bipartite graph HRG = (V,E) with twotypes of vertices.
One set of vertices represents theconcepts (the category vertices (Vc), and a secondset of vertices represents the basic-level/instancepairs that produced the concepts (the member pairvertices (Vmp)).
We create an edge e(u, v) ?
Ebetween u ?
Vcand v ?
Vmpwhen the con-cept represented by u was harvested by the basic-level/instance pair represented by v, with the weightof the edge defined as the number of times that thelower pair found the concept on the web.We use the Hypernym Relation Graph to rankthe intermediate concepts based on each node?s In-Degree, which is the sum of the weights on thenode?s incoming edges.
Formally, In-Degree(u) =??
(u,v)?Ew(u, v).
Intuitively, a concept will beranked highly if it was harvested by many differentcombinations of basic-level/instance terms.However, this scoring function does not deter-mine whether a concept is more or less general thanthe initial root concept.
For example, when har-vesting animal categories, the system may learn theword ?species?, which is a very common term asso-ciated with animals, but also applies to non-animalssuch as plants.
To prevent the inclusion of over-general terms and constrain the search to remain?below?
the root concept, we apply a Concept Posi-tioning Test (CPT): We issue the following two webqueries:(a) Concept such as RootConcept and <X>(b) RootConcept such as Concept and <X>If (b) returns more web hits than (a), then the con-cept passes the test, otherwise it fails.
The first (mosthighly ranked) concept that passes CPT becomes thenew seed concept for the next bootstrapping cycle.In principle, we could use all the concepts that passthe CPT for bootstrapping2.
However, for practicalreasons (primarily limitations on web querying), werun the algorithm for 10 iterations.3 Related WorkMany algorithms have been developed to automat-ically acquire semantic class members using a va-riety of techniques, including co-occurrence statis-tics (Riloff and Shepherd, 1997; Roark and Char-niak, 1998), syntactic dependencies (Pantel andRavichandran, 2004), and lexico-syntactic patterns(Riloff and Jones, 1999; Fleischman and Hovy,2002; Thelen and Riloff, 2002).The work most closely related to ours is that of(Hearst, 1992) who introduced the idea of apply-ing hyponym patterns to text, which explicitly iden-tify a hyponym relation between two terms (e.g.,2The number of ranked concepts that pass CPT changes ineach iteration.
Also, the wildcard * is important for counts, ascan be verified with a quick experiment using Google.950?such authors as <X>?).
In recent years, sev-eral researchers have followed up on this idea usingthe web as a corpus.
(Pasca, 2004) applies lexico-syntactic hyponym patterns to the Web and use thecontexts around them for learning.
KnowItAll (Et-zioni et al, 2005) applies the hyponym patterns toextract instances from the Web and ranks them byrelevance using mutual information.
(Kozareva etal., 2008) introduced a bootstrapping scheme usingthe doubly-anchored pattern (DAP) that is guidedthrough graph ranking.
This approach reported asignificant improvement from 5% to 18% over ap-proaches using singly-anchored patterns like thoseof (Pasca, 2004) and (Etzioni et al, 2005).
(Snow et al, 2005) describe a dependency pathbased approach that generates a large number ofweak hypernym patterns using pairs of noun phrasespresent in WordNet.
They build a classifier usingthe different hypernym patterns and find among thehighest precision patterns those of (Hearst, 1992).Snow et al report performance of 85% precisionat 10% recall and 25% precision at 30% recall for5300 hand-tagged noun phrase pairs.
(McNamee etal., 2008) use the technique of (Snow et al, 2005)to harvest the hypernyms of the proper names.
Theaverage precision on 75 automatically detected cat-egories is 53%.
The discovered hypernyms wereintergrated in a Question Answering system whichshowed an improvement of 9% when evaluated on aTREC Question Answering data set.Recently, (Ritter et al, 2009) reported hypernymlearning using (Hearst, 1992) patterns and manuallytagged common and proper nouns.
All hypernymcandidates matching the pattern are acquired, andthe candidate terms are ranked by mutual informa-tion.
However, they evaluate the performance oftheir hypernym algorithm by considering only thetop 5 hypernyms given a basic-level concept or in-stance.
They report 100% precision at 18% recall,and 66% precision at 72% recall, considering onlythe top-5 list.
Necessarily, using all the results re-turned will result in lower precision scores.
In con-trast to their approach, our aim is to first acquire au-tomatically with minimal supervision the basic-levelconcepts for given root concept.
Thus, we almostentirely eliminate the need for humans to providehyponym seeds.
Second, we evaluate the perfor-mance of our approach not by measuring the top-ranked 5 hypernyms given a basic-level concept, butconsidering all harvested hypernyms of the concept.Unlike (Etzioni et al, 2005), (Pasca, 2007) and(Snow et al, 2005), we learn both instances and con-cepts simultaneously.Some researchers have also worked on reorga-nizing, augmenting, or extending semantic conceptsthat already exist in manually built resources suchas WordNet (Widdows and Dorow, 2002; Snow etal., 2005) or Wikipedia (Ponzetto and Strube, 2007).Work in automated ontology construction has cre-ated lexical hierarchies (Caraballo, 1999; Cimianoand Volker, 2005; Mann, 2002), and learned seman-tic relations such as meronymy (Berland and Char-niak, 1999; Girju et al, 2003).4 EvaluationThe root concepts discussed in this paper are An-imals and People, because they head large taxo-nomic structures that are well-represented in Word-Net.
Throughout these experiments, we used as theinitial SeedTerm2 lions for Animals and Madonnafor People (by specifically choosing a proper namefor People we force harvesting down to the level ofindividual instances).
To collect data, we submittedthe DAP patterns as web queries to Google, retrievedthe top 1000 web snippets per query, and kept onlythe unique ones.
In total, we collected 1.1 GB ofsnippets for Animals and 1.5 GB for People.
Thealgorithm was allowed to run for 10 iterations.The algorithm learns a staggering variety of termsthat is much more diverse than we had antici-pated.
In addition to many basic-level concepts orinstances, such as dog and Madonna respectively,and many intermediate concepts, such as mammals,pets, and predators, it also harvested categories thatclearly seemed useful, such as laboratory animals,forest dwellers, and endangered species.
Many otherharvested terms were more difficult to judge, includ-ing bait, allergens, seafood, vectors, protein, andpests.
While these terms have an obvious relation-ship to Animals, we have to determine whether theyare legitimate and valuable subconcepts of Animals.A second issue involves relative terms that arehard to define in an absolute sense, such as nativeanimals and large mammals.A complete evaluation should answer the followingthree questions:951?
Precision: What is the correctness of the har-vested concepts?
(How many of them are sim-ply wrong, given the root concept?)?
Recall: What is the coverage of the harvestedconcepts?
(How many are missing, below agiven root concept?)?
How correct is the taxonomic structurelearned?Given the number and variety of terms obtained,we initially decided that an automatic evaluationagainst existing resources (such as WordNet orsomething similar) would be inadequate becausethey do not contain many of our harvested terms,even though many of these terms are clearly sensi-ble and potentially valuable.
Indeed, the whole pointof our work is to learn concepts and taxonomies thatgo above and beyond what is currently available.However, it is necessary to compare withsomething, and it is important not to skirt the issueby conducting evaluations that measure subsets ofresults, or that perhaps may mislead.
We thereforedecided to compare our results against WordNet andto have human annotators judge as many results aswe could afford (to obtain a measure of Precisionand the legitimate extensions beyond WordNet).Unfortunately, it proved impossible to measureRecall against WordNet, because this requires as-certaining the number of synsets in WordNet be-tween the root and its basic-level categories.
Thisrequires human judgment, which we could not af-ford.
We plan to address this question in futurework.
Also, assessing the correctness of the learnedtaxonomy structure requires the manual assessmentof each classification link proposed by the systemthat is not already in WordNet, a task also beyondour budget to complete in full.
Some results?forjust basic-level terms and intermediate concepts, butnot among intermediate-level concepts?are shown inSection 4.3.We provide Precision scores using the followingmeasures, where terms refers to the harvested terms:PrWN=#terms found in WordNet#terms harvested by systemPrH=#terms judged correct by human#terms harvested by systemNotInWN = #terms judged correct by human butnot in WordNetWe conducted three sets of experiments.
Ex-periment 1 evaluates the results of using DAP tolearn basic-level concepts for Animals and instancesfor People.
Experiment 2 evaluates the results ofusing DAP?1 to harvest intermediate concepts be-tween each root concept and its basic-level conceptsor instances.
Experiment 3 evaluates the taxonomystructure that is produced via the links between theinstances and intermediate concepts.4.1 Experiment 1: Basic-Level Concepts andInstancesIn this section we discuss the results of harvest-ing the basic-level Animal concepts and People in-stances.
The bootstrapping algorithm ranks the har-vested terms by their Out-Degree score and consid-ers as correct only those with Out-Degree > 0.
Inten iterations, the bootstrapping algorithm produced913 Animal basic-level concepts and 1, 344 Peopleinstances that passed this Out-Degree criterion.4.1.1 Human EvaluationThe harvested terms were labeled by humanjudges as either correct or incorrect with respect tothe root concept.
Table 1 shows the Precision of thetop-ranked N terms, with N shown in incrementsof 100.
Overall, the Animal terms yielded 71%(649/913) Precision and the People terms yielded95% Precision (1,271/1,344).
Figure 1 shows thathigher-ranked Animal terms are more accurate thanlower-ranked terms, which indicates that the scor-ing function did its job.
For People terms, accuracywas very high throughout the ranked list.
Overall,these results show that the bootstrapping algorithmgenerates a large number of correct instances of highquality.4.1.2 WordNet EvaluationTable 1 shows a comparison of the harvestedterms against the terms present in WordNet.Note that the Precision measured against WordNet(PrWN) for People is dramatically different fromthe Precision based on human judgments (PrH).This can be explained by looking at the NotInWNcolumn, which shows that 48 correct Animal terms95200.10.20.30.40.50.60.70.80.91100  200  300  400  500  600  700  800  900PrecisionRankAnimal Basic-level Concepts00.10.20.30.40.50.60.70.80.91200  400  600  800  1000  1200PrecisionRankPeople InstancesFigure 1: Ranked Basic-Concepts and Instances.and 986 correct People instances are not present inWordNet (primarily, for people, because WordNetcontains relatively few proper names).
These resultsshow that there is substantial room for improvementin WordNet?s coverage of these categories.
For Ani-mals, the precision measured against WordNet is ac-tually higher than the precision measured by humanjudges, which may indicate that the judges failed torecognize some correct animal terms.PrWNPrHNotInWNAnimal .79 .71 48People .23 .95 986Table 1: Instance Evaluation.4.1.3 Evaluation against Prior WorkTo assess how well our algorithm compares withprevious semantic class learning methods, we com-pared our results to those of (Kozareva et al, 2008).Our work was inspired by that approach?in fact, weuse that previous algorithm as the first step of ourbootstrapping process.
The novelty of our approachis the insertion of an additional bootstrapping stagethat iteratively learns new intermediate concepts us-ing DAP?1 and the Concept Positioning Test, fol-lowed by the subsequent use of the newly learnedintermediate concepts in DAP to expand the searchspace beyond the original root concept.
This leadsto the discovery of additional basic-level terms or in-stances, which are then recycled in turn to discovernew intermediate concepts, and so on.Consequently, we can compare the results pro-duced by the first iteration of our algorithm (be-fore intermediate concepts are learned) to those of(Kozareva et al, 2008) for the Animal and Peoplecategories, and then compare again after 10 boot-strapping iterations of intermediate concept learn-ing.
Figure 2 shows the number of harvested con-cepts for Animals and People after each bootstrap-ping iteration.
Bootstrapping with intermediate con-cepts produces nearly 5 times as many basic-levelconcepts and instances than (Kozareva et al, 2008)obtain, while maintaining similar levels of precision.The intermediate concepts help so much becausethey steer the learning process into new (yet still cor-rect) regions of the search space after each iteration.For instance, in the first iteration, the pattern ?ani-mals such as lions and *?
harvests about 350 basic-level concepts, but only animals that are mentionedin conjunction with lions are learned.
Of these, an-imals typically quite different from lions, such asgrass-eating kudu, are often not discovered.However, in the second iteration, the intermediateconcept Herbivore is chosen for expansion.
The pat-tern ?herbivore such as antelope and *?
discoversmany additional animals, including kudu, that co-occur with antelope but do not co-occur with lions.Table 2 shows examples of the 10 top-rankedbasic-level concepts and instances that were learnedfor 3 randomly-selected intermediate Animal andPeople concepts (IConcepts) that were acquired dur-ing bootstrapping.
In the next section, we present an95305001000150020002500300035001  2  3  4  5  6  7  8  9  10#ItemsLearnedIterationsAnimal Intermediate ConceptsAnimal Basic-level Concepts050010001500200025003000350040001  2  3  4  5  6  7  8  9  10#ItemsLearnedIterationsPeople Intermediate ConceptsPeople InstancesFigure 2: Learning Curves.evaluation of the intermediate concept terms.4.2 Experiment 2: Intermediate ConceptsIn this section we discuss the results of harvestingthe intermediate-level concepts.
Given the variety ofthe harvested results, manual judgment of correct-ness required an in-depth human annotation study.We also compare our harvested results against theconcept terms in WordNet.4.2.1 Human EvaluationWe hired 4 annotators (undergraduates at a dif-ferent institution) to judge the correctness of the in-termediate concepts.
We created detailed annota-tion guidelines that define 14 annotation labels foreach of the Animal and People classes, as shownin Table 3.
The labels are clustered into 4 majorPEOPLEIConcept InstancesDictators: Adolf Hitler, Joseph Stalin, Benito Mussolini, Lenin,Fidel Castro, Idi Amin, Slobodan Milosevic,Hugo Chavez, Mao Zedong, Saddam HusseinCelebrities: Madonna, Paris Hilton, Angelina Jolie, Britney ,Spears, Tom Cruise, Cameron Diaz, Bono,Oprah Winfrey, Jennifer Aniston, Kate MossWriters: William Shakespeare, James Joyce, Charles Dickens,Leo Tolstoy, Goethe, Ralph Waldo Emerson,Daniel Defoe, Jane Austen, Ernest Hemingway,Franz KafkaANIMALIConcept Basic-level TermsCrustacean: shrimp, crabs, prawns, lobsters, crayfish, mysids,decapods, marron, ostracods, yabbiesPrimates: baboons, monkeys, chimpanzees, apes, marmosets,chimps, orangutans, gibbons, tamarins, bonobosMammal: mice, whales, seals, dolphins, rats, deer, rabbits,dogs, elephants, squirrelsTable 2: Learned People and Animals Terms.types: Correct, Borderline, BasicConcept, and Not-Concept.
The details of our annotation guidelines,the reasons for the intermediate labels, and the anno-tation study can be found in (Kozareva et al, 2009).ANIMALTYPE LABEL EXAMPLESCorrect GeneticAnimal reptile,mammalBehavioralByFeeding predator, grazerBehaviorByHabitat saltwater mammalBehaviorSocialIndiv herding animalBehaviorSocialGroup herd, packMorphologicalType cloven-hoofed animalRoleOrFunction pet, parasiteBorderline NonRealAnimal dragonsEvaluativeTerm varmint, foxOtherAnimal critter, fossilBasicConcept BasicAnimal dog, hummingbirdNotConcept GeneralTerm model, catalystNotAnimal topic, favoriteGarbageTerm brates, malsPEOPLETYPE LABEL EXAMPLESCorrect GeneticPerson Caucasian, SaxonNonTransientEventRole stutterer, gourmandTransientEventRole passenger, visitorPersonState dwarf, schizophrenicFamilyRelation aunt, motherSocialRole fugitive, heroNationOrTribe Bulgarian, ZuluReligiousAffiliation Catholic, atheistBorderline NonRealPerson biblical figuresOtherPerson colleagues, couplesBasicConcept BasicPerson child, womanRealPerson Barack ObamaNotConcept GeneralTerm image, figureNotPerson books, eventsTable 3: Intermediate Concept Annotation LabelsWe measured pairwise inter-annotator agreementacross the four labels using the Fleiss kappa (Fleiss,1971).
The ?
scores ranged from 0.61?0.71 forAnimals (average ?=0.66) and from 0.51?0.70 forPeople (average ?=0.60).
These agreement scoresseemed good enough to warrant using these humanjudgments to estimate the accuracy of the algorithm.The bootstrapping algorithm harvested 3, 549 An-imal and 4, 094 People intermediate concepts in teniterations.
After In-Degree ranking was applied,954we chose a random sample of intermediate conceptswith frequency over 1, which was given to four hu-man judges for annotation.
Table 4 summarizes thelabels assigned by the four annotators (A1?
A4).The top portion of Table 4 shows the results for allthe intermediate concepts (437 Animal terms and296 People terms), and the bottom portion shows theresults only for the concepts that passed the ConceptPositioning Test (187 Animal terms and 139 Peopleterms).
Accuracy is computed in two ways: Acc1 isthe percent of intermediate concepts labeled as Cor-rect; Acc2 is the percent of intermediate conceptslabeled as either Correct or Borderline.Without the CPT, accuracies range from 53?66%for Animals and 75?85% for People.
After ap-plying the CPT, the accuracies increase to 71?84%for animals and 82?94% for people.
These resultsconfirm that the Concept Positioning Test is effec-tive at removing many of the undesirable terms.Overall, these results demonstrate that our algorithmproduced many high-quality intermediate concepts,with good precision.Figure 3 shows accuracy curves based on therankings of the intermediate concepts (based on In-Degree scores).
The CPT clearly improves accu-racy even among the most highly ranked concepts.For example, the Acc1 curves for animals show thatnearly 90% of the top 100 intermediate conceptswere correct after applying the CPT, whereas only70% of the top 100 intermediate concepts were cor-rect before.
However, the CPT also eliminates manydesirable terms.
For People, the accuracies are stillrelatively high even without the CPT, and a muchlarger set of intermediate concepts is learned.Animals PeopleA1A2A3A4A1A2A3A4Correct 246 243 251 230 239 231 225 221Borderline 42 26 22 29 12 10 6 4BasicConcept 2 8 9 2 6 2 9 10NotConcept 147 160 155 176 39 53 56 61Acc1 .56 .56 .57 .53 .81 .78 .76 .75Acc2 .66 .62 .62 .59 .85 .81 .78 .76Animals after CPT People after CPTA1A2A3A4A1A2A3A4Correct 146 133 144 141 126 126 114 116Borderline 11 15 9 13 6 2 2 0BasicConcept 2 8 9 2 0 1 7 7NotConcept 28 31 25 31 7 10 16 16Acc1 .78 .71 .77 .75 .91 .91 .82 .83Acc2 .84 .79 .82 .82 .95 .92 .83 .83Table 4: Human Intermediate Concept Evaluation.00.10.20.30.40.50.60.70.80.9150  100  150  200  250  300  350  400PrecisionRankAnimal Intermediate ConceptsnoCPTCnoCPTCBwithCPTCwithCPTCB00.10.20.30.40.50.60.70.80.9150  100  150  200  250  300PrecisionRankPeople Intermediate ConceptsnoCPTCnoCPTCBwithCPTCwithCPTCBFigure 3: Intermediate Concept Precision at Rank N.4.2.2 WordNet EvaluationWe also compared the intermediate concepts har-vested by the algorithm to the contents of WordNet.The results are shown in Table 5.
WordNet contains20% of the Animal concepts and 51% of the Peopleconcepts learned by our algorithm, which confirmsthat many of these concepts were considered to bevaluable taxonomic terms by the WordNet develop-ers.
However, our human annotators judged 57%of the Animal and 84% of the People concepts tobe correct, which suggests that our algorithm gen-erates a substantial number of additional conceptsthat could be used to enrich taxonomic structure inWordNet.955PrWNPrHNotInWNAnimal .20 (88/437) .57 (248/437) 204People .51 (152/296) .85 (251/296) 108Table 5: WordNet Intermediate Concept Evaluation.4.3 Experiment 3: Taxonomic LinksIn this section we evaluate the classification (taxon-omy) that is learned by evaluating the links betweenthe intermediate concepts and the basic-level con-cept/instance terms.
That is, when our algorithmclaims that isa(X,Y), how often is X truly a subcon-cept of Y?
For example, isa(goat, herbivore) wouldbe correct, but isa(goat, bird) would not.
Again,since WordNet does not contain all the harvestedconcepts, we conduct both a manual evaluation anda comparison against WordNet.4.3.1 Manual and WordNet EvaluationsCreating and evaluating the full taxonomic struc-ture between the root and the basic-level or instanceterms is future work.
Here we evaluate simply theaccuracy of the taxonomic links between basic-levelconcepts/instances and intermediate concepts as har-vested, but not between intermediate concepts.
Foreach pair, we extracted all harvested links and deter-mined whether the same links appear in WordNet.The links were also given to human judges.
Table 6shows the results.ISA PrWNPrHNotInWNAnimal .47(912/1940) .88 (1716/1940) 804People .23 (318/908) .94 (857/908) 539Table 6: WordNet Taxonomic Evaluation.The results show that WordNet lacks nearly halfof the taxonomic relations that were generated bythe algorithm: 804 Animal and 539 People links.5 ConclusionWe describe a novel extension to the DAP approachfor discovering basic-level concepts or instances andtheir superconcepts given an initial root concept.
Byappropriate filling of different positions in DAP, thealgorithm alternates between ?downward?
and ?up-ward?
learning.
A key resulting benefit is that eachnew intermediate-level term acquired restarts har-vesting in a new region of the concept space, whichallows previously unseen concepts to be discoveredwith each bootstrapping cycle.We also introduce the Concept Positioning Test,which serves to confirm that a harvested conceptfalls into the desired part of the search space rela-tive to either a superordinate or subordinate conceptin the growing taxonomy, before it is selected forfurther harvesting using the DAP.These algorithms can augment other term harvest-ing algorithms recently reported.
But in order tocompare different algorithms, it is important to com-pare results to a standard.
WordNet is our best can-didate at present.
But WordNet is incomplete.
Ourresults include a significantly large number of in-stances of People (which WordNet does not claimto cover), a number comparable to the results of (Et-zioni et al, 2005; Pasca, 2007; Ritter et al, 2009).Rather surprisingly, our results also include a largenumber of basic-level and intermediate concepts forAnimals that are not present in WordNet, a categoryWordNet is actually fairly complete about.
Thesenumbers show clearly that it is important to conductmanual evaluation of term harvesting algorithms inaddition to comparing to a standard resource.AcknowledgmentsThis research was supported in part by grants fromthe National Science Foundation (NSF grant no.
IIS-0429360), and the Department of Homeland Se-curity, ONR Grant numbers N0014-07-1-0152 andN00014-07-1-0149.
We are grateful to the anno-tators at the University of Pittsburgh who helpedus evaluate this work: Jay Fischer, David Halpern,Amir Hussain, and Taichi Nakatani.ReferencesM.
Banko, M. Cafarella, S. Soderland, M. Broadhead,and O.Etzioni.
2007.
Open information extractionfrom the web.
In Proceedings of International JointConference on Artificial Itelligence, pages 2670?2676.M.
Berland and E. Charniak.
1999.
Finding Parts in VeryLarge Corpora.
In Proc.
of the 37th Annual Meeting ofthe Association for Computational Linguistics.S.
Caraballo.
1999.
Automatic Acquisition of aHypernym-Labeled Noun Hierarchy from Text.
InProc.
of the 37th Annual Meeting of the Associationfor Computational Linguistics, pages 120?126.P.
Cimiano and J. Volker.
2005.
Towards large-scale,open-domain and ontology-based named entity classi-fication.
In Proceeding of RANLP-05, pages 166?172.956O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.2005.
Unsupervised named-entity extraction from theweb: an experimental study.
Artificial Intelligence,165(1):91?134, June.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase (Language, Speech, and Communication).May.M.B.
Fleischman and E.H. Hovy.
2002.
Fine grainedclassification of named entities.
In Proceedings of theCOLING conference, August.J.L.
Fleiss.
1971.
Measuring nominal scale agree-ment among many raters.
Psychological Bulletin,76(5):378?382.R.
Girju, A. Badulescu, and D. Moldovan.
2003.
Learn-ing semantic constraints for the automatic discovery ofpart-whole relations.
In HLT-NAACL.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In COLING, pages 539?545.Z.
Kozareva, E. Riloff, and E. Hovy.
2008.
Seman-tic class learning from the web with hyponym pat-tern linkage graphs.
In Proceedings of ACL-08: HLT,pages 1048?1056.
Association for Computational Lin-guistics.Z.
Kozareva, E. Hovy, and E. Riloff.
2009.
Learning andevaluating the content and structure of a term taxon-omy.
In AAAI-09 Spring Symposium on Learning byReading and Learning to Read.G.
Mann.
2002.
Fine-grained proper noun ontologies forquestion answering.
In COLING-02 on SEMANET,pages 1?7.P.
McNamee, R. Snow, P. Schone, and J. Mayfield.
2008.Learning named entity hyponyms for question answer-ing.
In Proceedings of the Third International JointConference on Natural Language Processing.P.
Pantel and D. Ravichandran.
2004.
Automatically la-beling semantic classes.
In HLT-NAACL, pages 321?328.M.
Pasca.
2004.
Acquisition of categorized named en-tities for web search.
In Proceedings of CIKM, pages137?145.M.
Pasca.
2007.
Weakly-supervised discovery of namedentities using web search queries.
In CIKM, pages683?690.S.
Ponzetto and M. Strube.
2007.
Deriving a large scaletaxonomy from wikipedia.
In Proceedings of the 22ndNational COnference on Artificial Intelligence (AAAI-07), pages 1440?1447.E.
Riloff and R. Jones.
1999.
Learning Dictionaries forInformation Extraction by Multi-Level Bootstrapping.In Proceedings of the Sixteenth National Conferenceon Artificial Intelligence.E.
Riloff and J. Shepherd.
1997.
A Corpus-Based Ap-proach for Building Semantic Lexicons.
In Proceed-ings of the Second Conference on Empirical Methodsin Natural Language Processing, pages 117?124.A.
Ritter, S. Soderland, and O. Etzioni.
2009.
What isthis, anyway: Automatic hypernym discovery.
In Pro-ceedings of AAAI-09 Spring Symposium on Learningby Reading and Learning to Read, pages 88?93.B.
Roark and E. Charniak.
1998.
Noun-phrase Co-occurrence Statistics for Semi-automatic SemanticLexicon Construction.
In Proceedings of the 36thAnnual Meeting of the Association for ComputationalLinguistics, pages 1110?1116.E.
Rosch, 1978.
Principles of Categorization, pages 27?48.R.
Snow, D. Jurafsky, and A. Y. Ng.
2005.
Learningsyntactic patterns for automatic hypernym discovery.In NIPS.M.
Thelen and E. Riloff.
2002.
A Bootstrapping Methodfor Learning Semantic Lexicons Using Extraction Pat-tern Contexts.
In Proceedings of the 2002 Conferenceon Empirical Methods in Natural Language Process-ing, pages 214?221.D.
Widdows and B. Dorow.
2002.
A graph model for un-supervised lexical acquisition.
In Proceedings of the19th international conference on Computational lin-guistics, pages 1?7.J.
Yi and W. Niblack.
2005.
Sentiment mining in web-fountain.
In ICDE ?05: Proceedings of the 21st In-ternational Conference on Data Engineering, pages1073?1083.957
