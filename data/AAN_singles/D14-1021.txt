Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 177?182,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsSyntactic SMT Using a Discriminative Text Generation ModelYue Zhang Kai Song?
Linfeng Song?SUTD, Singapore NEU, China ICT/CAS, Chinayue zhang@sutd.edu.sg songkai.sk@alibaba-inc.com songlinfeng@ict.ac.cnJingbo Zhu Qun LiuNEU, China CNGL, Ireland and ICT/CAS, Chinazhujingbo@mail.neu.edu.cn qliu@computing.dcu.ieAbstractWe study a novel architecture for syntacticSMT.
In contrast to the dominant approachin the literature, the system does not relyon translation rules, but treat translationas an unconstrained target sentence gen-eration task, using soft features to cap-ture lexical and syntactic correspondencesbetween the source and target languages.Target syntax features and bilingual trans-lation features are trained consistently ina discriminative model.
Experiments us-ing the IWSLT 2010 dataset show that thesystem achieves BLEU comparable to thestate-of-the-art syntactic SMT systems.1 IntroductionTranslation rules have been central to hierarchi-cal phrase-based and syntactic statistical machinetranslation (SMT) (Galley et al., 2004; Chiang,2005; Liu et al., 2006; Quirk et al., 2005; Marcu etal., 2006; Shen and Joshi, 2008; Xie et al., 2011).They are attractive by capturing the recursivenessof languages and syntactic correspondences be-tween them.
One important advantage of trans-lation rules is that they allow efficient decodingby treating MT as a statistical parsing task, trans-forming a source sentence to its translation via re-cursive rule application.The efficiency takes root in the fact that targetword orders are encoded in translation rules.
Thisfact, however, also leads to rule explosion, noiseand coverage problems (Auli et al., 2009), whichcan hurt translation quality.
Flexibility of functionword usage, rich morphology and paraphrasing alladd to the difficulty of rule extraction.
In addition,restricting target word orders by hard translationrules can also hurt output fluency.?
* Work done while visiting Singapore University ofTechnology and Design (SUTD)Figure 1: Overall system architecture.A potential solution to the problems above is totreat translation as a generation task, represent-ing syntactic correspondences using soft features.Both adequacy and fluency can potentially be im-proved by giving full flexibility to target synthe-sis, and leaving all options to the statistical model.The main challenge to this method is a signifi-cant increase in the search space (Knight, 1999).To this end, recent advances in tackling complexsearch tasks for text generation offer some so-lutions (White and Rajkumar, 2009; Zhang andClark, 2011).In this short paper, we present a preliminary in-vestigation on the possibility of building a syn-tactic SMT system that does not use hard transla-tion rules, by utilizing recent advances in statisti-cal natural language generation (NLG).
The over-all architecture is shown in Figure 1.
Translationis performed by first parsing the source sentence,then transferring source words and phrases to theirtarget equivalences, and finally synthesizing thetarget output.We choose dependency grammar for both thesource and the target syntax, and adapt the syntac-tic text synthesis system of Zhang (2013), whichperforms dependency-based linearization.
Thelinearization task for MT is different from themonolingual task in that not all translation optionsare used to build the output, and that bilingual cor-respondences need to be taken into account dur-177ing synthesis.
The algorithms of Zhang (2013) aremodified to perform word selection as well as or-dering, using two sets of features to control trans-lation adequacy and fluency, respectively.Preliminary experiments on the IWSLT1 2010data show that the system gives BLEU compara-ble to traditional tree-to-string and string-to-treetranslation systems.
It demonstrates the feasibilityof leveraging statistical NLG techniques for SMT,and the possibility of building a statistical transfer-based MT system.2 ApproachThe main goal being proof of concept, we keepthe system simple by utilizing existing methodsfor the main components, minimizing engineer-ing efforts.
Shown in Figure 1, the end-to-endsystem consists of two main components: lexicaltransfer and synthesis.
The former provides can-didate translations for (overlapping) source wordsand phrases.
Although lexicons and rules canbe used for this step, we take a simple statisti-cal alignment-based approach.
The latter searchesfor a target translation by constructing dependencytrees bottom-up.
The process can be viewed asa syntax-based generation process from a bag ofoverlapping translation options.2.1 Lexical transferWe perform word alignment using IBM model 4(Brown et al., 1993), and then extract phrase pairsaccording to the alignment and automatically-annotated target syntax.
In particular, consistent(Och et al., 1999) and cohesive (Fox, 2002) phrasepairs are extracted from intersected alignments inboth directions: the target side must form a pro-jective span, with a single root, and the source sidemust be contiguous.
A resulting phrase pair con-sists of the source phrase, its target translation, aswell as the head position and head part-of-speech(POS) of the target span, which are useful for tar-get synthesis.
We further restrict that neither thesource nor the target side of a valid phrase paircontains over s words.Given an input source sentence, the lexicaltransfer unit finds all valid target translation op-tions for overlapping source phrases up to size s,and feeds them as inputs to the target synthesis de-coder.
The translation options with a probability1International Workshop on Spoken Language Transla-tion, http://iwslt2010.fbk.eubelow ?
?
Pmaxare filtered out, where Pmaxis theprobability of the most probable translation.
Herethe probability of a target translation is calculatedas the count of the translation divided by the countof all translations of the source phrase.2.2 SynthesisThe synthesis module is based on the monolingualtext synthesis algorithm of Zhang (2013), whichconstructs an ordered dependency tree given a bagof words.
In the bilingual setting, inputs to the al-gorithm are translation options, which can be over-lapping and mutually exclusive, and not necessar-ily all of which are included in the output.
As aresult, the decoder needs to perform word selec-tion in addition to word ordering.
Another differ-ence between the bilingual and monolingual set-tings is that the former requires translation ade-quacy in addition to output fluency.We largely rely on the monolingual system forMT decoding.
To deal with overlapping transla-tion options, a source coverage vector is used toimpose mutual exclusiveness on input words andphrases.
Each element in the coverage vector isa binary value that indicates whether a particularsource word has been translated in the correspond-ing target hypothesis.
For translation adequacy,we use a set of bilingual features on top of the setof monolingual features for text synthesis.2.2.1 SearchThe search algorithm is the best-first algorithm ofZhang (2013).
Each search hypothesis is a par-tial or full target-language dependency tree, andhypotheses are constructed bottom-up from leafnodes, which are translation options.
An agendais used to maintain a list of search hypothesis tobe expanded, and a chart is used to record a setof accepted hypotheses.
Initially empty, the chartis a beam of size k ?
n, where n is the numberof source words and k is a positive integer.
Theagenda is a priority queue, initialized with all leafhypotheses (i.e.
translation options).
At each step,the highest-scored hypothesis e is popped off theagenda, and expanded by combination with all hy-potheses on the chart in all possible ways, withthe set of newly generated hypotheses e1, e2, ...eNbeing put onto the agenda, and e being put ontothe chart.
When two hypotheses are combined,they can be put in two different orders, and in eachcase different dependencies can be constructed be-tween their head words, leading to different new178dependency syntaxWORD(h) ?
POS(h) ?
NORM(size) ,WORD(h) ?
NORM(size), POS(h) ?
NORM(size)POS(h) ?
POS(m) ?
POS(b) ?
dirPOS(h) ?
POS(hl) ?
POS(m) ?
POS(mr) ?
dir (h > m),POS(h) ?
POS(hr) ?
POS(m) ?
POS(ml) ?
dir (h < m)WORD(h) ?
POS(m) ?
POS(ml) ?
dir ,WORD(h) ?
POS(m) ?
POS(mr) ?
dirPOS(h) ?
POS(m) ?
POS(m1) ?
dir ,POS(h) ?
POS(m1) ?
dir , POS(m) ?
POS(m1) ?
dirWORD(h) ?
POS(m) ?
POS(m1) ?
POS(m2) ?
dir ,POS(h) ?
POS(m) ?
POS(m1) ?
POS(m2) ?
dir ,...dependency syntax for completed wordsWORD(h) ?
POS(h) ?
WORD(hl) ?
POS(hl),POS(h) ?
POS(hl),WORD(h) ?
POS(h) ?
POS(hl),POS(h) ?
WORD(hl) ?
POS(hl) ,WORD(h) ?
POS(h) ?
WORD(hr) ?
POS(hr),POS(h) ?
POS(hr),...surface string patterns (B?bordering index)WORD(B ?
1) ?
WORD(B), POS(B ?
1) ?
POS(B),WORD(B ?
1) ?
POS(B), POS(B ?
1) ?
WORD(B),WORD(B ?
1) ?
WORD(B) ?
WORD(B + 1),WORD(B ?
2) ?
WORD(B ?
1) ?
WORD(B),POS(B ?
1) ?
POS(B) ?
POS(B + 1),...surface string patterns for complete sentencesWORD(0), WORD(0) ?
WORD(1),WORD(size ?
1),WORD(size ?
1) ?
WORD(size ?
2),POS(0), POS(0) ?
POS(1),POS(0) ?
POS(1) ?
POS(2),...Table 1: Monolingual feature templates.hypotheses.
The decoder expands a fixed numberL hypotheses, and then takes the highest-scoredchart hypothesis that contains over ?
?
n words asthe output, where ?
is a real number near 1.0.2.2.2 Model and trainingA scaled linear model is used by the decoder toscore search hypotheses:Score(e) =~?
?
?
(e)|e|,where ?
(e) is the global feature vector of the hy-pothesis e, ~?
is the parameter vector of the model,and |e| is the number of leaf nodes in e. Thescaling factor |e| is necessary because hypothe-ses with different numbers of words are comparedwith each other in the search process to capturetranslation equivalence.While the monolingual features of Zhang(2013) are applied (example feature templatesfrom the system are shown in Table 1), an addi-tional set of bilingual features is defined, shownphrase translation featuresPHRASE(m) ?
PHRASE(t), P (trans),bilingual syntactic featuresPOS(th) ?
POS(tm) ?
dir ?
LEN(path),WORD(th) ?
POS(tm) ?
dir ?
LEN(path),POS(th) ?
WORD(tm) ?
dir ?
LEN(path),WORD(th) ?
WORD(tm) ?
dir ?
LEN(path),WORD(sh) ?
WORD(sm) ?
dir ?
LEN(path),WORD(sh) ?
WORD(th) ?
dir ?
LEN(path),WORD(sm) ?
WORD(tm) ?
dir ?
LEN(path),bilingual syntactic features (LEN(path) ?
3)POS(th) ?
POS(tm) ?
dir ?
LABELS(path),WORD(th) ?
POS(tm) ?
dir ?
LABELS(path),POS(th) ?
WORD(tm) ?
dir ?
LABELS(path),WORD(th) ?
WORD(tm) ?
dir ?
LABELS(path),WORD(sh) ?
WORD(sm) ?
dir ?
LABELS(path),WORD(sh) ?
WORD(th) ?
dir ?
LABELS(path),WORD(sm) ?
WORD(tm) ?
dir ?
LABELS(path),POS(th) ?
POS(tm) ?
dir ?
LABELSPOS(path),WORD(th) ?
POS(tm) ?
dir ?
LABELSPOS(path),POS(th) ?
WORD(tm) ?
dir ?
LABELSPOS(path),WORD(th) ?
WORD(tm) ?
dir ?
LABELSPOS(path),WORD(sh) ?
WORD(sm) ?
dir ?
LABELSPOS(path),WORD(sh) ?
WORD(th) ?
dir ?
LABELSPOS(path),WORD(sm) ?
WORD(tm) ?
dir ?
LABELSPOS(path),Table 2: Bilingual feature templates.in Table 2.
In the tables, s and t represent thesource and target, respectively; h and m repre-sent the head and modifier in a dependency arc,respectively; hland hrrepresent the neighboringwords on the left and right of h, respectively; mland mrrepresent the neighboring words on the leftand right of m, respectively; m1and m2repre-sent the closest and second closest sibling of m onthe side of h, respectively.
dir represents the arcdirection (i.e.
left or right); PHRASE representsa lexical phrase; P(trans) represents the source-to-target translation probability from the phrase-table, used as a real-valued feature; path repre-sents the shortest path in the source dependencytree between the two nodes that correspond to thetarget head and modifier, respectively; LEN(path)represents the number of arcs on path, normalizedto bins of [5, 10, 20, 40+]; LABELS(path) repre-sents the array of dependency arc labels on path;LABELSPOS(path) represents the array of depen-dency arc labels and source POS on path.
In addi-tion, a real-valued four-gram language model fea-ture is also used, with four-grams extracted fromthe surface boundary when two hypothesis arecombined.We apply the discriminative learning algorithmof Zhang (2013) to train the parameters ~?.
The al-gorithm requires training examples that consist offull target derivations, with leaf nodes being inputtranslation options.
However, the readily available179training examples are automatically-parsed targetderivations, with leaf nodes being the referencetranslation.
As a result, we apply a search pro-cedure to find a derivation process, through whichthe target dependency tree is constructed from asubset of input translation options.
The searchprocedure can be treated as a constrained decod-ing process, where only the oracle tree and its subtrees can be constructed.
In case the set of transla-tion options cannot lead to the oracle tree, we ig-nore the training instance.2 Although the ignoredtraining sentence pairs cannot be utilized for train-ing the discriminative synthesizer, they are never-theless used for building the phrase table and train-ing the language model.3 ExperimentsWe perform experiments on the IWSLT 2010Chinese-English dataset, which consists of train-ing sentence pairs from the dialog task (dialog)and Basic Travel and Expression Corpus (BTEC).The union of dialog and BTEC are taken as ourtraining set, which contains 30,033 sentence pairs.For system tuning, we use the IWSLT 2004 test set(also released as the second development test setof IWSLT 2010), which contains 500 sentences.For final test, we use the IWSLT 2003 test set (alsoreleased as the first development test set of IWSLT2010), which contains 506 sentences.The Chinese sentences in the datasets are seg-mented using NiuTrans3 (Xiao et al., 2012), whilePOS-tagging of both English and Chinese is per-formed using ZPar4 version 0.5 (Zhang and Clark,2011).
We train the English POS-tagger using theWSJ sections of the Penn Treebank (Marcus et al.,1993), turned into lower-case.
For syntactic pars-ing of both English and Chinese, we use the de-fault models of ZPar 0.5.We choose three baseline systems: a string-to-tree (S2T) system, a tree-to-string (T2S) systemand a tree-to-tree (T2T) system (Koehn, 2010).The Moses release 1.0 implementations of allthree systems are used, with default parameter set-tings.
IRSTLM5 release 5.80.03 (Federico et al.,2008) is used to train a four-gram language models2This led to the ignoring of over 40% of the training sen-tence pairs.
For future work, we will consider substitute or-acles from reachable target derivations by using maximumsentence level BLEU approximation (Nakov et al., 2012) orMETEOR (Denkowski and Lavie, 2011) as selection criteria.3http://www.nlplab.com/NiuPlan/NiuTrans.ch.html4http://sourceforge.net/projects/zpar/5http://sourceforge.net/apps/mediawiki/irstlmSystem T2S S2T T2T OURSBLEU 32.65 36.07 28.46 34.24Table 3: Final results.SOURCE:????????
?REF: I have a terrible headache .OURS: now , I have a headache .SOURCE:?????????
?REF: I ?d like a twin room with a bath please .OURS: a twin room , I ?ll find a room with a bath .SOURCE:?????????
?REF: can you change yen into dollars ?OURS: please change yen into dollars .SOURCE:?????
?REF: roast chicken , please .OURS: please have roast chicken .SOURCE:????????
?REF: take two tablets after every meal .OURS: please eat after each meal .SOURCE:???
?REF: check , please .OURS: I have to check - out , please .SOURCE:????????????
?REF: yes , well , that ?s our specialty .OURS: ah , the food that ?s right .SOURCE:????
?REF: my air conditioner is n?t working .OURS: the air - conditioner does n?t work .Table 4: Sample output sentences.over the English training data, which is applied tothe baseline systems and our system.
Kneser-Neysmoothing is used to train the language model.We use the tuning set to determine the optimalnumber of training iterations.
The translation op-tion filter ?
is set to 0.1; the phrase size limit s isset to 5 in order to verify the effectiveness of syn-thesis; the number of expanded nodes L is set to200; the chart factor k is set to 16 for a balance be-tween efficiency and accuracy; the goal parameter?
is set to 0.8.The final scores of our system and the baselinesare shown in Table 3.
Our system gives a BLEUof 34.24, which is comparable to the baseline sys-tems.
Some example outputs are shown in Table 4.Manual comparison does not show significant dif-ferences in overall translation adequacy or fluencybetween the outputs of the four systems.
However,an observation is that, while our system can pro-duce more fluent outputs, the choice of translationoptions can be more frequently incorrect.
Thissuggests that while the target synthesis componentis effective under the bilingual setting, a strongerlexical selection component may be necessary forbetter translation quality.1804 Related workAs discussed in the introduction, our work isclosely related to previous studies on syntacticMT, with the salient difference that we do not relyon hard translation rules, but allow free target syn-thesis.
The contrast can be summarized as ?trans-lation by parsing?
vs ?translation by generation?.There has been a line of research on genera-tion for translation.
Soricut and Marcu (2006) usea form of weighted IDL-expressions (Nederhofand Satta, 2004) for generation.
Bangalore et al.
(2007) treats MT as a combination of global lex-ical transfer and word ordering; their generationcomponent does not perform lexical selection, re-lying on an n-gram language model to order targetwords.
Goto et al.
(2012) use a monotonic phrase-based system to perform target word selection, andtreats target ordering as a post-processing step.More recently, Chen et al.
(2014) translate sourcedependencies arc-by-arc to generate pseudo targetdependencies, and generate the translation by re-ordering of arcs.
In contrast with these systems,our system relies more heavily on a syntax-basedsynthesis component, in order to study the useful-ness of statistical NLG on SMT.With respect to syntax-based word ordering,Chang and Toutanova (2007) and He et al.
(2009)study a simplified word ordering problem by as-suming that the un-ordered target dependency treeis given.
Wan et al.
(2009) and Zhang and Clark(2011) study the ordering of a bag of words, with-out input syntax.
Zhang et al.
(2012), Zhang(2013) and Song et al.
(2014) further extended thisline of research by adding input syntax and allow-ing joint inflection and ordering.
de Gispert et al.
(2014) use a phrase-structure grammer for wordordering.
Our generation system is based on thework of Zhang (2013), but further allows lexicalselection.Our work is also in line with the work of Lianget al.
(2006), Blunsom et al.
(2008), Flanigan etal.
(2013) and Yu et al.
(2013) in that we build adiscriminative model for SMT.5 ConclusionWe investigated a novel system for syntactic ma-chine translation, treating MT as an unconstrainedgeneration task, solved by using a single discrim-inative model with both monolingual syntax andbilingual translation features.
Syntactic corre-spondence is captured by using soft features ratherthan hard translation rules, which are used by mostsyntax-based statistical methods in the literature.Our results are preliminary in the sense thatthe experiments were performed using a relativelysmall dataset, and little engineering effort wasmade on fine-tuning of parameters for the base-line and proposed models.
Our Python imple-mentation gives the same level of BLEU scorescompared with baseline syntactic SMT systems,but is an order of magnitude slower than Moses.However, the results demonstrate the feasibility ofleveraging text generation techniques for machinetranslation, directly connecting the two currentlyrather separated research fields.
The system is notstrongly dependent on the specific generation al-gorithm, and one potential of the SMT architec-ture is that it can directly benefit from advances instatistical NLG technology.AcknowledgementThe work has been supported by the Singa-pore Ministration of Education Tier 2 projectT2MOE201301 and the startup grant SRG ISTD2012 038 from SUTD.
We thank the anonymousreviewers for their constructive comments.ReferencesMichael Auli, Adam Lopez, Hieu Hoang, and PhilippKoehn.
2009.
A systematic analysis of translationmodel search spaces.
In Proc.
WMT, pages 224?232.Srinivas Bangalore, Patrick Haffner, and Stephan Kan-thak.
2007.
Statistical machine translation throughglobal lexical selection and sentence reconstruction.In Proc.
ACL, pages 152?159.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In Proc.
ACL, pages 200?208.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathe-matics of statistical machine translation: Parameterestimation.
Computational Linguistics, 19(2):263?311.Pi-Chuan Chang and Kristina Toutanova.
2007.
A dis-criminative syntactic word order model for machinetranslation.
In Proc.
ACL, pages 9?16.Hongshen Chen, Jun Xie, Fandong Meng, WenbinJiang, and Qun Liu.
2014.
A dependency edge-based transfer model for statistical machine transla-tion.
In Proc.
COLING 2014, pages 1103?1113.181David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Proc.ACL, pages 263?270.Adria` de Gispert, Marcus Tomalin, and Bill Byrne.2014.
Word ordering with phrase-based grammars.In Proc.
EACL, pages 259?268.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic metric for reliable optimization andevaluation of machine translation systems.
In Proc.WMT, pages 85?91.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: an open source toolkit forhandling large scale language models.
In Proc.
In-terspeech, pages 1618?1621.Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.2013.
Large-scale discriminative training for statis-tical machine translation using held-out line search.In Proc.
NAACL, pages 248?258.Heidi Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In Proc.
EMNLP, pages 304?311.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proc.
HLT-NAACL, pages 273?280.Isao Goto, Masao Utiyama, and Eiichiro Sumita.
2012.Post-ordering by parsing for Japanese-English sta-tistical machine translation.
In Proc.
ACL, pages311?316.Wei He, Haifeng Wang, Yuqing Guo, and Ting Liu.2009.
Dependency based Chinese sentence realiza-tion.
In Proc.
ACL/AFNLP, pages 809?816.Kevin Knight.
1999.
Squibs and Discussions: Decod-ing Complexity in Word-Replacement TranslationModels.
Computational Linguistics, 25(4):607?615.Phillip Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.P.
Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach tomachine translation.
In Proc.
COLING/ACL, pages761?768.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
COLING/ACL, pages 609?616.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical machinetranslation with syntactified target language phrases.In Proc.
EMNLP, pages 44?52.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: The penn treebank.
Com-putational linguistics, 19(2):313?330.Preslav Nakov, Francisco Guzman, and Stephan Vo-gel.
2012.
Optimizing for sentence-level BLEU+1yields short translations.
In Proc.
Coling, pages1979?1994.Mark-Jan Nederhof and Giorgio Satta.
2004.
Idl-expressions: a formalism for representing and pars-ing finite languages in natural language processing.J.
Artif.
Intell.
Res.
(JAIR), 21:287?317.Franz Josef Och, Christoph Tillmann, and HermannNey.
1999.
Improved alignment models for statis-tical machine translation.
In Proc.
EMNLP, pages20?28.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal smt.
In Proc.
ACL, pages 271?279.Libin Shen and Aravind Joshi.
2008.
LTAG depen-dency parsing with bidirectional incremental con-struction.
In Proc.
EMNLP, pages 495?504.Linfeng Song, Yue Zhang, Kai Song, and Qun Liu.2014.
Joint morphological generation and syntacticlinearization.
In Proc.
AAAI, pages 1522?1528.Radu Soricut and Daniel Marcu.
2006.
Stochastic lan-guage generation using widl-expressions and its ap-plication in machine translation and summarization.In Proc.
ACL, pages 1105?1112.Stephen Wan, Mark Dras, Robert Dale, and Ce?cileParis.
2009.
Improving grammaticality in statisti-cal sentence generation: Introducing a dependencyspanning tree algorithm with an argument satisfac-tion model.
In Proc.
EACL, pages 852?860.Michael White and Rajakrishnan Rajkumar.
2009.Perceptron reranking for CCG realization.
In Proc.the EMNLP, pages 410?419.Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li.2012.
NiuTrans: An open source toolkit for phrase-based and syntax-based machine translation.
InProc.
ACL Demos, pages 19?24.Jun Xie, Haitao Mi, and Qun Liu.
2011.
A noveldependency-to-string model for statistical machinetranslation.
In Proc.
EMNLP, pages 216?226.Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.2013.
Max-violation perceptron and forced decod-ing for scalable MT training.
In Proc.
EMNLP,pages 1112?1123.Yue Zhang and Stephen Clark.
2011.
Syntax-basedgrammaticality improvement using CCG and guidedsearch.
In Proc.
EMNLP, pages 1147?1157.Yue Zhang, Graeme Blackwood, and Stephen Clark.2012.
Syntax-based word ordering incorporating alarge-scale language model.
In Proc.
EACL, pages736?746.Yue Zhang.
2013.
Partial-tree linearization: General-ized word ordering for text synthesis.
In Proc.
IJ-CAI, pages 2232?2238.182
