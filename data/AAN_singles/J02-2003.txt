c?
2002 Association for Computational LinguisticsClass-Based Probability Estimation Usinga Semantic HierarchyStephen Clark?
David Weir?University of Edinburgh University of SussexThis article concerns the estimation of a particular kind of probability, namely, the probabilityof a noun sense appearing as a particular argument of a predicate.
In order to overcome theaccompanying sparse-data problem, the proposal here is to define the probabilities in terms ofsenses from a semantic hierarchy and exploit the fact that the senses can be grouped into classesconsisting of semantically similar senses.
There is a particular focus on the problem of howto determine a suitable class for a given sense, or, alternatively, how to determine a suitablelevel of generalization in the hierarchy.
A procedure is developed that uses a chi-square test todetermine a suitable level of generalization.
In order to test the performance of the estimationmethod, a pseudo-disambiguation task is used, together with two alternative estimation methods.Each method uses a different generalization procedure; the first alternative uses the minimumdescription length principle, and the second uses Resnik?s measure of selectional preference.
Inaddition, the performance of our method is investigated using both the standard Pearson chi-square statistic and the log-likelihood chi-square statistic.1.
IntroductionThis article concerns the problem of how to estimate the probabilities of noun sensesappearing as particular arguments of predicates.
Such probabilities can be useful for avariety of natural language processing (NLP) tasks, such as structural disambiguationand statistical parsing, word sense disambiguation, anaphora resolution, and languagemodeling.
To see how such knowledge can be used to resolve structural ambiguities,consider the following prepositional phrase attachment ambiguity:Example 1Fred ate strawberries with a spoon.The ambiguity arises because the prepositional phrase with a spoon can attach to eitherstrawberries or ate.
The ambiguity can be resolved by noting that the correct sense ofspoon is more likely to be an argument of ?ate-with?
than ?strawberries-with?
(Li andAbe 1998; Clark and Weir 2000).The problem with estimating a probability model defined over a large vocabularyof predicates and noun senses is that this involves a huge number of parameters,which results in a sparse-data problem.
In order to reduce the number of parameters,we propose to define a probability model over senses in a semantic hierarchy and?
Division of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LW, UK.
E-mail:stephenc@cogsci.ed.ac.uk.?
School of Cognitive and Computing Sciences, University of Sussex, Brighton, BN1 9QH, UK.
E-mail:david.weir@cogs.susx.ac.uk.188Computational Linguistics Volume 28, Number 2to exploit the fact that senses can be grouped into classes consisting of semanticallysimilar senses.
The assumption underlying this approach is that the probability of aparticular noun sense can be approximated by a probability based on a suitably chosenclass.
For example, it seems reasonable to suppose that the probability of (the foodsense of) chicken appearing as an object of the verb eat can be approximated in someway by a probability based on a class such as FOOD.There are two elements involved in the problem of using a class to estimate theprobability of a noun sense.
First, given a suitably chosen class, how can that classbe used to estimate the probability of the sense?
And second, given a particular nounsense, how can a suitable class be determined?
This article offers novel solutions toboth problems, and there is a particular focus on the second question, which can bethought of as how to find a suitable level of generalization in the hierarchy.1The semantic hierarchy used here is the noun hierarchy of WordNet (Fellbaum1998), version 1.6.
Previous work has considered how to estimate probabilities us-ing classes from WordNet in the context of acquiring selectional preferences (Resnik1998; Ribas 1995; Li and Abe 1998; McCarthy 2000), and this previous work has alsoaddressed the question of how to determine a suitable level of generalization in thehierarchy.
Li and Abe use the minimum description length principle to obtain a levelof generalization, and Resnik uses a simple technique based on a statistical measureof selectional preference.
(The work by Ribas builds on that by Resnik, and the workby McCarthy builds on that by Li and Abe.)
We compare our estimation method withthose of Resnik and Li and Abe, using a pseudo-disambiguation task.
Our methodoutperforms these alternatives on the pseudo-disambiguation task, and an analysis ofthe results shows that the generalization methods of Resnik and Li and Abe appearto be overgeneralizing, at least for this task.Note that the problem being addressed here is the engineering problem of es-timating predicate argument probabilities, with the aim of producing estimates thatwill be useful for NLP applications.
In particular, we are not addressing the problemof acquiring selectional restrictions in the way this is usually construed (Resnik 1993;Ribas 1995; McCarthy 1997; Li and Abe 1998; Wagner 2000).
The purpose of using asemantic hierarchy for generalization is to overcome the sparse data problem, ratherthan find a level of abstraction that best represents the selectional restrictions of somepredicate.
This point is considered further in Section 5.The next section describes the noun hierarchy from WordNet and gives a moreprecise description of the probabilities to be estimated.
Section 3 shows how a classfrom WordNet can be used to estimate the probability of a noun sense.
Section 4 showshow a chi-square test is used as part of the generalization procedure, and Section 5describes the generalization procedure.
Section 6 describes the alternative class-basedestimation methods used in the pseudo-disambiguation experiments, and Section 7presents those experiments.2.
The Semantic HierarchyThe noun hierarchy of WordNet consists of senses, or what Miller (1998) calls lexicalizedconcepts, organized according to the ?is-a-kind-of?
relation.
Note that we are usingconcept to refer to a lexicalized concept or sense and not to a set of senses; we use class torefer to a set of senses.
There are around 66,000 different concepts in the noun hierarchy1 A third element of the problem, namely, how to obtain arguments of predicates as training data, is notconsidered here.
We assume the existence of such data, obtained from a treebank or shallow parser.189Clark and Weir Class-Based Probability Estimationof WordNet version 1.6.
A concept in WordNet is represented by a ?synset,?
which isthe set of synonymous words that can be used to denote that concept.
For example,the synset for the concept ?cocaine?2 is { cocaine, cocain, coke, snow, C }.
Let syn(c) be thesynset for concept c, and let cn(n) = { c |n ?
syn(c) } be the set of concepts that can bedenoted by noun n.The hierarchy has the structure of a directed acyclic graph (although only around1% of the nodes have more than one parent), where the edges of the graph constitutewhat we call the ?direct?isa?
relation.
Let isa be the transitive, reflexive closure ofdirect?isa; then c?
isa c implies c?
is a kind of c. If c?
isa c, then c is a hypernym of c?
andc?
is a hyponym of c. In fact, the hierarchy is not a single hierarchy but instead consists ofnine separate subhierarchies, each headed by the most general kind of concept, such as?entity?, ?abstraction?, ?event?, and ?psychological feature?.
For the purposes of this workwe add a common root dominating the nine subhierarchies, which we denote ?root?.There are some important points that need to be clarified regarding the hierarchy.First, every concept in the hierarchy has a nonempty synset (except the notional con-cept ?root?).
Even the most general concepts, such as ?entity?, can be denoted by somenoun; the synset for ?entity?
is { entity, something }.
Second, there is an important distinc-tion between an individual concept and a set of concepts.
For example, the individualconcept ?entity?
should not be confused with the set or class consisting of conceptsdenoting kinds of entities.
To make this distinction clear, we use c = { c?
| c?
isa c }to denote the set of concepts dominated by concept c, including c itself.
For exam-ple, ?animal?
is the set consisting of those concepts corresponding to kinds of animals(including ?animal?
itself).The probability of a concept appearing as an argument of a predicate is written p(c |v, r), where c is a concept in WordNet, v is a predicate, and r is an argument position.3The focus in this article is on the arguments of verbs, but the techniques discussedcan be applied to any predicate that takes nominal arguments, such as adjectives.
Theprobability p(c | v, r) is to be interpreted as follows: This is the probability that somenoun n in syn(c), when denoting concept c, appears in position r of verb v (givenv and r).
The example used throughout the article is p(?dog?
| run, subj), which isthe conditional probability that some noun in the synset of ?dog?, when denoting theconcept ?dog?, appears in the subject position of the verb run.
Note that, in practice,no distinction is made between the different senses of a verb (although the techniquesdo allow such a distinction) and that each use of a noun is assumed to correspond toexactly one concept.43.
Class-Based Probability EstimationThis section explains how a set of concepts, or class, from WordNet can be used toestimate the probability of an individual concept.
More specifically, we explain howa set of concepts c?, where c?
is some hypernym of concept c, can be used to estimatep(c | v, r).
(Recall that c?
denotes the set of concepts dominated by c?, including c?
itself.
)One possible approach would be simply to substitute c?
for the individual concept c.This is a poor solution, however, since p(c?
| v, r) is the conditional probability that2 Angled brackets are used to denote concepts in the hierarchy.3 The term predicate is used loosely here, in that the predicate does not have to be a semantic object butcan simply be a word form.4 A recent paper that extends the acquisition of selectional preferences to sense-sense relationships isAgirre and Martinez (2001).190Computational Linguistics Volume 28, Number 2some noun denoting a concept in c?
appears in position r of verb v. For example,p(?animal?
| run, subj) is the probability that some noun denoting a kind of animalappears in the subject position of the verb run.
Probabilities of sets of concepts areobtained by summing over the concepts in the set:p(c?
| v, r) =?c???c?p(c??
| v, r) (1)This means that p(?animal?
| run, subj) is likely to be much greater than p(?dog?
|run, subj) and thus is not a good approximation of p(?dog?
| run, subj).What can be done, though, is to condition on sets of concepts.
If it can be shownthat p(v | c?, r), for some hypernym c?
of c, is a reasonable approximation of p(v | c, r),then we have a way of estimating p(c | v, r).
The probability p(v | c, r) can be obtainedfrom p(c | v, r) using Bayes?
theorem:p(c | v, r) = p(v | c, r) p(c | r)p(v | r) (2)Since p(c | r) and p(v | r) are conditioned on the argument slot only, we assumethese can be estimated satisfactorily using relative frequency estimates.
Alternatively,a standard smoothing technique such as Good-Turing could be used.5 This leaves p(v |c, r).
Continuing with the ?dog?
example, the proposal is to estimate p(run | ?dog?, subj)using a relative-frequency estimate of p(run | ?animal?, subj) or an estimate based on asimilar, suitably chosen class.
Thus, assuming this choice of class, p(?dog?
| run, subj)would be approximated as follows:p(?dog?
| run, subj) ?
p(run | ?animal?, subj)p(?dog?
| subj)p(run | subj) (3)The following derivation shows that if p(v | c?i , r) = k for each child c?i of c?, andp(v | c?, r) = k, then p(v | c?, r) is also equal to k:p(v | c?, r) = p(c?
| v, r) p(v | r)p(c?
| r)(4)=p(v | r)p(c?
| r)(?ip(c?i | v, r) + p(c?
| v, r))(5)=p(v | r)p(c?
| r)(?ip(v | c?i , r)p(c?i | r)p(v | r) + p(v | c?, r)p(c?
| r)p(v | r))(6)=1p(c?
| r)(?ik p(c?i | r) + k p(c?
| r))(7)=kp(c?
| r)(?ip(c?i | r) + p(c?
| r))(8)= k (9)5 Unsmoothed estimates were used in this work.191Clark and Weir Class-Based Probability EstimationNote that the proof applies only to a tree, since the proof assumes that c?
is partitionedby c?
and the sets of concepts dominated by each of the daughters of c?, which is notnecessarily true for a directed acyclic graph (DAG).
WordNet is a DAG but is a closeapproximation to a tree, and so we assume this will not be a problem in practice.6The derivation in (4)?
(9) shows how probabilities conditioned on sets of conceptscan remain constant when moving up the hierarchy, and this suggests a way of findinga suitable set, c?, as a generalization for concept c: Initially set c?
equal to c and moveup the hierarchy, changing the value of c?, until there is a significant change in p(v |c?, r).
Estimates of p(v | c?i , r), for each child c?i of c?, can be compared to see whetherp(v | c?, r) has significantly changed.
(We ignore the probability p(v | c?, r) and considerthe probabilities p(v | c?i , r) only.)
Note that this procedure rests on the assumption thatp(v | c, r) is close to p(v | c, r).
(In fact, p(v | c, r) is equal to p(v | c, r) when c is a leafnode.)
So when finding a suitable level for the estimation of p(?sandwich?
| eat, obj),for example, we first assume that p(eat | ?sandwich?, obj) is a good approximation ofp(eat | ?sandwich?, obj) and then apply the procedure to p(eat | ?sandwich?, obj).A feature of the proposed generalization procedure is that comparing probabilitiesof the form p(v | C, r), where C is a class, is closely related to comparing ratios ofprobabilities of the form p(C | v, r)/p(C | r) (for a given verb and argument position):p(v | C, r) = p(C | v, r)p(C | r) p(v | r) (10)Note that, for a given verb and argument position, p(v | r) is constant across classes.Equation (10) is of interest because the ratio p(C | v, r)/p(C | r) can be interpreted as ameasure of association between the verb v and class C. This ratio is similar to point-wise mutual information (Church and Hanks 1990) and also forms part of Resnik?sassociation score, which will be introduced in Section 6.
Thus the generalization pro-cedure can be thought of as one that finds ?homogeneous?
areas of the hierarchy,that is, areas consisting of classes that are associated to a similar degree with the verb(Clark and Weir 1999).Finally, we note that the proposed estimation method does not guarantee that theestimates form a probability distribution over the concepts in the hierarchy, and so anormalization factor is required:psc(c | v, r) =p?
(v | [c, v, r], r) p?(c|r)p?(v|r)?c?
?C p?
(v | [c?, v, r], r)p?(c?|r)p?
(v|r)(11)We use psc to denote an estimate obtained using our method (since the techniquefinds sets of semantically similar senses, or ?similarity classes?)
and [c, v, r] to denotethe class chosen for concept c in position r of verb v; p?
denotes a relative frequencyestimate, and C denotes the set of concepts in the hierarchy.Before providing the details of the generalization procedure, we give the relative-frequency estimates of the relevant probabilities and deal with the problem of am-6 Li and Abe (1998) also develop a theoretical framework that applies only to a tree and turn WordNetinto a tree by copying each subgraph with multiple parents.
One way to extend the experiments inSection 7 would be to investigate whether this transformation has an impact on the results of thoseexperiments.192Computational Linguistics Volume 28, Number 2biguous data.
The relative-frequency estimates are as follows:p?
(c | r) = f (c,r)f (r) =?v?
?V f (c, v?, r)?v??V?c?
?C f (c?, v?, r)(12)p?
(v | r) = f (v,r)f (r) =?c?
?C f (c?, v, r)?v??V?c?
?C f (c?, v?, r)(13)p?
(v | c?, r) = f (c?,v,r)f (c?,r)=?c???c?
f (c?
?, v, r)?v??V?c???c?
f (c?
?, v?, r)(14)where f (c, v, r) is the number of (n, v, r) triples in the data in which n is being used todenote c, and V is the set of verbs in the data.
The problem is that the estimates aredefined in terms of frequencies of senses, whereas the data are assumed to be in theform of (n, v, r) triples: a noun, verb, and argument position.
All the data used in thiswork have been obtained from the British National Corpus (BNC), using the systemof Briscoe and Carroll (1997), which consists of a shallow-parsing component that isable to identify verbal arguments.We take a simple approach to the problem of estimating the frequencies of senses,by distributing the count for each noun in the data evenly among all senses of thenoun:f?
(c, v, r) =?n?syn(c)f (n, v, r)|cn(n)| (15)where f?
(c, v, r) is an estimate of the number of times that concept c appears in positionr of verb v, and |cn(n)| is the cardinality of cn(n).
This is the approach taken byLi and Abe (1998), Ribas (1995), and McCarthy (2000).7 Resnik (1998) explains howthis apparently crude technique works surprisingly well.
Alternative approaches aredescribed in Clark and Weir (1999) (see also Clark [2001]), Abney and Light (1999),and Ciaramita and Johnson (2000).4.
Using a Chi-Square Test to Compare ProbabilitiesIn this section we show how to test whether p(v | c?, r) changes significantly whenconsidering a node higher in the hierarchy.
Consider the problem of deciding whetherp(run | ?canine?, subj) is a good approximation of p(run | ?dog?, subj).
(?canine?
is theparent of ?dog?
in WordNet.)
To do this, the probabilities p(run | c?i , subj) are comparedusing a chi-square test, where the c?i are the children of ?canine?.
In this case, the nullhypothesis of the test is that the probabilities p(run | ci, subj) are the same for eachchild ci.
By judging the strength of the evidence against the null hypothesis, howsimilar the true probabilities are likely to be can be determined.
If the test indicatesthat the probabilities are sufficiently unlikely to be the same, then the null hypothesisis rejected, and the conclusion is that p(run | ?canine?, subj) is not a good approximationof p(run | ?dog?, subj).An example contingency table, based on counts obtained from a subset of the BNCusing the system of Briscoe and Carroll, is given in Table 1.
(Recall that the frequenciesare estimated by distributing the count for a noun equally among the noun?s senses;this explains the fractional counts.)
One column contains estimates of counts arising7 Resnik takes a similar approach but divides the count evenly among the noun?s senses and all thehypernyms of those senses.193Clark and Weir Class-Based Probability EstimationTable 1Contingency table for the children of ?canine?
in the subject position of run.ci f?
(ci, run, subj) f?
(ci, subj) ?
f?
(ci, run, subj) f?
(ci, subj) =?v?V f?
(ci, v, subj)?bitch?
0.3 (0.5) 26.7 (26.6) 27.0?dog?
12.8 (10.5) 620.4 (622.7) 633.2?wolf?
0.3 (0.6) 38.7 (38.4) 39.0?jackal?
0.0 (0.3) 20.0 (19.7) 20.0?wild dog?
0.0 (0.0) 3.0 (3.0) 3.0?hyena?
0.0 (0.2) 10.0 (9.8) 10.0?fox?
0.0 (1.2) 72.3 (71.1) 72.313.4 791.1 804.5from concepts in ci appearing in the subject position of the verb run: f?
(ci, run, subj).
Asecond column presents estimates of counts arising from concepts in ci appearing inthe subject position of a verb other than run.
The figures in brackets are the expectedvalues if the null hypothesis is true.There is a choice of which statistic to use in conjunction with the chi-square test.The usual statistic encountered in textbooks is the Pearson chi-square statistic, de-noted X2:X2 =?i,j(oij ?
eij)2eij(16)where oij is the observed value for the cell in row i and column j, and eij is thecorresponding expected value.
An alternative statistic is the log-likelihood chi-squarestatistic, denoted G2:8G2 = 2?i,joij logeoijeij(17)The two statistics have similar values when the counts in the contingency table arelarge (Agresti 1996).
The statistics behave differently, however, when the table containslow counts, and, since corpus data are likely to lead to some low counts, the questionof which statistic to use is an important one.
Dunning (1993) argues for the use of G2rather than X2, based on an analysis of the sampling distributions of G2 and X2, andresults obtained when using the statistics to acquire highly associated bigrams.
Weconsider Dunning?s analysis at the end of this section, and the question of whether touse G2 or X2 will be discussed further there.
For now, we continue with the discussionof how the chi-square test is used in the generalization procedure.For Table 1, the value of G2 is 3.8, and the value of X2 is 2.5.
Assuming a level ofsignificance of ?
= 0.05, the critical value is 12.6 (for six degrees of freedom).
Thus,for this ?
value, the null hypothesis would not be rejected for either statistic, and theconclusion would be that there is no reason to suppose that p(run | ?canine?, subj) isnot a reasonable approximation of p(run | ?dog?, subj).8 An alternative formula for G2 is given in Dunning (1993), but the two are equivalent.194Computational Linguistics Volume 28, Number 2Table 2Contingency table for the children of ?liquid?
in the object position of drink.ci f?
(ci, drink, obj) f?
(ci, obj) ?
f?
(ci, drink, obj) f?
(ci, obj) =?v?V f?
(ci, v, obj)?beverage?
261.0 (238.7) 2,367.7 (2,390.0) 2,628.7?supernatant?
0.0 (0.1) 1.0 (0.9) 1.0?alcohol?
11.5 (9.4) 92.0 (94.1) 103.5?ammonia?
0.0 (0.8) 8.5 (7.7) 8.5?antifreeze?
0.0 (0.1) 1.0 (0.9) 1.0?distillate?
0.0 (0.5) 6.0 (5.5) 6.0?water?
12.0 (31.6) 335.7 (316.1) 347.7?ink?
0.0 (2.9) 32.0 (29.1) 32.0?liquor?
0.7 (1.1) 11.6 (11.2) 12.3285.2 2,855.5 3,140.7As a further example, Table 2 gives counts for the children of ?liquid?
in the objectposition of drink.
Again, the counts have been obtained from a subset of the BNCusing the system of Briscoe and Carroll.
Not all the sets dominated by the children of?liquid?
are shown, as some, such as ?sheep dip?, never appear in the object positionof a verb in the data.
This example is designed to show a case in which the nullhypothesis is rejected.
The value of G2 for this table is 29.0, and the value of X2 is21.2.
So for G2, even if an ?
value as low as 0.0005 were being used (for which thecritical value is 27.9 for eight degrees of freedom), the null hypothesis would still berejected.
For X2, the null hypothesis is rejected for ?
values greater than 0.005.
Thisseems reasonable, since the probabilities associated with the children of ?liquid?
andthe object position of drink would be expected to show a lot of variation across thechildren.A key question is how to select the appropriate value for ?.
One solution is totreat ?
as a parameter and set it empirically by taking a held-out test set and choosingthe value of ?
that maximizes performance on the relevant task.
For example, Clarkand Weir (2000) describes a prepositional phrase attachment algorithm that employsprobability estimates obtained using the WordNet method described here.
To set thevalue of ?, the performance of the algorithm on a development set could be com-pared across different values of ?, and the value that leads to the best performancecould be chosen.
Note that this approach sets no constraints on the value of ?
: Thevalue could be as high as 0.995 or as low as 0.0005, depending on the particularapplication.There may be cases in which the conditions for the appropriate application of a chi-square test are not met.
One condition that is likely to be violated is the requirementthat expected values in the contingency table not be too small.
(A rule of thumboften found in textbooks is that the expected values should be greater than five.)
Oneresponse to this problem is to apply some kind of thresholding and either ignorecounts below the threshold, or apply the test only to tables that do not contain lowcounts.
Ribas (1995), Li and Abe (1998), McCarthy (2000), and Wagner (2000) all usesome kind of thresholding when dealing with counts in the hierarchy (although not inthe context of a chi-square test).
Another approach would be to use Fisher?s exact test(Agresti 1996; Pedersen 1996), which can be applied to tables regardless of the size of195Clark and Weir Class-Based Probability Estimationthe counts they contain.
The main problem with this test is that it is computationallyexpensive, especially for large contingency tables.What we have found in practice is that applying the chi-square test to tables dom-inated by low counts tends to produce an insignificant result, and the null hypothesisis not rejected.
The consequences of this for the generalization procedure are thatlow-count tables tend to result in the procedure moving up to the next node in thehierarchy.
But given that the purpose of the generalization is to overcome the sparse-data problem, moving up a node is desirable, and therefore we do not modify the testfor tables with low counts.The final issue to consider is which chi-square statistic to use.
Dunning (1993)argues for the use of G2 rather than X2, based on the claim that the sampling distri-bution of G2 approaches the true chi-square distribution quicker than the samplingdistribution of X2.
However, Agresti (1996, page 34) makes the opposite claim: ?Thesampling distributions of X2 and G2 get closer to chi-squared as the sample size nincreases.
.
.
.
The convergence is quicker for X2 than G2.
?In addition, Pedersen (2001) questions whether one statistic should be preferredover the other for the bigram acquisition task and cites Cressie and Read (1984), whoargue that there are some cases where the Pearson statistic is more reliable than thelog-likelihood statistic.
Finally, the results of the pseudo-disambiguation experimentspresented in Section 7 are at least as good, if not better, when using X2 rather than G2,and so we conclude that the question of which statistic to use should be answered ona per application basis.5.
The Generalization ProcedureThe procedure for finding a suitable class, c?, to generalize concept c in position rof verb v works as follows.
(We refer to c?
as the ?similarity class?
of c with respectto v and r and the hypernym c?
as top(c, v, r), since the chosen hypernym sits at the?top?
of the similarity class.)
Initially, concept c is assigned to a variable top.
Then,by working up the hierarchy, successive hypernyms of c are assigned to top, and thisprocess continues until the probabilities associated with the sets of concepts dominatedby top and the siblings of top are significantly different.
Once a node is reached thatresults in a significant result for the chi-square test, the procedure stops, and top isreturned as top(c, v, r).
In cases where a concept has more than one parent, the parentis chosen that results in the lowest value of the chi-square statistic, as this indicatesthe probabilities are the most similar.
The set top(c, v, r) is the similarity class of c forverb v and position r. Figure 1 gives an algorithm for determining top(c, v, r).Figure 2 gives an example of the procedure at work.
Here, top(?soup?, stir, obj) isbeing determined.
The example is based on data from a subset of the BNC, with 303cases of an argument in the object position of stir.
The G2 statistic is used, together withan ?
value of 0.05.
Initially, top is set to ?soup?, and the probabilities correspondingto the children of ?dish?
are compared: p(stir | ?soup?, obj), p(stir | ?lasagne?, obj), p(stir |?haggis?, obj), and so on for the rest of the children.
The chi-square test results in a G2value of 14.5, compared to a critical value of 55.8.
Since G2 is less than the critical value,the procedure moves up to the next node.
This process continues until a significantresult is obtained, which first occurs at ?substance?
when comparing the children of?object?.
Thus ?substance?
is the chosen level of generalization.Now we show how the chosen level of generalization varies with ?
and how itvaries with the size of the data set.
A note of clarification is required before presentingthe results.
In related work on acquiring selectional preferences (Ribas 1995; McCarthy196Computational Linguistics Volume 28, Number 2Algorithm top(c, v, r):top ?
csig result ?
falsecomment parentmin gives lowest G2 value, G2minwhile not sig result & top = ?root?
doG2min ?
?for all parents of top docalculate G2 for sets dominated by children of parentif G2 < G2minthen G2min ?
G2parentmin ?
parentendif chi-square test for parentmin is significantthen sig result ?
trueelse move up to next node: top ?
parentminendreturn topFigure 1An algorithm for determining top(c, v, r).haggislasagnedishnourishmentfoodfare beveragecoursemealsubstanceobjectuid poisonartifactgroundentitysoupG2: 14:5, critical value: 55:8G2: 5:4, crit val: 16:9G2: 5:5, crit val: 16:9G2: 29:9, crit val: 58:1G2: 141:1, crit val: 37:7Figure 2An example generalization: Determining top(?soup?, stir, obj).197Clark and Weir Class-Based Probability Estimation1997; Li and Abe 1998; Wagner 2000), the level of generalization is often determined fora small number of hand-picked verbs and the result compared with the researcher?sintuition about the most appropriate level for representing a selectional preference.According to this approach, if ?sandwich?
were chosen to represent ?hotdog?
in theobject position of eat, this might be considered an undergeneralization, since ?food?might be considered more appropriate.
For this work we argue that such an evaluationis not appropriate; since the purpose of this work is probability estimation, the mostappropriate level is the one that leads to the most accurate estimate, and this may ormay not agree with intuition.
Furthermore, we show in Section 7 that to generalizeunnecessarily can be harmful for some tasks: If we already have lots of data regarding?sandwich?, why generalize any higher?
Thus the purpose of this section is not to showthat the acquired levels are ?correct,?
but simply to show how the levels vary with ?and the sample size.To show how the level of generalization varies with changes in ?, top(c, v, obj)was determined for a number of hand-picked (c, v, obj) triples over a range of valuesfor ?.
The triples were chosen to give a range of strongly and weakly selecting verbsand a range of verb frequencies.
The data were again extracted from a subset of theBNC using the system of Briscoe and Carroll (1997), and the G2 statistic was used inthe chi-square test.
The results are shown in Table 3.
The number of times the verboccurred with some object is also given in the table.The results suggest that the generalization level becomes more specific as ?
in-creases.
This is to be expected, since, given a contingency table chosen at random, ahigher value of ?
is more likely to lead to a significant result than a lower value of ?.We also see that, for some cases, the value of ?
has little effect on the level.
We wouldexpect there to be less change in the level of generalization for strongly selecting verbs,such as drink and eat, and a greater range of levels for weakly selecting verbs suchas see.
This is because any significant difference in probabilities is likely to be moremarked for a strongly selecting verb, and likely to be significant over a wider rangeof ?
values.
The table only provides anecdotal evidence, but provides some supportto this argument.To investigate more generally how the level of generalization varies with changesin ?, and also with changes in sample size, we took 6, 000 (c, v, obj) triples and calcu-lated the difference in depth between c and top(c, v, r) for each triple.
The 6, 000 tripleswere taken from the first experimental test set described in Section 7, and the train-ing data from this experiment were used to provide the counts.
(The test set containsnouns, rather than noun senses, and so the sense of the noun that is most probablegiven the verb and object slot was used.)
An average difference in depth was thencalculated.
To give an example of how the difference in depth was calculated, sup-pose ?dog?
generalized to ?placental mammal?
via ?canine?
and ?carnivore?
; in this casethe difference would be three.The results for various levels of ?
and different sample sizes are shown in Table 4.The figures in each column arise from using the contingency tables based on thecomplete training data, but with each count in the table multiplied by the percentageat the head of the column.
Thus the 50% column is based on contingency tables inwhich each original count is multiplied by 50%, which is equivalent to using a sampleone-half the size of the original training set.
Reading across a row shows how thegeneralization varies with sample size, and reading down a column shows how itvaries with ?.
The results show clearly that the extent of generalization decreaseswith an increase in the value of ?, supporting the trend observed in Table 3.
Theresults also show that the extent of generalization increases with a decrease in sample198Computational Linguistics Volume 28, Number 2Table 3Example levels of generalization for different values of ?.
(c, v, r), f(v, r) ?
(?coffee?, drink, obj) 0.0005 ?coffee??BEVERAGE??food?
.
.
.
?object?
?entity?0.05 ?coffee??BEVERAGE??food?
.
.
.
?object?
?entity?f (drink, obj) = 849 0.5 ?coffee??BEVERAGE??food?
.
.
.
?object?
?entity?0.995 ?coffee??BEVERAGE??food?
.
.
.
?object??entity?
(?hotdog?, eat, obj) 0.0005 ?hotdog??sandwich?
?snack food??DISH?
.
.
.
?food?
.
.
.
?entity?0.05 ?hotdog??sandwich?
?snack food??DISH?
.
.
.
?food?
.
.
.
?entity?f (eat, obj) = 1,703 0.5 ?hotdog??sandwich?
?snack food??DISH?
.
.
.
?food?
.
.
.
?entity?0.995 ?hotdog??SANDWICH?
?snack food??dish?
.
.
.
?food?
.
.
.
?entity?
(?Socrates?, kiss, obj) 0.0005 ?Socrates?
.
.
.
?person?
?life form?
?CAUSAL AGENT?
?entity?0.05 ?Socrates?
.
.
.
?person?
?life form?
?CAUSAL AGENT?
?entity?f (kiss, obj) = 345 0.5 ?Socrates?
.
.
.
?person?
?life form?
?CAUSAL AGENT?
?entity?0.995 ?Socrates?
.
.
.
?PERSON?
?life form?
?causal agent??entity?
(?dream?, remember, obj) 0.0005 ?dream?
.
.
.
?preoccupation?
?cognitive state?
?STATE?0.05 ?dream?
.
.
.
?preoccupation?
?cognitive state?
?STATE?f (remember, obj) = 1,982 0.5 ?dream?
.
.
.
?preoccupation?
?COGNITIVE STATE?
?state?0.995 ?dream?
.
.
.
?PREOCCUPATION?
?cognitive state??state?
(?man?, see, obj) 0.0005 ?man?
.
.
.
?mammal?
.
.
.
?ANIMAL?
?life form?
?entity?0.05 ?man?
.
.
.
?MAMMAL?
.
.
.
?animal?
?life form?
?entity?f (see, obj) = 16,757 0.5 ?man?
.
.
.
?MAMMAL?
.
.
.
?animal?
?life form?
?entity?0.995 ?MAN?
.
.
.
?mammal?
.
.
.
?animal?
?life form??entity?
(?belief?, abandon, obj) 0.0005 ?belief?
?mental object??cognition?
?PSYCHOLOGICAL FEATURE?0.05 ?belief?
?MENTAL OBJECT??cognition?
?psychological feature?f (abandon, obj) = 673 0.5 ?BELIEF?
?mental object??cognition?
?psychological feature?0.995 ?BELIEF?
?mental object??cognition?
?psychological feature?
(?nightmare?, have, obj) 0.0005 ?nightmare??dreaming??IMAGINATION?
.
.
.
?psychological feature?0.05 ?nightmare??dreaming??IMAGINATION?
.
.
.
?psychological feature?f (have, obj) = 93,683 0.5 ?nightmare??DREAMING??imagination?
.
.
.
?psychological feature?0.995 ?nightmare??DREAMING??imagination?
.
.
.
?psychological feature?Note: The selected level is shown in upper case.Table 4Extent of generalization for different values of ?
and sample sizes.?
100% 50% 10% 1%0.0005 3.3 3.9 5.0 5.60.05 2.8 3.5 4.6 5.60.5 2.1 2.9 4.1 5.40.995 1.2 1.5 2.6 3.9size.
Again, this is to be expected, since any difference in probability estimates is lesslikely to be significant for tables with low counts.6.
Alternative Class-Based Estimation MethodsThe approaches used for comparison are that of Resnik (1993, 1998), subsequentlydeveloped by Ribas (1995), and that of Li and Abe (1998), which has been adopted byMcCarthy (2000).
These have been chosen because they directly address the questionof how to find a suitable level of generalization in WordNet.199Clark and Weir Class-Based Probability EstimationThe first alternative uses the ?association score,?
which is a measure of how wella set of concepts, C, satisfies the selectional preferences of a verb, v, for an argumentposition, r:9A(C, v, r) = p(C | v, r) log2p(C | v, r)p(C | r) (18)An estimate of the association score, A?
(C, v, r), can be obtained using relative frequencyestimates of the probabilities.
The key question is how to determine a suitable level ofgeneralization for concept c, or, alternatively, how to find a suitable class to representconcept c (assuming the choice is from those classes that contain all concepts dom-inated by some hypernym of c).
Resnik?s solution to this problem (which he neatlyrefers to as the ?vertical-ambiguity?
problem) is to choose the class that maximizesthe association score.It is not clear that the class with the highest association score is always the mostappropriate level of generalization.
For example, this approach does not always gen-eralize appropriately for arguments that are negatively associated with some verb.
Tosee why, consider the problem of deciding how well the concept ?location?
satisfies thepreferences of the verb eat for its object.
Since locations are not the kinds of things thatare typically eaten, a suitable level of generalization would correspond to a class thathas a low association score with respect to eat.
However, ?location?
is a kind of ?entity?in WordNet,10 and choosing the class with the highest association score is likely toproduce ?entity?
as the chosen class.
This is a problem, because the association scoreof ?entity?
with respect to eat may be too high to reflect the fact that ?location?
is a veryunlikely object of the verb.Note that the solution to the vertical-ambiguity problem presented in the previoussections is able to generalize appropriately in such cases.
Continuing with the eat?location?
example, our generalization procedure is unlikely to get as high as ?entity?
(assuming a reasonable number of examples of eat in the training data), since theprobabilities corresponding to the daughters of ?entity?
are likely to be very differentwith respect to the object position of eat.The second alternative uses the minimum description length (MDL) principle.Li and Abe use MDL to select a set of classes from a hierarchy, together with theirassociated probabilities, to represent the selectional preferences of a particular verb.The preferences and class-based probabilities are then used to estimate probabilitiesof the form p(n | v, r), where n is a noun, v is a verb, and r is an argument slot.Li and Abe?s application of MDL requires the hierarchy to be in the form of athesaurus, in which each leaf node represents a noun and internal nodes represent theclass of nouns that the node dominates.
The hierarchy is also assumed to be in theform of a tree.
The class-based models consist of a partition of the set of nouns (leafnodes) and a probability associated with each class in the partition.
The probabilitiesare the conditional probabilities of each class, given the relevant verb and argumentposition.
Li and Abe refer to such a partition as a ?cut?
and the cut together with theprobabilities as a ?tree cut model.?
The probabilities of the classes in a cut, ?, satisfythe following constraint:?C?
?p(C | v, r) = 1 (19)9 The definition used here is that given by Ribas (1995).10 For example, the hypernyms of the concept ?Dallas?
are as follows: ?city?, ?municipality?,?urban area?, ?geographical area?, ?region?, ?location?, ?object?, ?entity?.200Computational Linguistics Volume 28, Number 2<abstraction><life_form><plant><object><entity><substance><set><root><mushroom><artifact><rope><food><pizza><lobster><fluid><solid><animal><lobster><time><space>Figure 3Possible cut returned by MDL.In order to determine the probability of a noun, the probability of a class is assumedto be distributed uniformly among the members of that class:p(n | v, r) = 1|C| p(C | v, r) for all n ?
C (20)Since WordNet is a hierarchy with noun senses, rather than nouns, at the nodes,Li and Abe deal with the issue of word sense ambiguity using the method describedin Section 3, by dividing the count for a noun equally among the concepts whosesynsets contain the noun.
Also, since WordNet is a DAG, Li and Abe turn WordNetinto a tree by copying each subgraph with multiple parents.
And so that each nounin the data appears (in a synset) at a leaf node, Li and Abe remove those parts of thehierarchy dominated by a noun in the data (but only for that instance of WordNetcorresponding to the relevant verb).An example cut showing part of the WordNet hierarchy is shown in Figure 3 (basedon an example from Li and Abe [1998]; the dashed lines indicate parts of the hierarchythat are not shown in the diagram).
This is a possible cut for the object position of theverb eat, and the cut consists of the following classes: ?life form?, ?solid?, ?fluid?, ?food?,?artifact?, ?space?, ?time?, ?set?.
(The particular choice of classes for the cut in this exampleis not too important; the example is designed to show how probabilities of senses areestimated from class probabilities.)
Since the class in the cut containing ?pizza?
is ?food?,the probability p(?pizza?
| eat, obj) would be estimated as p(?food?
| eat, obj)/|?food?|.Similarly, since the class in the cut containing ?mushroom?
is ?life form?, the probabilityp(?mushroom?
| eat, obj) would be estimated as p(?life form?
| eat, obj)/|?life form?|.The uniform-distribution assumption (20) means that cuts close to the root of thehierarchy result in a greater smoothing of the probability estimates than cuts near theleaves.
Thus there is a trade-off between choosing a model that has a cut near theleaves, which is likely to overfit the data, and a more general (simple) model near theroot, which is likely to underfit the data.
MDL looks ideally suited to the task of modelselection, since it is designed to deal with precisely this trade-off.
The simplicity of amodel is measured using the model description length, which is an information-theoretic201Clark and Weir Class-Based Probability Estimationterm and denotes the number of bits required to encode the model.
The fit to the datais measured using the data description length, which is the number of bits required toencode the data (relative to the model).
The overall description length is the sum ofthe model description length and the data description length, and the MDL principleis to select the model with the shortest description length.We used McCarthy?s (2000) implementation of MDL.
So that every noun is repre-sented at a leaf node, McCarthy does not remove parts of the hierarchy, as Li and Abedo, but instead creates new leaf nodes for each synset at an internal node.
McCarthyalso does not transform WordNet into a tree, which is strictly required for Li andAbe?s application of MDL.
This did create a problem with overgeneralization: Manyof the cuts returned by MDL were overgeneralizing at the ?entity?
node.
The reasonis that ?person?, which is close to ?entity?
and dominated by ?entity?, has two parents:?life form?
and ?causal agent?.
This DAG-like property was responsible for the over-generalization, and so we removed the link between ?person?
and ?causal agent?.
Thisappeared to solve the problem, and the results presented later for the average degreeof generalization do not show an overgeneralization compared with those given in Liand Abe (1998).7.
Pseudo-Disambiguation ExperimentsThe task we used to compare the class-based estimation techniques is a decision taskpreviously used by Pereira, Tishby, and Lee (1993) and Rooth et al (1999).
The task isto decide which of two verbs, v and v?, is more likely to take a given noun, n, as anobject.
The test and training data were obtained as follows.
A number of verb?directobject pairs were extracted from a subset of the BNC, using the system of Briscoe andCarroll.
All those pairs containing a noun not in WordNet were removed, and eachverb and argument was lemmatized.
This resulted in a data set of around 1.3 million(v, n) pairs.To form a test set, 3,000 of these pairs were randomly selected such that eachselected pair contained a fairly frequent verb.
(Following Pereira, Tishby, and Lee, onlythose verbs that occurred between 500 and 5,000 times in the data were considered.
)Each instance of a selected pair was then deleted from the data to ensure that the testdata were unseen.
The remaining pairs formed the training data.
To complete the testset, a further fairly frequent verb, v?, was randomly chosen for each (v, n) pair.
Therandom choice was made according to the verb?s frequency in the original data set,subject to the condition that the pair (v?, n) did not occur in the training data.
Giventhe set of (v, n, v?)
triples, the task is to decide whether (v, n) or (v?, n) is the correctpair.11We acknowledge that the task is somewhat artificial, but pseudo-disambiguationtasks of this kind are becoming popular in statistical NLP because of the ease withwhich training and test data can be created.
We also feel that the pseudo-disambig-uation task is useful for evaluating the different estimation methods, since it directlyaddresses the question of how likely a particular predicate is to take a given noun asan argument.
An evaluation using a PP attachment task was attempted in Clark andWeir (2000), but the evaluation was limited by the relatively small size of the PennTreebank.11 We note that this procedure does not guarantee that the correct pair is more likely than the incorrectpair, because of noise in the data from the parser and also because a highly plausible incorrect paircould be generated by chance.202Computational Linguistics Volume 28, Number 2Table 5Results for the pseudo-disambiguation task.Generalization technique % correct av.gen.
sd.gen.Similarity class?
= 0.0005 73.8 3.3 2.0?
= 0.05 73.4 2.8 1.9?
= 0.3 73.0 2.4 1.8?
= 0.75 73.9 1.9 1.6?
= 0.995 73.8 1.2 1.2Low class 73.6 0.9 1.0MDL 68.3 4.1 1.9Assoc 63.9 4.2 2.1Note: av.gen.
is the average number of generalized levels;sd.gen.
is the standard deviation.Using our approach, the disambiguation decision for each (v, n, v?)
triple was madeaccording to the following procedure:if maxc?cn(n)psc(c | v, obj) > maxc?cn(n)psc(c | v?, obj)then choose (v, n)else if maxc?cn(n)psc(c | v?, obj) > maxc?cn(n)psc(c | v, obj)then choose (v?, n)else choose at randomIf n has more than one sense, the sense is chosen that maximizes the relevant prob-ability estimate; this explains the maximization over cn(n).
The probability estimateswere obtained using our class-based method, and the G2 statistic was used for thechi-square test.
This procedure was also used for the MDL alternative, but using theMDL method to estimate the probabilities.Using the association score for each test triple, the decision was made accordingto the following procedure:if maxc?cn(n)maxc??h(c)A?
(c?, v, obj) > maxc?cn(n)maxc??h(c)A?
(c?, v?, obj)then choose (v, n)else if maxc?cn(n)maxc??h(c)A?
(c?, v?, obj) > maxc?cn(n)maxc??h(c)A?
(c?, v, obj)then choose (v?, n)else choose at randomWe use h(c) to denote the set consisting of the hypernyms of c. The inner maximizationis over h(c), assuming c is the chosen sense of n, which corresponds to Resnik?s methodof choosing a set to represent c. The outer maximization is over the senses of n, cn(n),which determines the sense of n by choosing the sense that maximizes the associationscore.The first set of results is given in Table 5.
Our technique is referred to as the?similarity class?
technique, and the approach using the association score is referred203Clark and Weir Class-Based Probability EstimationTable 6Results for the pseudo-disambiguation task with one-fifth training data.Generalization technique % correct av.gen.
sd.gen.Similarity class?
= 0.0005 66.7 4.5 1.9?
= 0.05 68.4 4.1 1.9?
= 0.3 70.2 3.7 1.9?
= 0.75 72.3 3.0 1.9?
= 0.995 72.4 1.9 1.6Low class 71.9 1.1 1.1MDL 62.9 4.7 1.9Assoc 62.6 4.1 2.0Note: av.gen.
is the average number of generalized levels;sd.gen.
is the standard deviation.to as ?Assoc.?
The results are given for a range of ?
values and demonstrate clearly thatthe performance of similarity class varies little with changes in ?
and that similarityclass outperforms both MDL and Assoc.12We also give a score for our approach using a simple generalization procedure,which we call ?low class.?
The procedure is to select the first class that has a countgreater than zero (relative to the verb and argument position), which is likely to returna low level of generalization, on the whole.
The results show that our generalizationtechnique only narrowly outperforms the simple alternative.
Note that, although lowclass is based on a very simple generalization method, the estimation method is stillusing our class-based technique, by applying Bayes?
theorem and conditioning on aclass, as described in Section 3; the difference is in how the class is chosen.To investigate the results, we calculated the average number of generalized levelsfor each approach.
The number of generalized levels for a concept c (relative to averb v and argument position r) is the difference in depth between c and top(c, v, r),as explained in Section 5.
For each test case, the number of generalized levels forboth verbs, v and v?, was calculated, but only for the chosen sense of n. The resultsare given in the third column of Table 5 and demonstrate clearly that both MDL andAssoc are generalizing to a greater extent than similarity class.
(The fourth columngives a standard deviation figure.)
These results suggest that MDL and Assoc areovergeneralizing, at least for the purposes of this task.To investigate why the value for ?
had no impact on the results, we repeated theexperiment, but with one fifth of the data.
A new data set was created by taking everyfifth pair of the original 1.3 million pairs.
A test set of 3,000 triples was created fromthis new data set, as before, but this time only verbs that occurred between 100 and1,000 times were considered.
The results using these test and training data are givenin Table 6.These results show a variation in performance across values for ?, with an opti-mal performance when ?
is around 0.75.
(Of course, in practice, the value for ?
wouldneed to be optimized on a held-out set.)
But even with this variation, similarity class isstill outperforming MDL and Assoc across the whole range of ?
values.
Note that the12 The results given for similarity class are different from those given in Clark and Weir (2001) becausethe probability estimates used in Clark and Weir (2001) were not normalized.204Computational Linguistics Volume 28, Number 2Table 7Disambiguation results for G2 and X2.?
value % correct (G2) % correct (X2)0.0005 73.8 (3.3) 74.1 (3.0)0.05 73.4 (2.8) 73.8 (2.5)0.3 73.0 (2.4) 74.1 (2.2)0.75 73.9 (1.9) 74.3 (1.8)0.995 73.8 (1.2) 73.3 (1.2)?
values corresponding to the lowest scores lead to a significant amount of general-ization, which provides additional evidence that MDL and Assoc are overgeneralizingfor this task.
The low-class method scores highly for this data set alo, but given thatthe task is one that apparently favors a low level of generalization, the high score isnot too surprising.As a final experiment, we compared the task performance using the X2, rather thanG2, statistic in the chi-square test.
The results are given in Table 7 for the completedata set.13 The figures in brackets give the average number of generalized levels.The X2 statistic is performing at least as well as G2, and the results show that theaverage level of generalization is slightly higher for G2 than X2.
This suggests a possibleexplanation for the results presented here and those in Dunning (1993): that the X2statistic provides a less conservative test when counts in the contingency table arelow.
(By a conservative test we mean one in which the null hypothesis is not easilyrejected.)
A less conservative test is better suited to the pseudo-disambiguation task,since it results in a lower level of generalization, on the whole, which is good for thistask.
In contrast, the task that Dunning considers, the discovery of bigrams, is betterserved by a more conservative test.8.
ConclusionWe have presented a class-based estimation method that incorporates a procedure forfinding a suitable level of generalization in WordNet.
This method has been shown toprovide superior performance on a pseudo-disambiguation task, compared with twoalternative approaches.
An analysis of the results has shown that the other approachesappear to be overgeneralizing, at least for this task.
One of the features of the gener-alization procedure is the way that ?, the level of significance in the chi-square test,is treated as a parameter.
This allows some control over the extent of generalization,which can be tailored to particular tasks.
We have also shown that the task perfor-mance is at least as good when using the Pearson chi-square statistic as when usingthe log-likelihood chi-square statistic.There are a number of ways in which this work could be extended.
One possibilitywould be to use all the classes dominated by the hypernyms of a concept, rather thanjust one, to estimate the probability of the concept.
An estimate would be obtained foreach hypernym, and the estimates combined in a linear interpolation.
An approachsimilar to this is taken by Bikel (2000), in the context of statistical parsing.There is still room for investigation of the hidden-data problem when data are usedthat have not been sense disambiguated.
In this article, a very simple approach is taken,13 ?2 performed slightly better than G2 using the smaller data set alo.205Clark and Weir Class-Based Probability Estimationwhich is to split the count for a noun evenly among the noun?s senses.
Abney and Light(1999) have tried a more motivated approach, using the expectation maximizationalgorithm, but with little success.
The approach described in Clark and Weir (1999) isshown in Clark (2001) to have some impact on the pseudo-disambiguation task, butonly with certain values of the ?
parameter, and ultimately does not improve on thebest performance.Finally, an issue that has not been much addressed in the literature (except byLi and Abe [1996]) is how the accuracy of class-based estimation techniques comparewhen automatically acquired classes, as opposed to the manually created classes fromWordNet, are used.
The pseudo-disambiguation task described here has also been usedto evaluate clustering algorithms (Pereira, Tishby, and Lee, 1993; Rooth et al, 1999),but with different data, and so it is difficult to compare the results.
A related issueis how the structure of WordNet affects the accuracy of the probability estimates.
Wehave taken the structure of the hierarchy for granted, without any analysis, but it maybe that an alternative design could be more conducive to probability estimation.AcknowledgmentsThis article is an extended and updatedversion of a paper that appeared in theproceedings of NAACL 2001.
The work onwhich it is based was carried out while thefirst author was a D.Phil.
student at theUniversity of Sussex and was supported byan EPSRC studentship.
We would like tothank Diana McCarthy for suggesting thepseudo-disambiguation task and providingthe MDL software, John Carroll forsupplying the data, and Ted Briscoe, GeoffSampson, Gerald Gazdar, Bill Keller, TedPedersen, and the anonymous reviewers fortheir helpful comments.
We would also liketo thank Ted Briscoe for presenting anearlier version of this article on our behalfat NAACL 2001.ReferencesAbney, Steven P. and Marc Light.
1999.Hiding a semantic hierarchy in a Markovmodel.
In Proceedings of the ACL Workshopon Unsupervised Learning in NaturalLanguage Processing, University ofMaryland, College Park, pages 1?8.Agirre, Eneko and David Martinez.
2001.Learning class-to-class selectionalpreferences.
In Proceedings of the Fifth ACLWorkshop on Computational LanguageLearning, Toulouse, France, pages 15?22.Agresti, Alan.
1996.
An Introduction toCategorical Data Analysis.
Wiley.Bikel, Daniel M. 2000.
A statistical modelfor parsing and word-sensedisambiguation.
In Proceedings of the JointSIGDAT Conference on Empirical Methods inNatural Language Processing and Very LargeCorpora, pages 155?163, Hong Kong.Briscoe, Ted and John Carroll.
1997.Automatic extraction of subcategorizationfrom corpora.
In Proceedings of the FifthACL Conference on Applied Natural LanguageProcessing, pages 356?363, Washington,DC.Church, Kenneth W. and Patrick Hanks.1990.
Word association norms, mutualinformation, and lexicography.Computational Linguistics, 16(1):22?29.Ciaramita, Massimiliano and Mark Johnson.2000.
Explaining away ambiguity:Learning verb selectional preference withBayesian networks.
In Proceedings of the18th International Conference onComputational Linguistics, pages 187?193,Saarbrucken, Germany.Clark, Stephen.
2001.
Class-Based StatisticalModels for Lexical Knowledge Acquisition.Ph.D.
dissertation, University of Sussex.Clark, Stephen and David Weir.
1999.
Aniterative approach to estimatingfrequencies over a semantic hierarchy.
InProceedings of the Joint SIGDAT Conferenceon Empirical Methods in Natural LanguageProcessing and Very Large Corpora, pages258?265, University of Maryland, CollegePark.Clark, Stephen and David Weir.
2000.
Aclass-based probabilistic approach tostructural disambiguation.
In Proceedingsof the 18th International Conference onComputational Linguistics, pages 194?200,Saarbrucken, Germany.Clark, Stephen and David Weir.
2001.Class-based probability estimation using asemantic hierarchy.
In Proceedings of theSecond Meeting of the North AmericanChapter of the Association for ComputationalLinguistics, pages 95?102, Pittsburgh.Cressie, Noel A. C. and Timothy R. C. Read.1984.
Multinomial goodness of fit tests.206Computational Linguistics Volume 28, Number 2Journal of the Royal Statistics Society Series B,46:440?464.Dunning, Ted.
1993.
Accurate methods forthe statistics of surprise and coincidence.Computational Linguistics, 19(1):61?74.Fellbaum, Christiane, editor.
1998.
WordNet:An Electronic Lexical Database.
MIT Press.Li, Hang and Naoki Abe.
1996.
Clusteringwords with the MDL principle.
InProceedings of the 16th InternationalConference on Computational Linguistics,pages 4?9, Copenhagen, Denmark.Li, Hang and Naoki Abe.
1998.
Generalizingcase frames using a thesaurus and theMDL principle.
Computational Linguistics,24(2):217?244.McCarthy, Diana.
1997.
Word sensedisambiguation for acquisition ofselectional preferences.
In Proceedings ofthe ACL/EACL Workshop on AutomaticInformation Extraction and Building of LexicalSemantic Resources for NLP Applications,pages 52?61, Madrid.McCarthy, Diana.
2000.
Using semanticpreferences to identify verbalparticipation in role switching.
InProceedings of the First Conference of theNorth American Chapter of the Association forComputational Linguistics, pages 256?263,Seattle.Miller, George A.
1998.
Nouns in WordNet.In Christiane Fellbaum, editor, WordNet:An Electronic Lexical Database.
MIT Press,pages 23?46.Pedersen, Ted.
1996.
Fishing for exactness.In Proceedings of the South-Central SAS UsersGroup Conference, Austin, pages 188?200.Pedersen, Ted.
2001.
A decision tree ofbigrams is an accurate predictor of wordsense.
In Proceedings of the Second Meetingof the North American Chapter of theAssociation for Computational Linguistics,pages 79?86, Pittsburgh.Pereira, Fernando, Naftali Tishby, andLillian Lee.
1993.
Distributional clusteringof English words.
In Proceedings of the 31stAnnual Meeting of the Association forComputational Linguistics, pages 183?190,Columbus, OH.Resnik, Philip.
1993.
Selection andInformation: A Class-Based Approach toLexical Relationships.
Ph.D. dissertation,University of Pennsylvania.Resnik, Philip.
1998.
WordNet andclass-based probabilities.
In ChristianeFellbaum, editor, WordNet: An ElectronicLexical Database.
MIT Press, pages 239?263.Ribas, Francesc.
1995.
On learning moreappropriate selectional restrictions.
InProceedings of the Seventh Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 112?118,Dublin.Rooth, Mats, Stefan Riezler, Detlef Prescher,Glenn Carroll, and Franz Beil.
1999.Inducing a semantically annotated lexiconvia EM-based clustering.
In Proceedings ofthe 37th Annual Meeting of the Association forComputational Linguistics, pages 104?111,University of Maryland, College Park.Wagner, Andreas.
2000.
Enriching a lexicalsemantic net with selectional preferencesby means of statistical corpus analysis.
InProceedings of the ECAI-2000 Workshop onOntology Learning, Berlin, pages 37?42.
