Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1425?1434,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsLearning a Phrase-based Translation Model from Mon-olingual Data with Application to Domain AdaptationJiajun Zhang and Chengqing ZongNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences, Beijing, China{jjzhang, cqzong}@nlpr.ia.ac.cnAbstractCurrently, almost all of the statistical ma-chine translation (SMT) models are trainedwith the parallel corpora in some specificdomains.
However, when it comes to a lan-guage pair or a different domain withoutany bilingual resources, the traditional SMTloses its power.
Recently, some researchworks study the unsupervised SMT for in-ducing a simple word-based translationmodel from the monolingual corpora.
Itsuccessfully bypasses the constraint ofbitext for SMT and obtains a relativelypromising result.
In this paper, we take astep forward and propose a simple but effec-tive method to induce a phrase-based modelfrom the monolingual corpora given an au-tomatically-induced translation lexicon or amanually-edited translation dictionary.
Weapply our method for the domain adaptationtask and the extensive experiments showthat our proposed method can substantiallyimprove the translation quality.1 IntroductionDuring the last decade, statistical machine trans-lation has made great progress.
Novel translationmodels, such as phrase-based models (Koehn eta., 2007), hierarchical phrase-based models(Chiang, 2007) and linguistically syntax-basedmodels (Liu et a., 2006; Huang et al, 2006; Gal-ley, 2006; Zhang et al 2008; Chiang, 2010;Zhang et al, 2011; Zhai et al, 2011, 2012) havebeen proposed and achieved higher and highertranslation performance.
However, all of thesestate-of-the-art translation models rely on theparallel corpora to induce translation rules andestimate the corresponding parameters.It is unfortunate that the parallel corpora arevery expensive to collect and are usually notavailable for resource-poor languages and formany specific domains even in a resource-richlanguage pair.Recently, more and more researchers concen-trated on taking full advantage of the monolin-gual corpora in both source and target languages,and proposed methods for bilingual lexicon in-duction from non-parallel data (Rapp, 1995,1999; Koehn and Knight, 2002; Haghighi et al,2008; Daum?
III and Jagarlamudi, 2011) andproposed unsupervised statistical machine trans-lation (bilingual lexicon is a byproduct) withonly monolingual corpora (Ravi and Knight,2011; Nuhn et al, 2012; Dou and Knight, 2012).In the bilingual lexicon induction (Koehn andKnight, 2002; Haghighi et al, 2008; Daum?
IIIand Jagarlamudi, 2011), with the help of the or-thographic and context features, researchersadopted an unsupervised method, such as canon-ical correlation analysis (CCA) model, to auto-matically induce the word translation pairs be-tween two languages from non-parallel data onlyrequiring that the monolingual data in each lan-guage are from a fairly comparable domain.The unsupervised statistical machine transla-tion method (Ravi and Knight, 2011; Nuhn et al,2012; Dou and Knight, 2012) viewed the trans-lation task as a decipherment problem and de-signed a generative model with the objectivefunction to maximize the likelihood of thesource language monolingual data.
To tackle thelarge-scale vocabulary, they mainly consideredthe word-based model (e.g.
IBM Model 3) andapplied the Bayesian method with Gibbs sam-pling or slice sampling.
Finally, they used thelearned translation model directly to translateunseen data (Ravi and Knight, 2011; Nuhn et al,2012) or incorporated the learned bilingual lexi-con as a new in-domain translation resource intothe phrase-based model which is trained without-of-domain data to improve the domain adap-tation performance in machine translation (Douand Knight, 2012).We can easily see that these unsupervisedmethods can only induce the word-based transla-tion rules (bilingual lexicon) at present.
It is abig challenge that whether we can induce phrase14251, word reordering example:?
??
?
??
??
||| the purpose of the invention is to ||| 0-0 0-3 1-4 2-2 3-1 4-5 4-62, idiom example:??
??
?
||| distinguish the true from the false ||| 0-0 1-2 1-5 2-1 2-43, unknown word translation:??
???
??
?
||| of the light-emitting diode chip ||| 0-2 1-2 2-4 3-0 3-1Table 1: Examples of new translation knowledge learned with the proposed phrase pair induction method.
Forthe three fields separated by ?|||?, the first two are respectively Chinese and English phrase, and the last one isthe word alignment between these two phrases.level translation rules and learn a phrase-basedmodel from the monolingual corpora.In this paper, we focus on exploring this di-rection and propose a simple but effective meth-od to induce the phrase-level translation rulesfrom monolingual data.
The main idea of ourmethod is to divide the phrase-level translationrule induction into two steps: bilingual lexiconinduction and phrase pair induction.Since many researchers have studied the bi-lingual lexicon induction, in this paper, wemainly concentrate ourselves on phrase pair in-duction given a probabilistic bilingual lexiconand two in-domain large monolingual data(source and target language).
In addition, wewill further introduce how to refine the inducedphrase pairs and estimate the parameters of theinduced phrase pairs, such as four standardtranslation features and phrase reordering featureused in the conventional phrase-based models(Koehn et al, 2007).
The induced phrase-basedmodel will be used to help domain adaptationfor machine translation.In the rest of this paper, we first explain withexamples to show what new translationknowledge can be learned with our proposedphrase pair induction method (Section 2), andthen we introduce the approach for probabilisticbilingual lexicon acquisition in Section 3.
In Sec-tion 4 and 5, we respectively present our methodfor phrase pair induction and introduce an ap-proach for phrase pair refinement and parameterestimation.
Section 6 will show the detailed ex-periments for the task of domain adaptation.
Wewill introduce some related work in Section 7and conclude this paper in Section 8.2 What Can We Learn with PhrasePair   Induction?Readers may doubt that if phrase pair inductionis performed only using bilingual lexicon andmonolingual data, what new translationknowledge can be learned?The bilingual lexicon can only express thetranslation equivalence between source- and tar-get-side word pair and has little ability to dealwith word reordering and idiom translation.
Incontrast, phrase pair induction can make up forthis deficiency to some extent.
Furthermore, ourmethod is able to learn some unknown wordtranslations.From the induced phrase pairs with our meth-od, we have conducted a deep analysis and findthat we can learn three kinds of new translationknowledge: 1) word reordering in a phrase pair;2) idioms; and 3) unknown word translations.Table 1 gives examples for each of the threekinds.
For the first example, the source and tar-get phrase are extracted respectively from mono-lingual data, each word in the source phrase hasa translation in the target phrase, but the wordorder is different.
The word order encoded in aphrase pair is difficult to learn in a word-basedSMT.
In the second example, the italic sourceword corresponds to two target words (in italic),and the phrase pair is an idiom which cannot belearned from word-based SMT.
In the third ex-ample, as we learn from the source and targetmonolingual text that the words around the italicones are translations with each other, thus wecannot only extract a new phrase pair but alsolearn a translation pair of unknown words initalic.3 Probabilistic Bilingual Lexicon Ac-quisitionIn order to induce the phrase pairs from the in-domain monolingual data for domain adaptation,the probabilistic bilingual lexicon is essential.In this paper, we acquire the probabilistic bi-lingual lexicon from two approaches: 1) build abilingual lexicon from large-scale out-of-domainparallel data; 2) adopt a manually collected in-domain lexicon.
This paper uses Chinese-to-English translation as a case study and electronicdata is the in-domain data we focus on.1426In Chinese-to-English translation, there arelots of parallel data on News.
Here, we utilizeabout 2.08 million sentence pairs1 in News do-main to learn a probabilistic bilingual lexicon.Basically, we can use GIZA++ (Och, 2003) toget the probabilistic lexicon.
However, the prob-lem is that each source-side word associates toomany possible translations which contain muchnoise.
For instance, in the lexicon obtained withGIZA++, each source-side word has about 13translations on average.
The noise of the lexiconcan influence the accuracy of the induced phrasepairs to a large extent.
To learn a lexicon with ahigh precision, we follow Munteanu and Marcu(2006) to apply Log-Likelihood-Ratios (Dunning,1993; Melamed, 2000; Moore, 2004a, 2004b) toestimate how strong the association is between asource-side word and its aligned target-side word.We employ the same algorithm used in (Munte-anu and Marcu, 2006) which first use the GI-ZA++ (with grow-diag-final-and heuristic) toobtain the word alignment between source andtarget words, and then calculate the associationstrength between the aligned words.
After usingthe log-likelihood-ratios algorithm2, we obtain aprobabilistic bilingual lexicon with bidirectionaltranslation probabilities from the out-of-domaindata.
In the final lexicon, the number of averagetranslations is only 5.
We call this lexicon LLR-lex.In the electronic domain, we manually collect-ed a lexicon which contains about 140k entries.It should be noted that there is no translationprobability in this lexicon.
In order to assignprobabilities to each entry, we apply the CorpusTranslation Probability which used in (Wu et al,2008): given an in-domain source languagemonolingual data, we translate this data with thephrase-based model trained on the out-of-domainNews data, the in-domain lexicon and the in-domain target language monolingual data (forlanguage model estimation).
With the sourcelanguage data and its translation, we estimate thebidirectional translation probabilities for eachentry in the original lexicon.
For the entrieswhose translation probabilities are not estimated,we just assign a uniform probability.
That is if asource word has n translations, then the transla-tion probability of target word given the sourceword is 1/n.
We call this lexicon Domain-lex.1 LDC category numbers are: LDC2000T50, LDC2003E14,LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27,LDC2005T10 and LDC2005T34.2  Following Moore (2004b), we use the threshold 10 onLLR to filter out unlikely translations.We combine LLR-lex and Domain-lex to obtainthe final probabilistic bilingual lexicon for phrasepair induction.4 Phrase Pair Induction MethodGiven a probabilistic bilingual lexicon and twomonolingual data, we present a simple but effec-tive method for phrase pair induction in this sec-tion.Figure 1: a na?ve algorithm for phrase pair induction.4.1 A Na?ve MethodWe first introduce a relatively na?ve way to ex-tract phrase pairs from the given resources.
For asource phrase (word sequence), we can reorderthe words in the phrase (permutation) first, andthen obtain the target phrases with the bilinguallexicon (translation), and finally check if the tar-get phrase is in the target monolingual data.
Thealgorithm is given in Figure 1.Figure 1 shows that the na?ve algorithm is veryeasy to implement.
However, the time complexi-ty is too high.
For each source phrase jis  (with?
?1 !j i?
?
permutations), suppose a source wordhas C translations on average and checkingwhether the target phrase ''jit  in T needs time?
?O T , then, phrase pair induction for a singlesource phrase needs time ?
??
?1 1 !j iO C T j i?
?
?
?.It is very time consuming.
One may designsmarter algorithms.
For example, one can collectdistinct n-grams from source and target monolin-gual data.
Then, for a source-side phrase withlength L, one can find the best translation candi-date using the probabilistic bilingual lexiconfrom the target-side phrases with the same lengthL.
The biggest disadvantage of these algorithmsis that they can only induce phrase pair (with theInput:   Probabilistic bilingual lexicon V (each source words maps a translation set V[s])Source language monolingual data S={sn} n=1...NTarget language monolingual data T={tm} m=1...MOutput: Phrase pairs  P1: For each distinct source-side phrase jis  in S:2:       If each jk is s?
in V:3: Collect [ ] jk k iV s ?4: For each permutation ''jis  of jis :5:        If ''jit  in T:     ' '[ ] ' [ , ]k kt V s k i j?
?6:  Add phrase pair ?
?
'',j ji is t into P1427same length) encoding word reordering, but can-not learn phrase pairs in different length.
Fur-thermore, they cannot learn idioms and unknownword translations from monolingual data.
Obvi-ously, these kind of approaches is not optimal.4.2 Phrase Pair Induction with InvertedIndexIn order to make the phrase pair induction botheffective and efficient, we propose a methodusing inverted index data structure which is usu-ally a central component of a typical search en-gine.The inverted index is employed to representthe target language monolingual data.
For a tar-get language word, the inverted index not onlyrecords the sentence position in monolingualdata, but also records the word position in a sen-tence.
Some examples are shown in Table 2.
Bydoing this, we do not need to iterate all the per-mutations of source language phrase jis  to ex-plore possible phrase pairs encoding word reor-dering.
Furthermore, it is possible to learn idiomtranslation and unknown word translations.
Wewill elaborate how to induce phrase pairs withthe help of inverted index.Target LanguageWordPositioncommunication (2,5), (106,20), ?, (23022, 12)?
?zoom (90,2), (280,21), ?, (90239,15)Table 2: Some examples of inverted index for tar-get language words, (2,5) means that ?communica-tion?
occurs at the 5th word of the 2nd sentence in thetarget monolingual data.The new algorithm for phrase pair induction ispresented in Figure 2.
Line 1 iterates all the dis-tinct phrases in the source-side monolingual data.It can be implemented by collecting all the dis-tinct n-grams in which n is the phrase length weare interested in (3 to 7 in this paper).
For eachdistinct source-side phrase, Line 2-5 efficientlycollects all the positions in the target monolin-gual data for the translations of each word in thesource phrase.
Line 6 sorts the positions so thatwe can easily find the position sequence belong-ing to a same sentence.
Line 8-9 discards all theposition sub-sequences that lack translations formore than one source-side words.
That is to saywe allow at most one unknown word in an in-duced phrase pair in order to make the inductionmore accurate.
Line 10 and Line 12 is the coreof this algorithm.
We first define a constraintbefore detailing the algorithm.Figure 2: Phrase pair induction using inverted index.Constraint: we require that there exists atmost one phrase in a target sentence that is thetranslation of the source-side phrase.According to our analysis, it is not often tofind that two phrases (length larger than 2) in asame sentence have the same meaning.
Even if ithappens, it is reasonable to keep the one with thehighest probability.
Given a position sequencebelonging to a same sentence, Line 10 smoothesthe probability of the single word gap accordingto the probabilities of the around words.
Singleword gap means that this word is not aligned butits left and right words are aligned with thewords of the source-side phrase.
Suppose thetarget sub-sequence isi i r jt t t?andi rt ?
is theonly word that is not aligned with source-sidewords.
We smooth the probability ?
?|i rp t null?as follows:?
??
?
?
??
??
?
?
?1 11 1min | , |, 1 12|| |,2i ji r i ri t j ti ri r t i r tp t s p t sif r or r jp t nullp t s p t sotherwise?
?
?
???
?
?
???
?
?
???
??
???
(1)The above formula means that if the left or theright side only has one word, then the smoothedprobability is one half of the minimum of theprobabilities of the two neighbors, otherwise thesmoothed probability is the average of the prob-abilities of the two neighbors.
This smoothingstrategy encourages that if more words aroundthe un-aligned word are translations of thesource-side phrase, then the gap word is morelikely to belong to the translations of the source-side phrase.Input:   Probabilistic bilingual lexicon V (each source word smaps a translation set V[s])Source language monolingual data S={sn} n=1...NInverted index representing target language monolin-gual data IMapOutput: Phrase pairs P1: For each distinct source-side phrase jis  in S:2:      positionArray = []3:      For each jk is s?
:4:            For each [ ]kt V s?
:5:       add  IMap[ t ]  into positionArray6:      Sort  positionArray7:      For each sequence in a same sentence in positionArray:8:              If more than 1 word in jis has no trans in the seq:9:                    Discard this seq and continue10:             Probability smoothing for single word gap11:             For all continuous position sub-sequence:12:                  Find the one kht  with maximum probability13:                 Add phrase pair ?
?,j ki hs t into P1428After probability smoothing of the single gapword, we are ready to extract the candidatetranslation of the source-side phrase.
Similarwith Line 9 in Figure 2, we further filter the tar-get continuous phrase if more than one word insource-side phrase has no translation in this tar-get phrase.
After that, we just choose the contin-uous target phrase with the largest probability iftwo or more continuous target phrases exist inthe same target sentence.
The probability of atarget-side phrase given the source-side phrase iscomputed similar to that of (Koehn et al, 2003)except that we impose length normalization:?
?
?
??
?
?
??
?1,11| , || ,nnlex i ji j aip t s a p t sj i j a ?
???
??
??
?
???
???
(2)where the alignment a is produced usingprobabilistic bilingual lexicon.
If a target wordin t is a gap word, we suppose there is a wordalignment between the target gap word and thesource-side null.Similarly, we can compute the probability ofsource-side phrase given the target-side phrase?
?| ,lexp s t a .
Then, we find the target-side phrasewhich has the biggest value of?
?
?
?| , | ,lex lexp t s a p s t a?
.
Line 13 in Figure 2 col-lects the induced phrase pairs.For the time complexity, it depends on thelength of positionArray, since the time complex-ity of the core algorithm (Line 7-13) is propor-tional to the length of positionArray.
If posi-tionArray contains almost all the positions in thetarget monolingual data T, then the worst timecomplexity will be ?
?logO T T  (for array sort).However, we find in the target monolingual data(1 million sentences) that each distinct wordhappens 110 times on average.
Then, for asources-side phrase with 7 words, the averagelength of positionArray will be 3850, since eachsource word has averagely 5 target translations(mentioned in Section 3).
Therefore, the algo-rithm is relatively efficient in the average case.5 Phrase Pair Refinement and Parame-terization5.1 Phrase Pair RefinementSome of the phrase pairs induced in Section 4may contain noise.
According to our analysis,we find that the biggest problem is that in thetarget-side of the phrase pair, there are two ormore identical words aligned to the same source-side word.
For example, we extract a phrase pairas follows:?
??
?
?of  business information ofIn the above phrase pair, there are two words?of?
in the target side and the first one is redun-dant.
The phrase pair induction algorithm pre-sented in Section 4 cannot deal with this situa-tion.
In this section, we propose a simple ap-proach to handle this problem.
For each entry inLLR-lex, such as (?, of), we can learn two kindsof information from the out-of-domain word-aligned sentence pairs: one is whether the targettranslation is before or after the translation of thepreceding source-side word (Order); the other iswhether the target translation is adjacent withthe translation of the preceding source-side word(Adjacency).
If the source-side word is the be-ginning of the phrase, we calculate the corre-sponding information with the succeeding wordinstead of the preceding word.
For the entries inDomain-lex, we constrain that the target transla-tion should be adjacent with the translations ofits source-side neighbors and translation order isthe same with the source-side words.With the Order and Adjacency information,we first check the order information, and thencheck the adjacency information if the dupli-cates cannot be handled using order information.For example, since (?, of) is an entry in LLR-lex and we have learned that ?of?
is much morelikely to be behind the translation of the suc-ceeding word.
Thus, the first word ?of?
can bediscarded.
This refinement can be applied beforefinding the phrase pair with maximum probabil-ity (Line 12 in Figure 2) so that the duplicatewords do not affect the calculation of translationprobability of phrase pair.5.2 Translation Probability EstimationIt is well known that in the phrase-based SMTthere are four translation probabilities and thereordering probability for each phrase pair.The translation probabilities in the traditionalphrase-based SMT include bidirectional phrasetranslation probabilities and bidirectional lexicalweights.
For the lexical weights, we can use the?
?| ,lexp s t a  and ?
?| ,lexp t s a computed in theabove section without length normalization.However, for the phrase-level probability, wecannot use maximum likelihood estimation sincethe phrase pairs are not extracted from parallelsentences.1429In this paper, we borrow and extend the idea of(Klementiev et al, 2012) to calculate the phrase-level translation probability with context infor-mation in source and target monolingual corpus.The value is calculated using a vector spacemodel.
With source and target vocabularies?
?1 2, , , Ns s s  and ?
?1 2, , , Mt t t , the source-sidephrase s and target-side phrase t can be respec-tively represented in an N- and M-dimensionalvector.
The k-th component of s?s contextualvector is computed using the method of (Fungand Yee, 1998) as follows:?
??
?, maxlog / 1k s k kw n n n?
?
?
(3)where,s knandkn denotes the number of times ksoccurs in the context of s and in the entire sourcelanguage monolingual data, andmaxn is the max-imum number of occurrence of any source-sideword in the source language monolingual data.The k-th element of t?s vector can be computedwith the same method.
We finally normalizethese vectors with L2-norm.With the s?s and t?s contextual vector represen-tations, we calculate two similarities: 1) projects?s vector into target side t  with the lexicalmapping p(t|s), and then get the similarity bycomputing the cosine of two angles between t?sand t ?s vectors; 2) project t?s vector into sourceside s  with the lexical mapping p(s|t), and thenobtain the similarity between s?s and s ?s vectors.These two contextual similarities will serve astwo phrase-level translation probabilities.5.3 Reordering Probability EstimationFor the reordering probabilities of newly inducedphrase pairs, we can also follow Klementiev et al(2012) to estimate these probabilities usingsource and target monolingual data.
The methodis to calculate six probabilities for monotone,swap or discontinuous cases.
For the phrase pair(?
??
??
, business information of), wefind a source sentence containing ?
??
?
?,and find a target sentence containing businessinformation of.
If there is another phrase pair?
?,s t ,  t  exactly follows business information ofand s  occurs in the same source sentence with?
??
?
?, then we compare the positionrelationship between s  and ?
??
??.
Weincrement the swap count if s  is just before ???
??.
After counting, we finally use max-imum likelihood estimation method to computethe reordering probabilities.6 Related WorkAs far as we know, few researchers study phrasepair induction from only monolingual data.There are three research works that are mostrelated with ours.
One is using an in-domainprobabilistic bilingual lexicon to extract sub-sentential parallel fragments from comparablecorpora (Munteanu and Marcu, 2006; Quirk et al,2007; Cettolo et al, 2010).
Munteanu and Marcu(2006) first extract the candidate parallel sen-tences from the comparable corpora and furtherextract the accurate sub-sentential bilingualfragments from the candidate parallel sentencesusing the in-domain probabilistic bilingual lexi-con.
Compared with their work, our focus is toinduce phrase pairs directly from monolingualdata rather than comparable data.
Thus, findingthe candidate parallel sentences is not possible inour situation.Another is to make full use of monolingual da-ta with transductive learning (Ueffing et al, 2007;Schwenk, 2008; Wu et al, 2008; Bertoldi andFederico, 2009).
For the target-side monolingualdata, they just use it to train language model, andfor the source-side monolingual data, they em-ploy a baseline (word-based SMT or phrase-based SMT trained with small-scale bitext) tofirst translate the source sentences, combiningthe source sentence and its target translation as abilingual sentence pair, and then train a newphrase-base SMT with these pseudo sentencepairs.
This method cannot learn idiom transla-tions and unknown word translations.The third is to estimate the translation parame-ters and reordering parameters using monolin-gual data given the phrase pairs (Klementiev etal., 2012).
Their work supposes the phrase pairsare already given and then corresponding param-eters can be learned with monolingual data.
Dif-ferent from their work, we concentrate ourselveson inducing phrase pairs from monolingual dataand then borrow some ideas from theirs for pa-rameter estimation.
Furthermore, we extend theircontextual similarity between source and targetphrases to both directions.7 Experiments7.1 Experimental SetupOur purpose is to induce phrase pairs to improvetranslation quality for domain adaptation.
Wehave introduced the out-of-domain data and theelectronic in-domain lexicon in Section 3.
Herewe introduce other information about the in-1430domain data.
Besides the in-domain lexicon, wehave collected respectively 1 million monolin-gual sentences in electronic area from the web.They are neither parallel nor comparable becausewe cannot even extract a small number of paral-lel sentence pairs from this monolingual datausing the method of (Munteanu and Marcu,2006).
We further employ experts to translate2000 Chinese electronic sentences into English.The first half is used as the tuning set (elec1000-tune) and the second half is employed as the test-ing set (elec1000-test).We construct two kinds of phrase-based mod-els using Moses (Koehn et al, 2007): one usesout-of-domain data and the other uses in-domaindata.
For the out-of-domain data, we build thephrase table and reordering table using the 2.08million Chinese-to-English sentence pairs, andwe use the SRILM toolkit (Stolcke, 2002) totrain the 5-gram English language model withthe target part of the parallel sentences and theXinhua portion of the English Gigaword.
For thein-domain electronic data, we first consider thelexicon as a phrase table in which we assign aconstant 1.0 for each of the four probabilities,and then we combine this initial phrase table andthe induced phrase pairs to form the new phrasetable.
The in-domain reordering table is createdfor the induced phrase pairs.
An in-domain 5-gram English language model is trained with thetarget 1 million monolingual data.We use BLEU (Papineni et al, 2002) scorewith shortest length penalty as the evaluationmetric and apply the pairwise re-sampling ap-proach (Koehn, 2004) to perform the signifi-cance test.7.2 Experimental ResultsIn this section, we first conduct experiments tofigure out how the translation performance de-grades when the domain changes.
To better illus-trate the comparison, we first use News data toevaluate the NIST evaluation tests and then usethe same News data to evaluate the electronictest sets.
For the NIST evaluation, we employChinese-to-English NIST MT03 as the tuning setand NIST MT05 as the test set.
Table 3 gives theresults.
It is obvious that, it is relatively highwhen using the News training data to evaluatethe same News test set.
However, when the testdomain is changed, the translation performancedecreases to a large extent.Given the in-domain bilingual lexicon and twomonolingual data, previous works also proposedsome good methods to explore the potential ofthe given data to improve the translation quality.Here, we implement their approaches and usethem as our strong baseline.
Wu et al (2008)regards the in-domain lexicon with corpus trans-lation probability as another phrase table andfurther use the in-domain language model be-sides the out-of-domain language model.
Table 4gives the results.
We can see from the table thatthe domain lexicon is much helpful and signifi-cantly outperforms the baseline with more than4.0 BLEU points.
When it is enhanced with thein-domain language model, it can further im-prove the translation performance by more than2.5 BLEU points.
This method has made gooduse of in-domain lexicon and the target-side in-domain monolingual data, but it does not takefull advantage of the in-domain source-sidemonolingual data.In order to use source-side monolingual data,Ueffing et al (2007), Schwenk (2008), Wu et al(2008) and Bertoldi and Federico (2009) em-ployed the transductive learning to first translatethe source-side monolingual data using the bestconfiguration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best trans-lation for each source-side sentence.
With thesource-side sentences and their translations, thenew phrase table and reordering table are built.Then, these resources are added into the bestconfiguration.
The experimental results are pre-sented in the last low of Table 4.
From the results,we see that transductive learning can further im-prove the translation performance significantlyby 0.6 BLEU points.In tranductive learning, in-domain lexicon andboth-side monolingual data have been explored.However, this method does not take full ad-vantage of both-side monolingual data because ituses source and target monolingual data individ-ually.
In our method, we explore fully the sourceand target monolingual data to induce translationequivalence on the phrase level.
In order to makethe phrase pair induction more efficient, we firstsort all the sentences in the both-side monolin-gual data according to the word hit rate in thebilingual lexicon.
Then, we conduct six sets ofexperiments respectively on the first 100k, 200k,300k, 500k and whole 1m sentences.
All the ex-periments are run based on the configurationwith BLEU 13.41 in Table 4, and we call thisconfiguration BestConfig.
Note that the unknownwords are only allowed if the source-side of aphrase pair has more than 3 words.
Table 5shows the results.1431Training Data Tune Data (NIST MT03) Test Data (NIST MT05)2.08M sentence pairs in News35.79 34.26Tune Data (elec1000-tune) Test Data (elec1000-test)7.93 6.69Table 3: Experimental results using News training data to test NIST evaluation data and electronic data (numbersdenote BLEU score points in percent).Method Tune (elec1000-tune) Test (elec1000-test)Baseline 7.93 6.69baseline + in-domain lexicon 10.97 10.87baseline + in-domain lexicon + in-domain language model13.72 13.41++Transductive Learning 14.13 14.01*Table 4: Experimental results using News training data, in-domain lexicon, language model and transductivelearning.
Bold figures mean that the results are statistically significant better than the baseline with p<0.01, and?++?
denotes the result is statistically significant better than baseline+in-domain lexicon.
?*?
means that theresult is statistically significant better than 13.41 with p<0.05.Method Tune (BLEU %) Test (BLEU %)BestConfig 13.72 13.41+phrase pair induction (100k) 14.23 14.06+phrase pair induction (200k) 14.45 14.24+phrase pair induction (300k) 14.76 14.83+++phrase pair induction (500k) 14.98 15.16+++phrase pair induction (1m) 15.11 15.30++Table 5: Experimental results of our phrase pair induction method.
Bold figures denotes the correspondingmethod significantly outperform the BestConfig with p<0.05.
Bold and Italic figures means the results are sig-nificantly better than that of BestConfig with p<0.01.
?++?
denotes that the corresponding approach performssignificantly better than Transductive Learning with p<0.01.Method Before Filtering After Filtering+phrase pair induction (100k) 72,615 8,724+phrase pair induction (200k) 108,948 12,328+phrase pair induction (300k) 136,529 17,505+phrase pair induction (500k) 150,263 19,862+phrase pair induction (1m) 169,172 21,486Table 6: the number of phrase pairs induced with different size of monolingual data.We can see from the table that our method ob-tains the best translation performance.
When us-ing the first 100k sentences for phrase pair induc-tion, it obtains a significant improvement overthe BestConfig by 0.65 BLEU points and canoutperform the transductive learning method.When we use more monolingual data, the per-formance becomes even better.
The method ofphrase pair induction using 300k sentences per-forms quite well.
It outperforms the BestConfigsignificantly with an improvement of 1.42 BLEUpoints and it also performs much better thantransductive learning method with gains of 0.82BLEU points.
With the monolingual data largerand larger, the gains become smaller and smallerbecause the word hit rate gets lower and lower.These experimental results empirically show theeffectiveness of our proposed phrase pair induc-tion method.A question remains that how many new phrasepairs are induced with different size of monolin-gual data.
Here, we give respectively the statis-tics before and after filtering with the 1000 testsentences.
Table 6 shows the statistics.
We cansee from the table that lots of new phrase pairscan be induced since the source and target mono-lingual data is in the same domain.
However,since the source and target monolingual data is1432far from parallel, most of the phrase pairs are notlong.
For example, in the 108,948 distinct phrasepairs, we find that the phrase pair distributionaccording to source-side length is (3:50.6%,4:35.6%, 5:3.3%, 6:9.8%, 7:0.7%).
It is easy tosee that the phrase pairs whose source-sidelength longer than 4 account for only a verysmall part.8 Conclusion and Future WorkThis paper proposes a simple but effective meth-od to induce phrase pairs from monolingual data.Given the probabilistic bilingual lexicon andboth-side monolingual data in the same domain,the method employs inverted index structure torepresent the target-side monolingual data, andinduce the translations for each distinct source-side phrase with the help of the bilingual lexicon.We further propose an approach to refine the re-sult phrase pairs to make them more accurate.We also introduce how to estimate the translationand reordering parameters for the induced phrasepairs with monolingual data.
Extensive experi-ments on domain adaptation have shown that ourmethod can significantly outperform previousmethods which also focus on exploring the in-domain lexicon and monolingual data.However, through the analysis we find that ourinduced phrase pairs still contain some noise,such as the words in source- and target-side ofthe phrase pair are all aligned but the target-sidephrase expresses the different meaning.
Further-more, our proposed method cannot learn expres-sions which are not lexical translations but aresemantic ones.
In the future, we will study fur-ther on these phenomena and propose new meth-ods to handle these problems.AcknowledgmentsThe research work has been funded by the Hi-Tech Research and Development Program (?863?Program) of China under Grant No.2011AA01A207, 2012AA011101 and2012AA011102, and also supported by the KeyProject of Knowledge Innovation of Program ofChinese Academy of Sciences under Grant No.KGZD-EW-501.
We would also like to thank theanonymous reviewers for their valuable sugges-tions.ReferencesNicola Bertoldi and Marcello Federico, 2009.
Domainadaptation for statistical machine translation withmonolingual resources.
In Proc.
of the FourthWorkshop on Statistical Machine Translation,pages 182-189.Mauro Cettolo, Marcello Federico and NicolaBertoldi, 2010.
Mining parallel fragments fromcomparable texts.
In Proc.
of the seventhInternational Workshop on Spoken LanguageTranslation (IWSLT), pages 227-234.David Chiang, 2007.
Hierarchical phrase-basedtranslation.
computational linguistics, 33 (2).pages 201-228.David Chiang, 2010.
Learning to translate with sourceand target syntax.
In Proc.
of ACL 2010, pages1443-1452.Hal Daum?
III and Jagadeesh Jagarlamudi, 2011.Domain adaptation for machine translation bymining unseen words.
In Proc.
of ACL-HLT2011.Qing Dou and Kevin Knight, 2012.
Large ScaleDecipherment for Out-of-Domain MachineTranslation.
In Proc.
of EMNLP-CONLL 2012.Ted Dunning, 1993.
Accurate methods for thestatistics of surprise and coincidence.computational linguistics, 19 (1).
pages 61-74.Pascale Fung and Lo Yuen Yee, 1998.
An IRapproach for translating new words fromnonparallel, comparable texts.
In Proc.
of ACL-COLING 1998., pages 414-420.Michel Galley, Jonathan Graehl, Kevin Knight,Daniel Marcu, Steve DeNeefe, Wei Wang andIgnacio Thayer, 2006.
Scalable inference andtraining of context-rich syntactic translationmodels.
In Proc.
of COLING-ACL 2006, pages961-968.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrickand Dan Klein, 2008.
Learning bilinguallexicons from monolingual corpora.
In Proc.
ofACL-08: HLT, pages 771-779.Liang Huang, Kevin Knight and Aravind Joshi, 2006.A syntax-directed translator with extendeddomain of locality.
In Proc.
of AMTA 2006,pages 1-8.Alexandre Klementiev, Ann Irvine, Chris Callison-Burch and David Yarowsky, 2012.
Towardstatistical machine translation without parallelcorpora.
In Proc.
of EACL 2012., pages 130-140.Philipp Koehn, 2004.
Statistical significance tests formachine translation evaluation.
In Proc.
ofEMNLP 2004., pages 388-395, Barcelona, Spain,July 25th-26th, 2004.Philipp Koehn, Hieu Hoang, Alexandra Birch,Marcello Federico, Nicola Bertoldi, BrookeCowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ond?ej Bojar, AlexandraConstantin and Evan Herbst, 2007.
Moses: Opensource toolkit for statistical machine translation.In Proc.
of ACL on Interactive Poster andDemonstration Sessions 2007., pages 177-180,Prague, Czech Republic, June 27th-30th, 2007.Philipp Koehn and Kevin Knight, 2002.
Learning atranslation lexicon from monolingual corpora.
In1433Proc.
of the ACL-02 workshop on Unsupervisedlexical acquisition, pages 9-16.Yang Liu, Qun Liu and Shouxun Lin, 2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
of COLING-ACL 2006,pages 609-616.I.
Dan Melamed, 2000.
Models of translationalequivalence among words.
computationallinguistics, 26 (2).
pages 221-249.Rorbert C. Moore, 2004a.
Improving IBM word-alignment model 1.
In Proc.
of ACL 2004.Rorbert C. Moore, 2004b.
On log-likelihood-ratiosand the significance of rare events.
In Proc.
ofEMNLP 2004., pages 333-340.Dragos Stefan Munteanu and Daniel Marcu, 2006.Extracting parallel sub-sentential fragments fromnon-parallel corpora.
In Proc.
of ACL-COLING2006.Malte Nuhn, Arne Mauser and Hermann Ney, 2012.Deciphering Foreign Language by CombiningLanguage Models and Context Vectors.
In Proc.of ACL 2012.Franz Josef Och and Hermann Ney., 2003.
Asystematic comparison of various statisticalalignment models.
computational linguistics, 29(1).
pages 19-51.Kishore Papineni, Salim Roukos, Todd Ward andWei-Jing Zhu, 2002.
BLEU: a method forautomatic evaluation of machine translation.
InProc.
of ACL 2002., pages 311-318.Chris Quirk, Raghavendra Udupa and Arul Menezes,2007.
Generative models of noisy translationswith applications to parallel fragment extraction.In Proc.
of the Machine Translation Summit XI,pages 377-384.Reinhard Rapp, 1995.
Identifying word translations innon-parallel texts.
In Proc.
of ACL 1995, pages320-322.Reinhard Rapp, 1999.
Automatic identification ofword translations from unrelated English andGerman corpora.
In Proc.
of ACL 1999, pages519-526.Sujith Ravi and Kevin Knight, 2011.
Decipheringforeign language.
In Proc.
of ACL 2011., pages12-21.Holger Schwenk, 2008.
Investigations on largescalelightly-supervised training for statistical machinetranslation.
In Proc.
of IWSLT 2008, pages 182-189.Andreas Stolcke, 2002.
SRILM-an extensiblelanguage modeling toolkit.
In Proc.
of 7thInternational Conference on Spoken LanguageProcessing, pages 901-904, Denver, Colorado,USA, September 16th-20th, 2002.Nicola Ueffing, Gholamreza Haffari and AnoopSarkar, 2007.
Transductive learning for statisticalmachine translation.
In Proc.
of ACL 2007.Hua Wu, Haifeng Wang and Chengqing Zong, 2008.Domain adaptation for statistical machinetranslation with domain dictionary andmonolingual corpora.
In Proc.
of COLING 2008.,pages 993-1000.Feifei Zhai, Jiajun Zhang, Yu Zhou and ChengqingZong, 2011.
Simple but effective approaches toimproving tree-to-tree model.
In Proc.
of MTSummit XIII 2011, pages 261-268.Feifei Zhai, Jiajun Zhang, Yu Zhou and ChengqingZong, 2012.
Tree-based translation without usingparse trees.
In Proc.
of COLING 2012, pages3037-3054.Jiajun Zhang, Feifei Zhai and Chengqing Zong, 2011.Augmenting string-to-tree translation modelswith fuzzy use of the source-side syntax.
In Proc.of EMNLP 2011, pages 204-215.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan and Sheng Li, 2008.
A treesequence alignment-based tree-to-tree translationmodel.
In Proc.
of ACL-08: HLT, pages 559-567.1434
