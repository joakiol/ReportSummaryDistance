Selecting Optimal Feature Template Subset for CRFsXingjun Xu1 and Guanglu Sun2 and Yi Guan1 andXishuang Dong1 and Sheng Li11: School of Computer Science and Technology,Harbin Institute of Technology,150001, Harbin, China2: School of Computer Science and Technology,Harbin University of Science and Technology150080, Harbin, Chinaxxjroom@163.com; guanglu.sun@gmail.comguanyi@hit.edu.cn; dongxishuang@gmail.comlisheng@hit.edu.cnAbstractConditional Random Fields (CRFs) are thestate-of-the-art models for sequential labe-ling problems.
A critical step is to selectoptimal feature template subset before em-ploying CRFs, which is a tedious task.
Toimprove the efficiency of this step, we pro-pose a new method that adopts the maxi-mum entropy (ME) model and maximumentropy Markov models (MEMMs) insteadof CRFs considering the homology be-tween ME, MEMMs, and CRFs.
Moreover,empirical studies on the efficiency and ef-fectiveness of the method are conducted inthe field of Chinese text chunking, whoseperformance is ranked the first place intask two of CIPS-ParsEval-2009.1 IntroductionConditional Random Fields (CRFs) are the state-of-the-art models for sequential labeling problem.In natural language processing, two aspects ofCRFs have been investigated sufficiently: one is toapply it to new tasks, such as named entity recog-nition (McCallum and Li, 2003; Li and McCallum,2003; Settles, 2004), part-of-speech tagging (Laf-ferty et al, 2001), shallow parsing (Sha and Perei-ra, 2003), and language modeling (Roark et al,2004); the other is to exploit new training methodsfor CRFs, such as improved iterative scaling (Laf-ferty et al, 2001), L-BFGS (McCallum, 2003) andgradient tree boosting (Dietterich et al, 2004).One of the critical steps is to select optimal fea-ture subset before employing CRFs.
McCallum(2003) suggested an efficient method of featureinduction by iteratively increasing conditional log-likelihood for discrete features.
However, sincethere are millions of features and feature selectionis an NP problem, this is intractable when search-ing optimal feature subset.
Therefore, it is neces-sary that selects feature at feature template level,which reduces input scale from millions of fea-tures to tens or hundreds of candidate templates.In this paper, we propose a new method thatadopts ME and MEMMs instead of CRFs to im-prove the efficiency of selecting optimal featuretemplate subset considering the homology betweenME, MEMMs, and CRFs, which reduces the train-ing time from hours to minutes without loss ofperformance.The rest of this paper is organized as follows.Section 2 presents an overview of previous workfor feature template selection.
We propose our op-timal method for feature template selection in Sec-tion 3.
Section 4 presents our experiments and re-sults.
Finally, we end this paper with some con-cluding remarks.2 Related WorkFeature selection can be carried out from two le-vels: feature level (feature selection, or FS), orfeature template level (feature template selection,or FTS).
FS has been sufficiently investigated andshare most concepts with FTS.
For example, thetarget of FS is to select a subset from original fea-ture set, whose optimality is measured by an eval-uation criterion (Liu and Yu, 2005).
Similarly, thetarget of FTS is to select a subset from originalfeature template set.
To achieve optimal featuresubset, two problems in original set must be elimi-nated: irrelevance and redundancy (Yu and Liu,2004).
The only difference between FS and FTS isthat the number of elements in feature template setis much less than that in feature set.Liu and Yu (2005) classified FS models intothree categories: the filter model, the wrappermodel, and the hybrid model.
The filter model(Hall 2000; Liu and Setiono, 1996; Yu and Liu,2004) relies on general characteristics of the datato evaluate and select feature subsets without anymachine learning model.
The wrapper model (Dyand Brodley, 2000; Kim et al, 2000; Kohavi andJohn, 1997) requires one predetermined machinelearning model and uses its performance as theevaluation criterion.
The hybrid model (Das, 2001)attempts to take advantage of the two models byexploiting their different evaluation criteria in dif-ferent search stages.There are two reasons to employ the wrappermodel to accomplish FTS: (1) The wrapper modeltends to achieve better effectiveness than that ofthe filter model with respect of a more direct eval-uation criterion; (2) The computational cost is trac-table because it can reduce the number of subsetssharply by heuristic algorithm according to thehuman knowledge.
And our method belongs tothis type.Lafferty (2001) noticed the homology betweenMEMMs and CRFs, and chose optimal MEMMsparameter vector as a starting point for training thecorresponding CRFs.
And the training process ofCRFs converges faster than that with all zero pa-rameter vectors.On the other hand, the general framework thatprocesses sequential labeling with CRFs has alsobeen investigated well, which can be described asfollows:1.
Converting the new problem to sequentiallabeling problem;2.
Selecting optimal feature template subset forCRFs;3.
Parameter estimation for CRFs;4.
Inference for new data.In the field of English text chunking (Sha andPereira, 2003), the step 1, 3, and 4 have been stu-died sufficiently, whereas the step 2, how to selectoptimal feature template subset efficiently, will bethe main topic of this paper.3 Feature Template Selection3.1 The Wrapper Model for FTSThe framework of FTS based on the wrappermodel for CRFs can be described as:1.
Generating the new feature template subset;2.
Training a CRFs model;3.
Updating optimal feature template subset if thenew subset is better;4.
Repeating step 1, 2, 3 until there are no newfeature template subsets.Let N denote the number of feature templates,the number of non-empty feature template subsetswill be (2N-1).
And the wrapper model is unable todeal with such case without heuristic methods,which contains:1.
Atomic feature templates are firstly added tofeature template subset, which is carried out by:Given the position i, the current word Wi and thecurrent part-of-speech Pi are firstly added to cur-rent feature template subset, and then Wi-1 and Pi-1,or Wi+1 and Pi+1, and so on, until the effectivenessis of no improvement.
Taking the Chinese textchunking as example, optimal atomic feature tem-plate subset is {Wi-3~Wi+3, Pi-3~Pi+3};2.
Adding combined feature templates properlyto feature template set will be helpful to improvethe performance, however, too many combinedfeature templates will result in severe data sparse-ness problem.
Therefore, we present three restric-tions for combined feature templates: (1) A com-bined feature template that contains more thanthree atomic templates are not allowable; (2) If acombined feature template contains three atomicfeature template, it can only contain at most oneatomic word template; (3) In a combined template,at most one word is allowable between the twomost adjacent atomic templates; For example, thecombined feature templates, such as {Pi-1, Pi, Pi+1,Pi+2}, {Wi, Wi+1, Pi},  and {Pi-1, Pi+2}, are not al-lowable, whereas the combined templates, such as{Pi, Pi+1, Pi+2}, {Pi-1, Wi, Pi+1}, and {Pi-1, Pi+1}, areallowable.3.
After atomic templates have been added, {Wi-1, Wi}, or {Wi, Wi+1}, or {Pi-1, Pi}, or {Pi, Pi+1} arefirstly added to feature template subset.
The tem-plate window is moved forward, and then back-ward.
Such process will repeat with expandingtemplate window, until the effectiveness is of noimprovement.Tens or hundreds of training processes are stillneeded even if the heuristic method is introduced.People usually employ CRFs model to estimate theeffectiveness of template subset However, this ismore tedious than that we use ME or MEMMsinstead.
The idea behind this lie in three aspects:first, in one iteration, the Forward-Backward Al-gorithm adopted in CRFs training is time-consuming; second, CRFs need more iterationsthan that of ME or MEMMs to converge becauseof larger parameter space; third, ME, MEMMs,and CRFs, are of the same type (log-linear models)and based on the same principle, as will be dis-cussed in detail as follows.3.2 Homology of ME, MEMMs and CRFsME, MEMMs, and CRFs are all based on the Prin-ciple of Maximum Entropy (Jaynes, 1957).
Themathematical expression for ME model is as for-mula (1):11( | ) exp( ( , ))( )mi iiP y x x yZ x f???
?
(1), and Z(x) is the normalization factor.MEMMs can be considered as a sequential ex-tension to the ME model.
In MEMMs, the HMMtransition and observation functions are replacedby a single function P(Yi|Yi-1, Xi).
There are threekinds of implementations of MEMMs (McCallumet al, 2000) in which we realized the second typefor its abundant expressiveness.
In implementationtwo, which is denoted as MEMMs_2 in this paper,a distributed representation for the previous stateYi-1 is taken as a collection of features withweights set by maximum entropy, just as we havedone for the observations Xi.
However, label biasproblem (Lafferty et al, 2001) exists in MEMMs,since it makes a local normalization of randomfield models.
CRFs overcome the label bias prob-lem by global normalization.Considering the homology between CRFs andMEMMs_2 (or ME), it is reasonable to supposethat a useful template for MEMMs_2 (or ME) isalso useful for CRFs, and vice versa.
And this is anecessary condition to replace CRFs with ME orMEMMs for FTS.3.3 A New Framework for FTSBesides the homology of these models, the othernecessary condition to replace CRFs with ME orMEMMs for FTS is that all kinds of feature tem-plates in CRFs can also be expressed by ME orMEMMs.
There are two kinds of feature templatesfor CRFs: one is related to Yi-1, which is denotedas g(Yi-1, Yi, Xi); the other is not related to Yi-1,which is denoted as f(Yi, Xi).
Both of them can beexpressed by MEMMs_2.
If there is only thesecond kind of feature templates in the subset, itcan also be expressed by ME.
For example, thefeature function f(Yi, Pi) in CRFs can be expressedby feature template {Pi} in MEMMs_2 or ME; andg(Yi-1, Yi, Pi) can be expressed by feature template{Yi-1, Pi} in MEMM_2.Therefore, MEMMs_2 or ME can be employedto replace CRFs as machine learning model forimproving the efficiency of   FTS.Then the new framework for FTS will be:1.
Generating the new feature template subset;2.
Training an MEMMs_2 or ME model;3.
Updating optimal feature template subsetif the new subset is better;4.
Repeating step 1, 2, 3 until there are nonew feature template subsets.The wrapper model evaluates the effectivenessof feature template subset by evaluating the modelon testing data.
However, there is a serious effi-ciency problem when decoding a sequence byMEMMs_2.
Given N as the length of a sentence,C as the number of candidate labels, the timecomplexity based on MEMMs_2 is O(NC2) whendecoding by viterbi algorithm.
Considering the Cdifferent Yi-1 for every word in a sentence, weneed compute P(Yi|Yi-1, Xi) (N.C) times forMEMMs_2.Reducing the average number of candidate labelC can help to improve the decoding efficiency.And in most cases, the Yi-1 in P(Yi|Yi-1, Xi) is notnecessary (Koeling, 2000; Osbome, 2000).
There-fore, to reduce the average number of candidatelabels C, it is reasonable to use an ME model tofilter the candidate label.
Given a threshold T (0<= T <= 1), the candidate label filtering algorithmis as follows:1.
CP = 0;2.
While CP <= Ta) Add the most probable candidate label Y?to viterbi algorithm;b) Delete Y?
from the candidate label set;c) CP = P(Y?|Xi) + CP.If the probability of the most probable candidatelabel has surpassed T, other labels are discarded.Otherwise, more labels need be added to viterbialgorithm.4 Evaluation and Result4.1 EvaluationWe evaluate the effectiveness and efficiency of thenew framework by the data set in the task two ofCIPS-ParsEval-2009 (Zhou and Li, 2010).
Theeffectiveness is supported by high F-1 measure inthe task two of CIPS-ParsEval-2009 (see Figure 1),which shows that optimal feature template subsetdriven by ME or MEMMs is also optimal forCRFs.
The efficiency is shown by significant de-cline in training time (see Figure 3), where thebaseline is CRFs, and comparative methods areME or MEMMs.We design six subsets of feature template setand six experiments to show the effectiveness andefficiency of the new framework.
As shown inTable 1 and Table 2, the 1~3 experiments showsthe influence of the feature templates, which areunrelated to Yi-1, for both ME and CRFs.
And the4~6 experiments show the influence of the featuretemplates, which are related to Yi-1, for bothMEMMs_2 and CRFs.
In table 1, six templatesubsets can be divided into two sets by relevanceof previous label: 1, 2, 3 and 4, 5, 6.
Moreover, thefirst set can be divided into 1, 2, and 3 by distancesbetween features with headwords;  the second setcan be divided into 4, 5 and 6 by relevance of ob-served value.
In order to ensure the objectivity ofcomparative experiments, candidate label filteringalgorithm is not adopted.Figure 1: the result in the task two of CIPS-ParsEval-20091 Wi, Wi-1, Wi-2, Wi+1, Wi+2, Pi, Pi-1, Pi-2, Pi+1,Pi+2, Wi-1_Wi, Wi_Wi+1, Wi-1_Wi+1, Pi-1_Pi,Pi-2_Pi-1, Pi_Pi+1, Pi-1_Pi+1, Pi-1_Pi_Pi+1, Pi-2_Pi-1_Pi,     Pi_Pi+1_Pi+2, Wi_Pi+1, Wi_Pi+2,Pi_Wi-1, Wi-2_Pi-1_Pi, Pi_Wi+1_Pi+1, Pi-1_Wi_Pi, Pi_Wi+12 Wi-3, Wi+3, Pi-3, Pi+3, Wi-3_Wi-2, Wi+2_Wi+3,Pi-3_Pi-2, Pi+2_Pi+33 Wi-4, Wi+4, Pi-4, Pi+4, Wi-4_Wi-3, Wi+3_Wi+4,Pi-4_Pi-3, Pi+3_Pi+44 Yi-15 Yi-1_Pi_Pi+1, Yi-1_Pi, Yi-1_Pi-1_Pi6 Yi-1_Pi-4, Yi-1_Pi+4Table 1: six subsets of feature template setid Model FT subset1 ME vs. CRFs 12 ME vs. CRFs 1, 23 ME vs. CRFs 1, 2, 34 MEMMs vs. CRFs 1, 2, 45 MEMMs vs. CRFs 1, 2, 4, 56 MEMMs vs. CRFs 1, 2, 4, 5, 6Table 2: six experiments4.2 Empirical ResultsThe F-measure curve is shown in Figure 2.
For thesame and optimal feature template subset, the F-1measure of CRFs is superior to that of ME becauseof global normalization; and it is superior to that ofMEMMs since it overcomes the label bias.Figure 2: the F-measure curveFigure 3: the training time curveThe significant decline in training time of thenew framework is shown in Figure 3, while thetesting time curve in Figure 4 and the total timecurve in Figure 5.
The testing time of ME is morethan that of CRFs because of local normalization;and the testing time of MEMMs_2 is much morethan that of CRFs because of N.C times of P(Yi|Yi-1, Xi) computation.Figure 4: the testing time curveFigure 5: the total time curveAll results of ME and MEMMs in figures arerepresented by the same line because perfor-mances of these two models are the same whenfeatures are only related to observed values.5 ConclusionsIn this paper, we propose a new optimal featuretemplate selection method for CRFs, which is car-ried out by replacing the CRFs with MEMM_2(ME) as the machine learning model to address theefficiency problem according to the homology ofthese models.
Heuristic method and candidate la-bel filtering algorithm, which can improve the ef-ficiency of FTS further, are also introduced.
Theeffectiveness and efficiency of the new method isconfirmed by the experiments on Chinese textchunking.Two problems deserve further study: one is toprove the homology of ME, MEMMs, and CRFstheoretically; the other is to expand the method toother fields.For any statistical machine learning model, fea-ture selection or feature template selection is acomputation-intensive step.
This work can be ade-quately reduced by means of analyzing the homol-ogy between models and using the model with lesscomputation amount.
Our research proves to be asuccessful attempt.ReferencesDas Sanmay.
2001.
Filters, wrappers and a boosting-based hybrid for feature selection.
In Proceedings ofthe Eighteenth International Conference on MachineLearning, pages 74?81.Dietterich Thomas G., Adam Ashenfelter, YaroslavBulatov.
2004.
Training Conditional Random Fieldsvia Gradient Tree Boosting.
In Proc.
of the 21th In-ternational Conference on Machine Learning(ICML).Dy Jennifer G., and Carla E. Brodley.
2000.
Featuresubset selection and order identification for unsuper-vised learning.
In Proceedings of the Seventeenth In-ternational Conference on Machine Learning, pages247?254.Hall Mark A.. 2000.
Correlation-based feature selectionfor discrete and numeric class machine learning.
InProceedings of the Seventeenth International Confe-rence on Machine Learning, pages 359?366.Jaynes, Edwin T.. 1957.
Information Theory and Statis-tical Mechanics.
Physical Review 106(1957), May.No.4, pp.
620-630.Kim YongSeog, W. Nick Street and Filippo Menczer.2000.
Feature Selection in Unsupervised Learningvia Evolutionary Search.
In Proceedings of the SixthACM SIGKDD International Conference on Know-ledge Discovery and Data Mining, pages 365?369.Koeling Rob.
2000.
Chunking with Maximum EntropyModels.
In Proceeding of CoNLL-2000 and LLL-2000, Lisbon, Portugal, 2000, pp.
139-141.Kohavi Ron, and George H. John.
1997.
Wrappers forfeature subset selection.
Artificial Intelligence, 97(1-2):273?324.Lafferty John, Andrew McCallum, and Fernando Perei-ra.
2001.
Conditional Random Fields: ProbabilisticModels for Segmenting and Labeling Sequence Data.Proceedings of the Eighteenth International Confe-rence on Machine Learning.Li Wei, and Andrew McCallum.
2003.
Rapid Devel-opment of Hindi Named Entity Recognition usingConditional Random Fields and Feature Induction.ACM Transactions on Asian Language InformationProcessing (TALIP).Liu Huan, and Lei Yu.
2005.
Toward Integrating Fea-ture Selection Algorithms for Classification andClustering.
IEEE Transactions on knowledge andData Engineering, v.17 n.4, p.491-502.Liu Huan, and Rudy Setiono.
1996.
A probabilistic ap-proach to feature selection - a filter solution.
In Pro-ceedings of the Thirteenth International Conferenceon Machine Learning, pages 319?327.McCallum Andrew.
2003.
Efficiently Inducing Featuresof Conditional Random Fields.
In Proceedings of theNineteenth Conference on Uncertainty in ArtificialIntelligence.McCallum Andrew, DAyne Freitag, Fernando Pereira.2000.
Maximum Entropy Markov Models for Infor-mation Extraction and Segmentation.
In Proceedingsof ICML'2000, Stanford, CA, USA, 2000, pp.
591-598.McCallum Andrew, and Wei Li.
2003.
Early Results forNamed Entity Recognition with Conditional RandomFields, Feature Induction and Web-Enhanced Lex-icons.
In Proceedings of The Seventh Conference onNatural Language Learning (CoNLL-2003), Edmon-ton, Canada.Osbome Miles.
2000.
Shallow Parsing as Part-of-speech Tagging.
In Proceeding of CoNLL-2000 andLLL-2000, Lisbon, Portugal, 2000,pp.
145-147.Roark Brian, Murat Saraclar, Michael Collins, andMark Johnson.
2004.
Discriminative language mod-eling with conditional random fields and the percep-tron algorithm.
Proceedings of the 42nd AnnualMeeting of the Association for Computational Lin-guistics.Settles Burr.
2004.
Biomedical Named Entity Recogni-tion Using Conditional Random Fields and Rich Fea-ture Sets.
COLING 2004 International Joint Work-shop on Natural Language Processing in Biomedi-cine and its Applications (NLPBA).Sha Fei, and Fernando Pereira.
2003.
Shallow Parsingwith Conditional Random Fields.
Proceedings of the2003 conference of the North American Chapter ofthe Association for Computational Linguistics onHuman Language Technology, Edmonton, Canada.Yu Lei, and Huan Liu.
2004.
Feature selection for high-dimensional data: a fast correlation-based filter solu-tion.
In Proceedings of the twentieth InternationalConference on Machine Learning, pages 856?863.Zhou Qiang, and Yumei Li.
2010.
Chinese Chunk Pars-ing Evaluation Tasks.
Journal of Chinese Informa-tion Processing.
