Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 55?64,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsNested Propositions in Open Information ExtractionNikita BhutaniDepartment of EECSUniversity of MichiganAnn Arbornbhutani@umich.eduH.
V. JagadishDepartment of EECSUniversity of MichiganAnn Arborjag@umich.eduDragomir RadevDepartment of EECSUniversity of MichiganAnn Arborradev@umich.eduAbstractThe challenges of Machine Reading andKnowledge Extraction at a web scale re-quire a system capable of extracting diverseinformation from large, heterogeneous cor-pora.
The Open Information Extraction (OIE)paradigm aims at extracting assertions fromlarge corpora without requiring a vocabularyor relation-specific training data.
Most sys-tems built on this paradigm extract binary re-lations from arbitrary sentences, ignoring thecontext under which the assertions are cor-rect and complete.
They lack the expres-siveness needed to properly represent and ex-tract complex assertions commonly found inthe text.
To address the lack of representa-tion power, we propose NESTIE, which usesa nested representation to extract higher-orderrelations, and complex, interdependent asser-tions.
Nesting the extracted propositions al-lows NESTIE to more accurately reflect themeaning of the original sentence.
Our ex-perimental study on real-world datasets sug-gests that NESTIE obtains comparable preci-sion with better minimality and informative-ness than existing approaches.
NESTIE pro-duces 1.7-1.8 times more minimal extractionsand achieves 1.1-1.2 times higher informative-ness than CLAUSIE.1 IntroductionSyntactic analyses produced by syntactic parsers area long way from representing the full meaning of thesentences parsed.
In particular, they cannot supportquestions like ?Who did what to whom?
?, ?Wheredid what happen??.
Owing to the large, hetero-geneous corpora available at web scale, traditionalapproaches to information extraction (Brin, 1998;Agichtein and Gravano, 2000) fail to scale to themillions of relations found on the web.
As a re-sponse, the paradigm of Open Information Extrac-tion (OIE) (Banko et al, 2007) has seen a rise ininterest as it eliminates the need for domain knowl-edge or relation-specific annotated data.
OIE sys-tems use a collection of patterns over the surfaceform or dependency tree of a sentence to extractpropositions of the form (arg1,rel,arg2).However, state-of-the-art OIE systems, REVERB(Fader et al, 2011) and OLLIE (Schmitz et al, 2012)focus on extracting binary assertions and suffer fromthree key drawbacks.
First, lack of expressivity ofrepresentation leads to significant information lossfor higher-order relations and complex assertions.This results in incomplete, uniformative and inco-herent prepositions.
Consider Example 1 in Fig-ure 1.
Important contextual information is eitherignored or is subsumed in over-specified argumentand relation phrases.
It is not possible to fix suchnuances by post-processing the propositions.
Thisaffects downstream applications like Question An-swering (Fader et al, 2014) which rely on correct-ness and completeness of the propositions.Second, natural language frequently includes re-lations presented in a non-canonical form that can-not be captured by a small set of extraction pat-terns that only extract relation mediated by verbsor a subset of verbal patterns.
Consider Ex-ample 2 in Figure 1 that asserts, ?Rozsa Hillis the third hill near the river?, ?Rozsa Hill isRose Hill?
and ?Rozsa Hill lies north of Cas-tle Hill?.
A verb-mediated pattern would extract551.
After giving 5,000 people a second chance at life, doctors are celebrating the 25th anniversary of Britain's first heart transplant.R: P1: (doctors, are celebrating the 25th anniversary of, Britain 's first heart transplant)O: P1: (doctors, are celebrating, the 25th anniversary of Britain's first heart transplant)N:P1: (doctors, are celebrating, the 25th anniversary of Britain's first heart transplant)P2: (doctors, giving, second chance at life)P3: (P1, after, P2)2.
Rozsa ( Rose ) Hill , the third hill near the river, lies north of Castle Hill.R: P1: (the third hill, lies north of, Castle Hill)O: P1: (the third hill, lies north of, Castle Hill)N: P1: (Rozsa, lies, north of Castle Hill)P2: (Rozsa Hill, is, third hill near the river)P3: (Rozsa Hill, is, Rose)3.
?A senior official in Iraq said the body, which was found by U.S. military police, appeared to have been thrown from a vehicle.
?R: P1: (Iraq, said, the body)P2: (the body, was found by, U.S. military police)O: P1: (A senior official in Iraq, said, the body which was found by U.S. military police)N:P1: (body, appeared to have been thrown, ?
)P2: (P1, from, vehicle)P3: (A senior official in Iraq, said, P2)P4: (U.S. military police, found, body)Figure 1: Example propositions from OIE systems: REVERB(R), OLLIE (O) and NESTIE(N).a triple, (the third hill, lies northof, Castle Hill) that is less informative thana triple, (Rozsa, lies, north of CastleHill) which is not mediated by a verb in theoriginal sentence.
Furthermore, these propositionsare not complete.
Specifically, queries of the form?What is the other name of Rozsa Hill?
?, ?Where isRozsa Hill located?
?, ?Which is the third hill nearthe river??
will either return no answer or return anuninformative answer with these propositions.
Sinceinformation is encoded at various granularity levels,there is a need for a representation rich enough to ex-press such complex relations and sentence construc-tions.Third, OIE systems tend to extract propositionswith long argument phrases that are not minimaland are difficult to disambiguate or aggregate fordownstream applications.
For instance, the argu-ment phrase, body which was found by U.S. militarypolice, is less likely to be useful than the argumentphrase, body in Example 3 in Figure 1.In this paper we present NESTIE, which over-comes these limitations by 1) expanding the propo-sition representation to nested expressions so addi-tional contextual information can be captured, 2)expanding the syntactic scope of relation phrasesto allow relations mediated by other syntactic en-tities like nouns, adjectives and nominal modifiers.NESTIE bootstraps a small set of extraction pat-terns that cover simple sentences and learns broad-coverage relation-independent patterns.
We believethat it is possible to adapt OIE systems that extractverb-based relations to process assertions denotingevents with many arguments, and learn other non-clausal relations found in the text.
With weakly-supervised learning techniques, patterns encodingthese relations can be learned from a limited amountof data containing sentence equivalence pairs.This article is organized as follows.
We pro-vide background on OIE in Sec.
2 followed by anoverview of our proposed solution in Sec.
3.
Wethen discuss how the extraction patterns for nestedrepresentations are learned in Sec.
4.
In Sec.
5,we compare NESTIE against alternative methods ontwo datasets: Wikipedia and News.
In Sec.
6, wediscuss related work on pattern-based informationextraction.2 BackgroundThe key goal of OIE is to obtain a shallow seman-tic representation of the text in the form of tuplesconsisting of argument phrases and a phrase thatexpresses the relation between the arguments.
Thephrases are identified automatically using domain-independent syntactic and lexical constraints.
SomeOIE systems are:TextRunner (Yates et al, 2007) WOE (Wu andWeld, 2010): They use a sequence-labeling graph-ical model on extractions labeled automatically us-ing heuristics or distant supervision.
Consequently,long-range dependencies, holistic and lexical as-pects of relations tend to get ignored.ReVerb (Fader et al, 2011): Trained with shallowsyntactic features, REVERB uses a logistic regres-sion classifier to extract relations that begin with a56verb and occur between argument phrases.Ollie (Schmitz et al, 2012): Bootstrapping fromREVERB extractions, OLLIE learns syntactic andlexical dependency parse-tree patterns for extrac-tion.
Some patterns reduce higher order relationsto ReVerb-style relation phrases.
Also, representa-tion is extended optionally to capture contextual in-formation about conditional truth and attribution forextractions.ClausIE (Del Corro and Gemulla, 2013): Us-ing linguistic knowledge and a small set of domain-independent lexica, CLAUSIE identifies and classi-fies clauses into clause types, and then generates ex-tractions based on the clause type.
It relies on a pre-defined set of rules on how to extract assertions in-stead of learning extraction patterns.
Also, it doesn?tcapture the relations between the clauses.There has been some work in open-domain in-formation extraction to extract higher-order rela-tions.
KRAKEN (Akbik and Lo?ser, 2012) uses apredefined set of rules based on dependency parseto identify fact phrases and argument heads withinfact phrases.
But unlike alternative approaches,it doesn?t canonicalize the fact phrases.
There isanother body of work in natural language under-standing that shares tasks with OIE.
AMR parsing(Banarescu et al, ), semantic role labeling (SRL)(Toutanova et al, 2008; Punyakanok et al, 2008)and frame-semantic parsing (Das et al, 2014).
Inthese tasks, verbs or nouns are analyzed to identifytheir arguments.
The verb or noun is then mapped toa semantic frame and roles of each argument in theframe are identified.
These techniques have gainedinterest with the advent of hand-constructed seman-tic resources like PropBank and FrameNet (Kings-bury and Palmer, 2002; Baker et al, 1998).
Gener-ally, the verb/noun and the semantically labeled ar-guments correspond to OIE propositions and, there-fore, the two tasks are considered similar.
Systemslike SRL-IE (Christensen et al, 2010) explore ifthese techniques can be used for OIE.
However,while OIE aims to identify the relation/predicate be-tween a pair of arguments, frame-based techniquesaim to identify arguments and their roles with re-spect to a predicate.
Hence, the frames won?t corre-spond to propositions when both the arguments can-not be identified for a binary relation or when thecorrect argument is buried in long argument phrases.DatasetSeed TemplatesPattern RepresentationFact ExtractionBootstrappingSyntactic paraphrasesSyntactic Patterns PropositionsPattern LearningFact ExtractionSeed ExtractionStatement Proposition Extraction Proposition LinkingPattern LearningPattern RepresentationFigure 2: System Architecture of NESTIE.3 Task Definition and NestIE OverviewTask: We focus on the task of OIE, where the sys-tem takes a natural language statement and extractsthe supported assertions.
This is achieved by us-ing an extractor that uses nested representations toextract propositions and a linker that connects ex-tracted propositions to capture context.Proposition-based Extractor: We propose aframework to extend open-domain binary-relationextractors to extract n-ary and complex rela-tions.
As not all assertions can be expressed as(arg1,rel,arg2), we learn syntactic patternsfor relations that are expressed as nested templateslike, (arg1,rel,(arg2,rel2,arg3)),((arg1,rel,arg2),rel2,arg3).Proposition Linking: In practice, it is infeasibleto enumerate simple syntactic pattern templates thatcapture the entire meaning of a sentence.
Also, in-creasing the complexity of templates would lead tosparsity issues while bootstrapping.
We assume thatthere is a finite set of inter-proposition relations thatcan be captured using a small set of rules which takeinto account the structural properties of the propo-sitions and syntactic dependencies between the rela-tion phrases of the propositions.System Evaluation: To compare NESTIE toother alternative methods, we conduct an experi-mental study on two real-world datasets: Wikipediaand News.
Propositions from each system are eval-uated for correctness, minimality, and informative-ness.57Template ExamplePattern: A body has been found by police.Representation: T: (arg1, [rel, by], arg2) (body, [found, by], police)Pattern: Fallujah is an Iraqi city.Representation: T: (arg1, be, arg2) (Fallujah, is, city)Pattern: Ghazi al-Yawar is new president of Iraq.Representation: T: (arg1, be, [arg2, rel2, arg3]) (Yawar, is, [president, of, Iraq])Pattern: 10,000 people in Africa died of Ebola.Representation: T1:([arg1, rel2, arg3], rel, arg2]T2: (T1, rel3, arg4) T1: ([people, in, Africa], died, ?)
T2: (T1, of, Ebola)arg1 arg2 relnsubj coparg1 arg2 relnsubjcoparg3rel2 = nmod(?!:agent).
*arg1 rel arg2nsubjpass nmod:agentarg1 rel | VB* arg2nsubjarg4rel3 = nmod(?!:agent).
*arg3rel2 = nmod.
* dobjFigure 3: Seed templates and corresponding representation.4 Proposition ExtractionFigure 2 illustrates the system architecture ofNESTIE.
First, a set of high-precision seed tem-plates is used to extract propositions.
A templatemaps a dependency parse-tree pattern to a triplerepresentation such as (arg1,rel,arg2) for bi-nary relations, or a nested triple representation suchas ((arg1,rel,arg2),rel2,arg3) for n-ary relations.
Furthermore, an argument is allowedto be a sequence of words, ?arg2 rel2 arg3?to capture its nominal modifiers.
Then, using a RTEdataset that contains syntactic paraphrases, NESTIElearns equivalent parse-tree patterns for each tem-plate in the seed set.
These patterns are used to ex-tract propositions which are then linked.4.1 Constructing Seed SetWe use a set of 13 hand-written templates.
Eachtemplate maps an extraction pattern for a simplesentence to corresponding representation.
A sub-set of these templates is shown in Figure 3.
Tocreate a seed set of propositions, we use the RTEdataset which is comprised of statements and theirentailed hypotheses.
We observed that most of thehypotheses were syntactic variants of the facts intheir corresponding statements.
These hypotheseswere also short with a single, independent clause.These shared sentence constructions could be cap-tured with a small set of templates.
We iterativelycreate templates until at least one proposition couldbe extracted for each hypothesis.
The propositionsfrom the hypotheses form the set for bootstrapping.For each seed proposition extracted from a hy-pothesis, the statement entailing the hypothesis con-tains all the content words of the proposition andexpresses the same information as the proposition.However, there is a closed class of words, such asprepositions, a subset of adverbs, determiners, verbsetc.
that does not modify the underlying meaning ofthe hypothesis or the statement and can be consid-ered auxiliary.
These were ignored while construct-ing the seed set.Example 1 Consider a statement-hypothesis pair,Statement: Paul Bremer, the top U.S. civilian admin-istrator in Iraq, and Iraq?s new president, Ghazi al-Yawar, visited the northern Iraqi city of Kirkuk.Hypothesis: Ghazi al-Yawar is the president of Iraq.The hypothesis is entailed in the statement.The seed templates extract propositions fromthe hypothesis: (al-Yawar,is,president,(al-Yawar,is,president of Iraq), and(al-Yawar,is president of,Iraq).Bootstrapping is a popular technique to gener-ate positive training data for information extraction(Collins and Singer, 1999; Hoffmann et al, 2011).We extend the bootstrapping techniques employed58in OLLIE and RENOUN, for n-ary and complex re-lations.
First, instead of learning dependency parse-tree patterns connecting the heads of the argumentphrases and the relation phrase connecting them, welearn the dependency parse-tree patterns connect-ing the heads of all argument and relation phrasesin the template.
This allows greater coverage ofcontext for the propositions and prevents the argu-ments/relations from being over-specified and/or un-informative.
Second, some of the relations in therepresentation are derived from the type of depen-dency, e.g.
type of nominal modifier.
As theserelations are implicit, and might not be present inthe paraphrase, they are ignored for learning.
In-tuitively, with such constraints, paraphrases ?Marygave John a car?
and ?Mary gave a car to John?can map to the same representation.4.2 Extraction Pattern LearningThe biggest challenge in information extraction isthe multitude of ways in which information can beexpressed.
Since it is not possible to enumerateall the different syntactic variations of an assertion,there is a need to learn general patterns that encodethe various ways of expressing the assertion.
In par-ticular, we learn the various syntactic patterns thatcan encode the same information as the seed patternsand hence can be mapped to same representation.NESTIE tries to learn the different ways in whichthe content words of a seed proposition from a hy-pothesis can be expressed in the statement that en-tails this hypothesis.
We use the Stanford depen-dency parser (De Marneffe et al, 2006) to parsethe statement and identify the path connecting thecontent words in the parse tree.
If such a path ex-ists, we retain the syntactic constraints on the nodesand edges in the path and ignore the surface formsof the nodes in the path.
This helps generalize thelearned patterns to unseen relations and arguments.NESTIE could learn 183 templates from the 13 seedtemplates.
Figure 4 shows a subset of these patterns.Example 2 Consider dependency parse-subtree ofthe statement and hypothesis from Example 1,Statement: Iraq poss??
president appos??
al ?
Y awarHypothesis: al?Y awar nsubj??
president of??
IraqA seed extraction pattern maps the parse-tree of the hypothesis to the representation,(arg1, be, arg2), returning proposition,(al-Yawar,is,president of Iraq).With bootstrapping, the syntactic pattern from thestatement is mapped to the same representation.4.3 Pattern MatchingOnce the extraction patterns are learned, we usethese patterns to extract propositions from new un-seen sentences.
We first parse a new sentence andmatch the patterns against the parse tree.
As the pat-terns only capture the heads of the arguments andrelations, we expand the extracted propositions toincrease the coverage of context of the argumentsas in the original sentence.Example 3 In the statement from Example 1, theextraction patterns capture the dependency path con-necting the head words: Iraq, administratorand Paul Bremer.
However, to capture the con-textual information, further qualification of the argu-ment node, administrator, is required.Following this observation, we expand thearguments on nmod, amod, compound,nummod, det, neg edges.
We expand therelations on advmod, neg, aux, auxpass,cop, nmod edges.
Only the dependency edges notcaptured in the pattern are considered for expansion.Also, the order of words from the original sentenceis retained in the argument phrases.4.4 Proposition LinkingNESTIE uses a nested representation to capture thecontext of extracted propositions.
The context couldinclude condition, attribution, belief, order, reasonand more.
Since it is not possible to generate or learnpatterns that can express these complex assertionsas a whole, NESTIE links the various propositionsfrom the previous step to generate nested proposi-tions that are complete and closer in meaning to theoriginal statement.The proposition linking module is based on theassumption that the inter-proposition relation can beinferred from the dependency parse of the sentencefrom which propositions were extracted.
Some ofthe rules employed to link the propositions are:?
The relation of proposition P1 has a relation-ship to the relation of proposition P2.59Template Seed Pattern Learned PatternPattern:Representation: T: (arg1, [rel, by], arg2)Pattern:Representation: T: (arg1, be, arg2)Pattern:Representation: T: (arg1, be, [arg2, rel2, arg3])Pattern:Representation: T1:([arg1, rel2, arg3], rel, arg2], T2: (T1, rel3, arg4)arg1 arg2 relnsubj coparg2 rel | VB* arg1nsubj dobjarg2 | NN* arg1 | NN*apposarg1 slot1 arg2nsubj dobj rel | VB*xcomparg1 rel arg2nsubjpass nmod:agentarg1 arg2 relnsubjcoparg3rel2 = nmod(?!:agent).
*arg1 rel | VB* arg2nsubjarg4rel3 = nmod(?!:agent).
*arg3rel2 = nmod.
* dobjarg1slot1 arg2 | JJccomparg3nsubj nsubjFigure 4: Syntactic Patterns learned using bootstrapping.Consider the statement, ?The accident happened af-ter the chief guest had left the event.?
and propo-sitions, P1: (accident, happen, ?)
and P2:(chief guest, had left, event).
Us-ing dependency edge, nmod:after, the linking re-turns (P1,after,P2).?
Proposition P1 is argument in proposition P2.Consider the statement, ?A senior offi-cial said the body appeared to have beenthrown from a vehicle.?
and propositions,P1: (body,appeared to have beenthrown from,vehicle) and P2: (seniorofficial,said,?).
The linking updates P2 to(senior official,said,P1).?
An inner nested proposition is replaced with amore descriptive alternative proposition.We use dependency parse patterns to link proposi-tions.
We find correspondences between: a ccompedge and a clausal complement, an advcl edge anda conditional, a nmod edge and a relation modi-fier.
For clausal complements, a null argument in thesource proposition is updated with the target propo-sition.
For conditionals and nominal modifiers, anew proposition is created with the source and targetpropositions as arguments.
The relation of the newproposition is derived from the target of the markedge from the relation head of target proposition.4.5 Comparison with OllieNESTIE uses an approach similar to OLLIE andWOE to learn dependency parse based syntactic pat-terns.
However, there are significant differences.First, OLLIE and WOE rely on extractions fromREVERB and Wikipedia info-boxes respectively forbootstrapping.
Most of these relations are binary.On the contrary, our algorithm is based on high-confidence seed templates that are more expressiveand hence learn patterns expressing different ways inwhich the proposition as a whole can be expressed.Though the arguments in OLLIE can be expanded toinclude the n-ary arguments, NESTIE encodes themin the seed templates and learns different ways ofexpressing these arguments.
Also, similar to OL-LIE, NESTIE can extract propositions that are notjust mediated by verbs.5 ExperimentsWe conducted an experimental study to compareNESTIE to other state-of-the-art extractors.
Wefound that it achieves higher informativeness andproduces more correct and minimal propositionsthan other extractors.5.1 Experimental SetupWe used two datasets released by (Del Corro andGemulla, 2013) in our experiments: 200 randomsentences from Wikipedia, and 200 random sen-tences from New York Times (NYT).
We compared60Dataset Reverb Ollie ClausIE NestIENYT datasetAvg.
Informativeness 1.437/5 2.09/5 2.32/5 2.762/5Correct 187/275 (0.680) 359/529 (0.678) 527/882 (0.597) 469/914 (0.513)Minimal (among correct) 161/187 (0.861) 238/359 (0.663) 199/527 (0.377) 355/469 (0.757)Wikipedia datasetAvg.
Informativeness 1.63/5 2.267/5 2.432/5 2.602/5Correct 194/258 (0.752) 336/582 (0.577) 453/769 (0.589) 415/827 (0.501)Minimal (among correct) 171/194 (0.881) 256/336 (0.761) 214/453 (0.472) 362/415 (0.872)Figure 5: Informativeness and number of correct and minimal extractions as fraction of total extractions.NESTIE against three OIE systems: REVERB, OL-LIE and CLAUSIE.
Since the source code for each ofthe extractors was available, we independently ranthe extractors on the two datasets.
Next, to make theextractions comparable, we configured the extrac-tors to generate triple propositions.
REVERB andCLAUSIE extractions were available as triples bydefault.
OLLIE extends its triple proposition repre-sentation.
So, we generated an additional extractionfor each of the possible extensions of a proposition.NESTIE uses a nested representation.
So, we simplyextracted the innermost proposition in a nested rep-resentation as a triple and allowed the subject andthe object in the outer proposition to contain a ref-erence to the inner triple.
By preserving referencesthe context of a proposition is retained while allow-ing for queries at various granularity levels.We manually labeled the extractions obtainedfrom all extractors to 1) maintain consistency, 2)additionally, assess if extracted triples were infor-mative and minimal.
Some extractors use heuris-tics to identify arguments and/or relation phraseboundaries, which leads to over-specific argumentsthat render the extractions unusable for other down-stream applications.
To assess the usability of ex-tractions, we evaluated them for minimality (Bastand Haussmann, 2013).
Furthermore, the goal ofour system is to extract as many propositions as pos-sible and lose as little information as possible.
Wemeasure this as informativeness of the set of the ex-tractions for a sentence.
Since computing informa-tiveness as a percentage of text contained in at leastone extraction could be biased towards long extrac-tions, we used an explicit rating scale to measureinformativeness.Two CS graduate student labeled each extractionfor correctness (0 or 1) and minimality (0 or 1).
Foreach sentence, they label the set of extractions for in-formativeness (0-5).
An extraction is marked correctif it is asserted in the text and correctly captures thecontextual information.
An extraction is consideredminimal if the arguments are not over-specified i.e.they don?t subsume another extraction or have con-junctions or are excessively long.
Lastly, they rankthe set of extractions on a scale of 0-5 (0 for bad,5 for good) based on the coverage of information inthe original sentence.
The agreement between label-ers was measured in terms of Cohens Kappa.5.2 Comparative ResultsThe results of our experimental study are summa-rized in Figure 5 which shows the number of cor-rect and minimal extractions, as well as the totalnumber of extractions for each extractor and dataset.For each dataset, we also report the macro-averageof informativeness reported by the labelers.
Wefound moderate inter-annotator agreement: 0.59 oncorrectness and 0.53 on minimality for both thedatasets.
Each extractor also includes a confidencescore for the propositions.
But since each extractorhas its unique method to find confidence, we com-pare the precision over all the extractions instead ofa subset of high-confidence extractions.NESTIE produced many more extractions, andmore informative extractions than other systems.There appears to be a trade-off between informa-tiveness and correctness (which are akin to recalland precision, respectively).
CLAUSEIE is the sys-tem with results closer to NESTIE than other sys-tems.
However, the nested representation and propo-sition linking used by NESTIE produce substantiallymore (1.7-1.8 times more) minimal extractions thanCLAUSEIE, which generates propositions from theconstituents of the clause.
Learning non-verb medi-61ated extraction patterns and proposition linking alsoincrease the syntactic scope of relation expressionsand context.
This is also reflected in the averageinformativeness score of the extractions.
NESTIEachieves 1.1-1.9 times higher informativeness scorethan the other systems.We believe that nested representation directly im-proves minimality, independent of other aspects ofextractor design.
To explore this idea, we conductedexperiments on OLLIE, which does not expand thecontext of the arguments heuristically unlike otherextractors.
Of the extractions labeled correct but notminimal by the annotators on the Wikipedia dataset,we identified extractions that satisfy one of: 1) hasan argument for which there is an equivalent extrac-tion (nested extractions), 2) shares the same subjectwith another extraction whose relation phrase con-tains the relation and object of this extraction (n-ary extractions), 3) has an object with conjunction.Any such extractions can be made minimal and in-formative with a nested representation.
73.75% ofthe non-minimal correct extractions met at least oneof these conditions, so by a post-processing step,we could raise the minimality score of OLLIE by17.65%, from 76.1% to 93.75%.5.3 Error Analysis of NestIEWe did a preliminary analysis of the errors madeby NESTIE.
We found that in most of the cases(about 33%-35%), extraction errors were due to in-correct dependency parsing.
This is not surprising asNESTIE relies heavily on the parser for learning ex-traction patterns and linking propositions.
An incor-rect parse affects NESTIE more than other systemswhich are not focused on extracting finer grained in-formation and can trade-off minimality for correct-ness.
An incorrect parse not only affects the patternmatching but also proposition linking which eitherfails to link two propositions or produces an incor-rect proposition.Example 4 Consider the statement, ?A day afterstrong winds stirred up the Hauraki Gulf and brokethe mast of Team New Zealand, a lack of windcaused Race 5 of the America?s Cup to be aban-doned today.?.
The statement entails following as-sertions:A1: ?strong winds stirred up the Hauraki Gulf?A2: ?strong winds broke the mast of Team NewZealand?A3: ?a lack of wind caused Race 5 of the America?sCup to be abandoned?A1 and A2 are parsed correctly.
A3 is parsedincorrectly with Race 5 as object of the verbcaused.
Some extractors either don?t capture A3or return an over-specified extraction, (a lack ofwind, caused, Race 5 of the America ?s Cup to beabandoned today).
Such an extraction is correct butnot minimal.To maintain minimality, NESTIE aims to extractpropositions, P1: (Race 5 of the America ?s Cup, beabandoned, ?)
and P2: (a lack of wind, caused, P1).However, it fails because of parser errors.
It extractsincorrect proposition, P3: (a lack of wind, caused,Race 5) corresponding to A3 and links it to propo-sitions for A1 and A2.
Linking an incorrect propo-sition generates more incorrect propositions whichhurt the system performance.However, we hope this problem can be allevi-ated to some extent as parsers become more robust.Another approach could be to use clause segmenta-tion to first identify clause boundaries and then useNESTIE on reduced clauses.
As the problem be-comes more severe for longer sentences, we wish toexplore clause processing for complex sentences infuture.Another source of errors was under-specifiedpropositions.
Since our nested representation al-lows null arguments for intransitive verb phrasesand for linking propositions, failure to find an ar-gument/proposition results in an under-specified ex-traction.
We found that 27% of the errors were be-cause of null arguments.
However, by ignoring ex-tractions with null arguments we found that preci-sion increases by only 4%-6% (on Wikipedia).
Thisexplains that many of the extractions with empty ar-guments were correct, and need special handling.Other sources of errors were: aggressive general-ization of an extraction pattern to unseen relations(24%), unidentified dependency types while parsinglong, complex sentences (21%), and errors in ex-panding the scope of arguments and linking extrac-tions (20%).626 Related WorkAs OIE has gained popularity to extract propositionsfrom large corpora of unstructured text, the problemof the extractions being uninformative and incom-plete has surfaced.
A recent paper (Bast and Hauss-mann, 2014) pointed out that a significant fractionof the extracted propositions is not informative.
Asimple inference algorithm was proposed that usesgeneric rules for each semantic class of predicate toderive new triples from extracted triples.
Though itimproved the informativeness of extracted triples, itdid not alleviate the problem of lost context in com-plex sentences.
We, therefore, create our own ex-tractions.Some recent works (Bast and Haussmann, 2013;Angeli et al, 2015) have tried to address the prob-lem of long and uninformative extractions in open-domain information extraction by finding short en-tailment or clusters of semantically related con-stituents from a longer utterance.
These clusters arereduced to triples using schema mapping to knownrelation types or using a set of hand-crafted rules.NESTIE shares similar objectives but uses boot-strapping to learn extraction patterns.Bootstrapping and pattern learning has a long his-tory in traditional information extraction.
Systemslike DIPRE (Brin, 1998), SNOWBALL (Agichteinand Gravano, 2000), NELL (Mitchell, 2010), andOLLIE bootstrap based on seed instances of a rela-tion and then learn patterns for extraction.
We fol-low a similar bootstrapping algorithm to learn ex-traction patterns for n-ary and nested propositions.Using a nested representation to express com-plex and n-ary assertions has been studied in closed-domain or ontology-aided information extraction.Yago (Suchanek et al, 2008) and (Nakashole andMitchell, 2015) extend binary relations to capturetemporal, geospatial and prepositional context infor-mation.
We study such a representation for open-domain information extraction.7 ConclusionsWe presented NESTIE, a novel open information ex-tractor that uses nested representation for expressingcomplex propositions and inter-propositional rela-tions.
It extends the bootstrapping techniques of pre-vious approaches to learn syntactic extraction pat-terns for the nested representation.
This allows it toobtain higher informativeness and minimality scoresfor extractions at comparable precision.
It produces1.7-1.8 times more minimal extractions and achieves1.1-1.2 times higher informativeness than CLAU-SEIE.
Thus far, we have tested our bootstrap learn-ing and proposition linking approaches only on asmall dataset.
We believe that its performance willimprove with larger datasets.
NESTIE can be seenas a step towards a system that has a greater aware-ness of the context of each extraction and providesinformative extractions to downstream applications.AcknowledgmentsThis research was supported in part by NSF grantsIIS 1250880 and IIS 1017296.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting relations from large plain-text collections.In Proceedings of the fifth ACM conference on Digitallibraries, pages 85?94.Alan Akbik and Alexander Lo?ser.
2012.
Kraken: N-aryfacts in open information extraction.
In Proceedingsof the AKBC-WEKEX, pages 52?56.Gabor Angeli, Melvin Johnson Premkumar, and Christo-pher D Manning.
2015.
Leveraging linguistic struc-ture for open domain information extraction.
In Pro-ceedings of ACL, pages 26?31.Collin F Baker, Charles J Fillmore, and John B Lowe.1998.
The berkeley framenet project.
In Proceedingsof the 17th international conference on Computationallinguistics-Volume 1, pages 86?90.Laura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider.
Abstract meaning representation (amr) 1.0specification.Michele Banko, Michael J Cafarella, Stephen Soderland,Matthew Broadhead, and Oren Etzioni.
2007.
Openinformation extraction for the web.
In IJCAI, vol-ume 7, pages 2670?2676.Hannah Bast and Elmar Haussmann.
2013.
Open infor-mation extraction via contextual sentence decomposi-tion.
In IEEE-ICSC 2013, pages 154?159.Hannah Bast and Elmar Haussmann.
2014.
More in-formative open information extraction via simple in-ference.
In Advances in information retrieval, pages585?590.
Springer.63Sergey Brin.
1998.
Extracting patterns and relationsfrom the world wide web.
In The World Wide Weband Databases, pages 172?183.
Springer.Janara Christensen, Stephen Soderland, Oren Etzioni,et al 2010.
Semantic role labeling for open infor-mation extraction.
In Proceedings of the NAACL-HLT2010, pages 52?60.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In Proceedingsof the joint SIGDAT conference on empirical methodsin natural language processing and very large cor-pora, pages 100?110.
Citeseer.Dipanjan Das, Desai Chen, Andre?
FT Martins, NathanSchneider, and Noah A Smith.
2014.
Frame-semanticparsing.
Computational linguistics, 40(1):9?56.Marie-Catherine De Marneffe, Bill MacCartney, Christo-pher D Manning, et al 2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of LREC, volume 6, pages 449?454.Luciano Del Corro and Rainer Gemulla.
2013.
Clausie:clause-based open information extraction.
In Proceed-ings of the IW3C2, pages 355?366.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of EMNLP, pages 1535?1545.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2014.
Open question answering over curated and ex-tracted knowledge bases.
In Proceedings of ACM-SIGKDD, pages 1156?1165.
ACM.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S Weld.
2011.
Knowledge-based weak supervision for information extraction ofoverlapping relations.
In Proceedings of ACL-HLT,pages 541?550.Paul Kingsbury and Martha Palmer.
2002.
From tree-bank to propbank.
In LREC.Tom Mitchell.
2010.
Never-ending learning.
Technicalreport, DTIC Document, Carnegie Mellon University.Ndapandula Nakashole and Tom M Mitchell.
2015.
Aknowledge-intensive model for prepositional phraseattachment.
In Proceedings of ACL, pages 365?375.V.
Punyakanok, D. Roth, and W. Yih.
2008.
The impor-tance of syntactic parsing and inference in semanticrole labeling.
Computational Linguistics, 34(2).Michael Schmitz, Robert Bart, Stephen Soderland, OrenEtzioni, et al 2012.
Open language learning for infor-mation extraction.
In Proceedings of EMNLP-CoNLL2012, pages 523?534.Fabian M Suchanek, Gjergji Kasneci, and GerhardWeikum.
2008.
Yago: A large ontology fromwikipedia and wordnet.
Web Semantics: Science, Ser-vices and Agents on the World Wide Web, 6(3):203?217.Kristina Toutanova, Aria Haghighi, and Christopher DManning.
2008.
A global joint model for semanticrole labeling.
Computational Linguistics, 34(2):161?191.Fei Wu and Daniel S Weld.
2010.
Open information ex-traction using wikipedia.
In Proceedings of the ACL,pages 118?127.Alexander Yates, Michael Cafarella, Michele Banko,Oren Etzioni, Matthew Broadhead, and StephenSoderland.
2007.
Textrunner: open information ex-traction on the web.
In Proceedings of NAACL-HLT:Demonstrations, pages 25?26.64
