Improving Document Clustering by Utilizing Meta-Data*Kam-Fai WongDepartment of SystemsEngineering andEngineering Management,The Chinese University ofHong Kongkfwong@se.cuhk.edu.hkNam-Kiu ChanCentre for Innovation andTechnology,The Chinese University ofHong Kongjussie@cintec.cuhk.edu.hkKam-Lai WongCentre for Innovation andTechnology,The Chinese University ofHong Kongklwong@cintec.cuhk.edu.hkAbstractIn this paper, we examine how to im-prove the precision and recall of docu-ment clustering by utilizing meta-data.We use meta-data through NewsML tagsto assist clustering and show that this ap-proach is effective through experimentson sample news data.
Experimental resultshows that clustering using NewsMLcould improve average recall and preci-sion over the same without usingNewsML by about 10%.
Our algorithmfacilitates effective e-business for thenews media and publishing industry toempower e-business.122.1IntroductionNowadays, people have great demand onknowledge and information, while informationoverload becoming one serious problem.
Newsmedia and publishing industry therefore try to suitcustomers?
need by using electronic informationmanagement system.
Document clustering algo-rithm has been introduced to group similar docu-ments together for easier searching and reading.Document clustering algorithm has beenwidely used in news media and publishing indus-try, which ensured it effectiveness over manualclustering.
With labor cost reduced and time saved,document clustering algorithm provides conven-ient clustered-news for users.To improve the accuracy of documentclustering algorithm, we suggest to provide moreflexible information for each document.
Under thehypothesis that document clustering algorithm canget better result with more information about dataused, we suggest that using additional meta-datacontained in NewsML standard could enhance theperformance of document clustering algorithms.We evaluated the effectiveness of usingmeta-data in the proposed clustering algorithm.We used Chinese electronic news sources for theevaluation.
The experiment showed that using themeta-data provided by NewsML achieved betterdocument clustering.The remaining of the paper is organized as fol-lows: In Section 2, we give an overview of currentdocument clustering approaches.
Section 3, analy-ses existing problems in document clustering andsuggests a solution using NewsML.
We show thetags that could be used in the algorithm and howthey are handled.
The performance of algorithmthrough experiments, and we will present the ex-perimental results regarding measures in precisionand recall.
In Section 5, we include a brief sum-mary and discussion on future work.Current ApproachesDifferent document clustering methods havebeen examined.
These conventional clusteringmethods mainly consist of two parts: constructionof a similarity matrix between documents andformulation of clustering algorithm to generateclusters.Similarity MatrixThe first step of conventional clusteringmethod is to construct a similarity matrix betweenthese documents so as to understand how docu-ments are similar to one another.
The constructedsimilarity matrix will later be used by the cluster-ing algorithm for generating clusters.
* Corresponding Author: Kam-Lai Wong (klwong@cintec.cuhk.edu.hk)Named entity method (Volk and Clematide,2001) is one of the widely used approaches forconstructing a similarity matrix.
Named Entitiesform the major components in a document.
Whenfundamental entities like Person, Company andGeographical names are detected, the algorithmcould understand the content of a document to acertain extent.Named entity method has also been investi-gated together with keywords to perform cluster-ing  (Lam, Meng, Wong and Yen, 2001).
Thesimilarity score is calculated based on the namedentity vectors and the keyword vector withweighting parameters to control the degree of em-phasis on the corresponding vectors.Concept terms method (Wong, Lam and Yen,1999) has been proposed in order to deal with theproblem of vocabulary switching.
The potentiallyconcept terms are basically the keywords derivedfrom a separated concept generation corpus.
Con-cept terms are selected based on the co-occurrencebetween a query and a document.However, named entities approach and con-cept terms approach contain some limitations: Theaccuracy of the clustering algorithm would be di-rectly proportional to the accuracy of algorithm.Thus, any error from identifications of named en-tities or concept terms will adversely affect thequality of the clustering algorithm as well.N-gram Algorithm (Lee, Cho and Park, 1999)has been introduced in order to avoid the afore-mentioned limitations.
An N-gram is a charactersequence of length N extracted from a document.The main idea of the N-gram approach is that thecharacter structure of a term can be used to findsemantically similar terms.
The approach assumesno prior linguistic knowledge about the text beingprocessed.
Moreover, there is no language-specific information used in the N-grams ap-proach, which qualifies this method as a language-independent approach.
By using N-grams, fre-quently appeared terms of each document can beextracted and compared to make the similaritymeasure.2.2 Clustering AlgorithmProbabilistic method is one of the com-monly used methods in document clustering.
Theaim of probabilistic method is to minimize theheterogeneity in each group with respect to thegroup representative based on statistical ap-proaches (Estivill-Castro and Yang, 2001).
Neuralnetwork is also used to perform a cyclic learningprocess for clustering (Grothkopf, Andernach andStevens-Rayburn, 1998).Hierarchical clustering methods includegroup-average clustering algorithm and single-link clustering algorithm (Johnson and Kargupta,2000; Tombros, Villa and Van Rijsbergen, 2002).Group average clustering is based on creating ahierarchical tree by initially creating a singletoncluster for each document.
The clusters aremerged to the parent node until the algorithm goalis achieved.
The algorithm merges document pairsin the resulting clusters by merging clusters in agreedy, bottom-up fashion.
A divide-and-conquerstrategy can be used to balance the cluster qualityand computational efficiency.The basic steps of group-average clusteringalgorithm are like this: On each iteration, it firstdivides the current pool of clusters into evenlysized buckets.
Group-average clustering is thenapplied to each bucket locally, merging smallerclusters into larger ones.
The time complexity forthe algorithm is O(kn), where k is bucket size andn is the number of documents.Single link clustering (Dunlop, 2000), on theother hand, is based on creating a hierarchical treeby continually inserting an additional node thatsatisfies the following criteria:- The new node is currently outside the hierar-chy- Of all similarities between nodes inside andoutside the hierarchy, the new node which hasthe strongest similarity is selected.
It is thenadded to the hierarchy at a level based on howstrong the similarity is.The approach is fairly fast and result in hier-archies where the closest nearest neighbours are atlower levels of the hierarchy.
However, it leads tonon-balanced clusters, and many node-node com-parisons can have the same strength of similaritythus many documents can be linked at the samelevel in the hierarchy.The accuracy of the above conventional clus-tering method, however, is generally low.
There-fore a new approach is proposed.3 Our Proposed MethodIn our proposal, we suggest the following ap-proaches for generating clusters.1.
Use Bi-gram to extract terms fromdocuments2.
Use <KeywordLine> Tag to look upkeyword terms for documents3.
Compare terms betweens documents toconstruct similarity matrix4.
Use <SubjectCode> Tag to group docu-ments to different subjects5.
Adjust similarity matrix by data pro-vided by step 46.
Apply group-average clustering algo-rithm to generate clustersWe have chosen to use Bi-gram algorithm toextract terms from documents.
The idea of bi-gram Algorithm is similar to N-grams?.
The rea-son of using bi-gram instead of N-gram is that ourexperiment mainly deals with Chinese (Big5)news.
Since Chinese terms are typically formedby two Chinese characters, bi-gram approach issufficient for this application.
Using bi-gram,moreover, would be a more effective approachwhen handling other two-byte code like Japaneseand Korean languages (Lee, Cho and Park, 1999).Using bi-grams instead of N-grams can re-duce system resources.
Since hundreds of docu-ments are handled each time, using N-gram wouldbe impractical.
For instance, using N-grams ap-proach for a document with M Chinese characterswould extract (M-1) + (M-2)+?+[M- (N-1)]terms.
This would require more time for compari-son than only M-1 terms for bi-grams.Figure 1: A sample document in NewsML format containing keywords and subject labelsWe suggest using NewsML(http://www.newsml.org) in our project.
NewsML,which is released by International Press and Tele-communications Council (IPTC) (www.iptc.org ),is an XML-based data format for news that is in-tended to use for the creation, transfer and deliv-ery of news.
All news is created based onNewsML data type definition (DTD) file.NewsML has been widely recognized globally.Noticeable users include Reuters, AFP, and Kyo-donews.A sample document in NewsML format isshown in Figure 1.
By using NewsML, a helpfultag called <KeywordLine> can be used in order tolook up existing keywords from a document.
Thekeywords, which could highly reflect the mainconcept of the document, would usually be givenby the author of the document and stored in the<KeywordLine> tag under NewsML.
Thus, the<KeywordLine> Tag is a useful indicator forkeyword extraction.We used NewsML tagged keywords forcomparison.
A score is computed for each key-word based on term frequency to reflect the levelof importance to the document.
A given keyword,however, may exist in more than two Chinesecharacters.
But in order to make easy and accuratecomparison, we extracted the keywords in twoChinese characters each and compare them withthe terms extracted by the bi-gram algorithm.These keywords would be more representativethan those extracted by bi-gram.
Thus, they wouldbe very useful for identifying the news content.We therefore applied higher weighting to theseterms.
In view of this, we assign the weighting ofNewsML tagged keywords ten times more thanthose extracted using bi-grams.Besides using only the keywords and termsfrom a news document, subject of the news is alsocrucial.
For example, news referring to entertain-ment should not be in the same cluster with onetalking about business news.
Understanding newssemantically to a certain extent can help improveclustering accuracy.
<SubjectCode> Tag inNewsML enabled the clustering algorithm to dis-tinguish which subject a particular news is refer-ring to.
Although the <SubjectCode> only gives arough concept of the news, it can significantlyimprove accuracy.In our experiment, we ensured that news withdifferent <SubjectCode> would not be put in thesame cluster.
The similarity measure of two piecesof news with different <SubjectCode> would,therefore, be zero.4 Experimental ResultsWe assessed the effectiveness of using meta-data for document clustering empirically.
We usedNewsML meta-data and Hong Kong Chinesenews articles for this purpose.Performance metrics are based on Recall andPrecision, which are defined as follows:In event Not in eventIn cluster A BNot in cluster C DRecall = A / (A+C) if A+C > 0Precision = A / (A+B) if A+B > 0To study the effect of NewsML, we calcu-lated the percentage of recall and precision underdifferent threshold values.
The threshold value (0-1) is a user-defined variable for clustering.
Thethreshold is the value where documents within acluster should have a similarity greater than it, andsimilarity between two documents is calculated byJaccard?s coefficient:cbaawwwwwherea is the set of terms present in both docu-mentsb is the set of terms present in document 1but absent in document 2c is the set of terms present in document 2but absent in document 1cba www ,,  are the sum of weights of therelated set.A higher threshold represent higher similarityis needed for documents to be put in the samecluster.Prior to the experiment, we matched eachpiece of news with specific Subject Codes definedby IPTC (http://www.iptc.org/site/subject-codes/index.html), as well as adding keywordswith respect to their contents.
After the prepara-tion process, we started our experiment and findout the recall and precision measures as abovestatedThe main difference between the algorithmswith and without NewsML is on the use of theKeywordLine and SubjectCode tags.
The evalua-tion steps are as follows:1.
Non-overlapping clusters are generated byour clustering system2.
Each generated cluster is matched to the mostsimilar sample event using a one-to-onematching method.3.
If the number of clusters generated is lessthan the number of events in the data set, dis-card the excessive sample events after match-ing.4.
If the number of clusters generated is greaterthan the number of events in the data set,empty events are added to the sample andthey are matched with the generated clusters.5.
Calculate recall and precision for each clusterpairs6.
Use the calculated recall and precision, toobtain the macro-average (Yang, Pierce andCarbonell, 1998).7.
Repeat the above steps on different thresholdvalues.Clustering Evaluation Result0.00%10.00%20.00%30.00%40.00%50.00%60.00%70.00%80.00%90.00%100.00%0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Recall WithNewsMLRecallWithoutNewsMLPrecisionWithNewsMLPrecisionWithoutNewsMLFigure 2: Clustering evaluation result for 158 news articles from three different publications4.1 4.2Experiment A: News Articles from Mul-tiple PublicationsWe began our evaluation with a set of datafrom three different publications.
We used 158news articles containing 31 distinct events (whichwas equivalent to 31 non-overlapping clusters) asour training set.
The events belonged to differentsector including local news, international news,sports news, entertainment news and businessnews.
Based on the IPTC Subject Reference Sys-tem (www.iptc.org), these news articles were clas-sified into 7 different subjects.The result is shown in Figure 2 where x-axisrepresents the threshold and y-axis represents thepercentage of the measure.The graph suggests that both recall and preci-sion values decrease with increasing threshold.The performance of using NewsML is better whenthe threshold is smaller.
The greatest differenceoccurs when threshold value = 0.2Moreover, we observe that the variation ofthe performance is greater when using NewsML.The precision and recall can drop about 40% bychanging the threshold.
In the contrast, withoutusing NewsML, the result is more steady.On average, recall and precision NewsML-based algorithm gives 16.15% and 12.09% respec-tively improvement of its non-NewsML counter-part.Experiment B: News Articles from theSame PublicationWe then carried out another experiment us-ing 503 news articles from one single publication.We tried to classify these news articles manuallyto events of related news, and then carried out thesame approach as the first experiment.
These 503news articles contained 126 distinct events andclassified to 12 different subjects with reference tothe IPTC Subject Reference System.We find out how the system performs whendealing with one single publication.
As we predict,news from only one single publication often usessame keywords or terms to express the same con-cept.
This simplified clustering.The result is shown in Figure 3 where x-axisrepresents the threshold and y-axis represents thepercentage of the measure.The performance of the system decreasewhen the threshold value increases, which is con-sistent with the first experiment.
From the graph,we observe that the performance is best when thethreshold is between 0.2 and 0.4, which meansthose values could best match human expectation.The graph also suggests that both recall andprecision gives a better result towards using meta-data.
On average 8.02% (recall) and 8.92% (preci-sion) improvement over the approach without us-ing NewsML meta-data are achieved.?????????????????????????????????????????????????????????????????????????????????????????????????
???
???
???
???
???
???
???
??????????????????????????????????????????????????????????????????????????????????????????????
?5 ConclusionsFrom the results of the two experiments, wehave found that using NewsML could improveboth the recall and precision of the document clus-tering algorithm by about 10%, over those withoutusing NewsML.In this paper, we demonstrate that the effec-tiveness of document clustering algorithm couldbe improved by utilizing meta-data in additionalto the original data content.
In our experiment, wechose NewsML as a representation of news con-tent with added meta-data.
We proposed to use the<KeywordLine> tags and <SubjectCode> tags inNewsML for clustering.Our proposed document clustering algorithmis a refinement of conventional group-averageclustering algorithm and bi-gram algorithm.
It hasbeen shown that NewsML could help conven-tional clustering methods improve both the recalland precision by about 10% on average.In order to demonstrate the practicality ofNewsML for e-business, we have deployedNewsML in developing an application calledNewsFocus1.
News Focus consists of a clusteringfunction.
The function is mainly for clusteringsimilar news from three different news sources inHong Kong.
News articles under NewsFocus areclustered by news events.In the future, we try to use different methodsto further improve the clustering performance.Inverted document frequency and cosine-angleformula, for example, have been widely used interms score calculation and matrix similarity cal-culation.
Top relevance terms can be used as key-words in case of any insufficiency of metadata.We will also try to use other tags in NewsML like<Headline> in supplement with <KeywordLine>and <SubjectCode> to give more metadata infor-mation for the document.
Weighting parametersmay also be applied to show the degree of empha-sis on using those metadata.Figure 3: Clustering evaluation result for 503 news articles from the same publicationReferencesM.
D. Dunlop, Development and evaluation of cluste-ing techniques for finding people, Proceedings of theThird International Conference Basel, Volume 34,2000V.
Estivill-Castro and J. Yang, Non-crisp Clustering byFast, Convergent, and Robust Algorithms, Principlesof Data Mining and Knowledge Discovery, Volume2168, 2001, pp.
103-114U.
Grothkopf, H. Andernach, S. Stevens-Rayburn, andM.
Gomez, Comparison of  Two ?Document SimilaritySearch Engines?, Library and Information Services inAstronomy III, ASP Conference Series, Volume 153,1998, pp.
85-92E.
L. Johnson and H. Kargupta, Collective, Hierarchi-cal Clustering from Distributed, Heterogeneous Data,Large-Scale Parallel Data Mining, Lecture Notes inArtificial Intelligence, Volume 1759, 2000, pp.
221-244Lam, W., Meng, H., Wong, K.L.
and Yen, J., UsingContextual Analysis for News Event Detection,International Journal of Intelligent Systems, 16(4),2001, pp.525-5461 Please visit http://www.cnewsml.org/clustering/jsp/index2.html for demonstrationJ.
H. Lee, H. Y. Cho and H. R. Park, N-gram-basedindexing for Korean text retrieval, Information Proc-essing and Management, Volume 35 Number 4, 1999,pp.
427-441A.
Tombros, R. Villa, C. J.
Van Rijsbergen, The  effec-tiveness of query-specific hierarchic clustering in in-formation retrieval, Information processing &management, Volume 38, 2002 , pp.
559-582M.
Volk and S. Clematide: Learn-Filter-Apply-Forget.Mixed Approaches to Named Entity Recognition.
Pro-ceedings of 6th International Workshop on Applica-tions of Natural Language for Information Systems.GI-Edition.
Lecture Notes in Informatics.
vol.
3.
Ma-drid: 2001.K.L.
Wong, W. Lam, J.
Yen, Interactive Chinese NewsEvent Detection and Tracking, Proceedings of TheSecond Asian Digital Library Conference, 1999,pp.30-43.Y.
Yang, T. Pierce, J. Carbonell, A Study on Retrospec-tive and On-Line Event Detection, Proceedings ofSIGIR-98, 21st ACM International Conference on Re-search and Development in Information Retrieval,1998, pp.28-36.
