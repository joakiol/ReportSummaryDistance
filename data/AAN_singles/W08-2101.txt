CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 1?8Manchester, August 2008Semantic Parsing for High-Precision Semantic Role LabellingPaola MerloLinguistics DepartmentUniversity of Geneva5 rue de Candolle1211 Gen`eve 4 Switzerlandmerlo@lettres.unige.chGabriele MusilloDepts of Linguistics and Computer ScienceUniversity of Geneva5 Rue de Candolle1211 Gen`eve 4 Switzerlandmusillo4@etu.unige.chAbstractIn this paper, we report experiments thatexplore learning of syntactic and seman-tic representations.
First, we extend astate-of-the-art statistical parser to pro-duce a richly annotated tree that identi-fies and labels nodes with semantic role la-bels as well as syntactic labels.
Secondly,we explore rule-based and learning tech-niques to extract predicate-argument struc-tures from this enriched output.
The learn-ing method is competitive with previoussingle-system proposals for semantic rolelabelling, yields the best reported preci-sion, and produces a rich output.
In com-bination with other high recall systems ityields an F-measure of 81%.1 IntroductionIn statistical natural language processing, consid-erable ingenuity and insight have been devoted todeveloping models of syntactic information, suchas statistical parsers and taggers.
Successes inthese syntactic tasks have recently paved the wayto applying novel statistical learning techniquesto levels of semantic representation, such as re-covering the logical form of a sentence for in-formation extraction and question-answering ap-plications (Miller et al, 2000; Ge and Mooney,2005; Zettlemoyer and Collins, 2007; Wong andMooney, 2007).In this paper, we also focus our interest on learn-ing semantic information.
Differently from otherwork that has focussed on logical form, however,we explore the problem of recovering the syn-tactic structure of the sentence, the propositionalc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.argument-structure of its main predicates, and thesubstantive labels assigned to the arguments in thepropositional structure, the semantic roles.
Thisrich output can be useful for information extrac-tion and question-answering, but also for anaphoraresolution and other tasks for which the structuralinformation provided by full syntactic parsing isnecessary.The task of semantic role labelling (SRL), as hasbeen defined by previous researchers (Gildea andJurafsky, 2002), requires collecting all the argu-ments that together with a verb form a predicate-argument structure.
In most previous work, thetask has been decomposed into the argument iden-tification and argument labelling subtasks: first thearguments of each specific verb in the sentence areidentified by classifying constituents in the sen-tence as arguments or not arguments.
The argu-ments are then labelled in a second step.We propose to produce the rich syntactic-semantic output in two steps, which are differentfrom the argument identification and argument la-belling subtasks.
First, we generate trees that bearboth syntactic and semantic annotation, such asthose in Figure 1.
The parse tree, however, doesnot explicitly encode information about predicate-argument structure, because it does not explicitlyassociate each semantic role to the verb that gov-erns it.
So, our second step consists in recoveringthe predicate-argument structure of each verb bygleaning this information in an already richly dec-orated tree.There are linguistic and computational reasonsto think that we can solve the joint problem ofrecovering the constituent structure of a sentenceand its lexical semantics.
From a linguistic pointof view, the assumption that syntactic distributionswill be predictive of semantic role assignments isbased on linking theory (Levin, 1986).
Linkingtheory assumes the existence of a hierarchy of se-1mantic roles which are mapped by default on ahierarchy of grammatical functions and syntacticpositions, and it attempts to predict the mappingof the underlying semantic component of a predi-cate?s meaning onto the syntactic structure.
For ex-ample, Agents are always mapped in syntacticallyhigher positions than Themes.
From a computa-tional point of view, if the internal semantics of apredicate determines the syntactic expressions ofconstituents bearing a semantic role, it is then rea-sonable to expect that knowledge about semanticroles in a sentence will be informative of its syn-tactic structure.
It follows rather naturally that se-mantic and syntactic parsing can be integrated intoa single complex task.Our proposal also addresses the problem of se-mantic role labelling from a slightly different per-spective.
We identify and label argument nodesfirst, while parsing, and we group them in apredicate-argument structure in a second step.
Ourexperiments investigate some of the effects that re-sult from organising the task of semantic role la-belling in this way, and the usefulness of somenovel features defined on syntactic trees.In the remainder of the paper, we first illustratethe data and the graphical model that formalise thearchitecture used and its extension for semanticparsing.
We then report on two kinds of exper-iments: we first evaluate the architecture on thejoint task of syntactic and semantic parsing andthen evaluate the joint approach on the task of se-mantic role labelling.
We conclude with a discus-sion which highlights the practical and theoreticalcontribution of this work.2 The DataOur experiments on joint syntactic and semanticparsing use data that is produced automatically bymerging the Penn Treebank (PTB) with PropBank(PRBK) (Marcus et al, 1993; Palmer et al, 2005),as shown in Figure 1.
PropBank encodes proposi-tional information by adding a layer of argumentstructure annotation to the syntactic structures ofthe Penn Treebank.1Verbal predicates in the PennTreebank (PTB) receive a label REL and their ar-guments are annotated with abstract semantic rolelabels, such as A0, A1, or AA for those comple-ments of the predicative verb that are consideredarguments.
Those complements of the verb la-1We use PRBK data as they appear in the CONLL 2005shared task.SNP-A0the authorityVPVBD-RELdroppedPP-TMPIN(TMP)atNPNNmidnightPP-DIRTO(DIR)toNPQP$ 2.80 trillionFigure 1: A sample syntactic structure with seman-tic role labels.belled with a semantic functional label in the orig-inal PTB receive the composite semantic role labelAM-X , where X stands for labels such as LOC,TMP or ADV, for locative, temporal and adverbialmodifiers respectively.
A tree structure with Prop-Bank labels is shown in Figure 1.
(The bold labelsare not relevant for the moment and they will beexplained later.
)3 The Syntactic and Semantic ParserArchitectureTo achieve the complex task of joint syntactic andsemantic parsing, we extend a current state-of-the-art statistical parser (Titov and Henderson, 2007)to learn semantic role annotation as well as syntac-tic structure.
The parser uses a form of left-cornerparsing strategy to map parse trees to sequences ofderivation steps.We choose this parser because it exhibits thebest performance for a single generative parser,and does not impose hard independence assump-tions.
It is therefore promising for extensionsto new tasks.
Following (Titov and Henderson,2007), we describe the original parsing architec-ture and our modifications to it as a DynamicBayesian network.
Our description is brief andlimited to the few aspects of interest here.
Formore detail, explanations and experiments see(Titov and Henderson, 2007).
A Bayesian networkis a directed acyclic graph that illustrates the statis-tical dependencies between the random variablesdescribing a set of events (Jensen, 2001).
Dy-namic networks are Bayesian networks applied tounboundedly long sequences.
They are an appro-priate model for sequences of derivation steps in2dtkSt?1 tsitt?cD Dt?1t?cS SDtFigure 2: The pattern on connectivity and the latentvectors of variables in an Incremental BayesianNetwork.parsing (Titov and Henderson, 2007).Figure 2 illustrates visually the main propertiesthat are of relevance for us in this parsing architec-ture.
Let T be a parse tree and D1, .
.
.
, Dmbe thesequence of parsing decisions that has led to thebuilding of this parse tree.
Let alo each parsingdecision be composed of smaller parsing decisionsd11, .
.
.
, d1k, and let al these decisions be indepen-dent.
Then,P (T ) = P (D1, .
.
.
, Dm)=?tP (Dt|D1, .
.
.
, Dt?1)=?t?kP (dtk|h(t, k))(1)where h(t, k) denotes the parse history for sub-decision dtk.The figure represents a small portion of the ob-served sequence of decisions that constitute the re-covery of a parse tree, indicated by the observedstates Di.
Specifically, it illustrates the pattern ofconnectivity for decision dtk.
As can be seen the re-lationship between different probabilistic parsingdecisions are not Markovian, nor do the decisionsinfluence each other directly.
Past decisions can in-fluence the current decision through state vectorsof independent latent variables, referred to as Si.These state vectors encode the probability distri-butions of features of the history of parsing steps(the features are indicated by stiin Figure 2).As can be seen from the picture, the patternof inter-connectivity allows previous non-adjacentstates to influence future states.
Not all statesin the history are relevant, however.
The inter-connectivity is defined dynamically based on thetopological structure and the labels of the tree thatis being developed.
This inter-connectivity de-pends on a notion of structural locality (Hender-son, 2003; Musillo and Merlo, 2006).22Specifically, the conditioning states are based on theIn order to extend this model to learn decisionsconcerning a joint syntactic-semantic representa-tion, the semantic information needs to be high-lighted in the model in several ways.
We modifythe network connectivity, and bias the learner.First, we take advantage of the network?s dy-namic connectivity to highlight the portion of thetree that bears semantic information.
We augmentthe nodes that can influence parsing decisions atthe current state by explicitly adding the vectorsof latent variables related to the most recent childbearing a semantic role label of either type (REL,A0 to A5 or AM-X) to the connectivity of thecurrent decision.
These additions yield a modelthat is sensitive to regularities in structurally de-fined sequences of nodes bearing semantic role la-bels, within and across constituents.
These exten-sions enlarge the locality domain over which de-pendencies between predicates bearing the RELlabel, arguments bearing an A0-A5 label, and ad-juncts bearing an AM-X role can be specified, andcapture both linear and hierarchical constraints be-tween predicates, arguments and adjuncts.
Enlarg-ing the locality domain this way ensures for in-stance that the derivation of the role DIR in Figure1 is not independent of the derivations of the rolesTMP, REL (the predicate) and A0.Second, this version of the Bayesian networktags its sentences internally.
Following (Musilloand Merlo, 2005), we split some part-of-speechtags into tags marked with semantic role labels.The semantic role labels attached to a non-terminaldirectly projected by a preterminal and belongingto a few selected categories (DIR, EXT, LOC, MNR,PRP, CAUS or TMP) are propagated down to thepre-terminal part-of-speech tag of its head.3Thisthird extension biases the parser to learn the rela-tionship between lexical items, semantic roles andthe constituents in which they occur.
This tech-nique is illustrated by the bold labels in Figure 1.We compare this augmented model to a sim-ple baseline parser, that does not present any ofthe task-specific enhancements described above,stack configuration of the left-corner parser and the derivationtree built so far.
The nodes in the partially built tree and stackconfiguration that are selected to determine the relevant statesare the following: top, the node on top of the pushdown stackbefore the current derivation move; the left-corner ancestor oftop (that is, the second top-most node on the parser stack);the leftmost child of top; and the most recent child of top, ifany.3Exploratory data analysis indicates that these tags are themost useful to disambiguate parsing decisions.3PTB/PRBK 24P R FBaseline 79.6 78.6 79.1ST 80.5 79.4 79.9ST+ EC 81.6 80.3 81.0Table 1: Percentage F-measure (F), recall (R), andprecision (P) of our joint syntactic and semanticparser on merged development PTB/PRBK data(section 24).
Legend of models: ST=Split Tags;EC=enhanced connectivity.other than being able to use the complex syntactic-semantic labels.
Our augmented model has a to-tal of 613 non-terminals to represent both the PTBand PropBank labels of constituents, instead of the33 of the original syntactic parser.
The 580 newlyintroduced labels consist of a standard PTB labelfollowed by a set of one or more PropBank seman-tic role such as PP-AM-TMP or NP-A0-A1.
As aresult of lowering the six AM-X semantic role la-bels, 240 new part-of-speech tags were introducedto partition the original tag set which consistedof 45 tags.
As already mentioned, argumental la-bels A0-A5 are specific to a given verb or a givenverb sense, thus their distribution is highly vari-able.
To reduce variability, we add the tag-verbpairs licensing these argumental labels to the vo-cabulary of our model.
We reach a total of 4970tag-word pairs.
These pairs include, among oth-ers, all the tag-verb pairs occuring at least 10 timesin the training data.
In this very limited form oflexicalisation, all other words are considered un-known.4 Parsing ExperimentsOur extended joint syntactic and semantic parserwas trained on sections 2-21 and validated on sec-tion 24 from the merged PTB/PropBank.
To eval-uate the joint syntactic and semantic parsing task,we compute the standard Parseval measures of la-belled recall and precision of constituents, takinginto account not only the original PTB labels, butalso the newly introduced PropBank labels.
Thisevaluation gives us an indication of how accuratelyand exhaustively we can recover this richer set ofsyntactic and semantic labels.
The results, com-puted on the development data set from section 24of the PTB with added PropBank annotation, areshown in Table 1.
As the table indicates, both theenhancements based on semantic roles yield an im-provement on the baseline.This task enables us to compare, albeit indi-rectly, our integrated method to other methodswhere semantic role labels are learnt separatelyfrom syntactic structure.
(Musillo and Merlo,2006) report results of a merging technique wherethe output of the semantic role annotation pro-duced by the best semantic role labellers in the2005 CONLL shared task is merged with the out-put of Charniak?s parser.
Results range betweenbetween 82.7% and 83.4% F-measure.
Our inte-grated method almost reaches this level of perfor-mance.The performance of the parser on the syntacticlabels only (note reported in Table 1) is slightly de-graded in comparison to the original syntax-onlyarchitecture (Henderson, 2003), which reportedan F-measure of 89.1% since we reach 88.4% F-measure for the best syntactic-semantic model (lastline of Table 1).
This level of performance is stillcomparable to other syntactic parsers often usedfor extraction of semantic role features (88.2% F-measure) (Collins, 1999).These results indicate that the extended parser isable to recover both syntactic and semantic labelsin a fully connected parse tree.
While it is true thatthe full fine-grained interpretation of the semanticlabel is verb-specific, the PropBank labels (A0,A1,etc) do respect some general trends.
A0 labels areassigned to the most agentive of the arguments,while A1 labels tend to be assigned to argumentsbearing a Theme role, and A2, A3, A4 and A5 la-bels are assigned to indirect object roles, while allthe AM-X labels tend to be assigned to adjuncts.The fact that the parser learns these labels with-out explicit annotation of the link between the ar-guments and the predicate to which they are as-signed, but based on the smoothed representationof the derivation of the parse tree and only verylimited lexicalisation, appears to confirm linkingtheory, which assumes a correlation between thesyntactic configuration of a sentence and the lexi-cal semantic labels.We need to show now that the quality of theoutput produced by the joint syntactic and seman-tic parsing is such that it can be used to performother tasks where semantic role information is cru-cial.
The most directly related task is semantic rolelabelling (SRL) as defined in the shared task ofCoNLL 2005.45 Extraction of Predicate-ArgumentStructuresAlthough there is reason to think that the goodperformance reported in the previous section isdue to implicit learning of the relationship of thesyntactic representation and the semantic role as-signments, the output produced by the parser doesnot explicitly encode the predicate-argument struc-tures.
Collecting these associations is required tosolve the semantic role labelling task as usually de-fined.
We experimented with two methods: a sim-ple rule-based method and a more complex learn-ing method.5.1 The rule-based methodThe rule-based extraction method is the naturalsecond step to solve the complete semantic rolelabelling task, after we identify and label seman-tic roles while parsing.
Since in our proposal, wesolve most of the problem in the first step, then weshould be able to collect the predicate-argumentpairs by simple, deterministic rules.
The simplic-ity of the method also provides a useful compari-son for more complex learning methods, which canbe justified only if they perform better than simplerule-based predicate-argument extraction.Our rule-based method automatically compilesfinite-state automatata defining the paths that con-nect the first node dominating a predicate to itssemantic roles from parse trees enriched with se-mantic role labels.4Such paths can then be used totraverse parse trees returned by the parsing modeland collect argument structures.
More specifically,a sample of sentences are randomly selected fromthe training section of the PTB/PRBK.
For eachpredicate, then, all the arguments left and right ofthe predicate and all the adjuncts left and rightrespectively are collected and filtered by simpleglobal constraints, thereby guaranteeing that onlyone type of obligatory argument label (A0 to A5)is assigned in each proposition.When evaluated on gold data, this rule-based ex-traction method reaches 94.9% precision, 96.9%recall, for an F-measure of 95.9%.
These resultsprovide an upper bound as well as indicating that,while not perfect, the simple extraction rules reacha very good level of correctness if the input fromthe first step, syntactic and semantic parsing, iscorrect.
The performance is much lower when4It uses VanNoord?s finite-state-toolkithttp://www.let.rug.nl/ vannoord/Fsa/.parses are not entirely correct, and semantic rolelabels are missing, as indicated by the results of72.9% precision, 66.7% (F-measure 69.7%), ob-tained when using the best automatic parse tree.The fact that performance depends on the qual-ity of the output of the first step, indicates thatthe extraction rules are sensitive to errors in theparse trees, as well as errors in the labelling.
Thisindicates that a learning method might be moreadapted to recover from these mistakes.5.2 The SVM learning methodIn a different approach to extract predicate argu-ment structures from the parsing output, the sec-ond step learns to associate the right verb to eachsemantically annotated node (srn) in the tree pro-duced in the first step.
Each individual (verb, srn)pair in the tree is either a positive example (the srnis a member of the verb?s argument structure) or anegative example (the argument either should nothave been labelled as an argument or it is not as-sociated to the verb).
The training examples areproduced by parsing section 2-21 of the mergedPTB/PRBK data with the joint syntactic-semanticparser and producing the training examples bycomparison with the CONLL 2005 gold proposi-tions.
There are approximately 800?000 trainingexamples in total.
These examples are used byan SVM classifier (Joachims, 1999).5.
Once thepredicate-argument structures are built, they areevaluated with the CONLL 2005 shared task cri-teria.5.3 The learning featuresThe features used for the extraction of thepredicate-argument structure reflect the syntacticproperties that are useful to identify the argumentsof a given verb.
We use syntactic and semanticnode label, the path between the verb and the argu-ment, and the part-of-speech tag of the verb, whichprovides useful information about the tense of theverb.
We also use novel features that encode min-imality conditions and locality constraints (Rizzi,1990).
Minimality is a typical property of natu-ral languages that is attested in several domains.In recovering predicate-argument structures, mini-mality guarantees that the arguments are related tothe closest verb in a predicate domain, which is notalways the verb to which they are connected by the5We use a radial basis function kernel, where parametersc and ?
were determined by a grid search on a small subset of2000 training examples.
They are set at c=8 and ?
= 0.03125.5shortest path.
For example, the subject of an em-bedded clause can be closer to the verb of the mainclause than to the predicate to which it should beattached.
Minimality is encoded as a binary featurethat indicates whether a verbw intervenes betweenthe verb v and the candidate argument srn.
Mini-mality is defined both in terms of linear precedence(indicated below as ?)
and of dominance withinthe same VP group.
A VP group is a stack of VPscovering the same compound verb group, such as[V Pshould [V Phave [V P[Vcome ]]]].
Formaldefinitions are given below:minimal(v, srn, w) =df8<:false if (v ?
w ?
srn or srn ?
w ?
v) andVPG-dominates(v, srn, w)true otherwiseVPG-dominates(v, srn, w) =df8<:true if VP ?
path(v, srn) andVP ?
VP-group directly dominating wfalse otherwiseIn addition to the minimality conditions, whichresolve ambiguity when two predicates compete togovern an argument, we use locality constraints tocapture distinct local relationships between a verband the syntactic position occupied by a candidateargument.
In particular, we distinguish between in-ternal arguments occupying a position dominatedby a VP node, external arguments occupying aposition dominated by an S node, and extractedarguments occupying a position dominated by anSBAR node.
To approximate such structural dis-tinctions, we introduce two binary features indicat-ing, respectively, whether there is a a node labelledS or SBAR on the path connecting the verb and thecandidate argument.6 Results and DiscussionTable 2 illustrates our results on semantic role la-belling.
Notice how much more precise the learn-ing method is than the rule-based method, whenthe minimality constraint is added.
The second andthird line indicate that this contribution is mostlydue to the minimality feature.
The fifth and sixthline however illustrate that these features togetherare more useful than the widely used feature path.Recall however, suffers in the learnt method.
Over-all, the learnt method is better than a rule-basedmethod only if path and either minimality or lo-cality constraints are added, thus suggesting thatPrec Rec FLearning all features 87.4 63.6 73.7Learning all ?min 75.4 66.2 70.5Learning all ?loc 87.4 63.6 73.6Rule-based 72.9 66.7 69.7Learning all ?path 80.6 60.9 69.4Learning all ?min ?loc 74.3 63.8 68.6Baseline 57.4 53.9 55.6Table 2: Results on the development section (24),rule-based, and learning, (with all features, andwithout path, minimality and locality constraints)compared to a closest verb baseline.the choice of features is crucial to reach a levelof performance that justifies the added complex-ity of a learning method.
Both methods are muchbetter than a baseline that attaches each role toa verb by the shortest path.6Notice that boththese approaches are not lexicalised, they apply toall verbs.
Learning experiments where the actualverbs were used showed a little degradation as wellas a very considerable increase in training times(precision: 87.0%; recall: 61.0%; F: 71.7%).7Some comments are in order to compare prop-erly our best results ?
the learning method withall features ?
to other methods.
Most of the bestperforming SRL systems are ensemble learners orrerankers, or they use external sources of infor-mation such as the PropBank frames files.
Whilethese techniques are effective to improve classifi-cation accuracy, we might want to compare the sin-gle systems, thus teasing apart the contribution ofthe features and the model from the contributionof the ensemble technique.
Table 3 reports the sin-gle systems?
performance on the test set.
These re-sults seem to indicate that methods like ours, basedon a first step of PropBank parsing, are compara-ble to other methods when learning regimes arefactored out, contrary to pessimistic conclusionsin previous work (Yi and Palmer, 2005).
(Yi andPalmer, 2005) share the motivation of our work.They observe that the distributions of semantic la-6In case of tie, the following verb is chosen for an A0 labeland the preceding verb is chosen for all the other labels.7We should notice that all these models encode the featurepath as syntactic path, because in exploratory data analysis wefound that this feature performed quite a bit better than pathencoded taking into account the semantic roles assigned to thenodes on the path.
Concerning the learning model, we noticethat a simpler, and much faster to train, linear SVM classifierperforms almost as well as the more complex RBF classifier.It is therefore preferable if speed is important.6Model CONLL 23 CommentsP R F(Surdeanu and Turmo, 2005) 80.3 73.0 76.5 Propbank frames to filter output, boosting(Liu et al, 2005) 80.5 72.8 76.4 Single system + simple post-processing(Moschitti et al, 2005) 76.6 75.2 75.9 Specialised kernels for each kind of roleThis paper 87.6 65.8 75.1 Single system and model, locality features(Ozgencil and McCracken, 2005) 74.7 74.2 74.4 Simple system, no external knowledge(Johansson and Nugues, 2005) 75.5 73.2 74.3 Uses only 3 sections for trainingTable 3: Final Semantic Role Labelling results on test section 23 of Propbank as encoded in the CONLLshared task for those CONLL 2005 participants not using ensemble learning or external resources.bels could potentially interact with the distribu-tions of syntactic labels and redefine the bound-aries of constituents, thus yielding trees that reflectgeneralisations over both these sources of infor-mation.
They also attempt to assign SRL whileparsing, by merging only the first two steps ofthe standard pipeline architecture, pruning and ar-gument identification.
Their parser outputs a bi-nary argument-nonargument distinction.
The ac-tual fine-grained labelling is performed, as in othermethods, by an ensemble classifier.
Results arenot among the best and Yi and Palmer concludethat PropBank parsing is too difficult and suffersfrom differences between chunk annotation andtree structure.
We think instead that the method ispromising, as shown by the results reported here,once the different factors that affect performanceare teased apart.Some qualitative observations on the errors areuseful.
On the one hand, as can be noticed in Table3, our learning method yields the best precision,but often the worse recall and it has the most ex-treme difference between these two scores.8Thisis very likely to be a consequence of the method.Since the assignment of the semantic role labelsproper is performed during parsing, the numberof nodes that require a semantic role is only 20%of the total.
Therefore the parser develops a biasagainst assigning these roles in general, and recallsuffers.9On the other hand, precision is very good,thanks to the rich context in which the roles are as-signed.This property of our method suggests that com-bining our results with those of other existing se-8This observation applies also in a comparison to the othersystems that participated in the CONLL shared task.9The SVM classifier, on the other hand, exceeds 94% inaccuracy and its F measures are situated around 87?88% de-pending on the feature sets.mantic role labellers might be beneficial, since theerrors it performs are quite different.
We testedthis hypothesis by combining our outputs, whichare the most precise, with the outputs of the sys-tem that reported the best recall (Haghighi et al,2005).
The combination, performed on sections24 and 23, gives priority to our system when itoutputs a non-null label (because of its high pre-cision) and uses the other system?s label when oursystem outputs a null label.
This combination pro-duces a result of 79.0% precision, 80.4% recall,and 79.7% F-measure for section 24, and 80.5%precision, 81.4% recall, and 81.0% F-measure forsection 23.
We conclude that the combination is in-deed able to exploit the positive aspects of both ap-proaches, as the F-measure of the combined resultis better than each individual result.
It is also thebest compared to the other systems of the CoNLLshared task.
Comparatively, we find that applyingthe same combination technique to the output ofthe system by (Haghighi et al, 2005) with the out-put of the best system in the CoNLL 2005 sharedtask (Punyakanok et al, 2005) yields combinedoutputs that are not as good as the better of thetwo systems (P:76.3%; R:78.6%; F:77.4% for sec-tion 24; P:78.5%; R:80.0%; F:79.3% for section23).
This result confirms our initial hypothesis,that combination of systems with different perfor-mance characteristics yields greater improvement.Another direct consequence of assigning rolesin a rich context is that in collecting arguments fora given verb we hardly need to verify global con-straints.
Differently from previous work that hadfound that global coherence constraints consider-ably improved performance (Punyakanok et al,2005), using global filtering contraints showed noimprovement in our learning model.
Thus, theseresults confirm the observations that a verb does7not assign its semantic roles independently of eachother (Haghighi et al, 2005).
Our method too canbe seen as a way of formulating the SRL problemin a way that is not simply classification of each in-stance independently.
Because identification of ar-guments and their labelling is done while parsing,the parsing history, both syntactic and semantic,is taken into account in identifying and labellingan argument.
Semantic role labelling is integratedin structured sequence prediction.
Further integra-tion of semantic role labelling in structured prob-abilistic models related to the one described herehas recently been shown to result in accurate syn-chronous parsers that derive both syntactic and se-mantic dependency representations (Henderson etal., 2008).7 ConclusionOverall our experiments indicate that an inte-grated approach to identification and labelling fol-lowed by predicate-argument recovery can solvethe problem of semantic role labelling at a levelof performance comparable to other approaches,as well as yielding a richly decorated syntactic-semantic parse tree.
The high precision of ourmethod yields very good results in combinationwith other high-recall systems.
Its shortcomingsindicates that future work lies in improving recall.AcknowledgmentsWe thank the Swiss NSF for supporting this research undergrant number 101411-105286/1, James Henderson for shar-ing the SSN software, and Xavier Carreras for providing theCoNLL-2005 data.
Part of this work was completed whilethe second author was visiting MIT/CSAIL, hosted by Prof.Michael Collins.ReferencesCollins, Michael John.
1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, University ofPennsylvania.Ge, Ruifang and Raymond J. Mooney.
2005.
A statisticalsemantic parser that integrates syntax and semantics.
InProcs of CONLL-05, Ann Arbor, Michigan.Gildea, Daniel and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguistics,28(3):245?288.Haghighi, Aria, Kristina Toutanova, and Christopher Man-ning.
2005.
A joint model for semantic role labeling.
InProcs of CoNLL-2005, pages 173?176, Ann Arbor, Michi-gan.Henderson, Jamie.
2003.
Inducing history representationsfor broad-coverage statistical parsing.
In Procs of NAACL-HLT?03, pages 103?110, Edmonton, Canada.Henderson, James, Paola Merlo, Gabriele Musillo and IvanTitov.
2008.
A latent variable model of synchronous pars-ing for syntactic and semantic dependencies.
In Procs ofCoNLL?08 Shared Task, Manchester, UK.Jensen, Finn V. 2001.
Bayesian networks and decisiongraphs.
Springer Verlag.Joachims, Thorsten.
1999.
Making large-scale svm learningpractical.
In Schlkopf, B., C. Burges, and A. Smola, edi-tors, Advances in Kernel Methods - Support Vector Learn-ing.
MIT Press.Johansson, Richard and Pierre Nugues.
2005.
Sparsebayesian classification of predicate arguments.
In Procsof CoNLL-2005, pages 177?180, Ann Arbor, Michigan.Levin, Lori.
1986.
Operations on lexical form: unaccusativerules in Germanic languages.
Ph.D. thesis, Massachus-setts Institute of Technology.Liu, Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and HuaijunLiu.
2005.
Semantic role labeling system using maximumentropy classifier.
In Procs of CoNLL-2005, pages 189?192, Ann Arbor, Michigan.Marcus, Mitch, Beatrice Santorini, and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of English: thePenn Treebank.
Computational Linguistics, 19:313?330.Miller, S., H. Fox, L. Ramshaw, and R. Weischedel.
2000.
Anovel use of statistical parsing to extract information fromtext.
In Procs of NAACL 2000.Moschitti, Alessandro, Ana-Maria Giuglea, BonaventuraCoppola, and Roberto Basili.
2005.
Hierarchical semanticrole labeling.
In Procs of CoNLL-2005, pages 201?204,Ann Arbor, Michigan.Musillo, Gabriele and Paola Merlo.
2005.
Lexical and struc-tural biases for function parsing.
In Procs of IWPT?05,pages 83?92, Vancouver, British Columbia, October.Musillo, Gabriele and Paola Merlo.
2006.
Accurate semanticparsing of the proposition bank.
In Procs of NAACL?06,New York, NY.Ozgencil, Necati Ercan and Nancy McCracken.
2005.
Se-mantic role labeling using libSVM.
In Procs of CoNLL-2005, pages 205?208, Ann Arbor, Michigan, June.Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005.The Proposition Bank: An annotated corpus of semanticroles.
Computational Linguistics, 31:71?105.Punyakanok, Vasin, Peter Koomen, Dan Roth, and Wen tauYih.
2005.
Generalized inference with multiple seman-tic role labeling systems.
In Procs of CoNLL-2005, AnnArbor, MI USA.Rizzi, Luigi.
1990.
Relativized minimality.
MIT Press, Cam-bridge, MA.Surdeanu, Mihai and Jordi Turmo.
2005.
Semantic rolelabeling using complete syntactic analysis.
In Procs ofCoNLL?05, Ann Arbor, Michigan.Titov, Ivan and James Henderson.
2007.
Constituent parsingwith Incremental Sigmoid Belief Networks.
In Procs ofACL?07, pages 632?639, Prague, Czech Republic.Wong, Yuk Wah and Raymond Mooney.
2007.
Learning syn-chronous grammars for semantic parsing with lambda cal-culus.
In Procs of ACL?07, pages 960?967, Prague, CzechRepublic.Yi, Szu-ting and Martha Palmer.
2005.
The integration ofsemantic parsing and semantic role labelling.
In Procs ofCoNLL?05, Ann Arbor, Michigan.Zettlemoyer, Luke and Michael Collins.
2007.
Online learn-ing of relaxed CCG grammars for parsing to logical form.In Procs of EMNLP-CoNLL?07, pages 678?687.8
