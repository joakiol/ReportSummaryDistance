Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 743?749,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsAutomatic Identification of Rhetorical QuestionsShohini BhattasaliDept.
of LinguisticsCornell UniversityIthaca, NY, USAJeremy CytrynDept.
of Computer ScienceCornell UniversityIthaca, NY, USA{sb2295, jmc677, eaf82}@cornell.eduElana FeldmanDept.
of LinguisticsCornell UniversityIthaca, NY, USAJoonsuk ParkDept.
of Computer ScienceCornell UniversityIthaca, NY, USAjpark@cs.cornell.eduAbstractA question may be asked not only to elicitinformation, but also to make a state-ment.
Questions serving the latter pur-pose, called rhetorical questions, are oftenlexically and syntactically indistinguish-able from other types of questions.
Still,it is desirable to be able to identify rhetor-ical questions, as it is relevant for manyNLP tasks, including information extrac-tion and text summarization.
In this paper,we explore the largely understudied prob-lem of rhetorical question identification.Specifically, we present a simple n-grambased language model to classify rhetori-cal questions in the Switchboard DialogueAct Corpus.
We find that a special treat-ment of rhetorical questions which incor-porates contextual information achievesthe highest performance.1 IntroductionRhetorical questions frequently appear in every-day conversations.
A rhetorical question is func-tionally different from other types of questions inthat it is expressing a statement, rather than seek-ing information.
Thus, rhetorical questions mustbe identified to fully capture the meaning of anutterance.
This is not an easy task; despite theirdrastic functional differences, rhetorical questionsare formulated like regular questions.Bhatt (1998) states that in principle, a givenquestion can be interpreted as either an informa-tion seeking question or as a rhetorical questionand that intonation can be used to identify the in-terpretation intended by the speaker.
For instance,consider the following example:(1) Did I tell you that writing a dissertationwas easy?Just from reading the text, it is difficult to tellwhether the speaker is asking an informationalquestion or whether they are implying that theydid not say that writing a dissertation was easy.However, according to our observation, whichforms the basis of this work, there are two cases inwhich rhetorical questions can be identified solelybased on the text.
Firstly, certain linguistic cuesmake a question obviously rhetorical, which canbe seen in examples (2) and (3)1.
Secondly, thecontext, or neighboring utterances, often revealthe rhetorical nature of the question, as we can seein example (4).
(2) Who ever lifted a finger to help George?
(3) After all, who has any time during theexam period?
(4) Who likes winter?
It is always cold andwindy and gray and everyone feels miser-able all the time.There has been substantial work in the areaof classifying dialog acts, within which rhetor-ical questions fall.
To our knowledge, priorwork on dialog act tagging has largely ignoredrhetorical questions, and there has not been anyprevious work specifically addressing rhetoricalquestion identification.
Nevertheless, classifica-tion of rhetorical questions is crucial and has nu-merous potential applications, including question-answering, document summarization, author iden-tification, and opinion extraction.We provide an overview of related work in Sec-tion 2, discuss linguistic characteristics of rhetor-ical questions in Section 3, describe the experi-mental setup in Section 4, and present and analyzethe experiment results in Section 5.
We find that,while the majority of the classification relies onfeatures extracted from the question itself, adding1See Section 3 for more details.743in n-gram features from the context improves theperformance.
An F1-score of 53.71% is achievedby adding features extracted from the precedingand subsequent utterances, which is about a 10%improvement from a baseline classifier using onlythe features from the question itself.2 Related workJurafsky et al.
(1997a) and Reithinger and Kle-sen (1997) used n-gram language modeling on theSwitchboard and Verbmobil corpora respectivelyto classify dialog acts.
Grau et al.
(2004) usesa Bayesian approach with n-grams to categorizedialog acts.
We also employ a similar languagemodel to achieve our results.Samuel et al.
(1999) used transformation-basedlearning on the Verbmobil corpus over a num-ber of utterance features such as utterance length,speaker turn, and the dialog act tags of adja-cent utterances.
Stolcke et al.
(2000) utilizedHidden Markov Models on the Switchboard cor-pus and used word order within utterances andthe order of dialog acts over utterances.
Zech-ner (2002) worked on automatic summarizationof open-domain spoken dialogues i.e., importantpieces of information are found in the back andforth of a dialogue that is absent in a written piece.Webb et al.
(2005) used intra-utterance featuresin the Switchboard corpus and calculated n-gramsfor each utterance of all dialogue acts.
For each n-gram, they computed the maximal predictivity i.e.,its highest predictivity value within any dialogueact category.
We utilized a similar metric for n-gram selection.Verbree et al.
(2006) constructed their baselinefor three different corpora using the performanceof the LIT set, as proposed by Samuel (2000).In this approach, they also chose to use a com-pressed feature set for n-grams and POS n-grams.We chose similar feature sets to classify rhetoricalquestions.Our work extends these approaches to dialogact classification by exploring additional featureswhich are specific to rhetorical question identifi-cation, such as context n-grams.3 Features for Identifying RhetoricalQuestionsIn order to correctly classify rhetorical ques-tions, we theorize that the choice of words in thequestion itself may be an important indicator ofspeaker intent.
To capture intent in the wordsthemselves, it makes sense to consider a commonunigram, while a bigram model will likely captureshort phrasal cues.
For instance, we might expectthe existence of n-grams such as well or you knowto be highly predictive features of the rhetoricalnature of the question.Additionally, some linguistic cues are helpfulin identifying rhetorical questions.
Strong nega-tive polarity items (NPIs), also referred to as em-phatic or even-NPIs in the literature, are consid-ered definitive markers.
Some examples are budgean inch, in years, give a damn, bat an eye, andlift a finger (Giannakidou 1999, van Rooy 2003).Gresillon (1980) notes that a question containing amodal auxiliary, such as could or would, togetherwith negation tends to be rhetorical.
Certain ex-pressions such as yet and after all can only ap-pear in rhetorical questions (Sadock 1971, Sadock1974).
Again, using common n-grams as featuresshould partially capture the above cues because n-gram segments of strong NPIs should occur morefrequently.We also wanted to incorporate common gram-matical sequences found in rhetorical questions.To that end, we can consider part of speech (POS)n-grams to capture common grammatical relationswhich are predictive.Similarly, for rhetorical questions, we expectcontext to be highly predictive for correct classi-fication.
For instance, the existence of a questionmark in the subsequent utterance when spoken bythe questioner, will likely be a weak positive cue,since the speaker may not have been expecting aresponse.
However, the existence of a questionmark by a different speaker may not be indicative.This suggests a need to decompose the context-based feature space by speaker.
Similarly, phrasesuttered prior to the question will likely give rise toa different set of predictive n-grams.Using these observations, we decided to im-plement a simple n-gram model incorporatingcontextual cues to identify rhetorical questions.Specifically, we used unigrams, bigrams, POS bi-grams, and POS trigrams of a question and its im-mediately preceding and following context as fea-ture sets.
Based on preliminary results, we did notuse trigrams or POS unigrams.
POS tags did notcapture sufficient contextual information and tri-grams were not implemented since the utterancesin our dataset were too small to fully utilize them.Also, to capture the contextual information, we744distinguish three distinct categories - questions,utterances immediately preceding questions, andutterances immediately following questions.
Inorder to capture the effect of a feature if it is usedby the same speaker versus a different speaker,we divided the feature space contextual utter-ances into four disjoint groups: precedent-same-speaker, precedent-different-speaker, subsequent-same-speaker, and subsequent-different-speaker.Features in each group are all considered indepen-dently.4 Experimental Setup4.1 DataFor the experiments, we used the SwitchboardDialog Act Corpus (Godfrey et al.
1992; Juraf-sky et al.
1997b), which contains labeled utter-ances from phone conversations between differ-ent pairs of people.
We preprocessed the data tocontain only the utterances marked as questions(rhetorical or otherwise), as well as the utterancesimmediately preceding and following the ques-tions.
Additionally, connectives like and and butwere marked as t con, the end of conversation wasmarked as t empty, and laughter was marked ast laugh.After filtering down to questions, we split thedata into 5960 questions in the training set and2555 questions in the test set.
We find the datasetto be highly skewed with only1282555or 5% of thetest instances labeled as rhetorical.
Because ofthis, a classifier that naively labels all questions asnon-rhetorical would achieve a 94.99% accuracy.Thus, we chose precision, recall and F1-measureas more appropriate metrics of our classifier per-formance.
We should note also that our results as-sume a high level of consistency of the hand anno-tations from the original taggging of the Switch-board Corpus.
However, based on our observationand the strict guidelines followed by annotators asmentioned in Jurafsky et al.
(1997a), we are rea-sonably confident in the reliability of the rhetori-cal labels.4.2 Learning AlgorithmWe experimented with both Naive Bayes and aSupport Vector Machine (SVM) classifiers.
OurNaive Bayes classifier was smoothed with an add-alpha Laplacian kernel, where alpha was selectedvia cross-validation.
For our SVM, to account forthe highly skewed nature of our dataset, we set thecost-factor based on the ratio of positive (rhetori-cal) to negative (non-rhetorical) questions in ourtraining set as in Morik et al.
(1999).
We tunedthe trade-off between margin and training error viacross validation over the training set.In early experiments, Naive Bayes performedcomparably to or outperformed SVM because thedimensionality of the feature space was relativelylow.
However, we found that SVM performedmore robustly over the large range and dimension-ality of features we employed in the later experi-ments.
Thus, we conducted the main experimentsusing SVMLite (Joachims 1999).As the number of parameters is linear in thenumber of feature sets, an exhaustive searchthrough the space would be intractable.
So as tomake this feasible, we employ a greedy approachto model selection.
We make a naive assumptionthat parameters of feature sets are independent orcodependent on up to one other feature set in thesame group.
Each pair of codependent feature setsis considered alone while holding other featuresets fixed.
Classifier parameters are also assumedto be independent for tuning purposes.In order to optimize search time without sam-pling the parameter space too coarsely, we em-ployed an adaptive refinement variant to a tradi-tional grid search.
First, we discretely sampled theCartesian product of dependent parameters sam-pled at regular geometric or arithmetic intervalsbetween a user-specified minimum and maximum.We then updated minimum and maximum valuesto center around the highest scoring sample andrecursed on the search with the newly downsizedspan for a fixed recursion depth d. In practice, wechoose k = 4 and d = 3.4.3 FeaturesUnigrams, bigrams, POS bigrams, and POS tri-grams were extracted from the questions andneighboring utterances as features, based on theanalysis in Section 3.
Then, feature selection wasperformed as follows.For all features sets, we considered both uni-gram and bigram features.
All unigrams and bi-grams in the training data are considered as po-tential candidates for features.
For each feature setabove, we estimated the maximal predictivity overboth rhetorical and non-rhetorical classes, corre-sponding to using the MLE of P (c|n), where ndenotes the n-gram and c is the class.
We usedthese estimates as a score and select the j n-grams745with the highest score for each n over each group,regardless of class, where j was selected via 4-foldcross validation.Each feature was then encoded as a simple oc-currence count within its respective group for agiven exchange.
The highest scoring unigramsand bigrams are as follows: ?you?, ?do?, ?what?,?to?, ?t con?, ?do you?, ?you know?, ?going to?,?you have?, and ?well ,?.POS features were computed by running a POStagger on all exchanges and and then picking thej-best n-grams as described above.
For our exper-iments, we used the maximum entropy treebankPOS tagger from the NLTK package (Bird et al.2009) to compute POS bigrams and trigrams.Lastly, in order to assess the relative value ofquestion-based and context-based features, we de-signed the following seven feature sets:?
Question (baseline)?
Precedent?
Subsequent?
Question + Precedent?
Question + Subsequent?
Precedent + Subsequent?
Question + Precedent + SubsequentThe question-only feature set serves as ourbaseline without considering context, whereas theother feature sets serve to test the power of thepreceding and following context alone and whenpaired with features from the question itself.Feature setAcc Pre Rec F1Error 95%Question92.41 35.00 60.16 44.257.59 ?1.02Precedent85.64 12.30 30.47 17.5314.36 ?1.36Subsequent78.98 13.68 60.16 22.2921.02 ?1.58Question +Precedent93.82 41.94 60.94 49.686.18 ?0.93Question +Subsequent93.27 39.52 64.84 49.116.73 ?0.97Precedent +Subsequent84.93 19.62 64.84 30.1415.07 ?1.38Question +Precedent +Subsequent94.87 49.03 59.38 53.715.13?
0.86Table 1: Experimental results (%)AC PC Utterance++ X: ?i mean, why not.
?- X: ?what are you telling that student?
?-+ X: ?t laugh why don?t we do that?
?- X: ?who, was in that.
?Table 2: Classification without Context Features (AC: ActualClass, P: Predicted Class.
X denotes the speaker)AC PC Utterances++X: ?t con you give them an f on something thatdoesn?t seem that bad to me.
?X: ?what are you telling that student?
?X: ?you?re telling them that, hey, you might as wellforget it, you know.
?-X: ?get homework done,?X: ?t con you know, where do you find the time?.Y:?well, in the first place it?s not your homework,?-+X: ?ha, ha, lots of luck.
?X: ?is she spayed.
?Y: ?yeah?.-Y: ?t con it says when the conversation is over justsay your good-byes and hang up.
?X: ?t laugh why don?t we do that?Y: ?i, guess so.
?Table 3: Classification with Context Features (AC: ActualClass, PC: Predicted Class.
X and Y denote the speakers)5 Results and AnalysisTable 1 shows the performance of the feature setscross-valided and trained on 5960 questions (withcontext) in the Switchboard corpus and tested onthe 2555 remaining questions.Our results largely reflect our intuition on theexpected utility of our various feature sets.
Fea-tures in the question group prove by far the mostuseful single source, while features within thesubsequent prove to be more useful than featuresin the precedent.
Somewhat surprisingly however,an F1-score of 30.14% is achieved by training oncontextual features alone while ignoring any cuesfrom the question itself, suggesting the power ofcontext in identifying a question as rhetorical.
Ad-ditionally, one of the highest scoring bigrams isyou know, matching our earlier intuitions.Some examples of the success and failings ofour system can be found in Table 2 and 3.
Forinstance, in our question-only feature space, thephrase what are you telling that student?
was in-correctly classified as non-rhetorical.
When thecontextual features were added in, the classifiercorrectly identified it as rhetorical as we might ex-pect.
Failure cases of our simple language modelbased system can be seen for instance in the falsepositive question is she spayed which is inter-746preted as rhetorical, likely due to the unigram yeahin the response.Overall, we achieve our best results when in-cluding both precedent and subsequent contextalong with the question in our feature space.
Thus,our results suggest that incorporating contextualcues from both directly before and after the ques-tion itself outperforms classifiers trained on anaive question-only feature space.5.1 Feature DimensionalityAfter model selection via cross validation, our to-tal feature space dimensionality varies between2914 for the precedent only feature set and 16615for the question + subsequent feature set.
Distinctn-gram and POS n-gram features are consideredfor each of same speaker and different speaker forprecedents and subsequents so as to capture thedistinction between the two.
Examining the rel-ative number of features selected for these sub-feature sets also gives a rough idea of the strengthof the various cues.
For instance, same speakerfeature dimensionality tended to be much lowerthan different speaker feature dimensionality, sug-gesting that considering context uttered by the re-spondent is a better cue as to whether the questionis rhetorical.
Additionally, unigrams and bigramstend to be more useful features than POS n-gramsfor the task of rhetorical question identification, orat least considering the less common POS n-gramsis not as predictive.5.2 Evenly Split DistributionAs the highly skewed nature of our data does notallow us to get a good estimate of error rate, wealso tested our feature sets on a subsection of thedataset with a 50-50 split between rhetorical andnon-rhetorical questions to get a better sense ofthe accuracy of our classifier.
The results can beseen in Table 4.
Our classifier achieves an accu-racy of 81% when trained on the questions aloneand 84% when integrating precedent and subse-quent context.
Due to the reduced size of theevenly split dataset, performing a McNemar?s testwith Edwards?
correction (Edwards 1948) doesnot allow us to reject the null hypothesis that thetwo experiments do not derive from the same dis-tribution with 95% confidence (?2= 1.49 giv-ing a 2-tailed p value of 0.22).
However, over thewhole skewed dataset, we find ?2= 30.74 giv-ing a 2-tailed p < 0.00001 so we have reason tobelieve that with a larger evenly-split dataset inte-grating context-based features provides a quantifi-able advantage.Feature setAcc Pre Rec F1 Error 95%Question81.25 82.71 78.01 80.29 0.19 ?0.05Question +Precedent +Subsequent84.38 88.71 78.01 83.02 0.16 ?0.04Table 4: Experimental results (%) on evenly distributed data(training set size: 670 & test set size: 288)6 ConclusionsIn this paper, we tackle the largely understud-ied problem of rhetorical question identification.While the majority of the classification relies onfeatures extracted from the question itself, addingin n-gram features from the context improves theperformance.
We achieve a 53.71% F1-score byadding features extracted from the preceding andthe subsequent utterances, which is about a 10%improvement from a baseline classifier using onlythe features from the question itself.For future work, we would like to employ morecomplicated features like the sentiment of the con-text, and dictionary features based on an NPI lex-icon.
Also, if available, prosodic information likefocus, pauses, and intonation may be useful.7 AcknowledgementsWe thank Mary Moroney and Andrea Hummelfor helping us identify linguistic characteristics ofrhetorical questions and the anonymous reviewersfor their thoughtful feedback.ReferencesJeremy Ang, Yang Liu, and Elizabeth Shriberg.2005.
Automatic dialog act segmentationand classification in multiparty meetings.
InICASSP (1), pages 1061?1064.Rajesh Bhatt.
1998.
Argument-adjunct asymme-tries in rhetorical questions.
In NELS 29.Steven Bird, Ewan Klein, and Edward Loper.2009.
Natural language processing withPython.
O?Reilly Media, Inc.Allen L. Edwards.
1948.
Note on the ?correctionfor continuity?
in testing the significance of thedifference between correlated proportions.
InPsychometrika, 13(3):185?187.Anastasia Giannakidou.
1999.
Affective depen-dencies In Linguistics and Philosophy, 22(4):367?421.
Springer747John J. Godfrey, Edward C. Holliman, and JaneMcDaniel.
1992.
Switchboard: telephonespeech corpus for research and development.In Acoustics, Speech, and Signal Processing,1992.
ICASSP-92., 1992 IEEE InternationalConference on, volume 1, pages 517?520 vol.1.Sergio Grau, Emilio Sanchis, Mar?
?a Jos?e Castro,David Vilar.
2004.
Dialogue act classificationusing a Bayesian approach In 9th ConferenceSpeech and Computer.Almuth Gresillon.
1980.
Zum linguistischen Sta-tus rhetorischer Fragen InZeitschrift f?ur ger-manistische Linguistik, 8(3): 273?289.Chung-Hye Han.
1998.
Deriving the interpreta-tion of rhetorical questions.
In Proceedings ofWest Coast Conference in Formal Linguistics,volume 16, pages 237?253.
Citeseer.T.
Joachims.
1999.
Making large-scale svmlearning practical.
Advances in kernel methods-support vector learning.Dan Jurafsky, Rebecca Bates, Noah Coccaro,Rachel Martin, Marie Meteer, Klaus Ries, Eliz-abeth Shriberg, Andreas Stolcke, Paul Tay-lor, Carol V. Ess-Dykema, et al.
1997a.
Au-tomatic detection of discourse structure forspeech recognition and understanding.
In Auto-matic Speech Recognition and Understanding,1997.
Proceedings., 1997 IEEE Workshop on,pages 88?95.
IEEE.Dan Jurafsky, Elizabeth Shriberg, and Debra Bi-asca.
1997b.
Switchboard SWBD-DAMSLshallow-discourse-function annotation codersmanual.
Technical Report Draft 13, Universityof Colorado, Institute of Cognitive Science.Simon Keizer, Anton Nijholt, et al.
2002.
Dia-logue act recognition with bayesian networksfor dutch dialogues.
In Proceedings of the 3rdSIGdial workshop on Discourse and dialogue-Volume 2, pages 88?94.
Association for Com-putational Linguistics.Katharina Morik, Peter Brockhausen, and T.Joachims.
1999.
Combining statistical learningwith a knowledge-based approach: a case studyin intensive care monitoring.
Technical report,Technical Report, SFB 475: Komplexit?atsre-duktion in Multivariaten Datenstrukturen, Uni-versit?at Dortmund.Norbert Reithinger and Martin Klesen.
1997.
Dia-logue act classification using language models.In EuroSpeech.
Citeseer.Jerrold M. Saddock.
1971.
Queclaratives In Sev-enth Regional Meeting of the Chicago Linguis-tic Society, 7: 223?232.Jerrold M. Saddock.
1974.
Toward a linguistictheory of speech acts Academic Press NewYorkKen Samuel, Sandra Carberry, and K. Vijay-Shanker.
1999.
Automatically selecting usefulphrases for dialogue act tagging.
arXiv preprintcs/9906016.Ken B. Samuel.
2000.
Discourse learning: aninvestigation of dialogue act tagging usingtransformation-based learning.
University ofDelaware.Elizabeth Shriberg, Andreas Stolcke, Dan Juraf-sky, Noah Coccaro, Marie Meteer, RebeccaBates, Paul Taylor, Klaus Ries, Rachel Martin,and Carol Van Ess-Dykema.
1998.
Can prosodyaid the automatic classification of dialog acts inconversational speech?
Language and speech,41(3-4):443?492.Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliz-abeth Shriberg, Rebecca Bates, Dan Jurafsky,Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer.
2000.
Dialogueact modeling for automatic tagging and recog-nition of conversational speech.
Computationallinguistics, 26(3):339?373.Robert van Rooy.
2003.
Negative polarity items inquestions: Strength as relevance In Journal ofSemantics, 20(3): 239?273.
Oxford UniversityPress.Anand Venkataraman, Andreas Stolcke, and Eliz-abeth Shriberg.
2002.
Automatic dialog act la-beling with minimal supervision.
In 9th Aus-tralian International Conference on Speech Sci-ence and Technology, SST 2002.Daan Verbree, Rutger Rienks, and Dirk Heylen.2006.
Dialogue-act tagging using smart fea-ture selection; results on multiple corpora.
InSpoken Language Technology Workshop, 2006.IEEE, pages 70?73.
IEEE.Volker Warnke, Ralf Kompe, Heinrich Nie-mann, and Elmar N?oth.
1997.
Integrated di-alog act segmentation and classification usingprosodic features and language models.
In EU-ROSPEECH.748Nick Webb, Mark Hepple, and Yorik Wilks.2005.
Dialogue act classification based onintra-utterance features.
In Proceedings of theAAAI Workshop on Spoken Language Under-standing.
Citeseer.Klaus Zechner.
2002.
Automatic summarizationof open-domain multiparty dialogues in diversegenres.
Computational Linguistics, 28(4):447?485.Matthias Zimmerman, Yang Liu, ElizabethShriberg, and Andreas Stolcke.
2005.
A*based joint segentation and classification of di-alog acts in multiparty meetings.
In AutomaticSpeech Recognition and Understanding, 2005IEEE Workshop on, pages 215?219.
IEEE.749
