Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 347?358,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsAspect Extraction with Automated Prior Knowledge LearningZhiyuan Chen Arjun Mukherjee Bing LiuDepartment of Computer ScienceUniversity of Illinois at ChicagoChicago, IL 60607, USA{czyuanacm,arjun4787}@gmail.com,liub@cs.uic.eduAbstractAspect extraction is an important task insentiment analysis.
Topic modeling is apopular method for the task.
However,unsupervised topic models often generateincoherent aspects.
To address the is-sue, several knowledge-based models havebeen proposed to incorporate prior knowl-edge provided by the user to guide mod-eling.
In this paper, we take a majorstep forward and show that in the big dataera, without any user input, it is possi-ble to learn prior knowledge automaticallyfrom a large amount of review data avail-able on the Web.
Such knowledge canthen be used by a topic model to discovermore coherent aspects.
There are two keychallenges: (1) learning quality knowl-edge from reviews of diverse domains,and (2) making the model fault-tolerantto handle possibly wrong knowledge.
Anovel approach is proposed to solve theseproblems.
Experimental results using re-views from 36 domains show that the pro-posed approach achieves significant im-provements over state-of-the-art baselines.1 IntroductionAspect extraction aims to extract target entitiesand their aspects (or attributes) that people haveexpressed opinions upon (Hu and Liu, 2004, Liu,2012).
For example, in ?The voice is not clear,?the aspect term is ?voice.?
Aspect extraction hastwo subtasks: aspect term extraction and aspectterm resolution.
Aspect term resolution groups ex-tracted synonymous aspect terms together.
For ex-ample, ?voice?
and ?sound?
should be grouped to-gether as they refer to the same aspect of phones.Recently, topic models have been extensivelyapplied to aspect extraction because they can per-form both subtasks at the same time while otherexisting methods all need two separate steps (seeSection 2).
Traditional topic models such asLDA (Blei et al, 2003) and pLSA (Hofmann,1999) are unsupervised methods for extracting la-tent topics in text documents.
Topics are aspectsin our task.
Each aspect (or topic) is a distributionover (aspect) terms.
However, researchers haveshown that fully unsupervised models often pro-duce incoherent topics because the objective func-tions of topic models do not always correlate wellwith human judgments (Chang et al, 2009).To tackle the problem, several semi-supervisedtopic models, also called knowledge-based topicmodels, have been proposed.
DF-LDA (Andrze-jewski et al, 2009) can incorporate two formsof prior knowledge from the user: must-linksand cannot-links.
A must-link implies that twoterms (or words) should belong to the same topicwhereas a cannot-link indicates that two termsshould not be in the same topic.
In a similar butmore generic vein, must-sets and cannot-sets areused in MC-LDA (Chen et al, 2013b).
Other re-lated works include (Andrzejewski et al, 2011,Chen et al, 2013a, Chen et al, 2013c, Mukher-jee and Liu, 2012, Hu et al, 2011, Jagarlamudi etal., 2012, Lu et al, 2011, Petterson et al, 2010).They all allow prior knowledge to be specified bythe user to guide the modeling process.In this paper, we take a major step further.
Wemine the prior knowledge directly from a largeamount of relevant data without any user inter-vention, and thus make this approach fully au-tomatic.
We hypothesize that it is possible tolearn quality prior knowledge from the big data(of reviews) available on the Web.
The intuitionis that although every domain is different, thereis a decent amount of aspect overlapping acrossdomains.
For example, every product domainhas the aspect/topic of ?price,?
most electronicproducts share the aspect ?battery?
and some alsoshare ?screen.?
Thus, the shared aspect knowl-347edge mined from a set of domains can poten-tially help improve aspect extraction in each ofthese domains, as well as in new domains.
Ourproposed method aims to achieve this objective.There are two major challenges: (1) learning qual-ity knowledge from a large number of domains,and (2) making the extraction model fault-tolerant,i.e., capable of handling possibly incorrect learnedknowledge.
We briefly introduce the proposedmethod below, which consists of two steps.Learning quality knowledge: Clearly, learnedknowledge from only a single domain can be er-roneous.
However, if the learned knowledge isshared by multiple domains, the knowledge ismore likely to be of high quality.
We thus proposeto first use LDA to learn topics/aspects from eachindividual domain and then discover the shared as-pects (or topics) and aspect terms among a sub-set of domains.
These shared aspects and aspectterms are more likely to be of good quality.
Theycan serve as the prior knowledge to guide a modelto extract aspects.
A piece of knowledge is a setof semantically coherent (aspect) terms which arelikely to belong to the same topic or aspect, i.e.,similar to a must-link, but mined automatically.Extraction guided by learned knowledge: Forreliable aspect extraction using the learned priorknowledge, we must account for possible errorsin the knowledge.
In particular, a piece of au-tomatically learned knowledge may be wrong ordomain specific (i.e., the words in the knowledgeare semantically coherent in some domains butnot in others).
To leverage such knowledge, thesystem must detect those inappropriate pieces ofknowledge.
We propose a method to solve thisproblem, which also results in a new topic model,called AKL (Automated Knowledge LDA), whoseinference can exploit the automatically learnedprior knowledge and handle the issues of incorrectknowledge to produce superior aspects.In summary, this paper makes the followingcontributions:1.
It proposes to exploit the big data to learn priorknowledge and leverage the knowledge in topicmodels to extract more coherent aspects.
Theprocess is fully automatic.
To the best of ourknowledge, none of the existing models for as-pect extraction is able to achieve this.2.
It proposes an effective method to learn qual-ity knowledge from raw topics produced usingreview corpora from many different domains.3.
It proposes a new inference mechanism fortopic modeling, which can handle incorrectknowledge in aspect extraction.2 Related WorkAspect extraction has been studied by many re-searchers in sentiment analysis (Liu, 2012, Pangand Lee, 2008), e.g., using supervised sequencelabeling or classification (Choi and Cardie, 2010,Jakob and Gurevych, 2010, Kobayashi et al, 2007,Li et al, 2010, Yang and Cardie, 2013) and us-ing word frequency and syntactic patterns (Huand Liu, 2004, Ku et al, 2006, Liu et al, 2013,Popescu and Etzioni, 2005, Qiu et al, 2011, So-masundaran and Wiebe, 2009, Wu et al, 2009, Xuet al, 2013, Yu et al, 2011, Zhao et al, 2012, Zhouet al, 2013, Zhuang et al, 2006).
However,these works only perform extraction but not as-pect term grouping or resolution.
Separate aspectterm grouping has been done in (Carenini et al,2005, Guo et al, 2009, Zhai et al, 2011).
Theyassume that aspect terms have been extracted be-forehand.To extract and group aspects simultaneously,topic models have been applied by researchers(Branavan et al, 2008, Brody and Elhadad, 2010,Chen et al, 2013b, Fang and Huang, 2012, Heet al, 2011, Jo and Oh, 2011, Kim et al, 2013,Lazaridou et al, 2013, Li et al, 2011, Lin andHe, 2009, Lu et al, 2009, Lu et al, 2012, Lu andZhai, 2008, Mei et al, 2007, Moghaddam and Es-ter, 2013, Mukherjee and Liu, 2012, Sauper andBarzilay, 2013, Titov and McDonald, 2008, Wanget al, 2010, Zhao et al, 2010).
Our proposed AKLmodel belongs to the class of knowledge-basedtopic models.
Besides the knowledge-based topicmodels discussed in Section 1, document labelsare incorporated as implicit knowledge in (Bleiand McAuliffe, 2007, Ramage et al, 2009).
Ge-ographical region knowledge has also been con-sidered in topic models (Eisenstein et al, 2010).All of these models assume that the prior knowl-edge is correct.
GK-LDA (Chen et al, 2013a) isthe only knowledge-based topic model that dealswith wrong lexical knowledge to some extent.
Aswe will see in Section 6, AKL outperformed GK-LDA significantly due to AKL?s more effective er-ror handling mechanism.
Furthermore, GK-LDAdoes not learn any prior knowledge.Our work is also related to transfer learning tosome extent.
Topic models have been used to help348Input: Corpora DL for knowledge learningTest corpora DT1: // STEP 1: Learning prior knowledge.2: for r = 0 to R do // Iterate R+ 1 times.3: for each domain corpus Di?DL do4: if r = 0 then5: Ai?
LDA(Di);6: else7: Ai?
AKL(Di,K);8: end if9: end for10: A?
?iAi;11: TC ?
Clustering(A);12: for each cluster Tj?
TC do13: Kj?
FPM(Tj);14: end for15: K ?
?jKj;16: end for17: // STEP 2: Using the learned knowledge.18: for each test corpus Di?DT do19: Ai?
AKL(Di,K);20: end forFigure 1: The proposed overall algorithm.transfer learning (He et al, 2011, Pan and Yang,2010, Xue et al, 2008).
However, transfer learn-ing in these papers is for traditional classificationrather than topic/aspect extraction.
In (Kang et al,2012), labeled documents from source domainsare transferred to the target domain to producetopic models with better fitting.
However, we donot use any labeled data.
In (Yang et al, 2011), auser provided parameter indicating the technical-ity degree of a domain was used to model the lan-guage gap between topics.
In contrast, our methodis fully automatic without human intervention.3 Overall AlgorithmThis section introduces the proposed overall algo-rithm.
It consists of two main steps: learning qual-ity knowledge and using the learned knowledge.Figure 1 gives the algorithm.Step 1 (learning quality knowledge, Lines 1-16): The input is the review corpora DL frommultiple domains, from which the knowledge isautomatically learned.
Lines 3 and 5 run LDA oneach review domain corpus Di?
DL to gener-ate a set of aspects/topics Ai(lines 2, 4, and 6-9 will be discussed below).
Line 10 unions thetopics from all domains to give A.
Lines 11-14cluster the topics in A into some coherent groups(or clusters) and then discover knowledgeKjfromeach group of topics using frequent pattern mining(FPM) (Han et al, 2007).
We will detail these inSection 4.
Each piece of the learned knowledgeis a set of terms which are likely to belong to thesame aspect.Iterative improvement: The above process canactually run iteratively because the learned knowl-edge K can help the topic model learn better top-ics in each domain Di?
DL, which results inbetter knowledge K in the next iteration.
This it-erative process is reflected in lines 2, 4, 6-9 and 16.We will examine the performance of the process atdifferent iterations in Section 6.2.
From the sec-ond iteration, we can use the knowledge learnedfrom the previous iteration (lines 6-8).
The learnedknowledge is leveraged by the new model AKL,which is discussed below in Step 2.Step 2 (using the learned knowledge, Lines 17-20): The proposed model AKL is employed to usethe learned knowledge K to help topic modelingin test domains DT , which can be DL or otherunseen domains.
The key challenge of this step ishow to use the learned prior knowledge K effec-tively in AKL and deal with possible errors in K.We will elaborate them in Section 5.Scalability: the proposed algorithm is naturallyscalable as both LDA and AKL run on each do-main independently.
Thus, for all domains, thealgorithm can run in parallel.
Only the resultingtopics need to be brought together for knowledgelearning (Step 1).
These resulting topics used inlearning are much smaller than the domain corpusas only a list of top terms from each topic are uti-lized due to their high reliability.4 Learning Quality KnowledgeThis section details Step 1 in the overall algorithm,which has three sub-steps: running LDA (or AKL)on each domain corpus, clustering the resultingtopics, and mining frequent patterns from the top-ics in each cluster.
Since running LDA is simple,we will not discuss it further.
The proposed AKLmodel will be discussed in Section 5.
Below wefocus on the other two sub-steps.4.1 Topic ClusteringAfter running LDA (or AKL) on each domain cor-pus, a set of topics is obtained.
Each topic isa distribution over terms (or words), i.e., termswith their associated probabilities.
Here, we useonly the top terms with high probabilities.
As dis-cussed earlier, quality knowledge should be shared349by topics across several domains.
Thus, it is nat-ural to exploit a frequency-based approach to dis-cover frequent set of terms as quality knowledge.However, we need to deal with two issues.1.
Generic aspects, such as price with aspectterms like cost and pricy, are shared by many(even all) product domains.
But specific as-pects such as screen, occur only in domainswith products having them.
It means that dif-ferent aspects may have distinct frequencies.Thus, using a single frequency threshold in thefrequency-based approach is not sufficient toextract both generic and specific aspects be-cause the generic aspects will result in numer-ous spurious aspects (Han et al, 2007).2.
A term may have multiple senses in differentdomains.
For example, light can mean ?of littleweight?
or ?something that makes things visi-ble?.
A good knowledge base should have thecapacity of handling this ambiguity.To deal with these two issues, we propose todiscover knowledge in two stages: topic clusteringand frequent pattern mining (FPM).The purpose of clustering is to group raw topicsfrom a topic model (LDA or AKL) into clusters.Each cluster contains semantically related topicslikely to indicate the same real-world aspect.
Wethen mine knowledge from each cluster using anFPM technique.
Note that the multiple senses of aterm can be distinguished by the semantic mean-ings represented by the topics in different clusters.For clustering, we tried k-means and k-medoids (Kaufman and Rousseeuw, 1990), andfound that k-medoids performs slightly better.One possible reason is that k-means is more sen-sitive to outliers.
In our topic clustering, each datapoint is a topic represented by its top terms (withtheir probabilities normalized).
The distance be-tween two data points is measured by symmetrisedKL-Divergence.4.2 Frequent Pattern MiningGiven topics within each cluster, this step findssets of terms that appear together in multiple top-ics, i.e., shared terms among similar topics acrossmultiple domains.
Terms in such a set are likelyto belong to the same aspect.
To find such sets ofterms within each cluster, we use frequent patternmining (FPM) (Han et al, 2007), which is suitedfor the task.
The probability of each term is ig-nored in FPM.FPM is stated as follows: Given a set of trans-actions T, where each transaction ti?
T is a setof items from a global item set I , i.e., ti?
I .
Inour context, tiis the topic vector comprising thetop terms of a topic (no probability attached).
Tis the collection of all topics within a cluster andI is the set of all terms in T. The goal of FPM isto find all patterns that satisfy some user-specifiedfrequency threshold (also called minimum supportcount), which is the minimum number of timesthat a pattern should appear in T. Such patternsare called frequent patterns.
In our context, a pat-tern is a set of terms which have appeared multipletimes in the topics within a cluster.
Such patternscompose our knowledge base as shown below.4.3 Knowledge RepresentationAs the knowledge is extracted from each clusterindividually, we represent our knowledge base asa set of clusters, where each cluster consists of aset of frequent 2-patterns mined using FPM, e.g.,Cluster 1: {battery, life}, {battery, hour},{battery, long}, {charge, long}Cluster 2: {service, support}, {support, cus-tomer}, {service, customer}Using two terms in a set is sufficient to cover thesemantic relationship of the terms belonging to thesame aspect.
Longer patterns tend to contain moreerrors since some terms in a set may not belong tothe same aspect as others.
Such partial errors hurtperformance in the downstream model.5 AKL: Using the Learned KnowledgeWe now present the proposed topic model AKL,which is able to use the automatically learnedknowledge to improve aspect extraction.5.1 Plate NotationDiffering from most topic models based on topic-term distribution, AKL incorporates a latent clus-ter variable c to connect topics and terms.
Theplate notation of AKL is shown in Figure 2.
Theinputs of the model are M documents, T top-ics and C clusters.
Each document m has Nmterms.
We model distribution P (cluster|topic)as ?
and distribution P (term|topic, cluster) as?
with Dirichlet priors ?
and ?
respectively.P (topic|document) is modeled by ?
with aDirichlet prior ?.
The terms in each document areassumed to be generated by first sampling a topicz, and then a cluster c given topic z, and finally350?
?
z c w NmM?
T ?
TXC?
?Figure 2: Plate notation for AKL.a term w given topic z and cluster c. This platenotation of AKL and its associated generative pro-cess are similar to those of MC-LDA (Chen et al,2013b).
However, there are three key differences.1.
Our knowledge is automatically mined whichmay have errors (or noises), while the priorknowledge for MC-LDA is manually providedand assumed to be correct.
As we will see inSection 6, using our knowledge, MC-LDA doesnot generate as coherent aspects as AKL.2.
Our knowledge is represented as clusters.
Eachcluster contains a set of frequent 2-patternswith semantically correlated terms.
They aredifferent from must-sets used in MC-LDA.3.
Most importantly, due to the use of the newform of knowledge, AKL?s inference mecha-nism (Gibbs sampler) is entirely different fromthat of MC-LDA (Section 5.2), which results insuperior performances (Section 6).
Note thatthe inference mechanism and the prior knowl-edge cannot be reflected in the plate notationfor AKL in Figure 2.In short, our modeling contributions are (1) thecapability of handling more expressive knowledgein the form of clusters, (2) a novel Gibbs samplerto deal with inappropriate knowledge.5.2 The Gibbs SamplerAs the automatically learned prior knowledge maycontain errors for a domain, AKL has to learnthe usefulness of each piece of knowledge dy-namically during inference.
Instead of assigningweights to each piece of knowledge as a fixed priorin (Chen et al, 2013a), we propose a new Gibbssampler, which can dynamically balance the useof prior knowledge and the information in the cor-pus during the Gibbs sampling iterations.We adopt a Blocked Gibbs sampler (Rosen-Zviet al, 2010) as it improves convergence and re-duces autocorrelation when the variables (topic zand cluster c in AKL) are highly related.
For eachterm wiin each document, we jointly sample atopic ziand cluster ci(containing wi) based on theconditional distribution in Gibbs sampler (will bedetailed in Equation 4).
To compute this distribu-tion, instead of considering how well zimatcheswith wionly (as in LDA), we also consider twoother factors:1.
The extent cicorroborates wigiven the corpus.By ?corroborate?, we mean whether those fre-quent 2-patterns in cicontaining wiare alsosupported by the actual information in the do-main corpus to some extent (see the measure inEquation 1 below).
If cicorroborates wiwell,ciis likely to be useful, and thus should alsoprovide guidance in determining zi.
Otherwise,cimay not be a suitable piece of knowledge forwiin the domain.2.
Agreement between ciand zi.
By agreementwe mean the degree that the terms (union of allfrequent 2-patterns of ci) in cluster ciare re-flected in topic zi.
Unlike the first factor, this isa global factor as it concerns all the terms in aknowledge cluster.For the first factor, we measure how well cicorroborates wigiven the corpus based on co-document frequency ratio.
As shown in (Mimnoet al, 2011), co-document frequency is a good in-dicator of term correlation in a domain.
Follow-ing (Mimno et al, 2011), we define a symmetricco-document frequency ratio as follows:Co-Doc(w,w?)
=D(w,w?)
+ 1(D(w) +D(w?
))?12+ 1(1)where (w,w?)
refers to each frequent 2-pattern inthe knowledge cluster ci.
D(w,w?)
is the numberof documents that contain both termsw andw?andD(w) is the number of documents containing w.A smoothing count of 1 is added to avoid the ratiobeing 0.For the second factor, if cluster ciand topic ziagree, the intuition is that the terms in ci(union ofall frequent 2-patterns of ci) should appear as topterms under zi(i.e., ranked top according to theterm probability under zi).
We define the agree-ment using symmetrised KL-Divergence betweenthe two distributions (DISTcand DISTz) cor-responding to ciand zirespectively.
As there isno prior preference on the terms of ci, we usethe uniform distribution over all terms in ciforDISTc.
For DISTz, as only top 20 terms un-der ziare usually reliable, we use these top terms351with their probabilities (re-normalized) to repre-sent the topic.
Note that a smoothing probability(i.e., a very small value) is also given to every termfor calculating KL-Divergence.
GivenDISTcandDISTz, the agreement is computed with:Agreement(c, z) =1KL(DISTc, DISTz)(2)The rationale of Equation 2 is that the lesser di-vergence between DISTcand DISTzimplies themore agreement between ciand zi.We further employ the Generalized Plya urn(GPU) model (Mahmoud, 2008) which was shownto be effective in leveraging semantically relatedwords (Chen et al, 2013a, Chen et al, 2013b,Mimno et al, 2011).
The GPU model here ba-sically states that assigning topic ziand cluster cito term wiwill not only increase the probabilityof connecting ziand ciwith wi, but also makeit more likely to associate ziand ciwith term w?where w?shares a 2-pattern with wiin ci.
Theamount of probability increase is determined bymatrixAc,w?,wdefined as:Ac,w?,w=????
?1, if w = w?
?, if (w,w?)
?
c, w 6= w?0, otherwise(3)where value 1 controls the probability increase ofw by seeingw itself, and ?
controls the probabilityincrease of w?by seeing w. Please refer to (Chenet al, 2013b) for more details.Putting together Equations 1, 2 and 3 into ablocked Gibbs Sampler, we can define the follow-ing sampling distribution in Gibbs sampler so thatit provides helpful guidance in determining theusefulness of the prior knowledge and in selectingthe semantically coherent topic.P (zi= t, ci= c|z?i, c?i,w, ?, ?, ?,A)??(w,w?)?cCo-Doc(w,w?
)?Agreement(c, t)?n?im,t+ ?
?Tt?=1(n?im,t?+ ?)??Vw?=1?Vv?=1Ac,v?,w??
n?it,c,v?+ ??Cc?=1(?Vw?=1?Vv?=1Ac?,v?,w??
n?it,c?,v?+ ?)??Vw?=1Ac,w?,wi?
n?it,c,w?+ ??Vv?=1(?Vw?=1Ac,w?,v??
n?it,c,w?+ ?
)(4)where n?idenotes the count excluding the currentassignment of ziand ci, i.e., z?iand c?i.
nm,tde-notes the number of times that topic twas assignedto terms in document m. nt,cdenotes the timesthat cluster c occurs under topic t. nt,c,vrefers tothe number of times that term v appears in clusterc under topic t.
?, ?
and ?
are predefined Dirichlethyperparameters.Note that although the above Gibbs sampler isable to distinguish useful knowledge from wrongknowledge, it is possible that there is no clustercorroborates for a particular term.
For every termw, apart from its knowledge clusters, we also adda singleton cluster for w, i.e., a cluster with onepattern {w,w} only.
When no knowledge clusteris applicable, this singleton cluster is used.
As asingleton cluster does not contain any knowledgeinformation but only the word itself, Equations 1and 2 cannot be computed.
For the values of sin-gleton clusters for these two equations, we assignthem as the averages of those values of all non-singleton knowledge clusters.6 ExperimentsThis section evaluates and compares the pro-posed AKL model with three baseline modelsLDA, MC-LDA, and GK-LDA.
LDA (Blei etal., 2003) is the most popular unsupervised topicmodel.
MC-LDA (Chen et al, 2013b) is a re-cent knowledge-based model for aspect extrac-tion.
GK-LDA (Chen et al, 2013a) handles wrongknowledge by setting prior weights using the ratioof word probabilities.
Our automatically extractedknowledge is provided to these models.
Note thatcannot-set of MC-LDA is not used in AKL.6.1 Experimental SettingsDataset.
We created a large dataset containingreviews from 36 product domains or types fromAmazon.com.
The product domain names arelisted in Table 1.
Each domain contains 1, 000 re-views.
This gives us 36 domain corpora.
We havemade the dataset publically available at the web-site of the first author.Pre-processing.
We followed (Chen et al, 2013b)to employ standard pre-processing like lemmatiza-tion and stopword removal.
To have a fair compar-ison, we also treat each sentence as a document asin (Chen et al, 2013a, Chen et al, 2013b).Parameter Settings.
For all models, posterior es-timates of latent variables were taken with a sam-pling lag of 20 iterations in the post burn-in phase(first 200 iterations for burn-in) with 2, 000 itera-tions in total.
The model parameters were tunedon the development set in our pilot experiments352Amplifier DVD Player Kindle MP3 Player Scanner Video PlayerBlu-Ray Player GPS Laptop Network Adapter Speaker Video RecorderCamera Hard Drive Media Player Printer Subwoofer WatchCD Player Headphone Microphone Projector Tablet WebcamCell Phone Home Theater System Monitor Radar Detector Telephone Wireless RouterComputer Keyboard Mouse Remote Control TV XboxTable 1: List of 36 domain names.and set to ?
= 1, ?
= 0.1, T = 15, and ?
= 0.2.Furthermore, for each cluster, ?
is set proportionalto the number of terms in it.
The other param-eters for MC-LDA and GK-LDA were set as intheir original papers.
For parameters of AKL, weused the top 15 terms for each topic in the clus-tering phrase.
The number of clusters is set tothe number of domains.
We will test the sensitiv-ity of these clustering parameters in Section 6.4.The minimum support count for frequent patternmining was set empirically to min(5, 0.4?#T),where #T is the number of transactions (i.e., thenumber of topics from all domains) in a cluster.Test Settings: We use two test settings as below:1.
(Sections 6.2, 6.3 and 6.4) Test on the same cor-pora as those used in learning the prior knowl-edge.
This is meaningful as the learning phraseis automatic and unsupervised (Figure 1).
Wecall this self-learning-and-improvement.2.
(Section 6.5) Test on new/unseen domain cor-pora after knowledge learning.6.2 Topic CoherenceThis sub-section evaluates the topics/aspects gen-erated by each model based on Topic Coher-ence (Mimno et al, 2011) in test setting 1.
Tra-ditionally, topic models have been evaluated us-ing perplexity.
However, perplexity on the held-out test set does not reflect the semantic coher-ence of topics and may be contrary to human judg-ments (Chang et al, 2009).
Instead, the met-ric Topic Coherence has been shown in (Mimno-1510-1490-1470-1450-14300 1 2 3 4 5 6TopicCoherenceAKL GK-LDAMC-LDA LDAFigure 3: Average Topic Coherence of each modelat different learning iterations (Iteration 0 is equiv-alent to LDA).et al, 2011) to correlate well with human judg-ments.
Recently, it has become a standard prac-tice to use Topic Coherence for evaluation of topicmodels (Arora et al, 2013).
A higher Topic Coher-ence value indicates a better topic interpretability,i.e., semantically more coherent topics.Figure 3 shows the average Topic Coherence ofeach model using knowledge learned at differentlearning iterations (Figure 1).
For MC-LDA orGK-LDA, this is done by replacing AKL in lines7 and 19 of Figure 1 with MC-LDA or GK-LDA.Each value is the average over all 36 domains.From Figure 3, we can observe the followings:1.
AKL performs the best with the highest TopicCoherence values at all iterations.
It is actu-ally the best in all 36 domains.
These show thatAKL finds more interpretable topics than thebaselines.
Its values stabilize after iteration 3.2.
Both GK-LDA and MC-LDA perform slightlybetter than LDA in iterations 1 and 2.
MC-LDA does not handle wrong knowledge.
Thisshows that the mined knowledge is of goodquality.
Although GK-LDA uses large wordprobability differences under a topic to detectwrong lexical knowledge, it is not as effectiveas AKL.
The reason is that as the lexical knowl-edge is from general dictionaries rather thanmined from relevant domain data, the wordsin a wrong piece of knowledge usually have avery large probability difference under a topic.However, our knowledge is mined from topwords in related topics including topics fromthe current domain.
The words in a piece of in-correct (or correct) knowledge often have sim-ilar probabilities under a topic.
The proposeddynamic knowledge adjusting mechanism inAKL is superior.Paired t-test shows that AKL outperforms allbaselines significantly (p < 0.0001).6.3 User EvaluationAs our objective is to discover more coherent as-pects, we recruited two human judges.
Here wealso use the test setting 1.
Each topic is annotatedas coherent if the judge feels that most of its top3530.60.70.80.91.0Camera Computer Headphone GPSPrecision@ 5AKL GK-LDA MC-LDA LDA0.60.70.80.91.0Camera Computer Headphone GPSPrecision@ 10AKL GK-LDA MC-LDA LDAFigure 4: Average Precision@5 (Left) and Precision@10 (Right) of coherent topics from four modelsin each domain.
(Headphone has a lot of overlapping topics in other domains while GPS has little.
)terms coherently represent a real-world productaspect; otherwise incoherent.
For a coherent topic,each top term is annotated as correct if it reflectsthe aspect represented by the topic; otherwise in-correct.
We labeled the topics of each modelat learning iteration 1 where the same pieces ofknowledge (extracted from LDA topics at learn-ing iteration 0) are provided to each model.
Afterlearning iteration 1, the gap between AKL and thebaseline models tends to widen.
To be consistent,the results later in Sections 6.4 and 6.5 also showeach model at learning iteration 1.
We also noticethat after a few learning iterations, the topics fromAKL model tend to have some resemblance acrossdomains.
We found that AKL with 2 learning it-erations achieved the best topics.
Note that LDAcannot use any prior knowledge.We manually labeled results from four domains,i.e., Camera, Computer, Headphone, and GPS.
Wechose Headphone as it has a lot of overlappingof topics with other domains because many elec-tronic products use headphone.
GPS was cho-sen because it does not have much topic overlap-ping with other domains as its aspects are mostlyabout Navigation and Maps.
Domains Camera andComputer lay in between.
We want to see howdomain overlapping influences the performance ofAKL.
Cohen?s Kappa scores for annotator agree-ment are 0.918 (for topics) and 0.872 (for terms).To measure the results, we computePrecision@n (or p@n) based on the anno-tations, which was also used in (Chen et al,2013b, Mukherjee and Liu, 2012).Figure 4 shows the precision@n results forn = 5 and 10.
We can see that AKL makes im-provements in all 4 domains.
The improvementvaries in domains with the most increase in Head-phone and the least in GPS as Headphone overlapsmore with other domains than GPS.
Note that if adomain shares aspects with many other domains,its model should benefit more; otherwise, it is rea-sonable to expect lesser improvements.
For thebaselines, GK-LDA and MC-LDA perform simi-larly to LDA with minor variations, all of whichare inferior to AKL.
AKL?s improvements overother models are statistically significant based onpaired t-test (p < 0.002).In terms of the number of coherent topics, AKLdiscovers one more coherent topic than LDA inComputer and one more coherent topic than GK-LDA and MC-LDA in Headphone.
For the otherdomains, the numbers of coherent topics are thesame for all models.Table 2 shows an example aspect (battery) andits top 10 terms produced by AKL and LDA foreach domain to give a flavor of the kind of im-provements made by AKL.
The results for GK-LDA and MC-LDA are about the same as LDA(see also Figure 4).
Table 2 focuses on the as-pects generated by AKL and LDA.
From Table 2,we can see that AKL discovers more correct andmeaningful aspect terms at the top.
Note thatthose terms marked in red and italicized are er-rors.
Apart from Table 2, many aspects are dra-matically improved by AKL, including some com-monly shared aspects such as Price, Screen, andCustomer Service.6.4 Sensitivity to Clustering ParametersThis sub-section investigates the sensitivity of theclustering parameters of AKL (again in test setting1).
The top sub-figure in Figure 5 shows the aver-age Topic Coherence values versus the top k termsper topic used in topic clustering (Section 4.1).The number of clusters is set to the number ofdomains (see below).
We can observe that usingk = 15 top terms gives the highest value.
This isintuitive as too few (or too many) top terms maygenerate insufficient (or noisy) knowledge.The bottom sub-figure in Figure 5 shows theaverage Topic Coherence given different number354Camera Computer Headphone GPSAKL LDA AKL LDA AKL LDA AKL LDAbattery battery battery battery hour long battery triplife card hour cable long battery hour batteryhour memory life speaker battery hour long hourlong life long dvi life comfortable model milecharge usb speaker sound charge easy life longextra hour sound hour amp uncomfortable charge lifeminute minute charge connection uncomfortable headset trip destinationcharger sd dvi life comfortable life purchase phoneshort extra tv hdmus period money older chargeaa device hdmus tv output hard compass modeTable 2: Example aspect Battery from AKL and LDA in each domain.
Errors are italicized in red.-1510-1490-1470-1450-14305 10 15 20 25 30Topic Coherence#Top Terms for Clustering-1510-1490-1470-1450-143020 30 40 50 60 70Topic Coherence#ClustersFigure 5: Average topic coherence of AKL versus#top k terms (Top) and #clusters (Bottom).-1490-1480-1470-1460-1450AKL GK-LDA MC-LDA LDATopicCoherenceFigure 6: Average topic coherence of each modeltested on new/unseen domain.of clusters.
We fix the number of top terms pertopic to 15 as it yields the best result (see the topsub-figure in Figure 5).
We can see that the per-formance is not very sensitive to the number ofclusters.
The model performs similarly for 30 to50 clusters, with lower Topic Coherence for lessthan 30 or more than 50 clusters.
The significancetest indicates that using 30, 40, and 50 clusters,AKL achieved significant improvements over allbaseline models (p < 0.0001).
With more do-mains, we should expect a larger number of clus-ters.
However, it is difficult to obtain the optimalnumber of clusters.
Thus, we empirically set thenumber of clusters to the number of domains inour experiments.
Note that the number of clus-ters (C) is expected to be larger than the numberof topics in one domain (T ) because C is for alldomains while T is for one particular domain.6.5 Test on New DomainsWe now evaluate AKL in test setting 2, i.e., the au-tomatically extracted knowledge K (Figure 1) isapplied in new/unseen domains other than those indomainsDL used in knowledge learning.
The aimis to see how K can help modeling in an unseendomain.
In this set of experiments, each domainis tested by using the learned knowledge from therest 35 domains.
Figure 6 shows the average TopicCoherence of each model.
The values are also av-eraged over the 36 tested domains.
We can see thatAKL achieves the highest Topic Coherence valuewhile LDA has the lowest.
The improvements ofAKL over all baseline models are significant withp < 0.0001.7 ConclusionsThis paper proposed an advanced aspect extractionframework which can learn knowledge automati-cally from a large number of review corpora andexploit the learned knowledge in extracting morecoherent aspects.
It first proposed a technique tolearn knowledge automatically by clustering andFPM.
Then a new topic model with an advancedinference mechanism was proposed to exploit thelearned knowledge in a fault-tolerant manner.
Ex-perimental results using review corpora from 36domains showed that the proposed method outper-forms state-of-the-art methods significantly.AcknowledgmentsThis work was supported in part by a grant fromNational Science Foundation (NSF) under grantno.
IIS-1111092.355ReferencesDavid Andrzejewski, Xiaojin Zhu, and Mark Craven.2009.
Incorporating domain knowledge into topicmodeling via Dirichlet Forest priors.
In Proceedingsof ICML, pages 25?32.David Andrzejewski, Xiaojin Zhu, Mark Craven, andBenjamin Recht.
2011.
A framework for incorpo-rating general domain knowledge into latent Dirich-let alocation using first-order logic.
In Proceedingsof IJCAI, pages 1171?1177.Sanjeev Arora, Rong Ge, Yonatan Halpern, DavidMimno, Ankur Moitra, David Sontag, Yichen Wu,and Michael Zhu.
2013.
A Practical Algorithm forTopic Modeling with Provable Guarantees.
In Pro-ceedings of ICML, pages 280?288.David M. Blei and Jon D McAuliffe.
2007.
SupervisedTopic Models.
In Proceedings of NIPS, pages 121?128.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.S R K Branavan, Harr Chen, Jacob Eisenstein, andRegina Barzilay.
2008.
Learning Document-LevelSemantic Properties from Free-Text Annotations.
InProceedings of ACL, pages 263?271.Samuel Brody and Noemie Elhadad.
2010.
An unsu-pervised aspect-sentiment model for online reviews.In Proceedings of NAACL, pages 804?812.Giuseppe Carenini, Raymond T Ng, and Ed Zwart.2005.
Extracting knowledge from evaluative text.In Proceedings of K-CAP, pages 11?18.Jonathan Chang, Jordan Boyd-Graber, Wang Chong,Sean Gerrish, and David Blei, M. 2009.
ReadingTea Leaves: How Humans Interpret Topic Models.In Proceedings of NIPS, pages 288?296.Zhiyuan Chen, Arjun Mukherjee, Bing Liu, MeichunHsu, Malu Castellanos, and Riddhiman Ghosh.2013a.
Discovering Coherent Topics Using GeneralKnowledge.
In Proceedings of CIKM, pages 209?218.Zhiyuan Chen, Arjun Mukherjee, Bing Liu, MeichunHsu, Malu Castellanos, and Riddhiman Ghosh.2013b.
Exploiting Domain Knowledge in AspectExtraction.
In Proceedings of EMNLP, pages 1655?1667.Zhiyuan Chen, Arjun Mukherjee, Bing Liu, MeichunHsu, Malu Castellanos, and Riddhiman Ghosh.2013c.
Leveraging Multi-Domain Prior Knowledgein Topic Models.
In Proceedings of IJCAI, pages2071?2077.Yejin Choi and Claire Cardie.
2010.
Hierarchical Se-quential Learning for Extracting Opinions and theirAttributes.
In Proceedings of ACL, pages 269?274.Jacob Eisenstein, Brendan O?Connor, Noah A Smith,and Eric P Xing.
2010.
A Latent Variable Modelfor Geographic Lexical Variation.
In Proceedings ofEMNLP, pages 1277?1287.Lei Fang and Minlie Huang.
2012.
Fine Granular As-pect Analysis using Latent Structural Models.
InProceedings of ACL, pages 333?337.Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,and Zhong Su.
2009.
Product feature categoriza-tion with multilevel latent semantic association.
InProceedings of CIKM, pages 1087?1096.Jiawei Han, Hong Cheng, Dong Xin, and Xifeng Yan.2007.
Frequent pattern mining: current status andfuture directions.
Data Mining and Knowledge Dis-covery, 15(1):55?86.Yulan He, Chenghua Lin, and Harith Alani.
2011.
Au-tomatically Extracting Polarity-Bearing Topics forCross-Domain Sentiment Classification.
In Pro-ceedings of ACL, pages 123?131.Thomas Hofmann.
1999.
Probabilistic Latent Seman-tic Analysis.
In Proceedings of UAI, pages 289?296.Minqing Hu and Bing Liu.
2004.
Mining and Summa-rizing Customer Reviews.
In Proceedings of KDD,pages 168?177.Yuening Hu, Jordan Boyd-Graber, and Brianna Sati-noff.
2011.
Interactive Topic Modeling.
In Pro-ceedings of ACL, pages 248?257.Jagadeesh Jagarlamudi, Hal Daum?e III, and Raghaven-dra Udupa.
2012.
Incorporating Lexical Priors intoTopic Models.
In Proceedings of EACL, pages 204?213.Niklas Jakob and Iryna Gurevych.
2010.
ExtractingOpinion Targets in a Single- and Cross-Domain Set-ting with Conditional Random Fields.
In Proceed-ings of EMNLP, pages 1035?1045.Yohan Jo and Alice H. Oh.
2011.
Aspect and senti-ment unification model for online review analysis.In Proceedings of WSDM, pages 815?824.Jeon-hyung Kang, Jun Ma, and Yan Liu.
2012.
Trans-fer Topic Modeling with Ease and Scalability.
InProceedings of SDM, pages 564?575.L Kaufman and P J Rousseeuw.
1990.
Finding groupsin data: an introduction to cluster analysis.
JohnWiley and Sons.Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, andShixia Liu.
2013.
A Hierarchical Aspect-SentimentModel for Online Reviews.
In Proceedings of AAAI,pages 526?533.Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.2007.
Extracting Aspect-Evaluation and Aspect-ofRelations in Opinion Mining.
In Proceedings ofEMNLP, pages 1065?1074.356Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.2006.
Opinion Extraction, Summarization andTracking in News and Blog Corpora.
In Proceed-ings of AAAI-CAAW, pages 100?107.Angeliki Lazaridou, Ivan Titov, and CarolineSporleder.
2013.
A Bayesian Model for JointUnsupervised Induction of Sentiment, Aspect andDiscourse Representations.
In Proceedings of ACL,pages 1630?1639.Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,Yingju Xia, Shu Zhang, and Hao Yu.
2010.Structure-Aware Review Mining and Summariza-tion.
In Proceedings of COLING, pages 653?661.Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang.
2011.Generating Aspect-oriented Multi-Document Sum-marization with Event-aspect model.
In Proceed-ings of EMNLP, pages 1137?1146.Chenghua Lin and Yulan He.
2009.
Joint senti-ment/topic model for sentiment analysis.
In Pro-ceedings of CIKM, pages 375?384.Kang Liu, Liheng Xu, and Jun Zhao.
2013.
SyntacticPatterns versus Word Alignment: Extracting Opin-ion Targets from Online Reviews.
In Proceedings ofACL, pages 1754?1763.Bing Liu.
2012.
Sentiment Analysis and Opinion Min-ing.
Morgan & Claypool Publishers.Yue Lu and Chengxiang Zhai.
2008.
Opinion inte-gration through semi-supervised topic modeling.
InProceedings of WWW, pages 121?130.Yue Lu, ChengXiang Zhai, and Neel Sundaresan.2009.
Rated aspect summarization of short com-ments.
In Proceedings of WWW, pages 131?140.Bin Lu, Myle Ott, Claire Cardie, and Benjamin K Tsou.2011.
Multi-aspect Sentiment Analysis with TopicModels.
In Proceedings of ICDM Workshops, pages81?88.Yue Lu, Hongning Wang, ChengXiang Zhai, and DanRoth.
2012.
Unsupervised discovery of opposingopinion networks from forum discussions.
In Pro-ceedings of CIKM, pages 1642?1646.Hosam Mahmoud.
2008.
Polya Urn Models.
Chap-man & Hall/CRC Texts in Statistical Science.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,and ChengXiang Zhai.
2007.
Topic sentiment mix-ture: modeling facets and opinions in weblogs.
InProceedings of WWW, pages 171?180.David Mimno, Hanna M. Wallach, Edmund Talley,Miriam Leenders, and Andrew McCallum.
2011.Optimizing semantic coherence in topic models.
InProceedings of EMNLP, pages 262?272.Samaneh Moghaddam and Martin Ester.
2013.
TheFLDA Model for Aspect-based Opinion Mining:Addressing the Cold Start Problem.
In Proceedingsof WWW, pages 909?918.Arjun Mukherjee and Bing Liu.
2012.
Aspect Extrac-tion through Semi-Supervised Modeling.
In Pro-ceedings of ACL, pages 339?348.Sinno Jialin Pan and Qiang Yang.
2010.
A Survey onTransfer Learning.
IEEE Trans.
Knowl.
Data Eng.,22(10):1345?1359.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1-2):1?135.James Petterson, Alex Smola, Tib?erio Caetano, WrayBuntine, and Shravan Narayanamurthy.
2010.
WordFeatures for Latent Dirichlet Allocation.
In Pro-ceedings of NIPS, pages 1921?1929.AM Popescu and Oren Etzioni.
2005.
Extracting prod-uct features and opinions from reviews.
In Proceed-ings of HLT, pages 339?346.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.2011.
Opinion Word Expansion and Target Extrac-tion through Double Propagation.
ComputationalLinguistics, 37(1):9?27.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D. Manning.
2009.
Labeled LDA: a su-pervised topic model for credit attribution in multi-labeled corpora.
In Proceedings of EMNLP, pages248?256.Michal Rosen-Zvi, Chaitanya Chemudugunta, ThomasGriffiths, Padhraic Smyth, and Mark Steyvers.2010.
Learning author-topic models from text cor-pora.
ACM Transactions on Information Systems,28(1):1?38.Christina Sauper and Regina Barzilay.
2013.
Auto-matic Aggregation by Joint Modeling of Aspects andValues.
J. Artif.
Intell.
Res.
(JAIR), 46:89?127.Swapna Somasundaran and J Wiebe.
2009.
Recog-nizing stances in online debates.
In Proceedings ofACL, pages 226?234.Ivan Titov and Ryan McDonald.
2008.
A joint modelof text and aspect ratings for sentiment summariza-tion.
In Proceedings of ACL, pages 308?316.Hongning Wang, Yue Lu, and Chengxiang Zhai.
2010.Latent aspect rating analysis on review text data: arating regression approach.
In Proceedings of KDD,pages 783?792.Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.2009.
Phrase dependency parsing for opinion min-ing.
In Proceedings of EMNLP, pages 1533?1541.Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and JunZhao.
2013.
Mining Opinion Words and OpinionTargets in a Two-Stage Framework.
In Proceedingsof ACL, pages 1764?1773.GR Xue, Wenyuan Dai, Q Yang, and Y Yu.
2008.Topic-bridged PLSA for cross-domain text classifi-cation.
In Proceedings of SIGIR, pages 627?634.357Bishan Yang and Claire Cardie.
2013.
Joint Inferencefor Fine-grained Opinion Extraction.
In Proceed-ings of ACL, pages 1640?1649.Shuang Hong Yang, Steven P. Crain, and HongyuanZha.
2011.
Bridging the language gap: Topic adap-tation for documents with different technicality.
InProceedings of AISTATS, pages 823?831.Jianxing Yu, Zheng-Jun Zha, Meng Wang, and Tat-Seng Chua.
2011.
Aspect Ranking: IdentifyingImportant Product Aspects from Online ConsumerReviews.
In Proceedings of ACL, pages 1496?1505.Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia.
2011.Constrained LDA for grouping product features inopinion mining.
In Proceedings of PAKDD, pages448?459.Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-ing Li.
2010.
Jointly Modeling Aspects and Opin-ions with a MaxEnt-LDA Hybrid.
In Proceedings ofEMNLP, pages 56?65.Yanyan Zhao, Bing Qin, and Ting Liu.
2012.
Col-location polarity disambiguation using web-basedpseudo contexts.
In Proceedings of EMNLP-CoNLL, pages 160?170.Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao.
2013.Collective Opinion Target Extraction in Chinese Mi-croblogs.
In Proceedings of EMNLP, pages 1840?1850.Li Zhuang, Feng Jing, and Xiao-Yan Zhu.
2006.Movie review mining and summarization.
In Pro-ceedings of CIKM, pages 43?50.358
