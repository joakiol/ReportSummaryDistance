Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 28?36,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUsing Web Selectors for the Disambiguation of All WordsHansen A. Schwartz and Fernando GomezSchool of Electrical Engineering and Computer ScienceUniversity of Central FloridaOrlando, FL 32816, USA{hschwartz, gomez}@cs.ucf.eduAbstractThis research examines a word sense dis-ambiguation method using selectors acquiredfrom the Web.
Selectors describe words whichmay take the place of another given wordwithin its local context.
Work in using Web se-lectors for noun sense disambiguation is gen-eralized into the disambiguation of verbs, ad-verbs, and adjectives as well.
Additionally,this work incorporates previously ignored ad-verb context selectors and explores the effec-tiveness of each type of context selector ac-cording to its part of speech.
Overall resultsfor verb, adjective, and adverb disambigua-tion are well above a random baseline andslightly below the most frequent sense base-line, a point which noun sense disambigua-tion overcomes.
Our experiments find that,for noun and verb sense disambiguation tasks,each type of context selector may assist targetselectors in disambiguation.
Finally, these ex-periments also help to draw insights about thefuture direction of similar research.1 IntroductionThe great amount of text on the Web has emergedas an unprecedented electronic source of naturallanguage.
Recently, word sense disambiguationsystems have fostered the size of the Web in or-der to supplant the issue of limited annotated dataavailability for supervised systems (Mihalcea, 2002;Agirre and Martinez, 2004).
Some unsupervised orminimally supervised methods use the Web more di-rectly in disambiguation algorithms that do not usea training set for the specific target words.One such minimally supervised method uses se-lectors acquired from the Web for noun sense disam-biguation by comparing the selectors of a given sen-tence to a target noun within the sentence (Schwartzand Gomez, 2008).
Although this work found strongresults, many aspects of the use of selectors was leftunexplored.
For one, the method was only appliedto noun sense disambiguation, focusing on the well-developed noun hypernym hierarchy within Word-Net (Miller et al, 1993).
Additionally, the role ofdifferent types of selectors was not extensively ex-plored, and adverb selectors were not used at all.
Weseek to address those issues.In this paper, we extend our method of using se-lectors from the Web for noun sense disambigua-tion into a more robust method of disambiguatingwords of all parts of speech.
After a brief back-ground on selectors and related work, we explainthe acquisition and empirical application of selec-tors from nouns, verbs, adjectives, pronouns/propernouns, and adverbs.
Finally, results are presentedfrom the SemEval-2007 coarse grained all-wordstask (Navigli et al, 2007), and we explore the influ-ence of various types of selectors on the algorithmin order to draw insight for future improvement ofWeb-based methods.2 BackgroundIn this section we describe related research in selec-tors and solving the problem of word sense disam-biguation (WSD).
Specifically, two types of WSDresearch are examined: works that used the Web indirect manner, and works which applied a similarityor relatedness measure.2.1 SelectorsThe term selector comes from (Lin, 1997), andrefers to a word which can take the place of anothergiven word within the same local context.
Although28Lin searched a dependency relationship database inorder to match local context, it is not yet possible toparse dependency relationships of the entire Web.
Inturn, one must search for text as local context.
Forexample, in the sentence below, the local context for?strikers?
would be composed of ?he addressed the?and ?at the rally.
?.He addressed the strikers at the rally.Previously, we introduced the idea of using selec-tors of other words in a sentence in addition to se-lectors of the target, the word being disambiguated(Schwartz and Gomez, 2008).
Words taking theplace of a target word are referred to as target selec-tors and words which take the place of other wordsin a sentence are referred to as context selectors.Context selectors can be classified further based ontheir part of speech.
In our example, if ?striker?
wasthe target word, the verb context selectors would beverbs replacing ?addressed?
and the noun context se-lectors would be nouns replacing ?rally?.Similarity is used to measure the relationship be-tween a target word and its target selectors, whilerelatedness measures the relationship between a tar-get word and context selectors from other parts ofthe sentence.
Thus, the use of selectors in disam-biguating words relies on a couple assumptions:1.
Concepts which appear in matching syntacticconstructions are similar.2.
Concepts which appear in the context of a giventarget word are related to the correct sense ofthe target word.Note that ?concept?
and ?word sense?
are used in-terchangeably throughout this paper.
This idea ofdistinguishing similarity and relatedness has an ex-tensive history (Rada et al, 1989; Resnik, 1999; Pat-wardhan et al, 2003; Budanitsky and Hirst, 2006),but most algorithms only find a use for one or theother.2.2 Related Word Sense DisambiguationA key aspect of using selectors for disambiguation isthe inclusion of context in the Web search queries.This was done in works by (Martinez et al, 2006)and (Yuret, 2007), which substituted relatives orsimilar words in place of the target word within agiven context.
The context, restricted with a win-dow size, helped to limit the results from the Web.These works followed (Mihalcea and Moldovan,1999; Agirre et al, 2001) in that queries were con-structed through the use of a knowledge-base, fill-ing the queries with pre-chosen words.
We also usecontext in the web search, but we acquire wordsmatching a wildcard in the search rather than incor-porate a knowledge-base to construct queries withpre-chosen relatives.
Consequently, the later half ofour algorithm uses a knowledge-base through simi-larity and relatedness measures.Some recent works have used similarity or relat-edness measures to assist with WSD.
Particuarly,(Patwardhan et al, 2003) provide evaluations of var-ious relatedness measures for word sense disam-biguation based on words in context.
These evalu-ations helped us choose the similarity and related-ness measures to use in this work.
Other works,such as (Sinha and Mihalcea, 2007), use similar-ity or relatedness measures over graphs connectingwords within a sentence.
Likewise, (Navigli and Ve-lardi, 2005) analyze the connectivity of concepts ina sentence among Structured Semantic Interconnec-tions (SSI), graphs of relationships based on manyknowledge sources.
These works do not use selec-tors or the Web.
Additionally, target selectors andcontext selectors provide an application for the dis-tinction between similarity and relatedness not usedin these other methods.Several ideas distinguish this current work fromour research described in (Schwartz and Gomez,2008).
The most notable aspect is that we have gen-eralized the overall method of using Web selectorsinto disambiguating verbs, adverbs, and adjectivesin addition to nouns.
Another difference is the in-clusion of selectors for adverbs.
Finally, we also ex-plore the actual impact that each type of selector hason the performance of the disambiguation algorithm.3 ApproachIn this section we describe the Web Selector algo-rithm such that verbs, adjectives, and adverbs aredisambiguated in addition to nouns.
The algorithmessentially runs in two steps: acquisition of selectorsand application of selectors.293.1 Acquisition of SelectorsSelectors are acquired for all appropriate parts ofspeech.
Whether the selectors are used as targetselectors or context selectors depends on the targetword with which they are being applied.
Thus, oneprocess can be used to acquire all noun, verb, adjec-tive, and adverb selectors.
Additionally, noun selec-tors can be acquired for pronouns and proper nouns(referred to as ?pro?
selectors).
These are regularnouns found to replace a pronoun or proper nounwithin their local context.The first step in acquisition is to construct a querywith a wildcard in place of the target.
In our ex-ample, with ?address?
as the target, the query is ?he* the strikers at the rally.?
Yahoo!
Web Services1provides the functionality for searching the web forphrases with wildcards.
Selectors are extracted fromthe samples returned from the web search by match-ing the words which take the place of the wildcard.All words not found in WordNet under the samepart of speech as the target are thrown out as wellas phrases longer than 4 words or those containingpunctuation.The system enters a loop where it:?
searches the web with a given query, and?
extracts selectors from the web samples.The query is truncated and the search is repeated un-til a goal for the number of selectors was reachedor the query becomes too short.
This approach, de-tailed in (Schwartz and Gomez, 2008), removes se-lect punctuation, determiners, and gradually short-ens the query one word at a time.
Selectors retrievedfrom a larger query are removed from the results ofsmaller queries as the smaller queries should sub-sume the larger query results.
Some selectors re-trieved for the example, with their correspondingweb query are listed in Table 1.3.2 Similarity and RelatednessTo apply selectors in disambiguation, similarity andrelatedness measures are used to compare the selec-tors with the target word.
We incorporate the useof a few previously defined measures over WordNet(Miller et al, 1993).
The WordNet::Similarity pack-age provides a flexible implementation of many ofthese measures (Pedersen et al, 2004).
We config-ured WordNet::Similarity for WordNet version 2.1,1http://developer.yahoo.com/search/He addressed the * at the rallycrowd:1He addressed * at the rallystudent:1, supporter:2He addressed * at theCouncil:1, Muslim:1, Saturday:1, Ugandan:1,analyst:2, attendee:20, audience:3, class:2,consumer:1, council:1, delegate:64, diplomat:2,employee:2, engineer:1, fan:1, farmer:1,globalization:1, graduate:5, guest:2, hundred:3,investor:1, issue:1, journalist:9, lawmaker:11,legislator:1, member:6, midshipman:1,mourner:1, official:2, parliamentarian:1,participant:17, patient:1, physician:18,reporter:8, sailor:1, secretary:1, soldier:3,staff:3, student:20, supporter:8, thousand:3,today:2, trader:1, troops:2, visitor:1, worker:1He * the strikers at thetreat:2He * the strikers atget:1, keep:1, price:1, treat:1Table 1: Lists of selectors for the target words ?striker?and ?address?
returned by corresponding web queries.the same version used to annotate our chosen exper-imental corpus.A relatedness measure was used with context se-lectors, and we chose the adapted Lesk algorithm(Banerjee and Pedersen, 2002).
An important char-acteristic of this measure is that it can handle multi-ple parts of speech.
For target selectors we soughtto use measures over the WordNet ontology in orderto most closely measure similarity.
An information-content (IC) measure (Resnik, 1999) was used fortarget selectors of nouns and verbs.
However, be-cause IC measures do not work with all parts ofspeech, we used the adapted Lesk algorithm as anapproximation of similarity for adjectives and ad-verbs.
Note that finding the best relatedness or sim-ilarity measure was outside the scope of this paper.The following function, based on Resnik?s wordsimilarity (Resnik, 1999), is used to find the maxsimilarity or relatedness between a concept and aword (specifically between a sense of the targetword, ct and a selector, ws).maxsr(ct, ws) = maxcs?ws[meas(ct, cs)]where cs is a sense of the selector and meas is asimilarity or relatedness measure.30Figure 1: General flow in applying selectors to wordsense disambiguation.
Note that the target selectors maybe any part of speech.3.3 Application of SelectorsNext, we briefly describe the empirical basis forscoring senses of the target word.
This step is out-lined in Figure 1.
The occurrences of selectors canbe converted to a probability of a selector, ws ap-pearing in a web query, q:p(ws, q)The senses of the target word are compared witheach selector.
For a given sense of the target word,ct, the similarity or relatedness from a selector andquery is computed as:SR(ct, ws, q) = p(ws, q) ?maxsr(ct, ws)senses(ws)where senses(ws) is the number of senses of theselector.As the queries get shorter, the accuracy of the se-lectors becomes weaker.
In turn, the SR value fromselectors is scaled by a ratio of the web query length,wql, to the original sentence length, sl.
This scalingis applied when the SR values for one target wordsense are summed:sum(ct, T ) =?q?qs(T )?ws?sels(q)SR(ct, ws, q)?
wqlslwhere qs(T ) represents the set of queries for a selec-tor type, T , and ws ranges over all selectors foundwith q, denoted sels(q).The general approach of disambiguation is to findthe sense of a target word which is most similar to alltarget selectors and most related to all context selec-tors.
This follows our assumptions about selectorsgiven in the background section.
Thus, similarityand relatedness values from different selector types(represented as Types) must be combined.
By ag-gregating the normalized sums from all types of se-lectors, we get a combined similarity/relatedness fora given target word sense:CSR(ct) =?T?Typesscale(T ) ?
sum(ct, T )maxci?wt[sum(ci, T )]where wt represents the set of all senses belonging tothe target word, and scale(T ) is a coefficient used toweight each type of selector.
This term is importantin this work, because our experiments explore theimpact of various selector types.The top sense is then chosen by looking at theCSR of all senses.
For some situations, specificallywhen other senses have a score within 5% of thetop CSR, the difference between concepts is verysmall.
In these cases, the concept with the lowestsense number in WordNet is chosen from among thetop scoring senses.4 ExperimentsOur experiments are run over the SemEval2007 Task7: coarse-grained English all-words.
The sense in-ventory was created by mapping senses in WordNet2.1 to the Oxford Dictionary of English (Navigli etal., 2007).
The corpus was composed of five docu-ments with differing domains resulting in 2269 an-notated word instances.
Our system runs on fine-grained WordNet senses, but evaluation is done bychecking if the predicted fine-grained sense maps tothe correct coarse-grained sense.
Many issues as-sociated with fine-grained annotation, such as thosebrought up in (Ide and Wilks, 2006) are avoidedthrough the use of this corpus.First, we apply the generalized Web selector algo-rithm in a straight-forward manner to the entire task.Then, we delve into analyzing the acquired selectorsand the influence of each type of context selector inorder to gain insights into future related work.31BLRand MED WS BLMFS53.43 70.21 76.02 78.89Table 2: Results as F1 Values of our system, WS,compared with baselines: random, BLRand; most fre-quent sense, BLMFS ; median system performance at Se-mEval07, MED.UPV-WSD NUS-PT SSI78.63 82.50 83.21Table 3: Results as F1 Values of top performing systemsfor the SemEval07 Task07 (UPV = (Buscaldi and Rosso,2007), NUS-PT = (Chan et al, 2007), and SSI = a taskorganizer?s system (Navigli and Velardi, 2005)).4.1 Evaluating All WordsIn this section, we seek to apply the algorithm to allinstances of the testing corpus in order to comparewith baselines and other disambiguation algorithms.Unless stated otherwise, all results are presented asF1 values, where F1 = 2?
P?RP+R .
For SemEval2007,all systems performed better than the random base-line of 53.43%, but only 4 of 13 systems achievedan F1 score higher than the MFS baseline of 78.89%(Navigli et al, 2007).Table 2 lists the results of applying the general-ized Web selector algorithm described in this paperin a straight-forward manner, such that all scale(T )are set to 1.
We see that this version of the systemperforms better than the median system in the Se-mEval07 task, but it is a little below the MFS base-line.
A comparison with top systems is seen in Table3.
Our overall results were just below that of the topsystem not utilizing training data, (UPV-WSD (Bus-caldi and Rosso, 2007)), and a little over 6 percent-age points below the top supervised system (NUS-PT (Chan et al, 2007)).The results are broken down by part of speechin Table 4.
We see that adjective disambiguationwas the furthest above our median point of refer-ence, and noun disambiguation results were abovethe MFS baseline.
On the other hand, our adverbdisambiguation results appear weakest compared tothe baselines.
Note that we previously reported anoun sense disambiguation F1 value of 80.20% onthe same corpus (Schwartz and Gomez, 2008).
Cur-rent results differ because the previous work usedN V A RMED 70.76 62.10 71.55 74.04WS 78.52 68.36 81.21 75.48BLMFS 77.44 75.30 84.25 87.50insts 1108 591 362 208Table 4: Results as F1 values (precision = recall) of oursystem by parts of speech (N = noun, V = verb, A = ad-jective, R = adverb).
insts = disambiguation instances ofeach part of speech.
For other keys see Table 2.different scale(T ) values as well as a custom nounsimilarity measure.4.2 Selector Acquisition AnalysisWe examine the occurrences of acquired selectors.Listed as the column headings of Table 5, selectorsare acquired for five parts of speech (pro is actuallya combination of two parts of speech: pronoun andproper noun).
The data in Table 5 is based on re-sults from acquiring selectors for our experimentalcorpus.
The information presented includes:insts instances which the algorithm attemptsto acquire selectors% w/ sels percentage of instances for whichselectors were acquiredsels/inst average number of selectors for aninstance (over all insts)unique/inst average number of unique selectors foran instance (over all insts)insts/sent average instances in a sentencenoun verb adj.
adverb proinsts 1108 591 362 208 370% w/ sels 54.5 65.8 61.0 57.2 27.0sels/inst 36.5 51.2 29.5 17.7 15.9unique/inst 11.6 13.1 8.4 4.1 5.6insts/sent 4.5 2.4 1.5 0.8 1.5Table 5: Various statistics on the acquired selectors forthe SemEval07 Task 7 broken down by part of speech.Row descriptions are in the text.The selector acquisition data provides useful in-formation.
In general, % w/ sels was low from be-ing unable to find text on the Web matching localcontext (even with truncated queries).
The lowest% w/ sels, found for pro, was expected consider-ing only nouns which replace the original words are32used (pronouns acquired were thrown out since theyare not compatible with the relatedness measures).There was quite a variation in the sels/inst dependingon the type, and all of these numbers are well belowthe upper-bound of 200 selectors acquired before thealgorithm stops searching.
It turned out that only15.9% of the instances hit this mark.
This meansthat most instances stopped acquiring selectors be-cause they hit the minimum query length (5 words).In fact, the average web query to acquire at least oneselector had a length of 6.7 words, and the bulk ofselectors came from shorter queries (with less con-text from shorter queries, the selectors returned arenot as strong).
We refer to the combination of quan-tity and quality issues presented above, in general,as the quality selector sparsity problem.Although quality and quantity were not ideal,when one considers data from the sentence level,things are more optimistic.
The average sentencehad 10.7 instances (of any part of speech listed),so when certain selector types were missing, oth-ers were present.
As explained previously, the tar-get selector and context selector distinction is madeafter the acquisition of selectors.
Thus, each in-stance is used as both (exception: pro instances werenever used as target selectors since they were notdisambiguated) .
Employing this fact, more infor-mation can be discovered.
For example, the aver-age noun was disambiguated with 36.5 target selec-tors, 122.9 verb context selectors (51.2 sels/inst *2.4 insts/sent), 44.3 adjective context selectors, 14.2adverb context selectors, and 23.9 pro context se-lectors.
Still, with the bulk of those selectors com-ing from short queries, the reliability of the selectorswas not strong.4.3 Exploring the Influence of Selector TypesThis section explores the influence of each contextselector on the disambiguation algorithm, by chang-ing the value of scale(T ) in the previously listedCSR function.Examining Table 6 reveals precision resultswhen disambiguating instances with target selec-tors, based only on the target word?s similarity withtarget selectors.
This serves as a bearing for inter-preting results of context selector variation.We tested how well each type of context selec-tor complements the target selectors.
Accordingly,wsd prec.
% insts.N 64.08 348V 52.86 227A 77.36 106R 58.39 56Table 6: Precision when disambiguating with target se-lectors only.
All instances contain target selectors andmultiple senses in WordNet.
(insts.
= number of in-stances disambiguated.
)wsd noun verb adj.
adverb proN 272 186 120 84 108V 211 167 110 80 103A 97 78 50 40 34R 47 44 30 17 26Table 7: Instance occurrences used for disambiguationwhen experimenting with all types of context selectors(listed as columns).
The rows represent the four parts ofspeech disambiguated.scale(target) was set to 1, and scale(T ) for allother context types were set to 0.
In order to limit ex-ternal influences, we did not predict words with onlyone sense in WordNet or instances where the CSRwas zero (indicating no selectors).
Additionally, weonly tested on examples which had at least one tar-get selector and at least one selector of the specifictype being examined.
This restriction ensures we areavoiding some of the quality selector sparsity prob-lem described in the analysis.
Nevertheless, resultsare expected to be a little lower than our initial testsas we are ignoring other types of selectors and notincluding monosemous words according to Word-Net.
Table 7 lists the instance occurrences for eachof the four parts of speech that were disambiguated,based on these restrictions.Figures 2 through 5 show graphs of the precisionscore while increasing the influence of each contextselector type.
Each graph corresponds to the disam-biguation of a different part of speech, and each linein a graph represents one of the five types of contextselectors:1. noun context2.
verb context3.
adjective context4.
adverb context5.
pro context336264666870727476780.25 1 4 16scale(T ) valuenounverbadjectiveadverbproFigure 2: The noun sense disambiguation precision whenvarying the scale(T ) value for each type of context selec-tor.
scale(target) is always 1.The lines are formed with a Bezier curve algorithm2on the precision data.
The horizontal line representsthe precision of only using the target selectors to dis-ambiguate instances with target selectors.
Precisioneither decreases or remains the same if any graphline was extended past the right-most boundary.When examining the figures, one should notewhen the precision increases as the scale value in-creases.
This indicates that increases in influence ofthe particular type of context selector improved theresults.
The x-axis increases exponentially, since wewould like a ratio of scale(T ) to scale(target), andat x = 1 the context selector has the same influenceas the target selector.We see that all types of context selectors improvethe results for noun and verb sense disambiguation.Thus, our inclusion of adverb context selectors wasworthwhile.
It is difficult to draw a similar conclu-sion from the adverb and adjective disambiguationgraphs (Figures 4 and 5), although it still appearsthat the noun context selectors are helpful for bothand the pro context selectors are helpful for the ad-jective task.
We also note that most selector types2http://www.gnuplot.info/docs/node124.html4045505560650.25 1 4 16scale(T ) valuenounverbadjectiveadverbproFigure 3: The verb sense disambiguation precision whenvarying the scale(T ) value for each type of context se-lector.
scale(target) is 1.achieve highest precision above a scale value of 1,indicating that the context selector should have moreinfluence than the target selectors.
This is proba-bly due to the existence of more selectors from con-text than those from the target word.
The results ofadverb disambiguation should be taken lightly, be-cause there were not many disambiguation instancesthat fit the restrictions (see Table 7).4.4 Discussion of Future WorkBased on the results of our analysis and experiments,we list two avenues of future improvement:1.
Automatic Alternative Query Construction:This idea is concerned with the quality andquantity of selectors acquired for which thereis currently a trade-off.
As one shortens thequery to receive more quantity, the qualitygoes down due to a less accurate local context.One may be able to side-step this trade-off bysearching with alternative queries which cap-ture just as much local context.
For example,the query ?He * the strikers at the rally?
canbe mapped into the passive transformation ?thestrikers were * at the rally by him?.
Query3460626466687072747678800.25 1 4 16scale(T ) valuenounverbadjectiveadverbproFigure 4: The adjective sense disambiguation precisionwhen varying the scale(T ) value for each type of contextselector.
scale(target) is 1.reconstruction can be accomplished by usinga constituent-based parser, which will help toproduce syntactic alternations and other trans-formations such as the dative.2.
Improving Similarity and Relatedness: Nounsense disambiguation was the only subtask topass the MFS baseline.
One reason we suspectfor this is that work in similarity and related-ness has a longer history over nouns than overother parts of speech (Budanitsky and Hirst,2006).
Additionally, the hypernym (is-a) re-lationship of the noun ontology of WordNetcaptures the notion of similarity more clearlythan the primary relationships of other parts ofspeech in WordNet.
Accordingly, future workshould look into specific measures of similarityfor each part of speech, and further improve-ment to relatedness measures which functionaccross different parts of speech.
A subtle pieceof this type of work may find a way to effec-tively incorporate pronouns in the measures, al-lowing less selectors to be thrown out.354045505560650.25 1 4 16scale(T ) valuenounverbadjectiveadverbproFigure 5: The adverb sense disambiguation precisionwhen varying the scale(T ) value for each type of con-text selector.
scale(target) is 1.5 ConclusionWe found the use of Web selectors to be a worth-while approach to the disambiguation of other partsof speech in addition to nouns.
However, resultsfor verb, adjective, and adverb disambiguation wereslightly below the most frequent sense baseline, apoint which noun sense disambiguation overcomes.The use of this type of algorithm is still rich withavenues yet to be taken for improvement.Future work may address aspects at all levels ofthe algorithm.
To deal with a quality selector spar-sity problem, a system might automatically formalternative web queries utilizing a syntactic parser.Research may also look into defining similarity mea-sures for adjectives and adverbs, and refining thesimilarity measures for nouns and verbs.
Neverthe-less, without these promising future extensions thesystem still performs well, only topped by one otherminimally supervised system.6 AcknowledgementThis research was supported by the NASA Engi-neering and Safety Center under Grant/CooperativeAgreement NNX08AJ98A.35ReferencesEneko Agirre and David Martinez.
2004.
UnsupervisedWSD based on automatically retrieved examples: Theimportance of bias.
In Proceedings of EMNLP 2004,pages 25?32, Barcelona, Spain, July.Eneko Agirre, Olatz Ansa, and David Martinez.
2001.Enriching wordnet concepts with topic signatures.
InIn Proceedings of the NAACL workshop on WordNetand Other Lexical Resources: Applications, Exten-sions and Customizations, Pittsburg, USA.Satanjeev Banerjee and Ted Pedersen.
2002.
An adaptedlesk algorithm for word sense disambiguation usingwordnet.
In Proceedings of the Third InternationalConference on Intelligent Text Processing and Com-putational Linguistics.
Mexico City, Mexico.Alexander Budanitsky and Graeme Hirst.
2006.
Evalu-ating wordnet-based measures of lexical semantic re-latedness.
Computational Linguistics, 32(1):13?47.Davide Buscaldi and Paolo Rosso.
2007.
UPV-WSD :Combining different WSD methods by means of fuzzyborda voting.
In Proceedings of SemEval-2007, pages434?437, Prague, Czech Republic, June.Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong.
2007.NUS-PT: Exploiting parallel texts for word sense dis-ambiguation in the english all-words tasks.
In Pro-ceedings of Proceedings of SemEval-2007, pages 253?256, Prague, Czech Republic, June.Nancy Ide and Yorick Wilks, 2006.
Word Sense Dis-ambiguation: Algorithms And Applications, chapter 3:Making Sense About Sense.
Springer.Dekang Lin.
1997.
Using syntactic dependency as lo-cal context to resolve word sense ambiguity.
In Pro-ceedings of the 35th annual meeting on Association forComputational Linguistics, pages 64?71.David Martinez, Eneko Agirre, and Xinglong Wang.2006.
Word relatives in context for word sense dis-ambiguation.
In Proceedings of the 2006 AustralasianLanguage Technology Workshop, pages 42?50.Rada Mihalcea and Dan I. Moldovan.
1999.
An auto-matic method for generating sense tagged corpora.
InProceedings of AAAI-99, pages 461?466.Rada Mihalcea.
2002.
Bootstrapping large sense taggedcorpora.
In Proceedings of the 3rd InternationalConference on Languages Resources and EvaluationsLREC 2002, Las Palmas, Spain, May.George Miller, R. Beckwith, Christiane Fellbaum,D.
Gross, and K. Miller.
1993.
Five papers on word-net.
Technical report, Princeton University.Roberto Navigli and Paola Velardi.
2005.
Structuralsemantic interconnections: A knowledge-based ap-proach to word sense disambiguation.
IEEE Trans.Pattern Anal.
Mach.
Intell., 27(7):1075?1086.Roberto Navigli, Kenneth C. Litkowski, and Orin Har-graves.
2007.
Semeval-2007 task 07: Coarse-grainedenglish all-words task.
In Proceedings of SemEval-2007, pages 30?35, Prague, Czech Republic.
Associa-tion for Computational Linguistics.Siddharth Patwardhan, S. Banerjee, and T. Pedersen.2003.
Using Measures of Semantic Relatednessfor Word Sense Disambiguation.
In Proceedingsof the Fourth International Conference on IntelligentText Processing and Computational Linguistics, pages241?257, Mexico City, Mexico, February.Ted Pedersen, S. Patwardhan, and J. Michelizzi.
2004.WordNet::Similarity - Measuring the Relatedness ofConcepts.
In Human Language Technology Confer-ence of the NAACL Demonstrations, pages 38?41,Boston, MA, May.R.
Rada, H. Mili, E. Bicknell, and M. Blettner.
1989.Development and application of a metric on semanticnets.
In IEEE Transactions on Systems, Man and Cy-bernetics, volume 19, pages 17?30.Philip Resnik.
1999.
Semantic similarity in a taxonomy:An information-based measure and its application toproblems of ambiguity in natural language.
Journal ofArtificial Intelligence Research, 11:95?130.Hansen A. Schwartz and Fernando Gomez.
2008.
Ac-quiring knowledge from the web to be used as selec-tors for noun sense disambiguation.
In CoNLL 2008:Proceedings of the Twelfth Conference on Computa-tional Natural Language Learning, pages 105?112,Manchester, England, August.Ravi Sinha and Rada Mihalcea.
2007.
Unsupervisedgraph-based word sense disambiguation using mea-sures of word semantic similarity.
Irvine, CA, Septem-ber.Deniz Yuret.
2007.
KU: Word sense disambiguation bysubstitution.
In Proceedings of SemEval-2007, pages207?214, Prague, Czech Republic, June.36
