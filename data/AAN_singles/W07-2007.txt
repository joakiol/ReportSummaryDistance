Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 36?41,Prague, June 2007. c?2007 Association for Computational LinguisticsSemEval-2007 Task 08: Metonymy Resolution at SemEval-2007Katja MarkertSchool of ComputingUniversity of Leeds, UKmarkert@comp.leeds.ac.ukMalvina NissimDept.
of Linguistics and Oriental StudiesUniversity of Bologna, Italymalvina.nissim@unibo.itAbstractWe provide an overview of the metonymyresolution shared task organised withinSemEval-2007.
We describe the problem,the data provided to participants, and theevaluation measures we used to assess per-formance.
We also give an overview of thesystems that have taken part in the task, anddiscuss possible directions for future work.1 IntroductionBoth word sense disambiguation and named entityrecognition have benefited enormously from sharedtask evaluations, for example in the Senseval, MUCand CoNLL frameworks.
Similar campaigns havenot been developed for the resolution of figurativelanguage, such as metaphor, metonymy, idioms andirony.
However, resolution of figurative language isan important complement to and extension of wordsense disambiguation as it often deals with wordsenses that are not listed in the lexicon.
For exam-ple, the meaning of stopover in the sentence He sawteaching as a stopover on his way to bigger thingsis a metaphorical sense of the sense ?stopping placein a physical journey?, with the literal sense listedin WordNet 2.0 but the metaphorical one not beinglisted.1 The same holds for the metonymic readingof rattlesnake (for the animal?s meat) in Roast rat-tlesnake tastes like chicken.2 Again, the meat read-1This example was taken from the Berkely Master Metaphorlist (Lakoff and Johnson, 1980) .2From now on, all examples in this paper are taken from theBritish National Corpus (BNC) (Burnard, 1995), but Ex.
23.ing of rattlesnake is not listed in WordNet whereasthe meat reading for chicken is.As there is no common framework or corpus forfigurative language resolution, previous computa-tional works (Fass, 1997; Hobbs et al, 1993; Barn-den et al, 2003, among others) carry out only small-scale evaluations.
In recent years, there has beengrowing interest in metaphor and metonymy resolu-tion that is either corpus-based or evaluated on largerdatasets (Martin, 1994; Nissim and Markert, 2003;Mason, 2004; Peirsman, 2006; Birke and Sarkaar,2006; Krishnakamuran and Zhu, 2007).
Still, apartfrom (Nissim and Markert, 2003; Peirsman, 2006)who evaluate their work on the same dataset, resultsare hardly comparable as they all operate within dif-ferent frameworks.This situation motivated us to organise the firstshared task for figurative language, concentrating onmetonymy.
In metonymy one expression is used torefer to the referent of a related one, like the use ofan animal name for its meat.
Similarly, in Ex.
1,Vietnam, the name of a location, refers to an event (awar) that happened there.
(1) Sex, drugs, and Vietnam have haunted BillClinton?s campaign.In Ex.
2 and 3, BMW, the name of a company, standsfor its index on the stock market, or a vehicle manu-factured by BMW, respectively.
(2) BMW slipped 4p to 31p(3) His BMW went on to race at Le MansThe importance of resolving metonymies has beenshown for a variety of NLP tasks, such as ma-36chine translation (Kamei and Wakao, 1992), ques-tion answering (Stallard, 1993), anaphora resolution(Harabagiu, 1998; Markert and Hahn, 2002) andgeographical information retrieval (Leveling andHartrumpf, 2006).Although metonymic readings are, like all figu-rative readings, potentially open ended and can beinnovative, the regularity of usage for word groupshelps in establishing a common evaluation frame-work.
Many other location names, for instance, canbe used in the same fashion as Vietnam in Ex.
1.Thus, given a semantic class (e.g.
location), onecan specify several regular metonymic patterns (e.g.place-for-event) that instances of the class are likelyto undergo.
In addition to literal readings, regu-lar metonymic patterns and innovative metonymicreadings, there can also be so-called mixed read-ings, similar to zeugma, where both a literal and ametonymic reading are evoked (Nunberg, 1995).The metonymy task is a lexical sample task forEnglish, consisting of two subtasks, one concentrat-ing on the semantic class location, exemplified bycountry names, and another one concentrating on or-ganisation, exemplified by company names.
Partici-pants had to automatically classify preselected coun-try/company names as having a literal or non-literalmeaning, given a four-sentence context.
Addition-ally, participants could attempt finer-grained inter-pretations, further specifying readings into prespec-ified metonymic patterns (such as place-for-event)and recognising innovative readings.2 Annotation CategoriesWe distinguish between literal, metonymic, andmixed readings for locations and organisations.
Inthe case of a metonymic reading, we also specifythe actual patterns.
The annotation categories weremotivated by prior linguistic research by ourselves(Markert and Nissim, 2006), and others (Fass, 1997;Lakoff and Johnson, 1980).2.1 LocationsLiteral readings for locations comprise locative(Ex.
4) and political entity interpretations (Ex.
5).
(4) coral coast of Papua New Guinea.
(5) Britain?s current account deficit.Metonymic readings encompass four types:- place-for-people a place stands for any per-sons/organisations associated with it.
These can begovernments (Ex.
6), affiliated organisations, incl.sports teams (Ex.
7), or the whole population (Ex.
8).Often, the referent is underspecified (Ex.
9).
(6) America did once try to ban alcohol.
(7) England lost in the semi-final.
(8) [.
.
. ]
the incarnation was to fulfil thepromise to Israel and to reconcile the worldwith God.
(9) The G-24 group expressed readiness to pro-vide Albania with food aid.- place-for-event a location name stands for anevent that happened in the location (see Ex.
1).- place-for-product a place stands for a productmanufactured in the place, as Bordeaux in Ex.
10.
(10) a smooth Bordeaux that was gutsy enoughto cope with our food- othermet a metonymy that does not fall into anyof the prespecified patterns, as in Ex.
11, where NewJersey refers to typical local tunes.
(11) The thing about the record is the influ-ences of the music.
The bottom end is veryNew York/New Jersey and the top is verymelodic.When two predicates are involved, triggering a dif-ferent reading each (Nunberg, 1995), the annotationcategory is mixed.
In Ex.
12, both a literal and aplace-for-people reading are involved.
(12) they arrived in Nigeria, hitherto a leadingcritic of [.
.
.
]2.2 OrganisationsThe literal reading for organisation names describesreferences to the organisation in general, where anorganisation is seen as a legal entity, which consistsof organisation members that speak with a collec-tive voice, and which has a charter, statute or definedaims.
Examples of literal readings include (amongothers) descriptions of the structure of an organisa-tion (see Ex.
13), associations between organisations(see Ex.
14) or relations between organisations andproducts/services they offer (see Ex.
15).37(13) NATO countries(14) Sun acquired that part of Eastman-KodakCos Unix subsidary(15) Intel?s Indeo video compression hardwareMetonymic readings include six types:- org-for-members an organisation stands forits members, such as a spokesperson or official(Ex.
16), or all its employees, as in Ex.
17.
(16) Last February IBM announced [.
.
.
](17) It?s customary to go to work in black orwhite suits.
[.
.
. ]
Woolworths wear them- org-for-event an organisation name is used to re-fer to an event associated with the organisation (e.g.a scandal or bankruptcy), as in Ex.
18.
(18) the resignation of Leon Brittan from Tradeand Industry in the aftermath of Westland.- org-for-product the name of a commercial or-ganisation can refer to its products, as in Ex.
3.- org-for-facility organisations can also stand forthe facility that houses the organisation or one of itsbranches, as in the following example.
(19) The opening of a McDonald?s is a majorevent- org-for-index an organisation name can be usedfor an index that indicates its value (see Ex.
2).- othermet a metonymy that does not fall into anyof the prespecified patterns, as in Ex.
20, where Bar-clays Bank stands for an account at the bank.
(20) funds [.
.
. ]
had been paid into BarclaysBank.Mixed readings exist for organisations as well.In Ex.
21, both an org-for-index and an org-for-members pattern are invoked.
(21) Barclays slipped 4p to 351p after confirm-ing 3,000 more job losses.2.3 Class-independent categoriesApart from class-specific metonymic readings, somepatterns seem to apply across classes to all names.
Inthe SemEval dataset, we annotated two of them.object-for-name all names can be used as meresignifiers, instead of referring to an object or set ofobjects.
In Ex.
22, both Chevrolet and Ford are usedas strings, rather than referring to the companies.
(22) Chevrolet is feminine because of its sound(it?s a longer word than Ford, has an openvowel at the end, connotes Frenchness).object-for-representation a name can refer to arepresentation (such as a photo or painting) of thereferent of its literal reading.
In Ex.
23, Malta refersto a drawing of the island when pointing to a map.
(23) This is Malta3 Data Collection and AnnotationWe used the CIA Factbook3 and the Fortune 500list as sampling frames for country and companynames respectively.
All occurrences (including plu-ral forms) of all names in the sampling frames wereextracted in context from all texts of the BNC, Ver-sion 1.0.
All samples extracted are coded in XMLand contain up to four sentences: the sentence inwhich the country/company name occurs, two be-fore, and one after.
If the name occurs at the begin-ning or end of a text the samples may contain lessthan four sentences.For both the location and the organisation subtask,two random subsets of the extracted samples wereselected as training and test set, respectively.
Beforemetonymy annotation, samples that were not under-stood by the annotators because of insufficient con-text were removed from the datsets.
In addition, asample was also removed if the name extracted wasa homonym not in the desired semantic class (for ex-ample Mr. Greenland when annotating locations).4For those names that do have the semantic classlocation or organisation, metonymy anno-tation was performed, using the categories describedin Section 2.
All training set annotation was carriedout independently by both organisers.
Annotationwas highly reliable with a kappa (Carletta, 1996) of3https://www.cia.gov/cia/publications/factbook/index.html4Given that the task is not about standard Named EntityRecognition, we assume that the general semantic class of thename is already known.38Table 1: Reading distribution for locationsreading train testliteral 737 721mixed 15 20othermet 9 11obj-for-name 0 4obj-for-representation 0 0place-for-people 161 141place-for-event 3 10place-for-product 0 1total 925 908Table 2: Reading distribution for organisationsreading train testliteral 690 520mixed 59 60othermet 14 8obj-for-name 8 6obj-for-representation 1 0org-for-members 220 161org-for-event 2 1org-for-product 74 67org-for-facility 15 16org-for-index 7 3total 1090 842.88/.89 for locations/organisations.5 As agreementwas established, annotation of the test set was car-ried out by the first organiser.
All cases which werenot entirely straightforward were then independentlychecked by the second organiser.
Samples whosereadings could not be agreed on (after a reconcil-iation phase) were excluded from both training andtest set.
The reading distributions of training and testsets for both subtasks are shown in Tables 1 and 2.In addition to a simple text format including onlythe metonymy annotation, we provided participantswith several linguistic annotations of both trainingand testset.
This included the original BNC tokeni-sation and part-of-speech tags as well as manuallyannotated dependency relations for each annotatedname (e.g.
BMW subj-of-slip for Ex.
2).4 Submission and EvaluationTeams were allowed to participate in the locationor organisation task or both.
We encouraged super-vised, semi-supervised or unsupervised approaches.Systems could be tailored to recognisemetonymies at three different levels of granu-5The training sets are part of the already available Mascaracorpus for metonymy (Markert and Nissim, 2006).
The test setswere newly created for SemEval.larity: coarse, medium, or fine, with an increasingnumber and specification of target classificationcategories, and thus difficulty.
At the coarse level,only a distinction between literal and non-literal wasasked for; medium asked for a distinction betweenliteral, metonymic and mixed readings; fine neededa classification into literal readings, mixed readings,any of the class-dependent and class-independentmetonymic patterns (Section 2) or an innovativemetonymic reading (category othermet).Systems were evaluated via accuracy (acc) andcoverage (cov), allowing for partial submissions.acc = # correct predictions# predictions cov =# predictions# samplesFor each target category c we also measured:precisionc = # correct assignments of c# assignments of crecallc = # correct assignments of c# dataset instances of cfscorec = 2precisioncrecallcprecisionc+recallcA baseline, consisting of the assignment of the mostfrequent category (always literal), was used for eachtask and granularity level.5 Systems and ResultsWe received five submissions (FUH, GYDER,up13, UTD-HLT-CG, XRCE-M).
All tackledthe location task; three (GYDER, UTD-HLT-CG,XRCE-M) also participated in the organisation task.All systems were full submissions (coverage of 1)and participated at all granularity levels.5.1 Methods and FeaturesOut of five teams, four (FUH, GYDER, up13,UTD-HLT-CG) used supervised machine learning,including single (FUH,GYDER, up13) as wellas multiple classifiers (UTD-HLT-CG).
A rangeof learning paradigms was represented (includinginstance-based learning, maximum entropy, deci-sion trees, etc.).
One participant (XRCE-M) built ahybrid system, combining a symbolic, supervisedapproach based on deep parsing with an unsuper-vised distributional approach exploiting lexical in-formation obtained from large corpora.Systems up13 and FUH used mostly shallow fea-tures extracted directly from the training data (in-cluding parts-of-speech, co-occurrences and collo-39cations).
The other systems made also use of syn-tactic/grammatical features (syntactic roles, deter-mination, morphology etc.).
Two of them (GYDERand UTD-HLT-CG) exploited the manually anno-tated grammatical roles provided by the organisers.All systems apart from up13 made use of exter-nal knowledge resources such as lexical databasesfor feature generalisation (WordNet, FrameNet,VerbNet, Levin verb classes) as well as other cor-pora (the Mascara corpus for additional training ma-terial, the BNC, and the Web).5.2 PerformanceTables 3 and 4 report accuracy for all systems.6 Ta-ble 5 provides a summary of the results with lowest,highest, and average accuracy and f-scores for eachsubtask and granularity level.7The task seemed extremely difficult, with 2 of the5 systems (up13,FUH) participating in the locationtask not beating the baseline.
These two systems re-lied mainly on shallow features with limited or nouse of external resources, thus suggesting that thesefeatures might only be of limited use for identify-ing metonymic shifts.
The organisers themselveshave come to similar conclusions in their own ex-periments (Markert and Nissim, 2002).
The sys-tems using syntactic/grammatical features (GYDER,UTD-HLT-CG, XRCE-M) could improve over thebaseline whether using manual annotation or pars-ing.
These systems also made heavy use of featuregeneralisation.
Classification granularity had only asmall effect on system performance.Only few of the fine-grained categories could bedistinguished with reasonable success (see the f-scores in Table 5).
These include literal readings,and place-for-people, org-for-members, and org-for-product metonymies, which are the most frequentcategories (see Tables 1 and 2).
Rarer metonymictargets were either not assigned by the systemsat all (?undef?
in Table 5) or assigned wrongly6Due to space limitations we do not report precision, recall,and f-score per class and refer the reader to each system de-scription provided within this volume.7The value ?undef?
is used for cases where the system didnot attempt any assignment for a given class, whereas the value?0?
signals that assignments were done, but were not correct.8Please note that results for the FUH system are slightly dif-ferent than those presented in the FUH system description pa-per.
This is due to a preprocessing problem in the FUH systemthat was fixed only after the run submission deadline.Table 5: Overview of scoresbase min max aveLOCATION-coarseaccuracy 0.794 0.754 0.852 0.815literal-f 0.849 0.912 0.888non-literal-f 0.344 0.576 0.472LOCATION-mediumaccuracy 0.794 0.750 0.848 0.812literal-f 0.849 0.912 0.889metonymic-f 0.331 0.580 0.476mixed-f 0.000 0.083 0.017LOCATION-fineaccuracy 0.794 0.741 0.844 0.801literal-f 0.849 0.912 0.887place-for-people-f 0.308 0.589 0.456place-for-event-f 0.000 0.167 0.033place-for-product-f 0.000 undef 0.000obj-for-name-f 0.000 0.667 0.133obj-for-rep-f undef undef undefothermet-f 0.000 undef 0.000mixed-f 0.000 0.083 0.017ORGANISATION-coarseaccuracy 0.618 0.732 0.767 0.746literal-f 0.800 0.825 0.810non-literal-f 0.572 0.652 0.615ORGANISATION-mediumaccuracy 0.618 0.711 0.733 0.718literal-f 0.804 0.825 0.814metonymic-f 0.553 0.604 0.577mixed-f 0.000 0.308 0.163ORGANISATION-fineaccuracy 0.618 0.700 0.728 0.713literal-f 0.808 0.826 0.817org-for-members-f 0.568 0.630 0.608org-for-event-f 0.000 undef 0.000org-for-product-f 0.400 0.500 0.458org-for-facility-f 0.000 0.222 0.141org-for-index-f 0.000 undef 0.000obj-for-name-f 0.250 0.800 0.592obj-for-rep-f undef undef undefothermet-f 0.000 undef 0.000mixed-f 0.000 0.343 0.135(low f-scores).
An exception is the object-for-name pattern, which XRCE-M and UTD-HLT-CGcould distinguish with good success.
Mixed read-ings also proved problematic since more than onepattern is involved, thus limiting the possibilitiesof learning from a single training instance.
OnlyGYDER succeeded in correctly identifiying a varietyof mixed readings in the organisation subtask.
Nosystems could identify unconventional metonymiescorrectly.
Such poor performance is due to the non-regularity of the reading by definition, so that ap-proaches based on learning from similar examplesalone cannot work too well.40Table 3: Accuracy scores for all systems for all the location tasks.8task ?
/ system ?
baseline FUH UTD-HLT-CG XRCE-M GYDER up13LOCATION-coarse 0.794 0.778 0.841 0.851 0.852 0.754LOCATION-medium 0.794 0.772 0.840 0.848 0.848 0.750LOCATION-fine 0.794 0.759 0.822 0.841 0.844 0.741Table 4: Accuracy scores for all systems for all the organisation taskstask ?
/ system ?
baseline UTD-HLT-CG XRCE-M GYDERORGANISATION-coarse 0.618 0.739 0.732 0.767ORGANISATION-medium 0.618 0.711 0.711 0.733ORGANISATION-fine 0.618 0.711 0.700 0.7286 Concluding RemarksThere is a wide range of opportunities for future fig-urative language resolution tasks.
In the SemEvalcorpus the reading distribution mirrored the actualdistribution in the original corpus (BNC).
Althoughrealistic, this led to little training data for severalphenomena.
A future option, geared entirely to-wards system improvement, would be to use a strat-ified corpus, built with different acquisition strate-gies like active learning or specialised search proce-dures.
There are also several options for expand-ing the scope of the task, for example to a widerrange of semantic classes, from proper names tocommon nouns, and from lexical samples to an all-words task.
In addition, our task currently coversonly metonymies and could be extended to otherkinds of figurative language.AcknowledgementsWe are very grateful to the BNC Consortium for let-ting us use and distribute samples from the BritishNational Corpus, version 1.0.ReferencesJ.A.
Barnden, S.R.
Glasbey, M.G.
Lee, and A.M. Walling-ton.
2003.
Domain-transcending mappings in a system formetaphorical reasoning.
In Proc.
of EACL-2003, 57-61.J.
Birke and A Sarkaar.
2006.
A clustering approach for thenearly unsupervised recognition of nonliteral language.
InProc.
of EACL-2006.L.
Burnard, 1995.
Users?
Reference Guide, British NationalCorpus.
BNC Consortium, Oxford, England.J.
Carletta.
1996.
Assessing agreement on classification tasks:The kappa statistic.
Computational Linguistics, 22:249-254.D.
Fass.
1997.
Processing Metaphor and Metonymy.
Ablex,Stanford, CA.S.
Harabagiu.
1998.
Deriving metonymic coercions fromWordNet.
In Workshop on the Usage of WordNet in NaturalLanguage Processing Systems, COLING-ACL ?98, 142-148,Montreal, Canada.J.R.
Hobbs, M.E.
Stickel, D.E.
Appelt, and P. Martin.
1993.Interpretation as abduction.
Artificial Intelligence, 63:69-142.S.
Kamei and T. Wakao.
1992.
Metonymy: Reassessment, sur-vey of acceptability and its treatment in machine translationsystems.
In Proc.
of ACL-92, 309-311.S.
Krishnakamuran and X. Zhu.
2007.
Hunting elusivemetaphors using lexical resources.
In NAACL 2007 Work-shop on Computational Approaches to Figurative Language.G.
Lakoff and M. Johnson.
1980.
Metaphors We Live By.Chicago University Press, Chicago, Ill.J.
Leveling and S. Hartrumpf.
2006.
On metonymy recogni-tion for gir.
In Proceedings of GIR-2006: 3rd Workshop onGeographical Information Retrieval.K.
Markert and U. Hahn.
2002.
Understanding metonymies indiscourse.
Artificial Intelligence, 135(1/2):145?198.K.
Markert and M. Nissim.
2002.
Metonymy resolution as aclassification task.
In Proc.
of EMNLP-2002, 204-213.K.
Markert and M. Nissim.
2006.
Metonymic proper names: Acorpus-based account.
In A. Stefanowitsch, editor, Corporain Cognitive Linguistics.
Vol.
1: Metaphor and Metonymy.Mouton de Gruyter, 2006.J.
Martin.
1994.
Metabank: a knowledge base ofmetaphoric language conventions.
Computational Intelli-gence, 10(2):134-149.Z.
Mason.
2004.
Cormet: A computational corpus-based con-ventional metaphor extraction system.
Computational Lin-guistics, 30(1):23-44.M.
Nissim and K. Markert.
2003.
Syntactic features and wordsimilarity for supervised metonymy resolution.
In Proc.
ofACL-2003, 56-63.G.
Nunberg.
1995.
Transfers of meaning.
Journal of Seman-tics, 12:109-132.Y Peirsman.
2006.
Example-based metonymy recognition forproper nouns.
In Student Session of EACL 2006.D.
Stallard.
1993.
Two kinds of metonymy.
In Proc.
of ACL-93, 87-94.41
