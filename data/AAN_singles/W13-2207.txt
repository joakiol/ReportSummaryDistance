Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 85?91,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsCUni Multilingual Matrix in the WMT 2013 Shared TaskKarel B?lek Daniel ZemanCharles University in Prague, Faculty of Mathematics and Physics,Institute of Formal and Applied LinguisticsMalostransk?
n?m?st?
25, CZ-11800 Praha, Czechiakb@karelbilek.com, zeman@ufal.mff.cuni.czAbstractWe describe our experiments withphrase-based machine translation forthe WMT 2013 Shared Task.
Wetrained one system for 18 translationdirections between English or Czechon one side and English, Czech, Ger-man, Spanish, French or Russian onthe other side.
We describe a set of re-sults with different training data sizesand subsets.
For the pairs containingRussian, we describe a set of indepen-dent experiments with slightly differenttranslation models.1 IntroductionWith so many official languages, Europe isa paradise for machine translation research.One of the largest bodies of electronicallyavailable parallel texts is being nowadays gen-erated by the European Union and its insti-tutions.
At the same time, the EU also pro-vides motivation and boosts potential marketfor machine translation outcomes.Most of the major European languages be-long to one of three branches of the Indo-European language family: Germanic, Ro-mance or Slavic.
Such relatedness is respon-sible for many structural similarities in Eu-ropean languages, although significant differ-ences still exist.
Within the language portfo-lio selected for the WMT shared task, English,French and Spanish seem to be closer to eachother than to the rest.German, despite being genetically relatedto English, differs in many properties.
Itsword order rules, shifting verbs from oneend of the sentence to the other, easily cre-ate long-distance dependencies.
Long Ger-man compound words are notorious for in-creasing out-of-vocabulary rate, which hasled many researchers to devising unsupervisedcompound-splitting techniques.
Also, upper-case/lowercase distinction is more importantbecause all German nouns start with an up-percase letter by the rule.Czech is a language with rich morphology(both inflectional and derivational) and rela-tively free word order.
In fact, the predicate-argument structure, often encoded by fixedword order in English, is usually captured byinflection (especially the system of 7 grammat-ical cases) in Czech.
While the free word orderof Czech is a problem when translating to En-glish (the text should be parsed first in orderto determine the syntactic functions and theEnglish word order), generating correct inflec-tional affixes is indeed a challenge for English-to-Czech systems.
Furthermore, the multitudeof possible Czech word forms (at least order ofmagnitude higher than in English) makes thedata sparseness problem really severe, hinder-ing both directions.Most of the above characteristics of Czechalso apply to Russian, another Slavic language.Similar issues have to be expected when trans-lating between Russian and English.
Still,there are also interesting divergences betweenRussian and Czech, especially on the syntacticlevel.
Russian sentences typically omit cop-ula in the present tense and there is also nodirect equivalent of the verb ?to have?.
Pe-riphrastic constructions such as ?there is XXXby him?
are used instead.
These differencesmake the Czech-Russian translation interest-85ing as well.
Interestingly enough, results ofmachine translation between Czech and Rus-sian has so far been worse than between En-glish and any of the two languages, languagerelatedness notwithstanding.Our goal is to run one system under assimilar conditions as possible to all eighteentranslation directions, to compare their trans-lation accuracies and see why some directionsare easier than others.
The current version ofthe system does not include really language-specific techniques: we neither split Germancompounds, nor do we address the peculiari-ties of Czech and Russian mentioned above.In an independent set of experiments, wetried to deal with the data sparseness of Rus-sian language with the addition of a backoffmodel with a simple stemming and some ad-ditional data; those experiments were done forRussian and Czech|English combinations.2 The Translation SystemBoth sets of experiments use the same ba-sic framework.
The translation system isbuilt around Moses1 (Koehn et al 2007).Two-way word alignment was computed us-ing GIZA++2 (Och and Ney, 2003), andalignment symmetrization using the grow-diag-final-and heuristic (Koehn et al 2003).Weights of the system were optimized usingMERT (Och, 2003).
No lexical reorderingmodel was trained.For language modeling we use the SRILMtoolkit3 (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chenand Goodman, 1998).3 General experimentsIn the first set of experiments we wanted touse the same setting for all language pairs.3.1 Data and Pre-processing PipelineWe applied our system to all the ten officiallanguage pairs.
In addition, we also exper-imented with translation between Czech onone side and German, Spanish, French or Rus-sian on the other side.
Training data forthese additional language pairs were obtained1http://www.statmt.org/moses/2http://code.google.com/p/giza-pp/3http://www-speech.sri.com/projects/srilm/by combining parallel corpora of the officiallysupported pairs.
For instance, to create theCzech-German parallel corpus, we identifiedthe intersection of the English sides of Czech-English and English-German corpora, respec-tively; then we combined the correspondingCzech and German sentences.We took part in the constrained task.
Un-less explicitly stated otherwise, the transla-tion model in our experiments was trained onthe combined News-Commentary v8 and Eu-roparl v7 corpora.4 Note that there is onlyNews Commentary and no Europarl for Rus-sian.
We were also able to evaluate severalcombinations with large parallel corpora: theUN corpus (English, French and Spanish),the Giga French-English corpus and CzEng(Czech-English).
We did not use any largecorpus for Russian-English.
Table 1 shows thesizes of the training data.Corpus SentPairs Tkns lng1 Tkns lng2cs-en 786,929 18,196,080 21,184,881de-en 2,098,430 55,791,641 58,403,756es-en 2,140,175 62,444,507 59,811,355fr-en 2,164,891 70,363,304 60,583,967ru-en 150,217 3,889,215 4,100,148de-cs 657,539 18,160,857 17,788,600es-cs 697,898 19,577,329 18,926,839fr-cs 693,093 19,717,885 18,849,244ru-cs 103,931 2,642,772 2,319,611Czengcs-en 14,833,358 204,837,216 235,177,231UNes-en 11,196,913 368,154,702 328,840,003fr-en 12,886,831 449,279,647 372,627,886Gigafr-en 22,520,400 854,353,231 694,394,577Table 1: Number of sentence pairs and tokensfor every language pair in the parallel trainingcorpus.
Languages are identified by their ISO639 codes: cs = Czech, de = German, en =English, es = Spanish, fr = French, ru = Rus-sian.
Every line corresponds to the respectiveversion of EuroParl + News Commentary; thesecond part presents the extra corpora.The News Test 2010 (2489 sentences ineach language) and 2012 (3003 sentences)data sets5 were used as development data forMERT.
BLEU scores reported in this paperwere computed on the News Test 2013 set4http://www.statmt.org/wmt13/translation-task.html\#download5http://www.statmt.org/wmt13/translation-task.html86(3000 sentences each language).
We do notuse the News Tests 2008, 2009 and 2011.All parallel and monolingual corpora un-derwent the same preprocessing.
They weretokenized and some characters normalizedor cleaned.
A set of language-dependentheuristics was applied in an attempt to re-store the opening/closing quotation marks (i.e.
"quoted" ?
?quoted?)
(Zeman, 2012).The data are then tagged and lemmatized.We used the Featurama tagger for Czechand English lemmatization and TreeTagger forGerman, Spanish, French and Russian lemma-tization.
All these tools are embedded in theTreex analysis framework (?abokrtsk?
et al2008).The lemmas are used later to compute wordalignment.
Besides, they are needed to ap-ply ?supervised truecasing?
to the data: wecast the case of the lemma to the form, rely-ing on our morphological analyzers and tag-gers to identify proper names, all other wordsare lowercased.
Note that guessing of the truecase is only needed for the sentence-initial to-ken.
Other words can typically be left in theiroriginal form, unless they are uppercased as aform of HIGHLIGHTING.3.2 ExperimentsBLEU scores were computed by our sys-tem, comparing truecased tokenized hypoth-esis with truecased tokenized reference trans-lation.
Such scores must differ from the officialevaluation?see Section 3.2.4 for discussion ofthe final results.The confidence interval for most of thescores lies between ?0.5 and ?0.6 BLEU %points.3.2.1 Baseline ExperimentsThe set of baseline experiments were trainedon the supervised truecased combination ofNews Commentary and Europarl.
As we hadlemmatizers for the languages, word alignmentwas computed on lemmas.
(But our previousexperiments showed that there was little dif-ference between using lemmas and lowercased4-character ?stems?.)
A hexagram languagemodel was trained on the monolingual versionof the News Commentary + Europarl corpus(typically a slightly larger superset of the tar-get side of the parallel corpus).3.2.2 Larger Monolingual DataBesides the monolingual halves of the par-allel corpora, additional monolingual datawere provided / permitted.
Our experimentsin previous years clearly showed that theCrawled News corpus (2007?2012), in-domainand large, contributed significantly to betterBLEU scores.
This year we included it inour baseline experiments for all language pairs:translation model on News Commentary +Europarl, language model on monolingual partof the two, plus Crawled News.In addition there are the Gigaword corporapublished by the Linguistic Data Consortium,available only for English (5th edition), Span-ish (3rd) and French (3rd).
Table 2 givesthe sizes and Table 3 compares BLEU scoreswith Gigaword against the baseline.
Gigawordmainly contains texts from news agencies andas such it should be also in-domain.
Neverthe-less, the crawled news are already so large thatthe improvement contributed by Gigaword israrely significant.Corpus Segments Tokensnewsc+euro.cs 830,904 18,862,626newsc+euro.de 2,380,813 59,350,113newsc+euro.en 2,466,167 67,033,745newsc+euro.es 2,330,369 66,928,157newsc+euro.fr 2,384,293 74,962,162newsc.ru 183,083 4,340,275news.all.cs 27,540,827 460,356,173news.all.de 54,619,789 1,020,852,354news.all.en 68,341,615 1,673,187,787news.all.es 13,384,314 388,614,890news.all.fr 21,195,476 557,431,929news.all.ru 19,912,911 361,026,791gigaword.en 117,905,755 4,418,360,239gigaword.es 31,304,148 1,064,660,498gigaword.fr 21,674,453 963,571,174Table 2: Number of segments (paragraphsin Gigaword, sentences elsewhere) and tokensof additional monolingual training corpora.?newsc+euro?
are the monolingual versions ofthe News Commentary and Europarl parallelcorpora.
?news.all?
denotes all years of theCrawled News corpus for the given language.87Direction Baseline Gigaworden-cs 0.1632en-de 0.1833en-es 0.2808 0.2856en-fr 0.2987 0.2988en-ru 0.1582cs-en 0.2328 0.2367de-en 0.2389 0.2436es-en 0.2916 0.2975fr-en 0.2887ru-en 0.1975 0.2003cs-de 0.1595cs-es 0.2170 0.2220cs-fr 0.2220 0.2196cs-ru 0.1660de-cs 0.1488es-cs 0.1580fr-cs 0.1420ru-cs 0.1506Table 3: BLEU scores of the baseline experi-ments (left column) on News Test 2013 data,computed by the system on tokenized data,versus similar setup with Gigaword.
The dif-ference was typically not significant.3.2.3 Larger Parallel DataVarious combinations with larger parallel cor-pora were also tested.
We do not have resultsfor all combinations because these experimentsneeded a lot of time and resources and not allof them finished in time successfully.In general the UN corpus seems to be of lowquality or too much off-domain.
It may helpa little if used in combination with news-euro.If used separately, it always hurts the results.The Giga French-English corpus gave thebest results for English-French as expected,even without the core news-euro data.
How-ever, training the model on data of this size isextremely demanding on memory and time.Finally, Czeng undoubtedly improvesCzech-English translation in both directions.The news-euro dataset is smaller for thislanguage pair, which makes Czeng stand outeven more.
See Table 4 for details.3.2.4 Final ResultsTable 5 compares our BLEU scores with thosecomputed at matrix.statmt.org.BLEU (without flag) denotes BLEU scoreDir Parallel Mono BLEUen-es news-euro +gigaword 0.2856en-es news-euro-un +gigaword 0.2844en-es un un+gigaw.
0.2016en-fr giga +gigaword 0.3106en-fr giga +newsall 0.3037en-fr news-euro-un +gigaword 0.3010en-fr news-euro +gigaword 0.2988en-fr un un 0.2933es-en news-euro +gigaword 0.2975es-en news-euro-un baseline 0.2845es-en un un+news 0.2067fr-en news-euro-un +gigaword 0.2914fr-en news-euro baseline 0.2887fr-en un un+news 0.2737Table 4: BLEU scores with different parallelcorpora.computed by our system, comparing truecasedtokenized hypothesis with truecased tokenizedreference translation.The official evaluation by matrix.statmt.org gives typically lower numbers, reflectingthe loss caused by detokenization and new(different) tokenization.3.2.5 EfficiencyThe baseline experiments were conductedmostly on 64bit AMD Opteron quad-core2.8 GHz CPUs with 32 GB RAM (decodingrun on 15 machines in parallel) and the wholepipeline typically required between a half anda whole day.However, we used machines with up to500 GB RAM to train the large language mod-els and translation models.
Aligning the UNcorpora with Giza++ took around 5 days.Giga French-English corpus was even worseand required several weeks to complete.
Us-ing such a large corpus without pruning is notpractical.4 Extra Experiments with RussianIn a separate set of experiments, we tried totake a basic Moses framework and change thesetup a little for better results on morpholog-ically rich languages.Tried combinations were Russian-Czech andRussian-English.88Direction BLEU BLEUl BLEUten-cs 0.1786 0.180 0.170en-de 0.1833 0.179 0.173en-es 0.2856 0.288 0.271en-fr 0.3010 0.270 0.259en-ru 0.1582 0.142 0.142cs-en 0.2527 0.259 0.244de-en 0.2389 0.244 0.230es-en 0.2856 0.288 0.271fr-en 0.2887 0.294 0.280ru-en 0.1975 0.203 0.191cs-de 0.1595 0.159 0.151cs-es 0.2220 0.225 0.210cs-fr 0.2220 0.191 0.181cs-ru 0.1660 0.150 0.149de-cs 0.1488 0.151 0.142es-cs 0.1580 0.160 0.152fr-cs 0.1420 0.145 0.137ru-cs 0.1506 0.151 0.144Table 5: Final BLEU scores.
BLEU is true-cased computed by the system, BLEUl isthe official lowercased evaluation by matrix.statmt.org.
BLEUt is official truecased eval-uation.
Although lower official scores are ex-pected, notice the larger gap in en-fr and cs-frtranslation.
There seems to be a problem inour French detokenization procedure.4.1 DataFor the additional Russian-to-Czech systems,we used following parallel data:?
UMC 0.1 (Klyueva and Bojar, 2008) ?
tri-parallel set, consisting of news articles ?93,432 sentences?
data mined from movie subtitles (de-scribed in further detail below) ?2,324,373 sentences?
Czech-Russian part of InterCorp ?
a cor-pus from translation of fiction books (?er-m?k and Rosen, 2012) ?
148,847 sentencesFor Russian-to-English translation, we usedcombination of?
UMC 0.1 ?
95,540 sentences?
subtitles ?
1,790,209 sentences?
Yandex English-Russian parallel corpus 6?
1,000,000 sentences?
wiki headlines from WMT website 7 ?514,859 sentences?
common crawl from WMT website ?878,386 sentencesAdded together, Russian-Czech paralleldata consisted of 2,566,615 sentences andEnglish-Czech parallel data consisted of4,275,961 sentences 8.We also used 765 sentences from UMC003as a devset for MERT training.We used the following monolingual corporato train language models.
Russian:?
Russian sides of all the parallel data ?4,275,961 sentences?
News commentary from WMT website ?150,217 sentences?
News crawl 2012 ?
9,789,861 sentencesFor Czech:?
Czech sides of all the parallel data ?2,566,615 sentences?
Data downloaded from Czech news arti-cles9 ?
1,531,403 sentences?
WebColl (Spoustov?
et al 2010) ?4,053,223 sentences?
PDT 10 ?
115,844 sentences?
Complete Czech Wikipedia ?
3,695,172sentences?
Sentences scraped from Czech socialserver okoun.cz ?
580,249 sentencesFor English:?
English sides of all the paralel data ?4,275,961 sentences?
News commentary from WMT website ?150,217 sentencesTable 6 and Table 7 shows the sizes of thetraining data.6https://translate.yandex.ru/corpus?lang=en7http://www.statmt.org/wmt13/translation-task.html8some sentences had to be removed for technicalreasons9http://thepiratebay.sx/torrent/7121533/10http://ufal.mff.cuni.cz/pdt2.0/89Corpus SentPairs Tok lng1 Tok lng2cs-ru 2,566,615 19,680,239 20,031,688en-ru 4,275,961 64,619,964 58,671,725Table 6: Number of sentence pairs and tokensfor every language pair.Corpus Sentences Tokensen mono 13,426,211 278,199,832ru mono 13,701,213 231,076,387cs mono 12,542,506 202,510,993Table 7: Number of sentences and tokens forevery language.4.1.1 Tokenization, taggingCzech and English data was tokenized andtagged using Mor?e tagger; Russian was to-kenized and tagged using TreeTagger.
Tree-Tagger also does lemmatization; however, wedidn?t use lemmas for alignment or translationmodels, since our experiments showed thatprimitive stemming got better results.However, what is important to mention isthat TreeTagger had problems with some cor-pora, mostly Common Crawl.
For some rea-son, Russian TreeTagger has problems with?dirty?
data?sentences in English, French orrandom non-unicode noise.
It either slowsdown significantly or stops working at all.
Forthis reason, we wrapped TreeTagger in a scriptthat detected those hangs and replaced theerroneous Russian sentences with bogus, one-letter Russian sentences (we can?t delete those,since the lines already exist in the opposite lan-guages; but since the pair doesn?t really makesense in the first place, it doesn?t matter asmuch).All the data are lowercased for all the mod-els and we recase the letters only at the veryend.4.1.2 Subtitle dataFor an unrelated project dealing with moviesubtitles translation, we obtained data fromOpenSubtitles.org for Czech and English sub-titles.
However, those data were not alignedon sentence level and were less structured?wehad thousands of .srt files with some sort ofmetadata.When exploiting the data from the subtitles,we made several observations:?
language used in subtitles is very differentfrom the language used in news articles?
one of the easiest and most accurate sen-tence alignments in movie subtitles is theone based purely on the time stamps?
allowing bigger differences in the timestamps in the alignment produced moredata, but less accurate?
the subtitles are terribly out of domain (asexperiments with using only the subtitledata showed us), but adding the corpusmined from the subtitles still increasesthe accuracy of the translation?
allowing bigger differences in the timestamps and, therefore, more (albeit lessaccurate) data always led to better resultsin our tests.In the end, we decided to pair as much sub-titles as possible, even with the risk of somebeing misaligned, because we found out thatthis helped the most.4.2 Translation model, language modelFor alignment, we used primitive stemmingthat takes just first 6 letters from a word.We found out that using this ?brute force?stemming?for reasons that will have to beexplored in a further research?return betterresults than regular lemmatization, for bothalignment and translation model, as describedfurther.For each language pair, we used a transla-tion model with two translation tables, one ofthem as backoff model.
More exactly, the pri-mary translation is from a form to a combina-tion of (lower case) form and tag, and the sec-ondary backoff translation is from a ?stem?
de-scribed above to a combination of (lower case)form and tag.We built two language models?one for tagsand one for lower case forms.The models were actually a mixed model us-ing interpolate option in SRILM?we trained adifferent language model for each corpus, andthen we mixed the language models using asmall development set from UMC003.904.3 Final ResultsThe final results from matrix.statmt.org arein the table Table 8.
You might notice a sharpdifference between lowercased and truecasedBLEU?that is due to a technical error thatwe didn?t notice before the deadline.Direction BLEUl BLEUtru-cs 0.158 0.135cs-ru 0.165 0.162ru-en 0.224 0.174en-ru 0.163 0.160Table 8: Lowercased and cased BLEU scores5 ConclusionWe have described two independent Moses-based SMT systems we used for the WMT2013 shared task.
We discussed experimentswith large data for many language pairs fromthe point of view of both the translation accu-racy and efficiency.AcknowledgementsThe work on this project was supported bythe grant P406/11/1499 of the Czech ScienceFoundation (GA?R), and by the grant 639012of the Grant Agency of Charles University(GAUK).ReferencesFranti?ek ?erm?k and Alexandr Rosen.
2012.
Thecase of InterCorp, a multilingual parallel cor-pus.
International Journal of Corpus Linguis-tics, 13(3):411?427.Stanley F. Chen and Joshua Goodman.
1998.
Anempirical study of smoothing techniques for lan-guage modeling.
In Technical report TR-10-98,Computer Science Group, Harvard, MA, USA,August.
Harvard University.Natalia Klyueva and Ond?ej Bojar.
2008.
UMC0.1: Czech-Russian-english multilingual corpus.In International Conference Corpus Linguistics.Reinhard Kneser and Hermann Ney.
1995.
Im-proved backing-off for m-gram language mod-eling.
In Proceedings of the IEEE InternationalConference on Acoustics, Speech and Signal Pro-cessing, pages 181?184, Los Alamitos, Califor-nia, USA.
IEEE Computer Society Press.Philipp Koehn, Franz Josef Och, and DanielMarcu.
2003.
Statistical phrase-based transla-tion.
In NAACL ?03: Proceedings of the 2003Conference of the North American Chapter ofthe Association for Computational Linguisticson Human Language Technology, pages 48?54,Morristown, NJ, USA.
Association for Compu-tational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ond?ej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open Source Toolkit for Statistical Ma-chine Translation.
In Proceedings of the 45thAnnual Meeting of the Association for Computa-tional Linguistics Companion Volume Proceed-ings of the Demo and Poster Sessions, pages177?180, Praha, Czechia, June.
Association forComputational Linguistics.Franz Josef Och and Hermann Ney.
2003.A systematic comparison of various statisticalalignment models.
Computational Linguistics,29(1):19?51.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In ACL?03: Proceedings of the 41st Annual Meetingon Association for Computational Linguistics,pages 160?167, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Johanka Spoustov?, Miroslav Spousta, and PavelPecina.
2010.
Building a web corpus of czech.In Proceedings of the Seventh International Con-ference on Language Resources and Evaluation(LREC?10), Valletta, Malta, may.
EuropeanLanguage Resources Association (ELRA).Andreas Stolcke.
2002.
Srilm ?
an extensible lan-guage modeling toolkit.
In Proceedings of Inter-national Conference on Spoken Language Pro-cessing, Denver, Colorado, USA.Zden?k ?abokrtsk?, Jan Pt?
?ek, and Petr Pa-jas.
2008.
TectoMT: Highly modular MT sys-tem with tectogrammatics used as transfer layer.In ACL 2008 WMT: Proceedings of the ThirdWorkshop on Statistical Machine Translation,pages 167?170, Columbus, OH, USA.
Associa-tion for Computational Linguistics.Daniel Zeman.
2012.
Data issues of the multi-lingual translation matrix.
In Proceedings of theSeventh Workshop on Statistical Machine Trans-lation, pages 395?400, Montr?al, Canada.
Asso-ciation for Computational Linguistics.91
