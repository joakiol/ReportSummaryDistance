Proceedings of NAACL HLT 2009: Short Papers, pages 97?100,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsLearning Combination Features with L1 RegularizationDaisuke Okanohara?
Jun?ichi Tsujii???
?Department of Computer Science, University of TokyoHongo 7-3-1, Bunkyo-ku, Tokyo, Japan?School of Informatics, University of Manchester?NaCTeM (National Center for Text Mining){hillbig,tsujii}@is.s.u-tokyo.ac.jpAbstractWhen linear classifiers cannot successfullyclassify data, we often add combination fea-tures, which are products of several originalfeatures.
The searching for effective combi-nation features, namely feature engineering,requires domain-specific knowledge and hardwork.
We present herein an efficient algorithmfor learning an L1 regularized logistic regres-sion model with combination features.
Wepropose to use the grafting algorithm with ef-ficient computation of gradients.
This enablesus to find optimal weights efficiently withoutenumerating all combination features.
By us-ing L1 regularization, the result we obtain isvery compact and achieves very efficient in-ference.
In experiments with NLP tasks, weshow that the proposed method can extract ef-fective combination features, and achieve highperformance with very few features.1 IntroductionA linear classifier is a fundamental tool for manyNLP applications, including logistic regressionmodels (LR), in that its score is based on a lin-ear combination of features and their weights,.
Al-though a linear classifier is very simple, it canachieve high performance on many NLP tasks,partly because many problems are described withvery high-dimensional data, and high dimensionalweight vectors are effective in discriminating amongexamples.However, when an original problem cannot behandled linearly, combination features are oftenadded to the feature set, where combination featuresare products of several original features.
Examplesof combination features are, word pairs in docu-ment classification, or part-of-speech pairs of headand modifier words in a dependency analysis task.However, the task of determining effective combina-tion features, namely feature engineering, requiresdomain-specific knowledge and hard work.Such a non-linear phenomenon can be implic-itly captured by using the kernel trick.
However,its computational cost is very high, not only duringtraining but also at inference time.
Moreover, themodel is not interpretable, in that effective featuresare not represented explicitly.
Many kernels meth-ods assume an L2 regularizer, in that many featuresare equally relevant to the tasks (Ng, 2004).There have been several studies to find efficientways to obtain (combination) features.
In the con-text of boosting, Kudo (2004) have proposed amethod to extract complex features that is similarto the item set mining algorithm.
In the context ofL1 regularization.
Dud?
?k (2007), Gao (2006), andTsuda (2007) have also proposed methods by whicheffective features are extracted from huge sets of fea-ture candidates.
However, their methods are stillvery computationally expensive, and we cannot di-rectly apply this kind of method to a large-scale NLPproblem.In the present paper, we propose a novel algorithmfor learning of an L1 regularized LR with combina-tion features.
In our algorithm, we can exclusivelyextract effective combination features without enu-merating all of the candidate features.
Our methodrelies on a grafting algorithm (Perkins and Theeiler,2003), which incrementally adds features like boost-ing, but it can converge to the global optimum.We use L1 regularization because we can obtaina sparse parameter vector, for which many of theparameter values are exactly zero.
In other words,learning with L1 regularization naturally has an in-trinsic effect of feature selection, which results in an97efficient and interpretable inference with almost thesame performance as L2 regularization (Gao et al,2007).The heart of our algorithm is a way to find afeature that has the largest gradient value of likeli-hood from among the huge set of candidates.
Tosolve this problem, we propose an example-wise al-gorithm with filtering.
This algorithm is very simpleand easy to implement, but effective in practice.We applied the proposed methods to NLP tasks,and found that our methods can achieve the samehigh performance as kernel methods, whereas thenumber of active combination features is relativelysmall, such as several thousands.2 Preliminaries2.1 Logistic Regression ModelIn this paper, we consider a multi-class logistic re-gression model (LR).
For an input x, and an outputlabel y ?
Y , we define a feature vector ?
(x, y) ?Rm.Then in LR, the probability for a label y, given aninput x, is defined as follows:p(y|x;w) = 1Z(x,w) exp(wT?
(x, y)) , (1)where w ?
Rm is a weight vector1 correspond-ing to each input dimension, and Z(x,w) =?y exp(wT?
(x, y)) is the partition function.We estimate the parameter w by a maximum like-lihood estimation (MLE) with L1 regularization us-ing training examples {(x1, y1), .
.
.
, (xn, yn)}:w?
= argminw?
L(w) + C?i|wi| (2)L(w) = ?i=1...nlog p(yi|xi;w)where C > 0 is the trade-off parameter between thelikelihood term and the regularization term.
This es-timation is a convex optimization problem.2.2 GraftingTo maximize the effect of L1 regularization, we usethe grafting algorithm (Perkins and Theeiler, 2003);namely, we begin with the empty feature set, andincrementally add effective features to the currentproblem.
Note that although this is similar to the1A bias term b is often considered by adding an additionaldimension to ?
(x, y)boosting algorithm for learning, the obtained resultis always optimal.
We explain the grafting algorithmhere again for the sake of clarity.The grafting algorithm is summarized in Algo-rithm 1.In this algorithm we retain two variables; w storesthe current weight vector, and H stores the set offeatures with a non-zero weight.
Initially, we setw = 0, and H = {}.
At each iteration, the fea-ture is selected that has the largest absolute value ofthe gradient of the likelihood.
Let vk = ?L(w)?wk bethe gradient value of the likelihood of a feature k.By following the definition, the value vk can be cal-culated as follows,vk =?i,y?i,y?k(xi, y), (3)where ?i,y = I(yi = y)?
p(yi|xi;w) and I(a) is 1if a is true and 0 otherwise.Then, we add k?
= argmaxk |vk| to H and opti-mize (2) with regard to H only.
The solution w thatis obtained is used in the next search.
The iterationis continued until |v?k| < C.We briefly explain why we can find the optimalweight by this algorithm.
Suppose that we optimize(2) with all features, and initialize the weights us-ing the results obtained from the grafting algorithm.Since all gradients of likelihoods satisfy |vk| ?
C,and the regularization term pushes the weight toward0 by C, any changes of the weight vector cannot in-crease the objective value in (2).
Since (2) is theconvex optimization problem, the local optimum isalways the global optimum, and therefore this is theglobal optimum for (2)The point is that, given an efficient method to esti-mate v?k without the enumeration of all features, wecan solve the optimization in time proportional to theactive feature, regardless of the number of candidatefeatures.
We will discuss this in the next section.3 Extraction of Combination FeaturesThis section presents an algorithm to compute, forcombination features, the feature v?k that has thelargest absolute value of the gradient.We propose an element-wise extraction method,where we make use of the sparseness of the trainingdata.In this paper, we assume that the values of thecombination features are less than or equal to theoriginal ones.
This assumption is typical; for exam-ple, it is made in the case where we use binary valuesfor original and combination features.98Algorithm 1 GraftingInput: training data (xi, yi) (i = 1, ?
?
?
, n) andparameter CH = {},w = 0loopv = ?L(w)?w (L(w) is the log likelihood term)k?
= argmaxk|vk| (The result of Algorithm 2)if |vk?
| < C then breakH = H ?
k?Optimize w with regards to Hend loopOutput w and HFirst, we sort the examples in the order of their?y |?i,y| values.
Then, we look at the examples oneby one.
Let us assume that r examples have beenexamined so far.
Let us definet = ?i?r,y?i,y?
(xi, y) (4)t?
= ?i>r,y??i,y?
(xi, y) t+ =?i>r,y?+i,y?
(xi, y)where ?
?i,y = min(?i,y, 0) and ?+i,y = max(?i,y, 0).Then, simple calculus shows that the gradientvalue for a combination feature k, vk, for whichthe original features are k1 and k2, is bounded be-low/above thus;tk + t?k < vk < tk + t+k (5)tk + max(t?k1, t?k2) < vk < tk + min(t+k1, t+k2).Intuitively, the upper bound of (5) is the case wherethe combination feature fires only for the exampleswith ?i,y ?
0, and the lower bound of (5is the casewhere the combination feature fires only for the ex-amples with ?i,y ?
0.
The second inequality arisesfrom the fact that the value of a combination featureis equal to or less than the values of its original fea-tures.
Therefore, we examine (5) and check whetheror not |vk| will be larger than C. If not, we can re-move the feature safely.Since the examples are sorted in the order of their?y |?i,y|, the bound will become tighter quickly.Therefore, many combination features are filteredout in the early steps.
In experiments, the weightsfor the original features are optimized first, and thenthe weights for combination features are optimized.This significantly reduces the number of candidatesfor combination features.Algorithm 2 Algorithm to return the feature that hasthe largest gradient value.Input: training data (xi, yi) and its ?i,y value(i = 1, .
.
.
, n, y = 1, .
.
.
, |Y |), and the param-eter C. Examples are sorted with respect to their?y |?i,y| values.t+ =?ni=1?y max(?i,y, 0)?
(x, y)t?
=?ni=1?y min(?i,y, 0)?
(x, y)t = 0, H = {} // Active Combination Featurefor i = 1 to n and y ?
Y dofor all combination features k in xi doif |vk| > C (Check by using Eq.
(5) ) thenvk := vk + ?i,y?k(xi, y)H = H ?
kend ifend fort+ := t+ ?max(?i,y, 0)?
(xi, y)t?
:= t?
?min(?i,y, 0)?
(xi, y)end forOutput: argmaxk?H vkAlgorithm 2 presents the details of the overall al-gorithm for the extraction of effective combinationfeatures.
Note that many candidate features will beremoved just before adding.4 ExperimentsTo measure the effectiveness of the proposedmethod (called L1-Comb), we conducted experi-ments on the dependency analysis task, and the doc-ument classification task.
In all experiments, the pa-rameterC was tuned using the development data set.In the first experiment, we performed Japanesedependency analysis.
We used the Kyoto Text Cor-pus (Version 3.0), Jan. 1, 3-8 as the training data,Jan.
10 as the development data, and Jan. 9 as thetest data so that the result could be compared to thosefrom previous studies (Sassano, 2004)2.
We used theshift-reduce dependency algorithm (Sassano, 2004).The number of training events was 11, 3332, each ofwhich consisted of two word positions as inputs, andy = {0, 1} as an output indicating the dependencyrelation.
For the training data, the number of orig-inal features was 78570, and the number of combi-nation features of degrees 2 and 3 was 5787361, and169430335, respectively.
Note that we need not seeall of them using our algorithm.2The data set is different from that in the CoNLL sharedtask.
This data set is more difficult.99Table 1: The performance of the Japanese dependencytask on the Test set.
The active features column showsthe number of nonzero weight features.DEP.
TRAIN ACTIVEACC.
(%) TIME (S) FEAT.L1-COMB 89.03 605 78002L1-ORIG 88.50 35 29166SVM 3-POLY 88.72 35720 (KERNEL)L2-COMB3 89.52 22197 91477782AVE.
PERCE.
87.23 5 45089In all experiments, combination features of de-grees 2 and 3 (the products of two or three originalfeatures) were used.We compared our methods using LR with L1regularization using original features (L1-Original),SVM with a 3rd-polynomial Kernel, LR with L2regularization using combination features with up to3 combinations (L2-Comb3), and an averaged per-ceptron with original features (Ave. Perceptron).Table 1 shows the result of the Japanese depen-dency task.
The accuracy result indicates that theaccuracy was improved with automatically extractedcombination features.
In the column of active fea-tures, the number of active features is listed.
Thisindicates thatL1 regularization automatically selectsvery few effective features.
Note that, in training,L1-Comb used around 100 MB, while L2-Comb3used more than 30 GB.
The most time consumingpart for L1-Comb was the optimization of the L1-LR problem.Examples of extracted combination features in-clude POS pairs of head and modifiers, such asHead/Noun-Modifier/Noun, and combinations ofdistance features with the POS of head.For the second experiment, we performed thedocument classification task using the Tech-TC-300data set (Davidov et al, 2004)3.
We used the tf-idfscores as feature values.
We did not filter out anywords beforehand.
The Tech-TC-300 data set con-sists of 295 binary classification tasks.
We dividedeach document set into a training and a test set.
Theratio of the test set to the training set was 1 : 4.
Theaverage number of features for tasks was 25, 389.Table 2 shows the results for L1-LR with combi-nation features and SVM with linear kernel4.
Theresults indicate that the combination features are ef-fective.3http://techtc.cs.technion.ac.il/techtc300/techtc300.html4SVM with polynomial kernel did not achieve significantimprovementTable 2: Document classification results for the Tech-TC-300 data set.
The column F2 shows the average of F2scores for each method of classification.F2L1-COMB 0.949L1-ORIG 0.917SVM (LINEAR KERNEL) 0.8965 ConclusionWe have presented a method to extract effectivecombination features for the L1 regularized logis-tic regression model.
We have shown that a simplefiltering technique is effective for enumerating effec-tive combination features in the grafting algorithm,even for large-scale problems.
Experimental resultsshow that a L1 regularized logistic regression modelwith combination features can achieve comparableor better results than those from other methods, andits result is very compact and easy to interpret.
Weplan to extend our method to include more complexfeatures, and apply it to structured output learning.ReferencesDavidov, D., E. Gabrilovich, and S. Markovitch.
2004.Parameterized generation of labeled datasets for textcategorization based on a hierarchical directory.
InProc.
of SIGIR.Dud?
?k, Miroslav, Steven J. Phillips, and Robert E.Schapire.
2007.
Maximum entropy density estima-tion with generalized regularization and an applicationto species distribution modeling.
JMLR, 8:1217?1260.Gao, J., H. Suzuki, and B. Yu.
2006.
Approximationlasso methods for language modeling.
In Proc.
ofACL/COLING.Gao, J., G. Andrew, M. Johnson, and K. Toutanova.2007.
A comparative study of parameter estimationmethods for statistical natural language processing.
InProc.
of ACL, pages 824?831.Kudo, T. and Y. Matsumoto.
2004.
A boosting algorithmfor classification of semi-structured text.
In Proc.
ofEMNLP.Ng, A.
2004.
Feature selection, l1 vs. l2 regularization,and rotational invariance.
In NIPS.Perkins, S. and J. Theeiler.
2003.
Online feature selec-tion using grafting.
ICML.Saigo, H., T. Uno, and K. Tsuda.
2007.
Mining com-plex genotypic features for predicting HIV-1 drug re-sistance.
Bioinformatics, 23:2455?2462.Sassano, Manabu.
2004.
Linear-time dependency analy-sis for japanese.
In Proc.
of COLING.100
