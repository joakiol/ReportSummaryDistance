Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 125?132, New York City, June 2006. c?2006 Association for Computational LinguisticsLearning Auxiliary Fronting with Grammatical InferenceAlexander ClarkDepartment of Computer ScienceRoyal Holloway University of LondonEgham, Surrey TW20 0EXalexc@cs.rhul.ac.ukRe?mi EyraudEURISE23, rue du Docteur Paul Michelon42023 Saint- ?Etienne Cedex 2Franceremi.eyraud@univ-st-etienne.frAbstractWe present a simple context-free gram-matical inference algorithm, and provethat it is capable of learning an inter-esting subclass of context-free languages.We also demonstrate that an implementa-tion of this algorithm is capable of learn-ing auxiliary fronting in polar interroga-tives (AFIPI) in English.
This has beenone of the most important test cases inlanguage acquisition over the last fewdecades.
We demonstrate that learningcan proceed even in the complete absenceof examples of particular constructions,and thus that debates about the frequencyof occurrence of such constructions are ir-relevant.
We discuss the implications ofthis on the type of innate learning biasesthat must be hypothesized to explain firstlanguage acquisition.1 IntroductionFor some years, a particular set of examples hasbeen used to provide support for nativist theoriesof first language acquisition (FLA).
These exam-ples, which hinge around auxiliary inversion in theformation of questions in English, have been con-sidered to provide a strong argument in favour ofthe nativist claim: that FLA proceeds primarilythrough innately specified domain specific mecha-nisms or knowledge, rather than through the oper-ation of general-purpose cognitive mechanisms.
Akey point of empirical debate is the frequency of oc-currence of the forms in question.
If these are van-ishingly rare, or non-existent in the primary linguis-tic data, and yet children acquire the construction inquestion, then the hypothesis that they have innateknowledge would be supported.
But this rests on theassumption that examples of that specific construc-tion are necessary for learning to proceed.
In thispaper we show that this assumption is false: that thisparticular construction can be learned without thelearner being exposed to any examples of that par-ticular type.
Our demonstration is primarily mathe-matical/computational: we present a simple experi-ment that demonstrates the applicability of this ap-proach to this particular problem neatly, but the datawe use is not intended to be a realistic representationof the primary linguistic data, nor is the particularalgorithm we use suitable for large scale grammarinduction.We present a general purpose context-free gram-matical algorithm that is provably correct under acertain learning criterion.
This algorithm incorpo-rates no domain specific knowledge: it has no spe-cific information about language; no knowledge ofX-bar schemas, no hidden sources of information toreveal the structure.
It operates purely on unanno-tated strings of raw text.
Obviously, as all learn-ing algorithms do, it has an implicit learning bias.This very simple algorithm has a particularly clearbias, with a simple mathematical description, that al-lows a remarkably simple characterisation of the setof languages that it can learn.
This algorithm doesnot use a statistical learning paradigm that has to betested on large quantities of data.
Rather it uses a125symbolic learning paradigm, that works efficientlywith very small quantities of data, while being verysensitive to noise.
We discuss this choice in somedepth below.For reasons that were first pointed out by Chom-sky (Chomsky, 1975, pages 129?137), algorithmsof this type are not capable of learning all of nat-ural language.
It turns out, however, that algorithmsbased on this approach are sufficiently strong tolearn some key properties of language, such as thecorrect rule for forming polar questions.In the next section we shall describe the disputebriefly; in the subsequent sections we will describethe algorithm we use, and the experiments we haveperformed.2 The DisputeWe will present the dispute in traditional terms,though later we shall analyse some of the assump-tions implicit in this description.
In English, po-lar interrogatives (yes/no questions) are formed byfronting an auxiliary, and adding a dummy auxiliary?do?
if the main verb is not an auxiliary.
For exam-ple,Example 1a The man is hungry.Example 1b Is the man hungry?When the subject NP has a relative clause that alsocontains an auxiliary, the auxiliary that is moved isnot the auxiliary in the relative clause, but the one inthe main (matrix) clause.Example 2a The man who is eating is hungry.Example 2b Is the man who is eating hungry?An alternative rule would be to move the first oc-curring auxiliary, i.e.
the one in the relative clause,which would produce the formExample 2c Is the man who eating is hungry?In some sense, there is no reason that childrenshould favour the correct rule, rather than the in-correct one, since they are both of similar com-plexity and so on.
Yet children do in fact, whenprovided with the appropriate context, produce sen-tences of the form of Example 2b, and rarely if everproduce errors of the form Example 2c (Crain andNakayama, 1987).
The problem is how to accountfor this phenomenon.Chomsky claimed first, that sentences of the typein Example 2b are vanishingly rare in the linguis-tic environment that children are exposed to, yetwhen tested they unfailingly produce the correctform rather than the incorrect Example 2c.
This isput forward as strong evidence in favour of innatelyspecified language specific knowledge: we shall re-fer to this view as linguistic nativism.In a special volume of the Linguistic Review, Pul-lum and Scholz (Pullum and Scholz, 2002), showedthat in fact sentences of this type are not rare at all.Much discussion ensued on this empirical questionand the consequences of this in the context of ar-guments for linguistic nativism.
These debates re-volved around both the methodology employed inthe study, and also the consequences of such claimsfor nativist theories.
It is fair to say that in spiteof the strength of Pullum and Scholz?s arguments,nativists remained completely unconvinced by theoverall argument.
(Reali and Christiansen, 2004) present a possiblesolution to this problem.
They claim that local statis-tics, effectively n-grams, can be sufficient to indi-cate to the learner which alternative should be pre-ferred.
However this argument has been carefully re-butted by (Kam et al, 2005), who show that this ar-gument relies purely on a phonological coincidencein English.
This is unsurprising since it is implausi-ble that a flat, finite-state model should be powerfulenough to model a phenomenon that is clearly struc-ture dependent in this way.In this paper we argue that the discussion aboutthe rarity of sentences that exhibit this particularstructure is irrelevant: we show that simple gram-matical inference algorithms can learn this propertyeven in the complete absence of sentences of thisparticular type.
Thus the issue as to how frequentlyan infant child will see them is a moot point.3 AlgorithmContext-free grammatical inference algorithms areexplored in two different communities: in gram-matical inference and in NLP.
The task in NLP isnormally taken to be one of recovering appropri-ate annotations (Smith and Eisner, 2005) that nor-mally represent constituent structure (strong learn-ing), while in grammatical inference, researchers126are more interested in merely identifying the lan-guage (weak learning).
In both communities, thebest performing algorithms that learn from raw posi-tive data only 1, generally rely on some combinationof three heuristics: frequency, information theoreticmeasures of constituency, and finally substitutabil-ity.
2 The first rests on the observation that stringsof words generated by constituents are likely to oc-cur more frequently than by chance.
The secondheuristic looks for information theoretic measuresthat may predict boundaries, such as drops in condi-tional entropy.
The third method which is the foun-dation of the algorithm we use, is based on the distri-butional analysis of Harris (Harris, 1954).
This prin-ciple has been appealed to by many researchers inthe field of grammatical inference, but these appealshave normally been informal and heuristic (van Za-anen, 2000).In its crudest form we can define it as follows:given two sentences ?I saw a cat over there?, and ?Isaw a dog over there?
the learner will hypothesizethat ?cat?
and ?dog?
are similar, since they appearin the same context ?I saw a __ there?.
Pairs ofsentences of this form can be taken as evidence thattwo words, or strings of words are substitutable.3.1 PreliminariesWe briefly define some notation.An alphabet ?
is a finite nonempty set of sym-bols called letters.
A string w over ?
is a finite se-quence w = a1a2 .
.
.
an of letters.
Let |w| denotethe length of w. In the following, letters will be in-dicated by a, b, c, .
.
., strings by u, v, .
.
.
, z, and theempty string by ?.
Let ??
be the set of all strings,the free monoid generated by ?.
By a language wemean any subset L ?
??.
The set of all substringsof a language L is denoted Sub(L) = {u ?
?+ :?l, r, lur ?
L} (notice that the empty word does notbelong to Sub(L)).
We shall assume an order ?
or on ?
which we shall extend to ??
in the normalway by saying that u ?
v if |u| < |v| or |u| = |v|and u is lexicographically before v.A grammar is a quadruple G = ?V, ?, P, S?where ?
is a finite alphabet of terminal symbols, V1We do not consider in this paper the complex and con-tentious issues around negative data.2For completeness we should include lexical dependenciesor attraction.is a finite alphabet of variables or non-terminals, Pis a finite set of production rules, and S ?
V is astart symbol.If P ?
V ?
(?
?V )+ then the grammar is said tobe context-free (CF), and we will write the produc-tions as T ?
w.We will write uTv ?
uwv when T ?
w ?
P .??
is the reflexive and transitive closure of ?.In general, the definition of a class L relies ona class R of abstract machines, here called rep-resentations, together with a function L from rep-resentations to languages, that characterize all andonly the languages of L: (1) ?R ?
R,L(R) ?
Land (2) ?L ?
L, ?R ?
R such that L(R) = L.Two representations R1 and R2 are equivalent iffL(R1) = L(R2).3.2 LearningWe now define our learning criterion.
This is identi-fication in the limit from positive text (Gold, 1967),with polynomial bounds on data and computation,but not on errors of prediction (de la Higuera, 1997).A learning algorithm A for a class of represen-tations R, is an algorithm that computes a functionfrom a finite sequence of strings s1, .
.
.
, sn to R. Wedefine a presentation of a language L to be an infinitesequence of elements of L such that every elementof L occurs at least once.
Given a presentation, wecan consider the sequence of hypotheses that the al-gorithm produces, writing Rn = A(s1, .
.
.
sn) forthe nth such hypothesis.The algorithm A is said to identify the class R inthe limit if for every R ?
R, for every presentationof L(R), there is an N such that for all n > N ,Rn = RN and L(R) = L(RN ).We further require that the algorithm needs onlypolynomially bounded amounts of data and compu-tation.
We use the slightly weaker notion defined byde la Higuera (de la Higuera, 1997).Definition A representation class R is identifiablein the limit from positive data with polynomial timeand data iff there exist two polynomials p(), q() andan algorithm A such that S ?
L(R)1.
Given a positive sample S of size m A returnsa representation R ?
R in time p(m), such that2.
For each representation R of size n there exists127a characteristic set CS of size less than q(n)such that if CS ?
S, A returns a representationR?
such that L(R) = L(R?
).3.3 Distributional learningThe key to the Harris approach for learning a lan-guage L, is to look at pairs of strings u and v and tosee whether they occur in the same contexts; that isto say, to look for pairs of strings of the form lur andlvr that are both in L. This can be taken as evidencethat there is a non-terminal symbol that generatesboth strings.
In the informal descriptions of this thatappear in Harris?s work, there is an ambiguity be-tween two ideas.
The first is that they should appearin all the same contexts; and the second is that theyshould appear in some of the same contexts.
We canwrite the first criterion as follows:?l, r lur ?
L if and only if lvr ?
L (1)This has also been known in language theory by thename syntactic congruence, and can be written u ?Lv.The second, weaker, criterion is?l, r lur ?
L and lvr ?
L (2)We call this weak substitutability and write it asu .=L v. Clearly u ?L v implies u .=L v when u isa substring of the language.
Any two strings that donot occur as substrings of the language are obviouslysyntactically congruent but not weakly substitutable.First of all, observe that syntactic congruence is apurely language theoretic notion that makes no ref-erence to the grammatical representation of the lan-guage, but only to the set of strings that occur init.
However there is an obvious problem: syntac-tic congruence tells us something very useful aboutthe language, but all we can observe is weak substi-tutability.When working within a Gold-style identificationin the limit (IIL) paradigm, we cannot rely on statis-tical properties of the input sample, since they willin general not be generated by random draws from afixed distribution.
This, as is well known, severelylimits the class of languages that can be learned un-der this paradigm.
However, the comparative sim-plicity of the IIL paradigm in the form when thereare polynomial constraints on size of characteristicsets and computation(de la Higuera, 1997) makes ita suitable starting point for analysis.Given these restrictions, one solution to this prob-lem is simply to define a class of languages wheresubstitutability implies congruence.
We call thesethe substitutable languages: A language L is substi-tutable if and only if for every pair of strings u, v,u .=L v implies u ?L v. This rather radical so-lution clearly rules out the syntax of natural lan-guages, at least if we consider them as strings ofraw words, rather than as strings of lexical or syn-tactic categories.
Lexical ambiguity alone violatesthis requirement: consider the sentences ?The rosedied?, ?The cat died?
and ?The cat rose from its bas-ket?.
A more serious problem is pairs of sentenceslike ?John is hungry?
and ?John is running?, whereit is not ambiguity in the syntactic category of theword that causes the problem, but rather ambigu-ity in the context.
Using this assumption, whetherit is true or false, we can then construct a simplealgorithm for grammatical inference, based purelyon the idea that whenever we find a pair of stringsthat are weakly substitutable, we can generalise thehypothesized language so that they are syntacticallycongruent.The algorithm proceeds by constructing a graphwhere every substring in the sample defines a node.An arc is drawn between two nodes if and only ifthe two nodes are weakly substitutable with respectto the sample, i.e.
there is an arc between u and v ifand only if we have observed in the sample stringsof the form lur and lvr.
Clearly all of the strings inthe sample will form a clique in this graph (considerwhen l and r are both empty strings).
The connectedcomponents of this graph can be computed in timepolynomial in the total size of the sample.
If thelanguage is substitutable then each of these compo-nents will correspond to a congruence class of thelanguage.There are two ways of doing this: one way, whichis perhaps the purest involves defining a reductionsystem or semi-Thue system which directly capturesthis generalisation process.
The second way, whichwe present here, will be more familiar to computa-tional linguists, and involves constructing a gram-mar.1283.4 Grammar constructionSimply knowing the syntactic congruence might notappear to be enough to learn a context-free gram-mar, but in fact it is.
In fact given the syntactic con-gruence, and a sample of the language, we can sim-ply write down a grammar in Chomsky normal form,and under quite weak assumptions this grammar willconverge to a correct grammar for the language.This construction relies on a simple property ofthe syntactic congruence, namely that is in fact acongruence: i.e.,u ?L v implies ?l, r lur ?L lvrWe define the syntactic monoid to be the quo-tient of the monoid ?
?/ ?L.
The monoid operation[u][v] = [uv] is well defined since if u ?L u?
andv ?L v?
then uv ?L u?v?.We can construct a grammar in the following triv-ial way, from a sample of strings where we are giventhe syntactic congruence.?
The non-terminals of the grammar are iden-tified with the congruence classes of the lan-guage.?
For any string w = uv , we add a production[w] ?
[u][v].?
For all strings a of length one (i.e.
letters of ?
),we add productions of the form [a] ?
a.?
The start symbol is the congruence class whichcontains all the strings of the language.This defines a grammar in CNF.
At first sight, thisconstruction might appear to be completely vacu-ous, and not to define any strings beyond those inthe sample.
The situation where it generalises iswhen two different strings are congruent: if uv =w ?
w?
= u?v?
then we will have two different rules[w] ?
[u][v] and [w] ?
[u?][v?
], since [w] is thesame non-terminal as [w?
].A striking feature of this algorithm is that it makesno attempt to identify which of these congruenceclasses correspond to non-terminals in the targetgrammar.
Indeed that is to some extent an ill-posedquestion.
There are many different ways of assign-ing constituent structure to sentences, and indeedsome reputable theories of syntax, such as depen-dency grammars, dispense with the notion of con-stituent structure all together.
De facto standards,such as the Penn treebank annotations are a some-what arbitrary compromise among many differentpossible analyses.
This algorithm instead relies onthe syntactic monoid, which expresses the combina-torial structure of the language in its purest form.3.5 ProofWe will now present our main result, with an outlineproof.
For a full proof the reader is referred to (Clarkand Eyraud, 2005).Theorem 1 This algorithm polynomially identi-fies in the limit the class of substitutable context-freelanguages.Proof (Sketch) We can assume without loss ofgenerality that the target grammar is in Chomskynormal form.
We first define a characteristic set, thatis to say a set of strings such that whenever the sam-ple includes the characteristic set, the algorithm willoutput a correct grammar.We define w(?)
?
??
to be the smallest word,according to ?, generated by ?
?
(?
?
V )+.
Foreach non-terminal N ?
V define c(N) to be thesmallest pair of terminal strings (l, r) (extending ?from ??
to ??
?
?
?, in some way), such that S ?
?lNr.We can now define the characteristic set CS ={lwr|(N ?
?)
?
P, (l, r) = c(N), w = w(?
)}.The cardinality of this set is at most |P | whichis clearly polynomially bounded.
We observe thatthe computations involved can all be polynomiallybounded in the total size of the sample.We next show that whenever the algorithm en-counters a sample that includes this characteristicset, it outputs the right grammar.
We write G?
forthe learned grammar.
Suppose [u] ??G?
v. Thenwe can see that u ?L v by induction on the max-imum length of the derivation of v. At each stepwe must use some rule [u?]
?
[v?][w?].
It is easyto see that every rule of this type preserves the syn-tactic congruence of the left and right sides of therules.
Intuitively, the algorithm will never generatetoo large a language, since the languages are sub-stitutable.
Conversely, if we have a derivation of astring u with respect to the target grammar G, by129construction of the characteristic set, we will have,for every production L ?
MN in the target gram-mar, a production in the hypothesized grammar ofthe form [w(L)] ?
[w(M)][w(N)], and for everyproduction of the form L ?
a we have a produc-tion [w(L)] ?
a.
A simple recursive argumentshows that the hypothesized grammar will generateall the strings in the target language.
Thus the gram-mar will generate all and only the strings required(QED).3.6 Related workThis is the first provably correct and efficient gram-matical inference algorithm for a linguistically in-teresting class of context-free grammars (but see forexample (Yokomori, 2003) on the class of very sim-ple grammars).
It can also be compared to An-gluin?s famous work on reversible grammars (An-gluin, 1982) which inspired a similar paper(Pilatoand Berwick, 1985).4 ExperimentsWe decided to see whether this algorithm withoutmodification could shed some light on the debatediscussed above.
The experiments we present hereare not intended to be an exhaustive test of the learn-ability of natural language.
The focus is on deter-mining whether learning can proceed in the absenceof positive samples, and given only a very weak gen-eral purpose bias.4.1 ImplementationWe have implemented the algorithm describedabove.
There are a number of algorithmic issuesthat were addressed.
First, in order to find whichpairs of strings are substitutable, the naive approachwould be to compare strings pairwise which wouldbe quadratic in the number of sentences.
A moreefficient approach maintains a hashtable mappingfrom contexts to congruence classes.
Caching hash-codes, and using a union-find algorithm for mergingclasses allows an algorithm that is effectively linearin the number of sentences.In order to handle large data sets with thousandsof sentences, it was necessary to modify the al-gorithm in various ways which slightly altered itsformal properties.
However for the experimentsreported here we used a version which performsthe man who is hungry died .the man ordered dinner .the man died .the man is hungry .is the man hungry ?the man is ordering dinner .is the man who is hungry ordering dinner ?
?is the man who hungry is ordering dinner ?Table 1: Auxiliary fronting data set.
Examplesabove the line were presented to the algorithm dur-ing the training phase, and it was tested on examplesbelow the line.exactly in line with the mathematical descriptionabove.4.2 DataFor clarity of exposition, we have used extremelysmall artificial data-sets, consisting only of sen-tences of types that would indubitably occur in thelinguistic experience of a child.Our first experiments were intended to determinewhether the algorithm could determine the correctform of a polar question when the noun phrase had arelative clause, even when the algorithm was not ex-posed to any examples of that sort of sentence.
Weaccordingly prepared a small data set shown in Ta-ble 1.
Above the line is the training data that thealgorithm was trained on.
It was then tested on all ofthe sentences, including the ones below the line.
Byconstruction the algorithm would generate all sen-tences it has already seen, so it scores correctly onthose.
The learned grammar also correctly generatedthe correct form and did not generate the final form.We can see how this happens quite easily since thesimple nature of the algorithm allows a straightfor-ward analysis.
We can see that in the learned gram-mar ?the man?
will be congruent to ?the man whois hungry?, since there is a pair of sentences whichdiffer only by this.
Similarly, ?hungry?
will be con-gruent to ?ordering dinner?.
Thus the sentence ?isthe man hungry ??
which is in the language, will becongruent to the correct sentence.One of the derivations for this sentence would be:[is the man hungry ?]
?
[is the man hungry] [?]
?
[is the man] [hungry] [?]
?
[is] [the man] [hungry][?]
?
[is] [the man][who is hungry] [hungry] [?]
?130it rainsit may rainit may have rainedit may be rainingit has rainedit has been rainingit is rainingit may have been raining?it may have been rained?it may been have rain?it may have been rainTable 2: English auxiliary data.
Training data abovethe line, and testing data below.
[is] [the man][who is hungry] [ordering dinner] [?
].Our second data set is shown in Table 2, and is afragment of the English auxiliary system.
This hasalso been claimed to be evidence in favour of na-tivism.
This was discussed in some detail by (Pilatoand Berwick, 1985).
Again the algorithm correctlylearns.5 DiscussionChomsky was among the first to point out the limi-tations of Harris?s approach, and it is certainly truethat the grammars produced from these toy exam-ples overgenerate radically.
On more realistic lan-guage samples this algorithm would eventually startto generate even the incorrect forms of polar ques-tions.Given the solution we propose it is worth look-ing again and examining why nativists have felt thatAFIPI was such an important issue.
It appears thatthere are several different areas.
First, the debatehas always focussed on how to construct the inter-rogative from the declarative form.
The problemhas been cast as finding which auxilary should be?moved?.
Implicit in this is the assumption that theinterrogative structure must be defined with refer-ence to the declarative, one of the central assump-tions of traditional transformational grammar.
Now,of course, given our knowledge of many differ-ent formalisms which can correctly generate theseforms without movement we can see that this as-sumption is false.
There is of course a relation be-tween these two sentences, a semantic one, but thisdoes not imply that there need be any particular syn-tactic relation, and certainly not a ?generative?
rela-tion.Secondly, the view of learning algorithms is verynarrow.
It is considered that only sentences of thatexact type could be relevant.
We have demonstrated,if nothing else, that that view is false.
The distinctioncan be learnt from a set of data that does not includeany example of the exact piece of data required: aslong as the various parts can be learned separately,the combination will function in the natural way.A more interesting question is the extent to whichthe biases implicit in the learning algorithm are do-main specific.
Clearly the algorithm has a strongbias.
It overgeneralises massively.
One of the advan-tages of the algorithm for the purposes of this paperis that its triviality allows a remarkably clear and ex-plicit statement of its bias.
But is this bias specific tothe domain of language?
It in no way refers to any-thing specific to the field of language, still less spe-cific to human language ?
no references to parts ofspeech, or phrases, or even hierarchical phrase struc-ture.
It is now widely recognised that this sort of re-cursive structure is domain-general (Jackendoff andPinker, 2005).We have selected for this demonstration an algo-rithm from grammatical inference.
A number of sta-tistical models have been proposed over the last fewyears by researchers such as (Klein and Manning,2002; Klein and Manning, 2004) and (Solan et al,2005).
These models impressively manage to ex-tract significant structure from raw data.
However,for our purposes, neither of these models is suitable.Klein and Manning?s model uses a variety of differ-ent cues, which combine with some specific initial-isation and smoothing, and an explicit constraint toproduce binary branching trees.
Though very im-pressive, the model is replete with domain-specificbiases and assumptions.
Moreover, it does not learna language in the strict sense (a subset of the set ofall strings), though it would be a simple modificationto make it perform such a task.
The model by Solanet al would be more suitable for this task, but againthe complexity of the algorithm, which has numer-ous components and heuristics, and the lack of a the-oretical justification for these heuristics again makesthe task of identifying exactly what these biases are,and more importantly how domain specific they are,131a very significant problem.In this model, the bias of the algorithm is com-pletely encapsulated in the assumption u .= v im-plies u ?
v. It is worth pointing out that this doesnot even need hierarchical structure ?
the modelcould be implemented purely as a reduction systemor semi-Thue system.
The disadvantage of usingthat approach is that it is possible to construct somebizarre examples where the number of reductionscan be exponential.Using statistical properties of the set of strings,it is possible to extend these learnability results toa more substantial class of context free languages,though it is unlikely that these methods could be ex-tended to a class that properly contains all naturallanguages.6 ConclusionWe have presented an analysis of the argument thatthe acquisition of auxiliary fronting in polar inter-rogatives supports linguistic nativism.
Using a verysimple algorithm based on the ideas of Zellig Har-ris, with a simple domain-general heuristic, we showthat the empirical question as to the frequency of oc-currence of polar questions of a certain type in child-directed speech is a moot point, since the distinctionin question can be learned even when no such sen-tences occur.Acknowledgements This work has been partiallysupported by the EU funded PASCAL Network ofExcellence on Pattern Analysis, Statistical Mod-elling and Computational Learning.ReferencesD.
Angluin.
1982.
Inference of reversible languages.Communications of the ACM, 29:741?765.Noam Chomsky.
1975.
The Logical Structure of Lin-guistic Theory.
University of Chicago Press.Alexander Clark and Remi Eyraud.
2005.
Identificationin the limit of substitutable context free languages.
InSanjay Jain, Hans Ulrich Simon, and Etsuji Tomita,editors, Proceedings of The 16th International Confer-ence on Algorithmic Learning Theory, pages 283?296.Springer-Verlag.S.
Crain and M. Nakayama.
1987.
Structure dependencein grammar formation.
Language, 63(522-543).C.
de la Higuera.
1997.
Characteristic sets for poly-nomial grammatical inference.
Machine Learning,(27):125?138.
Kluwer Academic Publishers.
Manu-factured in Netherland.E.
M. Gold.
1967.
Language indentification in the limit.Information and control, 10(5):447 ?
474.Zellig Harris.
1954.
Distributional structure.
Word,10(2-3):146?62.Ray Jackendoff and Steven Pinker.
2005.
The nature ofthe language faculty and its implications for the evolu-tion of language.
Cognition, 97:211?225.X.
N. C. Kam, I. Stoyneshka, L. Tornyova, J. D. Fodor,and W. G. Sakas.
2005.
Non-robustness of syntaxacquisition from n-grams: A cross-linguistic perspec-tive.
In The 18th Annual CUNY Sentence ProcessingConference, April.Dan Klein and Christopher D. Manning.
2002.
A gener-ative constituent-context model for improved grammarinduction.
In Proceedings of the 40th Annual Meetingof the ACL.Dan Klein and Chris Manning.
2004.
Corpus-based in-duction of syntactic structure: Models of dependencyand constituency.
In Proceedings of the 42nd AnnualMeeting of the ACL.Samuel F. Pilato and Robert C. Berwick.
1985.
Re-versible automata and induction of the english auxil-iary system.
In Proceedings of the ACL, pages 70?75.Geoffrey K. Pullum and Barbara C. Scholz.
2002.
Em-pirical assessment of stimulus poverty arguments.
TheLinguistic Review, 19(1-2):9?50.Florencia Reali and Morten H. Christiansen.
2004.Structure dependence in language acquisition: Uncov-ering the statistical richness of the stimulus.
In Pro-ceedings of the 26th Annual Conference of the Cogni-tive Science Society, Mahwah, NJ.
Lawrence Erlbaum.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: Training log-linear models on unlabeled data.In Proceedings of the 43rd Annual Meeting of the As-sociation for Computational Linguistics, pages 354?362, Ann Arbor, Michigan, June.Zach Solan, David Horn, Eytan Ruppin, and ShimonEdelman.
2005.
Unsupervised learning of natural lan-guages.
Proc.
Natl.
Acad.
Sci., 102:11629?11634.Menno van Zaanen.
2000.
ABL: Alignment-based learn-ing.
In COLING 2000 - Proceedings of the 18th Inter-national Conference on Computational Linguistics.Takashi Yokomori.
2003.
Polynomial-time identificationof very simple grammars from positive data.
Theoret-ical Computer Science, 298(1):179?206.132
