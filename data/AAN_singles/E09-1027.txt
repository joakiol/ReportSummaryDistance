Proceedings of the 12th Conference of the European Chapter of the ACL, pages 229?237,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsCognitively Motivated Features for Readability AssessmentLijun Feng  No?mie Elhadad  Matt HuenerfauthThe City University of New York,  Columbia University  The City University of New York,Graduate Center  New York, NY, USA  Queens College & Graduate CenterNew York, NY, USA  noemie@dbmi.columbia.edu New York, NY, USAlijun7.feng@gmail.com  matt@cs.qc.cuny.eduAbstractWe investigate linguistic features that correlatewith the readability of texts for adults with in-tellectual disabilities (ID).
Based on a corpusof texts (including some experimentally meas-ured for comprehension by adults with ID), weanalyze the significance of novel discourse-level features related to the cognitive factorsunderlying our users?
literacy challenges.
Wedevelop and evaluate a tool for automaticallyrating the readability of texts for these users.Our experiments show that our discourse-level, cognitively-motivated features improveautomatic readability assessment.1 IntroductionAssessing the degree of readability of a text hasbeen a field of research as early as the 1920's.Dale and Chall define readability as ?the sumtotal (including all the interactions) of all thoseelements within a given piece of printed materialthat affect the success a group of readers havewith it.
The success is the extent to which theyunderstand it, read it at optimal speed, and find itinteresting?
(Dale and Chall, 1949).
It has longbeen acknowledged that readability is a functionof text characteristics, but also of the readersthemselves.
The literacy skills of the readers,their motivations, background knowledge, andother internal characteristics play an importantrole in determining whether a text is readable fora particular group of people.
In our work, weinvestigate how to assess the readability of a textfor people with intellectual disabilities (ID).Previous work in automatic readability as-sessment has focused on generic features of atext at the lexical and syntactic level.
While suchfeatures are essential, we argue that audience-specific features that model the cognitive charac-teristics of a user group can improve the accura-cy of a readability assessment tool.
The contri-butions of this paper are: (1) we present a corpusof texts with readability judgments from adultswith ID; (2) we propose a set of cognitively-motivated features which operate at the discourselevel; (3) we evaluate the utility of these featuresin predicting readability for adults with ID.Our framework is to create tools that benefitpeople with intellectual disabilities (ID), specifi-cally those classified in the ?mild level?
of men-tal retardation, IQ scores 55-70.
About 3% ofthe U.S. population has intelligence test scores of70 or lower (U.S. Census Bureau, 2000).
Peoplewith ID face challenges in reading literacy.
Theyare better at decoding words (sounding them out)than at comprehending their meaning (Drew &Hardman, 2004), and most read below their men-tal age-level (Katims, 2000).
Our research ad-dresses two literacy impairments that distinguishpeople with ID from other low-literacy adults:limitations in (1) working memory and (2) dis-course representation.
People with ID haveproblems remembering and inferring informationfrom text (Fowler, 1998).
They have a slowerspeed of semantic encoding and thus units arelost from the working memory before they areprocessed (Perfetti & Lesgold, 1977; Hickson-Bilsky, 1985).
People with ID also have troublebuilding cohesive representations of discourse(Hickson-Bilsky, 1985).
As less information isintegrated into the mental representation of thecurrent discourse, less is comprehended.Adults with ID are limited in their choice ofreading material.
Most texts that they can readi-ly understand are targeted at the level of reada-bility of children.
However, the topics of thesetexts often fail to match their interests since theyare meant for younger readers.
Because of themismatch between their literacy and their inter-ests, users may not read for pleasure and there-fore miss valuable reading-skills practice time.In a feasibility study we conducted with adults229with ID, we asked participants what they enjoyedlearning or reading about.
The majority of oursubjects mentioned enjoying watching the news,in particular local news.
Many mentioned theywere interested in information that would be re-levant to their daily lives.
While for some ge-nres, human editors can prepare texts for theseusers, this is not practical for news sources thatare frequently updated and specific to a limitedgeographic area (like local news).
Our goal is tocreate an automatic metric to predict the reada-bility of local news articles for adults with ID.Because of the low levels of written literacyamong our target users, we intend to focus oncomprehension of texts displayed on a computerscreen and read aloud by text-to-speech software;although some users may depend on the text-to-speech software, we use the term readability.This paper is organized as follows.
Section 2presents related work on readability assessment.Section 3 states our research hypotheses and de-scribes our methodology.
Section 4 focuses onthe data sets used in our experiments, while sec-tion 5 describes the feature set we used for rea-dability assessment along with a corpus-basedanalysis of each feature.
Section 6 describes areadability assessment tool and reports on evalu-ation.
Section 7 discusses the implications of thework and proposes direction for future work.2 Related Work on Readability MetricsMany readability metrics have been establishedas a function of shallow features of texts, such asthe number of syllables per word and number ofwords per sentence (Flesch, 1948; McLaughlin,1969; Kincaid et al, 1975).
These so-called tra-ditional readability metrics are still used today inmany settings and domains, in part because theyare very easy to compute.
Their results, however,are not always representative of the complexityof a text (Davison and Kantor, 1982).
They caneasily misrepresent the complexity of technicaltexts, or reveal themselves un-adapted to a set ofreaders with particular reading difficulties.
Otherformulas rely on lexical information; e.g., theNew Dale-Chall readability formula consults astatic, manually-built list of ?easy?
words to de-termine whether a text contains unfamiliar words(Chall and Dale, 1995).Researchers in computational linguistics haveinvestigated the use of statistical language mod-els (unigram in particular) to capture the range ofvocabulary from one grade level to another (Siand Callan, 2001; Collins-Thompson and Callan,2004).
These metrics predicted readability betterthan traditional formulas when tested against acorpus of web pages.
The use of syntactic fea-tures was also investigated (Schwarm and Osten-dorf, 2005; Heilman et al, 2007; Petersen andOstendorf, 2009) in the assessment of text reada-bility for English as a Second Language readers.While lexical features alone outperform syntacticfeatures in classifying texts according to theirreading levels, combining the lexical and syntac-tic features yields the best results.Several elegant metrics that focus solely onthe syntax of a text have also been developed.The Yngve (1960) measure, for instance, focuseson the depth of embedding of nodes in the parsetree; others use the ratio of terminal to non-terminal nodes in the parse tree of a sentence(Miller and Chomsky, 1963; Frazier, 1985).These metrics have been used to analyze thewriting of potential Alzheimer's patients to detectmild cognitive impairments (Roark, Mitchell,and Hollingshead, 2007), thereby indicating thatcognitively motivated features of text are valua-ble when creating tools for specific populations.Barzilay and Lapata (2008) presented earlywork in investigating the use of discourse to dis-tinguish abridged from original encyclopediaarticles.
Their focus, however, is on style detec-tion rather than readability assessment per se.Coh-Metrix is a tool for automatically calculat-ing text coherence based on features such as re-petition of lexical items across sentences andlatent semantic analysis (McNamara et al,2006).
The tool is based on comprehension datacollected from children and college students.Our research differs from related work in thatwe seek to produce an automatic readability me-tric that is tailored to the literacy skills of adultswith ID.
Because of the specific cognitive cha-racteristics of these users, it is an open questionwhether existing readability metrics and featuresare useful for assessing readability for adultswith ID.
Many of these earlier metrics have fo-cused on the task of assigning texts to particularelementary school grade levels.
Traditionalgrade levels may not be the ideal way to scoretexts to indicate how readable they are for adultswith ID.
Other related work has used models ofvocabulary (Collins-Thompson and Callan,2004).
Since we would like to use our tool togive adults with ID access to local news stories,we choose to keep our metric topic-independent.Another difference between our approach andprevious approaches is that we have designed thefeatures used by our readability metric based on230the cognitive aspects of our target users.
For ex-ample, these users are better at decoding wordsthan at comprehending text meaning (Drew &Hardman, 2004); so, shallow features like ?sylla-ble count per word?
or unigram models of wordfrequency (based on texts designed for children)may be less important indicators of reading diffi-culty.
A critical challenge for our users is tocreate a cohesive representation of discourse.Due to their impairments in semantic encodingspeed, our users may have particular difficultywith texts that place a significant burden onworking memory (items fall out of memory be-fore they can be semantically encoded).While we focus on readability of texts, otherprojects have automatically generated texts forpeople with aphasia (Carroll et al, 1999) or lowreading skills (Williams and Reiter, 2005).3 Research Hypothesis and MethodsWe hypothesize that the complexity of a text foradults with ID is related to the number of entitiesreferred to in the text overall.
If a paragraph or atext refers to too many entities at once, the readerhas to work harder at mapping each entity to asemantic representation and deciding how eachentity is related to others.
On the other hand,when a text refers to few entities, less work isrequired both for semantic encoding and for in-tegrating the entities into a cohesive mental re-presentation.
Section 5.2 discusses some noveldiscourse-level features (based on the ?entitydensity?
of a text) that we believe will correlateto comprehension by adults with ID.To test our hypothesis, we used the followingmethodology.
We collected four corpora (as de-scribed in Section 4).
Three of them (Britannica,LiteracyNet and WeeklyReader) have been ex-amined in previous work on readability.
Thefourth (LocalNews) is novel and results from auser study we conducted with adults with ID.We then analyzed how significant each feature ison our Britannica and LiteracyNet corpora.
Fi-nally, we combined the significant features into alinear regression model and experimented withseveral feature combinations.
We evaluated ourmodel on the WeeklyReader and LocalNewscorpora.4 Corpora and Readability JudgmentsTo study how certain linguistic features indicatethe readability of a text, we collected a corpus ofEnglish text at different levels of readability.
Anideal corpus for our research would contain textsthat have been written specifically for our au-dience of adults with intellectual disabilities ?
inparticular if such texts were paired with alternateversions of each text written for a general au-dience.
We are not aware of such texts availableelectronically, and so we have instead mostlycollected texts written for an audience of child-ren.
The texts come from online and commercialsources, and some have been analyzed previous-ly by text simplification researchers (Petersenand Ostendorf, 2009).
Our corpus also containssome novel texts produced as part of an experi-mental study involving adults with ID.4.1 Paired and Graded Generic Corpora:Britannica, LiteracyNet, and WeeklyReaderThe first section of our corpus (which we refer toas Britannica) has 228 articles from the Encyclo-pedia Britannica, originally collected by (Barzi-lay and Elhadad, 2003).
This consists of 114articles in two forms: original articles written foradults and corresponding articles rewritten for anaudience of children.
While the texts are paired,the content of the texts is not identical: some de-tails are omitted from the child version, and addi-tional background is sometimes inserted.
Theresulting corpus is comparable in content.Because we are particularly interested in mak-ing local news articles accessible to adults withID, we collected a second paired corpus, whichwe refer to as LiteracyNet, consisting of 115news articles made available through (West-ern/Pacific Literacy Network / LiteracyNet,2008).
The collection of local CNN stories isavailable in an original and simplified/abridgedform (230 total news articles) designed for use inliteracy education.The third corpus we collected (Weekly Reader)was obtained from the Weekly Reader corpora-tion (Weekly Reader, 2008).
It contains articlesfor students in elementary school.
Each text islabeled with its target grade level (grade 2: 174articles, grade 3: 289 articles, grade 4: 428 ar-ticles, grade 5: 542 articles).
Overall, the corpushas 1433 articles.
(U.S. elementary school grades2 to 5 generally are for children ages 7 to 10.
)The corpora discussed above are similar tothose used by Petersen and Ostendorf (2009).While the focus of our research is adults with ID,most of the texts discussed in this section havebeen simplified or written by human authors tobe readable for children.
Despite the texts beingintended for a different audience than the focusof our research, we still believe these texts to be231of value.
It is rare to encounter electronicallyavailable corpora in which an original and a sim-plified version of a text is paired (as in the Bri-tannica and LiteracyNet corpora) or texts labeledas being at specific levels of readability (as in theWeekly Reader corpus).4.2 Readability-Specific Corpus: LocalNewsThe final section of our corpus contains localnews articles that are labeled with comprehen-sion scores.
These texts were produced for a fea-sibility study involving adults with ID.
Each textwas read by adults with ID, who then answeredcomprehension questions to measure their under-standing of the texts.
Unlike the previous corpo-ra, LocalNews is novel and was not investigatedby previous research in readability.After obtaining university approval for our ex-perimental protocol and informed consentprocess, we conducted a study with 14 adultswith mild intellectual disabilities who participatein daytime educational programs in the NewYork area.
Participants were presented with tenarticles collected from various local New Yorkbased news websites.
Some subjects saw theoriginal form of an article and others saw a sim-plified form (edited by a human author); no sub-ject saw both versions.
The texts were presentedin random order using software that displayedthe text on the screen, read it aloud using text-to-speech software, and highlighted each word as itwas read.
Afterward, subjects were asked aloudmultiple-choice comprehension questions.
Wedefined the readability score of a story as thepercentage of correct answers averaged acrossthe subjects who read that particular story.A human editor performed the text simplifica-tion with the goal of making the text more reada-ble for adults with mild ID.
The editor made thefollowing types of changes to the original newsstories: breaking apart complex sentences, un-embedding information in complex prepositionalphrases and reintegrating it as separate sentences,replacing infrequent vocabulary items with morecommon/colloquial equivalents, omitting sen-tences and phrases from the story that mentionentities and phrases extraneous to the maintheme of the article.
For instance, the originalsentence ?They?re installing an induction loopsystem in cabs that would allow passengers withhearing aids to tune in specifically to the driver?svoice.?
was transformed into ?They?re installinga system in cabs.
It would allow passengers withhearing aids to listen to the driver?s voice.
?This corpus of local news articles that havebeen human edited and scored for comprehen-sion by adults with ID is small in size (20 newsarticles), but we consider it a valuable resource.Unlike the texts that have been simplified forchildren (the rest of our corpus), these texts havebeen rated for readability by actual adults withID.
Furthermore, comprehension scores are de-rived from actual reader comprehension tests,rather than self-perceived comprehension.
Be-cause of the small size of this part of our corpus,however, we primarily use it for evaluation pur-poses (not for training the readability models).5 Linguistic Features and ReadabilityWe now describe the set of features we investi-gated for assessing readability automatically.Table 1 contains a list of the features ?
includinga short code name for each feature which may beused throughout this paper.
We have begun byimplementing the simple features used by theFlesh-Kincaid and FOG metrics: average numberof words per sentence, average number of syl-lables per word, and percentage of words in thedocument with 3+ syllables.5.1 Basic Features Used in Earlier WorkWe have also implemented features inspired byearlier research on readability.
Petersen and Os-tendorf (2009) included features calculated fromparsing the sentences in their corpus using theCharniak parser (Charniak, 2000): average parsetree height, average number of noun phrases persentence, average number of verb phrases persentence, and average number of SBARs per sen-tence.
We have implemented versions of most ofthese parse-tree-related features for our project.We also parse the sentences in our corpus usingCharniak?s parser and calculate the followingfeatures listed in Table 1: aNP, aN, aVP, aAdj,aSBr, aPP, nNP, nN, nVP, nAdj, nSBr, and nPP.5.2 Novel Cognitively-Motivated FeaturesBecause of the special reading characteristics ofour target users, we have designed a set of cogni-tively motivated features to predict readability oftexts for adults with ID.
We have discussed howworking memory limits the semantic encoding ofnew information by these users; so, our featuresindicate the number of entities in a text that thereader must keep in mind while reading eachsentence and throughout the entire document.
Itis our hypothesis that this ?entity density?
of a232text plays an important role in the difficulty ofthat text for readers with intellectual disabilities.The first set of features incorporates the Ling-Pipe named entity detection software (Alias-i,2008), which detects three types of entities: per-son, location, and organization.
We also use thepart-of-speech tagger in LingPipe to identify thecommon nouns in the document, and we find theunion of the common nouns and the named entitynoun phrases in the text.
The union of these twosets is our definition of ?entity?
for this set offeatures.
We count both the total number of?entity mentions?
in a text (each token appear-ance of an entity) and the total number of uniqueentities (exact-string-match duplicates onlycounted once).
Table 1 lists these features: nEM,nUE, aEM, and aUE.
We count the totals perdocument to capture how many entities the read-er must keep track of while reading the docu-ment.
We also expect sentences with more enti-ties to be more difficult for our users to semanti-cally encode due to working memory limitations;so, we also count the averages per sentence tocapture how many entities the reader must keepin mind to understand each sentence.To measure the working memory burden of atext, we?d like to capture the number of dis-course entities that a reader must keep in mind.However, the ?unique entities?
identified by thenamed entity recognition tool may not be a per-fect representation of this ?
several unique enti-ties may actually refer to the same real-worldentity under discussion.
To better model howmultiple noun phrases in a text refer to the sameentity or concept, we have also built features us-ing lexical chains (Galley and McKeown, 2003).Lexical chains link nouns in a document con-nected by relations like synonymy or hyponomy;chains can indicate concepts that recur through-out a text.
A lexical chain has both a length(number of noun phrases it includes) and a span(number of words in the document between thefirst noun phrase at the beginning of the chainand the last noun phrase that is part of the chain).We calculate the number of lexical chains in thedocument (nLC) and those with a span greaterthan half the document length (nLC2).
We be-lieve these features may indicate the number ofentities/concepts that a reader must keep in mindduring a document and the subset of very impor-tant entities/concepts that are the main topic ofthe document.
The average length and averagespan of the lexical chains in a document (aLCLand aLCS) may also indicate how many of thechains in the document are short-lived, whichmay mean that they are ancillary enti-ties/concepts, not the main topics.The final two features in Table 1 (aLCw andaLCe) use the concept of an ?active?
chain.
At aparticular location in a text, we define a lexicalchain to be ?active?
if the span (between the firstand last noun in the lexical chain) includes thecurrent location.
We expect these features mayindicate the total number of concepts that thereader needs to keep in mind during a specificmoment in time when reading a text.
Measuringthe average number of concepts that the reader ofa text must keep in mind may suggest the work-ing memory burden of the text over time.
Wewere unsure if individual words or individualnoun-phrases in the document should be used asthe basic unit of ?time?
for the purpose of aver-aging the number of active lexical chains; so, weincluded both features.5.3 Testing the Significance of FeaturesTo select which features to include in our auto-matic readability assessment tool (in Section 6),Code FeatureaWPS average number of words per sentenceaSPW average number of syllables per word%3+S % of words in document with 3+ syllablesaNP avg.
num.
NPs per sentenceaN avg.
num.
common+proper nouns per sentenceaVP avg.
num.
VPs per sentenceaAdj avg.
num.
Adjectives per sentenceaSBr avg.
num.
SBARs per sentenceaPP avg.
num.
prepositional phrases per sentencenNP total number of NPs per sentencenN total num.
of common+proper nouns in documentnVP total number of VPs in the documentnAdj total number of Adjectives in the documentnSBr total number of SBARs in the documentnPP total num.
of prepositional phrases in documentnEM number of entity mentions in documentnUE number of unique entit ies in documentaEM avg.
num.
entity mentions per sentenceaUE avg.
num.
unique entit ies per sentencenLC number of lexical chains in documentnLC2 num.
lex.
chains, span > half document lengthaLCL average lexical chain lengthaLCS average lexical chain spanaLCw avg.
num.
lexical chains active at  each wordaLCn avg.
num.
lexical chains active at  each NPTable 1: Implemented Features233we analyzed the documents in our paired corpora(Britannica and LiteracyNet).
Because they con-tain a complex and a simplified version of eacharticle, we can examine differences in readabilitywhile holding the topic and genre constant.
Wecalculated the value of each feature for each doc-ument, and we used a paired t-test to determine ifthe difference between the complex and simpledocuments was significant for that corpus.Table 2 contains the results of this feature se-lection process; the columns in the table indicatethe values for the following corpora: Britannicacomplex, Britannica simple, LiteracyNet com-plex, and LiteracyNet simple.
An asterisk ap-pears in the ?Sig?
column if the difference be-tween the feature values for the complex vs.simple documents is statistically significant forthat corpus (significance level: p<0.00001).The only two features which did not show asignificant difference (p>0.01) between the com-plex and simple versions of the articles were:average lexical chain length (aLCL) and numberof lexical chains with span greater than half thedocument length (nLC2).
The lack of signific-ance for aLCL may be explained by the vast ma-jority of lexical chains containing few members;complex articles contained more of these chains?
but their chains did not contain more members.In the case of nLC2, over 80% of the articles ineach category contained no lexical chains whosespan was greater than half the document length.The rarity of a lexical chain spanning the majori-ty of a document may have led to there being nosignificant difference between complex/simple.6 A Readability Assessment ToolAfter testing the significance of features usingpaired corpora, we used linear regression and ourgraded corpus (Weekly Reader) to build a reada-bility assessment tool.
To evaluate the tool?susefulness for adults with ID, we test the correla-tion of its scores with the LocalNews corpus.6.1 Versions of Our ModelWe began our evaluation by implementing threeversions of our automatic readability assessmenttool.
The first version uses only those featuresstudied by previous researchers (aWPS, aSPW,%3+S, aNP, aN, aVP, aAdj, aSBr, aPP, nNP, nN,nVP, nAdj, nSBr, nPP).
The second version usesonly our novel cognitively motivated features(section 5.2).
The third version uses the union ofboth sets of features.
By building three versionsof the tool, we can compare the relative impactof our novel cognitively-motivated features.
Forall versions, we have only included those fea-tures that showed a significant difference be-tween the complex and simple articles in ourpaired corpora (as discussed in section 5.3).6.2 Learning Technique and Training DataEarly work on automatic readability analysisframed the problem as a classification task:creating multiple classifiers for labeling a text asbeing one of several elementary school gradelevels (Collins-Thompson and Callan, 2004).Because we are focusing on a unique user groupwith special reading challenges, we do not knowa priori what level of text difficulty is ideal forour users.
We would not know where to drawcategory boundaries for classification.
We alsoprefer that our assessment tool assign numericaldifficulty scores to texts.
Thus, after creatingthis tool, we can conduct further reading com-prehension experiments with adults with ID todetermine what threshold (for readability scoresassigned by our tool) is appropriate for our users.FeatureBrit.Com.Brit.Simp.
SigLitN.Com.LitN.Simp.
SigaWPS 20.13 14.37 * 17.97 12.95 *aSPW 1.708 1.655 * 1.501 1.455 *%3+S 0.196 0.177 * 0.12 0.101 *aNP 8.363 6.018 * 6.519 4.691 *aN 7.024 5.215 * 5.319 3.929 *aVP 2.334 1.868 * 3.806 2.964 *aAdj 1.95 1.281 * 1.214 0.876 *aSBr 0.266 0.205 * 0.793 0.523 *aPP 2.858 1.936 * 1.791 1.22 *nNP 798 219.2 * 150.2 102.9 *nN 668.4 190.4 * 121.4 85.75 *nVP 242.8 69.19 * 88.2 65.52 *nAdj 205 47.32 * 28.11 19.04 *nSBr 31.33 7.623 * 18.16 11.43 *nPP 284.7 70.75 * 41.06 26.79 *nEM 624.2 172.7 * 115.2 82.83 *nUE 355 117 * 81.56 54.94 *aEM 6.441 4.745 * 5.035 3.789 *aUE 4.579 3.305 * 3.581 2.55 *nLC 59.21 17.57 * 12.43 8.617 *nLC2 0.175 0.211 0.191 0.226aLCL 3.009 3.022 2.817 2.847aLCS 357 246.1 * 271.9 202.9 *aLCw 1.803 1.358 * 1.407 1.091 *aLCn 1.852 1.42 * 1.53 1.201 *Table 2: Feature Values of Paired Corpora234To select features for our model, we used ourpaired corpora (Britannica and LiteracyNet) tomeasure the significance of each feature.
Nowthat we are training a model, we make use of ourgraded corpus (articles from Weekly Reader).This corpus contains articles that have each beenlabeled with an elementary school grade level forwhich it was written.
We divide this corpus ?using 80% of articles as training data and 20% astesting data.
We model the grade level of thearticles using linear regression; our model is im-plemented using R (R Development Core Team,2008).6.3 Evaluation of Our Readability ToolWe conducted two rounds of training and evalua-tion of our three regression models.
We alsocompare our models to a baseline readability as-sessment tool: the popular Flesh-Kincaid GradeLevel index (Kincaid et al, 1975).In the first round of evaluation, we trained andtested our regression models on the WeeklyReader corpus.
This round of evaluation helpedto determine whether our feature-set and regres-sion technique were successfully modeling thoseaspects of the texts that were relevant to theirgrade level.
Our results from this round of eval-uation are presented in the form of average errorscores.
(For each article in the Weekly Readertesting data, we calculate the difference betweenthe output score of the model and the correctgrade-level for that article.)
Table 3 presents theaverage error results for the baseline system andour three regression models.
We can see that themodel trained on the shallow and parse-relatedfeatures out-performs the model trained only onour novel features; however, the best modeloverall is the one is trained on all of the features.This model predicts the grade level of WeeklyReader articles to within roughly 0.565 gradelevels on average.Readability Model (or baseline) Average ErrorBaseline: Flesh-Kincaid Index 2.569Basic Features Only 0.6032Cognitively Motivated Features Only 0.6110Basic + Cognitively-Motiv.
Features 0.5650Table 3: Predicting Grade Level of Weekly ReaderIn our second round of evaluation, we trainedthe regression model on the Weekly Reader cor-pus, but we tested it against the LocalNews cor-pus.
We measured the correlation between ourregression models?
output and the comprehen-sion scores of adults with ID on each text.
Forthis reason, we do not calculate the ?average er-ror?
; instead, we simply measure the correlationbetween the models?
output and the comprehen-sion scores.
(We expect negative correlationsbecause comprehension scores should increase asthe predicted grade level of the text goes down.
)Table 4 presents the correlations for our threemodels and the baseline system in the form ofPearson?s R-values.
We see a surprising result:the model trained only on the cognitively-motivated features is more tightly correlated withthe comprehension scores of the adults with ID.While the model trained on all features was bet-ter at assigning grade levels to Weekly Readerarticles, when we tested it on the local news ar-ticles from our user-study, it was not the top-performing model.
This result suggests that theshallow and parse-related features of texts de-signed for children (the Weekly Reader articles,our training data) are not the best predictors oftext readability for adults with ID.Readability Model (or baseline) Pearson?s RBaseline: Flesh-Kincaid Index -0.270Basic Features Only -0.283Cognitively Motivated Features Only -0.352Basic + Cognitively-Motiv.
Features -0.342Table 4: Correlation to User-Study Comprehension7 DiscussionBased on the cognitive and literacy skills ofadults with ID, we designed novel features thatwere useful in assessing the readability of textsfor these users.
The results of our study havesupported our hypothesis that the complexity of atext for adults with ID is related to the number ofentities referred to in the text.
These ?entity den-sity?
features enabled us to build models thatwere better at predicting text readability foradults with intellectual disabilities.This study has also demonstrated the value ofcollecting readability judgments from target us-ers when designing a readability assessment tool.The results in Table 4 suggest that modelstrained on corpora containing texts designed forchildren may not always lead to accurate modelsof the readability of texts for other groups oflow-literacy users.
Using features targeting spe-cific aspects of literacy impairment have allowedus to make better use of children?s texts whendesigning a model for adults with ID.7.1 Future WorkIn order to study more features and models ofreadability, we will require more testing data fortracking progress of our readability regression235models.
Our current study has illustrated theusefulness of texts that have been evaluated byadults with ID, and we therefore plan to increasethe size of this corpus in future work.
In addi-tion to using this corpus for evaluation, we maywant to use it to train our regression models.
Forthis study, we trained on Weekly Reader textlabeled with elementary school grade levels, butthis is not ideal.
Texts designed for children maydiffer from those that are best for adults with ID,and ?grade levels?
may not be the best way torank/rate text readability for these users.
Whileour user-study comprehension-test corpus is cur-rently too small for training, we intend to growthe size of this corpus in future work.We also plan on refining our cognitively moti-vated features for measuring the difficulty of atext for our users.
Currently, we use lexicalchain software to link noun phrases in a docu-ment that may refer to similar entities/concepts.In future work, we plan to use co-reference reso-lution software to model how multiple ?entitymentions?
may refer to a single discourse entity.For comparison purposes, we plan to imple-ment other features that have been used in earlierreadability assessment systems.
For example,Petersen and Ostendorf (2009) created lists of themost common words from the Weekly Readerarticles, and they used the percentage of words ina document not on this list as a feature.The overall goal of our research is to developa software system that can automatically simplifythe reading level of local news articles andpresent them in an accessible way to adults withID.
Our automatic readability assessment toolwill be a component in this future text simplifica-tion system.
We have therefore preferred to in-clude features in our tool that focus on aspects ofthe text that can be modified during a simplifica-tion process.
In future work, we will study howto use our readability assessment tool to guidehow a text revision system decides to modify atext to increase its readability for these users.7.2 Summary of ContributionsWe have contributed to research on automaticreadability assessment by designing a new me-thod for assessing the complexity of a text at thelevel of discourse.
Our novel ?entity density?features are based on named entity and lexicalchain software, and they are inspired by the cog-nitive underpinnings of the literacy challenges ofadults with ID ?
specifically, the role of slowsemantic encoding and working memory limita-tions.
We have demonstrated the usefulness ofthese novel features in modeling the grade levelof elementary school texts and in correlating toreadability judgments from adults with ID.Another contribution of our work is the collec-tion of an initial corpus of texts of local newsstories that have been manually simplified by ahuman editor.
Both the original and the simpli-fied versions of these stories have been evaluatedby adults with intellectual disabilities.
We haveused these comprehension scores in the evalua-tion phase of this study, and we have suggestedhow constructing a larger corpus of such articlescould be useful for training readability tools.More broadly, this project has demonstratedhow focusing on a specific user population, ana-lyzing their cognitive skills, and involving themin a user-study has led to new insights in model-ing text readability.
As Dale and Chall?s defini-tion (1949) originally argued, characteristics ofthe reader are central to the issue of readability.We believe our user-focused research paradigmmay be used to drive further advances in reada-bility assessment for other groups of users.AcknowledgementsWe thank the Weekly Reader Corporation formaking its corpus available for our research.
Weare grateful to Martin Jansche for his assistancewith the statistical data analysis and regression.ReferencesAlias-i.
2008.
LingPipe 3.6.0. http://alias-i.com/lingpipe (accessed October 1, 2008)Barzilay, R., Elhadad, N., 2003.
Sentence alignmentfor monolingual comparable corpora.
In ProcEMNLP, pp.
25-32.Barzilay R., Lapata, M., 2008.
Modeling Local Cohe-rence: An Entity-based Approach.
ComputationalLinguistics.
34(1):1-34.Carroll, J., Minnen, G., Pearce, D., Canning, Y., Dev-lin, S., Tait, J.
1999.
Simplifying text for language-impaired readers.
In Proc.
EACL Poster, p. 269.Chall, J.S., Dale, E., 1995.
Readability Revisited: TheNew Dale-Chall Readability Formula.
BrooklineBooks, Cambridge, MA.Charniak, E. 2000.
A maximum-entropy-inspiredparser.
In Proc.
NAACL, pp.
132-139.Collins-Thompson, K., and Callan, J.
2004.
A lan-guage modeling approach to predicting reading dif-ficulty.
In Proc.
NAACL, pp.
193-200.Dale, E. and J. S. Chall.
1949.
The concept of reada-bility.
Elementary English 26(23).236Davison, A., and Kantor, R.  1982.
On the failure ofreadability formulas to define readable texts: A casestudy from adaptations.
Reading Research Quar-terly, 17(2):187-209.Drew, C.J., and Hardman, M.L.
2004.
Mental retar-dation: A lifespan approach to people with intellec-tual disabilities (8th ed.).
Columbus, OH: Merrill.Flesch, R.  1948.
A new readability yardstick.
Jour-nal of Applied Psychology, 32:221-233.Fowler, A.E.
1998.
Language in mental retardation.In Burack, Hodapp, and Zigler (Eds.
), Handbook ofMental Retardation and Development.
Cambridge,UK: Cambridge Univ.
Press, pp.
290-333.Frazier, L.  1985.
Natural Language Parsing: Psy-chological, Computational, and Theoretical Pers-pectives, chapter Syntactic complexity, pp.
129-189.
Cambridge University Press.Galley, M., McKeown, K. 2003.
Improving WordSense Disambiguation in Lexical Chaining.
InProc.
IJCAI, pp.
1486-1488.Gunning, R. 1952.
The Technique of Clear Writing.McGraw-Hill.Heilman, M., Collins-Thompson, K., Callan, J., andEskenazi, M.  2007.
Combining lexical and gram-matical features to improve readability measures forfirst and second language texts.
In Proc.
NAACL,pp.
460-467.Hickson-Bilsky, L.  1985.
Comprehension and men-tal retardation.
International Review of Research inMental Retardation, 13: 215-246.Katims, D.S.
2000.
Literacy instruction for peoplewith mental retardation: Historical highlights andcontemporary analysis.
Education and Training inMental Retardation and Developmental Disabili-ties, 35(1): 3-15.Kincaid, J. P., Fishburne, R. P., Rogers, R. L., andChissom, B. S.  1975.
Derivation of new readabili-ty formulas for Navy enlisted personnel, ResearchBranch Report 8-75, Millington, TN.Kincaid, J., Fishburne, R., Rodgers, R., and Chisson,B.
1975.
Derivation of new readability formulasfor navy enlisted personnel.
Technical report, Re-search Branch Report 8-75, U.S.
Naval Air Station.McLaughlin, G.H.
1969.
SMOG grading - a newreadability formula.
Journal of Reading,12(8):639-646.McNamara, D.S., Ozuru, Y., Graesser, A.C., & Lou-werse, M. (2006) Validating Coh-Metrix., In Proc.Conference of the Cognitive Science Society, pp.573.Miller, G., and Chomsky, N.  1963.
Handbook ofMathematical Psychology, chapter Finatary modelsof language users, pp.
419-491.
Wiley.Perfetti, C., and Lesgold, A.
1977.
CognitiveProcesses in Comprehension, chapter DiscourseComprehension and sources of individual differ-ences.
Erlbaum.Petersen, S.E., Ostendorf, M. 2009.
A machine learn-ing approach to reading level assessment.
ComputerSpeech and Language, 23: 89-106.R Development Core Team.
2008.
R: A Languageand Environment for Statistical Computing.
Vienna,Austria: R Foundation for Statistical Computing.http://www.R-project.orgRoark, B., Mitchell, M., and Hollingshead, K.  2007.Syntactic complexity measures for detecting mildcognitive impairment.
In Proc.
ACL Workshop onBiological, Translational, and Clinical LanguageProcessing (BioNLP'07), pp.
1-8.Schwarm, S., and Ostendorf, M.  2005.
Reading levelassessment using support vector machines and sta-tistical language models.
In Proc.
ACL, pp.
523-530.Si, L., and Callan, J.
2001.
A statistical model forscientific readability.
In Proc.
CIKM, pp.
574-576.Stenner, A.J.
1996.
Measuring reading comprehensionwith the Lexile framework.
4th North AmericanConference on Adolescent/Adult Literacy.U.S.
Census Bureau.
2000.
Projections of the totalresident population by five-year age groups andsex, with special age categories: Middle series2025-2045.
Washington: U.S. Census Bureau, Po-pulations Projections Program, Population Division.Weekly Reader, 2008. http://www.weeklyreader.com(Accessed Oct., 2008).Western/Pacific Literacy Network / Literacyworks,2008.
CNN SF learning resources.http://literacynet.org/cnnsf/ (Accessed Oct., 2008).Williams, S., Reiter, E. 2005.
Generating readabletexts for readers with low basic skills.
In Proc.
Eu-ropean Workshop on Natural Language Genera-tion, pp.
140-147.Yngve, V.  1960.
A model and a hypothesis for lan-guage structure.
American Philosophical Society,104: 446-466.237
