Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 49?56,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsUnsupervised vs. supervised weight estimationfor semantic MT evaluation metricsChi-kiu LO and Dekai WUHKUSTHuman Language Technology CenterDepartment of Computer Science and EngineeringHong Kong University of Science and Technology{jackielo,dekai}@cs.ust.hkAbstractWe present an unsupervised approach to esti-mate the appropriate degree of contribution ofeach semantic role type for semantic transla-tion evaluation, yielding a semantic MT eval-uation metric whose correlation with humanadequacy judgments is comparable to that ofrecent supervised approaches but without thehigh cost of a human-ranked training corpus.Our new unsupervised estimation approachis motivated by an analysis showing that theweights learned from supervised training aredistributed in a similar fashion to the relativefrequencies of the semantic roles.
Empiri-cal results show that even without a trainingcorpus of human adequacy rankings againstwhich to optimize correlation, using insteadour relative frequency weighting scheme toapproximate the importance of each semanticrole type leads to a semantic MT evaluationmetric that correlates comparable with humanadequacy judgments to previous metrics thatrequire far more expensive human rankings ofadequacy over a training corpus.
As a result,the cost of semantic MT evaluation is greatlyreduced.1 IntroductionIn this paper we investigate an unsupervised ap-proach to estimate the degree of contribution of eachsemantic role type in semantic translation evalua-tion in low cost without using a human-ranked train-ing corpus but still yields a evaluation metric thatcorrelates comparably with human adequacy judg-ments to that of recent supervised approaches as inLo and Wu (2011a, b, c).
The new approach ismotivated by an analysis showing that the distri-bution of the weights learned from the supervisedtraining is similar to the relative frequencies of theoccurrences of each semantic role in the referencetranslation.
We then introduce a relative frequencyweighting scheme to approximate the importance ofeach semantic role type.
With such simple weight-ing scheme, the cost of evaluating translation of lan-guages with fewer resources available is greatly re-duced.For the past decade, the task of measuring the per-formance of MT systems has relied heavily on lex-ical n-gram based MT evaluation metrics, such asBLEU (Papineni et al, 2002), NIST (Doddington,2002), METEOR (Banerjee and Lavie, 2005), PER(Tillmann et al, 1997), CDER (Leusch et al, 2006)and WER (Nie?en et al, 2000) because of their sup-port on fast and inexpensive evaluation.
These met-rics are good at ranking overall systems by averagingtheir scores over the entire document.
As MT sys-tems improve, the focus of MT evaluation changesfrom generally reflecting the quality of each systemto assisting error analysis on each MT output in de-tail.
The failure of such metrics in evaluating trans-lation quality on sentence level are becoming moreapparent.
Though containing roughly the correctwords, the MT output as a whole sentence is stillquite incomprehensible and fails to express mean-ing that is close to the input.
Lexical n-gram basedevaluation metrics are surface-oriented and do notdo so well at ranking translations according to ad-equacy and are particularly poor at reflecting sig-nificant translation quality improvements on moremeaningful word sense or semantic frame choiceswhich human judges can indicate clearly.
Callison-Burch et al (2006) and Koehn and Monz (2006)even reported cases where BLEU strongly disagreeswith human judgment on translation quality.49Liu and Gildea (2005) proposed STM, a struc-tural approach based on syntax to addresses the fail-ure of lexical similarity based metrics in evaluatingtranslation grammaticality.
However, a grammaticaltranslation can achieve a high syntax-based score butstill contains meaning errors arising from confusionof semantic roles.
On the other hand, despite thefact that non-automatic, manually evaluations, suchas HTER (Snover et al, 2006), are more adequacyoriented and show a high correlation with human ad-equacy judgment, the high labor cost prohibits theirwidespread use.
There was also work on explicitlyevaluating MT adequacy with aggregated linguisticfeatures (Gime?nez and Ma`rquez, 2007, 2008) andtextual entailment (Pado et al, 2009).In the work of Lo and Wu (2011a), MEANTand its human variants HMEANT were introducedand empirical experimental results showed thatHMEANT, which can be driven by low-cost mono-lingual semantic roles annotators with high inter-annotator agreement, correlates as well as HTERand far superior than BLEU and other surfaced ori-ented evaluation metrics.
Along with additional im-provements to the MEANT family of metrics, Loand Wu (2011b) detailed the studies of the impact ofeach individual semantic role to the metric?s corre-lation with human adequacy judgments.
Lo and Wu(2011c) further discussed that with a proper weight-ing scheme of semantic frame in a sentence, struc-tured semantic role representation is more accurateand intuitive than flattened role representation for se-mantic MT evaluation metrics.The recent trend of incorporating more linguisticfeatures into MT evaluation metrics raise the dis-cussion on the appropriate approach in weightingand combining them.
ULC (Gime?nez and Ma`rquez,2007, 2008) uses uniform weights to aggregate lin-guistic features.
This approach does not capture theimportance of each feature to the overall translationquality to the MT output.
One obvious example ofdifferent semantic roles contribute differently to theoverall meaning is that readers usually accept trans-lations with errors in adjunct arguments as a validtranslation but not those with errors in core argu-ments.
Unlike ULC, Liu and Gildea (2007); Lo andWu (2011a) approach the weight estimation prob-lem by maximum correlation training which directlyoptimize the correlation with human adequacy judg-Figure 1: HMEANT structured role representation with aweighting scheme reflecting the degree of contribution ofeach semantic role type to the semantic frame.
(Lo andWu, 2011a,b,c).ments.
However, the shortcomings of this approachis that it requires a human-ranked training corpuswhich is expensive, especially for languages withlimited resource.We argue in this paper that for semantic MT eval-uation, the importance of each semantic role typecan easily be estimated using a simple unsupervisedapproach which leverage the relative frequencies ofthe semantic roles appeared in the reference transla-tion.
Our proposed weighting scheme is motivatedby an analysis showing that the weights learnedfrom supervised training are distributed in a similarfashion to the relative frequencies of the semanticroles.
Our results show that the semantic MT eval-uation metric using the relative frequency weight-ing scheme to approximate the importance of eachsemantic role type correlates comparably with hu-man adequacy judgments to previous metrics thatuse maximum correlation training, which requiresexpensive human rankings of adequacy over a train-ing corpus.
Therefore, the cost of semantic MT eval-uation is greatly reduced.2 Semantic MT evaluation metricsAdopting the principle that a good translation is onefrom which human readers may successfully un-derstand at least the basic event structure-?who didwhat to whom, when, where and why?
(Pradhan etal., 2004)-which represents the most essential mean-ing of the source utterances, Lo and Wu (2011a,b,c)50proposed HMEANT to evaluate translation utilitybased on semantic frames reconstructed by humanreader of machine translation output.
Monolingual(or bilingual) annotators must label the semanticroles in both the reference and machine translations,and then to align the semantic predicates and rolefillers in the MT output to the reference translations.These annotations allow HMEANT to then look atthe aligned role fillers, and aggregate the transla-tion accuracy for each role.
In the spirit of Oc-cam?s razor and representational transparency, theHMEANT score is defined simply in terms of aweighted f-score over these aligned predicates androle fillers.
More precisely, HMEANT is defined asfollows:1.
Human annotators annotate the shallow seman-tic structures of both the references and MToutput.2.
Human judges align the semantic frames be-tween the references and MT output by judgingthe correctness of the predicates.3.
For each pair of aligned semantic frames,(a) Human judges determine the translationcorrectness of the semantic role fillers.
(b) Human judges align the semantic rolefillers between the reference and MT out-put according to the correctness of the se-mantic role fillers.4.
Compute the weighted f-score over the match-ing role labels of these aligned predicates androle fillers.mi ?#tokens filled in frame i of MTtotal #tokens in MTri ?#tokens filled in frame i of REFtotal #tokens in REFMi, j ?
total # ARG j of PRED i in MTRi, j ?
total # ARG j of PRED i in REFCi, j ?
# correct ARG j of PRED i in MTPi, j ?
# partially correct ARG j of PRED i in MTprecision =?i miwpred+?
j w j(Ci, j+wpartialPi, j)wpred+?
j w jMi, j?i mirecall =?i riwpred+?
j w j(Ci, j+wpartialPi, j)wpred+?
j w jRi, j?i riHMEANT =2?precision?
recallprecision+ recallwhere mi and ri are the weights for frame,i, in theMT/REF respectively.
These weights estimate thedegree of contribution of each frame to the overallmeaning of the sentence.
Mi, j and Ri, j are the to-tal counts of argument of type j in frame i in theMT/REF respectively.
Ci, j and Pi, j are the count ofthe correctly and partial correctly translated argu-ment of type j in frame i in the MT.
wpred is theweight for the predicate and wj is the weights for thearguments of type j.
These weights estimate the de-gree of contribution of different types of semanticroles to the overall meaning of the semantic framethey attached to.
The frame precision/recall is theweighted sum of the number of correctly translatedroles in a frame normalized by the weighted sumof the total number of all roles in that frame in theMT/REF respectively.
The sentence precision/recallis the weighted sum of the frame precision/recall forall frames normalized by the weighted sum of the to-tal number of frames in MT/REF respectively.
Fig-ure 1 shows the internal structure of HMEANT.In the work of Lo and Wu (2011b), the correla-tion of all individual roles with the human adequacyjudgments were found to be non-negative.
There-fore, grid search was used to estimate the weightsof each roles by optimizing the correlation with hu-man adequacy judgments.
This approach requiresan expensive human-ranked training corpus whichmay not be available for languages with sparse re-sources.Unlike the supervised training approach, ourproposed relative frequency weighting scheme doesnot require additional resource other than the SRLannotated reference translation.3 Which roles contribute more in thesemantic MT evaluation metric?We begin with an investigation that suggests that therelative frequency of each semantic role (which canbe estimated in unsupervised fashion without humanrankings) approximates fairly closely its importanceas determined by previous supervised optimizationapproaches.
Since there is no ground truth on which51Role Deviation (GALE-A) Deviation (GALE-B) Deviation (WMT12)Agent -0.09 -0.05 0.03Experiencer 0.23 0.05 0.02Benefactive 0.02 0.04 -0.01Temporal 0.11 0.08 0.03Locative -0.05 -0.05 -0.07Purpose -0.01 0.03 -0.01Manner -0.01 0.00 -0.01Extent -0.02 0.00 -0.01Modal ?
0.04 0.01Negation ?
0.01 -0.01Other -0.12 0.05 -0.01Table 1: Deviation of relative frequency from optimized weight of each semantic role in GALE-A, GALE-B andWMT12semantic role contribute more to the overall meaningin a sentence for semantic MT evaluation, we firstshow that the unsupervised estimation are close tothe weights obtained from the supervised maximumcorrelation training on a human-ranked MT evalua-tion corpus.
More precisely, the weight estimationfunction is defined as follows:c j ?
# count of ARG j in REF of the test setw j =c j?
j c j3.1 Experimental setupFor our benchmark comparison, the evaluation datafor our experiment is the same two sets of sentences,GALE-A and GALE-B that were used in Lo and Wu(2011b).
The translation in GALE-A is SRL an-notated with 9 semantic role types, while those inGALE-B are SRL annotated with 11 semantic roletypes (segregating the modal and the negation rolesfrom the other role).To validate whether or not our hypothesis is lan-guage independent, we also construct an evalua-tion data set by randomly selecting 50 sentencesfrom WMT12 English to Czech (WMT12) transla-tion task test corpus, in which 5 systems (out of13 participating systems) were randomly picked fortranslation adequacy ranking by human readers.
Intotal, 85 sets of translations (with translations fromsome source sentences appear more than once in dif-ferent sets) were ranked.
The translation in WMT12are also SRL annotated with the tag set as GALE-B,i.e., 11 semantic role types.The weights wpred, w j and wpartial were estimatedusing grid search to optimize the correlation againsthuman adequacy judgments.3.2 ResultsInspecting the distribution of the trained weights andthe relative frequencies from all three data sets, asshown in table 1, we see that the overall pattern ofweights from unsupervised estimation has a fairlysmall deviation from the those learned via super-vised optimization.
To visualize more clearly theoverall pattern of the weights from the two estima-tion methods, we show the deviation of the unsuper-vised estimation from the supervised estimation.
Adeviation of 0 for all roles would mean that unsu-pervised and supervised estimation produce exactlyidentical weights.
If the unsupervised estimation ishigher than the supervised estimation, the deviationwill be positive and vice versa.What we see is that in almost all cases, the de-viation between the trained weight and the relativefrequency of each role is always within the range [-0.1, 0.1].Closer inspection also reveals the following moredetailed patterns:?
The weight of the less frequent adjunct argu-ments (e.g.
purpose, manner, extent, modal andnegation) from the unsupervised estimation ishighly similar to that learned from the super-52PRED estimation Deviation (GALE-A) Deviation (GALE-B) Deviation (WMT12)Method (i) 0.16 0.16 0.31Method (ii) 0.02 0.01 0.01Table 2: Deviation from optimized weight in GALE-A, GALE-B and WMT12 of the predicate?s weight as estimatedby (i) frequency of predicates in frames, relative to predicates and arguments; and (ii) one-fourth of agent?s weight.vised maximum correlation training.?
The unsupervised estimation usually gives ahigher weight to the temporal role than the su-pervised training would.?
The unsupervised estimation usually gives alower weight to the locative role than the super-vised training would but the two weights fromthe two approach are still high similar to eachother, yielding a deviation within the range of[-0.07, 0.07].?
There is an obvious outlier found in GALE-Awhere the deviation of the relative frequencyfrom the optimized weight is unusually high.This suggests that the optimized weights inGALE-A may be at the risk of over-fitting thetraining data.4 Estimating the weight for the predicateThe remaining question left to be investigatedis how we are to estimate the importance of thepredicate in an unsupervised approach.
One obviousapproach is to treat the predicate the same way asthe arguments.
That is, just like with arguments,we could weight predicates by the relative fre-quency of how often predicates occur in semanticframes.
However, this does not seem well motivatedsince predicates are fundamentally different fromarguments: by definition, every semantic frame isdefined by one predicate, and arguments are definedrelative to the predicate.On the other hand, inspecting the weights on thepredicate obtained from the supervised maximumcorrelation training, we find that the weight of thepredicate is usually around one-fourth of the weightof the agent role.
More precisely, the two weightestimation functions are defined as follows:cpred ?
# count of PRED in REF of the test setMethod (i) =cpredcpred +?
j c jMethod (ii) = 0.25 ?wagentWe now show that the supervised estimation ofthe predicate?s weight is closely approximated byunsupervised estimation.4.1 Experimental setupThe experimental setup is the same as that used insection 3.4.2 ResultsThe results in table 2 show that the trained weightof the predicate and its unsupervised estimation ofone-fourth of the agent role?s weight are highly sim-ilar to each other.
In all three data sets, the devia-tion between the trained weight and the heuristic ofone-fourth of the agent?s weight is always within therange [0.1, 0.2].On the other hand, treating the predicate the sameas arguments by estimating the unsupervised weightusing relative frequency largely over-estimates andhas a large deviation from the weight learned fromsupervised estimation.5 Semantic MT evaluation usingunsupervised weight estimatesHaving seen that the weights of the predicate andsemantic roles estimated by the unsupervised ap-proach fairly closely approximate those learnedfrom the supervised approach, we now show that theunsupervised approach leads to a semantic MT eval-uation metric that correlates comparably with hu-man adequacy judgments to one that is trained ona far more expensive human-ranked training corpus.5.1 Experimental setupFollowing the benchmark assessment in NIST Met-ricsMaTr 2010 (Callison-Burch et al, 2010), we as-sess the performance of the semantic MT evaluation53Metrics GALE-A GALE-B WMT12HMEANT (supervised) 0.49 0.27 0.29HMEANT (unsupervised) 0.42 0.23 0.20NIST 0.29 0.09 0.12METEOR 0.20 0.21 0.22TER 0.20 0.10 0.12PER 0.20 0.07 0.02BLEU 0.20 0.12 0.01CDER 0.12 0.10 0.14WER 0.10 0.11 0.17Table 3: Average sentence-level correlation with human adequacy judgments of HMEANT using supervised andunsupervised weight scheme on GALE-A, GALE-B and WMT12, (with baseline comparison of commonly usedautomatic MT evaluation metric.metric at the sentence level using Kendall?s rankcorrelation coefficient which evaluate the correla-tion of the proposed metric with human judgmentson translation adequacy ranking.
A higher the valuefor indicates a higher similarity to the ranking bythe evaluation metric to the human judgment.
Therange of possible values of correlation coefficient is[-1,1], where 1 means the systems are ranked in thesame order as the human judgment and -1 means thesystems are ranked in the reverse order as the hu-man judgment.
For GALE-A and GALE-B, the hu-man judgment on adequacy was obtained by show-ing all three MT outputs together with the Chinesesource input to a human reader.
The human readerwas instructed to order the sentences from the threeMT systems according to the accuracy of meaning inthe translations.
For WMT12, the human adequacyjudgments are provided by the organizers.The rest of the experimental setup is the same asthat used in section 3.5.2 ResultsTable 3 shows that HMEANT with the proposed un-supervised semantic role weighting scheme corre-late comparably with human adequacy judgments tothat optimized with a more expensive human-rankedtraining corpus, and, outperforms all other com-monly used automatic metrics (except for METEORin Czech).
The results from GALE-A, GALE-B andWMT12 are consistent.
These encouraging resultsshow that semantic MT evaluation metric could bewidely applicable to languages other than English.6 ConclusionWe presented a simple, easy to implement yet well-motivated weighting scheme for HMEANT to esti-mate the importance of each semantic role in eval-uating the translation adequacy.
Unlike the previ-ous metrics, the proposed metric does not requirean expensive human-ranked training corpus and stilloutperforms all other commonly used automatic MTevaluation metrics.
Interestingly, the distribution ofthe optimal weights obtained by maximum correla-tion training, is similar to the relative frequency ofoccurrence of each semantic role type in the refer-ence translation.
HMEANT with the new weight-ing scheme showed consistent results across differ-ent language pairs and across different corpora inthe same language pair.
With the proposed weight-ing scheme, the semantic MT evaluation metric isready to be used off-the-shelf without depending ona human-ranked training corpus.
We believe that ourcurrent work reduces the barrier for semantic MTevaluation for resource scarce languages sufficientlyso that semantic MT evaluation can be applied tomost other languages.AcknowledgmentsWe would like to thank Ondr?ej Bojar and all theannotators from the Charles University in Praguefor participating in the experiments.
This ma-terial is based upon work supported in part bythe Defense Advanced Research Projects Agency(DARPA) under BOLT contract no.
HR0011-12-C-0016, and GALE contract nos.
HR0011-06-C-0022 and HR0011-06-C-0023; by the Eu-54ropean Union under the FP7 grant agreementno.
287658; and by the Hong Kong ResearchGrants Council (RGC) research grants GRF621008,GRF612806, DAG03/04.EG09, RGC6256/00E, andRGC6083/99E.
Any opinions, findings and conclu-sions or recommendations expressed in this materialare those of the authors and do not necessarily reflectthe views of the RGC, EU, or DARPA.ReferencesSatanjeev Banerjee and Alon Lavie.
METEOR: An Auto-matic Metric for MT Evaluation with Improved Corre-lation with Human Judgments.
In Proceedings of the43th Annual Meeting of the Association of Computa-tional Linguistics (ACL-05), pages 65?72, 2005.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
Re-evaluating the role of BLEU in MachineTranslation Research.
In Proceedings of the 13th Con-ference of the European Chapter of the Associationfor Computational Linguistics (EACL-06), pages 249?256, 2006.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Pryzbocki, and Omar Zaidan.Findings of the 2010 Joint Workshop on StatisticalMachine Translation and Metrics for Machine Transla-tion.
In Proceedings of the Joint 5th Workshop on Sta-tistical Machine Translation and MetricsMATR, pages17?53, Uppsala, Sweden, 15-16 July 2010.G.
Doddington.
Automatic Evaluation of Machine Trans-lation Quality using N-gram Co-occurrence Statistics.In Proceedings of the 2nd International Conferenceon Human Language Technology Research (HLT-02),pages 138?145, San Francisco, CA, USA, 2002.
Mor-gan Kaufmann Publishers Inc.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
Linguistic Featuresfor Automatic Evaluation of Heterogenous MT Sys-tems.
In Proceedings of the 2nd Workshop on Sta-tistical Machine Translation, pages 256?264, Prague,Czech Republic, June 2007.
Association for Computa-tional Linguistics.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
A Smorgasbord ofFeatures for Automatic MT Evaluation.
In Proceed-ings of the 3rd Workshop on Statistical Machine Trans-lation, pages 195?198, Columbus, OH, June 2008.
As-sociation for Computational Linguistics.Philipp Koehn and Christof Monz.
Manual and Auto-matic Evaluation of Machine Translation between Eu-ropean Languages.
In Proceedings of the Workshop onStatistical Machine Translation, pages 102?121, 2006.Gregor Leusch, Nicola Ueffing, and Hermann Ney.
CDer:Efficient MT Evaluation Using Block Movements.
InProceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL-06), 2006.Ding Liu and Daniel Gildea.
Syntactic Features for Eval-uation of Machine Translation.
In Proceedings of theACL Workshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/or Summariza-tion, page 25, 2005.Ding Liu and Daniel Gildea.
Source-Language Fea-tures and Maximum Correlation Training for MachineTranslation Evaluation.
In Proceedings of the 2007Conference of the North American Chapter of the As-sociation of Computational Linguistics (NAACL-07),2007.Chi-kiu Lo and Dekai Wu.
MEANT: An Inexpensive,High-Accuracy, Semi-Automatic Metric for Evaluat-ing Translation Utility based on Semantic Roles.
InProceedings of the Joint conference of the 49th AnnualMeeting of the Association for Computational Linguis-tics : Human Language Technologies (ACL-HLT-11),2011.Chi-kiu Lo and Dekai Wu.
SMT vs. AI redux: How se-mantic frames evaluate MT more accurately.
In Pro-ceedings of the 22nd International Joint Conferenceon Artificial Intelligence (IJCAI-11), 2011.Chi-kiu Lo and Dekai Wu.
Structured vs. Flat SemanticRole Representations for Machine Translation Evalu-ation.
In Proceedings of the 5th Workshop on Syn-tax and Structure in Statistical Translation (SSST-5),2011.Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Her-mann Ney.
A Evaluation Tool for Machine Transla-tion: Fast Evaluation for MT Research.
In Proceed-ings of the 2nd International Conference on LanguageResources and Evaluation (LREC-2000), 2000.Sebastian Pado, Michel Galley, Dan Jurafsky, and ChrisManning.
Robust Machine Translation Evaluationwith Entailment Features.
In Proceedings of the Jointconference of the 47th Annual Meeting of the Associ-ation for Computational Linguistics and the 4th Inter-national Joint Conference on Natural Language Pro-cessing of the Asian Federation of Natural LanguageProcessing (ACL-IJCNLP-09), 2009.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
BLEU: A Method for Automatic Evaluationof Machine Translation.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL-02), pages 311?318, 2002.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H.Martin, and Dan Jurafsky.
Shallow Semantic ParsingUsing Support Vector Machines.
In Proceedings of55the 2004 Conference on Human Language Technologyand the North American Chapter of the Association forComputational Linguistics (HLT-NAACL-04), 2004.Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
A Study of Trans-lation Edit Rate with Targeted Human Annotation.
InProceedings of the 7th Conference of the Associationfor Machine Translation in the Americas (AMTA-06),pages 223?231, 2006.Christoph Tillmann, Stephan Vogel, Hermann Ney,Arkaitz Zubiaga, and Hassan Sawaf.
AcceleratedDP Based Search For Statistical Translation.
In Pro-ceedings of the 5th European Conference on SpeechCommunication and Technology (EUROSPEECH-97),1997.56
