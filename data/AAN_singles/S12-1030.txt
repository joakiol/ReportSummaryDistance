First Joint Conference on Lexical and Computational Semantics (*SEM), pages 218?227,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsAligning Predicate Argument Structures in Monolingual Comparable Texts:A New Corpus for a New TaskMichael Roth and Anette FrankDepartment of Computational LinguisticsHeidelberg UniversityGermany{mroth,frank}@cl.uni-heidelberg.deAbstractDiscourse coherence is an important aspect ofnatural language that is still understudied incomputational linguistics.
Our aim is to learnfactors that constitute coherent discourse fromdata, with a focus on how to realize predicate-argument structures (PAS) in a model that ex-ceeds the sentence level.
In particular, we aimto study the case of non-realized argumentsas a coherence inducing factor.
This task canbe broken down into two subtasks.
The firstaligns predicates across comparable texts, ad-mitting partial argument structure correspon-dence.
The resulting alignments and their con-texts can then be used for developing a coher-ence model for argument realization.This paper introduces a large corpus of com-parable monolingual texts as a prerequisite forapproaching this task, including an evaluationset with manual predicate alignments.
We il-lustrate the potential of this new resource forthe empirical investigation of discourse coher-ence phenomena.
Initial experiments on thetask of predicting predicate alignments acrosstext pairs show promising results.
Our findingsestablish that manual and automatic predicatealignments across texts are feasible and thatour data set holds potential for empirical re-search into a variety of discourse-related tasks.1 IntroductionResearch in the fields of discourse and pragmaticshas led to a number of theories that try to explain andformalize the effect of discourse coherence induc-ing elements either locally or globally.
For exam-ple, Centering Theory (Grosz et al, 1995) providesa framework to model local coherence by relatingthe choice of referring expressions to the salience ofan entity at certain stages of a discourse.
An exam-ple for a global coherence model would be Rhetori-cal Structure Theory (Mann and Thompson, 1988),which addresses overall text structure by means ofcoherence relations between the parts of a text.In addition to such theories, computational ap-proaches have been proposed to capture correspond-ing phenomena empirically.
A prominent exampleis the entity-based model by Barzilay and Lapata(2008).
In their approach, local coherence is mod-eled by the observation of sentence-to-sentence re-alization patterns of individual entities.
The learnedmodel reflects a key idea from Centering Theory,namely that adjacent sentences in a coherent dis-course are likely to involve the same entities.One shortcoming of Barzilay and Lapata?s model(and extensions of it) is that it only investigates overtrealization patterns in terms of grammatical func-tions.
These functions reflect explicit realizations ofpredicate argument structures (PAS), but they do notcapture the full range of salience factors.
In partic-ular, the model does not reflect the importance ofdiscourse entities that fill core roles of the predicate,but that remain implicit in the predicate?s local argu-ment structure.
We develop a specific set-up that al-lows us to further investigate the factors that governsuch a null-instantiations of argument positions (cf.Fillmore et al (2003)), as a special form of coher-ence inducing element in discourse.
We henceforthrefer to such cases as non-realized arguments.Our main hypothesis is that context specific re-alization patterns for PAS can be automatically218learned from a semantically parsed corpus of com-parable text pairs.
This assumption builds onthe success of previous research, where compara-ble and parallel texts have been exploited for arange of related learning tasks, e.g., unsuperviseddiscourse segmentation (Barzilay and Lee, 2004)and bootstrapping semantic analyzers (Titov andKozhevnikov, 2010).For our purposes, we are interested in finding cor-responding PAS across comparable texts that areknown to talk about the same events, and hence in-volve the same set of underlying event participants.By aligning predicates in such texts, we can investi-gate the factors that determine discourse coherencein the realization patterns for the involved partici-pants.
As a first step towards this overall goal, wedescribe the construction of a resource that containsmore than 160,000 document pairs that are known totalk about the same events and participants.
Exam-ple (1), extracted from our corpus of aligned texts,illustrates this point: Both texts report on the sameevent, in particular the (aligned) event of locatingvictims in an avalanche.
While (1.a) explicitly talksabout the location of this event, the role remains im-plicit in the second sentence of (1.b), given that itcan be recovered from the preceding sentence.
Infact, realization of this argument would impede thefluency of discourse by being overly repetitive.
(1) a. .
.
.
The official said that [no bodies]Arg1 hadbeen recovered [from the avalanches]Arg2 whichoccurred late Friday in the Central Asian coun-try near the Afghan border some 300 kilometers(185 miles) southeast of the capital Dushanbe.b.
Three other victims were trapped in anavalanche in the village of Khichikh.
[Noneof the victims bodies]Arg1 have been found[ ]Argm-loc.Our aim is to identify comparable predicationsacross pairs of texts, and to study the coherencefactors that determine the realization patterns of ar-gument structures (including roles that remain im-plicit) in discourse.
This can be achieved by consid-ering the full set of arguments that can be recoveredfrom the aligned predications, including both coreand non-core (i.e.
adjunct) roles.
However, in orderto relate PAS across texts to one another, we firstneed to identify corresponding predicates.In this paper, we construct a large data set to beused for the induction of a coherence model for ar-gument structure realization and related tasks.
Wediscuss the prospects of this data set for the studyof coherence factors in PAS realization.
Finally, wepresent first results on the initial task of predicatealignment across comparable monolingual texts.The remainder of this paper is structured as fol-lows: In Section 2, we discuss previous work in re-lated tasks.
Section 3 introduces the new task to-gether with a description of how we prepared a suit-able data set.
Section 4 discusses the potential bene-fits of the created resource in more detail.
Section 5presents experiments on predicate alignment usingthis new data set and outlines first results.
Finally,we conclude in Section 6 and discuss future work.2 Related WorkData sets comprising parallel texts have been re-leased for various different tasks, including para-phrase extraction and statistical machine translation(SMT).
While corpora for SMT are typically mul-tilingual (e.g.
Europarl, Koehn (2005)), there alsoexist monolingual parallel corpora that consist ofmultiple translations of one text into the same lan-guage (Barzilay and McKeown, 2001; Huang etal., 2002, inter alia).
Each translation can pro-vide alternative verbalizations of the same eventsbut little variation can be observed in context, asthe overall discourse remains the same.
A higherdegree of variation can be found in the MicrosoftResearch Paraphrase Corpus (e.g.
MSRPC, Dolanand Brockett (2005)), which consists of paraphrasesautomatically extracted from different sources.
Inthe MSRPC, however, original discourse contextsare not provided for each sentence.
In contrast totruly parallel monolingual corpora, there also exista range of comparable corpora that have been usedfor tasks such as (multi-document) summarization(McKeown and Radev, 1995, inter alia).
Corpora forthis task are collected manually and hence are rathersmall.
Our work presents a method to automaticallyconstruct a large corpus of text pairs describing thesame underlying events.In this novel corpus, we identify common eventsacross texts and investigate the argument structuresthat were realized in each context to establish a co-219herent discourse.
Different aspects related to thissetting have been studied in previous work.
For ex-ample, Filippova and Strube (2007) and Cahill andRiester (2009) examine factors that determine con-stituent order and Belz et al (2009) study the con-ditions for the use of different types of referring ex-pressions.
The specific set-up we examine allowsus to further investigate the factors that govern thenon-realization of an argument position, as a specialform of coherence inducing element in discourse.As in the aforementioned work, we are specificallyinterested in the generation of coherent discourses(e.g.
for summarization).
Yet, our work also com-plements research in discourse analysis.
A recentexample for such work is the Semeval 2010 Task 10(Ruppenhofer et al, 2010), which aims at linkingevents and their participants in discourse.
The pro-vided data sets for this task, however, are criticallysmall (438 train and 525 test sentences).
Eventu-ally, the corpus we present in this paper could alsobe beneficial for data-driven approaches to role link-ing in discourse.3 A Corpus for Aligning Predicationsacross Comparable TextsOur aim is to construct a corpus of comparable textsthat can be assumed to be about the same events,but include variation in textual presentation.
This re-quirement fits well with the news domain, for whichwe can trace varying textual sources for the sameunderlying events.The English Gigaword Fifth Edition (Parker et al,2011) corpus (henceforth just Gigaword) is one ofthe largest corpus collections for English.
It com-prises a total of 9.8 million newswire articles fromseven distinct sources.
For construction of our cor-pus we make use of all combinations of agency pairsin Gigaword.3.1 Corpus CreationIn order to extract pairs of articles describing thesame news event, we implemented the pairwise sim-ilarity method presented by Wubben et al (2009).The method is based on measuring word overlap innews headlines, weighting each word by its TF*IDFscore to give a higher impact to words occurringwith lower frequency.
As our focus is to providea high-quality data set for predicate alignment andfollow-up tasks, we impose an additional date con-straint to favor precision over recall.
We apply thisconstraint by requiring a pair of articles to be pub-lished within a two-day time frame in order to beconsidered as pairs of comparable news items.Following this two-step procedure, we extracted atotal of 167,728 document pairs, an overall collec-tion of 50 million word tokens.
We inspected about100 randomly selected document pairs and foundonly two of them describing different events.
Thisis in line with the results of Wubben et al who re-ported a precision of 93% without explicitly impos-ing a date constraint.
Overall, we found that mosttext pairs share a high degree of similarity and varyonly in length (up to 7.564 words with a mean andmedian of 301 and 213 words, respectively) and de-tail.
Closer examination of a development set of10 document pairs (described below) revealed thatwe can indeed find multiple cases where roles arenot locally filled in predicate argument structures.We show instances of this phenomenon, in whichaligned PAS help to resolve implicit role references,in Section 4.3.2 Gold Standard AnnotationWe pre-processed all texts using MATE tools(Bohnet, 2010; Bjo?rkelund et al, 2010), a pipelineof natural language processing modules including astate-of-the-art semantic role labeler that computesProp/NomBank annotations (Palmer et al, 2005;Meyers et al, 2008).
The output was used to providepre-labeled verbal and nominal predicates for anno-tation.
We asked two students1 to tag alignmentsof corresponding predicates in 70 text pairs derivedfrom the created corpus.
All document pairs wererandomly chosen from the AFP and APW sectionsof Gigaword with the constraint that each text con-sists of 100 to 300 words2.
We chose this constraintas longer text pairs contain a high number of unre-lated predicates, making this task difficult to managefor the annotators.Sure and possible links.
Following standard prac-tice in word alignment tasks (cf.
Cohn et al (2008))1Both annotators are students in Computational Linguistics,one undergraduate (A) and one postgraduate (B) student.2This constraint is satisfied by 75.3% of the documents.220the annotators were instructed to distinguish be-tween sure (S) and possible (P) alignments, depend-ing on how certainly, in their opinion, two predi-cates (including their arguments) describe the sameevent.
The following examples show cases of predi-cate pairings marked as sure (S link) (2) and as pos-sible (P link) alignments (3):(2) a.
The regulator ruled on September 27 that Nas-daq too was qualified to bid for OMX [.
.
.
]3b.
The authority [.
.
. ]
had already approved a sim-ilar application by Nasdaq.4(3) a. Myanmar?s military government said earlier thisyear it has released some 220 political prisoners[.
.
.
]5b.
The government has been regularly releasingmembers of Suu Kyi?s National League forDemocracy party [.
.
.
]6Replaceability.
As a guideline for decidingwhether two predicates are to be aligned, theannotators were given the following two criteria: 1)whether the predicates are replaceable in a givencontext and 2) whether they share (potentiallyimplicit) arguments.Missing context.
In case one text does not provideenough context to decide whether two predicates inthe paired documents refer to the same event, analignment should not be marked as sure.Similar predicates.
Annotators were told explic-itly that sure links can be used even if two predicatesare semantically different but have the same mean-ing in context.
Example (4) illustrates such a case:(4) a.
The volcano roared back to life two weeks ago.b.
It began erupting last month.1-to-1 vs. n-to-m. We asked the annotators to findas many 1-to-1 correspondences as possible and toprefer 1-to-1 matches over n-to-m alignments.
Incase of multiple mentions (cf.
Example (5)) of thesame event, we further asked the annotators to pro-vide only one S link per predicate and mark remain-ing cases as P links.
If possible, the S link should3Source document ID: AFP ENG 20071112.02354Source document ID: APW ENG 20071112.06455Source document ID: AFP ENG 20020301.00416Source document ID: APW ENG 20020301.0132be used for the pairing of PAS with the highest in-formation overlap (e.g.
?performa3???performb2?
in(5)).
If there is no difference in information over-lap, the predicate pair that occurs first in both textsshould be marked as a sure alignment (e.g.
?singa1???performb1?
in (5)).
The intuition behind this guide-line is that the first mention introduces the actualevent while later mentions just (co-)refer or add fur-ther information.
(5) a. Susan Boyle said she will singa1 in front ofBritain?s Prince Charles (.
.
. )
?It?s going to bea privilege to be performinga2 before His RoyalHighness,?
the singer said (.
.
. )
British copy-right laws will allow her to performa3 the hit infront of the prince and his wife.7b.
British singing sensation Susan Boyle is goingto performb1 for Prince Charles (.
.
. )
The showstar will performb2 her version of Perfect Dayfor Charles and his wife Camilla.83.3 Development and Evaluation Data SetsIn total, the annotators (A/B) aligned 487/451 sureand 221/180 possible alignments with a Kappa score(Cohen, 1960) of 0.86.
Following Brockett (2007),we computed agreement on labeled annotations, in-cluding unaligned predicate pairs as an additionalnull category.
For the construction of a gold stan-dard, we merged the alignments from both annota-tors by taking the union of all possible alignmentsand the intersection of all sure alignments.
Caseswhich involved a sure alignment on which the anno-tators disagreed were resolved in a group discussionwith the first author.
We split the final corpus into adevelopment set of 10 document pairs and a test setof 60 document pairs.Table 1 summarizes information about the result-ing annotations in the development and test sets,respectively.
It gives information about the pairedtexts (PT): number of predicates marked in prepro-cessing (nouns and verbs), the set of manual predi-cate alignments (PA): sure and possible, as well asinformation about whether they were annotated forpredicates of the same PoS (N,V) or lemma.Finally, as a rough indicator for diverging ar-gument structures captured in the annotated align-7Source document ID: AFP ENG 20101102.00288Source document ID: APW ENG 20101102.0923221Dev Set Test Setnb.
of PT 10 60nb.
marked predicates 395 3,453nb.
marked nouns 168 1,531nb.
marked verbs 227 1,922sure PA/PT: avg.
(total) 3.9 (35) 7.4 (446)poss.
PA/PT: avg.
(total) 4.8 (43) 6.0 (361)same PoS in PA (N/V) 88.5% (24/42) 82.4% (242/423)same lemma in PA 53.8% (42) 47.5% (383)unequal nb.
args in PA 30.8% (24) 39.7% (320)Table 1: Information on Paired Texts (PT) and manualPredicate Alignments (PA) in development and test setments, we analyzed the number of PAs that involvea different number of arguments.4 Potential of AggregationIn this section, we analyze the predicate alignmentsin our manually annotated data set, to illustrate thepotential of aggregating corresponding PAS acrosscomparable texts.We are particularly interested in cases of non-realization of arguments, and thus take a closer lookat alignments involving roles that are not filled intheir local PAS.
We extract a subset of such casesby extracting pairs of aligned predicates that con-tain a different number of realized arguments.
Wedeliberately focus on the more restricted core rolesin this exposition, but will consider the full rangeof roles for developing a comprehensive coherencemodel for argument structure realization.9 Our se-lection of alignment examples is drawn from the de-velopment set.The following excerpts are from a pair of com-parable texts describing a news report on Chadianrefugees crossing into Nigeria:(6) a.
The Chadians said [they]Arg0 had fled [ ]Arg1 infear of their lives.10b.
The United Nations says[some 20,000 refugees]Arg0 have fled[into Cameroon]Arg1.11In both examples, the Arg0 role of the predicate fledis filled, but Arg1 has not been realized in (6.a).
Note9Accordingly, the number of PAs involving diverging rolerealizations in Table 1 is strongly underestimated.10Source document ID: AFP ENG 20080205.023011Source document ID: APW ENG 20080206.0766that the sentence is still part of a coherent discourseas fillers for the omitted role can be inferred fromthe preceding discourse context.
Aggregating thealigned PAS presents an effective means to identifysuch appropriate fillers.Example (7) presents another text pair, reportingon elections in Iraq, in which role realizations differfor the same hold event.
(7) a.
He said (.
.
. )
[elections]Arg1 will be held [ ]Arg0to form a government.12b.
The president (.
.
. )
said Wednesday[his country]Arg0 will definitely hold[elections]Arg1 in 2004.13Here, the changes in argument realization goalong with a diathesis alternation, while the pair in(6) exemplifies a case of lexical licensing for omis-sion of a role.14Example (8.b) illustrates a case in which the Arg1of a decline event is involved in a preceding pred-ication (rise) and thus has already been overtly re-alized.
The constructional properties of the subse-quent predicates decline as a participle and noun, re-spectively, are more adverse to overt realization ofthe Arg1 role.
Suppression of Arg1 in such casesyields a much more coherent discourse as comparedto their realization.
This is brought out by the con-structed examples in (a?/b?
), which are both highlyrepetitive.
(8) a.
The closely watched [index]Arg1 rose to 93.7. .
.
after declining for .
.
.
months.15a?.
?
.
.
.
after the index declining for .
.
.
months.b.
Consumer confidence rose .
.
.
following threemonths of dramatic decline [ ]Arg1.16b?.
?
.
.
.
following three months of dramatic decline[of consumer confidence]Arg1.As showcased by the previous examples, the de-cision on whether to realize a role filler in a lo-cal PAS can be rather complex.
Obviously, the12Source document ID: AFP ENG 20031015.035313Source document ID: APW ENG 20031015.023614These different configurations are termed constructionalvs.
lexical licensors in the SemEval 2010 Task 10 (Ruppen-hofer et al, 2010).15Source document ID: AFP ENG 20011228.036516Source document ID: APW ENG 20011228.0572222Figure 1: The predicates of two sentences (white: ?The company has said it plans to restate its earnings for 2000through 2002.?
; gray: ?The company had announced in January that it would have to restate earnings (.
.
.
)?)
from theMicrosoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts.above instances do not provide exhaustive informa-tion for grounding all such decisions.
A comprehen-sive model of discourse coherence will need to esti-mate the argument realization potential of differentpredicates and roles from larger corpora.
But as canbe seen from the discussed examples, training a se-mantic model with suitable discourse features on allpredicate argument structures in a large corpus suchas ours will provide indicative range of realizationdecisions.5 ExperimentsThis section presents an initial experiment using anunsupervised graph-based clustering method for thetask of aligning predicates across comparable texts.We describe the alignment model, two baselines aswell as the experimental setting and results.175.1 Clustering ModelSimilarity Measures.
We define a number of sim-ilarity measures between predicates, which makeuse of complementary lexical information.
Onesource of information are token-based frequencycounts, which we compute over all documentsfrom the AFP and APW sections of Gigaword18.Given two lemmatized predicates and their respec-tive PAS, we employ the following four similarity17The technicalities of this model, including detailed def-initions of the similarity measures, are described elsewhere(manuscript, under submission).18These sections make up 56.6% of documents in Gigaword.measures: Similarity in WordNet (simWN) and Verb-Net (simVN), distributional similarity (simDist) andbag-of-word similarity of arguments (simArgs).
Thefirst three measures are type-based, whereas the lat-ter is token-based.Graph Representation.
The input for graph clus-tering is a bi-partite graph representation for pairsof texts to be predicate-aligned.
In this graph, eachnode represents a PAS that was assigned during pre-processing (cf.
Section 3).
Edges are inserted be-tween pairs of predicates that are from two distincttexts.
A weight is assigned to each edge by a com-bination of the introduced similarity measures.Clustering algorithm.
The graph clusteringmethod uses minimum cuts (or Mincuts) in orderto partition the bipartite text graph into clusters ofaligned predicates.
Each Mincut operation dividesa graph into two disjoint sub-graphs, such that thesum of weights of removed edges will be minimal.As the goal is to induce clusters consisting of pairsof similar predicates, a maximum number of twonodes per cluster is set as stopping criterion.
Weapply Mincut recursively to the input graph andresulting sub-graphs until we reach the stoppingcriterion.
Figure 1 shows an example of a graphclustered by the Mincut approach.5.2 SettingWe perform evaluations of the graph-based align-ment model (henceforth called Clustering) on the223task of inducing predicate alignments across com-parable monolingual texts.
We evaluate on the man-ually annotated gold alignments in the test data setdescribed in Section 3.2.Parameter Tuning.
As the graph representationbecomes rather inefficient to handle using edges be-tween all predicate pairs, we use the developmentset of 10 text pairs to estimate a threshold for addingedges.
We found the best similarity threshold to bean edge weight of 2.5.
Note that the edge weights arecalculated as a weighted linear combination of fourdifferent similarity measures.
Subsequently, we alsotune the weighting scheme for similarity measureson the development set.
We found the best perform-ing combination of weights to be 0.09, 0.19, 0.48and 0.24 for simWN, simVN, simDist and simArgs, re-spectively.Baselines.
A simple baseline for this task is toalign all predicates whose lemmas are identical(SameLemma).
As a more sophisticated baseline,we make use of alignment tools commonly used instatistical machine translation (SMT).
We train ourown word alignment model using the state-of-the-arttool Berkeley Aligner (Liang et al, 2006).
As wordalignment tools require pairs of sentences as input,we first extract paraphrases for this baseline using are-implementation of the paraphrase detection sys-tem by Wan et al (2006).
In the following sections,we abbreviate this model as WordAlign.5.3 ResultsFollowing Cohn et al (2008) we measure precisionas the number of predicted alignments also anno-tated in the gold standard divided by the total num-ber of predictions.
Recall is measured as the num-ber of correctly predicted sure alignments devidedby the total number of sure alignments in the goldstandard.
We subsequently compute the F1-score asthe harmonic mean between precision and recall.Table 2 presents the results for our model andthe two baselines.
From all four approaches,WordAlign performs worst.
We identify two mainreasons for this: On the one hand, the paraphrasedetection does not perform perfectly.
Hence, theextracted sentence pairs do not always contain goldalignments.
On the other hand, even sentence pairsthat contain gold alignments are generally less paral-Precision Recall F1WordAlign 19.7% 15.2% 17.2%SameLemma 40.3% 60.3% 48.3%Clustering 59.7% 50.7% 54.8%Table 2: Results for all models on our test set; significantimprovements (p<0.005) over the results given in eachprevious line are marked in bold face.lel compared to a typical SMT setting, which makesthem harder to align.We observe that the majority of all sure align-ments (60.3%) can be retrieved by applying theSameLemma model, yet at a low precision (40.3%).While the Clustering model only recalls 50.7% ofall cases, it clearly outperforms SameLemma interms of precision (+19.4% points), an importantfactor for us as we plan to use the alignments insubsequent tasks.
With 54.8%, Clustering alsoachieves the best overall F1-score.
We computedstatistical significance of result differences with apaired t-test (Cohen, 1995), yielding significance atthe 99.5% level for precision and F1-score.5.4 Analysis of ResultsWe perform an analysis of the output of the Clus-tering model on the development set to categorizecorrect and incorrect alignment decisions.19 In to-tal, the model missed 13 out of 35 sure alignments(Type I errors) and predicted 23 alignments not an-notated in the gold standard (Type II errors).
SixType I errors (46%) occurred when the lemma of anaffected predicate occurred more than once in a textand the model missed the correct link.
Vice versa,we find 18 Type II errors (78%) that were made be-cause of a high predicate similarity despite low ar-gument overlap.
An example is given in (9).
(9) a.
The US alert (.
.
. )
followed intelligence reportsthat .
.
.
20b.
The Foreign Ministry announcement called onJapanese citizens to be cautious .
.
.
21While argument overlap itself can be low even forcorrect alignments, the results clearly indicate that19We decided to leave the test set untouched for further exper-iments.
Due to parameter tuning, the results on the developmentset alo provide us with an upper bound of the proposed model.20Source document ID: AFP ENG 20101004.036721Source document ID: APW ENG 20101004.0207224a better integration of context is necessary: Exam-ple (10.a) illustrates a case in which the agent of awarning event is not realized.
Here, contextual in-formation is required to correctly align it to one ofthe warning events in (10.b).
This involves inferencebeyond the local PAS.
(10) a.
The US alert (.
.
. )
is one step down from a full[travel]Arg1 warning [ ]Arg0.20b.
Japan has issued a travel alert .
.
.
(which)follows similar warnings [from Ameri-can and British authorities]Arg0.
(.
.
. )
An offi-cial said it was highly unusual for [Tokyo]Arg0to issue such a warning .
.
.
21On the positive side, Clustering achieves a precisionof 61.4% and a recall of 65.7% on the developmentset.
Example (11) shows a correctly aligned PASpair that involves non-realized arguments:(11) a. .
.
.
the Governing Council has established[a committee]Arg0 to draft [a constitution]Arg1.22b.
A .. resolution calls on the GoverningCouncil for elections and the drafting [ ]Arg0[of a new constitution]Arg1.23In (11.a), the follow-up sentences will refer back tothe committee that will draft the new Iraqi constitu-tion, hence the institution has to be introduced in thediscourse at this point.
In contrast, excerpt (11.b) isthe last sentence of a news report.
Since it presentsa summary, introducing new (omissible) entities atthis point would not concord with general coherenceprinciples.6 ConclusionIn this paper, we presented a novel corpus of compa-rable texts that provides full discourse contexts foralternative verbalizations.
The motivation for theconstruction of this corpus is to acquire empiricaldata for studying discourse coherence factors relatedto argument structure realization.
A special phe-nomenon we are interested in are discourse-relatedfactors that license the omission of argument roles.Our data set satisfies two conditions that are es-sential for the purported task: the texts are about22Source document ID: AFP ENG 20031015.035323Source document ID: APW ENG 20031015.0236.the same events and constitute alternative verbaliza-tions.
Selected from the Gigaword corpus, the doc-uments pertain to the news domain, and satisfy thefurther constraint that we have access to the full sur-rounding discourse context.
The constructed corpuscould thus be profitable for a range of other tasksthat need to investigate factors for knowledge aggre-gation, such as summarization, or inference in dis-course, such as textual entailment.In total, we derived more than 160,000 documentpairs from all pairwise combinations of newswiresources in the English Gigaword Fifth Edition.
Us-ing a subset of these pairs, we constructed a devel-opment and an evaluation data set with gold align-ments that relate predications with (possibly partial)PAS correspondence.
We established that the anno-tation task, while difficult, can be performed withgood inter-annotator agreement (?
at 0.86).We presented first experiments on the task of au-tomatically predicting predicate alignments.
Thisstep is essential to gather empirical evidence of dif-ferent PAS realizations for the same event, givenvarying discourse contexts.
Analysis of the datashows that the aligned predications capture a widevariety of sources and variations of coherence ef-fects, including constructional, lexical and discoursephenomena.In future work, we will enhance our model by in-corporating more refined semantic similarity mea-sures including discourse-based criteria for estab-lishing cross-document alignments.
Given that ourdata set includes sets of aligned documents fromseveral newswire sources, we will explore transitiv-ity constraints across multiple document pairs in or-der to further enhance the precision of the alignmentmodel.
We will then proceed to the ultimate aim ofour work: the development of a coherence model forargument structure realization, including the designof an appropriate task and evaluation setting.AcknowledgementsWe are grateful to the Landesgraduiertenfo?rderungBaden-Wu?rttemberg for funding within the researchinitiative ?Coherence in language processing?
ofHeidelberg University.
We thank Danny Rehl andLukas Funk for annotation and Kathrin Spreyer, Tae-Gil Noh and Carina Silberer for helpful discussion.225ReferencesRegina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Computa-tional Linguistics, 34(1):1?34.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Proceedings ofthe Human Language Technology Conference of theNorth American Chapter of the Association for Com-putational Linguistics, Boston, Mass., 2?7 May 2004,pages 113?120.Regina Barzilay and Kathleen R. McKeown.
2001.
Ex-tracting paraphrases from a parallel corpus.
In Pro-ceedings of the 39th Annual Meeting of the Associ-ation for Computational Linguistics, Toulouse, pages50?57.Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.2009.
The grec main subject reference generationchallenge 2009: overview and evaluation results.
InProceedings of the 2009 Workshop on Language Gen-eration and Summarisation, pages 79?87.Anders Bjo?rkelund, Bernd Bohnet, Love Hafdell, andPierre Nugues.
2010.
A high-performance syntac-tic and semantic dependency parser.
In Coling 2010:Demonstration Volume, pages 33?36, Beijing, China,August.
Coling 2010 Organizing Committee.Bernd Bohnet.
2010.
Top accuracy and fast dependencyparsing is not a contradiction.
In Proceedings of the23rd International Conference on Computational Lin-guistics (Coling 2010), pages 89?97, Beijing, China,August.
Coling 2010 Organizing Committee.Chris Brockett.
2007.
Aligning the RTE 2006 Corpus.Microsoft Research.Aoife Cahill and Arndt Riester.
2009.
Incorporating in-formation status into generation ranking.
In Proceed-ings of the Joint Conference of the 47th Annual Meet-ing of the ACL and the 4th International Joint Confer-ence on Natural Language Processing of the AFNLP,pages 817?825, Suntec, Singapore, August.
Associa-tion for Computational Linguistics.Jacob Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Measure-ment, 20:37?46.Paul R. Cohen.
1995.
Empirical methods for artificialintelligence.
MIT Press, Cambridge, MA, USA.Trevor Cohn, Chris Callison-Burch, and Mirella Lap-ata.
2008.
Constructing Corpora for Development andEvaluation of Paraphrase Systems.
34(4).William B. Dolan and Chris Brockett.
2005.
Automat-ically constructing a corpus of sentential paraphrases.In Proceedings of the Third International Workshop onParaphrasing.Katja Filippova and Michael Strube.
2007.
Generat-ing constituent order in German clauses.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics, Prague, Czech Republic,23?30 June 2007, pages 320?327.Charles J. Fillmore, Christopher R. Johnson, andMiriam R.L.
Petruck.
2003.
Background toFrameNet.
International Journal of Lexicography,16.3:235?250.Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225.Shudong Huang, David Graff, and George Doddington.2002.
Multiple-Translation Chinese Corpus.
Linguis-tic Data Consortium, Philadelphia.Philipp Koehn, 2005.
Europarl: A parallel corpus forstatistical machine translation, volume 5, pages 79?86.Percy Liang, Benjamin Taskar, and Dan Klein.
2006.Alignment by agreement.
In North American Associ-ation for Computational Linguistics (NAACL), pages104?111.William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory.
Toward a functional the-ory of text organization.
Text, 8(3):243?281.Kathleen R. McKeown and Dragomir Radev.
1995.
Gen-erating summaries of multiple news articles.
In Pro-ceedings of the 18th Annual International ACM-SIGIRConference on Research and Development in Informa-tion Retrieval, Seattle, Wash., 9?13 July 1995, pages74?82.
Reprinted in Advances in Automatic Text Sum-marization, Mani, I. and Maybury, M.T.
(Eds.
), Cam-bridge, Mass.
: MIT Press, 1999, pp.381-389.Adam Meyers, Ruth Reeves, and Catherine Macleod.2008.
NomBank v1.0.
Linguistic Data Consortium,Philadelphia.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?105.Robert Parker, David Graff, Jumbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English Gigaword Fifth Edi-tion.
Linguistic Data Consortium, Philadelphia.Josef Ruppenhofer, Caroline Sporleder, Roser Morante,Collin Baker, and Martha Palmer.
2010.
SemEval-2010 Task 10: Linking Events and Their Participantsin Discourse.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluations, pages 45?50, Up-psala, Sweden, July.Ivan Titov and Mikhail Kozhevnikov.
2010.
Bootstrap-ping semantic analyzers from non-contradictory texts.226In Proceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, Uppsala, Swe-den, 11?16 July 2010, pages 958?967.Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris.2006.
Using dependency-based features to take the?Para-farce?
out of paraphrase.
In Proceedings of theAustralasian Language Technology Workshop, pages131?138.Sander Wubben, Antal van den Bosch, Emiel Krahmer,and Erwin Marsi.
2009.
Clustering and matchingheadlines for automatic paraphrase acquisition.
InProceedings of the 12th European Workshop on Nat-ural Language Generation (ENLG 2009), pages 122?125, Athens, Greece, March.
Association for Compu-tational Linguistics.227
