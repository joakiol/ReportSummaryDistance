Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 216?223,Sydney, July 2006. c?2006 Association for Computational LinguisticsDistributed Language Modeling for N -best List Re-rankingYing Zhang Almut Silja Hildebrand Stephan VogelLanguage Technologies Institute, Carnegie Mellon University5000 Forbes Ave. Pittsburgh, PA 15213, U.S.A.{joy+, silja+, vogel+}@cs.cmu.eduAbstractIn this paper we describe a novel dis-tributed language model for N -best listre-ranking.
The model is based on theclient/server paradigm where each serverhosts a portion of the data and providesinformation to the client.
This model al-lows for using an arbitrarily large corpusin a very efficient way.
It also providesa natural platform for relevance weightingand selection.
We applied this model ona 2.97 billion-word corpus and re-rankedthe N -best list from Hiero, a state-of-the-art phrase-based system.
Using BLEU as ametric, the re-ranked translation achievesa relative improvement of 4.8%, signifi-cantly better than the model-best transla-tion.1 IntroductionStatistical language modeling has been widelyused in natural language processing applicationssuch as Automatic Speech Recognition (ASR),Statistical Machine Translation (SMT) (Brown etal., 1993) and Information Retrieval (IR) (Ponteand Croft, 1998).Conventional n-gram language modelingcounts the frequency of all the n-grams in acorpus and calculates the conditional probabilitiesof a word given its history of n ?
1 wordsP (wi|wi?1i?n+1).
As the corpus size increases,building a high order language model offlinebecomes very expensive if it is still possible(Goodman, 2000).In this paper, we describe a new approach oflanguage modeling using a distributed comput-ing paradigm.
Distributed language modeling canmake use of arbitrarily large training corpora andprovides a natural way for language model adap-tation.We applied the distributed LM to the task of re-ranking the N -best list in statistical machine trans-lation and achieved significantly better translationquality when measured by the BLEU metric (Pap-ineni et al, 2001).2 N -best list re-rankingWhen translating a source language sentence finto English, the SMT decoder first builds a trans-lation lattice over the source words by applying thetranslation model and then explores the lattice andsearches for an optimal path as the best translation.The decoder uses different models, such as thetranslation model, n-gram language model, fertil-ity model, and combines multiple model scores tocalculate the objective function value which favorsone translation hypothesis over the other (Och etal., 2004).Instead of outputting the top hypothesis e(1)based on the decoder model, the decoder can out-put N (usually N = 1000) alternative hypotheses{e(r)|r = 1, .
.
.
, N} for one source sentence andrank them according to their model scores.Figure 1 shows an example of the output from aSMT system.
In this example, alternative hypoth-esis e(2) is a better translations than e(1) accordingto the reference (Ref) although its model score islower.SMT models are not perfect, it is unavoidableto have a sub-optimal translation output as themodel-best by the decoder.
The objective of N -best list re-ranking is then to re-rank the trans-lation hypotheses using features which are notused during decoding so that better translationscan emerge as ?optimal?
translations.
Our exper-216f : , 2001#?)?I9]??{/G?
?Ref: Since the terrorist attacks on the United States in 2001e(1): since 200 year , the united states after the terroristattacks in the incidente(2): since 2001 after the incident of the terrorist attacks onthe united statese(3): since the united states 2001 threats of terrorist attacksafter the incidente(4): since 2001 the terrorist attacks after the incidente(5): since 200 year , the united states after the terroristattacks in the incidentFigure 1: An example of N -best list.iments (section 5.1) have shown that the oracle-best translation from a typical N -best list could be6 to 10 BLEU points better than the model-besttranslation.In this paper we use the distributed languagemodel on very large data to re-rank the N -best list.2.1 Sentence likelihoodThe goal of a language model is to determinethe probability, or in general the ?likelihood?
ofa word sequence w1 .
.
.
wm (wm1 for short) givensome training data.
The standard language model-ing approach breaks the sentence probability downinto:P (wm1 ) =?iP (wi|wi?11 ) (1)Under the Markov or higher order Markov processassumption that only the closest n?
1 words havereal impact on the choice of wi, equation 1 is ap-proximated to:P (wm1 ) =?iP (wi|wi?1i?n+1) (2)The probability of a word given its history can beapproximated with the maximum likelihood esti-mate (MLE) without any smoothing:P (wi|wi?1i?n+1) ?C(wii?n+1)C(wi?1i?n+1)(3)In addition to the standard n-gram probabilityestimation, we propose 3 sentence likelihood met-rics.?
L0: Number of n-grams matched.The simplest metric for sentence likelihood isto count how many n-grams in this sentencecan be found in the corpus.L0(wm1 ) =?i,ji?j?
(wji ) (4)?
(wji ) ={1 : C(wji ) > 00 : C(wji ) = 0(5)For example, L0 for sentence in figure 2 is 52because 52 n-grams have non-zero counts.?
Ln1 : Average interpolated n-gram conditionalprobability.Ln1 (wm1 ) =( m?i=1n?k=1?kP (wi|wi?1i?k+1)) 1m(6)P (wi|wi?1i?k+1) is approximated from the n-gram counts (Eq.
3) without any smoothing.
?k is the weight for k-gram conditional prob-ability,?
?k = 1.Ln1 is similar to the standard n-gram LMexcept the probability is averaged over thewords in the sentence to prevent shorter sen-tences being favored unfairly.?
L2: Sum of n-gram?s non-compositionalityFor each matched n-gram, we consider allthe possibilities to cut/decompose it into twoshort n-grams, for example ?the terrorist at-tacks on the united states?
could be decom-posed into (?the?, ?terrorist attacks on theunited states?)
or (?the terrorist?, ?attackson the united states?
), ... , or (?the ter-rorist attacks on the united?, ?states?).
Foreach cut, calculate the point-wise mutual in-formation (PMI) between the two short n-grams.
The one with the minimal PMIis the most ?natural?
cut for this n-gram.The PMI over the natural cut quantifies thenon-compositionality Inc of an n-gram wji .The higher the value of Inc(wji ) the morelikely wji is a meaningful constituent, in otherwords, it is less likely that wji is composedfrom two short n-grams just by chance (Ya-mamoto and Church, 2001).Define L2 formally as:L2(wm1 ) =?i,ji?jInc(wji ) (7)217Inc(wji ) =??
?minkI(wki ;wjk+1) : C(wji ) > 00 : C(wji ) = 0(8)I(wki ;wjk+1) = logP (wji )P (wki )P (wjk+1)(9)3 Distributed language modelThe fundamental information required to calculatethe likelihood of a sentence is the frequency of n-grams in the corpus.
In conventional LM train-ing, all the counts are collected from the corpus Dand saved to disk for probability estimation.
Whenthe size of D becomes large and/or n is increasedto capture more context, the count file can be toolarge to be processed.Instead of collecting n-gram counts offline, weindex D using a suffix array (Manber and Myers,1993) and count the occurrences of wii?n+1 in Don the fly.3.1 Calculate n-gram frequency using suffixarrayFor a corpus D with N words, locating all the oc-currences of wii?n+1 takes O(logN ).
Zhang andVogel (2005) introduce a search algorithm whichlocates all the m(m+ 1)/2 embedded n-grams ina sentence of m words within O(m ?
logN ) time.Figure 2 shows the frequencies of all the embed-ded n-grams in sentence ?since 2001 after the in-cident of the terrorist attacks on the united states?matched against a 26 million words corpus.
Forexample, unigram ?after?
occurs 4.43?104 times,trigram ?after the incident?
occurs 106 times.
Thelongest n-gram that can be matched is the 8-gram?of the terrorist attacks on the united states?
whichoccurs 7 times in the corpus.3.2 Client/Server paradigmTo load the corpus and its suffix array index intothe memory, each word token needs 8 bytes.
Forexample, if the corpus has 50 million words,400MB memory is required.
For the English1 Gi-gaWord2 corpus which has 2.7 billion words, the1Though we used English data for our experiments in thispaper, the approach described here is language independent.2http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2005T12total memory required is 22GB.
It is practicallyimpossible to fit such data into the memory of anysingle machine.To make use of the large amount of data, wedeveloped a distributed client/server architecturefor language modeling.
Client/server is the mostcommon paradigm of distributed computing atpresent (Leopold, 2001).
The paradigm describesan asymmetric relationship between two type ofprocesses, of which one is the client, and the otheris the server.
The server process manages some re-sources and offers a service which can be used byother processes.
The client is a process that needsthe service in order to accomplish its task.
It sendsa request to the server and asks for the executionof a task that is covered by the service.We split the large corpus D into d non-overlapping chunks.
One can easily verify that forany n-gram wii?n+1 the count of its occurrences inD is the sum of its occurrences in all the chunks,i.e.,C(wii?n+1)|D =?dC(wii?n+1)|Dd (10)Each server3 loads one chunk of the corpus withits suffix array index.
The client sends an Englishsentence w1 .
.
.
wm to each of the servers and re-quests for the count information of all the n-gramsin the sentence.
The client collects the count infor-mation from all the servers, sums up the counts foreach n-gram and then calculates the likelihood ofthe sentence.The client communicates with the servers viaTCP/IP sockets.
In our experiments, we used150 servers running on 26 computers to serve oneclient.
Multiple clients can be served at the sametime if needed.
The process of collecting countsand calculating the sentence probabilities takesabout 1 to 2 ms for each English sentence (averagelength 23.5 words).
With this architecture, we caneasily make use of larger corpora by adding addi-tional data servers.
In our experiments, we used allthe 2.7 billion word data in the English Gigawordcorpus without any technical difficulties.3A server is a special program that provides services toclient processes.
It runs on a physical computer but the con-cept of server should not be confused with the actual machinethat runs it.
In practice, one computer usually hosts multipleservers at the same time.218n since 2001 after the incident of the terrorist attacks on the united states1 2.19?104 7559 4.43?104 1.67?106 2989 6.9?105 1.67?106 6160 9278 2.7?105 1.67?106 5.1?104 3.78?1042 165 105 1.19?104 1892 34 2.07?105 807 1398 1656 5.64?104 3.72?104 3.29?1043 6 56 106 6 3 162 181 216 545 605 2.58?1044 0 0 0 1 0 35 67 111 239 4245 0 0 0 0 0 15 34 77 2326 0 0 0 0 0 10 23 767 0 0 0 0 0 7 238 0 0 0 0 0 7Figure 2: Frequencies of all the embedded n-grams in sentence ?since 2001 after the incident of theterrorist attacks on the united states.
?4 ?More data is better data?
or?Relevant data is better data?Although statistical systems usually improve withmore data, performance can decrease if additionaldata does not fit the test data.
There have beendebates in the data-driven NLP community as towhether ?more data is better data?
or ?relevantdata is better data?.
For N -best list re-ranking, thequestion becomes: ?should we use all the data tore-rank the hypotheses for one source sentence, orselect some corpus chunks that are believed to berelevant to this sentence?
?Various relevance measures are proposed in(Iyer and Ostendorf, 1999) including content-based relevance criteria and style-based criteria.
Inthis paper, we use a very simple relevance metric.Define corporaDd?s relevance to a source sentenceft as:R(Dd, ft) =N?r=1L0(e(r)t )|Dd (11)R(Dd, ft) estimates how well a corpus Dd cancover the n-grams in the N -best list of a sourcesentence.
The higher the coverage, the more rele-vant Dd is.In the distributed LM architecture, the clientfirst sends N translations of ft to all the servers.From the returned n-gram matching information,client calculates R(Dd, ft) for each server, andchoose the most relevant (e.g., 20) servers for ft.The n-gram counts returned from these relevantservers are summed up for calculating the likeli-hood of ft. One could also assign weights to the n-gram counts returned from different servers duringthe summation so that the relevant data has moreimpact than the less-relevant ones.5 ExperimentsWe used the N -best list generated by the HieroSMT system (Chiang, 2005).
Hiero is a statis-tical phrase-based translation model that uses hi-erarchical phrases.
The decoder uses a trigramlanguage model trained with modified Kneser-Neysmoothing (Kneser and Ney, 1995) on a 200 mil-lion words corpus.
The 1000-best list was gen-erated on 919 sentences from the MT03 Chinese-English evaluation set.All the data from the English Gigaword corpusplus the English side of the Chinese-English bilin-gual data available from LDC are used.
The 2.97billion words data is split into 150 chunks, eachhas about 20 million words.
The original orderis kept so that each chunk contains data from thesame news source and a certain period of time.For example, chunk Xinhua2003 has all the Xin-hua News data from year 2003 and NYT9499 038has the last 20 million words from the New YorkTimes 1994-1999 corpus.
One could split thedata into larger(smaller) chunks which will requireless(more) servers.
We choose 20 million words asthe size for each chunk because it can be loaded byour smallest machine and it is a reasonable granu-larity for selection.In total, 150 corpus information servers run on26 machines connected by the standard EthernetLAN.
One client sends each English hypothesistranslations to all 150 servers and uses the returnedinformation to re-rank.
The whole process takesabout 600 seconds to finish.We use BLEU scores to measure the transla-tion accuracy.
A bootstrapping method is used tocalculate the 95% confidence intervals for BLEU(Koehn, 2004; Zhang and Vogel, 2004).5.1 Oracle score of the N -best listBecause of the spurious ambiguity, there are only24,612 unique hypotheses in the 1000-best list, onaverage 27 per source sentence.
This limits the po-tential of N -best re-ranking.
Spurious ambiguityis created by the decoder where two hypothesesgenerated from different decoding path are con-sidered different even though they have identicalword sequences.
For example, ?the terrorist at-tacks on the united states?
could be the output ofdecoding path [the terrorist attacks][on the united219states] and [the terrorist attacks on] [the unitedstates].We first calculate the oracle score from the N -best list to verify that there are alternative hypothe-ses better than the model-best translation.
The or-acle best translations are created by selecting thehypothesis which has the highest sentence BLEUscore for each source sentence.
Yet a critical prob-lem with BLEU score is that it is a function ofthe entire test set and does not give meaningfulscores for single sentences.
We followed the ap-proximation described in (Collins et al, 2005) toget around this problem.
Given a test set with Tsentences, N hypotheses are generated for eachsource sentence ft. Denote e(r)t as the r-th rankedhypothesis for ft. e(1)t is the model-best hypoth-esis for this sentence.
The baseline BLEU scoresare calculated based on the model-best translationset {e(1)t |t = 1, .
.
.
, T}.Define the BLEU sentence-level gain for e(r)tas:GBLEUe(r)t =BLEU{e(1)1 , e(1)2 , .
.
.
, e(r)t , .
.
.
, e(r)T }?
BLEU{e(1)1 , e(1)2 , .
.
.
, e(1)t , .
.
.
, e(r)T }GBLEUe(r)t calculates the gain if we switch themodel-best hypothesis e(1)t using e(r)t for sentenceft and keep the translations for the rest of the testset untouched.With the estimated sentence level gain for eachhypothesis, we can construct the oracle best trans-lation set by selecting the hypotheses with thehighest BLEU gain for each sentence.
Oracle bestBLEU translation set is: {e(r?t )t |t = 1, .
.
.
, T}where r?t = argmaxr GBLEUe(r)t .Model-bestScore Confidence Interval OracleBLEU 31.44 [30.49, 32.33] 37.48Table 1: BLEU scores for the model-best andoracle-best translations.Table 1 shows the BLEU score of the approxi-mated oracle best translation.
The oracle score is7 points higher than the model-best scores eventhough there are only 27 unique hypotheses foreach sentence on average.
This confirms our ob-servation that there are indeed better translationsin the N -best list.5.2 Training standard n-gram LM on largedata for comparisonBesides comparing the distributed language modelre-ranked translations with the model-best transla-tions, we also want to compare the distributed LMwith the the standard 3-gram and 4-gram languagemodels on the N -best list re-ranking task.Training a standard n-gram model for a 2.9 bil-lion words corpora is much more complicated andtedious than setting up the distributed LM.
Be-cause of the huge size of the corpora, we couldonly manage to train a test-set specific n-gram LMfor this experiment.First, we split the corpora into smaller chunksand generate n-gram count files for each chunk.Each count file is then sub-sampled to entrieswhere all the words are listed in the vocabularyof the N -best list (5,522 word types).
We mergeall the sub-sampled count files into one and trainthe standard language model based on it.We manage to train a 3-gram LM using the2.97 billion-word corpus.
Resulting LM requires2.3GB memory to be loaded for the re-ranking ex-periment.A 4-gram LM for this N -best list is of 13 GBin size and can not be fit into the memory.
Wesplit the N -best list into 9 parts to reduce the vo-cabulary size of each sub N -best list to be around1000 words.
The 4-gram LM tailored for each subN -best list is around 1.5 to 2 GB in size.Training higher order standard n-gram LMswith this method requires even more partitions ofthe N -best list to get smaller vocabularies.
Whenthe vocabulary becomes too small, the smoothingcould fail and results in unreliable LM probabili-ties.Adapting the standard n-gram LM for each in-dividual source sentence is almost infeasible givenour limited computing resources.
Thus we do nothave equivalent n-gram LMs to be compared withthe distributed LM for conditions where the mostrelevant data chunks are used to re-rank the N -bestlist for a particular source sentence.5.3 ResultsTable 2 lists results of the re-ranking experimentsunder different conditions.
The re-ranked trans-lation improved the BLEU score from 31.44 to22032.64, significantly better than the model-besttranslation.Different metrics are used under the same datasituation for comparison.
L0, though extremelysimple, gives quite nice results on N -best list re-ranking.
With only one corpus chunk (the mostrelevant one) for each source sentence, L0 im-proved the BLEU score to 32.22.
We suspect thatL0 works well because it is inline with the natureof BLEU score.
BLEU measures the similarity be-tween the translation hypothesis and human refer-ence by counting how many n-grams in MT canbe found in the references.Instead of assigning weights 1 to all thematched n-grams in L0, L2 weights each n-gramby its non-compositionality.
For all data condi-tions, L2 consistently gives the best results.Metric family L1 is close to the standard n-gramLM probability estimation.
Because no smoothingis used, L31 performance (32.00) is slightly worsethan the standard 3-gram LM result (32.22).
Onthe other hand, increasing the length of the historyin L1 generally improves the performance.Figure 3 shows the BLEU score of the re-rankedtranslation when using different numbers of rele-vant data chunks for each sentence.
The selecteddata chunks may differ for each sentences.
Forexample, the 2 most relevant corpora for sentence1 are Xinhua2002 and Xinhua2003 while for sen-tence 2 APW2003A and NYT2002D are more rel-evant.
When we use the most relevant data chunk(about 20 million words) to re-rank the N -best list,36 chunks of data will be used at least once for919 different sentences, which accounts for about720 million words in total.
Thus the x-axis in fig-ure 3 should not be interpreted as the total amountof data used but the number of the most relevantcorpora used for each sentence.All three metrics in figure 3 show that usingall data together (150 chunks, 2.97 billion words)does not give better discriminative powers than us-ing only some relevant chunks.
This supports ourargument in section 4 that relevance selection ishelpful in N -best list re-ranking.
In some casesthe re-ranked N -best list has a higher BLEU scoreafter adding a supposedly ?less-relevant?
corpuschunk and a lower BLEU score after adding a?more-relevant?
chunk.
This indicates that the rel-evance measurement (Eq.
11) is not fully reflect-ing the real ?relevance?
of a data chunk for a sen-tence.
With a better relevance measurement one32.1532.232.2532.332.3532.432.4532.532.5532.632.6532.70  20  40  60  80  100  120  140  160Bleu ScoreNumber of corpus chunks used for each source sentence (*20M=corpus size used)"L0""L1""L2"Figure 3: BLEU score of the re-ranked best hy-pothesis vs. the number of the most relevant cor-pus chunks used to re-rank the n-best list for eachsentences.
L0: number of n-grams matched; L1:average interpolated n-gram conditional probabil-ity; L2: sum of n-grams?
non-compositionality.would expect to see the curves in figure 3 to bemuch smoother.6 Related work and discussionYamamoto and Church (2001) used suffix arraysto compute the frequency and location of an n-gram in a corpus.
The frequencies are used to find?interesting?
substrings which have high mutualinformation.Soricut et al (2002) build a Finite State Ac-ceptor (FSA) to compactly represent all possibleEnglish translations of a source sentence accord-ing to the translation model.
All sentences in abig monolingual English corpus are then scannedby this FSA and those accepted by the FSA areconsidered as possible translations for the sourcesentence.
The corpus is split into hundreds ofchunks for parallel processing.
All the sentencesin one chunk are scanned by the FSA on one pro-cessor.
Matched sentences from all chunks arethen used together as possible translations.
Theassumption of this work that possible translationsof a source sentence can be found as exact matchin a big monolingual corpus is weak even for verylarge corpus.
This method can easily fail to findany possible translation and return zero proposedtranslations.Kirchhoff and Yang (2005) used a factored 3-gram model and a 4-gram LM (modified KNsmoothing) together with seven system scores tore-rank an SMT N -best.
They improved thetranslation quality of their best baseline (Spanish-221# of Relevant Chunks per.
Sent 1 2 5 10 20 1503-gram KN 32.22 32.084-gram KN 32.22 32.53L0 32.27 32.38 32.40 32.47 32.51 32.48L31 32.00 32.14 32.14 32.15 32.16L41 32.18 32.36 32.28 32.44 32.41L51 32.21 32.33 32.35 32.41 32.37L61 32.19 32.22 32.37 32.45 32.40 32.41L71 32.22 32.29 32.37 32.44 32.40L2 32.29 32.52 32.61 32.55 32.64 32.56Table 2: BLEU scores of the re-ranked translations.
Baseline score = 31.44English) from BLEU 30.5 to BLEU 31.0.Iyer and Ostendorf (1999) select and weightdata to train language modeling for ASR.
The datais selected based on its relevance for a topic or thesimilarity to data known to be in the same domainas the test data.
Each additional document is clas-sified to be in-domain or out-of-domain accord-ing to cosine distance with TF-IDF term weights,POS-tag LM and a 3-gram word LM.
n-gramcounts from the in-domain and the additionally se-lected out-of-domain data are then combined withan weighting factor.
The combined counts areused to estimate a LM with standard smoothing.Hildebrand et al (2005) use information re-trieval to select relevant data to train adapted trans-lation and language models for an SMT system.Si et al (2002) use unigram distribution simi-larity to select the document collection which ismost relevant to the query documents.
Their workis mainly focused on information retrieval appli-cation.7 Conclusion and future workIn this paper, we presented a novel distributedlanguage modeling solution.
The distributed LMis capable of using an arbitrarily large corpusto estimate the n-gram probability for arbitrarilylong histories.
We applied the distributed lan-guage model to N -best re-ranking and improvedthe translation quality by 4.8% when evaluated bythe BLEU metric.
The distributed LM provides aflexible architecture for relevance selection, whichmakes it possible to select data for each individualtest sentence.
Our experiments have shown thatrelevant data has better discriminative power thanusing all the data.We will investigate different relevance weight-ing schemes to better combine n-gram statisticsfrom different data sources.
We are planning tointegrate the distributed LM in the statistical ma-chine translation decoder in the near future.8 AcknowledgementWe would like to thank Necip Fazil Ayan andPhilip Resnik for providing Hiero system?s N -bestlist and allowing us to use it for this work.ReferencesPeter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: pa-rameter estimation.
Comput.
Linguist., 19(2):263?311.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of ACL 2005, pages 263?270, Ann Arbor,MI, June 2005.
ACL.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL 2005, pages531?540, Ann Arbor, MI, June.J.
Goodman.
2000.
A bit of progress in languagemodeling.
Technical report, Microsoft Research, 56Fuchun Peng.Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,and Alex Waibel.
2005.
Adaptation of the transla-tion model for statistical machine translation basedon information retrieval.
In Proceedings of the 10thEAMT conference ?Practical applications of ma-chine translation?, pages 133?142, Budapest, May.R.
Iyer and M. Ostendorf.
1999.
Relevance weightingfor combining multi-domain data for n-gram lan-guage modeling.
Comptuer Speech and Language,13(3):267?282.222Katrin Kirchhoff and Mei Yang.
2005.
Improved lan-guage modeling for statistical machine translation.In Proceedings of the ACL Workshop on Buildingand Using Parallel Texts, pages 125?128, Ann Ar-bor, Michigan, June.
Association for ComputationalLinguistics.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the IEEE International Conference onAcoustics, Speech and Signal Processing, volume 1,pages 181?184.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP 2004, Barcelona, Spain, July.Claudia Leopold.
2001.
Parallel and Distributed Com-puting: A Survey of Models, Paradigms and Ap-proaches.
John Wiley & Sons, Inc., New York, NY,USA.Udi Manber and Gene Myers.
1993.
Suffix arrays:a new method for on-line string searches.
SIAM J.Comput., 22(5):935?948.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2004.A smorgasbord of features for statistical machinetranslation.
In Proceedings of the 2004 Meeting ofthe North American chapter of the Association forComputational Linguistics (NAACL-04), Boston.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
Technical Report RC22176(W0109-022), IBM Research Division, Thomas J. WatsonResearch Center.Jay M. Ponte and W. Bruce Croft.
1998.
A languagemodeling approach to information retrieval.
In Re-search and Development in Information Retrieval,pages 275?281.Luo Si, Rong Jin, Jamie Callan, and Paul Ogilvie.2002.
A language modeling framework for resourceselection and results merging.
In CIKM ?02: Pro-ceedings of the eleventh international conferenceon Information and knowledge management, pages391?397, New York, NY, USA.
ACM Press.Radu Soricut, Kevin Knight, and Daniel Marcu.
2002.Using a large monolingual corpus to improve trans-lation accuracy.
In AMTA ?02: Proceedings ofthe 5th Conference of the Association for MachineTranslation in the Americas on Machine Transla-tion: From Research to Real Users, pages 155?164,London, UK.
Springer-Verlag.Mikio Yamamoto and Kenneth W. Church.
2001.
Us-ing suffix arrays to compute term frequency and doc-ument frequency for all substrings in a corpus.
Com-put.
Linguist., 27(1):1?30.Ying Zhang and Stephan Vogel.
2004.
Measuring con-fidence intervals for the machine translation evalu-ation metrics.
In Proceedings of The 10th Interna-tional Conference on Theoretical and Methodologi-cal Issues in Machine Translation, October.Ying Zhang and Stephan Vogel.
2005.
An effi-cient phrase-to-phrase alignment model for arbitrar-ily long phrase and large corpora.
In Proceedingsof the Tenth Conference of the European Associa-tion for Machine Translation (EAMT-05), Budapest,Hungary, May.
The European Association for Ma-chine Translation.223
