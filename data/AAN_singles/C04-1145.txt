Morpheme-based Derivation ofBipolar Semantic Orientation of Chinese WordsRaymond W.M.
Yuen, Terence Y.W.
Chan, Tom B.Y.
Lai, O.Y.
Kwong, Benjamin K.Y.
T'souLanguage Information Sciences Research Centre, the City University of Hong Kong83 Tat Chee Avenue, Hong Kong{ wmyuen, dcywchan, cttomlai, rlolivia, rlbtsou}@cityu.edu.hkAbstractThe evaluative character of a word is called itssemantic orientation (SO).
A positive SOindicates desirability (e.g.
Good, Honest) anda negative SO indicates undesirability (e.g.,Bad, Ugly).
This paper presents a method,based on Turney (2003), for inferring the SOof a word from its statistical association withstrongly-polarized words and morphemes inChinese.
It is noted that morphemes are muchless numerous than words, and that also asmall number of fundamental morphemes maybe used in the modified system to greatadvantage.
The algorithm was tested on 1,249words (604 positive and 645 negative) in acorpus of 34 million words, and was run with20 and 40 polarized words respectively, givinga high precision (79.96% to 81.05%), but alow recall (45.56% to 59.57%).
The algorithmwas then run with 20 polarized morphemes, orsingle characters, in the same corpus, giving ahigh precision of 80.23% and a high recall of85.03%.
We concluded that morphemes inChinese, as in any language, constitute a dis-tinct sub-lexical unit which, though small innumber, has greater linguistic significancethan words, as seen by the significant en-hancement of results with a much smallercorpus than that required by Turney.1.
IntroductionThe semantic orientation (SO) of a word indicatesthe direction in which the word deviates from thenorm for its semantic group or lexical field (Lehrer,1974).
Words that encode a desirable state (e.g.,beautiful) have a positive SO, while words thatrepresent undesirable states (e.g.
absurd) have anegative SO (Hatzivassiloglou and Wiebe, 2000).Hatzivassiloglou and Mckeown (1997) used thewords ?and?, ?or?, and ?but?
as linguistic cues toextract adjective pairs.
Turney (2003) assessed theSO of words using their occurrences near strongly-polarized words like ?excellent?
and ?poor?
withaccuracy from 61% to 82%, subject to corpus size.Turney?s algorithm requires a colossal corpus(hundred billion words) indexed by the AltaVistasearch engine in his experiment.
Undoubtedly,internet texts have formed a very large and easily-accessible corpus.
However, Chinese texts ininternet are not segmented so it is not cost-effective to use them.This paper presents a general strategy forinferring SO for Chinese words from theirassociation with some strongly-polarizedmorphemes.
The modified system of usingmorphemes was proved to be more effective thanstrongly-polarized words in a much smaller corpus.Related work and potential applications of SOare discussed in section 2.Section 3 illustrates one of the methods ofTurney?s model for inferring SO, namely,Pointwise Mutual Information (PMI), based on thehypothesis that the SO of a word tends tocorrespond to the SO of its neighbours.The experiment with polarized words ispresented in section 4.
The test set includes 1,249words (604 positive and 645 negative).
In a corpusof 34 million word tokens, 410k word types, thealgorithm is run with 20 and 40 polarized words,giving a precision of 79.96% and 81.05%, and arecall  of 45.56% and 59.57%, respectively.The system is further modified by usingpolarized morphemes in section 5.
We firstevaluate the distinction of Chinese morphemes tojustify why the modification can probably givesimpler and better results, and then introduce amore scientific selection of polarized morphemes.A high precision of 80.23% and a greatly increasedrecall of 85.03% are yielded.In section 6, the algorithm is run with 14, 10 and6 morphemes, giving a precision of 79.15%,79.89% and 75.65%, and a recall of 79.50%,73.26% and 66.29% respectively.
It shows that thealgorithm can be also effectively run with 6 to 10polarized morphemes in a smaller corpus.The conclusion and future work are discussed insection 7.2.
Related Work and ApplicationsHatzivassiloglou and Mckeown (1997) presented amethod for automatically assigning a + or ?orientation label to adjectives known to have someSO by the linguistic constraints on the use ofadjectives in conjunctions.
For example, ?and?links adjectives that have the same SO, while ?but?links adjectives that have opposite SO.
Theydevised an algorithm based on such constraints toevaluate 1,336 manually-labeled adjectives (657positive and 679 negative) with 97% accuracy in acorpus of 21 million words.Turney (2003) introduced a method forautomatically inferring the direction and intensityof the SO of a word from its statistical associationwith a set of positive and negative paradigm words,i.e., strongly-polarized words.
The algorithm wasevaluated on 3,596 words (1,614 positive and1,982 negative) including adjectives, adverbs,nouns, and verbs.
An accuracy of 82.8% wasattained in a corpus of hundred billion words.SO can be used to classify reviews (e.g., moviereviews) as positive or negative (Turney, 2002),and applied to subjectivity analysis such asrecognizing hostile messages, classifying emails,mining reviews (Wiebe et al, 2001).
The first stepof those applications is to recognize that the text issubjective and then the second step, naturally, is todetermine the SO of the subjective text.
Also, itcan be used to summarize argumentative articleslike editorials of news media.
A summarizationsystem would benefit from distinguishingsentences intended to present factual materialsfrom those intended to present opinions, sincemany summaries are meant to include only facts.3.
SO from Association-PMITurney (2003) examined SO-PMI (PointwiseMutual Information) and SO-LSA (LatentSemantic Analysis).
SO-PMI will be our focus inthe following parts.
PMI is defined as:PMI(word1, word2)=log2( )()()&(2121wordpwordpwordwordp )where p(word1 & word2) is the probability thatword1 and word2 co-occur.
If the words arestatistically independent, the probability that theyco-occur is given by the product p(word1) p(word2).The ratio between p(word1 & word2) and p(word1)p(word2) is a measure of the degree of statisticaldependence between the words.
The SO of a givenword is calculated from the strength of itsassociation with a set of positive words, minus thestrength of its association with a set of negativewords.
Thus the SO of a word, word, is calculatedby SO-PMI as follows:SO-PMI(word) =?PwordspwordpwordwordPMI ),(  - ?NwordsnwordnwordwordPMI ),(where Pwords is a set of 7 positive paradigmwords (good, nice, excellent, positive, fortunate,correct, and superior) and Nwords is a set of 7negative paradigm words (bad, nasty, poor,negative, unfortunate, wrong, and inferior).
Those14 words were chosen by intuition and based onopposing pairs (good/bad, excellent/poor, etc.
).The words are rather insensitive to context, i.e.,?excellent?
is positive in almost all contexts.A word, word, is classified as having a positiveSO when SO-PMI(word) is positive and a negativeSO when SO-PMI(word) is negative.Turney (2003) used the Alta Vista Advancedsearch engine with a NEAR operator, whichconstrains the search to documents that contain thewords within ten words of one another, in eitherorder.
Three corpora were tested.
AV-ENG is thelargest corpus covering 350 million web pages(English only) indexed by Alta Vista.
The mediumcorpus is a 2% subset of AV-ENG corpus calledAV-CA (Canadian domain only).
The smallestcorpus TASA is about 0.5% of AV-CA andcontains various short documents.One of the lexicons used in Turney?s experimentis the GI lexicon (Stone et al, 1966), whichconsists of 3,596 adjectives, adverbs, nouns, andverbs, 1,614 positive and 1,982 negative.Table 1 shows the precision of SO-PMI with theGI lexicon in the three corpora.Precision Percent offull test setSize oftest set AV-ENG AV-CA TASA100% 3596 82.84% 76.06% 61.26%75% 2697 90.66% 81.76% 63.92%50% 1798 95.49% 87.26% 47.33%25% 899 97.11% 89.88% 68.74%Approx.
no.
ofwords 1x10112x109 1x107Table 1: The precision of SO-PMI with the GIlexiconThe strength (absolute value) of the SO wasused as a measure of confidence that the wordswill be correctly classified.
Test set words weresorted in descending order of the absolute value oftheir SO and the top ranked words (the highestconfidence words) were then classified.
Forexample, the second row (starting with 75%) intable 1 shows the precision when the top 75% wereclassified and the last 25% (with lowest confidence)were ignored.
We will employ this measure ofconfidence in the following experiments.Turney concluded that SO-PMI requires a largecorpus (hundred billion words), but it is simple,easy to implement, unsupervised, and it is notrestricted to adjectives.4.
Experiment with Chinese WordsIn the following experiments, we applied Turney?smethod to Chinese.
The algorithm was run with 20and then 40 paradigm words for comparison.
Theexperiment details include:NEAR Operator: it was applied to constrainthe search to documents that contain the wordswithin ten words of one another, in either order.Corpus: the LIVAC synchronous corpus (Tsouet al, 2000, http://www.livac.org) was used.
Itcovers 9-year news reports of Chinesecommunities including Hong Kong, Beijing andTaiwan, and we used a sub-corpus with about 34million word tokens and 410k word types.Test Set Words: a combined set of twodictionaries of polarized words (Guo, 1999, Wang,2001) was used to evaluate the results.
WhileLIVAC is an enormous Chinese corpus, its size isstill far from the hundred-billion-word corpus usedby Turney.
It is likely that some words in thecombined set are not used in the 9-year corpus.
Toavoid a skewed recall, the number of test set wordsused in the corpus is given in table 2.
In otherwords, the recall can be calculated by the totalnumber of words used in the corpus, but not bythat recorded in the dictionaries.
The differencebetween two numbers is just 100.Polarity Total no.
of thetest set wordsWords used inthe 9-year corpusPositive 629 604Negative 721 645Total 1350 1249Table 2: Number of the test set wordsParadigm words: The paradigm words werechosen using intuition and based on opposing pairs,as Turney (2003) did.
The first experiment wasconducted with 10 positive and 10 negativeparadigm words, as follows,Pwords: (honest), (clever), (sufficient),(lucky), (right), (excellent),(prosperous), (kind), (brave), (humble)Nwords: (hypocritical), (foolish),(deficient), (unlucky), (wrong), (adverse),(unsuccessful), (violent), (cowardly),(arrogant)The experiment was then repeated by increasingthe number of paradigm words to 40.
Theparadigm words added are:Pwords: (mild), (favourable),(successful), (positive), (active),(optimistic), (benign), (attentive),(promising), (incorrupt)Nwords: (radical), (unfavourable),(failed), (negative), (passive),(pessimistic), (malignant), (inattentive),(indifferent), (corrupt)4.1 ResultsTables 3 and 4 show the precision and recall ofSO-PMI by two sets of paradigm words.% of test set 100% 75% 50% 25%Size of test set 1249 937 625 312Extracted Set 569 427 285 142Precision 79.96% 86.17% 86.99% 90.16%Recall 45.56%Table 3: Precision and Recall of the SO-PMI of the20 paradigm word test set% of  test set 100% 75% 50% 25%Size of test set 1249 937 625 312Extracted Set 744 558 372 186Precision 81.05% 86.02% 88.71% 94.09%Recall 59.57%Table 4: Precision and Recall of the SO-PMI of the40 paradigm word test setThe results of both sets gave a satisfactoryprecision of 80% even in 100% confidence.However, the recall was just 45.56% under the 20-word condition, and rose to 59.57% under the 40-word condition.
The 15% rise was noted.To further improve the recall performance, weexperimented with a modified algorithm based onthe distinct features of Chinese morphemes.5.
Experiment with Chinese MorphemesTaking morphemes to be smallest linguisticmeaningful unit, Chinese morphemes are mostlymonosyllabic and single characters, although thereare some exceptional poly-syllabic morphemes like(grape),  (coffee), which are mostlyloanwords.
In the following discussion, weconsider morphemes to be monosyllabic andrepresented by single characters.It is observed that many poly-syllabic wordswith the same SO incorporate a common set ofmorphemes.
The fact suggests the possibility ofusing paradigm morphemes instead of words.Unlike English, the constituent morphemes of aChinese word are often free-standing monosyllabicwords.
It is note-worthy that words in ancientChinese were much more mono-morphemic thanmodern Chinese.
The evolution from monosyllabicword to disyllabic word may have its origin in thephonological simplification which has given rise tohomophony, and which has affected the efficacy ofcommunication.
To compensate for this, manymore related disyllabic words have appeared inmodern Chinese (Tsou, 1976).
There are threebasic constructions for deriving disyllabic words inChinese, including:(1) combination of synonyms or nearsynonyms ( , warm, genial, =warm, mild,=warm, genial)(2) combination of semantically relatedmorphemes ( , =affair, =circumstances)(3) The affixation of minor suffixes whichserve no primary grammatical function ( ,=village, =zi, suffix)The three processes for deriving disyllabicmorphemes in Chinese outlined here should beviewed as historical processes.
The extent to whichsuch processes may be realized by native speakersto be productive synchronically bears furtherexploration.
Of the three processes, the first two,i.e., synonym and near-synonym compounding, areused frequently by speakers for purposes ofdisambiguation.
In view of this development, theevolution from monosyllabic words in ancientChinese to disyllabic words in modern Chinesedoes not change the inherent meaning of themorphemes (words in ancient Chinese) in manycases.
The SO of a word often conforms to that ofits morphemes.In English, there are affixal morphemes like dis-,un- (negation prefix), or ?less (suffix meaningshort-age), -ful (suffix meaning ?to have a propertyof?
), we can say ?careful?
or ?careless?
to expandthe meaning of ?care?.
However, it is impossible toconstruct a word like ?
*ful-care?, ?
*less-care?.However, in Chinese, the position of a morphemein many disyllabic words is far more flexible in theformation of synonym and near-synonymcompound words.
For instance, ?
?
(honor) is apart of two similar word ?
?
(honor-bright) and?
?(outstanding-honor).
Morphemes in Chineseare like a ?zipped file?
of the same file types.
Whenit unzips, all the words released have the same SO.5.1 Probability of Constituent Morphemesof Words with the Same SOMost morphemes can contribute to positive ornegative words, regardless of their inherentmeaning.
For example, ?
?
(luck) has inherently apositive meaning, but it can construct both positiveword ?
?
(lucky) or a negative word ?
?(unlucky).
Thus it is not easy to define theparadigm set simply by intuition.
But we canassign a probability value for a morpheme informing polarized words on the basis of corpusdata.The first step is to come up with possibleparadigm morphemes by intuition in a large set ofpolarized words.
With the LIVAC synchronouscorpus, the types and tokens of the wordsconstructed by the selected morphemes can easilybe extracted.
The word types, excluding propernouns, are then manually-labeled as negative,neutral or positive.
Then to obtain the probabilitythat a polar morpheme generates words with thesame SO, the tokens of the polarized word typescarrying the morpheme are divided by the tokensof all word types carrying the morpheme.
Forexample, given a negative morpheme, m1, theprobability that it appears in negative words intoken, P(m1, -ve) is given by:1m Carrying  WordtypesAll of Tokens1m Carrying rdtypesNegativeWo of TokensPositive morphemes can be done likewise.
Tennegative morphemes and ten positive morphemeswere chosen as in table 5.
Their values ofP(morpheme, orientation) are all above 0.95.+ve Morpheme -ve Morpheme1  (gift) (hurt)2  (win)3  (good) (doubt)4  (secure) (difficult)5  (rich) (rush)6  (health)7  (happy) (explode)8  (honor) (ban)9 (hardworking) (collapse)10 (smooth) (reject)Derived Types 7383 2048Tokens 247249 166335Table 5: Selected positive and negativemorphemesThose morphemes were extracted from a 5-yearsubset of the LIVAC corpus.
A morpheme, free toconstruct new words, may construct hundreds ofwords but those words with extremely lowfrequency can be regarded as ?noise?.
The ?noise?may be ?creative use?
or even incorrect use.
Thus,the number of ready-to-label word types formedfrom a particular morpheme was limited to 50, butit must cover 80% of the tokens of all word typescarrying the morpheme in the corpus (i.e., 80%dominance).
For example, if the morpheme m1constructs 120 word types with 10,000 tokens, andthe first 50 high-frequency words can reach 8,000tokens, then the remaining 70 low-frequency wordtypes, or noise, are discarded.
Otherwise, thenumber of sampled words would be expanded to anumber (over 50) fulfilling 80% dominance.5.2 Results and EvaluationIn table 6, the precision of 80.23% is slightly betterthan 79.96% of the 20-word condition, and just 1%lower than that of the 40-word condition.
However,the recall drastically increases from 45.56%, or59.57% under the 40-word condition, to 85.03%.In other words, the algorithm run with 20 Chineseparadigm morphemes resulted not only in highprecision but also much higher recall than Chineseparadigm words in the same corpus.% of test set 100% 75% 50% 25%Size of test set 1249 937 625 312Extracted Set 1062 797 531 266Precision 80.23% 85.44% 90.96% 96.61%Recall 85.03%Table 6: Precision and Recall of SO-PMI of the 20paradigm morpheme test setSince the morphemes were chosen from a subsetof the corpus for evaluation, we repeated theexperiment in a separate 1-year corpus (2001-2002).
The results in table 7 reflect a similarpattern in the two corpora ?
both words andmorphemes can get high precision, but morphemescan double the recall of words.40 Words 20 MorphemesSize of test set 1065Extracted Set 333 671Precision (Full Set) 75.38% 73.62%Recall 31.27% 63.00%Table 7: Precision (full test set only) and Recall ofSO-PMI of 40 paradigm words and 20 paradigmmorphemes in 1-year corpusIt is assumed that a smaller corpus easily leadsto the algorithm?s low recall because many low-frequency words in the test set barely associatewith the paradigm words.
To examine theassumption, the results were further analyzed withthe frequency of the test set words.
First, theoccurrence of the test set words in the 9-yearcorpus was counted, then the median of thefrequency, 44 in this case, was taken.
The resultswere divided into two sections from the medianvalue, and the recall of two sections was calculatedrespectively, as in table 8.?Table 8: Morpheme-based and word-based recallof high-frequency and low-frequency wordsThe results showed that high-frequency wordscould be largely extracted by the algorithm withboth morphemes (99.80% recall) and words(89.45% recall).
However, paradigm words gave26.55% recall of low-frequency words, whereasparadigm morphemes gave 67.66%.
They showedthat morphemes outperform words in the retrievalof low-frequency words.Colossal corpora like Turney?s hundred-billion-word corpus can compensate for the lowperformance of paradigm words in low-frequencywords.
Such a large corpus has been easily-accessible since the emergence of internet, but it isnot cost-effective to use the Chinese texts from theinternet because those texts are not segmented.Another way of compensation is the expansion ofparadigm words, but doubling the number ofparadigm words just raised the recall from 45.56%to 59.57%, as shown in section 4.
The supervisedcost is not reasonable if the number of paradigmwords is further expanded.Morphemes, or single characters in Chinese,naturally occur more frequently than words in anarticle, so 20 morphemes can be more discretely-distributed over texts than 20 or even 40 words.The results show that some morphemes alwaysretain their inherent SO when becomingconstituents in other derived words.
Suchmorphemes are like a zipped file of the same SO,when the algorithm is run with 20 paradigmmorphemes, it is actually run by thousands ofparadigm words.
Consequently, the recall coulddouble while the high precision was not affected.It may be argued that the labour cost of definingthe SO of 20 morphemes is not sufficiently loweither.
The following experiments will demonstratethat decreasing the number of morphemes can alsogive satisfactory results.6.
Experiment with different number ofmorphemesThe following experiments were done respectivelyby decreasing the number of morphemes, i.e., 14and 10 morphemes, chosen from table 5.
Thealgorithm was then run with 3 groups of 6 differentmorphemes, in which the morphemes weredifferent, and the combination of morphemes ineach group was random.
The morphemes in eachgroup are shown in table 9.
Other conditions forthe experiments were unchanged.6.1 Results and EvaluationTable 10 shows the results with different numberof morphemes, and table 11 shows those fordifferent groups of 6 morphemes.
For convenientcomparison, the tables only show the results of thefull test set, i.e., no threshold filtering.It is shown that the recall falls as the number ofmorphemes is reduced.
However, even the averagerecall 66.29% under the 6-morpheme condition isstill higher than that under the 40-word condition(59.57%).
In section 5, it was evaluated that lowrecall could be attributed to the low frequency oftest set words.
Therefore, 6 to 10 morphemes arealready ideal for deducing the SO of high-frequency words.Number of morphemes usedMorpheme 20 14 10 6 (Gp1)6(Gp2)6(Gp3)P(gift) 1   1P(good) 1 1 1 1P(happy) 1 1  1P(rich) 1 1   1P(honor) 1 1 1  1P (smooth) 1 1 1  1P(win) 1     1P(secure) 1     1P(health) 1 1 1   1P(hardworking) 1 1 1N (doubt) 1 1 1 1N (explode) 1 1  1N (ban) 1 1 1 1N (rash) 1 1 1  1N (greedy) 1 1 1  1N (difficult) 1 1 1  1N (hurt) 1 1    1N (rush) 1     1N (collapse) 1     1N (reject) 1Table 9: Morphemes selected for differentexperimental sets, P=+ve, N=-ve, 1=?selected?,Gp= GroupNumber of morphemes usedNo of morphemes 20 14 10Size of test set 1249 1249 1249Extracted Set 1062 993 915Precision (%) 80.23 79.15 79.89Recall (%) 85.03 79.50 73.26Table 10: Precision and Recall of SO-PMI of thetest set words with different no.
of morphemesGroup ofMorphemesGroup 1 Group 2 Group 3 AverageSize of test set 1249 1249 1249 1249Extracted Set 837 776 871 828Precision (%) 79.69 78.48 68.77 75.65Recall (%) 67.01 62.13 69.74 66.29Table 11: Precision and Recall of SO-PMI of thetest set words with 3 different groups of 6morphemesThe precision remains high from 20 morphemesto 6 morphemes, but from table 10 the precisionvaries with different sets of morphemes.
Group 3gave the lowest precision of 68.77%, whereasother groups gave a high precision close to 80%.The limited space of this paper cannot allow adetailed investigation into the reasons for thisresult, only some suggestions can be made.The precision may be related to the dominantlexical types of the words constructed by themorphemes and those of the test set words.
Lexicaltypes should be carefully considered in thealgorithm for Chinese because Chinese is anisolating language - no form change.
For example,the word ?
?
(recover) can appear in differentpositions of a sentence, such as the followingexamples extracted from the corpus:(1)?
?
(...Americaneconomy is gradually recovering?
)(2) ?
(?most people is now pessimistic about theeconomy recovery)(3) ?
(?decelerates the recovery, but also makes thefuture unpredictable.
)English allows different forms of ?recovery, like?recovery?, ?recovering?, ?recovered?
but Chinesedoes not.
Lexical types are thus an important factorfor the precision performance.
Another way ofsolving the problems of lexical types is theautomatic extraction of meaningful units(Danielsson, 2003).
Simply, meaningful units aresome frequently-used patterns which consist of twoor more words.
It is useful to automatically extractthe meaningful units with SO in future.Syntactic markers like negation, and creativeuses like ironical expression of adding quotationmarks can also affect the precision.
Here is anexample from the corpus:(?HONEST BUSINESSMAN?).
The quotationmark (?
?
in English) is to actually express theopposite meaning of words within the mark, i.e.,HONEST means DISHONEST in this case.
Suchmarkers should further be handled, just as with theuse of ?so-called?.6 Conclusion and Future WorkThis paper presents an algorithm based onTurney?s model (2003) for inferring SO of Chinesewords from their association with strongly-polarized Chinese morphemes.
The algorithm wasrun with 20 and 40 strongly-polarized Chinesewords respectively in a corpus of 34 million words,giving a high precision of 79.96% and 81.05%, buta low recall of 45.56% and 59.57%.
The algorithmwas then run with 20 Chinese polarizedmorphemes, or single characters, in the samecorpus, giving a high precision of 80.23% and aneven high recall of 85.03%.
The algorithm wasfurther run with just 14, 10 and 6 morphemes,giving a precision of 79.15%, 79.89% and 75.65%,and a recall of 79.50%, 73.26% and 66.29%respectively.Thus, conveniently defined morphemes inChinese enhance the effectiveness of the algorithmby simplifying processing and yielding betterresults even in a smaller corpus compared withwhat Turney (2003) used.
Just 6 to 10 morphemescan give satisfactory results in a smaller corpus.The efficient application of Turney?s algorithmwith help of colossal corpus like hundred-billion-word corpus is matched by the ready availability ofinternet texts.
However, the same convenience isnot available to Chinese because of the heavy costof word segmentation.The efficient application of Turney?s algorithmwith help of colossal corpus like hundred-billion-word corpus is matched by the ready availability ofinternet texts.
However, the same convenience isnot available to Chinese because of the heavy costof word segmentation.In our experiment, all syntactic markers areignored.
Better results can be expected if syntacticmarkers are taken into consideration.
An obviousexample is negation (not, never) which cancounteract the polarity of a word.
In future, we willtry to handle negation and other syntactic markers.The lists of the probability of morphemesforming polarized words in section 5.2 can behandled by the concept of decision list (Yarowsky,2000) which has not been applied in this paper forsimplification.
In the future, decision lists can beemployed to systematically include the loadedfeatures of morphemes.The experiment can be conducted with differentsets of paradigm morphemes, and on corpora ofdifferent sizes.
With the LIVAC synchronouscorpus (Tsou et al, 2000), it should be possible tocompare the SO of some words in differentcommunities like Beijing, Hong Kong and Taipei.The data would be valuable for cultural studies ifthe SO of some words fluctuates in differentcommunities.SO from association can be also applied to thejudgment of news articles like editorials oncelebrities.
Given a celebrity name or organizationname, we can calculate, using SO-PMI, thestrength of SO of the ?given word?, i.e., the name.Then we would be able to tell whether the newsabout the target is positive or negative.
Forexample, we tried to calculate the SO-PMI of thename ?George W Bush?, the U.S. President, withthousands of polarized Chinese words in thecorpus, it was found that the SO-PMI of ?Bush?was about -200 from January to February, 2003,and plunged to -500 from March to April, 2003,when U.S. launched an ?unauthorized war?
againstIraq.
Such useful applications will be furtherinvestigated in future.ReferencesDANIELSSON, P. 2003.
Automatic Extraction ofMeaningful Units from Corpora.
InternationalJournal of Corpus Linguistics, 8(1), 109-127.GUO XIAN-ZHEN, ZHANG WEI, LIU JIN, WANGLING-LING.
1999.
ChangYong BaoBianYi CiYuXiangJie CiDian ( ).Commercial Press, Beijing.HATZIVASSILOGLOU, V., AND MCKEOWN, K.R.1997.
Predicting the Semantic Orientation ofAdjectives.
Proceedings of the 35th Annual Meetingof the Association for Computational Linguistics andthe 8th Conference of the European Chapter of theACL, Madrid, Spain, 174-181.HATZIVASSILOGLOU, V. AND WIEBE, J.M.
2000.Effects of Adjective Orientation and Grad-ability onSentence Subjectivity.
Proceedings of 18thInternational Conference on ComputationalLinguistics (Coling?00), Saarbr?cken, Germany.LEHRER, A.
1974.
Semantic Fields and LexicalStructure.
North Holland, Amsterdam and New York.STONE, P.J., DUNPHY, D. C., SMITH, M. S., ANDOGILVIE, D. M. 1966.
The General Inquirer: AComputer Approach to Content Analysis.
MIT Press,Cambridge, MA.TSOU, B.K.
1976.
Homophony and Internal Change inChinese.
Computational Analyses of Asian andAfrican Languages 3, 67-86.TSOU, B.K., TSOI, W.F., LAI, T.B.Y., HU, J. ANDCHAN, S.W.K.
2000.
LIVAC, A ChineseSynchronous Corpus, and Some Applications.Proceedings of the ICCLC International Conferenceon Chinese Language Computing.
Chicago, 233-238.TURNEY, P.D.
2002.
Thumbs up or Thumbs down?Semantic Orientation Applied to UnsupervisedClassification of Reviews.
Proceedings of theAssociation for Computational Linguistics 40thAnniversary Meeting, University of Pennsylvania,Philadelphia, PA, USA.TURNEY, P.D.
& LITTMAN, M.L.
2003.
MeasuringPraise and Criticism: Inference of SemanticOrientation from Association.
ACM Transactions onInformation System (TOIS), 21(4), pp315-346.WANG GUO-ZHANG.
2001.
A Dictionary of ChinesePraise and Blame Words ().
Sinolingua, Beijing.WIEBE, J.M., BRUCE, R., BELL, M. MARTIN, M.,AND WILSON, T. 2001.
A Corpus Study ofEvaluative and Speculative Language.
Proceedingsof the Second ACL SIG on Dialogue Work-shop onDiscourse and Dialogue.
Aalborg, Denmark.YAROWSKY, D. 2000.
Hierarchical Decision Lists forWord Sense Disambiguation.
Computers and theHumanities, 34(1-2).
