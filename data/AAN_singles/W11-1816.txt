Proceedings of BioNLP Shared Task 2011 Workshop, pages 112?120,Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational LinguisticsBioNLP Shared Task 2011: Supporting ResourcesPontus Stenetorp: Goran Topic? Sampo PyysaloTomoko Ohta Jin-Dong Kim; and Jun?ichi Tsujii$Tsujii Laboratory, Department of Computer Science, University of Tokyo, Tokyo, Japan:Aizawa Laboratory, Department of Computer Science, University of Tokyo, Tokyo, Japan; Database Center for Life Science,Research Organization of Information and Systems, Tokyo, Japan$Microsoft Research Asia, Beijing, People?s Republic of China{pontus,goran,smp,okap}@is.s.u-tokyo.ac.jpjdkim@dbcls.rois.ac.jpjtsujii@microsoft.comAbstractThis paper describes the supporting resourcesprovided for the BioNLP Shared Task 2011.These resources were constructed with thegoal to alleviate some of the burden of sys-tem development from the participants and al-low them to focus on the novel aspects of con-structing their event extraction systems.
Withthe availability of these resources we also seekto enable the evaluation of the applicability ofspecific tools and representations towards im-proving the performance of event extractionsystems.
Additionally we supplied evaluationsoftware and services and constructed a vi-sualisation tool, stav, which visualises eventextraction results and annotations.
These re-sources helped the participants make sure thattheir final submissions and research effortswere on track during the development stagesand evaluate their progress throughout the du-ration of the shared task.
The visualisationsoftware was also employed to show the dif-ferences between the gold annotations andthose of the submitted results, allowing theparticipants to better understand the perfor-mance of their system.
The resources, evalu-ation tools and visualisation tool are providedfreely for research purposes and can be foundat http://sites.google.com/site/bionlpst/1 IntroductionFor the BioNLP?09 Shared Task (Kim et al, 2009),the first in the ongoing series, the organisers pro-vided the participants with automatically generatedsyntactic analyses for the sentences from the anno-tated data.
For evaluation purposes, tools were madepublicly available as both distributed software andonline services.
These resources were well received.A majority of the participants made use of one ormore of the syntactic analyses, which have remainedavailable after the shared task ended and have beenemployed in at least two independent efforts study-ing the contribution of different tools and forms ofsyntactic representation to the domain of informa-tion extraction (Miwa et al, 2010; Buyko and Hahn,2010).
The evaluation software for the BioNLP?09Shared Task has also been widely adopted in subse-quent studies (Miwa et al, 2010; Poon and Vander-wende, 2010; Bjo?rne et al, 2010).The reception and research contribution from pro-viding these resources encouraged us to continueproviding similar resources for the BioNLP SharedTask 2011 (Kim et al, 2011a).
Along with theparses we also encouraged the participants and ex-ternal groups to process the data with any NLP (Nat-ural Language Processing) tools of their choice andmake the results available to the participants.We provided continuous verification and evalua-tion of the participating systems using a suite of in-house evaluation tools.
Lastly, we provided a toolfor visualising the annotated data to enable the par-ticipants to better grasp the results of their experi-ments and to help gain a deeper understanding ofthe underlying concepts and the annotated data.
Thispaper presents these supporting resources.2 DataThis section introduces the data resources providedby the organisers, participants and external groupsfor the shared task.112Task Provider ToolCO University of Utah ReconcileCO University of Zu?rich UZCRSCO University of Turku TEESREL University of Turku TEESTable 1: Supporting task analyses provided, TEESis the Turku Event Extraction System and UZCRSis the University of Zu?rich Coreference ResolutionSystem2.1 Supporting task analysesThe shared task included three Supporting Tasks:Coreference (CO) (Nguyen et al, 2011), Entity re-lations (REL) (Pyysalo et al, 2011b) and Gene re-naming (REN) (Jourde et al, 2011).
In the sharedtask schedule, the supporting tasks were carried outbefore the main tasks (Kim et al, 2011b; Pyysaloet al, 2011a; Ohta et al, 2011; Bossy et al, 2011)in order to allow participants to make use of analy-ses from the systems participating in the SupportingTasks for their main task event extraction systems.Error analysis of BioNLP?09 shared task sub-missions indicated that coreference was the mostfrequent feature of events that could not be cor-rectly extracted by any participating system.
Fur-ther, events involving statements of non-trivial rela-tions between participating entities were a frequentcause of extraction errors.
Thus, the CO and RELtasks were explicitly designed to support parts ofthe main event extraction tasks where it had beensuggested that they could improve the system per-formance.Table 1 shows the supporting task analyses pro-vided to the participants.
For the main tasks, weare currently aware of one group (Emadzadeh et al,2011) that made use of the REL task analyses in theirsystem.
However, while a number of systems in-volved coreference resolution in some form, we arenot aware of any teams using the CO task analysesspecifically, perhaps due in part to the tight sched-ule and the somewhat limited results of the CO task.These data will remain available to allow future re-search into the benefits of these resources for eventextraction.2.2 Syntactic analysesFor syntactic analyses we provided parses for allthe task data in various formats from a wide rangeof parsers (see Table 2).
With the exception ofthe Pro3Gres1 parser (Schneider et al, 2007), theparsers were set up and run by the task organisers.The emphasis was put on availability for researchpurposes and variety of parsing models and frame-works to allow evaluation of their applicability fordifferent tasks.In part following up on the results of Miwa et al(2010) and Buyko and Hahn (2010) regarding theimpact on performance of event extraction systemsdepending on the dependency parse representation,we aimed to provide several dependency parse for-mats.
Stanford Dependencies (SD) and CollapsedStanford Dependencies (SDC), as described by deMarneffe et al (2006), were generated by convert-ing Penn Treebank (PTB)-style (Marcus et al, 1993)output using the Stanford CoreNLP Tools2 into thetwo dependency formats.
We also provided Confer-ence on Computational Natural Language Learningstyle dependency parses (CoNLL-X) (Buchholz andMarsi, 2006) which were also converted from PTB-style output, but for this we used the conversiontool3 from Johansson and Nugues (2007).
Whilethis conversion tool was not designed with convert-ing the output from statistical parsers in mind (butrather to convert between treebanks), it has previ-ously been applied successfully for this task (Miyaoet al, 2008; Miwa et al, 2010).The text from all documents provided were splitinto sentences using the Genia Sentence Splitter4(S?tre et al, 2007) and then postprocessed using aset of heuristics to correct frequently occurring er-rors.
The sentences were then tokenised using a to-kenisation script created by the organisers intendedto replicate the tokenisation of the Genia Tree Bank(GTB) (Tateisi et al, 2005).
This tokenised andsentence-split data was then used as input for allparsers.We used two deep parsers that provide phrasestructure analysis enriched with deep sentence struc-1https://files.ifi.uzh.ch/cl/gschneid/parser/2http://nlp.stanford.edu/software/corenlp.shtml3http://nlp.cs.lth.se/software/treebank converter/4http://www-tsujii.is.s.u-tokyo.ac.jp/y-matsu/geniass/113Name Format(s) Model Availability BioNLP?09Berkeley PTB, SD, SDC, CoNLL-X News Binary, Source NoC&C CCG, SD Biomedical Binary, Source YesEnju HPSG, PTB, SD, SDC, CoNLL-X Biomedical Binary NoGDep CoNLL-X Biomedical Binary, Source YesMcCCJ PTB, SD, SDC, CoNLL-X Biomedical Source YesPro3Gres Pro3Gres Combination ?
NoStanford PTB, SD, SDC, CoNLL-X Combination Binary, Source YesTable 2: Parsers, the formats for which their output was provided and which type of model that was used.
Theavailability column signifies public availability (without making an explicit request) for research purposestures, for example predicate-argument structure forHead-Driven Phrase Structure Grammar (HPSG).First we used the C&C Combinatory CategorialGrammar (CCG) parser5 (C&C) by Clark and Cur-ran (2004) using the biomedical model described inRimell and Clark (2009) which was trained on GTB.Unlike all other parsers for which we supplied SDand SDC dependency parses, the C&C output wasconverted from its native format using a separateconversion script provided by the C&C authors.
Re-grettably we were unable to provide CoNLL-X for-mat output for this parser due to the lack of PTB-style output.
The other deep parser used was theHPSG parser Enju6 by Miyao and Tsujii (2008), alsotrained on GTB.We also applied the frequently adopted StanfordParser7 (Klein and Manning, 2003) using a mixedmodel which includes data from the biomedical do-main, and the Charniak Johnson re-ranking parser8(Charniak and Johnson, 2005) using the self-trainedbiomedical model from McClosky (2009) (McCCJ).For the BioNLP?09 shared task it was observedthat the Bikel parser9 (Bikel, 2004), which used anon-biomedical model and can be argued that it usesthe somewhat dated Collins?
parsing model (Collins,1996), did not contribute towards event extractionperformance as strongly as other parses supplied forthe same data.
We therefore wanted to supply aparser that can compete with the ones above in a do-main which is different from the biomedical domainto see whether conclusions could be drawn as to the5http://svn.ask.it.usyd.edu.au/trac/candc/6http://www-tsujii.is.s.u-tokyo.ac.jp/enju/7http://nlp.stanford.edu/software/lex-parser.shtml8ftp://ftp.cs.brown.edu/pub/nlparser/9http://www.cis.upenn.edu/dbikel/software.htmlimportance of using a biomedical model.
For thiswe used the Berkeley parser10 (Petrov et al, 2006).Lastly we used a native dependency parser, the GE-NIA Dependency parser (GDep) by Sagae and Tsujii(2007).At least one team (Choudhury et al, 2011) per-formed experiments on some of the provided lexi-cal analyses and among the 14 submissions for theEPI and ID tasks, 13 submissions utilised tools forwhich resources were provided by the organisers ofthe shared task.
We intend to follow up on whetheror not the majority of the teams ran the tools them-selves or used the provided analyses.2.3 Other analysesThe call for analyses was open to all interested par-ties and all forms of analysis.
In addition to the Sup-porting Task analyses (CO and REL) and syntacticanalyses provided by various groups, the Universityof Antwerp CLiPS center (Morante et al, 2010) re-sponded to the call providing negation/speculationanalyses in the BioScope corpus format (Szarvas etal., 2008).Although this resource was not utilised by the par-ticipants for the main task, possibly due to a lack oftime, it is our hope that by keeping the data availableit can lead to further development of the participat-ing systems and analysis of BioScope and BioNLPST-style hedging annotations.3 ToolsThis section presents the tools produced by the or-ganisers for the purpose of the shared task.10http://code.google.com/p/berkeleyparser/1141 10411007-E1 Regulation <Exp>regulate[26-34] <Theme>TNF-alpha[79-88] ?
?<Excerpt>[regulate] an enhancer activity in the third intron of [TNF-alpha]2 10411007-E2 Gene_expression <Exp>activity[282-290] <Theme>TNF-alpha[252-261] ?
?<Excerpt>[TNF-alpha] gene displayed weak [activity]3 10411007-E3 +Regulation <Exp>when[291-295] <Theme>E2 <Excerpt>[when]Figure 1: Text output from the BioNLP?09 Shared Event Viewer with line numbering and newline markingsFigure 2: An illustration of collective (sentence 1)and distributive reading (sentence 2).
?Theme?
isabbreviated as ?Th?
and ?Protein?
as ?Pro?
whenthere is a lack of space3.1 VisualisationThe annotation data in the format specified by theshared task is not intended to be human-readable ?yet researchers need to be able to visualise the datain order to understand the results of their experi-ments.
However, there is a scarcity of tools that canbe used for this purpose.
There are three availablefor event annotations in the BioNLP ST format thatwe are aware of.One is the BioNLP?09 Shared Task EventViewer11, a simple text-based annotation viewer: itaggregates data from the annotations, and outputs itin a format (Figure 1) that is meant to be further pro-cessed by a utility such as grep.Another is What?s Wrong with My NLP12, whichvisualises relation annotations (see Figure 3a) ?
butis unable to display some of the information con-tained in the Shared Task data.
Notably, the distribu-tive and collective readings of an event are not dis-tinguished (Figure 2).
It also displays all annotationson a single line, which makes reading and analysinglonger sentences, let alne whole documents, some-what difficult.The last one is U-Compare13 (Kano et al, 2009),11http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask/downloads.shtml12http://code.google.com/p/whatswrong/13http://u-compare.org/bionlp2009.htmlwhich is a comprehensive suite of tools designed formanaging NLP workflows, integrating many avail-able services.
However, the annotation visualisationcomponent, illustrated in Figure 3b, is not optimisedfor displaying complex event structures.
Each anno-tation is marked by underlining its text segment us-ing a different colour per annotation type, and a rolein an event is represented by a similarly coloured arcbetween the related underlined text segments.
Theimplementation leaves some things to be desired:there is no detailed information added in the displayunless the user explicitly requests it, and then it isdisplayed in a separate panel, away from the text itannotates.
The text spacing makes no allowance forthe annotations, with opaque lines crossing over it,with the effect of making both the annotations andthe text hard to read if the annotations are above acertain degree of complexity.As a result of the difficulties of these existingtools, in order to extract a piece of annotated textand rework it into a graph that could be embeddedinto a publication, users usually read off the annota-tions, then create a graph from scratch using vectordrawing or image editing software.To address these issues, we created a visualisa-tion tool named stav (stav Text Annotation Visual-izer), that can read the data formatted according tothe Shared Task specification and aims to present itto the user in a form that can be grasped at a glance.Events and entities are annotated immediately abovethe text, and the roles within an event by labelledarcs between them (Figure 3c).
In a very complexgraph, users can highlight the object or associationof interest to follow it even more easily.
Special fea-tures of annotations, such as negation or speculation,are shown by unique visual cues, and more in-depth,technical information that is usually not required canbe requested by floating the mouse cursor over theannotation (as seen in Figure 5).We took care to minimise arc crossovers, and to115(a) Visualisation using What?s Wrong with My NLP(b) Visualisation using U-Compare(c) Visualisation using stavFigure 3: Different visualisations of complex textual annotations of Dickensheets et al (1999)116Figure 4: A screenshot of the stav file-browserkeep them away from the text itself, in order to main-tain text readability.
The text is spaced to accommo-date the annotations between the rows.
While thisdoes end up using more screen real-estate, it keepsthe text legible, and annotations adjacent to the text.The text is broken up into lines, and each sentenceis also forced into a new line, and given a numer-ical identifier.
The effect of this is that the text islaid out vertically, like an article would be, but withlarge spacing to accomodate the annotations.
Thearcs are similarly continued on successive lines, andcan easily be traced ?
even in case of them spanningmultiple lines, by the use of mouseover highlight-ing.
To preserve the distributionality information ofthe annotation, any event annotations are duplicatedfor each event, as demonstrated in the example inFigure 2.stav is not limited to the Shared Task datasets withappropriate configuration settings, it could also vi-sualise other kinds of relational annotations such as:frame structures (Fillmore, 1976) and dependencyparses (de Marneffe et al, 2006).To achieve our objectives above, we use the Dy-namic Scalable Vector Graphics (SVG) functional-ity (i.e.
SVG manipulated by JavaScript) providedby most modern browsers to render the WYSIWYG(What You See Is What You Get) representation ofthe annotated document.
An added benefit fromthis technique is that the installation process, if any,is very simple: although not all browsers are cur-rently supported, the two that we specifically testedagainst are Safari14 and Google Chrome15; the for-mer comes preinstalled with the Mac OS X oper-ating system, while the latter can be installed evenby relatively non-technical users.
The design is keptmodular using a dispatcher pattern, in order to al-low the inclusion of the visualiser tool into otherJavaScript-based projects.
The client-server archi-tecture also allows centralisation of data, so that ev-ery user can inspect an uploaded dataset without thehassle of downloading and importing into a desktopapplication, simply by opening an URL which canuniquely identify a document, or even a single an-notation.
A screenshot of the stav file browser canbe seen in Figure 4.3.2 Evaluation ToolsThe tasks of BioNLP-ST 2011 exhibit very highcomplexity, including multiple non-trivial subprob-lems that are partially, but not entirely, independentof each other.
With such tasks, the evaluation of par-ticipating systems itself becomes a major challenge.Clearly defined evaluation criteria and their preciseimplementation is critical not only for the compari-son of submissions, but also to help participants fol-low the status of their development and to identifythe specific strengths and weaknesses of their ap-proach.A further challenge arising from the complexityof the tasks is the need to process the relatively in-tricate format in which annotations are represented,which in turn carries a risk of errors in submissions.To reduce the risk of submissions being rejected orthe evaluation showing poor results due to format-ting errors, tools for checking the validity of the fileformat and annotation semantics are indispensable.For these reasons, we placed emphasis in the or-ganisation of the BioNLP-ST?11 on making tools forformat checking, validation and evaluation availableto the participants already during the early stages ofsystem development.
The tools were made avail-able in two ways: as downloads, and as online ser-vices.
With downloaded tools, participants can per-form format checking and evaluation at any timewithout online access, allowing more efficient op-timisation processes.
Each task in BioNLP-ST also14http://www.apple.com/safari15http://www.google.com/chrome117Figure 5: An example of a false negative illustrated by the evaluation tools in co-ordination with stavmaintained an online evaluation tool for the develop-ment set during the development period.
The onlineevaluation is intended to provide an identical inter-face and criteria for submitted data as the final on-line submission system, allowing participants to bebetter prepared for the final submission.
With on-line evaluation, the organisers could also monitorsubmissions to ensure that there were no problemsin, for example, the evaluation software implemen-tations.The system logs of online evaluation systemsshow that the majority of the participants submit-ted at least one package with formatting errors, con-firming the importance of tools for format checking.Further, most of the participants made use of the on-line development set evaluation at least once beforetheir final submission.To enhance the evaluation tools we drew upon thestav visualiser to provide a view of the submitted re-sults.
This was done by comparing the submittedresults and the gold data to produce a visualisationwhere errors are highlighted, as illustrated in Fig-ure 5.
This experimental feature was available forthe EPI and ID tasks and we believe that by doing soit enables participants to better understand the per-formance of their system and work on remedies forcurrent shortcomings.4 Discussion and ConclusionsAmong the teams participating in the EPI and IDtasks, a great majority utilised tools for which re-sources were made available by the organisers.
Wehope that the continued availability of the parses willencourage further investigation into the applicabilityof these and similar tools and representations.As for the analysis of the supporting analyses pro-vided by external groups and the participants, we areso far aware of only limited use of these resourcesamong the participants, but the resources will re-main available and we are looking forward to seefuture work using them.To enable reproducibility of our resources, weprovide a publicly accessible repository containingthe automated procedure and our processing scriptsused to produce the released data.
This repositoryalso contains detailed instructions on the options andversions used for each parser and, if the software li-cense permits it, includes the source code or binarythat was used to produce the processed data.
For thecases where the license restricts redistribution, in-structions and links are provided on how to obtainthe same version that was used.
We propose that us-ing a multitude of parses and formats can benefit notjust the task of event extraction but other NLP tasksas well.We have also made our evaluation tools and visu-alisation tool stav available along with instructionson how to run it and use it in coordination with theshared task resources.
The responses from the par-ticipants in relation to the visualisation tool werevery positive, and we see this as encouragement toadvance the application of visualisation as a way tobetter reach a wider understanding and unificationof the concept of events for biomedical event extrac-tion.All of the resources described in this paper areavailable at http://sites.google.com/site/bionlpst/.118AcknowledgementsWe would like to thank Jari Bjo?rne of the Uni-versity of Turku BioNLP group; Gerold Schneider,Fabio Rinaldi, Simon Clematide and Don Tuggenerof the Univerity of Zurich Computational Linguis-tics group; Roser Morante of University of AntwerpCLiPS center; and Youngjun Kim of the Univer-sity of Utah Natural Language Processing ResearchGroup for their generosity with their time and exper-tise in providing us with supporting analyses.This work was supported by Grant-in-Aid forSpecially Promoted Research (MEXT, Japan) andthe Royal Swedish Academy of Sciences.ReferencesDaniel M. Bikel.
2004.
Intricacies of Collins?
ParsingModel.
Computational Linguistics, 30(4):479?511.J.
Bjo?rne, F. Ginter, S. Pyysalo, J. Tsujii, andT.
Salakoski.
2010.
Complex event extraction atPubMed scale.
Bioinformatics, 26(12):i382.Robert Bossy, Julien Jourde, Philippe Bessie`res, Marteenvan de Guchte, and Claire Ne?dellec.
2011.
BioNLPShared Task 2011 - Bacteria Biotope.
In Proceedingsof the BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proceed-ings of the Tenth Conference on Computational Nat-ural Language Learning, pages 149?164.
Associationfor Computational Linguistics.E.
Buyko and U. Hahn.
2010.
Evaluating the impactof alternative dependency graph encodings on solv-ing event extraction tasks.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 982?992.
Association forComputational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt DiscriminativeReranking.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL?05), pages 173?180.Pallavi Choudhury, Michael Gamon, Chris Quirk, andLucy Vanderwende.
2011.
MSR-NLP entry inBioNLP Shared Task 2011.
In Proceedings ofthe BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.S.
Clark and J.R. Curran.
2004.
Parsing the WSJ us-ing CCG and log-linear models.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, page 103.
Association for Com-putational Linguistics.Michael John Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
In Proceed-ings of the 34th Annual Meeting of the Associationfor Computational Linguistics, pages 184?191, SantaCruz, California, USA, June.
Association for Compu-tational Linguistics.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InProceedings of the Fifth International Conferenceon Language Resources and Evaluation (LREC?06),pages 449?454.H.L.
Dickensheets, C. Venkataraman, U. Schindler, andR.P.
Donnelly.
1999.
Interferons inhibit activation ofSTAT6 by interleukin 4 in human monocytes by in-ducing SOCS-1 gene expression.
Proceedings of theNational Academy of Sciences of the United States ofAmerica, 96(19):10800.Ehsan Emadzadeh, Azadeh Nikfarjam, and GracielaGonzalez.
2011.
A generalizable and efficient ma-chine learning approach for biological event extractionfrom text.
In Proceedings of the BioNLP 2011 Work-shop Companion Volume for Shared Task, Portland,Oregon, June.
Association for Computational Linguis-tics.Charles J. Fillmore.
1976.
Frame semantics and the na-ture of language.
Annals of the New York Academy ofSciences, 280(1):20?32.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProc.
of the 16th Nordic Conference on ComputationalLinguistics (NODALIDA).Julien Jourde, Alain-Pierre Manine, Philippe Veber,Kare?n Fort, Robert Bossy, Erick Alphonse, andPhilippe Bessie`res.
2011.
BioNLP Shared Task 2011- Bacteria Gene Interactions and Renaming.
In Pro-ceedings of the BioNLP 2011 Workshop CompanionVolume for Shared Task, Portland, Oregon, June.
As-sociation for Computational Linguistics.Yoshinobu Kano, William Baumgartner, Luke McCro-hon, Sophia Ananiadou, Kevin Cohen, Larry Hunter,and Jun?ichi Tsujii.
2009.
U-Compare: share andcompare text mining tools with UIMA.
Bioinformat-ics, 25(15):1997?1998, May.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP?09 Shared Task on Event Extraction.In Proceedings of Natural Language Processing inBiomedicine (BioNLP) NAACL 2009 Workshop, pages1?9.119Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, RobertBossy, and Jun?ichi Tsujii.
2011a.
Overviewof BioNLP Shared Task 2011.
In Proceedings ofthe BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-nori Yonezawa.
2011b.
Overview of the Genia Eventtask in BioNLP Shared Task 2011.
In Proceedingsof the BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.D.
Klein and C.D.
Manning.
2003.
Fast exact infer-ence with a factored model for natural language pars-ing.
Advances in neural information processing sys-tems, pages 3?10.M.P Marcus, B. Santorini, and M.A Marcinkiewicz.1993.
Building a large annotated corpus of English:The Penn Tree Bank.
Computational Linguistics,pages 313?318.D.
McClosky.
2009.
Any Domain Parsing: AutomaticDomain Adaptation for Natural Language Parsing.Ph.D.
thesis, Ph.
D. thesis, Department of ComputerScience, Brown University.M.
Miwa, S. Pyysalo, T. Hara, and J. Tsujii.
2010.
Eval-uating Dependency Representation for Event Extrac-tion.
In In the 23rd International Conference on Com-putational Linguistics (COLING 2010), pages 779?787.Y.
Miyao and J. Tsujii.
2008.
Feature forest models forprobabilistic HPSG parsing.
Computational Linguis-tics, 34(1):35?80.Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-suzaki, and Jun?ichi Tsujii.
2008.
Task-oriented eval-uation of syntactic parsers and their representations.
InProceedings of ACL-08: HLT, pages 46?54, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.R.
Morante, V. Van Asch, and W. Daelemans.
2010.Memory-based resolution of in-sentence scopes ofhedge cues.
CoNLL-2010: Shared Task, page 40.Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii.
2011.Overview of the Protein Coreference task in BioNLPShared Task 2011.
In Proceedings of the BioNLP 2011Workshop Companion Volume for Shared Task, Port-land, Oregon, June.
Association for ComputationalLinguistics.Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii.
2011.Overview of the Epigenetics and Post-translationalModifications (EPI) task of BioNLP Shared Task2011.
In Proceedings of the BioNLP 2011 WorkshopCompanion Volume for Shared Task, Portland, Oregon,June.
Association for Computational Linguistics.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the 44thannual meeting of the Association for ComputationalLinguistics, pages 433?440.
Association for Computa-tional Linguistics.H.
Poon and L. Vanderwende.
2010.
Joint inferencefor knowledge extraction from biomedical literature.In Human Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 813?821.
Association for Computational Linguistics.Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-livan, Chunhong Mao, Chunxia Wang, Bruno So-bral, Jun?ichi Tsujii, and Sophia Ananiadou.
2011a.Overview of the Infectious Diseases (ID) task ofBioNLP Shared Task 2011.
In Proceedings ofthe BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii.2011b.
Overview of the Entity Relations (REL) sup-porting task of BioNLP Shared Task 2011.
In Pro-ceedings of the BioNLP 2011 Workshop CompanionVolume for Shared Task, Portland, Oregon, June.
As-sociation for Computational Linguistics.Laura Rimell and Stephen Clark.
2009.
Porting alexicalized-grammar parser to the biomedical domain.Journal of Biomedical Informatics, 42(5):852 ?
865.Biomedical Natural Language Processing.R.
S?tre, K. Yoshida, A. Yakushiji, Y. Miyao, Y. Matsub-yashi, and T. Ohta.
2007.
AKANE system: protein-protein interaction pairs in BioCreAtIvE2 challenge,PPI-IPS subtask.
In Proceedings of the SecondBioCreative Challenge Workshop, pages 209?212.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependency pars-ing and domain adaptation with LR models and parserensembles.
In Proceedings of the CoNLL 2007 SharedTask.G.
Schneider, M. Hess, and P. Merlo.
2007.
Hybridlong-distance functional dependency parsing.
Unpub-lished PhD thesis, Institute of Computational Linguis-tics, University of Zurich.G.
Szarvas, V. Vincze, R. Farkas, and J. Csirik.
2008.The BioScope corpus: annotation for negation, uncer-tainty and their scope in biomedical texts.
In Proceed-ings of the Workshop on Current Trends in BiomedicalNatural Language Processing, pages 38?45.
Associa-tion for Computational Linguistics.Y.
Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii.
2005.Syntax Annotation for the GENIA corpus.
In Proceed-ings of the IJCNLP, pages 222?227.120
