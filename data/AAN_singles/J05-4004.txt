Induction of Word and Phrase Alignmentsfor Automatic Document SummarizationHal Daume?
III?Information Sciences InstituteUniversity of Southern CaliforniaDaniel Marcu?Information Sciences InstituteUniversity of Southern CaliforniaCurrent research in automatic single-document summarization is dominated by two effective,yet na?
?ve approaches: summarization by sentence extraction and headline generation via bag-of-words models.
While successful in some tasks, neither of these models is able to adequatelycapture the large set of linguistic devices utilized by humans when they produce summaries.One possible explanation for the widespread use of these models is that good techniques havebeen developed to extract appropriate training data for them from existing document/abstractand document/ headline corpora.
We believe that future progress in automatic summarizationwill be driven both by the development of more sophisticated, linguistically informed models,as well as a more effective leveraging of document/abstract corpora.
In order to open the doorsto simultaneously achieving both of these goals, we have developed techniques for automaticallyproducing word-to-word and phrase-to-phrase alignments between documents and their human-written abstracts.
These alignments make explicit the correspondences that exist in such docu-ment/abstract pairs and create a potentially rich data source from which complex summarizationalgorithms may learn.
This paper describes experiments we have carried out to analyze the abilityof humans to perform such alignments, and based on these analyses, we describe experiments forcreating them automatically.
Our model for the alignment task is based on an extension of thestandard hidden Markov model and learns to create alignments in a completely unsupervisedfashion.
We describe our model in detail and present experimental results that show that ourmodel is able to learn to reliably identify word- and phrase-level alignments in a corpus of?document, abstract?
pairs.1.
Introduction and Motivation1.1 MotivationWe believe that future success in automatic document summarization will be madepossible by the combination of complex, linguistically motivated models and effectiveleveraging of data.
Current research in summarization makes a choice between thesetwo: one either develops sophisticated, domain-specific models that are subsequentlyhand-tuned without the aid of data, or one develops na?
?ve general models that can?
4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292.
Email: {hdaume,marcu}@isi.edu.Submission received: 12 January 2005; revised submission received: 3 May 2005; accepted forpublication: 27 May 2005.?
2006 Association for Computational LinguisticsComputational Linguistics Volume 31, Number 4Figure 1Example alignment of a single abstract sentence with two document sentences.be trained on large amounts of data (in the form of corpora of document/extract ordocument/headline pairs).
One reason for this is that currently available technologiesare only able to extract very coarse and superficial information that is inadequatefor training complex models.
In this article, we propose a method to overcome thisproblem: automatically generating word-to-word and phrase-to-phrase alignmentsbetween documents and their human-written abstracts.1To facilitate discussion and to motivate the problem, we show in Figure 1 a rela-tively simple alignment between a document fragment and its corresponding abstractfragment from our corpus.2 In this example, a single abstract sentence (shown alongthe top of the figure) corresponds to exactly two document sentences (shown along thebottom of the figure).
If we are able to automatically generate such alignments, one canenvision the development of models of summarization that take into account effectsof word choice, phrasal and sentence reordering, and content selection.
Such modelscould be simultaneously linguistically motivated and data-driven.
Furthermore, suchalignments are potentially useful for current-day summarization techniques, includingsentence extraction, headline generation, and document compression.A close examination of the alignment shown in Figure 1 leads us to three obser-vations about the nature of the relationship between a document and its abstract, andhence about the alignment itself: Alignments can occur at the granularity of words and of phrases. The ordering of phrases in an abstract can be different from the ordering ofphrases in the document. Some abstract words do not have direct correspondents in the document,and many document words are never used in an abstract.In order to develop an alignment model that could recreate such an alignment, weneed our model to be able to operate both at the word level and at the phrase level, weneed it to be able to allow arbitrary reorderings, and we need it to be able to accountfor words on both the document and abstract side that have no direct correspondence.
Inthis paper, we develop an alignment model that is capable of learning all these aspectsof the alignment problem in a completely unsupervised fashion.1 We will use the words abstract and summary interchangeably.
When we wish to emphasize that aparticular summary is extractive, we will refer to it as an extract.2 As part of the tokenization step, any possessive form is split off from its noun, and represented as POSS.In most cases, this involves separating an ??s?
and replacing it with POSS, as in ?John?s?
?
?John POSS.
?For consistency, we have treated ?it?s?/?its?
as a special case: ?its?
(the possessive) is converted to?it POSS.?506Daume?
and Marcu Alignments for Automatic Document Summarization1.2 Shortcomings of Current Summarization ModelsCurrent state-of-the-art automatic single-document summarization systems employone of three techniques: sentence extraction, bag-of-words headline generation, ordocument compression.
Sentence extraction systems take full sentences from a doc-ument and concatenate them to form a summary.
Research in sentence extractioncan be traced back to work in the mid 1950s and late 1960s by Luhn (1956) andEdmundson (1969).
Recent techniques are startlingly not terribly divergent from theseoriginal methods; see Mani and Maybury (1999); Marcu (2000); Mani (2001) for a com-prehensive overview.
Headline generation systems, on the other hand, typically extractindividual words from a document to produce a very short headline-style summary;see Banko, Mittal, and Witbrock (2000); Berger and Mittal (2000); Schwartz, Zajic, andDorr (2002) for representative examples.
Between these two extremes, there has beena relatively modest amount of work in sentence simplification (Chandrasekar, Doran,and Bangalore 1996; Mahesh 1997; Carroll et al 1998; Grefenstette 1998; Jing 2000;Knight and Marcu 2002) and document compression (Daume?
III and Marcu 2002;Daume?
III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases,and sentences are selected in an extraction process.While such approaches have enjoyed some success, they all suffer from modelingshortcomings.
Sentence extraction systems and document compression models makeunrealistic assumptions about the summarization task (namely, that extraction is suffi-cient and that sentences are the appropriate level of granularity).
Headline generationsystems employ very weak models that make an incorrect bag-of-words assumption.This assumption allows such systems to learn limited transformations to produceheadlines from arbitrary documents, but such transformations are not nearly complexenough to adequately model anything beyond indicative summaries at a length ofaround 10 words.
Bag-of-words models can learn what the most important words tokeep in a headline are, but say nothing about how to structure them in a well-formed,grammatical headline.In our own work on document compression models (Daume?
III and Marcu 2002;Daume?
III and Marcu 2004), both of which extend the sentence compression model ofKnight and Marcu (2002), we assume that sentences and documents can be summa-rized exclusively through deletion of contiguous text segments.
In Knight and Marcu?sdata, we found that from a corpus of 39,060 abstract sentences, only 1,067 were createdfrom corresponding document sentences via deletion of contiguous segments.
In otherwords, only 2.7% of the sentences in real ?document, abstract?
pairs can be explainedby the model proposed by Knight and Marcu (2002).
Such document compressionmodels do not explain the rich set of linguistic devices employed, for example, inFigure 1.1.3 Prior Work on AlignmentsIn the sentence extraction community, there exists a wide variety of techniques for(essentially) creating alignments between document sentences and abstract sentences(Kupiec, Pedersen, and Chen 1995; Teufel and Moens 1997; Marcu 1999); see alsoBarzilay and Elhadad (2003); Quirk, Brockett, and Dolan (2004) for work describingalignments for the monolingual paraphrasing task.
These techniques typically takeinto account information such as lexical overlap, synonymy, ordering, length, dis-course structure, and so forth.
The sentence alignment problem is a comparativelysimple problem to solve, and current approaches work quite well.
Unfortunately, these507Computational Linguistics Volume 31, Number 4alignments are the least useful, because they can only be used to train sentence ex-traction systems.In the context of headline generation, simple statistical models are used for aligningdocuments and headlines (Banko, Mittal, and Witbrock 2000; Berger and Mittal 2000;Schwartz, Zajic, and Dorr 2002), based on IBM Model 1 (Brown et al 1993).
Thesemodels treat documents and headlines as simple bags of words and learn probabilisticword-based mappings between the words in the documents and the words in theheadlines.
Such mappings can be considered word-to-word alignments, but as ourresults show (see Section 5), these models are too weak for capturing the sophisticatedoperations that are employed by humans in summarizing texts.To date, there has been very little work on the word alignment task in the contextof summarization.
The most relevant work is that of Jing (2002), in which a hiddenMarkov alignment model is applied to the task of identifying word and phrase-levelcorrespondences between documents and abstracts.
Unfortunately, this model is onlyable to align words that are identical up to their stems, and thus suffers from a problemof recall.
This also makes it ill-suited to the task of learning how to perform abstraction,in which one would desire to know how words get changed.
For example, Jing?s modelcannot identify any of the following alignments from Figure 1: (Connecting Point ?Connecting Point Systems), (Mac ?
Macintosh), (retailer ?
seller), (Macintosh ?
AppleMacintosh systems) and (January 1989 ?
last January).Word alignment (and, to a lesser degree, phrase alignment) has been an activetopic of research in the machine translation community.
Based on these efforts, onemight be initially tempted to use readily available alignment models developed in thecontext of machine translation, such as GIZA++ (Och and Ney 2003), to obtain word-level alignments in ?document, abstract?
corpora.
However, as we will show (Section 5),the alignments produced by such a system are inadequate for the ?document, abstract?alignment task.1.4 Article StructureIn this article, we describe a novel, general model for automatically inducing word-and phrase-level alignments between documents and their human-written abstracts.Beginning in Section 2, we will describe the results of human annotation of such align-ments.
Based on this annotation, we will investigate the empirical linguistic propertiesof such alignments, including lexical transformations and movement.
In Section 3, wewill introduce the statistical model we use for deriving such alignments automatically.The inference techniques are based on those of semi-Markov models, extensions ofhidden Markov models that allow for multiple simultaneous observations.After our discussion of the model structure and algorithms, we discuss the variousparameterizations we employ in Section 4.
In particular, we discuss three distinct mod-els of movement, two of which are well-known in the machine translation alignmentliterature, and a third one that exploits syntax in a novel, ?light?
manner.
We also discussseveral models of lexical rewriting, based on identities, stems, WordNet synonymy,and automatically induced lexical replacements.
In Section 5, we present experimentalresults that confirm that our model is able to learn the hidden structure in our corpusof ?document, abstract?
pairs.
We compare our model against well-known alignmentmodels designed for machine translation as well as a state-of-the-art alignment modelspecifically designed for summarization (Jing 2002).
Additionally, we discuss errors thatthe model currently makes, supported by some relevant examples and statistics.
Weconclude with some directions for future research (Section 6).508Daume?
and Marcu Alignments for Automatic Document SummarizationTable 1Ziff-Davis corpus statistics.Sub-corpus AnnotatedAbstracts Documents Abstracts DocumentsDocuments 2033 45Sentences 13k 82k 244 2kWords 261k 2.5M 6.4k 49kUnique words 14k 42k 1.9k 5.9k45k 6kSentences/Doc 6.28 40.83 5.42 45.3Words/Doc 128.52 1229.71 142.33 1986.16Words/Sent 20.47 28.36 26.25 24.202.
Human-produced AlignmentsIn order to decide how to design an alignment model and to judge the quality of thealignments produced by a system, we first need to create a set of ?gold standard?alignments.
To this end, we asked two human annotators to manually constructsuch alignments between documents and their abstracts.
These ?document, abstract?pairs were drawn from the Ziff-Davis collection (Marcu 1999).
Of the roughly 7,000documents in that corpus, we randomly selected 45 pairs for annotation.
We added tothis set of 45 pairs the 2,000 shorter documents from this collection, and all the workdescribed in the remainder of this paper focuses on this subset of 2,033 ?document,abstract?
pairs.3 Statistics for this sub-corpus and for the pairs selected for annotationare shown in Table 1.
As can be simply computed from this table, the compression ratein this corpus is about 12%.
The first five human-produced alignments were completedseparately and then discussed; the last 40 were done independently.2.1 Annotation GuidelinesAnnotators were asked to perform word-to-word and phrase-to-phrase alignmentsbetween abstracts and documents, and to classify each alignment as either possible (P)or sure (S), where S ?
P, following the methodology used in the machine translationcommunity (Och and Ney 2003).
The direction of containment (S ?
P) is because beinga sure alignment is a stronger requirement than being a possible alignment.
A full descrip-tion of the annotation guidelines is available in a document available with the alignmentsoftware on the first author?s web site (http://www.isi.edu/?hdaume/HandAlign).Here, we summarize the main points.The most important instruction that annotators were given was to align everythingin the summary to something.
This was not always possible, as we will discuss shortly,but by and large it was an appropriate heuristic.
The second major instruction was tochoose alignments with maximal consecutive length: If there are two possible alignmentsfor a phrase, the annotators were instructed to choose the one that will result in thelongest consecutive alignment.
For example, in Figure 1, this rule governs the choice of3 The reason there are 2,033 pairs, not 2,045, is that 12 of the original 45 pairs were among the 2,000shortest, so the 2,033 pairs are obtained by taking the 2,000 shortest and adding to them the 33 pairs thatwere annotated and not already among the 2,000 shortest.509Computational Linguistics Volume 31, Number 4the alignment of the word Macintosh on the summary side: lexically, it could be alignedto the final occurrence of the word Macintosh on the document side, but by aligning it toApple Macintosh systems, we are able to achieve a longer consecutive sequence of alignedwords.The remainder of the instructions have to do primarily with clarifying particularlinguistic phenomena including punctuation, anaphora (for entities, annotators are toldto feel free to align names to pronouns, for instance) and metonymy, null elements,genitives, appositives, and ellipsis.2.2 Annotator AgreementTo compute annotator agreement, we employed the kappa statistic.
To do so, we treatthe problem as a sequence of binary decisions: given a single summary word anddocument word, should the two be aligned?
To account for phrase-to-phrase align-ments, we first converted these into word-to-word alignments using the ?all pairs?heuristic.
By looking at all such pairs, we wound up with 7.2 million items over whichto compute the kappa statistic (with two annotators and two categories).
Annotatoragreement was strong for sure alignments and fairly weak for possible alignments.When considering only sure alignments, the kappa statistic for agreement was 0.63(though it dropped drastically to 0.42 on possible alignments).In performing the annotation, we found that punctuation and non-content wordsare often very difficult to align (despite the discussion of these issues in the alignmentguidelines).
The primary difficulty with function words is that when the summarizershave chosen to reorder words to use slightly different syntactic structures, there arelexical changes that are hard to predict.4 Fortunately, for many summarization tasks,it is much more important to get content words right, rather than function words.When words on a stop list of 58 function words and punctuation were ignored, thekappa value rose to 0.68.
Carletta (1995) has suggested that kappa values over 0.80reflect very strong agreement and that kappa values between 0.60 and 0.80 reflect goodagreement.52.3 Results of AnnotationAfter the completion of these alignments, we can investigate some of their properties.Such an investigation is interesting both from the perspective of designing a model andfrom a linguistic perspective.In the alignments, we found that roughly 16% of the abstract words are left un-aligned.
This figure includes both standard lexical words and punctuation.
Of this16%, 4% are punctuation marks (though not all punctuation is unaligned) and 7%are function words.
The remaining 5% are words that would typically be consideredcontent words.
This rather surprising result tells us that any model we build needsto be able to account for a reasonable portion of the abstract to not have a directcorrespondence to any portion of the document.4 For example, the change from I gave a gift to the boy.
to The boy received a gift from me.
is relativelystraightforward; however, it is a matter of opinion whether to and from should be aligned ?
they serve thesame role, but certainly do not mean the same thing.5 All annotator agreement figures are calculated only on the last 40 ?document, abstract?
pairs, which wereannotated independently.510Daume?
and Marcu Alignments for Automatic Document SummarizationTo get a sense of the importance of producing alignments at the phrase level, wecomputed that roughly 75% of the alignments produced by humans involve only oneword on both sides.
In 80% of the alignments, the summary side is a single word (thusin 5% of the cases, a single summary word is aligned to more than one documentword).
In 6.1% of the alignments, the summary side involved a phrase of length two,and in 2.2% of the cases it involved a phrase of length three.
In all these numbers, careshould be taken to note that the humans were instructed to produce phrase alignmentsonly when word alignments were impossible.
Thus, it is entirely likely that summaryword i is aligned to document word j and summary word i + 1 is aligned to documentword j + 1, in which case we count this as two singleton alignments, rather than analignment of length two.
These numbers suggest that looking at phrases in addition towords is empirically important.Lexical choice is another important aspect of the alignment process.
Of all thealigned summary words and phrases, the corresponding document word or phrase wasexactly the same as that on the summary side in 51% of the cases.
When this constraintwas weakened to looking only at stems (for multi-word phrases, a match meant thateach corresponding word matched up to stem), this number rose to 67%.
When brokendown into cases of singletons and non-singletons, we saw that 86% of singletons areidentical up to stem, and 48% of phrases are identical up to stem.
This suggests thatlooking at stems, rather than lexical items, is useful.Finally, we investigated the issue of adjacency in the alignments.
Specifically, weconsider the following question: Given that a summary phrase ending at position iis aligned to a document phrase ending at position j, what is a likely position in thedocument for the summary phrase beginning at position i + 1?
It turns out that thisis overwhelmingly j + 1.
In Figure 2, we have plotted the frequencies of such relativejumps over the human-aligned data.
This graph suggests that a model biased towardstepping forward monotonically in the document is likely to be appropriate.
However,it should also be noted that backward jumps are also quite common, suggesting that amonotonic alignment model is inappropriate for this task.3.
Statistical Alignment ModelBased on linguistic observations from the previous section, we reach several conclu-sions regarding the development of a statistical model to produce such alignments.First, the model should be able to produce alignments between phrases of arbitrarylength (but perhaps with a bias toward single words).
Second, it should not be con-strained by any assumptions of monotonicity or word (or stem) identity, but it mightbe able to realize that monotonicity and word and stem identity are good indicators ofalignment.
Third, our model must be able to account for words on the abstract sidethat have no correspondence on the document side (following the terminology from themachine translation community, we will refer to such words as null generated).3.1 Generative StoryBased on these observations, and with an eye toward computational tractability, weposit the following generative story for how a summary is produced, given a document:1.
Repeat until the whole summary is generated:(a) Choose a document position j and jump there.511Computational Linguistics Volume 31, Number 4Figure 2Analysis of the motion observed in documents when considering a movement of +1 on thesummary side.
(b) Choose a document phrase length l.(c) Generate a summary phrase based on the document phrasespanning positions j to j + l.2.
Jump to the end of the document.In order to account for null generated summary words, we augment the above gen-erative story with the option to jump to a specifically designated null state from which asummary phrase may be generated without any correspondence in the document.
Frominspection of the human-aligned data, most such null generated words are functionwords or punctuation; however, in some cases, there are pieces of information in thesummary that truly did not exist in the original document.
The null generated wordscan account for these as well (additionally, the null generated words allow the modelto ?give up?
when it cannot do anything better).
We require that summary phrasesproduced from the null state have length 1, so that in order to generate multiple nullgenerated words, they must be generated independently.In Figure 3, we have shown a portion of the generative process that would give riseto the alignment in Figure 1.This generative story implicitly induces an alignment between the document andthe summary: the summary phrase is considered to be aligned to the document phrase512Daume?
and Marcu Alignments for Automatic Document SummarizationFigure 3Beginning and end of the generative process that gave rise to the alignment in Figure 1, which isreproduced here for convenience.that ?generated?
it.
In order to make this computationally tractable, we must introducesome conditional independence assumptions.
Specifically, we assume the following:1.
Decision (a) in our generative story depends only on the position of theend of the current document phrase (i.e., j + l).2.
Decision (b) is conditionally independent of every other decision.3.
Decision (c) depends only on the phrase at the current document position.3.2 Statistical ModelBased on the generative story and independence assumptions described above, we canmodel the entire summary generation process according to two distributions: jump( j?
| j + l), the probability of jumping to position j?
in the documentwhen the previous phrase ended at position j + l. rewrite(s | dj:j+l), the rewrite probability of generating summary phrase sgiven that we are considering the sub-phrase of d beginning at position jand ending at position j + l.Specific parameterizations of the distributions jump and rewrite will be discussed inSection 4 to enable the focus here to be on the more general problems of inference anddecoding in such a model.
The model described by these independence assumptionsvery much resembles that of a hidden Markov model (HMM), where states in the statespace are document ranges and emissions are summary words.
The difference is thatinstead of generating a single word in each transition between states, an entire phraseis generated.
This difference is captured by the semi-Markov model or segmental HMMframework, described in great detail by Ostendorf, Digalakis, and Kimball (1996); seealso Ferguson (1980); Gales and Young (1993); Mitchell, Jamieson, and Harper (1995);Smyth, Heckerman, and Jordan (1997); Ge and Smyth (2000); Aydin, Altunbasak, andBorodovsky (2004) for more detailed descriptions of these models as well as other ap-plications in speech processing and computational biology.
In the following subsections,we will briefly discuss the aspects of inference that are relevant to our problem, butthe interested reader is directed to Ostendorf, Digalakis, and Kimball (1996) for moredetails.3.3 Creating the State SpaceGiven our generative story, we can construct a semi-HMM to calculate precisely thealignment probabilities specified by our model in an efficient manner.
A semi-HMM is513Computational Linguistics Volume 31, Number 4fully defined by a state space (with designated start and end states), an output alpha-bet, transition probabilities, and observation probabilities.
The semi-HMM functionslike an HMM: Beginning at the start state, stochastic transitions are made through thestate space according to the transition probabilities.
At each step, one or more observa-tions are generated.
The machine stops when it reaches the end state.In our case, the state set is large, but well structured.
There is a unique initial state?start?, a unique final state ?end?, and a state for each possible document phrase.
Thatis, for a document of length n, for all 1 ?
i ?
i?
?
n, there is a state that correspondsto the document phrase beginning at position i and ending at position i?, which wewill refer to as ri,i?
.
There is also a null state for each document position r?,i.
Thus, S ={?start?, ?end?}
?
{ri,i?
: 1 ?
i ?
i?
?
n} ?
{r?,i : 1 ?
i ?
n}.
The output alphabet consistsof each word found in S, plus the end-of-sentence word ?.
We only allow the word ?to be emitted on a transition to the end state.
The transition probabilities are managedby the jump model, and the emission probabilities are managed by the rewrite model.Consider the document a b (the semi-HMM for which is shown in Figure 4) inthe case when the corresponding summary is c d. Suppose the correct alignment isthat c d is aligned to a and b is left unaligned.
Then, the path taken through thesemi-HMM is ?start?
?
a ?
?end?.
During the transition ?start?
?
a, c d is emitted.During the transition a ?
?end?, ?
is emitted.3.4 Expectation MaximizationThe alignment task, as described above, is a chicken-and-egg problem: if we knewthe model components (namely, the rewrite and jump tables), we would be able toefficiently find the best alignment.
Similarly, if we knew the correct alignments, wewould be able to estimate the model components.
Unfortunately, we have neither.Expectation maximization is a general technique for learning in such chicken-and-eggsituations (Dempster, Laird, and Rubin 1977; Boyles 1983; Wu 1983).
The basic idea is tomake a guess at the alignments, and then use this guess to estimate the parameters forthe relevant distributions.
We can use these re-estimated distributions to make a betterguess at the alignments, and then use these (ideally better) alignments to re-estimatethe parameters.Figure 4Schematic drawing of the semi-HMM (with some transition probabilities) for the document a b.514Daume?
and Marcu Alignments for Automatic Document SummarizationFormally, the EM family of algorithms tightly bound the log of an expectation ofa function by the expectation of the log of that function, through the use of Jensen?sinequality (Jensen 1906).
The tightness of the bound means that when we attempt toestimate the model parameters, we may do so over expected alignments, rather than thetrue (but unknown) alignments.
EM gives formal guarantees of convergence, but is onlyguaranteed to find local maxima.3.5 Model InferenceAll the inference techniques utilized in this paper are standard applications of semi-Markov model techniques.
The relevant equations are summarized in Figure 5 anddescribed here.
In all these equations, the variables t and t?
range over phrases in thesummary (specifically, the phrase st:t?
), and the variables i and j range over phrases in thedocument.
The interested reader is directed to Ostendorf, Digalakis, and Kimball (1996)for more details on the generic form of these models and their inference techniques.Unfortunately, the number of possible alignments for a given ?document,summary?
pair is exponential in the length of the summary.
This would make a na?
?veimplementation of the computation of p(s | d) intractable without a more clever solu-tion.
Instead, we are able to employ a variant of the forward algorithm to compute theseprobabilities recursively.
The basic idea is to compute the probability of generating aprefix of the summary and ending up at a particular position in the document (this isknown as the forward probability).
Since our independence assumptions tell us thatit does not matter how we got to this position, we can use this forward probabilityto compute the probability of taking one more step in the summary.
At the end, thedesired probability p(s | d) is simply the forward probability of reaching the end ofthe summary and document simultaneously.
The forward probabilities are calculatedin the ?
table in Figure 5.
This equation essentially says that the probability of emittingthe first t ?
1 words of the summary and ending at position j in the document canbe computed by summing over our previous position (t?)
and previous state (i) andmultiplying the probability of getting there (?i(t?
+ 1)) with the probability of movingfrom there to the current position.Figure 5Summary of inference equations for a semi-Markov model.515Computational Linguistics Volume 31, Number 4The second standard inference problem is the calculation of the best alignment: theViterbi alignment.
This alignment can be computed in exactly the same fashion as theforward algorithm, with two small changes.
First, the forward probabilities implicitlyinclude a sum over all previous states, whereas the Viterbi probabilities replace thiswith a max operator.
Second, in order to recover the actual Viterbi alignment, we keeptrack of which previous state this max operator chose.
This is computed by filling outthe ?
table from Figure 5.
This is almost identical to the computation of the forwardprobabilities, except that instead of summing over all possible t?
and i, we take themaximum over those variables.The final inference problem is parameter re-estimation.
In the case of standardHMMs, this is known as the Baum-Welch, Baum-Eagon or Forward-Backward algo-rithm (Baum and Petrie 1966; Baum and Eagon 1967).
By introducing backward proba-bilities analogous to the forward probabilities, we can compute alignment probabilitiesof suffixes of the summary.
The backward table is the ?
table in Figure 5, which isanalogous to the ?
table, except that the computation proceeds from the end to the start.By combining the forward and backward probabilities, we can compute the ex-pected number of times a particular alignment was made (the E-step in the EM frame-work).
Based on these expectations, we can simply sum and normalize to get newparameters (the M-step).
The expected transitions are computed according to the ?
table,which makes use of the forward and backward probabilities.
Finally, the re-estimatedjump probabilities are given by a?
and the re-estimated rewrite probabilities are given byb?, which are essentially relative frequencies of the fractional counts given by the ?s.The computational complexity for the Viterbi algorithm and for the parameter re-estimation is O(N2T2), where N is the length of the summary and T is the number ofstates (in our case, T is roughly the length of the document times the maximum phraselength allowed).
However, we will typically bound the maximum length of a phrase;we are unlikely to otherwise encounter enough training data to get reasonable estimatesof emission probabilities.
If we enforce a maximum observation sequence length of l,then this drops to O(N2Tl).
Moreover, if the transition network is sparse, as it is inour case, and the maximum out-degree of any node is b, then the complexity drops toO (NTbl).4.
Model ParameterizationBeyond the conditional independence assumptions made by the semi-HMM, there arenearly no additional constraints that are imposed on the parameterization (in termsof the jump and rewrite distributions) of the model.
There is one additional technicalrequirement involving parameter re-estimation, which essentially says that the expec-tations calculated during the forward-backward algorithm must be sufficient statisticsfor the parameters of the jump and rewrite models.
This constraint simply requires thatwhatever information we need to re-estimate their parameters is available to us fromthe forward-backward algorithm.4.1 Parameterizing the Jump ModelRecall that the responsibility of the jump model is to compute probabilities of the formjump(j?
| j), where j?
is a new position and j is an old position.
We have explored severalpossible parameterizations of the jump table.
The first simply computes a table oflikely jump distances (i.e., jump forward 1, jump backward 3, etc.).
The second models516Daume?
and Marcu Alignments for Automatic Document SummarizationTable 2Jump probability decomposition; the source state is either the designated start state, thedesignated end state, a document phrase position spanning from i to i?
(denoted ri,i? )
or a nullstate corresponding to position i (denoted r?,i).source target probability?start?
ri,i?
jumprel(i)ri,i?
rj,j?
jumprel( j ?
i?)ri,j?
?end?
jumprel(m + 1 ?
i?)?start?
r?,i jumprel(?
)jumprel(i)r?,i rj,j?
jumprel( j ?
i)r?,i r?,j jumprel(?
)jumprel( j ?
i)r?,i ?end?
jumprel(m + 1 ?
i)ri,i?
r?,j jumprel(?
)jumprel( j ?
i?
)this distribution as a Gaussian (though, based on Figure 2 this is perhaps not the bestmodel).
Both of these models have been explored in the machine translation commu-nity.
Our third parameterization employs a novel syntax-aware jump model that at-tempts to take advantage of local syntactic information in computing jumps.4.1.1 The Relative Jump Model.
In the relative jump model, we keep a table of countsfor each possible jump distance, and compute jump(j?
| j) = jumprel(j?
?
j).
Each possi-ble jump type and its associated probability is shown in Table 2.
By these calculations,regardless of document phrase lengths, transitioning forward between two consecutivesegments will result in jumprel(1).
When transitioning from the start state p to stateri,i?
, the value we use is a jump length of i.
Thus, if we begin at the first word in thedocument, we incur a transition probability of j1.
There are no transitions into p. Weadditionally remember a specific transition jumprel(?)
for the probability of transition-ing to a null state.
It is straightforward to estimate these parameters based on theestimations from the forward-backward algorithm.
In particular, jumprel(i) is simplythe relative frequency of length i jumps, and jumprel(?)
is simply the count of jumps thatend in a null state to the total number of jumps.
The null state remembers the positionwe ended in before we jumped there, and so to jump out of a null state, we make a jumpbased on this previous position.64.1.2 Gaussian Jump Model.
The Gaussian jump model attempts to alleviate the spar-sity of data problem in the relative jump model by assuming a parametric form to thejumps.
In particular, we assume there is a mean jump length ?
and a jump variance ?2,and then the probability of a jump of length i is given by:i ?
Nor(?,?2) ?
exp[1?2(i ?
?
)2](1)Some care must be taken in employing this model, since the normal distributionis defined over a continuous space.
Thus, when we discretize the calculation, the nor-malizing constant changes slightly from that of a continuous normal distribution.
In6 In order for the null state to remember where we were, we actually introduce one null state for eachdocument position, and require that from a document phrase di:j, we can only jump to null state ?j.517Computational Linguistics Volume 31, Number 4practice, we normalize by summing over a sufficiently large range of possible is.
The pa-rameters ?
and ?2 are estimated by computing the mean jump length in the expectationsand its empirical variance.
We model null states identically to the relative jump model.4.1.3 Syntax-Aware Jump Model.
Both of the previously described jump models areextremely na?
?ve in that they look only at the distance jumped and completely ignorewhat is being jumped over.
In the syntax-aware jump model, we wish to enable themodel to take advantage of syntactic knowledge in a very weak fashion.
This is quitedifferent from the various approaches to incorporating syntactic knowledge into ma-chine translation systems, wherein strong assumptions about the possible syntacticoperations are made (Yamada and Knight 2001; Eisner 2003; Gildea 2003).To motivate this model, consider the first document sentence shown with its syn-tactic parse tree in Figure 6.
Though it is not always the case, forward jumps of distancemore than one are often indicative of skipped words.
From the standpoint of the rela-tive jump models, jumping over the four words tripled it ?s sales and jumping over thefour words of Apple Macintosh systems are exactly the same.7 However, intuitively, wewould be much more willing to jump over the latter than the former.
The latter phraseis a full syntactic constituent, while the first phrase is just a collection of nearby words.Furthermore, the latter phrase is a prepositional phrase (and prepositional phrasesmight be more likely dropped than other phrases), while the former phrase includesa verb, a pronoun, a possessive marker, and a plain noun.To formally capture this notion, we parameterize the syntax-aware jump modelaccording to the types of phrases being jumped over.
That is, to jump over tripled it ?ssales would have probability jumpsyn(VBD PRP POS NNS) while to jump over of AppleMacintosh systems would have probability jumpsyn(PP).
In order to compute the prob-abilities for jumps over many components, we factorize so that the first probabil-ity becomes jumpsyn(VBD)jumpsyn(PRP)jumpsyn(POS)jumpsyn(NNS).
This factorizationexplicitly encodes our preference for jumping over single units rather than severalsyntactically unrelated units.In order to work with this model, we must first parse the document side of thecorpus; we used Charniak?s parser (Charniak 1997).
Given the document parse trees,the re-estimation of the components of this probability distribution is done by sim-ply counting what sorts of phrases are being jumped over.
Again, we keep a singleparameter jumpsyn(?)
for jumping to null states.
To handle backward jumps, we simplyconsider a duplication of the tag set, where jumpsyn(NP-f) denotes a forward jump overan NP, and jumpsyn(NP-b) denotes a backward jump over an NP.84.2 Parameterizing the Rewrite ModelAs observed from the human-aligned summaries, a good rewrite model should be ableto account for alignments between identical word and phrases, between words thatare identical up to stem, and between different words.
Intuition (as well as further7 As can be seen from this example, we have preprocessed the data to split off possessive terms, such as themapping from its to it ?s.8 In general, there are many ways to get from one position to another.
For instance, to get from systems toJanuary, we could either jump forward over an RB and a JJ, or we could jump forward over an ADVP andbackward over an NN.
In our version, we restrict all jumps to the same direction, and take the shortestjump sequence, in terms of number of nodes jumped over.518Daume?
and Marcu Alignments for Automatic Document SummarizationFigure 6The syntactic tree for an example document sentence.investigations of the data) also suggest that synonymy is an important factor to take intoconsideration in a successful rewrite model.
We account for each of these four factors infour separate components of the model and then take a linear interpolation of them toproduce the final probability:rewrite(s | d) = ?idrewriteid(s | d) + ?stemrewritestem(s | d) (2)+ ?wnrewritewn(s | d) + ?rwrewriterw(s | d) (3)where the ?s are constrained to sum to unity.
The four rewrite distributions used are: idis a word identity model, which favors alignment of identical words; stem is a modeldesigned to capture the notion that matches at the stem level are often sufficient foralignment (i.e., walk and walked are likely to be aligned); wn is a rewrite model basedon similarity according to WordNet; and wr is the basic rewrite model, similar to atranslation table in machine translation.
These four models are described in detail inthis section, followed by a description of how to compute their ?s during EM.4.2.1 Word Identity Rewrite Model.
The form of the word identity rewrite model is:rewriteid(s | d) = ?s=d.
That is, the probability is 1 exactly when s and d are identical,and 0 when they differ.
This model has no parameters.4.2.2 Stem Identity Rewrite Model.
The form of the stem identity rewrite model isvery similar to that of the word identity model:rewritestem(s | d) = 1Zd?|s|=|d||s|?i=1?stem(si )=stem(di ) (4)That is, the probability of a phrase s given d is uniform over all phrases s?
thatmatch d up to stem (and are of the same length, i.e., |s?| = |d|), and zero otherwise.
The519Computational Linguistics Volume 31, Number 4normalization constant is computed offline based on a pre-computed vocabulary.
Thismodel also has no parameters.4.2.3 WordNet Rewrite Model.
In order to account for synonymy, we allow documentphrases to be rewritten to semantically ?related?
summary phrases.
To compute thevalue for rewritewn(s | d), we first require that both s and d can be found in WordNet.
Ifeither cannot be found, then the probability is zero.
If they both can be found, thenthe graph distance between their first senses is computed (we traverse the hyper-nymy tree up until they meet).
If the two paths do not meet, then the probability isagain taken to be zero.
We place an exponential model on the hypernym tree-baseddistance:rewritewn(s | d) = 1Zdexp [?
?dist(s, d)] (5)Here, dist is calculated distance, taken to be +?
whenever either of the failureconditions is met.
The single parameter of this model is ?, which is computed accordingto the maximum likelihood criterion from the expectations during training.
The nor-malization constant Zd is calculated by summing over the exponential distribution forall s?
that occur on the summary side of our corpus.4.2.4 Lexical Rewrite Model.
The lexical rewrite model is the ?catch all?
model tohandle the cases not handled by the above models.
It is analogous to a translation-table(t-table) in statistical machine translation (we will continue to use this terminologyfor the remainder of the article), and simply computes a matrix of (fractional) countscorresponding to all possible phrase pairs.
Upon normalization, this matrix gives therewrite distribution.4.2.5 Estimation of the Weight Parameters.
In order to weight the four models, weneed to estimate values for the ?
components.
This computation can be performedinside of the EM iterations by considering for each rewritten pair its expectation ofbelonging to each of the models.
We use these expectations to maximize the likelihoodwith respect to the ?s and then normalize them so they sum to one.4.3 Model PriorsIn the standard HMM case, the learning task is simply one of parameter estimation,wherein the maximum likelihood criterion under which the parameters are typicallytrained performs well.
However, in our model, we are, in a sense, simultaneouslyestimating parameters and selecting a model: The model selection is taking place at thelevel of deciding how to segment the observed summary.
Unfortunately, in such modelselection problems, likelihood increases monotonically with model complexity.
Thus,EM will find for us the most complex model; in our case, this will correspond to amodel in which the entire summary is produced at once, and no generalization will bepossible.This suggests that a criterion other than maximum likelihood (ML) is moreappropriate.
We advocate the maximum a posteriori (MAP) criterion in this case.
While520Daume?
and Marcu Alignments for Automatic Document SummarizationML optimizes the probability of the data given the parameters (the likelihood), MAPoptimizes the product of the probability of the parameters with the likelihood (theunnormalized posterior).
The difficulty in our model that makes ML estimation performpoorly is centered in the lexical rewrite model.
Under ML estimation, we will simplyinsert an entry in the t-table for the entire summary for some uncommon or uniquedocument word and are done.
However, a priori we do not believe that such a parameteris likely.
The question then becomes how to express this in a way that inference remainstractable.From a statistical point of view, the t-table is nothing but a large multinomial model(technically, one multinomial for each possible document phrase).
Under a multinomialdistribution with parameter ?
with J-many components (with all ?j positive and sum-ming to one), the probability of an observation x is given by p (x | ?)
=?Jj=1 ?xjj (here, weconsider x to be a vector of length J in which all components are zero except for one,corresponding to the actual observation).This distribution belongs to the exponential family and therefore has a natural conju-gate distribution.
Informally, two distributions are conjugate if you can multiply themtogether and get the original distribution back.
In the case of the multinomial, the conju-gate distribution is the Dirichlet distribution.
A Dirichlet distribution is parameterizedby a vector ?
of length J with ?j ?
0, but not necessarily summing to one.
The Dirichletdistribution can be used as a prior distribution over multinomial parameters and hasdensity:p (?
| ?)
=?
(?Jj=1 ?j)?Jj=1 ?(?j)?Jj=1?
?j?1j .The fraction before the product is simply a normalization term that ensures that theintegral over all possible ?
integrates to one.The Dirichlet is conjugate to the multinomial because when we compute theposterior of ?
given ?
and x, we arrive back at a Dirichlet distribution: p (?
| x,?)
?p (x | ?)
p (?
| ?)
?
?Jj=1 ?xj+?j?1j .
This distribution has the same density as the originalmodel, but a ?fake count?
of ?j ?
1 has been added to component j.
This means thatif we are able to express our prior beliefs about the multinomial parameters foundin the t-table in the form of a Dirichlet distribution, the computation of the MAPsolution can be performed exactly as described before, but with the appropriate fakecounts added to the observed variables (in our case, the observed variables are thealignments between a document phrase and a summary phrase).
The application ofDirichlet priors to standard HMMs has previously been considered in signal process-ing (Gauvain and Lee 1994).
These fake counts act as a smoothing parameter, sim-ilar to Laplace smoothing (Laplace smoothing is the special case where ?j = 2 forall j).In our case, we believe that singleton rewrites are worth 2 fake counts, that lexicalidentity rewrites are worth 4 fake counts and that stem identity rewrites are worth3 fake counts.
Indeed, since a singleton alignment between identical words satisfiesall of these criteria, it will receive a fake count of 9.
The selection of these counts isintuitive, but clearly arbitrary.
However, this selection was not ?tuned?
to the data toget better performance.
As we will discuss later, inference in this model over the sizesof documents and summaries we consider is quite computationally expensive.
As isappropriate, we specified this prior according to our prior beliefs, and left the rest to theinference mechanism.521Computational Linguistics Volume 31, Number 44.4 Parameter InitializationWe initialize all the parameters uniformly, but in the case of the rewrite parameters,since there is a prior on them, they are effectively initialized to the maximum likelihoodsolution under their prior.5.
Experimental ResultsThe experiments we perform are on the same Ziff-Davis corpus described in the intro-duction.
In order to judge the quality of the alignments produced, we compare themagainst the gold-standard references annotated by the humans.
The standard precisionand recall metrics used in information retrieval are modified slightly to deal with thesure and possible alignments created during the annotation process.
Given the set Sof sure alignments, the set S ?
P of possible alignments, and a set A of hypothesizedalignments, we compute the precision as |A ?
P|/|A| and the recall as |A ?
S|/|S|.One problem with these definitions is that phrase-based models are fond of makingphrases.
That is, when given an abstract containing the man and a document alsocontaining the man, a human will align the to the and man to man.
However, a phrase-based model will almost always prefer to align the entire phrase the man to the man.
Thisis because it results in fewer probabilities being multiplied together.To compensate for this, we define soft precision (SoftP in the tables) by countingalignments where a b is aligned to a b the same as ones in which a is aligned to a and b isaligned to b.
Note, however, that this is not the same as a aligned to a b and b aligned tob.
This latter alignment will, of course, incur a precision error.
The soft precision metricinduces a new, soft F-Score, labeled SoftF.Often, even humans find it difficult to align function words and punctuation.
A listof 58 function words and punctuation marks that appeared in the corpus (henceforthcalled the ignore-list) was assembled.
We computed precision and recall scores both onall words and on all words that do not appear in the ignore-list.5.1 Systems ComparedOverall, we compare various parameter settings of our model against three other sys-tems.
First, we compare against two alignment models developed in the context ofmachine translation.
Second, we compare against the Cut and Paste model developed inthe context of ?summary decomposition?
by Jing (2002).
Each of these systems will bediscussed in more detail shortly.
However, the machine translation alignment modelsassume sentence pairs as input.
Moreover, even though the semi-Markov model is basedon efficient dynamic programming techniques, it is still too inefficient to run on verylong ?document, abstract?
pairs.To alleviate both of these problems, we preprocess our ?document, abstract?
corpusdown to an ?extract, abstract?
corpus, and then subsequently apply our models to thissmaller corpus (see Figure 7).
In our data, doing so does not introduce significantnoise.
To generate the extracts, we paired each abstract sentence with three sentencesfrom the corresponding document, selected using the techniques described by Marcu(1999).
In an informal evaluation, 20 such pairs were randomly extracted and evaluatedby a human.
Each pair was ranked as 0 (document sentences contain little to noneof the information in the abstract sentence), 1 (document sentences contain some ofthe information in the abstract sentence) or 2 (document sentences contain all of the522Daume?
and Marcu Alignments for Automatic Document SummarizationFigure 7Pictorial representation of the conversion of the ?document, abstract?
corpus to an ?extract,abstract?
corpus.information).
Of the 20 random examples, none were labeled as 0; 5 were labeled as 1;and 15 were labeled as 2, giving a mean rating of 1.75.
We refer to the resulting corpusas the ?extract, abstract?
corpus, statistics for which are shown in Table 3.
Finally, for faircomparison, we also run the Cut and Paste model only on the extracts.95.1.1 Machine Translation Models.
We compare against several competing systems,the first of which is based on the original IBM Model 4 for machine translation(Brown et al 1993) and the HMM machine translation alignment model (Vogel, Ney,and Tillmann 1996) as implemented in the GIZA++ package (Och and Ney 2003).We modified the code slightly to allow for longer inputs and higher fertilities, butotherwise made no changes.
In all of these setups, 5 iterations of Model 1 were run,followed by five iterations of the HMM model.
For Model 4, 5 iterations of Model 4were subsequently run.In our model, the distinction between the summary and the document is clear, butwhen using a model from machine translation, it is unclear which of the summaryand the document should be considered the source language and which should beconsidered the target language.
By making the summary the source language, we areeffectively requiring that the fertility of each summary word be very high, or that manywords are null generated (since we must generate all of the document).
By makingthe document the source language, we are forcing the model to make most documentwords have zero fertility.
We have performed experiments in both directions, but thelatter (document as source) performs better in general.In order to seed the machine translation model so that it knows that word identityis a good solution, we appended our corpus with sentence pairs consisting of one sourceword and one target word, which were identical.
This is common practice in the ma-chine translation community when one wishes to cheaply encode knowledge from adictionary into the alignment model.5.1.2 Cut and Paste Model.
We also tested alignments using the Cut and Paste sum-mary decomposition method (Jing 2002), based on a non-trainable HMM.
Briefly,9 Interestingly, the Cut and Paste method actually achieves higher performance scores when run on onlythe extracts rather than the full documents.523Computational Linguistics Volume 31, Number 4Table 3Ziff-Davis extract corpus statistics.Abstracts Extracts DocumentsDocuments 2033 2033Sentences 13k 41k 82kWords 261k 1M 2.5MUnique words 14k 26k 42k29kSentences/Doc 6.28 21.51 40.83Words/Doc 128.52 510.99 1229.71Words/Sent 20.47 23.77 28.36the Cut and Paste HMM searches for long contiguous blocks of words in thedocument and abstract that are identical (up to stem).
The longest such sequencesare aligned.
By fixing a length cutoff of n and ignoring sequences of length lessthan n, one can arbitrarily increase the precision of this method.
We found thatn = 2 yields the best balance between precision and recall (and the highest F-measure).On this task, this model drastically outperforms the machine translation models.5.1.3 The Semi-Markov Model.
While the semi-HMM is based on a dynamic pro-gramming algorithm, the effective search space in this model is enormous, even formoderately sized ?document, abstract?
pairs.
The semi-HMM system was then trainedon this ?extract, abstract?
corpus.
We also restrict the state-space with a beam, sized at50% of the unrestricted state-space.
With this configuration, we run ten iterations of theforward-backward algorithm.
The entire computation time takes approximately 8 dayson a 128-node cluster computer.We compare three settings of the semi-HMM.
The first, semi-HMM-relative, usesthe relative movement jump table; the second, semi-HMM-Gaussian, uses the Gaussianparameterized jump table; the third, semi-HMM-syntax, uses the syntax-based jumpmodel.5.2 Evaluation ResultsThe results, in terms of precision, recall, and F-score, are shown in Table 4.
The first threecolumns are when these three statistics are computed over all words.
The next threecolumns are when these statistics are only computed over words that do not appear inour ignore list of 58 stop words.
Under the methodology for combining the two humanannotations by taking the union, either of the human scores would achieve a precisionand recall of 1.0.
To give a sense of how well humans actually perform on this task, wecompare each human against the other.As we can see from Table 4, none of the machine translation models is well suitedto this task, achieving, at best, an F-score of 0.298.
The flipped models, in which thedocument sentences are the source language and the abstract sentences are the targetlanguage perform significantly better (comparatively).
Since the MT models are notsymmetric, going the bad way requires that many document words have zero fertility,which is difficult for these models to cope with.524Daume?
and Marcu Alignments for Automatic Document SummarizationTable 4Results on the Ziff-Davis corpus.All Words Non-Stop WordsSystem SoftP Recall SoftF SoftP Recall SoftFHuman1 0.727 0.746 0.736 0.751 0.801 0.775Human2 0.680 0.695 0.687 0.730 0.722 0.726HMM (Sum=Src) 0.120 0.260 0.164 0.139 0.282 0.186Model 4 (Sum=Src) 0.117 0.260 0.161 0.135 0.283 0.183HMM (Doc=Src) 0.295 0.250 0.271 0.336 0.267 0.298Model 4 (Doc=Src) 0.280 0.247 0.262 0.327 0.268 0.295Cut and Paste 0.349 0.379 0.363 0.431 0.385 0.407semi-HMM-relative 0.456 0.686 0.548 0.512 0.706 0.593semi-HMM-Gaussian 0.328 0.573 0.417 0.401 0.588 0.477semi-HMM-syntax 0.504 0.701 0.586 0.522 0.712 0.606The Cut and Paste method performs significantly better, which is to be expected,since it is designed specifically for summarization.
As one would expect, this methodachieves higher precision than recall, though not by very much.
The fact that the Cutand Paste model performs so well, compared to the MT models, which are able to learnnon-identity correspondences, suggests that any successful model should be able totake advantage of both, as ours does.Our methods significantly outperform both the IBM models and the Cut and Pastemethod, achieving a precision of 0.522 and a recall of 0.712, yielding an overall F-score of0.606 when stop words are not considered.
This is still below the human-against-humanF-score of 0.775 (especially considering that the true human-against-human scores are1.0), but significantly better than any of the other models.Among the three settings of our jump table, the syntax-based model performsbest, followed by the relative jump model, with the Gaussian model coming in worst(though still better than any other approach).
Inspecting Figure 2, the fact that theGaussian model does not perform well is not surprising; the data shown there is verynon-Gaussian.
A double-exponential model might be a better fit, but it is unlikely thatsuch a model will outperform the syntax based model, so we did not perform thisexperiment.5.3 Error AnalysisThe first mistake frequently made by our model is to not align summary words to null.In effect, this means that our model of null-generated summary words is lacking.
Anexample of this error is shown in Example 1 in Figure 8.
In this example, the modelhas erroneously aligned from DOS in the abstract to from DOS in the document (theerror is shown in bold).
This alignment is wrong because the context of from DOS in thedocument is completely different from the context it appears in the summary.
However,the identity rewrite model has overwhelmed the locality model and forced this incor-rect alignment.
To measure the frequency of such errors, we have post-processed oursystem?s alignments so that whenever a human alignment contains a null-generatedsummary word, our model also predicts that this word is null-generated.
Doing so willnot change our system?s recall, but it can improve the precision.
Indeed, in the caseof the relative jump model, the precision jumps from 0.456 to 0.523 (F-score increases525Computational Linguistics Volume 31, Number 4Figure 8Erroneous alignments are in bold.
(Top) Example of an error made by our model (from fileZF207-585-936).
From DOS should be null generated, but the model has erroneously alignedit to an identical phrase that appeared 11 sentences earlier in the document.
(Bottom) Error(from ZF207-772-628); The DMP 300 should be aligned to the printer but is instead aligned toa far-away occurrence of The DMP 300.from 0.548 to 0.594) in the case of all words and from 0.512 to 0.559 (F-score increasesfrom 0.593 to 0.624).
This corresponds to a relative improvement of roughly 8% F-score.Increases in score for the syntax-based model are roughly the same.The second mistake our model frequently makes is to trust the identity rewritemodel too strongly.
This problem has to do either with synonyms that do not appearfrequently enough for the system to learn reliable rewrite probabilities, or with corefer-ence issues, in which the system chooses to align, for instance, Microsoft to Microsoft,rather than Microsoft to the company, as might be correct in context.
As suggestedby this example, this problem is typically manifested in the context of coreferentialnoun phrases.
It is difficult to perform a similar analysis of this problem as for theaforementioned problem (to achieve an upper bound on performance), but we canprovide some evidence.
As mentioned before, in the human alignments, roughly 51% ofall aligned phrases are lexically identical.
In the alignments produced by our model (onthe same documents), this number is 69%.
In the case of stem identity, the hand-aligneddata suggests that stem identity should hold in 67% of the cases; in our alignments,this number was 81%.
An example of this sort of error is shown in Example 2 inFigure 8.
Here, the model has aligned The DMP 300 in the abstract to The DMP 300 inthe document, while it should have been aligned to the printer due to locality constraints(note that the model also misses the (produces ?
producing) alignment, likely as a side-effect of it making the error depicted in bold).In Table 5, we have shown examples of common errors made by our system (thesewere randomly selected from a much longer list of errors).
These examples are shownout of their contexts, but in most cases, the error is clear even so.
In the first column, weshow the summary phrase in question.
In the second column, we show the documentphrase to which it should be aligned, and in the third column, we show the documentphrase that our model aligned it to (or null).
In the right column, we classify the model?salignment as incorrect or partially correct.The errors shown in Table 5 show several weaknesses of the model.
For instance,in the first example, it aligns to port with to port, which seems correct without con-text, but the chosen occurrence of to port in the document is in the discussion ofa completely different porting process than that referred to in the summary (and is526Daume?
and Marcu Alignments for Automatic Document SummarizationTable 5Ten example phrase alignments from the hand-annotated corpus; the last column indicateswhether the semi-HMM correctly aligned this phrase.Summary Phrase True Phrase Aligned Phrase Classto port can port to port incorrectOS - 2 the OS / 2 OS / 2 partialwill use will be using will using partialword processing programs word processors word processing incorrectconsists of also includes null of partialwill test will also have to test will test partialthe potential buyer many users the buyers incorrectThe new software Crosstalk for Windows new software incorrectare generally powered by run on null incorrectOracle Corp. the software publisher Oracle Corp. incorrectseveral sentences away).
The seventh and tenth examples (The new software and OracleCorp., respectively) show instances of the coreference error that occurs commonly.6.
Conclusion and DiscussionCurrently, summarization systems are limited to either using hand-annotated data orusing weak alignment models at the granularity of sentences, which serve as suitabletraining data only for sentence extraction systems.
To train more advanced extractionsystems, such as those used in document compression models or in next-generationabstraction models, we need to better understand the lexical correspondences betweendocuments and their human written abstracts.
Our work is motivated by the desire toleverage the vast number of ?document, abstract?
pairs that are freely available on theInternet and in other collections and to create word- and phrase-aligned ?document,abstract?
corpora automatically.This article presents a statistical model for learning such alignments in a com-pletely unsupervised manner.
The model is based on an extension of a hidden Markovmodel, in which multiple emissions are made in the course of one transition.
We havedescribed efficient algorithms in this framework, all based on dynamic programming.Using this framework, we have experimented with complex models of movement andlexical correspondences.
Unlike the approaches used in machine translation, where onlyvery simple models are used, we have shown how to efficiently and effectively leveragesuch disparate knowledge sources and WordNet, syntax trees, and identity models.We have empirically demonstrated that our model is able to learn the complex struc-ture of ?document, abstract?
pairs.
Our system outperforms competing approaches, in-cluding the standard machine translation alignment models (Brown et al 1993; Vogel,Ney, and Tillmann 1996) and the state-of-the-art Cut and Paste summary alignmenttechnique (Jing 2002).We have analyzed two sources of error in our model, including issues of null-generated summary words and lexical identity.
Within the model itself, we have alreadysuggested two major sources of error in our alignment procedure.
Clearly more workneeds to be done to fix these problems.
One approach that we believe will be particu-larly fruitful would be to add a fifth model to the linearly interpolated rewrite modelbased on lists of synonyms automatically extracted from large corpora.
Additionally,527Computational Linguistics Volume 31, Number 4investigating the possibility of including some sort of weak coreference knowledge intothe model might serve to help with the second class of errors made by the model.One obvious aspect of our method that may reduce its general usefulness is thecomputation time.
In fact, we found that despite the efficient dynamic programmingalgorithms available for this model, the state space and output alphabet are simplyso large and complex that we were forced to first map documents down to extractsbefore we could process them (and even so, computation took roughly 1,000 processorhours).
Though we have not pursued it in this work, we do believe that there isroom for improvement computationally, as well.
One obvious first approach wouldbe to run a simpler model for the first iteration (for example, Model 1 from machinetranslation (Brown et al 1993), which tends to be very recall oriented) and use this to seesubsequent iterations of the more complex model.
By doing so, one could recreate theextracts at each iteration using the previous iteration?s parameters to make better andshorter extracts.
Similarly, one might only allow summary words to align to words foundin their corresponding extract sentences, which would serve to significantly speed uptraining and, combined with the parameterized extracts, might not hurt performance.A final option, but one that we do not advocate, would be to give up on phrases andtrain the model in a word-to-word fashion.
This could be coupled with heuristic phrasalcreation as is done in machine translation (Och and Ney 2000), but by doing this, onecompletely loses the probabilistic interpretation that makes this model so pleasing.Aside from computational considerations, the most obvious future effort along thelines of this model is to incorporate it into a full document summarization system.
Sincethis can be done in many ways, including training extraction systems, compressionsystems, headline generation systems, and even extraction systems, we left this tofuture work so that we could focus specifically on the alignment task in this article.Nevertheless, the true usefulness of this model will be borne out by its application totrue summarization tasks.AcknowledgmentsWe wish to thank David Blei for helpfultheoretical discussions related to this projectand Franz Josef Och for sharing his technicalexpertise on issues that made thecomputations discussed in this paperpossible.
We sincerely thank the anonymousreviewers of an original conference versionof this article as well reviewers of this longerversion, all of whom gave very usefulsuggestions.
Some of the computationsdescribed in this work were made possibleby the High Performance Computing Centerat the University of Southern California.This work was partially supported byDARPA-ITO grant N66001-00-1-9814, NSFgrant IIS-0097846, NSF grant IIS-0326276,and a USC Dean Fellowship to HalDaume?
III.ReferencesAydin, Zafer, Yucel Altunbasak, and MarkBorodovsky.
2004.
Protein secondarystructure prediction with semi-MarkovHMMs.
In Proceedings of the IEEEInternational Conference on Acoustics, Speechand Signal Processing (ICASSP), May 17?21.Banko, Michele, Vibhu Mittal, and MichaelWitbrock.
2000.
Headline generation basedon statistical translation.
In Proceedingsof the Conference of the Association forComputational Linguistics (ACL),pages 318?325, Hong Kong, October 1?8.Barzilay, Regina and Noemie Elhadad.
2003.Sentence alignment for monolingualcomparable corpora.
In Proceedings ofthe Conference on Empirical Methods inNatural Language Processing (EMNLP),pages 25?32.Baum, Leonard E. and J. E. Eagon.
1967.
Aninequality with applications to statisticalestimation for probabilistic functions ofMarkov processes and to a model ofecology.
Bulletins of the AmericanMathematical Society, 73:360?363.Baum, Leonard E. and Ted Petrie.
1966.Statistical inference for probabilisticfunctions of finite state Markov chains.Annals of Mathematical Statistics,37:1554?1563.528Daume?
and Marcu Alignments for Automatic Document SummarizationBerger, Adam and Vibhu Mittal.
2000.Query-relevant summarization usingFAQs.
In Proceedings of the Conference of theAssociation for Computational Linguistics(ACL), pages 294?301, Hong Kong,October 1?8.Boyles, Russell A.
1983.
On the convergenceof the EM algorithm.
Journal of the RoyalStatistical Society, B(44):47?50.Brown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, and Robert L.Mercer.
1993.
The mathematics ofstatistical machine translation: Parameterestimation.
Computational Linguistics,19(2):263?311.Carletta, Jean.
1995.
Assessing agreementon classification tasks: The kappastatistic.
Computational Linguistics, 22(2):249?254.Carroll, John, Guido Minnen, YvonneCanning, Siobhan Devlin, and John Tait.1998.
Practical simplification of Englishnewspaper text to assist aphasic readers.In Proceedings of the AAAI-98 Workshop onIntegrating Artificial Intelligence andAssistive Technology.Chandrasekar, Raman, Christy Doran, andSrinivas Bangalore.
1996.
Motivations andmethods for text simplification.
InProceedings of the International Conference onComputational Linguistics (COLING),pages 1041?1044, Copenhagen,Denmark.Charniak, Eugene.
1997.
Statistical parsingwith a context-free grammar and wordstatistics.
In Proceedings of the FourteenthNational Conference on Artificial Intelligence(AAAI?97), pages 598?603, Providence, RI,July 27?31.Daume?
III, Hal and Daniel Marcu.
2002.
Anoisy-channel model for documentcompression.
In Proceedings of theConference of the Association forComputational Linguistics (ACL),pages 449?456.Daume?
III, Hal and Daniel Marcu.
2004.
Atree-position kernel for documentcompression.
In Proceedings of the FourthDocument Understanding Conference (DUC2004), Boston, MA, May 6?7.Dempster, Arthur P., Nan M. Laird, andDonald B. Rubin.
1977.
Maximumlikelihood from incomplete data via theEM algorithm.
Journal of the Royal StatisticalSociety, B39(1):1?37.Edmundson, H. P. 1969.
New methods inautomatic abstracting.
Journal of theAssociation for Computing Machinery,16(2):264?285.
Reprinted in Advances inAutomatic Text Summarization, I. Mani andM.
T. Maybury, (eds.
).Eisner, Jason.
2003.
Learning non-isomorphictree mappings for machine translation.
InProceedings of the Conference of theAssociation for Computational Linguistics(ACL), Sapporo, Japan.Ferguson, Jack D. 1980.
Variable durationmodels for speech.
In Proceedingsof the Symposium on the Application ofHidden Markov Models to Textand Speech, pages 143?179, October,Princeton, NJ.Gales, Mark J. F. and Steve J.
Young.
1993.The theory of segmental hidden Markovmodels.
Technical report, CambridgeUniversity Engineering Department.Gauvain, Jean-Luc and Chin-Hui Lee.
1994.Maximum a-posteriori estimation formultivariate Gaussian mixtureobservations of Markov chains.
IEEETransactions Speech and Audio Processing,2:291?298.Ge, Xianping and Padhraic Smyth.
2000.Segmental semi-Markov models forchange-point detection with applicationsto semiconductor manufacturing.Technical report, University of Californiaat Irvine, March.Gildea, Daniel.
2003.
Loosely tree-basedalignment for machine translation.
InProceedings of the Conference of theAssociation for Computational Linguistics(ACL), pages 80?87, Sapporo, Japan.Grefenstette, Gregory.
1998.
Producingintelligent telegraphic text reduction toprovide an audio scanning service forthe blind.
In Working Notes of the AAAISpring Symposium on Intelligent TextSummarization, pages 111?118, StanfordUniversity, Stanford, CA.Jensen, J. L. W. V. 1906.
Sur les fonctionsconvexes et les ine?galite?s entre les valeursmoyennes.
Acta Mathematica, 30:175?193.Jing, Hongyan.
2000.
Sentence reduction forautomatic text summarization.
InProceedings of the Conference of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL),pages 310?315, Seattle, WA.Jing, Hongyan.
2002.
Using hidden Markovmodeling to decompose human-writtensummaries.
Computational Linguistics,28(4):527?544.Knight, Kevin and Daniel Marcu.
2002.Summarization beyond sentenceextraction: A probabilistic approach tosentence compression.
Artificial Intelligence,139(1):91?107.529Computational Linguistics Volume 31, Number 4Kupiec, Julian, Jan O. Pedersen, and FrancineChen.
1995.
A trainable documentsummarizer.
In Proceedings of the AnnualACM Conference on Research andDevelopment in Information Retrieval,pages 68?73.Luhn, H. P. 1956.
The automatic creation ofliterature abstracts.
In I. Mani and M.Maybury, editors, Advances in AutomaticText Summarization.
MIT Press, Cambridge,MA, pages 58?63.Mahesh, Kavi.
1997.
Hypertext summaryextraction for fast document browsing.
InProceedings of the AAAI Spring Symposiumon Natural Language Processing for the WorldWide Web, pages 95?103, Stanford, CA.Mani, Inderjeet.
2001.
AutomaticSummarization, volume 3 of NaturalLanguage Processing.
John Benjamins,Amsterdam/Philadelphia.Mani, Inderjeet and Mark Maybury, editors.1999.
Advances in Automatic TextSummarization.
MIT Press, Cambridge,MA.Marcu, Daniel.
1999.
The automaticconstruction of large-scale corpora forsummarization research.
In Proceedings ofthe 22nd Conference on Research andDevelopment in Information Retrieval(SIGIR?99), pages 137?144, Berkeley, CA,August 15?19.Marcu, Daniel.
2000.
The Theory and Practiceof Discourse Parsing and Summarization.MIT Press, Cambridge, MA.Mitchell, Carl D., Leah H. Jamieson, andMary P. Harper.
1995.
On the complexity ofexplicit duration HMMs.
IEEE Transactionson Speech and Audio Processing, 3(3).Och, Franz Josef and Hermann Ney.
2000.Improved statistical alignment models.In Proceedings of the Conference of theAssociation for Computational Linguistics(ACL), pages 440?447, October,Hong Kong, China.Och, Franz Josef and Hermann Ney.
2003.
Asystematic comparison of variousstatistical alignment models.
ComputationalLinguistics, 29(1):19?51.Ostendorf, Mari, Vassilis Digalakis, andOwen Kimball.
1996.
From HMMs tosegment models: A unified view ofstochastic modeling for speechrecognition.
IEEE Transactions on Speechand Audio Processing, 4(5):360?378,September.Quirk, Chris, Chris Brockett, and WilliamDolan.
2004.
Monolingual machinetranslation for paraphrase generation.
InProceedings of the Conference on EmpiricalMethods in Natural Language Processing(EMNLP), pages 142?149, Barcelona, Spain.Schwartz, Richard, David Zajic, and BonnieDorr.
2002.
Automatic headline generationfor newspaper stories.
In Proceedings of theDocument Understanding Conference (DUC),pages 78?85, Philadelphia.Smyth, Padhraic, David Heckerman, andMichael I. Jordan.
1997.
Probabilisticindependence networks for hiddenMarkov probability models.
NeuralComputation, 9(2):227?269.Teufel, Simone and Mark Moens.
1997.Sentence extraction as a classification task.In In ACL/EACL-97 Workshop on Intelligentand Scalable Text Summarization,pages 58?65.Vogel, Stephan, Hermann Ney, andChristoph Tillmann.
1996.
HMM-basedword alignment in statistical translation.In Proceedings of the International Conferenceon Computational Linguistics (COLING),pages 836?841.Wu, Jeff C. F. 1983.
On the convergenceproperties of the EM algorithm.
The Annalsof Statistics, 11:95?103.Yamada, Kenji and Kevin Knight.
2001.
Asyntax-based statistical translation model.In Proceedings of the Conference of theAssociation for Computational Linguistics(ACL), pages 523?530.Zajic, David, Bonnie Dorr, and RichardSchwartz.
2004.
BBN/UMD at DUC-2004:Topiary.
In Proceedings of the FourthDocument Understanding Conference (DUC2004), pages 112?119, Boston, MA,May 6?7.530
