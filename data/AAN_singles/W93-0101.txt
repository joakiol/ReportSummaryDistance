Word Sense Disambiguation by Human Subjects:Computat ional  and Psycholinguistic ApplicationsThomas E. AhlswedeComputer Science Dept.Central Michigan UniversityMr.
Pleasant, MI 48859ahlswede @cps?01.
cps.
cmich, eduDavid LorandEarlham CollegeRichmond, IN 47374davel@yang, earlham, bitnetAbstractAlthough automated word sense disambiguation has become a popular activity withincomputational lexicology, evaluation of the accuracy of disambiguation systems isstill mostly limited to manual checking by the developer.
This paper describes ourwork in collecting data on the disambiguation behavior of human subjects, withthe intention of providing (I) a norm against which dictionary-based systems (andperhaps others) can be evaluated, and (2) a source of psycholinguistic informationabout previously unobserved aspects of human disambiguation, for the use of bothpsycholinguists and computational researchers.
We also describe two of our mostimportant tools: a questionnaire of ambiguous test words in various contexts, anda hypertext user interface for efficient and powerful collection of data from humansubjects.1 The need for a metric of disambiguationResearch in automatic lexical disambiguation has been going on for decades, and in recentyears experimental disambiguation systems have proliferated.
The problem of determiningthe accuracy of these systems has been little recognized: the usual check for correctnessis a comparison of the test results against he experimenter's own judgment.
Even lessconsidered has been the question of what constitutes correctness in disambiguation, be-yond the intuitive recognition that some disambiguations are better ("correct") and othersworse ("incorrect").A common approach to disambiguation is to select among the homographs and sensesprovided by a machine-readable dictionary (e.g.
Lesk \[1986\], Byrd \[1989\], Krovetz \[1989\],Slator \[1989\], Guthrie et al \[1990\], Ide and Veronis \[1990\], and Veronis and Ide \[1990\].Dictionaries deal with the ambiguity of words by providing multiple definitions for suffi-ciently ambiguous words.
These multiple definitions may be homographs (distinct wordsof unrelated meaning, whose written forms coincide) or senses (related but nonidenticalmeanings of a single word).The inadequacy of a finite, discrete set of sense definitions to resolve all ambiguitieshas been pointed out by Boguraev and Pustejovsky \[1990\], Kilgarriff \[1991\], and Ahlswede\[forthcoming\].
For the practical task of disambiguation i  natural anguage processing,however, the dictionary is a valuable and convenient source of sense distinctions; in ourview, the best single source.2 Evaluations of Human and Automatic  Disambigua-t ionMany previous tudies of human disambiguation have been from a psycholinguistic pointof view.
Simpson and Burgess \[1988\], surveying some of these studies, identify three basicmodels of ambiguity processing: (1) restriction by context, (2) ordered access, and (3)multiple access.
Prather and Swinney \[1988\] consider whether the lexical component ofhuman language processing is modular, i.e., acts independently of other components, orwhether it interacts with other components.Computationally oriented evaluations of human disambiguation began as incidentaladjuncts to computational projects.
Amsler and White \[1979\], with the help of assistants,manually (i.e., by human judgment) disambiguated the nouns and verbs used in defini-tions in the Merriam-Webster Pocket Dictionary.
In an informal study, they found thattheir disambiguators' self-consistency on repeat performance was high (84%) but theirconsistency with respect o each other was lower.The need for some means of evaluating automatic disambiguation methods, more rig-orous than the experimenter's personal judgment, has become more obvious with therecent growing interest in the topic.
Gale, Church and Yarowsky \[1992\], for instance,have followed the approach of estimating upper and lower bounds on the performance ofa system.3 Prel iminary experimentsThe project described in this paper began when one of us (Ahlswede) wrote disambiguationprograms based on those of Lesk \[1986\] and Ide and Veronis \[1990\] for application indictionary and corpus research.
Lesk claimed 50-70% accuracy on short samples of literaryand journalistic input.
Ide and Veronis claimed a 90% accuracy rate for their program,although they explained that they had tested it against strongly distinct definitions -mainly homographs rather than senses.After running the programs on test data containing ambiguities at both homographand sense level, and evaluating the results, Ahlswede doubted whether, given this subtlermix of ambiguities, even a single human judge would achieve 90% consistency on successiveevaluations of the same output; moreover, the consistency among multiple judges mightwell be much lower.
Ahlswede recruited seven colleagues and friends to evaluate the testdata, then compared their disambiguations of the test data against each other.
The levelof agreement averaged only 66% among the various human informants, ranging from 31%to 88% between pairs of informants \[Ahlswede, forthcoming\].This figure was based on a simple pairwise comparison strategy.
The informants ratedeach sense definition of a test word with a "1" indicating that it correctly representedthe meaning of the word as used in the test text; "-1" if the definition did not correctlyrepresent the meaning; and "0" if for any reason the informant could not decide one wayor the other.Pairs of informants were then compared by matching their ratings of the sense defini-tions of each word.
The pair were considered to agree on a test word if at least one sensereceived a "1" from both informants and if no sense receiving a "1" from either informantwas given a "-1" by the other.This scoring method had the advantage of simplicity, but it did not reflect the agree-ment implicit in the rejection as well as the selection of senses by both informants.
But therelative weight of common rejections and common selections among the senses of a giventest word depends on the total number of senses, which varies widely.
No discrete-valuedscoring mechanism seems able to solve this problem.A pairwise scoring procedure that gives much more plausible results is the coefficientof correlation, applied to the parallel evaluations by the informants being compared.
Itclearly distinguishes the relatively high agreement expected from human subjects from therelatively low agreement predicted for primitive automatic disambiguation systems, andfrom the more or less random behavior of a control series of random "disambiguations.
"Table 1.
Pairwise correlations of performance of human, machine and control disambigua-tions of test textshl h2 h3 h4 h5 h6 h6a h7 ml mla m2 alh2 .737h3 .463  .497h4  .743  .669  .428h5  .719 .679 .477h6 .793 .747 .531h6a .831 .797 .528h7 .524 .523 .455.643.674 .817.755 .825 .926.524 .606 .565 ?
543ml .196  .136  .080  .186  .204  .180 .163 .148mla .281 .220 .154 .235 .274 .274 .252 .240 .723m2 .097 .083  .016  .033  .100  .104  .085  .033 .027  .060a l  .013  .014  - .061  .011 .014  .016  .016 .011 .011 .010  .012rand  .017 - .008  .047 .010 - .008  .005 - .000  .008 - .000  .014  .024  - .016Notes:1. hl through h7 are human informants; h6 took the test twice.. ml  and mla  are implementations of Lesk's algorithm.
In mla,  the test texts werepreviously disambiguated for part of speech; senses of inappropriate parts of speechwere assumed incorrect, and left out of the test data.3.
m2 is a spreading activation algorithm related to the Ide-Veronis algorithm.4.
al is a control in which all senses of all test words received a "1".
In our first scoringstrategy, al achieved absurdly high scores.5.
rand is a control created by randomly scrambling the sequence of answers in one ofthe human samples.These results suggested that a very high accuracy rate is not so much unrealisticas meaningless: which of the human informants hould the computer agree with, if thehumans cannot agree among themselves?For this reason, the informal experiment has led to the development of a larger andmore formal test of human disambiguation performance.
The main areas of innovation are(1) a much more systematically designed questionnaire, to be administered to hundreds ofsubjects rather than only seven, and (2) a user interface to facilitate both the completionof the questionnaire by this large number of human subjects, and our analysis of theirperformance.
The biggest advantage of a computerized interface is that we can studythe timing of subjects' responses: valuable information that could not be recorded in theoriginal written test.Combined with the user interface, the questionnaire is adapted for administrationto human informants, but it can be adapted with little effort for use with dictionary-based disambiguation programs, as was done with its written (but also machine-readable)predecessor.4 Design of the QuestionnaireThe prototype version of the present questionnaire was a printed list of 100 test texts,each with an ambiguous word highlighted and a list of definitions following.
Subjectstypically took the test home, and reported needing anything from half an hour to severaldays to complete the questionnaire.The test was difficult to complete for several reasons.
The test texts were themselvesdictionary definitions, chosen at random from the machine-readable version of the CollinsEnglish Dictionary (CED).
(This was because the project grew out of an effort specificallyto disambiguate definition texts in the CED.)
Many of the words being defined by thetest texts were highly obscure, e.g.paduasoy  n. a rich strong silk fabric used for hangings, vestments, etc.OrI nd ia  paper  n. another name (not in technical usage) for bible paper\[Ahlswede and Lorand, 1993\]Disambiguation was done (as it still is in the present questionnaire) by choosing one ormore from a set of dictionary definitions of the highlighted word.
This was hard work, andvolunteers were hard to find.
Therefore, though the present version of the questionnaireavoids "hard" words except where these are explicitly being studied, it is still tough enoughthat we pay our subjects a small honorarium.Like its prototype, the present questionnaire consists of 100 test texts, each with anambiguous test word or short phrase (e.g., ring up, go over).
The number 100 was chosen,based on our experience with the prototype, as a compromise between a smaller test,easier on the subject but less informative, and a larger test which might be prohibitivelydifficult or time-consuming to take.5 The Test TextsSource.
The test texts have been selected in part to represent a wide variety of writtenEnglish, while using a minimum of different sources in order to facilitate comparisonwithin each category as well as between categories.
The distribution was:4?
24 General nonfiction (house and garden management tips, extracted from theVADisc corpus \[Fox, 1988\])?
24 Fiction (selections from short stories by Mark Twain)?
24 Journalism (The Wall Street Journal (WSJ), extracted from the ACL-DCI Corpus\[Liberman, 1991\];.?
20 Definitions from the CED \[Collins, 1974\] (selected from definitions used in theprototype questionnaire)?
8 special texts (constructed to test specific interesting ambiguities)One of the original criteria for both test words and test texts, neutrality betweenBritish and American usage \[Ahlswede, 1992\], was found virtually impossible to maintain.The CED is British, and many if not most of its multi-sense ntries include definitionsof idiomatic British usages.
To leave these out would be to risk distorting the results asa metric for a disambiguation program that used the CED as a whole without excludingthose particular definitions.
The other categories are American, and in the interest ofconsistency, American idioms were freely permitted as well.Several other criteria for selecting test texts were retained and followed:1.
Difficulty of resolution.
This can only be estimated subjectively until the question-naire results are in, except for the twenty dictionary definitions, where a rough measure ofdifficulty of resolution is provided by the "coefficient of certainty" \[Ahlswede, forthcom-ing\].A second measure, the "coefficient of dissent", specifically measures disagreement asopposed to uncertainty.
The high negative correlation between coefficient of certainty andcoefficient of dissent (-0.942) indicated that, in practice, there was little difference betweenwidespread uncertainty and widespread isagreement.Partly because of the apparent lack of importance in this distinction, and partly forthe convenience ofautomating the questionnaire, the "0" option in the prototype has beeneliminated.
The subject is forced to decide "yes" or "no" to each sense.Size of context.
The test texts are complete sentences, or (in the case of CED) completedefinitions.
In some cases phrases have been deleted with ellipsis, where the full textseemed unmanageably long and the deleted phrase irrelevant to the disambiguation f thetest word.
The net sentence length ranges from 5 to 28 with a median of 14.
Results sofar indicate, as did Lesk's observations, that sentence length does not significantly affectperformance.Global context was early recognized as a potential problem: human disambiguationdecisions are made not only on the basis of the immediate sentence-level context, butalso on an awareness of the domain: for instance, the word capital is likely (though notcertain) to mean one thing in the Wall Street Journal and another thing in a politicaleditorial about the federal government.Since the test texts are short and have no global context whatever, we compensateby adding a small parenthetical note at the end of each text, identifying it as "WSJ","Tips", "CED", "Twain" or "special".
The meaning of these short tags is explained tothe subject, and though not the same as actual global context, they provide explicitly theinformation the reader normally deduces during reading.6 The  Test WordsAn factor which is probably important, but impossible to measure, is the familiarity ofa test word.
Two contrasting intuitions about familiarity are (1) an unfamiliar wordshould be harder to disambiguate because its senses are less well known to the informant;but (2) a familiar word should be harder because it is likely to have more senses andhomographs.
Since familiarity is not only completely subjective, but also varies widelyfrom one individual to another, we turn to a much more measurable criterion:Frequency.
An unanswered question is whether it is more appropriate to measure wordfrequency based on the specialized corpora from which the texts are extracted, or basedon a single average word frequency list.
The texts taken from the CED, the Wall StreetJournal, and the "Tips", having been extracted from multi-million-word corpora, can bemeasured separately.
Unfortunately, we have no online corpus of Mark Twain's works,and the "special" texts are, by definition, not from any corpus at all.Part of speech.
Studies of disambiguation have focused almost exclusively on nouns,verbs and adjectives, and hardly at all on "function words" such as prepositions, con-junctions, and those adverbs not derived from adjectives.
(An exception is Brugman andLakoff \[1988\], who study the word over.)
We are interested in in both kinds of words.Therefore the test words include 28 nouns, 22 verbs, 19 adjectives, 16 adverbs (none in-ly), and 15 assorted prepositions, conjunctions and pronouns.Given the combination of a British dictionary with such ultra-American sources asMark Twain, we were unable to guarantee variety neutrality in our test words as in ourtest texts.
An alternative, however, was to include among the "special" texts two withstrong variety bias: I took the tube to the repair shop, ambiguous in British but not inAmerican, and It was a long and unpleasant fall,, ambiguous in American hut not (or lessso) in British.
These were added in the hope that native or learned speakers of BritishEnglish would handle them differently than speakers of American English.7 The  User  Inter faceAn important feature of the questionnaire is its user interface.
This was developed by oneof us (Lorand) in Macintosh HyperCard.The interface consists of four principal modules ("stacks" in hypertext erminology):(1) a top-level stack that drives the interface as a whole; (2) a "demographics" tack thatmanages a menu of demographic and identifying information that the subject fills out; (3)the "import questionnaire" stack, which allows the questionnaire to exist independentlyof the interface as an editable text file, and to be reinserted into the interface as desired,e.g., after changes have been made; (4) the questionnaire itself, translated automaticallyinto MetaTalk, the MetaCard programming language.8 The  demograph ics  stackThe menu of the demographics stack first solicits non-identifying portions of the subject'sSocial Security number and birthday, which are hashed to form a unique, confidential IDfor that subject.
The menu then solicits potentially relevant demographic nformation:age, gender, native/non-native speaker of English, number of years speaking English ifnon-native, and highest educational degree.
This last is an extremely rough measureof literacy, but no better one is available, and the preliminary experiment showed thatdoctoral-level subjects agreed more closely with each other than the non-doctoral subjectsdid either with the doctorates or with each other \[Ahlswede, forthcoming\].The ID and the demographic information are written to a text file in numerically codedform.
The subject may then begin the questionnaire or cancel.9 The questionnaire stackThe questionnaire is implemented as a series of windows, one for each test text and itsassociated efinitions.
The test text is displayed at the top of the window, with the testword in boldface.
Below is a subwindow containing the definitions.
The subject clicks ona definition to identify it as a good disambiguation; the typeface of the selected efinitionchanges to boldface.
Clicking on a selected definition will de-select it and its typefacewill change back to regular.
Any number of definitions may be selected.
If, as sometimeshappens, there are too many definitions to fit within the subwindow, it can be scrolledup and down to give access to all the definitions.
Arrow buttons at the bottom right andbottom left enable the subject to go ahead to the next text or back to the previous one.Every action by the subject is logged, as is its time, in the log file.
Thus when thesubject is done, we have a complete record of his or her actions, of the time at which eachaction took place, and thus of the interval between each pair of actions.10 The SubjectsSo far, most of the subjects recruited have been students, with some faculty and staff.We are presently recruiting off campus.
Probably thanks to the honorarium, responsehas been enthusiastic: well over the 100 subjects we considered necessary for an adequatesample.
Because we are still occupied with data collection, intensive analysis of the datahas not begun yet.11 ConclusionsAs we administer the questionnaire, we are developing approaches to the analysis of theresulting data.
When we have acquired a large enough collection of performances, we willbegin formal analysis.Our first concern in this effort has been to develop a useful corpus or set of "norms"of human disambiguation behavior, against which automatic disambiguation systems, atleast those based on machine-readable dictionaries, can be compared.
We also believe,however, that our results will be interesting to psycholinguists studying human disam-biguation: since our approach as been different from previous psycholinguistic experi-ments, we expect hat considerable new knowledge will emerge from the data we are nowgathering.12 ReferencesAhlswede, Thomas E. and David Lorand, 1993.
The Ambiguity Questionnaire: A Studyof Lexical Disambiguation by Human Informants.
Proc.
of the Fifth Annual MidwestArtificial Intelligence and Cognitive Science Society Conference, Chesterton, Indiana, pp.21-25.Ahlswede, Thomas E., 1992.
Issues in the Design of Test Data for Lexical Disambiguationby Humans and Machines.
/f2Proc.
of the Fourth Annual Midwest Artificial Intelligenceand Cognitive Science Society Conference, Starved Rock, Illinois, pp.
112-116.Ahlswede, Thomas E., forthcoming.
An Experiment in Human vs. Machine Disambigua-tion of Word Senses.
(Submitted).Ahlswede, Thomas E. and Martha Evens, 1988.
Generating a Relational Lexicon from aMachine-Readable Dictionary.
International Journal of Lexicography, vol.
1, no.
3, pp.214-237.Boguraev, Branimir, and James Pustejovsky, 1990.
Lexical Ambiguity and the Role ofKnowledge Representation in Lexicon Design.
Proc.
of COLING-90, Helsinki, vol.
2, pp.36-41.Brugman, Claudia, and George Lakoff, 1989.
Cognitive Topology and Lexical Networks.In Small et al, eds., pp.
477-508.Byrd, Roy, 1989.
Discovering Relationships among Word Senses.
In Dictionaries in theElectronic Age: Proc.
of the Fifth Annual Conference of the UW Centre for the NewOED, Waterloo, Ontario, pp.
67-80.Collins English Dictionary, 1974.
Collins, Birmingham.Fox, Edward, 1988.
Virginia Disc One.
Virginia Polytechnic Institute, Blacksburg, Vir-ginia.Gale, William, Kenneth Church and David Yarowsky, 1992.
Estimating Upper and LowerBounds on the Performance ofWord Sense Disambiguation Programs.
Proc.
of the 30thAnnual Meeting of the ACL, Newark, Delaware.Guthrie, Louise, Brian M. Slator, Yorick Wilks, and Rebecca Bruce, 1990.
Is there contentin empty heads?
Proc.
of COLING-90, Helsinki, vol.
3, pp.
138-143.Hirst, Graeme, 1987.
Semantic Interpretation and the Resolution of Ambiguity.
Cam-bridge University Press, Cambridge.8Ide, Nancy M. and Jean Veronis, 1990.
Mapping Dictionaries: A Spreading ActivationApproach.
Proc.
of the 6th Annual Conference of the UW Centre for New OED, Waterloo,Ontario, pp.
52-64.Krovetz, Robert, 1989.
Lexical Acquisition and Information Retrieval.
Proc.
of the FirstInternational Lexical Acquisition Workshop, IJCAI, Detroit, 1989.Lesk, Michael, 1986.
Automatic Sense Disambiguation Using Machine Readable Dictio-naries: How to Tell a Pine Cone from an Ice Cream Cone.
Proc.
of SIGDOC, Toronto,pp.
1-9.Liberman, Mark, 1991.
ACL/DCI Corpus, University of Pennsylvania.Prather, P. A., and David A. Swinney, 1988.
Lexical Processing and Ambiguity Resolu-tion: An Autonomous Process in an Interactive Box.
In Small et al, eds., pp.
289-310.Pustejovsky, James, 1989.
Current Issues in Computational Lexical Semantics.
Proc.
ofthe Fourth Conference of the European Chapter of the ACL,, Manchester, pp.
xvii-xxv.Simpson, Greg B. and Curt Burgess, 1988.
Implications of Lexical Ambiguity Resolutionfor Word Recognition and Comprehension.
In Small et al, eds., pp.
271-288.Slator, Brian M., 1989.
Using Context for Sense Preference.
Proc.
of the First Interna-tional Lexical Acquisition Workshop, IJCAI, Detroit, 1989.Small, Steven L., Garrison W. Cottrell, and Michael K. Tanenhaus, eds., 1988.
LexicalAmbiguity Resolution.
Morgan Kaufman, San Mateo, California.Veronis, Jean, and Nancy M. Ide, 1990.
Word Sense Disambiguation with Very LargeNeural Networks Extracted from Machine Readable Dictionaries.
Proc.
of COLING-90,Helsinki, vol.
2, pp.
398-394.
