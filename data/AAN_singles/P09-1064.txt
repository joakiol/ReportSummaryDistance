Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 567?575,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPFast Consensus Decoding over Translation ForestsJohn DeNero David Chiang and Kevin KnightComputer Science Division Information Sciences InstituteUniversity of California, Berkeley University of Southern Californiadenero@cs.berkeley.edu {chiang, knight}@isi.eduAbstractThe minimum Bayes risk (MBR) decoding ob-jective improves BLEU scores for machine trans-lation output relative to the standard Viterbi ob-jective of maximizing model score.
However,MBR targeting BLEU is prohibitively slow to op-timize over k-best lists for large k. In this pa-per, we introduce and analyze an alternative toMBR that is equally effective at improving per-formance, yet is asymptotically faster ?
running80 times faster than MBR in experiments with1000-best lists.
Furthermore, our fast decodingprocedure can select output sentences based ondistributions over entire forests of translations, inaddition to k-best lists.
We evaluate our proce-dure on translation forests from two large-scale,state-of-the-art hierarchical machine translationsystems.
Our forest-based decoding objectiveconsistently outperforms k-best list MBR, givingimprovements of up to 1.0 BLEU.1 IntroductionIn statistical machine translation, output transla-tions are evaluated by their similarity to humanreference translations, where similarity is most of-ten measured by BLEU (Papineni et al, 2002).A decoding objective specifies how to derive finaltranslations from a system?s underlying statisticalmodel.
The Bayes optimal decoding objective isto minimize risk based on the similarity measureused for evaluation.
The corresponding minimumBayes risk (MBR) procedure maximizes the ex-pected similarity score of a system?s translationsrelative to the model?s distribution over possibletranslations (Kumar and Byrne, 2004).
Unfortu-nately, with a non-linear similarity measure likeBLEU, we must resort to approximating the ex-pected loss using a k-best list, which accounts foronly a tiny fraction of a model?s full posterior dis-tribution.
In this paper, we introduce a variantof the MBR decoding procedure that applies effi-ciently to translation forests.
Instead of maximiz-ing expected similarity, we express similarity interms of features of sentences, and choose transla-tions that are similar to expected feature values.Our exposition begins with algorithms over k-best lists.
A na?
?ve algorithm for finding MBRtranslations computes the similarity between everypair of k sentences, entailing O(k2) comparisons.We show that if the similarity measure is linear infeatures of a sentence, then computing expectedsimilarity for all k sentences requires only k sim-ilarity evaluations.
Specific instances of this gen-eral algorithm have recently been proposed for twolinear similarity measures (Tromble et al, 2008;Zhang and Gildea, 2008).However, the sentence similarity measures wewant to optimize in MT are not linear functions,and so this fast algorithm for MBR does not ap-ply.
For this reason, we propose a new objectivethat retains the benefits of MBR, but can be op-timized efficiently, even for non-linear similaritymeasures.
In experiments using BLEU over 1000-best lists, we found that our objective providedbenefits very similar to MBR, only much faster.This same decoding objective can also be com-puted efficiently from forest-based expectations.Translation forests compactly encode distributionsover much larger sets of derivations and arise nat-urally in chart-based decoding for a wide varietyof hierarchical translation systems (Chiang, 2007;Galley et al, 2006; Mi et al, 2008; Venugopalet al, 2007).
The resulting forest-based decodingprocedure compares favorably in both complexityand performance to the recently proposed lattice-based MBR (Tromble et al, 2008).The contributions of this paper include a linear-time algorithm for MBR using linear similarities,a linear-time alternative to MBR using non-linearsimilarity measures, and a forest-based extensionto this procedure for similarities based on n-gramcounts.
In experiments, we show that our fast pro-cedure is on average 80 times faster than MBRusing 1000-best lists.
We also show that usingforests outperforms using k-best lists consistentlyacross language pairs.
Finally, in the first pub-lished multi-system experiments on consensus de-567coding for translation, we demonstrate that bene-fits can differ substantially across systems.
In all,we show improvements of up to 1.0 BLEU fromconsensus approaches for state-of-the-art large-scale hierarchical translation systems.2 Consensus Decoding AlgorithmsLet e be a candidate translation for a sentence f ,where e may stand for a sentence or its derivationas appropriate.
Modern statistical machine trans-lation systems take as input some f and score eachderivation e according to a linear model of fea-tures:?i ?i ?
?i(f, e).
The standard Viterbi decod-ing objective is to find e?
= arg maxe ?
?
?
(f, e).For MBR decoding, we instead leverage a sim-ilarity measure S(e; e?)
to choose a translation us-ing the model?s probability distribution P(e|f),which has support over a set of possible transla-tions E. The Viterbi derivation e?
is the mode ofthis distribution.
MBR is meant to choose a trans-lation that will be similar, on expectation, to anypossible reference translation.
To this end, MBRchooses e?
that maximizes expected similarity tothe sentences in E under P(e|f):1e?
= arg maxe EP(e?|f)[S(e; e?
)]= arg maxe?e?
?EP(e?|f) ?
S(e; e?
)MBR can also be interpreted as a consensus de-coding procedure: it chooses a translation similarto other high-posterior translations.
Minimizingrisk has been shown to improve performance forMT (Kumar and Byrne, 2004), as well as otherlanguage processing tasks (Goodman, 1996; Goeland Byrne, 2000; Kumar and Byrne, 2002; Titovand Henderson, 2006; Smith and Smith, 2007).The distribution P(e|f) can be induced from atranslation system?s features and weights by expo-nentiating with base b to form a log-linear model:P (e|f) =b???(f,e)?e?
?E b???(f,e?
)We follow Ehling et al (2007) in choosing b usinga held-out tuning set.
For algorithms in this sec-tion, we assume that E is a k-best list and b hasbeen chosen already, so P(e|f) is fully specified.1Typically, MBR is defined as arg mine?EE[L(e; e?)]
forsome loss function L, for example 1 ?
BLEU(e; e?).
Thesedefinitions are equivalent.2.1 Minimum Bayes Risk over Sentence PairsGiven any similarity measure S and a k-bestlist E, the minimum Bayes risk translation canbe found by computing the similarity between allpairs of sentences in E, as in Algorithm 1.Algorithm 1 MBR over Sentence Pairs1: A?
?
?2: for e ?
E do3: Ae ?
04: for e?
?
E do5: Ae ?
Ae + P (e?|f) ?
S(e; e?
)6: if Ae > A then A, e??
Ae, e7: return e?We can sometimes exit the inner for loop early,whenever Ae can never become larger than A(Ehling et al, 2007).
Even with this shortcut, therunning time of Algorithm 1 is O(k2 ?
n), wheren is the maximum sentence length, assuming thatS(e; e?)
can be computed in O(n) time.2.2 Minimum Bayes Risk over FeaturesWe now consider the case when S(e; e?)
is a lin-ear function of sentence features.
Let S(e; e?)
bea function of the form?j ?j(e) ?
?j(e?
), where?j(e?)
are real-valued features of e?, and ?j(e) aresentence-specific weights on those features.
Then,the MBR objective can be re-written asarg maxe?E EP(e?|f)[S(e; e?
)]= arg maxe?e?
?EP (e?|f) ?
?j?j(e) ?
?j(e?
)= arg maxe?j?j(e)[?e?
?EP (e?|f) ?
?j(e?
)]= arg maxe?j?j(e) ?
EP(e?|f)[?j(e?)].
(1)Equation 1 implies that we can find MBR trans-lations by first computing all feature expectations,then applying S only once for each e. Algorithm 2proceduralizes this idea: lines 1-4 compute featureexpectations, and lines 5-11 find the translationwith highest S relative to those expectations.
Thetime complexity is O(k ?n), assuming the numberof non-zero features ?(e?)
and weights ?
(e) growlinearly in sentence length n and all features andweights can be computed in constant time.568Algorithm 2 MBR over Features1: ???
[0 for j ?
J ]2: for e?
?
E do3: for j ?
J such that ?j(e?)
6= 0 do4: ?
?j ?
?
?j + P (e?|f) ?
?j(e?
)5: A?
?
?6: for e ?
E do7: Ae ?
08: for j ?
J such that ?j(e) 6= 0 do9: Ae ?
Ae + ?j(e) ?
?
?j10: if Ae > A then A, e??
Ae, e11: return e?An example of a linear similarity measure isbag-of-words precision, which can be written as:U(e; e?)
=?t?T1?
(e, t)|e|?
?
(e?, t)where T1 is the set of unigrams in the language,and ?
(e, t) is an indicator function that equals 1if t appears in e and 0 otherwise.
Figure 1 com-pares Algorithms 1 and 2 using U(e; e?).
Otherlinear functions have been explored for MBR, in-cluding Taylor approximations to the logarithm ofBLEU (Tromble et al, 2008) and counts of match-ing constituents (Zhang and Gildea, 2008), whichare discussed further in Section 3.3.2.3 Fast Consensus Decoding usingNon-Linear Similarity MeasuresMost similarity measures of interest for machinetranslation are not linear, and so Algorithm 2 doesnot apply.
Computing MBR even with simplenon-linear measures such as BLEU, NIST or bag-of-words F1 seems to require O(k2) computationtime.
However, these measures are all functionsof features of e?.
That is, they can be expressed asS(e;?(e?))
for a feature mapping ?
: E ?
Rn.For example, we can express BLEU(e; e?)
=exp"?1 ?|e?||e|?
?+144Xn=1lnPt?Tnmin(c(e, t), c(e?, t))Pt?Tnc(e, t)#In this expression, BLEU(e; e?)
references e?
onlyvia its n-gram count features c(e?, t).22The length penalty?1 ?
|e?||e|?
?is also a function of n-gram counts: |e?| =Pt?T1c(e?, t).
The negative part oper-ator (?)?
is equivalent to min(?, 0).Choose a distribution P over a set of translations EMBR over Sentence PairsCompute pairwise similarityCompute expectationsMax expected similarity Max feature similarity3/3 1/4 2/51/3 4/4 0/52/3 0/4 5/5MBR over FeaturesE [?
(efficient)] = 0.6E [?
(forest)] = 0.7E [?
(decoding)] = 0.7E [?
(for)] = 0.3E [?
(rusty)] = 0.3E [?
(coating)] = 0.3E [?
(a)] = 0.4E [?
(fish)] = 0.4E [?
(ain?t)] = 0.4c1 c2 c3r1r2r312323I ... telescopeYo vi al hombre con el telescopioI ... saw the ... man with ... telescopethe ... telescope0.4?saw the?
?man with?0.6?saw the?1.0?man with?E [r(man with)] = 0.4 + 0.6 ?
1.050.050.250.450.650.8511,660 513,245 514,830Total model score for 1000 translationsCorpusBLEU022.545.067.590.0Hiero SBMT70.284.656.661.451.150.5Viterbi n-gram precisionForest n-gram precision at Viterbi recallForest n-gram precision for Er(t) ?
1Forest samples (b?2)Forest samples (b?5)Viterbi translationsU(e2; e1) =|efficient||efficient for rusty coating|EU(e1; e?)
= 0.3(1+ 13)+0.4?23= 0.667EU(e2; e?)
= 0.375EU(e3; e?)
= 0.520U(e1;E?)
= 0.6+0.7+0.73= 0.667U(e2;E?)
= 0.375U(e3;E?)
= 0.520P (e1|f) = 0.3 ; e1 = efficient forest decodingP (e2|f) = 0.3 ; e2 = efficient for rusty coatingP (e3|f) = 0.4 ; e3 = A fish ain?t forest decodingFigure 1: For the linear similarity measure U(e; e?
), whichcomputes unigram precision, the MBR translation can befound by iterating either over s ntence pairs (Algorithm 1) orover features (Algorithm 2).
These two algorithms take thesame input (step 1), but diverge in their consensus computa-tions (steps 2 & 3).
However, they produce identical resultsfor U and any other linear similarity measure.Following the structure of Equation 1, we canchoose a translation e based on the feature expec-tations of e?.
In particular, we can choosee?
= arg maxe?ES(e;EP(e?|f)[?(e?)]).
(2)This objective differs from MBR, but has a simi-lar consensus-building structure.
We have simplymoved the expectation inside the similarity func-tion, just as we did in Equation 1.
This new ob-jective can be optimized by Algorithm 3, a pro-cedure that runs in O(k ?
n) time if the count ofnon-zero features in e?
and the computation timeof S(e;?(e?))
are both linear in sentence length n.This fast consensus decoding procedure sharesthe same structure as linear MBR: first we com-pute feature expectations, then we choose the sen-tence that is most similar to those expectations.
Infact, Algorithm 2 is a special case of Algorithm 3.Lines 7-9 of the former and line 7 of the latter areequivalent for linear S(e; e?).
Thus, for any linearsimilarity measure, Algorithm 3 is an algorithmfor minimum Bayes risk decoding.569Algorithm 3 Fast Consensus Decoding1: ???
[0 for j ?
J ]2: for e?
?
E do3: for j ?
J such that ?j(e?)
6= 0 do4: ?
?j ?
?
?j + P (e?|f) ?
?j(e?
)5: A?
?
?6: for e ?
E do7: Ae ?
S(e; ??
)8: if Ae > A then A, e??
Ae, e9: return e?As described, Algorithm 3 can use any sim-ilarity measure that is defined in terms of real-valued features of e?.
There are some nuancesof this procedure, however.
First, the preciseform of S(e;?(e?))
will affect the output, butS(e;E[?(e?)])
is often an input point for which asentence similarity measure S was not originallydefined.
For example, our definition of BLEUabove will have integer valued ?(e?)
for any realsentence e?, butE[?(e?
)]will not be integer valued.As a result, we are extending the domain of BLEUbeyond its original intent.
One could imagine dif-ferent feature-based expressions that also produceBLEU scores for real sentences, but produce dif-ferent values for fractional features.
Some caremust be taken to define S(e;?(e?))
to extend nat-urally from integer-valued to real-valued features.Second, while any similarity measure can inprinciple be expressed as S(e;?(e?))
for a suffi-ciently rich feature space, fast consensus decodingwill not apply effectively to all functions.
For in-stance, we cannot naturally use functions that in-clude alignments or matchings between e and e?,such as METEOR (Agarwal and Lavie, 2007) andTER (Snover et al, 2006).
Though these functionscan in principle be expressed in terms of featuresof e?
(for instance with indicator features for wholesentences), fast consensus decoding will only beeffective if different sentences share many fea-tures, so that the feature expectations effectivelycapture trends in the underlying distribution.3 Computing Feature ExpectationsWe now turn our focus to efficiently comput-ing feature expectations, in service of our fastconsensus decoding procedure.
Computing fea-ture expectations from k-best lists is trivial, butk-best lists capture very little of the underlyingmodel?s posterior distribution.
In place of k-bestChoose a distribution P over a set of translations EMBR over Sentence PairsCompute pairwise similarityCompute expectationsMax expected similarity Max feature similarity3/3 1/4 2/51/3 4/4 0/52/3 0/4 5/5MBR over FeaturesE [?
(efficient)] = 0.6E [?
(forest)] = 0.7E [?
(decoding)] = 0.7E [?
(for)] = 0.3E [?
(rusty)] = 0.3E [?
(coating)] = 0.3E [?
(a)] = 0.4E [?
(fish)] = 0.4E [?
(ain?t)] = 0.4c1 c2 c3r1r2r31232350.050.250.450.650.8511,660 513,245 514,830Total model score for 1000 translationsCorpusBLEU020406080Hiero SBMT56.661.451.150.5N-grams from baseline translationN-grams with high expected countForest samples (b?2)Forest samples (b?5)Viterbi translationsU(e2; e1) =|efficient||efficient for rusty coating|EU(e1; e?)
= 0.3(1+ 13)+0.4?23= 0.667EU(e2; e?)
= 0.375EU(e3; e?)
= 0.520U(e1;E?)
= 0.6+0.7+0.73= 0.667U(e2;E?)
= 0.375U(e3;E?)
= 0.520P (e1|f) = 0.3 ; e1 = efficient forest decodingP (e2|f) = 0.3 ; e2 = efficient for rusty coatingP (e3|f) = 0.4 ; e3 = A fish ain?t forest decodingI ... telescopeYo vi al hombre con el telescopioI ... saw the ... man with ... telescopethe ... telescope0.4?saw the?
?man with?0.6?saw the?1.0?man with?E [c(e, ?man with?)]
=?hP (h|f) ?
c(h, ?man with?
)= 0.4 ?
1 + (0.6 ?
1.0) ?
1Figure 2: This translation forest for a Spanish sentence en-codes two English parse trees.
Hyper-edges (boxes) are an-notated with normalized transition probabilities, as well asthe bigrams produced by each rule application.
The expectedcount of the bigram ?man with?
is the sum of posterior prob-abilities of the two hyper-edges that produce it.
In this exam-ple, we normalized inside scores at all nodes to 1 for clarity.lists, compact encodings of translation distribu-tions have proven effective for MBR (Zhang andGildea, 2008; Tromble et al, 2008).
In this sec-tion, we consider BLEU in particular, for whichthe relevant features ?
(e) are n-gram counts up tolength n = 4.
We show how to compute expec-tations of these counts efficiently from translationforests.3.1 Translation ForestsTranslation forests compactly encode an exponen-tial number of output translations for an inputsentence, along with their model scores.
Forestsarise naturally in chart-based decoding proceduresfor many hierarchical translation systems (Chiang,2007).
Exploiting forests has proven a fruitful av-enue of research in both parsing (Huang, 2008)and machine translation (Mi et al, 2008).Formally, translation forests are weightedacyclic hyper-graphs.
The nodes are states in thedecoding process that include the span (i, j) of thesentence to be translated, the grammar symbol sover that span, and the left and right context wordsof the translation relevant for computing n-gramlanguage model scores.3 Each hyper-edge h rep-resents the application of a synchronous rule r thatcombines nodes corresponding to non-terminals in3Decoder states can include additional information aswell, such as local configurations for dependency languagemodel scoring.570r into a node spanning the union of the child spansand perhaps some additional portion of the inputsentence covered directly by r?s lexical items.
Theweight of h is the incremental score contributedto all translations containing the rule application,including translation model features on r and lan-guage model features that depend on both r andthe English contexts of the child nodes.
Figure 2depicts a forest.Each n-gram that appears in a translation e is as-sociated with some h in its derivation: the h corre-sponding to the rule that produces the n-gram.
Un-igrams are produced by lexical rules, while higher-order n-grams can be produced either directly bylexical rules, or by combining constituents.
Then-gram language model score of e similarly de-composes over the h in e that produce n-grams.3.2 Computing Expected N-Gram CountsWe can compute expected n-gram counts effi-ciently from a translation forest by appealing tothe linearity of expectations.
Let ?
(e) be a vectorof n-gram counts for a sentence e. Then, ?
(e) isthe sum of hyper-edge-specific n-gram count vec-tors ?
(h) for all h in e. Therefore, E[?
(e)] =?h?e E[?
(h)].To compute n-gram expectations for a hyper-edge, we first compute the posterior probability ofeach h, conditioned on the input sentence f :P(h|f) =(?e:h?eb???(f,e))(?eb???
(f,e))?1,where e iterates over translations in the forest.
Wecompute the numerator using the inside-outside al-gorithm, while the denominator is the inside scoreof the root node.
Note that many possible deriva-tions of f are pruned from the forest during decod-ing, and so this posterior is approximate.The expected n-gram count vector for a hyper-edge is E[?
(h)] = P(h|f) ?
?(h).
Hence, aftercomputing P (h|f) for every h, we need only sumP(h|f) ?
?
(h) for all h to compute E[?(e)].
Thisentire procedure is a linear-time computation inthe number of hyper-edges in the forest.To complete forest-based fast consensus de-coding, we then extract a k-best list of uniquetranslations from the forest (Huang et al, 2006)and continue Algorithm 3 from line 5, whichchooses the e?
from the k-best list that maximizesBLEU(e;E[?(e?
)]).3.3 Comparison to Related WorkZhang and Gildea (2008) embed a consensus de-coding procedure into a larger multi-pass decodingframework.
They focus on inversion transductiongrammars, but their ideas apply to richer models aswell.
They propose an MBR decoding objectiveof maximizing the expected number of matchingconstituent counts relative to the model?s distri-bution.
The corresponding constituent-matchingsimilarity measure can be expressed as a linearfunction of features of e?, which are indicators ofconstituents.
Expectations of constituent indicatorfeatures are the same as posterior constituent prob-abilities, which can be computed from a transla-tion forest using the inside-outside algorithm.
Thisforest-based MBR approach improved translationoutput relative to Viterbi translations.Tromble et al (2008) describe a similar ap-proach using MBR with a linear similarity mea-sure.
They derive a first-order Taylor approxima-tion to the logarithm of a slightly modified defini-tion of corpus BLEU4, which is linear in n-gramindicator features ?
(e?, t) of e?.
These features areweighted by n-gram counts c(e, t) and constants?
that are estimated from held-out data.
The lin-ear similarity measure takes the following form,where Tn is the set of n-grams:G(e; e?)
= ?0|e|+4?n=1?t?Tn?t ?
c(e, t) ?
?
(e?, t).Using G, Tromble et al (2008) extend MBR toword lattices, which improves performance overk-best list MBR.Our approach differs from Tromble et al (2008)primarily in that we propose decoding with an al-ternative to MBR using BLEU, while they proposedecoding with MBR using a linear alternative toBLEU.
The specifics of our approaches also differin important ways.First, word lattices are a subclass of forests thathave only one source node for each edge (i.e., agraph, rather than a hyper-graph).
While forestsare more general, the techniques for computingposterior edge probabilities in lattices and forestsare similar.
One practical difference is that theforests needed for fast consensus decoding are4The log-BLEU function must be modified slightly toyield a linear Taylor approximation: Tromble et al (2008)replace the clipped n-gram count with the product of an n-gram count and an n-gram indicator function.571generated already by the decoder of a syntactictranslation system.Second, rather than use BLEU as a sentence-level similarity measure directly, Tromble et al(2008) approximate corpus BLEU with G above.The parameters ?
of the approximation must be es-timated on a held-out data set, while our approachrequires no such estimation step.Third, our approach is also simpler computa-tionally.
The features required to compute G areindicators ?
(e?, t); the features relevant to us arecounts c(e?, t).
Tromble et al (2008) compute ex-pected feature values by intersecting the transla-tion lattice with a lattices for each n-gram t. Bycontrast, expectations of c(e?, t) can all be com-puted with a single pass over the forest.
This con-trast implies a complexity difference.
LetH be thenumber of hyper-edges in the forest or lattice, andT the number of n-grams that can potentially ap-pear in a translation.
Computing indicator expec-tations seems to require O(H ?
T ) time because ofautomata intersections.
Computing count expec-tations requires O(H) time, because only a con-stant number of n-grams can be produced by eachhyper-edge.Our approaches also differ in the space of trans-lations from which e?
is chosen.
A linear similar-ity measure like G allows for efficient search overthe lattice or forest, whereas fast consensus decod-ing restricts this search to a k-best list.
However,Tromble et al (2008) showed that most of the im-provement from lattice-based consensus decodingcomes from lattice-based expectations, not search:searching over lattices instead of k-best lists didnot change results for two language pairs, and im-proved a third language pair by 0.3 BLEU.
Thus,we do not consider our use of k-best lists to be asubstantial liability of our approach.Fast consensus decoding is also similar in char-acter to the concurrently developed variational de-coding approach of Li et al (2009).
Using BLEU,both approaches choose outputs that match ex-pected n-gram counts from forests, though differin the details.
It is possible to define a similar-ity measure under which the two approaches areequivalent.55For example, decoding under a variational approxima-tion to the model?s posterior that decomposes over bigramprobabilities is equivalent to fast consensus decoding withthe similarity measure B(e; e?)
=Qt?T2hc(e?,t)c(e?,h(t))ic(e,t),where h(t) is the unigram prefix of bigram t.4 Experimental ResultsWe evaluate these consensus decoding techniqueson two different full-scale state-of-the-art hierar-chical machine translation systems.
Both systemswere trained for 2008 GALE evaluations, in whichthey outperformed a phrase-based system trainedon identical data.4.1 Hiero: a Hierarchical MT PipelineHiero is a hierarchical system that expresses itstranslation model as a synchronous context-freegrammar (Chiang, 2007).
No explicit syntactic in-formation appears in the core model.
A phrasediscovery procedure over word-aligned sentencepairs provides rule frequency counts, which arenormalized to estimate features on rules.The grammar rules of Hiero all share a singlenon-terminal symbol X , and have at most twonon-terminals and six total items (non-terminalsand lexical items), for example:my X2 ?s X1 ?
X1 de mi X2We extracted the grammar from training data usingstandard parameters.
Rules were allowed to spanat most 15 words in the training data.The log-linear model weights were trained us-ing MIRA, a margin-based optimization proce-dure that accommodates many features (Crammerand Singer, 2003; Chiang et al, 2008).
In additionto standard rule frequency features, we includedthe distortion and syntactic features described inChiang et al (2008).4.2 SBMT: a Syntax-Based MT PipelineSBMT is a string-to-tree translation system withrich target-side syntactic information encoded inthe translation model.
The synchronous grammarrules are extracted from word aligned sentencepairs where the target sentence is annotated witha syntactic parse (Galley et al, 2004).
Rules mapsource-side strings to target-side parse tree frag-ments, and non-terminal symbols correspond totarget-side grammatical categories:(NP (NP (PRP$ my) NN2 (POS ?s)) NNS1)?NNS1 de mi NN2We extracted the grammar via an array of criteria(Galley et al, 2006; DeNeefe et al, 2007; Marcuet al, 2006).
The model was trained using min-imum error rate training for Arabic (Och, 2003)and MIRA for Chinese (Chiang et al, 2008).572Arabic-EnglishObjective Hiero SBMTMin.
Bayes Risk (Alg 1) 2h 47m 12h 42mFast Consensus (Alg 3) 5m 49s 5m 22sSpeed Ratio 29 142Chinese-EnglishObjective Hiero SBMTMin.
Bayes Risk (Alg 1) 10h 24m 3h 52mFast Consensus (Alg 3) 4m 52s 6m 32sSpeed Ratio 128 36Table 1: Fast consensus decoding is orders of magnitudefaster than MBR when using BLEU as a similarity measure.Times only include reranking, not k-best list extraction.4.3 Data ConditionsWe evaluated on both Chinese-English andArabic-English translation tasks.
Both Arabic-English systems were trained on 220 millionwords of word-aligned parallel text.
For theChinese-English experiments, we used 260 mil-lion words of word-aligned parallel text; the hi-erarchical system used all of this data, and thesyntax-based system used a 65-million word sub-set.
All four systems used two language models:one trained from the combined English sides ofboth parallel texts, and another, larger, languagemodel trained on 2 billion words of English text(1 billion for Chinese-English SBMT).All systems were tuned on held-out data (1994sentences for Arabic-English, 2010 sentences forChinese-English) and tested on another dataset(2118 sentences for Arabic-English, 1994 sen-tences for Chinese-English).
These datasets weredrawn from the NIST 2004 and 2005 evaluationdata, plus some additional data from the GALEprogram.
There was no overlap at the segment ordocument level between the tuning and test sets.We tuned b, the base of the log-linear model,to optimize consensus decoding performance.
In-terestingly, we found that tuning b on the samedataset used for tuning ?was as effective as tuningb on an additional held-out dataset.4.4 Results over K-Best ListsTaking expectations over 1000-best lists6 and us-ing BLEU7 as a similarity measure, both MBR6We ensured that k-best lists contained no duplicates.7To prevent zero similarity scores, we also used a standardsmoothed version of BLEU that added 1 to the numerator anddenominator of all n-gram precisions.
Performance resultsArabic-EnglishExpectations Similarity Hiero SBMTBaseline - 52.0 53.9104-best BLEU 52.2 53.9Forest BLEU 53.0 54.0Forest Linear G 52.3 54.0Chinese-EnglishExpectations Similarity Hiero SBMTBaseline - 37.8 40.6104-best BLEU 38.0 40.7Forest BLEU 38.2 40.8Forest Linear G 38.1 40.8Table 2: Translation performance improves when computingexpected sentences from translation forests rather than 104-best lists, which in turn improve over Viterbi translations.
Wealso contrasted forest-based consensus decoding with BLEUand its linear approximation, G. Both similarity measures areeffective, but BLEU outperforms G.and our variant provided consistent small gains of0.0?0.2 BLEU.
Algorithms 1 and 3 gave the samesmall BLEU improvements in each data conditionup to three significant figures.The two algorithms differed greatly in speed,as shown in Table 1.
For Algorithm 1, we ter-minated the computation of E[BLEU(e; e?)]
foreach e whenever e could not become the maxi-mal hypothesis.
MBR speed depended on howoften this shortcut applied, which varied by lan-guage and system.
Despite this optimization, ournew Algorithm 3 was an average of 80 times fasteracross systems and language pairs.4.5 Results for Forest-Based DecodingTable 2 contrasts Algorithm 3 over 104-best listsand forests.
Computing E[?(e?)]
from a transla-tion forest rather than a 104-best list improved Hi-ero by an additional 0.8 BLEU (1.0 over the base-line).
Forest-based expectations always outper-formed k-best lists, but curiously the magnitudeof benefit was not consistent across systems.
Webelieve the difference is in part due to more ag-gressive forest pruning within the SBMT decoder.For forest-based decoding, we compared twosimilarity measures: BLEU and its linear TaylorapproximationG from section 3.3.8 Table 2 showswere identical to standard BLEU.8We did not estimate the ?
parameters of G ourselves;instead we used the parameters listed in Tromble et al(2008), which were also estimated for GALE data.
Wealso approximated E[?
(e?, t)] with a clipped expected count573Choose a distribution P over a set of translations EMBR over Sentence PairsCompute pairwise similarityCompute expectationsMax expected similarity Max feature similarity3/3 1/4 2/51/3 4/4 0/52/3 0/4 5/5MBR over FeaturesE [?
(efficient)] = 0.6E [?
(forest)] = 0.7E [?
(decoding)] = 0.7E [?
(for)] = 0.3E [?
(rusty)] = 0.3E [?
(coating)] = 0.3E [?
(a)] = 0.4E [?
(fish)] = 0.4E [?
(ain?t)] = 0.4c1 c2 c3r1r2r31232350.050.250.450.650.8511,660 513,245 514,830Total model score for 1000 translationsCorpusBLEU020406080Hiero SBMT56.661.451.150.5N-grams from baseline translationsN-grams with high expected countForest samples (b?2)Forest samples (b?5)Viterbi translationsU(e2; e1) =|efficient||efficient for rusty coating|EU(e1; e?)
= 0.3(1+13)+0.4?23= 0.667EU(e2; e?)
= 0.375EU(e3; e?)
= 0.520U(e1;E?)
=0.6+0.7+0.73= 0.667U(e2;E?)
= 0.375U(e3;E?)
= 0.520P (e1|f) = 0.3 ; e1 = efficient forest decodingP (e2|f) = 0.3 ; e2 = efficient for rusty coatingP (e3|f) = 0.4 ; e3 = A fish ain?t forest decodingI ... telescopeYo vi al hombre con el telescopioI ... saw the ... man with ... telescopethe ... telescope0.4?saw the?
?man with?0.6?saw the?1.0?man with?E [c(e, ?man with?)]
=?hP (h|f) ?
c(h, ?man with?
)= 0.4 ?
1 + (0.6 ?
1.0) ?
1N-gramPrecisionFigure 3: N -grams with high expected count are more likelyto appear in the reference translation that n-grams in thetranslation model?s Viterbi translation, e?.
Above, we com-pare the precision, relative to reference translations, of sets ofn-grams chosen in two ways.
The left bar is the precision ofthe n-grams in e?.
The right bar is the precision of n-gramswith E[c(e, t)] > ?.
To justify this comparison, we chose ?so that both methods of choosing n-grams gave the same n-gram recall: the fraction of n-grams in reference translationsthat also appeared in e?
or had E[c(e, t)] > ?.that both similarities were effective, but BLEUoutperformed its linear approximation.4.6 AnalysisForest-based consensus decoding leverages infor-mation about the correct translation from the en-tire forest.
In particular, consensus decodingwith BLEU chooses translations using n-gramcount expectations E[c(e, t)].
Improvements intranslation quality should therefore be directly at-tributable to information in these expected counts.We endeavored to test the hypothesis that ex-pected n-gram counts under the forest distributioncarry more predictive information than the base-line Viterbi derivation e?, which is the mode of thedistribution.
To this end, we first tested the pre-dictive accuracy of the n-grams proposed by e?
:the fraction of the n-grams in e?
that appear in areference translation.
We compared this n-gramprecision to a similar measure of predictive accu-racy for expected n-gram counts: the fraction ofthe n-grams t with E[c(e, t)] ?
?
that appear ina reference.
To make these two precisions com-parable, we chose ?
such that the recall of ref-erence n-grams was equal.
Figure 3 shows thatcomputing n-gram expectations?which sum overtranslations?improves the model?s ability to pre-dict which n-grams will appear in the reference.min(1,E[c(e?, t)]).
Assuming an n-gram appears at mostonce per sentence, these expressions are equivalent, and thisassumption holds for most n-grams.Reference translation:Mubarak said that he received a telephone call fromSharon in which he said he was ?ready (to resume ne-gotiations) but the Palestinians are hesitant.
?Baseline translation:Mubarak said he had received a telephone call fromSharon told him he was ready to resume talks with thePalestinians.Fast forest-based consensus translation:Mubarak said that he had received a telephone call fromSharon told him that he ?was ready to resume the nego-tiations) , but the Palestinians are hesitant.
?Figure 4: Three translations of an example Arabic sentence:its human-generated reference, the translation with the high-est model score under Hiero (Viterbi), and the translationchosen by forest-based consensus decoding.
The consensustranslation reconstructs content lost in the Viterbi translation.We attribute gains from fast consensus decodingto this increased predictive accuracy.Examining the translations chosen by fast con-sensus decoding, we found that gains in BLEU of-ten arose from improved lexical choice.
However,in our hierarchical systems, consensus decodingdid occasionally trigger large reordering.
We alsofound examples where the translation quality im-proved by recovering content that was missingfrom the baseline translation, as in Figure 4.5 ConclusionWe have demonstrated substantial speed increasesin k-best consensus decoding through a new pro-cedure inspired by MBR under linear similaritymeasures.
To further improve this approach, wecomputed expected n-gram counts from transla-tion forests instead of k-best lists.
Fast consensusdecoding using forest-based n-gram expectationsand BLEU as a similarity measure yielded con-sistent improvements over MBR with k-best lists,yet required only simple computations that scalelinearly with the size of the translation forest.The space of similarity measures is large andrelatively unexplored, and the feature expectationsthat can be computed from forests extend beyondn-gram counts.
Therefore, future work may showadditional benefits from fast consensus decoding.AcknowledgementsThis work was supported under DARPA GALE,Contract No.
HR0011-06-C-0022.574ReferencesAbhaya Agarwal and Alon Lavie.
2007.
METEOR:An automatic metric for MT evaluation with highlevels of correlation with human judgments.
In Pro-ceedings of the Workshop on Statistical MachineTranslation for the Association of ComputationalLinguistics.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics.Koby Crammer and Yoram Singer.
2003.
Ultracon-servative online algorithms for multiclass problems.Journal of Machine Learning Research, 3:951?991.Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What can syntax-based MT learnfrom phrase-based MT?
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing and CoNLL.Nicola Ehling, Richard Zens, and Hermann Ney.
2007.Minimum Bayes risk decoding for BLEU.
In Pro-ceedings of the Association for Computational Lin-guistics: Short Paper Track.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of HLT: the North American Chapterof the Association for Computational Linguistics.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of the Association for Computational Lin-guistics.Vaibhava Goel and William Byrne.
2000.
MinimumBayes-risk automatic speech recognition.
In Com-puter, Speech and Language.Joshua Goodman.
1996.
Parsing algorithms and met-rics.
In Proceedings of the Association for Compu-tational Linguistics.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of the Associa-tion for Machine Translation in the Americas.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofthe Association for Computational Linguistics.Shankar Kumar and William Byrne.
2002.
MinimumBayes-risk word alignments of bilingual texts.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing.Shankar Kumar and William Byrne.
2004.
MinimumBayes-risk decoding for statistical machine transla-tion.
In Proceedings of the North American Chapterof the Association for Computational Linguistics.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009.Variational decoding for statistical machine transla-tion.
In Proceedings of the Association for Compu-tational Linguistics and IJCNLP.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical machinetranslation with syntactified target language phrases.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of the Associationfor Computational Linguistics.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof the Association for Computational Linguistics.David Smith and Noah Smith.
2007.
Probabilisticmodels of nonprojective dependency trees.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing and CoNLL.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Transla-tion in the Americas.Ivan Titov and James Henderson.
2006.
Loss mini-mization in parse reranking.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing.Roy Tromble, Shankar Kumar, Franz Josef Och, andWolfgang Macherey.
2008.
Lattice minimumBayes-risk decoding for statistical machine transla-tion.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.Ashish Venugopal, Andreas Zollmann, and StephanVogel.
2007.
An efficient two-pass approach tosynchronous-CFG driven statistical MT.
In Pro-ceedings of HLT: the North American Associationfor Computational Linguistics Conference.Hao Zhang and Daniel Gildea.
2008.
Efficient multi-pass decoding for synchronous context free gram-mars.
In Proceedings of the Association for Compu-tational Linguistics.575
