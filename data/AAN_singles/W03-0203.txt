Computer-Aided Generation of Multiple-Choice TestsRuslan Mitkov, Le An HaSchool of Humanities, Languages and Social SciencesUniversity of Wolverhampton, WV1 1SBEmail {r.mitkov, l.a.ha}@wlv.ac.ukAbstractThis paper describes a novel computer-aidedprocedure for generating multiple-choice tests fromelectronic instructional documents.
In addition toemploying various NLP techniques including termextraction and shallow parsing, the program makesuse of language resources such as a corpus andWordNet.
The system generates test questions anddistractors, offering the user the option to post-editthe test items.1.
IntroductionMultiple-choice tests have proved to be an efficient toolfor measuring students' achievement.1 The manualconstruction of such tests, however, is a time-consuming and labour-intensive task.In this paper we seek to provide an alternative to thelengthy and demanding activity of developing multiple-choice tests and propose a new, NLP-based approachfor generating tests from narrative texts (textbooks,encyclopaedias).
The approach uses a simple set oftransformational rules, a shallow parser, automatic termextraction, word sense disambiguation, a corpus andWordNet.
While in the current experiment we have usedan electronic textbook in linguistics to automaticallygenerate test items in this area, we should note that themethodology is general and can be extended topractically any other area.To the best of our knowledge, no related work hasbeen reported addressing such a type of application.21 This work is not concerned with (and does not discuss)the issue of whether multiple-choice tests are betterassessment methodology that other types of tests.
Whatit focuses on is a new NLP methodology to generatemultiple-choice tests about facts explicitly stated in atext.2 Fairon (1999) reports that their exercises ?can take theappearance of a multiple choice test?
(if distractors areadded), but does not explain exactly as to how this canbe done.2.
NLP-based methodology for generationof multiple-choice test itemsThe proposed methodology for generating multiple-choice test items is based on the premise that questionsshould focus on key concepts rather than addressing lesscentral and even irrelevant concepts or ideas.
Thereforethe first stage of the procedure is to identify domain-specific terms which serve as ?anchors?
of eachquestion.
By way of example, syntax is a primecandidate for a domain-specific term in the sentence"Syntax is the branch of linguistics which studies theway words are put together into sentences".
Thissentence can be then transformed into questions askingabout this term such as "Which branch of linguisticsstudies the way words are put together into sentences?
"or "Which discipline studies the way words are puttogether into sentences?"
both of which can act as stemsin multiple-choice test items.Another important premise is that distractors3should be as semantically close to the correct answer aspossible so that no additional clues are provided for thestudents.
Semantically close distractors are moreplausible and therefore better at distinguishing good,confident students from poor and uncertain ones.
In theabove example, the distractors for the correct answersyntax should preferably be semantics or pragmaticsand not chemistry or football, for instance.In order to keep the test item comprehensible andavoid additional complexity, the test questions aregenerated from declarative sentences using simpletransformational rules which, in turn, results in onlyminimal change of the original wording.Underpinned by the above principles, a system forcomputer-aided generation of multiple-choice test itemsfrom instructional documents in electronic form hasbeen implemented.
The system is built on separatecomponents, which perform the following tasks: (i) termextraction, (ii) selection of distractors and (iii) questiongeneration.3 Known also as ?distracters?
in the literature of classicaltest theory.2.1 Term extractionTo retrieve terms, nouns and noun phrases are firstidentified, using the FDG shallow parser (Tapanainenand J?rvinen 1997).
Next, their frequency is countedand sorted, and nouns with a frequency over a certainthreshold4 are considered as key terms.
In addition, nounphrases having these key terms as heads, and satisfyingthe regular expression [AN]+N or [AN]*NP[AN]*N(Justeson and Katz 1996), are considered as terms.Although this method is very simple,5 the results showthat, for this particular application, the performance ismore than acceptable (only 3 questions did not address adomain-specific term).
One of the main reasons not toemploy more complicated methods for term extractionderives from the small size of the corpus used in thecurrent experiment (10 000 words).It should be noted that, from a keyword, as in thecase of the keyword "phrase", a list of semanticallyclose terms including noun phrase, verb phrase,adjective phrase and adverb phrase can be obtained.
Inaddition, a word sense disambiguation program is usedto identify the correct sense of the alternatives giventhat WordNet frequently returns an unnecessarily highnumber of senses.
The word sense disambiguationalgorithm compares the definition of sense (as extractedfrom WordNet) and the context of the keyword (wordsaround the keyword in the corpus).As an illustration, in the following extract (Kies2003)(1) A prepositional phrase at the beginning of asentence constitutes an introductory modifier.one of the terms identified is introductory modifierwhich can serve as an ?anchor?
for generating  the testquestion.2.2 Selection of distractorsWordNet is consulted to compute concepts semanticallyclose to the correct answer/concept which can then beselected as distractors.
WordNet retrieves hypernyms,hyponyms, and coordinates of the term, if applicable.
IfWordNet returns too many concepts, those appearing inthe corpus are given preference.
If, as in (1), the term is4 For this particular project the threshold has beendetermined through experiments.
The value of thethreshold of course depends on a number of parameterssuch as the size of the corpus, number of nouns etc.5 We experimented with the tf.idf method for key termextraction and noted that while precision is slightlyhigher, recall is much lower.
As the time needed tovalidate a question is much less than the time needed toproduce it, we believe that the recall rate is moreimportant.a noun phrase and WordNet fails to return anysemantically close concept, the corpus is searched fornoun phrases with the same head which are then used asdistractors.6 As an illustration, the electronic textbookcontains the following noun phrases with modifier as thehead, each one of which can act as a distractor: modifierthat accompanies a noun, associated modifier,misplaced modifier.
As a result, the program generatesthe following multiple-choice test item:(2) What does a prepositional phrase at thebeginning of a sentence constitute?i.
a modifier that accompanies anounii.
an associated modifieriii.
an introductory modifieriv.
a misplaced modifier2.3 Generation of test questionsSentences eligible for question generation are thosecontaining domain-specific terms.
Another condition fora sentence to be eligible is that its structure is of SVO orSV type.7 Currently, a number of simple questiongeneration rules have been implemented.
Example rulesinclude the transformation of an SVO sentence in whichthe subject is a term, into the question  "Which HVO"where H is a hypernym of the term.
Such a rule wouldgenerate the question "Which part of speech is the mostcentral element in a clause" from the sentence "The verbis the most central element in a clause".
This ruleoperates in several variants, one being that if thehypernym is a key term, then a ?Which kind of?question may be generated  (e.g.
?Transitive verbsrequire objects?
would trigger the question "Which kindof verbs require objects?").
Another rule often usedtransforms an SVO sentence with object representing aterm into the question "What do/does/did the S V".
Byway of example, this rule would convert the sentence inexample (1) into the question "What does aprepositional phrase at the beginning of a sentenceconstitute?
"The system makes use of agreement rules whichensure the grammaticality of the question generated.These rules also check for agreement between conceptsmentioned in the question and the distractors.
As anillustration, in addition to the local agreement in thequestion "What kind of phrases can act as adjectives,6 In the rare case of the program not being able toextract suitable distractors from WordNet or/and fromthe corpus, no test item is generated.7 Sentences of such types are identified by the FDGparser which returns syntax functions.?
?29 of 36Which kind of pronoun will agree with thesubject in number, person, and gender?relative pronounsecond person pronounindefinite pronounreflexive pronoun?
?adverbs and nouns", the alternatives selected will beplural (e.g.
infinitive phrases, prepositional phrases,adverbial phrases, noun phrases).
On the other hand,the alternatives belonging to the test item featuring thequestion "What grammatical category does aprepositional phrase at the beginning of a sentenceconstitute?"
will be singular.The generation strategy of multiple-choice itemsincluded additional genre-specific heuristics such asdiscounting examples for further processing, excludingsentences that refer to tables or previously mentionedentities, not splitting compound verbs, etc.3.
In-class experiments and system interfaceWe introduced a controlled set8 of the generated testitems into a classroom environment in order to obtainsufficient evaluation data related to theiracceptability/revision and quality.
The controlled setcurrently consists of 24 test items generated with thehelp of the program and 12 items produced manually.A total of 45 undergraduate students inlanguage/linguistics took the class test.
The majority ofstudents were from our university, but several studentswere studying in other UK or European Universities.Students were asked not to spend more than 2 minuteson a test question.Figure 1: A snapshot of the interfaceThe system works through the QuestionmarkPerception web-based testing software which in additionto providing a user-friendly interface, computes diversestatistics related to the test questions answered.
Figure1 shows the interface of the system in a class testenvironment.
The test item displayed is one of the 248 Only items approved by a linguistics lecturer wereused in the experiment (e.g.
it was made sure that theitems addressed material covered by undergraduatestudents).items generated with the help of the system that are usedin the experiment.9The current experimental setting does not look at theproblem of delivering a balanced test of preset overalldifficulty based on random (or constraint-driven)selection of test items.
Instead, it focuses on exploringthe feasibility of the computer-aided procedure and onthe quality of the test items produced.4.
EvaluationIn order to validate the efficiency of the method, weevaluated the performance of the system in two differentways.
Firstly, we investigated the efficiency of theprocedure by measuring the average time needed toproduce a test item with the help of the program asopposed to the average time needed to produce a testitem manually.10 Secondly, we examined the quality ofthe items generated with the help of the program, andcompared it with the quality of the items producedmanually.
The quality was assessed via standard testtheory measures such as discriminating power anddifficulty of each test item, and the usefulness of eachalternative was applied.4.1 The procedure of generating test items with thehelp of the program and its efficiencyThe first step of the procedure consists of the automaticgeneration of test items.
The items so generated werethen either (i) declared as ?worthy?
and accepted fordirect use without any revision, or further post-editedbefore being put into use, or (ii) declared as ?unworthy?and discarded.
?Unworthy?
items were those that did notfocus on a central concept or required too muchrevision, and so they were rejected.The items selected for further post-editing requiredminor, fair or major revisions.
?Minor?
revisiondescribes minor syntactical post-editing of the testquestion, including minor operations such insertions ofarticles, correction of spelling and punctuation.
?Fair?revision refers to some grammatical post-editing of thetest question, including re-ordering or deletion of wordsand replacement of one distractor at most.
?Major?revision applied to the generated test items involvedmore substantial grammatical revision of the testquestion and replacement of two or more of the9 The position of the correct answer (in this case?reflexive pronoun?)
is generated randomly.10 Two graduate students in linguistics acted as post-editors.
The same students were involved in theproduction of test items manually.
The texts used wereselected with care so that possible influence ofpotentially similar or familiar texts was minimised.
Seealso the discussion in section 5 on the effect offamiliarity.distractors.
As an illustration, the automaticallygenerated test item(3) Which kind of language unit seem to be themost obvious component of language, and anytheory that fails to account for the contributionof words to the functioning of language isunworthy of our attention?
(a) word(b) name(c) syllable(d) morphemewas not acceptable in this form and required thedeletion of the text ?and any theory that fails to accountfor the contribution of words to the functioning oflanguage is unworthy of our attention?
which wasclassed as ?fair?
revision.From a total of about 575 items automaticallygenerated by the program, 57% were deemed to be?worthy?
i.e.
considered for further use.
From theworthy items, 6% were approved for direct class test usewithout any post-editing and 94% were subjected topost-editing.
From the items selected for revision, 17%needed minor revision, 36% needed fair revision and47% needed major revision.The time needed to produce 300 test items with thehelp of the program, including the time necessary toreject items, accept items for further editing or approvefor direct use, amounted to 9 hours.
The time needed tomanually produce 65 questions was 7 hours and 30minutes.
This results in an average of 1 minute and 48seconds to produce a test item with the help of theprogram and an average of 6 minutes and 55 seconds todevelop a test item manually (Table 1).items produced Timeaveragetimeper itemcomputer-aided 300 540' 1' 48''Manual 65 450' 6' 55''Table 1: Effectiveness of the method.4.2 Analysis of the items generated with the help ofthe programItem analysis is an important procedure in classical testtheory which provides information as to how well eachitem has functioned.
The item analysis for multiple-choice tests usually consists of the followinginformation (Gronlund 1982): (i) the difficulty of theitem, (ii) the discriminating power and (iii) theusefulness11 of each alternative.
This information cantell us if a specific test item was too easy or too hard,how well it discriminated between high and low scorerson the test and whether all of the alternatives functionedas intended.
Such types of analysis help improve testitems or discard defective items.In order to conduct this type of analysis, we used asimplified procedure, described in  (Gronlund 1982).We arranged the test papers in order from the highestscore to the lowest score.
We selected one third of thepapers and called this the upper group (15 papers).
Wealso selected the same number of papers with the lowestscores and called this the lower group (15 papers).
Foreach item, we counted the number of students in theupper group who selected each alternative; we made thesame count for the lower group.
(i) Item DifficultyWe estimated the Item Difficulty (ID) by establishingthe percentage of students from the two groups whoanswered the item correctly (ID = C/T x 100, where C isthe number who answered the item correctly and T isthe total number of students who attempted the item).From the 24 items subjected to analysis, there were 0too difficult and 3 too easy items.12 The average itemdifficulty was 0.75.
(ii) Discriminating PowerWe estimated the item's Discriminating Power (DP)by comparing the number students in the upper andlower groups who answered the item correctly.
It isdesirable that the discrimination is positive which meansthat the item differentiates between students in the sameway that the total test score does.13 The formula forcomputing the Discriminating Power is as follows: DP= (CU ?
CL): T/2 where CU is the number of students inthe upper group who answered the item correctly andCL - the number of the students in the lower group that11 Originally called ?effectiveness?.
We chose to termthis type of analysis ?usefulness?
to distinguish it fromthe (cost/time) ?effectiveness?
of the (semi-) automaticprocedure as opposed to the manual construction oftests.12 For experimental purposes, we consider an item to be?too difficult?
if ID  0.15 and an item ?too easy?
if ID 0.85.13 Zero DP is obtained when an equal number ofstudents in each group respond to the item correctly.
Onthe other hand, negative DP is obtained when morestudents in the lower group than the upper group answercorrectly.
Items with zero or negative DP should beeither discarded or improved.did so.
Here again T is the total number of studentsincluded in the item analysis.14 The average DP for theset of items used in the class test was 0.40.
From theanalysed test items, there were was only one item thathad a negative discrimination.
(iii) Usefulness of the distractorsThe usefulness of the distractors is estimated bycomparing the number of students in the upper andlower groups who selected each incorrect alternative.
Agood distractor should attract more students from thelower group than the upper group.The evaluation of the distractors estimated theaverage difference between students in the lower andupper groups to be 1.92.
Distractors classed as poor arethose that attract more students from the upper groupthan from the lower group, and there were 6 suchdistractors.
On the other hand, we term distractors notuseful if they are selected by no student.
The evaluationshowed that there were 3 distractors deemed not useful.4.3 Analysis of the items constructed manuallyAn experiment worthwhile pursing was to conduct itemanalysis of the manually produced test items andcompare the results obtained regarding the itemsproduced with the help of the program.
A set of 12manually produced items were subjected to the abovethree types of item analysis.
There were 0 too difficultand 1 too easy items.
The average item difficulty of theitems was 0.59.
The average discriminating power wasassessed to be 0.25 and there were 2 items with negativediscrimination.
The evaluation of the usefulness of thedistractors resulted in an average difference betweenstudents in the upper and lower groups of 1.18.
Therewere 10 distractors that attracted more students from the14 Maximum positive DP is obtained only when allstudents in the upper group answer correctly and no onein the lower group does.
An item that has a maximumDP (1.0) would have an ID 0.5; therefore, test authorsare advised to construct items at the 0.5 level ofdifficulty.upper group and were therefore, declared as poor and 2distractors not selected at all, and therefore deemed tobe not useful.Table 2 summarises the item analysis results forboth test items produced with the help of the programand those produced by hand.5.
Discussion and plans for future workThe evaluation results clearly show that the constructionof multiple-choice test items with the help of theprogram is much more effective than purely manualconstruction.
We believe that this is the main advantageof the proposed methodology.
As an illustration, thedevelopment of a test databank of considerable sizeconsisting of 1000 items would require 30 hours ofhuman input when using the program, and 115 hours ifdone manually.
This has direct financial implications asthe time and cost in developing test items would bedramatically cut.At the same time, the test item analysis shows thatthe quality of test items produced with the help programis not compromised in exchange for time and laboursavings.
The test items produced with of the programwere evaluated as being of very satisfactory quality.
Asa matter of fact, in many cases they scored even betterthan those manually produced.
Whereas the itemdifficulty factor assessed for manual items emerges asbetter15, of those produced with the help of the program,there were only 3 too easy items and 0 too difficult ones.In addition, whilst the values obtained for thediscriminating power are not as high as we would havedesired, the items produced with the help of the programscored much better on that measure and what is alsovery important, is that there was only one item amongthem with negative discrimination (as opposed to 2from those manually constructed).
Finally, the analysisof the distractors confirms that it is not possible to classthe manually produced test items as better quality thanthe ones produced with the help of the program.
The testitems generated with the help of the program scored15 Ideally, item difficulty should be around the mark of0.5item difficulty item discriminating power usefulness of distractorsavgitemdifficultytooeasyToodifficultaveragediscriminatingpowernegativediscriminatingpowerpoornotusefulTotalavgdifferencecomputer-aided0.75 3 0 0.4 1 6 3 65 1.92manual 0.59 1 0 0.25 2 10 2 33 1.18Table 2: Item analysisbetter on the number of distractors deemed as not useful,were assessed to contain fewer poor distractors and hada higher average difference between students in thelower and upper groups.In order to ensure a more objective assessment of theefficiency of the procedure, we plan to run the followingexperiment.
At least 6 months after a specific set ofitems has been produced with the help of the program,the post-editors involved will be asked to produceanother, based on the same material, manually.Similarly, after such a period items originally producedmanually will be produced by the same post-editorswith the help of the program.
Such an experiment isexpected to extinguish any effect of familiarity and toprovide a more objective measure as to how computer-aided construction of tests is more effective than manualproduction.It should be noted that the post-editors were notprofessional test developers.
It would be interesting toinvestigate the impact of the program on professionaltest developers.
This is an experiment envisaged as partof our future work.In addition to extending the set of test items to beevaluated and the samples of students taking the test,further work includes experimenting with moresophisticated term extraction techniques and with othermore elaborate models for measuring semanticsimilarity of concepts.
We would like to test thefeasibility of using collocations from an appropriatedomain corpus with a view to extending the choice ofplausible distractors.
We also envisage the developmentof a more comprehensive grammar for generatingquestions, which in turn will involve studying andexperimenting with existing question generationtheories.
As our main objective has been to investigatethe feasibility of the methodology, we have so farrefrained from more advanced NLP processing of theoriginal documents such as performing anaphoraresolution and temporal or spatial reasoning which willcertainly allow for more questions to be generated.Future work also envisages evaluation as to what extentthe questions cover the course material.
Finally, eventhough the agreement between post-editors appears tobe a complex issue, we would like to investigate it inmore depth.
This agreement should be measured onsemantic rather than syntactic principles, as the post-editors may produce syntactically different testquestions which are semantically equivalent.
Similarly,different distractors may be equally good if they areequal in terms of semantic distance to the correctanswer.6.
ConclusionThis paper describes a novel NLP-based and computer-aided procedure for the construction of multiple-choicetests from instructional documents in electronic form.The results from the evaluation conducted suggest thatthe new procedure is very effective in terms of time andlabour, and that the test items produced with the help ofthe program are not of inferior quality to those producedmanually.ReferencesFairon, C. (1999).
?A Web-based System for AutomaticLanguage Skill Assessment: EVALING?.Proceedings of Computer Mediated LanguageAssessment and Evaluation in Natural LanguageProcessing Workshop.Gronlund, N. (1982) Constructing achievement tests.New York: Prentice-Hall Inc.Justeson, J. S. and S. L. Katz (1996) ?Technicalterminology: some linguistic properties and analgorithm for identification in text?.
NaturalLanguage Engineering, 3, (2), 259-289.Kies, D. (2003) Modern English Grammar.
Onlinetextbook.http://www.papyr.com/hypertextbooks/engl_126/book126.htmTapanainen, P. and J?rvinen, T. (1997) ?A non-projective dependency parser?.
Proceedings of the5th Conference of Applied Natural LanguageProcessing (ANLP-5), 64-71.
