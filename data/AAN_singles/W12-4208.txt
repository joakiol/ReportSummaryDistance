Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67?75,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsExtracting Semantic Transfer Rules from Parallel Corporawith SMT Phrase AlignersPetter Haugereid and Francis BondLinguistics and Multilingual StudiesNanyang Technological Universitypetterha@ntu.edu.sg bond@ieee.orgAbstractThis paper presents two procedures for ex-tracting transfer rules from parallel corporafor use in a rule-based Japanese-English MTsystem.
First a ?shallow?
method wherethe parallel corpus is lemmatized before it isaligned by a phrase aligner, and then a ?deep?method where the parallel corpus is parsed bydeep parsers before the resulting predicatesare aligned by phrase aligners.
In both pro-cedures, the phrase tables produced by thephrase aligners are used to extract semantictransfer rules.
The procedures were employedon a 10 million word Japanese English paral-lel corpus and 190,000 semantic transfer ruleswere extracted.1 IntroductionJust like syntactic and semantic information finds itsway into SMT models and contribute to improvedquality of SMT systems, rule-based systems bene-fit from the inclusion of statistical models, typicallyin order to rank the output of the components in-volved.
In this paper, we present another way of im-proving RBMT systems with the help of SMT tools.The basic idea is to learn transfer rules from paral-lel texts: first creating alignments of predicates withthe help of SMT phrase aligners and then extractingsemantic transfer rules from these.
We discuss twoprocedures for creating the alignments.
In the firstprocedure the parallel corpus is lemmatized beforeit is aligned with two SMT phrase aligners.
Thenthe aligned lemmas are mapped to predicates withthe help of the lexicons of the parsing grammar andthe generating grammar.
Finally, the transfer rulesare extracted from the aligned predicates.
In the sec-ond procedure, the parallel corpus is initially parsedby the parsing grammar and the generating gram-mar.
The grammars produce semantic representa-tions, which are represented as strings of predicates.This gives us a parallel corpus of predicates, abouta third of the size of the original corpus, which wefeed the phrase aligners.
The resulting phrase tableswith aligned predicates are finally used for extrac-tion of semantic transfer rules.The two procedures complement each other.
Thefirst procedure is more robust and thus learns frommore examples although the resulting rules are lessreliable.
Here we extract 127,000 semantic transferrules.
With the second procedure, which is more ac-curate but less robust, we extract 113,000 semantictransfer rules.
The union of the procedures gives atotal of 190,000 unique rules for the Japanese En-glish MT system Jaen.2 Semantic TransferJaen is a rule-based machine translation system em-ploying semantic transfer rules.
The medium for thesemantic transfer is Minimal Recursion Semantics,MRS (Copestake et al, 2005).
The system consistsof the two HPSG grammars: JACY, which is usedfor the parsing of the Japanese input (Siegel andBender, 2002) and the ERG, used for the generationof the English output (Flickinger, 2000).
The thirdcomponent of the system is the transfer grammar,which transfers the MRS representation produced bythe Japanese grammar into an MRS representationthat the English grammar can generate from: Jaen(Bond et al, 2011).At each step of the translation process, the output67SourceLanguageAnalysis ffMRS-Bitext?Grammar?
?TreebankControllerReranker ffMRS- TargetLanguageGenerationGrammar?
?TreebankffSL?TLSemanticTransfer6MRS?Interactive Use6?Batch Processing6?Figure 1: Architecture of the Jaen MT system.is ranked by stochastic models.
In the default con-figuration, only the 5 top ranked outputs at each stepare kept, so the maximum number of translations is125 (5x5x5).
There is also a final reranking using acombined model (Oepen et al, 2007).The architecture of the MT system is illustrated inFigure 1, where the contribution of the transfer ruleextraction from parallel corpora is depicted by thearrow going from Bitext to Semantic Transfer.Most of the rules in the transfer grammar aresimple predicate changing rules, like the rule formapping the predicate ?_hon_n_rel?
onto the predi-cate ?_book_v_1_rel?.
Other rules are more com-plex, and transfers many Japanese relations intomany English relations.
In all, there are 61 typesof transfer rules, the most frequent being the rulesfor nouns translated into nouns (44,572), noun nouncompounds translated into noun noun compounds(38,197), and noun noun compounds translated intoadjective plus noun (27,679).
31 transfer rule typeshave less than 10 instances.
The most common ruletypes are given in Table 1.11Some of the rule types are extracted by only one ex-traction method.
This holds for the types n_adj+n_mtr,n+n+n_n+n_mtr, n+n_n_mtr, pp+np_np+pp_mtr, andarg1+pp_arg1+pp_mtr, adj_pp_mtr, and preposition_mtr.The lemmatized extraction method extracts rules for triplecompounds n+n+n_n+n.
This is currently not done withthe semantic extraction method, since a template for a triplecompound would include 8 relations (each noun also has aquantifier and there are two compound relations in between),and the number of input relations are currently limited to 5 (butcan be increased).
The rest of the templates are new, and theyhave so far only been successfully integrated with the semanticextraction method.The transfer grammar has a core set of 1,415hand-written transfer rules, covering functionwords, proper nouns, pronouns, time expressions,spatial expressions, and the most common openclass items.
The rest of the transfer rules (190,356unique rules) are automatically extracted from par-allel corpora.The full system is available from http://moin.delph-in.net/LogonTop (differentcomponents have different licenses, all are opensource, mainly LGPL and MIT).3 Two methods of rule extractionThe parallel corpus we use for rule extraction isa collection of four Japanese English parallel cor-pora and one bilingual dictionary.
The corporaare the Tanaka Corpus (2,930,132 words: Tanaka,2001), the Japanese Wordnet Corpus (3,355,984words: Bond, Isahara, Uchimoto, Kuribayashi, andKanzaki, 2010), the Japanese Wikipedia corpus(7,949,605 words),2 and the Kyoto University TextCorpus with NICT translations (1,976,071 words:Uchimoto et al, 2004).
The dictionary is Edict(3,822,642 words: Breen, 2004).
The word totalsinclude both English and Japanese words.The corpora were divided into into development,test, and training data.
The training data from thefour corpora plus the bilingual dictionary was usedfor rule extraction.
The combined corpus used forrule extraction consists of 9.6 million English wordsand 10.4 million Japanese words (20 million wordsin total).3.1 Extraction from a lemmatized parallelcorpusIn the first rule extraction procedure we extractedtransfer rules directly from the surface lemmas ofthe parallel text.
The four parallel corpora weretokenized and lemmatized, for Japanese with theMeCab morphological analyzer (Kudo et al, 2004),and for English with the Freeling analyzer (Padr?et al, 2010), with MWE, quantities, dates and sen-tence segmentation turned off.
(The bilingual dic-tionary was not tokenized and lemmatized, since theentries in the dictionary are lemmas).2The Japanese-English Bilingual Corpus of Wikipedia?sKyoto Articles: http://alaginrc.nict.go.jp/WikiCorpus/index_E.html.68Rule type Hand Lemma Pred Intersect Union Totalnoun_mtr 64 32,033 31,575 19,100 44,508 44,572n+n_n+n_mtr 0 32,724 18,967 13,494 38,197 38,197n+n_adj+n_mtr 0 22,777 15,406 10,504 27,679 27,679arg12+np_arg12+np_mtr 0 9,788 1,774 618 10,944 10,944arg1_v_mtr 22 8,325 1,031 391 8,965 8,987pp_pp_mtr 2 146 8,584 19 8,711 8,713adjective_mtr 27 4,914 4,034 2,183 6,765 6,792arg12_v_mtr 50 4,720 1,846 646 5,920 5,970n_adj+n_mtr 1 - 4,695 - 4,695 4,696n+n_n_mtr 0 2,591 3,273 1,831 4,033 4,033n+n+n_n+n_mtr 0 3,380 - - 3,376 3,376n+adj-adj-mtr 2 633 2,586 182 3,037 3,039n_n+n_mtr 1 - 2,229 - 2,229 2,230pp-adj_mtr 27 1,008 971 1 1,978 2,005p+n+arg12_arg12_mtr 1 1,796 101 35 1,862 1,863pp+np_np+pp_mtr 0 - 1,516 - 1,516 1,516pp+arg12_arg12_mtr 0 852 62 26 888 888arg1+pp_arg1+pp_mtr 1 - 296 - 296 297monotonic_mtr 139 - - - - 139adj_pp_mtr 0 - 112 - 112 112preposition_mtr 53 - 34 - 34 87arg123_v_mtr 3 30 14 8 36 39Table 1: Most common mtr rule types.
The numbers in the Hand column show the number of hand-written rulesfor each type.
The numbers in the Lemma column, show the number of rules extracted from the lemmatized parallelcorpus.
The numbers in the Pred column show the number of rules extracted from the semantic parallel corpus.
TheIntersect column, shows the number of intersecting rules of Lemma and Pred, and the Union column show the numberof distinct rules of Lemma and Pred.We then used MOSES (Koehn et al, 2007) andAnymalign (Lardilleux and Lepage, 2009) to alignthe lemmatized parallel corpus.
We got two phrasetables with 10,812,423 and 5,765,262 entries, re-spectively.
MOSES was run with the default set-tings, and Anymalign ran for approximately 16hours.We selected the entries that had (i) a translationprobability, P(English|Japanese) of more than 0.1,3(ii) an absolute frequency of more than 1,4 (iii) fewerthan 5 lemmas on the Japanese side and fewer than 43This number is set based on a manual inspection of thetransfer rules produced.
The output for each transfer rule tem-plate is inspected, and for some of the templates, in particularthe multi-word expression templates, the threshold is set higher.4The absolute frequency number can, according to AdrienLardilleux (p.c.
), be thought of as a confidence score.
Thelarger, the more accurate and reliable the translation probabili-ties.
1 is the lowest score.lemmas on the English side,5 and (iv) lexical entriesfor all lemmas in Jacy for Japanese and the ERG forEnglish.
This gave us 2,183,700 Moses entries and435,259 Anymalign entries, all phrase table entrieswith a relatively high probability, containing lexicalitems known both to the parser and the generator.The alignments were a mix of one-to-one-or-many and many-to-one-or-many.
For each lemmain each alignment, we listed the possible predicatesaccording to the lexicons of the parsing grammar(Jacy) and the generating grammar (ERG).
Sincemany lemmas are ambiguous, we often ended upwith many semantic alignments for each surfacealignment.
If a surface alignment contained 3 lem-mas with two readings each, we would get 8 (2x2x2)semantic alignments.
However, some of the seman-5These numbers are based on the maximal number of lem-mas needed for the template matching on either side.69tic relations associated with a lemma had very rarereadings.
In order to filter out semantic alignmentswith such rare readings, we parsed the training cor-pus and made a list of 1-grams of the semantic rela-tions in the highest ranked output.
Only the relationsthat could be linked to a lemma with a probabilityof more than 0.2 were considered in the semanticalignment.
The semantic alignments were matchedagainst 16 templates.
Six of the templates are simpleone-to-one mapping templates:1. noun ?
noun2.
adjective ?
adjective3.
adjective ?
intransitive verb4.
intransitive verb ?
intransitive verb5.
transitive verb ?
transitive verb6.
ditransitive verb ?
ditransitive verbThe rest of the templates have more than onelemma on the Japanese side and one or more lem-mas on the English side.
In all, we extracted 126,964rules with this method.
Some of these are relativelysimple, such as 7 which takes a noun compound andtranslates it into a single noun, or 8 which takes aVP and translates it into a VP (without checking forcompositionality, if it is a common pattern we willmake a rule for it).7. n+n?
n(1) ?minor???-?test?
?-?had?I had a quiz.8.
arg12+np?
arg12+np_mtr(2) ??that??-?job??-?
?-?finished?I finished the job.Other examples, such as 9 are more complex, herethe rule takes a Japanese noun-adjective combina-tion and translates it to an adjective, with the exter-nal argument in Japanese (the so-called second sub-ject) linked to the subject of the English adjective.Even though we are applying the templates to learnrules to lemma n-grams, in the translation systemthese rules apply to the semantic representation, sothey can apply to a wide variety of syntactic vari-ations (we give an example of a relative clause be-low).9. n+adj?
adj(3) ?-?previous?-?winter?-?snow??
?-?much-be?Previous winter was snowy.
(4) ?-?snow??much?winter?
?-?was?It was a snowy winter.Given the ambiguity of the lemmas used for theextraction of transfer rules, we were forced to fil-ter semantic relations that have a low probability inorder to avoid translations that do not generalize.One consequence of this is that we were not buildingrules that should have been built in cases where anambiguous lemma has one dominant reading, andone or more less frequent, but plausible, readings.Another consequence is that we were building ruleswhere the dominant reading is used, but where a lessfrequent reading is correct.
The method is not veryprecise since it is based on simple 1-gram counts,and we are not considering the context of the indi-vidual lemma.
A way to improve the quality of theassignment of the relation to the lemma would be touse a tagger or a parser.
However, instead of goingdown that path, we decided to parse the whole par-allel training corpus with the parsing grammar andthe generation grammar of the MT system and pro-duce a parallel corpus of semantic relations insteadof lemmas.
In this way, we use the linguistic gram-mars as high-precision semantic taggers.3.2 Extraction from a parallel corpus ofpredicatesThe second rule extraction procedure is based on aparallel corpus of semantic representations, ratherthan lemmatized sentences.
We parsed the train-ing corpus (1,578,602 items) with the parsing gram-mar (Jacy) and the generation grammar (ERG) ofthe MT system, and got a parse with both grammarsfor 630,082 items.
The grammars employ statisticalmodels trained on treebanks in order to select themost probable analysis.
For our semantic corpus,70we used the semantic representation of the highestranked analysis on either side.The semantic representation produced by theERG for the sentence The white dog barks is given inFigure 2.
The relations in the MRSs are representedin the order they appear in the analysis.6 In the se-mantic parallel corpus we kept the predicates, e.g._the_q_rel, _white_a_1_rel, and so on, but we didnot keep the information about linking.
For verbs,we attached information about the valency.
Verbsthat were analyzed as intransitive, like bark in Fig-ure 2, were represented with a suffix 1x, where 1indicates argument 1 and x indicates a referentialindex: _bark_v_1_rel@1x.
If a verb was analyzedas being transitive or ditransitive, this would be re-flected in the suffix: _give_v_1_rel@1x2x3x.
Theitem corresponding to The white dog barks in the se-mantic corpus would be _the_q_rel _white_a_1_rel_dog_n_1_rel _bark_v_1_rel@1x.The resulting parallel corpus of semantic rep-resentations consists of 4,712,301 relations forJapanese and 3,806,316 relations for English.
Thismeans that the size of the semantic parallel corpusis a little more than a third of the lemmatized paral-lel corpus.
The grammars used for parsing are deeplinguistic grammars, and they do not always performvery well on out of domain data, like for example theJapanese Wikipedia corpus.
One way to increase thecoverage of the grammars would be to include ro-bustness rules.
This would decrease the reliabilityof the assignment of semantic relations, but still bemore reliable than simply using 1-grams to assignthe relation.The procedure for extracting semantic transferrules from the semantic parallel corpus is similarto the procedure for extraction from the lemmatizedcorpus.
The major difference is that the semanticcorpus is disambiguated by the grammars.As with the lemmatized corpus, the semantic par-allel corpus was aligned with MOSES and Anyma-lign.
They produced 4,830,000 and 4,095,744 align-ments respectively.
Alignments with more than 5relations on either side and with a probability ofless than 0.01 were filtered out.7 This left us with6Each predicate has the character span of the correspondingword(s) attached.7A manual inspection of the rules produced by the templatematching showed that most of the rules produced for several of4,898,366 alignments, which were checked against22 rule templates.8 This produced 112,579 rules,which is slightly fewer than the number of rulesextracted from the lemmatized corpus (126,964).49,187 of the rules overlap with the rules extractedfrom the lemmatized corpus, which gives us a totalnumber of unique rules of 190,356.
The distributionof the rules is shown in Table 1.Some of the more complex transferrules types like p+n+arg12_arg12_mtr andpp+arg12_arg12_mtr were extracted in far greaternumbers from the lemmatized corpus than fromthe corpus of semantic representations.
This ispartially due to the fact that the method involvingthe lemmatized corpus is more robust, which meansthat the alignments are done on 3 times as muchdata as the method involving the corpus of semanticpredicates.
Another reason is that the numberof items that need to be aligned to match thesekinds of multi-word templates is larger when therules are extracted from the corpus of semanticrepresentations.
(For example, a noun relationalways has a quantifier binding it, even if there is noparticular word expressing the quantifier.)
Since thenumber of items to be aligned is bigger, the chanceof getting an alignment with a high probability thatmatches the template becomes smaller.One of the transfer rule templates (pp_pp_mtr)generates many more rules with the method in-volving the semantic predicates than the methodinvolving lemmas.
This is because we restrictedthe rule to only one preposition pair (_de_p_rel?
_by_p_means_rel) with the lemmatized corpusmethod, while all preposition pairs are accepted withthe semantic predicate method since the confidencein the output of this method is higher.4 Experiment and ResultsIn order to compare the methods for rule extraction,we made three versions of the transfer grammar, oneincluding only the rules extracted from the lemma-the templates were good, even with a probability as low as 0.01.For some of the templates, the threshold was set higher.8The reason why the number of rule templates is higher withthis extraction method, is that the confidence in the results ishigher.
This holds in particular for many-to-one rules, were thequality of the rules extracted with from the lemmatized corpusis quite low.71???????????????????????
?mrsLTOP h1 hINDEX e2 eRELS???????
?_the_q_rel<0:3>LBL h3 hARG0 x5 xRSTR h6 hBODY h4 h???????,????
?_white_a_1_rel<4:9>LBL h7 hARG0 e8 eARG1 x5?????,??
?_dog_n_1_rel<10:13>LBL h7ARG0 x5???,????
?_bark_v_1_rel<14:20>LBL h9 hARG0 e2ARG1 x5??????HCONS???
?qeqHARG h6LARG h7???????????????????????????
?Figure 2: MRS of The white dog barkstized corpus (Lemm), one including only the rulesextracted from the corpus of semantic representa-tions (Pred), and one including the union of the two(Combined).
In the Combined grammar, the Lemmrules with a probability lower than 0.4 were filteredout if the input relation(s) are already translated byeither handwritten rules or Pred rules since the con-fidence in the Lemm rules is lower.Since the two methods for rule extraction involvedifferent sets of templates, we also made two ver-sions of the transfer grammar including only the 15templates used in both Lemm and Pred.
These werenamed LemmCore and PredCore.The five versions of the transfer grammar weretested on sections 003, 004, and 005 of the TanakaCorpus (4,500 test sentences), and the results areshown in Table 2.
The table shows how the ver-sions of Jaen performs with regard to parsing (con-stant), transfer, generation, and overall coverage.
Italso shows the NEVA9 scores of the highest rankedtranslated sentences (NEVA), and the highest NEVAscore of the 5 highest ranked translations (Oracle).The F1 is calculated based on the overall coverageand the NEVA.The coverage of Lemm and Pred is the same;20.8%, but Pred gets a higher NEVA score thanLemm (21.11 vs. 18.65), and the F1 score is onepercent higher.
When the Lemm and Pred rules arecombined in Combined, the coverage is increasedby almost 6%.
This increase is due to the fact thatthe Lemm and Pred rule sets are relatively compli-9NEVA (N-gram EVAluation: Forsbom (2003)) is a modi-fied version of BLEU.mentary.
Although the use of the Lemm and Predtransfer grammars gives the same coverage (20.8%),only 648 (14.4%) of the test sentences are translatedby both systems.
The NEVA score of Combined isbetween that of Lemm and Pred while the F1 scorebeats both Lemm and Pred.When comparing the core versions of Lemm andPred, LemmCore and PredCore, we see the sametrend, namely that coverage is about the same andthe NEVA score is higher when the Pred rules areused.644 of the test sentences were translated by allversions of the transfer grammar (Lemm, Pred, andCombined).
Table 3 shows how the different ver-sions of Jaen perform on these sentences.
The re-sults show that the quality of the transfer rules ex-tracted from the MRS parallel corpus is higher thanthe quality of the transfer rules based on the lemma-tized parallel corpus.
It also shows that there is asmall decrease of quality when the rules from thelemmatized parallel corpus are added to the rulesfrom the MRS corpus.Version NEVALemmatized 20.44MRS 23.55Lemma + MRS 23.04Table 3: NEVA scores of intersecting translationsThe two best-performing versions of JaEn, Predand Combined, were compared to MOSES (see Ta-ble 4 and Table 5).
The BLEU scores were calcu-lated with multi-bleu.perl, and the METEOR72Parsing Transfer Generation Overall NEVA Oracle F1LemmCore 3590/4500 1661/3590 930/1661 930/4500 18.65 22.99 19.6179.8% 46.3% 56.0% 20.7%Lemm 3590/4500 1674/3590 938/1674 938/4500 18.65 22.99 19.6979.8% 46.6% 56.0% 20.8%PredCore 3590/4500 1748/3590 925/1748 925/4500 20.40 24.81 20.4879.8% 48.7% 52.9% 20.6%Pred 3590/4500 1782/3589 937/1782 937/4500 21.11 25.75 20.9679.8% 49.7% 52.6% 20.8%Combined 3590/4500 2184/3589 1194/2184 1194/4500 19.77 24.00 22.6679.8% 60.9% 54.7% 26.5%Table 2: Evaluation of the Tanaka Corpus Test Datascores were calculated with meteor-1.3.jarusing default settings.10 The human score is a directcomparison, an evaluator11 was given the Japanesesource, a reference translation and the output fromthe two systems, randomly presented as A or B.They then indicated which they preferred, or if thequality was the same (in which case each systemgets 0.5).
All the translations, including the refer-ence translations, were tokenized and lower-cased.In both comparisons, MOSES gets better BLEU andMETEOR scores, while the Jaen translation is pre-ferred by the human evaluator in 58 out of 100 cases.BLEU METEOR HUMANJaEn First 16.77 28.02 58MOSES 30.19 31.98 42Table 4: BLEU Comparison of Jaen loaded with theCombined rules, and MOSES (1194 items)BLEU METEOR HUMANJaEn 18.34 29.02 58MOSES 31.37 32.14 42Table 5: BLEU Comparison of Jaen loaded with the Predrules, and MOSES (936 items)The two systems make different kinds of mis-takes.
The output of Jaen is mostly grammatical,10The METEOR evaluation metric differs from BLEU in thatit does not only give a score for exact match, but it also givespartial scores for stem, synonym, and paraphrase matches.11A Japanese lecturer at NTU, trilingual in English, Japaneseand Korean, not involved in the development of this system, butwith experience in Japanese/Korean MT research.but it may not always make sense.
An example of anonsense translation from Jaen is given in (5).12(5) S: ??????????
?R: We eat fish raw.M: We eat fish raw.J: We eat fish in the camcorder.Jaen sometimes gets the arguments wrong:(6) S: ??????????
?R: He was elected president.M: He was elected president.J: The president chose him.The output of Moses on the other hand is morelikely to lack words in the translation, and it is alsomore likely to be ungrammatical.
A translation witha missing word is shown in (7).
(7) S: ?????????????
?R: The curtains were drawn slowly.M: The curtain was slowly.J: The curtain was drawn slowly.Missing words become extra problematic when anegation is not transferred:(8) S: ???????????
?R: We shouldn?t have any prejudice.M: You should have a bias.J: I shouldn?t have prejudice.Sometimes the Moses output is lacking so manywords that it is impossible to follow the meaning:12The examples below are taken from the development dataof the Tanaka Corpus.
?S?
stands for ?Source?, ?R?
stands for?Reference translation?, ?M?
stands for ?Moses translation,?
and?J?
stands for ?Jaen translation.
?73(9) S: ??????????????
?R: Our brains control our activities.M: The brain to us.J: The brain is controlling our activities.Also the output of Moses is more likely to be un-grammatical, as illustrated in (10) and (11).
(10) S: ????????????
?R: I have a deep love for Japan.M: I is devoted to Japan.J: I am deeply loving Japan.
(11) S: ????????????
?R: She wrung the towel dry.M: She squeezed pressed the towel.J: She wrung the towel hard.5 DiscussionIn order to get a system with full coverage, Jaencould be used with Moses as a fallback.
This wouldcombine the precision of the rule-based system withthe robustness of Moses.
The coverage and the qual-ity of Jaen itself can be extended by using moretraining data.
Our experience is that this holds evenif the training data is from a different domain.
Byadding training data, we are incrementally addingrules to the system.
We still build the rules we builtbefore, plus some more rules extracted from the newdata.
Learning rules that are not applicable for thetranslation task does not harm or slow down the sys-tem.
Jaen has a rule pre-selection program which,before each translation task selects the applicablerules.
When the system does a batch translation of1,500 sentences, the program selects about 15,000 ofthe 190,000 automatically extracted rules, and onlythese will be loaded.
Rules that have been learnedbut are not applicable are not used.13We can also extend the system by adding moretransfer templates.
So far, we are using 23 templates,and by adding new templates for multi-word expres-sions, we can increase the precision.The predicate alignments produced from the par-allel corpus of predicates are relatively precise sincethe predicates are assigned by the grammars.
Thisallows us to extract transfer rules from alignments13The pre-selection program speeds up the system by a factorof three.that are given a low probability (down to 0.01) bythe aligner.We would also like to get more from the data wehave, by making the parser more robust.
Two ap-proaches that have been shown to work with othergrammars is making more use of morphological in-formation (Adolphs et al, 2008) or adding robust-ness rules (Cramer and Zhang, 2010).6 ConclusionWe have shown how semantic transfer rules can belearned from parallel corpora that have been alignedin SMT phrase tables.
We employed two strategies.The first strategy was to lemmatize the parallel cor-pus and use SMT aligners to create phrase tables oflemmas.
We then looked up the relations associatedwith the lemmas using the lexicons of the parser andgenerator.
This gave us a phrase table of alignedrelations.
We were able to extract 127,000 rulesby matching the aligned relations with 16 semantictransfer rule templates.The second strategy was to parse the parallel cor-pus with the parsing grammar and the generatinggrammar of the MT system.
This gave us a paral-lel corpus of predicates, which, because of lack ofcoverage of the grammars, was about a third the sizeof the full corpus.
The parallel corpus of predicateswas aligned with SMT aligners, and we got a sec-ond phrase table of aligned relations.
We extracted113,000 rules by matching the alignments against 22rule templates.
These transfer rules produced thesame number of translation as the rules producedwith the first strategy (20.8%), but they proved tobe more precise.The two rule extraction methods complementeach other.
About 30% of the sentences translatedwith one rule set are not translated by the other.
Bymerging the two rule sets into one, we increased thecoverage of the system to 26.6%.
A human evalua-tor preferred Jaen?s translation to that of Moses for58 out of a random sample of 100 translations.ReferencesPeter Adolphs, Stephan Oepen, Ulrich Callmeier,Berthold Crysmann, Dan Flickinger, and BerndKiefer.
2008.
Some fine points of hybrid natu-ral language parsing.
In European Language Re-74sources Association (ELRA), editor, Proceedingsof the Sixth International Language Resourcesand Evaluation (LREC?08), pages 1380?1387.Marrakech, Morocco.Francis Bond, Hitoshi Isahara, Kiyotaka Uchimoto,Takayuki Kuribayashi, and Kyoko Kanzaki.
2010.Japanese WordNet 1.0.
In 16th Annual Meeting ofthe Association for Natural Language Processing,pages A5?3.
Tokyo.Francis Bond, Stephan Oepen, Eric Nichols, DanFlickinger, Erik Velldal, and Petter Haugereid.2011.
Deep open source machine transla-tion.
Machine Translation, 25(2):87?105.URL http://dx.doi.org/10.1007/s10590-011-9099-4, (Special Issue onOpen source Machine Translation).James W. Breen.
2004.
JMDict: a Japanese-multilingual dictionary.
In Coling 2004 Workshopon Multilingual Linguistic Resources, pages 71?78.
Geneva.Ann Copestake, Dan Flickinger, Carl Pollard, andIvan A.
Sag.
2005.
Minimal Recursion Seman-tics.
An introduction.
Research on Language andComputation, 3(4):281?332.Bart Cramer and Yi Zhang.
2010.
Constrainingrobust constructions for broad-coverage parsingwith precision grammars.
In Proceedings ofCOLING-2010, pages 223?231.
Beijing.Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering, 6(1):15?28.
(Special Issue on Effi-cient Processing with HPSG).Eva Forsbom.
2003.
Training a super model look-alike: Featuring edit distance, n-gram occurrence,and one reference translation.
In In Proceedingsof the Workshop on Machine Translation Evalua-tion.
Towards Systemizing MT Evaluation.Philipp Koehn, Wade Shen, Marcello Federico,Nicola Bertoldi, Chris Callison-Burch, BrookeCowan, Chris Dyer, Hieu Hoang, Ondrej Bo-jar, Richard Zens, Alexandra Constantin, EvanHerbst, Christine Moran, and Alexandra Birch.2007.
Moses: Open source toolkit for statisticalmachine translation.
In Proceedings of the ACL2007 Interactive Presentation Sessions.
Prague.URL http://www.statmt.org/moses/.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying Conditional Random Fields toJapanese Morphological Analysis.
In Dekang Linand Dekai Wu, editors, Proceedings of EMNLP2004, pages 230?237.
Association for Computa-tional Linguistics, Barcelona, Spain.Adrien Lardilleux and Yves Lepage.
2009.Sampling-based multilingual alignment.
InProceedings of Recent Advances in NaturalLanguage Processing (RANLP 2009), pages214?218.
Borovets, Bulgaria.Stephan Oepen, Erik Velldal, Jan Tore L?nning,Paul Meurer, and Victoria Rosen.
2007.
Towardshybrid quality-oriented machine translation.
onlinguistics and probabilities in MT.
In 11th Inter-national Conference on Theoretical and Method-ological Issues in Machine Translation: TMI-2007, pages 144?153.Llu?s Padr?, Miquel Collado, Samuel Reese, Ma-rina Lloberes, and Irene Castell?n.
2010.
Freel-ing 2.1: Five years of open-source language pro-cessing tools.
In Proceedings of 7th LanguageResources and Evaluation Conference (LREC2010).
La Valletta.
(http://nlp.lsi.upc.edu/freeling.Melanie Siegel and Emily M. Bender.
2002.
Effi-cient deep processing of Japanese.
In Proceed-ings of the 3rd Workshop on Asian Language Re-sources and International Standardization at the19th International Conference on ComputationalLinguistics, pages 1?8.
Taipei.Yasuhito Tanaka.
2001.
Compilation of a multilin-gual parallel corpus.
In Proceedings of PACLING2001, pages 265?268.
Kyushu.
(http://www.colips.org/afnlp/archives/pacling2001/pdf/tanaka.pdf).Kiyotaka Uchimoto, Yujie Zhang, Kiyoshi Sudo,Masaki Murata, Satoshi Sekine, and HitoshiIsahara.
2004.
Multilingual aligned paral-lel treebank corpus reflecting contextual in-formation and its applications.
In GillesS?rasset, editor, COLING 2004 MultilingualLinguistic Resources, pages 57?64.
COLING,Geneva, Switzerland.
URL http://acl.ldc.upenn.edu/W/W04/W04-2208.bib.75
