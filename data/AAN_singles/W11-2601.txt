Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1?9,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsDialect Translation:Integrating Bayesian Co-segmentation Models with Pivot-based SMTMichael Paul and Andrew Finch and Paul R. Dixon and Eiichiro SumitaNational Institute of Information and Communications TechnologyMASTAR ProjectKyoto, Japanmichael.paul@nict.go.jpAbstractRecent research on multilingual statistical ma-chine translation (SMT) focuses on the usageof pivot languages in order to overcome re-source limitations for certain language pairs.This paper proposes a new method to translatea dialect language into a foreign language byintegrating transliteration approaches basedon Bayesian co-segmentation (BCS) modelswith pivot-based SMT approaches.
The ad-vantages of the proposed method with respectto standard SMT approaches are three fold:(1) it uses a standard language as the pivot lan-guage and acquires knowledge about the re-lation between dialects and the standard lan-guage automatically, (2) it reduces the transla-tion task complexity by using monotone de-coding techniques, (3) it reduces the num-ber of features in the log-linear model thathave to be estimated from bilingual data.
Ex-perimental results translating four Japanesedialects (Kumamoto, Kyoto, Okinawa, Os-aka) into four Indo-European languages (En-glish, German, Russian, Hindi) and two Asianlanguages (Chinese, Korean) revealed thatthe proposed method improves the translationquality of dialect translation tasks and outper-forms standard pivot translation approachesconcatenating SMT engines for the majorityof the investigated language pairs.1 IntroductionThe translation quality of SMT approaches heavilydepends on the amount and coverage of the bilin-gual language resources available to train the statis-tical models.
There are several data collection ini-tiatives1 amassing and distributing large amounts oftextual data.
For frequently used language pairs likeFrench-English, large-sized text data sets are read-ily available.
However, for less frequently used lan-guage pairs, only a limited amount of bilingual re-sources are available, if any at all.In order to overcome language resource limi-tations, recent research on multilingual SMT fo-cuses on the use of pivot languages (de Gispert andMarino, 2006; Utiyama and Isahara, 2007; Wu andWang, 2007; Bertoldi et al, 2008; Koehn et al,2009).
Instead of a direct translation between twolanguages where only a limited amount of bilingualresources is available, the pivot translation approachmakes use of a third language that is more appropri-ate due to the availability of more bilingual corporaand/or its relatedness to the source/target language.In most of the previous research, English has beenthe pivot language of choice due to the richness ofavailable language resources.
However, recent re-search on pivot translation has shown that the usageof non-English pivot languages can improve trans-lation quality of certain language pairs, especiallywhen translating from or into Asian languages (Paulet al, 2009).This paper focuses on the translation of dialects,i.e., a variety of a language that is characteristic ofa particular group of the language?s speakers, intoa foreign language.
A standard dialect (or stan-dard language) is a dialect that is recognized asthe ?correct?
spoken and written form of the lan-guage.
Dialects typically differ in terms of mor-phology, vocabulary and pronunciation.
Various1LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info1methods have been proposed to measure relatednessbetween dialects using phonetic distance measures(Nerbonne and Heeringa, 1997), string distance al-gorithms (Heeringa et al, 2006; Scherrer, 2007), orstatistical models (Chitturi and Hansen, 2008).Concerning data-driven natural language process-ing (NLP) applications like machine translation(MT), however, linguistic resources and tools usu-ally are available for the standard language, but notfor dialects.
In order to create dialect language re-sources, previous research utilized explicit knowl-edge about the relation between the standard lan-guage and the dialect using rule-based and statisticalmodels (Habash et al, 2005; Sawaf, 2010).
In addi-tion, applying the linguistic tools for the standardlanguage to dialect resources is often insufficient.For example, the task of word segmentation, i.e.,the identification of word boundaries in continuoustext, is one of the fundamental preprocessing stepsof MT applications.
In contrast to Indo-Europeanlanguages like English, many Asian languages likeJapanese do not use a whitespace character to sep-arate meaningful word units.
However, the applica-tion of a linguistically motivated standard languageword segmentation tool to a dialect corpus resultsin a poor segmentation quality due to morphologicaldifferences in verbs and adjectives, thus resulting ina lower translation quality for SMT systems that ac-quire the translation knowledge automatically froma parallel text corpus (Paul et al, 2011).This paper differs from previous research in thefollowing aspects:?
it reduces the data sparseness problem of di-rect translation approaches by translating aresource-limited dialect language into a foreignlanguage by using the resource-rich standardlanguage as the pivot language.?
it is language independent and acquires knowl-edge about the relation between the standardlanguage and the dialect automatically.?
it avoids segmentation mismatches between theinput and the translation model by mapping thecharacterized dialect language, i.e., each char-acter is treated as a single token, to the wordsegmentation of the standard language using aBayesian co-segmentation model.?
it reduces the translation task complexity by us-ing monotone decoding techniques.?
it reduces the number of features in the log-linear model that have to be estimated frombilingual data.The details of the proposed dialect translationmethod are described in Section 2.
Experimentswere carried out for the translation of four Japanesedialects (Kumamoto, Kyoto, Okinawa, Osaka) intofour Indo-European languages (English, German,Russian, Hindi) and two Asian languages (Chinese,Korean).
The utilized language resources and theoutline of the experiments are summarized in Sec-tion 3.
The results reveal that the integration ofBayesian co-segmentation models with pivot-basedSMT improves the translation quality of dialect toforeign language translation tasks and that the pro-posed system outperforms standard pivot translationapproaches concatenating SMT engines that trans-late the dialect into the standard language and thestandard language MT output into the foreign lan-guage for the majority of the investigated languagepairs.2 Dialect TranslationSpoken language translation technologies attempt tobridge the language barriers between people withdifferent native languages who each want to engagein conversation by using their mother-tongue.
Forstandard languages, multilingual speech translationservices like the VoiceTra2 system for travel conver-sations are readily available.
However, such tech-nologies are not capable of dealing with dialect lan-guages due to the lack of language resources and thehigh development costs of building speech transla-tion components for a large number of dialect varia-tions.In order to reduce such problems, the dialecttranslation method proposed in this paper integratestwo different methods of transducing a given dialectinput sentence into a foreign language.
In the firststep, the close relationship between the local andstandard language is exploited to directly map char-acter sequences in the dialect input to word seg-ments in the standard language using a Bayesian co-2http://mastar.jp/translation/voicetra-en.html2segmentation approach, details of which are given inSection 2.1.
The proposed transliteration method isdescribed in Section 2.2.
The advantages of the pro-posed Bayesian co-segmentation approach are twofold: it reduces the translation complexity and itavoids segmentation inconsistencies between the in-put and the translation models.
In the second step,a state-of-the-art phrase-based SMT system trainedon a large amount of bilingual data is applied to ob-tain high-quality foreign language translations as de-scribed in Section 2.3.2.1 Bayesian Co-segmentationThe method for mapping the dialect sentences intothe standard language word segments is a directcharacter-to-character mapping between the lan-guages.
This process is known as translitera-tion.
Many transliteration methods have previouslybeen proposed, including methods based on string-similarity measures between character sequences(Noeman and Madkour, 2010) or generation-basedmodels (Lee and Chang, 2003; Tsuji and Kageura,2006; Jiampojamarn et al, 2010).In this paper, we use a generative Bayesian modelsimilar to the one from (DeNero et al, 2008) whichoffers several benefits over standard transliterationtechniques: (1) the technique has the ability to trainmodels whilst avoiding over-fitting the data, (2)compact models that have only a small number ofwell-chosen parameters are constructed, (3) the un-derlying generative transliteration model is based onthe joint source-channel model (Li et al, 2004), and(4) the model is symmetric with respect to sourceand target language.
Intuitively, the model has twobasic components: a model for generating an out-come that has already been generated at least oncebefore, and a second model that assigns a probabil-ity to an outcome that has not yet been produced.Ideally, to encourage the re-use of model parame-ters, the probability of generating a novel bilingualsequence pair should be considerably lower then theprobability of generating a previously observed se-quence pair.
The probability distribution over thesebilingual sequence pairs (including an infinite num-ber of unseen pairs) can be learned directly from un-labeled data by Bayesian inference of the hidden co-segmentation of the corpus.The co-segmentation process is driven by aDirichlet process, which is a stochastic process de-fined over a set S (in our case, the set of all pos-sible bilingual sequence pairs) whose sample pathis a probability distribution on S. The underlyingstochastic process for the generation of a corpuscomposed of bilingual phrase pairs (sk,tk) can bewritten in the following form:G|?,G0 ?
DP (?,G0)(sk, tk)|G ?
G (1)G is a discrete probability distribution over allthe bilingual sequence pairs according to a Dirichletprocess prior with a base measure G0 and concen-tration parameter ?.
The concentration parameter?
> 0 controls the variance of G; intuitively, thelarger ?
is, the more similar G0 will be to G.For the base measure that controls the genera-tion of novel sequence pairs, we use a joint spellingmodel that assigns probability to new sequence pairsaccording to the following joint distribution:G0((s, t)) = p(|s|)p(s||s|)?
p(|t|)p(t||t|)= ?|s|s|s|!
e?
?sv?|s|s ??|t|t|t|!
e?
?tv?|t|t (2)where |s| and |t| are the length in characters ofthe source and target sides of the bilingual sequencepair; vs and vt are the vocabulary sizes of the sourceand target languages respectively; and ?s and ?t arethe expected lengths3 of the source and target.According to this model, source and target se-quences are generated independently: in each casethe sequence length is chosen from a Poisson dis-tribution, and then the sequence itself is generatedgiven the length.
Note that this model is able toassign a probability to arbitrary bilingual sequencepairs of any length in the source and target sequence,but favors shorter sequences in both.The generative model is given in Equation 3.
Theequation assigns a probability to the kth bilingualsequence pair (sk, tk) in a derivation of the corpus,given all of the other sequence pairs in the history sofar (s?k, t?k).
Here ?k is read as: ?up to but notincluding k?.p((sk, tk))|(s?k, t?k))= N((sk, tk)) + ?G0((sk, tk))N + ?
(3)3Following (Xu et al, 2008), we assign the parameters ?s,?t and ?, the values 2, 2 and 0.3 respectively.3Input: Random initial corpus segmentationOutput: Unsupervised co-segmentation of the corpusaccording to the modelforeach iter=1 to NumIterations doforeach bilingual word-pair w ?
randperm(W) doforeach co-segmentation ?i of w doCompute probability p(?i|h)where h is the set of data (excluding w) andits hidden co-segmentationendSample a co-segmentation ?i from thedistribution p(?i|h)Update countsendendAlgorithm 1: Blocked Gibbs SamplingIn this equation, N is the total number of bilingualsequence pairs generated so far and N((sk, tk)) isthe number of times the sequence pair (sk, tk) hasoccurred in the history.
G0 and ?
are the base mea-sure and concentration parameter as before.We used a blocked version of a Gibbs samplerfor training, which is similar to that of (Mochihashiet al, 2009).
We extended their forward filtering/ backward sampling (FFBS) dynamic programingalgorithm in order to deal with bilingual segmenta-tions (see Algorithm 1).
We found our sampler con-verged rapidly without annealing.
The number ofiterations was set by hand after observing the con-vergence behavior of the algorithm in pilot experi-ments.
We used a value of 75 iterations through thecorpus in all experiments reported in this paper.
Formore details on the Bayesian co-segmentation pro-cess, please refer to (Finch and Sumita, 2010).2.2 Dialect to Standard LanguageTransductionA Bayesian segmentation model is utilized to trans-form unseen dialect sentences into the word seg-mentation of the standard language by using thejoint-source channel framework proposed by (Li etal., 2004).
The joint-source channel model, alsocalled the n-gram transliteration model, is a jointprobability model that captures information on howthe source and target sentences can be generatedsimultaneously using transliteration pairs, i.e., themost likely sequence of source characters and tar-get words according to a joint language model builtfrom the co-segmentation from the Bayesian model.Suppose that we have a dialect sentence ?
=l1l2 .
.
.
lL and a standard language sentence ?
=s1s2 .
.
.
sS where li are dialect characters, sj areword tokens of the standard language, and thereexists an alignment ?
=< l1 .
.
.
lq, s1 >, .
.
.
, <lr .
.
.
lL, sS >, 1 ?
q < r ?
L of K translitera-tion units.
Then, an n-gram transliteration model isdefined as the transliteration probability of a translit-eration pair < l, s >k depending on its immediate npreceding transliteration pairs:P (?, ?, ?)
=K?k=1P (< l, s >k|< l, s >k?1k?n+1) (4)For the experiments reported in this paper, we im-plemented the joint-source channel model approachas a weighted finite state transducer (FST) usingthe OpenFst toolkit (Allauzen et al, 2007).
TheFST takes the sequence of dialect characters as itsinput and outputs the co-segmented bilingual seg-ments from which the standard language segmentsare extracted.2.3 Pivot-based SMTRecent research on speech translation focuses oncorpus-based approaches, and in particular on statis-tical machine translation (SMT), which is a machinetranslation paradigm where translations are gener-ated on the basis of statistical models whose param-eters are derived from the analysis of bilingual textcorpora.
SMT formulates the problem of translat-ing a source language sentence src into a target lan-guage sentence trg as a maximization problem ofthe conditional probability:argmaxtrg p(src|trg) ?
p(trg) (5)where p(src|trg) is called a translation model(TM ) and represents the generation probabilityfrom trg into src, and p(trg) is called a languagemodel (LM ) and represents the likelihood of the tar-get language (Brown et al, 1993).
During the trans-lation process (decoding), a score based on the sta-tistical model probabilities is assigned to each trans-lation hypothesis and the one that gives the highestprobability is selected as the best translation.The translation quality of SMT approaches heav-ily depends on the amount and coverage of the bilin-gual language resources available to train the statis-tical models.
In the context of dialect translation,4where only few bilingual language resources (if anyat all) are available for the dialect and the foreignlanguage, only a relatively low translation qualitycan be obtained.
In order to obtain better transla-tions, we apply a pivot translation approach.
Pivottranslation is the translation from a source language(SRC) to a target language (TRG) through an inter-mediate pivot (or bridging) language (PVT).
In thispaper, we select the standard language as the pivotlanguage.Within the SMT framework, various couplingstrategies like cascading, phrase-table composition,or pseudo-corpus generation have been proposed.For the experiments reported in this paper, we uti-lized the cascading approach because it is compu-tational less expensive, but still performs compara-bly well compared to the other pivot translation ap-proaches.
In the first step, the dialect input is tran-scribed into the standard language as described inSection 2.1.
Next, the obtained standard languageMT output is translated into the target language us-ing SMT models trained on the much larger lan-guage resources.3 ExperimentsThe effects of integrating Bayesian co-segmentationmodels with pivot-based SMT are investigated usingthe Basic Travel Expressions Corpus (BTEC), whichis a collection of sentences that bilingual travel ex-perts consider useful for people traveling abroad(Kikui et al, 2006).
For the dialect translation ex-periments, we selected Japanese (ja), a language thatdoes not naturally separate word units, and the di-alects from the Kumamoto (jaku), Kyoto (jaky), Ok-inawa (jaok), and Osaka (jaos) areas.
All dialectsshare the same Japanese writing system that com-bines logographic Chinese characters and two syl-labic scripts, i.e., hiragana (used for native Japanesewords) and katakana (used for foreign loanwordsor onomatopoeia).
For the target language, we in-vestigated four Indo-European languages, i.e., En-glish (en), German (de), Russian (ru), and Hindi(hi) and two Asian languages, i.e., Chinese (zh)and Korean (ko).
The corpus statistics are summa-rized in Table 1, where Voc specifies the vocabularysize and Len the average sentence length of the re-spective data sets.
These languages differ largelyTable 1: Language ResourcesLanguage Voc Len Order Unit InflJapanese ja 17,168 8.5 SOV none moderateEnglish en 15,390 7.5 SVO word moderateGerman de 25,716 7.1 SVO word highRussian ru 36,199 6.4 SVO word highHindi hi 33,629 7.8 SOV word highChinese zh 13,343 6.8 SVO none lightKorean ko 17,246 8.1 SOV phrase moderatein word order (Order: subject-object-verb (SOV),subject-verb-object (SVO)), segmentation unit (Unit:phrase, word, none), and degree of inflection (Infl:high, moderate, light).
Concerning word segmenta-tion, the corpora were preprocessed using language-specific word segmentation tools that are widely-accepted within the MT community for languagesthat do not use white spaces to separate word/phrasetokens, i.e., CHASEN4 for Japanese and ICTCLAS5for Chinese.
For all other languages, simple to-kenization tools were applied.
All data sets werecase-sensitive with punctuation marks preserved.The language resources were randomly split intothree subsets for the evaluation of translation quality(eval, 1k sentences), the tuning of the SMT modelweights (dev, 1k sentences) and the training of thestatistical models (train, 160k sentences).
For thedialect languages, a subset of 20k sentences wasused for the training of translation models for allof the resource-limited language pairs.
In order toavoid word segmentation errors from the standardlanguage segmentation tool beeing applied to dialectresources, these models are trained on bitext, wherethe local dialect source sentence is characterized andthe target language is segmented using language-specific segmentation tools.For the training of the SMT models, standard wordalignment (Och and Ney, 2003) and language mod-eling (Stolcke, 2002) tools were used.
Minimumerror rate training (MERT) was used to tune the de-coder?s parameters on the dev set using the techniqueproposed in (Och and Ney, 2003).
For the trans-lation, an inhouse multi-stack phrase-based decoderwas used.
For the evaluation of translation quality,we applied the standard automatic evaluation metric4http://chasen-legacy.sourceforge.jp5http://www.nlp.org.cn5Table 2: SMT-based Direct Translation QualityBLEU (%)SRC ja jaku jaky jaok jaosTRG (160k) (20k) (20k)en 56.51 32.84 32.27 31.81 30.99 31.97de 51.73 26.24 25.06 25.71 24.37 25.18ru 50.34 23.67 23.12 23.19 22.30 22.07hi 49.99 21.10 20.46 20.40 19.72 20.96zh 48.59 33.80 32.72 33.15 32.66 32.96ko 64.52 53.31 52.93 51.24 49.40 51.57BLEU, which calculates the geometric mean of n-gram precision by the system output with respect toreference translations with the addition of a brevitypenalty to punish short sentences.
Scores range be-tween 0 (worst) and 1 (best) (Papineni et al, 2002).For the experiments reported here, single translationreferences were used.3.1 Direct TranslationTable 2 summarizes the translation performance ofthe SMT engines used to directly translate the sourcelanguage dialects into the foreign language.
Forthe large training data condition (160k), the high-est BLEU scores are obtained for the translation ofJapanese into Korean followed by English, German,Russian, and Hindi with Chinese seeming to be themost difficult translation task out of the investigatedtarget languages.
For the standard language (ja), thetranslation quality for the small data condition (20k)that corresponds to the language resources used forthe translation of the dialect languages is also given.For the Asian target languages, gains of 11%?14%BLEU points are obtained when increasing the train-ing data size from 20k to 160k.
However, an evenlarger increase (24%?27% BLEU points) in trans-lation quality can be seen for all Indo-European tar-get languages.
Therefore, larger gains are to beexpected when the pivot translation framework isapplied to the translation of dialect languages intoIndo-European languages compared to Asian targetlanguages.
Comparing the evaluation results for thesmall training data condition, the highest scores areachieved for the standard language for all target lan-guages, indicating the difficulty in translating the di-alects.
Moreover, the Kumamoto dialect seems to bethe easiest task, followed by the Kyoto dialect andthe Osaka dialect.
The lowest BLEU scores wereTable 3: SMT-based Pivot Translation QualityBLEU (%)SRC jaku jaky jaok jaosTRG (SMTSRC?ja+SMTja?TRG)en 52.10 50.66 45.54 49.50de 47.51 46.33 39.42 44.82ru 44.59 43.83 38.25 42.87hi 45.89 44.01 36.87 42.95zh 45.14 44.26 40.96 44.20ko 60.76 59.67 55.59 58.62obtained for the translation of the Okinawa dialect.3.2 SMT-based Pivot TranslationThe SMT engines of Table 2 are then utilized withinthe framework of the SMT-based pivot translationby (1) translating the dialect input into the stan-dard language using the SMT engines trained on the20k data sets and (2) translating the standard lan-guage MT output into the foreign language usingthe SMT engines trained on the 160k data sets.
Thetranslation quality of the SMT-based pivot transla-tion experiments are summarized in Table 3.
Largegains of 6.2%?25.4% BLEU points compared tothe direct translation results are obtained for all in-vestigated language pairs, showing the effectivenessof pivot translation approaches for resource-limitedlanguage pairs.
The largest gains are obtained forjaku, followed by jaos, jaky, and jaok.
Therefore, theeasier the translation task, the larger the improve-ments of the pivot translation approach.3.3 Bayesian Co-segmentation ModelThe proposed method differs from the standard pivottranslation approach in that a joint-source channeltransducer trained from a Bayesian co-segmentationof the training corpus is used to transliterate the di-alect input into the standard language, as describedin Section 2.2.
This process generates the co-segmented bilingual segments simultaneously in amonotone way, i.e., the order of consecutive seg-ments on the source side as well as on the target sideare the same.
Similarly, the decoding process of theSMT approaches can also be carried out monotoni-cally.
In order to investigate the effect of word orderdifferences for the given dialect to standard languagetransduction task, Table 4 compares the transla-tion performance of SMT approaches with (reorder-6Table 4: Dialect to Standard Language TransductionBLEU (%)SRC jaku jaky jaok jaosEngine (decoding) (SRC?ja)BCS (monotone) 91.55 86.74 80.36 85.04SMT (monotone) 88.39 84.87 74.27 82.86(reordering) 88.39 84.73 74.26 82.66ing) and without (monotone) distortion models tothe monotone Bayesian co-segmentation approach(BCS).
Only minor differences between SMT decod-ing with and without reordering are obtained.
Thisshows that the grammatical structure of the dialectsentences and the standard language sentences arevery similar, thus justifying the usage of monotonedecoding strategies for the given task.
The compari-son of the SMT-based and the BCS-based transduc-tion of the dialect sentences into the standard lan-guage shows that the Bayesian co-segmentation ap-proach outperforms the SMT approach significantly,gaining 1.9% / 2.2% / 3.2% / 6.1% BLEU points forjaky / jaos / jaku / jaok, respectively.3.4 BCS-based Pivot TranslationThe translation quality of the proposed method,i.e.
the integration of the Bayesian co-segmentationmodels into the pivot translation framework, aregiven in Table 5.
The overall gains of the proposedmethod compared to (a) the direct translation ap-proach (see Table 2) and (b) the SMT-based pivottranslation approach (see Table 3) are summarized inTable 6.
The results show that the BCS-based pivottranslation approach also largely outperforms thedirect translation approach, gaining 5.9%?25.3%BLEU points.
Comparing the two pivot translationapproaches, the proposed BCS-based pivot transla-tion method gains up to 0.8% BLEU points overthe concatenation of SMT engines for the Indo-European target languages, but is not able to im-prove the translation quality for translating into Ko-rean and Chinese.
Interestingly, the SMT-basedpivot translation approach seems to be better for lan-guage pairs where only small relative gains from thepivot translation approach are achieved when trans-lating the dialect into a foreign language.
For exam-ple, Korean is a language closely related to Japaneseand the SMT models from the small data conditionalready seem to cover enough information to suc-Table 5: BCS-based Pivot Translation QualityBLEU (%)SRC jaku jaky jaok jaosTRG (BCSSRC?ja+SMTja?TRG)en 52.42 50.68 45.58 50.22de 47.52 46.74 39.93 45.60ru 45.29 44.08 38.39 43.53hi 45.72 44.71 37.60 43.56zh 45.15 43.92 40.15 44.06ko 60.26 59.14 55.33 58.13Table 6: Gains of BCS-based Pivot TranslationBLEU (%)SRC jaku jaky jaok jaosTRG on SMT-based Pivot (Direct) Translationen +0.32 +0.02 +0.04 +0.72(+20.15) (+18.87) (+14.59) (+18.25)de +0.01 +0.41 +0.51 +0.78(+22.46) (+21.03) (+15.56) (+20.50)ru +0.70 +0.25 +0.14 +0.66(+22.17) (+20.89) (+16.09) (+21.46)hi -0.17 +0.70 +0.73 +0.61(+25.26) (+24.31) (+17.88) (+22.60)zh +0.01 -0.34 -0.81 -0.14(+12.43) (+10.77) (+7.49) (+11.10)ko -0.50 -0.53 -0.26 -0.49(+7.33) (+7.90) (+5.93) (+6.56)cessfully translate the dialect languages into Korean.In the case of Chinese, the translation quality foreven the large data condition SMT engines is rela-tively low.
Therefore, improving the quality of thestandard language input might have only a small im-pact on the overall pivot translation performance, ifany at all.
On the other hand, the proposed methodcan be successfully applied for the translation of lan-guage pairs where structural differences have a largeimpact on the translation quality.
In such a transla-tion task, the more accurate transduction of the di-alect structure into the standard language can affectthe overall translation performance positively.4 ConclusionIn this paper, we proposed a new dialect transla-tion method for resource-limited dialect languageswithin the framework of pivot translation.
In the firststep, a Bayesian co-segmentation model is learnedto transduce character sequences in the dialect sen-tences into the word segmentation of the standard7language.
Next, an FST-based joint-source channelmodel is applied to unseen dialect input sentences tomonotonically generate co-segmented bilingual seg-ments from which the standard language segmentsare extracted.
The obtained pivot sentence is thentranslated into the foreign language using a state-of-the-art phrase-based SMT engine trained on a largecorpus.Experiments were carried out for the translationof four Japanese dialects into four Indo-Europeanas well as into two Asian languages.
The re-sults revealed that the Bayesian co-segmentationmethod largely improves the quality of the stan-dard language sentence generated from a dialect in-put compared to SMT-based translation approaches.Although significant improvements of up to 0.8%in BLEU points are achieved for certain targetlanguages, such as all of the investigated Indo-European languages, it is difficult to transfer thegains obtained by the Bayesian co-segmentationmodel to the outcomes for the pivot translationmethod.Further research will have to investigate featureslike language relatedness, structural differences,and translation model complexity to identify indica-tors of translation quality that could enable the selec-tion of BCS-based vs. SMT-based pivot translationapproaches for specific language pairs to improvethe overall system performance further.In addition we would like to investigate the ef-fects of using the proposed method for translatingforeign languages into dialect languages.
As theBayesian co-segmentation model is symmetric withrespect to source and target language, we plan toreuse the models learned for the experiments pre-sented in this paper and hope to obtain new insightsinto the robustness of the Bayesian co-segmentationmethod when dealing with noisy data sets like ma-chine translation outputs.AcknowledgmentsThis work is partly supported by the Grant-in-Aidfor Scientific Research (C) Number 19500137.ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
Open-Fst: A General and Efficient Weighted Finite-StateTransducer Library.
In Proc.
of the 9th Interna-tional Conference on Implementation and Applicationof Automata, (CIAA 2007), volume 4783 of LectureNotes in Computer Science, pages 11?23.
Springer.http://www.openfst.org.Nicola Bertoldi, Madalina Barbaiani, Marcello Federico,and Roldano Cattoni.
2008.
Phrase-Based statisticalmachine translation with Pivot Languages.
In Proc.
ofthe 5th International Workshop on Spoken LanguageTranslation (IWSLT), pages 143?149, Hawaii, USA.Peter Brown, Stephen Della Pietra, Vincent Della Pietra,and Robert Mercer.
1993.
The mathematics of statis-tical machine translation: Parameter estimation.
Com-putational Linguistics, 19(2):263?311.Ragul Chitturi and John Hansen.
2008.
Dialect Clas-sification for online podcasts fusing Acoustic andLanguage-based Structural and Semantic Information.In Proc.
of the 46th Annual Meeting of the Associa-tion for Computational Linguistics - Human LanguageTechnologies (ACL-HLT), Companion Volume, pages21?24, Columbus, USA.Adria de Gispert and Jose B. Marino.
2006.
Catalan-English statistical machine translation without paral-lel corpus: bridging through Spanish.
In Proc.
of 5thInternational Conference on Language Resources andEvaluation (LREC), pages 65?68, Genoa, Italy.John DeNero, Alex Bouchard-Co?te?, and Dan Klein.2008.
Sampling Alignment Structure under aBayesian Translation Model.
In Proc.
of Conferenceon Empirical Methods on Natural Language Process-ing (EMNLP), Hawaii, USA.Andrew Finch and Eiichiro Sumita.
2010.
A BayesianModel of Bilingual Segmentation for Transliteration.In Proc.
of the 7th International Workshop on SpokenLanguage Translation (IWSLT), pages 259?266, Paris,France.Nizar Habash, Owen Rambow, and George Kiraz.
2005.Morphological Analysis and Generation for ArabicDialects.
In Proc.
of the ACL Workshop on Computa-tional Approaches to Semitic Languages, pages 17?24,Ann Arbor, USA.Wilbert Heeringa, Peter Kleiweg, Charlotte Gosskens,and John Nerbonne.
2006.
Evaluation of String Dis-tance Algorithms for Dialectology.
In Proc.
of theWorkshop on Linguistic Distances, pages 51?62, Syd-ney, Australia.Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma,Aditya Bhargava, Qing Dou, Mi-Young Kim, andGrzegorz Kondrak.
2010.
Transliteration Generationand Mining with Limited Training Resources.
In Proc.of the 2010 Named Entities Workshop (NEWS), pages39?47, Uppsala, Sweden.8Genichiro Kikui, Seiichi Yamamoto, ToshiyukiTakezawa, and Eiichiro Sumita.
2006.
Compar-ative study on corpora for speech translation.
IEEETransactions on Audio, Speech and Language,14(5):1674?1682.Philipp Koehn, Alexandra Birch, and Ralf Steinberger.2009.
462 Machine Translation Systems for Europe.In Proc.
of the MT Summit XII, Ottawa, Canada.Chun-Jen Lee and Jason S. Chang.
2003.
Acqui-sition of English-Chinese transliterated word pairsfrom parallel-aligned texts using a statistical machinetransliteration model.
In Proc.
of the HLT-NAACL2003 Workshop on Building and using parallel texts,Volume 3, pages 96?103, Edmonton, Canada.Haizhou Li, Min Zhang, and Jian Su.
2004.
A jointsource-channel model for machine transliteration.
InProc.
of the 42nd ACL, pages 159?166, Barcelona,Spain.Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.2009.
Bayesian unsupervised word segmentation withnested Pitman-Yor language modeling.
In Proc of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP (ACL-IJCNLP), pages 100?108, Suntec, Singapore.John Nerbonne and Wilbert Heeringa.
1997.
Measur-ing Dialect Distance Phonetically.
In Proc.
of the ACLSpecial Interest Group in Computational Phonology,pages 11?18, Madrid, Spain.Sara Noeman and Amgad Madkour.
2010.
LanguageIndependent Transliteration Mining System Using Fi-nite State Automata Framework.
In Proc.
of the 2010Named Entities Workshop (NEWS), pages 57?61, Up-psala, Sweden.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proc.
of the 40th An-nual Meeting on Association for Computational Lin-guistics (ACL), pages 311?318, Philadelphia, USA.Michael Paul, Hirofumi Yamamoto, Eiichiro Sumita, andSatoshi Nakamura.
2009.
On the Importance of PivotLanguage Selection for Statistical Machine Transla-tion.
In Proc.
of the North American Chapter of theAssociation for Computational Linguistics - HumanLanguage Technologies (NAACL HLT), pages 221?224, Boulder, USA.Michael Paul, Andrew Finch, and Eiichiro Sumita.2011.
Word Segmentation for Dialect Translation.LNCS Lectures Note in Computer Science, Springer,6609:55?67.Hassan Sawaf.
2010.
Arabic Dialect Handling in HybridMachine Translation.
In Proc.
of the 9th Conference ofthe Association for Machine Translation in the Ameri-cas (AMTA), Denver, USA.Yves Scherrer.
2007.
Adaptive String Distance Mea-sures for Bilingual Dialect Lexicon Induction.
In Proc.of the ACL Student Research Workshop, pages 55?60,Prague, Czech Republic.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proc.
of the International Con-ference on Spoken Language Processing (ICSLP), Vol-ume 2, pages 901?904, Denver, USA.Keita Tsuji and Kyo Kageura.
2006.
Automatic gen-eration of JapaneseEnglish bilingual thesauri basedon bilingual corpora.
J.
Am.
Soc.
Inf.
Sci.
Technol.,57:891?906.Masao Utiyama and Hitoshi Isahara.
2007.
A Compari-son of Pivot Methods for Phrase-Based Statistical Ma-chine Translation.
In Proc.
of Human Language Tech-nologies (HLT), pages 484?491, New York, USA.Hua Wu and Haifeng Wang.
2007.
Pivot Language Ap-proach for Phrase-Based Statistical Machine Transla-tion.
In Proc.
of the 45th Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages856?863, Prague, Czech Republic.Jia Xu, Jianfeng Gao, Kristina Toutanova, and HermannNey.
2008.
Bayesian semi-supervised Chinese wordsegmentation for Statistical Machine Translation.
InProc.
of the 22nd International Conference on Com-putational Linguistics (COLING), pages 1017?1024,Manchester, United Kingdom.9
