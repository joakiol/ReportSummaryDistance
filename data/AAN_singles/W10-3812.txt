Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 92?100,COLING 2010, Beijing, August 2010.A Discriminative Syntactic Model for Source Permutationvia Tree TransductionMaxim Khalilov and Khalil Sima?anInstitute for Logic, Language and ComputationUniversity of Amsterdam{m.khalilov,k.simaan}@uva.nlAbstractA major challenge in statistical machinetranslation is mitigating the word or-der differences between source and tar-get strings.
While reordering and lexicaltranslation choices are often conducted intandem, source string permutation priorto translation is attractive for studying re-ordering using hierarchical and syntacticstructure.
This work contributes an ap-proach for learning source string permu-tation via transfer of the source syntaxtree.
We present a novel discriminative,probabilistic tree transduction model, andcontribute a set of empirical upperboundson translation performance for English-to-Dutch source string permutation undersequence and parse tree constraints.
Fi-nally, the translation performance of ourlearning model is shown to outperform thestate-of-the-art phrase-based system sig-nificantly.1 IntroductionFrom its beginnings, statistical machine transla-tion (SMT) has faced a word reordering challengethat has a major impact on translation quality.While standard mechanisms embedded in phrase-based SMT systems, e.g.
(Och and Ney, 2004),deal efficiently with word reordering within a lim-ited window of words, they are still not expectedto handle all possible reorderings that involvewords beyond this relatively narrow window, e.g.,(Tillmann and Ney, 2003; Zens and Ney, 2003;Tillman, 2004).
More recent work handles wordorder differences between source and target lan-guages using hierarchical methods that draw onInversion Transduction Grammar (ITG), e.g., (Wuand Wong, 1998; Chiang, 2005).
In principle,the latter approach explores reordering defined bythe choice of swapping the order of sibling sub-trees under each node in a binary parse-tree of thesource/target sentence.An alternative approach aims at minimizing theneed for reordering during translation by permut-ing the source sentence as a pre-translation step,e.g., (Collins et al, 2005; Xia and McCord, 2004;Wang et al, 2007; Khalilov, 2009).
In effect,the translation process works with a model forsource permutation (s ?
s?)
followed by trans-lation model (s?
?
t), where s and t are sourceand target strings and s?
is the target-like permutedsource string.
In how far can source permutationreduce the need for reordering in conjunction withtranslation is an empirical question.In this paper we define source permutation asthe problem of learning how to transfer a givensource parse-tree into a parse-tree that minimizesthe divergence from target word-order.
We modelthe tree transfer ?s ?
?s?
as a sequence of local,independent transduction operations, each trans-forming the current intermediate tree ?s?i into thenext intermediate tree ?s?i+1 , with ?s0 = ?s and?s?n = ?s?
.
A transduction operation merely per-mutes the sequence of n > 1 children of a singlenode in an intermediate tree, i.e., unlike previouswork, we do not binarize the trees.
The numberof permutations is factorial in n, and learning asequence of transductions for explaining a sourcepermutation can be computationally rather chal-lenging (see (Tromble and Eisner, 2009)).
Yet,92from the limited perspective of source string per-mutation (s ?
s?
), another challenge is to inte-grate a figure of merit that measures in how far s?resembles a plausible target word-order.We contribute solutions to these challengingproblems.
Firstly, we learn the transductionoperations using a discriminative estimate ofP (pi(?x) |Nx, ?x, contextx), where Nx is the la-bel of node (address) x, Nx ?
?x is the context-free production under x, pi(?x) is a permutation of?x and contextx represents a surrounding syntac-tic context.
As a result, this constrains {pi(?x)}only to those found in the training data, and itconditions the transduction application probabil-ity on its specific contexts.
Secondly, in every se-quence s?0 = s, .
.
.
, s?n = s?
resulting from a treetransductions, we prefer those local transductionson ?s?i?1 that lead to source string permutation s?ithat are closer to target word order than s?i?1; weemploy s?
language model probability ratios as ameasure of word order improvement.In how far does the assumption of source per-mutation provide any window for improvementover a phrase-based translation system?
We con-duct experiments on translating from English intoDutch, two languages which are characterizedby a number of systematic divergences betweenthem.
Initially, we conduct oracle experimentswith varying constraints on source permutationto set upperbounds on performance relative toa state-of-the-art system.
Translating the oraclesource string permutation (obtained by untanglingthe crossing alignments) offers a large margin ofimprovement, whereas the oracle parse tree per-mutation provides a far smaller improvement.
Aminor change to the latter to also permute con-stituents that include words aligned with NULL,offers further improvement, yet lags bahind barestring permutation.
Subsequently, we presenttranslation results using our learning approach,and exhibit a significant improvement in BLEUscore over the state-of-the-art baseline system.Our analysis shows that syntactic structure canprovide important clues for reordering in trans-lation, especially for dealing with long distancecases found in, e.g., English and Dutch.
Yet, treetransduction by merely permuting the order of sis-ter subtrees might turn out insufficient.2 Baseline: Phrase-based SMTGiven a word-aligned parallel corpus, phrase-based systems (Och and Ney, 2002; Koehn et al,2003) work with (in principle) arbitrarily largephrase pairs (also called blocks) acquired fromword-aligned parallel data under a simple defi-nition of translational equivalence (Zens et al,2002).
The conditional probabilities of one phrasegiven its counterpart are interpolated log-linearlytogether with a set of other model estimates:e?I1 = argmaxeI1{ M?m=1?mhm(eI1, fJ1 )}(1)where a feature function hm refer to a systemmodel, and the corresponding ?m refers to the rel-ative weight given to this model.
A phrase-basedsystem employs feature functions for a phrase pairtranslation model, a language model, a reorderingmodel, and a model to score translation hypothesisaccording to length.
The weights ?m are usuallyoptimized for system performance (Och, 2003) asmeasured by BLEU (Papineni et al, 2002).
Tworeordering methods are widely used in phrase-based systems.Distance-based A simple distance-based re-ordering model default for Moses system is thefirst reordering technique under consideration.This model provides the decoder with a cost lin-ear to the distance between words that should bereordered.MSD A lexicalized block-oriented data-drivenreordering model (Tillman, 2004) considers threedifferent orientations: monotone (M), swap (S),and discontinuous (D).
The reordering probabili-ties are conditioned on the lexical context of eachphrase pair, and decoding works with a block se-quence generation process with the possibility ofswapping a pair of blocks.3 Related Work on Source PermutationThe integration of linguistic syntax into SMTsystems offers a potential solution to reorderingproblem.
For example, syntax is successfullyintegrated into hierarchical SMT (Zollmann and93Venugopal, 2006).
Similarly, the tree-to-stringsyntax-based transduction approach offers a com-plete translation framework (Galley et al, 2006).The idea of augmenting SMT by a reorderingstep prior to translation has often been shown toimprove translation quality.
Clause restructuringperformed with hand-crafted reordering rules forGerman-to-English and Chinese-to-English tasksare presented in (Collins et al, 2005) and (Wanget al, 2007), respectively.
In (Xia and McCord,2004; Khalilov, 2009) word reordering is ad-dressed by exploiting syntactic representations ofsource and target texts.Other reordering models operate provide thedecoder with multiple word orders.
For ex-ample, the MaxEnt reordering model describedin (Xiong et al, 2006) provides a hierarchi-cal phrasal reordering system integrated withina CKY-style decoder.
In (Galley and Manning,2008) the authors present an extension of the fa-mous MSD model (Tillman, 2004) able to handlelong-distance word-block permutations.
Comingup-to-date, in (PVS, 2010) an effective applicationof data mining techniques to syntax-driven sourcereordering for MT is presented.Recently, Tromble and Eisner (2009) definesource permutation as learning source permuta-tions; the model works with a preference matrixfor word pairs, expressing preference for their twoalternative orders, and a corresponding weightmatrix that is fit to the parallel data.
The hugespace of permutations is then structured using abinary synchronous context-free grammar (BinaryITG) with O(n3) parsing complexity, and the per-mutation score is calculated recursively over thetree at every node as the accumulation of therelative differences between the word-pair scorestaken from the preference matrix.
Application toGerman-to-English translation exhibits some per-formance improvement.Our work is in the general learning directiontaken in (Tromble and Eisner, 2009) but differsboth in defining the space of permutations, usinglocal probabilistic tree transductions, as well as inthe learning objective aiming at scoring permuta-tions based on a log-linear interpolation of a lo-cal syntax-based model with a global string-based(language) model.4 Pre-Translation Source PermutationGiven a word-aligned parallel corpus, we definethe source string permutation as the task of learn-ing to unfold the crossing alignments betweensentence pairs in the parallel corpus.
Let be givena source-target sentence pair s ?
t with wordalignment set a between their words.
Unfold-ing the crossing instances in a should lead to asmonotone an alignment a?
as possible between apermutation s?
of s and the target string t. Con-ducting such a ?monotonization?
on the parallelcorpus gives two parallel corpora: (1) a source-to-permutation parallel corpus (s ?
s?)
and(2) a source permutation-to-target parallel corpus(s?
?
t).
The latter corpus is word-aligned au-tomatically again and used for training a phrase-based translation system, while the former corpusis used for training our model for pre-translationsource permutation via parse tree transductions.Figure 1: Example of crossing alignments andlong-distance reordering using a source parse tree.In itself, the problem of permuting the sourcestring to unfold the crossing alignments is compu-tationally intractable (see (Tromble and Eisner,2009)).
However, different kinds of constraintscan be made on unfolding the crossing alignmentsin a.
A common approach in hierarchical SMT isto assume that the source string has a binary parsetree, and the set of eligible permutations is definedby binary ITG transductions on this tree.
This de-fines permutations that can be obtained only byat most inverting pairs of children under nodes ofthe source tree.
Figure 1 exhibits a long distancereordering of the verb in English-to-Dutch transla-tion: inverting the order of the children under theVP node would unfold the crossing alignment.944.1 Oracle PerformanceAs has been shown in the literature (Costa-jussa`and Fonollosa, 2006; Khalilov and Sima?an, 2010;Wang et al, 2007), source and target texts mono-tonization leads to a significant improvement interms of translation quality.
However it is notknown how many alignment crossings can be un-folded under different parse tree conditions.
In or-der to gauge the impact of corpus monotonizationon translation system performance, we trained aset of oracle translation systems, which createtarget sentences that follow the source languageword order using the word alignment links andvarious constraints.
(a) Word alignment.
(b) Parse tree and corre-sponding alignment.
(c) Word alignmentand ADJP span.Figure 2: Reordering example.The set-up of our experiments and corpus char-acteristics are detailed in Section 5.
Table 1 re-ports translation scores of the oracle systems.
No-tice that all the numbers are calculated on the re-aligned corpora.
Baseline results are provided forinformative purposes.String permutation The first oracle systemunder consideration is created by traversingthe string from left to right and unfolding allcrossing alignment links (we call this systemoracle-string).
For example in Figure 2(a),the oracle-string system generates a string ?doso gladly?
swapping the words ?do?
and?gladly?
without considering the parse tree.The first line of the table shows the performanceof the oracle-string system with monotone sourceand target portions of the corpus.Oracle under tree constraint We use a syntac-tic parser for parsing the English source sentencesthat provide n-ary constituency parses.
Now weconstrain unfolding crossing alignments only tothose alignment links which agree with the struc-ture of the source-side parse tree and consider theconstituents which include aligned tokens only.Unfolding a crossing alignment is modeled as per-muting the children of a node in the parse tree.
Werefer to this oracle system as oracle-tree.
For ex-ample provided in Figure 2(b), there is no way toconstruct a monotonized version of the sentencesince the word ?so?
is aligned to NULL and im-pedes swapping the order of VB and ADJP underthe VP.Oracle under relaxed tree constraint Theoracle-tree system does not permute the wordswhich are both (1) not found in the alignment and(2) are spanned by the sub-trees sibling to the re-ordering constituents.
Now we introduce a re-laxed version of the parse tree constraint: the or-der of the children of a node is permuted whenthe node covers the reordering constituents andalso when the frontier contains leaf nodes alignedwith NULL (oracle-span).
For example, in Fig-ure 2(c) the English word ?so?
is not aligned, butaccording to the relaxed version, must move to-gether with the word ?gladly?
since they sharea parent node (ADJP).Source BLEU NISTbaseline dist 24.04 6.29baseline MSD 24.04 6.28oracle?
string 27.02 6.51oracle?
tree 24.09 6.30oracle?
span 24.95 6.37Table 1: Translation scores of oracle systems.The main conclusion which can be drawn fromthe oracle results is that there is a possibility forrelatively big (?3 BLEU points) improvementwith complete unfolding of crossing alignmentsand very limited (?0.05 BLEU points) with thesame done under the parse tree constraint.
A tree-based system that allows for permuting unalignedwords that are covered by a dominating parentnode shows more improvement in terms of BLEUand NIST scores (?0.9 BLEU points).The gap between oracle-string and oracle-treeperformance is due to alignment crossings which95cannot be unfolded under trees (illustrated in Fig-ure 3), but possibly also due to parse and align-ment errors.Figure 3: Example of alignment crossing that doesnot agree with the parse tree.4.2 Source Permutation via SyntacticTransferGiven a parallel corpus with string pairs s ?
twith word alignment a, we create a source per-muted parallel corpus s ?
s?
by unfolding thecrossing alignments in a: this is done by scanningthe string s from left to right and moving words in-volved in crossing alignments to positions wherethe crossing alignments are unfolded).
The sourcestrings s are parsed, leading to a single parse tree?s per source string.Our model aims at learning from the sourcepermuted parallel corpus s ?
s?
a probabilisticoptimization argmaxpi(s) P (pi(s) | s, ?s).
We as-sume that the set of permutations {pi(s)} is de-fined through a finite set of local transductionsover the tree ?s.
Hence, we view the permutationsleading from s to s?
as a sequence of local treetransductions ?s?0 ?
.
.
.
?
?s?n , where s?0 = sand s?n = s?
, and each transduction ?s?i?1 ?
?s?iis defined using a tree transduction operation thatat most permutes the children of a single node in?s?i?1 as defined next.A local transduction ?s?i?1 ?
?s?i is modelledby an operation that applies to a single node withaddress x in ?s?i?1 , labeled Nx, and may permutethe ordered sequence of children ?x dominated bynode x.
This constitutes a direct generalization ofthe ITG binary inversion transduction operation.We assign a conditional probability to each suchlocal transduction:P (?s?i | ?s?i?1) ?
P (pi(?x) | Nx ?
?x, Cx) (2)where pi(?x) is a permutation of ?x (the orderedsequence of node labels under x) and Cx is a localtree context of node x in tree ?s?i?1 .
One wrin-kle in this definition is that the number of possi-ble permutations of ?x is factorial in the lengthof ?x.
Fortunately, the source permuted trainingdata exhibits only a fraction of possible permuta-tions even for longer ?x sequences.
Furthermore,by conditioning the probability on local context,the general applicability of the permutation is re-strained.Given this definition, we define the probabil-ity of the sequence of local tree transductions?s?0 ?
.
.
.
?
?s?n asP (?s?0 ?
.
.
.
?
?s?n) =n?i=1P (?s?i | ?s?i?1) (3)The problem of calculating the most likely per-mutation under this transduction model is madedifficult by the fact that different transduction se-quences may lead to the same permutation, whichdemands summing over these sequences.
Fur-thermore, because every local transduction condi-tions on local context of an intermediate tree, thisquickly risks becoming intractable (even when weuse packed forests).
In practice we take a prag-matic approach and greedily select at every inter-mediate point ?s?i?1 ?
?s?i the single most likelylocal transduction that can be conducted on anynode of the current intermediate tree ?s?i?1 usingan interpolation of the term in Equation 2 withstring probability ratios as follows:P (pi(?x) | Nx ?
?x, Cx)?P (s?i?1)P (s?i)The rationale behind this log-linear interpolationis that our source permutation approach aims atfinding the optimal permutation s?
of s that canserve as input for a subsequent translation model.Hence, we aim at tree transductions that are syn-tactically motivated that also lead to improvedstring permutation.
In this sense, the tree trans-duction definitions can be seen as an efficient and96syntactically informed way to define the space ofpossible permutations.We estimate the string probabilities P (s?i) us-ing 5-gram language models trained on the s?side of the source permuted parallel corpus s ?s?
.
We estimate the conditional probabilityP (pi(?x) | Nx ?
?x, Cx) using a Maximum-Entropy framework, where feature functions aredefined to capture the permutation as a class, thenode label Nx and its head POS tag, the childsequence ?x together with the corresponding se-quence of head POS tags and other features corre-sponding to different contextual information.We were particularly interested in those linguis-tic features that motivate reordering phenomenafrom the syntactic and linguistic perspective.
Thefeatures that were used for training the permuta-tion system are extracted for every internal nodeof the source tree that has more than one child:?
Local tree topology.
Sub-tree instances thatinclude parent node and the ordered se-quence of child node labels.?
Dependency features.
Features that deter-mine the POS tag of the head word of the cur-rent node, together with the sequence of POStags of the head words of its child nodes.?
Syntactic features.
Three binary featuresfrom this class describe: (1) whether the par-ent node is a child of the node annotated withthe same syntactic category, (2) whether theparent node is a descendant of the node an-notated with the same syntactic category, and(3) if the current subtree is embedded into a?SENT-SBAR?
sub-tree.
The latter feature in-tends to model the divergence in word orderin relative clauses between Dutch and En-glish which is illustrated in Figure 1.In initial experiments we piled up all feature func-tions into a single model.
Preliminary resultsshowed that the system performance increases ifthe set of patterns is split into partial classes con-ditioned on the current node label.
Hence, wetrained four separate MaxEnt models for the cate-gories with potentially high number of crossingalignments, namely VP, NP, SENT, and SBAR.For combinatory models we use the following no-tations: M4 = ?i?
[ NP, VP, SENT, SBAR] Mi and M2 =?i?
[VP, SENT] Mi.5 Experiments and resultsThe SMT system used in the experiments wasimplemented within the open-source MOSEStoolkit (Koehn et al, 2007).
Standard train-ing and weight tuning procedures which wereused to build our system are explained in detailson the MOSES web page1.
The MSD modelwas used together with a distance-based reorder-ing model.
Word alignment was estimated withGIZA++ tool2 (Och, 2003), coupled with mk-cls3 (Och, 1999), which allows for statistical wordclustering for better generalization.
An 5-gramtarget language model was estimated using theSRI LM toolkit (Stolcke, 2002) and smoothedwith modified Kneser-Ney discounting.
We usethe Stanford parser4 (Klein and Manning, 2003)as a source-side parsing engine.
The parser wastrained on the English treebank set provided with14 syntactic categories and 48 POS tags.
Theevaluation conditions were case-sensitive and in-cluded punctuation marks.
For Maximum En-tropy modeling we used the maxent toolkit5.Data The experiment results were obtained us-ing the English-Dutch corpus of the European Par-liament Plenary Session transcription (EuroParl).Training corpus statistics can be found in Table 2.Dutch EnglishSentences 1.2 M 1.2 MWords 32.9 M 33.0 MAverage sentence length 27.20 27.28Vocabulary 228 K 104 KTable 2: Basic statistics of the English-Dutch Eu-roParl training corpus.The development and test datasets were ran-domly chosen from the corpus and consisted of1http://www.statmt.org/moses/2code.google.com/p/giza-pp/3http://www.fjoch.com/mkcls.html4http://nlp.stanford.edu/software/lex-parser.shtml5http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html97500 and 1,000 sentences, respectively.
Both wereprovided with one reference translation.Results Evaluation of the system performanceis twofold.
In the first step, we analyze the qual-ity of reordering method itself.
In the next stepwe look at the automatic translation scores andevaluate the impact which the choice of reorder-ing strategy has on the translation quality.
Inboth stages of evaluation, the results are con-trasted with the performance shown by the stan-dard phrase-based SMT system (baseline) andwith oracle results.Source reordering analysis Table 3 shows theparameters of the reordered system allowing to as-sess the effectiveness of reordering permutations,namely: (1) a total number of crossings found inthe word alignment (#C), (2) the size of the re-sulting phrase table (PT), (3) BLEU, NIST, andWER scores obtained using monotonized parallelcorpus (oracle) as a reference.All the numbers are calculated on the re-alignedcorpora.
Calculations are done on the basis of the100,000 line extraction from the corpus6 and cor-responding alignment matrix.
The baseline rowsshow the number of alignment crossings found inthe original (unmonotonized) corpus.System #C PT ScoresBLEU NIST WEROraclestring 54.6K 48.4M - - -tree 187.3K 30.3M 71.73 17.01 16.77span 146.9K 33.0M 73.41 17.11 15.73Baselinesbaselines 187.0K 29.8M 71.70 17.07 16.55Category modelsMNP 188.9K 29.7M 71.63 17.07 16.52MV P 168.1K 29.8M 73.17 17.16 15.99MSENT 171.0K 29.8M 73.08 17.08 16.10MSBAR 188.6K 29.8M 72.89 16.90 16.41Combinatory modelsM4 193.2K 29.1M 70.98 16.85 16.78M2 165.4K 29.9M 73.07 16.92 15.88Table 3: Main parameters of the tree-based re-ordering system.6A smaller portion of the corpus is used for analysis inorder to reduce evaluation time.Translation scores The evaluation results forthe development and test corpora are reported inTable 4.
They include two baseline configurations(dist and MSD), oracle results and contrasts themwith the performance shown by different combi-nations of single-category tree-based reorderingmodels.
Best scores within each experimental sec-tion are placed in cells filled with grey.System Dev TestBLEU BLEU NISTbaseline dist 23.88 24.04 6.29baseline MSD 24.07 24.04 6.28oracle-string 26.28 27.02 6.50oracle-tree 23.84 24.09 6.30oracle-span 24.79 24.95 6.35MNP 23.79 23.81 6.27MV P 24.16 24.55 6.29MSENT 24.27 24.56 6.32MSBAR 23.99 24.12 6.27M4 23.50 23.86 6.29M2 24.28 24.64 6.33Table 4: Experimental results.Analysis The number of crossingsfound in word alignment intersection andBLEU/NIST/WER scores estimated on reordereddata vs. monotonized data report the reorderingalgorithm effectiveness.
A big gap between num-ber of crossings and total number of reorderingsper corpus found in oracle-string system7 andbaseline systems demonstrates the possible reduc-tion of system?s non-monotonicity.
The differencein number of crossings and BLEU/NIST/WERscores between the oracle-span and the bestperforming MaxEnt models (namely, M2) showsthe level of performance of the prediction module.A number of distinct phrase translation pairs inthe translation table implicitly reveals the general-ization capabilities of the translation system sinceit simplifies the translation task.
From the otherhand, increased number of shorter phrases can addnoise in the reordered data and makes decodingmore complex.
Hence, the size of phrase table it-self can not be considered as a robust indicator ofits translation potential.7The number of crossings for oracle configuration is notzero since this parameter is calculated on the re-aligned cor-pus.98Table 4 shows that three of six MaxEnt re-ordering systems outperform baseline systems byabout 0.5-0.6 BLEU points, that is statisticallysignificant8.
The combination of NP, NP, SENT,and SBAR models do not show good performancepossibly due to increased sparseness of reorder-ing patterns.
However, the system that consideronly the MV P and MSENT models achieves 0.62BLEU score gain over the baseline configurations.The main conclusion which can be drawn fromanalysis of Tables 3 and 4 is that there is anevident correlation between characteristics of re-ordering system and performance demonstratedby the translation system trained on the corpuswith reordered source part.Example Figure 4 exemplifies the sentencesthat presumably benefits from the monotonizationof the source part of the parallel corpus.
The ex-ample demonstrates a pervading syntactic distinc-tion between English and Dutch: the reordering ofverb-phrase subconstituents VP NP PP within therelative clause into PP NP VP.6 Conclusions and future workWe introduced a tree-based reordering model thataims at monotonizing the word order of source8All statistical significance calculations are done for a95% confidence interval and 1 000 resamples, following theguidelines from (Koehn, 2004).and target languages as a pre-translation step.
Ourmodel avoids complete generalization of reorder-ing instances by using tree contexts and limit-ing the permutations to data instances.
From alearning perspective, our work shows that navigat-ing a large space of intermediate tree transforma-tions can be conducted effectively using both thesource-side syntactic tree and a language modelof the idealized (target-like) source-permuted lan-guage.We have shown the potential for translationquality improvement when target sentences arecreated following the source language word or-der (?3 BLEU points over the standard phrase-based SMT) and under parse tree constraint (?0.9BLEU points).
As can be seen from these re-sults, our model exhibits competitive translationperformance scores compared with the standarddistance-based and lexical reordering.The gap between the oracle and our system?sresults leaves room for improvement.
We intendto study extensions of the current tree transfermodel to narrow this performance gap.
As a firststep we are combining isolated models for con-crete syntactic categories and aggregating morefeatures into the MaxEnt model.
Algorithmic im-provements, such as beam-search and chart pars-ing, could allow us to apply our method to fullparse-forests as opposed to a single parse tree.
(a) Original parse tree.
(b) Reordered parse tree.Src: that ... to lead the Commission during the next five-year termRef.
: dat ... om de komende vijf jaar de Commissie te leidenBaseline MSD: dat ... om het voortouw te nemen in de Commissie tijdens de komende vijf jaarRrd src: that ... during the next five-year term the Commission to leadM2 : dat ... om de Commissie tijdens de komende vijf jaar te leiden(c) Translations.Figure 4: Example of tree-based monotonization.99ReferencesD.
Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedingsof ACL?05, pages 263?270.M.
Collins, P. Koehn, and I. Kuc?erova?.
2005.
Clauserestructuring for statistical machine translation.
InProceedings of ACL?05, pages 531?540.M.
R. Costa-jussa` and J.
A. R. Fonollosa.
2006.Statistical machine reordering.
In Proceedings ofHLT/EMNLP?06, pages 70?76.M.
Galley and Ch.
D. Manning.
2008.
A simple andeffective hierarchical phrase reordering model.
InProceedings of EMNLP?08, pages 848?856.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. De-Neefe, W. Wang, and I. Thaye.
2006.
Scalable in-ference and training of context-rich syntactic trans-lation models.
In Proc.
of COLING/ACL?06, pages961?968.M.
Khalilov and K. Sima?an.
2010.
Source reorderingusing maxent classifiers and supertags.
In Proc.
ofEAMT?10, pages 292?299.M.
Khalilov.
2009.
New statistical and syntactic mod-els for machine translation.
Ph.D. thesis, Universi-tat Polite`cnica de Catalunya, October.D.
Klein and C. Manning.
2003.
Accurate unlexi-calized parsing.
In Proceedings of the 41st AnnualMeeting of the ACL?03, pages 423?430.Ph.
Koehn, F. Och, and D. Marcu.
2003.
Statisticalphrase-based machine translation.
In Proceedingsof the HLT-NAACL 2003, pages 48?54.Ph.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Con-stantin, and E. Herbst.
2007.
Moses: open-sourcetoolkit for statistical machine translation.
In Pro-ceedings of ACL 2007, pages 177?180.Ph.
Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP?04, pages 388?395.F.
Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical ma-chine translation.
In Proceedings of ACL?02, pages295?302.F.
Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Compu-tational Linguistics, 30(4):417?449.F.
Och.
1999.
An efficient method for determin-ing bilingual word classes.
In Proceedings of ACL1999, pages 71?76.F.
Och.
2003.
Minimum error rate training in statisti-cal machine translation.
In Proceedings of ACL?03,pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL?02, pages 311?318.A.
PVS.
2010.
A data mining approach tolearn reorder rules for SMT.
In Proceedings ofNAACL/HLT?10, pages 52?57.A.
Stolcke.
2002.
SRILM: an extensible languagemodeling toolkit.
In Proceedings of SLP?02, pages901?904.C.
Tillman.
2004.
A unigram orientation model forstatistical machine translation.
In Proceedings ofHLT-NAACL?04, pages 101?104.C.
Tillmann and H. Ney.
2003.
Word reordering anda dynamic programming beam search algorithm forstatistical machine translation.
Computational Lin-guistics, 1(29):93?133.R.
Tromble and J. Eisner.
2009.
Learning linear order-ing problems for better translation.
In Proceedingsof EMNLP?09, pages 1007?1016.C.
Wang, M. Collins, and Ph.
Koehn.
2007.
Chinesesyntactic reordering for statistical machine transla-tion.
In Proceedings of EMNLP-CoNLL?07, pages737?745.D.
Wu and H. Wong.
1998.
Machine translation wih astochastic grammatical channel.
In Proceedings ofACL-COLING?98, pages 1408?1415.F.
Xia and M. McCord.
2004.
Improving a statisticalMT system with automatically learned rewrite pat-terns.
In Proceedings of COLING?04, pages 508?514.D.
Xiong, Q. Liu, and S. Lin.
2006.
Maximum en-tropy based phrase reordering model for statisticalmachine translation.
In Proceedings of ACL?06,pages 521?528.R.
Zens and H. Ney.
2003.
A comparative study onreordering constraints in statistical machine transla-tion.
In Proceedings of ACL?03, pages 144?151.R.
Zens, F. Och, and H. Ney.
2002.
Phrase-based sta-tistical machine translation.
In Proceedings of KI:Advances in Artificial Intelligence, pages 18?32.A.
Zollmann and A. Venugopal.
2006.
Syntax aug-mented machine translation via chart parsing.
InProceedings of NAACL?06, pages 138?141.100
