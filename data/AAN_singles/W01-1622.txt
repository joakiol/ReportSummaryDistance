Adding extra input/output modalities to a spoken dialogue systemJanienke STURM, Fusi WANG, Bert CRANENA2RT, Dept.
Language and Speech, Nijmegen UniversityErasmusplein 16525 HT Nijmegen, The Netherlands{janienke.sturm | f.wang | b.cranen}@let.kun.nlAbstractThis paper describes a prototype of a multi-modal railway information system that wasbuilt by extending an existing speech-onlysystem.
The purpose of the extensions is toalleviate a number of shortcomings ofspeech-only interfaces.1IntroductionFor a long time, speech has been the only mo-dality for input and output in telephone-based in-formation systems.
Speech is often considered tobe the most natural form of input for such sys-tems, since people have always used speech asthe primary means of communication.
More-over, to use a speech-only system a simple tele-phone suffices and no additional devices are re-quired.
Obviously, in situations where bothhands and eyes are busy, speech is definitelypreferable over other modalities like pen/mouse.However, speech-only interfaces have alsoshown a number of shortcomings that result inless effective and less efficient dialogues.The aim of the research described in this paper isto assess the extent to which multimodal in-put/output can help to improve effectiveness, ef-ficiency and user satisfaction of informationsystems in comparison with unimodal systems.This paper describes how, within the frameworkof the MATIS1 (Multimodal Access to Transac-tion and Information Services) project we devel-oped a prototype of a multimodal railway infor-mation system by extending a speech-only ver-sion in such a way that it supports screen outputand point-and-click actions of the user as input.This system is a typical example of a simple ap-plication that can be implemented using a slot-filling paradigm and may stand model for vari-ous other form filling applications.First, a number of problems are described thatarise in speech-only interfaces.
Then we brieflydescribe the architecture of the speech-only1http://www.ipo.tue.nl/projects/matis/railway information system.
Next, we describein more detail how we added multimodality tothis version of the system and explain why wethink this may help to solve the shortcomings ofspeech-only systems.
We conclude this paper bydiscussing several open issues that we intend tosolve by means of user tests with the multimodalsystem.2 Shortcomings of speech-only interfacesOne of the issues that all dialogue systems withspoken input have to cope with is the imperfec-tion of the speech recogniser.
Even in very lim-ited domains and with a small vocabulary speechrecognition is never 100% accurate, if only be-cause people may use OoD (Out of Domain) orOoV (Out of Vocabulary) words.
To ensure thatthe user does not end up with wrong informa-tion, all slot values entered by the user must beconfirmed.
This can be done either explicitly ina separate question or implicitly, i.e.
incorpo-rated in the next question.
Explicit confirmationresults in a lot of extra turns, which means thatthe dialogue becomes less efficient and is oftenperceived as tedious, especially if all user utter-ances are understood correctly.
Implicit confir-mation, by contrast, does not necessarily in-crease the number of turns.
However, it appearsthat users have difficulty in grasping the conceptof implicit confirmation [Sturm, 1999].
Thingsrun smoothly as long as the information to beconfirmed is correct.
If the speech recognitionresult is incorrect and wrong input expressionsare confirmed implicitly, users tend to get con-fused and fail to repair the mistake that wasmade by the speech recogniser.In order to reduce the need for confirmation,confidence measures may be used.
A confidencescore is an estimate of how certain one can bethat the recognition result is indeed correct.
Us-ing confidence scores in combination with oneor more thresholds, would for instance allow todecide upon 1) ignoring the recognition result (ifthe confidence is minimal), 2) confirming therecognition result or 3) accepting the recognitionresult without confirmation (if the confidence ismaximal).
Unfortunately, it is virtually impossi-ble to define thresholds in such a way that nofalse accepts (a user utterance is actually mis-recognised but has a confidence score that ex-ceeds the threshold) and no false rejects (userinput was recognised correctly but has a confi-dence score that falls below the threshold) arecaused.
False rejects are not very harmful, al-though they do cause superfluous confirmationquestions, and thus reduce the efficiency of thedialogue.
False accepts, however, may becomedisastrous for the dialogue, since they cause in-correct values to be accepted without any con-firmation.
As a consequence, this strategy doesnot seem very attractive for speech-only sys-tems.Another problem with speech-only informationsystems is the way in which the eventual infor-mation is presented to the user.
Shadowing ex-periments with different railway informationsystems indicate that users have difficulties un-derstanding and writing down a travel advicepresented in spoken form, especially if one ormore transfers are involved [Claassen, 2000].Last, and perhaps foremost, it appears that usershave difficulty in building a correct mentalmodel of the functionality and the status of aspeech-only system.
This lack of understandingexplains problems with exceptions handling, andthe user?s uncertainty as to what one can (orperhaps must) say at any given moment.3 Multimodality in MATISThe first goal of the MATIS project is to inves-tigate to what extent graphical output along withspeech prompts can solve the problems that aredue to the lack of a consistent mental model.
If,for example, recognition results are not onlyconfirmed (implicitly) in speech prompts for ad-ditional input, but also displayed in the corre-sponding field on the screen, detecting recogni-tion errors may become easier.
The same shouldhold for navigation through the list of possibleconnections that is returned after the input iscomplete and a database query can be per-formed.If no keyboard is available speech is ideal formaking selections from long implicit lists, suchas the departure city.
However, other fields in aform may offer only a small number of options,which can easily be displayed on a screen.
In therailway information system this holds for theswitch that identifies the time as departure or ar-rival time (and to a large extent also for enteringthe date, which usually is today or tomorrow).Selections from short lists are most easily madeby means of point-and-click operations.
There-fore, we decided to add this input mode tospeech input.3.1  System OverviewOur multimodal railway information system isan extended version of the mixed-initiativespeech-only railway information system (OVIS)developed in the NWO-TST programme2.
This isa very different starting point from most otherprojects in multimodal human-machine interac-tion, that seem to add speech to what is basicallya user-driven desktop application.
The user in-terface consists of a telephone handset in combi-nation with a screen and a mouse.
The MATISsystem inherited an architecture in which mod-ules communicate with each other using TCPsocket connections under the control of a centralmodule (Phrisco) (cf.
Figure 1).
The grey shadedmodules have been added or extended forMATIS.PhriscoAutel ServerSpeechRecognitionConfidenceCalculationNLP DatabaseMulti-modalInput EventHandlerDialogueManagerMulti-modalOutput EventHandlerNaturalLanguageGenerationSpeechSynthesisScreenInput   OutputScreen outputSpeech outputSpeech inputPoint & click inputFigure 1 Overview of the MATIS systemIn the next sections we will focus on the mod-ules that have been added or changed and howthese modules help to solve some of the prob-lems described in Section 2.2http://odur.let.rug.nl:4321/3.2 Screen outputAt the start of a dialogue an empty form isshown on the screen.
In the course of the dia-logue the fields are filled with the values pro-vided by the user, who can use speech to fill allfive slots in the form in a mixed-initiative dia-logue, or use the mouse to select text fields andto make list selections.
Once all slots have beenfilled, a travel advice is retrieved from the data-base and presented to the user in spoken and intextual form.3.3 Mouse inputExperiments have been conducted using a Wiz-ard of Oz simulation of the MATIS system, toestablish to what extent subjects use the mousein addition to speech and in what way mouse in-put is used in an interaction that is essentially theoriginal mixed-initiative spoken dialogue[Terken, 2001].
It appeared that half of the sub-jects used mouse input as well as speech inputand that mouse input was primarily used tomake selections from short lists, and much lessto select editable text fields.
The latter was donemostly in the context of error correction.3.4 Confidence calculationConfidence measures (CM) for spoken input canbe calculated in different ways.
In the MATISsystem the CM is based on an N-best list ofsentence hypotheses that is generated by thespeech recogniser [R?ber, 1997].
This N-bestconfidence score rests on the assumption thatwords that occur in more entries in the N-bestlist are more likely to be correct:where P(hi) is the likelihood score of sentencehypothesis i in the N-best list.
In this manner aCM is calculated for each word in the utterance.The N-best CM may give rise to a specificproblem: if the N-best list contains only one en-try, (1) automatically yields a maximum confi-dence score for each word in the utterance.
Off-line experiments have shown that 3% of all N-best lists consisting of only one sentence actu-ally contained recognition errors.
Consequently,even if we only trust words with a maximumCM score, the false accept rate will be at least3%.
Other off-line experiments have shown thatsome improvement may be expected from com-bining the N-best CM with another CM thatdoes not have this artefact.When a user fills a specific slot in the form us-ing speech (s)he has to indicate which slot needsto be filled and provide a value for this slot.
Toobtain a CM for the slot value, the CMs of allwords that were used to specify this value haveto be combined.
In the current implementationthis was done by taking their mean.3.5 Multimodal Input Event HandlerThe information coming from the NLP module(in response to a spoken prompt) and from themouse (that is active all the time) must be prop-erly combined.
This task is taken care of by themultimodal input event handler.
To combine theinformation streams correctly, a time stamp mustbe attached to the inputs, indicating the temporalinterval in which the action took place.
This timeinterval is needed to decide which events shouldbe combined [Oviatt, 1997].Furthermore, speech and mouse input may con-tain complementary, redundant or unrelated in-formation.
Complementary information (e.g.clicking on the ?destination?
field and saying?Rotterdam?)
is unified before it is sent to thedialogue manager.
Unrelated information (e.g.clicking to select departure time while sayingone or more station names) is first merged andthen sent to the dialogue manager.
In the case ofredundant information (e.g.
clicking on ?tomor-row?
while saying ?tomorrow?
), the informationcoming from the mouse is used to adapt the CMscore attached to the speech input.
Due to speechrecognition errors, ?redundant?
information maybe conflicting (if the recogniser returns ?tomor-row?
in the same time slot where ?today?
isclicked).
To solve this problem the informationwith the highest CM score will be trusted.3.6 Dialogue managementThe dialogue manager of the unimodal systemwas adapted in order to be able to use the CMsto decide on the confirmation strategy.
In thepresent prototype we use only one threshold todecide upon the strategy.
Values with a CMscore below the threshold are shown on thescreen and confirmed explicitly in the spokendialogue.
Values with a CM score exceeding thethreshold are only shown on the screen.
In caseall or most values have a high CM score, thisstrategy speeds up the dialogue considerably.Preliminary experiments suggest that providingfeedback visually as well as orally helps the user?
?=?== NiiNhWiinbesthPhPCM i1:1)()((1)to develop an adequate model of the system.Also, since the user knows exactly what the in-formation status of the system is at each point inthe dialogue, correcting errors should be easier,which in turn will result in more effective dia-logues.
We are convinced that an increase in ef-fectiveness and efficiency can be achieved, es-pecially if the visual output is combined withauditory prompts that are more concise than inthe speech-only system.3.7 Multimodal Output Event HandlerIn a multimodal system a decision has to bemade as to whether the feedback to the usermust be presented orally, visually, or in bothways.
This is the task of the multimodal outputevent handler.
For the time being we have de-cided to send all the output from the dialoguemanager to the natural language generationmodule and the screen.4 Discussion and conclusionsIn this paper we have described the architectureof a multimodal train timetable informationsystem that was built by extending a speech-onlyversion.
Most of the desired functionality of themodules that we added or changed was specifiedon the basis of off-line experiments and findingsin the literature.
The system is now ready to betested by real users.Adding visual feedback has been shown to helpin several respects.
In Terken (2001) it wasshown that the visual feedback helps the user tobuild a mental model of the task at hand.
Fur-thermore, we argued that visual feedback maybe interpreted as a form of implicit verification,which helps the user to detect recognition errors.This allows to apply confidence thresholds toavoid confirmation turns, even if a number offalse accepts occur.
This is in contrast withspeech-only systems, where false accepts willremain unnoticed.User tests with our present prototype are neededto verify whether the additional modalities doindeed help to increase efficiency, effectivenessand user satisfaction.
These tests will be con-ducted in the near future.
In the current proto-type a number of ad hoc choices were made.
Weexpect that several of these choices will have tobe revised based on the outcomes of the tests.CM scores that are calculated for individualwords must be transformed into scores forslot/value pairs.
This can be done in severalways: by taking the mean score, the maximumscore, weighting the scores for values and slots,etc.
In the current prototype we take the mean ofthe scores of the words that yielded a certainslot/value pair, but more sophisticated methodsmay be needed.In principle it is possible to go beyond the cur-rent design and give feedback on the status ofthe slots (confirmed or not, changeable or not) inaddition to showing their values.
This mightprevent the user from getting lost in the dia-logue.
However, it is not yet clear whether addi-tional visual attributes can be designed that areself-explanatory and will not confuse the user.
Itmight be useful to enable the user to correct in-formation by clicking the field that contains in-correct information and saying the correct in-formation.
Also, showing a list of alternativerecognition hypotheses from which the user canselect the correct one, might help.
In the currentsystem we have not implemented this option.Currently, the complete output of the dialoguemanager is sent both to the speech output mod-ule and the screen.
Informal tests have shownthat the speech output designed for a speech-only system is much too verbose.
Especially theoral presentation of the travel advice can be ashort summary, e.g.
consisting of only the de-parture and arrival times, when the complete ad-vice is also presented on the screen.5 AcknowledgementThis work was supported by a grant from Sen-ter/EZ in the framework of IOP-MMI.6 ReferencesB.
R?ber (1997), Obtaining confidence measuresfrom sentence probabilities, Proceedings Euro-speech?97, pp.
739-742.W.
Claassen (2000), Using recall measurements andsubjective ratings to assess the usability of railroadtravel plans presented by telephone, Technical Re-port #123, NWO Priority Programme on Languageand Speech Technology.S.
Oviatt, A. DeAngeli, and K. Kuhn (1997), Inte-gration and synchronization of input modes duringmultimodal human-computer interaction, in Pro-ceedings of CHI '97, pp.
415-422.J.
Sturm, E. den Os and L. Boves (1999), Issues inspoken dialogue systems: Experiences with theDutch ARISE system, Proceedings ESCA Work-shop on Interactive Dialogue in Multimodal Sys-tems, pp.
1-4.J.
Terken and S. te Riele (2001), Supporting the con-struction of a user model in speech-only interfacesby adding multimodality, Submitted to Eurospeech2001.
