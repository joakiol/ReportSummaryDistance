Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 500?510,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsSingle-Agent vs. Multi-Agent Techniques for ConcurrentReinforcement Learning of Negotiation Dialogue PoliciesKallirroi Georgila, Claire Nelson, David TraumUniversity of Southern California Institute for Creative Technologies12015 Waterfront Drive, Playa Vista, CA 90094, USA{kgeorgila,traum}@ict.usc.eduAbstractWe use single-agent and multi-agent Rein-forcement Learning (RL) for learning dia-logue policies in a resource allocation ne-gotiation scenario.
Two agents learn con-currently by interacting with each otherwithout any need for simulated users(SUs) to train against or corpora to learnfrom.
In particular, we compare the Q-learning, Policy Hill-Climbing (PHC) andWin or Learn Fast Policy Hill-Climbing(PHC-WoLF) algorithms, varying the sce-nario complexity (state space size), thenumber of training episodes, the learningrate, and the exploration rate.
Our re-sults show that generally Q-learning failsto converge whereas PHC and PHC-WoLFalways converge and perform similarly.We also show that very high graduallydecreasing exploration rates are requiredfor convergence.
We conclude that multi-agent RL of dialogue policies is a promis-ing alternative to using single-agent RLand SUs or learning directly from corpora.1 IntroductionThe dialogue policy of a dialogue system decideson which actions the system should perform givena particular dialogue state (i.e., dialogue context).Building a dialogue policy can be a challengingtask especially for complex applications.
For thisreason, recently much attention has been drawnto machine learning approaches to dialogue man-agement and in particular Reinforcement Learning(RL) of dialogue policies (Williams and Young,2007; Rieser et al, 2011; Jur?c??
?cek et al, 2012).Typically there are three main approaches tothe problem of learning dialogue policies usingRL: (1) learn against a simulated user (SU), i.e.,a model that simulates the behavior of a real user(Georgila et al, 2006; Schatzmann et al, 2006);(2) learn directly from a corpus (Henderson et al,2008; Li et al, 2009); or (3) learn via live interac-tion with human users (Singh et al, 2002; Ga?si?c etal., 2011; Ga?si?c et al, 2013).We propose a fourth approach: concurrentlearning of the system policy and the SU policyusing multi-agent RL techniques.
Both agents aretrained simultaneously and there is no need forbuilding a SU separately or having access to a cor-pus.1As we discuss below, concurrent learningcould potentially be used for learning via live in-teraction with human users.
Moreover, for negoti-ation in particular there is one more reason in fa-vor of concurrent learning as opposed to learningagainst a SU.
Unlike slot-filling domains, in nego-tiation the behaviors of the system and the user aresymmetric.
They are both negotiators, thus build-ing a good SU is as difficult as building a goodsystem policy.So far research on using RL for dialogue pol-icy learning has focused on single-agent RL tech-niques.
Single-agent RL methods make the as-sumption that the system learns by interacting witha stationary environment, i.e., an environment thatdoes not change over time.
Here the environ-ment is the user.
Generally the assumption thatusers do not significantly change their behaviorover time holds for simple information providingtasks (e.g., reserving a flight).
But this is not nec-essarily the case for other genres of dialogue, in-cluding negotiation.
Imagine a situation where anegotiator is so uncooperative and arrogant thatthe other negotiators decide to completely changetheir negotiation strategy in order to punish her.Therefore it is important to investigate RL ap-proaches that do not make such assumptions aboutthe user/environment.1Though corpora or SUs may still be useful for bootstrap-ping the policies and encoding real user behavior (see sec-tion 6).500Multi-agent RL is designed to work for non-stationary environments.
In this case the envi-ronment of a learning agent is one or more otheragents that can also be learning at the same time.Therefore, unlike single-agent RL, multi-agent RLcan handle changes in user behavior or in the be-havior of other agents participating in the inter-action, and thus potentially lead to more realis-tic dialogue policies in complex dialogue scenar-ios.
This ability of multi-agent RL can also haveimportant implications for learning via live inter-action with human users.
Imagine a system thatlearns to change its strategy as it realizes that aparticular user is no longer a novice user, or that auser no longer cares about five star restaurants.We apply multi-agent RL to a resource alloca-tion negotiation scenario.
Two agents with dif-ferent preferences negotiate about how to shareresources.
We compare Q-learning (a single-agent RL algorithm) with two multi-agent RL al-gorithms: Policy Hill-Climbing (PHC) and Winor Learn Fast Policy Hill-Climbing (PHC-WoLF)(Bowling and Veloso, 2002).
We vary the scenariocomplexity (i.e., the quantity of resources to beshared and consequently the state space size), thenumber of training episodes, the learning rate, andthe exploration rate.Our research contributions are as follows: (1)we propose concurrent learning using multi-agentRL as a way to deal with some of the issues of cur-rent approaches to dialogue policy learning (i.e.,the need for SUs and corpora), which may alsopotentially prove useful for learning via live inter-action with human users; (2) we show that concur-rent learning can address changes in user behav-ior over time, and requires multi-agent RL tech-niques and variable exploration rates; (3) to ourknowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies; (4)for the first time, the above techniques are appliedto a negotiation domain; and (5) this is the firststudy that compares Q-learning, PHC, and PHC-WoLF in such a variety of situations (varying alarge number of parameters).The paper is structured as follows.
Section 2presents related work.
Section 3 provides a briefintroduction to single-agent RL and multi-agentRL.
Section 4 describes our negotiation domainand experimental setup.
In section 5 we presentour results.
Finally, section 6 concludes and pro-vides some ideas for future work.2 Related WorkMost research in RL for dialogue management hasbeen done in the framework of slot-filling applica-tions such as restaurant recommendations (Lemonet al, 2006; Thomson and Young, 2010; Ga?si?cet al, 2012; Daubigney et al, 2012), flight reser-vations (Henderson et al, 2008), sightseeing rec-ommendations (Misu et al, 2010), appointmentscheduling (Georgila et al, 2010), etc.
RL hasalso been applied to question-answering (Misu etal., 2012), tutoring domains (Tetreault and Litman,2008; Chi et al, 2011), and learning negotiationdialogue policies (Heeman, 2009; Georgila andTraum, 2011; Georgila, 2013).As mentioned in section 1, there are three mainapproaches to the problem of learning dialoguepolicies using RL.In the first approach, a SU is hand-crafted orlearned from a small corpus of human-human orhuman-machine dialogues.
Then the dialogue pol-icy can be learned by having the system interactwith the SU for a large number of dialogues (usu-ally thousands of dialogues).
Depending on theapplication, building a realistic SU can be just asdifficult as building a good dialogue policy.
Fur-thermore, it is not clear what constitutes a goodSU for dialogue policy learning.
Should the SUresemble real user behavior as closely as possi-ble, or should it exhibit some degree of random-ness to explore a variety of interaction patterns?Despite much research on the issue, these are stillopen questions (Schatzmann et al, 2006; Ai andLitman, 2008; Pietquin and Hastie, 2013).In the second approach, no SUs are required.Instead the dialogue policy is learned directly froma corpus of human-human or human-machine dia-logues.
For example, Henderson et al (2008) useda combination of RL and supervised learning tolearn a dialogue policy in a flight reservation do-main, whereas Li et al (2009) used Least-SquaresPolicy Iteration (Lagoudakis and Parr, 2003), anRL-based technique that can learn directly fromcorpora, in a voice dialer application.
However,collecting such corpora is not trivial, especially innew domains.
Typically, data are collected in aWizard-of-Oz setup where human users think thatthey interact with a system while in fact they inter-act with a human pretending to be the system, orby having human users interact with a preliminaryversion of the dialogue system.
In both cases theresulting interactions are expected to be quite dif-501ferent from the interactions of human users withthe final system.
In practice this means that dia-logue policies learned from such data could be farfrom optimal.The first experiment on learning via live inter-action with human users (third approach) was re-ported by Singh et al (2002).
They used RL tohelp the system with two choices: how much ini-tiative it should allow the user, and whether or notto confirm information provided by the user.
Re-cently, learning of ?full?
dialogue policies (not justchoices at specific points in the dialogue) via liveinteraction with human users has become possi-ble with the use of Gaussian processes (Engel etal., 2005; Rasmussen and Williams, 2006).
Typi-cally learning a dialogue policy is a slow processrequiring thousands of dialogues, hence the needfor SUs.
Gaussian processes have been shown tospeed up learning.
This fact together with easyaccess to a large number of human users throughcrowd-sourcing has allowed dialogue policy learn-ing via live interaction with human users (Ga?si?c etal., 2011; Ga?si?c et al, 2013).Space constraints prevent us from providing anexhaustive list of previous work on using RL fordialogue management.
Thus below we focus onlyon research that is directly related to our work,specifically research on concurrent learning of thepolicies of multiple agents, and the application ofRL to negotiation domains.So far research on RL in the dialogue commu-nity has focused on using single-agent RL tech-niques where the stationary environment is theuser.
Most approaches assume that the user goalis fixed and that the behavior of the user is ratio-nal.
Other approaches account for changes in usergoals (Ma, 2013).
In either case, one can build auser simulation model that is the average of dif-ferent user behaviors or learn a policy from a cor-pus that contains a variety of interaction patterns,and thus safely assume that single-agent RL tech-niques will work.
However, in the latter case ifthe behavior of the user changes significantly overtime then the assumption that the environment isstationary will no longer hold.There has been a lot of research on multi-agentRL in the optimal control and robotics communi-ties (Littman, 1994; Hu andWellman, 1998; Buso-niu et al, 2008).
Here two or more agents learn si-multaneously.
Thus the environment of an agent isone or more other agents that continuously changetheir behavior because they are also learning at thesame time.
Therefore the environment is no longerstationary and single-agent RL techniques do notwork well or do not work at all.
We are particu-larly interested in the work of Bowling and Veloso(2002) who proposed the PHC and PHC-WoLF al-gorithms that we use in this paper.
We chose thesetwo algorithms because, unlike other multi-agentRL methods (Littman, 1994; Hu and Wellman,1998), they do not make assumptions that do notalways hold and do not require quadratic or linearprogramming that does not always scale.English and Heeman (2005) were the first in thedialogue community to explore the idea of con-current learning of dialogue policies.
However,English and Heeman (2005) did not use multi-agent RL but only standard single-agent RL, inparticular an on-policy Monte Carlo method (Sut-ton and Barto, 1998).
But single-agent RL tech-niques are not well suited for concurrent learningwhere each agent is trained against a continuouslychanging environment.
Indeed, English and Hee-man (2005) reported problems with convergence.Chandramohan et al (2012) proposed a frame-work for co-adaptation of the dialogue policy andthe SU using single-agent RL.
They applied In-verse Reinforcement Learning (IRL) (Abbeel andNg, 2004) to a corpus in order to learn the rewardfunctions of both the system and the SU.
Further-more, Cuay?ahuitl and Dethlefs (2012) used hier-archical multi-agent RL for co-ordinating the ver-bal and non-verbal actions of a robot.
Cuay?ahuitland Dethlefs (2012) did not use PHC or PHC-WoLF and did not compare against single-agentRL methods.With regard to using RL for learning negotia-tion policies, the amount of research that has beenperformed is very limited compared to slot-filling.English and Heeman (2005) learned negotiationpolicies for a furniture layout task.
Then Hee-man (2009) extended this work by experiment-ing with different representations of the RL statein the same domain (this time learning againsta hand-crafted SU).
In both cases, to reduce thesearch space, the RL state included only infor-mation about e.g., whether there was a pendingproposal rather than the actual value of this pro-posal.
Paruchuri et al (2009) performed a theo-retical study on how Partially Observable MarkovDecision Processes (POMDPs) can be applied tonegotiation domains.502Georgila and Traum (2011) built argumentationdialogue policies for negotiation against users ofdifferent cultural norms in a one-issue negotiationscenario.
To learn these policies they trained SUson a spoken dialogue corpus in a florist-grocernegotiation domain, and then tweaked these SUstowards a particular cultural norm using hand-crafted rules.
Georgila (2013) learned argumen-tation dialogue policies from a simulated corpusin a two-issue negotiation scenario (organizing aparty).
Finally, Nouri et al (2012) used IRL tolearn a model for cultural decision-making in asimple negotiation game (the Ultimatum Game).3 Single-Agent vs. Multi-AgentReinforcement LearningReinforcement Learning (RL) is a machine learn-ing technique used to learn the policy of an agent,i.e., which action the agent should perform givenits current state (Sutton and Barto, 1998).
The goalof an RL-based agent is to maximize the reward itgets during an interaction.
Because it is very dif-ficult for the agent to know what will happen inthe rest of the interaction, the agent must select anaction based on the average reward it has previ-ously observed after having performed that actionin similar contexts.
This average reward is calledexpected future reward.
Single-agent RL is usedin the framework of Markov Decision Processes(MDPs) (Sutton and Barto, 1998) or Partially Ob-servable Markov Decision Processes (POMDPs)(Williams and Young, 2007).
Here we focus onMDPs.An MDP is defined as a tuple (S, A, T , R, ?
)where S is the set of states (representing differentcontexts) which the agent may be in, A is the setof actions of the agent, T is the transition func-tion S ?
A ?
S ?
[0, 1] which defines a set oftransition probabilities between states after takingan action, R is the reward function S ?
A ?
<which defines the reward received when taking anaction from the given state, and ?
is a factor thatdiscounts future rewards.
Solving the MDP meansfinding a policy pi : S ?
A.
The quality of thepolicy pi is measured by the expected discounted(with discount factor ?)
future reward also calledQ-value, Qpi: S ?
A?<.A stochastic game is defined as a tuple (n, S,A1...n, T , R1...n, ?)
where n is the number ofagents, S is the set of states, Aiis the set of ac-tions available for agent i (and A is the joint ac-tion space A1?
A2?
... ?
An), T is the transi-tion function S ?
A ?
S ?
[0, 1] which definesa set of transition probabilities between states af-ter taking a joint action, Riis the reward functionfor the ith agent S ?
A ?
<, and ?
is a factorthat discounts future rewards.
The goal is for eachagent i to learn a mixed policy pii: S ?
Ai?
[0,1] that maps states to mixed strategies, which areprobability distributions over the agent?s actions,so that the agent?s expected discounted (with dis-count factor ?)
future reward is maximized.Stochastic games are a generalization of MDPsfor multi-agent RL.
In stochastic games there aremany agents that select actions and the next stateand rewards depend on the joint action of all theseagents.
The agents can have different rewardfunctions.
Partially Observable Stochastic Games(POSGs) are the equivalent of POMDPs for multi-agent RL.
In POSGs, the agents have different ob-servations, and uncertainty about the state they arein and the beliefs of their interlocutors.
POSGsare very hard to solve but new algorithms continu-ously emerge in the literature.In this paper we use three algorithms: Q-learning, Policy Hill-Climbing (PHC), and Winor Learn Fast Policy Hill-Climbing (PHC-WoLF).PHC is an extension of Q-learning.
For all threealgorithms, Q-values are updated as follows:Q(s, a)?
(1??
)Q(s, a)+?
(r + ?maxa?Q(s?, a?
))(1)In Q-learning, for a given state s, the agentperforms the action with the highest Q-value forthat state.
In addition to Q-values, PHC andPHC-WoLF also maintain the current mixed pol-icy pi(s, a).
In each step the mixed policy is up-dated by increasing the probability of selecting thehighest valued action according to a learning rate?
(see equations (2), (3), and (4) below).pi(s, a)?
pi(s, a) + ?sa(2)?sa={?
?saif a 6= argmaxa?Q(s, a?
)?a?6=a?sa?otherwise(3)?sa= min(pi(s, a),?|Ai| ?
1)(4)The difference between PHC and PHC-WoLF isthat PHC uses a constant learning rate ?
whereas503PHC-WoLF uses a variable learning rate (seeequation (5) below).
The main idea is that whenthe agent is ?winning?
the learning rate ?Wshouldbe low so that the opponents have more time toadapt to the agent?s policy, which helps with con-vergence.
On the other hand when the agent is?losing?
the learning rate ?LFshould be high sothat the agent has more time to adapt to the otheragents?
policies, which also facilitates conver-gence.
Thus PHC-WoLF uses two learning rates?Wand ?LF.
PHC-WoLF determines whether theagent is ?winning?
or ?losing?
by comparing thecurrent policy?s pi(s, a) expected payoff with thatof the average policy p?i(s, a) over time.
If the cur-rent policy?s expected payoff is greater then theagent is ?winning?, otherwise it is ?losing?.?
=????????Wif{??
?pi(s, ??
)Q(s, ??)
>??
?p?i(s, ??
)Q(s, ??
)?LFotherwise(5)More details about Q-learning, PHC, and PHC-WoLF can be found in (Sutton and Barto, 1998;Bowling and Veloso, 2002).As discussed in sections 1 and 2, single-agentRL techniques, such as Q-learning, are not suit-able for multi-agent RL.
Nevertheless, despite itsshortcomings Q-learning has been used success-fully for multi-agent RL (Claus and Boutilier,1998).
Indeed, as we see in section 5, Q-learningcan converge to the optimal policy for small statespaces.
However, as the state space size increasesthe performance of Q-learning drops (compared toPHC and PHC-WoLF).4 Domain and Experimental SetupOur domain is a resource allocation negotiationscenario.
Two agents negotiate about how to shareresources.
For the sake of readability from now onwe will refer to apples and oranges.The two agents have different goals.
Also,they have human-like constraints of imperfect in-formation about each other; they do not knoweach other?s reward function or degree of rational-ity (during learning our agents can be irrational).Thus a Nash equilibrium (if there exists one) can-not be computed in advance.
Agent 1 cares moreabout apples and Agent 2 cares more about or-anges.
Table 1 shows the points that Agents 1and 2 earn for each apple and each orange that theyhave at the end of the negotiation.Agent 1 Agent 2apple 300 200orange 200 300Table 1: Points earned by Agents 1 and 2 for eachapple and each orange that they have at the end ofthe negotiation.Agent 1: offer-2-2 (I offer you 2 A and 2 O)Agent 2: offer-3-0 (I offer you 3 A and 0 O)Agent 1: offer-0-3 (I offer you 0 A and 3 O)Agent 2: offer-4-0 (I offer you 4 A and 0 O)Agent 1: accept (I accept your offer)Figure 1: Example interaction between Agents 1and 2 (A: apples, O: oranges).We use a simplified dialogue model with twotypes of speech acts: offers and acceptances.
Thedialogue proceeds as follows: one agent makes anoffer, e.g., ?I give you 3 apples and 1 orange?, andthe other agent may choose to accept it or make anew offer.
The negotiation finishes when one ofthe agents accepts the other agent?s offer or timeruns out.We compare Q-learning with PHC and PHC-WoLF.
For all algorithms and experiments eachagent is rewarded only at the end of the dialoguebased on the negotiation outcome (see Table 1).Thus the two agents have different reward func-tions.
There is also a penalty of -10 for each agentaction to ensure that dialogues are not too long.Also, to avoid long dialogues, if none of the agentsaccepts the other agent?s offers, the negotiationfinishes after 20 pairs of exchanges between thetwo agents (20 offers from Agent 1 and 20 offersfrom Agent 2).An example interaction between the two agentsis shown in Figure 1.
As we can see, each agentcan offer any combination of apples and oranges.So if we haveX apples and Y oranges for sharing,there can be (X + 1) ?
(Y + 1) possible offers.For example if we have 2 apples and 2 orangesfor sharing, there can be 9 possible offers: ?offer-0-0?, ?offer-0-1?, ..., ?offer-2-2?.
For our exper-iments we vary the number of fruits to be sharedand choose to keep X equal to Y .Table 2 shows our state representation, i.e., thestate variables that we keep track of with all thepossible values they can take, whereX is the num-504Current offer: (X + 1) ?
(Y + 1) possiblevaluesHow many times the current offer has alreadybeen rejected: (0, 1, 2, 3, or 4)Is the current offer accepted: yes, noTable 2: State variables.ber of apples and Y is the number of oranges to beshared.
The third variable is always set to ?no?
un-til one of the agents accepts the other agent?s offer.Table 3 shows the state and action space sizesfor different numbers of apples and oranges to beshared used in our experiments below.
The num-ber of actions includes the acceptance of an of-fer.
Table 3 also shows the number of state-actionpairs (Q-values).
As we will see in section 5, eventhough the number of states for each agent is notlarge, it takes many iterations and high explorationrates for convergence due to the fact that bothagents are learning at the same time and the as-sumption of interacting with a stationary environ-ment no longer holds.
For comparison, in (Englishand Heeman, 2005) the state specification for eachagent included 5 binary variables resulting in 32possible states.
English and Heeman (2005) kepttrack of whether there was an offer on the table butnot of the actual value of the offer.
For our task itis essential to keep track of the offer values, whichof course results in much larger state spaces.
Also,in (English and Heeman, 2005) there were 5 possi-ble actions resulting in 160 state-action pairs.
Ourstate and action spaces are much larger and fur-thermore we explore the effect of different stateand action space sizes on convergence.During learning the two agents interact for5 epochs.
Each epoch contains N number ofepisodes.
We vary N from 25,000 up to 400,000with a step of 25,000 episodes.
English and Hee-man (2005) trained their agents for 200 epochs,where each epoch contained 200 episodes.We also vary the exploration rate per epoch.In particular, in the experiments reported in sec-tion 5.1 the exploration rate is set as follows: 0.95for epoch 1, 0.8 for epoch 2, 0.5 for epoch 3, 0.3for epoch 4, and 0.1 for epoch 5.
Section 5.2 re-ports results again with 5 epochs of training but aconstant exploration rate per epoch set to 0.3.
Anexploration rate of 0.3 means that 30% of the timethe agent will select an action randomly.Finally, we vary the learning rate.
For PHC-#States #Actions #State-ActionPairs1 A & O 40 5 2002 A & O 90 10 9003 A & O 160 17 27204 A & O 250 26 65005 A & O 360 37 133206 A & O 490 50 245007 A & O 640 65 41600Table 3: State space, action space, and state-actionspace sizes for different numbers of apples and or-anges to be shared (A: apples, O: oranges).WoLF we set ?W= 0.05 and ?LF= 0.2 (see sec-tion 3).
These values were chosen with exper-imentation and the basic idea is that the agentshould learn faster when ?losing?
and slower when?winning?.
For PHC we explore two cases.
In thefirst case which from now on will be referred toas PHC-W, we set ?
to be equal to ?W(also usedfor PHC-WoLF).
In the second case which fromnow on will be referred to as PHC-LF, we set ?to be equal to ?LF(also used for PHC-WoLF).
Sounlike PHC-WoLF, PHC-W and PHC-LF do notuse a variable learning rate.
PHC-W always learnsslowly and PHC-LF always learns fast.In all the above cases, training stops after 5epochs.
Then we test the learned policies againsteach other for one more epoch the size of which isthe same as the size of the epochs used for train-ing.
For example, if the policies were learnedfor 5 epochs with each epoch containing 25,000episodes, then for testing the two policies will in-teract for another 25,000 episodes.
For compari-son, English and Heeman (2005) had their agentsinteract for 5,000 dialogues during testing.
To en-sure that the policies do not converge by chance,we run the training and test sessions 20 times eachand we report averages.
Thus all results presentedin section 5 are averages of 20 runs.5 ResultsGiven that Agent 1 is more interested in applesand Agent 2 cares more about oranges, the maxi-mum total utility solution would be the case whereeach agent offers to get al the fruits it cares aboutand to give its interlocutor all the fruits it does notcare about, and the other agent accepts this of-fer.
Thus, when converging to the maximum to-tal utility solution, in the case of 4 fruits (4 ap-505ples and 4 oranges), the average reward of thetwo agents should be 1200 minus 10 for makingor accepting an offer.
For 5 fruits the average re-ward should be 1500 minus 10, and so forth.
Wecall 1200 (or 1500) the convergence reward, i.e.,the reward after converging to the maximum to-tal utility solution if we do not take into accountthe action penalty.
For example, in the case of 4fruits, if Agent 1 starts the negotiation, after con-verging to the maximum total utility solution theoptimal interaction should be: Agent 1 makes anoffer to Agent 2, namely 0 apples and 4 oranges,and Agent 2 accepts.
Thus the reward for Agent 1is 1190, the reward for Agent 2 is 1190, and the av-erage reward of the two agents is also 1190.
Also,the convergence reward for Agent 1 is 1200 andthe convergence reward for Agent 2 is also 1200.Below, in all the graphs that we provide, weshow the average distance from the convergencereward.
This is to make all graphs comparablebecause in all cases the optimal average distancefrom the convergence reward of the two agentsshould be equal to 10 (make the optimal offeror accept the optimal offer that the other agentmakes).
The formulas for calculating the averagedistance from the convergence reward are:AD1=?nrj=1|CR1?R1j|nr(6)AD2=?nrj=1|CR2?R2j|nr(7)AD =AD1+AD22(8)whereCR1is the convergence reward for Agent 1,R1jis the reward of Agent 1 for run j, CR2is theconvergence reward for Agent 2, and R2jis thereward of Agent 2 for run j.
Moreover, AD1isthe average distance from the convergence rewardfor Agent 1, AD2is the average distance from theconvergence reward for Agent 2, and AD is theaverage of AD1and AD2.
All graphs of section 5show AD values.
Also, nris the number of runs(in our case always equal to 20).
Thus in the caseof 4 fruits, we will have CR1=CR2=1200, and iffor all runs R1j=R2j=1190, then AD=10.5.1 Variable Exploration RateIn this section we report results with different ex-ploration rates per training epoch (see section 4).Q- PHC- PHC- PHC-learning LF W WoLF1 A & O 10.5 10 10 102 A & O 10.3 10.3 10 103 A & O 11.7 10 10 104 A & O 15 11.8 11.7 11.75 A & O 45.4 29.5 26.5 22.96 A & O 60.8 33.4 46.1 33.97 A & O 95 56 187.8 88.6Table 4: Average distance from convergence re-ward over 20 runs for 100,000 episodes per epochand for different numbers of fruits to be shared (A:apples, O: oranges).
The best possible value is 10.Table 4 shows the average distance from the con-vergence reward over 20 runs for 100,000 episodesper epoch, for different numbers of fruits, andfor all four methods (Q-learning, PHC-LF, PHC-W, and PHC-WoLF).
It is clear that as the statespace becomes larger 100,000 training episodesper epoch are not enough for convergence.
Also,for 1, 2, and 3 fruits all algorithms converge andperform comparably.
As the number of fruits in-creases, Q-learning starts performing worse thanthe multi-agent RL algorithms.
For 7 fruits PHC-W appears to perform worse than Q-learning butthis is because, as we can see in Figure 5, in thiscase more than 400,000 episodes per epoch are re-quired for convergence.
Thus after only 100,000episodes per epoch all policies still behave some-what randomly.Figures 2, 3, 4, and 5 show the average distancefrom the convergence reward as a function of thenumber of episodes per epoch during training, for4, 5, 6, and 7 fruits respectively.
For 4 fruits ittakes about 125,000 episodes per epoch and for 5fruits it takes about 225,000 episodes per epoch forthe policies to converge.
This number rises to ap-proximately 350,000 for 6 fruits and becomes evenhigher for 7 fruits.
Q-learning consistently per-forms worse than the rest of the algorithms.
Thedifferences between PHC-LF, PHC-W, and PHC-WoLF are insignificant, which is a bit surprisinggiven that Bowling and Veloso (2002) showed thatPHC-WoLF performed better than PHC in a seriesof benchmark tasks.
In Figures 2 and 3, PHC-LFappears to be reaching convergence slightly fasterthan PHC-W and PHC-WoLF but this is not statis-tically significant.506Figure 2: 4 fruits and variable exploration rate:Average distance from convergence reward duringtesting (20 runs).
The best possible value is 10.Figure 3: 5 fruits and variable exploration rate:Average distance from convergence reward duringtesting (20 runs).
The best possible value is 10.5.2 Constant Exploration RateIn this section we report results with a constantexploration rate for all training epochs (see sec-tion 4).
Figures 6 and 7 show the average dis-tance from the convergence reward as a function ofthe number of episodes per epoch during training,for 4 and 5 fruits respectively.
Clearly having aconstant exploration rate in all epochs is problem-atic.
For 4 fruits, after 225,000 episodes per epochthere is still no convergence.
For comparison, witha variable exploration rate it took about 125,000episodes per epoch for the policies to converge.Likewise for 5 fruits.
After 400,000 episodes perepoch there is still no convergence.
For compari-son, with a variable exploration rate it took about225,000 episodes per epoch for convergence.Figure 4: 6 fruits and variable exploration rate:Average distance from convergence reward duringtesting (20 runs).
The best possible value is 10.Figure 5: 7 fruits and variable exploration rate:Average distance from convergence reward duringtesting (20 runs).
The best possible value is 10.The above results show that, unlike single-agentRL where having a constant exploration rate isperfectly acceptable, here a constant explorationrate does not work.6 Conclusion and Future WorkWe used single-agent RL and multi-agent RL forlearning dialogue policies in a resource allocationnegotiation scenario.
Two agents interacted witheach other and both learned at the same time.
Theadvantage of this approach is that it does not re-quire SUs to train against or corpora to learn from.We compared a traditional single-agent RL al-gorithm (Q-learning) against two multi-agent RLalgorithms (PHC and PHC-WoLF) varying thescenario complexity (state space size), the number507Figure 6: 4 fruits and constant exploration rate:Average distance from convergence reward duringtesting (20 runs).
The best possible value is 10.Figure 7: 5 fruits and constant exploration rate:Average distance from convergence reward duringtesting (20 runs).
The best possible value is 10.of training episodes, and the learning and explo-ration rates.
Our results showed that Q-learningis not suitable for concurrent learning given thatit is designed for learning against a stationary en-vironment.
Q-learning failed to converge in allcases, except for very small state space sizes.
Onthe other hand, both PHC and PHC-WoLF alwaysconverged (or in the case of 7 fruits they neededmore training episodes) and performed similarly.We also showed that in concurrent learning veryhigh gradually decreasing exploration rates are re-quired for convergence.
We conclude that multi-agent RL of dialogue policies is a promising alter-native to using single-agent RL and SUs or learn-ing directly from corpora.The focus of this paper is on comparing single-agent RL and multi-agent RL for concurrent learn-ing, and studying the implications for convergenceand exploration/learning rates.
Our next step istesting with human users.
We are particularly in-terested in users whose behavior changes duringthe interaction and continuous testing against ex-pert repeat users, which has never been done be-fore.
Another interesting question is whether cor-pora or SUs may still be required for designingthe state and action spaces and the reward func-tions of the interlocutors, bootstrapping the poli-cies, and ensuring that information about the be-havior of human users is encoded in the resultinglearned policies.
Ga?si?c et al (2013) showed that itis possible to learn ?full?
dialogue policies just viainteraction with human users (without any boot-strapping using corpora or SUs).
Similarly, con-current learning could be used in an on-line fash-ion via live interaction with human users.
Or al-ternatively concurrent learning could be used off-line to bootstrap the policies and then these poli-cies could be improved via live interaction withhuman users (again using concurrent learning toaddress possible changes in user behavior).
Theseare open research questions for future work.Furthermore, we intend to apply multi-agent RLto more complex negotiation domains, e.g., exper-iment with more than two types of resources (notjust apples and oranges) and more types of actions(not just offers and acceptances).
We would alsolike to compare policies learned with multi-agentRL techniques with policies learned with SUs orfrom corpora both in simulation and with humanusers.
Finally, we aim to experiment with differ-ent feature-based representations of the state andaction spaces.
Currently all possible deal combi-nations are listed as possible actions and as ele-ments of the state, which can quickly lead to verylarge state and action spaces as the application be-comes more complex (in our case as the number offruits increases).
However, abstraction is not triv-ial because the agents have no guarantee that thevalue of a deal is a simple function of the value ofits parts, and values may differ for different agents.AcknowledgmentsClaire Nelson sadly died in May 2013.
We con-tinued and completed this work after her pass-ing away.
She is greatly missed.
This work wasfunded by the NSF grant #1117313.508ReferencesPieter Abbeel and Andrew Y. Ng.
2004.
Apprentice-ship learning via inverse reinforcement learning.
InProc.
of the International Conference on MachineLearning, Bannf, Alberta, Canada.Hua Ai and Diane Litman.
2008.
Assessing dialog sys-tem user simulation evaluation measures using hu-man judges.
In Proc.
of the Annual Meeting of theAssociation for Computational Linguistics, Colum-bus, Ohio, USA.Michael Bowling and Manuela Veloso.
2002.
Multi-agent learning using a variable learning rate.
Artifi-cial Intelligence, 136(2):215?250.L.
Busoniu, R. Babuska, and B.
De Schutter.
2008.A comprehensive survey of multiagent reinforce-ment learning.
IEEE Transactions on Systems, Man,and Cybernetics, Part C: Applications and Reviews,38(2):156?172.Senthilkumar Chandramohan, Matthieu Geist, FabriceLef`evre, and Olivier Pietquin.
2012.
Co-adaptationin spoken dialogue systems.
In Proc.
of the Interna-tional Workshop on Spoken Dialogue Systems, Paris,France.Min Chi, Kurt VanLehn, Diane Litman, and PamelaJordan.
2011.
Empirically evaluating the ap-plication of reinforcement learning to the induc-tion of effective and adaptive pedagogical strategies.User Modeling and User-Adapted Interaction, 21(1-2):137?180.Caroline Claus and Craig Boutilier.
1998.
The dynam-ics of reinforcement learning in cooperative multia-gent systems.
In Proc.
of the National Conferenceon Artificial Intelligence.Heriberto Cuay?ahuitl and Nina Dethlefs.
2012.
Hier-archical multiagent reinforcement learning for coor-dinating verbal and nonverbal actions in robots.
InProc.
of the ECAI Workshop on Machine Learningfor Interactive Systems, Montpellier, France.Lucie Daubigney, Matthieu Geist, Senthilkumar Chan-dramohan, and Olivier Pietquin.
2012.
A compre-hensive reinforcement learning framework for dia-logue management optimization.
IEEE Journal ofSelected Topics in Signal Processing, 6(8):891?902.Yaakov Engel, Shie Mannor, and Ron Meir.
2005.
Re-inforcement learning with Gaussian processes.
InProc.
of the International Conference on MachineLearning, Bonn, Germany.Michael S. English and Peter A. Heeman.
2005.Learning mixed initiative dialogue strategies by us-ing reinforcement learning on both conversants.
InProc.
of the Conference on Empirical Methods inNatural Language Processing, Vancouver, Canada.M.
Ga?si?c, Filip Jur?c??
?cek, Blaise Thomson, Kai Yu, andSteve Young.
2011.
On-line policy optimisationof spoken dialogue systems via live interaction withhuman subjects.
In Proc.
of the IEEE AutomaticSpeech Recognition and Understanding Workshop,Big Island, Hawaii, USA.Milica Ga?si?c, Matthew Henderson, Blaise Thomson,Pirros Tsiakoulis, and Steve Young.
2012.
Pol-icy optimisation of POMDP-based dialogue systemswithout state space compression.
In Proc.
of theIEEE Workshop on Spoken Language Technology,Miami, Florida, USA.M.
Ga?si?c, C. Breslin, M. Henderson, D. Kim,M.
Szummer, B. Thomson, P. Tsiakoulis, andS.
Young.
2013.
On-line policy optimisation ofBayesian spoken dialogue systems via human inter-action.
In Proc.
of the International Conference onAcoustics, Speech and Signal Processing, Vancou-ver, Canada.Kallirroi Georgila and David Traum.
2011.
Reinforce-ment learning of argumentation dialogue policies innegotiation.
In Proc.
of Interspeech, Florence, Italy.Kallirroi Georgila, James Henderson, and OliverLemon.
2006.
User simulation for spoken dialoguesystems: Learning and evaluation.
In Proc.
of Inter-speech, Pittsburgh, Pennsylvania, USA.Kallirroi Georgila, Maria K. Wolters, and Johanna D.Moore.
2010.
Learning dialogue strategies fromolder and younger simulated users.
In Proc.
ofthe Annual SIGdial Meeting on Discourse and Di-alogue, Tokyo, Japan.Kallirroi Georgila.
2013.
Reinforcement learning oftwo-issue negotiation dialogue policies.
In Proc.
ofthe Annual SIGdial Meeting on Discourse and Dia-logue, Metz, France.Peter A. Heeman.
2009.
Representing the reinforce-ment learning state in a negotiation dialogue.
InProc.
of the IEEE Automatic Speech Recognitionand Understanding Workshop, Merano, Italy.James Henderson, Oliver Lemon, and KallirroiGeorgila.
2008.
Hybrid reinforcement/supervisedlearning of dialogue policies from fixed datasets.Computational Linguistics, 34(4):487?511.Junling Hu and Michael P. Wellman.
1998.
Multia-gent reinforcement learning: Theoretical frameworkand an algorithm.
In Proc.
of the International Con-ference on Machine Learning, Madison, Wisconsin,USA.Filip Jur?c??
?cek, Blaise Thomson, and Steve Young.2012.
Reinforcement learning for parameter esti-mation in statistical spoken dialogue systems.
Com-puter Speech and Language, 26(3):168?192.Michail G. Lagoudakis and Ronald Parr.
2003.
Least-squares policy iteration.
Journal of Machine Learn-ing Research, 4:1107?1149.509Oliver Lemon, Kallirroi Georgila, and James Hender-son.
2006.
Evaluating effectiveness and portabil-ity of reinforcement learned dialogue strategies withreal users: The TALK TownInfo evaluation.
In Proc.of the IEEEWorkshop on Spoken Language Technol-ogy, Palm Beach, Aruba.Lihong Li, Jason D. Williams, and Suhrid Balakrish-nan.
2009.
Reinforcement learning for dialog man-agement using least-squares policy iteration and fastfeature selection.
In Proc.
of Interspeech, Brighton,United Kingdom.Michael L. Littman.
1994.
Markov games as a frame-work for multi-agent reinforcement learning.
InProc.
of the International Conference on MachineLearning, New Brunswick, New Jersey, USA.Yi Ma.
2013.
User goal change model for spoken dia-log state tracking.
In Proc.
of the NAACL-HLT Stu-dent Research Workshop, Atlanta, Georgia, USA.Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake,Chiori Hori, Hideki Kashioka, Hisashi Kawai, andSatoshi Nakamura.
2010.
Modeling spoken deci-sion making dialogue and optimization of its dia-logue strategy.
In Proc.
of the Annual SIGdial Meet-ing on Discourse and Dialogue, Tokyo, Japan.Teruhisa Misu, Kallirroi Georgila, Anton Leuski, andDavid Traum.
2012.
Reinforcement learning ofquestion-answering dialogue policies for virtual mu-seum guides.
In Proc.
of the Annual SIGdial Meet-ing on Discourse and Dialogue, Seoul, South Korea.Elnaz Nouri, Kallirroi Georgila, and David Traum.2012.
A cultural decision-making model for nego-tiation based on inverse reinforcement learning.
InProc.
of the Cognitive Science Conference, Sapporo,Japan.P.
Paruchuri, N. Chakraborty, R. Zivan, K. Sycara,M.
Dudik, and G. Gordon.
2009.
POMDP basednegotiation modeling.
In IJCAI Workshop on Mod-eling Intercultural Collaboration and Negotiation,Pasadena, California, USA.Olivier Pietquin and Helen Hastie.
2013.
A surveyon metrics for the evaluation of user simulations.Knowledge Engineering Review, 28(1):59?73.Carl Edward Rasmussen and Christopher K. I.Williams.
2006.
Gaussian Processes for MachineLearning.
MIT Press.Verena Rieser, Simon Keizer, Xingkun Liu, and OliverLemon.
2011.
Adaptive information presentationfor spoken dialogue systems: Evaluation with hu-man subjects.
In Proc.
of the European Workshopon Natural Language Generation, Nancy, France.Jost Schatzmann, Karl Weilhammer, Matt Stuttle, andSteve Young.
2006.
A survey of statistical user sim-ulation techniques for reinforcement-learning of dia-logue management strategies.
Knowledge Engineer-ing Review, 21(2):97?126.Satinder Singh, Diane Litman, Michael Kearns, andMarilyn Walker.
2002.
Optimizing dialogue man-agement with reinforcement learning: Experimentswith the NJFun system.
Journal of Artificial Intelli-gence Research, 16:105?133.Richard S. Sutton and Andrew G. Barto.
1998.
Rein-forcement Learning: An Introduction.
MIT Press.Joel R. Tetreault and Diane J. Litman.
2008.
A rein-forcement learning approach to evaluating state rep-resentations in spoken dialogue systems.
SpeechCommunication, 50(8-9):683?696.Blaise Thomson and Steve Young.
2010.
Bayesianupdate of dialogue state: A POMDP framework forspoken dialogue systems.
Computer Speech andLanguage, 24(4):562?588.Jason D. Williams and Steve Young.
2007.
Scal-ing POMDPs for spoken dialog management.
IEEETransactions on Audio, Speech, and Language Pro-cessing, 15(7):2116?2129.510
