SESSION 3: SPOKEN LANGUAGE SYSTEMS l l IJohn Makhoul, ChairBBN Systems and Technologies, 10 Moulton St., Cambridge, MA 02138BACKGROUNDTwo years ago, the DARPA SpokenLanguage Systems (SLS) CoordinatingCommittee made a decision to develop theATIS (Airline Travel Information System)database for use as a common domain inwhich spoken language systems will bedeveloped and evaluated.
Since then, therehas been significant work done to develop?
a consistent and rich ATIS database,?
data collection methodologies andscenarios, andmethods for use in the commonevaluation of spontaneous peechrecognition and understanding of textand speech input.Previously, there had been two sets ofevaluations, in June 90 and February 91, ofinitial versions of the ATIS database.
Inboth cases, the available data to be used fortraining was minimal and most of thespeech training data was read, with only asmall amount of spontaneous training data.Since February 91, the ATIS database hasbeen updated and, in an effort to quicklycollect a larger amount of training and testdata, a concerted effort has taken place incollecting data at five different sites(AT&T, BBN, CMU, MIT, and SRI).
(See \[1\] for details.)
About 10,000spontaneous utterances were collected, ofwhich about half were annotated (texttranscriptions, reference answers, etc.)
byDecember 20.
Thus, for the first time sincethe decision to adopt ATIS as the commontask for evaluation, the different sites hadavailable to them sufficient amounts oftraining data that is similar in nature to thedata to be used in testing the systems, albeitthe different sites did not have much time towork on the new data before the evaluationwas performed.Also, in the last two years, there have beenchanges in the evaluation methodologies.For the evaluation of spontaneous speech,the methodology has not changed much.The error rate is still computed as the sumof substitutions, deletions, and insertions,given a transcription of the speech.
(Wordfragments and nonspeech events are notincluded in the evaluation.)
Since thepercentage of new words in the test datahas been quite minimal, no specialconsideration for new words is made.
Forevaluating natural language understandingfrom text and spoken languageunderstanding from speech, the answer to aquery is compared against a referenceanswer.
The understanding error rate isthen computed as the sum of the percentageof queries for which a system gives 'noanswer' and twice the percentage of queriesfor which the system gives a false answer.THE SESSIONThis session was devoted to presentationsfrom the six sites that performedevaluations on the February 92 ATISspeech, natural language, and spokenlanguage tests.
These sites includedAT&T, BBN, CMU, MIT, Paramax, andSRI.The results how considerable performanceimprovements since a year ago.
In speechrecognition, much of the improvement inperformance is attributable tothe significantincrease in the amount of appropriatetraining data, which al lowed the65development of better acoustic models andbetter language models.
In naturallanguage understanding, there has alsobeen substantial  improvement inperformance, due to further systemdevelopment as well as the availability ofmore appropriate raining data.Much of the discussion period centered onthe differences in performance on datacollected from the different sites.
Forexample, the error rates on the datacollected at MIT were significantly lowerthan the others, while the data from AT&Tand SRI resulted in higher error rates.These differences may have been due to thedifferences in the amounts of training datacollected at the different sites \[1\].
Also, theAT&T and SRI data appeared to possess alarger amount of spontaneous peecheffects.
In general, the fact that all subjectswho were employed in the collection ofdata were unexperienced may have resultedin a higher overall error rate.
There werecalls to bnng back some of the subjects forfurther testing to test the effects of subjectexperience on performance.Now that a significant amount of trainingdata is available, it will be interesting to seehow much improvement in performancecan be achieved by working on this data fora reasonable amount of time.REFERENCES\[1\] MADCOW, "Multi-Site Data Collectionfor a Spoken Language Corpus," Session 1in this workshop.66
