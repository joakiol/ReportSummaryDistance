Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 52?57,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsA Data Mining Approach to Learn Reorder Rules for SMTAvinesh PVSIIIT HyderabadLanguage Technologies Research Centreavinesh@research.iiit.ac.inAbstractIn this paper, we describe a syntax basedsource side reordering method for phrase-based statistical machine translation (SMT)systems.
The source side training corpus isfirst parsed, then reordering rules are auto-matically learnt from source-side phrases andword alignments.
Later the source side train-ing and test corpus are reordered and givento the SMT system.
Reordering is a commonproblem observed in language pairs of distantlanguage origins.
This paper describes an au-tomated approach for learning reorder rulesfrom a word-aligned parallel corpus using as-sociation rule mining.
Reordered and gener-alized rules are the most significant in our ap-proach.
Our experiments were conducted onan English-Hindi EILMT corpus.1 IntroductionIn recent years SMT systems (Brown et al,1990), (Yamada and Knight, 2001), (Chiang,2005), (Charniak et al, 2003) have been in focus.
Itis easy to develop a MT system for a new pair of lan-guages using an existing SMT system and a parallelcorpora.
It isn?t a surprise to see SMT being attrac-tive in terms of less human labour as compared totraditional rule-based systems.
However to achievegood scores SMT requires large amounts of sentencealigned parallel text.
Such resources are availableonly for few languages, whereas for many languagesthe online resources are low.
So we propose an ap-proach for a pair of resource rich and resource poorlanguages.Some of the previous approaches include (Collinset al, 2005), (Xia and McCord, 2004).
Formerdescribes an approach for reordering the sourcesentence in German-English MT system.
Theirapproach involves six transformations on the parsedsource sentence.
Later propose an approach whichautomatically extracts rewrite patterns by parsingthe source and target sides of the training corpusfor French-English pair.
These rewritten patternsare applied to the source sentence so that the sourceand target word orders are similar.
(Costa-jussa`and Fonollosa, 2006) consider Part-Of-Speech(POS) based source reordering as a translationtask.
These approaches modify the source languageword order before decoding in order to produce aword order similar to the target language.
Laterthe reordered sentence is given as an input to thestandard phrase-based decoder to be translatedwithout the reordering condition.We propose an approach along the same linesthose described above.
Here we follow a datamining approach to learn the reordering/rewriterules applied on an English-Hindi MT system.
Therest of the paper is organized as follows.
In Section2 we briefly describe our approach.
In Section 3 wepresent a rule learning framework using AssociationRule Mining (Agrawal et al, 1993).
Section 4consists of experimental setup and sample ruleslearnt.
We present some discussion in Section 5 andfinally detail proposed future work in Section 6.52RB9.
1511SADVP,,PPIN NP,NP VP .ADJPNP NPRBS NNSmostLike2 things 3145however, 6VBZNNbusiness 7 RBis 8 notJJrisk?free10 12VPVBNbeen 14ADVPneverRB has 13VBZ* Reodered Nodes1:6  2:3  3:4  4:7  5:1  6:2  7:8  8:18  9:17 10:13  11:16  12:10  13:14  14:14  15:19Alignments: Source Position : Target PositionCCandVP VPFigure 1: English-Hindi Example2 ApproachOur approach is inspired by Association rule min-ing, a popular concept in data mining for discoveringinteresting relations between items in large transac-tion records.
For example, the rule {milk, bread} ?
{butter} found in the customer database would indi-cate if a customer buys milk and bread together, heor she is also likely to buy butter.
Similar notionscan be projected to the learning of reorder rules.
Forexample, {NNP, VB, NNP} ?
{1,3,2} would indi-cate if NNP,VB and NNP occur together in sourcetext, then its ordering on the target side would be{1,3,2}.
The original problem of association rulemining doesn?t consider the order of items in therule, whereas in our problem order is important aswell.In this approach we start with extracting the mostfrequent patterns from the English language model.The English language model consists of both POSand chunk tag n-gram model built using SRILMtoolkit 1.
Then to learn the reordering rules for thesepatterns we used a word-aligned English-Hindi par-allel corpus, where the alignments are generated us-ing GIZA++ (Och and Ney, 2003).
These align-ments are used to learn the rewrite rules by calculat-ing the target positions of the source nodes.
Fig 1shows an English phrase structure tree (PS) 2 and its1http://www-speech.sri.com/projects/srilm/2Stanford Parser: http://nlp.stanford.edu/software/lex-alignments corresponding to the target sentence.2.1 Calculation of target position:Target position of a node is equal to the targetposition of the head among the children (Aho andUllman, 1972).
For example the head node of a NPis the right most NN, NNP, NNS (or) NNX.
Rulesdeveloped by Collins are used to calculate the headnode (Collins, 2003).Psn(T,Node)=Psn(T,Head(Node))In Fig 1, Position of VP in target side is 18.Psn(T,VP)=Psn(T,Head(VP))=Psn(T,VBZ)=183 Association rule miningWe modified the original definition by Rakesh Agar-wal to suit our needs (Agrawal et al, 1993; Srikantand Agrawal, 1995) .
The problem here is definedas: Let E=P:{e1,e2,e3,...en } be a sequence of Nchildren of a node P. Let A={a1,a2,a3,...an } be thealignment set of the corresponding set E.Let D=P:{ S1,S2,S3,...Sm } be set consisting of allpossible ordered sequence of children of the node P,Ex: S1=S:{NP,VP,NP}, where S is the parent nodeand NP, VP and NP are its children.
Each set in Dhas a unique ID, which represents the occurrence ofthe source order of the children.
A rule is definedas an implication of the form X?Y where X?E andparser.shtml53Y?Target Positions(E,A).
The sets of items X andY are called LHS and RHS of the rule.
To illus-trate the concepts, we use a simple example fromthe English-Hindi parallel corpus.Consider the set of items I={Set of POStags}?
{Set of Chunk tags}.
For Example,I={NN,VBZ,NNS,NP,VP} and an example rulecould be {NN,VBZ,NNS} ?
{1,3,2}, which meansthat when NN, VBZ and NNS occur in a continuouspattern they are reordered to 1,3 and 2 positionsrespectively on the target side.
The above exampleis a naive example.
If we consider the trainingcorpus with the alignments we could use constraintson various measures of significance.
We use thebest-known constraints, namely minimum thresholdsupport and confidence.
The support supp(X) of anitemset X is defined as the proportion of sentenceswhich contain the itemset.
The confidence of a ruleis defined asconf(X?Y)=supp(X?Y)/supp(X).Association rules require language specific mini-mum support and minimum confidence at the sametime.
To achieve this, association rule learning isdone in two steps.
Firstly, minimum support is ap-plied to find all frequent itemsets in the source lan-guage model.
In the second step, these frequentitemsets and the minimum confidence constraintsare used to generate rules from the word-aligned par-allel corpus.3.1 Frequent Pattern miningFor the first task of collecting the most frequentitemsets we used Fpgrowth algorithm 3 (Borgelt,2005) implemented by Christian Borgelt.
We useda POS and a chunk tag English language model.
Ina given parse tree the pattern model based on the or-der of pre-terminals is called POS language modeland the pattern model based on the Non-terminals iscalled the Chunk language model.
The below algo-rithm is run on every Non-terminal and pre-terminalnode of a parse tree.
In the modified version of min-ing frequent itemsets we also include generalizationof the frequent sets, similar to the work done by(Chiang, 2005).3http://www.borgelt.net/fpgrowth.htmlSteps for extracting frequent LHSs: ConsiderX1,X2,X3,X4,...Xx are all possible children of anode S. The transaction here is the sequence of chil-dren of the node S. The sample example is shown inFig 2.1.
Collect all occurrences of the children of a nodeand their frequencies from the transactions andname the set L1.2.
Calculate L2=L1 ?
L1 which is the frequencyset of two elements.3.
Similarly calculate Ln, till n = maximum pos-sible children of parent S.4.
Once the maximum possible set is calculated,K-best frequent sets are collected and then el-ements which occur above a threshold(?)
arecombined to form a single element.Ex, most common patterns occurring as a chil-dren of NP are {JJ,NN,NN},{JJ,NN} etc.5.
The threshold was calculated based on variousexperiments, and then set to ?=20% less thanthe frequency of least frequent itemset betweenthe elements of the two L?s.For example,L3={JJ,NN}?
{NN}={JJ,NN,NNP}.If freq{JJ,NN}=10, and freq{NNP}=20 and{JJ,NN,NNP}=9, ?=10-(20% of 10)=8.So {JJ,NN} ?
X1.This way the generalized rules are learntfor all the tables (Ln, Ln?1..L3).
Using thesegeneralized rules, the initial transactions aremodified.6.
Recalculate L1,L2,..Ln based on the rules learntabove.
Continue the process until no new rulesare extracted at the end of the iteration.3.2 Generate rulesThe second problem is to generate association rulesfor these large itemsets with the constraints of min-imal confidence.
Suppose one of the large itemsetsof a parent node S is Lk, Lk = P:{e1,e2,,ek }, as-sociation rules with these itemsets are generated inthe following way: Firstly a set P:{ e1,e2,..ek } is54=10X2 X3 X4 X5X1X1 X2 X3X1 X2 X4X0 X3 X5X2 X3X3 X5X1 X2X0 X3corpusX1 X2 X3(patterns in the)TransactionsX2 12X3 20X4 25X5 10LX1 X2 X3 X1 X2 K?bestFrequentItemsetsX12ThresholdX1 15SX3 X4L 2 = L L110*1.............. ...........15939Update ValFigure 2: N-stage Generalizationmatched with the source sequences of parent P andthen their corresponding alignment information isused to generate the target sequence.
The numberson the rhs represent the position of the elements inthe target sentence.
Then by checking the constraintconfidence this rule can be determined as interestingor not.
Constraint confidence used here is the prob-ability of occurrence of the non-monotone rule.If c1,c2,c3,c4...cx are the children of a Node X.LHS is the original order of the children.
RHSis the sorted order of the children on the basis ofPsn(T,Psn(S,ci)), where 1?i?x.From Fig 1, let us consider the top node and findthe rule based on the head based method.Suppose that given from the above frequencyruleLk = S:{?PP?
?,?
?ADVP?
?,?
?NP?
?VP?
}Children(S) = ?PP?
?,?
?ADVP?
?,?
?NP?
?VP?
?.
?The target positions are calculated as shown inTable 1: Target Positions of Children(S)Psn(T,?PP?)
= Psn(T,1) =6Psn(T,?,?)
= Psn(T,4) =7Psn(T,?ADVP?)
= Psn(T,5) =1Psn(T,?,?)
= Psn(T,6) =2Psn(T,?NP?)
=Psn(T,7) =8Psn(T,?VP?)
=Psn(T,8) =18Psn(T,?.?)
= Psn(T,15) =19the Table 1.
RHS is calculated based on the targetpositions.LHS = PP , ADVP , NP VP .RHS = 3 4 1 2 5 6 73.2.1 Use of Generalization:The above rule generated is the most commonlyoccurring phenomenon in English to Hindi machinetranslation.
It is observed that adverbial phrasegenerally occurs at the beginning of the sentenceon the Hindi side.
The rule generated above willbe captured less frequently because the exactpattern in LHS is rarely matched.
Using the abovegeneralization in frequent itemset mining we canmerge all the most frequent occurring patterns intoa common pattern.The above example pattern is modified to the belowusing the generalization technique.Rule: X1 ADVP , X2?
2 3 1 43.2.2 Rules and their ApplicationThese generated rules are taken to calculate theprobability of the non-monotone rules with respectto monotone rules.
If the probability of the non-monotone rule was?0.5 then the rule was appendedto the final list.
The final list included all the gener-alized and non-generalized rules of different parentnodes.The final list of rules is applied on both trainingand test corpus based on the longest possible se-quence match.
If the rule matches, then the sourcestructures are reordered as per the rule.
Specificrules are given more priority over the generalizedrules.554 ExperimentsTable 2, Table 3 show some of the high frequencyand generalized rules.
The total number of ruleslearnt were 727 for a 11k training corpus.
Numberof generalizations learnt were 54.Table 2: Most Frequent RulesRule LHS RHS1 IN NP 2 12 NP VP NP 1 3 23 NP PP 2 14 VBG PP 2 15 VBZ ADVP NP 2 3 1Table 3: Generalized RulesRule LHS RHS1 X1 ADVP , X2 2 3 1 42 X3 VBZ?VBG X4 1 3 23 ADVP X5 .
2 1 34 MD RB X6 3 1 25 VB X7 NP-TMP 2 3 1Once the training and test sentences are reorderedusing the above rules, they are fed to the Moses sys-tem.
It is clear that without reordering the perfor-mace of the system is worst.
Training and test dataconsisted of 11,300 and 500 sentences respectively.Table 4: Evaluation on MosesConfig Blue Score NISTMoses Without Reorder 0.2123 5.5315Moses + Our Reorder 0.2329 5.6605Moses With Reorder 0.2475 5.70695 DiscussionOur method showed a drop in terms of blue scoreas compared to Moses reordering; this is proba-bly due to the reordering based on lexicalized rulesin Moses.
The above generalization works effec-tively in case of the Stanford parser as it stitchesthe nodes at top level.
English-Hindi tourism corpusdistributed as a part of ICON 2008 shared task.
Ourlearning based on phrase structure doesn?t handlethe movement of children across nodes.
Whereas,dependency structure based rule learning would helpin handling more constructs in terms of word-levelreordering patterns.
Some of the least frequent pat-terns are actually interesting patterns in terms of re-ordering.
Learning these kinds of patterns would bea challenging task.6 Future WorkWork has to be done in terms of prioritization of therules, for example first priority should be given tomore specific rules (the one with constraints) then tothe general rules.
More constraints with respect tomorphological features would also help in improv-ing the diversity of the rules.
We will also lookinto the linguistic clause based reordering featureswhich would help in reordering of distant pair of lan-guages.
Manual evaluation of the output will throwsome light on the effectiveness of this system.
Tofurther evaluate the approach we would also try theapproach on someother distant language pairs.ReferencesRakesh Agrawal, Tomasz Imielin?ski, and Arun Swami.1993.
Mining association rules between sets of itemsin large databases.
In SIGMOD ?93: Proceedings ofthe 1993 ACM SIGMOD international conference onManagement of data, pages 207?216, New York, NY,USA.
ACM.Alfred V. Aho and Jeffrey D. Ullman.
1972.
The The-ory of Parsing, Translation and Compiling, volume 1.Prentice-Hall, Englewood Cliffs, NJ.Christian Borgelt.
2005.
An implementation of the fp-growth algorithm.
In OSDM ?05: Proceedings of the1st international workshop on open source data min-ing, pages 1?5, New York, NY, USA.
ACM.Peter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-ferty, Robert L. Mercer, and Paul S. Roossin.
1990.
Astatistical approach to machine translation.
COMPU-TATIONAL LINGUISTICS, 16(2):79?85.Eugene Charniak, Kevin Knight, and Kenji Yamada.2003.
Syntax-based language models for statisticalmachine translation.
In MT Summit IX.
Intl.
Assoc.for Machine Translation.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In In ACL, pages263?270.56Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In ACL ?05: Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, pages 531?540, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Technical report.Marta R. Costa-jussa` and Jose?
A. R. Fonollosa.
2006.Statistical machine reordering.
In EMNLP ?06: Pro-ceedings of the 2006 Conference on Empirical Meth-ods in Natural Language Processing, pages 70?76,Morristown, NJ, USA.
Association for ComputationalLinguistics.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Ramakrishnan Srikant and Rakesh Agrawal.
1995.
Min-ing generalized association rules.
In Research ReportRJ 9963, IBM Almaden Research.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical mt system with automatically learned rewritepatterns.
In COLING ?04: Proceedings of the 20thinternational conference on Computational Linguis-tics, page 508, Morristown, NJ, USA.
Association forComputational Linguistics.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In ACL ?01: Proceedingsof the 39th Annual Meeting on Association for Compu-tational Linguistics, pages 523?530, Morristown, NJ,USA.
Association for Computational Linguistics.57
