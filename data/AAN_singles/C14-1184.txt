Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1950?1961, Dublin, Ireland, August 23-29 2014.Why Gender and Age Prediction from Tweets is Hard:Lessons from a Crowdsourcing ExperimentDong Nguyen14?Dolf Trieschnigg14A.
Seza Do?gru?oz23Rilana Gravel4Mari?et Theune1Theo Meder4Franciska de Jong1(1) Human Media Interaction, University of Twente, Enschede, The Netherlands(2) Netherlands Institute for Advanced Studies, Wassenaar, NL(3) Tilburg School of Humanities, Tilburg University, Tilburg, NL(4) Meertens Institute, Amsterdam, The Netherlands?Corresponding author: d.nguyen@utwente.nlAbstractThere is a growing interest in automatically predicting the gender and age of authors from texts.However, most research so far ignores that language use is related to the social identity of speak-ers, which may be different from their biological identity.
In this paper, we combine insightsfrom sociolinguistics with data collected through an online game, to underline the importanceof approaching age and gender as social variables rather than static biological variables.
In ourgame, thousands of players guessed the gender and age of Twitter users based on tweets alone.We show that more than 10% of the Twitter users do not employ language that the crowd as-sociates with their biological sex.
It is also shown that older Twitter users are often perceivedto be younger.
Our findings highlight the limitations of current approaches to gender and ageprediction from texts.1 IntroductionA major thrust of research in sociolinguistics aims to uncover the relationship between social variablessuch as age and gender, and language use (Holmes and Meyerhoff, 2003; Eckert and McConnell-Ginet,2013; Eckert, 1997; Wagner, 2012).
In line with scholars from a variety of disciplines, including the so-cial sciences and philosophy, sociolinguists consider age and gender as social and fluid variables (Eckert,2012).
Gender and age are shaped depending on the societal context, the culture of the speakers involvedin a conversation, the individual experiences and the multitude of social roles: a female teenager mightalso be a high school student, a piano player, a swimmer, etc.
(Eckert, 2008).Speakers use language as a resource to construct their identity (Bucholtz and Hall, 2005).
For example,a person?s gender identity is constructed through language by using linguistic features associated withmale or female speech.
These features gain social meaning in a cultural and societal context.
On Twitter,users construct their identity through interacting with other users (Marwick and boyd, 2011).
Dependingon the context, they may emphasize specific aspects of their identity, which leads to linguistic variationboth within and between speakers.
We illustrate this with the following three tweets:Tweet 1: I?m walking on sunshine <3 #and don?t you feel goodTweet 2: lalaloveya <3Tweet 3: @USER loveyou ;DIn these tweets, we find linguistic markers usually associated with females (e.g.
a heart representedas <3).
Indeed, 77% of the 181 players guessed that a female wrote these tweets in our online game.However, this is a 16-year old biological male, whose Twitter account reveals that he mostly engageswith female friends.
Therefore, he may have accommodated his style to them (Danescu-Niculescu-Mizilet al., 2011) and as a result he employs linguistic markers associated with the opposite biological sex.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1950Most of the NLP research focusing on predicting gender and age has approached these variables as bi-ological and static, rather than social and fluid.
For example, current approaches use supervised machinelearning models trained on tweets from males and females.
However, the resulting stereotypical modelsare ineffective for Twitter users who tweet differently from what is to be expected from their biologicalsex.As explained above, language use is based on social gender and age identity, and not on biological sexand chronological age.
In other words, treating gender and age as fixed biological variables in analyzinglanguage use is too simplistic.
By comparing the biological sex and chronological age of Twitter userswith how they are perceived by the crowd (as an indication of socially constructed identities), we shedlight on the difficulty of predicting gender and age from language use and draw attention to the inherentlimitations of current approaches.As has been demonstrated in several studies, the crowd can be used for experimentation (e.g., Munroet al.
(2010)).
Our study illustrates the value of the crowd for the study of human behavior, in particularfor the experimental study of the social dimension of language use.
To collect data, we created an onlinegame (an example of gamification (Deterding et al., 2011)) in which thousands of players (the crowd)guessed the biological sex and age of Twitter users based on only the users?
tweets.
While variancebetween annotators has traditionally been treated as noise, more recently variation is being treated as asignal rather than noise (Aroyo and Welty, 2013).
For example, Makatchev and Simmons (2011) analyzehow English utterances are perceived differently across language communities.This paper follows this trend, treating variation as meaningful information.
We assume that the crowd?sperception (based on the distribution of the players?
guesses) is an indication of to what extent Twitterusers emphasize their gender and age identity in their tweets.
For example, when a large proportion ofthe players guess the same gender for a particular user, the user is assumed to employ linguistic markersthat the crowd associates with gender-specific speech (e.g.
iconic hearts used by females).Our contributions are as follows:?
We demonstrate the use of gamification to study sociolinguistic research problems (Section 3).?
We study the difficulty of predicting an author?s gender (Section 4) and age (Section 5) from textalone by analyzing prediction performance by the crowd.
We relate our results to sociolinguistictheories and show that approaching gender and age as fixed biological variables is too simplistic.?
Based on our findings, we reflect on current approaches to predicting age and gender from text, anddraw attention to the limitations of these approaches (Section 6).2 Related WorkGender Within sociolinguistics, studies on gender and language have a long history (Eckert andMcConnell-Ginet, 2013).
More recently, the NLP community has become increasingly interested inthis topic.
Most of the work aims at predicting the gender of authors based on their text, thereby focusingmore on prediction performance than sociolinguistic insights.A variety of datasets have been used, including Twitter (Rao et al., 2010; Bamman et al., 2014; Fink etal., 2012; Bergsma and Van Durme, 2013; Burger et al., 2011), blogs (Mukherjee and Liu, 2010; Schler etal., 2005), telephone conversations (Garera and Yarowsky, 2009), YouTube (Filippova, 2012) and chatsin social networks (Peersman et al., 2011).
Females tend to use more pronouns, emoticons, emotionwords, and blog words (lol, omg, etc.
), while males tend to use more numbers, technology words, andlinks (Rao et al., 2010; Bamman et al., 2014; Nguyen et al., 2013).
These differences have also beenexploited to improve sentiment classification (Volkova et al., 2013) and cyberbullying detection (Dadvaret al., 2012).To the best of our knowledge, the study by Bamman et al.
(2014) is the only computational study thatapproaches gender as a social variable.
By clustering Twitter users based on their tweets, they show thatmultiple gendered styles exist.
Unlike their study, we use the crowd and focus on implications for genderand age prediction.1951Figure 1: Screenshot of the game.
Text is translated into English (originally in Dutch).
Left shows theinterface when the user needs to make a guess.
Right shows the feedback interface.Age Eckert (1997) makes a distinction between chronological (number of years since birth), biological(physical maturity), and social age (based on life events).
Most of the studies on language and agefocus on chronological age.
However, speakers with the same chronological age can have very differentpositions in society, resulting in variation in language use.
Computational studies on language use andage usually focus on automatic (chronological) age prediction.
This has typically been modeled asa classification problem, although this approach often suffers from ad hoc and dataset dependent ageboundaries (Rosenthal and McKeown, 2011).
In contrast, recent works also explored predicting age as acontinuous variable and predicting lifestages (Nguyen et al., 2013; Nguyen et al., 2011) .Similar to studies on gender prediction, a variety of resources have been used for age prediction, in-cluding Twitter (Rao et al., 2010; Nguyen et al., 2013), blogs (Rosenthal and McKeown, 2011; Goswamiet al., 2009), chats in social networks (Peersman et al., 2011) and telephone conversations (Garera andYarowsky, 2009).
Younger people use more alphabetical lengthening, more capitalization of words,shorter words and sentences, more self-references, more slang words, and more Internet acronyms(Rosenthal and McKeown, 2011; Nguyen et al., 2013; Rao et al., 2010; Goswami et al., 2009; Pen-nebaker and Stone, 2003; Barbieri, 2008).3 DataTo study how people perceive the gender and age identity of Twitter users based on their tweets, wecreated an online game.
Players were asked to guess the gender and age of Twitter users from tweets.The game was part of a website (TweetGenie, www.tweetgenie.nl) that also hosted an automatic systemthat predicts the gender and age of Twitter users based on their tweets (Nguyen et al., 2014).
To attractplayers, a link to the game was displayed on the page with the results of the automatic prediction, andvisitors were challenged to test if they were better than the automatic system (TweetGenie).3.1 Twitter DataWe sampled Dutch Twitter users in the fall of 2012.
We employed external annotators to annotate thebiological sex and chronological age (in years) using all information available through tweets, the Twitterprofile and external social media profiles such as Facebook and Linkedin.
In total over 3000 Twitter userswere annotated.
For more details regarding the collection of the dataset we refer to Nguyen et al.
(2013).We divided the data into train and test sets.
200 Twitter users were randomly selected from the testset to be included in the online game (statistics are shown in Table 1).
Named entities were manuallyanonymized to conceal the user?s identity.
Names in tweets were replaced by ?similar?
names (e.g.
afirst name common in a certain region in the Netherlands was replaced with another common name inthat region).
This was done without knowing the actual gender and age of the Twitter users.
Links werereplaced with a general [LINK] token and user mentions with @USER.1952Gender and age F, <20 M, <20 F, [20-40) M, [20-40) F, ?40 M, ?40Frequency 61 60 24 23 17 15Table 1: Statistics Twitter users in our game3.2 Online GameGame Setup The interface of the game is shown in Figure 1.
Players guessed the biological sex (maleor female) and age (years) of a Twitter user based on only the tweets.
For each user, {20, 25, 30, 35,40} tweets were randomly selected.
For a particular Twitter user, the same tweets were displayed to allplayers.
Twitter users were randomly selected to be displayed to the players.To include an entertainment element, players received feedback after each guess.
They were shownthe correct age and gender, the age and gender guessed by the computer, and the average guessed ageand gender distribution by the other players.
In addition, a score was shown of the player versus thecomputer.Collection In May 2013, the game was launched.
Media attention resulted in a large number of visitors(Nguyen et al., 2014).
We use the data collected from May 13, 2013 to August 21, 2013, resulting ina total of 46,903 manual guesses.
Players tweeted positively about the game, such as ?
@USER Do youknow what is really addictive?
?Are you better than Tweetgenie?
...?
and ?
@USER Their game is quitefun!?
(tweets translated to English).We filter sessions that do not seem to contain genuine guesses: when the entered age is 80 yearsor above, or 8 or below.
These thresholds were based on manual inspection, and chosen because it isunlikely that the shown tweets are from users of such ages.
For each guess, we registered a session IDand an IP address.
A new session started after 2 hours of inactivity.
To study player performance morerobustly, we excluded multiple sessions of the same player.
After three or more guesses had been madein a session, all next sessions from the same IP address were discarded.Statistics Statistics of the data are shown in Table 2.
Figure 2 shows the distribution of the number ofguesses per session.
The longest sessions consisted of 18 guesses.
Some of our analyses require multipleguesses per player.
In that case, we only include players having made at least 7 guesses.1 2 3 4 5 6 7 8 9 10 >10Number of guessesFrequency0100030005000Figure 2: Number of guesses per session# guesses 41,989# sessions 15,724Avg.
time (sec) per guess 46Avg.
# guesses / session 2.67Table 2: Statistics online game (aftercleaning)We calculate the time taken for a guess by taking the time difference between two guesses (therefore,no time for the first guess in each session could be measured).
For each Twitter user, we calculate theaverage time that was taken to guess the gender and age of the user.
(Figure 3a).
There is a significantcorrelation (Pearson?s r = 0.291, p < 0.001) between the average time the players took to evaluate thetweets of a Twitter user and the number of displayed tweets.There is also a significant correlation between the average time taken for a user and the entropy overgender guesses (Pearson?s r = 0.410, p < 0.001), and the average time taken for a user and the standarddeviation of the age guesses (Pearson?s r = 0.408, p < 0.001).
Thus, on average, players spent more timeon Twitter users for whom it was more difficult to estimate gender and age.1953Avg time taken for Twitter user (sec)Frequency02040608030 35 40 45 50 55 60 65(a) Average time taken for Twitter usersTurnAveragetime taken303234362 3 4 5 6 7(b) Average time taken per turnFigure 3: Time taken in gameWe observe that as the game progresses, players tend to take less time to make a guess.
This is shownin Figure 3b, which shows the average time taken for a turn (restricted to players with at least 7 guesses).There was no significant correlation between time spent on a guess and the performance of players andwe did not find trends of performance increase or decrease as players progressed in the game.3.3 Automatic PredictionBesides studying human performance, we also compare the predictions of humans with those of anautomatic system.
We split the data into train and test sets using the same splits as used by Nguyen et al.(2013).
We train a logistic regression model to predict gender (male or female), and a linear regressionmodel to predict the age (in years) of a person.More specifically, given an input vector x ?
Rm, x1, .
.
.
, xmrepresent features.
In the case of genderclassification (e.g.
y ?
{?1, 1}), the model estimates a conditional distribution P (y|x, ?)
= 1/(1 +exp(?y(?0+ x>?
))), where ?0and ?
are the parameters to estimate.
Age is treated as a regressionproblem, and we find a prediction y?
?
R for the exact age of a person y ?
R using a linear regressionmodel: y?
= ?0+ x>?.
We use Ridge (also called L2) regularization to prevent overfitting.We make use of the liblinear (Fan et al., 2008) and scikit-learn (Pedregosa et al., 2011) libraries.We only use unigram features, since they have proven to be very effective for gender (Bamman et al.,2014; Peersman et al., 2011) and age (Nguyen et al., 2013) prediction.
Parameters were tuned usingcross-validation on the training set.4 GenderMost of the computational work on language and gender focuses on gender classification, treating genderas fixed and classifying speakers into females and males.
However, this assumes that gender is fixed andis something people have, instead of something people do (Butler, 1990).In this section, we first analyze the task difficulty by studying crowd performance on inferring genderfrom tweets.
We observe a relatively large group of Twitter users who employ language that the crowdassociates with the opposite biological sex.
This, then, raises questions about the upper bound that aprediction system based on only text can achieve.Next, we place Twitter users on a gender continuum based on the guesses of the players and show thattreating gender as a binary variable is too simplistic.
While historically gender has been treated as binary,researchers in fields such as sociology (Lorber, 1996) and sociolinguistics (Holmes and Meyerhoff, 2003;Bergvall et al., 1996) find this view too limited.
Instead, we assume the simplest extension beyond abinary variable: a one-dimensional gender continuum (or scale) (Bergvall et al., 1996).
For example,Bergvall (1999) talks about a ?continuum of humans?
gendered practices?.
While these previous studieswere based on qualitative analyses, we take a quantitative approach using the crowd.19544.1 Task DifficultyMajority vote We study crowd performance using a system based on the majority of the players?guesses.
Majority voting has proven to be a strong baseline to aggregate votes (e.g.
in crowdsourcingsystems (Snow et al., 2008; Le et al., 2010)).
On average, we have 210 guesses per Twitter user, providingsubstantial evidence per Twitter user.
A system based on majority votes achieves an accuracy of 84%(Table 3a shows a confusion matrix).
Table 3b shows a confusion matrix of the majority predictionsversus the automatic system.
We find that the biological sex was predicted incorrectly by both themajority vote system and the automatic system for 21 out of the 200 Twitter users (10.5%, not in Table).Automatic classification systems on English tweets achieve similar performances as our majority votesystem (e.g.
Bergsma and Van Durme (2013) report an accuracy of 87%, Bamman et al.
(2014) 88%).More significantly, the results suggest that 10.5% (automatic + majority) to 16% (majority) of the DutchTwitter users do not employ language that the crowd associates with their biological sex.
As said, thisraises the question of whether we can expect much higher performances by computational systems basedon only language use.Biological sexMale FemaleCrowdMale 82 16Female 16 86(a) Crowd (majority)CrowdMale FemaleAutomaticMale 68 22Female 30 80(b) Automatic vs crowdTable 3: Confusion matrices crowd predictionIndividual players versus an automatic system When considering players with 7 or more guesses,the average accuracy for a player is 0.71.
Our automatic system achieves an accuracy of 0.69.
The smallnumber of tweets per Twitter user in our data (20-40) makes it more difficult to automatically predictgender.Entropy We characterize the difficulty of inferring a user?s gender by calculating the entropy for eachTwitter user based on the gender guesses (Figure 4a).
We find that the difficulty varies widely acrossusers, and that there are no distinct groups of ?easy?
and ?difficult?
users.
However, we do observe aninteraction effect between the entropy of the gender guesses and the ages of the Twitter users.
At anaggregate level, we find no significant trend.
Analyzing females and males separately, we observe asignificant trend with females (Pearson?s r = 0.270, p < 0.01), suggesting that older female Twitter userstend to emphasize other aspects than their gender in tweets (as perceived by the crowd).PersonsEntropy0.20.61.00 50 100 150 200(a) Entropy over gender guesses05101520250.0 0.5 1.0Proportion of people that guessed maleFrequency BiologicalsexMaleFemale(b) A histogram of all Twitter users and the proportion ofplayers who guessed the users were male.
For example,there are 25 female users for which 10 - 20% of the playersguessed they were male.Figure 4: Gender prediction19554.2 Binarizing Gender, a Good Approach?Using data collected through the online game we quantitatively put speakers on a gender continuumbased on how their tweets are perceived by the crowd.
For each Twitter user, we calculate the proportionof players who guessed the users were male and female.
A plot is displayed in Figure 4b.
We can makethe following observations:First, the guesses by the players are based on their expectations about what kind of behaviour andlanguage is used by males and females.
The plot shows that for some users, almost all players guessedthe same gender, indicating that these expectations are quite strong and that there are stylistic markersand topics that the crowd strongly associates with males or females.Second, if treating gender as a binary variable is reasonable, we would expect to see two distinctgroups.
However, we observe quite an overlap between the biological males and females.
There are 1)users who conform to what is expected based on their biological sex, 2) users who deviate from what isexpected, 3) users whose tweets do not emphasize a gender identity or whose tweets have large variationusing language associated with both genders.
We investigated whether this is related to their use ofTwitter (professional, personal, or both), but the number of Twitter users in our dataset who used Twitterprofessionally was small and not sufficient to draw conclusions.We now illustrate our findings using examples.
The first example is a 15-year old biological femalefor who the crowd guessed most strongly that she is female (96% of n=220).
Three tweets from her areshown below.
She uses language typically associated with females, talking about spending time withher girlfriends and the use of stylistic markers such as hearts and alphabetical lengthening.
Thus, sheconforms strongly to what the crowd expects from her biological sex.Tweet 4: Gezellig bij Emily en Charlotte.Translation: Having fun with Emily and Charlotte.Tweet 5: Hiiiiii schatjesss!Translation: Hiiiiii cutiesss!Tweet 6: ?
@USERBelow are two tweets from a 40 year old biological female who does not employ linguistic markersstrongly associated with males or females.
Therefore, only 46% of the crowd (n=200) was able to guessthat she is female.Tweet 7: Ik viel op mijn bek.
En het kabinet ook.
Geinig toch?
#CatshuisTranslation: I went flat on my face.
And the cabinet as well.
Funny right?
#CatshuisTweet 8: Jeemig.
Ik kan het bijna niet volgen allemaal.Translation: Jeez.
I almost can?t follow it all.Twitter users vary in how much they emphasize their gender in their tweets.
As a result, the difficultyof inferring gender from tweets varies across persons, and treating gender as a binary variable ignoresmuch of the interesting variation within and between persons.Automatic system We now analyze whether an automatic system is capable of capturing the positionof Twitter users on the gender continuum (as perceived by the crowd).
We calculate the correlationbetween the proportion of male guesses (i.e.
the position on the gender continuum) and the scores ofthe logistic regression classifier: ?0+ x>?.
While the training data was binary (users were labeled asmale or female), a reasonable Spearman correlation of ?
= 0.584 (p < 0.001) was obtained between theclassifier score and the score based on the crowd?s perception.
We did not observe a significant relationbetween the score of the classifier (corresponding to the confidence of the gender prediction) and age.19565 AgeWe start with an analysis of task difficulty, by studying crowd performance on inferring age from tweets.Next, we show that it is particularly hard to accurately infer the chronological age of older Twitter usersfrom tweets.5.1 Task DifficultyThe crowd?s average guesses As with a system based on majority vote for gender prediction, we testthe performance of a system that predicts the ages of Twitter users based on the average of all guesses.We find that such a system achieves a Mean Absolute Error (MAE) of 4.844 years and a Pearson?scorrelation of 0.866.
Although the correlation is high, the absolute errors are quite large.
We find that thecrowd has difficulty predicting the ages of older Twitter users.
There is a positive correlation (Pearson?s?
= 0.789) between the absolute errors and the actual age of Twitter users.
There is a negative correlationbetween the errors (predicted - actual age) and the actual age of Twitter users (Pearson?s ?
= -0.872).We calculate the standard deviation over all the age guesses for a user (Figure 5a) to measure thedifficulty of inferring a user?s age.
There is a positive correlation between age and standard deviation ofthe guesses (?
= 0.691), which indicates that players have more difficulty in guessing the ages of olderTwitter users.Individual players versus an Automatic System To estimate the performance of individual players,we restrict our attention to players with at least 7 guesses.
We find that individual players are, on average,5.754 years off.
A linear regression system achieves a MAE of 6.149 years and a Pearson correlation of0.812.
The small number of tweets in our data (20-40) increases the difficulty of the task for automaticsystems.AgeStandard deviation24681010 20 30 40 50 60(a) Standard deviation and actual ageActual agePredicted age10203040506010 20 30 40 50 60Correct lineHuman prediction(b) Average age prediction by humans.Figure 5: Age prediction5.2 Inferring the Age of Older Twitter UsersFigure 5b shows the average player predictions with the actual age of the Twitter users.
The red line isthe ?perfect?
line, i.e.
the line when the predictions would match the exact age.
Black represents a fittedLOESS curve (Cleveland et al., 1992) based on the human predictions.
We find that the players tendto overpredict the age of younger Twitter users, but even more strikingly, on average they consistentlyunderpredict the age of older Twitter users.
The prediction errors already start at the end of the 20s, andthe gap between actual and predicted age increases with age.This could be explained by sociolinguistic studies that have found that people between 30 and 55 yearsuse standard forms the most, because they experience the maximum societal pressure in the workplace toconform (Holmes, 2013).
On Twitter, this has been observed as well: Nguyen et al.
(2013) found fewerlinguistic differences between older age groups than between younger age groups.
This makes it difficultfor the crowd to accurately estimate the ages of older Twitter users.
Younger people and retired peopleuse more non-standard forms (Holmes, 2013).
Unfortunately, our dataset does not contain enough retiredusers to analyze whether this trend is also present on Twitter.19576 DiscussionWe now discuss the implications of our findings for research on automatically predicting the gender andage of authors from their texts.Age and gender as social variables Most computational research has treated gender and age as fixed,biological variables.
The dominant approach is to use supervised machine learning methods to generalizeacross a large number of examples (e.g.
texts written by females and males).
While the learned modelsso far are effective at predicting age and gender of most people, they learn stereotypical behaviour andtherefore provide a simplistic view.First, by using the crowd we have shown that Twitter users emphasize their gender and age in varyingdegrees and in different ways, so that for example, treating gender as a binary variable is too simplistic(Butler, 1990; Eckert and McConnell-Ginet, 2013).
Many users do not employ the stereotypical languageassociated with their biological sex, making models that take a static view of gender ineffective for suchusers.
More detailed error analyses of the prediction systems will increase understanding of the reasonsfor incorrect predictions, and shed light on the relation between language use and social variables.Second, models that assume static variables will not be able to model the interesting variation (Eisen-stein, 2013).
Models that build on recent developments in sociolinguistics will be more meaningful andwill also have the potential to contribute to new sociolinguistic insights.
For example, modeling whatinfluences speakers to show more or less of their identity through language, or jointly modeling varia-tion between and within speakers, are in our opinion interesting research directions.
The ever increasingamounts of social media data offer opportunities to explore these research directions.Sampling We have shown that the difficulty of tasks such as gender and age prediction varies acrosspersons.
Therefore, creating datasets for such tasks requires maximum attention.
For example, whena dataset is biased towards people who show a strong gender identity (e.g.
by sampling followers ofaccounts highly associated with males or females, such as sororities (Rao et al., 2010)), the resultsobtained on such a set may not be representative of a more random set (as observed when classifyingpolitical affiliation (Cohen and Ruths, 2013)).Task difficulty Our study also raises the question of what level of performance can be obtained fortasks such as predicting gender and age from only language use.
Since we often form an impressionbased on someone?s writing, crowd performance is a good indicator of the task difficulty.
While thecrowd performance does not need to be the upper bound, it does indicate that it is difficult to predictgender and age of a large number of Twitter users.When taking the majority label, only 84% of the users were correctly classified according to theirbiological sex.
This suggests that about 16% of the Dutch Twitter users do not use language that thecrowd associates with their biological sex.We also found that it is hard to accurately estimate the ages of older Twitter users, and we relatedthis to sociolinguistics studies who found less linguistic differences in older age groups due to societalpressure in the workplace.Limitations A limitation of our work is that we focused on language variation between persons, and noton variation within persons.
However, speakers vary their language depending on the context and theirconversation partners (e.g.
accommodation effects were found in social media (Danescu-Niculescu-Mizilet al., 2011)).
For example, we assigned Twitter users an overall ?score?
by placing them on a gendercontinuum, ignoring the variation we find within users.Crowdsourcing as a tool to understand NLP tasks Most research on crowdsourcing within the NLPcommunity has focused on how the crowd can be used to obtain fast and large amounts of annotations.This study is an example of how the crowd can be used to obtain a deeper understanding of an NLP task.We expect that other tasks where disagreement between annotators is meaningful (i.e.
it is not only dueto noise), could potentially benefit from crowdsourcing experiments as well.19587 ConclusionIn this paper, we demonstrated the successful use of the crowd to study the relation between languageuse and social variables.
In particular, we took a closer look at inferring gender and age from languageusing data collected through an online game.
We showed that treating gender and age as fixed variablesignores the variety of ways people construct their identity through language.Approaching age and gender as social variables will allow for richer analyses and more robust systems.It has implications ranging from how datasets are created to how results are interpreted.
We expect thatour findings also apply to other social variables, such as ethnicity and status.
Instead of only focusing onperformance improvement, we encourage NLP researchers to also focus on what we can learn about therelation between language use and social variables using computational methods.AcknowledgementsThis research was supported by the Royal Netherlands Academy of Arts and Sciences (KNAW)and the Netherlands Organization for Scientific Research (NWO), grants IB/MP/2955 (TINPOT) and640.005.002 (FACT).
The third author is supported through the Digital Humanities research grant byTilburg University and a NIAS research fellowship.
The authors would like to thank the players of theTweetGenie game.ReferencesLora Aroyo and Chris Welty.
2013.
Crowd truth: Harnessing disagreement in crowdsourcing a relation extractiongold standard.
In Proceedings of WebSci?13.David Bamman, Jacob Eisenstein, and Tyler Schnoebelen.
2014.
Gender identity and lexical variation in socialmedia.
Journal of Sociolinguistics, 18(2):135?160.Federica Barbieri.
2008.
Patterns of age-based linguistic variation in American English.
Journal of Sociolinguis-tics, 12(1):58?88.Shane Bergsma and Benjamin Van Durme.
2013.
Using conceptual class attributes to characterize social mediausers.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages710?720.Victoria L. Bergvall, Janet M. Bing, and Alice F. Freed.
1996.
Rethinking Language and Gender Research: Theoryand Practice.
Routledge.Victoria L. Bergvall.
1999.
Toward a comprehensive theory of language and gender.
Language in society,28(02):273?293.Mary Bucholtz and Kira Hall.
2005.
Identity and interaction: A sociocultural linguistic approach.
Discoursestudies, 7(4-5):585?614.John D. Burger, John Henderson, George Kim, and Guido Zarrella.
2011.
Discriminating gender on Twitter.
InProceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1301?1309.Judith Butler.
1990.
Gender Trouble: Feminism and the Subversion of Identity.
Routledge.William S. Cleveland, Eric Grosse, and William M. Shyu.
1992.
Local regression models.
Statistical models in S,pages 309?376.Raviv Cohen and Derek Ruths.
2013.
Classifying political orientation on Twitter: It?s not easy!
In Proceedings ofthe Seventh International AAAI Conference on Weblogs and Social Media, pages 91?99.Maral Dadvar, Franciska de Jong, Roeland Ordelman, and Dolf Trieschnigg.
2012.
Improved cyberbullying de-tection using gender information.
In Proceedings of the Twelfth Dutch-Belgian Information Retrieval Workshop(DIR 2012), pages 23?25.Cristian Danescu-Niculescu-Mizil, Michael Gamon, and Susan Dumais.
2011.
Mark my words!
: linguistic styleaccommodation in social media.
In Proceedings of the 20th international conference on World Wide Web, pages745?754.1959Sebastian Deterding, Dan Dixon, Rilla Khaled, and Lennart Nacke.
2011.
From game design elements to game-fulness: Defining ?gamification?.
In Proceedings of the 15th International Academic MindTrek Conference:Envisioning Future Media Environments, pages 9?15.Penelope Eckert and Sally McConnell-Ginet.
2013.
Language and gender.
Cambridge University Press.Penelope Eckert.
1997.
Age as a sociolinguistic variable.
The handbook of sociolinguistics, pages 151?167.Penelope Eckert.
2008.
Variation and the indexical field.
Journal of Sociolinguistics, 12(4):453?476.Penelope Eckert.
2012.
Three waves of variation study: the emergence of meaning in the study of sociolinguisticvariation.
Annual Review of Anthropology, 41:87?100.Jacob Eisenstein.
2013.
What to do about bad language on the internet.
In Proceedings of the Annual Conferenceof the North American Chapter of the Association for Computational Linguistics, pages 359?369.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR: A libraryfor large linear classification.
Journal of Machine Learning Research, 9:1871?1874.Katja Filippova.
2012.
User demographics and language in an implicit social network.
In Proceedings of the 2012Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural LanguageLearning, pages 1478?1488.Clayton Fink, Jonathon Kopecky, and Maksym Morawski.
2012.
Inferring gender from the content of tweets:A region specific example.
In Proceedings of the Sixth International AAAI Conference on Weblogs and SocialMedia.Nikesh Garera and David Yarowsky.
2009.
Modeling latent biographic attributes in conversational genres.
InProceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International JointConference on Natural Language Processing of the AFNLP, pages 710?718.Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.
2009.
Stylometric analysis of bloggers?
age and gender.In Proceedings of the Third International ICWSM Conference, pages 214?217.Janet Holmes and Miriam Meyerhoff.
2003.
The handbook of language and gender.
Wiley-Blackwell.Janet Holmes.
2013.
An introduction to sociolinguistics.
Routledge.John Le, Andy Edmonds, Vaughn Hester, and Lukas Biewald.
2010.
Ensuring quality in crowdsourced searchrelevance evaluation: The effects of training question distribution.
In Proceedings of the SIGIR 2010 Workshopon Crowdsourcing for Search Evaluation (CSE 2010), pages 21?26.Judith Lorber.
1996.
Beyond the binaries: Depolarizing the categories of sex, sexuality, and gender*.
SociologicalInquiry, 66(2):143?160.Maxim Makatchev and Reid Simmons.
2011.
Perception of personality and naturalness through dialogues bynative speakers of American English and Arabic.
In Proceedings of the SIGDIAL 2011: the 12th AnnualMeeting of the Special Interest Group on Discourse and Dialogue, pages 286?293.Alice E. Marwick and danah boyd.
2011.
I tweet honestly, I tweet passionately: Twitter users, context collapse,and the imagined audience.
New Media & Society, 13(1):114?133.Arjun Mukherjee and Bing Liu.
2010.
Improving gender classification of blog authors.
In Proceedings of the2010 Conference on Empirical Methods in Natural Language Processing, pages 207?217.Robert Munro, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, TylerSchnoebelen, and Harry Tily.
2010.
Crowdsourcing and language studies: the new generation of linguisticdata.
In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?sMechanical Turk, pages 122?130.Dong Nguyen, Noah A Smith, and Carolyn P. Ros?e.
2011.
Author age prediction from text using linear regression.In Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences,and Humanities, pages 115?123.Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, and Theo Meder.
2013.
?How old do you think I am??
: A studyof language and age in Twitter.
In Proceedings of the Seventh International AAAI Conference on Weblogs andSocial Media, pages 439?448.1960Dong Nguyen, Dolf Trieschnigg, and Theo Meder.
2014.
Tweetgenie: Development, evaluation, and lessonslearned.
In Proceedings of COLING 2014.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-napeau, Matthieu Brucher, Matthieu Perrot, and?Edouard Duchesnay.
2011.
Scikit-learn: Machine learning inPython.
Journal of Machine Learning Research, 12:2825?2830.Claudia Peersman, Walter Daelemans, and Leona Van Vaerenbergh.
2011.
Predicting age and gender in online so-cial networks.
In Proceedings of the 3rd international workshop on Search and mining user-generated contents,pages 37?44.James W. Pennebaker and Lori D. Stone.
2003.
Words of wisdom: Language use over the life span.
Journal ofpersonality and social psychology, 85(2):291?301.Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta.
2010.
Classifying latent user attributesin Twitter.
In Proceedings of the 2nd international workshop on Search and mining user-generated contents,pages 37?44.Sara Rosenthal and Kathleen McKeown.
2011.
Age prediction in blogs: a study of style, content, and online be-havior in pre- and post-social media generations.
In Proceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics, pages 763?772.Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James W. Pennebaker.
2005.
Effects of age and genderon blogging.
In Proceedings of AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs,pages 199?205.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and Andrew Y. Ng.
2008.
Cheap and fast?but is it good?
:Evaluating non-expert annotations for natural language tasks.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pages 254?263.Svitlana Volkova, Theresa Wilson, and David Yarowsky.
2013.
Exploring demographic language variations toimprove multilingual sentiment analysis in social media.
In Proceedings of the 2013 Conference on EmpiricalMethods on Natural Language Processing, pages 1815?1827.Suzanne E. Wagner.
2012.
Age grading in sociolinguistic theory.
Language and Linguistics Compass, 6(6):371?382.1961
