LR RECURSIVE  TRANSIT ION NETWORKSFOR EARLEY AND TOMITA PARSINGMark PerlinSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213Internet: perlin@cs.cmu.eduABSTRACT*Efficient syntactic and semantic parsing forambiguous context-free languages are generallycharacterized as complex, specialized, highly formalalgorithms.
In fact, they are readily constructed fromstraightforward recursive Iransition etworks (RTNs).In this paper, we introduce LR-RTNs, and thencomputationally motivate a uniform progression frombasic LR parsing, to Earley's (chart) parsing,concluding with Tomita's parser.
These apparentlydisparate algorithms are unified into a singleimplementation, which was used to automaticallygenerate all the figures in this paper.1.
INTRODUCTIONAmbiguous context-free grammars (CFGs) arecurrently used in the syntactic and semanticprocessing of natural language.
For efficient parsing,two major computational methods are used.
The firstis Earley's algorithm (Earley, 1970), which mergesparse trees to reduce the computational dependenceon input sentence l ngth from exponential to cubiccost.
Numerous variations on Earley's dynamicprogramming method have developed into a family ofchart parsing (Winograd, 1983) algorithms.
Thesecond is Tomita's algorithm (Tomita, 1986), whichgeneralizes Knuth's (Knuth, 1965) and DeRemer's(DeRemer, 1971) computer language LR parsingtechniques.
Tomita's algorithm augments the LRparsing "set of items" construction with Earley'sideas.What is not currently appreciated is the continuitybetween these apparently distinct computationalmethods.?
Tomita has proposed (Tomita, 1985) constructinghis algorithm from Earley's parser, instead ofDeRemer's LR parser.
In fact, as we shall show,Earley's algorithm may be viewed as one formof LR parsing.?
Incremental constructions of Tomita's algorithm(Heering, Klint, and Rekers, 1990) maysimilarly be viewed as just one point along acontinuum of methods.
* This work was supported in part by grant R29LM 04707 from the National Library of Medicine,and by the Pittsburgh NMR Institute.The apparent distinctions between these relatedmethods follows from the distinct complex formaland mathematical apparati (Lang, 1974; Lang, 1991)currently employed to construct these CF parsingalgorithms.To effect a uniform synthesis of these methods, inthis paper we introduce LR Recursive TransitionNetworks (LR-RTNs) as a simpler framework onwhich to build CF parsing algorithms.
While RTNs(Woods, 1970) have been widely used in ArtificialIntelligence (AI) for natural language parsing, theirrepresentational advantages have not been fullyexploited for efficiency.
The LR-RTNs, however, areefficient, and shall be used to construct"(1) a nondeterministic parser,(2) a basic LR(0) parser,(3) Earley's algorithm (and the chart parsers), and(4) incremental nd compiled versions of Tomita'salgorithm.Our uniform construction has advantages over thecurrent highly formal, non-RTN-based, nonuniformapproaches toCF parsing:?
Clarity of algorithm construction, permitting LR,Earley, and Tomita parsers to be understood asa family of related parsing algorithm.?
Computational motivation and justification foreach algorithm in this family.?
Uniform extensibility of these syntactic methodsto semantic parsing.?
Shared graphical representations, useful inbuilding interactive programming environmentsfor computational linguists.?
Parallelization of these parsing algorithms.?
All of the known advantages of RTNs, togetherwith efficiencies of LR parsing.All of these improvements will be discussed in thepaper.2.
LR  RECURSIVE  TRANSIT IONNETWORKSA transition etwork is a directed graph, used as afinite state machine (Hopcroft and Ullman, 1979).The network's nodes or edges are labelled; in thispaper, we shall label the nodes.
When an inputsentence is read, state moves from node to node.
Asentence is accepted if reading the entire sentencedirects the network traversal so as to arrive at an98W( ( )(c))S(Start symbol S(A) (B) (O) (E)~ Nonterminalsymbol S) Rule #1md VPFigure 1.
Expanding Rule#l: S---~NP VP.
A.
Expanding the nonterminal symbol S. B.
Expanding the rule node forRule #1.
C. Expanding the symbol node VP.
D. Expanding the symbol node NP.
E, Expanding the start node S.accepting node.
To increase computational powerfrom regular languages to context-free languages,recursive transition etworks (RTNs) are introduced.instantiation of the Rule's chain indicates the partialprogress in sequencing the Rule's right-hand-sidesymbols.An RTN is a forest of disconnected transitionnetworks, each identified by a nonterminal label.
Allother labels are terminal abels.
When, in traversinga transition network, a nonterminal abel isencountered, control recursively passes to thebeginning of the correspondingly labelled transitionnetwork.
Should this labelled network besuccessfully traversed, on exit, control returns back tothe labelled calling node.The linear text of a context-free grammar can becast into an RTN structure (Perlin, 1989).
This isdone by expanding each grammar rule into a linearchain.
The top-down expansion amounts to a partialevaluation (Futamura, 1971) of the rule into acomputational expectation: an eventual bottom-updata-directed instantiation that will complete theexpansion.Figure 1, for example, shows the expansion of thegrammar rule #1 S---~NP VP.
First, the nonterminalS, which labels this connected component, isexpanded as a nonterminal node.
One method forrealizing this nonterminal node, is via Rule#l; its rulenode is therefore xpanded.
Rule#1 sets up theexpectation for the VP symbol node, which in turnsets up the expectation for the NP symbol node.
NP,the first symbol node in the chain, creates the startnode S. In subsequent processing, posting aninstance of this start symbol would indicate anexpectation to instantiate the entire chain of Rule#l,thereby detecting a nonterminal symbol S. PartialThe expansion i  Figure 1 constructs an LR-RTN.That is, it sets up a Left-to-fight parse of a Rightmostderivation.
Such derivations are developed in thenext Section.
As used in AI natural language parsing,RTNs have more typically been LL-RTNs, foreffecting parses of leftmost derivations (Woods,1970), as shown in Figure 2A.
(Other, more efficient,control structures have also been used (Kaplan,1973).)
Our shift from LL to LR, shown in Figure2B, uses the chain expansion to set up a subsequentdata-driven completion, thereby permitting reaterparsing efficiency.In Figure 3, we show the RTN expansion of thesimple grammar used in our first set of examples:S -+ NP  VPNP- - )N  i DNVP  --) V NP  .Chains that share identical prefixes are merged(Perlin, 1989) into a directed acyclic graph (DAG)(Aho, Hopcroft, and Ullman, 1983).
This makes ourRTN a forest of DAGs, rather than trees.
Forexample, the shared NP start node initiates the chainsfor Rules #2 and #3 in the NP component.In augmented recursive transition networks(ATNs) (Woods, 1970), semantic onstraints may beexpressed.
These constraints can employ casegrammars, functional grammars, unification, and soon (Winograd, 1983).
In our RTN formulation,semantic testing occurs when instantiating rule nodes:failing a constraint removes a parse from further( )(A)( )(B)Figure 2.
A.
An LL-RTN for S~NP VP.
This expansion does not set up an expectation for a data-driven leftwardparse.
B.
The corresponding LR-RTN.
The rightmost expansion sets up subsequent data-driven leftward parses.99processing.
This approach applies to every parsingalgorithm in this paper, and will not be discussedfurther.Figure 3.
The RTN of an entire grammar.
The threeconnected components correspond to the threenonterminals in the grammar.
Each symbol node inthe RTN denotes a subsequence originating from itslefimost start symbol.3.
NONDETERMINIST IC  DERIVAT IONSA grammar's RTN can be used as a template forparsing.
A sentence (the data) directs theinstantiation of individual rule chains into a parsetree.
The RTN instances exactly correspond toparse.tree nodes.
This is most easily seen withnondeterministic rightmost derivations.Given an input sentence of n words, we mayderive a sentence in the language with thenondeterministic algorithm (Perlin, 1990):Put an instance of nonterminalnode S into the last column.From right to left, for everycolumn :From top to bottom, within thecolumn :(i) Recursively expand thecolumn top-down bynondeterminist ic selection ofrule instances.
(2) Install the next (leftward)symbol instance.In substep (1), following selection, a rule node and itsimmediately downward symbol node are instantiated.The instantiation process creates a new object thatinherits from the template RTN node, addinginformation about column position and local linkconnections.For example, to derive "I Saw A Man" we wouldnondeterministically select and instantiate the correctrule choices #1, #4, #2, and #3, as in Figure 4.Following the algorithm, the derivation is (twodimensionally) top-down: top-to-bottom and right-to-left.
To actually use this nondeterministic derivationalgorithm to obtain all parses, one might enumerateand test all possible sequences of rules.
This,however, has exponential cost in n, the input size.
Amore efficient approach is to reverse the top-downderivation, and recursively generate the parse(s)bottom-up from the input data.
( )\[cJFigure 4.
The completed top-down derivation (parse-tree) of "I Saw A Man".
Each parse-tree symbolnode denotes a subsequence of a recognized RTNchain.
Rule #0 connects a word to its terminalsymbol(s).4.
BAS IC  LR(0)  PARSINGTo construct a parser, we reverse the above top-down nondeterministic derivation teChnique into abottom-up deterministic algorithm.
We first build aninefficient LR-parser, illustrating the reversal.
Forefficiency, we then introduce the Follow-Set, andmodify our parser accordingly.4.1 AN INEFFICIENT BLR(0) PARSERA simple, inefficient parsing algorithm forcomputing all possible parse-trees is:Put an instance of start node Sinto the 0 column.From left to right, for everycolumn :From bottom to top, within thecolumn :(i) In i t ia l ize the column wi ththe input word.
(2) Recurs ive ly  complete  thecolumn bot tom-up us ing theINSERT method.This reverses the derivation algorithm into bottom-upgeneration: bottom-to-top, and left-to-right.
In theinner loop, the Step (1) initialization isstraightforward; we elaborate Step (2).100Step (2) uses the following method (Perlin, 1991)to insert instances of RTN nodes:INSERT ( instance )(ASK instance(I) L ink up with predecessorinstances.
(2) Instal l  self.
(3) ENQUEUE successor instancesfor insertion.
}In (1), links are constructed between the instance andits predecessor instances.
In (2), the instancebecomes available for cartesian product formation.In (3), the computationally nontrivial step, theinstance nqueues any successor instances within itsown column.
Most of the INSERT action is done byinstances of symbol and rule RTN nodes.Using our INSERT method, a new symbolinstance in the parse-tree links with predecessorinstances, and installs itself.
If the symbol's RTNnode leads upwards to a rule node, one new ruleinstance successor isenqueued; otherwise, not.Rule instances enqueue their successors in a morecomplicated way, and may require cartesian productformation.
A rule instance must instantiate andenqueue all RTN symbol nodes from which theycould possibly be derived.
At most, this is the setSAME-LABEL(rule) ={ N ?
RTN I N is a symbol node, andthe label of N is identical to the label of therule's nonterminal successor node }.For every symbol node in SAME-LABEL(rule),instances may be enqueued.
If X ?
SAME-LABEL(rule) immediately follows a start node, i.e., itbegins a chain, then a single instance of it isenqueued.If Y e SAME-LABEL(rule) does not immediatelyfollow a start node, then more effort is required.
LetX be the unique RTN node to the left of Y. Everyinstantiated node in the parse tree is the root of somesubtree that spans an interval of the input sentence.Let the left border j be the position just to left of thisinterval, and k be the rightmost position, i.e., thecurrent column.Then, as shown in Figure 5, for every instance xof X currently in position j, an instance y (of Y) is avalid extension of subsequence x that has supportfrom the input sentence data.
The cartesian product{ x I x an instance of X in column j }x { rule instance}forms the set of all valid predecessor pairs for newinstances of Y.
Each such new instance y of Y isenqueued, with some x and the rule instance as itstwo predecessors.
Each y is a parse-tree noderepresenting further progress in parsing asubsequence.RTN chain - X - Y -x" y '~x'.
y'x, yposit ion ~ ai i' i" j kFigure 5.
The symbol node Y has a left neighborsymbol node X in the RTN.
The instance y of Y isthe root ofa parse-subtree that spans (j+l ak).Therefore, the rule instance r enqueues (at leasO allinstances of y, indexed by the predecessor p oduct:{ x in column j } ?
{r }.4.2.
US ING THE FOLLOW-SETAlthough a rule parse-node is restricted toenqueue successor instances of RTN nodes in SAME-LABEL(rule), it can be constrained further.Specifically, if the sentence data gives no evidencefor a parse-subtree, the associated symbol nodeinstance need never be generated.
This restrictioncan be determined column-by-column as the parsingprogresses.We therefore extend our bottom-up parsingalgorithm to:Put an instance of start node Sinto the 0 column.From left to right, for everycolumn:From bottom to top, with in  thecolumn :(I) In i t ia l ize the column withthe input word.
(2) Recurs ive ly  complete thecolumn bottom-up us ing theINSERT method.
(3) Compute the column's(rightward) Fol low-Set.With the addition of Step (3), this defines our BasicLR(O), or BLR(O), parser.
We now describe theFollow-Set.Once an RTN node X has been instantiated insome column, it sets up an expectation for?
The RTN node(s) Yg that immediately follow it;?
For each immediate follower Yg, all those RTNsymbol nodes Wg,h that initiate chains thatcould recursively ead up to Yg.This is the Follow-Set (Aho, Sethi, and Ullman,1986).
The Follow-Set(X) is computed irectly fromthe RTN by the recursion:101Follow-Set(X)LET Resu l t  c-For every unv is i ted  RTN node Yfo l lowing X:Resu l t  e- ( Y } toIF Y's label is a terminalsymbol,THEN O;ELSE Fo l low-Set  of thestart symbol of Y's labelReturn Resu l tAs is clear from the re, cursive definition,Follow-Set (tog {Xg}) = tog Follow-Set (Xg).Therefore, the Follow-Set of a column's symbolnodes can be deferred to Step (3) of the BLR(0)parsing algorithm, after the determination of all thenodes has completed.
By only recursing on unvisitednodes, this traversal of the grammar RTN has timecost O(IGI) (Aho, Sethi, and UUman, 1986), where IGI>_ IRTNI is the size of the grammar (or its RTNgraph).
A Follow-Set computation is illustrated inFigure 6.Figure 6.
The Follow-Set (highlighted in thedisplay) of RTN node V consists of the immediatelyfollowing nonterminal node NP, and the two nodesimmediately following the start NP node, D and N.Since D and N are terminal symbols, the traversalhalts.The set of symbol RTN nodes that a rule instancer spanning (j+l,k) can enqueue is therefore notSAME-LABEL(rule),but the possibly smaller set of RTN nodesSAME-LABEL(rule) n Follow-Set(j).To enqueue r's successors in INSERT,LET Nodes = SAME-LABEL(ru le)  rhFo l low-Set  (j) .For every RTN node Y in Nodes,create and enqueue all instancesy inY :Let X be the lef tward RTN symbolnode ne ighbor  of Y.Let PROD ={x I x an instance of X inco lumn j) x (r), if X exists;{r}, otherwise.Enqueue all members  of PROD asinstances of y.The cartesian product PROD is nonempty, since aninstantiated rule anticipates those elements of PRODmandated by Follow-Sets of preceding columns.
Thepruning of Nodes by the Follow-Set eliminates allbottom-up arsing that cannot lead to a parse-subtreeat column k.In the example in Figure 7, Rule instance r is inposition 4, with j=3 and k=4.
We have:SAME-LABEL(r) = {N 2, N 3 },i.e, the two symbol nodes labelled N in thesequences ofRules #2 and #3, shown in theLR-RTN of Figure 6.Follow-Set(3) =Follow-Set(I D2 })= {N21.Therefore, SAME-LABEL(r)c~Follow-Set(3) = {N2}.??
\[Figure 7.
Th~)) r\] srule instance r can only instantiate thesingle successor instance N2.
r uses the RTN to findthe left RTN neighbor D of N 2. r then computes thecartesian product of instance d with r as {d}x{r},generating the successor instance of N 2 shown.5.
EARLEY 'S  PARSING ALGORITHMNatural anguages such as English are ambiguous.A single sentence may have multiple syntacticstructures.
For example, extending our simplegrammar with rules accounting for Prepositions andPrepositional-Phrases (Tomita, 1986)S -9 S PPNP -9 NP PPPP -9 P NP,the sentence "I saw a man on the hill with a telescopethrough the window" has 14 valid derivations, Inparsing, separate reconstructions of these differentparses can lead to exponential cost.For parsing efficiency, partially constructedinstance-trees can be merged (Earley, 1970).
Asbefore, parse-node x denotes a point along a parse-sequence, say, v-w-x.
The left-border i of this parse-sequence is the left-border of the leftmost parse-nodein the sequence.
All parse-sequences of RTN symbolnode X that cover columns i+l through k may becollected into a single equivalence class X(i,k).
For102the purposes of (1) continuing with the parse and (2)disambiguating parse-trees, members of X(i,k) areindistinguishable.
Over an input sentence oflength n,there are therefore no more than O(n 2) equivalenceclasses of X.Suppose X precedes Y in the RTN.
When aninstance y of Y is added m position k, k.<_n, and thecartesian product is formed, there are only O(k 2)possible equivalence classes of X for y to combinewith.
Summing over all n positions, there are nomore than O(n 3) possible product formations with Yin parsing an entire sentence.Merging is effected by adding a MERGE step toINSERT:INSERT ( instance )(instance ~- MERGE (instance)ASK instance(1) Link up with predecessorinstances.
(2) Instal l  self.
(3) ENQUEUE successor instancesfor insertion.
}The parsing merge predicate considers twoinstantiated sequences quivalent when:(1) Their RTN symbol nodes X are the same.
(2) They are in the same column k.(3) They have identical left borders i.The total number of links formed by INSERT duringan entire parse, accounting for every grammar RTNnode, is O(n3)xO(IGI).
The chart parsers are a familyof algorithms that couple efficient parse-tree mergingwith various control organizations (Winograd, 1983).6.
TOMITA 'S  PARSING ALGORITHMIn our BLR(0) parsing algorithm, even withmerging, the Follow-Set is computed at everycolumn.
While this computation is just O(IGI), it canbecome a bottleneck with the very large grammarsused in machine translation.
By caching the requisiteFollow-Set computations into a graph, subsequentFollow-Set computation is reduced.
This incrementalconstruction is similar to (Heering, Klint, and Rekers,1990)'s, asymptotically constructing Tomita's all-paths LR parsing algorithm (Tomita, 1986).The Follow-Set cache (or LR-table) can bedynamically constructed by Call-Graph Caching(Perlin, 1989) during the parsing.
Every time aFollow-Set computation is required, it is looked up inthe cache.
When not present, the Follow-Set iscomputed and cached as a graph.Following DeRemer (DeRemer, 1971), eachcached Follow-Set node is finely partitioned, asneeded, into disjoint subsets indexed by the RTNlabel name, as shown in the graphs of Figure 8.
Thepartitioning reduces the cache size: instead ofallowing all possible subsets of the RTN, the cachegraph nodes contain smaller subsets of identicallylabelled symbol nodes.When a Follow-Set node has the same subset of(A)?
5I~-N ~V~3-'~D~4~N(!
))II \ [ i i - - - - I  P--2 P P- -5( P (~ !
L (~)  ( 1 V--3 - -4- -N?
\[ \] I (oFigure 8.
(A) A parse of "I Saw A Man" using the grammar in Oromita, 1986).
(B) The Follow-Set cachedynamically constructed uring parsing.
Each cache node represents a subset of RTN symbol nodes.
The numbersindicate order of appearance; the lettered nodes partition their preceding node by symbol name.
Since the cache wascreated on an as-needed basis, its shape parallels the shape of the parse-tree.
(C) Compressing the shape of (B).103P P PP ~ ~ P'-23~-- ~1 F~--51 1P-'I 3~-'D--'I 5 -N  1P'--I 9~.D--21--N(A)1 ~-~N ~V ~3/ - -~,  4 ~N(B)Figure 9.
The LR table cache graph when parsing "I Saw A Man On The Hill With A Telescope Through TheWindow" (A) without cache node merging, and (B) with merging.grammar symbol nodes as an already existingFollow-Set node, it is merged into the older node'sequivalence lass.
This avoids redundant expansions,without which the cache would be an infinite tree ofparse paths, rather than a graph.
A comparison isshown in Figure 9.
If the entire LR-table cache isneeded, an ambiguous entence containing allpossible lexical categories at each position can bepresented; convergence follows from the finiteness ofthe subset construction.7.
IMPLEMENTATION ANDCURRENT WORKWe have developed an interactive graphicalprogramming environment for constructing LR-parsers.
It uses the color MAC/II computer in theObject LISP extension of Common LISP.
Thesystem is built on CACHE TM (Perlin, ?
1990), ageneral Call-Graph Caching system for animating AIalgorithms.The RTNs are built from grammars.
A variety ofLR-RTN-based parsers, including BLR(0), with orwithout merging, and with or without Follow-Setcaching have been constructed.
Every algorithmdescribed in this paper is implemented.
Visualizationis heavily exploited.
For example, selecting an LR-table cache node will select all its members in theRTN display.
The graphical animation componentautomatically drew all the RTNs and parse-trees inthe Figures, and has generated color slides useful inteaching.Fine-grained parallel implementations of BLR(0)on the Connection Machine are underway to reducethe costly cartesian product step to constant time.
Weare also adding semantic constraints.8.
CONCLUSIONWe have introduced BLR(0), a simple bottom-upLR RTN-based CF parsing algorithm.
We explicitlyexpand grammars to RTNs, and only then constructour parsing algorithm.
This intermediate stepeliminates the complex algebra usually associatedwith parsing, and renders more transparent the closerelations between different parsers.Earley's algorithm is seen to be fundamentally anLR parser.
Earley's propose expansion step is arecursion analogous to our Follow-Set raversal of theRTN.
By explicating the LR-RTN graph in thecomputation, o other complex data structures arerequired.
The efficient merging is accomplished byusing an option available to BLR(0): merging parsenodes into equivalence lasses.Tomita's algorithm uses the cached LR Follow-Setoption, in addition to merging.
Again, by using theRTN as a concrete data structure, the technical featsassociated with Tomita's parser disappear.
His sharedpacked forest follows immediately from our mergeoption.
His graph stack and his parse forest are, forus, the same entity: the shared parse tree.
Even theLR table is seen to derive from this parsing activity,particularly with incremental construction from theRTN.104Bringing the RTN into parsing as an explicitrealization of the original grammar appears to be aconceptual nd implementational improvement overless uniform treatments.ACKNOWLEDGMENTSNumerous conversations with Jaime Carbonellwere helpful in developing these ideas.
I thank thestudents at CMU and in the Tools for Al tutorialwhose many questions helped clarify this approach.REFERENCESAho, A.V., Hopcrofl, J.E., and Ullman, J.D.1983.
Data Structures and Algorithms.
Reading, MA:Addison-Wesley.Aho, A.V., Sethi, R., and Ullman, J.D.
1986.Compilers: Principles, Techniques and Tools.Reading, MA: Addison-Wesley.DeRemer, F. 1971.
Simple LR(k) grammars.Communications of the ACM, 14(7): 453-460.Earley, J.
1970.
An Efficient Context-FreeParsing Algorithm.
Communications of the ACM,13(2): 94-102,Futamura, Y.
1971.
Partial evaluation ofcomputation process - an approach to a compiler-compiler.
Comp.
Sys.
Cont., 2(5): 45-50,Heering, J., Klint, P., and Rekers, J.
1990.Incremental Generation of Parsers.
IEEE Trans.Software Engineering, 16(12): 1344-1351.Hopcroft, J.E., and Ullman, J.D.
1979.Introduction to Automata Theory, Languages, andComputation.
Reading, Mass.
: Addison-Wesley.Kaplan, R.M.
1973.
A General SyntacticProcessor.
In Natural Language Processing, Rustin,R., ed., 193-241.
New York, NY: Algorithmics Press.Knuth, D.E.
1965.
On the Translation ofLanguages from Left to Right.
Information andControl, 8(6): 607-639.Lang, B.
1974.
Deterministic techniques forefficient non-deterministic parsers.
In Proc, SecondColloquium Automata, Languages and Programming,255-269.
l_xxx:kx, J., ed., (Lecture Notes in ComputerScience, vol.
14), New York: Springer-Verlag.Lang, B.
1991.
Towards a Uniform FormalFramework for Parsing.
In Current Issues in ParsingTechnology, Tomita, M., ed., 153-172.
Boston:Kluwer Academic Publishers.Perlin, M.W.
1989.
Call-Graph Caching:Transforming Programs into Networks.
In Proc.
ofthe Eleventh Int.
Joint Conf.
on Artificial Intelligence,122-128.
Detroit, Michigan, Morgan Kaufmann.Perlin, M.W.
1990.
Progress in Call-GraphCaching, Tech Report, CMU-CS-90-132, Carnegie-Mellon University.Perlin, M.W.
1991.
RETE and Chart Parsingfrom Bottom-Up Call-Graph Caching, submitted toconference, Carnegie Mellon University.Perlin, M.W.
?
1990.
CACHETS: a ColorAnimated Call-grapH Environment, ver.
1.3,Common LISP MACINTOSH Program, Pittsburgh,PA.Tomita, M. 1985.
An Efficient Context-FreeParsing Algorithm for Natural Languages.
InProceedings of the Ninth IJCAI, 756-764.
LosAngeles, CA,.Tomita, M. 1986.
Efficient Parsing for NaturalLanguage.
Kluwar Publishing.Winograd, T. 1983.
Language as a CognitiveProcess, Volume I: Syntax.
Reading, MA: Addison-Wesley.Woods, W.A.
1970.
Transition networkgrammars for natural language analysis.
Comm ACM,13(10): 591-606.105
