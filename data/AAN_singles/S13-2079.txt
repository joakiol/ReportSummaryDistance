Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 478?482, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsColumbia NLP: Sentiment Detection of Subjective Phrases in Social MediaSara RosenthalDepartment of Computer ScienceColumbia UniversityNew York, NY 10027, USAsara@cs.columbia.eduKathleen McKeownDepartment of Computer ScienceColumbia UniversityNew York, NY 10027, USAkathy@cs.columbia.eduAbstractWe present a supervised sentiment detectionsystem that classifies the polarity of subjec-tive phrases as positive, negative, or neutral.
Itis tailored towards online genres, specificallyTwitter, through the inclusion of dictionariesdeveloped to capture vocabulary used in on-line conversations (e.g., slang and emoticons)as well as stylistic features common to socialmedia.
We show how to incorporate thesenew features within a state of the art systemand evaluate it on subtask A in SemEval-2013Task 2: Sentiment Analysis in Twitter.1 IntroductionPeople use social media to write openly about theirpersonal experiences, likes and dislikes.
The follow-ing sentence from Twitter is a typical example: ?To-morrow I?m coming back from Barcelona...I don?twant!
:(((?.
The ability to detect the sentiment ex-pressed in social media can be useful for understand-ing what people think about the restaurants theyvisit, the political viewpoints of the day, and theproducts they buy.
These sentiments can be usedto provided targeted advertising, automatically gen-erate reviews, and make various predictions, such aspolitical outcomes.In this paper we develop a sentiment detection al-gorithm for social media that classifies the polarityof sentence phrases as positive, negative, or neutraland test its performance in Twitter through the par-ticipation in the expression level task (subtask A)of the SemEval-2013 Task 2: Sentiment Analysisin Twitter (Wilson et al 2013) which the authorshelped organize.
To do so, we build on previouswork on sentiment detection algorithms for the moreformal news genre, notably the work of Agarwal etal (2009), but adapt it for the language of social me-dia, in particular Twitter.
We show that exploitinglexical-stylistic features and dictionaries geared to-ward social media are useful in detecting sentiment.In this rest of this paper, we discuss related work,including the state of the art sentiment system (Agar-wal et al 2009) our method is based on, the lexiconswe used, our method, and experiments and results.2 Related WorkSeveral recent papers have explored sentiment anal-ysis in Twitter.
Go et al2009) and Pak andParoubek (2010) classify the sentiment of tweetscontaining emoticons using n-grams and POS.
Bar-bosa and Feng (2010) detect sentiment using a po-larity dictionary that includes web vocabulary andtweet-specific social media features.
Berminghamand Smeaton (2010) compare polarity detection intwitter to blogs and movie reviews using lexical fea-tures.
Agarwal et al2011) perform polarity senti-ment detection on the entire tweet using features thatare somewhat similar to ours: the DAL, lexical fea-tures (e.g.
POS and n-grams), social media features(e.g.
slang and hashtags) and tree kernel features.
Incontrast to this related work, our approach is gearedtowards predicting sentiment is at the phrase level asopposed to the tweet level.3 LexiconsSeveral lexicons are used in our system.
We use theDAL and expand it with WordNet, as it was used in478Corpus DALNNP(PostDAL)WordLength-eningWordNet Wiktionary EmoticonsPunctuation& NumbersNotCoveredTwitter - Train 42.9% 19.2% 1.4% 10.2% 12.7% 0.3% 1.5% 11.7%Twitter - Dev 57.3% 13.8% 1.1% 7.1% 12.2% 0.4% 2.7% 5.4%Twitter - Test 49.9% 15.6% 1.4% 9.6% 12.1% 0.5% 1.6% 9.3%SMS - Test 60.1% 3.6% 0.6% 7.9% 14.7% 0.6% 1.9% 10.3%Table 1: Coverage for each of the lexicons in the training and test corpora?s.the original work (Agarwal et al 2009), and expandit further to use Wiktionary and an emoticon lexicon.We consider proper nouns that are not in the DAL tobe objective.
We also shorten words that are length-ened to see if we can find the shortened version inthe lexicons (e.g.
sweeeet?
sweet).
The coverageof the lexicons for each corpus is shown in Table 1.3.1 DALThe Dictionary of Affect and Language (DAL)(Whissel, 1989) is an English language dictionaryof 8742 words built to measure the emotional mean-ing of texts.
In addition to using newswire, it wasalso built from individual sources such as interviewson abuse, students?
retelling of a story, and adoles-cent?s descriptions of emotions.
It therefore covers abroad set of words.
Each word is given three scores(pleasantness - also called evaluation (ee), active-ness (aa), and imagery (ii)) on a scale of 1 (low)to 3 (high).
We compute the polarity of a chunk inthe same manner as the original work (Agarwal etal., 2009), using the sum of the AE Space Score?s(|?ee2 + aa2|) of each word within the chunk.3.2 WordNetThe DAL does cover a broad set of words, but wewill still often encounter words that are not includedin the dictionary.
Any word that is not in the DALand is not a proper noun is accessed in WordNet(Fellbaum, 1998) 1 and, if it exists, the DAL scoresof the synonyms of its first sense are used in itsplace.
In addition to the original approach, if thereare no synonyms we look at the hypernym.
We thencompute the average scores (ee, aa, and ii) of all thewords and use that as the score for the word.1We cannot use SentiWordNet because we are interested inthe DAL scores3.3 WiktionaryWe use Wiktionary, an online dictionary, to supple-ment the common words that are not found in Word-Net and the DAL.
We first examine all ?form of?
re-lationships for the word such as ?doesnt?
is a ?mis-spelling of?
?doesn?t?, and ?tonite?
is an ?alternateform of?
?tonight?.
If no ?form of?
relationships ex-ist, we take all the words in the definitions that havetheir own Wiktionary page and look up the scoresfor each word in the DAL.
(e.g., the verb definitionfor LOL (laugh out loud) in Wiktionary is ?To laughout loud?
with ?laugh?
having its own Wiktionarydefinition; it is therefore looked up in the DAL andthe score for ?laugh?
is used for ?LOL?.)
We thencompute the average scores (ee, aa, and ii) of all thewords and use that as the score for the word.3.4 Emoticon Dictionaryemoticon :) :D <3 :( ;)definition happy laughter love sad winkTable 2: Popular emoticons and their definitionsWe created a simple lexicon to map commonemoticons to a definition in the DAL.
We looked atover 1000 emoticons gathered from several lists onthe internet2 and computed their frequencies withina LiveJournal blog corpus.
(In the future we wouldlike to use an external Twitter corpus).
We keptthe 192 emoticons that appeared at least once andmapped each emoticon to a single word definition.The top 5 emoticons and their definitions are shownin Table 2.
When an emoticon is found in a tweet welook up its definition in the DAL.4 MethodsWe run our data through several pre-processing stepsto preserve emoticons and expand contractions.
We2www.chatropolis.com, www.piology.org, en.wikipedia.org479General Social MediaFeature Example Feature ExampleCapital Words Hello Emoticons :)Out of Vocabulary duh Acronyms LOLPunctuation .
Repeated Questions ??
?Repeated Punctuation #@.
Exclamation Points !Punctuation Count 5 Repeated Exclamations !!!
!Question Marks ?
Word Lengthening sweeeetEllipses ... All Caps HAHAAvg Word Length 5 Links/Images www.url.comTable 3: List of lexical-stylistic features and examples.then pre-process the sentences to add Part-of-Speechtags (POS) and chunk the sentences using the CRFtagger and chunker (Phan, 2006a; Phan, 2006b).The chunker uses three labels, ?B?
(beginning), ?I?
(in), and ?O?
(out).
The ?O?
label tends to be ap-plied to punctuation which one typically wants toignore.
However, in this context, punctation can bevery important (e.g.
exclamation points, and emoti-cons).
Therefore, we append words/phrases taggedas O to the prior B-I chunk.We apply the dictionaries to the preprocessed sen-tences to generate lexical, syntactic, and stylisticfeatures.
All sets of features were reduced using chi-square in Weka (Hall et al 2009).4.1 Lexical and Syntactic FeaturesWe include POS tags and the top 500 n-gram fea-tures(Agarwal et al 2009).
We experimented withdifferent amounts of n-grams and found that morethan 500 n-grams reduced performance.The DAL and other dictionaries are used alongwith a negation state machine(Agarwal et al 2009)to determine the polarity for each word in the sen-tence.
We include all the features described in theoriginal system (Agarwal et al 2009).4.2 Lexical-Stylistic FeaturesWe include several lexical-stylistic features (see Ta-ble 3) that can occur in all datasets.
We divide thesefeatures into two groups, general: ones that arecommon across online and traditional genres, andsocial media: one that are far more common in on-line genres.
Examples of general style features areexclamation points and ellipses.
Examples of socialmedia style features are emoticons and word length-ening.
Word lengthening is a common phenomenonFigure 1: Percentage of lexical-stylistic features that arenegative (top), neutral (middle), and positive (bottom) inthe Twitter training corpus.in social media where letters are repeated to indi-cate emphasis (e.g.
sweeeet).
It is particularly com-mon in opinionated words (Brody and Diakopoulos,2011).
The count values of each feature was normal-ized by the number of words in the phrase.The percentage of lexical-stylistic features thatare positive/negative/neutral is shown in Figure 1.For example, emoticons tend to indicate a positivephrase in Twitter.
Each stylistic feature accounts forless than 2% of the sentence but at least one of thestylistic features exists in 61% of the Tweets.We also computed the most frequent emoticons(<3, :D), acronyms (lol), and punctuation symbols(#) within a subset of the Twitter training set andincluded those as additional features.5 Experiments and ResultsThis task was evaluated on the Twitter dataset pro-vided by Semeval-2013 Task 2, subtask A, which theauthors helped organize.
Therefore, a large portionof time was spent on creating the dataset.480Experiment Twitter SMSDev TestMajority 36.3 38.1 31.5Just DAL 70.1 72.3 67.1WordNet 72.2 73.6 67.7Wiktionary 72.8 73.7 68.7Style 71.5 73.7 69.7n-grams 75.2 75.7 72.5WordNet+Style 73.2 74.6 70.1Dictionaries+Style 74.0 75.0 70.2Dictionaries+Style+n-grams 75.8 77.6 73.3Table 4: Experiments using the Twitter corpus.
Resultsare shown using average F-measure of the positive andnegative class.
All experiments include the DAL.
Thedictionaries refer to WordNet, Wiktionary, and Emoticon.Style refers to Lexical-Stylistic features.
All results ex-ceed the majority baseline significantly.We ran all of our experiments in Weka (Hall etal., 2009) using Logistic Regression.
We also exper-imented with other learning methods but found thatthis worked best.
All results are shown using the av-erage F-measure of the positive and negative class.We tuned our system for Semeval-2013 Task 2,subtask A, using the provided development set andran it on the provided Twitter and SMS test data.Our results are shown in Table 4 with all resultsbeing statistically significant over a majority base-line.
We also use the DAL as a baseline to in-dicate how useful lexical-stylistic features (specifi-cally those geared towards social media) and the dic-tionaries are in improving the performance of sen-timent detection of phrases in online genres in con-trast to using just the DAL.
The results that are statis-tically significant (computed using the Wilcoxon?stest, p ?
.02) shown in bold.
Our best results foreach dataset include all features with an average F-measure of 77.6% and 73.3% for the Twitter andSMS test sets respectively resulting in a significantimprovement of more than 5% for each test set overthe DAL baseline.At the time of submission, we had not experi-mented with n-grams, and therefore chose the Dic-tionaries+Style system as our final version for theofficial run resulting in a rank of 12/22 (75% F-measure) for Twitter and 13/19 (70.2% F-measure)for SMS.
Our rank with the best system, which in-cludes n-grams, would remain the same for Twitter,but bring our rank up to 10/19 for SMS.We looked more closely at the impact of our newfeatures and as one would expect, feature selectionfound the general and social media style features(e.g.
emoticons, :(, lol, word lengthening) to be use-ful in Twitter and SMS data.
Using additional onlinedictionaries is useful in Twitter and SMS, which isunderstandable because they both have poor cover-age in the DAL and WordNet.
In all cases usingn-grams was the most useful which indicates thatcontext is most important.
Using Dictionaries andStyle in addition to n-grams did provide a signifi-cant improvement in the Twitter test set, but not inthe Twitter Dev and SMS test set.6 Conclusion and Future WorkWe have explored whether social media features,Wiktionary, and emoticon dictionaries positively im-pact the accuracy of polarity detection in Twitter andother online genres.
We found that social media re-lated features can be used to predict sentiment inTwitter and SMS.
In addition, Wiktionary helps im-prove the word coverage and though it does not pro-vide a significant improvement over WordNet, it canbe used in place of WordNet.
On the other hand, wefound that using the DAL and n-grams alone does al-most as well as the best system.
This is encouragingas it indicates that content is important and domainindependent sentiment systems can do a good job ofpredicting sentiment in social media.The results of the SMS messages dataset indicatethat even though the online genres are different, thetraining data in one online genre can indeed be usedto predict results with reasonable accuracy in theother online genre.
These results show promise forfurther work on domain adaptation across differentkinds of social media.7 AcknowledgementsThis research was partially funded by (a) the ODNI,IARPA, through the U.S. Army Research Lab and(b) the DARPA DEFT Program.
All statements offact, opinion or conclusions contained herein arethose of the authors and should not be construed asrepresenting the official views, policies, or positionsof IARPA, the ODNI, the Department of Defense, orthe U.S. Government.481ReferencesApoorv Agarwal, Fadi Biadsy, and Kathleen R. Mcke-own.
2009.
Contextual phrase-level polarity analysisusing lexical affect scoring and syntactic n-grams.
InProceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, EACL ?09, pages 24?32, Stroudsburg, PA, USA.Association for Computational Linguistics.Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,and Rebecca Passonneau.
2011.
Sentiment analy-sis of twitter data.
In Proceedings of the Workshopon Language in Social Media (LSM 2011), pages 30?38, Portland, Oregon, June.
Association for Computa-tional Linguistics.Luciano Barbosa and Junlan Feng.
2010.
Robust senti-ment detection on twitter from biased and noisy data.In COLING (Posters), pages 36?44.Adam Bermingham and Alan F. Smeaton.
2010.
Clas-sifying sentiment in microblogs: is brevity an advan-tage?
In Jimmy Huang, Nick Koudas, Gareth J. F.Jones, Xindong Wu, Kevyn Collins-Thompson, andAijun An, editors, CIKM, pages 1833?1836.
ACM.Samuel Brody and Nicholas Diakopoulos.
2011.Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!
using wordlengthening to detect sentiment in microblogs.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 562?570,Edinburgh, Scotland, UK., July.
Association for Com-putational Linguistics.Christiane Fellbaum, editor.
1998.
WordNet An Elec-tronic Lexical Database.
The MIT Press, Cambridge,MA ; London, May.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.Processing, pages 1?6.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18, November.Alexander Pak and Patrick Paroubek.
2010.
Twit-ter as a corpus for sentiment analysis and opinionmining.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Bente Maegaard, Joseph Mariani,Jan Odijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh Interna-tional Conference on Language Resources and Evalu-ation (LREC?10), Valletta, Malta, may.
European Lan-guage Resources Association (ELRA).Xuan-Hieu Phan.
2006a.
Crfchunker: Crf english phrasechunker.Xuan-Hieu Phan.
2006b.
Crftagger: Crf english phrasetagger.C.
M. Whissel.
1989.
The dictionary of affect in lan-guage.
In R. Plutchik and H. Kellerman, editors, Emo-tion: theory research and experience, volume 4, Lon-don.
Acad.
Press.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, AlanRitter, Sara Rosenthal, and Veselin Stoyanov.
2013.Semeval-2013 task 2: Sentiment analysis in twitter.In Proceedings of the 7th International Workshop onSemantic Evaluation.
Association for ComputationalLinguistics.482
