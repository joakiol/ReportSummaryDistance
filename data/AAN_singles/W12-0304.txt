Proceedings of the EACL 2012 Workshop on Computational Linguistics and Writing, pages 27?34,Avignon, France, April 23, 2012. c?2012 Association for Computational LinguisticsGoogle Books N-gram Corpus used as a Grammar CheckerRogelio Nazar Irene RenauUniversity Institute of Applied LinguisticsUniversitat Pompeu FabraRoc Boronat 13808018 Barcelona, Spain{rogelio.nazar,irene.renau}@upf.eduAbstractIn this research we explore the possibil-ity of using a large n-gram corpus (GoogleBooks) to derive lexical transition probabil-ities from the frequency of word n-gramsand then use them to check and suggest cor-rections in a target text without the need forgrammar rules.
We conduct several experi-ments in Spanish, although our conclusionsalso reach other languages since the proce-dure is corpus-driven.
The paper reportson experiments involving different typesof grammar errors, which are conductedto test different grammar-checking proce-dures, namely, spotting possible errors, de-ciding between different lexical possibili-ties and filling-in the blanks in a text.1 IntroductionThis paper discusses a series of early experimentson a methodology for the detection and correc-tion of grammatical errors based on co-occurrencestatistics using an extensive corpus of n-grams(Google Books, compiled by Michel et al, 2011).We start from two complementary assumptions:on the one hand, books are published accurately,that is to say, they usually go through differentphases of revision and correction with high stan-dards and thus a large proportion of these textscan be used as a reference corpus for inferring thegrammar rules of a language.
On the other hand,we hypothesise that with a sufficiently large cor-pus a high percentage of the information aboutthese rules can be extracted with word n-grams.Thus, although there are still many grammaticalerrors that cannot be detected with this method,there is also another important group which can beidentified and corrected successfully, as we willsee in Section 4.Grammatical errors are the most difficult andcomplex type of language errors, because gram-mar is made up of a very extensive number ofrules and exceptions.
Furthermore, when gram-mar is observed in actual texts, the panorama be-comes far more complicated, as the number ofexceptions grows and the variety and complexityof syntactical structures increase to an extent thatis not predicted by theoretical studies of gram-mar.
Grammar errors are extremely important,and the majority of them cannot be considered tobe performance-based because it is the meaningof the text and therefore, the success or failure ofcommunication, that is compromised.To our knowledge, no grammar book or dictio-nary has yet provided a solution to all the prob-lems a person may have when he or she writesand tries to follow the grammar rules of language.Doubts that arise during the writing process arenot always clearly associated to a lexical unit, orthe writer is not able to detect such an associa-tion, and this makes it difficult to find the solutionusing a reference book.In recent years, some advances have been madein the automatic detection of grammar mistakes(see Section 2).
Effective rule-based methodshave been reported, but at the cost of a very time-consuming task and with an inherent lack of flex-ibility.
In contrast, statistical methods are easierand faster to implement, as well as being moreflexible and adaptable.
The experiment we willdescribe in the following sections is the first partof a more extensive study.
Most probably, thelogical step to follow in order to continue sucha study will be a hybrid approach, based on both27statistics and rules.
Hence, this paper aims to con-tribute to the statistical approach applied to gram-mar checking.The Google Books N-gram Corpus is adatabase of n-grams of sequences of up to 5 wordsand records the frequency distribution of each unitin each year from 1500 onwards.
The bulk of thecorpus, however, starts from 1970, and that is theyear we took as a starting point for the materialthat we used to compile our reference corpus.The idea of using this database as a grammarchecker is to analyse an input text and detect anysequence of words that cannot be found in then-gram database (which only contains n-gramswith frequency equal to or greater than 40) and,eventually, to replace a unit in the text with onethat makes a frequent n-gram.
More specifically,we conduct four types of operations: acceptinga text and spotting possible errors; inflecting alemma into the appropriate form in a given con-text; filling-in the blanks in a text; and selecting,from a number of options, the most probable wordform for a given context.
In order to evaluate thealgorithm, we applied it to solve exercises from aSpanish grammar book and also tested the detec-tion of errors in a corpus of real errors made bysecond language learners.The paper is organised as follows: we first of-fer a brief description of related work, and thenexplain our methodology for each of the experi-ments.
In the next section, we show the evaluationof the results in comparison to the Microsoft Wordgrammar checker and, finally, we draw some con-clusions and discuss lines of future work.2 Related WorkRule-based grammar checking started in the1980s and crystallised in the implementation ofdifferent tools: papers by MacDonald (1983),Heidorn et al (1982) or Richardson and Braden-Harder (1988) describe some of them (see Lea-cock et al, 2010, for a state of the art relatedto studies focused on language learning).
Thisapproach has continued to be used until recently(see Arppe, 2000; Johannessen et al, 2002; andmany others) and is the basis of the work re-lated with the popular grammar checker in Mi-crosoft Word (different aspects of the tool aredescribed in Dolan et al, 1993; Jensen et al,1993; Gamon et al, 1997 and Heidorn, 2000:181-207, among others).
The knowledge-rich ap-proach needs mechanisms to take into account er-rors within a rigid system of rules, and thus differ-ent strategies were implemented to gain flexibility(Weischedel and Black, 1980; Douglas and Dale,1992; Schneider and McCoy, 1998 and others).Bolt (1992) and Kohut and Gorman (1995) eval-uated several grammar checkers available at thetime and concluded that, in general, none of theproposed strategies achieved high percentages ofsuccess.There are reasons to believe that the limita-tions of rule-based methods could be overcomewith statistical or knowledge-poor approaches,which started to be used for natural languageprocessing in the late 1980s and 1990s.
Atwell(1987) was among the first to use a statistical andknowledge-poor approach to detect grammaticalerrors in POS-tagging.
Other studies, such asthose by Knight and Chandler (1994) or Han etal.
(2006), for instance, proved more successfulthan rule-based systems in the task of detectingarticle-related errors.
There are also other studies(Yarowsky, 1994; Golding, 1995 or Golding andRoth, 1996) that report the application of deci-sion lists and Bayesian classifiers for spell check-ing; however, these models cannot be applied togrammar error detection.
Burstein et al (2004)present an idea similar to the present paper, sincethey use n-grams for grammar checking.
In theircase, however, the model is much more compli-cated since it uses a machine learning approachtrained on a corpus of correct English and usingPOS-tags bigrams as features apart from word bi-grams.
In addition, they use a series of statisticalassociation measures instead of using plain fre-quency.Other proposals of a similar nature are thosewhich use the web as a corpus (More?
et al,2004; Yin et al, 2008; Whitelaw et al, 2009), al-though the majority of these authors also applydifferent degrees of processing of the input text,such as lemmatisation, POS-tagging and chunk-ing.
Whitelaw et al (2009), working on spellchecking, are among the few who disregard ex-plicit linguistic knowledge.
Sjo?bergh (2009) at-tempted a similar approach for grammar check-ing in Swedish, but with modest results.
Nazar(in press) reports on an experiment where cor-pus statistics are used to solve a German-languagemultiple choice exam, the result being a scoresimilar to that of a native speaker.
The sys-28tem does not use any kind of explicit knowledgeof German grammar or vocabulary: answers arefound by simply querying a search engine and se-lecting the most frequent combination of words.The present paper is a continuation and extensionof that idea, now with a specific application tothe practical problem of checking the grammar oftexts in Spanish.In spite of decades of work on the subject ofgrammar-checking algorithms, as summarised inthe previous lines, the general experience withcommercial grammar checkers is still disappoint-ing, the most serious problem being that in thevast majority of cases errors in the analysed textsare left undetected.
We believe that, in this con-text, a very simple grammar checker based on cor-pus statistics could prove to be helpful, at least asa complement to the standard procedures.3 MethodologyIn essence, the idea for this experiment is rathersimple.
In all the operations, we contrast the se-quences of words as they are found in an inputtext with those recorded in Google?s database.
Inthe error detection phase, the algorithm will flagas an error any sequence of two words that is notfound in the database, unless either of the twowords is not found individually in the database,in which case the sequence is ignored.
The idea isthat in a correction phase the algorithm will out-put a ranked list of suggestions to replace each de-tected error in order to make the text match the n-grams of the database.
The following subsectionsoffer a detailed description of the methodology ofeach experiment.
For the evaluation, we testedwhether the algorithm could solve grammar exer-cises from a text-book (Montol?
?o, 2000), which isone of the most widely used Spanish text-booksfor academic writing for native speakers, cover-ing various topics such as pronouns, determiners,prepositions, verb tenses, and so on.
In addition,for error detection we used a corpus of L2 learners(Lozano, 2009).3.1 Error DetectionError detection is, logically, the first phase ofa grammar checking algorithm and, in practice,would be followed by some correction operation,such as those described in 3.2 to 3.4.
In the er-ror detection procedure, the algorithm accepts aninput sentence or text and retrieves the frequencyof all word types (of forms as they appear in thetext and not the lemmata) as well as all the dif-ferent bigrams as sequences of word forms, ex-cluding punctuation signs.
The output of this pro-cess is the same text with two different types offlags indicating, on the one hand, that a particularword is not found or is not frequent enough and,on the other hand, that a bigram is not frequent.The frequency threshold can be an arbitrary pa-rameter, which would measure the ?sensitivity?
ofthe grammar checker.
As already mentioned, theminimum frequency of Google n-grams is 40.As the corpus is very large, there are a largenumber of proper nouns, even names that are un-usual in Spanish.
For example, in the sentence En1988 Jack Nicholson, Helen Hunt y Kim Basingerrecibieron sendos Oscar (?In 1988 Jack Nichol-son, Helen Hunt and Kim Basinger each receivedone Oscar?
), bigrams such as y Kim or, of course,others like Jack Nicholson are considered frequentby the system because these actors are famous inthe Spanish context, but this is not the case forthe bigram Mart?
?n Fiz, belonging to another sen-tence, which is considered infrequent and treatedas an error (false positive), because the name ofthis Spanish athlete does not appear with suffi-cient frequency.
Future versions will address thisissue.3.2 Multiple Choice ExercisesIn this scenario, the algorithm is fed with a sen-tence or text which has a missing word and a se-ries of possibilities from which to decide the mostappropriate one for that particular context.For instance, given an input sentence such as Elcoche se precipito?
por *un,una* pendiente (?Thecar plunged down a slope?
), the algorithm hasto choose the correct option between un and una(i.e., the masculine and feminine forms of the in-definite article).Confronted with this input data, the algorithmcomposes different trigrams with each possibilityand one word immediately to the left and rightof the target position.
Thus, in this case, one ofthe trigrams would be por un pendiente and, sim-ilarly, the other would be por una pendiente.
Asin 3.1., the selection procedure is based on a fre-quency comparison of the trigrams in the n-gramdatabase, which in this case favours the first op-tion, which is the correct one.In case the trigram is not found in the database,29there are two back-off operations, consisting inseparating each trigram into two bigrams, with thefirst and second position in one case and the sec-ond and third in the other.
The selected optionwill be the one with the two bigrams that, addedtogether, have the highest frequency.3.3 InflectionIn this case, the exercise consists in selecting theappropriate word form of a given lemma in agiven context.
Thus, for instance, in another ex-ercise from Montol?
?o?s book, No le *satisfacer*en absoluto el acuerdo al que llegaron con sussocios alemanes (?
[He/She] is not at all satisfiedwith the agreement reached with [his/her] Ger-man partners?
), the algorithm has to select the cor-rect verbal inflection of the lemma satisfacer.This operation is similar to the previous one,the only difference being that in this case we usea lexical database of Spanish that allows us to ob-tain all the inflected forms of a given lemma.
Inthis case, then, the algorithm searches for the tri-gram le * en, where * is defined as all the inflec-tional paradigm of the lemma.3.4 Fill-in the blanksThe operation of filling-in the blank spaces ina sentence is another typical grammar exercise.In this case, the algorithm accepts an input sen-tence such as Los asuntos * ma?s preocupan a lasociedad son los relacionados con la econom?
?a(?The issues of greatest concern to society arethose related to the economy?
), from the samesource, and suggests a list of candidates.
As inthe previous cases, the algorithm will search for atrigram such as asuntos * ma?s, where the * wild-card in this case means any word, or more pre-cisely, the most frequent word in that position.
Inthe case of the previous example, which is an ex-ercise about relative pronouns, the most frequentword in the corpus and the correct option is que.4 Results and Evaluation4.1 Result of error detectionThe results of our experiments are summarisedin Table 1, where we distinguish between differ-ent types of grammar errors and correction opera-tions.
The table also offers a comparison of theperformance of the algorithm against MicrosoftWord 2007 with the same dataset.
In the first col-umn of the table we divide the errors into differ-ent types as classified in Montol?
?o?s book.
Perfor-mance figures are represented as usual in infor-mation retrieval (for details, see Manning et al,2008): the columns represent the numbers of truepositives (t p), which are those errors that were ef-fectively detected by each system; false negatives( f n) referring to errors that were not detected,and false positives ( f p), consisting in those casesthat were correct, but which the system wronglyflagged as errors.
These values allowed us to de-fine precision (P) as t p/(t p + f p), recall (R) ast p/(t p+ f n) and F1 as 2.P.R/(P+R).The algorithm detects (with a success rate of80.59%), for example, verbs with an incorrectmorphology, such as *apreto (instead of aprieto,?I press?).
Nevertheless, the system also makesmore interesting detections, such as the incorrectselection of the verb tense, which requires infor-mation provided by the context: Si os vuelve amolestar, no *volved a hablar con e?l (?If [he]bothers you again, do not talk to him again?).
Inthis sentence, the correct tense for the second verbis volva?is, as the imperative in negative sentencesis made with the subjunctive.
In the same way,it is possible to detect incorrect uses of the ad-jective sendos (?for each other?
), which cannot beput after the noun, among other particular con-straints: combinations such as *los sendos actores(?both actors?)
or *han cerrado filiales sendas(?they have closed both subsidiaries?)
are markedas incorrect by the system.In order to try to balance the bias inherent toa grammar text-book, we decided to replicate theexperiment with real errors.
The decision to ex-tract exercises from a grammar book was basedon the idea that this book would contain a di-verse sample of the most typical mistakes, andin this sense it is representative.
But as the ex-amples given by the authors are invented, theyare often uncommon and unnatural, and of coursethis frequently has a negative effect on perfor-mance.
We thus repeated the experiment us-ing sentences from the CEDEL2 corpus (Lozano,2009), which is a corpus of essays in Spanishwritten by non-native speakers with different lev-els of proficiency.For this experiment, we only used essays writ-ten by students classified as ?very advanced?.
Weextracted 65 sentences, each containing one error.30This Experiment Word 2007Type of error tp fn fp % P % R % F1 tp fn fp % P % R % F1gerund 9 8 9 50 52.94 51.42 9 8 1 90 52.94 66.66verb morphology 54 17 13 80.59 76.05 78.25 60 11 3 95.23 84.50 89.54numerals 4 9 7 36.36 30.76 33.32 6 7 0 100 46.15 63.15grammatical number 10 8 1 90.90 55.55 68.95 10 8 1 90.90 55.55 68.95prepositions 25 40 17 59.52 38.46 46.72 13 52 0 100 20 33.33adjective ?sendos?
5 0 1 83.33 100 90.90 1 4 0 100 20 33.33various 55 52 52 51.40 51.40 51.40 33 74 10 76.74 30.84 43.99total 162 134 100 61.83 54.72 58.05 132 164 15 89.79 44.59 59.58Table 1: Summary of the results obtained by our algorithm in comparison to Word 2007Since the idea was to check grammar, we only se-lected material that was orthographically correct,any minor typos being corrected beforehand.
Incomparison with the mistakes dealt with in thegrammar book, the kind of grammatical problemsthat students make are of course very different.The most frequent type of errors in this samplewere gender agreement (typical in students withEnglish as L1), lexical errors, prepositions andothers such as problems with pronouns or withtransitive verbs, among others.Results of this second experiment are sum-marised in Table 2.
Again, we compare perfor-mance against Word 2007 on the same dataset.
Inthe case of this experiment, lexical errors and gen-der agreement show the best performance becausethese phenomena appear at the bigram level, asin *Despue?s del boda (?after the wedding?)
whichshould be feminine (de la boda), or *una tranv?
?aele?ctrica (?electric tram?)
which should be mas-culine (un tranv??a).
But there are other caseswhere the error involves elements that are sep-arated from each other by long distances and ofcourse will not be solved with the type of strategywe are discussing, as in the case of *un pa?
?s dondeel estilo de vida es avanzada (?a country withan advanced lifestyle?
), where the adjective avan-zada is wrongly put in feminine when it should bemasculine (avanzado), because it modifies a mas-culine noun estilo.In general, results of the detection phase arefar from perfect but at least comparable to thoseachieved by Word in these categories.
The maindifference between the performance of the two al-gorithms is that ours tends to flag a much largernumber of errors, incurring in many false posi-tives and severely degrading performance.
Thebehaviour of Word is the opposite, it tends to flagfewer errors, thus leaving many errors undetected.It can be argued that, in a task like this, it is prefer-able to have false positives rather than false neg-atives, because the difficult part of producing atext is to find the errors.
However, a system thatproduces many false positives will lose the con-fidence of the user.
In any case, more importantthan a difference in precision is the fact that bothsystems tend to detect very different types of er-rors, which reinforces the idea that statistical al-gorithms could be a useful complement to a rule-based system.4.2 Result of multiple choice exerciseThe results of the multiple choice exercise in thebook are shown in Table 3.
Again, we comparedperformance with that achieved by Word.
In orderto make this program solve a multiple choice ex-ercise we submitted the different possibilities foreach sentence and checked whether it was able todetect errors in the wrong sentences and leave thecorrect ones unflagged.Results in this case are similar in generalto those reported in Section 4.1.
An exampleof a correct trial is with the fragment *el,la*ge?nesis del problema (?the genesis of the prob-lem?
), where the option selected by the algorithmis la ge?nesis (feminine gender).
In contrast, it isnot capable of giving the correct answer when thecontext is very general, such as in *los,las* pen-dientes son uno de los complementos ma?s vendi-dos como regalo (?Earrings are one of the acces-sories most frequently sold as a gift?
), in whichthe words to choose from are at the beginning ofthe sentence and they are followed by son (?theyare?
), which comes from ser, perhaps the mostfrequent and polysemous Spanish verb.
The cor-rect answer is los (masculine article), but the sys-tem offers the incorrect las (feminine) because ofthe polysemy of the word, since las pendientesalso exist, but means ?the slopes?
or even ?the onespending?.31This Experiment Word 2007Type of error tp fn fp % P % R % F1 tp fn fp % P % R % F1gender agreement 9 6 3 75 60 66.66 7 8 0 100 46.66 63.63lexical selection 16 10 4 80 61.53 69.56 4 22 0 100 15.38 26.66prepositions 2 11 2 50 15.38 23.52 0 13 0 0 0 0various 4 7 5 44.44 36.36 39.99 3 8 3 50 27.27 35.29total 31 34 17 64.58 47.69 54.86 14 51 3 82.35 21.53 34.14Table 2: Replication of the experiment with a corpus of non-native speakers (CEDEL2, Lozano, 2009)Trials This Experiment Word 2007Type of error Correct % P Correct % Padverbs 9 8 88.89 5 55.55genre 10 7 70.00 3 30confusion DO-IO 4 2 50.00 2 50Table 3: Solution of the multiple choice exercise4.3 Result of inflection exerciseResults in the case of the inflection exercise aresummarised in Table 4.
When giving verb forms,results are correct in 66.67% of the cases.
Forinstance, in the case of La mayor?
?a de la gente*creer* que... (?The majority of people thinkthat...?
), the correct answer is cree, among otherpossibilities such as creen (plural) or cre?
?a (past).But results are generally unsuccessful (22.22%)when choosing the correct tense, such as in thecase of Si el problema me *atan?er* a m?
?, ya hu-biera hecho algo para remediarlo (?If the prob-lem was of my concern, I would have alreadydone something to solve it?).
In this example, thecorrect verb tense is atan?era or atan?ese, both ofwhich are forms for the third person past subjunc-tive used in conditional clauses, but the systemgives atan?e, a correct form for the verb atan?erthat, nevertheless, cannot be used in this sentence.As it can be seen, the problem is extremely diffi-cult for a statistical procedure (there are around60 verb forms in Spanish), and this may explainwhy the results of this type of exercise were moredisappointing.Type of error Trials Correct % Pverb number 9 6 66.67verb tense 9 2 22.22Table 4: Results of the inflection exercise4.4 Result of filling-in the blanksWhen asked to restore a missing word in a sen-tence, the algorithm is capable of offering the cor-rect answer in cases such as El abogado * de-fendio?
al peligroso asesino... (?The lawyer -who-defended the dangerous murderer...?
), where themissing word is que.
Other cases were not solvedcorrectly, as the fragment * a?cida manzana (?theacid apple?
), because the bigram la a?cida is muchless frequent than lluvia a?cida, ?acid rain?, thewrong candidate proposed by the system.
Resultsof this exercise are summarised in Table 5.Type of error Trials Correct % Particles 7 4 57.14pronouns 7 3 42.86Table 5: Results of the fill-in-the-blank exercise5 Conclusions and Future WorkIn the previous sections we have outlined a firstexperiment in the detection of different typesof grammar errors.
In summary, the algorithmis able to detect difficult mistakes such as *in-formes conteniendo (instead of informes que con-ten?
?an ?reports that contained?
: a wrong use ofthe gerund) or *ma?scaras antigases (instead ofma?scaras antiga?s ?gas masks?, an irregular plu-ral), which are errors that were not detected byMS Word.One of the difficulties we found is that, despitethe fact that the corpus used is probably the mostextensive corpus ever compiled, there are bigramsthat are not present in it.
This is not surprising,since one of the functions of linguistic compe-tence is the capacity to represent and make com-prehensible strings of words which have neverbeen produced before.
Another problem is thatfrequency is not always useful for detecting mis-takes, because the norm can be very separatedfrom real use.
An example of this is that, in one ofthe error detection exercises, the system considers32that the participle fre?
?dos (?fried?)
is incorrect be-cause it is not in the corpus, but the participle isactually correct, even when the majority of speak-ers think that only the irregular form (frito) is nor-mative.
The opposite is also true: some incor-rect structures are very frequently used and manyspeakers perceive them as correct, such as ayernoche instead of ayer por la noche (?last night?
),or some very common Gallicisms such as *medi-das a tomar instead of medidas por tomar ?mea-sures to be taken?, or *asunto a discutir (?matterto discuss?)
which should be asunto para discutir.Several ideas have been put forward to addressthese difficulties in future improvements to thisresearch, such as the use of trigrams and longern-grams instead of only bigrams for error detec-tion.
POS-tagging and proper noun detection arealso essential.
Another possibility is to comple-ment the corpus with different Spanish corpora,including press articles and other sources.
Weare also planning to repeat the experiment witha new version of the n-gram database this timenot as plain word forms but as classes of ob-jects such that the corpus will have greater powerof generalisation.
Following another line of re-search that we have already started (Nazar andRenau, in preparation), we will produce clustersof words according to their distributional similar-ity, which will result in a sort of Spanish taxon-omy.
This can be accomplished because all thewords that represent, say, the category of vehi-cles are, in general, very similar as regards theirdistribution.
Once we have organised the lex-icon of the corpus into categories, we will re-place those words by the name of the categorythey belong to, for instance, PERSON, NUMBER,VEHICLE, COUNTRY, ORGANISATION, BEVER-AGE, ANIMAL, PLANT and so on.
By doing this,the Google n-gram corpus will be useful to repre-sent a much more diverse variety of n-grams thanthose it actually contains.
The implications of thisidea go far beyond the particular field of grammarchecking and include the study of collocationsand of predicate-argument structures in general.We could ask, for instance, which are the mosttypical agents of the Spanish verb disparar (toshoot).
Searching for the trigram los * dispararonin the database, we can learn, for instance, thatthose agents can be soldados (soldiers), espan?oles(Spaniards), guardias (guards), polic?
?as (police-men), can?ones (cannons), militares (the military),ingleses (the British), indios (indians) and so on.Such a line of study could produce interesting re-sults and greatly improve the rate of success ofour grammar checker.AcknowledgmentsThis research has been made possible thanksto funding from the Spanish Ministry of Sci-ence and Innovation, project: ?Agrupacio?nsema?ntica y relaciones lexicolo?gicas en eldiccionario?, lead researcher J. DeCesaris(HUM2009-07588/FILO); APLE: ?Procesosde actualizacio?n del le?xico del espan?ol a partirde la prensa?, 2010-2012, lead researcher: M.T.
Cabre?
(FFI2009-12188-C05-01/FILO) andFundacio?n Comillas in relation with the project?Diccionario de aprendizaje del espan?ol comolengua extranjera?.
The authors would like tothank the anonymous reviewers for their helpfulcomments, Cristo?bal Lozano for providing thenon-native speaker corpus CEDEL2, Mark An-drews for proofreading, the team of the CIBERHPC Platform of Universitat Pompeu Fabra(Silvina Re and Milton Hoz) and the people thatcompiled and decided to share the Google BooksN-gram corpus with the rest of the community(Michel et al, 2010).ReferencesAntti Arppe.
2000.
Developing a Grammar Checkerfor Swedish.
Proceedings of the Twelfth NordicConference in Computational Linguistics.
Trond-heim, Norway, pp.
5?77.Eric Steven Atwell.
1987.
How to Detect Grammati-cal Errors in a Text without Parsing it.
Proceedingsof the Third Conference of the European Associ-ation for Computational Linguistics, Copenhagen,Denmark, pp.
38?45.Philip Bolt.
1992.
An Evaluation of Grammar-Checking Programs as Self-help Learning Aids forLearners of English as a Foreign Language.
Com-puter Assisted Language Learning, 5(1):49?91.Jill Burstein, Martin Chodorow, Claudia Leacock.2004.
Automated Essay Evaluation: the CriterionWriting Service.
AI Magazine, 25(3):27?36.William B. Dolan, Lucy Vanderwende, Stephen D.Richardson.
1993.
Automatically Deriving Struc-tured Knowledge Base from On-Line Dictionaries.Proceedings of the Pacific ACL.
Vancouver, BC.Shona Douglas, Robert Dale.
1992.
Towards robustPATR.
Proceedings of the 15th International Con-ference on Computational Linguistics, Nantes, pp.468?474.33Michael Gamon, Carmen Lozano, Jessie Pinkham,Tom Reutter.
1997.
Practical Experience withGrammar Sharing in Multilingual NLP.
Proceed-ings of the Workshop on Making NLP Work.
ACLConference, Madrid.Andrew Golding.
1995.
A Bayesian Hybrid Methodfor Context Sensitive Spelling Correction.
Proceed-ings of the Third Workshop on Very Large Corpora,pp.
39?53.Andrew Golding, Dan Roth.
1996.
Applying Win-now to Context Sensitive Spelling Correction.
Pro-ceedings of the International Conference on Ma-chine Learning, pp.
182?190.Na-Rae Han, Martin Chodorow, Claudia Leacock.2006.
Detecting Errors in English Article Usage bynon-Native Speakers.
Natural Language Engineer-ing, 12(2), pp.
115?129.George E. Heidorn.
2000.
Intelligent writing assis-tance.
In Dale, R, Moisl H, Somers H, eds.
Hand-book of Natural Language Processing: Techniquesand Applications for the Processing of Language asText.
New York: Marcel Dekker.George E. Heidorn, Karen Jensen, Lance A. Miller,Roy J. Byrd, Martin Chodorow.
1982.
The EPIS-TLE text-critiquing system.
IBM Systems Journal,21, pp.
305?326.Karen Jensen, George E. Heidorn, Stephen Richard-son, eds.
1993.
Natural Language Processing: ThePNLP Approach.
Kluwer Academic Publishers.Jane Bondi Johannessen, Kristin Hagen, Pia Lane.2002.
The Performance of a Grammar Checkerwith Deviant Language Input.
Proceedings of the19th International Conference on ComputationalLinguistics.
Taipei, Taiwan, pp.
1?8.Kevin Knight, Ishwar Chandler.
1994.
AutomatedPostediting of Documents.
Proceedings of NationalConference on Artificial Intelligence, Seattle, USA,pp.
779?784.Gary F. Kohut, Kevin J. Gorman.
1995.
TheEffectiveness of Leading Grammar/Style SoftwarePackages in Analyzing Business Students?
Writing.Journal of Business and Technical Communication,9:341?361.Claudia Leacock, Martin Chodorow, Michael Gamon,Joel Tetreault.
2010.
Automated Grammatical Er-ror Detection for Language Learners.
USA: Mor-gal and Claypool.Cristo?bal Lozano.
2009.
CEDEL2: Corpus Escritodel Espan?ol L2.
In: Bretones Callejas, Carmen M.et al (eds) Applied Linguistics Now: Understand-ing Language and Mind.
Almer?
?a: Universidad deAlmer??a.
Almer?
?a, pp.
197?212.Nina H. Macdonald.
1983.
The UNIX Writer?s Work-bench Software: Rationale and Design.
Bell SystemTechnical Journal, 62, pp.
1891?1908.Chistopher D. Manning, Prabhakar Raghavan, Hin-rich Schu?tze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press.Jean-Baptiste Michel, Yuan Kui Shen, Aviva PresserAiden, Adrian Veres, Matthew K. Gray, The GoogleBooks Team, Joseph P. Pickett, Dale Hoiberg, DanClancy, Peter Norvig, Jon Orwant, Steven Pinker,Martin A. Nowak, Erez Lieberman Aiden.
2011.Quantitative Analysis of Culture Using Millions ofDigitized Books.
Science 331(6014), pp.
176?182.Estrella Montol?
?o, ed.
2000.
Manual pra?ctico de es-critura acade?mica.
Barcelona: Ariel.Joaquim More?, Salvador Climent, Antoni Oliver.2004.
A Grammar and Style Checker Based on In-ternet Searches.
Proceedings of LREC 2004, Lis-bon, Portugal.Rogelio Nazar.
In press.
Algorithm qualifies for C1courses in German exam without previous knowl-edge of the language: an example of how corpuslinguistics can be a new paradigm in Artificial In-telligence.
Proceedings of Corpus Linguistics Con-ference, Birmingham, 20-22 July 2011.Rogelio Nazar, Irene Renau.
In preparation.
A co-ocurrence taxonomy from a general language cor-pus.
Proceedings of the 15th EURALEX Interna-tional Congress, Oslo, 7-11 August 2012.Stephen Richardson, Lisa Braden-Harder.
1988.The Experience of Developing a Large-Scale Natu-ral Language Text Processing System: CRITIQUE.Proceedings of the Second Conference on AppliedNatural Language Processing (ANLC ?88).
ACL,Stroudsburg, PA, USA, pp.
195?202.David Schneider, Kathleen McCoy.
1998.
Recogniz-ing Syntactic Errors in the Writing of Second Lan-guage Learners.
Proceedings of the 36th AnnualMeeting of the ACL and 17th International Con-ference on Computational Linguistics, Montreal,Canada, pp.
1198?1204.Jonas Sjo?bergh.
2009.
The Internet as a Norma-tive Corpus: Grammar Checking with a Search En-gine.
Techical Report, Dept.
of Theoretical Com-puter Science, Kungliga Tekniska Ho?gskolan.Ralph M. Weischedel, John Black.
1980.Responding-to potentially unparseable sentences.American Journal of Computational Linguistics,6:97?109.Casey Whitelaw, Ben Hutchinson, Grace Y. Chung,Gerard Ellis.
2009.
Using the Web for LanguageIndependent Spell Checking and Autocorrection.Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, Singa-pore, pp.
890?899.David Yarowsky.
1994 Decision Lists for Lexi-cal Ambiguity Resolution: Application to AccentRestoration in Spanish and French.
Proceedings ofthe ACL Conference, pp.
88?95.Xing Yin, Jiangfeng Gao, William B. Dolan.
2008.A Web-based English Proofing System for Englishas a Second Language Users.
Proceedings of the3rd International Joint Conference on Natural Lan-guage Processing, Hyderabad, India, pp.
619?624.34
