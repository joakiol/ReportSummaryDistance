Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 10?19,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSimulating Language Change in the Presence of Non-Idealized SyntaxW.
Garrett MitchenerMathematics DepartmentDuke UniversityBox 90320Durham, NC 27708wgm@math.duke.eduAbstractBoth Middle English and Old French hada syntactic property called verb-second orV2 that disappeared.
In this paper de-scribes a simulation being developed toshed light on the question of why V2 isstable in some languages, but not oth-ers.
The simulation, based on a Markovchain, uses fuzzy grammars where speak-ers can use an arbitrary mixture of ideal-ized grammars.
Thus, it can mimic thevariable syntax observed in Middle En-glish manuscripts.
The simulation sup-ports the hypotheses that children use thetopic of a sentence for word order acqui-sition, that acquisition takes into accountthe ambiguity of grammatical informationavailable from sample sentences, and thatspeakers prefer to speak with more regu-larity than they observe in the primary lin-guistic data.1 IntroductionThe paradox of language change is that on the onehand, children seem to learn the language of theirparents very robustly, and yet for example, the En-glish spoken in 800 AD is foreign to speakers ofModern English, and Latin somehow diverged intonumerous mutually foreign languages.
A number ofmodels and simulations have been studied using his-torical linguistics and acquisition studies to build onone another (Yang, 2002; Lightfoot, 1999; Niyogiand Berwick, 1996).
This paper describes the ini-tial stages of a long term project undertaken in con-sultation with Anthony Kroch, designed to integrateknowledge from these and other areas of linguisticsinto a mathematical model of the entire history ofEnglish.
As a first step, this paper examines theverb-second phenomenon, which has caused somedifficulty in other simulations.
The history of En-glish and other languages requires simulated popu-lations to have certain long-term behaviors.
Assum-ing that syntax can change without a non-syntacticdriving force, these requirements place informativerestrictions on the acquisition algorithm.
Specifi-cally, the behavior of this simulation suggests thatchildren are aware of the topic of a sentence and useit during acquisition, that children take into accountwhether or not a sentence can be parsed by multiplehypothetical grammars, and that speakers are awareof variety in their linguistic environment but do notmake as much use of it individually.As discussed in (Yang, 2002) and (Kroch, 1989),both Middle English and Old French had a syntac-tic rule, typical of Germanic languages, known asverb-second or V2, in which top-level sentences arere-organized: The finite verb moves to the front, andthe topic moves in front of that.
These two lan-guages both lost V2 word order.
Yang (2002) alsostates that other Romance languages once had V2and lost it.
However, Middle English is the only Ger-manic language to have lost V2.A current hypothesis for how V2 is acquired sup-poses that children listen for cue sentences that can-not be parsed without V2 (Lightfoot, 1999).
Specifi-cally, sentences with an initial non-subject topic and10finite verb are the cues for V2:(1) [CP TopicXP CV [IP Subject .
.
.
]](2) [[On ?is g?r] wolde [?e king Stephne t?-cen.
.
.
]][[in this year] wanted [the king Stephenseize.
.
.
]]?During this year king Stephen wanted toseize.
.
.
?
(Fischer et al, 2000, p. 130)This hypothesis suggests that the loss of V2 can beattributed to a decline in cue sentences in speech.Once the change is actuated, feedback from thelearning process propels it to completion.Several questions immediately arise: Can the ini-tial decline happen spontaneously, as a consequenceof purely linguistic factors?
Specifically, can apurely syntactic force cause the decline of cue sen-tences, or must it be driven by a phonological ormorphological change?
Alternatively, given the ro-bustness of child language acquisition, must the ini-tial decline be due to an external event, such as con-tact or social upheaval?
Finally, why did MiddleEnglish and Old French lose V2, but not German,Yiddish, or Icelandic?
And what can all of this sayabout the acquisition process?Yang and Kroch suggest the following hypothesisconcerning why some V2 languages, but not all, areunstable.
Middle English (specifically, the southerndialects) and Old French had particular features thatobscured the evidence for V2 present in the primarylinguistic data available for children:?
Both had underlying subject-verb-object(SVO) word order.
For a declarative sentencewith topicalized subject, an SVO+V2 grammargenerates the same surface word order asan SVO grammar without V2.
Hence, suchsentences are uninformative as to whetherchildren should use V2 or not.
According toestimates quoted in (Yang, 2002) and (Light-foot, 1999), about 70% of sentences in modernV2 languages fall into this category.?
Both allowed sentence-initial adjuncts, whichcame before the fronted topic and verb.?
Subject pronouns were different from full NPsubjects in both languages.
In Middle English,subject pronouns had clitic-like properties thatcaused them to appear to the left of the finiteverb, thereby placing the verb in third position.Old French was a pro-drop language, so subjectpronouns could be omitted, leaving the verbfirst.The Middle English was even more complex due toits regional dialects.
The northern dialect was heav-ily influenced by Scandinavian invaders: Sentence-initial adjuncts were not used, and subject pronounswere treated the same as full NP subjects.Other Germanic languages have some of thesefactors, but not all.
For example, Icelandic has un-derlying SVO order but does not allow additionaladjuncts.
It is therefore reasonable to suppose thatthese confounds increase the probability that naturalvariation or an external influence might disturb theoccurrence rate of cue sentences enough to actuatethe loss of V2.An additional complication, exposed bymanuscript data, is that the population seemsto progress as a whole.
There is no indication thatsome speakers use a V2 grammar exclusively andthe rest never use V2, with the decline in V2 comingfrom a reduction in the number of exclusively V2speakers.
Instead, manuscripts show highly variablerates of use of unambiguously V2 sentences, sug-gesting that all individuals used V2 at varying rates,and that the overall rate decreased from generationto generation.
Furthermore, children seem to usemixtures of adult grammars during acquisition(Yang, 2002).
These features suggest that modelingonly idealized adult speech may not be sufficient;rather, the mixed speech of children and adults ina transitional environment is crucial to formulatinga model that can be compared to acquisition andmanuscript data.A number of models and simulations of languagelearning and change have been formulated (Niyogiand Berwick, 1996; Niyogi and Berwick, 1997;Briscoe, 2000; Gibson and Wexler, 1994; Mitchener,2003; Mitchener and Nowak, 2003; Mitchener andNowak, 2004; Komarova et al, 2001) based on thesimplifying assumption that speakers use one gram-mar exclusively.
Frequently, V2 can never be lost in11such simulations, perhaps because the learning al-gorithm is highly sensitive to noise.
For example,a simple batch learner that accumulates sample sen-tences and tries to pick a grammar consistent withall of them might end up with a V2 grammar on thebasis of a single cue sentence.The present work is concerned with developingan improved simulation framework for investigatingsyntactic change.
The simulated population consistsof individual simulated people called agents that canuse arbitrary mixtures of idealized grammars calledfuzzy grammars.
Fuzzy grammars enable the sim-ulation to replicate smooth, population-wide transi-tions from one dominant idealized grammar to an-other.
Fuzzy grammars require a more sophisticatedlearning algorithm than would be required for anagent to acquire a single idealized grammar: Agentsmust acquire usage rates for the different idealizedgrammars rather than a small set of discrete param-eter values.2 Linguistic specifics of the simulationThe change of interest is the loss of V2 in MiddleEnglish and Old French, in particular why V2 wasunstable in these languages but not in others.
There-fore, the idealized grammars allowed in this simu-lation will be limited to four: All have underlyingsubject-verb-object word order, and allow sentence-initial adjuncts.
The options are V2 or not, and pro-drop or not.
Thus, a grammar is specified by a pairof binary parameter values.
For simplicity, the pro-drop parameter as in Old French is used rather thantrying to model the clitic status of Middle Englishsubject pronouns.Sentences are limited to a few basic types ofdeclarative statements, following the degree-0 learn-ing hypothesis (Lightfoot, 1999): The sentence mayor may not begin with an adjunct, the subject maybe either a full noun phrase or a pronoun, and theverb may optionally require an object or a subject.A verb, such as rain, that does not require a subjectis given an expletive pronoun subject if the grammaris not pro-drop.
Additionally, either the adjunct, thesubject, or the object may be topicalized.
For a V2grammar, the topicalized constituent appears just be-fore the verb; otherwise it is indicated only by spo-ken emphasis.A fuzzy grammar consists of a pair of beta distri-butions with parameters ?
and ?, following the con-vention from (Gelman et al, 2004) that the densityfor Beta(?, ?)
isp(x) = ?(?
+ ?)?(?)?(?)x??1(1?
x)?
?1, 0 < x < 1.
(3)Each beta distribution controls one parameter in theidealized grammar.1 The special case of Beta(1, 1)is the uniform distribution, and two such distribu-tions are used as the initial state for the agent?s fuzzygrammar.
The density for Beta(1 + m, 1 + n) is abump with peak at m/(m + n) that grows sharperfor larger values of m and n. Thus, it incorporates anatural critical period, as each additional data pointchanges the mean less and less, while allowing forvariation in adult grammars as seen in manuscripts.To produce a sentence, an agent with fuzzy gram-mar (Beta(?1, ?1),Beta(?2, ?2)) constructs an ide-alized grammar from a pair of random parametersettings, each 0 or 1, selected as follows.
The agentpicks a random number Qj ?
Beta(?j , ?j), thensets parameter j to 1 with probability Qj and 0 withprobability 1 ?Qj .
An equivalent and faster opera-tion is to set parameter j to 1 with probability ?j and0 with probability 1?
?j , where ?j = ?j/(?j +?j)is the mean of Beta(?j , ?j).To learn from a sentence, an agent first con-structs a random idealized grammar as before.
If thegrammar can parse the sentence, then some of theagent?s beta distributions are adjusted to increase theprobability that the successful grammar is selectedagain.
If the grammar cannot parse the sentence,then no adjustment is made.
To adjust Beta(?, ?
)to favor 1, the agent increments the first parame-ter, yielding Beta(?
+ 1, ?).
To adjust it to favor0, the agent increments the second parameter, yield-ing Beta(?, ?
+ 1).Within this general framework, many variationsare possible.
For example, the initial state of anagent, the choice of which beta distributions to up-date for particular sentences, and the social structure(who speaks to who) may all be varied.1The beta distribution is the conjugate prior for usingBayesian inference to estimate the probability a biased coin willcome up heads: If the prior distribution is Beta(?, ?
), the pos-terior after m heads and n tails is Beta(?
+ m, ?
+ n).12The simulation in (Briscoe, 2002) also makes useof Bayesian learning, but within an algorithm forwhich learners switch abruptly from one idealizedgrammar to another as estimated probabilities crosscertain thresholds.
The smoother algorithm usedhere is preferable because children do not switchabruptly between grammars (Yang, 2002).
Further-more, this algorithm allows simulations to includechildren?s highly variable speech.
Children learn-ing from each other is thought be an important forcein certain language changes; for example, a recentchange in the Icelandic case system, known as da-tive sickness, is thought to be spreading through thismechanism.3 Adaptation for Markov chain analysisTo the learning model outlined so far, we add the fol-lowing restrictions.
The social structure is fixed in aloop: There are n agents, each of which converseswith its two neighbors.
The parameters ?j and ?jare restricted to be between 1 and N .
Thus, the pop-ulation can be in one of N 4n possible states, whichis large but finite.Time is discrete with each time increment rep-resenting a single sentence spoken by some agentto a neighbor.
The population is represented by asequence of states (Xt)t?Z.
The population is up-dated as follows by a transition function Xt+1 =?
(Xt, Ut) that is fed the current population state plusa tuple of random numbers Ut.
One agent is selecteduniformly at random to be the hearer.
With probabil-ity pr, that agent dies and is replaced by a baby in aninitial state (Beta(1, 1),Beta(1, 1)).
With probabil-ity 1 ?
pr, the agent survives and hears a sentencespoken by a randomly selected neighbor.Two variations of the learning process are ex-plored here.
The first, called LEARN-ALWAYS,serves as a base line: The hearer picks an idealizedgrammar according to its fuzzy grammar, and triesto parse the sentence.
If it succeeds, it updates anyone beta distribution selected at random in favor ofthe parameter that led to a successful parse.
If theparse fails, no update is made.
This algorithm is sim-ilar to Naive Parameter Learning with Batch (Yang,2002, p. 24), but adapted to learn a fuzzy grammarrather than an idealized grammar, and to update theagent?s knowledge of only one syntactic parameterat a time.The second, called PARAMETER-CRUCIAL, is thesame except that the parameter is only updated ifit is crucial to the parse: The agent tries to parsethe sentence with that parameter in the other set-ting.
If the second parse succeeds, then the param-eter is not considered crucial and is left unchanged,but if it fails, then the parameter is crucial and theoriginal setting is reinforced.
This algorithm buildson LEARN-ALWAYS by restricting learning to sen-tences that are more or less unambiguous cues forthe speaker?s setting for one of the syntactic param-eters.
The theory of cue-based learning assumes thatchildren incorporate particular features into theirgrammar upon hearing specific sentences that unam-biguously require them.
This process is thought tobe a significant factor in language change (Lightfoot,1999) as it provides a feedback mechanism: Once aparameter setting begins to decline, cues for it willbecome less frequent in the population, resulting infurther decline in the next generation.
A difficultywith the theory of cue-based learning is that it is un-clear what exactly ?unambiguous?
should mean, be-cause realistic language models generally have caseswhere no single sentence type is unique to a particu-lar grammar or parameter setting (Yang, 2002, p. 34,39).
The definition of a crucial parameter preservesthe spirit of cue-based learning while avoiding po-tential difficulties inherent in the concept of ?unam-biguous.
?These modifications result in a finite-state Markovchain with several useful properties.
It is irreducible,which means that there is a strictly positive proba-bility of eventually getting from any initial state toany other target state.
To see this, observe that thereis a tiny but strictly positive probability that in thenext several transitions, all the agents will die andthe following sentence exchanges will happen justright to bring the population to the target state.
ThisMarkov chain is also aperiodic, which means thatat any time t far enough into the future, there is astrictly positive probability that the chain will havereturned to its original state.
Aperiodicity is a con-sequence of irreducibility and the fact that there isa strictly positive probability that the chain does notchange states from one time step to the next.
Thathappens when a hearer fails to parse a sentence, forexample.
An irreducible aperiodic Markov chain al-13ways has a stationary distribution.
This is a proba-bility distribution on its states, normally denoted pi,such that the probability that Xt = x converges topi(x) as t ?
?
no matter what the initial state X0is.
Furthermore, the transition function preserves pi,which means that if X is distributed according to pi,then so is ?(X,U).
The stationary distribution rep-resents the long term behavior of the Markov chain.Agents have a natural partial ordering  definedby(Beta(?1, ?1),Beta(?2, ?2)) (Beta(?
?1, ??1),Beta(?
?2, ?
?2))if and only if?1 ?
?
?1, ?1 ?
?
?1, ?2 ?
?
?2, and ?2 ?
??2.
(4)This ordering means that the left-hand agent isslanted more toward 1 in both parameters.
Not allpairs of agent states are comparable, but there areunique maximum and minimum agent states underthis partial ordering,Amax = (Beta(N, 1),Beta(N, 1)),Amin = (Beta(1, N),Beta(1, N)),such that all agent states A satisfy Amax  A Amin.
Let us consider two population states X andY and denote the agents in X by Aj and the agentsin Y by Bj , where 1 ?
j ?
n. The populationstates may also be partially ordered, as we can defineX  Y to mean all corresponding agents satisfyAj  Bj .
There are also maximum and minimumpopulation states Xmax and Xmin defined by settingall agent states to Amax and Amin, respectively.A Markov chain is monotonic if the set of stateshas a partial ordering with maximum and minimumelements and a transition function that respects thatordering.
There is a perfect sampling algorithmcalled monotonic coupling from the past (MCFTP)that generates samples from the stationary distribu-tion pi of a monotonic Markov chain without requir-ing certain properties of it that are difficult to com-pute (Propp and Wilson, 1996).
The partial ordering on population states was constructed so that thisalgorithm could be used.
The transition function ?mostly respects this partial ordering, that is, if X Y , then with high probability ?
(X,U)  ?
(Y,U).This monotonicity property is why ?
was defined tochange only one agent per time step, and why thelearning algorithms change that agent?s knowledgeof at most one parameter per time step.
However,?
does not quite respect , because one can con-struct X , Y , and U such that X  Y but ?
(X,U)and ?
(Y,U) are not comparable.
So, MCFTP doesnot necessarily produce correctly distributed sam-ples.
However, it turns out to be a reasonable heuris-tic, and until further theory can be developed and ap-plied to this problem, it is the best that can be done.The MCFTP algorithm works as follows.
We sup-pose that (Ut)t?Z is a sequence of tuples of randomnumbers, and that (Xt)t?Z is a sequence of randomstates such that each Xt is distributed according to piand Xt+1 = ?
(Xt, Ut).
We will determine X0 andreturn it as the random sample from the distributionpi.
To determine X0, we start at time T < 0 with alist of all possible states, and compute their futuresusing ?
and the sequence of Ut.
If ?
has been cho-sen properly, many of these paths will converge, andwith any luck, at time 0 they will all be in the samestate.
If this happens, then we have found a time Tsuch that no matter what XT is, there is only onepossible value for X0, and that random state is dis-tributed according to pi as desired.
Otherwise, wecontinue, starting twice as far back at time 2T , andso on.
This procedure is generally impractical if thenumber of possible states is large.
However, if theMarkov chain is monotonic, we can take the short-cut of only looking at the two paths starting at Xmaxand Xmin at time T .
If these agree at time 0, then allother paths are squeezed in between and must agreeas well.4 TweakingSince this simulation is intended to be used to studythe loss of V2, certain long term behavior is desir-able.
Of the four idealized grammars available inthis simulation, three ought to be fairly stable, sincethere are languages of these types that have retainedthese properties for a long time: SVO (French,English), SVO+V2 (Icelandic), and SVO+pro-drop(Spanish).
The fourth, SVO+V2+pro-drop, ought tobe unstable and give way to SVO+pro-drop, sinceit approximates Old French before it changed.
Inany case, the population ought to spend most of itstime in states where most of the agents use one of14the four grammars predominantly, and neighboringagents should have similar fuzzy grammars.In preliminary experiments, the set of possiblesentences did not contain expletive subject pro-nouns, sentence initial adverbs, or any indication ofspoken stress.
Thus, the simulated SVO languagewas a subset of all the others, and SVO+pro-dropwas a subset of SVO+V2+pro-drop.
Consequently,the PARAMETER-CRUCIAL learning algorithm wasunable to learn either of these languages because thenon-V2 setting was never crucial: Any sentence thatcould be parsed without V2 could also be parsedwith it.
In later experiments, the sentences andgrammars were modified to include expletive pro-nouns, thereby ensuring that SVO is not a subset ofSVO+pro-drop or SVO+V2+pro-drop.
In addition,marks were added to sentences to indicate spokenstress on the topic.
In the simulated V2 languages,topics are always fronted, so such stress can onlyappear on the initial constituent, but in the simulatednon-V2 languages it can appear on any constituent.This modification ensures that no language withinthe simulation is a subset of any of the others.The addition of spoken stress is theoretically plau-sible for several reasons.
First, the acquisition ofword order and case marking requires children toinfer the subject and object of sample sentences,meaning that such thematic information is availablefrom context.
It is therefore reasonable to assumethat the thematic context also allows for inferenceof the topic.
Second, Chinese allows topics to bedropped where permitted by discourse, a feature alsoobserved in the speech of children learning English.These considerations, along with the fact that thesimulation works much better with topic markingsthan without, suggests that spoken emphasis on thetopic provides positive evidence that children use todetermine that a language is not V2.It turns out that the maximum value N allowedfor ?j and ?j must be rather large.
If it is toosmall, the population tends to converge to a satu-rated state where all the agents are approximatelyA?
= (Beta(N,N),Beta(N,N)).
This state repre-sents an even mixture of all four grammars and isclearly unrealistic.
To see why this happens, imag-ine a fixed linguistic environment and an isolatedagent learning from this environment with no birth-and-death process.
This process is a Markov chainwith a single absorbing state A?, meaning that oncethe learner reaches state A?
it cannot change to anyother state: Every learning step requires increasingone of the numerical parameters in the agent?s state,and if they are all maximal, then no further changecan take place.
Starting from any initial state, theagent will eventually reach the absorbing state.
Thenumber of states for an agent must be finite for prac-tical and theoretical reasons, but by making N verylarge, the time it takes for an agent to reach A?
be-comes far greater than its life span under the birth-and-death process, thereby avoiding the saturationproblem.
With pr = 0.001, it turns out that 5000 isan appropriate value for N , and effectively no agentscome close to saturation.After some preliminary runs, the LEARN-ALWAYS algorithm seemed to produce extremely in-coherent populations with no global or local con-sensus on a dominant grammar.
Furthermore,MCFTP was taking an extremely long time un-der the PARAMETER-CRUCIAL algorithm.
An ad-ditional modification was put in place to encour-age agents toward using predominantly one gram-mar.
The best results were obtained by modify-ing the speaking algorithm so that agents prefer tospeak more toward an extreme than the linguisticdata would indicate.
For example, if the data sug-gests that they should use V2 with a high probabilityof 0.7, then they use V2 with some higher probabil-ity, say, 0.8.
If the data suggests a low value, say 0.3,then they use an even lower value, say 0.2.
The orig-inal algorithm used the mean ?j of beta distributionBeta(?j , ?j) as the probability of using 1 for pa-rameter j.
The biased speech algorithm uses f(?j)instead, where f is a sigmoid functionf(?)
= 11 + exp(2k ?
4k?)
(5)that satisfies f(1/2) = 1/2 and f ?
(1/2) = k. Thenumerical parameter k can be varied to exagger-ate the effect.
This modification leads to some in-crease in coherence with the LEARN-ALWAYS al-gorithm; it has minimal effect on the samples ob-tained with the PARAMETER-CRUCIAL algorithm,however MCFTP becomes significantly faster.The biased speech algorithm can be viewed as asmoother form of the thresholding operation used in(Briscoe, 2002), discussed earlier.
An alternative in-15terpretation is that the acquisition process may in-volve biased estimates of the usage frequencies ofsyntactic constructions.
Language acquisition re-quires children to impose regularity on sample data,leading to creoles and regularization of vocabulary,for instance (Bickerton, 1981; Kirby, 2001).
Thisaddition to the simulation is therefore psychologi-cally plausible.5 ResultsIn all of the following results, the bound on ?j and?j is N = 5000, the sigmoid slope is k = 2, theprobability that an agent is replaced when selectedis pr = 0.001, and there are 40 agents in the pop-ulation configured in a loop where each agent talksto its two neighbors.
See Figure 1 for a key to thenotation used in the figures.First, let us consider the base line LEARN-ALWAYS algorithm.
Typical sample populations,such as the one shown in Figure 2, tend to be glob-ally and locally incoherent, with neighboring agentsfavoring completely different grammars.
The resultsare even worse without the biased speech algorithm.A sample run using the PARAMETER-CRUCIALlearning algorithm is shown in Figure 3.
This pop-ulation is quite coherent, with neighbors generallyfavoring similar grammars, and most speakers us-ing non-V2 languages.
Remember that the picturerepresents the internal data of each agent, and thattheir speech is biased to be more regular than theirexperience.
There is a region of SVO+V2 spanningthe second row, and a region of SVO+pro-drop onthe fourth row with some SVO+V2+pro-drop speak-ers.
Another sample dominated by V2 with largerregions of SVO+V2+pro-drop is shown in Figure 4.A third sample dominated by non-pro-drop speakersis shown in Figure 5.
The MCFTP algorithm startswith a population of all Amax and one of Amin andreturns a sample that is a possible future of both;hence, both V2 and pro-drop may be lost and gainedunder this simulation.In addition to sampling from the stationary distri-bution pi of a Markov chain, MCFTP estimates thechain?s mixing time, which is how large t must befor the distribution of Xt to be ?-close to pi (in totalvariation distance).
The mixing time is roughly howlong the chain must run before it ?forgets?
its initialstate.
Since this Markov chain is not quite mono-tonic, the following should be considered a heuristicback-of-the-napkin calculation for the order of mag-nitude of the time it takes for a linguistic environ-ment to forget its initial state.
Figures 3 and 4 require29 and 30 doubling steps in MCFTP, which indicatesa mixing time of around 228 steps of the Markovchain.
Each agent has a probability pr of dying andbeing replaced if it is selected.
Therefore, the proba-bility of an agent living to age m is (1?pr)mpr, witha mean of (1?pr)/pr .
For pr = 0.001, this gives anaverage life span of 999 listening interactions.
Eachagent is selected to listen or be replaced with proba-bility 1/40, so the average lifespan is approximately40, 000 steps of the Markov chain, which is between215 and 216.
Hence, the mixing time is on the orderof 228?16 = 4096 times the lifespan of an individualagent.
In real life, taking a lifespan to be 40 years,that corresponds to at least 160, 000 years.
Further-more, this is an underestimate, because true humanlanguage is far more complex and should have aneven longer mixing time.
Thus, this simulation sug-gests that the linguistic transitions we observe in reallife taking place over a few decades are essentiallytransient behavior.6 Discussion and conclusionWith reasonable parameter settings, populations inthis simulation are able to both gain and lose V2, animprovement over other simulations, including ear-lier versions of this one, that tend to always convergeto SVO+V2+pro-drop.
Furthermore, such changescan happen spontaneously, without an externally im-posed catastrophe.
The simulation does not give rea-sonable results unless learners can tell which com-ponent of a sentence is the topic.
Preliminary re-sults suggest that the PARAMETER-CRUCIAL learn-ing algorithm gives more realistic results than theLEARN-ALWAYS algorithm, supporting the hypoth-esis that much of language acquisition is based oncue sentences that are in some sense unambiguousindicators of the grammar that generates them.
Tim-ing properties of the simulation suggest that it takesmany generations for a population to effectively for-get its original state, suggesting that further researchshould focus on the simulation?s transient behaviorrather than on its stationary distribution.16In future research, this simulation will be ex-tended to include other possible grammars, partic-ularly approximations of Middle English and Ice-landic.
That should be an appropriate level of detailfor studying the loss of V2.
For studying the riseof V2, the simulation should also include V1 gram-mars as in Celtic languages, where the finite verbraises but the topic remains in place.
According toKroch (personal communication) V2 is thought toarise from V1 languages rather than directly fromSOV or SVO languages, so the learning algorithmshould be tuned so that V1 languages are more likelyto become V2 than non-V1 languages.The learning algorithms described here do not in-clude any bias in favor of unmarked grammaticalfeatures, a property that is thought to be necessaryfor the acquisition of subset languages.
One couldeasily add such a bias by starting newborns withnon-uniform prior information, such as Beta(1, 20)for example.
It is generally accepted that V2 ismarked based on derivational economy.2 Pro-drop ismore complicated, as there is no consensus on whichsetting is marked.3 The correct biases are not obvi-ous, and determining them requires further research.Further extensions will include more complexpopulation structure and literacy, with the goal ofeventually comparing the results of the simulation todata from the Pennsylvania Parsed Corpus of MiddleEnglish.ReferencesDerek Bickerton.
1981.
Roots of Language.
KaromaPublishers, Inc., Ann Arbor.E.
J. Briscoe.
2000.
Grammatical acquisition: Induc-tive bias and coevolution of language and the languageacquisition device.
Language, 76(2):245?296.E.
J. Briscoe.
2002.
Grammatical acquisition and lin-guistic selection.
In E. J. Briscoe, editor, LinguisticEvolution through Language Acquisition: Formal andComputational Models.
Cambridge University Press.2Although Hawaiian Creole English and other creoles fronttopic and wh-word rather than leaving them in situ, so it is un-clear to what degree movement is marked (Bickerton, 1981).3On one hand, English-speaking children go through a pe-riod of topic-drop before learning that subject pronouns areobligatory, suggesting some form of pro-drop is the default(Yang, 2002).
On the other hand, creoles are thought to rep-resent completely unmarked grammars, and they are generallynot pro-drop (Bickerton, 1981).Olga Fischer, Ans van Kemenade, Willem Koopman, andWim van der Wurff.
2000.
The Syntax of Early En-glish.
Cambridge University Press.Andrew Gelman, John B. Carlin, Hal S. Stern, and Don-ald B. Rubin.
2004.
Bayesian Data Analysis.
Chap-man & Hall/CRC, second edition.E.
Gibson and K. Wexler.
1994.
Triggers.
LinguisticInquiry, 25:407?454.Simon Kirby.
2001.
Spontaneous evolution of linguisticstructure: an iterated learning model of the emergenceof regularity and irregularity.
IEEE Transactions onEvolutionary Computation, 5(2):102?110.Natalia L. Komarova, Partha Niyogi, and Martin A.Nowak.
2001.
The evolutionary dynamics of gram-mar acquisition.
Journal of Theoretical Biology,209(1):43?59.Anthony Kroch.
1989.
Reflexes of grammar in patternsof language change.
Language Variation and Change,1:199?244.David Lightfoot.
1999.
The Development of Language:Acquisition, Changes and Evolution.
Blackwell Pub-lishers.W.
Garrett Mitchener and Martin A. Nowak.
2003.
Com-petitive exclusion and coexistence of universal gram-mars.
Bulletin of Mathematical Biology, 65(1):67?93,January.W.
Garrett Mitchener and Martin A. Nowak.
2004.Chaos and language.
Proceedings of the Royal Societyof London, Biological Sciences, 271(1540):701?704,April.
DOI 10.1098/rspb.2003.2643.W.
Garrett Mitchener.
2003.
Bifurcation analysis of thefully symmetric language dynamical equation.
Jour-nal of Mathematical Biology, 46:265?285, March.Partha Niyogi and Robert C. Berwick.
1996.
A languagelearning model for finite parameter spaces.
Cognition,61:161?193.Partha Niyogi and Robert C. Berwick.
1997.
A dynami-cal systems model for language change.
Complex Sys-tems, 11:161?204.James Gary Propp and David Bruce Wilson.
1996.
Ex-act sampling with coupled Markov chains and applica-tions to statistical mechanics.
Random Structures andAlgorithms, 9(2):223?252.Charles D. Yang.
2002.
Knowledge and Learning in Nat-ural Language.
Oxford University Press, Oxford.17SVO SVO+V2 SVO+pro-dropSVO+V2+pro-drop(Beta(?1,?1), Beta(?2,?2))x= ?1+?1?1 y= ?2+?2?2V2 pro-dropxyFigure 1: Key to illustrations.
Each agent is drawn as a box, with a dot indicating its fuzzy grammar.
Themeans of its beta distributions are used as the coordinates of the dot.
The distribution for the V2 parameteris used for the horizontal component, and the distribution for the pro-drop parameter is used for the verticalcomponent.
Agents using predominantly one of the four possible idealized grammars have their dot in oneof the corners as shown.Figure 2: A population of 40 under the LEARN-ALWAYS algorithm.
Each agent speaks to its neighbors, andthe population should be read left to right and bottom to top.
The rightmost agent in each row is neighborswith the leftmost agent in the next row up.
The bottom left agent is neighbors with the top right agent.18Figure 3: A population of 40 under the PARAMETER-CRUCIAL algorithm.
Each agent speaks to its neigh-bors, and the population should be read left to right and bottom to top.Figure 4: A population of 40 under the PARAMETER-CRUCIAL algorithm.
Each agent speaks to its neigh-bors, and the population should be read left to right and bottom to top.Figure 5: A population of 40 under the PARAMETER-CRUCIAL algorithm.
Each agent speaks to its neigh-bors, and the population should be read left to right and bottom to top.19
