Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1080?1089,Honolulu, October 2008. c?2008 Association for Computational LinguisticsLatent-Variable Modeling of String Transductionswith Finite-State Methods?Markus Dreyer and Jason R. Smith and Jason EisnerDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, MD 21218, USA{markus,jsmith,jason}@cs.jhu.eduAbstractString-to-string transduction is a central prob-lem in computational linguistics and naturallanguage processing.
It occurs in tasks as di-verse as name transliteration, spelling correc-tion, pronunciation modeling and inflectionalmorphology.
We present a conditional log-linear model for string-to-string transduction,which employs overlapping features over la-tent alignment sequences, and which learns la-tent classes and latent string pair regions fromincomplete training data.
We evaluate our ap-proach on morphological tasks and demon-strate that latent variables can dramaticallyimprove results, even when trained on smalldata sets.
On the task of generating mor-phological forms, we outperform a baselinemethod reducing the error rate by up to 48%.On a lemmatization task, we reduce the errorrates in Wicentowski (2002) by 38?92%.1 IntroductionA recurring problem in computational linguisticsand language processing is transduction of charac-ter strings, e.g., words.
That is, one wishes to modelsome systematic mapping from an input string x toan output string y.
Applications include:?
phonology: underlying representation ?
surfacerepresentation?
orthography: pronunciation?
spelling?
morphology: inflected form ?
lemma, or differ-ently inflected form?
fuzzy name matching (duplicate detection) andspelling correction: spelling?
variant spelling?This work was supported by the Human Language Tech-nology Center of Excellence and by National Science Founda-tion grant No.
0347822 to the final author.
We would also liketo thank Richard Wicentowski for providing us with datasets forlemmatization, and the anonymous reviewers for their valuablefeedback.?
lexical translation (cognates, loanwords, translit-erated names): English word?
foreign wordWe present a configurable and robust frameworkfor solving such word transduction problems.
Ourresults in morphology generation show that the pre-sented approach improves upon the state of the art.2 Model StructureA weighted edit distance model (Ristad and Yian-ilos, 1998) would consider each character in isola-tion.
To consider more context, we pursue a verynatural generalization.
Given an input x, we evalu-ate a candidate output y by moving a sliding windowover the aligned (x, y) pair.
More precisely, sincemany alignments are possible, we sum over all thesepossibilities, evaluating each alignment separately.1At each window position, we accumulate log-probability based on the material that appears withinthe current window.
The window is a few charac-ters wide, and successive window positions over-lap.
This stands in contrast to a competing approach(Sherif and Kondrak, 2007; Zhao et al, 2007)that is inspired by phrase-based machine translation(Koehn et al, 2007), which segments the input stringinto substrings that are transduced independently, ig-noring context.21At the other extreme, Freitag and Khadivi (2007) use noalignment; each feature takes its own view of how (x, y) relate.2We feel that this independence is inappropriate.
By anal-ogy, it would be a poor idea for a language model to score astring highly if it could be segmented into independently fre-quent n-grams.
Rather, language models use overlapping n-grams (indeed, it is the language model that rescues phrase-based MT from producing disjointed translations).
We believephrase-based MT avoids overlapping phrases in the channelmodel only because these would complicate the modeling ofreordering (though see, e.g., Schwenk et al (2007) and Casacu-berta (2000)).
But in the problems of section 1, letter reorderingis rare and we may assume it is local to a window.1080Figure 1: One of many possible alignment strings A forthe observed pair breaking/broke, enriched with latentstrings `1 and `2.
Observed letters are shown in bold.
Thebox marks a trigram to be scored.
See Fig.
2 for featuresthat fire on this trigram.Joint n-gram models over the input and output di-mensions have been used before, but not for mor-phology, where we will apply them.3 Most notableis the local log-linear grapheme-to-phoneme modelof Chen (2003), as well as generative models forthat task (Deligne et al (1995), Galescu and Allen(2001), Bisani and Ney (2002)).We advance that approach by adding new latentdimensions to the (input, output) tuples (see Fig.
1).4This enables us to use certain linguistically inspiredfeatures and discover unannotated information.
Ourfeatures consider less or more than a literal n-gram.On the one hand, we generalize with features thatabstract away from the n-gram window contents; onthe other, we specialize the n-gram with features thatmake use of the added latent linguistic structure.In section 5, we briefly sketch our framework forconcisely expressing and efficiently implementingmodels of this form.
Our framework uses familiarlog-linear techniques for stochastic modeling, andweighted finite-state methods both for implementa-tion and for specifying features.
It appears generalenough to cover most prior work on word transduc-tion.
We imagine that it will be useful for futurework as well: one might easily add new, linguisti-cally interesting classes of features, each class de-fined by a regular expression.2.1 Basic notationWe use an input alphabet ?x and output alphabet?y.
We conventionally use x ?
?
?x to denote theinput string and y ?
?
?y to denote the output string.3Clark (2001) does use pair HMMs for morphology.4Demberg et al (2007) similarly added extra dimensions.However, their added dimensions were supervised, not latent,and their model was a standard generative n-gram model whosegeneralization was limited to standard n-gram smoothing.There are many possible alignments between xand y.
We represent each as an alignment stringA ?
?
?xy, over an alignment alphabet of orderedpairs, ?xydef= ((?x ?
{})?
(?y ?
{}))?
{(, )}.For example, one alignment of x = breakingwith y = broke is the 9-character string A =(b,b)(r,r)(e,o)(a, )(k,k)(,e)(i, )(n, )(g, ).It is pictured in the first two lines of Fig.
1.The remainder of Fig.
1 shows how we intro-duce latent variables, by enriching the alignmentcharacters to be tuples rather than pairs.
Let ?def=(?xy ?
?`1 ?
?`2 ?
?
?
?
?
?`K ), where ?`i are al-phabets used for the latent variables `i.FSA and FST stand for ?finite-state acceptor?
and?finite-state transducer,?
while WFSA and WFSTare their weighted variants.
The ?
symbol denotescomposition.Let T be a relation and w a string.
We write T [w]to denote the image of w under T (i.e., range(w ?T )), a set of 0 or more strings.
Similarly, if W is aweighted language (typically encoded by a WFSA),we write W [w] to denote the weight of w in W .Let pix ?
??
?
?
?x denote the deterministic reg-ular relation that projects an alignment string to itscorresponding input string, so that pix[A] = x. Sim-ilarly, define piy ?
??
?
?
?y so that piy[A] = y. LetAxy be the set of alignment strings A compatiblewith x and y; formally, Axydef= {A ?
??
: pix[A] =x?piy[A] = y}.
This set will range over all possiblealignments between x and y, and also all possibleconfigurations of the latent variables.2.2 Log-linear modelingWe use a standard log-linear model whose featuresare defined on alignment strings A ?
Axy, allow-ing them to be sensitive to the alignment of x and y.Given a collection of features fi : ??
?
R with as-sociated weights ?i ?
R, the conditional likelihoodof the training data isp?
(y | x) =?A?Axy exp?i ?ifi(A)?y?
?A?Axy?exp?i ?ifi(A)(1)Given a parameter vector ?, we compute equa-tion (1) using a finite-state machine.
We define aWFSA,U?, such thatU?
[A] yields the unnormalizedprobability u?
(A)def= exp?i ?ifi(A) for any A ???.
(See section 5 for the construction.)
To obtain1081the numerator of equation (1), with its?A?Axy , wesum over all paths in U?
that are compatible with xand y.
That is, we build x ?
pi?1x ?
U?
?
piy ?
y andsum over all paths.
For the denominator we build thelarger machine x ?
pi?1x ?
U?
and again compute thepathsum.
We use standard algorithms (Eisner, 2002)to compute the pathsums as well as their gradientswith respect to ?
for optimization (section 4.1).Below, we will restrict our notion of valid align-ment strings in ??.
U?
is constructed not to acceptinvalid ones, thus assigning them probability 0.Note that the possible output strings y?
in the de-nominator in equation (1) may have arbitrary length,leading to an infinite summation over alignmentstrings.
Thus, for some values of ?, the sum inthe denominator diverges and the probability dis-tribution is undefined.
There exist principled waysto avoid such ?
during training.
However, in ourcurrent work, we simply restrict to finitely manyalignment strings (given x), by prohibiting as invalidthose with > k consecutive insertions (i.e., charac-ters like (,a)).5 Finkel et al (2008) and others havesimilarly bounded unary rule cycles in PCFGs.2.3 Latent variablesThe alignment between x and y is a latent ex-planatory variable that helps model the distributionp(y | x) but is not observed in training.
Other latentvariables can also be useful.
Morphophonologicalchanges are often sensitive to phonemes (whereas xand y may consist of graphemes); syllable bound-aries; a conjugation class; morpheme boundaries;and the position of the change within the form.Thus, as mentioned in section 2.1, we enrich thealignment string A so that it specifies additional la-tent variables to which features may wish to refer.In Fig.
1, two latent strings are added, enabling thefeatures in Fig.
2(a)?(h).
The first character is not5We set k to a value between 1 and 3, depending on the tasks,always ensuring that no input/output pairs observed in trainingare excluded.
The insertion restriction does slightly enlarge theFSA U?
: a state must keep track of the number of consecutive symbols in the immediately preceding x input, and for a fewstates, this cannot be determined just from the immediately pre-ceding (n ?
1)-gram.
Despite this, we found empirically thatour approximation is at least as fast as the exact method of Eis-ner (2002), who sums around cyclic subnetworks to numericalconvergence.
Furthermore, our approximation does not requireus to detect divergence during training.Figure 2: The boxes (a)-(h) represent some of the featuresthat fire on the trigram shown in Fig.
1.
These features areexplained in detail in section 3.just an input/output pair, but the 4-tuple (b,b,2,1).Here, `1 indicates that this form pair (breaking /broke) as a whole is in a particular cluster, or wordclass, labeled with the arbitrary number 2.
Notice inFig.
1 that the class 2 is visible in all local windowsthroughout the string.
It allows us to model how cer-tain phenomena, e.g.
the vowel change from ea too, are more likely in one class than in another.
Formpairs in the same class as the breaking / broke ex-ample might include the following Germanic verbs:speak, break, steal, tear, and bear.Of course, word classes are latent (not labeled inour training data).
Given x and y, Axy will includealignment strings that specify class 1, and othersthat are identical except that they specify class 2;equation (1) sums over both possibilities.6 In a validalignment stringA, `1 must be a constant string suchas 111... or 222..., as in Fig.
1, so that it spec-ifies a single class for the entire form pair.
See sec-tions 4.2 and 4.3 for examples of what classes werelearned in our experiments.The latent string `2 splits the string pair into num-bered regions.
In a valid alignment string, the re-gion numbers must increase throughout `2, althoughnumbers may be skipped to permit omitted regions.To guide the model to make a useful division intoregions, we also require that identity characters suchas (b,b) fall in even regions while change charac-ters such as (e,o) (substitutions, deletions, or inser-6The latent class is comparable to the latent variable on thetree root symbol S in Matsuzaki et al (2005).1082tions) fall in odd regions.7 Region numbers must notincrease within a sequence of consecutive changesor consecutive identities.8 In Fig.
1, the start of re-gion 1 is triggered by e:o, the start of region 2 bythe identity k:k, region 3 by :e.Allowing region numbers to be skipped makes itpossible to consistently assign similar labels to sim-ilar regions across different training examples.
Ta-ble 2, for example, shows pairs that contain a vowelchange in the middle, some of which contain an ad-ditional insertion of ge in the begining (verbinden/ verbunden, reibt / gerieben).
We expect the modelto learn to label the ge insertion with a 1 and vowelchange with a 3, skipping region 1 in the exampleswhere the ge insertion is not present (see section4.2, Analysis).In the next section we describe features over theseenriched alignment strings.3 FeaturesOne of the simplest ways of scoring a string is an n-gram model.
In our log-linear model (1), we includengram features fi(A), each of which counts the oc-currences in A of a particular n-gram of alignmentcharacters.
The log-linear framework lets us includengram features of different lengths, a form of back-off smoothing (Wu and Khudanpur, 2000).We use additional backoff features on alignmentstrings to capture phonological, morphological, andorthographic generalizations.
Examples are found infeatures (b)-(h) in Fig.
2.
Feature (b) matches voweland consonant character classes in the input andoutput dimensions.
In the id/subst ngram feature,we have a similar abstraction, where the characterclasses ins, del, id, and subst are defined over in-put/output pairs, to match insertions, deletions, iden-tities (matches), and substitutions.In string transduction tasks, it is helpful to in-clude a language model of the target.
While thiscan be done by mixing the transduction model witha separate language model, it is desirable to in-clude a target language model within the transduc-7This strict requirement means, perhaps unfortunately, that asingle region cannot accommodate the change ayc:xyz unlessthe two y?s are not aligned to each other.
It could be relaxed,however, to a prior or an initialization or learning bias.8The two boundary characters #, numbered 0 and max(max=6 in our experiments), are neither changes nor identities.tion model.
We accomplish this by creating targetlanguage model features, such as (c) and (g) fromFig.
2, which ignore the input dimension.
We alsohave features which mirror features (a)-(d) but ig-nore the latent classes and/or regions (e.g.
features(e)-(h)).Notice that our choice of ?
only permits mono-tonic, 1-to-1 alignments, following Chen (2003).We may nonetheless favor the 2-to-1 alignment(ea,o) with bigram features such as (e,o)(a,).
A?collapsed?
version of a feature will back off fromthe specific alignment of the characters within a win-dow: thus, (ea,o) is itself a feature.
Currently, weonly include collapsed target language model fea-tures.
These ignore epsilons introduced by deletionsin the alignment, so that collapsed ok fires in a win-dow that contains ok.4 ExperimentsWe evaluate our model on two tasks of morphol-ogy generation.
Predicting morphological forms hasbeen shown to be useful for machine translation andother tasks.9 Here we describe two sets of exper-iments: an inflectional morphology task in whichmodels are trained to transduce verbs from one forminto another (section 4.2), and a lemmatization task(section 4.3), in which any inflected verb is to be re-duced to its root form.4.1 Training and decodingWe train ?
to maximize the regularized10 conditionallog-likelihood11?(x,y?
)?Clog p?(y?
| x) + ||?||2/2?2, (2)where C is a supervised training corpus.
To max-imize (2) during training, we apply the gradient-based optimization method L-BFGS (Liu and No-cedal, 1989).129E.g., Toutanova et al (2008) improve MT performanceby selecting correct morphological forms from a knowledgesource.
We instead focus on generalizing from observed formsand generating new forms (but see with rootlist in Table 3).10The variance ?2 of the L2 prior is chosen by optimizing ondevelopment data.
We are also interested in trying an L1 prior.11Alternatives would include faster error-driven methods(perceptron, MIRA) and slower max-margin Markov networks.12This worked a bit better than stochastic gradient descent.1083To decode a test example x, we wish to findy?
= argmaxy??
?y p?
(y | x).
Constructively, y?
is thehighest-probability string in the WFSA T [x], whereT = pi?1x ?U?
?piy is the trained transducer that mapsx nondeterministically to y. Alas, it is NP-hard tofind the highest-probability string in a WFSA, evenan acyclic one (Casacuberta and Higuera, 2000).The problem is that the probability of each string yis a sum over many paths in T [x] that reflect differ-ent alignments of y to x.
Although it is straightfor-ward to use a determinization construction (Mohri,1997)13 to collapse these down to a single path pery (so that y?
is easily read off the single best path),determinization can increase the WFSA?s size expo-nentially.
We approximate by pruning T [x] back toits 1000-best paths before we determinize.14Since the alignments, classes and regions are notobserved in C, we do not enjoy the convex objec-tive function of fully-supervised log-linear models.Training equation (2) therefore converges only tosome local maximum that depends on the startingpoint in parameter space.
To find a good startingpoint we employ staged training, a technique inwhich several models of ascending complexity aretrained consecutively.
The parameters of each morecomplex model are initialized with the trained pa-rameters of the previous simpler model.Our training is done in four stages.
All weightsare initialized to zero.
?
We first train only fea-tures that fire on unigrams of alignment charac-ters, ignoring features that examine the latent stringsor backed-off versions of the alignment characters(such as vowel/consonant or target language modelfeatures).
The resulting model is equivalent toweighted edit distance (Ristad and Yianilos, 1998).?
Next,15 we train all n-grams of alignment charac-ters, including higher-order n-grams, but no backed-off features or features that refer to latent strings.13Weighted determinization is not always possible, but it isin our case because our limit to k consecutive insertions guar-antees that T [x] is acyclic.14This value is high enough; we see no degradations in per-formance if we use only 100 or even 10 best paths.
Below that,performance starts to drop slightly.
In both of our tasks, ourconditional distributions are usually peaked: the 5 best outputcandidates amass > 99% of the probability mass on average.Entropy is reduced by latent classes and/or regions.15When unclamping a feature at the start of stages ??
?, weinitialize it to a random value from [?0.01, 0.01].13SIA.
liebte, pickte, redete, rieb, trieb, zuzog13SKE.
liebe, picke, rede, reibe, treibe, zuziehe2PIE.
liebt, pickt, redet, reibt, treibt, zuzieht13PKE.lieben, picken, reden, reiben, treiben, zuziehen2PKE.
abbrechet, entgegentretet, zuziehetz.
abzubrechen, entgegenzutreten, zuzuziehenrP.
redet, reibt, treibt, verbindet, u?berfischtpA.geredet, gerieben, getrieben, verbunden, u?berfischtTable 2: CELEX forms used in our experiments.
Changesfrom one form to the other are in bold (information notgiven in training).
The changes from rP to pA are verycomplex.
Note also the differing positions of zu in z.?
Next, we add backed-off features as well as allcollapsed features.
?
Finally, we train all features.In our experiments, we permitted latent classes 1?2 and, where regions are used, regions 0?6.
Forspeed, stages ???
used a pruned ?
that includedonly ?plausible?
alignment characters: a may notalign to b unless it did so in the trained stage-(1)model?s optimal alignment of at least one trainingpair (x, y?
).4.2 Inflectional morphologyWe conducted several experiments on the CELEXmorphological database.
We arbitrarily consid-ered mapping the following German verb forms:1613SIA ?
13SKE, 2PIE ?
13PKE, 2PKE ?
z,and rP ?
pA.17 We refer to these tasks as 13SIA,2PIE, 2PKE and rP.
Table 2 shows some examplesof regular and irregular forms.
Common phenomenainclude stem changes (ei:ie), prefixes inserted af-ter other morphemes (abzubrechen) and circumfixes(gerieben).We compile lists of form pairs from CELEX.
Foreach task, we sample 2500 data pairs without re-placement, of which 500 are used for training, 1000as development and the remaining 1000 as test data.We train and evaluate models on this data and repeat16From the available languages in CELEX (German, Dutch,and English), we selected German as the language with themost interesting morphological phenomena, leaving the mul-tilingual comparison for the lemmatization task (section 4.3),where there were previous results to compare with.
The 4 Ger-man datasets were picked arbitrarily.17A key to these names: 13SIA=1st/3rd sg.
ind.
past;13SKE=1st/3rd sg.
subjunct.
pres.
; 2PIE=2nd pl.
ind.
pres.
;13PKE=1st/3rd pl.
subjunct.
pres.
; 2PKE=2nd pl.
subjunct.pres.
; z=infinitive; rP=imperative pl.
; pA=past part.1084Features Taskng vc tlm tlm-coll id lat.cl.
lat.reg.
13SIA 2PIE 2PKE rPngrams x 82.3 (.23) 88.6 (.11) 74.1 (.52) 70.1 (.66)ngrams+xx x 82.8 (.21) 88.9 (.11) 74.3 (.52) 70.0 (.68)x x 82.0 (.23) 88.7 (.11) 74.8 (.50) 69.8 (.67)x x x 82.5 (.22) 88.6 (.11) 74.9 (.50) 70.0 (.67)x x x 81.2 (.24) 88.7 (.11) 74.5 (.50) 68.6 (.69)x x x x 82.5 (.22) 88.8 (.11) 74.5 (.50) 69.2 (.69)x x 82.4 (.22) 88.9 (.11) 74.8 (.51) 69.9 (.68)x x x 83.0 (.21) 88.9 (.11) 74.9 (.50) 70.3 (.67)x x x 82.2 (.22) 88.8 (.11) 74.8 (.50) 70.0 (.67)x x x x 82.9 (.21) 88.6 (.11) 75.2 (.50) 69.7 (.68)x x x x 81.9 (.23) 88.6 (.11) 74.4 (.51) 69.1 (.68)x x x x x 82.8 (.21) 88.7 (.11) 74.7 (.50) 69.9 (.67)ngrams+x+latentx x x x x x 84.8 (.19) 93.6 (.06) 75.7 (.48) 81.8 (.43)x x x x x x 87.4 (.16) 93.8 (.06) 88.0 (.28) 83.7 (.42)x x x x x x x 87.5 (.16) 93.4 (.07) 87.4 (.28) 84.9 (.39)Moses3 73.9 (.40) 92.0 (.09) 67.1 (.70) 67.6 (.77)Moses9 85.0 (.21) 94.0 (.06) 82.3 (.31) 70.8 (.67)Moses15 85.3 (.21) 94.0 (.06) 82.8 (.30) 70.8 (.67)Table 1: Exact-match accuracy and average edit distance (the latter in parentheses) versus the correct answer on theGerman inflection task, using different combinations of feature classes.
The label ngrams corresponds to the secondstage of training, ngrams+x to the third where backoff features may fire (vc = vowel/consonant, tlm = target LM, tlm-coll = collapsed tlm, id = identity/substitution/deletion features), and ngrams+x+latent to the fourth where featuressensitive to latent classes and latent regions are allowed to fire.
The highest n-gram order used is 3, except for Moses9and Moses15 which examine windows of up to 9 and 15 characters, respectively.
We mark in bold the best result foreach dataset, along with all results that are statistically indistinguishable (paired permutation test, p < 0.05).the process 5 times.
All results are averaged overthese 5 runs.Table 1 and Fig.
3 report separate results afterstages ?, ?, and ?
of training, which include suc-cessively larger feature sets.
These are respectivelylabeled ngrams, ngrams+x, and ngrams+x+latent.In Table 1, the last row in each section shows thefull feature set at that stage (cf.
Fig.
3), while earlierrows test feature subsets.18Our baseline is the SMT toolkit Moses (Koehn etal., 2007) run over letter strings rather than wordstrings.
It is trained (on the same data splits) tofind substring-to-substring phrase pairs and translatefrom one form into another (with phrase reorderingturned off).
Results reported as moses3 are obtainedfrom Moses runs that are constrained to the samecontext windows that our models use, so the maxi-mum phrase length and the order of the target lan-guage model were set to 3.
We also report resultsusing much larger windows, moses9 and moses15.18The number k of consecutive insertions was set to 3.Results.
The results in Table 1 show that includinglatent classes and/or regions improves the resultsdramatically.
Compare the last line in ngrams+xto the last line in ngrams+x+latent.
The accuracynumbers improve from 82.8 to 87.5 (13SIA), from88.7 to 93.4 (2PIE), from 74.7 to 87.4 (2PKE), andfrom 69.9 to 84.9 (rP).19 This shows that error re-ductions between 27% and 50% were reached.
On3 of 4 tasks, even our simplest ngrams method beatsthe moses3 method that looks at the same amount ofcontext.20 With our full model, in particular usinglatent features, we always outperform moses3?andeven outperform moses15 on 3 of the 4 datasets, re-ducing the error rate by up to 48.3% (rP).
On thefourth task (2PIE), our method and moses15 are sta-tistically tied.
Moses15 has access to context win-dows of five times the size than we allowed ourmethods in our experiments.19All claims in the text are statistically significant under apaired permutation test (p < .05).20This bears out our contention in footnote 2 that a ?segment-ing?
channel model is damaging.
Moses cannot fully recover byusing overlapping windows in the language model.1085While the gains from backoff features in Table 1were modest (significant gains only on 13SIA), thelearning curve in Fig.
3 suggests that they were help-ful for smaller training sets on 2PKE (see ngrams vsngrams+x on 50 and 100) and helped consistentlyover different amounts of training data for 13SIA.Analysis.
The types of errors that our system (andthe moses baseline) make differ from task to task.Due to lack of space, we mainly focus on the com-plex rP task.
Here, most errors come from wronglycopying the input to the output, without making achange (40-50% of the errors in all models, exceptfor our model with latent classes and no regions,where it accounts for only 30% of the errors).
Thisis so common because about half of the training ex-amples contain identical inputs and outputs (as inthe imperative berechnet and the participle (ihr habt)berechnet).
Another common error is to wrongly as-sume a regular conjugation (just insert the prefix ge-at the beginning).
Interestingly, this error by sim-plification is more common in the Moses models(44% of moses3 errors, down to 40% for moses15)than in our models, where it accounts for 37% ofthe errors of our ngrams model and only 19% if la-tent classes or latent regions are used; however, itgoes up to 27% if both latent classes and regionsare used.21 All models for rP contain errors wherewrong analogies to observed words are made (ver-schweisst/verschwissen in analogy to the observeddurchweicht/durchwichen, or bebt/geboben in anal-ogy to hebt/gehoben).
In the 2PKE task, most errorsresult from inserting the zu morpheme at a wrongplace or inserting two of them, which is alwayswrong.
This error type was greatly reduced by la-tent regions, which can discover different parame-ters for different positions, making it easier to iden-tify where to insert the zu.Analysis of the 2 latent classes (when used) showsthat a split into regular and irregular conjugationshas been learned.
For the rP task we compute,for each data pair in development data, the poste-rior probabilities of membership in one or the otherclass.
98% of the regular forms, in which the pastparticiple is built with ge- .
.
.
-t, fall into one class,21We suspect that training of the models that use classes andregions together was hurt by the increased non-convexity; an-nealing or better initialization might help.Figure 3: Learning curves for German inflection tasks,13SIA (left) and 2PKE (right), as a function of the num-ber of training pairs.
ngrams+x means all backoff fea-tures were used, ngrams+x+latent means all latent fea-tures were used in addition.
Moses15 examines windowsof up to 15 characters.which in turn consists nearly exclusively (96%) ofthese forms.
Different irregular forms are lumpedinto the other class.The learned regions are consistent across differentpairs.
On development data for the rP task, 94.3%of all regions that are labeled 1 are the insertion se-quence (,ge), region 3 consists of vowel changes93.7% of the time; region 5 represents the typicalsuffixes (t,en), (et,en), (t,n) (92.7%).
In the2PKE task, region 0 contains different prefixes (e.g.entgegen in entgegenzutreten), regions 1 and 2 areempty, region 3 contains the zu affix, region 4 thestem, and region 5 contains the suffix.The pruned alignment alphabet excluded a fewgold standard outputs so that the model containspaths for 98.9%?99.9% of the test examples.
Weverified that the insertion limit did not hurt oracleaccuracy.4.3 LemmatizationWe apply our models to the task of lemmatization,where the goal is to generate the lemma given an in-flected word form.
We compare our model to Wicen-towski (2002, chapter 3), an alternative supervisedapproach.
Wicentowski?s Base model simply learnshow to replace an arbitrarily long suffix string of aninput word, choosing some previously observed suf-fix?
suffix replacement based on the input word?s1086Without rootlist (generation) With rootlist (selection)Wicentowski (2002) This paper Wicentowski (2002) This paperLang.
Base Af.
WFA.
n n+x n+x+l Base Af.
WFA.
n n+x n+x+lBasque 85.3 81.2 80.1 91.0 (.20) 91.1 (.20) 93.6 (.14) 94.5 94.0 95.0 90.9 (.29) 90.8 (.31) 90.9 (.30)English 91.0 94.7 93.1 92.4 (.09) 93.4 (.08) 96.9 (.05) 98.3 98.6 98.6 98.7 (.04) 98.7(.04) 98.7(.04)Irish 43.3 - 70.8 96.8 (.07) 97.0 (.06) 97.8 (.04) 43.9 - 89.1 99.6 (.02) 99.6 (.02) 99.5 (.03)Tagalog 0.3 80.3 81.7 80.5 (.32) 83.0 (.29) 88.6 (.19) 0.8 91.8 96.0 97.0 (.07) 97.2 (.07) 97.7 (.05)Table 3: Exact-match accuracy and average edit distance (the latter in parentheses) on the 8 lemmatization tasks (2tasks ?
4 languages).
The numbers from Wicentowski (2002) are for his Base, Affix and WFAffix models.
Thenumbers for our models are for the feature sets ngrams, ngrams+x, ngrams+x+latent.
The best result per task is inbold (as are statistically indistinguishable results when we can do the comparison, i.e., for our own models).
Corpussizes: Basque 5,842, English 4,915, Irish 1,376, Tagalog 9,479.final n characters (interpolating across different val-ues of n).
His Affix model essentially applies theBase model after stripping canonical prefixes andsuffixes (given by a user-supplied list) from the inputand output.
Finally, his WFAffix uses similar meth-ods to also learn substring replacements for a stemvowel cluster and other linguistically significant re-gions in the form (identified by a deterministic align-ment and segmentation of training pairs).
This ap-proach is a bit like our change regions combinedwith Moses?s region-independent phrase pairs.We compare against all three models.
Note thatAffix and WFAffix have an advantage that our mod-els do not, namely, user-supplied lists of canonicalaffixes for each language.
It is interesting to seehow our models with their more non-committal tri-gram structure compare to this.
Table 3 reports re-sults on the data sets used in Wicentowski (2002),for Basque, English, Irish, and Tagalog.
Follow-ing Wicentowski, 10-fold cross-validation was used.The columns n+x and n+x+l mean ngram+x andngram+x+latent, respectively.
As latent variables,we include 2 word classes but no change regions.22For completeness, Table 3 also compares with Wi-centowski (2002) on a selection (rather than genera-tion) task.
Here, at test time, the lemma is selectedfrom a candidate list of known lemmas, namely, allthe output forms that appeared in training data.23These additional results are labeled with rootlist inthe right half of Table 3.On the supervised generation task without rootlist,22The insertion limit k was set to 2 for Basque and 1 for theother languages.23Though test data contained no (input, output) pairs fromtraining data, it reused many of the output forms, since manyinflected inputs are to be mapped to the same output lemma.our models outperform Wicentowski (2002) by alarge margin.
Comparing our results that use la-tent classes (n+x+l) with Wicentowski?s best mod-els we observe error reductions ranging from about38% (Tagalog) to 92% (Irish).
On the selection taskwith rootlist, we outperform Wicentowski (2002) inEnglish, Irish, and Tagalog.Analysis.
We examined the classes learned on En-glish lemmatization by our ngrams+x+latent model.For each of the input/output pairs in developmentdata, we found the most probable latent class.
Forthe most part, the 2 classes are separated based onwhether or not the correct output ends in e. Thisuse of latent classes helped address many errors likewronging / wronge or owed / ow).
Such missing orsurplus final e?s account for 72.5% of the errors forngrams and 70.6% of the errors for ngrams+x, butonly 34.0% of the errors for ngrams+x+latent.The test oracles are between 99.8% ?
99.9%, dueto the pruned alignment alphabet.
As on the inflec-tion task, the insertion limit does not exclude anygold standard paths.5 Finite-State Feature ImplementationWe used the OpenFST library (Allauzen et al, 2007)to implement all finite-state computations, using theexpectation semiring (Eisner, 2002) for training.Our model is defined by the WFSA U?, which isused to score alignment strings in ??
(section 2.2).We now sketch how to construct U?
from features.n-gram construction The construction that wecurrently use is quite simple.
All of our currentfeatures fire on windows of width ?
3.
We builda WFSA with the structure of a 3-gram language1087model over ??.
Each of the |?|2 states rememberstwo previous alignment characters ab of history; foreach c ?
?, it has an outgoing arc that accepts c (andleads to state bc).
The weight of this arc is the totalweight (from ?)
of the small set of features that firewhen the trigram window includes abc.
By conven-tion, these also include features on bc and c (whichmay be regarded as backoff features ?bc and ?
?c).Since each character in ?
is actually a 4-tuple, thistrigram machine is fairly large.
We build it lazily(?on the fly?
), constructing arcs only as needed todeal with training or test data.Feature templates Our experiments use over50,000 features.
How do we specify these featuresto the above construction?
Rather than writing ordi-nary code to extract features from a window, we findit convenient to harness FSTs as a ?little language?
(Bentley, 1986) for specifying entire sets of features.A feature template T is an nondeterministic FSTthat maps the contents of the sliding window, suchas abc, to one or more features, which are alsodescribed as strings.24 The n-gram machine de-scribed above can compute T [((a?b)?c)?]
to findout what features fire on abc and its suffixes.
Onesimple feature template performs ?vowel/consonantbackoff?
; e.g., it maps abc to the feature namedVCC.
Fig.
2 showed the result of applying severalactual feature templates to the window shown inFig.
1.
The extended regular expression calculusprovides a flexible and concise notation for writ-ing down these FSTs.
As a trivial example, the tri-gram ?vowel/consonant backoff?
transducer can bedescribed as T = V V V , where V is a transducerthat performs backoff on a single alignment charac-ter.
Feature templates should make it easy to experi-ment with adding various kinds of linguistic knowl-edge.
We have additional algorithms for compilingU?
from a set of arbitrary feature templates,25 in-cluding templates whose features consider windowsof variable or even unbounded width.
The details arebeyond the scope of this paper, but it is worth point-ing out that they exploit the fact that feature tem-plates are FSTs and not arbitrary code.24Formally, if i is a string naming a feature, then fi(A)counts the number of positions in A that are immediately pre-ceded by some string in T?1[i].25Provided that the total number of features is finite.6 ConclusionsThe modeling framework we have presented hereis, we believe, an attractive solution to most stringtransduction problems in NLP.
Rather than learn thetopology of an arbitrary WFST, one specifies thetopology using a small set of feature templates, andsimply trains the weights.We evaluated on two morphology generationtasks.
When inflecting German verbs we, even withthe simplest features, outperform the moses3 base-line on 3 out of 4 tasks, which uses the same amountof context as our models.
Introducing more sophis-ticated features that have access to latent classes andregions improves our results dramatically, even onsmall training data sizes.
Using these we outper-form moses9 and moses15, which use long contextwindows, reducing error rates by up to 48%.
On thelemmatization task we were able to improve the re-sults reported in Wicentowski (2002) on three out offour tested languages and reduce the error rates by38% to 92%.
The model?s errors are often reason-able misgeneralizations (e.g., assume regular con-jugation where irregular would have been correct),and it is able to use even a small number of latentvariables (including the latent alignment) to captureuseful linguistic properties.In future work, we would like to identify a set offeatures, latent variables, and training methods thatport well across languages and string-transductiontasks.
We would like to use features that look atwide context on the input side, which is inexpen-sive (Jiampojamarn et al, 2007).
Latent variableswe wish to consider are an increased number ofword classes; more flexible regions?see Petrov etal.
(2007) on learning a state transition diagram foracoustic regions in phone recognition?and phono-logical features and syllable boundaries.
Indeed, ourlocal log-linear features over several aligned latentstrings closely resemble the soft constraints used byphonologists (Eisner, 1997).
Finally, rather than de-fine a fixed set of feature templates as in Fig.
2,we would like to refine empirically useful featuresduring training, resulting in language-specific back-off patterns and adaptively sized n-gram windows.Many of these enhancements will increase the com-putational burden, and we are interested in strategiesto mitigate this, including approximation methods.1088ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-ciech Skut, and Mehryar Mohri.
2007.
OpenFst: Ageneral and efficient weighted finite-state transducerlibrary.
In Proc.
of CIAA, volume 4783.Jon Bentley.
1986.
Programming pearls [column].
Com-munications of the ACM, 29(8), August.Maximilian Bisani and Hermann Ney.
2002.
Inves-tigations on jointmultigram models for grapheme-to-phoneme conversion.Francisco Casacuberta and Colin De La Higuera.
2000.Computational complexity of problems on probabilis-tic grammars and transducers.
In Proc.
of the 5th Inter-national Colloquium on Grammatical Inference: Al-gorithms and Applications, volume 1891.Francisco Casacuberta.
2000.
Inference of finite-state transducers by using regular grammars and mor-phisms.
In A.L.
Oliveira, editor, Grammatical Infer-ence: Algorithms and Applications, volume 1891.Stanley F. Chen.
2003.
Conditional and joint models forgrapheme-to-phoneme conversion.
In Proc.
of Inter-speech.Alexander Clark.
2001.
Learning morphology with PairHidden Markov Models.
In Proc.
of the Student Work-shop at the 39th Annual Meeting of the Association forComputational Linguistics, Toulouse, France, July.Sabine Deligne, Francois Yvon, and Fre?de?ric Bimbot.1995.
Variable-length sequence matching for phonetictranscription using joint multigrams.
In Eurospeech.Vera Demberg, Helmut Schmid, and Gregor Mo?hler.2007.
Phonological constraints and morphologicalpreprocessing for grapheme-to-phoneme conversion.In Proc.
of ACL, Prague, Czech Republic, June.Jason Eisner.
1997.
Efficient generation in primitive Op-timality Theory.
In Proc.
of ACL-EACL, Madrid, July.Jason Eisner.
2002.
Parameter estimation for probabilis-tic finite-state transducers.
In Proc.
of ACL.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, conditionalrandom field parsing.
In Proceedings of ACL-08:HLT, pages 959?967, Columbus, Ohio, June.
Associa-tion for Computational Linguistics.Dayne Freitag and Shahram Khadivi.
2007.
A sequencealignment model based on the averaged perceptron.
InProc.
of EMNLP-CoNLL.Lucian Galescu and James F. Allen.
2001.
Bi-directionalconversion between graphemes and phonemes using ajoint N-gram model.Sittichai Jiampojamarn, Grzegorz Kondrak, and TarekSherif.
2007.
Applying many-to-many alignmentsand hidden markov models to letter-to-phoneme con-version.
In Proc.
of NAACL-HLT, Rochester, NewYork, April.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proc.of ACL, Companion Volume, Prague, Czech Republic,June.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Math.
Programming, 45(3, (Ser.
B)):503?528.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProc.
of ACL, Ann Arbor, Michigan, June.Mehryar Mohri.
1997.
Finite-state transducers in lan-guage and speech processing.
Computational Linguis-tics, 23(2).Slav Petrov, Adam Pauls, and Dan Klein.
2007.
Learningstructured models for phone recognition.
In Proc.
ofEMNLP-CoNLL.Eric Sven Ristad and Peter N. Yianilos.
1998.
Learn-ing string-edit distance.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 20(5).Holger Schwenk, Marta R. Costa-jussa, and Jose A.R.
Fonollosa.
2007.
Smooth bilingual n-gram transla-tion.
In Proc.
of EMNLP-CoNLL, pages 430?438.Tarek Sherif and Grzegorz Kondrak.
2007.
Substring-based transliteration.
In Proc.
of ACL, Prague, CzechRepublic, June.Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.2008.
Applying morphology generation models tomachine translation.
In Proceedings of ACL-08: HLT,Columbus, Ohio, June.Richard Wicentowski.
2002.
Modeling and LearningMultilingual Inflectional Morphology in a MinimallySupervised Framework.
Ph.D. thesis, Johns-HopkinsUniversity.Jun Wu and Sanjeev Khudanpur.
2000.
Efficient trainingmethods for maximum entropy language modeling.
InProc.
of ICSLP, volume 3, Beijing, October.Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vo-gel.
2007.
A log-linear block transliteration modelbased on bi-stream HMMs.
In Proc.
of NAACL-HLT,Rochester, New York, April.1089
