Using the Distribution of Performance for Studying StatisticalNLP Systems and CorporaYuval KrymolowskiDepartment of Mathematics and Computer ScienceBar-Ilan University52900 Ramat Gan, IsraelAbstractStatistical NLP systems are fre-quently evaluated and compared onthe basis of their performances ona single split of training and testdata.
Results obtained using a singlesplit are, however, subject to sam-pling noise.
In this paper we ar-gue in favour of reporting a distri-bution of performance figures, ob-tained by resampling the trainingdata, rather than a single num-ber.
The additional informationfrom distributions can be used tomake statistically quantified state-ments about differences across pa-rameter settings, systems, and cor-pora.1 IntroductionThe common practice in evaluating statisticalNLP systems is using a standard corpus (e.g.,Penn TreeBank for parsing, Reuters for textcategorization) along with a standard split be-tween training and test data.
As systems im-prove, it becomes harder to achieve additionalimprovements, and the performance of vari-ous state-of-the-art systems is approximatelyidentical.
This makes performance compar-isons difficult.In this paper, we argue in favour of studyingthe distribution of performance, and presentconclusions drawn from studying the recalldistribution.
This distribution provides mea-sures for answering the following questions:Q1: Comparing systems on given data: Isclassifier A better than classifier B forgiven training and test data?Q2: Adequacy of training data to test data:Is a system trained on dataset X ade-quate for analysing dataset Y ?
Are fea-tures from X indicative in Y ?Q3: Comparing data sets with a given sys-tem: If a different training set improvesthe result of system A on dataset Y1, willthis be the case on dataset Y2 as well?The answers to these questions can provideuseful insight into statistical NLP systems.In particular, about sensitivity to features inthe training data, and transferability.
Theseproperties can be different even when similarperformance is reported.A statistical treatment of Question 1 ispresented by Yeh (2000).
He tests for thesignificance of performance differences onfixed training and test data sets.
In otherrelated works, Martin and Hirschberg (1996)provides an overview of significance testsof error differences in small samples, andDietterich (1998) discusses results of a num-ber of tests.Questions 2 and 3 have been frequentlyraised in NLP, but not explicitly addressed,since the prevailing evaluation methods pro-vide no means of addressing them.
In thispaper we propose addressing all three ques-tions with a single experimental methodology,which uses the distribution of recall.2 MotivationWords, parts-of-speech (POS), words, or anyfeature in text may be regarded as outcomesof a statistical process.
Therefore, wordcounts, count ratios, and other data used increating statistical NLP models are statisti-cal quantities as well, and as such prone tosampling noise.
Sampling noise results fromthe finiteness of the data, and the particularchoice of training and test data.A model is an approximation or a more ab-stract representation of training data.
Onemay look at a model as a collection of es-timators analogous, e.g., to the slope calcu-lated by linear regression.
These estimatorsare statistics with a distribution related to theway they were obtained, which may be verycomplicated.
The performance figures, beingdependent on these estimators, have a distri-bution function which may be difficult to findtheoretically.
This distribution gives rise tointrinsic noise.Performance comparisons based on a singlerun or a few runs do not take these noises intoaccount.
Because we cannot assign the result-ing statements a confidence measure, they aremore qualitative than quantitative.
The de-gree to which we can accept such statementsdepends on the noise level and more generally,on the distribution of performance.In this paper, we use recall as a perfor-mance measure (cf.
Section 4.4 and Section3.2 in (Yeh, 2000)).
Recall samples are ob-tained by resampling from training data andtraining classifiers on these samples.The resampling methods used here arecross-validation and bootstrap (Efron andGong, 1983; Efron and Tibshirani, 1993,cf.
Section 3).
Section 4 presents the experi-mental goals and setup.
Results are presentedand discussed in Section 5, and a summary isprovided in Section 6.3 The Bootstrap MethodThe bootstrap is a re-sampling techniquedesigned for obtaining empirical distribu-tions of estimators.
It can be thoughtof as a smoothed version of k-fold cross-validation (CV).
The method has been ap-plied to decision tree and bayesian classifiersby Kohavi (1995) and to neural networks by,e.g., LeBaron and Weigend (1998).In this paper, we use the bootstrap methodto obtain the distribution of performance of asystem which learns to identify non-recursivenoun-phrases (base-NPs).
While there are afew refinements of the method, the intentionof this paper is to present the benefits of ob-taining distributions, rather than optimisingbias or variance.
We do not aim to study theproperties of bootstrap estimation.Let a statistic S = S(x1, .
.
.
, xn) be a func-tion of the independent observations {xi}ni=1of a statistical variable X.
The bootstrapmethod constructs the distribution functionof S by successively re-sampling x with re-placements.After B samples, we have a set of bootstrapsamples {xb1, .
.
.
, xbn}Bb=1, each of which yieldsan estimate S?b for S. The distribution of S?
isthe bootstrap estimate for the distribution ofS.
That distribution is mostly used for esti-mating the standard deviation, bias, or confi-dence interval of S.In the present work, xi are the base-NP in-stances in a given corpus, and the statistic Sis the recall on a test set.4 Experimental SetupThe aim of our experiments is to test whetherthe recall distribution can be helpful in an-swering the questions Q1?Q3 mentioned inthe introduction of this paper.The data and learning algorithms are pre-sented in Sections 4.1 and 4.2.
Section 4.3describes the sampling method in detail.
Sec-tion 4.4 motivates the use of recall and de-scribes the experiments.4.1 DataWe used Penn-Treebank (Marcus et al, 1993)data, presented in Table 1.
Wall-Street Jour-nal (WSJ) Sections 15-18 and 20 were usedby Ramshaw and Marcus (1995) as trainingand test data respectively for evaluating theirbase-NP chunker.
These data have since be-come a standard for evaluating base-NP sys-tems.The WSJ texts are economic newspaperreports, which often include elaborated sen-tences containing about six base-NPs on theSource Sentences Words BaseNPsWSJ 15-18 8936 229598 54760WSJ 20 2012 51401 12335ATIS 190 2046 613WSJ 20a 100 2479 614WSJ 20b 93 2661 619Table 1: Data sourcesaverage.The ATIS data, on the other hand, area collection of customer requests related toflight schedules.
These typically include shortsentences which contain only three base-NPson the average.
For example:I have a friend living in Denverthat would like to visit mehere in Washington DC .The structure of sentences in the ATIS datadiffers significantly from that in the WSJdata.
We expect this difference to be reflectedin the recall of systems tested on both datasets.The small size of the ATIS data can influ-ence the results as well.
To distinguish thesize effect from the structural differences, wedrew two equally small samples from WSJSection 20.
These samples, WSJ20a andWSJ20b, consist of the first 100 and the fol-lowing 93 sentences respectively.
There is aslight difference in size because sentences werekept complete, as explained Section 4.3.4.2 Learning AlgorithmsWe evaluated base-NP learning systems basedon two algorithms: MBSL (Argamon et al,1999) and SNoW (Mun?oz et al, 1999).MBSL is a memory-based system whichrecords, for each POS sequence containing aborder (left, right, or both) of a base-NP, thenumber of times it appears with that bordervs.
the number of times it appears withoutit.
It is possible to set an upper limit on thelength of the POS sequences.Given a sentence, represented by a sequenceof POS tags, the system examines each sub-sequence for being a base-NP.
This is doneby attempting to tile it using POS sequencesthat appeared in the training data with thebase-NP borders at the same locations.For the purpose of the present work, sufficeit to mention that one of the parameters isthe context size (c).
It denotes the maximalnumber of words considered before or after abase-NP when recording sub-sequences con-taining a border.SNoW (Roth, 1998, ?Sparse Network ofWinnow?)
is a network architecture of Win-now classifiers (Littlestone, 1988).
Winnowis a mistake-driven algorithm for learning alinear separator, in which feature weights areupdated by multiplication.
The Winnow al-gorithm is known for being able to learn welleven in the presence of many noisy features.The features consist of one to four consec-utive POSs in a 3-word window around eachPOS.
Each word is classified as a beginning ofa base-NP, as an end, or neither.4.3 Sampling MethodIn generating the training samples we sampledcomplete sentences.
In MBSL, an un-markedboundary may be counted as a negative ex-ample for the POS-subsequences which con-tains it.
Therefore, sampling only part of thebase-NPs in a sentence will generate negativeexamples.For SNoW, each word is an example, butmost of the words are neither a beginning noran end of a base-NP.
Random sampling ofwords might generate a sample with an im-proper balance between the three classes.To avoid these problems, we sampledfull sentences instead of words or instances.Within a good approximation, it can be as-sumed that base-NP patterns in a sentence donot correlate.
The base-NP instances drawnfrom the sampled sentences can therefore beregarded as independent.As described at the end of Sec.
4.1, theWSJ20a and WSJ20b data were created sothat they contain 613 instances, like the ATISdata.
In practice, the number of instancesexceeds 613 slightly due to the full-sentenceconstraint.
For the purpose of this work, it isenough that their size is very close to the sizeof ATIS.Dataset Sentences Base-NPsTraining 8938 ?
48 54763 ?
2Unique: 5648 ?
34Table 2: Sentence and instant counts for thebootstrap samples.
The second line refers tounique sentences in the training data.We used the WSJ15-18 dataset for train-ing.
This dataset contains n0 = 54760 base-NP instances.
The number of instances in abootstrap sample depends on the number ofinstances in the last sampled sentence.
AsTable 2 shows, it is slightly more than n0.For k-CV sampling, the data were dividedinto k random distinct parts, each containingn0k ?2 instances.
Table 3 shows the number ofrecall samples in each experiment (MBSL andSNoW experiments were carried out seper-ately).Method MBSL SNoWBootstrap 2200 1000CV (total folds) 1500 1000Table 3: Number of bootstrap samples andtotal CV folds.4.4 ExperimentsWe trained SNoW and MBSL; the latter us-ing context sizes of c=1 and c=3.
Data setsWSJ20, ATIS, WSJ20a, and WSJ20b wereused for testing.
MBSL runs with the twoc values were conducted on the same trainingsamples, therefore it is possible to comparetheir results directly.Each run yielded recall and precision.
Re-call may be viewed as the expected 0-1 loss-function on the given test sample of instances.Precision, on the other hand, may be viewedas the expected 0-1 loss on the sample of in-stances detected by the learning system.
Careshould be taken when discussing the distribu-tion of precision values because this samplevaries from run to run.
We will therefore onlyanalyse the distribution of recall in this work.In the following, r1 and r3 denote recallsamples of MBSL with c = 1 and c = 3,with standard deviations ?1 and ?3.
?13 de-notes the cross-correlation between r1 and r3.SNoW recall and standard deviation will bedenoted by rSN and ?SN.To approach the questions raised in the in-troduction we made the following measure-ments:Q1: System comparison was addressed bycomparing r1 and r3 on the same test data.With samples at hand, we obtained an esti-mate of P (r3 > r1).Q2: We studied training and test adequacythrough the effect of more specific features onrecall, and on its standard deviation.Setting c = 3 takes into account sequenceswith context of two and three words in ad-dition to those with c = 1.
Sequences withlarger context are more specific, and an im-provement in recall implies that they are in-formative in the test data as well.For particular choices of parameters andtest data, the recall spread yields an estimateof the training sampling noise.
On inade-quate data, where the statistics differ signif-icantly from those in the training data, evensmall changes in the model can lead to a no-ticeable difference in recall.
This is becausethe model relies on statistics which appearrelatively rarely in the test data.
Not onlydo these statistics provide little informationabout the problem, but even small differencesin weighting them are relatively influential.Therefore, the more training and test datadiffer from each other, the more spread we canexpect in results.Q3: For comparing test data sets with asystem, we used cross-correlations between r1,r3, or rSN samples obtained on these data sets.We know that WSJ data are different fromATIS data, and so expect the results on WSJto correlate with ATIS results less than withother WSJ results.5 Results and DiscussionFor each of the five test datasets, Table 4 re-ports averages and standard deviations of r1,r3, and rSN obtained by 3, 5, 10, and 20-foldcross-validation, and by bootstrap.
?13 andP (r3 > r1) are reported as well.We discuss our results by considering towhat extent they provide information for an-swering the three questions:Q1 ?
Comparing systems on given data:For the WSJ data sets, the difference betweenr3 and r1 was well above their standard de-viations, and r3 > r1 nearly always.
ForATIS, the standard deviation of the differ-ence (?2r3?r1 = (?1)2 + (?3)2 ?
2?1?3 ?
?13)was small due to the high ?13, and r1 > r3nearly always.Q2 ?
The adequacy of training and testsets: It is clear that adding more specificfeatures, by increasing the context, improvedrecall on the WSJ test data and degraded iton the ATIS data.
This is likely to be an indi-cation of the difference in syntactic structurebetween ATIS and WSJ texts.Another evidence of structural differencecomes from standard deviations.
The spreadof the ATIS results always exceeded that ofthe WSJ results, with all three experiments.That difference cannot be solely attributedto the small size of ATIS, since WSJ20aand WSJ20b results displayed a much smallerspread.
Indeed, these results had a widerstandard deviation than WSJ20, probablydue to the smaller size, but not as wide asATIS.
This indicates that base-NPs in ATIStext have different characteristics than thosein WSJ texts.Q3 ?
Comparing datasets by a system:Table 5 reports, for each pair of datasets, thecorrelation between the 5-fold CV recall sam-ples of each experiment on these datasets.The correlations change with CV fold num-ber, 5-fold results were chosen as they repre-sent intermediary values.Both MBSL experiments yielded negligiblecorrelations of ATIS results with any WSJdata set, whether large or small.
These corre-lations were always weaker than with WSJ20aand WSJ20b, which are about the same size.This is due to ATIS being a different kindof text.
The correlation between WSJ20a andWSJ20b results was also weak.
This may bedue to their small sizes; these texts might notshare enough features to make a significantcorrelation.SNoW results were highly correlated for allpairs.
That behaviour is markedly differentfrom the MBSL results, and indicates a highlevel of noise in the SNoW features.
Indeed,Winnow is able to learn well in the presenceof noise, but that noise causes the high corre-lations observed here.5.1 Further ObservationsThe decrease of ?13 with CV fold number isrelated to stabilization of the system.
As thefolds become larger, training samples becomemore similar to each other, and the spread ofresults decreases.
This effect was not visiblein the SNoW data, most likely due to the highlevel of noise in the features.
This noise alsocontributes to the higher standard deviationof SNoW results.6 Summary and Further ResearchIn this work, we used the distribution of re-call to address questions concerning base-NPlearning systems and corpora.
Two of thesequestions, of training and test adequacy, andof comparing data sets using NLP systems,were not addressed before.The recall distributions were obtained usingCV and bootstrap resampling.We found differences between algorithmswith similar recall, related to the features theyuse.We demonstrated that using an inadequatetest set may lead to noisy performance results.This effect was observed with two differentlearning algorithms.
We also reported a casewhen changing a parameter of a learning al-gorithm improved results on one dataset butdegraded results on another.We used classifiers as ?similarity rulers?,for producing a similarity measure betweendatasets.
Classifiers may have various prop-erties as similarity rulers, even when their re-calls are similar.
Each classifier should bescaled differently according to its noise level.This demonstrates the way we can use clas-sifiers to study data, as well as use data tostudy classifiers.Test Method MBSL SNoWdata E(r1)?
?1 E(r3)?
?3 ?13 P (r3 > r1) E(rSN)?
?SN3-CV 89.64 ?
0.16 91.26 ?
0.12 0.36 100% 90.18 ?
1.015-CV 89.75 ?
0.14 91.43 ?
0.10 0.30 100% 90.37 ?
1.03WSJ 20 10-CV 89.80 ?
0.12 91.53 ?
0.08 0.25 100% 90.47 ?
1.1120-CV 89.81 ?
0.11 91.56 ?
0.07 0.28 100% 90.51 ?
1.19Bootstrap 89.58 ?
0.17 91.16 ?
0.14 0.42 100% 89.83 ?
0.93E(?)
89.74 91.58 91.233-CV 85.70 ?
2.03 83.99 ?
1.87 0.82 3% 83.70 ?
4.115-CV 85.76 ?
1.87 83.69 ?
1.57 0.79 1% 83.53 ?
4.52ATIS 10-CV 85.90 ?
1.31 84.78 ?
0.92 0.78 4% 83.38 ?
5.1420-CV 85.78 ?
1.16 83.28 ?
0.85 0.77 0% 83.23 ?
5.36Bootstrap 85.72 ?
1.95 84.69 ?
1.95 0.81 16% 83.50 ?
3.35E(?)
85.81 83.20 85.483-CV 89.45 ?
0.42 91.25 ?
0.56 0.33 100% 90.84 ?
1.045-CV 89.66 ?
0.36 91.64 ?
0.54 0.32 100% 91.07 ?
1.15WSJ 20a 10-CV 89.79 ?
0.28 91.85 ?
0.49 0.20 100% 91.14 ?
1.2620-CV 89.82 ?
0.23 91.89 ?
0.44 0.18 100% 91.11 ?
1.39Bootstrap 89.42 ?
0.47 91.55 ?
0.57 0.33 99% 90.76 ?
1.00E(?)
89.73 92.18 90.073-CV 88.95 ?
0.41 90.12 ?
0.39 0.37 99% 89.79 ?
0.815-CV 89.03 ?
0.36 90.15 ?
0.31 0.31 99% 89.81 ?
0.84WSJ 20b 10-CV 89.06 ?
0.33 90.14 ?
0.22 0.28 99% 89.83 ?
0.8620-CV 89.07 ?
0.27 90.13 ?
0.18 0.22 100% 89.87 ?
0.88Bootstrap 89.00 ?
0.44 90.17 ?
0.44 0.38 98% 89.93 ?
0.80E(?)
89.01 91.55 90.79Table 4: Recall statistic summary for MBSL with contexts c = 1 and c = 3, and SNoW.
TheE(?)
figures were obtained using the full training set.
Note the monotonic change of standarddeviation with fold number.
The s.d.
of the bootstrap samples are closest to those of low-foldCV samples.5-CV WSJ 20b WSJ 20a ATISr1 r3 rSN r1 r3 rSN r1 r3 rSNWSJ 20 0.33 0.19 0.72 0.26 0.29 0.78 0.08 0.02 0.76ATIS -0.01 0.00 0.59 0.02 -0.01 0.63WSJ 20a 0.07 0.04 0.59Table 5: Cross-correlations between recalls of the three experiments on the test data for five-foldCV.
Correlations of r1 capture dataset similarity in the best way.By using MBSL with different context sizes,our results provide insights into the relationbetween training and test data sets, in termsof general and specific features.
That issue be-comes important when one plans to use a sys-tem trained on certain data set for analysingan arbitrary text.
Another approach to thistopic, examining the effect of using lexicalbigram information, which is very corpus-specific, appears in (Gildea, 2001).In our experiments with systems trained onWSJ data, there was a clear difference be-tween their behaviour on other WSJ data andon the ATIS data set, in which the structureof base-NPs is different.
That difference wasobserved with correlations and standard devi-ations.
This shows that resampling the train-ing data is essential for noticing these struc-ture differences.To control the effect of small size of theATIS dataset, we provided two equally-smallWSJ data sets.
The effect of different genreswas stronger than that of the small-size.In future study, it would be helpful to studythe distribution of recall using training andtest data from a few genres, across genres,and on combinations (e.g.
?known-similaritycorpora?
(Kilgarriff and Rose, 1998)).
Thiswill provide a measure of the transferabilityof a model.We would like to study whether there is arelation between bootstrap and 2 or 3-CV re-sults.
The average number of unique base-NPs in a random bootstrap training sampleis about 63% of the total training instances(Table 2).
That corresponds roughly to thesize of a 3-CV training sample.
More work isrequired to see whether this relation betweenbootstrap and low-fold CV is meaningful.We also plan to study the distribution ofprecision.
As mentioned in Sec.
4.4, the pre-cisions of different runs are now taken fromdifferent sample spaces.
This makes the boot-strap estimator unsuitable, and more study isrequired to overcome this problem.ReferencesS.
Argamon, I. Dagan, and Y. Krymolowski.
1999.A memory-based approach to learning shallownatural language patterns.
Journal of Experi-mental and Theoretical AI, 11:369?390.
CMP-LG/9806011.T.
G. Dietterich.
1998.
Approximate statisti-cal tests for comparing supervised classifica-tion learning algorithms.
Neural Computation,10(7).Bradley Efron and Gail Gong.
1983.
A leisurelylook at the bootstrap, the jackknife, and cross-validation.
Am.
Stat., 37(1):36?48.Bradley Efron and Robert J. Tibshirani.
1993.
AnIntroduction to the Bootstrap.
Chapman andHall, New York.Daniel Gildea.
2001.
Corpus variation and parserperformance.
In Proc.
2001 Conf.
on Empir-ical Methods in Natural Language Processing(EMNLP?2001), Carnegie Mellon University,Pittsburgh, June.
ACL-SIGDAT.Adam Kilgarriff and Tony Rose.
1998.
Mea-sures for corpus similarity and homogeneity.
InProc.
3rd Conf.
on Empirical Methods in Nat-ural Language Processing (EMNLP?3), pages46?52, Granada, Spain, June.
ACL-SIGDAT.Ron Kohavi.
1995.
A study of cross-validationand bootstrap for accuracy estimation andmodel selection.
In proceedings of the Inter-national Joint Conference on Artificial Intelli-gence, pages 1137?1145.B.
LeBaron and A. S. Weigend.
1998.
A boot-strap evaluation of the effect of data splittingon financial time series.
IEEE Transactions onNeural Networks, 9(1):213?220, January.N.
Littlestone.
1988.
Learning quickly whenirrelevant attributes abound: A new linear-threshold algorithm.
Machine Learning, 2:285?318.M.
P. Marcus, B. Santorini, andM.
Marcinkiewicz.
1993.
Building a large anno-tated corpus of English: The Penn Treebank.Computational Linguistics, 19(2):313?330,June.J.
Martin and D. Hirschberg.
1996.
Small samplestatistics for classification error rates II: Confi-dence intervals and significance tests.
Technicalreport, Dept.
of Information and Computer Sci-ence, University of California, Irvine.
TechnicalReport 96-22.M.
Mun?oz, V. Punyakanok, D. Roth, and D. Zi-mak.
1999.
A learning approach to shallowparsing.
In EMNLP-VLC?99, the Joint SIG-DAT Conference on Empirical Methods in Nat-ural Language Processing and Very Large Cor-pora, pages 168?178, June.L.
A. Ramshaw and M. P. Marcus.
1995.
Textchunking using transformation-based learning.In Proceedings of the Third Workshop on VeryLarge Corpora.D.
Roth.
1998.
Learning to resolve natural lan-guage ambiguities: A unified approach.
Inproc.
of the Fifteenth National Conference onArtificial Intelligence, pages 806?813, MenloPark, CA, USA, July.
AAAI Press.Alexander Yeh.
2000.
More accurate tests forthe statistical significance of result differences.In 18th International Conference on Computa-tional Linguistics (COLING), pages 947?953,July.
