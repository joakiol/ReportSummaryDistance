Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961?970,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsGenerating Confusion Sets for Context-Sensitive Error CorrectionAlla Rozovskaya and Dan RothUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801{rozovska,danr}@illinois.eduAbstractIn this paper, we consider the problem of gen-erating candidate corrections for the task ofcorrecting errors in text.
We focus on thetask of correcting errors in preposition usagemade by non-native English speakers, usingdiscriminative classifiers.
The standard ap-proach to the problem assumes that the set ofcandidate corrections for a preposition con-sists of all preposition choices participatingin the task.
We determine likely prepositionconfusions using an annotated corpus of non-native text and use this knowledge to producesmaller sets of candidates.We propose several methods of restrictingcandidate sets.
These methods exclude candi-date prepositions that are not observed as validcorrections in the annotated corpus and takeinto account the likelihood of each preposi-tion confusion in the non-native text.
We findthat restricting candidates to those that are ob-served in the non-native data improves boththe precision and the recall compared to theapproach that views all prepositions as pos-sible candidates.
Furthermore, the approachthat takes into account the likelihood of eachpreposition confusion is shown to be the mosteffective.1 IntroductionWe address the problem of generating candidate cor-rections for the task of correcting context-dependentmistakes in text, mistakes that involve confusingvalid words in a language.
A well-studied instanceof this problem ?
context-sensitive spelling errors ?has received a lot of attention in natural languageresearch (Golding and Roth, 1999; Carlson et al,2001; Carlson and Fette, 2007; Banko and Brill,2001).
The context-sensitive spelling correction taskaddresses the problem of correcting spelling mis-takes that result in legitimate words, such as confus-ing their and there or your and you?re.
In this task, acandidate set or a confusion set is defined that spec-ifies a list of confusable words, e.g., {their, there}or {cite, site, sight}.
Each occurrence of a confus-able word in text is represented as a vector of fea-tures derived from a small context window aroundthe target.
A classifier is trained on text assumedto be error-free, replacing each target word occur-rence (e.g.
their) with a confusion set consisting of{their, there}, thus generating both positive and neg-ative examples, respectively, from the same context.Given a text to correct, for each word in text that be-longs to the confusion set the classifier predicts themost likely candidate in the confusion set.More recently, work in error correction has takenan interesting turn and focused on correcting errorsmade by English as a Second Language (ESL) learn-ers, with a special interest given to errors in articleand preposition usage.
These mistakes are some ofthe most common mistakes for non-native Englishspeakers of all proficiency levels (Dalgish, 1985;Bitchener et al, 2005; Leacock et al, 2010).
Ap-proaches to correcting these mistakes have adoptedthe methods of the context-sensitive spelling cor-rection task.
A system is usually trained on well-formed native English text (Izumi et al, 2003; Eeg-Olofsson and Knuttson, 2003; Han et al, 2006; Fe-lice and Pulman, 2008; Gamon et al, 2008; Tetreault961and Chodorow, 2008; Elghaari et al, 2010; Tetreaultet al, 2010), but several works incorporate intotraining error-tagged data (Gamon, 2010; Han etal., 2010) or error statistics (Rozovskaya and Roth,2010b).
The classifier is then applied to non-nativetext to predict the correct article/preposition in con-text.
The possible candidate selections include theset of all articles or all prepositions.While in the article correction task the candidateset is small (a, the, no article), systems for correct-ing preposition errors, even when they consider themost common prepositions, may include between 9to 34 preposition classes.
For each preposition inthe non-native text, every other candidate in the con-fusion set is viewed as a potential correction.
Thisapproach, however, does not take into account thatwriters do not make mistakes randomly: Not all can-didates are equally likely given the preposition cho-sen by the author and errors may depend on the firstlanguage (L1) of the writer.
In this paper, we de-fine L1-dependent candidate sets for the prepositioncorrection task (Section 4.1).
L1-dependent can-didate sets reflect preposition confusions observedwith the speakers of the first language L1.
We pro-pose methods of enforcing L1-dependent candidatesets in training and testing.We consider mistakes involving the top ten En-glish prepositions.
As our baseline system, we traina multi-class classifier in one-vs-all approach, whichis a standard approach to multi-class classification.In this approach, a separate binary classifier for eachpreposition pi, 1 ?
i ?
10, is trained, s.t.
all piexamples are positive examples for the classifier andall other nine classes act as negative examples.
Thus,for each preposition pi in non-native text there areten1 possible prepositions that the classifier can pro-pose as corrections for pi.We contrast this baseline method to two methodsthat enforce L1-dependent candidate sets in train-ing.
First, we train a separate classifier for eachpreposition pi on the prepositions that belong to L1-dependent candidate set of pi.
In this setting, thenegative examples for pi are those that belong to L1-dependent candidate set of pi.The second method of enforcing L1-dependent1This includes the preposition pi itself.
If proposed by theclassifier, it would not be flagged as an error.candidate sets in training is to train on native datawith artificial preposition errors in the spirit of Ro-zovskaya and Roth (2010b), where the errors mimicthe error rates and error patterns of the non-nativetext.
This method requires more knowledge, sinceit uses a distribution of errors from an error-taggedcorpus.We also propose a method of enforcing L1-dependent candidate sets in testing, through the useof a confidence threshold.
We consider two ways ofapplying a threshold: (1) the standard way, when acorrection is proposed only if the classifier?s con-fidence is sufficiently high and (2) L1-dependentthreshold, when a correction is proposed only if itbelongs to L1-dependent candidate set.We show that the methods of restricting candidatesets to L1-dependent confusions improve the prepo-sition correction system.
We demonstrate that re-stricting candidate sets to those prepositions that areconfusable in the data by L1 writers is beneficial,when compared to a system that assumes an unre-stricted candidate set by considering as valid correc-tions all prepositions participating in the task.
Fur-thermore, we find that the most effective method isthe one that uses knowledge about the likelihoods ofpreposition confusions in the non-native text intro-duced through artificial errors in training.The rest of the paper is organized as follows.First, we describe related work on error correction.Section 3 presents the ESL data and statistics onpreposition errors.
Section 4 describes the meth-ods of restricting candidate sets in training and test-ing.
Section 5 describes the experimental setup.
Wepresent and discuss the results in Section 6.
The keyfindings are summarized in Table 5 and Fig.
1 inSection 6.
We conclude with a brief discussion ofdirections for future work.2 Related WorkWork in text correction has focused primarily oncorrecting context-sensitive spelling errors (Goldingand Roth, 1999; Banko and Brill, 2001; Carlson etal., 2001; Carlson and Fette, 2007) and mistakesmade by ESL learners, especially errors in articleand preposition usage.Roth (1998) takes a unified approach to resolvingsemantic and syntactic ambiguities in natural lan-962guage by treating several related problems, includ-ing word sense disambiguation, word selection, andcontext-sensitive spelling correction as instances ofthe disambiguation task.
Given a candidate set or aconfusion set of confusable words, the task is to se-lect the most likely candidate in context.
Examplesof confusion sets are {sight, site, cite} for context-sensitive spelling correction, {among, between} forword selection, or a set of prepositions for the prepo-sition correction problem.Each occurrence of a candidate word in text isrepresented as a vector of features.
A classifier istrained on a large corpus of error-free text.
Giventext to correct, for each word in text that belongs tothe confusion set the classifier is used to predict themost likely candidate in the confusion set given theword?s context.In the same spirit, models for correcting ESL er-rors are generally trained on well-formed native text.Han et al (2006) train a maximum entropy model tocorrect article mistakes.
Chodorow et.
al (2007),Tetreault and Chodorow (2008), and De Felice andPulman (2008) train a maximum entropy model andDe Felice and Pulman (2007) train a voted percep-tron algorithm to correct preposition errors.
Gamonet al (2008) train a decision tree model and a lan-guage model to correct errors in article and preposi-tion usage.
Bergsma et al (2009) propose a Na?
?veBayes algorithm with web-scale N-grams as fea-tures, for preposition selection and context-sensitivespelling correction.The set of valid candidate corrections for a targetword includes all words in the confusion set.
For thepreposition correction task, the entire set of prepo-sitions considered for the task is viewed as the setof possible corrections for each preposition in non-native text.
Given a preposition with its surround-ing context, the model selects the most likely prepo-sition from the set of all candidates, where the setof candidates consists of nine (Felice and Pulman,2008), 12 (Gamon, 2010), or 34 (Tetreault et al,2010; Tetreault and Chodorow, 2008) prepositions.2.1 Using Error-tagged Data in TrainingSeveral recent works explore ways of using anno-tated non-native text when training error correctionmodels.One way to incorporate knowledge about whichconfusions are likely with ESL learners into the errorcorrection system is to train a model on error-taggeddata.
Preposition confusions observed in the non-native text can then be included in training, by us-ing the preposition chosen by the author (the sourcepreposition) as a feature.
This is not possible with asystem trained on native data, because each sourcepreposition is always the correct preposition.Han et al (2010) train a model on partially anno-tated Korean learner data.
The error-tagged modeltrained on one million prepositions obtains a slightlyhigher recall and a significant improvement in preci-sion (from 0.484 to 0.817) over a model fives timeslarger trained on well-formed text.Gamon (2010) proposes a hybrid system forpreposition and article correction, by incorporatingthe scores of a language model and class probabil-ities of a maximum entropy model, both trained onnative data, into a meta-classifier that is trained ona smaller amount of annotated ESL data.
The meta-classifier outperforms by a large margin both of thenative models, but it requires large amounts of ex-pensive annotated data, especially in order to correctpreposition errors, where the problem complexity ismuch larger.Rozovskaya and Roth (2010b) show that by intro-ducing into native training data artificial article er-rors it is possible to improve the performance of thearticle correction system, when compared to a clas-sifier trained on native data.
In contrast to Gamon(2010) and Han et al (2010) that use annotated datafor training, the system is trained on native data, butthe native data are transformed to be more like L1data through artificial article errors that mimic theerror rates and error patterns of non-native writers.This method is cheaper, since obtaining error statis-tics requires much less annotated data than training.Moreover, the training data size is not restricted bythe amount of the error-tagged data available.
Fi-nally, the source article of the writer can be used intraining as a feature, in the exact same way as withthe models trained on error-tagged data, providingknowledge about which confusions are likely.
Un-like article errors, preposition errors lend themselvesvery well to a study of confusion sets because the setof prepositions participating in the task is a lot big-ger than the set of article choices.9633 ESL Data3.1 Preposition Errors in Learner DataPreposition errors are one of the most common mis-takes that non-native speakers make.
In the Cam-bridge Learner Corpus2 (CLC), which contains databy learners of different first language backgroundsand different proficiency levels, preposition errorsaccount for about 13.5% of all errors and occur onaverage in 10% of all sentences (Leacock et al,2010).
Similar error rates have been reported forother annotated ESL corpora, e.g.
(Izumi et al,2003; Rozovskaya and Roth, 2010a; Tetreault et al,2010).
Learning correct preposition usage in En-glish is challenging for learners of all first languagebackgrounds (Dalgish, 1985; Bitchener et al, 2005;Gamon, 2010; Leacock et al, 2010).3.2 The Annotated CorpusWe use data from an annotated corpus of essayswritten by ESL students.
The essays were fully cor-rected and error-tagged by native English speakers.For each preposition used incorrectly by the author,the annotator also indicated the correct prepositionchoice.
Rozovskaya and Roth (2010a) provide a de-tailed description of the annotation of the data.The annotated data include sentences by speakersof five first language backgrounds: Chinese, Czech,Italian, Russian, and Spanish.
The Czech, Italian,Russian and Spanish data come from the Interna-tional Corpus of Learner English (ICLE, (Grangeret al, 2002)), which is a collection of essays writ-ten by advanced learners of English.
The Chinesedata is a part of the Chinese Learners of English cor-pus (CLEC, (Gui and Yang, 2003)) that contains es-says by students of all levels of proficiency.
Table 1shows preposition statistics based on the annotateddata.The combined data include 4185 prepositions,8.4% of which were judged to be incorrect by theannotators.
Table 1 demonstrates that the error ratesin the Chinese speaker data, for which different pro-ficiency levels are available, are 2 or 3 times higherthan the error rates in other language groups.
Thedata for other languages come from very advancedlearners and, while there are also proficiency differ-2http://www.cambridge.org/eltSource Total Incorrect Errorlanguage preps.
preps.
rateChinese 953 144 15.1%Czech 627 28 4.5%Italian 687 43 6.3%Russian 1210 85 7.0%Spanish 708 52 7.3%All 4185 352 8.4%Table 1: Statistics on prepositions in the ESL data.Column Incorrect denotes the number of prepositionsjudged to be incorrect by the native annotators.
ColumnError rate denotes the proportion of prepositions used in-correctly.ences among advanced speakers, their error rates aremuch lower.We would also like to point out that we take asthe baseline3 for the task the accuracy of the non-native data, or the proportion of prepositions usedcorrectly.
Using the error rate numbers shown inTable 1, the baseline for Chinese speakers is thus84.9%, and for all the data combined it is 91.6%.3.3 Preposition Errors and L1We focus on preposition confusion errors, mistakesthat involve an incorrectly selected preposition4.
Weconsider ten most frequent prepositions in English:on, from, for, of, about, to, at, in, with, and by5.We mentioned in Section 2 that not all prepositionconfusions are equally likely to occur and preposi-tion errors may depend on the first language of thewriter.
Han et al (2010) show that preposition er-rors in the annotated corpus by Korean learners arenot evenly distributed, some confusions occurringmore often than others.
We also observe that con-fusion frequencies differ by L1.
This is consistentwith other studies, which show that learners?
errorsare influenced by their first language (Lee and Sen-eff, 2008; Leacock et al, 2010).3It is argued in Rozovskaya and Roth (2010b) that the mostfrequent class baselines are not relevant for error correctiontasks.
Instead, the error rate in the data need to be considered,when determining the baseline.4We do not address errors of missing or extraneous preposi-tions.5It is common to restrict the systems that detect errors inpreposition usage to the top prepositions.
In the CLC corpus,the usage of the ten most frequent prepositions accounts for82% of all preposition errors (Leacock et al, 2010).9644 Methods of Improving Candidate SetsIn this section, we describe methods of restrictingcandidate sets according to the first language of thewriter.
For the preposition correction task, the stan-dard approach considers all prepositions participat-ing in the task as valid corrections for every prepo-sition in the non-native data.In Section 3.3, we pointed out that (1) not allpreposition confusions are equally likely to occurand (2) preposition errors may depend on the firstlanguage of the writer.
The methods of restrictingconfusion sets proposed in this work use knowledgeabout which prepositions are confusable based onthe data by speakers of language L1.We refer to the preposition originally chosen bythe author in the non-native text as the source prepo-sition, and label denotes the correct prepositionchoice, as chosen by the annotator.
Consider, for ex-ample, the following sentences from the annotatedcorpus.1.
We ate by*/with our hands .2.
To tell the truth , time spent in jail often changes prisoners to*/forthe worse.3.
And the problem that immediately appeared was that men wereunable to cope with the new woman image .In example 1, the annotator replaced by with with;by is the source preposition and with is the label.
Inexample 2, to is the source and for is the label.
Inexample 3, the preposition with is judged as correct.Thus, with is both the source and the label.4.1 L1-Dependent Confusion SetsLet source preposition pi denote a preposition thatappears in the data by speakers of L1.
Let Conf-Set denote the set of all prepositions that the sys-tem can propose as a correction for source preposi-tion pi.
We define two types of confusion sets Con-fSet.
An unrestricted confusion set AllConfSet in-cludes all ten prepositions.
L1-dependent confusionset L1ConfSet(pi) is defined as follows:Definition L1ConfSet(pi) = {pj |?
a sentence inwhich an L1 writer replaced preposition pj with pi }For example, in the Spanish speaker data, fromis used incorrectly in place of of and for.
Then forSpanish speakers, L1ConfSet(from)={from, of, for}.Source L1ConfSet(pi)prep.
pion {on, about, of, to, at, in, with, by}by {with, by, in}from {of, from, for}Table 2: L1-dependent confusion sets for three preposi-tions based on data by Chinese speakers.Table 2 shows for Chinese speakers three preposi-tions and their L1-dependent confusion sets.We now describe methods of enforcing L1-dependent confusion sets in training and testing.4.2 Enforcing L1-dependent Confusion Sets inTrainingWe propose two methods of enforcing L1-dependentconfusion sets in training.
They are contrasted tothe typical method of training a multi-class 10-wayclassifier, where each class corresponds to one of theten participating prepositions.First, we describe the typical training setting.NegAll Training proceeds in a standard way oftraining a multi-class classifier (one-vs-all ap-proach) on all ten prepositions using well-formed native English data.
For each prepo-sition pi, pi examples are positive and the othernine prepositions are negative examples.We now describe two methods of enforcing L1-dependent confusion sets in training.NegL1 This method explores the difference be-tween training with nine types as negative ex-amples and (fewer than nine) L1-dependentnegative examples.For every preposition pi, we train a classifierusing only examples that are in L1ConfSet(pi).In contrast to NegAll, for each source prepo-sition, the negative examples are not all othernine types, but only those that belong inL1ConfSet(pi).
For each language L1, we trainten classifiers, one for each source preposition.For source preposition pi in test, we consultthe classifier for pi.
In this model, the con-fusion set for source pi is restricted throughtraining, since for source pi, the possible can-didate replacements are only those that theclassifier sees in training, and they are all inL1ConfSet(pi).965Training Negative examplesdata NegAll NegL1Clean NegAll-Clean NegL1-CleanErrorL1 NegAll-ErrorL1 -Table 3: Training conditions that result in unrestricted(All) and L1-dependent training paradigms.ErrorL1 This method restricts the candidate set toL1ConfSet(pi) by generating artificial preposi-tion errors in the spirit of Rozovskaya and Roth(2010b).
The training data are thus no longerwell-formed or clean, but augmented with L1error statistics.
Specifically, each prepositionpi in training is replaced with a different prepo-sition pj with probability probConf, s.t.probConf = prob(pi|pj) (1)Suppose 10% of all source prepositions to inthe Russian speaker data correspond to labelfor.
Then for is replaced with to with proba-bility 0.1.The classifier uses in training the source prepo-sition as a feature, which cannot be done whentraining on well-formed text, as discussed inSection 2.1.
By providing the source prepo-sition as a feature, we enforce L1-dependentconfusion sets in training, because the systemlearns which candidate corrections occur withsource preposition pi.
An important distinctionof this approach is that it does not simply pro-vide L1-dependent confusion sets in training:Because errors are generated using L1 writers?error statistics, the likelihood of each candidatecorrection is also provided.
This approach isalso more knowledge-intensive, as it requiresannotated data to obtain error statistics.It should be noted that this method is orthogo-nal to the NegAll and NegL1 methods of train-ing described above and can be used in con-junction with each of them, only that it trans-forms the training data to account in a morenatural way for ESL writing.We combine the proposed methods NegAll,NegL1 with the Clean or ErrorL1 methods and cre-ate three training approaches shown in Table 3.4.3 Restricting Confusion Sets in TestingTo reduce the number of false alarms, correctionsystems generally use a threshold on the confidenceof the classifier, following (Carlson et al, 2001), andpropose a correction only when the confidence of theclassifier is above the threshold.
We show in Section5 that the system trained on data with artificial er-rors performs competitively even without a thresh-old.
The other systems use a threshold.
We considertwo ways of applying a threshold6:1.
ThreshAll A correction for source prepositionpi is proposed only when the confidence ofthe classifier exceeds the threshold.
For eachpreposition in the non-native data, this methodconsiders all candidates as valid corrections.2.
ThreshL1Conf A correction for source prepo-sition pi is proposed only when the confi-dence of the classifier exceeds the empiricallyfound threshold and the preposition proposedas a correction for pi is in the confusion setL1ConfSet(pi).5 Experimental SetupIn this section, we describe experiments with L1-dependent confusion sets.
Combining the threetraining conditions shown in Table 3 with the twoways of thresholding described in Section 4.3, webuild four systems7:1.
NegAll-Clean-ThreshAll This system assumesboth in training and in testing stages that allpreposition confusions are possible.
The sys-tem is trained as a multi-class 10-way classifier,where for each preposition pi, all other nineprepositions are negative examples.
In testing,when applying the threshold, all prepositionsare considered as valid corrections.2.
NegAll-Clean-ThreshL1 This system istrained exactly as NegAll-Clean-ThreshAllbut in testing only corrections that belong6Thresholds are found empirically: We divide the evaluationdata into three equal parts and to each part apply the threshold,which is optimized on the other two parts of the data.7In testing, it is not possible to consider a confusion setlarger than the one used in training.
Therefore, ThreshAll isonly possible with NegAll training condition.966to L1ConfSet(pi) are considered as validcorrections for pi.3.
NegL1-Clean-ThreshL1 For each prepositionpi, a separate classifier is trained on the prepo-sitions that are in L1ConfSet(pi), where pi ex-amples are positive and a set of (fewer thannine) pi-dependent prepositions are negative.Only corrections that belong to L1ConfSet(pi)are considered as valid corrections for pi.8 Tenpi-dependent classifiers for each L1 are trained.4.
NegAll-ErrorL1-NoThresh A system is trainedas a multi-class 10-way classifier with artifi-cial preposition errors that mimic the errorsrates and confusion patterns of the non-nativetext.
For each L1, an L1-dependent system istrained.
This system does not use a threshold.We discuss this in more detail below.The system NegAll-Clean-ThreshAll is our base-line system.
It assumes both in training and in test-ing that all preposition confusions are possible.All of the systems are trained on the same set ofword and part-of-speech features using the same setof training examples.
Features are extracted from awindow of eight words around the preposition andinclude words, part-of-speech tags and conjunctionsof words and tags of lengths two, three, and four.Training data are extracted from English Wikipediaand the New York Times section of the Gigawordcorpus (Linguistic Data Consortium, 2003).In each training paradigm, we follow a discrimi-native approach, using an online learning paradigmand making use of the Averaged Perceptron Algo-rithm (Freund and Schapire, 1999) ?
we use theregularized version in Learning Based Java9 (LBJ,(Rizzolo and Roth, 2007)).
While classical Per-ceptron comes with generalization bound related tothe margin of the data, Averaged Perceptron alsocomes with a PAC-like generalization bound (Fre-und and Schapire, 1999).
This linear learning al-gorithm is known, both theoretically and experi-mentally, to be among the best linear learning ap-proaches and is competitive with SVM and Logistic8ThreshAll is not possible with this training option, as thesystem never proposes a correction that is not in L1ConfSet(pi).9LBJ code is available at http://cogcomp.cs.illinois.edu/page/softwareRegression, while being more efficient in training.It also has been shown to produce state-of-the-artresults on many natural language applications (Pun-yakanok et al, 2008).6 Results and DiscussionTable 4 shows performance of the four systemsby the source language.
For each source lan-guage, the methods that restrict candidate sets intraining or testing outperform the baseline systemNegAll-Clean-ThreshAll that does not restrict can-didate sets.
The NegAll-ErrorL1-NoThresh systemperforms better than the other three systems for alllanguages, except for Italian.
In fact, for the Czechspeaker data, all systems other than NegAll-ErrorL1-NoThresh, have a precision and a recall of 0, sinceno errors are detected10.Source System Acc.
P Rlang.CHNegAll-Clean-ThreshAll 84.78 47.58 11.46NegAll-Clean-ThreshL1 84.84 48.05 15.28NegL1-Clean-ThreshL1 84.94 50.87 11.46NegAll-ErrorL1-NoThresh 86.36 55.27 27.43Baseline 84.89CZNegAll-Clean-ThreshAll 94.74 0.00 0.00NegAll-Clean-ThreshL1 94.98 0.00 0.00NegL1-Clean-ThreshL1 94.66 0.00 0.00NegAll-ErrorL1-NoThresh 95.85 75.00 10.71Baseline 95.53ITNegAll-Clean-ThreshAll 93.23 26.14 8.14NegAll-Clean-ThreshL1 94.03 51.59 18.60NegL1-Clean-ThreshL1 93.16 35.00 16.28NegAll-ErrorL1-NoThresh 93.60 44.95 10.47Baseline 93.74RUNegAll-Clean-ThreshAll 92.73 31.11 3.53NegAll-Clean-ThreshL1 93.02 48.81 8.24NegL1-Clean-ThreshL1 92.44 34.42 8.82NegAll-ErrorL1-NoThresh 93.14 52.38 12.94Baseline 92.98SPNegAll-Clean-ThreshAll 91.95 26.14 5.77NegAll-Clean-ThreshL1 92.02 28.64 5.77NegL1-Clean-ThreshL1 92.44 40.00 7.69NegAll-ErrorL1-NoThresh 93.71 77.50 19.23Baseline 92.66Table 4: Performance results for the 4 systems.
All sys-tems, except for NegAll-ErrorL1-NoThresh, use a thresh-old, which is optimized for accuracy on the developmentset.
Baseline denotes the percentage of prepositions usedcorrectly in the data.
The baseline allows us to evaluatethe systems with respect to accuracy, the percentage ofprepositions, on which the prediction of the system is thesame as the label.
Averaged results over 2 runs.10The Czech data set is the smallest and contains a total of627 prepositions and only 28 errors.967The NegAll-ErrorL1-NoThresh system does notuse a threshold.
However, as shown in Fig.
1, itis possible to increase the precision of the NegAll-ErrorL1-NoThresh system by applying a threshold,at the expense of a lower recall.While the ordering of the systems with respect toquality is not consistent from Table 4, due to modesttest data sizes, Table 5 and Fig.
1 show results for themodels on all data combined and thus give a betteridea of how the systems compare against each other.Table 5 shows performance results for alldata combined.
Both NegAll-Clean-ThreshL1 andNegL1-Clean-ThreshL1 achieve a better precisionand recall over the system with an unrestricted can-didate set NegAll-Clean-ThreshAll.
Recall that bothof the systems restrict candidate sets, the former attesting stage, the latter by training a separate clas-sifier for each source preposition.
NegAll-Clean-ThreshL1 performs slightly better than NegL1-Clean-ThreshL1.
We hypothesize that the NegAll-Clean-ThreshAll performance may be affected be-cause the classifiers for different source preposi-tions contain different number of classes, depend-ing on the size of L1ConfSet confusion sets, whichmakes it more difficult to find a unified thresh-old.
The best performing system overall is NegAll-ErrorL1-NoThresh.
While NegAll-Clean-ThreshL1and NegL1-Clean-ThreshL1 restrict candidate sets,NegAll-ErrorL1-NoThresh also provides informa-tion about the likelihood of each confusion, whichbenefits the classifier.
The differences betweenNegAll-ErrorL1-ThreshL1 and each of the otherthree systems are statistically significant11 (McNe-mar?s test, p < 0.01).
The table also demon-strates that the results on the correction task mayvary widely.
For example, the recall varies by lan-guage between 10.47% and 27.43% for the NegAll-ErrorL1-NoThresh system.
The highest recall num-bers are obtained for Chinese speakers.
Thesespeakers also have the highest error rate, as we notedin Section 3.11Tests of statistical significance compare the combined re-sults from all language groups for each model.
For example, tocompare the model NegAll-Clean-ThreshAll to NegAll-ErrorL1-NoThresh, we combine the results from the five language-specific models NegAll-ErrorL1-NoThresh and compare themto the results on the combined data from the five languagegroups achieved by the model NegAll-Clean-ThreshAll.System Acc.
P RNegAll-Clean-ThreshAll 90.90 31.11 7.95NegAll-Clean-ThreshL1 91.11 37.82 12.78NegL1-Clean-ThreshL1 90.97 34.34 9.66NegAll-ErrorL1-NoThresh 92.23 58.47 19.60Table 5: Comparison of the performance of the 4 sys-tems on all data combined.
All systems, except forNegAll-ErrorL1-NoThresh, use a threshold, which is op-timized for accuracy on the development set.
The dif-ferences between NegAll-ErrorL1-ThreshL1 and each ofthe other three systems are statistically significant (Mc-Nemar?s test, p < 0.01).Finally, Fig.
1 shows precision/recall curves forthe systems12.
The curves are obtained by varyinga decision threshold for each system.
Before we ex-amine the differences between the models, it shouldbe noted that in error correction tasks precision isfavored over recall due to the low level of error.0204060801000 10 20 30 40 50 60PRNegAll-Clean-ThreshAllNegAll-Clean-ThreshL1NegAll-ErrorL1-ThreshL1??????????????????????
?Figure 1: Precision and recall (%) for three mod-els: NegAll-Clean-ThreshAll, NegAll-Clean-ThreshL1,and NegAll-ErrorL1-ThreshL1.The curves demonstrate that NegAll-Clean-ThreshL1 and NegAll-ErrorL1-ThreshL1 are supe-rior to the baseline system NegAll-Clean-ThreshAll:on the same recall points, the precision for bothsystems is consistently better than for the base-12NegL1-Clean-ThreshL1 is not shown, since it is similar inits behavior to NegAll-Clean-ThreshL1.968line model13.
Moreover, while restricting candi-date sets improves the results, providing informa-tion to the classifier about the likelihoods of differ-ent confusions is more helpful, which is reflectedin the precision differences between NegAll-Clean-ThreshL1 and NegAll-ErrorL1-ThreshL1.
In fact,NegAll-ErrorL1-ThreshL1 achieves a higher preci-sion compared to the other systems, even when nothreshold is used (Tables 4 and 5).
This is because,unlike the other models, this system does not tend topropose too many false alarms.6.1 Comparison to Other SystemsIt is difficult to compare performance to other sys-tems, since training and evaluation are not per-formed on the same data, and results may varywidely depending on the first language and profi-ciency level of the writer.
However, in Table 6 welist several systems and their performance on thetask.
Tetreault et al (2010) train on native data andobtain a precision of 48.6% and a recall of 22.5%with top 34 prepositions on essays from the Testof English as a Foreign Language exams.
Han etal.
(2010) obtain a precision of 81.7% and a recallof 13.2% using a model trained on partially error-tagged data by Korean speakers on top ten preposi-tions.
A model trained on 2 million examples fromclean text achieved on the same data set a precisionof 46.3% and a recall of 11.6%.Gamon (2010) shows precision/recall curves onthe combined task of detecting missing, extrane-ous and confused prepositions.
For recall points10% and 20%, precisions of 55% and 40%, respec-tively, are obtained.
For our data, a recall of 10%corresponds to a precision of 46% for the worst-performing model and 78% for the best-performingmodel.
For 20% recall, we obtain a precision of33% for the worst-performing model and 58% forthe best-performing model.
We would like to em-phasize that these comparisons should be interpretedwith caution.13While significance tests did not show differences betweenNegAll-Clean-ThreshAll and NegAll-Clean-ThreshL1, perhapsdue to a modest test set size, the curves demonstrate that the lat-ter system indeed provides a stable advantage over the baselineunrestricted approach.7 Conclusion and Future WorkIn this paper, we proposed methods for improvingcandidate sets for the task of detecting and correct-ing errors in text.
To correct errors in prepositionusage made by non-native speakers of English, weproposed L1-dependent confusion sets that deter-mine valid candidate corrections using knowledgeabout preposition confusions observed in the non-native text.
We found that restricting candidates toSystem Training Data P RTetreault et al, 2010 native; 34 preps.
48.6 22.5Han et al, 2010 partially error-tagged; 81.7 13.210 preps.Han et al, 2010 native; 10 preps.
46.3 11.6Gamon, 2010 native; 12 preps.+ 33.0 10.0extraneous+missingGamon, 2010 native+error-tagged; 55.0 10.012 preps.+extraneous+missingNegAll-Clean-ThreshAll native; 10 preps.
46.0 10.0NegAll-ErrorL1-ThreshL1 native with 78.0 10.0L1 error statistics;10 preps.Table 6: Comparison to other systems.
Please notethat a direct comparison is not possible, since the systemsare trained and evaluated on different data sets.
Gamon(2010) also considers missing and extraneous prepositionerrors.those that are observed in the non-native data im-proves both the precision and the recall compared toa classifier that considers as possible candidates theset of all prepositions.
Furthermore, the approachthat takes into account the likelihood of each prepo-sition confusion is shown to be the most effective.The methods proposed in this paper make use ofselect characteristics that the error-tagged data canprovide.
We would also like to compare the pro-posed methods to the quality of a model trained onerror-tagged data.
Improving the system is also inour future work, but orthogonal to the current con-tribution.AcknowledgmentsWe thank Nick Rizzolo for helpful discussions onLBJ.
We also thank Peter Chew and the anonymousreviewers for their insightful comments.
This re-search is partly supported by a grant from the U.S.Department of Education.969ReferencesM.
Banko and E. Brill.
2001.
Scaling to very very largecorpora for natural language disambiguation.
In Pro-ceedings of 39th Annual Meeting of the Association forComputational Linguistics, pages 26?33, Toulouse,France, July.J.
Bitchener, S. Young, and D. Cameron.
2005.
The ef-fect of different types of corrective feedback on ESLstudent writing.
Journal of Second Language Writing.A.
Carlson and I. Fette.
2007.
Memory-based context-sensitive spelling correction at web scale.
In Proceed-ings of the IEEE International Conference on MachineLearning and Applications (ICMLA).A.
J. Carlson, J. Rosen, and D. Roth.
2001.
Scalingup context sensitive text correction.
In Proceedings ofthe National Conference on Innovative Applications ofArtificial Intelligence (IAAI), pages 45?50.M.
Chodorow, J. Tetreault, and N.-R. Han.
2007.
Detec-tion of grammatical errors involving prepositions.
InProceedings of the Fourth ACL-SIGSEM Workshop onPrepositions, pages 25?30, Prague, Czech Republic,June.
Association for Computational Linguistics.G.
Dalgish.
1985.
Computer-assisted ESL research.CALICO Journal, 2(2).J.
Eeg-Olofsson and O. Knuttson.
2003.
Automaticgrammar checking for second language learners - theuse of prepositions.
Nodalida.A.
Elghaari, D. Meurers, and H. Wunsch.
2010.
Ex-ploring the data-driven prediction of prepositions inenglish.
In Proceedings of COLING 2010, Beijing,China.R.
De Felice and S. Pulman.
2007.
Automatically ac-quiring models of preposition use.
In Proceedings ofthe Fourth ACL-SIGSEM Workshop on Prepositions,pages 45?50, Prague, Czech Republic, June.R.
De Felice and S. Pulman.
2008.
A classifier-based ap-proach to preposition and determiner error correctionin L2 English.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics (Col-ing 2008), pages 169?176, Manchester, UK, August.Y.
Freund and R. E. Schapire.
1999.
Large margin clas-sification using the perceptron algorithm.
MachineLearning, 37(3):277?296.M.
Gamon, J. Gao, C. Brockett, A. Klementiev,W.
Dolan, D. Belenko, and L. Vanderwende.
2008.Using contextual speller techniques and languagemodeling for ESL error correction.
In Proceedings ofIJCNLP.M.
Gamon.
2010.
Using mostly native data to correcterrors in learners?
writing.
In NAACL, pages 163?171,Los Angeles, California, June.A.
R. Golding and D. Roth.
1999.
A Winnow basedapproach to context-sensitive spelling correction.
Ma-chine Learning, 34(1-3):107?130.S.
Granger, E. Dagneaux, and F. Meunier.
2002.
Inter-national Corpus of Learner English.
Presses universi-taires de Louvain.S.
Gui and H. Yang.
2003.
Zhongguo Xuexizhe YingyuYuliaohu.
(Chinese Learner English Corpus).
Shang-hai Waiyu Jiaoyu Chubanshe.
(In Chinese).N.
Han, M. Chodorow, and C. Leacock.
2006.
Detectingerrors in English article usage by non-native speakers.Journal of Natural Language Engineering, 12(2):115?129.N.
Han, J. Tetreault, S. Lee, and J. Ha.
2010.
Us-ing an error-annotated learner corpus to develop andESL/EFL error correction system.
In LREC, Malta,May.E.
Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-hara.
2003.
Automatic error detection in the Japaneselearners?
English spoken data.
In The Companion Vol-ume to the Proceedings of 41st Annual Meeting ofthe Association for Computational Linguistics, pages145?148, Sapporo, Japan, July.C.
Leacock, M. Chodorow, M. Gamon, and J. Tetreault.2010.
Morgan and Claypool Publishers.J.
Lee and S. Seneff.
2008.
An analysis of grammaticalerrors in non-native speech in English.
In Proceedingsof the 2008 Spoken Language Technology Workshop.V.
Punyakanok, D. Roth, and W. Yih.
2008.
The impor-tance of syntactic parsing and inference in semanticrole labeling.
Computational Linguistics, 34(2).N.
Rizzolo and D. Roth.
2007.
Modeling DiscriminativeGlobal Inference.
In Proceedings of the First Inter-national Conference on Semantic Computing (ICSC),pages 597?604, Irvine, California, September.
IEEE.D.
Roth.
1998.
Learning to resolve natural language am-biguities: A unified approach.
In Proceedings of theNational Conference on Artificial Intelligence (AAAI),pages 806?813.A.
Rozovskaya and D. Roth.
2010a.
Annotating ESLerrors: Challenges and rewards.
In Proceedings of theNAACL Workshop on Innovative Use of NLP for Build-ing Educational Applications.A.
Rozovskaya and D. Roth.
2010b.
Training paradigmsfor correcting errors in grammar and usage.
In Pro-ceedings of the NAACL-HLT.J.
Tetreault and M. Chodorow.
2008.
The ups anddowns of preposition error detection in ESL writing.In Proceedings of the 22nd International Conferenceon Computational Linguistics (Coling 2008), pages865?872, Manchester, UK, August.J.
Tetreault, J.
Foster, and M. Chodorow.
2010.
Usingparse features for preposition selection and error de-tection.
In ACL.970
