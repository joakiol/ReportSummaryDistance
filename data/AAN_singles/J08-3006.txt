Book ReviewsSemisupervised Learning for Computational LinguisticsSteven Abney(University of Michigan)Boca Raton, FL: Chapman & Hall / CRC (Computer science and data analysis series,edited by David Madigan et al), 2007, xi+308 pp; hardbound, ISBN 978-1-58488-559-7,$79.95, ?44.99Reviewed byVincent NgUniversity of Texas at DallasSemi-supervised learning is by no means an unfamiliar concept to natural languageprocessing researchers.
Labeled data has been used to improve unsupervised parameterestimation procedures such as the EM algorithm and its variants since the beginningof the statistical revolution in NLP (e.g., Pereira and Schabes 1992).
Unlabeled datahas also been used to improve supervised learning procedures, the most notable ex-amples being the successful applications of self-training and co-training to word sensedisambiguation (Yarowsky 1995) and named entity classification (Collins and Singer1999).Despite its increasing importance, semi-supervised learning is not a topic that istypically discussed in introductorymachine learning texts (e.g., Mitchell 1997; Alpaydin2004) or NLP texts (e.g., Manning and Schu?tze 1999; Jurafsky and Martin 2000).1Consequently, to learn about semi-supervised learning research, one has to consult themachine-learning literature.
This can be a daunting task for NLP researchers who havelittle background in machine learning.
Steven Abney?s book Semisupervised Learning forComputational Linguistics is targeted precisely at such researchers, aiming to providethem with a ?broad and accessible presentation?
of topics in semi-supervised learning.According to the preamble, the reader is assumed to have taken only an introductorycourse in NLP ?that include[s] statistical methods?concretely the material containedin Jurafsky and Martin (2000) and Manning and Schu?tze (1999).?
Nonetheless, I agreewith the author that any NLP researcher who has a solid background in machinelearning is ready to ?tackle the primary literature on semisupervised learning, and willprobably not find this book particularly useful?
(page 11).As the author promises, the book is self-contained and quite accessible to thosewho have little background in machine learning.
In particular, of the 12 chaptersin the book, three are devoted to preparatory material, including: a brief introduc-tion to machine learning, basic unconstrained and constrained optimization tech-niques (e.g., gradient descent and the method of Lagrange multipliers), and relevantlinear-algebra concepts (e.g., eigenvalues, eigenvectors, matrix and vector norms,1 Although Manning and Schu?tze (1999) and Jurafsky and Martin (2000) do discuss self-training, they doso only in the context of Yarowsky?s word sense disambiguation algorithm.Computational Linguistics Volume 34, Number 3diagonalization).
The remaining chapters focus roughly on six types of semi-supervisedlearning methods:2 Self-training.
After introducing the self-training algorithm and its variants,the author discusses its applications in NLP and its relationship to othersemi-supervised learning algorithms. Agreement-based methods.
The co-training algorithm, along with atheoretical analysis of its conditional independence assumption and itsapplications in NLP, are presented.
Additionally, a random field, whichpenalizes disagreement among neighboring nodes, is introduced as analternative way of enforcing agreement. Clustering algorithms.
Basic hard clustering algorithms (e.g., k-means,graph mincuts, hierarchical clustering), EM (as a soft clusteringalgorithm), and their role in semi-supervised learning are discussed. Boundary-oriented methods.
Two discriminative learning algorithms,boosting and support vector machines, are introduced as a means tofacilitate the discussion of their semi-supervised counterparts: co-boostingand transductive SVMs. Label propagation in graphs.
In graph-based approaches to semi-supervisedlearning, the labels of the labeled nodes are propagated to the unlabelednodes, with the goal of maximizing the agreement of the labels ofproximate nodes.
The author shows that this goal is equivalent to finding aharmonic function given the labeled nodes, and presents several algorithms,including the method of relaxation, for computing this function. Spectral methods.
Spectral methods for semi-supervised learning can beviewed as interpolation across a partially labeled graph as describedpreviously using a ?standing wave.?
The author explains the connectionbetween such a wave and the spectrum of a matrix, and establishes therelationship of spectral clustering algorithms to other semi-supervisedlearners, including graph mincuts, random walks, and label propagation.The book is rich in theory and algorithms, and although it is targeted at those wholack relevant mathematical background, each theory and algorithm is presented in arigorous manner.Another nice feature of the book is that it reveals the connection among seeminglydisparate ideas.
As mentioned earlier, it shows that many semi-supervised learnerscan in fact be viewed as self-training; also, the description of the connection betweenspectral clustering and other semi-supervised learners is insightful.In addition, I like the organization of the book.
One reason is the presentation ofco-training: Although the algorithm is presented in Chapter 2, its theoretical underpin-nings are not described until Chapter 9.
This enables the reader to see its applicationsin NLP (in Chapter 3) before going through the mathematics, which could be importantfor researchers who are linguistically but not mathematically oriented.
Another reason2 The presentation of the methods here does not reflect the order in which they are introduced in thebook; rather, it is motivated by the book?s Section 1.3, which gives an overview of the ?leading ideas?of the book.450Book Reviewsis that the preparatory material is presented on a need-to-know basis.
This allows thediscussion of algorithmic ideas as soon as the reader grasps the relevant fundamentals.For instance, function optimization and basic linear algebra concepts are presentedin separate chapters, with the latter being deferred to Chapter 11, right before thediscussion of spectral clustering in Chapter 12.Whereas the discussion of self-training and co-training is complemented by theirapplication to NLP problems, the same is not true for the remaining semi-supervisedlearners described in the book.
The reader is often left to imagine the potential NLPapplications of these learners, and as a consequence is unable to gain an understandingof the state of the art of semi-supervised learning for NLP.
In fact, given its scarcity ofNLP applications, the book perhaps does not merit its current title.
It does have a richbibliography on semi-supervised learning for NLP, but most of the references are notcited in the text.The book also lacks a discussion of the practical issues in applying the semi-supervised learners.
For instance, the author does not mention that in practice it isnot easy to choose k in k-means clustering, merely describing k as a parameter of theclustering algorithm.
As another example, when introducing the EM algorithm, theauthor applies it to a generative model that can be expressed in exponential form,without acknowledging that one of themost difficult issues surrounding the applicationof EM concerns the design of the right generative model given the data.
The lack ofNLP applications in the book has unfortunately enabled the author to sidestep thesepractical issues.
On a related note, one can hardly find any discussions of the strengthsand weaknesses of the semi-supervised learners in the book.
This could leave the readerwithout the ability to choose the best learner for a given NLP problem, and is probablyanother undesirable consequence of the book?s reluctance to discuss NLP applications.The author?s decision to focus exclusively on semi-supervised classification prob-lems effectively limits the scope of the book.
One consequence of this decision is that thereader may not be able to apply the EM algorithm to train a hidden Markov model forsolving sequence-learning problems as basic as part-of-speech tagging upon completionof this book.
Given the recent surge of interest in structure prediction in the NLPcommunity, and the fact that co-training and semi-supervised EM have been appliedto structure-prediction problems such as statistical parsing and part-of-speech tagging,the book?s sole focus on classification problem is perhaps one of its weaknesses.There are a few occasions on which the reader might not get a complete pictureof the capability of an algorithm.
For instance, the reader might think that spectralmethods can be applied only to binary classification tasks, owing to the book?s exclusivefocus on such tasks in its discussion of spectral clustering.
Similarly for the treatmentof support vector machines: The reader may get the impression that SVMs cannot beused to learn non-linear functions, as the discussion of kernels is deliberately omitteddue to their irrelevance to transductive learning.
Although it is important to keep thepresentation focused, I believe that the author could easily have removed potential con-fusions by explicitly stating the full capability of an algorithm and referring the readerto the relevant papers for details.Given the rapid growth of semi-supervised learning research in the past decade,there is currently a need for a broad and accessible reference to this area of research.Abney?s book serves this purpose in spite of the aforementioned weaknesses, and Ibelieve that it is a useful starting point for any non?machine-learning experts whointend to apply semi-supervised learning techniques to their research.
As someone whohas some prior knowledge of semi-supervised learning, I still find this book insightful:It reveals deep connections among apparently disparate ideas.
If I were to teach a course451Computational Linguistics Volume 34, Number 3on semi-supervised learning for NLP, I would undoubtedly use this book as a primaryreference.ReferencesAlpaydin, Ethem.
2004.
Introduction toMachine Learning.
The MIT Press,Cambridge, MA.Collins, Michael and Yoram Singer.
1999.Unsupervised models for named entityclassification.
In Proceedings of the 1999Joint Conference on Empirical Methods inNatural Language Processing and VeryLarge Corpora, pages 100?110, CollegePark, MD.Jurafsky, Daniel and James H. Martin.
2000.Speech and Language Processing.
PrenticeHall, Upper Saddle River, NJ.Manning, Christopher D. and HinrichSchu?tze.
1999.
Foundations of StatisticalNatural Language Processing.
The MITPress, Cambridge, MA.Mitchell, Tom M.?1997.
Machine Learning.McGraw Hill, Columbus, OH.Pereira, Fernando and Yves Schabes.
1992.Inside-outside reestimation from partiallybracketed corpora.
In Proceedings of the30th Annual Meeting of the Association forComputational Linguistics, pages 128?135,Newark, DE.Yarowsky, David.
1995.
Unsupervised wordsense disambiguation rivaling supervisedmethods.
In Proceedings of the 33rdAnnual Meeting of the Association forComputational Linguistics, pages 189?196,Cambridge, MA.Vincent Ng is an assistant professor in the Department of Computer Science at the University ofTexas at Dallas.
He is also affiliated with the university?s Human Language Technology ResearchInstitute, where he conducts research on statistical natural language processing and teaches un-dergraduate and graduate courses inmachine learning.
Ng?s address is: Department of ComputerScience, University of Texas at Dallas, Richardson, TX 75080-0688; e-mail: vince@hlt.utdallas.edu.452
