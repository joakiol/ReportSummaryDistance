Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 797?802,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsTransforming trees into hedges and parsing with ?hedgebank?
grammarsMahsa Yarmohammadi?, Aaron Dunlop?and Brian Roark?
?Oregon Health & Science University, Portland, Oregon?Google, Inc., New Yorkyarmoham@ohsu.edu, {aaron.dunlop,roarkbr}@gmail.comAbstractFinite-state chunking and tagging meth-ods are very fast for annotating non-hierarchical syntactic information, and areoften applied in applications that do notrequire full syntactic analyses.
Scenar-ios such as incremental machine transla-tion may benefit from some degree of hier-archical syntactic analysis without requir-ing fully connected parses.
We introducehedge parsing as an approach to recover-ing constituents of length up to some max-imum span L. This approach improves ef-ficiency by bounding constituent size, andallows for efficient segmentation strategiesprior to parsing.
Unlike shallow parsingmethods, hedge parsing yields internal hi-erarchical structure of phrases within itsspan bound.
We present the approach andsome initial experiments on different infer-ence strategies.1 IntroductionParsing full hierarchical syntactic structures iscostly, and some NLP applications that could ben-efit from parses instead substitute shallow prox-ies such as NP chunks.
Models to derive suchnon-hierarchical annotations are finite-state, so in-ference is very fast.
Still, these partial annota-tions omit all but the most basic syntactic segmen-tation, ignoring the abundant local structure thatcould be of utility even in the absence of fully con-nected structures.
For example, in incremental (si-multaneous) machine translation (Yarmohammadiet al, 2013), sub-sentential segments are trans-lated independently and sequentially, hence thefully-connected syntactic structure is not generallyavailable.
Even so, locally-connected source lan-guage parse structures can inform both segmen-tation and translation of each segment in such atranslation scenario.One way to provide local hierarchical syntacticstructures without fully connected trees is to fo-cus on providing full hierarchical annotations forstructures within a local window, ignoring globalconstituents outside that window.
We follow theXML community in naming structures of this typehedges (not to be confused with the rhetorical de-vice of the same name), due to the fact that they arelike smaller versions of trees which occur in se-quences.
Such structures may be of utility to var-ious structured inference tasks, as well as withina full parsing pipeline, to quickly constrain subse-quent inference, much as finite-state models suchas supertagging (Bangalore and Joshi, 1999) orchart cell constraints (Roark and Hollingshead,2008; Roark et al, 2012) are used.In this paper, we consider the problem of hedgeparsing, i.e., discovering every constituent oflength up to some span L. Similar constraintshave been used in dependency parsing (Eisnerand Smith, 2005; Dreyer et al, 2006), where theuse of hard constraints on the distance betweenheads and dependents is known as vine parsing.It is also reminiscent of so-called Semi-Markovmodels (Sarawagi and Cohen, 2004), which allowfinite-state models to reason about segments ratherthan just tags by imposing segment length limits.In the XML community, trees and hedges are usedfor models of XML document instances and forthe contents of elements (Br?uggemann-Klein andWood, 2004).
As far as we know, this paper isthe first to consider this sort of partial parsing ap-proach for natural language.We pursue this topic via tree transformation,whereby non-root non-terminals labeling con-stituents of span > L in the tree are recursivelyelided and their children promoted to attach totheir parent.
In such a way, hedges are sequen-tially connected to the top-most non-terminal inthe tree, as demonstrated in Figure 1.
After apply-ing such a transform to a treebank, we can inducegrammars and modify parsing to search as neededto recover just these constituents.In this paper, we propose several methods to797a)33,1 13116LQYHVWRUVIRU$'-3--WUHDFKHURXV9%UHPDLQ930'ZLOO93'7 -- 11WKH KLJK\LHOG PDUNHW13,1RI33--PXFK1313116$QDO\VWV139%3DUH--FRQFHUQHG,1WKDW6936$'-36%$5b)33,1 13116LQYHVWRUVIRU$'-3--WUHDFKHURXV9%UHPDLQ930'ZLOO93'7 -- 11WKH KLJK\LHOG PDUNHW13,1RI33--PXFK1313116$QDO\VWV13 9%3DUH--FRQFHUQHG,1WKDW6Figure 1: a) Full parse tree, b) Hedge parse tree with maximum constituent span of 7 (L = 7).parse hedge constituents and examine their accu-racy/efficiency tradeoffs.
This is compared witha baseline of parsing with a typically inducedcontext-free grammar and transforming the resultvia the hedge transform, which provides a ceilingon accuracy and a floor on efficiency.
We inves-tigate pre-segmenting the sentences with a finite-state model prior to hedge parsing, and achievelarge speedups relative to hedge parsing the wholestring, though at a loss in accuracy due to cas-cading segmentation errors.
In all cases, we findit crucial that our ?hedgebank?
grammars be re-trained to match the conditions during inference.2 MethodsIn this section, we present the details of our ap-proach.
First, we present the simple tree transformfrom a full treebank parse tree to a (root attached)sequence of hedges.
Next, we discuss modifica-tions to inference and the resulting computationalcomplexity gains.
Finally, we discuss segmentingto further reduce computational complexity.2.1 Hedge Tree TransformThe hedge tree transform converts the originalparse tree into a hedge parse tree.
In the resultinghedge parse tree, every child of the top-most nodespans at most L words.
To transform an originaltree to a hedge tree, we remove every non-terminalwith span larger than L and attach its children to itsparent.
We label span length on each node by re-cursively summing the span lengths of each node?schildren, with terminal items by definition havingspan 1.
A second top-down pass evaluates eachnode before evaluating its children, and removesnodes spanning > L words.
For example, the spanof the non-root S, SBAR, ADJP, and VP nodes inFigure 1(a) have spans between 10 and 13, henceare removed in the tree in Figure 1(b).If we apply this transform to an entire tree-bank, we can use the transformed trees to inducea PCFG for parsing.
Figure 2 plots the percentageof constituents from the original WSJ Penn tree-bank (sections 2-21) retained in the transformedversion, as we vary the maximum span length pa-rameter L. Over half of constituents have span 3 orless (which includes frequent base noun phrases);L = 7 covers approximately three quarters of theoriginal constituents, and L = 15 over 90%.
Mostexperiments in this paper will focus on L = 7,which is short enough to provide a large speedupyet still cover a large fraction of constituents.2.2 Hedge ParsingAs stated earlier, our brute-force baseline ap-proach is to parse the sentence using a full context-free grammar (CFG) and then hedge-transform theresult.
This method should yield a ceiling on798Pct.ofconstituentsretained0 5 10 15 205060708090100Maximum ?span ?size ?
(L)Percentage?of?constituents?retainedcesstocomparedwithgrammarstrainedontrans-formedtrees;butitwillbeslowertoparse.Weaimtodramaticallyimproveefficiencyuponthisbaselinewhilelosingaslittleaccuracyaspossible.Sincewelimitthespanofnon-terminalla-bels,wecanconstrainthesearchperformedbytheparser,greatlyreducetheCYKprocessingtime.Inessence,weperformnoworkinchartcellsspan-ningmorethanLwords,exceptforthecellsalongtheperipheryofthechart,whicharejustusedtoconnectthehedgestotheroot.ConsidertheflattreeinFigure1.ForusebyaCYKparsingal-gorithm,treesarebinarizedpriortogrammarin-duction,resultinginspecialnon-terminalscreatedbybinarization.Otherthanthesymbolattherootofthetree,theonlyconstituentswithspanlengthgreaterthanLinthebinarizedtreewillbelabeledwiththesespecialbinarizationnon-terminals.Fur-ther,ifthebinarizationsystematicallygroupstheleftmostortherightmostchildrenunderthesenewnon-terminals(themostcommonstrategy),thenconstituentswithspangreaterthanLwilleitherbeginatthefirstword(leftmostgrouping)orendatthelastword(rightmostgrouping),furthercon-strainingthenumberofcellsinthechartrequiringwork.ComplexityofparsingwithafullCYKparserisO(n3|G|)wherenisthelengthofinputand|G|isthegrammarsizeconstant.Incontrast,complex-ityofparsingwithahedgeconstrainedCYKisre-ducedtoO((nL2+n2)|G|).Toseethatthisisthecase,considerthatthereareO(nL)cellsofspanLorless,andeachhasamaximumofLmidpoints,whichaccountsforthefirstterm.Beyondthese,thereareO(n)remainingactivecellswithO(n)possiblemidpoints,whichaccountsforthesecondterm.Note,however,thattheworkintheselattercellsmaybeless,sincethesetofpossiblenon-terminalsisreduced.ItispossibletoparsewithastandardlyinducedPCFGusingthissortofhedgeconstrainedpars-ingthatonlyconsidersasubsetofthechartcells,andspeedupsareachieved,howeverthisisclearlynon-optimal,sincethemodelisill-suitedtocom-bininghedgesintoflatstructuresattherootofthetree.Thisresultsindegradationofparsingper-formancebytensofpointsofF-measureversusstandardparsing.Instead,inallscenarioswhereathechartisconstrainedtosearchforhedges,welearnagrammarfromahedgetransformedtree-bank,matchedtothemaximumlengthallowedbytheparser,whichwecallahedgebankgrammar.AhedgebankgrammarisafullyfunctionalPCFGanditcanbeusedwithanystandardparsingal-gorithm,i.e.,thesearenotgenerallyfinite-stateequivalentmodels.However,usingtheBerke-leygrammarlearner(seeSection3),wefindthathedgebankgrammarsaretypicallysmallerthantreebankgrammars,alsocontributingtoparsingspeedupviathegrammarconstant.Auniquepropertyofhedgeconstituentscom-paredtoconstituentsintheoriginalparsetreesisthattheyaresequentiallyconnectedtotheTOPnode.Thispropertyenablesustochunkthesentenceintosegmentsthatcorrespondtocom-pletehedges,andparsethesegmentsindepen-dently(andsimultaneously)insteadofparsingtheentiresentence.Insection2.3,wepresentourap-proachtohedgesegmentation.NotethatparsingsegmentswithagrammartrainedonwholestringsMaximum span size (L)Figure 2: Percentage of constituents retained at variousspan length parametershedge-parsing accuracy, as it has access to richcontextual information (as compared to grammarstrained on transformed trees).
Naturally, inferencewill be slow; we aim to improve efficiency uponthis baseline while minimizing accuracy loss.Since we limit the span of non-terminal la-bels, we can constrain the search performed by theparser, greatly reduce the CYK processing time.
Inessence, we perform no work in chart cells span-ning more than L words, except for the cells alongthe periphery of the chart, which are just used toconnect the hedges to the root.
Consider the flattree in Figure 1(b).
For use by a CYK parsing al-gorithm, trees are binarized prior to grammar in-duction, resulting in special non-terminals createdby binarization.
Other than the symbol at the rootof the tree, the only constituents with span lengthgreater than L in the binarized tree will be labeledwith these special binarization non-terminals.
Fur-ther, if the binarization systematically groups theleftmost or the rightmost children under these newnon-terminals (the most common strategy), thenconstituents with span greater than L will eitherbegin at the first word (leftmost grouping) or endat the last word (rightmost), further constrainingthe number of cells in the chart requiring work.Complexity of parsing with a full CYK parser isO(n3|G|) where n is the length of input and |G| isthe grammar size constant.
In contrast, complex-ity of parsing with a hedge constrained CYK is re-duced to O((nL2+n2)|G|).
To see that this is thecase, consider that there are O(nL) cells of span Lor less, and each has a maximum of L midpoints,which accounts for the first term.
Beyond these,there are O(n) remaining active cells with O(n)possible midpoints, which accounts for the secondterm.
Note also that these latter cells (spanning> L words) may be less expensive, as the set ofpossible non-terminals is reduced to only those in-troduced by binarization.It is possible to parse with a standardly inducedPCFG using this sort of hedge constrained pars-ing that only considers a subset of the chart cells,and speedu s are achiev d, however this is clearlynon-optimal, since the mod l is ill-suited to com-bining hedges into flat structures at the root of thetree.
Space constraints preclude inclusion of tri-als with this method, b t the net result is a se-vere degradation in accuracy (tens of points of F-measure) versus standar parsing.
Thus, we traina grammar in a matched condition, which we callit a hedgebank grammar.
A hedgebank gram-mar is a fully functional PCFG which is learnedfrom a hedge transformed treebank.
A hedgebankgrammar can be used with any standard parsingalgorithm, i.e., these are not generally finite-stateequivalent models.
However, using the Berke-ley grammar learner (see ?3), we find that hedge-bank grammars are typically smaller than tree-bank grammars, reducing the grammar constantand contributing to faster inference.A unique property of hedge constituents com-pared to constituents in the original parse treesis that they are sequentially connected to the top-most node.
This property enables us to chunk thesentence into segments that correspond to com-plete hedges, and parse the segments indepen-dently (and simultaneously) instead of parsing theentire sentence.
In section 2.3, we present our ap-proach to hedge segmentation.In all scenarios where the chart is constrainedto search for hedges, we learn a hedgebank gram-mar, which is matched to the maximum length al-lowed by the parser.
In the pre-segmentation sce-nario, we first decompose the hedge transformedtreebank into its hedge segments and then learn ahedgebank grammar from the new corpus.2.3 Hedge SegmentationIn this section we present our segmentation modelwhich takes the input sentence and chunks it intoappropriate segments for hedge parsing.
We treatthis as a binary classification task which decidesif a word can begin a new hedge.
We use hedgesegmentation as a finite-state pre-processing stepfor hedge context-free parsing.Our task is to learn which words can begin(B) a hedge constituent.
Given a set of labeledpairs (S,H) where S is a sentence of n wordsw1.
.
.
wnand H is its hedge parse tree, word wbbelongs to B if there is a hedge constituent span-ning wb.
.
.
wefor some e ?
b and wbbelongs to?Botherwise.
To predict the hedge boundaries moreaccurately, we grouped consecutive unary or POS-799tag hedges together under a new non-terminal la-beled G. Unlabeled segmentation tags for thewords in the example sentence in Figure 1(b) are:?Analysts/B are/?B concerned/?B that/?B much/Bof/?B the/?B high-yield/?B market/?B will/Bremain/?B treacherous/?B for/?B investors/?B ./B?In addition to the simple unlabeled segmentationwith B and?B tags, we try a labeled segmenta-tion with BCand?BCtags where C is hedge con-stituent type.
We restrict the types to the most im-portant types ?
following the 11 chunk types an-notated in the CoNLL-2000 chunking task (Sangand Buchholz, 2000) ?
by replacing all other typeswith a new type OUT.
Thus, ?Analysts?
is labeledBG; ?much?, BNP; ?will?, BVPand so on.To automatically predict the class of each wordposition, we train a multi-class classifier from la-beled training data using a discriminative linearmodel, learning the model parameters with the av-eraged perceptron algorithm (Collins, 2002).
Wefollow Roark et al (2012) in the features they usedto label words as beginning or ending constituents.The segmenter extracts features from word andPOS-tag input sequences and hedge-boundary tagoutput sequences.
The feature set includes tri-grams of surrounding words, trigrams of surround-ing POS tags, and hedge-boundary tags of the pre-vious words.
An additional orthographical fea-ture set is used to tag rare1and unknown words.This feature set includes prefixes and suffixes ofthe words (up to 4 characters), and presence ofa hyphen, digit, or an upper-case character.
Re-ported results are for a Markov order-2 segmenter,which includes features with the output classes ofthe previous two words.3 Experimental ResultsWe ran all experiments on the WSJ Penn Tree-bank corpus (Marcus et al, 1999) using section2-21 for training, section 24 for development, andsection 23 for testing.
We performed exhaustiveCYK parsing using the BUBS parser2(Bodenstabet al, 2011) with Berkeley SM6 latent-variablegrammars (Petrov and Klein, 2007) learned by theBerkeley grammar trainer with default settings.We compute accuracy from the 1-best Viterbitree extracted from the chart using the standardEVALB script.
Accuracy results are reported asprecision, recall and F1-score, the harmonic meanbetween the two.
In all trials, we evaluate accuracywith respect to the hedge transformed reference1Rare words occur less than 5 times in the training data.2https://code.google.com/p/bubs-parserHedge Parsing Acc/EffParser P R F1 w/sFull w/full CYK 88.8 89.2 89.0 2.4Hedgebank 87.6 84.4 86.0 25.7Table 1: Hedge parsing results on section 24 for L = 7.treebank, i.e., we are not penalizing the parser fornot discovering constituents longer than the max-imum length.
Segmentation accuracy is reportedas an F1-score of unlabeled segment bracketing.We ran timing tests on an Intel 2.66GHz proces-sor with 3MB of cache and 2GB of memory.
Notethat segmentation time is negligible compared tothe parsing time, hence is omitted in reported time.Efficiency results are reported as number of wordsparsed per second (w/s).Table 1 presents hedge parsing accuracy onthe development set for the full parsing baseline,where the output of regular PCFG parsing is trans-formed to hedges and evaluated, versus parsingwith a hedgebank grammar, with no segmenta-tion of the strings.
We find an order of magnitudespeedup of parsing, but at the cost of 3 percent F-measure absolute.
Note that most of that loss isin recall, indicating that hedges predicted in thatcondition are nearly as reliable as in full parsing.Table 2 shows the results on the developmentset when segmenting prior to hedge parsing.
Thefirst row shows the result with no segmentation,the same as the last row in Table 1 for ease of ref-erence.
The next row shows behavior with per-fect segmentation.
The final two rows show per-formance with automatic segmentation, using amodel that includes either unlabeled or labeledsegmentation tags, as described in the last section.Segmentation accuracy is better for the model withlabels, although overall that accuracy is rather low.We achieve nearly another order of magnitudespeedup over hedge parsing without segmentation,but again at the cost of nearly 5 percent F1.Table 3 presents results of our best configura-tions on the eval set, section 23.
The results showthe same patterns as on the development set.
Fi-nally, Figure 3 shows the speed of inference, la-Table 2: Hedge segmentation and parsing results on section24 for L = 7.Segmen- Seg Hedge Parsing Acc/Efftation F1 P R F1 w/sNone n/a 87.6 84.4 86.0 25.7Oracle 100 91.3 88.9 90.1 188.6Unlabeled 80.6 77.2 75.3 76.2 159.1Labeled 83.8 83.1 79.5 81.3 195.8800Segmentation GrammarSegmentation Acc Hedge Parsing Acc/EffP R F1 P R F1 w/sNone Full w/full CYK n/a 90.3 90.3 90.3 2.7None Hedgebank n/a 88.3 85.3 86.8 26.2Labeled Hedgebank 84.0 86.6 85.3 85.1 81.1 83.0 203.0Table 3: Hedge segmentation and parsing results on test data, section 23, for L = 7.WordspersecondFull ?ParsingHedge ?No ?SegHedge ?With ?Seg0 5 10 15 200200400600800Maximum ?span ?size ?
(L)Words?parsed?per?secondWordspersecondFull ?ParsingHedge ?No ?SegHedge ?With ?Seg0 5 10 15 200200400600800Maximum ?span ?size ?
(L)Words?parsed?per?secondHedgePrecision0 5 10 15 207580859095Maximum ?span ?size ?
(L)Hedge?PrecisionHedgeRecall0 5 10 15 207580859095Maximum ?span ?size ?
(L)Hedge?RecallFigure 4: Hedge parsing a) efficiency, and b) accuracy on test data, section 23, for L = 3  20.Maximum span size (L) axi um span size (L)a) b)Figure 3: Hedge parsing a) efficiency, and b) accuracy on test data, section 23, for L = 3?20.beled precision and labeled recall of annotatinghedge constituents on the test set as a functionof the maximum span parameter L, versus thebaseline parser.
Keep in mind that the numberof reference constituents increases as L increases,hence both precision and recall can decrease asthe parameter grows.
Segmentation achieves largespeedups for smaller L values, but the accuracydegradation is consistent, pointing to the need forimproved segmentation.4 Conclusion and Future WorkWe proposed a novel partial parsing approach forapplications that require a fast syntactic analysisof the input beyond shallow bracketing.
The span-limit parameter allows tuning the annotation of in-ternal structure as appropriate for the applicationdomain, trading off annotation complexity againstinference time.
These properties make hedge pars-ing potentially very useful for incremental text orspeech processing, such as streaming text analysisor simultaneous translation.One interesting characteristic of these anno-tations is that they allow for string segmenta-tion prior to inference, provided that the segmentboundaries do not cross any hedge boundaries.
Wefound that baseline segmentation models did pro-vide a significant speedup in parsing, but that cas-cading errors remain a problem.There are many directions of future work topursue here.
First, the current results are all forexhaustive CYK parsing, and we plan to per-form a detailed investigation of the performanceof hedgebank parsing with prioritization and prun-ing methods of the sort available in BUBS (Bo-denstab et al, 2011).
Further, this sort of annota-tion seems well suited to incremental parsing withbeam search, which has been shown to achievehigh accuracies even for fully connected parsing(Zhang and Clark, 2011).
Improvements to thetransform (e.g., grouping items not in hedges un-der non-terminals) and to the segmentation model(e.g., increasing precision at the expense of recall)could improve accuracy without greatly reducingefficiency.
Finally, we intend to perform an ex-trinsic evaluation of this parsing in an on-line tasksuch as simultaneous translation.AcknowledgmentsThis work was supported in part by NSF grant#IIS-0964102.
Any opinions, findings, conclu-sions or recommendations expressed in this pub-lication are those of the authors and do not neces-sarily reflect the views of the NSF.801ReferencesSrinivas Bangalore and Aravind K. Joshi.
1999.
Su-pertagging: An approach to almost parsing.
Com-putational Linguistics, 25(2):237?265.Nathan Bodenstab, Aaron Dunlop, Keith Hall, andBrian Roark.
2011.
Beam-width prediction for ef-ficient context-free parsing.
In Proceedings of the49th Annual Meeting ACL: HLT, pages 440?449.Anne Br?uggemann-Klein and Derick Wood.
2004.Balanced context-free grammars, hedge grammarsand pushdown caterpillar automata.
In ExtremeMarkup Languages.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof the conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 1?8.Markus Dreyer, David A. Smith, and Noah A. Smith.2006.
Vine parsing and minimum risk rerankingfor speed and precision.
In Proceedings of theTenth Conference on Computational Natural Lan-guage Learning (CoNLL), pages 201?205.Jason Eisner and Noah A. Smith.
2005.
Parsing withsoft and hard constraints on dependency length.
InProceedings of the Ninth International Workshop onParsing Technology (IWPT), pages 30?41.Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor.
1999.
Treebank-3.
Linguistic Data Consortium, Philadelphia.Slav Petrov and Dan Klein.
2007.
Learning and infer-ence for hierarchically split PCFGs.
In Proceedingsof the 22nd national conference on Artificial intelli-gence, pages 1663?1666.Brian Roark and Kristy Hollingshead.
2008.
Classi-fying chart cells for quadratic complexity context-free inference.
In Proceedings of the 22nd Inter-national Conference on Computational Linguistics,pages 745?751.Brian Roark, Kristy Hollingshead, and Nathan Boden-stab.
2012.
Finite-state chart constraints for reducedcomplexity context-free parsing pipelines.
Compu-tational Linguistics, 38(4):719?753.Erik F. Tjong Kim Sang and Sabine Buchholz.2000.
Introduction to the CoNLL-2000 shared task:Chunking.
In Proceedings of Conference on Com-putational Natural Language Learning (CoNLL),pages 127?132.Sunita Sarawagi and William W. Cohen.
2004.
Semi-Markov conditional random fields for informationextraction.
In Advances in Neural Information Pro-cessing Systems (NIPS), pages 1185?1192.Mahsa Yarmohammadi, Vivek K. Rangarajan Sridhar,Srinivas Bangalore, and Baskaran Sankaran.
2013.Incremental segmentation and decoding strategiesfor simultaneous translation.
In Proceedings of the6th International Joint Conference on Natural Lan-guage Processing (IJCNLP), pages 1032?1036.Yue Zhang and Stephen Clark.
2011.
Syntactic pro-cessing using the generalized perceptron and beamsearch.
Computational Linguistics, 37(1):105?151.802
