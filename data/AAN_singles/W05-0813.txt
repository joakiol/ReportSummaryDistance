Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 87?90,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Symmetric Probabilistic AlignmentRalf D. Brown Jae Dong Kim Peter J. Jansen Jaime G. CarbonellLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213{ralf,jdkim,pjj,jgc}@cs.cmu.eduAbstractWe recently decided to develop a newalignment algorithm for the purposeof improving our Example-Based Ma-chine Translation (EBMT) system?s per-formance, since subsentential alignment iscritical in locating the correct translationfor a matched fragment of the input.
Un-like most algorithms in the literature, thisnew Symmetric Probabilistic Alignment(SPA) algorithm treats the source and tar-get languages in a symmetric fashion.In this short paper, we outline our basicalgorithm and some extensions for usingcontext and positional information, andcompare its alignment accuracy on theRomanian-English data for the shared taskwith IBM Model 4 and the reported resultsfrom the prior workshop.1 Symmetric Probabilistic Alignment(SPA)In subsentential alignment, mappings are producedfrom words or phrases in the source language sen-tence and those words or phrases in the target lan-guage sentence that best express their meaning.An alignment algorithm takes as input a bilingualcorpus consisting of corresponding sentence pairsand strives to find the best possible alignment in thesecond for selected n-grams (sequences of n words)in the first language.
The alignments are based ona number of factors, including a bilingual dictionary(preferably a probabilistic one), the position of thewords, invariants such as numbers and punctuation,and so forth.For our baseline algorithm, we make the follow-ing simplifying assumptions, each of which we in-tend to relax in future work, and the last of whichhas already been partially relaxed:1.
A fixed bilingual probabilistic dictionary isavailable.2.
Fragments (word sequences) are translated in-dependently of surrounding context.3.
Contiguous fragments of source language textare translated into contiguous fragments in thetarget language text.Unlike the work of (Marcu and Wong, 2002),our alignment algorithm is not generative and doesnot use the idea of a bag of concepts from whichthe phrases in the sentence pair arise.
It is, rather,intended to find the corresponding target-languagephrase given a specific source-language phrase of in-terest, as required by our EBMT system after find-ing a match between the input and the training data(Brown, 2004).1.1 Baseline AlgorithmOur baseline algorithm is based on maximizing theprobability of bi-directional translations of individ-ual words between a selected n-gram in the sourcelanguage and every possible n-gram in the corre-sponding paired target language sentence.
No posi-tional preference assumptions are made, nor are anylength preservation assumptions made.
That is, ann-gram may translate to an m-gram, for any val-ues of n or m bounded by the source and targetsentence lengths, respectively.
Finally a smooth-ing factor is used to avoid singularities (i.e.
avoid-ing zero-probabilities for unknown words, or wordsnever translated before in a way consistent with thedictionary).87Given a source-language sentenceS1 : s0, s1, ..., si, ..., si+k, ..., sn (1)in the bilingual corpus, where si, ..., si+k is a phraseof interest, and the corresponding target languagesentence S2 isS2 : t0, t1, ..., tj , ..., tj+l, ..., tm (2)the values of j and l are to be determined.Then the segment we try to obtain is the targetfragment F?T with the highest probability of all pos-sible fragments of S2 to be a mutual translation withthe given source fragment, orF?T = argmax{FT } (p(si, ..., si+k ?
tj, ..., tj+l))(3)All possible segments can be checked in O(m2)time, where m is the target language length, becausewe will check m 1-word segments, m?
1 two-wordsegments, and so on.
If we bound the target languagen-grams to a maximal length k, then the complexityis linear, i.e.
O(km).The score of the best possible alignment is com-puted as follows: Let LT be the Target LanguageVocabulary, s a source word, ti be target segmentwords, and V = {ti ?
{LT }|i ?
1} the translationword set of s,We define the translation relation probabilityp(Tr(s) ?
{t0, t1, ..., tk}) as follows:1. p(Tr(s) ?
{t0, t1, ..., tk}) = max(p(ti|s))for all ti ?
{t0, t1, ..., tk} when {ti|ti ?
{t0, t1, ..., tk}} is not empty.2.
p(Tr(s) ?
{t0, t1, ..., tk}) = 0 otherwise.Then the score of the best alignment isSF?T = max{FT }SFT (4)where the score can be written as two componentsSFT = P1 ?
P2 (5)which can be further specified asP1 =( k?m=0max (p (Tr(si+m) ?
{tj...j+l}) , ?
))1k+1(6)P2 =( l?n=0max (p (Tr(tj+n) ?
{si...i+k}) , ?
))1l+1(7)where ?
is a very small probability used as a smooth-ing value.1.2 Length PenaltyThe ratio between source and target segment (n-gram) lengths should be comparable to the ratio be-tween the lengths of the source and target sentences,though certainly variation is possible.
Therefore, weadd a penalty function to the alignment probabilitythat increases with the discrepancy between the tworatios.Let the length of the source language segment bei and the length of a target language segment underconsideration be j.
Given a source language sen-tence length of n (in the corpus sentence containingthe fragment) and its corresponding target languagelength of m. The expected target segment length isthen given by j?
= i?
mn .
Further defining an allow-able difference AD, our implementation calculatesthe length penalty LP as follows, with the value ofthe exponent determined empirically:LPFT = min??
(|j ?
j?|AD)4, 1??
(8)The score for a segment including the penalty func-tion is then:SFT ?
SFT ?
(1?
LPFT ) (9)Note that, as intended, the score is forced to 0 whenthe length difference |j ?
j?| > AD.1.3 Distortion PenaltyFor closely-related language pairs which tend tohave similar word orders, we introduce a distortionpenalty to penalize the alignment score of any can-didate target fragment which is out of the expectedposition range.
First, we calculate CE , the expectedcenter of the candidate target fragment using CFS ,the center of the source fragment and the ratio oftarget- to source-sentence length.CE = CFS ?mn (10)88Then we calculate an allowed distance limit of thecenter Dallowed using a constant distance limit valueDL and the ratio of actual target sentence length toaverage target sentence length.Dallowed = DL ?mmaverage(11)Let Dactual be the actual distance difference be-tween the candidate target fragment?s center and theexpected center, and setSFT ???
?0, ifDactual ?
DallowedSFT(Dactual?Dallowed+1)2 , otherwise(12)Furthermore, we think that we can apply thispenalty to language pairs which have lower word-order similarities than e.g.
French-English.
Becausethere might exist certain positional relationships be-tween such language pairs, if we can calculate theexpected position using each language?s sentencestructure, we can apply a distortion penalty to thecandidate alignments.1.4 Anchor ContextIf the adjacent words of the source fragment and thecandidate target fragment are translations of eachother, we expect that this alignment is more likelyto be correct.
We boost SFT with the anchor contextalignment score SACp ,SACp = P (si?1 ?
tj?1) ?
P (si+k ?
tj+l) (13)SFT ?
(SFT )?
?
(SACp)1??
(14)Empirically, we found this combination gives thebest score for French-English when ?
= 0.6 andfor Romanian-English when ?
= 0.8, and leads tobetter results than the similar formulaSFT ?
?
?
SFT + (1?
?)
?
SACp (15)2 Experimental DesignIn previous work (Kim et al, 2005), we tested ouralignment method on a set of French-English sen-tence pairs taken from the Canadian Hansard corpusand on a set of English-Chinese sentence pairs, andcompared the results to human alignments.
For thepresent workshop, we chose to use the Romanian-English data which had been made available.Due to a lack of time prior to the period of theshared task, we merely re-used the parameters whichhad been tuned for French-English, rather than tun-ing the alignment parameters specifically for the de-velopment data.SPA was run under three experimental conditions.In the first, labeled ?SPA (c)?
in Tables 1 and 2, SPAwas instructed to examine only contiguous targetphrases as potential alignments for a given sourcephrase.
In the second, labeled ?SPA (n)?, a noncon-tiguous target algnment consisting of two contigu-ous segments with a gap between them was permit-ted in addition to contiguous target algnments.
Thethird condition (?SPA (h)?)
examined the impact ofa small amount of manual alignment information onthe selection of contiguous alignments.
Unlike thefirst two conditions, the presence of additional databeyond the training corpus forces SPA(h) into theUnlimited Resources track.We had a native Romanian speaker hand-align204 sentence pairs from the training corpus, andextracted 732 distinct translation pairs from thosealignments, of which 450 were already present inthe automatically-generated dictionaries.
The newtranslation pairs were added to the dictionaries forthe SPA(h) condition and the translation probabili-ties for the existing pairs were increased to reflectthe increased confidence in their correctness.
Hadmore time been available, we would have investi-gated more sophisticated means of integrating thehuman knowledge into the translation dictionaries.3 Results and ConclusionsTable 1 compares the performance of SPA on whatis now the development data against the submissionswith the best AER values reported by (Mihalceaand Pedersen, 2003) for the participants in the 2003workshop, including CMU, MITRE, RALI, Univer-sity of Alberta, and XRCE 1.
As SPA generates onlySURE alignments, the values in Table 1 are SUREalignments under the NO-NULL-Align scoring con-dition for all systems except Fourday, which did notgenerate SURE alignments.Despite the fact that SPA was designed specifi-cally for phrase-to-phrase alignments rather than the1Citations for individual participants?
papers have beenomitted for space reasons; all appear in the same proceedings.89Method Prec% Rec% F1% AERSPA (c) 64.47 62.68 63.56 36.44SPA (n) 64.38 62.70 63.53 36.47SPA (h) 64.61 62.55 63.56 36.44Fourday 52.83 42.86 47.33 52.67UMD.RE.2 58.29 49.99 53.82 46.61BiBr 70.65 55.75 62.32 41.39Ralign 92.00 45.06 60.49 35.24XRCEnolm 82.65 62.44 71.14 28.86Table 1: Romanian-English alignment results (De-velopment Set, NO-NULL-Align)word-to-word alignments needed for the shared taskand was not tuned for this corpus, its performance iscompetitive with the best of the systems previouslyused for the shared task.
We thus decided to submitruns for the official 2005 evaluation, whose resultingscores are shown in Table 2.On the development set, noncontiguous align-ments resulted in slightly lower precision than con-tiguous alignments, which was not unexpected, butrecall does not increase enough to improve F1 orAER.
The modified dictionaries improved preci-sion slightly, as anticipated, but lowered recall suffi-ciently to have no net effect on F1 or AER.The evaluation set proved to be very similar in dif-ficulty to the development data, resulting in scoresthat were very close to those achieved on the dev-testset.
Noncontiguous alignments again proved to havea very small negative effect on AER resulting fromreduced precision, but this time the altered dictionar-ies for SPA(h) resulted in a substantial reduction inrecall, considerably harming overall performance.After the shared task was complete, we performedsome tuning of the alignment parameters for theRomanian-English development test set, and foundthat the French-English-tuned parameters were closeto optimal in performance.
The AER on the develop-ment test set for the SPA(c) contiguous alignmentscondition decreased from 36.44% to 36.11% afterthe re-tuning.4 Future WorkEnhancements in the extraction of word-to-wordalignments from what is fundamentally a phrase-to-phrase alignment algorithm could probably furtherMethod Prec% Recall% F1% AER%SPA (c) 64.96 61.34 63.10 36.90SPA (n) 64.91 61.34 63.07 36.93SPA (h) 64.60 60.54 62.50 37.50Table 2: Evaluation results (NO-NULL-Align)improve results on the Romanian-English data.
Wealso intend to investigate principled, seamless inte-gration of manual alignments and dictionaries withprobabilistic ones, since the ad hoc method proveddetrimental.
Finally, a more detailed performanceanalysis is in order, to determine whether the closebalance of precision and recall is inherent in the bidi-rectionality of the algorithm or merely coincidence.5 AcknowledgementsWe would like to thank Lucian Vlad Lita for provid-ing manual alignments.ReferencesRalf D. Brown.
2004.
A Modified Burrows-WheelerTransform for Highly-Scalable Example-Based Trans-lation.
In Machine Translation: From Real Usersto Research, Proceedings of the 6th Conference ofthe Association for Machine Translation in the Amer-icas (AMTA-2004), volume 3265 of Lecture Notesin Artificial Intelligence, pages 27?36.
Springer Ver-lag, September-October.
http://www.cs.cmu.-edu/?ralf/papers.html.Jae Dong Kim, Ralf D. Brown, Peter J. Jansen, andJaime G. Carbonell.
2005.
Symmetric ProbabilisticAlignment for Example-Based Translation.
In Pro-ceedings of the Tenth Workshop of the European Asso-cation for Machine Translation (EAMT-05), May.
(toappear).Daniel Marcu and William Wong.
2002.
A Phrase-Based, Joint Probability Model for Statistical MachineTranslation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP-2002), July.
http://www.isi.edu/-?marcu/papers.html.Rada Mihalcea and Ted Pedersen.
2003.
An Evalua-tion Exercise for Word Alignment.
In Proceedings ofthe HLT-NAACL 2003 Workshop: Building and UsingParallel Texts: Data Driven Machine Translation andBeyond, pages 1?10.
Association for ComputationalLinguistics, May.90
