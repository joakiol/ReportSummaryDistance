Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 93?95,New York City, June 2006. c?2006 Association for Computational LinguisticsA Graph-Search Framework for GeneId Ranking(Extended Abstract)William W. CohenMachine Learning DepartmentCarnegie Mellon UniversityPittsburgh PA 15213wcohen@cs.cmu.edu1 IntroductionOne step in the curation process is geneId finding?the task of finding the database identifier of everygene discussed in an article.
GeneId-finding wasstudied experimentally in the BioCreatIvE challenge(Hirschman et al, 2005), which developed testbedproblems for each of three model organisms (yeast,mice, and fruitflies).
Here we consider geneId rank-ing, a relaxation of geneId-finding in which the sys-tem provides a ranked list of genes that might bediscussed by the document.
We show how multi-ple named entity recognition (NER) methods canbe combined into a single high-performance geneId-ranking system.2 Methods and ResultsWe focused on the mouse dataset, which was thehardest for the BioCreatIvE participants.
Thisdataset consists of several parts.
The gene synonymlist consists of 183,142 synonyms for 52,594 genes;the training data consists of 100 mouse-relevantMedline abstracts, associated with the MGI geneId?sfor those genes that are mentioned in the abstract;the evaluation data consists of an additional 50mouse-relevant Medline abstracts, also associatedwith the MGI geneId?s as above; the test data con-sists of an additional 250 mouse-relevant Medlineabstracts, again associated with MGI geneId?s; fi-nally the historical data consists of 5000 mouse-relevant Medline abstracts, each of which is associ-ated with the MGI geneId?s for all genes which are(a) associated with the article according to the MGIdatabase, and (b) mentioned in the abstract, as deter-mined by an automated procedure based on the genesynonym list.1 We also annotated the evaluation-data for NER evaluation.We used two closely related gene-protein NERsystems in our experiments, both trained usingMinorthird (Min, 2004) on the YAPEX corpus(Franze?n et al, 2002).
The likely-protein extractorwas designed to have high precision and lower re-call, and the possible-protein extractor was designedto have high recall and lower precision.
As shown inTable 1, the likely-protein extractor performs wellon the YAPEX test set, but neither system performswell on the mouse evaluation data?here, they per-form only comparably to exact matching against thesynonym dictionary.
This performance drop is typ-ical when learning-based NER systems are testedon data from a statistical distribution different fromtheir training set.As a baseline for geneId-ranking, we used a stringsimilarity metric called soft TFIDF, as implementedin the SecondString open-source software package(Cohen and Ravikumar, 2003), and soft-matched ex-tracted gene names against the synonym list.
Ta-ble 2 shows the mean average precision on the eval-uation data.
Note that the geneId ranker based onpossible-protein performs statistically significantlybetter2 than the one based on likely-protein, eventhough possible-protein has a lower F score.To combine these two NER systems, we representall information as a labeled directed graph which in-1The training data and evaluation data are subsets of theBioCreatIvE ?devtest?
set.
The historical data was called ?train-ing data?
in the BioCreatIvE publications.
The test data is thesame as the blind test set used in BioCreatIvE.2With z = 3.1, p > 0.995 using a two-tailed paired test.93Precis.
Recall Fmouse evallikely-prot 0.667 0.268 0.453possible-prot 0.304 0.566 0.396dictionary 0.245 0.439 0.314YAPEX testlikely-prot 0.872 0.621 0.725YAPEX system 0.678 0.664 0.671Table 1: Performance of the NER systems on themouse evaluation corpus and the YAPEX test cor-pus.Mean AveragePrecision (MAP)mouse evaluation datalikely-prot + softTFIDF 0.450possible-prot + softTFIDF 0.626graph-based ranking 0.513+ extra links 0.730+ extra links & learning 0.807Table 2: Mean average precision of several geneId-ranking methods on the 50 abstracts from the mouseevaluation dataset.cludes the test abstracts, the extracted names, thesynonym list, and the historical data.
We then useproximity in a graph for ranking.
The graph usedis illustrated in Figure 1.
Nodes in this graph canbe either files, strings, terms, or user-defined types.Abstracts and gene synonyms are represented as fileand string nodes, respectively.
Files are linked tothe terms (i.e., the words) that they contain, andterms are linked to the files that contain them.3 Filenodes are also linked to string nodes correspondingto the output of an NER system on that file.
(Stringnodes are simply short files.)
The graph also con-tains geneId nodes and synonym string nodes cre-ated from the dictionary, and for each historical-dataabstract, we include links to its associated geneIdnodes.Given this graph, gene identifiers for an abstractare generated by traversing the graph away from theabstract node, and looking for geneId nodes that are?close?
to the abstract according to a certain proxim-3In fact, all edges have inverses in the graph.Figure 1: Part of a simplified version of the graphused for geneId ranking.ity measure for nodes.
Similarity between two nodesis defined by a lazy walk process, similar to PageR-ank with decay.
The details of this are described inthe full paper and elsewhere (Minkov et al, 2006).Intuitively, however, this measures the similarity oftwo nodes by the weighted sum of all paths that con-nect the nodes, where shorter paths will be weightedexponentially higher than longer paths.
One conse-quence of this measure is that information associ-ated with paths like the one on the left-hand side ofthe graph?which represents a soft-match between alikely-protein and a synonym?can be reinforced byother types of paths, like the one on the right-handside of the figure.As shown in Table 2, the graph-based approachhas performance intermediate between the two base-line systems.
However, the baseline approaches in-clude some information which is not available in thegraph, e.g., the softTFIDF distances, and the implicitknowledge of the ?importance?
of paths from an ab-stract to a synonym via an NER-extracted string.
Toinclude this information, we inserted extra edges la-beled proteinToSynonym between the extracted pro-tein strings x and comparable synonyms y, and also?short-cut?
edges in the graph that directly link ab-stracts x to geneId nodes reachable via one of the?important?
paths described above.As Table 2 shows, graph search with the aug-mented graph does indeed improve MAP perfor-mance on the mouse evaluation data: performanceis better than the simple graph, and also better than94MAP Avg Max Fmouse test datalikely-prot + softTFIDF 0.368 0.421possible-prot + softTFIDF 0.611 0.672graph-based ranking 0.640 0.695+ extra links & learning 0.711 0.755Table 3: Mean average precision of several geneId-ranking methods on the 250 abstracts from themouse test dataset.either of the baseline methods described above.Finally we extended the lazy graph walk to pro-duce, for each node x reached on the walk, a featurevector summarizing the walk.
Intuitively, the fea-ture vector records certain features of each edge inthe graph, weighting these features according to theprobability of traversing the edge.
We then use alearning-to-rank method (Collins and Duffy, 2002)to rerank the top 100 nodes.
Table 2 shows thatlearning improves performance.
In combination, thetechniques described have improved MAP perfor-mance to 0.807, an improvement of nearly 80% overthe most natural baseline (i.e., soft-matching the dic-tionary to the NER method with the best F measure).As a final prospective test, we applied these meth-ods to the 250-abstract mouse test data.
We com-pared their performance to the graph-based searchmethod combined with a reranking postpass learnedfrom the 100-abstract mouse training data.
The per-formance of these methods is summarized in Ta-ble 3.
The somewhat lower performance is proba-bly due to variation in the two samples.4 We alsocomputed the maximal F-measure (over any thresh-old) of each ranked list produced, and then averagedthese measures over all queries.
This is compara-ble to the best F1 scores in the BioCreatIvE work-shop, although the averaging for BioCreatIvE wasdone differently.3 ConclusionWe evaluate several geneId-ranking systems, inwhich an article is associated with a ranked list ofpossible gene identifiers.
We find that, when used4For instance, the test-set abstracts contain somewhat moreproteins on average (2.2 proteins/abstract) than the evaluation-set abstracts (1.7 proteins/abstract).in the most natural manner, the F-measure perfor-mance of an NER systems does not correlate wellwith MAP of the geneId-ranker based on it: rather,the NER system with higher recall, but lower overallperformance, has significantly better performancewhen used for geneId-ranking.We also present a graph-based scheme for com-bining NER systems, which allows many types ofinformation to be combined.
Combining this sys-tem with learning produces performance much bet-ter than either NER system can achieve alone.
Onaverage, 68% of the correct proteins will be found inthe top two elements of the list, 84% will be foundin the top five elements, and more than 90% willbe found in the top ten elements.
This level of per-formance is probably good enough to be of use incuration.AcknowledgementThe authors with to thank the organizers of BioCre-atIvE, Bob Murphy, Tom Mitchell, and EinatMinkov.
The work described here is supported byNIH K25 grant DA017357-01.ReferencesWilliam W. Cohen and Pradeep Ravikumar.
2003.
Sec-ondString: An open-source Java toolkit of approxi-mate string-matching techniques.
Project web page,http://secondstring.sourceforge.net.Michael Collins and Nigel Duffy.
2002.
New ranking algo-rithms for parsing and tagging: Kernels over discrete struc-tures, and the voted perceptron.
In Proceedings of the ACL.Kristofer Franze?n, Gunnar Eriksson, Fredrik Olsson, LarsAsker Per Lide?n, and Joakim Coster.
2002.
Protein namesand how to find them.
International Journal of Medical In-formatics, 67(1-3):49?61.Lynette Hirschman, Alexander Yeh, Christian Blaschke, andAlfonso Valencia.
2005.
Overview of BioCreAtIvE: criti-cal assessment of information extraction for biology.
BMCBioinformatics, 6(S1).2004.
Minorthird: Methods for identifying names and ontolog-ical relations in text using heuristics for inducing regularitiesfrom data.
http://minorthird.sourceforge.net.Einat Minkov, William Cohen, and Andrew Ng.
2006.
A graphframework for contextual search and name disambiguationin email.
In SIGIR ?06: Proceedings of the 29th annual in-ternational ACM SIGIR conference on research and devel-opment in information retrieval, August.
To appear.95
