Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 603?608,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsSemantic Information and Derivation Rules for Robust Dialogue ActDetection in a Spoken Dialogue SystemWei-Bin Liang1 Chung-Hsien Wu2Department of Computer Science andInformation EngineeringNational Cheng Kung UniversityTainan, Taiwan1liangnet@gmail.com2chunghsienwu@gmail.comChia-Ping ChenDepartment of Computer Scienceand EngineeringNational Sun Yat-sen UniversityKaohsiung, Taiwancpchen@mail.cse.nsysu.edu.twAbstractIn this study, a novel approach to robust di-alogue act detection for error-prone speechrecognition in a spoken dialogue system isproposed.
First, partial sentence trees are pro-posed to represent a speech recognition out-put sentence.
Semantic information and thederivation rules of the partial sentence treesare extracted and used to model the relation-ship between the dialogue acts and the deriva-tion rules.
The constructed model is then usedto generate a semantic score for dialogue actdetection given an input speech utterance.
Theproposed approach is implemented and evalu-ated in a Mandarin spoken dialogue system fortour-guiding service.
Combined with scoresderived from the ASR recognition probabil-ity and the dialogue history, the proposed ap-proach achieves 84.3% detection accuracy, anabsolute improvement of 34.7% over the base-line of the semantic slot-based method with49.6% detection accuracy.1 IntroductionAn intuitive framework for spoken dialogue system(SDS) can be regarded as a chain process.
Specifi-cally, the automatic speech recognition (ASR) mod-ule accepts the user?s utterance Ut and returns astring of words Wt The spoken language under-standing (SLU) module converts Wt to an abstractrepresentation of the user?s dialogue act (DA).
Thedialogue management (DM) module determines theuser?s dialogue act A?t and accordingly decides thecurrent act of the system.
The system DA is con-verted to a surface representation by natural lan-Figure 1: Details of the SLU and DM modules.guage generation in the textual form, which ispassed to a text-to-speech synthesizer for speechwaveform generation.
The cycle repeats when theuser responds with a new utterance.
Clearly, one cansee that the inference of the user?s overall intentionvia DA detection is an important task in SDS.Figure 1 depicts the training and test phases ofthe SLU module and the DM module in our system.The dataflow for training and testing are indicatedby blue arrows and red arrows, respectively.
Theinput word sequences are converted to partial sen-tence trees (PST) (Wu and Chen, 2004) in the PSTConstruction block.
The derivation rule (DR) Gen-eration block extracts derivation rules from the train-ing text.
The DR-DA matrix is created after cluster-ing the sentences into different dialogue acts (DAs),counting the occurrences the DRs in DA, and intro-ducing an entropy-based weighting scheme (Belle-garda, 2000).
This matrix is pivotal in the computa-tion of the lexical score.
Finally, the lexical, the his-tory, and the ASR scores are combined to decide the603optimal dialogue act, and a proper action by the sys-tem is taken.
In our system, not only the clean textdata but also the noisy ASR output data are used inorder to take the error-proneness of ASR output intoaccount.
Furthermore, a predefined keyword list isused and the keyword tokens are replaced by the cor-responding named entity classes (NEC) in order toobtain a compact feature set.2 Models for Dialogue Act DetectionReferring to the SDS depicted in Figure 1, the DAdetection can be formulated as follows.
At turn t,the most likely DA is determined byA?t = argmaxA??
Pr(A|Ut, Ht), (1)where Ut is the user?s utterance, Ht is the dialoguehistorical information, and ?
= {A1, .
.
.
, Aq} is theset of DAs.
Using the maximum approximation forsummation, (1) can be written asA?t = argmaxA??
?WPr(A,W|Ut, Ht)?
argmaxA?
?maxWPr(A,W|Ut, Ht)= argmaxA?
?,WPr(W|Ut, Ht)Pr(A|W, Ut, Ht),(2)where W is the ASR output.
Since the ASR outputis independent of Ht given Ut, the ASR-related firstterm in (2) can be re-written asPr(W|Ut, Ht) = Pr(W|Ut) ?
f(W, Ut), (3)where the function f(W, Ut) is introduced as theASR score function.
In addition, assuming that theinformation provided by Ut is completely conveyedin W, we can approximate the second term in (3) bythe product of two functionsPr(A|W, Ut, Ht) = Pr(A|W, Ht)?
g(A,W) h(A,Ht),(4)where g(A,W) is introduced as the lexical scorefunction, and h(A,Ht) is introduced as the historyscore function.
Thus, (3) can be re-written asA?t ?
argmaxA?
?,Wf(W, Ut) g(A,W) h(A,Ht).
(5)In Sections 3 and 4, we specify and explain how thescores in (5) are computed.Figure 2: An example of a dialogue management mod-ule using n-gram model for dialogue act sequence in thedomain of historic spot.3 ASR Score and History ScoreFor the ASR score, we use the conventional recog-nition probability of the ASR recognition model.For the history score, similar to the schemes usedin (Hori et al, 2009c; Hori et al, 2009b; Hori et al,2009a), a back-off bi-gram model for DA sequenceis estimated from the data collected by the SDS.
Theestimated bi-gram model is used to calculate the his-tory score.
That is,h(A,Ht) = Pr(At = A | At?1).
(6)Essentially, (6) is based on a Markov model assump-tion for the chain of the dialogue acts.
Figure 2shows an example of dialogue controlling model ofan SDS.
In this example, each state represents a DA.A dialogue begins with the greeting state and endswith the ending state.
During a session, a user caninquire the system about the provided services andthen choose one service to continue (e.g., the loop-back connection in Figure 2).4 The Lexical Score FunctionThe main challenge of this system is the computa-tion of the lexical score g(A,W).
In this paper, wepropose a novel data-driven scheme incorporatingmany techniques.4.1 Construction of Partial Sentence TreeIn an SDS, it is often beneficial to define a set ofkeywords K, and a set of non-keywords N .
Eachword w ?
K should be indicative of the DA ofthe sentence.
The set of sentences S containingat least one keyword in K, can be represented asS = N ?
(K N ?
)+, where K+ means a string of oneor more words in K. Given a sentence s ?
S , a par-tial sentence is formed by keeping all the keywordsin s and some of the non-keywords in s. These604Figure 3: Construction of the partial sentence tree for thesentence Where is the Anping-Fort.partial sentences can be compiled in a tree, calledthe partial sentence tree (PST) and denoted as T (s).The motivation for using PST is to achieve robustDA detection as the ASR module could be error-prone in adverse environments.
In addition, wordsthat are not confidently recognized are replaced bya special non-keyword token called Filler.
Specif-ically, we compute the z-score (Larsen and Marx,2000) of each word w in the ASR output.
Figure 3illustrates the PST for the sentences: Where is theAnping-Fort.
There are two keywords Where andAnping-Fort and two non-keywords is and the.
Notethat with 2 non-keywords in the original sentence s,we have 22 = 4 partial sentences in the PST T (s).4.2 Extraction of the Derivation RulesAfter text processing, a sentence s is parsed by thestatistical Stanford parser (S-parser) (Levy and Man-ning, 2003).
Let the grammar of the S-parser bedenoted as a 5-tuple G = (V ,?,P, S,D) whereV is the variable (non-terminal) set,?
is the termi-nal symbol set,P is the production rule set, S is thesentence symbol, and D is a function defined on Pfor rule probability (Jurafsky and Martin, 2009).
Aderivation rule is defined to be a derivation of theform A ?
B ?
w where A,B ?
V and w ?
?.The parsing result of the exemplar sentence s repre-sented in the parenthesized expression is shown inFigure 4.
From the parsing result, four DRs are ex-tracted.
Essentially, we have one DR for each lexicalword in the sentence.
Totally, given a corpus, l rulesare extracted and defined as D = {R1, R2, .
.
.
, Rl}.Based on PST T (s) and DR set D, a vector rep-resentation v(s) for sentence s can be constructedaccording to the DRs used in T (s).
That isvi(s) ={1, if Ri ?
T (s)0, otherwise(7)Parse Result Derivation Rule(Root DR1: WHADVP (WRB Where)(SINV DR2: VP (VBZ is)(FRAG DR3: NP (DT the)(WHADVP (WRB Where))) DR4: NP (NNP Anping-Fort)(VP (VBZ is))(NP (DT the) (NNP Anping-Fort))))Figure 4: The parse result (left) and the extracted deriva-tion rules (right) for the exemplar sentence s.For example, v(s) = [1 0 1 0]T means that there arefour derivation rules, of which R1 and R3 are usedin T (s).
The motivation for using DRs instead ofthe lexical words is to incorporate the part-of-speech(POS) tags information.
POS tags are helpful inthe disambiguation of noun-and-verb homonyms inChinese.
Moreover, the probabilistic nature of theS-parser renders the DRs extracted from the pars-ing results quite robust and consistent, even for theerror-prone ASR output sentences.4.3 Generation of Dialogue ActsThe basic idea of data-driven DA is to cluster sen-tences in the set and identify the clusters as formedby the sentences of the same DA.
In this work, thespectral clustering algorithm (von Luxburg, 2007) isemployed for sentence clustering.
Specifically, sup-pose we have n vectors represented as C = {vk ,v(sk), k = 1, .
.
.
, n} converted from sentences ac-cording to (7).
From C, we construct an n ?
n sim-ilarity matrix M , in which each element Mkk?
isa symmetric nonnegative distance measure betweenvk and vk?
.
In this work, we use the cosine measure.The matrix M can be regarded as the adjacency ma-trix of a graph G with node set N and edge set E ,where N is 1-to-1 correspondent to the set C, and Ecorresponds to the non-zero entries in M .
The nor-malized Laplacian matrix of M isL , I ?D?
12MD?
12 , (8)where D is a diagonal matrix with entriesDkk?
= ?kk?n?j=1Mkj .
(9)It has been shown (von Luxburg, 2007) that the mul-tiplicity of the eigenvalue 0 for L equals the num-ber of disjoint connected components in G. In ourimplementation, we find the q eigenvectors of thenormalized Laplacian matrix of M of the smallest605eigenvalues.
We put these eigenvectors in an n ?
qorthogonal matrix Q, and cluster the row vectors toq clusters.
Each cluster correspond to a data-drivenDA Aj , and the n sentences are classified accordingto the cluster they belong to.In order to use the DRs in a PST as a knowl-edge source for DA detection, we essentially need tomodel the relationship between the random DA andthe random DR. Denote the random DA by X andthe random DR by Y .
Given a text corpus, let nij bethe accumulated count that Ri occurs in a sentencelabeled as Aj .
From nij , the conditional probabilityof Y = Aj given X = Ri can be defined as?ij = p?
(Y = Aj |X = Ri) , nij?qj?=1 nij?, (10)where j = 1, .
.
.
, q.
The normalized entropy for theconditional probability function (10) isi = ?1log qq?j=1?ij log ?ij .
(11)From (10) and (11), a matrix ?
can be constructedby ?ij = (1 ?
i)?ij .
We call ?
the derivation-rule dialogue-act (DR-DA) matrix, in which eachrow corresponds to a derivation rule and each col-umn corresponds to a dialogue act.4.4 Distance MeasureIn our system, the lexical score g(A,W) in (5) isfurther broken into two termsg(A,W) ?
gR(A, s)gN (A,W) (12)where gR(A, s) is called the DR score andgN (A,W) is called the named entity score.
Notethat s denotes the sentence after text processing.
Thecosine distance measure is employed for the deriva-tion rule score,gR(A = Aj , s) = max?
?T (s)bT?aj|b?||aj |(13)where bT?
is the vector representation (using the co-ordinates of the DRs) of a partial sentence ?
in T (s),and aj is the jth column vector in the DR-DA matrix?.
For the named entity score, we use the approxi-mationgN (A,W) =?k?
(A,?k) (14)NEC/SC Name entities/WordsCity Tainan, Taipei, KaohsiungSpot Anping-Fort, Sun-Moon LakeGreeting Welcome, HelloEnding Thanks, ByeTable 1: Examples of named entity classes (NEC) andsemantic classes (SC)where ?k is the kth named entity in W. Note that?(A,?)
is estimated from a training corpus by rela-tive frequencies.5 Experiments and DiscussionTo evaluate the proposed method of dialogue act de-tection for robust spoken dialogue system, we adoptthe commonly-used Wizard-of-Oz approach (Fraserand Gilbert, 1991) to harvest the Tainan-city tour-guiding dialogue corpus in a lab environment andexperiment with simulated noisy ASR results.
Thedetails are given in this section.
Two types of datafrom different sources are collected for this work.The first type of data, called A-data, is a travel infor-mation data set harvested from the databases avail-able on the web, e.g., Wikipedia and Google Map.A-data consists of 1, 603 sentences with 317 wordtypes.
The second type of data, called Q-data, is theedited transcription of a speech data set simulatinghuman-computer dialogues in a lab environment.
Q-data is intended for the system to learn to handle thevarious situations, e.g., misunderstanding the user?sintention.
It consists of 144 dialogues with 1, 586 ut-terances.
From the Q-data, 28 named entity classesand 796 derivation rules were obtained from the S-parser.
Table 1 gives some examples of the selectedNECs and semantic classes.5.1 Experimental ConditionsA Mandarin speech recognition engine was real-ized using the HTK (Young et al, 2006), which iscommonly used in research and development.
Forspeech features, 39 dimensions were used, includ-ing 12 dimensions of mel-frequency cepstral coeffi-cients (MFCCs), one dimension of log energy, andtheir delta and acceleration features.
In total, theacoustic models are composed of 153 subsyllableand 37 particle models (e.g., EN, MA, OU) based606number of DA types 37 38 39detection accuracy 82.7 84.3 77.2Table 2: Detection accuracies with varying numbers ofDA types.on Hidden Markov Model (HMM) with 32 Gaus-sian mixture components per state.
For the lan-guage model, SRILM toolkit (Stolcke, 2002) wasemployed to estimate a bi-gram model with the Q-data.
The average word accuracy of the ASR moduleis 86.1% with a lexicon of 297 words.
Note that thevocabulary size is small due to a limited domain.
5-fold cross validation method was utilized for systemevaluation.As shown in Table 2, one can see that 38 DA typesachieve the best performance for the proposed detec-tion model.
Therefore, we use 38 DA types (q = 38)in our system.
Note that some exemplar DAs areshown in Figure 2.5.2 Incremental EvaluationWe incrementally add techniques in our SDS un-til the complete proposed overall system is imple-mented, to observe the effect of these techniques.The detection accuracies are shown in Table 3.
Inthis table, the third column (ASR) represents the re-sults of the experiment using the ASR transcriptsdirectly.
The fourth column (REF) uses the refer-ence transcripts, so it represents the case with per-fect ASR.
The first (40%-sim) and second (60%-sim) column represents the simulation where 40%and 60% of the words in the reference transcriptsare retained, respectively.
There are five sets of ex-periments summarized in this table.
For the base-line, each keyword corresponds to a coordinate inthe vector representation for a sentence.
The resultsare shown in the first row (baseline).
In the secondset of experiments (NEC), the keywords are replacedby their NEC.
In the third set of experiments (PST),the PST representation for a sentence is used.
Inthe fourth set of experiments (DR), the derivationrule representation of a sentence is used.
Finally, theentropy-normalized DR-DA matrix is used to repre-sent sentences, and the results are shown in the lastrow (DR-DA).
There are strong improvements whenNEC (from 49.6% to 56.8%) and PST (from 56.8%to 76.2%) representations are introduced.
Moreover,40%-sim 60%-sim ASR REFbaseline 17.2 32.6 49.6 60.9NEC 22.4 36.8 56.8 76.9PST 29.8 49.2 76.2 91.1DR 26.3 48.0 81.6 92.1DR-DA 26.3 47.4 82.9 93.3Table 3: Detection accuracies of cascading componentsfor the lexical score.value of ?L 0.5 0.6 0.7 0.8Accuracy (%) 84.3 84.6 85.1 84.9Table 4: Evaluation on different weighted product fusionthe DR and DR-DA representations also lead to sig-nificant improvements, achieving 81.6% to 82.9%,respectively.
For the other conditions of 40%-sim,60%-sim, and REF, similar improvements of usingNEC and PST are observed.
Using DR-DA, how-ever, suffers from performance degradation whenthe keywords are randomly discarded.5.3 Evaluation on the Weighting SchemeWe examine the effect of different weighted productfusion and rewrite the formulation in (5) asA?t ?
argmaxA?
?,W[f(W, Ut)g(A,W)]?A [h(A,Ht)]?L(15)where ?A is the weight for the ASR score and thelexical score, ?L is the weight of the history score,and ?A + ?L = 1.
Table 4 shows the results thathistory information will effect on the DA detection,because it was estimated by the dialogue turns thatcaptured the user behaviors.6 ConclusionsIn this paper, a noise-robust dialogue act detectionusing named entity classes, partial sentence trees,derivation rules, and entropy-based dialogue act-derivation rule matrix is investigated.
Data-drivendialogue acts are created by the spectral cluster-ing algorithm, which is applied on the vectors ofsentences represented by the derivation rules.
Ourspoken dialogue system benefits when the proposedcomponents are integrated incrementally.
For thefully integrated system, we find that the proposedapproach achieves 84.3% detection accuracy.607ReferencesJ.
Bellegarda.
2000.
Exploiting latent semantic informa-tion in statistical language modeling.
Proceedings ofthe IEEE, 88:1279?1296.N.
Fraser and G. N. Gilbert.
1991.
Simulating speechsystems.
Computer Speech and Language, 5(1):81?99.C.
Hori, K. Ohtake, T. Misu, H. Kashioka, and S. Naka-mura.
2009a.
Recent advances in wfst-based dialogsystem.
In Proc.
INTERSPEECH, pages 268?271.C.
Hori, K. Ohtake, T. Misu, H. Kashioka, and S. Naka-mura.
2009b.
Statistical dialog management appliedto wfst-based dialog systems.
In Proc.
IEEE Inter-national Conference on Acoustics Speech and SignalProcessing (ICASSP), pages 4793?4796.C.
Hori, K. Ohtake, T. Misu, H. Kashioka, and S. Naka-mura.
2009c.
Weighted finite state transducer basedstatistical dialog management.
In Proc.
ASRU.D.
Jurafsky and J. H. Martin.
2009.
Speech and Lan-guage Processing, 2nd Edition.
Pearson Education.R.
J. Larsen and M. L. Marx.
2000.
An Introduction toMathematical Statistics and Its Applications, 3rd Edi-tion.
ISBN: 0139223037.R.
Levy and C. Manning.
2003.
Is it harder to parsechinese, or the chinese treebank?
In Proc.
AnnualMeeting of ACL, pages 439?446.A.
Stolcke.
2002.
Srilm - an extensible language model-ing toolkit.
In Proc.
International Conference on Spo-ken Language Processing, pages 901?904.U.
von Luxburg.
2007.
A tutorial on spectral clustering.Statistics and Computing, 17(4).C.-H. Wu and Y.-J.
Chen.
2004.
Recovery fromfalse rejection using statistical partial pattern trees forsentence verification.
Speech Communication, 43(1-2):71?88.Steve J.
Young, D. Kershaw, J. Odell, D. Ollason,V.
Valtchev, and P. Woodland.
2006.
The HTK BookVersion 3.4.
Cambridge University Press.608
