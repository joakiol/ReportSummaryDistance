Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 89?97,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsGenerating Shifting Sentiment for a Conversational AgentSimon WhiteheadUniversity of Melbourne, Australiasrwhitehead@gmail.comLawrence CavedonRMIT University, Australialawrence.cavedon@rmit.edu.auAbstractWe investigate techniques for generating al-ternative output sentences with varying sen-timent, using (an approximation to) theValentino method, based on SentiWordNet, ofGuerini et al We extend this method by filter-ing out unacceptable candidate sentences, us-ing bigrams sourced from different corpora todetermine whether lexical substitutions are ap-propriate in the given context.
We also com-pare the generated candidates against humanjudgements of whether the desired sentimentshift has occurred: our results suggest limi-tations with the overall knowledge-based ap-proach, and we propose potential directionsfor improvement.1 IntroductionThe design of more natural or believable conver-sational agents (Bates, 1994; Pelachaud and Bilvi,2003) requires the need for such agents to communi-cate affectively, by the display of emotion or attitudetowards objects, other agents, or states of affairs.More engaging or influential agents may seek to ac-tually affect their conversational partner at a deeperlevel, for example, by influencing their emotionalstate (van der Sluis and Mellish, 2008).
Previouswork in this area has explored the use of gestures andfacial expression (Caridakis et al, 2007) and rhythmand prosody of speech (Zovato et al, 2008) for ex-pressing affect; however there has been little workon generation of affective language in dialogue.Our general approach is inspired by (Fleischmanand Hovy, 2002)?s work on generating differentsurface-level versions of utterance content, depend-ing on an agent?s appraisals towards objects, char-acters and events in its environment.
While theirapproach is effective, it relies on manual creationof lexical alternatives, customized to the applicationdomain.
We are interested in approaches that willscale, and can be applied domain-independently.While our ultimate aim is generation of languagethat relects emotional state, in this work we in-vestigate the automatic generation of varying ?sen-timent?
in output utterances; we focus on senti-ment mainly due to the recent development of use-ful resources for this task.
(Guerini et al, 2008)?sValentino system is an approach to automaticallygenerating candidate output utterances with differ-ent sentiment from an original; the authors suggestECAs as a possible application scenario for theirtechniques.
We explore this suggestion, implement-ing a lexical substitution (McCarthy and Navigli,2007) approach to dialogue generation with sen-timent, using the Valentino approach and associ-ated resources.
Lexical substitution approaches raisewell-known challenges, and we investigate a numberof techniques to address these in Section 4; for ex-ample, using bigrams and grammatical relations todetermine which substitutions are acceptable basedon their context in a sentence.1Our techniques show improvement over naive lex-ical substitution; however, an evaluation with humansubjects suggests that a deeper problem is that even?acceptable?
candidate sentences generated by themethod do not match human judgements with re-spect to sentiment shift: i.e., alternatives labeled asmore positive (resp., negative) than the original bythe system are often seen as a sentiment shift in theopposite direction by human judges (Section 5).2 Background: ValentinoThe Valentino2 system (Guerini et al, 2008) is atool developed from WordNet and SentiWordNet1Guerini et al suggest this as an area for further work.2VALENced Text INOculator89designed to produce more positively or negativelyslanted versions of text.
Input to the system consistsof a short sentence, and a target valence (between-1 and 1), which indicates the desired polarity andmagnitude of sentiment in the modified output text.Valentino uses a number of strategies for adding, re-moving, or substituting certain words in order to al-ter the overall sentiment of the sentence.
Table 1shows examples of Valentino output for different tar-get valences, with modifications in italics.To perform the word-substitution, (Guerini et al,2008) created a resource of OVVTs3: vectors of se-mantically related terms which may substitute forone another.
The OVVTs were constructed us-ing structural analysis of WordNet, and are dividedinto adjectives, nouns, and verbs.
(Guerini et al,2008) also constructed a separate resource of Mod-ifier OVVTs which list adverbs that can be used tomodify verbs.
Modifier OVVTs were created usingverbs extracted from certain FrameNet4 categories,then recording which adverbs occur next to theseverbs in the British National Corpus (BNC).
Eachterm in the Valentino resource was assigned a senti-ment valence, which corresponds to the SentiWord-Net score of its parent WordNet synset.
Table 2shows part of an OVVT containing the noun ?man?.5Term POS Sense Valencehunk n 1 0.375man n 1 0dude n 1 -0.125beau n 2 -0.125Table 2: (Abridged) example of an OVVTTo generate a modified sentence, (Guerini et al,2008) apply the following strategies to each word6until the sentence valence (total of term valences)meets the target:1.
Paraphrase: Lemmas with only one senseare replaced by their WordNet gloss, which isscored for sentiment using the OVVTs;3We assume OVVT stands for Ordered Vector of ValencedTerms; this is not explicit in (Guerini et al, 2008).4http://framenet.icsi.berkeley.edu/5All our examples and evaluations are using a version of theOVVTs made available by Marco Guerini on May 13, 2009.6Actually, to the lemma of each word.2.
Use of most frequent senses: The OVVTs aresearched using only the most frequent senses;3.
Adjective modification: Adjectives are re-placed with their stronger/weaker alternativessuch that the target valence is not exceeded;4.
Verb modification: Verbs are modified by in-serting, removing, or replacing intensifier ordowntoner adverbs.The final sentence is rendered as surface text bytransforming each of the inserted lemmas back intothe original morphology.
(Guerini et al, 2008) suggest their system?s po-tential application to dialogue generation in an ECA,enabling emotional variation.
However, they do notpresent an evaluation of Valentino?s effectiveness.We expect that not all output utterances generatedusing their method will be sensible in the context ofa believable ECA, for the following reasons:Unconventional Word Usage: Upon inspection,we found the OVVTs often contain severalwords which are no longer conventionally used(e.g.
?beau?).
For an ECA to be believable, wehypothesise that such unpopular words shouldnot be considered as potential candidates forsubstitution.Incorrect Grammatical Context: The naive ver-sion of the Valentino method assumes that allwords in an OVVT can be substituted for oneanother regardless of their context in the sen-tence (see Table 3); Guerini et al propose thisas an area for future work.
We explore semi-informed solutions using bigrams and gram-matical relations to eliminate syntactically in-correct substitutions.... Williams was not interested (in) girls... Williams was not concerned (with) girls... Williams was not fascinated (by) girlsTable 3: Illustration of grammatical context issues3 ImplementationWe implemented a lexical substitution approach tovarying valence, closely following the Valentino ap-proach described in (Guerini et al, 2008).
We did90Valence Sentencen/a Bob admitted that John is absolutely the best guy1.0 Bob wholeheartedly admitted that John is absolutely a superb hunk0.5 Bob openly admitted that John is highly the redeemingest signor0.0 Bob admitted that John is highly a well-behaved sir-0.5 Bob sadly confessed that John is nearly a well-behaved beau-1.0 Bob harshly confessed that John is pretty an acceptable eunuchTable 1: Example of Valentino sentiment shifting (Guerini et al, 2008)not implement all the above strategies?in partic-ular, we did not implement paraphrasing, adverbmodification, or morphology synthesis; rather wefocused on developing techniques that would ad-dress the lexical substitution issues described above.As with Valentino, we calculate sentence valenceby summing the valences of all terms in the sentencewhich are present in the OVVTs7.
However, as avariation on Valentino, we aggregated sentence shiftinto five broad categories: ?major positive shift?
;?minor positive shift?
; ?no shift?
; ?minor negativeshift?
; ?major negative shift?.Since most OVVTs contain only lemmas, we firstperformed lemmatisation using the MorphAdorner8package.
To locate a term in the OVVTs, we firstsearch for the original word morphology, then if nomatch is found we try using the lemma.As with (Guerini et al, 2008), we included candi-dates frommultiple senses of a matching word; how-ever, rather than stopping at the third most frequentsense, we explored up to sense forty so as to increasethe number of possible substitutions for terms.9 Weperformed a very naive version of word sense dis-ambiguation (WSD) (see below), but lack of WSDwas an issue (discussed later).Alternative sentences were generated by modify-ing at most a single word; this reduces the explo-sion in the number of alternatives, but the methodsdescribed could just as easily apply to alternativesconstructed by varying multiple words.The novel aspect of our implementation was the?candidate filtering?
techniques: i.e.
techniquesfor deciding whether to accept a candidate replace-7Since we ignore adverbs, we do not include these whenscoring a sentence.8http://morphadorner.northwestern.edu/9Increasing this further increased the number of alternativesbut did not improve performance.ment term as substitute in a given sentence; this wasspecifically designed to address the issues above.
Inthe next section, we describe filtering techniques us-ing simple bigrams and grammatical relations, andevaluate the effectiveness of each.4 Evaluation: Candidate FilteringThe data set we used for this evaluation consisted of25 sentences, randomly extracted from the BNC.10The sentences were sourced from the BNC to avoidany bias which may have been introduced had thetest sentences been created manually.
We requiredthat each test sentence satisfy the following condi-tions11:1.
The sentence must contain between 6 and 10words (to reflect length of a typical dialogueutterance);2.
The sentence must contain at least one termwhich is found in the OVVTs (otherwise itwould be pointless for evaluation purposes);the term may have any valence.12Our second filtering technique requires informa-tion about the grammatical relations between termsin a sentence (illustrated in Figure 1).
For this, weused a version of the BNC which was pre-processedwith the RASP parser (Briscoe et al, 2006).Our gold standard for candidate acceptability wascreated using the first author?s judgements.13 In or-10The size of our test data set was capped at 25 due to the timerequired to create the gold standard (i.e., judging 1030 substitu-tions consistently).11These constraints reduced our sample set from the ?4.6million sentences in the BNC to approx.
627,000 sentences.12The sentence can theoretically be valence-shifted by sub-stituting that term, regardless of the term?s valence.13With more time we would of course have preferred to usemultiple annotators.
However, the judgement task was simple91der to be judged as an ACCEPT by the annotator,a generated sentence needed to satisfy the followingcriteria (otherwise it was labelled REJECT):1.
Semantic Equivalence: The new sentenceshould convey reasonably equivalent semanticscompared to the original: e.g., phrases such as?young boy?
and ?small boy?
were consideredacceptably close;142.
Grammatical Correctness: The new sentenceshould not contain grammatical errors.
For thegold standard, terms were manually convertedinto their original morphological form beforeannotation (e.g., if the lemma ?speak?
replacedan instance of ?shouted?, then it was convertedto ?spoke?
).4.1 Evaluation MethodologyTo evaluate each candidate selection method, weperformed the following procedure for each of our25 test sentences:1.
Find all matching15 terms and retrieve the va-lence score of each;2.
For each matching term:(a) Retrieve the corresponding list of alterna-tive terms from the OVVTs;(b) Generate several different candidate sen-tences by substituting each alternativeterm into the original sentence;(c) Apply the chosen candidate selectiontechnique to each generated sentence, andlabel each as ACCEPT or REJECT (forstep 3);3.
Compare all system classifications to our goldstandard (automatically), and mark each as ei-ther a true positive (TP), false positive (FP),true negative (TN), or false negative (FN).We then used the TP, FP, TN and FN counts tocompute the accuracy, precision, recall and F-scoreenough for us to believe it to be reliable.14A fairly liberal view of ?semantic equivalence?
was taken;for example, for our purposes we consider all sentences in Table1 to be more-or-less semantically equivalent.15A matching term is defined as a term which has a corre-sponding entry in the OVVTs.across all generated sentences.
These metrics areused to compare the relative performance betweeneach of our candidate selection methods.We describe each of our techniques and the re-sults; we present all the measurements in a singletable (Table 5).164.2 Candidate filtering using bigramsFor each candidate sentence generated, we exam-ined the bigrams including the newly substitutedterm.
If both17 bigrams appear in the BNC, we takethis as an indication that the substitution is accept-able, and we accept the candidate sentence.
Other-wise, the candidate is rejected.
We pre-processed theBNC to extract 8,463,295 unique bigrams, formattedas lemma/pos lemma/pos pairs, where lemmais the lemmatised word, and pos is the WordNetPOS.
As a simple attempt to address word-sense dis-ambiguation, we discriminated on POS18 when ex-tracting and matching these bigrams.
For example,?drive/n home/n?
and ?drive/v home/n?would be considered separate bigrams, as the term?drive?
occurs with different POS in each.
We choseto lemmatise all bigrams due to the relatively smallsize of the BNC.
Also, we did not consider bigramswhich are interrupted by sentence punctuation, asthis indicates a phrase break.We take this bigram approach as our base-line.19 This simple technique has reasonable accu-racy (0.752: see Table 5) but this is due largely to thehigh number of true negatives produced.
The falsenegatives are mainly caused by the BNC?s relativelylimited bigram coverage.To address this issue, we sourced our bigramsfrom the Google Web 1T Corpus, which coversapproximately one trillion words of English textsourced from publicly accessible web pages.
Com-pared with the BNC, it has much greater coverage,containing ?314 million bigrams.
However, Web1T does not contain POS information, and due toits size we did not lemmatise the bigrams.
Using a16Note that had we performed no filtering, all TN?s wouldbecome FP?s and all FN?s would befome TP?s.17For terms beginning/ending a sentence (or phrase sur-rounded by punctuation), we only examine one bigram.18We differentiated only adjectives, nouns, verbs, and ad-verbs; all other POS were considered equivalent for the pur-poses of bigram extraction.19A lower baseline would be to perform no filtering.92smaller corpus, these differences may reduce cov-erage and bigram matching accuracy.
However wehypothesise that using the Web 1T corpus, such lim-itations should be outweighed by its sheer size.From Table 5, we see a substantial increase in re-call over our previous baseline, which supports ourhypothesis that using a larger corpus would increasetrue positives and reduce false negatives.
However,the increased coverage of the Web 1T corpus bringswith it more opportunities for false positives, thenumber of which has increased dramatically fromour baseline, causing a reduction in precision andaccuracy.
Despite this, due to increased recall, weachieved an improvement in overall F-score.Due to its web-based nature, the Web 1T corpuswill contain more errors than a corpus sourced frompublished print, such as the BNC.
Bigrams whichoccur infrequently may be a source of noise.
Wehypothesized that a substitution is acceptable if itsreplacement bigrams occur in some reasonable pro-portion to the original bigrams.
Hence, we experi-mented with bigram frequency ratios, where a can-didate is accepted only if its ratio exceeds a giventhreshold The ratio is calculated as fr/fo, wherefr and fo represent the replacement and originalbigram frequencies, respectively.
We repeated ourWeb 1T bigrams experiment for several ratio thresh-olds between 0 and 0.9, and measured the changes inaccuracy, precision and recall.
Our results showedthat frequency ratio thresholding can reduce falsepositives, leading to slightly increased precision forcertain ratios.
However, true positives are also re-duced, and we sacrifice significant recall for onlyminor gains in precision.4.3 Filter using grammaticalrelationsCandidate selection using bigrams is a somewhatna?
?ve approach, as it considers only the surface textwithout regard for the underlying grammatical rela-tions (GRs) between terms.
To illustrate, considerthe example shown in Table 4.We observed that alternatives for ?lovely?
such as?picturesque?
and ?scenic?
were falsely rejected us-ing BNC bigrams.20 As bigrams, ?picturesque fam-ily?
and ?scenic family?
seem like unnatural ways20These candidates were accepted using the Web 1T corpus.Context on their lovely family holidaysTerm lovelyAlt.s handsome, picturesque, pretty,splendid, scenic, resplendent, ...Table 4: Sample context & replacements for ?lovely?of describing a family.
However, in this context?lovely?
modifies ?holiday?, not ?family?
: this dis-tinction is not picked up using simple bigrams.
Toaddress this limitation, we extended our bigram can-didate selection technique to consider grammaticalrelations (GRs).Our GR technique uses an input sentence inRASP format.
We only change one term per sen-tence as before; however we first extract the term?sGRs from the RASP annotation.
We convert eachbinary21 GR into a GR-bigram using the original or-dering of terms in the sentence.
Figure 1 illustratesthe GRs for our example sentence, and how suchtranslate into GR-bigrams.
?On     their     lovely     family     holidays?ncmodncmoddetmod/possGR-bigrams extracted for ?lovely?:1.
?lovely holidays?Figure 1: Grammatical relations and GR-bigramsBy converting GRs into bigrams, we can take ad-vantage of Web 1T?s extensive coverage.
However,due to our restrictions on GR types, it is possible toobtain zero GR-bigrams for some words in a sen-tence.
This happens when the word has no modifieror comparative relations associated with it.
For thesewords, we revert to our bigram selection technique.Our results for candidate selection using GRs areagain shown in Table 5.
Surprisingly, this techniqueperforms worse than using regular bigrams for allmetrics when compared to our baseline.
We suspectour GR selection technique performs no better than21We only examine binary comparative and modifier GRtypes, as RASP provides many other syntactic relations whichwe deemed not relevant to our task.93Web 1T bigrams simply due to the corpus?
extensivecoverage, which leads to a similar amount of falsepositives.SelectionTechniqueBNCBigramsWeb 1TBigramsWeb 1TGRsTrue positives 22 55 150% 54 145%False positives 45 155 244% 169 276%True negatives 288 178 -38% 164 -43%False negatives 57 24 -58% 25 -56%Accuracy 0.752 0.566 -25% 0.529 -30%Precision 0.328 0.262 -20% 0.242 -26%Recall 0.278 0.696 150% 0.684 145%F-score 0.301 0.381 26% 0.358 19%Table 5: Collated results for all experiments4.4 Error AnalysisTo explain our experimental results, we first look athow the performance changes between our differentversions relative to the baseline (i.e., BNC Bigrams):see Table 5.
Note first that, while all methods in-creased the number of true positives and decreasedfalse negatives, any performance gains were simplydrowned out by the massive increases in false posi-tives that occurred: this is the main cause of our lowprecision and recall.
For the following discussion,we focus on the use of Web IT bigrams, which wasthe best performing filtering technique.Since false positives are the most importantsource of error to avoid in an ECA, we focus onthese.
We examined the false positive instancesand categorised each error into the following fourgroups.
The distribution of errors into these cate-gories is shown in Table 6.Category No.
FP % of all FPChange in Meaning 76 49.03%Incorrect WSD 42 27.10%Phrase/Metaphor 31 20.00%Grammatical 6 3.87%Total 155 100%Table 6: Distribution of classification errors4.4.1 Change in meaningA major limitation of the OVVT resource is thatseveral of the alternative terms simply cause toomuch semantic change even when the correct senseof the original term is detected.
For example, somealternatives for ?winner?
are words such as ?sleeper?,?upsetter?, and ?walloper?.
In the context of thephrase ?Cash prizes will be offered to the winners?,we will almost always prefer the generic ?winner?.We suspect this limitation arises due to the meth-ods used to construct the OVVTs; in particular theuse of the WordNet hyponym and hypernym re-lations.
For example, the ?thing?
category in Word-Net encompasses a multitude of more specific terms,such as ?ornament?, ?structure?, ?surface?, and ?in-stallation?.
These terms all made their way into theOVVT for ?thing?, yet they are rarely appropriatesubstitutions for ?thing?.
Conversely, we may notwish to replace any specific terms with the moregeneric ?thing?
as this removes too much meaning.As this kind of error accounted for almost halfof our false positives, addressing this limitationmay lead to significant gains in performance.
Thislikely requires a more conservative approach to con-structing the OVVTs themselves, e.g., by incorpo-rating corpus-based information, as per (Guerini etal., 2008)?s approach to constructing the Modifier-OVVTs): the technique for mining appropriate verb-adverb pairings from the BNC could be generalisedto include other POS types.Related to the problem of semantic change is theidea of context-dependent semantics.
For example,certain qualifiers have opposing effects dependingon the appraisal of the subject: consider a ?longterm illness?
compared to a ?long term vacation?.One possible solution to this problem is to modifythe way valences are calculated to take into accountwhich terms modify one another.4.4.2 Incorrect word-sense disambiguationThe WSD approach used in our work adaptedfrom (Guerini et al, 2008) is only a crude approx-imation to a complex problem; the WSD-relatedproblems could at least be alleviated by incorpo-rating a more sophisticated WSD approach into thepipeline.
However, even if we could determine thecorrect sense of each word, we are still left with thelimitation that the OVVTs are not exhaustive in theircoverage, with several word senses missing.944.4.3 Phrases and metaphorsSeveral false positives were caused by phrasessuch as ?long term?.
Metaphors were a similarcause for error, e.g.
?stepping stone?.
Phrase andmetaphor detection should improve our technique?sperformance, especially since the OVVTs containseveral phrases; however, these are known difficultchallenges in themselves.4.4.4 Grammatical errorsA grammatical error occurs when the alternativeterm is acceptable semantically, yet further syntacticmodification to the sentence is needed to preservecorrect grammar: see Table 3.An extension of our bigram approach could be touse a larger window around replaced words to assessthe suitability of a substitution.
Recent work hasshown this technique could be used to rank poten-tial substitutions in order of acceptability (Hawker,2007) and is worth considering as future work.4.4.5 Limitations of bigrams and corpuscoverageIn some cases, our bigram selection technique isineffective when the term being changed is flankedby stop words.
In a corpus of sufficient size and cov-erage, the majority of terms will occur next to stopwords far more often than they occur next to other,less common terms.
Hence, bigrams containing stopwords were a common source of false positives.This limitation could be addressed in future workby extending our grammatical relation technique toinclude ternary GRs, which provide relations fornoun-verb phrases such as ?solution to fitness?
and?solution to health?.
Given these, we could acceptor reject based on the presence of the accompany-ing trigrams in the Web 1T corpus.
As described in(Hawker, 2007), use of an even larger window, suchas 4-grams and 5-grams around replaced terms mayalso address this issue, however the size of the Web1T corpus for larger N-grams presents serious pro-cessing challenges.225 Evaluation: Sentiment ShiftThe technqiues described above attempt to create ac-ceptable candidates to shift sentiment.
However, this22(Hassan et al, 2007) describes a successful approach to lex-ical substitution that combines multiple knowledge sources.leaves open the question as to whether the techniquehas its desired effect: i.e.
appropriately shifting sen-timent.
We designed an experiment which aims tomeasure correlation between human judgements ofthe sentiment shift in our generated candidates, andour system?s representation of sentiment shift.We presented subjects with an original sentence,along with one of the generated candidates.
Oursix subjects had no specialised knowledge of thetask and were all native English speakers.
Sub-jects were asked to judge the modified sentence forchange in sentiment relative to the original accord-ing to the five shift categories described earlier (i.e.,major/minor positive/negative/no shift).
In orderto avoid bias and to clarify the task, we explainedthat sentiment should be separated from changes inmeaning, or the reader?s opinions about the sen-tences.
Instead, we urged subjects to ask themselvesthe question: ?Is the author of the second sentencesaying what they?re saying in a more positive ormore negative way, compared to the first sentence?
?The sentences used were extracted from the BNCat random, using the restrictions listed above.
Weextracted 250 sentences to be used as the originals,each of which was used as input to our sentimentshifting system.
For each original sentence, we pro-duced all possible candidates using our best per-forming candidate selection method, Web 1T Bi-grams.
We also limited our generation to changingone term per sentence, as to not produce a combi-natorial explosion in the number of candidates gen-erated.
This produced approximately 3000 modi-fied candidates, including several candidates with nosentiment shift.Upon inspection, we found many generated can-didates contained the types of errors describedabove.
Hence, we manually extracted original andmodified sentences until we had a total of 50 origi-nals, and 100 shifted sentences.
In selecting whichsentences to keep, we chose ones which soundedthe most natural, or had the least amount of seman-tic change from the original.
Manual selection wasperformed in order to prevent introducing any biasinto judgements when a subject is confronted witha grammatically incorrect or unnatural sentence.
Wealso aimed for a fairly even distribution of the shifted95sentences into the five sentiment shift intervals.235.1 Results and analysisWe performed a pairwise Kendall?s Tau rank cor-relation (Kendall and Gibbons, 1962), which com-pares each human?s judgements with the system?ssentiment shift, for all 100 generated sentences.Kendall?s Tau measures the correlation between twodistributions on a scale of -1 to 1, with 1 indicatingtotal agreement; -1 indicating total disagreement;and 0 indicating no (or random) correlation.We measured the correlation using the five senti-ment shift intervals, and also using judgement po-larities, i.e.
whether a score is positive, nega-tive or zero.
We only report on polarity results asthe finer-grained comparison showed similar resultswith slightly less correlation.Our results are shown in Table 7; Kendall?s Taucorrelations are shown above the shaded diagonal,while the corresponding p-values for statistical sig-nificance are shown below the diagonal.Kendall's Tau Correlationsys h1 h2 h3 h4 h5 h6p-valuesys  0.075 0.024 -0.099 0.034 0.022 -0.078h1 0.413  0.276 0.423 0.417 0.339 0.249h2 0.790 0.002  0.406 0.348 0.361 0.198h3 0.273 0.000 0.000  0.418 0.300 0.343h4 0.708 0.000 0.000 0.000  0.325 0.277h5 0.810 0.000 0.000 0.001 0.000  0.189h6 0.393 0.006 0.029 0.000 0.002 0.040Table 7: Kendall?s Tau rank correlation between system(sys) and human (hi) judgement polaritiesAlthough the correlation observed between inter-annotator judgements of polarity was fairly low, itis statistically significant in all cases using a confi-dence level of p < 0.05.
While this indicates therewas some agreement between human annotators, therelatively low correlation indicates that judging sen-timent is a fairly subjective task.
However, we sawno correlation between the human judgements andour system?s representation of sentiment shift.23Note: the judgement of which sentiment-shift category asentence-pair fell into was made by the system (and subjects);the manual intervention in the experiment design was to removeunacceptable sentence-pairs.The poor correlation between human and systempolarities can possibly be attributed to a numberof reasons.
(Guerini et al, 2008) mention that inSentiWordNet, several of the WordNet synsets arevalenced incorrectly, with many having a valence ofzero, which we also observed in the OVVT resource.Our survey results suggest that SentiWordNet in itscurrent form is not ideally suited to the task of gen-erating sentiment in text using the Valentino method.SentiWordNet may be effective when classifyingthe sentiment of large texts; the valence scores canbe considered to reflect the degree to which eachword represents a sentiment ?feature?.
However, itis somewhat unrealistic to assume that every termwill have the same effect on sentiment in all con-texts; assigning words a ?universal?
sentiment scoreseems non-intuitive, and a finer-grained representa-tion of sentiment is needed for short texts such asdialogue utterances.In sentiment generation, when choosing a re-placement term from a set of alternatives, we aremore interested in each candidate?s effect on senti-ment, relative to the other candidates.
While a re-source of semantically clustered terms is needed forthis task (such as the OVVTs), terms within eachcluster need to be ranked for sentiment in a localisedway, taking account of positivity or negativity rela-tive to other terms in the cluster.
Upon inspectionof several OVVTs, this ranking is a straightforwardtask for a human to perform (if time-consuming).However, the context of a substitution often de-termines its effects of sentiment.
Hence, we ar-gue that future work in sentiment generation usingknowledge-based techniques should extend existingresources to encompass ranking of candidates in acontextual way, rather than ranking them staticallyout of context.
For example, an MRE-style (Traumet al, 2003) approach could be used which goes be-yond scoring the overall sentiment of an utterance,but considers how sentiment (or attitude) is directedtowards agents, objects and events.Acknowledgements: We thankMarco Guerini for kindlyproviding us with the OVVTs resource, Tim Baldwin forhelpful suggestions for the evaluation in Section 5, andthe referees for valuable feedback.
Cavedon?s contribu-tion was partially supported by the Australian ResearchCouncil under Linkage Grant LP0882013.96ReferencesJoseph Bates.
1994.
The Role of Emotion in BelievableAgents.
Communications of the ACM, 37(7):122?125.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The Second Release of the RASP System.
In Proceed-ings of ACL, pages 77?80, Sydney.G.
Caridakis, A. Raouzaiou, E. Bevacqua, M. Mancini,K.
Karpouzis, L. Malatesta, and C. Pelachaud.
2007.Virtual Agent Multimodal Mimicry of Humans.
Lan-guage Resources and Evaluation, 41(3):367?388.Michael Fleischman and Eduard Hovy.
2002.
TowardsEmotional Variation in Speech-Based Natural Lan-guage Generation.
In Proceedings of the Second In-ternational Natural Language Generation Conference,pages 57?64, New York.Marco Guerini, Carlo Strapparava, and Oliviero Stock.2008.
Valentino: A Tool for Valence Shifting of Natu-ral Language Texts.
In Proceedings of the 6th Interna-tional Conference on Language Resources and Evalu-ation, Marrakech.Samer Hassan, Andras Csomai, Carmen Banea, RaviSinha, and Rada Mihalcea.
2007.
Unt: Subfinder:combining knowledge sources for automatic lexicalsubstitution.
In Proc.
Fourth Int.
Workshop on Se-mantic Evaluations (SemEval 2007), pages 410?413,Prague.Tobias Hawker.
2007.
USYD: WSD and Lexical Sub-stitution Using the Web1T Corpus.
In Proc.
4th Int.Workshop on Semantic Evaluations (SemEval 2007),pages 446?453, Prague.M.G.
Kendall and J.D.
Gibbons.
1962.
Rank CorrelationMethods.
Griffin London.Diana McCarthy and Roberto Navigli.
2007.
SemEval-2007 task 10: English lexical substitution task.
InProc.
Fourth Int.
Workshop on Semantic Evaluations(SemEval 2007), pages 48?53, Prague.Catherine Pelachaud and Massimo Bilvi.
2003.
Compu-tational Model of Believable Conversational Agents.In Communication in Multiagent Systems, volume2650 of Lecture Notes in Computer Science, pages300?317.
Springer.David Traum, Michael Fleischman, and Eduard Hovy.2003.
NL Generation for Virtual Humans in a Com-plex Social Environment.
In In Proceedings of theAAAI Spring Symposium on Natural Language Gener-ation in Spoken and Written Dialogue, pages 151?158,Palo Alto.Ielka van der Sluis and Chris Mellish.
2008.
TowardsAffective Natural Language Generation: Empirical In-vestigations.
In Proceedings of the Symposium on Af-fective Language in Human and Machine, AISB, pages9?16, Aberdeen.E.
Zovato, F. Tini Brunozzi, and M. Danieli.
2008.
Inter-play between pragmatic and acoustic level to embodyexpressive cues in a Text to Speech system.
In Pro-ceedings of the Symposium on Affective Language inHuman and Machine, AISB, pages 88?91, Aberdeen.97
