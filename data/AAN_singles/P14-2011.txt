Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 61?67,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsA Feature-Enriched Tree Kernel for Relation ExtractionLe Sun      and      Xianpei HanState Key Laboratory of Computer ScienceInstitute of Software, Chinese Academy of SciencesHaiDian District, Beijing, China.
{sunle, xianpei}@nfs.iscas.ac.cnAbstractTree kernel is an effective technique for rela-tion extraction.
However, the traditional syn-tactic tree representation is often too coarse orambiguous to accurately capture the semanticrelation information between two entities.
Inthis paper, we propose a new tree kernel,called feature-enriched tree kernel (FTK),which can enhance the traditional tree kernelby: 1) refining the syntactic tree representationby annotating each tree node with a set of dis-criminant features; and 2) proposing a newtree kernel which can better measure the syn-tactic tree similarity by taking all features intoconsideration.
Experimental results show thatour method can achieve a 5.4% F-measure im-provement over the traditional convolutiontree kernel.1 IntroductionRelation Extraction (RE) aims to identify a set ofpredefined relations between pairs of entities intext.
In recent years, relation extraction has re-ceived considerable research attention.
An effec-tive technique is the tree kernel (Zelenko et al,2003; Zhou et al, 2007; Zhang et al, 2006; Qianet al, 2008), which can exploit syntactic parse treeinformation for relation extraction.
Given a pair ofentities in a sentence, the tree kernel-based REmethod first represents the relation informationbetween them using a proper sub-tree (e.g., SPT ?the sub-tree enclosed by the shortest path linkingthe two involved entities).
For example, the threesyntactic tree representations in Figure 1.
Then thesimilarity between two trees are computed using atree kernel, e.g., the convolution tree kernel pro-posed by Collins and Duffy (2001).
Finally, newrelation instances are extracted using kernel basedclassifiers, e.g., the SVM classifier.Unfortunately, one main shortcoming of thetraditional tree kernel is that the syntactic tree rep-resentation usually cannot accurately capture theFigure 1.
The ambiguity of possessive structurerelation information between two entities.
This ismainly due to the following two reasons:1) The syntactic tree focuses on representingsyntactic relation/structure, which is often toocoarse or ambiguous to capture the semantic re-lation information.
In a syntactic tree, each nodeindicates a clause/phrase/word and is only labeledwith a Treebank tag (Marcus et al, 1993).
TheTreebank tag, unfortunately, is usually too coarseor too general to capture semantic information.For example, all the three trees in Figure 1 sharethe same possessive syntactic structure, but ex-press quite different semantic relations: where?Mary?s brothers?
expresses PER-SOC Familyrelation, ?Mary?s toys?
expresses Possession rela-tion, and ?New York?s airports?
expresses PHYS-Located relation.2) Some critical information may lost duringsub-tree representation extraction.
For example,in Figure 2, when extracting SPT representation,all nodes outside the shortest-path will be pruned,such as the nodes [NN plants] and [POS ?s] in treeT1.
In this pruning process, the critical infor-mation ?word town is the possessor of the posses-sive phrase the town?s plants?
will be lost, whichin turn will lead to the misclassification of theDISC relation between one and town.This paper proposes a new tree kernel, referredas feature-enriched tree kernel (FTK), which caneffectively resolve the above problems by enhanc-ing the traditional tree kernel in following ways:1) We refine the syntactic tree representa-tion by annotating each tree node with a set of dis-criminant features.
These features are utilized toNPNP NNNN POSMary 'sbrothers(a) (b) (c)NPNP NNNN POSMary 'stoysNPNP NNNN POSNY 'sairports61better capture the semantic relation informationbetween two entities.
For example, in order to dif-ferentiate the syntactic tree representations in Fig-ure 1, FTK will annotate them with several fea-tures indicating ?brother is a male sibling?, ?toyis an artifact?, ?New York is a city?, ?airport isfacility?, etc.2) Based on the refined syntactic tree repre-sentation, we propose a new tree kernel ?
feature-enriched tree kernel, which can better measure thesimilarity between two trees by also taking all fea-tures into consideration.Figure 2.
SPT representation extractionWe have experimented our method on the ACE2004 RDC corpus.
Experimental results show thatour method can achieve a 5.4% F-measure im-provement over the traditional convolution treekernel based method.This paper is organized as follows.
Section 2describes the feature-enriched tree kernel.
Section3 presents the features we used.
Section 4 dis-cusses the experiments.
Section 5 briefly reviewsthe related work.
Finally Section 6 concludes thispaper.2 The Feature-Enriched Tree KernelIn this section, we describe the proposed feature-enriched tree kernel (FTK) for relation extraction.2.1 Refining Syntactic Tree RepresentationAs described in above, syntactic tree is often toocoarse or too ambiguous to represent the semanticrelation information between two entities.
To re-solve this problem, we refine the syntactic treerepresentation by annotating each tree node witha set of discriminant features.Figure 3.
Syntactic tree enriched with featuresSpecifically, for each node  in a syntactic tree, we represent it as a tuple:where  is its phrase label (i.e., its Treebank tag),and  is a feature vector which indicates thecharacteristics of node , which is represented as:where fi is a feature and is associated with a weight.
The feature we used includes charac-teristics of relation instance, phrase properties andcontext information (See Section 3 for details).For demonstration, Figure 3 shows the feature-enriched version of tree T2 and tree T4 in Figure2.
We can see that, although T2 and T4 share thesame syntactic structure, the annotated featurescan still differentiate them.
For example, the NP5node in tree T2 and the NP5 node in tree T4 aredifferentiated using their features Possessive-Phrase and PPPhrase, which indicate that NP5 inT2 is a possessive phrase, meanwhile NP5 in T4 isa preposition phrase.2.2 Feature-Enriched Tree KernelThis section describes how to take into accountthe annotated features for a better tree similarity.In Collins and Duffy?s convolution tree kernel(CTK), the similarity between two trees T1 and T2is the number of their common sub-trees:Using this formula, CTK only considers whethertwo enumerated sub-trees have the identical syn-tactic structure (the indicator  is 1 if theNPNP PPCDoneINE1ofNPNPNNtownE2DTthePOS'sNNplantsNPNP PPCDoneINE1ofNPNPNNtownE2DTthePruneNPNP PPCDoneINE1ofNPNPDT NNthe teamsE2PPINin USANPNNPruneNPNP PPCDoneINE1ofNPNPNNteamsE2DTthe(T1)(T2)(T3) (T4)NPNP PPCDoneINE1ofNPNPNNtownE2DTthe(T2)PossessivePhrase, RootPath:NP-PP,Contain_Arg2_GPE, ...Possessor, Contain_Arg2_GPE,RootPath:NP-PP-NP,EndWithPOS, ...EntType:GPE, MentionType:NOM,RootPath:NP-PP-NP-NP, ...WN:town, WN:district, WN:region,WN:location, Match_Arg2_GPE ...PPPhrase, RootPath:NP-PP,Contain_Arg2_ORG, ...PP_Head, RootPath:NP-PP-NP,Contain_Arg2_ORG,...EntType:ORG, MentionType:NOM,RootPath:NP-PP-NP-NP, ...WN:team, WN:social_unit,WN:group, WN:organization,Match_Arg2_ORG ...Feature Vector12 3456 7 89 10 1112141315NPNP PPCDoneINE1ofNPNPNNteamE2DTthe(T4) 12 3456 7 89 10121411131562two sub-trees  and  have the identical syntac-tic structure and 0 otherwise).
Such an assumptionmakes CTK can only capture the syntactic struc-ture similarity between two trees, while ignoringother useful information.To resolve the above problem, the feature-en-riched tree kernel (FTK) compute the similaritybetween two trees as the sum of the similaritiesbetween their common sub-trees:where  is the similarity between enumer-ated sub-trees  and , which is computed as:where  is the same indicator function as inCTK; is a pair of aligned nodes betweenand , where  and  are correspondingly inthe same position of tree  and ;  is theset of all aligned node pairs;  is thefeature vector similarity between node  and ,computed as the dot product between their featurevectors  and .Notice that, if all nodes are not annotated withfeatures,  will be equal to .
In thisperspective, we can view  as a similarityadjusted version of , i.e.,  onlyconsiders whether two nodes are equal, in contrastfurther considers the feature similaritybetween two nodes.The Computation of FTK.
As the same asCTK, FTK can be efficiently computed as:where  is the set of nodes in tree , andevaluates the sum of the similarities ofcommon sub-trees rooted at node  and node ,which is recursively computed as follows:1) If the production rules of  and  are differ-ent,  = 0;2) If both  and  is pre-terminal nodes,;Otherwise go to step 3;3) Calculate  recursively as:?
(n1;n2) = ??
(1 + sim(n1; n2))?#ch(n1)Xk=1(1 + ?
(ch(n1; k); ch(n2; k))3 Features for Relation ExtractionThis section presents the features we used to en-rich the syntactic tree representation.3.1 Instance FeatureRelation instances of the same type often sharesome common characteristics.
In this paper, weadd the following instance features to the rootnode of a sub-tree representation:1) Syntactico-Semantic structure.
A fea-ture indicates whether a relation instance has thefollowing four syntactico-semantic structures in(Chan & Roth, 2011) ?
Premodifiers, Possessive,Preposition, Formulaic and Verbal.2) Entity-related information of argu-ments.
Features about the entity information ofarguments, including: a) #TP1-#TP2: the concatof the major entity types of arguments; b) #ST1-#ST2: the concat of the sub entity types of argu-ments; c) #MT1-#MT2: the concat of the mentiontypes of arguments.3) Base phrase chunking features.
Fea-tures about the phrase path between two argu-ments and the phrases?
head before and after thearguments, which are the same as the phrasechunking features in (Zhou, et al, 2005).3.2 Phrase FeatureAs discussed in above, the Treebank tag is toocoarse to capture the property of a phrase node.Therefore, we enrich each phrase node with fea-tures about its lexical pattern, its content infor-mation, and its lexical semantics:1) Lexical Pattern.
We capture the lexicalpattern of a phrase node using the following fea-tures: a) LP_Poss: A feature indicates the node isa possessive phrase; b) LP_PP: A feature indi-cates the node is a preposition phrase; c) LP_CC:A feature indicates the node is a conjunctionphrase; d) LP_EndWithPUNC: A feature indicatesthe node ends with a punctuation; e) LP_EndWith-POSS: A feature indicates the node ends with apossessive word.2) Content Information.
We capture theproperty of a node?s content using the followingfeatures: a) MB_#Num: The number of mentionscontained in the phrase; b) MB_C_#Type: A fea-ture indicates that the phrase contains a mentionwith major entity type #Type; c) MW_#Num: Thenumber of words within the phrase.3) Lexical Semantics.
If the node is a pre-terminal node, we capture its lexical semantic byadding features indicating its WordNet sense in-formation.
Specifically, the first WordNet senseof the terminal word, and all this sense?s hyponymsenses will be added as features.
For example,WordNet senses {New York#1, city#1, district#1,63region#1, ?}
will be added as features to the [NNNew York]  node in Figure 1.3.3 Context Information FeatureThe context information of a phrase node is criti-cal for identifying the role and the importance ofa sub-tree in the whole relation instance.
This pa-per captures the following context information:1) Contextual path from sub-tree root tothe phrase node.
As shown in Zhou et al (2007),the context path from root to the phrase node is aneffective context information feature.
In this paper,we use the same settings in (Zhou et al, 2007), i.e.,each phrase node is enriched with its context pathsof length 1, 2, 3.2) Relative position with arguments.
Weobserved that a phrase?s relative position with therelation?s arguments is useful for identifying therole of the phrase node in the whole relation in-stance.
To capture the relative position infor-mation, we define five possible relative positionsbetween a phrase node and an argument, corre-sponding match, cover, within, overlap and other.Using these five relative positions, we capture thecontext information using the following features:a) #RP_Arg1Head_#Arg1Type: a feature in-dicates the relative position of a phrase node withargument 1?s head phrase, where #RP is the rela-tive position (one of match, cover, within, overlap,other), and #Arg1Type is the major entity type ofargument 1.
One example feature may beMatch_Arg1Head_LOC.b) #RP_Arg2Head_#Arg2Type: The relativeposition with argument 2?s head phrase;c) #RP_Arg1Extend_#Arg1Type: The rela-tive position with argument 1?s extended phrase;d) #PR_Arg2Extend_#Arg2Type: The rela-tive position with argument 2?s extended phrase.Feature weighting.
Currently, we set al fea-tures with an uniform weight , which isused to control the relative importance of the fea-ture in the final tree similarity: the larger the fea-ture weight, the more important the feature in thefinal tree similarity.4 Experiments4.1 Experimental SettingTo assess the feature-enriched tree kernel, weevaluate our method on the ACE RDC 2004 cor-pus using the same experimental settings as (Qianet al, 2008).
That is, we parse all sentences usingthe Charniak?s parser (Charniak, 2001), relationinstances are generated by iterating over all pairsof entity mentions occurring in the same sentence.In our experiments, we implement the feature-en-riched tree kernel by extending the SVMlight (Joa-chims, 1998) with the proposed tree kernel func-tion (Moschitti, 2004).
We apply the one vs. oth-ers strategy for multiple classification using SVM.For SVM training, the parameter C is set to 2.4 forall experiments, and the tree kernel parameter ?
istuned to 0.2 for FTK and 0.4 (the optimal param-eter setting used in Qian et al(2008)) for CTK.4.2 Experimental Results4.2.1 Overall performanceWe compare our method with the standard convo-lution tree kernel (CTK) on the state-of-the-artcontext sensitive shortest path-enclosed tree rep-resentation (CSPT, Zhou et al, 2007).
We exper-iment our method with four different feature set-tings, correspondingly: 1) FTK with only instancefeatures ?
FTK(instance); 2) FTK with onlyphrase features ?
FTK(phrase); 3) FTK with onlycontext information features ?
FTK(context); and4) FTK with all features ?
FTK.
The overall per-formance of CTK and FTK is shown in Table 1,the F-measure improvements over CTK are alsoshown inside the parentheses.
The detailed perfor-mance of FTK on the 7 major relation types ofACE 2004 is shown in Table 2.P(%) R(%) FCTK 77.1 61.3 68.3 (-------)FTK(instance) 78.5 64.6 70.9 (+2.6%)FTK(phrase) 78.3 64.2 70.5 (+2.2%)FTK(context) 80.1 67.5 73.2 (+4.9%)FTK 81.2 67.4 73.7 (+5.4%)Table 1.
Overall PerformanceRelation Type P(%) R(%) F ImprEMP-ORG 84.7 82.4 83.5 5.8%PER-SOC 79.9 70.7 75.0 1.0%PHYS 73.3 64.4 68.6 7.0%ART 83.6 57.5 68.2 1.7%GPE-AFF 74.7 56.6 64.4 4.3%DISC 81.6 48.0 60.5 6.6%OTHER-AFF 74.2 36.8 49.2 1.0%Table 2.
FTK on the 7 major relation types andtheir F-measure improvement over CTKFrom Table 1 and 2, we can see that:1) By refining the syntactic tree with discri-minant features and incorporating these featuresinto the final tree similarity, FTK can significantlyimprove the relation extraction performance:compared with the convolution tree kernel base-line CTK, our method can achieve a 5.4% F-meas-ure improvement.642) All types of features can improve the per-formance of relation extraction: FTK can corre-spondingly get 2.6%, 2.2% and 4.9% F-measureimprovements using instance features, phrase fea-tures and context information features.3) Within the three types of features, contextinformation feature can achieve the highest F-measure improvement.
We believe this may be-cause: ?
The context information is useful inproviding clues for identifying the role and the im-portance of a sub-tree; and ?
The context-free as-sumption of CTK is too strong, some critical in-formation will lost in the CTK computation.4) The performance improvement of FTKvaries significantly on different relation types: inTable 2, most performance improvement gainsfrom the EMP-ORG, PHYS, GPE-AFF and DISCrelation types.
We believe this may because thediscriminant features will better complement thesyntactic tree for capturing EMP-ORG, PHYS,GPE-AFF and DISC relation.
On contrast the fea-tures may be redundant to the syntactic infor-mation for other relation types.System P(%) R(%) FQian et al, (2008): composite kernel 83.0 72.0 77.1Zhou et al, (2007): composite kernel 82.2 70.2 75.8Ours: FTK with CSPT 81.2 67.4 73.7Zhou et al, (2007): context sensitiveCTK with CSPT81.1 66.7 73.2Ours: FTK with SPT 81.1 66.2 72.9Jiang & Zhai (2007): MaxEnt classi-fier with features74.6 71.3 72.9Zhang et al, (2006): composite kernel  76.1 68.4 72.1Zhao & Grishman, (2005): Compositekernel69.2 70.5 70.4Zhang et al, (2006): CTK with SPT 74.1 62.4 67.7Table 3.
Comparison of different systems on theACE RDC 2004 corpus4.2.2 Comparison with other systemsFinally, Table 3 compares the performance of ourmethod with several other systems.
From Table 3,we can see that FTK can achieve competitive per-formance: ?
It achieves a 0.8% F-measure im-provement over the feature-based system of Jiang& Zhai (2007); ?
It achieves a 0.5% F-measureimprovement over a state-of-the-art tree kernel:context sensitive CTK with CSPT of Zhou et al,(2007); ?
The F-measure of our system is slightlylower than the current best performance on ACE2004 (Qian et al, 2008) ?
73.7 vs. 77.1, we believethis is because the system of (Qian et al, 2008)adopts two extra techniques: composing tree ker-nel with a state-of-the-art feature-based kernel andusing a more proper sub-tree representation.
Webelieve these two techniques can also be used tofurther improve the performance of our system.5 Related WorkThis section briefly reviews the related work.
Aclassical technique for relation extraction is tomodel the task as a feature-based classificationproblem (Kambhatla, 2004; Zhou et al, 2005;Jiang & Zhai, 2007; Chan & Roth, 2010; Chan &Roth, 2011), and feature engineering is obviouslythe key for performance improvement.
As an al-ternative, tree kernel-based method implicitly de-fines features by directly measuring the similaritybetween two structures (Bunescu and Mooney,2005; Bunescu and Mooney, 2006; Zelenko et al2003; Culotta and Sorensen, 2004; Zhang et al,2006).
Composite kernels were also be used (Zhaoand Grishman, 2005; Zhang et al, 2006).The main drawback of the current tree kernel isthat the syntactic tree representation often cannotaccurately capture the relation information.
To re-solve this problem, Zhou et al (2007) took the an-cestral information of sub-trees into consideration;Reichartz and Korte (2010) incorporated depend-ency type information into a tree kernel; Plank andMoschitti (2013) and Liu et al (2013) embeddedsemantic information into tree kernel.
Bloehdornand Moschitti (2007a, 2007b) proposed SyntacticSemantic Tree Kernels (SSTK), which can cap-ture the semantic similarity between leaf nodes.Moschitti (2009) proposed a tree kernel whichspecify a kernel function over any pair of nodesbetween two trees, and it was further extended andapplied in other tasks in (Croce et al, 2011; Croceet al, 2012; Mehdad et al, 2010).6 Conclusions and Future WorkThis paper proposes a feature-enriched tree kernel,which can: 1) refine the syntactic tree representa-tion; and 2) better measure the similarity betweentwo trees.
For future work, we want to develop afeature weighting algorithm which can accuratelymeasure the relevance of a feature to a relation in-stance for better RE performance.AcknowledgmentsThis work is supported by the National NaturalScience Foundation of China under Grants no.61100152 and 61272324, and the Open Project ofBeijing Key Laboratory of Internet Culture andDigital Dissemination Research under Grants no.ICDD201204.65ReferencesAgichtein, E. and Gravano, L. 2000.
Snowball: Ex-tracting relations from large plain-text collections.In: Proceedings of the 5th ACM Conference on Dig-ital Libraries, pp.
85?94.Plank, B. and Moschitti, A.
2013.
Embedding SemanticSimilarity in Tree Kernels for Domain Adaptation ofRelation Extraction?.
In: Proceedings of ACL 2013.Banko, M., Cafarella, M. J., Soderland, S., Broadhead,M.
and Etzioni, O.
2007.
Open information extrac-tion from the Web.
In: Proceedings of the 20th Inter-national Joint Conference on Artificial Intelligence,pp.
2670?2676.Bunescu, R. and Mooney, R. 2005.
A shortest path de-pendency kernel for relation extraction.
In: Pro-ceedings of the Human Language Technology Con-ference and the Conference on Empirical Methodsin Natural Language Processing, pp.724?731.Bloehdorn, S. and Moschitti, A.
2007a.
Combined Syn-tactic and Semantic Kernels for Text Classification.In: Proceedings of the 29th European Conference onInformation Retrieval (ECIR).Bloehdorn, S. and Moschitti, A.
2007b.
Structure andsemantics for expressive text kernels.
In: Proceedingof ACM 16th Conference on Information andKnowledge Management (CIKM).Bunescu, R. and Mooney.
R., 2006.
Subsequence ker-nels for relation extraction.
In: Advances in NeuralInformation Processing Systems 18, pp.
171?178.Charniak, E., 2001.
Immediate-head parsing for lan-guage models.
In: Proceedings of the 39th AnnualMeeting on Association for Computational Linguis-tics, pp.
124-131.Chan, Y. S. and Roth, D. 2010.
Exploiting backgroundknowledge for relation extraction.
In: Proceedingsof the 23rd International Conference on Computa-tional Linguistics, pp.
152?160.Chan, Y. S. and Roth, D. 2011.
Exploiting syntactico-semantic structures for relation extraction.
In: Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics, pp.
551?560.Croce, D., Moschitti, A. and Basili, R. 2011.
Struc-tured lexical similarity via convolution kernels ondependency trees.
In: Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing, pp.
1034?1046.Croce, D., Moschitti, A., Basili, R. and Palmer, M.2012.
Verb Classification using Distributional Sim-ilarity in Syntactic and Semantic Structures.
In: Pro-ceedings of ACL 2012, pp.
263-272.Culotta, A. and Sorensen, J.
2004.
Dependency treekernels for relation extraction.
In: Proceedings ofthe 42nd Annual Meeting of the Association forComputational Linguistics, pp.
423?429.Grishman, R. and Sundheim, B.
1996.
Message under-standing conference-6: A brief history.
In: Proceed-ings of the 16th International Conference on Com-putational Linguistics, pp.
466?471.Collins, M. and Duffy, N., 2001.
Convolution Kernelsfor Natural Language.
In: Proceedings of NIPS2001.Liu, D., et al 2013.
Incorporating lexical semanticsimilarity to tree kernel-based Chinese relation ex-traction.
In: Proceedings of Chinese Lexical Seman-tics 2013.Jiang, J. and Zhai, C. 2007.
A systematic exploration ofthe feature space for relation extraction.
In: Pro-ceedings of the Human Language Technology Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics, pp.
113?120.Joachims, T.  1998.
Text  Categorization  with  Support Vector  Machine:  learning  with  many  rele-vant  features.
ECML-1998: 137-142.Kambhatla, N. 2004.
Combining lexical, syntactic, andsemantic features with maximum entropy models forextracting relations.
In: the Proceedings of 42st An-nual Meeting of the Association for ComputationalLinguistics, pp.
178?181.Krause, S., Li, H., Uszkoreit, H., & Xu, F. 2012.
Large-scale learning of relation-extraction rules with dis-tant supervision from the web.
In: Proceedings ofISWC 2012, pp.
263-278.Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B.1993.
Building a large annotated corpus of English:The Penn Treebank.
Computational linguistics,19(2), 313-330.Moschitti, A.
2004.
A study on Convolution Kernels forShallow Semantic Parsing.
In: Proceedings of the42-th Conference on Association for ComputationalLinguistic (ACL-2004).Moschitti, A.
2009.
Syntactic and semantic kernels forshort text pair categorization.
In: Proceedings of the12th Conference of the European Chapter of theACL (EACL 2009), pp.
576?584.Mehdad, Y., Moschitti, A. and Zanzotto, F. 2010.
Syn-tactic/Semantic Structures for Textual EntailmentRecognition.
In: Proceedings of Human LanguageTechnology - North American chapter of the Asso-ciation for Computational Linguistics.Mintz, M., Bills, S., Snow, R. and Jurafsky D. 2009.Distant supervision for relation extraction withoutlabeled data.
In: Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the Associationfor Computational Linguistics and the 4th Interna-tional Joint Conference on Natural Language Pro-cessing of the AFNLP, pp.
1003?1011.66Qian L., Zhou G., Kong F., Zhu Q., and Qian P., 2008.Exploiting constituent dependencies for tree kernelbased semantic relation extraction.
In: Proceedingsof the 22nd International Conference on Computa-tional Linguistics, pp.
697-704.Reichartz, F. and H. Korte, et al  2010.
Semantic rela-tion extraction with kernels over typed dependencytrees.
In: Proceedings of the 16th ACM SIGKDDinternational conference on Knowledge discoveryand data mining.Zelenko, D., Aone, C., and Richardella, A.
2003.
Ker-nel methods for relation extraction.
Journal of Ma-chine Learning Research, 3:1083?1106.Zhang, M., Zhang, J., and Su, J.
2006.
Exploring syn-tactic features for relation extraction using a convo-lution tree kernel.
In: Proceedings of the HumanLanguage Technology Conference and the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 288?295.Zhang, M., Zhang, J., Su, J. and Zhou, G. 2006.
A com-posite kernel to extract relations between entitieswith both flat and structured features.
In Proceed-ings of the 21st International Conference on Com-putational Linguistics and the 44th Annual Meetingof the Association for Computational Linguistics,pages 825?832.Zhao, S. and Grishman, R. 2005.
Extracting relationswith integrated information using kernel methods.In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics, pages419?426.Zhou, G., Su, J., Zhang, J., and Zhang, M. 2005.
Ex-ploring various knowledge in relation extraction.
InProceedings of the 43rd Annual Meeting of the As-sociation for Computational Linguistics, pages 427?434.Zhou, G. and Zhang M. 2007.
Extracting relation in-formation from text documents by exploring varioustypes of knowledge.
Information Processing & Man-agement 43(4): 969--982.Zhou, G., et al 2007.
Tree kernel-based relation ex-traction with context-sensitive structured parse treeinformation.
In: Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pp.
728?736.67
