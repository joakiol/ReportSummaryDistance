Discourse Segmentation of Multi-Party ConversationMichel Galley Kathleen McKeownColumbia UniversityComputer Science Department1214 Amsterdam AvenueNew York, NY 10027, USA{galley,kathy}@cs.columbia.eduEric Fosler-LussierColumbia UniversityElectrical Engineering Department500 West 120th StreetNew York, NY 10027, USAfosler@ieee.orgHongyan JingIBM T.J. Watson Research CenterYorktown Heights, NY 10598, USAhjing@us.ibm.comAbstractWe present a domain-independent topicsegmentation algorithm for multi-partyspeech.
Our feature-based algorithm com-bines knowledge about content using atext-based algorithm as a feature andabout form using linguistic and acous-tic cues about topic shifts extracted fromspeech.
This segmentation algorithm usesautomatically induced decision rules tocombine the different features.
The em-bedded text-based algorithm builds on lex-ical cohesion and has performance compa-rable to state-of-the-art algorithms basedon lexical information.
A significant er-ror reduction is obtained by combining thetwo knowledge sources.1 IntroductionTopic segmentation aims to automatically divide textdocuments, audio recordings, or video segments,into topically related units.
While extensive researchhas targeted the problem of topic segmentation ofwritten texts and spoken monologues, few have stud-ied the problem of segmenting conversations withmany participants (e.g., meetings).
In this paper, wepresent an algorithm for segmenting meeting tran-scripts.
This study uses recorded meetings of typi-cally six to eight participants, in which the informalstyle includes ungrammatical sentences and overlap-ping speakers.
These meetings generally do not havepre-set agendas, and the topics discussed in the samemeeting may or may not related.The meeting segmenter comprises two compo-nents: one that capitalizes on word distribution toidentify homogeneous units that are topically cohe-sive, and a second component that analyzes conver-sational features of meeting transcripts that are in-dicative of topic shifts, like silences, overlaps, andspeaker changes.
We show that integrating featuresfrom both components with a probabilistic classifier(induced with c4.5rules) is very effective in improv-ing performance.In Section 2, we review previous approaches tothe segmentation problem applied to spoken andwritten documents.
In Section 3, we describe thecorpus of recorded meetings intended to be seg-mented, and the annotation of its discourse structure.In Section 4, we present our text-based segmenta-tion component.
This component mainly relies onlexical cohesion, particularly term repetition, to de-tect topic boundaries.
We evaluated this segmenta-tion against other lexical cohesion segmentation pro-grams and show that the performance is state-of-the-art.
In the subsequent section, we describe conver-sational features, such as silences, speaker change,and other features like cue phrases.
We present amachine learning approach for integrating these con-versational features with the text-based segmenta-tion module.
Experimental results show a markedimprovement in meeting segmentation with the in-corporation of both sets of features.
We close withdiscussions and conclusions.2 Related WorkExisting approaches to textual segmentation can bebroadly divided into two categories.
On the onehand, many algorithms exploit the fact that topicsegments tend to be lexically cohesive.
Embodi-ments of this idea include semantic similarity (Mor-ris and Hirst, 1991; Kozima, 1993), cosine similarityin word vector space (Hearst, 1994), inter-sentencesimilarity matrix (Reynar, 1994; Choi, 2000), en-tity repetition (Kan et al, 1998), word frequencymodels (Reynar, 1999), or adaptive language models(Beeferman et al, 1999).
Other algorithms exploita variety of linguistic features that may mark topicboundaries, such as referential noun phrases (Pas-sonneau and Litman, 1997).In work on segmentation of spoken docu-ments, intonational, prosodic, and acoustic indica-tors are used to detect topic boundaries (Grosz andHirschberg, 1992; Nakatani et al, 1995; Hirschbergand Nakatani, 1996; Passonneau and Litman, 1997;Hirschberg and Nakatani, 1998; Beeferman et al,1999; Tu?r et al, 2001).
Such indicators includelong pauses, shifts in speaking rate, great range inF0 and intensity, and higher maximum accent peak.These approaches use different learning mecha-nisms to combine features, including decision trees(Grosz and Hirschberg, 1992; Passonneau and Lit-man, 1997; Tu?r et al, 2001) exponential models(Beeferman et al, 1999) or other probabilistic mod-els (Hajime et al, 1998; Reynar, 1999).3 The ICSI Meeting CorpusWe have evaluated our segmenter on the ICSI Meet-ing corpus (Janin et al, 2003).
This corpus is one ofa growing number of corpora with human-to-humanmulti-party conversations.
In this corpus, record-ings of meetings ranged primarily over three differ-ent recurring meeting types, all of which concernedspeech or language research.1 The average durationis 60 minutes, with an average of 6.5 participants.They were transcribed, and each conversation turnwas marked with the speaker, start time, end time,and word content.From the corpus, we selected 25 meetings to besegmented, each by at least three subjects.
Weopted for a linear representation of discourse, sincefiner-grained discourse structures (e.g.
(Grosz andSidner, 1986)) are generally considered to be diffi-cult to mark reliably.
Subjects were asked to markeach speaker change (potential boundary) as eitherboundary or non-boundary.
In the resulting anno-tation, the agreed segmentation based on majority1While it would be desirable to have a broader variety ofmeetings, we hope that experiments on this corpus will stillcarry some generality.opinion contained 7.5 segments per meeting on av-erage, while the average number of potential bound-aries is 770.
We used Cochran?s Q (1950) to eval-uate the agreement among annotators.
Cochran?stest evaluates the null hypothesis that the numberof subjects assigning a boundary at any position israndomly distributed.
The test shows that the inter-judge reliability is significant to the 0.05 level for 19of the meetings, which seems to indicate that seg-ment identification is a feasible task.24 Segmentation based on Lexical CohesionPrevious work on discourse segmentation of writtentexts indicates that lexical cohesion is a strong in-dicator of discourse structure.
Lexical cohesion isa linguistic property that pertains to speech as well,and is a linguistic phenomenon that can also be ex-ploited in our case: while our data does not havethe same kind of syntactic and rhetorical structureas written text, we nonetheless expect that informa-tion from the written transcription alone should pro-vide indications about topic boundaries.
In this sec-tion, we describe our work on LCseg, a topic seg-menter based on lexical cohesion that can handleboth speech and text, but that is especially designedto generate the lexical cohesion feature used in thefeature-based segmentation described in Section 5.4.1 Algorithm DescriptionLCseg computes lexical chains, which are thoughtto mirror the discourse structure of the underly-ing text (Morris and Hirst, 1991).
We ignore syn-onymy and other semantic relations, building a re-stricted model of lexical chains consisting of sim-ple term repetitions, hypothesizing that major topicshifts are likely to occur where strong term repeti-tions start and end.
While other relations betweenlexical items also work as cohesive factors (e.g.
be-tween a term and its super-ordinate), the work onlinear topic segmentation reporting the most promis-ing results account for term repetitions alone (Choi,2000; Utiyama and Isahara, 2001).The preprocessing steps of LCseg are common tomany segmentation algorithms.
The input documentis first tokenized, non-content words are removed,2Four other meetings failed short the significance test, whilethere was little agreement on the two last ones (p > 0.1).and remaining words are stemmed using an exten-sion of Porter?s stemming algorithm (Xu and Croft,1998) that conflates stems using corpus statistics.Stemming will allow our algorithm to more accu-rately relate terms that are semantically close.The core algorithm of LCseg has two main parts:a method to identify and weight strong term repeti-tions using lexical chains, and a method to hypothe-size topic boundaries given the knowledge of multi-ple, simultaneous chains of term repetitions.A term is any stemmed content word within thetext.
A lexical chain is constructed to consist of allrepetitions ranging from the first to the last appear-ance of the term in the text.
The chain is divided intosubchains when there is a long hiatus of h consecu-tive sentences with no occurrence of the term, whereh is determined experimentally.
For each hiatus, anew division is made and thus, we avoid creatingweakly linked chains.For all chains that have been identified, we use aweighting scheme that we believe is appropriate tothe task of inducing the topical or sub-topical struc-ture of text.
The weighting scheme depends on twofactors:Frequency: chains containing more repeatedterms receive a higher score.Compactness: shorter chains receive a higherweight than longer ones.
If two chains of differentlengths contain the same number of terms, we assigna higher score to the shortest one.
Our assumptionis that the shorter one, being more compact, seemsto be a better indicator of lexical cohesion.3We apply a variant of a metric commonly usedin information retrieval, TF.IDF (Salton and Buck-ley, 1988), to score term repetitions.
If R1 .
.
.
Rn isthe set of all term repetitions collected in the text,t1 .
.
.
tn the corresponding terms, L1 .
.
.
Ln their re-spective lengths,4 and L the length of the text, theadapted metric is expressed as follows, combiningfrequency (freq(ti)) of a term ti and the compact-ness of its underlying chain:score(Ri) = freq(ti) ?
log( LLi )3The latter parameter might seem controversial at first, andone might assume that longer chains should receive a higherscore.
However we point out that in a linear model of dis-course, chains that almost span the entire text are barely indica-tive of any structure (assuming boundaries are only hypothe-sized where chains start and end).4All lengths are expressed in number of sentences.In the second part of the algorithm, we combineinformation from all term repetitions to compute alexical cohesion score at each sentence break (or,in the case of spoken conversations, speaker turnbreak).
This step of our algorithm is very similarin spirit to TextTiling (Hearst, 1994).
The idea is towork with two adjacent analysis windows, each offixed size k. For each sentence break, we determinea lexical cohesion function by computing the cosinesimilarity at the transition between the two windows.Instead of using word counts to compute similarity,we analyze lexical chains that overlap with the twowindows.
The similarity between windows (A andB) is computed with:5cosine(A,B) =?iwi,A?wi,B??iw2i,A?iw2i,Bwherewi,?
={score(Ri) if Ri overlaps ?
?
{A,B}0 otherwiseThe similarity computed at each sentence breakproduces a plot that shows how lexical cohesionchanges over time; an example is shown in Figure 1.The lexical cohesion function is then smoothed us-ing a moving average filter, and minima become po-tential segment boundaries.
Then, in a manner quitesimilar to (Hearst, 1994), the algorithm determinesfor every local minimum mi how sharp of a changethere is in the lexical cohesion function.
The algo-rithm looks on each side of mi for maxima of cohe-sion, and once it eventually finds one on each side (land r), it computes the hypothesized segmentationprobability:p(mi) = 12 [LCF(l) + LCF(r) ?
2 ?
LCF(m)]where LCF(x) is the value of the lexical cohesionfunction at x.This score is supposed to capture the sharpness ofthe change in lexical cohesion, and give probabilitiesclose to 1 for breaks like sentence 179 in Figure 1.Finally, the algorithm selects the hypothesizedboundaries with the highest computed probabilities.If the number of reference boundaries is unknown,the algorithm has to make a guess.
It computes the5Normalizing anything in these windows has little ef-fect, since the cosine similarity is scale invariant, that iscosine(?xa, xb) = cosine(xa, xb) for ?
> 0.20 40 60 80 100 120 140 160 180 200 220 240 2600.20.30.40.50.60.70.80.91Figure 1: Application of the LCseg algorithm on the concatenation of 16 WSJ stories.
Numbers on thex-axis represent sentence indices, and y-axis represents the lexical cohesion function.
The representativeexample presented here is segmented by LCseg with an error of Pk = 15.79, while the average performanceof the algorithm is Pk = 15.31 on the WSJ test corpus (unknown number of segments).mean and the variance of the hypothesized probabil-ities of all potential boundaries (local minima).
Aswe can see in Figure 1, there are many local minimathat do not correspond to actual boundaries.
Thus,we ignore all potential boundaries with a probabilitylower than plimit.
For the remaining points, we com-pute the threshold using the average (?)
and standarddeviation (?)
of the p(mi) values, and each potentialboundary mi above the threshold ???
??
is hypoth-esized as a real boundary.4.2 EvaluationWe evaluate LCseg against two state-of-the-art seg-mentation algorithms based on lexical cohesion(Choi, 2000; Utiyama and Isahara, 2001).
We usethe error metric Pk proposed by Beeferman et al(1999) to evaluate segmentation accuracy.
It com-putes the probability that sentences k units (e.g.
sen-tences) apart are incorrectly determined as being ei-ther in different segments or in the same one.
Sinceit has been argued in (Pevzner and Hearst, 2002) thatPk has some weaknesses, we also include results ac-cording to the WindowDiff (WD) metric (which isdescribed in the same work).A test corpus of concatenated6 texts extractedfrom the Brown corpus was built by Choi (2000)to evaluate several domain-independent segmenta-tion algorithms.
We reuse the same test corpus forour evaluation, in addition to two other test corporawe constructed to test how segmenters scale acrossgenres and how they perform with texts with various6Concatenated documents correspond to reference seg-ments.number of segments.7 We designed two test corpora,each of 500 documents, using concatenated textsextracted from the TDT and WSJ corpora, rangingfrom 4 to 22 in number of segments.LCseg depends on several parameters.
Parametertuning was performed on three tuning corpora of onethousand texts each.8 We performed searches for theoptimal settings of the four tunable parameters in-troduced above; the best performance was achievedwith h = 11 (hiatus length for dividing a chain intoparts), k = 2 (analysis window size), plimit = 0.1and ?
= 12 (thresholding limits for the hypothesizedboundaries).As shown in Table 1, our algorithm is signifi-cantly better than (Choi, 2000) (labeled C99) onall three test corpora, according to a one-sided t-test of the null hypothesis of equal mean at the 0.01level.
It is not clear whether our algorithm is betterthan (Utiyama and Isahara, 2001) (U00).
When thenumber of segments is provided to the algorithms,our algorithm is significantly better than Utiyama?son WSJ, better on Brown (but not significant), andsignificantly worse on TDT.
When the number ofboundaries is unknown, our algorithm is insignifi-cantly worse on Brown, but significantly better onWSJ and TDT ?
the two corpora designed to havea varying number of segments per document.
In thecase of the Meeting corpus, none of the algorithmsare significantly different than the others, due to the7All texts in Choi?s test corpus have exactly 10 segments.We are concerned that the adjustments of any algorithm param-eters might overfit this predefined number of segments.8These texts are different from the ones used for evaluation.Brown corpusknown unknownPk WD Pk WDC99 11.19% 13.86% 12.07% 14.57%U00 8.77% 9.44% 9.76% 10.32%LCseg 8.69% 9.42% 10.49% 11.37%p-val.
0.42 0.48 0.027 0.0037TDT corpusC99 9.37% 11.91% 10.18% 12.72%U00 4.70% 6.29% 8.70% 11.12%LCseg 6.15% 8.41% 6.95% 9.09%p-val.
1.1e-05 2.8e-07 4.5e-05 2.8e-05WSJ corpusC99 19.61% 26.42% 22.32% 29.81%U00 15.18% 21.54% 17.71% 24.06%LCseg 12.21% 18.25% 15.31% 22.14%p-val.
1.4e-08 1.7e-08 2.6e-04 0.0063Meeting corpusC99 33.79% 37.25% 47.42% 58.08%U00 31.99% 34.49% 37.39% 40.43%LCseg 26.37% 29.40% 31.91% 35.88%p-val.
0.026 0.14 0.14 0.23Table 1: Comparison C99 and U00.
The p-values inthe table are the results of significance tests betweenU00 and LCseg.
Bold-faced values are scores thatare statistically significant.small test set size.In conclusion, LCseg has a performance compara-ble to state-of-the-art text segmentation algorithms,with the added advantage of computing a segmen-tation probability at each potential boundary.
Thisinformation can be effectively used in the feature-based segmenter to account for lexical cohesion, asdescribed in the next section.5 Feature-based SegmentationIn the previous section, we have concentrated exclu-sively on the consideration of content (through lexi-cal cohesion) to determine the structure of texts, ne-glecting any influence of form.
In this section, weexplore formal devices that are indicative of topicshifts, and explain how we use these cues to build asegmenter targeting conversational speech.5.1 Probabilistic ClassifiersTopic segmentation is reduced here to a classifica-tion problem, where each utterance break Bi is ei-ther considered a topic boundary or not.
We usestatistical modeling techniques to build a classifierthat uses local features (e.g.
cue phrases, pauses)to determine if an utterance break corresponds toa topic boundary.
We chose C4.5 and C4.5rules(Quinlan, 1993), two programs to induce classifi-cation rules in the form of decision trees and pro-duction rules (respectively).
C4.5 generates an un-pruned decision tree, which is then analyzed byC4.5rules to generate a set of pruned productionrules (it tries to find the most useful subset of them).The advantage of pruned rules over decision trees isthat they are easier to analyze, and allow combina-tion of features in the same rule (feature interactionsare explicit).The greedy nature of decision rule learning algo-rithms implies that a large set of features can leadto bad performance and generalization capability.
Itis desirable to remove redundant and irrelevant fea-tures, especially in our case since we have little datalabeled with topic shifts; with a large set of fea-tures, we would risk overfitting the data.
We triedto restrict ourselves to features whose inclusion ismotivated by previous work (pauses, speech rate)and added features that are specific to multi-speakerspeech (overlap, changes in speaker activity).5.2 FeaturesCue phrases: previous work on segmentation hasfound that discourse particles like now, well pro-vide valuable information about the structure of texts(Grosz and Sidner, 1986; Hirschberg and Litman,1994; Passonneau and Litman, 1997).
We analyzedthe correlation between words in the meeting cor-pus and labeled topic boundaries, and automaticallyextracted utterance-initial cue phrases9 that are sta-tistically correlated with boundaries.
For every wordin the meeting corpus, we counted the number of itsoccurrences near any topic boundary, and its num-ber of appearances overall.
Then, we performed ?2significance tests (e.g.
figure 2 for okay) under thenull hypothesis that no correlation exists.
We se-lected terms whose ?2 value rejected the hypothesisunder a 0.01-level confidence (the rejection criterionis ?2 ?
6.635).
Finally, induced cue phrases whoseusage has never been described in other work wereremoved (marked with ?
in Table 3).
Indeed, thereis a risk that the automatically derived list of cuephrases could be too specific to the word usage in9As in (Litman and Passonneau, 1995), we restrict ourselvesto the first lexical item of any utterance, plus the second one ifthe first item is also a cue word.Near boundary Distantokay 64 740Other 657 25896Table 2: okay (?2 = 89.11, df = 1, p < 0.01).okay 93.05 but 13.57shall ?
27.34 so 11.65anyway 23.95 and 10.99we?re ?
17.67 should ?
10.21alright 16.09 good ?
7.70let?s ?
14.54Table 3: Automatically selected cue phrases.these meetings.Silences: previous work has found that ma-jor shifts in topic typically show longer silences(Passonneau and Litman, 1993; Hirschberg andNakatani, 1996).
We investigated the presence ofsilences in meetings and their correlation with topicboundaries, and found it necessary to make a distinc-tion between pauses and gaps (Levinson, 1983).
Apause is a silence that is attributable to a given party,for example in the middle of an adjacency pair, orwhen a speaker pauses in the middle of her speech.Gaps are silences not attributable to any party, andlast until a speaker takes the initiative of continuingthe discussion.
As an approximation of this distinc-tion, we classified a silence that follows a question orin the middle of somebody?s speech as a pause, andany other silences as a gap.
While the correlation be-tween long silences and discourse boundaries seemto be less pervasive in meetings than in other speechcorpora, we have noticed that some topic boundariesare preceded (within some window) by numerousgaps.
However, we found little correlation betweenpauses and topic boundaries.Overlaps: we also analyzed the distribution ofoverlapping speech by counting the average overlaprate within some window.
We noticed that, manytimes, the beginning of segments are characterizedby having little overlapping speech.Speaker change: we sometimes noticed a corre-lation between topic boundaries and sudden changesin speaker activity.
For example, in Figure 2, itis clear that the contribution of individual speakersto the discussion can greatly change from one dis-course unit to the next.
We try to capture significantchanges in speakership by measuring the dissimilar-ity between two analysis windows.
For each poten-tial boundary, we count for each speaker i the num-ber of words that are uttered before (Li) and after(Ri) the potential boundary (we limit our analysisto a window of fixed size).
The two distributionsare normalized to form two probability distributionsl and r, and significant changes of speakership aredetected by computing their Jensen-Shannon diver-gence:JS(l, r) = 12 [D(l||avgl,r) + D(r||avgl,r)]where D(l||r) is the KL-divergence between thetwo distributions.Lexical cohesion: we also incorporated the lexi-cal cohesion function computed by LCseg as a fea-ture of the multi-source segmenter in a manner simi-lar to the knowledge source combination performedby (Beeferman et al, 1999) and (Tu?r et al, 2001).Note that we use both the posterior estimate com-puted by LCseg and the raw lexical cohesion func-tion as features of the system.5.3 Features: Selection and CombinationFor every potential boundary Bi, the classifier ana-lyzes features in a window surrounding Bi to decidewhether it is a topic boundary or not.
It is generallyunclear what is the optimal window size and howfeatures should be analyzed.
Windows of varioussizes can lead to different levels of prediction, andin some cases, it might be more appropriate to onlyextract features preceding or following Bi.We avoided making arbitrary choices of parame-ters; instead, for any feature F and a set F1, .
.
.
, Fnof possible ways to measure the feature (differentwindow sizes, different directions), we picked the Fithat is in isolation the best predictor of topic bound-aries (among F1, .
.
.
, Fn).
Table 4 presents for eachfeature the analysis mode that is the most useful onthe training data.5.4 EvaluationWe performed 25-fold cross-validation for evaluat-ing the induced probabilistic classifier, computingthe average of Pk and WD on the held-out meet-ings.
Feature selection and decision rule learning0 10 20 30Figure 2: speaker activity in a meeting.
Each row represent the speech activity of one speaker, utterance ofwords being represented as black.
Vertical lines represent topic shifts.
The x-axis represents time.Feature Tag Size (sec.)
SideCue phrases CUE 5 bothSilence (gaps) SIL 30 leftOverlap?
OVR 30 rightSpeaker activity ACT 5 bothLexical cohesion LC 30 both?
: the size of the window that was used to compute theJS-divergence was also determined automatically.Table 4: Parameters for feature analysis.is always performed on sets of 24 meetings, whilethe held-out data is used for testing.
Table 5 givessome examples of the type of rules that are learned.The first rule states that if the value for the lexicalcohesion (LC) function is low at the current sen-tence break, there is at least one CUE phrase, thereis less than three seconds of silence to the left of thebreak,10 and a single speaker holds the floor for alonger period of time than usual to the right of thebreak, then we have a topic break.
In general, wefound that the derived rules show that lexical cohe-sion plays a stronger role than most other featuresin determining topic breaks.
Nonetheless, the quan-titative results summarized in table 6, which corre-spond to the average performance on the held-outsets, show that the integration of conversational fea-tures with the text-based segmenter outperforms ei-ther alone.6 ConclusionsWe presented a domain-independent segmentationalgorithm for multi-party conversation that inte-grates features based on content with features basedon form.
The learned combination of features resultsin a significant increase in accuracy over previous10Note that rules are not always meaningful in isolation andit is likely that a subordinate rule in the tree to this one would dofurther tests on silence to determine if a topic boundary exists.Condition Decision Conf.LC ?
0.67,CUE ?
1,OVR ?
1.20,SIL ?
3.42 yes 94.1LC ?
0.35,SIL > 3.42,OVR ?
4.55 yes 92.2CUE ?
1,ACT > 0.1768,OVR ?
1.20,LC ?
0.67 yes 91.6. .
.default noTable 5: A selection of the most useful rules learnedby C4.5rules along with their confidence levels.Times for OVR and SIL are expressed in seconds.Pk WDfeature-based 23.00% 25.47%LCseg 31.91% 35.88%U00 37.39% 40.43%p-value 2.14e-04 3.30e-04Table 6: Performance of the feature-based seg-menter on the test data.approaches to segmentation when applied to meet-ings.
Features based on form that are likely to in-dicate topic shifts are automatically extracted fromspeech.
Content based features are computed by asegmentation algorithm that utilizes a metric of lex-ical cohesion and that performs as well as state-of-the-art text-based segmentation techniques.
It worksboth with written and spoken texts.
The text-basedsegmentation approach alone, when applied to meet-ings, outperforms all other segmenters, although thedifference is not statistically significant.In future work, we would like to investigate theeffects of adding prosodic features, such as pitchranges, to our segmenter, as well as the effect ofusing errorful speech recognition transcripts as op-posed to manually transcribed utterances.An implementation of our lexical cohesion seg-menter is freely available for educational or researchpurposes.11AcknowledgmentsWe are grateful to Julia Hirschberg, Dan Ellis, Eliz-abeth Shriberg, and Mari Ostendorf for their helpfuladvice.
We thank our ICSI project partners for grant-ing us access to the meeting corpus and for usefuldiscussions.
This work was funded under the NSFproject Mapping Meetings (IIS-012196).ReferencesD.
Beeferman, A. Berger, and J. Lafferty.
1999.
Statisti-cal models for text segmentation.
Machine Learning,34(1?3):177?210.F.
Choi.
2000.
Advances in domain independent lineartext segmentation.
In Proc.
of NAACL?00.W.
Cochran.
1950.
The comparison of percentages inmatched samples.
Biometrika, 37:256?266.B.
Grosz and J. Hirschberg.
1992.
Some intonationalcharacteristics of discourse structure.
In Proc.
ofICSLP-92, pages 429?432.B.
Grosz and C. Sidner.
1986.
Attention, intentions andthe structure of discourse.
Computational Linguistics,12(3).M.
Hajime, H. Takeo, and O. Manabu.
1998.
Text seg-mentation with multiple surface linguistic cues.
InCOLING-ACL, pages 881?885.M.
Hearst.
1994.
Multi-paragraph segmentation of ex-pository text.
In Proc.
of the ACL.J.
Hirschberg and D. Litman.
1994.
Empirical studieson the disambiguation of cue phrases.
ComputationalLinguistics, 19(3):501?530.J.
Hirschberg and C. Nakatani.
1996.
A prosodic anal-ysis of discourse segments in direction-giving mono-logues.
In Proc.
of the ACL.J.
Hirschberg and C. Nakatani.
1998.
Acoustic indicatorsof topic segmentation.
In Proc.
of ICSLP.A.
Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,N.
Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-cke, and C. Wooters.
2003.
The ICSI meeting corpus.In Proc.
of ICASSP-03, Hong Kong (to appear).11http://www.cs.columbia.edu/?galley/research.htmlM.-Y.
Kan, J. Klavans, and K. McKeown.
1998.
Linearsegmentation and segment significance.
In Proc.
6thWorkshop on Very Large Corpora (WVLC-98).H.
Kozima.
1993.
Text segmentation based on similaritybetween words.
In Proc.
of the ACL.S.
Levinson.
1983.
Pragmatics.
Cambridge UniversityPress.D.
Litman and R. Passonneau.
1995.
Combining multi-ple knowledge sources for discourse segmentation.
InProc.
of the ACL.J.
Morris and G. Hirst.
1991.
Lexcial cohesion computedby thesaural relations as an indicator of the structure oftext.
Computational Linguistics, 17:21?48.C.
Nakatani, J. Hirschberg, and B. Grosz.
1995.
Dis-course structure in spoken language: Studies onspeech corpora.
In AAAI-95 Symposium on EmpiricalMethods in Discourse Interpretation.R.
Passonneau and D. Litman.
1993.
Intention-basedsegmentation: Human reliability and correlation withlinguistic cues.
In Proc.
of the ACL.R.
Passonneau and D. Litman.
1997.
Discourse seg-mentation by human and automated means.
Compu-tational Linguistics, 23(1):103?139.L.
Pevzner and M. Hearst.
2002.
A critique and im-provement of an evaluation metric for text segmenta-tion.
Computational Linguistics, 28 (1):19?36.R.
Quinlan.
1993.
C4.5: Programs for Machine Learn-ing.
Machine Learning.
Morgan Kaufmann.J.
Reynar.
1994.
An automatic method of finding topicboundaries.
In Proc.
of the ACL.J.
Reynar.
1999.
Statistical models for topic segmenta-tion.
In Proc.
of the ACL.G.
Salton and C. Buckley.
1988.
Term weighting ap-proaches in automatic text retrieval.
Information Pro-cessing and Management, 24(5):513?523.G.
Tu?r, D. Hakkani-Tu?r, A. Stolcke, and E. Shriberg.2001.
Integrating prosodic and lexical cues for auto-matic topic segmentation.
Computational Linguistics,27(1):31?57.M.
Utiyama and H. Isahara.
2001.
A statistical modelfor domain-independent text segmentation.
In Proc.
ofthe ACL.J.
Xu and B. Croft.
1998.
Corpus-based stemming usingcooccurrence of word variants.
ACM Transactions onInformation Systems, 16(1):61?81.
