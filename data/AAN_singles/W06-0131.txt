Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 177?180,Sydney, July 2006. c?2006 Association for Computational LinguisticsPOC-NLW Template for Chinese Word SegmentationBo Chenchb615@gmail.comWeiran Xuxuweiran@263.netTao Pengppttbupt@gmail.comJun Guoguojun@bupt.edu.cnPattern Recognition and Intelligent System LabBeijing University of Posts and TelecommunicationsBeijing 100876, P. R. ChinaAbstractIn this paper, a language tagging tem-plate named POC-NLW (position of acharacter within an n-length word) is pre-sented.
Based on this template, a two-stage statistical model for Chinese wordsegmentation is constructed.
In thismethod, the basic word segmentation isbased on n-gram language model, and aHidden Markov tagger based on thePOC-NLW template is used to imple-ment the out-of-vocabulary (OOV) wordidentification.
The system participated inthe MSRA_Close and UPUC_Closeword segmentation tracks at SIGHANBakeoff 2006.
Results returned by thisbakeoff are reported here.1 IntroductionIn Chinese word segmentation, there are twoproblems still remain, one is the resolution ofambiguity, and the other is the identification ofso-called out-of-vocabulary (OOV) or unknownwords.
In order to resolve these two problems, atwo-stage statistical word segmentation strategyis adopted in our system.
The first stage is op-tional, and the whole segmentation can be ac-complished in the second stage.
In the first stage,the n-gram language model is employed to im-plement basic word segmentation including dis-ambiguation.
In the second stage, a languagetagging template named POC-NLW (position ofa character within an n-length word) is intro-duced to accomplish unknown word identifica-tion as template-based character tagging.The remainder of this paper is organized asfollows.
In section 2 and section 3, a briefly de-scription of the main methods adopted in oursystem is given.
Results of our system at thisbakeoff are reported in section 4.
At last, conclu-sions are derived in section 5.2 The Basic Word Segmentation StageIn the first stage, the basic word segmentation isaccomplished.
The key issue in this stage is theambiguity problem, which is mainly caused bythe fact that a Chinese character can occur in dif-ferent word internal positions in different words(Xue, 2003).
A lot of machine learning tech-niques have been applied to resolve this problem,the n-gram language model is one of the mostpopular ones among them (Fu and Luke, 2003;Li et al, 2005).
As such, we also employed n-gram model in this stage.When a sentence is inputted, it is first seg-mented into a sequence of individual characters(e.g.
ASCII strings, basic Chinese characters,punitions, numerals and so on), marked as C1,n.According to the system?s dictionary, severalword sequences W1,m will be constructed as can-didates.
The function of the n-gram model is tofind out the best word sequence W* correspondsto C1,n, which has the maximum integrated prob-ability, i.e.,trigramforWWWPbigramforWWPCWPWmiiiiWmiiiWnmWmmm??=??=??
?=12111,1,1*),|(maxarg)|(maxarg)|(maxarg,1,1,1177The Maximum Likelihood method was used toestimate the word n-gram probabilities used inour model, and the linear interpolation method(Jelinek and Mercer, 1980) was applied tosmooth these estimated probabilities.3 The OOV Word Identification StageThe n-gram method is based on the exiting gramsin the model, so it is good at judging the connect-ing relationship among known words, but doesnot have the ability to deal with unknown wordsin substance.
Therefore, another OOV wordidentification model is required.OOV words are regarded as words that do notexist in a system?s machine-readable dictionary,and a more detailed definition can be found in(Wu and Jiang, 2000).
In general, Chinese wordcan be created through compounding or abbrevi-ating of most of existing characters and words.Thus, the key to solve the OOV word identifica-tion lies on whether the new word creationmechanisms in Chinese language can be ex-tracted.
Therefore, a POC-NLW language tag-ging template is introduced to explore such in-formation on the character-level within words.3.1 The POC-NLW TemplateMany character-level based works have beendone for the Chinese word segmentation, includ-ing the LMR tagging methods (Xue, 2003; Na-kagawa.
2004), the IWP mechanism (Wu andJiang, 2000).
Based on these previous works, thisPOC-NLW template was derived.
Assume thatthe length of a word is the number of componentcharacters in it, the template is consist of twocomponent: Lmax and a Wl-Pn tag set.
Lmax to de-note the maximum length of a word expressed bythe template; a Wl-Pn tag denotes that this tag isassigned to a character at the n-th position withina l-length word, .
Apparently, thesize of this tag set isln ,,2,1 L=2/)1( maxmax LL ?+For example, the Chinese word ???
?
istagged as:?
W2P1, ?
W2P2and ?????
is tagged as:?
W3P1, ?
W3P2, ?
W3P3In the example, two words are tagged by thetemplate respectively, and the Chinese character???
has been assigned two different tags.In a sense, the Chinese word creation mecha-nisms could be extracted through statistics of thetags for each character on a certain large corpus.On the other hand, while a character sequencein a sentence is tagged by this template, the wordboundaries are obvious.
Meanwhile, the wordsegmentation is implemented.In addition, in this template, known words andunknown words are both regarded as sequencesof individual characters.
Thus, the basic wordsegmentation process, the disambiguation proc-ess and the OOV word identification process canbe accomplished in a unified process.
Thereby,this model can also be used alone to implementthe word segmentation task.
This characteristicwill make the word segmentation system muchmore efficient.3.2 The HMM TaggerForm the description of POC-NLW template, itcan be found that the word segmentation couldbe implemented as POC-NLW tagging, which issimilar to the so-called part-of-speech (POS)tagging problem.
In POS tagging, HiddenMarkov Model (HMM) was applied as one of themost significant methods, as described in detailin (Brants, 2000).
The HMM method can achievehigh accuracy in tagging with low processingcosts, so it was adopted in our model.According to the definition of POC-NLWtemplate, the state set of HMM corresponds tothe Wl-Pn tag set, and the symbol set is com-posed of all characters.
However, the initial stateprobability matrix and the state transition prob-ability matrix are not composed of all of the tagsin the state set.
To express more clearly, we de-fine two subset of the state set:?
Begin Tag Set (BTS): this set is con-sisted of tag which can occur in the beg-ging position in a word.
Apparently, thesetags must have the Wl-P1 form.?
End Tag Set (ETS): correspond to BTS,tags in this set should occur in the end po-sition, and their form should be like Wl-Pl.Apparently, the size of BTS is Lmax as well asof ETS.
Thus, the initial state probability matrixcorresponds to BTS instead of the whole state set.On the other hand, because of the word internalcontinuity, if the current tag Wl-Pn is not in ETS,than the next tag must be Wl-P(n+1).
In otherwords, the case in which the transition probabil-ity is need is that when the current tag is in ETSand the next tag belongs to BTS.
So, the statetransition matrix in our model corresponds toBTSETS ?
.178The probabilities used in HMM were definedsimilarly to those in POS tagging, and were es-timated using the Maximum Likelihood methodfrom the training corpus.In the two-stage strategy, the output word se-quence of the first stage is transferred into thesecond stage.
The items in the sequence, includ-ing individual characters and words, which donot have a bigram or trigram relationship withthe surrounding items, are picked out with itssurrounding items to compose several sequencesof items.
These item sequences are processed bythe HMM tagger to form new item sequences.
Atlast, these processed items sequences are com-bined into the whole word sequence as the finaloutput.4 Results and Analysis4.1 SystemThe system submitted at this bakeoff was a two-stage one, as describe at beginning of this paper.The model used in the first stage was trigram,and the Lmax of the template used in the secondstage was set to 7.In addition to the tags defined in the templatebefore, a special tag is introduced into our Wl-Pntag set to indicate all those characters that occurafter the Lmax-th position in an extremely long(longer than Lmax) word., formulized as WLmax-P(Lmax+1).
And then, there are 28 basic tags(from W1-P1 to W7-P7) and the special one W7-P8.For instance, using the special tag, the word????????????
(form the MSRACorpus ) is tagged as:?
W7-P1  ?
W7-P2  ?
W7-P3  ?
W7-P4?
W7-P5  ?
W7-P6  ?
W7-P7  ?
W7-P8?
W7-P8  ?
W7-P84.2 Results at SIGHAN Bakeoff 2006Our system participated in the MSRA_Close andUPUC_Close track at the SIGHAN Bakeoff2006.
The test results are as showed in Table 1.Corpus MSRA UPUCF-measure 0.951 0.918Recall 0.956 0.932Precision 0.947 0.904IV Recall 0.972 0.969OOV Recall 0.493 0.546OOV Precision 0.569 0.757Table 1.
Results at SIGHAN Bakeoff 2006The performances of our system on the twocorpuses can rank in the half-top group amongthe participated systems.We notice that the accuracies on known wordsegmentation are relatively better than on OOVwords segmentation.
This appears somewhat un-expected.
In the close experiments we had doneon the PKU and MSR corpuses of SIGHANBakeoff 2005, the relative performance of OOVRecall was much more outstanding than of the F-measure.We think this is due to the inappropriate pa-rameters used in n-gram model, which over-guarantees the performance of basic word seg-mentation.
It can be seen on the IV Recall (high-est in UPUC_Close track).
For only the best out-put sequence of the n-gram model is transferredto the HMM tagger, some potential unknownwords may be miss-split in the early stage.
Thus,the OOV Recall is not very good, and this alsoaffects the overall performance.On the other hand, the performances of OOVidentification on UPUC are much better than onMSRA, while the performances of overall seg-mentation accuracy on UPUC are worse than onMSRA.
This phenomenon also happened in ourexperiments on the Bakeoff 2005 corpuses ofPKU and MSR.
In the PKU test data, the rate ofOOV words according is 0.058 while in MSR is0.026.
Thus, it can be conclude that the moreunknown words occur, the more significant abil-ity of OOV words identification appears.In addition, the relative performance of OOVPrecision are much better.
This demonstrates thatthe OOV identification ability of our system isappreciable.
In other words, the POC-NLW tag-ging method introduced is effective to some ex-tent.5 CONCLUSION AND FURTHERWORKIn this paper, a POC-NLW template is presentedfor word segmentation, which aims at exploringthe word creation mechanisms in Chinese lan-guage by utilizing the character-level informa-tion to.
A two-stage strategy was applied in oursystem to combine the n-gram model based wordsegmentation and OOV word identification im-plemented by a HMM tagger.
Test results showthat the method achieved high performance onword segmentation, especially on unknownwords identification.
Therefore, the method is apractical one that can be implemented as an inte-179gral component in actual Chinese NLP applica-tions.From the results, it can safely conclude thatmethod introduced here does find some charac-ter-level information, and the information couldeffectively conduct the word segmentation andunknown words identification.
For this is the firsttime we participate in this bakeoff, and the workhas been done as a integral part of another sys-tem during the past two months, the implementa-tion of the segmentation system we submitted iscoarse.
A lot of improvements, on either theo-retical methods or implementation techniques,are required in our future work, including thesmoothing techniques in the n-gram model andthe HMM model, the refine of the features ex-traction method and the POC-NLW template it-self, the more harmonious integration strategyand so on.AcknowledgementsThis work is partially supported by NSFC(National Natural Science Foundation of China)under Grant No.60475007, Key Project of Chi-nese Ministry of Education under GrantNo.02029 and the Foundation of Chinese Minis-try of Education for Century Spanning Talent.ReferencesAndi Wu, and Zixin Jiang.
2000.
Statistically-enhanced new word identification in a rule-basedChinese system.
Proceedings of the 2nd ChineseLanguage Processing Workshop, 46-51.Frederick Jelinek, and Robert L. Mercer.
1980.
Inter-polated Estimation of Markov Source Parametersfrom Sparse Data.
Proceedings of Workshop onPattern Recognition in Practice, Amsterdam, 381-397.Guohong Fu, and Kang-Kwong Luke.
2003.
A Two-stage Statistical Word Segmentation System forChinese.
Proceedings of the Second SIGHANWorkshop on Chinese Language Processing, 156-159.Heng Li, Yuan Dong, Xinnian Mao, Haila Wang, andWu Liu.
2005.
Chinese Word Segmentation inFTRD Beijing.
Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing, 150-153.Nianwen Xue.
2003.
Chinese Word Segmentation asCharacter Tagging.
International Journal of Com-putational Linguistics and Chinese Language Pro-cession, 8(1):29?48.Tetsuji Nakagawa.
2004.
Chinese and Japanese WordSegmentation Using Word-Level and Character-Level Information.
Proceedings of the 20th Inter-national Conference on Computational Linguistics,466?472.Thorsten Brants.
2000.
TnT ?
A Statistical Part-of-Speech Tagger.
Proceedings of the Sixth Confer-ence on Applied Natural Language ProcessingANLP-2000, 224?231.180
