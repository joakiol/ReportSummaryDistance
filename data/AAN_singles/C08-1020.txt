Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 153?160Manchester, August 2008Hybrid processing for grammar and style checkingBerthold Crysmann?, Nuria Bertomeu?, Peter Adolphs?, Dan Flickinger?, Tina Klu?wer???
Universita?t Bonn, Poppeldorfer Allee 47, D-53115 Bonn, {bcr,tkl}@ifk.uni-bonn.de?
Zentrum fu?r Allgemeine Sprachwissenschaft, Berlin, nuria.bertomeu@dfki.de?
DFKI GmbH, Berlin, {peter.adolphs,kluewer}@dfki.de?
CSLI, Stanford University, danf@csli.stanford.eduAbstractThis paper presents an implemented hy-brid approach to grammar and stylechecking, combining an industrial pattern-based grammar and style checker with bi-directional, large-scale HPSG grammarsfor German and English.
Under this ap-proach, deep processing is applied selec-tively based on the error hypotheses of ashallow system.
We have conducted a com-parative evaluation of the two components,supporting an integration scenario wherethe shallow system is best used for error de-tection, whereas the HPSG grammars adderror correction for both grammar and con-trolled language style errors.1 IntroductionWith the enormous amount of multilingual techni-cal documentation produced by companies nowa-days grammar and controlled language checking(henceforth: style checking) is becoming an appli-cation highly in demand.
It is not only a helpfultool for authors, but also facilitates the translationof documents into foreign languages.
Through theuse of controlled language by the authors, docu-ments can be automatically translated more suc-cessfully than with the use of free language.
Stylechecking should make authors aware of the con-structions which should not be used, as well asaiding in reformulating them.
This can save a lotof translation costs for companies producing largeamounts of mulitilingual documentation.
Anotherapplication of grammar and style checking is thedevelopment of tutorial systems for learning a for-eign language, as well as any kind of authoring sys-tem for non-native speakers.Previous approaches to grammar and stylechecking can be divided into those based on fi-nite state methods and those based on linguisti-cally motivated grammars.
To the former group be-long e.g.
the systems FLAG (Bredenkamp et al,2000a; Bredenkamp et al, 2000b) and MultiLintc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.
(Haller, 1996; Schmidt-Wigger, 1998).
The basicapproach taken by such systems is the descriptionof error patterns through finite state automata.
Theautomata access the textual input enriched withannotations from shallow linguistic analysis com-ponents, such as part-of-speech tagging, morphol-ogy and chunking.
In FLAG, for instance, the an-notation delivered by the shallow components isintegrated into a complex feature structure.
Rulesare defined as finite state automata over featurestructures.
The great advantages of such systemsare their robustness and efficient processing, whichmake them highly suitable for real-life grammarand style checking applications.
However, sinceshallow modules usually cannot provide a full syn-tactic analysis, the coverage of these systems islimited to error types not requiring a broader (non-local) syntactic context for their detection.
There-fore their precision in the recognition of non-localerrors is not satisfactory.Another short-coming of most shallow ap-proaches to grammar checking is that they typi-cally do not provide error correction: owing to theabsence of an integrated target grammar, genera-tion of repairs cannot take the syntactic contextinto account: as a result, some of the repairs sug-gested by shallow systems are not globally well-formed.Grammar-based error checking constitutes theother main strand in language checking technol-ogy.
These systems are typically equipped with amodel of target well-formedness.
The main prob-lem, when applied to the task of error checkingis that the sentences that are the focus of a gram-mar checker are ideally outside the scope of thegrammar.
To address this problem, grammar-basedcheckers typically employ robustness techniques(Ravin, 1988; Jensen et al, 1993; Douglas, 1995;Menzel, 1998; Heinecke et al, 1998).
The addi-tion of robustness features, while inevitable for agrammar-based approach, has the disadvantage ofconsiderably slowing down runtime performance.Another issue with purely grammar-based check-ing is related to the scarce distribution of actualerrors: thus, most effort is spent on the processingof perfectly impeccable utterances.
Finally, sincecoverage of real-world grammars is never perfect,these system also have difficulty to distinguish be-153tween extragrammatical and truly ungrammaticalsentences.
Conversely, since grammars often over-generate, a successful parse does not guaranteewellformedness either.One of the two major robustness techniquesused in the context of grammar-based languagechecking are constraint relaxation (see e.g.
(Dou-glas, 1995; Menzel, 1998; Heinecke et al, 1998)),which is typically realised by means of modifica-tions to the parser (e.g.
relaxation levels, robust uni-fication).
An alternative approach is error anticipa-tion where errors are explicitly modelled by meansof grammar rules, so-called MAL-rules (McCoy etal., 1996).
This approach has already been inves-tigated with an HPSG grammar, the ERG (Copes-take and Flickinger, 2000), in the scenario of a tu-torial system for language learning by (Bender etal., 2004).
We will follow this approach in the partof our hybrid system based on deep processing.Finite state methods and linguistically motivatedgrammars are not only compatible, but also com-plementary.
Shallow methods are robust and effi-cient, while deep processing based on grammarsprovides high precision and detail.
With the fo-cussed application of deep analysis in finite statebased grammar and style checking systems, bothcoverage and precision can be improved, whilethe performance remains acceptable for real-worldapplications.
The combination of shallow anddeep components, hybrid processing, has alreadybeen investigated in several modular architectures,such as GATE (Gaizauskas et al, 1996), White-board (Crysmann et al, 2002) and Heart-of-Gold(Callmeier et al, 2004).
Moreover, the improve-ment in efficiency and robustness in deep process-ing together with methods for its efficient applica-tion makes the employment of deep processing inreal-world applications quite feasible.
Hybrid pro-cessing has been used for applications such as in-formation extraction and question answering.
Butto the best of our knowledge, the application of hy-brid processing to grammar and style checking hasnot been previously investigated.In this paper, we present an implemented proto-type of a hybrid grammar and style checking sys-tem for German and English, called Checkpoint.As the baseline shallow system we have takenan industrial strength grammar and controlled lan-guage style checker, which is based on the FLAGtechnology.
The deep processing platform used inthe project is the PET parser (Callmeier, 2000)operating on wide-coverage English and GermanHPSG grammars, the English Resource Grammar(ERG) (Copestake and Flickinger, 2000) and theGerman Grammar (GG) (Mu?ller and Kasper, 2000;Crysmann, 2005; Crysmann, 2007), respectively.The ERG and the GG have been developed for over15 years and have already been used as deep pro-cessing engines in the Heart-of-Gold hybrid pro-cessing platform.
We have developed an approachfor the selective application of deep processingbased on the error hypotheses of the shallow sys-tem.
Error detection in the deep system follows aMAL-rule approach.
In order to compare the ben-efits of the selective application of deep process-ing with its nonselective application, we have de-veloped two scenarios: one parallel and one inte-grated.
While the parallel (nonselective) scenarioenables improvement in both recall and precision,the integrated (selective) scenario only enables im-provement in precision.
However, the performanceof the integrated approach is much better.
We havealso investigated several possibilities of integratingdeep processing in the selective scenario.
Since theHPSG grammars are suitable both for parsing andgeneration, the system can successfully provideboth error corrections and paraphrases of stylisticerrors.
For the purpose of investigation, evaluationand statistical parse ranking, we have collected andannotated several corpora of texts from technicalmanuals.
Finally, the approach has been evaluatedregarding error detection and performance.2 The approachCheckpoint has two main goals: (a) improving theprecision and recall of existing pattern-based gram-mar and style checking systems for error typeswhose detection requires considering more thanthe strictly local syntactic context; and (b) gener-ating error corrections for both grammar and styleerrors.
Accordingly, we have chosen to focus oncertain error types based on the difficulties of thepattern-based system.2.1 Anticipation of grammar errorsGrammar errors are detected by means of erroranticipation rules, or MAL-rules.
MAL-rules ex-actly model errors, so that erroneous sentences canbe parsed by the grammar.
For this purpose weenlarged two HPSG grammars for German, theGG, and English, the ERG, with MAL-rules forerror types that were problematic for the pattern-based shallow system.
For German the followingphenomena have been handled: subject verb agree-ment (subject verb agreement), NP internal agree-ment (NP internal agreement), confusion of thecomplementiser ?dass?
with the homophonous pro-noun or determiner ?das?
(dass das), as well asediting errors, such as local and non local repeti-tion of words (repetitions).
Here follow some ex-amples (taken from the FLAG error corpus (Beckeret al, 2000), and die tageszeitung ?taz?, a Germannewspaper):(1) Auch in AOL gibt es Newsgroups, diedieses Thema diskutiert [=diskutieren].
(FLAG)Also in AOL are there newgroups, which (Pl)this topic discuss (Sg).
?There are also newsgroups in AOL which dis-cuss this topic.
?154(2) Ich habe dem ganze [=ganzen] Geschehenvon meinem Sofa aus zugesehen.
(FLAG)I have the whole (wrong adj.
form) eventsfrom my couch out watched.
?I have watched the whole events from mycouch.?
(3) Vor allem im Su?den .
.
.
fu?hrten [=haben] dieLiberalen der MR einen heftigen Wahlkampfgegen die PS gefu?hrt.
(taz, June 2007)Above all in the south .
.
.
led (past tense) theliberals of the MR a hard election campaignagainst the PS led (past participle).
?Particularly in the south, the liberals of theMR led a hard election campaign against thePS.
?For English, MAL-rules for errors concerningsubject verb agreement and missing determinerswere implemented.2.2 Detection of stylistic errorsStylistic errors are grammatical constructions thatare dispreferred in a particular register or typeof document.
Sometimes certain constructions arenot desirable because machine translation systemshave problems dealing with them or because theyprevent easy understanding.
In such cases a con-trolled language approach is taken, where the prob-lematic constructions are paraphrased into equiv-alent less problematic constructions.
Since theseconstructions are grammatical they can be parsedand, thus, detected.
A generation of a paraphraseis possible based on the semantic representationobtained through parsing.
For German the follow-ing phenomena were handled: passive, future andimplicit conditional sentences, as in the followingexample:(4) Wartet man zulange, kriegt man keine Karten.Waits one too long, gets one no tickets.
?If one waits too long one gets no tickets.
?Correct: Wenn man zulange wartet, kriegtman keine Karten.For English we focussed on the following phenom-ena: passive (avoid passive), future (avoid future),modal verbs (avoid modal verbs), subjunctive(avoid subjunctive), stand-alone deictic pro-nouns (use this that these those with noun) andclause order in conditional sentences (condi-tion must precede action).2.3 Integrated vs. parallel scenariosWe have developed two integration scenarios: anintegrated one and a parallel one.
In the parallelscenario the pattern-based shallow system and thedeep processing parser run independently of eachother, that is, all sentences are parsed independentof whether the shallow system has found an errorin them.
In the integrated scenario the deep parseris only called for those sentences where the shal-low system has detected some error of the typeof those which Checkpoint is able to process (enu-merated in subsection 2.1).
The parallel scenarioallows improvement in the recall of the shallowsystem, since Checkpoint can find errors that theshallow system has not found.
In the integratedscenario, on the contrary, only the precision of theshallow system can be improved, since Checkpointdeparts from the hypotheses of the shallow system.The integrated scenario, however, promises to per-form better in time than the parallel scenario, sinceonly a fraction of the whole text has to be scannedfor errors.
Moreover, the performance of the inte-grated system can also be improved with the se-lective activation of the MAL-rules that model thespecific errors found by the shallow system.
Thisgreatly reduces the enormous search space of theparsing algorithms and the processing time result-ing from the simultaneous processing of severalMAL-rules.The integration of the shallow system and thedeep parser has been achieved through an exten-sion of the PET parser that allows it to receive anykind of input information and integrate this intothe chart.
This preprocessing information can be,for example, part-of-speech tagging, morphologyand lemmatisation, and already guides the parsingprocess.
It allows, for instance, recognition of un-known words or identification of the correct lexi-cal entry in cases where there is ambiguity.
An in-put format in terms of feature structures, the ?Fea-ture Structure Chart?
(FSC) format, has been devel-oped for this purpose (Adolphs et al, 2008).
Theshallow system, thus, produces a feature structurechart, based on the information delivered by thevarious shallow modules, and this information isgiven as input to the PET deep parser, which readsit and integrates it into the chart.Error hypotheses from the shallow system arepassed to the deep parser by means of specific fea-tures in the input feature structure (MAL-features)of every input token in the FSC, permitting selec-tive activation of MAL-rules.
To this end, the origi-nal FSC generated by the shallow system, whichcontains information on the part-of-speech, thelemma and morphological features such as num-ber, gender and case, will be extended with MAL-features.
These MAL-features correspond to theclass of some MAL-rule in the grammar and haveboolean values.
Signs in the grammar are speci-fied for these MAL-features.
MAL-rules are de-fined such that they can only take as their daughtersedges with a positive value for the correspondingMAL-feature.
All information in the FSC input to-kens is passed to the tokens in the chart througha feature called TOKEN in lexical items.
Thus, er-ror hypotheses are passed from the input tokens tothe lexical items in the chart by stating that the val-ues of the MAL-features in the lexical items are155equal to the values of the MAL-features in the cor-responding input tokens in the FSC.The values of the MAL-features are obtainedby checking the error report delivered by the shal-low system.
For certain errors detected by the shal-low system there is a mapping to MAL-features.The value of a MAL-feature will be set to ?+?
ifthe shallow system has found the correspondingerror.
The rest of the MAL-features can be set to?bool?
if we want to allow other MAL-rules tofire (which can improve recall, but increases am-biguity and, consequently, has a negative effect onperformance).
The values of the rest of the MAL-features can also be set to ?-?, if we want to preventother MAL-rules from firing (which allows im-provement only in precision, but limits ambiguityand, consequently, results in better performance).There is also the possibility of activating the rel-evant MAL-features only for those tokens whichare, according to the shallow system, within the er-ror span, instead of activating the MAL-featuresfor all tokens in the erroneous sentence.2.4 Generation of corrections andparaphrasesOne of the advantages of using deep processingin grammar and style checking is the possibilityof generating corrections and paraphrases whichobey the constraints imposed by the syntactic con-text.
Since the HPSG grammars that we are usingare suitable both for parsing and generation, thisis straightforward.
Robust parsing delivers as out-put a semantic representation in the Minimal Re-cursion Semantics formalism (MRS) (Copestakeet al, 2006) of the sentence which can be used forgeneration with the LKB (Carroll et al, 1999).The MAL-rules directly assign well-formed se-mantic representations from which a correct sur-face string can be generated.
In the case of stylis-tic errors, transfer rules are used to generate thedesired paraphrase, using MRS-to-MRS mappingrules modelled on the semantic transfer-based ma-chine translation approach of (L?nning et al,2004).We identified two areas where generation of re-pairs will actually provide a considerable addedvalue to a grammar checking system: first, for non-native speakers, simple highlighting of the errorlocation is often insufficient, since the user maynot be familiar with the rules of the language.
Sec-ond, some areas, in particular stylistic ones mayinvolve considerable rearrangement of the entiresentence.
In these cases, generation of repairs andparaphrases can reduce editing cost and also min-imise the issue of editing errors associated withnon-local phenomena.The generator and HPSG grammars we use areable to provide a range of realisations for a givensemantic input.
As a result, realisation ranking isof utmost importance.
In order to select repairswhich are both smooth and maximally faithful tothe input, modulo the error site, of course, we com-bined two methods: a discriminative PCFG-modeltrained on a generation treebank, enhanced by ann-gram language model, cf.
(Velldal and Oepen,2005), and an alignment approach that chooses themost conservative edit from a set of input realisa-tions.
As our similarity measure, we employed avariant of BLEU score (NEVA), suggested in (Fors-bom, 2003).
The probabilistic ranking models wetrained achieve an exact match accuracy of 73%for both English (Velldal and Oepen, 2005) andGerman (as evaluated on the subset of TiGer theerror corpus was based on).3 Error corporaIn order to learn more about the frequencies of thedifferent error types, to induce statistical modelsthat allow us to obtain the best parse in the do-main of technical manuals and to evaluate our im-plemented approach to grammar and style check-ing, we collected and manually annotated corporafrom the domain of technical documentation.Since errors in pre-edited text tend to be veryscarcely distributed, manual annotation is quitecostly.
As a result, instance of certain well-knownerror types cannot be tested in a greater variety oflinguistic environments.
To overcome this problem,we semi-automatically derived an additional errorcorpus from a treebank of German.English For purposes of evaluation in a realworld scenario, we constructed a corpus for En-glish, consisting of 12241 sentences (169459words) from technical manuals.
The corpus wassemi-automatically annotated with several types ofgrammar and style errors.
For this purpose annota-tion guidelines were developed, which containedthe description of the errors together with exam-ples of each and their possible corrections.
The an-notation took place in two phases.
First, we wantedto find out about the precision of the shallow sys-tem, so we ran the shallow system over the data.This resulted in an annotation for each error foundconsisting of the erroneous sentence, the error spanand the type of error.
The annotators, who were na-tive speakers, then decided whether the errors hadbeen correctly detected.
In the second phase, weaimed to create a gold standard, so as to be able toevaluate both the shallow system and Checkpointregarding recall and precision.
For this purpose, weextracted the errors that had been annotated as cor-rectly detected in the previous phase and the an-notators only had to find the non-detected errorsin the rest of the corpus.
For the latter, they alsomarked the span and identified the error type.Subsets of these two datasets were treebankedwith the corresponding HPSG grammars.
We em-ployed the treebanking methodology developed forRedwoods (Oepen et al, 2002), which involved156first parsing a corpus and recording for each itemthe alternative analyses (the parse forest) assignedby the grammar, then manually identifying the cor-rect analysis (if available) within that parse forest.This approach provides both a gold standard syn-tactic/semantic analysis for each parsed item, andpositive and negative training data for building anaccurate statistical model for automatic parse selec-tion.German For German, we pursued a complemen-tary approach towards corpus construction.
Herethe focus lay on creating a test and evaluation cor-pus that provided instances of common error typesin a variety of linguistic contexts.
Since manualerror annotation is highly costly, owing to scarceerror distributions in pre-edited text, we chose toautomatically derive an error corpus from an ex-isting treebank resource.
As for the error types, wefocussed on those errors which are arguably perfor-mance errors, as e.g.
missing final consonants in in-flectional endings, the confusion of homophonouscomplementiser and relative pronoun, or else, edit-ing errors, such as local and non-local duplicates.We introduced instances of errors in a sub-corpus of the German TiGer treebank (Brantset al, 2002), nicknamed TiG-ERR, consisting of77275 words (5652 sentences) from newspapertexts.
All the sentences in this subcorpus wereparsable, so that an evaluation of Checkpoint inthe ideal situation of 100% coverage could be car-ried out.
The artificially introduced errors wereof the following types: subject verb agreement,NP internal agreement, dass/das, and repetitions,all of them already illustrated with examples in sec-tion 2.1.Additionally, we annotated a corpus of technicaldocuments for these error types to estimate the dis-tribution of these error types in pre-edited text.4 Error modelsIn order to construct a statistical parse-rankingmodel which could determine the intended use ofa MAL-rule in the analysis of a sentence where thegrammar produced analyses both with and withoutMAL-rules, the English treebank was constructedusing the version of the ERG which included theMAL-rules.
4000 sentences from the English cor-pus were presented to the parser, of which 86.8%could be parsed with the ERG, and of these, the an-notators found an intended analysis for 2500 sen-tences, including some which correctly used MAL-rules.
From these annotations, a customised parseselection model was computed and then used inparsing all of the corpus, this time recording onlythe one analysis determined to be most likely ac-cording to this model.
We also compared accu-racy of error detection based on this new modelwith the accuracy of a pre-existing parse-selectionmodel trained on tourism data for LOGON, andconfirmed that the new model indeed improvedover the old one.For German, we have not created a specific sta-tistical model yet, but, instead, we have used an ex-isting parse selection model (Crysmann, 2008) andcombined it with some heuristics which enable usto select the best error hypothesis.
The heuristicscheck for each parsed sentence whether there is ananalysis containing no MAL-rule.
If there is oneand this is not ranked as the best parse, it is movedto the first position in the parse list.
As a result, wecan eliminate a high percentage of false alarms.5 Evaluation resultsWe have evaluated the English and the German ver-sions of Checkpoint against the corpora describedin section 3.German For German we have taken as a testcorpus standard the TiG-ERR subcorpus contain-ing the automatically introduced errors, and haveparsed all its sentences.
The following table showsthe frequencies of the different types of handled er-rors in the corpus of technical manuals, the FLAGerror corpus (Becker et al, 2000), and in the TiG-ERR corpus.
The electronic version of the FLAGcorpus consists of 14,492 sentences, containing1,547 grammar or style errors.ERROR TYPE MANUALS FLAG TiG-ERRNP internal agr 119 180 2258subject verb agr 17 63 748dass/das 1 152 75repetitions 19 n/a 2571Table 1: Frequencies of the error types for GermanThe following charts show the values for recalland precision for the shallow system and Check-point.
As you can see, Checkpoint improves therecall for the error types subject verb agreementand NP internal agreement, whereas the precisionremains more or less the same.
For the error typedass/das Checkpoint improves both recall and pre-cision.
For the error type repetitions, which is onlypartially handled by the spell checker in the shal-low system, Checkpoint reaches considerable re-call and precision values.Deep processing on average improves the recallof the shallow system by 21% and the precisionremains equal at 0.83.
According to the error fre-quencies in the corpus of technical manuals, deepprocessing would improve the recall of the shallowsystem by only 1.7%, since the error types sub-ject verb agreement, NP internal agreement anddass/das only make up 6.57% of the total amountof annotated errors.
However, as we found out later,the corpora of technical manuals consist of textsthat have already undergone correction, so the er-rors are very sparse.157Figure 1: Checkpoint values for recall and preci-sion for GermanFigure 2: Values for recall and precision for theshallow system for GermanThrough the MAL-rules the coverage of the GGon the TiG-ERR corpus increased to 85% - 95%,whereas without the MAL-rules the coverage was10%.
This 10% coverage included overgenerationby the grammar, as well as sentences that, after theautomatic insertion of errors, still remained gram-matical, although they didn?t express the intendedmeaning any more.The performance of the parallel and integratedscenarios was compared.
The ambiguity of theMAL-rules, that is, the possibility of applying sev-eral MAL-rules to a unique error, considerably de-teriorates the performance when processing sen-tences containing several errors.
In a subcorpuscontaining NP internal agreement errors, the aver-age processing time per sentence increases from8.3 seconds with the selective activation of MAL-rules to 31.4 seconds with the activation of allMAL-rules.
Particularly the MAL-rules modelingthe error subject verb agreement are a source ofambiguity.
If these MAL-rules are only selectivelyactivated the average processing time per sentencedecreases to 14.9 seconds.Finally, we have evaluated the performance ofthe German grammar in the task of error correction,using non-local duplicates and adjectival agree-ment errors as a test bed.
For these error types,the German HPSG grammar generated repairs for85.4% of the detected non-local duplicates and90% of the detected agreement errors.English For English we have only implementedand evaluated the parallel scenario.
The focus forEnglish evaluation was the recognition of thosestylistic errors whose correction requires a re-structuring of the sentence, and the generation ofthe corresponding paraphrases.
The recognition ofsuch error types is not based on MAL-rules, buton certain already existing rules in the grammar.The approach was evaluated taking the manuallyannotated English corpus of technical manuals asa gold standard.
The following table shows the fre-quencies of the error types handled by Checkpoint.ERROR TYPE OCCURRENCESavoid future 404avoid modal verbs 657avoid passive 213Table 2: Frequencies of the error types for EnglishThe PET parser with the ERG reached 86.1%coverage on the full corpus.
The following chartsshow the values for recall and precision for Check-point and the shallow system.Figure 3: Checkpoint values for recall and preci-sion for EnglishFigure 4: Values for recall and precision for theshallow system for EnglishAs one can see, for the stylistic errorsavoid future and avoid modal verbs, Checkpointreaches values which, although relatively high, arelower than the shallow system.
In most cases aparaphrase for these errors can be constructed,so the improvement Checkpoint provides here isthe generation of corrections.
For the error typeavoid passive the precision is not so high, whichis due in part to mistakes in the manual annotation.The passive sentences found by Checkpoint areactually passive sentences.
However, these were158not annotated as passives, because the annotatorswere told to annotate only those stylistic errorsfor which a paraphrase was possible.
The samehappens for stylistic errors like avoid subjunctive,use this that these those with noun and condi-tion must precede action.
In principle, Check-point is very good at finding these types of errors,but we cannot yet present a reliable evaluation here,since only those errors were annotated for whicha paraphrase was possible.
This approach is rea-sonable, since no error alarm should be producedwhen there is no other possibility of expressing thesame.
However, since we have not yet developeda method which allows us to automatically distin-guish those cases for which a paraphrase is possi-ble from those for which none is, we would needto annotate all occurrences of a phenomenon in thecorpus, and introduce a further annotation tag forthe paraphrase potential of the sentence.Nevertheless, even if the grammar-based re-search prototype cannot beat the industrial pattern-based system in terms of f-measures, we still be-lieve that the results are highly valuable in the con-text of our integrated hybrid scenario: Since thefull reversibility of the ERG has already been estab-lished independently by (Velldal and Oepen, 2005),the combined system is able to generate error cor-rection for a great proportion of the errors detectedby the shallow component.
This includes 80% andabove for avoid future and avoid modal verbs.6 Summary and conclusionsIn this paper we have presented an implementedapproach to grammar and style checking based onhybrid processing.
The hybrid system has two com-ponents: a shallow grammar and style checkingsystem based on the FLAG technology, and thePET deep parser operating on linguistically moti-vated grammars for German and English.
The Ger-man version of the hybrid system improves the re-call and in certain cases the precision of the shal-low system and generates error corrections.
ForEnglish, the hybrid system in most cases success-fully generates paraphrases of sentences contain-ing stylistic errors.
Although we only have ex-plored some of the possibilities of integrating deepand shallow processing for the grammar and stylechecking application, these results speak for thefeasibility of using hybrid processing in this task.We have developed an integrated strategy whichforwards the output of the shallow system, includ-ing both the output from several pre-processinglinguistic modules and the error hypotheses, as in-put to the deep parser.
This procedure not only im-proves the robustness of the deep parser with therecognition of unknown words and reduces ambi-guity by instantiating only those lexical items con-sistent with the hypotheses of the POS tagger orthe morphology; but it also allows the selectiveapplication of grammar rules, which considerablyreduces the search space for parsing and, conse-quently, improves performance.
Based on the errorhypotheses of the shallow system, the selective ap-plication of grammar rules is achieved by positingfeatures in the Feature Structure Chart whose par-ticular values are a pre-condition for MAL-rulesto apply.
The improvement in performance sug-gests that this strategy can be extensible to parsingin general based on pre-processing components.Given the output of a chunker, for example, certainsyntactic configurations can already be excluded.Having features whose values allow one to switchoff certain rules not compatible with these con-figurations would considerably reduce the searchspace.On the other hand, we have run the two mod-ules independently from each other to find outhow the recall of the shallow system can be im-proved by deep processing.
The fact that for sev-eral error types, such as subject verb agreementand NP internal agreement, recall can be consider-ably improved suggests that, in order not to parseall sentences, the shallow system should send anerror hypothesis to the deep system when findingparticular syntactic configurations which may indi-cate the occurrence of such errors.
In this way, sucherror hypotheses, although not reliably detectableby the shallow system alone, could be confirmedor discarded with a focussed application of deepprocessing, which would not be as resource con-suming as parsing every sentence.One of the results of the experiment has beenan on-line demonstration system.
The running sys-tem shows that the different modules can be eas-ily combined with each other.
Our hybrid approach,however, is generic and portable.
Although imple-mented for our specific baseline system, it can inprinciple be used with other shallow systems.AcknowledgementsThe research reported in this paper has been car-ried out as part of the DFKI project Checkpoint,running from February until November 2007.
Theproject was funded by the ProFIT program of theFederal State of Berlin and the EFRE program ofthe European Union.ReferencesAdolphs P., S. Oepen, U. Callmeier, B. Crysmann, andB.
Kiefer.
2008.
Some Fine Points of Hybrid NaturalLanguage Parsing.
Proceedings LREC-2008, Mar-rakech, Morocco.Becker M., A. Bredenkamp, B. Crysmann, and J. Klein.2003.
Annotation of error types for a German news-group corpus.
In A.
Abeille?, editor, Treebanks.Building and Using Parsed Corpora, number 20 inText, Speech And Language Technology.
Kluwer,Dordrecht.159Bender, E. M., D. Flickinger, S. Oepen, A. Walsh,and T. Baldwin.
2004.
Arboretum: Using a preci-sion grammar for grammar checking in call.
In In-STIL/ICALL symposium 2004.
NLP and speech tech-nologies in advanced language learning systems.Venice, Italy.Brants, T., S. Dipper, S. Hansen, W. Lezius, and G.Smith.
2002.
The TIGER Treebank.
In Proceedingsof the Workshop on Treebanks and Linguistic Theo-ries.
Sozopol.Bredenkamp, A., B. Crysmann and M. Petrea.
2000.Looking for errors: A declarative formalism forresource-adaptive language checking.
In Proceed-ings LREC-2000.
Athens, Greece.Bredenkamp, A., B. Crysmann and M. Petrea.
2000.Building multilingual controlled language perfor-mance checkers.
In Proceedings of the CLAW 2000.Seattle, WA.Callmeier, U., A. Eisele, U. Scha?fer, and M. Siegel.2004.
The Deepthought core architecture frame-work.
In Proceedings of LREC-2004, 1205?1208,Lisbon, Portugal.Callmeier, Ulrich.
2000.
PET ?
a platform for ex-perimentation with efficient HPSG processing tech-niques.
Natural Language Engineering 6(1):99?108.Carroll, John and Ann Copestake and Dan Flickingerand Victor Poznanski.
1999.
An efficient chart gen-erator for semi-lexicalist grammars.
Proceedings ofENLG, pp.
86?95.Copestake, A., and D. Flickinger.
2000.
An open-source grammar development environment andbroad-coverage english grammar using HPSG.
InProceedings LREC-2000.
Athens, Greece.Copestake, A., D. Flickinger, C. Pollard, and I. Sag.2006.
Minimal recursion semantics: an introduction.Research on Language and Computation 3(4):281?332.Crysmann, B., A. Frank, B. Kiefer, S. Mu?ller, G. Neu-mann, J. Piskorski, U. Scha?fer, M. Siegel, H. Uszko-reit, F. Xu, M. Becker, and H.-U.
Krieger.
An inte-grated architecture for shallow and deep processing.In Proceedings of ACL 2002, University of Pennsyl-vania, Philadelphia, 2002.Crysmann B.
2005.
Relative clause extraposition inGerman: An efficient and portable implementation.Research on Language and Computation, 3(1):61?82.Crysmann B.
2007 Local ambiguity packing and dis-continuity in German.
In T. Baldwin, M. Dras,J.
Hockenmaier, T. H. King, and G. van Noord, ed-itors, Proceedings of the ACL 2007 Workshop onDeep Linguistic Processing, pages 144?151, Prague,Czech Republic.Crysmann B.
2008.
Parse Selection with a GermanHPSG Grammar.
In S. Ku?bler and G. Penn, editors,Proceedings of the ACL 2008 Workshop on ParsingGerman (PaGe), pages 9?15, Columbus, Ohio, USA.Douglas, S. 1995.
Robust PATR for error detec-tion and correction.
In A. Schoeter and C.
Vogel(Eds.
)Nonclassical feature systems, Vol.
10, pp.
139-156.
Centre for Cognitive Science, University of Ed-inburgh.Forsbom, E. 2003.
Training a Super Model Look-Alike.
Proceedings of the MT Summit IX Workshop?Towards Systemizing MT Evaluation?, pp.
29-36.Gaizauskas, R., H. Cunningham, Y. Wilks, P. Rodgersand K. Humphreys.
1996.
GATE: An environmentto support research and development in natural lan-guage engineering.
In Proceedings of the 8th IEEEinternational conference on tools with artificial in-telligence.
Toulouse, France.Jensen, K., G. E., Heidorn and S. D. Richardson (Eds.).1993.
Natural language processing: The PLNLP ap-proach.
Boston - Dordrecht - London.Haller, J.
1996.
MULTILINT: A technical documenta-tion system with multilingual intelligence.
In Trans-lating and the computer 18.
London.Heinecke, J., J. Kunze, W. Menzel, and I. Schroeder.1998.
Eliminative parsing with graded constraints.In Proceedings ACL/Coling 1998, Vol.
I, pp.
526-530.
Universite de Montreal, Montreal, Quebec,Canada.L?nning J. T. , S. Oepen, D. Beermann, L. Hellan, J.Carroll, H. Dyvik, D. Flickinger, J.
B. Johannessen,P.
Meurer, T. Nordga?rd, V. Rose?n and E. Velldal.2004.
LOGON.
A Norwegian MT effort.
In Pro-ceedings of the Workshop in Recent Advances inScandinavian Machine Translation.
Uppsala, Swe-den.McCoy, K. F., C. A. Pennington, and L. Z. Suri.
1996.English error correction: A syntactic user modelbased on principled ?mal-rule?
scoring.
In Proceed-ings of UM-96, the Fifth International Conference onUser Modeling, pp.
59-66.
Kona, Hawaii.Menzel, W. 1998.
Constraint satisfaction for robustparsing of natural language.
In Theoretical and Ex-perimental Artificial Intelligence, 10 (1), 77-89.Mu?ller, S., and W. Kasper.
2000.
HPSG analysis ofGerman.
In W. Walster (Ed.
), Verbmobil: Foun-dations of Speech-to-Speech Translation, 238?253Springer, Berlin.Oepen, S., K. Toutanova, S. Shieber, C. Manning, D.Flickinger and T Brants.
2002.
The LinGO Red-woods Treebank.
Motivation and Preliminary Appli-cations.
In Proceedings of COLING 2002.
Taipei,Taiwan.Ravin, Y.
1998.
Grammar errors and style weaknessesin a text-critiquing system.
In IEEE Transactions onCommunication, 31 (3)Schmidt-Wigger, A.
1998.
Grammar and style check-ing for German.
In Proceedings of CLAW 98.
Pitts-burgh, PA.Velldal, E. and S. Oepen.
2005.
Maximum entropymodels for realization ranking.
In Proceedings ofthe 10th MT-Summit (X), Phuket, Thailand.160
