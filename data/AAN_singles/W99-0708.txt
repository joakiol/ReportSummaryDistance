MDL-based DCG Induct ion for NP  Identi f icat ionMiles OsborneAlfa Informatica, University of Groningen, NLosborne@let.rug.nlAbstractWe introduce a learner capable of automatically extend-ing large, manually written natural anguage DefiniteClause Grammars with missing syntactic rules.
It isbased upon the Minimum Description Length princi-ple, and can be trained upon either just raw text, orelse raw text additionally annotated with parsed cor-pora.
As a demonstration f the learner, we show howfull Noun Phrases (NPs that might contain pre or post-modifying phrases and might also be recursively nested)can be identified in raw text.
Preliminary results ob-tained by varying the amount of syntactic informationin the training set suggests that raw text is less usefulthan additional NP bracketing information.
However,using all syntactic information i  the training set doesnot produce a significant improvement over just brack-eting information.1 Introduct ionIdentification of Noun Phrases (NPs) in free text hasbeen tackled in a number of ways (for example, \[25, 9,2\]).
Usually however, only relatively simple NPs, suchas 'base' NPs (NPs that do not contain nested NPsor postmodifying clauses) are recovered.
The motiva-tion for this decision seems to be pragmatic, driven inpart by a lack of technology capable of parsing largequantities of free text.
With the advent of broad cover-age grammars (for example \[15\] and attendant efficientparsers \[11\], however, we need not make this restriction:we now can identify 'full' NPs, NPs that might containpre and/or post-modifying complements, in free text.Full NPs m'e more interesting than base NPs to esti-mate:?
They are (at least) context free, unlike base NPswhich are finite state.
They can contain pre- andpost-modifying phrases, and so proper identifica-tion can in the worst case imply full-scale pars-ing/grammar learning.?
Recursive nesting of NPs means that each nominalhead needs to be associated with each NP.
Base NPssimply group all potential heads together in a flatstructure.As a (partial) response to these challenges, we iden-tify full NPs by treating the task as a special case offull-scale sentential Definite Clause Grammar (DCG)learning.
Our approach is based upon the MinimumDescription Length (MDL) principle.
Here, we do notexplain MDL, but instead refer the reader to the liter-ature (for example, see \[26, 27, 29, 12, 22\]).
Althougha DCG learning approach to NP identification is farmore computationally demanding than any other NPlearning technique reported, it does provide a usefultest-bed for exploring some of the (syntactic) factorsinvolved with NP identification.
By contrast, other ap-proaches at NP identification more usually only con-sider lexical/part-of-speech influences.In this paper, we consider, from an estimation per-spective, how dependent NPs are upon their (surround-ing) syntactic ontext.
We varied the information con-tent of the training set and measured the effect his hadupon NP identification accuracy.
Results suggest that:?
Use of any syntactic information, in addition" to rawtext during estimation, produces better esults thanestimation from raw text alone.?
NPs containing an internal annotation (nonterminalsin addition to NPs) are harder to estimate than NPsthat do not contain these additional nonterminals.?
Training with NP annotated sentences and trainingwith sentences annotated with full sentential parsesproduce very similar results to each other.We stress that the last finding is provisional, and furtherinvestigation is necessary to verify it.The structure of the rest of this paper is as follows.Section 2 gives an overview of our approach, whilst sec-tion 3 goes into estimation and modelling details.
We61do not start induction ab initio, but instead base es-timation upon manually written grammars.
Section 4briefly describes the particular grammar used in thisresearch, whilst section 5 relates our work to others.Section 6 presents an experimental evaluation of ourlearner.
The paper ends with a discussion Of our find-ings.2 Overv iewOur learner is probabilistic, and starts with a DCG-based language model M0.
Parameters are initially es-timated from parsed corpora, annotated in terms ofthe non-terminal set used by the DCG.
It incremen-tally processes each sentence s~ in the list of sentencesSo...  's# ...  sn.
If a sentence s# cannot be generated (thegrammar contained within the model lacks the requiredrules), we need to find a new model with a high, non-zero posterior probability given the sentences so.
.
.
s#seen so far.
Our (for computational reasons, necessarilysuboptimal) approach selects uch a model by" carryingout a local search over the space of models with a non-zero posterior probability, given all sentences see so far.We use a MDL-based prior to help us compute a pos-terior probability.
Analogously to Pereira and Schabes(P+S) \[23\], we also constrain the search using parsedcorpora.
Unlike P+S, we not only use parsed corpora toconstrain parameter estimation, we also use it to con-strain model selection.
We replace M0 with the newlyconstructed (locally) maximal a posterior model andafter processing all sentences in this incremental man-ner, terminate with a model that generates all sentencesseen in the training set.Key aspects of our approach are:Incremental learning.
We only construct rules neces-sary to parse sentences in training set.
This reducesthe computational burden and enables us to learnwith grammars that use large (> 30) feature sets.
Bycontrast, batch approaches that compile-out all rulesthat can be expressed with a fixed category set andwith rules limited to some length can only deal withfar smaller feature sets, thereby preventing inductionof realistic grammars.Initialisation with a model containing manually writ-ten rules, with parameters estimated from parsedcorpora.
This alleviates ome of the pitfalls of lo-cal search and, by definition, makes estimation faster(our initial model is already a reasonable stimate).Lari and Young demonstrated this point when theyused an Hidden Markov Model as an approximationof a Stochastic Context Free Grammar SCFG \[19\].Note that in general any manually written grammar62will undergenerate, so there is still a need for newrules to be induced.?
Ability to induce 'fair' models from raw text.
Wedo not select models solely on the basis of likelihood(when training material is limited, such models tendto overfit); instead, we select models in terms of theirMDL-based prior probability and likelihood.
MDL-based estimation usually reduces overfitting (since,with limited training material, we select a model thatfits th~ training material well, but not too well) \[22\].?
Learning from positive-only data.
We do not requirenegative xamples, nor do we require human inter-vention.
This enables us to transparently use gram-mar learning as part of standard text parsing.?
Use of parsed corpora llows us to induce models thatencode semantic and pragmatic preferences.
Rawtext is known to underconstrain duction \[13\] andeven with an MDL-based prior, when training uponraw text, we would be unlikely to estimate a modelwhose hidden variables (grammar ules) resulted inlinguistically plausible derivations.
Parsed corporasupplies ome of this missing information.3 Estimation DetailsEstimation of a model, given a training set of'sentencesand an associated set of manually constructed parses,consists of four steps: probabilistic modelling of DCGs,model construction, search (constrained by parsed cor-pora) , and model estimation.
We now explain thesesteps in turn.P robab i l i s t i c  Mode l l ing  o f  DCGsDCGs in our approach are modelled in terms of acompression-based prior probability and a SCFG-basedlikelihood probability.
The prior assigns high probabil-ity to compact models, and low probabilities to verbose,idiomatic models.
As such, it favours simple grammarsover more complex possibilities.
The likelihood proba-bility describes how well we can encode the training setin terms of the model.
We now turn to the specifcationof likelihood and prior probabilities in our system.L ike l ihood Probab i l i ty  To specify a likelihoodprobability for DCGs, we have opted to use a SCFG,which consists of a set of context free grammar ulesalong with an associated set of parameters \[6\].
Each pa-rameter models the way we might expand non-terminalsin a top-down derivation process, and within a SCFG,we associate one such parameter with each distinct con-text free rule.
However, DCG rules are feature-based,and so not directly equivalent o simple context freerules.
In order to define a SCFG over DCG rules, weneed to interpret hem in a context-free manner.
Oneway to achieve this is as follows.
For each category inthe grammar that is distinct in terms of features, inventan atomic non-terminal symbol.
With these atomicsymbols, create a SCFG by mapping each category in aDCG rule to an atomic symbol, yielding a context free(backbone) grammar, and with this grammar, specifya SCFG, /Vii.
Naturally, this is not the most accurateprobabilistic model for feature-based grammars, but forthe interim, is sufficient (see Abney for a good discus-sion of how one might define a more accurate proba-bilistic model for feature-based grammars \[1\]).SCFGs are standardly defined as follows.
Let P(A -~(~ \] A) be the probability of expanding (backbone) non-terminal symbol A with the (backbone) rule A --+ owhen deriving some sentence si.
The probability of thejth derivation of si is defined as the product of the prob-abilities of all backbone rules used in that derivation.That is, if derivation j followed from an application ofthe rules A~ -~ c~ .
.
.
.
, A~ ~ a~,glP~eriv(Sz \[ M,) = H P(A~ --+ aJ,) (1)i=lThe probability of a seiatence is then defined as the sumof the probabilities of all n ways we can derive it:t lP~(si I Mi) = Y~ P~eriv(Si IMi) (2)3=1Having modelled DCGs as SCFGs, we can immedi-ately specify the likelilmod probability of Mi generatinga sample of sentences so... sn, as:nP(so...  sa I M,) = H Ps(s, I Mi) (3)j=0This treats each sentence as being independently gen-erated from each other sentence.P r io r  P robab i l i ty  Specifying a prior for DCGsamounts to encoding the rules and the associated pa-rameters.
We encode DCG rules in terms of an integergiving the length, in categories, of the rule (requiringlog* (n) bits, where log* is Rissannen's encoding schemefor integers), and a list of that many encoded categories.Each category consists of a list of features, drawn froma finite set of features, and to each feature there is avalue.
In general, each feature will have a separate setof possible values.
Within manually written DCGs, theway a feature is assigned a value is sensitive to the po-sition, in a rule, of the category containing the featurein question.
Hence, if we number the categories of arule.
we can work out the probability that a particularfeature, in a given category, will take a certain value.Let P(v I fO be the probability that feature f takesthe value v, in category i of all rules in the grammar.Each value can now be encoded with a prefix code of- log(P(v  I f i )  bits in length.
Encoding a categorysimply amounts to a (fixed length) sequence of such en-coded features, assuming some canonical ordering uponfeatures.
Note we do not learn lexical entries and so notnot need to encode them.To encode the model parameters, we simply use Ris-sannen's prefix coding scheme for integers to encode arule's frequency.
We do not directly encode probabili-ties.
since these will be inaccurate when the frequency,used to estimate that probability, is low.
Rissanuen'sscheme has the property that small integers are assignedshorter codes than longer integers.
In our context, thiswill favour low frequencies over highcr ones, which isundesirable, given the fact that we want, for estimationaccuracy, to favour higher frequencies.
Hence, insteadof encoding an integer i in log*(/) bits (as, for exam-ple, Keller and Lutz roughly do \[18\]), we encode it inlog*(Z - i) bits, where Z is a mlmber larger than anyfrequency.
This will mean that higher frequencies areassigned shorter code words, as intended.The prior probability of a model )IL, containing aDCG G and an associated parameter set is:P(Mi )  = 2 -(lg(M')+Ip(M')) -t- C (4)where:IdIAM,) = Y .\[log*(I r I) + Z - log(PO' I/,))1 (5)rEG t=l /EFis description length of the gramma," andtAM,)  = l og ' (Z  - y(,.))
(6)rEGis the description length of the paranaeters.
C is a con-stant ensuring that the prior sums to one; F is the setof features used to describe categories: \] r \] is the lengthof a DCG rule r seen f(r) times.Apart from being a prior over DCG rules, our schemehas the pleasing property that it assigns longer codewords to rules containing categories in unlikely posi-tions than to rules containing categories in expected po-sitions.
For example, our scheme would assign a longerlist of code words to the categories expressing a rulesuch as Det -~ Det NP  than to the list of categoriesexpressing a rule such as NP --+ Det NP.
Also, ourcoding scheme favours shorter ules than longer rules,which is desirable, given the fact that, generally speak-ing.
rules in natural language grammars tend to beshort.63!Poster io r  P robab i l i ty  In summary, the probabilityof a model, given a set of training examples is:P(M, I So .
.
.
s , )  =\[2- U#{M' )+'P{ M' D +C\].H~.=O PS(s~IM.)P(so.
.
.s .
)(7)Mode l  Const ruct ionFor lack of space, we only sketch our model construc-tion strategy.
In brief, our approach is (largely) mono-tonic, in that we extend an old model with new rulesconstructed from pieces of manually written rules, suchthat the new and old rules result in a parse for the cur-rent sentence (and the old rules alone fail to parse thecurrent sentence).
In more detail, whenever we fail toparse a sentence with the manually written DCG, weuse an optimised chart parser \[10\] to construct all lo-cal trees licensed by the manually written grammar.
Wenext consider ways adjacent local trees licensed by man-ually written DCG rules may be combined into largerlocal trees (ie invent rules whose right hand side con-sists of categories that spell out the mother categories ofthese adjacent local trees; the left hand side will be oneof these right-hand side categories, with the possibilityof having its bar level raised).
The parser packs all lo-cal trees in space polynomial with respect o sentencelength.
If, within self-imposed bounds upon compu-tation (for example, limiting the number of local treesjoined together), we succeed in constructing at least onetree that spans the entire sentence, we can build a newmodel by extracting all new rules seen in that tree andadding them to the old model.Note that periodically, it is useful to prune (and re-normalise) the model of rules seen only once in the pre-viously encountered sequence of sentences.
Such prunedrules are likely to have arisen either due to marked con-structions, noise in the training material, or rules thatappeared to be promising, but did not receive any sub-sequent support and as such have little predictive util-it.v.
Pruning the model is a non-monotonic operationand hard to formally justify, but nevertheless useful.Whitten, Cleary and Bell also comment upon the use-fulness of resetting the model \[3\].Our model construction approach as the followingproperties: rules constructed all encode a version ofX-Syntax, which weakly constrains the space of pos-sible rules \[8, 21\]; analyses produced using manuallywritten rules are favoured over those produced usinglearnt rules (by virtue of computation being resource-bounded): this mirrors the empirical fact that whenextending manually written grammars, only a few.
rulesare necessary, and those required generally 'join' to-gether local trees generated by manually written rules.64SearchModel construction may produce an exponential num-ber of parses for a sentence and for computational rea-sons, we are unable to evaluate all the models encodedwithin these parses.
We therefore use a probabilisticunpacking strategy that efficiently selects the n mostlikely parses, where n is much less thml the total num-ber of parses possible for some sentence \[11\].
There isinsufficient space here to describe how we rank parses,but the underlying parse selection model is based uponthe SCFG used to evaluate models.
Currently.
it is notlexicalised, so parse selection performance is subject othe well-known limitations of non-lexicalised SCFGs \[5\].Whilst estimating models, we simultaneously estimatethe parse selection model in terms of tile parse used toproduce the model picked.
Probabilistic unpacking iscrucial to our approach, and it is this that makes ourlearner computationally feasible.After extracting n parses, we can then go on to con-struct k I models, evaluate their posterior probabilities,and then select the model that maximises this term.However, as was shown by P-t-S, when training materialconsists of just (limited quantities) of raw text.
classi-cal, single model parameter estimation often results in amodel that produces worse parse selection results thanwhen the estimation process is constrained to only con-sider derivations compatible with the parsecl corpora.In our context, we can use parsed corpora to constrainboth parameter estimation and also model selection.We simply re-rank the n parses produced uring modelconstruction using a tree similarity metric that com-pares how 'close' an automatically constructed parseis to a manually written parse, and take the q parsesthat all minimise the metric and are all scored equallywell \[17\].
From these q parses we can then build mod-els as usual.
When q = 1, there is no need to relyupon MDL-based model selection.
Otherwise.
when qis greater than one, we have a set of parses, all equallyconsistent with the manually created tree, and so fall-back upon the usual model selection strate~.v.
Our useof parsed corpora differs from P+S's in that we use it asa soft constraint: we may still keep parses even if theyviolate constraints in the manually constructed parsetree.
The reason for this decision is that we do notconstruct all possible parses for a sentence., and so attimes may not produce a parse consistent with a manu-ally created parse.
Also, it is not clear whether parsedcorpora is sufficiently reliable for it to be trusted abso-lutely.
Clearly there will be a link between the amountof information present in the parsed corpora mid thequality of the estimated model.
In the experimental~k may be less than or equal to n. depending upon whichindependence assumptions are made by" the modelsection of this paper, we consider this issue.EstimationWhen computing a model's posterior probability, weestimate the description length of features, P(v \] f,),the model parameters, P(A -+ a I A) and the likelihoodprobability, P(so... sn \]Mi)-The feature description length is estimated by count-ing the number of times a given feature takes ome valuein some category position, and applying amaximal like-lihood estimator to arrive at a probability.
The modelparameters are estimated by counting the number oftimes a given backbone rule was seen in the previousn parses just produced, and then again using a maxi-mal likelihood estimator to produce a probability.
Weestimate, as we cannot afford to globally recompute,the likelihood probability using the following approxi-mations.
Only a fixed number of n previously seen sen-tences that cannot be parsed using the manually writ-ten rules are considered in the likelihood computation.We assume that the parses of these sentences remainsconstant across alternative models, but the derivationprobabilities might vary.
We also assume that the stringprobability of each sentence is reasonably well approx-imated by a single parse.4 The  GrammarThe grammar we extend with learning, (called the TagSequence Grammar \[7\], or TSG for short) was developedwith regard to coverage, and when compiled consists of455 object rules.
It does not parse sequences of wordsdirectly, but instead assigns derivations to sequencesof part-of-speech tags (using the CLAWS2 tagset \[4\]).The grammar is relatively shallow, (for example, it doesnot fully analyse unbounded ependencies) but it doesmake an attempt o deal with common constructions,such as dates or names, commonly found in corpora, butof little theoretical interest.
Furthermore, it integratesinto the syntax a text grammar, grouping utterancesinto units that reduce the overall ambiguity.For the experiments reported here, we manually ex-tended TSG with four extra rules.
These extra rulesdealt with obvious oversights when parsing the WSJ.5 Re la ted  WorkOur approach isclosely related to Stolcke's model merg-ing work \[29\].
Apart from differences inprior and likeli-hood computation, the main divergence is that our workis motivated by the need to deal with undergenerationin broad-coverage, manually written natural anguagegrammars (for example \[15\]).
Although we do not gointo the issues here, estimation of rules missing fromsuch g-rammars i  different from estimating rammars65ab initio.
This is because rules missing from any real-istic grammar are all likely to have a low frequency inany given corpus, and so will be harder to differenti-ate from competing, incorrect rules purely on the basisof statistical properties alone.
We know of no otherwork reporting automated extension of broad-coveragegrammars using MDL and parsed corpora.One of the anonymous reviewers wanted to know howour work related to Explanation-Based Learning (EBL)\[20\].
EBL is not concerned with induction of rules: itdeals with finding more efficient ways to use existingrules.
For example, in NLP, EBL has been used to re-duce the time taken to parse sentences with large gram-mars (\[28\]).
EBL does not extend the coverage of anygiven grammar, unlike our approach.
In our opinion,it would be better to view our learner as all InductiveLogic Programming system specialised for DCG induc-tion.6 Exper imentsFor our experiments we used material supplied bythe CoNLL99 workshop organisers.
This consisted of48,224 fully parsed training sentences, and a disjointset of 984 testing sentences.
Both sets were randomlydrawn from the parsed section of the Wall Street Jour-nal.
The test set came in two versions, differing fromeach other in how the sentences were marked:up.
Thefirst version consisted of sentences with NP bracketingsmarked (results using this test set are given in table 1).The second version had NP bracketings marked, andwithin each marked NP, there was an internal parse(results for this version are in table 2).
These parseswere labelled with Penn Nonterminals.
Each test sen-tence was trivially rooted with an S symbol (necessaryfor the evaluation software).
To make this clearer, if anoriginal Wall Street Journal parse, stripped of tags andnonterminal decorations was:(S (NP (NP Bailey Controls), (VP based (PP in(NP (NP Wickliffe), (NP Ohio)))).)
(VP makes(NP computerized industrial controls ystcms)).
)the version containing just NP bracketing would be:(S (NP (NP Bailey Controls), based in (NP (NPWickliffe), (NP Ohio)),) makes (NP computerizedindustrial controls ystems) .whilst the version containing parsed NPs would be:(S (NP (NP Bailey Controls), (VP based (PP in(NP (NP Wickliffe), (NP Ohio)))),) makes (NPcomputerized industrial controls ystems).
)For computational reasons, we could not deal withall sentences in the training set.
and when learning!rules, we limited ourselves to ser/tences with a maxi-mum length of 15 tokens.
During evaluation, we usedsentences with a maximum length of 30 tokens.
Thisreduced the training set to 10,249 parsed sentences, andthe test set to 739 sentences.
Finally, we retagged theCoNLL99 material with the Claws2 tagset (required byTSG).
Evaluation was carried out by Parseval (whichreports unlabelled bracketing results: we do not reportlabelled results as TSG does not use the Penn Non-terminal set) \[16\].
Note that evaluation is based uponbracketing, and not word accuracy.
For example, if wefailed to include one word in a NP that contains fourother words, we would have a bracketing accuracy of0.
On the other hand, a word accuracy result would be80%.Asa comparison, we evaluated TSG upon the testingmaterial.
This is experiment 5 in tables 1 and 2.
Theother four experiments differed from each other in termsof what the learner was trained upon:1.
Just tagged sentences.2.
Tagged Sentences with NP bracketings marked.
Wereduced WSJ parses to include just their NP brack-etings.3.
Tagged sentences with NPs bracketing annotatedwith an internal parse.
Again, we mapped WSJparses to reduced parses containing just annotatedNPs.4.
Tagged sentences with a full Wall Street Journalparse.For each experiment, we report the size of the finalgrammar, the percentage of testing sentences covered(assigned a full parse), crossing rates, recall and preci-sion results with respect o testing sentences with NPsbracketed and those containing annotated NPs.
For thebracketing task, we mapped full parses, produced bynmdels, to parses just containing NP bracketing.
Forthe aimotation task.
we mapped full parses to parsescontaining just NPs with an internal annotation.
Notethat within our grammatical framework, the best map-ping is not clear (since parses produced by our modelshave categories using multiple bar levels, whilst WSJparses ouly use a single level).
As a guess, we treatedbar 1 and bar 2 nominal categories as being NPs.
Thismeans that our precision results are lowered, since ingeneral, we produce more NPs than would be predictedby a WSJ parse.In each case.
we evaluate the performance of a modelin terms of the highest ranked parse, and secondly, interms of the "best' parse, out of the top 10 parses pro-duced.
Here "best' means the parse produced that isclosest, in terms of a weighted sum crossing rates, pre-cision and recall, to the manually selected parse.
Thisfinal set of results gives an indication of how well oursystem would perform if it had a much better parseselection mechanism.
Best figures are marked in paren-theses.Figure 1 gives our results for the bracketing task,whilst figure 2 gives our results for the annotation task.Model size and coverage results were id~-ltical for bothtests, so the second table omits them.Exp Size % Gen CR R P1 2687 78 1.27 (1.02) 66.5 (73.9) 51.5 (55.312 2774 91 1.34 (0.99) 64.6 (73.5) 50.8 (55.6:3 2782 91 1.29 (0.97) 64.6 (73 5) 50.9 (55.6:4 2592 90 1.34 (1.01) 64.8 (73.2) 50.5 (55.3:5 459 63 1.2 (0.95) 68.7 (75.8) 53 2 (56.9:Figure 1: NP Identification Results (Bracketing)Exp CR R P1 2.08 (1.52) 57.99 (67.7) 48.1 (54.6)2 2.27 (1.54) 56.5 (67.0) 46.0 (54.2)3 2.22 (1.52) 56.5 (66.9) 50.9 (54.2)4 2.32 (1.63) 56.5 (66.7) 45.4 (53.1)5 1.85 (1.40) 59.2 (69.1) 51.4 (57.2)Figure 2: NP Identification Results (Annotation)Firstly, when compared with other work on NP re-covery, our results are poor.
As was mentioned in thesearch section, this is largely due to our system beingbased upon a language model that has well known lim-itations.
Furthermore, as was argued in the iutroduc-tion, full NPs are by definition harder to identify thanbase NPs, so we would expect our results to be worse.Secondly, we see that the bracketing task is easier thanthe annotation task: generally, the results in table 1 arebetter than the results in table 2.
Given the fact thatthe annotation search space is larger than the brack-eting space, this should come as no surprise.
Turningnow to the individual experiments, we see that parsedcorpora (experiments 2, 3 and 4) is all informative con-straint upon NP induction.
Rules learnt using parsedcorpora better capture regularities than do rules learntfrom just raw text (experiment 1).
This is shown by theincreased coverage results of experiments 2.3 and 4 over1.
In terms of crossing rates, recall and precision, noclear story has emerged.
Surprisingly, there seems to beminimal difference in coverage when using either anno-tated NPs or full parses.
This could be due to a numberof reasons, such as WSJ NPs being more reliably an-notated than other phrases, simple artifactual problems66with the learner, the evaluation metrics being too coarseto show any real differences, etc.
Further, qualitativeinvestigation should determine whether there are anydifferences in the parses that TSG alone cannot assignto sentences.Due to time constraints, we did not measure statisti-cal significance tests between the various experiments.A later version of this paper (available from the author,osborne@let.rug.nl) will report these tests.7 ConclusionWe presented an MDL-based incremental DCG learner.Experimental evaluation showed that estimation is pos-sible using just raw sentences, but that better resultsare possible when additional parsed corpora is used.Evaluation also showed that this parsed corpora neednot be that detailed, and that NP bracketing informa-tion produced similar results to using full WSJ parses.This final results eems counterintuitive, and merits fur-ther investigation.Future work on the learner will be in three main di-rections:?
Abandonment of the SCFG as the basis of the lan-guage model.
We are considering either Abney'srandom fields \[1\] or Goodman's Probabilistic Fea-ture Grammmars \[14\] as a replacement.
Apart fromperformance improvements, altering the model classshould allow empirical investigation of the MDLclaim that model classes can be evaluated in termsof compression.
So, if we discover even more com-pact models using (say) Goodman's scheme than wecould using our SCFG, we might deduce that this isthe case.
Naturally: lexicalisation would enter intoany scheme ntertained.?
Use of semantics in estimation.
We have at our dis-posal a large grammar augmented with a composi-tional semantics \[15\].
Again, this should lead to bet-ter results.?
Prior we!ghting.
As is well known, MDL-based learn-ers sometimes improve from weighting the prior withrespect o tile likelihood.
Schemes, such as Quinlanand Rivest's \[24\], fall outside of the coding frameworkand (effectively) replicate the training set.
We intendto pursue encoding-based schemes that achieve the.same purpose.AcknowledgmentsWe would like to thank Erik Tjong Kim Sang for helpwith organising CoNLL99 and the two anonymous re-viewers for commenting on the paper.
This work wassupported by the EU Project Sparkle LE-2111 m~d tileTMR Project Learning Computational Grammars.References\[1\] Steven P. Abney.
Stochastic Attribute-ValueGrammars.
Computational Linguistics, 23(4):597-618, December 1997.\[2\] Shlomo Argamon, Ido Dagan, and Yuval Kry-molowski.
A Memory-Based Approach toLearning Shallow Natural Language Patterns.In Proceedings of the 17 Ih bzternational Con-ference on Computational Linguistics, 1998.ht tp: / /xxx.lanl.gov / ps/ cmp-lg\[ 9806011.\[3\] Timothy C. Bell, John G. Cleary, and Ian H. Wit-ten.
Text Compression.
Advanced Reference Se-ries.
Prentice Hall, 1990.\[4\] Ezra Black, Roger Garside, and Geoffrey Leech,editors.
Statistically driven computer grammarsof English the IBM-Lancaster approach.
Rodopi,1993.\[5\] Ezra Black, Fred Jelinek.
Jolm Lafferty, andDavid M. Magerman.
Towards History-basedGrammars: Using Richer Models for Probabilis-tic Parsing.
In 31 st Annual Meeting of the Asso-ciation for Computational Lmguistzcs, pages 31-37, Ohio State University, Columbus, Ohio, USA.June 1993.\[6\] T. Booth.
Probabilistie representation f formallanguages.
In Tenth Annual IEEE Symposzum onSwitching and Automata Theory, October 1969.\[7\] Ted Briscoe and John Carroll.
Automatic Extrac-tion of Subcategorization from Corpora.
In Pro-ceedings of the 5 th Conference on Applied NLP.pages 356-363, Washington, DC, 1996.\[8\] Ted Briscoe and Nick Waegner.
RobustStochastic Parsing Using the Inside-Outside Algo-rithm.
In Proceedings of the AAAI Workshop onStatistically-based Techniques in Natural LanguageProcessing, 1992.\[9\] Claire Cardie and David Pierce.
Error-DrivenPruning of Treebank Grammars for Base NounPhrase Identificatio.
In Proceedings of the 17 th In-ternational Conference on Computational Linguis-tics, pages 218-224, 1998.\[10\] John Carroll.
Practical Unification-based Parsingof Natural Language.
PhD thesis.
University ofCambridge, March 1993.\[11\] John Carroll and Ted Briscoe.
Probabilistic nor-malisation and unpacking of packed parse forestsfor unification-based grammars.
In Proceedings of67IIIIIIIIIIIIIIIIIII\[12\]\[13\]\[14\]\[15\]\[16\]\[17\]\[18\]\[191\[201\[21\]the AAAI  Fall Symposium on Probabilistic Ap-proaches to Natural Language, pages 33-38.
Cam-bridge, MA, 1992.Carl de Marcken.
Unsupervised Language Acquisi-tion.
PhD thesis, MIT, 1996.E.
M. Gold.
Language Identification to the Limit.Information and Control, 10:447-474, 1967.Joshua Goodman.
Probabilistic Feature Gram-mars.
In 5 th International Workshop on Pars-ing Technologies, MIT, Cambridge, Massachusetts,USA, September 1997.Claire Grover, Ted Briscoe, John Carroll, andBran Boguraev.
The Alvey Natural Language ToolsGrammar (4 ta Release).
Technical report, Univer-sity of Cambridge Computer Laboratory, 1993.Philip Harrison, Steven Abney, Ezra Black, DanFlickinger, Ralph Grishman Claudia Gdaniec,Donald Hindle, Robert Ingria, Mitch Marcus,Beatrice Santorini, and Tomek Strzalkowski.
Eval-uating Syntax Performance of Parser/Grammarsof English.
In Jeannette G. Neal and Sharon M.Walter, editors, Natural Language Processing Sys-tems Evaluation Workshop, Technical Report RL-TR-91-362, 1991.Eirik ttektoen.
Probabilistic Parse Selection Basedon Semantic Cooccurrences.
In 5 th InternationalWorkshop on Parsing Technologies.
pages 113-122,MIT.
Cambridge, Massachusetts.
USA.
September1997.Bill Keller and Riidi Lutz.
Evolving StochasticContext-Free Grammars from Examples Using aMinimum Description Length Principle.
In Work-sop on Automata Induction, Grammatical Infer-ence and Language Acquisition, Nashville, Ten-nessee, USA, July 1997.
ICML097.K.
Lari and S. J.
Young.
The estimation of stochas-tic context-free grammars using the Inside-OutsideAlgorithm.
Computer Speech and Language, 4:35-56, 1990.T.
Mitchell, R. Keller, and S. Kedar-Cabelli.Explanation-based generalization: A unifyingview.
Machine Learning, 1.1:47-80, 1986.Miles Osborne and Derek Bridge.
Learningunification-based grammars using the Spoken En-glish Corpus.
In Grammatical Inference and Ap-plications, pages 260-270.
Springer Verlag.
1994.68\[22\] Miles Osborne and Ted Briscoe.
Learning Stochas-tic Categorial Grammars.
In T. Mark Ellison, ed-itor, CoNLL97, pages 80-87.
ACL, July 1997.\[23\] Fernando Pereira and Yves Schabes.
Inside-outside reestimation from partially bracketed cor-pora.
In Proceedings of the 30 th ACL.
University ofDelaware, Newark, Delaware, pages 128-135, 1992.\[24\] J. Ross Quinlan and Ronald L. Rivest.
Infer-ring decision trees using the minimtml descriptionlength principle.
Information and Computation,80:227-248, 1989.\[25\] Lance A. Rarnshaw and Mitchell P. Marcus.
TextChunking Using Transformation-Based L arning,.Ill Proceedings of the 3 "a ACL Workshop on VeryLarge Corpora, pages 82-94, June 1995.\[26\] Jorma Rissanen.
Stochastw Complexity in Statis-tical Inquiry.
Series in Computer Science -Volume15.
World Scientifc, 1989.\[27\] Jorma Rissanen and Eric Sven Ristad.
LanguageAcquisition in the MDL Framework.
In Eric SvenRistad, editor, Language Computation.
AmericanMathemtatical Society, Philedelphia, 1994.\[28\] Christer Samuelsson and Manny Rayner.
Quan-tiative Evaluation of Explanation-Based Learningas an Optimisation Tool for a Large-Scale Natu-ral Language System.
In Proceedings o/the 12 thInternational Joint Conference on Artificzal Intel-ligence, pages 609-615, 1991.\[29\] Andreas Stolcke and Stephen Omohundro.
Induc-ing Probabilistic Grammars by Baye~sian ModelMerging.
In Grammatical Inference and Applica-tions, pages 106-118.
Springer Verlag.
1994.
