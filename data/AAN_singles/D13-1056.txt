Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 590?600,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsSemi-Markov Phrase-based Monolingual AlignmentXuchen Yao and Benjamin Van DurmeJohns Hopkins UniversityBaltimore, MD, USAChris Callison-Burch?University of PennsylvaniaPhiladelphia, PA, USAPeter ClarkAllen Institute for Artificial IntelligenceSeattle, WA, USAAbstractWe introduce a novel discriminative model forphrase-based monolingual alignment using asemi-Markov CRF.
Our model achieves state-of-the-art alignment accuracy on two phrase-based alignment datasets (RTE and para-phrase), while doing significantly better thanother strong baselines in both non-identicalalignment and phrase-only alignment.
Addi-tional experiments highlight the potential ben-efit of our alignment model to RTE, para-phrase identification and question answering,where even a naive application of our model?salignment score approaches the state of the art.1 IntroductionVarious NLP tasks can be treated as an alignmentproblem: machine translation (aligning words in onelanguage with words in another language), ques-tion answering (aligning question words with the an-swer phrase), textual entailment recognition (align-ing premise with hypothesis), paraphrase detection(aligning semantically equivalent words), etc.
Eventhough most of these tasks involve only a single lan-guage, alignment research has primarily focused onthe bilingual setting (i.e., machine translation) ratherthan monolingual.
Moreover, most work has con-sidered token-based approaches over phrase-based.1Here we seek to address this imbalance by proposingbetter phrase-based models for monolingual wordalignment.
?Performed while faculty at Johns Hopkins University.1In this paper we use the term token-based alignment forone-to-one alignment and phrase-based for non one-to-onealignment, and word alignment in general for both.Most token-based alignment models can extrin-sically handle phrase-based alignment to some ex-tent.
For instance, in the case of NYC align-ing to New York City, the single source wordNYC may align three times separately to the tar-get words: NYC?New, NYC?York, NYC?City.Or in the case of identical alignment, New YorkCity aligning to New York City is simplyNew?New, York?York, City?City.
How-ever, it is not as clear how to token-align New York(as a city) with New York City.
The problem ismore prominent when aligning phrasal paraphrasesor multiword expressions, such as pass away andkick the bucket.
This suggests an intrinsi-cally phrase-based alignment model.The token aligner jacana-align (Yao et al 2013a)has achieved state-of-the-art result on the task ofmonolingual alignment, based on previous work ofBlunsom and Cohn (2006).
It employs a ConditionalRandom Field (Lafferty et al 2001) to align tokensfrom the source sentence to tokens in the target sen-tence, by treating source tokens as ?observation?
andtarget tokens as ?hidden states?.
However, it is notdesigned to handle phrase-based alignment, largelydue to the Markov nature of the underlying model:a state can only span one token each time, makingit unable to align multiple consecutive tokens (i.e.
aphrase).
We extend this model by introducing semi-Markov states for phrase-based alignment: a statecan instead span multiple consecutive time steps,thus aligning phrases on the source side.
Also, wemerge phrases on the target side to phrasal states,allowing the model to align phrases on the targetside as well.
We evaluate the resulting semi-Markov590CRF model on the task of phrase-based alignment,and then show a basic application in the NLP tasksof recognizing textual entailment, paraphrase iden-tification, and question answering sentence ranking.The final phrase-based aligner is open-source.22 Related WorkMost work in monolingual alignment employs de-pendency tree/graph matching algorithms, includ-ing tree edit distance (Punyakanok et al 2004;Kouylekov and Magnini, 2005; Heilman and Smith,2010; Yao et al 2013b), Particle Swarm Optimiza-tion (Mehdad, 2009), linear regression/classificationmodels (Chambers et al 2007; Wang and Manning,2010), and min-cut (Roth and Frank, 2012).
Theseworks inherently only support token-based align-ment, with phrase-like alignment achieved by firstmerging tokens to phrases as a preprocessing step.The MANLI aligner (MacCartney et al 2008)and its derivations (Thadani and McKeown, 2011;Thadani et al 2012) are the first known phrase-based aligners specifically designed for aligning En-glish sentence pairs.
It applies discriminative per-ceptron learning with various features and handlesphrase-based alignment of arbitrary phrase lengths.MANLI suffers from slow decoding time due to itslarge search space.
This was optimized by Thadaniand McKeown (2011) through Integer Linear Pro-gramming (ILP), where benefiting from modern ILPsolvers they showed an order-of-magnitude speedupin decoding.
Also, various syntactic constraints canbe easily added, significantly improving exact align-ment match rate for whole sentence pairs.
Besidesthe common application of textual entailment andquestion answering, monolingual alignment has alsobeen applied in the field of text generation (Barzilayand Lee, 2003; Pang et al 2003).Word alignment has been more explored in ma-chine translation.
The IBM models (Brown et al1993) allow many-to-one alignment and are essen-tially asymmetric.
Phrase-based MT historicallyrelied on heuristics (Koehn, 2010) to merge twosets of word alignment in opposite directions toyield phrasal alignment.
Later, researchers explorednon-heuristic phrase-based methods.
Among them,Marcu and Wong (2002) described a joint proba-2http://code.google.com/p/jacana/bility model that generates both the source and tar-get sentences simultaneously.
All possible pairs ofphrases in both sentences are enumerated and thenpruned with statistical evidence.
Deng and Byrne(2008) explored token-to-phrase alignment basedon HMM models (Vogel et al 1996) by explic-itly modeling the token-to-phrase probability andphrase lengths.
However, the token-to-phrase align-ment is only in one direction: each target state stillonly spans one source word, and thus alignment onthe source side is limited to tokens.
Andre?s-Ferrerand Juan (2009) extended the HMM-based methodto Hidden Semi-Markov Models (HSMM) (Osten-dorf et al 1996), allowing phrasal alignments onthe source side.
Finally, Bansal et al(2011) unifiedthe HSMM models with the alignment by agreementframework (Liang et al 2006), achieving phrasalalignment that agreed in both directions.Despite successful usage of generative semi-Markov models in bilingual alignment, this has notbeen followed with models in discriminative mono-lingual alignment.
Essentially monolingual align-ment would benefit more from discriminative mod-els with various feature extractions (just like thosedefined in MANLI) than generative models withoutany predefined feature (just like how they were usedin bilingual alignment).
To combine the strengths ofboth semi-Markov models and discriminative train-ing, we propose to use the semi-Markov ConditionalRandom Field (Sarawagi and Cohen, 2004), whichwas first used in information extraction to tag con-tinuous segments of input sequences and outper-formed conventional CRFs in the task of named en-tity recognition.
We describe this model in the fol-lowing section.3 The Alignment ModelOur objective is to define a model that supportsphrase-based alignment of arbitrary phrase length.In this section we first describe a regular CRFmodel that supports one-to-one token-based align-ment (Blunsom and Cohn, 2006; Yao et al 2013a),then extend it to phrase-based alignment with thesemi-Markov model.5913.1 Token-based ModelGiven a source sentence s of length M , and a targetsentence t of lengthN , the alignment from s to t is asequence of target word indices a, where ai?
[1,M ] ?
[0, N ].
We specify that when ai = 0, source word siis aligned to a NULL state, i.e., deleted.
This modelsa many-to-one alignment from source to target: mul-tiple source words can be aligned to the same targetword, but not vice versa.
One-to-many alignmentcan be obtained by running the aligner in the otherdirection.
The probability of alignment sequence aconditioned on both s and t is then:p(a | s, t) =exp(?i,k ?kfk(ai?1, ai, s, t))Z(s, t)This assumes a first-order Conditional RandomField (Lafferty et al 2001).
Since the word align-ment task is evaluated over F1, instead of directlyoptimizing it, we choose a much easier objective(Gimpel and Smith, 2010) and add a cost functionto the normalizing function Z(s, t) in the denomi-nator:Z(s, t) =?a?exp(?i,k?kfk(a?i?1, a?i, s, t)+cost(ay, a?
))where ay is the true alignments.
cost(ay, a?)
canbe viewed as special ?features?
that encourage de-coding to be consistent with true labels.
It is onlycomputed during training in the denominator be-cause in the numerator cost(ay,ay) = 0.
Ham-ming cost is used in practice without learning theweights (i.e., uniform weights).
The more inconsis-tence there is between ay and a?, the more penalizedis the decoding sequence a?
through the cost func-tion.3.2 Phrase-based ModelThe token-based model supports 1 : 1 alignment.We first extend it in the direction of ls : 1, wherea target state spans ls words on the source side (lssource words align to 1 target word).
Then we ex-tend it in the direction of 1 : lt, where lt is the tar-get phrase length a source word aligns to (1 sourceword aligns to lt target words).
The final combinedshops areShops closed up for now until MarchNULLclosedtemp.areShopsdownshops-are...-... 7..140123456closed-down 15Figure 1: A semi-Markov phrase-based modelexample and the desired Viterbi decoding path.Shaded horizontal circles represent the sourcesentence (Shops are closed up for nowuntil March) and hollow vertical circles repre-sent the hidden states with state IDs for the targetsentence (Shops are temporarily closeddown).
State 0, a NULL state, is designated for dele-tion.
One state (e.g.
state 3 and 15) can span multi-ple consecutive source words (a semi-Markov prop-erty) for aligning phrases on the source side.
Stateswith an ID larger than the target sentence lengthindicate ?phrasal states?
(states 6-15 in this exam-ple), where consecutive target tokens are merged foraligning phrases on the target side.
Combining thesemi-Markov property and phrasal states yields forinstance, a 2?2 alignment between closed up inthe source and closed down in the target.model supports ls : lt alignment.
Throughout thissection we use Figure 1 as an illustrative example,which shows phrasal alignment between the sourcesentence: (Shops are closed up for nowuntil March) and the target sentence: (Shopsare temporarily closed down).1 : 1 alignment is a special case of ls : 1 align-ment where the target side state spans ls = 1 sourceword, i.e., at each time step i, the source side word592si aligns to one state ai and the next aligned stateai+1 only depends on the current state ai.
This isthe Markovian property of the CRF.
When ls > 1,during the time frame [i, i + ls), all source words[ai, ai+ls) share the same state ai.
Or in other words,the state ai ?spans?
the following ls time steps.
TheMarkovian property still holds ?outside?
the timeframe ls, i.e., ai+ls still only depends on ai, the pre-vious state ls time steps ago.
But ?within?
the timeframe ls, the Markovian property does not hold anymore: [ai, ..., ai+ls?1] are essentially the same stateai.
This is the semi-Markov property .
States can bedistinguished by this property into two types: semi-Markovian states and Markovian states.We have generalized the regular CRF to a semi-Markov CRF.
Now we define it by generalizing thefeature function:p(a | s, t) =exp(?i,k,ls ?kfk(ai?ls , ai, s, t))Z(s, t)At time i, the k-th feature function fk mainlyextracts features from the pair of source words(si?ls , ..., si] and target word tai (still with a spe-cial case that ai = 0 marks for deletion).
Inferenceis still Viterbi-like: except for the fact during maxi-mization, the Viterbi algorithm not only checks theprevious one time step, but all ls time steps.
Sup-pose the allowed maximal source phrase length isLs, define Vi(a | s, t) as the highest score along thedecoding path until time i ending with state a:Vi(a | s, t) = maxa1,a2,...ai?1p(a1, a2, .
.
.
, ai = a | s, t)then the recursive maximization is:Vi(a | s, t) = maxa?maxls=1...Ls[Vi?ls(a?| s, t)+?i(a?, a, ls, s, t)]with factor:?i(a?, a, ls, s, t) =?k?kfk(a?i?ls , ai, s, t)and the best alignment a can be obtained by back-tracking the last state aM from VM (aM | s, t).Training a semi-Markov CRF is very similar tothe inference, except for replacing maximizationwith summation.
The forward-backward algorithmshould also be used to dynamically compute the nor-malization function Z(s, t).
Compared to regularCRFs, a semi-Markov CRF has a decoding timecomplexity of O(LsMN2), a constant factor Ls(usually 3 or 4) slower.To extend from 1 : 1 alignment to 1 : lt alignmentwith one source word aligning to lt target words,we simply explode the state space by Lt times withLt the maximal allowed target phrase length.
Thusthe states can be represented as an N ?
Lt ma-trix.
The state at (j, lt) represents the target phrase[tj , ..., tj+lt).
In this paper we distinguish states bythree types: NULL state (j = 0, lt = 0), token state(lt = 1) and phrasal state (lt > 1).To efficiently store and compute these states, welinearize the two dimensional matrix with a linearfunction mapping uniquely between the state ID andthe target phrase offset/span.
Suppose the targetphrase tj of length ltj ?
[1, Lt] holds a positionptj ?
[1, N ], and the source word si is aligned tothis state (ptj , ltj ), a tuple for (position, span).
Thenstate ID asi is computed as:asi(ptj , ltj ) ={ptj ltj = 1N + (ptj ?
1)?
Lt + ltj 1 < ltj ?
LtAssume in Figure 1, Lt = 2, then the state ID forthe phrasal state (5, 2) closed-down with ptj = 5for the position of word down and ltj = 2 for thespan of 2 words (looking ?backward?
from the worddown) is: 5 + (5?
1)?
2 + 2 = 15.Similarly, given a state id asi , the original targetphrase position and length can be recovered throughinteger division and modulation.
Thus during decod-ing, if one output state is 15, we would know that ituniquely comes from the phrasal state (5,2), repre-senting the target phrase closed down.This two dimensional definition of state space ex-pands the number of states from 1 + N to 1 +LtN .
Thus the decoding complexity becomesO(M(LtN)2) = O(L2tMN2) with a usual valueof 3 or 4 for Lt.Now we have defined separately the ls : 1 modeland the 1 : lt model.
We can simply merge them to593have an ls : lt alignment model.
The semi-Markovproperty makes it possible for any target states toalign phrases on the source side, while the two di-mensional state mapping makes it possible for anysource words to align phrases on the target side.
Forinstance, in Figure 1, the phrasal state a15 repre-sents the two-word phrase closed down on thetarget side, while still spanning for two words on thesource side, allowing a 2?
2 alignment.
State a15 isphrasal, and at source word position 3 and 4 (span-ning closed up) it is semi-Markovian.
The finaldecoding complexity is O(LsL2tMN2), a factor of30 ?
60 times slower than the token-based model(with a typical value of 3 or 4 for Ls and Lt).In the following we describe features.3.3 Feature DesignWe reused features in the original token-basedmodel based on string similarity, POS tags, position,WordNet, distortion and context.
Then we used anadditional chunker to mark phrase boundaries onlyfor feature extraction:Chunking Features are binary indicators ofwhether the phrase types of two phrases match.Also, we added indicators for mappings betweensource phrase types and target phrase types, such as?vp2np?, meaning that a verb phrase in the source ismapped to a noun phrase in the target.Moreover, we introduced the following lexicalfeatures:PPDB Features (Ganitkevitch et al 2013) in-clude various similarity scores derived from a para-phrase database with 73 million phrasal and 8 mil-lion lexical paraphrases.
Various paraphrase condi-tional probability was employed.
For instance, forthe ADJP/VP phrase pair capable of and ableto, there are the following minus-log probabilities:p(lhs|e1) = 0.1, p(lhs|e2) = 0.3, p(e1|lhs) = 5.0p(e1|e2) = 1.3, p(e2|lhs) = 6.7, p(e2|e1) = 2.8p(e1|e2, lhs) = 0.6, p(e2|e1, lhs) = 2.3where e1/e2 are the phrase pair, and lhs is theleft hand side syntactic non-terminal symbol.
Wedid not use the syntactic part (e.g., the NP ofNNS ?
the NNS of NP) of PPDB as we did notmake the assumption that the input sentence pairswere well-formed (and newswire-like) English, oreven of a language with a parser available.
Also, forphrasal alignments, we ruled out those paraphrasesspanning multiple syntactic structures, or of differ-ent syntactic structures (indicated as [X] in PPDB),for instance, and crazy?
, mad.Semantic Relatedness Feature is a single scalednumber in [0, 1] from the best performing system(Han et al 2013) of the *Sem 2013 Semantic Tex-tual Similarity (STS) task.
We included this fea-ture mainly to deal with cases where ?related?
wordscannot be well measured by either paraphrases ordistributional similarities.
For instance, in one align-ment dataset annotators aligned married withwife.
Adding a few other words as comparison, theHan et al(2013) system gives the following similar-ity scores:married/wife: 0.85married/husband: 0.84married/child: 0.10married/stone: 0.01Name Phylogeny Feature (Andrews et al 2012)is a similarity feature with a string transducer tomodel how one name evolves to another.
Examplesbelow show how similar is the name Bill associ-ated with other names in log probability:Bill/Bill: -0.8Bill/Billy: -5.2Bill/William: -13.6Bill/Mary: -18.6Finally, one decision we made during featuredesign was not to use any parsing-based features,with a permissive assumption that the input mightnot be well-formed English, or even not completesentences (such as fragmented snippets from websearch).
The ?deepest?
linguistic processing stays atthe level of tagging and chunking, making the modelmore easily extendable to other languages.3.4 Feature ValueIn this phrase-based model, the width of a state spanover the source words depends on the competitionbetween features fired on the phrases as a whole vs.the consecutive but individual tokens.
We found itcritical to assign feature values ?fairly?
among to-kens and phrases to make sure that semi-Markovstates and phrasal states fire up often enough forphrasal alignments.594train test length %align.MSR06 800 800 29/11 36%Edinburgh++ 715 305 22/22 78%Table 1: Statistics of the two manually aligned cor-pora, divided into training and test in sentence pairs.The length column shows average lengths of sourceand target sentences in a pair.
%align.
is the per-centage of aligned tokens.To illustrate this in a simplified way, takeclosed up?closed down in Figure 1, and as-sume the only feature is the normalized number ofmatching tokens in the pair.
Then this feature firingon the following pairs would have values (the nor-malization factor is the maximal phrase length):closed?closed 1.0closed up?closed 0.5closed up?up 0.5closed up?closed down 0.5...?...
...The desired alignment closed up?closeddown would not have survived the state com-petition due to its weak feature value.
In thiscase the model would simply prefer a token align-ment closed?closed and up?...
(probablyNULL).Thus we upweighted feature values by the max-imum source or target phrase length to encour-age phrasal alignments, in this case closed up?closed down:1.0.
Then this alignment wouldhave a better chance to be picked out with additionalfeatures, such as with the PPDB and Semantic Relat-edness Features, which are also upweighted by max-imum phrase lengths.4 Experiment4.1 Data PreparationThere are two annotated datasets for training andtesting.
MSR063 (Brockett, 2007) has annotatedalignments on the 2006 PASCAL RTE2 develop-ment and test corpora, with 1600 pairs in total.3http://www.cs.biu.ac.il/?nlp/files/RTE_2006_Aligned.zip1x1 1x2 1x3 2x2 2x3 3x3 moreMSR06 89.2 1.9 0.3 5.7 0.0 1.9 0.8EDB++ 81.9 3.5 0.8 8.3 0.4 3.0 2.1Table 2: Percentage of various alignment sizes(undirectional, e.g., 1x2 and 2x1 are merged) af-ter synthesizing phrasal alignment from token align-ment in the training portion of two corpora.Semantically equivalent words and phrases in thepremise and hypothesis sentences are aligned in amanner analogous to alignments in statistical ma-chine translation.
This dataset is asymmetric: onaverage the premises contain 29 words and the hy-potheses 11 words.
Edinburgh++4 (Thadani et al2012) is a revised version of the Edinburgh para-phrase corpus(Cohn et al 2008) with sentencesfrom the following resources: 1. the Multiple-Translation Chinese corpus; 2.
Jules Verne?s novelTwenty Thousand Leagues Under the Sea.
3. theMicrosoft Research paraphrase corpus (Dolan et al2004).
The corpus is more balanced and symmetric:the source and target sentences are both 22 wordslong on average.
Table 1 shows some statistics.Both corpora contain mostly token-based align-ment.
For MSR06, MacCartney et al(2008) showedthat setting the allowable phrase size to be greaterthan one only increased F1 by 0.2%.
For Ed-inburgh++, the annotation guideline5 explicitly in-structs to ?prefer smaller alignments whenever pos-sible?.
Statistics shows that single token alignmentcounts 96% and 95% of total alignments in these twocorpora separately.
With such a heavy imbalance to-wards only token-based alignment, a phrase-basedaligner would learn feature weights that award tokenalignments more than phrasal alignments.Thus we synthesized phrasal alignments fromcontinuous monotonic token alignments in these twocorpora.
We first ran the OpenNLP chunker throughthe corpora.
Then for each phrase pair, if each tokenin the source phrase is aligned to a token in the tar-get phrase in a monotonic way, and vice versa, we4http://www.ling.ohio-state.edu/?scott/#edinburgh-plusplus5http://staffwww.dcs.shef.ac.uk/people/T.Cohn/paraphrase_guidelines.pdf595merge these alignments to form one single phrasalalignment.6 Table 2 lists the percentage of vari-ous alignment sizes after the merge.
Two obser-vations can be made: first, the portion of phrasalalignments increases to 10% ?
20% after merging;second, allowing a maximal phrase length of 3 cov-ers 98% ?
99% of total alignments, thus a phraselength larger than 3 would be a bad trade-off for cov-erage vs speed.4.2 Baselines and Evaluation MetricsMacCartney et al(2008) and Yao et al(2013a)showed that the traditional MT bilingual alignerGIZA++ (Och and Ney, 2003) presented weak re-sults on the task of monolingual alignment.
Thuswe instead used four other strong baselines:Meteor (Denkowski and Lavie, 2011): a sys-tem for evaluating machine translation by aligningMT output with reference sentences.
It is designedfor the task of monolingual alignment and supportsphrasal alignment.
We used version 1.4 and defaultweights to optimize by maximum accuracy.MANLI-constraint (Thadani and McKeown,2011): a re-implemented MANLI system with ILP-powered decoding for speed and hard syntactic con-straints to boost exact match rate, with reportednumbers on MSR06.MANLI-joint (Thadani et al 2012): an im-proved version of MANLI-constraint that not onlymodels phrasal alignments, but also alignments be-tween dependency arcs, with reported numbers onthe original Edinburgh paraphrase corpus.jacana-token (Yao et al 2013a): a token-based aligner with state-of-the-art performance onMSR06.Note that the jacana-token aligner is open-source,so we were able to re-train it with exactly thesame feature set used by our phrase-based model.This allows a fair comparison of model performance(token-based vs. phrase-based).
The MANLI* sys-tems are not available, thus we only reported theirnumbers from published papers.The standard evaluation metrics for alignmentsare precision (P), recall (R), F1, and exact matching6a few examples: two Atlanta-basedcompanies?two Atlanta companies, theUK?the UK, the 17-year-old?the teenager,was held?was held.rate (E) based on either tokens (two tokens are con-sidered aligned iff they are aligned) or phrases (twotokens are considered aligned iff they are containedwithin phrases that are aligned).
Following Thadaniet al(2012), we only report the results based ontoken alignments (which allows a partial credit iftheir containing phrases are not aligned), even forthe phrase-based alignment task.
The reasoning isthat if a phrase-based aligner is already doing bet-ter than a token aligner in terms of token alignmentscores, then the difference in terms of phrase align-ment scores will be even larger.
Thus showing thesuperiority of token alignment scores is sufficient.4.3 Implementation and TrainingThe elements in the phrase-based model: dynamicstate indices, semi-Markov and phrasal states, arenot typically found in standard CRF implementa-tions.
Thus we implemented the phrase-based modelin the Scala programming language, which is fullyinteroperable with Java, using one semi-MarkovCRF package7 as a reference.
We used the L2 reg-ularizer and LBFGS for optimization.
OpenNLP8provided the POS tagger and chunker and JWNL9interfaced with WordNet (Fellbaum, 1998).4.4 ResultsTable 3 gives scores (in bigger fonts) of differentaligners on MSR06 and Edinburgh++ and their cor-responding phrasal versions.
Overall, the token-based aligner did the best on the original corpora, inwhich single token alignment counts more than 95%of total alignment.
The phrase-based aligner didslightly worse.
We think the main reason was that itoutput more phrasal alignment, which in turn harmsscores in token-based evaluation (for instance, if thegold alignment is New?New, York?York, thenthe phrasal alignment of New York?New Yorkwould only have half the precision because it inher-ently also aligns New in the source with York inthe target.).
Further investigation showed that on theEdinburgh++ corpus, over-generated phrase-basedalignment, when evaluated under just token align-ment, contributed hurting about 1.1% of overall F1,7http://crf.sf.net8http://opennlp.apache.org/9http://jwordnet.sf.net/596a gap that would make the phrase aligner (85.9%)outperform the token aligner (86.4%).On the phrasal alignment corpora (represented byMSR06P and EDB++P in Table 3), the phrase-basedaligner did significantly better.
Note that the over-all F1 and exact match rate are still much lowerthan those scores obtained from the original corpora,suggesting that the phrasal corpora present a muchharder task.
Furthermore, as a more ?fair?
com-parison between the two aligners, we synthesizedphrasal alignments from the output of the token-based aligner, just as how the phrased-based corporawere prepared, then evaluated its performance again.Still, on the EDB++P corpus, the token aligner wasabout 1.6% (current difference is 69.1% vs. 72.8%)worse than the phrase-based aligner.Also, we want to emphasize that since the token-based aligner and the phrase-based aligner sharedexactly the same features and lexical resources, theperformance boost of the phrase-based aligner onthe phrasal corpora results from a better model de-sign: it is the semi-Markov property and phrasalstates making the phrase-based aligner better.To further investigate the performance of alignerswith respect to different types of alignment, we di-vided the scores into those for identical alignments(such as New?New) and non-identical alignments(such as wife?spouse), indicated by the sub-scripts i and n in Table 3.
In terms of identicalalignment, most aligners were able to score morethan 90%, but for non-identical alignment there wasnoticeable decrease.
Still, on the phrasal alignmentcorpora, the phrase-based model has a much largerrecall score for non-identical alignment than others.We also divided scores with respect to token-onlyalignment and phrase-only alignment.
Due to spacelimit, we only show results on synthesized Edin-burgh++, in Table 4.
Meteor and the token alignerinherently have either very limited or no support forphrasal alignment, thus they had very low scoreson phrase-only alignment.
We then ran the align-ers in two directions and merged the results with the?union?
MT heuristic to get better phrase support.But that still did not bring F1p?s up to over 5%.The phrase-based aligner baseline Meteor didworse than our aligners.
We think there are two rea-sons: First, Meteor was not trained on these corpora.Second, Meteor only does strict word, stem, syn-SystemP% R% F1%E%Pi/Pn Ri/Rn F1i/F1nMSR06(78.6%) Meteor82.5 81.2 81.915.089.9/39.9 97.3/24.6 93.5/30.5MANLI-cons.
89.5 86.2 87.8 33.0token93.6 83.5 88.332.196.6/77.7 96.9/35.6 96.8/48.8phrase92.1 82.8 86.829.195.7/65.0 95.9/34.7 95.8/45.2MSR06P(59.0%) Meteor82.5 68.3 74.77.389.9/40.1 97.3/8.8 93.5/14.5token92.9 66.1 77.213.595.5/77.5 94.3/11.1 94.9/19.5phrase83.5 77.0 80.114.394.9/55.5 94.2/48.1 94.5/51.5EDB++(75.2%) Meteor88.3 80.5 84.212.794.0/61.4 97.8/24.1 95.9/34.7MANLI-jnt* 76.6 83.8 79.2 12.2token91.3 82.0 86.415.096.4/63.9 97.4/36.4 96.9/46.4phrase90.4 81.9 85.913.796.0/57.4 97.8/38.3 96.9/46.0EDB++P(51.7%) Meteor88.4 60.6 71.92.994.0/61.9 97.0/6.5 95.5/11.7token90.7 55.8 69.12.396.2/58.6 91.3/7.1 93.7/12.7phrase82.3 65.3 72.81.695.6/60.4 93.1/34.3 94.4/43.8Table 3: Results on original (mostly token) and phrasal(P) alignment corpora, where (x%) indicates how muchalignment is identical alignment, such as New?New.
E%stands for exact (perfect) match rate.
Subscript i standsfor corresponding scores for ?identical?
alignment and nfor ?non-identical?.
*: scores of MANLI-joint were forthe original Edinburgh corpus instead of Edinburgh++(with hand corrections) so it is not a direct comparison.onym and paraphrase matching but does not use anystring similarity measures; this can be supported bythe large difference between, for instance, F1i andF1n.
In general Meteor did well on identical align-ment, but not so well on non-identical alignment.5 ApplicationsNatural language alignment can be applied to vari-ous NLP tasks.
While how to most effectively apply597SystemP% R% F1%E%Pt/Pp Rt/Rp F1t/F1pEDB++PMeteor88.4 60.6 71.92.959.5/14.9 90.6/1.1 71.8/2.0token90.7 55.8 69.12.359.4/21.4 85.5/0.9 70.1/1.7phrase82.3 65.3 72.81.673.3/48.0 73.5/44.2 73.4/46.0Table 4: Same results on the phrasal Edinburgh++ cor-pus but with scores divided by token-only alignment(subscript t) and phrase-only alignment (subscript p).it is another topic, we simply show in this section us-ing just alignment scores in binary prediction prob-lems.
Specifically, we pick the tasks of recognizingtextual entailment (RTE), paraphrase identification(PP), and question answering sentence ranking (QA)described in Heilman and Smith (2010):RTE: predicting whether a hypothesis can be in-ferred from the premise, with training data fromRTE-1/2 and RTE-3 dev, and test from RTE-3 test.PP: predicting whether two sentences are para-phrases, with training and test data from the MSRParaphrase Corpus (Dolan et al 2004).QA: predicting whether a sentence contains theanswer to the question, with training data fromTREC-8 to TREC-12 and test data from TREC-13.For each aligned pair, we can compute a normal-ized decoding score.
Following MacCartney et al(2008), we select a threshold score and predict trueif the decoding score is above this threshold.
For thetasks of RTE and PP, we tuned this threshold w.r.tthe maximal accuracy on the training set, then re-ported performance on the test set.
For the task ofQA, since the evaluation methods in Mean AveragePrecision and Mean Reciprocal Rank only need aranked list of answer sentences, and the scores onthe test set are sufficient to provide the ranking, wedid not tune anything on training but instead directlyran the aligner on the test set.
All three tasks sharedthe same aligner model trained on the superset ofMSR06 and Edinburgh++.
Results are reported inTable 5.
We could not report on Meteor as Meteordoes not explicitly output alignment scores.We did not expect the aligners to beat any of thesystem A% P% R%de Marneffe et al(2006) 60.5 61.8 60.2MacCartney and Manning (2008) 64.3 65.5 63.9Heilman and Smith (2010) 62.8 61.9 71.2the token aligner 59.1 61.2 55.4our phrasal aligner 57.6 57.2 68.8(a) Recognizing Textual Entailmentsystem A% P% R%Wan et al(2006) 75.6 77 90Das and Smith (2009) 73.9 74.9 91.3Heilman and Smith (2010) 73.2 75.7 87.8the token aligner 70.0 72.6 88.1our phrasal aligner 68.1 68.6 95.8(b) Paraphrase Identificationsystem MAP MRRCui et al(2005) 0.4271 0.5259Wang et al(2007) 0.6029 0.6852Heilman and Smith (2010) 0.6091 0.6917Yao et al(2013b) 0.6307 0.7477the token aligner 0.5982 0.6582our phrasal aligner 0.6165 0.7333(c) Question Answering Sentence RankingTable 5: Results (Accuracy, Precision, Recall, MeanAverage Precision, Mean Reciprocal Rank) on thetasks of RTE, PP and QA.state-of-the-art result since no sophisticated modelswere additionally used but only the alignment score.Still, the aligners showed competitive performance.It still follows the pattern from the alignment exper-iment that the phrasal aligner had higher recall andlower precision than the token aligner in the task ofRTE and PP.
In the QA task, the phrasal aligner per-formed better than all systems except for the top one.6 ConclusionWe have introduced a phrase-to-phrase alignmentmodel based on semi-Markov Conditional RandomFields.
The combination of semi-Markov states andphrasal states makes phrasal alignment on both thesource and target sides possible.
The final phrase-598based aligner performed the best on two phrasalalignment corpora and showed its potential usagein three NLP tasks.
Future work includes aligningdiscontinuous (gappy) phrases and integrating align-ment more closely in NLP applications.AcknowledgementWe thank Vulcan Inc. for funding this work.
We alsothank Jason Smith, Travis Wolfe, Frank Ferraro andthe three anonymous reviewers for their commentsand suggestion.ReferencesJesu?s Andre?s-Ferrer and Alfons Juan.
2009.
A phrase-based hidden semi-markov approach to machine trans-lation.
In Procedings of European Association forMachine Translation (EAMT), Barcelona, Spain, May.European Association for Machine Translation.Nicholas Andrews, Jason Eisner, and Mark Dredze.2012.
Name phylogeny: a generative model of stringvariation.
In Proceedings of EMNLP 2012.Mohit Bansal, Chris Quirk, and Robert Moore.
2011.Gappy phrasal alignment by agreement.
In Proceed-ings of ACL, Portland, Oregon, June.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of NAACL, pages16?23.Phil Blunsom and Trevor Cohn.
2006.
Discriminativeword alignment with conditional random fields.
InProceedings of ACL2006, pages 65?72.Chris Brockett.
2007.
Aligning the RTE 2006 corpus.Technical report, Microsoft Research.Peter F Brown, Vincent J Della Pietra, Stephen A DellaPietra, and Robert L Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational linguistics, 19(2):263?311.Nathanael Chambers, Daniel Cer, Trond Grenager,David Hall, Chloe Kiddon, Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage, Eric Yeh, andChristopher D Manning.
2007.
Learning alignmentsand leveraging natural logic.
In Proceedings of theACL-PASCAL Workshop on Textual Entailment andParaphrasing, pages 165?170.Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.2008.
Constructing corpora for the development andevaluation of paraphrase systems.
Computational Lin-guistics, 34(4):597?614, December.Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng Chua.
2005.
Question answering passage re-trieval using dependency relations.
In Proceedingsof the 28th annual international ACM SIGIR confer-ence on Research and development in information re-trieval, SIGIR ?05, pages 400?407, New York, NY,USA.
ACM.Dipanjan Das and Noah A. Smith.
2009.
Paraphraseidentification as probabilistic quasi-synchronousrecognition.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP, pages 468?476, Suntec,Singapore, August.
Association for ComputationalLinguistics.Marie-Catherine de Marneffe, Bill MacCartney, TrondGrenager, Daniel Cer, Anna Rafferty, and Christo-pher D Manning.
2006.
Learning to distinguish validtextual entailments.
In Second Pascal RTE ChallengeWorkshop.Yonggang Deng and William Byrne.
2008.
HMM wordand phrase alignment for statistical machine transla-tion.
Audio, Speech, and Language Processing, IEEETransactions on, 16(3):494?507.Michael Denkowski and Alon Lavie.
2011.
Meteor 1.3:Automatic Metric for Reliable Optimization and Eval-uation of Machine Translation Systems.
In Proceed-ings of the EMNLP 2011 Workshop on Statistical Ma-chine Translation.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Un-supervised construction of large paraphrase corpora:exploiting massively parallel news sources.
In Pro-ceedings of COLING, Stroudsburg, PA, USA.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The ParaphraseDatabase.
In Proceedings of NAACL-HLT, pages 758?764.Kevin Gimpel and Noah A. Smith.
2010.
Softmax-margin CRFs: training log-linear models with costfunctions.
In NAACL 2010, pages 733?736.Lushan Han, Abhay Kashyap, Tim Finin, James May-field, and Jonathan Weese.
2013.
UMBC-EBIQUITY-CORE: Semantic Textual Similarity Systems.
In Pro-ceedings of the Second Joint Conference on Lexicaland Computational Semantics.Michael Heilman and Noah A. Smith.
2010.
Treeedit models for recognizing textual entailments, para-phrases, and answers to questions.
In Proceedings ofNAACL 2010, pages 1011?1019, Los Angeles, Cali-fornia, June.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press, New York, NY, USA.Milen Kouylekov and Bernardo Magnini.
2005.
Recog-nizing textual entailment with tree edit distance algo-rithms.
In PASCAL Challenges on RTE, pages 17?20.599John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the Eighteenth InternationalConference on Machine Learning, ICML ?01, pages282?289, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of NAACL.Bill MacCartney and Christopher D Manning.
2008.Modeling semantic containment and exclusion in nat-ural language inference.
In Proceedings of ACL 2008,pages 521?528.Bill MacCartney, Michel Galley, and Christopher D Man-ning.
2008.
A phrase-based alignment model for nat-ural language inference.
In Proceedings of EMNLP,pages 802?811.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proceedings of EMNLP-2002, pages 133?139.Yashar Mehdad.
2009.
Automatic cost estimation fortree edit distance using particle swarm optimization.In Proceedings of the ACL-IJCNLP 2009 ConferenceShort Papers, pages 289?292.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational linguistics, 29(1):19?51.Mari Ostendorf, Vassilios V Digalakis, and Owen A Kim-ball.
1996.
From HMM?s to segment models: a uni-fied view of stochastic modeling for speech recogni-tion.
IEEE Transactions on Speech and Audio Pro-cessing, 4(5):360?378.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations: Ex-tracting paraphrases and generating new sentences.
InProceedings of NAACL, pages 102?109.Vasin Punyakanok, Dan Roth, and Wen T. Yih.
2004.Mapping Dependencies Trees: An Application toQuestion Answering.
In Proceedings of the 8th In-ternational Symposium on Artificial Intelligence andMathematics, Fort Lauderdale, Florida.Michael Roth and Anette Frank.
2012.
Aligningpredicates across monolingual comparable texts usinggraph-based clustering.
In Proceedings of EMNLP-CoNLL, pages 171?182, Jeju Island, Korea, July.Sarawagi Sarawagi and William Cohen.
2004.
Semi-markov conditional random fields for information ex-traction.
Advances in Neural Information ProcessingSystems, 17:1185?1192.Kapil Thadani and Kathleen McKeown.
2011.
Optimaland syntactically-informed decoding for monolingualphrase-based alignment.
In Proceedings of ACL short.Kapil Thadani, Scott Martin, and Michael White.
2012.A joint phrasal and dependency model for paraphrasealignment.
In Proceedings of COLING 2012: Posters,pages 1229?1238, Mumbai, India, December.
TheCOLING 2012 Organizing Committee.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of the 16th conference on Com-putational linguistics - Volume 2, COLING ?96, pages836?841.Stephen Wan, Mark Dras, Robert Dale, and Ce?cile Paris.2006.
Using dependency-based features to take the?para-farce?
out of paraphrase.
In Proceedings of theAustralasian Language Technology Workshop.Mengqiu Wang and Christopher D. Manning.
2010.Probabilistic tree-edit models with structured latentvariables for textual entailment and question answer-ing.
In Proceedings of COLING, pages 1164?1172,Stroudsburg, PA, USA.Mengqiu Wang, Noah A. Smith, and Teruko Mitamura.2007.
What is the Jeopardy Model?
A Quasi-Synchronous Grammar for QA.
In Proceedings ofEMNLP-CoNLL, pages 22?32, Prague, Czech Repub-lic, June.Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark.
2013a.
A Lightweight andHigh Performance Monolingual Word Aligner.
InProceedings of ACL 2013 short, Sofia, Bulgaria.Xuchen Yao, Benjamin Van Durme, Peter Clark, andChris Callison-Burch.
2013b.
Answer Extraction asSequence Tagging with Tree Edit Distance.
In Pro-ceedings of NAACL 2013.600
