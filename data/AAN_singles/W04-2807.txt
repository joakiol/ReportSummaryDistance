Different Sense Granularities for Different ApplicationsMartha Palmer, Olga Babko-Malaya, Hoa Trang DangUniversity of Pennsylvania{mpalmer/malayao/htd}@linc.cis.upenn.eduAbstractThis paper describes an hierarchical approachto WordNet sense distinctions that providesdifferent types of automatic Word Sense Dis-ambiguation (WSD) systems, which performat varying levels of accuracy.
For tasks wherefine-grained sense distinctions may not be es-sential, an accurate coarse-grained WSD sys-tem may be sufficient.
The paper discusses thecriteria behind the three different levels ofsense granularity, as well as the machine learn-ing approach used by the WSD system.1 IntroductionThe difficulty of finding consistent criteria for makingsense distinctions has been thoroughly attested to in theliterature (Kilgarriff, ?97, Hanks, ?00).
Difficulties havebeen found with truth-theoretical criteria, linguistic crite-ria and definitional criteria (Sparck-Jones, ?86, Geer-aerts, ?93).
In spite of the proliferation of dictionaries,there is no methodology by which two lexicographersworking independently are guaranteed to derive the sameset of distinctions for a given word, with objects andevents vying for which is the most difficult to character-ize (Cruse, ?86, Apresjan, ?74, Pustejovsky, ?91, ?95).On the other hand, accurate Word Sense Disambiguation(WSD) could significantly improve the precision of In-formation Retrieval by ensuring that the senses of verbsin the retrieved documents match the sense of the verb inthe query.
For example, the two queries What do youcall a successful movie?
and Whom do you call for asuccessful movie?
submitted to AskJeeves both retrievethe same set of documents, even though they are askingquite different questions, referencing very differentsenses of call.
The documents retrieved are also not veryrelevant, again because they do not distinguish whichmatches contain relevant senses and which do not.Tips on Being a Successful Movie Vampire ...
I shallcall the police.Successful Casting Call & Shoot for ``Clash of Em-pires'' ... thank everyone for their participation in themaking of yesterday's movie.Demme's casting is also highly entertaining, although Iwouldn't go so far as to call it successful.
This  movie'sresemblance to its predecessor is pretty vague...VHS Movies: Successful Cold Call Selling: Over 100New Ideas, Scripts, and Examples from the Nation'sForemost Sales Trainer.The two senses of call in the two queries can be easilydistinguished by their differing predicate-argumentstructures.
They are also separate senses in WordNet,but WordNet has an additional 26 senses for call, and thecurrent best performance of an automatic Word SenseDisambiguation system this type of polysemous verb isonly 60.2% (Dang and Palmer, 2002).
Is it possible thatsense distinctions that are less fine-grained than Word-Net?s distinctions could be made more reliably, andcould still benefit this type of NLP application?The idea of underspecification as a solution to WSD hasbeen proposed in Buitelaar 2000 (among others), whopointed out that for some applications, such as documentcategorization, information retrieval, and informationextraction it may be sufficient to know if a given wordbelongs to a certain class of WordNet senses or under-specified sense.
On the other hand, there is evidence thatmachine translation of languages as diverse as Chineseand English will require all of the fine-grained sensedistinctions that WordNet is capable of providing, andeven more (Ng, et al2003, Palmer, et.
al., to appear).An hierarchical approach to verb senses, of the type dis-cussed in this paper, presents obvious advantages for theproblem of word sense disambiguation.
The human an-notation task is simplified, since there are fewer choicesat each level and clearer distinctions between them.
Theautomated systems can combine training data fromclosely related senses to overcome the sparse data prob-lem, and both humans and systems can back off to amore coarse-grained choice when fine-grained choicesprove too difficult.The approach to verb senses presented in this paper as-sumes three different levels of sense distinctions: Prop-Bank Framesets, WordNet groupings, and WordNetsenses.
In a project for the semantic annotation of predi-cate-argument structure, PropBank, we have madecoarse-grained sense distinctions for the 700 mostpolysemous verbs in the Penn TreeBank (Kingsbury andPalmer, ?02).
These distinctions are based primarily ondifferent subcategorization frames that require differentargument label annotations.
In a separate project, as dis-cussed in Palmer et al2004, we have groupedSENSEVAL-2 verb senses (which came from WordNet1.7).
These manual groupings were shown to reconcile asubstantial portion of the manual and automatic taggingdisagreements, showing that many of these disagree-ments are fairly subtle (Palmer, et.al., ?04).The tree levels of sense distinctions form a continuum ofgranularity.
Our criterion for the Framesets, being pri-marily syntactic, is also the most clear cut.
These distinc-tions are based primarily on usages of a verb that havedifferent numbers of predicate-arguments, however theyalso separate verb senses on semantic grounds, if thesesenses are not closely related.
Sense groupings providean intermediate level of hierarchy, where groups aredistinguished by more fine-grained criteria.
BothFrameset and grouping distinctions can be made consis-tently by humans and systems (over 90% accuracy forFramesets and 82% for groupings) and are surprisinglycompatible; 95% of our groups map directly onto a sin-gle PropBank sense.2  Background2.1 PropbankPropBank [Kingsbury & Palmer, 2002] is an annotationof the Wall Street Journal portion of the Penn TreebankII [Marcus, 1994] with dependency structures (or `predi-cate-argument' structures), using sense tags for highlypolysemous words and semantic role labels for each de-pendency.
An important goal is to provide consistentsemantic role labels across different syntactic realiza-tions of the same verb, as in the window in [ARG0 John]broke [ARG1 the window] and [ARG1 The window] broke.PropBank can provide frequency counts for (statistical)analysis or generation components in a machine transla-tion system, but provides only a shallow semantic analy-sis in that the annotation is close to the syntacticstructure and each verb is its own predicate.In addition to the annotated corpus, PropBank provides alexicon that lists, for each broad meaning of each anno-tated verb, its Frameset, i.e., the possible arguments inthe predicate and their labels and all possible syntacticrealizations.
The notion of ``meaning'' used is fairlycoarse-grained, and it is typically motivated from differ-ing syntactic behavior.
The Frameset alo includes a``descriptor'' field for each role which is intended for useduring annotation and as documentation, but which doesnot have any theoretical standing.
The collection ofFrameset entries for a verb is referred to as the verb'sframe.
As an example of a PropBank entry, we give theframe for the verb leave below.
Currently, there areframes for over 3,000 verbs, with a total of just over4,300 Framesets described.
Of these 3,000 verb frames,only a small percentage 21.8 % (700) have more thanone Frameset, with less than 100 verbs with 4 or more.The process of sense-tagging the PropBank corpus withthe Frameset tags has just been completed.The criteria used for the Framesets are primarily syntac-tic and clear cut.
The guiding principle is that two verbmeanings are distinguished as different framesets if theyhave distinct subcategorization frames.
For example, theverb ?leave?
has 2 framesets with the following frames,illustrated by the examples in (1) and (2):Frameset 1:  move away fromArg0:entity leavingArg1:place leftFrameset 2:  giveArg0:giver / leaverArg1:thing givenArg2:benefactive / given-to(1) John left the room.
(2) Mary left her daughter-in-law her pearls in her will2.2 WordNet  Sense GroupingsIn a separate project, as part of Senseval tagging exer-cises, we have developed a lexicon with another level ofcoarse-grained distinctions, as described below.The Senseval-1 workshop (Kilgarriff and Palmer, 2000)provided convincing evidence that supervised automaticsystems can perform word sense disambiguation (WSD)satisfactorily, given clear, consistent sense distinctionsand suitable training data.
However, the Hector lexiconthat was used as the sense inventory was very small andunder proprietary constraints, and the question remainedwhether it was possible to have a publicly available,broad-coverage lexical resource for English and otherlanguages, with the requisite clear, consistent sense dis-tinctions.Subsequently, the Senseval-2 (Edmonds and Cotton,2001) exercise was run, which included WSD tasks for10 languages.
A concerted effort was made to use exist-ing WordNets as sense inventories because of theirwidespread popularity and availability.
Each languagehad a choice between the lexical sample task and the all-words task.
The most polysemous words in the EnglishLexical Sample task are the 29 verbs, with an averagepolysemy of 16.28 senses using the pre-release versionof WordNet 1.7.
Double blind annotation by two lin-guistically trained annotators was performed on corpusinstances, with a third linguist adjudicating between in-ter-annotator differences to create the ?Gold Standard.
?The average inter-annotator agreement rate was only71%, which is comparable to the 73% agreement for allwords in SemCor, with a much lower average polysemy.However, a comparison of system performance on wordsof similar polysemy in Senseval-1 and Senseval-2showed very little difference in accuracy (Palmer et al,submitted).
In spite of the lower inter-annotator agree-ment figures for Senseval-2, the double blind annotationand adjudication provided a reliable enough filter to en-sure consistently tagged data with WordNet senses.Even so, the high polysemy of the WordNet 1.7 entrieson average poses a challenge for automatic word sensedisambiguation.
In addition, WordNet only gives a flatlisting of alternative senses, unlike most standard dic-tionaries which are more structured and often providehierarchical entries.
To address this lack, the verbs weregrouped by two or more people, with differences beingreconciled, and the sense groups were used for coarse-grained scoring of the systems.The criteria used for groupings included syntactic andsemantic ones.
Syntactic structure performed two dis-tinct functions in our groupings.
Recognizable alterna-tions with similar corresponding predicate-argumentstructures were often a factor in choosing to groupsenses together, as in the Levin classes and PropBank,whereas distinct subcategorization frames were also of-ten a factor in putting senses in separate groups.
Fur-thermore, senses were grouped together if they weremore specialized versions of a general sense.
The se-mantic criteria for grouping senses separately includeddifferences in semantic classes of arguments (abstractversus concrete, animal versus human, animacy versusinanimacy, different instrument types...), differences inthe number and type of arguments (often reflected in thesubcategorization frame as discussed above), differencesin entailments (whether an argument refers to a createdentity or a resultant state), differences in the type ofevent (abstract, concrete, mental, emotional...), whetherthere is a specialized subject domain, etc.Senseval-2 verb inter-annotator disagreements were re-duced by more than a third when evaluated against thegroups, from 29% to 18%, and by over half in a separatestudy, from 28% to 12%.
A similar number of randomgroups provided almost no benefit to the inter-annotatoragreement figures (74% instead of 71%), confirming thegreater coherence of the manual groupings.3 Mapping of Sense Groups to FramesetsGroupings of senses for Senseval-2, as discussed above,use both syntactic and semantic criteria.
Propbank, onthe other hand, uses mostly syntactic cues to divide verbsenses into framesets.
As a result, framesets are moregeneral than sense-groups and usually incorporate sev-eral sense groups.
We have been investigating whetheror not the groups developed for SENSEVAL-2 can providean intermediate level of hierarchy in between the Prop-Bank Framesets and the WN 1.7 senses, and our initialresults are promising.
Based on our existing WN 1.7tags and frameset tags of the Senseval2 verbs in the PennTreeBank, 95% of the verb instances map directly fromsense groups to framesets, with each frameset typicallycorresponding to two or more sense groups, as illustratedby the tables 1-4 for the verbs ?serve?, ?leave?, ?pull?, and?see?1  below.As the tables 1-4 illustrate, the criteria used to split theFramesets into groups are as follows:1) Syntactic Frames.
Most verb senses which allow syn-tactic alternations (such as transitive/inchoative, unspeci-fied object deletion, etc) are analyzed as one sensegroup.
However, in some cases, as illustrated by the verbleave, intransitive and transitive uses are distinguished asdifferent sense groups:Group 1: DEPART (Ship leaves at midnight)Group 2: LEAVE BEHIND (She left a mess.
)The DEPART sense of the verb can be used transitively ifthe object specifies the place of departure.
The LEAVEBEHIND sense is more general and allows syntactic varia-tion as well as different semantic types of NPs.
In Prop-Bank, these groups are unified as one frameset (Frameset1 MOVE AWAY FROM).1 All these verbs have one or more additional framesets, whichcorrespond to one group or sense, and therefore are not in-cluded hereFrameset  Senseval-2 Groupings Examples from WordNetGROUP 1:WN1 (function)WN3(contribute to)WN12 (answer)His freedom served him wellThe scandal served to increase his popularityNothing else will serveGROUP 2:WN2 (do duty)WN13 (do military service)She served in CongressShe served in VietnamGROUP 5:WN7 (devote one?s efforts)WN10 (attend to)She served the art of musicMay I serve you?serve 01:  Act, workRoles:Arg0:workerArg1:job,  projectArg2:employerGROUP 3:WN4 (be used by)WN8 (serve well)WN14 (service)The garage served to shelter horsesArt serves commerceMale animals serve the females for breedingpurposesTable 1.
Frameset  serve 01.Frameset  Senseval-2 Groupings Examples from WordNetGROUP 2:WN2 (leave behind)WN12 (be survived by)WN14  (forget)She left a messHe left six children I left my keysGROUP 1:WN1  (go away)WN5 (exit, go out)WN8 (depart)The ship leaves at midnightLeave the roomThe teenager left homeGROUP 3:WN3 (to act)WN7 (result in)The inflation left them pennilessHer blood left a stain on the napkinSINGLETONWN4 (leave behind)Leave it as isleave 02: Move awayfromRoles:Arg0:entity leavingArg1:thing leftArg2 :attribute / sec-ondary predicationSINGLETONWN6 (allow for, provide)Leave lots of time for the tripTable 2.
Frameset leave 02.2.
Optional Arguments.
In PropBank verbs of mannerof motion and verbs of directed motion are usuallygrouped into one frameset.
For example, one of theframesets of the verb pull (TRY) TO CAUSE MOTIONunifies the following two group senses:Group 1: MOVE ALONG (pull a sled)Group 2: MOVE INTO A CERTAIN DIRECTION (The vanpulled up)Although the frame for the frameset 1 of the verb pullhas a ?direction?
argument, this argument does nothave to be present (or implied), and verbs with thisframe can also be understood as verbs of manner ofmotion in PropBank.3) Syntactic variation of arguments.
Syntactic varia-tion in objects can also be used to distinguish sensegroups, but are not taken into consideration for distin-guishing framesets.
Here both noun phrases and sen-Frameset  Senseval-2 Groupings Examples from WordNetGROUP 1:WN1 (draw)WN4(apply force)WN9 (cause to move)WN10 (operate)WN13 (hit)Pull a sledPull the ropeA declining dollar pulled  down the export figuresPull the oarsPull the ballGROUP 2:WN2 (attract)WN12 (rip)The ad pulled in many potential customersPull the cooked chicken into stripsGROUP 3:WN3 (move)WN7 (steer)The car pulls to the rightPull the car overpull.01:  try to cause motionRoles:Arg0:pullerArg1:thing pulledArg2: direction or predicationArg3:extent, distance movedGROUP 4:WN6 (pull out)WN15 (extract)WN17(take away)The mugger pulled a knife on his victimPull weedsPull the old soup cans from the shelfTable 3.
Frameset pull 01.Frameset  Senseval-2 Groupings Examples from WordNetGROUP 1:WN1 (perceive by sight)WN7 (watch)WN19 (observe as if with an eye)WN20 (examine)Can you the bird?See a movieThe camera saw the burglaryI must see your passportGROUP 3:WN3 (witness)WN6 (learn)I want to see the resultsI see that you have been promotedGROUP 4:WN5 (consider)WN24 (interpret)I don?t see the situation quite as negativelyWhat message do you see in this letter?GROUP 5:WN8 (determine)WN10 (check)WN14 (attend)See whether it worksSee that the curtains are closedCould you see about lunch?see.01: viewRoles:Arg0:viewerArg1:thing viewedArg2:secondary attributeGROUP 6:WN11 (see a professional)WN15 (receive as a guest)You should see a lawyerThe doctor will see you nowTable 4.
Frameset see 01.tential complements are contained in the same frame-set.
These could also be distinguished by the type ofevent, a physical perception vs. an abstract or mentalperception, but these would also not distinguished byPropBank.Group 1: PERCEIVE BY SIGHT (Can you see the bird?
)Group 5: DETERMINE, CHECK (See whether it works)4) Semantic classes of arguments.
Differences in se-mantic classes of arguments, such as ANIMACY versusINANIMACY, are also not considered for distinguishingframesets.
The verb serve, for example, has the follow-ing group senses, the second of which requires anANIMATE agent, which are unified as one frameset inPropBank:Group 1: FUNCTION (His freedom served him well)Group 2: WORK (He served in Congress)Most of the criteria which are used to split Framesetsinto groupings, as the tables above illustrate, are se-mantic.
These distinctions, although more fine-grainedthan Framesets, are still more easily distinguished thanWordNet senses.Mismatches between Framesets and groupings usuallyoccur for the following two reasons.
First, some sensescan be missing in the PropBank, if they do not occur inthe corpus.
Second, given that PropBank is an annota-tion of the Wall Street Journal, it often distinguishesobscure financial senses of the verb as separate senses.4 Experiments with Automatic WSDWe have also been investigating the suitability of thesedistinctions for training automatic Word SenseDisambiguation systems.
The system that we used totag verbs with their frameset is the same maximumentropy system as that of Dang and Palmer (2002),including both topical and local features.
Topicalfeatures looked for the presence of keywords occurringanywhere in the sentence and any surroundingsentences provided as context (usually one or twosentences).
The set of keywords is specific to eachlemma to be disambiguated, and is determinedautomatically from training data so as to minimize theentropy of the probability of the senses conditioned onthe keyword.The local features for a verb w in a particular sentencetend to look only within the smallest clause containingw.
They include collocational features requiring nolinguistic prepro essing beyond part-of-speech tagging(1), syntactic features that capture relationsbetween the verb and its complements (2-4), and se-mantic features that incorporate information aboutnoun classes for objects (5-6):1) the word w, the part of speech of w, andwords at positions -2, -1, +1, +2, relative to w2) whether or not the sentence is passive3) whether there is a subject, direct object, indi-rect object, or clausal complement (a comple-ment whose node label is S in the parse tree)4) the words (if any) in the positions of subject,direct object, indirect object, particle, preposi-tional complement (and its object)5) a Named Entity tag (PERSON,ORGANIZATION, LOCATION) for propernouns appearing in (4).6) all possible WordNet synsets and hypernymsfor the nouns appearing in (4).The system performed well on the English verbs inSenseval-2, achieving an accuracy of 60.2% when tag-ging verbs with their fine-grained WordNet senses, and70.2% when tagging with the more coarse-grainedsense groups.Verb Framesets Instances Accuracycall 11 522 0.835carry 4 195 0.933develop 2 240 0.938draw 3 94 0.926dress 3 15 0.800drive 2 99 0.808keep 5 136 0.919leave 3 147 0.762live 4 125 0.888play 5 98 0.806pull 6 88 0.784see 2 187 0.995serve 2 150 0.967strike 10 59 0.610train 2 17 0.941treat 2 51 0.863turn 14 141 0.638use 2 820 0.988wash 2 8 0.875work 7 398 0.955Table 5.
Frameset tagging resultsFor frameset tagging, we collected a total of 3590 in-stances of 20 verbs in the PropBank corpus that hadbeen annotated with their framesets.
The verbs all hadmore than one possible frameset and were a subset ofthe ones used for the English lexical sample task ofSenseval-2.
Local features for frameset taging wereextracted using the gold-standard part-of-speech tagsand bracketing of the Penn Treebank.
Table 5 showsthe number of framesets, the number of instances, andthe system accuracy for each verb using 10-fold cross-validation.
The overall accuracy of our automaticframeset tagging was 90.0%, compared to a baselineaccuracy of 73.5% if verbs are tagged with their mostfrequent frameset.
While the data is only a subset ofthat used in Senseval-2, it is clear that framesets can bemuch more reliably tagged than fine-grained WordNetsenses and even sense groups.ConclusionThis paper described an hierarchical approach toWordNet sense distinctions that provided differenttypes of automatic Word Sense Disambiguation (WSD)systems, which perform at varying levels of accuracy.We have described three different levels of sensegranularity, with PropBank Framesets being the mostsyntactic, the most coarse-grained, and most easilyreproduced.
A set of manual groupings devised forSenseval2 provides a middle level of granularity thatmediates between Framesets and WordNet.
For taskswhere fine-grained sense distinctions may not be essen-tial such as an AskJeeves information retrieval task, anaccurate coarse-grained WSD system such as ourFrameset tagger may be sufficient.
There is evidence,however, that machine translation of languages as di-verse as Chinese and English might require all of thefine-grained sense distinctions of WordNet, and evenmore (Ng, et al2003, Palmer, et.
al., to appear).ReferencesApresjan, J. D.
.
(1974) Regular polysemy, Linguistics,142:5?32.Atkins, S. (1993) Tools for computer-aided corpuslexicography: The Hector Project.
Actu Linguis-tica Hunguricu, 41:5-72.Buitelaar, P.P (2000).
Reducing Lexical SemanticComplexity with Systematic Polysemous Classesand Underspecification.
In Poceedings of theANLP Workshop on Syntactic and Semantic Com-plexity in NLP Systems.
Seattle, WA.Cruse, D. A., (1986), Lexical Semantics, CambridgeUniversity Press, Cambridge, UK, 1986.Dang, H. T. and Palmer, M., (2002).
Combining Con-textual Features for Word Sense Disambiguation.In Proceedings of the Workshop on Word SenseDisambiguation: Recent Successes and Future Di-rections, Philadelphia, Pa.Edmonds, P. and Cotton, S. (2001).
SENSEVAL-2:Overview.
In Proceedings of SENSEVAL-2: Sec-ond International Workshop on Evaluating WordSense Disambiguation Systems, ACL-SIGLEX,Toulouse, France.Hanks, P., (2000), Do word meanings exist?
Com-puters and the Humanities, Special Issue onSENSEVAL, 34(1-2).Geeraerts, D., (1993), Vagueness's puzzles, polysemy'svagaries, Cognitive Linguistics, 4.Kilgarriff, A., (1997), I don't believe in word senses,Computers and the Humanities, 31(2).Kilgarriff, A. and Palmer, M., (2000), Introduction tothe special issue on Senseval, Computers and theHumanities, 34(1-2):1-13.Kingsbury, P., and Palmer, M, (2002), From TreeBankto PropBank, Third International Conference onLanguage  Resources and Evaluation, LREC-02,Las Palmas, Canary Islands, Spain, May 28- June3.Marcus, M, (1994), The Penn TreeBank: A revisedcorpus design for extracting predicate argumentstructure, In Proceedings of the ARPA HumanLanguage Technology Workshop, Princeton, NJ.Ng, H. T., & Wang, B., & Chan, Y. S. (2003).Exploiting Parallel Texts for Word Sense Disam-biguation: An Empirical Study.
In the Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics (ACL-03).
Sapporo,Japan, July.Palmer, M., Dang, H. T., and Fellbaum, C., (to appear,2004), Making fine-grained and coarse-grainedsense distinctions, both manually and automati-cally, under revision for Natural Language Engi-neering.Pustejovsky, J.
(1991) The Generative Lexicon,  inComputational Linguistics 17(4).Pustejovsky, J.
(1995) The Generative Lexicon, Cam-bridge, MIT Press, Mass.
