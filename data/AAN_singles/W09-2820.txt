Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 103?104,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPJUNLG-MSR: A Machine Learning Approach of Main SubjectReference Selection with Rule Based ImprovementSamir GuptaDepartment of Computer Science andEngineering, Jadavpur University.Kolkata-700032, India.samir.ju@gmail.comSivaji BandopadhyayDepartment of Computer Science andEngineering, Jadavpur University.Kolkata-700032, India.sivaji_cse_ju@yahoo.comAbstractThe GREC-MSR task is to generate appropri-ate references to an entity in the context of apiece of discourse longer than a sentence.
InMSR ?09 run of this task, the main aim is toselect the actual main subject reference(MSR) from a list of given referential expres-sions that is appropriate in context.
We used amachine learning approach augmented withsome rules to select the most appropriate ref-erential expression.
Our approach uses thetraining set for learning and then combinessome of the rules found by observation to im-prove the system.1 IntroductionIn this paper we provide a description of our sys-tem for the GREC MSR task of Generation Chal-lenges 2009.
GREC-2.0 Corpus of 2,000Wikipedia introduction sections in which refer-ences to the main subject of the Wikipedia articlehave been annotated was provided to us by the or-ganizers.
The corpus was divided into five differ-ent domains like cities, countries, mountains,people and rivers.The basic approach we used was to develop abaseline system first by training the system on thetraining set.
This system then selects the most fre-quent referential expression based on a number ofparameters of the corresponding reference.
Afterevaluation on the development set we used the de-velopment set to deduce certain rules based on ob-servation and iteratively added these rules to thesystem and evaluated resulting performance.
Thusthe system development can be divided into twophases which are discussed in sections 2 and 3.2 Baseline System: Training and Classifi-cationThe machine learning approach we used for thebaseline system was domain independent andhence was build by populating a single databasewith the training set data.
First we parsed the con-tents of the XML files of the training sets using aJava DOM XML Parser.
Then we inserted thetraining set data into the database named grecwhich had two tables: parsed_ref and possi-ble_refex.
There is a one to many mapping frompossible_refex to parsed_ref.
The possible_refexcontains all possible REFEX elements i.e.
referen-tial expressions  possible while parsed_ref containsall the parsed references of the training set withattributes such as syncat, semcat, paragraph num-ber, reference number (with respect to a para-graph), sentence number and a foreign key refex idreferring to the possible_refex table.The prediction of the referential expression wasdone based on features such as the semantic cate-gory, syntactic category, paragraph number, refer-ence number with respect to a paragraph andsentence number of the referent.
One examplefrom the database is, if the semcat of the referenceis cities, syncat is np-subj, paragraph number is 2,ref number is 1 and sentence number equals 1 thenin 74% of the cases of the training set the referen-tial expression was with refex id=1 (i.e.type=common, emphatic=no, head=nominal and103case= plain) and refex id = 4 (i.e.
type=name, em-phatic=no, head=nominal and case= plain)  had thesecond highest count (19.6%).
Thus we selectedthe most frequent refex from the possible referen-tial expressions corresponding to the feature set ofthe reference, based on their count in the trainingset populated database.
These decision rules withtheir associated probabilities are stored in a tablewhich served as our model for classification.
Whena number of referential expressions from thealt_refex match from the list of the given refexesthen we select the refex with the longest surfaceform.
In certain case when the refex was not in thealt_refex element we select the second best casefrom our decision model.
Results of this intermedi-ate baseline system are given in Table 1.Domain StringAcc.Reg08typeAcc.MeanEditDis-tanceNorm.meanedit dis-tanceCities 0.404 0.495 1.657 0.575Countr.
0.468 0.576 1.467 0.471Mount.
0.567 0.646 1.192 0.380People 0.576 0.673 0.902 0.379Rivers 0.6 0.6 1.06 0.36Overall 0.532 0.62 1.205 0.421Table 1: Baseline Results3 Rule based ImprovementAfter the baseline system was evaluated on thedevelopment set we iteratively added some rules tooptimize the system output.
These rules are ap-plied only when a reference matches the belowstated condition, otherwise the result from thebaseline system was used.The different rules that we deduced are as follows:?
The referential expression is empty if itsimmediate preceding word is a conjunctionand the referent?s syncat is np-subj.
Thusthe surface form of the refex is null.?
In the people domain if the best case out-put from the baseline results in Reg-type  =?name?
and if earlier in the paragraph theperson?s full name has been referred to,then subsequent references will have ashorter version of the referential expres-sion i.e.
shorter surface form (example:Zinn?s instead of Howard Zinn?s)?
If the same sentence spans two or morereferences then generally a pronoun formis used if a noun has been used earlier.?
Generally common form of the noun isused instead of the baseline pronoun out-put if words like in, for, to, of, in precedesthe reference (maximum distance 3words).
This rule is applied to all domainsexcept people.The first and the last rules had some effect tothe system but the improvement from the otherrules was very negligible.
Final results are tabu-lated in Table 2.4 ResultsWe provide final results of our system in Table 2Script geval.pl was provided by the organizers forthis purpose.
We see that inclusion of the aboverules in the system increased it?s accuracy by al-most 4-5%.
More rules can be added to system bystudying cases of the training set which do not getclassified correctly by the best case baseline sys-tem.
Overall reg08 accuracy, precision and recallwere 66.4 %.Domain StringAcc.Reg08typeAcc.MeanEditDist.Norm.meaneditDist.Cities 0.434 0.525 1.596 0.544Countr.
0.5 0.619 1.381 0.431Mount.
0.583 0.663 1.158 0.363People 0.659 0.756 0.746 0.296Rivers 0.65 0.65 0.95 0.31Overall 0.575 0.664 1.12 0.377Table 2: Final ResultsReferencesAnja Belz and Albert Gatt.
2008.
Grec Main SubjectReference Generation Challenge 2009: Participants?Pack.http://www.nltg.brighton.ac.uk/research/genchal09Anja Belz, Eric Kow, Jette Viethen, Albert Gatt.
2008.The GREC Challenge 2008: Overview and Evalua-tion Results.
In Proceedings of the Fifth Interna-tional Natural Language Generation Conference(INLG-2008) pages 183-192.104
