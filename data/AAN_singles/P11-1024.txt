Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 230?238,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAn exponential translation model for target language morphologyMichael SubotinPaxfire, Inc.Department of Linguistics & UMIACS, University of Marylandmsubotin@gmail.comAbstractThis paper presents an exponential modelfor translation into highly inflected languageswhich can be scaled to very large datasets.
Asin other recent proposals, it predicts target-side phrases and can be conditioned on source-side context.
However, crucially for the taskof modeling morphological generalizations, itestimates feature parameters from the entiretraining set rather than as a collection of sepa-rate classifiers.
We apply it to English-Czechtranslation, using a variety of features captur-ing potential predictors for case, number, andgender, and one of the largest publicly avail-able parallel data sets.
We also describe gen-eration and modeling of inflected forms un-observed in training data and decoding proce-dures for a model with non-local target-sidefeature dependencies.1 IntroductionTranslation into languages with rich morphologypresents special challenges for phrase-based meth-ods.
Thus, Birch et al(2008) find that transla-tion quality achieved by a popular phrase-based sys-tem correlates significantly with a measure of target-side, but not source-side morphological complexity.Recently, several studies (Bojar, 2007; Avramidisand Koehn, 2009; Ramanathan et al, 2009; Yen-iterzi and Oflazer, 2010) proposed modeling target-side morphology in a phrase-based factored mod-els framework (Koehn and Hoang, 2007).
Underthis approach linguistic annotation of source sen-tences is analyzed using heuristics to identify rel-evant structural phenomena, whose occurrences arein turn used to compute additional relative frequency(maximum likelihood) estimates predicting target-side inflections.
This approach makes it difficultto handle the complex interplay between differentpredictors for inflections.
For example, the ac-cusative case is usually preserved in translation, sothat nouns appearing in the direct object position ofEnglish clauses tend to be translated to words withaccusative case markings in languages with richermorphology, and vice versa.
However, there areexceptions.
For example, some verbs that placetheir object in the accusative case in Czech may berendered as prepositional constructions in English(Naughton, 2005):David was looking for JanaDavid hledal JanuDavid searched Jana-ACCConversely, direct objects of some English verbscan be translated by nouns with genitive casemarkings in Czech:David asked Jana where Karel wasDavid zeptal se Jany kde je KarelDavid asked SELF Jana-GEN where is KarelFurthermore, English noun modifiers are oftenrendered by Czech possessive adjectives and a ver-bal complement in one language is commonly trans-lated by a nominalizing complement in another lan-guage, so that the part of speech (POS) of its headneed not be preserved.
These complications make itdifficult to model morphological phenomena using230closed-form estimates.
This paper presents an alter-native approach based on exponential phrase mod-els, which can straightforwardly handle feature setswith arbitrarily elaborate source-side dependencies.2 Hierarchical phrase-based translationWe take as our starting point David Chiang?s Hierosystem, which generalizes phrase-based translationto substrings with gaps (Chiang, 2007).
Considerfor instance the following set of context-free ruleswith a single non-terminal symbol:?A , A ?
?
?A1A2 , A1A2 ?
?A , A ?
?
?
d?A1 ide?esA2 , A1A2 ideas ?
?A , A ?
?
?
incolores , colorless ?
?A , A ?
?
?
vertes , green ?
?A , A ?
?
?
dormentA , sleepA ?
?A , A ?
?
?
furieusement , furiously ?It is one of many rule sets that would suffice togenerate the English translation 1b for the Frenchsentence 1a.1a.
d?
incolores ide?es vertes dorment furieusement1b.
colorless green ideas sleep furiouslyAs shown by Chiang (2007), a weighted gram-mar of this form can be collected and scored by sim-ple extensions of standard methods for phrase-basedtranslation and efficiently combined with a languagemodel in a CKY decoder to achieve large improve-ments over a state-of-the-art phrase-based system.The translation is chosen to be the target-side yieldof the highest-scoring synchronous parse consistentwith the source sentence.
Although a variety ofscores interpolated into the decision rule for phrase-based systems have been investigated over the years,only a handful have been discovered to be consis-tently useful.
In this work we concentrate on ex-tending the target-given-source phrase model1.3 Exponential phrase models with sharedfeaturesThe model used in this work is based on the familiarequation for conditional exponential models:1To avoid confusion with features of the exponential mod-els described below we shall use the term ?model?
rather than?feature?
for the terms interpolated using MERT.p(Y |X) =e~w?~f(X,Y )?Y ?
?GEN(X) e~w?~f(X,Y ?
)where ~f(X,Y ) is a vector of feature functions,~w is a corresponding weight vector, so that ~w ?~f(X,Y ) =?iwifi(X,Y ), and GEN(X) is aset of values corresponding to Y .
For a target-given-source phrase model the predicted outcomesare target-side phrases ry, the model is conditionedon a source-side phrase rx together with some con-text, and each GEN(X) consists of target phrasesry co-occurring with a given source phrase rx in thegrammar.Maximum likelihood estimation for exponentialmodel finds the values of weights that maximize thelikelihood of the training data, or, equivalently, itslogarithm:LL(~w) = logM?m=1p(Ym|Xm) =M?m=1log p(Ym|Xm)where the expressions range over all training in-stances {m}.
In this work we extend the objectiveusing an `2 regularizer (Ng, 2004; Gao et al, 2007).We obtain the counts of instances and features fromthe standard heuristics used to extract the grammarfrom a word-aligned parallel corpus.Exponential models and other classifiers havebeen used in several recent studies to conditionphrase model probabilities on source-side context(Chan et al2007; Carpuat and Wu 2007a; Carpuatand Wu 2007b).
However, this has been gener-ally accomplished by training independent classi-fiers associated with different source phrases.
Thisapproach is not well suited to modeling target-language inflections, since parameters for the fea-tures associated with morphological markings andtheir predictors would be estimated separately frommany, generally very small training sets, therebypreventing the model from making precisely thekind of generalization beyond specific phrases thatwe seek to obtain.
Instead we continue the approachproposed in Subotin (2008), where a single modeldefined by the equations above is trained on all of thedata, so that parameters for features that are sharedby rule sets with difference source sides reflect cu-mulative feature counts, while the standard relative231frequency model can be obtained as a special caseof maximum likelihood estimation for a model con-taining only the features for rules.2 Recently, Jeonget al(2010) independently proposed an exponentialmodel with shared features for target-side morphol-ogy in application to lexical scores in a treelet-basedsystem.4 FeaturesThe feature space for target-side inflection modelsused in this work consists of features tracking thesource phrase and the corresponding target phrasetogether with its complete morphological tag, whichwill be referred to as rule features for brevity.
Thefeature space also includes features tracking thesource phrase together with the lemmatized repre-sentation of the target phrase, called lemma featuresbelow.
Since there is little ambiguity in lemmati-zation for Czech, the lemma representations werefor simplicity based on the most frequent lemmafor each token.
Finally, we include features associ-ating aspects of source-side annotation with inflec-tions of aligned target words.
The models includefeatures for three general classes of morphologicaltypes: number, case, and gender.
We add inflec-tion features for all words aligned to at least one En-glish verb, adjective, noun, pronoun, or determiner,excepting definite and indefinite articles.
A sepa-rate feature type marks cases where an intended in-flection category is not applicable to a target wordfalling under these criteria due to a POS mismatchbetween aligned words.4.1 NumberThe inflection for number is particularly easy tomodel in translating from English, since it is gen-erally marked on the source side, and POS taggersbased on the Penn treebank tag set attempt to inferit in cases where it is not.
For word pairs whosesource-side word is a verb, we add a feature markingthe number of its subject, with separate features fornoun and pronoun subjects.
For word pairs whosesource side is an adjective, we add a feature markingthe number of the head of the smallest noun phrasethat contains it.2Note that this model is estimated from the full parallel cor-pus, rather than a held-out development set.4.2 CaseAmong the inflection types of Czech nouns, the onlytype that is not generally observed in English anddoes not belong to derivational morphology is in-flection for case.
Czech marks seven cases: nomi-nal, genitive, dative, accusative, vocative, locative,and instrumental.
Not all of these forms are overtlydistinguished for all lexical items, and some wordsthat function syntactically as nouns do not inflect atall.
Czech adjectives also inflect for case and theircase has to match the case of their governing noun.However, since the source sentence and its anno-tation contain a variety of predictors for case, wemodel it using only source-dependent features.
Thefollowing feature types for case were included:?
The structural role of the aligned source wordor the head of the smallest noun phrase con-taining the aligned source word.
Features wereincluded for the roles of subject, direct object,and nominal predicate.?
The preposition governing the smallest nounphrase containing the aligned source word, ifit is governed by a preposition.?
An indicator for the presence of a possessivemarker modifying the aligned source word orthe head of the smallest noun phrase containingthe aligned source word.?
An indicator for the presence of a numeralmodifying the aligned source word or the headof the smallest noun phrase containing thealigned source word.?
An indication that aligned source word modi-fied by quantifiers many, most, such, or half.These features would be more properly definedbased on the identity of the target word alignedto these quantifiers, but little ambiguity seemsto arise from this substitution in practice.?
The lemma of the verb governing the alignedsource word or the head of the smallest nounphrase containing the aligned source word.This is the only lexicalized feature type used inthe model and we include only those featureswhich occur over 1,000 times in the trainingdata.232wx1wx2wx3wy1wy2wy3wx4r1r2observed dependency: wx2?
wx3assumed dependency: wy1?
wy3Figure 1: Inferring syntactic dependencies.Features corresponding to aspects of the sourceword itself and features corresponding to aspects ofthe head of a noun phrase containing it were treatedas separate types.4.3 GenderCzech nouns belong to one of three cases: feminine,masculine, and neuter.
Verbs and adjectives have toagree with nouns for gender, although this agree-ment is not marked in some forms of the verb.
Incontrast to number and case, Czech gender generallycannot be predicted from any aspect of the Englishsource sentence, which necessitates the use of fea-tures that depend on another target-side word.
This,in turn, requires a more elaborate decoding proce-dure, described in the next section.
For verbs weadd a feature associating the gender of the verb withthe gender of its subject.
For adjectives, we add afeature tracking the gender of the governing nouns.These dependencies are inferred from source-sideannotation via word alignments, as depicted in fig-ure 1, without any use of target-side dependencyparses.5 Decoding with target-side modeldependenciesThe procedure for decoding with non-local target-side feature dependencies is similar in its generaloutlines to the standard method of decoding with alanguage model, as described in Chiang (2007).
Thesearch space is organized into arrays called charts,each containing a set of items whose scores can becompared with one another for the purposes of prun-ing.
Each rule that has matched the source sen-tence belongs to a rule chart associated with itslocation-anchored sequence of non-terminal and ter-minal source-side symbols and any of its aspectswhich may affect the score of a translation hypothe-sis when it is combined with another rule.
In the caseof the language model these aspects include any ofits target-side words that are part of still incompleten-grams.
In the case of non-local target-side depen-dencies this includes any information about featuresneeded for this rule?s estimate and tracking sometarget-side inflection beyond it or features trackingtarget-side inflections within this rule and needed forcomputation of another rule?s estimate.
We shall re-fer to both these types of information as messages,alluding to the fact that it will need to be conveyed toanother point in the derivation to finish the compu-tation.
Thus, a rule chart for a rule with one non-terminal can be denoted as as?xi1i+1Axjj1+1, ?
?,where we have introduced the symbol ?
to representthe set of messages associated with a given item inthe chart.
Each item in the chart is associated witha score s, based on any submodels and heuristic es-timates that can already be computed for that itemand used to arrange the chart items into a priorityqueue.
Combinations of one or more rules that spana substring of terminals are arranged into a differ-ent type of chart which we shall call span charts.
Aspan chart has the form [i1, j1;?1], where ?1 is a setof messages, and its items are likewise prioritized bya partial score s1.The decoding procedure used in this work is basedon the cube pruning method, fully described in Chi-ang (2007).
Informally, whenever a rule chart iscombined with one or more span charts correspond-ing to its non-terminals, we select best-scoring itemsfrom each chart and update derivation scores by per-forming any model computations that become pos-sible once we combine the corresponding items.Crucially, whenever an item in one of the chartscrosses a pruning threshold, we discard the rest ofthat chart?s items, even though one of them couldgenerate a better-scoring partial derivation in com-233bination with an item from another chart.
It is there-fore important to estimate incomplete model scoresas well as we can.
We estimate these scores by com-puting exponential models using all features withoutnon-local dependencies.Schematically, our decoding procedure can be il-lustrated by three elementary cases.
We take theexample of computing an estimate for a rule whoseonly terminal on both sides is a verb and which re-quires a feature tracking the target-side gender in-flection of the subject.
We make use of a cachestoring all computed numerators and denominatorsof the exponential model, which makes it easy torecompute an estimate given an additional featureand use the difference between it and the incompleteestimate to update the score of the partial deriva-tion.
In the simplest case, illustrated in figure 2, thenon-local feature depends on the position within thespan of the rule?s non-terminal symbol, so that itsmodel estimate can be computed when its rule chartis combined with the span chart for its non-terminalsymbol.
This is accomplished using a feature mes-sage, which indicates the gender inflection for thesubject and is denoted as mf (i), where the indexi refers to the position of its ?recipient?.
Figure 3illustrates the case where the non-local feature liesoutside the rule?s span, but the estimated rule lies in-side a non-terminal of the rule which contains thefeature dependency.
This requires sending a rulemessage mr(i), which includes information aboutthe estimated rule (which also serves as a pointer tothe score cache) and its feature dependency.
The fi-nal example, shown in figure 4, illustrates the casewhere both types of messages need to be propagateduntil we reach a rule chart that spans both ends ofthe dependency.
In this case, the full estimate for arule is computed while combining charts neither ofwhich corresponds directly to that rule.A somewhat more formal account of the decod-ing procedure is given in figure 5, which shows apartial set of inference rules, generally following theformalism used in Chiang (2007), but simplifyingit in several ways for brevity.
Aside from the no-tation introduced above, we also make use of twoupdating functions.
The message-updating functionum(?)
takes a set of messages and outputs anotherset that includes those messages mr(k) and mf (k)whose destination k lies outside the span i, j of theASbAV1 2mf(2)ScorecacheFigure 2: Non-local dependency, case A.ASbAV1 2mr(1)ScorecacheFigure 3: Non-local dependency, case B.ASbAV1 2Scorecachemr(1)AdvA3mf(3)Figure 4: Non-local dependency, case C.234Figure 5: Simplified set of inference rules for decodingwith target-side model dependencies.chart.
The score-updating function us(?)
computesthose model estimates which can be completed us-ing a message in the set ?
and returns the differencebetween the new and old scores.6 Modeling unobserved target inflectionsAs a consequence of translating into a morphologi-cally rich language, some inflected forms of targetwords are unobserved in training data and cannotbe generated by the decoder under standard phrase-based approaches.
Exponential models with sharedfeatures provide a straightforward way to estimateprobabilities of unobserved inflections.
This is ac-complished by extending the sets of target phrasesGEN(X) over which the model is normalized byincluding some phrases which have not been ob-served in the original sets.
When additional rulefeatures with these unobserved target phrases are in-cluded in the model, their weights will be estimatedeven though they never appear in the training exam-ples (i.e, in the numerator of their likelihoods).We generate unobserved morphological variantsfor target phrases starting from a generation proce-dure for target words.
Morphological variants forwords were generated using the U?FAL MORPHOtool (Kolovratn?
?k and Pr?ikryl, 2008).
The forms pro-duced by the tool from the lemma of an observed in-flected word form were subjected to several restric-tions:?
For nouns, generated forms had to match theoriginal form for number.?
For verbs, generated forms had to match theoriginal form for tense and negation.?
For adjectives, generated forms had to matchthe original form for degree of comparison andnegation.?
For pronouns, excepting relative and interrog-ative pronouns, generated forms had to matchthe original form for number, case, and gender.?
Non-standard inflection forms for all POS wereexcluded.The following criteria were used to select rules forwhich expanded inflection sets were generated:?
The target phrase had to contain exactly oneword for which inflected forms could be gen-erated according to the criteria given above.?
If the target phrase contained prepositions ornumerals, they had to be in a position not ad-jacent to the inflected word.
The rationale forthis criterion was the tendency of prepositionsand numerals to determine the inflection of ad-jacent words.?
The lemmatized form of the phrase had to ac-count for at least 25% of target phrases ex-tracted for a given source phrase.The standard relative frequency estimates for thep(X|Y ) phrase model and the lexical models do notprovide reasonable values for the decoder scores forunobserved rules and words.
In contrast, exponen-tial models with surface and lemma features can bestraightforwardly trained for all of them.
For the ex-periments described below we trained an exponen-tial model for the p(Y |X) lexical model.
For greaterspeed we estimate the probabilities for the othertwo models using interpolated Kneser-Ney smooth-ing (Chen and Goodman, 1998), where the surfaceform of a rule or an aligned word pair plays to roleof a trigram, the pairing of the source surface formwith the lemmatized target form plays the role of abigram, and the source surface form alone plays therole of a unigram.2357 Corpora and baselinesWe investigate the models using the 2009 editionof the parallel treebank from U?FAL (Bojar andZ?abokrtsky?, 2009), containing 8,029,801 sentencepairs from various genres.
The corpus comes withautomatically generated annotation and a random-ized split into training, development, and testingsets.
Thus, the annotation for the development andtesting sets provides a realistic reflection of whatcould be obtained for arbitrary source text.
TheEnglish-side annotation follows the standards of thePenn Treebank and includes dependency parses andstructural role labels such as subject and object.
TheCzech side is labeled with several layers of annota-tion, of which only the morphological tags and lem-mas are used in this study.
The Czech tags followthe standards of the Prague Dependency Treebank2.0.The impact of the models on translation accuracywas investigated for two experimental conditions:?
Small data set: trained on the news portion ofthe data, containing 140,191 sentences; devel-opment and testing sets containing 1500 sen-tences of news text each.?
Large data set: trained on all the training data;developing and testing sets each containing1500 sentences of EU, news, and fiction data inequal portions.
The other genres were excludedfrom the development and testing sets becausemanual inspection showed them to contain aconsiderable proportion of non-parallel sen-tences pairs.All conditions use word alignments produced bysequential iterations of IBM model 1, HMM, andIBM model 4 in GIZA++, followed by ?diag-and?symmetrization (Koehn et al, 2003).
Thresholdsfor phrase extraction and decoder pruning were setto values typical for the baseline system (Chiang,2007).
Unaligned words at the outer edges of rulesor gaps were disallowed.
A 5-gram language modelwith modified interpolated Kneser-Ney smoothing(Chen and Goodman, 1998) was trained by theSRILM toolkit (Stolcke, 2002) on a set of 208 mil-lion running words of text obtained by combiningthe monolingual Czech text distributed by the 2010ACL MT workshop with the Czech portion of thetraining data.
The decision rule was based on thestandard log-linear interpolation of several models,with weights tuned by MERT on the developmentset (Och, 2003).
The baselines consisted of the lan-guage model, two phrase translation models, twolexical models, and a brevity penalty.The proposed exponential phrase model containsseveral modifications relative to a standard phrasemodel (called baseline A below) with potential toimprove translation accuracy, including smoothedestimates and estimates incorporating target-sidetags.
To gain better insight into the role played bydifferent elements of the model, we also tested a sec-ond baseline phrase model (baseline B), which at-tempted to isolate the exponential model itself fromauxiliary modifications.
Baseline B was differentfrom the experimental condition in using a gram-mar limited to observed inflections and in replac-ing the exponential p(Y |X) phrase model by a rel-ative frequency phrase model.
It was different frombaseline A in computing the frequencies for thep(Y |X) phrase model based on counts of taggedtarget phrases and in using the same smoothed es-timates in the other models as were used in the ex-perimental condition.8 Parameter estimationParameter estimation was performed using a modi-fied version of the maximum entropy module fromSciPy (Jones et al, 2001) and the LBFGS-B algo-rithm (Byrd et al, 1995).
The objective includedan `2 regularizer with the regularization trade-offset to 1.
The amount of training data presented apractical challenge for parameter estimation.
Sev-eral strategies were pursued to reduce the computa-tional expenses.
Following the approach of Mannet al(2009), the training set was split into manyapproximately equal portions, for which parameterswere estimated separately and then averaged for fea-tures observed in multiple portions.
The sets of tar-get phrases for each source phrase prior to genera-tion of additional inflected variants were truncatedby discarding extracted rules which were observedwith frequency less than the 200-th most frequenttarget phrase for that source phrase.Additional computational challenges remained236due to an important difference between models withshared features and usual phrase models.
Featuresappearing with source phrases found in developmentand testing data share their weights with features ap-pearing with other source phrases, so that filteringthe training set for development and testing data af-fects the solution.
Although there seems to be noreason why this would positively affect translationaccuracy, to be methodologically strict we estimateparameters for rule and lemma features without in-flection features for larger models, and then com-bine them with weights for inflection feature esti-mated from a smaller portion of training data.
Thisshould affect model performance negatively, since itprecludes learning trade-offs between evidence pro-vided by the different kinds of features, and there-fore it gives a conservative assessment of the re-sults that could be obtained at greater computationalcosts.
The large data model used parameters for theinflection features estimated from the small data set.In the runs where exponential models were used theyreplaced the corresponding baseline phrase transla-tion model.9 Results and discussionTable 1 shows the results.
Aside from the two base-lines described in section 7 and the full exponen-tial model, the table also reports results for an ex-ponential model that excluded gender-based features(and hence non-local target-side dependencies).
Thehighest scores were achieved by the full exponentialmodel, although baseline B produced surprisinglydisparate effects for the two data sets.
This sug-gests a complex interplay of the various aspects ofthe model and training data whose exploration couldfurther improve the scores.
Inclusion of gender-based features produced small but consistent im-provements.
Table 2 shows a summary of the gram-mars.We further illustrate general properties of thesemodels using toy examples and the actual param-eters estimated from the large data set.
Table 3shows representative rules with two different sourcesides.
The column marked ?no infl.?
shows modelestimates computed without inflection features.
Onecan see that for both rule sets the estimated probabil-ities for rules observed a single time is only slightlyCondition Small set Large setBaseline A 0.1964 0.2562Baseline B 0.2067 0.2522Expon-gender 0.2114 0.2598Expon+gender 0.2128 0.2615Table 1: BLUE scores on testing.
See section 7 for adescription of the baselines.Condition Total rules Observed rulesSmall set 17,089,850 3,983,820Large set 39,349,268 23,679,101Table 2: Grammar sizes after and before generation ofunobserved inflections (all filtered for dev/test sets).higher than probabilities for generated unobservedrules.
However, rules with relatively high countsin the second set receive proportionally higher es-timates, while the difference between the singletonrule and the most frequent rule in the second set,which was observed 3 times, is smoothed away toan even greater extent.
The last two columns showmodel estimates when various inflection features areincluded.
There is a grammatical match betweennominative case for the target word and subject po-sition for the aligned source word and between ac-cusative case for the target word and direct objectrole for the aligned source word.
The other pair-ings represent grammatical mismatches.
One cansee that the probabilities for rules leading to correctcase matches are considerably higher than the alter-natives with incorrect case matches.rx Count Case No infl.
Sb Obj1 1 Dat 0.085 0.037 0.0351 3 Acc 0.086 0.092 0.2041 0 Nom 0.063 0.416 0.0632 1 Instr 0.007 0.002 0.0032 31 Nom 0.212 0.624 0.1692 0 Acc 0.005 0.002 0.009Table 3: The effect of inflection features on estimatedprobabilities.23710 ConclusionThis paper has introduced a scalable exponentialphrase model for target languages with complexmorphology that can be trained on the full parallelcorpus.
We have showed how it can provide esti-mates for inflected forms unobserved in the trainingdata and described decoding procedures for featureswith non-local target-side dependencies.
The resultssuggest that the model should be especially usefulfor languages with sparser resources, but that per-formance improvements can be obtained even for avery large parallel corpus.AcknowledgmentsI would like to thank Philip Resnik, Amy Weinberg,Hal Daume?
III, Chris Dyer, and the anonymous re-viewers for helpful comments relating to this work.ReferencesE.
Avramidis and P. Koehn.
2008.
Enriching Morpholog-ically Poor Languages for Statistical Machine Transla-tion.
In Proc.
ACL 2008.A.
Birch, M. Osborne and P. Koehn.
2008.
PredictingSuccess in Machine Translation.
The Conference onEmpirical Methods in Natural Language Processing(EMNLP), 2008.O.
Bojar.
2007.
English-to-Czech Factored MachineTranslation.
In Proceedings of the Second Workshopon Statistical Machine Translation.O.
Bojar and Z.
Z?abokrtsky?.
2009.
Large ParallelTreebank with Rich Annotation.
Charles University,Prague.
http://ufal.mff.cuni.cz/czeng/czeng09/, 2009.R.
H. Byrd, P. Lu and J. Nocedal.
1995.
A Limited Mem-ory Algorithm for Bound Constrained Optimization.SIAM Journal on Scientific and Statistical Computing,16(5), pp.
1190-1208.M.
Carpuat and D. Wu.
2007a.
Improving StatisticalMachine Translation using Word Sense Disambigua-tion.
Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL 2007).M.
Carpuat and D. Wu.
2007b.
How Phrase Sense Dis-ambiguation outperforms Word Sense Disambiguationfor Statistical Machine Translation.
11th Conferenceon Theoretical and Methodological Issues in MachineTranslation (TMI 2007)Y.S.
Chan, H.T.
Ng, and D. Chiang.
2007.
Word sensedisambiguation improves statistical machine transla-tion.
In Proc.
ACL 2007.S.F.
Chen and J.T.
Goodman.
1998.
An EmpiricalStudy of Smoothing Techniques for Language Mod-eling.
Technical Report TR-10-98, Computer ScienceGroup, Harvard University.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201-228.J.
Gao, G. Andrew, M. Johnson and K. Toutanova.
2007.A Comparative Study of Parameter Estimation Meth-ods for Statistical Natural Language Processing.
InProc.
ACL 2007.M.
Jeong, K. Toutanova, H. Suzuki, and C. Quirk.
2010.A Discriminative Lexicon Model for Complex Mor-phology.
The Ninth Conference of the Association forMachine Translation in the Americas (AMTA-2010).E.
Jones, T. Oliphant, P. Peterson and others.SciPy: Open source scientific tools for Python.http://www.scipy.org/P.
Koehn and H. Hoang.
2007.
Factored translation mod-els.
The Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), 2007.P.
Koehn, F.J. Och, and D. Marcu.
2003.
StatisticalPhrase-Based Translation.
In Proceedings of the Hu-man Language Technology Conference (HLT-NAACL2003).D.
Kolovratn?
?k and L. Pr?ikryl.
2008.
Pro-grama?torska?
dokumentace k projektu Morfo.http://ufal.mff.cuni.cz/morfo/, 2008.G.
Mann, R. McDonald, M. Mohri, N. Silberman, D.Walker.
2009.
Efficient Large-Scale DistributedTraining of Conditional Maximum Entropy Models.Advances in Neural Information Processing Systems(NIPS), 2009.J.
Naughton.
2005.
Czech.
An Essential Grammar.
Rout-ledge, 2005.A.Y.
Ng.
2004.
Feature selection, L1 vs. L2 regular-ization, and rotational invariance.
In Proceedings ofthe Twenty-first International Conference on MachineLearning.F.J.
Och.
2003.
Minimum Error Rate Training for Statis-tical Machine Translation.
In Proc.
ACL 2003.A.
Ramanathan, H. Choudhary, A. Ghosh, P. Bhat-tacharyya.
2009.
Case markers and Morphology: Ad-dressing the crux of the fluency problem in English-Hindi SMT.
In Proc.
ACL 2009.A.
Stolcke.
2002.
SRILM ?
An Extensible LanguageModeling Toolkit.
International Conference on Spo-ken Language Processing, 2002.M.
Subotin.
2008.
Generalizing Local Translation Mod-els.
Proceedings of SSST-2, Second Workshop onSyntax and Structure in Statistical Translation.R.
Yeniterzi and K. Oflazer.
2010.
Syntax-to-Morphology Mapping in Factored Phrase-Based Sta-tistical Machine Translation from English to Turkish.In Proc.
ACL 2010.238
