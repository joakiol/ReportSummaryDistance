Combining Segmenter and Chunker for Chinese Word SegmentationMasayuki Asahara, Chooi Ling Goh, Xiaojie Wang, Yuji MatsumotoGraduate School of Information ScienceNara Institute of Science and Technology, Japan{masayu-a,ling-g,xiaoji-w,matsu}@is.aist-nara.ac.jpAbstractOur proposed method is to use a HiddenMarkov Model-based word segmenter and aSupport Vector Machine-based chunker forChinese word segmentation.
Firstly, input sen-tences are analyzed by the Hidden MarkovModel-based word segmenter.
The word seg-menter produces n-best word candidates to-gether with some class information and confi-dence measures.
Secondly, the extracted wordsare broken into character units and each char-acter is annotated with the possible word classand the position in the word, which are thenused as the features for the chunker.
Finally, theSupport Vector Machine-based chunker bringscharacter units together into words so as to de-termine the word boundaries.1 MethodsWe participate in the closed test for all four sets of datain Chinese Word Segmentation Bakeoff.
Our method isbased on the following two steps:1.
The input sentence is segmented into a word se-quence by Hidden Markov Model-based word seg-menter.
The segmenter assigns a word class witha confidence measure for each word at the hiddenstates.
The model is trained by Baum-Welch algo-rithm.2.
Each character in the sentence is annotated with theword class tag and the position in the word.
Then-best word candidates derived from the word seg-menter are also extracted as the features.
A sup-port vector machine-based chunker corrects the er-rors made by the segmenter using the extracted fea-tures.We will describe each of these steps in more details.1.1 Hidden Markov Model-based Word SegmenterOur word segmenter is based on Hidden Markov Model(HMM).
We first decide the number of hidden states(classes) and assume that the each word can belong toall the classes with some probability.
The problem is de-fined as a search for the sequence of word classes C =c1, .
.
.
, cn given a word sequence W = w1, .
.
.
, wn.
Thetarget is to find W and C for a given input S that maxi-mizes the following probability:argmaxW,CP (W |C)P (C)We assume that the word probability P (W |C) is con-strained only by its word class, and that the class prob-ability P (C) is constrained only by the class of the pre-ceding word.
These probabilities are estimated by theBaum-Welch algorithm using the training material (See(Manning and Schu?tze., 1999)).
The learning process isbased on the Baum-Welch algorithm and is the same asthe well-known use of HMM for part-of-speech taggingproblem, except that the number of states are arbitrarilydetermined and the initial probabilities are randomly as-signed in our model.1.2 Correction by Support Vector Machine-basedChunkerWhile the HMM-based word segmenter achieves goodaccuracy for known words, it cannot identify compoundwords and out-of-vocabulary words.
Therefore, we in-troduce a Support Vector Machine(below SVM)-basedchunker (Kudo and Matsumoto, 2001) to cover the er-rors made by the segmenter.
The SVM-based chunkerre-assigns new word boundaries to the output of the seg-menter.An SVM (Vapnik, 1998) is a binary classifier.
Sup-pose we have a set of training data for a binary classproblem: (x1, y1), .
.
.
, (xN , yN ), where xi ?
Rn is afeature vector of the i th sample in the training data andyi ?
{+1,?1} is the label of the sample.
The goal is tofind a decision function which accurately predicts y foran unseen x.
An SVM classifier gives a decision functionf(x) for an input vector x wheref(x) = sign(?zi?SV?iyiK(x, zi) + b).f(x) = +1 means that x is a positive member, andf(x) = ?1 means that x is a negative member.
The vec-tors zi are called support vectors, which receive a non-zero weight ?i.
Support vectors and the parameters aredetermined by solving a quadratic programming prob-lem.
K(x, z) is a kernel function which maps vectorsinto a higher dimensional space.
We use a polynomialkernel of degree 2 given by K(x, z) = (1 + x ?
z)2.The SVM classifier determines the position tag foreach character.
We introduce the word class tag as thefeature, which is generated by the word segmenter.
Sincewe perform chunking by character units, the feature usedby the classifier will be the information for the characterunit.The training data for our SVM-based chunker is con-structed from the output of the HMM-based word seg-menter defined in the previous section.
In the currentsetting, the HMM produces all the possible tags (classlabels) for each of the word within a predefined probabil-ity bound.
All the words in the output are then segmentedinto characters, and each of the characters is tagged withpairs of a word class and a position tag.
For example,in the paired tag ?0-B?, ?0?
is a class label of the wordwhich the character belongs to and ?B?
indicates the char-acter?s position in the word.
The number of classes isdetermined in advance of the HMM learning.
The po-sition tag consists of the following four tags (S/B/E/I):S means a single-character word; B is the first charac-ter in a multi-character word; E is the last character in amulti-character word; I is the intermediate character in amulti-character word longer than 2 characters.
As shownin Figure 1, we set the HMM-based word segmenter toproduce the classes of n-best word candidates to take intoaccount multiple possibility of word boundaries.The correct word boundary can be defined by assigningeither of two kinds of tags to each of the characters.
Lookat the rightmost column of Figure 1 named as ?ChunkerOutputs.?
The label ?B?
in this column shows that thecharacter is the first character of a correct word, and ?I?shows that the character is the other part of a word.
Thismeans that the preceding positions of ?B?
tags are theword boundaries.Those two tags correspond to the two classes of theSVM chunker: In the training (and test) phrase, we usewindow size of two characters to the left and right direc-tion to learn (and estimate) the class for a character.
Forexample, the shadowed parts in Figure 1 are used as theFigure 1: The Extracted Features for the Chunkerfeatures to learn (or estimate) the word boundary tag ?I?for the character ?
?.2 Model ValidationTo find out the best setting of learning, we would like todetermine ?the number of word classes?
and ?the depth ofn-best word candidates?
by using some sort of confidencemeasure.
We perform validation experiments for thesetwo types of parameters by using the training materialprovided.2.1 Validation Tests for HMM-based WordSegmenterThe first validation experiment is to determine ?the num-ber of word classes?
of the HMM.
80% of the material isused for the HMM training, and the other 20% is used asthe validation set.
We test two settings for the number ofclasses ?
5 & 10.
The results are shown in Table 1.Table 1: Validation Results for HMMData # of classes Rec.
Prec.
FAS 5 0.845 0.768 0.804AS 10 0.900 0.857 0.878CTB 5 0.909 0.844 0.875CTB 10 0.912 0.848 0.879HK 5 0.867 0.742 0.799HK 10 0.867 0.741 0.799PK 5 0.942 0.902 0.921PK 10 0.944 0.905 0.924In most cases, models perform slightly better with theincreasing of the number of classes.
When the corpussize is large like the Academia Sinica data, this tendencybecomes more significant.Whereas it is known that the Baum Welch algorithm isvery sensitive to the initialization of the classes, we ran-domly assigned the initial classes without making mucheffort.
There are two reasons: (1) Since the word seg-menter outputs are used as the clues to the chunker in ourmethod, we only need some consistent class annotations.
(2) The initial classes did not affect on the word segmen-tation accuracy in our pilot experiments.2.2 Validation Tests for SVM-based ChunkerThe second validation test is for the chunking model todetermine both ?the number of word classes?
and ?thedepth of the n-best candidates?.
80% of the material usedfor the HMM training, another 10% is used for the chunk-ing model training and the last 10% is used for the val-idation test.
The results are shown in Table 2, 3 and 4.Since the training of this model is time- and resource-consuming, the Academia Sinica data being very largecould not get enough time to finish the validation test.Table 2: Validation Results (CTB) for Chunking# of classes n-best Rec.
Prec.
F5 1 0.957 0.930 0.9435 2 0.957 0.931 0.9445 3 0.957 0.930 0.9435 4 0.957 0.930 0.94310 1 0.956 0.929 0.94310 2 0.957 0.928 0.94210 3 0.956 0.929 0.94210 4 0.955 0.928 0.941Table 3: Validation Results (HK) for Chunking# of classes n-best Rec.
Prec.
F5 1 0.853 0.793 0.8225 2 0.859 0.799 0.8285 3 0.859 0.799 0.8285 4 0.859 0.800 0.82810 1 0.856 0.793 0.82310 2 0.858 0.797 0.82610 3 0.857 0.796 0.82610 4 0.858 0.797 0.826The results show that the chunker actually improvesthe word segmentation accuracy compared with the out-put of the HMM word segmenter for these three data sets.The segmentation errors made by the word segmenter forcompound words and unknown words are corrected.
TheTable 4: Validation Results (PK) for Chunking# of classes n-best Rec.
Prec.
F5 1 0.960 0.934 0.9475 2 0.961 0.935 0.9485 3 0.962 0.936 0.9495 4 0.962 0.935 0.94810 1 0.961 0.932 0.94610 2 0.962 0.935 0.94810 3 0.961 0.934 0.94710 4 0.961 0.934 0.947improvement in Chinese Treebank (CTB) data set is sig-nificant, because the data set contains many compoundwords.There is no significant difference in the results betweenthe different depths of n-best answers.
Still, we choosethe best model for the test materials among them.
If weneed to have a faster analyzer, we should employ only thebest answer of the word segmentation.For the HMM, the larger number of classes tends toget better accuracy than smaller ones.
However, for thechunking model, the result is the other way round.
Themodel with the smaller number of classes gets slightlybetter accuracy.
So, there should be trade-off betweensmaller and larger number of classes.3 Final Models for Test MaterialFor the final models, 80% of the training material is usedfor HMM training and 100% of the material is used forthe chunking model training.
The parameters, namely?the number of word classes?
and ?the depth of n-bestword candidates?, are determined by the validation testsdescribed in Section 2.
While there is no significant dif-ference between the depths of n-best answers, we choosethe best model among them for the testing.
The parame-ters are shown in Table 7.We cannot create the model using all the originalAcademia Sinica data because of its large size.
Therefore,we use 80% of the data for HMM training (5 classes) andonly 10% for chunking model training (with only the bestcandidates).Table 7: The Models for the Test Material?
with respect to F-Measure in Our Validation TestData # of classes n-best FAS 5 1 N/ACTB 5 2 0.943HK 5 4 0.828PK 5 3 0.948Table 5: Throughput Speeds (characters per second)Data Word Seg.
(# of words) Fea.
Ext.
(n-best) Chunker (# of SV) Total SpeedAS 57000 (462750) 7640 (Only Best) 279 (96452) 241CTB 54400 (77324) 4040 (to 2nd Best) 894 (16736) 671HK 38900 (93231) 3870 (to 4th Best) 649 (14904) 524PK 57400 (215865) 6209 (to 3rd Best) 254 (49736) 200Table 6: Results for the Test MaterialsData T. Rec.
T. Prec.
F OOV Rec.
IV Rec.
RankingAS 0.944 0.945 0.945 0.574 0.952 3rd/6CTB 0.852 0.807 0.829 0.412 0.949 8th/10HK 0.940 0.908 0.924 0.415 0.980 5th/6PK 0.933 0.916 0.924 0.357 0.975 2nd/44 Throughput SpeedsAs described, our system is based on three modules:HMM-based word segmenter, Feature extractor andSVM-based chunker.
The word segmenter is composedby ChaSen (written in C/C++) (Matsumoto et.
al., 2003)which is adopted for GB/Big5 encoding.
The featureextractor is written in Perl.
The SVM-based chunker iscomposed by YamCha (written in C++) (Kudo and Mat-sumoto, 2001).Table 5 shows the speeds 1 of the three modules indi-vidually and of the total system.
?# of words?
means thesize of the word segmenter lexicon.
Note that, if a wordbelongs to more than one class, we regard them as differ-ent words in our definition.
?# of SV?
means the numberof support vectors in the chunker.
The total system speeddepends highly on that of the chunker.
It is known thatthe speed of SVM classifiers depends on the number ofsupport vectors and the number of features.5 ConclusionWe presented our method for Chinese Word Segmenta-tion Bakeoff in 2nd SIGHAN Workshop.
The results forthe test materials are shown in Table 6.
The proposedmethod is purely corpus-based statistical/machine learn-ing method.
Although we did not incorporate any heuris-tic rules (e.g.
part-of-speeches, functional words andconcatenation for numbers) into the model, the methodachieved considerable accuracy for the word segmenta-tion task.AcknowledgmentsWe thank Mr. Taku Kudo of NAIST for his developmentof the SVM-based chunker YamCha.1The throughput speeds are measured on a machine: In-tel(R) Xeon(TM) CPU 2.80GHz ?
2, Memory 4GB, RedHatLinux 9.ReferencesT.
Kudo and Y. Matsumoto.
2001.
Chunking with Sup-port Vector Machines.
In Proc.
of NAACL 2001, pages192?199.C.
D. Manning and H. Schu?tze.
1999.
Foundation ofStatistical Natural Language Processing.
Chapter 9.Markov Models, pages 317?340.Y.
Matsumoto, A. Kitauchi, T. Yamashita, Y. Hirano, K.Takaoka and M. Asahara 2003.
Morphological Ana-lyzer ChaSen-2.3.0 Users Manual Tech.
Report.
NaraInstitute of Science and Technology, Japan.L.
A. Ramshaw and M. P. Marcus.
1995 Text chunkingusing transformation-bases learning In Proc.
of the 3rdWorkshop on Very Large Corpora, pages 83?94.V.
N. Vapnik.
1998.
Statistical Learning Theory.
AWiley-Interscience Publication.
