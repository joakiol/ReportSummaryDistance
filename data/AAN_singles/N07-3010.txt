Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 37?40,Rochester, April 2007. c?2007 Association for Computational LinguisticsUnsupervised Natural Language Processing using Graph ModelsChris BiemannNLP Dept., University of LeipzigJohannisgasse 2604103 Leipzig, Germanybiem@informatik.uni-leipzig.deAbstractIn the past, NLP has always been basedon the explicit or implicit use of linguisticknowledge.
In classical computer linguis-tic applications explicit rule based ap-proaches prevail, while machine learningalgorithms use implicit knowledge forgenerating linguistic knowledge.
Thequestion behind this work is: how far canwe go in NLP without assuming explicitor implicit linguistic knowledge?
Howmuch efforts in annotation and resourcebuilding are needed for what level of so-phistication in text processing?
This worktries to answer the question by experi-menting with algorithms that do not pre-sume any linguistic knowledge in thesystem.
The claim is that the knowledgeneeded can largely be acquired by know-ledge-free and unsupervised methods.Here, graph models are employed for rep-resenting language data.
A new graphclustering method finds related lexicalunits, which form word sets on variouslevels of homogeneity.
This is exempli-fied and evaluated on language separationand unsupervised part-of-speech tagging,further applications are discussed.1 Introduction1.1 Unsupervised and Knowledge-FreeA frequent remark on work dealing with unsuper-vised methods in NLP is the question: ?Why nottake linguistic knowledge into account??
While forEnglish, annotated corpora, classification exam-ples, sets of rules and lexical semantic word nets ofhigh coverage do exist, this does not reflect thesituation for most of even the major world lan-guages.
Further, as e.g.
Lin (1997) notes, hand-made and generic resources often do not fit theapplication domain, whereas resources createdfrom and for the target data will not suffer fromthese discrepancies.Shifting the workload from creating resourcesmanually to developing generic methods, a one-size-fits-all solution needing only minimal adapta-tion to new domains and other languages comesinto reach.1.2 Graph ModelsThe interest in incorporating graph models intoNLP arose quite recently, and there is still a highpotential exploiting this combination (cf.
Wid-dows, 2005).
An important parallelism betweenhuman language and network models is the smallworld structure of lexical networks both builtmanually and automatically (Steyvers andTenenbaum, 2005), providing explanation forpower-law distributions like Zipf?s law and others,see Biemann (2007).
For many problems in NLP, agraph representation is an intuitive, natural anddirect way to represent the data.The pure vector space model (cf.
Sch?tze,1993) is not suited to highly skewed distributionsomni-present in natural language.
Computationallyexpensive, sometimes lossy transformations haveto be applied for effectiveness and efficiency inprocessing.
Graph models are a veritable alterna-tive, as the equivalent of zero-entries in the vectorrepresentation are neither represented nor have to37be processed, rendering dimensionality reductiontechniques unnecessary while still retaining theexact information.1.3 RoadmapFor the entirety of this research, nothing more isrequired as input data than plain, tokenized text,separated into sentences.
This is surely quite a bitof knowledge that is provided to the system, butunsupervised word boundary and sentence bound-ary detection is left for future work.
Three steps areundertaken to identify similar words on differentlevels of homogeneity: same language, same part-of-speech, or same distributional properties.
Figure1 shows a coarse overview of the processing stepsdiscussed in this work.Figure 1: Coarse overview: From multilingual in-put to typed relations and instances2 Methods in Unsupervised ProcessingHaving at hand neither explicit nor implicit knowl-edge, but in turn the goal of identifying structure ofequivalent function, the only possibility that is leftin unsupervised and knowledge-free processing isstatistics and clustering.2.1 Co-occurrence StatisticsAs a building block, co-occurrence statistics areused in several components of the system de-scribed here.
A significance measure for co-occurrence is a means to distinguish between ob-servations that are there by chance and effects thattake place due to an underlying structure.Throughout, the likelihood ratio (Dunning, 1993)is used as significance measure because of its sta-ble performance in various evaluations, yet manymore measures are possible.
Dependent on the con-text range in co-occurrence calculation, they willbe called sentence-based or neighbor-based co-occurrences in the remainder of this paper.
Theentirety of all co-occurrences of a corpus is calledits co-occurrence graph.
Edges are weighted by co-occurrence significance; often a threshold on edgeweight is applied.2.2 Graph ClusteringFor clustering graphs, a plethora of algorithms ex-ist that are motivated from a graph-theoretic view-point, but often optimize NP-complete measures(cf.
?
?ma and Schaeffer, 2005), making them non-applicable to lexical data that is naturally repre-sented in graphs with millions of vertices.
In Bie-mann and Teresniak (2005) and more detailed inBiemann (2006a), the Chinese Whispers (CW)Graph Clustering algorithm is described, which is arandomized algorithm with edge-linear run-time.The core idea is that vertices retain class labelswhich are inherited along the edges: In an updatestep, a vertex gets assigned the predominant labelin its neighborhood.
For initialization, all verticesget different labels, and after a handful of updatesteps per vertex, almost no changes in the labelingare observed ?
especially small world graphs con-verge fast.
CW can be viewed as a more efficientmodification and simplification of Markov ChainClustering (van Dongen, 2000), which requires fullmatrix multiplications.CW is parameter-free, non-deterministic andfinds the number of clusters automatically ?
a fea-ture that is welcome in NLP, where the number ofdesired clusters (e.g.
in word sense induction) isoften unknown.3 Results3.1 Language SeparationClustering the sentence-based co-occurrence graphof a multilingual corpus with CW, a languageseparator with almost perfect performance is im-plemented in the following way: The clusters rep-resent languages; a sentence gets assigned the labelof the cluster with the highest lexical overlap be-tween sentence and cluster.
The method is evalu-ated in (Biemann and Teresniak, 2005) by sortingmonolingual material that has been artificiallymixed together.
Dependent on similarities of lan-guages, the method works almost error-free fromabout 100-1,000 sentences per language on.
For38languages with different encoding, it is possible toun-mix corpora of size factors up to 10,000 for themonolingual parts.In a nutshell, comparable scores to supervisedlanguage identifiers are reached without training.Notice that the number of languages in a multilin-gual chunk of text is unknown.
This prohibits anyclustering method that needs the number of clus-ters to be specified be-forehand.3.2 Unsupervised POS TaggingUnlike in standard POS tagging, there is neither aset of predefined categories, nor annotation in atext.
As POS tagging is not a system for its ownsake, but serves as a preprocessing step for systemsbuilding upon it, the names and the number ofcategories are very often not important.The system presented in Biemann (2006b) usesCW clustering on graphs constructed by distribu-tional similarity to induce a lexicon of supposedlynon-ambiguous words w.r.t.
POS by selecting onlysafe bets and excluding questionable cases fromthe lexicon.
In this implementation, two clusteringsare combined, one for high and medium frequencywords, the other collecting medium and low fre-quency words.
High and medium frequency wordsare clustered by similarity of their stop word con-text feature vectors: a graph is built, including onlywords that are involved in highly similar pairs.Clustering this graph of typically 5,000 verticesresults in several hundred clusters, which are fur-ther used as POS categories.
To extend the lexicon,words of medium and low frequency are clusteredusing a graph that encodes similarity of neighbor-based co-occurrences.
Both clusterings are mappedby overlapping elements into a lexicon that pro-vides POS information for some 50,000 words.
Forobtaining a clustering on datasets of this size, aneffective algorithm like CW is crucial.
Using thislexicon, a trigram tagger with a morphological ex-tension is trained, which assigns a tag to every to-ken in the corpus.The tagsets obtained with this method are usu-ally more fine-grained than standard tagsets andreflect syntactic as well as semantic similarity.Figure 2 demonstrates the domain-dependence onthe tagset for MEDLINE: distinguishing e.g.
ill-nesses and error probabilities already in the tagsetmight be a valuable feature for relation extractiontasks.Size Sample words1613 colds, apnea, aspergilloma, ACS,breathlessness, lesions, perforations, ...1383 proven, supplied, engineered, distin-guished, constrained, omitted, ?589 dually, circumferentially, chronically,rarely, spectrally, satisfactorily, ...124 1-min, two-week, 4-min, 2-day, ?6 P<0.001, P<0.01, p<0.001, p<0.01, ...Figure 2: Some examples for MEDLINE tagset:Number of lex.
entries per tag and sample words.In Biemann (2006b), the tagger output was di-rectly compared to supervised taggers for English,German and Finnish via information-theoreticmeasures.
While it is possible to compare the con-tribution of different components of a system rela-tively along this scale, it only gives a poorimpression on the utility of the unsupervised tag-ger?s output.
Therefore, the tagger was evaluatedindirectly in machine learning tasks, where POStags are used as features.
Biemann et al (2007)report that for standard Named Entity Recognition,Word Sense Disambiguation and Chunking tasks,using unsupervised POS tags as features helpsabout as much as supervised tagging: Overall, al-most no significant differences between resultscould be observed, supporting the initial claim.3.3 Word Sense Induction (WSI)Co-occurrences are a widely used data source forWSI.
The methodology of Dorow and Widdows(2003) was adopted: for the focus word, obtain itsgraph neighborhood (all vertices that are connectedvia edges to the focus word vertex and edges be-tween these).
Clustering this graph with CW andregarding clusters as senses, this method yieldscomparable results to Bordag (2006), tested usingthe unsupervised evaluation framework presentedthere.
More detailed results are reported in Bie-mann (2006a).4 Further Work4.1 Word Sense Disambiguation (WSD)The encouraging results in WSI enable support inautomatic WSD systems.
As described by Agirre etal.
(2006), better performance can be expected ifthe WSI component distinguishes between a largenumber of so-called micro-senses.
This illustrates a39principle of unsupervised NLP: It is not importantto reproduce word senses found by introspection;rather, it is important that different usages of aword can be reliably distinguished, even if the cor-responding WordNet sense is split into several sub-senses.4.2 Distributional Thesaurus with RelationsIt is well understood that distributional similarityreflects semantic similarity and can be used toautomatically construct a distributional thesaurusfor frequent words (Lin, 1997; inter al).
Until now,most works aiming at semantic similarity rely on aparser that extracts dependency relations.
Theclaim here again is that similarity on parser outputmight be replaced by similarity on a pattern basis,(cf.
Davidov and Rappoport 2006).
For class-basedgeneralization in these patterns, the system de-scribed in section 3.2 might prove useful.
Prelimi-nary experiments revealed that similarity onsignificantly co-occurring patterns is able to pro-duce very promising similarity rankings.
A cluster-ing of these with CW leads to thesaurus entriescomparable to thesauri like Roget?s.Clustering not only words based on similarityof patterns, but also patterns based on similarity ofwords enables us to identify clusters of patternswith different relations they manifest.5 ConclusionThe claim of this work is that unsupervised NLPcan support and/or replace preprocessing steps inNLP that have previously been achieved by a largeamount of manual work, i.e.
annotation, rule con-struction or resource building.
This is proven em-pirically on the tasks of language identification andpart-of-speech tagging, exemplified on WSD anddiscussed for thesaurus construction and relationextraction.
The main contributions of the disserta-tion that is summarized here are:?
A framework for unsupervised NLP?
An efficient graph clustering algorithm?
An unsupervised language separator?
An unsupervised POS taggerThe main advantage of unsupervised NLP,namely language independence, will enable theimmediate processing of all languages and do-mains for which a large amount of text is elec-tronically available.ReferencesE.
Agirre, D. Mart?nez, O. L?pez de Lacalle and A. So-roa.
2006.
Evaluating and optimizing the parametersof an unsupervised graph-based WSD algorithm.Proceedings of Textgraphs-06, New York, USAC.
Biemann and S. Teresniak.
2005.
Disentangling fromBabylonian Confusion ?
Unsupervised LanguageIdentification.
Proc.
CICLing-2005, Mexico CityC.
Biemann.
2006a.
Chinese Whispers - an EfficientGraph Clustering Algorithm and its Application toNatural Language Processing Problems.
Proceedingsof Textgraphs-06, New York, USAC.
Biemann.
2006b.
Unsupervised Part-of-Speech Tag-ging Employing Efficient Graph Clustering.
Proceed-ings of COLING/ACL-06 SRW, Sydney, AustraliaC.
Biemann.
2007.
A Random Text Model for the Gen-eration of Statistical Language Invariants.
Proceed-ings of HLT-NAACL-07, Rochester, USAC.
Biemann, C. Giuliano and A. Gliozzo.
2007.
Unsu-pervised POS tagging supporting supervised meth-ods.
Manuscript to appearS.
Bordag.
2006.
Word Sense Induction: Triplet-BasedClustering and Automatic Evaluation.
Proceedings ofEACL-06.
Trento, ItalyD.
Davidov, A. Rappoport.
2006.
Efficient Unsuper-vised Discovery of Word Categories Using Symmet-ric Patterns and High Frequency Words.
Proceedingsof COLING/ACL-06, Sydney, AustraliaS.
van Dongen.
2000.
A cluster algorithm for graphs.Technical Report INS-R0010, CWI, Amsterdam.B.
Dorow and D. Widdows.
2003.
Discovering Corpus-Specific Word Senses.
In EACL-2003 ConferenceCompanion, Budapest, HungaryT.
Dunning.
1993.
Accurate Methods for the Statisticsof Surprise and Coincidence.
Computational Linguis-tics, 19(1), pp.
61-74D.
Lin.
1997.
Automatic Retrieval and Clustering ofSimilar Words.
Proc.
COLING-97, Montreal, CanadaH.
Sch?tze.
1993.
Word Space.
Proceedings of NIPS-5,Morgan Kaufmann, San Francisco, CA, USAJ.
?
?ma and S.E.
Schaeffer.
2005.
On the NP-completeness of some graph cluster measures.
Tech-nical Report cs.CC/0506100, http://arxiv.org/.M.
Steyvers, J.
B. Tenenbaum.
2005.
The large-scalestructure of semantic networks.
Cog.
Science, 29(1)D. Widdows.
2005.
Geometry and Meaning.
CSLI Lec-ture notes #172, Stanford, USA40
