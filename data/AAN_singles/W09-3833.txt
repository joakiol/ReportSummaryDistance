Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 214?217,Paris, October 2009. c?2009 Association for Computational LinguisticsSmoothing fine-grained PCFG lexiconsTejaswini DeoskarILLCUniversity of Amsterdamt.deoskar@uva.nlMats RoothDept.
of Linguistics and CISCornell Universitymr249@cornell.eduKhalil Sima?anILLCUniversity of Amsterdamk.simaan@uva.nlAbstractWe present an approach for smoothingtreebank-PCFG lexicons by interpolatingtreebank lexical parameter estimates withestimates obtained from unannotated datavia the Inside-outside algorithm.
ThePCFG has complex lexical categories,making relative-frequency estimates froma treebank very sparse.
This kind ofsmoothing for complex lexical categoriesresults in improved parsing performance,with a particular advantage in identify-ing obligatory arguments subcategorizedby verbs unseen in the treebank.1 IntroductionLexical scarcity is a problem faced by all sta-tistical NLP applications that depend on anno-tated training data, including parsing.
One wayof alleviating this problem is to supplement super-vised models with lexical information from unla-beled data.
In this paper, we present an approachfor smoothing the lexicon of a treebank PCFGwith frequencies estimated from unannotated datawith Inside-outside estimation (Lari and Young,1990).
The PCFG is an unlexicalised PCFG, butcontains complex lexical categories (akin to su-pertags in LTAG (Bangalore and Joshi, 1999) orCCG (Clark and Curran, 2004)) encoding struc-tural preferences of words, like subcategorization.The idea behind unlexicalised parsing is that thesyntax and lexicon of a language are largely inde-pendent, being mediated by ?selectional?
proper-ties of open-class words.
This is the intuition be-hind lexicalised formalisms like CCG: here lexicalcategories are fine-grained and syntactic in nature.Once a word is assigned a lexical category, theword itself is not taken into consideration furtherin the syntactic analysis.
Fine-grained categoriesimply that lexicons estimated from treebanks willbe extremely sparse, even for a language like En-glish with a large treebank resource like the PennTreebank (PTB) (Marcus et al, 1993).
Smoothinga treebank lexicon with an external wide-coveragelexicon is problematic due to their respective rep-resentations being incompatible and without anobvious mapping, assuming that the external lexi-con is probabilistic to begin with.
In this paper, westart with a treebank PCFG with fine-grained lex-ical categories and re-estimate its parameters on alarge corpus of unlabeled data.
We then use re-estimates of lexical parameters (i.e.
pre-terminalto terminal rule probabilities) to smooth the orig-inal treebank lexical parameters by interpolationbetween the two.
Since the treebank PCFG itself isused to propose analyses of new data, the mappingproblem is inherently taken care of.
The smooth-ing procedure takes into account the fact that unsu-pervised estimation has benefits for unseen or low-frequency lexical items, but the treebank relative-frequency estimates are more reliable in the caseof high-frequency items.2 Treebank PCFGIn order to have fine-grained and linguistic lexi-cal categories (like CCG) within a simple formal-ism with well-understood estimation methods, wefirst build a PCFG containing such categories fromthe PTB.
The PCFG is unlexicalised (with lim-ited lexicalization of certain function words, likein Klein and Manning (2003)).
It is created byfirst transforming the PTB (Johnson, 1998) in anappropriate way and then extracting a PCFG fromthe transformed trees (Deoskar and Rooth, 2008).All functional tags in the PTB (such as NP-SBJ,PP-TMP, etc.)
are maintained, as are all emptycategories, making long-distance dependencies re-coverable.
The PCFG is trained on the standardtraining sections of the PTB and performs at thestate-of-the-art level for unlexicalised PCFGs, giv-ing 86.6% f-score on Sec.
23.214VPVB.npaddNPfour moreBoeingsPP-TMPby 1994PP-CLRto thetwo units.
(a) An NP PP subcategorization frame marked on theverb ?add?
as np.
Note that the arguments NP and PP-CLR are part of the subcategorization frame and arerepresented locally on the verb but the adjunct PP-TMP is not.VPVBG.s.e.toseekingS.e.to+E-NP+ VP.toTOtoVPavoid..(b) An S frame on the verb ?seeking?
: +E-NP+ represents the empty subject of theS.
Note that structure internal to S is alsomarked on the verb.VPVb.sbthinkSBAR+C+ Sthe consumeris right(c) An SBAR frame: +C+ is theempty complementizer.Figure 1: Subcategorized structures are marked as features on the verbal POS category.An important feature of our PCFG is that pre-terminal categories for open-class items like verbs,nouns and adverbs are more complex than PTBPOS tags.
They encode information about thestructure selected by the lexical item, in effect,its subcategorization frame.
A pre-terminal in ourPCFG consists of the standard PTB POS tag, fol-lowed by a sequence of features incorporated intoit.
Thus, each PTB POS tag can be considered tobe divided into multiple finer-grained ?supertags?by the incorporated features.
These features en-code the structure selected by the words.
We fo-cus on verbs in this paper, as they are importantstructural determiners.
A sequence of one or morefeatures forms the ?subcategorization frame?
of averb: three examples are shown in Figure 1.
Thefeatures are determined by a fully automated pro-cess based on PTB tree structure and node labels.There are 81 distinct subcategorization frames forverbal categories.
The process can be repeated forother languages with a treebank annotated in thePTB style which marks arguments like the PTB.3 Unsupervised Re-estimationInside-outside (henceforth I-O) (Lari and Young,1990), an instance of EM, is an iterative estima-tion method for PCFGs that, given an initial modeland a corpus of unannotated data, produces mod-els that assign increasingly higher likelihood tothe corpus at each iteration.
I-O often leads tosub-optimal grammars, being subject to the well-known problem of local maxima, and dependenceon initial conditions (de Marcken, 1995) (althoughthere have been positive results using I-O as well,for e.g.
Beil et al (1999)).
More recently, Deoskar(2008) re-estimated an unlexicalised PTB PCFGusing unlabeled Wall Street Journal data.
Theycompared models for which all PCFG parameterswere re-estimated from raw data to models forwhich only lexical parameters were re-estimated,and found that the latter had better parsing results.While it is common to constrain EM either bygood initial conditions or by heuristic constraints,their approach used syntactic parameters from atreebank model to constrain re-estimation of lex-ical parameters.
Syntactic parameters are rela-tively well-estimated from a treebank, not being assparse as lexical parameters.
At each iteration, there-estimated lexicon was interpolated with a tree-bank lexicon, ensuring that re-estimated lexiconsdid not drift away from the treebank lexicon.We follow their methodology of constrainedEM re-estimation.
Using the PCFG with finelexical categories (as described in ?2) as the ini-tial model, we re-estimate its parameters from anunannotated corpus.
The lexical parameters ofthe re-estimated PCFG form its probabilistic ?lex-icon?, containing the same fine-grained categoriesas the original treebank PCFG.
We use this re-estimated ?lexicon?
to smooth the lexical proba-bilities in the treebank PCFG.4 Smoothing based on a POS tagger : theinitial model.In order to use the treebank PCFG as an initialmodel for unsupervised estimation, new wordsfrom the unannotated training corpus must be in-cluded in it ?
if not, parameter values for newwords will never be induced.
Since the treebankmodel contains no information regarding correctfeature sequences for unseen words, we assign allpossible sequences that have occurred in the tree-bank model with the POS tag of the word.
Weassign all possible sequences to seen words as215well ?
although the word is seen, the correct fea-ture sequence for a structure in a training sentencemight still be unseen with that word.
This is doneas follows: a standard POS-tagger (TreeTagger,(Schmid, 1994)) is used to tag the unlabeled cor-pus.
A frequency table cpos(w, ?)
consisting ofwords and POS-tags is extracted from the result-ing corpus, where w is the word and ?
its POStag.
The frequency cpos(w, ?)
is split amongst allpossible feature sequences ?
for that POS tag inproportion to treebank marginals t(?, ?)
and t(?
)cpos(w, ?, ?)
= t(?, ?)t(?)
cpos(w, ?)
(1)Then the treebank frequency t(w, ?, ?)
and thescaled corpus frequency are interpolated to get asmoothed model tpos.
We use ?=0.001, giving asmall weight initially to the unlabeled corpus.tpos(w, ?, ?)
= (1?
?
)t(w, ?, ?)
+ ?cpos(w, ?, ?
)(2)The first term will be zero for words unseen in thetreebank: their distribution in the smoothed modelwill be the average treebank distribution over allpossible feature sequences for a POS tag.
Forseen words, the treebank distribution over featuresequence is largely maintained, but a small fre-quency is assigned to unseen sequences.5 Smoothing based on EM re-estimationAfter each iteration i of I-O, the expected countscemi(w, ?, ?)
under the model instance at itera-tion (i ?
1) are obtained.
A smoothed treebanklexicon temi is obtained by linearly interpolatingthe smoothed treebank lexicon tpos(w, ?, ?)
and ascaled re-estimated lexicon c?emi(w, ?, ?
).temi(w, ?, ?)
= (1??
)tpos(w, ?, ?
)+?c?emi (w, ?, ?
)(3)where 0 < ?
< 1.
The term c?emi(w, ?, ?)
is ob-tained by scaling the frequencies cemi(w, ?, ?)
ob-tained by I-O, ensuring that the treebank lexicon isnot swamped with the large training corpus1.c?emi(w, ?, ?)
= t(?, ?
)?w cemi(w, ?, ?
)cemi(w, ?, ?)(4)?
determines the relative weights given to thetreebank and re-estimated model for a word.
Sinceparameters of high-frequency words are likelyto be more accurate in the treebank model, weparametrize ?
as ?f according to the treebank fre-quency f = t(w, ?
).1Note that in Eq.
4, the ratio of the two terms involvingcemi is the conditional, lexical probability Pemi(w|?, ?
).6 ExperimentsThe treebank PCFG is trained on sections 0-22 ofthe PTB, with 5000 sentences held-out for evalu-ation.
We conducted unsupervised estimation us-ing Bitpar (Schmid, 2004) with unannotated WallStreet Journal data of 4, 8 and 12 million words,with sentence length <25 words.
The treebankand re-estimated models are interpolated with ?
=0.5 (in Eq.
3).
We also parametrize ?
for treebankfrequency of words ?
optimizing over a develop-ment set gives us the following values of ?f fordifferent ranges of treebank word frequencies.if t(w, ?)
<= 5 , ?f = 0.5if 5 < t(w, ?)
<= 15 , ?f = 0.25if 15 < t(w, ?)
<= 50 , ?f = 0.05if t(w, ?)
> 50 , ?f = 0.005(5)Evaluations are on held-out data from the PTBby stripping all PTB annotation and obtainingViterbi parses with the parser Bitpar.
In additionto standard PARSEVAL measures, we also eval-uate parses by another measure specific to sub-categorization2 : the POS-tag+feature sequence onverbs in the Viterbi parse is compared against thecorresponding tag+feature sequence on the trans-formed PTB gold tree, and errors are counted.
Thetag-feature sequence correlates to the structure se-lected by the verb, as exemplified in Fig.
1.7 ResultsThere is a statistically significant improvement3in labeled bracketing f-score on Sec.
23 whenthe treebank lexicon is smoothed with an EM-re-estimated lexicon.
In Table 1, tt refers to the base-line treebank model, smoothed using the POS-tag smoothing method (from ?4) on the test data(Sec.
23) in order to incorporate new words fromthe test data4.
tpos refers to the initial model forre-estimation, obtained by smoothed the treebankmodel with the POS-tag smoothing method withthe large unannotated corpus (4 million words).This model understandably does not improve overtt for parsing Sec.
23. tem1,?=0.5 is the modelobtained by smoothing with an EM-re-estimatedmodel with a constant interpolation factor ?
=0.5.
This model gives a statistically significant im-provement in f-score over both tt and tpos.
Thelast model tem1,?f is obtained by smoothing with2PARSEVAL measures are known to be insensitive to sub-categorization (Carroll et al, 1998).3A randomized version of a paired-sample t-test is used.4This is always done before parsing test data.216tt tpos tem1,?=0.5 tem1,?fRecall 86.48 86.48 86.72 87.44Precision 86.61 86.63 86.95 87.15f-score 86.55 86.56 *86.83 *87.29Table 1: Labeled bracketing F-score on section 23.an interpolation factor as in Eq.
5 : this is the bestmodel with a statistically significant improvementin f-score over tt, tpos and tem1,?=0.5.Since we expect that smoothing will be advanta-geous for unseen or low-frequency words, we per-form an evaluation targeted at identifying struc-tures subcategorized by unseen verbs.
Table 2shows the error reduction in identifying subcat.frames in Viterbi parses, of unseen verbs and alsoof all verbs (seen and unseen) in the testset.
Abreakup of error by frame type for unseen verbs isalso shown (here, only frames with >10 token oc-currences in the test data are shown).
In all cases(unseen verbs and all verbs) we see a substantialerror reduction.
The error reduction improves withlarger amounts of unannotated training data.8 Discussion and ConclusionsWe have shown that lexicons re-estimated with I-O can be used to smooth unlexicalised treebankPCFGs, with a significant increase in f-score evenin the case of English with a large treebank re-source.
We expect this method to have moreimpact for languages with a smaller treebank orricher tag-set.
An interesting aspect is the substan-tial reduction in subcategorization error for un-seen verbs for which no word-specific informationabout subcategorization exists in the unsmoothedor POS-tag-smoothed lexicon.
The error reductionin identifying subcat.
frames implies that someconstituents (such as PPs) are not only attachedcorrectly but also identified correctly as arguments(such as PP-CLR) rather than as adjuncts.There have been previous attempts to use POS-tagging technologies (such as HMM or maximum-entropy based taggers) to enhance treebank-trained grammars (Goldberg et al (2009) for He-brew, (Clark and Curran, 2004) for CCG).
The re-estimation method we use builds full parse-trees,rather than use local features like taggers do, andhence might have a benefit over such methods.
Aninteresting option would be to train a ?supertag-ger?
on fine-grained tags from the PTB and to su-pertag a large corpus to harvest lexical frequen-Frame # tokens %Error %Error %Error(test) tpos tem1 Reduc.All unseen (4M words) 1258 33.47 22.81 31.84All unseen (8M words) 1258 33.47 22.26 33.49All unseen (12M words) 1258 33.47 21.86 34.68transitive 662 23.87 18.73 21.52intransitive 115 38.26 33.91 11.36NP PP-CLR 121 34.71 32.23 7.14PP-CLR 73 27.4 20.55 25SBAR 124 12.1 12.1 0S 12 83.33 58.33 30NP NP 10 90 80 11.11PRT NP 21 38.1 33.33 12.5s.e.to (see Fig.1b) 50 16 12 25NP PP-DIR 11 63.64 54.55 14.28All verbs (4M) 11710 18.5 16.84 8.97Table 2: Subcat.
error for verbs in Viterbi parses.cies.
This would form another (possibly higher)baseline for the I-O re-estimation approach pre-sented here and is the focus of our future work.ReferencesS.
Bangalore and A. K. Joshi.
1999.
Supertagging: An Ap-proach to Almost Parsing.
Computational Linguistics,25:237?265.F.
Beil, G. Carroll, D. Prescher, S. Riezler, and M. Rooth.1999.
Inside-outside estimation of a lexicalized PCFG forGerman.
In ACL 37.J.
Carroll, G. Minnen, and E. Briscoe.
1998.
Can subcate-gorization probabilities help parsing.
In 6th ACL/SIGDATWorkshop on Very Large Corpora.S.
Clark and J. R. Curran.
2004.
The Importance of Supertag-ging for Wide-Coverage CCG Parsing.
In 22nd COLING.Carl de Marcken.
1995.
On the unsupervised induction ofPhrase Structure grammars.
In Proceedings of the 3rdWorkshop on Very Large Corpora.T.
Deoskar.
2008.
Re-estimation of Lexical Parameters forTreebank PCFGs.
In 22nd COLING.Tejaswini Deoskar and Mats Rooth.
2008.
Induction ofTreebank-Aligned Lexical Resources.
In 6th LREC.Y.
Goldberg, R. Tsarfaty, M. Adler, and M. Elhadad.
2009.Enhancing Unlexicalized Parsing Performance using aWide Coverage Lexicon, Fuzzy Tag-set Mapping, andEM-HMM-based Lexical Probabilities.
In EACL-09.M.
Johnson.
1998.
PCFG models of linguistic tree represen-tations.
Computational Linguistics, 24(4).D.
Klein and C. Manning.
2003.
Accurate unlexicalized pars-ing.
In ACL 41.K.
Lari and S. J.
Young.
1990.
The estimation of stochas-tic context-free grammars using the Inside-Outside algo-rithm.
Computer Speech and Language, 4:35?56.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a Large Annotated Corpus of English: The PennTreebank.
Computational Linguistics, 19(2):313?330.H.
Schmid.
1994.
Probabilistic Part-of-Speech Tagging Us-ing Decision Trees.
In International Conference on NewMethods in Language Processing.H.
Schmid.
2004.
Efficient Parsing of Highly AmbiguousCFGs with Bit Vectors.
In 20th COLING.217
