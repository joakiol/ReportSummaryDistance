Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 28?36,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAnnotating ESL Errors: Challenges and RewardsAlla Rozovskaya and Dan RothUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801{rozovska,danr}@illinois.eduAbstractIn this paper, we present a corrected and error-tagged corpus of essays written by non-nativespeakers of English.
The corpus contains63000 words and includes data by learners ofEnglish of nine first language backgrounds.The annotation was performed at the sentencelevel and involved correcting all errors in thesentence.
Error classification includes mis-takes in preposition and article usage, errorsin grammar, word order, and word choice.
Weshow an analysis of errors in the annotatedcorpus by error categories and first languagebackgrounds, as well as inter-annotator agree-ment on the task.We also describe a computer program that wasdeveloped to facilitate and standardize the an-notation procedure for the task.
The programallows for the annotation of various types ofmistakes and was used in the annotation of thecorpus.1 IntroductionWork on automated methods for detecting and cor-recting context dependent mistakes (e.g., (Goldingand Roth, 1996; Golding and Roth, 1999; Carlsonet al, 2001)) has taken an interesting turn over thelast few years, and has focused on correcting mis-takes made by non-native speakers of English.
Non-native writers make a variety of errors in grammarand word usage.
Recently, there has been a lot ofeffort on building systems for detecting mistakes inarticle and preposition usage (DeFelice, 2008; Eeg-Olofsson, 2003; Gamon et al, 2008; Han et al,2006; Tetreault and Chodorow, 2008b).
Izumi et al(2003) consider several error types, including articleand preposition mistakes, made by Japanese learn-ers of English, and Nagata et al (2006) focus on theerrors in mass/count noun distinctions with an ap-plication to detecting article mistakes also made byJapanese speakers.
Article and preposition mistakeshave been shown to be very common mistakes forlearners of different first language (L1) backgrounds(Dagneaux et al, 1998; Gamon et al, 2008; Izumiet al, 2004; Tetreault and Chodorow, 2008a), butthere is no systematic study of a whole range of er-rors non-native writers produce, nor is it clear whatthe distribution of different types of mistakes is inlearner language.In this paper, we describe a corpus of sentenceswritten by English as a Second Language (ESL)speakers, annotated for the purposes of developingan automated system for correcting mistakes in text.Although the focus of the annotation were errorsin article and preposition usage, all mistakes in thesentence have been corrected.
The data for anno-tation were taken from two sources: The Interna-tional Corpus of Learner English (ICLE, (Grangeret al, 2002a)) and Chinese Learners of English Cor-pus (CLEC, (Gui and Yang, 2003)).
The annotatedcorpus includes data from speakers of nine first lan-guage backgrounds.
To our knowledge, this is thefirst corpus of non-native English text (learner cor-pus) of fully-corrected sentences from such a diversegroup of learners1.
The size of the annotated corpusis 63000 words, or 2645 sentences.
While a corpus1Possibly, except for the Cambridge Learner Corpushttp://www.cambridge.org/elt28of this size may not seem significant in many natu-ral language applications, this is in fact a large cor-pus for this field, especially considering the effort tocorrect all mistakes, as opposed to focusing on onelanguage phenomenon.
This corpus was used in theexperiments described in the companion paper (Ro-zovskaya and Roth, 2010).The annotation schema that we developed wasmotivated by our special interest in errors in arti-cle and preposition usage, but also includes errorsin verbs, morphology, and noun number.
The cor-pus contains 907 article corrections and 1309 prepo-sition corrections, in addition to annotated mistakesof other types.While the focus of the present paper is on anno-tating ESL mistakes, we have several goals in mind.First, we present the annotation procedure for thetask, including an error classification schema, anno-tation speed, and inter-annotator agreement.
Sec-ond, we describe a computer program that we de-veloped to facilitate the annotation of mistakes intext.
Third, having such a diverse corpus allowsus to analyze the annotated data with respect to thesource language of the learner.
We show the anal-ysis of the annotated data through an overall break-down of error types by the writer?s first language.We also present a detailed analysis of errors in arti-cle and preposition usage.
Finally, it should be notedthat there are currently very few annotated learnercorpora available.
Consequently, systems are eval-uated on different data sets, which makes perfor-mance comparison impossible.
The annotation ofthe data presented here is available2 and, thus, canbe used by researchers who obtain access to theserespective corpora3.The rest of the paper is organized as follows.First, we describe previous work on the annotationof learner corpora and statistics on ESL mistakes.Section 3 gives a description of the annotation pro-cedure, Section 4 presents the annotation tool thatwas developed for the purpose of this project andused in the annotation.
We then present error statis-tics based on the annotated corpus across all errortypes and separately for errors in article and preposi-tion usage.
Finally, in Section 6 we describe how we2Details about the annotation are accessible fromhttp://L2R.cs.uiuc.edu/?cogcomp/3The ICLE and CLEC corpora are commercially available.evaluate inter-annotator agreement and show agree-ment results for the task.2 Learner Corpora and Error TaggingIn this section, we review research in the annota-tion and error analysis of learner corpora.
For areview of learner corpus research see, for exam-ple, (D?
?az-Negrillo, 2006; Granger, 2002b; Pravec,2002).
Comparative error analysis is difficult, asthere are no standardized error-tagging schemas, butwe can get a general idea about the types of errorsprevalent with such speakers.
Izumi et al (2004a)describe a speech corpus of Japanese learners of En-glish (NICT JLE).
The corpus is corrected and anno-tated and consists of the transcripts (2 million words)of the audio-recordings of the English oral profi-ciency interview test.
In the NICT corpus, whoseerror tag set consists of 45 tags, about 26.6% of er-rors are determiner related, and 10% are prepositionrelated, which makes these two error types the mostcommon in the corpus (Gamon et al, 2008).
TheChinese Learners of English corpus (CLEC, (Guiand Yang, 2003)) is a collection of essays writtenby Chinese learners of beginning, intermediate, andadvanced levels.
This corpus is also corrected anderror-tagged, but the tagging schema does not allowfor an easy isolation of article and preposition errors.The International Corpus of Learner English (ICLE,(Granger et al, 2002a)) is a corpus of argumenta-tive essays by advanced English learners.
The cor-pus contains 2 million words of writing by Europeanlearners from 14 mother tongue backgrounds.
Whilethe entire corpus is not error-tagged, the French sub-part of the corpus along with other data by Frenchspeakers of a lower level of proficiency has been an-notated (Dagneaux et al, 1998).
The most com-mon errors for the advanced level of proficiencywere found to be lexical errors (words) (15%), regis-ter (10%), articles (10%), pronouns (10%), spelling(8%) , verbs (8%).In a study of 53 post-intermediate ESOL (mi-grant) learners in New Zealand (Bitchener et al,2005), the most common errors were found to beprepositions (29%), articles (20%), and verb tense(22%).
Dalgish (1985) conducted a study of er-rors produced by ESL students enrolled at CUNY.It was found that across students of different first29languages, the most common error types among24 different error types were errors in article us-age (28%), vocabulary error (20-25%) (word choiceand idioms), prepositions (18%), and verb-subjectagreement (15%).
He also noted that the speakers oflanguages without article system made considerablymore article errors, but the breakdown of other errortypes across languages was surprisingly similar.3 Annotation3.1 Data SelectionData for annotation were extracted from the ICLEcorpus (Granger et al, 2002a) and CLEC (Gui andYang, 2003).
As stated in Section 2, the ICLE con-tains data by European speakers of advanced levelof proficiency, and the CLEC corpus contains es-says by Chinese learners of different levels of pro-ficiency.
The annotated corpus includes sentenceswritten by speakers of nine languages: Bulgarian,Chinese, Czech, French, German, Italian, Polish,Russian, and Spanish.
About half of the sentencesfor annotation were selected based on their scoreswith respect to a 4-gram language model built usingthe English Gigaword corpus (LDC2005T12).
Thiswas done in order to exclude sentences that wouldrequire heavy editing and sentences with near-nativefluency, sentences with scores too high or too low.Such sentences would be less likely to benefit froma system on preposition/article correction.
The sen-tences for annotation were a random sample out ofthe remaining 80% of the data.To collect more data for errors in preposition us-age, we also manually selected sentences that con-tained such errors.
This might explain why the pro-portion of preposition errors is so high in our data.3.2 Annotation ProcedureThe annotation was performed by three nativespeakers of North American English, one under-graduate and two graduate students, specializing inforeign languages and Linguistics, with previous ex-perience in natural language annotation.
A sentencewas presented to the annotator in the context of theessay from which it was extracted.
Essay contextcan become necessary, especially for the correctionof article errors, when an article is acceptable in thecontext of a sentence, but is incorrect in the contextof the essay.
The annotators were also encouragedto propose more than one correction, as long as allof their suggestions were consistent with the essaycontext.3.3 Annotation SchemaWhile we were primarily interested in article andpreposition errors, the goal of the annotation was tocorrect all mistakes in the sentence.
Thus, our er-ror classification schema4, though motivated by ourinterest in errors in article and preposition usage,was also intended to give us a general idea aboutthe types of mistakes ESL students make.
A betterunderstanding of the nature of learners?
mistakes isimportant for the development of a robust automatedsystem that detects errors and proposes corrections.Even when the focus of a correction system is onone language phenomenon, we would like to haveinformation about all mistakes in the context: Errorinformation around the target article or prepositioncould help us understand how noisy data affect theperformance.But more importantly, a learner corpus with er-ror information could demonstrate how mistakes in-teract in a sentence.
A common approach to de-tecting and correcting context-sensitive mistakes isto deal with each phenomenon independently, butsometimes errors cannot be corrected in isolation.Consider, for example, the following sentences thatare a part of the corpus that we annotated.1.
?I should know all important aspects of English.?
?
?I shouldknow all of the important aspects of English.?2.
?But some of the people thought about him as a parodist of arhythm-n-blues singer.?
?
?But some people considered him tobe a parodist of a rhythm-n-blues singer.?3.
?...to be a competent avionics engineer...?
?
...?to become com-petent avionics engineers...?4.
?...which reflect a traditional female role and a traditional attitudeto a woman...?
?
?...which reflect a traditional female role anda traditional attitude towards women...?5.
?Marx lived in the epoch when there were no entertainments.??
?Marx lived in an era when there was no entertainment.
?In the examples above, errors interact with one an-other.
In example 1, the context requires a definitearticle, and the definite article, in turn, calls for the4Our error classification was inspired by the classificationdeveloped for the annotation of preposition errors (Tetreault andChodorow, 2008a).30preposition ?of?.
In example 2, the definite articleafter ?some of?
is used extraneously, and deleting italso requires deleting preposition ?of?.
Another caseof interaction is caused by a word choice error: Thewriter used the verb ?thought?
instead of ?consid-ered?
; replacing the verb requires also changing thesyntactic construction of the verb complement.
Inexamples 3 and 4, the article choice before the words?engineer?
and ?woman?
depends on the numbervalue of those nouns.
To correctly determine whicharticle should be used, one needs to determine firstwhether the context requires a singular noun ?engi-neer?
or plural ?engineers?.
Finally, in example 5,the form of the predicate in the relative clause de-pends on the number value of the noun ?entertain-ment?.For the reasons mentioned above, the annotationinvolved correcting all mistakes in a sentence.
Theerrors that we distinguish are noun number, spelling,verb form, and word form, in addition to article andpreposition errors .
All other corrections, the major-ity of which are lexical errors, were marked as wordreplacement, word deletion, and word insertion.
Ta-ble 1 gives a description of each error type.4 Annotation ToolIn this section, we describe a computer program thatwas developed to facilitate the annotation process.The main purpose of the program is to allow an an-notator to easily mark the type of mistake, when cor-recting it.
In addition, the tool allows us to providethe annotator with sufficient essay context.
As de-scribed in Section 3, sentences for annotation camefrom different essays, so each new sentence was usu-ally extracted from a new context.
To ensure thatthe annotators preserved the meaning of the sentencebeing corrected, we needed to provide them with theessay context.
A wider context could affect the an-notator?s decision, especially when determining thecorrect article choice.
The tool allowed us to effi-ciently present to the annotator the essay context foreach target sentence.Fig.
1 shows the program interface.
The sentencefor annotation appears in the white text box and theannotator can type corrections in the box, as if work-ing in a word processor environment.
Above and be-low the text box we can see the context boxes, wherethe rest of the essay is shown.
Below the lower con-text box, there is a list of buttons.
The pink buttonsand the dark green buttons correspond to differenterror types, the pink buttons are for correcting arti-cle and preposition errors, and the dark green but-tons ?
for correcting other errors.
The annotator canindicate the type of mistake being corrected by plac-ing the cursor after the word that contains an errorand pressing the button that corresponds to this er-ror type.
Pressing on an error button inserts a pair ofdelimiters after the word.
The correction can then beentered between the delimiters.
The yellow buttonsand the three buttons next to the pink ones are theshortcuts that can be used instead of typing in arti-cles and common preposition corrections.
The but-ton None located next to the article buttons is usedfor correcting cases of articles and prepositions usedsuperfluously.
To correct other errors, the annotatorneeds to determine the type of error, insert the corre-sponding delimiters after the word by pressing oneof the error buttons and enter the correction betweenthe delimiters.The annotation rate for the three annotators variedbetween 30 and 40 sentences per hour.Table 2 shows sample sentences annotated withthe tool.
The proposed corrections are located insidethe delimiters and follow the word to which the cor-rection refers.
When replacing a sequence of words,the sequence was surrounded with curly braces.
Thisis useful if a sequence is a multi-word expression,such as at last.5 Annotation StatisticsIn this section, we present the results of the anno-tation by error type and the source language of thewriter.Table 3 shows statistics for the annotated sen-tences by language group and error type.
Becausethe sub-corpora differ in size, we show the numberof errors per hundred words.
In total, the annotatedcorpus contains 63000 words or 2645 sentences oflearner writing.
Category punctuation was not spec-ified in the annotation, but can be easily identifiedand includes insertion, deletion, and replacement ofpunctuation marks.
The largest error category isword replacement, which combines deleted, insertedwords and word substitutions.
This is followed by31Error type Description ExamplesArticle error Any error involving an article ?Women were indignant at [None/the] inequalityfrom men.
?Preposition error Any error involving a preposition ?...to change their views [to/for] the better.
?Noun number Errors involving plural/singularconfusion of a noun?Science is surviving by overcoming the mistakes notby uttering the [truths/truth] .
?Verb form Errors in verb tense and verb inflec-tions?He [write/writes] poetry.
?Word form Correct lexeme, but wrong suffix ?It is not [simply/simple] to make professional army.
?Spelling Error in spelling ?...if a person [commited/committed] a crime...?Word insertion, deletion,or replacementOther corrections that do not fallinto any of the above categories?There is a [probability/possibility] that today?s fan-tasies will not be fantasies tomorrow.
?Table 1: Error classification used in annotationFigure 1: Example of a sentence for annotation as it appears in the annotation tool window.
The target sentence isshown in the white box.
The surrounding essay context is shown in the brown boxes.
The buttons appear below theboxes with text: pink buttons (for marking article and preposition errors), dark green (for marking other errors), ligh tgreen (article buttons) and yellow (preposition buttons).Annotated sentence Corrected errors1.
Television becomes their life , and in many cases it replaces their real life /lives/ noun number (life ?
lives)2.
Here I ca n?t $help$ but mention that all these people were either bankers or theHeads of companies or something of that kind @nature, kind@.word insertion (help); word replacement (kind ?
kind,nature)3.
We exterminated *have exterminated* different kinds of animals verb form (exterminated ?
have exterminated)4.
... nearly 30000 species of plants are under the <a> serious threat of disappear-ance |disappearing|article replacement (the ?
a); word form (disappear-ance ?
disappearing)5.
There is &a& saying that laziness is the engine of the <None> progress article insertion (a); article deletion (the)6.
...experience teaches people to strive to <for> the <None> possible things preposition replacement (to ?
for); article deletion(the)Table 2: Examples of sentences annotated using the annotation tool.
Each type of mistake is marked using a differentset of delimiters.
The corrected words are enclosed in the delimiters and follow the word to which the correctionrefers.
In example 2, the annotator preserved the author?s choice kind and added a better choice nature.32Source Total Total Errors per Corrections by Error Typelanguage sent.
words 100 words Articles Prepo- Verb Word Noun Word Spell.
Word Punc.sitions form form number order repl.Bulgarian 244 6197 11.9 10.3% 12.1% 3.5% 3.1% 3.0% 2.0% 5.0% 46.7% 14.2%Chinese 468 9327 15.1 12.7% 27.2% 7.9% 3.1% 4.6% 1.4% 5.4% 26.2% 11.3%Czech 296 6570 12.9 16.3% 10.8% 5.2% 3.4% 2.7% 3.2% 8.3% 32.5% 17.5%French 238 5656 5.8 6.7% 17.4% 2.1% 4.0% 4.6% 3.1% 9.8% 12.5% 39.8%German 198 5086 11.4 4.0% 13.0% 4.3% 2.8% 1.9% 2.9% 4.7% 15.4% 51.0%Italian 243 6843 10.6 5.9% 16.6% 6.4% 1.4% 3.0% 2.4% 4.6% 20.5% 39.3%Polish 198 4642 10.1 15.1% 16.3% 4.0% 1.3% 1.3% 2.3% 2.1% 12.3% 45.2%Russian 464 10844 13.0 19.2% 17.8% 3.7% 2.5% 2.5% 2.1% 5.0% 28.3% 18.8%Spanish 296 7760 15.0 11.5% 14.2% 6.0% 3.8% 2.6% 1.6% 11.9% 37.7% 10.7%All 2645 62925 12.2 12.5% 17.1% 5.2% 2.9% 3.0% 2.2% 6.5% 28.2% 22.5%Table 3: Error statistics on the annotated data by source language and error typethe punctuation category, which comprises 22% ofall corrections.
About 12% of all errors involve ar-ticles, and prepositions comprise 17% of all errors.We would expect the preposition category to be lesssignificant if we did not specifically look for such er-rors, when selecting sentences for annotation.
Twoother common categories are spelling and verb form.Verb form combines errors in verb conjugation anderrors in verb tense.
It can be observed from thetable that there is a significantly smaller proportionof article errors for the speakers of languages thathave articles, such as French or German.
Lexicalerrors (word replacement) are more common in lan-guage groups that have a higher rate of errors per100 words.
In contrast, the proportion of punctua-tion mistakes is higher for those learners that makefewer errors overall (cf.
French, German, Italian,and Polish).
This suggests that punctuation errorsare difficult to master, maybe because rules of punc-tuation are not generally taught in foreign languageclasses.
Besides, there is a high degree of variationin the use of punctuation even among native speak-ers.5.1 Statistics on Article CorrectionsAs stated in Section 2, article errors are one of themost common mistakes made by non-native speak-ers of English.
This is especially true for the speak-ers of languages that do not have articles, but for ad-vanced French speakers this is also a very commonmistake (Dagneaux et al, 1998), suggesting that ar-ticle usage in English is a very difficult language fea-ture to master.Han et al (2006) show that about 13% of nounphrases in TOEFL essays by Chinese, Japanese, andRussian speakers have article mistakes.
They alsoshow that learners do not confuse articles randomlyand the most common article mistakes are omissionsand superfluous article usage.
Our findings are sum-marized in Table 4 and are very similar.
We alsodistinguish between the superfluous use of a andthe, we allows us to observe that most of the casesof extraneously used articles involve article the forall language groups.
In fact, extraneous the is themost common article mistake for the majority ofour speakers.
Superfluous the is usually followedby the omission of the and the omission of a. An-other statistic that our table demonstrates and thatwas shown previously (e.g.
(Dalgish, 1985)) is thatlearners whose first language does not have articlesmake more article mistakes: We can see from col-umn 3 of the table that the speakers of German,French and Italian are three to four times less likelyto make an article mistake than the speakers of Chi-nese and all of the Slavic languages.
The only ex-ception are Spanish speakers.
It is not clear whetherthe higher error rate is only due to a difference inoverall language proficiency (as is apparent from theaverage number of mistakes by these speakers in Ta-ble 3) or to other factors.
Finally, the last column inthe table indicates that confusing articles with pro-nouns is a relatively common error and on averageaccounts for 10% of all article mistakes5.
Currentarticle correction systems do not address this errortype.5An example of such confusion is ?
To pay for the crimes,criminals are put in prison?, where the is used instead of their.33Source Errors Errors Article mistakes by error typelanguage total per 100 Miss.
Miss.
Extr.
Extr.
Confu- Mult.
Otherwords the a the a sion labelsBulgarian 76 1.2 9% 25% 41% 3% 8% 1% 13%Chinese 179 1.9 20% 12% 48% 4% 7% 2% 7%Czech 138 2.1 29% 13% 29% 9% 7% 4% 9%French 22 0.4 9% 14% 36% 14% 0% 23% 5%German 23 0.5 22% 9% 22% 4% 8% 9% 26%Italian 43 0.6 16% 40% 26% 2% 9% 0% 7%Polish 71 1.5 37% 18% 17% 8% 11% 4% 4%Russian 271 2.5 24% 18% 31% 6% 11% 1% 9%Spanish 134 1.7 16% 10% 51% 7% 3% 1% 10%All 957 1.5 22% 16% 36% 6% 8% 3% 9%Table 4: Distribution of article mistakes by error type and source language of the writer.
Confusion error type refers toconfusing articles a and the.
Multiple labels denotes cases where the annotator specified more than one article choice,one of which was used by the learner.
Other refers to confusing articles with possessive and demonstrative pronouns.5.2 Statistics on Preposition CorrectionsTable 5 shows statistics on errors in preposition us-age.
Preposition mistakes are classified into threecategories: replacements, insertions, and deletions.Unlike with article errors, the most common typeof preposition errors is confusing two prepositions.This category accounts for more than half of all er-rors, and the breakdown is very similar for all lan-guage groups.
The fourth category in the table, withoriginal, refers to the preposition usages that werefound acceptable by the annotators, but with a bet-ter suggestion provided.
We distinguish this caseas a separate category because preposition usage ishighly variable, unlike, for example, article usage.Tetreault and Chodorow (Tetreault and Chodorow,2008a) show that agreement between two nativespeakers on a cloze test targeting prepositions isabout 76%, which demonstrates that there are manycontexts that license multiple prepositions.6 Inter-annotator AgreementCorrecting non-native text for a variety of mistakesis challenging and requires a number of decisions onthe part of the annotator.
Human language allows formany ways to express the same idea.
Furthermore, itis possible that the corrected sentence, even when itdoes not contain clear mistakes, does not sound likea sentence produced by a native speaker.
The latteris complicated by the fact that native speakers differwidely with respect to what constitutes acceptableusage (Tetreault and Chodorow, 2008a).To date, a common approach to annotating non-native text has been to use one rater (Gamon et al,Source Errors Errors Mistakes by error typelanguage total per 100 Repl.
Ins.
Del.
Withwords orig.Bulgarian 89 1.4 58% 22% 11% 8%Chinese 384 4.1 52% 24% 22% 2%Czech 91 1.4 51% 21% 24% 4%French 57 1.0 61% 9% 12% 18%German 75 1.5 61% 8% 16% 15%Italian 120 1.8 57% 22% 12% 8%Polish 77 1.7 49% 18% 16% 17%Russian 251 2.3 53% 21% 17% 9%Spanish 165 2.1 55% 20% 19% 6%All 1309 2.1 54% 21% 18% 7%Table 5: Distribution of preposition mistakes by errortype and source language of the writer.
With orig refers toprepositions judged as acceptable by the annotators, butwith a better suggestion provided.2008; Han et al, 2006; Izumi et al, 2004; Na-gata et al, 2006).
The output of human annota-tion is viewed as the gold standard when evaluatingan error detection system.
The question of reliabil-ity of using one rater has been raised in (Tetreaultand Chodorow, 2008a), where an extensive reliabil-ity study of human judgments in rating prepositionusage is described.
In particular, it is shown thatinter-annotator agreement on preposition correctionis low (kappa value of 0.63) and that native speakersdo not always agree on whether a specific preposi-tion constitutes acceptable usage.We measure agreement by asking an annotatorwhether a sentence corrected by another person iscorrect.
After all, our goal was to make the sentencesound native-like, without enforcing that errors arecorrected in the same way.
One hundred sentencesannotated by each person were selected and the cor-34Agreement set Rater Judged Judgedcorrect incorrectAgreement set 1 Rater #2 37 63Rater #3 59 41Agreement set 2 Rater #1 79 21Rater #3 73 27Agreement set 3 Rater #1 83 17Rater #2 47 53Table 6: Annotator agreement at the sentence level.
Thenumber next to the agreement set denotes the annotatorwho corrected the sentences on the first pass.
Judged cor-rect denotes the proportion of sentences in the agreementset that the second rater did not change.
Judged incorrectdenotes the proportion of sentences, in which the secondrater made corrections.rections were applied.
This corrected set was mixedwith new sentences and given to the other two anno-tators.
In this manner, each annotator received twohundred sentences corrected by the other two anno-tators.
For each pair of the annotators, we computeagreement based on the 100 sentences on which theydid a second pass after the initial corrections by thethird rater.
To compute agreement at the sentencelevel, we assign the annotated sentences to one ofthe two categories: ?correct?
and ?incorrect?
: A sen-tence is considered ?correct?
if a rater did not makeany corrections in it on the second pass 6.
Table 6shows for each agreement set the number of sen-tences that were corrected on the second pass.
Onaverage, 40.8% of the agreement set sentences be-long to the ?incorrect?
category, but the proportionof ?incorrect?
sentences varies across annotators.We also compute agreement on the two cate-gories, ?correct?
and ?incorrect?.
The agreementand the kappa values are shown in Table 7.
Agree-ment on the sentences corrected on the second passvaries between 56% to 78% with kappa values rang-ing from 0.16 to 0.40.
The low numbers reflect thedifficulty of the task and the variability of the na-tive speakers?
judgments about acceptable usage.
Infact, since the annotation requires looking at sev-eral phenomena, we can expect a lower agreement,when compared to agreement rate on one languagephenomenon.
Suppose rater A disagrees with raterB on a given phenomenon with probability 1/4,then, when there are two phenomena, the probabil-ity that he will disagree with at least on of them is6We ignore punctuation corrections.Agreement set Agreement kappaAgreement set 1 56% 0.16Agreement set 2 78% 0.40Agreement set 3 60% 0.23Table 7: Agreement at the sentence level.
Agreementshows how many sentences in each agreement set wereassigned to the same category (?correct?, ?incorrect?)
foreach of the two raters.1 ?
9/16 = 7/16.
And the probability goes downwith the number of phenomena.7 ConclusionIn this paper, we presented a corpus of essays by stu-dents of English of nine first language backgrounds,corrected and annotated for errors.
To our knowl-edge, this is the first fully-corrected corpus that con-tains such diverse data.
We have described an anno-tation schema, have shown statistics on the error dis-tribution for writers of different first language back-grounds and inter-annotator agreement on the task.We have also described a program that was devel-oped to facilitate the annotation process.While natural language annotation, especially inthe context of error correction, is a challenging andtime-consuming task, research in learner corporaand annotation is important for the development ofrobust systems for correcting and detecting errors.AcknowledgmentsWe thank the anonymous reviewers for their helpfulcomments.
This research is partly supported by agrant from the U.S. Department of Education.ReferencesJ.
Bitchener, S. Young and D. Cameron.
2005.
The Ef-fect of Different Types of Corrective Feedback on ESLStudent Writing.
Journal of Second Language Writ-ing.A.
J. Carlson and J. Rosen and D. Roth.
2001.
ScalingUp Context Sensitive Text Correction.
IAAI, 45?50.M.
Chodorow, J. Tetreault and N-R. Han.
2007.
De-tection of Grammatical Errors Involving Prepositions.Proceedings of the Fourth ACL-SIGSEM Workshop onPrepositions.E.
Dagneaux, S. Denness and S. Granger.
1998.Computer-aided Error Analysis.
System, 26:163?174.35G.
Dalgish.
1985.
Computer-assisted ESL Research.CALICO Journal, 2(2).G.
Dalgish.
1991.
Computer-Assisted Error Analysisand Courseware Design: Applications for ESL in theSwedish Context.
CALICO Journal, 9.R.
De Felice and S. Pulman.
2008.
A Classifier-BasedApproach to Preposition and Determiner Error Correc-tion in L2 English.
In Proceedings of COLING-08.A.
D?
?az-Negrillo and J.
Ferna?ndez-Dom??nguez.
2006.Error Tagging Systems for Learner Corpora.
RESLA,19:83-102.J.
Eeg-Olofsson and O. Knuttson.
2003.
AutomaticGrammar Checking for Second Language Learners -the Use of Prepositions.
In Nodalida.M.
Gamon, J. Gao, C. Brockett, A. Klementiev, W.Dolan, D. Belenko and L. Vanderwende.
2008.
UsingContextual Speller Techniques and Language Model-ing for ESL Error Correction.
Proceedings of IJCNLP.A.
R. Golding and D. Roth.
1996.
Applying Winnowto Context-Sensitive Spelling Correction.
ICML, 182?190.A.
R. Golding and D. Roth.
1999.
A Winnow based ap-proach to Context-Sensitive Spelling Correction.
Ma-chine Learning, 34(1-3):107?130.S.
Granger, E. Dagneaux and F. Meunier.
2002.
Interna-tional Corpus of Learner EnglishS.
Granger.
2002.
A Bird?s-eye View of Learner Cor-pus Research.
Computer Learner Corpora, SecondLanguage Acquisition and Foreign Language Teach-ing, Eds.
S. Granger, J.
Hung and S. Petch-Tyson,Amsterdam: John Benjamins.
3?33.S.
Gui and H. Yang.
2003.
Zhongguo Xuexizhe YingyuYuliaohu.
(Chinese Learner English Corpus).
Shang-hai Waiyu Jiaoyu Chubanshe.
(In Chinese).N.
Han, M. Chodorow and C. Leacock.
2006.
Detect-ing Errors in English Article Usage by Non-nativeSpeakers.
Journal of Natural Language Engineering,12(2):115?129.E.
Izumi, K. Uchimoto, T. Saiga and H. Isahara.
2003.Automatic Error Detection in the Japanese LeanersEnglish Spoken Data.
ACL.E.
Izumi, K. Uchimoto and H. Isahara.
2004.
TheOverview of the SST Speech Corpus of JapaneseLearner English and Evaluation through the Exper-iment on Automatic Detection of Learners?
Errors.LREC.E.
Izumi, K. Uchimoto and H. Isahara.
2004.
TheNICT JLE Corpus: Exploiting the Language Learner?sSpeech Database for Research and Education.
Inter-national Journal of the Computer, the Internet andManagement, 12(2):119?125.R.
Nagata, A. Kawai, K. Morihiro, and N. Isu.
2006.
AFeedback-Augmented Method for Detecting Errors inthe Writing of Learners of English.
ACL/COLING.N.
Pravec.
2002.
Survey of learner corpora.
ICAMEJournal, 26:81?114.A.
Rozovskaya and D. Roth 2010.
Training Paradigmsfor Correcting Errors in Grammar and Usage.
In Pro-ceedings of the NAACL-HLT, Los-Angeles, CA.J.
Tetreault and M. Chodorow.
2008.
Native Judgmentsof Non-Native Usage: Experiments in Preposition Er-ror Detection.
COLING Workshop on Human Judg-ments in Computational Linguistics, Manchester, UK.J.
Tetreault and M. Chodorow.
2008.
The Ups andDowns of Preposition Error Detection in ESL Writing.COLING, Manchester, UK.36
