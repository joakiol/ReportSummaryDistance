Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, page 28,Columbus, Ohio, USA June 2008. c?2008 Association for Computational LinguisticsInvited Talk:Counting RankingsJason RiggleUniversity of Chicagojriggle@uchicago.eduAbstractIn this talk, I present a recursive algorithm to calculate the number of rankings that are consistent with aset of data (optimal candidates) in the framework of Optimality Theory (OT; Prince and Smolensky 1993).1Computing this quantity, which I call r-volume, makes possible a simple and effective Bayesian heuristic inlearning ?
all else equal, choose candidates that are preferred by the highest number of rankings consistentwith previous observations.
This heuristic yields an r-volume learning algorithm (RVL) that is guaranteedto make fewer than k lg k errors while learning rankings of k constraints.
This log-linear error bound isan improvement over the quadratic bound of Recursive Constraint Demotion (RCD; Tesar and Smolensky1996) and it is within a logarithmic factor of the best possible mistake bound for any OT learning algorithm.Computing r-volume: The violations in an OT tableau can be given as a [n ?
k] array of integers inwhich the first row t1corresponds to the winner.
Following Prince (2002), the ranking information can beextracted by comparing t1with each ?losing?
row t2, ..., tn to create an Elementary Ranking Condition asfollows: erc(t1, ti) = ?
?1, ..., ?k?
where ?j = L if t1,j < ti,j , ?j = W if t1,j > ti,j , and ?j = e otherwise.The meaning of ?
is that at least one constraint associated with W dominates all those associated with L.input C1C2C3candidate t1* ** winnercandidate t2** * erc(t1, t2) = ?W, L, e ?
i.e.
t1beats t2if C1outranks C2candidate t3** erc(t1, t3) = ?L, L, W?
i.e.
t1beats t3if C3outranks C1and C2candidate t4*** * erc(t1, t4) = ?L, W, W?
i.e.
t1beats t4if C2or C3outranks C1For a set E of length-k ERCs, E?wi denotesa set E?
derived from E by removing ERCswith W in column i and removing column i.r-vol(Ek)=?1?i?k??
?0 if xi = L for any x ?
E(k ?
1)!
if xi = W for all x ?
Er (E ?
wi) otherwiseMistake bounds: To make predictions, RVL selects in each tableau the candidate that yields the highestr-volume when the ERCs that allow it to win are combined with E (the ERCs for past winners).
To establisha mistake bound, assume that the RVL chooses candidate e when, in fact, candidate o was optimal accordingto the target ranking RT .
Assuming e 6= o, the rankings that make o optimal must be half or fewer of therankings consistent with E or else RVL would have chosen o.
Because all rankings that make candidatesother than o optimal will be eliminated once the ERCs for o are added to E, each error reduces the numberof rankings consistent with all observed data by at least half and thus there can be no more than lg k!
errors.Applications: The r-volume seems to encode ?restrictiveness?
in a way similar to Tesar and Prince?s(1999) r-measure.
As a factor in learning, it predicts typological frequency (cf.
Bane and Riggle 2008) andpriors other than the ?flat?
distribution over rankings can easily be included to test models of ranking bias.More generally, this research suggests the concept of g-volume for any parameterized model of grammar.1Full bibliography available on the Rutgers Optimality Archive (roa.rutgers.edu) with the paper Counting Rankings.28
