Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 375?378,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsRALI: Automatic weighting of text window distancesBernard Brosseau-Villeneuve*#, Noriko Kando#, Jian-Yun Nie** Universit?
de Montr?al, Email: {brosseab, nie}@iro.umontreal.ca# National Institute of Informatics, Email: {bbrosseau, kando}@nii.ac.jpAbstractSystems using text windows to modelword contexts have mostly been usingfixed-sized windows and uniform weights.The window size is often selected by trialand error to maximize task results.
Wepropose a non-supervised method for se-lecting weights for each window distance,effectively removing the need to limit win-dow sizes, by maximizing the mutual gen-eration of two sets of samples of the sameword.
Experiments on Semeval WordSense Disambiguation tasks showed con-siderable improvements.1 IntroductionThe meaning of a word can be defined by thewords that accompany it in the text.
This is theprinciple often used in previous studies on WordSense Disambiguation (WSD) (Ide and V?ronis,1998; Navigli, 2009).
In general, the accompa-nying words form a context vector of the targetword, or a probability distribution of the contextwords.
For example, under the unigram bag-of-word assumption, this means building p(x|t) =count(x,t)?x?count(x?,t), where count(x, t) is the count ofco-occurrences of word x with the target word tunder a certain criterion.
In most studies, x and tshould co-occur within a window of up to k wordsor sentences.
The bounds are usually selected asto maximize system performance.
Occurrences in-side the window usually weight the same with-out regard to their position.
This is counterintu-itive.
Indeed, a word closer to the target word usu-ally has a greater semantic constraint on the tar-get word than a more distant word.
Some studieshave also proposed decaying factors to decreasethe importance of more distant words in the con-text vector.
However, the decaying functions aredefined manually.
It is unclear that the functionsdefined can capture the true impact of the con-text words on the target word.
In this paper, wepropose an unsupervised method to automaticallylearn the optimal weight of a word according to itsdistance to the target word.
The general idea usedto determine such weight is that, if we randomlydetermine two sets of texts containing the targetword, the resulting probability distributions for itscontext words in the two sets should be similar.Therefore, the weights of context words at differ-ent distance are determined so as to maximize themutual generation probabilities of two sets of sam-ples.
Experimentation on Semeval-2007 Englishand Semeval-2010 Japanese lexical sample taskdata shows that improvements can automaticallybe attained on simple Naive Bayes (NB) systemsin comparison to the best manually selected fixedwindow system.The remainder of this paper is organized as fol-lows: example uses of text windows and relatedwork are presented in Section 2.
Our methodis presented in Section 3.
In Section 4 and5, we show experimental results on English andJapanese WSD.
We conclude in Section 6 withdiscussion and further possible extensions.2 Uses of text windowsModeling the distribution of words around onetarget word has many uses.
For instance, theXu&Croft co-occurrence-based stemmer (Xu andCroft, 1998) uses window co-occurrence statis-tics to calculate the best equivalence classes fora group of word forms.
They suggest using win-dows of up to 100 words.
Another example can befound in WSD systems, where a shorter window ispreferred.
In Semeval-2007, top performing sys-tems on WSD tasks, such as NUS-ML (Cai et al,2007), made use of bag-of-word features aroundthe target word.
In this case, they found that thebest results can be achieved using a window sizeof 3.375Both these systems limit the size of their win-dows for different purposes.
The former aims tomodel the topic of the documents containing theword rather than the word?s meaning.
The latterlimits the size because bag-of-word features fur-ther from the target word would not be sufficientlyrelated to its meaning (Ide and V?ronis, 1998).
Wesee that because of sparsity issues, there is a com-promise between taking few, highly related words,or taking several, lower quality words.In most current systems, all words in a windoware given equal weight, but we can easily under-stand that the occurrences of words should gener-ally count less as they become farther; they forma long tail that we should use.
Previous work pro-posed using non-linear functions of the distanceto model the relation between two words.
For in-stance, improvements can be obtained by using anexponential function (Gao et al, 2002).
Yet, thereis no evidence that the exponential ?
with its man-ually selected parameter ?
is the best function.3 Computing weights for distancesIn this section, we present our method for choos-ing howmuch a word should count according to itsdistance to the target word.
First, for some defini-tions, let C be a corpus, W a set of text windows,cW,i,xthe count of occurrences of word x at dis-tance i in W , cW,ithe sum of these counts, and ?ithe weight put on one word at distance i. Then,PML,W(x) =?i?icW,i,x?i?icW,i(1)is the maximum likelihood estimator for x. Tocounter the zero-probability problem, we applyDirichlet smoothing with the collection languagemodel as a prior:PDir,W(x) =?i?icW,i,x+ ?WP (x|C)?i?icW,i+ ?W(2)The pseudo-count ?Wis found by using Newton?smethod via leave-one-out estimation.
We followthe procedure shown in (Zhai and Lafferty, 2002),but since occurrences have different weights, thelog-likelihood is changed toL?1(?|W, C) = (3)?i?x?V?icW,i,xlog?icW,i,x?
?i+?P (x|C)?j?jcW,j?
?i+?To find the best weights for our model we pro-pose the following:?
Let T be the set of all windows containingthe target word.
We randomly split this setinto two sets A and B.?
We want to find ?
?that maximizes the mu-tual generation of the two sets, by minimizingtheir cross-entropy:l(?)
= H(PML,A, PDir,B) + H(PML,B, PDir,A)(4)In other words, we want ?ito represent howmuch an occurrence at distance i models the con-text better than the collection language model,whose counts are controlled by the Dirichletpseudo-count.
We hypothesize that target wordsoccurs in limited contexts, and as we get fartherfrom them, the possibilities become greater, re-sulting in sparse and less related counts.3.1 Gradient descentWe propose a simple gradient descent minimiz-ing (4) over ?.
For the following experiments,we used one single curve for all words in a task.We used the mini-batch type of gradient descent:the gradients of a fixed amount of target words aresummed, a gradient step is done, and the procesis repeated while cycling the data.
The startingstate was with all ?ito one, the batch size of 50and a learning rate of 1.
We notice that as the al-gorithm progress, weights on close distances in-crease and the farthest decrease.
As further dis-tances contribute less and less, middle distancesstart to decay more and more, until at some point,all distances but the closest start to decrease, head-ing towards a degenerate solution.
We thereforesuggest using the observation of several consecu-tive decreases of all except ?1as an end criterion.We used 10 consecutive steps for our experiments.4 Experiments on Semeval-2007 EnglishLexical SampleThe Semeval workshop holds WSD tasks such asthe English Lexical Sample (ELS) (Pradhan et al,2007).
It consists of a selected set of polysemouswords, contained within passages where a sensetaken from a sense inventory is manually anno-tated.
The task is to create supervised classifiersmaximizing accuracy on test data.Since there are only 50 words and instances arefew, we judged there was not enough data to com-pute weights.
Instead, we used the AP Newswirecorpus of the TREC collection (CD 1 & 2).
Words376were stemmed with the Porter stemmer and textwindows were grouped for all words.
For sim-plicity and efficiency, windows to the right and tothe left were considered independent, and we onlykept words with between 30 and 1000 windows.Also, only windows with a size of 100, which wasconsidered big enough without any doubt, werekept.
A stop list of the top 10 frequent words wasused, but place holders were left in the windows topreserve the distances.
Multiple consecutive stopwords (ex: ?of the?)
were merged, and the tar-get word, being the same for all samples of a set,was ignored.
This results in 32,650 sets contain-ing 5,870,604 windows.
In Figure 1, we can seethe resulting weight curve.0 20 40 60 80 100distance0.00.20.40.60.81.0weightFigure 1: Weight curve for AP NewswireSince the curve converges, words over the 100thdistance were assigned the minimumweight foundin the curve.
From this we constructed NB modelswhose class priors used an absolute discounting of0.5.
The collection language model used the con-catenation of the AP collection and the Semevaldata.
As the unstemmed target word is an impor-tant feature it was added to the models.
It?s weightwas chosen to be 0.7 by maximizing accuracy onone-held-out cross-validation of the training data.The results are listed in Table 1.System Cross-Val (%) Test set (%)Prior only 78.66 77.76Best uniform 85.48 83.28RALI-2 88.23 86.45Table 1: WSD accuracy on Semeval-2007 ELCWe used two baselines: most frequent sense(prior only), and the best uniform (except targetword) fixed size window found from extensivesearch on the training data.
The best settings werea window of size 4, with a weight of 4.4 on thetarget word and a Laplace smoothing of 2.9.
Theimprovements seen using our system are substan-tial, beating most of the systems originally pro-posed for the task (Pradhan et al, 2007).
Outof 15 systems, the best results had accuracies of89.1*, 89.1*, 88.7, 86.9 and 86.4 (* indicates post-competition submissions).
Notice that most wereusing Support Vector Machine (SVM) with bag-of-word features in a very small window, local col-locations and POS tags.
In our future work, wewill investigate the applications of SVM with ournew term weighting scheme.5 Experiments on Semeval-2010Japanese WSDThe Semeval-2010 Japanese WSD task (Okumuraet al, 2010) consists of 50 polysemous wordsfor which examples were taken from the BC-CWJ tagged corpus.
It was manually segmented,tagged, and annotated with senses taken from theIwanami Kokugo dictionary.
The task is identicalto the ELS of the previous experiment.Since the data was again insufficient to com-pute curves, we used the Mainichi-2005 corpus ofNTCIR-8.
We tried to reproduce the same kindof segmentation as the training data by using theChasen parser with UniDic.
For the corpus andSemeval data, conjugations (setsuzoku-to, jod?-shi, etc.
), particles (all jo-shi), symbols (blanks,kig?, etc.
), and numbers were stripped.
When abase-form reading was present (for verbs and ad-jectives), the token was replaced by the Kanjis(chinese characters) in the word writing concate-nated with the base-form reading.
This treatmentis somewhat equivalent to the stemming+stop listof the ELS tasks.
The resulting curve can be seenin Figure 2.The NB models are the same as in the previousexperiments.
Target words were again added thesame way as in the ELS task.
The best fixed win-dow model was found to have a window size of 1with a target word weight of 0.6 and used manualDirichlet smoothing with a pseudo-count of 110.We submited two systems with the following set-tings: RALI-1 used manual Dirichlet smoothingand 0.9 for the target word.
RALI-2 used auto-3770 20 40 60 80 100distance0.00.20.40.60.81.0weightFigure 2: Weight curve for Mainichi Shinbun 2005matic Dirichlet smoothing and 1.7 for the targetword weight.
Results are listed in Table 2.System Cross-Val (%) Test set (%)prior only 75.23 68.96Best uniform 82.29 76.12RALI-1 82.77 75.92RALI-2 83.05 76.36Table 2: WSD accuracy on Semeval-2010 JWSDAs we can see, the results are not significantlydifferent from the best uniform model.
This maybe due to differences in the segmentation parame-ters of our external corpus.
Another reason couldbe that the systems use almost the same weights:the best fixed window had size 1, and the Japanesecurve is steeper than the English one.This steeper curve can be explained by thegrammatical structure of the Japanese language.While English can be considered a Subject-Verb-Complement language, Japanese is consid-ered Subject-Complement-Verb.
Verbs are mostlyfound at the end of the sentence, far from their sub-ject, and vice versa.
The window distance is there-fore less useful in Japanese than in English sinceit has more non-local dependencies.
These resultsshow that the curves work as expected even in dif-ferent languages.6 ConclusionsThis paper proposed an unsupervised method forfinding weights for counts in text windows ac-cording to their distance to the target word.
Re-sults from the Semeval-2007 English lexical sam-ple showed a substantial improvement in preci-sion.
Yet, as we have seen with the Japanese task,window distance is not always a good indicator ofword relatedness.
Fortunately, we can easily imag-ine extensions to the current scheme that bins wordcounts by factors other than word distance.
For in-stance, we could also bin counts by parsing treedistance, sentence distance or POS-tags.AcknowledgmentsThe authors would like to thank Florian Boudinand Satoko Fujisawa for helpful comments onthis work.
This work is partially supportedby Japanese MEXT Grant-in-Aid for ScientificResearch on Info-plosion (#21013046) and theJapanese MEXT Research Student Scholarshipprogram.ReferencesJun Fu Cai, Wee Sun Lee, and Yee Whye Teh.
2007.Nus-ml: improving word sense disambiguation us-ing topic features.
In SemEval ?07 Proceedings,pages 249?252, Morristown, NJ, USA.
Associationfor Computational Linguistics.Jianfeng Gao, Ming Zhou, Jian-Yun Nie, HongzhaoHe, and Weijun Chen.
2002.
Resolving query trans-lation ambiguity using a decaying co-occurrencemodel and syntactic dependence relations.
In SI-GIR ?02 Proceedings, pages 183?190, New York,NY, USA.
ACM.Nancy Ide and Jean V?ronis.
1998.
Introduction tothe special issue on word sense disambiguation: thestate of the art.
Comput.
Linguist., 24(1):2?40.Roberto Navigli.
2009.
Word sense disambiguation: Asurvey.
ACM Comput.
Surv., 41(2):1?69.Manabu Okumura, Kiyoaki Shirai, Kanako Komiya,and Hikaru Yokono.
2010.
Semeval-2010 task:Japanese wsd.
In SemEval ?10 Proceedings.
Associ-ation for Computational Linguistics.Sameer S. Pradhan, Edward Loper, Dmitriy Dligach,and Martha Palmer.
2007.
Semeval-2007 task 17:English lexical sample, srl and all words.
In Se-mEval ?07 Proceedings, pages 87?92, Morristown,NJ, USA.
Association for Computational Linguis-tics.Jinxi Xu and W. Bruce Croft.
1998.
Corpus-based stemming using cooccurrence of word vari-ants.
ACM Trans.
Inf.
Syst., 16(1):61?81.ChengXiang Zhai and John Lafferty.
2002.
Two-stagelanguage models for information retrieval.
In SIGIR?02 Proceedings, pages 49?56, NewYork, NY, USA.ACM.378
