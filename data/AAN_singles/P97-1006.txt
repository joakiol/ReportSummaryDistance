Document  Classification Using a Finite Mixture ModelHang Li Ken j i  Yamanish iC&C Res.
Labs.
,  NEC4-1-1 Miyazak i  M iyamae-ku  Kawasak i ,  216, JapanEmai l :  { l ihang,yamanis i}  @sbl.cl .nec.co.j  pAbst rac tWe propose a new method of classifyingdocuments into categories.
We define foreach category a finite mixture model basedon soft clustering of words.
We treat theproblem of classifying documents as thatof conducting statistical hypothesis testingover finite mixture models, and employ theEM algorithm to efficiently estimate pa-rameters in a finite mixture model.
Exper-imental results indicate that our methodoutperforms existing methods.1 In t roduct ionWe are concerned here with the issue of classifyingdocuments into categories.
More precisely, we beginwith a number of categories (e.g., 'tennis, soccer,skiing'), each already containing certain documents.Our goal is to determine into which categories newlygiven documents ought to be assigned, and to do soon the basis of the distribution of each document'swords.
1Many methods have been proposed to addressthis issue, and a number of them have proved tobe quite effective (e.g.,(Apte, Damerau, and Weiss,1994; Cohen and Singer, 1996; Lewis, 1992; Lewisand Ringuette, 1994; Lewis et al, 1996; Schutze,Hull, and Pedersen, 1995; Yang and Chute, 1994)).The simple method of conducting hypothesis testingover word-based istributions in categories (definedin Section 2) is not efficient in storage and suffersfrom the data sparseness problem, i.e., the numberof parameters in the distributions is large and thedata size is not sufficiently large for accurately es-timating them.
In order to address this difficulty,(Guthrie, Walker, and Guthrie, 1994) have proposedusing distributions based on what we refer to as hard1A related issue is the retrieval, from a data base, ofdocuments which are relevant to a given query (pseudo-document) (e.g.,(Deerwester et al, 1990; Fuhr, 1989;Robertson and Jones, 1976; Salton and McGill, 1983;Wong and Yao, 1989)).clustering of words, i.e., in which a word is assignedto a single cluster and words in the same cluster aretreated uniformly.
The use of hard clustering might,however, degrade classification results, since the dis-tributions it employs are not always precise enoughfor representing the differences between categories.We propose here to employ soft chsterinf,  i.e.,a word can be assigned to several different clustersand each cluster is characterized by a specific wordprobability distribution.
We define for each cate-gory a finite mixture model, which is a linear com-bination of the word probability distributions of theclusters.
We thereby treat the problem of classify-ing documents as that of conducting statistical hy-pothesis testing over finite mixture models.
In or-der to accomplish ypothesis testing, we employ theEM algorithm to efficiently and approximately cal-culate from training data the maximum likelihoodestimates of parameters in a finite mixture model.Our method overcomes the major drawbacks ofthe method using word-based istributions and themethod based on hard clustering, while retainingtheir merits; it in fact includes those two methodsas special cases.
Experimental results indicate thatour method outperforrrLs them.Although the finite mixture model has alreadybeen used elsewhere in natural language processing(e.g.
(Jelinek and Mercer, 1980; Pereira, Tishby,and Lee, 1993)), this is the first work, to the best ofknowledge, that uses it in the context of documentclassification.2 P rev ious  WorkWord-based  methodA simple approach to document classification is toview this problem as that of conducting hypothesistesting over word-based istributions.
In this paper,we refer to this approach as the word-based method(hereafter, eferred to as WBM).2We borrow from (Pereira, Tishby, and Lee, 1993)the terms hard clustering and soft clustering, which wereused there in a different ask.39Letting W denote a vocabulary (a set of words),and w denote a random variable representing anyword in it, for each category ci (i = 1, .
.
.
,n) ,  wedefine its word-based distribution P(wIci) as a his-togram type of distribution over W. (The num-ber of free parameters of such a distribution is thusI W\[ -  1).
WBM then views a document as a sequenceof words:d = Wl , ' ' "  , W N (1)and assumes that each word is generated indepen-dently according to a probability distribution of acategory.
It then calculates the probability of a doc-ument with respect o a category asNP(dlc,) = P(w,, .
.
.
,~Nle,)  = 1-~ P(w, lc,), (2)t= land classifies the document into that category forwhich the calculated probability is the largest.
Weshould note here that a document's probability withrespect o each category is equivMent o the likeli-hood of each category with respect o the document,and to classify the document into the category forwhich it has the largest probability is equivalent toclassifying it into the category having the largestlikelihood with respect o it.
Hereafter, we will useonly the term likelihood and denote it as L(dlci).Notice that in practice the parameters in a dis-tribution must be estimated from training data.
Inthe case of WBM, the number of parameters is large;the training data size, however, is usually not suffi-ciently large for accurately estimating them.
Thisis the data .sparseness problem that so often standsin the way of reliable statistical language processing(e.g.
(Gale and Church, 1990)).
Moreover, the num-ber of parameters in word-based istributions i toolarge to be efficiently stored.Method based on hard clusteringIn order to address the above difficulty, Guthrieet.al, have proposed a method based on hard cluster-ing of words (Guthrie, Walker, and Guthrie, 1994)(hereafter we will refer to this method as HCM).
Letcl,...,c,~ be categories.
HCM first conducts hardclustering of words.
Specifically, it (a) defines a vo-cabulary as a set of words W and defines as clustersits subsets kl , .
.
- ,k,n satisfying t3~=xk j = W andki fq k j  = 0 (i  j )  (i.e., each word is assigned onlyto a single cluster); and (b) treats uniformly all thewords assigned to the same cluster.
HCM then de-fines for each category ci a distribution of the clus-ters P(kj \[ci) (j = 1 , .
.
.
,m) .
It replaces each wordwt in the document with the cluster kt to which itbelongs (t = 1,--., N).
It assumes that a cluster ktis distributed according to P(kj\[ci) and calculatesthe likelihood of each category ci with respect tothe document byNL(dle,) -- L (k l , .
.
.
,  kNlci) = H e(k ,  le,).t= l(3)Table 1: Frequencies of wordsracket stroke shot goal kick ballcl 4 1 2 1 0 2c2 0 0 0 3 2 2Table 2: Clusters and words (L = 5,M = 5)' kl racket, stroke, shotks kick.
k 3 goal, ballTable 3: Frequencies of clusterskl ks k3c 1 7 0 3c2 0 2 5There are any number of ways to create clusters inhard clustering, but the method employed is crucialto the accuracy of document classification.
Guthrieet.
al.
have devised a way suitable to documentationclassification.
Suppose that there are two categoriescl ='tennis' and c2='soccer,' and we obtain from thetraining data (previously classified documents) thefrequencies of words in each category, such as thosein Tab.
1.
Letting L and M be given positive inte-gers, HCM creates three clusters: kl, k2 and k3, inwhich kl contains those words which are among theL most frequent words in cl, and not among the Mmost frequent in c2; k2 contains those words whichare among the L most frequent words in cs, andnot among the M most frequent in Cl; and k3 con-tains all remaining words (see Tab.
2).
HCM thencounts the frequencies of clusters in each category(see Tab.
3) and estimates the probabilities of clus-ters being in each category (see Tab.
4).
3 Supposethat a newly given document, like d in Fig.
i, is tobe classified.
HCM cMculates the likelihood values3We calculate the probabilities here by using the so-called expected likelihood estimator (Gale and Church,1990):.
f (k j lc ,  ) + 0.5 ,P(k3lc~) = f -~- -~-~ x m (4)where f(kjlci ) is the frequency of the cluster kj in ci,f(ci) is the total frequency of clusters in cl, and m is thetotal number of clusters.40Table 4: Probability distributions of clusterskl k2 k3cl 0.65 0.04 0.30cs 0.06 0.29 0.65L(dlCl ) and L(dlc2) according to Eq.
(3).
(Tab.
5shows the logarithms of the resulting likelihood val-ues.)
It then classifies d into cs, as log s L(dlcs ) islarger than log s L(dlc 1).d = kick, goal, goal, ballFigure 1: Example documentTable 5: Calculating log likelihood valueslog2 L(dlct )= 1 x log s .04 + 3 ?
log s .30 = -9.85log s L(d\]cs)= 1 ?
log s .29 + 3 x log s .65 = -3.65HCM can handle the data sparseness problemquite well.
By assigning words to clusters, it candrastically reduce the number of parameters to beestimated.
It can also save space for storing knowl-edge.
We argue, however, that the use of hard clus-tering still has the following two problems:1.
HCM cannot assign a word ?0 more than onecluster at a time.
Suppose that there is anothercategory c3 = 'skiing' in which the word 'ball'does not appear, i.e., 'ball' will be indicative ofboth cl and c2, but not cs.
If we could assign'ball' to both kt and k2, the likelihood value forclassifying a document containing that word tocl or c2 would become larger, and that for clas-sifying it into c3 would become smaller.
HCM,however, cannot do that.2.
HCM cannot make the best use of informationabout the differences among the frequencies ofwords assigned to an individual cluster.
For ex-ample, it treats 'racket' and 'shot' uniformly be-cause they are assigned to the same cluster kt(see Tab.
5).
'Racket' may, however, be moreindicative of Cl than 'shot,' because it appearsmore frequently in cl than 'shot.'
HCM failsto utilize this information.
This problem willbecome more serious when the values L and Min word clustering are large, which renders theclustering itself relatively meaningless.From the perspective of number of parameters,HCM employs models having very few parameters,and thus may not sometimes represent much usefulinformation for classification.3 F in i te  M ix ture  Mode lWe propose a method of document classificationbased on soft clustering of words.
Let c l , - - .
, cnbe categories.
We first conduct the soft cluster-ing.
Specifically, we (a) define a vocabulary as aset W of words and define as clusters a number ofits subsets k l , .
- - ,  k,n satisfying u'~=lk j = W; (no-tice that ki t3 kj = 0 (i ~ j) does not necessarilyhold here, i.e., a word can be assigned to several dif-ferent clusters); and (b) define for each cluster kj(j = 1, .
.
.
,  m) a distribution Q(w\[kj) over its words()"\]~wekj Q(w\[kj) = 1) and a distribution P(wlkj)satisfying:!
Q(wlki); wek  i, P(wlkj) 0; w ?
(5)where w denotes a random variable representing anyword in the vocabulary.
We then define for each cat-egory ci (i = 1, .
.
.
,  n) a distribution of the clustersP(kj Ici), and define for each category a linear com-bination of P(w\]kj):P(wlc~) = ~ P(kjlc~) x P(wlk.i) (6)j=las the distribution over its words, which is referredto as afinite mixture model(e.g., (Everitt and Hand,1981)).We treat the problem of classifying a documentas that of conducting the likelihood ratio test overfinite mixture models.
That is, we view a documentas a sequence of words,d= wl ,  " " , WN (7)where wt(t = 1, .
- .
,N )  represents a word.
Weassume that each word is independently generatedaccording to an unknown probability distributionand determine which of the finite mixture mod-els P(w\[ci)(i = 1, .
.
.
,n)  is more likely to be theprobability distribution by observing the sequence ofwords.
Specifically, we calculate the likelihood valuefor each category with respect o the document by:L(d\[ci) = L(wl , .
.
.
,wglc i )= I-\[~=1 P(wt lc , ): n =l P(k ic,) x P(w, lk ))(8)We then classify it into the category having thelargest likelihood value with respect o it.
Hereafter,we will refer to this method as FMM.FMM includes WBM and HCM as its specialcases.
If we consider the specific case (1) in whicha word is assigned to a single cluster and P(wlkj) isgiven by {1.
(9) P(wlkj)= O; w~k~,41where Ikjl denotes the number of elements belongingto kj, then we will get the same classification resultas in HCM.
In such a case, the likelihood value foreach category ci becomes:L(dlc,) = I-I;:x (P(ktlci) x P~wtlkt))= 1-It=~ P(ktlci) x l-It=lP(Wtlkt),(lo)where kt is the cluster corresponding to wt.
Sincethe probability P(wt\]kt) does not depend on eate-N gories, we can ignore the second term YIt=l P(wt Ikt)in hypothesis testing, and thus our method essen-tially becomes equivalent to HCM (c.f.
Eq.
(3)).Further, in the specific case (2) in which m = n,for each j, P(wlkj) has IWl parameters: P(wlkj) =P(wlcj), and P(kjlci ) is given by1; i = j, P(kjlci)= O; i# j ,  (11)the likelihood used in hypothesis testing becomesthe same as that in Eq.
(2), and thus our methodbecomes equivalent to WBM.4 Es t imat ion  and  Hypothes isTes t ingIn this section, we describe how to implement ourmethod.Creat ing  c lustersThere are any number of ways to create clusters on agiven set of words.
As in the case of hard clustering,the way that clusters are created is crucial to thereliability of document classification.
Here we giveone example approach to cluster creation.Table 6: Clusters and wordsIk l  Iracket, stroke, shot, bal l lks kick, goal, ballWe let the number of clusters equal that of cat-egories (i.e., m = n) 4 and relate each cluster kito one category ci (i = 1, - - .
,n) .
We then assignindividual words to those clusters in whose relatedcategories they most frequently appear.
Letting 7(0 _< 7 < 1) be a predetermined threshold value, ifthe following inequality holds:f(wlci) > 7, (t2)f(w)then we assign w to ki, the cluster related to ci,where f(wlci) denotes the frequency of the word win category ci, and f(w) denotes the total frequencyofw.
Using the data in Tab.l,  we create two clusters:kt and k2, and relate them to ct and c2, respectively.4One can certainly assume that m > n.For example, when 7 = 0.4, we assign 'goal' to k2only, as the relative frequency of 'goal' in c~ is 0.75and that in cx is only 0.25.
We ignore in documentclassification those words which cannot be assignedto any cluster using this method, because they arenot indicative of any specific category.
(For example,when 7 >_ 0.5 'ball' will not be assigned into anycluster.)
This helps to make classification efficientand accurate.
Tab.
6 shows the results of creatingclusters.Es t imat ing  P(wlk j)We then consider the frequency of a word in a clus-ter.
If a word is assigned only to one cluster, we viewits total frequency as its frequency within that clus-ter.
For example, because 'goal' is assigned only toks, we use as its frequency within that cluster the to-tal count of its occurrence in all categories.
If a wordis assigned to several different clusters, we distributeits total frequency among those clusters in propor-tion to the frequency with which the word appearsin each of their respective related categories.
Forexample, because 'ball' is assigned to both kl andk2, we distribute its total frequency among the twoclusters in proportion to the frequency with which'ball' appears in cl and c2, respectively.
After that,we obtain the frequencies of words in each cluster asshown in Tab.
7.Table 7: Distributed frequencies of wordsracket stroke shot goal kick ballkl 4 1 2 0 0 2k2 0 0 0 4 2 2We then estimate the probabilities of words ineach cluster, obtaining the results in Tab.
8.
5Table 8: Probability distributions of wordsracket stroke shot goal kick ballkl 0.44 0.11 0.22 0 0 0.22k2 0 0 0 0.50 0.25 0.25Estimating P( kj \]ci)Let us next consider the estimation of P(kj\[ci).There are two common methods for statistical esti-mation, the maximum likelihood estimation method5We calculate the probabilities by employing themaximum likelihood estimator:/(kAc0 (13) P(kilci)- f(ci) 'where f(kj\]cl) is the frequency of the cluster kj in ci,and f(cl) is the total frequency of clusters in el.42Table 10: Calculating log likelihood values\[log~L(d\[cl)= log2(.14?
.25)+2x log2(.14x .50)+log2(.86x.22 +.14x  .25) :  -14 .67\ [I log S L(dlc2 ) 1og2(.96 x .25) + 2 x log2(.96 x .50) + 1og2(.04 x .22 T .96 ?
.25) -6.18 ITable 9: Probability distributions of clusterskl k2Cl 0.86 0.14c2 0.04 0.96and the Bayes estimation method.
In their imple-mentation for estimating P(kj Ici), however, both ofthem suffer from computational intractability.
TheEM algorithm (Dempster, Laird, and Rubin, 1977)can be used to efficiently approximate he maximumlikelihood estimator of P(kj \[c~).
We employ here anextended version of the EM algorithm (Helmbold etal., 1995).
(We have also devised, on the basis ofthe Markov chain Monte Carlo (MCMC) technique(e.g.
(Tanner and Wong, 1987; Yamanishi, 1996)) 6,an algorithm to efficiently approximate the Bayesestimator of P(kj \[c~).
)For the sake of notational simplicity, for a fixed i,let us write P(kjlci) as Oj and P(wlkj) as Pj(w).Then letting 9 = (01, ' " ,0m),  the finite mixturemodel in Eq.
(6) may be written asrnP(wlO) = ~0~ x Pj(w).
(14)j= lFor a given training sequence wl'"WN, the maxi-mum likelihood estimator of 0 is defined as the valuewhich maximizes the following log likelihood func-tion) L(O) = ~' log OjPj(wt) .
(15)~- \ j= lThe EM algorithm first arbitrarily sets the initialvalue of 0, which we denote as 0(0), and then suc-cessively calculates the values of 6 on the basis of itsmost recent values.
Let s be a predetermined num-ber.
At the lth iteration (l -: 1, .
.
- ,  s), we calculate= by0~ ' ) :  0~ '-1) (~?
(VL(00-1)) j -  1)+ 1),  (16)where ~ > 0 (when ~ = 1, Hembold et al 's versionsimply becomes the standard EM algorithm), and6We have confirmed in our preliminary experimentthat MCMC performs lightly better than EM in docu-ment classification, but we omit the details here due tospace limitations.~TL(O) denotesv L(O) = ( 0L001 "'" O0,nOL ) .
(17)After s numbers of calculations, the EM algorithmoutputs 00) = (0~O,... ,0~ )) as an approximate of0.
It is theoretically guaranteed that the EM al-gorithm converges to a local minimum of the givenlikelihood (Dempster, Laird, and Rubin, 1977).For the example in Tab.
1, we obtain the resultsas shown in Tab.
9.Test ingFor the example in Tab.
1, we can calculate ac-cording to Eq.
(8) the likelihood values of the twocategories with respect o the document in Fig.
1(Tab.
10 shows the logarithms of the likelihood val-ues).
We then classify the document into categoryc2, as log 2 L(d\]c2) is larger than log 2 L(dlcl).5 Advantages  o f  FMMFor a probabilistic approach to document classifica-tion, the most important thing is to determine whatkind of probability model (distribution) to employas a representation of a category.
It must (1) ap-propriately represent a category, as well as (2) havea proper preciseness in terms of number of param-eters.
The goodness and badness of selection of amodel directly affects classification results.The finite mixture model we propose is particu-larly well-suited to the representation f a category.Described in linguistic terms, a cluster correspondsto a topic and the words assigned to it are relatedto that topic.
Though documents generally concen-trate on a single topic, they may sometimes referfor a time to others, and while a document is dis-cussing any one topic, it will naturally tend to usewords strongly related to that topic.
A document inthe category of 'tennis' is more likely to discuss thetopic of 'tennis,' i.e., to use words strongly relatedto 'tennis,' but it may sometimes briefly shift to thetopic of 'soccer,' i.e., use words strongly related to'soccer.'
A human can follow the sequence of wordsin such a document, associate them with related top-ics, and use the distributions of topics to classify thedocument.
Thus the use of the finite mixture modelcan be considered as a stochastic implementation fthis process.The use of FMM is also appropriate from theviewpoint of number of parameters.
Tab.
11 showsthe numbers of parameters in our method (FMM),43Table 11: Num.
of parametersWBM O(n. IWl)HCM O(n. m)FMM o(Ikl+n'm)HCM, and WBM, where IW\] is the size of a vocab-ulary, Ikl is the sum of the sizes of word clustersm (i.e.,Ikl -- E~=I Ikil), n is the number of categories,and m is the number of clusters.
The number ofparameters in FMM is much smaller than that inWBM, which depends on IWl, a very large num-ber in practice (notice that Ikl is always smallerthan IWI when we employ the clustering method(with 7 > 0.5) described in Section 4.
As a result,FMM requires less data for parameter estimationthan WBM and thus can handle the data sparsenessproblem quite well.
Furthermore, it can economizeon the space necessary for storing knowledge.
Onthe other hand, the number of parameters in FMMis larger than that in HCM.
It is able to represent thedifferences between categories more precisely thanHCM, and thus is able to resolve the two problems,described in Section 2, which plague HCM.Another advantage of our method may be seen incontrast to the use of latent semantic analysis (Deer-wester et al, 1990) in document classification anddocument retrieval.
They claim that their methodcan solve the following problems:synonymy problem how to group synonyms, like'stroke' and 'shot,' and make each relativelystrongly indicative of a category even thoughsome may individually appear in the categoryonly very rarely;po lysemy problem how to determine that a wordlike 'ball' in a document refers to a 'tennis ball'and not a 'soccer ball,' so as to classify the doc-ument more accurately;dependence problem how to use de-pendent words, like 'kick' and 'goal,' to maketheir combined appearance in a document moreindicative of a category.As seen in Tab.6, our method also helps resolve allof these problems.6 P re l iminary  Exper imenta l  Resu l t sIn this section, we describe the results of the exper-iments we have conducted to compare the perfor-mance of our method with that of HCM and others.As a first data set, we used a subset of the Reutersnewswire data prepared by Lewis, called Reuters-21578 Distribution 1.0.
7 We selected nine overlap-ping categories, i.e.
in which a document may be-rReuters-21578 is available athttp://www.research.att.com/lewis.long to several different categories.
We adopted theLewis Split in the corpus to obtain the training dataand the test data.
Tabs.
12 and 13 give the de-tails.
We did not conduct stemming, or use stopwords .
We then applied FMM, HCM, WBM , anda method based on cosine-similarity, which we de-note as COS 9, to conduct binary classification.
Inparticular, we learn the distribution for each cate-gory and that for its complement category from thetraining data, and then determine whether or not toclassify into each category the documents in the testdata.
When applying FMM, we used our proposedmethod of creating clusters in Section 4 and set 7to be 0, 0.4, 0.5, 0.7, because these are representativevalues.
For HCM, we classified words in the sameway as in FMM and set 7 to be 0.5, 0.7, 0.9, 0.95.
(Notice that in HCM, 7 cannot be set less than 0.5.
)Table 12: The first data setNum.
of doc.
in training data 707Num.
of doc in test data 228Num.
of (type of) words 10902Avg.
num.
of words per doe.
310.6Table 13: Categories in the first data setI wheat,corn,oilseed,sugar,coffeesoybean,cocoa,rice,cotton \]Table 14: The second data setNum.
of doc.
training data 13625Num.
of doc.
in test data 6188Num.
of (type of) words 50301Avg.
num.
of words per doc.
181.3As a second data set, we used the entire Reuters-21578 data with the Lewis Split.
Tab.
14 gives thedetails.
Again, we did not conduct stemming, or usestop words.
We then applied FMM, HCM, WBM ,and COS to conduct binary classification.
When ap-plying FMM, we used our proposed method of creat-ing clusters and set 7 to be 0, 0.4, 0.5, 0.7.
For HCM,we classified words in the same way as in FMM andset 7 to be 0.5, 0.7, 0.9, 0.95.
We have not fully com-pleted these experiments, however, and here we only8'Stop words' refers to a predetermined list of wordscontaining those which are considered not useful for doc-ument classification, such as articles and prepositions.9In this method, categories and documents obe clas-sified are viewed as vectors of word frequencies, and thecosine value between the two vectors reflects similarity(Salton and McGill, 1983).44Table 15: Tested categories in the second data setearn,acq,crude,money-fx,gr aininterest,trade,ship,wheat,corn \]give the results of classifying into the ten categorieshaving the greatest numbers of documents in the testdata (see Tab.
15).For both data sets, we evaluated each method interms of precision and recall by means of the so-called micro-averaging 10When applying WBM, HCM, and FMM, ratherthan use the standard likelihood ratio testing, weused the following heuristics.
For simplicity, supposethat there are only two categories cl and c2.
Letting?
be a given number larger than or equal 0, we assigna new document d in the following way:~ (logL(dlcl) - logL(dlc2)) > e; d --* cl, (logL(dlc2) logL(dlct)) > ~; d---+ cu,otherwise; unclassify d,(is)where N is the size of document d. (One can easilyextend the method to cases with a greater ~umber ofcategories.)
11 For COS, we conducted classificationin a similar way.Fig s. 2 and 3 show precision-recall curves for thefirst data set and those for the second data set, re-spectively.
In these graphs, values given after FMMand HCM represent 3' in our clustering method (e.g.FMM0.5, HCM0.5,etc).
We adopted the break-evenpoint as a single measure for comparison, which isthe one at which precision equals recall; a higherscore for the break-even point indicates better per-formance.
Tab.
16 shows the break-even point foreach method for the first data set and Tab.
17 showsthat for the second data set.
For the first data set,FMM0 attains the highest score at break-even point;for the second data set, FMM0.5 attains the highest.We considered the following questions:(1) The training data used in the experimen-tation may be considered sparse.
Will a word-clustering-based method (FMM) outperform a word-based method (WBM) here?
(2) Is it better to conduct soft clustering (FMM)than to do hard clustering (HCM)?
(3) With our current method of creating clusters,as the threshold 7 approaches 0, FMM behaves muchlike WBM and it does not enjoy the effects of clus-tering at all (the number of parameters is as largel?In micro-averaging(Lewis and Ringuette, 1994), pre-cision is defined as the percentage of classified ocumentsin all categories which are correctly classified.
Recall isdefined as the percentage of the total documents in allcategories which are correctly classified.nNotice that words which are discarded in the duster-ing process should not to be counted in document size.I0.g0.80.7~ 0.60.50.40.30.2~" .
.
.
.
_ ' :~ .
.
"HCM0.S" -e-..~" .
....
::.':.
~ ?HCM0.7" .-v,--,.-" " .
.~ " '~"~.
.
"HCMO.9" ~- -.~/  - " "-~, "HCM0.g5" -~'--?
."
~, "., "FMM0" -e-.-/ ~.
~ "FMM0.4" "+--~- .
.
/ '~ ~--~ "FMM0.5" -e  --y .
.
.
.
-,, "FMMO.7"/.~::::~:..-~-- '-,., .
-1  .......... : ..... .0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9recallFigure 2: Precision-recall curve for the first data setcI IO.g0.80.70.60,50.40.30.20.1"WBM" "+- -"HCM0.5" -D-"HCM0.7 = K- -GI, "" "HCMO.g" ~.
-.. " " '~-  .... "HCMO.g5" "~-- -" ' " ' l~  ~3~ "FMMO" -e.
-.".
"~.
~ ..-Q ?FMM0.4" -+--?
.
.
.
.
,.
:" .
.
",,,.
"FMM0.5" -Q--% " -,~ "FMM0.7 ~"-... "... ~, ~?0, 012 0:~ 01, 0:s 0:0 0:, 0:8 01,recallFigure 3: Precision-recall curve for the second datasetas in WBM).
This is because in this case (a) a wordwill be assigned into all of the clusters, (b) the dis-tribution of words in each cluster will approach thatin the corresponding category in WBM, and (c) thelikelihood value for each category will approach thatin WBM (recall case (2) in Section 3).
Since creatingclusters in an optimal way is difficult, when cluster-ing does not improve performance we can at leastmake FMM perform as well as WBM by choosing7 = 0.
The question now is "does FMM performbetter than WBM when 7 is 0?
"In looking into these issues, we found the follow-ing:(1) When 3' >> 0, i.e., when we conduct clustering,FMM does not perform better than WBM for thefirst data set, but it performs better than WBM forthe second data set.Evaluating classification results on the basis ofeach individual category, we have found that forthree of the nine categories in the first data set,45Table 16: Break-even pointCOSWBMHCM0.5HCM0.7HCM0.9HCM0.95FMM0FMM0.4FMM0.5FMM0.7for thq first data set0.600.620.320.420.540.510.660.540.520.42Table 17: Break-even point for theCOS 10.52WBM !0.62HCM0.5 10.47HCM0.7 i0.51HCM0.9 10.55HCM0.95 0.31FMM0 i0.62FMM0.4 0.54FMM0.5 0.67FMM0.7 0.62second data setFMM0.5 performs best, and that in two of the tencategories in the second data set FMM0.5 performsbest.
These results indicate that clustering some-times does improve classification results when weuse our current way of creating clusters.
(Fig.
4shows the best result for each method for the cate-gory 'corn' in the first data set and Fig.
5 that for'grain' in the second data set.
)(2) When 3' >> 0, i.e., when we conduct clustering,the best of FMM almost always outperforms that ofHCM.
(3) When 7 = 0, FMM performs better thanWBM for the first data set, and that it performsas well as WBM for the second data set.In summary, FMM always outperforms HCM; insome cases it performs better than WBM; and ingeneral it performs at least as well as WBM.For both data sets, the best FMM results are supe-rior to those of COS throughout.
This indicates thatthe probabilistic approach is more suitable than thecosine approach for document classification based onword distributions.Although we have not completed our experimentson the entire Reuters data set, we found that the re-sults with FMM on the second data set are almost asgood as those obtained by the other approaches re-ported in (Lewis and Ringuette, 1994).
(The resultsare not directly comparable, because (a) the resultsin (Lewis and Ringuette, 1994) were obtained froman older version of the Reuters data; and (b) theyt0,90.80.70.80.8'COS"" ' ~ /  , "HCMO.9" ~- .? '
~  "~., "FMMO.8", /  "-~o'., ?
'., o'.~ o'., o.~ oi?
oi, o'.8 o'.8ror,~Figure 4: Precision-recall curve for category 'corn'1?.90.80.70,60.50.40.30.2O.t"".. k~,?
... ~"h~MO.7" "e--  " ,FMI?~.$I 0'., 0'., 0'., 0'., 0'.8 0'., 0., 0.?
01,Figure 5: Precision-recall curve for category 'grain'used stop words, but we did not.
)We have also conducted experiments on the Su-sanne corpus data t2 and confirmed the effectivenessof our method.
We omit an explanation of this workhere due to space limitations.7 Conc lus ionsLet us conclude this paper with the following re-marks:1.
The primary contribution of this research isthat we have proposed the use of the finite mix-ture model in document classification.2.
Experimental results indicate that our methodof using the finite mixture model outperformsthe method based on hard clustering of words.3.
Experimental results also indicate that in somecases our method outperforms the word-based12The Susanne corpus, which has four non-overlappingcategories, is ~va~lable at ftp://ota.ox.ac.uk46method when we use our current method of cre-ating clusters.Our future work is to include:1. comparing the various methods over the entireReuters corpus and over other data bases,2.
developing better ways of creating clusters.Our proposed method is not limited to documentclassification; it can also be applied to other natu-ral language processing tasks, like word sense dis-ambiguation, in which we can view the context sur-rounding a ambiguous target word as a documentand the word-senses to be resolved as categories.AcknowledgementsWe are grateful to Tomoyuki Fujita of NEC for hisconstant encouragement.
We also thank Naoki Abeof NEC for his important suggestions, and Mark Pe-tersen of Meiji Univ.
for his help with the English ofthis text.
We would like to express pecial apprecia-tion to the six ACL anonymous reviewers who haveprovided many valuable comments and criticisms.ReferencesApte, Chidanand, Fred Damerau, and Sholom M.Weiss.
1994.
Automated learning of decision rulesfor text categorization.
ACM Tran.
on Informa-tion Systems, 12(3):233-251.Cohen, William W. and Yoram Singer.
1996.Context-sensitive learning methods for text cat-egorization.
Proc.
of SIGIR'96.Deerwester, Scott, Susan T. Dumais, George W.Furnas, Thomas K. Landauer, and Richard Harsh-man.
1990.
Indexing by latent semantic analysis.Journ.
of the American Society for InformationScience, 41(6):391-407.Dempster, A.P., N.M. Laird, and D.B.
Rubin.
1977.Maximum likelihood from incomplete data via theem algorithm.
Journ.
of the Royal Statistical So-ciety, Series B, 39(1):1-38.Everitt, B. and D. Hand.
1981.
Finite Mixture Dis-tributions.
London: Chapman and Hall.Fuhr, Norbert.
1989.
Models for retrieval with prob-abilistic indexing.
Information Processing andManagement, 25(1):55-72.Gale, Williams A. and Kenth W. Church.
1990.Poor estimates of context are worse than none.Proc.
of the DARPA Speech and Natural LanguageWorkshop, pages 283-287.Guthrie, Louise, Elbert Walker, and Joe Guthrie.1994.
Document classification by machine: The-ory and practice.
Proc.
of COLING'94, pages1059-1063.Helmbold, D., R. Schapire, Y. Siuger, and M. War-muth.
1995.
A comparison of new and old algo-rithm for a mixture estimation problem.
Proc.
ofCOLT'95, pages 61-68.Jelinek, F. and R.I. Mercer.
1980.
Interpolated esti-mation of markov source parameters from sparsedata.
Proc.
of Workshop on Pattern Recognitionin Practice, pages 381-402.Lewis, David D. 1992.
An evaluation of phrasal andclustered representations on a text categorizationtask.
Proc.
of SIGIR'9~, pages 37-50.Lewis, David D. and Marc Ringuette.
1994.
A com-parison of two learning algorithms for test catego-rization.
Proc.
of 3rd Annual Symposium on Doc-ument Analysis and Information Retrieval, pages81-93.Lewis, David D., Robert E. Schapire, James P.Callan, and Ron Papka.
1996.
Training algo-rithms for linear text classifiers.
Proc.
of SI-GIR '96.Pereira, Fernando, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of english words.Proc.
of ACL '93, pages 183-190.Robertson, S.E.
and K. Sparck Jones.
1976.
Rel-evance weighting of search terms.
Journ.
ofthe American Society for Information Science,27:129-146.Salton, G. and M.J. McGiU.
1983.
Introduction toModern Information Retrieval.
New York: Mc-Graw Hill.Schutze, Hinrich, David A.
Hull, and Jan O. Peder-sen. 1995.
A comparison of classifiers and doc-ument representations for the routing problem.Proc.
of SIGIR '95.Tanner, Martin A. and Wing Hung Wong.
1987.The calculation of posterior distributions by dataaugmentation.
Journ.
of the American StatisticalAssociation, 82(398):528-540.Wong, S.K.M.
and Y.Y.
Ya~.
1989.
A probabilitydistribution model for information retrieval.
In-formation Processing and Management, 25(1):39-53.Yamanishi, Kenji.
1996.
A randomized approxima-tion of the mdl for stochastic models with hiddenvariables.
Proc.
of COLT'96, pages 99-109.Yang, Yiming and Christoper G. Chute.
1994.
Anexample-based mapping method for text catego-rization and retrieval.
A CM Tran.
on InformationSystems, 12(3):252-277.47
