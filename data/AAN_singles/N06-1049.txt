Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 383?390,New York, June 2006. c?2006 Association for Computational LinguisticsWill Pyramids Built of Nuggets Topple Over?Jimmy Lin1,2,3 and Dina Demner-Fushman2,31College of Information Studies2Department of Computer Science3Institute for Advanced Computer StudiesUniversity of MarylandCollege Park, MD 20742, USAjimmylin@umd.edu, demner@cs.umd.eduAbstractThe present methodology for evaluatingcomplex questions at TREC analyzes an-swers in terms of facts called ?nuggets?.The official F-score metric represents theharmonic mean between recall and pre-cision at the nugget level.
There is animplicit assumption that some facts aremore important than others, which is im-plemented in a binary split between ?vi-tal?
and ?okay?
nuggets.
This distinc-tion holds important implications for theTREC scoring model?essentially, sys-tems only receive credit for retrieving vi-tal nuggets?and is a source of evalua-tion instability.
The upshot is that formany questions in the TREC testsets, themedian score across all submitted runs iszero.
In this work, we introduce a scor-ing model based on judgments from mul-tiple assessors that captures a more refinednotion of nugget importance.
We demon-strate on TREC 2003, 2004, and 2005 datathat our ?nugget pyramids?
address manyshortcomings of the present methodology,while introducing only minimal additionaloverhead on the evaluation flow.1 IntroductionThe field of question answering has been movingaway from simple ?factoid?
questions such as ?Whoinvented the paper clip??
to more complex informa-tion needs such as ?Who is Aaron Copland??
and?How have South American drug cartels been usingbanks in Liechtenstein to launder money?
?, whichcannot be answered by simple named-entities.
Overthe past few years, NIST through the TREC QAtracks has implemented an evaluation methodologybased on the notion of ?information nuggets?
to as-sess the quality of answers to such complex ques-tions.
This paradigm has gained widespread accep-tance in the research community, and is currently be-ing applied to evaluate answers to so-called ?defini-tion?, ?relationship?, and ?opinion?
questions.Since quantitative evaluation is arguably the sin-gle biggest driver of advances in language technolo-gies, it is important to closely examine the charac-teristics of a scoring model to ensure its fairness, re-liability, and stability.
In this work, we identify apotential source of instability in the nugget evalua-tion paradigm, develop a new scoring method, anddemonstrate that our new model addresses some ofthe shortcomings of the original method.
It is ourhope that this more-refined evaluation model canbetter guide the development of technology for an-swering complex questions.This paper is organized as follows: Section 2provides a brief overview of the nugget evaluationmethodology.
Section 3 draws attention to the vi-tal/okay nugget distinction and the problems it cre-ates.
Section 4 outlines our proposal for building?nugget pyramids?, a more-refined model of nuggetimportance that combines judgments from multipleassessors.
Section 5 describes the methodology forevaluating this new model, and Section 6 presentsour results.
A discussion of related issues appears inSection 7, and the paper concludes with Section 8.3832 Evaluation of Complex QuestionsTo date, NIST has conducted three large-scale eval-uations of complex questions using a nugget-basedevaluation methodology: ?definition?
questions inTREC 2003, ?other?
questions in TREC 2004 andTREC 2005, and ?relationship?
questions in TREC2005.
Since relatively few teams participated inthe 2005 evaluation of ?relationship?
questions, thiswork focuses on the three years?
worth of ?defini-tion/other?
questions.
The nugget-based paradigmhas been previously detailed in a number of pa-pers (Voorhees, 2003; Hildebrandt et al, 2004; Linand Demner-Fushman, 2005a); here, we presentonly a short summary.System responses to complex questions consist ofan unordered set of passages.
To evaluate answers,NIST pools answer strings from all participants, re-moves their association with the runs that producedthem, and presents them to a human assessor.
Us-ing these responses and research performed duringthe original development of the question, the asses-sor creates an ?answer key?
comprised of a list of?nuggets?
?essentially, facts about the target.
Ac-cording to TREC guidelines, a nugget is defined asa fact for which the assessor could make a binarydecision as to whether a response contained thatnugget (Voorhees, 2003).
As an example, relevantnuggets for the target ?AARP?
are shown in Table 1.In addition to creating the nuggets, the assessor alsomanually classifies each as either ?vital?
or ?okay?.Vital nuggets represent concepts that must be in a?good?
definition; on the other hand, okay nuggetscontribute worthwhile information about the targetbut are not essential.
The distinction has importantimplications, described below.Once the answer key of vital/okay nuggets is cre-ated, the assessor goes back and manually scoreseach run.
For each system response, he or she de-cides whether or not each nugget is present.
Thefinal F-score for an answer is computed in the man-ner described in Figure 1, and the final score of asystem run is the mean of scores across all ques-tions.
The per-question F-score is a harmonic meanbetween nugget precision and nugget recall, whererecall is heavily favored (controlled by the ?
param-eter, set to five in 2003 and three in 2004 and 2005).Nugget recall is computed solely on vital nuggetsvital 30+ million membersokay Spends heavily on research & educationvital Largest seniors organizationvital Largest dues paying organizationvital Membership eligibility is 50+okay Abbreviated name to attract boomersokay Most of its work done by volunteersokay Receives millions for product endorsementsokay Receives millions from product endorsementsTable 1: Answer nuggets for the target ?AARP?.Letr # of vital nuggets returned in a responsea # of okay nuggets returned in a responseR # of vital nuggets in the answer keyl # of non-whitespace characters in the entireanswer stringThenrecall (R) = r/Rallowance (?)
= 100?
(r + a)precision (P) ={1 if l < ?1?
l?
?l otherwiseFinally, the F?
= (?2 + 1)?
P ?R?2 ?
P +R?
= 5 in TREC 2003, ?
= 3 in TREC 2004, 2005.Figure 1: Official definition of F-score.
(which means no credit is given for returning okaynuggets), while nugget precision is approximated bya length allowance based on the number of both vi-tal and okay nuggets returned.
Early in a pilot study,researchers discovered that it was impossible for as-sessors to enumerate the total set of nuggets con-tained in a system response (Voorhees, 2003), whichcorresponds to the denominator in the precision cal-culation.
Thus, a penalty for verbosity serves as asurrogate for precision.Note that while a question?s answer key onlyneeds to be created once, assessors must manuallydetermine if each nugget is present in a system?s re-sponse.
This human involvement has been identifiedas a bottleneck in the evaluation process, althoughwe have recently developed an automatic scoringmetric called POURPRE that correlates well with hu-man judgments (Lin and Demner-Fushman, 2005a).384Testset # q?s 1 vital 2 vitalTREC 2003 50 3 10TREC 2004 64 2 15TREC 2005 75 5 16Table 2: Number of questions with few vital nuggetsin the different testsets.3 What?s Vital?
What?s Okay?Previously, we have argued that the vital/okay dis-tinction is a source of instability in the nugget-based evaluation methodology, especially given themanner in which F-score is calculated (Hildebrandtet al, 2004; Lin and Demner-Fushman, 2005a).Since only vital nuggets figure into the calculationof nugget recall, there is a large ?quantization ef-fect?
for system scores on topics that have few vitalnuggets.
For example, on a question that has onlyone vital nugget, a system cannot obtain a non-zeroscore unless that vital nugget is retrieved.
In reality,whether or not a system returned a passage contain-ing that single vital nugget is often a matter of luck,which is compounded by assessor judgment errors.Furthermore, there does not appear to be any reliableindicators for predicting the importance of a nugget,which makes the task of developing systems evenmore challenging.The polarizing effect of the vital/okay distinctionbrings into question the stability of TREC evalua-tions.
Table 2 shows statistics about the number ofquestions that have only one or two vital nuggets.Compared to the size of the testset, these numbersare relatively large.
As a concrete example, ?F16?
isthe target for question 71.7 from TREC 2005.
Theonly vital nugget is ?First F16s built in 1974?.
Thepractical effect of the vital/okay distinction in itscurrent form is the number of questions for whichthe median system score across all submitted runs iszero: 22 in TREC 2003, 41 in TREC 2004, and 44in TREC 2005.An evaluation in which the median score for manyquestions is zero has many shortcomings.
For one,it is difficult to tell if a particular run is ?better?
thananother?even though they may be very different inother salient properties such as length, for exam-ple.
The discriminative power of the present F-scoremeasure is called into question: are present systemsthat bad, or is the current scoring model insufficientto discriminate between different (poorly perform-ing) systems?Also, as pointed out by Voorhees (2005), a scoredistribution heavily skewed towards zero makesmeta-analysis of evaluation stability hard to per-form.
Since such studies depend on variability inscores, evaluations would appear more stable thanthey really are.While there are obviously shortcomings to thecurrent scheme of labeling nuggets as either ?vital?or ?okay?, the distinction does start to capture theintuition that ?not all nuggets are created equal?.Some nuggets are inherently more important thanothers, and this should be reflected in the evaluationmethodology.
The solution, we believe, is to solicitjudgments from multiple assessors and develop amore refined sense of nugget importance.
However,given finite resources, it is important to balance theamount of additional manual effort required with thegains derived from those efforts.
We present the ideaof building ?nugget pyramids?, which addresses theshortcomings noted here, and then assess the impli-cations of this new scoring model against data fromTREC 2003, 2004, and 2005.4 Building Nugget PyramidsAs previously pointed out (Lin and Demner-Fushman, 2005b), the question answering and sum-marization communities are converging on the taskof addressing complex information needs from com-plementary perspectives; see, for example, the re-cent DUC task of query-focused multi-documentsummarization (Amigo?
et al, 2004; Dang, 2005).From an evaluation point of view, this provides op-portunities for cross-fertilization and exchange offresh ideas.
As an example of this intellectual dis-course, the recently-developed POURPRE metric forautomatically evaluating answers to complex ques-tions (Lin and Demner-Fushman, 2005a) employsn-gram overlap to compare system responses to ref-erence output, an idea originally implemented in theROUGE metric for summarization evaluation (Linand Hovy, 2003).
Drawing additional inspirationfrom research on summarization evaluation, weadapt the pyramid evaluation scheme (Nenkova andPassonneau, 2004) to address the shortcomings of385the vital/okay distinction in the nugget-based evalu-ation methodology.The basic intuition behind the pyramidscheme (Nenkova and Passonneau, 2004) issimple: the importance of a fact is directly relatedto the number of people that recognize it as such(i.e., its popularity).
The evaluation methodologycalls for assessors to annotate Semantic ContentUnits (SCUs) found within model reference sum-maries.
The weight assigned to an SCU is equalto the number of annotators that have marked theparticular unit.
These SCUs can be arranged in apyramid, with the highest-scoring elements at thetop: a ?good?
summary should contain SCUs from ahigher tier in the pyramid before a lower tier, sincesuch elements are deemed ?more vital?.This pyramid scheme can be easily adapted forquestion answering evaluation since a nugget isroughly comparable to a Semantic Content Unit.We propose to build nugget pyramids for answersto complex questions by soliciting vital/okay judg-ments from multiple assessors, i.e., take the originalreference nuggets and ask different humans to clas-sify each as either ?vital?
or ?okay?.
The weight as-signed to each nugget is simply equal to the numberof different assessors that deemed it vital.
We thennormalize the nugget weights (per-question) so thatthe maximum possible weight is one (by dividingeach nugget weight by the maximum weight of thatparticular question).
Therefore, a nugget assigned?vital?
by the most assessors (not necessarily all)would receive a weight of one.1The introduction of a more granular notion ofnugget importance should be reflected in the calcu-lation of F-score.
We propose that nugget recall bemodified to take into account nugget weight:R =?m?Awm?n?V wnWhere A is the set of reference nuggets that arematched within a system?s response and V is the setof all reference nuggets; wm and wn are the weightsof nuggetsm and n, respectively.
Instead of a binarydistinction based solely on matching vital nuggets,all nuggets now factor into the calculation of recall,1Since there may be multiple nuggets with the highest score,what we?re building is actually a frustum sometimes.
:)subjected to a weight.
Note that this new scoringmodel captures the existing binary vital/okay dis-tinction in a straightforward way: vital nuggets geta score of one, and okay nuggets zero.We propose to leave the calculation of nugget pre-cision as is: a system would receive a length al-lowance of 100 non-whitespace characters for ev-ery nugget it retrieved (regardless of importance).Longer answers would be penalized for verbosity.Having outlined our revisions to the standardnugget-based scoring method, we will proceed todescribe our methodology for evaluating this newmodel and demonstrate how it overcomes many ofthe shortcomings of the existing paradigm.5 Evaluation MethodologyWe evaluate our methodology for building ?nuggetpyramids?
using runs submitted to the TREC 2003,2004, and 2005 question answering tracks (2003?definition?
questions, 2004 and 2005 ?other?
ques-tions).
There were 50 questions in the 2003 testset,64 in 2004, and 75 in 2005.
In total, there were 54runs submitted to TREC 2003, 63 to TREC 2004,and 72 to TREC 2005.
NIST assessors have man-ually annotated nuggets found in a given system?sresponse, and this allows us to calculate the final F-score under different scoring models.We recruited a total of nine different assessors forthis study.
Assessors consisted of graduate studentsin library and information science and computer sci-ence at the University of Maryland as well as volun-teers from the question answering community (ob-tained via a posting to NIST?s TREC QA mailinglist).
Each assessor was given the reference nuggetsalong with the original questions and asked to clas-sify each nugget as vital or okay.
They were pur-posely asked to make these judgments without refer-ence to documents in the corpus in order to expeditethe assessment process?our goal is to propose a re-finement to the current nugget evaluation methodol-ogy that addresses shortcomings while minimizingthe amount of additional effort required.
Combinedwith the answer key created by the original NISTassessors, we obtained a total of ten judgments forevery single nugget in the three testsets.22Raw data can be downloaded at the following URL:http://www.umiacs.umd.edu/?jimmylin3862003 2004 2005Assessor Kendall?s ?
zeros Kendall?s ?
zeros Kendall?s ?
zeros0 1.00 22 1.00 41 1.00 441 0.908 20 0.933 36 0.888 432 0.896 21 0.916 43 0.900 413 0.903 21 0.917 38 0.897 394 0.912 20 0.914 42 0.879 565 0.873 23 0.926 40 0.841 536 0.889 29 0.908 32 0.894 397 0.900 22 0.930 37 0.890 548 0.909 18 0.932 29 0.891 359 0.879 26 0.908 49 0.877 58average 0.896 22.2 0.920 38.7 0.884 46.2Table 3: Kendall?s ?
correlation between system scores generated using ?official?
vital/okay judgments andeach assessor?s judgments.
(Assessor 0 represents the original NIST assessors.
)We measured the correlation between systemranks generated by different scoring models usingKendall?s ?
, a commonly-used rank correlation mea-sure in information retrieval for quantifying the sim-ilarity between different scoring methods.
Kendall?s?
computes the ?distance?
between two rankings asthe minimum number of pairwise adjacent swapsnecessary to convert one ranking into the other.
Thisvalue is normalized by the number of items beingranked such that two identical rankings produce acorrelation of 1.0; the correlation between a rank-ing and its perfect inverse is ?1.0; and the expectedcorrelation of two rankings chosen at random is0.0.
Typically, a value of greater than 0.8 is con-sidered ?good?, although 0.9 represents a thresholdresearchers generally aim for.We hypothesized that system ranks are relativelyunstable with respect to individual assessor?s judg-ments.
That is, how well a given system scoresis to a large extent dependent on which assessor?sjudgments one uses for evaluation.
This stems froman inescapable fact of such evaluations, well knownfrom studies of relevance in the information retrievalliterature (Voorhees, 1998).
Humans have legitimatedifferences in opinion regarding a nugget?s impor-tance, and there is no such thing as ?the correct an-swer?.
However, we hypothesized that these varia-tions can be smoothed out by building ?nugget pyra-mids?
in the manner we described.
Nugget weightsreflect the combined judgments of many individualassessors, and scores generated with weights takeninto account should correlate better with each indi-vidual assessor?s opinion.6 ResultsTo verify our hypothesis about the instability of us-ing any individual assessor?s judgments, we calcu-lated the Kendall?s ?
correlation between systemscores generated using the ?official?
vital/okay judg-ments (provide by NIST assessors) and each individ-ual assessor?s judgments.
This is shown in Table 3.The original NIST judgments are listed as ?assessor0?
(and not included in the averages).
For all scoringmodels discussed in this paper, we set ?, the param-eter that controls the relative importance of preci-sion and recall, to three.3 Results show that althoughofficial rankings generally correlate well with rank-ings generated by our nine additional assessors, theagreement is far from perfect.
Yet, in reality, theopinions of our nine assessors are not any less validthan those of the NIST assessors?NIST does notoccupy a privileged position on what constitutes agood ?definition?.
We can see that variations in hu-man judgments do not appear to be adequately cap-tured by the current scoring model.Table 3 also shows the number of questions forwhich systems?
median score was zero based oneach individual assessor?s judgments (out of 503Note that ?
= 5 in the official TREC 2003 evaluation.3872003 2004 20050 0.934 0.943 0.9011 0.962 0.940 0.9502 0.938 0.948 0.9523 0.938 0.947 0.9504 0.936 0.922 0.9145 0.916 0.956 0.8876 0.916 0.950 0.9587 0.949 0.933 0.9278 0.964 0.972 0.9539 0.912 0.899 0.881average 0.936 0.941 0.927Table 4: Kendall?s ?
correlation between systemrankings generated using the ten-assessor nuggetpyramid and those generated using each individualassessor?s judgments.
(Assessor 0 represents theoriginal NIST assessors.
)questions for TREC 2003, 64 for TREC 2004, and75 for TREC 2005).
These numbers are worrisome:in TREC 2004, for example, over half the questions(on average) have a median score of zero, and overthree quarters of questions, according to assessor 9.This is problematic for the various reasons discussedin Section 3.To evaluate scoring models that combine the opin-ions of multiple assessors, we built ?nugget pyra-mids?
using all ten sets of judgments in the manneroutlined in Section 4.
All runs submitted to eachof the TREC evaluations were then rescored usingthe modified F-score formula, which takes into ac-count a finer-grained notion of nugget importance.Rankings generated by this model were then com-pared against those generated by each individual as-sessor?s judgments.
Results are shown in Table 4.As can be seen, the correlations observed are higherthan those in Table 3, meaning that a nugget pyramidbetter captures the opinions of each individual asses-sor.
A two-tailed t-test reveals that the differences inaverages are statistically significant (p << 0.01 forTREC 2003/2005, p < 0.05 for TREC 2004).What is the effect of combining judgments fromdifferent numbers of assessors?
To answer thisquestion, we built ten different nugget pyramidsof varying ?sizes?, i.e., combining judgments fromone through ten assessors.
The Kendall?s ?
corre-0.860.880.90.920.940.960.9811  2  3  4  5  6  7  8  9  10Kendall'stauNumber of assessorsTREC 2003TREC 2004TREC 2005Figure 2: Average agreement (Kendall?s ? )
betweenindividual assessors and nugget pyramids built fromdifferent numbers of assessors.0.30.350.40.450.50.550.60.650.71  2  3  4  5  6  7  8  9  10FractionofquestionswhosemedianscoreiszeroNumber of assessorsTREC 2003TREC 2004TREC 2005Figure 3: Fraction of questions whose median scoreis zero plotted against number of assessors whosejudgments contributed to the nugget pyramid.lations between scores generated by each of theseand scores generated by each individual assessor?sjudgments were computed.
For each pyramid, wecomputed the average across all rank correlations,which captures the extent to which that particularpyramid represents the opinions of all ten assessors.These results are shown in Figure 2.
The increasein Kendall?s ?
that comes from adding a second as-sessor is statistically significant, as revealed by atwo-tailed t-test (p << 0.01 for TREC 2003/2005,p < 0.05 for TREC 2004), but ANOVA reveals nostatistically significant differences beyond two as-sessors.From these results, we can conclude that addinga second assessor yields a scoring model that is sig-nificantly better at capturing the variance in humanrelevance judgments.
In this respect, little is gainedbeyond two assessors.
If this is the only advantage388provided by nugget pyramids, then the boost in rankcorrelations may not be sufficient to justify the ex-tra manual effort involved in building them.
As weshall see, however, nugget pyramids offer other ben-efits as well.Evaluation by our nugget pyramids greatly re-duces the number of questions whose median scoreis zero.
As previously discussed, a strict vital/okaysplit translates into a score of zero for systems thatdo not return any vital nuggets.
However, nuggetpyramids reflect a more refined sense of nugget im-portance, which results in fewer zero scores.
Fig-ure 3 shows the number of questions whose medianscore is zero (normalized as a fraction of the en-tire testset) by nugget pyramids built from varyingnumbers of assessors.
With four or more assessors,the number of questions whose median is zero forthe TREC 2003 testset drops to 17; for TREC 2004,23 for seven or more assessors; for TREC 2005, 27for nine or more assessors.
In other words, F-scoresgenerated using our methodology are far more dis-criminative.
The remaining questions with zero me-dians, we believe, accurately reflect the state of theart in question answering performance.An example of a nugget pyramid that combinesthe opinions of all ten assessors is shown in Table 5for the target ?AARP?.
Judgments from the originalNIST assessors are also shown (cf.
Table 1).
Notethat there is a strong correlation between the originalvital/okay judgments and the refined nugget weightsbased on the pyramid, indicating that (in this case,at least) the intuition of the NIST assessor matchesthat of the other assessors.7 DiscussionIn balancing the tradeoff between advantages pro-vided by nugget pyramids and the additional man-ual effort necessary to create them, what is the opti-mal number of assessors to solicit judgments from?Results shown in Figures 2 and 3 provide some an-swers.
In terms of better capturing different asses-sors?
opinions, little appears to be gained from goingbeyond two assessors.
However, adding more judg-ments does decrease the number of questions whosemedian score is zero, resulting in a more discrim-inative metric.
Beyond five assessors, the numberof questions with a zero median score remains rela-1.0 vital Largest seniors organization0.9 vital Membership eligibility is 50+0.8 vital 30+ million members0.7 vital Largest dues paying organization0.2 okay Most of its work done by volunteers0.1 okay Spends heavily on research & education0.1 okay Receives millions for product endorsements0.1 okay Receives millions from product endorsements0.0 okay Abbreviated name to attract boomersTable 5: Answer nuggets for the target ?AARP?
withweights derived from the nugget pyramid buildingprocess.tively stable.
We believe that around five assessorsyield the smallest nugget pyramid that confers theadvantages of the methodology.The idea of building ?nugget pyramids?
is an ex-tension of a similarly-named evaluation scheme indocument summarization, although there are impor-tant differences.
Nenkova and Passonneau (2004)call for multiple assessors to annotate SCUs, whichis much more involved than the methodology pre-sented here, where the nuggets are fixed and asses-sors only provide additional judgments about theirimportance.
This obviously has the advantage ofstreamlining the assessment process, but has the po-tential to miss other important nuggets that were notidentified in the first place.
Our experimental results,however, suggest that this is a worthwhile tradeoff.The explicit goal of this work was to develop scor-ing models for nugget-based evaluation that wouldaddress shortcomings of the present approach, whileintroducing minimal overhead in terms of additionalresource requirements.
To this end, we have beensuccessful.Nevertheless, there are a number of issues thatare worth mentioning.
To speed up the assessmentprocess, assessors were instructed to provide ?snapjudgments?
given only the list of nuggets and the tar-get.
No additional context was provided, e.g., docu-ments from the corpus or sample system responses.It is also important to note that the reference nuggetswere never meant to be read by other people?NISTmakes no claim for them to be well-formed de-scriptions of the facts themselves.
These answer389keys were primarily note-taking devices to assist inthe assessment process.
The important question,however, is whether scoring variations caused bypoorly-phrased nuggets are smaller than the varia-tions caused by legitimate inter-assessor disagree-ment regarding nugget importance.
Our experimentsappear to suggest that, overall, the nugget pyramidscheme is sound and can adequately cope with thesedifficulties.8 ConclusionThe central importance that quantitative evaluationplays in advancing the state of the art in languagetechnologies warrants close examination of evalua-tion methodologies themselves to ensure that theyare measuring ?the right thing?.
In this work, wehave identified a shortcoming in the present nugget-based paradigm for assessing answers to complexquestions.
The vital/okay distinction was designedto capture the intuition that some nuggets are moreimportant than others, but as we have shown, thiscomes at a cost in stability and discriminative powerof the metric.
We proposed a revised model that in-corporates judgments from multiple assessors in theform of a ?nugget pyramid?, and demonstrated howthis addresses many of the previous shortcomings.
Itis hoped that our work paves the way for more ac-curate and refined evaluations of question answeringsystems in the future.9 AcknowledgmentsThis work has been supported in part by DARPAcontract HR0011-06-2-0001 (GALE), and hasgreatly benefited from discussions with EllenVoorhees, Hoa Dang, and participants at TREC2005.
We are grateful for the nine assessors whoprovided nugget judgments.
The first author wouldlike to thank Esther and Kiri for their loving support.ReferencesEnrique Amigo?, Julio Gonzalo, Victor Peinado, AnselmoPen?as, and Felisa Verdejo.
2004.
An empirical studyof information synthesis task.
In Proceedings of the42nd Annual Meeting of the Association for Computa-tional Linguistics (ACL 2004).Hoa Dang.
2005.
Overview of DUC 2005.
In Proceed-ings of the 2005 Document Understanding Conference(DUC 2005) at NLT/EMNLP 2005.Wesley Hildebrandt, Boris Katz, and Jimmy Lin.
2004.Answering definition questions with multiple knowl-edge sources.
In Proceedings of the 2004 Human Lan-guage Technology Conference and the North AmericanChapter of the Association for Computational Linguis-tics Annual Meeting (HLT/NAACL 2004).Jimmy Lin and Dina Demner-Fushman.
2005a.
Auto-matically evaluating answers to definition questions.In Proceedings of the 2005 Human Language Technol-ogy Conference and Conference on Empirical Methodsin Natural Language Processing (HLT/EMNLP 2005).Jimmy Lin and Dina Demner-Fushman.
2005b.
Evalu-ating summaries and answers: Two sides of the samecoin?
In Proceedings of the ACL 2005 Workshop onIntrinsic and Extrinsic Evaluation Measures for MTand/or Summarization.Chin-Yew Lin and Eduard Hovy.
2003.
Automaticevaluation of summaries using n-gram co-occurrencestatistics.
In Proceedings of the 2003 Human Lan-guage Technology Conference and the North AmericanChapter of the Association for Computational Linguis-tics Annual Meeting (HLT/NAACL 2003).Ani Nenkova and Rebecca Passonneau.
2004.
Evalu-ating content selection in summarization: The pyra-mid method.
In Proceedings of the 2004 Human Lan-guage Technology Conference and the North AmericanChapter of the Association for Computational Linguis-tics Annual Meeting (HLT/NAACL 2004).Ellen M. Voorhees.
1998.
Variations in relevance judg-ments and the measurement of retrieval effectiveness.In Proceedings of the 21st Annual International ACMSIGIR Conference on Research and Development inInformation Retrieval (SIGIR 1998).Ellen M. Voorhees.
2003.
Overview of the TREC2003 question answering track.
In Proceedings of theTwelfth Text REtrieval Conference (TREC 2003).Ellen M. Voorhees.
2005.
Using question series to eval-uate question answering system effectiveness.
In Pro-ceedings of the 2005 Human Language TechnologyConference and Conference on Empirical Methods inNatural Language Processing (HLT/EMNLP 2005).390
