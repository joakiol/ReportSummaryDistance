Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 272?279,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsThe Infinite TreeJenny Rose Finkel, Trond Grenager, and Christopher D. ManningComputer Science Department, Stanford UniversityStanford, CA 94305{jrfinkel, grenager, manning}@cs.stanford.eduAbstractHistorically, unsupervised learning tech-niques have lacked a principled techniquefor selecting the number of unseen compo-nents.
Research into non-parametric priors,such as the Dirichlet process, has enabled in-stead the use of infinite models, in which thenumber of hidden categories is not fixed, butcan grow with the amount of training data.Here we develop the infinite tree, a new infi-nite model capable of representing recursivebranching structure over an arbitrarily largeset of hidden categories.
Specifically, wedevelop three infinite tree models, each ofwhich enforces different independence as-sumptions, and for each model we define asimple direct assignment sampling inferenceprocedure.
We demonstrate the utility ofour models by doing unsupervised learningof part-of-speech tags from treebank depen-dency skeleton structure, achieving an accu-racy of 75.34%, and by doing unsupervisedsplitting of part-of-speech tags, which in-creases the accuracy of a generative depen-dency parser from 85.11% to 87.35%.1 IntroductionModel-based unsupervised learning techniques havehistorically lacked good methods for choosing thenumber of unseen components.
For example, k-means or EM clustering require advance specifica-tion of the number of mixture components.
Butthe introduction of nonparametric priors such as theDirichlet process (Ferguson, 1973) enabled develop-ment of infinite mixture models, in which the num-ber of hidden components is not fixed, but emergesnaturally from the training data (Antoniak, 1974).Teh et al (2006) proposed the hierarchical Dirich-let process (HDP) as a way of applying the Dirichletprocess (DP) to more complex model forms, so as toallow multiple, group-specific, infinite mixture mod-els to share their mixture components.
The closelyrelated infinite hidden Markov model is an HMMin which the transitions are modeled using an HDP,enabling unsupervised learning of sequence modelswhen the number of hidden states is unknown (Bealet al, 2002; Teh et al, 2006).We extend this work by introducing the infinitetree model, which represents recursive branchingstructure over a potentially infinite set of hiddenstates.
Such models are appropriate for the syntacticdependency structure of natural language.
The hid-den states represent word categories (?tags?
), the ob-servations they generate represent the words them-selves, and the tree structure represents syntactic de-pendencies between pairs of tags.To validate the model, we test unsupervised learn-ing of tags conditioned on a given dependency treestructure.
This is useful, because coarse-grainedsyntactic categories, such as those used in the PennTreebank (PTB), make insufficient distinctions to bethe basis of accurate syntactic parsing (Charniak,1996).
Hence, state-of-the-art parsers either supple-ment the part-of-speech (POS) tags with the lexicalforms themselves (Collins, 2003; Charniak, 2000),manually split the tagset into a finer-grained one(Klein and Manning, 2003a), or learn finer grainedtag distinctions using a heuristic learning procedure(Petrov et al, 2006).
We demonstrate that the tagslearned with our model are correlated with the PTBPOS tags, and furthermore that they improve the ac-curacy of an automatic parser when used in training.2 Finite TreesWe begin by presenting three finite tree models, eachwith different independence assumptions.272C?
pikH ?kz1z2 z3x1 x2 x3Figure 1: A graphical representation of the finiteBayesian tree model with independent children.
Theplate (rectangle) indicates that there is one copy ofthe model parameter variables for each state k ?
C .2.1 Independent ChildrenIn the first model, children are generated indepen-dently of each other, conditioned on the parent.
Lett denote both the tree and its root node, c(t) the listof children of t, ci(t) the ith child of t, and p(t) theparent of t. Each tree t has a hidden state zt (in a syn-tax tree, the tag) and an observation xt (the word).1The probability of a tree is given by the recursivedefinition:2Ptr(t) = P(xt|zt)?t??c(t)P(zt?
|zt)Ptr(t?
)To make the model Bayesian, we must define ran-dom variables to represent each of the model?s pa-rameters, and specify prior distributions for them.Let each of the hidden state variables have C possi-ble values which we will index with k. Each state khas a distinct distribution over observations, param-eterized by ?k, which is distributed according to aprior distribution over the parameters H:?k|H ?
HWe generate each observation xt from some distri-bution F (?zt) parameterized by ?zt specific to itscorresponding hidden state zt.
If F (?k)s are multi-nomials, then a natural choice for H would be aDirichlet distribution.3The hidden state zt?
of each child is distributedaccording to a multinomial distribution pizt specificto the hidden state zt of the parent:xt|zt ?
F (?zt)zt?
|zt ?
Multinomial(pizt)1To model length, every child list ends with a distinguishedstop node, which has as its state a distinguished stop state.2We also define a distinguished node t0, which generates theroot of the entire tree, and P (xt0 |zt0) = 1.3A Dirichlet distribution is a distribution over the possibleparameters of a multinomial distributions, and is distinct fromthe Dirichlet process.Each multinomial over children pik is distributed ac-cording to a Dirichlet distribution with parameter ?:pik|?
?
Dirichlet(?, .
.
.
, ?
)This model is presented graphically in Figure 1.2.2 Simultaneous ChildrenThe independent child model adopts strong indepen-dence assumptions, and we may instead want mod-els in which the children are conditioned on morethan just the parent?s state.
Our second model thusgenerates the states of all of the children c(t) simul-taneously:Ptr(t) = P(xt|zt)P((zt?)t??c(t)|zt)?t??c(t)Ptr(t?
)where (zt?)t?
?c(t) indicates the list of tags of the chil-dren of t. To parameterize this model, we replace themultinomial distribution pik over states with a multi-nomial distribution ?k over lists of states.42.3 Markov ChildrenThe very large domain size of the child lists in thesimultaneous child model may cause problems ofsparse estimation.
Another alternative is to use afirst-order Markov process to generate children, inwhich each child?s state is conditioned on the previ-ous child?s state:Ptr(t) = P(xt|zt)?|c(t)|i=1P(zci(t)|zci?1(t), zt)Ptr(t?
)For this model, we augment all child lists with a dis-tinguished start node, c0(t), which has as its statea distinguished start state, allowing us to capturethe unique behavior of the first (observed) child.
Toparameterize this model, note that we will need todefine C(C + 1) multinomials, one for each parentstate and preceding child state (or a distinguishedstart state).3 To Infinity, and Beyond .
.
.This section reviews needed background materialfor our approach to making our tree models infinite.3.1 The Dirichlet ProcessSuppose we model a document as a bag of wordsproduced by a mixture model, where the mixturecomponents might be topics such as business, pol-itics, sports, etc.
Using this model we can generate a4This requires stipulating a maximum list length.27300.20.40.60.8100.20.40.60.81P(xi = "game")P(xi = "profit")Figure 2: Plot of the density function of a Dirich-let distribution H (the surface) as well as a drawG (the vertical lines, or sticks) from a Dirichletprocess DP(?0,H) which has H as a base mea-sure.
Both distributions are defined over a sim-plex in which each point corresponds to a particularmultinomial distribution over three possible words:?profit?, ?game?, and ?election?.
The placement ofthe sticks is drawn from the distribution H , and isindependent of their lengths, which is drawn from astick-breaking process with parameter ?0.document by first generating a distribution over top-ics pi, and then for each position i in the document,generating a topic zi from pi, and then a word xifrom the topic specific distribution ?zi .
The worddistributions ?k for each topic k are drawn from abase distribution H .
In Section 2, we sample Cmultinomials ?k from H .
In the infinite mixturemodel we sample an infinite number of multinomi-als from H , using the Dirichlet process.Formally, given a base distribution H and a con-centration parameter ?0 (loosely speaking, this con-trols the relative sizes of the topics), a Dirichlet pro-cess DP(?0,H) is the distribution of a discrete ran-dom probability measure G over the same (possiblycontinuous) space that H is defined over; thus it is ameasure over measures.
In Figure 2, the sticks (ver-tical lines) show a draw G from a Dirichlet processwhere the base measure H is a Dirichlet distributionover 3 words.
A draw comprises of an infinite num-ber of sticks, and each corresponding topic.We factor G into two coindexed distributions: pi,a distribution over the integers, where the integerrepresents the index of a particular topic (i.e., theheight of the sticks in the figure represent the proba-bility of the topic indexed by that stick) and ?, rep-resenting the word distribution of each of the top-N?
?0 Hpi ?kzixipi|?0 ?
GEM(?0)?k|H ?
Hzi|pi ?
pixi|zi,?
?
F (?zi) N??
?0?
Hpij ?kzjixji(a) (b)Figure 3: A graphical representation of a simpleDirichlet process mixture model (left) and a hierar-chical Dirichlet process model (right).
Note that weshow the stick-breaking representations of the mod-els, in which we have factored G ?
DP(?0,H) intotwo sets of variables: pi and ?.ics (i.e., the location of the sticks in the figure).
Togenerate pi we first generate an infinite sequence ofvariables pi?
= (pi?k)?k=1, each of which is distributedaccording to the Beta distribution:pi?k|?0 ?
Beta(1, ?0)Then pi = (pik)?k=1 is defined as:pik = pi?k?k?1i=1(1?
pi?i)Following Pitman (2002) we refer to this process aspi ?
GEM(?0).
It should be noted that?
?k=1 pik =1,5 and P (i) = pii.
Then, according to the DP,P (?i) = pii.
The complete model, is shown graphi-cally in Figure 3(a).To build intuition, we walk through the process ofgenerating from the infinite mixture model for thedocument example, where xi is the word at posi-tion i, and zi is its topic.
F is a multinomial dis-tribution parameterized by ?, and H is a Dirichletdistribution.
Instead of generating all of the infinitemixture components (pik)?k=1 at once, we can buildthem up incrementally.
If there are K known top-ics, we represent only the known elements (pik)Kk=1and represent the remaining probability mass piu =5This is called the stick-breaking construction: we start witha stick of unit length, representing the entire probability mass,and successively break bits off the end of the stick, where theproportional amount broken off is represented by pi?k and theabsolute amount is represented by pik.274?1 ?2 ?3 ?4 ?5 ?6 ?7 .
.
.?
:pij :.
.
.Figure 4: A graphical representation of pij , a brokenstick, which is distributed according to a DP with abroken stick ?
as a base measure.
Each ?k corre-sponds to a ?k.1 ?
(?Kk=1 pik).
Initially we have piu = 1 and?
= ().For the ith position in the document, we first drawa topic zi ?
pi.
If zi 6= u, then we find the coin-dexed topic ?zi .
If zi = u, the unseen topic, wemake a draw b ?
Beta(1, ?0) and set piK+1 = bpiuand pinewu = (1 ?
b)piu.
Then we draw a parame-ter ?K+1 ?
H for the new topic, resulting in pi =(pi1, .
.
.
, piK+1, pinewu ) and ?
= (?1, .
.
.
, ?K+1).
Aword is then drawn from this topic and emitted bythe document.3.2 The Hierarchical Dirichlet ProcessLet?s generalize our previous example to a corpusof documents.
As before, we have a set of sharedtopics, but now each document has its own charac-teristic distribution over these topics.
We representtopic distributions both locally (for each document)and globally (across all documents) by use of a hier-archical Dirichlet process (HDP), which has a localDP for each document, in which the base measure isitself a draw from another, global, DP.The complete HDP model is represented graphi-cally in Figure 3(b).
Like the DP, it has global bro-ken stick ?
= (?k)?k=1 and topic specific word dis-tribution parameters ?
= (?k)?k=1, which are coin-dexed.
It differs from the DP in that it also has lo-cal broken sticks pij for each group j (in our casedocuments).
While the global stick ?
?
GEM(?
)is generated as before, the local sticks pij are dis-tributed according to a DP with base measure ?
:pij ?
DP(?0,?
).We illustrate this generation process in Figure 4.The upper unit line represents ?, where the size ofsegment k represents the value of element ?k, andthe lower unit line represents pij ?
DP(?0,?)
for aparticular group j.
Each element of the lower stickwas sampled from a particular element of the upperstick, and elements of the upper stick may be sam-pled multiple times or not at all; on average, largerelements will be sampled more often.
Each element?k, as well as all elements of pij that were sampledfrom it, corresponds to a particular ?k.
Critically,several distinct pij can be sampled from the same?k and hence share ?k; this is how components areshared among groups.For concreteness, we show how to generate a cor-pus of documents from the HDP, generating onedocument at a time, and incrementally construct-ing our infinite objects.
Initially we have ?u = 1,?
= (), and piju = 1 for all j.
We start with thefirst position of the first document and draw a localtopic y11 ?
pi1, which will return u with probabil-ity 1.
Because y11 = u we must make a draw fromthe base measure, ?, which, because this is the firstdocument, will also return u with probability 1.
Wemust now break ?u into ?1 and ?newu , and break pi1uinto pi11 and pinew1u in the same manner presented forthe DP.
Since pi11 now corresponds to global topic1, we sample the word x11 ?
Multinomial(?1).
Tosample each subsequent word i, we first sample thelocal topic y1i ?
pi1.
If y1i 6= u, and pi1y1i corre-sponds to ?k in the global stick, then we sample theword x1i ?
Multinomial(?k).
Once the first docu-ment has been sampled, subsequent documents aresampled in a similar manner; initially piju = 1 fordocument j, while ?
continues to grow as more doc-uments are sampled.4 Infinite TreesWe now use the techniques from Section 3 to createinfinite versions of each tree model from Section 2.4.1 Independent ChildrenThe changes required to make the Bayesian inde-pendent children model infinite don?t affect its ba-sic structure, as can be witnessed by comparing thegraphical depiction of the infinite model in Figure 5with that of the finite model in Figure 1.
The in-stance variables zt and xt are parameterized as be-fore.
The primary change is that the number ofcopies of the state plate is infinite, as are the numberof variables pik and ?k.Note also that each distribution over possiblechild states pik must also be infinite, since the num-ber of possible child states is potentially infinite.
Weachieve this by representing each of the pik variablesas a broken stick, and adopt the same approach of275?|?
?
GEM(?)pik|?0,?
?
DP(?0,?
)?k|H ?
H??
?
?0 pikH ?kz1z2 z3x1 x2 x3Figure 5: A graphical representation of the infiniteindependent child model.sampling each pik from a DP with base measure ?.For the dependency tree application, ?k is a vectorrepresenting the parameters of a multinomial overwords, and H is a Dirichlet distribution.The infinite hidden Markov model (iHMM) orHDP-HMM (Beal et al, 2002; Teh et al, 2006) isa model of sequence data with transitions modeledby an HDP.6 The iHMM can be viewed as a specialcase of this model, where each state (except the stopstate) produces exactly one child.4.2 Simultaneous ChildrenThe key problem in the definition of the simulta-neous children model is that of defining a distribu-tion over the lists of children produced by each state,since each child in the list has as its domain the posi-tive integers, representing the infinite set of possiblestates.
Our solution is to construct a distribution Lkover lists of states from the distribution over individ-ual states pik.
The obvious approach is to sample thestates at each position i.i.d.:P((zt?)t?
?c(t)|pi) =?t??c(t)P(zt?
|pi) =?t?
?c(t)pizt?However, we want our model to be able to rep-resent the fact that some child lists, ct, are moreor less probable than the product of the individualchild probabilities would indicate.
To address this,we can sample a state-conditional distribution overchild lists ?k from a DP with Lk as a base measure.6The original iHMM paper (Beal et al, 2002) predates, andwas the motivation for, the work presented in Teh et al (2006),and is the origin of the term hierarchical Dirichlet process.However, they used the term to mean something slightly differ-ent than the HDP presented in Teh et al (2006), and presented asampling scheme for inference that was a heuristic approxima-tion of a Gibbs sampler.Thus, we augment the basic model given in the pre-vious section with the variables ?
, Lk, and ?k:Lk|pik ?
Deterministic, as described above?k|?, Lk ?
DP(?, Lk)ct|?k ?
?kAn important consequence of defining Lk locally(instead of globally, using ?
instead of the piks) isthat the model captures not only what sequences ofchildren a state prefers, but also the individual chil-dren that state prefers; if a state gives high proba-bility to some particular sequence of children, thenit is likely to also give high probability to other se-quences containing those same states, or a subsetthereof.4.3 Markov ChildrenIn the Markov children model, more copies of thevariable pi are needed, because each child state mustbe conditioned both on the parent state and on thestate of the preceding child.
We use a new set ofvariables piki, where pi is determined by the par-ent state k and the state of the preceding sibling i.Each of the piki is distributed as pik was in the basicmodel: piki ?
DP(?0,?
).5 InferenceOur goal in inference is to draw a sample from theposterior over assignments of states to observations.We present an inference procedure for the infinitetree that is based on Gibbs sampling in the directassignment representation, so named because we di-rectly assign global state indices to observations.7Before we present the procedure, we define a fewcount variables.
Recall from Figure 4 that each statek has a local stick pik, each element of which cor-responds to an element of ?.
In our sampling pro-cedure, we only keep elements of pik and ?
whichcorrespond to states observed in the data.
We definethe variable mjk to be the number of elements of thefinite observed portion of pik which correspond to ?jand njk to be the number of observations with statek whose parent?s state is j.We also need a few model-specific counts.
For thesimultaneous children model we need njz, which is7We adapt one of the sampling schemes mentioned by Tehet al (2006) for use in the iHMM.
This paper suggests twosampling schemes for inference, but does not explicitly presentthem.
Upon discussion with one of the authors (Y. W. Teh,2006, p.c.
), it became clear that inference using the augmentedrepresentation is much more complicated than initially thought.276the number of times the state sequence z occurredas the children of state j.
For the Markov chil-dren model we need the count variable n?jik whichis the number of observations for a node with statek whose parent?s state is j and whose previous sib-ling?s state is i.
In all cases we represent marginalcounts using dot-notation, e.g., n?k is the total num-ber of nodes with state k, regardless of parent.Our procedure alternates between three distinctsampling stages: (1) sampling the state assignmentsz, (2) sampling the counts mjk, and (3) samplingthe global stick ?.
The only modification of the pro-cedure that is required for the different tree mod-els is the method for computing the probabilityof the child state sequence given the parent stateP((zt?)t?
?c(t)|zt), defined separately for each model.Sampling z.
In this stage we sample a state foreach tree node.
The probability of node t being as-signed state k is given by:P(zt = k|z?t,?)
?
P(zt = k, (zt?)t??s(t)|zp(t))?
P((zt?)t?
?c(t)|zt = k) ?
f?xtk (xt)where s(t) denotes the set of siblings of t, f?xtk (xt)denotes the posterior probability of observation xtgiven all other observations assigned to state k, andz?t denotes all state assignments except zt.
In otherwords, the probability is proportional to the productof three terms: the probability of the states of t andits siblings given its parent zp(t), the probability ofthe states of the children c(t) given zt, and the pos-terior probability of observation xt given zt.
Notethat if we sample zt to be a previously unseen state,we will need to extend ?
as discussed in Section 3.2.Now we give the equations for P((zt?)t?
?c(t)|zt)for each of the models.
In the independent childmodel the probability of generating each child is:Pind(zci(t) = k|zt = j) =njk + ?0?knj?
+ ?0Pind((zt?)t?
?c(t)|zt = j) =?t??c(t)Pind(zt?
|zt = j)For the simultaneous child model, the probability ofgenerating a sequence of children, z, takes into ac-count how many times that sequence has been gen-erated, along with the likelihood of regenerating it:Psim((zt?)t?
?c(t) = z|zt = j) =njz + ?Pind(z|zt = j)nj?
+ ?Recall that ?
denotes the concentration parameterfor the sequence generating DP.
Lastly, we have theDT NN IN DT NN VBD PRP$ NN TO VB NN EOSThe man in the corner taught his dachshund to play golf EOSFigure 6: An example of a syntactic dependency treewhere the dependencies are between tags (hiddenstates), and each tag generates a word (observation).Markov child model:Pm(zci(t) = k|zci?1(t) = i, zt = j) =n?jik + ?0?kn?ji?
+ ?0Pm((zt?)t?
?c(t)|zt) =?|c(t)|i=1Pm(zci(t)|zci?1(t), zt)Finally, we give the posterior probability of an ob-servation, given that F (?k) is Multinomial(?k), andthat H is Dirichlet(?, .
.
.
, ?).
Let N be the vocab-ulary size and n?k be the number of observations xwith state k. Then:f?xtk (xt) =n?xtk + ?n?
?k + N?Sampling m. We use the following procedure,which slightly modifies one from (Y. W. Teh, 2006,p.c.
), to sample each mjk:SAMPLEM(j, k)1 if njk = 02 then mjk = 03 else mjk = 14 for i?
2 to njk5 do if rand() < ?0?0+i?16 then mjk = mjk + 17 return mjkSampling ?.
Lastly, we sample ?
using the Di-richlet distribution:(?1, .
.
.
, ?K , ?u) ?
Dirichlet(m?1, .
.
.
,m?K , ?0)6 ExperimentsWe demonstrate infinite tree models on two dis-tinct syntax learning tasks: unsupervised POS learn-ing conditioned on untagged dependency trees andlearning a split of an existing tagset, which improvesthe accuracy of an automatic syntactic parser.For both tasks, we use a simple modification ofthe basic model structure, to allow the trees to gen-erate dependents on the left and the right with dif-ferent distributions ?
as is useful in modeling natu-ral language.
The modification of the independentchild tree is trivial: we have two copies of each of277the variables pik, one each for the left and the right.Generation of dependents on the right is completelyindependent of that for the left.
The modifications ofthe other models are similar, but now there are sepa-rate sets of pik variables for the Markov child model,and separate Lk and ?k variables for the simultane-ous child model, for each of the left and right.For both experiments, we used dependency treesextracted from the Penn Treebank (Marcus et al,1993) using the head rules and dependency extrac-tor from Yamada and Matsumoto (2003).
As is stan-dard, we used WSJ sections 2?21 for training, sec-tion 22 for development, and section 23 for testing.6.1 Unsupervised POS LearningIn the first experiment, we do unsupervised part-of-speech learning conditioned on dependency trees.To be clear, the input to our algorithm is the de-pendency structure skeleton of the corpus, but notthe POS tags, and the output is a labeling of eachof the words in the tree for word class.
Since themodel knows nothing about the POS annotation, thenew classes have arbitrary integer names, and arenot guaranteed to correlate with the POS tag def-initions.
We found that the choice of ?0 and ?
(the concentration parameters) did not affect the out-put much, while the value of ?
(the parameter forthe base Dirichlet distribution) made a much largerdifference.
For all reported experiments, we set?0 = ?
= 10 and varied ?.We use several metrics to evaluate the wordclasses.
First, we use the standard approach ofgreedily assigning each of the learned classes to thePOS tag with which it has the greatest overlap, andthen computing tagging accuracy (Smith and Eisner,2005; Haghighi and Klein, 2006).8 Additionally, wecompute the mutual information of the learned clus-ters with the gold tags, and we compute the clusterF-score (Ghosh, 2003).
See Table 1 for results ofthe different models, parameter settings, and met-rics.
Given the variance in the number of classeslearned it is a little difficult to interpret these results,but it is clear that the Markov child model is thebest; it achieves superior performance to the inde-pendent child model on all metrics, while learningfewer word classes.
The poor performance of thesimultaneous model warrants further investigation,but we observed that the distributions learned by that8The advantage of this metric is that it?s comprehensible.The disadvantage is that it?s easy to inflate by adding classes.Model ?
# Classes Acc.
MI F1Indep.
0.01 943 67.89 2.00 48.290.001 1744 73.61 2.23 40.800.0001 2437 74.64 2.27 39.47Simul.
0.01 183 21.36 0.31 21.570.001 430 15.77 0.09 13.800.0001 549 16.68 0.12 14.29Markov 0.01 613 68.53 2.12 49.820.001 894 75.34 2.31 48.73Table 1: Results of part unsupervised POS taggingon the different models, using a greedy accuracymeasure.model are far more spiked, potentially due to doublecounting of tags, since the sequence probabilities arealready based on the local probabilities.For comparison, Haghighi and Klein (2006) re-port an unsupervised baseline of 41.3%, and a bestresult of 80.5% from using hand-labeled prototypesand distributional similarity.
However, they train onless data, and learn fewer word classes.6.2 Unsupervised POS SplittingIn the second experiment we use the infinite treemodels to learn a refinement of the PTB tags.
Weinitialize the set of hidden states to the set of PTBtags, and then, during inference, constrain the sam-pling distribution over hidden state zt at each node tto include only states that are a refinement of the an-notated PTB tag at that position.
The output of thistraining procedure is a new annotation of the wordsin the PTB with the learned tags.
We then comparethe performance of a generative dependency parsertrained on the new refined tags with one trained onthe base PTB tag set.
We use the generative de-pendency parser distributed with the Stanford fac-tored parser (Klein and Manning, 2003b) for thecomparison, since it performs simultaneous taggingand parsing during testing.
In this experiment, un-labeled, directed, dependency parsing accuracy forthe best model increased from 85.11% to 87.35%, a15% error reduction.
See Table 2 for the full resultsover all models and parameter settings.7 Related WorkThe HDP-PCFG (Liang et al, 2007), developed atthe same time as this work, aims to learn state splitsfor a binary-branching PCFG.
It is similar to oursimultaneous child model, but with several impor-tant distinctions.
As discussed in Section 4.2, in ourmodel each state has a DP over sequences, with abase distribution that is defined over the local child278Model ?
AccuracyBaseline ?
85.11Independent 0.01 86.180.001 85.88Markov 0.01 87.150.001 87.35Table 2: Results of untyped, directed dependencyparsing, where the POS tags in the training data havebeen split according to the various models.
At testtime, the POS tagging and parsing are done simulta-neously by the parser.state probabilities.
In contrast, Liang et al (2007)define a global DP over sequences, with the basemeasure defined over the global state probabilities,?
; locally, each state has an HDP, with this globalDP as the base measure.
We believe our choice tobe more linguistically sensible: in our model, for aparticular state, dependent sequences which are sim-ilar to one another increase one another?s likelihood.Additionally, their modeling decision made it diffi-cult to define a Gibbs sampler, and instead they usevariational inference.
Earlier, Johnson et al (2007)presented adaptor grammars, which is a very simi-lar model to the HDP-PCFG.
However they did notconfine themselves to a binary branching structureand presented a more general framework for defin-ing the process for splitting the states.8 Discussion and Future WorkWe have presented a set of novel infinite tree modelsand associated inference algorithms, which are suit-able for representing syntactic dependency structure.Because the models represent a potentially infinitenumber of hidden states, they permit unsupervisedlearning algorithms which naturally select a num-ber of word classes, or tags, based on qualities ofthe data.
Although they require substantial techni-cal background to develop, the learning algorithmsbased on the models are actually simple in form, re-quiring only the maintenance of counts, and the con-struction of sampling distributions based on thesecounts.
Our experimental results are preliminary butpromising: they demonstrate that the model is capa-ble of capturing important syntactic structure.Much remains to be done in applying infinitemodels to language structure, and an interesting ex-tension would be to develop inference algorithmsthat permit completely unsupervised learning of de-pendency structure.AcknowledgmentsMany thanks to Yeh Whye Teh for several enlight-ening conversations, and to the following mem-bers (and honorary member) of the Stanford NLPgroup for comments on an earlier draft: ThadHughes, David Hall, Surabhi Gupta, Ani Nenkova,Sebastian Riedel.
This work was supported by aScottish Enterprise Edinburgh-Stanford Link grant(R37588), as part of the EASIE project, and bythe Advanced Research and Development Activity(ARDA)?s Advanced Question Answering for Intel-ligence (AQUAINT) Phase II Program.ReferencesC.
E. Antoniak.
1974.
Mixtures of Dirichlet processes with ap-plications to Bayesian nonparametrics.
Annals of Statistics,2:1152?1174.M.J.
Beal, Z. Ghahramani, and C.E.
Rasmussen.
2002.
Theinfinite hidden Markov model.
In Advances in Neural Infor-mation Processing Systems, pages 577?584.E.
Charniak.
1996.
Tree-bank grammars.
In AAAI 1996, pages1031?1036.E.
Charniak.
2000.
A maximum-entropy-inspired parser.
InHLT-NAACL 2000, pages 132?139.M.
Collins.
2003.
Head-driven statistical models for natural lan-guage parsing.
Computational Linguistics, 29(4):589?637.T.
S. Ferguson.
1973.
A Bayesian analysis of some nonpara-metric problems.
Annals of Statistics, 1:209?230.J.
Ghosh.
2003.
Scalable clustering methods for data mining.
InN.
Ye, editor, Handbook of Data Mining, chapter 10, pages247?277.
Lawrence Erlbaum Assoc.A.
Haghighi and D. Klein.
2006.
Prototype-driven learning forsequence models.
In HLT-NAACL 2006.M.
Johnson, T. Griffiths, and S. Goldwater.
2007.
Adaptorgrammars: A framework for specifying compositional non-parametric Bayesian models.
In NIPS 2007.D.
Klein and C. D. Manning.
2003a.
Accurate unlexicalizedparsing.
In ACL 2003.D.
Klein and C. D. Manning.
2003b.
Factored A* search formodels over sequences and trees.
In IJCAI 2003.P.
Liang, S. Petrov, D. Klein, and M. Jordan.
2007.
Nonpara-metric PCFGs using Dirichlet processes.
In EMNLP 2007.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a large annotated corpus of English: The PennTreebank.
Computational Linguistics, 19(2):313?330.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.
Learningaccurate, compact, and interpretable tree annotation.
In ACL44/COLING 21, pages 433?440.J.
Pitman.
2002.
Poisson-Dirichlet and GEM invariant distribu-tions for split-and-merge transformations of an interval par-tition.
Combinatorics, Probability and Computing, 11:501?514.N.
A. Smith and J. Eisner.
2005.
Contrastive estimation: Train-ing log-linear models on unlabeled data.
In ACL 2005.Y.
W. Teh, M.I.
Jordan, M. J. Beal, and D.M.
Blei.
2006.
Hier-archical Dirichlet processes.
Journal of the American Statis-tical Association, 101:1566?1581.H.
Yamada and Y. Matsumoto.
2003.
Statistical dependencyanalysis with support vector machines.
In Proceedings ofIWPT, pages 195?206.279
