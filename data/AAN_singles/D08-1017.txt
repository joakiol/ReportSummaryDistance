Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157?166,Honolulu, October 2008. c?2008 Association for Computational LinguisticsStacking Dependency ParsersAndre?
F. T.
Martins??
Dipanjan Das?
Noah A. Smith?
Eric P.
Xing?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal{afm,dipanjan,nasmith,epxing}@cs.cmu.eduAbstractWe explore a stacked framework for learn-ing to predict dependency structures for natu-ral language sentences.
A typical approach ingraph-based dependency parsing has been toassume a factorized model, where local fea-tures are used but a global function is opti-mized (McDonald et al, 2005b).
RecentlyNivre and McDonald (2008) used the outputof one dependency parser to provide featuresfor another.
We show that this is an exampleof stacked learning, in which a second pre-dictor is trained to improve the performanceof the first.
Further, we argue that this tech-nique is a novel way of approximating richnon-local features in the second parser, with-out sacrificing efficient, model-optimal pre-diction.
Experiments on twelve languagesshow that stacking transition-based and graph-based parsers improves performance over ex-isting state-of-the-art dependency parsers.1 IntroductionIn this paper we address a representation-efficiencytradeoff in statistical natural language processingthrough the use of stacked learning (Wolpert,1992).
This tradeoff is exemplified in dependencyparsing, illustrated in Fig.
1, on which we focus inthis paper:?
Exact algorithms for dependency parsing (Eis-ner and Satta, 1999; McDonald et al, 2005b)are tractable only when the model makes verystrong, linguistically unsupportable independenceassumptions, such as ?arc factorization?
for non-projective dependency parsing (McDonald andSatta, 2007).?
Feature-rich parsers must resort to search orgreediness, (Ratnaparkhi et al, 1994; Sagae andLavie, 2005; Hall et al, 2006), so that parsingsolutions are inexact and learned models may besubject to certain kinds of bias (Lafferty et al,2001).A solution that leverages the complementarystrengths of these two approaches?described in de-tail by McDonald and Nivre (2007)?was recentlyand successfully explored by Nivre and McDonald(2008).
Our contribution begins by reinterpretingand generalizing their parser combination scheme asa stacking of parsers.We give a new theoretical motivation for stackingparsers, in terms of extending a parsing model?s fea-ture space.
Specifically, we view stacked learning asa way of approximating non-local features in a lin-ear model, rather than making empirically dubiousindependence (McDonald et al, 2005b) or structuralassumptions (e.g., projectivity, Eisner, 1996), usingsearch approximations (Sagae and Lavie, 2005; Hallet al, 2006; McDonald and Pereira, 2006), solving a(generally NP-hard) integer linear program (Riedeland Clarke, 2006), or adding latent variables (Titovand Henderson, 2007).
Notably, we introduce theuse of very rich non-local approximate features inone parser, through the output of another parser.Related approaches are the belief propagation algo-rithm of Smith and Eisner (2008), and the ?tradingof structure for features?
explored by Liang et al157Figure 1: A projective dependency graph.Figure 2: Non-projective dependency graph.those that assume each dependency decision is in-dependent modulo the global structural constraintthat dependency graphs must be trees.
Such mod-els are commonly referred to as edge-factored sincetheir parameters factor relative to individual edgesof the graph (Paskin, 2001; McDonald et al,2005a).
Edge-factored models have many computa-tional benefits, most notably that inference for non-projective dependency graphs can be achieved inpolynomial time (McDonald et al, 2005b).
The pri-mary problem in treating each dependency as in-dependent is that it is not a realistic assumption.Non-local information, such as arity (or valency)and neighbouring dependencies, can be crucial toobtaining high parsing accuracies (Klein and Man-ning, 2002; McDonald and Pereira, 2006).
How-ever, in the data-driven parsing setting this can bepartially adverted by incorporating rich feature rep-resentations over the input (McDonald et al, 2005a).The goal of this work is to further our currentunderstanding of the computational nature of non-projective parsing algorithms for both learning andinference within the data-driven setting.
We start byinvestigating and extending the edge-factored modelof McDonald et al (2005b).
In particular, we ap-peal to the Matrix Tree Theorem for multi-digraphsto design polynomial-time algorithms for calculat-ing both the partition function and edge expecta-tions over all possible dependency graphs for a givensentence.
To motivate these algorithms, we showthat they can be used in many important learningand inference problems including min-risk decod-ing, training globally normalized log-linear mod-els, syntactic language modeling, and unsupervisedlearning via the EM algorithm ?
none of which havepreviously been known to have exact non-projectiveimplementations.We then switch focus to models that account fornon-local information, in particular arity and neigh-bouring parse decisions.
For systems that model ar-ity constraints we give a reduction from the Hamilto-nian graph problem suggesting that the parsing prob-lem is intractable in this case.
For neighbouringparse decisions, we extend the work of McDonaldand Pereira (2006) and show that modeling verticalneighbourhoods makes parsing intractable in addi-tion to modeling horizontal neighbourhoods.
A con-sequence of these results is that it is unlikely thatexact non-projective dependency parsing is tractablefor any model assumptions weaker than those madeby the edge-factored models.1.1 Related WorkThere has been extensive work on data-driven de-pendency parsing for both projective parsing (Eis-ner, 1996; Paskin, 2001; Yamada and Matsumoto,2003; Nivre and Scholz, 2004; McDonald et al,2005a) and non-projective parsing systems (Nivreand Nilsson, 2005; Hall and No?va?k, 2005; McDon-ald et al, 2005b).
These approaches can often beclassified into two broad categories.
In the first cat-egory are those methods that employ approximateinference, typically through the use of linear timeshift-reduce parsing algorithms (Yamada and Mat-sumoto, 2003; Nivre and Scholz, 2004; Nivre andNilsson, 2005).
In the second category are thosethat employ exhaustive inference algorithms, usu-ally by making strong independence assumptions, asis the case for edge-factored models (Paskin, 2001;McDonald et al, 2005a; McDonald et al, 2005b).Recently there have also been proposals for exhaus-tive methods that weaken the edge-factored assump-tion, including both approximate methods (McDon-ald and Pereira, 2006) and exact methods through in-teger linear programming (Riedel and Clarke, 2006)or branch-and-bound algorithms (Hirakawa, 2006).For grammar based models there has been limitedwork on empirical systems for non-projective pars-ing systems, notable exceptions include the workof Wang and Harper (2004).
Theoretical studies ofnote include the work of Neuhaus and Bo?ker (1997)showing that the recognition problem for a mini-$ Figure 1: A projective dependency graph.Figure 2: Non-projective dependency graph.those that assume each dependency decision is in-dependent modulo the global structural constraintthat dependency graphs must be trees.
Such mod-els are commonly referred to as edge-factored sincetheir parameters factor relative to individual edgesof the graph (Paskin, 2001; McDonald et al,2005a).
Edge-factored models have many computa-tional benefits, most notably that inference for non-projective dependency graphs can be achieved inpolynomial time (McDonald et al, 2005b).
The pri-mary problem in treating each dependency as in-dependent is that it is not a realistic assumption.Non-local information, such as arity (or valency)and eighbouring dependencies, can be crucial toobtaining high parsing accuracies (Klein and Man-ning, 2002; McDonald and P reira, 2006).
How-ever, i the data-dr ven parsing etting this can bepartially adverted by incorporating rich feature rep-resentations over the input (McDonald et al, 2005a).The goal of this work is to further our curr ntunderstanding of the computational nature of non-projective parsing algorithms for both learning andinference within the data-driven setting.
We start byinvestigating and extending the edge-factored modelof McDonald et al (2005b).
In particular, we ap-peal to the Matrix Tree Theorem for multi-digraphsto design polynomial-time algorithms for calculat-ing both the partition function and edge expecta-tions over all possible dependency graphs for a givensentence.
To motivate these algorithms, we showthat they can be used in many important learningand inference problems including min-risk decod-ing, training globally normalized log-linear mod-els, syntactic language modeling, and unsupervisedlearning via the EM algorithm ?
none of which havepreviously been known t have exact n n-projectiveimplementations.We th switch focus to models that account fornon-local information, in particular arity and neigh-bouring parse decisions.
For systems that model ar-ity constraints we give a reduction from the Hamilto-nian graph problem suggesting that the parsing prob-lem is intractable in this case.
For neighbouringparse decisions, we extend the work of McDonaldand Pereira (2006) and show that modeling verticalneighbourhoods makes parsing intractable in addi-tion to modeling horizontal neighbourhoods.
A con-sequence of these results is that it is unlikely thatexact non-projective dependency parsing is tractablefor any model assumptions weaker than those madeby the edge-factored models.1.1 Related W rkThere has been extensive work on data-driven de-pendency parsing for both projective parsing (Eis-ner, 1996; Paskin, 2001; Yamada and Matsumoto,2003; Nivre and Scholz, 2004; McDonald et al,2005a) and non-projective parsing systems (Nivreand Nilsson, 2005; Hall and No?va?k, 2005; McDon-ald et al, 2005b).
These approaches can often beclassified into two broad categories.
In the first cat-egory are those methods that employ approximateinference, typically through the use of linear timeshift-reduce parsing algorit ms (Yamad and Mat-sumoto, 2003; Nivre and Scholz, 2004; Nivre andNilsson, 2005).
I the second category are thosethat employ exhaustive inference algorithms, usu-ally by making strong independence assumptions, ais the case for edge-factored mod ls (Paskin, 2001;McDonald et al, 2005a; McDonald et al, 2005b).Recently there have also been proposals for exhaus-tive methods that weaken the edge-factored assump-tion, including both approximate methods (McDon-ald and Pereira, 2006) and exact methods through in-teger linear programming (Riedel and Clarke, 2006)or branch-and-bound algorithms (Hirakawa, 2006).For grammar based models there has been limitedwork on empirical systems for non-projective pars-ing systems, notable exceptions include the workof Wang and Harper (2004).
Theoretical studies ofnote include the work of Neuhaus and Bo?ker (1997)showing that the recognition problem for a mini-$Figure 1: A projective dependency parse (top), and a non-projective dependency parse (bottom) for two Englishsentences; examples from McDonald and Satta (2007).
(2008).This paper focuses on dependency parsing, whichhas become widely used in relation extraction (Cu-lotta and Sorensen, 2004), machine translation(Ding and Palmer, 2005), question answering (Wanget al, 2007), and many other NLP applications.We show that stacking methods outperform the ap-proximate ?second-order?
parser of McDonald andPereira (2006) on twelve languages and can be usedwithin that approximatio to chieve even better re-sults.
These results are similar in sp rit t (Nivre andMcDonald, 008), but with the following novel con-tributio s:?
a stacking interpretation,?
a ric r fe tu e set that includes non-l c l f atures(shown here to improve erform nc ), and?
a variety of stacking architectures.Using stacking with rich features, we obtain resultscomp titive ith Nivre an McDonald (2008) whilepreserving the fast qua ratic parsing ime of arc-factored spanning tree algorithms.The paper is organiz d as follows.
We discuss re-lated prior work on dependency parsing and stackingin ?2.
Our model is given in ?3.
A novel analysis ofstacking in linear models is given in ?4.
Experimentsare presented in ?5.2 Background and Related WorkWe briefly review work on the NLP task of depen-dency parsing and the machine learning frameworkknown as stacked learning.2.1 Dep dency ParsingDependency syntax is a lightweight syntactic rep-resentation that models a sentence as a graph wherethe words are vertices and syntactic relationships aredirected edges (arcs) connecting heads to their argu-ments and modifiers.Dependency parsing is often viewed computa-tionally as a structured prediction problem: for eachinput sentence x, with n words, exponentially manycandidate depend ncy trees y ?
Y(x) are possible inprinciple.
We den te each tree by its set of verticesand directed arcs, y = (Vy, Ay).
A legal depen-dency tree has n+ 1 verti es, each corresponding toone word plus a ?wall?
symbol, $, assumed to be thehidden root of the sentence.
In a valid dependencytree, each vertex except the root has exactly one par-ent.
In the projective case, arcs cannot cross whendepicted on one side of the sentence; in the non-projective cas , this constraint is not impos d (seeFig.
1).2.1.1 Graph-based vs. transition-based modelsMost recent work on dependency parsing can becategorized as graph-based or transition-based.
Ingraph-based parsing, dependency trees are scoredby factoring the tree into its arcs, and parsing isperfo med by searching for the highest scoring tree(Eis er 1996; McD nald et al, 2005b).
Transition-based parsers model the sequ nce of ecisions ofshift-reduce parser, given previous deci ions andcurrent state, and par ing is perfor ed by greedilychoosing the highest scori g transition out of eachsuccessive parsing state or by searching for the bestsequence of transitions (Ratnaparkhi et al, 1994;Yamada and Matsumoto, 2003; Nivre et al, 2004;Sagae and Lavie, 2005; Hall et al, 2006).Both approaches most commonly use linear mod-els to assign scores to arcs or decisions, so that ascore is a dot-product of a feature vector f and alearned weight vector w.In sum, these two lines of researc use differentapproximations to achieve trac ability.
Transition-based approaches solve sequence of local prob-lems in sequ nce, sacrificing global opti ality guar-antees and possibly expressive power (Abney et al,1999).
Graph-based methods perform global in-ference using score factorizations that correspondto strong independence assumptions (discussed in158?2.1.2).
Recently, Nivre and McDonald (2008) pro-posed combining a graph-based and a transition-based parser and have shown a significant improve-ment for several languages by letting one of theparsers ?guide?
the other.
Our stacked formalism(to be described in ?3) generalizes this approach.2.1.2 Arc factorizationIn the successful graph-based method of McDon-ald et al (2005b), an arc factorization independenceassumption is used to ensure tractability.
This as-sumption forbids any feature that depends on twoor more arcs, permitting only ?arc-factored?
features(i.e.
features that depend only on a single candidatearc a ?
Ay and on the input sequence x).
This in-duces a decomposition of the feature vector f(x, y)as:f(x, y) =?a?Ayfa(x).Parsing amounts to solving arg maxy?Y(x)w>f(x, y), where w is a weight vector.
Witha projectivity constraint and arc factorization, theparsing problem can be solved in cubic time bydynamic programming (Eisner, 1996), and with aweaker ?tree?
constraint (permitting nonprojectiveparses) and arc factorization, a quadratic-timealgorithm exists (Chu and Liu, 1965; Edmonds,1967), as shown by McDonald et al (2005b).
Inthe projective case, the arc-factored assumption canbe weakened in certain ways while maintainingpolynomial parser runtime (Eisner and Satta, 1999),but not in the nonprojective case (McDonald andSatta, 2007), where finding the highest-scoring treebecomes NP-hard.McDonald and Pereira (2006) adopted an approx-imation based on O(n3) projective parsing followedby rearrangement to permit crossing arcs, achievinghigher performance.
In ?3 we adopt a frameworkthat maintains O(n2) runtime (still exploiting theChu-Liu-Edmonds algorithm) while approximatingnon arc-factored features.2.2 Stacked LearningStacked generalization was first proposed byWolpert (1992) and Breiman (1996) for regression.The idea is to include two ?levels?
of predictors.
Thefirst level, ?level 0,?
includes one or more predictorsg1, .
.
.
, gK : Rd ?
R; each receives input x ?
Rdand outputs a prediction gk(x).
The second level,?level 1,?
consists of a single function h : Rd+K ?R that takes as input ?x, g1(x), .
.
.
gK(x)?
and out-puts a final prediction y?
= h(x, g1(x), .
.
.
gK(x)).The predictor, then, combines an ensemble (the gk)with a meta-predictor (h).Training is done as follows: the training data aresplit into L partitions, and L instances of the level0 predictor are trained in a ?leave-one-out?
basis.Then, an augmented dataset is formed by lettingeach instance output predictions for the partition thatwas left out.
Finally, each level 0 predictor is trainedusing the original dataset, and the level 1 predictoris trained on the augmented dataset, simulating thetest-time setting when h is applied to a new instancex concatenated with ?gk(x)?k.This framework has also been applied to classifi-cation, for example with structured data.
Some ap-plications (including here) use only one classifier atlevel 0; recent work includes sequence labeling (Co-hen and de Carvalho, 2005) and inference in condi-tional random fields (Kou and Cohen, 2007).
Stack-ing is also intuitively related to transformation-basedlearning (Brill, 1993).3 Stacked Dependency ParsingWe next describe how to use stacked learning forefficient, rich-featured dependency parsing.3.1 ArchitectureThe architecture consists of two levels.
At level 0we include a single dependency parser.
At runtime,this ?level 0 parser?
g processes an input sentence xand outputs the set of predicted edges that make upits estimation of the dependency tree, y?0 = g(x).
Atlevel 1, we apply a dependency parser?in this work,always a graph-based dependency parser?that usesbasic factored features plus new ones from the edgespredicted by the level 0 parser.
The final parser pre-dicts parse trees as h(x, g(x)), so that the total run-time is additive in calculating h(?)
and g(?
).The stacking framework is agnostic about theform of g and h and the methods used to learn themfrom data.
In this work we use two well-known,publicly available dependency parsers, MSTParser(McDonald et al, 2005b),1 which implements ex-1http://sourceforge.net/projects/mstparser159act first-order arc-factored nonprojective parsing(?2.1.2) and approximate second-order nonprojec-tive parsing, and MaltParser (Nivre et al, 2006),which is a state-of-the-art transition-based parser.2We do not alter the training algorithms used in priorwork for learning these two parsers from data.
Us-ing the existing parsers as starting points, we willcombine them in a variety of ways.3.2 TrainingRegardless of our choices for the specific parsers andlearning algorithms at level 0 and level 1, training isdone as sketched in ?2.2.
Let D be a set of trainingexamples {?xi, yi?}i.1.
Split training data D into L partitionsD1, .
.
.
,DL.2.
Train L instances of the level 0 parser in the fol-lowing way: the l-th instance, gl, is trained onD?l = D \ Dl.
Then use gl to output predic-tions for the (unseen) partition Dl.
At the end,an augmented dataset D?
=?Ll=1 D?l is built, sothat D?
= {?xi, g(xi), yi?}i.3.
Train the level 0 parser g on the original trainingdata D.4.
Train the level 1 parser h on the augmented train-ing data D?.The runtime of this algorithm is O(LT0+T1), whereT0 and T1 are the individual runtimes required fortraining level 0 and level 1 alone, respectively.4 Two Views of Stacked ParsingWe next describe two motivations for stackingparsers: as a way of augmenting the features of agraph-based dependency parser or as a way to ap-proximate higher-order models.4.1 Adding Input FeaturesSuppose that the level 1 classifier is an arc-factoredgraph-based parser.
The feature vectors will take theform3f(x, y) = f1(x, y) ^ f2(x, y?0, y)=?a?Ayf1,a(x) ^ f2,a(x, g(x)),2http://w3.msi.vxu.se/?jha/maltparser3We use^ to denote vector concatenation.where f1(x, y) =?a?Ay f1,a(x) are regu-lar arc-factored features, and f2(x, y?0, y) =?a?Ay f2,a(x, g(x)) are the stacked features.
Anexample of a stacked feature is a binary featuref2,a(x, g(x)) that fires if and only if the arc a waspredicted by g, i.e., if a ?
Ag(x); such a feature wasused by Nivre and McDonald (2008).It is difficult in general to decide whether the in-clusion of such a feature yields a better parser, sincefeatures strongly correlate with each other.
How-ever, a popular heuristic for feature selection con-sists of measuring the information gain provided byeach individual feature.
In this case, we may obtaina closed-form expression for the information gainthat f2,a(x, g(x)) provides about the existence or notof the arc a in the actual dependency tree y.
Let Aand A?
be binary random variables associated withthe events a ?
Ay and a?
?
Ag(x), respectively.
Wehave:I(A;A?)
=?a,a??
{0,1}p(a, a?)
log2p(a, a?)p(a)p(a?
)= H(A?)??a?
{0,1}p(a)H(A?|A = a).Assuming, for simplicity, that at level 0 the prob-ability of false positives equals the probability offalse negatives (i.e., Perr , p(a?
= 0|a = 1) =p(a?
= 1|a = 0)), and that the probability oftrue positives equals the probability of true negatives(1 ?
Perr = p(a?
= 0|a = 0) = p(a?
= 1|a = 1)),the expression above reduces to:I(A;A?)
= H(A?)
+ Perr log2 Perr+ (1?
Perr) log2(1?
Perr)= H(A?
)?Herr,where Herr denotes the entropy of the probability oferror on each arc?s prediction by the level 0 classi-fier.
If Perr ?
0.5 (i.e.
if the level 0 classifier isbetter than random), then the information gain pro-vided by this simple stacked feature increases with(a) the accuracy of the level 0 classifier, and (b) theentropy H(A?)
of the distribution associated with itsarc predictions.4.2 Approximating Non-factored FeaturesAnother way of interpreting the stacking frameworkis as a means to approximate a higher order model,160such as one that is not arc-factored, by using stackedfeatures that make use of the predicted structurearound a candidate arc.
Consider a second-ordermodel where the features decompose by arc and byarc pair:f(x, y) =?a1?Ay?
?fa1(x) ^?a2?Ayfa1,a2(x)??
.Exact parsing under such model, with arbitrarysecond-order features, is intractable (McDonald andSatta, 2007).
Let us now consider a stacked modelin which the level 0 predictor outputs a parse y?.
Atlevel 1, we use arc-factored features that may bewritten asf?
(x, y) =?a1?Ay?
?fa1(x) ^?a2?Ay?fa1,a2(x)??
;this model differs from the previous one only by re-placing Ay by Ay?
in the index set of the second sum-mation.
Since y?
is given, this makes the latter modelarc-factored, and therefore, tractable.
We can nowview f?
(x, y) as an approximation of f(x, y); indeed,we can bound the score approximation error,?s(x, y) =???w?>f?
(x, y)?w>f(x, y)???
,where w?
and w stand respectively for the parameterslearned for the stacked model and those that wouldbe learned for the (intractable) exact second ordermodel.
We can bound ?s(x, y) by spliting it intotwo terms: ?s(x, y) =???(w?
?w)>f?
(x, y) + w>(f?
(x, y)?
f(x, y))???????(w?
?w)>f?
(x, y)????
??
?,?str(x,y)+???w>(f?
(x, y)?
f(x, y))????
??
?,?sdec(x,y);where we introduced the terms ?str and ?sdec thatreflect the portion of the score approximation errorthat are due to training error (i.e., different parame-terizations of the exact and approximate models) anddecoding error (same parameterizations, but differ-ent feature vectors).
Using Ho?lder?s inequality, theformer term can be bounded as:?str(x, y) =???(w?
?w)>f?
(x, y)????
?w?
?w?1 ?
?f?
(x, y)???
?w?
?w?1 ;where ?.
?1 and ?.??
denote the `1-norm and sup-norm, respectively, and the last inequality holdswhen the features are binary (so that ?f?
(x, y)??
?1).
The proper way to bound the term ?w?
?w?1depends on the training algorithm.
As for the de-coding error term, it can bounded for a given weightvector w, sentence x, candidate tree y, and level 0prediction y?.
Decomposing the weighted vector asw = w1 ^ w2, w2 being the sub-vector associ-ated with the second-order features, we have respec-tively: ?sdec(x, y) =???w>(f?
(x, y)?
f(x, y))???=???????a1?Ayw>2???a2?Ay?fa1,a2(x)??a2?Ayfa1,a2(x)??????????a1?Ay?a2?Ay??Ay??
?w>2 fa1,a2(x)?????a1?Ay|Ay?
?Ay| ?
maxa2?Ay??Ay??
?w>2 fa1,a2(x)??
?=?a1?Ay2L(y, y?)
?
maxa2?Ay??Ay??
?w>2 fa1,a2(x)???
,where Ay?
?Ay , (Ay?
?Ay) ?
(Ay ?Ay?)
denotesthe symmetric difference of the sets Ay?
and Ay,which has cardinality 2L(y, y?
), i.e., twice the Ham-ming distance between the sequences of heads thatcharacterize y and the predicted parse y?.
UsingHo?lder?s inequality, we have both??
?w>2 fa1,a2(x)???
?
?w2?1 ?
?fa1,a2(x)??and??
?w>2 fa1,a2(x)???
?
?w2??
?
?fa1,a2(x)?1.Assuming that all features are binary valued, wehave that ?fa1,a2(x)??
?
1 and that ?fa1,a2(x)?1 ?Nf,2, where Nf,2 denotes the maximum number ofactive second order features for any possible pair ofarcs (a1, a2).
Therefore:?sdec(x, y) ?
2nL(y, y?)
min{?w2?1, Nf,2??w2??
},where n is the sentence length.
Although this boundcan be loose, it suggests (intuitively) that the scoreapproximation degrades as the predicted tree y?
getsfarther away from the true tree y (in Hamming dis-tance).
It also degrades with the magnitude ofweights associated with the second-order features,161Name DescriptionPredEdge Indicates whether the candidate edgewas present, and what was its label.Sibling Lemma, POS, link label, distance anddirection of attachment of the previousand and next predicted siblingsGrandParents Lemma, POS, link label, distance anddirection of attachment of the grandpar-ent of the current modifierPredHead Predicted head of the candidate modifier(if PredEdge=0)AllChildren Sequence of POS and link labels of allthe predicted children of the candidateheadTable 1: Feature sets derived from the level 0 parser.Subset DescriptionA PredEdgeB PredEdge+SiblingC PredEdge+Sibling+GrandParentsD PredEdge+Sibling+GrandParents+PredHeadE PredEdge+Sibling+GrandParents+PredHead+AllChildrenTable 2: Combinations of features enumerated in Table 1used for stacking.
A is a replication of (Nivre and Mc-Donald, 2008), except for the modifications described infootnote 4.which suggests that a separate regularization of thefirst-order and stacked features might be beneficialin a stacking framework.As a side note, if we set each component ofthe weight vector to one, we obtain a boundon the `1-norm of the feature vector difference,???f?
(x, y)?
f(x, y)???1?
2nL(y, y?
)Nf,2.5 ExperimentsIn the following experiments we demonstrate the ef-fectiveness of stacking parsers.
As noted in ?3.1, wemake use of two component parsers, the graph-basedMSTParser and the transition-based MaltParser.5.1 Implementation and Experimental DetailsThe publicly available version of MSTParser per-forms parsing and labeling jointly.
We adapted thissystem to first perform unlabeled parsing, then la-bel the arcs using a log-linear classifier with accessto the full unlabeled parse (McDonald et al, 2005a;McDonald et al, 2005b; McDonald and Pereira,2006).
In stacking experiments, the arc labels fromthe level 0 parser are also used as a feature.4In the following subsections, we refer to our mod-ification of the MSTParser as MST 1O (the arc-factored version) and MST 2O (the second-orderarc-pair-factored version).
All our experiments usethe non-projective version of this parser.
We refer tothe MaltParser as Malt .We report experiments on twelve languages fromthe CoNLL-X shared task (Buchholz and Marsi,2006).5 All experiments are evaluated using thelabeled attachment score (LAS), using the defaultsettings.6 Statistical significance is measured us-ing Dan Bikel?s randomized parsing evaluation com-parator with 10,000 iterations.7 The additional fea-tures used in the level 1 parser are enumerated inTable 1 and their various subsets are depicted in Ta-ble 2.
The PredEdge features are exactly the six fea-tures used by Nivre and McDonald (2008) in theirMSTMalt parser; therefore, feature set A is a repli-cation of this parser except for modifications notedin footnote 4.
In all our experiments, the number ofpartitions used to create D?
is L = 2.5.2 Experiment: MST 2O + MST 2OOur first experiment stacks the highly accurateMST 2O parser with itself.
At level 0, the parseruses only the standard features (?5.1), and at level 1,these are augmented by various subsets of featuresof x along with the output of the level 0 parser, g(x)(Table 2).
The results are shown in Table 3.
Whilewe see improvements over the single-parser baseline4We made other modifications to MSTParser, implement-ing many of the successes described by (McDonald et al,2006).
Our version of the code is publicly available at http://www.ark.cs.cmu.edu/MSTParserStacked.
Themodifications included an approximation to lemmas for datasetswithout lemmas (three-character prefixes), and replacing mor-phology/word and morphology/lemma features with morphol-ogy/POS features.5The CoNLL-X shared task actually involves thirteen lan-guages; our experiments do not include Czech (the largestdataset), due to time constraints.
Therefore, the average resultsplotted in the last rows of Tables 3, 4, and 5 are not directlycomparable with previously published averages over thirteenlanguages.6http://nextens.uvt.nl/?conll/software.html7http://www.cis.upenn.edu/?dbikel/software.html162MST2O+MST 2O, A+MST 2O, B+MST 2O, C+MST 2O, D+MST 2O, EArabic 67.88 66.91 67.41 67.68 67.37 68.02Bulgarian 87.31 87.39 87.03 87.61 87.57 87.55Chinese 87.57 87.16 87.24 87.48 87.42 87.48Danish 85.27 85.39 85.61 85.57 85.43 85.57Dutch 79.99 79.79 79.79 79.83 80.17 80.13German 87.44 86.92 87.32 87.32 87.26 87.04Japanese 90.93 91.41 91.21 91.35 91.11 91.19Portuguese 87.12 87.26 86.88 87.02 87.04 86.98Slovene 74.02 74.30 74.30 74.00 74.14 73.94Spanish 82.43 82.17 82.35 82.81 82.53 82.75Swedish 82.87 82.99 82.95 82.51 83.01 82.69Turkish 60.11 59.47 59.25 59.47 59.45 59.31Average 81.08 80.93 80.94 81.05 81.04 81.05Table 3: Results of stacking MST 2O with itself at both level 0 and level 1.
Column 2 enumerates LAS for MST 2O.Columns 3?6 enumerate results for four different stacked feature subsets.
Bold indicates best results for a particularlanguage.for nine languages, the improvements are small (lessthan 0.5%).
One of the biggest concerns about thismodel is the fact that it stacks two predictors thatare very similar in nature: both are graph-based andshare the features f1,a(x).
It has been pointed out byBreiman (1996), among others, that the success ofensemble methods like stacked learning strongly de-pends on how uncorrelated the individual decisionsmade by each predictor are from the others?
deci-sions.8 This experiment provides further evidencefor the claim.5.3 Experiment: Malt + MST 2OWe next use MaltParser at level 0 and the second-order arc-pair-factored MST 2O at level 1.
Thisextends the experiments of Nivre and McDonald(2008), replicated in our feature subset A.Table 4 enumerates the results.
Note that thebest-performing stacked configuration for each andevery language outperforms MST 2O, corroborat-ing results reported by Nivre and McDonald (2008).The best performing stacked configuration outper-forms Malt as well, except for Japanese and Turk-ish.
Further, our non-arc-factored features largelyoutperform subset A, except on Bulgarian, Chinese,8This claim has a parallel in the cotraining method (Blumand Mitchell, 1998), whose performance is bounded by the de-gree of independence between the two feature sets.and Japanese.
On average, the best feature config-uration is E, which is statistically significant overMalt and MST 2O with p < 0.0001, and over fea-ture subset A with p < 0.01.5.4 Experiment: Malt + MST 1OFinally, we consider stacking MaltParser with thefirst-order, arc-factored MSTParser.
We view thisapproach as perhaps the most promising, since it isan exact parsing method with the quadratic runtimecomplexity of MST 1O.Table 5 enumerates the results.
For all twelvelanguages, some stacked configuration outperformsMST 1O and also, surprisingly, MST 2O, the sec-ond order model.
This provides empirical evi-dence that using rich features from MaltParser atlevel 0, a stacked level 1 first-order MSTParser canoutperform the second-order MSTParser.9 In onlytwo cases (Japanese and Turkish), the MaltParserslightly outperforms the stacked parser.On average, feature configuration D performsthe best, and is statistically significant over Malt ,MST 1O, and MST 2O with p < 0.0001, and overfeature subset A with p < 0.05.
Encouragingly, thisconfiguration is barely outperformed by configura-9Recall that MST 2O uses approximate search, as opposedto stacking, which uses approximate features.163MaltMST2OMalt +MST2O, AMalt +MST2O, BMalt +MST2O, CMalt +MST2O, DMalt +MST2O, EArabic 66.71 67.88 68.56 69.12 68.64 68.34 68.92Bulgarian 87.41 87.31 88.99 88.89 88.89 88.93 88.91Chinese 86.92 87.57 88.41 88.31 88.29 88.13 88.41Danish 84.77 85.27 86.45 86.67 86.79 86.13 86.71Dutch 78.59 79.99 80.75 81.47 81.47 81.51 81.29German 85.82 87.44 88.16 88.50 88.56 88.68 88.38Japanese 91.65 90.93 91.63 91.43 91.59 91.61 91.49Portuguese 87.60 87.12 88.00 88.24 88.30 88.18 88.22Slovene 70.30 74.02 76.62 76.00 76.60 76.18 76.72Spanish 81.29 82.43 83.09 83.73 83.47 83.21 83.43Swedish 84.58 82.87 84.92 84.60 84.80 85.16 84.88Turkish 65.68 60.11 64.35 64.51 64.51 65.07 65.21Average 80.94 81.08 82.52 82.58 82.65 82.59 82.71Table 4: Results of stacking Malt and MST 2O at level 0 and level 1, respectively.
Columns 2?4 enumerate LAS forMalt , MST 2O and Malt + MST 2O as in Nivre and McDonald (2008).
Columns 5?8 enumerate results for four otherstacked feature configurations.
Bold indicates best result for a language.tion A of Malt + MST 2O (see Table 4), the dif-ference being statistically insignificant (p > 0.05).This shows that stacking Malt with the exact, arc-factored MST 1O bridges the difference between theindividual MST 1O and MST 2O models, by approx-imating higher order features, but maintaining anO(n2) runtime and finding the model-optimal parse.5.5 Disagreement as a Confidence MeasureIn pipelines or semisupervised settings, it is use-ful when a parser can provide a confidence measurealongside its predicted parse tree.
Because stackedpredictors use ensembles with observable outputs,differences among those outputs may be used to es-timate confidence in the final output.
In stacked de-pendency parsing, this can be done (for example) bymeasuring the Hamming distance between the out-puts of the level 0 and 1 parsers, L(g(x), h(x)).
In-deed, the bound derived in ?4.2 suggests that thesecond-order approximation degrades for candidateparses y that are Hamming-far from g(x); therefore,if L(g(x), h(x)) is large, the best score s(x, h(x))may well be ?biased?
due to misleading neighbor-ing information provided by the level 0 parser.We illustrate this point with an empirical analysisof the level 0/1 disagreement for the set of exper-iments described in ?5.3; namely, we compare the0 2 4 6 8 100.650.70.750.80.850.90.951L(g(x),h(x))SentenceAveraged AccuracyLevel 0Level 1Level 0 (Overall)Level 1 (Overall)Figure 2: Accuracy as a function of token disagreementbetween level 0 and level 1.
The x-axis is the Hammingdistance L(g(x), h(x)), i.e., the number of tokens wherelevel 0 and level 1 disagree.
The y-axis is the accuracyaveraged over sentences that have the specified Hammingdistance, both for level 0 and level 1.164MaltMST1OMST2OMalt +MST1O, AMalt +MST1O, BMalt +MST1O, CMalt +MST1O, DMalt +MST1O, EArabic 66.71 66.81 67.88 68.40 68.50 68.20 68.42 68.68Bulgarian 87.41 86.65 87.31 88.55 88.67 88.75 88.71 88.79Chinese 86.92 86.60 87.57 87.67 87.73 87.83 87.67 87.61Danish 84.77 84.87 85.27 86.59 86.27 86.21 86.35 86.15Dutch 78.59 78.95 79.99 80.53 81.51 80.71 81.61 81.37German 85.82 86.26 87.44 88.18 88.30 88.20 88.36 88.42Japanese 91.65 91.01 90.93 91.55 91.53 91.51 91.43 91.57Portuguese 87.60 86.28 87.12 88.16 88.26 88.46 88.26 88.36Slovene 70.30 73.96 74.02 75.84 75.64 75.42 75.96 75.64Spanish 81.29 81.07 82.43 82.61 83.13 83.13 83.09 82.99Swedish 84.58 81.88 82.87 84.86 84.62 84.64 84.82 84.76Turkish 65.68 59.63 60.11 64.49 64.97 64.47 64.63 64.61Average 80.94 80.33 81.08 82.28 82.42 82.29 82.44 82.41Table 5: Results of stacking Malt and MST 1O at level 0 and level 1, respectively.
Columns 2?4 enumerate LAS forMalt , MST 1O and MST 2O.
Columns 5?9 enumerate results for five different stacked feature configurations.
Boldindicates the best result for a language.level 0 and level 1 predictions under the best overallconfiguration (configuration E of Malt+MST2O).Figure 2 depicts accuracy as a function of level 0-level 1 disagreement (in number of tokens), aver-aged over all datasets.We can see that performance degrades steeplywhen the disagreement between levels 0 and 1 in-creases in the range 0?4, and then behaves more ir-regularly but keeping the same trend.
This suggeststhat the Hamming distance L(g(x), h(x)) is infor-mative about parser performance and may be usedas a confidence measure.6 ConclusionIn this work, we made use of stacked learning to im-prove dependency parsing.
We considered an archi-tecture with two layers, where the output of a stan-dard parser in the first level provides new featuresfor a parser in the subsequent level.
During learning,the second parser learns to correct mistakes made bythe first one.
The novelty of our approach is in theexploitation of higher-order predicted edges to simu-late non-local features in the second parser.
We pro-vided a novel interpretation of stacking as featureapproximation, and our experimental results showrich-featured stacked parsers outperforming state-of-the-art single-layer and ensemble parsers.
No-tably, using a simple arc-factored parser at level 1,we obtain an exact O(n2) stacked parser that outper-forms earlier approximate methods (McDonald andPereira, 2006).AcknowledgmentsThe authors thank the anonymous reviewers forhelpful comments, Vitor Carvalho, William Cohen,and David Smith for interesting discussions, andRyan McDonald and Joakim Nivre for providingus their code and preprocessed datasets.
A.M. wassupported by a grant from FCT through the CMU-Portugal Program and the Information and Com-munications Technologies Institute (ICTI) at CMU.N.S.
was supported by NSF IIS-0713265 and anIBM faculty award.
E.X.
was supported by NSFDBI-0546594, DBI-0640543, and IIS-0713379.ReferencesS.
P. Abney, D. A. McAllester, and F. Pereira.
1999.
Re-lating probabilistic grammars and automata.
In Pro-ceedings of ACL.A.
Blum and T. Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In Proceedingsof COLT.L.
Breiman.
1996.
Stacked regressions.
Machine Learn-ing, 24:49.165E.
Brill.
1993.
A Corpus-Based Approach to LanguageLearning.
Ph.D. thesis, University of Pennsylvania.S.
Buchholz and E. Marsi.
2006.
CoNLL-X shared taskon multilingual dependency parsing.
In Proceedingsof CoNLL.Y.
J. Chu and T. H. Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14:1396?1400.W.
W. Cohen and V. Rocha de Carvalho.
2005.
Stackedsequential learning.
In Proceedings of IJCAI.A.
Culotta and J. Sorensen.
2004.
Dependency tree ker-nels for relation extraction.
In Proceedings of ACL.Y.
Ding and M. Palmer.
2005.
Machine translation usingprobabilistic synchronous dependency insertion gram-mar.
In Proceedings of ACL.J.
Edmonds.
1967.
Optimum branchings.
Journal of Re-search of the National Bureau of Standards, 71B:233?240.J.
Eisner and G. Satta.
1999.
Efficient parsing for bilex-ical context-free grammars and head automaton gram-mars.
In Proceedings of ACL.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proceedings ofCOLING.J.
Hall, J. Nivre, and J. Nilsson.
2006.
Discriminativeclassifiers for deterministic dependency parsing.
InProceedings of ACL.Z.
Kou and W. W. Cohen.
2007.
Stacked graphical mod-els for efficient inference in Markov random fields.
InProceedings of SDM.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof ICML.P.
Liang, H.
Daume?, and D. Klein.
2008.
Structure com-pilation: trading structure for features.
In Proceedingsof ICML.R.
McDonald and J. Nivre.
2007.
Characterizing theerrors of data-driven dependency parsing models.
InProceedings of EMNLP-CoNLL.R.
T. McDonald and F. C. N. Pereira.
2006.
Online learn-ing of approximate dependency parsing algorithms.
InProceedings of EACL.R.
McDonald and G. Satta.
2007.
On the complexityof non-projective data-driven dependency parsing.
InProceedings of IWPT.R.
McDonald, K. Crammer, and F. Pereira.
2005a.
On-line large-margin training of dependency parsers.
InProceedings of ACL.R.
T. McDonald, F. Pereira, K. Ribarov, and J. Ha-jic.
2005b.
Non-projective dependency parsing us-ing spanning tree algorithms.
In Proceedings of HLT-EMNLP.R.
McDonald, K. Lerman, and F. Pereira.
2006.
Multi-lingual dependency analysis with a two-stage discrim-inative parser.
In Proceedings CoNLL.J.
Nivre and R. McDonald.
2008.
Integrating graph-based and transition-based dependency parsers.
InProceedings of ACL-HLT.J.
Nivre, J.
Hall, and J. Nilsson.
2004.
Memory-baseddependency parsing.
In Proceedings of CoNLL.J.
Nivre, J.
Hall, J. Nilsson, G. Eryig?it, and S. Marinov.2006.
Labeled pseudo-projective dependency pars-ing with support vector machines.
In Proceedings ofCoNLL.A.
Ratnaparkhi, S. Roukos, and R. T. Ward.
1994.
Amaximum entropy model for parsing.
In Proceedingsof ICSLP.S.
Riedel and J. Clarke.
2006.
Incremental integer linearprogramming for non-projective dependency parsing.In Proceedings of EMNLP.K.
Sagae and A. Lavie.
2005.
A classifier-based parserwith linear run-time complexity.
In Proceedings ofIWPT.D.
A. Smith and J. Eisner.
2008.
Dependency parsing bybelief propagation.
In Proceedings of EMNLP.I.
Titov and J. Henderson.
2007.
A latent variable modelfor generative dependency parsing.
In Proceedings ofIWPT.M.
Wang, N. A. Smith, and T. Mitamura.
2007.
What isthe Jeopardy model?
A quasi-synchronous grammarfor QA.
In Proceedings of EMNLP-CoNLL.D.
Wolpert.
1992.
Stacked generalization.
Neural Net-works, 5(2):241?260.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In Pro-ceedings of IWPT.166
