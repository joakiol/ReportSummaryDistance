Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 403?408,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA Corpus of Sentence-level Revisions in Academic Writing:A Step towards Understanding Statement Strength in CommunicationChenhao TanDept.
of Computer ScienceCornell Universitychenhao@cs.cornell.eduLillian LeeDept.
of Computer ScienceCornell Universityllee@cs.cornell.eduAbstractThe strength with which a statement ismade can have a significant impact on theaudience.
For example, international rela-tions can be strained by how the media inone country describes an event in another;and papers can be rejected because theyoverstate or understate their findings.
It isthus important to understand the effects ofstatement strength.
A first step is to be ableto distinguish between strong and weakstatements.
However, even this problemis understudied, partly due to a lack ofdata.
Since strength is inherently relative,revisions of texts that make claims are anatural source of data on strength differ-ences.
In this paper, we introduce a corpusof sentence-level revisions from academicwriting.
We also describe insights gainedfrom our annotation efforts for this task.1 IntroductionIt is important for authors and speakers to find theappropriate ?pitch?
to convey a desired messageto the public.
Indeed, sometimes heated debatescan arise around the choice of statement strength.For instance, on March 1, 2014, an attack at Kun-ming?s railway station left 29 people dead andmore than 140 others injured.1In the aftermath,Chinese media accused Western media of ?soft-pedaling the attack and failing to state clearly thatit was an act of terrorism?.2In particular, regard-ing the statement by the US embassy that referredto this incident as the ?terrible and senseless actof violence in Kunming?, a Weibo user posted ?Ifyou say that the Kunming attack is a ?terrible and1http://en.wikipedia.org/wiki/2014_Kunming_attack2http://sinosphere.blogs.nytimes.com/2014/03/03/u-n-security-council-condemns-terrorist-attack-in-kunming/senseless act of violence?, then the 9/11 attack canbe called a ?regrettable traffic incident?
?.3This example is striking but not an isolated case,for settings in which one party is trying to con-vince another are pervasive; scenarios range fromcourt trials to conference submissions.
Since thestrength and scope of an argument can be a cru-cial factor in its success, it is important to under-stand the effects of statement strength in commu-nication.A first step towards addressing this question isto be able to distinguish between strong and weakstatements.
As strength is inherently relative, it isnatural to look at revisions that change statementstrength, which we refer to as ?strength changes?.Though careful and repeated revisions are presum-ably ubiquitous in politics, legal systems, and jour-nalism, it is not clear how to collect them; on theother hand, revisions to research papers may bemore accessible, and many researchers spend sig-nificant time on editing to convey the right mes-sage regarding the strength of a project?s contribu-tions, novelty, and limitations.
Indeed, statementstrength in science communication matters to writ-ers: understating contributions can affect whetherpeople recognize the true importance of the work;at the same time, overclaiming can cause papers tobe rejected.With the increasing popularity of e-print ser-vices such as the arXiv4, strength changes in scien-tific papers are becoming more readily available.Since the arXiv started in 1991, it has become?the standard repository for new papers in mathe-matics, physics, statistics, computer science, biol-ogy, and other disciplines?
(Krantz, 2007).
An in-triguing observation is that many researchers sub-mit multiple versions of the same paper on arXiv.For instance, among the 70K papers submitted in3http://www.huffingtonpost.co.uk/2014/03/03/china-kunming-911_n_4888748.html4http://arxiv.org/403ID Pairs1S1: The algorithm is studied in this paper .S2: The algorithm is proposed in this paper .2S1: ... circadian pattern and burstiness in human communication activity .S2: ... circadian pattern and burstiness in mobile phone communication .3S1: ... using minhash techniques , at a significantly lower cost and with same privacy guarantees .S2: ... using minhash techniques , with lower costs .4S1: the rows and columns of the covariate matrix then have certain physical meanings ...S2: the rows and columns of the covariate matrix could have different meanings ...5S1: they maximize the expected revenue of the seller but induce efficiency loss .S2: they maximize the expected revenue of the seller but are inefficient .Table 1: Examples of potential strength differences.2011, almost 40% (27.7K) have multiple versions.Many differences between these versions consti-tute a source of valid and motivated strength dif-ferences, as can be seen from the sentential revi-sions in Table 1.
Pair 1 makes the contributionseem more impressive by replacing ?studied?
with?proposed?.
Pair 2 downgrades ?human commu-nication activity?
to ?mobile phone communica-tion?.
Pair 3 removes ?significantly?
and the em-phasis on ?same privacy guarantees?.
Pair 4 showsan insertion of hedging, a relatively well-knowntype of strength reduction.
Pair 5 is an interestingcase that shows the complexity of this problem: onthe one hand, S2 claims that something is ?ineffi-cient?, which is an absolute statement, comparedto ?efficiency loss?
in S1, where the possibility ofefficiency still exists; on the other hand, S1 em-ploys an active tone that emphasizes a causal rela-tionship.The main contribution of this work is to providethe first large-scale corpus of sentence-level revi-sions for studying a broad range of variations instatement strength.
We collected labels for a sub-set of these revisions.
Given the possibility of allkinds of disagreement, the fair level of agreement(Fleiss?
Kappa) among our annotators was decent.But in some cases, the labels differed from our ex-pectations, indicating that the general public caninterpret the strength of scientific statements dif-ferently from researchers.
The participants?
com-ments may further shed light on science commu-nication and point to better ways to define and un-derstand strength differences.2 Related Work and DataHedging, which can lead to strength differences,has received some attention in the study of sciencecommunication (Salager-Meyer, 2011; Lewin,1998; Hyland, 1998; Myers, 1990).
The CoNLL2010 Shared Task was devoted to hedge detection(Farkas et al, 2010).
Hedge detection was alsoused to understand scientific framing in debatesover genetically-modified organisms in food (Choiet al, 2012).Revisions on Wikipedia have been shown use-ful for various applications, including spellingcorrection (Zesch, 2012), sentence compression(Yamangil and Nelken, 2008), text simplification(Yatskar et al, 2010), paraphrasing (Max and Wis-niewski, 2010), and textual entailment (Zanzottoand Pennacchiotti, 2010).
But none of the cat-egories of Wikipedia revisions previously exam-ined (Daxenberger and Gurevych, 2013; Bronnerand Monz, 2012; Mola-Velasco, 2011; Potthast etal., 2008; Daxenberger and Gurevych, 2012) re-late to statement strength.
After all, the objectiveof editing on Wikipedia is to present neutral andobjective articles.Public datasets of science communication areavailable, such as the ACL Anthology,5collec-tions of NIPS papers,6and so on.
These datasetsare useful for understanding the progress of disci-plines or the evolution of topics.
But the lack ofedit histories or revisions makes them not imme-diately suitable for studying strength differences.Recently, there have been experiments with openpeer review.7Records from open reviewing canprovide additional insights into the revision pro-cess once enough data is collected.5http://aclweb.org/anthology/6http://nips.djvuzone.org/txt.html7http://openreview.net404title abstract intro middle conclusion0.0100000.0200000.0300000.0400000.0500000.0600000.0700000.0800000.0900000.0numberofchanges57% 71% 65%58%62%deletiontyporewrite(a) Number of changes vs sections.?middle?
refers to the sections be-tween introduction and conclusion.math cond-mat astro-ph cs quant-ph0.0100000.0200000.0300000.0400000.0500000.0600000.0numberofchanges 57%61% 67% 56% 59%deletiontyporewrite(b) Top 5 categories in number ofchanges.stat q-bio q-fin cs quant-ph0.00.10.20.30.40.5numberofchangespersentence 54% 58% 58% 56% 59%deletiontyporewrite(c) Top 5 categories in number ofchanges over the number of sen-tences.Figure 1: In all figures, different colors indicate different types of changes.3 Dataset DescriptionOur main dataset was constructed from all paperssubmitted in 2011 on the arXiv.
We first extractedthe textual content from papers that have multipleversions of tex source files.
All mathematical en-vironments were ignored.
Section titles were notincluded in the final texts but are used in align-ment.In order to align the first version and the fi-nal version of the same paper, we first did macroalignment of paper sections based on section titles.Then, for micro alignment of sentences, we em-ployed a dynamic programming algorithm similarto that of Barzilay and Elhadad (2003).
Instead ofcosine similarity, we used an idf-weighted longest-common-subsequence algorithm to define the sim-ilarity between two sentences, because changes inword ordering can also be interesting.
Formally,the similarity score between sentence i and sen-tence j is defined asSimpi, jq ?Weighted-LCSpSi, Sjqmaxp?wPSiidfpwq,?wPSjidfpwqq,where Siand Sjrefer to sentence i and sentence j.Since it is likely that a new version adds or deletesa large sequence of sentences, we did not impose askip penalty.
We set the mismatch penalty to 0.1.8In the end, there are 23K papers where the firstversion was different from the last version.9We8We did not allow cross matching (i.e., i?
j?1, i?1?j), since we thought matching this case as pi ?
1, iq ?
j ori ?
pj, j ?
1q can provide context for annotation purposes.But in the end, we focused on labeling very similar pairs.This decision had little effect.9This differs from the number in Section 1 because arti-cles may not have the tex source available, or the differencesbetween versions may be in non-textual content.categorize sentential revisions into the followingthree types:?
Deletion: we cannot find a match in the finalversion.?
Typo: all sequences in a pair of matched sen-tences are typos, where a sequence-level typois one where the edit distance between thematched sequences is less than three.?
Rewrite: matched sentences that are not ty-pos.
This type is the focus of this study.What kinds of changes are being made?
Onemight initially think that typo fixes represent alarge proportion of revisions, but this is not cor-rect, as shown in Figure 1a.
Deletions represent asubstantial fraction, especially in the middle sec-tion of a paper.
But it is clear that the majority ofchanges are rewrites; thus revisions on the arXivindeed provide a great source for potential strengthdifferences.Who makes changes?
Figure 1b shows that theMath subarchive makes the largest number ofchanges.
This is consistent with the mathematicscommunity?s custom of using the arXiv to get find-ings out early.
In terms of changes per sentence(Figure 1c), statistics and quantitative studies arethe top subareas.Further, Figure 2 shows the effect of the numberof authors.
It is interesting that both in terms ofsheer number and percentage, single-authored pa-pers have the most changes.
This could be becausea single author enjoys greater freedom and hasstronger motivation to make changes, or becausemultiple authors tend to submit a more polishedinitial version.
This echoes the finding in Posner405You should mark S2 as Stronger if?
(R1) S2 strengthens the degree of some aspect of S1, for example, S1 has the word ?better?,whereas S2 uses ?best?, or S2 removes the word ?possibly??
(R2) S2 adds more evidence or justification (we don?t count adding details)?
(R3) S2 sounds more impressive in some other way: the authors?
work is more important/novel-/elegant/applicable/etc.If instead S1 is stronger than S2 according to the reasons above, select Weaker.
If the changesaren?t strengthenings or weakenings according to the reason above, select No Strength Change.If there are both strengthenings and weakenings, or you find that it is really hard to tell whether thechange is stronger or weaker, then select I can?t tell.Table 2: Definition of labels in our labeling tasks.1 2 3 4 5 >5number of authors46.048.050.052.054.056.058.060.062.064.0numberofchanges(a) Number of changes vsnumber of authors.1 2 3 4 5 >5number of authors26%27%28%29%30%percentageofchanges(b) Percentage of changedsentences vs number of au-thors.Figure 2: Error bars represent standard error.
(a):up until 5 authors, a larger number of authors in-dicates a smaller number of changes.
(b): per-centage is measured over the number of sentencesin the first version; there is an interior minimumwhere 2 or 3 authors make the smallest percentageof sentence changes on a paper.and Baecker (1992) that the collaborative writingprocess differs considerably from individual writ-ing.
Also, more than 25% of the first versions arechanged, which again shows that substantive editsare being made in these resubmissions.4 Annotating Strength DifferencesIn order to study statement strength, reliablestrength-difference labels are needed.
In this sec-tion, we describe how we tried to define strengthdifferences, compiled labeling instructions, andgathered labels using Amazon Mechanical Turk.Label definition and collection procedure.
Wefocused on matched sentences from abstractsand introductions to maximize the proportion ofstrength differences (as opposed to factual/nostrength changes).
We required pairs to have sim-ilarity score larger than 0.5 in our labeling task tomake pairs more comparable.
We also replacedall math environments with ?
[MATH]?.10We ob-tained 108K pairs that satisfy the above condi-tions, available at http://chenhaot.com/pages/statement-strength.html.
Tocreate the pool of pairs for labeling, we randomlysampled 1000 pairs and then removed pairs thatwe thought were processing errors.We used Amazon Mechanical Turk.
It mayinitially seem surprising to have annotations oftechnical statements not done by domain experts;we did this intentionally because it is common tocommunicate unfamiliar topics to the public in po-litical and science communication (we commenton non-expert rationales later).
We use the follow-ing set of labels: Stronger, Weaker, No StrengthChange, I can?t tell.
Table 2 gives our definitions.The instructions included 8 pairs as examples and10 pairs to label as a training exercise.
Partici-pants were then asked to choose labels and writemandatory comments for 50 pairs.
According tothe comments written by participants, we believethat they did the labeling in good faith.Quantitative overview.
We collected 9 labelseach for 500 pairs.
Among the 500 pairs, Fleiss?Kappa was 0.242, which indicates fair agreement(Landis and Koch, 1977).
We took a conserva-tive approach and only considered pairs with anabsolute majority label, i.e., at least 5 of 9 label-ers chose the same label.
There are 386 pairs thatsatisfy this requirement (93 weaker, 194 stronger,99 no change).
On this subset of pairs, Fleiss?Kappa is 0.322, and 74.4% of pairs were strengthchanges.
Considering all the possible disagree-ment, this result was acceptable.Qualitative observations.
We were excitedabout the labels from these participants: despite10These decisions were made based on the results and feed-back that we got from graduate students in an initial labeling.406ID Matched sentences and comments1S1: ... using data from numerics and experiments .S2: ... using data sets from numerics in the point particle limit and one experimental data set .
(stronger) S2 is more specific in its description which seems stronger.
(weaker) ?one experimental data set?
weakens the sentence2S1: we also proved that if [MATH] is sufficiently homogeneous then ...S2: we also proved that if [MATH] is not totally disconnected and sufficiently homogeneous then ...(stronger) We have more detail/proof in S2(stronger) the words ?not totally disconnected?
made the sentence sound more impressive.3S1: we also show in general that vectors of products of jack vertex operators form a basis of symmetric functions .S2: we also show in general that the images of products of jack vertex operators form a basis of symmetric functions .
(weaker) Vectors sounds more impressive than images(weaker) sentence one is more specific4S1: in the current paper we discover several variants of qd algorithms for quasiseparable matrices .S2: in the current paper we adapt several variants of qd algorithms to quasiseparable matrices .
(stronger) in S2 Adapt is stronger than just the word discover.
adapt implies more of a proactive measure.
(stronger) s2 sounds as if they?re doing something with specifics already, rather than hunting for a way to do itTable 3: Representative examples of surprising labels, together with selected labeler comments.the apparent difficulty of the task, we found thatmany labels for the 386 pairs were reasonable.However, in some cases, the labels were counter-intuitive.
Table 3 shows some representative ex-amples.First, participants tend to take details as evi-dence even when these details are not germane tothe statement.
For pair 1, while one turker pointedout the decline in number of experiments, mostturkers simply labeled it as stronger because it wasmore specific.
?Specific?
turned out to be a com-mon reason used in the comments, even though wesaid in the instructions that only additional justifi-cation and evidence matter.
This echoes the find-ing in Bell and Loftus (1989) that even unrelateddetails influenced judgments of guilt.Second, participants interpret constraints/condi-tions not in strictly logical ways, seeming to carelittle about scope at times.
For instance, the ma-jority labeled pair 2 as ?stronger?.
But in S2 forthat pair, the result holds for strictly fewer pos-sible worlds.
But it should be said that thereare cases that labelers interpreted logically, e.g.,?compelling evidence?
subsumes ?compelling ex-perimental evidence?.Both of the above cases share the property thatthey seem to be correlated with a tendency tojudge lengthier statements as stronger.
Anotherinteresting case that does not share this character-istic is that participants can have a different un-derstanding of domain-specific terms.
For pair 3,the majority thought that ?vectors?
sounds moreimpressive than ?images?
; for pair 4, the major-ity considered ?adapt?
stronger than ?discover?.This issue is common when communicating newtopics to the public not only in science commu-nication but also in politics and other scenarios.
Itmay partly explain miscommunications and misin-terpretations of scientific studies in journalism.115 Looking aheadOur observations regarding the annotation resultsraise questions regarding what is a generalizableway to define strength differences, how to use thelabels that we collected, and how to collect la-bels in the future.
We believe that this corpus ofsentence-level revisions, together with the labelsand comments from participants, can provide in-sights into better ways to approach this problemand help further understand strength of statements.One interesting direction that this enables is apotentially new kind of learning problem.
Thecomments indicate features that humans thinksalient.
Is it possible to automatically learn newfeatures from the comments?The ultimate goal of our study is to understandthe effects of statement strength on the public,which can lead to various applications in publiccommunication.AcknowledgmentsWe thank J. Baldridge, J. Boyd-Graber, C.Callison-Burch, and the reviewers for helpfulcomments; P. Ginsparg for providing data; and S.Chen, E. Kozyri, M. Lee, I. Lenz, M. Ott, J. Park,K.
Raman, M. Reitblatt, S. Roy, A. Sharma, R.Sipos, A. Swaminathan, L. Wang, W. Xie, B. Yangand the anonymous annotators for all their label-ing help.
This work was supported in part by NSFgrant IIS-0910664 and a Google Research Grant.11http://www.phdcomics.com/comics/archive.php?comicid=1174407ReferencesRegina Barzilay and Noemie Elhadad.
2003.
Sentencealignment for monolingual comparable corpora.
InProceedings of the 2003 conference on Empiricalmethods in natural language processing, pages 25?32.Brad E Bell and Elizabeth F Loftus.
1989.
Trivial per-suasion in the courtroom: The power of (a few) mi-nor details.
Journal of Personality and Social Psy-chology, 56(5):669.Amit Bronner and Christof Monz.
2012.
User EditsClassification Using Document Revision Histories.In Proceedings of the 13th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 356?366.Eunsol Choi, Chenhao Tan, Lillian Lee, CristianDanescu-Niculescu-Mizil, and Jennifer Spindel.2012.
Hedge detection as a lens on framing in theGMO debates: A position paper.
In Proceedingsof the Workshop on Extra-Propositional Aspects ofMeaning in Computational Linguistics, pages 70?79.Johannes Daxenberger and Iryna Gurevych.
2012.
ACorpus-Based Study of Edit Categories in Featuredand Non-Featured Wikipedia Articles.
In COLING,pages 711?726.Johannes Daxenberger and Iryna Gurevych.
2013.Automatically Classifying Edit Categories inWikipedia Revisions.
In Proceedings of the 2013Conference on Empirical Methods in NaturalLanguage Processing.Rich?ard Farkas, Veronika Vincze, Gy?orgy M?ora, J?anosCsirik, and Gy?orgy Szarvas.
2010.
The CoNLL-2010 shared task: Learning to detect hedges andtheir scope in natural language text.
In CoNLL?Shared Task, pages 1?12.Ken Hyland.
1998.
Hedging in scientific researcharticles.
John Benjamins Pub.
Co., Amsterdam;Philadelphia.Steven G. Krantz.
2007.
How to Write Your First Pa-per.
Notices of the AMS.J.
Richard Landis and Gary G. Koch.
1977.
TheMeasurement of Observer Agreement for Categor-ical Data.
Biometrics, 33(1):159?174.Beverly A. Lewin.
1998.
Hedging: Form and func-tion in scientific research texts.
In Genre Studiesin English for Academic Purposes, volume 9, pages89?108.
Universitat Jaume I.Aurlien Max and Guillaume Wisniewski.
2010.Mining Naturally-occurring Corrections and Para-phrases from Wikipedia?s Revision History.
In Pro-ceedings of The seventh international conference onLanguage Resources and Evaluation.Santiago M Mola-Velasco.
2011.
Wikipedia Vandal-ism Detection.
In Proceedings of the 20th Interna-tional Conference Companion on World Wide Web,pages 391?396.Greg Myers.
1990.
Writing biology: Texts in the socialconstruction of scientific knowledge.
University ofWisconsin Press, Madison, Wis.Ilona R Posner and Ronald M Baecker.
1992.
Howpeople write together [groupware].
In SystemSciences, 1992.
Proceedings of the Twenty-FifthHawaii International Conference on, pages 127?138.Martin Potthast, Benno Stein, and Robert Ger-ling.
2008.
Automatic Vandalism Detection inWikipedia.
In Advances in Information Retrieval,pages 663?668.
Springer Berlin Heidelberg.Franc?oise Salager-Meyer.
2011.
Scientific discourseand contrastive linguistics: hedging.
European Sci-ence Editing, 37(2):35?37.Elif Yamangil and Rani Nelken.
2008.
MiningWikipedia Revision Histories for Improving Sen-tence Compression.
In Proceedings of the 46thAnnual Meeting of the Association for Computa-tional Linguistics on Human Language Technolo-gies: Short Papers, pages 137?140.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of sim-plicity: Unsupervised extraction of lexical simplifi-cations from Wikipedia.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 365?368.Fabio Massimo Zanzotto and Marco Pennacchiotti.2010.
Expanding textual entailment corpora fromWikipedia using co-training.
In Proceedings of the2nd Workshop on Collaboratively Constructed Se-mantic Resources.Torsten Zesch.
2012.
Measuring Contextual FitnessUsing Error Contexts Extracted from the WikipediaRevision History.
In Proceedings of the 13th Con-ference of the European Chapter of the Associationfor Computational Linguistics, pages 529?538.408
