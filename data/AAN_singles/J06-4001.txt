ACL Lifetime Achievement AwardOld Linguists Never Die, They Only GetObligatorily Deleted?Eva Hajic?ova??
?Charles University1.
IntroductionMartin Kay, in his speech delivered in 2005 on receipt of his ACL Lifetime AchievementAward, specified computational linguistics as follows: ?Computational linguistics istrying to do what linguists do in a computational manner?
(Kay 2005, page 429).
Ibelieve it is a legitimate question for a computational linguist to ask what linguists do.Coming from Prague, it is then quite a natural question for me to look back and torecollect what the ?old?
linguists (who never die but get obligatorily deleted on thevisible surface) with the background of the world-famous Prague Linguistic School(PLS) contributed to linguistic studies and perhaps to suggest what aspects of theirheritage are even today fruitful for computational linguistics.First, to place the PLS in the course of the development of linguistic studies, itshould be recalled that the Prague Linguistic Circle belongs to the first bodies that tookpart in the transition of the older diachronic paradigm of linguistics to a synchronictheory of language.
Soon after its first session (taking place in 1926 in the office ofthe chairman of the Circle till his death in 1945), the Circle entered the internationalscene first of all with its systematically elaborated phonological theory.
Starting withthe Hague Linguistic Congress (see Actes 1928), Praguian phonology became the pilotdiscipline of structural linguistics.
This approach was far from unified, but the strengthof the Circle was in its spirit of dialogue, which kept the Circle receptive to new ideas,rather than in any set of postulates commonly professed.
In my talk I will concentrateon three fundamental Prague School tenets, which I believe to have their validity alsoin the modern context of linguistic theory and computational linguistics.
What I have inmind here is the Circle?s structural and functional orientation, as well as the attention ithas paid to the opposition of the center and the periphery of language structure, basedon the concept of markedness.2.
The Structural Point of View of PLSThe PLS is generally (and truly) characterized by two attributes: structural and func-tional.
Let us first turn to the structural point of view, namely, the School?s endeavorto view language as a system of systems rather than to study individual phenomena asad hoc, non-systematic issues.
The Circle shared de Saussure?s understanding of lan-guage as a system of (bilateral) signs, in which only oppositions rather than fixed?
Logo on the Indiana University Linguistic Club tee-shirt, 1984.??
Matematicko-fyzika?ln??
fakulta, Univerzita Karlova, Malostranske?
na?mest??
25, CZ-11800 Praha,Czech Republic.
This paper is the text of the talk given on receipt of the ACL?s LifetimeAchievement Award in 2006.?
2006 Association for Computational LinguisticsComputational Linguistics Volume 32, Number 4entities play a role.
As mentioned above, this attitude was most apparently reflectedin the study of phonology as a system displaying distinctive features and employingthe notion of binary oppositions.
Jakobson (1929) presented the phonological repertory(both in synchrony and in diachrony) as a system of oppositions (mainly binary, priva-tive), based on acoustic distinctive features and understood as the clue to the sound andmeaning relationship.Along with phonemes and morphemes, the sentence was also recognized as one ofthe fundamental fields of systematic oppositions, that is, as an ingredient of la langue.Mathesius (1928, 1936) formulated a concept of functional syntax; a structural view ofsyntax, based on the dependency relation, was elaborated by Tesnie`re (1934), a Frenchmember of the Circle, who was a professor of the Ljubljana University; his monographwas published only posthumously (Tesnie`re 1959), but his papers were known inPrague, and his approach to syntax was applied to Czech by S?milauer (1947), whocombined dependency syntax with a constituent-based view of the relation betweenpredicate and subject.Dependency-based approaches, which understand the verb as the center of thesentence structure and describe this structure on the basis of binary relations betweenheads and their modifiers, have been for a long time a matter of Continental syntactictheories rather than of the mainstream syntactic approaches on the other side of theAtlantic.
However, the notion of head can be found also in Bloomfield (1933) whenreferring to the names of the main constituents of the sentence, that is, NP (noun phrase,with N as its head) and VP (verb phrase, with V as its head).
In the framework ofthe Chomskyan approach, originally based exclusively on the concept of immediateconstituents, the notion of head becomes the basic notion of X-bar theory.
Originally,four categories were singled out as possible heads of their respective maximal pro-jections, namely, N, V, Adj, and P(rep); as remarked by James McCawley (personalcommunication, around 1990), such a theory may be interesting unless the specificationof the set of basic categories grows beyond some reasonable limit.
McCawley?s criticalremark reflected the gradual development of X-bar theory, which allowed practicallyany constituent (or, more generally speaking, any arbitrary symbol for a grammati-cal value) to act as the head, dependent on the needs of the analysis of this or thatconstruction.1The very name head-driven phrase structure grammar, an influential theory combiningan immediate constituent approach with elements of a dependency-based approach,as proposed by Pollard and Sag (1994), explicitly points out that the theory takes ac-count of the main element within a constituent.
Although their approach is constituentbased (working with a lexically based X-bar syntactic theory; [Pollard and Sag 1994,page 362]), the authors are aware that the notion of constituent structure is widespreadbut that it is not based on sufficiently convincing direct evidence.
The authors refer toHudson?s (1984) approach and claim that it belongs to exceptions that do not overesti-mate the constituent structuring of sentence elements.It is sometimes doubted if the direction of the dependency relation, namely, thedetermination of the governor and the dependent in each pair (syntagm) can be reliablystated.
We believe that in the prototypical case, the main criterion for this distinction1 To be fair, I should add that one of the rare attempts at a more explicit characterization of the notionof head can be found in Adger?s monograph on minimalism (Adger 2003, page 75), which mentionstwo criteria: one based on the distribution of the whole constituent and the other taking into accountwhich constituent determines the reference of the whole constituent.458Hajic?ova?
Old Linguists Never Diecan be based on the possibility that, in the endocentric constructions, the dependentcan be absent (not just deleted on the surface).
Thus, for example, in Yesterday, my fatherworked for the whole day in the garden it is possible to leave out the dependents yesterday,my, for the whole day, and in the garden without the sentence losing its grammaticality.However, there are exocentric pairs such as (to) find something, where neither of the twomembers of the pair can be deleted and for which, therefore, the mentioned methodby itself cannot help to find out which element is the governor and which is thedependent.
What helps here is the principle of analogy on the level of parts of speech:On the basis of the existence of verbs without objects it can be concluded that in suchpairs the verb is the governor (in our particular example, (to) find).
In the same vein,subject (Actor) can be understood as dependent on the verb, because there are verbsalso without a subject: In It is raining (Latin Pluit), It is just a surface ?filler,?
absentin the sentence structure proper.
This view is also supported by the observation basedon the annotators?
agreement when assigning the dependency structures (trees) in thePrague Dependency Treebank: The annotators did not have too many troubles with thedetermination of the direction of dependency; if there was a disagreement, it concernedtheir assignment of the type (value) of the dependency relation (see Hajic?ova?, Pajas, andVesela?
2002).There is one linguistic phenomenon?present more or less in every language?allsyntactic theories have to bother about, namely, the relation between syntactic struc-ture and word order (the discontinuity of constituents, for which Gazdar [1981] intro-duced the term unbounded dependencies, used, for example, also by Pollard and Sag [1994,pages 157ff.
]; see also the term long-distance dependencies used by some other authors) or,in terms of formal dependency descriptions, the non-projectivity of syntactic construc-tions.
Informally speaking, the strongly restrictive condition of projectivity says that if anode a depends on b and there is a node c between a and b in the linear ordering, c issubordinated to b (where subordinated means an irreflexive transitive closure of depends).See Figure 1 for an example of non-projective parts of a tree; the vertical line leadingfrom a intersects the dependency edge leading from a to c.2The more restricted the formal syntactic description is, the more valuable are theobservations based on it; in this sense, the condition of projectivity might well serveits purpose.
However, there are seemingly many non-projective constructions in thesurface shapes of the sentences.
The task then is to attempt to classify the constructionsin which the condition of projectivity is not met in the surface shape of the sentenceand to attempt at a description not only meeting the condition as far as the core of thelanguage system (see Section 4) is concerned, but also accounting by some simple, well-defined means for the cases of superficial non-projectivity (a preliminary formulationof movement rules specified as a transition from projective underlying trees to stringsof morphemes in which the condition of projectivity cannot be applied can be found inSgall [1997] and Hajic?ova?
and Sgall [2003]).
This is, of course, a rather strong hypothesisthat has to be verified and made more precise on the basis of systematic empiricalresearch.
It should be mentioned in this connection that it is in line with the Praguianapproach that function words are distinguished from autosemantic words and that onlythe latter constitute nodes of their own in the underlying trees; from this it follows thatin the numerous cases in which the ?non-projectivity?
of surface word order concerns2 Projectivity as a property of word order important for a formal description of language was alreadystated by Hays (1960, 1964) and Lecerf (1960) in formal grammar; the condition of projectivity (indifferent forms that have been proved as equivalent) was defined by Marcus (1965) and used in manywritings working with dependency descriptions.459Computational Linguistics Volume 32, Number 4Figure 1Examples of non-projective parts of a dependency tree.auxiliary verbs or conjunctions, and so forth, the projectivity of underlying syntacticstructure is not at stake.The introduction of the notion of a head brings into the foreground the connection ofgrammar and lexicon; the necessity of such a relationship was already quite apparent inthe earlier works of Fillmore (1966, 1968) introducing the so-called case grammar, whichexplicitly follows up Tesnie`re?s notion of valency.
The term ?case?
does not directly referto case as a morphological category but to the meaning (function) of a (morphological)case: for example, Addressee is a prototypical meaning (function) of Dative, Agentiveis a prototypical meaning of Nominative, and so on.
The concept of valency is crucialin that it reflects the fundamental aspect of the presence of grammatical informationin the lexicon: The valency frame is a part of the lexical entry, in which the obligatoryand optional syntactic kinds of dependents of the given (head) word are registered.It is also well known that Fillmore?s theory (as well as the thematic roles of Gruber[1967]) played a substantial role in the introduction of theta roles (and theta grids) inthe Chomskyan model of government and binding (later called, more appropriately, thePrinciples and Parameters model).Fillmore himself explicitly mentioned that, when proposing his case grammar,he did not primarily consider which formal description his approach would fit into.However, he presents an example of how his approach can be formulated in terms of aphrase structure model: The sentence S can be decomposed into two parts, Modality andProposition; the Proposition in turn can be articulated into the verb and a set of nounphrases, which are characterized by one of the case markers, that is, K1NP, K2NP, .
.
.
,KnNP.
Each of these noun phrases can then be decomposed into the noun phrase properand the given case marker ki (Agentive, Addressee, Objective, etc.).
The analysis ofRobinson (1969, 1970) devoted to the relation between Fillmore?s approach and thatof transformational grammar (of that time) throws an interesting light on the possiblityof a smooth transition from a phrase-based approach to a dependency-based one, whichis more transparent and economical.
In Fillmore?s proposal, the case relations, that is,the relations of the noun phrase to the verb, are actually captured twice, once as themarker ki and once as the characteristics of the given phrase (KiNP).
It is then possibleto work with a pure dependency tree structure, where the root of the tree is the verb,and the nouns (or, as the case may be, other word categories) depend on the root asdependents with a certain type of relation.It goes without saying that Fillmore?s case grammar and its follow-up frame netsconception has influenced in a substantial way many of the contemporary approachesnot only to treebank annotation and computational lexicology (cf., e.g., Fillmore et al2003) but also the work on underlying sentence structure in general.Two ?historically?
motivated and seemingly contradictory observations are in placehere: It can be documented by references to the development of linguistic theory in the460Hajic?ova?
Old Linguists Never Diepast 50 years that the deeper the analysis goes, the more the need of an introduction ofthe distinction between the notions of ?head?
and ?modifier?
(predicate, argument) isfelt.
Let us only recall here such approaches as:(i) the lexicosemantic analysis by Katz and Postal (1964), who work with thenotions of head and modifier when specifying selection restrictions;(ii) the distinction between surface-oriented constituent structure and the(underlying) functional structure in lexical functional grammar by Bresnan(1978) and Kaplan and Bresnan (1982);(iii) the above-mentioned case grammar by Fillmore, motivated by theconviction that Chomskyan deep structure (with its specification of ?deep?subject as a constituent of S regardless of the (different) semantic relationsof the given NP to the verb) is not deep enough to capture the realunderlying structure of the sentence; and(iv) the consecutive introduction of theta roles into the government andbinding theory.3On the other hand, dependency-based considerations have gradually and evasivelypenetrated to the ?data?-oriented statistical methods and treebank annotations.
As thefreshest example, let us only refer to the recent EACL 2006 conference in Trento and theHLT-NAACL 2006 conference in New York with its CoNLL-X Shared Task on Multilin-gual Dependency Parsing (working with treebanks of 12 languages, of different sizes).In other words, the seemingly surface oriented analysis is prevailingly dependencybased.4A possible explanation for this apparent contradiction may be looked for in theeconomy and transparency of the dependency-based trees: In their applications, thedata-oriented systems also aim at a representation of the meaning of the surface shapesof sentences (whatever one can understand by ?meaning?
), so that their attention isfocused on a most transparent and economic way (avoiding ?extra?
nodes for phrasessuch as NPs, VPs, .
.
.
, etc.)
from the surface to the depth.
Dependency analysis offerssuch a way.53 We have restricted our attention here only to systems staying in principle within the developmentof the Chomskyan paradigm or originating as a reaction to it.
However, when discussing the relationbetween or combination of constituent-based and dependency-based grammars, special attentionshould be paid to the lexicalized tree-adjoining grammars (LTAG) continuing the original conceptionof tree-adjoining grammar (TAG) as proposed by A. K. Joshi (see, e.g., Joshi 1985), which has servedas a basis for many studies on formal grammar as well as from the NLP domain.
The similaritybetween LTAG and a dependency-based description in relation to the model using the so-calledsupertags (which encode syntactic information in terms of dependency) is analyzed by Joshi andSrinivas (1994).4 It should be recalled in this connection that within machine translation dependency-based systems(sometimes in combination with phrase structure) were already at play in the early times of MT;see, for example, the works of B. Vauquois, one of the founders of computational linguistics (Vauquois1975; Vauquois and Chappuy 1985) and the systems developed in Japan under the influence andguidance (direct or indirect) of M. Nagao (see the survey in Nagao [1989]).5 In a similar vein, Steedman (2005) argues that the use of statistical language models is the onlyway to create a computer program that automatically analyzes sentences on the basis of broadlyconceived grammars (with due regard to ambiguities) such as dependency-based grammars orgrammars specifying heads (governors); according to Steedman, these grammars work well becausethey reflect a mixture of semantic information and information based on knowledge of the world.461Computational Linguistics Volume 32, Number 4Among urgent questions to be asked with regard to the approaches to sentencestructure, there are then the following issues:(i) Is it more appropriate to analyze a sentence such as In this garden, she wasreading a book on the history of Spain yesterday as having the complex verbform was reading as its head, with she, (a) book, and garden as its dependents,or to see the basic characteristics of its structure in distinguishing whetherin this garden or yesterday is more immediately connected with its verb?
(ii) Do we have clearer criteria for answering the former or the latter of thesetwo questions?3.
Prague School FunctionalismThe other attribute of Prague structuralism is functional.
Mathesius (1928, 1936), in-spired especially by the philosophy of language of Marty (1908), presented his theory offunctional grammar, based on the concept of function as related to universal intentionalacts and treated as a dichotomy of functional onomatology and functional syntax.Mathesius combined this universal dichotomy with the language-specific oppositionof function and form.
As Sgall (1987) pointed out, the core of the system of languagewas conceived of as consisting of levels, the units of which have their functions in thatthey represent or express units of the adjacent higher levels, up to the non-linguisticlayer of cognitive content.
The units of the system were understood as constitutinghierarchies in which some of them function as certain parts of the others.
Thus, forexample, phonemes were defined and delimited one against the other on a functionalbasis (two different phonemes can distinguish two morphemes), and the establishedrepertory of distinctive features gave a firm foundation to the description of the systemof phonemes as a structured whole.
Strings of phonemes (morphs, in more modernterminology) are understood as expressing morphemes, and sequences of morphemesas expressing sentence structure.Another important aspect of the functional approach is to view language as afunctioning system, adapted to its communicative role, diversified in more or lessdifferent social and local varieties, and to describe the sentence structure as adaptedto its functioning in discourse.This leads me to pay attention to the information structure (in our terms, topic?focus articulation) of the sentence.
Let me first look again at the history of the issue.
Itwas the Prague scholar Mathesius (1929, 1938) who introduced the study of informationstructure into structural linguistics, preferring the terms Thema and Rhema (used beforein German linguistics by H. Ammann) to the older psychologisches Subjekt and Pra?dikat(used by G. von der Gabelentz, H. Paul, and others), and understanding the former(the topic) as one of the functions of the subject in English.
He distinguished topicproper, comment (focus) proper, and the accompanying elements of either of thesetwo parts.
Later on, one of Mathesius?
followers, Jan Firbas, extended the hierarchicalunderstanding of the information structure of the sentence by postulating a scale ofcommunicative dynamism.
The Praguian concepts met a favorable response withincontinental linguistics (one should mention in this connection especially the works byBritish linguists M. A. K. Halliday and H. W. Kirkwood, several German linguists suchas J. Esser, R. Bartsch, and J. Jacobs, the French linguist J.-M. Zemb, and others).
How-ever, only the syntactic or word order consequences (or, as the case may be, conditions)462Hajic?ova?
Old Linguists Never Dieof different sentence articulations into topic and focus were mostly taken into account,and its relevance for and effects on the coherence of discourse.A new impetus into the study of information structure was given by Petr Sgall,who was the first to come up with examples testifying to the semantic effectsof this issue and claiming that two utterance tokens differing in their topic?focusarticulation are tokens of two different sentences, that is, that topic and focus belongto the language system rather than only to the use of language in communication (Sgall1967, page 205ff.).
As a matter of fact, the split of transformational grammar into thegenerative and the interpretative semantics wings coming out at the same time operatedwith arguments based on sentences that in Praguian terms differ only in their topic?focus structure (this fact, of course, not being recognized by the authors): See Lakoff?s(1969) examples: Many men read few books against Few books are read by many men, Johntalked about many problems to few girls versus John talked to few girls about many problems.To be fair to the other side of the dispute, Chomsky (1965, page 224) noticed the semanticdifference between the sentences Everybody in the room knows at least two languages andAt least two languages are known by everybody in the room and was in doubt as to whetherthis difference should be ascribed to the difference between active and passive; heremarks that such a distinction might be described in terms of topic (as Lakoff [1969]notes, in this consideration, the influence of Halliday [1967?1968] played its role).
Inhis reaction to the generative semanticists?
criticism of the ?shallowness?
of his deepstructure, Chomsky (1968) was even more inclined to use notions related to what inpresent-day terms would be called the information structure of the sentence; he claimsthat in the semantic interpretation of the sentence, one should take into account thedistinction between what he calls presupposition and focus, and a related notion ofthe range of permissible focus.
There are two interesting points in his approach: First,Chomsky connects these syntactic issues with the placement of the intonation centerin the spoken form of the sentence, and second, he connects the possible operationalcriterion for the determination of the choice of focus from the range of permissible focuswith the scope of negation.
In particular, to decide what is the focus of the answer to Wasit an ex-convict with a red SHIRT that he was warned to look out for?, one should considerpossible different negative continuations such as No, he was warned to look out for anAUTOMOBILE SALESMAN, or .
.
.
for an ex-convict wearing DUNGARIES, .
.
.
for an ex-convict with a CARNATION, .
.
.
for an ex-convict with a red TIE.One could argue that it is the presence of structures with quantification rather thanthe topic?focus articulation of the quoted examples that is responsible for the indicatedsemantic distinction.
However, the Praguian writings from the sixties convincinglydemonstrate that it is not difficult to find sentences without quantification that exhibitthe same phenomenon (for reasons I will mention in a minute, in the examples, thecapitals indicate the intonation center): Russian is spoken in SIBERIA versus In Siberia,RUSSIAN is spoken, or John works on his dissertation on WEEKENDS versus On weekends,John works on his DISSERTATION.
In Russian linguistics, such examples have beendiscussed as Kurit?
ZDES?
versus Zdes?
KURIT?.
The sentences quoted also documentthat the difference cannot be ascribed to the active/passive distinction; neither can itbe claimed that the word order always plays a decisive role: Consider Halliday?s (1970)famous example from a London underground station: Dogs must be CARRIED.
With thesame word order, but with a change in the placement of the intonation center one getsa certainly unwanted interpretation: DOGS must be carried would imply that everybodystepping on the escalator has to carry a dog (in a similar vein as Carry DOGS.).
Aplausible explanation of the semantic difference covering all these cases is to describethem in terms of difference in their information structure.463Computational Linguistics Volume 32, Number 4This had not been recognized or at least commonly accepted for some time on aninternational scale until the appearance of Mats Rooth?s Ph.D. dissertation in 1984.6 Al-though restricted to prosodic focus (pointing out that the difference in truth conditionsbetween such sentences as Mary only introduced BILL to Sue and Mary only introducedBill to SUE is only in the location of focus, denoted here by capitals), Rooth?s workwas an impetus for an increasing interest in the related issues, first in the domain offormal semantics (here the influential role of Barbara H. Partee should be emphasized),but soon literally everywhere.
Let us mention in this context that semantic consid-erations apparently stood behind the conception of combinatory categorial grammarfirst proposed by Steedman (1996, 2000); his introduction of floating constituents, thedivision line between which is given by the articulation of the sentence with regard to itsinformation structure rather than fixed, determined once for all.
Steedman, in contrast tomany other researchers presently working in the domain of information structure, paysa due respect to the close relation between information structure, syntactic sentencestructure, and prosody; in this respect, also the work on corpus annotation led by himis a pioneering enterprise (Calhoun et al 2005).7Due respect paid to the description of the information structure of the sentence isalso crucial for the study of discourse structure and coherence.
It might be interestingto note in this connection that the first systematic study indicating such a relation?although in terms influenced by the then prevailing psychological view of language?is Weil?s (1844) study on the order of words.
The author introduces the notions ofprogression of ?ideas,?
distinguishing ?progression?
and marche paralle`le: In the formersequence (segment), the given sentence B is connected to the preceding sentence A bystarting with a reference to the idea that was at the end of A, whereas in the latter, thesentences ?march in parallel,?
that is, they begin with a reference to the same idea.
Itis not difficult to see an analogy between this view and a more modern and explicittreatment of the so-called centering theory (Grosz, Joshi, and Weinstein 1995) and itsshifts of centers.I am dwelling at such length on the issues of information structure not just becauseit is my favorite child (and, indeed, it is), but because I am fully convinced about theimportance of this issue for an adequate description of the sentence structure, bothin formal description of language as well as in natural language processing.
Let meillustrate by a personal recollection that I am not beating a straw man.
Some timeago (if I am not mistaken, it was in 1989) I was invited for an IBM-organized MTconference in Garmisch-Partenkirchen to deliver a talk on the Praguian approach toMT.
Naturally enough, I devoted most of my time to illustrate examples of translationsfrom and to several European languages that topic?focus articulation as an importantaspect the translation (be it human or automatic) has to take into account.
The group of6 From a different perspective, the term focus was used by Grosz (1977).
The author adopted apsychological point of view of focus of attention and considered the sentence focus to be that item that isin such a focus, that is, in our terms the topic of the sentence (what the sentence is about).
Grosz?
approachhas found its continuation in the centering theory mentioned below.7 Let us note in this connection that the difficulties of a syntactic description based on phrase structure foran adequate capturing of the topic?focus articulation were pointed out already in Sgall, Hajic?ova?, andBenes?ova?
(1973, page 163ff.)
and in Hajic?ova?
and Sgall (1975) and illustrated on examples such as Thisyear we will spend two weeks on Mallorca used in the context of How will you spend your holidays this year?, i.e.with the focus part of the sentence being two weeks on Mallorca.
Working with phrase structure, it wouldbe very difficult to characterize the two groups as a single phrase; in a similar vein, to determine thetopic of the sentence as a single phrase is also difficult: if the question were Where do you spend two holidayweeks this year?, the focus of the answer would be on Mallorca, with the topic being this year we will spendtwo weeks.464Hajic?ova?
Old Linguists Never Diepeople attending the meeting was extremely nice and friendly, and it was no wonderthat the program chairs, Margaret King and Jonathan Slocum, could dare to make theconcluding session a sort of fun ascribing to each of the papers some characteristicevaluative attribute: It was quite symptomatic of those times that the issues discussedin my paper were characterized as ?least important for MT?
(it may be of interest torecall that Mercer?s paper on the IBM statistical approach to MT delivered there wasevaluated as ?crazy science fiction?
).The question should then be discussed in which way is it possible to describe theinterplay of the dependency (or constituency)-based patterning of the sentence and thetopic?focus articulation (or information strcuture) of the sentence as two basic aspectsof syntax (now cf.
Hajic?ova?
and Sgall, in press).
Is it true that a dependency-based viewof the underlying structure as the core of the language system (in which there are nonodes corresponding to function words and the left-to-right order of lexical items meetsthe condition of projectivity; cf.
Section 2 above) might be useful in this respect?I am happy to see that much has changed in this particular domain of studies sincethose times; I cannot say I welcome all the changes, but it is encouraging to see that thetwo Praguian tenets I have discussed so far?namely, the dependency approach and thedue regard to the information structure?have found an undisputable appraisal withinour field.4.
The Core of Language and the PeripheryThe third Praguian notion I would like to mention is the distinction made between thecenter (core) of language and the periphery.
This distinction is closely connected withthe notion of markedness; markedness, characterizing the intrinsic asymmetry of binary(and other) oppositions (not only in phonology, but also in morphology, in semiotics,and in many other domains), was first systematically presented by R. Jakobson.
Itwas properly understood and used as an organizing principle of sign systems, alsoin connection with language universals and language acquisition.
As Battistella (1995)notes, this notion belongs to those aspects of the Prague linguistic theory that in someform have been taken over by Chomsky, who applied it, albeit in a different shape, inhis Principles and Parameters theory, as proposed in the early 1980s.Although the relationships between the two oppositions of marked versus un-marked phenomena and the core versus the periphery of the system of language arefar from straightforward (see Sgall 2002, 2004), it can be claimed that because languageis more stable in its core, regularities in language should be searched for first in this core;only then is it possible to penetrate into the subtleties and irregularities of the periphery.The relatively simple pattern of the core of language (in Sgall?s view, not far from thetransparent pattern of propositional calculus) makes it possible for children to learnthe regularities of their mother tongue on the basis of shared human mental capacities,instantiated also by systems such as those of elementary arithmetic or Aristotelian logic.The freedom of linguistic behavior, limited only by the speakers?
desire to be understoodby their audience, offers space for the flexibility of the large and complex periphery (i.e.,not only of individual exceptions, but also by most different sets of marked phenomenadetermined by contextual conditions and lists).The question to be asked then is which of the two possible approaches to howto project this view to a formal description of language is to be preferred: to attemptto describe all phenomena ?at once,?
that is, to consider language as a whole and todescribe all phenomena ?at a single layer,?
or to proceed from the core of the system toits periphery.465Computational Linguistics Volume 32, Number 45.
Corpus Annotation as a Test of Linguistic TheoriesAt the beginning of my talk, I promised to suggest which aspects of Praguian heritage(and in a more general view, of linguistics as such) I believe to have been fruitfulfor computational linguistics.
When talking about the three particular aspects I havechosen, I have tried to make some suggestions as to urgent questions to be asked.
Let mefinish my talk by an illustration taken from the presently flourishing field of languageresources, corpus annotation, and evaluation.It has been already commonly accepted in computational and corpus linguistics thatgrammatical (or lexical semantic, etc.)
annotation does not ?spoil?
a corpus, because theannotation is done ?in addition?
to the raw corpus.
Thus, on the contrary, annotationmay and should bring an additional value to the corpus.
However, there are somenecessary conditions for an annotation to fulfil this aim: Its scenario should be carefully(i.e., systematically and consistently) designed, and it should be based on a soundlinguistic theory.
This view is corroborated by the existence of annotated corpora ofvarious languages (even if their creation is mostly done manually but supported byannotator-friendly software tools or semiautomatic procedures): the Penn Treebank, itssuccessors as PropBank or the Penn Discourse Treebank, Tiger, the Prague DependencyTreebank, and several others.
These conditions being met, corpus annotation serves,among other things, as an invaluable test for the linguistic theories standing behindthe annotation schemes, and as such represents an irreplaceable resource of linguis-tic information for the construction and enrichment of grammars, both formal andtheoretical.This claim can be documented by the case of the multilayered annotation of thePrague Dependency Treebank (PDT; see, e.g., Hajic?
1998), which is based on the frame-work of the Functional Generative Description (see, e.g., Sgall, Hajic?ova?, and Panevova?1986).
It is important to note that the PDT annotation concerns not only the surfaceand morphemic shape of sentences, but also (and first of all) the underlying sentencestructure (tectogrammatical layer), which elucidates phenomena hidden on the sur-face although unavoidable for the representation of the meaning and functioning ofthe sentence, for modeling its comprehension and studying its semantico-pragmaticinterpretation, for the work on lexical semantics, and for dictionary buildup and manyother aims.We have tried to demonstrate on certain selected grammatical and discourse phe-nomena (in Hajic?ova?, Sgall 2006) that the process of the annotation during the lastdecade and its results have allowed for an enrichment of this framework in severalregards.
In particular, our examples were taken from the domain of the condition ofprojectivity, classification of dependency relations, topic?focus articulation (the biparti-tion of the sentence into topic and focus and the cannonical underlying word order inthe focus of the sentence), and some aspects of discourse structure.6.
Final RemarksI have always been an optimist, and therefore let me go back to the Indiana Univer-sity Linguistic Club logo from 1984 quoted in the title of my talk: I strongly believethat old linguists never die, they only get obligatorily deleted.
Deletions concern thesurface rather than the underlying structure so that we may hope that while the oldlinguists?
bodies may lie a-moldering in their graves, the best of their ideas will bemarching on.466Hajic?ova?
Old Linguists Never DieAcknowledgmentsI would like to express my most sincerethanks to the ACL for the Award and thusfor having given me the opportunity to paytribute in this talk to my Praguian teachers.
Iwould also like to express my deep gratitudeto my mentor and colleague, Petr Sgall, thefounder of Czech computational linguistics,whose original ideas as well as broad scopeof knowledge and vision have made itpossible for the Prague School linguisticideas to cross over the boundaries of time, ofgeographic zones, and of linguistic trendsand orientation.
Last but not least, I wouldlike to pay credit to my younger colleaguesand students, the energy, commitment, andfriendliness of whom makes me not to thinkof age and to enjoy my professional life.ReferencesActes du Premier Congre`s international deslinguistes a` la Haye.
1928.
A. W. Sijthoff,Leiden.Adger, David.
2003.
Core Syntax.
A MinimalistApproach.
Oxford University Press, Oxford.Battistella, Edwin.
1995.
Jakobson andChomsky on markedness.
In E. Hajic?ova?,M.
C?ervenka, O. Les?ka, and P. Sgall,editors, Prague Linguistic Circle Papers 1.John Benjamins, Amsterdam, pages 55?72.Bloomfield, Leonard.
1933.
Language.
Holt,Rinehart and Winston, New York.Bresnan, Joan.
1978.
A realistictransformational grammar.
In M. Halleet al, editor, Linguistic Theory andPsychological Reality.
MIT Press,Cambridge, MA, pages 1?59.Calhoun, Sasha, Malvina Nissim, MarkSteedman, and Jason Brenier.
2005.
Aframework for annotating informationstructure in discourse.
In A. Meyers,editor, Pie in the Sky.
Proceedings of theACL Workshop 2005.
Ann Arbor, MI,pages 45?52.Chomsky, Noam.
1965.
Aspects of the Theory ofSyntax.
MIT, Cambridge, MA.Chomsky, Noam.
1968.
Deep structure,surface structure and semanticinterpretation.
In D. D. Steinberg andL.
A. Jakobovits, editors, Semantics: AnInterdisciplinary Reader in Philosophy,Linguistics and Psychology.
CambridgeUniversity Press, Cambridge,pages 183?216.Fillmore, Charles J.
1966.
Toward a moderntheory of case.
In D. A. Reibel and S. A.Schane, editors, Modern Studies in English.Prentice-Hall, Englewood Cliffs, NJ,pages 361?375.Fillmore, Charles J.
1968.
The case for case.
InE.
Bach and R. Harms, editors, Universalsin Linguistic Theory.
Holt, Rinehart, andWinston, New York, pages 1?90.Gazdar, Gerald.
1981.
Unboundeddependencies and coordinate structure.Linguistic Inquiry, 12:155?184.Grosz, Barbara J.
1977.
The Representation andUse of Focus in Dialogue Understanding.Ph.D.
thesis, University of California,Berkeley, CA.Grosz, Barbara J., Aravind K. Joshi, and ScottWeinstein.
1995.
Centering: A frameworkfor modeling the local coherence ofdiscourse.
Computational Linguistics,21(2):203?225.Gruber, Jeffrey S. 1967.
Functions of thelexicon in formal descriptive grammar.Technical Report (TM)-3770/00, SystemsDevelopment Corporation, Santa Monica.Hajic?, Jan. 1998.
Building a syntacticallyannotated corpus: The PragueDependency Treebank.
In E. Hajic?ova?,editor, Issues of Valency and Meaning.Studies in Honour of Jarmila Panevova?.Karolinum, Prague, pages 106?132.Hajic?ova?, Eva, Petr Pajas, and Kater?inaVesela?.
2002.
Corpus annotation on thetectogrammatical layer: Summarizing ofthe first stages of evaluation.
The PragueBulletin of Mathematical Linguistics,77:5?18.Hajic?ova?, Eva and Petr Sgall.
1975.
Topic andfocus in transformational grammar.
Papersin Linguistics, 8(1?2):13?58.Hajic?ova?, Eva and Petr Sgall.
2003.Dependency syntax in functionalgenerative description.
In Vilmos Agelet al, editors, Dependenz und Valenz, Vol.
1.Walter de Gruyter, Berlin, pages 570?592.Hajic?ova?, Eva and Petr Sgall.
2006.
Corpusannotation as a test of a linguistic theory.In Proceedings of LREC 2006, Genoa.Hajic?ova?, Eva and Petr Sgall.
Forthcoming.The fundamental significance ofinformation structure.
In C. Caffi andH.
Haberland et al, editors, FutureProspects of Pragmatics.Halliday, Michael A. K. 1967?1968.
Noteson transitivity and theme in English.Journal of Linguistics, 3:37?81, 199?244;4:179?215.Halliday, Michael A. K. 1970.
A Course inSpoken English: Intonation.
OxfordUniversity Press, Oxford.Hays, David G. 1960.
Grouping anddependency theories.
In Proceedings of the467Computational Linguistics Volume 32, Number 4National Symposium on Machine Translation,pages 258?266, Englewood Cliffs, NJ.Hays, David G. 1964.
Dependency theory:A formalism and some observations.Language, 40:511?525.Hudson, Richard.
1984.
Word Grammar.Blackwell, Oxford.Jakobson, Roman.
1929.
Remarques surl?e?volution phonologique du russecompare?e a?
celle des autres langues slaves.Travaux du Cercle Linguistique de Prague,volume 2.Joshi, Aravind.
1985.
Tree-adjoininggrammars: How much context-sensitivityis required to provide reasonablestructural descriptions?
In D. Dowty,editor, Natural Language Processing.Cambridge University Press, Cambridge,pages 206?250.Joshi, Aravind and Bangalore Srinivas.1994.
Disambiguation of super parts ofspeech (or supertags): Almost parsing.In Proceedings of the 15th InternationalConference on Computational Linguistics(COLING 1994), pages 154?160,Kyoto, Japan.Kaplan, Ronald and Joan Bresnan.
1982.Lexical-Functional Grammar: A formalsystem for grammatical representation.In Joan Bresnan, editor, The MentalRepresentation of GrammaticalRelations.
MIT Press, Cambridge, MA,pages 173?281.Katz, Jerrold J. and Paul M. Postal.
1964.
AnIntegrated Theory of Linguistic Descriptions.MIT Press, Cambridge, MA.Kay, Martin.
2005.
A life of language.Computational Linguistics, 31(4):425?438.Lakoff, George.
1969.
On generativesemantics.
In D. D. Steinberg and L. A.Jakobovits, editors, Semantics: AnInterdisciplinary Reader in Philosophy,Linguistics and Pyschology.
CambridgeUniversity Press, Cambridge,pages 232?296.Lecerf, Yves.
1960.
Programme des conflits,mode`le des conflits.
TraductionAutomatique, 1(4):11?18; 1(5):17?36.Marcus, Solomon.
1965.
Sur la notion deprojectivite?.
Zeitschrift fu?r mathematischeLogik und Grundlagen der Mathematik,11:181?192.Marty, Anton.
1908.
Untersuchungen zurGrundlegung der allgemeinen Grammatik undSprachphilosophie 1.
Halle/S.Mathesius, Vile?m.
1928.
On linguisticcharacterology.
Actes, pages 56?63.Mathesius, Vile?m.
1929.
Zur satzperspektiveim modernen Englisch.
Archiv fu?r dasStudium der neueren Sprachen undLiteraturen, 155:202?210.Mathesius, Vile?m.
1936.
On some problemsof the systematic analysis of grammar.Travaux du Cercle Linguistique de Prague,volume 6, pages 95?107.Nagao, Makoto.
1989.
Machine Translation:How Far Can It Go?
Oxford UniversityPress, Oxford.Pollard, Carl and Ivan A.
Sag.
1994.Head-driven Phrase Structure Grammar.University of Chicago Press, Chicago andLondon.Robinson, Jane J.
1969.
Case, category andconfiguration.
Journal of Linguistics,6:57?80.Robinson, Jane J.
1970.
Dependencystructures and transformational rules.Language, 46:259?285.Sgall, Petr.
1967.
Functional sentenceperspective in a generative description.Prague Studies in Mathematical Linguistics,2:203?225.Sgall, Petr.
1987.
Prague functionalismand topic vs. focus.
In Rene?
Dirven andVile?m Fried, editors, Functionalism inLinguistics.
John Benjamins PublishingCompany, Amsterdam/Philadelphia,pages 169?189.Sgall, Petr.
1997.
On the usefulness ofmovement rules.
In B. Caron, editor, Actesdu 16e Congre`s International des Linguistes(Paris 20-25 juillet 1997).
Elsevier Science,Oxford.Sgall, Petr.
2002.
Freedom of language: Itsnature, its sources and its consequences.Prague Linguistic Circle Papers, 4:309?329.Sgall, Petr.
2004.
Types of languages and thesimple pattern of the core of language.
InP.
Sterkenburg, editor, LinguisticsToday?Facing a Greater Challenge (Plenarylectures from CIL 17).
John Benjamins,Amsterdam/Philadelphia, pages 243?265.Sgall, Petr, Eva Hajic?ova?, and Eva Benes?ova?.1973.
Topic, Focus and Generative Semantics.Scriptor, Kronberg/Taunus.Sgall, Petr, Eva Hajic?ova?, and JarmilaPanevova?.
1986.
The Meaning of the Sentencein Its Semantic and Pragmatic Aspects.Reidel, Dordrecht; Academia, Prague.S?milauer, Vladim??r.
1947.
Novoc?eska?
skladba[The syntax of Modern Czech].
Mikuta,Prague.Steedman, Mark.
1996.
Surface Structure andInterpetation.
The MIT Press, Cambridge,MA.Steedman, Mark.
2000.
Information structureand the syntax?phonology interface.Linguistic Inquiry, 31:649?689.468Hajic?ova?
Old Linguists Never DieSteedman, Mark.
2005.
Grammar acquisitionby child and machine.
Invited Talk at the17th European Summer School of Language,Logic and Information, Edinburgh.Tesnie`re, Lucien.
1934.
Comment construireune syntaxe.
Bulletin de la Faculte?
des lettresde Strasbourg, 12(7):219?229.Tesnie`re, Lucien.
1959.
Ele?ments de SyntaxeStructurale.
Klinksieck, Paris.Vauquois, Bernard.
1975.
La traductionautomatique a` grenoble.
Documents delinguistique quantitative, 24.Vauquois, Bernard and Sylviane Chappuy.1985.
Static grammars: A formalism for thedescription of linguistic models.
InProceedings of the Conference on Theoreticaland Methodological Issues in MachineTranslation, pages 298?322, ColgateUniversity, Hamilton, New York.Weil, Henri.
1844.
De l?ordre des mots dans leslangues anciennes compare?es aux languesmodernes (The Order of Words in the AncientLanguages Compared with That of ModernLanguages), Paris; Amsterdam [1978].469
