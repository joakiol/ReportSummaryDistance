Automatic Corpus-Based Thai Word Extractionwith the C4.5 Learning AlgorithmVIRACH SORNLERTLAMVANICH, TANAPONG POTIPITI AND THATSANEECHAROENPORNNational Electronics and Computer Technology Centel,National Science and Technology Development Agency,Ministry of Science and Technology Environntent,22 '~1 Floor Gypsum Metiw)olitan Tower 539/2 Sriayudhya Rd.
Rajthevi Bangkok 10400 ThailatMEmail: virach@nectec.or.th, tanapong@nectec.or.th, thatsanee@nectec.or.thAbstract"Word" is difficult to define in the languages thatdo not exhibit explicit word boundary, such asThai.
Traditional methods on defining words forthis kind of languages have to depend on humanjudgement which bases on unclear criteria o1"procedures, and have several limitations.
Thispaper proposes an algorithm for word extractionfrom Thai texts without borrowing a hand fromword segmentation.
We employ the c4.5 learningalgorithm for this task.
Several attributes uch asstring length, frequency, nmtual information andentropy are chosen for word/non-worddetermination.
Our experiment yields highprecision results about 85% in both training andtest corpus.1 In t roduct ionin the Thai language, there is no explicit wordboundary; this causes a lot of problems in Thailanguage processing including wordsegmentation, information retrieval, machinetranslation, and so on.
Unless there is regularity indefining word entries, Thai language processingwill never be effectively done.
The existing Thailanguage processing tasks mostly rely on thehand-coded dictionaries to acquire the informationabout words.
These manually created ictionarieshave a lot of drawbacks.
First, it cannot deal withwords that are not registered in the dictionaries.Second, because these dictionaries are manuallycreated, they will never cover all words that occurin real corpora.
This paper, therefore, proposes anautomatic word-extraction algorithm, whichhopefully can overcome this Thai language-processing barrier.An essential and non-trivial task for thelanguages that exhibit inexplicit word boundarysuch as Thai, Japanese, and many other Asianlanguages undoubtedly is the task in identifyingword boundary.
"Word", generally, means a unitof expression which has universal intuitiverecognition by native speakers.
Linguistically,word can be considered as the most stable unitwhich has little potential to rearrangement and isuninterrupted as well.
"Uninterrupted" hereattracts our lexical knowledge bases so much.There are a lot of uninterrupted sequences ofwords functioning as a single constituent of asentence.
These uninterrupted strings, of courseare not the lexical entries in a dictionary, but eachoccurs in a very high frequency.
The way to pointout whether they are words or not is notdistinguishable even by native speakers.
Actually,it depends on individual judgement.
For example,a Thai may consider 'oonfila~mu' (exercise) a wholeword, but another may consider 'n~n~m~' as acompound: 'oon' (take)+ 'filg~' (power)+ 'too' (body).Computationally, it is also difficult to decidewhere to separate a string into words.
Eventhough it is reported that the accuracy of recentword segmentation using a dictionary and someheuristic methods is in a high level.
Currently,lexicographers can make use of large corpora andshow the convincing results from the experimentsover corpora.
We, therefore, introduce here a newefficient method for consistently extracting andidentifying a list of acceptable Thai words.2 Previous WorksReviewing the previous works on Thai wordextraction, we found only the work ofSornlertlamvanich and Tanaka (1996).
Theyemployed the fiequency of the sorted character n-grams to extract Thai open compounds; the stringsthat experienced a significant change ofoccurrences when their lengths are extended.
Thisalgorithm reports about 90% accuracy of Thai802open compound extraction.
However, thealgorithm emphasizes on open compotmdextraction and has to limit tile range of n-gram to4-20 grams for the computational reason.
Thiscauses limitation in the size of corpora andefficiency in the extraction.The other works can be found in theresearch on the Japanese language.
Nagao et al(1994) has provided an effective method toconstruct a sorted file that facilitates thecalculation of n-gram data.
But their algorithm didnot yield satisfactory accuracy; there were manyiuwflid substrings extracted.
The following work(lkehara et al, 1995) improved the sorted file toavoid repeating in counting strings.
The extractioncesult was better, but the determination of thelongest strings is always made consecutively fromleft to right.
If an erroneous tring is extracted, itserrors will propagate through the rest of the input:~trings.
:3 Our Approach3.1 The C4.5 Learning AlgorithmDecision tree induction algorithms have beensuccessfully applied for NLP problems such assentence boundary dismnbiguation (Pahner et al1997), parsing (Magerman 1995) and wordsegmentation (Mekuavin et al 1997).
We employthe c4.5 (Quinhln 1993) decision tree inductionprogram as the learning algorithm for wordextraction.The induction algorithm proceeds byevaluating content of a series of attributes anditeratively building a tree fiom the attribute valueswith the leaves of the decision tree being the valueof the goal attribute.
At each step of learningprocedure, the evolving tree is branched on theattribute that pal-titions tile data items with thehighest information gain.
Branches will be addeduntil all items in the training set arc classified.
Toreduce the effect of overfitting, c4.5 prunes theentire decision tree constructed.
It recursivelyexamines each subtree to determine whetherreplacing it with a leaf or brauch woukt reduceexpected error rate.
This pruning makes thedecision tree better in dealing with tile datadifferent froul tile training data.3.2 AttributesWe treat the word extraction problem as theproblem of word/nou-word string disambiguation.The next step is to identify the attributes that areable to disambiguate word strings flom non-wordstrings.
The attributes used for the learningalgorithm are as follows.3.2.1 Left Mutual hfomlation and Right Mutualh{fbrmationMutual information (Church et al 1991) ofrandom variable a and b is the ratio of probabilitythat a and b co-occur, to tile indepeudentprobability that a and b co-occur.
High mutualinformation indicates that a and b co-occur lnorethan expected by chance.
Our algorithm employsleft and right mutual information as attributes inword extraction procedure.
Tile left mutualinformation (Lm), and right mutual information(Rm) of striug ayz are defined as:Lm(xyz)  -Rm(xyr.)
-p(xyz.)p(x)p(yz)p(xy~.
)p ( ,y )p (z )wherex is the leftmost character ofayzy is the lniddle substring ol'ayzis the rightmost character of :tlVzp( ) is tile probability function.If xyz is a word, both Lm(xyz) and Rm(~yz) shouldbe high.
On the contra W, if .rye is a non-wordstring but consists of words and characters, eitherof its left or right mutual information or both lnustbe low.
For example, 'ml~qn~" ( n'(a Thai alphabet)'fl~anq'(The word means appear in Thai.)
) musthave low left mutual information.3.2.2 Left Entropy and Right EntropyEutropy (Shannon 1948) is the informationmeasuring disorder of wu'iables.
The left and rightentropy is exploited as another two attributes inour word extraction.
Left entropy (Le), and rightentropy (Re) of stringy are defined as:803Le(y) = - Z p(xy I Y)' Iog2p(xYlY)V.r~ ARe(y) = - Z p(yz l y ) " log 2 p(yz l y )Vz~Awherey is the considered string,A is the set of all alphabetsx, z is any alphabets in A.I fy  is a word, the alphabets that come before andaflery should have varieties or high entropy.
If yis not a complete word, either of its left or rightentropy, or both must be low.
For example, 'ahan'is not a word but a substring of word 'O~3n~l'(appear).
Thus the choices of the right adjacentalphabets to '~qn' must be few and the rightentropy of 'ahw, when the right adjacent alphabetis '~', must be low.3.2.3 FrequencyIt is obvious that the iterative occurrences ofwords must be higher than those of non-wordstrings.
String frequency is also usefulinformation for our task.
Because the stringfrequency depends on the size of corpus, wenormalize the count of occurrences by dividing bythe size of corpus and multiplying by the averagevalue of Thai word length:F(s) = N(s).AvlScwheres is the considered stringN(s) is the number of the occurrencesof s in corpusSc is the size of corpusAvl is the average Thai word length.We employed the frequency value as anotherattribute for the c4.5 learning algorithm.3.2.4 LengthShort strings are more likely to happen by chancethan long strings.
Then, short and long stringsshould be treated ifferently in the disambiguationprocess.
Therefore, string length is also used as anattribute for this task.3.2.5 Functional WordsFunctional words such as '~' (will) and '~' (then)are frequently used in Thai texts.
These functionalwords are used often enough to mislead theoccurrences of string patterns.
To filter out thesenoisy patterns from word extraction process,discrete attribute Func(s):Func(s) : 1 if string s containsfnnctional words,= 0 if otherwise,is applied.3.2.6 First Two and Last Two CharactersA very useful process for our disambiguation is tocheck whether the considered string complies withThai spelling rules or not.
We employ the wordsin the Thai Royal Institute dictionary as spellingexamples for the first and last two characters.Then we define attributes Fc(s)and Lc(s) forthis task as follows.N(s, s2*)Fc(s )  -NDN(*s,,_l  s,, ) Lc( s ) -NDwhere s is the considered string andS .= S IS2 .
.
.Sn_ IS  nN(sls2* ) is the number of words inthe dictionary that begin with s~s 2N(*s,_ls,,) is the nmnber ofwords in the dictionary thatend with s,,_~s,,ND is the number of words inthe dictionary.3.3 Applying C4.5 to Thai Word ExtractionThe process of applying c4.5 to our wordextraction problem is shown in Figure 1.
Firstly,we construct a training set for the c4.5 learningalgorithm.
We apply Yamamoto et al(1998)'salgorithm to extract all strings from a plain andunlabelled I-MB corpus which consists of 75articles from various fields.
For practical andreasonable purpose, we select only the 2-to-30-character strings that occur more than 2 times,804Extracting Stringsfromthe TrainingCorpusComputing the\]Attributes IValue JiTagging theStrings 1'qVi Extracting Stringsfromthe Test Corpus~ t ~  theAttributesValueJ -  --We r ~1 ExtractionFigure.
1 : Overview o1' the ProcessRe > 1.78 / ,-2~Lm 14233--:.
/ is notaword '\/ \ \Y//" \~  N.2" Func= 0 "> s nota wori s  a wordFigure 2: Exanlple of the Decision treehave positive right and left entropy, and conformto simple Thai spelling rules.
To this step, we getabout 30,000 strings.
These strings are lnalmallytagged as words or non-word strings.
The strings'statistics explained above are calculated for eachstring.
Then the strings' attributes and tags areused as the training example for the learningalgorithln.
The decision tree is then constructedfrom the training data.In order to test the decision tree, anotherplain I-MB corpus (the test corpus), whichconsists of 72 articles fi'om various fields, isemployed.
All strings in the test corpus areextracted and filtered out by the same process asused in the training set.
After the filtering process,we get about 30,000 strings to be tested.
These30,000 strings are manually tagged in order thatthe precision and recall of the decision tree can beevaluated.
The experimental results will bediscussed in the next section.4 Exper imental  Results4.1 The ResultsTo measure the accuracy of the algorithln, weconsider two statistical values: precision andrecall.
The precision of our algorithm is 87.3% forthe training set and 84.1% for the test set.
Therecall of extraction is 56% in both training andtest sets.
We compare the recall of our wordextraction with the recall from using the ThaiRoyal Institute dictionary (RID).
The recall frolnour approach and from using RID are comparableand our approach should outperform the existingdictionary for larger corpora.
Both precision andrecall fiom training and test sets are quite close.This indicates that the created decision tree isrobust for unseen data.
Table 3 also shows thatmore than 30% of the extracted words are notfound in RID.
These would be the new entries forthe dictionary.Table 1 : The precision of word extractionNo.
of stringsextracted by thedecision treeTraining 1882Set (100%)'lest Set 1815(100%)No.
of No.
of non-words word stringsextracted extracted1643 239(87.3%) (12.7%)1526 289(84.1%) (15.9%)Table 2: Tile recall of word extractionTrainingSetTest SetNo.
of wordsthat ill 30,000stringsextractedNo.
of wordsextracted bythe decisiont reeNo.
of wordsin corpus thatare foundRID2933 1643 1833(100%) (56.0%) (62.5%)2720 1526 1580(100%) (56.1%) (58.1%)805Table 3: Words extractedNo.
of wordsextracted bythe decisiontreeby the decisionNo.
of wordsextracted bythe decisiontree which isinRIDtree and RIDNo.
of wordsextracted bythe decisiontree which isnot in RIDTraining 1643 1082 561Set (100.0%) (65.9%) (34.1%)Test Set 1526 1046 480(100.1%) (68.5%) (31.5%)4.2 The Relationship of Accuracy, Occurrenceand LengthIn this section, we consider the relationship of theextraction accuracy to the string lengths andoccurrences.
Figure 2 and 3 depict that bothprecision and recall have tendency to increase asstring occurrences are getting higher.
This impliesthat the accuracy should be higher for largercorpora.
Similarly, in Figure 4 and 5, the accuracytends to be higher in longer strings.
The newcreated words or loan words have tendency to belong.
Our extraction, then, give a high accuracyand very useful for extracting these new createdwords.T ra in  in  g. .
.
.
.
.
.
T cs t, r 1 r I I I I2 6 10  14  18  22  26  3O 34  3 \ [{0 ccur rcncc  (x  I O0  )Figurc 3: Prccision-Occurrence R lationshiplOOZ .~4o - -T ra in ing2o .
.
.
.
.
.
Testo r r r T T 1 T T ?
?2 6 10  14  18  22  26  30  34  38Occurrence (xl00)Figure 4: Recall-Occurrence RelationshiplOO"~ 40  r,2O0120I T raining.
.
.
.
.
.
TcstT r E r i r ~ i1 3 5 7 9 11  13  15  17Length (No.
of characters)Figure 5: Precision-Length RelationshipI 90I 8o1 70i 6050 i!40?~" 3020. .
.
.
.
.
TestlO0 ?
i i1 3 5 7 ?
11 13  15  17\ [ , cng lh  (No .
of  characters )Figure 6: Prccision-Length P,elationship5 ConclusionIn this paper, we have applied the c4.5 learningalgorithm for the task of Thai word extraction.C4.5 can construct a good decision tree forword/non-word disambiguation.
The learnedattributes, which are mutual information, entropy,word frequency, word length, functional words,first two and last two characters, can captureuseful information for word extraction.
Ourapproach yields about 85% and 56% in precisionand recall measures respectively, which iscomparable to employing an existing dictionary.The accuracy should be higher in larger corpora.Our future work is to apply this algorithm withlarger corpora to build a corpus-based Thaidictionary.
And hopefully, out" approach should besuccessful for other non-word-boundarylanguages.AcknowledgementSpecial thanks to Assistant Professor MikioYamamoto for providing the useful program toextract all substrings from the corpora in lineartime.806ReferencesChurch, K.W., Robert L. and Mark L.Y.
(1991) A Status Report on ACL/DCL.Proceedings of 7 a' Annual Co#(ference ofthe UW Centre New OED attd TextReseatrh: Using Corpora, pp.
84-91Ikehara, S., Shirai, S. and Kawaoka, T. (1995)Automatic Extraction of UninterruptedCollocations by n-gran~ Statistics.
Piwceeding q\[The fitwt Annual Meeting of the Association forNatural Language Processing, pp.
313-316 (inJapancse)Magerman, D.M.
(1995) Statistical decision-treemodels for parsing., hwceeding of 33rdAmtual Meeting of Association for ComputationalLinguisticsMeknavin, S., Charoenpornsawat, P. and Kijsirikul, B.
(1997) Feature-based Thai Word Segmentation.Proceeding of the Natural Language ProcessingPacific Rim Symposium 1997, pp.
35-46Nagao, M. and Mort, S. (1994) A New Method of N-gram Statistics for Large Number of n andAutomatic Extraction of Words and Phrases fl'omLarge Text l)ata of Japanese.
Proceeding ofCOLING 94, Vol.
1, pp.
611-15Pahner, D.D.
and Hearst M.A.
(1997) AdaptiveMultilingual Sentence Boundary Disambiguation.ComputationalLinguistics Vol.27, pp.
241-267Quinhm, J.R. (1993) C4.5 Programs for MachineLearning.Morgan Publishers San Mated,California, 302 p.Shannon, C.E.
(1948) A Mathematical Theory ofCommunicatiomJ.
Bell System Technical Jolu'nal27, pp.
379-423Sornlertlamvanich, V. and Tanaka, H. (1996) TheAutomatic Extraction of Open Compounds fromText.
Proceeding o\[ COLING 96 Vol.
2, pp.
1143-1146Yamamoto, M. and Church, K.W.
(1998) Using SuffixArrays to Compare Term Frequency andDocument Frequency for All Substrings in Corpus.Proceeding of Sixth Workshop on Veo' LargeCorpora pp.
27-37807
