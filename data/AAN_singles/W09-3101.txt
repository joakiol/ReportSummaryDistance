Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, page 1,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPInvited PresentationRepetition and Language Models and Comparable CorporaKen ChurchHuman Language Technology Center of ExcellenceJohns Hopkins UniversityKenneth.Church@jhu.eduI will discuss a couple of non-standard fea-tures that I believe could be useful for workingwith comparable corpora.
Dotplots have beenused in biology to find interesting DNA sequences.Biology is interested in ordered matches, whichshow up as (possibly broken) diagonals in dot-plots.
Information Retrieval is more interested inunordered matches (e.g., cosine similarity), whichshow up as squares in dotplots.
Parallel corporahave both squares and diagonals multiplexed to-gether.
The diagonals tell us what is a translationof what, and the squares tell us what is in the samelanguage.
I would expect dotplots of compara-ble corpora would contain lots of diagonals andsquares, though the diagonals would be shorterand more subtle in comparable corpora than in par-allel corpora.There is also an opportunity to take advantageof repetition in comparable corpora.
Repetition isvery common.
Standard bag-of-word models inInformation Retrieval do not attempt to model dis-course structure such as given/new.
The first men-tion in a news article (e.g., ?Manuel Noriega, for-mer President of Panama?)
is different from sub-sequent mentions (e.g., ?Noriega?).
Adaptive lan-guage models were introduced in Speech Recogni-tion to capture the fact that probabilities change oradapt.
After we see the first mention, we shouldexpect a subsequent mention.
If the first men-tion has probability p, then under standard (bag-of-words) independence assumptions, two men-tions ought to have probability p2, but we findthe probability is actually closer to p/2.
Adapta-tion matters more for meaningful units of text.
InJapanese, words (meaningful sequences of char-acters) are more likely to be repeated than frag-ments (meaningless sequences of characters fromwords that happen to be adjacent).
In newswire,we find more adaptation for content words (propernouns, technical terminology and good keywordsfor information retrieval), and less adaptation forfunction words, clich?s and ordinary first names.There is more to meaning than frequency.
Contentwords are not only low frequency, but likely to berepeated.1
