Extracting Multiword Expressions with A Semantic TaggerScott S. L. PiaoDept.
of Linguistics and MELLancaster Universitys.piao@lancaster.ac.ukPaul RaysonComputing DepartmentLancaster Universitypaul@comp.lancs.ac.ukDawn ArcherDept.
of Linguistics and MELLancaster Universityd.archer@lancaster.ac.ukAndrew WilsonDept.
of Linguistics and MELLancaster Universityeiaaw@exchange.lancs.ac.ukTony McEneryDept.
of Linguistics and MELLancaster Universityamcenery@lancaster.ac.ukAbstractAutomatic extraction of multiwordexpressions (MWE) presents a toughchallenge for the NLP communityand corpus linguistics.
Althoughvarious statistically driven or knowl-edge-based approaches have beenproposed and tested, efficient MWEextraction still remains an unsolvedissue.
In this paper, we present ourresearch work in which we testedapproaching the MWE issue using asemantic field annotator.
We use anEnglish semantic tagger (USAS) de-veloped at Lancaster University toidentify multiword units which de-pict single semantic concepts.
TheMeter Corpus (Gaizauskas et al,2001; Clough et al, 2002) built inSheffield was used to evaluate ourapproach.
In our evaluation, this ap-proach extracted a total of 4,195MWE candidates, of which, aftermanual checking, 3,792 were ac-cepted as valid MWEs, producing aprecision of 90.39% and an esti-mated recall of 39.38%.
Of the ac-cepted MWEs, 68.22% or 2,587 arelow frequency terms, occurring onlyonce or twice in the corpus.
Theseresults show that our approach pro-vides a practical solution to MWEextraction.1 Introduction2Automatic extraction of Multiword ex-pressions (MWE) is an important issue in theNLP community and corpus linguistics.
Anefficient tool for MWE extraction can be use-ful to numerous areas, including terminologyextraction, machine translation, bilin-gual/multilingual MWE alignment, automaticinterpretation and generation of language.
Anumber of approaches have been suggestedand tested to address this problem.
However,efficient extraction of MWEs still remains anunsolved issue, to the extent that Sag et al(2001b) call it ?a pain in the neck of NLP?.In this paper, we present our work inwhich we approach the issue of MWE extrac-tion by using a semantic field annotator.
Spe-cifically, we use the UCREL SemanticAnalysis System (henceforth USAS), devel-oped at Lancaster University to identify mul-tiword units that depict single semanticconcepts, i.e.
multiword expressions.
We havedrawn from the Meter Corpus (Gaizauskas etal., 2001; Clough et al, 2002) a collection ofBritish newspaper reports on court stories toevaluate our approach.
Our experiment showsthat it is efficient in identifying MWEs, inparticular MWEs of low frequencies.
In thefollowing sections, we describe this approachto MWE extraction and its evaluation.Related WorksGenerally speaking, approaches to MWEextraction proposed so far can be divided intothree categories: a) statistical approachesbased on frequency and co-occurrence affin-ity, b) knowledge?based or symbolic ap-proaches using parsers, lexicons and languagefilters, and c) hybrid approaches combiningdifferent methods (Smadja 1993; Dagan andChurch 1994; Daille 1995; McEnery et al1997; Wu 1997; Wermter et al 1997; Mi-chiels and Dufour 1998; Merkel and Anders-son 2000; Piao and McEnery 2001; Sag et al2001a, 2001b; Biber et al 2003).In practice, most statistical approaches uselinguistic filters to collect candidate MWEs.Such approaches include Dagan and Church?s(1994) Termight Tool.
In this tool, they firstcollect candidate nominal terms with a POSsyntactic pattern filter, then use concordancesto identify frequently co-occurring multiwordunits.
In his Xtract system, Smadja (1993)first extracted significant pairs of words thatconsistently co-occur within a single syntacticstructure using statistical scores called dis-tance, strength and spread, and then exam-ined concordances of the bi-grams to findlonger frequent multiword units.
Similarly,Merkel and Andersson (2000) compared fre-quency-based and entropy based algorithms,each of which was combined with a languagefilter.
They reported that the entropy-basedalgorithm produced better results.One of the main problems facing statisticalapproaches, however, is that they are unableto deal with low-frequency MWEs.
In fact,the majority of the words in most corporahave low frequencies, occurring only once ortwice.
This means that a major part of truemultiword expressions are left out by statisti-cal approaches.
Lexical resources and parsersare used to obtain better coverage of the lexi-con in MWE extraction.
For example, Wu(1997) used an English-Chinese bilingualparser based on stochastic transductiongrammars to identify terms, including multi-word expressions.
In their DEFI Project, Mi-chiels and Dufour (1998) used dictionaries toidentify English and French multiword ex-pressions and their translations in the otherlanguage.
Wehrli (1998) employed a genera-tive grammar framework to identify com-pounds and idioms in their ITS-2 MTEnglish-French system.
Sag et al (2001b)introduced Head-driven Phrase StructureGrammar for analyzing MWEs.
Like purestatistical approaches, purely knowledge-based symbolic approaches also face prob-lems.
They are language dependent and notflexible enough to cope with complex struc-tures of MWEs.
As Sag et al (2001b) sug-gest, it is important to find the right balancebetween symbolic and statistical approaches.In this paper, we propose a new approachto MWEs extraction using semantic field in-formation.
In this approach, multiword unitsdepicting single semantic concepts are recog-nized using the Lancaster USAS semantictagger.
We describe that system and the algo-rithms used for identifying single and multi-word units in the following section.3Lancaster Semantic taggerThe USAS system has been in develop-ment at Lancaster University since 1990 1 .Based on POS annotation provided by theCLAWS tagger (Garside and Smith, 1997),USAS assigns a set of semantic tags to eachitem in running text and then attempts to dis-ambiguate the tags in order to choose themost likely candidate in each context.
Itemscan be single words or multiword expressions.The semantic tags indicate semantic fieldswhich group together word senses that arerelated by virtue of their being connected atsome level of generality with the same mentalconcept.
The groups include not only syno-nyms and antonyms but also hypernyms andhyponyms.The initial tagset was loosely based onTom McArthur's Longman Lexicon of Con-temporary English (McArthur, 1981) as thisappeared to offer the most appropriate thesau-rus type classification of word senses for thiskind of analysis.
The tagset has since beenconsiderably revised in the light of practicaltagging problems met in the course of the re-search.
The revised tagset is arranged in ahierarchy with 21 major discourse fields ex-panding into 232 category labels.
The follow-ing list shows the 21 labels at the top level ofthe hierarchy (for the full tagset, see website:http://www.comp.lancs.ac.uk/ucrel/usas).1 This work is continuing to be supported by the Bene-dict project, EU project IST-2001-34237.A general and abstract termsB the body and the individualC arts and craftsE emotionF food and farmingG government and the public domainH architecture, buildings, houses and thehomeI money and commerce in industryK entertainment, sports and gamesL life and living thingsM movement, location, travel and trans-portN numbers and measurementO substances, materials, objects andequipmentP educationQ linguistic actions, states and processesS social actions, states and processesT timeW the world and our environmentX psychological actions, states andprocessesY science and technologyZ names and grammatical wordsCurrently, the lexicon contains just over37,000 words and the template list containsover 16,000 multiword units.
These resourceswere created manually by extending and ex-panding dictionaries from the CLAWS taggerwith observations from large text corpora.Generally, only the base form of nouns andverbs are stored in the lexicon and a lemmati-sation procedure is used for look-up.
How-ever, the base form is not sufficient in somecases.
Stubbs (1996: 40) observes that ?mean-ing is not constant across the inflected formsof a lemma?, and Tognini-Bonelli (2001: 92)notes that lemma variants have differentsenses.In the USAS lexicon, each entry consistsof a word with one POS tag and one or moresemantic tags assigned to it.
At present, incases where a word has more than one syntac-tic tag, it is duplicated (i.e.
each syntactic tagis given a separate entry).The semantic tags for each entry in thelexicon are arranged in approximate rank fre-quency order to assist in manual post editing,and to allow for gross automatic selection ofthe common tag, subject to weighting by do-main of discourse.In the multi-word-unit list, each templateconsists of a pattern of words and part-of-speech tags.
The semantic tags for each tem-plate are arranged in rank frequency order inthe same way as the lexicon.
Various types ofmultiword expressions are included: phrasalverbs (e.g.
stubbed out), noun phrases (e.g.
skiboots), proper names (e.g.
United States), trueidioms (e.g.
life of Riley).Figure 1 below shows samples of the actualtemplates used to identify these MWUs.
Eachof these example templates has only one se-mantic tag associated with it, listed on theright-hand end of the template.
However, thesecond example (ski boot) combines theclothing (B5) and sports (K5.1) fields into onetag.
The pattern on the left of each templateconsists of a sequence of words joined to POStags with the underscore character.
The wordsand POS fields can include the asterisk wild-card character to allow for inflectional vari-ants and to write more powerful templateswith wider coverage.
USAS templates canmatch discontinuous MWUs, and this is illus-trated by the first example, which includesoptional intervening POS items markedwithin curly brackets.
Thus this template canmatch stubbed out and stubbed the cigaretteout.
?Np?
is used to match simple nounphrases identified with a noun-phrase chun-ker.stub*_* {Np/P*/R*} out_RP    O4.6-ski_NN1 boot*_NN*          B5/K5.1United_* States_N*              Z2life_NN1 of_IO Riley_NP1        K1Figure 1 Sample of USAS multiword templatesAs in the case of grammatical tagging, thetask of semantic tagging subdivides broadlyinto two phases: Phase I (Tag assignment):attaching a set of potential semantic tags toeach lexical unit and Phase II (Tag disam-biguation): selecting the contextually appro-priate semantic tag from the set provided byPhase I. USAS makes use of seven majortechniques or sources of information in phaseII.
We will list these only briefly here, sincethey are described in more detail elsewhere(Garside and Rayson, 1997).1.
POS tag.
Some senses can be elimi-nated by prior POS tagging.
The CLAWSpart-of-speech tagger is run prior to semantictagging.2.
General likelihood ranking for single-word and MWU tags.
In the lexicon andMWU list senses are ranked in terms of fre-quency, even though at present such rankingis derived from limited or unverified sourcessuch as frequency-based dictionaries, pasttagging experience and intuition.3.
Overlapping MWU resolution.
Nor-mally, semantic multi-word units take priorityover single word tagging, but in some cases aset of templates will produce overlappingcandidate taggings for the same set of words.A set of heuristics is applied to enable themost likely template to be treated as the pre-ferred one for tag assignment.4.
Domain of discourse.
Knowledge ofthe current domain or topic of discourse isused to alter rank ordering of semantic tags inthe lexicon and template list for a particulardomain.5.
Text-based disambiguation.
It hasbeen claimed (by Gale et al 1992) on the ba-sis of corpus analysis that to a very large ex-tent a word keeps the same meaningthroughout a text.6.
Contextual rules.
The templatemechanism is also used in identifying regularcontexts in which a word is constrained tooccur in a particular sense.7.
Local probabilistic disambiguation.
Itis generally supposed that the correct seman-tic tag for a given word is substantially de-termined by the local surrounding context.After automatic tag assignment has beencarried out, manual post-editing can takeplace, if desired, to ensure that each word andidiom carries the correct semantic classifica-tion.From these seven disambiguation meth-ods, our main interest in this paper is the thirdtechnique of overlapping MWU resolution.When more than one template match overlapsin a sentence, the following heuristics are ap-plied in sequence:1.
Prefer longer templates over shortertemplates2.
For templates of the same length, pre-fer shorter span matches over longerspan matches (a longer span indicatesmore intervening items for discon-tinuous templates)3.
If the templates do not apply to thesame sequence of words, prefer theone that begins earlier in the sentence4.
For templates matching the same se-quence of words, prefer the onewhich contains the more fully definedtemplate pattern (with fewer wild-cards in the word fields)5.
Prefer templates with a more fully de-fined first word in the template6.
Prefer templates with fewer wildcardsin the POS tagsThese six rules were found to differentiatein all cases of overlapping MWU templates.Cases which failed to be differentiated indi-cated that two (or more) templates in ourMWU list were in fact identical, apart fromthe semantic tags and required merging to-gether.4 Experiment of MWE extractionIn order to test our approach of extractingMWEs using semantic information, we firsttagged the newspaper part of the METERCorpus with the USAS tagger.
We then col-lected the multiword units assigned as a singlesemantic unit.
Finally, we manually checkedthe results.The Meter Corpus chosen as the test datais a collection of court reports from the Brit-ish Press Association (PA) and some leadingBritish newspapers (Gaizauskas 2001; Cloughet al, 2002).
In our experiment, we used thenewspaper part of the corpus containing 774articles with more than 250,000 words.
It pro-vides a homogeneous corpus (in the sense thatthe reports come from a restricted domain ofcourt events) and is thus a good source fromwhich to extract domain-specific MWEs.Another reason for choosing this corpus isthat it has not been used in training the USASsystem.
As an open test, we assume the re-sults of the experiment should reflect true ca-pability of our approach for real-lifeapplications.The current USAS tagger may assign mul-tiple possible semantic tags for a term when itfails to disambiguate between them.
As men-tioned previously, the first one denotes themost likely semantic field of the term.
There-fore, in our experiment we chose the first tagwhen such situations arose.A major problem we faced in our experi-ment is the definition of a MWE.
Although ithas been several years since people started towork on MWE extraction, we found that thereis, as yet, no available ?clear-cut?
definitionfor MWEs.
We noticed various possible defi-nitions have been suggested for MWE/MWU.For example, Smadja (1993) suggests a basiccharacteristic of collocations and multiwordunits is recurrent, domain-dependent and co-hesive lexical clusters.
Sag et el.
(2001b) sug-gest that MWEs can roughly be defined as?idiosyncratic interpretations that cross wordboundaries (or spaces)?.
Biber et al (2003)describe MWEs as lexical bundles, whichthey go on to define as combinations of wordsthat can be repeated frequently and tend to beused frequently by many different speak-ers/writers within a register.Although it is not difficult to interpretthese deifications in theory, things becamemuch more complicated when we undertookour practical checking of the MWE candi-dates.
Quite often, we experienced disagree-ment between us about whether or not toaccept a MWE candidate as a good one.
Inpractice, we generally followed Biber et al?sdefinition, i.e.
accept a candidate MWE as agood one if it can repeatedly co-occur in thecorpus.Another difficulty we experienced relatesto estimating recall.
Because the MWEs in theMETER Corpus are not marked-up, we couldnot automatically calculate the number ofMWEs contained in the corpus.
Conse-quently, we had to manually estimate this fig-ure.
Obviously it is not practical to manuallycheck though the whole corpus within thelimited time allowed.
Therefore, we had toestimate the recall on a sample of the corpus,as will be described in the following section.5 EvaluationIn this section, we analyze the results ofthe MWE extraction in detail for a fullevaluation of our approach to MWE extrac-tion.Overall, after we processed the test corpus,the USAS tagger extracted 4,195 MWE can-didates from the test corpus.
After manuallychecking through the candidates, we selected3,792 as good MWEs, resulting in overallprecision of 90.39%.As we explained earlier, due to the diffi-culty of obtaining the total number of trueMWEs in the entire test corpus, we had toestimate recall of the MWE extraction on asample corpus.
In detail, we first randomlyselected fifty texts containing 14,711 wordsfrom the test corpus, then manually marked-up good MWEs in the sample texts, finallycounted the number of the marked-up MWUs.As a result, 1,511 good MWEs were found inthe sample.
Since the number of automaticallyextracted good MWEs in the sample is 595,the recall on the sample is calculated as fol-lows:Recall=(595?1511)?100%=39.38%.Considering the homogenous feature ofthe test data, we assume this local recall isroughly approximate to the global recall ofthe test corpus.To analyze the performance of USAS inrespect to the different semantic field catego-ries, we divided candidates according to theassigned semantic tag, and calculated the pre-cision for each of them.
Table 1 lists theseprecisions, sorting the semantic fields by thenumber of MWE candidates (refer to section3 for definitions of the twenty-one main se-mantic field categories).
As shown in this ta-ble, the USAS semantic tagger obtainedprecisions between 91.23% to 100.00% foreach semantic field except for the field of?names and grammatical words?
denoted byZ.
As Z was the biggest field (containing45.39% of the total MWEs and 43.12% of theaccepted MWEs), we examined these MWEsmore closely.
We discovered that numerouspairs of words are tagged as person names(Z1) and geographical names (Z2) by mistake,e.g.
Blackfriars crown (tagged as Z1), stabbedConstance (tagged as Z2) etc.SemanticfieldTotalMWEsAcceptedMWEsPrecisionZ 1,904  1,635 85.87%T 497  459 92.35%A 351  328 93.44%M 254  241 94.88%N 227  211 92.95%S 180  177 98.33%B 131  128 97.71%G 118  110 93.22%X 114  104 91.23%I 74  72 97.30%Q 67  63 94.03%E 58  53 91.38%H 53  52 98.11%K 48  45 93.75%P 39  37 94.87%O 32  29 90.63%F 24  24 100.00%L 11  11 100.00%Y 6  6 100.00%C 5  5 100.00%W 2  2 100.00%Total 4,195 3,792 90.39%Table 1: Precisions for different semantic catego-riesAnother possible factor that affects theperformance of the USAS tagger is the lengthof the MWEs.
To observe the performance ofour approach from this perspective, wegrouped the MWEs by their lengths, and thenchecked precision for each of the categories.Table 2 shows the results (once again, theyare sorted in descending order by MWElengths).
As we might expect, the number ofMWEs decreases as the length increases.
Infact, bi-grams alone constitute 80.52% and81.88% of the candidate and accepted MWEsrespectively.
The precision also showed agenerally increasing trend as the MWE lengthincreases, but with a major divergence of tri-grams.
One main type of error occurred on tri-grams is that those with the structure ofCIW(capital-initial word) + conjunction +CIW tend to be tagged as Z2 (geographicalname).
The table shows relatively high preci-sion for longer MWEs, reaching 100% for 6-grams.
Because the longest MWEs extractedhave six words, no longer MWEs could beexamined.MWElengthTotalMWEsAcceptedMWEsPrecision2 3,378 3,105 91.92%3 700 575 82.14%4 95 91 95.44%5 18 17 94.44%6 4 4 100.00%Total 4,195 3,792 90.39%Table 2: Precisions for MWEs of different lengthsAs discussed earlier, purely statistical al-gorithms of MWE extraction generally filterout candidates of low frequencies.
However,such low-frequency terms in fact form majorpart of MWEs in most corpora.
In our study,we attempted to investigate the possibility ofextracting low frequency MWEs by usingsemantic field annotation.
We divided MWEsinto different frequency groups, then checkedprecision for each of the categories.
Table 3shows the results, which are sorted by thecandidate MWE frequencies.
As we expected,69.46% of the candidate MWEs and 68.22%of the accepted MWEs occur in the corpusonly once or twice.
This means that, with afrequency filter of Min(f)=3, a purely statisti-cal algorithm would exclude more than half ofthe candidates from the process.Freq.
ofMWETotalnumberAcceptedMWEsPrecision1 2,164  1,892 87.43%2 750  695 92.67%3 - 4 616 570 92.53%5 - 7 357 345 96.64%8 - 20 253 238 94.07%21 - 117 55 52 94.55%Total 4,195 3,792 90.39%Table 3: Precisions for MWEs with different fre-quenciesTable 3 also displays an interesting rela-tionship between the precisions and the fre-quencies.
Generally, we would expect betterprecisions for MWEs of higher frequencies,as higher co-occurrence frequencies are ex-pected to reflect stronger affinity between thewords within the MWEs.
By and large,slightly higher precisions were obtained forthe latter groups of higher frequencies (5?7,8-20 and 21-117) than those for the precedinglower frequency groups, i.e.
94.07%-96.64%versus 87.43%-92.67%.
Nevertheless, for thelatter three groups of the higher frequencies(5-7, 8-20 and 21?117) the precision did notincrease as the frequency increases, as weinitially expected.When we made a closer examination ofthe error MWEs in this frequency range, wefound that some frequent domain-specificterms are misclassified by the USAS tagger.For example, since the texts in the test corpusare newspaper reports of court stories, manylaw courts (e.g.
Manchester crown court,Norwich crown court) are frequently men-tioned throughout the corpus, causing highfrequencies of such terms (f=20 and f=31 re-spectively).
Unfortunately, the templates usedin the USAS tagger did not capture them ascomplete terms.
Rather, fragments were as-signed a Z1 person name tag (e.g.
Manchestercrown).
A solution to this type of problem isto improve the multiword unit templates usedin the USAS tagger.
Other possible solutionsmay include incorporating a statistical algo-rithm to help detect boundaries of completeMWEs.When we examined the error distributionwithin the semantic fields more closely, wefound that most errors occurred within the Zand T categories (refer to Table 1).
The errorsoccurring in these semantic field categoriesand their sub-divisions make up 76.18% ofthe total errors (403).
Table 4 shows the errordistribution across 14 sub-divisions (for defi-nitions of these subdivisions, see: website:http://www.comp.lancs.ac.uk/ucrel/usas).
No-tice that the majority of errors are from foursemantic sub-categories: Z1, Z2, Z3 and T1.3.Notice, also, that the first two of these ac-count for 60.55% of the total errors.
Thisshows that the main cause of the errors in theUSAS tool is the algorithm and lexical entriesused for identifying names - personal andgeographical and, to a lesser extent, the algo-rithm and lexical entries for identifying peri-ods of time.
If these components of the USAScan be improved, a much higher precision canbe expected.In sum, our evaluation shows that our se-mantic approach to MWE extraction is effi-cient in identifying MWEs, in particular thoseof lower frequencies.
In addition, a reasona-bly wide lexical coverage is obtained, as indi-cated by the recall of 39.38%, which isimportant for terminology building.
Our ap-proach provides a practical way for extractingMWEs on a large scale, which we envisagecan be useful for both linguistic research andpractical NLP applications.Stag  Err.
Stag  Err.Z1:person names 119 T1.1.1:time-past 1Z2:geog.
names 125 T1.1.2:time-present 1Z3:other names 16 T1.2:time-momentary 8Z4:discourse bin 3 T1.3:time-period 23Z5:gram.
bin 2 T2:time-begin/end 2Z8:pronouns etc.
2 T3:time-age 1Z99:unmatched 2 T4:time-early/late 2Table 4: Errors for some semantic sub-divisions6 ConclusionIn this paper, we have shown that it is apractical way to extract MWEs using seman-tic field information.
Since MWEs are lexicalunits carrying single semantic concepts, it isreasonable to consider the issue of MWE ex-traction as an issue of identifying word bun-dles depicting single semantic units.
The maindifficulty facing such an approach is that veryfew reliable automatic tools available foridentifying lexical semantic units.
However, asemantic field annotator, USAS, has beenbuilt in Lancaster University.
Although it wasnot built aiming to the MWE extraction, wethought it might be very well suited for thispurpose.
Our experiment shows that theUSAS tagger is indeed an efficient tool forMWE extraction.Nevertheless, the current semantic taggerdoes not provide a complete solution to theproblem.
During our experiment, we foundthat not all of the multiword units it collectsare valid MWEs.
An efficient algorithm isneeded for distinguishing between free wordcombinations and relatively fixed, closelyaffiliated word bundles.ReferencesDouglas Biber, Susan Conrad and Viviana Cortes.2003.
Lexical bundles in speech and writing: aninitial taxonomy.
In A. Wilson, P. Rayson andT.
McEnery (eds.)
Corpus Linguistics by theLune: a festschrift for Geoffrey Leech, pp.
71-92.
Peter Lang, Frankfurt.Paul Clough, Robert Gaizauskas and S. L. Piao.2002.
Building and annotating a corpus for thestudy of journalistic text reuse.
In Proceedingsof the 3rd International Conference on Lan-guage Resources and Evaluation (LREC-2002),pp.
1678-1691.
Los Palmas de Gran Canaria,Spain.Ido Dagan, and Ken Church.
1994.
Termight:identifying and translating technical terminol-ogy.
In Proceedings of the 4th Conference onApplied Natural Language Processing, pp.
34-40.
Stuttgard, German.B?atrice Daille.
1995.
Combined approach forterminology extraction: lexical statistics andlinguistic filtering.
Technical paper.
UCREL,Lancaster University.Robert Gaizauskas, Jonathan Foster, YorickWilks, John Arundel, Paul Clough and ScottPiao.
2001.
The METER corpus: a corpus foranalysing journalistic text reuse.
In the Pro-ceedings of the Corpus Linguistics 2001, pp:214-223.
Lancaster, UK.William Gale, Kenneth Church, and DavidYarowsky.
1992.
One sense per discourse.
InProceedings of the 4th DARPA Speech andNatural Language Workshop, pp 233-7.Roger Garside and Nick Smith.
1997.
A hybridgrammatical tagger: CLAWS4.
In R. Garside,G.
Leech and A. McEnery (eds.
), CorpusAnnotation: Linguistic Information fromComputer Text Corpora, pp.
102-121.
Long-man, London.
Roger Garside and Paul Rayson.
1997.
Higher-level annotation tools.
In.
Roger Garside, Geof-frey Leech, and Tony McEnery (eds.)
CorpusAnnotation: Linguistic Information from Com-puter Text Corpora, pp.
179 - 193.
Longman,London.Tom McArthur.
1981.
Longman Lexicon ofContemporary English.
Longman, London.Tony McEnery, Lang?
Jean-Marc, Oakes Michaeland V?ronis Jean.
1997.
The exploitation ofmultilingual annotated corpora for term extrac-tion.
In Garside Roger, Leech Geoffrey andMcEnery Anthony (eds), Corpus annotation ---linguistic information from computer text cor-pora, pp 220-230.
London & New York, Long-man.Magnus Merkel and Mikael Andersson.
2000.Knowledge-lite extraction of multi-word unitswith language filters and entropy thresholds.
InProceedings of 2000 Conference User-OrientedContent-Based Text and Image Handling(RIAO'00), pages 737--746, Paris, France.Archibald Michiels and Nicolas Dufour.1998.DEFI, a tool for automatic multi-word unit rec-ognition, meaning assignment and translationselection.
In Proceedings of the First Interna-tional Conference on Language Resources &Evaluation, pp.
1179-1186.
Granada, Spain.Scott Songlin Piao and Tony McEnery.
2001.Multi-word unit alignment in English-Chineseparallel corpora.
In the Proceedings of the Cor-pus Linguistics 2001, pp.
466-475.
Lancaster,UK.Ivan A.
Sag, Francis Bond, Ann Copestake andDan Flickinger.
2001a.
Multiword Expressions.LinGO Working Paper No.
2001-01.
StanfordUniversity, CA.Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake and Dan Flickinger.
2001b.
Multi-word Expressions: A Pain in the Neck for NLP.LinGO Working Paper No.
2001-03.
StanfordUniversity, CA.Frank Smadja.
1993.
Retrieving collocations fromtext: Xtract.
Computational Linguistics19(1):143-177.Michael Stubbs.
1996.
Text and corpus analysis:computer-assisted studies of language and cul-ture.
Blackwell, Oxford.Elena Tognini-Bonelli.
2001.
Corpus linguistics atwork.
Benjamins, The Netherlands.Eric Wehrli.
1998.
Translating idioms.
In Pro-ceedings of COLING-ACL ?98, Montreal, Can-ada, Vol.
2, pp.
1388-1392.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
Computational Linguistics 23(3): 377-401.
