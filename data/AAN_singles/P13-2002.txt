Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7?11,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsExact Maximum Inference for the Fertility Hidden Markov ModelChris QuirkMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USAchrisq@microsoft.comAbstractThe notion of fertility in word alignment(the number of words emitted by a sin-gle state) is useful but difficult to model.Initial attempts at modeling fertility usedheuristic search methods.
Recent ap-proaches instead use more principled ap-proximate inference techniques such asGibbs sampling for parameter estimation.Yet in practice we also need the single bestalignment, which is difficult to find us-ing Gibbs.
Building on recent advances indual decomposition, this paper introducesan exact algorithm for finding the sin-gle best alignment with a fertility HMM.Finding the best alignment appears impor-tant, as this model leads to a substantialimprovement in alignment quality.1 IntroductionWord-based translation models intended to modelthe translation process have found new uses iden-tifying word correspondences in sentence pairs.These word alignments are a crucial training com-ponent in most machine translation systems.
Fur-thermore, they are useful in other NLP applica-tions, such as entailment identification.The simplest models may use lexical infor-mation alone.
The seminal Model 1 (Brownet al, 1993) has proved very powerful, per-forming nearly as well as more complicatedmodels in some phrasal systems (Koehn et al,2003).
With minor improvements to initializa-tion (Moore, 2004) (which may be important(Toutanova and Galley, 2011)), it can be quitecompetitive.
Subsequent IBM models includemore detailed information about context.
Models2 and 3 incorporate a positional model based onthe absolute position of the word; Models 4 and5 use a relative position model instead (an Englishword tends to align to a French word that is nearbythe French word aligned to the previous Englishword).
Models 3, 4, and 5 all incorporate a no-tion of ?fertility?
: the number of French words thatalign to any English word.Although these latter models covered a broadrange of phenomena, estimation techniques andMAP inference were challenging.
The au-thors originally recommended heuristic proce-dures based on local search for both.
Such meth-ods work reasonably well, but can be computation-ally inefficient and have few guarantees.
Thus,many researchers have switched to the HMMmodel (Vogel et al, 1996) and variants with moreparameters (He, 2007).
This captures the posi-tional information in the IBM models in a frame-work that admits exact parameter estimation infer-ence, though the objective function is not concave:local maxima are a concern.Modeling fertility is challenging in the HMMframework as it violates the Markov assump-tion.
Where the HMM jump model considers onlythe prior state, fertility requires looking acrossthe whole state space.
Therefore, the standardforward-backward and Viterbi algorithms do notapply.
Recent work (Zhao and Gildea, 2010) de-scribed an extension to the HMM with a fertilitymodel, using MCMC techniques for parameter es-timation.
However, they do not have a efficientmeans of MAP inference, which is necessary inmany applications such as machine translation.This paper introduces a method for exact MAPinference with the fertility HMM using dual de-composition.
The resulting model leads to sub-stantial improvements in alignment quality.72 HMM alignmentLet us briefly review the HMM translation modelas a starting point.
We are given a sequence ofEnglish words e = e1, .
.
.
, eI .
This model pro-duces distributions over French word sequencesf = f1, .
.
.
, fJ and word alignment vectors a =a1, .
.
.
, aJ , where aj ?
[0..J ] indicates the En-glish word generating the jth French word, 0 rep-resenting a special NULL state to handle systemat-ically unaligned words.Pr(f ,a|e) = p(J |I)J?j=1p(aj |aj?1) p(fj?
?eaj)The generative story begins by predicting the num-ber of words in the French sentence (hence thenumber of elements in the alignment vector).
Thenfor each French word position, first the alignmentvariable (English word index used to generate thecurrent French word) is selected based on only theprior alignment variable.
Next the French word ispredicted based on its aligned English word.Following prior work (Zhao and Gildea, 2010),we augment the standard HMM with a fertility dis-tribution.Pr(f ,a|e) =p(J |I)I?i=1p(?i|ei)J?j=1p(aj |aj?1) p(fj?
?eaj)(1)where ?i =?Jj=1 ?
(i, aj) indicates the number oftimes that state j is visited.
This deficient modelwastes some probability mass on inconsistent con-figurations where the number of times that a statei is visited does not match its fertility ?i.
Follow-ing in the footsteps of older, richer, and wiser col-leagues (Brown et al, 1993),we forge ahead un-concerned by this complication.2.1 Parameter estimationOf greater concern is the exponential complex-ity of inference in this model.
For the standardHMM, there is a dynamic programming algorithmto compute the posterior probability over wordalignments Pr(a|e, f).
These are the sufficientstatistics gathered in the E step of EM.The structure of the fertility model violates theMarkov assumptions used in this dynamic pro-gramming method.
However, we may empiricallyestimate the posterior distribution using Markovchain Monte Carlo methods such as Gibbs sam-pling (Zhao and Gildea, 2010).
In this case,we make some initial estimate of the a vector,potentially randomly.
We then repeatedly re-sample each element of that vector conditionedon all other positions according to the distribu-tion Pr(aj |a?j , e, f).
Given a complete assign-ment of the alignment for all words except the cur-rent, computing the complete probability includ-ing transition, emission, and jump, is straightfor-ward.
This estimate comes with a computationalcost: we must cycle through all positions of thevector repeatedly to gather a good estimate.
Inpractice, a small number of samples will suffice.2.2 MAP inference with dual decompositionDual decomposition, also known as Lagrangianrelaxation, is a method for solving complexcombinatorial optimization problems (Rush andCollins, 2012).
These complex problems are sepa-rated into distinct components with tractable MAPinference procedures.
The subproblems are re-peatedly solved with some communication overconsistency until a consistent and globally optimalsolution is found.Here we are interested in the problem of find-ing the most likely alignment of a sentence paire, f .
Thus, we need to solve the combinatorial op-timization problem argmaxa Pr(f ,a|e).
Let usrewrite the objective function as follows:h(a) =I?i=1?
?log p(?i|ei) +?j,aj=ilog p(fj |ei)2?
?+J?j=1(log p(aj |aj?1) +log p(fj?
?eaj)2)Because f is fixed, the p(J |I) term is constant andmay be omitted.
Note how we?ve split the opti-mization into two portions.
The first captures fer-tility as well as some component of the translationdistribution, and the second captures the jump dis-tribution and the remainder of the translation dis-tribution.Our dual decomposition method follows thissegmentation.
Define ya as ya(i, j) = 1 if aj = i,and 0 otherwise.
Let z ?
{0, 1}I?J be a binary8u(0)(i, j) := 0 ?i ?
1..I, j ?
1..Jfor k = 1 to Ka(k) := argmaxa(f(a) +?i,j u(k?1)(i, j)ya(i, j))z(k) := argmaxz(g(z)?
?i,j u(k?1)(i, j)z(i, j))if ya = zreturn a(k)end ifu(k)(i, j) := u(k)(i, j) + ?k(ya(k)(i, j)?
z(k)(i, j))end forreturn a(K)Figure 1: The dual decomposition algorithm forthe fertility HMM, where ?k is the step size at thekth iteration for 1 ?
k ?
K, and K is the maxnumber of iterations.matrix.
Define the functions f and g asf(a) =J?j=1(log p(aj |aj?1) +12 log p(fj?
?eaj))g(z) =I?i=1(log p(?
(zi)|ei) +J?j=1z(i, j)2 log p(fj |ei))Then we want to findargmaxa,zf(a) + g(z)subject to the constraints ya(i, j) = z(i, j)?i, j.Note how this recovers the original objective func-tion when matching variables are found.We use the dual decomposition algorithmfrom Rush and Collins (2012), reproducedhere in Figure 1.
Note how the langrangianadds one additional term word, scaled by avalue indicating whether that word is alignedin the current position.
Because it is onlyadded for those words that are aligned, wecan merge this with the log p(fj?
?eaj) termsin both f and g. Therefore, we can solveargmaxa(f(a) +?i,j u(k?1)(i, j)ya(i, j))us-ing the standard Viterbi algorithm.The g function, on the other hand, does not havea commonly used decomposition structure.
Luck-ily we can factor this maximization into pieces thatallow for efficient computation.
Note that g sumsover arbitrary binary matrices.
Unlike the HMM,where each French word must have exactly oneEnglish generator, this maximization allows eachz(i, j) := 0 ?
(i, j) ?
[1..I]?
[1..J ]v := 0for i = 1 to Ifor j = 1 to Jx(j) := (log p(fj |ei) , j)end forsort x in descending order by first componentmax := log p(?
= 0|ei) , arg := 0, sum := 0for f = 1 to Jsum := sum+ x[f, 1]if sum+ log p(?
= f |ei) > maxmax := sum+ log p(?
= f |ei)arg := fend ifend forv := v +maxfor f = 1 to argz(i, x[f, 2]) := 1end forend forreturn z, vFigure 2: Algorithm for finding the arg max andmax of g, the fertility-related component of thedual decomposition objective.French word to have zero or many generators.
Be-cause assignments that are in accordance betweenthis model and the HMM will meet the HMM?sconstraints, the overall dual decomposition algo-rithm will return valid assignments, even thoughindividual selections for this model may fail tomeet the requirements.As the scoring function g can be decomposedinto a sum of scores for each row?i gi (i.e., thereare no interactions between distinct rows of thematrix) we can maximize each row independently:maxzI?i=1gi(zi) =I?i=1maxzgi(zi)Within each row, we seek the best of all 2J pos-sible configurations.
These configurations maybe grouped into equivalence classes based on thenumber of non-zero entries.
In each class, themax assignment is the one using words with thehighest log probabilities; the total score of this as-signment is the sum those log probabilities andthe log probability of that fertility.
Sorting thescores of each cell in the row in descending or-der by log probability allows for linear time com-putation of the max for each row.
The algorithmdescribed in Figure 2 finds this maximal assign-ment in O(IJ log J) time, generally faster thanthe O(I2J) time used by Viterbi.We note in passing that this maximizer is pick-ing from an unconstrained set of binary matri-9ces.
Since each English word may generate asmany French words as it likes, regardless of allother words in the sentence, the underlying ma-trix have many more or many fewer non-zero en-tries than there are French words.
A straightfor-ward extension to the algorithm of Figure 2 returnsonly z matrices with exactly J nonzero entries.Rather than maximizing each row totally indepen-dently, we keep track of the best configurationsfor each number of words generated in each row,and then pick the best combination that sums to J :another straightforward exercise in dynamic pro-gramming.
This refinement does not change thecorrectness of the dual decomposition algorithm;rather it speeds the convergence.3 Fertility distribution parametersOriginal IBM models used a categorical distribu-tion of fertility, one such distribution for each En-glish word.
This gives EM a great amount of free-dom in parameter estimation, with no smoothingor parameter tying of even rare words.
Prior workaddressed this by using the single parameter Pois-son distribution, forcing infrequent words to sharea global parameter estimated from the fertility ofall words in the corpus (Zhao and Gildea, 2010).We explore instead a feature-rich approach toaddress this issue.
Prior work has exploredfeature-rich approaches to modeling the transla-tion distribution (Berg-Kirkpatrick et al, 2010);we use the same technique, but only for the fertil-ity model.
The fertility distribution is modeled asa log-linear distribution of F , a binary feature set:p(?|e) ?
exp (?
?
F (e, ?)).
We include a simpleset of features:?
A binary indicator for each fertility ?.
Thisfeature is present for all words, acting assmoothing.?
A binary indicator for each word id and fer-tility, if the word occurs more than 10 times.?
A binary indicator for each word length (inletters) and fertility.?
A binary indicator for each four letter wordprefix and fertility.Together these produce a distribution that canlearn a reasonable distribution not only for com-mon words, but also for rare words.
Includingword length information aids in for languages withcompounding: long words in one language maycorrespond to multiple words in the other.Algorithm AER (G?E) AER (E?G)HMM 24.0 21.8FHMM Viterbi 19.7 19.6FHMM Dual-dec 18.0 17.4Table 1: Experimental results over the 120 evalu-ation sentences.
Alignment error rates in both di-rections are provided here.4 EvaluationWe explore the impact of this improved MAP in-ference procedure on a task in German-Englishword alignment.
For training data we use the newscommentary data from the WMT 2012 translationtask.1 120 of the training sentences were manuallyannotated with word alignments.The results in Table 1 compare several differ-ent algorithms on this same data.
The first line isa baseline HMM using exact posterior computa-tion and inference with the standard dynamic pro-gramming algorithms.
The next line shows the fer-tility HMM with approximate posterior computa-tion from Gibbs sampling but with final alignmentselected by the Viterbi algorithm.
Clearly fertil-ity modeling is improving alignment quality.
Theprior work compared Viterbi with a form of localsearch (sampling repeatedly and keeping the max),finding little difference between the two (Zhao andGildea, 2010).
Here, however, the difference be-tween a dual decomposition and Viterbi is signifi-cant: their results were likely due to search error.5 Conclusions and future workWe have introduced a dual decomposition ap-proach to alignment inference that substantiallyreduces alignment error.
Unfortunately the algo-rithm is rather slow to converge: after 40 iterationsof the dual decomposition, still only 55 percentof the test sentences have converged.
We are ex-ploring improvements to the simple sub-gradientmethod applied here in hopes of finding faster con-vergence, fast enough to make this algorithm prac-tical.
Alternate parameter estimation techniquesappear promising given the improvements of dualdecomposition over sampling.
Once the perfor-mance issues of this algorithm are improved, ex-ploring hard EM or some variant thereof mightlead to more substantial improvements.1www.statmt.org/wmt12/translation-task.html10ReferencesTaylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,John DeNero, and Dan Klein.
2010.
Painless un-supervised learning with features.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 582?590, LosAngeles, California, June.
Association for Compu-tational Linguistics.Peter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:parameter estimation.
Computational Linguistics,19(2):263?311.Xiaodong He.
2007.
Using word-dependent transitionmodels in HMM-based word alignment for statisti-cal machine translation.
In Proceedings of the Sec-ond Workshop on Statistical Machine Translation,pages 80?87, Prague, Czech Republic, June.
Asso-ciation for Computational Linguistics.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedingsof the 2003 Human Language Technology Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics.Robert C. Moore.
2004.
Improving ibm word align-ment model 1.
In Proceedings of the 42nd Meet-ing of the Association for Computational Linguistics(ACL?04), Main Volume, pages 518?525, Barcelona,Spain, July.Alexander M Rush and Michael Collins.
2012.
A tuto-rial on dual decomposition and lagrangian relaxationfor inference in natural language processing.
Jour-nal of Artificial Intelligence Research, 45:305?362.Kristina Toutanova and Michel Galley.
2011.
Whyinitialization matters for ibm model 1: Multiple op-tima and non-strict convexity.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 461?466, Portland, Oregon, USA, June.Association for Computational Linguistics.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statisticaltranslation.
In COLING.Shaojun Zhao and Daniel Gildea.
2010.
A fast fertil-ity hidden markov model for word alignment usingMCMC.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Process-ing, pages 596?605, Cambridge, MA, October.
As-sociation for Computational Linguistics.11
