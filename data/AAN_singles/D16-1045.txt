Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 469?478,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsThe Structured Weighted Violations Perceptron AlgorithmRotem Dror and Roi ReichartFaculty of Industrial Engineering and Management, Technion, IIT{rtmdrr@campus|roiri@ie}.technion.ac.ilAbstractWe present the Structured Weighted ViolationsPerceptron (SWVP) algorithm, a new struc-tured prediction algorithm that generalizes theCollins Structured Perceptron (CSP, (Collins,2002)).
Unlike CSP, the update rule of SWVPexplicitly exploits the internal structure of thepredicted labels.
We prove the convergenceof SWVP for linearly separable training sets,provide mistake and generalization bounds,and show that in the general case these boundsare tighter than those of the CSP special case.In synthetic data experiments with data drawnfrom an HMM, various variants of SWVPsubstantially outperform its CSP special case.SWVP also provides encouraging initial de-pendency parsing results.1 IntroductionThe structured perceptron ((Collins, 2002), hence-forth denoted CSP) is a prominent training algo-rithm for structured prediction models in NLP, dueto its effective parameter estimation and simple im-plementation.
It has been utilized in numerous NLPapplications including word segmentation and POStagging (Zhang and Clark, 2008), dependency pars-ing (Koo and Collins, 2010; Goldberg and Elhadad,2010; Martins et al, 2013), semantic parsing (Zettle-moyer and Collins, 2007) and information extrac-tion (Hoffmann et al, 2011; Reichart and Barzilay,2012), if to name just a few.Like some training algorithms in structured pre-diction (e.g.
structured SVM (Taskar et al, 2004;Tsochantaridis et al, 2005), MIRA (Crammer andSinger, 2003) and LaSo (Daume?
III and Marcu,2005)), CSP considers in its update rule the differ-ence between complete predicted and gold standardlabels (Sec.
2).
Unlike others (e.g.
factored MIRA(McDonald et al, 2005b; McDonald et al, 2005a)and dual-loss based methods (Meshi et al, 2010)) itdoes not exploit the structure of the predicted label.This may result in valuable information being lost.Consider, for example, the gold and predicted de-pendency trees of Figure 1.
The substantial differ-ence between the trees may be mostly due to the dif-ference in roots (are and worse, respectively).
Pa-rameter update w.r.t this mistake may thus be moreuseful than an update w.r.t the complete trees.In this work we present a new perceptron algo-rithm with an update rule that exploits the struc-ture of a predicted label when it differs from thegold label (Section 3).
Our algorithm is called TheStructured Weighted Violations Perceptron (SWVP)as its update rule is based on a weighted sum of up-dates w.r.t violating assignments and non-violatingassignments: assignments to the input example, de-rived from the predicted label, that score higher (forviolations) and lower (for non-violations) than thegold standard label according to the current model.Our concept of violating assignment is based onHuang et al (2012) that presented a variant of theCSP algorithm where the argmax inference problemis replaced with a violation finding function.
Theirupdate rule, however, is identical to that of the CSPalgorithm.
Importantly, although CSP and the abovevariant do not exploit the internal structure of thepredicted label, they are special cases of SWVP.In Section 4 we prove that for a linearly separabletraining set, SWVP converges to a linear separator of469the data under certain conditions on the parametersof the algorithm, that are respected by the CSP spe-cial case.
We further prove mistake and generaliza-tion bounds for SWVP, and show that in the generalcase the SWVP bounds are tighter than the CSP?s.In Section 5 we show that SWVP allows ag-gressive updates, that exploit only violating assign-ments derived from the predicted label, and morebalanced updates, that exploit both violating andnon-violating assignments.
In experiments with syn-thetic data generated by an HMM, we demonstratethat various SWVP variants substantially outper-form CSP training.
We also provide initial encour-aging dependency parsing results, indicating the po-tential of SWVP for real world NLP applications.2 The Collins Structured PerceptronIn structured prediction the task is to find a mappingf : X ?
Y , where y ?
Y is a structured objectrather than a scalar, and a feature mapping ?
(x, y) :X ?
Y(x) ?
Rd is given.
In this work we denoteY(x) = {y?|y?
?
DY Lx}, where Lx, a scalar, is thesize of the allowed output sequence for an input xandDY is the domain of y?i for every i ?
{1, .
.
.
Lx}.1 Our results, however, hold for the general case ofan output space with variable size vectors as well.The CSP algorithm (Algorithm 1) aims to learna parameter (or weight) vector w ?
Rd, that sepa-rates the training data, i.e.
for each training example(x, y) it holds that: y = arg maxy?
?Y(x) w ??
(x, y?
).To find such a vector the algorithm iterates overthe training set examples and solves the above in-ference (argmax) problem.
If the inferred labely?
differs from the gold label y the update w =w + ??
(x, y, y?)
is performed.
For linearly separa-ble training data (see definition 4), CSP is proved toconverge to a vector w separating the training data.Collins and Roark (2004) and Huang et al (2012)expanded the CSP algorithm by proposing variousalternatives to the argmax inference problem whichis often intractable in structured prediction problems(e.g.
in high-order graph-based dependency parsing(McDonald and Pereira, 2006)).
The basic idea is re-placing the argmax problem with the search for a vi-olation: an output label that the model scores higher1In the general case Lx is a set of output sizes, which maybe finite or infinite (as in constituency parsing (Collins, 1997)).Algorithm 1 The Structured Perceptron (CSP)Input: data D = {xi, yi}ni=1, feature mapping ?Output: parameter vector w ?
RdDefine: ??
(x, y, z) , ?
(x, y)?
?
(x, z)1: Initialize w = 0.2: repeat3: for each (xi, yi) ?
D do4: y?
= arg maxy?
?Y(xi)w ?
?
(xi, y?
)5: if y?
6= yi then6: w = w + ??
(xi, yi, y?
)7: end if8: end for9: until Convergencethan the gold standard label.
The update rule in theseCSP variants is, however, identical to the CSP?s.
We,in contrast, propose a novel update rule that exploitsthe internal structure of the model?s prediction re-gardless of the way this prediction is generated.3 The Structured Weighted ViolationsPerceptron (SWVP)SWVP exploits the internal structure of a predictedlabel y?
6= y for a training example (x, y) ?
D,by updating the weight vector with respect to sub-structures of y?.
We start by presenting the funda-mental concepts at the basis of our algorithm.3.1 Basic ConceptsSub-structure Sets We start with two fundamen-tal definitions: (1) An individual sub-structure of astructured object (or label) y ?
DY Lx , denoted withJ , is defined to be a subset of indexes J ?
[Lx];2and (2) A set of substructures for a training example(x, y), denoted with JJx, is defined as JJx ?
2[Lx].Mixed Assignment We next define the concept ofa mixed assignment:Definition 1.
For a training pair (x, y) and a pre-dicted label y?
?
Y(x), y?
6= y, a mixed assign-ment (MA) vector denoted as mJ(y?, y) is definedwith respect to J ?
JJx as follows:mJk (y?, y) ={y?k k ?
Jyk elseThat is, a mixed assignment is a new label, de-rived from the predicted label y?, that is identical toy?
in all indexes in J and to y otherwise.
For sim-plicity we denote mJ(y?, y) = mJ when the refer-ence y?
and y labels are clear from the context.2We use the notation [n] = {1, 2, .
.
.
n}.470Consider, for example, the trees of Figure 1, as-suming that the top tree is y, the middle tree is y?and J = [2, 5].3 In the mJ(y?, y) (bottom) tree theheads of all the words are identical to those of the toptree, except for the heads of mistakes and of then.Violation The next central concept is that of a vi-olation, originally presented by Huang et al (2012):Definition 2.
A triple (x, y, y?)
is said to be a vio-lation with respect to a training example (x, y) anda parameter vector w if for y?
?
Y(x) it holds thaty?
6= y and w ???
(x, y, y?)
?
0.The SWVP algorithm distinguishes betweenMAs that are violations, and ones that are not.
Fora triplet (x, y, y?)
and a set of substructures JJx ?2[Lx] we provide the following notations:I(y?, y, JJx)v = {J ?
JJx|mJ 6= y,w???
(x, y,mJ) ?
0}I(y?, y, JJx)nv = {J ?
JJx|mJ 6= y,w???
(x, y,mJ) > 0}This notation divides the set of substructures intotwo subsets, one consisting of the substructuresthat yield violating MAs and one consisting of thesubstructures that yield non-violating MAs.
Hereagain when the reference label y?
and the setJJx are known we denote: I(y?, y, JJx)v = Iv,I(y?, y, JJx)nv = Inv and I = Iv ?
Inv.Weighted Violations The key idea of SWVP isthe exploitation of the internal structure of the pre-dicted label in the update rule.
For this aim at eachiteration we define the set of substructures, JJx, andthen, for each J ?
JJx, update the parameter vector,w, with respect to the mixed assignments, MAJ ?s.This is a more flexible setup compared to CSP, aswe can update with respect to the predicted output(if it is a violation, as is promised if inference is per-formed via argmax), if we wish to do so, as well aswith respect to other mixed assignments.Naturally, not all mixed assignments are equallyimportant for the update rule.
Hence, we weigh thedifferent updates using a weight vector ?.
This pa-per therefore extends the observation of Huang et al(2012) that perceptron parameter update can be per-formed w.r.t violations (Section 2), by showing thatw can actually be updated w.r.t linear combinationsof mixed assignments, under certain conditions onthe selected weights.3We index the dependency tree words from 1 onwards.Some mistakes are worse than others.Some mistakes are worse than others.Some mistakes are worse than others.Figure 1: Example parse trees: gold tree (y, top), predicted tree(y?, middle) with arcs differing from the gold?s marked with adashed line, and mJ(y?, y) for J = [2, 5] (bottom tree).3.2 AlgorithmWith these definitions we can present the SWVP al-gorithm (Algorithm 2).
SWVP is in fact a familyof algorithms differing with respect to two decisionsthat can be made at each pass over each training ex-ample (x, y): the choice of the set JJx and the im-plementation of the SETGAMMA function.SWVP is very similar to CSP except for in theupdate rule.
Like in CSP, the algorithm iterates overthe training data examples and for each example itfirst predicts a label according to the current param-eter vector w (inference is discussed in Section 4.2,property 2).
The main difference from CSP is in theupdate rule (lines 6-12).
Here, for each substructurein the substructure set, J ?
JJx, the algorithm gen-erates a mixed assignment mJ (lines 7-9).
Then, wis updated with a weighted sum of the mixed assign-ments (line 11), unlike in CSP where the update isheld w.r.t the predicted assignment only.The ?
(mJ) weights assigned to each of the??
(x, y,mJ) updates are defined by a SETGAMMAfunction (line 10).
Intuitively, a ?
(mJ) weightshould be higher the more the mixed assignmentis assumed to convey useful information that canguide the update of w in the right direction.
In Sec-tion 4 we detail the conditions on SETGAMMA un-der which SWVP converges, and in Section 5 wedescribe various SETGAMMA implementations.Going back to the example of Figure 1, one wouldassume (Sec.
1) that the head word prediction forworse is pivotal to the substantial difference betweenthe two top trees (UAS of 0.2).
CSP does not directlyexploit this observation as it only updates its param-eter vector with respect to the differences betweencomplete assignments: w = w + ??
(x, y, z).In contrast, SWVP can exploit this observation invarious ways.
For example, it can generate a mixed471assignment for each of the erroneous arcs where allother words are assigned their correct arc (accordingto the gold tree) except for that specific arc whichis kept as in the bottom tree.
Then, higher weightscan be assigned to errors that seem more central thanothers.
We elaborate on this in the next two sections.Algorithm 2 The Structured Weighted Violations PerceptronInput: data D = {xi, yi}ni=1, feature mapping ?Output: parameter vector w ?
RdDefine: ??
(x, y, z) , ?
(x, y)?
?
(x, z)1: Initialize w = 0.2: repeat3: for each (xi, yi) ?
D do4: y?
= arg maxy?
?Y(xi)w ?
?
(xi, y?
)5: if y?
6= yi then6: Define: JJxi ?
2[Lxi ]7: for J ?
JJxi do8: Define: mJ s.t.
mJk ={y?k k ?
Jyik else9: end for10: ?
= SETGAMMA()11: w = w + ?J?Iv?Inv?
(mJ )??
(xi, yi,mJ )12: end if13: end for14: until Convergence4 TheoryWe start this section with the convergence conditionson the ?
vector which weighs the mixed assignmentupdates in the SWVP update rule (line 11).
Then,using these conditions, we describe the relation be-tween the SWVP and the CSP algorithms.
Afterthat, we prove the convergence of SWVP and anal-yse the derived properties of the algorithm.?
Selection Conditions Our main observation inthis section is that SWVP converges under two con-ditions: (a) the training set D is linearly separable;and (b) for any parameter vector w achievable bythe algorithm, there exists (x, y) ?
D with JJx ?2[Lx], such that for the predicted output y?
6= y,SETGAMMA returns a ?
weight vector that respectsthe ?
selection conditions defined as follows:Definition 3.
The ?
selection conditions for theSWVP algorithm are (I = Iv ?
Inv):(1)?J?I?
(mJ) = 1.
?
(mJ) ?
0, ?J ?
I.
(2) w ??J?I?(mJ)??
(xi, yi,mJ) ?
0.With this definition we are ready to prove the fol-lowing property.SWVP Generalizes the CSP Algorithm We nowshow that the CSP algorithm is a special case ofSWVP.
CSP can be derived from SWVP when tak-ing: JJx = {[Lx]}, and ?
(m[Lx]) = 1 for every(x, y) ?
D. With these parameters, the ?
selectionconditions hold for every w and y?.
Condition (1)holds trivially as there is only one ?
coefficient and itis equal to 1.
Condition (2) holds as y?
= m[Lx] andhence I = {[Lx]} and w ?
?J?I??
(x, y,mJ) ?
0.4.1 Convergence for Linearly Separable DataHere we give the theorem regarding the convergenceof the SWVP in the separable case.
We first define:Definition 4.
A data set D = {xi, yi}ni=1 is linearlyseparable with margin ?
> 0 if there exists somevector u with ?u?2 = 1 such that for all i:u ???
(xi, yi, z) ?
?,?z ?
Y(xi).Definition 5.
The radius of a data set D ={xi, yi}ni=1 is the minimal scalar R s.t for all i:???
(xi, yi, z)?
?
R, ?z ?
Y(xi).We next extend these definitions:Definition 6.
Given a data set D = {xi, yi}ni=1 anda set JJ = {JJxi ?
2[Lxi ]|(xi, yi) ?
D}, D islinearly separable w.r.t JJ , with margin ?JJ > 0if there exists a vector u with ?u?2 = 1 suchthat: u ?
??
(xi, yi,mJ(z, yi)) ?
?JJ for all i, z ?Y(xi), J ?
JJxi .Definition 7.
The mixed assignment radius w.r.t JJof a data set D = {xi, yi}ni=1 is a constant RJJ s.tfor all i it holds that:???
(xi, yi,mJ(z, yi))?
?
RJJ , ?z ?
Y(xi), J ?
JJxi .With these definitions we can make the followingobservation (proof in A):Observation 1.
For linearly separable data D anda set JJ , every unit vector u that separates the datawith margin ?, also separates the data with respect tomixed assignments with JJ , with margin ?JJ ?
?.Likewise, it holds that RJJ ?
R.We can now state our convergence theorem.While the proof of this theorem resembles that ofthe CSP (Collins, 2002), unlike the CSP proof theSWVP proof relies on the ?
selection conditions pre-sented above and on the Jensen inequality.472Theorem 1.
For any dataset D, linearly separablewith respect to JJ with margin ?JJ > 0, the SWVPalgorithm terminates after t ?
(RJJ )2(?JJ )2 steps, whereRJJ is the mixed assignment radius of D w.r.t.
JJ .Proof.
Let wt be the weight vector before the tthupdate, thus w1 = 0.
Suppose the tth update occurson example (x, y), i.e.
for the predicted output y?it holds that y?
6= y.
We will bound ?wt+1?2 fromboth sides.First, it follows from the update rule of the algorithmthat: wt+1 = wt + ?J?Iv?Inv?(mJ)??
(x, y,mJ).For simplicity, in this proof we will use the notationIv ?
Inv = I .
Hence, multiplying each side of theequation by u yields:u ?
wt+1 = u ?
wt + u ??J?I?(mJ)??
(x, y,mJ)= u ?
wt +?J?I?
(mJ)u ???
(x, y,mJ)?
u ?
wt +?J?I?
(mJ)?JJ (margin property)?
u ?
wt + ?JJ ?
.
.
.
?
t?JJ .The last inequality holds because ?J?I ?
(mJ) =1.
From this we get that ?wt+1?2 ?
(?JJ)2t2 since?u?=1.
Second,?wt+1?2 = ?wt +?J?I?(mJ)??
(x, y,mJ)?2= ?wt?2 + ??J?I?(mJ)??
(x, y,mJ)?2+ 2wt ??J?I?(mJ)??
(x, y,mJ).From ?
selection condition (2) we get that:?wt+1?2 ?
?wt?2 + ??J?I?(mJ)??
(x, y,mJ)?2?
?wt?2 +?J?I?(mJ)???
(x, y,mJ)?2?
?wt?2 + (RJJ)2.
(radius property)The inequality one before the last results from theJensen inequality which holds due to (a) ?
selectioncondition (1); and (b) the squared norm function be-ing convex.
From this we finally get:?wt+1?2 ?
?wt?2 + (RJJ)2 ?
.
.
.
?
t(RJJ)2.Combining the two steps we get:(?JJ)2t2 ?
?wt+1?2 ?
t(RJJ)2.From this it is easy to derive the upper bound in thetheorem: t ?
(RJJ )2(?JJ )2 .4.2 Convergence PropertiesWe next point on three properties of the SWVP al-gorithm, derived from its convergence proof:Property 1 (tighter iterations bound) The con-vergence proof of CSP (Collins, 2002) is given fora vector u that linearly separates the data, with mar-gin ?
and for a data radius R. Following observation1, it holds that in our case, u also linearly separatesthe data with respect to mixed assignments with aset JJ and with margin ?JJ ?
?.
Together with thedefinition of RJJ ?
R we get that: (RJJ )2(?JJ )2 ?
R2?2 .This means that the bound on the number of updatesmade by SWVP is tighter than the bound of CSP.Property 2 (inference) From the ?
selection con-ditions it holds that any label from which at least oneviolating MA can be derived through JJx is suitablefor an update.
This is because in such a case we canchoose, for example, a SETGAMMA function thatassigns the weight of 1 to that MA, and the weightof 0 to all other MAs.Algorithm 2 employs the argmax inference func-tion, following the basic reasoning that it is a goodchoice to base the parameter update on.
Importantly,if the inference function is argmax and the algorithmperforms an update (y?
6= y), this means that y?, theoutput of the argmax function, is a violating MA bydefinition.
However, it is obvious that solving the in-ference problem and the optimal ?
assignment prob-lems jointly may result in more informed parameter(w) updates.
We leave a deeper investigation of thisissue to future research.Property 3 (dynamic updates) The ?
selec-tion conditions paragraph states two conditions ((a)and (b)) under which the convergence proof holds.While it is trivial for SETGAMMA to generate a ?vector that respects condition (a), if there is a pa-rameter vector w?
achievable by the algorithm forwhich SETGAMMA cannot generate ?
that respectscondition (b), SWVP gets stuck when reaching w?.This problem can be solved with dynamic up-dates.
A deep look into the convergence proof re-veals that the set JJx and the SETGAMMA func-tion can actually differ between iterations.
Whilethis will change the bound on the number of it-erations, it will not change the fact that the algo-rithm converges if the data is linearly separable.This makes SWVP highly flexible as it can always473back off to the CSP setup of JJx = {[Lx]}, and?
(x, y) ?
D : ?
(m[Lx]) = 1, update its parametersand continue with its original JJ and SETGAMMAwhen this option becomes feasible.
If this does nothappen, the algorithm can continue till convergencewith the CSP setup.4.3 Mistake and Generalization BoundsThe following bounds are proved: the number ofupdates in the separable case (see Theorem 1); thenumber of mistakes in the non-separable case (seeAppendix B); and the probability to misclassify anunseen example (see supplementary material).
It canbe shown that in the general case these bounds aretighter than those of the CSP special case.
We nextdiscuss variants of SWVP.5 Passive Aggressive SWVPHere we present types of update rules that can beimplemented within SWVP.
Such rule types are de-fined by: (a) the selection of ?, which should respectthe ?
selection conditions (see Definition 3) and (b)the selection of JJ = {JJx ?
2[Lx]|(x, y) ?
D},the substructure sets for the training examples.?
Selection A first approach we consider is the ag-gressive approach4 where only mixed assignmentsthat are violations {mJ : J ?
Iv} are exploited(i.e.
for all J ?
Inv, ?
(mJ) = 0).
Note, that inthis case condition (2) of the ?
selection conditionstrivially holds as: w ?
?J?Iv?(mJ)??
(x, y,mJ) ?
0.The only remaining requirement is that condition (1)also holds, i.e.
that?J?Iv ?
(mJ) = 1.The opposite, passive approach, exploits onlynon-violating MA?s {mJ : J ?
Inv}.
How-ever, such ?
assignments do not respect ?selection condition (2), as they yield: w ?
?J?Inv ?(mJ)??
(x, y,mJ) ?
0 which holds ifand only if for every J ?
Inv, ?
(mJ) = 0 that inturn contradicts condition (1).Finally, we can take a balanced approach whichgives a positive ?
coefficient for at least one violat-ing MA and at least one positive ?
coefficient fora non-violating MA.
This approach is allowed bySWVP as long as both ?
selection conditions hold.4We borrow the term passive-aggressive from (Crammer etal., 2006), despite the substantial difference between the works.We implemented two weighting methods, bothbased on the concept of margin:(1) Weighted Margin (WM): ?
(mJ) =|w???
(x,y,mJ )|??J??JJx|w???(x,y,mJ?
)|?
(2) Weighted Margin Rank (WMR):?
(mJ) =(|JJx|?r|JJx|)?
.
where r is therank of |w ?
??
(x, y,mJ(y?, y))| among the|w ???
(x, y,mJ ?
(y?, y))| values for J ?
?
JJx.Both schemes were implemented twice, within abalanced approach (denoted as B) and an aggressiveapproach (denoted as A).5 The aggressive schemesrespect both ?
selection conditions.
The balancedschemes, however, respect the first condition but notnecessarily the second.
Since all models that employthe balanced weighting schemes converged after atmost 10 iterations, we did not impose this condition(which we could do by, e.g., excluding terms for J ?Inv till condition (2) holds).JJ Selection Another choice that strongly affectsthe updates made by SWVP is that of JJ .
A choiceof JJx = 2[Lx], for every (x, y) ?
D results in anupdate rule which considers all possible mixing as-signments derived from the predicted label y?
andthe gold label y.
Such an update rule, however, re-quires computing a sum over an exponential numberof terms (2Lx) and is therefore highly inefficient.Among the wide range of alternative approaches,in this paper we exploit single difference mixed as-signments.
In this approach we define: JJ ={JJx = {{1}, {2}, .
.
.
{Lx}}|(x, y) ?
D}.
For atraining pair (x, y) ?
D, a predicted label y?
andJ = {j} ?
JJx, we will have:mJk (y?, y) ={yk k 6= jy?k k = jUnder this approach for the pair (x, y) ?
D onlyLx terms are summed in the SWVP update rule.We leave a further investigation of JJ selection ap-proaches to future research.6 ExperimentsSynthetic Data We experiment with syn-thetic data generated by a linear-chain, first-5For the aggressive approach the equations for schemes(1) and (2) are changed such that JJx is replaced withI(y?, y, JJx)v .474order Hidden Markov Model (HMM, (Ra-biner and Juang, 1986)).
Our learning al-gorithm is a liner-chain conditional randomfield (CRF, (Lafferty et al, 2001)): P (y|x) =1Z(x)?i=1:Lx exp(w ?
?
(yi?1, yi, x)) (where Z(x)is a normalization factor) with binary indicator fea-tures {xi, yi, yi?1, (xi, yi), (yi, yi?1), (xi, yi, yi?1)}for the triplet (yi, yi?1, x).A dataset is generated by iteratively sampling Kitems, each is sampled as follows.
We first sam-ple a hidden state, y1, from a uniform prior distri-bution.
Then, iteratively, for i = 1, 2, .
.
.
, Lx wesample an observed state from the emission prob-ability and (for i < Lx) a hidden state from thetransition probability.
We experimented in 3 setups.In each setup we generated 10 datasets that weresubsequently divided to a 7000 items training set,a 2000 items development set and a 1000 items testset.
In all datasets, for each item, we set Lx = 8.We experiment in three conditions: (1) simple(++),learnable(+++), (2) simple(++), learnable(++) and(3) simple(+), learnable(+).6For each dataset (3 setups, 10 datasets per setup)we train variants of the SWVP algorithm differing inthe ?
selection strategy (WM or WMR, Section 5),being aggressive (A) or passive (B), and in their ?parameter (?
= {0.5, 1, .
.
.
, 5}).
Training is doneon the training subset and the best performing vari-ant on the development subset is applied to the testsubset.
For CSP no development set is employedas there is no hyper-parameter to tune.
We reportaveraged accuracy (fraction of observed states forwhich the model successfully predicts the hiddenstate value) across the test sets, together with thestandard deviation.Dependency Parsing We also report initial de-pendency parsing results.
We implemented our algo-rithms within the TurboParser (Martins et al, 2013).6Denoting Dx = [Cx], Dy = [Cy], and a permuta-tion of a vector v with perm(v), the parameters of the dif-ferent setups are: (1) simple(++), learnable(+++): Cx =5, Cy = 3, P (y?|y) = perm(0.7, 0.2, 0.1), P (x|y) =perm(0.75, 0.1, 0.05, 0.05, 0.05).
(2) simple(++), learn-able(++): Cx = 5, Cy = 3, P (y?|y) = perm(0.5, 0.3, 0.2),P (x|y) = perm(0.6, 0.15, 0.1, 0.1, 0.05).
(3) sim-ple(+), learnable(+): Cx = 20 , Cy = 7 ,P (y?|y) = perm(0.7, 0.2, 0.1, 0, .
.
.
, 0)), P (x|y) =perm(0.4, 0.2, 0.1, 0.1, 0.1, 0, .
.
.
, 0).That is, every other aspect of the parser: featureset, probabilistic pruning algorithm, inference algo-rithm etc., is kept fixed but training is performedwith SWVP.
We compare our results to the parserperformance with CSP training (which comes withthe standard implementation of the parser).We experiment with the datasets of the CoNLL2007 shared task on multilingual dependency pars-ing (Nilsson et al, 2007), for a total of 9 languages.We followed the standard train/test split of thesedataset.
For SWVP, we randomly sampled 1000 sen-tences from each training set to serve as develop-ment sets and tuned the parameters as in the syn-thetic data experiments.
CSP is trained on the train-ing set and applied to the test set without any devel-opment set involved.
We report the Unlabeled At-tachment Score (UAS) for each language and model.7 ResultsSynthetic Data Table 1 presents our results.
In allthree setups an SWVP algorithm is superior.
Av-eraged accuracy differences between the best per-forming algorithms and CSP are: 3.72 (B-WMR,(simple(++), learnable(+++))), 5.29 (B-WM, (sim-ple(++), learnable(++))) and 5.18 (A-WM, (sim-ple(+), learnable(+))).
In all setups SWVP outper-forms CSP in terms of averaged performance (ex-cept from B-WMR for (simple(+), learnable(+))).Moreover, the weighted models are more stable thanCSP, as indicated by the lower standard deviationof their accuracy scores.
Finally, for the more sim-ple and learnable datasets the SWVP models outper-form CSP in the majority of cases (7-10/10).We measure generalization from development totest data in two ways.
First, for each SWVP algo-rithm we count the number of times its ?
parame-ter results in an algorithm that outperforms the CSPon the development set but not on the test set (notshown in the table).
Of the 120 comparisons re-ported in the table (4 SWVP models, 3 setups, 10comparisons per model/setup combination) this hap-pened once (A-MV, (simple(++), learnable(+++)).Second, we count the number of times the best de-velopment set value of the ?
hyper-parameter is alsothe best value on the test set, or the test set accu-racy with the best development set ?
is at most 0.5%lower than that with the best test set ?.
The Gener-475simple(++), learnable(+++) simple(++), learnable(++) simple(+), learnable(+)Model Acc.
(std) # Wins Gener.
Acc.
(std) # Wins Gener.
Acc.
(std) # Wins Gener.B-WM 75.47(3.05) 9/10 10/10 63.18 (1.32) 9/10 10/10 28.48 (1.9) 5/10 10/10B-WMR 75.96 (2.42) 8/10 10/10 63.02 (2.49) 9/10 10/10 24.31 (5.2) 4/10 10/10A-WM 74.18 (2.16) 7/10 10/10 61.65 (2.30) 9/10 10/10 30.45 (1.0) 6/10 10/10A-WMR 75.17 (3.07) 7/10 10/10 61.02 (1.93) 8/10 10/10 25.8 (3.18) 2/10 10/10CSP 72.24 (3.45) NA NA 57.89 (2.85) NA NA 25.27(8.55) NA NATable 1: Overall Synthetic Data Results.
A- and B- denote an aggressive and a balanced approaches, respectively.
Acc.
(std) isthe average and the standard deviation of the accuracy across 10 test sets.
# Wins is the number of test sets on which the SWVPalgorithm outperforms CSP.
Gener.
is the number of times the best ?
hyper-parameter value on the development set is also the bestvalue on the test set, or the test set accuracy with the best development set ?
is at most 0.5% lower than that with the best test set ?.First Order Second OrderLanguage CSP B-WM Top B-WM Test B-WM CSP B-WM Top B-WM Test B-WMEnglish 86.34 86.4 86.7 86.7 88.02 87.82 87.82 87.92Chinese 84.60 84.5 85.04 85.05 86.82 86.69 86.83 87.02Arabic 79.09 79.17 79.21 79.21 76.07 75.94 76.09 76.09Greek 80.41 80.20 80.28 80.28 80.31 80.40 80.40 80.61Italian 84.63 84.64 84.74 84.70 84.03 84.08 84.15 84.28Turkish 83.05 82.89 82.89 82.89 83.02 83.04 83.04 83.31Basque 79.47 79.54 79.54 79.54 80.52 80.57 80.63 80.64Catalan 88.51 88.46 88.50 88.5 88.71 88.81 88.81 88.82Hungarian 80.17 80.07 80.07 80.21 80.61 80.45 80.45 80.55Average 83.69 83.65 83.77 83.79 83.12 83.08 83.13 83.35Table 2: First and second order dependency parsing UAS results for CSP trained models, as well as for models trained with SWVPwith a balanced ?
selection (B) and with a weighted margin (WM) strategy.
For explanation of the B-WM, Top B-WM, and TestB-WM see text.
For each language and parsing order we highlight the best result in bold font, but this do not include results fromTest B-WM as it is provided only as an upper bound on the performance of SWVP.alization column of the table shows that this has nothappened in all of the 120 runs of SWVP.Dependency Parsing Results are given in Table2.
For the SWVP trained models we report threenumbers: (a) B-WM is the standard setup where the?
hyper parameter is tuned on the development data;(b) For Top B-WM we first selected the models witha UAS score within 0.1% of the best developmentdata result, and of these we report the UAS of themodel that performs best on the test set; and (c) TestB-WM reports results when ?
is tuned on the test set.This measure provides an upper bound on SWVPwith our simplistic JJ (Section 5).Our results indicate the potential of SWVP.
De-spite our simple JJ set, Top B-WM and Test B-WMimprove over CSP in 5/9 and 6/9 cases in first orderparsing, respectively, and in 7/9 cases in second or-der parsing.
In the latter case, Test B-WM improvesthe UAS over CSP in 0.22% on average across lan-guages.
Unfortunately, SWVP still does not gener-alize well from train to test data as indicated, e.g., bythe modest improvements B-WM achieves over CSPin only 5 of 9 languages in second order parsing.8 ConclusionsWe presented the Structured Weighted ViolationsPerceptron (SWVP) algorithm, a generalization ofthe Structured Perceptron (CSP) algorithm that ex-plicitly exploits the internal structure of the pre-dicted label in its update rule.
We proved the conver-gence of the algorithm for linearly separable trainingsets under certain conditions on its parameters, andprovided generalization and mistake bounds.In experiments we explored only very simple con-figurations of the SWVP parameters - ?
and JJ .Nevertheless, several of our SWVP variants out-performed the CSP special case in synthetic dataexperiments.
In dependency parsing experiments,SWVP demonstrated some improvements over CSP,but these do not generalize well.
While we find theseresults somewhat encouraging, they emphasize theneed to explore the much more flexible ?
and JJselection strategies allowed by SWVP (Sec.
4.2).
Infuture work we will hence develop ?
and JJ selec-tion algorithms, where selection is ideally performedjointly with inference (property 2, Sec.
4.2), to makeSWVP practically useful in NLP applications.476A Proof Observation 1.Proof.
For every training example (x, y) ?
D, itholds that: ?z?Y(x)mJ(z, y) ?
Y(x).
As u sepa-rates the data with margin ?, it holds that:u ???
(x, y,mJ(z, y)) ?
?JJx , ?z ?
Y(x), J ?
JJx.u ???
(x, y, z) ?
?, ?z ?
Y(x).Therefore also ?JJx ?
?.
As the last inequal-ity holds for every (x, y) ?
D we get that ?JJ =min(x,y)?D ?JJx ?
?.From the same considerations it holds that RJJ ?R.
This is because RJJ is the radius of a sub-set of the dataset with radius R (proper subset if?
(x, y) ?
D, [Lx] /?
JJx, non-proper subset oth-erwise).B Mistake Bound - Non Separable CaseHere we provide a mistake bound for the algorithmin the non-separable case.
We start with the follow-ing definition and observation:Definition 8.
Given an example (xi, yi) ?
D, for au, ?
pair define:ri = u ?
?
(xi, yi)?
maxz?Y(xi)u ?
?
(xi, z)i = max{0, ?
?
ri}riJJ = u ?
?
(xi, yi)?maxz?Y(xi),J?JJxiu ?
?
(xi,mJ(z, yi))Finally define: Du,?
=?n?i=12iObservation 2.
For all i: ri ?
riJJ .Observation 2 easily follows from Definition 8.Following this observation we denote: rdiff =mini{riJJ ?
ri} ?
0 and present the next theorem:Theorem 2.
For any training sequence D, for thefirst pass over the training set of the CSP and theSWVP algorithms respectively, it holds that:#mistakes?
CSP ?
minu:?u?=1,?>0(R+Du,?
)2?2 .#mistakes?
SWV P ?
minu:?u?=1,?>0(RJJ +Du,?)2(?
+ rdiff )2 .As RJJ ?
R (Observation 1) and rdiff ?
0,we get a tighter bound for SWVP.
The proof for#mistakes-CSP is given at (Collins, 2002).
Theproof for #mistakes-SWVP is given below.Proof.
We transform the representation ?
(x, y) ?Rd into a new representation ?
(x, y) ?
Rd+n as fol-lows: for i = 1, ..., d : ?i(x, y) = ?i(x, y), forj = 1, ..., n : ?d+j(x, y) = ?
if (x, y) = (xj , yj)and 0 otherwise, where ?
> 0 is a parameter.Given a u, ?
pair define v ?
Rd+n as follows: fori = 1, ..., d : vi = ui, for j = 1, ..., n : vd+j = j?
.Under these definitions we have:v ?
?
(xi, yi)?
v ?
?
(xi, z) ?
?, ?i, z ?
Y(xi).For every i, z ?
Y(xi), J ?
JJxi :v ?
?
(xi, yi)?
v ?
?
(xi,mJ(z, yi)) ?
?
+ rdiff .??
(xi, yi)?
?
(xi,mJ(z, yi))?2 ?
(RJJ)2 + ?2.Last, we have,?v?2 = ?u?2 +n?i=12i?2 = 1 +D2u,?
?2 .We get that the vector v?v?
linearly separates thedata with respect to single decision assignments withmargin ??1+D2U,??2.
Likewise, v?v?
linearly separatesthe data with respect to mixed assignments with JJ ,with margin ?+rdiff?1+Du,??2.
Notice that the first passof SWVP with representation ?
is identical to thefirst pass with representation ?
because the param-eter weight for the additional features affects only asingle example of the training data and do not affectthe classification of test examples.
By theorem 1 thismeans that the first pass of SWVP with representa-tion ?
makes at most ((RJJ )2+?2)(?+rdiff )2 ?
(1 + D2u,?
?2).We minimize this w.r.t ?, which gives: ?
=?RJJDu,?, and obtain the result guaranteed in thetheorem.AcknowledgmentsThe second author was partly supported by a re-search grant from the GIF Young Scientists?
Pro-gram (No.
I-2388-407.6/2015): Syntactic Parsingin Context.477ReferencesMichael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proc.
ofACL.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proc.
of ACL, pages16?23.Michael Collins.
2002.
Discriminative training methodsfor hidden markov models: Theory and experimentswith perceptron algorithms.
In Proc.
of EMNLP, pages1?8.Koby Crammer and Yoram Singer.
2003.
Ultraconser-vative online algorithms for multiclass problems.
TheJournal of Machine Learning Research, 3:951?991.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
The Journal of Machine Learn-ing Research, 7:551?585.Hal Daume?
III and Daniel Marcu.
2005.
Learning assearch optimization: Approximate large margin meth-ods for structured prediction.
In Proc.
of ICML, pages169?176.Yoav Freund and Robert E Schapire.
1999.
Large marginclassification using the perceptron algorithm.
Machinelearning, 37(3):277?296.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Proc.
of NAACL-HLT 2010, pages742?750.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S Weld.
2011.
Knowledge-based weak supervision for information extraction ofoverlapping relations.
In Proc.
of ACL.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Proc.of NAACL-HLT, pages 142?151.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proc.
of ACL, pages 1?11.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proc.
of ICML.Andre?
FT Martins, Miguel Almeida, and Noah A Smith.2013.
Turning on the turbo: Fast third-order non-projective turbo parsers.
In Prc.
of ACL short papers,pages 617?622.Ryan T McDonald and Fernando CN Pereira.
2006.
On-line learning of approximate dependency parsing algo-rithms.
In Proc.
of EACL.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005a.
Online large-margin training of dependencyparsers.
In Proc.
of ACL, pages 91?98.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005b.
Non-projective dependency parsingusing spanning tree algorithms.
In Proc.
of EMNLP-HLT, pages 523?530.Ofer Meshi, David Sontag, Tommi Jaakkola, and AmirGloberson.
2010.
Learning efficiently with approxi-mate inference via dual losses.
In Proc.
of ICML.Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007.The conll 2007 shared task on dependency parsing.In Proceedings of the CoNLL shared task session ofEMNLP-CoNLL, pages 915?932.
sn.Lawrence Rabiner and Biing-Hwang Juang.
1986.
Anintroduction to hidden markov models.
ASSP Maga-zine, IEEE, 3(1):4?16.Roi Reichart and Regina Barzilay.
2012.
Multi eventextraction guided by global constraints.
In Proc.
ofNAACL-HLT 2012, pages 70?79.Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004.Max-margin markov networks.
In Proc.
of NIPS.Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-mann, and Yasemin Altun.
2005.
Large marginmethods for structured and interdependent output vari-ables.
In Journal of Machine Learning Research,pages 1453?1484.Luke S Zettlemoyer and Michael Collins.
2007.
Onlinelearning of relaxed ccg grammars for parsing to logicalform.
In Proc.
of EMNLP-CoNLL, pages 678?687.Yue Zhang and Stephen Clark.
2008.
Joint word seg-mentation and pos tagging using a single perceptron.In proc.
of ACL, pages 888?896.478
