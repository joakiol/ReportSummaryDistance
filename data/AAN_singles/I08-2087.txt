A Structured Prediction Approach for Statistical Machine TranslationDakun Zhang*          Le Sun?
Wenbo Li**Institute of Software, Graduate UniversityChinese Academy of SciencesBeijing, China, 100080{dakun04,liwenbo02}@iscas.cn?Institute of SoftwareChinese Academy of SciencesBeijing, China, 100080sunle@iscas.cnAbstractWe propose a new formally syntax-basedmethod for statistical machine translation.Transductions between parsing trees aretransformed into a problem of sequencetagging, which is then tackled by a search-based structured prediction method.
Thisallows us to automatically acquire transla-tion knowledge from a parallel corpuswithout the need of complex linguisticparsing.
This method can achieve compa-rable results with phrase-based method(like Pharaoh), however, only about tenpercent number of translation table is used.Experiments show that the structured pre-diction approach for SMT is promising forits strong ability at combining words.1 IntroductionStatistical Machine Translation (SMT) is attract-ing more attentions than rule-based and example-based methods because of the availability of largetraining corpora and automatic techniques.
How-ever, rich language structure is difficult to be inte-grated in the current SMT framework.
Most of theSMT approaches integrating syntactic structuresare based on probabilistic tree transducers (tree-to-tree model).
This leads to a large increase in themodel complexity (Yamada and Knight 2001;Yamada and Knight 2002; Gildea 2003; Galley etal.
2004; Knight and Graehl 2005; Liu et al 2006).However, formally syntax-based methods proposesimple but efficient ways to parse and translatesentences (Wu 1997; Chiang 2005).In this paper, we propose a new model of SMTby using structured prediction to perform tree-to-tree transductions.
This model is inspired by Sa-gae and Lavie (2005), in which a stack-based rep-resentation of monolingual parsing trees is used.Our contributions lie in the extension of this rep-resentation to bilingual parsing trees based onITGs and in the use of a structured predictionmethod, called SEARN (Daum?
III et al 2007), topredict parsing structures.Furthermore, in order to facilitate the use ofstructured prediction method, we perform anothertransformation from ITG-like trees to label se-quence with the grouping of stack operations.Then the structure preserving problem in transla-tion is transferred to a structured prediction onetackled by sequence labeling method such as inPart-of-Speech (POS) tagging.
This transforma-tion can be performed automatically without com-plex linguistic information.
At last, a modifiedsearch process integrating structure information isperformed to produce sentence translation.
Figure1 illustrates the process flow of our model.
Be-sides, the phrase extraction is constrained by ITGs.Therefore, in this model, most units are wordbased except that we regard those complex wordalignments as a whole (i.e.
phrase) for the simplic-ity of ITG-like tree representations.B ilingual S en tencesG IZ A + +  T ra in ing(B id irec tiona l)W ord  A lignm en ts(g row -d iag -fina l)S truc tu red  Info rm ation(T ra in ing  by  S E A R N )L anguage  M odelM ono lingualS en tencesSearch  e*M ax im ize  P r(e)*P r(f|e )Inpu tSource L anguageS en tenceO utpu tT arge t L anguageS en tenceS tack -based  O pera tionsT rans la tion  M odelIT G -like  T reesFigure 1: Chart of model frameworkThe paper is organized as follows: related workis show in section 2.
The details of the transforma-649tion from word alignments to structured parsingtrees and then to label sequence are given in sec-tion 3.
The structured prediction method is de-scribed in section 4.
In section 5, a beam searchdecoder with structured information is described.Experiments are given for three European lan-guage pairs in section 6 and we conclude our pa-per with some discussions.2 Related WorkThis method is similar to block-orientation model-ing (Tillmann and Zhang 2005) and maximumentropy based phrase reordering model (Xiong etal.
2006), in which local orientations (left/right) ofphrase pairs (blocks) are learned via MaxEnt clas-sifiers.
However, we assign shift/reduce labelingof ITGs taken from the shift-reduce parsing, andclassifier is learned via SEARN.
This paper ismore elaborated by assigning detailed stack-operations.The use of structured prediction to SMT is alsoinvestigated by (Liang et al 2006; Tillmann andZhang 2006; Watanabe et al 2007).
In contrast,we use SEARN to estimate one bilingual parsingtree for each sentence pair from its word corre-spondences.
As a consequence, the generation oftarget language sentences is assisted by this struc-tured information.Turian et al (2006) propose a purely discrimi-native learning method for parsing and translationwith tree structured models.
The word alignmentsand English parse tree were fed into the GenParsystem (Burbank et al 2005) to produce binarizedtree alignments.
In our method, we predict treestructures from word alignments through severaltransformations without involving parser and/ortree alignments.3 Transformation3.1 Word Alignments and ITG-like TreeFirst, following Koehn et al (2003), bilingual sen-tences are trained by GIZA++ (Och and Ney 2003)in two directions (from source to target and targetto source).
Then, two resulting alignments are re-combined to form a whole according to heuristicrules, e.g.
grow-diag-final.
Second, based on theword alignment matrix, one unique parsing treecan be generated according to ITG constraintswhere the ?left-first?
constraint is posed.
That is tosay, we always make the leaf nodes as the rightsons as possible as they can.
Here we present twobasic operations for mapping tree items, one is inorder and the other is in reverse order (see Figure2).
Basic word alignments are in (a), while (b) istheir corresponding alignment matrix.
They can bedescribed using ITG-like trees (c).f1 f1       f2e1        *e2                  * f1/e1 f2/e2(1a) (1b) (1c)f1       f2e1                  *e2        * f1/e2 f2/e1(2a) (2b) (2c)f1/e1 Sf2/e2 S,R+(1d)f1/e2 Sf2/e1 S,R-(2d)f2f1 f2e1 e2e1 e2Figure 2: Two basic representations for tree itemsFigure 3: ?inside-out?
transpositions (a) and (b) with twotypical complex sequences (c) and (d).
In (c) and (d), wordcorrespondence f2-e2 is also extracted as sub-alignments.The two widely known situations that cannot bedescribed by ITGs are called ?inside-out?
transpo-sitions (Figure 3 a & b).
Since they cannot be de-composed in ITGs, we consider them as basicunits.
In this case, phrase alignment is used.
In ourmodel, more complex situations exist for the wordcorrespondences are generated automatically fromGIZA++.
At the same time, we also keep the sub-alignments in those complex situations in order toextend the coverage of translation options.
Thesub-alignments are restricted to those that can bedescribed by the two basic operations.
In otherwords, for our ITG-like tree, the nodes are mostlyword pairs, except some indecomposable wordsequences pairs.
Figure 3 shows four typical com-plex sequences viewed as phrases.Therefore, our ITG-like trees take some phrasealignments into consideration and we also keepthe sub-alignments in these situations.
Tree itemsin our model are restricted to minimum constitu-ents for the simplicity of parsing tree generation.Then we extract those word pairs from tree items,instead of all the possible word sequences, as ourtranslation table.
In this way, we can greatly re-duce the number of translation pairs to be consid-eration.6503.2 SHIFT and REDUCE OperationsSagae and Lavie (2005) propose a constituency-based parsing method to determine sentence de-pendency structures.
This method is simple andefficient, which makes use of SHIFT and RE-DUCE operations within a stack framework.
Thiskind of representations can be easily learned by aclassifier with linear time complexity.In their method, they build a parse tree of a sen-tence one word at a time just as in a stack parser.At any time step, they either shift a new word onto the stack, or reduce the top two elements on thestack into a new non-terminal.Sagae and Lavie?s algorithms are designed formonolingual parsing problem.
We extend it torepresent our ITG-like tree.
In our problem, eachword pairs can be viewed as tree items (nodes).To handle our tree alignment problem, we need todefine two REDUCE operations: REDUCE inorder and REDUCE in reverse order.
We definethese three basic operations as follows:?
S: SHIFT - push the current item onto thestack.?
R+: REDUCE in order - pop the first twoitems from the stack, and combine them inthe original order on the target side, thenpush back.?
R-: REDUCE in reverse order - pop thefirst two items from the stack, and combinethem in the reverse order on the target side,then push back.Using these operators, our ITG-like tree istransformed to serial stack operations.
In Figure 2,(d) is such a representation for the two basicalignments.
Therefore, the structure of wordaligned sentences can be transformed to an opera-tion sequence, which represents the bilingual pars-ing correspondences.After that, we attach these operations to eachcorresponding tree item like a sequence labelingproblem.
We need to perform another ?grouping?step to make sure only one operation is assignedto each item, such as ?S,R+?, ?S,R-,R+?, etc.Then, those grouped operations are regarded as awhole and performed as one label.
The number ofthis kind of labels is decided by the training cor-pus1.
Having defined such labels, the prediction of1 This set of labels is quite small and only 16 for the French-English training set with 688,031 sentences.tree structures is transformed to a label predictionone.
That is, giving word pairs as input, we trans-form them to their corresponding labels (stackoperations) in the output.
At the same time, treetransductions are encoded in those labels.
Once allthe ?labels?
are performed, there should be onlyone element in the stack, i.e.
the generating sen-tence translation pairs.
See Appendix A for a morecomplete example in Chinese-English with ourdefined operations.Another constraint we impose is to keep theleast number of elements in stack at any time.
Iftwo elements on the top of the stack can be com-bined, we combine them to form a single item.This constraint can avoid having too many possi-ble operations for the last word pair, which maymake future predictions difficult.4 Structured PredictionSEARN is a machine learning method proposedrecently by Daum?
III et al (2007) to solve struc-tured prediction problems.
It can produce a highprediction performance without compromisingspeed, simplicity and generality.
By incorporatingthe search and learning process, SEARN can solvethe complex problems without having to performexplicit decoding any more.In most cases, a prediction of input x in domainX into output y in domain Y, like SVM and deci-sion trees, cannot keep the structure informationduring prediction.
SEARN considers this problemas a cost sensitive classification one.
By definingfeatures and a loss function, it performs a costsensitive learning algorithm to learn predictions.During each iteration, the optimal policy (decidedby previous classifiers) generates new trainingexamples through the search space.
These data areused to adjust performance for next classifier.Then, iterations can keep this algorithm to per-form better for prediction tasks.
Structures arepreserved for it integrates searching and learningat the same time.4.1 Parsing Tree PredictionFor our problem, using SEARN to predict thestack-based ITG-like trees, given word alignmentsas input, can benefit from the advantages of thisalgorithm.
With the structured learning method,we can account for the sentence structures andtheir correspondence between two languages at651the same time.
Moreover, it keeps the translatingstructures from source to target.As we have transformed the tree-to-tree transla-tion problem into a sequence labeling one, all weneed to solve is a tagging problem similar to aPOS tagging (Daum?
III et al 2006).
The inputsequence x is word pairs and output y is the groupof SHIFT and REDUCE operations.
For sequencelabeling problem, the standard loss function isHamming distance, which measures the differencebetween the true output and the predicting one:?=ttt yyyyHL )?,()?,( ?
(1)where ?
is 0 if two variables are equal, and 1 oth-erwise.5 DecoderWe use a left-to-right beam search decoder to findthe best translation given a source sentence.
Com-pared with general phrase-based beam search de-coder like Pharaoh (Koehn 2004), this decoderintegrates structured information and does notneed distortion cost and other costs (e.g.
futurecosts) any more.
Therefore, the best translationcan be determined by:})()|({maxarg* )(elengthlmeepefpe ?=     (2)where ?
is a factor of word length penalty.
Simi-larly, the translation probability  can befurther decomposed into:)|( efp?=iii efefp )|()|( ?
(3)and )|( ii ef?
represents the probability distribu-tion of word pairs.Instead of extracting all possible phrases fromword alignments, we consider those translationpairs from the nodes of ITG-like trees only.
LikePharaoh, we calculate their probability as a com-bination of 5 constituents: phrase translation prob-ability (in both directions), lexical translationprobability (in both directions) and phrase penalty(default is set at 2.718).
The corresponding weightis trained through minimum error rate method(Och 2003).
Parameters of this part can be calcu-lated in advance once tree structures are generatedand can be stored as phrase translation table.5.1 Core AlgorithmAnother important question is how to preservesentence structures during decoding.
A left-to-right monotonous search procedure is needed.Giving the source sentence, word translation can-didates can be determined according to the trans-lation table.
Then, several rich features like cur-rent and previous source words are extractedbased on these translation pairs and source sen-tence.
After that, our structured prediction learn-ing method will be used to predict the output ?la-bels?, which produces a bilingual parsing tree.Then, a target output will be generated for the cur-rent partial source sentence as soon as bilingualparsing trees are formed.
The output of this parttherefore contains syntactic information for struc-ture.For instance, given the current source partiallike ?f1 f2?, we can generate their translationword pair sequences with the translation table,like ?f1/e1 f2/e2?, ?f1/e3 f2/e4?
and so on.
Thecorresponding features are then able to be decidedfor the next predicting process.
Once the outputpredictions (i.e.
stack operations) are decided, thebilingual tree structures are formed at the sametime.
As a consequence, results of these opera-tions are the final translations which we reallyneed.At each stage of translation, language modelparameters can be added to adjust the total costsof translation candidates and make the pruningprocess reasonable.
The whole sentence is thenprocessed by incrementally constructing the trans-lation hypotheses.
Lastly, the element in the lastbeam with the minimum cost is the final transla-tion.
In general, the translation process can be de-scribed in the following way:5.2 Recombining and PruningDifferent translation options can combine to formthe same fragment by beam search decoder.
Re-combining is therefore needed here to reduce thesearch space.
So, only the one with the lowest costis kept when several fragments are identical.
Thisrecombination is a risk-free operation to improvesearching efficiency.Another pruning method used in our system ishistogram pruning.
Only n-best translations are652allowed for the same source part in each stack (e.g.n=100).
In contrast with traditional beam searchdecoder, we generate our translation candidatesfrom the same input, instead of all allowed wordpairs elsewhere.
Therefore the pruning is muchmore reasonable for each beam.
There is no rela-tive threshold cut off compared with Pharaoh.In the end, the complexities for decoding arethe main concern of our method.
In practice, how-ever, it will not exceed the  (m forsentence length, N for stack size and Tn for al-lowed translation candidates).
This is based on theassumption that our prediction process (tackled bySEARN) is fed with three features (only one for-mer item is associated), which makes it no need offull sentence predictions at each time.
)**( TnNmO6 ExperimentWe validate our method using the corpus from theshared task on NAACL 2006 workshop for statis-tical machine translation2.
The difference of ourmethod lies in the framework and different phrasetranslation table.
Experiments are carried on allthe three language pairs (French-English, Ger-man-English and Spanish-English) and perform-ances are evaluated by the providing test sets.
Sys-tem parameters are adjusted with developmentdata under minimum error rate training.For SEARN, three features are chosen to use:the current source word, the word before it and thecurrent target word.
As we do not know the realtarget word order before decoding, the corre-sponding target word?s position cannot be used asfeatures.
Besides, we filter the features less than 5times to reduce the training complexities.The classifier we used in the training process isbased on perceptron because of its simplicity andperformance.
We modified Daum?
III?s script3 tofit our method and use the default 5 iterations foreach perceptron-based training and 3 itertaions forSEARN.6.1 Results for different language pairsThe  final  results  of  our  system,  named Amasis,and baseline system Pharaoh (Koehn and Monz2006) for three language pairs are listed in Table 1.The last three lines are the results of Pharaoh withphrase length from 1 to 3.
However, the length of2 http://www.statmt.org/wmt06/shared-task/3 http://www.cs.utah.edu/~hal/searn/SimpleSearn.tgz05000100001500020000kPharaoh 15724573 12667210 19367713Amasis 1522468 1715732 1572069F-E G-E S-EFigure 4: Numbers of translation table0.0%5.0%10.0%15.0%20.0%25.0%30.0%35.0%40.0%Pharaoh 3.7% 5.1% 3.5%Amasis 32.2% 33.0% 36.4%F-E G-E S-EFigure 5: Percent of single word translation pairs (only oneword in the source side)F-E G-E S-EIn Out In Out In OutAmasis 27.44 18.41 23.02 15.97 27.51 23.35Pharaoh1 20.54 14.07 17.53 12.13 23.23 20.24Pharaoh2 27.71 19.41 23.36 15.77 28.88 25.28Pharaoh3 30.01 20.77 24.40 16.58 30.58 26.51Table 1: BLEU scores for different language pairs.
In - In-domain test, Out - Out-of-domain test.phrases for Amasis is determined by ITG-like treenodes and there is no restriction for it.Even without producing higher BLEU scoresthan Pharaoh, our approach is still interesting forthe following reasons.
First, the number of phrasetranslation pairs is greatly reduced in our system.The ratio of translation table number in ourmethod (Amasis) to Pharaoh, for French-Englishis 9.68%, for German-English is 13.54%, forSpanish-English is 8.12% (Figure 4).
This meansthat our method is more efficient at combiningwords and phrases during translation.
The reasonsfor the different ratio for the three languages arenot very clear, maybe are related to the flexibilityof word order of source language.
Second, wecount the single word translation pairs (only oneword in the source side) as shown in Figure 5.There are significantly more single word transla-tions in our method.
However, the translationquality can be kept at the same level under thiscircumstance.
Third, our current experimental re-sults are produced with only three common fea-tures (the corresponding current source and targetword and the last source one) without any linguis-tics information.
More useful features are ex-pected to be helpful like POS tags.
Finally, theperformance can be further improved if we use amore powerful classifier (such as SVM or ME)with more iterations.6537 ConclusionOur method provides a simple and efficient wayto solve the word ordering problem partiallywhich is NP-hard (Knight 1999).
It is word basedexcept for those indecomposable word sequencesunder ITGs.
However, it can achieve comparableresults with phrase-based method (like Pharaoh),while much fewer translation options are used.For the structure prediction process, only 3 com-mon features are preserved and perceptron-basedclassifiers are chosen for the use of simplicity.
Weargue that this approach is promising when morefeatures and more powerful classifiers are used asDaum?
III et al (2007) stated.Our contributions lie in the integration of struc-ture prediction for bilingual parsing trees throughserial transformations.
We reinforce the power offormally syntax-based method by using structuredprediction method to obtain tree-to-tree transduc-tions by the transforming from word alignments toITG-like trees and then to label sequences.
Thus,the sentence structures can be better accounted forduring translating.AcknowledgementsThis work is partially supported by National Natural ScienceFoundation of China under grant #60773027, #60736044 andby ?863?
Key Projects #2006AA010108.
We would like tothank anonymous reviewers for their detailed comments.Appendix A.
A Complete Example in Chinese-Englishwith Our Defined OperationsWord alignmentsITG-like treeSHIFT-REDUCE label sequence?
?/a   S?
?/to learn about  S?
?/Chinese  S,R+?
?/music   S,R+?/?
S,R+?
?/great   S?/?
S,R+?
?/way   S,R+,R-,R+Stack status when operations finish??
??
??
??
?
?
?
?
?
?/ a great way to learn about Chinese musicReferencesA.
Burbank, M. Carpuat, et al 2005.
Final Report of the 2005Language Engineering Workshop on Statistical MachineTranslation by Parsing.
Johns Hopkins UniversityD.
Chiang.
2005.
A Hierarchical Phrase-Based Model forStatistical Machine Translation.
In ACL, pages 263-270.M.
Galley, M. Hopkins, et al 2004.
What's in a translationrule?
In HLT-NAACL, Boston, MA.D.
Gildea.
2003.
Loosely Tree-Based Alignment for MachineTranslation.
In ACL, pages 80-87, Sapporo, Japan.H.
Daum?
III, J. Langford, et al 2007.
Search-based Struc-tured Prediction.
Under review by the Machine LearningJournal.
http://pub.hal3.name/daume06searn.pdf.H.
Daum?
III, J. Langford, et al 2006.
Searn in Practice.http://pub.hal3.name/daume06searn-practice.pdf.K.
Knight.
1999.
Decoding Complexity in Word-Replacement Translation Models.
Computational Linguis-tics 25(4): 607-615.K.
Knight and J. Graehl.
2005.
An Overview of ProbabilisticTree Transducers for Natural Language Processing.
InCICLing, pages 1-24.P.
Koehn.
2004.
Pharaoh: A Beam Search Decoder forPhrase-Based Statistical Machine Translation Models.
InProc.
of AMTA, pages 115-124.P.
Koehn and C. Monz.
2006.
Manual and Automatic Evalua-tion of Machine Translation between European Languages.In Proc.
on the Workshop on Statistical Machine Transla-tion, pages 102-121, New York City.P.
Koehn, F. J. Och, et al 2003.
Statistical Phrase-BasedTranslation.
In HLT-NAACL, pages 127-133.P.
Liang, A. Bouchard, et al 2006.
An End-to-EndDiscriminative Approach to Machine Translation.
In ACL.Y.
Liu, Q. Liu, et al 2006.
Tree-to-String Alignment Tem-plate for Statistical Machine Translation.
In ACL.F.
J. Och.
2003.
Minimum Error Rate Training in StatisticalMachine Translation.
In ACL, pages 160-167.F.
J. Och and H. Ney.
2003.
A Systematic Comparison ofVarious Statistical Alignment Models.
ComputationalLinguistics 29(1): 19-51.K.
Sagae and A. Lavie.
2005.
A Classifier-Based Parser withLinear Run-Time Complexity.
In IWPT, pages 125-132.C.
Tillmann and T. Zhang.
2005.
A Localized PredictionModel for Statistical Machine Translation.
In ACL.C.
Tillmann and T. Zhang.
2006.
A Discriminative GlobalTraining Algorithm for Statistical MT.
in ACL.J.
Turian, B. Wellington, et al 2006.
Scalable DiscriminativeLearning for Natural Language Parsing and Translation.
InProceedings of NIPS, Vancouver, BC.T.
Watanabe, J. Suzuki, et al 2007.
Online Large-MarginTraining for Statistical Machine Translation.
In EMNLP.D.
Wu.
1997.
Stochastic Inversion Transduction Grammarsand Bilingual Parsing of Parallel Corpora.
ComputationalLinguistics 23(3): 377-404.D.
Xiong, Q. Liu, et al 2006.
Maximum Entropy BasedPhrase Reordering Model for Statistical Machine Transla-tion.
In ACL, pages 521-528.K.
Yamada and K. Knight.
2001.
A Syntax-based StatisticalTranslation Model.
In ACL, pages 523-530.K.
Yamada and K. Knight.
2002.
A Decoder for Syntax-based Statistical MT.
In ACL, pages 303-310.654
