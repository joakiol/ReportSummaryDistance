Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 320?331, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsGeneralized Higher-Order Dependency Parsing with Cube PruningHao Zhang Ryan McDonaldGoogle, Inc.{haozhang,ryanmcd}@google.comAbstractState-of-the-art graph-based parsers use fea-tures over higher-order dependencies that relyon decoding algorithms that are slow anddifficult to generalize.
On the other hand,transition-based dependency parsers can eas-ily utilize such features without increasing thelinear complexity of the shift-reduce systembeyond a constant.
In this paper, we attempt toaddress this imbalance for graph-based pars-ing by generalizing the Eisner (1996) algo-rithm to handle arbitrary features over higher-order dependencies.
The generalization is atthe cost of asymptotic efficiency.
To accountfor this, cube pruning for decoding is utilized(Chiang, 2007).
For the first time, label tupleand structural features such as valencies canbe scored efficiently with third-order featuresin a graph-based parser.
Our parser achievesthe state-of-art unlabeled accuracy of 93.06%and labeled accuracy of 91.86% on the stan-dard test set for English, at a faster speed thana reimplementation of the third-order model ofKoo et al2010).1 IntroductionThe trade-off between rich features and exact de-coding in dependency parsing has been well docu-mented (McDonald and Nivre, 2007; Nivre and Mc-Donald, 2008).
Graph-based parsers typically trade-off rich feature scope for exact (or near exact) de-coding, whereas transition-based parsers make theopposite trade-off.
Recent research on both parsingparadigms has attempted to address this.In the transition-based parsing literature, the fo-cus has been on increasing the search space of thesystem at decoding time, as expanding the featurescope is often trivial and in most cases only leads toa constant-time increase in parser complexity.
Themost common approach is to use beam search (Duanet al2007; Johansson and Nugues, 2007; Titov andHenderson, 2007; Zhang and Clark, 2008; Zhangand Nivre, 2011), but more principled dynamic pro-gramming solutions have been proposed (Huang andSagae, 2010).
In all cases inference remains approx-imate, though a larger search space is explored.In the graph-based parsing literature, the mainthrust of research has been on extending the Eisnerchart-parsing algorithm (Eisner, 1996) to incorpo-rate higher-order features (McDonald and Pereira,2006; Carreras, 2007; Koo and Collins, 2010).
Asimilar line of research investigated the use of inte-ger linear programming (ILP) formulations of pars-ing (Riedel and Clarke, 2006; Martins et al2009;Martins et al2010).
Both solutions allow for exactinference with higher-order features, but typically ata high cost in terms of efficiency.
Furthermore, spe-cialized algorithms are required that deeply exploitthe structural properties of the given model.
Upgrad-ing a parser to score new types of higher-order de-pendencies thus requires significant changes to theunderlying decoding algorithm.
This is in stark con-trast to transition-based systems, which simply re-quire the definition of new feature extractors.In this paper, we abandon exact search in graph-based parsing in favor of freedom in feature scope.We propose a parsing algorithm that keeps the back-bone Eisner chart-parsing algorithm for first-orderparsing unchanged.
Incorporating higher-order fea-tures only involves changing the scoring function of320potential parses in each chart cell by expanding thesignature of each chart item to include all the non-local context required to compute features.
The corechart-parsing algorithm remains the same regardlessof which features are incorporated.
To control com-plexity we use cube pruning (Chiang, 2007) with thebeam size k in each cell.
Furthermore, dynamic pro-gramming in the style of Huang and Sagae (2010)can be done by merging k-best items that are equiv-alent in scoring.
Thus, our method is an applica-tion of integrated decoding with a language modelin MT (Chiang, 2007) to dependency parsing, whichhas previously been applied to constituent parsing(Huang, 2008).
However, unlike Huang, we onlyhave one decoding pass and a single trained model,while Huang?s constituent parser maintains a sep-arate generative base model from a following dis-criminative re-ranking model.
We draw connectionsto related work in Section 6.Our chart-based approximate search algorithm al-lows for features on dependencies of an arbitrary or-der ?
as well as over non-local structural proper-ties of the parse trees ?
to be scored at will.
Inthis paper, we use first to third-order features ofgreater varieties than Koo and Collins (2010).
Ad-ditionally, we look at higher-order dependency arc-label features, which is novel to graph-based pars-ing, though commonly exploited in transition-basedparsing (Zhang and Nivre, 2011).
This is becauseadding label tuple features would introduce a largeconstant factor of O(|L|3), where |L| is the size ofthe label set L, into the complexity for exact third-order parsing.
In our formulation, only the top-ranked labelled arcs would survive in each cell.
Asa result, label features can be scored without combi-natorial explosion.
In addition, we explore the useof valency features counting how many modifiers aword can have on its left and right side.
In the past,only re-rankers on k-best lists of parses produced bya simpler model use such features due to the diffi-culty of incorporating them into search (Hall, 2007).The final parser with all these features is both ac-curate and fast.
In standard experiments for English,the unlabeled attachment score (UAS) is 93.06%,and the labeled attachment score (LAS) is 91.86%.The UAS score is state-of-art.
The speed of ourparser is 220 tokens per second, which is over 4times faster than an exact third-order parser that at-Figure 1: Example Sentence.tains UAS of 92.81% and comparable to the state-of-the-art transition-based system of Zhang and Nivre(2011) that employs beam search.2 Graph-based Dependency ParsingDependency parsers produce directed relationshipsbetween head words and their syntactic modifiers.Each word modifies exactly one head, but can haveany number of modifiers itself.
The root of a sen-tence is a designated special symbol which all wordsin the sentence directly or indirectly modify.
Thus,the dependency graph for a sentence is constrainedto be a directed tree.
The directed syntactic rela-tionships, aka dependency arcs or dependencies forshort, can often be labeled to indicate their syntacticrole.
Figure 1 gives an example dependency tree.For a sentence x = x1 .
.
.
xn, dependency pars-ing is the search for the set of head-modifier depen-dency arcs y?
such that y?
= argmaxy?Y(x) f(x, y),where f is a scoring function.
As mentioned before,y?
must represent a directed tree.
|Y(x)| is then theset of valid dependency trees for x and grows ex-ponentially with respect to its length |x|.
We fur-ther define L as the set of possible arc labels and usethe notation (i l??
j) ?
y to indicate that there is adependency from head word xi to modifier xj withlabel l in dependency tree y.In practice, f(x, y) is factorized into scoring func-tions on parts of (x, y).
For example, in first-order dependency parsing (McDonald et al2005),f(x, y) is factored by the individual arcs:y?
= argmaxy?Y(x)f(x, y) = argmaxy?Y(x)?
(i l?
?j)?yf(i l??
j)The factorization of dependency structures into arcsenables an efficient dynamic programming algo-rithm with running time O(|x|3) (Eisner, 1996), forthe large family of projective dependency structures.Figure 2 shows the parsing logic for the Eisneralgorithm.
It has two types of dynamic program-ming states: complete items and incomplete items.321Complete items correspond to half-constituents, andare represented as triangles graphically.
Incompleteitems correspond to dependency arcs, and are repre-sented as trapezoids.
The Eisner algorithm is the ba-sis for the more specialized variants of higher-orderprojective dependency parsing.Second-order sibling models (McDonald andPereira, 2006) score adjacent arcs with a commonhead.
In order to score them efficiently, a new statecorresponding to modifier pairs was introduced tothe chart-parsing algorithm.
Due to the careful fac-torization, the asymptotic complexity of the revisedalgorithm remains O(|x|3).
The resulting scoringfunction is:y?
= argmaxy?Y(x)?
(i l?
?j,i l??
?k)?yf(i l??
j, i l???
k)where (i l??
j, i l???
k) ?
y indicates two adja-cent head-modifier relationships in dependency treey, one from xi to xj with label l and another fromxi to xk with label l?.
Words xj and xk are com-monly referred to as siblings.
In order to maintaincubic parsing complexity, adjacent dependencies arescored only if the modifiers occur on the same sidein the sentence relative to the head.Second-order grandchild models (Carreras, 2007)score adjacent arcs in length-two head-modifierchains.
For example, if word xi modifies word xjwith label l, but itself has a dependency to modi-fier xk with label l?, then we would add a scoringfunction f(j l??
i l???
k).
These are called grand-child models as they can score dependencies be-tween a word and its modifier?s modifiers, i.e., xkis the grandchild of xj in the above example.
Thestates in the Eisner algorithm need to be augmentedwith the indices to the outermost modifiers in orderto score the outermost grandchildren.
The resultingalgorithm becomes O(|x|4).Finally, third-order models (Koo and Collins,2010) score arc triples such as three adjacent sib-ling modifiers, called tri-siblings, or structures look-ing at both horizontal contexts and vertical contexts,e.g., grand-siblings that score a word, its modifierand its adjacent grandchildren.
To accommodatethe scorers for these sub-graphs, even more special-ized dynamic programming states were introduced.The Koo and Collins (2010) factorization enables(a) = +(b) = +Figure 2: Structures and rules for parsing first-order mod-els with the (Eisner, 1996) algorithm.
This shows onlythe construction of right-pointing dependencies and notthe symmetric case of left-pointing dependencies.the scoring of certain types of third-order dependen-cies with O(|x|4) decoder run-time complexity.Each of these higher-order parsing algorithmsmakes a clever factorization for the specific modelin consideration to keep complexity as low as possi-ble.
However, this results in a loss of generality.3 Generalizing Eisner?s AlgorithmIn this section, we generalize the Eisner algorithmwithout introducing new parsing rules.
The general-ization is straight-forward: expand the dynamic pro-gramming state to incorporate feature histories.
Thisis done on top of the two distinct chart items in theO(|x|3) Eisner chart-parsing algorithm (Figure 2).The advantage of this approach is that it maintainsthe simplicity of the original Eisner algorithm.
Un-fortunately, it can increase the run-time complex-ity of the algorithm substantially, but we will em-ploy cube pruning to regain tractability.
Because ourhigher-order dependency parsing algorithm is basedthe Eisner algorithm, it is currently limited to pro-duce projective trees only.3.1 Arbitrary n-th-order dependency parsingWe start with the simplest case of sibling models.
Ifwe want to score sibling arcs, at rule (b) in Figure 2,we can see that the complete item lying betweenthe head and the modifier (the middle of the threeitems) does not contain information about the out-ermost modifier of the head, which is the previousdependency constructed and the sibling to the mod-ifier of the dependency currently being constructed.This fact suggests that, in order to score modifierbigrams, the complete item states should be aug-mented by the outermost modifier.
We can aug-ment the chart items with such information, which322(a) = +(b) = +Figure 3: Structures and rules for parsing models basedon modifier bigrams, with a generalized (Eisner, 1996)algorithm.
Here the dashed arrows indicate additional in-formation stored in each chart-cell.
Specifically the pre-vious modifier in complete chart items.is shown in Figure 3.
It refines the complete itemsby storing the previously constructed dependency tothe outermost modifiers.
Note that now the signatureof the complete items is not simply the end-point in-dexes, but contains the index of the outer modifier.Using this chart item augmentation it is now pos-sible to score both first-order arcs as well as second-order sibling arcs.
In fact, by symmetry, the newdynamic program can also score the leftmost andrightmost grandchildren of a head-modifier pair, inrule (a) and rule (b) respectively.
By counting thenumber of free variables in each parsing rule, wesee that the parsing complexity is O(|x|5), which ishigher than both McDonald and Pereira (2006) andCarreras (2007).
The added complexity comes fromthe fact that it is now possible to score a third-orderdependency consisting of the head, the modifier, thesibling, and the outermost grandchild jointly.We can go further to augment the complete andincomplete states with more parsing history.
Fig-ure 4 shows one possible next step of generaliza-tion.
We generalize the states to keep track of thelatest two modifiers of the head.
As a result, it be-comes possible to score tri-siblings involving threeadjacent modifiers and grand-siblings involving twooutermost grandchildren ?
both of which comprisethe third-order Model 2 of Koo and Collins (2010) ?plus potentially any additional interactions of theseroles.
Figure 5 shows another possible generaliza-tion.
We keep modifier chains up to length two inthe complete states.
The added history enables thecomputation of features for great-grandchildren re-lationships: (h l??
m l???
gc l????
ggc).In general, we can augment the complete and in-complete states with n variables representing the(a) = +(b) = +Figure 4: Structures and rules for parsing models basedon modifier trigrams in horizontal contexts, with a gener-alized (Eisner, 1996) algorithm.
Here the dashed arrowsindicate the previous two modifiers to the head in eachchart item.
(a) = +(b) = +Figure 5: Structures and rules for parsing models basedon modifier trigrams in vertical contexts, with a gener-alized (Eisner, 1996) algorithm.
Here the dashed arrowsindicate the modifier to the head and the modifier?s mod-ifier, forming a modifier chain of length two.possible parsing histories and loop over the crossproduct of the histories in the innermost loop of Eis-ner algorithm.
The cardinality of the cross productis |x|n ?
|x|n.
Thus, the complexity of the algo-rithm augmented by n variables is O(|x|3 ?
|x|2n) =O(|x|3+2n), where n ?
0.
Note that this complexityis for unlabeled parsing.
A factor of |L| for all ora subset of the encoded arcs must be multiplied inwhen predicting labeled parse structures.3.2 History-based dependency parsingThe previous n modifiers, either horizontal or ver-tical, is a potential signature of parsing history.
Wecan put arbitrary signatures of parsing history intothe chart items so that when we score a new item,we can draw the distinguishing power of featuresbased on an arbitrarily deep history.
For example,consider the position of a modifier, which is the po-sition in which it occurs amongst its siblings relativeto the location of the head.
We can store the positionof the last modifier into both chart states.
In com-plete states, this signature tells us the position of theoutermost modifier, which is the valency of the headin the left or right half-constituent.323In the extreme case, we can use full subtrees ashistories, although the cardinality of the set of his-tories would quickly become exponential, especiallywhen one considers label ambiguity.
Regardless, thehigh complexity associated with this generalization,even for second or third-order models, requires us toappeal to approximate search algorithms.3.3 Advantage of the generalizationThe complexity analysis earlier in this section re-veals the advantage of such a generalization scheme.It factorizes a dynamic programming state for de-pendency parsing into two parts: 1) the structuralstate, which consists of the boundaries of incom-plete and complete chart items, and accounts for theO(|x|3) term in the analysis, and 2) the feature his-tory, which is a signature of the internal content of asub-parse and accounts for the O(|x|2n) term.
Therules of the deductive parsing system ?
the Eisner al-gorithm ?
stay the same as long as the structural rep-resentation is unchanged.
To generalize the parserto handle richer features, one can simply enrich thefeature signature and the scoring function withoutchanging the structural state.
A natural grouping ofstates follows where all sub-parses sharing the samechart boundaries are grouped together.
This group-ing will enable the cube pruning in Section 4 for ap-proximate search.There is another advantage of keeping the Eis-ner parsing logic unchanged: derivations one-to-onecorrespond to dependency parse trees.
Augmentingthe complete and incomplete states does not intro-duce spurious ambiguity.
This grouping view is use-ful for proving this point.
Introducing higher orderfeatures in each chart item will cause sub-derivationsto be re-ranked only.
As a result, the final Viterbiparse can differ from the one from the standard Eis-ners algorithm.
But the one-to-one correspondencestill holds.4 Approximate Search with Cube PruningIn machine translation decoding, an n-gram lan-guage model can be incorporated into a translationmodel by augmenting the dynamic programmingstates for the translation model with the boundaryn ?
1 words on the target side.
The complexityfor exact search involves a factor of |x|4n?4 in thehierarchical phrase-based model of Chiang (2007),where |x| is the input sentence length.
The standardtechnique is to force a beam size k on each transla-tion state so that the possible combinations of lan-guage model histories is bounded by k2.
Further-more, if the list of k language model states are sortedfrom the lowest cost to the highest cost, we can as-sume the best combinations will still be among thecombinations of the top items from each list, al-though the incorporation of n-gram features breaksthe monotonic property of the underlying semi-ring.Cube pruning is based on this approximation(Chiang, 2007).
It starts with the combination of thetop items in the lists to be combined.
At each step, itputs the neighbors of the current best combination,which consists of going one position down in one ofthe k-best lists, into a priority queue.
The algorithmstops when k items have been popped off from thequeue.
At the final step, it sorts the popped itemssince they can be out-of-order.
It reduces the combi-nation complexity from O(k2) to O(k ?
log(k)).Our history-augmented parsing is analogous toMT decoding.
The possible higher-order historiescan similarly be limited to at most k in each com-plete or incomplete item.
The core loop of the gener-alized algorithm which has a complexity of O(|x|2n)can similarly be reduced to O(k ?log(k)).
Therefore,the whole parsing algorithm remains O(|x|3) re-gardless how deep we look into parsing history.
Fig-ure 6 illustrates the computation.
We apply rule (b)to combine two lists of augmented complete itemsand keep the combinations with the highest modelscores.
With cube pruning, we only explore cells at(0, 0), (0, 1), (1, 0), (2, 0), and (1, 1), without theneed to evaluate scoring functions for the remainingcells in the table.
Similar computation happens withrule (a).In this example cube pruning does find the high-est scoring combination, i.e., cell (1, 1).
However,note that the scores are not monotonic in the order inwhich we search these cells as non-local features areused to score the combinations.
Thus, cube pruningmay not find the highest scoring combination.
Thisapproximation is at the heart of cube pruning.4.1 RecombinationThe significance of using feature signatures is thatwhen two combinations result in a state with the324identical feature signature the one with the highestscore survives.
This is the core principle of dynamicprogramming.
We call it recombination.
It denotesthe same meaning as state-merging in Huang andSagae (2010) for transition-based parsers.In cube pruning, with recombination, the k-bestitems in each chart cell are locally optimal (in thepruned search space) over all sub-trees with anequivalent state for future combinations.
The cubepruning algorithm without recombination degener-ates to a recursive k-best re-scoring algorithm sinceeach of the k-best items would be unique by itselfas a sub-tree.
It should be noted that by workingon a chart (or a forest, equivalently) the algorithm isalready applying recombination at a coarser level.In machine translation, due to its large searchspace and the abstract nature of an n-gram languagemodel, it is more common to see many sub-treeswith the same language model feature signature,making recombination crucial (Chiang, 2007).
Inconstituent parser reranking (Huang, 2008), recom-bination is less likely to happen since the rerank-ing features capture peculiarities of local tree struc-tures.
For dependency parsing, we hypothesize thatthe higher-order features are more similar to the n-gram language model features in MT as they tend tobe common features among many sub-trees.
But asthe feature set becomes richer, recombination tendsto have a smaller effect.
We will discuss the empiri-cal results on recombination in Section 5.4.5 ExperimentsWe define the scoring function f(x, y) as a linearclassifier between a vector of features and a corre-sponding weight vector, i.e., f(x, y) = w ?
?
(x, y).The feature function ?
decomposes with respect toscoring function f .
We train the weights to optimizethe first-best structure.
We use the max-loss vari-ant of the margin infused relaxed algorithm (MIRA)(Crammer et al2006) with a hamming-loss marginas is common in the dependency parsing literature(Martins et al2009; Martins et al2010).
MIRAonly requires a first-best decoding algorithm, whichin our case is the approximate chart-based parsingalgorithms defined in Sections 3 and 4.
Because ourdecoding algorithm is approximate, this may lead toinvalid updates given to the optimizer (Huang and=0 : 1 : 2 :+0 : f = 2.5 f = 1 f = 21 : f = 1.5 f = 3.2 f = 0.52 : f = 2.3 f = 3 f = 1.8...Figure 6: Combining two lists of complete items withcube pruning.Fayong, 2012).
However, we found that ignoring ormodifying such updates led to negligible differencesin practice.
In all our experiments, we train MIRAfor 8 epochs and use a beam of k = 5 during de-coding.
Both these values were determined on theEnglish development data.5.1 FeaturesThe feature templates we use are drawn from thepast work on graph-based parsing and transition-based parsing.
The base templates for the higher-order dependencies are close to Koo and Collins(2010), with the major exception that our featuresinclude label-tuple information.
The basic featuresinclude identities, part of speech tags, and labels ofthe words in dependency structures.
These atomicfeatures are conjoined with the directions of arcs tocreate composite n-gram features.
The higher-orderdependency features can be categorized into the fol-lowing sub-groups, where we use h to indicate thehead, m the modifier, s the modifier?s sibling and gca grandchild word in a dependency part.?
(labeled) modifier features: (h l??
m)?
(labeled) sibling features: (h l??
m,h l???
s)?
(labeled) outermost grandchild features:(h l??
m l???
gc)?
(labeled) tri-sibling features:(h l??
m,h l???
s, h l????
s2)?
(labeled) grand-sibling features:(h l??
m l???
gc, h l??
m l????
gc2),325?
(labeled) sibling and grandchild conjoined features:(h l??
m,h l???
s,m l????
gc)The general history features include valencies ofwords conjoined with the directions of the dominat-ing arcs.
The positions of the modifiers are also con-joined with the higher-order dependency features inthe previous list.The features that are new compared to Koo andCollins (2010) are the label tuple features, the sib-ling and grandchild conjoined features, and the va-lency features.
We determine this feature set basedon experiments on the development data for English.In Section 5.3 we examine the impact of these newfeatures on parser performance.5.2 Main ResultsOur first set of results are on English dependen-cies.
We used the Penn WSJ Treebank converted todependencies with Penn2Malt1 conversion softwarespecifying Yamada and Matsumoto head rules andMalt label set.
We used the standard splits of thisdata: sections 2-21 for training; section 22 for vali-dation; and section 23 for evaluation.
We evaluatedour parsers using standard labeled accuracy scores(LAS) and unlabeled accuracy scores (UAS) exclud-ing punctuation.
We report run-times in tokens persecond.
Part-of-speech tags are predicted as inputusing a linear-chain CRF.Results are given in Table 1.
We compare ourmethod to a state-of-the-art graph-based parser (Kooand Collins, 2010) as well as a state-of-the-arttransition-based parser that uses a beam (Zhangand Nivre, 2011) and the dynamic programmingtransition-based parser of Huang and Sagae (2010).Additionally, we compare to our own implementa-tion of exact first to third-order graph-based parsingand the transition-based system of Zhang and Nivre(2011) with varying beam sizes.There are a number of points to make.
First,approximate decoding with rich features and cubepruning gives state-of-the-art labeled and unlabeledparsing accuracies relative to previously reported re-sults.
This includes the best graph-based parsingresults of Koo and Collins (2010), which has nearidentical performance, as well as the best beam-based and dynamic-programming-based transition1http://w3.msi.vxu.se/?nivre/research/Penn2Malt.htmlParser UAS LAS Toks/SecHuang and Sagae (2010) 92.1- - -Zhang and Nivre (2011) 92.9- 91.8- -Zhang and Nivre (reimpl.)
(beam=64) 92.73 91.67 760Zhang and Nivre (reimpl.)
(beam=256) 92.75 91.71 190Koo and Collins (2010) 93.04 - -1st-order exact (reimpl.)
91.80 90.50 20702nd-order exact (reimpl.)
92.40 91.12 11103rd-order exact (reimpl.)
92.81 -?
50this paper 93.06 91.86 220Table 1: Comparing this work in terms of parsing accu-racy compared to state-of-the-art baselines on the Englishtest data.
We also report results for a re-implementationof exact first to third-order graph-based parsing and a re-implementation of Zhang and Nivre (2011) in order tocompare parser speed.
?Our exact third-order implemen-tation currently only supports unlabeled parsing.parsers (Huang and Sagae, 2010; Zhang and Nivre,2011).
Second, at a similar toks/sec parser speed,our method achieves better performance than thetransition-based model of Zhang and Nivre (2011)with a beam of 256.
Finally, compared to an im-plementation of an exact third-order parser ?
whichprovides us with an apples-to-apples comparison interms of features and runtime ?
approximate decod-ing with cube pruning is both more accurate andwhile being 4-5 times as fast.
It is more accurate asit can easily incorporate more complex features andit is faster since its asymptotic complexity is lower.We should point out that our third-order reimple-mentation is a purely unlabeled parser as we do nothave an implementation of an exact labeled third-order parser.
This likely under estimates its accu-racy, but also significantly overestimates its speed.Next, we looked at the impact of our systemon non-English treebanks.
Specifically we fo-cused on two sets of data.
The first is the Chi-nese Treebank converted to dependencies.
Herewe use the identical training/validation/evaluationsplits and experimental set-up as Zhang and Nivre(2011).
Additionally, we evaluate our system oneight other languages from the CoNLL 2006/2007shared-task (Buchholz and Marsi, 2006; Nivre etal., 2007).
We selected the following four datasets since they are primarily projective treebanks(<1.0% non-projective arcs): Bulgarian and Span-ish from CoNLL 2006 as well as Catalan and Ital-ian from CoNLL 2007.
Currently our method isrestricted to predicting strictly projective trees as it326uses the Eisner chart parsing algorithm as its back-bone.
We also report results from four additionalCoNLL data sets reported in Rush and Petrov (2012)in order to directly compare accuracy.
These areGerman, Japanese, Portuguese and Swedish.
For alldata sets we measure UAS and LAS excluding punc-tuation and use gold tags as input to the parser as isstandard for these data sets.Results are given in Table 2.
Here we compare toour re-implementations of Zhang and Nivre (2011),exact first to third-order parsing and Rush and Petrov(2012) for the data sets in which they reported re-sults.
We again see that approximate decoding withrich features and cube pruning has higher accu-racy than transition-based parsing with a large beam.In particular, for the ZH-CTB data set, our systemis currently the best reported result.
Furthermore,our system returns comparable accuracy with exactthird-order parsing, while being significantly fasterand more flexible.5.3 Ablation studiesIn this section, we analyze the contributions fromeach of the feature groups.
Each row in Table 3 usesa super set of features than the previous row.
Allsystems use our proposed generalized higher-orderparser with cube-pruning.
I.e., they are all using theEisner chart-parsing algorithm with expanded fea-ture signatures.
The only difference between sys-tems is the set of features used.
This allows us to seethe improvement from additional features.The first row uses no higher-order features.
Itis equivalent to the first-order model from Table 1.The only difference is that it uses the k-best algo-rithm to find the first-best, so it has additional over-head compared to the standard Viterbi algorithm.Each of the following rows gets a higher accuracythan its previous row by adding more higher or-der features.
Putting in the sibling and grandchildconjoined features and the valency features yieldsa further improvement over the approximation ofKoo and Collins (2010).
Thus, the addition of newhigher-order features, including valency, extra third-order, and label tuple features, results in increasedaccuracy.
However, this is not without cost as therun-time in terms of tokens/sec decreases (300 to220).
But this decrease is not asymptotic, as it wouldbe if one were to exactly search over our final modelHigher-order Features UAS LAS Toks/Secnone 91.74 90.46 1510McDonald (2006) features + labels 92.48 91.25 860Carreras (2007) features + labels 92.85 91.66 540Koo (2010) features + labels 92.92 91.75 300all features 93.06 91.86 220Table 3: Generalized higher-order parsing with cubepruning using different feature sets.Beam Recombination UAS LAS Toks/Sec2 no 92.86 91.63 2802 yes 92.89 91.65 2605 no 93.05 91.85 2405 yes 93.06 91.86 23010 yes 93.05 91.85 140Table 4: Showing the effect of better search on accuracyand speed on the English test data with a fixed model.with these additional features, e.g., valency would atleast multiply an additional O(n) factor.5.4 Impact of Search ErrorsSince our decoding algorithm is not exact, it couldreturn sub-optimal outputs under the current model.We analyze the effect of search errors on accuraciesin Table 4.
We vary the beam size at each cell andswitch the option for signature-based recombinationto make search better or worse to see how much im-pact it has on the final accuracy.The results indicate that a relatively small per-cellbeam is good enough.
Going from a beam of 2 to5 increases accuracy notably, but going to a largerbeam size has little effect but at a cost in terms ofefficiency.
This suggests that most of the parser am-biguity is represented in the top-5 feature signaturesat each chart cell.
Furthermore, recombination doeshelp slightly, but more so at smaller beam sizes.If we keep the beam size constant but enlargethe feature scope from second-order to third-order,one would expect more search errors to occur.
Wemeasured this empirically by computing the num-ber of sentences where the gold tree had a highermodel score than the predicted tree in the Englishevaluation data.
Indeed, larger feature scopes dolead to more search errors, but the absolute num-ber of search errors is usually quite small ?
thereare only 19 search errors using second-order featuresand 32 search errors using third-order plus valencyfeatures out of 2416 English test sentences.
Partof the reason for this is that there are only 12 la-327Zhang and Nivre Zhang and Nivre Rush 1st-order 2nd-order 3rd-order(reimpl.)
(reimpl.)
and exact exact exactLanguage (beam=64) (beam=256) Petrov?
(reimpl.)
(reimpl.)
(reimpl.)
this paperBG-CONLL 92.22 / 87.87 92.28 / 87.91 91.9- / - 91.98 / 87.13 93.02 / 88.13 92.96 / - 93.08 / 88.23CA-CONLL 93.76 / 87.74 93.83 / 87.85 92.83 / 86.22 93.45 / 87.19 94.07 / - 94.00 / 88.08DE-CONLL 89.18 / 86.50 88.94 / 86.58 90.8- / - 89.28 / 86.06 90.87 / 87.72 91.29 / - 91.35 / 88.42ES-CONLL 86.64 / 83.25 86.62 / 83.11 85.35 / 81.53 86.80 / 82.91 87.26 / - 87.48 / 84.05IT-CONLL 85.51 / 81.12 85.45 / 81.10 84.98 / 80.23 85.46 / 80.66 86.49 / - 86.54 / 82.15JA-CONLL 92.70 / 91.03 92.76 / 91.09 92.3- / - 93.00 / 91.03 93.20 / 91.25 93.36 / - 93.24 / 91.45PT-CONLL 91.32 / 86.98 91.28 / 86.88 91.5- / - 90.36 / 85.77 91.36 / 87.22 91.66 / - 91.69 / 87.70SV-CONLL 90.84 / 85.30 91.00 / 85.42 90.1- / - 89.32 / 82.06 90.50 / 83.01 90.32 / - 91.44 / 84.58ZH-CTB 86.04 / 84.48?
86.14 / 84.57 84.38 / 82.62 86.63 / 84.95 86.77 / - 86.87 / 85.19AVG 89.80 / 86.03 89.81 / 86.06 89.05 / 84.74 90.14 / 85.89 90.46 / - 90.63 / 86.65Table 2: UAS/LAS for experiments on non-English treebanks.
Numbers in bold are the highest scoring system.
Zhangand Nivre is a reimplementation of Zhang and Nivre (2011) with beams of size 64 and 256.
Rush and Petrov arethe UAS results reported in Rush and Petrov (2012).
Nth-order exact are implementations of exact 1st-3rd orderdependency parsing.
?For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best resultreported on this data set.
?It should be noted that Rush and Petrov (2012) do not jointly optimize labeled and unlabeleddependency structure, which we found to often help.
This, plus extra features, accounts for the differences in UAS.bels in the Penn2Malt label set, which results in lit-tle non-structural ambiguity.
In contrast, Stanford-style dependencies contain a much larger set of la-bels (50) with more fine-grained syntactic distinc-tions (De Marneffe et al2006).
Training and test-ing a model using this dependency representation2increases the number of search errors of the fullmodel to 126 out 2416 sentences.
But that is stillonly 5% of all sentences and significantly smallerwhen measured per dependency.6 Related WorkAs mentioned in the introduction, there has beennumerous studies on trying to reconcile the rich-features versus exact decoding trade-off in depen-dency parsing.
In the transition-based parsing lit-erature this has included the use of beam search toincrease the search space (Duan et al2007; Johans-son and Nugues, 2007; Titov and Henderson, 2007;Zhang and Clark, 2008; Zhang and Nivre, 2011).Huang and Sagae (2010) took a more principled ap-proach proposing a method combining shift-reduceparsing with dynamic programming.
They showedhow feature signatures can be compiled into dy-2This model gets 90.4/92.8 LAS/UAS which is comparableto the UAS of 92.7 reported by Rush and Petrov (2012).namic programming states and how best-first searchcan be used to find the optimal transition sequence.However, when the feature scope becomes large,then the state-space and resulting search space canbe either intractable or simply non-practical to ex-plore.
Thus, they resort to an approximate beamsearch that still exploring an exponentially-largerspace than greedy or beam-search transition-basedsystems.
One can view the contribution in this pa-per as being the complement of the work of Huangand Sagae (2010) for graph-based systems.
Our ap-proach also uses approximate decoding in order toexploit arbitrary feature scope, while still exploringan exponentially-large search space.
The primarydifference is how the system is parameterized, overdependency sub-graphs or transitions.
Another criti-cal difference is that a chart-based algorithm, thoughstill subject to search errors, is less likely to be hin-dered by an error made at one word position be-cause it searches over many parallel alternatives in abottom-up search as opposed to a left-to-right pass.In the graph-based parsing literature, exact pars-ing algorithms for higher-order features have beenstudied extensively (McDonald and Pereira, 2006;Carreras, 2007; Koo and Collins, 2010), but at ahigh computational cost as increasing the order of amodel typically results in an asymptotic increase in328running time.
ILP formulations of parsing (Riedeland Clarke, 2006; Martins et al2009; Martins etal., 2010) also allow for exact inference with higher-order features, but again at a high computationalcost as ILP?s have, in the worst-case, exponentialrun-time with respect to the sentence length.
Stud-ies that have abandoned exact inference have fo-cused on sampling (Nakagawa, 2007), belief prop-agation (Smith and Eisner, 2008), Lagrangian re-laxation (Koo et al2010; Martins et al2011),and more recently structured prediction cascades(Weiss and Taskar, 2010; Rush and Petrov, 2012).However, these approximations themselves are oftencomputationally expensive, requiring multiple de-coding/sampling stages in order to produce an out-put.
All the methods above, both exact and approx-imate, require specialized algorithms for every newfeature that is beyond the scope of the previous fac-torization.
In our method, the same parsing algo-rithm can be utilized (Eisner?s + cube pruning) justwith slight different feature signatures.Our proposed parsing model draws heavily on thework of Huang (2008).
Huang introduced the ideaof ?forest rescoring?, which uses cube pruning toenable the incorporation of non-local features intoa constituency parsing model providing state-of-theart performance.
This paper is the extension of suchideas to dependency parsing, also giving state-of-the-art results.
An important difference between ourformulation and forest rescoring is that we only haveone decoding pass and a single trained model, whileforest rescoring, as formulated by Huang (2008),separates a generative base model from a follow-ing discriminative re-ranking model.
Hence, ourformulation is more akin to the one pass decodingalgorithm of Chiang (2007) for integrated decodingwith a language model in machine translation.
Thisalso distinguishes it from previous work on depen-dency parse re-ranking (Hall, 2007) as we are notre-ranking/re-scoring the output of a base model butusing a single decoding algorithm and learned modelat training and testing.This work is largely orthogonal to other attemptsto speed up chart parsing algorithms.
This in-cludes work on coarse-to-fine parsing (Charniak andJohnson, 2005; Petrov and Klein, 2007; Rush andPetrov, 2012), chart-cell closing and pruning (Roarkand Hollingshead, 2008; Roark and Hollingshead,2009), and dynamic beam-width prediction (Boden-stab et al2011).
Of particular note, Rush andPetrov (2012) report run-times far better than ourcube pruning system.
At the heart of their system isa linear time vine-parsing stage that prunes most ofthe search space before higher-order parsing.
Thiseffectively makes their final system linear time inpractice as the higher order models have far fewerparts to consider.
One could easily use the samefirst-pass pruner in our cube-pruning framework.In our study we use cube pruning only for de-coding and rely on inference-based learning algo-rithms to train model parameters.
Gimpel and Smith(2009) extended cube pruning concepts to partition-function and marginal calculations, which would en-able the training of probabilistic graphical models.Finally, due to its use of the Eisner chart-parsingalgorithm as a backbone, our model is fundamen-tally limited to predicting projective dependencystructures.
Investigating extensions of this work tothe non-projective case is an area of future study.Work on defining bottom-up chart-parsing algo-rithms for non-projective dependency trees couldpotentially serve as a mechanism to solving thisproblem (Go?mez-Rodr?
?guez et al2009; Kuhlmannand Satta, 2009; Go?mez-Rodr?
?guez et al2010).7 ConclusionIn this paper we presented a method for general-ized higher-order dependency parsing.
The methodworks by augmenting the dynamic programmingsignatures of the Eisner chart-parsing algorithm andthen controlling complexity via cube pruning.
Theresulting system has the flexibility to incorporate ar-bitrary feature history while still exploring an ex-ponential search space efficiently.
Empirical resultsshow that the system gives state-of-the-art accura-cies across numerous data sets while still maintain-ing practical parsing speeds ?
as much as 4-5 timesfaster than exact third-order decoding.Acknowledgments: We would like to thank Sasha Rushand Slav Petrov for help modifying their hypergraph pars-ing code.
We would also like to thank the parsing teamat Google for providing interesting discussions and newideas while we conducted this work, as well as commentson earlier drafts of the paper.329ReferencesN.
Bodenstab, A. Dunlop, K. Hall, and B. Roark.
2011.Beam-width prediction for efficient context-free pars-ing.
In Proc.
ACL.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proc.
ofCoNLL.X.
Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proc.
of the CoNLLShared Task Session of EMNLP-CoNLL.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
InProc.
ACL.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2).K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online passive-aggressive al-gorithms.
Journal of Machine Learning Research.M.
De Marneffe, B. MacCartney, and C.D.
Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In Proc.
of LREC.X.
Duan, J. Zhao, and B. Xu.
2007.
Probabilistic parsingaction models for multi-lingual dependency parsing.In Proc.
of EMNLP-CoNLL.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: an exploration.
In Proc.
of COL-ING.K.
Gimpel and N.A.
Smith.
2009.
Cube summing,approximate inference with non-local features, anddynamic programming without semirings.
In Proc.EACL.C.
Go?mez-Rodr?
?guez, M. Kuhlmann, G. Satta, andD.
Weir.
2009.
Optimal reduction of rule length in lin-ear context-free rewriting systems.
In Proc.
NAACL.C.
Go?mez-Rodr?
?guez, M. Kuhlmann, and G. Satta.
2010.Efficient parsing of well-nested linear context-freerewriting systems.
In Proc.
NAACL.K.
Hall.
2007.
K-best spanning tree parsing.
In Proc.
ofACL.L.
Huang and S. Fayong.
2012.
Structured perceptronwith inexact search.
In Proc.
of NAACL.L.
Huang and K. Sagae.
2010.
Dynamic programmingfor linear-time incremental parsing.
In Proc.
of ACL.L.
Huang.
2008.
Forest reranking: Discriminative pars-ing with non-local features.
In Proc.
of ACL.R.
Johansson and P. Nugues.
2007.
Incremental de-pendency parsing using online learning.
In Proc.
ofEMNLP-CoNLL.T.
Koo and M. Collins.
2010.
Efficient third-order de-pendency parsers.
In Proc.
of ACL.T.
Koo, A.
Rush, M. Collins, T. Jaakkola, and D. Son-tag.
2010.
Dual decomposition for parsing with non-projective head automata.
In Proc.
of EMNLP.M.
Kuhlmann and G. Satta.
2009.
Treebank grammartechniques for non-projective dependency parsing.
InProc.
EACL.A.
F. T. Martins, N. Smith, and E. P. Xing.
2009.
Con-cise integer linear programming formulations for de-pendency parsing.
In Proc.
of ACL.A.
F. T. Martins, N. Smith, E. P. Xing, P. M. Q. Aguiar,and M. A. T. Figueiredo.
2010.
Turbo parsers: Depen-dency parsing by approximate variational inference.In Proc.
of EMNLP.A.
F. T. Martins, N. Smith, M. A. T. Figueiredo, andP.
M. Q. Aguiar.
2011.
Dual decomposition withmany overlapping components.
In Proc of EMNLP.R.
McDonald and J. Nivre.
2007.
Characterizing theerrors of data-driven dependency parsing models.
InProc.
of EMNLP-CoNLL.R.
McDonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In Proc.of EACL.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In Proc.of ACL.T.
Nakagawa.
2007.
Multilingual dependency parsingusing global features.
In Proc.
of EMNLP-CoNLL.J.
Nivre and R. McDonald.
2008.
Integrating graph-based and transition-based dependency parsers.
InProc.
of ACL.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nils-son, S. Riedel, and D. Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.
In Proc.
ofEMNLP-CoNLL.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In Proc.
NAACL.S.
Riedel and J. Clarke.
2006.
Incremental integer linearprogramming for non-projective dependency parsing.In Proc.
of EMNLP.B.
Roark and K. Hollingshead.
2008.
Classifying chartcells for quadratic complexity context-free inference.In Proc.
COLING.B.
Roark and K. Hollingshead.
2009.
Linear complexitycontext-free parsing pipelines via chart constraints.
InProce.
NAACL.A.
Rush and S. Petrov.
2012.
Efficient multi-pass depen-dency pruning with vine parsing.
In Proc.
of NAACL.D.
Smith and J. Eisner.
2008.
Dependency parsing bybelief propagation.
In Proc.
of EMNLP.I.
Titov and J. Henderson.
2007.
Fast and robust mul-tilingual dependency parsing with a generative latentvariable model.
In Proc.
of EMNLP-CoNLL.D.
Weiss and B. Taskar.
2010.
Structured prediction cas-cades.
In Proc.
of AISTATS.330Y.
Zhang and S. Clark.
2008.
A Tale of TwoParsers: Investigating and Combining Graph-basedand Transition-based Dependency Parsing.
In Proc.of EMNLP.Y.
Zhang and J. Nivre.
2011.
Transition-based depen-dency parsing with rich non-local features.
In Proc.
ofACL-HLT, volume 2.331
