Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1399?1404,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsApplication of Localized Similarity for Web DocumentsPeter Reber?ekZemantaCelov?ka cesta 32Ljubljana, Sloveniapeter.rebersek@zemanta.comMateja Verlic?ZemantaCelov?ka cesta 32Ljubljana, Sloveniamateja.verlic@zemanta.comAbstractIn this paper we present a novel approach toautomatic creation of anchor texts for hyper-links in a document pointing to similar doc-uments.
Methods used in this approach rankparts of a document based on the similarityto a presumably related document.
Ranks arethen used to automatically construct the bestanchor text for a link inside original documentto the compared document.
A number of dif-ferent methods from information retrieval andnatural language processing are adapted forthis task.
Automatically constructed anchortexts are manually evaluated in terms of relat-edness to linked documents and compared tobaseline consisting of originally inserted an-chor texts.
Additionally we use crowdsourc-ing for evaluation of original anchors and au-tomatically constructed anchors.
Results showthat our best adapted methods rival the preci-sion of the baseline method.1 IntroductionOne of the features of hypertext documents are hy-perlinks that point to other resources ?
pictures,videos, tweets, or other hypertext documents.
Afairly familiar category of the latter is related arti-cles; these usually appear at the end of a news articleor a blog post with the title of the target document asanchor text.
The target document is similar in con-tent to original document; it may tell the story fromanother point of view, it may be a more detailed ver-sion of a part of the events in the original document,etc.
Another category are the in-text links; these ap-pear inside the main body of text and use some ofthe existing text as anchor.
Ideally the anchor text isselected in such a way that it conveys some informa-tion about the target document; in reality sometimesjust an adverb (e.g.
here, there) is used, or even thedestination URL may serve as anchor.Our goal is to develop a system that automaticallyconstructs in-text links, i.e.
for a query documentfinds a target document and an appropriate part ofthe text of the query document that serves as the an-chor text for the hyperlink.
We want the target docu-ment to be similar in content to the query documentand the anchor text to indicate that content.There are many potential uses for such a system,especially for simplifying and streamlining docu-ment creation.
This includes authors of blogs thatmay use the system for adding related content fromother sources without exhausting manual search forsuch material.
It may also be used when writinga scientific paper, automatically adding citations toother relevant papers inside the main body.
This ac-celerates the writing, again reducing the time spentsearching for possible existing research in the field.A citation can be considered an in-text link withouta defined starting point.We have addressed the problem in two steps, sep-arately finding a similar document, and finding theanchor text for it.
Since the retrieval of similar doc-uments was a research focus for many years and isthus better researched, we have decided in this pa-per to focus on the placement of the anchor text fora link to a preselected document.This paper is organized as follows: related workis discussed in Section 2, the methods, corpus, andevaluation are described in Section 3, followed by1399results and discussion in Section 4 and ending withconclusions in Section 5.2 Related WorkSemantic similarity of textual documents offers away to organize the increasing number of availabledocuments.
It can be used in many applications suchas summarization, educational systems, finding du-plicated bug reports in software testing (Lintean etal., 2010), plagiarism detection (Kasprzak and Bran-dejs, 2010), and research of a scientific field (Kober-stein and Ng, 2006).
Documents can vary in lengthfrom microblogs (Twitter) and sentences (Li et al2006; Koberstein and Ng, 2006) to paragraphs (Lin-tean et al 2010) and larger documents (Budanitskyand Hirst, 2006).There is also commercial software such as nRe-late1, Zemanta2 and OpenCalais3 with functionalitythat ranges from named entity recognition (NER)and event detection to related content.
Publishersuse in-house tools that offer automatic retrieval ofin-house similar documents.Most of the methods for comparing documents fo-cus on the query document as a whole.
The calcu-lated score therefore belongs to the whole documentand nothing can be said about more or less similarparts of the document.
Our goal is to localize thesimilarity to a part of the query document, a para-graph, sentence, or even a part of the sentence that ismost similar to another document.
This part of thequery document can then serve as anchor text for ahyperlink connection to the similar document.Plagiarism detection methods (Alzahrani et al2012; Monostori et al 2002) have a task of veri-fying the originality of the document.
Extrinsic pla-giarism detection methods compare two documentsto determine if some of the material in one is pla-giarised from the other.
Methods range from sim-ple exact substring matching to more advanced oneslike semantic based methods that are able to recog-nize paraphrasing and refactoring (Alzahrani et al2012).
These methods have localization of similarityalready built-in as they are searching for parts of thetext that seem to be plagiarised.
We have focused on1nRelate: http://www.nrelate.com/2Zemanta: http://www.zemanta.com/3OpenCalais: http://www.opencalais.com/one such method, the winner of the PAN 2010 chal-lenge (Kasprzak and Brandejs, 2010).
This methoduses shared n-grams from the two documents in or-der to determine if one of them is plagiarised.Another similar research is automatic citationplacement for scientific papers.
Most of the work(Strohman et al 2007; McNee et al 2002) is con-cerned with putting citations at the end of the paper(non-localized), which is a task similar to insertingrelated articles for a news article at the of the text.There have been some attempts to place the citationsin the main body of text (Tang and Zhang, 2009; Heet al 2011), typically used when referring to an ideaor method.Tang and Zang (2009) used a placeholder con-straint: the query document must contain placehold-ers for citations, i.e.
the places in text where citationmight be inserted.
Their method then just ranks allpossible documents for a particular placeholder andchooses the best ranked document as a result.
Doc-uments are ranked on the basis of a learned topicmodel, obtained by a two-layer Restricted Boltz-mann Machine.He et al2011) made a step further towards gen-erality of a citation location; they divide the text intooverlapping windows and then decide which win-dows are viable citation context.
The best methodfor deciding which citation context to use was a de-pendency feature model, an ensemble method using17 different features and decision trees.Named entity recognition (NER) also offers a use-ful insight into document similarity.
If two docu-ments share a named entity (NE), it is more likelythey are similar.
Detected NEs may also serve as an-chor text for the link.
NER is a fairly researched field(Finkel et al 2005; Ratinov et al 2011; Bunescuand Pasca, 2006; Kulkarni et al 2009; Milne andWitten, 2008) and is also used in several commer-cial applications such as Zemanta, OpenCalais andAlchemyAPI4, which are able to automatically in-sert links for a NE pointing to a knowledge base suchas Wikipedia or IMDB.
However, at this point theyare unable to link to arbitrary documents, but maybe useful in conjunction with other methods.4AlchemyAPI: http://www.alchemyapi.com/14003 Methodology3.1 CorpusWe have chosen 100 web articles (posts) at ran-dom from the end of January 2012.
We extractedthe body and title of each document.
All thepresent in-text links were also extracted and filtered.First, automatic filtering was applied to remove un-wanted categories of links (videos, definition pageson wikipedia and imdb, etc.
), and articles that weredeemed too short for similarity comparison.
Thethreshold was set at 200 words of automaticallyscraped body text of a linked document.All the remaining links were manually checkedto ensure the integrity of link targets.
This way wecollected 265 articles (hereinafter related articles -RA).
A number of different methods were then usedto calculate similarity rank and select the best part ofthe post text to be used as anchor text for a hyperlinkpointing to the originally linked RA.We have used CrowdFlower5, a crowdsourcingplatform, to evaluate how many of the 265 post?RApairs were really related; the final corpus thus con-sisted of 236 pairs.3.2 EvaluationWe have used each of the methods described in Sub-section 3.3 to automatically construct anchor text foreach of the 236 pairs of documents in the final cor-pus.
If a method could not find a suitable anchor,no result was returned; on average there were 147anchors per method.
All the automatically createdlinks were then manually scored by the authors withan in-house evaluation tool using scores and guide-lines summarized in Table 1.
To calculate precisionand recall, we have counted scores 2 and 3 as posi-tive result.Additionally we crowdsourced the evaluation ofresults for some of the methods.
For this task we pre-pared a special description of evaluation tasks anddefined a set of questions for collecting results.
Weprovided simplified guidelines for assigning scoresto automatically created anchors and set a confi-dence threshold of 0.55 for an assignment to be con-sidered valid.
It is important to mention that the useof crowdsourcing for such tasks has to be carefully5CrowdFlower: http://crowdflower.com/Score Description0 Anchor does not signify anything aboutRA or gets it wrong1 Some connection can be established (an-chor is a shared Named Entity, NounPhrase, Verb Phrase, etc.
)2 Anchor is a good estimation of RA top-ics, but not wholly (anchor is a non-maintopic in RA)3 RA topics can be directly inferred fromthe anchorTable 1: Scores used for internal evaluation of automati-cally created anchorsplanned, because many issues related to monetaryincentives, which are out of the scope of this paper,may arise.3.3 Methods for constructing anchor textsWe have adapted a number of methods from a vari-ety of sources to test how they perform for our exactpurpose.
Below is a short overview of the differentmethods used in this work.3.3.1 Longest chunkThis method is based on natural language pro-cessing and extensively uses NLTK package (Birdet al 2009); the text is first tokenized with the de-fault NLTK tokenizer, and then POS tagged with oneof the included POS taggers.
After much testing, wehave decided on a combination of Brill ?
Trigram ?Bigram ?
Unigram ?
Affix ?
Regex backoff taggerwith noun as default tag.
The trainable parts of thetagger were trained on the included CoNLL 2000tagged corpus.Before chunking was applied, we also simplifiedsome tags and removed some others to get a simplerstructure of POS tags.
We then used a regex chun-ker to find a sequence of a proper noun and a verbseparated by zero or more other tokens.
We havealso tested a proper noun - verb - proper noun com-bination, but there were even fewer results, so thisdirection was abandoned.3.3.2 Latent Semantic Indexing (LSI) basedA corpus is represented in LSI (Deerwester etal., 1990) as a large matrix of term occurrences1401in individual documents.
The rank of the matrixis then reduced using singular value decompositionthat groups together terms that occur in similar con-text which should therefore account for synonyms.We have used a tool called gensim (R?ehu?r?ek andSojka, 2010) that enabled us to quickly train a LSImodel using the whole corpus and index just the re-lated articles.
In order to localize the similarity andplace an anchor, we split the source document intoparagraphs and compute similarity scores betweentarget document and each paragraph of the sourcedocument.
We then split the paragraph with thehighest score into sentences and again obtain scoresfor each.
The sentence with the best score is thenchosen as the result.3.3.3 Sorted n-gramsDrawing on plagiarism detection, the winningmethod from the PAN 2010 (Kasprzak and Bran-dejs, 2010) seemed a viable choice.
The basis ofthe method is comparing n-grams of the source andthe destination documents.
First, the text was againtokenized with NLTK, removed stopwords and to-kens with two or less characters.
Then overlappingn-grams were constructed.
We have deviated fromKasprzak?s merging policy and decided to mergetwo results if they are less than 20 tokens apart.
Wealso required only one shared n-gram to consider thedocuments similar.
Results were ranked based onthe number of shared tokens within each.3.3.4 Unigrams tf*idfThis method uses unigram tf*idf weighted scores.Since we had a closed system, we used corpus-widefrequencies; stopwords were also removed.
We havescored tokens in the source document with tf*idfsummary of the destination document; tokens notin summary are given a zero weight.
We have ex-perimentally determined that a summary of just top150 tokens improves results.
Sentences were rankedbased on the sum of its tokens weights.
We also in-cluded NEs from Zemanta API response for bothsource and destination document.
Sentences con-taining shared NEs get their score multiplied by thesum of shared NE tf*idf weights.
The result wasthen the sentence with the highest score.Manual CrowdFlowerP R P ROriginal links 0.691 0.691 0.981 0.432Sorted 5-grams 0.822 0.254Sorted 4-grams 0.741 0.352Sorted 3-grams 0.680 0.424 0.956 0.275Longest Chunk 0.080 0.075 0.907 0.165Unigrams tf*idf 0.626 0.242 0.882 0.127LSI based 0.648 0.640Table 2: Precision and recall for manual and Crowd-Flower evaluation3.3.5 BaselineOur baseline was a method that inserted links thatwere originally present in the source documents.This method was used to compare our automaticmethods to what people are actually linking in thereal world.4 Evaluation Results and DiscussionResults are presented as precision and recall fordifferent methods and both evaluations in Table 2.Empty cells in the table indicate that these methodswere not evaluated using CrowdFlower.
Recall isthe fraction of relevant results out of all the possibleresults (236) and precision is the fraction of relevantresults out of all the retrieved results.The first thing we notice is the general disagree-ment between results from the authors and Crowd-Flower workers; the latter tend to give higher scores,which leads to higher precision and recall.
The rea-son for this might be in the authors?
backgroundknowledge and thus higher expectations.As a contrast almost half of CrowdFlower work-ers stated they don?t blog and of the rest, more thana third of them don?t link out, i.e.
do not use re-lated articles.
We also have only 74% median inter-annotator agreement leading us to believe that someof the annotators answered without being familiarwith the question (monetary incentive issue).Furthermore, CrowdFlower results for originallinks (our baseline) indicate that almost all of themwere recognized as relevant, while our evaluatorsdiscarded 30% of them.
Clearly seen in the re-sults of different sorted n-grams methods is also theprecision-recall trade-off.14025 ConclusionBased on evaluation results and despite differencesbetween the evaluators with background knowledgeand the crowds, we can conclude that that our ap-proach for automatic construction of in-text linksrivals manual creation by professional writers andbloggers and is thus a promising direction for fur-ther research.AcknowledgementThis work was partially funded by the SlovenianMinistry of Higher Education, Science and Technol-ogy, and the European Union ?
European RegionalDevelopment Fund.ReferencesS.M.
Alzahrani, N. Salim, and A. Abraham.
2012.
Un-derstanding plagiarism linguistic patterns, textual fea-tures, and detection methods.
Systems, Man, and Cy-bernetics, Part C: Applications and Reviews, IEEETransactions on, 42(2):133?149.Steven Bird, Ewan Klein, and Edward Loper.
2009.
Nat-ural Language Processing with Python.
O?Reilly Me-dia.Alexander Budanitsky and Graeme Hirst.
2006.
Evalu-ating wordnet-based measures of lexical semantic re-latedness.
Comput.
Linguist., 32(1):13?47, March.Razvan Bunescu and Marius Pasca.
2006.
Using ency-clopedic knowledge for named entity disambiguation.In Proceedings of the 11th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL-06), pages 9?16, Trento, Italy.Scott Deerwester, Susan T. Dumais, George W Furnas,Thomas K Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican society for information science, 41(6):391?407.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 363?370, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Qi He, Daniel Kifer, Jian Pei, Prasenjit Mitra, and C. LeeGiles.
2011.
Citation recommendation without authorsupervision.
In Proceedings of the fourth ACM inter-national conference on Web search and data mining,WSDM ?11, pages 755?764, New York, NY, USA.ACM.Jan Kasprzak and Michal Brandejs.
2010.
Improving thereliability of the plagiarism detection system lab reportfor pan at clef 2010.Jonathan Koberstein and Yiu-Kai Ng.
2006.
Us-ing word clusters to detect similar web documents.In Proceedings of the First international conferenceon Knowledge Science, Engineering and Manage-ment, KSEM?06, pages 215?228, Berlin, Heidelberg.Springer-Verlag.Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, andSoumen Chakrabarti.
2009.
Collective annotationof wikipedia entities in web text.
In Proceedingsof the 15th ACM SIGKDD international conferenceon Knowledge discovery and data mining, KDD ?09,pages 457?466, New York, NY, USA.
ACM.Yuhua Li, David McLean, Zuhair A. Bandar, James D.O?Shea, and Keeley Crockett.
2006.
Sentence sim-ilarity based on semantic nets and corpus statistics.IEEE Trans.
on Knowl.
and Data Eng., 18(8):1138?1150, August.Mihai Lintean, Cristian Moldovan, Vasile Rus, andDanielle McNamara.
2010.
The role of local andglobal weighting in assessing the semantic similarityof texts using latent semantic analysis.
In Proceedingsof the 23rd International Florida Artificial IntelligenceResearch Society Conference, Daytona Beach, FL.Sean M. McNee, Istvan Albert, Dan Cosley, PrateepGopalkrishnan, Shyong K. Lam, Al Mamunur Rashid,Joseph A. Konstan, and John Riedl.
2002.
On the rec-ommending of citations for research papers.
In Pro-ceedings of the 2002 ACM conference on Computersupported cooperative work, CSCW ?02, pages 116?125, New York, NY, USA.
ACM.David Milne and Ian H. Witten.
2008.
Learning to linkwith wikipedia.
In Proceedings of the 17th ACM con-ference on Information and knowledge management,CIKM ?08, pages 509?518, New York, NY, USA.ACM.Kriszti?n Monostori, Raphael Finkel, Arkady Zaslavsky,G?bor Hod?sz, and M?t?
Pataki.
2002.
Comparisonof overlap detection techniques.
In PeterM.A.
Sloot,AlfonsG.
Hoekstra, C.J.Kenneth Tan, and JackJ.
Don-garra, editors, Computational Science ?
ICCS 2002,volume 2329 of Lecture Notes in Computer Science,pages 51?60.
Springer Berlin Heidelberg.Lev Ratinov, Dan Roth, Doug Downey, and Mike An-derson.
2011.
Local and global algorithms for dis-ambiguation to Wikipedia.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies - Volume1, HLT ?11, pages 1375?1384, Stroudsburg, PA, USA.Association for Computational Linguistics.Radim R?ehu?r?ek and Petr Sojka.
2010.
Software Frame-work for Topic Modelling with Large Corpora.
In Pro-1403ceedings of the LREC 2010 Workshop on New Chal-lenges for NLP Frameworks, pages 45?50, Valletta,Malta, May.
ELRA.Trevor Strohman, W. Bruce Croft, and David Jensen.2007.
Recommending citations for academic papers.In In Proceedings of the 30th Annual InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval (SIGIR?07, pages 705?706.Jie Tang and Jing Zhang.
2009.
A discriminative ap-proach to topic-based citation recommendation.
InThanaruk Theeramunkong, Boonserm Kijsirikul, NickCercone, and Tu-Bao Ho, editors, Advances in Knowl-edge Discovery and Data Mining, volume 5476 ofLecture Notes in Computer Science, pages 572?579.Springer Berlin Heidelberg.1404
