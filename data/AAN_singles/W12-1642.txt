Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 304?313,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsFocused Meeting Summarization via Unsupervised Relation ExtractionLu WangDepartment of Computer ScienceCornell UniversityIthaca, NY 14853luwang@cs.cornell.eduClaire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853cardie@cs.cornell.eduAbstractWe present a novel unsupervised frameworkfor focused meeting summarization that viewsthe problem as an instance of relation extrac-tion.
We adapt an existing in-domain rela-tion learner (Chen et al, 2011) by exploit-ing a set of task-specific constraints and fea-tures.
We evaluate the approach on a decisionsummarization task and show that it outper-forms unsupervised utterance-level extractivesummarization baselines as well as an exist-ing generic relation-extraction-based summa-rization method.
Moreover, our approach pro-duces summaries competitive with those gen-erated by supervised methods in terms of thestandard ROUGE score.1 IntroductionFor better or worse, meetings play an integral rolein most of our daily lives ?
they let us share infor-mation and collaborate with others to solve a prob-lem, to generate ideas, and to weigh options.
Notsurprisingly then, there is growing interest in devel-oping automatic methods for meeting summariza-tion (e.g., Zechner (2002), Maskey and Hirschberg(2005), Galley (2006), Lin and Chen (2010), Mur-ray et al (2010a)).
This paper tackles the task of fo-cused meeting summarization , i.e., generating sum-maries of a particular aspect of a meeting rather thanof the meeting as a whole (Carenini et al, 2011).For example, one might want a summary of just theDECISIONS made during the meeting, the ACTIONITEMS that emerged, the IDEAS discussed, or theHYPOTHESES put forth, etc.Consider, for example, the task of summarizingthe decisions in the dialogue snippet in Figure 1.
Thefigure shows only the decision-related dialogue acts(DRDAs) ?
utterances associated with one or moredecisions.1 Each DRDA is labeled numerically ac-cording to the decision it supports; so the first twoutterances support DECISION 1 as do the final twoutterances in the snippet.
Manually constructed de-cision abstracts for each decision are shown at thebottom of the figure.2 These constitute the decision-focused summary for the snippet.Notice that many portions of the DRDAs are notrelevant to the decision itself: they often begin withphrases that identify the utterance within the dis-course as potentially introducing a decision (e.g.,?Maybe that could be?, ?It seems like you?re gonnahave?
), but do not themselves describe the decision.We will refer to this portion of a DRDA (underlinedin Figure 1) as the Decision Cue.Moreover, the decision cue is generally directlyfollowed by the actual Decision Content (e.g., ?be alittle apple?, ?have rubber cases?).
Decision Contentphrases are denoted in Figure 1 via italics and squarebrackets.
Importantly, it is just the decision contentportion of the utterance that should be considered forincorporation into the focused summary.1These are similar, but not completely equivalent, to the de-cision dialogue acts (DDAs) of (Bui et al, 2009), (Ferna?ndez etal., 2008), (Frampton et al, 2009).2Murray et al (2010b) show that users much prefer abstrac-tive summaries over extracts when the text to be summarizedis a conversation.
In particular, extractive summaries drawnfrom group conversations can be confusing to the reader with-out additional context; and the noisy, error-prone, disfluent textof speech transcripts is likely to result in extractive summarieswith low readability.304C: Say the standby button is quite kinda separate from all theother functions.
(1)C: Maybe that could be [a little apple].
(1)C: It seems like you?re gonna have [rubber cases], as well as[buttons].
(2)A: [Rubber buttons] require [rubber case].
(2)A: You could have [your company badge] and [logo].
(3)A: I mean a lot of um computers for instance like like on the oneyou?ve got there, it actually has a sort of um [stick on badge].
(3)C: Shall we go [for single curve], just to compromise?
(2)B: We?ll go [for single curve], yeah.
(2)C: And the rubber push buttons, rubber case.
(2)D: And then are we going for sort of [one button] shaped[like a fruit].
<vocalsound> Or veg.
(1)D: Could be [a red apple], yeah.
(1)Decision Abstracts (Summary)DECISION 1: The group decided to make the standby buttonin the shape of an apple.DECISION 2: The remote will also feature a rubber case andrubber buttons, and a single-curved design.DECISION 3: The remote will feature the company logo,possibly in a sticker form.Figure 1: Clip from the AMI meeting corpus (Carletta et al,2005).
A, B, C and D refer to distinct speakers; the numbersin parentheses indicate the associated meeting decision: DECI-SION 1, 2 or 3.
Also shown is the gold-standard (manual) ab-stract (summary) for each decision.
Colors indicate overlappingvocabulary between utterances and the summary.
Underlining,italics, and [bracketing] are decscribed in the running text.This paper presents an unsupervised frameworkfor focused meeting summarization that supports thegeneration of abstractive summaries.
(Note that wedo not currently generate actual abstracts, but ratheraim to identify those Content phrases that shouldcomprise the abstract.)
In contrast to existing ap-proaches to focused meeting summarization (e.g.,Purver et al (2007), Ferna?ndez et al (2008), Bui etal.
(2009)), we view the problem as an informationextraction task and hypothesize that existing meth-ods for domain-specific relation extraction can bemodified to identify salient phrases for use in gener-ating abstractive summaries.Very generally, information extraction methodsidentify a lexical ?trigger?
or ?indicator?
that evokesa relation of interest and then employ syntactic in-formation, often in conjunction with semantic con-straints, to find the ?target phrase?
or ?argumentconstituent?
to be extracted.
Relation instances,then, are represented by indicator-argument pairs(Chen et al, 2011).Figure 1 shows some possible indicator-argumentpairs for identifying the Decision Content phrasesin the dialogue sample.
Content indicator wordsare shown in italics; the Decision Content targetphrases are the arguments.
For example, in thefourth DRDA, ?require?
is the indicator, and ?rub-ber buttons?
and ?rubber case?
are both arguments.Although not shown in Figure 1, it is also possibleto identify relations that correspond to the DecisionCue phrases.3Specifically, we focus on the task of decision sum-marization and, as in previous work in meeting sum-marization (e.g., Ferna?ndez et al (2008), Wang andCardie (2011)), assume that all decision-related ut-terances (DRDAs) have been identified.
We adaptthe unsupervised relation learning approach of Chenet al (2011) to separately identify relations asso-ciated with decision cues vs. the decision contentwithin DRDAs by defining a new set of task-specificconstraints and features to take the place of thedomain-specific constraints and features of the orig-inal model.
Output of the system is a set of extractedindicator-argument decision content relations (seethe ?OUR METHOD?
sample summary of Table 6)that can be used as the basis of the decision abstract.We evaluate the approach (using the AMI cor-pus (Carletta et al, 2005)) under two input set-tings ?
in the True Clusterings setting, we assumethat the DRDAs for each meeting have been per-fectly grouped according to the decision(s) each sup-ports; in the System Clusterings setting, an auto-mated system performs the DRDA-decision pairing.The results show that the relation-based summariza-tion approach outperforms two extractive summa-rization baselines that select the longest and the mostrepresentative utterance for each decision, respec-tively.
(ROUGE-1 F score of 37.47% vs. 32.61%and 33.32% for the baselines given the True Cluster-ings of DRDAs.)
Moreover, our approach performsadmirably in comparison to two supervised learningalternatives (scores of 35.61% and 40.87%) that aimto identify the important tokens to include in the de-cision abstract given the DRDA clusterings.
In con-trast to our approach which is transferable to differ-ent domains or tasks, these methods would requirelabeled data for retraining for each new meeting cor-pus.3Consider, for example, the phrases underlined in the sixthand seventh DRDAs.
?I mean?
and ?shall we?
are two typicalDecision Cue phrases where ?mean?
and ?shall?
are possibleindicators with ?I?
and ?we?
as their arguments, respectively.305Finally, in order to compare our approach to an-other relation-based summarization technique, wemodify the multi-document summarization systemof Hachey (2009) to the single-document meetingscenario.
Here again, our proposed approach per-forms better (37.47% vs. 34.69%).
Experiments un-der the System Clusterings setting produce the sameoverall results, albeit with lower scores for all of thesystems and baselines.In the remainder of the paper, we review relatedwork in Section 2 and give a high-level descriptionof the relation-based approach to focused summa-rization in Section 3.
Sections 4, 5 and 6 present themodifications to the Chen et al (2011) relation ex-traction model required for its instantiation for themeeting summarization task.
Sections 7 and 8 pro-vide our experimental setup and results.2 Related WorkMost research on spoken dialogue summariza-tion attempts to generate summaries for full dia-logues (Carenini et al, 2011).
Only recently, how-ever, has the task of focused summarization, and de-cision summarization, in particular, been addressed.Ferna?ndez et al (2008) and Bui et al (2009) em-ploy supervised learning methods to rank phrasesor words for inclusion in the decision summary.In comparison, Ferna?ndez et al (2008) find thatthe phrase-based approach yields better recall thantoken-based methods, concluding that phrases havethe potential to support better summaries.
Input totheir system, however, is narrowed down (manually)from the full set of DRDAs to the subset that is use-ful for summarization.
In addition, they evaluatetheir system w.r.t.
informative phrases or words thathave been manually annotated within this DRDAsubset.
We are instead interested in comparing ourextracted relations to the abstractive summaries.In contrast to our phrase-based approach, we pre-viously explored a collection of supervised and un-supervised learning methods for utterance-level (i.e.,dialogue act) and token-level decision summariza-tion (Wang and Cardie, 2011).
We adopt here thetwo unsupervised baselines (utterance-level sum-maries) from that work for use in our evaluation.We further employ their supervised summarizationmethods as comparison points for token-level sum-marization, adding additional features for consis-tency with the other approaches in the evaluation.Murray et al (2010a) develop an integer linear pro-gramming approach for focused summarization atthe utterance-level, selecting sentences that covermore of the entities mentioned in the meeting as de-termined through the use of an external ontology.The most relevant previous work is Hachey(2009), which uses relational representations to fa-cilitate sentence-ranking for multi-document sum-marization.
The method utilizes generic relation ex-traction to represent the concepts in the documentsas relation instances; summaries are generated basedon a set cover algorithm that selects a subset ofthe sentences that best cover the weighted concepts.Thus, the goal of Hachey?s approach is sentence ex-traction rather than phrase extraction.
Although hisrelation extraction method, like ours (see Section4), is probabilistic and unsupervised (he uses LatentDirichelt Allocation (Blei et al, 2003)), the relationsare limited to pairs of named-entities, which is notappropriate for our decision summarization setting.Nevertheless, we will adapt his approach for com-parison with our relation-based summarization tech-nique and include it for evaluation.3 Focused Summarization as Relation Ex-tractionGiven the DRDAs for each meeting grouped (notnecessarily correctly) according to the decisionsthey support, we put each cluster of DRDAs (or-dered according to time within the cluster) into one?decision document?.
The goal will be to pro-duce one decision abstract for each such decisiondocument.
We obtain constituent and dependencyparses using the Stanford parser (Klein and Man-ning, 2003; de Marneffe et al, 2006).
With the cor-pus of constituent-parsed decision documents as theinput, we will use and modify Chen et al (2011)?ssystem to identify decision cue relations and deci-sion content relations for each cluster.4 (Section 6will make clear how the learned decision cue rela-tions will be used to identify decision content re-lations.)
The salient decision content relation in-stances will be returned as decision summary com-4Other unsupervised relation learning methods might alsobe appropriate (e.g., Open IE (Banko et al, 2007)), but theygenerally model relations between pairs of entities and grouprelations only according to lexical similarity.306ponents.Designed for in-domain relation discovery fromstandard written texts (e.g., newswire), however, theChen et al (2011) system cannot be applied to ourtask directly.
In our setting, for example, neither thenumber of relations nor the relation types is knownin advance.In the following sections, we describe the modi-fications needed for the spoken meeting genre anddecision-focused summarization task.
In particular,Chen et al (2011) provide two mechanisms that al-low for this type of tailoring: the feature set used tocluster potential relation instances into groups/types,and a set of global constraints that characterize thegeneral qualities (e.g., syntactic form, prevalence,discourse behavior) of a good relation for the task.4 ModelIn this section, we describe the Chen et al (2011)probabilistic relation learning model used for bothDecision Cue and Decision Content relation extrac-tion.
The parameter estimation and constraint en-coding through posterior inference are presented inSection 5.The relation learning model takes as input clus-ters of DRDAs, sorted according to utterance timeand concatenated into one decision document.
Weassume one decision will be made per document.The goal for the model is to explain how the de-cision documents are generated from the latent re-lation variables.
The posterior regularization tech-nique (Section 5) biases inference to adhere to thedeclarative constraints on relation instances.
In gen-eral, instead of extracting relation instances strictlysatisfying a set of human-written rules, features andconstraints are designed to allow the model to revealdiverse relation types and to ensure that the identi-fied relation instances are coherent and meaningful.For each decision document, we select the relationinstance with highest probability for each relationtype and concatenate them to form the decision sum-mary.We restrict the eligible indicators to be a noun orverb, and eligible arguments to be a noun phrase(NP), prepositional phrase (PP) or clause introducedby ?to?
(S).
Given a pre-specified number of relationtypes K, the model employs a set of features ?i(w)and ?a(x) (see Section 6) to describe the indicator?0?k,?
|?i|i i ?k,?bi i ?k,?
|?a|a a ?k,?ba a ?k K?0?i(w)|?i| W ?a(x)|?a|Xsd,kzd,kad,kid,k K DFigure 2: Graphical model representation for the relationlearning model.
D is the number of decision documents (eachdecision document consists of a cluster of DRDAs).
K is thenumber of relation types.
W and X represent the number of in-dicators and arguments in the decision document.
|?i| and |?a|are the number of features for indicator and argument.word w and argument constituent x.
Each relationtype k is associated with a set of feature distributions?k and a location distribution ?k.
?k include four pa-rameter vectors: ?ik for indicator words, ?bik for non-indicator words, ?ak for argument constituents, and?bak for non-argument constituents.
Each decisiondocument is divided into L equal-length segmentsand the location parameter vector ?k describes theprobability of relation k arising from each segment.The plate diagram for the model is shown in Fig-ure 2.
The generative process and likelihood of themodel are shown in Appendix A.5 Parameter Estimation and Inference viaPosterior RegularizationIn order to specify global preferences for the rela-tion instances (e.g.
the syntactic structure of the ex-pressions), we impose inequality constraints on ex-pectations of the posterior distributions during infer-ence (Graca et al, 2008).5.1 Variational inference with ConstraintsSuppose we are interested in estimating the posteriordistribution p(?, z|x) of a model in general, where?, z and x are parameters to estimate, latent vari-ables and observations, respectively.
We aim to finda distribution q(?, z) ?
Q that minimizes the KL-divergence to the true posteriorKL(q(?, z)?p(?, z|x)) (1)307A mean-field assumption is made for variationalinference, where q(?, z) = q(?)q(z).
Then we canminimize Equation 1 by performing coordinate de-scent on q(?)
and q(z).
Now we intend to have fine-level control on the posteriors to induce meaningfulsemantic parts.
For instance, we would like most ofthe extracted relation instances to satisfy a set of pre-defined syntactic patterns.
As presented in (Graca etal., 2008), a general way to put constraints on pos-terior q is through bounding expectations of givenfunctions: Eq[f(z)] ?
b, where f(z) is a determin-istic function of z, and b is a pre-specified threshold.For instance, define f(z) as a function to count thenumber of generated relation instances that meet thepre-defined syntactic patterns, then most of the ex-tracted relation instances will have the desired syn-tactic structures.By using the mean-field assumption, the model inSection 4 is factorized asq(?, ?, z, i, a) =K?k=1q(?k; ?
?k)q(?ik; ?
?ik)q(?bik ; ?
?bik )q(?ak ?
?ak)q(?bak ; ?
?bak )?D?d=1q(zd,k, id,k, ad,k; c?d,k) (2)The constraints are encoded in the inequalitiesEq[f(z, i, a)] ?
b or Eq[f(z, i, a)] ?
b, and affectthe inference as described above.
Updates for theparameters are discussed in Appendix B.5.2 Task-Specific Constraints.We define four types of constraints for the decisionrelation extraction model.Syntactic Constraints.
Syntactic constraints arewidely used for information extraction (IE) systems(Snow et al, 2005; Banko and Etzioni, 2008), as ithas been shown that most relations are expressed viaa small number of common syntactic patterns.
Foreach relation type, we require at least 80%5 of theinduced relation instances in expectation to matchone of the following syntactic patterns:?
The indicator is a verb and the argument is a nounphrase.
The headword of the argument is the directobject of the indicator or the nominal subject of theindicator.5Experiments show that this threshold is suitable for deci-sion relation extraction, so we adopt it from (Chen et al, 2011).?
The indicator is a verb and the argument is a prepo-sitional phrase or a clause starting with ?to?.
Theindicator and the argument have the same parent inthe constituent parsing tree.?
The indicator is a noun and is the headword of anoun phrase, and the argument is a prepositionalphrase.
The noun phrase with the indicator as itsheadword and the argument have the same parent inthe constituent parsing tree.For relation k, let f(zk, ik, ak) count the numberof induced indicator ik and argument ak pairs thatmatch one of the patterns above, and b is set to 0.8D,whereD is the number of decision documents.
Thenthe syntactic constraint is encoded in the inequalityEq[f(zk, ik, ak)] ?
b.Prevalence Constraints.
The prevalence con-straint is enforced on the number of times a relationis instantiated, in order to guarantee that every rela-tion has enough instantiations across the corpus andis task-relevant.
Again, we require each relation tohave induced instances in at least 80% of decisiondocuments.Occurrence Constraints.
Diversity of relationtypes is enforced through occurrence constraints.
Inparticular, for each decision document, we restricteach word to trigger at most two relation types as in-dicator and occur at most twice as part of a relation?sargument in expectation.
An entire span of argumentconstituent can appear in at most one relation type.Discourse Constraints.
The discourse constraintcaptures the insight that the final decision on an is-sue is generally made, or at least restated, at the endof the decision-related discussion.
As each decisiondocument is divided into four equal parts, we re-strict 50% of the relation instances to be from thelast quarter of the decision documents.6 FeaturesTable 1 lists the features we use for discoveringboth the decision cue relations and decision con-tent relations.
We start with a collection of domain-independent BASIC FEATURES shown to be use-ful in relation extraction (Banko and Etzioni, 2008;Chen et al, 2011).
Then we add MEETING FEA-TURES, STRUCTURAL FEATURES and SEMANTICFEATURES that have been found to be good pre-dictors for decision detection (Hsueh and Moore,2007) or meeting and decision summarization (Gal-308Basic Featuresunigram (stemmed)part-of-speech (POS)constituent label (NP, VP, S/SBAR (start with ?to?
))dependency labelMeeting FeaturesDialogue Act (DA) typespeaker roletopicStructural Features (Galley, 2006) (Wang and Cardie, 2011)in an Adjacency Pair (AP)?if in an AP, AP typeif in an AP, the other part is decision-related?if in an AP, the source part or target part?if in an AP and is source part, is the target positive feedback?if in an AP and is target part, is the source a question?Semantic Features (from WordNet) (Miller, 1995)first Synset of head word with the given POSfirst hypernym path for the first synset of head wordOther Features (only for Argument)number of words (without stopwords)has capitalized word or nothas proper noun or notTable 1: Features for Decision Cue and Decision Content re-lation extraction.
All features, except the last type of features,are used for both the indicator and argument.
(An AdjacencyPair (AP) is an important conversational analysis concept (Schegloffand Sacks, 1973).
In the AMI corpus, an AP pair consists of a sourceutterance and a target utterance, produced by different speakers.
)ley, 2006; Murray and Carenini, 2008; Ferna?ndez etal., 2008; Wang and Cardie, 2011).
Features em-ployed only for argument?s are listed in the last cat-egory in Table 1.After applying the features in Table 1 and theglobal constraints from Section 5 in preliminary ex-periments, we found that the extracted relation in-stances are mostly derived from decision cue rela-tions.
Sample decision cue relations and instancesare displayed in Table 2 and are not necessarily sur-prising: previous research (Hsueh and Moore, 2007)has observed the important role of personal pro-nouns, such as ?we?
and ?I?, in decision-making ex-pressions.
Notably, the decision cue is always fol-lowed by the decision content.
As a result, we in-clude two additional features (see Table 3) that relyon the cues to identify the decision content.
Finally,we disallow content relation instances with an argu-ment containing just a personal pronoun.7 Experiment SetupThe Corpus.
We evaluate our approach on theAMI meeting corpus (Carletta et al, 2005) that con-sists of 140 multi-party meetings with a wide rangeDecision Cue Relations Relation InstancesGroup Wrap-up / Recap we have, we are, we say, we wantPersonal Explanation I mean, I think, I guess, I (would) saySuggestion do we, we (could/should) doFinal Decision it is (gonna), it will, we willTable 2: Sample Decision Cue relation instances.
The wordsin parentheses are filled for illustration purposes, while they arenot part of the relation instances.Discourse Featuresclause position (first, second, other)position to the first decision cue relation if any (before, after)Table 3: Additional features for Decision Content relation ex-traction, inspired by Decision Cue relations.
Both indicator andargument use those features.of annotations.
The 129 scenario-driven meetingsinvolve four participants playing different roles ona design team.
Importantly, the corpus includes ashort (usually one-sentence), manually constructedabstract summarizing each decision discussed in themeeting.
In addition, all of the dialogue acts thatsupport (i.e., are relevant to) each decision are an-notated as such.
We use the manually constructeddecision abstracts as gold-standard summaries.System Inputs.
We consider two system input set-tings.
In the True Clusterings setting, we usethe AMI annotations to create perfect partitioningsof the DRDAs for input to the summarization sys-tem; in the System Clusterings setting, we em-ploy a hierarchical agglomerative clustering algo-rithm used for this task in previous work (Wang andCardie, 2011).
The Wang and Cardie (2011) cluster-ing method groups DRDAs according to their LDAtopic distribution similarity.
As better approachesfor DRDA clustering become available, they couldbe employed instead.Evaluation Metrics.
We use the widely acceptedROUGE (Lin and Hovy, 2003) evaluation measure.We adopt the ROUGE-1 and ROUGE-SU4 met-rics from (Hachey, 2009), and also use ROUGE-2.
We choose the stemming option of the ROUGEsoftware at http://berouge.com/ and removestopwords from both the system and gold-standardsummaries.Training and Parameters.
The Dirichlet hyper-parameters are set to 0.1 for the priors.
When train-ing the model, ten random restarts are performedand each run stops when reaching a convergencethreshold (10?5).
Then we select the posterior with309the lowest final free energy.
For the parametersused in posterior constraints, we either adopt themfrom (Chen et al, 2011) or choose them arbitrarilywithout tuning in the spirit of making the approachdomain-independent.We compare our decision summarization ap-proach with (1) two unsupervised baselines, (2)the unsupervised relation-based approach of Hachey(2009), (3) two supervised methods, and (4) an up-perbound derived from the gold standard decisionabstracts.The LONGEST DA Baseline.
As in Riedhammeret al (2010) and Wang and Cardie (2011), this base-line simply selects the longest DRDA in each clus-ter as the summary.
Thus, this baseline performsutterance-level decision summarization.
Althoughit?s possible that decision content is spread over mul-tiple DRDAs in the cluster, this baseline and the nextallow us to determine summary quality when sum-maries are restricted to a single utterance.The PROTOTYPE DA Baseline.
Following Wangand Cardie (2011), the second baseline selects thedecision cluster prototype (i.e., the DRDA with thelargest TF-IDF similarity with the cluster centroid)as the summary.The Generic Relation Extraction (GRE) Methodof Hachey (2009).
Hachey (2009) presentsa generic relation extraction (GRE) for multi-document summarization.
Informative sentencesare extracted to form summaries instead of relationinstances.
Relation types are discovered by LatentDirichlet Allocation, such that a probability isoutput for each relation instance given a topic(equivalent to relation).
Their relation instances arenamed entity(NE)-mention pairs conforming to aset of pre-specified rules.
For comparison, we usethese same rules to select noun-mention pairs ratherthan NE-mention pairs, which is better suited tomeetings, which do not contain many NEs.66Because an approximate set cover algorithm is used inGRE, one decision-related dialogue act (DRDA) is extractedeach time until the summary reaches the desired length.
We runtwo sets of experiments using this GRE system with differentoutput summaries ?
one selects one entire DRDA as the finalsummary (as Hachey (2009) does), and another one outputs therelation instances with highest probability conditional on eachrelation type.
We find that the first set of experiments gets betterTrue ClusteringsR-1 R-2 R-SU4PREC REC F1 F1 F1BaselinesLongest DA 34.06 31.28 32.61 12.03 13.58Prototype DA 40.72 28.21 33.32 12.18 13.46GRE5 topics 38.51 30.66 34.13 11.44 13.5410 topics 39.39 31.01 34.69 11.28 13.4215 topics 38.00 29.83 33.41 11.40 12.8020 topics 37.24 30.13 33.30 10.89 12.95Supervised MethodsCRF 53.95 26.57 35.61 11.52 14.07SVM 42.30 41.49 40.87 12.91 16.29Our Method5 Relations 39.33 35.12 37.10 12.05 14.2910 Relations 37.94 37.03 37.47 12.20 14.5915 Relations 37.36 37.43 37.39 11.47 14.0020 Relations 37.27 37.64 37.45 11.40 13.90Upperbound 100.00 45.05 62.12 33.27 34.89Table 4: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4) scores for summaries produced by the baselines,GRE (Hachey, 2009)?s best results, the supervised methods, ourmethod and an upperbound ?
all with perfect/true DRDA clus-terings.Supervised Learning (SVMs and CRFs).
Wealso compare our approach to two supervised learn-ing methods ?
Support Vector Machines (Joachims,1998) with RBF kernel and order-1 ConditionalRandom Fields (Lafferty et al, 2001) ?
trained us-ing the same features as our system (see Tables 1and 3) to identify the important tokens to include inthe decision abstract.
Three-fold cross validation isconducted for both methods.Upperbound.
We also compute an upperboundthat reflects the gap between the best possible ex-tractive summaries and the human-written abstractsaccording to the ROUGE score: for each cluster ofDRDAs, we select the words that also appear in theassociated decision abstract.8 Results and DiscussionTable 4 illustrates that, using True (DRDA) Clus-terings our method outperforms the two baselinesand the generic relation extraction (GRE) based sys-tem in terms of F score in ROUGE-1 and ROUGE-SU4 with varied numbers of relations.
Note that forGRE based approach, we only list out their best re-sults for utterance-level summarization.
If using thesalient relation instances identified by GRE as thesummaries, the ROUGE results will be significantlyperformance than the second, so we only report the best resultsfor their system in this paper.310System ClusteringsR-1 R-2 R-SU4PREC REC F1 F1 F1BaselinesLongest DA 17.06 11.64 13.84 2.76 3.34Prototype DA 18.14 10.11 12.98 2.84 3.09GRE5 topics 17.10 9.76 12.40 3.03 3.4110 topics 16.28 10.03 12.35 3.00 3.3615 topics 16.54 10.90 13.04 2.84 3.2820 topics 17.25 8.99 11.80 2.90 3.23Supervised MethodsCRF 47.36 15.34 23.18 6.12 9.21SVM 39.50 18.49 25.19 6.15 9.86Our Method5 Relations 16.12 18.93 17.41 3.31 5.5610 Relations 16.27 18.93 17.50 3.32 5.6915 Relations 16.42 19.14 17.68 3.47 5.7520 Relations 16.75 18.25 17.47 3.33 5.64Table 5: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4) scores for summaries produced by the baselines,GRE (Hachey, 2009)?s best results, the supervised methods andour method ?
all with system clusterings.lower.
When measured by ROUGE-2, our methodstill have better or comparable performances thanother unsupervised methods.
Moreover, our sys-tem achieves F scores in between those of the su-pervised learning methods, performing better thanthe CRF in both recall and F score.
The recall scorefor the upperbound in ROUGE-1, on the other hand,indicates that there is still a wide gap between theextractive summaries and human-written abstracts:without additional lexical information (e.g., seman-tic class information, ontologies) or a real languagegeneration component, recall appears to be a bottle-neck for extractive summarization methods that se-lect content only from decision-related dialogue acts(DRDAs).Results using the System Clusterings (Table 5)are comparable, although all of the system and base-line scores are much lower.
Supervised methods getthe best F scores largely due to their high precision;but our method attains the best recall in ROUGE-1.Discussion.
To better exemplify the summariesgenerated by different systems, sample output foreach method is shown in Table 6.
The GRE systemuses an approximate algorithm for set cover extrac-tion, we list the first three selected DRDA in order.We see from the table that utterance-level extractivesummaries (Longest DA, Prototype DA, GRE) makemore coherent but still far from concise and compactDRDA (1): Uh the batteries, uh we also thought about that already,DRDA (2): uh will be chargeable with uh uh an option for amount stationDRDA (3): Maybe it?s better to to include rechargeable batteriesDRDA (4): We already decided that on the previous meeting.DRDA (5): which you can recharge through the docking station.DRDA (6): normal plain batteries you can buy at the supermarketor retail shop.
Yeah.Decision Abstract: The remote will use rechargeable batterieswhich recharge in a docking station.Longest DA & Prototype DA: normal plain batteries you canbuy at the supermarket or retail shop.
Yeah.GRE: 1st: normal plain batteries you can buy at the supermarketor retail shop.
Yeah.2nd: which you can recharge through the docking station.3rd: uh will be chargeable with uh uh an option for a mount stationSVM: batteries include rechargeable batteries decided rechargedocking stationCRF: chargeable station rechargeable batteriesOur Method: <option, for a mount station>,<include, rechargeable batteries>,<decided, that on the previous meeting>,<recharge, through the docking station>,<buy, normal plain batteries>Table 6: Sample system outputs by different methods are inthe third cell (methods?
names are in bold).
First cell containsthe six DRDAs supporting the decision abstracted in the secondcell.abstracts.
On the other hand, the supervised methods(SVM, CRF) that produce token-level extracts betteridentify the overall content of the decision abstract.Unfortunately, they require human annotation in thetraining phase; in addition, the output is ungrammat-ical and lacks coherence.
In comparison, our sys-tem presents the decision summary in the form ofphrase-based relations that provide a relatively com-prehensive expression.9 ConclusionsWe present a novel framework for focused meet-ing summarization based on unsupervised relationextraction.
Our approach is shown to outperformunsupervised utterance-level extractive summariza-tion baselines as well as an existing generic relation-extraction-based summarization method.
Our ap-proach also produces summaries competitive withthose generated by supervised methods in terms ofthe standard ROUGE score.
Overall, we find thatrelation-based methods for focused summarizationhave potential as a technique for supporting the gen-eration of abstractive decision summaries.Acknowledgments This work was supported in part byNational Science Foundation Grants IIS-0968450 andIIS-1111176, and by a gift from Google.311ReferencesMichele Banko and Oren Etzioni.
2008.
The TradeoffsBetween Open and Traditional Relation Extraction.
InProceedings of ACL-08: HLT, Columbus, Ohio.Michele Banko, Michael J Cafarella, Stephen Soderl,Matt Broadhead, and Oren Etzioni.
2007.
Open in-formation extraction from the web.
In In IJCAI, pages2670?2676.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022, March.Trung H. Bui, Matthew Frampton, John Dowding, andStanley Peters.
2009.
Extracting decisions frommulti-party dialogue using directed graphical modelsand semantic similarity.
In Proceedings of the SIG-DIAL 2009 Conference, pages 235?243.Giuseppe Carenini, Gabriel Murray, and Raymond Ng.2011.
Methods for Mining and Summarizing Text Con-versations.
Morgan & Claypool Publishers.Jean Carletta, Simone Ashby, Sebastien Bourban,Mike Flynn, Thomas Hain, Jaroslav Kadlec, VasilisKaraiskos, Wessel Kraaij, Melissa Kronenthal, Guil-laume Lathoud, Mike Lincoln, Agnes Lisowska, andMccowan Wilfried Post Dennis Reidsma.
2005.
Theami meeting corpus: A pre-announcement.
In In Proc.MLMI, pages 28?39.Harr Chen, Edward Benson, Tahira Naseem, and ReginaBarzilay.
2011.
In-domain relation discovery withmeta-constraints via posterior regularization.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies - Volume 1, HLT ?11, pages 530?540,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure trees.
In LREC.Raquel Ferna?ndez, Matthew Frampton, John Dowding,Anish Adukuzhiyil, Patrick Ehlen, and Stanley Peters.2008.
Identifying relevant phrases to summarize de-cisions in spoken meetings.
INTERSPEECH-2008,pages 78?81.Matthew Frampton, Jia Huang, Trung Huu Bui, and Stan-ley Peters.
2009.
Real-time decision detection inmulti-party dialogue.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 3 - Volume 3, pages 1133?1141.Michel Galley.
2006.
A skip-chain conditional randomfield for ranking meeting utterances by importance.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, pages 364?372.Joao Graca, Kuzman Ganchev, and Ben Taskar.
2008.Expectation maximization and posterior constraints.In J.C. Platt, D. Koller, Y.
Singer, and S. Roweis, edi-tors, Advances in Neural Information Processing Sys-tems 20, pages 569?576.
MIT Press, Cambridge, MA.Ben Hachey.
2009.
Multi-document summarisation us-ing generic relation extraction.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing: Volume 1 - Volume 1, EMNLP?09, pages 420?429, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Pei-yun Hsueh and Johanna Moore.
2007.
What deci-sions have you made: Automatic decision detection inconversational speech.
In In NAACL/HLT 2007.Thorsten Joachims.
1998.
Text categorization with Sup-port Vector Machines: Learning with many relevantfeatures.
In Claire Ne?dellec and Ce?line Rouveirol,editors, Machine Learning: ECML-98, volume 1398,chapter 19, pages 137?142.
Berlin/Heidelberg.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics - Volume 1, ACL ?03, pages 423?430, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the Eighteenth InternationalConference on Machine Learning, ICML ?01, pages282?289, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Shih-Hsiang Lin and Berlin Chen.
2010.
A risk mini-mization framework for extractive speech summariza-tion.
In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics, ACL ?10,pages 79?87, Stroudsburg, PA, USA.
Association forComputational Linguistics.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic evalu-ation of summaries using n-gram co-occurrence statis-tics.
In Proceedings of the 2003 Conference of theNorth American Chapter of the Association for Com-putational Linguistics on Human Language Technol-ogy - Volume 1, pages 71?78.Sameer Maskey and Julia Hirschberg.
2005.
ComparingLexical, Acoustic/Prosodic, Structural and DiscourseFeatures for Speech Summarization.
In Proc.
Euro-pean Conference on Speech Communication and Tech-nology (Eurospeech).George A. Miller.
1995.
Wordnet: a lexical database forenglish.
Commun.
ACM, 38:39?41, November.Gabriel Murray and Giuseppe Carenini.
2008.
Summa-rizing spoken and written conversations.
In Proceed-312ings of the Conference on Empirical Methods in Natu-ral Language Processing, pages 773?782.Gabriel Murray, Giuseppe Carenini, and Raymond Ng.2010a.
Interpretation and transformation for abstract-ing conversations.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, HLT ?10, pages 894?902, Stroudsburg, PA, USA.Association for Computational Linguistics.Gabriel Murray, Giuseppe Carenini, and Raymond T. Ng.2010b.
Generating and validating abstracts of meetingconversations: a user study.
In INLG?10.Matthew Purver, John Dowding, John Niekrasz, PatrickEhlen, Sharareh Noorbaloochi, and Stanley Peters.2007.
Detecting and summarizing action items inmulti-party dialogue.
In in Proceedings of the 8th SIG-dial Workshop on Discourse and Dialogue.Korbinian Riedhammer, Benoit Favre, and DilekHakkani-Tu?r.
2010.
Long story short - global unsu-pervised models for keyphrase based meeting summa-rization.
Speech Commun., 52(10):801?815, October.E.
A. Schegloff and H. Sacks.
1973.
Opening up clos-ings.
Semiotica, 8(4):289?327.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning Syntactic Patterns for Automatic HypernymDiscovery.
In Lawrence K. Saul, Yair Weiss, andLe?on Bottou, editors, Advances in Neural InformationProcessing Systems 17, pages 1297?1304.
MIT Press,Cambridge, MA.Lu Wang and Claire Cardie.
2011.
Summarizing deci-sions in spoken meetings.
In Proceedings of the Work-shop on Automatic Summarization for Different Gen-res, Media, and Languages, pages 16?24, Portland,Oregon, June.
Association for Computational Linguis-tics.Klaus Zechner.
2002.
Automatic summarization ofopen-domain multiparty dialogues in diverse genres.Comput.
Linguist., 28:447?485, December.Appendix A Generative ProcessThe entire generative process is as follows (?Dir?and ?Mult?
refer to the Dirichlet distribution andmultinomial distribution):1.
For each relation type k:(a) For each indicator feature ?i, draw feature distribu-tions ?ik,?i , ?bik,?i ?
Dir(?0)(b) For each argument feature ?a, draw feature distri-butions ?ak,?a , ?bak,?a ?
Dir(?0)(c) Draw location distribution ?k ?
Dir(?0)2.
For each relation type k and decision document d:(a) Select decision document segment sd,k ?Mult(?k)(b) Select DRDA zd,k uniformly from segment sd,k,and indicator id,k and argument constituent ad,kuniformly from DRDA zd,k3.
For each indicator word w in every decision document d:(a) For each indicator feature ?i(w) ?Mult( 1Z?Kk=1?k,?i), where ?k,?i is ?ik,?i ifid,k = w and ?bik,?i otherwise.
Z is thenormalization factor.4.
For each argument constituent x in every decision docu-ment d:(a) For each indicator feature ?a(x) ?Mult( 1Z?Kk=1?k,?a), where ?k,?a is ?ak,?aif ad,k = x and ?bak,?a otherwise.
Z is thenormalization factor.Given ?0 and ?0, The joint distribution of a set offeature parameters ?, the location distributions ?, aset of DRDAs z, and the selected indicators i andarguments a is:P (?, ?, z, i, a; ?0, ?0) =K?k=1P (?ik; ?0)P (?bik ; ?0)P (?ak |?0)P (?bak ; ?0)P (?k;?0)?
(D?d=1P (id,k; zd,k)P (ad,k; zd,k)P (zd,k; sd,k)P (sd,k;?k)?
(P (w = id,k; ?ik)?w 6=id,kP (w; ?bik ))?
(P (x = ad,k; ?ak)?x 6=ad,kP (x; ?bak )))Appendix B Updates for the ParametersThe constraints put on the posterior will only affectthe update for q(z).
For q(?
), the update isq(?)
= argminq(?)KL(q(?)?q?(?
)), (3)where q?(?)
?
expEq(z)[log p(?, z, x)], and q(?
)is updated to q?(?).
For q(z), the update isq(z) = argminq(z)KL(q(z)?q?(z))s.t.
Eq(z)[fc(z)] ?
bc, ?c ?
C (4)where q?
(z) ?
expEq(?
)[log p(?, z, x)].
Equa-tion 4 is easily solved via the dual (Graca et al,2008) (Chen et al, 2011).313
