Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 580?589,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPTopic-wise, Sentiment-wise, or Otherwise?Identifying the Hidden Dimension for Unsupervised Text ClassificationSajib Dasgupta and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{sajib,vince}@hlt.utdallas.eduAbstractWhile traditional work on text clusteringhas largely focused on grouping docu-ments by topic, it is conceivable that a usermay want to cluster documents along otherdimensions, such as the author?s mood,gender, age, or sentiment.
Without know-ing the user?s intention, a clustering al-gorithm will only group documents alongthe most prominent dimension, which maynot be the one the user desires.
To ad-dress this problem, we propose a novelway of incorporating user feedback intoa clustering algorithm, which allows auser to easily specify the dimension alongwhich she wants the data points to be clus-tered via inspecting only a small numberof words.
This distinguishes our methodfrom existing ones, which typically re-quire a large amount of effort on the partof humans in the form of document an-notation or interactive construction of thefeature space.
We demonstrate the viabil-ity of our method on several challengingsentiment datasets.1 IntroductionText clustering is one of the most important appli-cations in Natural Language Processing (NLP).
Acommon approach to this problem consists of (1)computing the similarity between each pair of doc-uments, each of which is typically represented as abag of words; and (2) using an unsupervised clus-tering algorithm to partition the documents.
Themajority of existing work on text clustering hasfocused on topic-based clustering, where high ac-curacies can be achieved even for datasets with alarge number of classes (e.g., 20 Newsgroups).On the other hand, there has been relatively lit-tle work on sentiment-based clustering and the re-lated task of unsupervised polarity classification,where the goal is to cluster (or classify) a set ofdocuments (e.g., reviews) according to the po-larity (e.g., ?thumbs up?
or ?thumbs down?)
ex-pressed by the author in an unsupervised man-ner.
Despite the large amount of recent work onsentiment analysis and opinion mining, much ofit has focused on supervised methods (e.g., Panget al (2002), Kim and Hovy (2004), Mullen andCollier (2004)).
One weakness of these existingsupervised polarity classification systems is thatthey are typically domain- and language-specific.Hence, when given a new domain or language,one needs to go through the expensive process ofcollecting a large amount of annotated data in or-der to train a high-performance polarity classifier.Some recent attempts have been made to leverageexisting sentiment corpora or lexica to automati-cally create annotated resources for new domainsor languages.
However, such methods requirethe existence of either a parallel corpus/machinetranslation engine for projecting/translating anno-tations/lexica from a resource-rich language to thetarget language (Banea et al, 2008; Wan, 2008),or a domain that is ?similar?
enough to the targetdomain (Blitzer et al, 2007).
When the target do-main or language fails to meet this requirement,sentiment-based clustering or unsupervised polar-ity classification become appealing alternatives.Unfortunately, to our knowledge, these tasks arelargely under-investigated in the NLP community.Turney?s (2002) work is perhaps one of the mostnotable examples of unsupervised polarity classi-fication.
However, while his system learns the se-mantic orientation of the phrases in a review in anunsupervised manner, this information is used topredict the polarity of a review heuristically.Despite its practical significance, sentiment-based clustering is a challenging task.
To illus-trate its difficulty, consider the task of clusteringa set of movie reviews.
Since each review maycontain a description of the plot and the author?s580sentiment, a clustering algorithm may cluster re-views along either the plot dimension or the senti-ment dimension; and without knowing the user?sintention, they will be clustered along the mostprominent dimension.
Assuming the usual bag-of-words representation, the most prominent di-mension will more likely be plot, as it is not un-common for a review to be devoted almost exclu-sively to the plot, with the author briefly express-ing her sentiment only at the end of the review.Even if the reviews contain mostly subjective ma-terial, the most prominent dimension may still notbe sentiment, due to the fact that many reviews aresentimentally ambiguous.
Specifically, a reviewermay have negative opinions on the actors but at thesame time talk enthusiastically about how muchshe enjoyed the plot.
The presence of both posi-tive and negative sentiment-bearing words in thesereviews renders the sentiment dimension hidden(i.e., less prominent) as far as clustering is con-cerned.
Therefore, there is no guarantee that theclustering algorithm will automatically produce asentiment-based clustering of the reviews.Hence, it is important for a user to provide feed-back on the clustering process to ensure that thereviews are clustered along the sentiment dimen-sion, possibly in an interactive manner.
One wayto do this would be to ask the user to annotatea small number of reviews with polarity infor-mation, possibly through an active learning pro-cedure to minimize human intervention (Dredzeand Crammer, 2008).
Another way would be tohave the user explicitly identify the relevant fea-tures (in our case, the sentiment-bearing words) atthe beginning of the clustering process (Liu et al,2004), or incrementally construct the set of rele-vant features in an interactive fashion (Bekkermanet al, 2007; Raghavan and Allan, 2007; Roth andSmall, 2009).
In addition, the user may supplyconstraints on which pairs of documents must ormust not appear in the same cluster (Wagstaff etal., 2001), or simply tell the algorithm whethertwo clusters should be merged or split during theclustering process (Balcan and Blum, 2008).
It isworth noting that many of these feedback mech-anisms were developed by machine learning re-searchers for general clustering tasks and not forsentiment-based clustering.Our goal in this paper is to propose a novelmechanism allowing a user to cluster a set of docu-ments along the desired dimension, which may bea hidden dimension, with very limited user feed-back.
In comparison to the aforementioned feed-back mechanisms, ours is arguably much simpler:we only require that the user select a dimensionby examining a small number of features for eachdimension, as opposed to having the user gener-ate the feature space in an interactive manner oridentify clusters that need to be merged or split.
Inparticular, identifying clusters for merging or split-ting in Balcan and Blum?s algorithm may not be aseasy as it appears: for each MERGE or SPLIT de-cision the user makes, she has to sample a largenumber of documents from the cluster(s), readthrough the documents, and base her decision onthe extent to which the documents are (dis)similarto each other.
Perhaps more importantly, our hu-man experiments involving five users indicate thatall of them can easily identify the sentiment di-mension based on the features, thus providing sug-gestive evidence that our method is viable.In sum, our contributions in this paper are three-fold.
First, we propose a novel feedback mecha-nism for clustering allowing a user to easily spec-ify the dimension along which she wants datapoints to be clustered and apply the mechanismto the challenging, yet under-investigated problemof sentiment-based clustering.
Second, spectrallearning, which is the core of our method, has notbeen applied extensively to NLP problems, and wehope that our work can increase the awareness ofthis powerful machine learning technique in theNLP community.
Finally, we demonstrate the via-bility of our method not only by evaluating its per-formance on sentiment datasets, but also via a setof human experiments, which is typically absentin papers that involve algorithms for incorporatinguser feedback.The rest of the paper is organized as follows.Section 2 presents the basics of spectral clustering,which will facilitate the discussion of our feedbackmechanism in Section 3.
We describe our humanexperiments and evaluation results on several sen-timent datasets in Section 4, and present our con-clusions in Section 5.2 Spectral ClusteringWhen given a clustering task, an important ques-tion to ask is: which clustering algorithm shouldwe use?
A popular choice is k-means.
Neverthe-less, it is well-known that k-means has the majordrawback of not being able to separate data points581that are not linearly separable in the given featurespace (e.g., see Dhillon et al (2004) and Cai et al(2005)).
Spectral clustering algorithms were de-veloped in response to this problem with k-means.The central idea behind spectral clustering is to(1) construct a low-dimensional space from theoriginal (typically high-dimensional) space whileretaining as much information about the originalspace as possible, and (2) cluster the data points inthis low-dimensional space.
The rest of this sec-tion provides the details of spectral clustering.2.1 AlgorithmAlthough there are several well-known spectralclustering algorithms in the literature (e.g., Weiss(1999), Shi and Malik (2000), Kannan et al(2004)), we adopt the one proposed by Ng et al(2002), as it is arguably the most widely-used.
Thealgorithm takes as input a similarity matrix S cre-ated by applying a user-defined similarity functionto each pair of data points.
Below are the mainsteps of the algorithm:1.
Create the diagonal matrix D whose (i,i)-th entry is the sum of the i-th row of S,and then construct the Laplacian matrix L =D?1/2SD?1/2.2.
Find the eigenvalues and eigenvectors of L.3.
Create a new matrix from the m eigenvectorsthat correspond to the m largest eigenvalues.14.
Each data point is now rank-reduced to apoint in the m-dimensional space.
Normal-ize each point to unit length (while retainingthe sign of each value).5.
Cluster the resulting data points using k-means.In essence, each dimension in the reduced spaceis defined by exactly one eigenvector.
The reasonwhy eigenvectors with large eigenvalues are usedis that they capture the largest variance in the data.As a result, each of them can be thought of as re-vealing an important dimension of the data.2.2 Clustering with EigenvectorsAs Ng et al (2002) point out, ?different authorsstill disagree on which eigenvectors to use, andhow to derive clusters from them?.
There are twocommon methods for deriving clusters using theeigenvectors.
These methods will serve as ourbaselines in our evaluation.1For brevity, we will refer to the eigenvector with the n-thlargest eigenvalue simply as the n-th eigenvector.Method 1: Using the second eigenvector onlyThe first method is to use only the second eigen-vector, e2, to partition the points.
Besides reveal-ing one of the most important dimensions of thedata, this eigenvector induces an intuitively idealpartition of the data ?
the partition induced by theminimum normalized cut of the similarity graph2,where the nodes are the data points and the edgeweights are the pairwise similarity values of thepoints (Shi and Malik, 2000).
Clustering in a one-dimensional space is trivial: since we have a lin-earization of the points, all we need to do is todetermine a threshold for partitioning the points.However, we follow Ng et al (2002) and clusterusing 2-means in this one-dimensional space.Method 2: Using m eigenvectorsRecall from Section 2.1 that after eigen-decomposing the Laplacian matrix, each datapoint is represented by m co-ordinates.
In thesecond method, we simply use 2-means to clusterthe data points in this m-dimensional space,effectively exploiting all of the m eigenvectors.3 Our ApproachAs mentioned before, sentiment-based clusteringis challenging, in part due to the fact that the re-views can be clustered along more than one di-mension.
In this section, we propose and incor-porate a user feedback mechanism into a spec-tral clustering algorithm, which makes it easy fora user to specify the dimension along which shewants to cluster the data points.Recall that our method first applies spectralclustering to reveal the most important dimensionsof the data, and then lets the user select the de-sired dimension.
To motivate the importance ofuser feedback, it helps to understand why the twobaseline clustering algorithms described in Sec-tion 2.2, which are also based on spectral meth-ods but do not rely on user feedback, may not al-ways yield a sentiment-based clustering.
To be-gin with, consider the first method, where onlythe second eigenvector is used to induce the par-tition.
Recall that the second eigenvector revealsthe most prominent dimension of the data.
Hence,if sentiment is not the most prominent dimension(which can happen if the non-sentiment-bearing2Using the normalized cut (as opposed to the usual cut)ensures that the size of the two clusters are relatively bal-anced, avoiding trivial cuts where one cluster is empty andthe other is full.
See Shi and Malik (2000) for details.582words outnumber the sentiment-bearing words inthe bag-of-words representation of a review), thenthe resulting clustering of the reviews may not besentiment-oriented.
A similar line of reasoningcan be used to explain why the second baselineclustering algorithm, which clusters based on allof the eigenvectors in the low-dimensional space,may not always work well.
Since each eigenvectorcorresponds to a different dimension (and, in par-ticular, some of them correspond to non-sentimentdimensions), using all of them to represent a re-view may hamper the accurate computation of thesimilarity of two reviews as far as clustering alongthe sentiment dimension is concerned.
In the restof this section, we discuss the major steps of ouruser-feedback mechanism in detail.Step 1: Identify the important dimensionsTo identify the important dimensions of the givenreviews, we take the top eigenvectors computedfrom the eigen-decomposition of the Laplacianmatrix, which is in turn formed from the input sim-ilarity matrix.
We compute the similarity betweentwo reviews by taking the dot product of their fea-ture vectors (see Section 4.1 for details on featurevector generation).
Following Ng et al, we set thediagonal entries of the similarity matrix to 0.Step 2: Identify the relevant featuresGiven the eigen-decomposition from Step 1, wefirst obtain the second through the fifth eigenvec-tors3, which as mentioned above, correspond tothe most important dimensions of the data.
Then,we ask the user to select one of the four dimen-sions defined by these eigenvectors according totheir relevance to sentiment.
One way to do thisis to (1) induce one partition of the reviews fromeach of the four eigenvectors, using a procedureidentical to Method 1 in Section 2.2, and (2) havethe user inspect the four partitions and decidewhich corresponds most closely to a sentiment-based clustering.
The main drawback associatedwith this kind of user feedback is that the user mayhave to read a large number of reviews in order tomake a decision.
Hence, to reduce human effort,we employ an alternative procedure: we (1) iden-tify the most informative features for characteriz-ing each partition, and (2) have the user inspectjust the features rather than the reviews.While traditional feature selection techniquessuch as log-likelihood ratio and information3The first eigenvector is not used because it is a constantvector, meaning that it cannot be used to partition the data.gain can be applied to identify these informa-tive features (see Yang and Pedersen (1997)for an overview), we employ a more sophisti-cated feature-ranking method that we call max-imum margin feature ranking (MMFR).
Recallthat a maximum margin classifier (e.g., a supportvector machine) separates data points from twoclasses while maximizing the margin of separa-tion.
Specifically, a maximum margin hyperplaneis defined by w ?
x ?
b = 0, where x is a fea-ture vector representing an arbitrary data point,and w (a weight vector) and b (a scalar) are pa-rameters that are learned by solving the followingconstrained optimization problem:argmin12?w?2+ C?i?isubject toci(w ?
xi?
b) ?
1?
?i, 1 ?
i ?
n,where ci?
{+1,?1} is the class of the i-th train-ing point xi, ?iis the degree of misclassificationof xi, and C is a regularization parameter that bal-ances training error and model complexity.We use w to identify the most informative fea-tures for a partition.
Note that a feature with alarge positive weight is strongly indicative of thepositive class, whereas a feature with a large neg-ative weight is strongly indicative of the negativeclass.
In other words, the most informative fea-tures are those with large absolute weight values.We exploit this observation and identify the mostinformative features for a partition by (1) trainingan SVM classifier4 on the partition, where datapoints in the same cluster belong to the same class;(2) sorting the features according to the SVM-learned feature weights; and (3) generating tworanked lists of informative features using the topand bottom 100 features, respectively.Given the ranked lists generated for each of thefour partitions, the user will select one of the parti-tions/dimensions as most relevant to sentiment byinspecting as many features in the ranked lists asneeded.
After picking the most relevant dimen-sion, the user will label one of the two feature listsassociated with this dimension as POSITIVE andthe other as NEGATIVE.
Since each feature listrepresents one of the clusters, the cluster associ-ated with the positive list is labeled POSITIVE and4All the SVM classifiers in this paper are trained usingthe SVMlight package (Joachims, 1999), with the learningparameters set to their default values.583the cluster associated with the negative list is la-beled NEGATIVE.In comparison to existing user feedback mech-anisms for assisting a clustering algorithm, oursrequires comparatively little human intervention:we only require that the user select a dimension byexamining a small number of features, as opposedto having the user construct the feature space oridentify clusters that need to be merged or split asis required with other methods.Step 3: Identify the unambiguous reviewsThere is a caveat, however.
As mentioned in theintroduction, many reviews contain both positiveand negative sentiment-bearing words.
These am-biguous reviews are more likely to be clusteredincorrectly than their unambiguous counterparts.Now, since the ranked lists of features are derivedfrom the partition, the presence of these ambigu-ous reviews can adversely affect the identificationof informative features using MMFR.
As a result,we remove the ambiguous reviews before derivinginformative features from a partition.We employ a simple method for identifying un-ambiguous reviews.
In the computation of eigen-values, each data point factors out the orthogo-nal projections of each of the other data pointswith which they have an affinity.
Ambiguous datapoints receive the orthogonal projections fromboth the positive and negative data points, andhence they have near zero values in the pivoteigenvectors.
We exploit this important informa-tion.
The basic idea is that the data points withnear zero values in the eigenvectors are more am-biguous than those with large absolute values.
Asa result, we posit 250 reviews from each clusterwhose corresponding values in the eigenvector arefarthest away from zero as unambiguous, and in-duce the ranked list of features only from the re-sulting 500 unambiguous reviews.5Step 4: Cluster along the selected dimensionFinally, we employ the 2-means algorithm to clus-ter all the reviews along the dimension (i.e., theeigenvector) selected by the user, regardless ofwhether a review is ambiguous or not.5Note that 500 is a somewhat arbitrary choice.
Under-lying this choice is our assumption that a fraction of the re-views is unambiguous.
As we will see in the evaluation sec-tion, these 500 reviews can be classified with a high accuracy;consequently, the features induced from the resulting clus-ters are also of high quality.
Additional experiments revealthat the list of top-ranking features does not change signifi-cantly when induced from a smaller number of unambiguousreviews.4 Evaluation4.1 Experimental SetupDatasets.
We use five sentiment classificationdatasets, including the widely-used movie reviewdataset [MOV] (Pang et al, 2002) as well as fourdatasets containing reviews of four different typesof products from Amazon [books (BOO), DVDs(DVD), electronics (ELE), and kitchen appliances(KIT)] (Blitzer et al, 2007).
Each dataset has2000 labeled reviews (1000 positives and 1000negatives).
To illustrate the difference betweentopic-based clustering and sentiment-based clus-tering, we will also show topic-based clusteringresults on POL, a dataset created by taking all thedocuments from two sections of 20 Newsgroups,namely, sci.crypt and talks.politics.To preprocess a document, we first tokenize anddowncase it, and then represent it as a vector ofunigrams, using frequency as presence.
In ad-dition, we remove from the vector punctuation,numbers, words of length one, and words that oc-cur in only a single review.
Following the commonpractice in the information retrieval community,we also exclude words with high document fre-quency, many of which are stopwords or domain-specific general-purpose words (e.g., ?movies?
inthe movie domain).
A preliminary examinationof our evaluation datasets reveals that these wordstypically comprise 1?2% of a vocabulary.
The de-cision of exactly how many terms to remove fromeach dataset is subjective: a large corpus typicallyrequires more removals than a small corpus.
To beconsistent, we simply sort the vocabulary by doc-ument frequency and remove the top 1.5%.Evaluation metrics.
We employ two evaluationmetrics.
First, we report results in terms of the ac-curacy achieved on the 2000 labeled reviews foreach dataset.
Second, following Kamvar et al(2003), we evaluate the clusters produced by ourapproach against the gold-standard clusters usingthe Adjusted Rand Index (ARI).
ARI ranges from?1 to 1; better clusterings have higher ARI values.4.2 Baseline SystemsClustering using the second eigenvector only.As our first baseline, we adopt Shi and Malik?s ap-proach and cluster the reviews using only the sec-ond eigenvector, e2, as described in Section 2.2.Results on POL and the five sentiment datasets are584Accuracy Adjusted Rand IndexSystem Variation POL MOV KIT BOO DVD ELE POL MOV KIT BOO DVD ELEBaseline: 2nd eigenvector 93.7 70.9 69.7 58.9 55.3 50.8 0.76 0.17 0.15 0.03 0.01 0.01Baseline: m eigenvectors 95.9 59.3 63.2 60.1 62.5 63.8 0.84 0.03 0.07 0.04 0.06 0.08Our approach 93.7 70.9 69.7 69.5 70.8 65.8 0.76 0.17 0.15 0.15 0.17 0.10Table 1: Results in terms of accuracy and Adjusted Rand Index for the six datasets.shown in row 1 of Table 1.6 As we can see, thisbaseline achieves an accuracy of 90% on POL, buta much lower accuracy (of 50?70%) on the sen-timent datasets.
The same performance trend canbe observed with ARI.
These results provide sup-port for the claim that sentiment-based clusteringis more difficult than topic-based clustering.In addition, it is worth noting that the base-line achieves much lower accuracies and ARI val-ues on BOO, DVD, and ELE than on the re-maining two sentiment datasets.
Since e2cap-tures the most prominent dimension, these resultssuggest that sentiment dimension is not the mostprominent dimension in these three datasets.
Infact, this is intuitively plausible.
For instance,in the book domain, positive book reviews typ-ically contain a short description of the content,with the reviewer only briefly expressing her sen-timent somewhere in the review.
Similarly for theelectronics domain: electronic product reviews aretypically aspect-oriented, with the reviewer talk-ing about the pros and cons of each aspect of theproduct (e.g., battery, durability).
Since the re-views are likely to contain both positive and nega-tive sentiment-bearing words, the sentiment-basedclustering is unlikely to be captured by e2.Clustering using top five eigenvectors.
As oursecond baseline, we represent each data pointusing the top five eigenvectors (i.e., e1throughe5), and cluster them using 2-means in this 5-dimensional space, as described in Section 2.2.Hence, this can be thought of as an ?ensemble?approach, where the clustering decision is collec-tively made by the five eigenvectors.Results are shown in row 2 of Table 1.
Incomparison to the first baseline, we see improve-ments in accuracy and ARI for the three datasetson which the first baseline performs poorly (i.e.,BOO, DVD, and ELE), with the most drasticimprovement observed on ELE.
On the otherhand, performance on the remaining two senti-6Owing to the randomness in the choice of seeds for 2-means, these and all other experimental results involving 2-means are averaged over ten independent runs.ment datasets deteriorates.
These results can beattributed to the fact that for BOO, DVD, andELE, e2does not capture the sentiment dimension,but since some other eigenvector in the ensembledoes, we see improvements.
On the other hand, e2has already captured the sentiment dimension inMOV and KIT; as a result, employing additionaldimensions, which may not be sentiment-related,may only introduce noise into the computation ofthe similarities between the reviews.4.3 Our ApproachHuman experiments.
Unlike the two baselines,our approach requires users to specify which of thefour dimensions (defined by the second throughfifth eigenvectors) are most closely related to sen-timent by inspecting a set of features derived fromthe unambiguous reviews for each dimension us-ing MMFR.
To better understand how easy it isfor a human to select the desired dimension giventhe features, we performed the experiment inde-pendently with five humans (all of whom are com-puter science graduate students not affiliated withthis research) and computed the agreement rate.More specifically, for each dataset, we showedeach human judge the top 100 features for eachcluster according to MMFR (see Tables 4?6 fora snippet).
In addition, we informed them of theintended dimension: for example, for POL, thejudge was told that the intended clustering is Poli-tics vs. Science.
Also, if she determined that morethan one dimension was relevant to the intendedclustering, she was instructed to rank these dimen-sions in terms of their degree of relevance, wherethe most relevant one would appear first in the list.The dimensions (expressed in terms of the IDsof the eigenvectors) selected by each of the fivejudges for each dataset are shown in Table 2.
Theagreement rate (shown in the last row of the ta-ble) was computed based on only the highest-ranked dimension selected by each judge.
As wecan see, perfect agreement is achieved for four ofthe five sentiment datasets, and for the remainingtwo datasets, near-perfect agreement is achieved.585Judge POL MOV KIT BOO DVD ELE1 2,3,4 2 2 4 3 32 2,4 2 2 4 3 33 4 2,4 4 4 3 34 2,3 2 2 4 3 3,45 2 2 2 4 3 3Agr 80% 100% 80% 100% 100% 100%Table 2: Human agreement rate.POL MOV KIT BOO DVD ELEAcc 99.8 87.0 87.6 86.2 87.4 77.6Table 3: Accuracies on unambiguous documents.These results together with the fact that it took 5?6 minutes to identify the relevant dimension, indi-cate that asking a human to determine the intendeddimension based on solely the ?informative?
fea-tures is a viable task.Clustering results.
Next, we cluster all 2000documents for each dataset using the dimensionselected by the majority of the human judges.
Theclustering results are shown in row 3 of Table 1.
Incomparison to the better baseline for each dataset,we see that our approach performs substantiallybetter on BOO, DVD and ELE, at almost the samelevel on MOV and KIT, but slightly worse on POL.Note that the improvements observed for BOO,DVD and ELE can be attributed to the failure of e2to capture the sentiment dimension.
Perhaps mostimportantly, by exploiting human feedback, ourapproach has achieved more stable performanceacross the datasets than the baselines, with accura-cies ranging from 65.8% to 93.7% and ARI rang-ing from 0.10 to 0.76.Role of unambiguous documents.
Recall thatthe features with the largest MMFR were com-puted from the unambiguous documents only.
Toget an intuitive understanding of the role of unam-biguous documents in our approach, we show inTable 3 the accuracy when the unambiguous doc-uments in each dataset were clustered using theeigenvector selected by the majority of the judges.As we can see, the accuracy of each dataset ishigher than the corresponding accuracy shown inrow 3 of Table 1.
In fact, an accuracy of more than85% was achieved on all but one dataset.
This sug-gests that our method of identifying unambiguousdocuments is useful.Note that it is crucial to be able to achieve a highaccuracy on the unambiguous documents: if clus-tering accuracy is low, the features induced fromthe clusters may not be an accurate representationof the corresponding dimension, and the humanjudge may have a difficult time identifying the in-tended dimension.
In fact, some human judges re-ported difficulty in identifying the correct dimen-sion for the ELE dataset, and this can be attributedin part to the low accuracy achieved on the unam-biguous documents.Features as summary.
Recall that the methodwe proposed represents each dimension with asmall number of features and asks a user to se-lect the desired dimension by inspecting the corre-sponding feature lists.
In other words, each featurelist serves as a ?summary?
of its corresponding di-mension, and inspecting the features induced foreach dimension can give us insights into the dif-ferent dimensions of a dataset.
Hence, if a user isnot sure how she wants the data points to be clus-tered (due to lack of knowledge of the data, forinstance), our automatically induced features mayserve as an overview of the different dimensionsof the data.
To better understand whether thesefeatures can indeed provide a user with additionaluseful information about a dataset, we show in Ta-bles 4?6 the top ten features induced for each clus-ter and each dimension for the six datasets.
As anexample, consider the MOV dataset.
Inspectingthe induced features, we can determine that it hasa sentiment dimension (e2), as well as a humor vs.thriller dimension (e4).
In other words, if we clus-ter along e2, we get a sentiment-based clustering;and if we cluster along e4, we obtain a genre-based(humor vs. thriller) clustering.User feedback vs. labeled data.
Recall that ourtwo baselines are unsupervised, whereas our ap-proach can be characterized as semi-supervised, asit relies on user feedback to select the intended di-mension.
Hence, it should not be surprising to seethat the average clustering performance of our ap-proach is better than that of the baselines.To do a fairer comparison, we conduct anotherexperiment in which we compare our approachagainst a semi-supervised sentiment classificationsystem, which uses transductive SVM as the un-derlying semi-supervised learner.
More specifi-cally, the goal of this experiment is to determinehow many labeled documents are needed in or-der for the transductive learner to achieve the samelevel of performance as our approach.
To answerthis question, we first give the transductive learneraccess to the 2000 documents for each dataset as586POL MOVe2e3e4e5e2e3e4e5C1C1C1C1C1C1C1C1serder beyer serbs escrow relationship production jokes startsarmenian arabs palestinians serial son earth kids personturkey andi muslims algorithm tale sequences live sawarmenians research wrong chips husband aliens animation feelingmuslims israelis department ensure perfect war disney livessdpa tim bosnia care drama crew animated toldargic uci live strong focus alien laughs happendavidian ab matter police strong planet production amdbd@ura z@virginia freedom omissions beautiful horror voice felttroops holocaust politics excepted nature evil hilarious happenedC2C2C2C2C2C2C2C2sternlight escrow standard internet worst sex thriller comicwouldn sternlight sternlight uucp stupid romantic killer sequencespgp algorithm des uk waste school murder michaelcrypto access escrow net bunch relationship crime supportingalgorithm net employer quote wasn friends police careerisn des net ac video jokes car productionlikely privacy york co worse laughs dead peteraccess uk jake didn boring sexual killed styleidea systems code ai guess cute starts latestcryptograph pgp algorithm mit anyway mother violence entertainingTable 4: Top ten features induced for each dimension for the POL and MOV domains.
The shaded columnscorrespond to the dimensions selected by the human judges.
e2, .
.
., e5are the top eigenvectors; C1and C2are the clusters.BOO ELEe2e3e4e5e2e3e4e5C1C1C1C1C1C1C1C1history series loved must mouse music easy amazonmust man highly wonderful cable really used cablemodern history easy old cables ipod card cardimportant character enjoyed feel case too fine recommendtext death children away red little using dvdreference between again children monster headphones problems cameraexcellent war although year picture hard fine fastprovides seems excellent someone kit excellent drive farbusiness political understand man overall need computer printerboth american three made paid fit install pictureC2C2C2C2C2C2C2C2plot buy money boring working worked money phonedidn bought bad series never problem worth offthought information nothing history before never amazon workedboring easy waste pages phone item over powergot money buy information days amazon return batterycharacter recipes anything between headset working years unitcouldn pictures doesn highly money support much setll look already page months months headphones phonesending waste instead excellent return returned sony rangefan copy seems couldn second another received littleTable 5: Top ten features induced for each dimension for the BOO and ELE domains.
The shaded columnscorrespond to the dimensions selected by the human judges.
e2, .
.
., e5are the top eigenvectors; C1and C2are the clusters.unlabeled data.
Next, we randomly sample 50 un-labeled documents and assign them the true label.We then re-train the classifier and compute its ac-curacy on the 2000 documents.
We keep addingmore labeled data (50 in each iteration) until itreaches the accuracy achieved by our system.
Re-sults of this experiment are shown in Table 7.
Ow-ing in the randomness involved in the selection ofunlabeled documents, these results are averagedover ten independent runs.
As we can see, our587KIT DVDe2e3e4e5e2e3e4e5C1C1C1C1C1C1C1C1love works really pan worth music video moneyclean water nice oven bought collection music qualitynice clean works cooking series excellent found videosize work too made money wonderful feel worthset ice quality pans season must bought foundkitchen makes small better fan loved workout versioneasily thing sturdy heat collection perfect daughter picturesturdy need little cook music highly recommend wasterecommend keep think using tv makes our specialprice best item clean thought special disappointed soundC2C2C2C2C2C2C2C2months price ve love young worst series sawstill item years coffee between money cast watchedback set love too actors thought fan lovednever ordered never recommend men boring stars enjoyworked amazon clean makes cast nothing original wholemoney gift months over seems minutes comedy gotdid got over size job waste actors familyamazon quality pan little beautiful saw worth seriesreturn received been maker around pretty classic seasonmachine knives pans cup director reviews action likedTable 6: Top ten features induced for each dimension for the KIT and DVD domains.
The shaded columnscorrespond to the dimensions selected by the human judges.
e2, .
.
., e5are the top eigenvectors; C1and C2are the clusters.POL MOV KIT BOO DVD ELE# labels 400 150 200 350 350 200Table 7: Transductive SVM results.user feedback is equivalent to the effort of hand-annotating 275 documents per dataset on average.Multiple relevant dimensions.
As seen fromTable 2, some human judges selected more thanone dimension for some datasets (e.g., 2,3,4 forPOL; 2,4 for MOV; and 3,4 for ELE).
However,we never took into account these ?extra?
dimen-sions in our previous experiments.
To better un-derstand whether these extra dimensions can helpimprove accuracy and ARI, we conduct anotherexperiment in which we apply 2-means to clus-ter the documents in a space that is defined byall of the selected dimensions.
The final accu-racy turns out to be 95.9%, 70.9%, and 67.5% forPOL, MOV, and ELE respectively, which is con-siderably better than using only the optimal di-mension and suggests that the extra dimensionscontain useful information.5 ConclusionsUnsupervised clustering algorithms typicallygroup objects along the most prominent di-mension, in part owing to their objective ofsimultaneously maximizing inter-cluster similar-ity and intra-cluster dissimilarity.
Hence, if theuser?s intended clustering dimension is not themost prominent dimension, these unsupervisedclustering algorithms will fail miserably.
Toaddress this problem, we proposed to integrate anovel user feedback mechanism into a spectralclustering algorithm, which allows us to minethe intended, possibly hidden, dimension of thedata and produce the desired clustering.
Thismechanism differs from competing methods inthat it requires very limited feedback: to select theintended dimension, the user only needs to inspecta small number of features.
We demonstrated itsviability via a set of human and automatic experi-ments with unsupervised sentiment classification,obtaining promising results.In future work, we plan to explore several ex-tensions to our proposed method.
First, we plan touse our user-feedback method in combination withexisting methods (e.g., Bekkerman et al (2007))for improving its performance.
For instance, in-stead of having the user construct a relevant fea-ture space from scratch, she can simply extendthe set of informative features identified for theuser-selected dimension.
Second, since none ofthe steps in our method is specifically designedfor sentiment classification, we plan to apply it toother non-topic-based text classification tasks.588AcknowledgmentsWe thank the three anonymous reviewers for theirinvaluable comments on an earlier draft of the pa-per.
This work was supported in part by NSFGrant IIS-0812261.ReferencesMaria-Florina Balcan and Avrim Blum.
2008.
Clus-tering with interactive feedback.
In Proceedings ofALT, pages 316?328.Carmen Banea, Rada Mihalcea, Janyce Wiebe, andSamer Hassan.
2008.
Multilingual subjectivityanalysis using machine translation.
In Proceedingsof EMNLP, pages 127?135.Ron Bekkerman, Hema Raghavan, James Allan, andKoji Eguchi.
2007.
Interactive clustering of textcollections according to a user-specified criterion.In Proceedings of IJCAI, pages 684?689.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In Proceedings of the ACL, pages 440?447.Deng Cai, Xiaofei He, and Jiawei Han.
2005.
Doc-ument clustering using locality preserving indexing.IEEE Transactions on Knowledge and Data Engi-neering, 17(12):1624?1637.Inderjit Dhillon, Yuqiang Guan, and Brian Kulis.
2004.Kernel k-means, spectral clustering and normalizedcuts.
In Proceedings of KDD, pages 551?556.Mark Dredze and Koby Crammer.
2008.
Active learn-ing with confidence.
In Proceedings of ACL-08:HLTShort Papers (Companion Volume), pages 233?236.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Bernhard Scholkopf andAlexander Smola, editors, Advances in Kernel Meth-ods - Support Vector Learning, pages 44?56.
MITPress.Sepandar Kamvar, Dan Klein, and Chris Manning.2003.
Spectral learning.
In Proceedings of IJCAI,pages 561?566.Ravi Kannan, Santosh Vempala, and Adrian Vetta.2004.
On clusterings: Good, bad and spectral.
Jour-nal of the ACM, 51(3):497?515.Soo-Min Kim and Eduard Hovy.
2004.
Determiningthe sentiment of opinions.
In Proceedings of COL-ING, pages 1367?1373.Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip S. Yu.2004.
Text classification by labeling words.
In Pro-ceedings of AAAI, pages 425?430.Tony Mullen and Nigel Collier.
2004.
Sentimentanalysis using support vector machines with diverseinformation sources.
In Proceedings of EMNLP,pages 412?418.Andrew Ng, Michael Jordan, and Yair Weiss.
2002.On spectral clustering: Analysis and an algorithm.In Advances in NIPS 14.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification us-ing machine learning techniques.
In Proceedings ofEMNLP, pages 79?86.Hema Raghavan and James Allan.
2007.
An interac-tive algorithm for asking and incorporating featurefeedback into support vector machines.
In Proceed-ings of SIGIR, pages 79?86.Dan Roth and Kevin Small.
2009.
Interactive featurespace construction using semantic information.
InProceedings of CoNLL, pages 66?74.Jianbo Shi and Jitendra Malik.
2000.
Normalized cutsand image segmentation.
IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 22(8):888?905.Peter Turney.
2002.
Thumbs up or thumbs down?
Se-mantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the ACL, pages417?424.Kiri Wagstaff, Claire Cardie, Seth Rogers, and Ste-fan Schro?dl.
2001.
Constrained k-means cluster-ing with background knowledge.
In Proceedings ofICML, pages 577?584.Xiaojun Wan.
2008.
Using bilingual knowledge andensemble techniques for unsupervised Chinese sen-timent analysis.
In Proceedings of EMNLP, pages553?561.Yair Weiss.
1999.
Segmentation using eigenvectors: Aunifying view.
In Proceedings of ICCV, pages 975?982.Yiming Yang and Jan Pedersen.
1997.
A comparativestudy on feature selection in text categorization.
InProceedings of ICML, pages 412?420.589
