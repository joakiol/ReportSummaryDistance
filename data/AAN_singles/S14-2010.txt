Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81?91,Dublin, Ireland, August 23-24, 2014.SemEval-2014 Task 10: Multilingual Semantic Textual SimilarityEneko Agirrea, Carmen Baneab?, Claire Cardiec, Daniel Cerd, Mona Diabe?Aitor Gonzalez-Agirrea, Weiwei Guof, Rada Mihalceab, German Rigaua, Janyce WiebegaUniversity of the Basque CountryBasque Country, SpainbUniversity of MichiganAnn Arbor, MIcCornell UniversityIthaca, NYdGoogle Inc.Mountain View, CAeGeorge Washington UniversityWashington, DCfColumbia UniversityNew York, NYgUniversity of PittsburghPittsburgh, PAAbstractIn Semantic Textual Similarity, systemsrate the degree of semantic equivalencebetween two text snippets.
This year,the participants were challenged with newdata sets for English, as well as the in-troduction of Spanish, as a new languagein which to assess semantic similarity.For the English subtask, we exposed thesystems to a diversity of testing scenar-ios, by preparing additional OntoNotes-WordNet sense mappings and news head-lines, as well as introducing new gen-res, including image descriptions, DEFTdiscussion forums, DEFT newswire, andtweet-newswire headline mappings.
ForSpanish, since, to our knowledge, this isthe first time that official evaluations areconducted, we used well-formed text, byfeaturing sentences extracted from ency-clopedic content and newswire.
The an-notations for both tasks leveraged crowd-sourcing.
The Spanish subtask engaged 9teams participating with 22 system runs,and the English subtask attracted 15 teamswith 38 system runs.1 Introduction and motivationGiven two snippets of text, Semantic Textual Sim-ilarity (STS) captures the notion that some textsare more similar than others, measuring their de-gree of semantic equivalence.
Textual similar-ity can range from complete unrelatedness to ex-act semantic equivalence, and a graded similar-ity intuitively captures the notion of intermediate?carmennb@umich.edu, mtdiab@gwu.eduThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/shades of similarity, as pairs of text may differfrom some minor nuanced aspects of meaning, torelatively important semantic differences, to shar-ing only some details, or to simply being relatedto the same topic (cf.
Section 2).One of the goals of the STS task is to create aunified framework for combining several seman-tic components that otherwise have historicallytended to be evaluated independently and with-out characterization of impact on NLP applica-tions.
By providing such a framework, STS al-lows for an extrinsic evaluation of these modules.Moreover, such an STS framework itself could inturn be evaluated intrinsically and extrinsically asa grey/black box within various NLP applicationssuch as Machine Translation (MT), Summariza-tion, Generation, Question Answering (QA), etc.STS is related to both Textual Entailment (TE)and Paraphrasing, but differs in a number of waysand it is more directly applicable to a number ofNLP tasks.
STS is different from TE inasmuchas it assumes bidirectional graded equivalence be-tween the pair of textual snippets.
In the case ofTE the equivalence is directional, e.g.
a car is avehicle, but a vehicle is not necessarily a car.
STSalso differs from both TE and Paraphrasing (in asfar as both tasks have been defined to date in theliterature) in that, rather than being a binary yes/nodecision (e.g.
a vehicle is not a car), we defineSTS to be a graded similarity notion (e.g.
a ve-hicle and a car are more similar than a wave anda car).
A quantifiable graded bidirectional notionof textual similarity is useful for a myriad of NLPtasks such as MT evaluation, information extrac-tion, question answering, summarization, etc.In 2012 we held the first pilot task at SemEval2012, as part of the *SEM 2012 conference, withgreat success: 35 teams participated with 88 sys-tem runs (Agirre et al., 2012).
In addition, we held81year dataset pairs source2012 MSRpar 1500 newswire2012 MSRvid 1500 videos2012 OnWN 750 glosses2012 SMTnews 750 MT eval.2012 SMTeuroparl 750 MT eval.2013 HDL 750 newswire2013 FNWN 189 glosses2013 OnWN 561 glosses2013 SMT 750 MT eval.2014 HDL 750 newswire headlines2014 OnWN 750 glosses2014 Deft-forum 450 forum posts2014 Deft-news 300 news summary2014 Images 750 image descriptions2014 Tweet-news 750 tweet-news pairsTable 2: English subtask: Summary of train (2012and 2013) and test (2014) datasets.a DARPA sponsored workshop at Columbia Uni-versity.1In 2013, STS was selected as the offi-cial Shared Task of the *SEM 2013 conference,with two subtasks: The Core task, which is sim-ilar to the 2012 task; and a Pilot task on Typed-similarity between semi-structured records.
TheCore task attracted 34 participants with 89 runs,and the Typed-similarity task attracted 6 teamswith 14 runs.For STS 2014 we defined two subtasks: En-glish and Spanish.
For the English subtask we pro-vided five test datasets: two datasets that extendalready released genres (the OntoNotes-WordNetsense mappings and news headlines) and threenew genres: image descriptions, DEFT discus-sion forum data and newswire, as well as tweet-newswire headline mappings.
Participants coulduse all datasets released in 2012 and 2013 as train-ing data.
The Spanish subtask introduced two di-verse datasets on different genres, namely ency-clopedic descriptions extracted from the SpanishWikipedia and contemporary Spanish newswire.For the Spanish subtask, the participants had ac-cess to a limited amount of labeled data, consist-ing of 65 sentence pairs, which they could use fortraining.2 Task Description2.1 English SubtaskThe English dataset comprises pairs of news head-lines (HDL), pairs of glosses (OnWN), image de-scriptions (Images), DEFT-related discussion fo-rums (Deft-forum) and news (Deft-news), and1http://www.cs.columbia.edu/?weiwei/workshop/tweet comments and newswire headline mappings(Tweets).For HDL, we used naturally occurring newsheadlines gathered by the Europe Media Moni-tor (EMM) engine (Best et al., 2005) from sev-eral different news sources.
EMM clusters to-gether related news.
Our goal was to generatea balanced data set across the different similar-ity ranges, hence we built two sets of headlinepairs: (i) a set where the pairs come from the sameEMM cluster, (ii) and another set where the head-lines come from a different EMM cluster, thenwe computed the string similarity between thosepairs.
Accordingly, we sampled 375 headline pairsof headlines that occur in the same EMM cluster,aiming for pairs equally distributed between min-imal and maximal similarity using simple stringsimilarity.
We sampled other 375 pairs from thedifferent EMM cluster in the same manner.For OnWN, we used the sense definition pairsof OntoNotes (Hovy et al., 2006) and WordNet(Fellbaum, 1998).
Different from previous tasks,the two definition sentences in a pair belong to dif-ferent senses.
We sampled 750 pairs based on astring similarity ranging from 0.5 to 1.The Images data set is a subset of the PAS-CAL VOC-2008 data set (Rashtchian et al., 2010),which consists of 1,000 images and has been usedby a number of image description systems.
It wasalso sampled from string similarity values between0.6 and 1.Deft-forum and Deft-news are from DEFTdata.2Deft-forum contains the forum post sen-tences, and Deft-news are news summaries.
Weselected 450 pairs for Deft-forum and 300 pairs forDeft-news.
They are sampled evenly from stringsimilarities falling in the interval 0.6 to 1.The Tweets data set contains tweet-news pairsselected from the corpus released in (Guo et al.,2013), where each pair contains a sentence thatpertains to the news title, while the other one rep-resents a Twitter comment on that particular news.They are evenly sampled from string similarityvalues between 0.5 and 1.Table 1 shows the explanations and values as-sociated with each score between 5 and 0.
Asin prior years, we used Amazon Mechanical Turk(AMT)3to crowdsource the annotation of the En-glish pairs.4Annotators are presented with the2LDC2013E19, LDC2012E543www.mturk.com4For STS 2013, we used CrowdFlower as a front-end to82Score English Spanish5/4 The two sentences are completely equivalent, as they mean the same thing.The bird is bathing in the sink.Birdie is washing itself in the water basin.El p?ajaro se esta ba?nando en el lavabo.El p?ajaro se est?a lavando en el aguamanil.4 The two sentences are mostly equivalent, but some unimportant details differ.In May 2010, the troops attempted to invadeKabul.The US army invaded Kabul on May 7th lastyear, 2010.3 The two sentences are roughly equivalent, but some important information differs/missing.John said he is considered a witness but not asuspect.
?He is not a suspect anymore.?
John said.John dijo que ?el es considerado como testigo, yno como sospechoso.?
?El ya no es un sospechoso,?
John dijo.2 The two sentences are not equivalent, but share some details.They flew out of the nest in groups.They flew into the nest together.Ellos volaron del nido en grupos.Volaron hacia el nido juntos.1 The two sentences are not equivalent, but are on the same topic.The woman is playing the violin.The young lady enjoys listening to the guitar.La mujer est?a tocando el viol?
?n.La joven disfruta escuchar la guitarra.0 The two sentences are completely dissimilar.John went horse back riding at dawn with awhole group of friends.Sunrise at dawn is a magnificent view to takein if you wake up early enough for it.Al amanecer, Juan se fue a montar a caballocon un grupo de amigos.La salida del sol al amanecer es una magn?
?ficavista que puede presenciar si usted se despiertalo suficientemente temprano para verla.Table 1: Similarity scores with explanations and examples for the English and Spanish subtasks, wherethe sentences in Spanish are translations of the English ones.A similarity score of 5 in English is mirrored by a maximum score of 4 in Spanish; the definitions pertaining to scores 3 and 4in English were collapsed under a score of 3 in Spanish, with the definition ?The two sentences are mostly equivalent, but somedetails differ.
?detailed instructions provided in Figure 1, andare asked to label each STS sentence pair on oursix point scale, selecting from a dropdown box.Five sentence pairs are presented to each annota-tor at once, per human intelligence task (HIT), ata payrate of $0.20; we collect five separate anno-tations per sentence pair.
Annotators were only el-igible to work on the task if they had the Mechan-ical Turk Master Qualification.
This is a specialAmazon Mechanical Turk, since it provides numerous usefultools to assist in running a successful annotation project usingcrowdsourcing, such as support for hidden ?golden?
questionsthat can be used both to train annotators and to automaticallystop people who repeatedly make mistakes from contribut-ing to the task.
However, in 2013, CrowdFlower droppedAmazon Mechanical Turk as an annotation source.
When wetried running pairs for STS 2014 on CrowdFlower using thesame templates that were successfully used for the 2013 task,we found that we obtained significantly degraded annotationquality, with an average Pearson (AMT provider vs. rest ofAMT providers) of only 22.8%.
In contrast, when we ran thetask for 2014 on AMT, we obtained a one-vs-rest annotationof 73.6%.qualification conferred by AMT (using a prioritystatistical model) to annotators who consistentlymaintain a very high level of quality across a vari-ety of tasks from numerous requesters).
Access tothese skilled workers entails a 20% surcharge.To monitor the quality of the annotations, weuse the gold dataset of 105 pairs that were manu-ally annotated by the task organizers during STS2013.
We include one of these gold pairs in eachset of five sentence pairs, where the gold pairs areindistinguishable from the rest.
Unlike when weran on CrowdFlower for STS 2013, the gold pairsare not used for training purposes, nor are workersautomatically banned from the task if they maketoo many mistakes on annotating them.
Rather, thegold pairs are only used to help in identifying andremoving the data associated with poorly perform-ing annotators.
With few exceptions, 90% of theanswers from each individual annotator fall within+/-1 of the answers selected by the organizers for83Figure 1: Annotation instructions for English subtask.the gold dataset.The distribution of scores obtained from theAMT providers in the Deft-forum, Deft-news,OnWN and tweet-news datasets is roughly uni-form across the different grades of similarity, al-though the scores are slightly higher for tweet-news.
Compared to the other data sets, the scoresfor OnWN, were more bimodal, ranging between4.6 to 5 and 0 to 0.4, when compared to middlevalues (2.6-3.4).In order to assess the annotation quality, wemeasure the correlation of each annotator with theaverage of the rest of the annotators, and then aver-age the results.
This approach to estimate the qual-ity is identical to the method used for evaluations(see Section 3), and it can thus be considered asthe upper bound of the systems.
The inter-taggercorrelation for each English dataset is as follows:?
HDL: 79.4%?
OnWN: 67.2%?
Deft-forum: 58.6%?
Deft-news: 70.7%?
Images: 83.6%?
Tweets-news: 74.4%The correlation figures are generally high (over70%), with the exception of the OnWN and Deftdatasets, which score 67.2% and 58.6%, respec-tively.
The reason for the low inter-tagger correla-tion on OnWN compared to the higher correlationsin previous years is that we only used unmappedsense definitions, i.e., the two sentences in a pairbelong to two different senses.
For the Deft-forumdataset, we found that similarity values tend to belower than in the other datasets, and more annota-tion disagreements happen in these low similarityvalues.2.2 Spanish SubtaskThe Spanish subtask follows a setup similar to theEnglish subtask, except that the similarity scoreswere adapted to fit a range from 0 to 4 (see Table1).
We thought that the distinction between a scoreof 3 and 4 for the English task will pose more dif-ficulty for us in conveying into Spanish, as the soledifference between the two lies in how the annota-tors perceive the importance of additional detailsor missing information with respect to the core se-mantic interpretation of the pair.
As this aspect en-tails a subjective judgement, and since it is the firsttime that a Spanish STS evaluation is organized,we casted the annotation guidelines into straight-forward and unambiguous instructions, and thusopted to use a similarity range from 0 to 4.Prior to the evaluation window, we released 65Spanish sentence pairs for trial / training.
In or-der to evaluate system performance under differ-84ent scenarios, we developed two test datasets, oneextracted from the Spanish Wikipedia5(December2013 dump) and one from contemporary news ar-ticles collected from media in Spanish (February2014).2.2.1 Spanish WikipediaThe Wikipedia dump was processed using theParse::MediaWikiDump Perl library.
We removedall titles, html tags, wiki tags and hyperlinks(keeping only the surface forms).
Each article wassplit into paragraphs, where the first paragraphwas considered to be the article?s abstract, whilethe remaining ones were deemed to be its content.Each of these were split into sentences using thePerl library Uplug::PreProcess::SentDetect, andonly the sentences longer than eight words wereused.
We iteratively computed the lexical simi-larity6between every sentence in the abstract andevery sentence in the content, and retained thosepairs whose sentence length ratio was higher than0.5, and their similarity scored over 0.35.The final set of sentence pairs was split into fivebins, and their scores normalized to range from0 to 1.
The more interesting and difficult pairswere found, perhaps not surprisingly, in bins 0 and1, where synonyms/short paraphrases where morefrequent.
An example extracted from those bins,where the text in italics highlights the differencesbetween the two sentences:?
?America?
es el segundo continente m?asgrande del planeta, despu?es de Asia.?America?
is the second largest continent in the world,following Asia.?
America corresponde a la segunda masa detierra m?as grande del planeta, luego de Asia.America is the second largest land mass on the planet,after Asia.The Spanish verb ?Es?
maps to (En:7is), ?cor-responde a?
(En: corresponds to), the phrase ?elsegundo continente?
(En: the second continent) isequivalent to ?la segunda masa de tierra?
(the sec-ond land mass), and ?despues?
(En: following) to?luego?
(En: after).
Despite the difference in vo-cabulary choice, the two sentences are paraphrasesof each other.From the candidate pairs, we manually selected324 sentence pairs, in order to ensure a diverse5es.wikipedia.org6Algorithm based on the Linux diff command (Algo-rithm::Diff Perl module).7?En?
stands for English.and challenging set.
This set was annotated in twoways, first by two graduate students in ComputerScience who are native speakers of Spanish, andsecond by using AMT.The AMT framework was set up to containseven sentence pairs per HIT, where six of themwere part of the test dataset, while one was usedfor control.
AMT providers were eligible to com-plete a task if they had more than 500 acceptedHITs, with 90%+ acceptance rate.8We paid $0.30per HIT, and each HIT was annotated by five AMTproviders.
We sought to ensure that only Spanishspeaking annotators would complete the HITs byproviding all the information related to the task (itstitle, abstract, description, guidelines and exam-ples), as well as the control pair in Spanish only.The participants were instructed to label the pairson a scale from 0 to 4 (see Table 1).
Each sentencepair was followed by a comment text box, whichthe AMT providers used to provide the topic of thesentences, corrections, etc.The two students achieved a Pearson correla-tion of 0.6974 on the Wikipedia dataset.
To seehow their judgement compares to the crowd wis-dom, we averaged the AMT scores for each pair,and computed their correlation with our annota-tors, obtaining 0.824 and 0.742, respectively.
Sur-prisingly enough, both these correlation values arehigher than the correlation among the annotatorsthemselves.
When averaging the annotator scoresand comparing them with the AMT providers?average score per pair, the correlation becomes0.8546, indicating that the task is well defined,and that the annotations contributed by the AMTproviders are of satisfactory quality.
Given thesescores, the gold standard was annotated using theaverage AMT provider judgement per pair.2.2.2 Spanish NewsThe second Spanish dataset was extracted fromnews articles published in Spanish language me-dia from around the world in February 2014.
Thehyperlinks to the articles were obtained by pars-ing the ?International?
page of Spanish GoogleNews,9which aggregates or clusters in real timearticles describing a particular event from a di-verse pool of news sites, where each grouping8Initially, Amazon had automatically upgraded our anno-tation task to require Master level providers (as those partici-pating in the English annotations), yet after approximately 4days, no HIT had been completed.9news.google.es85is labeled with the title of one of the predomi-nant articles.
By leveraging these clusters of linkspointing to the sites where the articles were orig-inally published, we are able to gather raw textthat has a high probability of containing seman-tically similar sentences.
We encountered severaldifficulties while mining the articles, ranging fromeach article having its own formatting depend-ing on the source site, to advertisements, cookierequirements, to encoding for Spanish diacritics.We used the lynx text-based browser,10which wasable to standardize the raw articles to a degree.The output of the browser was processed using arule based approach taking into account continu-ous text span length, ratio of symbols and num-bers to the text, etc., in order to determine whena paragraph is part of the article content.
Afterthat, a second pass over the predictions correctedmislabeled paragraphs if they were preceded andfollowed by paragraphs identified as content.
Allthe content pertaining to articles on the same eventwas joined, sentence split, and diff pairwise simi-larities were computed.
The set of candidate sen-tences followed the same requirements as for theWikipedia dataset, namely length ratio higher than0.5 and similarity score over 0.35.
From these, wemanually extracted 480 sentence pairs which weredeemed to pose a challenge to an automated sys-tem.Due to the high correlations obtained betweenthe AMT providers?
scores and the annotators?scores on Wikipedia, the news dataset was onlyannotated using AMT, following exactly the sametask setup as for Wikipedia.3 EvaluationEvaluation of STS is still an open issue.STS experiments have traditionally used Pearsonproduct-moment correlation between the systemscores and the GS scores, or, alternatively, Spear-man rank order correlation.
In addition, we alsoneed a method to aggregate the results from eachdataset into an overall score.
The analysis per-formed in (Agirre and Amig?o, In prep) shows thatPearson and averaging across datasets are the bestsuited combination in general.
In particular, Pear-son is more informative than Spearman, in thatSpearman only takes the rank differences into ac-count, while Pearson does account for value dif-ferences as well.
The study also showed that other10lynx.browser.orgalternatives need to be considered, depending onthe requirements of the target application.We leave application-dependent evaluations forfuture work, and focus on average Pearson correla-tion.
When averaging, we weight each individualcorrelation by the size of the dataset.
In order tocompute statistical significance among system re-sults, we use a one-tailed parametric test based onFisher?s z-transformation (Press et al., 2002, equa-tion 14.5.10).
In addition, English subtask partic-ipants could provide an optional confidence mea-sure between 0 and 100 for each of their predic-tions.
Team RTM-DCU is the only one who hasprovided these, and the evaluation of their runs us-ing weighted Pearson (Pozzi et al., 2012) is listedat the end of Table 3.Participants11could take part in the shared taskwith a maximum of 3 system runs per subtask.3.1 English SubtaskIn order to provide a simple word overlap baseline(Baseline-tokencos), we tokenize the input sen-tences splitting on white spaces, and then repre-sent each sentence as a vector in the multidimen-sional token space.
Each dimension has 1 if the to-ken is present in the sentence, 0 otherwise.
Vectorsimilarity is computed using the cosine similaritymetric.We also run the freely available system, Take-Lab (?Sari?c et al., 2012), which yielded state of theart performance in STS 2012 and strong resultsout-of-the-box in 2013.1215 teams participated in the English subtask,submitting 38 system runs.
One team submittedthe results past the deadline, as explicitly markedin Table 3.
After the submission deadline expired,the organizers published the gold standard and par-ticipant submissions on the task website, in orderto ensure a transparent evaluation process.Table 3 shows the results of the English sub-task, with runs listed in alphabetical order.
Thecorrelation in each dataset is given, followed11Participating teams: Bielefeld SC (McCrae et al.,2013), BUAP (Vilari?no et al., 2014), DLS@CU (Sultan etal., 2014b), FBK-TR (Vo et al., 2014), IBM EG (no in-formation), LIPN (Buscaldi et al., 2014), Meerkat Mafia(Kashyap et al., 2014), NTNU (Lynum et al., 2014), RTM-DCU (Bic?ici and Way, 2014), SemantiKLUE (Proisi et al.,2014), StanfordNLP (Socher et al., 2014), TeamZ (Gupta,2014), UMCC DLSI SemSim (Chavez et al., 2014), UNAL-NLP (Jimenez et al., 2014), UNED (Martinez-Romo et al.,2011), UoW (Rios, 2014).12Code is available at http://ixa2.si.ehu.es/stswiki86Run Name deft deft Headl images OnWN tweet Weighted mean Rankforum news newsBaseline-tokencos 0.353 0.596 0.510 0.513 0.406 0.654 0.507 -TakeLab 0.333 0.716 0.720 0.742 0.793 0.650 0.678 -Bielefeld SC-run1 0.211 0.432 0.321 0.368 0.367 0.415 0.354 32Bielefeld SC-run2 0.211 0.431 0.311 0.356 0.361 0.409 0.347 33BUAP-EN-run1 0.456 0.686 0.689 0.697 0.654 0.771 0.671 19DLS@CU-run1 0.483 0.766 0.765 0.821 0.723 0.764 0.734 7DLS@CU-run2 0.483 0.766 0.765 0.821 0.859 0.764 0.761 1FBK-TR-run1 0.322 0.523 0.547 0.601 0.661 0.462 0.535 25FBK-TR-run2 0.167 0.421 0.485 0.521 0.572 0.359 0.441 28FBK-TR-run3 0.305 0.405 0.471 0.489 0.551 0.438 0.459 27IBM EG-run1 0.474 0.743 0.737 0.801 0.760 0.730 0.722 8IBM EG-run2 0.464 0.641 0.710 0.747 0.732 0.696 0.684 15LIPN-run1 0.454 0.640 0.653 0.809 - 0.551 0.508 26LIPN-run2 0.084 - - - - - 0.010 35Meerkat Mafia-Hulk 0.449 0.785 0.757 0.790 0.787 0.757 0.735 6Meerkat Mafia-pairingWords 0.471 0.763 0.760 0.801 0.875 0.779 0.761 2Meerkat Mafia-SuperSaiyan 0.492 0.771 0.767 0.768 0.802 0.765 0.741 5NTNU-run1 0.437 0.714 0.722 0.800 0.835 0.411 0.663 20NTNU-run2 0.508 0.766 0.753 0.813 0.777 0.792 0.749 4NTNU-run3 0.531 0.781 0.784 0.834 0.850 0.675 0.755 3SemantiKLUE-run1 0.337 0.608 0.728 0.783 0.848 0.632 0.687 14SemantiKLUE-run2 0.349 0.643 0.733 0.773 0.855 0.640 0.694 13StanfordNLP-run1 0.319 0.635 0.636 0.758 0.627 0.669 0.627 22StanfordNLP-run2 0.304 0.679 0.621 0.715 0.625 0.636 0.610 24StanfordNLP-run3 0.342 0.650 0.602 0.754 0.609 0.638 0.614 23UMCC DLSI SemSim-run1 0.475 0.662 0.632 0.742 0.813 0.675 0.682 16UMCC DLSI SemSim-run2 0.469 0.662 0.625 0.739 0.814 0.654 0.676 18UMCC DLSI SemSim-run3 0.283 0.385 0.267 0.436 0.603 0.278 0.381 30UNAL-NLP-run1 0.504 0.721 0.762 0.807 0.782 0.614 0.711 12UNAL-NLP-run2 0.383 0.730 0.765 0.771 0.827 0.403 0.657 21UNAL-NLP-run3 0.461 0.722 0.761 0.778 0.843 0.658 0.721 9UNED-run22 p np 0.104 0.315 0.037 0.324 0.509 0.490 0.310 34UNED-runS5K 10 np 0.118 0.506 0.057 0.498 0.488 0.579 0.379 31UNED-runS5K 3 np 0.094 0.564 0.018 0.607 0.577 0.670 0.431 29UoW-run1 0.342 0.751 0.754 0.776 0.799 0.737 0.714 11UoW-run2 0.342 0.587 0.754 0.788 0.799 0.628 0.682 17UoW-run3 0.342 0.763 0.754 0.788 0.799 0.753 0.721 10?RTM-DCU-run1 0.434 0.697 0.620 0.699 0.806 0.688 0.671?RTM-DCU-run2 0.397 0.681 0.613 0.666 0.799 0.669 0.651?RTM-DCU-run3 0.308 0.556 0.630 0.647 0.800 0.553 0.608?RTM-DCU-run1 0.418 0.685 0.622 0.698 0.833 0.687 0.673?RTM-DCU-run2 0.383 0.674 0.609 0.663 0.826 0.669 0.653?RTM-DCU-run3 0.273 0.553 0.633 0.644 0.825 0.568 0.611Table 3: English evaluation results.
Results at the top correspond to out-of-the-box systems.
Results atthe bottom correspond to results using the confidence score.Notes: ?-?
for not submitted, ???
for post-deadline submission.87by the mean correlation (the official measure),and the rank of the run.
The highest correla-tions are for OnWN (87.5%, by Meerkat Mafia)and images (83.4%, by NTNU), followed byTweets (79.2%, by NTNU), HEADL (78.4%, byNTNU) and deft news and forums (78.1% and53.1%, respectively, by NTNU).
Compared to theinter-annotator agreement correlation, the rankingamong datasets is very similar, with the exceptionof OnWN, as it gets the best score but has very lowagreement.
One possible reason is that the partic-ipants used previously available data.
The resultsof the best 4 top system runs are significantly dif-ferent (p-value < 0.05) from the 5th top scoringsystem run and below.
The top 4 systems did notshow statistical significant variation among them.Only three runs (cf.
lower rows in Table 3) in-cluded non-uniform confidence scores, barely af-fecting their ranking.Interestingly, the two top performing systemson the English STS sub-task are both unsuper-vised.
DLS@CU (Sultan et al., 2014b) presentsan unsupervised algorithm which predicts the STSscore based on the proportion of word alignmentsin the two sentences.
Two related words arealigned depending on how similar the two wordsare, and also on how similar the contexts of thewords are in the respective sentences (Sultan et al.,2014a).
Meerkat Mafia pairingWords (Kashyapet al., 2014) also follows a fully unsupervised ap-proach.
The authors train LSA on an English cor-pus of three billion words using a sliding windowapproach, resulting in a vocabulary size of 29,000words associated with 300 dimensions.
They ac-count for named entities and out-of-vocabularywords by leveraging external resources such asDBpedia13and Wordnik.14In Spanish, the sys-tem equivalent to this run ranked second followinga cross-lingual approach, by applying the Englishsystem to the translated version of the dataset (see3.2).The Table also shows the results of TakeLab,which was trained with all datasets from previ-ous years.
TakeLab would rank 18th, ten absolutepoints below the best system, a smaller differencethan in 2013.13dbpedia.org14wordnick.com3.2 Spanish SubtaskThe Spanish subtask attracted 9 teams with 22participating systems, out of which 16 were su-pervised and 6 unsupervised.
The participantswere from both Spanish (Colombia, Cuba, Mex-ico, Spain), and non-Spanish speaking countries(two teams from France, Germany, Ireland, UK,US).
The evaluation results appear in Table 4.The top ranking system is the 2nd run ofUMCC DLSI SemSim (Chavez et al., 2014),which achieves a weighted correlation of 0.807.
Itentails a cross-lingual approach, as it leverages aSVM-based English framework, by mapping theSpanish words to their English equivalent usingthe most common sense in WordNet 3.0.
The clas-sifier uses a combination of features, such as thosederived from traditional knowledge-based ((Lea-cock and Chodorow, 1998; Wu and Palmer, 1994;Lin, 1998), and others) and corpus-based metrics(LSA (Landauer et al., 1997)), paired with lexi-cal features (such as Dice-Similarity, Euclidean-distance, etc.).
It is trained on a cumulative En-glish STS dataset comprising train and test datareleased as part of tasks in SemEval2012 (Agirreet al., 2012) and *Sem 2013 (Agirre et al., 2013),as well as training data available from tasks 1 and10 in SemEval 2014.
Interestingly enough, run 2of the system performs better than run 1, despitethe fact that it uses half the features, and focuseson string based similarity measures only.
This dif-ference between runs is noticed on the Wikipediadataset only, and it amounts to 4% Pearson corre-lation.
While the system had a robust performanceon the Spanish subtask, for English, its overallrank was 16, 18, and 33, respectively.Coming in close at only 0.3% difference, isMeerkat-Mafia PairingAvg (run 2) (Kashyap etal., 2014), which also follows a cross-lingual ap-proach, by applying the system the team devel-oped for the English subtask to the translated ver-sion of the datasets (see 3.1).
The interesting as-pect of their work is that in their first submission(run 1), they only consider the similarity result-ing from the sentence pair translation through theGoogle Translate service.15In the second run,they expand each sentence to 20 possible combi-nations by accounting for the multiple translationmeanings of a given word, and considering the av-erage similarity of all resulting pairs.
While thefirst run achieves a weighted correlation of 73.8%,15translate.google.com88Run Name System type Wikipedia News Weighted mean RankBielefeld-SC-run1 unsupervised* 0.263 0.554 0.437 22Bielefeld-SC-run2 unsupervised* 0.265 0.555 0.438 21BUAP-run1 supervised 0.550 0.679 0.627 17BUAP-run2 unsupervised 0.640 0.764 0.714 14RTM-DCU-run1 supervised 0.422 0.700 0.588 18RTM-DCU-run2 supervised 0.369 0.625 0.522 20RTM-DCU-run3 supervised 0.424 0.641 0.554 19LIPN-run1 supervised 0.652 0.826 0.756 11LIPN-run2 supervised 0.716 0.832 0.785 6LIPN-run3 supervised 0.716 0.809 0.771 10Meerkat-Mafia-run1 unsupervised 0.668 0.785 0.738 13Meerkat-Mafia-run2 unsupervised 0.743 0.845 0.804 2Meerkat-Mafia-run3 supervised 0.738 0.822 0.788 5TeamZ-run1 supervised 0.610 0.717 0.674 15TeamZ-run2 supervised 0.604 0.710 0.667 16UMCC-DLSI-run1 supervised 0.741 0.825 0.791 4UMCC-DLSI-run2 supervised 0.7802 0.825 0.807 1UNAL-NLP-run1 weakly supervised 0.7803 0.815 0.801 3UNAL-NLP-run2 supervised 0.757 0.783 0.772 9UNAL-NLP-run3 supervised 0.689 0.796 0.753 12UoW-run1 supervised 0.748 0.800 0.779 7UoW-run2 supervised 0.748 0.800 0.779 8Table 4: Spanish evaluation results in terms of Pearson correlation.the second one performs significantly better at80.4%, indicating that the additional context mayalso include multiple instances of accurate trans-lations, hence significantly impacting the overallsimilarity score.
In English, the system equiva-lent to run 2 in Spanish, namely Meerkat Mafia-pairingWords, achieves a competitive ranked per-formance across all six datasets, ranking second,at an order of 10?4distance from the top sys-tem.
This supports the claim that, despite its unsu-pervised nature, the system is quite versatile andhighly competitive with the top performing super-vised frameworks, and that it may achieve an evenhigher performance in Spanish if accurate sen-tence translations were provided.Overall, most systems were cross-lingual, rely-ing on different translation approaches, such as 1)translating the test data into English (as the twosystems above), and then exporting the score ob-tained for the English sentences back to Spanish,or 2) performing automatic translation of the En-glish training data, and learning a classifier di-rectly in Spanish.
(Buscaldi et al., 2014) supple-mented their training dataset with human annota-tions conducted in Spanish, using definition pairsextracted from a Spanish dictionary.
A differentangle was explored by (Rios, 2014), who proposeda multilingual framework using transfer learningacross English and Spanish by training on tradi-tional lexical, knowledge-based and corpus-basedfeatures.
The semantic similarity task was ap-proached from a monolingual perspective as well(Gupta, 2014), by focusing on Spanish resources,such as the trial data we released as part of thesubtask, and the Spanish WordNet;16these wereleveraged using meta-learning over variations ofoverlap-based metrics.
Following the same line,(Bic?ici and Way, 2014) pursued language inde-pendent methods, who avoided relying on task ordomain specific information through the usage ofreferential translation machines.
This approachmodels textual semantic similarity as a decision interms of translation quality between two datasets(in our case Spanish STS trial and test data) givenrelevant examples from an in-language referencecorpus.In comparison to the correlations obtained in theEnglish subtask, where the highest weighted meanwas 76.1%, for Spanish, we obtained 80.7%, prob-ably due to the more formal nature of the datasets,since Wikipedia and news articles employ mostlywell formed and grammatically correct sentences,and we selected all snippets to be longer than 8words.
The overall correlation scores obtained forEnglish were hurt by the deft-forum data, whichscored significantly lower (at a maximum corre-lation of 50.8%), when compared to all the otherdatasets whose correlation was higher than 70%.The OnWN data was most similar to our test sets,and it attained a maximum of 85.9%.16grial.uab.es/descarregues.php894 ConclusionThis year?s STS task comprised a multilingualflair, by introducing Spanish datasets alongside theEnglish ones.
In English, the datasets sought to ex-pose the participating teams to more diverse sce-narios compared to the previous years, by intro-ducing image descriptions, forum and newswiregenre, and tweet-newswire headline mappings.For Spanish, two datasets were developed consist-ing of encyclopedic and newswire text acquiredfrom Spanish sources.
Overall, the English sub-task attracted 15 teams (with 38 system varia-tions), while the Spanish subtask had 9 teams(with 22 system runs).
Most teams from the Span-ish subtask have also submitted runs for the En-glish evaluations.AcknowledgmentsThe authors are grateful to Ver?onica P?erez-Rosasand Vanessa Loza for their help with the anno-tations for the Spanish subtask.
This material isbased in part upon work supported by NationalScience Foundation CAREER award #1361274and IIS award #1018613, by DARPA-BAA-12-47 DEFT grant #12475008, and by MINECOCHIST-ERA READERS and SKATER projects(PCIN-2013-002-C02-01, TIN2012-38584-C06-02).
Aitor Gonzalez Agirre is supported by a doc-toral grant from MINECO.
Any opinions, find-ings, and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of the NationalScience Foundation or the Defense Advanced Re-search Projects Agency.ReferencesEneko Agirre and Enrique Amig?o.
In prep.
Exploringevaluation measures for semantic textual similarity.In Unpublished manuscript.Eneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In *SEM 2012:The First Joint Conference on Lexical and Compu-tational Semantics ?
Volume 1: Proceedings of themain conference and the shared task, and Volume 2:Proceedings of the Sixth International Workshop onSemantic Evaluation (SemEval 2012), pages 385?393, Montr?eal, Canada, 7-8 June.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 SharedTask: Semantic textual similarity, including a pi-lot on typed-similarity.
In The Second Joint Con-ference on Lexical and Computational Semantics(*SEM 2013), pages 32?43.Clive Best, Erik van der Goot, Ken Blackler, TefiloGarcia, and David Horby.
2005.
Europe me-dia monitor - system description.
In EUR Report22173-En, Ispra, Italy.Ergun Bic?ici and Andy Way.
2014.
RTM-DCU: Ref-erential translation machines for semantic similarity.In Proceedings of the 8th International Workshop onSemantic Evaluation (SemEval-2014), Dublin, Ire-land.Davide Buscaldi, Jorge J. Garcia Flores, Joseph LeRoux, Nadi Tomeh, and Belem Priego Sanchez.2014.
LIPN: Introducing a new geographical con-text similarity measure and a statistical similaritymeasure based on the Bhattacharyya coefficient.
InProceedings of the 8th International Workshop onSemantic Evaluation (SemEval-2014), Dublin, Ire-land.Alexander Chavez, Hector Davila, Yoan Gutierrez,Antonio Fernandez-Orquin, Andr?es Montoyo, andRafael Munoz.
2014.
UMCC DLSI SemSim: Mul-tilingual system for measuring semantic textual sim-ilarity.
In Proceedings of the 8th InternationalWorkshop on Semantic Evaluation (SemEval-2014),Dublin, Ireland.Christiane Fellbaum.
1998.
WordNet - An electroniclexical database.
MIT Press.Weiwei Guo, Hao Li, Heng Ji, and Mona Diab.
2013.Linking tweets to news: A framework to enrich on-line short text data in social media.
In Proceedingsof the 51th Annual Meeting of the Association forComputational Linguistics, pages 239?249.Anubhav Gupta.
2014.
TeamZ: Measuring semantictextual similarity for Spanish using an overlap-basedapproach.
In Proceedings of the 8th InternationalWorkshop on Semantic Evaluation (SemEval-2014),Dublin, Ireland.Eduard Hovy, Mitchell Marcus, Martha Palmer,Lance Ramshaw, and Ralph Weischedel.
2006.OntoNotes: The 90% solution.
In Proceedings ofthe Human Language Technology Conference of theNorth American Chapter of the ACL, pages 57?60.Sergio Jimenez, George Due?nas, Julia Baquero, andAlexander Gelbukh.
2014.
UNAL-NLP: Combin-ing soft cardinality features for semantic textual sim-ilarity, relatedness and entailment.
In Proceedingsof the 8th International Workshop on Semantic Eval-uation (SemEval-2014), Dublin, Ireland.Abhay Kashyap, Lushan Han, Roberto Yus, JenniferSleeman, Taneeya Satyapanich, Sunil Gandhi, andTim Finin.
2014.
Meerkat Mafia: Multilingual andcross-level semantic textual similarity systems.
InProceedings of the 8th International Workshop on90Semantic Evaluation (SemEval-2014), Dublin, Ire-land.Thomas K. Landauer, Darrell Laham, Bob Rehder, andM.
E. Schreiner.
1997.
How well can passage mean-ing be derived without using word order?
A compar-ison of latent semantic analysis and humans.
Cogni-tive Science.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context and WordNet similarity forword sense identification.
In WordNet: An Elec-tronic Lexical Database, pages 305?332.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the Fifteenth In-ternational Conference on Machine Learning, pages296?304, Madison, Wisconsin.Andr?e Lynum, Partha Pakray, Bj?orn Gamb?ack, andSergio Jimenez.
2014.
NTNU: Measuring se-mantic similarity with sublexical feature represen-tations and soft cardinality.
In Proceedings of the8th International Workshop on Semantic Evaluation(SemEval-2014), Dublin, Ireland.Juan Martinez-Romo, Lourdes Araujo, Javier Borge-Holthoefer, Alex Arenas, Jos?e A. Capit?an, andJos?e A. Cuesta.
2011.
Disentangling categori-cal relationships through a graph of co-occurrences.Phys.
Rev.
E, 84:046108, Oct.John P. McCrae, Philipp Cimiano, and Roman Klinger.2013.
Orthonormal explicit topic analysis for cross-lingual document matching.
In Proceedings of the2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1732?1740, Seattle,Washington, USA.Francesco Pozzi, Tiziana Di Matteo, and Tomaso Aste.2012.
Exponential smoothing weighted correla-tions.
The European Physical Journal B, 85(6).William H. Press, Saul A. Teukolsky, William T. Vet-terling, and Brian P. Flannery.
2002.
Numericalrecipes: The art of scientific computing V 2.10 withLinux or single-screen license.
Cambridge Univer-sity Press.Thomas Proisi, Stefan Evert, Paul Greiner, and BesimKabashi.
2014.
SemantiKLUE: Robust semanticsimilarity at multiple levels using maximum weightmatching.
In Proceedings of the 8th InternationalWorkshop on Semantic Evaluation (SemEval-2014),Dublin, Ireland.Cyrus Rashtchian, Peter Young, Micah Hodosh, andJulia Hockenmaier.
2010.
Collecting image annota-tions using Amazon?s Mechanical Turk.
In Proceed-ings of the NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s Mechan-ical Turk, CSLDAMT ?10, pages 139?147, Strouds-burg, PA, USA.Miguel Rios.
2014.
UoW: Multi-task learning Gaus-sian process for semantic textual similarity.
In Pro-ceedings of the 8th International Workshop on Se-mantic Evaluation (SemEval-2014), Dublin, Ireland.Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-pher D. Manning, and Andrew Y. Ng.
2014.Grounded compositional semantics for finding anddescribing images with sentences.
Transactionsof the Association for Computational Linguistics,pages 207?218.Md Arafat Sultan, Steven Bethard, and Tamara Sum-ner.
2014a.
Back to basics for monolingual align-ment: Exploiting word similarity and contextual ev-idence.
Transactions of the Association for Compu-tational Linguistics, 2:219?230.Md Arafat Sultan, Steven Bethard, and Tamara Sum-ner.
2014b.
DLS@CU: Sentence similarity fromword aligment.
In Proceedings of the 8th Interna-tional Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland.Darnes Vilari?no, David Pinto, Sa?ul Le?on, Mireya To-var, and Beatriz Beltr?an.
2014.
BUAP: Evaluatingfeatures for multilingual and cross-level semantictextual similarity.
In Proceedings of the 8th Interna-tional Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland.Ngoc Phuoc An Vo, Tommaso Caselli, and OctavianPopescu.
2014.
FBK-TR: Applying SVM withmultiple linguistic features for cross-level semanticsimilarity.
In Proceedings of the 8th InternationalWorkshop on Semantic Evaluation (SemEval-2014),Dublin, Ireland.Frane?Sari?c, Goran Glava?s, Mladen Karan, Jan?Snajder,and Bojana Dalbelo Ba?si?c.
2012.
Takelab: Sys-tems for measuring semantic text similarity.
In Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 441?448,Montr?eal, Canada, 7-8 June.Zhibiao Wu and Martha Palmer.
1994.
Verbs seman-tics and lexical selection.
In Proceedings of the 32ndannual meeting on Association for ComputationalLinguistics, pages 133?138, Las Cruces, New Mex-ico.91
