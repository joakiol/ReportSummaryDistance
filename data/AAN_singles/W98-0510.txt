A Case Study in Implementing Dependency-Based GrammarsMade BOURDON, Lyne DA SYLVA, Michel GAGNON,Alma KHARRAT, Sonja KNOLL, Anna MACLACHLANLes Logiciels Machina Sapiens inc.3535, chemin de la Reine-Marie, bureau 420,Montr~.al, QC,Canadambourdon,ldasylva, mgaguon,akharrat'sknoll,amaclachlan @machinasapiens.comhttp://www.machinasapiens.comAbstractIn creating an English grammarchecking software product, weimplemented a large-coverage rammarbased on the dependency grammarformalism.
This implementationrequired some adaptation of currentlinguistic description to prevent seriousovergeneration f parse trees.
Here, ?
wepresent one particular example, that ofpreposition stranding and danglingprepositions, where implementing analternative to existing linguistic analysesis warranted to limit such over-generation.IntroductionImplementing a linguistic theory such asdependency grammar leads to many types ofproblems (see the discussion in Fuchs et al1993, p.121ff, among others).
We will focuson a problem typical of large-scale NLPimplementations: some theoretical descriptionsentail unforeseen computational costs.The linguistic phenomenon chosen to illustratethis problem is the case of so-called strandedand dangling prepositions in English.
We willshow how our initial description, akin to thosepresented in the dependency grammarliterature, led to inefficiency in the parser.
Bymodifying the grammatical nalysis in somecases, rather than the parser itself, an overallimprovement was achieved.This problem raises the issue of the difficultiesinherent in the large-scale implementation f atheoretical grammar which has been designedto describe linguistic phenomena and not asthe basis of a parser.1 An implementation of a broad-coverage dependency-based grammarThe grammar constitutes the backbone of ourgrammar checker software for differentlanguages, including French, Spanish, Englishand Portuguese.
Our checkers belong to thethird generation of such products, whichperform a complete and detailed grammaticalanalysis of sentences.A commercially viable grammar checker mustcatch all the errors in a text and only those?
errors.
Crucially,.itmust do so in a relativelyshort time on moderately powerful machines.Performing an accurate linguistic analysis oftexts requires time and appropriate strategies.Some developers avoid the problemsassociated with performing a completegrammatical nalysis by using local (or semi-local) methods of processing instead.
It seemsobvious, however, that the more linguisticknowledge a checker has, the better its chancesof identifying errors.Our grammar checker, which performs acomplete linguistic analysis of all sentences, isbased on a dependency grmmnar.
This type ofgrammar was originally perceived as beingintuitively more efficient in computationalterms, allowing simple descriptions that can be?
parsed in an incremental manner.
It has indeedproved to be efficient in our implementation.Some of that efficiency is due to the initialconstraints placed on the system, including thefollowing among others.
First, every word inthe input sentence corresponds to a node inthe structure built (with minor exceptions).Second, only adjacent subtrees may becombined.
Third, each node may have at mostone father.
These restrictions also limit thetypes of linguistic analyses we can implement,as we will illustrate.881.1  The  grammar  checker  sof twareThe coverage and accuracy of the linguisticdescriptions on which our grammar checker isbased determine its strength.
Grammaticalstructures that are difficult to describe andexplain are not necessarily considered by thelayman as being particularly problematic (takefor example coordination).
Moreover, acommercial product cannot survive if it fails totreat some cases that are obvious to a user.
Forexample, punctuation falls outside thedescription provided by most syntactictheories but it is pervasive in writtent exts andmust be handled and perhaps corrected.Our grammar checker is aimed at the generalpublic and is designed to analyze written textsfrom a range of domains and in a range ofstyles.
It therefore requires a grammar with avery broad coverage as well as a very extensivelexicon.It is implemented in C++.
The English lexiconconsists of 65,000 English root words.Syntactic structures handled by the parserinclude the core of grammar (noun groups,verb groups, prepositional groups, etc.)
as wellas: declaratives, interrogatives, relatives,exclamatives, imperatives, comparatives,superlatives, most coordinate structures, manyelliptical structures, punctuation, constructionsbelonging to the grammar of correspondence,J such as addresses, and some types of idiomaticexpressions.The grammar checker proceeds in thefollowing way.
It starts by performing a lexicalanalysis.
Some phonetic approximation rulesmay be used to deal with unrecognized wordsor to resolve incomplete parses.
The syntacticcomponent uses a set of dependency rules,which involve some simplification of thestructures postulated within the literature ondependency grammar.
Once an analysis isgenerated, grammatical corrections areperformed and the result is displayed to theuser .Theoretical approaches modell ing howhumans parse language often start or finishwith a semantic representation.
Our parser,however, deals only with surface structure.There is no semantic component o ourproduct per se, but a small number of semanticfeatures are used.
Given the commercialsuccess of our grammar checker, it can beconsidered a successful implementation.1 .2  A centra l  p rob lemOne of the key problems in implementing anNLP system is deal ing with combinatorialexplosion: in attempting to produce theanalysis for a sentence given a potentially verylarge set of rules, some strategies must be usedto reduce the search space.
Otherwise the timenecessary to complete the computation may betoo long.We will not exhaust he types of difficultiesthat were encountered and solved, but willfocus primarily on one problem stemmingfrom a linguistic analysis which entailed thecreation of a large search space: stranded anddangling prepositions.2 A ?problemat ic  phenomenon:s t randed and  dangl ing prepos i t ionsTwo classes of lone prepositions which are notfollowed by a complement are known asdangling and stranded prepositions.
There areseveral contexts where these prepositions arefound and they are very common in standardEnglish.
We will focus primarily on pseudo-passive and relative contexts, and mentionanother context in our conclusion.One example of a pseudo-passive, that is apassive with a stranded preposition, is given in(1) and some relative clause examples areillustrated in (2), where the preposition is saidto be dangling (Mel'~uk, 1987, p.124).
Theprepositions are indicated in bold in each case.
(I) Pseudo-passive (stranding)He was yelled at.
(2) Relatives (with dangling prepositions)a.
They knew the man we talked about.b.
They knew the man who he thinks we talkedabout.Note that these lone prepositions are not usedin the same contexts as particles like out whichforms a phrasal verb with take in the sentenceHe took the garbage out.
In this latter casewhile there is a debate as to where thepreposition should attach (to the verb take orto the NP the garbage), no NP is missing, orextraposed, contrary to the examples above.Outside the realm of dependency grammar, inphrase structure grammar, the analysis of suchsentences would have the lone prepositionabout belonging to the verb phrase headed by89talked.
In the phrase structure approach ofGazdar et al(1985, p.147), for example, therewould be an empty category and SLASHnotation, as indicated in (3).O) Generalized Phrase S~ucune Grammar nalysisVP/NPV PP/NPtalkedP NP \[ +NULL \]/NIPabout  eIn dependency grammar, the dependencerelations are the crucial ones, rather thanconstituency.
There have been different viewson what relations a lone preposition bears tothe other elements in these types ofconstructions.
We will present analyses fromthe conceptions of dependency grammarproposed by Mel'~uk, and by Hudson, both ofwhom treat such constructions.2 .2  Me l '~uk 's  AnalysesAccording to Mel'~uk (1987, pp.82, 124-125),a preposition must have a dependent NP,except sn the following cases.
Strandedprepositions have no dependent, and danglingprepositions may or may not have the usualdependent.
If the dangling preposition doeshave a dependent, i  is not attached in the usualway, as we will illustrate.Starting with pseudo-passives, prepositionstranding occurs when the dependent NP in anactive construction becomes the grammaticalsubject in the related pseudo-passiveconstruction.
Here are two examples fromMel'~uk.
(4) Peter deals with the problem.
(5) The problem is dealt with by Peter.One consequence of passivization is theconversion of one of the surface syntacticrelations, known as SSyntRels in Mel'~uk'sterminology (see the discussion in Mel'~uk,1987, p.31).
In particular, the relation thatsubordinates the preposition (and itsdependent NP) to the active verb in (4) is notthe same as the relation between thoseelements in (5).
The corresponding structuresin (4a) and (5a) below are derived from thediagram in Mel'~uk (1987, p.124), with thepassive agent omitted.In (4a) the preposition with and its dependentthe problem are subordinated to the verb dealsby the 2rid Completive SSyntRel.
In contrast,in (Sa) the latter SSyntRel is not tolerated in apassive construction and therefore a specialSSyntRel, the Colligative, is posited especiallyfor this construction.
(~) A~dea ls  ..Peter withproblemL /the(Sa) Pseudo-passive (stranding)i sp rob  ~~dea l tthe withMel'~uk makes a distinction between thesestranded preposit ions and dangl ingprepositions.
The dangling preposition, unlikethe stranded preposition, keeps the originalSSyntRel that subordinates it to the verb.Consider the data for dangling prepositions inrelatives.
The basic sentence in (6) has norelativization, while dangling prepositions canbe found in sentences involving relativeclauses such as (7) and (8) (examples fromMel'~uk) :(6) I talked to all the accountants.
(7) All the accountants whom I talk to sayreceivables are piling up.
(8) All the accountants I talk to say receivables arepiling up.In (6) the normal Prepositional SSyntRelholds between the preposition and itscomplement, as illustrated in (6a) below.
Thedangling preposition in (7) continues to heada SSyntRel that subordinates its displacedcomplement whom labelled a PrepositionalSSyntRel, as in (7a).
Finally, in (8), sincewhom is deleted, there is no such relation andthe preposition has no dependents, as in (8a)(structures adapted from Mel'~uk 1987,pp.
130, 366):90(6a) Preposition with usual complementtalkto Prepos/t~.a/accountants(7a) Dangling with relative pronounta lktowhom(Sa) Dangling preposition without pronountalkto2.3  Hudson 's  Ana lysesThe analysis of lone prepositions in Hudson(1990) involves somewhat different relations.He points out that in his theory, WordGrammar, multiple relations between twoelements are allowed, and that a word maydepend on more than one head simultaneously(see the discussion in Hudson 1992, p.145).Dangling prepositions in relatives areacknowledged but not given an explicitanalysis in Hudson (1990).
He does, however,provide the following analysis for a pseudo-passive with a stranded preposition (adaptedfrom Hudson 1990, p.348).
(9) Stranded Prepositionwas plementHe < ""J~" yelled~~~Notice that the stranded preposition in (9)bears three dependency relations.
It is a post-dependent of the verb yelled, it has thepronoun he as its complement, and it bears arelation special to pseudo-passives withstranded prepositions labelled passive-link.3 Implementation problemsThere are a number of problems associatedwith the implementation f these theoreticalapproaches.
Note that there is disagreementbetween linguists as to the optimal treatment.
Iis perhaps no coincidence that a problemwhich presents theoretical difficulties is alsomore problematic to implement.In structures like (9) above, one node, namelythe node containing the word he, has threefathers: was, yel led and at.
We cannotimplement this structure directly since in ourimplementation, each node in a structure has aunique father node and only adjacent nodesmay be linked by a relation.
This strategyreduces considerably the number ofintermediate trees to be examined whileconstructing a given tree.
It also simplifiestraversal of trees.Consider next dangling prepositions inrelatives, such as the man whom we talkedabout, which pose a similar problem.
Wecannot implement Mel'~uk's analysis illustratedin (10) since the node whom has two fathers:talked and about.
(lo)nuanthe talkL~lw~.__weaboutPrepos/~ona/Our initial hypothesis for implementing suchrelatives was simply to attach the danglingpreposition to the verb immediately to its leftas in (11) while allowing other constraints toverify that the relative pronoun is correctlylicensed within the structure.
(IDman/ .
.
I  ~ .
.
Ithe talkedwhom we aboutThis choice of implementation led to anumber of serious efficiency problems.
Themain problem we will address is that too manytrees were being produced and therefore toomuch time was being wasted.91A preposition must have a complement inorder to attach to the verb with its normalrelation.
However, a dangling prepositionattaches to the verb without its complement.
Inthe course of analyzing every sentencecontaining a preposition after the verb, thepreposition was attached both as dangling andthe tree where they are licensed.
That is,attachment of a lone preposition is permittedonly when there is a context hat permits sucha preposition, such as a passive verb or arelative clause structure.
We show how oursolution reduced parsing time.not as dangling, since deciding whether a 4 .1  Pseudo-pass ivesgiven preposition is dangling or not can bedifficult locally.
Even if the invalid analyses For pseudo-passives, our implementation iscan eventually be discarded, their generationgreatly increases parsing time.
We will see inthe next section how this problem disappears ifthe analysis is slightly modified.Let us examine some other means of avoidingmisanalysis and overgeneration of trees forthese sentences, and show how these means areinadequate.
First, one could verify thecategory of the word following the prepositionsince a lone preposition would not be followedby a nominal complement.
This kind ofrestriction must be used with great care,especially in English in which words oftenbelong to several categories and inflection isnot rich enough to help disambiguate betweencategories.
Consider some concrete xamplessuch as (12) where the preposition in is notdangling and (13) where about is a danglingpreposition.
In (12), this restriction does nothelp the parser since shops can be a verb andyet it is the complement of the preposition.Similarly, in (13), checking the category ofshops is not sufficient o determine that aboutis a dangling preposition since shops can beeither a verb or a noun.
(12) He sold them in shops.
(13) The man we talked about shops here.Secondly, one could propose a strategy wherethe preposition would attach to the verb onlyafter the relative clause has been attached tothe noun.
This presupposes, however, thatsubtrees can be combined arbitrarily, i.e.
byjoining together any intermediate (non-root)nodes in the construction of the tree.
This isproblematic because it potentially creates treeswith two roots.
Moreover, this augmentation fthe system is not warranted.
We already havean efficient strategy that is not arbitrary whichallows the combination of complete subtreesonly.4 So lu t ionsTo avoid superfluous tree building, we allowlone prepositions to attach only at the point incloser to that of Mel'~uk than to that ofHudson.
Recall that Hudson's analysis in (9)involved multiple fathers.
We thus chose not toimplement his complement relation between atand he and only a single relation betweenyelled and at in sentences like He was yelledat.Following Hudson we use a distinctive relation,which we label prep-strand instead of passive-link.
In addition, we use a set of constraints tocheck that the preposition is indeedappropriate to the verb (that yell can take at).Thus our analysis is as follows, where eachnode has at most one father and where onlyone relation holds between any pair of nodes:(14) implementation of pseudo-passivewasHe yelled prep-strandat4.2  Re la t ivesWhile our solution for pseudo-passives closelyfollows that of Mel'~uk, the case of relatives ismore complex.
Recall that some of hisanalyses of relatives involved multiple fathers.In order to avoid this problem in relativeclauses, the dangling preposition is attached inour implementation not to the governing verb,but to the noun which is the antecedent of therelative.
To see the advantage of our analysis,consider sentences where the relative is notintroduced by a wh-word in examples like(15) (previously (2a)).
(15) The man we talked about.
(15a) our original solution~ a l k e d  thewe about92(15b) implemented solutionmanthe aboutweIn this example, the dangling preposition isonly licensed by the presence of the relativeclause.
Instead of (15a), we therefore preferthe analysis in (15b), where the preposition isattached to the head noun man once a relativeclause has been created.
Rather than imple-menting a relation between talked and aboutwe verify compatibility between the verb andits prepositional complement independently.Note that an incomplete relative clause iscreated: we talked.
Some constraints arerelaxed and checked at a higher level to ensurethe ultimate completeness of the overallstructure.Next, consider long-distance r latives uch asin (16) (previously (2a)) and the analyses in(16a) and (16b).
(16) The man who he thinks we talked about(16a) our original solutionmanthe thinkswho he talkedwe about(16b) implemented solutionmanthe thinks aboutwho he talkedweIn (16), we combine the problem of danglinga preposition with that of unboundeddependency.
Within our system, it isimpossible to attach the relative pronoun whoto the verb which subcategorizes for it in theselong-distance r latives because of word order.In the same way, the dangling prepositionabout does not attach to talked but rather itattaches at a higher level, to man.These analyses have crucially solved theproblem of tree overgeneration.
Theattachment of lone prepositions may be madeonce the licensing criteria are met (passivevoice, relativization or other such contexts).Therefore only those subtrees which will likelylead to a complete and successful analysis willbe built.4.3 Remaining problemsOur analysis presupposes that the danglingpreposition occurs as the last element in therelative clause.
There are rare cases whereanother element can follow the danglingpreposition, such as (17).
(17) The man we talked about o MarySince the dangling preposition about  isattached to man,  to avoid crossing ofdependency relations, we would have to attachthe phrase to Mary to the node man instead ofattaching it more naturally to the verb talked.The analysis is shown in (18).
(18) tough-adjective with an extra PPman/ _~ ~ t ~ " ~  _3.the talked about Marywe toNote that there are cases where a prepositionalphrase can attach to a noun following arelative.
Thus the construction i  (17) wouldhave the same analysis as that in (19).
(19) The man we talked about with glassesConstructions such as the one in (17) are notmarked constructions.
However, given theirlow frequency relative to the high frequencyof preposition dangling in general, ourconstrained analysis is justified in terms ofcomputational efficiency.5 Conc lus ionOur solution led to an overall improvement ofthe parser's performance.
This type of solutionis, of course, only one of many ways to reducethe size of the search space.
We have foundthat the problem of combinatorial explosion inparsing English is even greater than it is inFrench due to the higher incidence of lexicalambiguity in English.
Our adaptation ofanalyses found in the literature was thereforedeemed necessary.A related problem for which we have not comeacross a theoretical analysis is loneprepositions in the context of so-called tough-93IIilIIIIIIlIIIIIIIadjectives.
These adjectives can take aninfinitival complement whose object is missing(as in (20)).
Infinitives with a prepositionalobject are also possible complements oftough-adjectives (as in (21)), and this isanother context where a lone preposition islicensed, as exemple (22) illustrates.
(20) Bill is easy to love.
(21) It is easy to work for Bill.
(22) Bill is easy to work for.Sentences like (22) require a complextheoretical analysis.
A dependency relationshould hold between the preposition for andthe noun Bill while the latter is also the subjectof the tough-adjective predicate.Our analysis for this case was dictated by thesame considerations as for the other cases.While the preposition depends on thepreceding verb, it is licensed by the presenceof the tough-adjective.
Just as with the othercases, then, the preposition is attached high upin the structure at the point where it is licensed.Here, for is attached to the adjective, after theinfinitival complement has been attached.
(23) implementation f tough-adjective constructioni sBill easy-to forworkOur solution to the problem of loneprepositions has been influenced primarily byconsiderations of implementation.
It remainsto be seen what types of consequences thisadaptation entails in terms of semantics.In conclusion, we have presented a set of datathat highlights an important constraint onmany implementations, including our own:Linguistic descriptions must be modelled insuch a way as to optimize performance.AcknowledgementsWe would like to thank Les Logiciels MachinaSapiens inc. for supporting us in writing thispaper.
We are endebted to all the people, pastand present, who have contributed to thedevelopment of the grammar checkers.We thank Mary Howatt for editing advice andanonymous reviewers for their usefulcomments.
All errors remain those of theauthors.ReferencesFucKs, Catherine, Laurence Danlos, Anne LacheretoDujour, Daniel Luzzati and Bernard Victorfi (1993)Linguistique t traitements automatiques deslangues, Hachette, Paris.Gazdar, Gerald, Ewan Klein, Geoffrey Pullum andIvan Sag.
(1985).Generalized Phrase StructureGrammar, Harvard University Press, Cambridge.Hudson, Richard (1984) Word Grammar, BasilBlackwell, Oxford.Hudson, Richard (1990) English Word Grammar,Basil Blackwell, Oxford.Hudson, Richard and Norman Fraser (1992)"Inheritance inWord Grammar", in ComputationalLinguistics 18.2, MIT Press.Mel'euk, Igor A.
(1987) Surface Syntax of English.
AFormal Model within the Meaning-TextFramework, Benjamins, Amsterdam.Mel'~uk, Igor A.
(1988) Dependency S ntax: Theoryand Practice, State University of New York Press,Albany.94
