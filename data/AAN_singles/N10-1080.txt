Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 555?563,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsThe Best Lexical Metric forPhrase-Based Statistical MT System OptimizationDaniel Cer, Christopher D. Manning and Daniel JurafskyStanford UniversityStanford, CA 94305, USAAbstractTranslation systems are generally trained tooptimize BLEU, but many alternative metricsare available.
We explore how optimizingtoward various automatic evaluation metrics(BLEU, METEOR, NIST, TER) affects the re-sulting model.
We train a state-of-the-art MTsystem using MERT on many parameteriza-tions of each metric and evaluate the result-ing models on the other metrics and also us-ing human judges.
In accordance with popularwisdom, we find that it?s important to train onthe same metric used in testing.
However, wealso find that training to a newer metric is onlyuseful to the extent that the MT model?s struc-ture and features allow it to take advantage ofthe metric.
Contrasting with TER?s good cor-relation with human judgments, we show thatpeople tend to prefer BLEU and NIST trainedmodels to those trained on edit distance basedmetrics like TER or WER.
Human prefer-ences for METEOR trained models varies de-pending on the source language.
Since usingBLEU or NIST produces models that are morerobust to evaluation by other metrics and per-form well in human judgments, we concludethey are still the best choice for training.1 IntroductionSince their introduction, automated measures of ma-chine translation quality have played a critical rolein the development and evolution of SMT systems.While such metrics were initially intended for eval-uation, popular training methods such as minimumerror rate training (MERT) (Och, 2003) and mar-gin infused relaxed algorithm (MIRA) (Crammerand Singer, 2003; Watanabe et al, 2007; Chiang etal., 2008) train translation models toward a specificevaluation metric.
This makes the quality of the re-sulting model dependent on how accurately the au-tomatic metric actually reflects human preferences.The most popular metric for both comparing sys-tems and tuning MT models has been BLEU.
WhileBLEU (Papineni et al, 2002) is relatively simple,scoring translations according to their n-gram over-lap with reference translations, it still achieves a rea-sonable correlation with human judgments of trans-lation quality.
It is also robust enough to use for au-tomatic optimization.
However, BLEU does have anumber of shortcomings.
It doesn?t penalize n-gramscrambling (Callison-Burch et al, 2006), and sinceit isn?t aware of synonymous words or phrases, it caninappropriately penalize translations that use them.Recently, there have been efforts to develop bet-ter evaluation metrics.
Metrics such as TranslationEdit Rate (TER) (Snover et al, 2006; Snover et al,2009) and METEOR1 (Lavie and Denkowski, 2009)perform a more sophisticated analysis of the trans-lations being evaluated and the scores they producetend to achieve a better correlation with human judg-ments than those produced by BLEU (Snover et al,2009; Lavie and Denkowski, 2009; Przybocki et al,2008; Snover et al, 2006).Their better correlations suggest that we mightobtain higher quality translations by making use ofthese new metrics when training our models.
We ex-pect that training on a specific metric will producethe best performing model according to that met-1METEOR: Metric for Evaluation of Translation with Ex-plicit ORdering.555ric.
Doing better on metrics that better reflect humanjudgments seems to imply the translations producedby the model would be preferred by human judges.However, there are four potential problems.
First,some metrics could be susceptible to systematic ex-ploitation by the training algorithm and result inmodel translations that have a high score accordingto the evaluation metric but that are of low qual-ity.2 Second, other metrics may result in objectivefunctions that are harder to optimize.
Third, somemay result in better generalization performance attest time by not encouraging overfitting of the train-ing data.
Finally, as a practical concern, metrics usedfor training cannot be too slow.In this paper, we systematically explore these fourissues for the most popular metrics available to theMT community.
We examine how well models per-form both on the metrics on which they were trainedand on the other alternative metrics.
Multiple mod-els are trained using each metric in order to deter-mine the stability of the resulting models.
Selectmodels are scored by human judges in order to deter-mine how performance differences obtained by tun-ing to different automated metrics relates to actualhuman preferences.The next sections introduce the metrics and ourtraining procedure.
We follow with two sets of coreresults, machine evaluation in section 5, and humanevaluation in section 6.2 Evaluation MetricsDesigning good automated metrics for evaluatingmachine translations is challenging due to the vari-ety of acceptable translations for each foreign sen-tence.
Popular metrics produce scores primarilybased on matching sequences of words in the systemtranslation to those in one or more reference trans-lations.
The metrics primarily differ in how they ac-count for reorderings and synonyms.2.1 BLEUBLEU (Papineni et al, 2002) uses the percentageof n-grams found in machine translations that alsooccur in the reference translations.
These n-gramprecisions are calculated separately for different n-2For example, BLEU computed without the brevity penaltywould likely result in models that have a strong preference forgenerating pathologically short translations.gram lengths and then combined using a geometricmean.
The score is then scaled by a brevity penaltyif the candidate translations are shorter than the ref-erences, BP = min(1.0, e1?len(R)/len(T )).
Equa-tion 1 gives BLEU using n-grams up to lengthN fora corpus of candidate translations T and referencetranslations R. A variant of BLEU called the NISTmetric (Doddington, 2002) weights n-gram matchesby how informative they are.BLEU:N =(N?n=1n-grams(T?R)n-grams(T )) 1NBP (1)While easy to compute, BLEU has a number ofshortcomings.
Since the order of matching n-gramsis ignored, n-grams in a translation can be randomlyrearranged around non-matching material or othern-gram breaks without harming the score.
BLEUalso does not explicitly check whether informationis missing from the candidate translations, as it onlyexamines what fraction of candidate translation n-grams are in the references and not what fractionof references n-grams are in the candidates (i.e.,BLEU ignores n-gram recall).
Finally, the metricdoes not account for words and phrases that havesimilar meanings.2.2 METEORMETEOR (Lavie and Denkowski, 2009) computesa one-to-one alignment between matching words ina candidate translation and a reference.
If a wordmatches multiple other words, preference is given tothe alignment that reorders the words the least, withthe amount of reordering measured by the number ofcrossing alignments.
Alignments are first generatedfor exact matches between words.
Additional align-ments are created by repeatedly running the align-ment procedure over unaligned words, first allowingfor matches between word stems, and then allow-ing matches between words listed as synonyms inWordNet.
From the final alignment, the candidatetranslation?s unigram precision and recall is calcu-lated, P = matcheslength trans andR =matcheslength ref .
These twoare then combined into a weighted harmonic mean(2).
To penalize reorderings, this value is then scaledby a fragmentation penalty based on the number ofchunks the two sentences would need to be broken556into to allow them to be rearranged with no crossingalignments, P?,?
= 1 ?
?(chunksmatches)?.F?
=PR?P + (1 ?
?)R(2)METEOR?,?,?
= F?
?
P?,?
(3)Equation 3 gives the final METEOR score as theproduct of the unigram harmonic mean, F?, and thefragmentation penalty, P?,?
.
The free parameters ?,?, and ?
can be used to tune the metric to humanjudgments on a specific language and variation ofthe evaluation task (e.g., ranking candidate transla-tions vs. reproducing judgments of translations ade-quacy and fluency).2.3 Translation Edit RateTER (Snover et al, 2006) searches for the shortestsequence of edit operations needed to turn a can-didate translation into one of the reference transla-tions.
The allowable edits are the insertion, dele-tion, and substitution of individual words and swapsof adjacent sequences of words.
The swap opera-tion differentiates TER from the simpler word errorrate (WER) metric (Nie?en et al, 2000), which onlymakes use of insertions, deletions, and substitutions.Swaps prevent phrase reorderings from being exces-sively penalized.
Once the shortest sequence of op-erations is found,3 TER is calculated simply as thenumber of required edits divided by the referencetranslation length, or average reference translationlength when multiple are available (4).TER =min editsavg ref length(4)TER-Plus (TERp) (Snover et al, 2009) extendsTER by allowing the cost of edit operations to betuned in order to maximize the metric?s agreementwith human judgments.
TERp also introduces threenew edit opertions: word stem matches, WordNetsynonym matches, and multiword matches using atable of scored paraphrases.3Since swaps prevent TER from being calculated exactly us-ing dynamic programming, a beam search is used and this canoverestimate the number of required edits.3 MERTMERT is the standard technique for obtaining a ma-chine translation model fit to a specific evaluationmetric (Och, 2003).
Learning such a model cannotbe done using gradient methods since the value ofthe objective function only depends on the transla-tion model?s argmax for each sentence in the tun-ing set.
Typically, this optimization is performed asa series of line searches that examines the value ofthe evaluation metric at critical points where a newtranslation argmax becomes preferred by the model.Since the model score assigned to each candidatetranslation varies linearly with changes to the modelparameters, it is possible to efficiently find the globalminimum along any given search direction with onlyO(n2) operations when n-best lists are used.Using our implementation of MERT that allowsfor pluggable optimization metrics, we tune mod-els to BLEU:N for N = 1 .
.
.
5, TER, two con-figurations of TERp, WER, several configurationsof METEOR, as well as additive combinations ofthese metrics.
The TERp configurations includethe default configuration of TERp and TERpA:the configuration of TERp that was trained tomatch human judgments for NIST Metrics MATR(Matthew Snover and Schwartz, 2008; Przybocki etal., 2008).
For METEOR, we used the standard ME-TEOR English parameters (?
= 0.8, ?
= 2.5, ?
=0.4), and the English parameters for the rankingME-TEOR (?
= 0.95, ?
= 0.5, ?
= 0.5),4 whichwas tuned to maximize the metric?s correlation withWMT-07 human ranking judgements (Agarwal andLavie, 2008).
The default METEOR parameters fa-vor longer translations than the other metrics, sincehigh ?
values place much more weight on unigramrecall than precision.
Since this may put modelstuned to METEOR at a disadvantage when beingevaluated by the other metrics, we also use a variantof the standard English model and of ranking ME-TEOR with ?
set to 0.5, as this weights both recalland precision equally.For each iteration of MERT, 20 random restartswere used in addition to the best performing pointdiscovered during earlier iterations of training.54Agarwal and Lavie (2008) report ?
= 0.45, however the0.8.2 release of METEOR uses ?
= 0.5 for ranking English.5This is not necessarily identical with the point returned bythe most recent MERT iteration, but rather can be any point557Since MERT is known to be sensitive to what restartpoints are provided, we use the same series of ran-dom restart points for each model.
During each it-eration of MERT, the random seed is based on theMERT iteration number.
Thus, while a different setof random points is selected during each MERT iter-ation, on any given iteration all models use the sameset of points.
This prevents models from doing betteror worse just because they received different startingpoints.
However, it is still possible that certain ran-dom starting points are better for some evaluationmetrics than others.4 ExperimentsExperiments were run using Phrasal (Cer et al,2010), a left-to-right beam search decoder thatachieves a matching BLEU score to Moses (Koehnet al, 2007) on a variety of data sets.
During de-coding we made use of a stack size of 100, set thedistortion limit to 6, and retrieved 20 translation op-tions for each unique source phrase.Using the selected metrics, we train both Chi-nese to English and Arabic to English models.6 TheChinese to English models are trained using NISTMT02 and evaluated on NIST MT03.
The Arabicto English experiments use NIST MT06 for train-ing and GALE dev07 for evaluation.
The resultingmodels are scored using all of the standalone metricsused during training.4.1 Arabic to EnglishOur Arabic to English system was based on a wellranking 2009 NIST submission (Galley et al, 2009).The phrase table was extracted using all of the al-lowed resources for the constrained Arabic to En-glish track.
Word alignment was performed usingthe Berkeley cross-EM aligner (Liang et al, 2006).Phrases were extracted using the grow heuristic(Koehn et al, 2003).
However, we threw away allphrases that have a P (e|f) < 0.0001 in order to re-duce the size of the phrase table.
From the aligneddata, we also extracted a hierarchical reorderingmodel that is similar to popular lexical reorderingmodels (Koehn et al, 2007) but that models swapscontaining more than just one phrase (Galley andreturned during an earlier iteration of MERT.6Given the amount of time required to train a TERpAmodel,we only present TERpA results for Chinese to English.Manning, 2008).
A 5-gram language model was cre-ated with the SRI language modeling toolkit (Stol-cke, 2002) using all of the English material fromthe parallel data employed to train the phrase tableas well as Xinhua Chinese English Parallel News(LDC2002E18).7 The resulting decoding model has16 features that are optimized during MERT.4.2 Chinese to EnglishFor our Chinese to English system, our phrase ta-ble was built using 1,140,693 sentence pairs sam-pled from the GALE Y2 training data.
The Chinesesentences were word segmented using the 2008 ver-sion of Stanford Chinese Word Segmenter (Chang etal., 2008; Tseng et al, 2005).
Phrases were extractedby running GIZA++ (Och and Ney, 2003) in bothdirections and then merging the alignments usingthe grow-diag-final heuristic (Koehn et al, 2003).From the merged alignments we also extracted abidirectional lexical reordering model conditionedon the source and the target phrases (Koehn et al,2007).
A 5-gram language model was created withthe SRI language modeling toolkit (Stolcke, 2002)and trained using the Gigaword corpus and Englishsentences from the parallel data.
The resulting de-coding model has 14 features to be trained.5 ResultsAs seen in tables 1 and 2, the evaluation metric weuse during training has a substantial impact on per-formance as measured by the various other metrics.There is a clear block structure where the best classof metrics to train on is the same class that is usedduring evaluation.
Within this block structure, wemake three primary observations.
First, the bestperforming model according to any specific metricconfiguration is usually not the model we trained tothat configuration.
In the Chinese results, the modeltrained on BLEU:3 scores 0.74 points higher onBLEU:4 than the model actually trained to BLEU:4.In fact, the BLEU:3 trained model outperforms allother models on BLEU:N metrics.
For the Arabicresults, training on NIST scores 0.27 points higher7In order to run multiple experiments in parallel on the com-puters available to us, the system we use for this work differsfrom our NIST submission in that we remove the Google n-gram language model.
This results in a performance drop ofless than 1.0 BLEU point on our dev data.558Train\Eval BLEU:1 BLEU:2 BLEU:3 BLEU:4 BLEU:5 NIST TER TERp WER TERpA METR METR-r METR METR-r?
= 0.5 ?
= 0.5BLEU:1 75.98 55.39 40.41 29.64 21.60 11.94 78.07 78.71 68.28 73.63 41.98 59.63 42.46 60.02BLEU:2 76.58 57.24 42.84 32.21 24.09 12.20 77.09 77.63 67.16 72.54 43.20 60.91 43.59 61.56BLEU:3 76.74 57.46 43.13 32.52 24.44 12.22 76.53 77.07 66.81 72.01 42.94 60.57 43.40 60.88BLEU:4 76.24 56.86 42.43 31.80 23.77 12.14 76.75 77.25 66.78 72.01 43.29 60.94 43.10 61.27BLEU:5 76.39 57.14 42.93 32.38 24.33 12.40 75.42 75.77 65.86 70.29 43.02 61.22 43.57 61.43NIST 76.41 56.86 42.34 31.67 23.57 12.38 75.20 75.72 65.78 70.11 43.11 61.04 43.78 61.84TER 73.23 53.39 39.09 28.81 21.18 12.73 71.33 71.70 63.92 66.58 38.65 55.49 41.76 59.07TERp 72.78 52.90 38.57 28.32 20.76 12.68 71.76 72.16 64.26 66.96 38.51 56.13 41.48 58.73TERpA 71.79 51.58 37.36 27.23 19.80 12.54 72.26 72.56 64.58 67.30 37.86 55.10 41.16 58.04WER 74.49 54.59 40.30 29.88 22.14 12.64 71.85 72.34 63.82 67.11 39.76 57.29 42.37 59.97METR 73.33 54.35 40.28 30.04 22.39 11.53 84.74 85.30 71.49 79.47 44.68 62.14 42.99 60.73METR-r 74.20 54.99 40.91 30.66 22.98 11.74 82.69 83.23 70.49 77.77 44.64 62.25 43.44 61.32METR:0.5 76.36 56.75 42.48 31.98 24.00 12.44 74.94 75.32 66.09 70.14 42.75 60.98 43.86 61.38METR-r:0.5 76.49 56.93 42.36 31.70 23.68 12.21 77.04 77.58 67.12 72.23 43.26 61.03 43.63 61.67Combined ModelsBLEU:4-TER 75.32 55.98 41.87 31.42 23.50 12.62 72.97 73.38 64.46 67.95 41.50 59.11 43.50 60.82BLEU:4-2TERp 75.22 55.76 41.57 31.11 23.25 12.64 72.48 72.89 64.17 67.43 41.12 58.82 42.73 60.86BLEU:4+2MTR 75.77 56.45 42.04 31.47 23.48 11.98 79.96 80.65 68.85 74.84 44.06 61.78 43.70 61.48Table 1: Chinese to English test set performance on MT03 using models trained using MERT on MT02.
In each column,cells shaded blue are better than average and those shaded red are below average.
The intensity of the shading indicatesthe degree of deviation from average.
For BLEU, NIST, and METEOR, higher is better.
For edit distance metrics likeTER and WER, lower is better.Train\Eval BLEU:1 BLEU:2 BLEU:3 BLEU:4 BLEU:5 NIST TER TERp WER METR METR-r METR METR-r?
= 0.5 ?
= 0.5BLEU:1 79.90 65.35 54.08 45.14 37.81 10.68 46.19 61.04 49.98 49.74 67.79 49.19 68.12BLEU:2 80.03 65.84 54.70 45.80 38.47 10.75 45.74 60.63 49.24 50.02 68.00 49.71 68.27BLEU:3 79.87 65.71 54.59 45.67 38.34 10.72 45.86 60.80 49.18 49.87 68.32 49.61 67.67BLEU:4 80.39 66.14 54.99 46.05 38.70 10.82 45.25 59.83 48.69 49.65 68.13 49.66 67.92BLEU:5 79.97 65.77 54.64 45.76 38.44 10.75 45.66 60.55 49.11 49.89 68.33 49.64 68.19NIST 80.41 66.27 55.22 46.32 38.98 10.96 44.11 57.92 47.74 48.88 67.85 49.88 68.52TER 79.69 65.52 54.44 45.55 38.23 10.75 43.36 56.12 47.11 47.90 66.49 49.55 68.12TERp 79.27 65.11 54.13 45.35 38.12 10.75 43.36 55.92 47.14 47.83 66.34 49.43 67.94WER 79.42 65.28 54.30 45.51 38.27 10.78 43.44 56.13 47.13 47.82 66.33 49.38 67.88METR 75.52 60.94 49.84 41.17 34.12 9.93 52.81 70.08 55.72 50.92 68.55 48.47 66.89METR-r 77.42 62.91 51.67 42.81 35.61 10.24 49.87 66.26 53.17 50.95 69.29 49.29 67.89METR:0.5 79.69 65.14 53.94 45.03 37.72 10.72 45.80 60.44 49.34 49.78 68.31 49.23 67.72METR-r:0.5 79.76 65.12 53.82 44.88 37.57 10.67 46.53 61.55 50.17 49.66 68.57 49.58 68.25Combined ModelsBLEU:4-TER 80.37 66.31 55.27 46.36 39.00 10.96 43.94 57.46 47.46 49.00 67.10 49.85 68.41BLEU:4-2TERp 79.65 65.53 54.54 45.75 38.48 10.80 43.42 56.16 47.15 47.90 65.93 49.09 67.90BLEU:4+2METR 79.43 64.97 53.75 44.87 37.58 10.63 46.74 62.03 50.35 50.42 68.92 49.70 68.37Table 2: Arabic to English test set performance on dev07 using models trained using MERT on MT06.
As above, in eachcolumn, cells shaded blue are better than average and those shaded red are below average.
The intensity of the shadingindicates the degree of deviation from average.on BLEU:4 than training on BLEU:4, and outper-forms all other models on BLEU:N metrics.Second, the edit distance based metrics (WER,TER, TERp, TERpA)8 seem to be nearly inter-changeable.
While the introduction of swaps al-lows the scores produced by the TER metrics toachieve better correlation with human judgments,our models are apparently unable to exploit this dur-ing training.
This maybe due to the monotone na-8In our implementation of multi-reference WER, we use thelength of the references that result in the lowest sentence levelWER to divide the edit costs.
In contrast, TER divides by theaverage reference length.
This difference can sometimes resultin WER being lower than the corresponding TER.
Also, as canbe seen in the Arabic to English results, TERp scores sometimesdiffer dramatically from TER scores due to normalization andtokenization differences (e.g., TERp removes punctuation priorto scoring, while TER does not).ture of the reference translations and the fact thathaving multiple references reduces the need for re-orderings.
However, it is possible that differencesbetween training to WER and TER would becomemore apparent using models that allow for longerdistance reorderings or that do a better job of cap-turing what reorderings are acceptable.Third, with the exception of BLEU:1, the perfor-mance of the BLEU, NIST, and the METEOR ?=.5models appears to be more robust across the otherevaluation metrics than the standardMETEOR,ME-TEOR ranking, and edit distance based models(WER, TER, TERp, an TERpA).
The latter mod-els tend to do quite well on metrics similar to whatthey were trained on, while performing particularlypoorly on the other metrics.
For example, on Chi-nese, the TER and WER models perform very well559on other edit distance based metrics, while perform-ing poorly on all the other metrics except NIST.While less pronounced, the same trend is also seenin the Arabic data.
Interestingly enough, while theTER, TERp and standard METEOR metrics achievegood correlations with human judgments, modelstrained to them are particularly mismatched in ourresults.
The edit distance models do terribly on ME-TEOR and METEOR ranking, while METEOR andMETEOR ranking models do poorly on TER, TERp,and TERpA.Training Itr MERT Training Itr MERTMetric Time Metric TimeBLEU:1 13 21:57 NIST 15 78:15BLEU:2 15 32:40 TER 7 21:00BLEU:3 19 45:08 TERp 9 19:19BLEU:4 10 24:13 TERpA 8 393:16BLEU:5 16 46:12 WER 13 33:53BL:4-TR 9 21:07 BL:4-2TRp 8 22:03METR 12 39:16 METR 0.5 18 42:04METR R 12 47:19 METR R:0.5 13 25:44Table 3: Chinese to English MERT iterations and trainingtimes, given in hours:mins and excluding decoder time.5.1 Other ResultsOn the training data, we see a similar block struc-ture within the results, but there is a different patternamong the top performers.
The tables are omitted,but we observe that, for Chinese, the BLEU:5 modelperforms best on the training data according to allhigher order BLEU metrics (4-7).
On Arabic, theBLEU:6 model does best on the same higher orderBLEU metrics (4-7).
By rewarding higher order n-gram matches, these objectives actually find minimathat result in more 4-gram matches than the mod-els optimized directly to BLEU:4.
However, the factthat this performance advantage disappears on theevaluation data suggests these higher order modelsalso promote overfitting.Models trained on additive metric blends tendto smooth out performance differences betweenthe classes of metrics they contain.
As expected,weighting the metrics used in the additive blends re-sults in models that perform slightly better on thetype of metric with the highest weight.Table 3 reports training times for select Chineseto English models.
Training to TERpA is very com-putationally expensive due to the implementation ofthe paraphrase table.
The TER family of metricstends to converge in fewer MERT iterations thanthose trained on other metrics such as BLEU, ME-TEOR or even WER.
This suggests that the learningobjective provided by these metrics is either easier tooptimize or they more easily trap the search in localminima.5.2 Model VarianceOne potential problem with interpreting the resultsabove is that learning with MERT is generally as-sumed to be noisy, with different runs of the al-gorithm possibly producing very different models.We explore to what extent the results just presentedwere affected by noise in the training procedure.
Weperform multiple training runs using select evalua-tion metrics and examining how consistent the re-sulting models are.
This also allows us to deter-mine whether the metric used as a learning criteriainfluences the stability of learning.
For these experi-ments, Chinese to English models are trained 5 timesusing a different series of random starting points.
Asbefore, 20 random restarts were used during eachMERT iteration.In table 4, models trained to BLEU andMETEORare relatively stable, with the METEOR:0.5 trainedmodels being the most stable.
The edit distancemodels, WER and TERp, vary more across train-ing runs, but still do not exceed the interesting crossmetric differences seen in table 1.
The instability ofWER and TERp, with TERp models having a stan-dard deviation of 1.3 in TERp and 2.5 in BLEU:4,make them risky metrics to use for training.6 Human EvaluationThe best evaluation metric to use during training isthe one that ultimately leads to the best translationsaccording to human judges.
We perform a humanevaluation of select models using Amazon Mechan-ical Turk, an online service for cheaply performingsimple tasks that require human intelligence.
To usethe service, tasks are broken down into individualunits of work known as human intelligence tasks(HITs).
HITs are assigned a small amount of moneythat is paid out to the workers that complete them.For many natural language annotation tasks, includ-ing machine translation evaluation, it is possible toobtain annotations that are as good as those pro-560Train\Eval ?
BLEU:1 BLEU:3 BLEU:4 BLEU:5 TERp WER METEOR METEOR:0.5BLEU:1 0.17 0.56 0.59 0.59 0.36 0.58 0.42 0.24BLEU:3 0.38 0.41 0.38 0.32 0.70 0.49 0.44 0.33BLEU:4 0.27 0.29 0.29 0.27 0.67 0.50 0.41 0.29BLEU:5 0.17 0.14 0.19 0.21 0.67 0.75 0.34 0.17TERp 1.38 2.66 2.53 2.20 1.31 1.39 1.95 1.82WER 0.62 1.37 1.37 1.25 1.31 1.21 1.10 1.01METEOR 0.80 0.56 0.48 0.44 3.71 2.69 0.69 1.10METEOR:0.5 0.32 0.11 0.09 0.11 0.23 0.12 0.07 0.11Table 4: MERT model variation for Chinese to English.
We train five models to each metric listed above.
Thecollection of models trained to a given metric is then evaluated using the other metrics.
We report the resultingstandard devation for the collection on each of the metrics.
The collection with the lowest varience is bolded.Model Pair % Preferred p-valueChineseMETR R vs. TERp 60.0 0.0028BLEU:4 vs. TERp 57.5 0.02NIST vs. TERp 55.0 0.089NIST vs. TERpA 55.0 0.089BLEU:4 vs. TERpA 54.5 0.11BLEU:4 vs. METR R 54.5 0.11METR:0.5 vs. METR 54.5 0.11METR:0.5 vs. METR R 53.0 0.22METR vs. BLEU:4 52.5 0.26BLEU:4 vs. METR:0.5 52.5 0.26METR vs. TERp 52.0 0.31NIST vs. BLEU:4 52.0 0.31BLEU:4 vs. METR R:0.5 51.5 0.36WER vs. TERp 51.5 0.36TERpA vs. TERp 50.5 0.47ArabicBLEU:4 vs. METR R 62.0 < 0.001NIST vs. TERp 56.0 0.052BLEU:4 vs. METR:0.5 55.5 0.069BLEU:4 vs. METR 54.5 0.11METR R:0.5 vs METR R 54.0 0.14NIST vs. BLEU:4 51.5 0.36WER vs. TERp 51.5 0.36METR:0.5 vs METR 51.5 0.36TERp vs. BLEU:4 51.0 0.42BLEU:4 vs. METR R:0.5 50.5 0.47Table 5: Select pairwise preference for models trained todifferent evaluation metrics.
For A vs. B, preferred indi-cates how often A was preferred to B.
We bold the bettertraining metric for statistically significant differences.duced by experts by having multiple workers com-plete each HIT and then combining their answers(Snow et al, 2008; Callison-Burch, 2009).We perform a pairwise comparison of the trans-lations produced for the first 200 sentences of ourChinese to English test data (MT03) and our Arabicto English test data (dev07).
The HITs consist of apair of machine translated sentences and a single hu-man generated reference translation.
The referenceis chosen at random from those available for eachsentence.
Capitalization of the translated sentencesis restored using an HMM based truecaser (Lita etal., 2003).
Turkers are instructed to ?.
.
.
select themachine translation generated sentence that is eas-iest to read and best conveys what is stated in thereference?.
Differences between the two machinetranslations are emphasized by being underlined andbold faced.9 The resulting HITs are made availableonly to workers in the United States, as pilot experi-ments indicated this results in more consistent pref-erence judgments.
Three preference judgments areobtained for each pair of translations and are com-bined using weighted majority vote.As shown in table 5, in many cases the quality ofthe translations produced by models trained to dif-ferent metrics is remarkably similar.
Training to thesimpler edit distance metric WER produces transla-tions that are as good as those from models tuned tothe similar but more advanced TERp metric that al-lows for swaps.
Similarly, training to TERpA, whichmakes use of both a paraphrase table and edit costs9We emphasize relative differences between the two trans-lations rather than the difference between each translation andthe reference in order to avoid biasing evaluations toward editdistance metrics.561tuned to human judgments, is no better than TERp.For the Chinese to English results, there is a sta-tistically significant human preference for transla-tions that are produced by training to BLEU:4 anda marginally significant preferences for training toNIST over the default configuration of TERp.
Thiscontrasts sharply with earlier work showing thatTER and TERp correlate better with human judge-ments than BLEU (Snover et al, 2009; Przybockiet al, 2008; Snover et al, 2006).
While it is as-sumed that, by using MERT, ?improved evaluationmeasures lead directly to improved machine trans-lation quality?
(Och, 2003), these results show im-proved correlations with human judgments are notalways sufficient to establish that tuning to a metricwill result in higher quality translations.
In the Ara-bic results, we see a similar pattern where NIST ispreferred to TERp, again with marginal signficance.Strangely, however, there is no real difference be-tween TERp vs. BLEU:4.For Arabic, training to rankingMETEOR is worsethan BLEU:4, with the differences being very sig-nificant.
The Arabic results also trend toward sug-gesting that BLEU:4 is better than either standardMETEOR and METEOR ?
0.5.
However, for theChinese models, training to standard METEOR andMETEOR ?
0.5 is about as good as training toBLEU:4.
In both the Chinese and Arabic results, theMETEOR ?
0.5 models are at least as good as thosetrained to standard METEOR and METEOR rank-ing.
In contrast to the cross evaluation metric results,where the differences between the ?
0.5 models andthe standard METEOR models were always fairlydramatic, the human preferences suggest there is of-ten not much of a difference in the true quality of thetranslations produced by these models.7 ConclusionTraining to different evaluation metrics follows theexpected pattern whereby models perform best onthe same type of metric used to train them.
How-ever, models trained using the n-gram based metrics,BLEU and NIST, are more robust to being evaluatedusing the other metrics.Edit distance models tend to do poorly when eval-uated on other metrics, as do models trained usingMETEOR.
However, training models to METEORcan be made more robust by setting ?
to 0.5, whichbalances the importance the metric assigns to preci-sion and recall.The fact that the WER, TER and TERp modelsperform very similarly suggests that current phrase-based translation systems lack either the features orthe model structure to take advantage of swap editoperations.
The situation might be improved by us-ing a model that does a better job of both captur-ing the structure of the source and target sentencesand their allowable reorderings, such as a syntac-tic tree-to-string system that uses contextually richrewrite rules (Galley et al, 2006), or by making useof larger more fine grained feature sets (Chiang etal., 2009) that allow for better discrimination be-tween hypotheses.Human results indicate that edit distance trainedmodels such as WER and TERp tend to pro-duce lower quality translations than BLEU or NISTtrained models.
Tuning to METEOR works reason-ably well for Chinese, but is not a good choice forArabic.
We suspect that the newer RYPT metric(Zaidan and Callison-Burch, 2009), which directlymakes use of human adequacy judgements of sub-strings, would obtain better human results than theautomated metrics presented here.
However, likeother metrics, we expect performance gains still willbe sensitive to how the mechanics of the metric inter-act with the structure and feature set of the decodingmodel being used.BLEU and NIST?s strong showing in both the ma-chine and human evaluation results indicates thatthey are still the best general choice for trainingmodel parameters.
We emphasize that improvedmetric correlations with human judgments do notimply that models trained to a metric will result inhigher quality translations.
We hope future workon developing new evaluation metrics will explicitlyexplore the translation quality of models trained tothem.AcknowledgementsThe authors thank Alon Lavie for suggesting set-ting ?
to 0.5 when training to METEOR.
This workwas supported by the Defense Advanced ResearchProjects Agency through IBM.
The content doesnot necessarily reflect the views of the U.S. Gov-ernment, and no official endorsement should be in-ferred.562ReferencesAbhaya Agarwal and Alon Lavie.
2008.
METEOR,M-BLEU and M-TER: Evaluation metrics for high-correlation with human rankings of machine transla-tion output.
In StatMT workshop at ACL.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of BLEU in ma-chine translation research.
In EACL.Chris Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using Amazon?s Me-chanical Turk.
In EMNLP.Daniel Cer, Michel Galley, Christopher D. Manning, andDan Jurafsky.
2010.
Phrasal: A statistical machinetranslation toolkit for exploring new model features.In NAACL.Pi-Chuan Chang, Michel Galley, and Christopher D.Manning.
2008.
Optimizing chinese word segmen-tation for machine translation performance.
In StatMTworkshop at ACL.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In EMNLP.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine translation.In NAACL.Koby Crammer and Yoram Singer.
2003.
Ultraconserva-tive online algorithms for multiclass problems.
JMLR,3:951?991.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In HLT.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In EMNLP.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In ACL.Michel Galley, Spence Green, Daniel Cer, Pi-ChuanChang, and Christopher D. Manning.
2009.
Stanforduniversity?s arabic-to-english statistical machine trans-lation system for the 2009 NIST evaluation.
In NISTOpen Machine Translation Evaluation Meeting.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In NAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL.Alon Lavie and Michael J. Denkowski.
2009.
TheMETEOR metric for automatic evaluation of machinetranslation.
Machine Translation, 23.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In NAACL.Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, andNanda Kambhatla.
2003. tRuEcasIng.
In ACL.Bonnie Dorr Matthew Snover, Nitin Madnani andRichard Schwartz.
2008.
TERp system description.In MetricsMATR workshop at AMTA.Sonja Nie?en, Franz Josef Och, and Hermann Ney.
2000.An evaluation tool for machine translation: Fast eval-uation for MT research.
In LREC.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In ACL.M.
Przybocki, K. Peterson, and S. Bronsart.
2008.Official results of the ?Metrics for MAchine TRans-lation?
Challenge (MetricsMATR08).
Techni-cal report, NIST, http://nist.gov/speech/tests/metricsmatr/2008/results/.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In AMTA.Matthew Snover, Nitin Madnani, Bonnie J. Dorr, andRichard Schwartz.
2009.
Fluency, adequacy, orHTER?
: exploring different human judgments with atunable MT metric.
In StatMT workshop at EACL).Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Ng.
2008.
Cheap and fast ?
but is it good?
Eval-uating non-expert annotations for natural languagetasks.
In EMNLP.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In ICSLP.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher D. Manning.
2005.
A con-ditional random field word segmenter.
In SIGHAN.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online large-margin training for statis-tical machine translation.
In EMNLP-CoNLL.Omar F. Zaidan and Chris Callison-Burch.
2009.
Feasi-bility of human-in-the-loop minimum error rate train-ing.
In EMNLP, pages 52?61, August.563
