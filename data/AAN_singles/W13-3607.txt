Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 52?61,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsConstrained grammatical error correction usingStatistical Machine TranslationZheng YuanComputer LaboratoryUniversity of CambridgeUnited Kingdomzy249@cam.ac.ukMariano FeliceComputer LaboratoryUniversity of CambridgeUnited Kingdommf501@cam.ac.ukAbstractThis paper describes our use of phrase-based statistical machine translation (PB-SMT) for the automatic correction of er-rors in learner text in our submission tothe CoNLL 2013 Shared Task on Gram-matical Error Correction.
Since the lim-ited training data provided for the taskwas insufficient for training an effectiveSMT system, we also explored alternativeways of generating pairs of incorrect andcorrect sentences automatically from otherexisting learner corpora.
Our approachdoes not yield particularly high perfor-mance but reveals many problems that re-quire careful attention when building SMTsystems for error correction.1 IntroductionMost approaches to error correction for non-nativetext are based on machine learning classifiers forspecific error types (Leacock et al 2010; Daleet al 2012).
Thus, for correcting determineror preposition errors, for example, a multiclassmodel is built that uses a set of features from thelocal context around the target and predicts the ex-pected article or preposition.
If the output of theclassifier is the same as the original sentence, thesentence is not corrected.
Otherwise, a correctionis made based on the predicted class.
This is thede facto approach to error correction and is widelyadopted in previous work.Building effective classifiers requires identifica-tion of features types from the text that discrimi-nate well correcting each specific error type, suchas part-of-speech tags of neighbouring words, n-gram statistics, etc., which in turn require addi-tional linguistic resources.
Classifiers designed tocorrect only one type of error do not perform wellon nested or sequential errors.
Correcting morethan one type of error requires building and com-bining multiple classifiers.
These factors makethe solution highly dependent on engineering de-cisions (e.g.
as regards features and algorithms)as well as complex and laborious to extend to newtypes.An attractive and simpler alternative is to thinkof error correction as a translation task.
The un-derlying idea is that a statistical machine transla-tion (SMT) system should be able to translate textwritten in ?bad?
(incorrect) English into ?good?
(correct) English.
An advantage of using this ap-proach is that there is no need for an explicit en-coding of the contexts that surround each error (i.e.features) since SMT systems learn contextually-appropriate source-target mappings from the train-ing data.
Likewise, they do not require any specialmodification for correcting multiple error types se-quentially, since they generate an overall correctedversion of the sentence fixing as much as possiblefrom what they have learnt.
Provided the system istrained using a sufficiently large parallel corpus ofincorrect-to-correct sentences, the model shouldhandle all the observed errors without any furtherexplicit information like previously detected errortypes, context or error boundaries, and so forth.The increasing performance of state-of-the-artSMT systems also suggests they could prove suc-cessful for other applications, such as error cor-rection.
In fact, SMT systems have been success-fully used in a few such experiments, as we re-port below.
The work presented here builds uponthese initial experiments and explores the factorsthat may affect the performance of such systems.The remainder of this paper is organised as fol-lows: Section 2 gives a summary of previous re-search using SMT for error correction, Section 3describes our approach and resources, and Sec-tion 4 reports our experiments and results.
Sec-tion 5 discusses a number of issues related to theperformance of our system and reports some at-52tempts at improving it while Section 6 includesour official performance in the shared task.
Fi-nally, Section 7 provides conclusions and ideas forfuture work.2 Related WorkBrockett et al(2006) describe the use of anSMT system for correcting a set of 14 count-able/uncountable nouns which are often confus-ing for learners of English as a second language.Their training data consists of a large corpus ofsentences extracted from news articles which weredeliberately modified to include typical countabil-ity errors involving the target words as observedin a Chinese learner corpus.
Artificial errors areintroduced in a deterministic manner using hand-coded rules including operations such as chang-ing quantifiers (much?
many), generating plurals(advice?
advices) or inserting unnecessary deter-miners.
Experiments show their SMT system wasgenerally able to beat the standard Microsoft Word2003 grammar checker, although it produced a rel-atively higher rate of erroneous corrections.Similar experiments were carried out by Mizu-moto et al(2011) for correcting Japanese as asecond language.
However, their training corpuscomprised authentic learner sentences togetherwith corrections made by native speakers on a so-cial learning network website.
Because the origi-nal data has no explicit annotation of error types,the resulting SMT system is not type-constrained.Their results show that the approach is a viableway of obtaining very high performance at a rela-tively low cost provided a large amount of train-ing data is available.
These claims were latersupported by similar experiments using Englishtexts written by Japanese students (Mizumoto etal., 2012)Ehsan and Faili (2013) trained SMT systemsfor correcting grammatical errors and context-sensitive spelling mistakes in English and Farsi.Datasets are obtained by injecting artificial errorsinto well-formed treebank sentences using prede-fined error templates.
Whenever an original sen-tence from the corpus matches one of these tem-plates, a pair of correct and incorrect sentencesis generated.
This process is repeated multipletimes if a single sentence matches more than oneerror template, thereby generating many pairs forthe same original sentence.
A comparison be-tween the proposed systems and rule-based gram-mar checkers show they are complementary, witha hybrid system achieving the best performance.Other approaches using machine translation forerror correction are not aimed at training SMT sys-tems but rather at using them as auxiliary tools forproducing round-trip translations (i.e.
translationsinto a pivot foreign language and back into En-glish) which are used for subsequent post-editingof the original sentence (Hermet and De?silets,2009; Madnani et al 2012).
This differs fromour work in that we focus on training and adapt-ing SMT systems to make all the targeted correc-tions sequentially rather than using them as ?blackboxes?
on top of which other systems are built.3 MethodWe approach error correction as a translation taskfrom incorrect into correct English.
Several SMTsystems are built using different training dataand the best one is selected for further refine-ment.
Given the CoNLL-2013 shared task spec-ification, systems are required to correct five spe-cific error types involving articles and determin-ers (ArtOrDet), noun number (Nn), prepositions(Prep), subject-verb agreement (SVA) and verbforms (Vform) and must ignore other errors in or-der to achieve a good score.3.1 DataThe training data provided for the task is a sub-set of the NUCLE v2.3 corpus (Dahlmeier et al2013), which comprises essays written in Englishby students at the National University of Singa-pore.
The original corpus contains around 1,400essays, which amount to 1,220,257 tokens, butsince a portion of this data (25 essays of about 500words each) was included in the test set, we es-timate the remaining 1,375 essays in the trainingset contain around 1,207,757 tokens.
All the sen-tences were manually annotated by human expertsusing a set of 27 error types, although we used afiltered version containing only the five types se-lected for the shared task.Because the size of the supplied training datais too small to train an effective SMT system, weused additional data from the Cambridge LearnerCorpus1 (CLC).
In particular, we derived newpairs of incorrect and correct sentences using the1http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom/item3646603/Cambridge-International-Corpus-Cambridge-Learner-Corpus/53publicly available scripts from the First Certificatein English (FCE) (Yannakoudakis et al 2011) andothers from the International English LanguageTesting System (IELTS) examinations, which in-clude mainly academic writing.
These corporainclude about 16,068 sentences (532,033 tokens)and 64,628 sentences (1,361,841 tokens) respec-tively.
Given that the error annotation scheme usedin the CLC is more detailed than the one used inNUCLE, a mapping had to be defined so that wecould produce corrections only for the five targeterror types (Table 1).3.2 Generating Artificial ErrorsFollowing previous approaches, we decided to in-crease the size of our training set by introducingnew sentences containing artificial errors.
Thishas many potential advantages.
First, it is an eco-nomic and efficient way of generating error-taggeddata, which otherwise requires manual annotationand is difficult to obtain.
Second, it allows us tointroduce only the types of errors we want, thusgiving us the ability to imitate the original NU-CLE data and circumvent annotation incompati-bility.
Finally, we can choose our initial sentencesso that they match specific requirements, such astopic, length, linguistic phenomena, etc.Again, we use a publicly available portion of theCLC formed by all the corrected samples featuredon the English Vocabulary Profile2 (EVP) website.These sentences come from a variety of examina-tions at different levels and amount to 18,830 sen-tences and approximately 351,517 tokens.In order to replicate NUCLE errors in EVP sen-tences as accurately as possible, we applied thefollowing procedure:1.
We extract all the possible correction pat-terns from the NUCLE v2.3 gold standardand rewrite them as correct-fragment ?incorrect-fragment.
Two types of patterns areextracted, one in terms of lexical items (i.e.surface forms/words) and another using part-of-speech (PoS) tags.
Table 2 shows somesample patterns.2.
For each correct sentence in the EVP (target),we generate a pseudo-source sentence by ap-plying zero or more of extracted rules.2http://www.englishprofile.org/index.php?option=com_content&view=article&id=4&Itemid=5Figure 1: An example of the artificial error injec-tion process.Our approach is very naive and assumes allerror-injection rules have equal probability.The injection of errors is incremental andnon-overlapping.
Figure 1 illustrates this pro-cedure.3.
Lexical patterns take precedence over PoSpatterns.
However, because the application ofa rule is decided randomly, a sentence mightend up being distorted by both types of pat-terns, only one, or none at all (i.e.
no er-rors are introduced).
In the last case, boththe source and target sentences contain cor-rect versions.4.
A parallel corpus is built using the error-injected sentences on the source side andtheir original (correct) versions on the targetside.As we explain in Section 4, this corpus is com-bined with other training data in order to build dif-ferent SMT systems.3.3 ToolsAll our systems were built using the Moses SMTsystem (Koehn et al 2007), together with Giza++(Och and Ney, 2003) for word alignment and theIRSTLM Toolkit (Federico et al 2008) for lan-guage modelling.
For training factored models(Koehn, 2010, Chapter 10) which use PoS infor-mation, we use RASP?s PoS tagger (Briscoe etal., 2006).
Sentence segmentation, tokenisationand PoS tagging for artificial error generation werecarried out using NLTK (Bird et al 2009).54NUCLE v2.3 CLCError Category Tag Error Category TagArticle or determiner ArtOrDetIncorrect determiner inflection DIDeterminer agreement error AGDWrong determiner because of noun countability CDDerivation of determiner error DDIncorrect determiner form FDMissing determiner MDReplace determiner RDUnnecessary determiner UDNoun number NnCountability of noun error CNWrong noun form FNIncorrect noun inflection INNoun agreement error AGNPreposition PrepDerivation of preposition error DTWrong preposition form FTMissing preposition MTReplace preposition RTUnnecessary preposition UTSubject-verb agreement SVAVerb agreement error AGVDeterminer agreement error AGDVerb form VformWrong verb form FVIncorrect verb inflection IVDerivation of verb error DVIncorrect tense of verb TVMissing verb MVTable 1: Mapping of error tags between NUCLE v2.3 and the CLC.Lexical PoSPattern Example Pattern Examplehas?
have temperature has risen?temperature have risenNN?
NNS information?informationsto be used?
to be use technology to be used?technology to be useDT NNP?
NNP the US?
USduring?
for during the early 60s?for the early 60sNN VBZ VBN?
NN VBP VBN expenditure is reduced ?expenditure are reducedTable 2: Sample error injection patterns extracted from the NUCLE v2.3 corpus.4 Experiments and ResultsWe first built a baseline SMT system using onlythe NUCLE v2.3 corpus and compared it to othersystems trained on incremental additions of the re-maining corpora.
All our systems were trained us-ing 4-fold cross-validation where the training setfor each run always included the full FCE, IELTSand EVP corpora but only 3/4 of the NUCLE data,leaving the remaining fourth chunk for testing.This training method allowed us to concentrate onhow the system performed on NUCLE data.Performance was evaluated in terms of preci-sion, recall and F1 as computed by the M2 Scorer(Dahlmeier and Ng, 2012), with the maximumnumber of unchanged words per edit set to 3 (aninitial suggestion by the shared task organiserswhich was eventually changed for the official eval-uation).
The average performance of each systemis reported in Table 3.In general, results show that precision tends todrop as we add more training data whereas recalland F1 slightly increase.
This suggests that ouradditional corpora do not resemble NUCLE verymuch, although they allow the system to correctsome further errors.
Contrary to our expectations,the biggest difference between precision and re-call is observed when we add the EVP-deriveddata, which was deliberately engineered to repli-cate NUCLE errors.
Although it has been reportedthat artificial errors often cause drops in perfor-mance (Sjo?bergh and Knutsson, 2005; Foster andAndersen, 2009), in our case this may also be dueto differences in form (e.g.
sentence length, gram-matical structures covered, error coding) and con-tent (i.e.
topics) between our source (EVP) andtarget (NUCLE) corpora as well as poor controlover the artificial error generation process.
In fact,our method does not explicitly consider error con-texts, error type distribution or other factors that55Model P R F1 ?NUCLE 0.1505 0.1530 0.1517 0.0201NUCLE+FCE 0.1547 0.1518 0.1532 0.0216NUCLE+FCE+IELTS 0.1217 0.2068 0.1532 0.0151NUCLE+FCE+IELTS+EVP 0.1187 0.2183 0.1538 0.0206Table 3: Performance of our lexical SMT models.The best results are marked in bold.
Standard devi-ation (?)
indicates how stable/homogeneous eachdataset is (lower values are better).certainly have an impact on the quality of the gen-erated sentences and may introduce noise if notcontrolled.
Nevertheless, the system trained on allfour corpora yields the best F1 performance.We also tested factored models which includePoS information.
Results are shown in Table 4.The same behaviour is observed for the metrics,although values for precision are now generallyhigher while values for recall are lower.
Again,the best system in terms of F1 is the one trained onall our corpora, slightly outperforming our previ-ous best system.5 Error Analysis and FurtherImprovementsWhen building error correction systems, minimis-ing the number of cases where correct languageis flagged as incorrect is often regarded as moreimportant than covering a large number of errors.Technically, this means high precision is often pre-ferred over high recall, especially when it is diffi-cult to achieve both (as is the case for our systems).A closer observation of the training data, transla-tion tables and system output reveals a series ofissues that are affecting performance, which aresummarised below.In order to test some solutions to these prob-lems, we used our best system as a baseline andretrained it to include each proposed modificationindividually.
Results are included in Table 5 andreferenced accordingly.5.1 Size of training corpusWith slightly over a million tokens, the NUCLEcorpus seems too small to train an efficient SMTsystem.
However, the additional data we were ableto use differs from the NUCLE corpus in terms oflearner-level, native language, and the tasks beingattempted.Model P R F1 ?NUCLE 0.1989 0.1013 0.1342 0.0165NUCLE+FCE 0.2248 0.0933 0.1319 0.0151NUCLE+FCE+IELTS 0.1706 0.1392 0.1533 0.0163NUCLE+FCE+IELTS+EVP 0.1696 0.1480 0.1581 0.0148Table 4: Performance of our PoS factoredSMT models.
The best results are marked inbold.
Standard deviation (?)
indicates how sta-ble/homogeneous each dataset is (lower values arebetter).5.2 Word reorderingIn many cases, our system made corrections by re-ordering words.
Since the five error types in theshared task rarely implied reordering, this causedunnecessary edits that harmed precision, as in thefollowing example.Original sentenceHigh Temperture Behaviour Of Candidate...System hypothesisHigh Behaviour Of Temperture Candidate...Gold standardHigh Temperture Behaviour Of Candidate...(unchanged)Disabling word reordering in our system helpedto avoid this problem and increased precisionwithout harming recall (Table 5 #1).5.3 Limited translation modelBecause of the relatively small size of our train-ing corpus, the resulting phrase tables used by ourSMT systems contain very general alignments (i.e.corrections) with high probability, which are oftenapplied in inappropriate contexts and result in alarge number of miscorrections.In order to minimise this effect, we forced ourSMT system to output the alignments that wereused for correcting each sentence in our devel-opment sets and deleted from the phrase tablethose which consistently caused deviations fromthe gold standard.
This was done by manuallycomparing our systems?
hypotheses to their gold-standard versions and identifying common pat-terns in the alignments that led to miscorrections,such as to?
to the, have?
have a, people?
peo-ple to, etc.
1,120 out of the total 11,421,886 align-ments in the original translation table were re-moved (?0,01%).
Removing such alignments re-56sults in higher precision but lower recall, as shownin Table 5 #2.We also observed that the system was bi-ased towards making unnecessary insertions ofthe definite article before some specific nouns.This means that the system would almost alwayschange words like cost, elderly or government forthe cost, the elderly or the government, regardlessof whether this fits the context or not.
We believethis is due to the lack of sufficient training sam-ples where these words remained unaltered on thesource and target side, so we decided to augmentthe NUCLE corpus by adding a copy of all thecorrected versions of the sentences on both sides.Then, the system should learn that these words canalso remain unchanged in corrections.
Table 5 #3shows this improves precision but harms recall.Out-of-vocabulary words (i.e.
words not seenduring training) are a also common problem inSMT systems, and this is directly related to theamount of data available for training.
In our sys-tems, all out-of-vocabulary words were directlytransferred from source to target.
That is, when-ever our system encounters a word it has not seenpreviously, it keeps it unchanged.
Because of theway our SMT system works, there is no explicitgeneration of verb or noun forms so unless the sys-tem has learnt this from appropriate contexts (forexample, that a progressive tense is consistentlybeing used after a preposition), it is unable to makesuch corrections.5.4 Inability to distinguish betweenprepositionsWe also observed that our systems did not oftencorrect prepositions.
We believed this was due tothe PoS language model using the same tag forall prepositions and therefore being unable to dis-tinguish when each preposition must be used.
Infact, when using an ordinary PoS language model,the original PoS patterns match those of the ex-pected corrections (i.e.
the expected correction hasa preposition and the hypothesis has one too) so nochange is proposed.
The following example illus-trates this problem.Original sentence... the need toward energy ...DT NN PREP NNSystem hypothesis... the need toward energy ...DT NN PREP NN(unchanged)Expected output (not in gold standard)... the need for energy ...DT NN PREP NNHowever, when the PoS language model ismodified to use preposition-specific tags, the dif-ference between the original sentence and the ex-pected output should be detected and fixed by thesystem, as shown below.Original sentence... the need toward energy ...DT NN PREP TOWARD NNSystem hypothesis... the need for energy ...DT NN PREP FOR NN(unchanged)Expected output (not in gold standard)... the need for energy ...DT NN PREP FOR NNWe expected this change to improve system per-formance.
Although it increased recall, it loweredprecision (Table 5 #4).5.5 Unnecessary editsIn many cases, our system makes good correctionswhich are not considered to belong to any of thetarget error types, as illustrated in the followingexample.Original sentenceThus, we should not compare now with the pastbut we need to worried about the future prob-lems that caused by this situation.System hypothesisThus, we should not compare now with the pastbut we need to worry about the future problemsthat are caused by this situation.Gold standardThus, we should not compare now with the pastbut we need to worry about the future problemsthat caused by this situation.We believe this can be traced to two maincauses.
First, there is no clear-cut definition ofeach error type, so it is not possible to know theannotation criteria or scope of each error type.Therefore, inferring this information from the an-notated examples may result in poor error map-ping between the CLC and NUCLE, making thesystem learn corrections that are not part of our57target set and miss others which are actually use-ful.
For example, it is not clear if ?verb form?
er-rors (Vform) include change of tense or the addi-tion of missing verbs.
Second, because SMT sys-tems learn from all parts of a parallel corpus andmaximise fluency using a general language model,it is hard to limit the corrections to a predefinedset of error types.
Using a larger language modelbased on the corrected version of the CLC con-firms this: precision drops while recall improves(Table 5 #5).5.6 Gold-standard annotationThe original NUCLE corpus contains correctionsfor 27 error types.
However, the version usedfor the shared task only includes 5 error typesand discards all the remaining corrections.
Be-cause nested and context-dependent errors arevery frequent, the systematic removal of annota-tions which do not belong to these five types oftengenerates mutilated or partly-corrected sentences,a deficiency that has also been reported in othershared tasks (Kochmar et al 2012).
Here is a typ-ical example.Original sentenceThese approaches may not produce effect soon,but it is sustainable for the future generation.Corrected sentenceThese approaches may not produce [immediateeffects]Wci, but [they]Pref [are]SVA [useful]Wcifor the future [generations]Nn.Type-constrained sentenceThese approaches may not produce effectsoon, but it [are]SVA sustainable for the future[generations]Nn.These ill-formed sentences are particularlyharmful for SMT systems which, unlike classi-fiers, work at a global rather than local level.
Asa result, many corrections proposed by our sys-tem are considered incorrect because they do notmatch the gold-standard version, as shown below.Original sentenceAlthough it is essential for all the fields, ...System hypothesisAlthough it is essential for all the fields, ...(unchanged)# System settings P R F10 NUCLE+FCE+IELTS+EVP 0.1696 0.1480 0.15811 Disabled reordering 0.1702 0.1480 0.15832 Removal of incorrect alignments 0.1861 0.1399 0.15983 Double NUCLE data 0.1792 0.1229 0.14584 Detailed Prep PoS tags 0.1632 0.1504 0.15655 Bigger LMs 0.1532 0.1676 0.16016 Final system (0+1+2+3+5) 0.1844 0.1375 0.1575Table 5: Performance of the baseline system plusdifferent individual settings.
Bold values indicatean improvement over the original baseline system.Gold standardAlthough it [are]SVA essential for all the fields,...This raises the question of how to design an ef-fective and challenging shared task.5.7 Scoring criteriaThe official evaluation using the M2 scorer is sen-sitive to capitalisation and white space, althoughthese error types were not part of the task.
Boththis fact and the lack of alternative corrections foreach gold-standard edit leave out many other validcorrections, which in turn means true system per-formance is underestimated.5.8 Other factorsDifferences between the training and test data canalso affect performance, such as changes in thewriters?
native language, their level of languageproficiency or the topic of their compositions.The final system submitted to the shared taskis a combination of our best factored model (i.e.baseline) plus a selection of improvements (Ta-ble 5 #6).6 Official Evaluation ResultsSystems were evaluated using a set of 50 essayscontaining about 500 words each (?25,000 wordsin total) which were written in response to two dif-ferent prompts.
One of these prompts had beenused for a subset of the training data while theother was new.
No error annotations were initiallyavailable for this set.
As we mentioned above,the M2 scorer was set to be sensitive to capitalisa-tion and white space as well as limit the maximumnumber of unchanged tokens per edit to 2.Initially, each participating team received theirofficial system results individually.
After the gold-standard annotations of the test set were released,58Evaluation round Corr.editsProp.editsGoldeditsP R F1First (pre-revision) 166 424 1643 0.3915 0.1010 0.1606Second (post-revision) 222 426 1565 0.5211 0.1419 0.2230Table 6: Official results of our system before andafter revision of the test set annotations.
The num-ber of correct, proposed and gold edits are also in-cluded for comparison.many participants raised concerns about their ac-curacy so they were given the opportunity to sub-mit alternative annotations.
These suggestionswere manually revised by a human annotator andmerged into a new test set which was used to re-score all the submitted systems in a second officialevaluation round.
Evaluation results of our sys-tem in both rounds (before and after revision ofthe test set annotations) are included in Table 6.Although this measure helped overcome some ofthe problems described in Section 5.6, other prob-lems such as whitespace and case sensitivity werenot addressed.In both evaluation rounds, our system scoresthird in terms of precision, which is particularlyencouraging for error correction environmentswhere precision is preferred over recall.
How-ever, these values should be considerably higherin order to prove useful in applications like self-assessment and tutoring systems (Andersen et al2013).Results also reveal precision on the test set isconsiderably higher than in our cross-validationexperiments.
This may be partly a result of thelarger amount of training data in our final systemand/or greater grammatical or thematic similaritybetween the test and training sets.Table 7 shows the distribution of system editsby error type.
The results suggest that lexical het-erogeneity in the contexts surrounding errors is afactor in performance, which might be improvedthrough larger training sets.7 Conclusions and Future WorkIn this paper we have described the use of SMTtechniques for building an error correction system.We trained lexical and factored phrase-based sys-tems using incremental combinations of trainingdata and observed that, in general, recall increasesat the expense of precision.
However, this mightbe due to structural and thematic differences in thecorpora we used.
We also tried a relatively sim-ple mechanism for injecting artificial errors intoError Type Pre-revision Post-revisionCorr.
Missed Unnec.
Corr.
Missed Unnec.ArtOrDet 104 586 161 134 548 132Nn 30 366 25 38 362 20Prep 11 301 18 13 246 15SVA 7 116 0 8 103 0Vform 14 108 41 29 84 25Other 0 0 13 0 0 12TOTAL 166 1477 258 222 1343 204Table 7: Distribution of system edits by errortype for the two official evaluation rounds (beforeand after revision of the test annotations).
?Corr.
?stands for correct edits, ?Missed?
for missed ed-its and ?Unnec.?
for unnecessary edits.
The cate-gory ?Other?
includes changes made by our systemwhich do not belong to any of the other categories.new data, which caused a drop in precision but in-creased recall and F1.Cross-validation experiments show that our sys-tems were unable to achieve particularly high per-formance (with precision, recall and F1 consis-tently below 0.20).
A careful analysis revealedmany factors that affect system performance, suchas annotation criteria, training parameters and cor-pus size and heterogeneity.
Our final system sub-mitted to the CoNLL 2013 shared task was de-signed to circumvent some of these problems andmaximise precision.Plans for future work include more detailed er-ror analysis and the implementation of new solu-tions to avoid drops in performance.
We wouldalso like to test our approach in an unrestrictedscenario (i.e.
using corpora which are not limitedto a fixed number of error types) and use moreflexible evaluation schemes.
We believe furtherstudy of the methods used for generating artificialerrors is also vital to help SMT systems become auseful approach to error correction.AcknowledgementsWe would like to thank Ted Briscoe and EkaterinaKochmar for their valuable comments and sugges-tions.
We are also grateful to ?istein Andersenfrom iLexIR Ltd. for giving us additional feed-back, providing us with corrected data to buildour language models and granting access to pairedsamples of the CLC for training our systems.
Ourgratitude goes also to Cambridge English Lan-guage Assessment, a division of Cambridge As-sessment, for supporting this research.59References?istein E. Andersen, Helen Yannakoudakis, FionaBarker, and Tim Parish.
2013.
Developing and test-ing a self-assessment and tutoring system.
In Pro-ceedings of the Eighth Workshop on Innovative Useof NLP for Building Educational Applications, BEA2013, pages 32?41, Atlanta, GA, USA, June.
Asso-ciation for Computational Linguistics.Steven Bird, Edward Loper, and Ewan Klein.2009.
Natural Language Processing with Python.O?Reilly Media Inc.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Pro-ceedings of the COLING/ACL on Interactive presen-tation sessions, COLING-ACL ?06, pages 77?80,Sydney, Australia.
Association for ComputationalLinguistics.Chris Brockett, William B. Dolan, and Michael Ga-mon.
2006.
Correcting ESL Errors Using PhrasalSMT Techniques.
In Proceedings of the 21st In-ternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics, pages 249?256, Sydney,Australia, July.
Association for Computational Lin-guistics.Daniel Dahlmeier and Hwee Tou Ng.
2012.
Bet-ter evaluation for grammatical error correction.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL 2012, pages 568 ?
572, Montreal, Canada.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a Large Annotated Corpus ofLearner English: The NUS Corpus of Learner En-glish.
In Proceedings of the 8th Workshop on In-novative Use of NLP for Building Educational Ap-plications, BEA 2013, Atlanta, Georgia, USA.
Toappear.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
Hoo 2012: A report on the preposition anddeterminer error correction shared task.
In Pro-ceedings of the Seventh Workshop on Building Ed-ucational Applications Using NLP, pages 54?62,Montre?al, Canada, June.
Association for Computa-tional Linguistics.Nava Ehsan and Heshaam Faili.
2013.
Grammaticaland context-sensitive error correction using a sta-tistical machine translation framework.
Software:Practice and Experience, 43(2):187?206.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: an open source toolkit forhandling large scale language models.
In Proceed-ings of the 9th Annual Conference of the Interna-tional Speech Communication Association, INTER-SPEECH 2008, pages 1618?1621, Brisbane, Aus-tralia, September.
ISCA.Jennifer Foster and ?istein Andersen.
2009.
Gen-errate: Generating errors for use in grammatical er-ror detection.
In Proceedings of the Fourth Work-shop on Innovative Use of NLP for Building Edu-cational Applications, pages 82?90, Boulder, Col-orado, June.
Association for Computational Linguis-tics.Matthieu Hermet and Alain De?silets.
2009.
Using firstand second language models to correct prepositionerrors in second language authoring.
In Proceedingsof the Fourth Workshop on Innovative Use of NLPfor Building Educational Applications, EdAppsNLP?09, pages 64?72, Boulder, Colorado.
Associationfor Computational Linguistics.Ekaterina Kochmar, ?istein Andersen, and TedBriscoe.
2012.
Hoo 2012 error recognition and cor-rection shared task: Cambridge university submis-sion report.
In Proceedings of the Seventh Workshopon Building Educational Applications Using NLP,pages 242?250, Montreal, Canada.
Association forComputational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Prague, Czech Republic.Association for Computational Linguistics.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press, New York, NY, USA,1st edition.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault.
2010.
Automated GrammaticalError Detection for Language Learners.
Morganand Claypool Publishers.Nitin Madnani, Joel Tetreault, and Martin Chodorow.2012.
Exploring grammatical error correction withnot-so-crummy machine translation.
In Proceedingsof the Seventh Workshop on Building EducationalApplications Using NLP, pages 44?53, Montreal,Canada.
Association for Computational Linguistics.Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-gata, and Yuji Matsumoto.
2011.
Mining Re-vision Log of Language Learning SNS for Auto-mated Japanese Error Correction of Second Lan-guage Learners.
In Proceedings of 5th InternationalJoint Conference on Natural Language Processing,pages 147?155, Chiang Mai, Thailand, November.Asian Federation of Natural Language Processing.Tomoya Mizumoto, Yuta Hayashibe, Mamoru Ko-machi, Masaaki Nagata, and Yuji Matsumoto.
2012.The effect of learner corpus size in grammatical er-ror correction of ESL writings.
In Proceedings ofCOLING 2012: Posters, pages 863?872, Mumbai,60India, December.
The COLING 2012 OrganizingCommittee.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Comput.
Linguist., 29(1):19?51, March.Jonas Sjo?bergh and Ola Knutsson.
2005.
Faking er-rors to avoid making errors: Very weakly supervisedlearning for error detection in writing.
In Proceed-ings of RANLP 2005, pages 506?512, Borovets, Bul-garia, September.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading esol texts.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages180?189, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.61
