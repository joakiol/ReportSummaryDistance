Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 632?639,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsConstituent Parsing with Incremental Sigmoid Belief NetworksIvan TitovDepartment of Computer ScienceUniversity of Geneva24, rue Ge?ne?ral DufourCH-1211 Gene`ve 4, Switzerlandivan.titov@cui.unige.chJames HendersonSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, United Kingdomjames.henderson@ed.ac.ukAbstractWe introduce a framework for syntacticparsing with latent variables based on a formof dynamic Sigmoid Belief Networks calledIncremental Sigmoid Belief Networks.
Wedemonstrate that a previous feed-forwardneural network parsing model can be viewedas a coarse approximation to inference withthis class of graphical model.
By construct-ing a more accurate but still tractable ap-proximation, we significantly improve pars-ing accuracy, suggesting that ISBNs providea good idealization for parsing.
This gener-ative model of parsing achieves state-of-the-art results on WSJ text and 8% error reduc-tion over the baseline neural network parser.1 IntroductionLatent variable models have recently been of in-creasing interest in Natural Language Processing,and in parsing in particular (e.g.
(Koo and Collins,2005; Matsuzaki et al, 2005; Riezler et al, 2002)).Latent variables provide a principled way to in-clude features in a probability model without need-ing to have data labeled with those features in ad-vance.
Instead, a labeling with these features canbe induced as part of the training process.
Thedifficulty with latent variable models is that evensmall numbers of latent variables can lead to com-putationally intractable inference (a.k.a.
decoding,parsing).
In this paper we propose a solution tothis problem based on dynamic Sigmoid Belief Net-works (SBNs) (Neal, 1992).
The dynamic SBNswhich we peopose, called Incremental Sigmoid Be-lief Networks (ISBNs) have large numbers of latentvariables, which makes exact inference intractable.However, they can be approximated sufficiently wellto build fast and accurate statistical parsers which in-duce features during training.We use SBNs in a generative history-based modelof constituent structure parsing.
The probability ofan unbounded structure is decomposed into a se-quence of probabilities for individual derivation de-cisions, each decision conditioned on the unboundedhistory of previous decisions.
The most common ap-proach to handling the unbounded nature of the his-tories is to choose a pre-defined set of features whichcan be unambiguously derived from the history (e.g.
(Charniak, 2000; Collins, 1999)).
Decision prob-abilities are then assumed to be independent of allinformation not represented by this finite set of fea-tures.
Another previous approach is to use neuralnetworks to compute a compressed representation ofthe history and condition decisions on this represen-tation (Henderson, 2003; Henderson, 2004).
It ispossible that an unbounded amount of informationis encoded in the compressed representation via itscontinuous values, but it is not clear whether this isactually happening due to the lack of any principledinterpretation for these continuous values.Like the former approach, we assume that thereare a finite set of features which encode the relevantinformation about the parse history.
But unlike thatapproach, we allow feature values to be ambiguous,and represent each feature as a distribution over (bi-nary) values.
In other words, these history featuresare treated as latent variables.
Unfortunately, inter-632preting the history representations as distributionsover discrete values of latent variables makes the ex-act computation of decision probabilities intractable.Exact computation requires marginalizing out the la-tent variables, which involves summing over all pos-sible vectors of discrete values, which is exponentialin the length of the vector.We propose two forms of approximation for dy-namic SBNs, a neural network approximation anda form of mean field approximation (Saul and Jor-dan, 1999).
We first show that the previous neuralnetwork model of (Henderson, 2003) can be viewedas a coarse approximation to inference with ISBNs.We then propose an incremental mean field method,which results in an improved approximation overthe neural network but remains tractable.
The re-sulting parser achieves significantly higher accuracythan the neural network parser (90.0% F-measure vs89.1%).
We argue that this correlation between bet-ter approximation and better accuracy suggests thatdynamic SBNs are a good abstract model for naturallanguage parsing.2 Sigmoid Belief NetworksA belief network, or a Bayesian network, is a di-rected acyclic graph which encodes statistical de-pendencies between variables.
Each variable Si inthe graph has an associated conditional probabilitydistributions P (Si|Par(Si)) over its values giventhe values of its parents Par(Si) in the graph.
ASigmoid Belief Network (Neal, 1992) is a particu-lar type of belief networks with binary variables andconditional probability distributions in the form ofthe logistic sigmoid function:P (Si =1|Par(Si)) =11+exp(?
?Sj?Par(Si) JijSj),where Jij is the weight for the edge from variableSj to variable Si.
In this paper we consider a gen-eralized version of SBNs where we allow variableswith any range of discrete values.
We thus general-ize the logistic sigmoid function to the normalizedexponential (a.k.a.
softmax) function to define theconditional probabilities for non-binary variables.Exact inference with all but very small SBNsis not tractable.
Initially sampling methods wereused (Neal, 1992), but this is also not feasible forlarge networks, especially for the dynamic modelsof the type described in section 2.2.
Variationalmethods have also been proposed for approximat-ing SBNs (Saul and Jordan, 1999).
The main idea ofvariational methods (Jordan et al, 1999) is, roughly,to construct a tractable approximate model with anumber of free parameters.
The free parameters areset so that the resulting approximate model is asclose as possible to the original graphical model fora given inference problem.2.1 Mean Field Approximation MethodsThe simplest example of a variation method is themean field method, originally introduced in statis-tical mechanics and later applied to unsupervisedneural networks in (Hinton et al, 1995).
Let us de-note the set of visible variables in the model (i.e.
theinputs and outputs) by V and hidden variables byH = h1, .
.
.
, hl.
The mean field method uses a fullyfactorized distribution Q as the approximate model:Q(H|V ) =?iQi(hi|V ).where each Qi is the distribution of an individuallatent variable.
The independence between the vari-ables hi in this approximate distribution Q does notimply independence of the free parameters whichdefine the Qi.
These parameters are set to min-imize the Kullback-Leibler divergence (Cover andThomas, 1991) between the approximate distribu-tion Q(H|V ) and the true distribution P (H|V ):KL(Q?P ) =?HQ(H|V ) ln Q(H|V )P (H|V ) , (1)or, equivalently, to maximize the expression:LV =?HQ(H|V ) ln P (H, V )Q(H|V ) .
(2)The expression LV is a lower bound on the log-likelihood ln P (V ).
It is used in the mean fieldtheory (Saul and Jordan, 1999) to approximate thelikelihood.
However, in our case of dynamic graph-ical models, we have to use a different approachwhich allows us to construct an incremental parsingmethod without needing to introduce the additionalparameters proposed in (Saul and Jordan, 1999).We will describe our modification of the mean fieldmethod in section 3.3.6332.2 DynamicsDynamic Bayesian networks are Bayesian networksapplied to arbitrarily long sequences.
A new set ofvariables is instantiated for each position in the se-quence, but the edges and weights for these variablesare the same as in other positions.
The edges whichconnect variables instantiated for different positionsmust be directed forward in the sequence, therebyallowing a temporal interpretation of the sequence.Typically a dynamic Bayesian Network will only in-volve edges between adjacent positions in the se-quence (i.e.
they are Markovian), but in our parsingmodels the pattern of interconnection is determinedby structural locality, rather than sequence locality,as in the neural networks of (Henderson, 2003).Using structural locality to define the graph in adynamic SBN means that the subgraph of edges withdestinations at a given position cannot be determineduntil all the parser decisions for previous positionshave been chosen.
We therefore call these modelsIncremental SBNs, because, at any given positionin the parse, we only know the graph of edges forthat position and previous positions in the parse.
Forexample in figure 1, discussed below, it would notbe possible to draw the portion of the graph after t,because we do not yet know the decision dtk.The incremental specification of model structuremeans that we cannot use an undirected graphicalmodel, such as Conditional Random Fields.
Witha directed dynamic model, all edges connecting theknown portion of the graph to the unknown portionof the graph are directed toward the unknown por-tion.
Also there are no variables in the unknownportion of the graph whose values are known (i.e.
novisible variables), because at each step in a history-based model the decision probability is conditionedonly on the parsing history.
Only visible variablescan result in information being reflected backwardthrough a directed edge, so it is impossible for any-thing in the unknown portion of the graph to affectthe probabilities in the known portion of the graph.Therefore inference can be performed by simply ig-noring the unknown portion of the graph, and thereis no need to sum over all possible structures for theunknown portion of the graph, as would be neces-sary for an undirected graphical model.Figure 1: Illustration of an ISBN.3 The Probabilistic Model of ParsingIn this section we present our framework for syn-tactic parsing with dynamic Sigmoid Belief Net-works.
We first specify the form of SBN we propose,namely ISBNs, and then two methods for approx-imating the inference problems required for pars-ing.
We only consider generative models of pars-ing, since generative probability models are simplerand we are focused on probability estimation, notdecision making.
Although the most accurate pars-ing models (Charniak and Johnson, 2005; Hender-son, 2004; Collins, 2000) are discriminative, all themost accurate discriminative models make use of agenerative model.
More accurate generative modelsshould make the discriminative models which usethem more accurate as well.
Also, there are someapplications, such as language modeling, which re-quire generative models.3.1 The Graphical ModelIn ISBNs, we use a history-based model, which de-composes the probability of the parse as:P (T ) = P (D1, ..., Dm) =?tP (Dt|D1, .
.
.
, Dt?1),where T is the parse tree and D1, .
.
.
, Dm is itsequivalent sequence of parser decisions.
Instead oftreating each Dt as atomic decisions, it is convenientto further split them into a sequence of elementarydecisions Dt = dt1, .
.
.
, dtn:P (Dt|D1, .
.
.
, Dt?1) =?kP (dtk|h(t, k)),where h(t, k) denotes the parsing historyD1, .
.
.
, Dt?1, dt1, .
.
.
, dtk?1.
For example, a634decision to create a new constituent can be dividedin two elementary decisions: deciding to create aconstituent and deciding which label to assign to it.We use a graphical model to define our proposedclass of probability models.
An example graphicalmodel for the computation of P (dtk|h(t, k)) isillustrated in figure 1.The graphical model is organized into vectorsof variables: latent state variable vectors St?
=st?1 , .
.
.
, st?n , representing an intermediate state of theparser at derivation step t?, and decision variablevectors Dt?
= dt?1 , .
.
.
, dt?l , representing a parser de-cision at derivation step t?, where t?
?
t. Variableswhose value are given at the current decision (t, k)are shaded in figure 1, latent and output variables areleft unshaded.As illustrated by the arrows in figure 1, the prob-ability of each state variable st?i depends on all thevariables in a finite set of relevant previous state anddecision vectors, but there are no direct dependen-cies between the different variables in a single statevector.
Which previous state and decision vectorsare connected to the current state vector is deter-mined by a set of structural relations specified bythe parser designer.
For example, we could selectthe most recent state where the same constituent wason the top of the stack, and a decision variable rep-resenting the constituent?s label.
Each such selectedrelation has its own distinct weight matrix for theresulting edges in the graph, but the same weightmatrix is used at each derivation position where therelation is relevant.As indicated in figure 1, the probability of eachelementary decision dt?k depends both on the currentstate vector St?
and on the previously chosen ele-mentary action dt?k?1 from Dt?.
This probability dis-tribution has the form of a normalized exponential:P (dt?k = d|St?, dt?k?1)=?h(t?,k)(d) e?j Wdjst?j?d??h(t?,k)(d?)
e?jWd?jst?j, (3)where ?h(t?,k) is the indicator function of a set ofelementary decisions that may possibly follow theparsing history h(t?, k), and the Wdj are the weights.For our experiments, we replicated the same pat-tern of interconnection between state variables asdescribed in (Henderson, 2003).1 We also used the1In the neural network of (Henderson, 2003), our variablessame left-corner parsing strategy, and the same set ofdecisions, features, and states.
We refer the reader to(Henderson, 2003) for details.Exact computation with this model is nottractable.
Sampling of parse trees from the modelis not feasible, because a generative model defines ajoint model of both a sentence and a tree, thereby re-quiring sampling over the space of sentences.
Gibbssampling (Geman and Geman, 1984) is also impos-sible, because of the huge space of variables andneed to resample after making each new decision inthe sequence.
Thus, we know of no reasonable alter-natives to the use of variational methods.3.2 A Feed-Forward ApproximationThe first model we consider is a strictly incrementalcomputation of a variational approximation, whichwe will call the feed-forward approximation.
It canbe viewed as the simplest form of mean field approx-imation.
As in any mean field approximation, eachof the latent variables is independently distributed.But unlike the general case of mean field approxi-mation, in the feed-forward approximation we onlyallow the parameters of the distributions Qi to de-pend on the distributions of their parents.
This addi-tional constraint increases the potential for a largeKullback-Leibler divergence with the true model,defined in expression (1), but it significantly simpli-fies the computations.The set of hidden variables H in our graphicalmodel consists of all the state vectors St?
, t?
?
t,and the last decision dtk.
All the previously observeddecisions h(t, k) comprise the set of visible vari-ables V .
The approximate fully factorisable distri-bution Q(H|V ) can be written as:Q(H|V ) = qtk(dtk)?t?,i(?t?i)st?i (1 ?
?t?i)1?st?i .where ?t?i is the free parameter which determines thedistribution of state variable i at position t?, namelyits mean, and qtk(dtk) is the free parameter which de-termines the distribution over decisions dtk.Because we are only allowed to use informationabout the distributions of the parent variables tomap to their ?units?, and our dependencies/edges map to their?links?.635compute the free parameters ?t?i , the optimal assign-ment of values to the ?t?i is:?t?i = ?
(?t?i),where ?
denotes the logistic sigmoid function and?t?i is a weighted sum of the parent variables?
means:?t?i =?t???RS(t?)?jJ?(t?,t??
)ij ?t?
?j +?t???RD(t?)?kB?(t?,t??)idt?
?k, (4)where RS(t?)
is the set of previous positions withedges from their state vectors to the state vector at t?,RD(t?)
is the set of previous positions with edgesfrom their decision vectors to the state vector at t?,?
(t?, t??)
is the relevant relation between the positiont??
and the position t?, and J?ij and B?id are weightmatrices.In order to maximize (2), the approximate distri-bution of the next decisions qtk(d) should be set toqtk(d) =?h(t,k) (d) e?j Wdj?tj?d?
?h(t,k) (d?)
e?j Wd?j?tj, (5)as follows from expression (3).
The resulting esti-mate of the tree probability is given by:P (T ) ?
?t,kqtk(dtk).This approximation method replicates exactly thecomputation of the feed-forward neural networkin (Henderson, 2003), where the above means ?t?iare equivalent to the neural network hidden unit acti-vations.
Thus, that neural network probability modelcan be regarded as a simple approximation to thegraphical model introduced in section 3.1.In addition to the drawbacks shared by any meanfield approximation method, this feed-forward ap-proximation cannot capture backward reasoning.By backward (a.k.a.
top-down) reasoning we meanthe need to update the state vector means ?t?i afterobserving a decision dtk, for t?
?
t. The next sectiondiscusses how backward reasoning can be incorpo-rated in the approximate model.3.3 A Mean Field ApproximationThis section proposes a more accurate way to ap-proximate ISBNs with mean field methods, whichwe will call the mean field approximation.
Again,we are interested in finding the distribution Q whichmaximizes the quantity LV in expression (2).
Thedecision distribution qtk(dtk) maximizes LV when ithas the same dependence on the state vector means?tk as in the feed-forward approximation, namely ex-pression (5).
However, as we mentioned above, thefeed-forward computation does not allow us to com-pute the optimal values of state means ?t?i .Optimally, after each new decision dtk, we shouldrecompute all the means ?t?i for all the state vec-tors St?
, t?
?
t. However, this would make themethod intractable, due to the length of derivationsin constituent parsing and the interdependence be-tween these means.
Instead, after making each deci-sion dtk and adding it to the set of visible variables V ,we recompute only means of the current state vectorSt.The denominator of the normalized exponentialfunction in (3) does not allow us to compute LV ex-actly.
Instead, we use a simple first order approxi-mation:EQ[ln?d?h(t,k) (d) exp(?jWdjstj)]?
ln?d?h(t,k)(d) exp(?jWdj?tj), (6)where the expectation EQ[.
.
.]
is taken over the statevector St distributed according to the approximatedistribution Q.Unfortunately, even with this assumption there isno analytic way to maximize LV with respect to themeans ?tk, so we need to use numerical methods.Assuming (6), we can rewrite the expression (2) asfollows, substituting the true P (H, V ) defined bythe graphical model and the approximate distribu-tion Q(H|V ), omitting parts independent of ?tk:Lt,kV =?i?
?ti ln ?ti ?
(1 ?
?ti) ln(1 ?
?ti)+?ti?ti +?k?<k?h(t,k?)(dtk?)?jWdtk?j?tj??k?<kln???d?h(t,k?
)(d) exp(?jWdj?tj)?
?, (7)here, ?ti is computed from the previous relevant statemeans and decisions as in (4).
This expression is636concave with respect to the parameters ?ti, so theglobal maximum can be found.
We use coordinate-wise ascent, where each ?ti is selected by an efficientline search (Press et al, 1996), while keeping other?ti?
fixed.3.4 Parameter EstimationWe train these models to maximize the fit of theapproximate model to the data.
We use gradientdescent and a maximum likelihood objective func-tion.
This requires computation of the gradient ofthe approximate log-likelihood with respect to themodel parameters.
In order to compute these deriva-tives, the error should be propagated all the wayback through the structure of the graphical model.For the feed-forward approximation, computation ofthe derivatives is straightforward, as in neural net-works.
But for the mean field approximation, it re-quires computation of the derivatives of the means?ti with respect to the other parameters in expres-sion (7).
The use of a numerical search in the meanfield approximation makes the analytical computa-tion of these derivatives impossible, so a differentmethod needs to be used to compute their values.
Ifmaximization of Lt,kV is done until convergence, thenthe derivatives of Lt,kV with respect to ?ti are close tozero:F t,ki =?Lt,kV??ti?
0 for all i.This system of equations allows us to use implicitdifferentiation to compute the needed derivatives.4 Experimental EvaluationIn this section we evaluate the two approximationsto dynamic SBNs discussed in the previous section,the feed-forward method equivalent to the neuralnetwork of (Henderson, 2003) (NN method) and themean field method (MF method).
The hypothesiswe wish to test is that the more accurate approxima-tion of dynamic SBNs will result in a more accuratemodel of constituent structure parsing.
If this is true,then it suggests that dynamic SBNs of the form pro-posed here are a good abstract model of the natureof natural language parsing.We used the Penn Treebank WSJ corpus (Marcuset al, 1993) to perform the empirical evaluation ofthe considered approaches.
It is expensive to trainR P F1Bikel, 2004 87.9 88.8 88.3Taskar et al, 2004 89.1 89.1 89.1NN method 89.1 89.2 89.1Turian and Melamed, 2006 89.3 89.6 89.4MF method 89.3 90.7 90.0Charniak, 2000 90.0 90.2 90.1Table 1: Percentage labeled constituent recall (R),precision (P), combination of both (F1) on the test-ing set.the MF approximation on the whole WSJ corpus, soinstead we use only sentences of length at most 15,as in (Taskar et al, 2004) and (Turian and Melamed,2006).
The standard split of the corpus into training(sections 2?22, 9,753 sentences), validation (section24, 321 sentences), and testing (section 23, 603 sen-tences) was performed.2As in (Henderson, 2003; Turian and Melamed,2006) we used a publicly available tagger (Ratna-parkhi, 1996) to provide the part-of-speech tag foreach word in the sentence.
For each tag, there is anunknown-word vocabulary item which is used for allthose words which are not sufficiently frequent withthat tag to be included individually in the vocabu-lary.
We only included a specific tag-word pair in thevocabulary if it occurred at least 20 time in the train-ing set, which (with tag-unknown-word pairs) led tothe very small vocabulary of 567 tag-word pairs.During parsing with both the NN method and theMF method, we used beam search with a post-wordbeam of 10.
Increasing the beam size beyond thisvalue did not significantly effect parsing accuracy.For both of the models, the state vector size of 40was used.
All the parameters for both the NN andMF models were tuned on the validation set.
A sin-gle best model of each type was then applied to thefinal testing set.Table 1 lists the results of the NN approximationand the MF approximation, along with results of dif-2Training of our MF method on this subset of WSJ took lessthan 6 days on a standard desktop PC.
We would expect thata model for the entire WSJ corpus can be trained in about 3months time.
The training time is about linear with the num-ber of words, but a larger state vector is needed to accommo-date all the information.
The long training times on the entireWSJ would not allow us to tune the model parameters properly,which would have increased the randomness of the empiricalcomparison, although it would be feasible for building a sys-tem.637ferent generative and discriminative parsing meth-ods (Bikel, 2004; Taskar et al, 2004; Turian andMelamed, 2006; Charniak, 2000) evaluated in thesame experimental setup.
The MF model improvesover the baseline NN approximation, with an errorreduction in F-measure exceeding 8%.
This im-provement is statically significant.3 The MF modelachieves results which do not appear to be signifi-cantly different from the results of the best modelin the list (Charniak, 2000).
It should also be notedthat the model (Charniak, 2000) is the most accu-rate generative model on the standard WSJ parsingbenchmark, which confirms the viability of our gen-erative model.These experimental results suggest that Incre-mental Sigmoid Belief Networks are an appropriatemodel for natural language parsing.
Even approxi-mations such as those tested here, with a very strongfactorisability assumption, allow us to build quiteaccurate parsing models.
The main drawback of ourproposed mean field approach is the relative compu-tational complexity of the numerical procedure usedto maximize Lt,kV .
But this approximation has suc-ceeded in showing that a more accurate approxima-tion of ISBNs results in a more accurate parser.
Webelieve this provides strong justification for more ac-curate approximations of ISBNs for parsing.5 Related WorkThere has not been much previous work on graph-ical models for full parsing, although recently sev-eral latent variable models for parsing have beenproposed (Koo and Collins, 2005; Matsuzaki et al,2005; Riezler et al, 2002).
In (Koo and Collins,2005), an undirected graphical model is used forparse reranking.
Dependency parsing with dynamicBayesian networks was considered in (Peshkin andSavova, 2005), with limited success.
Their modelis very different from ours.
Roughly, it consideredthe whole sentence at a time, with the graphicalmodel being used to decide which words correspondto leaves of the tree.
The chosen words are thenremoved from the sentence and the model is recur-sively applied to the reduced sentence.Undirected graphical models, in particular Condi-3We measured significance of all the experiments in this pa-per with the randomized significance test (Yeh, 2000).tional Random Fields, are the standard tools for shal-low parsing (Sha and Pereira, 2003).
However, shal-low parsing is effectively a sequence labeling prob-lem and therefore differs significantly from full pars-ing.
As discussed in section 2.2, undirected graph-ical models do not seem to be suitable for history-based full parsing models.Sigmoid Belief Networks were used originallyfor character recognition tasks, but later a dynamicmodification of this model was applied to the rein-forcement learning task (Sallans, 2002).
However,their graphical model, approximation method, andlearning method differ significantly from those ofthis paper.6 ConclusionsThis paper proposes a new generative frameworkfor constituent parsing based on dynamic SigmoidBelief Networks with vectors of latent variables.Exact inference with the proposed graphical model(called Incremental Sigmoid Belief Networks) isnot tractable, but two approximations are consid-ered.
First, it is shown that the neural networkparser of (Henderson, 2003) can be considered as asimple feed-forward approximation to the graphicalmodel.
Second, a more accurate but still tractableapproximation based on mean field theory is pro-posed.
Both methods are empirically compared, andthe mean field approach achieves significantly betterresults, which are non-significantly different fromthe results of the most accurate generative parsingmodel (Charniak, 2000) on our testing set.
The factthat a more accurate approximation leads to a moreaccurate parser suggests that ISBNs are a good ab-stract model for constituent structure parsing.
Thisempirical result motivates research into more accu-rate approximations of dynamic SBNs.We focused in this paper on generative modelsof parsing.
The results of such a generative modelcan be easily improved by a discriminative rerank-ing model, even without any additional feature en-gineering.
For example, the discriminative train-ing techniques successfully applied in (Henderson,2004) to the feed-forward neural network model canbe directly applied to the mean field model pro-posed in this paper.
The same is true for rerank-ing with data-defined kernels, with which we would638expect similar improvements as were achieved withthe neural network parser (Henderson and Titov,2005).
Such improvements should situate the result-ing model among the best current parsing models.ReferencesDan M. Bikel.
2004.
Intricacies of Collins?
parsingmodel.
Computational Linguistics, 30(4).Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proc.
ACL, pages 173?180, Ann Arbor, MI.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proc.
ACL, pages 132?139, Seattle, Wash-ington.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania, Philadelphia, PA.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Proc.
ICML, pages 175?182,Stanford, CA.Thomas M. Cover and Joy A. Thomas.
1991.
Elementsof Information Theory.
John Wiley, New York, NY.S.
Geman and D. Geman.
1984.
Stochastic relaxation,Gibbs distributions, and the Bayesian restoration ofimages.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 6:721?741.James Henderson and Ivan Titov.
2005.
Data-definedkernels for parse reranking derived from probabilisticmodels.
In Proc.
ACL, Ann Arbor, MI.James Henderson.
2003.
Inducing history representa-tions for broad coverage statistical parsing.
In Proc.HLT-NAACL, pages 103?110, Edmonton, Canada.James Henderson.
2004.
Discriminative training ofa neural network statistical parser.
In Proc.
ACL,Barcelona, Spain.G.
Hinton, P. Dayan, B. Frey, and R. Neal.
1995.The wake-sleep algorithm for unsupervised neural net-works.
Science, 268:1158?1161.M.
I. Jordan, Z.Ghahramani, T. S. Jaakkola, and L. K.Saul.
1999.
An introduction to variational methods forgraphical models.
In Michael I. Jordan, editor, Learn-ing in Graphical Models.
MIT Press, Cambridge, MA.Terry Koo and Michael Collins.
2005.
Hidden-variablemodels for discriminative reranking.
In Proc.
EMNLP,Vancouver, B.C., Canada.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProc.
ACL, Ann Arbor, MI.Radford Neal.
1992.
Connectionist learning of beliefnetworks.
Artificial Intelligence, 56:71?113.Leon Peshkin and Virginia Savova.
2005.
Dependencyparsing with dynamic bayesian network.
In AAAI,20th National Conference on Artificial Intelligence,Pittsburgh, Pennsylvania.W.
Press, B. Flannery, S. Teukolsky, and W. Vetterling.1996.
Numerical Recipes.
Cambridge UniversityPress, Cambridge, UK.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging.
In Proc.
EMNLP, pages133?142, Univ.
of Pennsylvania, PA.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell, and Mark John-son.
2002.
Parsing the Wall Street Journal using aLexical-Functional Grammar and discriminative esti-mation techniques.
In Proc.
ACL, Philadelphia, PA.Brian Sallans.
2002.
Reinforcement Learning for Fac-tored Markov Decision Processes.
Ph.D. thesis, Uni-versity of Toronto, Toronto, Canada.Lawrence K. Saul and Michael I. Jordan.
1999.
Amean field learning algorithm for unsupervised neu-ral networks.
In Michael I. Jordan, editor, Learning inGraphical Models, pages 541?554.
MIT Press, Cam-bridge, MA.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In Proc.
HLT-NAACL,Edmonton, Canada.Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,and Christopher Manning.
2004.
Max-margin pars-ing.
In Proc.
EMNLP, Barcelona, Spain.Joseph Turian and Dan Melamed.
2006.
Advances indiscriminative parsing.
In Proc.
COLING-ACL, Syd-ney, Australia.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of the result differences.
In Proc.COLING, pages 947?953, Saarbruken, Germany.639
