Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
717?727, Prague, June 2007. c?2007 Association for Computational LinguisticsEffective Information Extraction with Semantic Affinity Patterns andRelevant RegionsSiddharth Patwardhan and Ellen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112{sidd,riloff}@cs.utah.eduAbstractWe present an information extraction systemthat decouples the tasks of finding relevantregions of text and applying extraction pat-terns.
We create a self-trained relevant sen-tence classifier to identify relevant regions,and use a semantic affinity measure to au-tomatically learn domain-relevant extractionpatterns.
We then distinguish primary pat-terns from secondary patterns and apply thepatterns selectively in the relevant regions.The resulting IE system achieves good per-formance on the MUC-4 terrorism corpusand ProMed disease outbreak stories.
Thisapproach requires only a few seed extractionpatterns and a collection of relevant and ir-relevant documents for training.1 IntroductionMany information extraction (IE) systems rely onrules or patterns to extract words and phrases basedon their surrounding context (e.g., (Soderland et al,1995; Riloff, 1996; Califf and Mooney, 1999; Yan-garber et al, 2000)).
For example, a pattern like?<subject> was assassinated?
can reliably identifya victim of a murder event.
Classification-based IEsystems (e.g., (Freitag, 1998; Freitag and McCal-lum, 2000; Chieu et al, 2003)) also generally de-cide whether to extract words based on properties ofthe words themselves as well as properties associ-ated with their surrounding context.In this research, we propose an alternative ap-proach to IE that decouples the tasks of finding a rel-evant region of text and finding a desired extraction.In a typical pattern-based IE system, the extractionpatterns perform two tasks: (a) they recognize thata relevant incident has occurred, and (b) they iden-tify and extract some information about that event.In contrast, our approach first identifies relevant re-gions of a document that describes relevant events,and then applies extraction patterns only in these rel-evant regions.This decoupled approach to IE has several po-tential advantages.
First, even seemingly good pat-terns can produce false hits due to metaphor and id-iomatic expressions.
However, by restricting theiruse to relevant regions of text, we could avoid suchfalse positives.
For example, ?John Kerry attackedGeorge Bush?
is a metaphorical description of a ver-bal tirade, but could be easily mistaken for a physi-cal attack.
Second, IE systems are prone to errors ofomission when relevant information is not explicitlylinked to an event.
For instance, a phrase like ?thegun was found...?
does not directly state that the thegun was used in a terrorist attack.
But if the gun ismentioned in a region that clearly describes a terror-ist attack, then it can be reasonably inferred to havebeen used in the attack.
Third, if the IE patterns arerestricted to areas of text that are known to be rel-evant, then it may suffice to use relatively generalpatterns, which may be easier to learn or acquire.Our approach begins with a relevant sentenceclassifier that is trained using only a few seed pat-terns and a set of relevant and irrelevant documents(but no sentence-level annotations) for the domain ofinterest.
The classifier is then responsible for identi-fying sentences that are relevant to the IE task.
Next,we learn ?semantically appropriate?
extraction pat-717terns by evaluating candidate patterns using a se-mantic affinity metric.
We then separate the pat-terns into primary and secondary patterns, and ap-ply them selectively to sentences based on the rel-evance judgments produced by the classifier.
Weevaluate our IE system on two data sets: the MUC-4 IE terrorism corpus and ProMed disease outbreakarticles.
Our results show that this approach workswell, often outperforming the AutoSlog-TS IE sys-tem which benefits from human review.2 Motivation and Related WorkOur research focuses on event-oriented informationextraction (IE), where the goal of the IE systemis to extract facts associated with domain-specificevents from unstructured text.
Many different ap-proaches to information extraction have been devel-oped, but generally speaking they fall into two cate-gories: classifier-based approaches and rule/pattern-based approaches.Classifier-based IE systems use machine learningtechniques to train a classifier that sequentially pro-cesses a document looking for words to be extracted.Examples of classifier-based IE systems are SRV(Freitag, 1998), HMM approaches (Freitag and Mc-Callum, 2000), ALICE (Chieu et al, 2003), and Re-lational Markov Networks (Bunescu and Mooney,2004).
The classifier typically decides whether aword should be extracted by considering features as-sociated with that word as well as features of thewords around it.Another common approach to information ex-traction uses a set of explicit patterns or rulesto find relevant information.
Some older sys-tems relied on hand-crafted patterns, while morerecent systems learn them automatically or semi-automatically.
Examples of rule/pattern-based ap-proaches to information extraction are FASTUS(Hobbs et al, 1997), PALKA (Kim and Moldovan,1993), LIEP (Huffman, 1996), CRYSTAL (Soder-land et al, 1995), AutoSlog/AutoSlog-TS (Riloff,1993; Riloff, 1996), RAPIER (Califf and Mooney,1999), WHISK (Soderland, 1999), ExDisco (Yan-garber et al, 2000), SNOWBALL (Agichtein andGravano, 2000), (LP)2 (Ciravegna, 2001), subtreepatterns (Sudo et al, 2003), predicate-argumentrules (Yakushiji et al, 2006) and KnowItAll(Popescu et al, 2004).One commonality behind all of these approachesis that they simultaneously decide whether a contextis relevant and whether a word or phrase is a desir-able extraction.
Classifier-based systems rely on fea-tures that consider both the word and its surround-ing context, and rule/pattern-based systems typi-cally use patterns or rules that match both the wordsaround a candidate extraction and (sometimes) prop-erties of the candidate extraction itself.There is a simplicity and elegance to having a sin-gle model that handles both of these problems at thesame time, but we hypothesized that there may bebenefits to decoupling these tasks.
We investigate analternative approach that involves two passes over adocument.
In the first pass, we apply a relevant re-gion identifier to identify regions of the text that ap-pear to be especially relevant to the domain of inter-est.
In the second pass, we apply extraction patternsinside the relevant regions.
We hypothesize threepossible benefits of this decoupled approach.First, if a system is certain that a region is rele-vant, then it can be more aggressive about searchingfor extractions.
For example, consider the domainof terrorist event reports, where a goal is to identifythe weapons that were used.
Existing systems gen-erally require rules/patterns to recognize a contextin which a weapon is explicitly linked to an eventor its consequences (e.g., ?attack with <np>?, or?<np> caused damage?).
However, weapons arenot always directly linked to an event in text, butthey may be inferred through context.
For instance,an article may mention that a weapon was ?found?or ?used?
without explicitly stating that it was in-volved in a terrorist event.
However, if we know inadvance that we are in a relevant context, then wecan reliably infer that the weapon was, most likely,used in the event.Second, some patterns may seem to be relevantlocally, but they can be deemed irrelevant when theglobal context is considered.
For example, considerthese sentences from the MUC-4 terrorism corpus:D?Aubuisson unleashed harsh attacks onDuarte ...Other brave minds that advocated reformhad been killed before in that struggle.Locally, patterns such as ?<subject> unleashed718attacks?
and ?<subject> had been killed?
seemlikely to identify the perpetrators and victims of aphysical attack.
But when read in the full contextof these sentences, it becomes clear that they are notrelated to a specific physical attack.Third, decoupling these tasks may simplify thelearning process.
Identifying relevant regionsamounts to a text classification task, albeit the goal isto identify not just relevant documents, but relevantsub-regions of documents.
Within a relevant regionthe patterns may not need to be as discriminating.So a more general learning approach may suffice.In this paper, we describe an IE system that con-sists of two decoupled modules for relevant sentenceidentification and extraction pattern learning.
InSection 3, we describe the self-trained sentence clas-sifier, which requires only a few seed patterns andrelevant and irrelevant documents for training.
Sec-tion 4 describes the extraction pattern learning mod-ule, which identifies semantically appropriate pat-terns for the IE system using a semantic affinity mea-sure.
Section 5 explains how we distinguish Primarypatterns from Secondary patterns.
Section 6 presentsexperimental results on two domains.
Finally, Sec-tion 7 lists our conclusions and future work.3 A Self-Trained Relevant SentenceClassifierOur hypothesis is that if a system can reliably iden-tify relevant regions of text, then extracting informa-tion only from these relevant regions can improve IEperformance.
There are many possible definitionsfor relevant region (e.g., Salton et al (1993), Callan(1994)), and exploring the range of possibilities isan interesting avenue for future work.
For our ini-tial investigations of this idea, we begin by simplydefining a sentence as our region size.
This has theadvantage of being an easy boundary line to draw(i.e., it is relatively easy to identify sentence bound-aries) and it is a small region size yet includes morecontext than most current IE systems do1.Our goal is to create a classifier that can determinewhether a sentence contains information that shouldbe extracted.
Furthermore, we wanted to create aclassifier that does not depend on manually anno-1Most IE systems only consider a context window consistingof a few words or phrases on either side of a potential extraction.tated sentence data so that our system can be eas-ily ported across domains.
Therefore, we devised amethod to self-train a classifier using a training setof relevant and irrelevant documents for the domain,and a few seed patterns as input.
However, this re-sults in an asymmetry in the training set.
By defini-tion, if a document is irrelevant to the IE task, thenit cannot contain any relevant information.
Con-sequently, all sentences in an irrelevant documentmust be irrelevant, so these sentences form our ini-tial irrelevant sentences pool.
In contrast, if a doc-ument is relevant to the IE task, then there must beat least one sentence that contains relevant informa-tion.
However, most documents contain a mix ofboth relevant and irrelevant sentences.
Therefore,the sentences from the relevant documents form ourunlabeled sentences pool.Figure 1 shows the self-training procedure, whichbegins with a handful of seed patterns to initiate thelearning process.
The seed patterns should be ableto reliably identify some information that is relevantto the IE task.
For instance, to build an IE system forterrorist incident reports, we used seed patterns suchas ?<subject> was kidnapped?
and ?assassinationof <np>?.
The patterns serve as a simple pattern-based classifier to automatically identify some rel-evant sentences.
In iteration 0 of the self-trainingloop (shown as dotted lines in Figure 1), the pattern-based classifier is applied to the unlabeled sentencesto automatically label some of them as relevant.Next, an SVM (Vapnik, 1995) classifier2 istrained using these relevant sentences and an equalnumber of irrelevant sentences randomly drawnfrom the irrelevant sentences pool.
We artificiallycreated a balanced training set because the set of ir-relevant sentences is initially much larger than theset of relevant sentences, and we want the classi-fier to learn how to identify new relevant sentences.The feature set consists of all unigrams that appearin the training set.
The SVM is trained using a lin-ear kernel with the default parameter settings.
In aself-training loop, the classifier is then applied to theunlabeled sentences, and all sentences that it classi-fies as relevant are added to the relevant sentencespool.
The classifier is then retrained with all of the2We used the freely available SVMlight (Joachims, 1998)implementation: http://svmlight.joachims.org719pattern?basedclassifierrelevantsentencesseedpatternsSVMclassifierunlabeledsentencesSVMtrainingirrelevantsentencesirrelevantdocumentsFigure 1: The Training Process to Create a Relevant Sentence Classifierrelevant sentences and an equal number of irrelevantsentences, and the process repeats.
We ran this self-training procedure for three iterations and then usedthe resulting classifier as our relevant sentence clas-sifier in the IE experiments described in Section 6.3.4 Learning Semantic Affinity-basedExtraction PatternsOne motivation for creating a relevant region classi-fier is to reduce the responsibilities of the extractionpatterns.
Once we know that we are in a domain-relevant area of text, patterns that simply identifywords and phrases belonging to a relevant seman-tic class may be sufficient.
In this section, we de-scribe a method to automatically identify semanti-cally appropriate extraction patterns for use with thesentence classifier.In previous work (Patwardhan and Riloff, 2006),we introduced a metric called semantic affinitywhich was used to automatically assign event rolesto extraction patterns.
Semantic affinity measuresthe tendency of a pattern to extract noun phrasesthat belong to a specific set of semantic categories.To use this metric for information extraction, amapping must be defined between semantic cate-gories and the event roles that are relevant to theIE task.
For example, one role in the terrorism do-main is physical target, which refers to physical ob-jects that are the target of an attack.
Most phys-ical targets fall into one of two general semanticcategories: BUILDING or VEHICLE.
Consequently,we define the mapping ?Target ?
BUILDING, VE-HICLE?.
Similarly, we might define the mapping?Victim ?
HUMAN, ANIMAL, PLANT?
to charac-terize possible victims of disease outbreaks.
Eachsemantic category must be mapped to a single eventrole.
This is a limitation of our approach for do-mains where multiple roles can be filled by the sameclass of fillers.
However, sometimes a general se-mantic class can be partitioned into subclasses thatare associated with different roles.
For example, inthe terrorism domain, both perpetrators and victimsbelong to the general semantic class HUMAN.
Butwe used the subclasses TERRORIST-HUMAN, whichrepresents likely perpetrator words (e.g., ?terrorist?,?guerrilla?, and ?gunman?)
and CIVILIAN-HUMAN,which represents ordinary people (e.g., ?photogra-pher?,?rancher?, and ?tourist?
), in order to generatedifferent semantic affinity estimates for the perpetra-tor and victim roles.To determine the semantic category of a noun, weuse the Sundance parser (Riloff and Phillips, 2004),which contains a dictionary of words that have se-mantic category labels.
Alternatively, a resourcesuch as WordNet (Fellbaum, 1998) could be usedto obtain this information.
All semantic categoriesthat cannot be mapped to a relevant event role aremapped to a special Other role.To estimate the semantic affinity of a pattern pfor an event role rk, the system computes f(p, rk),which is the number of pattern p?s extractions thathave a head noun belonging to a semantic categorymapped to rk.
These frequency counts are obtainedby applying each pattern to the training corpus andcollecting its extractions.
The semantic affinity of apattern p with respect to an event role rk is formallydefined as:sem aff(p, rk) =f(p, rk)?|R|i=1 f(p, ri)log2 f(p, rk) (1)where R is the set of event roles {r1, r2, .
.
.
, r|R|}.Semantic affinity is essentially the probability thata phrase extracted by pattern p will be a semanti-cally appropriate filler for role rk, weighted by thelog of the frequency.3 Note that it is possible for a3This formula is very similar to pattern ranking metrics usedby previous IE systems (Riloff, 1996; Yangarber et al, 2000),although not for semantics.720pattern to have a semantic affinity for multiple eventroles.
For instance, a terrorism pattern like ?attackon <np>?
may have a semantic affinity for bothTargets and Victims.To generate extraction patterns for an IE task, wefirst apply the AutoSlog (Riloff, 1993) extractionpattern generator to the training corpus exhaustively,so that it literally generates a pattern to extract everynoun phrase in the corpus.
Then for each event role,we rank the patterns based on their semantic affinityfor that role.Figure 2 shows the 10 patterns with the highest se-mantic affinity scores for 4 event roles.
In the terror-ism domain, we show patterns that extract weaponsand perpetrator organizations (PerpOrg).
In the dis-ease outbreaks domain, we show patterns that ex-tract diseases and victims.
The patterns rely on shal-low parsing, syntactic role assignment (e.g., subject(subject) and direct object (dobj) identification), andactive/passive voice recognition, but they are shownhere in a simplified form for readability.
The por-tion in brackets (between < and >) is extracted, andthe other words must match the surrounding con-text.
In some cases, all of the matched words areextracted (e.g., ?<# birds>?).
Most of the highest-ranked victim patterns recognize noun phrases thatrefer to people or animals because they are commonin the disease outbreak stories and these patterns donot extract information that is associated with anycompeting event roles.5 Distinguishing Primary and SecondaryPatternsSo far, our goal has been to find relevant areasof text, and then apply semantically appropriatepatterns in those regions.
Our expectation wasthat fairly general, semantically appropriate patternscould be effective if their range is restricted to re-gions that are known to be relevant.
If our relevantsentence classifier was perfect, then performing IEonly on relevant regions would be ideal.
However,identifying relevant regions is a difficult problem inits own right, and our relevant sentence classifier isfar from perfect.Consequently, one limitation of our proposed ap-proach is that no IE would be performed in sentencesthat are not deemed to be relevant by the classifier,Top Terrorism PatternsWeapon PerpOrg<subject> exploded <subject> claimedplanted <dobj> panama from <np>fired <dobj> <np> claimed responsibility<subject> was planted command of <np>explosion of <np> wing of <np><subject> was detonated kidnapped by <np><subject> was set off guerillas of <np>set off <dobj> <subject> operatinghurled <dobj> kingpins of <np><subject> was placed attacks by <np>Top Disease Outbreak PatternsDisease Victimcases of <np> <# people>spread of <np> <# cases>outbreak of <np> <# birds><#th outbreak> <# animals><# outbreaks> <subject> diedcase of <np> <# crows>contracted <dobj> <subject> knowoutbreaks of <np> <# pigs><# viruses> <# cattle>spread of <np> <# sheep>Figure 2: Top-Ranked Extraction Patternsand this could negatively affect recall.
We addressedthis issue by allowing reliable patterns to be appliedto all sentences in the text, irrespective of the outputof the sentence classifier.
For example, the pattern?<subject> was assassinated?
is a clear indicatorof a murder event, and does not need to be restrictedby the sentence classifier4.
We will refer to suchreliable patterns as Primary Patterns.
In contrast,patterns that are not necessarily reliable and need tobe restricted to relevant regions will be called Sec-ondary Patterns.To automatically distinguish Primary Patternsfrom Secondary Patterns, we compute the condi-tional probability of a pattern p being relevant,Pr(relevant | p), based on the relevant and irrele-vant documents in our training set.
We then definean upper conditional probability threshold ?u to sep-arate Primary patterns from Secondary Patterns.
Ifa pattern has a high correlation with relevant docu-ments, then our assumption is that it is generally areliable pattern that is not likely to occur in irrele-vant contexts.On the flip side, we can also use this condi-tional probability to weed out patterns that rarely4In other words, if such a pattern matches a sentence that isclassified as irrelevant, then the classifier is probably incorrect.721appear in relevant documents.
Such patterns (e.g.,?<subject> held?, ?<subject> saw?, etc.)
couldpotentially have a high semantic affinity for one ofthe semantic categories, but they are not likely to beuseful if they mainly occur in irrelevant documents.As a result, we also define a lower conditional proba-bility threshold ?l that identifies irrelevant extractionpatterns.The two thresholds ?u and ?l are used with seman-tic affinity to identify the most appropriate Primaryand Secondary patterns for the task.
This is done byfirst removing from our extraction pattern collectionall patterns with probability less than ?l.
For eachevent role, we then sort the remaining patterns basedon their semantic affinity score for that role and se-lect the top N patterns.
Next, we use the ?u prob-ability threshold to separate these N patterns intotwo subsets.
Patterns with a probability above ?uare considered to be Primary patterns for that role,and those below become the Secondary patterns.6 Experiments and Results6.1 Data SetsWe evaluated the performance of our IE system ontwo data sets: the MUC-4 terrorism corpus (Sund-heim, 1992), and a ProMed disease outbreaks cor-pus.
The MUC-4 IE task is to extract informationabout Latin American terrorist events.
We focusedour analysis on five MUC-4 string roles: perpetratorindividuals, perpetrator organizations, physical tar-gets, victims, and weapons.
The disease outbreakscorpus consists of electronic reports about diseaseoutbreak events.
For this domain we focused on twostring roles: diseases and victims5.The MUC-4 data set consists of 1700 documents,divided into 1300 development (DEV) texts, andfour test sets of 100 texts each (TST1, TST2, TST3,and TST4).
We used 1300 texts (DEV) as our train-ing set, 200 texts (TST1+TST2) for tuning, and 200texts (TST3+TST4) as a test set.
All 1700 docu-ments have answer key templates.
For the trainingset, we used the answer keys to separate the doc-uments into relevant and irrelevant subsets.
Anydocument containing at least one relevant event wasconsidered relevant.5The ?victims?
can be people, animals, or plants that areaffected by a disease.For the disease outbreak domain the data setwas collected from ProMed-mail6, an open-source,global electronic reporting system for outbreaksof infectious diseases.
We collected thousands ofProMed reports and created answer key templatesfor 245 randomly selected articles.
We used 125 asa tuning set, and 120 as the test set.
We used 2000different documents as the relevant documents fortraining.
Most of the ProMed articles contain emailheaders, footers, citations, and other snippets of non-narrative text, so we wrote a ?zoner?
program7 toautomatically strip off some of this extraneous in-formation.To obtain irrelevant documents, we collected4000 biomedical abstracts from PubMed8, a freearchive of biomedical literature.
We collected twiceas many irrelevant documents because the PubMedarticles are roughly half the size of the ProMed arti-cles, on average.
To ensure that the PubMed articleswere truly irrelevant (i.e.
did not contain any diseaseoutbreak reports) we used specific queries to excludedisease outbreak abstracts.The complete IE task involves the creation ofanswer key templates, one template per incident9.Template generation is a complex process, requir-ing coreference resolution and discourse analysis todetermine how many incidents were reported andwhich facts belong with each incident.
Our work fo-cuses on extraction pattern learning and not templategeneration, so we evaluated our systems directly onthe extractions themselves, before template genera-tion would take place.
This approach directly mea-sures how accurately the patterns find relevant infor-mation, without confounding factors from the tem-plate generation process.
For example, if a coref-erence resolver incorrectly decides that two extrac-tions are coreferent and merges them, then only oneextraction would be scored.
We used a head nounscoring scheme, where an extraction is consideredto be correct if its head noun matches the head nounin the answer key10.
Also, pronouns were discardedfrom both the system responses and the answer keyssince no coreference resolution is done.
Duplicate6http://www.promedmail.org7The term zoner was introduced by Yangarber et al (2002).8http://www.pubmedcentral.nih.gov9Many of the stories have multiple incidents per article.10For example, ?armed men?
will match ?5 armed men?.722extractions (e.g., the same string extracted by differ-ent patterns) were conflated before being scored, sothey count as just one hit or one miss.6.2 Relevant Sentence Classifier ResultsFirst, we evaluated the performance of the relevantsentence classifier described in Section 3.
We auto-matically generated seed patterns from the trainingtexts.
AutoSlog (Riloff, 1993) was used to gener-ate all extraction patterns that appear in the train-ing documents, and only those patterns with fre-quency > 50 were kept.
These were then rankedby Pr(relevant | p), and the top 20 patterns werechosen as seeds.
In the disease outbreak domain, 54patterns had a frequency > 50 and probability of 1.0.We wanted to use the same number of seeds in bothdomains for consistency, so we manually reviewedthem and used the 20 most domain-specific patternsas seeds.Due to the greater stylistic differences betweenthe relevant and irrelevant documents in the diseaseoutbreak domain (since they were gathered from dif-ferent sources), we decided to make the classifier forthat domain more conservative in classifying docu-ments as relevant.
To do this we used the predictionscores output by the SVM as a measure of confi-dence in the classification.
These scores are essen-tially the distance of the test examples from the sup-port vectors of the SVM.
For the disease outbreaksdomain we used a cutoff of 1.0 and in the terrorismdomain we used the default of 0.Since we do not have sentence annotated data,there is no direct way to evaluate the classifiers.However, we did an indirect evaluation by using theanswer keys from the tuning set.
If a sentence ina tuning document contained a string that occurredin the corresponding answer key template, then weconsidered that sentence to be relevant.
Otherwise,the sentence was deemed irrelevant.
This evaluationis not perfect for two reasons: (1) answer key stringsdo not always appear in relevant sentences.11, and(2) some arguably relevant sentences may not con-tain an answer key string (e.g., they may contain apronoun that refers to the answer, but the pronoun it-self is not the desired extraction).
However, judging11This happens due to coreference, e.g., when multiple oc-currences of an answer appear in a document, some of themmay occur in relevant sentences while others do not.Irrelevant RelevantAcc Rec Pr F Rec Pr FTerrorismIter #1 .84 .93 .89 .91 .41 .55 .47Iter #2 .84 .90 .91 .90 .54 .51 .53Iter #3 .82 .85 .92 .89 .63 .46 .53Disease OutbreaksIter #1 .75 .96 .76 .85 .21 .66 .32Iter #2 .71 .76 .82 .79 .58 .48 .53Iter #3 .63 .60 .85 .70 .72 .41 .52Table 1: Relevant Sentence Classifier Evaluationthe relevance of sentences without relying on answerkeys is also tricky, so we decided that this approachwas probably good enough to get a reasonable as-sessment of the classifier.
Using this criterion, 17%of the sentences in the terrorism articles are relevant,and 28% of the sentences in the disease outbreaksarticles are relevant.Table 1 shows the accuracy, recall, precision, andF scores of the SVM classifiers after each self-training iteration.
The classifiers generated after thethird iteration were used in our IE experiments.
Thefinal accuracy is 82% in the terrorism domain, and63% for the disease outbreaks domain.
The preci-sion on irrelevant sentences is high in both domains,but the precision on relevant sentences is relativelyweak.
Despite this, we will show in Section 6.3 thatthe classifier is effective for the IE task.
The rea-son why the classifier improves IE performance isbecause it favorably alters the proportion of relevantsentences that are passed along to the IE system.
Forexample, an analysis of the tuning set shows that re-moving the sentences deemed to be irrelevant by theclassifier increases the proportion of relevant sen-tences from 17% to 46% in the terrorism domain,and from 28% to 41% in the disease outbreaks do-main.We will also see in Section 6.3 that IE recall onlydrops a little when the sentence classifier is used,despite the fact that its recall on relevant sentencesis only 63% in terrorism and 72% for disease out-breaks.
One possible explanation is that the an-swer keys often contain multiple acceptable answerstrings (e.g., ?John Kennedy?
and ?JFK?
might bothbe acceptable answers).
On average, the answerkeys contain approximately 1.64 acceptable stringsper answer in the terrorism domain, and 1.77 accept-able strings per answer in the disease outbreaks do-723TerrorismPatterns App Rec Pr F Rec Pr FPerpInd PerpOrgASlogTS All .49 .35 .41 .33 .49 .40ASlogTS Rel .41 .50 .45 .27 .58 .37Target VictimASlogTS All .64 .42 .51 .52 .48 .50ASlogTS Rel .57 .49 .53 .48 .54 .51WeaponASlogTS All .45 .39 .42ASlogTS Rel .40 .51 .45Disease OutbreaksDisease VictimASlogTS All .51 .27 .36 .48 .35 .41ASlogTS Rel .46 .31 .37 .44 .38 .41Table 2: AutoSlog-TS Resultsmain.
Thus, even if the sentence classifier discardssome relevant sentences, an equally acceptable an-swer may be found in a different sentence.6.3 Information Extraction ResultsWe first conducted two experiments with an exist-ing IE pattern learner, AutoSlog-TS (Riloff, 1996)to give us a baseline against which to compare ourresults.
The ?All?
rows in Table 2 show these results,where ?All?
means that the IE patterns were appliedto all of the sentences in the test set.
AutoSlog-TS12produced F scores between 40-51% on the MUC-4test set, and 36-41% on the ProMed test set.
Theterrorism scores are competitive with the MUC-4scores reported by Chieu et al (2003), although theyare not directly comparable because those scores arebased on template generation.
Since we created theProMed test set ourselves, we are the first to reportresults on it13.Next, we evaluated the performance of AutoSlog-TS?
extraction patterns when they are applied only inthe sentences deemed to be relevant by our relevantsentence classifier.
The purpose of this experimentwas to determine whether the relevant sentence clas-sifier can be beneficial when used with IE patternsknown to be of good quality.
The ?Rel?
rows in Ta-12AutoSlog-TS was trained on a much larger data set of 4,958ProMed and 10,191 PubMed documents for the disease out-breaks domain.
AutoSlog-TS requires a human review of thetop-ranked patterns, which resulted in 396 patterns for the ter-rorism domain and 125 patterns for the disease outbreaks do-main.13Some previous work has been done with ProMed articles(Grishman et al, 2002a; Grishman et al, 2002b), but we are notaware of any IE evaluations on them.Disease VictimPatterns App Rec Pr F Rec Pr FASlogTS All .51 .27 .36 .48 .35 .41SA-50 All .51 .25 .34 .47 .41 .44SA-50 Rel .49 .31 .38 .44 .43 .43SA-50 Sel .50 .29 .36 .46 .41 .44SA-100 All .57 .22 .32 .52 .33 .40SA-100 Rel .55 .28 .37 .49 .36 .41SA-100 Sel .56 .26 .35 .51 .34 .41SA-150 All .66 .20 .31 .55 .27 .37SA-150 Rel .61 .26 .36 .51 .31 .38SA-150 Sel .63 .24 .35 .53 .29 .37SA-200 All .68 .19 .30 .56 .26 .36SA-200 Rel .63 .25 .35 .52 .30 .38SA-200 Sel .65 .23 .34 .54 .28 .37Table 3: ProMed Disease Outbreak Resultsble 2 show the scores for this experiment.
Precisionincreased substantially on all 7 roles, although withsome recall loss.
This shows that a sentence classi-fier that has a high precision on irrelevant sentencesbut only a moderate precision on relevant sentencescan be useful for information extraction.Tables 3 and 4 show the results of our IE system,which uses the top N Semantic Affinity (SA) pat-terns and the relevant sentence classifier.
We alsoshow the AutoSlog-TS results again in the top rowfor comparison.
The best F score for each role isshown in boldface.
We used a lower probabilitythreshold ?l of 0.5 to filter out irrelevant patterns.We then ranked the remaining patterns based on se-mantic affinity, and evaluated the performance of thetop 50, 100, 150, and 200 patterns.
The App columnindicates how the patterns were applied: for All theywere applied in all sentences in the test set, for Relthey were applied only in the relevant sentences (asjudged by our sentence classifier).
For the Sel con-dition, the Primary patterns were applied in all sen-tences but the Secondary patterns were applied onlyin relevant sentences.
To separate Primary and Sec-ondary patterns we used an upper probability thresh-old ?u of 0.8.Looking at the rows with the All condition, wesee that the semantic affinity patterns achieve goodrecall (e.g., the top 200 patterns have a recall over50% for most roles), but precision is often quite low.This is not surprising because high semantic affin-ity patterns do not necessarily have to be relevant tothe domain, so long as they recognize semanticallyappropriate things.724PerpInd PerpOrg Target Victim WeaponPatterns App Rec Pr F Rec Pr F Rec Pr F Rec Pr F Rec Pr FASlogTS All .49 .35 .41 .33 .49 .40 .64 .42 .51 .52 .48 .50 .45 .39 .42SA-50 All .24 .29 .26 .20 .42 .27 .42 .43 .42 .41 .43 .42 .53 .46 .50SA-50 Rel .19 .32 .24 .18 .60 .28 .38 .48 .42 .37 .52 .43 .41 .56 .48SA-50 Sel .20 .33 .25 .20 .54 .29 .42 .50 .45 .38 .52 .44 .43 .53 .48SA-100 All .40 .30 .34 .30 .43 .35 .56 .38 .45 .45 .37 .41 .55 .43 .48SA-100 Rel .36 .39 .38 .25 .59 .35 .52 .45 .48 .40 .47 .44 .45 .51 .48SA-100 Sel .38 .40 .39 .27 .55 .36 .56 .46 .50 .41 .47 .44 .47 .49 .48SA-150 All .50 .27 .35 .34 .39 .37 .62 .30 .40 .50 .33 .40 .55 .39 .45SA-150 Rel .46 .39 .42 .28 .58 .38 .56 .37 .45 .44 .45 .45 .45 .50 .47SA-150 Sel .48 .39 .43 .31 .55 .40 .60 .37 .46 .46 .44 .45 .47 .47 .47SA-200 All .73 .08 .15 .42 .43 .42 .64 .29 .40 .54 .32 .40 .64 .17 .27SA-200 Rel .67 .15 .24 .34 .61 .43 .58 .36 .45 .47 .43 .45 .52 .29 .37SA-200 Sel .71 .12 .21 .36 .58 .45 .61 .35 .45 .48 .43 .45 .53 .22 .31Table 4: MUC-4 Terrorism ResultsNext, we can compare each All row with the Relrow immediately below it.
We observe that in everycase precision improves, often dramatically.
Thisdemonstrates that our sentence classifier is havingthe desired effect.
However, observe that the preci-sion gain comes with some loss in recall points.Clearly, this drop in recall is due to the answersembedded inside relevant sentences incorrectly clas-sified as irrelevant.
To counter this, we apply the Pri-mary patterns to all the sentences.
Thus, if we com-pare each Rel row with the Sel row immediately be-low it, we see the effect of loosening the reins on thePrimary patterns (the Secondary patterns are still re-stricted to the relevant sentences).
In most cases, therecall improves with a relatively small drop in preci-sion, or no drop at all.
In the terrorism domain, thehighest F score for four of the five roles occurs underthe Sel condition.
In the disease outbreaks domain,the best F score for diseases occurs in the Rel con-dition, while the best score for victims is achievedunder both the All and the Sel conditions.Finally, we note that the best F scores producedby our information extraction system are higher thanthose produced by AutoSlog-TS for all of the rolesexcept Targets and Victims, and our best perfor-mance on Targets is only slightly lower.
These re-sults are particularly noteworthy because AutoSlog-TS requires a human to manually review the patternsand assign event roles to them.
In contrast, our ap-proach is fully automated.These results validate our hypothesis that decou-pling the processes of finding relevant regions andapplying semantically appropriate patterns can cre-ate an effective IE system.7 ConclusionsIn this work, we described an information extractionsystem based on a relevant sentence classifier andextraction patterns learned using a semantic affin-ity metric.
The sentence classifier was self-trainedusing only relevant and irrelevant documents plus ahandful of seed extraction patterns.
We showed thatseparating the task of relevant region identificationfrom that of pattern extraction can be effective for in-formation extraction.
In addition, we observed thatthe use of a relevant sentence classifier is beneficialfor an IE system.There are several avenues that need to be exploredfor future work.
First, it would be interesting to seeif the use of richer features can improve classifierperformance, and if that in turn improves the perfor-mance of the IE system.
We would also like to ex-periment with different region sizes and study theireffect on information extraction.
Finally, other tech-niques for learning semantically appropriate extrac-tion patterns could be investigated.AcknowledgmentsThis research was supported by NSF Grant IIS-0208985, Department of Homeland Security GrantN0014-07-1-0152, and the Institute for ScientificComputing Research and the Center for AppliedScientific Computing within Lawrence LivermoreNational Laboratory.
We are grateful to Sean Igoand Rich Warren for annotating the disease out-breaks corpus.725ReferencesE.
Agichtein and L. Gravano.
2000.
Snowball: Extract-ing Relations from Large Plain-Text Collections.
InProceedings of the Fifth ACM Conference on DigitalLibraries, pages 85?94, San Antonio, TX, June.R.
Bunescu and R. Mooney.
2004.
Collective Informa-tion Extraction with Relational Markov Networks.
InProceeding of the 42nd Annual Meeting of the Associ-ation for Computational Linguistics, pages 438?445,Barcelona, Spain, July.M.
Califf and R. Mooney.
1999.
Relational Learningof Pattern-matching Rules for Information Extraction.In Proceedings of the Sixteenth National Conferenceon Artificial Intelligence, pages 328?334, Orlando, FL,July.J.
Callan.
1994.
Passage-Level Evidence in DocumentRetrieval.
In Proceedings of the 17th Annual Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 302?310,Dublin, Ireland, July.H.
Chieu, H. Ng, and Y. Lee.
2003.
Closing theGap: Learning-Based Information Extraction RivalingKnowledge-Engineering Methods.
In Proceedings ofthe 41st Annual Meeting of the Association for Compu-tational Linguistics, pages 216?223, Sapporo, Japan,July.F.
Ciravegna.
2001.
Adaptive Information Extractionfrom Text by Rule Induction and Generalisation.
InProceedings of Seventeenth International Joint Con-ference on Artificial Intelligence, pages 1251?1256,Seattle, WA, August.C.
Fellbaum, editor.
1998.
WordNet: An electronic lexi-cal database.
MIT Press.D.
Freitag and A. McCallum.
2000.
Information Ex-traction with HMM Structures Learned by Stochas-tic Optimization.
In Proceedings of the SeventeenthNational Conference on Artificial Intelligence, pages584?589, Austin, TX, August.D.
Freitag.
1998.
Toward General-Purpose Learningfor Information Extraction.
In Proceedings of the36th Annual Meeting of the Association for Computa-tional Linguistics and 17th International Conferenceon Computational Linguistics, pages 404?408, Mon-treal, Quebec, August.R.
Grishman, S. Huttunen, and R. Yangarber.
2002a.
In-formation Extraction for Enhanced Access to DiseaseOutbreak Reports.
Journal of Biomedical Informatics,35(4):236?246, August.R.
Grishman, S. Huttunen, and R. Yangarber.
2002b.Real-Time Event Extraction for Infectious DiseaseOutbreaks.
In Proceedings of the 3rd Annual HumanLanguage Technology Conference, San Diego, CA,March.J.
Hobbs, D. Appelt, J.
Bear, D. Israel, M. Kameyama,M.
Stickel, and M. Tyson.
1997.
FASTUS: A Cas-caded Finite-state Transducer for Extracting Informa-tion for Natural-Language Text.
In E. Roche andY.
Schabes, editors, Finite-State Language Processing,pages 383?406.
MIT Press, Cambridge, MA.S.
Huffman.
1996.
Learning Information ExtractionPatterns from Examples.
In S. Wermter, E. Riloff,and G. Scheler, editors, Connectionist, Statistical, andSymbolic Approaches to Learning for Natural Lan-guage Processing, pages 246?260.
Springer, Berlin.T.
Joachims.
1998.
Text Categorization with SupportVector Machines: Learning with Many Relevant Fea-tures.
In Proceedings of the Tenth European Confer-ence on Machine Learning, pages 137?142, April.J.
Kim and D. Moldovan.
1993.
PALKA: A System forLexical Knowledge Acquisition.
In Proceedings ofthe Second International Conference on Informationand Knowledge Management, pages 124?131, Wash-ington, DC, November.S.
Patwardhan and E. Riloff.
2006.
Learning Domain-Specific Information Extraction Patterns from theWeb.
In Proceedings of the ACL 2006 Workshop onInformation Extraction Beyond the Document, pages66?73, Sydney, Australia, July.A.
Popescu, A. Yates, and O. Etzioni.
2004.
Class Ex-traction from the World Wide Web.
In Ion Muslea,editor, Adaptive Text Extraction and Mining: Papersfrom the 2004 AAAI Workshop, pages 68?73, San Jose,CA, July.E.
Riloff and W. Phillips.
2004.
An Introduction to theSundance and AutoSlog Systems.
Technical ReportUUCS-04-015, School of Computing, University ofUtah.E.
Riloff.
1993.
Automatically Constructing a Dictio-nary for Information Extraction Tasks.
In Proceedingsof the Eleventh National Conference on Artificial In-telligence, pages 811?816, Washington, DC, July.E.
Riloff.
1996.
Automatically Generating Extrac-tion Patterns from Untagged Text.
In Proceedings ofthe Thirteenth National Conference on Articial Intelli-gence, pages 1044?1049, Portland, OR, August.G.
Salton, J. Allan, and C. Buckley.
1993.
Approachesto Passage Retrieval in Full Text Information Systems.In Proceedings of the 16th Annual International ACMSIGIR Conference on Research and Development onInformation Retrieval, pages 49?58, Pittsburgh, PA,June.726S.
Soderland, D. Fisher, J. Aseltine, and W. Lehnert.1995.
CRYSTAL: Inducing a Conceptual Dictionary.In Proceedings of the Fourteenth International JointConference on Artificial Intelligence, pages 1314?1319, Montreal, Canada, August.S.
Soderland.
1999.
Learning Information ExtractionRules for Semi-Structured and Free Text.
MachineLearning, 34(1-3):233?272, February.K.
Sudo, S. Sekine, and R. Grishman.
2003.
An Im-proved Extraction Patterns Representation Model forAutomatic IE Pattern Acquisition.
In Proceedings ofthe 41st Annual Meeting of the Association for Compu-tational Linguistics, pages 224?231, Sapporo, Japan,July.B.
Sundheim.
1992.
Overview of the Fourth MessageUnderstanding Evaluation and Conference.
In Pro-ceedings of the Fourth Message Understanding Con-ference (MUC-4), pages 3?21, McLean, VA, June.V.
Vapnik.
1995.
The Nature of Statistical Learning The-ory.
Springer, New York, NY.A.
Yakushiji, Y. Miyao, T. Ohta, Y. Tateisi, and J. Tsu-jii.
2006.
Construction of Predicate-argument Struc-ture Patterns for Biomedical Information Extraction.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, pages 284?292, Sydney, Australia, July.R.
Yangarber, R. Grishman, P. Tapanainen, and S. Hut-tunen.
2000.
Automatic Acquisition of DomainKnowledge for Information Extraction.
In Proceed-ings of the 18th International Conference on Computa-tional Linguistics, pages 940?946, Saarbru?cken, Ger-many, August.R.
Yangarber, W. Lin, and R. Grishman.
2002.
Unsu-pervised Learning of Generalized Names.
In Proceed-ings of the 19th International Conference on Compu-tational Linguistics, pages 154?160, Taipei, Taiwan,August.727
