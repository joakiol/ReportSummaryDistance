Proceedings of the Workshop on Statistical Machine Translation, pages 15?22,New York City, June 2006. c?2006 Association for Computational LinguisticsMorpho-syntactic Arabic Preprocessing for Arabic-to-English StatisticalMachine TranslationAnas El Isbihani Shahram Khadivi Oliver BenderLehrstuhl fu?r Informatik VI - Computer Science DepartmentRWTH Aachen UniversityD-52056 Aachen, Germany{isbihani,khadivi,bender,ney}@informatik.rwth-aachen.deHermann NeyAbstractThe Arabic language has far richer sys-tems of inflection and derivation than En-glish which has very little morphology.This morphology difference causes a largegap between the vocabulary sizes in anygiven parallel training corpus.
Segmen-tation of inflected Arabic words is a wayto smooth its highly morphological na-ture.
In this paper, we describe somestatistically and linguistically motivatedmethods for Arabic word segmentation.Then, we show the efficiency of proposedmethods on the Arabic-English BTEC andNIST tasks.1 IntroductionArabic is a highly inflected language compared toEnglish which has very little morphology.
This mor-phological richness makes statistical machine trans-lation from Arabic to English a challenging task.
Ausual phenomenon in Arabic is the attachment of agroup of words which are semantically dependent oneach other.
For instance, prepositions like ?and?
and?then?
are usually attached to the next word.
Thisapplies also to the definite article ?the?.
In addi-tion, personal pronouns are attached to the end ofverbs, whereas possessive pronouns are attached tothe end of the previous word, which constitutes thepossessed object.
Hence, an Arabic word can be de-composed into ?prefixes, stem and suffixes?.
We re-strict the set of prefixes and suffixes to those showedin Table 1 and 2, where each of the prefixes and suf-fixes has at least one meaning which can be repre-sented by a single word in the target language.
Someprefixes can be combined.
For example the wordwbAlqlm (????AK.
?
which means ?and with the pen?
)has a prefix which is a combination of three pre-fixes, namely w, b and Al.
The suffixes we handlein this paper can not be combined with each other.Thus, the compound word pattern handled here is?prefixes-stem-suffix?.All possible prefix combinations that do not con-tain Al allow the stem to have a suffix.
Note thatthere are other suffixes that are not handled here,such as At ( H@), An ( 	?
@) and wn ( 	??)
which makethe plural form of a word.
The reason why we omitthem is that they do not have their own meaning.
Theimpact of Arabic morphology is that the vocabularysize and the number of singletons can be dramati-cally high, i.e.
the Arabic words are not seen oftenenough to be learned by statistical machine transla-tion models.
This can lead to an inefficient align-ment.In order to deal with this problem and to improvethe performance of statistical machine translation,each word must be decomposed into its parts.
In(Larkey et al, 2002) it was already shown that wordsegmentation for Arabic improves information re-trieval.
In (Lee et al, 2003) a statistical approachfor Arabic word segmentation was presented.
It de-composes each word into a sequence of morphemes(prefixes-stem-suffixes), where all possible prefixesand suffixes (not only those we described in Table 1and 2) are split from the original word.
A compa-rable work was done by (Diab et al, 2004), wherea POS tagging method for Arabic is also discussed.As we have access to this tool, we test its impacton the performance of our translation system.
In15Table 1: Prefixes handled in this work and their meanings.Prefix ?
?
?
?
H.
?
@Transliteration w f k l b AlMeaning and and then as, like in order to with, in the(Habash and Rambow, 2005) a morphology analyzerwas used for the segementation and POS tagging.
Incontrast to the methods mentioned above, our seg-mentation method is unsupervised and rule based.In this paper we first explain our statistical ma-chine translation (SMT) system used for testing theimpact of the different segmentation methods, thenwe introduce some preprocessing and normalizationtools for Arabic and explain the linguistic motiva-tion beyond them.
Afterwards, we present threeword segmentation methods, a supervised learningapproach, a finite state automaton-based segmenta-tion, and a frequency-based method.
In Section 5,the experimental results are presented.
Finally, thepaper is summarized in Section 6 .2 Baseline SMT SystemIn statistical machine translation, we are given asource language sentence fJ1 = f1 .
.
.
fj .
.
.
fJ ,which is to be translated into a target language sen-tence eI1 = e1 .
.
.
ei .
.
.
eI .
Among all possible tar-get language sentences, we will choose the sentencewith the highest probability:e?I?1 = argmaxI,eI1{Pr(eI1|fJ1 )} (1)The posterior probability Pr(eI1|fJ1 ) is modeled di-rectly using a log-linear combination of severalmodels (Och and Ney, 2002):Pr(eI1|fJ1 ) =exp(?Mm=1 ?mhm(eI1, fJ1 ))?e?I?1exp(?Mm=1 ?mhm(e?I?1 , fJ1 ))(2)The denominator represents a normalization factorthat depends only on the source sentence fJ1 .
There-fore, we can omit it during the search process.
As adecision rule, we obtain:e?I?1 = argmaxI,eI1{M?m=1?mhm(eI1, fJ1 )}(3)This approach is a generalization of the source-channel approach (Brown et al, 1990).
It has theadvantage that additional models h(?)
can be eas-ily integrated into the overall system.
The modelscaling factors ?M1 are trained with respect to the fi-nal translation quality measured by an error criterion(Och, 2003).We use a state-of-the-art phrase-based translationsystem including the following models: an n-gramlanguage model, a phrase translation model and aword-based lexicon model.
The latter two mod-els are used for both directions: p(f |e) and p(e|f).Additionally, we use a word penalty and a phrasepenalty.
More details about the baseline system canbe found in (Zens and Ney, 2004; Zens et al, 2005).3 Preprocessing and Normalization Tools3.1 TokenizerAs for other languages, the corpora must be first to-kenized.
Here words and punctuations (except ab-breviation) must be separated.
Another criterion isthat Arabic has some characters that appear only atthe end of a word.
We use this criterion to separatewords that are wrongly attached to each other.3.2 Normalization and SimplificationThe Arabic written language does not contain vow-els, instead diacritics are used to define the pronun-ciation of a word, where a diacritic is written underor above each character in the word.
Usually thesediacritics are omitted, which increases the ambigu-ity of a word.
In this case, resolving the ambiguityof a word is only dependent on the context.
Some-times, the authors write a diacritic on a word to helpthe reader and give him a hint which word is reallymeant.
As a result, a single word with the samemeaning can be written in different ways.
For exam-ple $Eb (I. ?
?)
can be read1 as sha?ab (Eng.
nation)or sho?ab (Eng.
options).
If the author wants to givethe reader a hint that the second word is meant, he1There are other possible pronunciations for the word $Ebthan the two mentioned.16Table 2: Suffixes handled in this work and their meanings.Suffix ?
?G ?
??
, ??
, A?
?Transliteration y ny k kmA, km, knMeaning my me you, your (sing.)
you, your (pl.
)Suffix A 	K ?
A?
??
, ??
, A?
?Transliteration nA h hA hmA, hm, hnMeaning us, our his, him her them, theircan write $uEb (I.
??)
or $uEab (I.
??).
To avoidthis problem we normalize the text by removing alldiacritics.After segmenting the text, the size of the sen-tences increases rapidly, where the number of thestripped article Al is very high.
Not every article inan Arabic sentence matches to an article in the targetlanguage.
One of the reasons is that the adjective inArabic gets an article if the word it describes is def-inite.
So, if a word has the prefix Al, then its adjec-tive will also have Al as a prefix.
In order to reducethe sentence size we decide to remove all these arti-cles that are supposed to be attached to an adjective.Another way for determiner deletion is described in(Lee, 2004).4 Word SegmentationOne way to simplify inflected Arabic text for a SMTsystem is to split the words in prefixes, stem andsuffixes.
In (Lee et al, 2003), (Diab et al, 2004)and (Habash and Rambow, 2005) three supervisedsegmentation methods are introduced.
However, inthese works the impact of the segmentation on thetranslation quality is not studied.
In the next subsec-tions we will shortly describe the method of (Diab etal., 2004).
Then we present our unsupervised meth-ods.4.1 Supervised Learning Approach (SL)(Diab et al, 2004) propose solutions to word seg-mentation and POS Tagging of Arabic text.
For thepurpose of training the Arabic TreeBank is used,which is an Arabic corpus containing news articlesof the newswire agency AFP.
In the first step the textmust be transliterated to the Buckwalter translitera-tion, which is a one-to-one mapping to ASCII char-acters.
In the second step it will be segmented andtokenized.
In the third step a partial lemmatization isdone.
Finally a POS tagging is performed.
We willtest the impact of the step 3 (segmentation + lemma-tization) on the translation quality using our phrasebased system described in Section 2.4.2 Frequency-Based Approach (FB)We provide a set of all prefixes and suffixes andtheir possible combinations.
Based on this set, wemay have different splitting points for a given com-pound word.
We decide whether and where to splitthe composite word based on the frequency of dif-ferent resulting stems and on the frequency of thecompound word, e.g.
if the compound word has ahigher frequency than all possible stems, it will notbe split.
This simple heuristic harmonizes the cor-pus by reducing the size of vocabulary, singletonsand also unseen words from the test corpus.
Thismethod is very similar to the method used for split-ting German compound words (Koehn and Knight,2003).4.3 Finite State Automaton-Based Approach(FSA)To segment Arabic words into prefixes, stem and onesuffix, we implemented two finite state automata.One for stripping the prefixes and the other for thesuffixes.
Then, we append the suffix automaton tothe other one for stripping prefixes.
Figure 1 showsthe finite state automaton for stripping all possibleprefix combinations.
We add the prefix s (?
), whichchanges the verb tense to the future, to the set ofprefixes which must be stripped (see table 1).
Thisprefix can only be combined with w and f. Our mo-tivation is that the future tense in English is built byadding the separate word ?will?.The automaton showed in Figure 1 consists of thefollowing states:?
S: the starting point of the automaton.?
E: tne end state, which can only be achieved if17S KALBLWFCEFigure 1: Finite state automaton for stripping pre-fixes off Arabic words.the resulting stem exists already in the text.?
WF: is achieved if the word begins with w or f.?
And the states , K, L, B and AL are achieved ifthe word begins with s, k, l, b and Al, respec-tively.To minimize the number of wrong segmentations,we restricted the transition from one state to theother to the condition that the produced stem occursat least one time in the corpus.
To ensure that mostcompound words are recognized and segmented, werun the segmenter itteratively, where after each it-eration the newly generated words are added to thevocabulary.
This will enable recognizing new com-pound words in the next iteration.
Experimentsshowed that running the segmenter twice is suffi-cient and in higher iterations most of the added seg-mentations are wrong.4.4 Improved Finite State Automaton-BasedApproach (IFSA)Although we restricted the finite state segmenter insuch a way that words will be segmented only if theyielded stem already exists in the corpus, we still getsome wrongly segmented words.
Thus, some newstems, which do not make sense in Arabic, occurin the segmented text.
Another problem is that thefinite state segmenter does not care about ambigui-ties and splits everything it recognizes.
For examplelet us examine the word frd (XQ 	?).
In one case, thecharacter f is an original one and therefore can notbe segmented.
In this case the word means ?per-son?.
In the other case, the word can be segmentedto ?f rd?
(which means ?and then he answers?
or?and then an answer?).
If the words Alfrd, frd andrd(XQ 	?
, XQ 	??
@ and XP) occur in the corpus, then the fi-nite state segmenter will transform the Alfrd (whichmeans ?the person?)
to Al f rd (which can be trans-lated to ?the and then he answers?).
Thus the mean-ing of the original word is distorted.
To solve allthese problems, we improved the last approach in away that prefixes and suffixes are recognized simul-taneously.
The segmentation of the ambiguous wordwill be avoided.
In doing that, we intend to postponeresolving such ambiguities to our SMT system.The question now is how can we avoid the seg-mentation of ambiguous words.
To do this, it is suf-ficient to find a word that contains the prefix as anoriginal character.
In the last example the word Al-frd contains the prefix f as an original character andtherefore only Al can be stripped off the word.
Thenext question we can ask is, how can we decide if acharacter belongs to the word or is a prefix.
We canextract this information using the invalid prefix com-binations.
For example Al is always the last prefixthat can occur.
Therefore all characters that occur ina word after Al are original characters.
This methodcan be applied for all invalid combinations to extractnew rules to decide whether a character in a word isan original one or not.On the other side, all suffixes we handle in thiswork are pronouns.
Therefore it is not possible tocombine them as a suffix.
We use this fact to makea decision whether the end characters in a word areoriginal or can be stripped.
For example the wordtrkhm (??
?QK) means ?he lets them?.
If we supposethat hm is a suffix and therefore must be stripped,then we can conclude that k is an original characterand not a suffix.
In this way we are able to extractfrom the corpus itself decisions whether and how aword can be segmented.In order to implement these changes the originalautomaton was modified.
Instead of splitting a wordwe mark it with some properties which corespondto the states traversed untill the end state.
On the18other side, we use the technique described above togenerate negative properties which avoid the corre-sponding kind of splitting.
If a property and its nega-tion belong to the same word then the property is re-moved and only the negation is considered.
At theend each word is split corresponding to the proper-ties it is marked with.5 Experimental Results5.1 Corpus StatisticsThe experiments were carried out on two tasks: thecorpora of the Arabic-English NIST task, whichcontain news articles and UN reports, and theArabic-English corpus of the Basic Travel Expres-sion Corpus (BTEC) task, which consists of typi-cal travel domain phrases (Takezawa et al, 2002).The corpus statistics of the NIST and BTEC corporaare shown in Table 3 and 5.
The statistics of thenews part of NIST corpus, consisting of the Ummah,ATB, ANEWS1 and eTIRR corpora, is shown in Ta-ble 4.
In the NIST task, we make use of the NIST2002 evaluation set as a development set and NIST2004 evaluation set as a test set.
Because the testset contains four references for each senence we de-cided to use only the first four references of the de-velopment set for the optimization and evaluation.In the BTEC task, C-Star?03 and IWSLT?04 coporaare considered as development and test sets, respec-tively.5.2 Evaluation MetricsThe commonly used criteria to evaluate the trans-lation results in the machine translation commu-nity are: WER (word error rate), PER (position-independent word error rate), BLEU (Papineni etal., 2002), and NIST (Doddington, 2002).
The fourcriteria are computed with respect to multiple ref-erences.
The number of reference translations persource sentence varies from 4 to 16 references.
Theevaluation is case-insensitive for BTEC and case-sensitive for NIST task.
As the BLEU and NISTscores measure accuracy, higher scores are better.5.3 Translation ResultsTo study the impact of different segmentation meth-ods on the translation quality, we apply differentword segmentation methods to the Arabic part of theBTEC and NIST corpora.
Then, we make use of thephrase-based machine translation system to translatethe development and test sets for each task.First, we discuss the experimental results on theBTEC task.
In Table 6, the translation results on theBTEC corpus are shown.
The first row of the table isthe baseline system where none of the segmentationmethods is used.
All segmentation methods improvethe baseline system, except the SL segmentationmethod on the development corpus.
The best per-forming segmentation method is IFSA which gener-ates the best translation results based on all evalua-tion criteria, and it is consistent over both develop-ment and evaluation sets.
As we see, the segmen-tation of Arabic words has a noticeable impact inimproving the translation quality on a small corpus.To study the impact of word segmentation meth-ods on a large task, we conduct two sets of experi-ments on the NIST task using two different amountsof the training corpus: only news corpora, and fullcorpus.
In Table 7, the translation results on theNIST task are shown when just the news corporawere used to train the machine translation models.As the results show, except for the FB method, allsegmentation methods improve the baseline system.For the NIST task, the SL method outperforms theother segmentation methods, while it did not achievegood results when comparing to the other methodsin the BTEC task.We see that the SL, FSA and IFSA segmentationmethods consistently improve the translation resultsin the BTEC and NIST tasks, but the FB methodfailed on the NIST task, which has a larger trainingcorpus .
The next step is to study the impact of thesegmentation methods on a very large task, the NISTfull corpus.
Unfortunately, the SL method failed onsegmenting the large UN corpus, due to the largeprocessing time that it needs.
Due to the negativeresults of the FB method on the NIST news corpora,and very similar results for FSA and IFSA, we wereinterested to test the impact of IFSA on the NISTfull corpus.
In Table 8, the translation results of thebaseline system and IFSA segmentation method forthe NIST full corpus are depicted.
As it is shown intable, the IFSA method slightly improves the trans-lation results in the development and test sets.The IFSA segmentation method generates thebest results among our proposed methods.
Itacheives consistent improvements in all three tasksover the baseline system.
It also outperforms the SL19Table 3: BTEC corpus statistics, where the Arabic part is tokenized and segmented with the SL, FB, FSAand the IFSA methods.ARABIC ENGLISHTOKENIZED SL FB FSA IFSATrain: Sentences 20KRunning Words 159K 176.2K 185.5K 190.3K 189.1K 189KVocabulary 18,149 14,321 11,235 11,736 12,874 7,162Dev: Sentences 506Running Words 3,161 3,421 3,549 3,759 3,715 5,005OOVs (Running Words) 163 129 149 98 118 NATest: Sentences 500Running Words 3,240 3,578 3,675 3,813 3,778 4,986OOVs (Running Words) 186 120 156 92 115 NATable 4: Corpus statistics for the news part of the NIST task, where the Arabic part is tokenized and seg-mented with SL, FB, FSA and IFSA methods.ARABIC ENGLISHTOKENIZED SL FB FSA IFSATrain: Sentences 284.9KRunning Words 8.9M 9.7M 12.2M 10.9M 10.9M 10.2MVocabulary 118.7K 90.5K 43.1K 68.4K 62.2K 56.1KDev: Sentences 1,043Running Words 27.7K 29.1K 37.3K 34.4K 33.5K 33KOOVs (Running Words) 714 558 396 515 486 NATest: Sentences 1,353Running Words 37.9K 41.7K 52.6K 48.6K 48.3K 48.3KOOVs (Running Words) 1,298 1,027 612 806 660 NAsegmentation on the BTEC task.Although the SL method outperforms the IFSAmethod on the NIST tasks, the IFSA segmentationmethod has a few notable advantages over the SLsystem.
First, it is consistent in improving the base-line system over the three tasks.
But, the SL methodfailed in improving the BTEC development corpus.Second, it is fast and robust, and capable of beingapplied to the large corpora.
Finally, it employs anunsupervised learning method, therefore can easilycope with a new task or corpus.We observe that the relative improvement overthe baseline system is decreased by increasing thesize of the training corpus.
This is a natural effectof increasing the size of the training corpus.
Asthe larger corpus provides higher probability to havemore samples per word, this means higher chanceto learn the translation of a word in different con-texts.
Therefore, larger training corpus makes a bet-ter translation system, i.e.
a better baseline, then itwould be harder to outperform this better system.Using the same reasoning, we can realize why theFB method achieves good results on the BTEC task,but not on the NIST task.
By increasing the sizeof the training corpus, the FB method tends to seg-ment words more than the IFSA method.
This over-segmentation can be compensated by using longerphrases during the translation, in order to considerthe same context compared to the non-segmentedcorpus.
Then, it would be harder for a phrase-basedmachine translation system to learn the translationof a word (stem) in different contexts.6 ConclusionWe presented three methods to segment Arabicwords: a supervised learning approach, a frequency-20Table 5: NIST task corpus statistics, where the Arabic part is tokenized and segmented with the IFSAmethod.ARABIC ENGLISHTOKENIZED IFSATrain: Sentences 8.5MRunning Words 260.5M 316.8M 279.2MVocabulary 510.3K 411.2K 301.2KDev: Sentences 1043Running Words 30.2K 33.3K 33KOOVs (Running Words) 809 399 NATest: Sentences 1353Running Words 40K 47.9K 48.3KOOVs (Running Words) 871 505 NATable 6: Case insensitive evaluation results for translating the development and test data of BTEC task afterperforming divers preprocessing.Dev TestmPER mWER BLEU NIST mPER mWER BLEU NIST[%] [%] [%] [%] [%] [%]Non-Segmented Data 21.4 24.6 63.9 10.0 23.5 27.2 58.1 9.6SL Segmenter 21.2 24.4 62.5 9.7 23.4 27.4 59.2 9.7FB Segmenter 20.9 24.4 65.3 10.1 22.1 25.8 59.8 9.7FSA Segmenter 20.1 23.4 64.8 10.2 21.1 25.2 61.3 10.2IFSA Segmenter 20.0 23.3 65.0 10.4 21.2 25.3 61.3 10.2based approach and a finite state automaton-basedapproach.
We explained that the best of our pro-posed methods, the improved finite state automaton,has three advantages over the state-of-the-art Arabicword segmentation method (Diab, 2000), supervisedlearning.
They are: consistency in improving thebaselines system over different tasks, its capabilityto be efficiently applied on the large corpora, and itsability to cope with different tasks.7 AcknowledgmentThis material is based upon work supported bythe Defense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-06-C-0023.Any opinions, findings and conclusions or recom-mendations expressed in this material are those ofthe author(s) and do not necessarily reflect the viewsof the Defense Advanced Research Projects Agency(DARPA).ReferencesP.
F. Brown, J. Cocke, S. A. Della Pietra, V. J. DellaPietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, andP.
S. Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, 16(2):79?85,June.M.
Diab, K. Hacioglu, and D. Jurafsky.
2004.
Automatictagging of arabic text: From raw text to base phrasechunks.
In D. M. Susan Dumais and S. Roukos, edi-tors, HLT-NAACL 2004: Short Papers, Boston, Mas-sachusetts, USA, May 2 - May 7.
Association forComputational Linguistics.M.
Diab.
2000.
An unsupervised method for multi-lingual word sense tagging using parallel corpora: Apreliminary investigation.
In ACL-2000 Workshop onWord Senses and Multilinguality, pages 1?9, HongKong, October.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics.
In Proc.
ARPA Workshop on Human LanguageTechnology.21Table 7: Case sensitive evaluation results for translating the development and test data of the news part ofthe NIST task after performing divers preprocessing.Dev TestmPER mWER BLEU NIST mPER mWER BLEU NIST[%] [%] [%] [%] [%] [%]Non-Segmented Data 43.7 56.4 43.6 9.9 46.1 58.0 37.4 9.1SL Segmenter 42.0 54.7 45.1 10.2 44.3 56.3 39.9 9.6FB Segmenter 43.4 56.1 43.2 9.8 45.6 57.8 37.2 9.2FSA Segmenter 42.9 55.7 43.7 9.9 44.8 56.9 38.7 9.4IFSA Segmenter 42.6 55.0 44.6 9.9 44.5 56.6 38.8 9.4Table 8: Case-sensitive evaluation results for translating development and test data of NIST task.Dev TestmPER mWER BLEU NIST mPER mWER BLEU NIST[%] [%] [%] [%] [%] [%]Non-Segmented Data 41.5 53.5 46.4 10.3 42.5 53.9 42.6 10.0IFSA Segmenter 41.1 53.2 46.7 10.2 42.1 53.6 43.4 10.1N.
Habash and O. Rambow.
2005.
Arabic tokeniza-tion, part-of-speech tagging and morphological dis-ambiguation in one fell swoop.
In Proc.
of the 43rdAnnual Meeting of the Association for ComputationalLinguistics (ACL?05), pages 573?580, Ann Arbor,Michigan, June.
Association for Computational Lin-guistics.P.
Koehn and K. Knight.
2003.
Empirical methods forcompound splitting.
In Proc.
10th Conf.
of the Europ.Chapter of the Assoc.
for Computational Linguistics(EACL), pages 347?354, Budapest, Hungary, April.L.
S. Larkey, L. Ballesteros, and M. E. Connell.
2002.Improving stemming for arabic information retrieval:light stemming and co-occurrence analysis.
In Proc.of the 25th annual of the international Associationfor Computing Machinery Special Interest Group onInformation Retrieval (ACM SIGIR), pages 275?282,New York, NY, USA.
ACM Press.Y.
S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Has-san.
2003.
Language model based Arabic word seg-mentation.
In E. Hinrichs and D. Roth, editors, Proc.of the 41st Annual Meeting of the Association for Com-putational Linguistics.Y.
S. Lee.
2004.
Morphological analysis for statisti-cal machine translation.
In D. M. Susan Dumais andS.
Roukos, editors, HLT-NAACL 2004: Short Papers,pages 57?60, Boston, Massachusetts, USA, May 2 -May 7.
Association for Computational Linguistics.F.
J. Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In Proc.
of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 295?302, Philadelphia, PA, July.F.
J. Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proc.
of the 41th AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 160?167, Sapporo, Japan, July.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proc.
of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 311?318, Philadelphia, PA, July.T.
Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, andS.
Yamamoto.
2002.
Toward a broad-coverage bilin-gual corpus for speech translation of travel conver-sations in the real world.
In Proc.
of the Third Int.Conf.
on Language Resources and Evaluation (LREC),pages 147?152, Las Palmas, Spain, May.R.
Zens and H. Ney.
2004.
Improvements in phrase-based statistical machine translation.
In Proc.
of theHuman Language Technology Conf.
(HLT-NAACL),pages 257?264, Boston, MA, May.R.
Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,J.
Xu, Y. Zhang, and H. Ney.
2005.
The RWTHphrase-based statistical machine translation system.
InProceedings of the International Workshop on SpokenLanguage Translation (IWSLT), pages 155?162, Pitts-burgh, PA, October.22
