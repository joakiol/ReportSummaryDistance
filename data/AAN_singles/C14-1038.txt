Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 387?397, Dublin, Ireland, August 23-29 2014.Author Verification Using Common N-Gram Profiles of Text DocumentsMagdalena Jankowska and Evangelos Milios and Vlado Ke?seljFaculty of Computer Science, Dalhousie University6050 University AvenueHalifax, NS B3H 4R2, Canada{jankowsk, eem, vlado}@cs.dal.caAbstractAuthorship verification is the problem of answering the question whether or not a sample textdocument was written by a specific person, given a few other documents known to be authored bythem.
We propose a proximity based method for one-class classification that applies the CommonN-Gram (CNG) dissimilarity measure.
The CNG dissimilarity (Ke?selj et al., 2003) is based onthe differences in the frequencies of n-grams of tokens (characters, words) that are most commonin the considered documents.
Our method utilizes the pairs of most dissimilar documents amongdocuments of known authorship.
We evaluate various variants of the method in the setting ofa single classifier or an ensemble of classifiers, on a multilingual authorship verification corpusof the PAN 2013 Author Identification evaluation framework.
Our method yields competitiveresults when compared to the results achieved by the participants of the PAN 2013 competitionon the entire set, as well as separately on two subsets ?
English and Spanish ones ?
out of thethree language subsets of the corpus.1 IntroductionThe task of computational detection of who wrote a given text is a widely studied linguistic and machinelearning problem with applications in domains such as forensics, security, criminal and civil law, or liter-ary research.
The authorship verification problem is a type of such a computational authorship analysistask, in which, given a set of documents written by one author, and a sample document, we are askedwhether or not this sample document was written by this given author.
This is different from the moretraditional problem of deciding who among a finite number of candidate authors for which we are givensample writings, wrote a document in question, and, albeit more difficult, is often considered to betterreflect the real-life problems related to authorship detection (Koppel et al., 2012).We describe our one-class proximity based classification method and evaluate it on the multilingualdataset of the Authorship Identification competition task of PAN 2013 (evaluation lab on uncoveringplagiarism, authorship, and social software misuse) (Juola and Stamatatos, 2013).During the competition, to which a variant of our method has been submitted (Jankowska et al., 2013),it yielded ranking 5th (joint) out of 18 with respect to the accuracy, and 1st rank out of 10 in the secondaryranking based on the area under the ROC curve (AUC), which evaluates the ordering of instances by theconfidence score.
In this paper we show some further experiments on how a different way of tuningthe classifier parameters, using solely the training dataset of the competition, as well as an ensembleof classifiers based on our method, without any parameter tuning, leads to competitive accuracy resultswhile still achieving high AUC values.2 Related WorkThe author analysis has been studied extensively in the context of the authorship attribution problem, inwhich there is a small set of candidate authors out of which the author of a questioned document is toThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/387be selected.
There are several papers (Stamatatos, 2009; Juola, 2008; Koppel et al., 2009) presentingexcellent surveys of this area.The two main categories (Stamatatos, 2009) of solutions for the problem are similarity based ap-proaches, in which a classification is performed in a Neigherst Neighbour scheme, attributing a sampletext to the author whose writing is most similar according to some measure, and machine-learning basedapproaches, in which each document by an author is treated as a data sample within a class, and a super-vised classifier is trained on these data.A more limited research has been performed on an open-set variant on this problem, in which it ispossible that none of the candidate authors wrote a document in question, with authorship verificationbeing the extreme case of an open-set problem with only one candidate.
The ?unmasking method?
forauthorship verification (Koppel and Schler, 2004) is successful for novel-length texts.
This approach,similarly as our method, falls into a category of intrinsic methods (Juola and Stamatatos, 2013); it usesonly the documents in question, without constructing classes of other authors.
The ensemble of one-classclassifiers (Halvani et al., 2013), which achieved high accuracy at the PAN 2013 Author Identificationcompetition, is also an example of such an intrinsic method.
It varies from our approach by using adifferent scheme of creating the dissimilarity between an unknown document and the known authorshipset of texts, based on the Nearest Neighbour technique (Tax, 2001), as well as by a different distancemeasure and features used.Another way of approaching the author verification problem is to cast it into a binary or multi-classclassification, by creating a class or classes of other authors.
The ?imposters?
method (Koppel and Win-ter, 2014) generates a very large set of texts by authors that did not write the questioned document,to transform the problem into a open-set author attribution problem with many candidates, handledby an ensemble-based similarity method (Koppel et al., 2011).
A modified version of the impostersmethod (Seidman, 2013) achieved first ranking in the PAN 2013 Authorship Identification competition.The method (Veenman and Li, 2013), which achieved the highest accuracy on the English set in thiscompetition, is also of such an extrinsic type; its first step is a careful selection of online documentssimilar to the ones in the problems.
The method (Ghaeini, 2013), which produces competitive orderingof verification instances, uses weighted k-NN approach using classes of other authors created from otherverification instances.3 MethodologyThe formulation of the authorship verification task for the Author Identification Task at PAN 2013 is thefollowing: ?Given a set of documents (no more than 10, possibly only one) by the same author, is anadditional (out-of-set) document also by that author??
(Juola and Stamatatos, 2013).We approach this task with an algorithm based on the idea of proximity based methods for one-classclassification.
In one-class classification framework, an object is classified as belonging or not belong-ing to a target class, while only sample examples of objects from the target class are available duringthe training phase.
Our method resembles the idea of the k-centers algorithm for one-class classifica-tion (Ypma et al., 1998; Tax, 2001), with k being equal to the number of all training documents in thetarget set (i.e., written by the given author).
The k-centers algorithm is suitable for cases when thereare many data points from the target class; it uses equal radius sphere boundaries around the target datapoints and compares the sample document to the closest such centre.
We propose a different classifica-tion condition, described below, utilizing the pairs of most dissimilar documents within the set of knowndocuments.Let A = {d1, ..., dk}, k ?
2, be the input set of documents written by a given author, which we willcall known documents.
If only one known document is provided, we split it in half and treat these twochunks as two known documents.
Let u be the input sample document, of which the authorship we are toverify, that is return the answer ?Yes?
or ?No?
to the posed question whether it was written by the givenauthor.Our algorithm calculates for each known document di, i = 1, 2, ..., k, the maximum dissimilar-ity between this document and all other known documents: Dmax(di, A), as well as the dissimilar-388ity between this document and the sample document u: D(di, u), and finally the dissimilarity ratior(di, u, A) =D(di,u)Dmax(di,A)(and thus r(di, u, A) < 1 means that there exists a known document moredissimilar to dithan u, while r(di, u, A) > 1 means that all the known documents are more similar todithan u).
The average M(u,A) of the dissimilarity ratio over all known documents d1, d2, ..., dkfromA, is the subject of the thresholding: the sample u is classified as written by the same person as theknown documents if and only if M(u,A) is at most equal to a selected threshold ?.
Notice that in thisframework the dissimilarity between the documents does not need to be a metric distance, i.e., it doesnot need to fulfil the triangle inequality (as is the case for the dissimilarity measure we choose).For the dissimilarity measure between documents we use the Common N-Gram (CNG) dissimilarity;proposed by Ke?selj et al.
(2003); this dissimilarity (or its variants) used in the Nearest Neighbour classi-fication scheme (Common N-gram classifier) was successfully applied to authorship classification tasks(Ke?selj et al., 2003; Juola, 2008; Stamatatos, 2007).
The CNG dissimilarity is based on the differencesin the usage frequencies of the most common n-grams of tokens (usually characters, but possibly othertokens) of the documents.
Each document is represented by a profile: a sequence of the most commoncharacter n-grams (strings of characters of the given length n from the document) coupled with theirfrequencies (normalized by the length of the document).
The dissimilarity between two documents ofthe profiles P1and P2is defined as follows:D(P1, P2) =?x?(P1?P2)(fP1(x)?
fP2(x)fP1(x)+fP2(x)2)2(1)where x is a character n-gram from the union of two profiles, and fPi(x) is the normalized frequency ofthe n-gram x in the the profile Pi, i = 1, 2 (fPi(x) = 0 whenever x does not appear in the profile Pi).The parameters of the dissimilarity are the length of the n-grams n and the length of the profile L. Asour method is based on the ratios of dissimilarities between documents, we take care that the documentsin a given problem are always represented by profiles of the same length.
We experiment with two waysof selecting the length of the profiles.
In the dynamic-length variant, the length of profiles is selectedseparately for each problem, based on the number of n-grams in the documents in the given instance(parametrized as a fraction f of all n-grams of the document that contains the least number of them).
Inthe fixed-length variant, we use a selected fixed length L of profiles.
For a one-class classifier we needto select two parameters defining the features used for dissimilarity (length of the n-grams n, and eitherthe fixed length L of a profile, or the fraction f defining the profile length), and the parameter ?
(forclassifying by thresholding the average dissimilarity ratio M ).We linearly scale the measure M to represent it as a confidence score in the range from 0 (the highestconfidence in the answer ?No? )
to 1 (the highest confidence in the answer ?Yes?
), with the answer ?Yes?given if and only if the confidence score is at least 0.5.
The value of M equal to ?
is transformed tothe score 0.5, values greater than ?
to the scores between 0 and 0.5, and values less than ?
to the scoresbetween 0.5 and 1 (a cutoff of 0.1 is applied, , i.e.
all values of M(u,A) < ?
?
cutoff are mapped to thescore 1, and all values of M(u,A) > ?
+ cutoff are mapped to the score 0).4 Training and test datasetsWe leverage the evaluation framework of the PAN 2013 competition task of Author Identification (Juolaand Stamatatos, 2013), the datasets of which were carefully created for authorship verification, witheffort made to match within each problem instance the texts by the same genre, register, theme and timeof writing.
The dataset consists of English, Greek and Spanish subsets.
In each instance, the numberof documents of known authorship is not greater than 10 (possibly only one).
The dataset is dividedinto the training set pan13-ai-train and the test set pan13-ai-test.
The training set was madeavailable for the participants before the competition; the test set was used to evaluate the submissionsand subsequently published (PAN, 2013).To enrich the training dataset for our competition submission, we also compiled ourselves two ad-ditional datasets using existing sets for other authorship identification tasks.
mod-pan12-aa-EN is389an English author verification set compiled from the fiction corpus for the Traditional Authorship At-tribution sub task of the PAN 2012 competition (PAN, 2012; Juola, 2012).
mod-Bpc-GR is a Greekauthor verification set compiled from the Greek dataset of journal articles (Stamatatos et al., 2000).
It isimportant to note that these sets are different from the competition dataset in that we did not attempt tomatch the theme or time of writing of the texts.Table 1 presents characteristics of the datasets.pan13-ai-traintotal English Spanish Greeknumber of problems 35 10 5 20mean of the known document number per problem 4.4 3.2 2.4 5.5mean length of documents in words 1226 1038 653 1362genre textbooks editorials, fiction articlespan13-ai-testtotal English Spanish Greeknumber of problems 85 30 25 30mean of the known document number per problem 4.1 4.2 3.0 4.9mean length of documents in words 1163 1043 890 1423genre textbooks editorials, fiction articlesmod-pan12-aa-ENtotal: Englishnumber of problems 22mean of the known document number per problem 2.0mean length of documents in words 4799genre fictionmod-Bpc-GRtotal: Greeknumber of problems 76mean of the known document number per problem 2.5mean length of documents in words 1120genre articlesTable 1: Characteristics of datasets used in our authorship verification experiments.5 Evaluation measuresIn our experiments we use two measures of evaluation, based on the measures proposed for the PAN 2013competition.
The accuracy is the fraction of all problems that have been answered correctly.
The AUCmeasure is the area under the ROC curve based on the confidence scores.
It is the nature of applicationsof authorship verification, such as forensics, that makes the confidence score and not only the binaryanswer, an important aspect of a solution (Gollub et al., 2013).For our method accuracy is equivalent to the measure that was used in the competition for the mainevaluation.
This measure is F1, defined based on the fact that in the competition it was allowed towithdraw an answer (i.e., use an ?I do not know?
option).
Precision and recall were defined as follows:recall =#correct answers#problems, precision =#correct answers#answers, and F1is the harmonic mean of precision andrecall.
For any method that, as our method, provides the answer ?Yes?
or ?No?
for all problem instances,the accuracy and F1are equivalent.3906 Types of classifiersA single classifier of our method requires two parameters defining the features to be used to representa document (the length of an n-gram and the length of a profile), as well as a selection of the thresholdfor the dissimilarity for the classification decision.
We tune and evaluate four version of such singleclassifiers.
Combining many such one-class classifiers, each using different combination of featuresdefining parameters, into one ensemble, allows to remove or mitigate the parameter tuning.
We describethe creation and the evaluation of four types of ensembles.Table 2 reports the considered space for feature defining parameters.
On a training set, for a givencombination of feature defining parameters (n,L) or (n,f ), we use the accuracy at the optimal threshold(a threshold ?
that maximizes the accuracy), as a measure of performance for these parameters.Parametersnlength of n-gramsL# of n-grams: profile length (fixed-length)f fraction of n-grams for profile length (dynamic-length)?threshold for classification?2+threshold for classification if at least 2 known documents are given?1threshold for classification if only one known document is givenSpace of considered parametersn for character n-grams{3, 4, ..., 9, 10}n for word n-grams{1, 2, 3}L{200, 500, 1000, 1500, 2000, 2500, 3000}f{0.2, 0.3, ..., 0.9, 1}single classifiers ensemblesEnglish Spanish Greek English Spanish GreekvD1 n 6 7 10 eC type characterf 0.75 (n,L) all in the considered space?
1.02 1.005 1.002 ?
1vF1 n 6 7 eW type wordL 2000 2000 (n,L) all in the considered space?2+1.02 1.008 ?
1?11.06 1.04 eCW type character, wordvF2 n 7 3 9 (n,L) all in the considered spaceL 3000 2000 3000 ?
1?2+1.014 1.014 0.997 eCW type character, word?11.056 1.126 1.060 (n,L) selected based on training datavD2 n 7 3 9 (61) (75) (43)f 0.8 0.6 0.8 ?
1?2+1.013 1.00530207 0.9966?11.053 1.089 1.059Table 2: Parameters for four variants of single one-class classifiers and four ensembles of one-classclassifiers based on our method.6.1 Single classifiersFor single character n-gram classifiers, we tuned the parameters for each language separately on trainingdata, by selecting feature defining parameters based on their performance, and selecting the thresholds391to correspond to the optimal thresholds.
Table 2 reports the parameters of four variants of single classi-fiers.
We include our two submissions to the PAN 2013 Authorship Identification competition: the finalsubmission vF1 and the preliminary submission vD1.
The other two classifiers were tuned and testedafter the competition.Our preliminary submission vD1 (Table 2) is tuned on pan13-ai-train, with f chosen ad-hoc.This is the only classifier among the reported variants that does not use a preprocessing of truncation ofall documents in a given problem instance to the length of the shortest document, which tend to increasethe accuracy for cases of a significant difference in the length of documents.For tuning of parameters of the final submission vF1 (Table 2) we use not only pan13-ai-train,but also additional training sets mod-pan12-aa-EN and mod-Bpc-GR.
We also introduce two thresh-old values: one for cases when there are at least two known documents, and another one for the caseswhen there is only one known document (which has to be divided in two).
The intuition behind this dou-ble threshold approach is that when there is only one known document, the two halves of it can be moresimilar to each other than in other cases.
After the parameters are selected based on subsets of trainingsets with only these problems that contain at least two known documents, the additional threshold isselected based on the optimal threshold on a modified ?1-only?
training set, from the problem of whichall known documents except of a random single one is removed.
For Spanish, with only three traininginstances with more than one known document, we use the same parameters as for English.For tuning of vF2 and vD2 (Table 2) we use only competition training data, without the additionalcorpora used for vF1.
Feature parameters are selected based on the performance on the subsets contain-ing at least two known documents, and on the ?1-only?
modified sets (which allows us to use the Spanishtraining set for tuning the Spanish classifiers).6.2 Ensembles of classifiersWe test ensembles of single one-class classifiers based on our method, with the ensemble combininganswers of the classifiers, and each classifier using different set of features.
An important advantage ofan ensemble is the alleviation of the problem of tuning the parameters.
Each classifier uses a differentcombination of parameters n and L defining the features.
And as many classifiers are used, instead oftuning the threshold of a single classifier based on some training data, the threshold of each classifieris set to some fixed value, with 1 being a natural choice, as it corresponds to checking whether or notthe unknown document is (on average) less similar to each given known document than the author?sdocument that is most dissimilar to this given known document.We test majority voting and voting weighted by the confidence scores of single classifiers.
For eachensemble we combine answers of the classifiers in order to obtain the confidence score of the ensemble.For majority voting the confidence score of the ensemble is the ratio of the number of classifiers thatoutput ?Yes?
to the total number of classifiers, the confidence score of the weighted voting is the averageof the confidence scores of the single classifiers.We experiment with n-grams being characters (utf8-encoded) and words (converted to uppercase).Table 2 summarize the ensembles.
The ensemble eC is of all character n-gram classifiers in our spaceof considered parameters n and L; eW is of all word n-gram classifiers; eCW is of all classifiers of eCand eW.
These ensembles do not use any training data.
We also create a classifier eCW sel (Table 2),which is a subset of the classifiers of eCW, selected based on the performance of the single classifierson the training data of the competition.
For each language separately, we remove classifiers that on thetraining data achieved lowest accuracies at their respective optimal thresholds, while keeping at least halfof the character based classifiers and at least half of the word based classifiers.
(For Spanish, eCW seland eCW differ just by one classifier: the only one that on the small Spanish training set has the optimalaccuracy less than 1.
)7 ResultsThe accuracy and the area under the ROC curve (AUC) values achieved by the variants of our methodon the PAN 2013 Author Identification test dataset are presented in Table 3.
The table states also the392best PAN 2013 competition results of other participants1(that is the results of these participants thatachieved the highest accuracy or AUC on any (sub)set).
There were 17 other participants for which thereare accuracy (or F1) results, 9 of which submitted also confidence scores evaluated by AUC.PAN 2013 Author Identification test datasetF1= accuracy except for Ghaeini,2013 AUCall English Spanish Greek all English Spanish Greeksingle classifiersvD1 0.718 0.733 0.760 0.667 0.790 0.837 0.846 0.718vF1 0.682 0.733 0.720 0.600 0.793 0.839 0.859 0.711vD2 0.729 0.767 0.760 0.667 0.805 0.850 0.936 0.704vF2 0.753 0.767 0.880 0.633 0.810 0.844 0.885 0.664ensembles of classifierseC majority 0.729 0.800 0.840 0.567 0.754 0.777 0.833 0.620weight 0.729 0.833 0.800 0.567 0.764 0.830 0.859 0.582eW majority 0.718 0.733 0.720 0.700 0.763 0.830 0.805 0.700weight 0.741 0.767 0.760 0.700 0.822 0.886 0.853 0.782eCW majority 0.800 0.833 0.840 0.733 0.755 0.817 0.821 0.633weight 0.741 0.800 0.840 0.600 0.780 0.842 0.853 0.622eCW sel majority 0.800 0.833 0.840 0.733 0.778 0.826 0.814 0.682weight 0.788 0.800 0.840 0.733 0.805 0.857 0.853 0.687boxed values: best competition results of other PAN 2013 Author Identification participantsSeidman,2013 0.753 0.800 0.600 0.833 0.735 0.792 0.583 0.824Veenman and Li,2013 ?
0.800 ?
?
?
?
?
?Halvani et al.,2013 0.718 0.700 0.840 0.633 ?
?
?
?Ghaeini,2013 0.606 0.691 0.667 0.461 0.729 0.837 0.926 0.527Table 3: Area under the ROC curve (AUC) and F1(which is equal to accuracy for all algorithms exceptfor (Ghaeini, 2013)) on the test dataset of PAN 2013 Author Identification competition task.
Results ofvariants of our method compared with competition results of those among other competition participantsthat achieved the highest value of any evaluation measure on any (sub)set.
The highest result in anycategory is bold; the highest result by other competition participants in any category is boxed.All variants of our method perform better on the English and Spanish subset than on the Greek one,both in terms of the accuracy and in terms of AUC.
On the Greek subset they are all outperformed byother competition participant(s).
This is most likely due to the fact that the Greek subset was created in away that makes it especially difficult for algorithms that are based on CNG character-based dissimilarity(Juola and Stamatatos, 2013), by using a variant of CNG dissimilarity for the character 3-grams in orderto select difficult cases.
This particularity of the set may also be the reason why the ensemble eC ofcharacter n-gram classifiers performed worse than other methods on this set.The variants of our method are competitive in terms of the ordering of the verification instances ac-cording to the confidence score as measured by AUC.
During the competition, our final submission vF1achieved the first ranking according to the AUC on the entire set, the highest AUC on the English subset,and the second-highest AUC values on the Spanish and Greek subset, out of 10 participants that submit-1The results of our methods are on the published competition dataset.
The results by other participants are the publishedcompetition results.
The actual competition evaluation set for Spanish may have some text in a different encoding then thepublished set; our final submission method vF1 yielded on it a different result than on the published dataset.393ted confidence scores.
All variants of our method perform better than any other competition participanton the entire set.
On the English subset the single classifiers and the ensembles with weighted votinghave AUC above 0.8, and out of those only eC has AUC lower than the best result by other participants.On the Spanish subset all variants of our method achieved AUC above 0.8, with vD2 achieving AUChigher than the best competition result on this subset.In terms of overall accuracy on the entire set, the ensembles combining character and word basedclassifiers: eCW with majority voting and eCW sel with both types of voting, achieve accuracy higherthen the best overall accuracy in the competition.
They also match or surpass the best competitionaccuracy on the English subset, and match the best competition accuracy on the Spanish subset.
Thehighest accuracy on the English subset was achieved by eC with weighted voting, eCW with majorityvoting, and eCW sel with majority voting (higher than the best competition result).
vF2 yields on theSpanish subset accuracy higher than the best competition result.For the ensembles of classifiers, on the English and Spanish subsets, the AUC for voting weighted bythe confidence scores are higher than the AUC for the majority voting, but not so on the Greek subset.This is consistent with the fact that on the Greek subset the confidence scores for single classifier variantsyield worse ordering (AUC) than on other sets.
Creation of eCW sel by removing from the ensembleeCW the classifiers that perform worst on the training data improves the Greek results, and slightly theEnglish results.We tested the statistical significance of accuracy differences between all pairs of accuracies reportedin Table 3 by the exact binomial McNemar?s test (Dietterich, 1998).
Only few of these differences arestatistically significant.
On the entire set these are: the difference between the accuracy of eCW withmajority voting and of eC with majority voting, vD1 and vF1, as well as the difference between theaccuracies of eCW sel with weighted voting and of vF1.
On the Greek subset, this is the differencebetween the accuracies of the submission (Seidman, 2013) and the lower accuracy of eC with weightedvoting.English mod-pan12-aa-EN Greek mod-Bpc-GRaccuracyAUCaccuracyAUCvD1 0.545 0.649 0.605 0.661vD2 0.727 0.826 0.566 0.698vF2 0.773 0.843 0.618 0.709eC majority 0.636 0.843 0.658 0.694weighted 0.682 0.806 0.671 0.703eW majority 0.636 0.674 0.750 0.757weighted 0.727 0.736 0.737 0.749eCW majority 0.636 0.785 0.737 0.725weighted 0.682 0.818 0.711 0.719eCW sel majority 0.636 0.789 0.750 0.742weighted 0.682 0.826 0.737 0.737Table 4: Accuracy and area under ROC curve (AUC) of our method on other English and Greek datasets.The sets were compiled by ourselves for the purpose of enriching training domain for other variant ofour classifier.
The highest result in any category is bold.The datasets mod-pan12-aa-EN and mod-Bpc-GR were compiled by ourselves from other au-thorship attribution sets for the purpose of enriching the training corpora for our final submission vF1.The comparison between results on the English and Greek subsets of vF1 with the results of vF2 (forwhich these additional sets were not used), shows that vF2 achieved better results on English data.
whilevF1 has higher AUC on Greek data.Though these additional sets were not created specifically for authorship verification evaluation, we394examine the results of our methods on these sets (with the exception of vF1, which is tuned on them).We present the results in Table 4. vD1 performs poorly on mod-pan12-aa-EN.
This is in part due tothe fact that in this set the documents in a given problem instance can differ significantly with respect tothe length, and the variant vD1 does not use the preprocessing of truncation all files withing a problemto the same length.
The variants vD2 and vF2 (which apply this truncation) yielded accuracy andAUC similar in value to the ones achieved on the PAN 2013 English subset.
The ensembles containingcharacter n-gram classifiers yielded similar AUC on mod-pan12-aa-EN as on the PAN2013 Englishsubset, close in value to 0.8.
But their accuracies are distinctly lower than the results on the Englishcompetition subset, with values below 0.7 (for each such an ensemble, vast majority of the misclassifiedinstances are false negatives: cases classified as not written by the same person when in fact they are).
Formod-Bpc-GR the single classifiers (with parameters tuned on the competition Greek subset) performrather poorly, with results similar but lower in values than the results yielded on the competition Greektest set.
The ensembles containing word n-gram based classifiers perform better than the ensemblescontaining only the character n-gram classifiers, yielding both AUC and accuracy in the range of 0.71 ?0.75.8 Future WorkIt will be of interest to investigate the relation between the performance of our method and the numberand the length of the considered texts.
An interesting direction indicated by results of our experiments isalso the analysis of the role of word n-grams and character n-grams for authorship verification dependingon the genre of the texts, and on the topical similarity between the documents.9 ConclusionsWe present our proximity based one-class classification method for authorship verification.
The methoduses for each document of known authorship the most dissimilar document of the same author, and exam-ines how much more or less similar is the questioned document.
We use Common N-Gram dissimilaritybased on differences in frequencies of character and word n-grams.We evaluate our method on the set of PAN 2013 Authorship Identification competition.
One variantof our method was submitted to the competition.
The ordering by scores indicating the confidence thatthe documents were written by the same person, yielded by our method, and evaluated by area underROC curve (AUC), is competitive with respect to other participants of the competition, overall, and onthe English and Spanish subsets.
On the entire set, AUC by each variant of our method is higher than thebest result by other participants.
In terms of accuracy, the method also performs better on the Englishand Spanish subsets of the dataset, and worse on the Greek one.
An ensemble combining character basedclassifiers and word based classifiers yields the best accuracy, surpassing the best competition result onthe entire set and on the English subset, while matching the best competition result on the Spanish subset.As all proximity based one-class classification algorithms, our method relies on a selected threshold onthe proximity between the questioned text and the set of documents of known authorship.
Additionally,a single classifier requires two parameters defining the features representing documents.
Ensembles ofclassifiers allow to alleviate the parameter tuning, by using many classifiers for many combinations offeature defining parameters, with a threshold fixed to 1 (a natural, albeit arbitrary, value).AcknowledgementsThis research was funded by a contract from the Boeing Company, Killam Predoctoral Scholarship, anda Collaborative Research and Development grant from the Natural Sciences and Engineering ResearchCouncil of Canada.ReferencesThomas G. Dietterich.
1998.
Approximate statistical tests for comparing supervised classification learning algo-rithms.
Neural Computation, 10:1895?1923.395M.R.
Ghaeini.
2013.
Intrinsic Author Identification Using Modified Weighted KNN - Notebook for PAN at CLEF2013.
In Pamela Forner, Roberto Navigli, and Dan Tufis, editors, CLEF 2013 Evaluation Labs and Workshop ?Working Notes Papers, September.Tim Gollub, Martin Potthast, Anna Beyer, Matthias Busse, Francisco M. Rangel Pardo, Paolo Rosso, EfstathiosStamatatos, and Benno Stein.
2013.
Recent trends in digital text forensics and its evaluation - plagiarismdetection, author identification, and author profiling.
In Pamela Forner, Henning M?uller, Roberto Paredes, PaoloRosso, and Benno Stein, editors, CLEF, volume 8138 of Lecture Notes in Computer Science, pages 282?302.Springer.Oren Halvani, Martin Steinebach, and Ralf Zimmermann.
2013.
Authorship Verification via k-Nearest NeighborEstimation - Notebook for PAN at CLEF 2013.
In Pamela Forner, Roberto Navigli, and Dan Tufis, editors,CLEF 2013 Evaluation Labs and Workshop ?
Working Notes Papers, September.Magdalena Jankowska, Vlado Ke?selj, and Evangelos Milios.
2013.
Proximity Based One-class Classification withCommon N-Gram Dissimilarity for Authorship Verification Task - Notebook for PAN at CLEF 2013.
In PamelaForner, Roberto Navigli, and Dan Tufis, editors, CLEF 2013 Evaluation Labs and Workshop ?
Working NotesPapers, September.Patrick Juola and Efstathios Stamatatos.
2013.
Overview of the Author Identification Task at PAN 2013.
InPamela Forner, Roberto Navigli, and Dan Tufis, editors, CLEF 2013 Evaluation Labs and Workshop ?
WorkingNotes Papers, September.Patrick Juola.
2008.
Authorship attribution.
Foundations and TrendsR?
in Information Retrieval, 1(3):233?334.Patrick Juola.
2012.
An overview of the traditional authorship attribution subtask.
In Pamela Forner, JussiKarlgren, and Christa Womser-Hacker, editors, CLEF (Online Working Notes/Labs/Workshop).Vlado Ke?selj, Fuchun Peng, Nick Cercone, and Calvin Thomas.
2003.
N-gram-based author profiles for au-thorship attribution.
In Proceedings of the Conference Pacific Association for Computational Linguistics, PA-CLING?03, pages 255?264, Dalhousie University, Halifax, Nova Scotia, Canada, August.Moshe Koppel and Jonathan Schler.
2004.
Authorship verification as a one-class classification problem.
In Pro-ceedings of the 21st International Conference on Machine Learning, ICML ?04, page 489?495, Banf, Alberta,Canada, July.
ACM.Moshe Koppel and Yaron Winter.
2014.
Determining if two documents are written by the same author.
Journal ofthe Association for Information Science and Technology, 65(1):178?187.Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009.
Computational methods in authorship attribution.Journal of the American Society for Information Science and Technology, 60(1):9?26.Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2011.
Authorship attribution in the wild.
LanguageResources and Evaluation, 45(1):83?94, March.Moshe Koppel, Jonathan Schler, Shlomo Argamon, and Yaron Winter.
2012.
The ?Fundamental Problem?
ofAuthorship Attribution.
English Studies, 93(3):284?291.PAN.
2012.
Dataset of PAN 2012, Author Identification task.
http://www.uni-weimar.de/medien/webis/research/events/pan-12/pan12-web/authorship.html.
Accessed on Apr 2, 2013.PAN.
2013.
Dataset of PAN 2013, Author Identification task.
http://www.uni-weimar.de/medien/webis/research/events/pan-13/pan13-web/author-identification.html.
Accessedon Oct 8, 2013.Shachar Seidman.
2013.
Authorship Verification Using the Impostors Method - Notebook for PAN at CLEF2013.
In Pamela Forner, Roberto Navigli, and Dan Tufis, editors, CLEF 2013 Evaluation Labs and Workshop ?Working Notes Papers, September.Efstathios Stamatatos, George Kokkinakis, and Nikos Fakotakis.
2000.
Automatic text categorization in terms ofgenre and author.
Computational Linguistics, 26(4):471?495, December.Efstathios Stamatatos.
2007.
Author identification using imbalanced and limited training texts.
In Proceedingof the 18th International Workshop on Database and Expert Systems Applications, DEXA?07, pages 237?241,Regensburg, Germany, September.Efstathios Stamatatos.
2009.
A survey of modern authorship attribution methods.
Journal of the American Societyfor Information Science and Technology, 60(3):538?556.396David Tax.
2001.
One Class Classification.
Concept-learning in the absence of counter-examples.
Ph.D. thesis,Delft University of Technology, June.Cor J. Veenman and Zhenshi Li.
2013.
Authorship Verification with Compression Features.
In Pamela Forner,Roberto Navigli, and Dan Tufis, editors, CLEF 2013 Evaluation Labs and Workshop ?
Working Notes Papers,September.Alexander Ypma, Er Ypma, and Robert P.W.
Duin.
1998.
Support objects for domain approximation.
In Pro-ceedings of International Conference on Artificial Neural Networks, pages 2?4, Skovde, Sweden, September.Springer.397
