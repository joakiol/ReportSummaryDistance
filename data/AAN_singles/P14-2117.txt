Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 718?724,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsSemantic Consistency: A Local Subspace Based Method for DistantSupervised Relation ExtractionXianpei Han    and       Le SunState Key Laboratory of Computer ScienceInstitute of Software, Chinese Academy of SciencesHaiDian District, Beijing, China.
{xianpei, sunle}@nfs.iscas.ac.cnAbstractOne fundamental problem of distant supervi-sion is the noisy training corpus problem.
Inthis paper, we propose a new distant supervi-sion method, called Semantic Consistency,which can identify reliable instances fromnoisy instances by inspecting whether an in-stance is located in a semantically consistentregion.
Specifically, we propose a semanticconsistency model, which first models the lo-cal subspace around an instance as a sparselinear combination of training instances, thenestimate the semantic consistency by exploit-ing the characteristics of the local subspace.Experimental results verified the effectivenessof our method.1 IntroductionRelation extraction aims to identify and categorizerelations between pairs of entities in text.
Due tothe time-consuming annotation process, one criti-cal challenge of relation extraction is the lack oftraining data.
To address this limitation, a promis-ing approach is distant supervision (DS), whichcan automatically gather labeled data by heuristi-cally aligning entities in text with those in aknowledge base (Mintz et al, 2009).
The under-lying assumption of distant supervision is thatevery sentence that mentions two entities is likelyto express their relation in a knowledge base.Relation Instance LabelS1: Jobs was the founder of Apple Founder-of, CEO-ofS2: Jobs joins Apple Founder-of, CEO-ofFigure 1.
Labeled instances by distant supervi-sion, using relations CEO-of(Steve Jobs, AppleInc.)
and Founder-of(Steve Jobs, Apple Inc.)The distant supervision assumption, unfortu-nately, can often fail and result in a noisy trainingcorpus.
For example, in Figure 1 DS assumptionwill wrongly label S1 as a CEO-of instance and S2as instance of Founder-of and CEO-of.
The noisytraining corpus in turn will lead to noisy extrac-tions that hurt extraction accuracy (Riedel et al,2010).Figure 2.
The regions the two instances in Figure1 located, where: 1) S1 locates in a semanticallyconsistent region; and 2) S2 locates in a semanti-cally inconsistent regionTo resolve the noisy training corpus problem,this paper proposes a new distant supervisionmethod, called Semantic Consistency, which caneffectively identify reliable instances from noisyinstances by inspecting whether an instance is lo-cated in a semantically consistent region.
Figure 2shows two intuitive examples.
We can see that,semantic consistency is an effective way to iden-tify reliable instances.
For example, in Figure 2 S1is highly likely a reliable Founder-of instance be-cause its neighbors are highly semantically con-sistent, i.e., most of them express the same rela-tion type ?
Founder-of.
On contrast S2 is highlylikely a noisy instance because its neighbors aresemantically inconsistent, i.e., they have a diverserelation types.
The problem now is how to modelthe semantic consistency around an instance.To model the semantic consistency, this paperproposes a local subspace based method.
Specifi-cally, given sufficient training instances, ourmethod first models each relation type as a linearsubspace spanned by its training instances.
Then,the local subspace around an instance is modeledand characterized by seeking the sparsest linearcombination of training instances which can re-construct the instance.
Finally, we estimate the se-mantic consistency of an instance by exploitingthe characteristics of its local subspace.+++++?????S2?
S1 ?+:  CEO-of?
:  Founder-of+?+??
?+    :  Manager-of:  CTO-of718This paper is organized as follows.
Section 2reviews related work.
Section 3 describes the pro-posed method.
Section 4 presents the experiments.Finally Section 5 concludes this paper.2 Related WorkThis section briefly reviews the related work.
Cra-ven and Kumlien (1999), Wu et al (2007) andMintz et al(2009) were several pioneer work ofdistant supervision.
One main problem of DS as-sumption is that it often will lead to false positivesin training data.
To resolve this problem, Bunescuand Mooney (2007), Riedel et al (2010) and Yaoet al (2010) relaxed the DS assumption to the at-least-one assumption and employed multi-in-stance learning techniques to identify wrongly la-beled instances.
Takamatsu et al (2012) proposeda generative model to eliminate noisy instances.Another research issue of distant supervision isthat a pair of entities may participate in more thanone relation.
To resolve this problem, Hoffmannet al (2010) proposed a method which can com-bine a sentence-level model with a corpus-levelmodel to resolve the multi-label problem.Surdeanu et al (2012) proposed a multi-instancemulti-label learning approach which can jointlymodel all instances of an entity pair and all theirlabels.
Several other research issues also havebeen addressed.
Xu et al (2013), Min et al (2013)and Zhang et al (2013) try to resolve the falsenegative problem raised by the incompleteknowledge base problem.
Hoffmann et al (2010)and Zhang et al (2010) try to improve the extrac-tion precision by learning a dynamic lexicon.3 The Semantic Consistency Model forRelation ExtractionIn this section, we describe our semantic con-sistency model for relation extraction.
We firstmodel the subspaces of all relation types in theoriginal feature space, then model and character-ize the local subspace around an instance, finallyestimate the semantic consistency of an instanceand exploit it for relation extraction.3.1 Testing Instance as a Linear Combina-tion of Training InstancesIn this paper, we assume that there exist k distinctrelation types of interest and each relation type isrepresented with an integer index from 1 to k. Forith relation type, we assume that totally ni traininginstances Vi = fvi;1;vi;2; :::;vi;nig have beencollected using DS assumption.
And each instanceis represented as a weighted feature vector, suchas the features used in (Mintz, 2009) or (Surdeanuet al, 2012), with each feature is TFIDF weightedby taking each instance as an individual document.To model the subspace of ith relation type inthe original feature space, a variety of modelshave been proposed to discover the underlyingpatterns of Vi.
In this paper, we make a simple andeffective assumption that the instances of a singlerelation type can be represented as the linearcombination of other instances of the same rela-tion type.
This assumption is well motived in rela-tion extraction, because although there is nearlyunlimited ways to express a specific relation, inmany cases basic principles of economy of ex-pression and/or conventions of genre will ensurethat certain systematic ways will be used to ex-press a specific relation (Wang et al, 2012).
Forexample, as shown in (Hearst, 1992), the IS-A re-lation is usually expressed using several regularpatterns, such as ?such NP as {NP ,}* {(or | and)}NP?
and ?NP {, NP}* {,} or other NP?.Based on the above assumption, we hold manyinstances for each relation type and directly usethese instances to model the subspace of a relationtype.
Specifically, we represent an instance y ofith type as the linear combination of training in-stances associated with ith type:y = ?i;1vi;1 + ?i;2vi;2 + ::: + +?i;nivi;ni   (1)for some scalars , with j = 1, 2, ?,ni.
For ex-ample, we can represent the CEO-of instance?Jobs was the CEO of Apple?
as the following lin-ear combination of CEO-of instances:?
0.8: Steve Ballmer is the CEO of Microsoft?
0.2: Rometty was served as the CEO of IBMFor simplicity, we arrange the given ni training in-stances of ith relation type as columns of a matrixAi = [vi;1;vi;2; :::;vi;ni], then we can write thematrix form of Formula 1 as:y = Aixi                           (2)where xi = [?i;1; :::; ?i;ni] is the coefficient vec-tor.
In this way, the subspace of a relation type isthe linear subspace spanned by its training in-stances, and if we can find a valid xi, we can ex-plain y as a valid instance of ith relation type.3.2 Local Subspace Modelingvia Sparse RepresentationBased on the above model, the local subspace ofan instance is modeled as the linear combinationof training instances which can reconstruct the in-stance.
Specifically, to model the local subspace,we first concatenate the n training instances of allk relation types:A = [A1;A2; :::; Ak]719Then the local subspace around y is modeled byseeking the solution of the following formula:y = Ax                           (3)However, because of the redundancy of train-ing instances, Formula 3 usually has more thanone solution.
In this paper, following the idea in(Wright et al, 2009) for robust face recognition,we use the sparsest solution (i.e., how to recon-struct an instance using minimal training in-stances), which have been shown is both discrimi-nant and robust to noisiness.
Concretely, we seekthe sparse linear combination of training instancesto reconstruct y by solving:(l1) : x?
= arg min kxk1 s.t.
kAx?yk2 ? "
(4)where x= [?1;1; :::;?1;n1; :::;?i;1;?i;2; :::;?i;ni; :::]is a coefficient vector which identifies the span-ning instances of y?s local subspace, i.e., the in-stances whose ??,?
?
0 .
In practice, the trainingcorpus may be too large to direct solve Formula 4.Therefore, this paper uses the K-Nearest-Neigh-bors (KNN) of y (1000 nearest neighbors in thispaper) to construct the training instance matrix Afor each y, and KNN can be searched very effi-ciently using specialized algorithms such as theLSH functions in (Andoni & Indyk, 2006).Through the above semantic decomposition,we can see that, the entries of x can encode theunderlying semantic information of instance y.For ith relation type, let  be a new vectorwhose only nonzero entries are the entries in x thatare associated with ith relation type, then we cancompute the semantic component correspondingto ith relation type as .
In this way atesting instance y will be decomposed into k se-mantic components, with each component corre-sponds to one relation type (with an additionalnoise component ):y= y1 + :::+yi + :::+yk + ?
(5)S1 = 0:8?264wasco-founderof...375+ 0:2?264JobsApplethe...375S2 = 0:1?join...?+ 0:1?join...?+ 0:1?join...?+ ...Figure 3.
The semantic decomposition of the twoinstances in Figure 1Figure 3 shows an example of semantic decom-position.
We can see that, the semantic decompo-sition can effectively summarize the semanticconsistency information of y?s local subspace: ifthe instances around an instance have diverse re-lation types (S2 for example), its information willbe scattered on many different semantic compo-nents.
On contrast if the instances around an in-stance have consistent relation types (S1 for ex-ample), most of its information will concentrateon the corresponding relation type.3.3 Semantic Consistency basedRelation ExtractionThis section describes how to estimate and exploitthe semantic consistency for relation extraction.Specifically, given y?s semantic decomposition:y= y1 + :::+yi + :::+yk + ?we observe that if instance y locates at a semanticconsistent region, then all its information will con-centrate on a specific component yi, with all othercomponents equal to zero vector 0.
However,modeling errors, expression ambiguity and noisyfeatures will lead to small nonzero components.Based on the above discussion, we define the se-mantic consistency of an instance as the semanticconcentration degree of its decomposition:Definition 1(Semantic Consistency).
For an in-stance y, its semantic consistency with ith relationtype is:Consistency(y; i) = kyik2Pi kyik2 + k?k2where Consistency(y, i)  and will be 1.0 ifall information of y is consistent with ith relationtype; on contrast it will be 0 if no information in yis consistent with ith relation type.Semantic Consistency based Relation Ex-traction.
To get accurate extractions, we deter-mine the relation type of y based on both: 1) Howmuch information in y is related to ith type; and 2)its semantic consistency score with ith type, i.e.,whether y is a reliable instance of ith type.To measure how much information in y is re-lated to ith relation type, we compute the propor-tio  of common information between y and yi:sim(y;yi) = y ?
yiy ?
y(6)Then the likelihood for a testing instance y ex-pressing ith relation type is scored by summariz-ing both its information and semantic consistency:rel(y; i) = sim(y;yi)?Consistency(y; i)and y will be classified into ith relation type if itslikelihood is larger than a threshold:rel(y; i) ?
?i                       (7)where  is a relation type specific thresholdlearned from training dataset.Founder-of CEO-ofFounder-of noiseCTO-of720Multi-Instance Evidence Combination.
It isoften that an entity pair will match more than onesentence.
To exploit such redundancy for moreconfident extraction, this paper first combines theevidence from different instances by combingtheir underlying components.
That is, given thematched m instances Y={y1, y2, ?, ym} for an en-tity pair (e1, e2), we first decompose each instanceas yj = yj1 + ::: + yjk + ?, then the entity-pairlevel decomposition y = y1 + :::+yk + ?
is ob-tained by summarizing semantic components ofdifferent instances: yi =P1?j?myji.
Finally, thelikelihood of an entity pair expressing ith relationtype is scored as:rel(Y; i) = sim(y;yi)Consistency(y; i)log(m+1)where  is a score used to encourageextractions with more matching instances.3.4 One further Issue for Distant Supervi-sion: Training Instance SelectionThe above model further provides new insightsinto one issue for distant supervision: training in-stance selection.
In this paper, we select informa-tive training instances by seeking a most compactsubset of instances which can span the whole sub-space of a relation type.
That is, all instances ofith type can be represented as a linear combinationof these selected instances.However, finding the optimal subset of traininginstances is difficult, as there exist 2N possible so-lutions for a relation type with N training instances.Therefore, this paper proposes an approximatetraining instance selection algorithm as follows:1) Computing the centroid of ith relation type asvi = P1?j?ni vi;j2) Finding the set of training instances whichcan most compactly span the centroid bysolving:(l1) : xi = arg min kxk1 s.t.
kAix?
vik2 ?
"3) Ranking all training instances according totheir absolute coefficient weight value ;4) Selecting top p percent ranked instances asfinal training instances.The above training instance selection has twobenefits.
First, it will select informative instancesand remove redundant instances: an informativeinstance will receive a high  value becausemany other instances can be represented using it;and if two instances are redundant, the sparse so-lution will only retain one of them.
Second, mostof the wrongly labeled training instances will befiltered, because these instances are usually notregular expressions of ith type, so they appearonly a few times and will receive a small .4 ExperimentsIn this section, we assess the performance of ourmethod and compare it with other methods.Dataset.
We assess our method using the KBPdataset developed by Surdeanu et al (2012).
TheKBP is constructed by aligning the relations froma subset of English Wikipedia infoboxes against adocument collection that merges two distinctsources: (1) a 1.5 million documents collectionprovided by the KBP shared task(Ji et al, 2010; Jiet al, 2011); and (2) a complete snapshot of theJune 2010 version of Wikipedia.
Totally 183,062training relations and 3,334 testing relations arecollected.
For tuning and testing, we used thesame partition as Surdeanu et al (2012): 40 que-ries for development and 160 queries for formalevaluation.
In this paper, each instance in KBP isrepresented as a feature vector using the featuresas the same as in (Surdeanu et al, 2012).Baselines.
We compare our method with fourbaselines as follows:?
Mintz++.
This is a traditional DS assump-tion based model proposed by Mintz et al(2009).?
Hoffmann.
This is an at-least-one as-sumption based multi-instance learning methodproposed by Hoffmann et al (2011).?
MIML.
This is a multi-instance multi-la-bel model proposed by Surdeanu et al (2012).?
KNN.
This is a classical K-Nearest-Neighbor classifier baseline.
Specifically, givenan entity pair, we first classify each matching in-stance using the labels of its 5 (tuned on trainingcorpus) nearest neighbors with cosine similarity,then all matching instances?
classification resultsare added together.Evaluation.
We use the same evaluation set-tings as Surdeanu et al (2012).
That is, we use theofficial KBP scorer with two changes: (a) relationmentions are evaluated regardless of their supportdocument; and (b) we score only on the subset ofgold relations that have at least one mention inmatched sentences.
For evaluation, we useMintz++, Hoffmann, and MIML implementationfrom Stanford?s MIMLRE package (Surdeanu etal., 2012) and implement KNN by ourselves.4.1 Experimental Results4.1.1 Overall ResultsWe conduct experiments using all baselines andour semantic consistency based method.
For our721method, we use top 10% weighted training in-stances.
All features occur less than 5 times arefiltered.
All l1-minimization problems in this pa-per are solved using the augmented Lagrangemultiplier algorithm (Yang et al, 2010), whichhas been proven is accurate, efficient, and robust.To select the classification threshold  for ith re-lation type, we use the value which can achievethe best F-measure on training dataset (with an ad-ditional restriction that precision should > 10%).Figure 4.
Precision/recall curves in KBP datasetSystem Precision Recall F1Mintz++ 0.260 0.250 0.255Hoffmann 0.306 0.198 0.241MIML 0.249 0.314 0.278KNN 0.261 0.295 0.277Our method 0.286 0.342 0.311Table 1.
The best F1-measures in KBP datasetFigure 4 shows the precision/recall curves ofdifferent systems, and Table 1 shows their bestF1-measures.
From these results, we can see that:1) The semantic consistency based methodcan achieve robust and competitive performance:in KBP dataset, our method correspondinglyachieves 5.6%, 7%, 3.3% and 3.4% F1 improve-ments over the Mintz++, Hoffmann, MIML andKNN baselines.
We believe this verifies that thesemantic consistency around an instance is an ef-fective way to identify reliable instances.2) From Figure 4 we can see that our methodachieves a consistent improvement on the high-re-call region of the KBP curves (when recall > 0.1).We believe this is because by modeling the se-mantic consistency using the local subspacearound each testing instance, our method can bet-ter solve the classification of long tail instanceswhich are not expressed using salient patterns.3) The local subspace around an instancecan be effectively modeled as a linear subspacespanned by training instances.
From Table 1 wecan see that both our method and KNN baseline(where the local subspace is spanned using its knearest neighbors) achieve competitive perfor-mance: even the simple KNN baseline can achievea competitive performance (0.277 in F1).
This re-sult shows: a) the effectiveness of instance-basedsubspace modeling; and b) by partitioning sub-space into many local subspaces, the subspacemodel is more adaptive and robust to model prior.4) The sparse representation is an effectiveway to model the local subspace using training in-stances.
Compared with KNN baseline, ourmethod can achieve a 3.4% F1 improvement.
Webelieve this is because: (1) the discriminative na-ture of sparse representation as shown in (Wrightet al, 2009); and (2) the sparse representationglobally seeks the combination of training in-stances to characterize the local subspace, on con-trast KNN uses only its nearest neighbor in thetraining data, which is more easily affected bynoisy training instances(e.g., false positives).4.1.2 Training Instance Selection ResultsTo demonstrate the effect of training instance se-lection, Table 2 reports our method?s performanceusing different proportions of training instances.Proportion 5% 10% 20% 100%Best F1 0.282 0.311 0.305 0.280Table 2.
The best F1-measures using differentproportions of top weighted training instancesFrom Table 2, we can see that: ?
Our training in-stance selection algorithm is effective: our methodcan achieve performance improvement using onlytop weighted instances.
?
The training instancesare highly redundant: using only 10% weightedinstances can achieve a competitive performance.5 Conclusion and Future WorkThis paper proposes a semantic consistencymethod, which can identify reliable instancesfrom noisy instances for distant supervised rela-tion extraction.
For future work, we want to de-sign a more effective instance selection algorithmand embed it into our extraction framework.AcknowledgmentsThis work is supported by the National NaturalScience Foundation of China under Grants no.61100152 and 61272324, and the National HighTechnology Development 863 Program of Chinaunder Grants no.
2013AA01A603.722ReferenceAndoni, Alexandr, and Piotr Indyk .
2006.
Near-opti-mal hashing algorithms for approximate nearestneighbor in high dimensions.
In: Foundations ofComputer Science, 2006,  pp.
459-468.Bunescu, Razvan, and Raymond Mooney.
2007.Learning to extract relations from the web usingminimal supervision.
In: ACL 2007, pp.
576.Craven, Mark, and Johan Kumlien.
1999.
Constructingbiological knowledge bases by extracting infor-mation from text sources.
In : Proceedings of AAAI1999.Downey, Doug, Oren Etzioni, and Stephen Soderland.2005.
A probabilistic model of redundancy in infor-mation extraction, In: Proceeding of IJCAI 2005.Gupta, Rahul, and Sunita Sarawagi.
2011.
Joint train-ing for open-domain extraction on the web: exploit-ing overlap when supervision is limited.
In: Pro-ceedings of WSDM 2011, pp.
217-226.Hearst, Marti A.
1992.
Automatic acquisition of hypo-nyms from large text corpora.
In: Proceedings ofCOLING 1992, pp.
539-545.Hoffmann, Raphael, Congle Zhang, and Daniel S.Weld.
2010.
Learning 5000 relational extractors.
In:Proceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, 2010, pp.286-295.Hoffmann, Raphael, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S. Weld.
2011.
Knowledge-based weak supervision for information extractionof overlapping relations.
In: Proceedings of ACL2011, pp.
541-550.Ji, Heng, Ralph Grishman, Hoa Trang Dang, Kira Grif-fitt, and Joe Ellis.
2010.
Overview of the TAC 2010knowledge base population track.
In: Proceedingsof the Text Analytics Conference.Ji, Heng, Ralph Grishman, Hoa Trang Dang, Kira Grif-fitt, and Joe Ellis.
2011.
Overview of the TAC 2011knowledge base population track.
In Proceedings ofthe Text Analytics Conference.Krause, Sebastian, Hong Li, Hans Uszkoreit, and FeiyuXu.
2012.
Large-Scale learning of relation-extrac-tion rules with distant supervision from the web.
In:ISWC 2012, pp.
263-278.Mintz, Mike, Steven Bills, Rion Snow, and Dan Juraf-sky.
2009.
Distant supervision for relation extrac-tion without labeled data.
In: Proceedings ACL-AFNLP 2009, pp.
1003-1011.Min, Bonan, Ralph Grishman, Li Wan, Chang Wang,and David Gondek.
2013.
Distant Supervision forRelation Extraction with an Incomplete KnowledgeBase.
In: Proceedings of NAACL-HLT 2013,pp.777-782.Min, Bonan, Xiang Li, Ralph Grishman, and Ang Sun.2012.
New york university 2012 system for kbp slotfilling.
In: Proceedings of TAC 2012.Nguyen, Truc-Vien T., and Alessandro Moschitti.2011.
Joint distant and direct supervision for rela-tion extraction.
In: Proceedings of IJCNLP 2011, pp.732-740.Riedel, Sebastian, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions with-out labeled text.
In: Machine Learning andKnowledge Discovery in Databases, 2010, pp.
148-163.Riedel, Sebastian, Limin Yao, Andrew McCallum, andBenjamin M. Marlin.
2013.
Relation Extractionwith Matrix Factorization and Universal Schemas.In: Proceedings of NAACL-HLT 2013, pp.
74-84.Roth, Benjamin, and Dietrich Klakow.
2013.
Combin-ing Generative and Discriminative Model Scores forDistant Supervision.
In: Proceedings of ACL 2013,pp.
24-29.Surdeanu, Mihai, Julie Tibshirani, Ramesh Nallapati,and Christopher D. Manning.
2012.
Multi-instancemulti-label learning for relation extraction.
In: Pro-ceedings of EMNLP-CoNLL 2012, pp.
455-465.Takamatsu, Shingo, Issei Sato, and Hiroshi Nakagawa.2012.
Reducing wrong labels in distant supervisionfor relation extraction.
In: ACL 2012,pp.
721-729.Wang, Chang, Aditya Kalyanpur, James Fan, BranimirK.
Boguraev, and D. C. Gondek.
2012.
Relation ex-traction and scoring in DeepQA.
In: IBM Journal ofResearch and Development, 56(3.4), pp.
9-1.Wang, Chang, James Fan, Aditya Kalyanpur, and Da-vid Gondek.
2011.
Relation extraction with relationtopics.
In: Proceedings of EMNLP 2011, pp.
1426-1436.Wright, John, Allen Y. Yang, Arvind Ganesh, ShankarS.
Sastry, and Yi Ma.
2009.
Robust face recognitionvia sparse representation.
In: Pattern Analysis andMachine Intelligence, IEEE Transactions on, 31(2),210-227Wu, Fei, and Daniel S. Weld.
2007.
Autonomously se-mantifying wikipedia.
In: Proceedings of CIKM2007,pp.
41-50.Xu, Wei, Raphael Hoffmann Le Zhao, and RalphGrishman.
2013.
Filling Knowledge Base Gaps forDistant Supervision of Relation Extraction.
In: Pro-ceedings of Proceedings of 2013, pp.
665-670.Yang, Allen Y., Shankar S. Sastry, Arvind Ganesh, andYi Ma.
2010.
Fast l1-Minimization Algorithms andAn Application in Robust Face Recognition: A Re-view.
In: Proceedings of ICIP 2010.Yao, Limin, Sebastian Riedel, and Andrew McCallum.2010.
Collective cross-document relation extraction723without labelled data.
In: Proceedings of EMNLP2010, pp.
1013-1023.Zhang, Congle, Raphael Hoffmann, and Daniel S.Weld.
2012.
Ontological smoothing for relation ex-traction with minimal supervision.
In: Proceedingsof AAAI 2012, pp.
157-163.Zhang, Xingxing, Zhang, Jianwen, Zeng, Junyu, Yan,Jun, Chen, Zheng and Sui, Zhifang.
2013.
TowardsAccurate Distant Supervision for Relational FactsExtraction.
In: Proceedings of ACL 2013, pp.
810-815.724
