Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 164?172,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsStructured Generative Models for Unsupervised Named-Entity ClusteringMicha Elsner, Eugene Charniak and Mark JohnsonBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{melsner,ec,mj}@cs.brown.eduAbstractWe describe a generative model for clusteringnamed entities which also models named en-tity internal structure, clustering related wordsby role.
The model is entirely unsupervised;it uses features from the named entity itselfand its syntactic context, and coreference in-formation from an unsupervised pronoun re-solver.
The model scores 86% on the MUC-7named-entity dataset.
To our knowledge, thisis the best reported score for a fully unsuper-vised model, and the best score for a genera-tive model.1 IntroductionNamed entity clustering is a classic task in NLP, andone for which both supervised and semi-supervisedsystems have excellent performance (Mikheev et al,1998; Chinchor, 1998).
In this paper, we describe afully unsupervised system (using no ?seed rules?
orinitial heuristics); to our knowledge this is the bestsuch system reported on the MUC-7 dataset.
In ad-dition, the model clusters the words which appearin named entities, discovering groups of words withsimilar roles such as first names and types of orga-nization.
Finally, the model defines a notion of con-sistency between different references to the same en-tity; this component of the model yields a significantincrease in performance.The main motivation for our system is the re-cent success of unsupervised generative models forcoreference resolution.
The model of Haghighiand Klein (2007) incorporated a latent variable fornamed entity class.
They report a named entity scoreof 61.2 percent, well above the baseline of 46.4, butstill far behind existing named-entity systems.We suspect that better models for named entitiescould aid in the coreference task.
The easiest way toincorporate a better model is simply to run a super-vised or semi-supervised system as a preprocess.
Toperform joint inference, however, requires an unsu-pervised generative model for named entities.
As faras we know, this work is the best such model.Named entities also pose another problem withthe Haghighi and Klein (2007) coreference model;since it models only the heads of NPs, it will fail toresolve some references to named entities: (?FordMotor Co.?, ?Ford?
), while erroneously mergingothers: (?Ford Motor Co.?, ?Lockheed Martin Co.?
).Ng (2008) showed that better features for match-ing named entities?
exact string match and an ?aliasdetector?
looking for acronyms, abbreviations andname variants?
improve the model?s performancesubstantially.
Yet building an alias detector is non-trivial (Uryupina, 2004).
English speakers know that?President Clinton?
is the same person as ?Bill Clin-ton?
, not ?President Bush?.
But this cannot be im-plemented by simple substring matching.
It requiressome concept of the role of each word in the string.Our model attempts to learn this role information byclustering the words within named entities.2 Related WorkSupervised named entity recognition now performsalmost as well as human annotation in English(Chinchor, 1998) and has excellent performance onother languages (Tjong Kim Sang and De Meul-der, 2003).
For a survey of the state of the art,164see Nadeau and Sekine (2007).
Of the featureswe explore here, all but the pronoun informationwere introduced in supervised work.
Supervised ap-proaches such as Black et al (1998) have used clus-tering to group together different nominals referringto the same entity in ways similar to the ?consis-tency?
approach outlined below in section 3.2.Semi-supervised approaches have also achievednotable success on the task.
Co-training (Riloff andJones, 1999; Collins and Singer, 1999) begins witha small set of labeling heuristics and gradually addsexamples to the training data.
Various co-trainingapproaches presented in Collins and Singer (1999)all score about 91% on a dataset of named entities;the inital labels were assigned using 7 hand-writtenseed rules.
However, Collins and Singer (1999)show that a mixture-of-naive-Bayes generative clus-tering model (which they call an EM model), initial-ized with the same seed rules, performs much morepoorly at 83%.Much later work (Evans, 2003; Etzioni et al,2005; Cucerzan, 2007; Pasca, 2004) relies on theuse of extremely large corpora which allow veryprecise, but sparse features.
For instance Etzioniet al (2005) and Pasca (2004) use web queries tocount occurrences of ?cities such as X?
and simi-lar phrases.
Although our research makes use of afairly large amount of data, our method is designedto make better use of relatively common contextualfeatures, rather than searching for high-quality se-mantic features elsewhere.Models of the internal structure of names havebeen used for cross-document coreference (Li et al,2004; Bhattacharya and Getoor, 2006) and a goal intheir own right (Charniak, 2001).
Li et al (2004)take named entity classes as a given, and developsboth generative and discriminative models to detectcoreference between members of each class.
Theirgenerative model designates a particular mention ofa name as a ?representative?
and generates all othermentions from it according to an editing process.Bhattacharya and Getoor (2006) operates only onauthors of scientific papers.
Their model accountsfor a wider variety of name variants than ours, in-cluding misspellings and initials.
In addition, theyconfirm our intuition that Gibbs sampling for infer-ence has insufficient mobility; rather than using aheuristic algorithm as we do (see section 3.5), theyuse a data-driven block sampler.
Charniak (2001)uses a Markov chain to generate 6 different com-ponents of people?s names, again assuming that theclass of personal names can be pre-distinguished us-ing a name list.
He infers coreference relationshipsbetween similar names appearing in the same docu-ment, using the same notion of consistency betweennames as our model.
As with our model, the clustersfound are relatively good, although with some mis-takes even on frequent items (for example, ?John?
issometimes treated as a descriptor like ?Secretary?
).3 System DescriptionLike Collins and Singer (1999), we assume that thenamed entities have already been correctly extractedfrom the text, and our task is merely to label them.We assume that all entities fit into one of the threeMUC-7 categories, LOC (locations), ORG (organi-zations), and PER (people).
This is an oversimplifi-cation; Collins and Singer (1999) show that about12% of examples do not fit into these categories.However, while using the MUC-7 data, we have noway to evaluate on such examples.As a framework for our models, we adopt adap-tor grammars (Johnson et al, 2007), a frameworkfor non-parametric Bayesian inference over context-free grammars.
Although our system does not re-quire the full expressive power of PCFGs, the adap-tor grammar framework allows for easy develop-ment of structured priors, and supplies a flexiblegeneric inference algorithm.
An adaptor grammar isa hierarchical Pitman-Yor process (Pitman and Yor,1997).
The grammar has two parts: a base PCFGand a set of adapted nonterminals.
Each adaptednonterminal is a Pitman-Yor process which expandseither to a previously used subtree or to a samplefrom the base PCFG.
The end result is a posteriordistribution over PCFGs and over parse trees foreach example in our dataset.Each of our models is an adaptor grammar basedon a particular base PCFG where the top nonter-minal of each parse tree represents a named entityclass.3.1 Core NP ModelWe begin our analysis by reducing each named-entity reference to the contiguous substring of165ROOT ?NE 0|NE 1|NE 2NE 0 ?
(NE 00)(NE 10)(NE 20)(NE 30)(NE 40)?NE 00 ?Words?Words ?Word (Words)Word ?Bill .
.
.Figure 1: Part of the grammar for core phrases.
(Paren-theses) mark optional nonterminals.
*Starred nontermi-nals are adapted.proper nouns which surrounds its head, which wecall the core (Figure 1).
To analyze the core, we usea grammar with three main symbols (NEx), one foreach named entity class x.
Each class has an asso-ciated set of lexical symbols, which occur in a strictorder (NE ix is the ith symbol for class x).
We canthink of the NE i as the semantic parts of a propername; for people, NE 0PER might generate titles andNE 1PER first names.
Each NE i is adapted, and canexpand to any string of words; the ability to gen-erate multiple words from a single symbol is use-ful both because it can learn to group collocationslike ?New York?
and because it allows the system tohandle entities longer than four words.
However, weset the prior on multi-word expansions very low, toavoid degenerate solutions where most phrases areanalyzed with a single symbol.
The system learnsa separate probability for each ordered subset of theNE i (for instance the rule NE 0 ?
NE 00 NE 20 NE 40),so that it can represent constraints on possible refer-ences; for instance, a last name can occur on its own,but not a title.3.2 Consistency ModelThis system captures some of our intuitions aboutcore phrases, but not all: our representation for ?BillClinton?
does not share any information with ?Presi-dent Bill Clinton?
except the named-entity class.
Toremedy this, we introduce a set of ?entity?
nonter-minals Ek, which enforce a weak notion of consis-tency.
We follow Charniak (2001) in assuming thattwo names are consistent (can be references to thesame entity) if they do not have different expansionsfor any lexical symbol.
In other words, a particu-lar entity EPER,Clinton has a title E0PER,Clinton =ROOT ?NE 0|NE 1|NE 2NE 0 ?E00|E01 .
.
.
E0kE00 ?(E000)(E100)(E200)(E300)(E400)?
?
E000 ?NE 00?NE 00 ?Words .
.
.Figure 2: Part of the consistency-enforcing grammar forcore phrases.
There are an infinite number of entitiesExk, all with their own lexical symbols.
Each lexicalsymbol Eixk expands to a single NE ix.
?President?, a first name E1PER,Clinton = ?Bill?
etc.These are generated from the class-specific distribu-tions, for instance E0PER,Clinton ?
E0PER, whichwe intend to be a distribution over titles in general.The resulting grammar is shown in Figure 2; theprior parameters for the entity-specific symbols Eixkare fixed so that, with overwhelming probability,only one expansion occurs.
We can represent anyfixed number of entities Ek with a standard adap-tor grammar, but since we do not know the correctnumber, we must extend the adaptor model slightlyto allow for an unbounded number.
We generate theEk from a Chinese Restaurant process prior.
(Gen-eral grammars with infinite numbers of nonterminalswere studied by (Liang et al, 2007b)).3.3 Modifiers, Prepositions and PronounsNext, we introduce two types of context informationderived from Collins and Singer (1999): nominalmodifiers and prepositional information.
A nominalmodifier is either the head of an appositive phrase(?Maury Cooper, a vice president?)
or a non-properprenominal (?spokesman John Smith?)1.
If the en-tity is the complement of a preposition, we extractthe preposition and the head of the governing NP (?afederally funded sewage plant in Georgia?).
Theseare added to the grammar at the named-entity classlevel (separated from the core by a special punctua-tion symbol).Finally, we add information about pronouns andwh-complementizers (Figure 3).
Our pronoun infor-mation is derived from an unsupervised coreferencealgorithm which does not use named entity informa-1We stem modifiers with the Porter stemmer.166ROOT ?Modifiers0 # NE 0 #Prepositions0 # Pronouns0 #.
.
.Pronouns0 ?Pronoun0 Pronouns0Pronouns0 ?Pronoun0 ?pers|loc|org |anypers ?i |he|she|who|me .
.
.loc ?where|which|it |itsorg ?which|it |they |we .
.
.Figure 3: A fragment of the full grammar.
The symbol# represents punctuation between different feature types.The prior for class 0 is concentrated around personal pro-nouns, although other types are possible.tion (Charniak and Elsner, 2009).
This algorithmuses EM to learn a generative model with syntactic,number and gender parameters.
Like Haghighi andKlein (2007), we give our model information aboutthe basic types of pronouns in English.
By settingup the base grammar so that each named-entity classprefers to associate to a single type of pronoun, wecan also determine the correspondence between ournamed-entity symbols and the actual named-entitylabels?
for the models without pronoun information,this matching is arbitrary and must be inferred dur-ing the evaluation process.3.4 Data PreparationTo prepare data for clustering with our system, wefirst parse it with the parser of Charniak and Johnson(2005).
We then annotate pronouns with Charniakand Elsner (2009).
For the evaluation set, we use thenamed entity data from MUC-7.
Here, we extractall strings in <ne> tags and determine their cores,plus any relevant modifiers, governing prepositionsand pronouns, by examining the parse trees.
In addi-tion, we supply the system with additional data fromthe North American News Corpus (NANC).
Herewe extract all NPs headed by proper nouns.We then process our data by merging all exam-ples with the same core; some merged examplesfrom our dataset are shown in Figure 4.
When twoexamples are merged, we concatenate their lists ofattack airlift airlift rescu # wing # of-commanderof-command with-run # ## air-india # # ## abels # # it ## gaudreau # # they he ## priddy # # he #spokesman bird bird bird director bird ford clin-ton director bird # johnson # before-hearingto-happened of-cartoon on-pressure under-medicareto-according to-allied with-stuck of-government of-photographs of-daughter of-photo for-embarrassingunder-instituted about-allegations for-workedbefore-hearing to-secretary than-proposition of-typical # he he his he my himself his he he he he ihe his his i i i he his #Figure 4: Some merged examples from an input file.
(#separates different feature types.
)modifiers, prepositions and pronouns (capping thelength of each list at 20 to keep inference tractable).For instance, ?air-india?
has no features outside thecore, while ?wing?
has some nominals (?attack?&c.)
and some prepositions (?commander-of?
&c.).This merging is useful because it allows us to do in-ference based on types rather than tokens (Goldwa-ter et al, 2006).
It is well known that, to interpo-late between types and tokens, Hierarchical Dirich-let Processes (including adaptor grammars) requirea deeper hierarchy, which slows down inference andreduces the mobility of sampling schemes.
By merg-ing examples, we avoid using this more complicatedmodel.
Each merged example also represents manyexamples from the training data, so we can summa-rize features (such as modifiers) observed through-out a large input corpus while keeping the size ofour input file small.To create an input file, we first add all the MUC-7 examples.
We then draw additional examplesfrom NANC, ranking them by how many featuresthey have, until we reach a specified number (largerdatasets take longer, but without enough data, resultstend to be poor).3.5 InferenceOur implementation of adaptor grammars is a mod-ified version of the Pitman-Yor adaptor grammar167sampler2, altered to deal with the infinite number ofentities.
It carries out inference using a Metropolis-within-Gibbs algorithm (Johnson et al, 2007), inwhich it repeatedly parses each input line using theCYK algorithm, samples a parse, and proposes thisas the new tree.To do Gibbs sampling for our consistency-enforcing model, we would need to sample a parsefor an example from the posterior over every pos-sible entity.
However, since there are thousands ofentities (the number grows roughly linearly with thenumber of merged examples in the data file), this isnot tractable.
Instead, we perform a restricted Gibbssampling search, where we enumerate the posterioronly for entities which share a word in their corewith the example in question.
In fact, if the sharedword is very common (occuring in more than .001 ofexamples), we compute the posterior for that entityonly .05 of the time3.
These restrictions mean thatwe do not compute the exact posterior.
In particular,the actual model allows entities to contain exampleswith no words in common, but our search proceduredoes not explore these solutions.For our model, inference with the Gibbs algo-rithm seems to lack mobility, sometimes falling intovery poor local minima from which it does not seemto escape.
This is because, if there are several ref-erences to the same named entity with slightly dif-ferent core phrases, once they are all assigned tothe wrong class, it requires a low-probability se-ries of individual Gibbs moves to pull them out.Similarly, the consistency-enforcing model gener-ally does not fully cluster references to common en-tities; there are usually several ?Bill Clinton?
clus-ters which it would be best to combine, but the se-quence of moves that does so is too improbable.
Thedata-merging process described above is one attemptto improve mobility by reducing the number of du-plicate examples.
In addition, we found that it was abetter use of CPU time to run multiple samplers withdifferent initialization than to perform many itera-tions.
In the experiments below, we use 20 chains,initializing with 50 iterations without using consis-tency, then 50 more using the consistency model,and evaluate the last sample from each.
We discard2Available at http://www.cog.brown.edu/ mj/Software.htm3We ignore the corresponding Hastings correction, as inpractice it leads to too many rejections.the 10 samples with worst log-likelihood and reportthe average score for the other 10.3.6 ParametersIn addition to the base PCFG itself, the system re-quires a few hyperparameter settings: Dirichlet pri-ors for the rule weights of rules in the base PCFG.Pitman-Yor parameters for the adapted nonterminalsare sampled from vague priors using a slice sam-pler (Neal, 2003).
The prior over core words wasset to the uniform distribution (Dirichlet 1.0) and theprior for all modifiers, prepositions and pronouns toa sparse value of .01.
Beyond setting these param-eters to a priori reasonable values, we did not opti-mize them.
To encourage the system to learn thatsome lexical symbols were more common than oth-ers, we set a sparse prior over expansions to sym-bols4.
There are two really important hyperparame-ters: an extremely biased prior on class-to-pronoun-type probabilities (1000 for the desired class, .0001for everything else), and a prior of .0001 for theWord ?Word Words rule to discourage symbolsexpanding to multiword strings.4 ExperimentsWe performed experiments on the named entitydataset from MUC-7 (Chinchor, 1998), using thetraining set as development data and the formal testset as test data.
The development set has 4936named entities, of which 1575 (31.9%) are locations,2096 (42.5%) are organizations and 1265 (25.6%)people.
The test set has 4069 named entities, 1321(32.5%) locations, 1862 (45.8%) organizations and876 (21.5%) people5.
We use a baseline whichgives all named entities the same label; this label ismapped to ?organization?.In most of our experiments, we use an input file of40000 lines.
For dev experiments, the labeled datacontributes 1585 merged examples; for test experi-ments, only 1320.
The remaining lines are derived4Expansions that used only the middle three symbolsNE1,2,3x got a prior of .005, expansions whose outermost sym-bol was NE0,4x got .0025, and so forth.
This is not so impor-tant for our final system, which has only 5 symbols, but wasdesigned during development to handle systems with up to 10symbols.510 entities are labeled location|organization; since thisfraction of the dataset is insignificant we score them as wrong.168Model AccuracyBaseline (All Org) 42.5Core NPs (no consistency) 45.5Core NPs (consistency) 48.5Context Features 83.3Pronouns 87.1Table 1: Accuracy of various models on developmentdata.Model AccuracyBaseline (All Org) 45.8Pronouns 86.0Table 2: Accuracy of the final model on test data.using the process described in section 3.4 from 5million words of NANC.To evaluate our results, we map our three inducedlabels to their corresponding gold label, then countthe overlap; as stated, this mapping is predictablyencoded in the prior when we use the pronoun fea-tures.
Our experimental results are shown in Table1.
All models perform above baseline, and all fea-tures contribute significantly to the final result.
Testresults for our final model are shown in Table 2.A confusion matrix for our highest-likelihood testsolution is shown as Figure 5.
The highest confusionclass is ?organization?, which is confused most oftenwith ?location?
but also with ?person?.
?location?
islikewise confused with ?organization?.
?person?
isthe easiest class to identify?
we believe this explainsthe slight decline in performance from dev to test,since dev has proportionally more people.Our mapping from grammar symbols to words ap-pears in Table 3; the learned prepositional and mod-ifier information is in Table 4.
Overall the resultsare good, but not perfect; for instance, the Persstates are mostly interpretable as a sequence of ti-tle - first name - middle name or initial - last name -loc org perLOC 1187 97 37ORG 223 1517 122PER 36 20 820Figure 5: Confusion matrix for highest-likelihood testrun.
Gold labels in CAPS, induced labels italicized.
Or-ganizations are most frequently confused.last name or post-title (similar to (Charniak, 2001)).The organization symbols tend to put nationalitiesand other modifiers first, and end with institutionaltypes like ?inc.?
or ?center?, although there is a sim-ilar (but smaller) cluster of types at Org2, suggest-ing the system has incorrectly found two analysesfor these names.
Location symbols seem to put en-tities with a single, non-analyzable name into Loc2,and use symbols 0, 1 and 3 for compound names.Loc4 has been recruited for time expressions, sinceour NANC dataset includes many of these, but wefailed to account for them in the model.
Sincethey appear in a single class here, we are optimisticthat they could be clustered separately if anotherclass and some appropriate features were added tothe prior.
Some errors do appear (?supreme court?and ?house?
as locations, ?minister?
and ?chairman?as middle names, ?newt gingrich?
as a multiwordphrase).
The table also reveals an unforeseen issuewith the parser: it tends to analyze the dateline be-ginning a news story along with the following NP(?WASHINGTON Bill Clinton said...?).
Thus com-mon datelines (?washington?, ?new york?
and ?losangeles?)
appear in state 0 for each class.5 DiscussionAs stated above, we aim to build an unsupervisedgenerative model for named entity clustering, sincesuch a model could be integrated with unsupervisedcoreference models like Haghighi and Klein (2007)for joint inference.
To our knowledge, the closestexisting system to such a model is the EM mix-ture model used as a baseline in Collins and Singer(1999).
Our system improves on this EM systemin several ways.
While they initialize with minimalsupervision in the form of 7 seed heuristics, ours isfully unsupervised.
Their results cover only exam-ples which have a prepositional or modifier feature;we adopt these features from their work, but labelall entities in the predefined test set, including thosethat appear without these features.
Finally, as dis-cussed, we find the ?person?
category to be the eas-iest to label.
33% of the test items in Collins andSinger (1999) were people, as opposed to 21% ofours.
However, even without the pronoun features,that is, using the same feature set, our system scoresequivalently to the EM model, at 83% (this score is169Pers0 Pers1 Pers2 Pers3 Pers4rep.
john (767) minister brown jr.sen.
(256) robert (495) j. smith (97) awashington david john (242) b smith (111)dr. michael l. johnson iiilos angeles james chairman newt gingrich williamssenate president e. king wilsonhouse richard m. miller brownnew york william (317) william (173) kennedy clintonpresident sen. (236) robert (155) martin simpsonrepublican george r. davis bOrg0 Org1 Org2 Org3 Org4american (137) national university research associationwashington american (182) inc. (166) medical centerwashington the new york corp. (156) news inc. (257)national international (136) college health corp. (252)first public institute (87) services co.los angeles united group communications committeenew house hospital development instituteroyal federal museum policy councilbritish home press affairs fundcalifornia world international (61) defense actLoc0 Loc1 Loc2 Loc3 Loc4washington (92) the texas county mondaylos angeles st. new york city thursdaysouth new washington (22) beach river (57)north national (69) united states valley tuesdayold east (65) baltimore island wednesdaygrand mount california river (71) hotelblack fort capitol park fridaywest (22) west (56) christmas bay halleast (21) lake bosnia house centerhaiti great san juan supreme court buildingTable 3: 10 most common words for each grammar symbol.
Words which appear in multiple places have observedcounts indicated in parentheses.170Pers-gov Pers-mod Org-gov Org-mod Loc-gov Loc-modaccording-to (1044) director president-of $ university-of calif.played-by spokesman chairman-of giant city-of newspap[er]directed-by leader director-of opposit[e] from-to stateled-by presid[ent] according-to (786) group town-of downtownmeeting-with attorney professor-at pp state-of n.y.from-to candid[ate] head-of compan[y] center-in warrantmet-with lawyer department-of journal out-of va.letter-to chairman member-of firm is-in fla.secretary-of counsel members-of state house-of p.m.known-as actor spokesman-for agenc[y] known-as itselfTable 4: 10 most common prepositional and modifier features for each named entity class.
Modifiers were Porter-stemmed; for clarity a reconstructed stem is shown in brackets.on dev, 25% people).
When the pronoun features areadded, our system?s performance increases to 86%,significantly better than the EM system.One motivation for our use of a structured modelwhich defined a notion of consistency between en-tities was that it might allow the construction ofan unsupervised alias detector.
According to themodel, two entities are consistent if they are in thesame class, and do not have conflicting assignmentsof words to lexical symbols.
Results here are atbest equivocal.
The model is reasonable at pass-ing basic tests?
?Dr.
Seuss?
is not consistent with?Dr.
Strangelove?, ?Dr.
Quinn?
etc, despite theirshared title, because the model identifies the sec-ond element of each as a last name.
Also correctly,?Dr.
William F. Gibson?
is judged consistent with?Dr.
Gibson?
and ?Gibson?
despite the missing el-ements.
But mistakes are commonplace.
In the?Gibson?
case, the string ?William F.?
is misana-lyzed as a multiword string, making the name in-consistent with ?William Gibson?
; this is probablythe result of a search error, which, as we explained,Gibbs sampling is unlikely to correct.
In other cases,the system clusters a family group together undera single ?entity?
nonterminal by forcing their firstnames into inappropriate states, for instance assign-ing Pers1 Bruce, Pers2 Ellen, Pers3 Jarvis, wherePers2 (usually a middle name) actually contains thefirst name of a different individual.
To improve thisaspect of our system, we might incorporate name-specific features into the prior, such as abbreviationsand the concept of a family name.
The most criticalimprovement, however, would be integration with agenerative coreference system, since the documentcontext probably provides hints about which entitiesare and are not coreferent.The other key issue with our system is inference.Currently we are extremely vulnerable to falling intolocal minima, since the complex structure of themodel can easily lock a small group of examplesinto a poor configuration.
(The ?William F. Gibson?case above seems to be one of these.)
In addition tothe block sampler used by Bhattacharya and Getoor(2006), we are investigating general-purpose split-merge samplers (Jain and Neal, 2000) and the per-mutation sampler (Liang et al, 2007a).
One inter-esting question is how well these samplers performwhen faced with thousands of clusters (entities).Despite these issues, we clearly show that it ispossible to build a good model of named entity classwhile retaining compatibility with generative sys-tems and without supervision.
In addition, we do areasonable job learning the latent structure of namesin each named entity class.
Our system improvesover the latent named-entity tagging in Haghighiand Klein (2007), from 61% to 87%.
This sug-gests that it should indeed be possible to improveon their coreference results without using a super-vised named-entity model.
How much improvementis possible in practice, and whether joint inferencecan also improve named-entity performance, remaininteresting questions for future work.AcknowledgementsWe thank three reviewers for their comments, andNSF for support via grants 0544127 and 0631667.171ReferencesIndrajit Bhattacharya and Lise Getoor.
2006.
A latentdirichlet model for unsupervised entity resolution.
InThe SIAM International Conference on Data Mining(SIAM-SDM), Bethesda, MD, USA.William J.
Black, Fabio Rinaldi, and David Mowatt.1998.
Facile: Description of the ne system used formuc-7.
In In Proceedings of the 7th Message Under-standing Conference.Eugene Charniak and Micha Elsner.
2009.
EM worksfor pronoun anaphora resolution.
In Proceedings ofthe Conference of the European Chapter of the As-sociation for Computational Linguistics (EACL-09),Athens, Greece.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proc.
of the 2005 Meeting of the Assoc.
forComputational Linguistics (ACL), pages 173?180.Eugene Charniak.
2001.
Unsupervised learning of namestructure from coreference data.
In NAACL-01.Nancy A. Chinchor.
1998.
Proceedings of the Sev-enth Message Understanding Conference (MUC-7)named entity task definition.
In Proceedings of theSeventh Message Understanding Conference (MUC-7), page 21 pages, Fairfax, VA, April.
version 3.5,http://www.itl.nist.gov/iaui/894.02/related projects/muc/.Michael Collins and Yorav Singer.
1999.
Unsupervisedmodels for named entity classification.
In Proceedingsof EMNLP 99.Silviu Cucerzan.
2007.
Large-scale named entity disam-biguation based on Wikipedia data.
In Proceedings ofEMNLP-CoNLL, pages 708?716, Prague, Czech Re-public, June.
Association for Computational Linguis-tics.Oren Etzioni, Michael Cafarella, Doug Downey, Anamaria Popescu, Tal Shaked, Stephen Soderl, Daniel S.Weld, and Er Yates.
2005.
Unsupervised named-entity extraction from the web: An experimental study.Artificial Intelligence, 165:91?134.Richard Evans.
2003.
A framework for named en-tity recognition in the open domain.
In Proceedingsof Recent Advances in Natural Language Processing(RANLP-2003), pages 137 ?
144, Borovetz, Bulgaria,September.Sharon Goldwater, Tom Griffiths, and Mark Johnson.2006.
Interpolating between types and tokens by es-timating power-law generators.
In Advances in NeuralInformation Processing Systems (NIPS) 18.Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric Bayesianmodel.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics, pages848?855.
Association for Computational Linguistics.Sonia Jain and Radford M. Neal.
2000.
A split-mergemarkov chain monte carlo procedure for the dirichletprocess mixture model.
Journal of Computational andGraphical Statistics, 13:158?182.Mark Johnson, Tom L. Griffiths, and Sharon Goldwa-ter.
2007.
Bayesian inference for PCFGs via Markovchain Monte Carlo.
In Proceedings of NAACL 2007.Xin Li, Paul Morie, and Dan Roth.
2004.
Identificationand tracing of ambiguous names: Discriminative andgenerative approaches.
In AAAI, pages 419?424.Percy Liang, Michael I. Jordan, and Ben Taskar.
2007a.A permutation-augmented sampler for DP mixturemodels.
In Proceedings of ICML, pages 545?552,New York, NY, USA.
ACM.Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.2007b.
The infinite PCFG using hierarchical Dirichletprocesses.
In Proceedings of EMNLP-CoNLL, pages688?697, Prague, Czech Republic, June.
Associationfor Computational Linguistics.A.
Mikheev, C. Grover, and M. Moens.
1998.
Descrip-tion of the LTG System Used for MUC-7.
In Pro-ceedings of the 7th Message Understanding Confer-ence (MUC-7), Fairfax, Virginia.David Nadeau and Satoshi Sekine.
2007.
A survey ofnamed entity recognition and classification.
Journalof Linguisticae Investigationes, 30(1).Radford M. Neal.
2003.
Slice sampling.
Annals ofStatistics, 31:705?767.Vincent Ng.
2008.
Unsupervised models for coreferenceresolution.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 640?649, Honolulu, Hawaii, October.
Associa-tion for Computational Linguistics.Marius Pasca.
2004.
Acquisition of categorized namedentities for web search.
In CIKM ?04: Proceedings ofthe thirteenth ACM international conference on Infor-mation and knowledge management, pages 137?145,New York, NY, USA.
ACM.Jim Pitman and Marc Yor.
1997.
The two-parameterPoisson-Dirichlet distribution derived from a stablesubordinator.
Ann.
Probab., 25:855?900.Ellen Riloff and Rosie Jones.
1999.
Learning dictio-naries for information extraction by multi-level boot-strapping.
In Proceedings of the Sixteenth NationalConference on Artificial Intelligence, pages 472?479.AAAI.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In WalterDaelemans and Miles Osborne, editors, Proceedingsof CoNLL-2003, pages 142?147.
Edmonton, Canada.Olga Uryupina.
2004.
Evaluating name-matching forcoreference resolution.
In Proceedings of LREC 04,Lisbon.172
