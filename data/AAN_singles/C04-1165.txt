Construction of an Objective Hierarchy of Abstract Conceptsvia Directional SimilarityKyoko Kanzaki  Eiko Yamamoto Hitoshi IsaharaComputational Linguistics Group,National Institute of Information and CommunicationsTechnology3-5 Hikari-dai, Seika-cho, Souraku-gun, Kyoto, Japan,{kanzaki, eiko, isahara}@nict.go.jpQing MaFaculty of Scienceand TechnologyRyukoku UniversitySeta, Otsu,520-2194, Japanqma@math.ryukoku.ac.jpAbstractThe method of organization of word mean-ings is a crucial issue with lexical databases.Our purpose in this research is to extract wordhierarchies from corpora automatically.
Ourinitial task to this end is to determine adjec-tive hyperonyms.
In order to find adjectivehyperonyms, we utilize abstract nouns.
Weconstructed linguistic data by extracting se-mantic relations between abstract nouns andadjectives from corpus data and classifyingabstract nouns based on adjective similarityusing a self-organizing semantic map, whichis a neural network model (Kohonen 1995).In this paper we describe how to hierarchi-cally organize abstract nouns (adjective hy-peronyms) in a semantic map mainly usingCSM.
We compare three hierarchical organi-zations of abstract nouns, according to CSM,frequency (Tf.CSM) and an alternative simi-larity measure based on coefficient overlap, toestimate hyperonym relations between words.1.
IntroductionA lexical database is necessary for computers,and even humans, to fully understand a word'smeaning because the lexicon is the origin of lan-guage understanding and generation.
Progress isbeing made in lexical database research, notablywith hierarchical semantic lexical databases suchas WordNet, which is used for NLP researchworldwide.When compiling lexical databases, it is impor-tant to consider what rules or phenomena shouldbe described as lexical meanings and how theselexical meanings should be formalized and storedelectronically.
This is a common topic of discus-sion in computational linguistics, especially inthe domain of computational lexical semantics.The method of organization of word meaningsis also a crucial issue with lexical databases.
Incurrent lexical databases and/or thesauri, abstractnouns indicating concepts are identified manuallyand words are classified in a top-down mannerbased on human intuition.
This is a good way tomake a lexical database for users with a specificpurpose.
However, word hierarchies based onhuman intuition tend to vary greatly dependingon the lexicographer, and there is often dis-agreement as to the make-up of the hierarchy.
Ifwe could find an objective method to organizeword meanings based on real data, we wouldavoid this variability.Our purpose in this research is to extract wordhierarchies from corpora automatically.
Our ini-tial task to this end is to determine adjective hy-peronyms.
In order to find adjective hyperonyms,we utilize abstract nouns.
Past linguistic researchhas focused on classifying the semantic relation-ship between abstract nouns and adjectives(Nemoto 1969, Takahashi 1975).We constructed linguistic data by extractingsemantic relations between abstract nouns andadjectives from corpus data and classifying ab-stract nouns based on adjective similarity using aself-organizing semantic map (SOM), which is aneural network model (Kohonen 1995).
The rela-tive proximity of words in the semantic map in-dicates their relative similarity.In previous research, word meanings havebeen statistically modeled based on syntactic in-formation derived from a corpus.
Hindle (1990)used noun-verb syntactic relations, and Hatzivas-siloglou and McKeown (1993) used coordinatedadjective-adjective modifier pairs.
These meth-ods are useful for the organization of words deepwithin a hierarchy, but do not seem to provide asolution for the top levels of the hierarchy.To find an objective hierarchical word struc-ture, we utilize the complementary similaritymeasure (CSM), which estimates a one-to-manyrelation, such as superordinate?subordinate rela-tions (Hagita and Sawaki 1995, Yamamoto andUmemura 2002).In this paper we propose an automated methodfor constructing adjective hierarchies by connect-ing strongly related abstract nouns in a top-downfashion within a semantic map, mainly usingCSM.
We compare three hierarchical organiza-tions of abstract nouns, according to CSM, fre-quency (Tf.CSM) and an alternative similaritymeasure based on coefficient overlap, to estimatehyperonym relations between words.2.
Linguistic clues to extract adjective hy-peronyms from corporaIn order to automatically extract adjective hy-peronyms we use syntactic and semantic relationsbetween words.There is a good deal of linguistic research fo-cused on the syntactic and semantic functions ofabstract nouns, including Nemoto (1969), Taka-hashi (1975), and Schmid (2000).
Takahashi(1975) illustrated the sentential function of ab-stract nouns with the following examples.a.
Yagi  wa  seishitsu  ga  otonashii.
(goat) topic (nature) subject (gentle)The nature of goats is gentleb.
Zou    wa   hana   ga     nagai.
(elephant) topic  (a nose) subject  (long)The nose of an elephant is longHe examined the differences in semantic func-tion between ?seishitsu (nature)?
in (a) and ?hana(nose)?
in (b), and explained that ?seishitsu (na-ture)?
in (a) indicates an aspect of something, i.e.,the goat, and ?hana (nose)?
in (b) indicates partof something, i.e., the elephant.
He recognizedabstract nouns in (a) as a hyperonym of the at-tribute that the predicative adjectives express.Nemoto (1969) identified expressions such as?iro ga akai (the color is red)?
and ?hayasa gahayai (the speed is fast)?
as a kind of meaningrepetition, or tautology.In this paper we define such abstract nounsthat co-occur with adjectives as adjective hy-peronyms.
We semi-automatically extracted fromcorpora 365 abstract nouns used as this kind ofhead noun, according to the procedures describedin Kanzaki et al (2000).
We collected abstractnouns from two year's worth of articles from theMainichi Shinbun newspaper, and extracted ad-jectives co-occurring with abstract nouns in themanner of (a) above from 100 novels, 100 essaysand 42 year's worth of newspaper articles, includ-ing 11 year's worth of Mainichi Shinbun articles,10 year's worth of Nihon Keizai Shinbun (Japa-nese economic newspaper) articles, 7 year's wor-th of Sangyoukinyuuryuutsu Shinbun (an eco-nomic newspaper) articles, and 14 year's worth ofYomiuri Shinbun articles.
The total number ofabstract noun types is 365, the number of adjec-tive types is 10,525, and the total number of ad-jective tokens is 35,173.
The maximum numberof co-occurring adjectives for a given abstractnoun is 1,594.3.
On the Self-Organizing Semantic Map3.1  Input dataAbstract nouns are located in the semantic mapbased on the similarity of co-occurring adjectivesafter iteratively learning over input data.In this research, we focus on abstract nounsco-occurring with adjectives.
In the semanticmap, there are 365 abstract nouns co-occurringwith adjectives.
The similarities between the 365abstract nouns are determined according to thenumber of common co-occurring adjectives.
Wemade a list such as the following.OMOI (feeling): ureshii (glad), kanashii (sad),shiawasena (happy), ?KIMOCHI (though): ureshii (glad), tanoshii (pleased),hokorashii (proud), ?KANTEN (viewpoint): igakutekina (medical),rekishitekina (historical), ...When two (or more) sets of adjectives withcompletely different characteristics co-occur withan abstract noun and the meanings of the abstractnoun can be distinguished correspondingly, wetreat them as two different abstract nouns.
Forexample, the Japanese abstract noun ?men?
istreated as two different abstract nouns with?men1?
meaning ?one side (of the characteristicsof someone or something)?
and ?men2?
meaning?surface?.
The former co-occurs with ?gentle?,?kind?
and so on.
The latter co-occurs with?rough?, ?smooth?
and so on.3.2  The Self-Organizing Semantic MapMa (2000) classified co-occurring words usinga self-organizing semantic map (SOM).We made a semantic map of the above-mentioned 365 abstract nouns using SOM, basedon the cosine measure.
The distribution of thewords in the map gives us a sense of the semanticdistribution of the words.
However, we could notprecisely identify the relations between words inthe map (Fig 1).
In Fig.
1 lines on the maps indi-cate close relations between word pairs.
In thecosine-based semantic map, there is no clear cor-respondence between word similarities and thedistribution of abstract nouns in the map.To solve this problem we introduced thecomplementary similarity measure (CSM).
Thissimilarity measure estimates one-to-manyrelations, such as superordinate?subordinaterelations (Hagita and Sawaki 1995, Yamamotoand Umemura 2002).
We can find thehierarchical distribution of words in the semanticmap according to the value of CSM (Fig 2).
Inthe CSM-based SOM, lines are concentrated atthe bottom right hand corner, that is, most ab-stract nouns are located at the bottom right-handcorner.Next, we find hierarchical relations betweenwhole abstract nouns, not between word pairs, onthe map automatically.4.
How to construct hierarchies of nominaladjective hyperonyms in the SemanticMap4.1 Similarity measures, CSM and Yates?correctionA feature of CSM is its ability to estimate hi-erarchical relations between words.
This similar-ity measure was developed for the recognition ofdegraded machine-printed text (Hagita and Sa-waki, 1995).
Yates?
correction is often used inorder to increase the accuracy of approximation.Hierarchical relations can be extracted accuratelywhen the CSM value is high.
Yates?
correctioncan extract different relations from high CSMvalues.
When the CSM value is low, the result isnot reliable, in which case we use Yates?
correc-tion.According to Yamamoto and Umemura (2002),who adopted CSM to classify words, CSM is cal-culated as follows.
))(( dbcabcadCSM++?=Yates?
correction is calculated as follows.
))()()(()2/|(| 2dbcadcbanbcadnYates++++?
?=Here n is the sum of the number of co-occurring adjectives; a indicates the number oftimes the two labels appear together; b indicatesthe number of times ?label 1?
occurs but ?label2?
does not; c is the number of times ?label 2?occurs but ?label 1?
does not; and d is the num-ber of times neither label occurs.
In our research,each ?label?
is an abstract noun, a indicates thenumber of adjectives co-occurring with both ab-stract nouns, b and c indicate the number of ad-jectives co-occurring with either abstract nounFigure 1.
The Cosine-based SOM of word similarity Figure 2.
The CSM-based SOM of word similarity(?label 1?
and ?label 2?, respectively), and d in-dicates the number of adjectives co-occurringwith neither abstract noun.
We calculated hierar-chical relations between word pairs using thesesimilarity measures.4.2 Construction of a hierarchy of abstractnouns using CSM and Yates' correc-tionThe hierarchy construction process is as fol-lows:1) Based on the results of CSM, ?koto (mat-ter)?
is the hyperonym of all abstract nouns.First, we connect super/sub-ordinate wordswith the highest CSM value while keeping thesuper-subordinate relation.2) When the normalized value of CSM islower, the number of extracted word pairs be-comes increasing overwhelmingly, and the reli-ability of CSM diminishes.
Word pairs with anormalized CSM value of less than 0.4 are lo-cated far from the common hyperonym ?koto(matter)?
on the semantic map.
If we construct ahierarchy using CSM values only, a long hierar-chy containing irrelevant words emerges.
In thiscase, the word pairs calculated by Yates' correc-tion are more accurate than those from CSM.
Wecombine words using Yates?
correction, when thevalue of CSM is less than 0.4.
When we connectword pairs with a high Yates?
value, we find ahyperonym of the super-ordinate noun of the pairand connect the pair to the hyperonym.
If a wordpair appears only in the Yates' correction data,that is, we cannot connect the pair with highYates?
value to the hyperonym with high CSMvalue, they are combined with ?koto (matter)?.3) Finally, if a short hierarchy is contained in alonger hierarchy, it is merged with the longerhierarchy and we insert ?koto (matter)?
at theroot of all hierarchies.4.3  ResultsThe number of groups obtained was 161.
At itsdeepest, the hierarchy was 15 words deep, and atits shallowest, it was 4 words deep.
Thefollowing is a breakdown of the number ofgroups at different depths in the hierarchy.The greatest concentration of groups is atdepth 7.
There are 140 groups from depth 5 todepth 10, which is 87% of all groups.The word that has the strongest relation with?koto (matter)?
is ?men1 (side1)?.
The number ofgroups in which ?koto (matter)?
and ?men1(side1)?
are hyperonyms is 96 (59.6%).
The larg-est number of groups after that is a group inwhich ?koto (matter)?, ?men1 (side1)?
and?imeeji (image)?
are hyperonyms.
The number ofgroups in this case is 59 groups, or 36.6% of thetotal.
With respect to the value of CSM, the co-occurring adjectives are similar to ?men1 (side1)?and ?imeeji (image)?.Other words that have a direct relation with?koto (matter)?
are ?joutai (state)?
and ?toki(when)?.
They have the most number of groupsafter ?men1 (side1)?
among all the children of?koto (matter)?.
The number of groups subsumedby ?joutai (state)?
group and ?toki (when)?
are 21and 19, respectively.
Other direct hyponyms of?koto (matter)?
are:ki (feeling): 6 groupsippou (while or grow ?er and er): 3 groupsme2 (eyes): 3 groupskatachi1 (in the form of): 3 groupsiikata (how to say): 2 groupsyarikata (how to): 2 groupsThere is little hierarchical structure to thesegroups, as they co-occur with few adjectives.4.4 The Hierarchies of abstract concepts inthe semantic mapIn the following semantic maps, where abstractnouns are distributed using SOM and CSM (seeSection 3), hierarchies of abstract nouns aredrawn with lines.
The bottom right hand corner is?koto (matter)?, a starting point for the distribu-tion of abstract nouns.Five main types of hierarchies are found frompatterns of lines on the map, as follows:The first figure, Fig.3, is hierarchies of ?kanji(feeling), kimochi (feeling) ??
on the semanticmap.
The location of hierarchies of ?yousu (as-pect), omomochi (look), kaotsuki (on one?s face),??
is similar to this type of the location.
Hierar-chies of ?sokumen (one side), imi (meaning),kanten (viewpoint),  kenchi (standpoint) ??
onDepth 4 5 6 7 8 9Groups 3 16 27 32 23 23Depth 10 11 12 13 14 15Groups 19 7 3 4 3 1Table 1: The depth of the hierarchy by CSMthe map are shown in Fig.
4.
The lines of the hi-erarchies go up from the bottom right hand cor-ner to the upper left hand corner and then turntowards the upper right hand corner.
The loca-tion of hierarchies of ?nouryoku (ability), sainou(talent) ??
is similar to this one.The hyperonym of ?teido (degree)?
is ?joutai(state)?.
In Fig.5 these abstract nouns are locatedat the bottom of the map.
The location of hierar-chies of ?kurai (rather than)?
and ?hou (compara-tively)?
are similar to this one.
The hierarchies of?joutai (state), joukyou (situation), yousou (as-pect), jousei (the state of affairs)?
are shown inFig.6.
The lines are found at a higher locationthan the line of ?teido(degree)?.
The lines of thehierarchies of ?joutai (state), ori (when), sakari(in the hight of), sanaka (while)?
are similar tothese lines.The lines of the hierarchies of ?seikaku (char-acter)?, ?gaikan (appearance)?and ?utsukushisa(beauty)?
are similar to each other.
We show thehierarchies of ?seikaku (character)?
in Fig.7.
The-se lines in Fig.7 are located from the right end tothe upper left hand corner.
From the following,we can find five main types of hierarchies.From the starting point ?
koto (matter)?,-The hierarchies of ?men (side), inshou (impres-sion), kanji (feeling), kibun (mood), kimochi(feeling)?-The hierarchies of ?men (side), sokumen (one-side), imi (meaning), kanten (viewpoint), kenchi(standpoint)?-The hierarchies of ?joutai (state), teido (degree)?-The hierarchies of ?joutai (state), jousei (situa-tion)?-The hierarchies of ?men (side), inshou (impres-sion), seikaku (character) or gaikan (appear-ance) or utsukushisa (beauty)?.The lines in Fig.8 are not peculiar, and appearin an area of the hierarchies of ?seikaku (charac-Fig.3: Hierarchies of?kimochi (feeling)?Fig.4:Hierarchies of?sokumen (one side)?Fig.5:Hierarchies of?teido (degree)?Fig8: Hierarchies of?kanshoku (feel)?Fig.6:  Hierarchies of?jousei (situation)?Fig.7:Hierarchies of?seikaku (character)?ter)?
in Fig.7.
As Fig.8 shows, the hierarchies of?men (side), inshou (impression), kanji (feeling),kanshoku (feel) or kansei (sensitivity)?
are lo-cated in the area of the hierarchies of ?seikaku(character)?, above the hierarchies of ?kimochi(feeling)?
in Fig.3.5.
Comparison of hierarchies of super-ordinate nouns of adjectives.We compare the hierarchy mentioned abovewith ones obtained from two kinds of data.1) Hierarchies obtained by:CSM and Yate?s correctioncorpus occurrence data (no frequency).2) Hierarchies obtained by:Tf.CSM and Yate?s correctioncorpus frequency data.3) Hierarchies obtained by:Overlap coefficient and Yates' correctioncorpus occurrence data (no frequency).As both CSM and the Overlap coefficient are?measures of inclusion?, we compared CSM andTf.CSM with the Overlap coefficient.The number of groups that were obtained byCSM, Tf.CSM and the Overlap coefficient arethe following.Table 2.
Total number of groups obtained from CSM,Tf.CSM and Ovlp (Overlap)groupsCSM 161Tf.CSM 158Ovlp 240The Depth of hierarchies obtained from CSM,Tf.CSM, and the Overlap coefficient are as fol-lows:Table 3.
The hierarchy depth for CSM, Tf.CSM,and the Overlap coefficientIn the case of CSM, there are 32 groups atdepth 7, which is the greatest number of groups.The greatest concentration of groups is at depth 5to 10.
In the case of Tf.CSM, the greatest numberof groups is 25 at depth 8.
The greatest concen-tration of groups is at depth 5 to 13.
In the case ofthe overlap coefficient, the greatest number ofgroups is 61 at depth 5.
The greatest concentra-tion of groups is at depth 3 to 7.0102030405060703 4 5 6 7 8 9 10 11 12 13 14 15CSMTf.CSMOvlpFrom this result, we can see that hierarchiesgenerated by Tf.CSM are relatively deep, andthose generated by the Overlap coefficient arerelatively shallow.In the case of the Overlap coefficient, abstractnouns in lower layers are sometimes directly re-lated to abstract nouns in the highest layers.
Onthe other hand, in hierarchies generated by CSMand Tf.CSM, abstract nouns in the highest layersare related to those in the lowest layers via ab-stract nouns in the middle layers.
The followingindicates the number of overlapping hierarchiesfor CSM, Tf.CSM and Overlap.Table 4.
The number of overlapping hierarchiesamong CSM, Tf.CSM and OverlapCSM&Tf.CSM 37CSM&Ovlp 7Tf.CSM&Ovlp 2CSM&Tf.CSM&Ovlp 7The hierarchy generated by Tf.CSM is thedeepest, and includes some hierarchies generatedby CSM and the Overlap coefficient.
The hierar-chy generated by CSM is more similar to the onemade by Tf.CSM than that for the Overlap coef-ficient: the number of completely correspondinghierarchies for CSM and Tf.CSM is 37, that forCSM and the Overlap coefficient is 7, and thatfor Tf.CSM and the Overlap coefficient is 2.
Thetotal number of hierarchies that correspond com-pletely between CSM, Tf.CSM and the Overlapcoefficient is 7, and the number of hierarchieswhich are generated by two of the methods andincluded in the third is 57.depth 3 4 5 6 7 8 9CSM 0 3 16 27 32 23 23Tf.CSM 1 5 10 18 13 25 11Ovlp 32 56 61 57 21 7 2depth 10 11 12 13 14 15CSM 19 7 3 4 3 1Tf.CSM 24 13 14 14 7 2Ovlp 2 0 0 0 0 0Figure 9.
Distribution of hierarchy depth for CSM,Tf.CSM, and Overlap coefficientWe investigated these 64 hierarchies precisely,checking adjectives appearing at each depth asindicated by an abstract noun in this paper.
In 6of these hierarchies, the same adjectives werefound at all levels of the hierarchy.
In 14 of theremaining 58 hierarchies, the same adjectiveswere found in all but the deepest level.
These20 hierarchies are the most plausible in the strictsense of the word.
Below, we give examples ofthese hierarchies.
In the next stage of this re-search, we intend to investigate the remaining 44hierarchies to determine the reason for the differ-ence in adjective content.The common hyperonym: koto (matter) ---men1 (side) ---sokumen (one side) ---imi (meaning) ---kanten (viewpoint) ---me2 (eyes) ---mikata (view) ---hyouka (evaluation) ---ippou (while or grow -er and er) ---ikioi (force) ---sokudo (speed) ---jikoku (time) ---6.
ConclusionWe have suggested how to make a hierarchyof adjectives automatically by connectingstrongly-related abstract nouns in a top-downfashion.
We generated a word hierarchy fromcorpus data by using a combination of twomethods: a self-organizing semantic map and adirectional similarity measure.
As our directionalsimilarity measure, we utilized the complement-ary similarity measure (CSM).
Then we com-pared the hierarchy generated by CSM with thatgenerated by Tf.CSM and the Overlap coefficient.In the case of Tf.CSM, the hierarchy is deeperthan the others because there are more abstractnouns in the middle layer.
In the case of theOverlap coefficient, the hierarchy is shallow, butthere are more hyponyms in the lower layer thanwith the other two methods.
As a result, thehierarchies generated by CSM have more com-mon hierarchical relations than those generatedby the other two methods.
In future work, we willanalyze common hierarchies made by the threemethods in detail and examine differences amongthem in order to generate an abstract conceptualhierarchy of adjectives.
We will then compareour hierarchy with thesauri compiled manually.After we have completed the experiment on Jap-anese adjectives, we are keen to investigate dif-ferences and similarities in adjective hypero-nyms between Japanese and other languages suchas English by means of our method.AcknowledgementWe would like to thank Dr. Masaki Murata ofNICT for allowing us to use his drawing tool.ReferencesNemoto, K. 1969.
The combination of the noun with?ga-Case?
and the adjective, Language research2for the computer, National Language Research In-stitute: 63-73/Takahashi, T. 1975.
A various phase related to thepart-whole relation investigated in the sentence,Studies in the Japanese language 103, The societyof Japanese Linguistics: 1-16.Kohonen, T. 1995.
Self-Organizing Maps, Springer.Hindle, D. 1990.
Noun Classification From Predicate-Argument Structures, In the Proceedings of the 28thAnnual Meeting of the Association for Computa-tional Linguistics: 268-275Hatzivassiloglou,V.
and McKeown,R.K.
1993.
To-wards the Automatic Identification of AdjectivalScales: Clustering Adjectives According to Mean-ing, In the Proceedings of the 31st Annual Meetingof the Association for Computational Linguistics:172-182.Hagita, N. and Sawaki, M. 1995.
Robust Recognitionof Degraded Machine-Printed Characters usingComplimentary Similarity Measure and Error-Correction Learning?In the Proceedings of theSPIE ?The International Society for Optical Engi-neering, 2442: 236-244.Yamamoto, E. and Umemura, K. 2002.
A SimilarityMeasure for estimation of One?to-Many Relation-ship in Corpus, Journal of Natural Language Proc-essing: 45-75.Hans-Jorg Shmid.
2000.
English Abstract Nouns asConceptual Shells, Mouton de Gruyter.Kanzaki, K., Ma., Q. and Isahara, H. (2000), Similari-ties and Differences among Semantic Behaviors ofJapanese Adnominal Constituents, In the Proceed-ings of the Syntactic and Semantic Complexity inNatural Language Processing Systems, ANLP andNAACL.Ma, Q., Kanzaki, K., Murata, M., Uchimoto, K. andIsahara, H. 2000.
Self-Organization Semantic Mapsof Japanese Noun in Terms of Adnominal Constitu-ents, In Proceedings of IJCNN?2000, Como, Italy,vol.6.
: 91-96.
