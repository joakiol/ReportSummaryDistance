Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 403?411,Beijing, August 2010Paraphrase Alignment for Synonym Evidence DiscoveryGintare?
Grigonyte?Saarland Universityg.grigonyte@hmf.vdu.ltJoa?o Cordeiro, Gae?l Dias,Rumen MoraliyskiHULTIGUniversity of Beira Interior{jpaulo,ddg,rumen}@di.ubi.ptPavel BrazdilLIAAD/FEPUniversity of Portopbrazdil@liaad.up.ptAbstractWe describe a new unsupervised approachfor synonymy discovery by aligning para-phrases in monolingual domain corpora.For that purpose, we identify phrasalterms that convey most of the conceptswithin domains and adapt a methodol-ogy for the automatic extraction and align-ment of paraphrases to identify para-phrase casts from which valid synonymsare discovered.
Results performed on twodifferent domain corpora show that gen-eral synonyms as well as synonymic ex-pressions can be identified with a 67.27%precision.1 IntroductionSynonymy is a specific type of a semantic re-lationship.
According to (Sowa and Siekmann,1994), a synonym is a word (or concept) thatmeans the same or nearly the same as anotherword (or concept).
It has been observed thatwords are similar if their contexts are similar (Fre-itag et al, 2005) and so synonymy detection hasreceived a lot of attention during the last decades.However, words used in the same context arenot necessarily synonyms and can embody dif-ferent semantic relationships such as hyponyms,meronyms or co-hyponyms (Heylen et al, 2008).In this paper, we introduce a new unsupervisedmethodology for synonym detection by extract-ing and aligning paraphrases on normalized do-main corpora1.
In particular, we study a specificstructure within aligned paraphrases, paraphrase1By normalized, we intend that phrasal terms have beenpreviously identified.casts, from which valid synonyms are discovered.In fact, we propose a new approach based on theidea that synonyms are substitutable words withina given domain corpus.
Results performed on twodifferent domain corpora, the Corpus of ComputerSecurity (COCS) and the Corpus of Cancer Re-search (COCR), show that general synonyms aswell as synonymic expressions can be identifiedwith a 67.27% precision performance.2 Related WorkAutomatic synonymy detection has been tackledin a variety of ways which we explain as follows.2.1 Pattern-based ApproachesThis approach to information extraction is basedon a technique called selective concept extractionas defined by (Riloff, 1993).
Selective conceptextraction is a form of text skimming that selec-tively processes relevant text while effectively ig-noring surrounding text that is thought to be ir-relevant to the domain.
The pioneer of pattern-based approaches (Hearst, 1992) has introducedlexico-syntactic patterns to automatically acquiregiven word semantic relationships.
Specific pat-terns like ?X and other Y?
or ?X such as Y?
wereused for hypernym-hyponym detection.
Later, theidea was extended and adapted for synonymy byother researchers such as (Roark and Charniak,1998), (Caraballo, 1999) and (Maynard and Pe-ters, 2009).
In general, manual pattern definitionis time consuming and requires linguistic skills.Usually, systems based on lexico-syntactic pat-terns perform with very high precision, but lowrecall due to the fact that these patterns are rare.However, recent work by (Ohshima and Tanaka,4032009) on Web data reported high recall figures.To avoid manual encoding of patterns, many su-pervised approaches have been proposed as sum-marized in (Stevenson and Greenwood, 2006).2.2 Distributional SimilarityDistributional similarity for capturing semanticrelatedness is relying on the hypothesis that se-mantically similar words share similar contexts.These methods vary in the level of supervisionfrom unsupervised to semi-supervised or to su-pervised.
The first type of methods includes thework of (Hindle, 1990), (Lin, 1998) and (Heylenet al, 2008) who used unsupervised methodsfor detecting word similarities based on shallow-parsed corpora.
Others have proposed unsuper-vised methodologies to solve TOEFL-like tests,instead of discovering synonyms (Turney, 2001),(Terra and Clarke, 2003) and (Freitag et al, 2005).Other researchers, such as (Girju et al, 2004),(Muller et al, 2006), (Wu and Zhou, 2003) and(Wei et al, 2009), have used language or knowl-edge resources to enhance the representation ofthe vector space model.
Unlike the pattern-basedapproach, the distributional similarity-based ap-proach shows low precision compared to high re-call.One obvious way to verify all the possible con-nections between words of the vocabulary em-ploys an exhaustive search.
However, compari-son based on word usage can only highlight thoseterms that are highly similar in meaning.
Thismethod of representation is usually unable to dis-tinguish between middle strength and weak se-mantic relations, or detect the relationship be-tween hapax-legomena.2.3 Hybrid ApproachesMore recently, approaches combining patternsand distributional similarity appeared to bring thebest of the two methodologies.
(Hagiwara etal., 2009) describe experiments that involve train-ing various synonym classifiers.
(Giovannetti etal., 2008) use syntactically parsed text and man-ually composed patterns together with distribu-tional similarity for detecting semantically relatedwords.
Finally, (Turney, 2008) proposes a super-vised machine learning approach for discoveringsynonyms, antonyms, analogies and associations.For that purpose, feature vectors are based on fre-quencies of patterns and classified by a SVM.2.4 Our Approach(Van der Plas and Tiedemann, 2006) state that?People use multiple ways to express the sameidea.
These alternative ways of conveying thesame information in different ways are referredto by the term paraphrase and in the case ofsingle words or phrasal terms sharing the samemeaning, we speak of synonyms?.
Based on this,we propose that in order to discover pairs of se-mantically related words (in the best case syn-onyms) that may be used in figurative or raresense, and as consequence impossible to be iden-tified by the distributional similarity approach,we need to have them highlighted by their lo-cal specific environment.
Here we differ fromthe pattern-based approach that use local generalenvironment.
We propose to align paraphrasesfrom domain corpora and discover words that arepossibly substitutable for one another in a givencontext (paraphrase casts), and as such are syn-onyms or near-synonyms.
Comparatively to exist-ing approaches, we propose an unsupervised andlanguage-independent methodology which doesnot depend on linguistic processing2, nor manualdefinition of patterns or training sets and leads tohigher precision when compared to distributionalsimilarity-based approaches.3 Normalization of the CorporaThe main goal of our research is to build knowl-edge resources in different domains that can ef-fectively be used in different NLP applications.As such, precision takes an important part in theoverall process of our methodology.
For that pur-pose, we first identify the phrasal terms (or multi-word units) present in the corpora.
Indeed, it hasbeen shown in many works that phrasal terms con-vey most of the specific contents of a given do-main.
Our approach to term extraction is basedon linguistic pattern matching and Inverse Doc-ument Frequency (IDF) measurements for term2We will see in the next section that we will use linguisticresources to normalize our corpora, but the methodology canbe applied to any raw text.404quality assurance as explained in (Avizienis et al,2009).
For that purpose, we present a domain in-dependent hybrid term extraction framework thatincludes the following steps.
First, the text ismorphologically annotated with the MPRO sys-tem (Maas et al, 2009).
Then grammar rules formorphological disambiguation, syntactic parsingand noun phrase detection are applied based onfinite-state automata technology, KURD (Carl andSchmidt-Wigger, 1998).
Following this, a vari-ant and non-basic term form detection is applied,as well as stop words removal.
Then, combiningrich morphological and shallow syntactical analy-sis with pattern matching techniques allows us toextract a wide span of candidate terms which arefinally filtered with the well-known IDF measure.4 Paraphrase IdentificationA few unsupervised metrics have been applied toautomatic paraphrase identification and extraction(Barzilay and McKeown, 2001) and (Dolan et al,2004).
However, these unsupervised methodolo-gies show a major drawback by extracting quasi-exact or even exact match pairs of sentences asthey rely on classical string similarity measures.Such pairs are useless for our research purpose.More recently, (Cordeiro et al, 2007a) proposedthe sumo metric specially designed for asymmet-rical entailed pair identification in corpora whichobtained better performance than previously es-tablished metrics, even in corpora with exclu-sively symmetrical entailed paraphrases as in theMicrosoft Paraphrase Research Corpus (Dolan etal., 2004).
This function states that for a givensentence pair ?Sa, Sb?, having m and n words ineach sentence and ?
lexical exclusive links (wordoverlaps, see figure 1) between them, its lexi-cal connection strength is computed as defined inEquations 1 and 2.Sumo(Sa, Sb) =??
?S(m,n, ?)
if S(m,n, ?)
< 10 if ?
= 0e?kS(m,n,?)
otherwise(1)whereS(m,n, ?)
= ?
log2(m? )
+ ?
log2(n?
)?, ?
?
[0, 1], ?
+ ?
= 1(2)Figure 1: 4 exclusive links between Sa and Sb.To obtain a paraphrase corpus, we compute allsentence pairs similarities Sumo(Sa, Sb) and se-lect only those pairs exceeding a given threshold,in our case threshold = 0.85, which is quite re-strictive, ensuring the selection of pairs stronglyconnected3.However, to take into account the normalizationof the corpus, little adjustments had to be inte-grated in the methodology proposed in (Cordeiroet al, 2007a).
Indeed, the original Sumo(., .
)function was under-weighting links that occurredbetween phrasal terms such as ?molecular labo-ratory?
or ?renal cancer?.
So, instead of countingthe number of lexical links among sentences, eachlink weights differently according to the wordlength in the connection, hence connections oflonger words will result in a larger value.
For ex-ample, in figure 1, instead of having ?
= 4, wecount ?
= 3 + 8 + 7 + 4 = 22.
This adjust-ment is important as multi-word units are treatedas longer words in the corpus.
This modificationhas also, as a side effect, under-evaluation of func-tional words which usually follow the Zipf?s Lawand give more importance to meaningful words inthe paraphrase extraction process.5 Paraphrase AlignmentIn order to usefully explore the evidence syn-onymy from paraphrases, sentence alignmenttechniques must be applied to paraphrases in or-der to identify paraphrase casts, i.e., substitutableword pairs as shown in figure 2.
As we can see,the paraphrase cast includes three parts: the leftsegment (context), a middle segment and the rightsegment (context).
In our figure the left and rightsegments (contexts) are identical.In the context of DNA sequence alignment,two main algorithms have been proposed: (1) theNeedleman-Wunsch algorithm (Needleman and3Further details about the sumo metric are available in(Cordeiro et al, 2007a).405Figure 2: A paraphrase cast.Wunsch, 1970) based on dynamic programmingwhich outputs a unique global alignment and (2)the Smith-Waterman (SW) algorithm (Smith andWaterman, 1981) which is an adaptation of theprevious algorithm and outputs local alignments.In the context of NLP, (Cordeiro et al, 2007a)proposed a combination of both algorithms de-pending on the structure of paraphrase.
How-ever, since any local alignment is a candidate forparaphrase casts, the SW algorithm revealed it-self more appropriate and was always chosen.
TheSW alignment algorithm uses dynamic program-ming to compute the optimal local alignments be-tween two sequences4.
This process requires firstthe definition of an alignment matrix (function),which governs the likelihood of alignment of twosymbols.
Thus we first build a matrix H such thatH(i, 0) = 0 and H(0, j) = 0, for 0 ?
i ?
m,and 0 ?
j ?
n, where m and n are the number ofwords in the paraphrase sentences.
The rest of theH elements are recursively calculated as in Equa-tion 3 where gs(., .)
is the gap-scoring functionand Sai (resp.
Sbj ) represents the ith (resp.
jth)word of sentence Sa (resp.
Sb).H(i, j) = max?????????0H(i?
1, j ?
1) + gs(Sai , Sbj ) MMisatchH(i?
1, j) + gs(Sai , ) DeletionH(i, j ?
1) + gs( , Sbj ) Insertion(3)Obviously, this algorithm is based on an align-ment function which exploits the alignment like-lihood between two alphabet symbols.
For DNAsequence alignments, this function is defined asa mutation matrix, scoring gene mutation and gapalignments.
In our case, we define the gap-scoring4In our case, the two sequences are the two sentences ofa paraphrasefunction gs(., .)
in Equations 4 and 5 which prior-itize the alignment of specific domain key termsi.e., single match, or key expressions i.e., phrasalmatch, (reward 50), as well as lexically similar5words such as ?programme?
and ?programming?for example.
In particular, for these similar wordsan adaptation of the well known Edit Distance isused, the c(., .)
function (5), which is explained inmore detail in (Cordeiro et al, 2007b).gs(Sai , Sbj ) =???????????????
?1 if (Sai = ?)
and (Sbj 6= ?
)?1 if (Sbj = ?)
and (Sai 6= ?
)10 Single Match50 Phrasal Matchc(Sai , Sbj ) Mismatch(4)wherec(Sai , Sbj ) = ?edist(Sai , Sbj )+maxseq(Sai , Sbj )(5)To obtain local alignments, the SW algorithm isapplied, using the alignment function defined withH(., .)
in 3.
The alignment of the paraphrase infigure 2 would give the result in figure 3.Figure 3: An alignment.6 Paraphrase CastsIn order to discover synonyms, we are looking forspecial patterns from the aligned paraphrase sen-tences, which naturally give us more evidence forthe existence of equivalent terms or expressions.Due to the topological aspect of such patterns, wedecided to name them paraphrase casts, or justcasts as shown in figure 2.
As we have mentionedearlier, the paraphrase cast includes three parts:the left segment (contextL), a middle segment andthe right segment (contextR).
In the following ex-ample the left and right segments (contexts) areidentical, but the middle segment includes differ-ent misaligned sequences of words, representedby wordSa and wordSb.contextL wordSa ----- contextRcontextL ----- wordSb contextR5This is why we have in equation 3 the label ?Mismatch?,where ?mismatch?
means different yet lexically near words.406We can attribute different levels of confidenceto different paraphrase casts.
Indeed, the largerthe contexts and the smaller the misaligned se-quences are, the more likely it is for single orphrasal terms to be synonyms or near-synonyms.Note that in the cast shown in figure 3, each con-text has a significant size, with four words oneach side, and the misaligned segments are in factequivalent expressions i.e.
?paper?
is a synonymof ?research article?.
In the analyzed domainthese expressions are equivalent and interchange-able and appear to be interchangeable in other do-mains.
For the purpose of this paper, we onlytake into account the casts where the misalignedsequences of words contain only one word or onemulti-word unit in each sentence.
That is, we havea one-to-one match.
However, no constraints areimposed on the contexts6.
So, all casts are com-puted and analyzed for synonym discovery and re-sults are provided in the next section.7 ExperimentsTo evaluate our methodology we have usedtwo different corpora, both from scientific do-mains built from abstracts of publications (seeTable 1).
The corpus of computer secu-rity (COCS) is a collection of 4854 abstractson computer security extracted from the IEEE(http://ieee.rkbexplorer.com/) repository7.
Thecorpus of cancer research (COCR) contains 3334domain specific abstracts of scientific publica-tions extracted from the PubMed8 on three typesof cancer: (1) the mammary carcinoma register(COCR1) consisting of 1500 abstracts, (2) thenephroblastoma register (COCR2) consisting of1500 abstracts, and (3) the rhabdoid tumor regis-ter (COCR3) consisting of 334 abstracts.
Fromthe paraphrase casts, we were able to automat-ically extract, without further processing, singlesynonymous word pairs, as well as synonymicmulti-word units, as can be seen in Table 2.
Forthat purpose we have used specific paraphrasecasts, whose aim was to privilege precision to6This issue will be discussed in the next section.7An example of an abstract can be viewed at:http://ieee.rkbexplorer.com/description/publication-005346188http://www.ncbi.nlm.nih.gov/pubmedCorpus COCS COCR1 COCR2 COCR3Tokens 412.265 336.745 227.477 46.215Sentences 18.974 15.195 10.575 2.321Aligned Pairs 589 994 511 125Casts without filter 320 10.217 2.520 48Casts with filter 202 361 292 16Table 1: Corporarecall.
In particular, we have removed all castswhich contained numbers or special characters i.e.casts with filter in Table 1.
However, no con-straints were imposed on the frequency of thecasts.
Indeed, all casts were included even iftheir overall frequency was just one.
AlthoughSynonym (COCS) Complementaryfrequency tuning frequency controlattack consequences attack impacterror-free operation error free operationpseudo code pseudo algorithmtolerance resiliencepackage loss message lossadjustable algorithm context-aware algorithmhelpful comment valuable commentSynonym (COCR) Complementarychildhood renal tumor childhood kidney tumorhypertrophy growthdoxorubicin vincristinecarcinomas of the kidney sarcoma of the kidneymetastasis neoplasmrenal tumor renal malignancyneoplastic thrombus tumor thrombusvincristine adriamycinTable 2: Synonyms for COCSmost of the word relationships were concernedwith synonymy, the other ones were not just er-rors, but lexically related words, namely examplesof antonymy, hyperonymy/hyponymy and associ-ations as shown in Table 3.
In the evaluation, weAntonym Complementarypositive sentinel nodes negative sentinel nodeshigher bits lower bitsolder version newer versionHypernym HyponymMulti-Tasking Virtual Machine Java Virtual Machinetherapy chemotherapyhormone breast cancer estrogen breast cancerAssociation Complementaryperformance reliabilitystatistical difference significant differencerelationship correlationTable 3: Other Word Semantic Relationships.have focused on the precision of the method.
Theevaluation of the extracted pairs was performedmanually by two domain experts.
In fact, four407different evaluations were carried out dependingon whether the adapted S(., .)
measure was used(or not) and whether the normalization of the cor-pora was used (or not).
The best results were ob-tained in all cases for the adapted S(., .)
measurewith the multi-word units.
Table 4 shows also theworst results for the COCS as a baseline (COCS(1)), i.e.
non-adapted S(., .)
and non-normalizedcorpus.
For the rest of the experiments we alwayspresent the results with the adapted S(., .)
mea-sure and normalized corpus.Corpus COCS (1) COCS (2)Precision 54.62% 71.29%Extracted Synonyms 130 144Errors 108 58Corpus COCR1 COCR2 COCR3Precision 69.80% 61.30% 75.00%Extracted Synonyms 252 178 12Errors 109 111 4Table 4: Overall Results7.1 DiscussionMany results have been published in the literature,especially tackling the TOEFL synonym detectionproblem which aims at identifying the correct syn-onym among a small set of alternatives (usuallyfour).
For that purpose, the best precision rate hasbeen reached by (Turney et al, 2003) with 97.50%who have exploited many resources, both statis-tical and linguistic.
However, our methodologytackles a different problem.
Indeed, our goalis to automatically extract synonyms from texts.The published works toward this direction havenot reached so good results.
One of the latest stud-ies was conducted by (Heylen et al, 2008) whoused distributional similarity measures to extractsynonyms from shallow parsed corpora with thehelp of unsupervised methods.
They report that?the dependency-based model finds a tightly re-lated neighbor for 50% of the target words and atrue synonym for 14%?.
So, it means that by com-paring all words in a corpus with all other words,one can expect to find a correct semantic relation-ship in 50% of the cases and a correct synonymin just 14%.
In that perspective, our approachreaches higher results.
Moreover, (Heylen et al,2008) use a frequency cut-off which only selectsfeatures that occur at least five times together withthe target word.
In our case, no frequency thresh-old is imposed to enable extraction of synonymswith low frequency, such as hapax legomena.
Thissituation is illustrated in figure 4.
We note thatmost of the candidate pairs appear only once inthe corpus.1 2 3 4 5SynonymousNon?synonymousFrequency of candidate pairsLog numberof candidatepairs0123456Figure 4: Synonyms Frequency Distribution.In order to assess the quality of our results,we calculated the similarity between all extractedpairs of synonyms following the distributionalanalysis paradigm as in (Moraliyski and Dias,2007) who build context9 feature vectors for nounsynonyms.
In particular, we used the cosine sim-ilarity measure and the Loglike association mea-sure (Dunning, 1993) as the weighting scheme ofthe context features.
The distribution of the simi-larity measure for all noun synonyms (62 pairs) isshown in figure 5.SimilarityFrequency0.0 0.2 0.4 0.6 0.8 1.0051015202530Figure 5: Synonym Pairs Similarity Distribution.The results clearly show that all extracted syn-onyms are highly correlated in terms of context.9In this case, the contexts are the surrounding nouns,verbs and adjectives in the closest chunks of a shallow parsedcorpus.408Nearly half of the cases have similarities higherthan 0.5.
It is important to notice that a spe-cific corpus10 was built to calculate as sharply aspossible the similarity measures as it is done in(Moraliyski and Dias, 2007).
Indeed, based onthe COCS and the COCR most statistics were in-significant leading to zero-valued features.
Thissituation is well-known as it is one of the majordrawbacks of the distributional analysis approachwhich needs huge quantities of texts to make se-cure decisions.
So we note that applying the distri-butional analysis approach to such small corporawould have led to rather poor results.
Even withan adapted corpus, figure 5 (left-most bar) showsthat there are no sufficient statistics for 30 pairs ofsynonyms.
Although the quality of the extractedpairs of synonyms is high, the precision remainsrelatively low with 67.27% precision on average.As we pointed out in the previous section, we didnot make any restrictions to the left and right con-texts of the casts.
However, the longer these con-texts are, compared to the misaligned sequence ofwords, the higher the chance is that we find a cor-rect synonym.
Table 5 shows the average lengthsof both cast contexts for synonyms and erroneouspairings, in terms of words (WCL) and charac-ters (CCL).
We also provide the ratio (R) betweenthe character lengths of the middle segment (i.e.misaligned character sequences) and the charac-ter lengths of the cast contexts (i.e.
right and leftsizes of equally aligned character sequences).
It isType WCL CCL RSynonyms 2.70 11.67 0.70Errors 2.45 8.05 0.55Table 5: Average Casts Contexts Lengthsclear that a more thorough study of the effects ofthe left and right contexts should be carried out toimprove precision of our approach, although thismay be to the detriment of recall.
Based on theresults of the ratio R11, we note that the larger thecast context is compared to the cast content, themore likely it is that the misaligned words are syn-onyms.10This corpus contains 125.888.439 words.11These results are statistically relevant with p?
value <0.001 using the Wilcoxon Rank-Sum Test.8 ConclusionsIn this paper we have introduced a new unsu-pervised methodology for synonym detection thatinvolves extracting and aligning paraphrases onnormalized domain corpora.
In particular, wehave studied a specific structure within alignedparaphrases, paraphrase casts, from which validsynonyms were discovered.
The overall preci-sion was 71.29% for the computer security do-main and 66.06% for the cancer research domain.This approach proved to be promising for ex-tracting synonymous words and synonymic multi-word units.
Its strength is the ability to effectivelywork with small domain corpora, without super-vised training, nor definition of specific language-dependent patterns.
Moreover, it is capable toextract synonymous pairs with figurative or raresenses which would be impossible to identify us-ing the distributional similarity approach.
Finally,our approach is completely language-independentas it can be applied to any raw text, not obli-gatorily normalized corpora, although the resultsfor domain terminology may be improved by theidentification of phrasal terms.However, further improvements of the methodshould be considered.
A measure of quality of theparaphrase casts is necessary to provide a mea-sure of confidence to the kind of extracted wordsemantic relationship.
Indeed, the larger the con-texts and the smaller the misaligned sequencesare, the more likely it is for single or phrasal termsto be synonyms or near-synonyms.
Further workshould also be carried out to differentiate the ac-quired types of semantically related pairs.
As itis shown in Table 3, some of the extracted pairswere not synonymic, but lexically related wordssuch as antonyms, hypernyms/hyponyms and as-sociations.
A natural follow-up solution for dis-criminating between semantic types of extractedpairs could involve context-based classification ofacquired casts pairs.
In particular, (Turney, 2008)tackled the problem of classifying different lexi-cal information such as synonymy, antonymy, hy-pernymy and association by using context words.In order to propose a completely unsupervisedmethodology, we could also follow the idea of(Dias et al, 2010) to automatically construct small409TOEFL-like tests based on sets of casts whichcould be handled with the help of different dis-tributional similarities.AcknowledgmentsWe thank anonymous reviewers whose commentshelped to improve this paper.
We also thankIFOMIS institute (Saarbrucken) and the ReSISTproject for allowing us to use the COCR andCOCS corpora.ReferencesAvizienis, A., Grigonyte, G., Haller, J., von Henke,F., Liebig, T., and Noppens, O.
2009.
Organiz-ing Knowledge as an Ontology of the Domain ofResilient Computing by Means of NLP - An Experi-ence Report.
In Proc.
of the 22th Int.
FLAIRS Conf.AAAI Press, pp.
474-479.Barzilay, R. and McKeown, K. R. 2001.
ExtractingParaphrases from a Parallel Corpus.
In Proc.
of the39th meeting of ACL, pp.
50-57.Caraballo, S. A.
1999.
Automatic Construction of aHypernym-labeled Noun Hierarchy from Text.
InProc.
of 37th meeting of ACL 1999, pp 120-126.Carl, M., and Schmidt-Wigger, A.
1998.
Shallow PostMorphological Processing with KURD.
In Proc.
ofthe Conf.
on New Methods in Language Processing.Cordeiro, J.P., Dias, G. and Brazdil, P. 2007.
LearningParaphrases from WNS Corpora.
In Proc.
of the20th Int.
FLAIRS Conf.
AAAI Press, pp.
193-198.Cordeiro, J.P., Dias, G. and Cleuziou, G. 2007.
Biol-ogy Based Alignments of Paraphrases for SentenceCompression.
In Proc.
of the 20th meeting of ACL,workshop PASCAL, pp.
177-184.Dias, G., Moraliyski, R., Cordeiro, J.P., Doucet, A. andAhonen-Myka, H. 2010.
Automatic Discovery ofWord Semantic Relations using Paraphrase Align-ment and Distributional Lexical Semantics Analy-sis.
In Journal of Natural Language Engineering,Cambridge University Press.
ISSN 1351-3249, pp.1-26.Dolan, B., Quirk, C. and Brockett, C. 2004.
Un-supervised Construction of Large Paraphrase Cor-pora: Exploiting Massively Parallel News Sources.In Proc.
of the 20th int.
Conf.
on ComputationalLinguistics.Dunning T. D 1993.
Accurate Methods for the Statis-tics of Surprise and Coincidence.
In ComputationalLinguistics, 19(1), pp.
61-74.Freitag, D., Blume, M., Byrnes, J., Chow, E., Kapa-dia, S., Rohwer, R. and Wang, Z.
2005.
New Ex-periments in Distributional Representations of Syn-onymy.
In Proc.
of 9th conf.
on Computational Nat-ural Language Learning, pp.
25-32.Giovannetti, E., Marchi, S. and Montemagni, S.2008.
Combining Statistical Techniques andLexico-Syntactic Patterns for Semantic RelationsExtraction from Text.
In Proc.
of the 5th Workshopon Semantic Web Applications and Perspectives.Girju, R., Giuglea, A. M., Olteanu, M., Fortu, O.,Bolohan, O. and Moldovan, D. 2004.
Support Vec-tor Machines Applied to the Classification of Se-mantic Relations in Nominalized Noun Phrases.
InProc.
of the HLT-NAACL Workshop on Computa-tional Lexical Semantics, pp.
68-75.Hagiwara, M. O. Y. and Katsuhiko, T. 2009.
Su-pervised Synonym Acquisition using DistributionalFeatures and Syntactic Patterns.
In Information andMedia Technologies 4(2), pp.
558-582.Hearst, M. A.
1992.
Automatic Acquisition of Hy-ponyms from Large Text Corpora.
In Proc.
of the14th conf.
on Computational Linguistics, pp.
539-545.Heylen, K., Peirsman, Y., Geeraerts, D. and Speelman,D.
2008.
Modelling Word Similarity: an Evalua-tion of Automatic Synonymy Extraction Algorithms.In Proc.
of the 6th LREC.Hindle, D. 1990.
Noun Classification from Predicate-Argument Structures.
In Proc.
of the 28th meetingof ACL, pp.
268-275.Inkpen, D. 2007.
A Statistical Model for Near-Synonym choice.
In ACM Trans.
Speech Lang.
Pro-cess.
4(1), 1-17.Kiefer, J.
1953.
Sequential Minimax Search for aMaximum.
In Proc.
of the American MathematicalSociety 4, pp.
502-506.Lin, D. 1998.
Automatic Retrieval and Clustering ofSimilar Words.
In Proc.
of the 17th Int.
Conf.
onComputational Linguistics, pp.
768-774.Maas, D., Rosener, Ch., and Theofilidis, A.
2009.Morphosyntactic and Semantic Analysis of Text:The MPRO Tagging Procedure.
Workshop on sys-tems and frameworks for computational morphol-ogy.
Springer., pp.
76-87.Maynard, D. F. A. and Peters, W. 2009.
Using lexico-syntactic Ontology Design Patterns for OntologyCreation and Population.
In Proc.
of the Workshopon Ontology Patterns.410Moraliyski, R., and Dias, G. 2007.
One Sense perDiscourse for Synonym Detection.
In Proc.
of theInt.
Conf.
On Recent Advances in NLP, Bulgaria,pp.
383-387.Muller, P., Hathout, N. and Gaume, B.
2006.
SynonymExtraction Using a Semantic Distance on a Dictio-nary.
In Proc.
of the 1st Workshop on Graph-BasedMethods for NLP, pp.
65-72.Needleman, S. B. and Wunsch, C. D. 1970.
A GeneralMethod Applicable to the Search for Similarities inthe Amino Acid Sequence of two Proteins.
In Jour-nal of Molecular Biology 48(3), pp.
443-453.Ohshima, H. and Tanaka, K. 2009.
Real Time Ex-traction of Related Terms by Bi-directional lexico-syntactic Patterns from the Web.
In Proc.
of the 3rdInt.
Conf.
on Ubiquitous Information Managementand Communication, pp.
441-449.Riloff, E. 1993.
Automatically Constructing a Dic-tionary for Information Extraction Tasks.
In Proc.of the 11th Nat.
Conf.
on Artificial Intelligence, pp.811-816.Roark, B. and Charniak, E. 1998.
Noun-phraseCo-occurrence Statistics for Semiautomatic Seman-tic Lexicon Construction.
In Proc.
of the 17thInt.
Conf.
on Computational Linguistics, pp.
1110-1116.Smith, T. and Waterman, M. 1981.
Identification ofCommon Molecular Subsequences.
In Journal ofMolecular Biology 147, pp.
195-197.Sowa, J. F. and Siekmann, J. H. 1994.
Concep-tual Structures: Current Practices.
Springer-VerlagNew York, Inc., Secaucus, NJ, USA.Stevenson, M. and Greenwood, M. 2006.
Compar-ing Information Extraction Pattern Models.
In Proc.of the Workshop on Information Extraction Beyondthe Document, ACL, pp.
29-35.Terra, E. and Clarke, C. 2003.
Frequency Estimatesfor Statistical Word Similarity Measures.
In Proc.of HTL/NAACL 2003, pp.
165-172.Turney, P. D. 2001.
Mining the Web for Synonyms:PMI-IR versus LSA on TOEFL.
Lecture Notes inComputer Science, 2167, pp.
491-502.Turney, P. D., Littman, M. L., Bigham, J. and Shnay-der, V. 2003.
Combining Independent Modules inLexical Multiple-choice Problems.
In Recent Ad-vances in NLP III: Selected Papers, pp.
101-110.Turney, P. D. 2008.
A Uniform Approach to Analogies,Synonyms, Antonyms and Associations.
In Proc .ofthe 22nd Int.
Conf.
on Computational Linguistics,pp.
905-912.Van der Plas, L. and Tiedemann, J.
2006.
Finding Syn-onyms Using Automatic Word Alignment and Mea-sures of Distributional Similarity.
In Proc.
of the21st Int.
Conf.
on Computational Linguistics, pp.866-873.Wei, X., Peng, F., Tseng, H., Lu, Y. and Dumoulin,B.
2009.
Context Sensitive Synonym Discovery forWeb Search Queries.
In Proc.
of the 18th ACMconference on Information and Knowledge Man-agement, pp.
1585-1588.Wu, H. and Zhou, M. 2003.
Optimizing SynonymExtraction Using Monolingual and Bilingual Re-sources.
In Proc.
of the 2nd Int.
Workshop on Para-phrasing, pp.
72-79.411
