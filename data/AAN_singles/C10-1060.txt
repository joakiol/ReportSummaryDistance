Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 528?536,Beijing, August 2010Unsupervised phonemic Chinese word segmentation using AdaptorGrammarsMark JohnsonDepartment of ComputingMacquarie UniversityMark.Johnson@mq.edu.auKatherine DemuthDepartment of LinguisticsMacquarie UniversityKatherine.Demuth@mq.edu.auAbstractAdaptor grammars are a framework forexpressing and performing inference overa variety of non-parametric linguisticmodels.
These models currently providestate-of-the-art performance on unsuper-vised word segmentation from phonemicrepresentations of child-directed unseg-mented English utterances.
This paper in-vestigates the applicability of these mod-els to unsupervised word segmentation ofMandarin.
We investigate a wide vari-ety of different segmentation models, andshow that the best segmentation accuracyis obtained frommodels that capture inter-word ?collocational?
dependencies.
Sur-prisingly, enhancing the models to exploitsyllable structure regularities and to cap-ture tone information does improve over-all word segmentation accuracy, perhapsbecause the information these elementsconvey is redundant when compared to theinter-word dependencies.1 Introduction and previous workThe word-segmentation task is an abstraction ofpart of the problem facing a child learning its na-tive language.
Fluent speech, even the speech di-rected at children, doesn?t come with silence orpauses delineating acoustic words the way thatspaces separate orthographic words in writing sys-tems like that of English.
Instead, as most peoplelistening to a language they don?t understand canattest, words in fluent speech ?run together?, and alanguage user needs to learn how to segment utter-ances of the language they are learning into words.This kind of word segmentation is presumably animportant first step in acquiring a language.
It isscientifically interesting to know what informa-tion might be useful for word segmentation, andjust how this information might be used.
Thesescientific questions have motivated a body of re-search on computational models of word segmen-tation.
Since as far as we can tell any child canlearn any human language, our goal is to developa single model that can learn to perform accurateword segmentation given input from any humanlanguage, rather than a model that specialised toperform well on a single language.
This paperextends the previous work on word segmentationby investigating whether one class of models thatwork very well with English input also work withChinese input.
These models will permit us tostudy the role that syllable structure constraintsand tone in Chinese might play in word segmenta-tion.While learners and fluent speakers undoubt-edly use a wide variety of cues to perform wordsegmentation, computational models since El-man (1990) have tended to focus on the useof phonotactic constraints (e.g., syllable-structureconstrains) and distributional information.
Brentand Cartwright (1996) introduced the standardform of theword segmentation task still studied to-day.
They extracted the orthographic representa-tions of child-directed speech from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) and ?pho-nologised?
them by looking up each word in apronouncing dictionary.
For example, the or-thographic utterance you want to see the bookis mapped to the sequence of pronunciations yuwant tu si D6 bUk, (the pronunciations are in an528ASCII encoding of the International Phonetic Al-phabet representation of English phonemes).
Theinput to the learner is obtained by concatenatingtogether the phonemic representations of each ut-terance?s words.
The learner?s task is to identifythe locations of the word boundaries in this se-quence, and hence identify the words (up to ho-mophony).
Brent and Cartwright (1996) pointedout the importance of both distributional informa-tion and phonotactic (e.g., syllable-structure) con-straints for word segmentation (see also Swingley(2005) and Fleck (2008)).Recently there has been considerable interest inapplying Bayesian inference techniques for non-parametric models to this problem.
Here the term?non-parametric?
does not mean that the modelshave no parameters, rather, it is used to distinguishthese models from the usual ?parametric models?that have a fixed finite vector of parameters.Goldwater et al (2006) introduced two non-parametric Bayesian models of word segmenta-tion, which are discussed in more detail in (Gold-water et al, 2009).
The unigram model, which as-sumes that each word is generated independentlyto form a sentence, turned out to be equivalentto a model originally proposed by Brent (1999).The bigram model improves word segmentationaccuracy by modelling bigram inter-word contex-tual dependencies, ?explaining away?
inter-worddependencies that would otherwise cause the uni-gram model to under-segment.
Mochihashi et al(2009) showed that segmentation accuracy couldbe improved by using a more sophisticated ?basedistribution?
and a dynamic programming sam-pling algorithm very similar to the one used withthe adaptor grammars below.
They also appliedtheir algorithm to Japanese and Chinese word seg-mentation, albeit from orthographic rather thanphonemic forms, so unfortunately their results arenot comparable with ours.Johnson et al (2007) introduced adaptor gram-mars as a grammar-based framework for express-ing a variety of non-parametric models, and pro-vided a dynamic programming Markov ChainMonte Carlo (MCMC) sampling algorithm forperforming Bayesian inference on these models.For example, the unigram model can be expressedas a simple adaptor grammar as shown below, andthe generic adaptor grammar inference procedureprovides a dynamic programming sampling algo-rithm for this model.
Johnson (2008b) showedhow a variety of different word segmentationmodels can be expressed as adaptor grammars, andJohnson and Goldwater (2009) described a num-ber of extensions and specialisations to the adaptorgrammar framework that improve inference speedand accuracy (we use these techniques in our workbelow).Previous work on unsupervised word segmen-tation from phonemic input has tended to concen-trate on English.
However, presumably childrenthe world over segment their first language inputin the same (innately-specified) way, so a correctprocedure should work for all possible human lan-guages.
However, as far as we are aware there hasbeen relatively little work on word segmentationfrom phonemic input except on English.
Johnson(2008a) investigated whether the adaptor gram-mars models that do very well on English also ap-ply to Sesotho (a Bantu language spoken in south-ern Africa with rich agglutinating morphology).He found that the models in general do very poorly(presumably because the adaptor grammars usedcannot model the complex morphology found inSesotho) and that the best segmentation accuracywas considerably worse than that obtained for En-glish, even when that model incorporated someBantu-specific information about morphology.
Ofcourse it may also be that the Sesotho and Englishcorpora are not really comparable: the Bernstein-Ratner corpus that Brent and other researchershave used for English was spoken to pre-linguistic1-year olds, whilemost non-English corpora are ofchild-directed speech to older children who are ca-pable of talking back, and hence these corpora arepresumably more complex.
We discuss this issuein more detail in section 4 below.2 A Chinese word segmentation corpusOur goal here is to prepare a Chinese corpus ofchild-directed speech that parallels the Englishone used by Brent and other researchers.
Thatcorpus was in broad phonemic form, obtained bylooking each word up in a pronouncing dictio-nary.
Here instead we make use of a corpus inPinyin format, which we translate into a broad529phonemic IPA format using the freely-availablePinyin-to-IPA translation program ?Pinyin toIPA Conversion Tools?
version 2.1 available onhttp://sourceforge.net/projects/py2ipa.We used the ?Beijing?
corpus (Tardif, 1993)available from the publicly-distributed Childescollection of corpora (MacWhinney and Snow,1985).
We are interested in child-directed speech(rather than children?s speech), so we removed allutterances from participants with an Id containing?Child?.
(Tardif (1993) points out that Chinese-speaking children typically have a much richersocial environment involving multiple adult care-givers than middle-class English-speaking chil-dren do, so we cannot simply collect only themother?s utterances, as was done for the Englishcorpus).
We also ignored all utterances with codes$INTERJ, $UNINT, $VOC and $PRMPT, as these arenot always linguistic utterances.
In addition, wedeleted all words that could not be analysed as asequence of syllables, such as ?xxx?
and ?hmm?,and also deleted ?cluck?.
The first few utterancesof the corpus in Pinyin format are:zen3me gei3 ta1 bei1 shang4 lai2 (1.)
?ta1: (.)
a1yi2 gei3 de (.)
ta1 gei3 de .hen3 jian3dan1 .We then fed these into the Pinyin-to-IPA trans-lation program, producing output of the followingformat:ts?n214m?
kei214 t?a55 pei55 ??
?51 lai35t?a55 a55i35 kei214 t?
t?a55 kei214 t?x?n214 t?i?n214tan55In the IPA format, the superscript indices in-dicate the tone patterns associated with syllables;these appear at the end of each syllable, as is stan-dard.
While we believe there are good linguisticreasons to analyse tones as associated with syl-lables, we moved all the tones so they immedi-ately followed the final vowel in each syllable.We did this because we thought that locating tonesafter the syllable-final consonant might give ourmodels a strong cue as to the location of sylla-ble boundaries, and since words often end at syl-lable boundaries, this would make the word seg-mentation problem artificially easier.
(Our modelstake a sequence of symbols as input, so the tonesmust be located somewhere in the sequence.
How-ever, the linguistically ?correct?
solution wouldprobably be to extend the models so they couldprocess input in an auto-segmental format (Gold-smith, 1990) where tones would be on a separatetier and unordered with respect to the segmentswithin a syllable.
)In order to evaluate the importance of tonefor our word-segmentation models we also con-structed a version of our corpus in which all toneswere removed.
We present results for all of ourmodels on two versions of the corpus, one thatcontains tones following the vowels, and anotherthat contains no tones at all.
These two cor-pora constitute the ?gold standard?
against whichour word segmentation models will be evaluated.These corpora contain 50,118 utterances, consist-ing of 187,533 word tokens.The training data provided to the word segmen-tation models is obtained by segmenting the golddata at all possible boundary locations.
Conso-nant clusters, diphthongs and tones (if present) aretreated as single units, so the training data appearsas follows:ts ?
214 n m ?
k e i 214 t?
a 55 p e i 55 ?
?
51 ?
l ai 35t?
a 55 a 55 i 35 k e i 214 t ?
t?
a 55 k e i 214 t ?x ?
214 n t?
i?
214 n t a 55 nThe task of a word-segmentation model isto identify which of these possible bound-ary locations correspond to actual word bound-aries.
The training corpus without tones contains531,384 segments, while the training corpus withtones contains 712,318 segments.3 Adaptor grammars for wordsegmentationAdaptor grammars were first introduced by John-son et al (2007) as a grammar-based frame-work for specifying hierarchical non-parametricBayesian models, and Johnson and Goldwater(2009) describes a number of implementation de-tails that significantly improve performance; theinterested reader should consult those papers for afull technical introduction.
Johnson (2008b) pro-posed a number of adaptor grammars for Englishword segmentation, which we review and mini-mally modify here so they can perform Chinese530word segmentation as well.
In section 4 we evalu-ate these adaptor grammars on the Chinese corpusjust described.The grammars vary along two orthogonal di-mensions, which correspond to the kinds of gen-eralisations that the model can learn.
The sim-plest grammar is the unigram adaptor grammar,which generates an utterance as an i.i.d.
sequencesof words, where each word is a sequence ofphonemes.
The collocation adaptor grammarscapture dependencies above the word level bygenerating collocations, or groups of words, asmemoized units.
The syllable adaptor grammarscapture dependencies below the word level bygenerating words as sequences of syllables ratherthan phonemes.3.1 Unigram adaptor grammarsIn order to motivate adaptor grammars as an ex-tension to Probabilistic Context-Free Grammars(PCFGs), consider an attempt to perform unsuper-vised word segmentation with a PCFG containingthe following rules (ignore the underlining of theWord non-terminal for now).Words?Words WordWords?WordWord?
PhonsPhons?
PhonPhons?
Phons PhonPhons?
Phons TonePhon?
ai | o | ?
| ?
| t??
| ?Tone?
35 | 55 | 214 | ?
(1)In this grammar, Phon expands to all thephonemes appearing in the phonemic trainingdata, and Tone expands to all of the tone patterns.
(In this and all of the other grammars in this paper,the start symbol is the non-terminal symbol of thefirst rule in the grammar.
This grammar, like allothers in this paper, is crafted so that aWord sub-tree can never begin with a Tone, so the presenceof tones does not make the segmentation problemharder).The trees generated by this grammar are suffi-ciently expressive to represent any possible seg-mentation of any sequence of phonemes intowords (including the true segmentation); a typi-cal segmentation is shown in Figure 1.
However,WordsWordsWordPhonsPhonsPhonsPhonpPhonuTone35WordPhonsPhonsPhonsPhonsPhonk?PhonaTone51PhonnFigure 1: A parse tree generated by the unigramgrammar, where adapted and non-adapted non-terminals are shown.
It depicts a possible segmen-tation of p u 35 k?
a 51 n.it should also be clear that no matter how we varythe probabilities on the rules of this grammar, thegrammar itself cannot encode the subset of treesthat correspond to words of the language.
In or-der to do this, a model would need to memorise theprobabilities of entire Word subtrees, since theseare the units that correspond to individual words,but this PCFG simply is not expressive enough todo this.Adaptor grammars learn the probabilities ofsubtrees in just this way.
An adaptor grammar isspecified via a set of rules or productions, just likea CFG, and the set of trees that an adaptor gram-mar generates is exactly the same as the CFG withthose rules.However, an adaptor grammar defines proba-bility distributions over trees in a completely dif-ferent fashion to a PCFG: for simplicity we fo-cus here on the sampling or predictive distribu-tion, which defines the probability of generatingan entire corpus of trees.
In a PCFG, the prob-ability of each non-terminal expanding using agiven rule is determined by the probability of thatrule, and is independent of the expansions of theother non-terminals in the tree.
In an adaptorgrammar a subset of the non-terminals are des-531ignated as adapted.
We indicate adapted non-terminals by underlining them, so Word is theonly adapted non-terminal in (1).
Unadapted non-terminals expand just as in a PCFG: a produc-tion is chosen according to the production prob-abilities.
An adapted non-terminal can expandin two different ways.
With probability propor-tional to n(t)?
aA an adapted non-terminal A ex-pands to a tree t rooted in A that has been pre-viously generated, while with probability propor-tional to m(A)aA + bA the adapted non-terminalA expands using some grammar rule, just as in aPCFG.
Here n(t) is the number of times tree t hasbeen previously generated,m(A) is the number oftrees rooted in A that have been previously gener-ated using grammar rules, and 0 ?
aA ?
1 andbA > 0 are adjustable parameters associated withthe adapted non-terminal A.Technically, this is known as a Pitman-Yor Pro-cess (PYP) with concentration parameters aA andbA, where the PCFG rules define the base distri-bution of the process.
(The PYP is a generalisa-tion of the Chinese Restaurant Process (CRP); aCRP is a PYP with parameter a = 0).
Ratherthan setting the concentration parameters by hand(there are two for each adapted non-terminal inthe grammar) we follow Johnson and Goldwater(2009) and put uniform Beta and vague Gammapriors on each of these parameters, and use sam-pling to explore their posterior values.Because the probability of selecting a tree t isproportional to n(t), an adaptor grammar is a kindof ?rich-get-richer?
process that generates power-law distributions.
Depending on the values of aAand bA, most of the probability mass can windup concentrated on just a few trees.
An adaptorgrammar is a kind of ?cache?
model, in whichpreviously generated subtrees are stored and morelikely to be reused in later sentences.
That is, whilean adapted non-terminal A can expand to any treerooted inA that can be constructed with the gram-mar rules, in practice it is increasingly likely toreuse the same trees over and over again.
It canbe viewed as a kind of tree substitution grammar(Joshi, 2003), but where the tree fragments (aswell as their probabilities) are learnt from the data.The unigram grammar is the simplest of theword segmentation models we investigate in thispaper (it is equivalent to the unigram model inves-tigated in Goldwater et al (2009)).
Because thegrammars we present below rapidly become longand complicated to read if each grammar rule isexplicitly stated, we adopt the following conven-tions.
We use regular expressions to abbreviateour grammars, with the understanding that the reg-ular expressions are always expanded produce aleft-recursive structure.
For example, the unigramgrammar in (1) is abbreviated as:Words?Word+Word?
Phon (Phon | Tone)?Phon?
ai | o | ?
| ?
| t??
| ?Tone?
35 | 55 | 214 | ?
(2)3.2 Collocation adaptor grammarsGoldwater et al (2006) and Goldwater et al(2009) demonstrated the importance of contex-tual dependencies for word segmentation, and pro-posed a bigram model in order to capture someof these.
It turns out that while the bigram modelcannot be expressed as an adaptor grammar, a col-location model, which captures similar kinds ofcontextual dependencies, can be expressed as anadaptor grammar (Johnson et al, 2007).
In a col-location grammar there are two different adaptednon-terminals; Word and Colloc; Word expandsexactly as in the unigram grammar (2), so it is notrepeated here.Collocs?
Colloc+Colloc?WordsWords?Word+(3)A collocation adaptor grammar caches bothwords and collocations (which are sequences ofwords).
An utterance is generated by generatingone or more collocations.
The PYP associatedwith collocations either regenerates a previouslygenerated collocation or else generates a ?fresh?collocation by generating a sequence of words ac-cording to the PYP model explained above.The idea of aggregating words into collocationscan be reapplied at a more abstract level by ag-gregating collocations into ?super-collocations?,which are sequences of collocations.
This in-volves adding the following additional rules to thegrammar in (3):532Colloc2s?
Colloc2+Colloc2?
Collocs+ (4)There are three PYPs in a grammar with 2 lev-els of collocations, arranged in a strict Bayesianhierarchy.
It should be clear that this process canbe repeated indefinitely; we investigate grammarswith up to three levels of collocations below.
(Itshould be possible to use Bayesian techniques tolearn the appropriate number of levels in the hier-archy, but we leave this for future work).3.3 Syllable structure adaptor grammarsBrent and Cartwright (1996) and others emphasisethe role that syllable-structure and other phono-tactic constraints might play in word segmenta-tion.
Johnson (2008b) pointed out that adaptorgrammars can learn at least some of these kindsof generalisations.
It?s not unreasonable to as-sume that language learners can learn to groupphonemes into syllables, and that they can exploitthis syllabic structure to perform word segmenta-tion.
The syllable-structure grammars we describebelow assume that word boundaries are alwaysaligned with syllable boundaries; this is not uni-versally true, but it is reliable enough to dramati-cally improve unsupervised word segmentation inEnglish.There is considerable cross-linguistic varia-tion in the syllable-structure and phonotactic con-straints operative in the languages of the world, sowe?d like to avoid ?building in?
language-specificconstraints into our model.
We therefore make therelatively conservative assumption that the childcan distinguish vowels from consonants, and thatthe child knows that syllables consist of Onsets,Nuclei and Codas, that Onsets and Codas consistof arbitrary sequences of consonants while Nucleiare arbitrary sequences of vowels and tones, andthat Onsets and Codas are optional.
Notice thatsyllable structure in both English and Chinese isconsiderably more constrained than this; we usethis simple model here because it has proved suc-cessful for English word segmentation.The syllable-structure adaptor grammars re-place the rules expanding Words with the follow-ing rules:Word?
SyllWord?
Syll SyllWord?
Syll Syll SyllWord?
Syll Syll Syll SyllSyll?
(Onset)?
RhyOnset?
C+Rhy?
Nucleus (Coda)?Nucleus?
V (V | Tone)?Coda?
C+C?
?
| t??
| ?V?
ai | o | ?
(5)In these rules the superscript ???
indicates op-tionality.
We used the relatively cumbersomemechanism of enumerating each possible numberof syllables per word (we permit words to consistof from 1 to 4 syllables, although ideally this num-ber would not be hard-wired into the grammar)because a relatively trivial modification of thisgrammar can distinguish word-initial and word-final consonant clusters from word-internal clus-ters.
Johnson (2008b) demonstrated that this sig-nificantly improves English word segmentationaccuracy.
We do not expect this to improve Chi-nese word segmentation because Chinese clustersdo not vary depending on their location within theword, but it will be interesting to see if the addi-tional cluster flexibility that is useful for Englishsegmentation hurts Chinese segmentation.In this version of the syllable-structure gram-mar, we replace the Word rules in the syllableadaptor grammar with the following:Word?
SyllIFWord?
SyllI SyllFWord?
SyllI Syll SyllFWord?
SyllI Syll Syll SyllF(6)and add the following rules expanding the newkinds of syllables to the rules in (5).SyllIF?
(OnsetI)?
RhyFSyllI?
(OnsetI)?
RhySyllF?
(OnsetI)?
RhyFSyll?
(Onset)?
RhyOnsetI?
C+RhyF?
Nucleus (CodaF)?CodaF?
C+(7)533SyllablesNone General SpecialisedUnigram 0.57 0.50 0.50Colloc 0.69 0.67 0.67Colloc2 0.72 0.75 0.75Colloc3 0.64 0.77 0.77Table 1: F-score accuracies of word segmenta-tions produced by the adaptor grammar models onthe Chinese corpus with tones.SyllablesNone General SpecialisedUnigram 0.56 0.46 0.46Colloc 0.70 0.65 0.65Colloc2 0.74 0.74 0.73Colloc3 0.75 0.76 0.77Table 2: F-score accuracies of word segmenta-tions produced by the adaptor grammar models onthe Chinese corpus without tones.These rules distinguish syllable onsets in word-initial position and syllable codas in word-finalposition; the standard adaptor grammarmachinerywill then learn distributions over onsets and codasin these positions that possibly differ from thosein word-internal positions.4 Results on Chinese word segmentationThe previous section described two dimensionsalong which adaptor grammars for word segmen-tation can independently vary.
Above the Wordlevel, there can be from zero to three levels of col-locations, yielding four different values for this di-mension.
Below theWord level, phonemes can ei-ther be treated as independent entities, or else theycan be grouped into onset, nuclei and coda clus-ters, and these can vary depending on where theyappear within a word.
Thus there are three dif-ferent values for the syllable dimension, so thereare twelve different adaptor grammars overall.
Inaddition, we ran all of these grammars on two ver-sions of the corpus, one with tones and one with-out tones, so we report results for 24 different runshere.The adaptor grammar inference procedure weused is the one described in Johnson and Goldwa-ter (2009).
We ran 1,000 iterations of 8 MCMCchains for each run, and we discarded all but last200 iterations in order to ?burn-in?
the sampler.The segmentation we predict is the one that occursthe most frequently in the samples that were notdiscarded.
As is standard, we evaluate the modelsin terms of token f-score; the results are presentedin Tables 1 and 2.In these tables, ?None?
indicates that the gram-mar does not model syllable structure, ?Gen-eral?
indicates that the grammar does not distin-guish word-peripheral from word-internal clus-ters, while ?Specialised?
indicates that it does.?Unigram?
indicates that the grammar does notmodel collocational structure, otherwise the super-script indicates the number of collocational levelsthat the grammar captures.Broadly speaking, the results are consistent withthe English word segmentation results using adap-tor grammars presented by Johnson (2008b).
Theunigram grammar segmentation accuracy is simi-lar to that obtained for English, but the results forthe other models are lower than the results for thecorresponding adaptor grammars on English.We see a general improvement in segmenta-tion accuracy as the number of collocation levelsincreases, just as for English.
However, we donot see any general improvements associated withmodelling syllables; indeed, it seems modellingsyllables causes accuracy to decrease unless collo-cational structure is also modelled.
This is some-what surprising, as Chinese has a very regular syl-labic structure.
It is not surprising that distin-guishing word-peripheral and word-medial clus-ters does not improve segmentation accuracy, asChinese does not distinguish these kinds of clus-ters.
There is also no sign of the ?synergies?
whenmodelling collocations and syllables together thatJohnson (2008b) reported.It is also surprising that tones seem to make lit-tle difference to the segmentation accuracy, sincethey are crucial for disambiguating lexical items.The segmentation accuracy of the models that cap-ture little or no inter-word dependencies (e.g., Un-igram, Colloc) improved slightly when the inputcontains tones, but the best-performing modelsthat capture a more complex set of inter-word de-534pendencies do equally well on the corpus withouttones as they do on the corpus with tones.
Becausethese models capture rich inter-word context (theymodel three levels of collocational structure), it ispossible that this context provides sufficient infor-mation to segment words even in the absence oftone information, i.e., the tonal information is re-dundant given the richer inter-word dependenciesthat these models capture.
It is also possible thatword segmentation may simply require less infor-mation than lexical disambiguation.One surprising result is the relatively poor per-formance of the Colloc3 model without syllablesbut with tones; we have no explanation for this.However, all 8 of the MCMC chains in this runproduced lower f-scores, so it unlikely to be sim-ply a random fluctuation produced by a single out-lier.Note that one should be cautious when compar-ing the absolute f-scores from these experimentswith those of the English study, as the English andChinese corpora differ in many ways.
As Tardif(1993) (the creator of the Chinese corpus) empha-sises, this corpus was collected in a much morediverse linguistic environment with child-directedspeech from multiple caregivers.
The children in-volved in the Chinese corpus were also older thanthe children in the English corpus, which may alsohave affected the nature of the corpus.5 ConclusionThis paper applied adaptor grammar models ofphonemic word segmentation originally devel-oped for English to Chinese data.
While the Chi-nese data was prepared in a very different wayto the English data, the adaptor grammars usedto perform Chinese word segmentation were verysimilar to those used for the English word seg-mentation.
They also achieved quite respectablef-score accuracies, which suggests that the samemodels can do well on both languages.One puzzling result is that incorporating syl-lable structure phonotactic constraints, which en-hances English word segmentation accuracy con-siderably, doesn?t seem to improve Chinese wordsegmentation to a similar extent.
This may reflectthe fact that the word segmentation adaptor gram-mars were originally designed and tuned for En-glish, and perhaps differently formulated syllable-structure constraints would work well for Chinese.But even if one can ?tune?
the adaptor grammarsto improve performance on Chinese, the challengeis doing this in a way that improves performanceon all languages, rather than just one.AcknowledgmentsThe authors would like to thank the US NSF forsupport for this research, which was begun whilethey were on the faculty at Brown University inthe USA.
The NSF supported this work throughNSF awards 0544127 and 0631667 to Mark John-son and Katherine Demuth.The adaptor grammar software isfreely available for download fromhttp://web.science.mq.edu.au/?mjohnson, andthe Chinese data was obtained from the Childesarchive.ReferencesBernstein-Ratner, N. 1987.
The phonology of parent-child speech.
In Nelson, K. and A. van Kleeck,editors, Children?s Language, volume 6.
Erlbaum,Hillsdale, NJ.Brent, M. and T. Cartwright.
1996.
Distributionalregularity and phonotactic constraints are useful forsegmentation.
Cognition, 61:93?125.Brent, M. 1999.
An efficient, probabilistically soundalgorithm for segmentation and word discovery.Machine Learning, 34:71?105.Elman, Jeffrey.
1990.
Finding structure in time.
Cog-nitive Science, 14:197?211.Fleck, Margaret M. 2008.
Lexicalized phonotac-tic word segmentation.
In Proceedings of ACL-08:HLT, pages 130?138, Columbus, Ohio, June.
Asso-ciation for Computational Linguistics.Goldsmith, John A.
1990.
Autosegmental and Metri-cal Phonology.
Basil Blackwell, Oxford, England.Goldwater, Sharon, Thomas L. Griffiths, and MarkJohnson.
2006.
Contextual dependencies in un-supervised word segmentation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 673?680,Sydney, Australia.
Association for ComputationalLinguistics.535Goldwater, Sharon, Thomas L. Griffiths, and MarkJohnson.
2009.
A Bayesian framework for wordsegmentation: Exploring the effects of context.Cognition, 112(1):21 ?
54.Johnson, Mark and Sharon Goldwater.
2009.
Im-proving nonparameteric Bayesian inference: exper-iments on unsupervised word segmentation withadaptor grammars.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North AmericanChapter of the Association forComputational Linguistics, pages 317?325, Boul-der, Colorado, June.
Association for ComputationalLinguistics.Johnson, Mark, Thomas L. Griffiths, and Sharon Gold-water.
2007.
Adaptor Grammars: A framework forspecifying compositional nonparametric Bayesianmodels.
In Scho?lkopf, B., J. Platt, and T. Hoffman,editors, Advances in Neural Information ProcessingSystems 19, pages 641?648.
MIT Press, Cambridge,MA.Johnson, Mark.
2008a.
Unsupervised word seg-mentation for Sesotho using adaptor grammars.
InProceedings of the Tenth Meeting of ACL SpecialInterest Group on Computational Morphology andPhonology, pages 20?27, Columbus, Ohio, June.Association for Computational Linguistics.Johnson, Mark.
2008b.
Using adaptor grammars toidentifying synergies in the unsupervised acquisitionof linguistic structure.
In Proceedings of the 46thAnnualMeeting of the Association of ComputationalLinguistics, Columbus, Ohio.
Association for Com-putational Linguistics.Joshi, Aravind.
2003.
Tree adjoining grammars.
InMikkov, Ruslan, editor, The Oxford Handbook ofComputational Linguistics, pages 483?501.
OxfordUniversity Press, Oxford, England.MacWhinney, Brian and Catherine Snow.
1985.
Thechild language data exchange system.
Journal ofChild Language, 12:271?296.Mochihashi, Daichi, Takeshi Yamada, and NaonoriUeda.
2009.
Bayesian unsupervised word segmen-tation with nested Pitman-Yor language modeling.In Proceedings of the Joint Conference of the 47thAnnualMeeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP, pages 100?108, Suntec, Singapore,August.
Association for Computational Linguistics.Swingley, Dan.
2005.
Statistical clustering and thecontents of the infant vocabulary.
Cognitive Psy-chology, 50:86?132.Tardif, Twila.
1993.
Adult-to-child speech and lan-guage acquisition in Mandarin Chinese.
Ph.D. the-sis, Yale University.536
