Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 64?72,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsTo Annotate More Accurately or to Annotate MoreDmitriy DligachDepartment of Computer ScienceUniversity of Colorado at BoulderDmitriy.Dligach@colorado.eduRodney D. NielsenThe Center for Computational Languageand Education ResearchUniversity of Colorado at BoulderRodney.Nielsen@colorado.eduMartha PalmerDepartment of LinguisticsDepartment of Computer ScienceUniversity of Colorado at BoulderMartha.Palmer@colorado.eduAbstractThe common accepted wisdom is thatblind double annotation followed by adju-dication of disagreements is necessary tocreate training and test corpora that resultin the best possible performance.
We pro-vide evidence that this is unlikely to be thecase.
Rather, the greatest value for yourannotation dollar lies in single annotatingmore data.1 IntroductionIn recent years, supervised learning has becomethe dominant paradigm in Natural Language Pro-cessing (NLP), thus making the creation of hand-annotated corpora a critically important task.
Acorpus where each instance is annotated by a sin-gle tagger unavoidably contains errors.
To im-prove the quality of the data, an annotation projectmay choose to annotate each instance twice andadjudicate the disagreements, thus producing the(largely) error-free gold standard.
For example,OntoNotes (Hovy et al, 2006), a large-scale an-notation project, chose this option.However, given a virtually unlimited supply ofunlabeled data and limited funding ?
a typical setof constraints in NLP ?
an annotation project mustalways face the realization that for the cost of dou-ble annotation, more than twice as much data canbe single annotated.
The philosophy behind thisalternative says that modern machine learning al-gorithms can still generalize well in the presenceof noise, especially when given larger amounts oftraining data.Currently, the commonly accepted wisdomsides with the view that says that blind doubleannotation followed by adjudication of disagree-ments is necessary to create annotated corpora thatleads to the best possible performance.
We pro-vide empirical evidence that this is unlikely to bethe case.
Rather, the greatest value for your an-notation dollar lies in single annotating more data.There may, however, be other considerations thatstill argue in favor of double annotation.In this paper, we also consider the arguments ofBeigman and Klebanov (2009), who suggest thatdata should be multiply annotated and then filteredto discard all of the examples where the annota-tors do not have perfect agreement.
We provideevidence that single annotating more data for thesame cost is likely to result in better system per-formance.This paper proceeds as follows: first, we out-line our evaluation framework in Section 2.
Next,we compare the single annotation and adjudica-tion scenarios in Section 3.
Then, we comparethe annotation scenario of Beigman and Klebanov(2009) with the single annotation scenario in Sec-tion 4.
After that, we discuss the results and futurework in section 5.
Finally, we draw the conclusionin Section 6.2 Evaluation2.1 DataFor evaluation we utilize the word sense data an-notated by the OntoNotes project.
The OntoNotesdata was chosen because it utilizes full double-blind annotation by human annotators and the dis-agreements are adjudicated by a third (more expe-64rienced) annotator.
This allows us to?
Evaluate single annotation results by usingthe labels assigned by the first tagger?
Evaluate double annotation results by usingthe labels assigned by the second tagger?
Evaluate adjudication results by using the la-bels assigned by the the adjudicator to the in-stances where the two annotators disagreed?
Measure the performance under various sce-narios against the double annotated and adju-dicated gold standard dataWe selected the 215 most frequent verbs in theOntoNotes data.
To make the size of the datasetmore manageable, we randomly selected 500 ex-amples of each of the 15 most frequent verbs.
Forthe remaining 200 verbs, we utilized all the an-notated examples.
The resulting dataset contained66,228 instances of the 215 most frequent verbs.Table 1 shows various important characteristics ofthis dataset averaged across the 215 verbs.Inter-tagger agreement 86%Annotator1-gold standard agreement 93%Share of the most frequent sense 70%Number of classes (senses) per verb 4.74Table 1: Data used in evaluation at a glance2.2 Cost of AnnotationBecause for this set of experiments we care pri-marily about the cost effectiveness of the annota-tion dollars, we need to know how much it coststo blind annotate instances and how much it coststo adjudicate disagreements in instances.
There isan upfront cost associated with any annotation ef-fort to organize the project, design an annotationscheme, set up the environment, create annotationguidelines, hire and train the annotators, etc.
Wewill assume, for the sake of this paper, that thiscost is fixed and is the same regardless of whetherthe data is single annotated or the data is doubleannotated and disagreements adjudicated.In this paper, we focus on a scenario where thereis essentially no difference in cost to collect ad-ditional data to be annotated, as is often the case(e.g., there is virtually no additional cost to down-load 2.5 versus 1.0 million words of text from theweb).
However, this is not always the case (e.g.,collecting speech can be costly).To calculate a cost per annotated instance forblind annotation, we take the total expenses asso-ciated with the annotators in this group less train-ing costs and any costs not directly associated withannotation and divide by the total number of blindinstance annotations.
This value, $0.0833, is theper instance cost used for single annotation.
Wecalculated the cost for adjudicating instances sim-ilarly, based on the expenses associated with theadjudication group.
The adjudication cost is an ad-ditional $0.1000 per instance adjudicated.
The perinstance cost for double blind, adjudicated data isthen computed as double the cost for single an-notation plus the per instance cost of adjudicationmultiplied by the percent of disagreement, 14%,which is $0.1805.We leave an analysis of the extent to which theup front costs are truly fixed and whether they canbe altered to result in more value for the dollar tofuture work.2.3 Automatic Word Sense DisambiguationFor the experiments we conduct in this study, weneeded a word sense disambiguation (WSD) sys-tem.
Our WSD system is modeled after the state-of-the-art verb WSD system described in (Dligachand Palmer, 2008).
We will briefly outline it here.We view WSD as a supervised learning prob-lem.
Each instance of the target verb is representedas a vector of binary features that indicate the pres-ence (or absence) of the corresponding features inthe neighborhood of the target verb.
We utilizeall of the linguistic features that were shown to beuseful for disambiguating verb senses in (Chen etal., 2007).To extract the lexical features we POS-tagthe sentence containing the target verb and thetwo surrounding sentences using MXPost soft-ware (Ratnaparkhi, 1998).
All open class words(nouns, verbs, adjectives, and adverbs) in thesesentences are included in our feature set.
In addi-tion to that, we use as features two words on eachside of the target verb as well as their POS tags.To extract the syntactic features we parse thesentence containing the target verb with Bikel?sconstituency parser and utilize a set of rules toidentify the features in Table 2.Our semantic features represent the semanticclasses of the target verb?s syntactic arguments65Feature ExplanationSubject and object - Presence of subject andobject- Head word of subjectand object NPs- POS tag of the headword of subject andobject NPsVoice - Passive or ActivePP adjunct - Presence of PP adjunct- Preposition word- Head word of thepreposition?s NPargumentSubordinate clause - Presence of subordinateclausePath - Parse tree path fromtarget verb to neighboringwords- Parse tree path fromtarget verb to subject andobject- Parse tree path fromtarget verb to subordinateclauseSubcat frame - Phrase structure ruleexpanding the targetverb?s parent node inparse treeTable 2: Syntactic featuressuch as subject and object.
The semantic classesare approximated as?
WordNet (Fellbaum, 1998) hypernyms?
NE tags derived from the output of Identi-Finder (Bikel et al, 1999)?
Dynamic dependency neighbors (Dligachand Palmer, 2008), which are extracted in anunsupervised way from a dependency-parsedcorpusOur WSD system uses the Libsvm softwarepackage (Chang and Lin, 2001) for classification.We accepted the default options (C = 1 and lin-ear kernel) when training our classifiers.
As is thecase with most WSD systems, we train a separatemodel per verb.3 Experiment OneThe results of experiment one show that in thesecircumstances, better performance is achieved bysingle annotating more data than by deploing re-sources towards ensuring that the data is annotatedmore accurately through an adjudication process.3.1 Experimental DesignWe conduct a number of experiments to comparethe effect of single annotated versus adjudicateddata on the accuracy of a state of the art WSD sys-tem.
Since OntoNotes does not have a specifiedtest set, for each word, we used repeated randompartitioning of the data with 10 trials and 10% intothe test set and the remaining 90% comprising thetraining set.We then train an SVM classifier on varying frac-tions of the data, based on the number of examplesthat could be annotated per dollar.
Specifically,in increments of $1.00, we calculate the numberof examples that can be single annotated and thenumber that can be double blind annotated and ad-judicated with that amount of money.The number of examples computed for singleannotation is selected at random from the train-ing data.
Then the adjudicated examples are se-lected at random from this subset.
Selecting fromthe same subset of data approaches pair statisti-cal testing and results in a more accurate statisticalcomparison of the models produced.Classifiers are trained on this data using the la-bels from the first round of annotation as the singleannotation labels and the final adjudicated labelsfor the smaller subset.
This procedure is repeatedten times and the average results are reported.For a given verb, each classifier createdthroughout this process is tested on the same dou-ble annotated and adjudicated held-out test set.3.2 ResultsFigure 1 shows a plot of the accuracy of the clas-sifiers relative to the annotation investment for atypical verb, to call.
As can be seen, the accu-racy is always higher when training on the largeramount of single annotated data than when train-ing on the amount of adjudicated data that had theequivalent cost of annotation.Figures 2 and 3 present results averaged overall 215 verbs in the dataset.
First, figure 2 showsthe average accuracy over all verbs by amount in-vested.
These accuracy curves are not smooth be-66Figure 1: Performance of single annotated vs. ad-judicated data by amount invested for to callcause the verbs all have a different number of totalinstances.
At various annotation cost values, all ofthe instances of one or more verbs will have beenannotated.
Hence, the accuracy values might jumpor drop by a larger amount than seen elsewhere inthe graph.Toward the higher dollar amounts the curve isdominated by fewer and fewer verbs.
We onlydisplay the dollar investments of up to $60 due tothe fact that only five verbs have more than $60?sworth of instances in the training set.Figure 2: Average performance of single anno-tated vs. adjudicated data by amount investedThe average difference in accuracy for Figure 2across all amounts of investment is 1.64%.Figure 3 presents the average accuracy relativeto the percent of the total cost to single annotateall of the instances for a verb.
The accuracy at agiven percent of total investment was interpolatedfor each verb using linear interpolation and thenaveraged over all of the verbs.Figure 3: Average performance of single anno-tated vs. adjudicated data by fraction of total in-vestmentThe average difference in accuracy for Figure 3across each percent of investment is 2.10%.Figure 4 presents essentially the same informa-tion as Figure 2, but as a reduction in error rate forsingle annotation relative to full adjudication.Figure 4: Reduction in error rate from adjudica-tion to single annotation scenario based on resultsin Figure 2The relative reduction in error rate averagedover all investment amounts in Figure 2 is 7.77%.Figure 5 presents the information in Figure 3as a reduction in error rate for single annotationrelative to full adjudication.The average relative reduction in error rate overthe fractions of total investment in Figure 5 is9.32%.67Figure 5: Reduction in error rate from adjudica-tion to single annotation scenario based on resultsin Figure 33.3 DiscussionFirst, it is worth noting that, when the amount ofannotated data is the same for both scenarios, ad-judicated data leads to slightly better performancethan single annotated data.
For example, considerFigure 3.
The accuracy at 100% of the total invest-ment for the double annotation and adjudicationscenario is 81.13%.
The same number of exam-ples can be single annotated for 0.0833 / 0.1805 =0.4615 of this dollar investment (using the costsfrom Section 2.2).
The system trained on thatamount of single annotated data shows a lower ac-curacy, 80.21%.
Thus, in this case, the adjudica-tion scenario brings about a performance improve-ment of about 1%.However, the main thesis of this paper is that in-stead of double annotating and adjudicating, it isoften better to single annotate more data becauseit is a more cost-effective way to achieve a higherperformance.
The results of our experiments sup-port this thesis.
At every dollar amount invested,our supervised WSD system performs better whentrained on single annotated data comparing to dou-ble annotated and adjudicated data.The maximum annotation investment amountfor each verb is the cost of single annotating allof its instances.
When the system is trained onthe amount of double annotated data possible atthis investment, its accuracy is 81.13% (Figure 3).When trained on single annotated data, the systemattains the same accuracy much earlier, at approxi-mately 60% of the total investment.
When trainedon the entire available single annotated data, thesystem reaches an accuracy of 82.99%, nearly a10% relative reduction in error rate over the samesystem trained on the adjudicated data obtained forthe same cost.Averaged over the 215 verbs, the single anno-tation scenario outperformed adjudication at everydollar amount investigated.4 Experiment TwoIn this experiment, we consider the arguments ofBeigman and Klebanov (2009).
They suggest thatdata should be at least double annotated and thenfiltered to discard all of the examples where therewere any annotator disagreements.The main points of their argument are as fol-lows.
They first consider the data to be dividableinto two types, easy (to annotate) cases and hardcases.
Then they correctly note that some anno-tators could have a systematic bias (i.e., could fa-vor one label over others in certain types of hardcases), which would in turn bias the learning ofthe classifier.
They show that it is theoreticallypossible that a band of misclassified hard casesrunning parallel to the true separating hyperplanecould mistakenly shift the decision boundary pastup to?N easy cases.We suggest that it is extremely unlikely that aconsequential number of easy cases would existnearer to the class boundary than the hard cases.The hard cases are in fact generally considered todefine the separating hyperplane.In this experiment, our goal is to determine howthe accuracy of classifiers trained on data labeledaccording to Beigman and Klebanov?s discard dis-agreements strategy compares empirically to theaccuracy resulting from single annotated data.
Asin the previous experiment, this analysis is per-formed relative to the investment in the annotationeffort.4.1 Experimental DesignWe follow essentially the same experimental de-sign described in section 3.1, using the same stateof the art verb WSD system.
We conduct a num-ber of experiments to compare the effect of singleannotated versus double annotated data.
We uti-lized the same training and test sets as the previousexperiment and similarly trained an SVM on frac-tions of the data representing increments of $1.00investments.As before, the number of examples designated68for single annotation is selected at random fromthe training data and half of that subset is selectedas the training set for the double annotated data.Again, selecting from the same subset of data re-sults in a more accurate statistical comparison ofthe models produced.Classifiers for each annotation scenario aretrained on the labels from the first round of an-notation, but examples where the second annota-tor disagreed are thrown out of the double anno-tated data.
This results in slightly less than half asmuch data in the double annotation scenario basedon the disagreement rate.
Again, the procedure isrepeated ten times and the average results are re-ported.For a given verb, each classifier createdthroughout this process is tested on the same dou-ble annotated and adjudicated held-out test set.4.2 ResultsFigure 6 shows a plot of the accuracy of the classi-fiers relative to the annotation investment for a typ-ical verb, to call.
As can be seen, the accuracy fora specific investment performing single annotationis always higher than it is for the same investmentin double annotated data.Figure 6: Performance of single annotated vs.double annotated data with disagreements dis-carded by amount invested for to callFigures 7 and 8 present results averaged overall 215 verbs in the dataset.
First, figure 7 showsthe average accuracy over all verbs by amountinvested.
Again, these accuracy curves are notsmooth because the verbs all have a different num-ber of total instances.
Hence, the accuracy val-ues might jump or drop by a larger amount at thepoints where a given verb is no longer included inthe average.Toward the higher dollar amounts the curve isdominated by fewer and fewer verbs.
As before,we only display the results for investments of upto $60.The average difference in accuracy for Figure 7across all amounts of investment is 2.32%.Figure 8 presents the average accuracy relativeto the percent of the total cost to single annotateall of the instances for a verb.
The accuracy ata given percent of total investment was interpo-lated for each verb and then averaged over all ofthe verbs.Figure 7: Average performance of single anno-tated vs. double annotated data with disagree-ments discarded by amount investedFigure 8: Average performance of single anno-tated vs. adjudicated data by fraction of total in-vestmentThe average difference in accuracy for Figure 8across all amounts of investment is 2.51%.69Figures 9 and 10 present this information as areduction in error rate for single annotation rela-tive to full adjudication.Figure 9: Reduction in error rate from adjudica-tion to single annotation scenario based on resultsin Figure 7The relative reduction in error rate averagedover all investment amounts in Figure 9 is 10.88%.Figure 10: Reduction in error rate from adjudica-tion to single annotation scenario based on resultsin Figure 8The average relative reduction in error rate overthe fractions of total investment in Figure 10 is10.97%.4.3 DiscussionAt every amount of investment, our supervisedWSD system performs better when trained on sin-gle annotated data comparing to double annotateddata with discarded cases of disagreements.The maximum annotation investment amountfor each verb is the cost of single annotating allof its instances.
When the system is trained onthe amount of double annotated data possible atthis investment, its accuracy is 80.78% (Figure 8).When trained on single annotated data, the systemreaches the same accuracy much earlier, at approx-imately 52% of the total investment.
When trainedon the entire available single annotated data, thesystem attains an accuracy of 82.99%, an 11.5%relative reduction in error rate compared to thesame system trained on the double annotated dataobtained for the same cost.The average accuracy of the single annotationscenario outperforms the double annotated withdisagreements discarded scenario at every dollaramount investigated.While this empirical investigation only lookedat verb WSD, it was performed using 215 distinctverb type datasets.
These verbs each have con-textual features that are essentially unique to thatverb type and consequently, 215 distinct classi-fiers, one per verb type, are trained.
Hence, thesecould loosely be considered 215 distinct annota-tion and classification tasks.The fact that for the 215 classification tasks thesingle annotation scenario on average performedbetter than the discard disagreements scenario ofBeigman and Klebanov (2009) strongly suggeststhat, while it is theoretically possible for annota-tion bias to, in turn, bias a classifier?s learning, itis more likely that you will achieve better resultsby training on the single annotated data.It is still an open issue whether it is generallybest to adjudicate disagreements in the test set orto throw them out as suggested by (Beigman Kle-banov and Beigman, 2009).5 Discussion and Future WorkWe investigated 215 WSD classification tasks,comparing performance under three annotationscenarios each with the equivalent annotation cost,single annotation, double annotation with dis-agreements adjudicated, and double annotationwith disagreements discarded.
Averaging over the215 classification tasks, the system trained on sin-gle annotated data achieved 10.0% and 11.5% rel-ative reduction in error rates compared to trainingon the equivalent investment in adjudicated anddisagreements discarded data, respectively.
Whilewe believe these results will generalize to other an-notation tasks, this is still an open question to bedetermined by future work.70There are probably similar issues in what wereconsidered fixed costs for the purposes of this pa-per.
For example, it may be possible to train fewerannotators, and invest the savings into annotatingmore data.
Perhaps more appropriately, it may befeasible to simply cut back on the amount of train-ing provided per annotator and instead annotatemore data.On the other hand, when the unlabeled datais not freely obtainable, double annotation maybe more suitable as a route to improving systemperformance.
There may also be factors otherthan cost-effectiveness which make double anno-tation desirable.
Many projects point to their ITArates and corresponding kappa values as a mea-sure of annotation quality, and of the reliability ofthe annotators (Artstein and Poesio, 2008).
TheOntoNotes project used ITA rates as a way of eval-uating the clarity of the sense inventory that wasbeing developed in parallel with the annotation.Lexical entries that resulted in low ITA rates wererevised, usually improving the ITA rate.
Calculat-ing these rates requires double-blind annotation.Annotators who consistently produced ITA rateslower than average were also removed from theproject.
Therefore, caution is advised in determin-ing when to dispense with double annotation in fa-vor of more cost effective single annotation.Double annotation can also be used to shed lighton other research questions that, for example, re-quire knowing which instances are ?hard.?
Thatknowledge may help with designing additional,richer annotation layers or with cognitive scienceinvestigations into human representations of lan-guage.Our results suggest that systems would likelybenefit more from the larger training datasets thatsingle annotation makes possible than from theless noisy datasets resulting from adjudication.Regardless of whether single or double annota-tion with adjudication is used, there will always benoise.
Hence, we see the further investigation ofalgorithms that generalize despite the presence ofnoise to be critical to the future of computationallinguistics.
Humans are able to learn in the pres-ence of noise, and our systems must follow suit.6 ConclusionDouble annotated data contains less noise thansingle annotated data and thus improves the per-formance of supervised machine learning systemsthat are trained on a specific amount of data.
How-ever, double annotation is expensive and the alter-native of single annotating more data instead is onthe table for many annotation projects.In this paper we compared the performance ofa supervised machine learning system trained ondouble annotated data versus single annotated dataobtainable for the same cost.
Our results clearlydemonstrate that single annotating more data canbe a more cost-effective way to improve the sys-tem performance in the many cases where the un-labeled data is freely available and there are noother considerations that necessitate double anno-tation.7 AcknowledgementsWe gratefully acknowledge the support of the Na-tional Science Foundation Grant NSF-0715078,Consistent Criteria for Word Sense Disambigua-tion, and the GALE program of the Defense Ad-vanced Research Projects Agency, Contract No.HR0011-06-C-0022, a subcontract from the BBN-AGILE Team.
Any opinions, findings, and conclu-sions or recommendations expressed in this mate-rial are those of the authors and do not necessarilyreflect the views of the National Science Founda-tion.ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Comput.Linguist., 34(4):555?596.Eyal Beigman and Beata Beigman Klebanov.
2009.Learning with annotation noise.
In ACL-IJCNLP?09: Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP: Volume 1, pages 280?287, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Beata Beigman Klebanov and Eyal Beigman.
2009.From annotator agreement to noise models.
Com-put.
Linguist., 35(4):495?503.Daniel M. Bikel, Richard Schwartz, and Ralph M.Weischedel.
1999.
An algorithm that learns what?sin a name.
Mach.
Learn., 34(1-3):211?231.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: a library for support vector machines.Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.71J.
Chen and M. Palmer.
2005.
Towards robust highperformance word sense disambiguation of englishverbs using rich linguistic features.
pages 933?944.Springer.Jinying Chen, Dmitriy Dligach, and Martha Palmer.2007.
Towards large-scale high-performance en-glish verb sense disambiguation by using linguisti-cally motivated features.
In ICSC ?07: Proceed-ings of the International Conference on SemanticComputing, pages 378?388, Washington, DC, USA.IEEE Computer Society.Dmitriy Dligach and Martha Palmer.
2008.
Novel se-mantic features for verb sense disambiguation.
InHLT ?08: Proceedings of the 46th Annual Meetingof the Association for Computational Linguistics onHuman Language Technologies, pages 29?32, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.Christiane Fellbaum.
1998.
WordNet: An electroniclexical database.
MIT press Cambridge, MA.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:the 90% solution.
In NAACL ?06: Proceedings ofthe Human Language Technology Conference of theNAACL, Companion Volume: Short Papers on XX,pages 57?60, Morristown, NJ, USA.
Association forComputational Linguistics.A.
Ratnaparkhi.
1998.
Maximum entropy models fornatural language ambiguity resolution.
Ph.D. the-sis, University of Pennsylvania.72
