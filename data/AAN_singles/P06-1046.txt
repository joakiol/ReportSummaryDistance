Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 361?368,Sydney, July 2006. c?2006 Association for Computational LinguisticsScaling Distributional Similarity to Large CorporaJames Gorman and James R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australia{jgorman2,james}@it.usyd.edu.auAbstractAccurately representing synonymy usingdistributional similarity requires large vol-umes of data to reliably represent infre-quent words.
However, the na?
?ve nearest-neighbour approach to comparing contextvectors extracted from large corpora scalespoorly (O(n2) in the vocabulary size).In this paper, we compare several existingapproaches to approximating the nearest-neighbour search for distributional simi-larity.
We investigate the trade-off be-tween efficiency and accuracy, and findthat SASH (Houle and Sakuma, 2005) pro-vides the best balance.1 IntroductionIt is a general property of Machine Learning thatincreasing the volume of training data increasesthe accuracy of results.
This is no more evidentthan in Natural Language Processing (NLP), wheremassive quantities of text are required to modelrare language events.
Despite the rapid increase incomputational power available for NLP systems,the volume of raw data available still outweighsour ability to process it.
Unsupervised learning,which does not require the expensive and time-consuming human annotation of data, offers anopportunity to use this wealth of data.
Curranand Moens (2002) show that synonymy extractionfor lexical semantic resources using distributionalsimilarity produces continuing gains in accuracyas the volume of input data increases.Extracting synonymy relations using distribu-tional similarity is based on the distributional hy-pothesis that similar words appear in similar con-texts.
Terms are described by collating informa-tion about their occurrence in a corpus into vec-tors.
These context vectors are then compared forsimilarity.
Existing approaches differ primarily intheir definition of ?context?, e.g.
the surroundingwords or the entire document, and their choice ofdistance metric for calculating similarity betweenthe context vectors representing each term.Manual creation of lexical semantic resourcesis open to the problems of bias, inconsistency andlimited coverage.
It is difficult to account for theneeds of the many domains in which NLP tech-niques are now being applied and for the rapidchange in language use.
The assisted or auto-matic creation and maintenance of these resourceswould be of great advantage.Finding synonyms using distributional similar-ity requires a nearest-neighbour search over thecontext vectors of each term.
This is computation-ally intensive, scaling to O(n2m) for the numberof terms n and the size of their context vectors m.Increasing the volume of input data will increasethe size of both n and m, decreasing the efficiencyof a na?
?ve nearest-neighbour approach.Many approaches to reduce this complexityhave been suggested.
In this paper we evaluatestate-of-the-art techniques proposed to solve thisproblem.
We find that the Spatial ApproximationSample Hierarchy (Houle and Sakuma, 2005) pro-vides the best accuracy/efficiency trade-off.2 Distributional SimilarityMeasuring distributional similarity first requiresthe extraction of context information for each ofthe vocabulary terms from raw text.
These termsare then compared for similarity using a nearest-neighbour search or clustering based on distancecalculations between the statistical descriptions oftheir contexts.3612.1 ExtractionA context relation is defined as a tuple (w, r, w?
)where w is a term, which occurs in some grammat-ical relation r with another word w?
in some sen-tence.
We refer to the tuple (r, w?)
as an attributeof w. For example, (dog, direct-obj, walk) indicatesthat dog was the direct object of walk in a sentence.In our experiments context extraction beginswith a Maximum Entropy POS tagger and chun-ker.
The SEXTANT relation extractor (Grefen-stette, 1994) produces context relations that arethen lemmatised.
The relations for each term arecollected together and counted, producing a vectorof attributes and their frequencies in the corpus.2.2 Measures and WeightsBoth nearest-neighbour and cluster analysis meth-ods require a distance measure to calculate thesimilarity between context vectors.
Curran (2004)decomposes this into measure and weight func-tions.
The measure calculates the similaritybetween two weighted context vectors and theweight calculates the informativeness of each con-text relation from the raw frequencies.For these experiments we use the Jaccard (1)measure and the TTest (2) weight functions, foundby Curran (2004) to have the best performance.?(r,w?)
min(w(wm, r, w?
), w(wn, r, w?))?(r,w?)
max(w(wm, r, w?
), w(wn, r, w?
))(1)p(w, r, w?)?
p(?, r, w?
)p(w, ?, ?
)?p(?, r, w?
)p(w, ?, ?
)(2)2.3 Nearest-neighbour SearchThe simplest algorithm for finding synonyms isa k-nearest-neighbour (k-NN) search, which in-volves pair-wise vector comparison of the targetterm with every term in the vocabulary.
Given ann term vocabulary and up to m attributes for eachterm, the asymptotic time complexity of nearest-neighbour search is O(n2m).
This is very expen-sive, with even a moderate vocabulary making theuse of huge datasets infeasible.
Our largest exper-iments used a vocabulary of over 184,000 words.3 Dimensionality ReductionUsing a cut-off to remove low frequency termscan significantly reduce the value of n. Unfortu-nately, reducing m by eliminating low frequencycontexts has a significant impact on the quality ofthe results.
There are many techniques to reducedimensionality while avoiding this problem.
Thesimplest methods use feature selection techniques,such as information gain, to remove the attributesthat are less informative.
Other techniques smooththe data while reducing dimensionality.Latent Semantic Analysis (LSA, Landauer andDumais, 1997) is a smoothing and dimensional-ity reduction technique based on the intuition thatthe true dimensionality of data is latent in the sur-face dimensionality.
Landauer and Dumais admitthat, from a pragmatic perspective, the same effectas LSA can be generated by using large volumesof data with very long attribute vectors.
Experi-ments with LSA typically use attribute vectors of adimensionality of around 1000.
Our experimentshave a dimensionality of 500,000 to 1,500,000.Decompositions on data this size are computation-ally difficult.
Dimensionality reduction is oftenused before using LSA to improve its scalability.3.1 HeuristicsAnother technique is to use an initial heuristiccomparison to reduce the number of full O(m)vector comparisons that are performed.
If theheuristic comparison is sufficiently fast and a suffi-cient number of full comparisons are avoided, thecost of an additional check will be easily absorbedby the savings made.Curran and Moens (2002) introduces a vector ofcanonical attributes (of bounded length k  m),selected from the full vector, to represent the term.These attributes are the most strongly weightedverb attributes, chosen because they constrain thesemantics of the term more and partake in feweridiomatic collocations.
If a pair of terms share atleast one canonical attribute then a full similaritycomparison is performed, otherwise the terms arenot compared.
They show an 89% reduction insearch time, with only a 3.9% loss in accuracy.There is a significant improvement in the com-putational complexity.
If a maximum of p posi-tive results are returned, our complexity becomesO(n2k + npm).
When p  n, the system willbe faster as many fewer full comparisons will bemade, but at the cost of accuracy as more possiblynear results will be discarded out of hand.4 Randomised TechniquesConventional dimensionality reduction techniquescan be computationally expensive: a more scal-362able solution is required to handle the volumes ofdata we propose to use.
Randomised techniquesprovide a possible solution to this.We present two techniques that have been usedrecently for distributional similarity: Random In-dexing (Kanerva et al, 2000) and Locality Sensi-tive Hashing (LSH, Broder, 1997).4.1 Random IndexingRandom Indexing (RI) is a hashing techniquebased on Sparse Distributed Memory (Kanerva,1993).
Karlgren and Sahlgren (2001) showed RIproduces results similar to LSA using the Test ofEnglish as a Foreign Language (TOEFL) evalua-tion.
Sahlgren and Karlgren (2005) showed thetechnique to be successful in generating bilinguallexicons from parallel corpora.In RI, we first allocate a d length index vec-tor to each unique attribute.
The vectors con-sist of a large number of 0s and small number(?)
number of randomly distributed ?1s.
Contextvectors, identifying terms, are generated by sum-ming the index vectors of the attributes for eachnon-unique context in which a term appears.
Thecontext vector for a term t appearing in contextsc1 = [1, 0, 0,?1] and c2 = [0, 1, 0,?1] would be[1, 1, 0,?2].
The distance between these contextvectors is then measured using the cosine measure:cos(?
(u, v)) = ~u ?
~v|~u| |~v| (3)This technique allows for incremental sampling,where the index vector for an attribute is only gen-erated when the attribute is encountered.
Con-struction complexity is O(nmd) and search com-plexity is O(n2d).4.2 Locality Sensitive HashingLSH is a probabilistic technique that allows theapproximation of a similarity function.
Broder(1997) proposed an approximation of the Jaccardsimilarity function using min-wise independentfunctions.
Charikar (2002) proposed an approx-imation of the cosine measure using random hy-perplanes Ravichandran et al (2005) used this co-sine variant and showed it to produce over 70%accuracy in extracting synonyms when comparedagainst Pantel and Lin (2002).Given we have n terms in an m?
dimensionalspace, we create d  m?
unit random vectors alsoof m?
dimensions, labelled {~r1, ~r2, ..., ~rd}.
Eachvector is created by sampling a Gaussian functionm?
times, with a mean of 0 and a variance of 1.For each term w we construct its bit signatureusing the functionh~r(~w) ={1 : ~r.
~w ?
00 : ~r.
~w < 0where ~r is a spherically symmetric random vectorof length d. The signature, w?, is the d length bitvector:w?
= {h~r1(~w), h ~r2(~w), .
.
.
, h ~rd(~w)}The cost to build all n signatures is O(nm?d).For terms u and v, Goemans and Williamson(1995) approximate the angular similarity byp(h~r(~u) = h~r(~v)) = 1??
(~u, ~u)pi (4)where ?
(~u, ~u) is the angle between ~u and ~u.
Theangular similarity gives the cosine bycos(?
(~u, ~u)) =cos((1 ?
p(h~r(~u) = h~r(~v)))pi)(5)The probability can be derived from the Hammingdistance:p(hr(u) = hr(v)) = 1 ?H(u?, v?
)d (6)By combining equations 5 and 6 we get the fol-lowing approximation of the cosine distance:cos(?
(~u, ~u)) = cos((H(u?, v?
)d)pi)(7)That is, the cosine of two context vectors is ap-proximated by the cosine of the Hamming distancebetween their two signatures normalised by thesize of the signatures.
Search is performed usingEquation 7 and scales to O(n2d).5 Data StructuresThe methods presented above fail to address then2 component of the search complexity.
Manydata structures have been proposed that can beused to address this problem in similarity search-ing.
We present three data structures: the vantagepoint tree (VPT, Yianilos, 1993), which indexespoints in a metric space, Point Location in Equal363Balls (PLEB, Indyk and Motwani, 1998), a proba-bilistic structure that uses the bit signatures gener-ated by LSH, and the Spatial Approximation Sam-ple Hierarchy (SASH, Houle and Sakuma, 2005),which approximates a k-NN search.Another option inspired by IR is attribute index-ing (INDEX).
In this technique, in addition to eachterm having a reference to its attributes, each at-tribute has a reference to the terms referencing it.Each term is then only compared with the termswith which it shares attributes.
We will give a the-oretically comparison against other techniques.5.1 Vantage Point TreeMetric space data structures provide a solution tonear-neighbour searches in very high dimensions.These rely solely on the existence of a compari-son function that satisfies the conditions of metri-cality: non-negativity, equality, symmetry and thetriangle inequality.VPT is typical of these structures and has beenused successfully in many applications.
The VPTis a binary tree designed for range searches.
Theseare searches limited to some distance from the tar-get term but can be modified for k-NN search.VPT is constructed recursively.
Beginning witha set of U terms, we take any term to be our van-tage point p. This becomes our root.
We now findthe median distance mp of all other terms to p:mp = median{dist(p, u)|u ?
U}.
Those termsu such that dist(p, u) ?
mp are inserted into theleft sub-tree, and the remainder into the right sub-tree.
Each sub-tree is then constructed as a newVPT, choosing a new vantage point from within itsterms, until all terms are exhausted.Searching a VPT is also recursive.
Given a termq and radius r, we begin by measuring the distanceto the root term p. If dist(q, p) ?
r we enter p intoour list of near terms.
If dist(q, p) ?
r ?
mp weenter the left sub-tree and if dist(q, p) + r > mpwe enter the right sub-tree.
Both sub-trees may beentered.
The process is repeated for each enteredsubtree, taking the vantage point of the sub-tree tobe the new root term.To perform a k-NN search we use a back-tracking decreasing radius search (Burkhard andKeller, 1973).
The search begins with r = ?,and terms are added to a list of the closest k terms.When the kth closest term is found, the radius isset to the distance between this term and the tar-get.
Each time a new, closer element is added tothe list, the radius is updated to the distance fromthe target to the new kth closest term.Construction complexity is O(n log n).
Searchcomplexity is claimed to be O(log n) for small ra-dius searches.
This does not hold for our decreas-ing radius search, whose worst case complexity isO(n).5.2 Point Location in Equal BallsPLEB is a randomised structure that uses the bitsignatures generated by LSH.
It was used byRavichandran et al (2005) to improve the effi-ciency of distributional similarity calculations.Having generated our d length bit signatures foreach of our n terms, we take these signatures andrandomly permute the bits.
Each vector has thesame permutation applied.
This is equivalent to acolumn reordering in a matrix where the rows arethe terms and the columns the bits.
After applyingthe permutation, the list of terms is sorted lexico-graphically based on the bit signatures.
The list isscanned sequentially, and each term is comparedto its B nearest neighbours in the list.
The choiceof B will effect the accuracy/efficiency trade-off,and need not be related to the choice of k. This isperformed q times, using a different random per-mutation function each time.
After each iteration,the current closest k terms are stored.For a fixed d, the complexity for the permuta-tion step is O(qn), the sorting O(qn log n) and thesearch O(qBn).5.3 Spatial Approximation Sample HierarchySASH approximates a k-NN search by precomput-ing some near neighbours for each node (terms inour case).
This produces multiple paths betweenterms, allowing SASH to shape itself to the dataset (Houle, 2003).
The following description isadapted from Houle and Sakuma (2005).The SASH is a directed, edge-weighted graphwith the following properties (see Figure 1):?
Each term corresponds to a unique node.?
The nodes are arranged into a hierarchy oflevels, with the bottom level containing n2nodes and the top containing a single rootnode.
Each level, except the top, will containhalf as many nodes as the level below.?
Edges between nodes are linked to consecu-tive levels.
Each node will have at most pparent nodes in the level above, and c childnodes in the level below.364AB C DE F G HI JK L12345Figure 1: A SASH, where p = 2, c = 3 and k = 2?
Every node must have at least one parent sothat all nodes are reachable from the root.Construction begins with the nodes being ran-domly distributed between the levels.
SASH isthen constructed iteratively by each node findingits closest p parents in the level above.
The par-ent will keep the closest c of these children, form-ing edges in the graph, and reject the rest.
Anynodes without parents after being rejected are thenassigned as children of the nearest node in the pre-vious level with fewer than c children.Searching is performed by finding the k nearestnodes at each level, which are added to a set ofnear nodes.
To limit the search, only those nodeswhose parents were found to be nearest at the pre-vious level are searched.
The k closest nodes fromthe set of near nodes are then returned.
The searchcomplexity is O(ck log n).In Figure 1, the filled nodes demonstrate asearch for the near-neighbours of some node q, us-ing k = 2.
Our search begins with the root nodeA.
As we are using k = 2, we must find the twonearest children of A using our similarity measure.In this case, C and D are closer than B.
We nowfind the closest two children of C and D. E is notchecked as it is only a child of B.
All other nodesare checked, including F and G, which are sharedas children by B and C .
From this level we choseG and H .
The final levels are considered similarly.At this point we now have the list of near nodesA, C , D, G, H , I , J , K and L. From this wechose the two nodes nearest q, H and I marked inblack, which are then returned.k can be varied at each level to force a largernumber of elements to be tested at the base of theSASH using, for instance, the equation:ki = max{ k1?h?ilog n , 12pc } (8)This changes our search complexity to:k1+1log nk1log n?1+ pc22 log n (9)We use this geometric function in our experiments.Gorman and Curran (2005a; 2005b) found theperformance of SASH for distributional similaritycould be improved by replacing the initial randomordering with a frequency based ordering.
In ac-cordance with Zipf?s law, the majority of termshave low frequencies.
Comparisons made withthese low frequency terms are unreliable (Curranand Moens, 2002).
Creating SASH with high fre-quency terms near the root produces more reliableinitial paths, but comparisons against these termsare more expensive.The best accuracy/efficiency trade-off wasfound when using more reliable initial paths ratherthan the most reliable.
This is done by folding thedata around some mean number of relations.
Foreach term, if its number of relations mi is greaterthan some chosen number of relations M, it isgiven a new ranking based on the score M2mi .
Oth-erwise its ranking based on its number of relations.This has the effect of pushing very high and verylow frequency terms away from the root.6 Evaluation MeasuresThe simplest method for evaluation is the directcomparison of extracted synonyms with a manu-ally created gold standard (Grefenstette, 1994).
Toreduce the problem of limited coverage, our evalu-ation combines three electronic thesauri: the Mac-quarie, Roget?s and Moby thesauri.We follow Curran (2004) and use two perfor-mance measures: direct matches (DIRECT) andinverse rank (INVR).
DIRECT is the percentageof returned synonyms found in the gold standard.INVR is the sum of the inverse rank of each match-ing synonym, e.g.
matches at ranks 3, 5 and 28365CORPUS CUT-OFF TERMS AVERAGERELATIONSPER TERMBNC 0 246,067 435 88,926 116100 14,862 617LARGE 0 541,722 975 184,494 281100 35,618 1,400Table 1: Extracted Context Informationgive an inverse rank score of 13 + 15 + 128 .
Withat most 100 matching synonyms, the maximumINVR is 5.187.
This more fine grained as it in-corporates the both the number of matches andtheir ranking.
The same 300 single word nounswere used for evaluation as used by Curran (2004)for his large scale evaluation.
These were chosenrandomly from WordNet such that they covereda range over the following properties: frequency,number of senses, specificity and concreteness.For each of these terms, the closest 100 terms andtheir similarity scores were extracted.7 ExperimentsWe use two corpora in our experiments: thesmaller is the non-speech portion of the BritishNational Corpus (BNC), 90 million words coveringa wide range of domains and formats; the largerconsists of the BNC, the Reuters Corpus Volume 1and most of the English news holdings of the LDCin 2003, representing over 2 billion words of text(LARGE, Curran, 2004).The semantic similarity system implemented byCurran (2004) provides our baseline.
This per-forms a brute-force k-NN search (NAIVE).
Wepresent results for the canonical attribute heuristic(HEURISTIC), RI, LSH, PLEB, VPT and SASH.We take the optimal canonical attribute vectorlength of 30 for HEURISTIC from Curran (2004).For SASH we take optimal values of p = 4 and c =16 and use the folded ordering taking M = 1000from Gorman and Curran (2005b).For RI, LSH and PLEB we found optimal valuesexperimentally using the BNC.
For LSH we chosed = 3, 000 (LSH3,000) and 10, 000 (LSH10,000),showing the effect of changing the dimensionality.The frequency statistics were weighted using mu-tual information, as in Ravichandran et al (2005):log( p(w, r, w?
)p(w, ?, ?
)p(?, r, w?))
(10)PLEB used the values q = 500 and B = 100.CUT-OFF5 100NAIVE 1.72 1.71HEURISTIC 1.65 1.66RI 0.80 0.93LSH10,000 1.26 1.31SASH 1.73 1.71Table 2: INVR vs frequency cut-offThe initial experiments on RI produced quitepoor results.
The intuition was that this wascaused by the lack of smoothing in the algo-rithm.
Experiments were performed using theweights given in Curran (2004).
Of these, mu-tual information (10), evaluated with an extralog2(f(w, r, w?)
+ 1) factor and limited to posi-tive values, produced the best results (RIMI).
Thevalues d = 1000 and ?
= 5 were found to producethe best results.All experiments were performed on 3.2GHzXeon P4 machines with 4GB of RAM.8 ResultsAs the accuracy of comparisons between terms in-creases with frequency (Curran, 2004), applying afrequency cut-off will both reduce the size of thevocabulary (n) and increase the average accuracyof comparisons.
Table 1 shows the reduction invocabulary and increase in average context rela-tions per term as cut-off increases.
For LARGE,the initial 541,722 word vocabulary is reduced by66% when a cut-off of 5 is applied and by 86%when the cut-off is increased to 100.
The averagenumber of relations increases from 97 to 1400.The work by Curran (2004) largely uses a fre-quency cut-off of 5.
When this cut-off was usedwith the randomised techniques RI and LSH, it pro-duced quite poor results.
When the cut-off wasincreased to 100, as used by Ravichandran et al(2005), the results improved significantly.
Table 2shows the INVR scores for our various techniquesusing the BNC with cut-offs of 5 and 100.Table 3 shows the results of a full thesaurus ex-traction using the BNC and LARGE corpora usinga cut-off of 100.
The average DIRECT score andINVR are from the 300 test words.
The total exe-cution time is extrapolated from the average searchtime of these test words and includes the setuptime.
For LARGE, extraction using NAIVE takes444 hours: over 18 days.
If the 184,494 word vo-cabulary were used, it would take over 7000 hours,or nearly 300 days.
This gives some indication of366BNC LARGEDIRECT INVR Time DIRECT INVR TimeNAIVE 5.23 1.71 38.0hr 5.70 1.93 444.3hrHEURISTIC 4.94 1.66 2.0hr 5.51 1.93 30.2hrRI 2.97 0.93 0.4hr 2.42 0.85 1.9hrRIMI 3.49 1.41 0.4hr 4.58 1.75 1.9hrLSH3,000 2.00 0.76 0.7hr 2.92 1.07 3.6hrLSH10,000 3.68 1.31 2.3hr 3.77 1.40 8.4hrPLEB3,000 2.00 0.76 1.2hr 2.85 1.07 4.1hrPLEB10,000 3.66 1.30 3.9hr 3.63 1.37 11.8hrVPT 5.23 1.71 15.9hr 5.70 1.93 336.1hrSASH 5.17 1.71 2.0hr 5.29 1.89 23.7hrTable 3: Full thesaurus extractionthe scale of the problem.The only technique to become less accuratewhen the corpus size is increased is RI; it is likelythat RI is sensitive to high frequency, low informa-tion contexts that are more prevalent in LARGE.Weighting reduces this effect, improving accuracy.The importance of the choice of d can be seen inthe results for LSH.
While much slower, LSH10,000is also much more accurate than LSH3,000, whilestill being much faster than NAIVE.
Introducingthe PLEB data structure does not improve the ef-ficiency while incurring a small cost on accuracy.We are not using large enough datasets to show theimproved time complexity using PLEB.VPT is only slightly faster slightly faster thanNAIVE.
This is not surprising in light of the origi-nal design of the data structure: decreasing radiussearch does not guarantee search efficiency.A significant influence in the speed of the ran-domised techniques, RI and LSH, is the fixed di-mensionality.
The randomised techniques use afixed length vector which is not influenced by thesize of m. The drawback of this is that the size ofthe vector needs to be tuned to the dataset.It would seem at first glance that HEURIS-TIC and SASH provide very similar results, withHEURISTIC slightly slower, but more accurate.This misses the difference in time complexity be-tween the methods: HEURISTIC is n2 and SASHn log n. The improvement in execution time overNAIVE decreases as corpus size increases and thiswould be expected to continue.
Further tuning ofSASH parameters may improve its accuracy.RIMI produces similar result using LARGE toSASH using BNC.
This does not include the costof extracting context relations from the raw text, sothe true comparison is much worse.
SASH allowsthe free use of weight and measure functions, butRI is constrained by having to transform any con-text space into a RI space.
This is important whenLARGECUT-OFF 0 5 100NAIVE 541,721 184,493 35,617SASH 10,599 8,796 6,231INDEX 5,844 13,187 32,663Table 4: Average number of comparisons per termconsidering that different tasks may require differ-ent weights and measures (Weeds and Weir, 2005).RI also suffers n2 complexity, where as SASH isn log n. Taking these into account, and that the im-provements are barely significant, SASH is a betterchoice.The results for LSH are disappointing.
It per-forms consistently worse than the other methodsexcept VPT.
This could be improved by usinglarger bit vectors, but there is a limit to the size ofthese as they represent a significant memory over-head, particularly as the vocabulary increases.Table 4 presents the theoretical analysis of at-tribute indexing.
The average number of com-parisons made for various cut-offs of LARGE areshown.
NAIVE and INDEX are the actual valuesfor those techniques.
The values for SASH areworst case, where the maximum number of termsare compared at each level.
The actual numberof comparisons made will be much less.
The ef-ficiency of INDEX is sensitive to the density ofattributes and increasing the cut-off increases thedensity.
This is seen in the dramatic drop in per-formance as the cut-off increases.
This problem ofdensity will increase as volume of raw input dataincreases, further reducing its effectiveness.
SASHis only dependent on the number of terms, not thedensity.Where the need for computationally efficiencyout-weighs the need for accuracy, RIMI providesbetter results.
SASH is the most balanced of thetechniques tested and provides the most scalable,high quality results.3679 ConclusionWe have evaluated several state-of-the-art tech-niques for improving the efficiency of distribu-tional similarity measurements.
We found that,in terms of raw efficiency, Random Indexing (RI)was significantly faster than any other technique,but at the cost of accuracy.
Even after our mod-ifications to the RI algorithm to significantly im-prove its accuracy, SASH still provides a better ac-curacy/efficiency trade-off.
This is more evidentwhen considering the time to extract context in-formation from the raw text.
SASH, unlike RI, alsoallows us to choose both the weight and the mea-sure used.
LSH and PLEB could not match eitherthe efficiency of RI or the accuracy of SASH.We intend to use this knowledge to process evenlarger corpora to produce more accurate results.Having set out to improve the efficiency of dis-tributional similarity searches while limiting anyloss in accuracy, we are producing full nearest-neighbour searches 18 times faster, with only a 2%loss in accuracy.AcknowledgementsWe would like to thank our reviewers for theirhelpful feedback and corrections.
This work hasbeen supported by the Australian Research Coun-cil under Discovery Project DP0453131.ReferencesAndrei Broder.
1997.
On the resemblance and containmentof documents.
In Proceedings of the Compression andComplexity of Sequences, pages 21?29, Salerno, Italy.Walter A. Burkhard and Robert M. Keller.
1973.
Some ap-proaches to best-match file searching.
Communications ofthe ACM, 16(4):230?236, April.Moses S. Charikar.
2002.
Similarity estimation techniquesfrom rounding algorithms.
In Proceedings of the 34thAnnual ACM Symposium on Theory of Computing, pages380?388, Montreal, Quebec, Canada, 19?21 May.James Curran and Marc Moens.
2002.
Improvements in au-tomatic thesaurus extraction.
In Proceedings of the Work-shop of the ACL Special Interest Group on the Lexicon,pages 59?66, Philadelphia, PA, USA, 12 July.James Curran.
2004.
From Distributional to Semantic Simi-larity.
Ph.D. thesis, University of Edinburgh.Michel X. Goemans and David P. Williamson.
1995.Improved approximation algorithms for maximum cutand satisfiability problems using semidefinite program-ming.
Journal of Association for Computing Machinery,42(6):1115?1145, November.James Gorman and James Curran.
2005a.
Approximatesearching for distributional similarity.
In ACL-SIGLEX2005 Workshop on Deep Lexical Acquisition, Ann Arbor,MI, USA, 30 June.James Gorman and James Curran.
2005b.
Augmenting ap-proximate similarity searching with lexical information.In Australasian Language Technology Workshop, Sydney,Australia, 9?11 November.Gregory Grefenstette.
1994.
Explorations in Automatic The-saurus Discovery.
Kluwer Academic Publishers, Boston.Michael E. Houle and Jun Sakuma.
2005.
Fast approximatesimilarity search in extremely high-dimensional data sets.In Proceedings of the 21st International Conference onData Engineering, pages 619?630, Tokyo, Japan.Michael E. Houle.
2003.
Navigating massive data sets vialocal clustering.
In Proceedings of the 9th ACM SIGKDDInternational Conference on Knowledge Discovery andData Mining, pages 547?552, Washington, DC, USA.Piotr Indyk and Rajeev Motwani.
1998.
Approximate near-est neighbors: towards removing the curse of dimension-ality.
In Proceedings of the 30th annual ACM Symposiumon Theory of Computing, pages 604?613, New York, NY,USA, 24?26 May.
ACM Press.Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000.Random indexing of text samples for latent semantic anal-ysis.
In Proceedings of the 22nd Annual Conference of theCognitive Science Society, page 1036, Mahwah, NJ, USA.Pentti Kanerva.
1993.
Sparse distributed memory and re-lated models.
In M.H.
Hassoun, editor, Associative Neu-ral Memories: Theory and Implementation, pages 50?76.Oxford University Press, New York, NY, USA.Jussi Karlgren and Magnus Sahlgren.
2001.
From words tounderstanding.
In Y. Uesaka, P. Kanerva, and H Asoh, ed-itors, Foundations of Real-World Intelligence, pages 294?308.
CSLI Publications, Stanford, CA, USA.Thomas K. Landauer and Susan T. Dumais.
1997.
A solutionto plato?s problem: The latent semantic analysis theory ofacquisition, induction, and representation of knowledge.Psychological Review, 104(2):211?240, April.Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In Proceedings of ACM SIGKDD-02,pages 613?619, 23?26 July.Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.2005.
Randomized algorithms and NLP: Using localitysensitive hash functions for high speed noun clustering.In Proceedings of the 43rd Annual Meeting of the ACL,pages 622?629, Ann Arbor, USA.Mangus Sahlgren and Jussi Karlgren.
2005.
Automatic bilin-gual lexicon acquisition using random indexing of parallelcorpora.
Journal of Natural Language Engineering, Spe-cial Issue on Parallel Texts, 11(3), June.Julie Weeds and David Weir.
2005.
Co-occurance retrieval:A flexible framework for lexical distributional similarity.Computational Linguistics, 31(4):439?475, December.Peter N. Yianilos.
1993.
Data structures and algorithms fornearest neighbor search in general metric spaces.
In Pro-ceedings of the fourth annual ACM-SIAM Symposium onDiscrete algorithms, pages 311?321, Philadelphia.368
