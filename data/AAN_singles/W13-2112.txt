Proceedings of the 14th European Workshop on Natural Language Generation, pages 98?102,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsOverview of the First Content Selection Challenge from Open SemanticWeb DataNadjet Bouayad-Agha1Gerard Casamayor1Leo Wanner1,21DTIC, University Pompeu Fabra2Institucio?
Catalana de Recerca i Estudis Avanc?atsBarcelona, Spainfirstname.lastname@upf.eduChris MellishComputing ScienceUniversity of AberdeenAberdeen AB24 3UE, UKc.mellish@abdn.ac.ukAbstractIn this overview paper we present the out-come of the first content selection chal-lenge from open semantic web data, fo-cusing mainly on the preparatory stagesfor defining the task and annotating thedata.
The task to perform was describedin the challenge?s call as follows: given aset of RDF triples containing facts abouta celebrity, select those triples that are re-flected in the target text (i.e., a short bi-ography about that celebrity).
From theinitial nine expressions of interest, finallytwo participants submitted their systemsfor evaluation.1 IntroductionIn (Bouayad-Agha et al 2012), we presented theNLG challenge of content selection from seman-tic web data.
The task to perform was describedas follows: given a set of RDF triples contain-ing facts about a celebrity, select those triples thatare reflected in the target text (i.e., a short biogra-phy about that celebrity).
The task first requireda data preparation stage that involved the follow-ing two subtasks: 1) data gathering and prepara-tion, that is, deciding which data and texts to use,then downloading and pairing them, and 2) work-ing dataset selection and annotation, that is, defin-ing the criteria/guidelines for determining when atriple is marked as selected in the target text, andproducing a corpus of triples annotated for selec-tion.There were initially nine interested participants(including the two organizing parties).
Five ofwhich participated in the (voluntary) triple anno-tation rounds.1 In the end, only two participantssubmitted their systems:1We would like to thank Angelos Georgaras and StasinosKonstantopoulos from NCSR (Greece) for their participationin the annotation rounds.UA: Roman Kutlak, Chris Mellish and Kees vanDeemter.
Department of Computing Science, University of Aberdeen, Scotland (UK).UIC: Hareen Venigalla and Barbara Di iEugenio.Department of Computer Science, Universityof Illinois at Chicago (USA).Before the presentation of the baseline evalua-tion of the submitted systems and the discussionof the results (Section 4), we outline the two datapreparation subtasks (Sections 2 and 3).
In Sec-tion 5, we then sketch some conclusions with re-gard to the achievements and future of the con-tent selection task challenge.
More details aboutthe data, annotation and resources described in thisoverview, as well as links for downloading the dataand other materials (e.g., evaluation results, code,etc.)
are available on the challenge?s website.22 Data gathering and preparationWe chose Freebase as our triple datastore.3,4 Weobtained the triple set for each person in the Turtleformat (ttl) by grepping the official Freebase RDFdump released on the 30th of December 2012 forall triples whose subject is the person?s URI; cer-tain meta-data and irrelevant triples (i.e., tripleswith specific namespaces such as ?base?
or ?com-mon?)
have been filtered out.Each triple set is paired with the person?s sum-mary biography typically available in Wikipedia,which consists of the first paragraph(s) precedingthe page?s table of contents5Our final corpus consists of 60000+ pairs, all ofwhich follow two restrictions that are supposed to2http://www.taln.upf.edu/cschallenge2013/3http://www.freebase.com4For a comparison between Freebase and DBPedia, seehttp://wiki.freebase.com/wiki/DBPedia.5For example, the first four paragraphs in the follow-ing page constitute the summary biography of that person:http://en.wikipedia.org/wiki/George Clooney.98maximize the chances of having interesting pairswith sufficient original and selected input triplesfor the challenge.
Firstly, the number of uniquepredicates in the input ttl must be greater than10.
The number 10 is estimated based on thefact that a person?s nationality, date and place ofbirth, profession, type and gender are almost al-ways available and selected, such that we need asomewhat large set to select content from in or-der to make the task minimally challenging.
Sec-ondly, the Wikipedia-extracted summary biogra-phy must contain more than 5 anchors and at least20% of the available anchors, where an anchor isa URI in the text (i.e., external href attribute valuein the html) pointing to another Wikipedia articlewhich is directly related to that person.
Given thatmost Freebase topics have a corresponding DBPe-dia entity with a Wikipedia article, anchors foundin the introductory text are an indicator of potentialrelevant facts available in Freebase and are com-municated in the text.
In other words, the anchorthreshold restriction is useful to discard pairs withvery few triples to annotate.
We found this crite-rion more reliable than the absolute length of thetext which is not necessarily proportional with thenumber of triples available for that person.3 Working Dataset selection andannotationThe manual annotation task consisted in emulat-ing the content selection task of a Natural Lan-guage Generation system, by marking in the tripledataset associated with a person the triples predi-cated in the summary biography of that person ac-cording to a set of guidelines.
We performed tworounds of annotations.
In the first round, partic-ipants were asked to select content for the samethree celebrities.
The objectives of this annota-tion, in which five individuals belonging to fourdistinct institutions participated, were 1) for par-ticipants to get acquainted with the content selec-tion task envisaged, the domain and guidelines,2) to validate the guidelines, and 3) to formallyevaluate the complexity of the task by calculat-ing inter-annotator agreement.
For the latter weused free-marginal multi-rater Kappa, as it seemedsuited for the annotation task (i.e.
independent rat-ings, discrete categories, multiple raters, annota-tors are not restricted in how they distribute cat-egories across cases) (Justus, 2005).
We obtainedan average Kappa of 0.92 across the three pairs forthe 5 annotators and 2 categories (selected, not se-lected), which indicates a high level of agreementand therefore validates our annotation guidelines.Our objective for the second round of annota-tions was to obtain a dataset for participants towork with.
In the end, we gathered 344 pairs from5 individuals of 5 distinct institutions.
It should benoted that although both rounds of annotations fol-low the anchor restriction presented in Section 2,the idea to set a minimum number of predicatesfor the larger corpus of 60000+ pairs came forthafter analysing the results of the second round andnoting the data sparsity in some pairs.
In what fol-lows, we detail how the triples were presented tohuman annotators and what were the annotationcriteria set forth in the guidelines.3.1 Data presentationA machine-readable triple consists of a subjectwhich is a Freebase machine id (mid), a predicateand an object which can either be a Freebase midor a literal, as shown in the following two triples:ns:m.0dvldns:people.person.spouse_sns:m.02kknf3 .ns:m.0dvldns:people.person.date_of_birth"1975-10-05"?
?xsd:datetime .Triples were transformed into a human-readableform.
In particular, each mid in object position(e.g., 02kknf3) was automatically mapped ontoan abbreviated description of the Freebase topicit refers to.
Thus, the triples above have beenmapped onto a tabular form consisting of (1) pred-icate, (2) object description, (3) object id, and (4)object types (for literals):(1) /people/person/spouse_s(2) "1998-11-22 - Jim Threapleton -2001-12-13 - Marriage -Freebase Data Team - Marriage"(3) /m/02kknf3(1) /people/person/date_of_birth(2) value(3) "1975-10-05"(4) "datetime"For each triple thus presented, annotators wereasked to mark 1) whether it was selected, 2) inwhich sentence(s) of the text did it appear, and3) which triples, if any, are its coreferents.
Twotriples are coreferent if their overlap in meaning issuch that either of them can be selected to repre-sent the content communicated by the same text99fragment and as such should not count as two sep-arate triples in the evaluation.
Thus, the same textmight say He is probably best known for his stintwith heavy metal band Godsmack and He has alsotoured and recorded with a number of other bandsincluding Detroit based metal band Halloween?The Heavy Metal Horror Show?
.
.
.
, thus refer-ring in two different sentences to near-equivalenttriples /music/artist/genre ?
?Heavymetal" and /music/artist/genre?
?Hard rock".3.2 Annotation criteriaAnnotators were asked to first read the text care-fully, trying to identify propositional units (i.e.,potential triples) and then to associate each iden-tified propositional unit with zero, one or more(coreferent) triples according to the followingrules:Rule 1.
One cannot annotate facts that are notpredicated and cannot be inferred from predicatesin the text.
In other words, all facts must begrounded in the text.
For example, in the sentenceHe starred in Annie Hall, the following is pred-icated: W.H.has profession actor andW.H.
acted in film Annie Hall.
Theformer fact can be inferred from the latter.
How-ever, the following is not predicated: (1) Personhas name W.H., (2) W.H.
is Male, and (3)W.H.
is Person.Rule 2.
In general, one can annotate moregeneric facts if they can be inferred from morespecific propositions in the text, but one cannotannotate specific facts just because a more gen-eral proposition is found in the text.
In the exam-ple He was a navigator, we can mark the triplesPerson has profession Sailor as wellas Person has profession Navigator(we would also mark them as coreferent).
How-ever, given the sentence He was a sailor, we can-not mark the triple Person has professionNavigator, unless we can infer it from the textor world knowledge.Rule 3.
One can annotate specific facts from atext where the predicate is too vague or general ifthe facts can be inferred from the textual context,from the available data, or using world knowledge.This rule subsumes four sub-cases:Rule 3.1.
The predicate in the proposition is toovague or general and can be associated with mul-tiple, more specific triples.
In this case, do notselect any triple.
In the example Film A was agreat commercial success, we have several triplesassociating the celebrity with Film A, as direc-tor, actor, writer, producer and composer and noneof them with a predicate resembling ?commercialsuccess?.
In this case there are no triples that canbe associated with the text.Rule 3.2.
The predicate in the proposition istoo vague or general, but according to the datathere is just one specific triple it can be associatedwith.
In this case, select that triple.
In the ex-ample Paris released Confessions of an Heiress,the term released could be associated with au-thored, wrote or published.
However, there is onlyone triple associating that subject with that object,which matches one of the interpretations (i.e., au-thoring) of the predicate.
Therefore that triple canbe selected.Rule 3.3.
The predicate in the proposition istoo vague or general, but one or more specifictriples can be inferred using world knowl-edge.
In this case, select all.
The sentenceHe is also a jazz clarinetist who performsregularly at small venues in Manhattan, canbe associated with the available triples W.H.profession Clarinetist and W.H.music/group member/instruments playedClarinet, even though for this latter triplethe person being in a group is not mentionedexplicitly.
However, this can be inferred frombasic world knowledge.Rule 3.4.
The predicate in the proposition istoo vague or general, but one or more specifictriples can be inferred using the textual context.In this case, select all.
In the example By themid-1960s Allen was writing and directing films.
.
.
Allen often stars in his own films .
.
.
Some ofthe best-known of his over 40 films are Annie Hall(1977) .
.
.
, the relations of the person with thefilm Annie Hall are that of writer, director andactor, as supported by the previous text.
There-fore we would annotate facts stating that the per-son wrote, directed and starred in Annie Hall.However, we wouldn?t annotate composer or pro-ducer triples if they existed.Rule 4.
A proposition can be associatedwith multiple facts with identical or over-lapping meanings.
In the example, WoodyAllen is a musician, we have the triplesW.H occupation musician and W.Hprofession musician, which have near100identical meanings.
Therefore, we mark bothtriples and indicate that they co-refer.
Thesentence Woody Allen won prize as best directorfor film Manhattan, on the other hand, can beassociated with non-coreferring triples W.H wonprize and W.H.
directed Manhattan.Rule 5.
If the text makes reference to a set offacts but it does not enumerate them explicitly, andthere is no reason to believe it makes reference toany of them in particular, then do not annotate in-dividual facts.
Thus, sentence Clint Eastwood hasseven children does not warrant marking each ofthe seven children triples as selected, given thatthey are not enumerated explicitly.Rule 6.
If the text makes a clear and unam-biguous reference to a fact, do not annotate anyother facts, even though they can be inferred fromit.
In other words, as explained in Rule 1, all an-notated triples must be grounded in the text.
Inthe sentence For his work in the films Unforgiven(1992) and Million Dollar Baby (2004), Eastwoodwon Academy Awards for Best Director and Pro-ducer of the Best Picture, we can infer from worldknowledge that the celebrity was nominated priorto winning the award in those categories.
How-ever, the text makes a clear reference only to thefact that he won the award and there is no reasonto believe that it is also predicating the fact that thecelebrity was nominated.4 Baseline evaluationBriefly speaking, the UA system uses a generalheuristic based on the cognitive notion of com-munal common ground regarding each celebrity,which is approximated by scoring each lexicalizedtriple (or property) associated with a celebrity ac-cording to the number of hits of the Google searchAPI.
Only the top-ranked triples are selected (Kut-lak et al2013).
The UIC system uses a smallset of rules for the conditional inclusion of pred-icates that was derived offline from the statisticalanalysis of the co-occurrence between predicatesthat are about the same topic or that share someshared arguments; only the best performing rulestested against a subset of the development set areincluded (Venigalla and Di Eugenio, 2013).For the baseline evaluation, we used the devel-opment set obtained in the second round annota-tion (see Section 3).
However, we only considerpairs obtained during the second round annotationthat 1) follow both restrictions presented in Sec-Baseline UIC UAPrecision 49 64 47Recall 67 50 39F1 51 51 42Table 1: Baseline evaluation results (%)tion 2, and 2) have no coreferring triples.
Thislast restriction was added to minimize errors be-cause we observed that annotators were not al-ways consistent in their annotation of triple coref-erence.6 We therefore considered 188 annotationsfrom the 344 annotations of the development set.Of these, we used 40 randomly selected annota-tions for evaluating the systems and 144 for es-timating a baseline that only considers the top 5predicates (i.e., the predicates most often selected)and the type-predicate.7.The evaluation results of the three systems(baseline, UIC and UA) are presented in Table 1.The figures in the table were obtained by compar-ing the triples selected and rejected by each systemagainst the manual annotation.
The performanceof the baseline is quite high.
The UA system basedon a general heuristic scores lower than the base-line, whilst the UIC system has a better precisionthan the baseline, albeit a lower recall.
This mightbe due, as the UA authors observe in their sum-mary (Venigalla and Di Eugenio, 2013), to ?thelarge number of predicates that are present onlyin a few files .
.
.
[which] makes it harder to de-cide whether we have to include these predicatesor not.
?5 ConclusionsWe have given an overview of the first content se-lection challenge from open semantic web data,focusing on the rather extensive and challengingtechnological and methodological work involvedin defining the task and preparing the data.
Unfor-tunately, despite agile participation in these early6Type-predicate triples were filtered out of the annotatedfiles in the development set whilst they were included in thelarge corpus made available to the candidates.
Therefore,we added type-predicate triples in the development set aposteriori for this evaluation.
These type-predicate triplesmight be coreferring with other triples, say ns:m.08rd51ns:type.object.type ns:film.actor andns:m.08rd5 people/person/profession"Actor" /m/02hrh1q .
Nonetheless, this was nottaken into account in the evaluation.7The top 5 predicates were (in descending order of fre-quency): music track, film actor, profession, date of birth andnationality101preparatory stages, the number of submitted sys-tems was limited.
Both of the presented systemswere data-intensive in that they usedeither a poolof textual knowledge or the corpus of triple dataprovided by the challenge in order to select themost relevant data.Unlike several previous challenges that involvemore traditional NLG tasks (e.g., surface realiza-tion, referring expression generation), content se-lection from large input semantic data is a rela-tively new research endeavour in the NLG com-munity that coincides with the rising interest instatistical approaches to NLG and dates back, tothe best of our knowledge, to (Duboue and McK-eown, 2003).
Furthermore, although we had ini-tially planned to produce a training set for thetask, the cost of manual annotation turned outto be prohibitive and the resulting corpus wasonly fit for development and baseline evaluation.Despite these setbacks, we believe that open se-mantic web data is a promising test-bed and ap-plication field for NLG-oriented content selec-tion (Bouayad-Agha et al 2013) and trust thatthis first challenge has prepared the ground forfollow up challenges with a larger participation.We would also like to encourage researchers fromNLG and Semantic Web research fields to exploitthe framework and materials developed during thecourse of this challenge to advance research incontent selection.ReferencesNadjet Bouayad-Agha, Gerard Casamayor, and LeoWanner.
2013.
Natural Language Generation in theContext of the Semantic Web.
Submitted to the Se-mantic Web Journal.Nadjet Bouayad-Agha, Gerard Casamayor, Chris Mel-lish, and Leo Wanner.
2012.
Content Selection fromSemantic Web Data.
INLG ?12 Proceedings of theSeventh International Natural Language GenerationConference.
Pages 146-149.Pablo A. Duboue and Kathleen R. McKeown.
2003.Statistical Acquisition of Content Selection Rulesfor Natural Language Generation Proceedings ofthe Conference on Empirical Methods for NaturalLanguage Processing (EMNLP).
Pages 121?128.Randolph, Justus J.
2005.
Free-marginal multiraterkappa (multirater Kfree): An alternative to fleissfixed-marginal multirater kappa.
Presented as theJoensuu University Learning and Instruction Sym-posium.Roman Kutlak, Chris Mellish and Kees van Deemter2013.
Content Selection Challenge University ofAberdeen entry Proceedings of the 14th EuropeanNatural Language Generation (ENLG) Workshop.Hareen Venigalla and Barbara Di Eugenio.
2013.
UIC-CSC: The Content Selection Challenge Entry fromthe University of Illinois at Chicago Proceedingsof the 14th European Natural Language Generation(ENLG) Workshop.102
