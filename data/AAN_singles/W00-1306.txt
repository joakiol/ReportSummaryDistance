Sample Selection for Statistical Grammar InductionRebecca  HwaDivision of Engineering and Applied Sciences?
Harvard UniversityCambridge, MA 02138 USArebecca@eecs.harvard.eduAbst rac tCorpus-based grz.mmar induction relies on us-ing many hand-parsed sentences as trainingexamples.
However, the construction of atraining corpus with detailed syntactic analy-sis for every sentence is a labor-intensive task.We propose to use sample selection methodsto minimize the amount of annotation eededin the training data, thereby reducing theworkload of the human annotators.
This pa-per shows that the amount of annotated train-ing data can be reduced by 36% without de-grading the quality of the induced grammars.1 In t roduct ion :Many learning problems in the domain ofnatural anguage processing need supervisedtraining.
For instance, it is difficult to inducea grammar from a corpus of raw text; but thetask becomes much easier when the trainingsentences are supplemented with their parsetrees.
However, appropriate supervised train-ing data may be difficult to obtain.
Existingcorpora might not contain the relevant ypeof supervision, and the data might not bein the domain of interest.
For example, onemight need morphological nalyses of the lex-icon in addition to the parse trees for inducinga grammar, or one might be interested in pro-cessing non-English languages for which thereis no annotated corpus.
Because supervisedtraining typically demands ignificant humaninvolvement (e.g., annotating the parse treesof sentences by hand), building a new corpus isa labor-intensive task.
Therefore, it is worth-while to consider ways of minimizing the sizeof the corpus to reduce the effort spent by an-notators.
* This material is based upon work supported by theNational Science Foundation under Grant No.
IRI9712068.
We thank Wheeler Rural for his plottingtool; and Stuart Shieber, Lillian Lee, Ric Crabbe, andthe anonymous reviewers for their comments on thepaper.There are two possible directions: onemight attempt o reduce the amount of anno-tations in each sentence, as was explored byHwa (1999); alternatively, one might attemptto reduce the number of training sentences.In this paper, we consider the latter approachusing sample selection, an interactive l arningmethod in which the machine takes the initia-tive of selecting potentially beneficial train-ing examples for the humans to annotate.
Ifthe system could accurately identify a subsetof examples with high Training Utility Values(TUV) out of a pool of unlabeled ata, theannotators would not need to waste time onprocessing uninformative examples.We show that sample selection can beapplied to grammar induction to producehigh quality grammars with fewer annotatedtraining sentences.
Our approach is to useuncertainty-based evaluation functions thatestimate the TUV of a sentence by quantify-ing the grammar's uncertainty about assign-ing a parse tree to this sentence.
We haveconsidered two functions.
The first is a sire-?
ple heuristic that approximates the grammar'suncertainty in terms of sentence lengths.
Thesecond computes uncertainty in terms of thetree entropy of the sentence.
This metric isdescribed in detail later.This paper presents an empirical studymeasuring the effectiveness of our evaluationfunctions at selecting training sentences fromthe Wall Street Journal (WSJ) corpus (Mar-cuset al, 1993) for inducing rammars.
Con-ducting the experiments with training poolsof different sizes, we have found that sampleselection based on tree entropy reduces a largetraining pool by 36% and a small training poolby 27%.
These results u res t  that sample se-lection can significantly reduce \]~uman effortexerted in building training corpora.452 Sample  Se lec t ionUnlike traditional learning systems that re-ceive training examples indiscriminately, alearning system that uses sample selectionactively influences its progress by choosingnew examples to incorporate into its trainingset.
Sample selection works with two typesof learning systems: a committee of learnersor a single learner.
The committee-based se-lection algorithm works with multiple learn-ers, each maintaining a different hypothesis(perhaps pertaining to different aspects of theproblem).
The candidate xamples that ledto the most disagreements among the differ-ent learners are considered to have the high-est TUV (Cohn et al, 1994; Freund et al,1997).
For computationally intensive prob-lems such as grammar induction, maintainingmultiple learners may be an impracticality.
Inthis work, we explore sample selection with asingle learner that keeps just one working hy-pothesis at all times.Figure 1 outlines the single-learner sampleselection training loop in pseudo-code.
Ini-tially, the training set, L ,  consists of a smallnumber of labeled examples, based on whichthe learner proposes its first hypothesis ofthe target concept, C. Also available to thelearner is a large pool of uulabeled trainingcandidates, U.
In each training iteration, theselection algorithm, Select(n, U, C, f) ,  ranksthe  candidates of U according to their ex-pected TUVs and returns the n candidateswith the highest values.
The algorithm com-putes the expected TUV of each candidate,u E U, with an evaluation function, f(u, C).This function may possibly rely on the hy-pothesis concept C to estimate the utility ofa candidate u.
The set of the n chosen candi-dates are then labeled by human and addedto the existing training set.
Rnnning thelearning algorithm~ Train(L), on the updatedtraining set, the system proposes a new hy-pothesis consistent with all the examples eenthus far.
The loop continues until one of threestopping conditions is met: the hypothesis iconsidered close enough to the target concept,all candidates are labeled, or all human re-sources are exhausted.Sample selection may be beneficial for manylearning tasks in natural language process-ing.
Although there exist abundant collec-tions of raw text, the high expense of man-ually annotating the text sets a severe lim-itation for many learning algorithms in nat-U is a Set of unlabeled candidates.L is a set of labeled training examples.C is the current hypothesis.Initialize:C +-- Train(L).RepeatN ~-- Select(n, U, C, f).U~-U-N.L ~-- L t2 Label(N).C ~ Train(L).Unti l  (C ---- Ctrue)Or (U  = O) or (human stops)Figure 1: The pseudo-code for the sample se-lection learning algorithmural language processing.
Sample selectionpresents an attractive solution to offset thislabeled data sparsity problem.
Thus far, ithas been successfully applied to several classi-fication applications.
Some examples includetext categorization (Lewis and Gale, 1994),part-of-speech tagging (Engelson and Dagan,1996), word-sense disambiguation (Fujii et al,1998), and prepositional-phrase attachment(Hwa, 2000).More difficult are learning problems whoseobjective is not classification, but generationof complex structures.
One example in this di-rection is applying sample selection to seman-tic parsing (Thompson et al, 1999), in whichsentences are paired with their semantic rep-resentation using a deterministic shift-reduceparser.
Our work focuses on another complexnatural anguage learning problem: inducinga stochastic ontext-free grammar that cangenerate syntactic parse trees for novel testsentences.Although abstractly, parsing with a gram-mar can be seen as a classification task of de-termining the structure of a sentence by se-lecting one tree out of a set of possible parsetrees, there are two major distinctions thatdifferentiate it from typical classification prob-lems.
First, a classifier usually chooses froma fixed set of categories, but in our domain,every sentence has a different set of possibleparse trees.
Second, for most classificationproblems, the the number of the possible cate-gories is relatively small, whereas the numberof potential parse trees for a sentence is expo-nential with respect o the sentence length.463 Grammar  Induct ionThe degree of difficulty of the task of learninga grammar from data depends on the quantityand quality of the training supervision.
Whenthe training corpus consists of a larg e reservoirof fully annotated parse trees, it is possibleto directly extract a grammar based on theseparse trees.
The success of recent high-qualityparsers (Charniak, 1997; Collins, 1997) relieson the availability of such treebank corpora.To work with smaller training corpora, thelearning system would require even more in-formation about the examples than their syn-tactic parse trees.
For instance, Hermjakoband Mooney (1997) have described a learningsystem that can build a deterministic shift-reduce parser from a small set of trainingexamples with the aid of detailed morpho-logical, syntactical, and semantic knowledgedatabases and step-by-step guidance from hu-man experts.The induction task becomes more chal-lenging as the amount of  supervision in thetraining data and background knowledge de-creases.
To compensate for the missing infor-mation, the learning process requires heuristicsearch to find locally optimal grammars.
Oneform of partially supervised ata might spec-ify the phrasal boundaries without specify-ing their labels by bracketing each constituentunit with a pair of parentheses (McNaughton,1967).
For example, the parse tree for the sen-tence '~Several fund managers expect a roughmarket this morning before prices stablize.
"is labeled as "((Several fund managers) (ex-pect ((a rough market) (this morning)) (be-fore (prices tabilize))).)"
As shown in Pereiraand Schabes (1992), an essentially unsuper-vised learning algorithm such as the Inside-Outside re-estimation process (Baker, 1979;Lari and Young, 1990) can be modified to takeadvantage of these bracketing constraints.For our sample selection experiment, wechose to work under the more stringent con-dition of partially supervised training data, asdescribed above, because our ultimate goal isto minimize the amount of annotation doneby humans in terms of both the number ofsentences and the number of brackets withinthe sentences.
Thus, the quality of our in-duced grammars hould not be compared tothose extracted from a fully annotated train-ing corpus.
The learning algorithm we use isa variant of the Inside-Outside algorithm thatinduces grammars expressed in the Probabilis-tic Lexicalized Tree Insertion Grammar ep-resentation (Schabes and Waters, 1993; Hwa,1998).
This formalism's Context-free quiva-lence and its lexicalized representation makethe training process efficient and computa-tionally plausible.4 Se lec t ive  Sampl ing  Eva luat ionFunct ionsIn this paper, we propose two uncertainty-based evaluation functions for estimating thetraining utilities of the candidate sentences.The first is a simple heuristic that uses thelength of a sentence to estimate uncertain-ties.
The second function computes uncer-tainty in terms of the entropy of the parsetrees that the hypothesis-grammar generatedfor the sentence.4.1 Sentence LengthLet us first consider a simple evaluationfunction that estimates the training utilityof a candidate without consulting the cur-rent hypothesis-grammar, G. The functionften(s,G) coarsely approximates the uncer-tainty of a candidate sentence s with itslength:flen(S, G) = length(s).The intuition behind this function is basedon the general observation that longer sen-tences tend to have complex structures andintroduce more opportunities for ambiguousparses.
Since the scoring only depends onsentence lengths, this naive evaluation func-tion orders the training pool deterministicallyregardless of either the current state of thegrammar or the annotation of previous train-ing sentences.
This approach as one majoradvantage: it is easy to compute and takesnegligible processing time.4.2 Tree Ent ropySentence length is not a very reliable indi-cator of uncertainty.
To measure the un-certainty of a sentence more accurately, theevaluation function must base its estimationon the outcome of testing the sentence onthe hypothesis-grammar.
When a stochasticgrammar parses a sentence, it generates a setof possible trees and associates a likelihoodvalue with each.
Typically, the most likelytree is taken to be the best parse for the sen-tence.We propose an evaluation function thatconsiders the probabilities of all parses.
The47set of probabilities of the possible parse treesfor a sentence defines a distribution that in-dicates the grammar's uncertainty about thestructure of the sentence.
For example, a uni-form distribution signifies that the grammaris at its highest uncertainty because all theparses are equally likely; whereas a distribu-tion resembling an impulse function suggeststhat the grammar is very certain because itfinds one parse much more likely than all oth-ers.
To quantitatively characterize a distribu-tion, we compute its entropy.Entropy measures the uncertainty ofassign-ing a value to a random variable over a dis-tribution.
Informally speaking, it is the ex-pected number of bits needed to encode theassignment.
A higher entropy value signifiesa higher degree of uncertainty.
At the highestuncertainty, the random variable is assignedone of n values over a uniform distribution,and the outcome would require log2 (n) bits toencode.More formally, let V be a discrete randomvariable that can take any possible outcomein set V. Let p(v) be the density functionp(v) = Pr(Y = v), v E l).
The entropy H(V)is the expected negative log likelihood of ran-dom variable V:H (V) = -EX  ( logdv(V ) ) ).= -vEYFurther details about the properties of en-tropy can be found in textbooks on informa-tion theory (Cover and Thomas, 1991).Determining the parse tree for a sentencefrom a set of possible parses can be viewed asassigning a value to a random variable.
Thus,a direct application of the entropy definitionto the probability distribution of the parses forsentence s in grammar G computes its tree en-tropy, TE(s, G), the expected number of bitsneeded to encode the distribution of possibleparses for s. Note that we cannot comparesentences ofdifferent lengths by their entropy.For two sentences of unequal engths, bothwith uniform distributions, the entropy of thelonger one is higher.
To normalize for sen-tence length, we define an evaluation functionthat computes the similarity between the ac-tual probability distribution and the uniformdistribution for a sentence of that length.
Fora sentence s of length l, there can be at most0(2 l) equally likely parse trees and its maxi-real entropy is 0(l) bits (Cover and Thomas,1991).
Therefore, we define the evaluationfunction, fte(s, G) to be the tree entropy di-vided by the sentence l ngth.TE(s, G)Ire(s, G) = length(s)"We now derive the expression for TE(s, G).Suppose that a sentence s can be generated bya grammar G with some non-zero probability,Pr(s \[ G).
Let V be the set of possible parsesthat G generated for s. Then the probabilitythat sentence s is generated by G is the sumof the probabilities of its parses.
That is:Pr(s \[G) = ~Pr (v lG) .vEYNote that Pr(v \[ G) reflects the probability ofone particular parse tree, v, in the grammarout of all possible parse trees for all possiblesentences that G accepts.
But in order to ap-ply the entropy definition from above, we needto specify adistribution of probabilities for theparses of sentence s such thatvr(v Is, o )= 1.vEVPr(v \[ s, G) indicates the likelihood that v isthe correct parse tree out of a set of possibleparses for s according to grammar G. It isalso the density function, p(v), for the distri-bution (i.e., the probability of assigning v toa random variable V).
Using Bayes Rule andnoting that Pr(v, s \[ G) = Pr(v \[ G) (becausethe existence of tree v implies the existence ofsentence s), we get:v(v) = vr (v  I s, G) = Vr( .
,  s I G) = Vr(v I G)Pr(s I G) Pr(s I G)"Replacing the generic density function termin the entropy definition, we derive the expres-sion for TE(s, G), the tree entropy of s:TE(s,G) = H(V)- - - -  -- Z PCv) Iog2P(V )vEV= - P (s I a) log2(?
(s I c )  )vEYPr(v l C)= - ~ Pr(s \[G) l?g2Pr(v \[ G)vEY+ ~ Pr(v \[ G) log hPr (s lG)vev Pr(s \[ G)48~,cv Pr(v l G) log2 Pr(v l G)Pr(s I G)E sv P (v I a)+ logs P (s I - ,  i b)~vev Pr(v \] G) l?g 2Pr (v IG  )Pr(s 1 a)+ log 2 Pr(s I G)Using the bottom-up, dynamic program-ming technique of computing Inside Proba-bilities (Lari and Young, 1990), we can ef-ficiently compute the probability of the sen-tence, Pr(s I G).
Similarly, the algorithmcan be modified to compute the quantity~\]v~vPr( v I G)log2(Pr(v I G)) (see Ap-pendix A).5 Exper imenta l  SetupTo determine the effectiveness of selectingtraining examples with the two proposed eval-uation functions, we compare them againsta baseline of random selection (frand(S, G) =rand()).
The task is to induce grammars fromselected sentences in the Wall Street Journal(WSJ) corpus, and to parse unseen test sen-tences with the trained gr~.mmars.
Becausethe vocabulary size (and the grammar sizeby extension) is very large, we have substi-tuted the words with their part-of-speech tagsto avoid additional computational complexityin training the grammar.
After replacing thewords with part-of-speech tags, the vocabu-lary size of the corpus is reduced to 47 tags.We repeat the study for two differentcandidate-pool sizes.
For the first experiment,we assume that there exists an abundant sup--ply of unlabeled ata.
Based on empirical ob-servations (as will be shown in Section 6), forthe task we are considering, the induction al-gorithm typically reaches its asymptotic limitafter training with 2600 sentences; therefore,it is sufficient to allow for a candidate-pool sizeof U = 3500 unlabeled WSJ sentences.
In thesecond experiment, we restrict the size of thecandidate-pool such that U contains only 900unlabeled sentences.
This experiment studieshow the paucity of training data affects theevaluation functions.For both experiments, each of the threeevaluation functions: frand, ften, and fte, isapplied to the sample selection learning algo-rithm shown in Figure 1, where concept C isthe current hypothesis-grammar G, and L, theset of labeled training data; initially consistsof 100 sentences.
In every iteration, n = 100new sentences are picked from U to be addedto L, and a new C is induced from the updatedL.
After the hypothesis-grammar is updated,it is tested.
The quality of the induced gram-max is judged by its ability to generate cor-rect parses for unseen test sentences.
We usethe consistent bracketing metric (i.e., the per-centage of brackets in the proposed parse notcrossing brackets of the true parse) to mea-sure parsing accuracy 1.
To ensure the staffs-tical significance of the results, we report theaverage of ten trials for each experiment 2.6 ResultsThe results of the two experiments are graph-ically depicted in Figure 2.
We plot learningrates of the induction processes using train-ing sentences selected by the three evaluationfunctions.
The learning rate relates the qual-ity of the induced grammars to the amount ofsupervised training data available.
In orderfor the induced grammar to parse test sen-tences with higher accuracy (x-axis), more su-pervision (y-axis) is needed.
The amount ofsupervision is measured in terms of the num-ber of brackets rather than sentences becauseit more accurately quantifies the effort spentby the human annotator.
Longer sentencestend to require more brackets than short ones,and thus take more time to analyze.
We deemone evaluation function more effective thananother if the smallest set of sentences it se-lected can train a grammar that performs atleast as well as the grammar trained under theother function and if the selected data con-tains considerably fewer brackets than that ofthe other function.Figure 2(a) presents the outcomes of thefirst experiment, in which the evaluation func-tions select training examples out of a largecandidate-pool.
We see that overall, sampleselection has a positive effect on the learningIThe unsupervised induction algorithm inducesgrammars that generate binary branching trees so thatthe number  of proposed brackets in a sentence is al-ways one fewer than the length of the sentence.
TheWSJ  corpus, on the other hand, favors a more fiat-tened tree structure with considerably fewer bracketsper sentence.
The consistent bracketing metric doesnot unfairly penalize a proposed parse tree for beingbinary branching.2We generate different candidate-pools by movinga fixed-size window across WSJ  sections 02 through05, advancing 400 sentences for each trial.
Sec~n 23is always used for testing.49E/ oa ~  .
.
.
.Pa.~ing accura,.~ on the ~(a)i t fs :?
J.
/ J...o?..o-- ~Farsir~ accuracy on the tt~t(b)Figure 2: The learning rates of the induction processes using examples elected by the threeevaluation functions for (a) when the candidate-pool is large, and (b) when the candidate-poolis small.grammar setbaseline-26length-17tree entropy-!411 avg.
training brackets t-test on bracket.avg.
\[ avg.
score33355 N/A 80.330288 better 80.321236 better 80.4t-test on score avgN/Anot sig.
worsenot sig.
worseTable 1: Summary of pair-wise t-test with 95% confidence comparing the best set of grammarsinduced with the baseline (after 26 selection iterations) to the sets of grammars induced underthe proposed evaluation functions (ften after 17 iterations, fte after 14 iterations).rate of the induction process.
For the base-line case, the induction process uses frand,in which training sentences are randomly se-lected.
The resulting grammars achieves anaverage parsing accuracy of 80.3% on the testsentences after seeing an average of 33355brackets in the training data.
The learningrate of the tree entropy evaluation function,fte, progresses much faster than the baseline.To induce a grammar that reaches the same80.3% parsing accuracy with the examples e-lected by fte, the learner equires, on average,21236 training brackets, reducing the amountof annotation by 36% comparing to the base-line.
While the simplistic sentence lengthevaluation function, f~en, is less helpful, itslearning rate still improves lightly faster thanthe baseline.
A grammar of comparable qual-ity can be induced from a set of training exam-ples selected by fzen containing an average of30288 brackets.
This provides a small reduc-tion of 9% from the baseline 3.
We consider aset of grammars to be comparable to the base-3In terms of the number of sentences, the baselinef~d used 2600 randomly chosen training sentences;.fze,~ selected the 1700 longest sentences as trainingdata; and fte selected 1400 sentences.line if its mean test score is at least as highas that of the baseline and if the difference ofthe means is not statistically significant (us-ing pair-wise t-test at 95% confidence).
Ta-ble 1 summarizes the statistical significance ofcomparing the best set of baseline grammarswith those of of f~en and ffte.Figure 2(b) presents the results of the sec-ond experiment, in which the evaluation func-tions only have access to a small candidatepool.
Similar to the previous experiment,grammars induced from training examples e-lected by fte require significantly less annota-tions than the baseline.
Under the baseline,frand, to train grammars with 78.5% parsingaccuracy on test data, an average of 11699brackets (in 900 sentences) is required.
In con-trast, fte can induce a comparable grammarwith an average of 8559 brackets (in 600 sen-tences), providing a saving of 27% in the num-ber of training brackets.
The simpler evalua-tion function f~n out:performs the baselineas well; the 600 sentences it selected have anaverage of 9935 brackets.
Table 2 shows thestatistical significance of these comParisons.A somewhat surprising outcome of the sec-ond study is that the grammars induced from50grammar setbaseline-9length-6tree entropy-6tree entropy-8II avg.
training brackets t-test on bracket avg.
avg.
score t-~est on score a~11699 N/A 78.5 N/A9936 better 78.5 not sig.
worse8559 better 78.5 not sig.
worse11242 better 79.1 bettertest vg.Table 2: Summary of pair-wise t-test with 95% confidence comparing the best set of grammarsinduced with the baseline (after 9 selection iterations) to the sets of grammars induced underthe proposed evaluation functions (ften after 6 iterations, fte after 6 and 8 iterations).the three methods did not parse with the sameaccuracy when all the sentences from the un-labeled pool have been added to the trainingset.
Presenting the training examples in dif-ferent orders changes the search path of theinduction process.
Trained on data selectedby fte, the induced grammar parses the testsentences with 79.1% accuracy, a small butstatistically significant improvement over thebaseline.
This suggests that, when faced witha dearth of training candidates, fte can makegood use of the available data to induce gram-mars that are comparable to those directly in-duced from more data.7 Conc lus ion  and  Future  WorkThis empirical study indicates that sample se-lection can significantly reduce the human ef-fort in parsing sentences for inducing gram-mars.
Our proposed evaluation function usingtree entropy selects helpful training examples.Choosing from a large pool of unlabeled can-didates, it significantly reduces the amount oftraining annotations needed (by 36% in theexperiment).
Although the reduction is lessdramatic when the pool of candidates i  small(by 27% in the experiment), the training ex-amples it selected helped to induce slightlybetter grammars.The current work suggests many potentialresearch directions on selective sampling forgrammar induction.
First, since the ideas be-hind the proposed evaluation fimctions aregeneral and independent of formalisms, wewould like to empirically determine their ef-fect on other parsers.
Next, we shall explorealternative formulations of evaluation func-tions for the single-learner system.
The cur-rent approach uses uncertainty-based evalua-tion functions; we hope to consider other fac-tors such as confidence about the parametersof the grammars and domain knowledge.
Wealso plan to focus on the constituent unitswithin a sentence as training examples.
Thus,the evaluation functions could estimate thetraining utilities of constituent units ratherthan full sentences.
Another area of interestis to experiment with committee-based sam-ple selection using multiple learners.
Finally,we are interested in applying sample selectionto other natural anguage learning algorithmsthat have been limited by the sparsity of an-notated ata.Re ferencesJames K. Baker.
1979.
Trainable grammars forspeech recognition.
In Proceedings of the SpringConference of the Acoustical Society of Amer-ica, pages 547-550, Boston, MA, June.Eugene Charniak.
1997.
Statistical parsing witha context-free grammar and word statistics.
InProceedings of the AAAI, pages 598-603, Prov-idence, RI.
AAAI Press/MIT Press.David Cohn, Les Atlas, and Richard Ladner.
1994.Improving generalization with active learning.Machine Learning, 15(2):201-221.Michael Collins.
1997.
Three generative, lexi-calised models for statistical parsing.
In Pro-ceedings of the 35th Annual Meeting of the A CL,pages 16-23, Madrid, Spain.Thomas M. Cover and Joy A. Thomas.
1991.
El-ements of Information Theory.
John Wiley.Sean P. Engelson and Ido Dagan.
1996.
Mhaimiz-ing manual annotation cost in supervised train-ing from copora.
In Proceedings ofthe 34th An-nual Meeting of the ACL, pages 319-326.Yoav Freund, H. Sebastian Seung, Eli Shamir, andNaftali Tishby.
1997.
Selective sampling usingthe query by committee algorithm.
MachineLearning, 28(2-3):133-168.Atsushi Fujii, Kentaro Inui, Takenobu Tokunaga,and Hozumi Tanaka.
1998.
Selective samplingfor example-based word sense disambiguation.Computational Linguistics, 24(4):573-598, De-cember.Ulf Hermjakob and Raymond J. Mooney.
1997.Learning parse and translation decisions fromexamples with rich context.
In Proceedings o/the Association for Computational Linguistics,pages 482-489.Rebecca Hwa.
1998.
An empiric~al evaluationof probabilistic lexicaiized tree insertion gram-51mars.
In Proceedings off COLING-ACL, vol-ume 1, pages 557-563.Rebecca Hwa.
1999.
Supervised grammar in-duction using training data with limited con-stituent information.
In Proceedings of37th An-nual Meeting of the ACL, pages 73-79, June.Rebecca Hwa.
2000.
Learning Probabilistic Lex-icalized Grammars for Natural Language Pro-cessing.
Ph.D. thesis, ttarvard University.Forthcoming.K.
Lari and S.J.
Young.
19!70.
The estimationof stochastic ontext-free grammars using theinside-outside algorithm.
Computer Speech andLanguage, 4:35-56.David D. Lewis and William A. Gale.
1994.
A se-quential algorithm for training text classifiers.In Proceedings ofthe 17th Annual InternationalACM SIGIR Conference on Research and De-velopment inInformation Retrieval, pages 3-12.Mitchell Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: thePenn Treebank.
Computational Linguistics,19(2):313--330.Robert McNaughton.
1967.
Parenthesis gram-mars.
Journal off the ACM, 2(3):490--500.Fernando Pereira nd Yves Schabes.
1992.
Inside-Outside reestimation from partially bracketedcorpora.
In Proceedings of the 30th AnnualMeeting o\] the ACL, pages 128-135, Newark,Delaware.Yves Schabes and Richard Waters.
1993.
Stochas-tic lexicalized context-free grammar.
In Pro-ceedings of the Third International Workshopon Parsing Technologies, pages 257-266.Cynthia A. Thompson, Mary Elaine Califf, andRaymond J. Mooney.
1999.
Active learningfor natural anguage parsing and informationextraction.
In Proceedings of 1CML-99, pages406-414, Bled, Slovenia.A Ef f ic ient  Computat ion  o f  T reeEnt ropyThe tree entropy of a sentence depends on thequantity ~vevPr (v  \[G)log~(Pr(v \] G)) de-scribed in Section 4.2, a snm of an exponentialnumber of parses.
Fortunately, through a dy-namic programming algorithm similar to thecomputation of the Inside Probabilities, thisquantity can be efficiently computed.
The ba-sic idea is to compose the tree entropy of theentire sentence from the tree entropy of thesubtrees.For illustrative purposes, we describe thecomputation process using a PCFG grammarexpressed in Chomsky Normal Form, in whicheach rule can have two forms: X ~ YZor X ---r a, where X, Y, Z are variables overnon-terminal symbols and a is a variable overterminal symbols.
Moreover, let the sym-bol S be the start symbol of the grammarG.
Following the notation of Lari and Young,we denote the inside probability as e(X, i,j),which represents the probability that a non-terminal X :~ wi .
.
.wj .
Similarly, we definea new function h(X, i, j) to represent the cor-responding entropy for the set of subtrees.h(X, i , j )  =-  P r ( .
I a)log (Pr(, IV)).vEX~wi...w~Therefore, ~vev Pr(v \[G)log 2 Pr(v l G ) canbe expressed as h(S, 1, n).We compute all possible h(X, i , j )  re-cursively.
The base case is h(X,i , i )  =-e(X ,  i, i) log2 (e(X, i, i)) since a non-terminalX can generate the symbol wi in exactly oneway.
For the more general case, h(X, i,j), weconsider all the possible rules with X on theleft hand side that might have contributed tobuild X =~ wi .
.
.
wj.j -1hr,k=i (x~YZ)The function hy, z,k(X, i , j )  is a portion ofh(X, i , j )  where Y =~ wi .
.
.wk and Z ~Wk+l... wj.
The non-terminals Yand Z may,in turn, generate their substrings with mul-tiple parses.
Let there be a parses for Ywi.
.
.
Wk and f~ parses for Z ~ Wk+l.. .w i.Let x denote the event of X --r YZ; y EYl,.
.
.
,Ya; and z E z l , .
.
.
, zz .
The proba-bility of one of the a x fl possible parses isPr(x)Pr(y)Pr(z), and hY, z,k is computed bysumming over all possible parses:hy, z,k(X, i, j)= -- ~ ,z  Pr(x)Pr(y)Pr(z)xlog 2 (Pr (x)Pr (y)Pr (z) )= - Z~,~Pr(x)Pr(y)Pr(z)?\[log 2Pr(x) + log 2 Pr(y) + log 2 Pr(z)\]= -Pr (x )  log 2 Pr(x)e(Y, i, k)e(Z, k+l, j)+Pr(x)h(Y,i, k)e(Z,k + 1,j)+Pr(x)e(Y, i, k)h(Z, k + 1,j).These equations can be modified to computethe tree entropy of sentences using a Prob-abilistic Lexicalized Tree Insertion Grammar(Hwa, 2000).52
