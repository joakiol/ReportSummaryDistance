Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 107?111,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsRecognizing Salient Entities in Shopping QueriesZornitsa Kozareva, Qi Li, Ke Zhai and Weiwei GuoYahoo!701 First AvenueSunnyvale, CA 94089zornitsa@kozareva.com{lqi|kzhai}@yahoo-inc.comweiwei@cs.columbia.eduAbstractOver the past decade, e-Commerce hasrapidly grown enabling customers to pur-chase products with the click of a button.But to be able to do so, one has to under-stand the semantics of a user query andidentify that in digital lifestyle tv, digitallifestyle is a brand and tv is a product.In this paper, we develop a series of struc-tured prediction algorithms for seman-tic tagging of shopping queries with theproduct, brand, model and product familytypes.
We model wide variety of featuresand show an alternative way to captureknowledge base information using embed-dings.
We conduct an extensive study over37, 000 manually annotated queries andreport performance of 90.92 F1indepen-dent of the query length.1 IntroductionRecent study shows that yearly e-Commerce salesin the U.S. top 100 Billion (Fulgoni, 2014).
Thisleads to substantially increased interest in build-ing semantic taggers that can accurately recognizeproduct, brand, model and product family types inshopping queries to better understand and matchthe needs of online shoppers.Despite the necessity for semantic understand-ing, yet most widely used approaches for prod-uct retrieval categorize the query and the offer(Kozareva, 2015) into a shopping taxonomy anduse the predicted category as a proxy for retrievingthe relevant products.
Unfortunately, such proce-dure falls short and leads to inaccurate product re-trieval.
Recent efforts (Manshadi and Li, 2009; Li,2010) focused on building CRF taggers that recog-nize basic entity types in shopping query such asbrands, types and models.
(Li, 2010) conducteda study over 4000 shopping queries and showedpromising results when huge knowledge bases arepresent.
(Pas?ca and Van Durme, 2008; Kozareva etal., 2008; Kozareva and Hovy, 2010) focused onusing Hearst patterns (Hearst, 1992) to learn se-mantic lexicons.
While such methods are promis-ing, they cannot be used to recognize all prod-uct entities in a query.
In parallel to the semanticquery understanding task, there have been seman-tic tagging efforts on the product offer side.
(Put-thividhya and Hu, 2011) recognize brand, size andcolor entities in eBay product offers, while (Kan-nan et al, 2011) recognized similar fields in Bingproduct catalogs.Despite these efforts, to date there are three im-portant questions, which have not been answered,but we address in our work.
(1) What is an alter-native method when product knowledge bases arenot present?
(2) Is the performance of the seman-tic taggers agnostic to the query length?
(3) Canwe minimize manual feature engineering for shop-ping query log tagging using neural networks?The main contributions of the paper are:?
Building semantic tagging framework forshopping queries.?
Leveraging missing knowledge base entriesthrough word embeddings learned on largeamount of unlabeled query logs.?
Annotating 37, 000 shopping queries withproduct, brand, model and product family en-tity types.?
Conducting a comparative and efficiencystudy of multiple structured prediction algo-rithms and settings.?
Showing that long short-term memory net-works reaches the best performance of 90.92F1and is agnostic to query length.1072 Problem Formulation and Modeling2.1 Task DefinitionWe define our task as given a shopping queryidentify and classify all segments that are prod-uct, brand, product family and model, where:-Product is generic term(s) for goods not specificto a particular manufacturer (e.g.
shirts).-Brand is the actual name of the product manu-facturer (e.g.
Calvin Klein).-Product Family is a brand-specific grouping ofproducts sharing the same product (e.g.
SamsungGalaxy).-Model is used by manufacturer to distinguishvariations (e.g.
for the brand Lexus has IS prod-uct family, which has model 200t and 300 FSport).For modeling, we denote with T ={?, t1, t2, .
.
.
, tK} the whole label space,where ?
indicates a word that is not a part of anentity and tistands for an entity category.
Thetagging models have to recognize the followingtypes product, brand, model, product family and?
(other) using the BIO schema (Tjong Kim Sang,2002).We denote as x = (x1, x2, .
.
.
, xM) a shoppingquery of length M .
The objective is to find thebest configuration?y such that:?y = argmaxyp(y|x),where y=(y1, y2, ..., yN) (N ?
M ) are the shop-ping query segments labeled with their corre-sponding entity category.
Each segment yicor-responds to a triple ?bi, ei, ti?
indicating the startindex biand end index eiof the sequence followedby the entity category ti?
T .
When ti= ?, thesegment contains only one word.2.2 Structured Prediction ModelsTo tackle the shopping tagging problem of querylogs, we use Conditional Random Fields (Laffertyet al, 2001, CRF)1, learning to search (Daum?e IIIet al, 2009, SEARN)2, structured percep-tron (Collins, 2002, STRUCTPERCEPTRON) anda long short-term memory networks extended byCRF layer (Hochreiter and Schmidhuber, 1997;Graves, 2012, LSTM-CRF).CRF: is a popular algorithms for sequence tag-ging tasks (Lafferty et al, 2001).
The objective is1taku910.github.io/crfpp/2github.com/JohnLangford/vowpal_wabbitto find the label sequence y = (y1, ..., yM) thatmaximizesp(y|x) =1Z?(x)exp{?
?
f(y,x)},where Z?
(x) is the normalization factor, ?
is theweight vector and f(y,x) is the extracted featurevector for the observed sequence x.SEARN is a powerful structured prediction al-gorithm, which formulates the sequence labelingproblem as a search process.
The objective is tofind the label sequence y = (y1, ..., yM) that max-imizesp(y|x) ??Mm=1I[C(x,y1,...,ym?1)=y?m],whereC(?)
is a cost sensitive multiclass classifierand y?
are the ground-truth labels.STRUCTPERCEPTRON is an extension of thestandard perceptron.
In our setting we model asegment-based search algorithm, where each unitis a segment of x (e.g., ?bi, ei?
), rather than a sin-gle word (e.g., xi).
The objective is to find thelabel sequence y = (y1, ..., yM) that maximizesp(y|x) ?
w>?
f(x,y),where f(x,y) represents the feature vector for in-stance x along with the configuration y and w isupdated as w?
w + f(x,?y)?
f(x,y).LSTM-CRF The above algorithms heavily relyon manually-crafted features to perform sequencetagging.
We decided to alleviate that by usinglong short-term memory networks with a CRFlayer.
Our model is similar to R-CRF (Mesnil etal., 2015), but for the hidden recurrent layer weuse LSTM (Hochreiter and Schmidhuber, 1997;Graves, 2012).
We denote with hithe hidden vec-tor produced by the LSTM cell at i-th token.
Thenthe conditional probability of y given a query xbecomes:p(y|x) =1Z(h)exp{?i(Whyihi+Wtyi,yi?1)},where Whyiis the weight vector corresponding tolabel yi, and Wtyi,yi?1is the transition score cor-responding to yiand yi?1.
During training, thevalues of Wh, Wt, the LSTM layer and the inputword embeddings are updated through the stan-dard back-propagation with AdaGrad algorithm.We also concatenate pre-trained word embeddingand randomly initialized embedding (50-d) for theknowledge-base types of each token and use thisinformation in the input layer.
In our experiments,we set the learning rate to 0.05 and take each queryas a mini-batch and run 5 epochs to finish training.108FeaturesCRF SEARN STRUCTPERCEPTRONP (%) R (%) F1P (%) R (%) F1P (%) R (%) F1POS 39.86 35.51 37.56 34.97 33.55 34.25 33.03 24.70 28.27KB 51.64 41.08 45.76 41.96 37.26 39.47 35.70 35.97 35.84WE 65.31 61.02 63.11 67.58 67.00 67.29 71.29 68.12 69.67LEX+ORTO+PSTNL + POS + KB 86.49 83.84 85.15 84.19 84.30 84.24 88.88 86.87 87.87LEX+ORTO+PSTNL + POS + WE 88.30 85.74 87.00 84.32 84.15 84.24 87.85 85.69 86.76LEX+ORTO+PSTNL + POS + KB + WE 88.86 86.29 87.55 84.30 84.50 84.40 89.18 87.10 88.13Table 1: Results from feature study.2.3 FeaturesLexical (LEX): are widely used N -gram features.We use unigrams of the current w0, previous w?1and next w+1words, and bigrams w?1w0andw0w+1.Orthographic (ORTO): are binary mutually non-exclusive features that check if w0, w?1and w+1contain all-digits, any-digit, start-with-digit-end-in-letter and start-with-letter-end-in-digit.
Theyare designed to capture model names like hero3and m560.Positional (PSTNL): are discrete features model-ing the position of the words in the query.
Theycapture the way people tend to write products andbrands in the query.Part-of-Speech (POS): capture nouns and propernames to better recognize products and brands.We use Stanford tagger (Toutanova et al, 2003).Knowledgebase (KB): are powerful semantic fea-tures (Tjong Kim Sang, 2002; Carreras et al,2002; Passos et al, 2014).
We automaticallycollected and manually validated 200K brands,products, models and product families items ex-tracted from Macy?s and Amazon websites.WordEmbeddings (WE): While external knowl-edge bases are great resource, they are expensiveto create and time-consuming to maintain.
We useword embeddings (Mikolov et al, 2013)3as acheap low-maintenance alternative for knowledgebase construction.
We train the embeddings over2.5M unlabeled shopping queries.
For each tokenin the query, we use as features the 200 dimen-sional embeddings of the top 5 most similar termsreturned by cosine similarity.3 Experiments and ResultsData Set To the best of our knowledge, there isno publicly available shopping query data anno-tated with product, brand, model, product familyand other categories.
To conduct our experiments,we collect 2.5M shopping queries through click3https://code.google.com/p/word2vec/logs (Hua et al, 2013).
We randomly sampled37, 000 unique queries from the head, torso andtail of a commercial web search engine and askedtwo independent annotators to tag the data.
Wemeasured the Kappa agreement of the editors andfound .92 agreement, which is sufficient to warrantthe goodness of the annotations.We randomly split the data into 80% for trainingand 20% for testing.
Table 2 shows the distributionof the entity categories in the data.Product Brand Model Prod.
Family ?Train 21,688 10,417 4,394 6,697 47,517Test 5,413 2,659 1,099 1,716 11,780Table 2: Entity category distribution.We tune all parameters on the training set using5-fold cross validation and report performance onthe test set.
All results are calculated with theCONLL evaluation script4.Performance w.r.t.
Features Table 1 shows theperformance of the different models and featurecombinations.
We use the individual features as abaseline.
The obtained results show that these areinsufficient to solve such a complex task.
We com-pared the performance of the KB and WE featureswhen combined with (LEX+ORTO+PSTNL) infor-mation.
As we can see, both KB and WE reachcomparable performance.
This study shows thattraining embeddings on large in-domain data ofshopping queries is a reliable and cheap sourcefor knowledge base construction, when such in-formation is not present.
In our study the bestperformance is reached when all features are com-bined.
Among all machine learning classifiers forwhich we manually designed features, structuredperception reaches the best performance of 88.13F1score.In addition to the feature combination andmodel comparison, we also study in Figure 1 thetraining time of each model in log scale against itsF1score.
SEARN is the fastest algorithm to train,4cnts.ua.ac.be/conll2000/chunking/conlleval.txt109CategoryCRF SEARN STRUCTPERCEPTRON LSTM-CRFP (%) R (%) F1P (%) R (%) F1P (%) R (%) F1P (%) R (%) F1brand 91.79 87.93 89.82 89.3 89.3 89.3 93.99 91.20 92.57 95.15 92.29 93.70model 86.28 80.71 83.40 80.7 78.9 79.8 85.56 80.89 83.16 87.25 85.90 86.57product 87.85 88.16 88.00 83.4 85.0 84.2 87.90 87.92 87.91 91.94 90.98 91.46product family 89.27 81.41 85.16 81.4 79.0 80.2 88.12 82.17 85.04 87.98 87.47 87.73Overall 88.86 86.29 87.55 84.3 84.5 84.4 89.18 87.10 88.13 91.61 90.24 90.92Table 3: Per category performance.lll82848688100 1000 10000training time (second)f?1scorefeaturesl LEX+ORTO+PSTNLLEX+ORTO+PSTNL+KBLEX+ORTO+PSTNL+KB+WELEX+ORTO+PSTNL+POSLEX+ORTO+PSTNL+POS+KBLEX+ORTO+PSTNL+POS+KB+WELEX+ORTO+PSTNL+POS+WELEX+ORTO+PSTNL+WEmodelsl l lCRF++ Searn StructPerceptronFigure 1: Training time vs F1performance.while CRF takes the longest time to train.
Amongall STRUCTPERCEPTRON offers the best balancebetween efficiency and performance in a real timesetting.Performance w.r.t.
Entity Category Table 3shows the performance of the algorithms with themanually designed features against the automati-cally induced ones with LSTM-CRF.
We showthe performance of each individual product en-tity category.
Compared to all models and set-tings, LSTM-CRF reaches the best performanceof 90.92 F1score.
The most challenging entitytypes are product family and model, due to their?wild?
and irregular nature.Performance w.r.t.
Query Length Finally, wealso study the performance of our approach withrespect to the different query length.
Figure 2shows the F1score of the two best performing al-gorithms LSTM-CRF and STRUCTPERCEPTRONagainst the different query length in the test set.Around 83% of the queries have length between 2to 5 words, the rest are either very short or verylong ones.
As it can be seen in Figure 2, inde-pendent of the query length, our models reach thesame performance for short and long queries.
Thisshows that the models are robust and agnostic tothe query length.1 2 3 4 5 6 7 8 9 10query length05001000150020002500numberof queries# of queries020406080100F-1scoreStructured PercetronLSTM_CRFFigure 2: F1performance with varying query length.4 Conclusions and Future WorkIn this work, we have defined the task of prod-uct entity recognition in shopping queries.
Wehave studied the performance of multiple struc-tured prediction algorithms to automatically rec-ognize product, brand, model and product familyentities.
Our comprehensive experimental studyand analysis showed that combining lexical, po-sitional, orthographic, POS, knowledge base andword embedding features leads to the best perfor-mance.
We showed that word embeddings trainedon large amount of unlabeled queries could sub-stitute knowledge bases when they are missingfor specialized domains.
Among all manuallydesigned feature classifiers STRUCTPERCEPTRONreached the best performance.
While among allalgorithms LSTM-CRF achieved the highest per-formance of 90.92 F1 score.
Our analysis showedthat our models reach robust performance inde-pendent of the query length.
In the future we planto tackle attribute identification to better under-stand queries like ?diamond shape emerald ring?,where diamond shape is a cut and emerald isa gemstone type.
Such fine-grained informationcould further enrich online shopping experience.110ReferencesXavier Carreras, Llu?
?s M`arques, and Llu?
?s Padr?o.2002.
Named entity extraction using adaboost.
InProceedings of CoNLL-2002, pages 167?170.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proc.
EMNLP,pages 1?8.Hal Daum?e III, John Langford, and Daniel Marcu.2009.
Search-based structured prediction.
MachineLearning, 75(3):297?325.Gian Fulgoni.
2014.
State of the US retail economy inq1 2014.
In Comscore, Technical Report.Alex Graves.
2012.
Supervised Sequence Labellingwith Recurrent Neural Networks, volume 385 ofStudies in Computational Intelligence.
Springer.Marti Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proc.
of the14th conference on Computational linguistics, pages539?545.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural Computation,9(8):1735?1780.Xian-Sheng Hua, Linjun Yang, Jingdong Wang, JingWang, Ming Ye, Kuansan Wang, Yong Rui, and JinLi.
2013.
Clickage: Towards bridging semantic andintent gaps via mining click logs of search engines.In Proceedings of the 21st ACM International Con-ference on Multimedia, MM ?13, pages 243?252.Anitha Kannan, Inmar E. Givoni, Rakesh Agrawal,and Ariel Fuxman.
2011.
Matching unstructuredproduct offers to structured product specifications.In Proceedings of the 17th ACM SIGKDD Inter-national Conference on Knowledge Discovery andData Mining, KDD ?11, pages 404?412.Zornitsa Kozareva and Eduard Hovy.
2010.
Learningarguments and supertypes of semantic relations us-ing recursive patterns.
In Proceedings of the 48thAnnual Meeting of the Association for Computa-tional Linguistics, pages 1482?1491.Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.2008.
Semantic class learning from the web withhyponym pattern linkage graphs.
In Proceedings ofACL-08: HLT, pages 1048?1056.Zornitsa Kozareva.
2015.
Everyone likes shopping!multi-class product categorization for e-commerce.In Proceedings of the 2015 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 1329?1333.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In icml, pages 282?289.Xiao Li.
2010.
Understanding the semantic struc-ture of noun phrase queries.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 1337?1345.Mehdi Manshadi and Xiao Li.
2009.
Semantic taggingof web search queries.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP: Volume 2 -Volume 2, pages 861?869.Gr?egoire Mesnil, Yann Dauphin, Kaisheng Yao,Yoshua Bengio, Li Deng, Dilek Z. Hakkani-T?ur,Xiaodong He, Larry P. Heck, G?okhan T?ur, DongYu, and Geoffrey Zweig.
2015.
Using recurrentneural networks for slot filling in spoken languageunderstanding.
IEEE/ACM Transactions on Audio,Speech & Language Processing, 23(3):530?539.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013.
Distributed represen-tations of words and phrases and their composition-ality.
volume abs/1310.4546, pages 3111?3119.Marius Pas?ca and Benjamin Van Durme.
2008.Weakly-supervised acquisition of open-domainclasses and class attributes from web documents andquery logs.
In Proceedings of ACL-08: HLT, pages19?27.Alexandre Passos, Vineet Kumar, and Andrew McCal-lum.
2014.
Lexicon infused phrase embeddings fornamed entity resolution.
CoRR, abs/1404.5367.Duangmanee (Pew) Putthividhya and Junling Hu.2011.
Bootstrapped named entity recognition forproduct attribute extraction.
In Proceedings of the2011 Conference on Empirical Methods in NaturalLanguage Processing, pages 1557?1567.Erik F. Tjong Kim Sang.
2002.
Introduction tothe conll-2002 shared task: Language-independentnamed entity recognition.
In Proceedings ofCoNLL-2002, pages 155?158.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology- Volume 1, pages 173?180.111
