Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 67?73,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsUnsupervised Transduction Grammar Inductionvia Minimum Description LengthMarkus Saers and Karteek Addanki and Dekai WuHuman Language Technology CenterDept.
of Computer Science and EngineeringHong Kong University of Science and Technology{masaers|vskaddanki|dekai}@cs.ust.hkAbstractWe present a minimalist, unsupervisedlearning model that induces relativelyclean phrasal inversion transduction gram-mars by employing the minimum descrip-tion length principle to drive search overa space defined by two opposing ex-treme types of ITGs.
In comparison tomost current SMT approaches, the modellearns a very parsimonious phrase trans-lation lexicons that provide an obviousbasis for generalization to abstract trans-lation schemas.
To do this, the modelmaintains internal consistency by avoid-ing use of mismatched or unrelated mod-els, such as word alignments or probabil-ities from IBM models.
The model in-troduces a novel strategy for avoiding thepitfalls of premature pruning in chunkingapproaches, by incrementally splitting anITGwhile using a second ITG to guide thissearch.1 IntroductionWe introduce an unsupervised approach to induc-ing parsimonious, relatively clean phrasal inver-sion transduction grammars or ITGs (Wu, 1997)that employs a theoretically well-founded mini-mum description length (MDL) objective to ex-plicitly drive two opposing, extreme ITGs to-wards one minimal ITG.
This represents a newattack on the problem suffered by most currentSMT approaches of learning phrase translationsthat require enormous amounts of run-time mem-ory, contain a high degree of redundancy, and failsto provide an obvious basis for generalization toabstract translation schemas.
In particular, phrasalSMT models such as Koehn et al(2003) and Chi-ang (2005) often search for candidate translationsegments and transduction rules by committingto a word alignment based on very different as-sumptions (Brown et al 1993; Vogel et al 1996),and heuristically derive lexical segment transla-tions (Och and Ney, 2003).
In fact, it is possibleto improve the performance by tossing away mostof the learned segmental translations (Johnson etal., 2007).
In addition to preventing such waste-fulness, our work aims to also provide an obvi-ous basis for generalization to abstract translationschemas by driving the search for phrasal rules bysimultaneously using two opposing types of ITGconstraints that have both individually been empir-ically proven to match phrase reordering patternsacross translations well.We adopt a more ?pure?
methodology for eval-uating transduction grammar induction than typ-ical system building papers.
Instead of embed-ding our learned ITG in the midst of many otherheuristic components for the sake of a short termboost in BLEU, we focus on scientifically under-standing the behavior of pure MDL-based searchfor phrasal translations, divorced from the effectof other variables, even though BLEU is naturallymuch lower this way.
The common practice ofplugging some aspect of a learned ITG into ei-ther (a) a long pipeline of training heuristics and/or(b) an existing decoder that has been patched upto compensate for earlier modeling mistakes, aswe and others have done before?see for exampleCherry and Lin (2007); Zhang et al(2008); Blun-som et al(2008, 2009); Haghighi et al(2009);Saers and Wu (2009, 2011); Blunsom and Cohn(2010); Burkett et al(2010); Riesa and Marcu(2010); Saers et al(2010); Neubig et al(2011,2012)?obscures the specific traits of the inducedgrammar.
Instead, we directly use our learnedITG in translation mode (any transduction gram-mar also represents a decoder when parsing withthe input sentence as a hard constraint) which al-lows us to see exactly which aspects of correcttranslation the transduction rules have captured.67When the structure of an ITG is induced withoutsupervision, it has so far been assumed that smallerrules get clumped together into larger rules.
Thisis a natural way to search, since maximum like-lihood (ML) tends to improve with longer rules,which is typically balanced with Bayesian priors(Zhang et al 2008).
Bayesian priors are also usedin Gibbs sampling (Blunsom et al 2008, 2009;Blunsom and Cohn, 2010), as well as other non-parametric learning methods (Neubig et al 2011,2012).
All of the above evaluate their models byfeeding them into mismatched decoders, making ithard to evaluate how accurate the learned modelsthemselves were.
In this work we take a radicallydifferent approach, and start with the longest rulespossible and attempt to segment them into shorterrules iteratively.
This makes ML useless, since ourinitial model maximizes it.
Instead, we balance theML objective with a minimum description length(MDL) objective, which let us escape the initialML optimum by rewarding model parsimony.Transduction grammars can also be inducedwith supervision from treebanks, which cuts downthe search space by enforcing external constraints(Galley et al 2006).
This complicates the learn-ing process by adding external constraints that arebound to match the translation model poorly.
Itdoes, however, constitute a way to borrow nonter-minal categories that help the translation model.MDL has been used before in monolingualgrammar induction (Gr?nwald, 1996; Stolcke andOmohundro, 1994), as well as to interpret visualscenes (Si et al 2011).
Our work is markedly dif-ferent in that we (a) induce an ITG rather than amonolingual grammar, and (b) focus on learningthe terminal segments rather than the nonterminalcategories.
Iterative segmentation has also beenused before, but only to derive a word alignmentas part of a larger pipeline (Vilar and Vidal, 2005).The paper is structured as follows: we start bydescribing theMDL principle (Section 2).
We thendescribe the initial ITGs (Section 3), followed bythe algorithm that induces an MDL-optimal ITGfrom them (Section 4).
After that we describe theexperiments (Section 5), and the results (Section6).
Finally, we offer some conclusions (Section 7).2 Minimum description lengthThe minimum description length principle is aboutfinding the optimal balance between the size of amodel and the size of some data given the model(Solomonoff, 1959; Rissanen, 1983).
Consider theinformation theoretical problem of encoding somedata with a model, and then sending both the en-coded data and the information needed to decodethe data (the model) over a channel; the minimumdescription length is the minimum number of bitssent over the channel.
The encoded data can be in-terpreted as carrying the information necessary todisambiguate the uncertainties that the model hasabout the data.
The model can grow in size and be-comemore certain about the data, and it can shrinkin size and become more uncertain about the data.Formally, description length (DL) is:DL (?, D) = DL (D|?)
+ DL (?
)where ?
is the model and D is the data.In practice, we rarely have complete data to trainon, so we need our models to generalize to unseendata.
Amodel that is very certain about the trainingdata runs the risk of not being able to generalize tonew data: it is over-fitting.
It is bad enough whenestimating the parameters of a transduction gram-mar, and catastrophic when inducing the structure.The information-theoretic view of the problemgives a hint at the operationalization of descrip-tion length of a corpus given a grammar.
Shannon(1948) stipulates that we can get a lower bound onthe number of bits required to encode a specificoutcome of a random variable.
We thus define de-scription length of the corpus given the grammarto be: DL (D|?)
= ?lgP (D|?
)Information theory is also useful for the descrip-tion length of the grammar: if we can find a wayto serialize the grammar into a sequence of tokens,we can figure out how that sequence can be opti-mally encoded.
To serialize an ITG, we first needto determine the alphabet that the message will bewritten in.
We need one symbol for every nonter-minal, L0- and L1-terminal.
We will also makethe assumption that all these symbols are used inat least one rule, so that it is sufficient to serial-ize the rules in order to express the entire ITG.We serialize a rule with a type marker, followedby the left-hand side nonterminal, followed by allthe right-hand side symbols.
The type marker iseither [] denoting the start of a straight rule, or??
denoting the start of an inverted rule.
Unaryrules are considered to be straight.
We serializethe ITG by concatenating the serialized form of allthe rules, assuming that each symbol can be serial-ized into?lgc bits where c is the symbol?s relativefrequency in the serialized form of the ITG.683 Initial ITGsTo tackle the exponential problem of searching foran ITG that minimizes description length, it is use-ful to contrast two extreme forms of ITGs.
De-scription length has two components, model lengthand data length.
We call an ITG that minimizesthe data at the expense of the model a long ITG;we call an ITG that minimizes the model at the ex-pense of the data a short ITG.1 The long ITG sim-ply has all the sentence pairs as biterminals:S ?
AA ?
e0..T0/f0..V0A ?
e0..T1/f0..V1...A ?
e0..TN /f0..VNwhere S is the start symbol, A is the nonterminal,N is the number of sentence pairs, Ti is the lengthof the ith output sentence (making e0..Ti the ith out-put sentence), and Vi is the length of the ith inputsentence (making f0..Vi the ith input sentence).
Theshort ITG is a token-based bracketing ITG:S ?
A, A?
[AA] , A?
?AA?,A?
e/f, A?
e/?, A?
?/fwhere, S is the start symbol, A is the nonterminalsymbol, e is an L0-token, f is an L1-token, and ?is the empty sequence of tokens.4 Shortening the long ITGTo shorten the long ITG,wewill identify good splitcandidates in the terminal rules by parsing themwith the short ITG, and commit to split candidatesthat give a net gain.
A split candidate is an exist-ing long terminal rule, information about where tosplit its right-hand side, and whether to invert theresulting two rules or not.
Consider the terminalrule A ?
es..t/fu..v; it can be split at any point Sin L0 and any point U in L1, giving the three rulesA ?
[AA], A ?
es..S/fu..U and A ?
eS..t/fU..vwhen it is split in straight order, and the three rulesA?
?AA?, A?
es..S/fU..v and A?
eS..t/fu..Uwhen it is split in inverted order.
We will refer tothe original long rule as r0, and the resulting threerules as r1, r2 and r3.To identify the split candidates and to figure outhow the probability mass of r0 is to be distributed1Long and short ITGs correspond well to ad-hoc andpromiscuous grammars in Gr?nwald (1996).Algorithm 1 Rule shortening.Gl ?
The long ITGGs ?
The short ITGrepeatcands?
collect_candidates(Gl, Gs)?
?
0removed?
{}repeatscore(cands)sort_by_delta(cands)for all c ?
cands dor ?
original_rule(c)if r /?
removed and ?c ?
0 thenGl ?
update_grammar(Gl, c)removed?
{r} ?
removed?
?
?
+ ?cend ifend foruntil ?
?
0until ?
?
0return Glto the new rules, we use the short ITG to biparse theright-hand side of r0.
The distribution is derivedfrom the inside probability of the bispans that thenew rules are covering in the chart, and we referto them as ?1, ?2 and ?3, where the index indi-cates which new rule they apply to.
This has theeffect of preferring to split a rule into parts that areroughly equally probable, as the size of the data isminimized when the weights are equal.To choose which split candidates to commit to,we need a way to estimate their impact on the to-tal MDL score of the model.
This breaks downinto two parts: the difference in description lengthof the grammar: DL (??)
?
DL (?)
(where ??
is?
after committing to the split candidate), and thedifference in description length of the corpus giventhe grammar: DL (D|??)
?
DL (D|?).
The twoare added up to get the total change in descriptionlength.The difference in grammar length is calcu-lated as described in Section 2.
The difference indescription length of the corpus given the grammarcan be calculated by biparsing the corpus, sinceDL (D|??)
= ?lgP (D|p?)
and DL (D|?)
=?lgP (D|p) where p?
and p are the rule probabil-ity functions of ??
and ?
respectively.
Bipars-ing is, however, a very costly process that we donot want to carry out for every candidate.
Instead,we assume that we have the original corpus proba-bility (through biparsing when generating the can-69Table 1: The results of decoding.
NIST and BLEU are the translation scores at each iteration, followedby the number of rules in the grammar, followed by the average (as measured by mean and mode) numberof English tokens in the rules.Iteration NIST BLEU Rules Mean Mode1 2.7015 11.97 43,704 7.20 62 4.0116 14.04 42,823 6.30 63 4.1654 16.58 41,867 5.68 24 4.3723 17.43 40,953 5.23 15 4.2032 18.78 40,217 4.97 16 4.1329 17.28 39,799 4.84 17 4.0710 17.31 39,587 4.79 18 4.0437 17.10 39,470 4.75 1didates), and estimate the new corpus probabilityfrom it (in closed form).
The new rule probabilityfunction p?
is identical to p, except that:p?
(r0) = 0p?
(r1) = p (r1) + ?1p (r0)p?
(r2) = p (r2) + ?2p (r0)p?
(r3) = p (r3) + ?3p (r0)We assume the probability of the corpus given thisnew rule probability function to be:P(D|p?
)= P (D|p) p?
(r1) p?
(r2) p?
(r3)p (r0)This gives the following description length differ-ence:DL (D|??)?
DL (D|?)
=?lgp?(r1)p?(r2)p?
(r3)p(r0)We will commit to all split candidates that are es-timated to lower the DL, restricting it so that anyoriginal rule is split only in the best way (Algo-rithm 1).5 Experimental setupTo test whether minimum description length is agood driver for unsupervised inversion transduc-tion induction, we implemented and executed themethod described above.
We start by initializingone long and one short ITG.
The parameters of thelong ITG cannot be adjusted to fit the data better,but the parameters of the short ITG can be tuned tothe right-hand sides of the long ITG.We do so withan implementation of the cubic time algorithm de-scribed in Saers et al(2009), with a beam widthof 100.
We then run the introduced algorithm.As training data, we use the IWSLT07 Chinese?English data set (Fordyce, 2007), which contains46,867 sentence pairs of training data, and 489Chinese sentences with 6 English reference trans-lations each as test data; all the sentences are takenfrom the traveling domain.
Since the Chinese iswritten without whitespace, we use a tool that triesto clump characters together into more ?word like?sequences (Wu, 1999).After each iteration, we use the long ITG totranslate the held out test set with our in-house ITGdecoder.
The decoder uses a CKY-style parsingalgorithm (Cocke, 1969; Kasami, 1965; Younger,1967) and cube pruning (Chiang, 2007) to inte-grate the language model scores.
The decoderbuilds an efficient hypergraph structure which isscored using both the induced grammar and a lan-guage model.
We use SRILM (Stolcke, 2002) fortraining a trigram language model on the Englishside of the training corpus.
To evaluate the re-sulting translations, we use BLEU (Papineni et al2002) and NIST (Doddington, 2002).We also perform a combination experiment,where the grammar at different stages of the learn-ing process (iterations) are interpolated with eachother.
This is a straight-forward linear interpola-tion, where the probabilities of the rules are addedup and the grammar is renormalized.
Althoughit makes little sense from an MDL point of viewto increase the size of the grammar so indiscrim-inately, it does make sense from an engineeringpoint of view, since more rules typically meansbetter coverage, which in turn typically means bet-ter translations of unknown data.6 ResultsAs discussed at the outset, rather than burying ourlearned ITG in many layers of unrelated heuristicsjust to push up the BLEU score, we think it is more70Table 2: The results of decoding with combined grammars.
NIST and BLEU are the translation scores foreach combination, followed by the number of rules in the grammar, followed by the average (as measuredby mean and mode) number of English tokens in the rules.Combination NIST BLEU Rules Mean Mode1?2 (2 grammars) 4.2426 15.28 74,969 6.69 63?4 (2 grammars) 4.5087 18.75 54,533 5.41 35?6 (2 grammars) 4.1897 18.19 44,264 4.86 17?8 (2 grammars) 4.0953 17.40 40,785 4.79 11?4 (4 grammars) 4.9234 19.98 109,183 6.19 55?8 (4 grammars) 4.1089 17.86 47,504 4.84 11?8 (8 grammars) 4.8649 20.41 124,423 5.92 3important to illuminate scientific understanding ofthe behavior of pure MDL-driven rule inductionwithout interference from other variables.
Directlyevaluating solely the ITG in translation mode?instead of (a) deriving word alignments from it bycommitting to only the one-best parse, but then dis-carding any trace of structure and/or (b) evaluatingit through a decoder that has been patched up tocompensate for deficiencies in disparate aspects oftranslation?allows us to see exactly how accuratethe learned transduction rules are.The results from the individual iterations (Table1) show that we learn very parsimonious modelsthat far outperforms the only other result we areaware of where an ITG is tested exactly as it waslearned without altering the model itself: Saers etal.
(2012) induce a pure ITG by iteratively chunk-ing rules, but they report significantly lower trans-lation quality (8.30 BLEU and 0.8554 NIST) de-spite a significantly larger ITG (251,947 rules).The average rule length also decreases as smallerreusable spans are found.
The English side of thetraining data has amean of 8.45 and amode of 7 to-kens per sentence, and these averages drop steadilyduring training.
It is very encouraging to see themode drop to one so quickly, as this indicates thatthe learning algorithm finds translations of individ-ual English words.
Not only are the rules gettingfewer, but they are also getting shorter.The results from the combination experiments(Table 2) corroborate the engineering intuition thatmore rules give better translations at the expense ofa larger model.
Using all eight grammars gives aBLEU score of 20.41, at the expense of approxi-mately tripling the size of the grammar.
All indi-vidual iterations benefit from being combined withother iterations?but for the very best iterationsmore additional data is needed to get this improve-ment; the fifth iteration, which excelled at BLEUscore needs to be combinedwith all other iterationsto see an improvement, whereas the first and sec-ond iterations only need each other to see an im-provement.7 ConclusionsWe have presented a minimalist, unsupervisedlearning model that induces relatively cleanphrasal ITGs by iteratively splitting existing rulesinto smaller rules using a theoretically well-founded minimum description length objective.The resulting translation model is very parsimo-nious and provide an obvious foundation for gen-eralization tomore abstract transduction grammarswith informative nonterminals.8 AcknowledgementsThis material is based upon work supported inpart by the Defense Advanced Research ProjectsAgency (DARPA) under BOLT contract no.HR0011-12-C-0016, and GALE contract nos.HR0011-06-C-0022 and HR0011-06-C-0023; bythe European Union under the FP7 grant agree-ment no.
287658; and by the Hong KongResearch Grants Council (RGC) research grantsGRF620811, GRF621008, and GRF612806.
Anyopinions, findings and conclusions or recommen-dations expressed in this material are those of theauthors and do not necessarily reflect the views ofDARPA, the EU, or RGC.ReferencesPhil Blunsom and Trevor Cohn.
Inducing syn-chronous grammars with slice sampling.
InHLT/NAACL2010, pages 238?241, Los Ange-les, California, June 2010.71Phil Blunsom, Trevor Cohn, and Miles Osborne.Bayesian synchronous grammar induction.
InProceedings of NIPS 21, Vancouver, Canada,December 2008.Phil Blunsom, Trevor Cohn, Chris Dyer, andMilesOsborne.
A gibbs sampler for phrasal syn-chronous grammar induction.
In Proceedings ofthe Joint Conference of the 47th Annual Meet-ing of the ACL and the 4th International JointConference on Natural Language Processing ofthe AFNLP, pages 782?790, Suntec, Singapore,August 2009.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
The Mathe-matics of Machine Translation: Parameter esti-mation.
Computational Linguistics, 19(2):263?311, 1993.David Burkett, John Blitzer, and Dan Klein.
Jointparsing and alignment with weakly synchro-nized grammars.
In HLT/NAACL?10, pages127?135, Los Angeles, California, June 2010.Colin Cherry and Dekang Lin.
Inversion transduc-tion grammar for joint phrasal translation mod-eling.
In Proceedings of SSST?07, pages 17?24,Rochester, New York, April 2007.David Chiang.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceed-ings of ACL?05, pages 263?270, Ann Arbor,Michigan, June 2005.David Chiang.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228, 2007.John Cocke.
Programming languages and theircompilers: Preliminary notes.
Courant Instituteof Mathematical Sciences, New York Univer-sity, 1969.George Doddington.
Automatic evaluation ofmachine translation quality using n-gram co-occurrence statistics.
InProceedings of HLT?02,pages 138?145, San Diego, California, 2002.C.
S. Fordyce.
Overview of the IWSLT 2007 eval-uation campaign.
In Proceedings IWSLT?07,pages 1?12, 2007.Michel Galley, Jonathan Graehl, Kevin Knight,Daniel Marcu, Steve DeNeefe, Wei Wang, andIgnacio Thayer.
Scalable inference and trainingof context-rich syntactic translation models.
InProceedings COLING/ACL?06, pages 961?968,Sydney, Australia, July 2006.Peter Gr?nwald.
A minimum descriptionlength approach to grammar inference in sym-bolic.
Lecture Notes in Artificial Intelligence,(1040):203?216, 1996.Aria Haghighi, John Blitzer, John DeNero, andDan Klein.
Better word alignments withsupervised itg models.
In Proceedings ofACL/IJCNLP?09, pages 923?931, Suntec, Sin-gapore, August 2009.Howard Johnson, Joel Martin, George Foster, andRoland Kuhn.
Improving translation qualityby discarding most of the phrasetable.
In Pro-ceedings EMNLP/CoNLL?07, pages 967?975,Prague, Czech Republic, June 2007.TadaoKasami.
An efficient recognition and syntaxanalysis algorithm for context-free languages.Technical Report AFCRL-65-00143, Air ForceCambridge Research Laboratory, 1965.Philipp Koehn, Franz Joseph Och, and DanielMarcu.
Statistical Phrase-Based Translation.In Proceedings of HLT/NAACL?03, volume 1,pages 48?54, Edmonton, Canada, May/June2003.Graham Neubig, Taro Watanabe, Eiichiro Sumita,Shinsuke Mori, and Tatsuya Kawahara.
Anunsupervised model for joint phrase alignmentand extraction.
In Proceedings of ACL/HLT?11,pages 632?641, Portland, Oregon, June 2011.Graham Neubig, Taro Watanabe, Shinsuke Mori,and Tatsuya Kawahara.
Machine translationwithout words through substring alignment.
InProceedings of ACL?12, pages 165?174, Jeju Is-land, Korea, July 2012.Franz Josef Och and Hermann Ney.
A SystematicComparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51, 2003.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
BLEU: a method for automaticevaluation of machine translation.
In Proceed-ings of ACL?02, pages 311?318, Philadelphia,Pennsylvania, July 2002.Jason Riesa andDanielMarcu.
Hierarchical searchfor word alignment.
In Proceedings of ACL?10,pages 157?166, Uppsala, Sweden, July 2010.Jorma Rissanen.
A universal prior for integers andestimation by minimum description length.
TheAnnals of Statistics, 11(2):416?431, June 1983.72Markus Saers and Dekai Wu.
Improving phrase-based translation via word alignments fromStochastic Inversion Transduction Grammars.In Proceedings of SSST?09, pages 28?36, Boul-der, Colorado, June 2009.Markus Saers and Dekai Wu.
Principled inductionof phrasal bilexica.
In Proceedings of EAMT?11,pages 313?320, Leuven, Belgium, May 2011.Markus Saers, Joakim Nivre, and Dekai Wu.Learning stochastic bracketing inversion trans-duction grammars with a cubic time biparsingalgorithm.
In Proceedings of IWPT?09, pages29?32, Paris, France, October 2009.Markus Saers, JoakimNivre, and DekaiWu.
Wordalignment with stochastic bracketing linear in-version transduction grammar.
In Proceedingsof HLT/NAACL?10, pages 341?344, Los Ange-les, California, June 2010.Markus Saers, Karteek Addanki, and Dekai Wu.From finite-state to inversion transductions: To-ward unsupervised bilingual grammar induc-tion.
In Proceedings of COLING 2012: Techni-cal Papers, pages 2325?2340, Mumbai, India,December 2012.Claude Elwood Shannon.
A mathematical theoryof communication.
The Bell System TechnicalJournal, 27:379?423, 623?656, July, October1948.Zhangzhang Si, Mingtao Pei, Benjamin Yao,and Song-Chun Zhu.
Unsupervised learningof event and-or grammar and semantics fromvideo.
In Proceedings of the 2011 IEEE ICCV,pages 41?48, November 2011.Ray J. Solomonoff.
A new method for discoveringthe grammars of phrase structure languages.
InIFIP Congress, pages 285?289, 1959.Andreas Stolcke and Stephen Omohundro.
Induc-ing probabilistic grammars by bayesian modelmerging.
In R. C. Carrasco and J. Oncina, ed-itors, Grammatical Inference and Applications,pages 106?118.
Springer, 1994.Andreas Stolcke.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of theInternational Conference on Spoken LanguageProcessing, pages 901?904, Denver, Colorado,September 2002.Juan Miguel Vilar and Enrique Vidal.
A recur-sive statistical translation model.
In ACL-2005Workshop on Building and Using Parallel Texts,pages 199?207, Ann Arbor, Jun 2005.Stephan Vogel, Hermann Ney, and Christoph Till-mann.
HMM-basedWord Alignment in Statisti-cal Translation.
In Proceedings of COLING-96,volume 2, pages 836?841, 1996.Dekai Wu.
Stochastic Inversion Transduc-tion Grammars and Bilingual Parsing of Par-allel Corpora.
Computational Linguistics,23(3):377?403, 1997.Zhibiao Wu.
LDC Chinese segmenter, 1999.Daniel H. Younger.
Recognition and parsing ofcontext-free languages in time n3.
Informationand Control, 10(2):189?208, 1967.Hao Zhang, Chris Quirk, Robert C. Moore, andDaniel Gildea.
Bayesian learning of non-compositional phrases with synchronous pars-ing.
In Proceedings of ACL/HLT?08, pages 97?105, Columbus, Ohio, June 2008.73
