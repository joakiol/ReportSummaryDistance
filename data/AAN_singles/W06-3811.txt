Workshop on TextGraphs, at HLT-NAACL 2006, pages 65?72,New York City, June 2006. c?2006 Association for Computational LinguisticsSynonym Extraction Using a Semantic Distance on a DictionaryPhilippe MullerIRIT ?
CNRS, UPS & INPTToulouse, Francemuller@irit.frNabil HathoutERSS ?
CNRS & UTMToulouse, Francehathout@univ-tlse2.frBruno GaumeIRIT ?
CNRS, UPS & INPTToulouse, Francegaume@irit.frAbstractSynonyms extraction is a difficult task toachieve and evaluate.
Some studies havetried to exploit general dictionaries forthat purpose, seeing them as graphs wherewords are related by the definition they ap-pear in, in a complex network of an ar-guably semantic nature.
The advantageof using a general dictionary lies in thecoverage, and the availability of such re-sources, in general and also in specialiseddomains.
We present here a method ex-ploiting such a graph structure to computea distance between words.
This distanceis used to isolate candidate synonyms fora given word.
We present an evaluation ofthe relevance of the candidates on a sam-ple of the lexicon.1 IntroductionThesaurus are an important resource in many naturallanguage processing tasks.
They are used to help in-formation retrieval (Zukerman et al, 2003), machineor semi-automated translation, (Ploux and Ji, 2003;Barzilay and McKeown, 2001; Edmonds and Hirst,2002) or generation (Langkilde and Knight, 1998).Since the gathering of such lexical information is adelicate and time-consuming endeavour, some efforthas been devoted to the automatic building of sets ofsynonyms words or expressions.Synonym extraction suffers from a variety ofmethodological problems, however.
Synonymy it-self is not an easily definable notion.
Totally equiv-alent words (in meaning and use) arguably do notexist, and some people prefer to talk about near-synonyms (Edmonds and Hirst, 2002).
A near-synonym is a word that can be used instead ofanother one, in some contexts, without too muchchange in meaning.
This leaves of lot of freedomin the degree of synonymy one is ready to accept.Other authors include ?related?
terms in the build-ing of thesaurus, such as hyponyms and hypernyms,(Blondel et al, 2004) in a somewhat arbitrary way.More generally, paraphrase is a preferred term re-ferring to alternative formulations of words or ex-pressions, in the context of information retrieval ormachine translation.Then there is the question of evaluating the results.Comparing to already existing thesaurus is a de-batable means when automatic construction is sup-posed to complement an existing one, or when a spe-cific domain is targeted, or when simply the auto-matic procedure is supposed to fill a void.
Manualverification of a sample of synonyms extracted is acommon practice, either by the authors of a studyor by independent lexicographers.
This of coursedoes not solve problems related to the definition ofsynonymy in the ?manual?
design of a thesaurus,but can help evaluate the relevance of synonyms ex-tracted automatically, and which could have beenforgotten.
One can hope at best for a semi-automaticprocedure were lexicographers have to weed out badcandidates in a set of proposals that is hopefully nottoo noisy.A few studies have tried to use the lexical informa-tion available in a general dictionary and find pat-terns that would indicate synonymy relations (Blon-65del et al, 2004; Ho and C?drick, 2004).
The generalidea is that words are related by the definition theyappear in, in a complex network that must be seman-tic in nature (this has been also applied to word sensedisambiguation, albeit with limited success (Veronisand Ide, 1990; H.Kozima and Furugori, 1993)).We present here a method exploiting the graph struc-ture of a dictionary, where words are related by thedefinition they appear in, to compute a distance be-tween words.
This distance is used to isolate can-didate synonyms for a given word.
We present anevaluation of the relevance of the candidates on asample of the lexicon.2 Semantic distance on a dictionary graphWe describe here our method (dubbed Prox) to com-pute a distance between nodes in a graph.
Basi-cally, nodes are derived from entries in the dictio-nary or words appearing in definitions, and there areedges between an entry and the word in its definition(more in section 3).
Such graphs are "small world"networks with distinguishing features and we hypo-thetize these features reflect a linguistic and seman-tic organisation that can be exploited (Gaume et al,2005).The idea is to see a graph as a Markov chain whosestates are the graph nodes and whose transitions areits edges, valuated with probabilities.
Then we sendrandom particles walking through this graph, andtheir trajectories and the dynamics of their trajec-tories reveal their structural properties.
In short, weassume the average distance a particle has made be-tween two nodes after a given time is an indicationof the semantic distance between these nodes.
Ob-viously, nodes located in highly clustered areas willtend to be separated by smaller distance.Formally, if G = (V,E) is a reflexive graph (eachnode is connected to itself) with |V | = n, we note[G] the n ?
n adjacency matrix of G that is suchthat [G]i,j (the ith row and jth column) is non nullif there is an edge between node i and node j and0 otherwise.
We can have different weights forthe edge between nodes (cf.
next section), but themethod will be similar.The first step is to turn the matrix into a Markovianmatrix.
We note [G?]
the Markovian matrix of G,such that[G?
]r,s =[G]r,s?x?V ([G]r,x)The sum of each line of G is different from 0 sincethe graph is reflexive.We note [G?
]i the matrix [G?]
multiplied i times by it-self.Let now PROX(G, i, r, s) be [G?]ir,s.
This is thusthe probability that a random particle leaving node rwill be in node s after i time steps.
This is the mea-sure we will use to determine if a node s is closerto a node r than another node t. The choice for iwill depend on the graph and is explained later (cf.section 4).3 Synonym extractionWe used for the experiment the XML tagged MRDTr?sor de la Langue Fran?aise informatis?
(TLFi)from ATILF (http://atilf.atilf.fr/), alarge French dictionary with 54,280 articles, 92,997entries and 271,166 definitions.
The extraction ofsynonyms has been carried out only for nouns, verbsand adjectives.
The basic assumption is that wordswith semantically close definitions are likely to besynonyms.
We then designed a oriented graphthat brings closer definitions that contain the samewords, especially when these words occur in the be-ginning.
We selected the noun, verb and adjectivedefinitions from the dictionary and created a recordfor each of them with the information relevant tothe building of the graph: the word or expressionbeing defined (henceforth, definiendum); its gram-matical category; the hierarchical position of the de-fined (sub-)sense in the article; the definition proper(henceforth definiens).Definitions are made of 2 members: a definiendumand a definiens and we strongly distinguish these 2types of objects in the graph.
They are representedby 2 types of nodes: a-type nodes for the words be-ing defined and for their sub-senses; o-type nodesfor the words that occur in definiens.For instance, the noun nostalgie ?nostalgia?
has 6 de-fined sub-senses numbered A.1, A.2, B., C., C. ?
andD.
:66NOSTALGIE, subst.
f?m.A.
1.
?tat de tristesse [...]2.
Trouble psychique [...]B.
Regret m?lancolique [...] d?sir d?un retour dansle pass?.C.
Regret m?lancolique [...] d?sir insatisfait.?
Sentiment d?impuissance [...]D. ?tat de m?lancolie [...]The 6 sub-senses yield 6 a-nodes in the graph plusone for the article entry:a.S.nostalgie article entrya.S.nostalgie.1_1 sub-sense A.
1.a.S.nostalgie.1_2 sub-sense A.
2.a.S.nostalgie.2 sub-sense B.a.S.nostalgie.3 sub-sense C.a.S.nostalgie.3_1 sub-sense C. ?a.S.nostalgie.4 sub-sense D.A-node tags have 4 fields: the node type (namely a);its grammatical category (S for nouns, V for verbsand A for adjectives); the lemma that correponds tothe definiendum; a representation of the hierarchi-cal position of the sub-sense in the dictionary arti-cle.
For instance, the A.
2. sub-sense of nostalgiecorresponds to the hierarchical position 1_2.O-nodes represent the types that occur in definiens.1A second example can be used to present them.
Theadjective jonceux ?rushy?
has two sub-senses ?re-sembling rushes?
and ?populated with rushes?
:Jonceux, -euse,a) Qui ressemble au jonc.b) Peupl?
de joncs.Actually, TLFi definitions are POS-tagged and lem-matized:Jonceux/Sa) qui/Pro ressembler/V au/D jonc/S ./Xb) peupl?/A de/Prep jonc/S ./X 2The 2 definiens yield the following o-type nodes inthe graph:o.Pro.qui; o.V.ressembler; o.D.au;o.S.jonc; o.X..; o.A.peupl?
; o.Prep.de1The tokens are represented by edges.2In this sentence, peupl?
is an adjective and not a verb.All the types that occur in definiens are represented,including the function words (pronouns, deter-miners...) and the punctuation.
Function wordsplay an important role in the graph because theybring closer the words that belong to the samesemantical referential classes (e.g.
the adjectivesof resemblance), that is words that are likely tobe synonyms.
Their role is also reinforced by themanner edges are weighted.A large number of TLFi definitions concernsphrases and locutions.
However, these definitionshave been removed from the graph because:?
their tokens are not identified in the definiens;?
their grammatical categories are not given inthe articles and are difficult to calculate;?
many lexicalized phrases are not sub-senses ofthe article entry.O-node tags have 3 fields: the node type (namely o);the grammatical category of the word; its lemma.The oriented graph built for the experiment thencontains one a-node for each entry and each entrysub-sense (i.e.
each definiendum) and one o-nodefor each type that occurs in a definition (i.e.
in adefiniens).
These nodes are connected as follows:1.
The graph is reflexive;2.
Sub-senses are connected to the words of theirdefiniens and vice versa (e.g.
there is an edgebetween a.A.jonceux.1 and o.Pro.qui,and another one between o.Pro.qui anda.A.jonceux.1).3.
Each a-node is connected to the a-nodesof the immediately lower hierarchicallevel but there is no edge between ana-node and the a-nodes of higher hier-archical levels (e.g.
a.S.nostalgieis connected to a.S.nostalgie.1_1,a.S.nostalgie.1_2,a.S.nostalgie.2, a.S.nostalgie.3and a.S.nostalgie.4, but none of thesub-senses is connected to the entry).674.
Each o-node is connected to the a-node thatrepresents its entry, but there is no edge be-tween the a-node representing an entry and thecorresponding o-node (e.g.
there is an edge be-tween o.A.jonceux and a.A.jonceux,but none between a.A.jonceux ando.A.jonceux).All edge weights are 1 with the exception ofthe edges representing the 9 first words of eachdefiniens.
For these words, the edge weight takesinto account their position in the definiens.
Theweight of the edge that represent the first token is10; it is 9 for the second word; and so on down to1.3These characteristics are illustrated by the fragmentof the graph representing the entry jonceux in table1.4 Experiment and resultsOnce the graph built, we used Prox to compute a se-mantic similarity between the nodes.
We first turnedthe matrix G that represent the graph into a Marko-vian matrix [G?]
as described in section 2 and thencomputed [G?
]5, that correspond to 5-steps paths inthe Markovian graph.4 For a given word, we haveextracted as candidate synonyms the a-nodes (i) ofthe same category as the word (ii) that are the clos-est to the o-node representing that word in the dictio-nary definitions.
Moreover, only the first a-node ofeach entry is considered.
For instance, the candidatesynonyms of the verb accumuler ?accumulate?
arethe a-nodes representing verbs (i.e.
their tags beginin a.V) that are the closer to the o.V.accumulernode.5-steps paths starting from an o-node representing aword w reach six groups of a-nodes:A1 the a-nodes of the sub-senses which have w intheir definition;3Lexicographic definitions usually have two parts: a genusand a differentia.
This edge weight is intended to favour thegenus part of the definiens.4The path length has been determined empirically.A2 the a-nodes of the sub-senses with definienscontaining the same words as those of A1;A3 the a-nodes of the sub-senses with definienscontaining the same words as those of A2;B1 the a-nodes of the sub-senses of the article of w.(These dummy candidates are not kept.
)B2 the a-nodes of the sub-senses with definienscontaining the same words as those of B1;B3 the a-nodes of the sub-senses with definienscontaining the same words as those of B2;The three first groups take advantage of the factthat synonyms of the definiendum are often used indefiniens.The question of the evaluation of the extraction ofsynonyms is a difficult one, as was already men-tioned in the introduction.
We have at our disposalseveral thesauri for French, with various coverages(from about 2000 pairs of synonyms, to 140,000),and a lot of discrepancies.5 If we compare the the-saurus with each other and restrict the comparisonto their common lexicon for fairness, we still havea lot of differences.
The best f-score is never above60%, and it raises the question of the proper goldstandard to begin with.
This is all the more distress-ing as the dictionary we used has a larger lexiconthan all the thesaurus considered together (roughlytwice as much).
As our main purpose is to build a setof synonyms from the TLF to go beyond the avail-able thesaurus, we have no other way but to havelexicographers look at the result and judge the qual-ity of candidate synonyms.
Before imposing thisworkload on our lexicographer colleagues, we tooka sample of 50 verbs and 50 nouns, and evaluatedthe first ten candidates for each, using the rankingmethod presented above, and a simpler version withequal weights and no distinction between sense lev-els or node types.
The basic version of the graphalso excludes nodes with too many neighbours, suchas "?tre" (be), "avoir" (have), "chose" (thing), etc.
).Two of the authors separately evaluated the candi-dates, with the synonyms from the existing thesauri5These seven classical dictionaries of synonyms are allavailable from http://www.crisco.unicaen.fr/dicosyn.html.68o.A.jonceuxa.A.jonceuxa.A.jonceux.1a.A.jonceux.2o.Pro.quio.V.ressemblero.D.auo.S.jonco.X..o.A.peupl?o.Prep.deo.A.jonceux 1 1a.A.jonceux 1 1 1a.A.jonceux.1 1 1 1 1 1 1a.A.jonceux.2 1 1 1 1 1o.Pro.qui 10 1o.V.ressembler 9 1o.D.au 8 1o.S.jonc 7 8 1o.X.. 6 7 1o.A.peupl?
10 1o.Prep.de 9 1Table 1: A fragment of the graph, presented as a matrix.already marked.
It turned out one of the judge wasmuch more liberal than the other about synonymy,but most synonyms accepted by the first were ac-cepted by the second judge (precision of 0.85).6We also considered a few baselines inspired by themethod.
Obviously a lot of synonyms appear in thedefinition of a word, and words in a definition tendto be consider close to the entry they appear in.
Sowe tried two different baselines to estimate this bias,and how our method improves or not from this.The first baseline considers as synonyms of a wordall the words of the same category (verbs or nounsin each case) that appear in a definition of the word,and all the entry the word appear in.
Then we se-lected ten words at random among this base.The second baseline was similar, but restricted to thefirst word appearing in a definition of another word.Again we took ten words at random in this set if itwas larger than ten, and all of them otherwise.We show the results of precision for the first can-didate ranked by prox, the first 5, and the first 10(always excluding the word itself).
In the case ofthe two baselines, results for the first ten are a bit6The kappa score between the two annotators was 0.5 forboth verbs and nouns, which only moderately satisfactory.misleading, since the average numbers of candidatesproposed by these methods were respectively 8 and6 for verbs and 9 and 5.6 for nouns (Table 2).
Also,nouns had an average of 5.8 synonyms in the exist-ing thesauri (when what was considered was the minbetween 10 and the number of synonyms), and verbshad an average of 8.9.We can see that both baselines outperformsweighted prox on the existing thesaurus for verbs,and that the simpler prox is similar to baseline 2 (firstword only).
For nouns, results are close between B2and the two proxs.
It is to be noted that a lot ofuncommon words appear as candidates, as they arerelated with very few words, and a lot of these donot appear in the existing thesauri.By looking precisely at each candidate (see judges?scores), we can see that both baselines are slightlyimproved (and still close to one another), but arenow beaten by both prox for the first and the first5 words.
There is a big difference between the twojudges, so Judge 2 has better scores than Judge 1 forthe baselines, but in each case, prox was better.
Itcould be troubling to see how good the second base-line is for the first 10 candidates, but one must re-member this baseline actually proposes 6 candidateson average (when prox was always at 10), makingit actually nothing more than a variation on the 569Existing Thesauri (V) Judge 1 Judge 2 ET (N) J1 J2baseline-1 1 0.30 0.42 0.38 0.06 0.12 0.125 0.29 0.39 0.375 0.08 0.12 0.1310 0.31 0.41 0.39 0.10 0.14 0.15baseline-2 1 0.32 0.52 0.44 0.21 0.22 0.235 0.36 0.50 0.446 0.21 0.24 0.2510 0.28 0.51 0.46 0.19 0.245 0.255simple prox 1 0.35 0.67 NA 0.27 0.415 0.4175 0.34 0.52 NA 0.137 0.215 0.23710 0.247 0.375 NA 0.123 0.17 0.19weighted prox 1 0.22 0.56 0.76 0.18 0.44 0.55 0.196 0.44 0.58 0.148 0.31 0.3910 0.17 0.36 0.47 0.10 0.22 0.3Table 2: Experimental results on a sample, V=verbs, N=nouns,candidate baseline, to which it should be comparedin all fairness (and we see that prox is much betterthere).
The difference between the two versions ofprox shows that a basic version is better for verbsand the more elaborate one is better for nouns, withoverall better results for verbs than for nouns.One could wonder why there was some many morecandidates marked as synonyms by both judges,compared to the original compilation of thesaurus.Mainly, it seemed to us that it can be accounted forby a lot of infrequent words, or old senses of wordsabsent for more restricted dictionaries.
We are cur-rently investigating this matter.
It could also be thatour sample picked out a lot of not so frequent wordssince they outnumber frequent words in such a largedictionary as the TLF.
An indication is the averagefrequency of words in a corpus of ten years of thejournal "Le Monde".
The 50 words picked out inour sample have an average frequency of 2000 oc-currences, while when we consider all our about 430candidates for synonymy, the average frequency is5300.The main conclusion to draw here is that our methodis able to recover a lot of synonyms that are in thedefinition of words, and some in definitions not di-rectly related, which seems to be an improvement onprevious attempts from dictionaries.
There is somearbitrariness in the method that should be furtherinvestigated (the length of the random walk for in-stance), but we believe the parameters are rather in-tuitive wrt to graph concepts.
We also have an as-sessment of the quality of the method, even thoughit is still on a sample.
The precision seems fair onthe first ten candidates, enough to be used in a semi-automatic way, coupled with a lexicographic analy-sis.5 Related workAmong the methods proposed to collect synonymyinformation, two families can be distinguished ac-cording to the input they consider.
Either a gen-eral dictionary is used (or more than one (Wu andZhou, 2003)), or a corpus of unconstrained textsfrom which lexical distributions are computed (sim-ple collocations or syntactic dependencies) (Lin,1998; Freitag et al, 2005) .
The approach of (Barzi-lay and McKeown, 2001) uses a related kind of re-source: multiple translations of the same text, withadditional constraints on availability, and problemsof text alignment, for only a third of the results be-ing synonyms (when compared to Wordnet).A measure of similarity is almost always used torank possible candidates.
In the case of distribu-tional approaches, similarity if determined from theappearance in similar contexts (Lin, 1998); in thecase of dictionary-based methods, lexical relationsare deduced from the links between words expressedin definitions of entries.Approaches that rely on distributional data have twomajor drawbacks: they need a lot of data, gener-ally syntactically parsed sentences, that is not al-ways available for a given language (English is anexception), and they do not discriminate well amonglexical relations (mainly hyponyms, antonyms, hy-pernyms) (Weeds et al, 2004) .
Dictionary-based70approaches address the first problem since dictionar-ies are readily available for a lot of language, evenelectronically, and this is the raison d?
?tre of our ef-fort.
As we have seen here, it is not an obvious taskto sort related terms with respect to synonymy, hy-pernymy, etc, just as with distribution approaches.A lot of work has been done to extract lexical rela-tions from the definitions taken in isolation (mostlyfor ontology building), see recently (Nichols et al,2005), with a syntactic/semantic parse, with usuallyresults around 60% of precision (that can be com-pared with the same baseline we used, all words inthe definition with the same category), on dictionar-ies with very small definitions (and thus a higherproportions of synonyms and hypernyms).
Estimat-ing the recall of such methods have not been done.Using dictionaries as network of lexical items orsenses has been quite popular for word sense dis-ambiguation (Veronis and Ide, 1990; H.Kozima andFurugori, 1993; Niwa and Nitta, 1994) before los-ing ground to statistical approaches, even though(Gaume et al, 2004; Mihalcea et al, 2004) tried a re-vival of such methods.
Both (Ho and C?drick, 2004)and (Blondel et al, 2004) build a graph of lexicalitems from a dictionary in a manner similar to ours.In the first case, the method used to compute similar-ity between two concepts (or words) is restricted toneighbors, in the graph, of the two concepts; in thesecond case, only directly related words are consid-ered as potential candidates for synonymy: for twowords to be considered synonyms, one has to appearin the definition of another.
In both cases, only 6or 7 words have been used as a test of synonymy,with a validation provided by the authors with "re-lated terms" (an unclear notion) considered correct.The similarity measure itself was evaluated on a setof related terms from (Miller and Charles, 1991), asin (Budanitsky and Hirst, 2001; Banerjee and Ped-ersen, 2003), with seemingly good results, but se-mantically related terms is a very different notion("car" and "tire" for instance are semantically relatedterms, and thus considered similar).We do not know of any dictionary-based graph ap-proach which have been given a larger evaluation ofits results.
Parsing definitions in isolation prevents acomplete coverage (we estimated that only 30% ofsynonyms pairs in the TLF can be found from defi-nitions).As for distributional approaches, (Barzilay andMcKeown, 2001) gets a very high precision (around90%) on valid paraphrases as judged by humans,among which 35% are synonymy relations in Word-net, 32% are hypernyms, 18% are coordinate terms.Discriminating among the paraphrases types is notaddressed.
Other approaches usually consider eithergiven sets of synonyms among which one is to bechosen (for a translation for instance) (Edmonds andHirst, 2002) or must choose a synonym word againstunrelated terms in the context of a synonymy test(Freitag et al, 2005), a seemingly easier task thanactually proposing synonyms.
(Lin, 1998) proposesa different methodology for evaluation of candidatesynonyms, by comparing similarity measures of theterms he provides with the similarity measures be-tween them in Wordnet, using various semantic dis-tances.
This makes for very complex evaluation pro-cedures without an intuitive interpretation, and thereis no assessment of the quality of the automated the-saurus.6 ConclusionWe have developed a general method to extract near-synonyms from a dictionary, improving on the twobaselines.
There is some arbitrariness in the param-eters we used, but we believe the parameters arerather intuitive wrt to graph concepts.7 There isroom for improvement obviously, also for a combi-nation with other methods to filter synonyms (withfrequency estimates for instance, such as tf.idf ormutual information measures).Clearly the advantage of using a dictionary is re-tained: there is no restriction of coverage, and wecould have used a specialised dictionary to build aspecialised thesaurus.
We have provided an assess-ment of the quality of the results, although thereis not much to compare it to (to the best of ourknowledge), since previous accounts only had cur-sory evaluation.7The lexical graph can be explored at http://prox.irit.fr.71AcknowledgmentsThis research has been supported by the CNRS pro-gram TCAN 04N85/0025.
We sincerely thank theATILF laboratory and Pr.
Jean-Marie Pierrel forthe opportunity they gave us to use the Tr?sor de laLangue Fran?aise informatis?.
We would also liketo thank Jean-Marc Destabeaux for his crucial helpin extracting the definitions from the TLFi.ReferencesS.
Banerjee and T. Pedersen.
2003.
Extended gloss over-laps as a measure of semantic relatedness.
In Proceed-ings of IJCAI-03, Acapulco, Mexico.Regina Barzilay and Kathleen R. McKeown.
2001.
Ex-tracting paraphrases from a parallel corpus.
In Proceed-ings of the 39th ACL, pages 00?00, Toulouse.Vincent D. Blondel, Anah?
Gajardo, Maureen Heymans,Pierre Senellart, and Paul Van Dooren.
2004.
A mea-sure of similarity between graph vertices: Applicationsto synonym extraction and web searching.
SIAM Review,46(4):647?666.A.
Budanitsky and G. Hirst.
2001.
Semantic distancein wordnet: An experimental, application-oriented eval-uation of five measures.
In Workshop on WordNet andOther Lexical Resources, NAACL 2001, Pittsburgh.Philip Edmonds and Graeme Hirst.
2002.
Near-Synonymy and lexical choice.
Computational Linguis-tics, 28(2):105?144.Dayne Freitag, Matthias Blume, John Byrnes, EdmondChow, Sadik Kapadia, Richard Rohwer, and ZhiqiangWang.
2005.
New experiments in distributional repre-sentations of synonymy.
In Proceedings of CoNLL, pages25?32, Ann Arbor, Michigan, June.
Association for Com-putational Linguistics.B.
Gaume, N. Hathout, and P. Muller.
2004.
Wordsense disambiguation using a dictionary for sense similar-ity measure.
In Proceedings of Coling 2004, volume II,pages 1194?1200, Gen?ve.B.
Gaume, F. Venant, and B. Victorri.
2005.
Hierarchyin lexical organization of natural language.
In D. Pumain,editor, Hierarchy in natural and social sciences, Metho-dos series, pages 121?143.
Kluwer.H.Kozima and T. Furugori.
1993.
Similarity betweenwords computed by spreading activation on an englishdictionary.
In Proceedings of the EACL, pages 232?239.Ngoc-Diep Ho and Fairon C?drick.
2004.
Lexical simi-larity based on quantity of information exchanged - syn-onym extraction.
In Proc.
of Intl.
Conf.
RIVF?04, Hanoi.Irene Langkilde and Kevin Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
In Pro-ceedings of COLING-ACL ?98, volume 1, pages 704?710, Montreal.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of COLING-ACL ?98,volume 2, pages 768?774, Montreal.Rada Mihalcea, Paul Tarau, and Elizabeth Figa.
2004.PageRank on semantic networks, with application toword sense disambiguation.
In Proceedings of Coling2004, Geneva.GA Miller and WG Charles.
1991.
Contextual corre-lates of semantic similarity.
Language and CognitiveProcesses, 6(1):1?28.Eric Nichols, Francis Bond, and Daniel Flickinger.
2005.Robust ontology acquisition from machine-readable dic-tionaries.
In Proceedings of IJCAI?05.Yoshiki Niwa and Yoshihiko Nitta.
1994.
Co-occurrencevectors from corpora vs. distance vectors from dictionar-ies.
In Proceedings of Coling 1994.Sabine Ploux and Hyungsuk Ji.
2003.
Amodel for matching semantic maps between languages(French/English, English/French).
Computational Lin-guistics, 29(2):155?178.J.
Veronis and N.M. Ide.
1990.
Word sense disambigua-tion with very large neural networks extracted from ma-chine readable dictionaries.
In COLING-90: Proceed-ings of the 13th International Conference on Computa-tional Linguistics, volume 2, pages 389?394, Helsinki,Finland.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributional similar-ity.
In Proceedings of Coling 2004, pages 1015?1021,Geneva, Switzerland, Aug 23?Aug 27.
COLING.Hua Wu and Ming Zhou.
2003.
Optimizing synonymsextraction with mono and bilingual resources.
In Pro-ceedings of the Second International Workshop on Para-phrasing, Sapporo, Japan.
Association for ComputationalLinguistics.Ingrid Zukerman, Sarah George, and Yingying Wen.2003.
Lexical paraphrasing for document retrieval andnode identification.
In Proceedings of the Second Inter-national Workshop on Paraphrasing, Sapporo, Japan.
As-sociation for Computational Linguistics.72
