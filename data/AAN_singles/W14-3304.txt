Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 66?70,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsYandex School of Data AnalysisRussian-English Machine Translation System for WMT14Alexey Borisov and Irina GalinskayaYandex School of Data Analysis16, Leo Tolstoy street, Moscow, Russia{alborisov, galinskaya}@yandex-team.ruAbstractThis paper describes the Yandex Schoolof Data Analysis Russian-English systemsubmitted to the ACL 2014 Ninth Work-shop on Statistical Machine Translationshared translation task.
We start with thesystem that we developed last year and in-vestigate a few methods that were success-ful at the previous translation task includ-ing unpruned language model, operationsequence model and the new reparameter-ization of IBM Model 2.
Next we proposea {simple yet practical} algorithm to trans-form Russian sentence into a more easilytranslatable form before decoding.
The al-gorithm is based on the linguistic intuitionof native Russian speakers, also fluent inEnglish.1 IntroductionThe annual shared translation task organizedwithin the ACL Workshop on Statistical MachineTranslation (WMT) aims to evaluate the state ofthe art in machine translation for a variety of lan-guages.
We participate in the Russian to Englishtranslation direction.The rest of the paper is organized as follows.Our baseline system as well as the experimentsconcerning the methods already discussed in lit-erature are described in Section 2.
In Section 3 wepresent an algorithm we use to transform the Rus-sian sentence before translation.
In Section 4 wediscuss the results and conclude.2 Initial System DevelopmentWe use all the Russian-English parallel data avail-able in the constraint track and the Common CrawlEnglish monolingual corpus.2.1 BaselineWe use the phrase-based Moses statistical ma-chine translation system (Koehn et al., 2007) withmostly default settings and a few changes (Borisovet al., 2013) made in the following steps.Data Preprocessing includes filtering out nonRussian-English sentence pairs and correction ofspelling errors.Phrase Table Smoothing uses Good-Turingscheme (Foster et al., 2006).Consensus Decoding selects the translationwith minimum Bayes risk (Kumar and Byrne,2004).Handling of UnknownWords comprises incor-poration of proper names from Wiki Headlinesparallel data provided by CMU1and translitera-tion.
We improve the transliteration algorithm inSection 2.4.Note that unlike last year we do not use wordalignments computed for the lemmatized wordforms.2.2 Language ModelWe use 5-gram unpruned language model withmodified Kneser-Ney discount estimated withKenLM toolkit (Heafield et al., 2013).2.3 Word alignmentWord alignments are generated using thefast_align tool (Dyer et al., 2013), which is muchfaster than IBM Model 4 from MGIZA++ (Gaoand Vogel, 2008) and outperforms the latter interms of BLEU.
Results are given in Table 1.2.4 TransliterationWe employ machine transliteration to generate ad-ditional translation options for out-of-vocabulary1http://www.statmt.org/wmt14/wiki-titles.tgz66MGIZA++ fast_alignRun Time 22 h 14 m 2h 49 mPerplexity?
ru?en 97.00 90.37?
en?ru 209.36 216.71BLEU?
WMT13 25.27 25.49?
WMT14 31.76 31.92Table 1: Comparison of word alignment tools:MGIZA++ vs. fast_align.
fast_align runs tentimes as fast and outperforms the IBM Model 4from MGIZA++ in terms of BLEU scores.words.
The transformation model we use is atransfeme based model (Duan and Hsu, 2011),which is analogous to translation model in phrase-based machine translation.
Transformation units,or transfemes, are trained with Moses using thedefault settings.
Decoding is very similar to beamsearch.
We build a trie from the words in Englishmonolingual corpus, and search in it, based on thetransformation model.2.5 Operation Sequence ModelThe Operation Sequence N-gram Model (OSM)(Durrani et al., 2011) integrates reordering opera-tions and lexical translations into a heterogeneoussequence of minimal translation units (MTUs) andlearns a Markov model over it.
Reordering deci-sions influence lexical selections and vice versathus improving the translation model.
We useOSM as a feature function in phrase-based SMT.Please, refer to (Durrani et al., 2013) for imple-mentation details.3 Morphological TransformationsRussian is a fusional synthetic language, mean-ing that the relations between words are redundantand encoded inside the words.
Adjectives altertheir form to reflect the gender, case, number andin some cases, animacy of the nouns, resulting indozens of different word forms matching a singleEnglish word.
An example is given in Table 2.Verbs in Russian are typically constructed fromthe morphemes corresponding to functional wordsin English (to, shall, will, was, were, has, have,had, been, etc.).
This Russian phenomenon leadsto two problems: data sparsity and high number ofone-to-many alignments, which both may result intranslation quality degradation.NumberSG PLCase GenderNOM MASC ?????
?NOM FEM ??????
?????
?NOM NEUT ?????
?GEN MASC ??????
?GEN FEM ??????
?????
?GEN NEUT ??????
?DAT MASC ??????
?DAT FEM ??????
?????
?DAT NEUT ??????
?ACC MASC, AN ??????
?ACC MASC, INAN ??????
?????
?ACC FEM ?????
?ACC NEUT ?????
?INS MASC ?????
?INS FEM ??????
?????
?INS FEM ?????
?INS NEUT ?????
?ABL MASC ?????
?ABL FEM ??????
?????
?ABL NEUT ?????
?Table 2: Russian word forms corresponding to theEnglish word "summer" (adj.
).Hereafter, we propose an algorithm to transformthe original Russian sentence into a more easilytranslatable form.
The algorithm is based on thelinguistic intuition of native Russian speakers, alsofluent in English.3.1 ApproachBased on the output from Russian morphologicalanalyzer we rewrite the input sentence based onthe following principles:1. the original sentence is restorable(by a Russian native speaker)2. redundant information is omitted3.
word alignment is less ambiguous3.2 AlgorithmThe algorithm consists of two steps.On the first step we employ in-house Rus-sian morphological analyzer similar to Mys-tem (Segalovich, 2003) to convert each word(WORD) into a tuple containing its canonical form(LEMMA), part of speech tag (POS) and a set67Category Abbr.
ValuesAnimacy ANIM AN, INANAspect ASP IMPERF, PERFCase CASE NOM, GEN, DAT, ACC, INS, ABLComparison Type COMP COMP, SURPGender GEND MASC, FEM, NEUTMood MOOD IND, IMP, COND, SBJVNumber NUM SG, PLParticiple Type PART ACT, PASSPerson PERS PERS1, PERS2, PERS3Tense TNS PRES, NPST, PSTTable 3: Morphological Categoriesof other grammemes associated with the word(GRAMMEMES).
The tuple is later referred to asLPG.
If the canonical form or part of speech areambiguous, we set LEMMA to WORD; POS to"undefined"; and GRAMMEMES to ?.
Gram-memes are grouped into grammatical categorieslisted in Table 3.WORD ??
LEMMA + POS + GRAMMEMESOn the second step, the LPGs are converted intotokens that, we hope, will better match Englishstructure.
Some grammemes result in separate to-kens, others stay with the lemma, and the rest getdropped.
The full set of morphological transfor-mations we use is given in Table 4.An example of applying the algorithm to a Rus-sian sentence is given in Figure 1.3.3 ResultsThe translation has been improved in severalways:Incorrect Use of Tenses happens quite often instatistical machine translation, which is especiallyvexing in simple cases such as asks instead ofasked, explains instead of explain along with moredifficult ones e.g.
has increased instead of wouldincrease.
The proposed algorithm achieves con-siderable improvement, since it explicitly modelstenses and all its relevant properties.Missing Articles is a common problem ofmost Russian-English translation systems, be-cause there are no articles in Russian.
Our modelcreates an auxiliary token for each noun, which re-flects its case and motivates an article.Use of Simple Vocabulary is not desirablewhen the source text is a vocabulary-flourished??????
??????????.adj+?
ins  ???
?.n+sgon a summer day?????
?, adj,{inan, dat|ins, ?, male|neut, sg|pl}???
?, ?noun,{inan, ins, male, sg}Figure 1: An illustration of the proposed algorithmto transform Russian sentence ??????
????
(let-nim dnem), meaning on a summer day, into a moreeasily translatable form.
First, for each word weextract its canonical form, part of speech tag and aset of associated morphological properties (gram-memes).
Then we apply hand-crafted rules (Ta-ble 4) to transform them into separate tokens.one.
News are full of academic, bookish, inkhorn,and other rare words.
Phrase Table smoothingmethods discount the translation probabilities forrare phrase pairs, preventing them from appearingin English translation, while many of these rarephrase pairs are correct.
The good thing is that thephrase pairs containing the transformed Russianwords may not be rare themselves, and thereby arenot discounted so heavily.
A more effective use ofEnglish vocabulary has been observed on WMT13test dataset (see Table 5).We have demonstrated the improvements on aqualitative level.
The quantitative results are sum-marized in Table 6 (baseline ?
without morpholog-ical transformations; proposed ?
with morpholog-ical transformations).68LPG?
tokensLEMMA, adj,{ANIM, CASE, COMP, GEND, NUM}?LEMMA.adj+COMPLEMMA, noun,{ANIM, CASE, GEND, NUM}?CASE LEMMA.n+NUMLEMMA, verb (ger), {ASP, TNS}?LEMMA.vg+ASP+TNSLEMMA, verb (inf), {ASP}?LEMMA.vi+ASPLEMMA, verb (part), {PART, ASP, TNS}?LEMMA.vp+PART+ASP+TNSLEMMA, verb (?
),{PART, ASP, MOOD, TENSE,NUM, PERS}?1.
TNS={PRES} | TNS={NPST} & ASP={IMPERF}a. PERS3 ?
PERS & SG ?
NUMLEMMA.v+pres+MOOD+PERS+NUMb.
otherwiseLEMMA.v+pres+MOOD2.
TNS={PST}ASP LEMMA.v+pst+MOOD3.
TNS={NPST} & ASP={IMPERF}fut LEMMA.v+MOOD4.
if ambiguousLEMMA.v+PART+ASP+MOOD+TNS+NUM+PERSLEMMA, OTHER, GRAMMEMES?LEMMA.POS+GRAMMEMESTable 4: A set of rules we use to transformthe LPGs (LEMMA, POS, GRAMMEMES), ex-tracted on the first step, into individual tokens.4 Discussion and ConclusionWe described the Yandex School of Data Anal-ysis Russian-English system submitted to theACL 2014 Ninth Workshop on Statistical MachineTranslation shared translation task.
The main con-tribution of this work is an algorithm to transformthe Russian sentence into a more easily translat-Input Translation???????????
(a) differences(raznoglasiya) (b) disputes??????????????
(a) promoter(propagandistom) (b) propagandist???????????????
(a) mainly(preimuschestvenno) (b) predominantlyTable 5: Morphological Transformations lead tomore effective use of English vocabulary.
Trans-lations marked with "a" were produced using thebaseline system; with "b" also use MorphologicalTransformations.Baseline ProposedDistinct Words 899,992 564,354OOV Words?
WMT13 829 590?
WMT14 884 660Perplexity?
ru?en 90.37 99.81?
en?ru 216.71 128.15BLEU?
WMT13 25.49 25.63?
WMT14 31.92 32.56Table 6: Results of Morphological Transforma-tions.
We improved the statistical characteristicsof our models by reducing the number of distinctwords by 37% and managed to translate 25% ofpreviously untranslated words.
BLEU scores wereimproved by 0.14 and 0.64 points for WMT13 andWMT14 test sets respectively.able form before decoding.
Significant improve-ments in human satisfaction and BLEU scoreshave been demonstrated from applying this algo-rithm.One limitation of the proposed algorithm is thatit does not take into account the relations betweenwords sharing the same root.
E.g.
the word ?????-???
(aistinyh) meaning stork (adj.)
is handled in-dependently from the word ????
(aist) meaningstork (n.).
Our system as well as the major onlineservices (Bing, Google, Yandex) transliterated thisword, but the word aistinyh does not make muchsense to a non-Russian reader.
It might be worth-while to study this problem in more detail.Another direction for future work is to applythe proposed algorithm in reverse direction.
Wesuggest the following two-step procedure.
English69sentence is first translated into Russian?
(Russianafter applying the morphological transformations),and at the next step it is translated again with anauxiliary SMT system trained on the (Russian*,Russian) parallel corpus created from the Russianmonolingual corpus.ReferencesAlexey Borisov, Jacob Dlougach, and Irina Galinskaya.2013.
Yandex school of data analysis machine trans-lation systems for wmt13.
In Proceedings of theEighth Workshop on Statistical Machine Translation(WMT), pages 97?101.
Association for Computa-tional Linguistics.Huizhong Duan and Bo-June Paul Hsu.
2011.
On-line spelling correction for query completion.
InProceedings of the 20th international conference onWorld Wide Web (WWW), pages 117?126.
ACM.Nadir Durrani, Helmut Schmid, and Alexander Fraser.2011.
A joint sequence translation model with in-tegrated reordering.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 1045?1054.
Associationfor Computational Linguistics.Nadir Durrani, Barry Haddow, Kenneth Heafield, andPhilipp Koehn.
2013.
Edinburgh?s machine trans-lation systems for european language pairs.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation (WMT), pages 112?119.
Associa-tion for Computational Linguistics.Chris Dyer, Victor Chahuneau, and Noah A Smith.2013.
A simple, fast, and effective reparameteriza-tion of IBM model 2.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT-NAACL), pages 644?648.Association for Computational Linguistics.George Foster, Roland Kuhn, and John Howard John-son.
2006.
Phrasetable smoothing for statistical ma-chine translation.
In Proceedings of the 44th AnnualMeeting of the Association for Computational Lin-guistics (ACL), pages 53?61.
Association for Com-putational Linguistics.Qin Gao and Stephan Vogel.
2008.
Parallel imple-mentations of word alignment tool.
In Proceedingsof the 46th Annual Meeting of the Association forComputational Linguistics (ACL), pages 49?57.
As-sociation for Computational Linguistics.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable modi-fied Kneser-Ney language model estimation.
In Pro-ceedings of the 51st Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages690?696, Sofia, Bulgaria, August.
Association forComputational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-burch, Richard Zens, Rwth Aachen,Alexandra Constantin, Marcello Federico, NicolaBertoldi, Chris Dyer, Brooke Cowan, Wade Shen,Christine Moran, and Ond?rej Bojar.
2007.
Moses:Open source toolkit for statistical machine trans-lation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguis-tics (ACL), pages 177?180.
Association for Compu-tational Linguistics.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine transla-tion.
In Proceedings of the Human Language Tech-nology Conference of the North American Chap-ter of the Association for Computational Linguis-tics (HLT-NAACL), pages 163?171.
Association forComputational Linguistics.Ilya Segalovich.
2003.
A fast morphological algorithmwith unknown word guessing induced by a dictio-nary for a web search engine.
In Hamid R. Arab-nia and Elena B. Kozerenko, editors, Proceedings ofthe International Conference on Machine Learning;Models, Technologies and Applications (MLMTA),pages 273?280, Las Vegas, NV, USA, June.
CSREAPress.70
