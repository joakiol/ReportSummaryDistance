Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 486?498,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsAugmenting String-to-Tree and Tree-to-String Translation withNon-Syntactic PhrasesMatthias Huck and Hieu Hoang and Philipp KoehnSchool of InformaticsUniversity of Edinburgh10 Crichton StreetEdinburgh EH8 9AB, UK{mhuck,hhoang,pkoehn}@inf.ed.ac.ukAbstractWe present an effective technique to easilyaugment GHKM-style syntax-based ma-chine translation systems (Galley et al.,2006) with phrase pairs that do not complywith any syntactic well-formedness con-straints.
Non-syntactic phrase pairs aredistinguished from syntactic ones in or-der to avoid harming effects.
We applyour technique in state-of-the-art string-to-tree and tree-to-string setups.
For tree-to-string translation, we furthermore investi-gate novel approaches for translating withsource-syntax GHKM rules in associationwith input tree constraints and input treefeatures.1 IntroductionSyntax-based statistical machine translation sys-tems utilize linguistic information that is obtainedby parsing the training data.
In tree-to-stringtranslation, source-side syntactic tree annotation isemployed, while string-to-tree translation exploitstarget-side syntax.
The syntactic parse tree an-notation constrains phrase extraction to syntacti-cally well-formed phrase pairs: spans of syntacticphrases must match constituents in the parse tree.Standard phrase-based and hierarchical phrase-based statistical machine translation systems, incontrast, allow all phrase pairs that are consistentwith the word alignment (Koehn et al., 2003; Chi-ang, 2005).A restriction of the phrase inventory to syntac-tically well-formed phrase pairs entails that possi-bly valuable information from the training data re-mains disregarded.
While we would expect phrasepairs that are not linguistically motivated to be lessreliable, discarding them altogether might be anoverly harsh decision.
The quality of an inventoryof syntactic phrases depends heavily on the treeannotation scheme and the quality of the syntac-tic parses of the training data.
Phrase pairs thatdo not span constituents in the tree annotation ob-tained from syntactic parses can provide reason-able alternative segmentations or alternative trans-lation options which prove to be valuable to thedecoder.In this work, we augment the phrase invento-ries of string-to-tree and tree-to-string translationsystems with phrase pairs that are not induced inthe syntax-based extraction.
We extract continu-ous phrases that are consistent with the word align-ment, without enforcing any constraints with re-spect to syntactic tree annotation.
Non-syntacticphrases are added as rules to the baseline syntacticgrammar with a fill-up technique.
New rules areonly added if their right-hand side does not existyet.
We extend the glue grammar with a specialglue rule to allow for application of non-syntacticphrases during decoding.
A feature in the log-linear model combination serves to distinguishnon-syntactic phrases from syntactic ones.
Duringdecoding, the decoder can draw on both syntacticand non-syntactic phrase table entries and producederivations which resort to both types of phrases.Such derivations yield hypotheses that make use ofthe alternative segmentations and translation op-tions provided through non-syntactic phrases.
Thesearch space is more diverse, and in some casesall hypotheses from purely syntax-based deriva-tions score worse than a translation that appliesone or more non-syntactic phrases.
We empiri-cally demonstrate that this technique can lead tosubstantial gains in translation quality.Our syntactic translation models conform to theGHKM syntax approach as proposed by Galley,Hopkins, Knight, and Marcu (Galley et al.,2004) with composed rules as in (Galley et al.,2006) and (DeNeefe et al., 2007).
State-of-the-art GHKM string-to-tree systems have recentlyshown very competitive performance in public486evaluation campaigns (Nadejde et al., 2013; Bo-jar et al., 2013).
We apply the GHKM approachnot only in a string-to-tree setting as in previouswork, but employ it to build tree-to-string sys-tems as well.
We conduct tree-to-string translationwith text input and additionally adopt translationwith tree input and input tree constraints as sug-gested for hierarchical translation by Hoang andKoehn (2010).
We also implement translation withtree input and feature-driven soft tree matching.The effect of augmenting the systems with non-syntactic phrases is evaluated for all variants.2 OutlineThe remainder of the paper is structured as fol-lows: We review some of the basics of syntax-based translation in the next section (Section 3)and sketch the characteristics of our GHKMstring-to-tree and tree-to-string translation frame-works.In Section 4, we describe our technique toaugment GHKM-style syntax-based systems withphrase pairs that do not comply with any syntacticwell-formedness constraints.Section 5 contains the empirical part of the pa-per.
We first describe our experimental setup (5.1),followed by a presentation of the translation re-sults (5.2).
We also include a few translation ex-amples (5.3) in order to illustrate the differencesbetween the syntax-based baseline systems andthe setups augmented with non-syntactic phrases.The empirical part is concluded with a brief dis-cussion (5.4).In the final part of the paper (Section 6), wegive a survey of previous work that has dealtwith problems related to overly restrictive syntac-tic grammars for statistical machine translation,inadequate syntactic parses, and insufficient cov-erage of syntactic phrase inventories.
A broadspectrum of diverse methods has been proposed inthe literature, many of which are quite dissimilarfrom ours but nevertheless related.
We concludethe paper in Section 7.3 Syntax-based TranslationIn syntax-based translation, a probabilistic syn-chronous context-free grammar (SCFG) is in-duced from bilingual training corpora.
The par-allel training data is word-aligned and annotatedwith syntactic parses on either target side (string-to-tree), source side (tree-to-string), or both (tree-to-tree).
A syntactic phrase extraction procedureextracts rules which are consistent with the word-alignment and conform with certain syntactic va-lidity constraints.Extracted rules are of the form A,B???,?
,?
?.The right-hand side of the rule ??,?
?
is a bilingualphrase pair that may contain non-terminal sym-bols, i.e.
?
?
(VF?
NF)+and ?
?
(VE?
NE)+,where VFand VEdenote the source and targetterminal vocabulary, and NFand NEdenote thesource and target non-terminal vocabulary, respec-tively.
The non-terminals on the source side andon the target side of rules are linked in a one-to-one correspondence.
The?relation defines thisone-to-one correspondence.
The left-hand sideof the rule is a pair of source and target non-terminals, A ?
NFand B ?
NE.Decoding is typically carried out with a parsing-based algorithm, in our case a customized versionof CYK+(Chappelier and Rajman, 1998).
Theparsing algorithm is extended to handle transla-tion candidates and to incorporate language modelscores via cube pruning (Chiang, 2007).3.1 GHKM String-to-Tree TranslationIn GHKM string-to-tree translation (Galley et al.,2004; Galley et al., 2006; DeNeefe et al., 2007),rules are extracted from training instances whichconsist of a source sentence, a target sentencealong with its constituent parse tree, and a wordalignment matrix.
This tuple is interpreted as adirected graph (the alignment graph), with edgespointing away from the root of the tree, and wordalignment links being edges as well.
A set ofnodes (the frontier set) is determined that con-tains only nodes with non-overlapping closure oftheir spans.1By computing frontier graph frag-ments?fragments of the alignment graph suchthat their root and all sinks are in the frontier set?the GHKM extractor is able to induce a minimalset of rules which explain the training instance.The internal tree structure can be discarded to ob-tain flat SCFG rules.
Minimal rules can be assem-bled to build larger composed rules.Non-terminals on target sides of string-to-treerules are syntactified.
The target non-terminal vo-cabulary of the SCFG contains the set of labelsof the frontier nodes, which is in turn a subset1The span of a node in the alignment graph is definedas the set of source-side words that are reachable from thisnode.
The closure of a span is the smallest interval of sourcesentence positions that covers the span.487TOPPUNC..CS-TOPS-TOPNP-OANNAutonomieADJApolitischeARTdieADVauchVMFINwolltenNP-SBPPERsiePUNC,,S-TOPADV.
.
.leiderunfortunately , .
.
.
, they also wanted political autonomy .Figure 1: Word-aligned training sentence pair with target-side syntactic annotation.of (or equal to) the set of constituent labels inthe parse tree.
It furthermore contains an initialnon-terminal symbol Q.
Source sides of the rulesare not decorated with syntactic annotation.
Thesource non-terminal vocabulary contains a singlegeneric non-terminal symbol X.In addition to the extracted grammar, the trans-lation system makes use of a special glue grammarwith an initial rule, glue rules, a final rule, and toprules.
The glue rules provide a fall back methodto just monotonically concatenate partial deriva-tions during decoding.
As we add tokens whichmark the sentence start (?<s>?)
and the sentenceend (?</s>?
), the rules in the glue grammar are ofthe following form:Initial rule:X,Q?
?<s> X?0,<s> Q?0?Glue rules:X,Q?
?X?0X?1,Q?0B?1?for all B ?
NEFinal rule:X,Q?
?X?0</s>,Q?0</s>?Top rules:X,Q?
?<s> X?0</s>,<s> B?0</s>?for all B ?
NE3.2 GHKM Tree-to-String TranslationThe described techniques for GHKM string-to-tree translation can be adjusted for tree-to-stringtranslation in a straightforward manner.
Rules areextracted from training instances which consist ofa source sentence along with its constituent parsetree, a target sentence, and a word alignment ma-trix.
We omit the details.For GHKM tree-to-string translation, we inves-tigate three decoding variants:Tree-to-string translation with text input.
Thedecoder can construct any source-side syn-tactic analysis that the grammar permits, verysimilar to string-to-tree translation.Tree-to-string translation with tree input andinput tree constraints.
Syntactic annotationover the input data is provided to the decoder.The source-side syntactic non-terminals of atree-to-string translation rule need to matchthe constituent span in the input sentence,otherwise the rule cannot be applied.
Thisvariant follows the method that was sug-gested for hierarchical translation by Hoangand Koehn (2010).Tree-to-string translation with tree input andinput tree features.
Syntactic annotationover the input data is provided to the decoder.No hard matching constraints are imposed,but the decoder is informed about matchesand mismatches of the syntactic annotation inthe rules and in the input tree.
It takes theminto account for the score computation.4 Non-Syntactic Phrases for GHKMTranslationThe syntactic constraints in GHKM extraction canunfortunately prevent useful phrase pairs from be-ing included in the phrase inventory.
Consider theexample in Figure 1: the highlighted phrase pair?also wanted,wollten auch?
cannot be extractedfrom this training instance for string-to-tree trans-lation.488In the standard phrase-based approach, in con-trast, all continuous phrases that are consistentwith the word alignment are extracted (Och et al.,1999; Och, 2002).
The set of continuous bilingualphrases BP( fJ1,eI1,A), given a training instancecomprising a source sentence fJ1, a target sentenceeI1, and a word alignment A?
{1, ..., I}?
{1, ...,J},is defined as follows:BP( fJ1,eI1,A) ={?
fj2j1,ei2i1?
: ?
(i, j) ?
A : i1?
i?
i2?
j1?
j ?
j2??
(i, j) ?
A : i1?
i?
i2?
j1?
j ?
j2}Consistency for continuous phrases is based uponmerely two constraints in this definition: (1.)
Atleast one source and target position within thephrase must be aligned, and (2.)
words from insidethe source phrase may only be aligned to wordsfrom inside the target phrase and vice versa.
Thehighlighted phrase pair from the example does notviolate these constraints.In order to augment our GHKM syntax-basedsystems with non-syntactic phrases, we obey thefollowing procedure:?
The setBP is extracted from all training in-stances, and phrase translation probabilitiesare computed separately from those in thesyntactic phrase inventory.?
Non-syntactic phrases are converted to rulesby providing a special left-hand side non-terminal X.?
A phrase table fill-up method is applied toenhance the syntactic phrase inventory withentries from the non-syntactic phrase inven-tory.
Non-syntactic rules are only added tothe final grammar if no syntactic rule withthe same (source and target) right-hand sideis present.
This method is inspired by pre-vious work in domain adaptation (Bisazza etal., 2011).?
The glue grammar is extended with a newglue ruleX,Q?
?X?0X?1,Q?0X?1?that enables the system to make use of non-syntactic rules in decoding.?
A binary feature is added to the log-linearmodel (Och and Ney, 2002) to distinguishnon-syntactic rules from syntactic ones, andto be able to assign a tuned weight to the non-syntactic part of the grammar.5 Empirical EvaluationWe evaluate the effect of augmenting GHKMsyntax-based translation systems?both string-to-tree and tree-to-string?with non-syntactic phrasepairs on the English?German language pair usingthe standard newstest sets of the Workshop on Sta-tistical Machine Translation (WMT) for testing.2The experiments are conducted with the open-source Moses implementations of GHKM rule ex-traction (Williams and Koehn, 2012) and decodingwith CYK+parsing and cube pruning (Hoang et al.,2009).5.1 Experimental SetupWe work with an English?German parallel train-ing corpus of around 4.5 M sentence pairs (af-ter corpus cleaning).
The parallel data origi-nates from three different sources which havebeen eligible for the constrained track of theACL 2014 Ninth Workshop on Statistical Ma-chine Translation shared translation task: Europarl(Koehn, 2005), News Commentary, and the Com-mon Crawl corpus as provided on the WMT web-site.
Word alignments are created by aligning thedata in both directions with MGIZA++(Gao andVogel, 2008) and symmetrizing the two trainedalignments (Och and Ney, 2003; Koehn et al.,2003).
For string-to-tree translation, we parse theGerman target side with BitPar (Schmid, 2004).3For tree-to-string translation, we parse the Englishsource side of the parallel data with the EnglishBerkeley Parser (Petrov et al., 2006).When extracting syntactic phrases, we imposeseveral restrictions for composed rules, in partic-ular a maximum number of twenty tree nodes perrule, a maximum depth of five, and a maximumsize of five.
We discard rules with non-terminalson their right-hand side if they are singletons in thetraining data.Only the 100 best translation options per dis-tinct source side with respect to the weightedphrase-level model scores are loaded by the de-coder.
The decoder is configured with a maximumchart span of 25 and a rule limit of 100.A standard set of models is used in the base-lines, comprising phrase translation probabilitiesand lexical translation probabilities in both direc-2http://www.statmt.org/wmt14/translation-task.html3We remove grammatical case and function informationfrom the annotation obtained with BitPar.489system dev newstest2013 newstest2014BLEU TER BLEU TER BLEU TERphrase-based 33.0 48.8 18.8 64.5 18.2 66.9+ lexicalized reordering 34.2 48.1 19.2 64.5 18.3 67.1string-to-string (syntax-directed extraction) 32.6 49.4 18.2}+0.565.4}?0.417.8}+0.568.0}?0.4+ non-syntactic phrases 33.4 49.0 18.7 65.0 18.3 67.6string-to-tree 33.6 48.7 19.5}+0.363.9}?0.318.6}+0.566.9}?0.7+ non-syntactic phrases 34.3 48.0 19.8 63.6 19.1 66.2tree-to-string 34.0 48.5 19.5}?0.263.8}+0.218.5}+0.267.0}?0.4+ non-syntactic phrases 33.9 48.4 19.3 64.0 18.7 66.6+ input tree constraints 33.7 48.4 19.3}+0.463.9}?0.318.3}+0.367.0}?0.5+ non-syntactic phrases 34.2 48.2 19.7 63.6 18.7 66.5+ input tree features 34.3 48.3 19.6}+0.363.7}?0.318.6}+0.267.0}?0.5+ non-syntactic phrases 34.4 48.1 19.9 63.4 18.8 66.5Table 1: English?German experimental results (truecase).
BLEU scores are given in percentage.tions, word and phrase penalty, an n-gram lan-guage model, a rule rareness penalty, and themonolingual PCFG probability of the tree frag-ment from which the rule was extracted (Williamset al., 2014).
Phrase translation probabilities aresmoothed via Good-Turing smoothing.The language model (LM) is a large inter-polated 5-gram LM with modified Kneser-Neysmoothing (Kneser and Ney, 1995; Chen andGoodman, 1998).
The target side of the parallelcorpus and the monolingual German News Crawlcorpora are employed as training data.
We usethe SRILM toolkit (Stolcke, 2002) to train the LMand rely on KenLM (Heafield, 2011) for languagemodel scoring during decoding.Model weights are optimized to maximizeBLEU (Papineni et al., 2002) with batch MIRA(Cherry and Foster, 2012) on 1000-best lists.
Weselected 2000 sentences from the newstest2008-2012 sets as a development set.
The selected sen-tences obtained high sentence-level BLEU scoreswhen being translated with a baseline phrase-based system, and do each contain less than30 words for more rapid tuning.
newstest2013 andnewstest2014 are used as unseen test sets.
Trans-lation quality is measured in truecase with BLEUand TER (Snover et al., 2006).4We apply a phrase length limit of five whenextracting non-syntactic phrases for the fill-up ofsyntactic phrase tables.4TER scores are computed with tercom version 0.7.25and parameters -N -s.5.2 Translation ResultsTable 1 comprises the results of our empirical eval-uation of the translation quality achieved by thedifferent systems.5.2.1 Phrase-based BaselinesWe set up two phrase-based baselines for com-parison.
Their set of models is the same as forthe syntax-based baselines, with the exception ofthe PCFG probability.
One of the phrase-basedsystems moreover utilizes a lexicalized reorder-ing model (Galley and Manning, 2008).
No non-standard advanced features (like an operation se-quence model or class-based LMs) are engrafted.The maximum phrase length is five, search is car-ried out with cube pruning at a k-best limit of1000.
A maximum number of 100 translation op-tions per source side are taken into account.5.2.2 String-to-String Contrastive SystemA further contrastive experiment is done with astring-to-string system.
The extraction methodfor this string-to-string system is GHKM syntax-directed with syntactic target-side annotation fromBitPar, as in the string-to-tree setup.
We actuallyextract the same rules but strip off the syntactic la-bels.
The final grammar contains rules with a sin-gle generic non-terminal instead of syntactic ones.Note that a side effect of this is that the phraseinventory of the string-to-string system contains490a larger amount of hierarchical phrases5than thestring-to-tree system, though the same rules areextracted.
The reason is that we discard single-ton hierarchical rules when we normalize the fre-quencies after extraction.
Many rules that are sin-gletons when the syntax decoration is taken intoaccount have in fact been seen multiple times ifsyntactic labels are not distinguished, due to pool-ing of counts.The string-to-string system is on newstest20131.0 points BLEU worse than the phrase-basedsystem with lexicalized reordering and on news-test2014 0.5 points BLEU.
We gain 0.5 pointsBLEU on both of the test sets if we augment thestring-to-string system with non-syntactic phrasesfrom the standard phrase-based extractor accord-ing to our procedure from Section 4.5.2.3 String-to-Tree SystemThe translation quality of the string-to-tree sys-tem surpasses the translation quality of the bet-ter phrase-based baseline slightly (by 0.3 pointsBLEU on both test sets).
The string-to-tree systemis clearly superior to the string-to-string system,which verifies that syntactic non-terminals are in-deed vital.
We get a nice gain of 0.5 points BLEUand 0.7 points TER on newstest2014 if we aug-ment the string-to-tree system with non-syntacticphrases.
The phrase-based system is outperformedby 0.8 points BLEU.5.2.4 Tree-to-String SystemsThe tree-to-string baseline with text input per-forms at the level of the string-to-tree baseline, butaugmenting it with non-syntactic phrases yieldsonly a small improvement or even harms a little(on newstest2013).Decoding with tree input and input tree con-straints causes a minor loss in translation qual-ity.
We however observed a decoding speed-up.
Ifwe employ non-syntactic phrases to augment thetree-to-string setup with input tree constraints, weprovide the new non-syntactic rules in the gram-mar with a particular property: their left-hand sidenon-terminal X can match any constituent span inthe input sentence.
The decoder would not beable to utilize non-syntactic phrases without thisrelaxation.
Syntactic phrases amount to an in-crease of up to 0.4 points BLEU (newstest2013)5We define hierarchical phrases as rules with non-terminals on their right-hand side, in contrast to lexicalphrases which are continuous rules with right-hand sides thatcontain terminal symbols only.and 0.5 points TER (newstest2014) in the tree-constrained setup.Our best tree-to-string setup takes tree input, butinvolves soft matching features instead of hard in-put tree constraints.
We incorporate two features,one that fires for matches and another one that firesfor mismatches.
The motivation for not relying onjust one feature which would penalize mismatchesis that the number of syntactic non-terminals inthe derivation can differ between hypotheses.
Notall constituent spans need to be matched (or mis-matched) by non-terminals, some can be over-laid through larger rules.6Tree-to-string transla-tion with input tree features benefits from beingaugmented with non-syntactic phrases by 0.2 to0.3 points BLEU.
The resulting system is mini-mally better than the best string-to-tree system onnewstest2013, and slightly worse than it on news-test2014.5.3 Translation ExamplesWe illustrate the differences between the syntax-based baseline systems and the setups augmentedwith non-syntactic phrases by means of two trans-lation examples from newstest2014.
Both exam-ples are string-to-tree translations.Figures 2 and 3 depict an example that cor-responds well to the word-aligned training sen-tence pair with target-side syntactic annotationfrom Figure 1.
Figure 2 shows the translation, seg-mentation, and parse tree derived by the string-to-tree baseline system as single-best output forthe preprocessed input sentence: ?the lessees wereagainst this and also wanted longer terms .?
Thereference translation is: ?Die P?chter waren dage-gen und wollten zudem l?ngere Laufzeiten.?
Fig-ure 3 shows the translation, segmentation, andparse tree derived by the string-to-tree system aug-mented with non-syntactic phrases.
There aretwo word substitutions with respect to the ref-erence in the latter translation, but they conveythe same meaning.
The baseline translation failsto convey the meaning, mostly because ?terms?is translated to the verb ?gesehen?, which is awrong syntactic analysis in the given context.
In-terestingly, the segmentation applied by the twosystems is rather similar, apart from the interval?also wanted?
which cannot be translated en blocby the baseline.
All rules in the baseline gram-6Also remember that we discarded the internal tree struc-ture to obtain flat SCFG rules.491Q</s>QTOP.VP-OCVVPPgesehenmehrADVauchVMFINwollteundS-TOPdagegenVAFINwarenNP-SBMieterQARTdieQ<s><s> the lessees were against this and also wanted longer terms .
</s>Reference: Die P?chter waren dagegen und wollten zudem l?ngere Laufzeiten.Figure 2: Translation and parse tree from the string-to-tree system.Q</s>QPUNC..QNP-OANNLaufzeitenl?ngereQXauchwolltenQKONundQS-TOPdagegenVAFINwarenNP-SBMieterQARTdieQ<s><s> the lessees were against this and also wanted longer terms .
</s>Reference: Die P?chter waren dagegen und wollten zudem l?ngere Laufzeiten.Figure 3: Translation and parse tree from the string-to-tree system augmented with non-syntactic phrases.mar that contain ?also wanted?
as part of theirsource side imply a larger source-side lexical con-text that is not present in the given sentence.
Noneof those rules matches the input.
The baselinehas to translate ?also?
and ?wanted?
separatelyand fails to translate the verb to a plural formGerman verb.
The next rule in bottom-up orderis already involved in the incorrect choice of averb for ?terms?.
The string-to-tree system aug-mented with non-syntactic phrases applies moreglue rules, but this is beneficial in the presentexample, as it breaks apart the faulty syntacticderivation.Figures 4 and 5 depict a second example.
Com-pared to the baseline, filling up the phrase tablewith non-syntactic phrases had the effect of disas-sembling the originally nicely built syntactic treestructure over the translation nearly completely.Four non-syntactic phrases are applied, three ofthem span over target-side punctuation marks.
Thebaseline translation is more literal and conveysthe meaning, but the system augmented with non-syntactic phrases produces a more fluent output.Its translation seems more natural and happens tomatch the reference in this case.492Q</s>TOP.S-TOPAP-PDbeeindruckendist,NP-SBS-RCVVFINspieltNP-SBTeamdasderin,WeiseundArtdie,S-TOPallenvonAA-MOmeistenam<s><s> most of all , the manner in which the team is playing is impressive .
</s>Reference: Vor allem die Art und Weise, wie die Mannschaft spielt, ist beeindruckend.Figure 4: Translation and parse tree from the string-to-tree system.Q</s>QX.beeindruckendistQX,spieltQNP-SBMannschaftdieQXwie,WeiseundArtQARTdieQXallemvorQ<s><s> most of all , the manner in which the team is playing is impressive .
</s>Reference: Vor allem die Art und Weise, wie die Mannschaft spielt, ist beeindruckend.Figure 5: Translation and parse tree from the string-to-tree system augmented with non-syntactic phrases.phrase table entries unfiltered dev newstest2013 newstest2014hier.
lexical hier.
lexical hier.
lexical hier.
lexicalphrase-based ?
184.9 M ?
25.3 M ?
29.0 M ?
28.0 Mstring-to-string 58.3 M 19.9 M 4.3 M 2.9 M 5.7 M 3.3 M 5.3 M 3.3 M+ non-syntactic phrases 58.3 M 191.1 M 4.3.
M 25.4 M 5.7 M 29.1 M 5.3 M 28.1 Mstring-to-tree 39.7 M 21.2 M 4.9 M 3.4 M 5.7 M 3.8 M 5.5 M 3.7 M+ non-syntactic phrases 39.7 M 192.4 M 4.9 M 25.8 M 5.7 M 29.6 M 5.5 M 28.6 Mtree-to-string 29.5 M 21.1 M 7.7 M 2.8 M 9.0 M 3.3 M 8.7 M 3.2 M+ non-syntactic phrases 29.5 M 192.6 M 7.7 M 26.1 M 9.0 M 29.9 M 8.7 M 28.9 MTable 2: Phrase inventory statistics for the different English?German translation systems.
?hier.?
de-notes hierarchical phrases, i.e.
rules with non-terminals on their right-hand side, ?lexical?
denotes con-tinuous phrases.4935.4 DiscussionA drawback of our method is that it increasesthe size of the synchronous context-free gram-mar massively.
Most phrase pairs from standardphrase-based extraction are actually not present inthe GHKM rule set, even with composed rules.A large fraction of the extracted non-syntacticphrases is such added to the phrase inventorythrough phrase table fill-up.
Table 2 shows thephrase inventory statistics for the different sys-tems.Another question relates to the glue rule appli-cations.
The application of a non-syntactic ruleis always accompanied with a respective glue ruleapplication in our implementation.
The string-to-tree baseline utilizes glue rules on average 3.0times in each single-best translation (measuredon newstest2014), the string-to-tree system aug-mented with non-syntactic phrases utilizes gluerules on average 7.0 times.
We considered an im-plementation that allows for embedding of non-syntactic rules into hierarchical rules (other thanthe glue rules) but did not see improvements withit as yet.
Furthermore, efficiency concerns becomemore relevant in such an implementation.6 Related WorkIssues with overly restrictive syntactic grammarsfor statistical machine translation, inadequate syn-tactic parses, and insufficient coverage have beentackled from several different directions in the lit-erature.A proposed approach to attain better syntac-tic phrase inventories is to restructure the syntac-tic parse trees in a preprocessing step (Wang etal., 2007; Wang et al., 2010; Burkett and Klein,2012).
This line of research aims at rearrangingparse trees in a way that makes them a better fitfor the requirements of the bilingual downstreamapplication.
Conversely, Fossum et al.
(2008) re-tain the structure of the parse trees and modify theword alignments.Marcu et al.
(2006) relax syntactic phrase ex-traction constraints in their SPMT Model 2 to al-low for phrases that do not match the span of onesingle constituent in the parse tree.
SPMT Model 2rules are created from spans that are consistentwith the word alignment and covered by multipleconstituents such that the union of the constituentsmatches the span.
Pseudo non-syntactic non-terminals are introduced for the left-hand sides ofSPMT Model 2 rules.
Special additional rules al-low for combination of those non-syntactic left-hand side non-terminals with genuine syntacticnon-terminals on the right-hand sides of otherrules during decoding.Another line of research took the hierarchicalphrase-based model (Chiang, 2005; Chiang, 2007)as a starting point and extended it with syntacticenhancements.
In their SAMT system, Zollmannand Venugopal (2006) labeled the non-terminalsof the hierarchical model with composite symbolsderived from the syntactic tree annotation.
Similarmethods have been applied with CCG labels (Al-maghout et al., 2012).
Venugopal et al.
(2009)and Stein et al.
(2010) keep the grammar of thenon-terminals of the hierarchical model unlabeledand apply the syntactic information in a separatemodel.
Other authors added features which firefor phrases complying with certain syntactic prop-erties while retaining all phrase pairs of the hier-archical model (Marton and Resnik, 2008; Vilar etal., 2008).In a tree-to-tree translation setting, Chiang(2010) proposed techniques to soften the syntac-tic constraints.
A fuzzy approach with complexnon-terminal symbols as in SAMT is employedto overcome the limitations during phrase extrac-tion.
In decoding, substitutions of non-terminalsare not restricted to matching ones.
Any left-hand side non-terminal can substitute any right-hand side non-terminal.
The decoder decides onthe best derivation based on the tuned weights of alarge number of binary features.Joining phrase inventories that come from mul-tiple origins is a common method in domain adap-tation (Bertoldi and Federico, 2009; Niehues andWaibel, 2012) but has also been applied in thecontexts of lightly-supervised training (Schwenk,2008; Huck et al., 2011) and of forced alignmenttraining (Wuebker et al., 2010).
For our purposes,we apply a fill-up method in the manner of the onethat has been shown to perform well for domainadaptation in earlier work (Bisazza et al., 2011).Previous research that resembles our work mosthas been presented by Liu et al.
(2006) and byHanneman and Lavie (2009).Liu et al.
(2006) allow for application of non-syntactic phrase pairs in their tree-to-string align-ment template (TAT) system.
The translationprobabilities for the non-syntactic phrases are ob-tained from a standard phrase-based extraction494pipeline.
A non-syntactic phrase pair can how-ever only be applied if its source side matchesa subtree in the parsed input sentence.
Syn-tactic and non-syntactic phrases are not distin-guished, and overlap between the syntactic andnon-syntactic part of the phrase inventory is notavoided.
The decoder picks the entry with thehigher phrase translation probability, which meansthat non-syntactic phrase table entries can super-sede syntactic entries.
The authors report im-provements of 0.6 points BLEU on the 2005 NISTChinese?English task with four reference trans-lations.Hanneman and Lavie (2009) examine non-syntactic phrases for tree-to-tree translation withthe Stat-XFER framework as developed atCarnegie Mellon University (Lavie, 2008).
Theycombine syntactic and non-syntactic phrase in-ventories and reestimate the probabilities for bothtypes of phrase pairs by adding up the observedabsolute frequencies.
Two combination schemesare evaluated: combination with all extractablevalid non-syntactic phrases (?direct combination?
)and combination with only those non-syntacticphrases whose source sides are not equal to thesource side of any syntactic phrase (?syntax-prioritized combination?).
On a French?Englishtranslation task, Hanneman and Lavie (2009) re-port improvements of around 2.6 points BLEU byadding non-syntactic phrases on top of their Stat-XFER syntactic baselines.
Their best setup how-ever does not reach the performance of a stan-dard phrase-based system, which is still 1.6 pointsBLEU better.Apart from the differences in the underly-ing syntax-based translation technology (string-to-tree/tree-to-string GHKM vs. TAT vs. Stat-XFER), our work also constitutes a novel contri-bution as compared to the previous approaches byLiu et al.
(2006) and Hanneman and Lavie (2009)with respect to the following:?
The phrase inventory is augmented with non-syntactic phrases by means of a fill-up tech-nique.
Overlap is prevented, whereas notonly new source sides, but also new target-side translation options can be added.?
The probabilities of syntactic phrase pairs arethe same as in the syntax-based baseline, andthe probabilities of the non-syntactic phrasepairs are the same as in a phrase-based sys-tem.
Counts of syntactic and non-syntacticphrases are not summed up to obtain new es-timates.?
Non-syntactic phrase pairs are distinguishedfrom syntactic ones with an additional fea-ture.7 ConclusionsString-to-tree and tree-to-string translation sys-tems can easily be augmented with non-syntacticphrases by means of phrase table fill-up, a specialnon-terminal symbol for left-hand sides of non-syntactic rules in the grammar, and an additionalglue rule.
A binary feature enables the system todistinguish non-syntactic phrases from syntacticones and?on the basis of the respective featureweight?to favor syntactically motivated phrasesduring decoding.Our results on an English?German translationtask demonstrate the beneficial effect of augment-ing GHKM translation systems with non-syntacticphrase pairs.
Empirical gains in translation qual-ity are up to 0.5 points BLEU and 0.7 points TERover the baseline on the recent test set of the sharedtranslation task of the ACL 2014 Ninth Workshopon Statistical Machine Translation.While GHKM-style syntactic translation hastypically been utilized in string-to-tree settings inprevious research, we have also adopted it to buildtree-to-string systems in this work.
Source syn-tax establishes interesting further directions forGHKM systems.
We investigated two of them: in-put tree constraints and input tree features.String-to-tree and tree-to-string GHKM sys-tems perform roughly at the same level in termsof translation quality.
Our best string-to-treesetup outperforms a phrase-based baseline by upto 0.8 points BLEU and 0.9 points TER (onnewstest2014), our best tree-to-string setup out-performs the phrase-based baseline by up to0.7 points BLEU and 1.1 points TER (on news-test2013).AcknowledgementsThe research leading to these results has re-ceived funding from the European Union Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreements no287658 (EU-BRIDGE)and no288487 (MosesCore).495ReferencesHala Almaghout, Jie Jiang, and Andy Way.
2012.
Ex-tending CCG-based Syntactic Constraints in Hierar-chical Phrase-Based SMT.
In Proc.
of the AnnualConf.
of the European Assoc.
for Machine Transla-tion (EAMT), pages 193?200, Trento, Italy, May.Nicola Bertoldi and Marcello Federico.
2009.
DomainAdaptation for Statistical Machine Translation withMonolingual Resources.
In Proc.
of the Workshopon Statistical Machine Translation (WMT), pages182?189, Athens, Greece, March.Arianna Bisazza, Nick Ruiz, and Marcello Federico.2011.
Fill-up versus Interpolation Methods forPhrase-based SMT Adaptation.
In Proc.
of theInt.
Workshop on Spoken Language Translation(IWSLT), pages 136?143, San Francisco, CA, USA,December.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut,and Lucia Specia.
2013.
Findings of the 2013Workshop on Statistical Machine Translation.
InProc.
of the Workshop on Statistical Machine Trans-lation (WMT), pages 1?44, Sofia, Bulgaria, August.David Burkett and Dan Klein.
2012.
TransformingTrees to Improve Syntactic Convergence.
In Proc.of the Conf.
on Empirical Methods for Natural Lan-guage Processing (EMNLP), Jeju Island, South Ko-rea, July.Jean-C?dric Chappelier and Martin Rajman.
1998.
AGeneralized CYK Algorithm for Parsing Stochas-tic CFG.
In Proc.
of the First Workshop on Tab-ulation in Parsing and Deduction, pages 133?137,Paris, France, April.Stanley F. Chen and Joshua Goodman.
1998.
AnEmpirical Study of Smoothing Techniques for Lan-guage Modeling.
Technical Report TR-10-98, Com-puter Science Group, Harvard University, Cam-bridge, MA, USA, August.Colin Cherry and George Foster.
2012.
Batch Tun-ing Strategies for Statistical Machine Translation.
InProc.
of the Human Language Technology Conf.
/North American Chapter of the Assoc.
for Compu-tational Linguistics (HLT-NAACL), pages 427?436,Montr?al, Canada, June.David Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In Proc.of the Annual Meeting of the Assoc.
for Computa-tional Linguistics (ACL), pages 263?270, Ann Ar-bor, MI, USA, June.David Chiang.
2007.
Hierarchical Phrase-BasedTranslation.
Computational Linguistics, 33(2):201?228, June.David Chiang.
2010.
Learning to Translate withSource and Target Syntax.
In Proc.
of the AnnualMeeting of the Assoc.
for Computational Linguistics(ACL), pages 1443?1452, Uppsala, Sweden, July.Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What Can Syntax-Based MT Learnfrom Phrase-Based MT?
In Proc.
of the 2007Joint Conf.
on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 755?763,Prague, Czech Republic, June.Victoria Fossum, Kevin Knight, and Steven Abney.2008.
Using Syntax to Improve Word AlignmentPrecision for Syntax-Based Machine Translation.
InProc.
of the Workshop on Statistical Machine Trans-lation (WMT), pages 44?52, Columbus, OH, USA,June.Michel Galley and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase Reorder-ing Model.
In Proc.
of the Conf.
on Empirical Meth-ods for Natural Language Processing (EMNLP),pages 847?855, Honolulu, HI, USA, October.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proc.
of the Human Language Technology Conf./ North American Chapter of the Assoc.
for Compu-tational Linguistics (HLT-NAACL), pages 273?280,Boston, MA, USA, May.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable Inference and Trainingof Context-Rich Syntactic Translation Models.
InProc.
of the 21st International Conf.
on Computa-tional Linguistics and 44th Annual Meeting of theAssoc.
for Computational Linguistics, pages 961?968, Sydney, Australia, July.Qin Gao and Stephan Vogel.
2008.
Parallel Implemen-tations of Word Alignment Tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, SETQA-NLP ?08, pages 49?57, Columbus, OH, USA, June.Greg Hanneman and Alon Lavie.
2009.
Decoding withSyntactic and Non-syntactic Phrases in a Syntax-based Machine Translation System.
In Proceedingsof the Third Workshop on Syntax and Structure inStatistical Translation, SSST ?09, pages 1?9, Boul-der, CO, USA, June.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proc.
of the Workshopon Statistical Machine Translation (WMT), pages187?197, Edinburgh, Scotland, UK, July.Hieu Hoang and Philipp Koehn.
2010.
ImprovedTranslation with Source Syntax Labels.
In Proc.
ofthe Workshop on Statistical Machine Translation(WMT), pages 409?417, Uppsala, Sweden, July.496Hieu Hoang, Philipp Koehn, and Adam Lopez.
2009.A Unified Framework for Phrase-Based, Hierarchi-cal, and Syntax-Based Statistical Machine Transla-tion.
In Proc.
of the Int.
Workshop on Spoken Lan-guage Translation (IWSLT), pages 152?159, Tokyo,Japan, December.Matthias Huck, David Vilar, Daniel Stein, and Her-mann Ney.
2011.
Lightly-Supervised Training forHierarchical Phrase-Based Machine Translation.
InProc.
of the EMNLP 2011 Workshop on Unsuper-vised Learning in NLP, pages 91?96, Edinburgh,Scotland, UK, July.Reinhard Kneser and Hermann Ney.
1995.
Im-proved Backing-Off for M-gram Language Model-ing.
In Proceedings of the International Conferenceon Acoustics, Speech, and Signal Processing, vol-ume 1, pages 181?184, Detroit, MI, USA, May.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Proc.of the Human Language Technology Conf.
/ NorthAmerican Chapter of the Assoc.
for ComputationalLinguistics (HLT-NAACL), pages 127?133, Edmon-ton, Canada, May/June.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proc.
of the MTSummit X, Phuket, Thailand, September.Alon Lavie.
2008.
Stat-XFER: A GeneralSearch-Based Syntax-Driven Framework for Ma-chine Translation.
In Alexander Gelbukh, editor,Computational Linguistics and Intelligent Text Pro-cessing, volume 4919 of Lecture Notes in ComputerScience, pages 362?375.
Springer Berlin Heidel-berg.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string Alignment Template for Statistical MachineTranslation.
In Proc.
of the 21st International Conf.on Computational Linguistics and the 44th AnnualMeeting of the Assoc.
for Computational Linguistics(ACL), pages 609?616, Sydney, Australia, July.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical Ma-chine Translation with Syntactified Target LanguagePhrases.
In Proc.
of the Conf.
on Empirical Methodsfor Natural Language Processing (EMNLP), pages44?52, Sydney, Australia.Yuval Marton and Philip Resnik.
2008.
Soft Syn-tactic Constraints for Hierarchical Phrased-BasedTranslation.
In Proc.
of the Annual Meeting of theAssoc.
for Computational Linguistics (ACL), pages1003?1011, Columbus, OH, USA, June.Maria Nadejde, Philip Williams, and Philipp Koehn.2013.
Edinburgh?s Syntax-Based Machine Transla-tion Systems.
In Proc.
of the Workshop on StatisticalMachine Translation (WMT), pages 170?176, Sofia,Bulgaria, August.Jan Niehues and Alex Waibel.
2012.
Detailed Analy-sis of Different Strategies for Phrase Table Adapta-tion in SMT.
In Proc.
of the Conf.
of the Assoc.
forMachine Translation in the Americas (AMTA), SanDiego, CA, USA, October/November.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive Training and Maximum Entropy Models for Sta-tistical Machine Translation.
In Proc.
of the AnnualMeeting of the Assoc.
for Computational Linguistics(ACL), pages 295?302, Philadelphia, PA, USA, July.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51,March.Franz Josef Och, Christoph Tillmann, and HermannNey.
1999.
Improved Alignment Models forStatistical Machine Translation.
In Proc.
of theJoint SIGDAT Conf.
on Empirical Methods in Nat-ural Language Processing and Very Large Corpora(EMNLP99), pages 20?28, University of Maryland,College Park, MD, USA, June.Franz Josef Och.
2002.
Statistical Machine Transla-tion: From Single-Word Models to Alignment Tem-plates.
Ph.D. thesis, RWTH Aachen University,Aachen, Germany, October.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
In Proc.
of theAnnual Meeting of the Assoc.
for ComputationalLinguistics (ACL), pages 311?318, Philadelphia, PA,USA, July.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, and In-terpretable Tree Annotation.
In Proc.
of the 21st In-ternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Assoc.
forComputational Linguistics, pages 433?440, Sydney,Australia, July.Helmut Schmid.
2004.
Efficient Parsing of HighlyAmbiguous Context-Free Grammars with Bit Vec-tors.
In Proc.
of the Int.
Conf.
on ComputationalLinguistics (COLING), Geneva, Switzerland, Au-gust.Holger Schwenk.
2008.
Investigations on Large-ScaleLightly-Supervised Training for Statistical MachineTranslation.
In Proc.
of the Int.
Workshop on Spo-ken Language Translation (IWSLT), pages 182?189,Waikiki, HI, USA, October.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Proc.
of the Conf.
of the Assoc.
forMachine Translation in the Americas (AMTA), pages223?231, Cambridge, MA, USA, August.497Daniel Stein, Stephan Peitz, David Vilar, and HermannNey.
2010.
A Cocktail of Deep Syntactic Fea-tures for Hierarchical Machine Translation.
In Proc.of the Conf.
of the Assoc.
for Machine Translationin the Americas (AMTA), Denver, CO, USA, Octo-ber/November.Andreas Stolcke.
2002.
SRILM ?
an Extensible Lan-guage Modeling Toolkit.
In Proc.
of the Int.
Conf.on Spoken Language Processing (ICSLP), volume 3,Denver, CO, USA, September.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2009.
Preference Grammars:Softening Syntactic Constraints to Improve Statis-tical Machine Translation.
In Proc.
of the Hu-man Language Technology Conf.
/ North AmericanChapter of the Assoc.
for Computational Linguistics(HLT-NAACL), pages 236?244, Boulder, CO, USA,June.David Vilar, Daniel Stein, and Hermann Ney.
2008.Analysing Soft Syntax Features and Heuristics forHierarchical Phrase Based Machine Translation.
InProc.
of the Int.
Workshop on Spoken LanguageTranslation (IWSLT), pages 190?197, Waikiki, HI,USA, October.Wei Wang, Kevin Knight, and Daniel Marcu.
2007.Binarizing Syntax Trees to Improve Syntax-BasedMachine Translation Accuracy.
In Proceedingsof the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 746?754, Prague, Czech Republic,June.Wei Wang, Jonathan May, Kevin Knight, and DanielMarcu.
2010.
Re-structuring, Re-labeling, andRe-aligning for Syntax-based Machine Translation.Computational Linguistics, 36(2):247?277, June.Philip Williams and Philipp Koehn.
2012.
GHKMRule Extraction and Scope-3 Parsing in Moses.
InProc.
of the Workshop on Statistical Machine Trans-lation (WMT), pages 388?394, Montr?al, Canada,June.Philip Williams, Rico Sennrich, Maria Nadejde,Matthias Huck, Eva Hasler, and Philipp Koehn.2014.
Edinburgh?s Syntax-Based Systems atWMT 2014.
In Proc.
of the Workshop on StatisticalMachine Translation (WMT), Baltimore, MD, USA,June.Joern Wuebker, Arne Mauser, and Hermann Ney.2010.
Training Phrase Translation Models withLeaving-One-Out.
In Proc.
of the Annual Meetingof the Assoc.
for Computational Linguistics (ACL),pages 475?484, Uppsala, Sweden, July.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax Augmented Machine Translation via Chart Pars-ing.
In Proc.
of the Workshop on Statistical MachineTranslation (WMT), pages 138?141, New York City,NY, USA, June.498
