Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 615?618,Dublin, Ireland, August 23-24, 2014.SZTE-NLP: Clinical Text Analysis with Named Entity RecognitionMelinda Katona and Rich?ard FarkasDepartment of InformaticsUniversity of Szeged?Arp?ad t?er 2., Szeged, 6720, Hungary{mkatona,rfarkas}@inf.u-szeged.huAbstractThis paper introduces our contribution tothe SemEval-2014 Task 7 on ?Analysis ofClinical Text?.
We implemented a sys-tem which combines MetaMap taggingsand Illinois NER Tagger.
MetaMap is de-veloped to link the text of medical doc-uments to the knowledge embedded inUMLS Metathesaurus.
The UMLS con-tains a very rich lexicon while the promiseof a NER system is to carry out context-sensitive tagging.
Our system?s perfor-mance was 0.345 F-measure in terms ofstrict evaluation and 0.551 F-measure interms of relaxed evaluation.1 IntroductionClinical notes and discharge summaries from thepatient?s medical history contain a huge amountof useful information for medical researchers andalso for hospitals.
The automatic identificationof these unstructured information is an impor-tant task for analysis of free-text electronic healthrecords.
Natural Language Processing (NLP)techniques provide a solution to process clinicaldocuments and to help patients understand thecontents of their clinical records (Tang et al., 2012;Lee et al., 2004).In this paper we introduce an approach whichdiscovers mentions of disorders in the free-text ofdischarge summaries.
The system participated inthe SemEval-2014 Task 7: Analysis of ClinicalText, Task A.Task A aims at the identifying of mentionconcepts that belong to the UMLS (Boden-reider, 2004) semantic group ?disorders?
andTask B is for mapping from each mention toThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/a unique UMLS/SNOMED-CT CUI (ConceptUnique Identifiers).
Here are a few examples fromthe task description:?
The rhythm appears to be atrial fibrillation.,,atrial fibrillation?
is a mention of type disor-ders with CUI C0004238?
The left atrium is moderately dilated.,,left atrium [...] dilated?
is a mention of typedisorders with CUI C0344720?
53 year old man s/p fall from ladder.,,fall from ladder?
is a mention of type disor-ders with CUI C0337212Many approaches have been published to solvethese problems cf.
(Skeppstedt et al., 2012; Pes-tian et al., 2007).2 ApproachAfter a text-normalization step we run a NamedEntity Recogniser (NER) on the documents.
ThisNER model was trained on the training set of theshared task.
It also employs a dictionary gatheredfrom UMLS through MetaMap tagging.
Our ini-tial experiments revealed that MetaMap (Aronsonand Lang, 2010) in its own gives a very poor pre-cision hence we decided to investigate a NER ap-proach which takes the context also into account.2.1 NormalizationClinical reports contain numerous special annota-tions, such as anonymized data (for example pa-tient name), etc.
We made the following steps tonormalize texts:?
We removed the unnecessary characters, suchas .
, !
?
# : ; ?
= + * ??
Then replaced the [****] anonymized tagswith REPLACED ANONYMOUS DATAnotation.6152.2 UMLS DictionaryOur NER system constructs features from dic-tionaries as well.
We created a dictionary fromUMLS with the help of MetaMap for incorporat-ing external knowledge into the NER.
The use of aspecialized dictionary is important because it con-tains phrases that occur in clinical texts.MetaMap (Aronson and Lang, 2010) is devel-oped to link the text of medical documents to theknowledge embedded in UMLS Metathesaurus.MetaMap employs natural language processingtechniques working at the lexical/syntactic lev-els, for example handling acronyms/abbrevations,POS tagging, word sense disambiguation and soon.Both the test and training datasets were usedfor creating our dictionary.
We used MetaMap tocollect disorders from raw texts.
After that, weremoved the redundant and most frequently usedcommon words, based on a list of the 5000 mostfrequent English words according to the Google?sn-gram corpus1.2.3 Named Entity RecognitionIn the task ?Analysis of Clinical Text?, our task isto recognize mentions of concepts that belong tothe UMLS semantic group ?disorder?, which canbe viewed as a subclass of named entities, so NERapproach is effective for this assignment.For training, we used the Illinois Named En-tity Recognition (Ratinov and Roth, 2009) sys-tem.
By default, Illinois NER contains Wikipediagazetters and categories, but in this task, we needone or more dictionary which contains disordersand other clinical text terminology.NER is typically viewed as a sequence label-ing problem.
The typical models include HMM(Rabiner, 1989), CRF (Lafferty et al., 2001) andsequential application of Perceptron or Winnow(Collins, 2002).
Illinois NER has several infer-ence algorithms: Viterbi, beamsearch, greedy left-to-right decoding.
In our approach, we used beam-search.
The beamsize was 3.
Initially, we usedbigger beamsize, but our empirical studies showedthat applying a small beamsize is more effective.Beside the decoding algorithm, an importantquestion that has been studied extensively in thecontext of shallow parsing which was somewhatoverlooked in the NER literature is the represen-1http://storage.googleapis.com/books/ngrams/books/datasetsv2.htmltation of text segments.
Illinois NER containsseveral representation schemes such as BIO andBILOU - two of the most popular schemes.
TheBIO scheme is employed to train classifiers thatidentify Beginning, the Inside and the Outside ofthe text segment.
The BILOU scheme is employedto train classifiers that identify the Beginning, theInside and the Last tokens of multi-token chunksas well as Unit-length chunks.
We used theBILOU scheme.The key intuition behind non-local features inNER has been that identical tokens should haveidentical label assignments.
Ratinov and Roth(2009) consider three approaches proposed in theliterature namely context aggregation, two-stageprediction aggregation and extended predictionhistory.
The combination of these approaches ismore stable and better than any approach takenalone.In our experiments we used the combinationof context aggregation and two-stage predictionaggregation.
Context aggregation is the fol-lowing approach in Illinois NER: for each to-ken instance xiwe used the tokens in the win-dow of size two around it as features: ci=xi?2, xi?1, xi, xi+1, xi+2.
If the same token (t)appears in several locations in the text for each in-stance xij(xi1, xi2, .
.
., xiN).
We also aggregatedthe context across all instances within 200 tokens.Context aggregation as done above can lead toan excessive number of features.
Some instancesof a token appear in easily-identifiable contexts.The resulting predictions were used as features ata second level of inference.
This is a two-stageprediction aggregation.3 Experimental ResultsOur system was developed and trained only on thetraining set provided by the organizers and wasevaluated on the test set.
The performance wasevaluated by Precision, Recall and F-measure inboth ?strict?
and ?relaxed?
modes.
?Strict?
meansthat a concept is recognized correctly if the start-ing and ending offsets are the same as in gold stan-dard and ?relaxed?
means that a disorder mentionis correctly recognized as long as it overlaps withthe gold standard disorder mention.3.1 DatasetFor training and testing, we used the datasets pro-vided by the shared task organisers.
The train-616Strict RelaxedP R F P R Foriginal NER 0.508 0.225 0.312 0.874 0.378 0.528NER with normalization 0.509 0.229 0.316 0.875 0.383 0.528NER with normalization and full dictionary 0.512 0.226 0.313 0.878 0.378 0.533NER with normalization and filtered dictionary 0.516 0.232 0.320 0.890 0.390 0.542Table 1: Evaluation results of our system on the training set (P - Precision, R - Recall, F - F-score).ing dataset contains of 398 notes from differentclinical documents including radiology reports,discharge summaries, and ECG/ECHO reports.For each note, disorder entities were annotatedbased on a pre-defined guideline and then mappedto SNOMED-CT concepts represented by UMLSCUIs.
The reference UMLS version was 2012AB.If a disorder entity could not be found, it wasmarked as CUI-less, otherwise marked with CUIidentifier.The training set was used for system develop-ment, and we evaluated the system on the test setof 133 notes.3.2 ResultsWe examined the contribution of our systems?steps.
Table 1 summarizes the results where thefirst column contains result of named entity tag-ger without any modification.
Normalization gaveonly a marginal improvement in accuracy.
Next,we employed all MetaMap matches as a featurefor the NER module.
This decreased recall, be-cause NER identified a lot of unnecessary expres-sion.
In our final and submitted system, we filteredthis dictionary as described in the previous section.Lastly, Table 2 shows our official evaluation re-sults.Strict RelaxedPrecision 0.547 0.884Recall 0.252 0.401F-score 0.345 0.551Table 2: Results of our submission on the test set.4 Error AnalysisIn both strict and relaxed evaluation modes, preci-sion is high but recall is low.
We have found threeimportant source of errors:?
multiple meaning words?
unknown disorders?
discontinuous phrasesA named entity tagger with context-aggregationmode does not monitor multiple meanings, so if aword has more occurrence, but in other meaning,it will be a bad tagging.
For example?Seizure-like activity with clamped jaw and leftlip twitching was then noted after several daysof treatment.
[...] Despite these therapies, shefailed to recover, and began to show further signsof increasing intracranial pressure with increasingseizure activity and posturing [...]?Our sequence labeling approach cannot recog-nize discontinuous phrases.
Even when every to-ken was marked, we took only continuous se-quences as named entity mentions.
For examplethe sentence?The left ventricular cavity is moderately di-lated.
?yields three errors in the strict evaluation sce-nario.
We did not recognise the three token-longphrase while predicted two false positive men-tions.
We also note that this shortcoming of ourapproach is the reason for the huge difference be-tween the achieved strict and relaxed scores.The last error category is unrecognised disor-ders.
For instance,?The PICC line was trimmed to the appropriatelength and advanced over the 0.018 wire with thetip int the axillary vein?Named entity tagger identified hepatitis B, buthepatitis C not because dictionary does not containit.
Expansion of dictionary increase accuracy.5 ConclusionIn this paper we examined a machine learningbased disorder recognition system using MetaMapand Illinois Named Entity Recognition.
IllinoisNER uses different dictionaries for training.
Wecreated a new filtered in-domain dictionary and weshowed that this dictionary is an important factor617for accuracy.
The results achieved on the trainingset and the test set show that the proposed clinicaldictionary creation procedure is efficient.AcknowledgementsMelinda Katona is supported by the EuropeanUnion and co-funded by the European SocialFund.
Project title: ?Telemedicine-focused re-search activities on the field of Matematics, In-formatics and Medical sciences?
(project num-ber: T?AMOP-4.2.2.A-11/1/KONV-2012-0073).Rich?ard Farkas was partially funded by the ?Hun-garian National Excellence Program?
(T?AMOP4.2.4.A/2-11-1-2012-0001), co-financed by theEuropean Social Fund.ReferencesAlan R. Aronson and Franois-Michel Lang.
2010.
AnOverview of MetaMap: Historical Perspective andRecent Advances.
JAMIA, 17:229?236.Olivier Bodenreider.
2004.
The Unified Medical Lan-guage System (UMLS): Integrating Biomedical Ter-minology.
Nucleic Acids Research, 32:267?270.Michael Collins.
2002.
Discriminative Training Meth-ods for Hidden Markov Models: Theory and Exper-iments with Perceptron Algorithms.
In Proceedingsof the ACL-02 Conference on Empirical Methods inNatural Language Processing - Volume 10, pages 1?
8.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional Random Fields:Probabilistic Models for Segmenting and LabelingSequence Data.
In Proceedings of the Eighteenth In-ternational Conference on Machine Learning, pages282 ?
289.Chih Lee, Wen-Juan Hou, and Hsin-Hsi Chen.
2004.Annotating Multiple Types of Biomedical Entities:A Single Word Classification Approach.
In Pro-ceedings of the International Joint Workshop onNatural Language Processing in Biomedicine andIts Applications, JNLPBA ?04, pages 80?83.John P. Pestian, Christopher Brew, Pawel Matykiewicz,D.
J. Hovermale, Neil Johnson, K. Bretonnel Co-hen, and Wlodzislaw Duch.
2007.
A Shared TaskInvolving Multi-label Classification of Clinical FreeText.
In Proceedings of the Workshop on BioNLP2007: Biological, Translational, and Clinical Lan-guage Processing, pages 97?104.Lawrence Rabiner.
1989.
A Tutorial on Hid-den Markov Models and Selected Applications inSpeech recognition.
Proceedings of the IEEE,77:257?286.L.
Ratinov and D. Roth.
2009.
Design Challenges andMisconceptions in Named Entity Recognition.
InCoNLL, 6.Maria Skeppstedt, Maria Kvist, and Hercules Dalianis.2012.
Rule-based Entity Recognition and Coverageof SNOMED CT in Swedish Clinical Text.
In Pro-ceedings of the Eight International Conference onLanguage Resources and Evaluation (LREC?12).Buzhou Tang, Hongxin Cao, Yonghui Wu, Min Jiang,and Hua Xu.
2012.
Clinical Entity Recognition Us-ing Structural Support Vector Machines with RichFeatures.
In Proceedings of the ACM Sixth In-ternational Workshop on Data and Text Mining inBiomedical Informatics, DTMBIO ?12, pages 13?20.618
