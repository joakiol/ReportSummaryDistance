Tagging Sentence BoundariesAndre i  M ikheevXana lys  Inc. and  The  Un ivers i ty  of  Ed inburgh2 Bucc leuch  P lace,  Ed inburgh  EH8 9LW,  UKmikheevOhar  lequ in ,  co .
ukAbst rac tIn this paper we tackle sentence boundary disam-biguation through a part-of-speech (POS) taggingframework.
We describe necessary changes in texttokenization and the implementation of a POS tag-ger and provide results of an evaluation of this sys-tem on two corpora.
We also describe an exten-sion of the traditional POS tagging by combiningit with the document-centered approach to propername identification and abbreviation handling.
Thismade the resulting system robust to domain andtopic shifts.1 In t roduct ionSentence boundary disambiguation (SBD) is an im-portant aspect in developing virtually any practi-cal text processing application - syntactic parsing,Information Extraction, Machine Translation, TextAlignment, Document Summarization, etc.
Seg-menting text into sentences in most cases is a sim-ple mat ter -  a period, an exclamation mark or aquestion mark usually signal a sentence boundary.However, there are cases when a period denotes adecimal point or is a part of an abbreviation andthus it does not signal a sentence break.
Further-more, an abbreviation itself can be the last tokenin a sentence, in which case its period acts at thesame time as part of this abbreviation and as theend-of-sentence indicator (fullstop).The first large class of sentence boundary disam-biguators uses manually built rules which are usuallyencoded in terms of regular expression grammarssupplemented with lists of abbreviations, commonwords, proper names, etc.
For instance, the Alem-bic workbench (Aberdeen et al, 1995) contains asentence splitting module which employs over 100regular-expression rules written in Flex.
To put to-gether a few rules which do a job is fast and easy, butto develop a good rule-based system is quite a labourconsuming enterprise.
Another potential shortcom-ing is that such systems are usually closely tailoredto a particular corpus and are not easily portableacross domains.Automatically trainable software is generally seenas a way of producing systems quickly re-trainablefor a new corpus, domain or even for another lan-guage.
Thus, the second class of SBD systems em-ploys machine learning techniques uch as decisiontree classifiers (Riley, 1989), maximum entropy mod-eling (MAXTERMINATOR) (Reynar and Ratna-parkhi, 1997), neural networks (SATZ) (Palmer andHearst, 1997), etc.. Machine learning systems treatthe SBD task as a classification problem, using fea-tures such as word spelling, capitalization, suffix,word class, etc., found in the local context of poten-tim sentence breaking punctuation.
There is, how-ever, one catch - all machine learning approaches tothe SBD task known to us require labeled examplesfor training.
This implies an investment in the an-notation phase.There are two corpora normally used for evalua-tion and development in a number of text process-ing tasks and in the SBD task in particular: theBrown Corpus and the Wall Street Journal (WSJ)corpus - both part of the Penn Treebank (Mar-cus, Marcinkiewicz, and Santorini, 1993).
Wordsin both these corpora are annotated with part-of-speech (POS) information and the text is split intodocuments, paragraphs and sentences.
This givesall necessary information for the development ofan SBD system and its evaluation.
State-of-the-art machine-learning and rule-based SBD systemsachieve the error rate of about 0.8-1.5% measuredon the Brown Corpus and the WSJ.
The best per-formance on the WSJ was achieved by a combinationof the SATZ system with the Alembic system - 0.5%error rate.
The best performance on the Brown Cor-pus, 0.2% error rate, was reported by (Riley, 1989),who trained a decision tree classifier on a 25 millionword corpus.1.1 Word-based  vs. Syntact i c  MethodsThe first source of ambiguity in end-of-sentencemarking is introduced by abbreviations: if we knowthat the word which precedes a period is not  an ab-breviation, then almost certainly this period denotesa sentence break.
However, if this word is an ab-breviation, then it is not that easy to make a cleardecision.
The second major source of information264for approaching the SBD task comes from the wordwhich follows the period or other sentence splittingpunctuation.
In general, when the following wordis punctuation, number or a lowercased word - theabbreviation is not sentence terminal.
When the fol-lowing word is capitalized the situation is less clear.If this word is a capitalized common word - this sig-nals start of another sentence, but if this word is aproper name and the previous word is an abbrevia-tion, then the situation is truly ambiguous.Most of the existing SBD systems are word-based.They employ only lexical information (word capital-ization, spelling, suffix, etc.)
to predict whether acapitalized word-token which follows a period is aproper name or is a common word.
Usually this isimplemented by applying the lexical lookup methodwhere a word is assigned its category according towhich word-list it belongs to.
This, however, isclearly an oversimplification.
For instance, the word"Black" is a frequent surname and at the same timeit is a frequent common word, thus the lexical infor-mation is not very reliable in this case.
But by em-ploying local context one can more robustly predictthat in the context "Black described.." this wordacts as a proper name and in the context "Blackumbrella.." this word acts as a common word.It is almost impossible to robustly estimate con-texts larger than single focal word using word-basedmethods - even bigrams of words are too sparse.
Forinstance, there are more than 50,000 distinct wordsin the Brown Corpus, thus there are 250`0o0 poten-tial word bigrams, but only a tiny fraction of themcan be observed in the corpus.
This is why wordsare often grouped into semantic lasses.
This, how-ever, requires large manual effort, is not scalable andstill covers only a fraction of the lexica.
Syntacticcontext is much easier to estimate because the num-ber of syntactic ategories i  much smaller than thenumber of distinct words.A standard way to identify syntactic ategories forword-tokens i part-of-speech (POS) tagging.
Theresyntactic ategories are represented as POS tags e.g.NNS - plural noun, VBD - verb past form, J JR - com-parative adjective, etc.
There exist several tag-setswhich are currently in use - some of them reflectonly the major syntactic information such as part-of-speech, number, tense, etc., whereas others reflectmore refined information such as verb subcategoriza-tion, distinction between mass and plural nouns, etc.Depending on the level of detail one tag-set canincorporate a few dozen tags where another can in-corporate a few hundred, but still such tags will beconsiderably less sparse than individual words.
Forinstance, there are only about 40 POS tags in thePenn Treebank tag-set, therefore there are only 240potential POS bigrams.
Of course, not every wordcombination and POS tag combination is possible,but these numbers give a rough estimation of themagnitude of required data for observing necessarycontexts for words and POS tags.
This is why the"lexical lookup" method is the major source of in-formation for word-based methods.The "lexical ookup" method for deciding whethera capitalized word in a position where capitalizationis expected (e.g.
after a fullstop) is a proper name ora common word gives about an 87o error rate on theBrown Corpus.
We developed and trained a POStagger which reduced this error more than by ha l f -achieving just above a 3% error rate.
On the WSJcorpus the POS tagging advantage was even greater:our tagger reduced the error rate from 1570 of thelexical lookup approach to 5%.
This suggests thatthe error rate of a sentence splitter can be reducedproportionally by using the POS tagging method-ology to predict whether a capitalized word after aperiod is a proper name or a common word.1.2 The  SATZ System(Palmer and Hearst, 1997) described an approachwhich recognized the potential of the local syntac-tic context for the SBD problem.
Their, system,SATZ, used POS information for words in the lo-cal context of potential sentence splitting punctu-ation.
However, what is interesting is that theyfound difficulty in applying a standard POS tag-ging framework for determining POS informationfor the words: "However, requiring a single part-of-speech assignment for each word introduces a pro-cessing circularity: because most part-of-speech tag-gers require predetermined sentence boundaries, theboundary disambiguation must be done before tag-ging.
But if the disambiguations done before tag-ging, no part-of-speech assignments are available forthe boundary determination system".Instead, they applied a simplified method.
TheSATZ system mapped Penn Treebank POS tags intoa set of 18 generic POS categories uch as noun, ar-ticle, verb, proper noun, preposition, etc.
Each wordwas replaced with a set of these generic categoriesthat it can take on.
Such sets of generic syntac-tic categories for three tokens before and three to-kens after the period constituted a context whichwas then fed into two kinds of classifiers (decisiontrees and neural networks) to make the predictions.This system demonstrated reasonable accm'acy(1.0% error rate on the WSJ corpus) and also ex-hibited robustness and portability when applied toother domains and languages.
However, the N-grams of syntactic ategory sets have two importantdisadvantages in comparison to the traditional POStagging which is usually largely based (directly orindirectly) on the N-grams of POS tags.
First, syn-tactic category sets are much sparser than syntacticcategories (POS tags) and, thus, require more datafor training.
Second, in the N-grams-only method265.
.
.
<W.
.
.
<W.
.
.
<WC='RB' A='N'>soon</W><W C='.
'>.</W> <W A='Y' C='NNP'>Mr</W><W C='A'>.</W>...C='VBD'>said</W> <W C='NNP' A='Y'>Mr</W><W C='A'>.</W> <W C='NNP'>Brown</W>.,,C=','>,</W> <W C='NNP' A='Y'>Tex</W><W C='*'>.</W> <W C='DT'>The</W>...Figure h Example of tokenization and markup.
Text is tokenized into tokens represented as XML elementswith attributes: A='Y' - abbreviation, A=' N'- not abbreviation, C - part-of-speech tag attribute, C='.
'fullstop, C='A' - part of abbreviation, C='* '  - a fullstop and part of abbreviation at the same time.no influence from the words outside the N-grams canbe traced, thus, one has to adopt N-grams of suffi-cient length which in its turn leads either to sparsecontexts or otherwise to sub-optimal discrimination.The SATZ system adopted N-grams of length six.In contrast o this, POS taggers can capture influ-ence of the words beyond an immediate N-gram and,thus, usually operate with N-grams of length two (bi-grams) or three (three-grams).
Furthermore, in thePOS tagging field there exist standard methods tocope with N-gram sparseness and unknown words.Also there have been developed methods for unsu-pervised training for some classes of POS taggers.1.3 Th is  PaperIn this paper we report on the integration of thesentence boundary disambiguation functionality intothe POS tagging framework.
We show that Sentencesplitting can be handled during POS tagging and theabove mentioned "circularity" can be tackled by us-ing a non-traditional tokenization and markup con-ventions for the periods.
We also investigate reduc-ing the importance of pre-existing abbreviation listsand describe guessing strategies for unknown abbre-viations.2 New Hand l ing  o f  Per iodsIn the traditional Treebank schema, abbreviationsare tokenized together with their trailing periodsand, thus, stand-alone periods unambiguously sig-nal end-of-sentence.
For handling the SBD task wesuggest okenizing periods separately from their ab-breviations and treating a period as an ambiguoustoken which can be marked as a fullstop ( ' .  '
), part-of-abbreviation ( '  A') or both ( '  * ' ) .
An example ofsuch markup is displayed on Figure 1.
Such markupallows us to treat the period similarly to all otherwords in the text: a word can potentially take onone of a several POS tags and the job of a tagger isto resolve this ambiguity.In our experiments we used the Brown Corpus andthe Wall Street Journal corpus both taken from thePenn Treebank (Marcus, Marcinkiewicz, and San-torini, 1993).
We converted both these corpora fromthe original format to our XML format (as displayedon Figure 1), split the final periods from the abbrevi-ations and assigned them with C= ' A ' and C= ' * ' tagsaccording to whether or not the abbreviation was thelast token in a sentence.
There were also quite a fewinfelicities in the original tokenization and taggingof the Brown Corpus which we corrected by hand.Using such markup it is straightforward to traina POS tagger which also disambiguates sentenceboundaries.
There is, however, one difference in theimplementation of such tagger.
Normally, a POStagger operates on a text-span which forms a sen-tence and this requires performing the SBD beforetagging.
However, we see no good reason why such atext-span should necessarily be a sentence, becausealmost all the taggers do not attempt o parse a sen-tence and operate only in the local window of twoto three tokens.The only reason why the taggers traditionally op-erate on the sentence level is because there exists atechnical issue of handling long text spans.
Sentencelength of 30-40 tokens seems to be a reasonable limitand, thus, having sentences pre-chunked before tag-ging simplifies life.
This issue, however, can be alsoaddressed by breaking the text into short text-spansat positions where the previous tagging history doesnot affect current decisions.
For instance, a bigramtagger operates within a window of two tokens, andthus a sequence of word-tokens can be terminatedat an unambiguous word because this unambiguousword token will be the only history used in taggingof the next token.
A trigram tagger operates withina window of three tokens, and thus a sequence ofword-tokens can be terminated when two unambigu-ous words follow each other.3 Tagg ing  Exper imentUsing the modified treebank we trained a tri-gramPOS tagger (Mikheev, 1997) based on a combinationof Hidden Markov Models (HMM) and MaximumEntropy (ME) technologies.
Words were clusteredinto ambiguity classes (Kupiec, 1992) according tosets of POS tags they can take on.
This is a stan-dard technique that was also adopted by the SATZsystem 1.
The tagger predictions were based on theambiguity class of the current word together with1The SATZ system operated with a reduced set of 18generic ategories instead of 40 POS tags of the Penn Tree-bank tag-set.266Table 1: POS Tagging on sentence splitting punctuation and ambiguously capitalized wordsTagger Feature Set Error on Sentence Punct.Upper BoundPOS TaggerPOS Tagger EnhancedBrown Corpus0.010.25%0.20%WSJ Corpus0.13O.39%0.31%POS Tagger/No abbr.
list 0.98% 1.95%POS Tagger Enhanced/No abbr.
list 0.65% 1.39%Error on Words in Mandatory Pos.Brown Corpus3.15%1.87%WSJ Corpu s4.72%3.22%3.19% 5.29%1.91% 3.28%the POS trigrams: hypothesized current POS tagand partially disambiguated POS tags of two previ-ous word-tokens.
We also collected a list of abbrevi-ations as explained later in this paper and used theinformation about whether a word is an abbrevia-tion, ordinary word or potential abbreviation (i.e.
aword which could not be robustly classified in thefirst two categories).
This tagger employed Maxi-mum Entropy models for tag transition and emissionestimates and Viterbi algorithm (Viterbi, 1967) forthe optimal path search.Using the forward-backward algorithm (Baum,1972) we trained our tagger in the unsupervisedmode i.e.
without using the annotation availablein the Brown Corpus and the WSJ.
For evaluationpurposes we trained our tagger on the Brown Cor-pus and applied it to the WSJ corpus and vice versa.We preferred this method to ten-fold cross-validationbecause this allowed us to produce only two taggingmodels instead of twenty and also this allowed us totest the tagger in harsher conditions when it is ap-plied to texts which are very distant from the onesit was trained on.In this research we concentrated on measuring theperformance only on two categories of word-tokens:on periods and other sentence-ending punctuationand on word-tokens in mandatory positions.
Manda-tory positions are positions which might require aword to be capitalized e.g.
after a period, quotes,brackets, in all-capitalized titles, etc.
At the evalua-tion we considered proper nouns (NNP), plural propernouns (NNPS) and proper adjectives 2 (JJP) to signala proper name, all all other categories were consid-ered to signal a common word or punctuation.
Wealso did not consider as an error the mismatch be-tween "."
and "*" categories because both of themsignal that a period denotes the end of sentence andthe difference between them is only whether this pe-riod follows an abbreviation or a regular word.In all our experiments we treated embedded sen-tence boundaries in the same way as normal sentenceboundaries.
The embedded sentence boundary oc-curs when there is a sentence inside a sentence.
This2These are adjectives like "American" which are alwayswritten capitalized.
We identified and marked them in theWSJ and Brown Corpus,can be a quoted direct speech sub-sentence inside asentence, this can be a sub-sentence embedded inbrackets, etc.
We considered closing punctuation ofsuch sentences equal to closing punctuation of ordi-nary sentences.There are two types of error the tagger can makewhen disambiguating sentence boundaries.
The firstone comes from errors made by the tagger in identi-fying proper names and abbreviations.
The secondone comes from the limitation of the POS taggingapproach to the SBD task.
This is when an abbrevi-ation is followed by a proper name: POS informationnormally is not sufficient o disambiguate such casesand the tagger opted to resolve all such cases as "notsentence boundary".
There are about 5-7% of suchcases in the Brown Corpus and the WSJ and themajority of them, indeed, do not signal a sentenceboundary.We can estimate the upper bound for our ap-proach by pretending that the tagger was able toidentify all abbreviations and proper names withperfect accuracy.
We can sinmlate this by using theinformation available in the treebank.
It turned outthat the tagger marked all the cases when an ab-breviation is followed by a proper name, punctua-tion, non-capitalized word or a number as "not sen-tence boundary".
All other periods were marked assentence-terminal.
This produced 0.01% error rateon the Brown Corpus and 0.13% error rate on theWSJ as displayed in the first row of Table 1.In practice, however, we cannot expect he taggerto be 100% correct and the second row of Table 1 dis-plays the actual results of applying our POS taggerto the Brown Corpus and tile WSJ.
General taggingperformance on both our corpora was a bit betterthan a 4% error rate which is in line with the stan-dard performance of POS taggers reported on thesetwo corpora.
On the capitalized words in manda-tory positions the tagger achieved a 3.1-4.7% errorrate which is an improvement over the lexical lookupapproach by 2-3 times.
On the sentence breakingpunctuation the tagger performed extremely well -an error rate of 0.39% on the WSJ and 0.25% onthe Brown Corpus.
If we compare these results withthe upper bound we see that the errors made by thetagger on the capitalized words and abbreviations267instigated about a 0.25% error rate on the sentenceboundaries.We also applied our tagger to single-case texts.We converted the WSJ and the Brown Corpus toupper-case only.
In contrast o the mixed case textswhere capitalization together with the syntactic in-formation provided very reliable evidence, syntacticinformation without capitalization is not sufficientto disambiguate sentence boundaries.
For the ma-jority of POS tags there is no clear preference as towhether they are used as sentence starting or sen-tence internal.
To minimize the error rate on singlecase texts, our tagger adopted a strategy to mark allperiods which follow al)breviations as "non-sentenceboundaries".
This gave a 1.98% error rate on theWSJ and a 0.51% error rate on the Brown Corpus.These results are in line with the results reported forthe SATZ system on single case texts.4 Enhanced Feature  Set(Mikheev, 1999) described a new approach to thedisambiguation of capitalized words in mandatorypositions.
Unlike POS tagging, this approachdoes not use local syntactic context, but rather itapplies the so-called document-centered approach.The essence of the document-centered approach isto scan the entire document for the contexts wherethe words in question are used unambiguously.
Suchcontexts give the grounds for resolving ambiguouscontexts.For instance, for the disambiguation of capital-ized words in mandatory positions the above rea-soning can be crudely summarized as follows: if wedetect that a word has been used capitalized in anunambiguous context (not in a mandatory position),this increases the chances for this word to act asa proper name in mandatory positions in the samedocument.
And, conversely, if a word is seen onlylowercased, this increases the chances to downcase itin mandatory positions of the same document.
Bycollecting sequences and unigrams of unambiguouslycapitalized and lowercased words in the documentand imposing special ordering of their applications(Mikheev, 1999) reports that the document-centeredapproach achieved a 0.4-0.7% error rate with cover-age of about 90% on the disambiguation of capital-ized words in mandatory positions.We decided to combine this approach with ourPOS tagging system in the hope of achieving betteraccuracy on capitalized words after the periods andtherefore improving the accuracy of sentence split-ting.
Although the document-centered approach tocapitalized words proved to be more accurate thanPOS tagging, the two approaches are complimentaryto each other since they use different ypes of infor-mation.
Thus, the hybrid system can bring at leasttwo advantages.
First, unassigned by the document-centered approach 10% of the ambiguously capital-ized words can be assigned using a standard POStagging method based on the local syntactic con-text.
Second, the local context can correct some ofthe errors made by the document-centered approach.To implement this hybrid approach we incorporatedthe assignments made by the document-centered ap-proach to the words in mandatory positions to ourPOS tagging model by simple linear interpolation.The third row of Table 1 displays the results ofthe application of the extended tagging model.
Wesee an improvement on proper name recognition byabout 1.5%: overall error rate of 1.87% on the BrownCorpus and overall error rate 3.22% on the WSJ.This in its turn allowed for better tagging of sen-tence boundaries : a 0.20% error rate on the BrownCorpus and a 0.31% error rate on the WSJ, whichcorresponds to about 20% cut in the error rate incomparision to the standard POS tagging.5 Handling of AbbreviationsInformation about whether a word is an abbrevia-tion or not is absolutely crucial for sentence splitting.Unfortunately, abbreviations do not form a closedset, i.e., one cannot list all possible abbreviations.It gets even worse - abbreviations can coincide withordinary words, i.e., "in" can denote an abbrevia-tion for "inches", "no" can denote an abbreviationfor "number", "bus" can denote an abbreviation for"business", etc.Obviously, a practical sentence splitter which inour case is a POS tagger, requires a module that canguess unknown abbreviations.
First, such a modulecan apply a well-known heuristic that single-wordabbreviations are short and normally do not includevowels (Mr., Dr., kg.).
Thus a word without vowelscan be guessed to be an abbreviation unless it is writ-ten in all capital letters which can be an acronym(e.g.
RPC).
A span of single letters, separated byperiods forms an abbreviation too (e.g.Y.M.C.A.
).Other words shorter than four characters and un-known words shorter than five characters hould betreated as potential abbreviations.
Although theseheuristics are accurate they manage to identify onlyabout 60% of all abbreviations in the text whichtranslates at 40% error rate as shown in the firstrow of Table 2.These surface-guessing heuristics can be supple-mented with the document-centered approach (DCA)to abbreviation guessing, which we call PositionalGuessing Strategy (PGS).
Although a short wordwhich is followed by a period can potentially be anabbreviation, the same word when occurring in thesame document in a different context can be unam-biguously classified as an ordinary word if it is usedwithout a trailing period, or it can be unambigu-ously classified as an abbreviation if it is used with a268Table 2: Error rate for different abbreviation identification methodsCorpussurface guesssurface guess and DCAsurface guess and DCA and abbr.
listtrailing period and is followed by a lowercased wordor a comma.
This allows us to assign such wordsaccordingly even in ambiguous contexts of the samedocument, i.e., when they are followed by a period.For instance, the word "Kong" followed by a pe-riod and then by a capitalized word cannot be safelyclassified as a regular word (non-abbreviation) andtherefore it is a potential abbreviation.
But if in thesame document we detect a context "lived in HongKong in 1993" this indicates that "Kong" is nor-mally written without a trailing period and henceis not an abbreviation.
Having established that,we can apply this findings to the non-evident con-texts and classify "Kong" as a regular word (non-abbreviation) throughout the document.
However,if we detect a context such as "Kong., said" this in-dicates that in this document "'Kong" is normallywritten with a trailing period and hence is an ab-breviation.
This gives us grounds to classify "Kong"as an abbreviation i  all its occurrences within thesame document.The positional guessing strategy relies on theassumption that there is a consistency of writingwithin the same document.
Different authors canwrite "Mr" or "Dr" with or without trailing periodbut we assume that the same author (the authorof a document) will write consistently.
However,there can occur a situation when a potential abbre-viation is used as a regular word and as an abbre-viation within the same document.
This is usuallythe case when an abbreviation coincides with a reg-ular word e.g.
"Sun."
(meaning Sunday) and "Sun"(the name of a newspaper).
To tackle this prob-lem, our strategy is to collect not only unigrams ofpotential abbreviations in unambiguous contexts asexplained earlier but also their bigrams with the pre-ceding word.
Now the positional guessing strategycan assign ambiguous instances on the basis of thebigrams it collected from the document.For instance, if in a document the system found acontext "vitamin C is" it stores the bigram "vitaminC" and the unigrarn "C" with the information thatit is a regular word.
If in the same document hesystem also detects a context "John C. later said" itstores the bigram "John C." and the unigram "C"with the information that it is an abbreviation.
Herewe have conflicting information for the word "C" -it was detected as acting as a regular word and as anabbreviation within the same document - so there isnot enough information to resolve ambiguous casespurely using the unigram.
However, some cases canbe resolved on the basis of the bigrams e.g.
the sys-tem will assign "C" as an abbreviation i an ambigu-ous context "... John C. Research ..." and it willassign "C" as a regular word (non-abbreviation) ian ambiguous context "... vitamin C. Research ..."When neither unigrams nor bigrams can help toresolve an ambiguous context for a potential abbre-viation, the system decides in favor of the more fre-quent category deduced from the current documentfor this potential abbreviation.
Thus if the word"In" was detected as acting as a non-abbreviation(preposition) five times in the current document andtwo times as abbreviation (for the state Indiana),in a context where neither of the bigrams collectedfrom the document can be applied, "In" is assignedas a regular word (non-abbreviation).
The last re-sort strategy is to assign all non-resolved cases asnon-abbreviations.Apart from the ability of finding abbreviations be-yond the scope of the surface guessing heuristics, thedocument-centered approach also allows for the clas-sification of some potential abbreviations a  ordinarywords, thus reducing the ambiguity for the sentencesplitting module.
The second row of Table 2 showsthe results when we supplemented the surface guess-ing heuristics with the document-centered approach.This alone gave a huge improvement over the surfaceguessing heuristics.Using our abbreviation guessing module and anunlabeled corpus from New York Times 1996 of300,000 words, we compiled a list of 270 abbrevia-tions which we then used in our tagging experimentstogether with the guessing module.
In this list weincluded abbreviations which were identified by ourguesser and which had a frequency of five or greater.When we combined the guessing module togetherwith the induced abbreviation list and applied it tothe Brown Corpus and the WSJ we measured about1% error rate on the identification of abbreviationas can be seen in the third row of Table 2.We also tested our POS tagger and the extendedtagging model in conjunction with the abbreviationguesser only, when the system was not equipped withthe list of abbreviations.
The error rate on capital-ized words went just a bit higher while the error269rate on the sentence boundaries increased by two-three times but still stayed reasonable.
In termsof absolute numbers, the tagger achieved a 0.98%error rate on the Brown Corpus and a 1.95% er-ror rate on the WSJ when disarnbiguating sentenceboundaries.
The extended system without the ab-breviation list was about 30% more accurate andachieved a 0.65% error rate on sentence splitting onthe Brown Corpus and 1.39% on the WSJ corpus asshown in the last row of Table 1.
The larger im-pact on the WSJ corpus can be explained by thefact that it has a higher proportion of abbreviationsthan the Brown Corpus.
In the Brown Corpus, 8%of potential sentence boundaries come after abbre-viations.
Tile WSJ is richer in abbreviations and17% of potential sentence boundaries come after ab-breviations.
Thus, unidentified abbreviations had ahigher impact on the error rate in the WSJ.6 Conc lus ionIn this paper we presented an approach which treatsthe sentence boundary disambiguation problem aspart of POS tagging.
In its "vanilla" version the sys-tem performed above the results recently quoted inthe literature for the SBD task.
When we combinedthe "vanilla" model with the document-centered ap-proach to proper name handling we measured abouta 20% further improvement in the performance onsentence splitting and about a 40% improvement oncapitalized word assignment.POS tagging approach to sentence splitting pro-duces models which are highly portable across differ-ent corpora: POS categories are much more frequentthan individual words and less affected by unseenwords.
This differentiates our approach from word-based sentence splitters.
In contrast o (Palmer andHearst, 1997), which also used POS categories aspredictive features, we relied on a proper POS tag-ging technology, rather than a shortcut o POS tagestimation.
This ensured higher accuracy of thePOS tagging method which cut the error rate of theSATZ system by 69%.
On the other hand because ofits simplicity the SATZ approach is probably easierto implement and faster to train than a POS tagger.On single-case texts the syntactic approach didnot show a considerable advantage to the word-basedmethods: all periods which followed abbreviationswere assigned as "sentence internal" and the resultsachieved by our system on the single-case texts werein line with that of the other systems.The abbreviation guessing module which com-bines the surface guessing heuristics with the doc-ument centered approach makes our system very ro-bust to new domains.
The system demonstratedstrong performance ven without being equippedwith a list of known abbreviations which, to ourknowledge, none of previously described SBD sys-tems could achieve.Another important advantage of our approach wesee is that it requires potentially a smaller amountof training data and this training data does not needto be labeled in any way.
In training a conventionalsentence splitter one usually collects periods withthe surrounding context and these samples have tobe manually labeled.
In our case a POS taggingmodel is trained on all available words, so syntacticdependencies between words which can appear in alocal context of a period can be established fromother parts of the text.
Our system does not requireannotated ata for training and can be unsupervis-edly trained from raw texts of approximately 300,000words or more.There are ways for further improvement of theperformance of our system by combining it with aword-based system which encodes pecific behaviorfor individual words.
This is similar to how theSATZ system was combined with the Alembic sys-tem.
This addresses the limitation of our syntacticapproach in treating cases when an abbreviation isfollowed by a proper name always as "non sentenceboundary".
In fact we encoded one simple rule thatan abbreviation which stands for an American state(e.g.
Ala. or Kan.) always is sentence terminal iffollowed by a proper name.
This reduced the errorrate on the WSJ from 0.31% to 0.25%.
Another av-enue for further development is to extend the systemto other languages.ReferencesAberdeen, J., J Burger, D. Day, L. Hirschman,P.
Robinson, and M. Vilain.
1995.
Mitre: De-scription of the alembic system used for muc-6.In The Proceedings of the Sixth Message Under-standing Conference (MUC-6), Columbia, Mary-land.
Morgan Kaufmann.Baum, L.E.
1972.
An inequality and associatedmaximization techique in statistical estimation forprobabilistic functions of a Markov process.
In-equalities 3 (1972) 1-8.Kupiec, Julian.
1992.
Robust part-of-speech taggingusing a hidden markov model.
Computer Speechand Language.Marcus, Mitchell, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Com-putational Linguistics, 19(2):313-329.Mikheev, A.
1999.
A knowledge-free method forcapitalized word disambiguation.
In Proceedingsof the 37th Conference of the Association forComputational Linguistics (ACL'99), pages 159-168.
University of Maryland.Mikheev, A., 1997.
LT POS - the LTG part o/speechtagger.
Language Technology Group, Universityof Edinburgh.
www.ltg.ed.ac.uk/software/pos.270Palmer, D. D. and M. A. Hearst.
1997.
Adaptivemultilingual sentence boundary disambiguation.Computational Linguistics.Reynar, J. C. and A. Ratnaparkhi.
1997.
A max-imum entropy approach to identifying sentenceboundaries.
In Proceedings of the Fifth A CL Con-ference on Applied Natural Language Processing(ANLP'97), Washington, D.C.Riley, M.D.
1989.
Some applications of tree-basedmodeling to speech and language indexing.
InProceedings of the DARPA Speech and Natu-ral Language Workshop, pages 339-352.
MorganKaufman.Viterbi, A.J.
1967.
Error bounds for convolutionalcodes and an asymptomatically optimal decod-ing algorithm.
IEEE Transactions on InformationTheory.271
