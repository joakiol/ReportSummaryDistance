Proceedings of NAACL-HLT 2013, pages 335?341,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsDiscriminative Training of 150 Million Translation Parametersand Its Application to PruningHendra Setiawan and Bowen ZhouIBM T.J. Watson Research CenterYorktown Heights, NY 10598, USA{hendras, zhou}@us.ibm.comAbstractUntil recently, the application of discrimina-tive training to log linear-based statistical ma-chine translation has been limited to tuningthe weights of a limited number of features ortraining features with a limited number of pa-rameters.
In this paper, we propose to scaleup discriminative training of (He and Deng,2012) to train features with 150 million pa-rameters, which is one order of magnitudehigher than previously published effort, andto apply discriminative training to redistributeprobability mass that is lost due to modelpruning.
The experimental results confirm theeffectiveness of our proposals on NIST MT06set over a strong baseline.1 IntroductionState-of-the-art statistical machine translation sys-tems based on a log-linear framework are parame-terized by {?,?
}, where the feature weights ?
arediscriminatively trained (Och and Ney, 2002; Chi-ang et al 2008b; Simianer et al 2012) by directlyoptimizing them against a translation-oriented met-ric such as BLEU.
The feature parameters ?
canbe roughly divided into two categories: dense fea-ture that measures the plausibility of each translationrule from a particular aspect, e.g., the rule transla-tion probabilities p(f |e) and p(e|f); and sparse fea-ture that fires when certain phenomena is observed,e.g., when a frequent word pair co-occured in a rule.In contrast to ?, feature parameters in ?
are usuallymodeled by generative models for dense features, orby indicator functions for sparse ones.
It is thereforedesirable to train the dense features for each rule in adiscriminative fashion to maximize some translationcriterion.
The maximum expected BLEU training of(He and Deng, 2012) is a recent effort towards thisdirection, and in this paper, we extend their workto a scaled-up task of discriminative training of thefeatures of a strong hierarchical phrase-based modeland confirm its effectiveness empirically.In this work, we further consider the applicationof discriminative training to pruned model.
Variouspruning techniques (Johnson et al 2007; Zens et al2012; Eck et al 2007; Lee et al 2012; Tomeh et al2011) have been proposed recently to filter transla-tion rules.
One common consequence of pruning isthat the probability distribution of many survivingrules become deficient, i.e.
?f p(f |e) < 1.
In prac-tice, others have chosen either to leave the prunedrules as it-is, or simply to re-normalize the proba-bility mass by distributing the pruned mass to sur-viving rules proportionally.
We argue that both ap-proaches are suboptimal, and propose a more prin-cipled method to re-distribute the probability mass,i.e.
using discriminative training with some trans-lation criterion.
Our experimental results demon-strate that at various pruning levels, our approachimproves performance consistently.
Particularly atthe level of 50% of rules being pruned, the discrimi-natively trained models performs better than the un-pruned baseline grammar.
This shows that discrim-inative training makes it possible to achieve smallermodels that perform comparably or even better thanthe baseline model.Our contributions in this paper are two-folded:First of all, we scale up the maximum expectedBLEU training proposed in (He and Deng, 2012) ina number of ways including using 1) a hierarchicalphrase-based model, 2) a richer feature set, and 3) alarger training set with a much larger parameter set,resulting in more than 150 million parameters in themodel being updated, which is one order magnitudehigher than the phrase-based model reported in (Heand Deng, 2012).
We are able to show a reasonable335improvement over this strong baseline.
Secondly,we combine discriminative training with pruningtechniques to reestimate parameters of pruned gram-mar.
Our approach is shown to alleviate the loss dueto pruning, and sometimes can even outperform thebaseline unpruned grammar.2 Discriminative Training of ?Given the entire training data {Fn, En}Nn=1, and cur-rent parameterization {?,?
}, we decode the sourceside of training data Fn to produce hypothesis{E?n}Nn=1.
Our goal is to update ?
towards ??
thatmaximizes the expected BLEU scores of the entiretraining data given the current ?:U(?)=??E?1...E?NP??
(E?1..E?N |F1..FN )B(E?1..E?N ) (1)where B(E?1...E?N ) is the BLEU score of the con-catenated hypothesis of the entire training data, fol-lowing (He and Deng, 2012).Eq.
1 summarizes over all possible combina-tions of E?1...E?N , which is intractable.
Hence wemake two simplifying approximations as follows.First, let the k-best hypotheses of the n-th sen-tence, E?n ={E?1n, ..., E?Kn}, approximate all itspossible translation.
In other words, we assumethat?Kk=1 P?
(E?kn|Fn) = 1, ?n.
Second, let thesum of sentence-level BLEU approximate the cor-pus BLEU.
We note that corpus BLEU is not strictlydecomposable (Chiang et al 2008a), however, asthe training data?s size N gets big as in our case, weexpect them to become more positively correlated.Under these assumptions and the fact that eachsentence is decoded independently, Eq.
1 can be al-gebraically simplified into:U(?)
=N?n=1K?k=1P?
(E?kn|Fn)B(E?kn) (2)where P?(E?kn|Fn)=P??(E?kn|Fn)/?
?k P??
(E?kn|Fn).We detail the process in the Appendix.To further simplify the problem and relate it withmodel pruning, we consider to update a subset of?
?
?
while keeping other parameterization of ?unchanged, where ?
= {?ij = p(ej |fi)} denotes ourparameter set that satisfies?j ?ij = 1 and ?ij ?
0.In experiments, we also consider {?ji = p(fi|ej)}.To alleviate overfitting, we introduce KL-distancebased reguralization as in (He and Deng, 2012).
Wethus arrive at the following objective function:O(?)
= log(U(?))?
?
?KL(?||?0)/N (3)where ?
controls the regularization term?s contribu-tion, and ?0 represents a prior parameter set, e.g.,from the conventional maximum likelihood training.The optimization algorithm is based on the Ex-tended Baum Welch (EBW) (Gopalakrishnan et al1991) as derived by (He and Deng, 2012).
The finalupdate rule is as follow:?
?ij =?n?k ?
(n, k, i, j) + U(?)?
?0ij/?+Di?ij?n?k?j ?
(n, k, i, j) + U(?
)?/?+Di(4)where ?
?ij is the updated parameter, ?
(n, k, i, j) =P?
(E?kn|Fn){B(E?kn) ?
Un(?
)}?l 1(fn,k,l =fi, en,k,l = ej); Un(?)
=?Kk=1 P?
(E?kn|Fn)B(E?kn);Di =?n,k,j max(0,??
(n, k, i, j)) and ?
is thecurrent feature?s weight.3 DT is Beneficial for PruningPruning is often a key part in deploying large-scaleSMT systems for many reasons, such as for reduc-ing runtime memory footprint and for efficiency.Many pruning techniques have been proposed to as-sess translation rules and filter rules out if they areless plausible than others.
While different pruningtechniques may use different criterion, they all as-sume that pruning does not affect the feature func-tion values of the surviving rules.
This assumptionmay be suboptimal for some feature functions thathave probabilistic sense since pruning will removea portion of the probability mass that is previouslyassigned to the pruned rules.
To be concrete, for therule translation probabilities ?ij under consideration,the constraint?j ?ij = 1 will not hold for all sourcerules i after pruning.
Previous works typically leftthe probability mass as it-is, or simply renormalizethe pruned mass, i.e.
?
?ij = ?ij/?j ?ij .We argue that applying the DT techniques to apruned grammar, as described in Sec.
2, providesa more principled method to redistribute the mass,i.e.
by quantizing how each rule contributes to theexpected BLEU score in comparison to other com-peting rules.
To empirically verify this, we consider336the significance test based pruning (Johnson et al2007), though our general idea can be appllied toany pruning techniques.
For our experiments, weuse the significance pruning tool that is available aspart of Moses decoder package (Koehn et al 2007).4 ExperimentsOur experiments are designed to serve two goals:1) to show the performance of discriminative train-ing of feature parameters ?
in a large-scale task;and 2) to show the effectiveness of DT when ap-plied to pruned grammar.
Our baseline system is astate-of-the-art hierarchical phrase-based system asdescribed in (Zhou et al 2008), trained on six mil-lion parallel sentences corpora that are available tothe DARPA BOLT Chinese-English task.
The train-ing corpora includes a mixed genre of news wire,broadcast news, web-blog and comes from varioussources such as LDC, HK Hansard and UN data.In total, there are 50 dense features in our trans-lation system.
In addition to the standard featureswhich include the rule translation probabilities, weincorporate features that are found useful for devel-oping a state-of-the-art baseline, e.g.
provenance-based lexical features (Chiang et al 2011).
We usea large 6-gram language model, which we train on a10 billion words monolingual corpus, including theEnglish side of our parallel corpora plus other cor-pora such as Gigaword (LDC2011T07) and GoogleNews.
To prevent possible over-fitting, we only keptthe rules that have at most three terminal words (plusup to two nonterminals) on the source side, resultingin a grammar with 167 million rules.Our discriminative training procedure includesupdating both ?
and ?, and we follow (He and Deng,2012) to optimize them in an alternate manner.
Thatis, when we optimize ?
via EBW, we keep ?
fixedand when we optimize ?, we keep ?
fixed.
We usePRO (Hopkins and May, 2011) to tune ?.For discriminative training of ?, we use a subsetof 550 thousands of parallel sentences selected fromthe entire training data, mainly to allow for faster ex-perimental cycle; they mainly come from news andweb-blog domains.
For each sentence of this subset,we generate 500-best of unique hypotheses using thebaseline model.
The 1-best and the oracle BLEUscores for this subset are 40.19 and 47.06 respec-tively.
Following (He and Deng, 2012), we focus ondiscriminative training of p(f |e) and p(e|f), whichin practice affects around 150 million of parameters;hence the title.For the tuning and development sets, we setaside 1275 and 1239 sentences respectively fromLDC2010E30 corpus.
The tune set is used by PROfor tuning ?
while the dev set is used to decide thebest DT model.
As for the blind test set, we re-port the performance on the NIST MT06 evaluationset, which consists of 1644 sentences from news andweb-blog domains.
Our baseline system?s perfor-mance on MT06 is 39.91 which is among the bestnumber ever published so far in the community.Table 1 compares the key components of ourbaseline system with that of (He and Deng, 2012).As shown, we are working with a stronger systemthan (He and Deng, 2012), especially in terms of thenumber of parameters under consideration |?|.He&Deng(2012) This paperModel phrase-based hierarchicaln-gram lm 3-gram 6-gram# features 10 50Max terminal 4 3|?| 9.2 M 150M# training data 750K 6MN for DT 750K 550Kmax K-best 100 500Table 1: Our system compares to He&Deng?s (2012).4.1 DT of 150 Million ParametersTo ensure the correctness of our implementation,we show in Fig 2, the first five EBW updates with?
= 0.10.
As shown, the utility function log(U(?
))increases monotonically but is countered by the KLterm, resulting in a smaller but consistent increaseof the objective function O(?).
This monotonically-increasing trend of the objective function confirmsthe correctness of our implementation since EBWalgorithm is a bound-based technique that ensuresgrowth transformations between updates.We then explore the optimal setting for ?
whichcontrols the contribution of the regularization term.Specifically, we perform grid search, exploring val-ues of ?
from 0.1 to 0.75.
For each ?
, we run severaliterations of discriminative training where each it-eration involves one simultaneous update of p(f |e)337and p(e|f) according to Eq.
4, followed by one up-date of ?
via PRO (as in (He and Deng, 2012)).
Intotal, we run 10 such iterations for each ?
.tau=0.01N tau=0.05N tau=0.10N tau=0.25N tau=0.50N tau=0.75N tau=0.100 32.22 32.22 32.22 32.22 32.22 32.22 0 32.221 32.33 32.24 32.39 32.42 32.5 32.34 1 32.392 32.39 32.34 32.63 32.45 32.41 32.33 2 32.633 32.37 32.29 32.54 32.32 32.24 32.45 3 32.544 32.35 32.18 32.41 32.45 32.41 32.38 4 32.415 32.38 32.21 32.45 32.62 32.31 32.08 5 32.456 32.26 32.27 32.68 32.45 32.26 32.28 6 32.687 32.17 32.15 32.45 32.54 32.37 32.15 7 32.458 31.93 32.26 32.29 32.56 32.25 32.31 8 32.299 32.1 32.36 32.25 32.33 32.23 32.54 9 32.2510 32.1 32.29 32.2 32.42 32.29 32.31 10 32.232.39 32.36 32.68 32.62 32.5 32.5433.090.01 0.05 0.1 0.25 0.5 0.750.01 (2) 0.05 (9) 0.10 (6) 0.25 (5) 0.50 (1) 0.75 (9)32.39 32.36 32.68 32.62 32.5 32.5432.22 32.22 32.22 32.22 32.22 32.2232.2$32.3$32.4$32.5$32.6$32.7$32.8$0$ 1$ 2$ 3$ 4$ 5$ 6$ 7$tau=0.10$32.39$ 32.36$32.68$ 32.62$32.5$ 32.54$32.22$32.2$32.25$32.3$32.35$32.4$32.45$32.5$32.55$32.6$32.65$32.7$0.01$(2)$ 0.05$(9)$ 0.10$(6)$ 0.25$(5)$ 0.50$(1)$ 0.75$(9)$DT$baseline$?
= 0.01NFigure 1: The dev set?s BLEU score (y-axis) on differentsetting of ?
(x-axis).
The grey line indicates the baselineperformance on dev set.
The number in bracket on the x-axis indicates the iteration at which the score is obtained.Across different ?
, we find that the first iterationprovides most of the gain while the subsequent iter-ations provide additional, smaller gain with occas-sional performance degradation; thus the translationperformance is not always monotonically increasingover iteration.
We report the best score of each ?
inFig.
1 and at which iteration that score is produced.As shown in Fig.
1, all settings of ?
improve over thebaseline and ?
= 0.10 gives the highest gain of 0.45BLEU score.
This improvement is in the same ball-park as in (He and Deng, 2012) though on a scaled-up task.
We next decode the MT06 using the bestmodel (i.e.
?
= 0.10 at 6-th iteration) observed onthe dev set, and obtained 40.33 BLEU with an im-provement of around 0.4 BLEU point.
We see thisresult as confirming the effectiveness of discrimi-native training but on a larger-scale task, adding towhat was reported by (He and Deng, 2012).4.2 DT for Significance PruningNext, we show the contribution of discriminativetraining for model pruning.
To do so, we prune thetranslation grammar so that its size becomes 50%,25%, 10% of the original grammar.
Respectively,we delete rules whose significance value below 15,50 and 500.
Table 2 compares the statistics of thepruned grammars and the unpruned one.
In particu-lar, columns 4 and 5 show the total averaged prob-ability mass of the remaining rules.
This statisticsprovides some indication of how deficient the fea-Figure 2: Objective function (O(??
)), the regularizationterm (KL(??))
and the unregularized objective function(log(U(??)))
for five EBW updates of updating p(ej |fi)tures are after pruning.
As shown, the total averagedprobability mass after pruning is below 100% andeven lower for the more aggressive pruning.To show that the deficiency is suboptimal, we con-siders two baseline systems: models with/withoutmass renormalization.
We tune a new ?
for eachmodel and use the new ?
to decode the dev and testsets.
The results are shown in columns 6 and 9 ofTable 2 where we show the results for the unnor-malized model in the brackets following the resultsfor the re-normalized model.
The results show thatpruning degrades the performances and that naivelyre-normalizing the model provides no significantchanges in performance.
Subsequently, we will fo-cus on the normalized models as the baseline as theyrepresents the starting points of our EBW iteration.Next, we run discriminative training that wouldreassign the probability mass to the surviving rules.First, we normalize p(f |e) and p(e|f), so that theysatisfy the sum to one constraint required by the al-gorithm.
Then, we run discriminative training onthese pruned grammars using ?
= 0.10 (i.e.
thesetting that gives the best performance for the un-pruned grammar as discussed in Section 4.1).
Wereport the results in columns 7 and 9 for the dev andtest sets respectively, as well as the gain over thebaseline system in columns 8 and 10.As shown in Table 2, DT provides a nice im-provement over the baseline model of no mass re-assignment.
For all pruning levels, DT can compen-sate the loss associated with pruning.
In particular,at 50% level of pruning, there is a loss about 0.4338size |f | |e| p(?|e) p(?|f) dev-set test-set (MT06)(%) (M) (M) baseline (un) DT (iter) gain baseline (un) DT gain100 59 50 1.00 1.00 32.22(32.08) 32.68 (6) +0.44 39.91 (39.71) 40.33 +0.4250 38 35 0.92 0.94 31.84 (32.02) 32.31 (6) +0.57 39.61(39.72) 40.08 +0.4725 14 14 0.87 0.91 31.39 (31.43) 31.68 (2) +0.29 39.23 (39.17) 39.43 +0.2010 4 3 0.77 0.84 27.27 (27.10) 27.82 (2) +0.55 36.01 (36.04) 36.43 +0.42Table 2: The statistics of grammars pruned at various level (column 1), including the number of unique source andtarget phrases (columns 2 & 3), total probability mass of the remaining rules for p(f |e) and p(e|f) (columns 4 & 5),the performance of the pruned model before and after discriminative training as well as the gain on the dev and thetest sets (columns 6 to 11).
The iteration at which DT gives the best dev set is indicated by the number enclosed bybracket in column 7.
The baseline performance is in italics, followed by a number in the bracket which refers to theperformance of using unnormalized model.
The above-the-baseline performances are in bold.BLEU point after pruning.
With the DT on prunedmodel, all pruning losses are reclaimed and the newpruned model is even better than the unpruned orig-inal model.
This empirical result shows that leavingprobability mass unassigned after pruning is sub-optimal and that discriminative training provides aprincipled way to redistribute the mass.5 ConclusionIn this paper, we first extend the maximum expectedBLEU training of (He and Deng, 2012) to traintwo features of a state-of-the-art hierarchical phrase-based system, namely: p(f |e) and p(e|f).
Com-pared to (He and Deng, 2012), we apply the algo-rithm to a strong baseline that is trained on a big-ger parallel corpora and comes with a richer featureset.
The number of parameters under considerationamounts to 150 million.
Our experiments show thatdiscriminative training these two features (out of 50)gives around 0.40 BLEU point improvement, whichis consistent with the conclusion of (He and Deng,2012) but in a much larger-scale system.Furthermore, we apply the algorithm to redis-tribute the probability mass of p(f |e) and p(e|f) thatis commonly lost due to conventional model prun-ing.
Previous techniques either leave the probabilitymass as it is or distribute it proportionally among thesurviving rules.
We show that our proposal of us-ing discriminative training to redistribute the massempirically performs better, demonstrating the ef-fectiveness of our proposal.AppendixWe describe the process to simplify Eq.
1 to Eq.
2,which is omitted in (He and Deng, 2012).
For con-ciseness, we drop the conditions and write P (E?i|Fi)as P (E?i).
We write Eq.
1 again below as Eq.
5 .?
?E?1...E?NN?i=1P (E?i|Fi) ?N?i=1B(E?i) (5)We first focus on the first sentence E1/F1 and ex-pand the related terms from the equation as follow:??E?1?
?E?2...E?NP (E?1)N?i=2P (E?i).
[B(E?1)+N?i=2B(E?i)]Expanding the inner summation, we arrive at:?
?E?1P (E?1)B(E?1)?
?E?2...E?NN?i=2P (E?i) +?
?E?1P (E?1)?
?E?2...E?NN?i=2P (E?i)N?i=2B(E?i)Due to the that?Kk=1 P?
(E?Kn |Fn) = 1, we canequate?
?E?2...E?N?Ni=2 P (E?i) and?
?E?1P (E?1) to1.
Thus, we arrive at:?
?E?1P (E?1)B(E?1) +?
?E?2...E?NN?i=2P (E?i)N?i=2B(E?i)Notice that the second term has the same formas Eq.
5 except that the starting index starts fromthe second sentence.
The same process can be per-formed and at the end, thus we can arrive at Eq.
2.339AcknowledgmentsWe thank Xiadong He for helpful discussions.
Wewould like to acknowledge the support of DARPAunder Grant HR0011-12-C-0015 for funding part ofthis work.
The views, opinions, and/or findings con-tained in this article/presentation are those of the au-thor/presenter and should not be interpreted as rep-resenting the official views or policies, either ex-pressed or implied, of the DARPA.ReferencesDavid Chiang, Steve DeNeefe, Yee Seng Chan, andHwee Tou Ng.
2008a.
Decomposability of transla-tion metrics for improved evaluation and efficient al-gorithms.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 610?619, Honolulu, Hawaii, October.
Associa-tion for Computational Linguistics.David Chiang, Yuval Marton, and Philip Resnik.
2008b.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 224?233, Honolulu, Hawaii,October.
Association for Computational Linguistics.David Chiang, Steve DeNeefe, and Michael Pust.
2011.Two easy improvements to lexical weighting.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 455?460, Portland, Oregon, USA,June.
Association for Computational Linguistics.Matthias Eck, Stephan Vogel, and Alex Waibel.
2007.Translation model pruning via usage statistics for sta-tistical machine translation.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Companion Volume, Short Papers,pages 21?24, Rochester, New York, April.
Associationfor Computational Linguistics.P.
S. Gopalakrishnan, Dimitri Kanevsky, Arthur Na?das,and David Nahamoo.
1991.
An inequality for ratio-nal functions with applications to some statistical esti-mation problems.
IEEE Transactions on InformationTheory, 37(1):107?113.Xiaodong He and Li Deng.
2012.
Maximum expectedbleu training of phrase and lexicon translation mod-els.
In Proceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 292?301, Jeju Island, Korea, July.Association for Computational Linguistics.Mark Hopkins and Jonathan May.
2011.
Tuning as rank-ing.
In Proceedings of the 2011 Conference on Empir-ical Methods in Natural Language Processing, pages1352?1362, Edinburgh, Scotland, UK., July.
Associa-tion for Computational Linguistics.Howard Johnson, Joel Martin, George Foster, and RolandKuhn.
2007.
Improving translation quality by dis-carding most of the phrasetable.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 967?975, Prague, Czech Republic, June.
Association forComputational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics Companion Volume Pro-ceedings of the Demo and Poster Sessions, pages 177?180, Prague, Czech Republic, June.
Association forComputational Linguistics.Seung-Wook Lee, Dongdong Zhang, Mu Li, Ming Zhou,and Hae-Chang Rim.
2012.
Translation model sizereduction for hierarchical phrase-based statistical ma-chine translation.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Linguis-tics (Volume 2: Short Papers), pages 291?295, Jeju Is-land, Korea, July.
Association for Computational Lin-guistics.Franz Josef Och and Hermann Ney.
2002.
Discrimi-native training and maximum entropy models for sta-tistical machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 295?302, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Patrick Simianer, Stefan Riezler, and Chris Dyer.
2012.Joint feature selection in distributed stochastic learn-ing for large-scale discriminative training in smt.
InProceedings of the 50th Annual Meeting of the Associ-ation for Computational Linguistics (Volume 1: LongPapers), pages 11?21, Jeju Island, Korea, July.
Asso-ciation for Computational Linguistics.N.
Tomeh, M. Turchi, G. Wisniewski, A. Allauzen, andF.
Yvon.
2011.
How good are your phrases?
assess-ing phrase quality with single class classification.
InProceedings of the International Workshop on SpokenLanguage Translation, pages 261?268.Richard Zens, Daisy Stanton, and Peng Xu.
2012.
Asystematic comparison of phrase table pruning tech-niques.
In Proceedings of the 2012 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,340pages 972?983, Jeju Island, Korea, July.
Associationfor Computational Linguistics.Bowen Zhou, Bing Xiang, Xiaodan Zhu, and YuqingGao.
2008.
Prior derivation models for formallysyntax-based translation using linguistically syntacticparsing and tree kernels.
In Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure inStatistical Translation (SSST-2), pages 19?27, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.341
