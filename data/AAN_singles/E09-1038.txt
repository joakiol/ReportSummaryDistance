Proceedings of the 12th Conference of the European Chapter of the ACL, pages 327?335,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsEnhancing Unlexicalized Parsing Performanceusing a Wide Coverage Lexicon, Fuzzy Tag-set Mapping,and EM-HMM-based Lexical ProbabilitiesYoav Goldberg1?
Reut Tsarfaty2?
Meni Adler1?
Michael Elhadad11Department of Computer Science, Ben Gurion University of the Negev{yoavg|adlerm|elhadad}@cs.bgu.ac.il2Institute for Logic, Language and Computation, University of AmsterdamR.Tsarfaty@uva.nlAbstractWe present a framework for interfacinga PCFG parser with lexical informationfrom an external resource following a dif-ferent tagging scheme than the treebank.This is achieved by defining a stochas-tic mapping layer between the two re-sources.
Lexical probabilities for rareevents are estimated in a semi-supervisedmanner from a lexicon and large unanno-tated corpora.
We show that this solu-tion greatly enhances the performance ofan unlexicalized Hebrew PCFG parser, re-sulting in state-of-the-art Hebrew parsingresults both when a segmentation oracle isassumed, and in a real-word parsing sce-nario of parsing unsegmented tokens.1 IntroductionThe intuition behind unlexicalized parsers is thatthe lexicon is mostly separated from the syntax:specific lexical items are mostly irrelevant for ac-curate parsing, and can be mediated through theuse of POS tags and morphological hints.
Thissame intuition also resonates in highly lexicalizedformalism such as CCG: while the lexicon cate-gories are very fine grained and syntactic in na-ture, once the lexical category for a lexical item isdetermined, the specific lexical form is not takeninto any further consideration.Despite this apparent separation between thelexical and the syntactic levels, both are usually es-timated solely from a single treebank.
Thus, while?Supported by the Lynn and William Frankel Center forComputer Sciences, Ben Gurion University?Funded by the Dutch Science Foundation (NWO), grantnumber 017.001.271.?Post-doctoral fellow, Deutsche Telekom labs at Ben Gu-rion UniversityPCFGs can be accurate, they suffer from vocabu-lary coverage problems: treebanks are small andlexicons induced from them are limited.The reason for this treebank-centric view inPCFG learning is 3-fold: the English treebank isfairly large and English morphology is fairly sim-ple, so that in English, the treebank does providemostly adequate lexical coverage1; Lexicons enu-merate analyses, but don?t provide probabilitiesfor them; and, most importantly, the treebank andthe external lexicon are likely to follow differentannotation schemas, reflecting different linguisticperspectives.On a different vein of research, current POS tag-ging technology deals with much larger quantitiesof training data than treebanks can provide, andlexicon-based unsupervised approaches to POStagging are practically unlimited in the amountof training data they can use.
POS taggers relyon richer knowledge than lexical estimates de-rived from the treebank, have evolved sophisti-cated strategies to handle OOV and can providedistributions p(t|w, context) instead of ?best tag?only.Can these two worlds be combined?
We pro-pose that parsing performance can be greatly im-proved by using a wide coverage lexicon to sug-gest analyses for unknown tokens, and estimatingthe respective lexical probabilities using a semi-supervised technique, based on the training pro-cedure of a lexicon-based HMM POS tagger.
Formany resources, this approach can be taken onlyon the proviso that the annotation schemes of thetwo resources can be aligned.We take Modern Hebrew parsing as our casestudy.
Hebrew is a Semitic language with rich1This is not the case with other languages, and also nottrue for English when adaptation scenarios are considered.327morphological structure.
This rich structure yieldsa large number of distinct word forms, resulting ina high OOV rate (Adler et al, 2008a).
This posesa serious problem for estimating lexical probabili-ties from small annotated corpora, such as the He-brew treebank (Sima?an et al, 2001).Hebrew has a wide coverage lexicon /morphological-analyzer (henceforth, KC Ana-lyzer) available2, but its tagset is different than theone used by the Hebrew Treebank.
These are notmere technical differences, but derive from dif-ferent perspectives on the data.
The Hebrew TBtagset is syntactic in nature, while the KC tagsetis lexicographic.
This difference in perspectiveyields different performance for parsers inducedfrom tagged data, and a simple mapping betweenthe two schemes is impossible to define (Sec.
2).A naive approach for combining the use of thetwo resources would be to manually re-tag theTreebank with the KC tagset, but we show this ap-proach harms our parser?s performance.
Instead,we propose a novel, layered approach (Sec.
2.1),in which syntactic (TB) tags are viewed as contex-tual refinements of the lexicon (KC) tags, and con-versely, KC tags are viewed as lexical clusteringof the syntactic ones.
This layered representationallows us to easily integrate the syntactic and thelexicon-based tagsets, without explicitly requiringthe Treebank to be re-tagged.Hebrew parsing is further complicated by thefact that common prepositions, conjunctions andarticles are prefixed to the following word andpronominal elements often appear as suffixes.
Thesegmentation of prefixes and suffixes can be am-biguous and must be determined in a specific con-text only.
Thus, the leaves of the syntactic parsetrees do not correspond to space-delimited tokens,and the yield of the tree is not known in advance.We show that enhancing the parser with externallexical information is greatly beneficial, both in anartificial scenario where the token segmentation isassumed to be known (Sec.
4), and in a more re-alistic one in which parsing and segmentation arehandled jointly by the parser (Goldberg and Tsar-faty, 2008) (Sec.
5).
External lexical informa-tion enhances unlexicalized parsing performanceby as much as 6.67 F-points, an error reductionof 20% over a Treebank-only parser.
Our resultsare not only the best published results for pars-ing Hebrew, but also on par with state-of-the-art2http://mila.cs.technion.ac.il/hebrew/resources/lexicons/lexicalized Arabic parsing results assuming gold-standard fine-grained Part-of-Speech (Maamouriet al, 2008).32 A Tale of Two ResourcesModern Hebrew has 2 major linguistic resources:the Hebrew Treebank (TB), and a wide coverageLexicon-based morphological analyzer developedand maintained by the Knowledge Center for Pro-cessing Hebrew (KC Analyzer).The Hebrew Treebank consists of sentencesmanually annotated with constituent-based syn-tactic information.
The most recent version (V2)(Guthmann et al, 2009) has 6,219 sentences, andcovers 28,349 unique tokens and 17,731 uniquesegments4.The KC Analyzer assigns morphological analy-ses (prefixes, suffixes, POS, gender, person, etc.
)to Hebrew tokens.
It is based on a lexicon ofroughly 25,000 word lemmas and their inflectionpatterns.
From these, 562,439 unique word formsare derived.
These are then prefixed (subject toconstraints) by 73 prepositional prefixes.It is interesting to note that even with thesenumbers, the Lexicon?s coverage is far from com-plete.
Roughly 1,500 unique tokens from the He-brew Treebank cannot be assigned any analysisby the KC Lexicon, and Adler et al(2008a) reportthat roughly 4.5% of the tokens in a 42M tokenscorpus of news text are unknown to the Lexicon.For roughly 400 unique cases in the Treebank, theLexicon provides some analyses, but not a correctone.
This goes to emphasize the productive natureof Hebrew morphology, and stress that robust lex-ical probability estimates cannot be derived froman annotated resource as small as the Treebank.Lexical vs. Syntactic POS Tags The analysesproduced by the KC Analyzer are not compatiblewith the Hebrew TB.The KC tagset (Adler et al, 2008b; Netzer etal., 2007; Adler, 2007) takes a lexical approach toPOS tagging (?a word can assume only POS tagsthat would be assigned to it in a dictionary?
), whilethe TB takes a syntactic one (?if the word in thisparticular positions functions as an Adverb, tag itas an Adverb, even though it is listed in the dictio-nary only as a Noun?).
We present 2 cases that em-phasize the difference: Adjectives: the Treebank3Our method is orthogonal to lexicalization and can beused in addition to it if one so wishes.4In these counts, all numbers are conflated to one canoni-cal form328treats any word in an adjectivial position as an Ad-jective.
This includes also demonstrative pronouns??
???
(this boy).
However, from the KC point ofview, the fact that a pronoun can be used to modifya noun does not mean it should appear in a dictio-nary as an adjective.
The MOD tag: similarly,the TB has a special POS-tag for words that per-form syntactic modification.
These are mostly ad-verbs, but almost any Adjective can, in some cir-cumstances, belong to that class as well.
This cat-egory is highly syntactic, and does not conform tothe lexicon based approach.In addition, many adverbs and prepositions inHebrew are lexicalized instances of a prepositionfollowed by a noun (e.g., ????
?, ?in+softness?,softly).
These can admit both the lexical-ized and the compositional analyses.
Indeed,many words admit the lexicalized analyses inone of the resource but not in the other (e.g.,?????
?for+benefit?
is Prep in the TB but onlyPrep+Noun in the KC, while for ???
?from+side?it is the other way around).2.1 A Unified ResourceWhile the syntactic POS tags annotation of the TBis very useful for assigning the correct tree struc-ture when the correct POS tag is known, there areclear benefits to an annotation scheme that can beeasily backed by a dictionary.We created a unified resource, in which everyword occurrence in the Hebrew treebank is as-signed a KC-based analysis.
This was done in asemi-automatic manner ?
for most cases the map-ping could be defined deterministically.
The rest(less than a thousand instances) were manually as-signed.
Some Treebank tokens had no analysesin the KC lexicon, and some others did not havea correct analysis.
These were marked as ?UN-KNOWN?
and ?MISSING?
respectively.5The result is a Treebank which is morpho-logically annotated according to two differentschemas.
On average, each of the 257 TB tagsis mapped to 2.46 of the 273 KC tags.6 While thisresource can serve as a basis for many linguisti-cally motivated inquiries, the rest of this paper is5Another solution would be to add these missing cases tothe KC Lexicon.
In our view this act is harmful: we don?twant our Lexicon to artificially overfit our annotated corpora.6A ?tag?
in this context means the complete morphologi-cal information available for a morpheme in the Treebank: itspart of speech, inflectional features and possessive suffixes,but not prefixes or nominative and accusative suffixes, whichare taken to be separate morphemes.devoted to using it for constructing a better parser.Tagsets Comparison In (Adler et al, 2008b),we hypothesized that due to its syntax-based na-ture, the Treebank morphological tagset is moresuitable than the KC one for syntax related tasks.Is this really the case?
To verify it, we simulate ascenario in which the complete gold morpholog-ical information is available.
We train 2 PCFGgrammars, one on each tagged version of the Tree-bank, and test them on the subset of the develop-ment set in which every token is completely cov-ered by the KC Analyzer (351 sentences).7 Theinput to the parser is the yields and disambiguatedpre-terminals of the trees to be parsed.
The parsingresults are presented in Table 1.
Note that this sce-nario does not reflect actual parsing performance,as the gold information is never available in prac-tice, and surface forms are highly ambiguous.Tagging Scheme Precision RecallTB / syntactic 82.94 83.59KC / dictionary 81.39 81.20Table 1: evalb results for parsing with Oraclemorphological information, for the two tagsetsWith gold morphological information, the TBtagging scheme is more informative for the parser.The syntax-oriented annotation scheme of theTB is more informative for parsing than the lexi-cographic KC scheme.
Hence, we would like ourparser to use this TB tagset whenever possible, andthe KC tagset only for rare or unseen words.A Layered Representation It seems that learn-ing a treebank PCFG assuming such a differenttagset would require a treebank tagged with thealternative annotation scheme.
Rather than assum-ing the existence of such an alternative resource,we present here a novel approach in which weview the different tagsets as corresponding to dif-ferent aspects of the morphosyntactic representa-tion of pre-terminals in the parse trees.
Each ofthese layers captures subtleties and regularities inthe data, none of which we would want to (andsometimes, cannot) reduce to the other.
We, there-fore, propose to retain both tagsets and learn afuzzy mapping between them.In practice, we propose an integrated represen-tation of the tree in which the bottommost layerrepresents the yield of the tree, the surface forms7For details of the train/dev splits as well as the grammar,see Section 4.2.329are tagged with dictionary-based KC POS tags,and syntactic TB POS tags are in turn mapped ontothe KC ones (see Figure 1).TB: KC: Layered:...JJ-ZYTB??...PRP-M-S-3-DEMKC??...JJ-ZYTBPRP-M-S-3-DEMKC??...INTB??????...INKC?...NN-F-SKC?????...INTBINKC?NN-F-SKC????
?Figure 1: Syntactic (TB), Lexical (KC) andLayered representationsThis representation helps to retain the informa-tion both for the syntactic and the morphologi-cal POS tagsets, and can be seen as capturing theinteraction between the morphological and syn-tactic aspects, allowing for a seamless integra-tion of the two levels of representation.
We re-fer to this intermediate layer of representation asa morphosyntactic-transfer layer and we formallydepict it as p(tKC |tTB).This layered representation naturally gives riseto a generative model in which a phrase level con-stituent first generates a syntactic POS tag (tTB),and this in turn generates the lexical POS tag(s)(tKC).
The KC tag then ultimately generates theterminal symbols (w).
We assume that a morpho-logical analyzer assigns all possible analyses to agiven terminal symbol.
Our terminal symbols are,therefore, pairs: ?w, t?, and our lexical rules are ofthe form t?
?w, t?.
This gives rise to the follow-ing equivalence:p(?w, tKC?|tTB) = p(tKC |tTB)p(?w, tKC?|tKC)In Sections (4, 5) we use this layered gener-ative process to enable a smooth integration ofa PCFG treebank-learned grammar, an externalwide-coverage lexicon, and lexical probabilitieslearned in a semi-supervised manner.3 Semi-supervised Lexical ProbabilityEstimationsA PCFG parser requires lexical probabilitiesof the form p(w|t) (Charniak et al, 1996).Such information is not readily available inthe lexicon.
However, it can be estimatedfrom the lexicon and large unannotated cor-pora, by using the well-known Baum-Welch(EM) algorithm to learn a trigram HMM taggingmodel of the form p(t1, .
.
.
, tn, w1, .
.
.
, wn) =argmax?p(ti|ti?1, ti?2)p(wi|ti), and takingthe emission probabilities p(w|t) of that model.In Hebrew, things are more complicated, aseach emission w is not a space delimited token, butrather a smaller unit (a morphological segment,henceforth a segment).
Adler and Elhadad (2006)present a lattice-based modification of the Baum-Welch algorithm to handle this segmentation am-biguity.Traditionally, such unsupervised EM-trainedHMM taggers are thought to be inaccurate, but(Goldberg et al, 2008) showed that by feeding theEM process with sufficiently good initial proba-bilities, accurate taggers (> 91% accuracy) can belearned for both English and Hebrew, based on a(possibly incomplete) lexicon and large amount ofraw text.
They also present a method for automat-ically obtaining these initial probabilities.As stated in Section 2, the KC Analyzer (He-brew Lexicon) coverage is incomplete.
Adleret al(2008a) use the lexicon to learn a MaximumEntropy model for predicting possible analyses forunknown tokens based on their orthography, thusextending the lexicon to cover (even if noisily) anyunknown token.
In what follows, we use KC Ana-lyzer to refer to this extended version.Finally, these 3 works are combined to createa state-of-the-art POS-tagger and morphologicaldisambiguator for Hebrew (Adler, 2007): initiallexical probabilities are computed based on theMaxEnt-extended KC Lexicon, and are then fedto the modified Baum-Welch algorithm, which isused to fit a morpheme-based tagging model overa very large corpora.
Note that the emission prob-abilities P (W |T ) of that model cover all the mor-phemes seen in the unannotated training corpus,even those not covered by the KC Analyzer.8We hypothesize that such emission probabili-ties are good estimators for the morpheme-basedP (T ?
W ) lexical probabilities needed by aPCFG parser.
To test this hypothesis, we use itto estimate p(tKC ?
w) in some of our models.4 Parsing with a Segmentation OracleWe now turn to describing our first set of exper-iments, in which we assume the correct segmen-8P (W |T ) is defined also for words not seen during train-ing, based on the initial probabilities calculation procedure.For details, see (Adler, 2007).330tation for each input sentence is known.
This isa strong assumption, as the segmentation stageis ambiguous, and segmentation information pro-vides very useful morphological hints that greatlyconstrain the search space of the parser.
However,the setting is simpler to understand than the onein which the parser performs both segmentationand POS tagging, and the results show some in-teresting trends.
Moreover, some recent studies onparsing Hebrew, as well as all studies on parsingArabic, make this oracle assumption.
As such, theresults serve as an interesting comparison.
Notethat in real-world parsing situations, the parser isfaced with a stream of ambiguous unsegmented to-kens, making results in this setting not indicativeof real-world parsing performance.4.1 The ModelsThe main question we address is the incorporationof an external lexical resource into the parsing pro-cess.
This is challenging as different resources fol-low different tagging schemes.
One way aroundit is re-tagging the treebank according to the newtagging scheme.
This will serve as a baselinein our experiment.
The alternative method usesthe Layered Representation described above (Sec.2.1).
We compare the performance of the two ap-proaches, and also compare them against the per-formance of the original treebank without externalinformation.We follow the intuition that external lexical re-sources are needed only when the informationcontained in the treebank is too sparse.
There-fore, we use treebank-derived estimates for reli-able events, and resort to the external resourcesonly in the cases of rare or OOV words, for whichthe treebank distribution is not reliable.Grammar and Notation For all our experi-ments, we use the same grammar, and changeonly the way lexical probabilities are imple-mented.
The grammar is an unlexicalizedtreebank-estimated PCFG with linguistically mo-tivated state-splits.9In what follows, a lexical event is a word seg-ment which is assigned a single POS thereby func-tioning as a leaf in a syntactic parse tree.
A rare9Details of the grammar: all functional information is re-moved from the non-terminals, finite and non-finite verbs, aswell as possessive and other PPs are distinguished, definite-ness structure of constituents is marked, and parent annota-tion is employed.
It is the same grammar as described in(Goldberg and Tsarfaty, 2008).
(lexical) event is an event occurring less than Ktimes in the training data, and a reliable (lexical)event is one occurring at least K times in the train-ing data.
We use OOV to denote lexical events ap-pearing 0 times in the training data.
count(?)
isa counting function over the training data, rarestands for any rare event, and wrare is a specificrare event.
KCA(?)
is the KC Analyzer function,mapping a lexical event to a set of possible tags(analyses) according to the lexicon.Lexical ModelsAll our models use relative frequency estimatedprobabilities for reliable lexical events: p(t ?w|t) = count(w,t)count(t) .
They differ only in their treat-ment of rare (including OOV) events.In our Baseline, no external resource is used.We smooth for rare and OOV events using a per-tag probability distribution over rare segments,which we estimate using relative frequency overrare segments in the training data: p(wrare|t) =count(rare,t)count(t) .
This is the way lexical probabilitiesin treebank grammars are usually estimated.We experiment with two flavours of lexicalmodels.
In the first, LexFilter, the KC Analyzer isconsulted for rare events.
We estimate rare eventsusing the same per-tag distribution as in the base-line, but use the KC Analyzer to filter out any in-compatible cases, that is, we force to 0 the proba-bility of any analysis not supported by the lexicon:p(wrare|t) ={count(rare,t)count(t) t ?
KCA(wrare)0 t /?
KCA(wrare)Our second flavour of lexical models, Lex-Probs, the KC Analyzer is consulted to proposeanalyses for rare events, and the probability of ananalysis is estimated via the HMM emission func-tion described in Section 3, which we denote B:p(wrare|t) = B(wrare, t)In both LexFilter and LexProbs, we resort tothe relative frequency estimation in case the eventis not covered in the KC Analyzer.Tagset RepresentationsIn this work, we are comparing 3 different rep-resentations: TB, which is the original Treebank,KC which is the Treebank converted to use the KCAnalyzer tagset, and Layered, which is the layeredrepresentation described above.The details of the lexical models vary accordingto the representation we choose to work with.For the TB setting, our lexical rules are of the form331ttb ?
w. Only the Baseline models are relevanthere, as the tagset is not compatible with that ofthe external lexicon.For the KC setting, our lexical rules are of the formtkc ?
w, and their probabilities are estimated asdescribed above.
Note that this setting requires ourtrees to be tagged with the new (KC) tagset, andparsed sentences are also tagged with this tagset.For the Layered setting, we use lexical rules ofthe form ttb ?
w. Reliable events are esti-mated as usual, via relative frequency over theoriginal treebank.
For rare events, we estimatep(ttb ?
w|ttb) = p(ttb ?
tkc|ttb)p(tkc ?
w|tkc),where the transfer probabilities p(ttb ?
tkc) areestimated via relative frequencies over the layeredtrees, and the emission probabilities are estimatedeither based on other rare events (LexFilter) orbased on the semi-supervised method described inSection 3 (LexProbs).The layered setting has several advantages:First, the resulting trees are all tagged with theoriginal TB tagset.
Second, the training proce-dure does not require a treebank tagged with theKC tagset: Instead of learning the transfer layerfrom the treebank we could alternatively base ourcounts on a different parallel resource, estimate itfrom unannotated data using EM, define it heuris-tically, or use any other estimation procedure.4.2 ExperimentsWe perform all our experiments on Version 2 ofthe Hebrew Treebank, and follow the train/test/devsplit introduced in (Tsarfaty and Sima?an, 2007):section 1 is used for development, sections 2-12for training, and section 13 is the test set, whichwe do not use in this work.
All the reported re-sults are on the development set.10 After removalof empty sentences, we have 5241 sentences fortraining, and 483 for testing.
Due to some changesin the Treebank11, our results are not directly com-parable to earlier works.
However, our baselinemodels are very similar to the models presentedin, e.g.
(Goldberg and Tsarfaty, 2008).In order to compare the performance of themodel on the various tagset representations (TBtags, KC tags, Layered), we remove from the testset 51 sentences in which at least one token ismarked as not having any correct segmentation inthe KC Analyzer.
This introduces a slight bias in10This work is part of an ongoing work on a parser, and thetest set is reserved for final evaluation of the entire system.11Normalization of numbers and percents, correcting ofsome incorrect trees, etc.favor of the KC-tags setting, and makes the testsomewhat easier for all the models.
However, itallows for a relatively fair comparison between thevarious models.12Results and DiscussionResults are presented in Table 2.13Baselinerare: < 2 rare: < 10Prec Rec Prec RecTB 72.80 71.70 67.66 64.92KC 72.23 70.30 67.22 64.31LexFilterrare: < 2 rare: < 10Prec Rec Prec RecKC 77.18 76.31 77.34 76.20Layered 76.69 76.40 76.66 75.74LexProbsrare: < 2 rare: < 10Prec Rec Prec RecKC 77.29 76.65 77.22 76.36Layered 76.81 76.49 76.85 76.08Table 2: evalb results for parsing with asegmentation Oracle.As expected, all the results are much lower thanthose with gold fine-grained POS (Table 1).When not using any external knowledge (Base-line), the TB tagset performs slightly better thanthe converted treebank (KC).
Note, however, thatthe difference is less pronounced than in the goldmorphology case.
When varying the rare wordsthreshold from 2 to 10, performance drops consid-erably.
Without external knowledge, the parser isfacing difficulties coping with unseen events.The incorporation of an external lexical knowl-edge in the form of pruning illegal tag assignmentsfor unseen words based on the KC lexicon (Lex-Filter) substantially improves the results (?
72 to?
77).
The additional lexical knowledge clearlyimproves the parser.
Moreover, varying the rarewords threshold in this setting hardly affects theparser performance: the external lexicon sufficesto guide the parser in the right direction.
Keep-ing the rare words threshold high is desirable, as itreduces overfitting to the treebank vocabulary.We expected the addition of the semi-supervised p(t ?
w) distribution (LexProbs) toimprove the parser, but found it to have an in-significant effect.
The correct segmentation seems12We are forced to remove these sentences because of theartificial setting in which the correct segmentation is given.
Inthe no-oracle setting (Sec.
5), we do include these sentences.13The layered trees have an extra layer of bracketing(tTB ?
tKC ).
We remove this layer prior to evaluation.332to remove enough ambiguity as to let the parserbase its decisions on the generic tag distributionfor rare events.In all the settings with a Segmentation Oracle,there is no significant difference between the KCand the Layered representation.
We prefer the lay-ered representation as it provides more flexibility,does not require trees tagged with the KC tagset,and produces parse trees with the original TB POStags at the leaves.5 Parsing without a Segmentation OracleWhen parsing real world data, correct token seg-mentation is not known in advance.
For method-ological reasons, this issue has either been set-aside (Tsarfaty and Sima?an, 2007), or dealt within a pipeline model in which a morphological dis-ambiguator is run prior to parsing to determine thecorrect segmentation.
However, Tsarfaty (2006)argues that there is a strong interaction betweensyntax and morphological segmentation, and thatthe two tasks should be modeled jointly, and notin a pipeline model.
Several studies followed thisline, (Cohen and Smith, 2007) the most recent ofwhich is Goldberg and Tsarfaty (2008), who pre-sented a model based on unweighted lattice pars-ing for performing the joint task.This model uses a morphological analyzer toconstruct a lattice over all possible morphologi-cal analyses of an input sentence.
The arcs ofthe lattice are ?w, t?
pairs, and a lattice parseris used to build a parse over the lattice.
TheViterbi parse over the lattice chooses a lattice path,which induces a segmentation over the input sen-tence.
Thus, parsing and segmentation are per-formed jointly.Lexical rules in the model are defined over thelattice arcs (t?
?w, t?|t), and smoothed probabil-ities for them are estimated from the treebank viarelative frequency over terminal/preterminal pairs.The lattice paths themselves are unweighted, re-flecting the intuition that all morphological anal-yses are a-priori equally likely, and that their per-spective strengths should come from the segmentsthey contain and their interaction with the syntax.Goldberg and Tsarfaty (2008) use a data-drivenmorphological analyzer derived from the treebank.Their better models incorporated some externallexical knowledge by use of an Hebrew spellchecker to prune some illegal segmentations.In what follows, we use the layered represen-tation to adapt this joint model to use as its mor-phological analyzer the wide coverage KC Ana-lyzer in enhancement of a data-driven one.
Then,we further enhance the model with the semi-supervised lexical probabilities described in Sec 3.5.1 ModelThe model of Goldberg and Tsarfaty (2008) uses amorphological analyzer to constructs a lattice foreach input token.
Then, the sentence lattice is builtby concatenating the individual token lattices.
Themorphological analyzer used in that work is datadriven based on treebank observations, and em-ploys some well crafted heuristics for OOV tokens(for details, see the original paper).
Here, we useinstead a morphological analyzer which uses theKC Lexicon for rare and OOV tokens.We begin by adapting the rare vs. reliable eventsdistinction from Section 4 to cover unsegmentedtokens.
We define a reliable token to be a tokenfrom the training corpus, which each of its possi-ble segments according to the training corpus wasseen in the training corpus at least K times.14 Allother tokens are considered to be rare.Our morphological analyzer works as follows:For reliable tokens, it returns the set of analysesseen for this token in the treebank (each analysisis a sequence of pairs of the form ?w, tTB?
).For rare tokens, it returns the set of analyses re-turned by the KC analyzer (here, analyses are se-quences of pairs of the form ?w, tKC?
).The lattice arcs, then, can take two possibleforms, either ?w, tTB?
or ?w, tKC?.Lexical rules of the form tTB ?
?w, tTB?
are reli-able, and their probabilities estimated via relativefrequency over events seen in training.Lexical rules of the form tTB ?
?w, tKC?are estimated in accordance with the transferlayer introduced above: p(tTB ?
?w, tKC?)
=p(tKC |tTB)p(?w, tKC?|tKC).The remaining question is how to estimatep(?w, tKC?|tKC).
Here, we use either the LexFil-ter (estimated over all rare events) or LexProbs(estimated via the semisupervised emission prob-abilities)models, as defined in Section 4.1 above.5.2 ExperimentsAs our Baseline, we take the best model of (Gold-berg and Tsarfaty, 2008), run against the current14Note that this is more inclusive than requiring that thetoken itself is seen in the training corpus at least K times, assome segments may be shared by several tokens.333version of the Treebank.15 This model uses thesame grammar as described in Section 4.1 above,and use some external information in the form of aspell-checker wordlist.
We compare this Baselinewith the LexFilter and LexProbs models over theLayered representation.We use the same test/train splits as described inSection 4.
Contrary to the Oracle segmentationsetting, here we evaluate against all sentences, in-cluding those containing tokens for which the KCAnalyzer does not contain any correct analyses.Due to token segmentation ambiguity, the re-sulting parse yields may be different than the goldones, and evalb can not be used.
Instead, we usethe evaluation measure of (Tsarfaty, 2006), alsoused in (Goldberg and Tsarfaty, 2008), which isan adaptation of parseval to use characters insteadof space-delimited tokens as its basic units.Results and DiscussionResults are presented in Table 3.rare: < 2 rare: < 10Prec Rec Prec RecBaseline 67.71 66.35 ?
?LexFilter 68.25 69.45 57.72 59.17LexProbs 73.40 73.99 70.09 73.01Table 3: Parsing results for the joint parsing+segtask, with varying external knowledgeThe results are expectedly lower than with thesegmentation Oracle, as the joint task is muchharder, but the external lexical information greatlybenefits the parser also in the joint setting.
Whilesignificant, the improvement from the Baseline toLexFilter is quite small, which is due to the Base-line?s own rather strong illegal analyses filteringheuristic.
However, unlike the oracle segmenta-tion case, here the semisupervised lexical prob-abilities (LexProbs) have a major effect on theparser performance (?
69 to ?
73.5 F-score), anoverall improvement of ?
6.6 F-points over theBaseline, which is the previous state-of-the art forthis joint task.
This supports our intuition that rarelexical events are better estimated using a largeunannotated corpus, and not using a generic tree-bank distribution, or sparse treebank based counts,and that lexical probabilities have a crucial role inresolving segmentation ambiguities.15While we use the same software as (Goldberg and Tsar-faty, 2008), the results reported here are significantly lower.This is due to differences in annotation scheme between V1and V2 of the Hebrew TBThe parsers with the extended lexicon were un-able to assign a parse to about 10 of the 483 testsentences.
We count them as having 0-Fscorein the table results.16 The Baseline parser couldnot assign a parse to more than twice that manysentences, suggesting its lexical pruning heuris-tic is quite harsh.
In fact, the unparsed sen-tences amount to most of the difference betweenthe Baseline and LexFilter parsers.Here, changing the rare tokens threshold hasa significant effect on parsing accuracy, whichsuggests that the segmentation for rare tokens ishighly consistent within the corpus.
When an un-known token is encountered, a clear bias shouldbe taken toward segmentations that were previ-ously seen in the same corpus.
Given that that ef-fect is remedied to some extent by introducing thesemi-supervised lexical probabilities, we believethat segmentation accuracy for unseen tokens canbe further improved, perhaps using resources suchas (Gabay et al, 2008), and techniques for incor-porating some document, as opposed to sentencelevel information, into the parsing process.6 ConclusionsWe present a framework for interfacing a parserwith an external lexicon following a differ-ent annotation scheme.
Unlike other studies(Yang Huang et al, 2005; Szolovits, 2003) inwhich such interfacing is achieved by a restrictedheuristic mapping, we propose a novel, stochasticapproach, based on a layered representation.
Weshow that using an external lexicon for dealingwith rare lexical events greatly benefits a PCFGparser for Hebrew, and that results can be furtherimproved by the incorporation of lexical probabil-ities estimated in a semi-supervised manner usinga wide-coverage lexicon and a large unannotatedcorpus.
In the future, we plan to integrate thisframework with a parsing model that is specifi-cally crafted to cope with morphologically rich,free-word order languages, as proposed in (Tsar-faty and Sima?an, 2008).Apart from Hebrew, our method is applicablein any setting in which there exist a small tree-bank and a wide-coverage lexical resource.
Forexample parsing Arabic using the Arabic Tree-bank and the Buckwalter analyzer, or parsing En-glish biomedical text using a biomedical treebankand the UMLS Specialist Lexicon.16When discarding these sentences from the test set, resulton the better LexProbs model leap to 74.95P/75.56R.334ReferencesM.
Adler and M. Elhadad.
2006.
An unsupervisedmorpheme-based hmm for hebrew morphologicaldisambiguation.
In Proc.
of COLING/ACL2006.Meni Adler, Yoav Goldberg, David Gabay, andMichael Elhadad.
2008a.
Unsupervised lexicon-based resolution of unknown words for full morpho-logical analysis.
In Proc.
of ACL 2008.Meni Adler, Yael Netzer, David Gabay, Yoav Goldberg,and Michael Elhadad.
2008b.
Tagging a hebrewcorpus: The case of participles.
In Proc.
of LREC2008.Meni Adler.
2007.
Hebrew Morphological Disam-biguation: An Unsupervised Stochastic Word-basedApproach.
Ph.D. thesis, Ben-Gurion University ofthe Negev, Beer-Sheva, Israel.Eugene Charniak, Glenn Carroll, John Adcock, An-thony Cassandra, Yoshihiko Gotoh, Jeremy Katz,Michael Littman, and John McCann.
1996.
Taggersfor parsers.
Artif.
Intell., 85(1-2):45?57.Shay B. Cohen and Noah A. Smith.
2007.
Joint mor-phological and syntactic disambiguation.
In Pro-ceedings of EMNLP-CoNLL-07, pages 208?217.David Gabay, Ziv Ben Eliahu, and Michael Elhadad.2008.
Using wikipedia links to construct word seg-mentation corpora.
In Proc.
of the WIKIAI-08 Work-shop, AAAI-2008 Conference.Yoav Goldberg and Reut Tsarfaty.
2008.
A single gen-erative model for joint morphological segmentationand syntactic parsing.
In Proc.
of ACL 2008.Yoav Goldberg, Meni Adler, and Michael Elhadad.2008.
Em can find pretty good hmm pos-taggers(when given a good start).
In Proc.
of ACL 2008.Noemie Guthmann, Yuval Krymolowski, Adi Milea,and Yoad Winter.
2009.
Automatic annotation ofmorpho-syntactic dependencies in a modern hebrewtreebank.
In Proc.
of TLT.Mohamed Maamouri, Ann Bies, and Seth Kulick.2008.
Enhanced annotation and parsing of the ara-bic treebank.
In INFOS 2008, Cairo, Egypt, March27-29, 2008.Yael Netzer, Meni Adler, David Gabay, and MichaelElhadad.
2007.
Can you tag the modal?
you should!In ACL07 Workshop on Computational Approachesto Semitic Languages, Prague, Czech.K.
Sima?an, A. Itai, Y.
Winter, A. Altman, and N. Nativ.2001.
Building a tree-bank of modern hebrew text.Traitement Automatique des Langues, 42(2).P.
Szolovits.
2003.
Adding a medical lexicon to anenglish parser.
In Proc.
AMIA 2003 Annual Sympo-sium.Reut Tsarfaty and Khalil Sima?an.
2007.
Three-dimensional parametrization for parsing morpholog-ically rich languages.
In Proc.
of IWPT 2007.Reut Tsarfaty and Khalil Sima?an.
2008.
Relational-realizational parsing.
In Proc.
of CoLING, pages889?896, Manchester, UK, August.
Coling 2008.Reut Tsarfaty.
2006.
Integrated Morphological andSyntactic Disambiguation for Modern Hebrew.
InProceedings of ACL-SRW-06.MS Yang Huang, MD Henry J. Lowe, PhD Dan Klein,and MS Russell J. Cucina, MD.
2005.
Improvedidentification of noun phrases in clinical radiologyreports using a high-performance statistical naturallanguage parser augmented with the umls specialistlexicon.
J Am Med Inform Assoc, 12(3), May.335
