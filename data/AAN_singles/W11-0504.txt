Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 25?32,Portland, Oregon, June 23, 2011. c?2011 Association for Computational LinguisticsWho wrote What Where: Analyzing the content of human and automaticsummariesKarolina Owczarzak and Hoa Trang DangInformation Access DivisionNational Institute of Standards and TechnologyGaithersburg, MD 20899karolina.owczarzak@nist.gov hoa.dang@nist.govAbstractAbstractive summarization has been a long-standing and long-term goal in automatic sum-marization, because systems that can generateabstracts demonstrate a deeper understandingof language and the meaning of documentsthan systems that merely extract sentencesfrom those documents.
Genest (2009) showedthat summaries from the top automatic sum-marizers are judged as comparable to manualextractive summaries, and both are judged tobe far less responsive than manual abstracts,As the state of the art approaches the limitsof extractive summarization, it becomes evenmore pressing to advance abstractive summa-rization.
However, abstractive summarizationhas been sidetracked by questions of whatqualifies as important information, and how dowe find it?
The Guided Summarization taskintroduced at the Text Analysis Conference2010 attempts to neutralize both of these prob-lems by introducing topic categories and listsof aspects that a responsive summary shouldaddress.
This design results in more similarhuman models, giving the automatic summa-rizers a more focused target to pursue, and alsoprovides detailed diagnostics of summary con-tent, which can can help build better meaning-oriented summarization systems.1 IntroductionWhat qualifies as important information and how dowe find it?
These questions have been leading re-search in automatic summarization since its begin-nings, and we are still nowhere near a definitiveanswer.
Worse, experiments with humans subjectssuggest a definitive answer might not even exist.With all their near-perfect language understandingand world knowledge, two human summarizers willstill produce two different summaries of the sametext, simply because they will disagree on what?simportant.
Fortunately, usually some of this infor-mation will overlap.
This is represented by the ideabehind the Pyramid evaluation framework (Nenkovaand Passonneau, 2004; Passonneau et al, 2005),where different levels of the pyramid represent theproportion of concepts (?Summary Content Units?,or SCUs) mentioned by 1 to n summarizers in sum-maries of the same text.
Usually, there are very fewSCUs that are mentioned by all summarizers, a fewmore that are mentioned by some of them, and thegreatest proportion are the SCUs that are mentionedby individual summarizers only.This variance in what should be a ?gold standard?makes research in automatic summarization meth-ods particularly difficult.
How can we reach a goalso vague and under-defined?
Using term frequencyto determine important concepts in a text has provento be very successful, largely because of its simplic-ity and universal applicability, but statistical meth-ods can only provide the most basic level of perfor-mance.
On the other hand, there is no real motiva-tion to use any deeper meaning-oriented text anal-ysis if we are not even certain what information tolook for in order to produce a responsive summary.To address these concerns, the Summarizationtrack at the 2010 Text Analysis Conference1 (TAC)introduced a new summarization task ?
GuidedSummarization ?
in which topics are divided into1All datasets available at http://www.nist.gov/tac/25narrow categories and a list of required aspects isprovided for each category.
This serves two pur-poses: first, it creates a more focused target for au-tomatic summarizers, neutralizing human varianceand pointing to concrete types of information thereader requires, and second, it provides a detaileddiagnostic tool to analyze the content of automaticsummaries, which can help build more meaning-oriented systems.
This paper shows how these ob-jectives were achieved in TAC 2010, looking at thesimilarity of human-crafted models, and then usingthe category and aspect information to look in depthat the differences between human and top automaticsummarizers, discovering strengths and weaknessesof automatic systems and areas for improvement.2 Topic-specific summarizationThe idea that different types of stories might requiredifferent approaches is not new, although the classi-fication varies from task to task.
Topic categorieswere present in Document Understanding Confer-ence2 (DUC) 2001, where topics were divided into:single-event, single-subject, biographical, multipleevents of same type, and opinion.
In their analy-sis of these results, Nenkova and Louis (2008) findthat summaries of articles in what they call topic-cohesive categories (single-event, single-subject, bi-ography) are of higher quality than those in non-cohesive categories (opinion, multiple event).In essence, categorizing topics into types is basedon the assumption that stories of the same type fol-low a specific template and include the same kindsof facts, and this predictability might be employedto improve the summarization process, since we atleast know what kinds of information are importantand what to look for.
This was shown, among others,by Bagga (1997), who analyzed source articles usedin the Message Understanding Conference (MUC)and graphed the distribution of facts in articles onair vehicle launches, terrorist attacks, joint ventures,and corporate personnel changes, finding that thesame kinds of facts appeared repeatedly.
A nat-ural conclusion is that Information Extraction (IE)methods might be helpful here, and in fact, Whiteet al (2001) presented an IE-based summarizationsystem for natural disasters, where they first filled2http://www-nlpir.nist.gov/projects/duc/an IE template with slots related to date, location,type of disaster, damage (people, physical effects),etc.
Similarly, Radev and McKeown (1998) used IEcombined with Natural Language Generation (NLG)in their SUMMON system.There are two ways to classify stories: accordingto their level of cohesiveness (to use the distinctionmade by Nenkova and Louis (2008)), and accord-ing to subject.
The first classification could helpus determine which topics would be easier for au-tomatic summarization, but the difficulty is relatedpurely to lexical characteristics of the text; as shownin Louis and Nenkova (2009), source document sim-ilarity in terms of word overlap is one of the pre-dictive features of multi-document summary qual-ity.
The second classification, according to subjectmatter, is what enables us to utilize more meaning-oriented approaches such as IE and attempt a deepersemantic analysis of the source text, and is what wedescribe in this paper.3 Guided summarization at TACThe new guided summarization task in 2010 wasdesigned with the second classification in mind,in order to afford the participants a chance toexplore deeper linguistic methods of text analy-sis.
There were five topic categories: (1) Acci-dents and Natural Disasters, (2) Attacks (Crimi-nal/Terrorist), (3) Health and Safety, (4) EndangeredResources, and (5) Trials and Investigations (Crim-inal/Legal/Other).3 In contrast to previous topic-specific summarization tasks, the Guided Summa-rization task also provided a list of required aspects,which described the type of information that shouldbe included in the summary (if such informationcould be found in source documents).
Summariz-ers also had the option of including any other in-formation they deemed important to the topic.
Thecategories and their aspects, shown in Table 1, weredeveloped on the basis of past DUC and TAC topicsand model summaries from years 2001-2009.Each topic came with 20 chronologically ordered3In the remainder of this paper, the following short forms areused for names of categories: Accidents = Accidents and Nat-ural Disasters; Attacks = Attacks; Health = Health and Safety;Resources = Endangered Resources; Trials = Trials and Inves-tigations.
Full description of the task is available at the TACwebsite.26Accidents Attacks Healthwhat what whatwhen when who affectedwhere where howwhy perpertrators whywho affected why countermeasuresdamages who affectedcountermeasures damagescountermeasuresResources Trialswhat whoimportance who investigatingthreats whycountermeasures chargespleadsentenceTable 1: Categories and aspects in TAC 2010 GuidedSummarization task.news articles.
The initial summaries were to be pro-duced on the basis of the first 10 documents.
Asin TAC 2008 and 2009, the 2010 Summarizationtask had an update component: using the second 10documents, summarizers were to produce an updatesummary under the assumption that the user had al-ready read the first set of source documents.
Thismeans that for the update part, there were two in-teracting conditions, with the requirement for non-redundancy taking priority over the requirement toaddress all category aspects.For each topic, four model summaries were writ-ten by human assessors.
All summaries were eval-uated with respect to linguistic quality (OverallReadability), content (Pyramid), and general quality(Overall Responsiveness).
Readability and Respon-siveness were judged by human assessors on a scalefrom 1 (very poor) to 5 (very good), while Pyramidis a score between 0 and 1 (in very rare cases, itexceeds 1, if the candidate summary contains moreSCUs than the average reference summary).Since this was the first year of Guided Summa-rization, only about half of the 43 participating sys-tems made some use of the provided categories andaspects, mostly using them and their synonyms asquery terms.3.1 Model summaries across yearsThe introduction of categories, which implies tem-plate story types, and aspects, which further nar-rows content selection, resulted in the parallel modelsummaries being much more similar to each otherthan in previous years, as represented by the Pyra-human automaticinitial update initial updatePyramid 2008 0.66 0.63 0.26 0.202009 0.68 0.60 0.26 0.202010 0.78 0.67 0.30 0.20Respons.2008 4.62 4.62 2.32 2.022009 4.66 4.48 2.32 2.172010 4.76 4.71 2.56 2.10Table 2: Macro-average Pyramid and Responsivenessscores for initial and update summaries for years 2008-2010.
Responsiveness scores for 2009 were scaled froma ten-point to a five-point scale.mid score, which measures information overlap be-tween a candidate summary and a set of refer-ence summaries.
Table 2 shows the macro-averagedPyramid and Responsiveness scores for years 2008-2010.
Both initial and update human summariesscore higher for Pyramid in 2010, and also gain a lit-tle in Responsiveness.
The macro-averages for auto-matic summarizers, on the other hand, increase onlyfor initial summaries, which we will discuss furtherin Section 3.4.
The similarity effect among modelsummaries can be more clearly seen in Table 3,which shows the percentage of Summary ContentUnits (SCUs, information ?nuggets?or simple facts)with different weights in Pyramids across the yearsbetween 2008-2010.
The weight of an SCU is sim-ply the number of model summaries in which thisinformation unit appears.
Pyramids in 2010 havegreater percentage of SCUs with weight > 1, andtheir proportion of weight-1 SCUs is below half ofall SCUs.
The difference is much more pronouncedfor the initial summaries, since the update compo-nent is restricted by the non-redundancy require-ment, resulting in more variance in content selectionafter the required aspects have been covered.43.2 Content coverage in TAC 2010During the Pyramid creation process, assessors ex-tracting SCUs from model summaries were asked tomark the aspect(s) relevant to each SCU.
This letsus examine and compare the distribution of infor-mation in human and automatic summaries.
Table 4shows macro-average SCU counts in Pyramids com-4Each summary could be up to 100 words long, and noincentive was given for writing summaries of shorter length;therefore, the goal for both human and automatic summarizerswas to fit as much relevant information as possible in the 100-word limit.27SCUweight 2008 2009 2010initial 4 9% 12% 22%3 14% 13% 18%2 22% 23% 24%1 55% 52% 36%update4 8% 7% 11%3 12% 12% 14%2 21% 20% 26%1 59% 62% 49%Table 3: Percentage of SCUs with weights 1?4 in pyra-mids for initial and update summaries for years 2008-2010.posed of four human summaries, and macro-averagecounts of matching SCUs in the summaries of the15 top-performing automatic summarizers (as deter-mined by their Responsiveness rank on initial sum-maries).5 Although automatic summaries find onlya small percentage of all available information (asrepresented by the number of Pyramid SCUs), theSCUs they find for the initial summaries are usuallythose of the highest weight, i.e.
encoding informa-tion that is the most essential to the topic.SCU distribution in human summaries is also in-teresting: Health, Resources, and Trials all havethe expected pyramid shape, with many low-weightSCUs at the base and few high-weight SCUs on top,but for Attacks and Accidents, the usual pattern isbroken and we see an hourglass shape instead, re-flecting the presence of many weight-4 SCUs.
Themost likely explanation is that these two categoriesare guided by a relatively long list of aspects (cf.Table 1), many of which have unique answers in thesource text.This is shown in more detail in Table 5, whichpresents aspect coverage by Pyramids and top 15automatic summarizers in terms of an average num-ber of SCUs relevant to a given aspect and an aver-age weight of an aspect-related SCU.
Only Attackand Accidents have aspects that tend to generate thesame answers from almost all human summarizers:when, where in Accidents and what, when, where,perpetrators, and who affected in Attacks all haveaverage weight of around 3.
The patterns hold forupdate summaries; although all values decrease and5We chose to use the top 15 out of 43 participating systemsin order to exclude outliers like systems that returned emptysummaries, and to measure the state-of-the-art in the summa-rization field.SCUweight initial updatepyramids automatic pyramids automaticAccidents4 6.4 3.2 1.9 0.53 3.7 1 3.43 0.82 6.9 1.6 6.1 0.61 7.9 0.8 7.6 0.7total 24.9 7.7 19.1 3.1Attacks4 7.7 4.9 3.7 13 3.1 0.8 3.7 0.82 5 1 5.3 0.81 5.6 0.5 9.4 0.7total 21.4 9.1 22.1 3.9Health4 4.9 1.8 1.6 0.43 4.2 0.8 2.6 0.72 5.3 0.6 4.9 0.81 10.6 0.9 12 0.8total 25 5 21 3Resources4 4.2 1.5 1.1 0.63 5.1 1.3 2.7 0.52 5 1 5.9 11 9.5 0.7 12.4 1total 23.8 5 22.1 3.4Trials4 4.4 2.6 3.4 1.23 5.7 2 3.3 0.52 7.8 1.6 5.7 0.61 9.2 0.5 8.5 0.6total 27.1 8.5 20.9 3.3Table 4: Macro-average SCU counts with weights 1?4 inpyramids and matching SCU counts in automatic sum-maries, for initial and update summaries.there is less overlap between models, answers tothese aspects are the most likely to occur in multi-ple summaries.The situation for top 15 automatic summarizersis even more interesting: while they contain rela-tively few matching SCUs, the SCUs they do findare those of high weight, as can be seen by compar-ing their SCU weight averages.
Even for ?other?,which covers ?all other information important forthe topic?
and is therefore more dependent on sum-mary writer?s subjective judgment and shows morecontent diversity, resulting in low-weight SCUs inthe Pyramid, the top automatic summarizers findthose most weighted.
It would seem, then, that thecontent selection methods are able to identify someof the most important facts; at the same time, thedensity of information in automatic summaries ismuch lower than in human summaries, indicatingthat the automatic content is either not compressedadequately, or that it includes non-relevant or re-peated information.28Avg SCU weight (avg SCU count)initial summaries update summariesPyramids automatic Pyramids automaticAccidentswhat 2.4 (4.4) 3.1 (1.9) 2.5 (2.7) 2.87 (0.6)when 3.6 (2.1) 3.7 (0.7) 3.7 (0.4) 4 (0.1)where 3.0 (3.6) 3.2 (1.3) 2.1 (1.1) 2.58 (0.4)why 2.6 (2.3) 3.1 (0.5) 2.4 (2.0) 3 (0.3)who aff 2.3 (4.9) 2.8 (1.5) 2.0 (4.1) 2.45 (0.6)damages 1.8 (2.4) 3.1 (0.5) 1.7 (1.9) 2.05 (0.2)counterm 2.1 (8.0) 2.7 (1.2) 2.0 (8.1) 2.4 (0.9)other 1.3 (0.4) 1.9 (0.1) 1.3 (0.6) 1 (0.0)Attackswhat 2.9 (3.1) 3.7 (1.6) 2.0 (1.4) 2.8 (0.4)when 3.4 (1.3) 3.8 (0.4) 2.4 (1.4) 2.2 (0.1)where 2.7 (2.9) 3.7 (1.2) 2.5 (0.9) 3.8 (0.3)perpetr 2.8 (3.6) 3.4 (1.0) 2.2 (3.0) 3.0 (0.9)why 2.1 (3.4) 2.8 (0.9) 1.8 (1.3) 1.6 (0.2)who aff 3.3 (4.0) 3.6 (1.7) 2.0 (2.0) 2.1 (0.3)damages 2.2 (0.9) 3.0 (0.2) 3.4 (0.7) 4.0 (0.1)counterm 2.3 (4.3) 2.8 (1.1) 2.1 (10.3) 2.6 (1.1)other 1.7 (1.3) 2.2 (0.1) 1.6 (2.6) 1.7 (0.2)Healthwhat 2.4 (6.0) 3.1 (1.6) 2.4 (2.9) 3.0 (0.7)who aff 2.0 (5.6) 2.6 (0.8) 1.8 (2.7) 2.0 (0.3)how 2.4 (6.6) 3.1 (1.1) 1.6 (2.7) 2.4 (0.3)why 2.2 (3.9) 2.9 (0.6) 1.7 (2.3) 2.1 (0.4)counterm 2.0 (6.3) 2.7 (0.8) 1.7 (10.4) 2.2 (1.0)other 1.1 (0.6) 1.9 (0.1) 1.2 (1.9) 1.6 (0.2)Resourceswhat 2.3 (3.2) 2.9 (1.3) 1.6 (1.4) 2.6 (0.4)importan 2.4 (3.1) 2.7 (0.3) 1.8 (1.9) 2.3 (0.2)threats 2.3 (7.6) 2.8 (1.6) 1.6 (6.8) 2.0 (1.1)counterm 2.0 (10.1) 2.8 (1.7) 1.7 (12.1) 2.2 (1.4)other 1.4 (0.7) 2.9 (0.1) 1.8 (1.2) 2.5 (0.1)Trialswho 2.7 (3.5) 3.2 (1.7) 2.7 (2.3) 3.2 (0.4)who inv 1.9 (5.5) 2.8 (0.8) 1.8 (3.3) 2.6 (0.5)why 2.6 (6.3) 3.1 (2.2) 1.8 (2.4) 2.3 (0.3)charges 2.7 (2.4) 3.2 (0.8) 2.4 (1.4) 2.5 (0.3)plead 2.0 (5.0) 2.9 (0.9) 2.1 (3.5) 3.0 (0.5)sentence 2.3 (2.7) 3.0 (0.5) 2.6 (6.0) 3.5 (0.8)other 1.5 (3.2) 2.0 (0.3) 1.7 (4.8) 2.4 (0.6)Table 5: Aspect coverage for Pyramids and top 15 auto-matic summarizers in TAC 2010.3.3 Effect of categories and aspectsSome categories in the Guided Summarization taskare defined in more detail than others, dependingon types of stories they represent.
Stories about at-tacks and accidents (and, to some extent, trials) tendto follow more predictable and detailed templates,which results in more similar models and better re-sults for automatic summarizers.
Figure 1 gives agraphic representation of the macro-average Pyra-mid and Responsiveness scores for human and top15 automatic summarizers, with exact scores in Ta-bles 6 and 7, where the first score marked with aletter is not statistically significant from any subse-quent score marked with the same letter, accordingto ANOVA (p>0.05).
Lack of significant differencebetween human Responsiveness scores in Table 6suggests that, for all categories, human summariesare highly and equally responsive, but a look at theirPyramid scores confirms that Attacks and Accidentsmodels tend to have more overlapping information.For automatic summaries, their Pyramid and Re-sponsiveness patterns are parallel.
Here Attacks,Accidents, and Trials contain on average morematching SCUs than Health and Resources, makingthese summaries more responsive.
One reason forthese differences might be that many systems rely onsentence positon in the extraction process, and firstsentences in these template stories often are a shortdescription of event including date, location, personsinvolved, in effect giving systems the unique-answeraspects mentioned in Section 3.2.
Table 5 showsthis distribution of matching information in more de-tail: for Attacks and Accidents, automatic summa-rizers match relatively more SCUs for what, where,when, who affected than for countermeasures, dam-ages, or other.
For Trials, again the easier aspectsare those that tend to appear at the beginning ofdocuments: who [is under investigation] and why.Stories in Health and Resources, the weakest cate-gories overall for automatic summarizers and withthe greatest amount of variance for human summa-rizers, are non-events, instead being closer to whatin past DUC tasks was described as a ?multi-event?or ?single subject?
story type.
Individual documentswithin the source set might sometimes follow thetypical event template (e.g.
describing individualinstances of coral reef destruction), but in generalthese categories require much more abstraction andrender the opening-sentence extraction strategy lesseffective.If the higher averages are really due to the infor-mation extracted with first sentences, then we wouldalso expect higher scores from Baseline 1, whichsimply selected the opening sentences of the mostrecent document, up to the 100-word limit.
And in-deed, as shown in Table 8, the partial Pyramid scoresfor Baseline 1 are the highest for exactly these ?con-crete?
categories and aspects, mostly for Attacks andAccidents, and aspects such aswhere, what, andwho(the score of 1 for Accidents other is an outlier, sincethere was only one SCU relevant for this calcula-tion and the baseline happened to match it).
On theother hand, its lowest performance is mostly con-centrated in Health and Resources, and in the more?vague?
aspects, like why, how, importance, coun-29Pyramid ResponsivenessinitialAttacks 0.857 A Trials 4.825 AAccidents 0.812 AB Accidents 4.821 ABResources 0.773 AB Attacks 4.786 ABCHealth 0.767 AB Health 4.750 ABCDTrials 0.751 B Resources 4.650 ABCDupdateTrials 0.749 A Attack 4.857 AAttacks 0.745 AB Trials 4.825 ABAccidents 0.700 AB Accidents 4.714 ABCHealth 0.610 C Health 4.625 ABCDResources 0.604 C Resources 4.600 ABCDTable 6: Macro-average Pyramid and Responsivenessscores per category for human summaries, comparisonacross categories.Pyramid ResponsivenessinitialAttacks 0.524 A Attacks 3.400 ATrials 0.446 B Accidents 3.362 ABAccidents 0.418 B Trials 3.167 ABCResources 0.323 C Resources 2.893 CDHealth 0.290 C Health 2.617 DupdateResources 0.286 A Resources 2.520 ATrials 0.261 AB Health 2.417 ABAttacks 0.251 ABC Trials 2.380 ABCHealth 0.236 BCD Attacks 2.286 ABCDAccidents 0.228 BCD Accidents 2.248 ABCDTable 7: Macro-average Pyramid and Responsivenessscores per category for top 15 automatic summaries, com-parison across categories.termeasures, and other.
We can conclude that earlysentence position is not a good predictor of such in-formation, and that automatic summarizers might dowell to diversify their methods of content identifi-cation based on what type of information they arelooking for.3.4 Initial and update summariesWhile the initial component is only guided bythe categories and aspects, the update componentis placed under an overarching condition of non-redundancy.
Update summaries should not repeatHighest LowestCategory Aspect score Category Aspect score(Accidents Other 1) Resources other 0Attacks WHERE 0.66 Health other 0Attacks WHAT 0.66 Attacks COUNTERM 0Trials WHO 0.6 Attacks other 0Attacks WHO AFF 0.56 Accidents WHY 0Accidents WHERE 0.44 Health WHO AFF 0Accidents WHAT 0.41 Trials SENTENCE 0.06Trials WHY 0.38 Health WHY 0.06Attacks PERP 0.34 Accidents DAMAGES 0.07Trials WHO INV 0.33 Health HOW 0.08Trials CHARGES 0.33 Resources IMPORTAN 0.09Table 8: Top Pyramid scores for Baseline 1, per aspect,for initial summaries.Figure 1: Macro-average Pyramid and Responsivenessscores in initial and update summaries, for humans andtop 15 automatic systems.
In each group, columns fromleft: Accidents, Attacks, Health, Resources, Trials.
As-terisk indicates significant drop from initial score.any information that can be found in the initial doc-ument set.
This restriction narrows the pool of po-tential summary elements to choose from.
More im-portantly, since the concrete aspects with unique an-swers like what, where, and when are likely to bementioned in the first set of document (and, by ex-tension, in the initial summaries), this shifts contentselection to aspects that generate more variance, likewhy, countermeasures, or other.
As shown in Fig-ure 1, while Responsiveness remains high for hu-man summarizers across categories, which meansthe content is still relevant to the topic, the Pyramidscores are lower in the update component, whichmeans the summarizers differ more in terms of whatinformation they extract from the source documents.Note that this is not the case for Trials, where thehuman performance for both Responsiveness andPyramid is practically identical for initial and up-date summaries.
The time course of trials is gener-ally longer than those for accidents and attacks, andmany of the later-occurring aspects such as plea andsentence are well-defined; hence the initial and up-date human summaries have similar Pyramid scores.Automatic summarizers, on the other hand, sufferthe greatest drop in those categories in which theywere the most successful before: Attacks, Acci-dents, and Trials, in effect rendering their perfor-mance across categories more or less even (cf.
Fig-30ure 1).A closer look at the aspect coverage in initial andupdate components confirms the differences in as-pect distribution.
Figure 2 gives four columns foreach aspect: the first two columns represent initialsummaries, the second two represent update sum-maries.
Dark columns in each pair are human sum-marizers, light columns are top 15 automatic sum-marizers.
For almost all aspects, humans find fewerrelevant (and new!)
facts in the update documents,with the exception of sentence in Trials, and coun-termeasures and other in all categories.
Logically,once all the anchoring information has been given(date, time, location, event), the only remaining rel-evant content to focus on are consequences of theevent (countermeasures, sentence), and possibly up-dates in victims and damages (who affected, dam-ages) as well as any other information that might berelevant.
A similar (though less consistent) patternholds for automatic summarizers.4 Summary and conclusionsInitial attempts at more complex treatments of anysubject often fail when faced with unrestricted, ?realworld?
input.
This is why almost all research insummarization remains centered around relativelysimple extractive methods.
Few developers try toincorporate syntactic parsing to compress summarysentences, and almost none want to venture into se-mantic decompositon of source text, since the com-plexity of these methods is the cause of potentialerrors.
Also, the tools might not deal particularlywell with different types of stories in the ?newswire?genre.
However, Genest (2009) showed the limitsof purely extractive summarization: their manual,extractive summarizer (HexTac) performed muchworse than human abstractors, and comparably tothe top automatic summarizers in TAC 2009.But if we want to see significant progress in ab-stractive summarization, it?s important to provide amore controlled environment for such experiments.TAC 2010 results show that, first of all, by guid-ing summary creation we end up with more similarhuman abstracts than in previous tasks (partly dueto the choice of template-like categories, and partlydue to the further guiding role of aspects).
Narrow-ing down possible summary content, while exclud-Figure 2: Average number of SCUs per aspect in initialand update summaries in TAC 2010.
Dark grey = Pyra-mids, light grey = top 15 automatic summarizers.
Thefirst pair of columns for each aspects shows initial sum-maries, the second pair shows update summaries.31ing variance due to subjective opinions among hu-man writers, creates in effect a more concrete in-formation model, and a single, unified informationmodel is an easier goal to emulate than relying onvague and subjective goals like ?importance?.
Outof five categories, Attacks and Accidents generatedthe most similar models, mostly because they re-quired concrete, unique-answer aspects like whereor when.
In Health and Resources, the aspects weremore subjective in nature, and the resulting variancewas greater.Moreover, the Guided Task provides a very valu-able and detailed diagnostic tool for system devel-opers: by looking at the system performance withineach aspect, we can find out which types of infor-mation it is better able to identify.
While the top au-tomatic summarizers managed to retrieve less thanhalf of relevant information at the best of times, thefacts they did retrieve were highly-weighted.
Theirbetter performance for certain aspects of Attacks,Accidents, and Trials could be ascribed to the factthat most of them rely on sentence position to deter-mine important information in the source document.A comparison of covered aspects suggests that sen-tence position might be a better indicator for sometypes of information than others.Since it was the first year of the Guided Task, onlysome of the teams used the provided category/aspectinformation; as the task continues, we hope to seemore participants adopting categories and aspectsto guide their summarization.
The predictable el-ements of each category invite the use of differ-ent techniques depending on the type of informa-tion sought, perhaps suggesting the use of Infor-mation Extraction methods.
Some categories mightbe easier to process than others, but even if theinformation-mining approach cannot be extended toall types of stories, at worst we will end up withbetter summarization for event-type stories, like at-tacks, accidents, or trials, which together comprise alarge part of reported news.ReferencesAmit Bagga and Alan W. Biermann.
1997.
Analyzingthe Complexity of a Domain With Respect To An In-formation Extraction Task.
Proceedings of the tenthInternational Conference on Research on Computa-tional Linguistics (ROCLING X), 175?194.Pierre-Etienne Genest, Guy Lapalme, and Mehdi Yousfi-Monod.
2009.
HEXTAC: the Creation of a ManualExtractive Run.
Proceedings of the Text Analysis Con-ference 2009.Annie Louis and Ani Nenkova.
2009.
Performanceconfidence estimation for automatic summarization.Proceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, 541?548.
Athens, Greece.Kathleen R. McKeown, Regina Barzilay, David Evans,Vasileios Hatzivassiloglou, Judith L. Klavans, AniNenkova, Carl Sable, Barry Schiffman, and SergeySigelman.
2002.
Tracking and summarizing news ona daily basis with Columbia?s Newsblaster.
Proceed-ings of the Second International Conference on Hu-man Language Technology Research, 280?285.
SanDiego, California.Ani Nenkova and Annie Louis.
2008.
Can You Summa-rize This?
Identifying Correlates of Input Difficultyfor Multi-Document Summarization.
Proceedings ofACL-08: HLT, 825?833.
Columbus, Ohio.Ani Nenkova and Rebecca J. Passonneau.
2004.
Evaluat-ing content selection in summarization: The Pyramidmethod.
Proceedings of the Human Language Tech-nology Conference of the North American Chapter ofthe Association for Computational Linguistics, 145?152.
Boston, MA.Rebecca J. Passonneau, Ani Nenkova, Kathleen McKe-own, and Sergey Sigelman.
2005.
Applying the Pyra-mid method in DUC 2005.
Proceedings of the 5thDocument Understanding Conference (DUC).
Van-couver, Canada.Dragomir R. Radev and Kathleen R. McKeown.
1998.Generating natural language summaries from mul-tiple on-line sources.
Computational Linguistics,24(3):470?500.Michael White, Tanya Korelsky, Claire Cardie, VincentNg, David Pierce, and Kiri Wagstaff.
Multidocumentsummarization via information extraction.
2001.
Pro-ceedings of the First International Conference on Hu-man Language Technology Research, 1?7.
San Diego,California.32
