Proceedings of NAACL HLT 2007, Companion Volume, pages 105?108,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsTagging Icelandic text using a linguistic and a statistical taggerHrafn Loftsson?Department of Computer ScienceReykjavik UniversityReykjavik IS-103, Icelandhrafn@ru.isAbstractWe describe our linguistic rule-based tag-ger IceTagger, and compare its tagging ac-curacy to the TnT tagger, a state-of-the-art statistical tagger, when tagging Ice-landic, a morphologically complex lan-guage.
Evaluation shows that the averagetagging accuracy is 91.54% and 90.44%,obtained by IceTagger and TnT, respec-tively.
When tag profile gaps in the lex-icon, used by the TnT tagger, are filledwith tags produced by our morphologicalanalyser IceMorphy, TnT?s tagging accu-racy increases to 91.18%.1 IntroductionIn this paper, we use a linguistic rule-based method(LRBM) and a data-driven method (DDM) for tagg-ing text in the morphologically complex Icelandiclanguage.We present a novel LRBM.
The tagger based onthis method, hereafter called IceTagger, uses about175 local rules for initial disambiguation, and a setof heuristics, to force feature agreement where ap-propriate, for further disambiguation.The average tagging accuracy of IceTagger is91.54%, compared to 90.44% achieved by the TnTtagger, a state-of-the-art statistical tagger (Brants,2000).
IceTagger makes 11.5% less errors than TnT.On the other hand, when tag profile gaps in the lex-icon, used by TnT, are filled with tags produced by?
The author is also affiliated with the Dept.
of ComputerScience, University of Sheffield, Sheffield, S1 4DP, UK.IceMorphy, our morphological analyser, TnT?s tagg-ing accuracy increases to 91.18%.
In that case, Ice-Tagger makes 4.1% less errors than TnT.The remainder of this paper is organised as fol-lows: In Sect.
2, we describe the different taggingmethods in more detail.
Sect.
3 briefly describes theIcelandic language and the tagset.
The componentsof IceTagger are described in Sect.
4, and evaluationresults are presented in Sect.
5.2 The tagging methodsDDMs use machine learning to automatically derivea language model from, usually, hand-annotated cor-pora.
An advantage of the DDMs is their languageand tagset independence property.
Their disadvan-tage is that a tagged corpus is essential for training.Furthermore, the limited window size used for dis-ambiguation (e.g.
three words) can be responsiblefor some of the tagging errors.One of the better known statistical data-driventagger is the TnT tagger (written in C).
The tag-ger uses a second order (trigram) Hidden Markovmodel.
The probabilities of the model are esti-mated from a training corpus using maximum like-lihood estimation.
New assignments of part-of-speech (POS) to words is found by optimising theproduct of lexical probabilities (p(wi|tj)) and con-textual probabilities (p(ti|ti?1, ti?2)) (where wi andti are the ith word and tag, respectively).In contrast to DDMs, LRBMs are developed withthe purpose of tagging a specific language using aparticular tagset.
The purpose of the rules is, usu-ally, to remove illegitimate tags fromwords based oncontext.
The advantage of LRBMs is that they do not105rely (to the same extent as DDMs) on the existenceof a tagged corpus, and rules can be written to referto words and tags in the entire sentence.
The con-struction of a linguistic rule-based tagger, however,has been considered a difficult and time-consumingtask (Voutilainen, 1995).One of the better known LRBMs is the Con-straint Grammar (CG) framework (Karlsson et al,1995), in which both POS and grammatical func-tions are tagged.
The EngCG-2 tagger, developedover several years and consisting of 3,600 rules, hasbeen shown to obtain high accuracy (Samuelssonand Voutilainen, 1997).The development time of our LRBM (written inJava; including a tokeniser, IceMorphy and Ice-Tagger) was 7 man-months, which can be consid-ered a short development time for a LRBM.
This ismainly due to the emphasis on using heuristics (seeSect.
4.3) for disambiguation, as opposed to writinga large number of local rules.3 The Icelandic language and its tagsetThe Icelandic language is one of the Nordic lan-guages.
The language is morphologically rich,mainly due to inflectional complexity.
A thoroughdescription of the language can, for example, befound in (?r?insson, 1994).The main Icelandic tagset, constructed in thecompilation of the tagged corpus Icelandic Fre-quency Dictionary (IFD) (Pind et al, 1991), is large(about 660 tags) compared to related languages.
Inthis tagset, each character in a tag has a particularfunction.
Table 1 shows the semantics of the nounand the adjective tags.To illustrate, consider the phrase ?fallegu hes-tarnir?
(beautiful horses).
The corresponding tagfor ?fallegu?
is ?lkfnvf ?, denoting adjective, mascu-line, plural, nominative, weak declension, positive;and the tag for ?hestarnir?
is ?nkfng?
denoting noun,masculine, plural, nominative with suffixed definitearticle.4 IceTaggerIceTagger consists of three main components: anunknown word guesser, local rules for initial disam-biguation and heuristics for further disambiguation.Both the local rules and the heuristics have been de-Char Category/ Symbol ?
semantics# Feature1 Word class n?noun, l?adjective2 Gender k?masculine, v?feminine,h?neuter, x?unspecified3 Number e?singular, f?plural,4 Case n?nominative, o?accusative,?
?dative, e?genitive5 Article g?with suffixed article5 Declension s?strong, v?weak6 Proper noun m?person, ?
?place, s?other6 Degree f?positive, m?comparative,e?superlativeTable 1: The semantics of the noun and the adjectivetags.veloped using linguistic knowledge and tuned usinga development corpus (described in Sect.
5).4.1 The unknown word guesserThe purpose of our morphological analyser Ice-Morphy, which is used as an unknown word guesserby IceTagger, is to generate all appropriate tagsfor a given word.
It uses a familiar approach tounknown word guessing, i.e.
it performs mor-phological/compound analysis and ending analysis(Mikheev, 1997; Nakov et al, 2003).
Additionally,IceMorphy includes an important module for handl-ing tag profile gaps (for a thorough description ofIceMorphy, consult (Loftsson, 2006a)).A tag profile gap arises when a particular word,listed in a lexicon derived from a corpus, has somemissing tags in its tag profile (set of possible tags).The missing tag(s) might just not have been encoun-tered during the derivation of the lexicon (e.g.
dur-ing training).
For each noun, adjective or verb, of aparticular morphological class, IceMorphy is able tofill in the gaps for the given word.To illustrate, consider the word ?konu?
(woman),and let us assume that only the tag ?nveo?
(denotingnoun, feminine, singular, accusative) is found in thelexicon.
Based on the ?u?
morphological suffix andthe accusative case of the tag, IceMorphy assumesthe word belongs to a particular morphological fem-inine noun class, in which singular accusative, dativeand genitive cases have the same word form.
Con-106sequently, IceMorphy generates the correct missingtags: ?nve??
and ?nvee?.4.2 Local rulesThe purpose of a local rule is to eliminate inappro-priate tags from words, based on a local context (awindow of 5 words; two words to the left and rightof the focus word).
This reductionistic approach iscommon in rule-based taggers.
It is, for example,used in the CG systems.In principle, the local rules are unordered.
The fir-ing of a rule is, however, dependent on the order ofthe words in a sentence.
A sentence to be tagged isscanned from left to right and all tags of each wordare checked in a sequence.
Depending on the wordclass (the first letter of the tag) of the focus word, thetoken is sent to the appropriate disambiguation rou-tine, which checks a variety of disambiguation con-straints applicable to the particular word class andthe surrounding words.
At each step, only tags forthe focus word are eliminated.The format of a local rule is: If <condition>eliminate tag t. A <condition> is a booleanexpression, whose individual components can re-fer to lexical forms or individual characters (wordclass/morphological features) of tags.
The follow-ing are examples of <condition> (L1/R1 and L2/R2denote tokens one and two to the left/right of the fo-cus word, F , respectively):L1.isOnlyWordClass(x) AND L2.isOnlyWordClass(y)R1.isWordClass(x) OR R2.isWordClass(y)L1.isWordClass(x) AND t.isCase(y) AND t.isGender(z)R1.lexeme.equals(x) AND F .isWordClass(y)To exemplify, consider the sentence part: ?vi?vorum .
.
.
?
(we were .
.
.
).
The word ?vi??
canhave the following five tags (?_?
is used as a sep-arator between tags): ?ao_a?_fp1fn_aa_nkeo?.
Forillustration purposes, it is sufficient to point outthat the first two tags denote prepositions govern-ing the accusative and the dative cases, respec-tively.
Since the following word is a verb (?vo-rum?)
and prepositions only precede nominals, arule, with <condition>=R1.isOnlyWordClass(Verb),eliminates preposition tags in this context, leavingonly the tags ?fp1fn_aa_nkeo?.The current version of our tagger has 175 localrules.
The rules are written in a separate file andcompiled to Java code.4.3 The heuristicsOnce local disambiguation has been carried out,each sentence is sent to a global heuristic mod-ule, consisting of a collection of algorithmic proce-dures.
Its purpose is to perform grammatical func-tion analysis, guess prepositional phrases (PPs) anduse the acquired knowledge to force feature agree-ment where appropriate.
We call these heuristicsglobal because, when disambiguating a particularword, a heuristic can refer to a word which is notin the nearest neighbourhood.The heuristics repeatedly scan each sentence andperform the following: 1) mark PPs, 2) mark verbs,3) mark subjects, 4) force subject-verb agreement,5) mark objects, 6) force subject-object agreement,7) force verb-object agreement, 8) force nominalagreement, and 9) force PP agreement.
Lastly, thedefault heuristic is simply to choose the most fre-quent tag according to frequency information de-rived from the IFD corpus.
A detailed description ofall the heuristics can be found in (Loftsson, 2006b).5 EvaluationFor evaluation, we used the pairs of ten training andtest corpora of the IFD corpus, produced by Helga-d?ttir (2004).
We used the first nine of these test cor-pora for evaluation, but the tenth one was set asideand used as the development corpus for IceTagger.For each test corpus (10% of the IFD) the corre-sponding training corpus (90% of the IFD) was usedto deduce the lexicon(s) used by TnT, IceTagger andIceMorphy.
When testing the two taggers, we thusmade sure that the ratio of unknown words was (al-most) the same.The accuracy of a base tagger, which assigns eachknown word its most frequent tag, and the most fre-quent noun tag/proper noun tag to lower case/uppercase unknown words, is 76.27% (see table 2).The average tagging accuracy of IceTagger for allwords is 91.54%, compared to 90.44% for TnT (seetable 2).
IceTagger makes 11.5% less errors thanTnT1.In order to improve the tagging accuracy of TnT,we used the tag profile gap filling mechanism of Ice-1TnT is very fast, it tags about 50,000 tokens/sec on a DellOptiplex GX620 Pentium 4, 3.20 GHz.
IceTagger tags about2,700 tokens/sec.107Words Base TnT TnT* IceTaggerUnkn.
4.39% 71.68% 72.75% 75.09%Known 81.84% 91.82% 92.53% 92.74%All 76.27% 90.44% 91.18% 91.54%Table 2: Average tagging accuracy of the varioustaggers.Morphy in the following manner.
Each record in thelexicon used by TnT consists of a word and the cor-responding tags found in the training corpus.
Addi-tionally, to facilitate lexical probability calculations,each tag is marked by its frequency (i.e.
how of-ten the tag appeared as a label for the given word).We made IceMorphy generate a ?filled?
lexicon suchthat each generated missing tag was marked with thefrequency 12.
We call the resulting tagger TnT*.
In-deed, when testing TnT*, we obtained an overall av-erage tagging accuracy of 91.18% (see table 2).
Ice-Tagger makes 4.1% less errors than TnT*.The development of IceTagger/IceMorphy took 7man-months, but it has been worth the effort.
First,IceTagger does make fewer errors than TnT, and itsaccuracy can probably be increased by improvingits individual components.
Secondly, we have usedIceTagger in various tagger combination methods tofurther increase the tagging accuracy of Icelandictext (Loftsson, 2006c).6 ConclusionIn this paper, we have compared the tagging accu-racy of our linguistic rule-based tagger, IceTagger,to the accuracy of TnT, a state-of-the-art statisticaltagger.IceTagger uses only about 175 local rules, but isable to achieve high accuracy through the use ofglobal heuristics along with automatic tag profilegap filling.
The average tagging accuracy of Ice-Tagger is 91.54%, compared to 90.44% obtained bythe TnT tagger.
On the other hand, we were able toobtain 91.18% accuracy using TnT along with thetag profile gap filling mechanism of IceMorphy.In future work, we would like to improve individ-ual components of IceTagger and IceMorphy, with2This seems logical since the missing tags were not found inthe training corpus and are, hence, infrequent.the purpose of further increasing the tagging accu-racy.ReferencesT.
Brants.
2000.
TnT: A statistical part-of-speech tag-ger.
In Proceedings of the 6th Conference on Appliednatural language processing, Seattle, WA, USA.S.
Helgad?ttir.
2004.
Testing Data-Driven Learning Al-gorithms for PoS Tagging of Icelandic.
In H. Holm-boe, editor, Nordisk Sprogteknologi 2004.
MuseumTusculanums Forlag.F.
Karlsson, A. Voutilainen, J.
Heikkil?, and A. Anttila.1995.
Constraint Grammar: A Language-Independent System for Parsing Unrestricted Text.Mouton de Gruyter, Berlin, Germany.H.
Loftsson.
2006a.
Tagging Icelandic text: A lin-guistic rule-based approach.
Technical Report CS-06-04, Department of Computer Science, University ofSheffield.H.
Loftsson.
2006b.
Tagging a Morphologically Com-plex Language Using Heuristics.
In T. Salakoski,F.
Ginter, S. Pyysalo, and T. Pahikkala, editors, Ad-vances in Natural Language Processing, 5th Interna-tional Conference on NLP, FinTAL 2006, Proceedings,Turku, Finland.H.
Loftsson.
2006c.
Tagging Icelandic text: An exper-iment with integrations and combinations of taggers.Language Resources and Evaluation, 40(2):175?181.A.
Mikheev.
1997.
Automatic Rule Induction for Un-known Word Guessing.
Computational Linguistics,21(4):543?565.P.
Nakov, Y. Bonev, G. Angelova, E. Cius, and W. Hahn.2003.
Guessing Morphological Classes of UnknownGerman Nouns.
In Proceedings of Recent Advances inNatural Language Processing, Borovets, Bulgaria.J.
Pind, F. Magn?sson, and S. Briem.
1991.
The Ice-landic Frequency Dictionary.
The Institute of Lexi-cography, University of Iceland, Reykjavik, Iceland.H.
?r?insson.
1994.
Icelandic.
In E. K?nig and J. Auw-era, editors, The Germanic Languages.
Routledge,London.C.
Samuelsson and A Voutilainen.
1997.
Comparinga Linguistic and a Stochastic tagger.
In Proceedingsof the 8th Conference of the European Chapter of theACL (EACL), Madrid, Spain.A.
Voutilainen.
1995.
A syntax-based part-of-speech an-alyzer.
In Proceedings of the 7th Conference of the Eu-ropean Chapter of the ACL (EACL), Dublin, Ireland.108
