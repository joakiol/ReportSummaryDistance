Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 455?466,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsExplaining the Stars: Weighted Multiple-Instance Learning forAspect-Based Sentiment AnalysisNikolaos PappasEPFL and Idiap Research InstituteRue Marconi 19CH-1920 Martigny, Switzerlandnikolaos.pappas@idiap.chAndrei Popescu-BelisIdiap Research InstituteRue Marconi 19CH-1920 Martigny, Switzerlandandrei.popescu-belis@idiap.chAbstractThis paper introduces a model of multiple-instance learning applied to the predic-tion of aspect ratings or judgments ofspecific properties of an item from user-contributed texts such as product reviews.Each variable-length text is represented byseveral independent feature vectors; oneword vector per sentence or paragraph.For learning from texts with known as-pect ratings, the model performs multiple-instance regression (MIR) and assigns im-portance weights to each of the sentencesor paragraphs of a text, uncovering theircontribution to the aspect ratings.
Next,the model is used to predict aspect ratingsin previously unseen texts, demonstratinginterpretability and explanatory power forits predictions.
We evaluate the model onseven multi-aspect sentiment analysis datasets, improving over four MIR baselinesand two strong bag-of-words linear mod-els, namely SVR and Lasso, by more than10% relative in terms of MSE.1 IntroductionSentiment analysis of texts provides a coarse-grained view of their overall attitude towards anitem, either positive or negative.
The recent abun-dance of user texts accompanied by real-valued la-bels e.g.
on a 5-star scale has contributed to the de-velopment of automatic sentiment analysis of re-views of items such as movies, books, music orother products, with applications in social com-puting, user modeling, and recommender systems.The overall sentiment of a text towards an itemoften results from the ratings of several specificaspects of the item.
For instance, the author ofa review might have a rather positive sentimentabout a movie, having particularly liked the plotand the music, but not too much the actors.
De-termining the ratings of each aspect automaticallyis a challenging task, which may seem to requirethe engineering of a large number of features de-signed to capture each aspect.
Our goal is to putforward a new feature-agnostic solution for ana-lyzing aspect-related ratings expressed in a text,thus aiming for a finer-grained, deeper analysis oftext meaning than overall sentiment analysis.Current state-of-the-art approaches to sentimentanalysis and aspect-based sentiment analysis, at-tempt to go beyond word-level features either byusing higher-level linguistic features such as POStagging, parsing, and knowledge infusion, or bylearning features that capture syntactic and seman-tic dependencies between words.
Once an appro-priate feature space is found, the ratings are typi-cally modeled using a linear model, such as Sup-port Vector Regression (SVR) with `2norm forregularization or Lasso Regression with `1norm.By treating a text globally, these models ignore thefact that the sentences of a text have diverse con-tributions to the overall sentiment or to the attitudetowards a specific aspect of an item.In this paper, we propose a new learning modelwhich answers the following question: ?To whatextent does each part of a text contribute to theprediction of its overall sentiment or the rating ofa particular aspect??
The model uses multiple-instance regression (MIR), based on the assump-tion that not all the parts of a text have the samecontribution to the prediction of the rating.
Specif-ically, a text is seen as a bag of sentences (in-stances), each of them modeled as a word vector.The overall challenge is to learn which sentencesrefer to a given aspect, and how they contribute tothe text?s attitude towards it, but the model appliesto overall sentiment analysis as well.
For instance,Figure 1 displays a positive global comment on aTED talk and the weights assigned to two of itssentences by MIR.455Figure 1: Analysis of a comment (bag of sentences{s1, ..., sj}) annotated by humans with the maxi-mal positive sentiment score (5 stars).
The weightsassigned by MIR reveal that s1has the greatest rel-evance to the overall sentiment.Using regularized least squares, we formulatean optimization objective to jointly assign instanceweights and regression hyperplane weights.
Then,an instance relevance estimation method is usedto predict aspect ratings, or global ones, in previ-ously unseen texts.
The parameters of the modelare learned using an alternating optimization pro-cedure inspired by Wagstaff and Lane (2007).
Ourmodel requires only text with ratings for training,with no particular assumption on the word fea-tures to be extracted, and provides interpretableexplanations of the predicted ratings through therelevance weights assigned to sentences.
We alsoshow that the model has reasonable computationaldemands.
The model is evaluated on aspect andsentiment rating prediction over seven datasets:five of them contain reviews with aspect labelsabout beers, audiobooks and toys (McAuley et al.,2012), and two contain TED talks with emotion la-bels, and comments on them with sentiment labels(Pappas and Popescu-Belis, 2013).
Our modeloutperforms previous MIR models and two stronglinear models for rating prediction, namely SVRand Lasso by more than 10% relative in terms ofMSE.
The improvement is observed even when thesophistication of the feature space increases.The paper is organized as follows.
Section 2shows how our model innovates with respect toprevious work on MIR and rating prediction.
Sec-tion 3 formulates the problem while Section 4 de-scribes previous MIR models.
Section 5 presentsour MIR model and learning procedure.
Section 6presents the datasets and evaluation methods.
Sec-tion 7 reports our results on rating prediction tasks,and provides examples of rating explanation.2 Related Work2.1 Multiple-Instance RegressionMultiple-instance regression (MIR) belongs to theclass of multiple-instance learning (MIL) prob-lems for real-valued output, and it is a variantof multiple regression where each data point maybe described by more than one vectors of values.Many MIL studies focused on classification (An-drews et al., 2003; Bunescu and Mooney, 2007;Settles et al., 2008; Foulds and Frank, 2010; Wanget al., 2011) while fewer focused on regression(Ray and Page, 2001; Davis and others, 2007;Wagstaff et al., 2008; Wagstaff and Lane, 2007).Related to document analysis, several MIR stud-ies have focused on news categorization (Zhangand Zhou, 2008; Zhou et al., 2009) or web-indexrecommendation (Zhou et al., 2005) but, to ourknowledge, no study has attempted to use MIR foraspect rating prediction or sentiment analysis withreal-valued labels.MIR was firstly introduced by Ray et al.
(2001),proposing an EM algorithm which assumes thatone primary instance per bag is responsible forits label.
Wagstaff and Lane (2007) proposed tosimultaneously learn a regression model and es-timate instance weights per bag for crop yieldmodeling (not applicable to prediction).
A simi-lar method which learns the internal structure ofbags using clustering was proposed by Wagstaff etal.
(2008) for crop yield prediction, and we willuse it for comparison in the present study.
Later,the method was adapted to map bags into a single-instance feature space by Zhang et al.
(2009).Wang et al.
(2008) assumed that each bag is gener-ated by random noise around a primary instance,while Wang et al.
(2012) represented bag labelswith a probabilistic mixture model.
Foulds etal.
(2010) concluded that various assumptions aredifferently suited to different tasks, and should bestated clearly when describing an MIR model.2.2 Rating Prediction from TextSentiment analysis aims at analyzing the polar-ity of a given text, either with classification (fordiscrete labels) or regression (for real-valued la-bels).
Early studies introduced machine learningtechniques for sentiment classification, e.g.
Panget al.
(2002), including unsupervised techniquesbased on the notion of semantic orientation ofphrases, e.g.
Turney et al.
(2002).
Other studiesfocused on subjectivity detection, i.e.
whether a456text span expresses opinions or not (Wiebe et al.,2004).
Rating inference was defined by Pang etal.
(2005) as multi-class classification or regres-sion with respect to rating scales.
Pang and Lee(2008) discusses the large range of features engi-neered for this task, though several recent stud-ies focus on feature learning (Maas et al., 2011;Socher et al., 2011), including the use of a deepneural network (Socher et al., 2013).
In contrast,we do not make any assumption about the natureor dimensionality of the feature space.The fine-grained analysis of opinions regardingspecific aspects or features of items is known asmulti-aspect sentiment analysis.
This task usu-ally requires aspect-related text segmentation, fol-lowed by prediction or summarization (Hu andLiu, 2004; Zhuang et al., 2006).
Most attempts toperform this task have engineered various featuresets, augmenting words with topic or content mod-els (Mei et al., 2007; Titov and McDonald, 2008;Sauper et al., 2010; Lu et al., 2011), or with lin-guistic features (Pang and Lee, 2005; Baccianellaet al., 2009; Qu et al., 2010; Zhu et al., 2012).Other studies have advocated joint modeling ofmultiple aspects (Snyder and Barzilay, 2007) ormultiple reviews for the same product (Li et al.,2011).
McAuley et al.
(2012) introduced new cor-pora of multi-aspect reviews, which we also partlyuse here, and proposed models for aspect detec-tion, sentiment summarization and rating predic-tion.
Lastly, joint aspect identification and senti-ment classification have been used for aggregatingproduct review snippets by Sauper at al.
(2013).None of the above studies considers the multiple-instance property of text in their modeling.3 MIR DefinitionLet us consider a set B of m bags withnumerical labels Y as input data D ={({b1j}dn1, y1), ..., ({bmj}dnm, ym)}, where bij?Rd(for 1 ?
j ?
ni) and yi?
R. Each bagBiconsists of nidata points (called ?instances?
),hence it is a matrix of nid-dimensional vectors,e.g.
word vectors.
The challenge is to infer thelabel of the bag given a variable number of in-stances ni.
This requires finding a set of bag rep-resentations X = {x1, .
.
.
, xm} of size m wherexi?
Rd, from which the class labels can be com-puted.
The goal is then to find a mapping fromthis representation, noted ?
: Rd?
R, which isable to predict the label of a given bag.
Ideally,assuming that X is the best bag representation forour task, we look for the optimal regression hyper-plane ?
which minimizes a loss function L plus aregularization term ?
as follows:?
= arg min?(L(Y,X,?)?
??
?loss+ ?(?)?
??
?reg.
)(1)Since the best set of representationsX for a task isgenerally unknown, one has to make assumptionsto define it or compute it jointly with the regres-sion hyperplane ?.
Thus, the main difficulty liesin finding a good assumption for X , as we willnow discuss.4 Previous MIR AssumptionsWe describe here three assumptions frequentlymade in past MIR studies, to which we will latercompare our model: aggregating all instances,keeping them as separate examples, or choosingthe most representative one (Wang et al., 2012).For each assumption, we will experiment withtwo state-of-the-art regression models (noted ab-stractly as f ), namely SVR (Drucker et al., 1996)and Lasso (Tibshirani, 1996) with respectively the`2and `1norms for regularization.The Aggregated algorithm assumes that eachbag is represented as a single d-dimensional vec-tor, which is the average of its instances (hencexi?
Rd).
Then, a regression model f is trainedon pairs of vectors and class labels, Dagg={(xi, yi) | i = 1, .
.
.
,m}, and the predicted classof an unlabeled bag Bi= {bij| j = 1, .
.
.
, ni} iscomputed as follows:y?
(Bi) = f(mean({bij| j = 1, .
.
.
, ni})) (2)In fact, a simple sum can also be used instead ofthe mean, and we observed in practice that with anappropriate regularization there is no difference onthe prediction performance between these options.This baseline corresponds to the typical approachfor text regression tasks, where each text sample isrepresented by a single vector in the feature space(e.g.
BOW with counts or TF-IDF weights).The Instance algorithm considers each of the in-stances in a bag as separate examples, by labelingeach of them with the bag?s label.
A regressionmodel f is learned over the training set made ofall vectors of all bags, Dins= {(bij, yi) | j =1, .
.
.
, ni; i = 1, .
.
.
,m}, assuming that there arem labeled bags.
To label a new bag Bi, given that457there is no representation xi, the method simplyaverages the predicted labels of its instances:y?
(Bi) = mean({f(bij) | j = 1, .
.
.
, ni}) (3)Instead of the average, the median value can alsobe used, which is more appropriate when the bagscontain outlying instances.The Prime algorithm assumes that a single in-stance in each bag is responsible for its label (Rayand Page, 2001).
This instance is called the pri-mary or prime one.
The method is similar to theprevious one, except that only one instance per bagis used as training data: Dpri= {(bpi, yi) | i =1, .
.
.
,m}, where bpiis the prime instance of theithbag Biand m is the number of bags.
Theprime instances are discovered through an itera-tive algorithm which refines the regression modelf .
Given an initial model f , in each iteration thealgorithm selects from each bag a prime candidatewhich is the instance with the lowest prediction er-ror.
Then, a new model is trained over the selectedprime candidates, until convergence.
For a newbag, the target class is computed as in Eq.
3.5 Proposed MIR ModelWe propose a new MIR model which assigns in-dividual relevance values (weights) to each in-stance of a bag, thus making fewer simplifyingassumptions than previous models.
We extendinstance-relevance algorithms such as (Wagstaffand Lane, 2007) by supporting high-dimensionalfeature spaces, as required for text regression, andby predicting both the class label and the con-tent structure of previously unseen (hence unla-beled) bags.
The former is achieved by minimiz-ing a regularized least squares loss (RLS) insteadof solving normal equations, which is prohibitivein large spaces.
The latter represents a significantimprovement over Aggregated and Instance algo-rithms, which are unable to pinpoint the most rel-evant instances with respect to the label of eachbag, being thus applicable only to bag label pre-diction.
Similarly, Prime only identifies the primeinstance when the bag is already labeled.
Instead,our model learns an optimal method to aggregateinstances, rather than a pre-defined one, and al-lows more degrees of freedom in the regressionmodel than previous ones.
Moreover, the weightof an instance is interpreted as its relevance bothin training and prediction.5.1 Instance Relevance AssumptionEach bag defines a bounded region of a hyper-plane orthogonal to the y-axis (the envelope of allits points).
The goal is to find a regression hy-perplane that passes through each bag Biand topredict its label by using at least one data pointxiwithin that bounded region.
Thus, the point xiis a convex combination of the points in the bag,in other words Biis represented by the weightedaverage of its instances bij:xi=ni?j=1?ijbij, ?ij?
0 andni?j=1?ij= 1 (4)where ?ijis the weight of the jthinstance of theithbag.
Each weight ?ijindicates the relevanceof an instance j to the prediction of the class yiofthe ithbag.
The constraint forces xito fall withinthe bounded region of the points in bag i and guar-antees that the ithbag will influence the regressor.5.2 Modeling Bag Structure and LabelsLet us consider a set ofm bags, where each bagBiis represented by its nid-dimensional instances,i.e.
Bi= {bij}dnialong with the set of target classlabels for each bag, Y = {yi}N, yi?
R. Therepresentation set of all Biin the feature space,X = {x1, .
.
.
, xm}, xi?
Rd, is obtained usingthe niinstance weights associated to each bag Bi,?i= {?ij}ni, ?ij?
[0, 1] which are initiallyunknown.
Thus, we look for a linear regressionmodel f that is able to model the target values us-ing the regression coefficients ?
?
Rd, where Xand Y are respectively the sets of training bags andtheir labels: Y = f(X) = ?TX .
We define a lossfunction according to the least squares objectivedependent on X , Y , ?
and the set of weight vec-tors ?
= {?1, .
.
.
, ?m} using Eq.
4 as follows:L(Y,X,?,?)
= ||Y ?
?TX||22(4)=N?i=1(yi?
?T(ni?j=1?ijbij))2=N?i=1(yi?
?T(Bi?i))2(5)Using the above loss function, accounting for theconstraints of our assumption in Eq.
4 and assum-ing `2-norm for regularization with 1and 2termsfor each ?i?
?
and ?
respectively, we obtain the458following least squares objective from Eq.
1:arg min?1,...,?m,?m?i=1(?2i???
?f1loss+ 1||?i||?
??
?f1reg.)?
??
?f2loss+ 2||?||2?
??
?f2reg.where ?2i=(yi?
?T(Bi?i))2, (6)subject to ?ij?
0 ?i, j and?nij=1?ij= 1 ?i.The selection of the `2-norm was based on prelim-inary results showing that it outperforms `1-norm.Other combinations of p-norm regularization canbe explored for f1and f2, e.g.
to learn sparser in-stance weights and denser regression coefficientsor vice versa.The above objective is non-convex and difficultto optimize because the minimization is with re-spect to all ?1, .
.
.
, ?mand ?
at the same time.
Asindicated in Eq.
6 above, we will note f1a modelthat is learned from the minimization only with re-spect to ?1, .
.
.
, ?mand f2a model obtained fromthe minimization with respect to ?
only.
In Eq.
6,we can observe that if one of the two is known orheld fixed, then the other one is convex and can belearned with the well-known least squares solvingtechniques.
In Section 5.3, we will describe an al-gorithm that is able to exploit this observation.Having computed ?1, .
.
.
, ?mand ?, we couldpredict a label for an unlabeled bag using Eq.
3,but would not be able to compute the weightsof the instances.
Moreover, information that hasbeen learned about the instances during the train-ing phase would not be used during prediction.For these reasons, we introduce a third regressionmodel f3with regression coefficients O ?
Rdas-suming a `2-norm for the regularization with 3term, which is trained on the relevance weightsobtained from the Eq.
6, Dw= {(bij, ?ij) | i =1, ...,m; j = 1, ..., ni}.
The optimization objec-tive for the f3model is the following:arg minON?i=1ni?j=1(?ij?OTbij)2?
??
?f3loss function+ 3||O||2?
??
?f3reg.
(7)This minimization can be easily performed withthe well-known least squares solving techniques.The learned model is able to estimate the weightsof the instances of an unlabeled bag during pre-diction time as:?
?i= f3(Bi) = ?TBi.
The?
?iweights are estimations which are influenced bythe relevance weights learned in our minimizationobjective of Eq.
6 but they are not constrained atprediction time.
To obtain interpretable weights,we can convert the estimated scores to the [0, 1]interval as follows:??i=??i/sum(??i).
Finally,the prediction of the label for the ithbag using theestimated instance weights?
?iis done as follows:y?
= f2(Bi) = ?TBi?
?i(8)5.3 Learning with Alternating ProjectionsAlgorithm 1 solves the non-convex optimizationproblem of Eq.
6 by using a powerful class ofmethods for finding the intersection of convex sets,namely alternating projections (AP).
The prob-lem is firstly divided into two convex problems,namely f1loss function and f2loss function,which are then solved in an alternating fashion.Like EM algorithms, AP algorithms do not havegeneral guarantees on their convergence rate, al-though, in practice, we found it acceptable at gen-erally fewer than 20 iterations.Algorithm 1 APWeights(B, Y , 1, 2, 3)1: Initialize(?1, .
.
.
, ?N,?, X)2: while not converged do3: for Biin B do4: ?i= cRLS(?TBi, Yi, 1) # f1model5: xi= Bi?Ti6: end for7: ?
= RLS(X,Y, 2) # f2model8: end while9: ?
= RLS({bij?i, j}, {?ij?i, j}, 3) # f3modelFigure 2: Visual representation for the training andtesting procedure of Algorithm 1.The algorithm takes as input the bags Bi, theirtarget class labels Y and the regularization terms1, 2, 3and proceeds as follows.
First, under afixed regression model (f2), it proceeds with f1to the optimal assignment of weights to the in-stances of each bag (projection of ?
vectors onthe ?ispace which is a ni-simplex) and com-putes its new representation set X .
Second, giventhe fixed instance weights, it trains a new regres-sion model (f2) using X (projection back to the ?459Bags Instances Dimension Aspect ratingsDataset Type Count Type Count Count ClassesBeerAdvocatereview1,200sentence12,189 19,418 feel, look, smell, taste, overallRateBeer (ES) 1,200 3,269 2,120 appearance, aroma, overall, palate, tasteRateBeer (FR) 1,200 4,472 903 appearance, aroma, overall, palate, tasteAudiobooks 1,200 4,886 3,971 performance, story, overallToys & Games 1,200 6,463 31,984 educational, durability, fun, overallTED comments comment 1,200 sentence 3,814 957 sentiment (polarity)TED talks commentsper talk1,200 comment 11,993 5,000 unconvincing, fascinating, persuasive,ingenious, longwinded, funny, inspir-ing, jaw-dropping, courageous, beauti-ful, confusing, obnoxiousTable 1: Description of the seven datasets used for aspect, sentiment and emotion rating prediction.space).
This procedure repeats until convergence,i.e.
when there is no more decrease on the trainingerror, or until a maximum number of iterations hasbeen reached.
The regression model f3is trainedon the weights learned from the previous steps.5.4 Complexity AnalysisThe overall time complexity T of Algorithm 1 interms of the input variables, noted h = {m, n?, d},with m being the number of bags, n?
the averagesize of the bags, and d the dimensionality of thefeature space (here, the size of word vectors), isderived as follows:T (h) = Tap(h) + Tf3(h)= O(m(n?2+ d2))+ O(mn?d2)= O(m(n?2+ d2+ n?d2)), (9)where Tapand Tf3are respectively the time com-plexity of the AP procedure and of training the f3model.
Eq.
9 shows that when n?
m, the modelcomplexity is linear with the input bags m and al-ways quadratic with the number of features d.Previous works on relevance assignment forMIR have prohibitive complexity for high-dimensional feature spaces or numerous bags andhence they are not most appropriate for text regres-sion tasks.
Wagstaff and Lane (2007) have cubictime complexity with the average bag size n?
andnumber of features d; Zhou et al.
(2009) use ker-nels, thus their complexity is quadratic with thenumber of bags m; and Wang et al.
(2011) havecubic time wrt.
d. Our formulation is thus com-petitive in terms of complexity.6 Data, Protocol and Metrics6.1 Aspect Rating DatasetsWe use seven datasets summarized in Table 1.Five publicly available datasets were built for as-pect prediction by McAuley et al.
(2012) ?
Beer-Advocate, Ratebeer (ES), RateBeer (FR), Audio-books and Toys & Games ?
and have aspect rat-ings assigned by their creators on the respectivewebsites.
On the set of comments on TED talksfrom Pappas and Popescu-Belis (2013), we aimto predict two things: talk-level emotion dimen-sions assigned by viewers through voting, andcomment polarity scores assigned by crowdsourc-ing.
The distributions of aspect ratings per datasetare shown in Figure 3.
Five datasets are in En-glish, one in Spanish (Ratebeer) and one in French(RateBeer), so our results will also demonstratethe language-independence of our method.From every dataset we kept 1,200 texts as bagsof sentences, but we also used three full-sizedatasets, namely Ratebeer ES (1,259 labeled re-views), Ratebeer FR (17,998) and Audiobooks(10,989).
The features for each of them are wordvectors with binary attributes signaling word pres-ence or absence, in a traditional bag-of-wordsmodel (BOW).
The word vectors are providedwith the first five datasets and we generated themfor the latter two, after lowercasing and stopwordremoval.
Moreover, for TED comments, we com-puted TF-IDF scores using the same dimension-ality as with BOW to experiment with a differentfeature space.
The target class labels were nor-malized by the maximum rating in their scale, ex-cept for TED talks where the votes were normal-ized by the maximum number of votes over all theemotion classes for each talk, and two emotions,?informative?
and ?ok?, were excluded as they areneutral ones.6.2 Evaluation ProtocolWe compare the proposed model, noted AP-Weights, with four baseline ones ?
Aggre-gated, Instance, Prime (Section 4) and Clus-460Figure 3: Distributions of rating values per aspect rating class for the seven datasets.tering (from github.com/garydoranjr/mcr), which is an instance relevance method pro-posed by Wagstaff et al.
(2008) for aspect ratingprediction.
First, for each aspect class, we opti-mize all methods on a development set of 25%of the data (300 randomly selected bags).
Then,we perform 5-fold cross-validation for every as-pect on each entire data set and report the averageerror scores using the optimal hyper-parametersper method.
In addition, we report for compar-ison the scores of AverageRating, which alwayspredicts the average rating over the training set.We report standard error metrics for regression,namely the Mean Absolute Error (MAE) and theMean Squared Error (MSE).
The former measuresthe average magnitude of errors in a set of predic-tions while the latter measures the average of theirsquares, which are defined over the test set of bagsBirespectively as MAE = (?ki=1|f(Bi)?yi|)/kand MSE = (?ki=1(f(Bi) ?
yi)2)/k.
The cross-validation scores are obtained by averaging theMAE and MSE scores on each fold.To find the optimal hyper-parameters for eachmodel, we perform 3-fold cross-validation on thedevelopment set using exhaustive grid-search overa fine-grained range of possible values and se-lect the ones that perform best in terms of MAE.The hyper-parameters to be optimized for thebaselines (except AverageRating) are the regular-ization terms ?2, ?1of their possible regressionmodel f , namely SVR which uses the `2normand Lasso which uses the `1norm.
As for AP-Weights, it relies on three regularization terms,namely 1, 2, 3of the `2-norm for f1, f2andf3regression models.
Lastly, for the Clusteringbaseline, we use the f2regression model, whichrelies on 2and the number of clusters k, opti-mized over {5, ..., 50} with step 5, for its cluster-ing algorithm, here k-Means.
All the regulariza-tion terms are optimized over the same range ofpossible values, noted a ?
10bwith a ?
{1, .
.
.
, 9}and b ?
{?4, .
.
.
,+4}, hence 81 values per term.For the regression models and evaluation proto-col, we use the scikit-learn machine learning li-brary (Pedregosa et al., 2012).
Our code and dataare available in the first author?s website.7 Experimental Results7.1 Aspect Rating PredictionThe results for aspect rating prediction are givenin Table 2.
The proposed APWeights methodoutperforms Aggregated (`2) and Aggregated (`1)i.e.
SVR and Lasso along with all other baselineson each case.
The SVR baseline has on average11% lower performance than APWeights in termsof MSE and about 6% in terms of MAE.
Simi-larly, the Lasso baseline has on average 13% lowerMSE and 8% MAE than APWeights.
As shownin Figure 4, APWeights also outperforms them foreach aspect in the five review datasets.
The In-stance method with `1performed well on BeerAd-vocate and Toys & Games (for MSE), and with `2performed well on Ratebeer (ES), RateBeer (FR)and Toys & Games (for MAE).
Therefore, theinstance-as-example assumption is quite appropri-ate for this task, however both options score be-low APWeights ?
by about 5% MAE, and 8%/9%MSE, respectively.
The Prime method with `1per-formed well only on the BeerAdvocate dataset andPrime with `2only on the Toys & Games dataset,always with lower scores than APWeights, namelyabout 9% MAE for both and 15%/18% MSE re-spectively.
This suggests that the primary-instance461REVIEW LABELSBeerAdvocate RateBeer (ES) RateBeer (FR) Audiobooks Toys & GamesModel \ Error MAE MSE MAE MSE MAE MSE MAE MSE MAE MSEAverageRating 14.20 3.32 16.59 4.31 12.67 2.69 21.07 6.75 20.96 6.75Aggregated (`1) 13.62 3.13 15.94 4.02 12.21 2.58 20.10 6.14 20.15 6.33Aggregated (`2) 14.58 3.68 14.47 3.41 12.32 2.70 19.08 5.99 18.99 5.93Instance (`1) 12.67 2.89 14.91 3.54 11.89 2.48 20.13 6.17 20.33 6.34Instance (`2) 13.74 3.28 14.40 3.39 11.82 2.40 19.26 6.04 19.70 6.59Prime (`1) 12.90 2.97 15.78 3.97 12.70 2.76 20.65 6.46 21.09 6.79Prime (`2) 14.60 3.64 15.05 3.68 12.92 2.98 20.12 6.59 20.11 6.92Clustering (`2) 13.95 3.26 15.06 3.64 12.23 2.60 20.50 6.48 20.59 6.52APWeights (`2) 12.24 2.66 14.18 3.28 11.37 2.27 18.89 5.71 18.50 5.57APW vs. SVR (%) +16.0 +27.7 +2.0 +3.8 +7.6 +15.6 +1.0 +4.5 +2.6 +6.0APW vs. Lasso (%) +10.1 +15.1 +11.0 +18.4 +6.8 +11.8 +6.0 +6.9 +8.1 +11.9APW vs. 2ndbest (%) +3.3 +7.8 +1.5 +3.3 +3.7 +4.9 +1.0 +4.5 +2.6 +6.0Table 2: Performance of aspect rating prediction (the lower the better) in terms of MAE and MSE (?
100)with 5-fold cross-validation.
All scores are averaged over all aspects in each dataset.
The scores of thebest method are in bold and the second best ones are underlined.
Significant improvements (paired t-test,p < 0.05) are in italics.
Fig.
4 shows MSE scores per aspect for three methods on five datasets.assumption is not the most appropriate for thistask.
Lastly, even though Clustering is an instancerelevance method, it has similar scores to Prime,presumably because the relevances are assignedaccording to the computed clusters and they arenot directly influenced by the task?s objective.To compare with the state-of-the-art results ob-tained by McAuley et al.
(2012), we experimentedwith three of their full-size datasets.
Splitting eachdataset in half for training vs. testing, and usingthe optimal settings from our experiments above,we measured the average MSE over all aspects.APWeights improved over Lasso by 10%, 26%and 17% MSE respectively on each dataset ?
theabsolute MSE scores are .038 for Lasso vs. .034for APWeights on Ratebeer SP; .023 vs. .017 onRatebeer FR; .063 vs. .052 on Audiobooks.
Sim-ilarly, when compared to the best SVM baselineprovided by the McAuley et al., our method im-proved by 32%, 43% and 35% respectively oneach dataset, though it did not use their ratingmodel.
Moreover, the best model proposed byMcAuley et al., which uses a joint rating modeland an aspect-specific text segmenter trained onhand-labeled data, reaches MSE scores of .03,.02 and .03, which is comparable to our modelthat does not use these features (.034, .017, .052),though it could benefit from them in the future.Lastly, as mentioned by the same authors, predic-tors which use segmented text, for example withtopic models as in (Lu et al., 2011), do not neces-sarly outperform SVR baselines; instead they havemarginal or even no improvements, therefore, wedid not further experiment with them.
Interes-SENT.
LABELS EMO.
LABELSTED comm.
TED talksModel \ Error MAE MSE MAE MSEAverageRating 19.47 5.05 17.86 6.06Aggregated (`1) 17.08 4.17 15.98 5.03Aggregated (`2) 16.88 4.47 15.24 4.97Instance (`1) 17.69 4.37 16.48 5.30Instance (`2) 16.93 4.24 16.10 5.57Prime (`1) 17.39 4.37 15.98 5.78Prime (`2) 18.03 4.91 16.74 5.94Clustering (`2) 17.64 4.34 17.71 6.02APWeights (`2) 15.91 3.95 15.02 4.89APW vs SVR (%) +5.7 +11.5 +1.5 +1.6APW vs Lasso (%) +6.8 +5.3 +6.0 +2.9APW vs 2nd(%) +5.7 +5.3 +1.5 +1.6Table 3: MAE and MSE (?
100) on sentimentand emotion prediction with 5-fold c.-v. Scoreson TED talks are averaged over the 12 emotions.The scores of the best method are in bold and thesecond best ones are underlined.
Significant im-provements (paired t-test, p < 0.05) are in italics.tignly, multiple-instance learning algorithms un-der several assumptions go beyond SVR baselineswith BOW and even more sophisticated featuressuch as TF-IDF (see below).7.2 Sentiment and Emotion PredictionOur method is also competitive for sentiment pre-diction over comments on TED talks, as well asfor talk-level emotion prediction with 12 dimen-sions from subsets of 10 comments on each talk(see Table 3).
APWeights outperforms SVR andLasso, as well as all other methods for each task.For sentiment prediction, SVR is outperformed by11% MSE and Lasso by 5%.
For emotion pre-462Figure 4: MSE scores of SVR, Lasso and APWeights for each aspect over the five review datasets.diction (averaged over all 12 aspects), differencesare smaller, at 1.6% and 2.9% respectively.
Thesesmaller differences could be explained by the factthat among the 10 most recent comments for eachtalk, many are not related to the emotion that thesystem tries to predict.As mentioned earlier, the proposed model doesnot make any assumption about the feature space.Thus, we examined whether the improvements itbrings remain present even with a different fea-ture space, for instance based on TF-IDF insteadof BOW with counts.
For sentiment prediction onTED comments, we found that by changing thefeature space to TF-IDF, strong baselines such asAggregated (`1) and (`2), i.e.
SVR and Lasso, im-prove their performance (16.25 and 16.59 MAE;4.16 and 3.97 MSE respectively).
However, AP-Weights still outperforms them on both MAE andMSE scores (15.35 and 3.63), improving overSVR by 5.5% on MAE and 12.5% on MSE, andover Lasso by 7.4% on MAE and 8.5% on MSE.These promising results suggest that improve-ments with APWeights could be observed also onmore sophisticated feature spaces.7.3 Interpreting the Relevance WeightsApart from predicting ratings, the MIR scores as-signed by our model reflect the contribution ofeach sentence to these predictions.To illustrate the explanatory power of our model(until a dataset for quantitative analysis becomesavailable), we provide examples of predictionson test data taken from the cross-validation foldsabove.
Table 5 displays the most relevant com-Sentences per comment?
?iy?iyi?Very brilliant and witty, as well asgreat improvisation.
?0.645.0 5.0?I enjoyed this one a lot.?
0.36?That?s great idea, I really like it!?
0.564.2 4.0?I can?t wait to try it, but first thing,I need a house with big windows,next year, maybe I can do that.
?0.44?Unfortunately countries are not ledby gifted children.
?0.482.4 2.0?They are either dictated by themost extreme personalities whocrave nothing but power or man-aged by politicians who are voted inby a far from gifted population.
?0.52?I am very disappointed by this,smug, cliched and missing so muchinformation as to be almost (...)?
?0.431.8 1.0?No mention of ship transport letssay 50% of all material transport,no mention of rail transport, (...)?0.29?I am sorry to be so negative, thisjust sounds like a sales pitch that hehas given too many times (...).
?0.28Table 4: Predicted sentiment for TED comments:yiis the actual sentiment, y?ithe predicted one, and?
?ithe estimated relevance of each sentence.ment for two correctly predicted emotions on twoTED talks, based on the?
?irelevance scores, alongwith the?
?iscores of the other comments, fortwo emotion classes: ?beautiful?
and ?courageous?.These comments appear to reflect correctly thefact that the respective emotion is the majority onein each of the comments.
As noted earlier, thistask is quite challenging since we use only the tenmost recent comments for each talk.Table 4 displays four TED comments selected463Class Top comment per talk (according to weights ?i)?
?idistributioninspiring?It seems to me that the idea worth spreading of this TED Talk is inspiring and key fora full life.
?No-one else is the authority on your potential.
You?re the only person thatdecides how far you go and what you?re capable of.?
It seems to me that teens actuallythink that.
As a child one is all knowing and all capable.
How did we get to the (...)?beautiful?The beauty of the nature.
It would be more interesting just integrates his thought andidea into a mobile device, like a mobile, so we can just turn on the nature gallery in anytime.
The paintings don?t look incidental but genuinely thought out, random perhaps, butwith a clear grand design behind the randomness.
Drawing is an art where it doesn?t (...)?funny?Funny story, but not as funny as a good ?knock, knock?
joke.
My favorite knock-knockjoke of all time is Cheech & Chong?s ?Dave?s Not Here?
gag from the early 1970s.
I?mstill waiting for someone to top it after all these years.
[Knock, knock] ?Who is it??
thevoice of an obviously stoned male answers from the other side of a door, (...)?courageous?I was a soldier in Iraq and part of the unit represented in this documentary.
I would ques-tion anyone that told you we went over there to kill Iraqi people.
I spent the better partof my time in Iraq protecting the Iraqi people from insurgents who came from countriesoutside of Iraq to kill Iraqi people.
We protected families men, women, and (...)?Table 5: Two examples of top comments (according to weights ?i) for correctly predicted emotions infour TED talks (score 1.0) and the distribution of weights over the 10 most recent comments in each talk.Figure 5: Top words based on ?
for predicting four emotions from comments on TED talks.from the test set of a given fold, for the comment-level sentiment prediction task.
The table alsoshows the?
?irelevance scores assigned to eachof the composing sentences, the predicted polar-ity scores y?iand the actual ones yi.
We observethat the sentences that convey the most sentimentare assigned higher scores than sentences with lesssentiment, always with respect to the global polar-ity level.
These examples suggest that, given thatAPWeights has more degrees of freedom for inter-pretation, it is able to assign relevance to parts ofa text (here, sentences) and even to words, whileother models can only consider words.
Hence, theassigned weights might be useful for other NLPtasks mentioned below.8 Conclusion and Future WorkThis paper introduced a novel MIR model for as-pect rating prediction from text, which learns in-stance relevance together with target labels.
To thebest of our knowledge, this has not been consid-ered before.
Compared to previous work on MIR,the proposed model is competitive and more effi-cient in terms of complexity.
Moreover, it is notonly able to assign instance relevances on labeledbags, but also to predict them on unseen bags.Compared to previous work on aspect ratingprediction, our model performs significantly bet-ter than BOW regression baselines (SVR, Lasso)without using additional knowledge or features.The improvements persist even when the sophis-tication of the features increases, suggesting thatour contribution may be orthogonal to feature en-gineering or learning.
Lastly, the qualitative eval-uation on test examples demonstrates that the pa-rameters learned by the model are not only usefulfor prediction, but they are also interpretable.In the future, we intend to test our model on sen-timent classification at the sentence-level, basedonly on document-level supervision (T?ackstr?omand McDonald, 2011).
Moreover, we will experi-ment with other model settings, such as regulariza-tion norms other than `2and feature spaces otherthan BOW or TF-IDF.
In the longer term, we planto investigate new methods to estimate instanceweights at prediction time, and to evaluate the im-pact of assigned weights on sentence ranking, seg-mentation or summarization.AcknowledgmentsThe work described in this article was sup-ported by the European Union through the inEventproject FP7-ICT n. 287872 (see http://www.inevent-project.eu).464ReferencesStuart Andrews, Ioannis Tsochantaridis, and ThomasHofmann.
2003.
Support vector machines formultiple-instance learning.
In Advances in Neu-ral Information Processing Systems, pages 561?568,Vancouver, British Columbia, Canada.Stefano Baccianella, Andrea Esuli, and Fabrizio Se-bastiani.
2009.
Multi-facet rating of product re-views.
In Mohand Boughanem, Catherine Berrut,Josiane Mothe, and Chantal Soule-Dupuy, editors,Advances in Information Retrieval, volume 5478 ofLecture Notes in Computer Science, pages 461?472.Springer Berlin Heidelberg.Razvan C. Bunescu and Raymond J. Mooney.
2007.Multiple instance learning for sparse positive bags.In Proceedings of the 24th Annual InternationalConference on Machine Learning, ICML ?07, Cor-vallis, OR, USA.Jesse Davis et al.
2007.
Tightly integrating rela-tional learning and multiple-instance regression forreal-valued drug activity prediction.
In Proceedingsof the 24th International Conference on MachineLearning, ICML ?07, pages 425?432, Corvallis, OR,USA.Harris Drucker, Chris J.C. Burges, Linda Kaufman,Alex Smola, and Vladimir Vapnik.
1996.
Supportvector regression machines.
In Advances in Neu-ral Information Processing systems, pages 155?161,Denver, CO, USA.James Foulds and Eibe Frank.
2010.
A review ofmulti-instance learning assumptions.
The Knowl-edge Engineering Review, 25:1:1?25.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the 10thACM SIGKDD Int.
Conf.
on Knowledge discoveryand data mining, KDD ?04, pages 168?177, Seattle,WA.Fangtao Li, Nathan Liu, Hongwei Jin, Kai Zhao, QiangYang, and Xiaoyan Zhu.
2011.
Incorporating re-viewer and product information for review ratingprediction.
In Proceedings of the 22nd InternationalJoint Conference on Artificial Intelligence - Volume3, IJCAI ?11, pages 1820?1825, Barcelona, Catalo-nia, Spain.Bin Lu, Myle Ott, Claire Cardie, and Benjamin K.Tsou.
2011.
Multi-aspect sentiment analysis withtopic models.
In Proceedings of the 11th IEEE In-ternational Conference on Data Mining Workshops,ICDMW ?11, pages 81?88, Washington, DC.Andrew L. Maas, Raymond E. Daly, Peter T. Pham,Dan Huang, Andrew Y. Ng, and Christopher Potts.2011.
Learning word vectors for sentiment analysis.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies - Volume 1, HLT ?11, pages142?150, Portland, OR.J.
McAuley, J. Leskovec, and D. Jurafsky.
2012.Learning attitudes and attributes from multi-aspectreviews.
In Proceedings of the 12th IEEE Inter-national Conference on Data Mining, ICDM ?12,pages 1020?1025, Brussels, Belgium.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,and ChengXiang Zhai.
2007.
Topic sentiment mix-ture: modeling facets and opinions in weblogs.
InProceedings of the 16th Int.
Conf.
on the World WideWeb, WWW ?07, pages 171?180, Banff, AB.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploit-ing class relationships for sentiment categorizationwith respect to rating scales.
In Proceedings of the43rd Annual Meeting on Association for Computa-tional Linguistics, ACL ?05, pages 115?124, AnnArbor, MI.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: Sentiment classification us-ing machine learning techniques.
In Proceedingsof the ACL Conf.
on Empirical Methods in Natu-ral Language Processing, EMNLP ?02, pages 79?86, Philadelphia, PA.Nikolaos Pappas and Andrei Popescu-Belis.
2013.Sentiment analysis of user comments for one-classcollaborative filtering over TED talks.
In 36th ACMSIGIR Conference on Research and Development inInformation Retrieval, SIGIR ?13, Dublin, Ireland.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake VanderPlas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and Edouard Duchesnay.
2012.Scikit-learn: Machine learning in python.
CoRR,abs/1201.0490.Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum.2010.
The bag-of-opinions method for review ratingprediction from sparse text patterns.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics, COLING ?10, pages 913?921,Beijing, China.Soumya Ray and David Page.
2001.
Multiple instanceregression.
In Proceedings of the 18th InternationalConference on Machine Learning, ICML ?01, pages425?432.Christina Sauper and Regina Barzilay.
2013.
Auto-matic aggregation by joint modeling of aspects andvalues.
Journal of Artificial Intelligence Research,46(1):89?127.Christina Sauper, Aria Haghighi, and Regina Barzi-lay.
2010.
Incorporating content structure intotext analysis applications.
In Proceedings of the2010 Conference on Empirical Methods in Natural465Language Processing, EMNLP ?10, pages 377?387,Cambridge, MA.Burr Settles, Mark Craven, and Soumya Ray.
2008.Multiple-instance active learning.
In Advances inNeural Information Processing Systems, NIPS ?08,pages 1289?1296, Vancouver, BC.Benjamin Snyder and Regina Barzilay.
2007.
Multi-ple aspect ranking using the good grief algorithm.In In Proceedings of the Human Language Technol-ogy Conference of the North American Chapter ofthe Association of Computational Linguistics, HLT-NAACL ?07, pages 300?307, Rochester, NY, USA.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?11, pages 151?161, Ed-inburgh, UK.Richard Socher, Alex Perelygin, Jean Y. Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?13, pages 1631?1642, Portland, OR.Oscar T?ackstr?om and Ryan McDonald.
2011.
Dis-covering fine-grained sentiment with latent variablestructured prediction models.
In Proceedings of the33rd European Conference on Advances in Infor-mation Retrieval, ECIR?11, pages 368?374, Berlin,Heidelberg.
Springer-Verlag.Robert Tibshirani.
1996.
Regression shrinkage and se-lection via the lasso.
Journal of the Royal StatisticalSociety (Series B), 58:267?288.Ivan Titov and Ryan McDonald.
2008.
Modelingonline reviews with multi-grain topic models.
InProceedings of the 17th international conference onWorld Wide Web, WWW ?08, pages 111?120, Bei-jing, China.Peter D. Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classi-fication of reviews.
In Proceedings of the 40th An-nual Meeting on Association for Computational Lin-guistics, ACL ?02, pages 417?424, Philadelphia, PA.Kiri L. Wagstaff and Terran Lane.
2007.
Salience as-signment for multiple-instance regression.
In ICML2007 Workshop on Constrained Optimization andStructured Output Spaces, Corvallis, Oregon, USA.Kiri L. Wagstaff, Terran Lane, and Alex Roper.
2008.Multiple-instance regression with structured data.
InProceedings of the IEEE International Conferenceon Data Mining Workshops, ICDMW ?08, pages291?300.Zhuang Wang, Vladan Radosavljevic, Bo Han, ZoranObradovic, and Slobodan Vucetic.
2008.
Aerosoloptical depth prediction from satellite observationsby multiple instance regression.
In Proceedings ofthe SIAM Int.
Conf.
on Data Mining, SDM ?08,pages 165?176, Atlanta, GA.Hua Wang, Feiping Nie, and Heng Huang.
2011.Learning instance specific distance for multi-instance classification.
In AAAI Conference on Arti-ficial Intelligence.Zhuang Wang, Liang Lan, and S. Vucetic.
2012.
Mix-ture model for multiple instance regression and ap-plications in remote sensing.
IEEE Transactions onGeoscience and Remote Sensing, 50(6):2226?2237.Janyce Wiebe, Theresa Wilson, Rebecca Bruce,Matthew Bell, and Melanie Martin.
2004.
Learn-ing subjective language.
Computational Linguistics,30(3):277?308, September.Min-Ling Zhang and Zhi-Hua Zhou.
2008.
M3MIML:A maximum margin method for multi-instancemulti-label learning.
In Data Mining, 2008.
ICDM?08.
Eighth IEEE International Conference on,pages 688?697, Dec.Min-Ling Zhang and Zhi-Hua Zhou.
2009.
Multi-instance clustering with applications to multi-instance prediction.
Applied Intelligence, 31(1):47?68.Zhi-Hua Zhou, Kai Jiang, and Ming Li.
2005.
Multi-instance learning based web mining.
Applied Intel-ligence, 22(2):135?147.Zhi-Hua Zhou, Yu-Yin Sun, and Yu-Feng Li.
2009.Multi-instance learning by treating instances as non-i.i.d.
samples.
In Proceedings of the 26th An-nual International Conference on Machine Learn-ing, ICML ?09, pages 1249?1256, Montreal, Que-bec, Canada.Jingbo Zhu, Chunliang Zhang, and Matthew Y. Ma.2012.
Multi-aspect rating inference with aspect-based segmentation.
IEEE Trans.
on Affective Com-puting, 3(4):469?481.Li Zhuang, Feng Jing, and Xiao-Yan Zhu.
2006.Movie review mining and summarization.
In Pro-ceedings of the 15th ACM International Conferenceon Information and Knowledge Management, CIKM?06, pages 43?50, Arlington, VA.466
