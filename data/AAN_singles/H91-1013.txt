Integration of Diverse Recognition Methodologies ThroughReevaluation of N-Best Sentence HypothesesM.
Ostendor~ A. Kannan~ S. Auagin$ O. KimballtR.
Schwartz.\]: J.R. Rohlieek~:t Boston University44 Cummington St.Boston, MA 02215ABSTRACTThis paper describes a general formalism for integrating twoor more speech recognition technologies, which could be devel-oped at different research sites using different recognition strate-gies.
In this formalism, one system uses the N-best search strat-egy to generate a list of candidate sentences; the list is rescorredby other systems; and the different scores axe combined to opti-mize performance.
Specifically, we report on combining the BUsystem based on stochastic segment models and the BBN sys-tem based on hidden Markov models.
In addition to facilitatingintegration of different systems, the N-best approach results ina large reduction in computation for word recognition using thestochastic segment modelINTRODUCTIONWhile most successful systems to date have been basedon hidden Markov models (HMMs), there may be utility incombining the HMM approach with some other very differ-ent approach.
For example, the research group at BostonUniversity is exploring the use of the Stochastic SegmentModel (SSM) \[9,11\] as an alternative to the HMM.
In con-trast to the HMM, the SSM scores a phoneme as a wholeentity, allowing a more detailed acoustic representation.
Iprevious work \[6\], it was demonstrated that the SSM is ef-fective in the task of phoneme recognition, with results onthe TIMIT database using context-independent phonememodels that are comparable to context-dependent HMMs.Thus, there is a good possibility that, with the proper useof context, the performance may surpass that of the HMMsystem.
Unfortunately, the computation required for theSSM is considerably greater than that for HMMs, makingit impractical to implement the standard optimal dynamicprogramming search algorithms.In this paper, we introduce a general formalism for inte-grating different speech recognition technologies, which alsoenables evaluation ofword recognition performance with theSSM.
In this approach, one recognition system uses the N-best search strategy to provide a list of sentence hypothe-ses.
A second system (presumably more complex) is used torescore these hypotheses, and the scores of the different sys-BBN Inc.10 Moulton St.Cambridge, MA 02138terns are combined, giving a new ranking of the sentence hy-potheses.
If the errors made by the two systems differ, thencombining the two sets of scores would yield an improve-ment in overall performance ( ither in terms of the percentof correct sentences or the average rank of the correct sen-tence).
The N-best formalism offers a means of reducingthe computation associated with combining the results oftwo systems by restricting the search space of the secondsystem.
It therefore also provides a lower cost mechanismfor evaluating word recognition performance of the SSM byitself, through simply ignoring the scores of the HMM inreranking the sentences.In the following section, we describe the integration method-ology in more detail.
Next, we present experimental resultscombining the stochastic segment model with the BBN By-blos system, including a result that incorporates statisticalgrammar scores as well as a benchmark result using theword-pair grammar.
Finally, we conclude with a discussionof possible implications and extensions of this work.INTEGRATION STRATEGYThe basic approach involves1.
computing the N best sentence hypotheses with onesystem;2. rescoring this list of hypotheses with a second system;and3.
combining the scores to improve overall performance.Although the scores from more than two systems can becombined using this methodology, we consider only two sys-tems here.
The BBN Byblos system was used to generatethe N best hypotheses, and the Boston University SSM sys-tem was used to rescore the N hypotheses.
Details of eachstep, based on the use of these two systems, are given below.N-Best  ScoringThe idea of scoring the N best sentence hypotheses wasintroduced by BBN as a strategy for integration of speech83and natural anguage \[3\].
Given a list of N candidate sen-tences, a natural anguage system could process the differ-ent hypotheses until reaching one that satisfied the syntacticand semantic onstraints of the task.
An exact, but some-what expensive algorithm for finding the N best sentencehypotheses was also described in \[3\].
Since then, severalsites have adopted the N-Best strategy for combining speechrecognition with natural language.
In addition, more effi-cient approximate scoring algorithms for finding the N Bestsentences have been developed (e.g., \[12,13\]).
These algo-rithms introduce only a short delay after finding the 1-Besthypothesis for finding the N-Best hypotheses.This same N-best scoring paradigm can be used for theintegration of different recognition techniques.
The maindifference is that, for the rescoring application, it is useful tohave the word and/or phoneme boundaries associated withthis hypothesis.
Since the recognition algorithm involvesmaximizing the joint probability of the HMM state sequenceand the observed ata, the boundaries can be obtained fromthe traceback array typically used in decoding.RescoringRescoring the list of hypotheses i  a constrained recogni-tion task, where the phoneme and/or word sequence is givenand the phonetic segmentation is optionally given.
Here weuse a stochastic segment model in rescoring, but any acous-tic model would be useful in this formalism.
(For example,a neural network model of phoneme segments i used in \[1\].
)The constrained recognition search is particularly useful forsegmental coustic models, which have a significantly largerrecognition search space than frame-based hidden Markovmodels.If the phoneme segmentations are given and assumedfixed, the computation required for rescoring is extremelysmall.
If the phoneme segmentations are not given for theN hypotheses, then rescoring is essentially automatic seg-mentation.
The maximum likelihood segmentation is givenby a dynamic programming algorithm, typically with min-imum and maximum phoneme duration constraints, as in\[9\].
Scoring a sentence with the optimal segmentation for amodel will yield better results than scoring according to thesegmentation determined by a different model, but the costin computation is significant (roughly a factor of 300 morethan using fixed segmentations).
Since we have found thestochastic segment model performance to be fairly sensitiveto boundary location, we anticipate that optimal segmen-tation may be very important.
A compromise strategy isto find the optimal segmentation subject to the constraintof being within a fixed number of frames of the HMM seg-mentation.
The constrained dynamic programming solutionappears to suffer no loss in performance and saves a factor of30 in computation relative to the unconstrained algorithm.A slight variation of the segmentation algorithm involvessearching for the optimal phone sequence and its segmen-tation, given a word sequence.
In other words, we allowalternative pronunciations in rescoring a sentence hypothe-sis.
We hypothesize that the use of alternative pronuncia-tions will significantly improve SSM word recognition per-formance, mainly because SSM phoneme recognition perfor-mance is much higher on the carefully hand-labeled TIMITdatabase than it is on the Resource Management Task (inwhich case we assume that the phone sequence assignedby the BBN single pronunciation recognizer is "correct").However, we have not investigated this question on a dic-tionary with a sufficiently rich set of pronunciations.
Theadditional cost of modeling multiple pronunciations shouldbe relatively small.Combin ing  ScoresAn important issue is how to combine the scores fromthe systems o as to optimize the performance of the overallsystem.
In this initial work, we chose to use a linear combi-nation of HMM log acoustic score, log grammar score, num-ber of words in the sentence (insertion penalty), number ofphonemes in the sentence, and SSM log acoustic score.
Thisis a simple extension of the current HMM system ranking,which uses the first three of these five measures.We estimate the set of weights that optimizes a general-ized mean of the rank of the correct answer:s1 re(S) = I'~ ~ r(i)~l ~" (1)i=1where r(i) is the rank of the correct answer in sentence i ofa set S of S sentences, and p determines the type of mean.For example, p = 1 specifies the average, p -- 2 specifiesthe root-mean-square, p ---- -1  specifies the harmonic mean,and p = -oo  only counts the percent correct.
For speechrecognition applications p = -oo  would be appropriate, butfor speech understanding applications, p = 1 might be moreuseful.
In practice we find that the different values of p didnot have a significant impact on the results.Estimation of the weights is an unconstrained multi-dimensional minimization problem.
The algorithm usedhere is Powell's method \[10\], which iteratively minimizes thegeneralized mean (Equation 1) by optimizing the weightsin successive conjugate directions.
Because the algorithmseemed to be sensitive to local optima, we determine theweights by trying several different initial points.
This strat-egy gave an increase in performance.EXPERIMENTAL RESULTSThe recognition experiments were based on the ResourceManagement (RM) corpus.
Both the BBN Byblos systemand the BU stochastic segment models were trained onthe speaker-independent SI109 corpus.
Both systems used84feature vectors comprised of 14 reel-warped cepstral coeffi-cients and the respective derivatives; the BBN system alsoused power and second derivatives of the cepstra.The basic BBN Byblos system is essentially the same asoriginally described in \[2\].
These experiments u ed context-dependent but not cross-word triphone models.
The mod-els are gender-dependent; the system scores a sentence withboth male and female models and then chooses the answerthat gives the highest score.
With few exceptions, the cor-rect speaker sex is chosen.
The Byblos system was usedto generate the top 20 sentence hypotheses for each utter-ance.
Experiments with larger numbers of hypotheses sug-gested that the additional rescoring computation was notwarranted.
This was due to the fact that, using the HMMmodels, the correct sentence was almost always includedwithin the top 20 hypotheses.Two different SSM systems were used to rescore thesehypotheses: one context-independent a d one using left-context phone models.
In both cases gender-dependentmodels are used, where the speaker sex was that chosenby the BBN system.
The model structure from the bestcase system found in previous tudies \[5\] was used.
Thissystem is based on independent samples, frame-dependentfeature transformations, and five distributions per model.Infrequently observed classes are modeled with a frame-dependent, model-independent tied covariance matrix, oth-erwise a model- and frame-dependent dovariance matrix isused.
Using more sophisticated estimation techniques, aswell as generalized triphones \[8\], would likely yield signifi-cant improvements for context-dependent models.
In addi-tion, recent work in time correlation modeling \[7\] could beused to improve performance, and this will be integratedinto a later version of the system.Results for two different est sets are described below.First, we investigated different score combinations on theFebruary 1989 RM test set.
Second, we report results on theFebruary 1991 RM benchmark test set, where the previoustest set is used to estimate weights for combining the scores.D i f fe rent  Score  Combinat ionsIn the first set of experiments, the N-best hypotheseswere generated using the Byblos system with a fully-connected'statistical bi-class grammar \[4\].
In this experiment, we useda grammar with 548 classes that had a perplexity of 23 onthe test set.
This system finds the correct sentence in thetop 20 hypotheses 98% of the time.
These sentences wererescored using the two different stochastic segment models.For each sentence hypothesis, the total score included thelog HMM acoustic score and/or the log SSM acoustic score(either context-independent or context-dependent).
In ad-dition, all score combinations included log grammar scores,word and phoneme count.
The weights for different combi-nations of scores were designed as described in the previousSystem ~o sent corr avg sent rankCI SSM, fixed segCI SSM, opt segCD SSM, opt segCD HMM, N-bestCD HMM, optimizedCD HMM + CI SSMCD HMM + CD SSM56.364.368.071.375.778.879.32.842.371.861.731.751.681.56Tab le  1: Percent sentence correct and average rank of correctsentence when it is in the top 20.
Results are based on theFeb.
1989 test set using a statistical c ass grammar.section, using the generalized mean optimization criterionwith p = -1.
Table 1 summarizes the performance of sev-eral different system combinations.The table shows improved performance for more com-plex versions of the stochastic segment model.
Using thefixed segmentations yields significantly lower performancefor the segment model, so all further experiments use theconstrained optimal segmentation.
The simple left-contextmodel results in improved performance over the context-independent model, both alone and in combination withthe HMM.
The HMM which uses triphone models outper-forms the SSM which uses left-context models; but the per-formance of the two systems is close in comparing percentsentence correct in the top N for N > 4 (see Figure 1).Table 1 also shows the improvement associated with therescoring formalism.
First, since the N-best search algo-rithm is sub-optimal, simply rescoring the hypotheses withthe original ttMM (referred to in the table and figure asan "optimized HMM") yields some improvement in perfor-mance.
More importantly, the results how that even at thelower level of performance of the SSM, combining the HMMand SSM scores yields improvement in performance, par-ticularly through raising the rank of the correct sentence.This is shown more clearly in Figure 1, which illustratesthe cumulative distribution function of percent of sentencescorrect in the top N hypotheses.As mentioned previously, this is a preliminary result, sowe expect additional improvement - both for the SSM aloneand the combined systems - from further research in SSMcontext modeling.Benchmark  Resu l tsA second experiment involved testing performance of thescoring combinations on the February 91 benchmark testset.
In this case, the 20 best sentence hypotheses were gen-erated using the word-pair grammar.
These sentences were85Percent Correct100.0095.0090.0085.0080.0075.00J S S \]J ?w ?e ?'
s ?
SCD HMM + SSM i?
.
.
.
.
.
.
.
.
.
o .
.ooooo .oo?
.
?oooo .Optimized HMMCD SSM2.00 4.00 6.00 8.00 10.00NF igure  1: Cumulative distr ibution function of percent sentences correct in the top N hypotheses for: (a) opt imized HMM, (b) con-text-dependent SSM, and (c) combined HMM and context-dependent SSM.restored using the context-independent SSM with the con-strained optimal segmentation algorithm.
The scores usedwere log HMM and SSM scores and word and phonemecounts; no grammar scores were used in this experiment.Weights were trained using the February 1989 test set.
Al-though p = --oo would be appropriate for this task, we usedp = -1  because of the sensitivity of the search to local op-tima.
In Table 2, we show benchmark test results for differ-ent combinations of HMM and SSM, with performance onthe February 1989 test set given for comparison.
For eachcase, we give the percent of the sentences recognized cor-rectly as the top choice and the average rank of the correctanswer when it is in the top 20..The HMM results reportedhere may be lower than other results reported in this pro-ceedings, since we are using a simpler version of the Byblossystem (specifically without cross-word phonetic models).As before, we find that the context-dependent HMM is out-performing the context-independent SSM, and that rescor-ing yields a small improvement in performance, mainly inaverage sentence rank.DISCUSSIONIn summary, we have introduced a new formalism for in-tegrating different speech recognition technologies based ongenerating the N best sentence hypotheses with one system,rescoring these hypotheses, and combining the scores of thedifferent systems.
This N-best rescoring formalism can beuseful in several ways.Specifically, it makes practical the implementation of acomputationally expensive system such as the StochasticSegment Model, and has allowed us to investigate the util-ity of the SSM for word recognition.
The results reportedhere are the first reported on the Resource ManagementSystem N-Best Optimal CI HMMI-IMM I-IMM SSM +SSMAvg sent rankFeb 89 2.13 2.15 3.07 2.11% sent corrFeb 89 67.7 69.7 50.0 70.0Feb 91 72.3 73.0 52.7 73.0% word errFeb 91 5.4 5.3 9.7 5.6Tab le  2: Percent sentence correct and average rank of correctsentence when it is in the top 20.
Results axe reported for devel-opment (Feb. 1989 test set) and bench.mark (Feb. 1991 test set),using a word-pair gram.max, but no grammax scores.task for the SSM.
Our initial results were much lower thanwould be predicted from phoneme recognition results onthe TIMIT database, underscoring the need for additionalsystem development.
The rescoring formalism will facih-tare further research in SSM word recognition, particularlyin the utilization of recent techniques developed for timecorrelation modeling and context modeling.
Research incontext-modeling is particularly facilitated by the rescoringformalism, since the computation time is the same order ofmagnitude as context-independent models.More generally, the rescoring formalism enables cross-site collaboration and fast evaluation of potential improve-ments in speech understanding associated with integrationof different knowledge sources.
It provides a simple mech-anism for integrating even radically different recognitiontechnologies, enabling higher performance than either tech-86nique alone.
The results reported here yield some improve-ment in performance, but we anticipate a greater effect withfuture improvements o the SSM.
Improvements can alsobe gained from further research on score combination, sincethe weight estimation technique was found to be very sensi-tive to initial starting points.
In addition, scores from verydifferent ypes of knowledge sources could be combined toimprove the performance of a speech understanding system.For example, if scores are combined after natural anguageprocessing, it would be possible to include a score whichrepresents the prosodic consistency of a parse \[14\].
This isone of many possible areas for future research.ACKNOWLEDGEMENTSThe authors gratefully acknowledge John Makhoul formany valuable comments.
This research was jointly fundedby NSF and DARPA under NSF grant number IRI-8902124.10.
W. H. Press, B. P. Flannery, S. A. Teukolsky and W. T.Vetterling, Numerical Recipes, Cambridge University Press,Cambridge 1986.11.
S. Roucos, M. Ostendoff, H. Gish, and A. Derr, "Stochas-tic Segment Modeling Using the Estimate-Maximize Algo-rithm," IEEE Int.
Con\].
Aeoust., Speech, Signal Processing,pp.
127-130, New York, New York, April 1988.12.
R. Schwartz and S. Austin, "Efficient, High PerformanceAlgorithms for N-Best Search," Proceedings of the ThirdDARPA Workshop on Speech and Natural Language, pp.6-11, June 1990.13.
F. K. Soong and E.-F. Huang, "A Tree-Trellis Based FastSearch for Finding the N-Best Sentence Hypotheses inContinuous Speech Recognition," Proceedings o\] the ThirdDARPA Workshop on Speech and Natural Language, pp.12-19, June 1990.14.
C. W. Wightman, N. M. Veilleux and M. Ostendorf "Us-ing Prosodic Phrasing in Syntactic Disambiguation: AnAnalysis-by-Synthesis Approach," this proceedings, 1991.REFERENCES1.
S. Austin, J. Makhoul, R. Schwartz and G. Zavaliagkos,"Continuous Speech Recognition Using Segmental NeuralNets," this proceedings.2.
Y. Chow ctal., "BYBLOS: The BBN Continuous SpeechRecognition System," IEEE Int.
Conf.
Acoust., Speech, Sig-nal Processing, pp.
89-92, 19877.3.
Y.-L. Chow and R. Schwartz, "The N-Best Algorithm:An Efficient Procedure for Finding Top N Sentence Hy-potheses," Proceedings of the Second DARPA Workshop onSpeech and Natural Language, pp.
199--202, October 1989.4.
A. Derr and R. Schwartz, "A Simple Statistical Class Gram-mar for Measuring Speech Recognition Performance," Pro-ceedings of the Second DARPA Workshop on Speech andNatural Language, pp.
147-149, October 1989.5.
V. Digalalds, M. Ostendoff and J. R. Rohlicek, "Improve-ments in the Stochastic Segment Model for Phoneme Recog-nition," Proceedings of the Second DARPA Workshop onSpeech and Natural Language, pp.
332-338, October 1989.6.
V. Digalakis, M. Ostendorf and J. R. Rohlicek, "Fast SearchAlgorithms for Connected Phone Recognition Using theStochastic Segment Model," manuscript submitted to IEEETrans.
Acoustic Speech and Signal Processing (a shorter ver-sion appeared Proceedings o\] the Third DARPA Workshopon Speech and Natural Language, June 1990).7.
V. Digalakis, J. R. Rohlicek and M. Ostendorf, "A Dynam-ical System Approach to Continuous Speech Recognition,"this proceedings, also to appear in the Proceedings of theInternational Conference on Acoustics, Speech and SignalProcessing, May 1991.8.
K.-F. Lee, "Context-dependent Phonetic Hidden MarkovModels for Speaker-Independent Continuous Speech Recog-nition," IEEE Trans.
Acoustic Speech and Signal Process-ing, Vol.
ASSP-38(4), pp.
599-609, April 1990.9.
M. Ostendoff and S. Roukos, "A Stochastic Segment Modelfor Phoneme-based Continuous Speech Recognition," IEEETrans.
Acoustic Speech and Signal Processing, Vol.
ASSP-37(12), pp.
1857-1869, December 1989.87
