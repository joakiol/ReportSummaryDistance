.The Effectiveness of Dictionary and Web-Based Answer RerankingChin-Yew LinInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292cyl@isi.eduAbstractWe describe an in-depth study of usinga dictionary (WordNet) and websearch engines (Altavista, MSN, andGoogle) to boost the performance ofan automated question answeringsystem, Webclopedia, in answeringdefinition questions.
The resultsindicate applying dictionary andweb-based answer reranking togetherincrease the performance ofWebclopedia on a set of 102 TREC-10definition questions by 25% in meanreciprocal rank score and 14% infinding answers in the top 5.1 IntroductionIn an attempt to further progress in informationretrieval research, the Text REtrieval Conference(TREC) sponsored by the National Institute ofStandards and Technology (NIST) started aseries of large-scale evaluations of domainindependent automated question answeringsystems in TREC-8 (Voorhees 2000) andcontinued in TREC-9 and TREC-10.
NTCIR(NII-NACSIS Test Collection for IR Systems,TREC?s counterpart in Japan) initiated itsquestion answering evaluation effort, QuestionAnswering Challenge (QAC) in 2001 (Fukumotoet al 2001).
Research systems participating inTRECs and the coming QAC focused on theproblem of answering closed-class questions thathave short fact-based answers (?factoids?)
from alarge collection of text.These systems bear a similar structure:(1) Question analysis ?
identify questionkeywords to be submitted to search engines(local or web), recognize question types, andsuggest expected answer types.
Although mostsystems rely on a taxonomy of expected answertypes, the number of nodes in the taxonomyvaries widely from single digits to a fewthousands.
For example, Abney et al (2000) used5; Ittycheriah et al (2001), 31; Hovy et al(2001), 140; Harabagiu et al (2001), 8,797.These taxonomies were mostly based on namedentities and WordNet (Fellbaum 1998).
Specialtypes such definition questions (ex: ?What is anatom??)
were added as necessary.
(2) Passage or Sentence retrieval ?
this aims toprovide a text pool of manageable size forextracting candidate answers.
Most topperforming systems in TRECs use their ownretrieval methods for passages (Brill et al 2001;Clarke et al 2001; Harabagiu et al 2001) orsentences (Hovy et al 2001).
(3) Candidate answer extraction ?
extractcandidate answers according to answer types.
Ifthe expected answer types are typical namedentities, information extraction engines (Bikel etal.
1999, Srihari and Li 2000) are used to extractcandidate answers.
Otherwise special answerpatterns are used to pinpoint answers.
Forexample, Soubbotin and Soubbotin (2001) createa set of 6 answer patterns for definition questions.
(4) Answer ranking ?
assign scores to candidateanswers according to their frequency in topranked passages (Abney et al 2000; Clarke et al2001), similarity to candidate answers extractedfrom external sources such as the web (Brill et al2001; Buchholz 2001) or WordNet (Harabagiu etal.
2001; Hovy et al 2001), density, distance, ororder of question keywords around thecandidates, similarity between the dependencystructures of questions and candidate answers(Harabagiu et al 2001; Hovy et al 2001;Ittycheriah et al 2001), and match of expectedanswer types.In this paper, we describe an in-depth study ofanswer reranking for definition questions.Definition questions account for over 100 (20%)test questions in TREC-10.
They are not namedentities that have been the cornerstones of many.high performance QA systems (Srihari and Li2000; Harabagiu et al 2001).By reranking we mean the following.
Assume aQA system such as Webclopedia (Section 3)provides an initial set of ranked candidateanswers from the TREC corpus.
The ranking isbased on the IR engine?s passage or sentencematch scores.
One can then measure theeffectiveness of utilizing resources such asWordNet or the web to rerank the initial results,hoping to achieve better mean reciprocal rank(MRR) and percent of correctness in the top 5(PTC5).Answer reranking is often overlooked.
Theanswer candidates (<= 400 instances perquestion) generated by Webclopedia from TRECcorpus included answers for 83% of 102definition questions used in this study (theTREC-10 definition questions).
However,Webclopedia ranked only 64% of them in the top5, giving an MRR score of 45%.
If a perfectanswer reranking function had been used, thebest achievable MRR would have been 83% (an84% increase over the original 45%).Section 2 gives a brief overview of TREC-10.Section 3 outlines the Webclopedia system.Section 4 defines definition questions anddescribes our dictionary and web-basedreranking methods.
Section 5 presentsexperiments and results.
We conclude withlessons learned and future work.2 TREC-10 Q&A TrackThe main task of the TREC-10 (Voorhees andHarman 2002) QA track required participants toreturn a ranked list of five answers of no morethan 50 bytes long per question that weresupported by the TREC-10 QA text collection.The TREC-10 QA document collection consistsof newspaper and newswire articles on TRECdisks 1 to 5.
It contains about 3 GB of texts.
Testquestions were drawn from filtered MSNSearchand AskJeeves logs.
NIST assessors then sifted500 questions from the filtered logs as test set.The questions were closed-class fact-based(?factoid?)
questions such as ?How far is it fromDenver to Aspen??
and ?What is an atom?
?.Mean reciprocal rank (MRR) was used as theindicator of system performance.
Each questionreceives a score as the reciprocal of the rank ofthe first correct answer in the 5 submittedresponses.
No score is given if none of the 5responses contain a correct answer.
MRR is thencomputed for a system by taking the mean of thereciprocal ranks of all questions.Besides MRR score, we are also interested inlearning how well a system places a correctanswer within the five responses regardless of itsrank.
We called this percent of correctness in thetop 5 (PCT5).
PCT5 is a precision related metricand indicates the upper bound that a system canachieve if it always places the correct answer asits first response.3 Webclopedia: An AutomatedQuestion Answering SystemWebclopedia?s architecture follows the principleoutlined in Section 1.
We briefly describe eachstage in the following.
Please refer to (Hovy et al2002) for more detail.
(1) Question Analysis: We used an in-houseparser, CONTEX (Hermjakob 2001), to parse andanalyze questions and relied on BBN?sIdentiFinder (Bikel et al, 1999) to provide basicnamed entity extraction capability.
(2) Document Retrieval/Sentence Ranking:The IR engine MG (Witten et al 1994) was usedto return at least 500 documents using Booleanqueries generated from the query formationstage.
However, fewer than 500 documents maybe returned when very specific queries are given.To decrease the amount of text to be processed,the documents were broken into sentences.
Eachsentence was scored using a formula that rewardsword and phrase overlap with the question andexpanded query words.
The ranked sentenceswere then filtered by expected answer types (ex:dates, metrics, and countries) and fed to theanswer extraction module.
(3) Candidate Answer Extraction: We againused CONTEX to parse each of the top Nsentences, marked candidate answers by namedentities and special answer patterns such asdefinition patterns, and then started the rankingprocess.
(4) Answer Ranking: For each candidate answerseveral steps of matching were performed.
Thematching process considered question keywordoverlaps, expected answer types, answerpatterns, semantic type, and the correspondence.of question and answer parse trees.
Scores weregiven according to the goodness of the matching.The candidate answers?
scores were comparedand ranked.
(5) Answer Reranking, Duplication Removal,and Answer output: For some special questiontype such as definition questions (e.g., ?What iscryogenics??
), we used WordNet glosses or websearch results to rerank the answers.
Duplicateanswers were removed and only one instance waskept to increase coverage.
The best 5 answerswere output.
Answer reranking is the main topicof this paper.
Section 4 presents these methods indetail.4 Dictionary and Web-BasedAnswer Reranking4.1 Definition QuestionsCompared to other question types, definitionquestions are special.
They are typically veryshort and in the form of ?What is|are (a|an) X?
?,where X is a 1 to 3 words term1, for example:?What is autism?
?, ?What is spider veins??
and?What is bangers and mash??.
As we learnedfrom past TREC experience, it was more difficultto find relevant documents for short queries.
Asstated earlier, over 20% of questions in TREC-10were of definition type, which was a reflection ofreal user queries mined from the web searchengine logs (Voorhees 2001).
Several topperforming systems in the evaluation treated thistype of question as a special category and most ofthem used definition answer patterns.
The bestperforming system, InsightSoft-M, (Soubbotinand Soubbotin 2001) used a set of six definitionpatterns including P1:{<Q; is/are; [a/an/the]; A>,<A; is/are; [a/an/the]; Q>} and P2:{<Q; comma;[a/an/the]; A; [comma/period]>, <A; comma;[a/an/the]; Q; [comma/period]>}, where Q is theterm to be defined and A is the candidate answer.The InsightSoft-M system returned 88 correctresponses based on these patterns.
The runner upsystem (Harabagiu et al 2001) used 12 answerpatterns with extension of WordNet hypernyms.They did not report their success rate forTREC-10 but according to Pa?ca (2001)2, this set1Among the 102 TREC-10 definition questions, 81asked the definition of one word; 19, two words; 2,three words.2Among them 31 were extracted through patternof patterns with WordNet extension extracted 59out of 67 definition questions in TREC-8 andTREC-9.The success stories of these systems indicatedthat carefully crafted answer patterns wereeffective in candidate answer extraction.However, just applying answer patterns blindlymight lead to disastrous results, as shown byHermjakob (2002), since correct and incorrectanswers were equally likely to match thesepatterns.
For example, for the question ?What isautism?
?, the following answers are found in theTREC-10 corpus using the patterns described bythe InsightSoft-M system:?
autismQ, a nourishingA, equivocal ??
autismQ, the disorder isA, in fact, ??
autismQ, the discovery could open newapproaches for treating tAhe ??
autismQ is a mental disorder that is a?severely incapacitatinAg ??
autismQ, the inability to communicatewith othersA.Obviously, patterns alone cannot distinguishwhich one is the best answer.
Some othermechanisms are necessary.
We propose twodifferent methods to solve this problem.
One is adictionary-based method using WordNet glossesand the other is to go directly to the web andcompile web glosses on the fly to help select thebest answers.
The effect of combining bothmethods was also studied.
We describe these twomethods in the following sections.4.2 Dictionary-Based RerankingUsing a dictionary to look up the definition of aterm is the most straightforward solution foranswering definition questions.
For example, thedefinition of autism in the WordNet is: ?anabnormal absorption with the self; marked bycommunication disorders and short attentionspan and inability to treat others as people?.However, we need to find a candidate answerstring from the TREC-10 corpus that isequivalent to this definition.
By inspection, wefind that candidate answers ?, ?, and ?
shownin the previous section are more compatible tothe definition and ?
seems to be the best one.To automate the decision process, we construct adefinition database based on the WordNet nounmatching and 27 were from WordNet hypernymexpansion..glosses.
Closed class words are thrown away andeach word wi in the glosses is assigned a glossweight wnis  as follows3:)1/log( += iwni nNswhere ni is the number of times word wioccurring in the WordNet noun glosses and N istotal number of occurrences of all noun glosswords in the WordNet.
The goodness of thematching Mwn for each candidate answer issimply the sum of the weight of the matchedword stems between its WordNet definition anditself.
For example, candidate answer ?
andautism?s WordNet definition have these matches:{inability5 ?
inabilitywn, communicate5 ?communicationwn, others5 ?
otherswn}.
Thereranking score Swn for each candidate answer isits original score multiplied by Mwn.
The finalranking is then sorted according to Swn, duplicateanswers are removed, and the top 5 answers areoutput.
Table 1 shows the top 5 answers returnedbefore and after applying dictionary-basedreranking.
It demonstrates that dictionary-basedreranking not only pushes the best answer to thefirst place but also boosts other lower rankedgood answers i.e.
?a mental disorder?
to thesecond place.Harabagiu et al (2001) also used WordNet toassist in answering definition questions.However, they took the hypernyms of the term tobe defined as the default answers while we usedits glosses.
The hypernym of ?autism?
is?syndrome?.
In this case it would not boost thedesired answer to the top but it would instead?validate?
?Down?s syndrome?
as a good answer.Further research is needed to investigate thetradeoff between using hypernyms and glosses.WordNet glosses were incorporated in IBM?sstatistical question answering system asdefinition features (Ittycheriah et al 2001).3This is essentially inverse document (WordNet glossentry) frequency (IDF) used in the informationretrieval research.However, they did not report the effectiveness ofthe features in definition answer extraction.Out of vocabulary words is the major problem ofdictionary-based reranking.
For example, noWordNet entry is found for ?e-coli?
butsearching the term ?e-coli?
at www.altavista.comand www.google.com yield the following:?
E. coli is a food borne illness.
Learnabout prevention, symptoms and risks,detection, ... Risks Detection RecentOutbreaks Resources The term E. coli isan abbreviation for the bacteriaEscherichia.
(1st hit, www.altavista.com)?
The E. coli Index (part of the WWWVirtual Library) ?
Description: Guide toinformation relating to the modelorganism Escherichia coli.
From theWWW Virtual Library.
(1st hit,www.google.com)This brings us to the web-based rerankingmethod that we introduce in the next section.4.3 Web-Based RerankingThe World Wide Web contains massive amountsof information covering almost any thinkabletopic.
The TREC-10 questions are typicalinstances of queries for which users tend tobelieve answers can be found from the web.However, the candidate answers extracted fromthe web have to find support in the TREC-10corpus in order to be judged as correct otherwisethey will be marked as unsupported.The search results of ?e-coli?
from two onlinesearch engines indicate that ?e-coli?
is anabbreviation for the bacteria Escherichia.However, to automatically identify ?e-coli?
as?Escherichia?
from these two pages is the sameQA problem that we set off to resolve.
The onlyadvantage of using the web instead of just theTREC-10 corpus is the assumption that the webcontains many more redundant candidateanswers due to its huge size.
Compared toTable 1.
Top 5 answers returned before (Webclopedia) and after (Webclopedia + WordNet)dictionary-based answer reranking for question ?What is autism??.
A ?-?
indicates wrong answers anda ?+?
indicates correct answers.Autism Webclopedia Webclopedia + WordNet1 - Down's syndrome + the inability to communicate with others2 - mental retardation + a mental disorder3 + the inability to communicate with others - NIL4 - NIL - Down's syndrome5 - a group of similar-looking diseases - mental retardation.Google?s 2,073,418,204 web pages4, TREC-10corpus contains only about 979,000 articles.For a given question, we first query the web,apply answer extraction algorithms over a set oftop ranked web pages (usually in the lowerhundreds), and then rank candidate answersaccording to their frequency in the set.
Thisassumes the more a candidate answer occurs inthe set the more likely it is the correct answer.Clarke et al (2001) and Brill et al (2001) bothapplied this principle and achieved good results.Instead of using Webclopedia to extractcandidate answers from the web and then projectback to the TREC-10 corpus, we treat the web asa huge dynamic dictionary.
We compile webglosses on the fly for each definition question andapply the same reranking procedure used in thedictionary-based method.
We detail theprocedure in the following.
(1) Query a search engine (e.g., Altavista) withthe term (e.g., ?e-coli?)
to be defined.
(2) Download the first R pages (e.g., R = 70).
(3) Extract context word ciw  within a window ofW (e.g., W = 10) words centered at the term to bedefined from each page.
Closed class words areignored.
These context words are used ascandidate web glosses.
(4) The gloss weight webis  for each word ciw  iscomputed as follows5:)1/log( +?= iiwebi nNtswhere ti is the frequency of ciw  in the set ofcontext words extracted in (3), N is the totalnumber of training questions, and ni is thenumber of training questions in which ciwoccurs.
(5) The goodness of the matching Mweb4This was the number that Google(www.google.com) advertised at its front page as ofJanuary 31, 2002.5This is essentially TFIDF (product of term frequencyand inverse document frequency) used in theinformation retrieval research.for each candidate answer is simply the sum ofthe weights of the matched word stems betweenits web gloss definition and itself.
Only wordswith gloss weight Tswebi ?
are used to computeMweb.
The value of T serves as a cut-off thresholdto filter out low confidence words.
(6) The reranking score Sweb for each candidateanswer is its original score multiplied by Mweb.The final ranking is then sorted according to Sweb,duplicate answers are removed, and the top 5answers are output.
Table 2 shows the top 5answers returned before and after applyingweb-based reranking for the question ?What isWimbledon??.
Google was used as the searchengine with T=5, W=10, and R=70.5 Experiments and ResultsWe used a set of 102 definition questions fromTREC-10 QA track as our test set.
Theperformance of Webclopedia without dictionaryor web-based answer reranking was used as thebaseline.
Webclopedia with dictionary-basedanswer reranking.To study the effect of using different searchengines, context window sizes, number of topranked web pages, and web gloss weight cut-offthreshold on the performance of web-basedanswer reranking, we had the following setup:?
Three search engines (E): Altavista (EA),Google (EG), and MSNSearch (EM).?
A run that combined all three searchengines?
results (EX).?
Two different context window sizes (W):5 (W5) and 10 (W10).?
Eleven sets of top ranked web pages(Rx): top 5, 10, 20, 30, 40, 50, 60, 70, 80,90, and 100.?
Two different gloss weight cut-offthresholds (T): 5 (T5) and 10 (T10).To investigate the performance of combiningdictionary and web-based answer reranking, weran the above setup again but each question?sreranking score Sweb+wn was the multiplication ofTable 2.
Top 5 answers returned before (Webclopedia) and after (Webclopedia + Google) web-basedanswer reranking for question ?What is Wimbledon??.
A ?-?
indicates wrong answers and a ?+?indicates correct answers.Wimbledon Webclopedia Webclopedia + Google (T=5, W=10, R=70)1 - the French Open and the U.S. Open.
+ the most famous front yard in tennis and scene2 - SW20, which includes a Japanese-style water garden - the French Open and the U.S. Open.3 + the most famous front yard in tennis and scene - NIL4 - NIL - Sampras' biggest letdown of the year5 - Sampras' biggest letdown of the year - Lawn Tennis & Croquet Club, home of the Wimbledon.its original score, web-based matching scoreMweb, and dictionary-based matching score Mwn.A total of 354 runs were performed.
Manualevaluation of these 354 runs was not impossiblebut would be time consuming.
We instead usedthe answer patterns provided by NIST to score allruns automatically.Due to space constraint, Table 3 shows the(MRR, PCT5) score pair for 90 runs out of 352runs.
The other two runs were the baseline runwith a score pair of (0.450, 0.637) and thedictionary-based run, (0.535, 0.667).
The bestrun was the combined dictionary and web-basedrun using Google as the search engine with10-word context window, 70 top ranked pages,and a gloss weight cut-off threshold of 5.Analyzing all runs according to Table 3, we madethe following observations.
(1) Dictionary-based reranking improvedbaseline performance by 19% in MRR and 5% inPCT5 (MRR: 0.535, PCT5: 0.667).
(2) The best web-based reranking (MRR: 0.539,PCT5: 0.676) was achieved with W=10, R=70,and T=5.
It was comparable to thedictionary-based reranking.
(3) Web-based reranking generally improvedresults.
Only 6 runs6 (not shown in the table) didworse in their MRR scores than just usingWebclopedia alone and these runs concentratedon low ranked page counts of 5 and 10.6These were EAT5W5R5 (0.437, 0.598), EAT10W5R5(0.434, 0.608), EAT10W10R5 (0.437, 0.598),EMT5W5R5 (0.436, 0.608), EMT10W5R5 (0.438, 0.608),and EMT10W10R5 (0.443, 0.618).
(4) Different search engines reached their bestperformance at different parameter settings.Overall Google did better.
(5) Combining multiple search engine results(runs designed with X and X+) did not alwaysimprove performance.
In some cases, it evendegraded system performance (ExT5W10R70:0.519, 0.637).
(6) Lower web gloss weight cut-off thresholdwas better at 5.
(7) Longer context window was better at 10 (notshown in the table).
(8) Taking top ranked pages of 50 to 90 pagesprovided better results.
(9) Combining dictionary and web-basedreranking always did better than using theweb-based method alone.
(10) Using WordNet and Google together wasalways better than just using WordNet alne inboth MRR and PCT5 (the underlined cells).5.1 Question DifficultyTo investigate the effectiveness of usingdictionary and web-based answer reranking onquestion of different difficulty, we definequestion difficulty as: )/(1 Nnd ?= , where nis the number of systems participating inTREC-10 that returned answers in top 5 and N isthe number of total runs (that is, 67 forTREC-10).
When d = 1 no systems provided ananswer in top 5; while d = 0 if all runs provided atleast one answer in top 5.
Table 4 shows theimprovement of MRR and PCT5 scores at fourdifferent question difficulty levels with fourdifferent system setups.
The results indicate thatusing either dictionary or web-based answerreranking improved system performance at alllevels.
The best results were achieved whenevidence from both resources was used.However, it also demonstrates the difficulty ofimproving performance on very hard questions(d>=0.75).
This implies we might need toconsider alternative methods to improve thesystem performance further.Table 3.
Results of 90 runs shown in (MRR, PCT5)score pair where A: Altavista, G: Google, M:MSNSearch, X: all three search engines, W: contextwindow size, R: number of top ranked web paged used,T: web gloss weight cut-off threshold.
Runs markedwith ?+?
indicate both dictionary and web-basedanswer reranking are used.Table 4.
System performance at different questiondifficulty levels.
(F: Webclopedia only, F+:Webclopedia with WordNet, FG: Webclopediawith Google, and F+G: Webclopedia withWordNet and Google)d>=0.00 (102) d>=0.25 (95) d>=0.50 (71) d>=0.75 (40)F (0.450,0.637) (0.394,0.611) (0.264,0.549) (0.084,0.375)F+ (0.535,0.667) (0.474,0.642) (0.323,0.592) (0.100,0.375)FG (0.539,0.676) (0.475,0.653) (0.319,0.592) (0.125,0.375)F+G (0.561,0.725) (0.498,0.705) (0.333,0.648) (0.128,0.400)W10R 10 W10R 30 W10R 50 W10R 70 W10R 90A T 5 (0.485,0.637) (0.499,0.637) (0.516,0.637) (0.525,0.667) (0.519,0.647)A T 10 (0.463,0.618) (0.497,0.637) (0.506,0.627) (0.511,0.657) (0.502,0.647)A +T 5 (0.503,0.667) (0.518,0.667) (0.538,0.676) (0.555,0.696) (0.548,0.696)A +T 10 (0.502,0.667) (0.515,0.667) (0.528,0.667) (0.528,0.676) (0.525,0.676)GT 5 (0.513,0.637) (0.515,0.647) (0.536,0.647) (0.539,0.676) (0.515,0.657)GT 10 (0.497,0.637) (0.503,0.647) (0.527,0.647) (0.523,0.657) (0.518,0.637)G+T 5 (0.551,0.686) (0.537,0.667) (0.557,0.676) (0.561,0.725) (0.547,0.706)G+T 10 (0.536,0.676) (0.530,0.676) (0.547,0.676) (0.544,0.706) (0.545,0.686)M T 5 (0.521,0.647) (0.513,0.627) (0.517,0.647) (0.514,0.637) (0.499,0.637)M T 10 (0.505,0.627) (0.499,0.608) (0.502,0.637) (0.488,0.627) (0.493,0.608)M +T 5 (0.543,0.676) (0.552,0.667) (0.544,0.676) (0.542,0.696) (0.533,0.676)M +T 10 (0.527,0.647) (0.537,0.647) (0.525,0.667) (0.519,0.696) (0.520,0.667)XT 5 (0.526,0.647) (0.539,0.676) (0.533,0.627) (0.519,0.637)* (0.515,0.627)XT 10 (0.509,0.618) (0.524,0.657) (0.532,0.627) (0.524,0.647) (0.517,0.637)X+T 5 (0.553,0.696) (0.551,0.696) (0.556,0.686) (0.550,0.696) (0.546,0.686)X+T 10 (0.531,0.657) (0.543,0.686) (0.555,0.686) (0.550,0.696) (0.546,0.686).6 ConclusionsWe described dictionary-based answer rerankingusing WordNet, web-based answer rerankingusing three different online search engines, andtheir evaluations at various parameter settings ona set of 102 TREC-10 definition questions.
Weshowed that using either approach aloneimproved MRR score by 19% and PCT5 score by5%  over the baseline.
However, the bestperformance was achieved when both methodswere used together.
In that setting a 25% increasein MRR score and 14% improvement in PCT5score were obtained.The difference on the best MRR and PCT5 scores(0.56 vs. 0.73) suggests neither dictionary-basednor web-based will solve the reranking problemcompletely.To improve the performance further, we needbetter ways to compile web glosses and combinethem with WordNet glosses.
We also need abetter combination function ?
a statistical modelfor combining patterns, dictionary, and webscores.
We have started investigating thepossibility of applying answer reranking to otherquestion types and exploring specialized webresources.ReferencesAbney, S., M. Collins, and A. Singhal.
2000.Answer Extraction.
In Proceedings of the AppliedNatural Language Processing Conference(ANLP-NAACL-00), Seattle, WA, 296?301.Bikel, D., R. Schwartz, and R. Weischedel.
1999.An Algorithm that Learns What?s in a Name.
InMachine Learning?Special Issue on NL Learning,34, 1?3.Brill, E., J. Lin, M. Banko, S. Dumais, and A. Ng.2001.
Data-Intensive Question Answering.
InProceedings of the TREC-10 Conference.
NIST,Gaithersburg, MD, 183?189.Buchholz, S. 2001.
Using Grammatical Relations,Answer Frequencies and the World Wide Web forTREC Question Answering.
In Proceedings of theTREC-10 Conference.
NIST, Gaithersburg, MD,496?503.Clarke, C.L.A., G.V.
Cormack, T.R.
Lynam, C.M.Li, and G.L.
McLearn.
2001.
Web ReinforcedQuestion Answering.
In Proceedings of the TREC-10Conference.
NIST, Gaithersburg, MD, 620?626.Fellbaum, Ch.
(ed).
1998.
WordNet: An ElectronicLexical Database.
Cambridge: MIT Press.Fukumoto, J, T. Kato, and F. Masui.
2001.
NTCIRWorkshop 3 QA Task ?
Question AnsweringChallenge (QAC).
http://research.nii.ac.jp/ntcir/workshop/qac/cfp-en.html.Harabagiu, S., D. Moldovan, M. Pa?ca, R.Mihalcea, M. Surdeanu, R. Buneascu, R. G?rju, V.Rus and P. Morarescu.
2001.
FALCON: BoostingKnowledge for Answer Engines.
In Proceedings ofthe 9th Text Retrieval Conference (TREC-9), NIST,479?488.Hermjakob, U.
2001.
Parsing and QuestionClassification for Question Answering.
InProceedings of the Workshop on Open-DomainQuestion Answering post-conference workshop ofACL-2001, Toulouse, France.Hermjakob, U.
2002.
Open Questions RegardingPrecision of the Insight Q&A System.
Personalcommunication..Hovy, E.H., U. Hermjakob, and C.-Y.
Lin.
2001.The Use of External Knowledge in Factoid QA.
InProceedings of the TREC-10 Conference.
NIST,Gaithersburg, MD, 166?174.E.H.
Hovy, U. Hermjakob, C-Y.
Lin, and D.Ravichandran.
2002.
Using Knowledge to FacilitatePinpointing of Factoid Answers.
In Proceedings of the19th International Conference on ComputationalLinguistics (COLING 2002), Taipei, Taiwan, August24 - September 1, 2002.Ittycheriah, A., M. Franz, and S. Roukos.
2001.IBM?s Statistical Question Answering System.
InProceedings of the TREC-10 Conference.
NIST,Gaithersburg, MD, 317?323.Pa?ca, M. 2001.
High Performance, Open-DomainQuestion Answering from Large Text Collections.Ph.D.
dissertation, Southern Methodist University,Dallas, TX.Soubbotin, M.M.
and S.M.
Soubbotin.
2001.Patterns of Potential Answer Expressions as Clues tothe Right Answer.
In Proceedings of the TREC-10Conference.
NIST, Gaithersburg, MD, 175?182.Srihari, R. and W. Li.
2000.
A QuestionAnswering System Supported by InformationExtraction.
In Proceedings of the 1st Meeting of theNorth American Chapter of the Association forComputational Linguistics (ANLP-NAACL-00),Seattle, WA, 166?172.Voorhees, E.M. 1999.
The TREC-8 Question andAnswering Track Report.
In Proceedings of theEighth Text REtrieval Conference (TREC-8), pages77?82, 2000.
NIST Special Publication 500-246.Voorhees, E. 2001.
Overview of the QuestionAnswering Track.
In Proceedings of the TREC-10Conference.
NIST, Gaithersburg, MD, 71?81.Voorhees, E.M. and D.K.
Harman.
2001.
TheProceedings of the Eighth Text REtrieval Conference(TREC-10), NIST.Witten, I.H., A. Moffat, and T.C.
Bell.
1994.Managing Gigabytes: Compressing and IndexingDocuments and Images.
New York: Van NostrandReinhold.
