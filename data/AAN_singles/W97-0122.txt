Using Word Frequency Lists to Measure Corpus Homogeneity andSimilarity between Corpora.Adam KilgarriffInformation Technology Research InstituteUniversity of BrightonLewes Road, Brighton BN2 4JG, UKemail: Adam.Ki lgarr i f f@i~ri .bton.
ac.ukAbstractHow similar are two corpora?
A measure of corpus similarity would be very useful forlexicography and language engineering.
Word frequency lists are cheap and easy to generateso a measure based on them would be of use as a quick guide in many circumstances; forexample, to judge how a newly available corpus related to existing resources, or how easy itmight be to port an NLP  system designed to work with one text type to work with another.We show that corpus similarity can only be interpreted in the light of corpus homogeneity.The paper presents a measure, based on the XX 2 statistic, for measuring both corpus imilarityand corpus homogeneity.
The measure is compared with a rank-based measure and shownto outperform it.
Some results are presented.
A method for evaluating the accuracy of themeasure is introduced and some results of using the measure are presented.1 IntroductionHow similar are two corpora?
The question arises on many occasions.
Does it matter whetherlexicographers use this corpora or that, or are they similar enough for it to make no difference?
(The original impetus for the research was the question, "are the fiction and jourDali~m parts ofthe Longman Lancaster Corpus and British National Corpus I (BNC) interchangeable?
x) In NLP,many useful results can be generated from corpora, but when can the results developed usingone corpus be applied to another?
There are also questions of more general interest.
Lookingat British national newspapers: is the Independent more like the Guardian or the Telegraph?There are many ways in which the question could be addressed, but the one we take here is totake texts from each newspaper and compare the frequencies of words used.
Given an accuratelypart-of-speech-tagged or parsed corpus, the same method could be applied to frequency lists ofparts-of-speech or syntactic constructions, and the methodological part of the paper would stillbe salient.
Section 2 presents the case for using word frequencies.How homogeneous is a corpus?
The question is both of interest in its own right, and is apreUminary to any quantitative approach to corpus similarity.
It is of interest in its own right,because a sublanguage corpus, or one containing only a specific language variety, has very differentcharacteristics to a general corpus (Biber, 1993), yet it is not obvious how a corpus's position onthis scale can be assessed.
It is of interest as a preliminary to a measure of corpus similarity,because it is not clear what a measure of similarity would mean if a homogeneous corpus was1 See hi;up: l / l ogo ,  oz .
ac .
ak/bnc231!being compared with a heterogeneous one.
For the statistical language mode11ing community, thepreferred approach to assessing homogeneity is by calculating perplexity, and the approach canbe extendlng to measuring similarity by calculating cross-entropy (Charniak, 1993).
In section 6compare these methods with the approach developed here.In this paper we present a method for measuring both corpus similarity and corpus homo-geneity.
In brief, the method (for the homogeneity case) is as follows:?
Divide the corpus into two halves by randomly placing texts in one of two subcorpora;?
Produce a word frequency list for each subcorpus;?
Calculate the X 2 statistic for the rtifference between the two subcorpora;?
Normalise;?
Iterate.
(to give different random halves);?
Interpret result by comparing values for different corpora.The only difPerences for the corpus-similarity case are that (1) one subcorpus is taken from thefirst corpus and the other from the second, and (2) the similarity value is to be interpreted byreference to the homogeneity measure for each corpus.After arguing the the case for using word frequency lists and describing related work, thepaper describes the various pitfalls the measure must avoid and presents ome first results.IIIIIIII2 Why word frequency lists?Are word frequency lists interesting?
Many think not.
There are two recurring themes amongstthe noes.
Firstly, what is hnportant about texts is their me~ing.
Since the message is thrown out mwhen a text is reduced to a frequency list, the heart of the text is jettisoned.
This arg-ment comes ifrom all quarters: the second comes mainly from linguists.
It is that, if we are to count, the objectswe should be counting are ones with a linguistic pedigree.
In relation to content, we should be ncounting word senses, or lexical units, since any list will be compromised if money bank and river |bank are counted together.
In relation to form, we should be counting rammatical constructions:nnmbers of relative clauses or passives tell us far more about the linguistic character of a textthan numbers of occur rences  of toho or which.
I' l~ iug  the general argument first, firstly, a text without its context is itself an abstraction.
wA transcript of a conversation is a more concise version than an audio tape (which is itself more mconcise than a video tape).
A newspaper article is understood more fully if the reader is well- ?versed in the political or other circumstances of its publication.
There is not a complete break mbetween texts, which present meaning, and frequency lists, which do not.
e lSecondly, our concern is with language corpora, not with texts.
While a text may be coherent Iin its me~uing, a corpus comprising multiple texts can scarcely be.
The objective in gatheringmultiple-text corpora is to identify a linguistic object in which the individual meanings of texts mare taken out of focus, to be replaced by the character of the whole.
IThirdly, as will be evident oo all workers in corpus-based computational linguistics, frequencylists are very useful representations of meaning for information retrieval, text categorisation, and IIgnumerous other purposes.
They are useful because they are a representation f the text which is Isusceptible to automatic, objective manipulation.
The full text is very rich in information, butI232 imIIIIIIIII)1IIIIIIthat information cannot be readily used to make, e.g., similarity judgements.
When a text orcorpus is represented as a frequency list, much information is lost, but the tradeoff is an objectthat is susceptible to statistical processing.To move on to the concerns regarding what is counted: in exploring word frequency lists weare also investigating a hypothesis.
Sinclair has postulatedEvery distinct sense of a word is associated with a distinction in form 2We take this one step further and postulate "no linguistic distinction without a word frequencydistinction'; any dii~erence in the linguistic character of two corpora will leave its trace in adi~erence between their word frequency lists.
It may not be evident which words will be morefrequent, and which less, if one corpus uses more relative clauses and less passives than another,on this hypothesis, some will be.An  advantage of using word frequency lists is that there is so much data: two corpora canbe compared in respect of thousands of data points (e.g., words).
Although money bar& andriver bank are counted together, corpora using the one and corpora using the other will tend tobe discriminated because the one corpus will use money, account and Barclays more, the other,river and grassy.
It is a research question to determine which words' frequencies vary for a givenvariation in linguistic structures (see the section on newspapers for an indication of how this canproceed).
For current purposes, we can happily pool the data, referring only to individual wordswhen we seek further insight into why we get the results we do.
Biber's work (see below) showshow quantitative methods can be used to discover and capture register differences, and some ofthe objects he counts are words (others being gr~mm~ical constructions), so his work providessome grounds for optimism.The hypothesis would of course be a very useful, if true.
Words are far easier to count accu-rately than syntactic ategories or word senses.
To count syntactic ategories requires linguistictheory to identify precisely what the syntactic category is; empirical research to identify thefeatures that indicate where it is present; and a computer program, to automatically identifyoccurrences.
The first two stages are likely to introduce theoretical disagreements, and the lasttwo, errors.
The prospects for two independent teams arriving at the same syntactic-constructionfrequency list for the same corpus are slim.
By contrast, if agreement is reached on a few tokenisa-tion issues (hyphens, clitics), the chances of two groups arriving at identical word frequency lists isvery good.
3 The rule that any string of alphanumerics surrounded by whitespace or punctuationis a word may have its shortcomings, but it makes word-counting very reliable.Word frequency lists axe cheap and easy to generate, so a measure of corpus similarity basedon them would be of use as a quick guide in many circumstances where a more extensive analysisof the two corpora was not viable; for example, to judge how a newly available corpus related toexisting resources, o a decision about buying it or installing it could be made, or as a prelhninaryassessment of how much customisation was likely to be necessary to port an NLP applicationfrom one domain (and corpus) to another.3 Related WorkThe only other piece of work the author has found which aims to measure similarity betweencorpora is (Joh~n~son and Hofland, 1989).
Their goal is to find which genres, within the LOB2Cited in (Moon, 1987, p 89)SAt least for languages haring certain typographical conventions, e.g., not Chinese or Japanese.233corpus, most resemble ach other.
They take the 89 most common words in the corpus, findtheir rank within each genre, and calculate a Spearman rank correlation statistic.
This methodis compared empirically with the X 2 method in some detail in section 6 below.There is a large body of work aiming to find words which are particularly characteristic ofone text, or corpus, in contrast to another.
4 This includes work on linguistic variation, authoridentification (Mosteller and Wallace, 1964) and information retrieval (Salton, 1989).
(Dunning,1993) and (Pedersen, 1996) shows how some of the methods which have been used in the past(particularly mutual information scores) are invalid for rare events, and introduce accurate mea-sures of how 'surprising' rare events are.
(Church and Gale, 1995a) show how Inverse DocumentFrequency, a measure based on the proportion of documents a word occurs in, can be used along-side word frequency to estimate how distinctive a word is of the texts it occurs in.
(Church andGale, 1995b) extend this work, showing how to model word ~equency distributions in a mannerconsistent with the fact that some words are evenly spread, while others tend to occur often indocuments where they occur at all.
As most of this work ~im~ to find good indexing terms forinformation retrieval, it is mostly concerned with middle-to-low frequency items, and differencesin topic rather than differences in register.There is a growing body of work which explores and quantifies the differences between corpora.Pre-eminent in this field is Biber (Biber, 1988; Biber, 1995), in whose studies the objective is toidentify the major dimensions oflinguistic variation across languages, and to identify the linguisticand functional characteristics which co-occur in the different registers of a language.
His methodinvolves counting a range of linguistic features in each text, and then using factor analysis todetermine which of the features co-occur.
Co-occurring features are then grouped together togive the dimensions of variation, and the texts (or corpora) of di~erent registers can be identifiedby their location with respect o these dimensions.A recent paper by (Sekine, 1997) explores the domain dependence of parsing.
He parsescorpora of various text genres, identifies the subtrees of depth 1 in each corpus, and counts thenumber of occurrences ofeach subtree.
This gives him a subtree frequency list for each corpus, andhe is then able to investigate which subtree are markedly different in frequency between corpora.Such work is highly salient for customizing parsers for particular domains.
In the current context,Sekine's ubtree frequency lists can readily be compared with word frequency lists to determinewhich lists are better for measuring corpus similarity and homogeneity.Within the literature on statistical language mode\]llng, there is much discussion of relatedquestions.
From an information-theoretic point of view, the theoretical answer to the problemis simple: entropy is a measure of a corpus's homogeneity, and the cross-entropy between twocorpora quantifies their similarity.
Entropy is not a quantity that can be directly measured.
Thestandard problem for statistical language modelling is to aim to find the model for which the'cross-entropy' of the model for the corpus is as low as possible.
For a perfect language model,the cross-entropy would be the entropy of the corpus (Church and Mercer, 1993; Charniak, 1993).The potential for using information-theoretic constructs to measure corpus similarity is a topic forcurrent research.
The Known Similarity Corpora evaluation methodology presented in Section 6will be applicable to the issue of assessing how well cross-entropy captures pre-theoretical notionsof corpus similarity and homogeneity.4For a fuller review, see (Kilgarriff, 1996)IIIIIIII-IIIIIIIIII234 III 4 Corpus homogeneityIIiIA corpus is a collection of texts.
The definition only serves to Show how heterogeneous a collectionof objects the word denotes.
One may contain hundreds of words, another, hundreds of millions.One may include a very small number of texts, with a one-text corpus as the limiting case; anothermay contain thousands of texts, sThese factozs present problems for a measure of corpus similarity.
It is not clear what, ifanything, a measure of the similarity of a thousand-word corpus and a million-word corpus, or aone-text corpus and a thousand-text corpus, would mean.
Also, most contain some texts that aremuch bigger than others.
Thus, in the BNC,  the shortest file (which approximates to a 'text')contains 25 words, and the longest, a hundred thousand times that many.
Two corpora of thesame size and the same number of texts may still have a very different shape, if, in one, one ofthe texts accounts for most of the corpus, whereas in the other, they are all of similar size.Like a corpus, a text can be large or small, heterogeneous or Imlform.
A corpus can containcomplete texts or sampled texts, as in the Brown corpus.How homogeneous is a corpus?
The first point to make is that there is no obvious way toapproach the question.
It is clear that the British National Corpus is less homogeneous than acorpus of software manuals, but it is not clear how to measure the difference.
The second is thatit is very similar to the question, "how similar are two corpora?"
Our approach to measuringhomogeneity is to randomly divide a corpus into two random halves and measure the similarity ofthe two halves, thus emphasising the relation between the two questions.
The third point is thatit is a pre-requisite to a measure of corpus similarity.
A judgement of similarity rnns the risk ofmeaninglessness if a homogeneous corpus is compared with a heterogeneous one.Our method provides figures which can be directly compared for corpus homo(/hetero)geneityand for corpus (dis)similarity.
(High scores correspond to heterogeneous corpora and dissimilarcorpora) The possible outcomes, for various permutations of the scores for homogeneity of corpus1 (corpl), homogeneity ofcorpus 2 (corp2), and corpus dissimilarity (dis), are presented in Table 1.corpl corp2equal equalequal equal much higherhigh low highdis Comment'equal same language variety/ieshigh low higherhigh high lowhigh high a bit higherlow low a bit higher, -different language varietiescorp2 is homogeneous and falls withinthe range of 'general' corplcorp2 is homogeneous and falls outsidethe range of 'general' corplimpossibleoverlapping; share some varietiessimilar varietiesTable 1: Interactions between homogeneity and similarity: a similarity measure can only beinterpreted with respect o homogeneity.The last two lines in the table point to the differences between general corpora and specificcorpora.
High scores for heterogeneity will be for general corpora, which embrace a numberof language varieties.
Corpus similarity between general corpora will be a matter of whether5A corpus may contain texts in di~erent languages: here, we only consider corpora which are essentially all inthe same language.235all the same language varieties are represented in each corpus, and in what proportions.
Lowheterogeneity scores will typically relate to corpora of a single language variety, so here, similarityscores may be interpreted as a measure of the distance between the two varieties.From the point of view of measuring corpus homogeneity or similarity, it is desirable to use amethod which m~r6mi.qes the significance of the division of a corpus into texts.
'Text' and 'doc-ument' are problematic constructs: any corpus-building project has to make a range of practicaldecisions about what is to be considered a text, determining, for example, whether all the poemsin a book of poetry count as one text, and how newspapers are going to be divided.S The onepoint at which our method uses the division into texts is in identifying the ch, mk.q of the corpusto be randomly placed in a subcorpus.
Any subdivisions of the corpus which tended to keepcontiguous material together and which gave an appropriate n,,mber of chunks (say, between 20and 200), all of approxlmately the same size, would be satisfactory.
One possibility is to treat acorpus as a single text, with chunks specified as "first 5,000 words", "next 5,000 words", etc., thestrategy adopted in the experiments described below.5At a Krst pass, it would appear that the chi-square test will serve to indicate whether two corporaare drawn from the same population, or whether two or more phenomena are signi~cantly fli~erentin their distributions between two corpora.
For a contingency table of dimensions m x n, if thenull hypothesis true, the statistic- E) 2E(where 0 is the observed value, E is the expected value calculated on the basis of the joint corpus,and the sum is over the cells of the contingency table) will be M-distributed with (rn - 1) x (n - 1)degrees of freedom.
T(HoR~nd and Joh~.n~son, 1982) use the test to identify where words are signi6cantly morefrequent in the LOB corpus (of British English) than in the Brown corpus (of American English).In the table where they make the comparison, the X2-value for each word is given, with the valuemarked 1, 2 or 3 if it exceeds the critical value of the statistic at any of three different significantlevels, so one might infer that the LOB-Brown diITerence was non-random.Looking at the LOB-Brown comparison, we find that very many words, including most verycommon words, are marked.
Much  of the time, the null hypothesis is defeated.
Does this show thatall those words have systematically different patterns of usage in British and American English?To test this, we took two corpora which were indisputably of the same language type: eachwas a random subset of the BNC.
The sampling was as follows: all texts shorter than 20,000 wordswere excluded and all others were truncated at 20,000 words.
The truncated texts were randomlyassigned to either corpus 1 or corpus 2, and frequency lists for each corpus were generated.As in the LOB-Brown comparison, for very many words, including most common words, thenull hypothesis was defeated.
This reveals a bald, obvious fact about language.
Words are notselected at random.
There is no a pr/ori reason to expect them to behave as if they had been,SThe appropriate theoretical response, as taken in the Text Encoding Initiative, is that texts are hierarchicallystructured, so 'same text' does not have a unique interpretation.VProvided all expected values are over a threshold of 5.
Where there is just one degree of freedom, Yates'correction is applied.236IIIIII!
!IIIiIIiIIItand indeed they do not.
The LOB-Brown differences cp-nuot in general be interpreted as British-American differences: it is in the nature of language that any two collections of texts covering arange of registers (and comprising,-say, less than a thousand samples of over a thousand wordseach) will show such differences.
While it might seem plausible that oddities would in some waybalance out to give a population that was indistinguishable from one where the individual words(as opposed to the individual texts) had been randomly selected, this turns out not to be thecase .Let us look closer at why this occurs.
A key word in the last paragraph is 'indistinguishable'.The null hypothesis we are testing is that both frequency lists were the outcome of randomselections from the s~me source.
Since words in a text are not random, we know that our corporaare not randomly generated.
The only question, then, is whether there is enough evidence tosay that they are not, with confidence.
In general, where a word is more common, there is moreevidence.
This is why a higher proportion of common words than of rare ones defeat the nullhypothesis.
As one statistics textbook puts it:None of the null hypotheses we have considered with respect o goodness of fit can beezactl~/true, so if we increase the sample size (and hence the value of X 2) we wouldultimately reach the point when all null hypotheses would be rejected.
All that the X 2test can tell us, then, is that the sample size is too small to reject the null hypothesis!
(Owen and Jones, 1977, p 359)For large corpora and common words, the sample size is no longer too small.
On the null hy-pothesis, the expected value for the (O-E)2/E term would be 0.5 s and would not vary with wordfrequency.
Table 2 shows that this term tends to be substantially higher than 0.5 and increaseswith word frequency.FClass First word Mean error term II\[ (Words in freq.
order) in class for items in classFirst 20 itemsNext 20 itemsNext 40 itemsNext 80 itemsNext 160 itemsNext 320 itemsNext 640 itemsNext 1280 itemsNext 2560 itemsNext 5120 itemsNext 10240 itemsNext 20480 itemsthearebeenfirstlittlelevelfrontfastpreciselyextractdiscontentfour-year-old55.147.725.629.817.312.812.513.713.912.58.04.7Table 2: Variation of (O-E)2/E term with word frequency for same-variety corpora.
The tablewas generated from a list, ordered by frequency, giving the term's value for each word.
The firstline of the table then states that the average of these values, for the first 20 items on the list (thefirst of which was the) was 55.1.SO.5 rather than 1 because there are two cells in the contingency table for each degree of freedom.2375.1 X 2 w i thout  the  nu l l  hypothes isWe cannot use the X 2 statistic for testing the null hypothesis, but nonetheless it does come closeto meeting our requirements.
The (O-E)2/E term gives a measure of the difference in a word'sfrequency between two corpora, and, while the measure tends to increase with word frequency,it does not increase by orders of magnitude.
The strategy we adopt is therefore to calculate X 2for (sub)corpus pairs, and then to use this as the measure of corpus similarity and homogeneity.The score is then norma\]ised by the nnrober of words used for the comparison (equivalent tothe numbers of degrees of freedom) to give a measure we shall call CBDF (Chi By Degrees ofFreedom).The question arises, wiMch words, and how many, should be used in the comparison.
Since theerror-term tends to increase with frequency, CBDF scores for will only be comparable if words ofthe same span of frequencies are used in the comparisons.
We simply used the N most frequentwords in the union of the two corpora to be compared.
The experiments below explore differentvalues for N.5.2 Normal i sa t ionAt a first pass, a measure of corpus homogeneity or similarity should be able to compare corporaof different sizes.
As we have seen, for all but purely random populations, (O-E)2/E tends toincrease with frequency.
Where corpora are larger, words will tend to be more frequent, so, forthe same level of corpus similarity or homogeneity and the same number of degrees of freedom,X ~ will be larger.
There is also a theoretical problem: it is not clear what it means to say thatcorpora of different sizes are equally homogeneous.
If corpus 1 is twice as large as corpus 2, do wecall them 'equally homogeneous' if corpus i contains twice as many language varieties as corpus 2,or the same number of language varieties but twice as much of each?
Is a corpus as homogeneousas a subcorpus we produce from it which contains a randomly selected half of its texts, or is it ashomogeneous as one that contains half of each of its texts?
It is not obvious, and I am currentlyinvestigating the question further.
The experiemtus described below all use same-size corpora.6 Evaluat ionTo invent a measure is easy.
To determine that it is a good measure is more di61cult.
In thissection, I first present some results suggesting the face validity of the measure.
Then I presenta method for evaluating the measure, and describe some experiments in which a X 2 test and aSpearman Rank Correlation test are compared.As a rnlnlrn~\] requirement, a measure isgood flit confirms our subjective judgements regardingcorpus similarity.
If it is evident o experts that corpus A is more like corpus B than corpus C,then the measure is invalidated if it does not confirm that A and B are more similar than A andC.
So that this style of comparison can be made, I compared 200,000-word corpora from eachof the language sources hown in Table 3.
(All were extracted from the BNC.)
The results arepresented in Table 4.To move beyond such purely qualitative valuation, I use sets of 'Known-Similarity Corpora'(KSC).
Each corpus within a set comprises text of two types, in varying fractions, as illustratedin Table 5.We can now say that Corpusl and Corpus2 are more similar than Corpusl and Corpus3;Corpus3 and Corpus4 are more similar than Corpus2 and Corpus5; and a number of other such238ShortGUAINDDMINMEFACACCDNBHANBMJGRATitle DescriptionThe GuardianThe IndependentDaily MirrorNew Musical ExpressThe FaceAccountancyDictionary of National BiographyH~+~ardBritish Medical JournalComputergramBroadsheet national newspaperBroadsheet national newspaperTabloid national newspaperWeekly pop/rock music magazineWeekly fashion magazineAccountancy periodicalComprises hort biographiesProceedings of ParliamentAcademic papers on medicineElectronic omputer-trade n wsletterTable 3: Corpora for first experiment.ACC ART BMJ  DMI DNB ENV FAC GRA GUA HAN IND NMEiIACCARTBMJDMIDNBENVFACGRAGUAHANINDNME4.6221.40 3.3820.16 23.50 8.0821.56 26.19 32.08 2.4740.56 30.07 40.14 35.1522.68 23.10 28.12 34.6520.49 25.14 31.14 7.7627.75 29.96 33.50 31.4014.06 18.37 22.68 11.4124.13 33.76 33.00 32.1412.76 17.83 22.96 13.9621.18 25.99 30.05 9.771.8641.50 2.6036.92 36.93 3.4345.26 28.96 34.3531.06 23.24 12.0452.25 32.03 31.2330.10 21.69 14.4539.41 34.77 5.842.2032.25 3.9236.21 22.6228.06 4.1131.39 15.093.6523.27 4.4433.25 16.56 3.10Table 4: CBDF homogeneity and similarity scores for twelve 200,000-word corpora.CorpuslCorpus2Corpus3Corpus4Corpus5Corpus6100% GUA80% GUA60% GUA40% GUA20% GUA0% GUA0% BMJ20% BMJ40% BMJ60% BMJ80% BMJ100% BMJTable 5: Example of Known-Similarity Corpora set.239judgements.
In fact, for this set, 55 such judgements can be made.
The number of such judge-ments, for a KSC set of n corpora, isn ( i ( i+1) )  ~(n --i) 1i----1 2A proposed metric ca~a now be scored.
An ideal metric would make the 'correct' judgementin all 55 cases.
Two metrics can be compared by seeing which makes the correct judgement moreof the time.There are some difBculties with the method.
Firstly, as mentioned above, there are differentways in which corpora can be different.
They can be different because they each represents onelanguage variety, and these varieties are different, or they can be different because they containdifferent mixes of the same varieties.
Clearly, this method only addresses the latter, and ideallythis approach to evaluation is paired with one where the focus is on subjective judgements of howsimilar distinct language varieties are.
9Secondly, if the corpora are small and the difference in proportions between the corpora isalso small, it is not clear that all the 'gold standard' assertions are in fact true.
There may bea medical supplement in one of the copies of the Guardian in the corpus, and one of the copiesof the BMJ may focus on social issues in medicine: perhaps, then, Corpus3 is more like Corpus5than Corpus4.To address this, the two language varieties in each KSC set were selected to be quite distinctfrom each other.
The procedure was as follows.
For each of the 33 texts sources represented byover 200,000 words in the BNC, the first 200,000 words were taken, homogeneity was calculated,and similarity between all pairs was calculated.
(The results in Table 4 show some of the results.
)Using these similarity scores, the corpora were clustered.
This showed that there was in fact onlyone major cluster, with the broadsheet newspapers at its centre.
Broadsheet newspapers wereused as one of the language varieties for each of the KSC sets; the other varieties were chosen onthe basis of (1) quantity of text (over 380,000 words were needed to construct the KSC sets), (2)ideally, it was neither too homogeneous (as that would tend to make the task too easy) nor tooheterogeneous (as then it would not be clear whether the gold standard was true), and (3) it wasnot too similar to 'broadsheet newspaper' (again, it would not be clear whether the gold standardwas true).
This last constraint could be interpreted as meaning that I did not make the task toodifficult: this was an unavoidable by-product of ensuring the validity of the gold standard.
It isnot pernicious ince the technique is only to be used to compare one corpus-s~m~larity s atisticwith another.The first three KSC sets contained, in addition to broadsheet-newspaper (for which I used acombination of Guardian and Independent material, hereafter 'ballast'), the sources in Table 6.The sets were constructed as in ratios of 5:0, 4:1, 3:2, 2:3, 1:4 and 0:5, with each corpus comprising200,000 words.I then computed corpus similarity.
I used both X 2 and Spearman l~nk  Correlation (hereafterspearman).
As, for evaluation purposes, I was concerned only with similarity measures and goldstandard statements did not relate to homogeneity scores, I used whole-corpus frequencies ratherthan repeatedly taking different random halves of each corpus and averaging results.This test turned out to be easy.
Provided more than ten words were used as data points, bothstatistics gave 100% correct answers.9UCltEL, the Unit for Computer R~earch into Language at the Lancaster University currently has a grantproposal for gathering data about human judgements of slm~larity between text genres.I!I1!!1i|,!i!!
!iI!I240ii!i!|I|lSource Horn Sire"Computergr_~m International nd 'Unigrum', two computer-industry newsletters 2.1 28.8Dictionary of National Biography 1.83 32.3'The Art Newspaper' 3.47 19.1Table 6: Sources for first evaluation experiment.
Horn is the homogeneity of each corpus.
Sireis its similarity to the Guardian.
The Guardian's homogeneity was 4.05, and its s~ml\]u~-ity to theIndependent was 4.31.
The Independent had homogeneity 4.11.Source Horn SireThe British Medical Journal 7.8 22.3Environment Digest 2.7 22.7Accountancy 4.3 13.4Table 7: Sources for second evaluation experiment.
Hom and Sim as above.To determine whether the one statistic performed better than the other, a more stringenttest was required.
Two of the sources already used, the DNB and Compugram/Unigr~m, hadbeen particularly dissimilar to the ballast.
For the next round, sources more siml\]ur to ballastwere selected (though the overriding censtr_~int was, again, quantity of text available).
The threefurther sources were all periodicals (see Table 7).Also the corpus size was halved to 100,000, and the ratios varied by tenths rather than fifths(2:8, 3:7, 4:6, 5:5, 4:6, 3:7 and 2:8; there was only enough texts for sets such as these.
)Each of these three KSC sets provided 105 gold standard statements.
The results are presentedin Table 8.The table shows that, for this task,?
both measures give correct ~n~wers most of the time,?
X 2 ~\]most always outperforms spearman,?
both methods tend to perform better if more data are provided up to a level of around 640data points, eg., comparing frequencies for the 640 most frequent words.For any word which is most frequent in the corpus with most ballast, next most frequent inthe corpus with next most ballast, and so on, if X p sirni\]~L"ity measures are computed using justthat word as evidence, the score would give 100% correct answers.
The is close to being such aword.
Just using the word the gives 99% correct un~wers for the BMJ KSC set, but little betterthan r hu~ce for ENV and ACC.6.1 NewspapersTo explore differences between ewspapers, I conducted a simple r experiment.
For each of thenational newspapers for which there was over half a million words of text in the BNC, I computedCBDF for each pair, using all words with expected frequency >-- 5 in both corpora s data points.The newspapers were the Guardian, Independent, Daily Telegraph, Daily Mirror and Today.
Allhomogeneity scores were below 4.2.
The CBDF value, for each pair, is shown in Table 9.
Thetable demonstrates that the up-market broadsheets, the Guardian, Independent and Telegraph,form one class, and the down-market tabloids, the Mirror and Today, another.241KSC set TopN % correctSpearman I X~.i0 55.2 89.520 69.5 89.540 76.1 89.580 92.3 95.2160 94.2 97.1320 90.4 97.1640 94.2 97.11280 92.3 96.12560 90.4 96.15120 92.3 96.110 36.1 77.120 71.4 89.540 85.7 88.580 83.8 86.6160 80 92.3320 92.3 95.2640 91.4 95.21280 86.6 96.12560 87.6 96.15120 88.5 96.110 42.8 71.420 70.4 87.640 83.8 81.980 81.9 82.8160 80 83.8320 76.1 87.6640 76.1 88.51280 78.0 85.72560 80 86.65120 82.8 187.6BritishMedicalJournalEnvironmentDigestAccountancyTable 8: Accuracy of Spearman and X 2 statistics for the three KSC sets, with various values for'TopN', the n11mber of words on which the comparison was based.
The N most frequent words inthe joint corpus were always used for the comparison, with N varying, hence ~J~opN'.2421 /tI1ItIIiI1!Mirror vs.
Independent lZl.5Mirror vs. GuardianIndependent vs. TodayMirror vs. TelegraphGuardian vs. TodayTelegraph vs. TodayTelegraph vs. GuardianTelegraph vs. IndependentMirror vs. TodayGuardian vs. Independent13.212.312.012.09.96.65.95.24.3Table 9: Newspaper corpora compared.The words that made the highest contributions to CBDF are also of interest.
For the Guardian-Mirror and Guardian-lndependent comparisons they are shown in Table 5.This Mirror-Guardian list immediately shows that the Guardian is far more "literate" than theMirror, according to Biber's (1995, Chapter 7) criteria for literacy.
For the Mirror-Guardian com-parison, we have observed differences in language variety, whereas for the Guardian-Independentcomparison, there being no systematic differences in style (beyond copy-editing policy on titles-- "m.r", "gen" ~ and number-words -- "million ", "billion"), the lists simply indicate that theGuardian material was taken from the Christmas of the Romanian revolution, whereas the Inde-pendent was from the period of the Conservative Party Conference in B\]Lackpool.7 Conclusion, current and future workA measure of corpus similarity has been presented.
It uses frequency information for the twocorpora, and the X 2 statistic.
The measure can also be used to quantify the homogeneity of acorpus.
The relation between corpus homogeneity and corpus shnilarity was considered in somedetail: a corpus similarity score must be interpreted relative to the homogeneity scores of thetwo corpora.
Homogeneity and similarity scores were calculated for various corpora where anindependent judgement of their similarity could be made, and there was a good fit between theindependent judgement and the (interpreted) similarity scores.
The experiments were performedusing word frequencies.
The same technique could be used with freque~Icies of subtrees or wordclasses or combinations of these.The measure is potentially of interest in lexicography and language engineering.
Word fre-quency lists are cheap and easy to generate so a measure based on them would be of use as aquick guide in many circ,lm~tances where a more extensive analysis of the two corpora was notviable, to judge, for example, how a newly available corpus related to existing resources.The paper also presents a method for evaluating measures of shzdlarity between corpora,based on sets of known-similarity corpora.
This evaluation strategy can be used to compare theresults of quite different techniques, so the x2-based statistic can be ,compared with measuresfrom statistical language modelling.Future work includes the issue of normalising the measure for size of corpus, so it can be usedto compare different-sized corpora.
Also, the relationship to perplexity and cross-entropy willbe explored, as will the relationship between grammatical and lexical similarity, as a prelude tointegrating the work with Biber's methods for quantifying the linguistic characteristics of corpora.243Mirror GuardianWORD POS ~ WORD POS?
PRON DET"  lshe PRONn't NOThe PRON's Vher DEThis DETwe PRON're vhbyou PRONmirror PNher PRONboss Nmy dpshim PRONstar Nafter PREPme PRONwas Vyour DET'm Vsaid Vmajor PNaway PARTmnm Nnight Na DETtv Njust ADVfergie PNGuardian IndependentWORD POS WORD POSthe mr PNof PREP  mil l ion NUMBERmr PN christmas PNits DET  bill Nwhich REL page N-PNgovernment N romania PNby PREP  gen PNthat CONJ  billion NUMus PN clowes PNeuropean ADJ-N ambulance Npolitical ADJ  romanian ADJpart F N guardian PNhowever ADV eastern PNeast PN summit Nkong PN europe PNhong PN dixons PNmarket N ceausescu PNper cent N aged PREPgroup N panama PNnational ADJ  december PNin PREP  barlow PNpresident PN hurd PNsoviet ADJ  havel PNeu~pe PN beijing PNcup N-PN bucharest PNmilitary ADJ  commons Nminister N-PN january PNnot NOTthatcher PNberlin PNsport Nconference Nin-short ADVrates Npeking PNlawson PNsir N-PNinterest Nblackpool PNfootball N-PNoctober PNknighton PNis V/ PREPmandela PNconservative ADJ-Nbase ADJ-NTable 10: Mirror-Guardian and Guardian-Independent comparisons: high-contrast words.
The30 most different words are listed, provided they are above a threshold for the X 2 value.
The listsfor the Guardian-Independent comparison are shorter because there were not 30 items scoringabove the threshold on either side of that comparison.
All words are normalised to lower case.Parts of speech are derived from BNC tags.
PN -- proper noun.
Hyphenated categories are thosefor which CLAWS, the tagger used for tagging the BNC, was uncertain which of the two tags toassign.II!iIi|I!ID.iI|IItl244IIIIIIIt|1|,,IIi1Il ferencesBiber, Douglas.
1988.
Variation across speech and writing.
Cambridge University Press.Biber, Douglas.
1993.
Using register-diversified corpora for general language studies.
Computa-tional Linguistics, 19(2):219-242.Biber, Douglas.
1995.
Dimensions in Register Variation.
Cambridge University Press.Charniak, Eugene.
1993.
Statistical Language Learning.
MIT Press, Cambridge, Mass.Church, Kenneth and William Gale.
1995a.
Inverse document frequency (IDF): a measure ofdeviations from Poisson.
In David Yarowsky and Kenneth Church, editors, Third Workshopon very large corpora, pages 121-130, MIT.Chui'ch, Kenneth and William Gale.
1995b.
Poisson mixtures.
Journal of Natural LanguageEngineering, 1(2):163-190.Church, Kenneth W. and Robert L. Mercer.
1993.
Introduction to the special issue on computa-tional inguistics using large corpora.
Computational Linguistics, 19(1):1-24.Dnnniug, Ted.
1993.
Accurate methods for the statistics of surprise and coincidence.
Computa-tional Linguistics, 19(1):61-74.Holland, K. and S. Johansson, editors.
1982.
Word Frequencies in British and American English.The Norwegian Computing Centre for the Humanities, Bergen, Norway.Johan~son, Stig and Knut Hofland, editors.
1989.
Frequency Analysis of English vocabulary andgrammar, based on the LOB corpus.
Clarendon, Oxford.Kilgarriff, Adam.
1996.
Which words are particularly characteristic of a text?
a survey ofstatistical pproaches.
In Language Engineering for Document Analysis and Recognition, pages33-40, Brighton, England, April.
AISB Workshop Series.Moon, Rosamtmd.
1987.
The analysis of meaning.
In John M. Sinclair, editor, Looking Up: AnAccount of the GOB UILD Project in Lexical Computing.
Collins, London, chapter 4.Mosteller, Frederick and David L. Wallace.
1964.
Applied Bayesian and Classical Inference - TheCase of The Federalist Papers.
Springer Series in Satisties, Springer-Verlag.Owen, Frank and Ronald Jones.
1977.
Statistics.
Polytech Publishers.Pedemen, Ted.
1996.
Fishing for exactness.
In Proc.
Conf.
South-Central SAS Users Group,Texas.Salton, Gerard.
1989.
Automatic Yezt Processing.
Addison-Wesley.Sekine, Satshi.
1997.
The domain dependence ofparsing.
In Proc.
Fifth Conference on AppliedNatural Language Processing, pages 96-102, Washington DC, April.
ACL.245
