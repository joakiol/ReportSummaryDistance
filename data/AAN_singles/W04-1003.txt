The Effects of Human Variation in DUC Summarization EvaluationDonna Harman and Paul OverRetrieval Group, Information Access DivisionNational Institute of Standards and TechnologyGaithersburg, MD 20899, USAdonna.harman@nist.gov,paul.over@nist.govAbstractThere is a long history of research in automatic textsummarization systems by both the text retrievaland the natural language processing communities,but evaluation of such systems?
output has alwayspresented problems.
One critical problem remainshow to handle the unavoidable variability in hu-man judgments at the core of all the evaluations.Sponsored by the DARPA TIDES project, NISTlaunched a new text summarization evaluation ef-fort, called DUC, in 2001 with follow-on workshopsin 2002 and 2003.
Human judgments provided thefoundation for all three evaluations and this paperexamines how the variation in those judgments doesand does not affect the results and their interpreta-tion.1 IntroductionResearch in summarization was one of the first ef-forts to use computers to ?understand?
language.Work was done back in the 1950s by many groups,including commercial services, to automaticallyproduce abstracts or lists of pertinent keywords fordocuments.
The interest in automatic summariza-tion of text has continued, and currently is enjoy-ing increased emphasis as demonstrated by the nu-merous summarization workshops held during thelast five years.
The DUC summarization evalu-ations (2001 ?
2004)(http://duc.nist.gov)sponsored by the DARPA TIDES project (Translin-gual Information Detection, Extraction, and Sum-marization) are prominent examples.
DUC has beenguided by a roadmap developed by members of thesummarization research community.Along with the research has come efforts to eval-uate automatic summarization performance.
Twomajor types of evaluation have been used: extrinsicevaluation, where one measures indirectly how wellthe summary performs by measuring performancein a task putatively dependent on the quality of thesummary, and intrinsic evaluation, where one mea-sures the quality of the created summary directly.Extrinsic evaluation requires the selection of atask that could use summarization and measurementof the effect of using automatic summaries insteadof the original text.
Critical issues here are the se-lection of a real task and the metrics that will be sen-sitive to differences in the quality of the summaries.This paper concerns itself with intrinsic evalua-tions.
Intrinsic evaluation requires some standard ormodel against which to judge summarization qual-ity and usually this standard is operationalized byfinding an existing abstract/text data set or by hav-ing humans create model summaries (Jing et al,1998).Intrinsic evaluations have taken two main forms:manual, in which one or more people evaluate thesystem-produced summary and automatic, in whichthe summary is evaluated without the human in theloop.
But both types involve human judgments ofsome sort and with them their inherent variability.Humans vary in what material they choose to in-clude in a summary and in how they express the con-tent.
Humans judgments of summary quality varyfrom one person to another and across time for oneperson.In DUC 2001 - 2003 human judgments haveformed the foundation of the evaluations and infor-mation has been collected each year on one or moresorts of variation in those judgments.
The followingsections examine this information and how the vari-ation in human input affected or did not affect theresults of those evaluations.2 Initial Design ?
DUC-2001Since the roadmap specified testing in DUC-2001 ofboth single and multi-document summarization, thedata sets and tasks were designed as follows.Sixty sets of approximately 10 documents eachwere provided as system input for this task.
Givensuch a set of documents, the systems were to au-tomatically create a 100-word generic summary foreach document.
Additionally they were to create ageneric summary of the entire set, one summary ateach of four target lengths (approximately 400, 200,Figure 1: SEE interface for judging per unit cover-age100, and 50 words).The sets of documents were assembled at NISTby 10 retired information analysts.
Each person se-lected six document sets, and then created a 100-word manual abstract for each document, and forthe entire document set at the 400, 200, 100 and50 word lengths.
Thirty of the sets (documents andmanual abstracts) were distributed as training dataand the remaining thirty sets of documents (withoutabstracts) were distributed as test data.Fifteen groups participated in DUC-2001, with11 of them doing single document summarizationand 12 of them doing the multi-document task.The evaluation plan as specified in the roadmapwas for NIST to concentrate on manual comparisonof the system results with the manually-constructedabstracts.
To this end a new tool was developedby Chin-Yew Lin at the Information Sciences In-stitute, University of Southern California (http://www.isi.edu/?cyl/SEE/).
This tool al-lows a summary to be rated in isolation as well ascompared to another summary for content overlap.Figure 1 shows one example of this interface.
Hu-man evaluation was done at NIST using the samepersonnel who created the manual abstracts (calledmodel summaries).One type of evaluation supported by SEE wascoverage, i.e., how well did the peer summaries(i.e., those being evaluated) cover the content of thedocuments (as expressed by the model summary).A pairwise summary comparison was used in thispart of the evaluation and judges were asked to dodetailed coverage comparisons.
SEE allowed thejudges to step through predefined units of the modelsummary (elementary discourse units/EDUs) (Sori-cut and Marcu, 2003) and for each unit of that sum-mary, mark the sentences in the peer summary thatexpressed [all(4), most(3), some(2), hardly any(1)or none(0)] of the content in the current model sum-mary unit.
The resulting ordered category scale[0-4] is treated as an interval scale in the coverage scorebased on feedback from the judges on how it wasused.
The coverage score for a given peer summaryis the mean of its scores against the EDUs of theassociated model (?
4 EDUs per summary for the50-word model summaries).
This process is muchmore complex than doing a simple overall compari-son using the entire summary but past evaluation ex-periences indicated that judges had more difficultymaking an overall decision than they did making de-cisions at each EDU.2.1 DUC-2001 Results - Effect of Variability inModelsRecall that there are two very different sources ofhuman variation in DUC-2001, as in all the DUCevaluations.
The first is the disagreement amongjudges as to how well a system summary covers themodel summary.
This is similar to what is seen inrelevance assessment for IR evaluations.
To the ex-tent that different judges are consistently more le-nient or strict, this problem has been handled inDUC by having the same judge look at all sum-maries for a given document set so that all peer sum-maries are affected equally and by having enoughdocument sets to allow averaging over judges tomitigate the effect of very strict or very lenientjudges.
If a judge?s leniency varies inconsistently ina way dependent on which system is being judged(i.e., if there is an interaction between the judge andthe system), then other strategies are needed.
(Datawas collected and analyzed in DUC-2002 to assessthe size of these interactions.
)Summarization has a second source of disagree-ment and that is the model summaries themselves.People write models that vary not only in writingstyle, but also in focus, i.e., what is important tosummarize in a document or document set.To shed light on variability in creation of modelsand their use, each of the 30 document sets in thetest set (plus the 300 individual documents) weresummarized independently by three summarizers -the one who had selected the documents plus twoothers.
These extra summaries were used as addi-tional peer human summaries in the main evaluationand also in a special study of the model effects onevaluation.This special study worked with a random subsetof 20 document sets (out of 30).
Each peer wasjudged twice more by a single person who had notdone the original judgment.
This person used thetwo extra models, neither of which had been createdby the person doing the judgments.
There was onlytime to do this for the multi-document summaries atlengths 50 and 200.2.2 Model DifferencesA first question is how much did the two mod-els differ.
One way of measuring this is by asimple n-gram overlap of the terms.
This wasdone based on software in the MEAD toolkit(http://www.summarization.com), with-out omitting the commonwords, nor doing anystemming, and the n-grams were allowed to spansentence boundaries.
The average unigram overlap(the number of unique unigrams in the intersec-tion/the number of unique unigrams in the union)for the two extra 50-word model summaries was0.151 and there were only 6 out of the 20 sets thathad any tri-gram overlap at all.
For the 200-wordsummaries, the average unigram overlap was 0.197,with 16 out of the 20 sets having tri-gram overlaps.These numbers seem surprisingly low, but anexamination of the summaries illustrates some ofthe reasons.
What follows are the two model pairswith the greatest and least unigram overlap in thetwo extra 50-word document set group.Document set 32, Judge G ?In March 1989,an Exxon oil tanker crashed on a reef near Valdez,Alaska, spilling 8.4 million gallons of oil intoPrince William Sound seriously damaging theenvironment.
The cleanup was slow and Exxon wassubject to severe compensation costs and indictmentby a federal jury on five criminal charges.
?Document set 32, Judge I ?On March 24, 1989, theExxon Valdez spilled 11.3 million gallons of crudeoil in Prince William Sound, Alaska.
Cleanup of thedisaster continued until September and cost almost$2 billion, but 117 miles of beach remained oily.Exxon announced an earnings- drop in January1990 and was ordered to resume cleaning on May1.
?Document set 14, Judge B ?U.S.
military air-craft crashes occur throughout the world moreoften than one might suspect.
They are normallyreported in the press; however, only those involvingmajor damage or loss of life attract extensive mediacoverage.
Investigations are always conducted.D04D06D08D11D13D14D19D22D27D28D32D34D37D39D43D45D53D54D56D570.100.150.200.25Docset1?gramoverlapFigure 2: DUC-2001 unigram overlap by documentset for the two extra 50-word modelsFlight safety records and statistics are kept for allaircraft models.
?Document set 14, Judge H ?1988 crashes in-cluded four F-16s, two F-14s, three A-10s, twoB-52s, two B-1Bs, and one tanker.
In 1989 one T-2trainer crashed.
1990 crashes included one F-16,one F-111, one F-4, one C-5A, and 17 helicopters.Other plane crashes occurred in 1975 (C-5B), 1984(B-52), 1987 (F-16), and 1994 (F-15).
?For document set 32, the two model creatorsare covering basically the same content, but areincluding slightly different details (and thereforewords).
But for document set 14, the two modelsare written at very different levels of granularity,with one person writing a very high-level analysiswhereas the other one gives only details.
Note thatthese are only examples of the variation seen acrossthe models; many other types of variations exist.Additionally there is a wide variation in overlapacross the 20 document sets (see Figure 2).
Thisdocument set variation is confounded with the hu-man variation in creating the models since therewere 6 different humans involved for the 20 doc-ument sets.2.3 Effects of Model Differences on DocumentSet Coverage ScoresFigure 3 shows the absolute value of the coveragescore differences between the two extra models foreach of the 20 document sets for the 50-word sum-maries.
The middle bar shows the median, the blackD04D06D08D11D13D14D19D22D27D28D32D34D37D39D43D45D53D54D56D570.00.51.01.52.02.53.0DocsetAbsolutedifferenceincoveragescoresFigure 3: DUC-2001 absolute coverage differencesby document set for the two extra 50-word modelsdot the average, and the box comprises the middle 2quartiles.
The open circles are outliers.There is a large variation across document sets,with some sets having much wider ranges in cov-erage score differences based on the two differ-ent models.
Looking across all 20 document sets,the average absolute coverage difference is 0.437or 47.8% of the highest scoring model for the 50-word summaries and 0.318 (42.5%) for the 200-word summaries.
This large difference in scores iscoming solely from the model difference since judg-ment is being made by the same person (althoughsome self-inconsistency is involved (Lin and Hovy,2002)).2.4 Relationship between Model Differencesand Coverage ScoresDoes a small unigram overlap in terms for the mod-els in a given document set predict a wide differ-ence in coverage scores for peers judged against themodels in that document set?
Comparing Figures 2and 3, or indeed graphing overlap against coverage(Figure 4) shows that there is little correlation be-tween these two.
One suspects that the humans areable to compensate for different word choice andthat the coverage differences shown in Figure 3 rep-resent differences in content in the models.2.5 Effects of Model Differences on per SystemCoverage ScoresHow does the choice of model for each documentset affect the absolute and relative coverage scorefor each system averaged across all document sets?Figure 5 shows the median coverage scores (50-0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.01.2Docset mean(|difference in coverage score| / max score)Model overlap*4Figure 4: DUC-2001 absolute coverage differencesvs overlap for two extra 50-word models0.00.20.40.60.81.01.2PeerCoverageL M N O P R S T U W Y ZFigure 5: DUC-2001 median coverage by systemsusing extra model sets (50-word summaries)word summaries) for the 12 systems using each ofthe two extra model sets.
The points for the cov-erage scores are connected within a given model tomake changes in rank with neighbors more obvious.It can be seen that the scores are close to each otherin absolute value and that the two lines track eachother in general.
(The same type of graph could beshown for the 200-word summaries, but here therewere even smaller differences between system rank-ings.
)What is being suggested (but not proven) by Fig-ure 5 is that the large differences seen in the modeloverlap are not reflected in the absolute or rela-tive system results for the DUC-2001 data exam-ined.
Most of the systems judged better against oneset of models are still better using different models.The correlation (Pearson?s) between median cover-age scores for the systems using the two extra modelsets is 0.641 (p < 0.05).
This surprising stability ofsystem rankings clearly needs further analysis be-yond this paper, but implies that the use of enoughinstances (document sets in this case) allows an av-eraging effect to stablize rankings.There are many system judgments going intothese averages, basically 20 document sets timesthe average number of model units judged per doc-ument set (?
4).
These 80 measurements shouldmake the means of the extra scorings better es-timates of the ?true?
coverage and hence morealike.
More importantly, Figure 5 suggests thatthere is minimal model/system interaction.
Al-though no analysis of variance (ANOVA) was run inDUC-2001, the ANOVAs for DUCs 2002 and 2003verify this lack of interaction.3 DUC-2002DUC-2002 was designed and evaluated in muchthe same manner as DUC-2001 to allow continu-ity of research and evaluation.
There were 60 moredocument sets with manual abstracts created in thesame way as the first 60 sets.
The target lengthsof the summaries were shortened to eliminate the400-word summary and to include a headline lengthsummary.
The SEE GUI was modified to replace thefive-point intervals [All, most, some, hardly any, ornone] with percentages [0, 20, 40, 60, 80, 100] toreflect their perception by judges and treatment byresearchers as a ratio scale.
Seventeen groups thattook part in DUC-2002, with 13 of them tacklingthe single document summary task (at 100 words)and 8 of them working on the multi-document task.3.1 DUC-2002 Results - Effect of Variability inJudgesBeyond the main evaluation, it was decided to mea-sure the variability of the coverage judgments, thistime holding the models constant.
For six of thedocument sets, each peer was judged three addi-tional times, each time by a different judge but us-ing the same model (not a model created by anyof the judges).
Whereas the judgment effect doesnot change the relative ranking of systems in theTREC information retrieval task (Voorhees, 1998),the task in coverage evaluation is much more cogni-tively difficult and needed further exploration.
InDUC the question being asked involves finding aD070 D071 D081 D094 D099 D1020.000.050.100.150.200.250.30DocsetMaximumabsolutecoveragedifferenceFigure 6: DUC-2002 maximum absolute coveragedifferences by document set for 50-word models0.000.050.100.150.200.250.30PeerCoverage16 19 20 24 25 26 28 29Figure 7: DUC-2002 median coverage by systemsusing extra judgment sets (50-word summaries)shared meaning between the content in each modelsummary unit and in the peer summary sentence,and determining how much meaning is shared ?
avery subjective judgment.3.2 Differences in the Coverage JudgmentsUsing the Same ModelThe average absolute coverage score difference be-tween the highest and lowest of the three extra scor-ings of each peer summary for the 50-word sum-maries was 0.079, which is a 47.6% difference(0.070 for the 200-word, or 37.1%).
This is aboutthe same percentage differences seen for the cover-age differences based on using different models inDUC-2001.Once again, there is a wide variation across thesix document sets (see Figure 6).
Even though themedian is similar across these sets, the variationis much larger for two of the document sets, andmuch smaller for two others.
The variation in cover-age score for the 200-word summaries is much less,similar to what was found in DUC-2001.3.3 Effects of Judgment Differences on perSystem Coverage ScoresFigure 7 shows how the extra judgment variationsaffected the average system coverage scores.
Thelines plotted are similar to those shown for theDUC-2001 model variations, one line for each setof extra judgments.
The scores again are very closetogether in absolute value and in general the systemsare ranked similarly.
In this case, the pairwise cor-relations (Pearson?s) were 0.840, 0.723, and 0.801(p < 0.05).
With only six document sets involved inthe averaging, versus the 20 used in DUC-2001, itis surprising that there is still so little effect.3.4 ANOVA ResultsThe extra three judgments per peer allowed for anal-ysis of variance (ANOVA) and estimates of the sizesof the various main effects and interactions.
Whilethe main effects (the judge, system, and documentset) can be large, they are by definition equally dis-tributed across all systems.
Although still signifi-cant, the three interactions modeled ?
judge/system,judge/docset, and system/docset, are much smaller(on the order of the noise, i.e., residuals) and so arenot likely to introduce a bias into the evaluation.Due to lack of space, only the ANOVA for DUC-2003 is included (see Table 1).4 DUC-2003For DUC-2003 it was decided to change the taskssomewhat.
In an effort to get the human sum-maries closer to a common focus, each of the multi-document summary tasks had some constrainingfactor.
There were four different tasks for sum-marization, one very short ?headline?
task for sin-gle documents (300 single documents in the testset), and three different multi-document summarytasks (each task had 30 document sets used in test-ing).
There were 21 groups that participated inDUC-2003, with 13 of them doing task 1, 16 do-ing task 2, 11 doing task 3 and only 9 trying task4.D305D312D315D322D323D326D330D339D355D358D362D363D364D365D368D369D377D382D384D388D397D405D410D414D419D427D432D433D440D4480.10.20.30.4Docset1?gramoverlapFigure 8: DUC-2003 unigram overlap by documentset for 100-word models4.1 DUC-2003 Results - Effect of Variability inJudges and ModelsBeyond the main evaluation it was decided to dofurther investigation into the effects of model andjudgment variation, in particular to focus on task 4(create short summaries of 10 documents that wererelevant to a given question).
Each of the 30 doc-ument sets in task 4 had four different model sum-maries built by four different people, and four judg-ments made where the judge in each case was themodel creator.
The two types of variations were de-liberately confounded for several reasons.
The firstwas that the variations had already been investigatedseparately and it was important to investigate thecombined effect.
The second related issue is thatthis confounding mimics the way the evaluation isbeing run, i.e.
the judges are normally using theirown model, not someone else?s model.
The thirdreason was to provide input to the proposed auto-matic evaluation (ROUGE) to be used in DUC-2004in which multiple models would be used but with nohuman judgments.4.2 Differences in Model/Judgment SetsThe n-gram overlap for the 30 document sets isshown in Figure 8 with six possible pairwise com-parisons for each set of four model summaries.
Theaverage unigram overlap is 0.200, but again a widevariation in overlap across the different documentsets.4.3 Effects of Model/Judgment DifferencesLooking only at the maximum and minimum scorein each set of four, the coverage score differencesD305D312D315D322D323D326D330D339D355D358D362D363D364D365D368D369D377D382D384D388D397D405D410D414D419D427D432D433D440D4480.000.050.100.150.200.250.30DocsetMaximumabsolutecoveragedifferenceFigure 9: DUC-2003 maximum absolute coveragedifferences by document set for 100-word models0.000.050.100.150.200.250.30PeerCoverage10 13 14 16 17 19 20 22 23Figure 10: DUC-2003 median coverage by systemsusing extra judgment sets (100-word summaries)are still high, with an average absolute coverage dif-ference of 0.139 or 69.1% difference.
Again thereis a wide variation across document set/judge pair(see Figure 9).Figure 10 shows the absolute coverage scores foreach system for each of the four model/judgmentpairs.
The difference in absolute scores is small,and the relative ranking of the systems is mostly un-changed.
For DUC-2003, the pairwise correlations(Pearson?s) are 0.899, 0.894, 0.837, 0.827, 0.794,and 0.889 (p < 0.05).
Additionally the scores arelower and closer than in earlier DUCs; this is proba-coverage = grand mean+ judge + system + docset+ judge/system + judge/docset+ system/docsetSource Df SS MS F Pr(F)judge 9 1.243 0.138 47.66 <.0001system 10 0.941 0.094 32.49 <.0001docset 29 1.313 0.045 15.62 <.0001jud/sys 90 0.282 0.003 1.08 0.2939jud/ds 79 1.010 0.012 4.41 <.0001sys/ds 289 3.087 0.010 3.68 <.0001resid 787 2.281 0.002Table 1: Analysis of Variance for DUC-2003bly because task 4 was a new task and systems werein a learning curve.4.4 ANOVA ResultsAn analysis of variance was also run on the DUC-2003 task 4 multiple models and judgments study,and results are presented in Table 1.
The abbre-viations for the column headings are as follows:Df (degrees of freedom), SS (sum of squares), MS(mean square), F (F value), Pr(F) (probability of Funder the null hypothesis).
The judge, system, anddocument set effects predominate as expected.
Al-though still significant, the three interactions mod-eled - judge/system (jud/sys), judge/docset (jud/ds)and system/docset (sys/ds) are smaller than any ofthe main effects.5 ConclusionsThe secondary experiments described in this paperwere by necessity small in scope and so are notconclusive.
Still they consistently suggest stabilityof the SEE-based coverage results reported in thefirst three DUCs, i.e., despite large variations in thehuman-generated model summaries and large vari-ations in human judgments of single-model cover-age, the ranking of the systems remained compara-tively constant when averaged over dozens of docu-ment sets, dozens of peer summaries, and 10 or sojudges.Note that this is only on average, i.e.
there willbe variations reflected in the individual documentsets and the scoring cannot be used reliably at thatlevel.
However, variation in human summaries re-flects the real application and one can only aim atimproved performance on average for better sum-mary methodology.Attempts to reduce or incorporate variability insummarization evaluation will and should continue,e.g., by use of ?factoids?
(van Halteren and Teufel,2003) or ?summarization content units?
(Passon-neau and Nenkova, 2004) as smaller units for gen-erating model summaries.
The use of constrainingfactors such as in DUC-2003 is helpful, but only insome cases since there are many types of summariesthat do not have natural constraints.
Variability is-sues will likely have to be dealt with for some timeand from a number of points of view.In manual evaluations the results of this studyneed to be confirmed using other data.
In ROUGE-like automatic evaluations that avoid variability injudgments and exploit variation in models, the ques-tion of how the number of models and their variabil-ity affect the quality of the ROUGE scoring needsstudy.Beyond laboratory-style evaluations, systembuilders need to attend to variability.
The averageshide variations that need to be analysed; systemsthat do well on average still need failure and suc-cess analysis on individual test cases in order to im-prove.
The variations in human performance stillneed to be studied to understand better why thesevariations are occurring and what this implies aboutthe acceptability of automatic text summarizationfor real end-users.
The effect of variability in train-ing data on the machine learning algorithms used inconstructing many summarization systems must beunderstood.ReferencesHongyan Jing, Regina Barzilay, Kathleen McKe-own, and Michael Elhadad.
1998.
Summariza-tion evaluation methods: Experiments and anal-ysis.
In Intelligent Text Summarization: Papersfrom the 1998 AAAI Spring Symposium, pages51?60.Chin-Yew Lin and Eduard Hovy.
2002.
Manual andautomatic evaluation of summaries.
In Proceed-ings of the ACL 2002 Workshop on Text Summa-rization, pages 45?51.Rebecca Passonneau and Ani Nenkova.
2004.Evaluating content selection in summarization:The pyramid method lexical information.
In Pro-ceedings of the Human Language TechnologyResearch Conference/North American Chapterof the Association of Computational Linguistics,pages 145?152.Radu Soricut and Daniel Marcu.
2003.
Sentencelevel discourse parsing using syntactic and lexicalinformation.
In Proceedings of the Human Lan-guage Technology Research Conference of theNorth American Chapter of the Association ofComputational Linguistics, pages 228?235.Hans van Halteren and Simone Teufel.
2003.
Ex-amining the consensus between human sum-maries: Initial experiments with factoid analysis.In Proceedings of the HLT-NAACL2003 Work-shop on Text Summarization, pages 57?64.Ellen M. Voorhees.
1998.
Variations in relevancejudgments and the measurement of retrieval ef-fectiveness.
In Proceedings of the 21th An-nual International ACM SIGIR Conference onResearch and Development in Information Re-trieval, pages 315?323.
ACM, New York.
