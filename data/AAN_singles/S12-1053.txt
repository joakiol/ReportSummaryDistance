First Joint Conference on Lexical and Computational Semantics (*SEM), pages 399?407,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsSemeval-2012 Task 8:Cross-lingual Textual Entailment for Content SynchronizationMatteo NegriFBK-irstTrento, Italynegri@fbk.euAlessandro MarchettiCELCTTrento, Italyamarchetti@celct.itYashar MehdadFBK-irstTrento, Italymehdad@fbk.euLuisa BentivogliFBK-irstTrento, Italybentivo@fbk.euDanilo GiampiccoloCELCTTrento, Italygiampiccolo@celct.itAbstractThis paper presents the first round of thetask on Cross-lingual Textual Entailment forContent Synchronization, organized withinSemEval-2012.
The task was designed to pro-mote research on semantic inference over textswritten in different languages, targeting at thesame time a real application scenario.
Par-ticipants were presented with datasets for dif-ferent language pairs, where multi-directionalentailment relations (?forward?, ?backward?,?bidirectional?, ?no entailment?)
had to beidentified.
We report on the training and testdata used for evaluation, the process of theircreation, the participating systems (10 teams,92 runs), the approaches adopted and the re-sults achieved.1 IntroductionThe cross-lingual textual entailment task (Mehdad etal., 2010) addresses textual entailment (TE) recog-nition (Dagan and Glickman, 2004) under the newdimension of cross-linguality, and within the newchallenging application scenario of content synchro-nization.Cross-linguality represents a dimension of the TErecognition problem that has been so far only par-tially investigated.
The great potential for integrat-ing monolingual TE recognition components intoNLP architectures has been reported in several ar-eas, including question answering, information re-trieval, information extraction, and document sum-marization.
However, mainly due to the absence ofcross-lingual textual entailment (CLTE) recognitioncomponents, similar improvements have not beenachieved yet in any cross-lingual application.
TheCLTE task aims at prompting research to fill thisgap.
Along such direction, research can now ben-efit from recent advances in other fields, especiallymachine translation (MT), and the availability of: i)large amounts of parallel and comparable corpora inmany languages, ii) open source software to com-pute word-alignments from parallel corpora, and iii)open source software to set up MT systems.
Webelieve that all these resources can positively con-tribute to develop inference mechanisms for multi-lingual data.Content synchronization represents a challengingapplication scenario to test the capabilities of ad-vanced NLP systems.
Given two documents aboutthe same topic written in different languages (e.g.Wiki pages), the task consists of automatically de-tecting and resolving differences in the informationthey provide, in order to produce aligned, mutuallyenriched versions of the two documents.
Towardsthis objective, a crucial requirement is to identify theinformation in one page that is either equivalent ornovel (more informative) with respect to the contentof the other.
The task can be naturally cast as anentailment recognition problem, where bidirectionaland unidirectional entailment judgments for two textfragments are respectively mapped into judgmentsabout semantic equivalence and novelty.
Alterna-tively, the task can be seen as a machine translationevaluation problem, where judgments about seman-tic equivalence and novelty depend on the possibilityto fully or partially translate a text fragment into theother.399Figure 1: ?bidirectional?, ?forward?, ?backward?
and?no entailment?
judgments for SP/EN CLTE pairs.The recent advances on monolingual TE on theone hand, and the methodologies used in Statisti-cal Machine Translation (SMT) on the other, offerpromising solutions to approach the CLTE task.
Inline with a number of systems that model the RTEtask as a similarity problem (i.e.
handling similar-ity scores between T and H as useful evidence todraw entailment decisions), the standard sentenceand word alignment programs used in SMT offer astrong baseline for CLTE.
However, although repre-senting a solid starting point to approach the prob-lem, similarity-based techniques are just approx-imations, open to significant improvements com-ing from semantic inference at the multilinguallevel (e.g.
cross-lingual entailment rules such as?perro???animal?).
Taken in isolation, similarity-based techniques clearly fall short of providing aneffective solution to the problem of assigning direc-tions to the entailment relations (especially in thecomplex CLTE scenario, where entailment relationsare multi-directional).
Thanks to the contiguity be-tween CLTE, TE and SMT, the proposed task pro-vides an interesting scenario to approach the issuesoutlined above from different perspectives, and largeroom for mutual improvement.2 The taskGiven a pair of topically related text fragments (T1and T2) in different languages, the CLTE task con-sists of automatically annotating it with one of thefollowing entailment judgments (see Figure 1 forSpanish/English examples of each judgment):?
bidirectional (T1?T2 & T1?T2): the twofragments entail each other (semantic equiva-lence);?
forward (T1?T2 & T16?T2): unidirectionalentailment from T1 to T2;?
backward (T16?T2 & T1?T2): unidirectionalentailment from T2 to T1;?
no entailment (T16?T2 & T16?T2): there isno entailment between T1 and T2 in both direc-tions;In this task, both T1 and T2 are assumed to betrue statements.
Although contradiction is relevantfrom an application-oriented perspective, contradic-tory pairs are not present in the dataset created forthe first round of the task.3 Dataset descriptionFour CLTE corpora have been created for the fol-lowing language combinations: Spanish/English(SP-EN), Italian/English (IT-EN), French/English(FR-EN), German/English (DE-EN).
The datasetsare released in the XML format shown in Figure 1.3.1 Data collection and annotationThe dataset was created following the crowdsourc-ing methodology proposed in (Negri et al, 2011),which consists of the following steps:1.
First, English sentences were manually ex-tracted from copyright-free sources (Wikipediaand Wikinews).
The selected sentences repre-sent one of the elements (T1) of each entail-ment pair;2.
Next, each T1 was modified through crowd-sourcing in various ways in order to ob-tain a corresponding T2 (e.g.
introduc-ing meaning-preserving lexical and syntacticchanges, adding and removing portions oftext);3.
Each T2 was then paired to the original T1,and the resulting pairs were annotated with oneof the four entailment judgments.
In order toreduce the correlation between the differencein sentences?
length and entailment judgments,400only the pairs where the difference between thenumber of words in T1 and T2 (length diff ) wasbelow a fixed threshold (10 words) were re-tained.1 The final result is a monolingual En-glish dataset annotated with multi-directionalentailment judgments, which are well dis-tributed over length diff values ranging from 0to 9;4.
In order to create the cross-lingual datasets,each English T1 was manually translated intofour different languages (i.e.
Spanish, German,Italian and French) by expert translators;5.
By pairing the translated T1 with the cor-responding T2 in English, four cross-lingualdatasets were obtained.To ensure the good quality of the datasets, all thecollected pairs were manually checked and correctedwhen necessary.
Only pairs with agreement betweentwo expert annotators were retained.
The final resultis a multilingual parallel entailment corpus, whereT1s are in 5 different languages (i.e.
English, Span-ish, German, Italian, and French), and T2s are in En-glish.
It?s worth mentioning that the monolingualEnglish corpus, a by-product of our data collectionmethodology, will be publicly released as a furthercontribution to the research community.23.2 Dataset statisticsEach dataset consists of 1,000 pairs (500 for trainingand 500 for test), balanced across the four entail-ment judgments (bidirectional, forward, backward,and no entailment).For each language combination, the distribu-tion of the four entailment judgments according tolength diff is shown in Figure 2.
Vertical bars rep-resent, for each length diff value, the proportionof pairs belonging to the four entailment classes.As can be seen, the length diff constraint appliedto the length difference in the monolingual English1Such constraint has been applied in order to focus as muchas possible on semantic aspects of the problem, by reduc-ing the applicability of simple association rules such as IFlength(T1)>length(T2) THEN T1?T2.2The cross-lingual datasets are already available for researchpurposes at http://www.celct.it/resourcesList.php.
The monolingual English dataset will be publicly releasedto non participants in July 2012.pairs (step 3 of the creation process) is substantiallyreflected in the cross-lingual datasets for all lan-guage combinations.
In fact, as shown in Table 1,the majority of the pairs is always included in thesame length diff range (approximately [-5,+5]) and,within this range, the distribution of the four classesis substantially uniform.
Our assumption is that suchdata distribution makes entailment judgments basedon mere surface features such as sentence length in-effective, thus encouraging the development of alter-native, deeper processing strategies.SP-EN IT-EN FR-EN DE-ENForward 104 132 121 179Backward 202 182 191 123No entailment 163 173 169 174Bidirectional 175 199 193 209ALL 644 686 674 685Table 1: CLTE pairs distribution within the -5/+5length diff range.4 Evaluation metrics and baselinesEvaluation results have been automatically com-puted by comparing the entailment judgments re-turned by each system with those manually assignedby human annotators.
The metric used for systems?ranking is accuracy over the whole test set, i.e.
thenumber of correct judgments out of the total numberof judgments in the test set.
Additionally, we calcu-lated precision, recall, and F1 measures for each ofthe four entailment judgment categories taken sep-arately.
These scores aim at giving participants thepossibility to gain clearer insights into their system?sbehavior on the entailment phenomena relevant tothe task.For each language combination, two baselinesconsidering the length difference between T1 and T2have been calculated (besides the trivial 0.25 accu-racy score obtained by assigning each test pair in thebalanced dataset to one of the four classes):?
Composition of binary judgments (Bi-nary).
To calculate this baseline an SVMclassifier is trained to take binary entailmentdecisions (?YES?, ?NO?).
The classifier useslength(T1)/length(T2) as a single feature tocheck for entailment from T1 to T2, andlength(T2)/length(T1) for the opposite direc-tion.
For each test pair, the unidirectional40101020304050607080-21 -18 -15 -12 -9 -6 -3 0 3 6 9no_entailmentforwardbidirectionalbackward(a) SP-EN01020304050607080-17 -14 -11 -8 -5 -2 1 4 7 10no_entailmentforwardbidirectionalbackward(b) IT-EN0102030405060708090-21 -18 -15 -12 -9 -6 -3 0 3 6 9 12no_entailmentforwardbidirectionalbackward(c) FR-EN0102030405060708090-14 -10 -7 -4 -1 2 5 8 11 14 17no_entailmentforwardbidirectionalbackward(d) DE-ENFigure 2: CLTE pairs distribution for different length diff values across all datasets.judgments returned by the two classifiers arecomposed into a single multi-directional judg-ment (?YES-YES?=?bidirectional?, ?YES-NO?=?forward?, ?NO-YES?=?backward?,?NO-NO?=?no entailment?);?
Multi-class classification (Multi-class).
Asingle SVM classifier is trained with the samefeatures to directly assign to each pair one ofthe four entailment judgments.Both the baselines have been calculated with theLIBSVM package (Chang and Lin, 2011), using alinear kernel with default parameters.
Baseline re-sults are reported in Table 2.Although the four CLTE datasets are derived fromthe same monolingual EN-EN corpus, baseline re-sults present slight differences due to the effect oftranslation into different languages.SP-EN IT-EN FR-EN DE-EN1-class 0.25 0.25 0.25 0.25Binary 0.34 0.39 0.39 0.40Multi-class 0.43 0.44 0.42 0.42Table 2: Baseline accuracy results.5 Submitted runs and resultsParticipants were allowed to submit up to five runsfor each language combination.
A total of 17 teamsregistered to participate in the task and downloadedthe training set.
Out of them, 12 downloaded thetest set and 10 (including one of the task organizers)submitted valid runs.
Eight teams produced submis-sions for all the language combinations, while twoteams participated only in the SP-EN task.
In total,92 runs have been submitted and evaluated (29 forSP-EN, and 21 for each of the other language pairs).402Despite the novelty and the difficulty of the problem,these numbers demonstrate the interest raised by thetask, and the overall success of the initiative.System name SP-EN IT-EN FR-EN DE-ENBUAP run1 0.350 0.336 0.334 0.330BUAP run2 0.366 0.344 0.342 0.268celi run1 0.276 0.278 0.278 0.280celi run2 0.336 0.338 0.300 0.352celi run3 0.322 0.334 0.298 0.350celi run4 0.268 0.280 0.280 0.274DirRelCond3 run1 0.300 0.280 0.362 0.336DirRelCond3 run2 0.300 0.284 0.360 0.336DirRelCond3 run3 0.300 0.338 0.384 0.364DirRelCond3 run4 0.344 0.316 0.384 0.374FBK run1* 0.502 - - -FBK run2* 0.490 - - -FBK run3* 0.504 - - -FBK run4* 0.500 - - -HDU run1 0.630 0.554 0.564 0.558HDU run2 0.632 0.562 0.570 0.552ICT run1 0.448 0.454 0.456 0.460JU-CSE-NLP run1 0.274 0.316 0.288 0.262JU-CSE-NLP run2 0.266 0.326 0.294 0.296JU-CSE-NLP run3 0.272 0.314 0.296 0.264Sagan run1 0.342 0.352 0.346 0.342Sagan run2 0.328 0.352 0.336 0.310Sagan run3 0.346 0.356 0.330 0.332Sagan run4 0.340 0.330 0.310 0.310SoftCard run1 0.552 0.566 0.570 0.550UAlacant run1 LATE 0.598 - - -UAlacant run2 0.582 - - -UAlacant run3 LATE 0.510 - - -UAlacant run4 0.514 - - -Highest 0.632 0.566 0.570 0.558Average 0.440 0.411 0.408 0.408Median 0.407 0.350 0.365 0.363Lowest 0.274 0.326 0.296 0.296Table 3: Accuracy results (92 runs) over the 4 lan-guage combinations.
Highest, average, median and low-est scores are calculated considering the best run for eachteam (*task organizers?
system).Accuracy results are reported in Table 3.
As canbe seen from the table, overall accuracy scores arequite different across language pairs, with the high-est result on SP-EN (0.632), which is considerablyhigher than the highest score on DE-EN (0.558).This might be due to the fact that most of the partic-ipating systems rely on a ?pivoting?
approach thataddresses CLTE by automatically translating T1 inthe same language of T2 (see Section 6).
Regard-ing the DE-EN dataset, pivoting methods might bepenalized by the lower quality of MT output whenGerman T1s are translated into English.The comparison with baselines results leads to in-teresting observations.
First of all, while all systemssignificantly outperform the lowest 1-class baseline(0.25), both other baselines are surprisingly hard tobeat.
This shows that, despite the effort in keep-ing the distribution of the entailment classes uni-form across different length diff values, eliminatingthe correlation between sentences?
length and cor-rect entailment decisions is difficult.
As a conse-quence, although disregarding semantic aspects ofthe problem, features considering such informationare quite effective.In general, systems performed better on the SP-EN dataset, with most results above the binary base-line (8 out of 10), and half of the systems above themulti-class baseline.
For the other language pairsthe results are lower, with only 3 out of 8 partici-pants above the two baselines in all datasets.
Aver-age results reflect this situation: the average scoresare always above the binary baseline, whereas onlythe SP-EN average result is higher than the multi-class baseline(0.44 vs. 0.43).To better understand the behaviour of each sys-tem (also in relation to the different language com-binations), Table 4 provides separate precision, re-call, and F1 scores for each entailment judgment,calculated over the best runs of each participatingteam.
Overall, the results suggest that the ?bidi-rectional?
and ?no entailment?
categories are moreproblematic than ?forward?
and ?backward?
judg-ments.
For most datasets, in fact, systems?
perfor-mance on ?bidirectional?
and ?no entailment?
is sig-nificantly lower, typically on recall.
Except for theDE-EN dataset (more problematic on ?forward?
),also average F1 results on these judgments are lower.This might be due to the fact that, for all datasets, thevast majority of ?bidirectional?
and ?no entailment?judgments falls in a length diff range where the dis-tribution of the four classes is more uniform (seeFigure 2).Similar reasons can justify the fact that ?back-ward?
entailment results are consistently higher onall datasets.
Compared with ?forward?
entailment,these judgments are in fact less scattered across theentire length diff range (i.e.
less intermingled withthe other classes).4036 ApproachesA rough classification of the approaches adopted byparticipants can be made along two orthogonal di-mensions, namely:?
Pivoting vs. Cross-lingual.
Pivoting meth-ods rely on the automatic translation of one ofthe two texts (either single words or the en-tire sentence) into the language of the other(typically English) in order perform monolin-gual TE recognition.
Cross-lingual methods as-sign entailment judgments without preliminarytranslation.?
Composition of binary judgments vs. Multi-class classification.
Compositional approachesmap unidirectional entailment decisions takenseparately into single judgments (similar to theBinary baseline in Section 4).
Methods basedon multi-class classification directly assign oneof the four entailment judgments to each testpair (similar to our Multi-class baseline).Concerning the former dimension, most of thesystems (6 out of 10) adopted a pivoting approach,relying on Google Translate (4 systems), MicrosoftBing Translator (1), or a combination of Google,Bing, and other MT systems (1) to produce EnglishT2s.
Regarding the latter dimension, the composi-tional approach was preferred to multi-class classi-fication (6 out of 10).
The best performing systemrelies on a ?hybrid?
approach (combining monolin-gual and cross-lingual alignments) and a compo-sitional strategy.
Besides the frequent recourse toMT tools, other resources used by participants in-clude: on-line dictionaries for the translation of sin-gle words, word alignment tools, part-of-speech tag-gers, NP chunkers, named entity recognizers, stem-mers, stopwords lists, and Wikipedia as an externalmultilingual corpus.
More in detail:BUAP [pivoting, compositional] (Vilarin?o et al,2012) adopts a pivoting method based on translatingT1 into the language of T2 and vice versa (GoogleTranslate3 and the OpenOffice Thesaurus4).
Simi-larity measures (e.g.
Jaccard index) and rules are3http://translate.google.com/4http://extensions.services.openoffice.org/en/taxonomy/term/233respectively used to annotate the two resulting sen-tence pairs with entailment judgments and combinethem in a single decision.CELI [cross lingual, compositional & multi-class] (Kouylekov, 2012) uses dictionaries for wordmatching, and a multilingual corpus extracted fromWikipedia for term weighting.
Word overlap andsimilarity measures are then used in different ap-proaches to the task.
In one run (Run 1), they areused to train a classifier that assigns separate en-tailment judgments for each direction.
Such judg-ments are finally composed into a single one for eachpair.
In the other runs, the same features are used formulti-class classification.DirRelCond3 [cross lingual, compositional](Perini, 2012) uses bilingual dictionaries (Freedict5and WordReference6) to translate content words intoEnglish.
Then, entailment decisions are taken com-bining directional relatedness scores between wordsin both directions (Perini, 2011).FBK [cross lingual, compositional & multi-class] (Mehdad et al, 2012a) uses cross-lingualmatching features extracted from lexical phrase ta-bles, semantic phrase tables, and dependency rela-tions (Mehdad et al, 2011; Mehdad et al, 2012b;Mehdad et al, 2012c).
The features are used formulti-class and binary classification using SVMs.HDU [hybrid, compositional] (Wa?schle andFendrich, 2012) uses a combination of binary clas-sifiers for each entailment direction.
The classifiersuse both monolingual alignment features based onMETEOR (Banerjee and Lavie, 2005) alignments(translations obtained from Google Translate), andcross-lingual alignment features based on GIZA++(Och and Ney, 2000) (word alignments learned onEuroparl).ICT [pivoting, compositional] (Meng et al,2012) adopts a pivoting method (using GoogleTranslate and an in-house hierarchical MT system),and the open source EDITS system (Kouylekov andNegri, 2010) to calculate similarity scores betweenmonolingual English pairs.
Separate unidirectionalentailment judgments obtained from binary classi-fier are combined to return one of the four validCLTE judgments.5http://www.freedict.com/6http://www.wordreference.com/404SP-ENForward Backward No entailment BidirectionalSystem name P R F1 P R F1 P R F1 P R F1BUAP spa-eng run2 0,337 0,664 0,447 0,406 0,568 0,473 0,333 0,088 0,139 0,391 0,144 0,211celi spa-eng run2 0,324 0,368 0,345 0,411 0,368 0,388 0,306 0,296 0,301 0,312 0,312 0,312DirRelCond3 spa-eng run4 0,358 0,608 0,451 0,444 0,448 0,446 0,286 0,032 0,058 0,243 0,288 0,264FBK spa-eng run3 0,515 0,704 0,595 0,546 0,568 0,557 0,447 0,304 0,362 0,482 0,440 0,460HDU spa-eng run2 0,607 0,656 0,631 0,677 0,704 0,690 0,602 0,592 0,597 0,643 0,576 0,608ICT spa-eng run1 0,750 0,240 0,364 0,440 0,472 0,456 0,395 0,560 0,464 0,436 0,520 0,474JU-CSE-NLP spa-eng run1 0,211 0,288 0,243 0,272 0,296 0,284 0,354 0,232 0,280 0,315 0,280 0,297Sagan spa-eng run3 0,225 0,200 0,212 0,269 0,224 0,245 0,418 0,448 0,432 0,424 0,512 0,464SoftCard spa-eng run1 0,602 0,616 0,609 0,650 0,624 0,637 0,471 0,448 0,459 0,489 0,520 0,504UAlacant spa-eng run1 LATE 0,689 0,568 0,623 0,645 0,728 0,684 0,507 0,544 0,525 0,566 0,552 0,559AVG.
0,462 0,491 0,452 0,476 0,5 0,486 0,412 0,354 0,362 0,43 0,414 0,415IT-ENForward Backward No entailment BidirectionalSystem name P R F1 P R F1 P R F1 P R F1BUAP ita-eng run2 0,324 0,456 0,379 0,327 0,672 0,440 0,538 0,056 0,101 0,444 0,192 0,268celi ita-eng run2 0,349 0,360 0,354 0,455 0,36 0,402 0,294 0,320 0,307 0,287 0,312 0,299DirRelCond3 ita-eng run3 0,323 0,488 0,389 0,480 0,288 0,360 0,331 0,368 0,348 0,268 0,208 0,234HDU ita-eng run2 0,564 0,600 0,581 0,628 0,648 0,638 0,551 0,520 0,535 0,500 0,480 0,490ICT ita-eng run1 0,661 0,296 0,409 0,554 0,368 0,442 0,427 0,448 0,438 0,383 0,704 0,496JU-CSE-NLP ita-eng run2 0,240 0,280 0,258 0,339 0,480 0,397 0,412 0,280 0,333 0,359 0,264 0,304Sagan ita-eng run3 0,306 0,296 0,301 0,252 0,216 0,233 0,395 0,512 0,446 0,455 0,400 0,426SoftCard ita-eng run1 0,602 0,616 0,609 0,617 0,696 0,654 0,560 0,448 0,498 0,481 0,504 0,492AVG.
0,421 0,424 0,410 0,457 0,466 0,446 0,439 0,369 0,376 0,397 0,383 0,376FR-ENForward Backward No entailment BidirectionalSystem name P R F1 P R F1 P R F1 P R F1BUAP fra-eng run2 0,447 0,272 0,338 0,291 0,760 0,420 0,250 0,016 0,030 0,449 0,320 0,374celi fra-eng run2 0,316 0,296 0,306 0,378 0,360 0,369 0,270 0,296 0,282 0,244 0,248 0,246DirRelCond3 fra-eng run3 0,393 0,576 0,468 0,441 0,512 0,474 0,387 0,232 0,290 0,278 0,216 0,243HDU fra-eng run2 0,564 0,672 0,613 0,582 0,736 0,650 0,676 0,384 0,490 0,500 0,488 0,494ICT fra-eng run1 0,750 0,192 0,306 0,517 0,496 0,506 0,385 0,656 0,485 0,444 0,480 0,462JU-CSE-NLP fra-eng run3 0,215 0,208 0,211 0,289 0,296 0,292 0,341 0,496 0,404 0,333 0,184 0,237Sagan fra-eng run1 0,244 0,168 0,199 0,297 0,344 0,319 0,394 0,568 0,466 0,427 0,304 0,355SoftCard fra-eng run1 0,551 0,608 0,578 0,649 0,696 0,672 0,560 0,488 0,521 0,513 0,488 0,500AVG.
0,435 0,374 0,377 0,431 0,525 0,463 0,408 0,392 0,371 0,399 0,341 0,364DE-ENForward Backward No entailment BidirectionalSystem name P R F1 P R F1 P R F1 P R F1BUAP deu-eng run1 0,395 0,120 0,184 0,248 0,224 0,235 0,344 0,688 0,459 0,364 0,288 0,321celi deu-eng run2 0,347 0,416 0,378 0,402 0,392 0,397 0,339 0,312 0,325 0,319 0,288 0,303DirRelCond3 deu-eng run4 0,429 0,312 0,361 0,408 0,552 0,469 0,367 0,320 0,342 0,298 0,312 0,305HDU deu-eng run1 0,559 0,528 0,543 0,600 0,696 0,644 0,540 0,488 0,513 0,524 0,520 0,522ICT deu-eng run1 0,718 0,224 0,341 0,493 0,552 0,521 0,390 0,512 0,443 0,439 0,552 0,489JU-CSE-NLP deu-eng run2 0,182 0,048 0,076 0,307 0,496 0,379 0,315 0,560 0,403 0,233 0,080 0,119Sagan deu-eng run1 0,250 0,168 0,201 0,239 0,256 0,247 0,405 0,600 0,484 0,443 0,344 0,387SoftCard deu-eng run1 0,568 0,568 0,568 0,611 0,640 0,625 0,521 0,488 0,504 0,496 0,504 0,500AVG.
0,431 0,298 0,332 0,414 0,476 0,440 0,403 0,496 0,434 0,390 0,361 0,368Table 4: precision, recall and F1 scores, calculated for each team?s best run for all the language combinations.JU-CSE-NLP [pivoting, compositional] (Neogiet al, 2012) uses Microsoft Bing translator7 to pro-duce monolingual English pairs.
Separate lexicalmapping scores are calculated (from T1 to T2 andvice-versa) considering different types of informa-tion and similarity metrics.
Binary entailment de-7http://www.microsofttranslator.com/cisions are then heuristically combined into singledecisions.Sagan [pivoting, multi-class] (Castillo and Car-denas, 2012) adopts a pivoting method using GoogleTranslate, and trains a monolingual system based ona SVM multi-class classifier.
A CLTE corpus de-rived from the RTE-3 dataset is also used as a sourceof additional training material.405SoftCard [pivoting, multi-class] (Jimenez et al,2012) after automatic translation with Google Trans-late, uses SVMs to learn entailment decisions basedon information about the cardinality of: T1, T2, theirintersection and their union.
Cardinalities are com-puted in different ways, considering tokens in T1 andT2, their IDF, and their similarity (computed withedit-distance)UAlacant [pivoting, multi-class] (Espla`-Gomiset al, 2012) exploits translations obtained fromGoogle Translate, Microsoft Bing translator, and theApertium open-source MT platform (Forcada et al,2011).8 Then, a multi-class SVM classifier is usedto take entailment decisions using information aboutoverlapping sub-segments as features.7 ConclusionDespite the novelty of the problem and the diffi-culty to capture multi-directional entailment rela-tions across languages, the first round of the Cross-lingual Textual Entailment for Content Synchroniza-tion task organized within SemEval-2012 was a suc-cessful experience.
This year a new interesting chal-lenge has been proposed, a benchmark for four lan-guage combinations has been released, baseline re-sults have been proposed for comparison, and amonolingual English dataset has been produced asa by-product which can be useful for monolingualTE research.
The interest shown by participantswas encouraging: 10 teams submitted a total of 92runs for all the language pairs proposed.
Overall,the results achieved on all datasets are encourag-ing, with best systems significantly outperformingthe proposed baselines.
It is worth observing that thenature of the task, which lies between semantics andmachine translation, led to the participation of teamscoming from both these communities, showing in-teresting opportunities for integration and mutualimprovement.
The proposed approaches reflect thissituation, with teams traditionally working on MTnow dealing with entailment, and teams tradition-ally participating in the RTE challenges now dealingwith cross-lingual alignment techniques.
Our ambi-tion, for the future editions of the CLTE task, is tofurther consolidate the bridge between the semanticsand MT communities.8http://www.apertium.org/AcknowledgmentsThis work has been partially supported by the EC-funded project CoSyne (FP7-ICT-4-24853).
Theauthors would also like to acknowledge GiovanniMoretti from CELCT for evaluation scripts andtechnical assistance, and the volunteer translatorsthat contributed to the creation of the dataset:Mar?
?a Sol Accossato, Laura Barthe?le?my, Clau-dia Biacchi, Jane Brendler, Amandine Chantrel,Hanna Cheda Patete, Ellen Clancy, Rodrigo DamianTejeda, Daniela Dold, Valentina Frattini, DeboraHedy Amato, Geniz Hernandez, Be?ne?dicte Jean-nequin, Beate Jones, Anne Kauffman, Marcia LauraZanoli, Jasmin Lewis, Alicia Lo?pez, Domenico Los-eto, Sabrina Luja?n Sa?nchez, Julie Mailfait, GabrieleMark, Nunzio Pruiti, Lourdes Rey Cascallar, SylvieMartlew, Aleane Salas Velez, Monica Scalici, An-dreas Schwab, Marianna Sicuranza, Chiara Sisler,Stefano Tordazzi, Yvonne.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Translationand/or Summarization, pages 65?72.Julio Castillo and Marina Cardenas.
2012.
Sagan: ACross Lingual Textual Entailment system based onMachine Traslation.
In Proceedings of the 6th Inter-national Workshop on Semantic Evaluation (SemEval2012).Chih-Chung Chang and Chih-Jen Lin.
2011.
Libsvm:a library for support vector machines.
ACM Trans-actions on Intelligent Systems and Technology (TIST),2(3):27.Ido Dagan and Oren Glickman.
2004.
Probabilistic Tex-tual Entailment: Generic Applied Modeling of Lan-guage Variability.
In Proceedings of the PASCALWorkshop of Learning Methods for Text Understand-ing and Mining.Miquel Espla`-Gomis, Felipe Sa?nchez-Mart?
?nez, andMikel L. Forcada.
2012.
UAlacant: Using OnlineMachine Translation for Cross-Lingual Textual Entail-ment.
In Proceedings of the 6th International Work-shop on Semantic Evaluation (SemEval 2012).Mikel L. Forcada, Ginest?
?-Rosell Mireia, Nordfalk Jacob,O?Regan Jim, Ortiz-Rojas Sergio, Pe?rez-Ortiz Juan A.,Sa?nchez-Mart?
?nez Felipe, Ram?
?rez-Sa?nchez Gema,406and Tyers Francis M. 2011.
Apertium: a Free/Open-Source Platform for Rule-Based Machine Translation.Machine Translation, 25(2):127?144.
Special Issue:Free/Open-Source Machine Translation.Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2012.
Soft Cardinality + ML: Learning Adap-tive Similarity Functions for Cross-lingual Textual En-tailment.
In Proceedings of the 6th InternationalWorkshop on Semantic Evaluation (SemEval 2012).Milen Kouylekov and Matteo Negri.
2010.
An open-source package for recognizing textual entailment.
InProceedings of the ACL 2010 System Demonstrations.Milen Kouylekov.
2012.
CELI: An Experiment withCross Language Textual Entailment.
In Proceedingsof the 6th International Workshop on Semantic Evalu-ation (SemEval 2012).Yashar Mehdad, Matteo Negri, and Marcello Federico.2010.
Towards Cross-Lingual Textual Entailment.
InProceedings of the 11th Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL HLT 2010).Yashar Mehdad, Matteo Negri, and Marcello Federico.2011.
Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies(ACL HLT 2011).Yashar Mehdad, Matteo Negri, and Jose?
G. C. de Souza.2012a.
FBK: Cross-Lingual Textual Entailment With-out Translation.
In Proceedings of the 6th Interna-tional Workshop on Semantic Evaluation (SemEval2012).Yashar Mehdad, Matteo Negri, and Marcello Federico.2012b.
Detecting Semantic Equivalence and Informa-tion Disparity in Cross-lingual Documents.
In Pro-ceedings of the 50th Annual Meeting of the Associationfor Computational Linguistics (ACL 2012).Yashar Mehdad, Matteo Negri, and Marcello Federico.2012c.
Match without a Referee: Evaluating MT Ade-quacy without Reference Translations.
In Proceedingsof the 7th Workshop on Statistical Machine Translation(WMT 2012).Fandong Meng, Hao Xiong, and Qun Liu.
2012.
ICT:A Translation based Cross-lingual Textual Entailment.In Proceedings of the 6th International Workshop onSemantic Evaluation (SemEval 2012).Matto Negri, Luisa Bentivogli, Yashar Mehdad, DaniloGiampiccolo, and Alessandro Marchetti.
2011.
Di-vide and Conquer: Crowdsourcing the Creation ofCross-Lingual Textual Entailment Corpora.
Proceed-ings of the 2011 Conference on Empirical Methods inNatural Language Processing (EMNLP 2011).Snehasis Neogi, Partha Pakray, Sivaji Bandyopadhyay,and Alexander Gelbukh.
2012.
JU-CSE-NLP: Lan-guage Independent Cross-lingual Textual EntailmentSystem.
In Proceedings of the 6th International Work-shop on Semantic Evaluation (SemEval 2012).Franz J. Och and Hermann Ney.
2000.
Improved Sta-tistical Alignment Models.
In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics (ACL 2000).Alpa?r Perini.
2011.
Detecting textual entailmentwith conditions on directional text relatedness scores.Studia Universitatis Babes-Bolyai Series Informatica,LVI(2):13?18.Alpa?r Perini.
2012.
DirRelCond3: Detecting TextualEntailment Across Languages With Conditions On Di-rectional Text Relatedness Scores.
In Proceedings ofthe 6th International Workshop on Semantic Evalua-tion (SemEval 2012).Darnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n,and Esteban Castillo.
2012.
BUAP: Lexical andSemantic Similarity for Cross-lingual Textual Entail-ment.
In Proceedings of the 6th International Work-shop on Semantic Evaluation (SemEval 2012).Katharina Wa?schle and Sascha Fendrich.
2012.
HDU:Cross-lingual Textual Entailment with SMT Features.In Proceedings of the 6th International Workshop onSemantic Evaluation (SemEval 2012).407
