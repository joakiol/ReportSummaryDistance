Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 238?249,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsDual Decomposition with Many Overlapping ComponentsAndre?
F. T.
Martins??
Noah A. Smith?
Pedro M. Q. Aguiar?
Ma?rio A. T.
Figueiredo?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA?Instituto de Sistemas e Robo?tica, Instituto Superior Te?cnico, Lisboa, Portugal?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal{afm,nasmith}@cs.cmu.edu, aguiar@isr.ist.utl.pt, mtf@lx.it.ptAbstractDual decomposition has been recently pro-posed as a way of combining complemen-tary models, with a boost in predictive power.However, in cases where lightweight decom-positions are not readily available (e.g., due tothe presence of rich features or logical con-straints), the original subgradient algorithmis inefficient.
We sidestep that difficulty byadopting an augmented Lagrangian methodthat accelerates model consensus by regular-izing towards the averaged votes.
We showhow first-order logical constraints can be han-dled efficiently, even though the correspond-ing subproblems are no longer combinatorial,and report experiments in dependency pars-ing, with state-of-the-art results.1 IntroductionThe last years have witnessed increasingly accuratemodels for syntax, semantics, and machine transla-tion (Chiang, 2007; Finkel et al, 2008; Petrov andKlein, 2008; Smith and Eisner, 2008; Martins etal., 2009a; Johansson and Nugues, 2008; Koo et al,2010).
The predictive power of such models stemsfrom their ability to break locality assumptions.
Theresulting combinatorial explosion typically demandssome form of approximate decoding, such as sam-pling, heuristic search, or variational inference.In this paper, we focus on parsers built from lin-ear programming relaxations, the so-called ?turboparsers?
(Martins et al, 2009a; Martins et al, 2010).Rush et al (2010) applied dual decomposition asa way of combining models which alone permitefficient decoding, but whose combination is in-tractable.
This results in a relaxation of the origi-nal problem that is elegantly solved with the sub-gradient algorithm.
While this technique has provenquite effective in parsing (Koo et al, 2010; Auliand Lopez, 2011) as well as machine translation(Rush and Collins, 2011), we show here that itssuccess is strongly tied to the ability of finding a?good?
decomposition, i.e., one involving few over-lapping components (or slaves).
With many compo-nents, the subgradient algorithm exhibits extremelyslow convergence (cf.
Fig.
2).
Unfortunately, alightweight decomposition is not always at hand, ei-ther because the problem does not factor in a naturalway, or because one would like to incorporate fea-tures that cannot be easily absorbed in few tractablecomponents.
Examples include features generatedby statements in first-order logic, features that vio-late Markov assumptions, or history features such asthe ones employed in transition-based parsers.To tackle the kind of problems above, we adoptDD-ADMM (Alg.
1), a recently proposed algorithmthat accelerates dual decomposition (Martins et al,2011).
DD-ADMM retains the modularity of thesubgradient-based method, but it speeds up consen-sus by regularizing each slave subproblem towardsthe averaged votes obtained in the previous round(cf.
Eq.
14).
While this yields more involved sub-problems (with a quadratic term), we show that ex-act solutions can still be efficiently computed forall cases of interest, by using sort operations.
Asa result, we obtain parsers that can handle very richfeatures, do not require specifying a decomposition,and can be heavily parallelized.
We demonstrate thesuccess of the approach by presenting experimentsin dependency parsing with state-of-the-art results.2 Background2.1 Structured PredictionLet x ?
X be an input object (e.g., a sentence), fromwhich we want to predict a structured output y ?Y (e.g., a parse tree).
The output set Y is assumedtoo large for exhaustive search to be tractable.
Weassume to have a model that assigns a score f(y) toeach candidate output, based on which we predicty?
= arg maxy?Yf(y).
(1)238Designing the model must obey certain practicalconsiderations.
If efficiency is the major concern,a simple model is usually chosen so that Eq.
1 canbe solved efficiently, at the cost of limited expressivepower.
If we care more about accuracy, a model withricher features and more involved score functionsmay be designed.
Decoding, however, will be moreexpensive, and approximations are often necessary.A typical source of intractability comes from thecombinatorial explosion inherent in the compositionof two or more tractable models (Bar-Hillel et al,1964; Tromble and Eisner, 2006).
Recently, Rushet al (2010) have proposed a dual decompositionframework to address NLP problems in which theglobal score decomposes as f(y) = f1(z1)+f2(z2),where z1 and z2 are two overlapping ?views?
of theoutput, so that Eq.
1 becomes:maximize f1(z1) + f2(z2)w.r.t.
z1 ?
Y1, z2 ?
Y2s.t.
z1 ?
z2.
(2)Above, the notation z1 ?
z2 means that z1 andz2 ?agree on their overlaps,?
and an isomorphismY ' {?z1, z2?
?
Y1?Y2 | z1 ?
z2} is assumed.
Wenext formalize these notions and proceed to compo-sitions of an arbitrary number of models.
Of specialinterest is the unexplored setting where this numberis very large and each component very simple.2.2 Decomposition into PartsA crucial step in the design of structured predictorsis that of decomposing outputs into parts (Taskar etal., 2003).
We assume the following setup:Basic parts.
We let R be a set of basic parts, suchthat each element y ?
Y can be identified with asubset of R. The exact meaning of a ?basic part?is problem dependent.
For example, in dependencyparsing, R can be the set of all possible dependencyarcs (see Fig.
1); in phrase-based parsing, it can bethe set of possible spans; in sequence labeling, it canbe the set of possible labels at each position.
Ouronly assumption is that we can ?read out?
y fromthe basic parts it contains.
For convenience, we rep-resent y as a binary vector, y = ?y(r)?r?R, wherey(r) = 1 if part r belongs to y, and 0 otherwise.Decomposition.
We generalize the decompositionin Eq.
2 by considering sets Y1, .
.
.
,YS for S ?
2.Figure 1: Parts used by our parser.
Arcs are the ba-sic parts: any dependency tree can be ?read out?
fromthe arcs it contains.
Consecutive siblings and grandpar-ent parts introduce horizontal and vertical Markovization(McDonald et al, 2006; Carreras, 2007).
We break thehorizontal Markov assumption via all siblings parts andthe vertical one through parts which indicate a directedpath between two words.
Inspired by transition-basedparsers, we also adopt head bigram parts, which look atthe heads attached to consecutive words.
Finally, we fol-low Martins et al (2009a) and have parts which indicateif an arc is non-projective (i.e., if it spans words that donot descend from its head).Each Ys is associated with its own set of parts Rs, inthe same sense as above; we represent the elementsof Ys as binary vectors zs = ?zs(r)?r?Rs .
Examplesare vectors indicating a tree structure, a sequence,or an assignment of variables to a factor, in whichcase it may happen that only some binary vectorsare legal.
Some parts in Rs are basic, while othersare not.
We denote by R?s = Rs ?
R the subset ofthe ones that are.
In addition, we assume that:?
R1, .
.
.
,RS jointly cover R, i.e., R ?
?Ss=1 Rs;?
Only basic parts may overlap, i.e., Rs ?
Rt ?R, ?s, t ?
{1, .
.
.
, S};?
Each zs ?
Ys is completely defined by its entriesindexed by elements of R?s, from which we canguess the ones in Rs \ R?s.
This implies that eachy ?
Y has a unique decomposition ?z1, .
.
.
, zS?.Fig.
1 shows several parts used in dependency pars-ing models; in phrase-based parsing, these could bespans and production rules anchored in the surfacestring; in sequence labeling, they can be unigram,bigram, and trigram labels.11There is a lot of flexibility about how to decompose themodel into S components: each set Rs can correspond to a sin-239Global consistency.
We want to be able to readout y ?
Y by ?gluing?
together the components?z1, .
.
.
, zS?.
This is only meaningful if they are?globally consistent,?
a notion which we make pre-cise.
Two components zs ?
Ys and zt ?
Yt are saidto be consistent (denoted zs ?
zt) if they agree ontheir overlaps, i.e., if zs(r) = zt(r), ?r ?
Rs ?
Rt.A complete assignment ?z1, .
.
.
, zS?
is globally con-sistent if all pairs of components are consistent.
Thisis equivalent to the existence of a witness vector?u(r)?r?R such that zs(r) = u(r), ?s, r ?
R?s.With this setup, assuming that the score functiondecomposes as f(z) = ?Ss=1 fs(zs), the decodingproblem (which extends Eq.
2 for S ?
2) becomes:P : maximize?Ss=1 fs(zs)w.r.t.
zs ?
Ys, ?s?u(r)?r?R ?
R|R|,s.t.
zs(r) = u(r), ?s, r ?
R?s.
(3)We call the equality constraints expressed in the lastline the ?agreement constraints.?
It is these con-straints that complicate the problem, which wouldotherwise be exactly separable into S subproblems.The dual decomposition method (Komodakis et al,2007; Rush et al, 2010) builds an approximation bydualizing out these constraints, as we describe next.2.3 Dual DecompositionWe describe dual decomposition in a slightly differ-ent manner than Rush et al (2010): we will firstbuild a relaxation of P (called P ?
), in which the en-tire approximation is enclosed.
Then, we dualize P ?,yielding problem D. In the second step, the dualitygap is zero, i.e., P ?
and D are equivalent.2Relaxation.
For each s ?
{1, .
.
.
, S} we considerthe convex hull of Ys,Zs ={ ?zs?Ysp(zs)zs????
p(zs) ?
0,?zs?Ysp(zs) = 1}.
(4)gle factor in a factor graph (Smith and Eisner, 2008), or to aentire subgraph enclosing several factors (Koo et al, 2010), oreven to a formula in Markov logic (Richardson and Domingos,2006).
In these examples, the basic parts may correspond toindividual variable-value pairs.2Instead of following the path P ?
P ?
?
D, Rush et al(2010) go straight from P to D via a Lagrangian relaxation.The two formulations are equivalent for linear score functions.We have that Ys = Zs ?
Z|Rs|; hence, problem P(Eq.
3) is equivalent to one in which each Ys is re-placed by Zs and the z-variables are constrained tobe integer.
By dropping the integer constraints, weobtain the following relaxed problem:P ?
: maximize?Ss=1 fs(zs)w.r.t.
zs ?
Zs, ?s?u(r)?r?R ?
R|R|,s.t.
zs(r) = u(r), ?s, r ?
R?s.
(5)If the score functions fs are convex, P ?
becomes aconvex program (unlike P , which is discrete); beinga relaxation, it provides an upper bound of P .Lagrangian.
Introducing a Lagrange multiplier?s(r) for each agreement constraint in Eq.
5, oneobtains the Lagrangian functionL(z, u, ?)
=?Ss=1(fs(zs) +?r?R?s ?s(r)zs(r))?
?r?R(?s:r?R?s ?s(r))u(r), (6)and the dual problem (the master)D : minimize?Ss=1 gs(?s)w.r.t.
?
= ?
?1, .
.
.
, ?S?s.t.
?s:r?R?s ?s(r) = 0, ?r ?
R,(7)where the gs(?s) are the solution values of the fol-lowing subproblems (the slaves):maximize fs(zs) +?r?R?s ?s(r)zs(r)w.r.t.
zs ?
Zs.
(8)We assume that strong duality holds (w.r.t.
Eqs.
5?7), hence we have P ?
P ?
= D.3Solving the dual.
Why is the dual formulation D(Eqs.
7?8) more appealing than P ?
(Eq.
5)?
The an-swer is that the components 1, .
.
.
, S are now de-coupled, which makes things easier provided eachslave subproblem (Eq.
8) can be solved efficiently.In fact, this is always a concern in the mind ofthe model?s designer when she chooses a decom-position (the framework that we describe in ?3,in some sense, alleviates her from this concern).If the score functions are linear, i.e., of the formfs(zs) =?r?Rs ?s(r)zs(r) for some vector ?s =?
?s(r)?r?Rs , then Eq.
8 becomes a linear program,for which a solution exists at a vertex of Zs (which3This is guaranteed if the score functions fs are linear.240in turn is an element of Ys).
Depending on the struc-ture of the problem, Eq.
8 may be solved by bruteforce, dynamic programming, or specialized combi-natorial algorithms (Rush et al, 2010; Koo et al,2010; Rush and Collins, 2011).Applying the projected subgradient method (Ko-modakis et al, 2007; Rush et al, 2010) to the mas-ter problem (Eq.
7) yields a remarkably simple algo-rithm, which at each round t solves the subproblemsin Eq.
8 for s = 1, .
.
.
, S, and then gathers thesesolutions (call them zt+1s ) to compute an ?averaged?vote for each basic part,ut+1(r) = 1?
(r)?s:r?R?s zt+1s (r), (9)where ?
(r) = |{s : r ?
Rs}| is the number of com-ponents which contain part r. An update of the La-grange variables follows,?t+1s (r) = ?ts(r)?
?t(zt+1s (r)?
ut+1(r)), (10)where ?t is a stepsize.
Intuitively, the algorithmpushes for a consensus among the slaves (Eq.
9),via an adjustment of the Lagrange multipliers whichtakes into consideration deviations from the aver-age (Eq.
10).
The subgradient method is guaran-teed to converge to the solution of D (Eq.
7), forsuitably chosen stepsizes (Shor, 1985; Bertsekas etal., 1999); it also provides a certificate of optimal-ity in case the relaxation is tight (i.e., P = D) andthe exact solution has been found.
However, con-vergence is slow when S is large (as we will showin the experimental section), and no certificates areavailable when there is a relaxation gap (P < P ?
).In the next section, we describe the DD-ADMM al-gorithm (Martins et al, 2011), which does not havethese drawbacks and shares a similar simplicity.3 Alternating Directions MethodThere are two reasons why subgradient-based dualdecomposition is not completely satisfying:?
it may take a long time to reach a consensus;?
it puts all its resources in solving the dual problemD, and does not attempt to make progress in theprimal P ?, which is closer to our main concern.44Our main concern is P ; however solving P ?
is often auseful step towards that goal, either because a good roundingscheme exists, or because one may build tighter relaxations toapproach P (Sontag et al, 2008; Rush and Collins, 2011).Taking a look back at the relaxed primal problemP ?
(Eq.
5), we see that any primal feasible solutionmust satisfy the agreement constraints.
This sug-gests that penalizing violations of these constraintscould speed up consensus.Augmented Lagrangian.
By adding a penaltyterm to Eq.
6, we obtain the augmented Lagrangianfunction (Hestenes, 1969; Powell, 1969):A?
(z, u, ?)
= L(z, u, ?)??2S?s=1?r?R?s(zs(r)?
u(r))2,(11)where the parameter ?
?
0 controls the intensityof the penalty.
Augmented Lagrangian methodsare well-known in the optimization community (see,e.g., Bertsekas et al (1999), ?4.2).
They alternateupdates to the ?-variables, while seeking to maxi-mize A?
with respect to z and u.
In our case, how-ever, this joint maximization poses difficulties, sincethe penalty term couples the two variables.
The al-ternating directions method of multipliers (ADMM),coined by Gabay and Mercier (1976) and Glowinskiand Marroco (1975), sidesteps this issue by perform-ing alternate maximizations,zt+1 = arg maxzA?
(z, ut, ?t), (12)ut+1 = arg maxuA?
(zt+1, u, ?t), (13)followed by an update of the Lagrange multipliersas in Eq.
10.
Recently, ADMM has attracted inter-est, being applied in a variety of problems; see therecent book by Boyd et al (2011) for an overview.As derived in the App.
A, the u-updates in Eq.
13have a closed form, which is precisely the averag-ing operation performed by the subgradient method(Eq.
9).
We are left with the problem of comput-ing the z-updates.
Like in the subgradient approach,the maximization in Eq.
12 can be separated into Sindependent slave subproblems, which now take theform:maximize fs(zs) +?r?R?s ?s(r)zs(r)??2?r?R?s(zs(r)?
ut(r))2w.r.t.
zs ?
Zs(x).
(14)Comparing Eq.
8 and Eq.
14, we observe that theonly difference is the presence in the latter of a241quadratic term which regularizes towards the pre-vious averaged votes ut(r).
Because of this term,the solution of Eq.
14 for linear score functions maynot be at a vertex (in contrast to the subgradientmethod).
We devote ?4 to describing exact and effi-cient ways of solving the problem in Eq.
14 for im-portant, widely used slaves.
Before going into de-tails, we mention another advantage of ADMM overthe subgradient algorithm: it knows when to stop.Primal and dual residuals.
Recall that the sub-gradient method provides optimality certificateswhen the relaxation is tight (P = P ?)
and an ex-act solution of P has been found.
While this is goodenough when tight relaxations are frequent, as in thesettings explored by Rush et al (2010), Koo et al(2010), and Rush and Collins (2011), it is hard toknow when to stop when a relaxation gap exists.We would like to have similar guarantees concern-ing the relaxed primal P ?.5 A general weakness ofsubgradient algorithms is that they do not have thiscapacity, and so are usually stopped by specifying amaximum number of iterations.
In contrast, ADMMallows to keep track of primal and dual residuals(Boyd et al, 2011).
This allows providing certifi-cates not only for the exact solution of P (when therelaxation is tight), but also to terminate when a nearoptimal solution of the relaxed problem P ?
has beenfound.
The primal residual rtP measures the amountby which the agreement constraints are violated:rtP =?Ss=1?r?R?s(zts(r)?
ut(r))2?r?R ?
(r); (15)the dual residual rtD is the amount by which a dualoptimality condition is violated (see Boyd et al(2011), p.18, for details).
It is computed via:rtD =?r?R ?(r)(ut(r)?
ut?1(r))2?r?R ?
(r), (16)Our stopping criterion is thus that these two residu-als are below a threshold, e.g., 1 ?
10?3.
The com-plete algorithm is depicted as Alg.
1.
As stated in5This problem is more important than it may look.
Problemswith many slaves tend to be less exact, hence relaxation gapsare frequent.
Also, when decoding is embedded in training, it isuseful to obtain the fractional solution of the relaxed primal P(rather than an approximate integer solution).
See Kulesza andPereira (2007) and Martins et al (2009b) for details.Algorithm 1 ADMM-based Dual Decomposition1: input: score functions ?fs(.
)?Ss=1, parameters ?, ?,thresholds P and D.2: initialize t?
13: initialize u1(r)?
0.5 and ?1s(r)?
0, ?s, ?r ?
R?s4: repeat5: for each s = 1, .
.
.
, S do6: make a zs-update, yielding zt+1s (Eq.
14)7: end for8: make a u-update, yielding ut+1 (Eq.
9)9: make a ?-update, yielding ?t+1 (Eq.
10)10: t?
t+ 111: until rt+1P < P and rt+1D < D (Eqs.
15?16)12: output: relaxed primal and dual solutions u, z, ?Martins et al (2011), convergence to the solution ofP ?
is guaranteed with a fixed stepsize ?t = ?
?, with?
?
[1, 1.618] (Glowinski and Le Tallec, 1989, Thm.4.2).
In our experiments, we set ?
= 1.5, and adapt?
as described in (Boyd et al, 2011, p.20).64 Solving the SubproblemsIn this section, we address the slave subproblems ofDD-ADMM (Eq.
14).
We show how these subprob-lems can be solved efficiently for several importantcases that arise in NLP applications.
Throughout,we assume that the score functions fs are linear, i.e.,they can be written as fs(zs) = ?r?Rs ?s(r)zs(r).This is the case whenever a linear model is used, inwhich case ?s(r) = 1?
(r)w ?
?
(x, r), where w is aweight vector and ?
(x, r) is a feature vector.
It isalso the scenario studied in previous work in dualdecomposition (Rush et al, 2010).
Under this as-sumption, and discarding constant terms, the slavesubproblem in Eq.
14 becomes:maxzs?Zs?r?Rs\R?s?s(r)zs(r)??2?r?R?s(zs(r)?
as(r))2.
(17)where as(r) = ut(r)+??1(?s(r)+?ts(r)).
Since Zsis a polytope, Eq.
17 is a quadratic program, whichcan be solved with a general purpose solver.
How-ever, that does not exploit the structure of Zs and isinefficient when |Rs| is large.
We next show that formany cases, a closed-form solution is available and6Briefly, we initialize ?
= 0.03 and then increase/decrease?
by a factor of 2 whenever the primal residual becomes > 10times larger/smaller than the dual residual.242can be computed inO(|Rs|) time, up to log factors.7Pairwise Factors.
This is the case where RPAIR ={r1, r2, r12}, where r1 and r2 are basic parts andr12 is their conjunction, i.e., we have YPAIR ={?z1, z2, z12?
| z12 = z1 ?
z2}.
This factor is use-ful to make conjunctions of variables participate inthe score function (see e.g.
the grandparent, sibling,and head bigram parts in Fig.
1).
The convex hullof YPAIR is the polytope ZPAIR = {?z1, z2, z12?
?
[0, 1]3 | z12 ?
z1, z12 ?
z2, z12 ?
z1 + z2 ?
1}, asshown by Martins et al (2010).
In this case, problem(17) can be written asmax ?12z12 ?
?2 [(z1 ?
a1)2 + (z2 ?
a2)2]w.r.t.
?z1, z2, z12?
?
[0, 1]3s.t.
z12 ?
z1, z12 ?
z2, z12 ?
z1 + z2 ?
1(18)and has a closed form solution (see App.
B).Uniqueness Quantification and XOR.
Manyproblems involve constraining variables to take asingle value: for example, in dependency parsing,a modifier can only take one head.
This can beexpressed as the statement ?
!y : Q(y) in first-orderlogic,8 or as a one-hot XOR factor in a factorgraph (Smith and Eisner, 2008; Martins et al,2010).
In this case, RXOR = {r1, .
.
.
, rn}, andYXOR = {?z1, .
.
.
, zn?
?
{0, 1}n |?ni=1 zi = 1}.The convex hull of YXOR is ZXOR = {?z1, .
.
.
, zn?
?
[0, 1]n | ?ni=1 zi = 1}.
Assume for the sake ofsimplicity that all parts in RXOR are basic.9 Up to aconstant, the slave subproblem becomes:minimize 12?ni=1(zi ?
ai)2w.r.t.
?z1, .
.
.
, zn?
?
[0, 1]ns.t.
?ni zi = 1.
(19)This is the problem of projecting onto the probabil-ity simplex, which can be done in O(n log n) timevia a sort operation (see App.
C).107This matches the asymptotic time that would be necessaryto solve the corresponding problems in the subgradient method,for which algorithms are straightforward to derive.
The point isthat with ADMM fewer instances of these subproblems need tobe solved, due to faster convergence of the master problem.8The symbol ?!
means ?there is one and only one.
?9A similar derivation can be made otherwise.10Also common is the need for constraining existence of ?atmost one?
element.
This can be reduced to uniqueness quantifi-cation by adding a dummy NULL label.Existential Quantification and OR.
Sometimes,only existence is required, not necessarily unique-ness.
This can be expressed with disjunctions, ex-istential quantifiers in first-order logic (?y : Q(y)),or as a OR factor.
In this case, ROR = {r1, .
.
.
, rn},YOR = {?z1, .
.
.
, zn?
?
{0, 1}n |?ni=1 zi = 1},and the convex hull is ZOR = {?z1, .
.
.
, zn?
?
[0, 1]n | ?ni=1 zi ?
1} (see Tab.
1 in Martins et al(2010)).
The slave subproblem becomes:minimize 12?ni=1(zi ?
ai)2w.r.t.
?z1, .
.
.
, zn?
?
[0, 1]ns.t.
?ni zi ?
1.
(20)We derive a procedure in App.
D to compute thisprojection in O(n log n) runtime, also with a sort.Negations.
The two cases above can be extendedto allow some of their inputs to be negated.
By achange of variables in Eqs.
19?20 it is possible toreuse the same black box that solves those problems.The procedure is as follows:1.
For i = 1, .
.
.
, n, set a?i = 1?ai if the ith variableis negated, and a?i = ai otherwise.2.
Obtain ?z?1, .
.
.
, z?n?
as the solution of Eqs.
19 or20 providing ?a?1, .
.
.
, a?n?
as input.3.
For i = 1, .
.
.
, n, set zi = 1?z?i if the ith variableis negated, and zi = z?i otherwise.The ability to handle negated variables adds agreat degree of flexibility.
From De Morgan?slaws, we can now handle conjunctions and impli-cations (since ?ni=1Qi(x) ?
R(x) is equivalent to?ni=1 ?Qi(x) ?R(x)).Logical Variable Assignments.
All previous ex-amples involve taking a group of existing variablesand defining a constraint.
Alternatively, we maywant to define a new variable which is the result ofan operation involving other variables.
For exam-ple, R(x) := ?
!y : Q(x, y).
This corresponds to theXOR-WITH-OUTPUT factor in Martins et al (2010).Interestingly, this can be expressed as a XOR whereR(x) is negated (i.e., either ?R(x) holds or exactlyone y satisfies Q(x, y), but not both).A more difficult problem is that of the OR-WITH-OUTPUT factor, expressed by the formula R(x) :=?y : Q(x, y).
We have ROR-OUT = {r0, .
.
.
, rn},and YOR-OUT = {?z0, .
.
.
, zn?
?
{0, 1}n | z0 =243# Slaves Runtime DescriptionTree ?
!h : arc(h,m), m 6= 0 O(n) O(n logn) Each non-root word has a headflow(h,m, k)?
arc(h,m) O(n3) O(1) Only active arcs may carry flowpath(m, d) := ?
!h : flow(h,m, d), m 6= 0 O(n2) O(n logn)path(h, d) := ?
!m : flow(h,m, d) O(n2) O(n logn) Paths and flows are consistentpath(0,m) := TRUE, flow(h,m,m) := TRUE (see Martins et al (2010))All siblings sibl(h,m, s) := arc(h,m) ?
arc(h, s) O(n3) O(1) By definitionGrandp.
grand(g, h,m) := arc(g, h) ?
arc(h,m) O(n3) O(1) By definitionHead Bigram bigram(b, h,m) := arc(b,m?
1) ?
arc(h,m), m 6= 0 O(n3) O(1) By definitionConsec.
Sibl.
lastsibl(h,m,m) := arc(h,m)?
!m ?
[h, k] : lastsibl(h,m, k) O(n2) O(n logn) Head automaton modellastsibl(h,m, k) := lastsibl(h,m, k + 1) (see supplementary material)?
nextsibl(h,m, k + 1) O(n3) O(1)arc(h,m) := ?
!s ?
[h,m] : nextsibl(h, s,m) O(n2) O(n logn)Nonproj.
Arc nonproj(h,m) := arc(h,m) ?
?k ?
[h,m] : ?path(h, k) O(n2) O(n logn) By definitionTable 1: First-order logic formulae underlying our dependency parser.
The basic parts are the predicate variablesarc(h,m) (indicating an arc linking head h to modifier m), path(a, d) (indicating a directed path from ancestora to descendant d), nextsibl(h,m, s) (indicating that ?h,m?
and ?h, s?
are consecutive siblings), nonproj(h,m)(indicating that ?h,m?
is a non-projective arc), as well as the auxiliary variables flow(h,m, d) (indicating that arc?h,m?
carries flow to d), and lastsibl(h,m, k) (indicating that, up to position k, the last seen modifier of h occurredat position m).
The non-basic parts are the pairwise factors sibl(h,m, s), grand(g, h,m), and bigram(b, h,m); aswell as each logical formula.
Columns 3?4 indicate the number of parts of each kind, and the time complexity forsolving each subproblem.
For a sentence of length n, there are O(n3) parts and the total complexity is O(n3 log n).
?ni=1 zi}.
The convex hull of YOR-OUT is the follow-ing set: ZOR-OUT = {?z0, .
.
.
, zn?
?
[0, 1]n | z0 ?
?ni=1 zi, z0 ?
zi, ?i = 1, .
.
.
, n} (Martins et al,2010, Tab.1).
The slave subproblem is:minimize 12?ni=0(zi ?
ai)2w.r.t.
?z0, .
.
.
, zn?
?
[0, 1]ns.t.
z0 ?
?ni=1 zi; z0 ?
zi, ?i = 1, .
.
.
, n.(21)The problem in Eq.
21 is more involved than theones in Eqs.
19?20.
Yet, there is still an efficientprocedure with runtime O(n log n) (see App.
E).By using the result above for negated variables, weare now endowed with a procedure for many othercases, such that AND-WITH-OUTPUT and formu-las with universal quantifiers (e.g., R(x) := ?y :Q(x, y)).
Up to a log-factor, the runtimes will belinear in the number of predicates.Larger Slaves.
The only disadvantage of DD-ADMM in comparison with the subgradient algo-rithm is that there is not an obvious way of solvingthe subproblem in Eq.
14 exactly for large combi-natorial factors, such as the TREE constraint in de-pendency parsing, or a sequence model.
Hence, ourmethod seems to be more suitable for decomposi-tions which involve ?simple slaves,?
even if theirnumber is large.
However, this does not rule out thepossibility of using this method otherwise.
Ecksteinand Bertsekas (1992) show that the ADMM algo-rithm may still converge when the z-updates are in-exact.
Hence the method may still work if the slavesare solved numerically up to some accuracy.
We de-fer this to future investigation.5 Experiments: Dependency ParsingWe used 14 datasets with non-projective depen-dencies from the CoNLL-2006 and CoNLL-2008shared tasks (Buchholz and Marsi, 2006; Surdeanuet al, 2008).
We also used a projective Englishdataset derived from the Penn Treebank by applyingthe standard head rules of Yamada and Matsumoto(2003).11 We did not force the parser to output pro-jective trees or unique roots for any of the datasets;everything is learned from the data.
We trained byrunning 10 iterations of the cost-augmented MIRAalgorithm (Crammer et al, 2006) with LP-relaxeddecoding, as in Martins et al (2009b).
Follow-ing common practice (Charniak and Johnson, 2005;Carreras et al, 2008), we employed a coarse-to-fineprocedure to prune away unlikely candidate arcs, asdescribed by Koo and Collins (2010).
To ensurevalid parse trees at test time, we rounded fractional11As usual, we train on sections ?02?21, use ?22 as validationdata, and test on ?23.
We ran SVMTool (Gime?nez and Marquez,2004) to obtain automatic part-of-speech tags for ?22?23.244solutions as described in Martins et al (2009a) (yet,solutions were integral most of the time).The parts used in our full model are the onesdepicted in Fig.
1.
Note that a subgradient-basedmethod could handle some of those parts efficiently(arcs, consecutive siblings, grandparents, and headbigrams) by composing arc-factored models, headautomata, and a sequence labeler.
However, nolightweight decomposition seems possible for incor-porating parts for all siblings, directed paths, andnon-projective arcs.
Tab.
1 shows the first-orderlogical formulae that encode the constraints in ourmodel.
Each formula gives rise to a subproblemwhich is efficiently solvable (see ?4).
By ablatingsome of rows of Tab.
1 we recover known methods:?
Resorting to the tree and consecutive sibling for-mulae gives one of the models in Koo et al(2010), with the same linear relaxation (a proofof this fact is included in App.
F);?
Resorting to tree, all siblings, grandparent, andnon-projective arcs, recovers a multi-commodityflow configuration proposed by Martins et al(2009a); the relaxation is also the same.12The experimental results are shown in Tab.
2.For comparison, we include the best published re-sults for each dataset (at the best of our knowledge),among transition-based parsers (Nivre et al, 2006;Huang and Sagae, 2010), graph-based parsers (Mc-Donald et al, 2006; Koo and Collins, 2010), hybridmethods (Nivre and McDonald, 2008; Martins et al,2008), and turbo parsers (Martins et al, 2010; Kooet al, 2010).
Our full model achieved the best re-ported scores for 7 datasets.
The last two columnsshow a consistent improvement (with the exceptionsof Chinese and Arabic) when using the full set offeatures over a second order model with grandparentand consecutive siblings, which is our reproductionof the model of Koo et al (2010).1312Although Martins et al (2009a) also incorporated consec-utive siblings in one of their configurations, our constraints aretighter than theirs.
See App.
F.13Note however that the actual results of Koo et al (2010)are higher than our reproduction, as can be seen in the secondcolumn.
The differences are due to the features that were usedand on the way the models were trained.
The cause is not searcherror: exact decoding with an ILP solver (CPLEX) revealed nosignificant difference with respect to our G+CS column.
Weleave further analysis for future work.Best known UAS G+CS FullArabic 80.18 [Ma08] 81.12 81.10 (-0.02)Bulgar.
92.88 [Ma10] 93.04 93.50 (+0.46)Chinese 91.89 [Ma10] 91.05 90.62 (-0.43)Czech 88.78 [Ma10] 88.80 89.46 (+0.66)English 92.57 [Ko10] 92.45 92.68 (+0.23)Danish 91.78 [Ko10] 91.70 91.86 (+0.16)Dutch 85.81 [Ko10] 84.77 85.53 (+0.76)German 91.49 [Ma10] 91.29 91.89 (+0.60)Japane.
93.42 [Ma10] 93.62 93.72 (+0.10)Portug.
93.03 [Ko10] 92.05 92.29 (+0.24)Slovene 86.21 [Ko10] 86.09 86.95 (+0.86)Spanish 87.04 [Ma10] 85.99 86.74 (+0.75)Swedish 91.36 [Ko10] 89.94 90.16 (+0.22)Turkish 77.55 [Ko10] 76.24 76.64 (+0.40)PTB ?23 93.04 [KC10] 92.19 92.53 (+0.34)Table 2: Unlabeled attachment scores, excluding punc-tuation.
In the second column, [Ma08] denotes Martinset al (2008), [KC10] is Koo and Collins (2010), [Ma10]is Martins et al (2010), and [Ko10] is Koo et al (2010).In columns 3?4, ?Full?
is our full model, and ?G+CS?
isour reproduction of the model of Koo et al (2010), i.e.,the same as ?Full?
but with all features ablated exceptedfor grandparents and consecutive siblings.AF +G+CS +AS +NP FullPTB ?22 91.02 92.13 92.32 92.36 92.41PTB ?23 91.36 92.19 92.41 92.50 92.53Table 3: Feature ablation experiments.
AF is an arc-factored model; +G+CS adds grandparent and consec-utive siblings; +AS adds all-siblings; +NP adds non-projective arcs; Full adds the bigram and directed paths.Feature ablation and error analysis.
We con-ducted a simple ablation study by training severalmodels on the English PTB with different sets offeatures.
Tab.
3 shows the results.
As expected, per-formance keeps increasing as we use models withgreater expressive power.
We show some concreteexamples in App.
G of sentences that the full modelparsed correctly, unlike less expressive models.Convergence speed and optimality.
Fig.
2 com-pares the performance of DD-ADMM and the sub-gradient algorithms in the validation section of thePTB.14 For the second order model, the subgradient14The learning rate in the subgradient method was set as ?t =?0/(1+Nincr(t)), as in Koo et al (2010), whereNincr(t) is thenumber of dual increases up to the tth iteration, and ?0 is chosento maximize dual decrease after 20 iterations (in a per sentencebasis).
Those preliminary iterations are not plotted in Fig.
2.245method has more slaves than in Koo et al (2010):it has a slave imposing the TREE constraint (whosesubproblems consists on finding a minimum span-ning tree) and several for the all-sibling parts, yield-ing an average number of 310.5 and a maximumof 4310 slaves.
These numbers are still manage-able, and we observe that a ?good?
UAS is achievedrelatively quickly.
The ADMM method has manymore slaves due to the multicommodity flow con-straints (average 1870.8, maximum 65446), yet itattains optimality sooner, as can be observed in theright plot.
For the full model, the subgradient-basedmethod becomes extremely slow, and the UAS scoreseverely degrades (after 1000 iterations it is 2%less than the one obtained with the ADMM-basedmethod, with very few instances having been solvedto optimality).
The reason is the number of slaves:in this configuration and dataset the average numberof slaves per instance is 3327.4, and the largest num-ber is 113207.
On the contrary, the ADMM methodkeeps a robust performance, with a large fraction ofoptimality certificates in early iterations.Runtime and caching strategies.
Despite its suit-ability to problems with many overlapping compo-nents, our parser is still 1.6 times slower than Kooet al (2010) (0.34 against 0.21 sec./sent.
in PTB?23), and is far beyond the speed of transition-basedparsers (e.g., Huang and Sagae (2010) take 0.04sec./sent.
on the same data, although accuracy islower, 92.1%).
Our implementation, however, is notfully optimized.
We next describe how considerablespeed-ups are achieved by caching the subproblems,following a strategy similar to Koo et al (2010).Fig.
3 illustrates the point.
After a few iterations,many variables u(r) see a consensus being achieved(i.e., ut(r) = zt+1s (r),?s) and enter an idle state:they are left unchanged by the u-update in Eq.
9,and so do the Lagrange variables ?t+1s (r) (Eq.
10).If by iteration t all variables in a subproblem s areidle, then zt+1s (r) = zts(r), hence the subproblemdoes not need to be resolved.15 Fig.
3 shows that15Even if not all variables are idle in s, caching may still beuseful: note that the z-updates in Eq.
14 tend to be sparse for thesubproblems described in ?4 (these are Euclidean projectionsonto polytopes with 0/1 vertices, which tend to hit corners).
An-other trick that may accelerate the algorithm is warm-starting:since many subproblems involve a sort operation, storing thesorted indexes may speedup the next round.200 400 600 800 1000Iterations020406080100% activeFull ADMM% active msgs% active subproblems% active varsFigure 3: Fraction of active variables, subproblems andmessages along DD-ADMM iterations (full model).
Thenumber of active messages denotes the total number ofvariables (active or not) that participate in an active factor.10-3 10-2 10-1 100 101Time ADMM (sec.
)10-310-210-1100101TimeCPLEX (sec.
)Elapsed TimesFigure 4: Runtimes of DD-ADMM and CPLEX on PTB?22 (each point is a sentence).
Average runtimes are0.362 (DD-ADMM) and 0.565 sec./sent.
(CPLEX).many variables and subproblems are left untouchedafter the first few rounds.Finally, Fig.
4 compares the runtimes of our im-plementation of DD-ADMM with those achieved bya state-of-the-art LP solver, CPLEX, in its best per-forming configuration: the simplex algorithm ap-plied to the dual LP.
We observe that DD-ADMMis faster in some regimes but slower in others.
Forshort sentences (< 15 words), DD-ADMM tends tobe faster.
For longer sentences, CPLEX is quite ef-fective as it uses good heuristics for the pivot stepsin the simplex algorithm; however, we observed thatit sometimes gets trapped on large problems.
Notealso that DD-ADMM is not fully optimized, and thatit is much more amenable to parallelization than thesimplex algorithm, since it is composed of many in-dependent slaves.
This suggests potentially signifi-cant speed-ups in multi-core environments.6 Related WorkRiedel and Clarke (2006) first formulated depen-dency parsing as an integer program, along withlogical constraints.
The multicommodity flow for-2460 200 400 600 800 1000Iterations8586878889909192UAS(%)AccuracyADMM FullSubgrad FullADMM Sec OrdSubgrad Sec Ord0 200 400 600 800 1000Iterations020406080100Certificates (%)Stopping CriteriaADMM Full (Tol<0.001)ADMM Full (Exact)Subgrad Full (Exact)ADMM Sec Ord (Tol<0.001)ADMM Sec Ord (Exact)Subgrad Sec Ord (Exact)Figure 2: UAS including punctuation (left) and fraction of optimality certificates (right) accross iterations of thesubgradient and DD-ADMM algorithms, in PTB ?22.
?Full?
is our full model; ?Sec Ord?
is a second-order modelwith grandparents and all siblings, for which the subgradient method uses a coarser decomposition with a TREE factor.Since subgradient and DD-ADMM are solving the same problems, the solid lines (as the dashed ones) would meet inthe limit, however subgradient converges very slowly for the full model.
The right plot shows optimality certificatesfor both methods, indicating that an exact solution of P has been found; for DD-ADMM we also plot the fraction ofinstances that converged to an accurate solution of P ?
(primal and dual residuals < 10?3) and hence can be stopped.mulation was introduced by Martins et al (2009a),along with some of the parts considered here.
Kooet al (2010) proposed a subgradient-based dual de-composition method that elegantly combines headautomata with maximum spanning tree algorithms;these parsers, as well as the loopy belief propagationmethod of Smith and Eisner (2008), are all instancesof turbo parsers (Martins et al, 2010).DD-ADMM has been proposed and theoreticallyanalyzed by Martins et al (2011) for problems rep-resentable as factor graphs.
The general ADMMmethod has a long-standing history in optimization(Hestenes, 1969; Powell, 1969; Glowinski and Mar-roco, 1975; Gabay and Mercier, 1976; Boyd et al,2011).
Other methods have been recently proposedto accelerate dual decomposition, such as Jojic et al(2010) and Meshi and Globerson (2011) (the latterapplying ADMM in the dual rather than the primal).While our paper shows limitations of the sub-gradient method when there are many overlappingcomponents, this method may still be advantageousover ADMM in problems that are nicely decom-posable, since it often allows reusing existing com-binatorial machinery.
Yet, the scenario we con-sider here is realistic in NLP, where we often haveto deal with not-lightly-decomposable constrainedproblems (e.g., exploiting linguistic knowledge).7 ConclusionWe have introduced new feature-rich turbo parsers.Since exact decoding is intractable, we solve an LPrelaxation through a recently proposed consensus al-gorithm, DD-ADMM, which is suitable for prob-lems with many overlapping components.
We studythe empirical runtime and convergence properties ofDD-ADMM, complementing the theoretical treat-ment in Martins et al (2011).
DD-ADMM com-pares favourably against the subgradient method inseveral aspects: it is faster to reach a consensus, ithas better stopping conditions, and it works betterin non-lightweight decompositions.
While its slavesubproblems are more involved, we derived closed-form solutions for many cases of interest, such asfirst-order logic formulas and combinatorial factors.DD-ADMM may be useful in other frameworksinvolving logical constraints, such as the modelsfor compositional semantics presented by Lianget al (2011).
Non-logical constraints may alsoyield efficient subproblems, e.g., the length con-straints in summarization and compression (Clarkeand Lapata, 2008; Martins and Smith, 2009; Berg-Kirkpatrick et al, 2011).
Finally, DD-ADMM canbe adapted to tighten its relaxations towards exactdecoding, as in Sontag et al (2008) and Rush andCollins (2011).
We defer this for future work.AcknowledgmentsWe thank all reviewers for their comments, Eric Xing forhelpful discussions, and Terry Koo and Sasha Rush foranswering questions about their parser and for providingcode.
A. M. was supported by a FCT/ICTI grant throughthe CMU-Portugal Program, and by Priberam.
Thiswork was partially supported by the FET programme(EU FP7), under the SIMBAD project (contract 213250).N.
S. was supported by NSF CAREER IIS-1054319.247ReferencesM.
Auli and A. Lopez.
2011.
A Comparison of LoopyBelief Propagation and Dual Decomposition for Inte-grated CCG Supertagging and Parsing.
In Proc.
ofACL.Y.
Bar-Hillel, M. Perles, and E. Shamir.
1964.
On for-mal properties of simple phrase structure grammars.Language and Information: Selected Essays on theirTheory and Application, pages 116?150.Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProc.
of ACL.D.
Bertsekas, W. Hager, and O. Mangasarian.
1999.Nonlinear programming.
Athena Scientific.D.P.
Bertsekas, A. Nedic, and A.E.
Ozdaglar.
2003.
Con-vex analysis and optimization.
Athena Scientific.S.
Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein.2011.
Distributed Optimization and Statistical Learn-ing via the Alternating Direction Method of Multipli-ers.
Now Publishers (to appear).J.P.
Boyle and R.L.
Dykstra.
1986.
A method for find-ing projections onto the intersections of convex sets inHilbert spaces.
In Advances in order restricted statis-tical inference, pages 28?47.
Springer Verlag.S.
Buchholz and E. Marsi.
2006.
CoNLL-X shared taskon multilingual dependency parsing.
In CoNLL.X.
Carreras, M. Collins, and T. Koo.
2008.
TAG, Dy-namic Programming, and the Perceptron for Efficient,Feature-rich Parsing.
In CONLL.X.
Carreras.
2007.
Experiments with a higher-order pro-jective dependency parser.
In CoNLL.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.
InProc.
ACL, pages 173?180.
Association for Computa-tional Linguistics Morristown, NJ, USA.D.
Chiang.
2007.
Hierarchical phrase-based translation.computational linguistics, 33(2):201?228.J.
Clarke and M. Lapata.
2008.
Global Inference for Sen-tence Compression An Integer Linear ProgrammingApproach.
JAIR, 31:399?429.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, andY.
Singer.
2006.
Online Passive-Aggressive Algo-rithms.
JMLR, 7:551?585.J.
Duchi, S. Shalev-Shwartz, Y.
Singer, and T. Chandra.2008.
Efficient projections onto the L1-ball for learn-ing in high dimensions.
In ICML.J.
Eckstein and D. Bertsekas.
1992.
On the Douglas-Rachford splitting method and the proximal point al-gorithm for maximal monotone operators.
Mathemat-ical Programming, 55(1):293?318.J.
R. Finkel, A. Kleeman, and C. D. Manning.
2008.
Effi-cient, feature-based, conditional random field parsing.Proceedings of ACL-08: HLT, pages 959?967.D.
Gabay and B. Mercier.
1976.
A dual algorithm forthe solution of nonlinear variational problems via finiteelement approximation.
Computers and Mathematicswith Applications, 2(1):17?40.J.
Gime?nez and L. Marquez.
2004.
Svmtool: A gen-eral pos tagger generator based on support vector ma-chines.
In Proc.
of LREC.R.
Glowinski and P. Le Tallec.
1989.
Augmented La-grangian and operator-splitting methods in nonlinearmechanics.
Society for Industrial Mathematics.R.
Glowinski and A. Marroco.
1975.
Surl?approximation, par e?le?ments finis d?ordre un, et lare?solution, par penalisation-dualite?, d?une classe deproble`mes de Dirichlet non line?aires.
Rev.
Franc.
Au-tomat.
Inform.
Rech.
Operat., 9:41?76.M.
Hestenes.
1969.
Multiplier and gradient methods.Jour.
Optim.
Theory and Applic., 4:302?320.L.
Huang and K. Sagae.
2010.
Dynamic programmingfor linear-time incremental parsing.
In Proc.
of ACL,pages 1077?1086.R.
Johansson and P. Nugues.
2008.
Dependency-basedSemantic Role Labeling of PropBank.
In EMNLP.V.
Jojic, S. Gould, and D. Koller.
2010.
Accelerated dualdecomposition for MAP inference.
In ICML.N.
Komodakis, N. Paragios, and G. Tziritas.
2007.MRF optimization via dual decomposition: Message-passing revisited.
In ICCV.T.
Koo and M. Collins.
2010.
Efficient third-order de-pendency parsers.
In Proc.
of ACL, pages 1?11.T.
Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-tag.
2010.
Dual decomposition for parsing with non-projective head automata.
In EMNLP.A.
Kulesza and F. Pereira.
2007.
Structured Learningwith Approximate Inference.
NIPS.P.
Liang, M.I.
Jordan, and D. Klein.
2011.
Learningdependency-based compositional semantics.
In Proc.Association for Computational Linguistics (ACL).A.
F. T. Martins and N. A. Smith.
2009.
Summarizationwith a joint model for sentence extraction and com-pression.
In NAACL-HLT Workshop on Integer LinearProgramming for NLP.A.
F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.2008.
Stacking dependency parsers.
In EMNLP.A.
F. T. Martins, N. A. Smith, and E. P. Xing.
2009a.Concise integer linear programming formulations fordependency parsing.
In ACL-IJCNLP.A.
F. T. Martins, N. A. Smith, and E. P. Xing.
2009b.Polyhedral outer approximations with application tonatural language parsing.
In ICML.A.
F. T. Martins, N. A. Smith, E. P. Xing, M. A. T.Figueiredo, and P. M. Q. Aguiar.
2010.
Turbo parsers:Dependency parsing by approximate variational infer-ence.
In EMNLP.248A.
F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,N.
A. Smith, and E. P. Xing.
2011.
An AugmentedLagrangian Approach to Constrained MAP Inference.In ICML.R.
McDonald, K. Lerman, and F. Pereira.
2006.
Multi-lingual dependency analysis with a two-stage discrim-inative parser.
In CoNLL.O.
Meshi and A. Globerson.
2011.
An Alternating Direc-tion Method for Dual MAP LP Relaxation.
In ECMLPKDD.J.
Nivre and R. McDonald.
2008.
Integrating graph-based and transition-based dependency parsers.
InACL-HLT.J.
Nivre, J.
Hall, J. Nilsson, G. Eryig?it, and S. Marinov.2006.
Labeled pseudo-projective dependency parsingwith support vector machines.
In Procs.
of CoNLL.S.
Petrov and D. Klein.
2008.
Sparse multi-scale gram-mars for discriminative latent variable parsing.
InProc.
of EMNLP.M.
Powell.
1969.
A method for nonlinear constraints inminimization problems.
In R. Fletcher, editor, Opti-mization, pages 283?298.
Academic Press.M.
Richardson and P. Domingos.
2006.
Markov logicnetworks.
Machine Learning, 62(1):107?136.S.
Riedel and J. Clarke.
2006.
Incremental integer linearprogramming for non-projective dependency parsing.In EMNLP.A.
M. Rush and M. Collins.
2011.
Exact decoding ofsyntactic translation models through lagrangian relax-ation.
In ACL.A.
Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010.On dual decomposition and linear programming relax-ations for natural language processing.
In EMNLP.N.
Shor.
1985.
Minimization methods for non-differentiable functions.
Springer.D.
Smith and J. Eisner.
2008.
Dependency parsing bybelief propagation.
In EMNLP.D.
Sontag, T. Meltzer, A. Globerson, Y. Weiss, andT Jaakkola.
2008.
Tightening LP relaxations for MAPusing message-passing.
In UAI.M.
Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, andJ.
Nivre.
2008.
The CoNLL-2008 shared task onjoint parsing of syntactic and semantic dependencies.CoNLL.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-marginMarkov networks.
In NIPS.R.W.
Tromble and J. Eisner.
2006.
A fast finite-staterelaxation method for enforcing global constraints onsequence decoding.
In Proc.
of NAACL, pages 423?430.M.
Wainwright and M. Jordan.
2008.
Graphical Models,Exponential Families, and Variational Inference.
NowPublishers.H.
Yamada and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
InIWPT.249
