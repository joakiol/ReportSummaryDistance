OVERVIEW OF THE SECOND TEXT RETRIEVALCONFERENCE (TREC-2)Donna HarmanNat ional  Inst itute o f  Standards and Techno logyGaithersburg,  MD.
208991.
INTRODUCTIONIn November of 1992 the first Text REtrieval Conference(TREC-1) was held at NIST (Harman 1993).
This confer-ence, co-sponsored by ARPA and NIST, brought ogetherinformation retrieval researchers to discuss their systemresults on the new TIPSTER test collection.
This was thefirst time that such groups had ever compared results onthe same data using the same evaluation methods, andrepresented a breakthrough in cross-system evaluation ininformation retrieval.
It was also the first time that mostof these groups had tackled such a large test collectionand required a major effort by all groups to scale up theirretrieval techniques.Since TREC is designed to evaluate system performanceboth in a routing (filtering or profiling) mode, and in anadhoc mode, both functions were tested.
The test designwas based on traditional information retrieval models, in-volving documents, "user" questions, and the "right an-swers" (Harman 1994a).
Participants were first sent twodisks of documents (about 2 gigabytes of data) and atraining set of 100 questions or topics.
They were alsosent lists of documents in the two disks that were consid-ered the "right answers" or relevant documents for each ofthe 100 topics.
The participants were asked to train theirsystems on this data, and at some point to signal theirreadiness for testing by submitting their system queriesfor a specific fifty of the topics.
The routing test consistedof each group running new test documents against hose50 queries.
The adhoc test consisted of running a new setof 50 topics against he old document set (the original 2disks).
In each case, the results of the retrieval systemswere submitted to NIST for evaluation.The documents in the test collection are from varioustypes of text, covering different writing styles and differ-ent information domains.
They include information fromthe Wall Street Journal, the San Jose Mercury News, theAP Newswire, and artcles from the Computer Selectdisks.
The documents were uniformly formatted into anSGML-Iike structure for easy handling by the TREC par-ticipants.351The topics used in the test collection are in the form of"user need" statements rather than more traditionalqueries.
They are designed to mimic a real user's need,and were written by people who are actual users of a re-trieval system.
Although the subject domain of the topicsis diverse, some consideration was given to the documentsto be searched.The relevance judgments or "right answers" were madeusing a sampling method, with the sample constructed bytaking the top 100 documents retrieved by each participat-ing system for a given topic and merging them into a poolfor manual relevance assessment.
This is a valid samplingmethod since all the systems used ranked retrieval meth-ods, with those documents most likely to be relevant re-turned first.
All systems were then evaluated against hecommon set of relevant documents, i.e.
the total numberof relevant documents found by all the systems combined.How well did the systems do with this test collection?Whereas the TREC-1 conference demonstrated a widerange of different approaches to the retrieval of text fromlarge document collections, the results could be viewedonly as very preliminary.
Not only were the deadlines forresults were very tight, but the huge scale-up in the size ofthe document collection required major work from allgroups in rebuilding their systems.
Much of this workwas simply a system engineering task: finding reasonabledata structures to use, getting indexing routines to be effi-cient enough to finish indexing the data, finding enoughstorage to handle the large inverted files and other struc-tures, etc.
Still, the results showed that the systems didthe task well, and that automatic onstruction of queriesfrom the topics did as well as, or better than, manual con-struction of queries.The second TREC conference (TREC-2) occurred in Au-gust of 1993, less than 10 months after the first confer-ence.
In addition to most of the TREC-1 groups, ninenew groups took part, bringing the total number of partici-pating groups to 31.Advanced Decision SystemsCarnegie Mellon UniversityCity University, LondonCornell UniversityEnvironment Research Institute of MichiganHNC Inc.Mead Data CentralPRC, Inc.Rutgers UniversitySwiss Federal Institute of Technology (ETH)Systems Environment CorporationTRW Systems Development DivisionUniversity of California - BerkeleyUniversity of Central FloridaUniversity of Massachusetts at AmherstVerity Inc.BellcoreCITRI, AustraliaConquests Inc.Dalhousie UniversityGE Research and Development CenterInstitute for Decision Systems ResearchNew York UniversityQueens CollegeSiemens Corporate Research Inc.Syracuse UniversityThinking Machines CorporationUniversitaet Dortmund, GermanyUniversity of California - UCLAUniversity of Illinois at ChicagoVPI&SU (Virginia Tech)Table 1:TREC-2 Participants (14 companies, 17 universities)2.
TREC-2  RESULTS2.1 IntroductionIn general the TREC-2 results showed significant im-provements over the TREC-1 results.
Many of the origi-nal TREC-1 groups were able to "complete" their systemrebuilding and tuning tasks.
The results for TREC-2therefore can be viewed as the "best first-pass" that mostgroups can accomplish on this large amount of data.
Theadhoc results in particular epresent baseline results fromthe scaling-up of current algorithms to large test collec-tions.
The better systems produced similar results, resultsthat are comparable to those seen using these algorithmson smaller test collections.The routing results showed even more improvement overTREC-1 routing results.
Some of this improvement wasdue to the availability of large numbers of accurate rele-vance judgments for training (unlike TREC-1), but mostof the improvements came from new research by partici-pating groups into the best ways of using the training da-ta.All references in this section are papers in the TREC-2proceedings (Harman 1994b).2.2 Adhoc ResultsThe adhoc evaluation used new topics (101-150) againstthe two disks of training documcnts (disks 1 and 2).There were 44 sets of results for adhoc evaluation inTREC-2, with 32 of them based on runs for the full dataset.
Of these, 23 used automatic construction of qucrics,9 used manual construction, and 2 used feedback.Figure 1 shows the recall/precision curves for the six352TREC-2 groups with the highest non-interpolated averageprecision using automatic construction of queries.
The re-sults marked "INQ001" are the INQUERY system fromthe University of Massachusetts ( ee Croft, Callan &Broglio paper).
This system uses probabilistic termweighting and a probabilistic inference net to combinevarious topic and document features.
The results marked"dortQ2", "Brkly3" and "cmlL2" are all based on the useof the Cornell SMART system, but with important varia-tions.
The "crnlL2" run is the basic SMART system fromComell University (see Buckley, Allan & Salton paper),but using less than optimal term weightings (by mistake).The "dortQ2" results from the University of Dortmundcome from using polynomial regression on the trainingdata to find weights for various pre-set erm features (seeFuhr, Pfeifer, Bremkamp, Pollmann & Buckley paper).The "Brkly3" results from the University of California tBerkeley come from performing logistic regression analy-sis to learn optimal weighting for various term frequencymeasures (see Cooper, Chen& Gey paper).
The "CLAR-TA" system from the CLARIT Corporation expands eachtopic with noun phrases found in a thesaurus that is auto-matically generated for each topic (see Evans & Leffertspaper).
The "Isiasm" results are from Bellcore (see Du-mais paper).
This group uses latent semantic indexing tocreate much larger vectors than the more traditional vec-tor-space models uch as SMART.
The run marked "lsi-asm" represents only the base SMART pre-processing re-sults, however.
Due to processing errors the "improved"LSI run produced unexpectedly poor results.Figure 2 shows the recall/precision curve for the sixTREC-2 groups with the highest non-interpolated averageprecision using manual construction of queries.
It shouldbe noted that varying amounts of manual interventionwere used.
The results marked "INQ002", "siems2", and"CLARTM" are automatically-generated queries withmanual modifications.
The "INQ002" results reflect vari-ous manual modifications made to the "INQ001" queries,with those modifications guided by strict rules.
The"siems2" results from Siemens Corporate Research, Inc.(see Voorhees paper) are based on the use of the ComellSMART system, but with the topics manually modified(the "not" phases removed).
These results were meant obe the base run for improvements u ing WordNet, but theimprovements did not materialize.
The "CLARTM" re-suits represent manual weighting of the query terms, asopposed to the automatic weighting of the terms that wasused in "CLARTA".
The results marked "Vtcms2", "Cn-Qst2", and "TOPIC2" are produced from queries con-structed completely manually.
The "Vtcms2" results arefrom Virginia Tech (see Fox & Shaw paper) and show theeffects of combining the results from SMART vector-space queries with the results from manually-constructedsoft Boolean P-Norm type queries.
The "CnQst2" results,from ConQuest Software (see Nelson paper), use a verylarge general-purpose mantic net to aid in constructingbetter queries from the topics, along with sophisticatedmorphological nalysis of the topics.
The results marked"TOPIC2" are from the TOPIC system by Verity Corp.(see Lehman & Reid paper) and reflect he use of an ex-pert system working off specially-constructed knowledgebases to improve performance.Several comments can be made with respect o these ad-hoc results.
First, the better esults (most of the automaticresults and the three top manual results) are very similarand it is unlikely that there is any statistical differencesbetween them.
There is clearly no "best" method, and thefact that these systems have very different approaches toretrieval, including different term weighting schemes, dif-ferent query construction methods, and different similaritymatch methods implies that there is much more to belearned about effective retrieval techniques.
Additionally,whereas the averages for the systems may be similar, thesystems do better on different topics and retrieve differentsubsets of the relevant documents.A second point that should be made is that the automaticquery construction methods continue to perform as wellas the manual construction methods.
Two groups (the IN-QUERY system and the CLARIT system) did explicitcomparision of manually-modified queries vs those thatwere not modified and concluded that manual modifica-tion provided no benefits.
The three sets of results basedon completely manually-generated queries had even poor-er performance than the manually-modified queries.
Notethat this result is specific to the very rich TREC topics; itis not clear that this will hold for the short opics normallyseen in other etrieval environments.As a final point, it should be noted that these adhoc results353represent significant improvements over the results fromTREC-1.
Figure 5 (after the routing results) shows acomparison of results for a typical system in TREC-1 andTREC-2.
Some of this improvement is due to improvedevaluation, but the difference between the curve marked"TREC-I" and the curve marked "TREC-2 looking at top200 only" shows significant performance improvement.Whereas this improvement could represent a difference intopics (the TREC-1 curve is for topics 51-100 and theTREC-2 curves are for topics 101-150), the TREC-2 top-ics are generally felt to be more difficult and therefore thisimprovement is likely to be an understatement of he actu-al improvements.Very few groups worked with less than the full documentcollection.
The system from New York University (seeStrzalkowski & Carballo paper) reflects a very intensiveuse of natural language processing (NLP) techniques, in-cluding a parse of the documents o help locate syntacticphrases, context-sensitive expansion of the queries, andother NLP improvements on statistical techniques.
In in-terests of space this graph is not shown; please refer to thepaper by this group in this proceedings.2.3 Routing ResultsThe routing evaluation used a subset of the training topics(topics 51-100 were used) against he new disk of testdocuments (disk 3).
There were 40 sets of results forrouting evaluation, with 32 of them based on runs for thefull data set.
Of the 32 systems using the full data set, 23used automatic construction of queries, and 9 used manu-al construction.Figure 3 shows the recall/precision curves for the sixTREC-2 groups with the highest non-interpolated averageprecision using automatic onstruction of the routingqueries.
Again three systems are based on the CornellSMART system.
The plot marked "crnlCl" is the actualSMART system, using the basic Rocchio relevance feed-back algorithms, and adding many terms (up to 500) fromthe relevant training documents o the terms in the topic.The "dortPl" results come from using a probabilistically-based relevance feedback instead of the vector-space algo-rithm, and adding only 20 terms from the relevant docu-ments to each query.
These two systems have the bestrouting results.
The "Brkly5" system uses logistic regres-sion on both the general frequency variables used in theiradhoc approach and on the query-specific relevance dataavailable for training with the routing topics.
The resultsmarked "cityr2" are from City University, London (seeRobertson, Walker, Jones, Hancock-Beaulieu & Gaffordpaper).
This group automatically selected variable num-bers of terms (10-25) from the training documents foreach topic (the topics themselves were not used as termsources), and then used traditional probabilistic reweight-ing to weight hese terms.
The "INQ003" results also useprobabilistic reweighting, but use the topic terms, expand-ed by 30 new terms per topic from the training docu-ments.
The results marked "lsir2" are more latent seman-tic indexing results from Belicore.
This run was made bycreating a filter of the singular-value decomposition vec-tor sum or centroid of all relevant documents for a topic(and ignoring the topic itself).Figure: 4 shows the recall/precision curves for the sixTREC-2 groups with the highest non-interpolated averageprecision using manual construction of the routingqueries.
The results marked "INQ004" are from the IN-QRY system using an inferential combination of the"INQ003" queries and manually modified queries createdfrom the topic.
The "trw2" results represent an adaptationof the TRW Fast Data Finder pattern matching system toallow use of term weighting (see Mettler paper).
Thequeries were manually constructed and the term weight-ing was learned from the training data.
The "gecrdl" re-sults from GE Research and Development Center (see Ja-cobs paper) also come from manually-constructedqueries, but using a general-purpose lexicon and the train-ing data to suggest input to the Boolean pattern matcher.The results marked "CLARTM" are similar to the"CLARTM" adhoc results except hat the training docu-ments were used as the source for thesaurus building, asopposed to using the top set of retrieved ocuments.
The"rutcombx" results from Rutgers University (see Belkin,Kantor, Cool & Quatrain paper) come from combining 5sets of manually-generated Boolean queries to optimizeperformance for each topic.
The results marked "TOP-IC2" are from the TOPIC system and reflect he use of anexpert system working off specially-constructed knowl-edge bases to improve performance.As was the case with the adhoc topics, the automaticquery construction methods continue to perform as wellas, or in this case, better than the manual constructionmethods.
A comparision of the two INQRY runs illus-trates this point and shows that all six results with manu-ally-generated queries perform worse than the six runswith automatically-generated queries.
The availability ofthe training data allows an automatic tuning of the queriesthat would be difficult to duplicate manually without ex-tensive analysis.Unlike the adhoc results, there are two runs ("crnlCl" and"dortPl") that are clearly better than the others, with a sig-nificant difference between the "cmlCl" results and the"dortPl" results and also significant differences betweenthese results and the rest of the automatically-generatedquery results.
In particular the use of so many terms (upto 500) for query expansion by the Cornell group was oneof the most interesting findings in TREC-2 and representsa departure from past results (see Buckley, Allan, &354Salton paper for more on this).As a final point, it should be noted that the routing resultsalso represent significant improvements over the resultsfrom TREC-1.
Figure 6 shows a comparison of results fora typical system in TREC-1 and TREC-2.
Some of thisimprovement is due to improved evaluation, but the differ-ence between the curve marked "TREC-I" and the curvemarked "TREC-2 looking at top 200 only" shows signifi-cant performance improvement.
There is more im-provement for the routing results than for the adhoc re-suits due to better training data (mostly non-existent forTREC-1) and to major efforts by many groups in newrouting algorithm experiments.3.
SUMMARYThe TREC-2 conference demonstrated a wide range ofdifferent approaches to the retrieval of text from largedocument collections.
There was significant improvementin retrieval performance over that seen in TREC-1, espe-cially in the muting task.
The availability of largeamounts of training data for routing allowed extensive x-perimentation i  the best use of that data, and many dif-ferent approaches were tried in TREC-2.
The automaticconstruction of queries from the topics continued to do aswell as, or better than, manual construction of queries,and this is encouraging for groups supporting the use ofsimple natural anguage interfaces for retrieval systems.The conference itself continued to provide an open forumfor exchange of results, and the increased participation bycommercial groups will speed the transfer of TREC algo-rithms into readily-available software products.There is a TREC-3 planned for November 1994, withmost of the TREC-2 participants returning, and a currentroster of over 55 groups participating.4.
REFERENCESHarman D.
(Ed.).
(1993).
The First Text REtrieval Confer-ence (TREC-1).
National Institute of Standards and Tech-nology Special Publication 500-207, Gaithersburg, Md.20899.Harman D. (1994a).
Data Preparation.
In: Merchant R.(Ed.
).The Proceedings of the TIPSTER Text Program -Phase I. San Mateo, California: Morgan Kaufmann Pub-lishing Co., 1994.Harman D.
(Ed.).
(1994b).
The Second Text REtrievalConference (TREC-2).
National Institute of Standardsand Technology Special Publication 500-215, Gaithers-burg, Md.
20899.0.80Best  Automat ic  Adhoc0.600 .400 .200 .000 .00  0 .20  0 .40  0 .60  0 .80  1 .00Reca l l_~ INQ001 + dor tQ2 + Brk ly3+ CLARTA o c rn lL2  + l s iasm1.001.00Best  Manua l  Adhoc0.900 .800 .700 .600 .500 .400 .300 .200 .100 .000 .00  0 .20  0 .40  0 .60  0 .80  1 .00Reca l l+ INQ002 + s iems2 _~_ CLARTM\[\] V tcms2 o CnQst2  _~_TOPIC2F igure 1 -- Best  Automat ic  Adhoc  ResultsF igure 2 -- Best  Manua l  Adhoc  Resul ts3550.600 .800 .400 .200 .000 .00 0 .20  0 .40 0 .60  0 .80 1.00Reca l l_~_ cmlC  1 _~_ dor tP1  _~_ c i ty r2o INQ003 + Brk ly5  ~ ls i r21.00Best  Automat ic  Rout ing0 .80Best  Manua l  Rout ing0 .600 .400 .200 .000 .o0  0 .20  0 .40 0 .60  0 .80 1 .o0Reca l lINQ004 __~_ t rw2 ~ gecrd  1+ CLARTM+ rutcombx + TOP IC21.00Figure 3 -- Best Automatic Routing ResultsFigure 4 -- Best Manual Routing Results3560.80.60.40.200.8Per fo rmance  Improvements  in  AdhocTREC-  1- TREC-20.0 0.2 0.4 0.6 0.8 1.0Reca l l_,~__ TREC-2  look ing  at top 200 on lyPer fo rmance  Improvements  In  Rout ing0.60.40.20L0.0 0.2 0.4 0.6 0.8 1.0Reca l l__._ TREC-2  look ing  at top 200 on ly  TR  EC - 1?
TREC-2Figure 5 -- Typical Improvements in Adhoc ResultsFigure 6 -- Typical Improvements in Routing Results357
