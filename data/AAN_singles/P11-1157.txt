Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1566?1576,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsEffective Measures of Domain Similarity for ParsingBarbara PlankUniversity of GroningenThe Netherlandsb.plank@rug.nlGertjan van NoordUniversity of GroningenThe NetherlandsG.J.M.van.Noord@rug.nlAbstractIt is well known that parsing accuracy suf-fers when a model is applied to out-of-domaindata.
It is also known that the most benefi-cial data to parse a given domain is data thatmatches the domain (Sekine, 1997; Gildea,2001).
Hence, an important task is to selectappropriate domains.
However, most previ-ous work on domain adaptation relied on theimplicit assumption that domains are some-how given.
As more and more data becomesavailable, automatic ways to select data that isbeneficial for a new (unknown) target domainare becoming attractive.
This paper evaluatesvarious ways to automatically acquire relatedtraining data for a given test set.
The resultsshow that an unsupervised technique based ontopic models is effective ?
it outperforms ran-dom data selection on both languages exam-ined, English and Dutch.
Moreover, the tech-nique works better than manually assigned la-bels gathered from meta-data that is availablefor English.1 Introduction and MotivationPrevious research on domain adaptation has focusedon the task of adapting a system trained on one do-main, say newspaper text, to a particular new do-main, say biomedical data.
Usually, some amountof (labeled or unlabeled) data from the new domainwas given ?
which has been determined by a human.However, with the growth of the web, more andmore data is becoming available, where each doc-ument ?is potentially its own domain?
(McCloskyet al, 2010).
It is not straightforward to determinewhich data or model (in case we have several sourcedomain models) will perform best on a new (un-known) target domain.
Therefore, an important is-sue that arises is how to measure domain similar-ity, i.e.
whether we can find a simple yet effectivemethod to determine which model or data is mostbeneficial for an arbitrary piece of new text.
More-over, if we had such a measure, a related question iswhether it can tell us something more about what isactually meant by ?domain?.
So far, it was mostlyarbitrarily used to refer to some kind of coherentunit (related to topic, style or genre), e.g.
: newspa-per text, biomedical abstracts, questions, fiction.Most previous work on domain adaptation, for in-stance Hara et al (2005), McClosky et al (2006),Blitzer et al (2006), Daume?
III (2007), sidesteppedthis problem of automatic domain selection andadaptation.
For parsing, to our knowledge only onerecent study has started to examine this issue (Mc-Closky et al, 2010) ?
we will discuss their approachin Section 2.
Rather, an implicit assumption of all ofthese studies is that domains are given, i.e.
that theyare represented by the respective corpora.
Thus, acorpus has been considered a homogeneous unit.
Asmore data is becoming available, it is unlikely thatdomains will be ?given?.
Moreover, a given corpusmight not always be as homogeneous as originallythought (Webber, 2009; Lippincott et al, 2010).
Forinstance, recent work has shown that the well-knownPenn Treebank (PT) Wall Street Journal (WSJ) ac-tually contains a variety of genres, including letters,wit and short verse (Webber, 2009).In this study we take a different approach.
Ratherthan viewing a given corpus as a monolithic entity,1566we break it down to the article-level and disregardcorpora boundaries.
Given the resulting set of doc-uments (articles), we evaluate various ways to au-tomatically acquire related training data for a giventest set, to find answers to the following questions:?
Given a pool of data (a collection of articlesfrom unknown domains) and a test article, isthere a way to automatically select data that isrelevant for the new domain?
If so:?
Which similarity measure is good for parsing??
How does it compare to human-annotated data??
Is the measure also useful for other languagesand/or tasks?To this end, we evaluate measures of domain sim-ilarity and feature representations and their impacton dependency parsing accuracy.
Given a collectionof annotated articles, and a new article that we wantto parse, we want to select the most similar articlesto train the best parser for that new article.In the following, we will first compare automaticmeasures to human-annotated labels by examiningparsing performance within subdomains of the PennTreebank WSJ.
Then, we extend the experiments tothe domain adaptation scenario.
Experiments wereperformed on two languages: English and Dutch.The empirical results show that a simple measurebased on topic distributions is effective for both lan-guages and works well also for Part-of-Speech tag-ging.
As the approach is based on plain surface-level information (words) and it finds related data ina completely unsupervised fashion, it can be easilyapplied to other tasks or languages for which anno-tated (or automatically annotated) data is available.2 Related WorkThe work most related to ours is McClosky et al(2010).
They try to find the best combination ofsource models to parse data from a new domain,which is related to Plank and Sima?an (2008).
Inthe latter, unlabeled data was used to create sev-eral parsers by weighting trees in the WSJ accord-ing to their similarity to the subdomain.
McCloskyet al (2010) coined the term multiple source domainadaptation.
Inspired by work on parsing accuracyprediction (Ravi et al, 2008), they train a linear re-gression model to predict the best (linear interpola-tion) of source domain models.
Similar to us, Mc-Closky et al (2010) regard a target domain as mix-ture of source domains, but they focus on phrase-structure parsing.
Furthermore, our approach differsfrom theirs in two respects: we do not treat sourcecorpora as one entity and try to mix models, butrather consider articles as base units and try to findsubsets of related articles (the most similar articles);moreover, instead of creating a supervised model (intheir case to predict parsing accuracy), our approachis ?simplistic?
: we apply measures of domain simi-larity directly (in an unsupervised fashion), withoutthe necessity to train a supervised model.Two other related studies are (Lippincott et al,2010; Van Asch and Daelemans, 2010).
Van Aschand Daelemans (2010) explore a measure of domaindifference (Renyi divergence) between pairs of do-mains and its correlation to Part-of-Speech taggingaccuracy.
Their empirical results show a linear cor-relation between the measure and the performanceloss.
Their goal is different, but related: rather thanfinding related data for a new domain, they want toestimate the loss in accuracy of a PoS tagger whenapplied to a new domain.
We will briefly discussresults obtained with the Renyi divergence in Sec-tion 5.1.
Lippincott et al (2010) examine subdomainvariation in biomedicine corpora and propose aware-ness of NLP tools to such variation.
However, theydid not yet evaluate the effect on a practical task,thus our study is somewhat complementary to theirs.The issue of data selection has recently been ex-amined for Language Modeling (Moore and Lewis,2010).
A subset of the available data is automati-cally selected as training data for a Language Modelbased on a scoring mechanism that compares cross-entropy scores.
Their approach considerably outper-formed random selection and two previous proposedapproaches both based on perplexity scoring.13 Measures of Domain Similarity3.1 Measuring Similarity AutomaticallyFeature Representations A similarity functionmay be defined over any set of events that are con-1We tested data selection by perplexity scoring, but foundthe Language Models too small to be useful in our setting.1567sidered to be relevant for the task at hand.
Forparsing, these might be words, characters, n-grams(of words or characters), Part-of-Speech (PoS) tags,bilexical dependencies, syntactic rules, etc.
How-ever, to obtain more abstract types such as PoS tagsor dependency relations, one would first need togather respective labels.
The necessary tools for thisare again trained on particular corpora, and will suf-fer from domain shifts, rendering labels noisy.Therefore, we want to gauge the effect of the sim-plest representation possible: plain surface charac-teristics (unlabeled text).
This has the advantagethat we do not need to rely on additional supervisedtools; moreover, it is interesting to know how far wecan get with this level of information only.We examine the following feature representa-tions: relative frequencies of words, relative fre-quencies of character tetragrams, and topic mod-els.
Our motivation was as follows.
Relative fre-quencies of words are a simple and effective rep-resentation used e.g.
in text classification (Manningand Schu?tze, 1999), while character n-grams haveproven successful in genre classification (Wu et al,2010).
Topic models (Blei et al, 2003; Steyversand Griffiths, 2007) can be considered an advancedmodel over word distributions: every article is repre-sented by a topic distribution, which in turn is a dis-tribution over words.
Similarity between documentscan be measured by comparing topic distributions.Similarity Functions There are many possiblesimilarity (or distance) functions.
They fall broadlyinto two categories: probabilistically-motivated andgeometrically-motivated functions.
The similarityfunctions examined in this study will be describedin the following.The Kullback-Leibler (KL) divergence D(q||r) isa classical measure of ?distance?2 between two prob-ability distributions, and is defined as: D(q||r) =?y q(y) logq(y)r(y) .
It is a non-negative, additive,asymmetric measure, and 0 iff the two distributionsare identical.
However, the KL-divergence is unde-fined if there exists an event y such that q(y) > 0but r(y) = 0, which is a property that ?makes itunsuitable for distributions derived via maximum-likelihood estimates?
(Lee, 2001).2It is not a proper distance metric since it is asymmetric.One option to overcome this limitation is to applysmoothing techniques to gather non-zero estimatesfor all y.
The alternative, examined in this paper, isto consider an approximation to the KL divergence,such as the Jensen-Shannon (JS) divergence (Lin,1991) and the skew divergence (Lee, 2001).The Jensen-Shannon divergence, which is sym-metric, computes the KL-divergence between q, r,and the average between the two.
We use the JSdivergence as defined in Lee (2001): JS(q, r) =12 [D(q||avg(q, r)) + D(r||avg(q, r))].
The asym-metric skew divergence s?, proposed by Lee (2001),mixes one distribution with the other by a degree de-fined by ?
?
[0, 1): s?
(q, r, ?)
= D(q||?r + (1 ??)q).
As ?
approaches 1, the skew divergence ap-proximates the KL-divergence.An alternative way to measure similarity is toconsider the distributions as vectors and applygeometrically-motivated distance functions.
Thisfamily of similarity functions includes the cosinecos(q, r) = q(y) ?
r(y)/||q(y)||||r(y)||, euclideaneuc(q, r) =??y(q(y)?
r(y))2 and variational(also known as L1 or Manhattan) distance function,defined as var(q, r) =?y |q(y)?
r(y)|.3.2 Human-annotated dataIn contrast to the automatic measures devised in theprevious section, we might have access to human an-notated data.
That is, use label information such astopic or genre to define the set of similar articles.Genre For the Penn Treebank (PT) Wall StreetJournal (WSJ) section, more specifically, the subsetavailable in the Penn Discourse Treebank, there ex-ists a partition of the data by genre (Webber, 2009).Every article is assigned one of the following genrelabels: news, letters, highlights, essays, errata, witand short verse, quarterly progress reports, notableand quotable.
This classification has been made onthe basis of meta-data (Webber, 2009).
It is well-known that there is no meta-data directly associatedwith the individual WSJ files in the Penn Treebank.However, meta-data can be obtained by looking atthe articles in the ACL/DCI corpus (LDC99T42),and a mapping file that aligns document numbers ofDCI (DOCNO) to WSJ keys (Webber, 2009).
Anexample document is given in Figure 1.
The meta-data field HL contains headlines, SO source info, and1568the IN field includes topic markers.<DOC><DOCNO> 891102-0186.
</DOCNO><WSJKEY> wsj_0008 </WSJKEY><AN> 891102-0186.
</AN><HL> U.S. Savings Bonds Sales@ Suspended by Debt Limit </HL><DD> 11/02/89 </DD><SO> WALL STREET JOURNAL (J) </SO><IN> FINANCIAL, ACCOUNTING, LEASING (FIN)BOND MARKET NEWS (BON) </IN><GV> TREASURY DEPARTMENT (TRE) </GV><DATELINE> WASHINGTON </DATELINE><TXT><p><s>The federal government suspended sales of U.S.savings bonds because Congress hasn?t liftedthe ceiling on government debt.</s></p> [...]Figure 1: Example of ACL/DCI article.
We have aug-mented it with the WSJ filename (WSJKEY).Topic On the basis of the same meta-data, wedevised a classification of the Penn Treebank WSJby topic.
That is, while the genre division has beenmostly made on the basis of headlines, we use theinformation of the IN field.
Every article is assignedone, more than one or none of a predefined set ofkeywords.
While their origin remains unclear,3these keywords seem to come from a controlledvocabulary.
There are 76 distinct topic markers.The three most frequent keywords are: TENDEROFFERS, MERGERS, ACQUISITIONS (TNM),EARNINGS (ERN), STOCK MARKET, OFFERINGS(STK).
This reflects the fact that a lot of arti-cles come from the financial domain.
But thecorpus also contains articles from more distant do-mains, like MARKETING, ADVERTISING (MKT),COMPUTERS AND INFORMATION TECHNOLOGY(CPR), HEALTH CARE PROVIDERS, MEDICINE,DENTISTRY (HEA), PETROLEUM (PET).4 Experimental Setup4.1 Tools & EvaluationThe parsing system used in this study is the MSTparser (McDonald et al, 2005), a state-of-the-artdata-driven graph-based dependency parser.
It is3It is not known what IN stands for, as also stated in MarkLiberman?s notes in the readme of the ACL/DCI corpus.
How-ever, a reviewer suggested that IN might stand for ?index terms?which seems plausible.a system that can be trained on a variety of lan-guages given training data in CoNLL format (Buch-holz and Marsi, 2006).
Additionally, the parser im-plements both projective and non-projective pars-ing algorithms.
The projective algorithm is used forthe experiments on English, while the non-projectivevariant is used for Dutch.
We train the parser usingdefault settings.
MST takes PoS-tagged data as in-put; we use gold-standard tags in the experiments.We estimate topic models using Latent DirichletAllocation (Blei et al, 2003) implemented in theMALLET4 toolkit.
Like Lippincott et al (2010),we set the number of topics to 100, and otherwiseuse standard settings (no further optimization).
Weexperimented with the removal of stopwords, butfound no deteriorating effect while keeping them.Thus, all experiments are carried out on data wherestopwords were not removed.We implemented the similarity measures pre-sented in Section 3.1.
For skew divergence, that re-quires parameter ?, we set ?
= .99 (close to KLdivergence) since that has shown previously to workbest (Lee, 2001).
Additionally, we evaluate the ap-proach on English PoS tagging using two differenttaggers: MXPOST, the MaxEnt tagger of Ratna-parkhi5 and Citar,6 a trigram HMM tagger.In all experiments, parsing performance is mea-sured as Labeled Attachment Score (LAS), the per-centage of tokens with correct dependency edge andlabel.
To compute LAS, we use the CoNLL 2007evaluation script7 with punctuation tokens excludedfrom scoring (as was the default setting in CoNLL2006).
PoS tagging accuracy is measured as the per-centage of correctly labeled words out of all words.Statistical significance is determined by Approxi-mate Randomization Test (Noreen, 1989; Yeh, 2000)with 10,000 iterations.4.2 DataEnglish - WSJ For English, we use the portion ofthe Penn Treebank Wall Street Journal (WSJ) thathas been made available in the CoNLL 2008 shared4http://mallet.cs.umass.edu/5ftp://ftp.cis.upenn.edu/pub/adwait/jmx/6Citar has been implemented by Danie?l de Kok and is avail-able at: https://github.com/danieldk/citar7http://nextens.uvt.nl/depparse-wiki/1569task.
This data has been automatically converted8into dependency structure, and contains three files:the training set (sections 02-21), development set(section 24) and test set (section 23).Since we use articles as basic units, we actuallysplit the data to get back original article boundaries.9This led to a total of 2,034 articles (1 million words).Further statistics on the datasets are given in Ta-ble 1.
In the first set of experiments on WSJ subdo-mains, we consider articles from section 23 and 24that contain at least 50 sentences as test sets (targetdomains).
This amounted to 22 test articles.EN: WSJ WSJ+G+B Dutcharticles 2,034 3,776 51,454sentences 43,117 77,422 1,663,032words 1,051,997 1,784,543 20,953,850Table 1: Overview of the datasets for English and Dutch.To test whether we have a reasonable system,we performed a sanity check and trained the MSTparser on the training section (02-21).
The resulton the standard test set (section 23) is identical topreviously reported results (excluding punctuationtokens: LAS 87.50, Unlabeled Attachment Score(UAS) 90.75; with punctuation tokens: LAS 87.07,UAS 89.95).
The latter has been reported in (Sur-deanu and Manning, 2010).English - Genia (G) & Brown (B) For the Do-main Adaptation experiments, we added 1,552 ar-ticles from the GENIA10 treebank (biomedical ab-stracts from Medline) and 190 files from the Browncorpus to the pool of data.
We converted the datato CoNLL format with the LTH converter (Johans-son and Nugues, 2007).
The size of the test files is,respectively: Genia 1,360 sentences with an aver-age number of 26.20 words per sentence; the Browntest set is the same as used in the CoNLL 2008shared task and contains 426 sentences with a meanof 16.80 words.8Using the LTH converter: http://nlp.cs.lth.se/software/treebank_converter/9This was a non-trivial task, as we actually noticed that somesentences have been omitted from the CoNLL 2008 shared task.10We use the GENIA distribution in Penn Treebank for-mat available at http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gz5 Experiments on English5.1 Experiments within the WSJIn the first set of experiments, we focus on the WSJand evaluate the similarity functions to gather re-lated data for a given test article.
We have 22 WSJarticles as test set, sampled from sections 23 and24.
Regarding feature representations, we examinedthree possibilities: relative frequencies of words, rel-ative frequencies of character tetragrams (both un-smoothed) and document topic distributions.In the following, we only discuss representationsbased on words or topic models as we found charac-ter tetragrams less stable; they performed sometimeslike their word-based counterparts but other times,considerably worse.Results of Similarity Measures Table 2 com-pares the effect of the different ways to select re-lated data in comparison to the random baseline forincreasing amounts of training data.
The table givesthe average over 22 test articles (rather than show-ing individual tables for the 22 articles).
We selectarticles up to various thresholds that specify the to-tal number of sentences selected in each round (e.g.0.3k, 1.2k, etc.
).11 In more detail, Table 2 shows theresult of applying various similarity functions (intro-duced in Section 3.1) over the two different featurerepresentations (w: words; tm: topic model) for in-creasing amounts of data.
We additionally provideresults of using the Renyi divergence.12Clearly, as more and more data is selected, thedifferences become smaller, because we are closeto the data limit.
However, for all data points lessthan 38k (97%), selection by jensen-shannon, varia-tional and cosine similarity outperform random dataselection significantly for both types of feature rep-resentations (words and topic model).
For selectionby topic models, this additionally holds for the eu-clidean measure.From the various measures we can see that se-lection by jensen-shannon divergence and varia-tional distance perform best, followed by cosinesimilarity, skew divergence, euclidean and renyi.11Rather than choosing k articles, as article length may differ.12The Renyi divergence (Re?nyi, 1961), also used by VanAsch and Daelemans (2010), is defined as D?
(q, r) = 1/(?
?1) log(?q?r1??
).15701% 3% 25% 49% 97%(0.3k) (1.2k) (9.6k) (19.2k) (38k)random 70.61 77.21 82.98 84.48 85.51w-js 74.07?
79.41?
83.98?
84.94?
85.68w-var 74.07?
79.60?
83.82?
84.94?
85.45w-skw 74.20?
78.95?
83.68?
84.60 85.55w-cos 73.77?
79.30?
83.87?
84.96?
85.59w-euc 73.85?
78.90?
83.52?
84.68 85.57w-ryi 73.41?
78.31 83.76?
84.46 85.46tm-js 74.23?
79.49?
84.04?
85.01?
85.45tm-var 74.29?
79.59?
83.93?
84.94?
85.43tm-skw 74.13?
79.42?
84.13?
84.82 85.73tm-cos 74.04?
79.27?
84.14?
84.99?
85.42tm-euc 74.27?
79.53?
83.93?
85.15?
85.62tm-ryi 71.26 78.64?
83.79?
84.85 85.58Table 2: Comparison of similarity measures basedon words (w) and topic model (tm): parsing accu-racy for increasing amounts of training data as averageover 22 WSJ articles (js=jensen-shannon; cos=cosine;skw=skew; var=variational; euc=euclidean; ryi=renyi).Best score (per representation) underlined, best overallscore bold; ?
indicates significantly better (p < 0.05)than random.Renyi divergence does not perform as well as otherprobabilistically-motivated functions.
Regardingfeature representations, the representation based ontopic models works slightly better than the respec-tive word-based measure (cf.
Table 2) and oftenachieves the overall best score (boldface).Overall, the differences in accuracy between thevarious similarity measures are small; but interest-ingly, the overlap between them is not that large.Table 3 and Table 4 show the overlap (in terms ofproportion of identically selected articles) betweenpairs of similarity measures.
As shown in Table 3,for all measures there is only a small overlap withthe random baseline (around 10%-14%).
Despitesimilar performance, topic model selection has inter-estingly no substantial overlap with any other word-based similarity measures: their overlap is at most41.6%.
Moreover, Table 4 compares the overlap ofthe various similarity functions within a certain fea-ture representation (here x stands for either topicmodel ?
left value ?
or words ?
right value).
Thetable shows that there is quite some overlap be-tween jensen-shannon, variational and skew diver-gence on one side, and cosine and euclidean onthe other side, i.e.
between probabilistically- andgeometrically-motivated functions.
Variational hasa higher overlap with the probabilistic functions.
In-terestingly, the ?peaks?
in Table 4 (underlined, i.e.the highest pair-wise overlaps) are the same for thedifferent feature representations.In the following we analyze selection by topicmodel and words, as they are relatively differentfrom each other, despite similar performance.
Forthe word-based model, we use jensen-shannon assimilarity function, as it turned out to be the bestmeasure.
For topic model, we use the simpler vari-ational metric.
However, very similar results wereachieved using jensen-shannon.
Cosine and eu-clidean did not perform as well.ran w-js w-var w-skw w-cos w-eucran ?
10.3 10.4 10.0 10.4 10.2tm-js 12.1 41.6 39.6 36.0 29.3 28.6tm-var 12.3 40.8 39.3 34.9 29.3 28.5tm-skw 11.8 40.9 39.7 36.8 30.0 30.1tm-cos 14.0 31.7 30.7 27.3 24.1 23.2tm-euc 14.6 27.5 27.2 23.4 22.6 22.1Table 3: Average overlap (in %) of similarity measure:random selection (ran) vs. measures based on words (w)and topic model (tm).x=tm/w x-js x-var x-skw x-cos x-euctm/w-var 76/74 ?
60/63 55/48 49/47tm/w-skw 69/72 60/63 ?
48/41 42/42tm/w-cos 57/42 55/48 48/41 ?
62/71tm/w-euc 47/41 49/47 42/42 62/71 ?Table 4: Average overlap (in %) for different featurerepresentations x as tm/w, where tm=topic model andw=words.
Highest pair-wise overlap is underlined.Automatic Measure vs. Human labels The nextquestion is how these automatic measures compareto human-annotated data.
We compare word-basedand topic model selection (by using jensen-shannonand variational, respectively) to selection based onhuman-given labels: genre and topic.
For genre, werandomly select larger amounts of training data fora given test article from the same genre.
For topic,the approach is similar, but as an article might have1571several topic markers (keywords in the IN field), werank articles by proportion of overlapping keywords.llllll0 5000 10000 15000 20000767880828486Averagenumber of sentencesAccuracyl randomwords?jstopic model?vargenretopic (IN fields)Figure 2: Comparison of automatic measures (words us-ing jensen-shannon and topic model using variational)with human-annotated labels (genre/topic).
Automaticmeasures outperform human labels (p < 0.05).Figure 2 shows that human-labels do actually notperform better than the automatic measures.
Bothare close to random selection.
Moreover, the lineof selection by topic marker (IN fields) stops early?
we believe the reason for this is that the IN fieldsare too fine-grained, which limits the number of ar-ticles that are considered relevant for a given testarticle.
However, manually aggregating articles onsimilar topics did not improve topic-based selectioneither.
We conclude that the automatic selectiontechniques perform significantly better than human-annotated data, at least within the WSJ domain con-sidered here.5.2 Domain Adaptation ResultsUntil now, we compared similarity measures by re-stricting ourselves to articles from the WSJ.
In thissection, we extend the experiments to the domainadaptation scenario.
We augment the pool of WSJarticles with articles coming from two other corpora:Genia and Brown.
We want to gauge the effective-ness of the domain similarity measures in the multi-domain setting, where articles are selected from thepool of data without knowing their identity (whichcorpus the articles came from).The test sets are the standard evaluation sets fromthe three corpora: the standard WSJ (section 23)and Brown test set from CoNLL 2008 (they contain2,399 and 426 sentences, respectively) and the Ge-nia test set (1,370 sentences).
As a reference, wegive results of models trained on the respective cor-pora (per-corpus models; i.e.
if we consider corporaboundaries and train a model on the respective do-main ?
this model is ?supervised?
in the sense that itknows from which corpus the test article came from)as well as a baseline model trained on all data, i.e.the union of all three corpora (wsj+genia+brown),which is a standard baseline in domain adapta-tion (Daume?
III, 2007; McClosky et al, 2010).WSJ Brown Genia(38k) (28k) (19k)random 86.58 73.81 83.77per-corpus 87.50 81.55 86.63union 87.05 79.12 81.57topic model (var) 87.11?
81.76?
86.77?words (js) 86.30 81.47?
86.44?Table 5: Domain Adaptation Results on English (signifi-cantly better: ?
than random; ?
than random and union).The learning curves are shown in Figure 3, thescores for a specific amount of data are given inTable 5.
The performance of the reference mod-els (per-corpus and union in Table 5) are indicatedin Figure 3 with horizontal lines: the dashed linerepresents the per-corpus performance (?supervised?model); the solid line shows the performance of theunion baseline trained on all available data (77k sen-tences).
For the former, the vertical dashed lines in-dicate the amount of data the model was trained on(e.g.
23k sentences for Brown).Simply taking all available data has a deteriorat-ing effect: on all three test sets, the performance ofthe union model is below the presumably best per-formance of a model trained on the respective corpus(per-corpus model).The empirical results show that automatic data se-lection by topic model outperforms random selec-tion on all three test sets and the union baseline intwo out of three cases.
More specifically, selectionby topic model outperforms random selection sig-nificantly on all three test sets and all points in thegraph (p < 0.001).
Selection by the word-basedmeasure (words-js) achieves a significant improve-1572lllll l0 10000 20000 30000 400008082848688wsj23allnumber of sentencesAccuracylllll ll0 10000 20000 30000 40000707580brownnumber of sentencesAccuracylllll ll0 10000 20000 30000 4000076788082848688genianumber of sentencesAccuracyl randomwords?jstopic model?varper?corpus modelunion (wsj+genia+brown)Figure 3: Domain Adaptation Results for English Parsing with Increasing Amounts of Training Data.
The vertical linerepresents the amount of data the per-corpus model is trained on.ment over the random baseline on two out of thethree test sets ?
it falls below the random baseline onthe WSJ test set.
Thus, selection by topic model per-forms best ?
it achieves better performance than theunion baseline with comparatively little data (Genia:4k; Brown: 19k ?
in comparison: union has 77k).Moreover, it comes very close to the supervised per-corpus model performance13 with a similar amountof data (cf.
vertical dashed line).
This is a very goodresult, given that the technique disregards the originof the articles and just uses plain words as informa-tion.
It automatically finds data that is beneficial foran unknown target domain.So far we examined domain similarity measuresfor parsing, and concluded that selection by topicmodel performs best, closely followed by word-based selection using the jensen-shannon diver-gence.
The question that remains is whether themeasure is more widely applicable: How does it per-form on another language and task?PoS tagging We perform similar Domain Adap-tation experiments on WSJ, Genia and Brown forPoS tagging.
We use two taggers (HMM and Max-Ent) and the same three test articles as before.
Theresults are shown in Figure 4 (it depicts the aver-age over the three test sets, WSJ, Genia, Brown, forspace reasons).
The left figure shows the perfor-mance of the HMM tagger; on the right is the Max-Ent tagger.
The graphs show that automatic train-ing data selection outperforms random data selec-13On Genia and Brown (cf.
Table 5) there is no significantdifference between topic model and per-corpus model.tion, and again topic model selection performs best,closely followed by words-js.
This confirms previ-ous findings and shows that the domain similaritymeasures are effective also for this task.llll ll l l0 10000 20000 30000 400000.900.920.940.960.98Average HMM taggernumber of sentencesAccuracyl randomwords?jstopic model?varllll ll l0 10000 20000 30000 400000.900.920.940.960.98Average MXPOST taggernumber of sentencesAccuracyl randomwords?jstopic model?varFigure 4: PoS tagging results, average over 3 test sets.6 Experiments on DutchFor Dutch, we evaluate the approach on a bigger andmore varied dataset.
It contains in total over 50k ar-ticles and 20 million words (cf.
Table 1).
In con-trast to the English data, only a small portion of thedataset is manually annotated: 281 articles.14Since we want to evaluate the performance ofdifferent similarity measures, we want to keep theinfluence of noise as low as possible.
Therefore,we annotated the remaining articles with a parsingsystem that is more accurate (Plank and van No-ord, 2010), the Alpino parser (van Noord, 2006).Note that using a more accurate parsing system totrain another parser has recently also been proposedby Petrov et al (2010) as uptraining.
Alpino is a14http://www.let.rug.nl/vannoord/Lassy/1573parser tailored to Dutch, that has been developedover the last ten years, and reaches an accuracy levelof 90% on general newspaper text.
It uses a condi-tional MaxEnt model as parse selection component.Details of the parser are given in (van Noord, 2006).lll llll0 5000 10000 15000 20000 25000 3000074767880828486Averagenumber of sentencesAccuracyl randomtopic model?varwords?jsFigure 5: Result on Dutch; average over 30 articles.Data and Results The Dutch dataset containsarticles from a variety of sources: Wikipedia15,EMEA16 (documents from the European MedicinesAgency) and the Dutch parallel corpus17 (DPC), thatcovers a variety of subdomains.
The Dutch arti-cles were parsed with Alpino and automatically con-verted to CoNLL format with the treebank conver-sion software from CoNLL 2006, where PoS tagshave been replaced with more fine-grained Alpinotags as that had a positive effect on MST.
The 281annotated articles come from all three sources.
Aswith English, we consider as test set articles withat least 50 sentences, from which 30 are randomlysampled.The results on Dutch are shown in Figure 5.
Do-main similarity measures clearly outperform randomdata selection also in this setting with another lan-guage and a considerably larger pool of data (20 mil-lion words; 51k articles).7 DiscussionIn this paper we have shown the effectiveness of asimple technique that considers only plain words asdomain selection measure for two tasks, dependency15http://ilps.science.uva.nl/WikiXML/16http://urd.let.rug.nl/tiedeman/OPUS/EMEA.php17http://www.kuleuven-kortrijk.be/DPCparsing and PoS tagging.
Interestingly, human-annotated labels did not perform better than the au-tomatic measures.
The best technique is based ontopic models, and compares document topic distri-butions estimated by LDA (Blei et al, 2003) usingthe variational metric (very similar results were ob-tained using jensen-shannon).
Topic model selec-tion significantly outperforms random data selectionon both examined languages, English and Dutch,and has a positive effect on PoS tagging.
More-over, it outperformed a standard Domain Adapta-tion baseline (union) on two out of three test sets.Topic model is closely followed by the word-basedmeasure using jensen-shannon divergence.
By ex-amining the overlap between word-based and topicmodel-based techniques, we found that despite sim-ilar performance their overlap is rather small.
Giventhese results and the fact that no optimization hasbeen done for the topic model itself, results are en-couraging: there might be an even better measurethat exploits the information from both techniques.So far, we tested a simple combination of the two byselecting half of the articles by a measure based onwords and the other half by a measure based on topicmodels (by testing different metrics).
However, thissimple combination technique did not improve re-sults yet ?
topic model alone still performed best.Overall, plain surface characteristics seem tocarry important information of what kind of data isrelevant for a given domain.
Undoubtedly, parsingaccuracy will be influenced by more factors than lex-ical information.
Nevertheless, as we have seen, lex-ical differences constitute an important factor.Applying divergence measures over syntactic pat-terns, adding additional articles to the pool ofdata (by uptraining (Petrov et al, 2010), selftrain-ing (McClosky et al, 2006) or active learning (Hwa,2004)), gauging the effect of weighting instancesaccording to their similarity to the test data (Jiangand Zhai, 2007; Plank and Sima?an, 2008), as wellas analyzing differences between gathered data arevenues for further research.AcknowledgmentsThe authors would like to thank Bonnie Webber andthe three anonymous reviewers for their valuablecomments on earlier drafts of this paper.1574ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain Adaptation with Structural Correspon-dence Learning.
In Proceedings of the 2006 Confer-ence on Empirical Methods in Natural Language Pro-cessing, Sydney, Australia.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-XShared Task on Multilingual Dependency Parsing.
InProceedings of the 10th Conference on ComputationalNatural Language Learning (CoNLL-X), pages 149?164, New York City.Hal Daume?
III.
2007.
Frustratingly Easy Domain Adap-tation.
In Proceedings of the 45th Meeting of the Asso-ciation for Computational Linguistics, Prague, CzechRepublic.Daniel Gildea.
2001.
Corpus Variation and Parser Per-formance.
In Proceedings of the 2001 Conference onEmpirical Methods in Natural Language Processing,Pittsburgh, PA.Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Adapting a Probabilistic Disambiguation Modelof an HPSG Parser to a New Domain.
In Robert Dale,Kam-Fai Wong, Jian Su, and Oi Yee Kwong, editors,Natural Language Processing IJCNLP 2005, volume3651 of Lecture Notes in Computer Science, pages199?210.
Springer Berlin / Heidelberg.Rebecca Hwa.
2004.
Sample Selection for StatisticalParsing.
Compututational Linguistics, 30:253?276,September.Jing Jiang and ChengXiang Zhai.
2007.
InstanceWeighting for Domain Adaptation in NLP.
In Pro-ceedings of the 45th Meeting of the Association forComputational Linguistics, pages 264?271, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Richard Johansson and Pierre Nugues.
2007.
ExtendedConstituent-to-dependency Conversion for English.
InProceedings of NODALIDA, Tartu, Estonia.Lillian Lee.
2001.
On the Effectiveness of the Skew Di-vergence for Statistical Language Analysis.
In In Ar-tificial Intelligence and Statistics 2001, pages 65?72,Key West, Florida.J.
Lin.
1991.
Divergence measures based on the Shannonentropy.
Information Theory, IEEE Transactions on,37(1):145 ?151, January.Tom Lippincott, Diarmuid O?
Se?aghdha, Lin Sun, andAnna Korhonen.
2010.
Exploring variation acrossbiomedical subdomains.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics, pages 689?697, Beijing, China, August.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Process-ing.
MIT Press, Cambridge Mass.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective Self-Training for Parsing.
In Pro-ceedings of Human Language Technology Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 152?159, Brooklyn,New York.
Association for Computational Linguistics.David McClosky, Eugene Charniak, and Mark Johnson.2010.
Automatic Domain Adaptation for Parsing.
InProceedings of Human Language Technology Confer-ence of the North American Chapter of the Associationfor Computational Linguistics, pages 28?36, Los An-geles, California, June.
Association for ComputationalLinguistics.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective Dependency Parsingusing Spanning Tree Algorithms.
In Proceedings ofHuman Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 523?530, Vancouver, British Columbia,Canada, October.
Association for Computational Lin-guistics.Robert C. Moore and William Lewis.
2010.
IntelligentSelection of Language Model Training Data.
In Pro-ceedings of the ACL 2010 Conference Short Papers,pages 220?224, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Eric W. Noreen.
1989.
Computer-Intensive Methodsfor Testing Hypotheses: An Introduction.
Wiley-Interscience.Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, andHiyan Alshawi.
2010.
Uptraining for Accurate Deter-ministic Question Parsing.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, pages 705?713, Cambridge, MA,October.
Association for Computational Linguistics.Barbara Plank and Khalil Sima?an.
2008.
SubdomainSensitive Statistical Parsing using Raw Corpora.
InProceedings of the 6th International Conference onLanguage Resources and Evaluation, Marrakech, Mo-rocco, May.Barbara Plank and Gertjan van Noord.
2010.
Grammar-Driven versus Data-Driven: Which Parsing System IsMore Affected by Domain Shifts?
In Proceedings ofthe 2010 Workshop on NLP and Linguistics: Findingthe Common Ground, pages 25?33, Uppsala, Sweden,July.
Association for Computational Linguistics.Sujith Ravi, Kevin Knight, and Radu Soricut.
2008.
Au-tomatic Prediction of Parser Accuracy.
In EMNLP?08: Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pages 887?1575896, Morristown, NJ, USA.
Association for Computa-tional Linguistics.A.
Re?nyi.
1961.
On measures of information and en-tropy.
In Proceedings of the 4th Berkeley Sympo-sium on Mathematics, Statistics and Probability, pages547?561, Berkeley.Satoshi Sekine.
1997.
The Domain Dependence of Pars-ing.
In In Proceedings of the Fifth Conference onApplied Natural Language Processing, pages 96?102,Washington D.C.Mark Steyvers and Tom Griffiths, 2007.
ProbabilisticTopic Models.
Lawrence Erlbaum Associates.Mihai Surdeanu and Christopher D. Manning.
2010.
En-semble Models for Dependency Parsing: Cheap andGood?
In Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages649?652, Los Angeles, California, June.
Associationfor Computational Linguistics.Vincent Van Asch and Walter Daelemans.
2010.
Us-ing Domain Similarity for Performance Estimation.
InProceedings of the 2010 Workshop on Domain Adap-tation for Natural Language Processing, pages 31?36,Uppsala, Sweden, July.
Association for ComputationalLinguistics.Gertjan van Noord.
2006.
At Last Parsing Is Now Oper-ational.
In TALN 2006 Verbum Ex Machina, Actes DeLa 13e Conference sur Le Traitement Automatique desLangues naturelles, pages 20?42, Leuven.Bonnie Webber.
2009.
Genre distinctions for Discoursein the Penn TreeBank.
In Proceedings of the 47thMeeting of the Association for Computational Linguis-tics, pages 674?682, Suntec, Singapore, August.
Asso-ciation for Computational Linguistics.Zhili Wu, Katja Markert, and Serge Sharoff.
2010.
Fine-Grained Genre Classification Using Structural Learn-ing Algorithms.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 749?759, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.Alexander Yeh.
2000.
More accurate tests for the statis-tical significance of result differences.
In Proceedingsof the 18th conference on Computational linguistics,pages 947?953, Morristown, NJ, USA.
Association forComputational Linguistics.1576
