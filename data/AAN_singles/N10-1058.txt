Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 373?376,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsImproving Semantic Role Classification with Selectional PreferencesBen?at Zapirain, Eneko AgirreIXA NLP GroupBasque Country Univ.{benat.zapirain,e.agirre}@ehu.esLlu?
?s Ma`rquezTALP Research CenterTechnical Univ.
of Catalonialluism@lsi.upc.eduMihai SurdeanuStanford NLP GroupStanford Univ.mihais@stanford.eduAbstractThis work incorporates Selectional Prefer-ences (SP) into a Semantic Role (SR) Clas-sification system.
We learn separate selec-tional preferences for noun phrases and prepo-sitional phrases and we integrate them in astate-of-the-art SR classification system bothin the form of features and individual classpredictors.
We show that the inclusion of therefined SPs yields statistically significant im-provements on both in domain and out of do-main data (14.07% and 11.67% error reduc-tion, respectively).
The key factor for successis the combination of several SP methods withthe original classification model using meta-classification.1 IntroductionSemantic Role Labeling (SRL) is the process ofextracting simple event structures, i.e., ?who?
did?what?
to ?whom?, ?when?
and ?where?.
Currentsystems usually perform SRL in two pipelined steps:argument identification and argument classification.While identification is mostly syntactic, classifica-tion requires semantic knowledge to be taken intoaccount.
Semantic information is usually capturedthrough lexicalized features on the predicate and thehead?word of the argument to be classified.
Sincelexical features tend to be sparse, SRL systems areprone to overfit the training data and generalizepoorly to new corpora.Indeed, the SRL evaluation exercises at CoNLL-2004 and 2005 (Carreras and Ma`rquez, 2005) ob-served that all systems showed a significant perfor-mance degradation (?10 F1 points) when applied totest data from a different genre of that of the trainingset.
Pradhan et al (2008) showed that this perfor-mance degradation is essentially caused by the argu-ment classification subtask, and suggested the lexi-cal data sparseness as one of the main reasons.
Thesame authors studied the contribution of the differentfeature types in SRL and concluded that the lexicalfeatures were the most salient features in argumentclassification (Pradhan et al, 2007).In recent work, we showed (Zapirain et al, 2009)how automatically generated selectional preferences(SP) for verbs were able to perform better than purelexical features in a role classification experiment,disconnected from a full-fledged SRL system.
SPsintroduce semantic generalizations on the type of ar-guments preferred by the predicates and, thus, theyare expected to improve results on infrequent andunknown words.
The positive effect was especiallyrelevant for out-of-domain data.
In this paper we ad-vance (Zapirain et al, 2009) in two directions:(1) We learn separate SPs for prepositions and verbs,showing improvement over using SPs for verbsalone.
(2) We integrate the information of several SP mod-els in a state-of-the-art SRL system (SwiRL1) andshow significant improvements in SR classifica-tion.
The key for the improvement lies in a meta-classifier, trained to select among the predictionsprovided by several role classification models.2 SPs for SR ClassificationSPs have been widely believed to be an impor-tant knowledge source when parsing and perform-ing SRL, especially role classification.
Still, presentparsers and SRL systems use just lexical features,which can be seen as the most simple form of SP,1http://www.surdeanu.name/mihai/swirl/373where the headword needs to be seen in the trainingdata, and otherwise the SP is not satisfied.
Gildeaand Jurafsky (2002) showed barely significant im-provements in semantic role classification of NPsfor FrameNet roles using distributional clusters.
In(Erk, 2007) a number of SP models are tested ina pseudo-task related to SRL.
More recently, weshowed (Zapirain et al, 2009) that several methodsto automatically generate SPs generalize well andoutperform lexical match in a large dataset for se-mantic role classification, but the impact on a fullsystem was not explored.In this work we apply a subset of the SP meth-ods proposed in (Zapirain et al, 2009).
These meth-ods can be split in two main families, depending onthe resource used to compute similarity: WordNet-based methods and distributional methods.
Bothfamilies define a similarity score between a word(the headword of the argument to be classified) and aset of words (the headwords of arguments of a givenrole).WordNet-based similarity: One of the modelsthat we used is based on Resnik?s similarity mea-sure (1993), referring to it as res.
The other model isan in-house method (Zapirain et al, 2009), referredas wn, which only takes into account the depth ofthe most common ancestor, and returns SPs that areas specific as possible.Distributional similarity: Following (Zapirain etal., 2009) we considered both first order and secondorder similarity.
In first order similarity, the simi-larity of two words was computed using the cosine(or Jaccard measure) of the co-occurrence vectors ofthe two words.
Co-occurrence vectors where con-structed using freely available software (Pado?
andLapata, 2007) run over the British National Corpus.We used the optimal parameters (Pado?
and Lapata,2007, p. 179).
We will refer to these similarities assimcos and simJac, respectively.
In contrast, sec-ond order similarity uses vectors of similar words,i.e., the similarity of two words was computed us-ing the cosine (or Jaccard measure) between thethesaurus entries of those words in Lin?s thesaurus(Lin, 1998).
We refer to these as sim2cos and sim2Jac.Given a target sentence with a verb and its argu-ments, the task of SR classification is to assign thecorrect role to each of the arguments.
When usingSPs alone, we only use the headwords of the ar-guments, and each argument is classified indepen-dently of the rest.
For each headword, we select therole (r) of the verb (c) which fits best the head word(w), where the goodness of fit (SPsim(v, r, w)) ismodeled using one of the similarity models above,between the headword w and the headwords seen intraining data for role r of verb v. This selection ruleis formalized as follows:Rsim(v, w) = arg maxr?Roles(v)SPsim(v, r, w) (1)In our previous work (Zapirain et al, 2009), wemodelled SPs for pairs of predicates (verbs) and ar-guments, independently of the fact that the argu-ment is a core argument (typically a noun) or anadjunct argument (typically a prepositional phrase).In contrast, (Litkowski and Hargraves, 2005) showthat prepositions have SPs of their own, especiallywhen functioning as adjuncts.
We therefore decidedto split SPs according to whether the potential argu-ment is a Prepositional Phrase (PP) or a Noun Phrase(NP).
For NPs, which tend to be core arguments2,we use the SPs of the verb (as formalized above).For PPs, which have an even distribution betweencore and adjunct arguments, we use the SPs of theprepositions alone, ignoring the verbs.
Implementa-tion wise, this means that in Eq.
(1), we change vfor p, where p is the preposition heading the PP.3 Experiments with SPs in isolationIn this section we evaluate the use of SPs for classi-fication in isolation, i.e., we use formula 1, and noother information.
In addition we contrast the useof both verb-role and preposition-role SPs, as com-pared to the use of verb-role SPs alone.The dataset used in these experiments (and in Sec-tion 4) is the same as provided by the CoNLL-2005shared task on SRL (Carreras and Ma`rquez, 2005).This dataset comprises several sections of the Prop-Bank corpus (news from the WSJ) as well as an ex-tract of the Brown Corpus.
Sections 02-21 are usedfor generating the SPs and training, Section 00 fordevelopment, and Section 23 for testing, as custom-ary.
The Brown Corpus is used for out-of-domaintesting, but due to the limited size of the providedsection, we extended it with instances from Sem-Link3.
Since the focus of this work is on argument2In our training data, NPs are adjuncts only 5% of the times3http://verbs.colorado.edu/semlink/374Verb-Role SPs Preposition-Role and Verb-Role SPsWSJ-test Brown WSJ-test Brownprec.
rec.
F1 prec.
rec.
F1 prec.
rec.
F1 prec.
rec.
F1lexical 70.75 26.66 39.43 59.39 05.51 10.08 82.98 43.77 57.31 68.47 13.60 22.69SPres 45.07 37.11 40.71 36.34 27.58 31.33 63.47 53.24 57.91 55.12 44.15 49.03SPwn 55.44 45.58 50.03 41.76 31.58 35.96 65.70 63.88 64.78 60.08 48.10 53.43SPsimJac 48.85 46.38 47.58 42.10 34.34 37.82 61.83 61.40 61.61 55.42 53.45 54.42SPsimcos 53.13 50.44 51.75 43.24 35.27 38.85 64.67 64.22 64.44 56.56 54.54 55.53SPsim2Jac61.76 58.63 60.16 51.97 42.39 46.69 70.82 70.33 70.57 62.37 60.15 61.24SPsim2cos 61.12 58.12 59.63 51.92 42.35 46.65 70.28 69.80 70.04 62.36 60.14 61.23Table 1: Results for SPs in isolation, left for verb SPs, and right both preposition and verb SPs.Labels proposed by the base modelsNumber of base models that proposed this datum?s labelList of actual base models that proposed this datum?s labelTable 2: Features of the binary meta-classifier.classification, we use the gold PropBank data toidentify argument boundaries.
Considering that SPscan handle only nominal arguments, in these exper-iments we used only arguments mapped to NPs andPPs containing a nominal head.
From the trainingsections, we extracted over 140K such arguments forthe supervised generation of SPs.
The developmentand test sections contain over 5K and 8K examples,respectively, and the portion of the Brown Corpuscomprises an amount of 8.1K examples.Table 1 lists the results of the different SPs in iso-lation.
The results reported in the left part of Table1 are comparable to those we reported in (Zapirainet al, 2009).
The differences are due to the fact thatwe do not discard roles like MOD, DIS, NEG andthat our previous work used only the subset of thedata that could be mapped to VerbNet (around 50%).All in all, the table shows that splitting SPs into verband preposition SPs yields better results, both in pre-cision and recall, improving F1 up to 10 points insome cases.4 Integrating SPs in a SRL systemFor these experiments we modified SwiRL (Sur-deanu et al, 2007): (a) we matched the gold bound-aries against syntactic constituents predicted inter-nally using the Charniak parser (Charniak, 2000);and (b) we classified these constituents with theirsemantic role using a modified version of SwiRL?sfeature set.We explored two different strategies for integrat-ing SPs in SwiRL.
The first, obvious method is toextend SwiRL?s feature set with features that modelthe preferences of the SPs, i.e., for each SP modelSPi we add a feature whose value is Ri.
The secondmethod combines SwiRL?s classification model andour SP models using meta-classification.
We optedfor a binary classification approach: first, for eachconstituent we generate n datums, one for each dis-tinct role label proposed by the pool of base models;then we use a binary meta-classifier to label eachcandidate role as correct or incorrect.
Table 2 liststhe features of the meta-classifier.
We trained themeta-classifier on the usual PropBank training par-tition, using cross-validation to generate outputs forthe base models that require the same training ma-terial.
At prediction time, for each candidate con-stituent we selected the role label that was classifiedas correct with the highest confidence.Table 3 compares the performance of bothcombination approaches against the standaloneSwiRL classifier.
We show results for both corearguments (Core), adjunct arguments (Arg) andall arguments combined (All).
In the table, theSwiRL+SP?
models stand for SwiRL classifiersenhanced with one feature from the correspond-ing SP.
Adding more than one SP-based feature toSwiRL did not improve results.
Our conjectureis that the SwiRL classifier enhanced with SP-based features does not learn relevant weights forthese features because their signal is ?drowned?
bySwiRL?s large initial feature set and the correlationbetween the different SPs.
This observation moti-vated the development of the meta-classifier.
Themeta-classifier shown in the table combines the out-put of the SwiRL+SP?
models with the predictionsof SP models used in isolation.
We implementedthe meta-classifier using Support Vector Machines(SVM)4 with a quadratic polynomial kernel, and4http://svmlight.joachims.org375WSJ-test BrownCore Adj All Core Adj AllSwiRL 93.25 81.31 90.83 84.42 57.76 79.52+SPRes 93.17 81.08 90.76 84.52 59.24 79.86+SPwn 92.88 81.11 90.56 84.26 59.69 79.73+SPsimJac 93.37 80.30 90.86 84.43 59.54 79.83+SPsimcos 93.33 80.92 90.87 85.14 60.16 80.50+SPsim2Jac93.03 82.75 90.95 85.62 59.63 80.75+SPsim2cos 93.78 80.56 91.23 84.95 61.01 80.48Meta 94.37 83.40 92.12 86.20 63.40 81.91Table 3: Classification accuracy for the combination ap-proaches.
+SPx stands for SwiRL plus each SP model.C = 0.01 (tuned in development).Table 3 indicates that four out of the sixSwiRL+SP?
models perform better than SwiRL indomain (WSJ-test), and all of them outperformSwiRL out of domain (Brown).
However, the im-provements are small and, generally, not statisticallysignificant.
On the other hand, the meta-classifieroutperforms SwiRL both in domain (14.07% errorreduction) and out of domain (11.67% error reduc-tion), and the differences are statistically signifi-cant (measured using two-tailed paired t-test at 99%confidence interval on 100 samples generated us-ing bootstrap resampling).
We also implementedtwo unsupervised voting baselines, one unweighted(each base model has the same weight) and oneweighted (each base model is weighted by its accu-racy in development).
However, none of these base-lines outperformed the standalone SwiRL classifier.This is further proof that, for SR classification, meta-classification is crucial because it can learn the dis-tinct specializations of the various base models.Finally, Table 3 shows that our approach yieldsconsistent improvements for both core and adjunctarguments.
Out of domain, we see a bigger accuracyimprovement for adjunct arguments (5.64 absolutepoints) vs. core arguments (1.78 points).
This isto be expected, as most core arguments fall underthe Arg0 and Arg1 classes, which can typically bedisambiguated based on syntactic information, i.e.,subject vs. object.
On the other hand, there are nosyntactic hints for adjunct arguments, so the systemlearns to rely more on SP information in this case.5 ConclusionsThis paper is the first work to show that SPs improvea state-of-the-art SR classification system.
Sev-eral decisions were crucial for success: (a) we de-ployed separate SP models for verbs and preposi-tions, which in conjunction outperform SP modelsfor verbs alone; (b) we incorporated SPs into SRclassification using a meta-classification approachthat combines eight base models, developed fromvariants of a state-of-the-art SRL system and theabove SP models.
We show that the resulting systemoutperforms the original SR classification system forarguments mapped to nominal or prepositional con-stituents.
The improvements are statistically sig-nificant both on in-domain and out-of-domain datasets.AcknowledgmentsThis work was partially supported by projects KNOW-2 (TIN2009-14715-C04-01 / 04), KYOTO (ICT-2007-211423) and OpenMT-2 (TIN2009-14675C03)ReferencesX.
Carreras and L. Ma`rquez.
2005.
Introduction to theCoNLL-2005 Shared Task: Semantic role labeling.
InProc.
of CoNLL.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In Proc.
of NAACL.K.
Erk.
2007.
A simple, similarity-based model for se-lectional preferences.
In Proc.
of ACL.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling ofsemantic roles.
Computational Linguistics, 28(3).D.
Lin.
1998.
Automatic retrieval and clustering of sim-ilar words.
In Proc.
of COLING-ACL.K.
Litkowski and O. Hargraves.
2005.
The preposi-tion project.
In Proceedings of the Workshop on TheLinguistic Dimensions of Prepositions and their Usein Computational Linguistic Formalisms and Applica-tions.S.
Pado?
and M. Lapata.
2007.
Dependency-based con-struction of semantic space models.
ComputationalLinguistics, 33(2).S.
Pradhan, W. Ward, and J. Martin.
2007.
Towards ro-bust semantic role labeling.
In Proc.
of NAACL-HLT.S.
Pradhan, W. Ward, and J. Martin.
2008.
Towards ro-bust semantic role labeling.
Computational Linguis-tics, 34(2).P.
Resnik.
1993.
Semantic classes and syntactic ambigu-ity.
In Proc.
of HLT.M.
Surdeanu, L. Ma`rquez, X. Carreras, and P.R.
Comas.2007.
Combination strategies for semantic role label-ing.
Journal of Artificial Intelligence Research, 29.B.
Zapirain, E. Agirre, and L. Ma`rquez.
2009.
General-izing over lexical features: Selectional preferences forsemantic role classification.
In Proc.
of ACL-IJCNLP.376
