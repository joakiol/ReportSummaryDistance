Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 7?12,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsThe Distributional Similarity of Sub-ParsesJulie Weeds, David Weir and Bill KellerDepartment of InformaticsUniversity of SussexBrighton, BN1 9QH, UK{juliewe, davidw, billk}@sussex.ac.ukAbstractThis work explores computing distribu-tional similarity between sub-parses, i.e.,fragments of a parse tree, as an extensionto general lexical distributional similaritytechniques.
In the same way that lexicaldistributional similarity is used to estimatelexical semantic similarity, we propose us-ing distributional similarity between sub-parses to estimate the semantic similarity ofphrases.
Such a technique will allow us toidentify paraphrases where the componentwords are not semantically similar.
Wedemonstrate the potential of the method byapplying it to a small number of examplesand showing that the paraphrases are moresimilar than the non-paraphrases.1 IntroductionAn expression is said to textually entail another ex-pression if the meaning of the second expression canbe inferred from the meaning of the first.
For exam-ple, the sentence ?London is an English city,?
tex-tually entails the sentence ?London is in England.
?As discussed by Dagan et al (2005) in their intro-duction to the first Recognising Textual EntailmentChallenge, identifying textual entailment can be seenas a subtask of a variety of other natural languageprocessing (NLP) tasks.
For example, Question An-swering (QA) can be cast as finding an answer whichis entailed by the proposition in the question.
Otheridentified tasks include summarization, paraphras-ing, Information Extraction (IE), Information Re-trieval (IR) and Machine Translation (MT).The Natural Habitats (NatHab) project1 (Weedset al, 2004; Owen et al, 2005) provides an inter-esting setting in which to study paraphrase and tex-1http://www.informatics.susx.ac.uk/projects/nathab/tual entailment recognition as a tool for natural lan-guage understanding.
The aim of the project is toenable non-technical users to configure their perva-sive computing environments.
They do this by stat-ing policies in natural language which describe howthey wish their environment to behave.
For exam-ple, a user, who wishes to restrict the use of theircolour printer to the printing of colour documents,might have as a policy, ?Never print black-and-whitedocuments on my colour printer.?
Similarly, a user,who wishes to be alerted by email when their mobilephone battery is low, might have as a policy, ?If mymobile phone battery is low then send me an email.
?The natural language understanding task is to in-terpret the user?s utterance with reference to a setof policy templates and an ontology of services (e.g.print) and concepts (e.g.
document).
The use of pol-icy templates and an ontology restricts the number ofpossible meanings that a user can express.
However,there is still considerable variability in the way thesepolicies can be expressed.
Simple variations on thetheme of the second policy above include, ?Send mean email whenever my mobile phone battery is low,?and ?If the charge on my mobile phone is low thenemail me.?
Our approach is to tackle the interpreta-tion problem by identifying parts of expressions thatare paraphrases of those expressions whose interpre-tation with respect to the ontology is more directlyencoded.
Here, we investigate extending distribu-tional similarity methods from words to sub-parses.The rest of this paper is organised as follows.
InSection 2 we discuss the background to our work.We consider the limitations of an approach based onlexical similarity and syntactic templates, which mo-tivates us to look directly at the similarity of largerunits.
In Section 3, we introduce our proposed ap-proach, which is to measure the distributional simi-larity of sub-parses.
In Section 4, we consider exam-ples from the Pascal Textual Entailment Challenge7Datasets2 (Dagan et al, 2005) and demonstrate em-pirically how similarity can be found between corre-sponding phrases when parts of the phrases cannotbe said to be similar.
In Section 5, we present ourconclusions and directions for further work.2 BackgroundOne well-studied approach to the identification ofparaphrases is to employ a lexical similarity func-tion.
As noted by Barzilay and Elhadad (2003), evena lexical function that simply computes word over-lap can accurately select paraphrases.
The prob-lem with such a function is not in the accuracy ofthe paraphrases selected, but in its low recall.
Onepopular way of improving recall is to relax the re-quirement for words in each sentence to be identi-cal in form, to being identical or similar in mean-ing.
Methods to find the semantic similarity of twowords can be broadly split into those which use lex-ical resources, e.g., WordNet (Fellbaum, 1998), andthose which use a distributional similarity measure(see Weeds (2003) for a review of distributional sim-ilarity measures).
Both Jijkoun and deRijke (2005)and Herrara et al (2005) show how such a measureof lexical semantic similarity might be incorporatedinto a system for recognising textual entailment be-tween sentences.Previous work on the NatHab project (Weeds etal., 2004) used such an approach to extend lexi-cal coverage.
Each of the user?s uttered words wasmapped to a set of candidate words in a core lexicon3,identified using a measure of distributional similar-ity.
For example, the word send is used when talk-ing about printing or about emailing, and a goodmeasure of lexical similarity would identify both ofthese conceptual services as candidates.
The bestchoice of candidate was then chosen by optimisingthe match between grammatical dependency rela-tions and paths in the ontology over the entire sen-tence.
For example, an indirect-object relation be-tween the verb send and a printer can be mapped tothe path in the ontology relating a print request toits target printer.As well as lexical variation, our previous work(Weeds et al, 2004) allowed a certain amount ofsyntactic variation via its use of grammatical depen-dencies and policy templates.
For example, the pas-sive ?paraphrase?
of a sentence can be identified bycomparing the sets of grammatical dependency rela-tions produced by a shallow parser such as the RASP2http://www.pascal-network.org/Challenges/RTE/3The core lexicon lists a canonical word form for eachconcept in the ontology.parser (Briscoe and Carroll, 1995).
In other words,by looking at grammatical dependency relations, wecan identify that ?John is liked by Mary,?
is a para-phrase of ?Mary likes John,?
and not of ?John likesMary.?
Further, where there is a limited number ofstyles of sentence, we can manually identify and listother templates for matches over the trees or sets ofdependency relations.
For example, ?If C1 then C2?is the same as ?C2 if C1?.However, the limitations of this approach, whichcombines lexical variation, grammatical dependencyrelations and template matching, become increas-ingly obvious as one tries to scale up.
As noted byHerrera (2005), similarity at the word level is notrequired for similarity at the phrasal level.
For ex-ample, in the context of our project, the phrases ?ifmy mobile phone needs charging?
and ?if my mobilephone battery is low?
have the same intended mean-ing but it is not possible to obtain the second bymaking substitutions for similar words in the first.
Itappears that ?X needs charging?
and ?battery (of X)is low?
have roughly similar meanings without theircomponent words having similar meanings.
Further,this does not appear to be due to either phrase beingnon-compositional.
As noted by Pearce (2001), it isnot possible to substitute similar words within non-compositional collocations.
In this case, however,both phrases appear to be compositional.
Wordscannot be substituted between the two phrases be-cause they are composed in different ways.3 ProposalRecently, there has been much interest in find-ing words which are distributionally similar e.g.,Lin (1998), Lee (1999), Curran and Moens (2002),Weeds (2003) and Geffet and Dagan (2004).
Twowords are said to be distributionally similar if theyappear in similar contexts.
For example, the twowords apple and pear are likely to be seen as theobjects of the verbs eat and peel, and this adds totheir distributional similarity.
The DistributionalHypothesis (Harris, 1968) proposes a connection be-tween distributional similarity and semantic simi-larity, which is the basis for a large body of workon automatic thesaurus construction using distribu-tional similarity methods (Curran and Moens, 2002;Weeds, 2003; Geffet and Dagan, 2004).Our proposal is that just as words have distribu-tional similarity which can be used, with at leastsome success, to estimate semantic similarity, so dolarger units of expression.
We propose that the unitof interest is a sub-parse, i.e., a fragment (connectedsubgraph) of a parse tree, which can range in sizefrom a single word to the parse for the entire sen-8mymobileneedsphone chargingmymobilephonelowisbatteryFigure 1: Parse trees for ?my mobile phone needscharging?
and ?my mobile phone battery is low?tence.
Figure 1 shows the parses for the clauses,?my mobile phone needs charging,?
and ?my mobilephone battery is low?
and highlights the fragments(?needs charging?
and ?battery is low?)
for which wemight be interested in finding similarity.In our model, we define the features or contexts ofa sub-parse to be the grammatical relations betweenany component of the sub-parse and any word out-side of the sub-parse.
In the example above, bothsub-parses would have features based on their gram-matical relation with the word phone.
The level ofgranularity at which to consider grammatical rela-tions remains a matter for investigation.
For exam-ple, it might turn out to be better to distinguishbetween all types of dependent or, alternatively, itmight be better to have a single class which coversall dependents.
We also consider the parents of thesub-parse as features.
In the example, ?Send me anemail if my mobile phone battery is low,?
this wouldbe that the sub-parse modifies the verb send i.e., ithas the feature, <mod-of, send>.Having defined these models for the unit of inter-est, the sub-parse, and for the context of a sub-parse,we can build up co-occurrence vectors for sub-parsesin the same way as for words.
A co-occurrence vec-tor is a conglomeration (with frequency counts) ofall of the co-occurrences of the target unit found ina corpus.
The similarity between two such vectorsor descriptions can then be found using a standarddistributional similarity measure (see Weeds (2003)).The use of distributional evidence for larger unitsthan words is not new.
Szpektor et al (2004) auto-matically identify anchors in web corpus data.
An-chors are lexical elements that describe the contextof a sentence and if words are found to occur withthe same set of anchors, they are assumed to beparaphrases.
For example, the anchor set {Mozart,1756} is a known anchor set for verbs with the mean-ing ?born in?.
However, this use of distributionalevidence requires both anchors, or contexts, to oc-cur simultaneously with the target word.
This dif-fers from the standard notion of distributional sim-ilarity which involves finding similarity between co-occurrence vectors, where there is no requirement fortwo features or contexts to occur simulultaneously.Our work with distributional similarity is a gen-eralisation of the approach taken by Lin and Pantel(2001).
These authors apply the distributional sim-ilarity principle to paths in a parse tree.
A pathexists between two words if there are grammaticalrelations connecting them in a sentence.
For exam-ple, in the sentence ?John found a solution to theproblem,?
there is a path between ?found?
and ?so-lution?
because solution is the direct object of found.Contexts of this path, in this sentence, are then thegrammatical relations <ncsubj, John> and <iobj,problem> because these are grammatical relationsassociated with either end of the path.
In their workon QA, Lin and Pantel restrict the grammatical re-lations considered to two ?slots?
at either end of thepath where the word occupying the slot is a noun.Co-occurrence vectors for paths are then built up us-ing evidence from multiple occurrences of the pathsin corpus data, for which similarity can then be cal-culated using a standard metric (e.g., Lin (1998)).In our work, we extend the notion of distributionalsimilarity from linear paths to trees.
This allows usto compute distributional similarity for any part ofan expression, of arbitrary length and complexity(although, in practice, we are still limited by datasparseness).
Further, we do not make any restric-tions as to the number or types of the grammaticalrelation contexts associated with a tree.4 Empirical EvidencePractically demonstrating our proposal requires asource of paraphrases.
We first looked at the MSRparaphrase corpus (Dolan et al, 2004) since it con-tains a large number of sentences close enough inmeaning to be considered paraphrases.
However, in-spection of the data revealed that the lexical overlapbetween the pairs of paraphrasing sentences in thiscorpus is very high.
The average word overlap (i.e.,the proportion of exactly identical word forms) cal-culated over the sentences paired by humans in thetraining set is 0.70, and the lowest overlap4 for suchsentences is 0.3.
This high word overlap makes thisa poor source of examples for us, since we wish tostudy similarity between phrases which do not sharesemantically similar words.4A possible reason for this is that candidate sentenceswere first identified automatically.9Consequently, for our purposes, the Pascal TextualEntailment Recognition Challenge dataset is a moresuitable source of paraphrase data.
Here the averageword overlap between textually entailing sentences is0.39 and the lowest overlap is 0.
This allows us toeasily find pairs of sub-parses which do not share sim-ilar words.
For example, in paraphrase pair id.19, wecan see that ?reduce the risk of diseases?
entails ?hashealth benefits?.
Similarly in pair id.20, ?may keepyour blood glucose from rising too fast?
entails ?im-proves blood sugar control,?
and in id.570, ?chargedin the death of?
entails ?accused of having killed.
?In this last example there is semantic similaritybetween the words used.
The word charged is seman-tically similar to accused.
However, it is not possibleto swap the two words in these contexts since we donot say ?charged of having killed.?
Further, there isan obvious semantic connection between the wordsdeath and killed, but being different parts of speechthis would be easily missed by traditional distribu-tional methods.Consequently, in order to demonstrate the poten-tial of our method, we have taken the phrases ?reducethe risk of diseases?, ?has health benefits?, ?chargedin the death of?
and ?accused of having killed?, con-structed corpora for the phrases and their compo-nents and then computed distributional similaritybetween pairs of phrases and their respective com-ponents.
Under our hypotheses, paraphrases will bemore similar than non-paraphrases and there will beno clear relation between the similarity of phrases asa whole and the similarity of their components.We now discuss corpus construction and distribu-tional similarity calculation in more detail.4.1 Corpus ConstructionIn order to compute distributional similarity betweensub-parses, we need to have seen a large number ofoccurrences of each sub-parse.
Since data sparse-ness rules out using traditional corpora, such as theBritish National Corpus (BNC), we constructed acorpus for each phrase by mining the web.
We alsoconstructed a similar corpus for each component ofeach phrase.
For example, for phrase 1, we con-structed corpora for ?reduce the risk of diseases?,?reduce?
and ?the risk of diseases?.
We do this in or-der to avoid only have occurrences of the componentsin the context of the larger phrase.
Each corpus wasconstructed by sending the phrase as a quoted stringto Altavista.
We took the returned list of URLs (upto the top 1000 where more than 1000 could be re-turned), removed duplicates and then downloadedthe associated files.
We then searched the files forthe lines containing the relevant string and addedPhrase Types Tokensreduce the risk of diseases 156 389reduce 3652 14082the risk of diseases 135 947has health benefits 340 884has 3709 10221health benefits 143 301charged in the death of 624 1739charged in 434 1011the death of 348 1440accused of having killed 88 173accused of 679 1760having killed 569 1707Table 1: Number of feature types and tokens ex-tracted for each Phraseeach of these to the corpus file for that phrase.
Eachcorpus file was then parsed using the RASP parser(version 3.?)
ready for feature extraction.4.2 Computing Distributional SimilarityFirst, a feature extractor is run over each parsed cor-pus file to extract occurrences of the sub-parse andtheir features.
The feature extractor reads in a tem-plate for each phrase in the form of dependency re-lations over lemmas.
It checks each sentence parseagainst the template (taking care that the same wordform is indeed the same occurrence of the word in thesentence).
When a match is found, the other gram-matical relations5 for each word in the sub-parse areoutput as features.
When the sub-parse is only aword, the process is simplified to finding grammati-cal relations containing that word.The raw feature file is then converted into a co-occurrence vector by counting the occurrences ofeach feature type.
Table 1 shows the number of fea-ture types and tokens extracted for each phrase.
Thisshows that we have extracted a reasonable numberof features for each phrase, since distributional sim-ilarity techniques have been shown to work well forwords which occur more than 100 times in a givencorpus (Lin, 1998; Weeds and Weir, 2003).We then computed the distributional similarity be-tween each co-occurrence vector using the ?-skewdivergence measure (Lee, 1999).
The ?-skew diver-gence measure is an approximation to the Kullback-Leibler (KL) divergence meassure between two dis-tributions p and q:D(p||q) =?xp(x)logp(x)q(x)5We currently retain all of the distinctions betweengrammatical relations output by RASP.10The ?-skew divergence measure is designed to beused when unreliable maximum likelihood estimates(MLE) of probabilities would result in the KL diver-gence being equal to ?.
It is defined as:dist?
(q, r) = D(r||?.q + (1?
?
).r)where 0 ?
?
?
1.
We use ?
= 0.99, since thisprovides a close approximation to the KL divergencemeasure.
The result is a number greater than orequal to 0, where 0 indicates that the two distribu-tions are identical.
In other words, a smaller distanceindicates greater similarity.The reason for choosing this measure is that itcan be used to compute the distance between anytwo co-occurrence vectors independent of any infor-mation about other words.
This is in contrast tomany other measures, e.g., Lin (1998), which use theco-occurrences of features with other words to com-pute a weighting function such as mutual information(MI) (Church and Hanks, 1989).
Since we only havecorpus data for the target phrases, it is not possiblefor us to use such a measure.
However, the ?-skewdivergence measure has been shown (Weeds, 2003)to perform comparably with measures which use MI,particularly for lower frequency target words.4.3 ResultsThe results, in terms of ?-skew divergence scores be-tween pairs of phrases, are shown in Table 2.
Eachset of three lines shows the similarity score betweena pair of phrases and then between respective pairsof components.
In the first two sets, the phrasesare paraphrases whereas in the second two sets, thephrases are not.From the table, there does appear to be some po-tential in the use of distributional similarity betweensub-parses to identify potential paraphrases.
In thefinal two examples, the paired phrases are not se-mantically similar, and as we would expect, their re-spective distributional similarities are less (i.e., theyare further apart) than in the first two examples.Further, we can see that there is no clear relationbetween the similarity of two phrases and the simi-larity of respective components.
However in 3 out of4 cases, the similarity between the phrases lies be-tween that of their components.
In every case, thesimilarity of the phrases is less than the similarityof the verbal components.
This might be what onewould expect for the second example since the com-ponents ?charged in?
and ?accused of?
are seman-tically similar.
However, in the first example, wewould have expected to see that the similarity be-tween ?reduce the risk of diseases?
and ?has healthPhrase 1 Phrase 2 Dist.reduce the risk of diseases has health benefits 5.28reduce has 4.95the risk of diseases health benefits 5.58charged in the death of accused of having killed 5.07charged in accused of 4.86the death of having killed 6.16charged in the death of has health benefits 6.04charged in has 5.54the death of health benefits 4.70reduce the risk of diseases accused of having killed 6.09reduce accused of 5.77the risk of diseases having killed 6.31Table 2: ?-skew divergence scores between pairs ofphrasesbenefits?
to be greater than either pair of compo-nents, which it is not.
The reason for this is not clearfrom just these examples.
However, possibilities in-clude the distributional similarity measure used, thefeatures selected from the corpus data and a combi-nation of both.
It may be that single words tend toexhibit greater similarity than phrases due to theirgreater relative frequencies.
As a result, it may benecessary to factor in the length or frequency of asub-parse into distributional similarity calculationsor comparisons thereof.5 Conclusions and Further WorkIn conclusion, it is clear that components of phrasesdo not need to be semantically similar for the encom-passing phrases to be semantically similar.
Thus,it is necessary to develop techniques which estimatethe semantic similarity of two phrases directly ratherthan combining similarity scores calculated for pairsof words.Our approach is to find the distributional similar-ity of the sub-parses associated with phrases by ex-tending general techniques for finding lexical distri-butional similarity.
We have illustrated this methodfor examples, showing how data sparseness can beovercome using the web.We have shown that finding the distributional sim-ilarity between phrases, as outlined here, may havepotential in identifying paraphrases.
In our exam-ples, the distributional similarities of paraphraseswas higher than non-paraphrases.
However, obvi-ously, more extensive evaluation of the technique isrequired before drawing more definite conclusions.In this respect, we are currently in the pro-cess of developing a gold standard set of similarphrases from the Pascal Textual Entailment Chal-11lenge dataset.
This task is not trivial since, eventhough pairs of sentences are already identified aspotential paraphrases, it is still necessary to ex-tract pairs of phrases which convey roughly the samemeaning.
This is because 1) some pairs of sentencesare almost identical in word content and 2) somepairs of sentences are quite distant in meaning sim-ilarity.
Further, it is also desirable to classify ex-tracted pairs of paraphrases as to whether they arelexical, syntactic, semantic or inferential in nature.Whilst lexical (e.g.
?to gather?
is similar to ?to col-lect?)
and syntactic (e.g.
?Cambodian sweatshop?is equivalent to ?sweatshop in Cambodia?)
are of in-terest, our aim is to extend lexical techniques to thesemantic level (e.g.
?X won presidential election?
issimilar to ?X became president?).
Once our analysisis complete, the data will be used to evaluate vari-ations on the technique proposed herein and also tocompare it empirically to other techniques such asthat of Lin and Pantel (2001).ReferencesRegina Barzilay and Noemie Elhadad.
2003.
Sentencealignment for monolingual comparable corpora.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP2003), pages25?33, Sapporo, Japan.Edward Briscoe and John Carroll.
1995.
Developing andevaluating a probabilistic lr parser of part-of-speechand punctuation labels.
In 4th ACL/SIGDAT Inter-national Workshop on Parsing Technologies, pages 48?58.Kenneth W. Church and Patrick Hanks.
1989.
Wordassociation norms, mutual information and lexicogra-phy.
In Proceedings of the 27th Annual Conference ofthe Association for Computational Linguistics (ACL-1989), pages 76?82.James R. Curran and Marc Moens.
2002.
Improve-ments in automatic thesaurus extraction.
In ACL-SIGLEX Workshop on Unsupervised Lexical Acquisi-tion, Philadelphia.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The pascal recognising textual entailment chal-lenge.
In Proceedings of the Recognising Textual En-tailment Challenge 2005.Bill Dolan, Chris Brockett, and Chris Quirk.
2004.
Un-supervised construction of large paraphrase corpora:Exploiting massively parallel news sources.
In Pro-ceedings of the 20th International Conference on Com-putational Linguistics, Geneva.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Maayan Geffet and Ido Dagan.
2004.
Feature vectorquality and distributional similarity.
In Proceedings ofthe 20th International Conference on ComputationalLinguistics (COLING-2004), pages 247?253, Geneva.Zelig S. Harris.
1968.
Mathematical Structures of Lan-guage.
Wiley, New York.Jesus Herrera, Anselmo Penas, and Felisa Verdejo.
2005.Textual entailment recognition based on dependencyanalysis and wordnet.
In Proceedings of the Recognis-ing Textual Entailment Challenge 2005, April.Valentin Jijkoun and Maarten de Rijke.
2005.
Recognis-ing textual entailment using lexical similarity.
In Pro-ceedings of the Recognising Textual Entailment Chal-lenge 2005, April.Lillian Lee.
1999.
Measures of distributional similarity.In Proceedings of the 37th Annual Meeting of the As-sociation for Computational Linguistics (ACL-1999),pages 23?32.Dekang Lin and Patrick Pantel.
2001.
Discovery of in-ference rules for question answering.
Natural LanguageEngineering, 7(4):343?360.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of the 36th Annual Meet-ing of the Association for Computational Linguisticsand the 17th International Conference on Computa-tional Linguistics (COLING-ACL ?98), pages 768?774,Montreal.Tim Owen, Ian Wakeman, Bill Keller, Julie Weeds, andDavid Weir.
2005.
Managing the policies of non-technical users in a dynamic world.
In IEEE Workshopon Policy in Distributed Systems, Stockholm, Sweden,May.Darren Pearce.
2001.
Synonymy in collocation extrac-tion.
In Proceedings of the NAACL Workshop onWordNet and Other Lexical Resources: Applications,Extensions and Customizations, Carnegie Mellon Uni-versity, Pittsburgh.Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-tura Coppola.
2004.
Scaling web-based acquisitionof entailment relations.
In Proceedings of EmpiricalMethods in Natural Language Processing (EMNLP)2004, Barcelona.Julie Weeds and David Weir.
2003.
A general frame-work for distributional similarity.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP-2003), Sapporo, Japan.Julie Weeds, Bill Keller, David Weir, Tim Owen, andIan Wakemna.
2004.
Natural language expression ofuser policies in pervasive computing environments.
InProceedings of OntoLex2004, LREC Workshop on On-tologies and Lexical Resources in Distributed Environ-ments, Lisbon, Portugal, May.Julie Weeds.
2003.
Measures and Applications of LexicalDistributional Similarity.
Ph.D. thesis, Department ofInformatics, University of Sussex.12
