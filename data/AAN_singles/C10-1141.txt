Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1254?1262,Beijing, August 2010Discriminative Training for Near-Synonym SubstitutionLiang-Chih Yu1, Hsiu-Min Shih2, Yu-Ling Lai2, Jui-Feng Yeh3 and Chung-Hsien Wu41Department of Information Management, Yuan Ze University2Department of Mathematics, National Chung Cheng University3Department of CSIE, National Chia-Yi University4Department of CSIE, National Cheng Kung UniversityContact: lcyu@saturn.yzu.edu.twAbstractNear-synonyms are useful knowledge re-sources for many natural language applica-tions such as query expansion for informationretrieval (IR) and paraphrasing for text gen-eration.
However, near-synonyms are not nec-essarily interchangeable in contexts due totheir specific usage and syntactic constraints.Accordingly, it is worth to develop algorithmsto verify whether near-synonyms do match thegiven contexts.
In this paper, we consider thenear-synonym substitution task as a classifica-tion task, where a classifier is trained for eachnear-synonym set to classify test examplesinto one of the near-synonyms in the set.
Wealso propose the use of discriminative trainingto improve classifiers by distinguishing posi-tive and negative features for each near-synonym.
Experimental results show that theproposed method achieves higher accuracythan both pointwise mutual information (PMI)and n-gram-based methods that have beenused in previous studies.1 IntroductionNear-synonym sets represent groups of wordswith similar meaning, which are useful knowl-edge resources for many natural language appli-cations.
For instance, they can be used for queryexpansion in information retrieval (IR) (Moldo-van and Mihalcea, 2000; Bhogal et al, 2007),where a query term can be expanded by its near-synonyms to improve the recall rate.
They canalso be used in an intelligent thesaurus that canautomatically suggest alternative words to avoidrepeating the same word in the composing oftext when there are suitable alternatives in itssynonym set (Inkpen and Hirst, 2006; Inkpen,2007).
These near-synonym sets can be derivedfrom manually constructed dictionaries such asWordNet (called synsets) (Fellbaum, 1998), Eu-roWordNet (Rodr?guez et al, 1998), or clustersderived using statistical approaches (Lin, 1998).Although the words in a near-synonym sethave similar meaning, they are not necessarilyinterchangeable in practical use due to their spe-cific usage and collocational constraints.
Pearce(2001) presented an example of collocationalconstraints for the context ?
coffee?.
In thegiven near-synonym set {strong, powerful}, theword ?strong?
is more suitable than ?powerful?to fill the gap, since ?powerful coffee?
is an anti-collocation.
Inkpen (2007) also presented severalexamples of collocations (e.g.
ghastly mistake)and anti-collocations (e.g.
ghastly error).
Yu etal.
(2007) described an example of the contextmismatch problem for the context ?
underthe bay?
and the near-synonym set {bridge,overpass, viaduct, tunnel} that represents themeaning of a physical structure that connectsseparate places by traversing an obstacle.
Theoriginal word (target word) in the given contextis ?tunnel?, and cannot be substituted by theother words in the same set since all the substitu-tions are semantically implausible.
Accordingly,it is worth to develop algorithms to verifywhether near-synonyms do match the given con-texts.
Applications can benefit from this abilityto provide more effective services.
For instance,a writing support system can assist users to se-lect an alternative word that best fits a givencontext from a list of near-synonyms.In measuring the substitutability of words, theco-occurrence information between a target word1254(the gap) and its context words is commonlyused in statistical approaches.
Edmonds (1997)built a lexical co-occurrence network from 1989Wall Street Journal to determine the near-synonym that is most typical or expected in agiven context.
Inkpen (2007) used the pointwisemutual information (PMI) formula to select thebest near-synonym that can fill the gap in agiven context.
The PMI scores for each candi-date near-synonym are computed using a largerweb corpus, the Waterloo terabyte corpus, whichcan alleviate the data sparseness problem en-countered in Edmonds?
approach.
FollowingInkpen?s approach, Gardiner and Dras (2007)also used the PMI formula with a different cor-pus (the Web 1T 5-gram corpus) to explorewhether near-synonyms differ in attitude.Yu et al (2007) presented a method to com-pute the substitution scores for each near-synonym based on n-gram frequencies obtainedby querying Google.
A statistical test is then ap-plied to determine whether or not a target wordcan be substituted by its near-synonyms.
Thedataset used in their experiments are derivedfrom the OntoNotes copus (Hovy et al, 2006;Pradhan et al, 2007), where each near-synonymset corresponds to a sense pool in OntoNotes.Another direction to the task of near-synonymsubstitution is to identify the senses of a targetword and its near-synonyms using word sensedisambiguation (WSD), comparing whether theywere of the same sense (McCarthy, 2002; Daganet al, 2006).
Dagan et al (2006) described thatthe use of WSD is an indirect approach since itrequires the intermediate sense identificationstep, and thus presented a sense matching tech-nique to address the task directly.In this paper, we consider the near-synonymsubstitution task as a classification task, where aclassifier is trained for each near-synonym set toclassify test examples into one of the near-synonyms in the set.
However, near-synonymsshare more common context words (features)than semantically dissimilar words in nature.Such similar contexts may decrease classifiers?ability to discriminate among near-synonyms.Therefore, we propose the use of a superviseddiscriminative training technique (Ohler et al,1999; Kuo and Lee, 2003; Zhou and He, 2009)to improve classifiers by distinguishing positiveand negative features for each near-synonym.
Toour best knowledge, this is the first study thatuses discriminative training for near-synonym orlexical substitution.
The basic idea of discrimi-native training herein is to adjust feature weightsaccording to the minimum classification error(MCE) criterion.
The features that contribute todiscriminating among near-synonyms will re-ceive a greater positive weight, whereas thenoisy features will be penalized and might re-ceive a negative weight.
This re-weightingscheme helps increase the separation of the cor-rect class against its competing classes, thus im-proves the classification performance.The proposed supervised discriminative train-ing is compared with two unsupervised methods,the PMI-based (Inkpen, 2007) and n-gram-based(Yu et al, 2007) methods.
The goal of theevaluation is described as follows.
Given a near-synonym set and a sentence with one of the near-synonyms in it, the near-synonym is deleted toform a gap in this sentence.
Figure 1 shows anexample.
Each method is then applied to predictan answer (best near-synonym) that can fill thegap.
The possible candidates are all the near-synonyms (including the original word) in thegiven set.
Ideally, the correct answers should beprovided by human experts.
However, such datais usually unavailable, especially for a large setof test examples.
Therefore, we follow Inkpen?sexperiments to consider the original word as thecorrect answer.
The proposed methods can thenbe evaluated by examining whether they can re-store the original word by filling the gap with thebest near-synonym.The rest of this work is organized as follows.Section 2 describes the PMI and n-gram-basedmethods for near-synonym substitution.
Section3 presents the discriminative training technique.Section 4 summarizes comparative results.
Con-clusions are finally drawn in Section 5.Sentence: This will make the           messageeasier to interpret.Original word: errorNear-synonym set: {error, mistake, oversight}Figure 1.
Example of a near-synonym set and asentence to be evaluated.12552 Unsupervised Methods2.1 PMI-based methodThe mutual information can measure the co-occurrence strength between a near-synonymand the words in a given context.
A higher mu-tual information score indicates that the near-synonym fits well in the given context, thus ismore likely to be the correct answer.
The point-wise mutual information (Church and Hanks,1991) between two words x and y is defined as2( , )( , ) log ,( ) ( )P x yPMI x yP x P y=            (1)where ( , ) ( , )P x y C x y N=  denotes the prob-ability that x and y co-occur; ( , )C x y  is thenumber of times x and y co-occur in the corpus,and N is the total number of words in the corpus.Similarly, ( ) ( )P x C x N= , where C(x) is thenumber of times x occurs in the corpus, and( ) ( )P y C y N= , where C(y) is the number oftimes y occurs in the corpus.
Therefore, (1) canbe re-written as2( , )( , ) log .
( ) ( )C x y NPMI x yC x C y?= ?
(2)Inkpen (2007) computed the PMI scores for eachnear-synonym using the Waterloo terabyte cor-pus and a context window of size 2k (k=2).Given a sentence s with a gap,1 1 2... ...      ... ...k k ks w w w w+= , the PMI score fora near-synonym NSi to fill the gap is defined as121( , ) ( , )( , ).== += +?
?kj j iikj ii kPMI NS s PMI NS wPMI NS w(3)The near-synonym with the highest score is con-sidered as the answer.
In this paper, we use theWeb 1T 5-gram corpus to compute PMI scores,the same as in (Gardiner and Dras, 2007).
Thefrequency counts C(?)
are retrieved from thiscorpus in the same manner within the 5-gramboundary.2.2 N-gram-based methodThe n-grams can capture contiguous word asso-ciations in given contexts.
Given a sentence witha gap, the n-gram scores for each near-synonymare computed as follows.
First, all possible n-grams containing the gap are extracted from thesentence.
Each near-synonym then fills the gapto compute a normalized frequency according to( )log ( ) 1( ) ,log ( )jjiNSiNSjC ngramZ ngramC NS+=         (4)where ( )jiNSC ngram  denotes the frequency of ann-gram containing a near-synonym, ( )jC NSdenotes the frequency of a near-synonym, and( )jiNSZ ngram  denotes the normalized frequencyof an n-gram, which is used to reduce the effectof high frequencies on measuring n-gram scores.All of the above frequencies are retrieved fromthe Web 1T 5-gram corpus.The n-gram score for a near-synonym to fillthe gap is computed as11( , ) ( ),== ?
jR ij NSiNGRAM NS s Z ngramR(5)where ( , )jNGRAM NS s  denotes the n-gramscore of a near-synonym, which is computed byaveraging the normalized frequencies of all then-grams containing the near-synonym, and R isthe total number of n-grams containing the near-synonym.
Again, the near-synonym with thehighest score is the proposed answer.
We hereinuse the 4-gram frequencies to compute n-gramscores as Yu et al (2007) reported that the use of4-grams achieved higher performance than theuse of bigrams and trigrams.3 Discriminative Training3.1 ClassifierThe supervised classification technique can alsobe applied to for near-synonym substitution.Each near-synonym in a set corresponds to aclass.
The classifiers for each near-synonym setare built from the labeled training data, i.e., acollection of sentences containing the near-synonyms.
Such training data is easy to obtainsince it requires no human annotation.
The train-ing data used herein is collected by extractingthe 5-grams containing the near-synonyms fromthe Web 1T 5-gram corpus.
The features usedfor training are the words occurring in the con-text of the near-synonyms in the 5-grams.1256For each near-synonym set, an F K?
feature-class matrix, denoted as M, is created for classi-fication.
The rows represent the F distinct words(features) and the columns represent the K near-synonyms (classes) in the same set.
Each entryin the matrix, mij, represents a weight of word irespect to near-synonym j, which is computed asthe number of times word i appears in the con-texts of near-synonym j divided by the totalnumber of context words of near-synonym j.This frequency-based weight can then be trans-formed into a probabilistic form, i.e., divided bythe sum of the weights of word i respect to allnear-synonyms.
Each test sentence is also trans-formed into an F-dimensional feature vector.
Let1[ ,..., ,..., ]= i Fx x x x  denote the feature vector ofan input sentence.
The classification is per-formed by computing the cosine similarity be-tween x and the column vectors (near-synonyms)in the matrix, defined as12 21 1arg max cos( , )arg max                             (6)arg max ,jj jjj jFi ijiF Fji iji iNS x mx mx mx mx m?== ==== ??
?iwhere jm  is the j-th column vector in the matrixM.
The near-synonym with the highest cosinesimilarity score, ?jNS , is the predicted class ofthe classifier.3.2 Minimum classification error criterionAccording to the decision rule of the classifier, aclassification error will occur when the near-synonym with the highest cosine score is not thecorrect class.
Table 1 shows some examples,where Example 3 is an example of misclassifica-tion.
On the other hand, although Example 2 is acorrect classification, it might be an ambiguouscase to classifiers since the scores are closeamong classes.
Therefore, if we can increase theseparation of the correct class from its compet-ing ones, then the classification performance canbe improved accordingly.
This can be accom-plished by adjusting the feature weights of thematrix M that have direct influence on the com-putation of cosine scores.
The discriminativetraining performs the adjustment in the trainingphase according to the minimum classificationerror criterion.
The detailed steps are as follows.Given an input vector x, the classifier com-putes the cosine scores between x and each classusing (6).
The discriminant function for a classcan thus be defined as the cosine measure; that is,( , ) cos( , ).=j jg x M x m             (7)where j denotes a class in K. Since the correctclass of each input vector is known in the train-ing phase, we can determine whether or not theinput vector is misclassified by comparing thediscriminant function (cosine score) of the cor-rect class against its competing classes.
In thecase of misclassification, the cosine score of thecorrect class will be smaller than the competingcosine scores.
Let k be the correct class of x, themisclassification function can be defined as( , )  ( , )  ( , ),k k kd x M g x M G x M= ?
+            (8)where ( , )kg x M  is the discriminant function forthe correct class k, and ( , )kG x M  is the anti-discriminant function that represents the other1K ?
competing classes, defined as11( , ) ( , ) ,1k jj kG x M g x MK????
?= ?
???
??
(9)When 1?
= , the anti-discriminant function( , )kG x M  is the average of all the competingcosine scores.
With the increase of ?
,( , )kG x M is gradually dominated by the biggestExample1 2 31 1( , ) cos( , )=g x M x m  0.9* 0.6* 0.82 2( , ) cos( , )=g x M x m  0.3 0.5 0.3*3 3( , ) cos( , )=g x M x m  0.2 0.4 0.1max ( , )?
=j k ig x M  0.3 0.5 0.8( , ) =kd x M  -0.6 -0.1 0.5( , ) =kl x M(?=5)0.047 0.378 0.924Table 1.
Examples of correct classificationand misclassification.
* denotes the scores of thecorrect class.1257competing class.
In the extreme case, i.e.,?
??
, the anti-discriminant function becomes( , ) max  ( , ).k jj kG x M g x M?=          (10)The misclassification function in (8) can thus berewritten as( , )  ( , ) max  ( , ),k k jj kd x M g x M g x M?= ?
+    (11)In this case, the classification error is determinedby comparing the discriminant function of thecorrect class against that of the biggest compet-ing class.
Obviously, ( , ) 0kd x M >  implies aclassification error.
For instance, in Example 3,the discriminant function for the correct class is2 ( , ) 0.3g x M = , and that of the biggest compet-ing class is 1 3max( ( , ), ( , )) 0.8=g x M g x M , thusthe classification error is ( , ) 0.5=kd x M .
On theother hand, the classification error will be anegative value for correct classifications, asshown in Example 1 and 2.Intuitively, a greater classification error alsoresults in a greater loss.
We herein use the sig-moid function as the class loss function; that is,1( , ) ( ) ,1 exp kk k dl x M l d ?
?= = +          (12)where ?
is a constant that controls the slope ofthe sigmoid function.
The sigmoid functionmaps the values of classification error within therange of 0 to 1.
For correct classifications, agreater separation of the correct class from thebiggest competing class leads to a greater nega-tive value of dk, i.e., a smaller classification error,resulting in a smaller loss tends asymptoticallyto 0 (Example 1), whereas a moderate loss isyielded if the separation was close (Example 2).For the cases of misclassification, a greater sepa-ration leads to a greater positive value of dk, i.e.,a greater classification error, resulting in agreater loss tends asymptotically to 1 (Example3).
The overall loss of the entire training set canthen be estimated as11( ) ( , ),= ?= ?
?kKkk x CL M l x MQ(13)where Ck denotes the set of training vectors ofclass k, and Q is the total number of vectors inthe training set.
The goal now is to minimize theloss.
According to the above discussions on thethree examples, to minimize the loss is to mini-mize the classification error, and to improve theseparation of the correct class against its compet-ing classes.
This can be accomplished by adjust-ing the feature weights of the matrix M to distin-guish positive and negative features for eachclass.
We herein adopt a gradient descentmethod such as the generalized probabilistic de-scent (GPD) algorithm (Katagiri et al, 1998) toupdate the matrix M. The detailed steps are asfollows.Let the feature weights of the matrix M be theparameter set to be adjusted.
The adjustment isperformed iteratively according to the followingupdate formula.
( 1) ( ) ( ) ( )( , ),?+ = ?
?t t t tt kM M l x M         (14)where t denotes the t-th iteration, ?
t  is an ad-justment step of a small positive real number,and ( ) ( )( , )?
t tkl x M is the gradient of the lossfunction, which is computed by the followingtwo parts( ) ( )( ) ( ) ( , )( , ) ,t tt t k kkk ijl d x Ml x Md m?
??
= ?
?
(15)where( )(1 ( )),k k k k kkll d l dd??
= ??
(16)and from (7), (8), and (9),( ) ( )( ) ( ) 1( ),                                           if( , ) ( , ) ( , ) ,,  if( , )?????
=???
= ?
??
??
?it tt tkk ji tijjj kx j kd x M G x M g x Mx j kmg x M(17)where xi is an element of the input vector x. Byreplacing ( , )k t tl x M?
in (14) with the two partsin (15), at each iteration each feature weight mijin M is adjusted by( )( 1)( ) ( ) 1( )( ),                                          if.
( , ) ( , ), if( , )????+????
+ =?
?
?= ?
??
?
??
??
?t kij t iktt tijk jt kij t i tk jj klm x j kdmG x M g x Mlm x j kd g x M(18)The weight xi represents whether or not a dimen-sion word occurs in an input sentence.
A zero1258weight indicates that the dimension word doesnot occur in the input sentence, thus the corre-sponding dimension of each column vector willnot be adjusted.
On the contrary, the correspond-ing dimension of the column vector of the cor-rect class ( j k= ) is adjusted by adding a value,while those of the competing classes ( j k? )
areadjusted by subtracting a value from them.
Aftera sequence of adjustments over the training set,the positive and negative features can be distin-guished by adjusting their weights that result in agreater positive or negative value for each ofthem.
The separation of the correct class againstits competing ones can thus be increased.The weight adjustment in (18) is in proportionto the adjustment step ?
t  and the slope of thesigmoid function k kl d?
?
.
The adjustment step?
t  can be determined empirically.
As (16) shows,the slope k kl d?
?
converges asymptotically tozero as the classification error dk approaches to avery large (or small) value.
This leads to a smallweight adjustment.
For instance, the weight ad-justment in Example 1 is small due to a smallvalue of dk, or, say, due to a large separation be-tween the correct class and its competing ones.This is reasonable because classifiers often per-form well in such cases.
Similarly, the weightadjustment in Example 3 (misclassification) isalso small due to a large value of dk.
A greateradjustment is not employed because such a largeseparation is difficult to be reduced significantly.Additionally, over-adjusting some features mayintroduce negative effects on other useful fea-tures in the matrix.
Therefore, discriminativetraining is more effective on the cases with amoderate value of dk, like Example 2.
Such casesusually fall within the decision boundary andtend to be confusing to classifiers.
Hence, im-proving the separation on such cases helps sig-nificantly improve the classification performance.4 Experimental Results4.1 Experiment setup1) Data: The near-synonym sets used for ex-periments included the seven sets (Exp1) and theeleven sets (Exp2) used in the previous studies(Edmonds, 1997; Inkpen, 2007; Gardiner andDras, 2007), as shown in Table 2.
The Web 1T5-gram corpus was used to build classifiers,where the corpus was randomly split into a train-ing set, a development set, and a test set with an8:1:1 ratio.
For efficiency consideration, we ran-domly sampled up to 100 5-grams from the testset for each near-synonym.
This sampling pro-cedure was repeated five times for evaluation ofthe classifiers.2) Classifiers: The classifiers were imple-mented using PMI, n-grams, and discriminativetraining (DT) methods, respectively.PMI: Given a near-synonym set and a test 5-gram with a gap, the PMI scores for each near-synonym were calculated using (3), where thesize of the context window k was set to 2.
Thefrequency counts between each near-synonymand its context words were retrieved from thetraining set.NGRAM: For each test 5-gram with a gap, allpossible 4-grams containing the gap were firstextracted (excluding punctuation marks).
Theaveraged 4-gram scores for each near-synonymwere then calculated using (5).
Again, the fre-quency counts of the 4-grams were retrievedfrom the training set.DT: For each near-synonym set, the matrix Mwas built from the training set.
Each 5-gram inthe development set was taken as input to itera-tively compute the cosine score, loss, classifica-tion error, respectively, and finally to adjust thefeature weights of M. The parameters of DT in-cluded ?
for the anti-discriminative function, ?0 20 40 60 80 100Iteration0.680.70.720.740.76AccuracyTest setDevelopment setFigure 2.
The change of classification accuracyduring discriminative training.1259for the sigmoid function, and t?
for the adjust-ment step.
The settings, 25?
= , 35?
= , and310?
?=t , were determined by performing DTfor several iterations through the training set.These setting were used for the following ex-periments.3) Evaluation metric: The answers proposedby each classifier are the near-synonyms withthe highest score.
The correct answers are thenear-synonyms originally in the gap of the test 5-grams.
The performance is measure by the accu-racy, which is defined as the number of correctanswers made by each classifier, divided by thetotal number of test 5-grams.In the following sections, we first demonstratethe effect of DT on classification performance,followed by the comparison of the classifiers.4.2 Evaluation on discriminative trainingThis experiment is to investigate the perform-ance change during discriminative training.
Fig-ure 2 shows the accuracy at the first 100 itera-tions for both development set and test set, withthe 8th set in Exp2 as an example.
The accuracyincreased rapidly in the first 20 iterations, andstabilized after the 40th iteration.
The discrimi-native training is stopped until the accuracy hasnot been changed over 30 iterations or the 300thiteration has been reached.Accuracy (%)No.
Near-synonym set No.
ofcases NGRAM PMI COS DT1.
difficult, hard, tough 300 58.60 61.67 60.13 63.132. error, mistake, oversight 300 68.47 78.33 77.20 79.203. job, task, duty 300 68.93 70.40 74.00 75.674. responsibility, burden, obligation, commitment 400 69.80 66.95 68.75 69.555. material, stuff, substance 300 70.20 67.93 71.07 75.136. give, provide, offer 300 58.87 66.47 64.13 68.277. settle, resolve 200 69.30 68.10 77.10 84.10Exp1 2,100 66.33 68.50 69.94 72.891. benefit, advantage, favor, gain, profit 500 71.44 69.88 69.44 71.362. low, gush, pour, run, spout, spurt, squirt, stream 800 65.45 65.00 68.68 71.083. deficient, inadequate, poor, unsatisfactory 400 65.65 69.40 70.35 74.354.afraid, aghast, alarmed, anxious, apprehensive,fearful, frightened, scared, terror-stricken* 789 49.84 44.74 47.00 49.335.disapproval, animadversion*, aspersion*, blame,criticism, reprehension* 300 72.80 79.47 80.00 82.536. mistake, blooper, blunder, boner, contretemps*,  error, faux pas*, goof, slip, solecism* 618 62.27 59.61 68.41 71.657. alcoholic, boozer*, drunk, drunkard, lush, sot 433 64.90 80.65 77.88 84.348. leave, abandon, desert, forsake 400 65.85 66.05 69.35 74.359.opponent, adversary, antagonist, competitor,enemy, foe, rival 700 58.51 59.51 63.31 67.1410.thin, lean, scrawny, skinny, slender, slim, spare,svelte, willowy*, wiry 734 57.74 61.99 55.72 64.5811. lie, falsehood, fib, prevarication*,  rationalization, untruth 425 57.55 63.58 69.46 74.21Exp2 6,099 61.69 63.32 65.15 69.26Table 2.
Accuracy of classifiers on Exp1 (7 sets) and Exp2 (11 sets).
The words marked with * areexcluded from the experiments because their 5-grams are very rare in the corpus.12604.3 Comparative resultsTable 2 shows the comparative results of theclassification accuracy for the 18 near-synonymsets (Exp1 + Exp2).
The accuracies for eachnear-synonym set were the average accuracies ofthe five randomly sampled test sets.
The cosinemeasure without discrimination training (COS)was also considered for comparison.
The resultsshow that NGRAM performed worst among thefour classifiers.
The major reason is that not all4-grams of the test examples can be found in thecorpus.
Instead of contiguous word associationsused by NGRAM, PMI considers the words inthe contexts independently to select the bestsynonyms.
The results show that PMI achievedbetter performance than NGRAM.
The two su-pervised methods, COS and DT, outperformedthe two unsupervised methods, NGRAM andPMI.
As indicated in the bold numbers, using thesupervised method alone (without DT), COSyielded higher average accuracy by 5% and 2%over NGRAM and PMI, respectively, on Exp1,and by 6% and 3%, respectively, on Exp2.
WhenDT was employed, the average accuracy wasfurther improved by 4% and 6% on Exp1 andExp2, respectively, compared with COS.The use of DT can improve the classificationaccuracy mainly because it can adjust the featureweights iteratively to improve the separation be-tween the correct class and its competing ones,which helps tackle the ambiguous test examplesthat fall within the decision boundary.
Table 3presents several positive and negative featuresfor the near-synonym set {mistake, error, over-sight}.
The feature weights were adjusted ac-cording to their contributions to discriminatingamong the near-synonyms in the set.
For in-stance, the features ?made?
and ?biggest?
bothreceived a positive value for the class ?mistake?,and a negative value for the competing classes?error?
and ?oversight?.
These positive andnegative weights help distinguish useful featuresfrom noisy ones for classifier training.
On theother hand, if the feature weights were evenlydistributed among the classes, these featureswould not be unlikely to contribute to the classi-fication performance.4.4 Accuracy of Rank 1 and Rank 2The accuracy presented in Table 2 was com-puted based on the classification results at Rank1, i.e., a test sample was considered correctlyclassified only if the near-synonym with thehighest score was the word originally in the gapof the test sample.
Similarly, the accuracy atRank 2 can be computed by considering the toptwo near-synonyms proposed by each classifier.That is, if either the near-synonym with thehighest or the second highest score was the cor-rect answer, the test sample was considered cor-rectly classified.
Table 4 shows the accuracy ofRank 1 and Rank 2 for each classifier.
The re-sults show that the improvement of Rank 1 accu-racy on Exp1 was about 20 to 30 percentagepoints, and was 25.76 in average.
For Exp2, theaverage improvement was 19.80 percentagepointsNear-synonym setFeaturesmistake error oversightmade 0.076 -0.004 -0.005biggest 0.074 -0.001 -0.004message -0.004 0.039 -0.010internal 0.001 0.026 -0.001supervision -0.001 -0.006 0.031audit -0.002 -0.003 0.028Table 3.
Example of feature weights after dis-criminative training.Exp1 Rank 1 Rank 2 Diff.NGRAM 66.33% 79.35% +19.63%PMI 68.50% 88.99% +29.91%COS 69.94% 89.93% +28.58%DT 72.89% 91.06% +24.93%Exp2 Rank 1 Rank 2 Diff.NGRAM 61.69% 68.48% +11.01%PMI 63.32% 79.11% +24.94%COS 65.15% 80.52% +23.59%DT 69.26% 82.86% +19.64%Table 4.
Accuracy of Rank 1 and Rank 2 foreach classifier.12615 ConclusionThis work has presented the use of discrimina-tive training for near-synonym substitution.
Thediscriminative training can improve classifica-tion performance by iteratively re-weighting thepositive and negative features for each class.This helps improve the separation of the correctclass against its competing ones, making classi-fiers more effective on ambiguous cases close tothe decision boundary.
Experimental resultsshow that the supervised discriminative trainingtechnique achieves higher accuracy than the twounsupervised methods, the PMI-based and n-gram-based methods.
The availability of a largelabeled training set alo encourages the use ofthe proposed supervised method.Future work will investigate on the use ofmultiple features for discriminating among near-synonyms.
For instance, the predicate-argumentstructure, which can capture long-distance in-formation, will be combined with currently usedlocal contextual features to boost the classifica-tion performance.
More experiments will also beconducted to evaluate classifiers?
ability to rankmultiple answers.ReferencesJ.
Bhogal, A. Macfarlane, and P. Smith.
2007.
A Re-view of Ontology based Query Expansion.
Infor-mation Processing & Management, 43(4):866-886.K.
Church and P. Hanks.
1991.
Word AssociationNorms, Mutual Information and Lexicography.Computational Linguistics, 16(1):22-29.I.
Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein,and C. Strapparava.
2006.
Direct Word SenseMatching for Lexical Substitution.
In Proc.
ofCOLING/ACL-06, pages 449-456.P.
Edmonds.
1997.
Choosing the Word Most Typicalin Context Using a Lexical Co-occurrence Net-work.
In Proc.
of ACL-97, pages 507-509.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press, Cambridge, MA.M.
Gardiner and M. Dras.
2007.
Exploring Ap-proaches to Discriminating among Near-Synonyms,In Proc.
of the Australasian Technology Workshop,pages 31-39.E.
H. Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
OntoNotes: The 90% Solu-tion.
In Proc.
of HLT/NAACL-06, pages 57?60.D.
Inkpen.
2007.
Near-Synonym Choice in an Intelli-gent Thesaurus.
In Proc.
of NAACL/HLT-07, pages356-363.D.
Inkpen and G. Hirst.
2006.
Building and Using aLexical Knowledge-base of Near-Synonym Differ-ences.
Computational Linguistics, 32(2):1-39.S.
Katagiri, B. H. Juang, and C. H. Lee.
1998.
PatternRecognition Using a Family of Design Algorithmsbased upon the Generalized Probabilistic DescentMethod, Proc.
of the IEEE, 86(11):2345-2373.H.
K. J. Kuo and C. H. Lee.
2003.
DiscriminativeTraining of Natural Language Call Routers, IEEETrans.
Speech and Audio Processing, 11(1):24-35.D.
Lin.
1998.
Automatic Retrieval and Clustering ofSimilar Words.
In Proc.
of ACL/COLING-98,pages 768-774.D.
McCarthy.
2002.
Lexical Substitution as a Taskfor WSD Evaluation.
In Proc.
of theSIGLEX/SENSEVAL Workshop on Word SenseDisambiguation at ACL-02, pages 109-115.D.
Moldovan and R. Mihalcea.
2000.
Using Wordnetand Lexical Operators to Improve InternetSearches.
IEEE Internet Computing, 4(1):34-43.U.
Ohler, S. Harbeck, and H. Niemann.
1999.
Dis-criminative Training of Language Model Classifi-ers, In Proc.
of Eurospeech-99, pages 1607-1610.D.
Pearce.
2001.
Synonymy in Collocation Extraction.In Proc.
of the Workshop on WordNet and OtherLexical Resources at NAACL-01.S.
Pradhan, E. H. Hovy, M. Marcus, M. Palmer, L.Ramshaw, and R. Weischedel.
2007.
OntoNotes: AUnified Relational Semantic Representation.
InProc.
of the First IEEE International Conferenceon Semantic Computing (ICSC-07), pages 517-524.H.
Rodr?guez, S. Climent, P. Vossen, L. Bloksma, W.Peters, A. Alonge, F. Bertagna, and A. Roventint.1998.
The Top-Down Strategy for Building Eeu-roWordNet: Vocabulary Coverage, Base Conceptsand Top Ontology, Computers and the Humanities,32:117-159.L.
C. Yu, C. H. Wu, A. Philpot, and E. H. Hovy.
2007.OntoNotes: Sense Pool Verification Using GoogleN-gram and Statistical Tests, In Proc.
of the On-toLex Workshop at the 6th International SemanticWeb Conference (ISWC-07).D.
Zhou and Y.
He.
2009.
Discriminative Training ofthe Hidden Vector State Model for Semantic Pars-ing, IEEE Trans.
Knowledge and Data Engineer-ing, 21(1):66-77.1262
