Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1466?1476,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsText-Driven Toponym Resolution using Indirect SupervisionMichael Speriosu Jason BaldridgeDepartment of LinguisticsUniversity of Texas at AustinAustin, TX 78712 USA{speriosu,jbaldrid}@utexas.eduAbstractToponym resolvers identify the specific lo-cations referred to by ambiguous place-names in text.
Most resolvers are based onheuristics using spatial relationships be-tween multiple toponyms in a document,or metadata such as population.
This pa-per shows that text-driven disambiguationfor toponyms is far more effective.
We ex-ploit document-level geotags to indirectlygenerate training instances for text classi-fiers for toponym resolution, and show thattextual cues can be straightforwardly in-tegrated with other commonly used ones.Results are given for both 19th centurytexts pertaining to the American Civil Warand 20th century newswire articles.1 IntroductionIt has been estimated that at least half of theworld?s stored knowledge, both printed and digi-tal, has geographic relevance, and that geographicinformation pervades many more aspects of hu-manity than previously thought (Petras, 2004;Skupin and Esperbe?, 2011).
Thus, there is valuein connecting linguistic references to places (e.g.placenames) to formal references to places (coor-dinates) (Hill, 2006).
Allowing for the queryingand exploration of knowledge in a geographicallyinformed way requires more powerful tools than akeyword-based search can provide, in part due tothe ambiguity of toponyms (placenames).Toponym resolution is the task of disambiguat-ing toponyms in natural language contexts to geo-graphic locations (Leidner, 2008).
It plays an es-sential role in automated geographic indexing andinformation retrieval.
This is useful for histori-cal research that combines age-old geographic is-sues like territoriality with modern computationaltools (Guldi, 2009), studies of the effect of histor-ically recorded travel costs on the shaping of em-pires (Scheidel et al, 2012), and systems that con-vey the geographic content in news articles (Teitleret al, 2008; Sankaranarayanan et al, 2009) andmicroblogs (Gelernter and Mushegian, 2011).Entity disambiguation systems such as those ofKulkarni et al (2009) and Hoffart et al (2011)disambiguate references to people and organiza-tions as well as locations, but these systems do nottake into account any features or measures uniqueto geography such as physical distance.
Here wedemonstrate the utility of incorporating distancemeasurements in toponym resolution systems.Most work on toponym resolution relies onheuristics and hand-built rules.
Some use sim-ple rules based on information from a gazetteer,such as population or administrative level (city,state, country, etc.
), resolving every instance ofthe same toponym type to the same location re-gardless of context (Ladra et al, 2008).
Others userelationships between multiple toponyms in a con-text (local or whole document) and look for con-tainment relationships, e.g.
London and Englandoccurring in the same paragraph or as the bigramLondon, England (Li et al, 2003; Amitay et al,2004; Zong et al, 2005; Clough, 2005; Li, 2007;Volz et al, 2007; Jones et al, 2008; Buscaldi andRosso, 2008; Grover et al, 2010).
Still others firstidentify unambiguous toponyms and then disam-biguate other toponyms based on geopolitical re-lationships with or distances to the unambiguousones (Ding et al, 2000).
Many favor resolutions oftoponyms within a local context or document thatcover a smaller geographic area over those that aremore dispersed (Rauch et al, 2003; Leidner, 2008;Grover et al, 2010; Loureiro et al, 2011; Zhanget al, 2012).
Roberts et al (2010) use relation-ships learned between people, organizations, andlocations from Wikipedia to aid in toponym reso-lution when such named entities are present, butdo not exploit any other textual context.1466Most of these approaches suffer from a majorweakness: they rely primarily on spatial relation-ships and metadata about locations (e.g., popu-lation).
As such, they often require nearby to-ponyms (including unambiguous or containing to-ponyms) to resolve ambiguous ones.
This reliancecan result in poor coverage when the required in-formation is missing in the context or when a doc-ument mentions locations that are neither nearbygeographically nor in a geopolitical relationship.There is a clear opportunity that most ignore:use non-toponym textual context.
Spatially rel-evant words like downtown that are not explicittoponyms can be strong cues for resolution (Hol-lenstein and Purves, 2012).
Furthermore, the con-nection between non-spatial words and locationshas been successfully exploited in data-drivenapproaches to document geolocation (Eisensteinet al, 2010, 2011; Wing and Baldridge, 2011;Roller et al, 2012) and other tasks (Hao et al,2010; Pang et al, 2011; Intagorn and Lerman,2012; Hecht et al, 2012; Louwerse and Benesh,2012; Adams and McKenzie, 2013).In this paper, we learn resolvers that use allwords in local or document context.
For example,the word lobster appearing near the toponym Port-land indicates the location is Portland in Mainerather than Oregon or Michigan.
Essentially, welearn a text classifier per toponym.
There are nomassive collections of toponyms labeled with lo-cations, so we train models indirectly using geo-tagged Wikipedia articles.
Our results show thesetext classifiers are far more accurate than algo-rithms based on spatial proximity or metadata.Furthermore, they are straightforward to combinewith such algorithms and lead to error reductionsfor documents that match those algorithms?
as-sumptions.Our primary focus is toponym resolution, so weevaluate on toponyms identified by human anno-tators.
However, it is important to consider theutility of an end-to-end toponym identification andresolution system, so we also demonstrate thatperformance is still strong when toponyms are de-tected with a standard named entity recognizer.We have implemented all the models discussedin this paper in an open source software packagecalled Fieldspring, which is available on GitHub:http://github.com/utcompling/fieldspringExplicit instructions are provided for preparingdata and running code to reproduce our results.Figure 1: Points representing the United States.2 Data2.1 GazetteerToponym resolvers need a gazetteer to obtain can-didate locations for each toponym.
Additionally,many gazetteers include other information such aspopulation and geopolitical hierarchy information.We use GEONAMES, a freely available gazetteercontaining over eight million entries worldwide.1Each location entry contains a name (sometimesmore than one) and latitude/longitude coordinates.Entries also include the location?s administrativelevel (e.g.
city or state) and its position in thegeopolitical hierarchy of countries, states, etc.GEONAMES gives the locations of regionalitems like states, provinces, and countries as singlepoints.
This is clearly problematic when we seekconnections between words and locations: e.g.
wemight learn that many words associated with theUSA are connected to a point in Kansas.
To getaround this, we represent regional locations as aset of points derived from the gazetteer.
Since re-gional locations are named in the entries for loca-tions they contain, all locations contained in theregion are extracted (in some cases over 100,000of them) and then k-means is run to find a smallerset of spatial centroids.
These act as a tractableproxy for the spatial extent of the entire region.
kis set to the number of 1?
by 1?
grid cells coveredby that region.
Figure 1 shows the points com-puted for the United States.2 A nice property ofthis representation is that it does not involve re-gion shape files and the additional programminginfrastructure they require.1Downloaded April 16, 2013 from www.geonames.org.2The representation also contains three points each inHawaii and Alaska not shown in Figure 1.1467Corpus docs toks types tokstop typestop ambavg ambmaxTRC-DEV 631 136k 17k 4356 613 15.0 857TRC-DEV-NER - - - 3165 391 18.2 857TRC-TEST 315 68k 11k 1903 440 13.7 857TRC-TEST-NER - - - 1346 305 15.7 857CWAR-DEV 228 33m 200k 157k 850 29.9 231CWAR-TEST 113 25m 305k 85k 760 31.5 231Table 1: Statistics of the corpora used for evaluation.
Columns subscripted by top give figures fortoponyms.
The last two columns give the average number of candidate locations per toponym token andthe number of candidate locations for the most ambiguous toponym.A location for present purposes is thus a set ofpoints on the earth?s surface.
The distance be-tween two locations is computed as the great circledistance between the closest pair of representativepoints, one from each location.2.2 Toponym Resolution CorporaWe need corpora with toponyms identified and re-solved by human annotators for evaluation.
TheTR-CONLL corpus (Leidner, 2008) contains 946REUTERS news articles published in August1996.
It has about 204,000 words and articlesrange in length from a few hundred words to sev-eral thousand words.
Each toponym in the corpuswas identified and resolved by hand.3 We placeevery third article into a test portion (TRC-TEST)and the rest in a development portion.
Since ourmethods do not learn from explicitly labeled to-ponyms, we do not need a training set.The Perseus Civil War and 19th Century Amer-ican Collection (CWAR) contains 341 books (58million words) written primarily about and duringthe American Civil War (Crane, 2000).
Toponymswere annotated by a semi-automated process: anamed entity recognizer identified toponyms, andthen coordinates were assigned using simple rulesand corrected by hand.
We divide CWAR into de-velopment (CWAR-DEV) and test (CWAR-TEST)sets in the same way as TR-CONLL.Table 1 gives statistics for both corpora, includ-ing the number and ambiguity of gold standardtoponyms for both as well as NER identified to-3We found several systematic types of errors in the origi-nal TR-CONLL corpus, such as coordinates being swappedfor some locations and some longitudes being zero or the neg-ative of their correct values.
We repaired many of these er-rors, though some more idiosyncratic mistakes remain.
We,along with Jochen Leidner, will release this updated versionshortly and will link to it from our Fieldspring GitHub page.ponyms for TR-CONLL.4 We use the pre-trainedEnglish NER from the OpenNLP project.52.3 Geolocated Wikipedia CorpusThe GEOWIKI dataset contains over one millionEnglish articles from the February 11, 2012 dumpof Wikipedia.
Each article has human-annotatedlatitude/longitude coordinates.
We divide the cor-pus into training (80%), development (10%), andtest (10%) at random and perform preprocessingto remove markup in the same manner as Wingand Baldridge (2011).
The training portion is usedhere to learn models for text-driven resolvers.3 Toponym ResolversGiven a set of toponyms provided via annotationsor identified using NER, a resolver must select acandidate location for each toponym (or, in somecases, a resolver may abstain).
Here, we describebaseline resolvers, a heuristic resolver based onthe usual cues used in most toponym resolvers,and several text-driven resolvers.
We also discusscombining heuristic and text-driven resolvers.3.1 Baseline ResolversRANDOM For each toponym, the RANDOM re-solver randomly selects a location from those as-sociated in the gazetteer with that toponym.POPULATION The POPULATION resolver se-lects the location with the greatest population foreach toponym.
It is generally quite effective, butwhen a toponym has several locations with largepopulations, it is often wrong.
Also, it can only beused when such information is available, and it is4States and countries are not annotated in CWAR, so wedo not evaluate end-to-end using NER plus toponym resolu-tion for it as there are many (falsely) false positives.5opennlp.apache.org1468less effective if the population statistics are from atime period different from that of the corpus.3.2 SPIDERLeidner (2008) describes two general and usefulminimality properties of toponyms:?
one sense per discourse: multiple tokens ofa toponym in the same text generally do notrefer to different locations in the same text?
spatial minimality: different toponyms in atext tend refer to spatially near locationsMany toponym resolvers exploit these (Smith andCrane, 2001; Rauch et al, 2003; Leidner, 2008;Grover et al, 2010; Loureiro et al, 2011; Zhanget al, 2012).
Here, we define SPIDER (SpatialProminence via Iterative Distance Evaluation andReweighting) as a strong representative of suchtextually unaware approaches.
In addition to cap-turing both minimality properties, it also identifiesthe relative prominence of the locations for eachtoponym in a given corpus.SPIDER resolves each toponym by finding thelocation for each that minimizes the sum distanceto all locations for all other toponyms in the samedocument.
On the first iteration, it tends to selectlocations that clump spatially: if Paris occurs withDallas, it will choose Paris, Texas even though thetopic may be a flight from Texas to France.
Furtheriterations bring Paris, France into focus by captur-ing its prominence across the corpus.
The key in-tuition is that most documents will discuss Paris,France and only a small portion of these mentionplaces close to Paris, Texas; thus, Paris, Francewill be selected on the first iteration for manydocuments (though not for the Dallas document).SPIDER thus assigns each candidate location aweight (initialized to 1.0), which is re-estimatedon each iteration.
The adjusted distance betweentwo locations is computed as the great circle dis-tance divided by the product of the two locations?weights.
At the end of an iteration, each candi-date location?s weight is updated to be the frac-tion of the times it was chosen times the numberof candidates for that toponym.
The weights areglobal, with one for each location in the gazetteer,so the same weight vector is used for each tokenof a given toponym on a given iteration.For example, if after the first iteration Paris,France is chosen thrice, Paris, Texas once, andParis, Arkansas never, the global weights of theselocations are (3/4)?3=2.25, (1/4)?3=.75, and(0/4)?3=0, respectively (assume, for the exam-ple, there are no other locations named Paris).
Thesum of the weights remains equal to the numberof candidate locations.
The updated weights areused on the next iteration, so Paris, France willseem ?closer?
since any distance computed to itis divided by a number greater than one.
Paris,Texas will seem somewhat further away, and Paris,Arkansas infinitely far away.
The algorithm con-tinues for a fixed number of iterations or until theweights do not change more than some thresh-old.
Here, we run SPIDER for 10 iterations; theweights have generally converged by this point.When only one toponym is present in a doc-ument, we simply select the candidate with thegreatest weight.
When there is no such weight in-formation, such as when the toponym does not co-occur with other toponyms anywhere in the cor-pus, we select a candidate at random.SPIDER captures prominence, but we stress itis not our main innovation: its purpose is to be abenchmark for text-driven resolvers to beat.3.3 Text-Driven ResolversThe text-driven resolvers presented in this sectionall use local context windows, document context,or both, to inform disambiguation.TRIPDL We use a document geolocatortrained on GEOWIKI?s document location labels.Others?such as Smith and Crane (2001)?haveestimated a document-level location to informtoponym resolution, but ours is the first we areaware of to use training data from a differentdomain to build a document geolocator that usesall words (not only toponyms) to estimate adocument?s location.
We use the document geolo-cation method of Wing and Baldridge (2011).
Itdiscretizes the earth?s surface into 1?
by 1?
gridcells and assigns Kullback-Liebler divergences toeach cell given a document, based on languagemodels learned for each cell from geolocatedWikipedia articles.
We obtain the probability of acell c given a document d by the standard methodof exponentiating the negative KL-divergence andnormalizing these values over all cells:P (c|d) = exp(?KL(c, d))?c?
exp(?KL(c?, d))This distribution is used for all toponyms t in dto define distributions PDL(l|t, d) over candidate1469locations of t in document d to be the portion ofP (c|d) consistent with the t?s candidate locations:PDL(l|t, d) =P (cl|d)?l?
?G(t) P (cl?
|d)where G(t) is the set of the locations for t in thegazetteer, and cl is the cell containing l. TRIPDL(Toponym Resolution Informed by Predicted Doc-ument Locations) chooses the location that maxi-mizes PDL.WISTR While TRIPDL uses an off-the-shelfdocument geolocator to capture the geographicgist of a document, WISTR (Wikipedia IndirectlySupervised Toponym Resolver) instead directlytargets each toponym.
It learns text classifiersbased on local context window features trained oninstances automatically extracted from GEOWIKI.To create the indirectly supervised training datafor WISTR, the OpenNLP named entity recog-nizer detects toponyms in GEOWIKI, and can-didate locations for each toponym are retrievedfrom GEONAMES.
Each toponym with a loca-tion within 10km of the document location is con-sidered a mention of that location.
For example,the Empire State Building Wikipedia article has ahuman-provided location label of (40.75,-73.99).The toponym New York is mentioned several timesin the article, and GEONAMES lists a New York at(40.71,-74.01).
These points are 4.8km apart, soeach mention of New York in the document is con-sidered a reference to New York City.Next, context windows w of twenty words toeach side of each toponym are extracted as fea-tures.
The label for a training instance is thecandidate location closest to the document loca-tion.
We extract 1,489,428 such instances for to-ponyms relevant to our evaluation corpora.
Theseinstances are used to train logistic regression clas-sifiers P (l|t, w) for location l and toponym t. Todisambiguate a new toponym, WISTR choosesthe location that maximizes this probability.Few such probabilistic toponym resolvers ex-ist in the literature.
Li (2007) builds a probabil-ity distribution over locations for each toponym,but still relies on nearby toponyms that could referto regions that contain that toponym and requireshand construction of distributions.
Other learn-ing approaches to toponym resolution (e.g.
Smithand Mann (2003)) require explicit unambiguousmentions like Portland, Maine to construct train-ing instances, while our data gathering methodol-ogy does not make such an assumption.
Overelland Ru?ger (2008) and Overell (2009) only usenearby toponyms as features.
Mani et al (2010)and Qin et al (2010) use other word types butonly in a local context, and they require toponym-labeled training data.
Our approach makes use ofall words in local and document context and re-quires no explicitly labeled toponym tokens.TRAWL We bring TRIPDL, WISTR, andstandard toponym resolution cues about ad-ministrative levels together with TRAWL (To-ponym Resolution via Administrative levels andWikipedia Locations).
The general form of a prob-abilistic resolver that utilizes such information toselect a location l?
for a toponym t in document dmay be defined asl?
= argmaxl P (l, al|t, d).where al is the administrative level (country, state,city) for l in the gazetteer.
This captures the factthat countries (like Sudan) tend to be referred tomore often than small cities (like Sudan, Texas).The above term is simplified as follows:P (l, al|t, d) = P (al|t, d)P (l|al, t, d)?
P (al|t)P (l|t, d)where we approximate the administrative levelprediction as independent of the document, andthe location as independent of administrative level.The latter term is then expressed as a linear combi-nation of the local context (WISTR) and the doc-ument context (TRIPDL):P (l|t, d) = ?tP (l|t, ct) + (1?
?t)PDL(l|t, d).
?t, the weight of the local context distribution, isset according to the confidence that a predictionbased on local context is correct:?t = f(t)f(t)+C ,where f(t) is the fraction of training instancesof toponym t of all instances extracted fromGEOWIKI.
C is set experimentally; C=.0001 wasthe optimal value for CWAR-DEV.
Intuitively, thelarger C is, the greater f(t) must be for the localcontext to be trusted over the document context.We define P (a|t), the administrative level com-ponent, to be the fraction of representative pointsfor a location l?
out of the number of representa-tives points for all candidate locations l ?
t,||Rl?||?l?
?t ||Rl?
||1470where ||Rl|| is the number of representative pointsof l. This boosts states and countries since higherprobability is assigned to locations with morepoints (and cities have just one point).Taken together, the above definitions yield theTRAWL resolver, which selects the optimal can-didate location l?
according tol?
= argmaxl P (al|t)(?tP (l|t, ct) + (1?
?t)PDL(l|t, d)).3.4 Combining Resolvers and BackoffSPIDER begins with uniform weights for eachcandidate location of each toponym.
WISTRand TRAWL both output distributions over theselocations based on outside knowledge sources,and can be used as more informed initializa-tions of SPIDER than the uniform ones.
Wecall these combinations WISTR+SPIDER andTRAWL+SPIDER.6WISTR fails to predict when encountering atoponym it has not seen in the training data, andTRIPDL fails when a toponym only has locationsin cells with no probability mass.
TRAWL failswhen both of these are true.
In these cases, weselect the candidate location geographically clos-est to the most likely cell according to TRIPDL?sP (c|d) distribution.3.5 Document SizeFor SPIDER, runtime is quadratic in the sizeof documents, so breaking up documents vastlyreduces runtime.
It also restricts the minimal-ity heuristic?appropriately?to smaller spans oftext.
For resolvers that take into account the sur-rounding document when determining how to re-solve a toponym, such as TRIPDL and TRAWL,it can often be beneficial to divide documents intosmaller subdocuments in order to get a better esti-mate of the overall geographic prominence of thetext surrounding a toponym, but at a more coarse-grained level than the local context models pro-vide.
For these reasons, we simply divide eachbook in the CWAR corpus into small subdocu-ments of at most 20 sentences.4 EvaluationMany prior efforts use a simple accuracy metric:the fraction of toponyms whose predicted location6We scale each toponym?s distribution as output byWISTR or TRAWL by the number of candidate locationsfor that toponym, since the total weight for each toponym inSPIDER is the number of candidate locations, not 1.is the same as the gold location.
Such a met-ric can be problematic, however.
The gazetteerused by a resolver may not contain, for a giventoponym, a location whose latitude and longitudeexactly match the gold label for the toponym (Lei-dner, 2008).
Also, some errors are worse than oth-ers, e.g.
predicting a toponym?s location to be onthe other side of the world versus predicting it tobe a different city in the same country?accuracydoes not reflect this difference.We choose a metric that instead measures thedistance between the correct and predicted loca-tion for each toponym and compute the mean andmedian of all such error distances.
This is usedin document geolocation work (Eisenstein et al,2010, 2011; Wing and Baldridge, 2011; Rolleret al, 2012) and is related to the root mean squareddistance metric discussed by Leidner (2008).It is important to understand performance onplain text (without gold toponyms), which is thetypical use case for applications using toponymresolvers.
Both the accuracy metric and the error-distance metric encounter problems when the setof predicted toponyms is not the same as the setof gold toponyms (regardless of locations), e.g.when a named entity recognizer is used to iden-tify toponyms.
In this case, we can use precisionand recall, where a true positive is defined as theprediction of a correctly identified toponym?s lo-cation to be as close as possible to its gold la-bel, given the gazetteer used.
False positives oc-cur when the NER incorrectly predicts a toponym,and false negatives occur when it fails to predict atoponym identified by the annotator.
When a cor-rectly identified toponym receives an incorrect lo-cation prediction, this counts as both a false nega-tive and a false positive.
We primarily present re-sults from experiments with gold toponyms but in-clude an accuracy measure for comparability withresults from experiments run on plain text witha named entity recognizer.
This accuracy met-ric simply computes the fraction of toponyms thatwere resolved as close as possible to their gold la-bel given the gazetteer.5 ResultsTable 2 gives the performance of the resolverson the TR-CONLL and CWAR test sets whengold toponyms are used.
Values for RANDOMand SPIDER are averaged over three trials.
TheORACLE row gives results when the candidate1471Resolver TRC-TEST CWAR-TESTMean Med.
A Mean Med.
AORACLE 105 19.8 100.0 0.0 0.0 100.0RANDOM 3915 1412 33.5 2389 1027 11.8POPULATION 216 23.1 81.0 1749 0.0 59.7SPIDER10 2180 30.9 55.7 266 0.0 57.5TRIPDL 1494 29.3 62.0 847 0.0 51.5WISTR 279 22.6 82.3 855 0.0 69.1WISTR+SPIDER10 430 23.1 81.8 201 0.0 85.9TRAWL 235 22.6 81.4 945 0.0 67.8TRAWL+SPIDER10 297 23.1 80.7 148 0.0 78.2Table 2: Accuracy and error distance metrics on test sets with gold toponyms.Figure 2: Visualization of how SPIDER clumpsmost predicted locations in the same region(above), on the CWAR-DEV corpus.
TRAWL?soutput (below) is much more dispersed.from GEONAMES closest to the annotated loca-tion is always selected.
The ORACLE mean andmedian error values on TR-CONLL are nonzerodue to errors in the annotations and inconsisten-cies stemming from the fact that coordinates fromGEONAMES were not used in the annotation ofTR-CONLL.On both datasets, SPIDER achieves errors andaccuracies much better than RANDOM, validatingthe intuition that authors tend to discuss placesnear each other more often than not, while somelocations are more prominent in a given corpusdespite violating the minimality heuristic.
Thetext-driven resolvers vastly outperform SPIDER,showing the effectiveness of textual cues for to-ponym resolution.The local context resolver WISTR is veryeffective: it has the highest accuracy forTR-CONLL, though two other text-based re-solvers also beat the challenging POPULATIONbaseline?s accuracy.
TRAWL achieves a bettermean distance metric for TR-CONLL, and whenused to seed SPIDER, it obtains the lowest meanerror on CWAR by a large margin.
SPIDERseeded with WISTR achieves the highest accu-racy on CWAR.
The overall geographic scopeof CWAR, a collection of documents about theAmerican Civil War, is much smaller than that ofTR-CONLL (articles about international events).This makes toponym resolution easier overall (es-pecially error distances) for minimality resolverslike SPIDER, which primarily seek tightly clus-tered sets of locations.
This behavior is quiteclear in visualizations of predicted locations suchas Figure 2.On the CWAR dataset, POPULATION performsrelatively poorly, demonstrating the fragility ofpopulation-based decisions for working with his-torical corpora.
(Also, we note that POPULATIONis not a resolver per se since it only ever predictsone location for a given toponym, regardless ofcontext.
)Table 3 gives results on TRC-TEST when NER-identified toponyms are used.
In this case, theORACLE results are less than 100% due to the lim-itations of the NER, and represent the best possibleresults given the NER we used.When resolvers are run on NER-identified to-ponyms, the text-driven resolvers that use lo-cal context again easily beat SPIDER.
WISTRachieves the best performance.
The named en-tity recognizer is likely better at detecting com-mon toponyms than rare toponyms due to the na-1472Resolver P R FORACLE 82.6 59.9 69.4RANDOM 25.1 18.2 21.1POPULATION 71.6 51.9 60.2SPIDER10 40.5 29.4 34.1TRIPDL 51.8 37.5 43.5WISTR 73.9 53.6 62.1WISTR+SPIDER10 73.2 53.1 61.5TRAWL 72.5 52.5 60.9TRAWL+SPIDER10 72.0 52.2 60.5Table 3: Precision, recall, and F-score of resolverson TRC-TEST with NER-identified toponyms.ture of its training data, and many more local con-text training instances were extracted from com-mon toponyms than from rare ones in Wikipedia.Thus, our model that uses only these local contextmodels does best when running on NER-identifiedtoponyms.
We also measured the mean and me-dian error distance for toponyms correctly identi-fied by the named entity recognizer, and found thatthey tended to be 50-200km worse than for goldtoponyms.
This also makes sense given the namedentity recognizer?s tendency to detect common to-ponyms: common toponyms tend to be more am-biguous than others.Results on TR-CONLL indicate much higherperformance than the resolvers presented by Lei-dner (2008), whose F-scores do not exceed 36.5%with either gold or NER toponyms.7 TRC-TESTis a subset of the documents Leidner uses (he didnot split development and test data), but the resultsstill come from overlapping data.
The most directcomparison is SPIDER?s F-score of 39.7% com-pared to his LSW03 algorithm?s 35.6% (both areminimality resolvers).
However, our evaluation ismore penalized since SPIDER loses precision forNER?s false positives (Jack London as a location)while Leidner only evaluated on actual locations.It thus seems fair to conclude that the text-drivenclassifiers, with F-scores in the mid-50?s, are muchmore accurate on the corpus than previous work.6 Error AnalysisTable 4 shows the ten toponyms that caused thegreatest total error distances from TRC-DEV withgold toponyms when resolved by TRAWL, the re-solver that achieves the lowest mean error on that7Leidner (2008) reports precision, recall, and F-score val-ues even with gold toponyms, since his resolvers can abstain.dataset among all our resolvers.Washington, the toponym contributing the mosttotal error, is a typical example of a toponym thatis difficult to resolve, as there are two very promi-nent locations within the United States with thename.
Choosing one when the other is correct re-sults in an error of over 4000 kilometers.
This oc-curs, for example, when TRAWL chooses Wash-ington state in the phrase Israel?s ambassador toWashington, where more knowledge about thestatus of Washington, D.C. as the political cen-ter of the United States (e.g.
in the form of moreor better contextual training instances) could over-turn the administrative level component?s prefer-ence for states.An instance of California in a baseball-relatednews article is incorrectly predicted to be the townCalifornia, Pennsylvania.
The context is: ...NewYork starter Jimmy Key left the game in the firstinning after Seattle shortstop Alex Rodriguez lineda shot off his left elbow.
The Yankees have lost12 of their last 19 games and their lead in the ALEast over Baltimore fell to five games.
At Califor-nia, Tim Wakefield pitched a six-hitter for his thirdcomplete game of the season and Mo Vaughn andTroy O?Leary hit solo home runs in the second in-ning as the surging Boston Red Sox won their thirdstraight 4-1 over the California Angels.
Bostonhas won seven of eight and is 20-6...
The pres-ence of many east coast cues?both toponym andotherwise?make it unsurprising that the resolverwould predict California, Pennsylvania despite theadministrative level component?s heavier weight-ing of the state.The average errors for the toponyms Australiaand Russia are fairly small and stem from differ-ences in how countries are represented across dif-ferent gazetteers, not true incorrect predictions.Table 5 shows the toponyms with the great-est errors from CWAR-DEV with gold toponymswhen resolved by WISTR+SPIDER.
Rome issometimes predicted as cities in Italy and otherparts of Europe rather than Rome, Georgia, thoughit correctly selects the city in Georgia more oftenthan not due to SPIDER?s preference for tightlyclumped sets of locations.
Mexico, however, fre-quently gets incorrectly selected as a city in Mary-land near many other locations in the corpus whenTRAWL?s administrative level component is notpresent.
Many other of the toponyms contributingto the total error such as Jackson and Lexington are1473Toponym N Mean TotalWashington 25 3229 80717Gaza 12 5936 71234California 8 5475 43797Montana 3 11635 34905WA 3 11221 33662NZ 2 14068 28136Australia 88 280 24600Russia 72 260 18712OR 2 9242 18484Sydney 12 1422 17067Table 4: Toponyms with the greatest total errordistances in kilometers from TRC-DEV with goldtoponyms resolved by TRAWL.
N is the numberof instances, and the mean error for each toponymtype is also given.Toponym N Mean TotalMexico 1398 2963 4142102Jackson 2485 1210 3007541Monterey 353 2392 844221Haymarket 41 15663 642170McMinnville 145 3307 479446Alexandria 1434 314 450863Eastport 184 2109 388000Lexington 796 442 351684Winton 21 15881 333499Clinton 170 1401 238241Table 5: Top errors from CWAR-DEV resolved byTRAWL+SPIDER.simply the result of many American towns sharingthe same names and a lack of clear disambiguatingcontext.7 ConclusionOur text-driven resolvers prove highly effectivefor both modern day newswire texts and 19th cen-tury texts pertaining to the Civil War.
They eas-ily outperform standard minimality toponym re-solvers, but can also be combined with them.
Thisstrategy works particularly well when predictingtoponyms on a corpus with relatively restrictedgeographic extents.
Performance remains goodwhen resolving toponyms identified automatically,indicating that end-to-end systems based on ourmodels may improve the experience of digital hu-manities scholars interested in finding and visual-izing toponyms in large corpora.AcknowledgementsWe thank: the three anonymous reviewers, GrantDeLozier, and the UT Austin Natural LanguageLearning reading group, for their helpful feed-back; Ben Wing, for his document geoloca-tion software; Jochen Leidner, for providing theTR-CONLL corpus as well as feedback on earlierversions of this paper; and Scott Nesbit, for pro-viding the annotations for the CWAR corpus.
Thisresearch was supported by a grant from the MorrisMemorial Trust Fund of the New York Commu-nity Trust.ReferencesB.
Adams and G. McKenzie.
Inferring thematicplaces from spatially referenced natural lan-guage descriptions.
Crowdsourcing GeographicKnowledge, pages 201?221, 2013.E.
Amitay, N. Har?El, R. Sivan, and A. Soffer.Web-a-Where: geotagging web content.
In Pro-ceedings of the 27th annual international ACMSIGIR conference on Research and developmentin information retrieval, pages 273?280, 2004.D.
Buscaldi and P. Rosso.
A conceptual density-based approach for the disambiguation of to-ponyms.
International Journal of GeographicalInformation Science, 22(3):301?313, 2008.P.
Clough.
Extracting metadata for spatially-aware information retrieval on the internet.
InProceedings of the 2005 workshop on Ge-ographic information retrieval, pages 25?30.ACM, 2005.G.
Crane.
The Perseus Digital Library, 2000.
URLhttp://www.perseus.tufts.edu.J.
Ding, L. Gravano, and N. Shivakumar.
Comput-ing geographical scopes of web resources.
InProceedings of the 26th International Confer-ence on Very Large Data Bases, pages 545?556,2000.J.
Eisenstein, B. O?Connor, N. Smith, and E. Xing.A latent variable model for geographic lexicalvariation.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural Lan-guage Processing, pages 1277?1287, 2010.J.
Eisenstein, A. Ahmed, and E. Xing.
Sparse ad-ditive generative models of text.
In Proceedingsof the 28th International Conference on Ma-chine Learning, pages 1041?1048, 2011.1474J.
Gelernter and N. Mushegian.
Geo-parsing mes-sages from microtext.
Transactions in GIS, 15(6):753?773, 2011.C.
Grover, R. Tobin, K. Byrne, M. Woollard,J.
Reid, S. Dunn, and J.
Ball.
Use of the Ed-inburgh geoparser for georeferencing digitizedhistorical collections.
Philosophical Transac-tions of the Royal Society A: Mathematical,Physical and Engineering Sciences, 368(1925):3875?3889, 2010.J.
Guldi.
The spatial turn.
Spatial Humanities: aProject of the Institute for Enabling, 2009.Q.
Hao, R. Cai, C. Wang, R. Xiao, J. Yang,Y.
Pang, and L. Zhang.
Equip tourists withknowledge mined from travelogues.
In Pro-ceedings of the 19th international conference onWorld wide web, pages 401?410, 2010.B.
Hecht, S. Carton, M. Quaderi, J. Scho?ning,M.
Raubal, D. Gergle, and D. Downey.
Ex-planatory semantic relatedness and explicit spa-tialization for exploratory search.
In Proceed-ings of the 35th international ACM SIGIR con-ference on Research and development in infor-mation retrieval, pages 415?424.
ACM, 2012.L.
Hill.
Georeferencing: The Geographic Associ-ations of Information.
MIT Press, 2006.J.
Hoffart, M. Yosef, I. Bordino, H. Fu?rstenau,M.
Pinkal, M. Spaniol, B. Taneva, S. Thater, andG.
Weikum.
Robust disambiguation of namedentities in text.
In Proceedings of the Con-ference on Empirical Methods in Natural Lan-guage Processing, pages 782?792.
Associationfor Computational Linguistics, 2011.L.
Hollenstein and R. Purves.
Exploring placethrough user-generated content: Using Flickrtags to describe city cores.
Journal of SpatialInformation Science, (1):21?48, 2012.S.
Intagorn and K. Lerman.
A probabilistic ap-proach to mining geospatial knowledge fromsocial annotations.
In Conference on Infor-mation and Knowledge Management (CIKM),2012.C.
Jones, R. Purves, P. Clough, and H. Joho.
Mod-elling vague places with knowledge from theweb.
International Journal of Geographical In-formation Science, 2008.S.
Kulkarni, A. Singh, G. Ramakrishnan, andS.
Chakrabarti.
Collective annotation ofWikipedia entities in web text.
In Proceedingsof the 15th ACM SIGKDD international confer-ence on Knowledge discovery and data mining,pages 457?466.
ACM, 2009.S.
Ladra, M. Luaces, O. Pedreira, and D. Seco.
Atoponym resolution service following the OGCWPS standard.
In Web and Wireless Geograph-ical Information Systems, volume 5373, pages75?85.
2008.J.
Leidner.
Toponym resolution in text: Anno-tation, Evaluation and Applications of SpatialGrounding of Place Names.
Universal Press,Boca Raton, FL, USA, 2008.H.
Li, R. Srihari, C. Niu, and W. Li.
InfoXtract lo-cation normalization: a hybrid approach to geo-graphic references in information extraction.
InProceedings of the HLT-NAACL 2003 workshopon Analysis of geographic references - Volume1, pages 39?44, 2003.Y.
Li.
Probabilistic toponym resolution and geo-graphic indexing and querying.
Master?s thesis,The University of Melbourne, Melbourne, Aus-tralia, 2007.V.
Loureiro, I. Anasta?cio, and B. Martins.
Learn-ing to resolve geographical and temporal ref-erences in text.
In Proceedings of the 19thACM SIGSPATIAL International Conference onAdvances in Geographic Information Systems,pages 349?352, 2011.M.
Louwerse and N. Benesh.
Representing spatialstructure through maps and language: Lord ofthe Rings encodes the spatial structure of Mid-dle Earth.
Cognitive science, 36(8):1556?1569,2012.I.
Mani, C. Doran, D. Harris, J. Hitzeman,R.
Quimby, J.
Richer, B. Wellner, S. Mardis,and S. Clancy.
SpatialML: annotation scheme,resources, and evaluation.
Language Resourcesand Evaluation, 44(3):263?280, 2010.S.
Overell.
Geographic Information Retrieval:Classification, Disambiguation and Modelling.PhD thesis, Imperial College London, 2009.S.
Overell and S. Ru?ger.
Using co-occurrencemodels for placename disambiguation.
Inter-national Journal of Geographical InformationScience, 22:265?287, 2008.Y.
Pang, Q. Hao, Y. Yuan, T. Hu, R. Cai, andL.
Zhang.
Summarizing tourist destinationsby mining user-generated travelogues and pho-1475tos.
Computer Vision and Image Understand-ing, 115(3):352 ?
363, 2011.V.
Petras.
Statistical analysis of geographic andlanguage clues in the MARC record.
Technicalreport, The University of California at Berkeley,2004.T.
Qin, R. Xiao, L. Fang, X. Xie, and L. Zhang.An efficient location extraction algorithm byleveraging web contextual information.
In Pro-ceedings of the 18th SIGSPATIAL InternationalConference on Advances in Geographic Infor-mation Systems, pages 53?60.
ACM, 2010.E.
Rauch, M. Bukatin, and K. Baker.
Aconfidence-based framework for disambiguat-ing geographic terms.
In Proceedings of theHLT-NAACL 2003 workshop on Analysis of ge-ographic references - Volume 1, pages 50?54,2003.K.
Roberts, C. Bejan, and S. Harabagiu.
Toponymdisambiguation using events.
In Proceedings ofthe 23rd International Florida Artificial Intelli-gence Research Society Conference, pages 271?276, 2010.S.
Roller, M. Speriosu, S. Rallapalli, B.
Wing, andJ.
Baldridge.
Supervised text-based geolocationusing language models on an adaptive grid.
InProceedings of EMNLP 2012, 2012.J.
Sankaranarayanan, H. Samet, B. Teitler,M.
Lieberman, and J. Sperling.
TwitterStand:news in tweets.
In Proceedings of the 17thACM SIGSPATIAL International Conference onAdvances in Geographic Information Systems,pages 42?51, 2009.W.
Scheidel, E. Meeks, and J. Weiland.
ORBIS:The Stanford geospatial network model of theroman world.
2012.A.
Skupin and A. Esperbe?.
An alternative mapof the United States based on an n-dimensionalmodel of geographic space.
Journal of Vi-sual Languages & Computing, 22(4):290?304,2011.D.
Smith and G. Crane.
Disambiguating geo-graphic names in a historical digital library.
InProceedings of the 5th European Conference onResearch and Advanced Technology for DigitalLibraries, pages 127?136, 2001.D.
Smith and G. Mann.
Bootstrapping toponymclassifiers.
In Proceedings of the HLT-NAACL2003 workshop on Analysis of geographic ref-erences - Volume 1, pages 45?49, 2003.B.
Teitler, M. Lieberman, D. Panozzo, J. Sankara-narayanan, H. Samet, and J. Sperling.
News-Stand: a new view on news.
In Proceedings ofthe 16th ACM SIGSPATIAL international con-ference on Advances in geographic informationsystems, page 18.
ACM, 2008.R.
Volz, J. Kleb, and W. Mueller.
Towardsontology-based disambiguation of geographicalidentifiers.
In Proceedings of the 16th Interna-tional Conference on World Wide Web, 2007.B.
Wing and J. Baldridge.
Simple supervised doc-ument geolocation with geodesic grids.
In Pro-ceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Hu-man Language Technologies, pages 955?964,2011.Q.
Zhang, P. Jin, S. Lin, and L. Yue.
Extractingfocused locations for web pages.
In Web-AgeInformation Management, volume 7142, pages76?89.
2012.W.
Zong, D. Wu, A.
Sun, E. Lim, and D. Goh.
Onassigning place names to geography related webpages.
In Proceedings of the 5th ACM/IEEE-CS joint conference on Digital libraries, pages354?362, 2005.1476
