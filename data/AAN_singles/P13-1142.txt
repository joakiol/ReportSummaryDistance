Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1446?1455,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsBRAINSUP: Brainstorming Support for Creative Sentence GenerationGo?zde O?zbalFBK-irstTrento, Italygozbalde@gmail.comDaniele PighinGoogle Inc.Zu?rich, Switzerlanddaniele.pighin@gmail.comCarlo StrapparavaFBK-irstTrento, Italystrappa@fbk.euAbstractWe present BRAINSUP, an extensibleframework for the generation of creativesentences in which users are able toforce several words to appear in the sen-tences and to control the generation pro-cess across several semantic dimensions,namely emotions, colors, domain related-ness and phonetic properties.
We evalu-ate its performance on a creative sentencegeneration task, showing its capability ofgenerating well-formed, catchy and effec-tive sentences that have all the good qual-ities of slogans produced by human copy-writers.1 IntroductionA variety of real-world scenarios involve talentedand knowledgable people in a time-consumingprocess to write creative, original sentences gen-erated according to well-defined requisites.
Forinstance, to advertise a new product it could bedesirable to have its name appearing in a punchysentence together with some keywords relevant formarketing, e.g.
?fresh?, or ?thirst?
for the adver-tisement of a drink.
Besides, it could be interestingto characterize the sentence with respect to a spe-cific color, like ?blue?
to convey the idea of fresh-ness, or to a color more related to the brand of thecompany, e.g.
?red?
for a new Ferrari.
Moreover,making the slogan evoke ?joy?
or ?satisfaction?could make the advertisement even more catchyfor customers.
On the other hand, there are manyexamples of provocative slogans in which copy-writers try to impress their readers by suscitatingstrong negative feelings, as in the case of anti-smoke campaigns (e.g., ?there are cooler ways todie than smoking?
or ?cancer cures smoking?
), orthe famous beer motto ?Guinness is not good foryou?.
As another scenario, creative sentence gen-eration is also a useful teaching device.
For ex-ample, the keyword or linkword method used forsecond language learning links the translation ofa foreign (target) word to one or more keywordsin the native language which are phonologicallyor lexically similar to the target word (Sagarra andAlba, 2006).
To illustrate, for teaching the Ital-ian word ?tenda?, which means ?curtain?
in En-glish, the learners are asked to imagine ?rubbinga tender part of their leg with a curtain?.
Thesewords should co-occur in the same sentence, butconstructing such sentences by hand can be a dif-ficult and very time-consuming process.
O?zbaland Strapparava (2011), who attempted to auto-mate the process, conclude that the inability to re-trieve from the web a good sentence for all casesis a major bottleneck.Although state of the art computational mod-els of creativity often produce remarkable results,e.g., Manurung et al (2008), Greene et al (2010),Guerini et al (2011), Colton et al (2012) just toname a few, to our best knowledge there is no at-tempt to develop an unified framework for the gen-eration of creative sentences in which users cancontrol all the variables involved in the creativeprocess to achieve the desired effect.In this paper, we advocate the use of syntacticinformation to generate creative utterances by de-scribing a methodology that accounts for lexicaland phonetic constraints and multiple semantic di-mensions at the same time.
We present BRAIN-SUP, an extensible framework for creative sen-tence generation in which users can control all theparameters of the creative process, thus generat-ing sentences that can be used for practical ap-plications.
First, users can define a set of key-words which must appear in the final sentence.Second, they can slant the output towards a spe-1446Domain Keywords BRAINSUP output examplescoffee waking,cupBetween waking and doing thereis a wondrous cup.coke drink, ex-haustionThe physical exhaustion wantsthe dark drink.health day, juice,sunshineWith juice and cereal the normalday becomes a summer sunshine.beauty kiss,lipsPassionate kiss, perfect lips.
?Lips and eyes want the kiss.mascara drama,lashLash your drama to the stage.
?A mighty drama, a biting lash.pickle crunch, bite Crunch your bite to the top.
?Crunch of a savage byte.
?
Alarge byte may crunch a little at-tention.soapskin,love,touchA touch of love is worth a fortuneof skin.
?
The touch of froth isthe skin of love.
?
A skin of wateris worth a touch of love.Table 1: A selection of sentences automaticallygenerated by BRAINSUP for specific domains.cific emotion, color or domain.
At the same time,they can require a sentence to include desired pho-netic properties, such as rhymes, alliteration orplosives.
The combination of these features al-lows for the generation of potentially catchy andmemorable sentences by establishing connectionsbetween linguistic, emotional (LaBar and Cabeza,2006), echoic and visual (Borman et al, 2005)memory, as exemplified by the system outputsshowcased in Table 1.
Other creative dimensionscan easily be plugged in, due to the inherentlymodular structure of the system.BRAINSUP supports the creative process bygreedily exploring a huge solution space to pro-duce completely novel utterances responding touser requisites.
It exploits syntactic constraints todramatically cut the size of the search space, thusmaking it possible to focus on the creative aspectsof sentence generation.2 Related workResearch in creative language generation hasbloomed in recent years.
In this section, we pro-vide a necessarily succint overview of a selectionof the studies that most heavily inspired and influ-enced the development of BRAINSUP.Humor generators are a notable class of sys-tems exploring new venues in computational cre-ativity (Binsted and Ritchie, 1997; McKay, 2002;Manurung et al, 2008).
Valitutti et al (2009)present an interactive system which generates hu-morous puns obtained through variation of famil-iar expressions with word substitution.
The varia-tion takes place considering the phonetic distanceand semantic constraints such as semantic similar-ity, semantic domain opposition and affective po-larity difference.
Possibly closer to slogan genera-tion, Guerini et al (2011) slant existing textual ex-pressions to obtain more positively or negativelyvalenced versions using WordNet (Miller, 1995)semantic relations and SentiWordNet (Esuli andSebastiani, 2006) annotations.
Stock and Strap-parava (2006) generate acronyms based on lexicalsubstitution via semantic field opposition, rhyme,rythm and semantic relations.
The model is lim-ited to the generation of noun phrases.Poetry generation systems face similar chal-lenges to BRAINSUP as they struggle to combinesemantic, lexical and phonetic features in a unifiedframework.
Greene et al (2010) describe a modelfor poetry generation in which users can controlmeter and rhyme scheme.
Generation is modeledas a cascade of weighted Finite State Transduc-ers that only accept strings conforming to the de-sired rhyming scheme.
Toivanen et al (2012) at-tempt to generate novel poems by replacing wordsin existing poetry with morphologically compat-ible words that are semantically related to a tar-get domain.
Content control and the inclusion ofphonetic features are left as future work and syn-tactic information is not taken into account.
TheElectronic Text Composition project1 is a corpusbased approach to poetry generation which recur-sively combines automatically generated linguisticconstituents into grammatical sentences.
Colton etal.
(2012) propose another data-driven approach topoetry generation based on simile transformation.The mood and theme of the poems are influencedby daily news.
Constraints about phonetic proper-ties of the selected words or their frequencies canbe enforced during retrieval.
Unlike these exam-ples, BRAINSUP makes heavy use of syntactic in-formation to enforce well-formed sentences and toconstraint the search for a solution, and providesan extensible framework in which various formsof linguistic creativity can easily be incorporated.Several slogan generators are available on theweb2, but their capabilities are very limited as theycan only replace single words or word sequenceswithin existing slogan.
This often results in syn-tactically incorrect outputs.
Furthermore, they donot allow for other forms of user control.1http://slought.org/content/111992E.g.
: http://www.procato.com/slogan+generator, http://www.sloganizer.net/en/,http://www.sloganmania.com/index.htm.14473 Architecture of BRAINSUPTo effectively support the creative process withuseful suggestions, we must be able to generatesentences conforming to the user needs.
First ofall, users can select the target words that need toappear in the sentence.
In the context of secondlanguage learning, these might be the words that alearner must associate in order to expand her vo-cabulary.
For slogan generation, the target wordscould be the key features of a product, or target-defining keywords that copywriters want to explic-itly mention.
On top of that, a user can character-ize the generated sentences according to severaldimensions, namely: 1) a specific semantic do-main, e.g.
: ?sports?
or ?blankets?
; 2) a specificemotion, e.g., ?joy?, ?anger?
or just ?negative?
; 3)a specific color, e.g., ?red?
or ?blue?
; 4) a com-bination of phonetic properties of the words thatwill appear in the sentence, i.e., rhymes, allitera-tions and plosives.
More formally, the user inputis a tuple: U = ?t,d, c, e, p,w?
, where t is theset of target words, d is a set of words defining thetarget domain, c and p are, respectively, the colorand the emotion towards which the user wants toslant the sentence, p represents the desired pho-netic features, and w is a set of weights that controlthe influence of each dimension on the generativeprocess, as detailed in Section 3.3.
For target anddomain words, users can explicitly select one ormore POSes to be considered, e.g., ?drink/verb?or ?drink/verb,noun?.The sentence generation process is based onmorpho-syntactic patterns which we automati-cally discover from a corpus of dependency parsedsentences P .
These patterns represent very gen-eral skeletons of well-formed sentences that weemploy to generate creative sentences by onlyfocusing on the lexical aspects of the process.Candidate fillers for each empty position (slot)in the patterns are chosen according to the lexi-cal and syntactic constraints enforced by the de-pendency relations in the patterns.
These con-straints are learned from relation-head-modifierco-occurrence counts estimated from a depen-dency treebank L. A beam search in the space ofall possible lexicalizations of a syntactic patternpromotes the words with the highest likelihood ofsatisfying the user specification.Algorithm 1 provides a high-level description ofthe creative sentence generation process.
Here, ?is a set of meta-parameters that affect search com-plexity and running time of the algorithm, suchas the minimum/maximum number of solutions toAlgorithm 1 SentenceGeneration(U,?,P,L): U is theuser specification, ?
is a set of meta-parameters; P and L aretwo dependency treebanks.O ?
?for all p ?
CompatiblePatterns?
(U,P) dowhile NotEnoughSolutions?
(O) doO ?
O ?
FillInPattern?
(U, p,L)return SelectBestSolutions?
(O)DT NNS VBD DT JJ NN IN DT NNThe * * a * * in the *det nsubjdobjdetamodpreppobjdetFigure 1: Example of a syntactic pattern.
A ?
*?represents an empty slot to be filled with a filler.be generated, the maximum number of patterns toconsider, or the maximum size of the generatedsentences.
CompatiblePatterns(?)
finds the mostfrequent syntactic patterns in P that are compat-ible with the user specification, as explained inSection 3.1; FillInPattern(?)
carries out the beamsearch, and returns the best solutions generated foreach pattern p given U .
The algorithm terminateswhen at least a minimum number of solutions havebeen generated, or when all the compatible pat-terns have been exhausted.
Finally, only the bestamong the generated solutions are shown to theuser.
More details about the search in the solutionspace are provided in Section 3.2.3.1 Pattern selectionWe generate creative sentences starting frommorpho-syntactic patterns which have been au-tomatically learned from a large corpus P .
Thechoice of the corpus from which the patternsare extracted constitutes the first element of thecreative sentence generation process, as differ-ent choices will generate sentences with differentstyles.
For example, a corpus of slogans or punch-lines can result in short, catchy and memorablesentences, whereas a corpus of simplified Englishwould be a better choice to learn a second lan-guage or to address low reading level audiences.A pattern is the syntactic skeleton of a classof sentences observed in P .
Within a pattern, asecond element of creativity involves the selec-tion of original combinations of words (fillers) thatdo not violate the grammaticality of the sentence.The patterns that we employ are automatic de-pendency trees from which all content-words havebeen removed, as exemplified in Figure 1.
Afterselecting the target corpus, we parse all the sen-tences with the Stanford Parser (Klein and Man-1448ning, 2003) and produce the patterns by strippingaway all content words from the parses.
Then,for each pattern we count how many times it hasbeen observed in the corpus.
Additionally, wekeep track of what kind of empty slots, i.e., emptypositions, are available in each pattern.
For ex-ample, the pattern in Figure 1 can accommodateup to two singular nouns (NN), one plural noun(NNS), one adjective (JJ) and one verb in the pasttense (VBD).
This information is needed to se-lect the patterns which are compatible with thetarget words t in the user specification U .
Forexample, this pattern is not compatible with t =[heading/VBG, edge/NN] as the pattern does nothave an empty slot for a gerundive verb, while itsatisfies t = [heading/NN, edge/NN] as it canaccommodate the two singular nouns.
While re-trieving patterns, we also need to enforce that apattern be not completely filled just by adding thetarget words t, as under these conditions therewould be no room to achieve any kind of creativeeffect.
Therefore, we also require that the pat-terns retrieved by CompatiblePatterns(?)
havemore empty slots than the size of t. The mini-mum and maximum number of excess slots in thepattern are two other meta-parameters controlledby ?.
CompatiblePatterns(?)
returns compati-ble patterns ordered by their frequency, i.e.
whengenerating solutions the first patterns that are ex-plored are the most frequently observed ones.
Inthis way, we achieve the following two objectives:1) we compensate for the unavoidable errors intro-duced by the automatic parser, as frequently ob-served parses are less likely to be the result ofan erroneous interpretation of a sentence; and 2)we generate sentences that are most likely to becatchy and memorable, being based on syntacticconstructs that are used more frequently.
To avoidalways selecting the same patterns for the samekinds of inputs, we add a small random compo-nent (also controlled by ?)
to the pattern sortingalgorithm, thus allowing for sentences to be gen-erated also from non-top ranked patterns.3.2 Searching the solution spaceWith the compatible patterns selected, we can ini-tiate a beam search in the space of all possiblelexicalizations of the patterns, i.e., the space ofall sentences that can be generated by respect-ing the syntactic constraints encoded by each pat-tern.
The process starts with a syntactic patternp containing only stop words, syntactic relationsand morphologic constraints (i.e., part-of-speechDT NNS VBD DT JJ NN IN DT NNThe fires X a * smoke in the *det nsubjdobjdetamodpreppobjdetFigure 2: A partially lexicalized sentence with ahighlighted empty slot marked with X.
The rele-vant dependencies to fill in the slot are shown inboldface.tags) for the empty slots.
The search advances to-wards a complete solution by selecting an emptyslot to fill and trying to place candidate fillers inthe selected position.
Each partially lexicalizedsolution is scored by a battery of scoring func-tions that compete to generate creative sentencesrespecting the user specificationU , as explained inSection 3.3.
The most promising solutions are ex-tended by filling another slot, until completely lex-icalized sentences, i.e., sentences without emptyslots, are generated.To limit the number of words that can occupya given position in a sentence, we define a set ofoperators that return a list of candidate fillers fora slot solely based on syntactic clues.
To achievethat, we analyze a large corpus of parsed sentencesL3 and store counts of observed head-relation-modifier (?h, r,m?)
dependency relations.
Let?r(h) be an operator that, when applied to a headword h in a relation r, returns the set of words inL which have been observed as modifiers for h inr with a specific POS.
To simplify the notation,we assume that the relation r also carries alongthe POS of the head and modifier slots.
As anexample, with respect to the tree depicted in Fig-ure 2, ?amod(smoke) would return all the wordswith POS equal to ?JJ?
that have been observed asadjective modifiers for the singular noun ?smoke?.We will refer to ?r(?)
as the dependency operatorfor r. For every ?r(?
), we also define an inversedependency operator ?
?1r (?
), which returns the listof the possible heads in r when applied to a mod-ifier word m. For instance, with respect to Fig-ure 2, ?
?1nsubj(fires) would return the set of verbs inthe past tense of which ?fires?
as a plural noun canbe a subject.While filling in a given slot X , the dependencyoperators can be combined to obtain a list of wordswhich are likely to occupy that position given thesyntactic constraints induced by the structure ofthe pattern.
Let W = ?i{wi} be the set of wordswhich are directly connected to the empty slot by3Distinct from the corpus used for pattern selection, P .1449a dependency relation.
Each word wi implies aconstraint that candidate fillers for X must satisfy.If wi is the head of X , then a direct operator isused to retrieve a list of fillers that satisfy the ithconstraint.
Conversely, if wi is a modifier of X ,an inverse operator is employed.As an example, let us consider the partiallycompleted sentence shown in Figure 2 havingan empty slot marked with X .
Here, the word?smoke?
is a modifier for X , to which it is con-nected by a dobj relation.
Therefore, we can ex-ploit ?
?1dobj(smoke) to obtain a ranked list of wordsthat can occupy X according to this constraint.Similarly, the ?
?1nsubj(fires) operator can be used toretrieve a list of verbs in the past tense that ac-cept ?fires?
as nsubj modifier.
Finally ?
?1prep(in)can further restrict our options to verbs that ac-cepts complements introduced by the preposition?in?.
For example, the words ?generated?, ?pro-duced?, ?caused?
or ?formed?
would be good can-didates to fill in the slot considering all the pre-vious constraints.
More formally, we can de-fine the set of candidate fillers for a slot X , CX ,as: CX = ?
?1rhX,X(hX) ?
(?wi|wi?MX ?rwi,X(wi)),where rwi,X is the type of relation between wi andX , MX is the set of modifiers of X and hX is thesyntactic head of X .4Concerning the order in which slots are filled,we start from those that have the highest num-ber of dependencies (both head or modifiers) thathave been already instantiated in the sentence, i.e.,we start from the slots that are connected to thehighest number of non-empty slots.
In doing sowe maximize the constraints that we can rely onwhen inserting a new word, and eventually gener-ate more reliable outputs.3.3 Filler selection and solution scoringWe have devised a set of feature functions that ac-count for different aspects of the creative sentencegeneration process.
By changing the weight w ofthe feature functions in U , users can control theextent to which each creativity component will af-fect the sentence generation process, and tune theoutput of the system to better match their needs.As explained in the remainder of this section, fea-ture functions are responsible for ranking the can-didate slot fillers to be used during sentence gen-eration and for selecting the best solutions to be4An empty slot does not generate constraints for X .
Inaddition, there might be cases in which it is not possible tofind a filler that satisfies all the constraints at the same time.In such cases, all the fillers that satisfy the maximum numberof constraints are considered.Algorithm 2 RankCandidates(U, f , c1, c2, s,X): c1and c2 are two candidate fillers for the slot X in the sentences = [s0, .
.
.
sn]; f is the set of feature functions; U is the userspecification.sc1 ?
s, sc2 ?
s, sc1 [X]?
c1, sc2 [X]?
c2for all f ?
SortFeatureFunctions?
(U, f) doif f(sc1 , U) > f(sc2 , U) then return c1  c2else if f(sc1 , U) < f(sc2 , U) thenreturn c1 ?
c2return c1 ?
c2shown to the users.Algorithm 2 details the process of ranking can-didate fillers.
To compare two candidates c1 and c2for the slot X in the sentence s, we first generatetwo sentences sc1 and sc2 in which the empty slotX is occupied by c1 and c2, respectively.
Then, wesort the feature functions based on their weightsin descending order, and in turn we apply themto score the two sentences.
As soon as we finda scorer for which one sentence is better than theother, we can take a decision about the ranking ofthe fillers.
This approach makes it possible to es-tablish a strict order of precedence among featurefunctions and to select fillers that have a highestchance of maximizing the user satisfaction.Concerning the scoring of partial solutions andcomplete sentences, we adopt a simple linear com-bination of scoring functions.
Let s be a (partial)sentence, f = [f0, .
.
.
, fk] be the vector of scor-ing functions and w = [w0, .
.
.
, wk] the associ-ated vector of weights in U .
The overall score of sis calculated as score(s, U) = ?ki=0wifi(s, U) .Solutions that do not contain all the required targetwords are discarded and not shown to the user.Currently, the model employs the following 12feature functions:Chromatic and emotional connota-tion.
The chromatic connotation of asentence s = [s0, .
.
.
, sn] is computed asf(s, U) =?si(sim(si, c) ?
?cj 6=c sim(si, cj)),where c is the user selected target color andsim(si, cj) is the degree of association betweenthe word si and the color cj as calculated byMohammad (2011).
All the words in the sentencewhich have an association with the target colorc give a positive contribution, while those thatare associated with a color ci 6= c contributenegatively.
Emotional connotation works exactlyin the same way, but in this case word-emotionassociations are taken from (Mohammad andTurney, 2010).Domain relatedness.
This feature function usesan LSA (Deerwester et al, 1990) vector space1450model to measure the similarity between the wordsin the sentence and the target domain d speci-fied by the user.
It is calculated as: f(s, U) =?di v(di)?
?si v(si)?
?di v(di)???
?si v(si)?where v(?)
returns the rep-resentation of a word in the vector space.Semantic cohesion.
This feature behaves ex-actly like domain relatedness, with the only dif-ference that it measures the similarity between thewords in the sentence and the target words t.Target-words scorer.
This feature functionsimply counts what fraction of the targetwords t is present in a partial solution:f(s, U) = (?si|si?t 1)/|t|.
The target word scorertakes care of enforcing the presence of the targetwords in the sentences.
Letting beam search findthe best placement for the target words comes atno extra cost and results in a simple and elegantmodel.Phonetic features (plosives, alliteration andrhyme).
All the phonetic features are based onthe phonetic representation of English words ofthe Carnegie Mellon University pronouncing dic-tionary (Lenzo, 1998).
The plosives feature is cal-culated as the ratio between the number of plo-sive sounds in a sentence and the overall num-ber of phonemes.
For the alliteration scorer, westore the phonetic representation of each word ins in a trie (i.e., prefix tree), and count how manytimes each node ni of the trie (corresponding to aphoneme) is traversed.
Let ci be the value of thecounts for ni.
The alliteration score is then cal-culated as f(s, U) = (?i|ci>1 ci)/?i ci.
Moresimply put, we count how many of the phoneticprefixes of the words in the sentence are repeated,and then we normalize this value by the total num-ber of phonemes in s. The rhyme feature worksexactly in the same way, with the only differencethat we invert the phonetic representation of eachword before adding it to the TRIE.
Thus, we givehigher scores to sentences in which several wordsshare the same phonetic ending.Variety scorer.
This feature function promotessentences that contain as many different words aspossible.
It is calculated as the number of distinctwords in the sentence over the size of the sentence.Unusual-words scorer.
To increase the abilityof the model to generate sentences containing non-trivial word associations, we may want to prefersolutions in which relatively uncommon words areemployed.
Inversely, we may want to lower lex-ical complexity to generate sentences more ap-propriate for certain education or reading levels.We define ci as the number of times each wordsi ?
s is observed in a corpus V .
Accord-ingly, the value of this feature is calculated as:f(s, U) = (1/|s|)(?si 1/ci).N-gram likelihood.
This is simply the likeli-hood of a sentence estimated by an n-gram lan-guage model, to enforce the generation of well-formed word sequences.
When a solution is notcomplete, in the computation we include only thesequences of contiguous words (i.e., not inter-rupted by empty slots) having length greater thanor equal to the order of the n-gram model.Dependency likelihood.
This feature is re-lated to the dependency operators introducedin Section 3.2 and it enforces sentences inwhich dependency chains are well formed.
Weestimate the probability of a modifier wordm and its head h to be in the relation ras pr(h,m) = cr(h,m)/(?hi?mi cr(hi,mi)),where cr(?)
is the number of times that mdepends on h in the dependency treebankL and hi,mi are all the head/modifier pairsobserved in L. The dependency-likelihoodof a sentence s can then be calculated asf(s, U) = exp(??h,m,r?
?r(s) log pr(h,m)), r(s)being the set of dependency relations in s.4 EvaluationWe evaluated our model on a creative sentencegeneration task.
The objective of the evaluationis twofold: we wanted to demonstrate 1) the effec-tiveness of our approach for creative sentence gen-eration, in general, and 2) the potential of BRAIN-SUP to support the brainstorming process behindslogan generation.
To this end, the annotation tem-plate included one question asking the annotatorsto rate the quality of the generated sentences asslogans.Five experienced annotators were asked to rate432 creative sentences according to the follow-ing criteria, namely: 1) Catchiness: is the sen-tence attractive, catchy or memorable?
[Yes/No]2) Humor: is the sentence witty or humorous?
[Yes/No]; 3) Relatedness: is the sentence seman-tically related to the target domain?
[Yes/No]; 4)Correctness: is the sentence grammatically cor-rect?
[Ungrammatical/Slightly disfluent/Fluent];5) Success: could the sentence be a good sloganfor the target domain?
[As it is/With minor edit-ing/No].
In these last two cases, the annotators1451were instructed to select the middle option onlyin cases where the gap with a correct/successfulsentence could be filled just by performing minorediting.
The annotation form had no default val-ues, and the annotators did not know how the eval-uated sentences were generated, or whether theywere the outcome of one or more systems.We started by collecting slogans from an on-line repository of slogans5.
Then, we randomlyselected a subset of these slogans and for each ofthem we generated an input specification U for thesystem.
We used the commercial domain of theadvertised product as the target domain d. Twoor three content words appearing in each sloganwere randomly selected as the target words t. Wedid so to simulate the brainstorming phase behindthe slogan generation process, where copywritersstart with a set of relevant keywords to come upwith a catchy slogan.
In all cases, we set the tar-get emotion to ?positive?
as we could not estab-lish a generally valid criteria to associate a spe-cific emotion to a product.
Concerning chromaticslanting, for target domains having a strong chro-matic correlation we allowed the system to slantthe generated sentences accordingly.
In the othercases, a random color association was selected.
Inthis manner, we produced 10 tuples ?t,d, c, e, p?.Then, from each tuple we produced 5 completeuser specifications by enabling or disabling differ-ent feature function combinations6.
The four com-binations of features are: base: Target-word scorer+ N-gram likelihood + Dependency likelihood +Variety scorer + Unusual-words scorer + Seman-tic cohesion; base+D: all the scorers in base +Domain relatedness; base+D+C: all the scorers inbase+D + Chromatic connotation; base+D+E: allthe scorers in base+D + Emotional connotation;base+D+P: all the scorers in base+D + Phoneticfeatures.
For each of the resulting 50 input config-urations, we generated up to 10 creative sentences.As the system could not generate exactly 10 solu-tions in all the cases, we ended up with a set of432 items to annotate.
The weights of the featurefunctions were set heuristically, due to the lackof an annotated dataset suitable to learn an opti-5http://www.tvacres.com/advertising_slogans.htm6An alternative strategy to keep the annotation effort un-der control would have been to generate fewer sentences froma larger number of inputs.
We adopted the former settingsince we regarded it as more similar to a brainstorming ses-sion, where the system proposes different alternatives to in-spire human operators.
Forcing BRAINSUP to only outputone or two sentences would have limited its ability to exploreand suggest potentially valuable outputs.MC Cat.
Hum.
Corr.
Rel.
Succ.
RND2 RND32 - - 16.67 - 22.22 - 37.043 47.45 39.58 43.52 13.66 44.21 62.50 49.384 33.10 37.73 32.18 21.99 22.22 31.25 12.355 19.44 22.69 07.64 64.35 11.34 06.25 01.23Table 2: Majority classes (%) for the five dimen-sions of the annotation.mal weight configuration.
We started by assign-ing the highest weight to the Target Word scorer(i.e., 1.0), followed by the Variety and UnusualWord scorers (0.99), the Phonetic Features, Chro-matic/Emotional Connotation and Semantic Co-hesion scorers (0.98) and finally the Domain, N -gram and Dependency Likelihood scorers (0.97).These settings allow us to enforce an order ofprecedence among the scorers during slot-filling,while giving them virtually equal relevance for so-lution ranking.As discussed in Section 3 we use two differ-ent treebanks to learn the syntactic patterns (P)and the dependency operators (L).
For these ex-periments, patterns were learned from a corpusof 16,000 proverbs (Mihalcea and Strapparava,2006), which offers a good selection of short sen-tences with a good potential to be used for slo-gan generation.
This choice seemed to be a goodcompromise as, to our best knowledge, there isno published slogan dataset with an adequate size.Besides, using existing slogans might have legalimplications that we might not be aware of.
De-pendency operators were learned by dependencyparsing the British National Corpus7.
To reducethe amount of noise introduced by the automaticparses, we only considered sentences having lessthan 20 words.
Furthermore, we only consideredsentences in which all the content words are listedin WordNet (Miller, 1995) with the observed partof speech.8 The LSA space used for the semanticfeature functions was also learned on BNC data,but in this case no filtering was applied.4.1 ResultsTo measure the agreement among the annota-tors, similarly to Mohammad (2011) and Ozbaland Strapparava (2012) we calculated the majorityclass for each dimension of the annotation task.
A7http://www.natcorp.ox.ac.uk/8Since the CMU pronouncing dictionary used by the pho-netic scorers is based on the American pronunciation ofwords, we actually pre-processed the whole BNC by replac-ing all British-English words with their American-Englishcounterparts.
To this end, we used the mapping available athttp://wordlist.sourceforge.net/.1452Cat.
Rel.
Hum.
Succ.
Corr.Yes 67.59 93.98 12.73 32.41 64.35Partly - - - 23.15 31.71No 32.41 06.02 87.27 44.44 03.94Table 3: Majority decisions (%) for each annota-tion dimension.majority class greater than or equal to 3 means thatthe absolute majority of the 5 annotators agreedon the same decision9.
Table 2 shows the ob-served agreement for each dimension.
The columnlabeled RND2 (RND3) shows the random agree-ment for a given number of annotators and a binary(ternary) decision.
For example, all five annotators(MC=5) agreed on the annotation of the catchinessof the slogans in 19.44% of the cases.
The randomchance of agreement for 5 annotators on the binarydecision problem is 6.25%.
The figures for MC ?4 are generally high, confirming a good agreementamong the annotators.
The agreement on the relat-edness of the slogans is especially high, with all 5annotators taking the same decision in almost twocases out of three, i.e., 64.35%.Table 3 lists the distribution of answers for eachdimension in the cases where a decision can betaken by majority vote.
The generated slogansare found to be catchy in more than 2/3 of thecases, (i.e., 67.59%), completely successful in 1/3of the cases (32.41%) and completely correct in2/3 of the cases (64.35%).
These figures demon-strate that BRAINSUP is very effective in gener-ating grammatical utterances that have all the ap-pealing properties of a successful slogan.
As forhumor, the sentences are found to have this prop-erty in only 12.73% of cases.
Even though thefigure is not very high, we should also considerthat BRAINSUP is not explicitly trying to gener-ate amusing utterances.
Concerning success, weshould point out that in 23.15% of the cases theannotators have found that the generated sloganshave the potential to be turned into successful onesonly with minor editing.
This is a very importantpiece of result, as it corroborates our claim thatBRAINSUP can indeed be a valuable tool for copy-writing, even when it does not manage to output aperfectly good sentence.
Similar conclusions canbe drawn concerning the correctness of the output,as in almost one third of the cases the slogans are9For the binary decisions (i.e., catchiness, relatedness andhumor), at least 3 annotators out of 5 must necessarily agreeon the same option.only affected by minor disfluencies.The relatedness figure is especially high, as inalmost 94% of the cases the majority of annota-tors found the slogans to be pertinent to the tar-get domain.
This result is not surprising, as allthe slogans are generated by considering keywordsthat already exist in real slogans for the same do-main.
Anyhow, this is exactly the kind of setting inwhich we expect BRAINSUP to be employed, i.e.,to support creative sentence generation startingfrom a good set of relevant keywords.
Nonethe-less, it is very encouraging to observe that the gen-eration process does not deteriorate the positiveimpact of the input keywords.We would also like to mention that in 63 cases(14.58%) the majority of the annotators have la-beled the slogans favorably across all 5 dimen-sions.
The examples listed in Table 1 are selectedfrom this set.
It is interesting to observe howthe word associations established by BRAINSUPcan result in pertinent yet unintentional rhetori-cal devices such as metaphors (?a summer sun-shine?
), puns (?lash your drama?)
and personifica-tions (?lips and eyes want?).
Some examples showthe effect of the phonetic features, e.g.
plosives in?passionate kiss, perfect lips?, alliteration in ?thedark drink?
and rhyming in ?lips and eyes wantthe kiss?.
In some cases, the output of BRAINSUPseems to be governed by mysterious philosophicalreasoning, as in the delicate examples generatedfor ?soap?.For comparison, Table 4 lists a selection ofthe examples that have been labeled as unsuc-cessful by the majority of raters.
In some cases,BRAINSUP is improperly selecting attributes thathighlight undesirable properties in the target do-main, e.g., ?A pleasant tasting, a heady wine?.
Toavoid similar errors, it would be necessary to rea-son about the valence of an attribute for a spe-cific domain.
In other cases, the N -gram and theDependency Likelihood features may introducephrases which are very cohesive but unrelated tothe rest of the sentence, e.g., ?Unscrupulous doc-tors smoke armored units?.
Many of these errorscould be solved by increasing the weight of theSemantic Cohesion and Domain Relatedness scor-ers.
In other cases, such as ?A sixth calorie maytaste an own good?
or ?A same sunshine is fewerthan a juice of day?, more sophisticated reason-ing about syntactic and semantic relations in theoutput might be necessary in order to enforce thegeneration of sound and grammatical sentences.We could not find a significant correlation be-1453Domain Keywords BRAINSUP output examplespleasure wine, tast-ingA pleasant tasting, a heady wine.?
A fruity tasting may drink asparkling wine.healthy day, juice,sunshineDrink juice of your sunshine, andyour weight will choose day ofyou.
?
A same sunshine is fewerthan a juice of day.cigarette doctors,smokeUnscrupulous doctors smoke ar-mored units.
?
Doctors smoke noarrow.mascara drama,lashThe such drama is the lash.soap skin, love,touchThe touch of skin is the love ofcacophony.
?
You love an ownskin for a first touch.coke calorie,taste, goodA sixth calorie may taste an owngood.coffee waking,cupYou cannot cup hands withoutwaking some fats.Table 4: Unsuccessful BRAINSUP outputs.tween the input variables (e.g., presence or ab-sence of phonetic features or chromatic slanting)and the outcome of the annotation, i.e.
the sys-tem by and large produces correct, catchy, relatedand (at least potentially) successful outputs regard-less of the specific input configurations.
In this re-spect, it should be noted that we did not carry outany kind of optimization of the feature weights,which might be needed to obtain more heavilycharacterized sentences.
Furthermore, to betterappreciate the contribution of the individual fea-tures, comparative experiments in which the usersevaluate the system before and after triggering afeature function might be necessary.
Concern-ing the correlation among output dimensions, weonly observed relatively high Spearman correla-tion between correctness and relatedness (0.65),and catchiness and success (0.68).5 ConclusionWe have presented BRAINSUP, a novel systemfor creative sentence generation that allows usersto control many aspects of the creativity process,from the presence of specific target words in theoutput, to the selection of a target domain, andto the injection of phonetic and semantic proper-ties in the generated sentences.
BRAINSUP makesheavy use of dependency parsed data and statisticscollected from dependency treebanks to ensure thegrammaticality of the generated sentences, and totrim the search space while seeking the sentencesthat maximize the user satisfaction.The system has been designed as a support-ing tool for a variety of real-world applications,from advertisement to entertainment and educa-tion, where at the very least it can be a valu-able support for time-consuming and knowledge-intensive sentence generation needs.
To demon-strate this point, we carried out an evaluation on acreative sentence generation benchmark showingthat BRAINSUP can effectively produce catchy,memorable and successful sentences that have thepotential to inspire the work of copywriters.To our best knowledge, this is the first system-atic attempt to build an extensible framework thatallows for multi-dimensional creativity while atthe same time relying on syntactic constraints toenforce grammaticality.
In this regard, our ap-proach is dual with respect to previous work basedon lexical substitution, which suffers from limitedexpressivity and creativity latitude.
In addition, byacquiring the lexicon and the sentence structurefrom two distinct corpora, we can guarantee thatthe sentences that we generate have never beenobserved.
We believe that our contribution con-stitutes a valid starting point for other researchersto deal with unexplored dimensions of creativity.As future work, we plan to use machine learn-ing techniques to estimate optimal weights for thefeature functions in different use cases.
We wouldalso like to consider syntactic clues while reason-ing about semantic properties of the sentence, e.g.,color and emotion associations, instead on relyingsolely on lexical semantics.
Concerning the exten-sion of the capabilities of BRAINSUP, we want toinclude common-sense knowledge and reasoningto profit from more sophisticated semantic rela-tions and to inject humor on demand.
Further tun-ing of BRAINSUP to build a dedicated system forslogan generation is also part of our future plans.After these improvements, we would like to con-duct a more focused evaluation on slogan genera-tion involving human copywriters and domain ex-perts in an interactive setting.We would like to conclude this paper with a pearlof BRAINSUP?s wisdom:It is wiser to believe in sciencethan in everlasting love.AcknowledgmentsGo?zde O?zbal and Carlo Strapparava were partiallysupported by the PerTe project (Trento RISE).1454ReferencesKim Binsted and Graeme Ritchie.
1997.
Computa-tional rules for generating punning riddles.
Humor -International Journal of Humor Research, 10(1):25?76, January.Andy Borman, Rada Mihalcea, and Paul Tarau.
2005.Pic-net: Pictorial representations for illustrated se-mantic networks.
In Proceedings of the AAAI SpringSymposium on Knowledge Collection from Volun-teer Contributors.Simon Colton, Jacob Goodwin, and Tony Veale.
2012.Full-FACE Poetry Generation.
In Proceedings ofthe 3rd International Conference on ComputationalCreativity, pages 95?102.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
JournalOf The American Society for Information Science,41(6):391?407.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sen-tiwordnet: A publicly available lexical resourcefor opinion mining.
In In Proceedings of the 5thConference on Language Resources and Evaluation(LREC?06), pages 417?422.Erica Greene, Tugba Bodrumlu, and Kevin Knight.2010.
Automatic analysis of rhythmic poetrywith applications to generation and translation.
InEMNLP, pages 524?533.Marco Guerini, Carlo Strapparava, and Oliviero Stock.2011.
Slanting existing text with valentino.
In Pro-ceedings of the 16th international conference on In-telligent user interfaces, IUI ?11, pages 439?440,New York, NY, USA.
ACM.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 423?430, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Kevin S. LaBar and Roberto Cabeza.
2006.
Cognitiveneuroscience of emotional memory.
Nature reviews.Neuroscience, 7(1):54?64, January.Kevin Lenzo.
1998.
The cmu pronouncing dictionary.http://www.speech.cs.cmu.edu/cgi-bin/cmudict.Ruli Manurung, Graeme Ritchie, Helen Pain, An-nalu Waller, Dave O?Mara, and Rolf Black.
2008.The Construction of a Pun Generator for LanguageSkills Development.
Applied Artificial Intelligence,22(9):841?869, October.J McKay.
2002.
Generation of idiom-based witticismsto aid second language learning.
In Twente Work-shop on Language Technology 20, pages 70?74.R.
Mihalcea and C. Strapparava.
2006.
Learning tolaugh (automatically): Computational models forhumor recognition.
Journal of Computational In-telligence, 22(2):126?142, May.George A. Miller.
1995.
Wordnet: A lexical databasefor english.
Communications of the ACM, 38:39?41.Saif M. Mohammad and Peter D. Turney.
2010.
Emo-tions evoked by common words and phrases: usingmechanical turk to create an emotion lexicon.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Gener-ation of Emotion in Text, CAAGET ?10, pages 26?34, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Saif Mohammad.
2011.
Even the abstract have color:Consensus in word-colour associations.
In Proceed-ings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human Lan-guage Technologies, pages 368?373, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Go?zde O?zbal and Carlo Strapparava.
2011.
Autom-atized Memory Techniques for Vocabulary Acquisi-tion in a Second Language.
In Alexander Verbraeck,Markus Helfert, Jose?
Cordeiro, and Boris Shishkov,editors, CSEDU, pages 79?87.
SciTePress.Gozde Ozbal and Carlo Strapparava.
2012.
A compu-tational approach to the automation of creative nam-ing.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 703?711, Jeju Island,Korea, July.
Association for Computational Linguis-tics.N.
Sagarra and M. Alba.
2006.
The key is in thekeyword: L2 vocabulary learning methods with be-ginning learners of spanish.
The Modern LanguageJournal, 90(2):228?243.Oliviero Stock and Carlo Strapparava.
2006.
Laughingwith hahacronym, a computational humor system.In proceedings of the 21st national conference onArtificial intelligence - Volume 2, pages 1675?1678.AAAI Press.J.
M. Toivanen, H. Toivonen, A. Valitutti, and O. Gross.2012.
Corpus-based Generation of Content andForm in Poetry.
In International Conference onComputational Creativity, pages 175?179.A.
Valitutti, C. Strapparava, , and O.
Stock.
2009.Graphlaugh: a tool for the interactive generation ofhumorous puns.
In Proceedings of ACII-2009, ThirdConference on Affective Computing and IntelligentInteraction, Demo track.1455
