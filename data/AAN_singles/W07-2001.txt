Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 1?6,Prague, June 2007. c?2007 Association for Computational LinguisticsSemEval-2007 Task 01: Evaluating WSDon Cross-Language Information RetrievalEneko AgirreIXA NLP groupUniversity of the Basque CountryDonostia, Basque Counntrye.agirre@ehu.esOier Lopez de LacalleIXA NLP groupUniversity of the Basque CountryDonostia, Basque Countryjibloleo@ehu.esGerman RigauIXA NLP groupUniversity of the Basque CountryDonostia, Basque Countrygerman.rigau@ehu.esBernardo MagniniITC-IRSTTrento, Italymagnini@itc.itArantxa OtegiIXA NLP groupUniversity of the Basque CountryDonostia, Basque Countryjibotusa@ehu.esPiek VossenIrion TechnologiesDelftechpark 262628XH Delft, NetherlandsPiek.Vossen@irion.nlAbstractThis paper presents a first attempt of anapplication-driven evaluation exercise ofWSD.
We used a CLIR testbed from theCross Lingual Evaluation Forum.
The ex-pansion, indexing and retrieval strategieswhere fixed by the organizers.
The par-ticipants had to return both the topics anddocuments tagged with WordNet 1.6 wordsenses.
The organization provided trainingdata in the form of a pre-processed Semcorwhich could be readily used by participants.The task had two participants, and the orga-nizer also provide an in-house WSD systemfor comparison.1 IntroductionSince the start of Senseval, the evaluation of WordSense Disambiguation (WSD) as a separate task is amature field, with both lexical-sample and all-wordstasks.
In the first case the participants need to tag theoccurrences of a few words, for which hand-taggeddata has already been provided.
In the all-words taskall the occurrences of open-class words occurring intwo or three documents (a few thousand words) needto be disambiguated.The community has long mentioned the neces-sity of evaluating WSD in an application, in orderto check which WSD strategy is best, and more im-portant, to try to show that WSD can make a differ-ence in applications.
The use of WSD in MachineTranslation has been the subject of some recent pa-pers, but less attention has been paid to InformationRetrieval (IR).With this proposal we want to make a first try todefine a task where WSD is evaluated with respectto an Information Retrieval and Cross-Lingual Infor-mation Retrieval (CLIR) exercise.
From the WSDperspective, this task will evaluate all-words WSDsystems indirectly on a real task.
From the CLIRperspective, this task will evaluate which WSD sys-tems and strategies work best.We are conscious that the number of possible con-figurations for such an exercise is very large (in-cluding sense inventory choice, using word sense in-duction instead of disambiguation, query expansion,WSD strategies, IR strategies, etc.
), so this first edi-tion focuses on the following:?
The IR/CLIR system is fixed.?
The expansion / translation strategy is fixed.?
The participants can choose the best WSDstrategy.1?
The IR system is used as the upperbound forthe CLIR systems.We think that it is important to start doing thiskind of application-driven evaluations, which mightshed light to the intricacies in the interaction be-tween WSD and IR strategies.
We see this as thefirst of a series of exercises, and one outcome of thistask should be that both WSD and CLIR communi-ties discuss together future evaluation possibilities.This task has been organized in collabora-tion with the Cross-Language Evaluation Forum(CLEF1).
The results will be analyzed in the CLEF-2007 workshop, and a special track will be pro-posed for CLEF-2008, where CLIR systems willhave the opportunity to use the annotated dataproduced as a result of the Semeval-2007 task.The task has a webpage with all the details athttp://ixa2.si.ehu.es/semeval-clir.This paper is organized as follows.
Section 2describes the task with all the details regardingdatasets, expansion/translation, the IR/CLIR systemused, and steps for participation.
Section 3 presentsthe evaluation performed and the results obtained bythe participants.
Finally, Section 4 draws the con-clusions and mention the future work.2 Description of the taskThis is an application-driven task, where the appli-cation is a fixed CLIR system.
Participants disam-biguate text by assigning WordNet 1.6 synsets andthe system will do the expansion to other languages,index the expanded documents and run the retrievalfor all the languages in batch.
The retrieval resultsare taken as the measure for fitness of the disam-biguation.
The modules and rules for the expansionand the retrieval will be exactly the same for all par-ticipants.We proposed two specific subtasks:1.
Participants disambiguate the corpus, the cor-pus is expanded to synonyms/translations andwe measure the effects on IR/CLIR.
Topics2 arenot processed.1http://www.clef-campaign.org2In IR topics are the short texts which are used by the sys-tems to produce the queries.
They usually provide extensiveinformation about the text to be searched, which can be usedboth by the search engine and the human evaluators.2.
Participants disambiguate the topics per lan-guage, we expand the queries to syn-onyms/translations and we measure the effectson IR/CLIR.
Documents are not processedThe corpora and topics were obtained from thead-hoc CLEF tasks.
The supported languages in thetopics are English and Spanish, but in order to limitthe scope of the exercise we decided to only use En-glish documents.
The participants only had to dis-ambiguate the English topics and documents.
Notethat most WSD systems only run on English text.Due to these limitations, we had the followingevaluation settings:IR with WSD of topics , where the participantsdisambiguate the documents, the disam-biguated documents are expanded to syn-onyms, and the original topics are used forquerying.
All documents and topics are in En-glish.IR with WSD of documents , where the partic-ipants disambiguate the topics, the disam-biguated topics are expanded and used forquerying the original documents.
All docu-ments and topics are in English.CLIR with WSD of documents , where the partic-ipants disambiguate the documents, the dis-ambiguated documents are translated, and theoriginal topics in Spanish are used for query-ing.
The documents are in English and the top-ics are in Spanish.We decided to focus on CLIR for evaluation,given the difficulty of improving IR.
The IR resultsare given as illustration, and as an upperbound ofthe CLIR task.
This use of IR results as a referencefor CLIR systems is customary in the CLIR commu-nity (Harman, 2005).2.1 DatasetsThe English CLEF data from years 2000-2005 com-prises corpora from ?Los Angeles Times?
(year1994) and ?Glasgow Herald?
(year 1995) amountingto 169,477 documents (579 MB of raw text, 4.8GBin the XML format provided to participants, see Sec-tion 2.3) and 300 topics in English and Spanish (thetopics are human translations of each other).
Therelevance judgments were taken from CLEF.
This2might have the disadvantage of having been pro-duced by pooling the results of CLEF participants,and might bias the results towards systems not usingWSD, specially for monolingual English retrieval.We are considering the realization of a post-hocanalysis of the participants results in order to ana-lyze the effect on the lack of pooling.Due to the size of the document collection, we de-cided that the limited time available in the competi-tion was too short to disambiguate the whole collec-tion.
We thus chose to take a sixth part of the corpusat random, comprising 29,375 documents (874MBin the XML format distributed to participants).
Notall topics had relevant documents in this 17% sam-ple, and therefore only 201 topics were effectivelyused for evaluation.
All in all, we reused 21,797relevance judgements that contained one of the doc-uments in the 17% sample, from which 923 are pos-itive3.
For the future we would like to use the wholecollection.2.2 Expansion and translationFor expansion and translation we used the publiclyavailable Multilingual Central Repository (MCR)from the MEANING project (Atserias et al, 2004).The MCR follows the EuroWordNet design, andcurrently includes English, Spanish, Italian, Basqueand Catalan wordnets tightly connected through theInterlingual Index (based on WordNet 1.6, but linkedto all other WordNet versions).We only expanded (translated) the senses returnedby the WSD systems.
That is, given a word like?car?, it will be expanded to ?automobile?
or ?railcar?
(and translated to ?auto?
or ?vago?n?
respectively) de-pending on the sense in WN 1.6.
If the systems re-turns more than one sense, we choose the sense withmaximum weight.
In case of ties, we expand (trans-late) all.
The participants could thus implicitly affectthe expansion results, for instance, when no sensecould be selected for a target noun, the participantscould either return nothing (or NOSENSE, whichwould be equivalent), or all senses with 0 score.
Inthe first case no expansion would be performed, inthe second all senses would be expanded, which isequivalent to full expansion.
This fact will be men-tioned again in Section 3.5.3The overall figures are 125,556 relevance judgements forthe 300 topics, from which 5700 are positiveNote that in all cases we never delete any of thewords in the original text.In addition to the expansion strategy used with theparticipants, we tested other expansion strategies asbaselines:noexp no expansion, original textfullexp expansion (translation in the case of Englishto Spanish expansion) to all synonyms of allsenseswsd50 expansion to the best 50% senses as returnedby the WSD system.
This expansion was triedover the in-house WSD system of the organizeronly.2.3 IR/CLIR systemThe retrieval engine is an adaptation of the Twenty-One search system (Hiemstra and Kraaij, 1998) thatwas developed during the 90?s by the TNO researchinstitute at Delft (The Netherlands) getting good re-sults on IR and CLIR exercises in TREC (Harman,2005).
It is now further developed by Irion technolo-gies as a cross-lingual retrieval system (Vossen et al,).
For indexing, the TwentyOne system takes NounPhrases as an input.
Noun Phases (NPs) are detectedusing a chunker and a word form with POS lexicon.Phrases outside the NPs are not indexed, as well asnon-content words (determiners, prepositions, etc.
)within the phrase.The Irion TwentyOne system uses a two-stage re-trieval process where relevant documents are firstextracted using a vector space matching and sec-ondly phrases are matched with specific queries.Likewise, the system is optimized for high-precisionphrase retrieval with short queries (1 up 5 wordswith a phrasal structure as well).
The system can bestripped down to a basic vector space retrieval sys-tem with an tf.idf metrics that returns documents fortopics up to a length of 30 words.
The stripped-downversion was used for this task to make the retrievalresults compatible with the TREC/CLEF system.The Irion system was also used for pre-processing.
The CLEF corpus and topics were con-verted to the TwentyOne XML format, normalized,and named-entities and phrasal structured detected.Each of the target tokens was identified by an uniqueidentifier.2.4 ParticipationThe participants were provided with the following:31. the document collection in Irion XML format2.
the topics in Irion XML formatIn addition, the organizers also provided some ofthe widely used WSD features in a word-to-wordfashion4 (Agirre et al, 2006) in order to make partic-ipation easier.
These features were available for bothtopics and documents as well as for all the wordswith frequency above 10 in SemCor 1.6 (which canbe taken as the training data for supervised WSDsystems).
The Semcor data is publicly available 5.For the rest of the data, participants had to sign andend user agreement.The participants had to return the input files en-riched with WordNet 1.6 sense tags in the requiredXML format:1. for all the documents in the collection2.
for all the topicsScripts to produce the desired output from word-to-word files and the input files were provided byorganizers, as well as DTD?s and software to checkthat the results were conformant to the respectiveDTD?s.3 Evaluation and resultsFor each of the settings presented in Section 2 wepresent the results of the participants, as well asthose of an in-house system presented by the orga-nizers.
Please refer to the system description papersfor a more complete description.
We also providesome baselines and alternative expansion (transla-tion) strategies.
All systems are evaluated accord-ing to their Mean Average Precision 6 (MAP) ascomputed by the trec eval software on the pre-existing CLEF relevance-assessments.3.1 ParticipantsThe two systems that registered sent the results ontime.PUTOP They extend on McCarthy?s predominantsense method to create an unsupervised methodof word sense disambiguation that uses auto-matically derived topics using Latent Dirichlet4Each target word gets a file with all the occurrences, andeach occurrence gets the occurrence identifier, the sense tag (ifin training), and the list of features that apply to the occurrence.5http://ixa2.si.ehu.es/semeval-clir/6http://en.wikipedia.org/wiki/Information retrievalAllocation.
Using topic-specific synset similar-ity measures, they create predictions for eachword in each document using only word fre-quency information.
The disambiguation pro-cess took aprox.
12 hours on a cluster of 48 ma-chines (dual Xeons with 4GB of RAM).
Notethat contrary to the specifications, this teamreturned WordNet 2.1 senses, so we had tomap automatically to 1.6 senses (Daude et al,2000).UNIBA This team uses a a knowledge-based WSDsystem that attempts to disambiguate all wordsin a text by exploiting WordNet relations.
Themain assumption is that a specific strategy foreach Part-Of-Speech (POS) is better than a sin-gle strategy.
Nouns are disambiguated basi-cally using hypernymy links.
Verbs are dis-ambiguated according to the nouns surroundingthem, and adjectives and adverbs use glosses.ORGANIZERS In addition to the regular partic-ipants, and out of the competition, the orga-nizers run a regular supervised WSD systemtrained on Semcor.
The system is based ona single k-NN classifier using the features de-scribed in (Agirre et al, 2006) and made avail-able at the task website (cf.
Section 2.4).In addition to those we also present some com-mon IR/CLIR baselines, baseline WSD systems, andan alternative expansion:noexp a non-expansion IR/CLIR baseline of thedocuments or topics.fullexp a full-expansion IR/CLIR baseline of thedocuments or topics.wsdrand a WSD baseline system which chooses asense at random.
The usual expansion is ap-plied.1st a WSD baseline system which returns the sensenumbered as 1 in WordNet.
The usual expan-sion is applied.wsd50 the organizer?s WSD system, where the 50%senses of the word ranking according to theWSD system are expanded.
That is, instead ofexpanding the single best sense, it expands thebest 50% senses.3.2 IR ResultsThis section present the results obtained by the par-ticipants and baselines in the two IR settings.
The4IRtops IRdocs CLIRno expansion 0.3599 0.3599 0.1446full expansion 0.1610 0.1410 0.2676UNIBA 0.3030 0.1521 0.1373PUTOP 0.3036 0.1482 0.1734wsdrand 0.2673 0.1482 0.26171st sense 0.2862 0.1172 0.2637ORGANIZERS 0.2886 0.1587 0.2664wsd50 0.2651 0.1479 0.2640Table 1: Retrieval results given as MAP.
IRtopsstands for English IR with topic expansion.
IR-docs stands for English IR with document expan-sion.
CLIR stands for CLIR results for translateddocuments.second and third columns of Table 1 present the re-sults when disambiguating the topics and the docu-ments respectively.
Non of the expansion techniquesimproves over the baseline (no expansion).Note that due to the limitation of the search en-gine, long queries were truncated at 50 words, whichmight explain the very low results of the full expan-sion.3.3 CLIR resultsThe last column of Table 1 shows the CLIR resultswhen expanding (translating) the disambiguateddocuments.
None of the WSD systems attains theperformance of full expansion, which would be thebaseline CLIR system, but the WSD of the organizergets close.3.4 WSD resultsIn addition to the IR and CLIR results we also pro-vide the WSD performance of the participants onthe Senseval 2 and 3 all-words task.
The documentsfrom those tasks were included alongside the CLEFdocuments, in the same formats, so they are treatedas any other document.
In order to evaluate, we hadto map automatically all WSD results to the respec-tive WordNet version (using the mappings in (Daudeet al, 2000) which are publicly available).The results are presented in Table 2, where we cansee that the best results are attained by the organizersWSD system.3.5 DiscussionFirst of all, we would like to mention that the WSDand expansion strategy, which is very simplistic, de-grades the IR performance.
This was rather ex-Senseval-2 all wordsprecision recall coverageORGANIZERS 0.584 0.577 93.61%UNIBA 0.498 0.375 75.39%PUTOP 0.388 0.240 61.92%Senseval-3 all wordsprecision recall coverageORGANIZERS 0.591 0.566 95.76%UNIBA 0.484 0.338 69.98%PUTOP 0.334 0.186 55.68%Table 2: English WSD results in the Senseval-2 andSenseval-3 all-words datasets.pected, as the IR experiments had an illustrationgoal, and are used for comparison with the CLIRexperiments.
In monolingual IR, expanding the top-ics is much less harmful than expanding the docu-ments.
Unfortunately the limitation to 50 words inthe queries might have limited the expansion of thetopics, which make the results rather unreliable.
Weplan to fix this for future evaluations.Regarding CLIR results, even if none of the WSDsystems were able to beat the full-expansion base-line, the organizers system was very close, which isquite encouraging due to the very simplistic expan-sion, indexing and retrieval strategies used.In order to better interpret the results, Table 3shows the amount of words after the expansion ineach case.
This data is very important in order to un-derstand the behavior of each of the systems.
Notethat UNIBA returns 3 synsets at most, and thereforethe wsd50 strategy (select the 50% senses with bestscore) leaves a single synset, which is the same astaking the single best system (wsdbest).
RegardingPUTOP, this system returned a single synset, andtherefore the wsd50 figures are the same as the ws-dbest figures.Comparing the amount of words for the two par-ticipant systems, we see that UNIBA has the leastwords, closely followed by PUTOP.
The organizersWSD system gets far more expanded words.
Theexplanation is that when the synsets returned by aWSD system all have 0 weights, the wsdbest expan-sion strategy expands them all.
This was not explicitin the rules for participation, and might have affectedthe results.A cross analysis of the result tables and the num-ber of words is interesting.
For instance, in the IRexercise, when we expand documents, the results in5English SpanishNo WSD noexp 9,900,818 9,900,818fullexp 93,551,450 58,491,767UNIBAwsdbest 19,436,374 17,226,104wsd50 19,436,374 17,226,104PUTOP wsdbest 20,101,627 16,591,485wsd50 20,101,627 16,591,485Baseline 1st 24,842,800 20,261,081WSD wsdrand 24,904,717 19,137,981ORG.
wsdbest 26,403,913 21,086,649wsd50 36,128,121 27,528,723Table 3: Number of words in the document col-lection after expansion for the WSD system and allbaselines.
wsdbest stands for the expansion strategyused with participants.the third column of Table 1 show that the ranking forthe non-informed baselines is the following: best forno expansion, second for random WSD, and thirdfor full expansion.
These results can be explainedbecause of the amount of expansion: the more ex-pansion the worst results.
When more informedWSD is performed, documents with more expansioncan get better results, and in fact the WSD system ofthe organizers is the second best result from all sys-tem and baselines, and has more words than the rest(with exception of wsd50 and full expansion).
Still,the no expansion baseline is far from the WSD re-sults.Regarding the CLIR result, the situation is in-verted, with the best results for the most productiveexpansions (full expansion, random WSD and no ex-pansion, in this order).
For the more informed WSDmethods, the best results are again for the organizersWSD system, which is very close to the full expan-sion baseline.
Even if wsd50 has more expandedwords wsdbest is more effective.
Note the very highresults attained by random.
These high results canbe explained by the fact that many senses get thesame translation, and thus for many words with fewtranslation, the random translation might be valid.Still the wsdbest, 1st sense and wsd50 results getbetter results.4 Conclusions and future workThis paper presents the results of a preliminary at-tempt of an application-driven evaluation exerciseof WSD in CLIR.
The expansion, indexing and re-trieval strategies proved too simplistic, and none ofthe two participant systems and the organizers sys-tem were able to beat the full-expansion baseline.Due to efficiency reasons, the IRION system hadsome of its features turned off.
Still the results areencouraging, as the organizers system was able toget very close to the full expansion strategy withmuch less expansion (translation).For the future, a special track of CLEF-2008 willleave the avenue open for more sophisticated CLIRtechniques.
We plan to extend the WSD annotationto all words in the CLEF English document collec-tion, and we also plan to contact the best performingsystems of the SemEval all-words tasks to have bet-ter quality annotations.AcknowledgementsWe wish to thank CLEF for allowing us to use their data, and theCLEF coordinator, Carol Peters, for her help and collaboration.This work has been partially funded by the Spanish educationministry (project KNOW)ReferencesE.
Agirre, O. Lopez de Lacalle, and D. Martinez.
2006.Exploring feature set combinations for WSD.
In Proc.of the SEPLN.J.
Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,B.
Magnini, and P. Vossen.
2004.
The MEANINGMultilingual Central Repository.
In Proceedings of the2.nd Global WordNet Conference, GWC 2004, pages23?30.
Masaryk University, Brno, Czech Republic.J.
Daude, L. Padro, and G. Rigau.
2000.
Mapping Word-Nets Using Structural Information.
In Proc.
of ACL,Hong Kong.D.
Harman.
2005.
Beyond English.
In E. M. Voorheesand D. Harman, editors, TREC: Experiment and Eval-uation in Information Retrieval, pages 153?181.
MITpress.D.
Hiemstra and W. Kraaij.
1998.
Twenty-One in ad-hocand CLIR.
In E.M. Voorhees and D. K. Harman, ed-itors, Proc.
of TREC-7, pages 500?540.
NIST SpecialPublication.P.
Vossen, G. Rigau, I. Alegria, E. Agirre, D. Farwell,and M. Fuentes.
Meaningful results for InformationRetrieval in the MEANING project.
In Proc.
of the3rd Global Wordnet Conference.6
