LEARNING TO RESOLVE BRIDGING REFERENCESMassimo Poesio,?
Rahul Mehta,?
Axel Maroudas,?
and Janet Hitzeman??Dept.
of Comp.
Science, University of Essex, UK poesio at essex dot ac dot uk?MITRE Corporation, USA hitz at mitre dot orgAbstractWe use machine learning techniques to find thebest combination of local focus and lexical distancefeatures for identifying the anchor of mereologicalbridging references.
We find that using first men-tion, utterance distance, and lexical distance com-puted using either Google or WordNet results in anaccuracy significantly higher than obtained in pre-vious experiments.1 IntroductionBRIDGING REFERENCES (BR) (Clark, 1977)?anaphoric expressions that cannot be resolvedpurely on the basis of string matching and thus re-quire the reader to ?bridge?
the gap using common-sense inferences?are arguably the most interestingand, at the same time, the most challenging prob-lem in anaphora resolution.
Work such as (Poesioet al, 1998; Poesio et al, 2002; Poesio, 2003) pro-vided an experimental confirmation of the hypoth-esis first put forward by Sidner (1979) that BRIDG-ING DESCRIPTIONS (BD)1 are more similar to pro-nouns than to other types of definite descriptions,in that they are sensitive to the local rather than theglobal focus (Grosz and Sidner, 1986).
This previ-uous work also suggested that simply choosing theentity whose description is lexically closest to thatof the bridging description among those in the cur-rent focus space gives poor results; in fact, better re-sults are obtained by always choosing as ANCHORof the bridging reference2 the first-mentioned entityof the previous sentence (Poesio, 2003).
But nei-ther source of information in isolation resulted in anaccuracy over 40%.
In short, this earlier work sug-gested that a combination of salience and lexical /1We will use the term bridging descriptions to indicatebridging references realized by definite descriptions, equatedhere with noun phrases with determiner the, like the top.2Following (Poesio and Vieira, 1998), we use the term ?an-chor?
as as a generalization of the term ANTECEDENT, to indi-cate the discourse entity which an anaphoric expression eitherrealizes, or is related to by an associative relation; reserving?antecedent?
for the cases of identity.commonsense information is needed to choose themost likely anchor; the problem remained of how tocombine this information.In the work described in this paper, we used ma-chine learning techniques to find the best combina-tion of local focus features and lexical distance fea-tures, focusing on MEREOLOGICAL bridging refer-ences:3 references referring to parts of an object al-ready introduced (the cabinet), such as the panels orthe top (underlined) in the following example fromthe GNOME corpus (Poesio et al, 2004).
(1) The combination of rare and expensive ma-terials used on [this cabinet]i indicates thatit was a particularly expensive commission.The four Japanese lacquer panels date from themid- to late 1600s and were created with a techniqueknown as kijimaki-e.For this type of lacquer, artisans sanded plain woodto heighten its strong grain and used it as the back-ground of each panel.
They then added the scenicelements of landscape, plants, and animals in raisedlacquer.
Although this technique was common inJapan, such large panels were rarely incorporatedinto French eighteenth-century furniture.Heavy Ionic pilasters, whose copper-filled flutesgive an added rich color and contrast to the gilt-bronze mounts, flank the panels.
Yellow jasper, asemiprecious stone, rather than the usual marble,forms the top.2 Two sources of information for bridgingreference resolution2.1 Lexical informationThe use of different sources of lexical knowledgefor resolving bridging references has been inves-tigated in a series of papers by Poesio et al allusing as dataset the Bridging Descriptions (BDs)contained in the corpus used by Vieira and Poesio3We make use of the classification of bridging referencesproposed by Vieira and Poesio (2000).
?Mereological?
bridgingreferences are one of the the ?WordNet?
bridging classes, whichcover cases where the information required to bridge the gapmay be found in a resource such as WordNet (Fellbaum, 1998):synonymy, hyponymy, and meronymy.(2000).
In these studies, the lexical distance be-tween a BD and its antecedent was used to choosethe anchor for the BD among the antecedents in theprevious five sentences.
In (Poesio et al, 1997;Vieira and Poesio, 2000) WordNet 1.6 was used asa lexical resource, with poor or mediocre results.These results were due in part to missing entriesand / or relations; in part to the fact that because ofthe monotonic organization of information in Word-Net, complex searches are required even to find ap-parently close associations (like that between wheeland car).
Similar results using WordNet 1.6 werereported at around the same time by other groups- e.g., (Humphreys et al, 1997; Harabagiu andMoldovan, 1998) and have been confirmed by morerecent studies studying both hyponymy (Markert etal., 2003) and more specifically mereological BDs.Poesio (2003) found that none of the 58 mereo-logical references in the GNOME corpus (discussedbelow) had a direct mereological link to their an-chor: for example, table is not listed as a possi-ble holonym of drawer, nor is house listed as apossible holonym for furniture.
Garcia-Almanza(2003) found that only 16 of these 58 mereologi-cal references could be resolved by means of morecomplex searches in WordNet, including followingthe hypernymy hierarchy for both the anchor andthe bridging reference, and a ?spreading activation?search.Poesio et al (1998) explored the usefulness ofvector-space representations of lexical meaning forBDs that depended on lexical knowledge about hy-ponymy and synonymy.
The HAL model discussedin Lund et al (1995) was used to find the anchorof the BDs in the dataset aleady used by Poesioet al (1997).
However, using vectorial represen-tations did not improve the results for the ?Word-Net?
BDs: for the synonymy cases the results werecomparable to those obtained with WordNet (4/12,33%), but for the hyponymy BDs (2/14, as opposedto 8/14 with WordNet) and especially for mereolog-ical references (2/12) they were clearly worse.
Onthe other hand, the post-hoc analysis of results sug-gested that the poor results were in part due to thelack of mechanisms for choosing the most salient(or most recent) BDs.The poor results for mereological BDs with bothWordNet and vectorial representations indicatedthat a different approach was needed to acquire in-formation about part-of relations.
Grefenstette?swork on semantic similarity (Grefenstette, 1993)and Hearst?s work on acquiring taxonomic informa-tion (Hearst, 1998) suggested that certain syntacticconstructions could be usefully viewed as reflect-ing underlying semantic relations.
In (Ishikawa,1998; Poesio et al, 2002) it was proposed thatsyntactic patterns (henceforth: CONSTRUCTIONS)such as the wheel of the car could indicate thatwheel and car stood in a part-of relation.4 Vector-based lexical representations whose elements en-coded the strength of associations identified bymeans of constructions like the one discussed wereconstructed from the British National Corpus, us-ing Abney?s CASS chunker.
These representationswere then used to choose the anchor of BDs, us-ing again the same dataset and the same methodsas in the previous two attempts, and using mutualinformation to determine the strength of associa-tion.
The results on mereological BDs?recall .67,precision=.73?were drastically better than those ob-tained with WordNet or with simple vectorial repre-sentations.
The results with the three types of lex-ical resources and the different types of BDs in theVieira / Poesio dataset are summarized in Table 1.Finally, a number of researchers recently arguedfor using the Web as a way of addressing datasparseness (Keller and Lapata, 2003).
The Webhas proven a useful resource for work in anaphoraresolution as well.
Uryupina (2003) used the Webto estimate ?Definiteness probabilities?
used as afeature to identify discourse-new definites.
Mark-ert et al (2003) used the Web and the construc-tion method to extract information about hyponymyused to resolve other-anaphora (achieving an fvalue of around 67%) as well as the BDs in theVieira-Poesio dataset (their results for these caseswere not better than those obtained by (Vieira andPoesio, 2000)).
Markert et al also found a sharpdifference between using the Web as a a corpusand using the BNC, the results in the latter case be-ing significantly worse than when using WordNet.Poesio (2003) used the Web to choose between thehypotheses concerning the anchors of mereologicalBDs in the GNOME corpus generated on the basis ofCentering information (see below).2.2 SalienceOne of the motivations behind Grosz and Sidner?s(1986) distinction between two aspects of the atten-tional state - the LOCAL FOCUS and the GLOBALFOCUS?is the difference between the interpretivepreferences of pronouns and definite descriptions.According to Grosz and Sidner, the interpretationfor pronouns is preferentially found in the local fo-cus, whereas that of definite descriptions is prefer-entially found in the global focus.4A similar approach was pursued in parallel by Berland andCharniak (1999).Synonymy Hyponymy Meronymy Total WN Total BDsBDs in Vieira / Poesio corpus 12 14 12 38 204Using WordNet 4 (33.3%) 8(57.1%) 3(33.3%) 15 (39%) 34 (16.7%)Using HAL Lexicon 4 (33.3%) 2(14.3%) 2(16.7%) 8 (22.2%) 46(22.7%)Using Construction Lexicon 1 (8.3%) 0 8(66.7%) 9 (23.7%) 34(16.7%)Table 1: BD resolution results using only lexical distance with WordNet, HAL-style vectorial lexicon,and construction-based lexicon.However, already Sidner (1979) hypothesizedthat BDs are different from other definite descrip-tions, in that the local focus is preferred for their in-terpretation.
As already mentioned, the error analy-sis of Poesio et al (1998) supported this finding: thestudy found that the strategy found to be optimal foranaphoric definite descriptions by Vieira and Poesio(2000), considering as equally likely all antecedentsin the previous five-sentence window (as opposed topreferring closer antecedents), gave poor results forbridging references; entities introduced in the lasttwo sentences and ?main entities?
were clearly pre-ferred.
The following example illustrates how thelocal focus affects the interpretation of a mereolog-ical BD, the sides, in the third sentence.
(2) [Cartonnier (Filing Cabinet)]i with Clock[This piece of mid-eighteenth-centuryfurniture]i was meant to be used like a modernfiling cabinet; papers were placed in [leather-fronted cardboard boxes]j (now missing) thatwere fitted into the open shelves.
[A large table]k decorated in the same mannerwould have been placed in front for workingwith those papers.Access to [the cartonnier]i?s lower half canonly be gained by the doors at the sides, be-cause the table would have blocked the front.The three main candidate anchors in this example?the cabinet, the boxes, and the table?all have sides.However, the actual anchor, the cabinet, is clearlythe Backward-Looking Center (CB) (Grosz et al,1995) of the first sentence after the title;5 and ifwe assume that entities can be indirectly realized?see (Poesio et al, 2004)?the cabinet is the CB ofall three sentences, including the one containing theBR, and therefore a preferred candidate.In (Poesio, 2003), the impact on associative BDresolution of both relatively simple salience features(such as distance and order or mention) and of morecomplex ones (such as whether the anchor was a CBor not) was studied using the GNOME corpus (dis-cussed below) and the CB-tracking techniques de-veloped to compare alternative ways of instantiating5The CB is Centering theory?s (Grosz et al, 1995) imple-mentation of the notion of ?topic?
or ?main entity?.the parameters of Centering by Poesio et al (2004).Poesio (2003) analyzed, first of all, the distance be-tween the BD and the closest mention of the an-chor, finding that of the 169 associative BDs, 77.5%had an anchor occurring either in the same sentence(59) or the previous one (72); and that only 4.2% ofanchors were realized more than 5 sentences back.These percentages are very similar to those foundwith pronouns (Hobbs, 1978).Next, Poesio analyzed the order of mention of theanchors of the 72 associative BD whose anchor wasin the previous sentence, finding that 49/72, 68%,were realized in first position.
This finding is con-sistent with the preference for first-mentioned enti-ties (as opposed to the most recent ones) repeatedlyobserved in the psychological literature on anaphora(Gernsbacher and Hargreaves, 1988; Gordon et al,1993).
Finally, Poesio examined the hypothesis thatfinding the anchor of a BD involves knowing whichentities are the CB and the CP in the sense of Cen-tering (Grosz et al, 1995).
He found that CB(U-1)is the anchor of 37/72 of the BDs whose anchor isin the previous utterance (51.3%), and only 33.6%overall.
(CP(U-1) was the anchor for 38.2% asso-ciative BDs.)
Clearly, simply choosing the CB(or the CP) of the previous sentence as the anchordoesn?t work very well.
However, Poesio also foundthat 89% of the anchors of associative BDs had beenCBs or CPs.
This suggested that while knowing thelocal focus isn?t sufficient to determine the anchorof a BD, restricting the search for anchors to CBsand CPs only might increase the precision of the BDresolution process.
This hypothesis was supportedby a preliminary test with 20 associative BDs.
Theanchor for a BD with head noun NBD was chosenamong the subset of all potential antecedents (PA)in the previous five sentences that had been CBs orCPs by calling Google (by hand) with the query ?theNBD of the NPA?, where NPA is the head noun of thepotential antecedent, and choosing the PA with thehighest hit count.
14 mereological BDs (70%) wereresolved correctly this way.3 MethodsThe results just discussed suggest that lexical infor-mation and salience information combine to deter-mine the anchor of associative BRs.
The goal of theexperiments discussed in this paper was to test morethoroughly this hypothesis using machine learningtechniques to combine the two types of informa-tion, using a larger dataset than used in this pre-vious work, and using completely automatic tech-niques.
We concentrated on mereological BDs,but our methods could be used to study other typesof bridging references, using, e.g., the constructionsused by Markert et al (2003).63.1 The corpusWe used for these experiments the GNOME corpus,already used in (Poesio, 2003).
An important prop-erty of this corpus for the purpose of studying BRresolution is that fewer types of BDs are annotatedthan in the original Vieira / Poesio dataset, but theannotation is reliable (Poesio et al, 2004).7 The cor-pus also contains more mereological BDs and BRsthan the original dataset used by Poesio and Vieira.The GNOME corpus contains about 500 sentencesand 3000 NPs.
A variety of semantic and discourseinformation has been annotated (the manual isavailable from the GNOME project?s home page athttp://www.hcrc.ed.ac.uk/ ?
gnome).Four types of anaphoric relations were annotated:identity (IDENT), set membership (ELEMENT),subset (SUBSET), and ?generalized possession?
(POSS), which also includes part-of relations.
Atotal of 2073 anaphoric relations were annotated;these include 1164 identity relations (includingthose realized with synonyms and hyponyms) and153 POSS relations.Bridging references are realized by noun phrasesof different types, including indefinites (as in Ibought a book and a page fell out (Prince, 1981)).Of the 153 mereological references, 58 mereologi-cal references are realized by definite descriptions.6In (Poesio, 2003), bridging descriptions based on set rela-tions (element, subset) were also considered, but we found thatthis class of BDs required completely different methods.7A serious problem when working with bridging referencesis the fact that subjects, when asked for judgments about bridg-ing references in general, have a great deal of difficulty inagreeing on which expressions in the corpus are bridging ref-erences, and what their anchors are (Poesio and Vieira, 1998).This finding raises a number of interesting theoretical questionsconcerning the extent of agreement on semantic judgments, butalso the practical question of whether it is possible to evalu-ate the performance of a system on this task.
Subsequent workfound, however, that restricting the type of bridging inferencesrequired does make it possible for annotators to agree amongthemselves (Poesio et al, 2004).
In the GNOME corpus onlya few types of associative relations are marked, but these canbe marked reliably, and do include part-of relations like thatbetween the top and the cabinet that we are concerned with.3.2 FeaturesOur classifiers use two types of input features.Lexical features Only one lexical feature wasused: lexical distance, but extracted from two dif-ferent lexical sources.Google distance was computed as in (Poesio,2003) (see also Markert et al (2003)): given headnouns NBD of the BD and NPA of a potential an-tecedent, Google is called (via the Google API) witha query of the form ?the NBD of the NPA?
(e.g., thesides of the table) and the number of hits NHits iscomputed.
ThenGoogle distance ={1 if NHits = 01NHits otherwiseThe query ?the NBD of NPA?
(e.g., the amount ofcream) is used when NPA is used as a mass noun(information about mass vs count is annotated in theGNOME corpus).
If the potential antecedent is a pro-noun, the head of the closest realization of the samediscourse entity is used.We also reconsidered WordNet (1.7.1) as an al-ternative way of establishing lexical distance, butmade a crucial change from the studies reportedabove.
Both earlier studies such as (Poesio et al,1997) and more recent ones (Poesio, 2003; Garcia-Almanza, 2003) had shown that mereological infor-mation in WordNet is extremely sparse.
However,these studies also showed that information about hy-pernyms is much more extensive.
This suggestedtrading precision for recall with an alternative wayof using WordNet to compute lexical distance: in-stead of requiring the path between the head pred-icate of the associative BD and the head predicateof the potential antecedent to contain at least onemereological link (various strategies for performinga search of this type were considered in (Garcia-Almanza, 2003)), consider only hypernymy and hy-ponymy links.To compute our second measure of lexical dis-tance between NBD and NPA defined as above,WordNet distance, the following algorithm wasused.
Let distance(s, s?)
be the number of hyper-nim links between concepts s and s?.
Then1.
Get from WordNet al the senses of both NBDand NPA;2.
Get the hypernym tree of each of these senses;3.
For each pair of senses sNBDi and sNPAj , findthe Most Specific Common Subsumer scommij(this is the closest concept which is an hyper-nym of both senses).4.
The ShortestWNDistance between NBD andNPA is then computed as the shortest distancebetween any of the senses of NBD and any ofthe senses of NPA:ShtstWNDist(NBD,NPA) =mini,j(distance(sNBDi , scomij ) + distance(scomij , sNPAj ))5.
Finally, a normalized WordNet distance in therange 0..1 is then obtained by dividing Shtst-WNDist by a MaxWNDist factor (30 in our ex-periments).
WordNet distance = 1 if no pathbetween the concepts was found.WN distance ={1 if no pathShtstWNDistMaxWNDist otherwiseSalience features In choosing the salience fea-tures we took into account the results in (Poesio,2003), but we only used features that were easy tocompute, hoping that they would approximate themore complex features used in (Poesio, 2003).
Thefirst of these features was utterance distance, thedistance between the utterance in which the BR oc-curs and the utterance containing the potential an-tecedent.
(Sentences are used as utterances, as sug-gested by the results of (Poesio et al, 2004).)
Asdiscussed above, studies such as (Poesio, 2003) sug-gested that bridging references were sensitive to dis-tance, in the same way as pronouns (Hobbs, 1978;Clark and Sengul, 1979).
This finding was con-firmed in our study; all anchors of the 58 mereo-logical BDs occurred within the previous five sen-tences, and 47/58 (81%) in the previous two.
(Itis interesting to note that no anchor occurred in thesame sentence as the BD.
)The second salience feature was boolean:whether the potential antecedent had been realizedin first mention position in a sentence (Poesio,2003; Gernsbacher and Hargreaves, 1988; Gordonet al, 1993).
Two forms of this feature were tried:local first mention (whether the entity had been re-alized in first position within the previous five sen-tences) and global first mention (whether it hadbeen realized in first position anywhere).
269 en-tities are realized in first position in the five sen-tences preceding one of the 58 BDs; 298 entities arerealized in first position anywhere in the precedingtext.
For 31/58 of the anchors of mereological BDs,53.5%, local first mention = 1; global first men-tion = 1 for 33/58 of anchors, 56.9%.3.3 Training MethodsConstructing the data set The data set used totrain and test BR resolution consisted of a set ofpositive instances (the actual anchors of the mere-ological BRs) and a set of negative instances (otherentities mentioned in the previous five sentences ofthe text).
However, preliminary tests showed thatsimply including all potential antecedents as nega-tive instances would make the data set too unbal-anced, particularly when only bridging descriptionswere considered: in this case we would have had58 positive instances vs. 1672 negative ones.
Wetherefore developed a parametric script that couldcreate datasets with different positive / negative ra-tios - 1:1, 1:2, 1:3 - by including, with each positiveinstance, a varying number of negative instances (1,2, 3, ...) randomly chosen among the other poten-tial antecedents, the number of negative instances tobe included for each positive one being a parameterchosen by the experimenter.
We report the resultsobtained with 1:1 and 1:3 ratios.The dataset thus constructed was used for bothtraining and testing, by means of a 10-fold cross-validation.Types of Classifiers Used Multi-layer percep-trons (MLPs) have been claimed to work well withsmall datasets; we tested both our own implemen-tation of an MLP with back-propagation in Mat-Lab 6.5, experimenting with different configura-tions, and an off-the-shelf MLP included in the WekaMachine Learning Library8, Weka-NN.
The bestconfiguration for our own MLP proved to be onewith a sigle hidden layer and 10 hidden nodes.
Wealso used the implementation of a Naive Bayes clas-sifier included in the Weka MLL, as Modjeska et al(2003) reported good results.4 Experimental ResultsIn the first series of experiments only mereologicalBridging Descriptions were considered (i.e., onlybridging references realized by the-NPs).
In asecond series of experiments we considered all 153mereological BRs, including ones realized with in-definites.
Finally, we tested a classifier trained onbalanced data (1:1 and 1:3) to find the anchors ofBDs among all possible anchors.4.1 Experiment 1: Mereological descriptionsThe GNOME corpus contains 58 mereological BDs.The five sentences preceding these 58 BDs containa total of 1511 distinct entities for which a headcould be recovered, possibly by examining their an-tecedents.
This means an average of 26 distinct po-tential antecedents per BD, and 5.2 entities per sen-tence.
The simplest baselines for the task of finding8The library is available fromhttp://www.cs.waikato.ac.nz/ml/weka/.the anchor are therefore 4% (by randomly choos-ing one antecedent among those in the previous fivesentences) and 19.2% (by randomly choosing oneantecedent among those in the previous sentenceonly).
As 4.6 entities on average were realized infirst mention position in the five sentences preced-ing a BD (269/58), choosing randomly among thefirst-mentioned entities gives a slighly higher accu-racy of 21.3%.A few further baselines can be established by ex-amining each feature separately.
Google didn?t re-turn any hits for 1089 out of 1511 distinct PAs, andno hit for 24/58 anchors; in 8/58 of cases (13.8%)the entity with the minimum Google distance is thecorrect anchor.
We saw before that the method forcomputing WordNet distance used in (Poesio, 2003)didn?t find a path for any of the mereological BDs;however, not trying to follow mereological linksworked much better, achieving the same accuracyas Google distance (8/58, 13.8%) and finding con-nections for much higher percentages of concepts:no path could be found for only 10/58 of actual an-chors, and for 503/1511 potential antecedents.Pairwise combinations of these features were alsoconsidered.
The best such combination, choosingthe first mentioned entity in the previous sentence,achieves an accuracy of 18/58, 31%.
These baselineresults are summarized in the following table.
No-tice how even the best baselines achieve pretty lowaccuracy, and how even simple ?salience?
measureswork better than lexical distance measures.Baseline AccuracyRandom choice between entities in previous 5 4%Random choice between entities in previous 1 19%Random choice between First Ment.
21.3%entities in previous 5Entity with min Google distance 13.8%Entity with min WordNet distance 13.8%FM entity in previous sentence 31%Min Google distance in previous sentence 17.2%Min WN distance in previous sentence 25.9%FM and Min Google distance 12%FM and Min WN distance 24.1%Table 2: Baselines for the BD taskThe features utterance distance, local first men-tion, and global f.m.
were used in all machine learn-ing experiments.
But since one of our goals was tocompare different lexical resources, only one lexi-cal distance feature was used in the first two experi-ment.The three classifiers were trained to classify a po-tential antecedent as either ?anchor?
or ?not anchor?.The classification results with Google distance andWN distance for all three classifiers and the 1:1 dataset (116 instances in total, 58 real anchor, 58 nega-tive instances), for all elements of the data set, andaveraging across the 10 cross-validations, are shownin Table 3.WN Distance Google Distance(Correct) (Correct)Our own MLP 92(79.3%) 89(76.7%)Weka NN 91(78.4%) 86(74.1%)Weka Naive Bayes 88(75.9%) 85(73.3%)Table 3: Classification results for BDsThese results are clearly better than those ob-tained with any of the baseline methods discussedabove.
The differences between WN distance andGoogle distance, and that between our own MLPand the Weka implementation of Naive Bayes, arealso significant (by a sign test, p ?
.05), whereasthe pairwise differences between our own MLP andWeka?s NN, and between this and the Naive Bayesclassifier, aren?t.
In other words, although we findlittle difference between using WordNet and Googleto compute lexical distance, using WordNet leads toslightly better results for BDs.
The next table showsprecision, recall and f-values for the positive datapoints, for the feature sets using WN distance andGoogle distance, respectively:Precision Recall F-valueWN features 75.4% 84.5% 79.6%Google features 70.6% 86.2% 77.6%Table 4: Precision and recall for positive instancesUsing a 1:3 dataset (3 negative data points foreach anchor), overall accuracy increases (to 82% us-ing Google distance) and accuracy with Google dis-tance is better than with Wordnet distance (80.6%);however, the precision and recall figures for thepositive data points get much worse: 56.7% withGoogle, 55.7% with Wordnet.4.2 All mereological referencesClearly, 58 positive instances is a fairly smalldataset.
In order to have a larger dataset, we in-cluded every bridging reference in the corpus, in-cluding those realized with indefinite NPs, thusbringing the total to 153 positive instances.
We thenran a second series of experiments using the samemethods as before.
The results were slightly lowerthan those for BDs only, but in this case there was nodifference between using Google and using WN.
F-measure on positive instances was 76.3% with WN,75.8% with Google.4.3 A harder testIn a last experiment, we used classifiers trained onbalanced and moderately unbalanced data to deter-mine the anchor of 6 randomly chosen BDs amongWN Distance Google Distance(Correct) (Correct)Weka NN 227(74.2%) 230(75.2%)Table 5: Classification results for all BDsall of their 346 possible antecedents in context.
Forthese experiments, we also tried to use both Googleand WordNet simultaneously.
The results for BDsare shown in Table 6.
The first column of the tablespecifies the lexical resource used; the second thedegree of balance; the next two columns percentagecorrect and F value on a testing set with the samebalance as the training set; the final two columnsperc.
correct and F value on the harder test set.The best results,F=.5, are obtained using bothGoogle and WN distance, and using a larger (if un-balanced) training corpus.
These results are not asgood as those obtained (by hand) by Poesio (which,however, used a complete focus tracking mecha-nism), but the F measure is still 66% higher thanthat obtained with the highest baseline (FM only),and not far off from the results obtained with directanaphoric definite descriptions (e.g., by (Poesio andAlexandrov-Kabadjov, 2004)).
It?s also confortingto note that results with the harder test improve themore data are used, which suggests that better re-sults could be obtained with a larger corpus.5 Related workIn recent years there has been a lot of work todevelop anaphora resolution algorithms using bothsymbolic and statistical methods that could be quan-titatively evaluated (Humphreys et al, 1997; Ng andCardie, 2002) but this work focused on identity rela-tions; bridging references were explicitly excludedfrom the MUC coreference task because of the prob-lems with reliability discussed earlier.
Thus, mostwork on bridging has been theoretical, like the workby Asher and Lascarides (1998).Apart from the work by Poesio et al, the mainother studies attempting quantitative evaluations ofbridging reference resolution are (Markert et al,1996; Markert et al, 2003).
Markert et al (1996)also argue for the need to use both Centering in-formation and conceptual knowledge, and attemptto characterize the ?best?
paths on the basis of ananalysis of part-of relations, but use a hand-coded,domain-dependent knowledge base.
Markert et al(2003) focus on other anaphora, using Hearst?
pat-terns to mine information about hyponymy from theWeb, but do not use focusing knowledge.6 Discussion and ConclusionsThe two main results of this study are, first ofall, that combining ?salience?
features with ?lexi-cal?
features leads to much better results than us-ing either method in isolation; and that these re-sults are an improvement over those previously re-ported in the literature.
A secondary, but still in-teresting, result is that using WordNet in a differentway ?taking advantage of its extensive informationabout hypernyms to obviate its lack of informationabout meronymy?obviates the problems previouslyreported in the literature on using WordNet for re-solving mereological bridging references, leading toresults comparable to those obtained using Google.
(Of course, from a practical perspective Google maystill be preferrable, particularly for languages forwhich no WordNet exists.
)The main limitation of the present work is thatthe number of BDs and BRs considered, while largerthan in our previous studies, is still fairly small.Unfortunately, creating a reasonably accurate goldstandard for this type of semantic interpretation pro-cess is slow work.
Our first priority will be thereforeto extend the data set, including also the originalcases studied by Poesio and Vieira.Current and future work will also include in-corporating the methods tested here in an actualanaphora resolution system, the GUITAR system(Poesio and Alexandrov-Kabadjov, 2004).
We arealso working on methods for automatically recog-nizing bridging descriptions, and dealing with othertypes of (non-associative) bridging references basedon synonymy and hyponymy.AcknowledgmentsThe creation of the GNOME corpus was supportedby the EPSRC project GNOME, GR/L51126/01.ReferencesN.
Asher and A. Lascarides.
1998.
Bridging.
Jour-nal of Semantics, 15(1):83?13.M.
Berland and E. Charniak.
1999.
Finding parts invery large corpora.
In Proc.
of the 37th ACL.H.
H. Clark and C. J. Sengul.
1979.
In search ofreferents for nouns and pronouns.
Memory andCognition, 7(1):35?41.H.
H. Clark.
1977.
Bridging.
In P. N. Johnson-Laird and P.C.
Wason, editors, Thinking: Read-ings in Cognitive Science.
Cambridge.C.
Fellbaum, editor.
1998.
WordNet: An electroniclexical database.
The MIT Press.A.
Garcia-Almanza.
2003.
Using WordNet formereological anaphora resolution.
Master?s the-sis, University of Essex.Lex Res Balance Perc on bal F on bal Perc on Hard F on HardWN 1:1 70.2% .7 80.2% .21:3 75.9% .4 91.7% 0Google 1:1 64.4% .7 63.6% .11.3 79.8% .5 88.4% .3WN + 1:1 66.3% .6 65.3% .2Google 1.3 77.9% .4 92.5% .5Table 6: Results using a classifier trained on balanced data on unbalanced ones.M.
A. Gernsbacher and D. Hargreaves.
1988.
Ac-cessing sentence participants.
Journal of Mem-ory and Language, 27:699?717.P.
C. Gordon, B. J. Grosz, and L. A. Gillion.
1993.Pronouns, names, and the centering of attentionin discourse.
Cognitive Science, 17:311?348.G.
Grefenstette.
1993.
SEXTANT: extracting se-mantics from raw text.
Heuristics.B.
J. Grosz and C. L. Sidner.
1986.
Attention, in-tention, and the structure of discourse.
Computa-tional Linguistics, 12(3):175?204.B.
J. Grosz, A. K. Joshi, and S. Weinstein.1995.
Centering.
Computational Linguistics,21(2):202?225.S.
Harabagiu and D. Moldovan.
1998.
Knowledgeprocessing on extended WordNet.
In (Fellbaum,1998), pages 379?405.M.
A. Hearst.
1998.
Automated discovery of Word-net relations.
In (Fellbaum, 1998).J.
R. Hobbs.
1978.
Resolving pronoun references.Lingua, 44:311?338.K.
Humphreys, R. Gaizauskas, S. Azzam,C.
Huyck, B. Mitchell, and H. Cunning-ham Y. Wilks.
1997.
Description of the LaSIE-IISystem as used for MUC-7.
In Proc.
of the 7thMessage Understanding Conference (MUC-7).T.
Ishikawa.
1998.
Acquisition of associative infor-mation and resolution of bridging descriptions.Master?s thesis, University of Edinburgh.F.
Keller and M. Lapata.
2003.
Using the Web toobtain frequencies for unseen bigrams.
Compu-tational Linguistics, 29(3).K.
Lund, C. Burgess, and R. A. Atchley.
1995.Semantic and associative priming in high-dimensional semantic space.
In Proc.
of the 17thConf.
of the Cogn.
Science Soc., pages 660?665.K.
Markert, M. Strube, and U. Hahn.
1996.Inferential realization constraints on functionalanaphora in the centering model.
In Proc.
of 18thConf.
of the Cog.
Science Soc., pages 609?614.K.
Markert, M. Nissim, and N.. Modjeska.
2003.Using the Web for nominal anaphora resolution.In Proc.
of the EACL Workshop on the Computa-tional Treatment of Anaphora, pages 39?46.N.
Modjeska, K. Markert, and M. Nissim.
2003.Using the Web in ML for anaphora resolution.
InProc.
of EMNLP-03, pages 176?183.V.
Ng and C. Cardie.
2002.
Improving machinelearning approaches to coreference resolution.
InProceedings of the 40th Meeting of the ACL.M.
Poesio and R. Vieira.
1998.
A corpus-based in-vestigation of definite description use.
Computa-tional Linguistics, 24(2):183?216, June.M.
Poesio, R. Vieira, and S. Teufel.
1997.
Resolv-ing bridging references in unrestricted text.
InR.
Mitkov, editor, Proc.
of the ACL Workshop onRobust Anaphora Resolution, pages 1?6, Madrid.M.
Poesio, S. Schulte im Walde, and C. Brew.
1998.Lexical clustering and definite description inter-pretation.
In Proc.
of the AAAI Spring Sympo-sium on Learning for Discourse, pages 82?89.M.
Poesio, T. Ishikawa, S. Schulte im Walde, andR.
Vieira.
2002.
Acquiring lexical knowledge foranaphora resolution.
In Proc.
of the 3rd LREC.M.
Poesio and M. Alexandrov-Kabadjov.
2004.
Ageneral-purpose, off the shelf anaphoric resolver.In Proc.
of the 4th LREC, Lisbon.M.
Poesio, R. Stevenson, B.
Di Eugenio, and J. M.Hitzeman.
2004.
Centering: A parametric theoryand its instantiations.
Comp.
Linguistics.
30(3).M.
Poesio.
2003.
Associative descriptions andsalience.
In Proc.
of the EACL Workshop onComputational Treatments of Anaphora.E.
F. Prince.
1981.
Toward a taxonomy of given-new information.
In P. Cole, editor, RadicalPragmatics, pages 223?256.
Academic Press.C.
L. Sidner.
1979.
Towards a computational the-ory of definite anaphora comprehension in En-glish discourse.
Ph.D. thesis, MIT.O.
Uryupina.
2003.
High-precision identificationof discourse-new and unique noun phrases.
InProc.
of ACL 2003 Stud.
Workshop, pages 80?86.R.
Vieira and M. Poesio.
2000.
An empirically-based system for processing definite descriptions.Computational Linguistics, 26(4), December.
