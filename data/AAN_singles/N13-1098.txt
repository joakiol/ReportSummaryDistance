Proceedings of NAACL-HLT 2013, pages 796?801,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsDifferences in User Responses to a Wizard-of-Oz versus Automated SystemJesse ThomasonUniversity of PittsburghPittsburgh, PA 15260, USAjdt34@pitt.eduDiane LitmanUniversity of PittsburghPittsburgh, PA 15260, USAlitman@cs.pitt.eduAbstractWizard-of-Oz experimental setup in a dia-logue system is commonly used to gather datafor informing an automated version of thatsystem.
Previous work has exposed depen-dencies between user behavior towards sys-tems and user belief about whether the sys-tem is automated or human-controlled.
Thiswork examines whether user behavior changeswhen user belief is held constant and the sys-tem?s operator is varied.
We perform a post-hoc experiment using generalizable prosodicand lexical features of user responses to a dia-logue system backed with and without a hu-man wizard.
Our results suggest that userresponses are different when communicatingwith a wizarded and an automated system, in-dicating that wizard data may be less reliablefor informing automated systems than gener-ally assumed.1 IntroductionIn a Wizard-of-Oz (WOZ) experimental setup, someor all of the automated portions of a dialogue sys-tem are replaced with a hidden, human evaluator.This setup is often used to gather data from userswho believe they are interacting with an automatedsystem (Wolska et al 2004; Andrews et al 2008;Becker et al 2011).
This data can inform a down-stream, real automated system.
A WOZ experimen-tal protocol calls for holding ?all other input and out-put .
.
.
constant so that the only unknown variableis who does the internal processing?
(Paek, 2001).Thus, hiding the human wizard?s input by layers ofsystem interface can render that system believablyautomated.An assumption of this WOZ data-gathering strat-egy is that user behavior will not vary substantiallybetween the WOZ and automated (AUT) experimen-tal setups.
However, it was shown in a dialogue sys-tem that training with a small set of data from an au-tomated system gave rise to better performance thantraining with a large set of data from an analogouswizarded system (Drummond and Litman, 2011).There, it was suggested that differences in systemautomation may be responsible for the performancegap.
It is possible that user responses to these dia-logue systems differed substantially.This paper aims to investigate this possibility bycomparing data between a wizarded and automatedversion of a tutoring dialogue system.
We hypoth-esize that what users say and how they say it willdiffer when the only change is whether the system?sspeech recognition and correctness evaluation com-ponents are wizarded or automated.2 Dialogue SystemThe data for this study is provided by the baselineconditions (one wizarded (WOZ) and one automated(AUT)) of two prior experiments with a spoken tu-torial dialogue system.
Users of this system werestudents recruited at our university, and each wasa native speaker of American English.
Users werenovices and were tutored in basic Newtonian physicsby the system.
Each was engaged by a set of di-alogues that illustrated one or more basic physicsconcepts.
Those dialogues included remedial sub-dialogues that were accessed when the users pro-796Tutor: So what are the forces acting on thepacket after it?s dropped from the plane?Student: um gravity then well air resistance isnegligible just gravityTutor: Fine.
So what?s the direction of the forceof gravity on the packet?Student: vertically downFigure 1: Tutor text is shown on a screen and read aloudvia text-to-speech.
The user responds verbally to the tu-tor?s queries.vided incorrect or off-topic answers.
These priorexperiments were examining the effects of systemadaptation in response to detected student uncer-tainty (Forbes-Riley and Litman(a), 2011; Forbes-Riley and Litman(b), 2011).
However, in this studywe consider only the baseline, non-adaptive condi-tions of those experiments.
Figure 1 shows a sampledialogue excerpt between the student and tutor.In the baseline conditions of the WOZ and AUTsystem past experiments, as shown in Figure 2, thesetups varied only by the system component respon-sible for understanding and evaluating a user?s ver-bal response.
Each student participated in only oneof the two setups, and students were not informedwhen the system was wizarded.
In the WOZ setup ahuman wizard marked student responses to promptsas correct or incorrect.
In the AUT setup, automaticspeech recognition was performed on student re-sponses1, and (in)correctness of answers was deter-mined using natural language understanding modelstrained from the WOZ experiment?s data.3 Post-Hoc ExperimentUsing both lexical and prosodic features, we aimedto determine whether there exist significant differ-ences in users?
turn-level responses to the WOZ andAUT systems.It was suspected that the imperfect accuracy2(87%) of the AUT system?s evaluations of the(in)correctness of user responses may have led toremedial sub-dialogues being accessed by the AUTsystem more often, since false-negatives accounted1The average word-error rate for these AUT responses was19%.2Agreement of ?
= 0.7 between the system and human.Figure 2: The workflow of the tutoring dialogue systemwith the WOZ setup component shown in solid, blue andthe AUT setup component shown in dashed, red.System #Users #Qu #TurnsWOZ 21 111 1542AUT 25 111 2034Table 1: Counts for users, unique questions, and userturns in each data set.for 72% of inaccurate evaluations.
To correct for thisimbalance, rather than comparing user responses toall questions, we compared the features of user re-sponses (turns) to each question individually.
Weomitted questions which were presented in only onesetup3 as well as turns for which a human transcriberfound no user speech.
Table 1 gives the numbers ofusers, number of unique questions asked, and totalnumber of user responses contained in the remain-ing data and used in our investigations.For prosodic features, we considered duration,pitch, and energy (RMS), each extracted usingopenSMILE (Eyben et al 2010).
From pitch andenergy, which contain many samples during a singleturn, we extracted features for maximum, minimum,mean, and standard deviation of these readings.
Wealso considered speech duration and the length ofthe pause before speech began.
This gave us a totalof 10 prosodic features.
To account for any differ-ences in recording environment and users?
voices,we normalized each prosodic feature by dividing itsvalue on each turn by its value in the first turn of thecurrent problem dialogue for that user.
This normal-3There were 3 such questions containing 6 user responses;each question was a remedial sub-dialogue accessed in the AUTbut not WOZ setup.797ization scheme was chosen for our analysis becauseit is used in the live system, though we note that al-ternative methods considering more user responsescould be explored in the future.For lexical features, we used the Linguistic In-quiry and Word Count (LIWC).
LIWC (Pennebakeret al 2001), a word-count dictionary, provides fea-tures representing the percentage of words in an ut-terance falling under particular categories.
Thoughstill a counting strategy, these categories capturehigher-level concepts than would simple unigrams.For example, one category is Tentative(T), whichincludes words such as ?maybe?, ?perhaps?, and?guess?.
Less abstract categories, such as Prepo-sitions(P), with words such as ?to?, ?with?, and?above?, are also generated by LIWC.
Using theseexample categories, the utterance ?Maybe above?would receive feature vector:?0, .
.
.
, 0, T = 50, 0, .
.
.
, 0, P = 50, 0, .
.
.
, 0?
(1)Human transcriptions of users?
speech were madeavailable post-hoc for both system versions.
We ex-tracted 69 LIWC categories as lexical features fromthese human transcriptions of each user turn.Between the WOZ and AUT setups, we lookedfor user response feature differences in two ways.First, a Welch?s two-tailed t-test was used to com-pare the distributions of each feature?s values be-tween WOZ and AUT user responses per question.We noted the features found to be significantly dif-ferent.
Second, we built classification models to dis-tinguish between user responses per question fromthe WOZ and AUT experiments.
For each question,a J484 decision tree model was trained and testedusing 10-fold cross validation via the Weka5 toolkit.Only questions with at least 10 responses betweenboth setups were considered.
Each model was com-pared against majority-class baseline for its respec-tive question by checking for statistically significantdifferences in the model?s accuracy.4We tried Logistic Regression and Support Vector classifiersbut these were consistently outperformed by J48.5http://www.cs.waikato.ac.nz/ml/weka4 Results4.1 Statistical Comparison of FeaturesThe number of questions for which at least onefeature differed statistically significantly was calcu-lated.
Since distinct sets of students were involved inthe WOZ and AUT setups, it is possible that some ofthese differences are inherent between the studentsand not resulting from the presence or absence of ahuman wizard.
To control for this possibility, we as-signed students randomly into two new groups (pre-serving the original class distribution in each newgroup) and tested for feature differences betweenthese new groups.
Table 2 summarizes the differ-ences found by each feature set.
We report onlyquestions for which at least one feature differed be-tween WOZ and AUT but not between these tworandom groups6.
Table 2 also shows the percentageof turns that those questions comprised in the cor-pus.
Prosodic and lexical features each differ for asubstantial portion of the corpus of turns, and whenboth sets are considered about 67% of the corpus iscaptured.Feature set #Qu % Corpus by TurnsProsodic 42 46.22%Lexical 33 35.46%Either 61 66.86%Table 2: Number of questions for which at least one fea-ture from the feature set was found to differ with signif-icance p < 0.05 between WOZ and AUT responses andthe percentage the corpus represented by those questions,weighted by the speech turns they comprise.After controlling for possible between-studentdifferences, all 10 prosodic features and 29 out of69 lexical features differed significantly (p < 0.05)for at least one question.
Table 3 gives the featureswhich were able to differentiate at least 10% of thecorpus by turns.These t-tests show there exist features which dif-fer for a substantial number of questions betweenthe two experimental setups.
Examination of Table6We repeated this random split procedure 10 times andfound, after omitting features found significant in any of the 10splits, that 58.08% of the corpus was still captured.
Less than2% of the turns belonged to questions with at least one featuredifferent through all 10 splits.798Feature % CbT #Qu #W>ADuration 22.15% 19 1RMS Min 16.86% 15 14Dictionary Words 15.13% 13 11pronoun 12.56% 10 10social 11.35% 9 8funct 10.99% 9 9Six Letter Words 10.91% 9 0Table 3: Features shown to differ with significance p <0.05 between WOZ and AUT responses in questionscomprising at least 10% of the corpus by turns (CbT).The numbers of questions these turns comprised and ofquestions with greater (W)OZ than (A)UT mean are alsogiven.Tutor: So how do these two forces?
directionscompare?Top two most common responses:WOZ(9),AUT(2): they are oppositeWOZ(3),AUT(8): oppositeLongest responses per tutor setup:WOZ Student: the relationship between the twoforces?
directions are towards each other sincethe sun is pulling the gravitational force of theearthAUT Student: they are opposite directionsFigure 3: The tutor question and select user responses toa question for which the Dictionary Words feature wasgreater for WOZ responses.3 in addition suggests that users used more wordswith the wizarded system.
For example, the fourthrow shows that all of the questions showing differ-ences for the LIWC category pronoun (the words?they?, ?he?, and ?it?
are popular in this corpus) ex-posed higher percentage of pronouns in the WOZutterances.
The usual dominance of the third row,Dictionary Words, by the WOZ utterances also re-flects this trend.
Figure 3 gives common and charac-teristic student responses for each setup on a ques-tion for which Dictionary Words differed signifi-cantly.
We next applied machine learning to clas-sify the experiment-of-origin of responses based onthese features.Figure 4: The J48 tree for the question ?Would you like todo another problem??.
Classification nodes are marked inblue and red for WOZ and AUT, respectively, and specify(#Instances:#Incorrect).4.2 Response Classification ExperimentsAfter removing questions with less than 10 re-sponses between the two setups, there remained 97questions totaling 2980 turns.
Of the J48 modelsbuilt and tested on each question, 21 of 97 out-performed the majority-class baseline accuracies forthose questions with significance p < 0.05.
These21 questions represented 32.79% of the corpus byturns.
We present in detail the two of these 21 ques-tions with the most turns.The question ?Would you like to do another prob-lem??
represented 6.11% of the corpus by turns andthe J48 model built for it, shown in Figure 4, out-performed the baseline accuracy with p < 0.001.While the Duration feature was the root node, a big-ger decision was made by Word Count ?
1, forwhich most responses were from AUT data.
Thisresult is consistent with literature (Schechtman andHorowitz, 2003; Rose?
and Torrey, 2005) that sug-gests that users interacting with automated systemswill be more curt.The question ?Now let?s find the forces exertedon the car in the vertical direction during the colli-sion.
First, what vertical force is always exerted onan object near the surface of the earth??
represented1.54% of the corpus by turns and the J48 model builtfor it, shown in Figure 5, outperformed the baselineaccuracy with p < 0.01.
Again, Duration emergedas the tree root, but here the biggest decision fell toRMS mean.
Student responses approximately louderthan the initial response to the tutor in this questiondialogue were marked, almost entirely accurately, asAUT.Since both trees were rooted at Duration, we sam-799Figure 5: The J48 tree for the question ?Now let?s find theforces exerted on the car in the vertical direction duringthe collision.
First, what vertical force is always exertedon an object near the surface of the earth??.
Classificationnodes are marked in blue and red for WOZ and AUT,respectively, and specify (#Instances:#Incorrect).pled common responses from each experiment forboth problems.
We noticed that hyper-articulation(speaking slowly, loudly, and enunciating each syl-lable) was more common in the AUT responses.
Forexample, one user answering ?Would you like to doanother problem??
took almost 4 seconds to clearlyand slowly pronounce the word ?yes?.
We suspectthat these hyper-articulations may have contributedto the classifiers?
ability to detect WOZ responsesbased on their brevity.The performance of the per-question J48 modelsshows, for a non-trivial portion of the turns, that theexperiment-of-origin can be classified based on gen-eralizable prosodic and lexical features alone.
Thetwo trees discussed above demonstrate the simplic-ity of the models needed to perform this separation.5 Discussion and Future WorkWe demonstrate that there exist significant differ-ences between user responses to a wizarded and anautomatic dialogue system?s questions, even whenthe contribution of the wizard is as atomic as speechrecognition and correctness evaluation.
Our gen-eralizable features are derived exclusively from therecordings of the users?
responses and human tran-scriptions of their speech.Because the role of the wizard in the WOZ setupwas limited to evaluating users?
spoken response to aprompt, our results suggest that user speech changesas a result of user confidence in the system?s ac-curacy.
For example, Figure 3 demonstrates thatusers in the WOZ setup used complete sentencesand gave long responses, where AUT users, possi-bly anticipating system error, used shorter (some-times one word) responses.
This relationship be-tween user confidence and user speech may be anal-ogous to observed differences like users?
longerspeech and typed responses to systems when toldthose systems are human-operated (Schechtman andHorowitz, 2003; Rose?
and Torrey, 2005).
Our re-sults suggest ways in which raw wizarded data mayfall short of ideal for training an automated system.Having established that differences exist, our fu-ture work will focus on deeper exploration of the na-ture of these differences in users?
responses.
We sus-pect users become less confident in the automatedsystem over time, so one direction of study will be tomeasure how the observed differences change overthe course of the dialogue.
We expect that they areminimal early on and become more pronounced inthe automated setup as users?
confidence is shaken.Additionally, some technical aspects of our method-ology may impact these and future results: using dif-ferent methods of normalization for user speech val-ues than the one from this paper may affect visibilityof observed differences between the setups.Future work may also attempt to address thesedifferences directly.
Intentional wizard error couldbe introduced to frustrate the user into respondingas she would to a less accurate system, analogousto intentional errors produced in user simulation inspoken dialogue systems (Lee and Eskenazi, 2012).This strategy would be further informed by stud-ies of the relationship between the system?s eval-uation accuracy and student responses?
deviationfrom wizarded responses.
Alternatively, post-hocdomain adaptation could be used to adjust the WOZdata.
Generalizable statistical classification domainadaptation (Daume?
and Marcu, 2006) and adapta-tion demonstrated to work well in NLP-specific do-mains (Jiang and Zhai, 2007) both have the potentialto adjust WOZ data to better match that seen by au-tomated systems.AcknowledgmentsThis work is funded by NSF award 0914615.
Wethank Scott Silliman for his support and Pamela Jor-dan, Wenting Xiong, and the anonymous reviewersfor their helpful suggestions and commentary.800ReferencesPierre Andrews, Suresh Manandhar, and Marco De Boni.2008.
Argumentative Human Computer Dialogue forAutomated Persuasion.
Proceedings of the 9th SIG-DIAL Workshop on Discourse and Dialogue, pages138-147, Columbus, June 2008.
Association for Com-putational Linguistics.Lee Becker, Wayne Ward, Sarel van Vuuren, MarthaPalmer.
2011.
DISCUSS: A dialogue move taxonomylayered over semantic representations.
InternationalWorkshop on Computational Semantics (IWCS), MainConference.
Association for Computational Linguis-tics.Hal Daume?
III and Daniel Marcu.
2006.
Domain Adap-tation for Statistical Classifiers.
Journal of ArtificialIntelligence Research, 26:101-126.
AI Access Foun-dation.Joanna Drummond and Diane Litman.
2011.
Examiningthe Impacts of Dialogue Content and System Automa-tion on Affect Models in a Spoken Tutorial DialogueSystem.
Proceedings of the SIGDIAL 2011 Confer-ence, Portland, Oregon, June.
Association for Com-putational Linguistics.Florian Eyben, Martin Wo?llmer, and Bjo?rn Schuller.2010.
Opensmile: The Munich Versatile and FastOpen-Source Audio Feature Extractor.
MM ?10 Pro-ceedings of the International Conference on Multime-dia, 1459-1462.Kate Forbes-Riley and Diane Litman.
2011.
Designingand Evaluating a Wizarded Uncertainty-Adaptive Spo-ken Dialogue Tutoring System.
Computer Speech andLanguage, 25(1): 105-126.Kate Forbes-Riley and Diane Litman.
2011.
Benefitsand Challenges of Real-Time Uncertainty Detectionand Adaptation in a Spoken Dialogue Computer Tutor.Speech Communication 2011, 53(9-10): 1115-1136.Jing Jiang and ChengXiang Zhai.
2007.
InstanceWeighting for Domain Adaptation in NLP.
Proceed-ings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 264-271, Prague,Czech Republic, June 2007.Sungjin Lee and Maxine Eskenazi.
2012.
An Unsu-pervised Approach to User Simulation: toward Self-Improving Dialog Systems.
Proceedings of the 13thAnnual Meeting of the Special Interest Group on Dis-course and Dialogue (SIGDIAL), pages 50-59, Seoul,South Korea, 5-6 July 2012.
Association for Compu-tational Linguistics.Tim Paek.
2001.
Empirical Methods for Evaluating Dia-log Systems.
SIGDIAL ?01 Proceedings of the SecondSIGdial Workshop on Discourse and Dialogue, 16:1-9.James Pennebaker, Martha Francis, and Roger Booth.2001.
Linguistic Inquiry and Word Count (LIWC):LIWC2001.
Lawrence Erlbaum Associates, Mahwah,NJ.Carolyn P. Rose?
and Cristen Torrey.
2005.
Inter-activity and Expectation: Eliciting Learning Ori-ented Behavior with Tutorial Dialogue Systems.Human-Computer Interaction-INTERACT 2005, 323-336.
Springer Berlin/Heidelberg.Nicole Schechtman and Leonard M. Horowitz.
2003.Media Inequality in Conversation: How People Be-have Differently When Interacting with Computersand People.
CHI ?03 Proceedings of the SIGCHI con-ference on Human factors in computing systems, 281-288.Magdalena Wolska, Ivana Kruijff-Korbayova?, HelmutHoracek.
2004.
Lexical-Semantic Interpretation ofLanguage Input in Mathematical Dialogs.
Proceed-ings of the ACL 2nd Workshop on Text Meaning andInterpretation, 81-88.801
