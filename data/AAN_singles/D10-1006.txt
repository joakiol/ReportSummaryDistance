Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 56?65,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsJointly Modeling Aspects and Opinions with a MaxEnt-LDA HybridWayne Xin Zhao?, Jing Jiang?, Hongfei Yan?, Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University, China?School of Information Systems, Singapore Management University, Singapore{zhaoxin,yhf}@net.pku.edu.cn, jingjiang@smu.edu.cn, lxm@pku.edu.cnAbstractDiscovering and summarizing opinions fromonline reviews is an important and challeng-ing task.
A commonly-adopted frameworkgenerates structured review summaries withaspects and opinions.
Recently topic mod-els have been used to identify meaningful re-view aspects, but existing topic models donot identify aspect-specific opinion words.
Inthis paper, we propose a MaxEnt-LDA hy-brid model to jointly discover both aspectsand aspect-specific opinion words.
We showthat with a relatively small amount of train-ing data, our model can effectively identify as-pect and opinion words simultaneously.
Wealso demonstrate the domain adaptability ofour model.1 IntroductionWith the dramatic growth of opinionated user-generated content, consumers often turn to onlineproduct reviews to seek advice while companies seereviews as a valuable source of consumer feedback.How to automatically understand, extract and sum-marize the opinions expressed in online reviews hastherefore become an important research topic andgained much attention in recent years (Pang and Lee,2008).
A wide spectrum of tasks have been studiedunder review mining, ranging from coarse-graineddocument-level polarity classification (Pang et al,2002) to fine-grained extraction of opinion expres-sions and their targets (Wu et al, 2009).
In partic-ular, a general framework of summarizing reviewsof a certain product is to first identify different as-pects (a.k.a.
features) of the given product and thenextract specific opinion expressions for each aspect.For example, aspects of a restaurant may includefood, staff, ambience and price, and opinion expres-sions for staff may include friendly, rude, etc.
Be-cause of the practicality of this structured summaryformat, it has been adopted in several previous stud-ies (Hu and Liu, 2004; Popescu and Etzioni, 2005;Brody and Elhadad, 2010) as well as some commer-cial systems, e.g.
the ?scorecard?
feature at Bingshopping1.Different approaches have been proposed to iden-tify aspect words and phrases from reviews.
Previ-ous methods using frequent itemset mining (Hu andLiu, 2004) or supervised learning (Jin and Ho, 2009;Jin et al, 2009; Wu et al, 2009) have the limitationthat they do not group semantically related aspectexpressions together.
Supervised learning also suf-fers from its heavy dependence on training data.
Incontrast, unsupervised, knowledge-lean topic mod-eling approach has been shown to be effective in au-tomatically identifying aspects and their representa-tive words (Titov and McDonald, 2008; Brody andElhadad, 2010).
For example, words such as waiter,waitress, staff and service are grouped into one as-pect.We follow this promising direction and extend ex-isting topic models to jointly identify both aspectand opinion words, especially aspect-specific opin-ion words.
Current topic models for opinion mining,which we will review in detail in Section 2, still lackthis ability.
But separating aspect and opinion wordscan be very useful.
Aspect-specific opinion wordscan be used to construct a domain-dependent senti-1http://www.bing.com/shopping56ment lexicon and applied to tasks such as sentimentclassification.
They can also provide more informa-tive descriptions of the product or service being re-viewed.
For example, using more specific opinionwords such as cozy and romantic to describe the am-bience aspect in a review summary is more meaning-ful than using generic words such as nice and great.To the best of our knowledge, Brody and Elhadad(2010) are the first to study aspect-specific opinionwords, but their opinion word detection is performedoutside of topic modeling, and they only consideradjectives as possible opinion words.In this paper, we propose a new topic modelingapproach that can automatically separate aspect andopinion words.
A novelty of this model is the inte-gration of a discriminative maximum entropy (Max-Ent) component with the standard generative com-ponent.
The MaxEnt component allows us to lever-age arbitrary features such as POS tags to help sepa-rate aspect and opinion words.
Because the supervi-sion relies mostly on non-lexical features, althoughour model is no longer fully unsupervised, the num-ber of training sentences needed is relatively small.Moreover, training data can also come from a differ-ent domain and yet still remain effective, making ourmodel highly domain adaptive.
Empirical evaluationon large review data sets shows that our model caneffectively identify both aspects and aspect-specificopinion words with a small amount of training data.2 Related WorkPioneered by the work of Hu and Liu (2004), reviewsummarization has been an important research topic.There are usually two major tasks involved, namely,aspect or feature identification and opinion extrac-tion.
Hu and Liu (2004) applied frequent itemsetmining to identify product features without supervi-sion, and considered adjectives collocated with fea-ture words as opinion words.
Jin and Ho (2009),Jin et al (2009) and Wu et al (2009) used super-vised learning that requires hand-labeled trainingsentences to identify both aspects and opinions.
Acommon limitation of these methods is that they donot group semantically related aspect expressions to-gether.
Furthermore, supervised learning usually re-quires a large amount of training data in order to per-form well and is not easily domain adaptable.Topic modeling provides an unsupervised andknowledge-lean approach to opinion mining.
Titovand McDonald (2008) show that global topic modelssuch as LDA (Blei et al, 2003) may not be suitablefor detecting rateable aspects.
They propose multi-grain topic models for discovering local rateable as-pects.
However, they do not explicitly separate as-pect and opinion words.
Lin and He (2009) proposea joint topic-sentiment model, but topic words andsentiment words are still not explicitly separated.Mei et al (2007) propose to separate topic and sen-timent words using a positive sentiment model anda negative sentiment model, but both models cap-ture general opinion words only.
In contrast, wemodel aspect-specific opinion words as well as gen-eral opinion words.Recently Brody and Elhadad (2010) propose todetect aspect-specific opinion words in an unsuper-vised manner.
They take a two-step approach by firstdetecting aspect words using topic models and thenidentifying aspect-specific opinion words using po-larity propagation.
They only consider adjectives asopinion words, which may potentially miss opinionwords with other POS tags.
We try to jointly captureboth aspect and opinion words within topic models,and we allow non-adjective opinion words.Another line of related work is about how to in-corporate useful features into topic models (Zhu andXing, 2010; Mimno and McCallum, 2008).
OurMaxEnt-LDA hybrid bears similarity to these recentmodels but ours is designed for opinion mining.3 Model DescriptionOur model is an extension of LDA (Blei et al, 2003)but captures both aspect words and opinion words.To model the aspect words, we use a modified ver-sion of the multi-grain topic models from (Titov andMcDonald, 2008).
Our model is simpler and yet stillproduces meaningful aspects.
Specifically, we as-sume that there are T aspects in a given collection ofreviews from the same domain, and each review doc-ument contains a mixture of aspects.
We further as-sume that each sentence (instead of each word as instandard LDA) is assigned to a single aspect, whichis often true based on our observation.To understand how we model the opinion words,let us first look at two example review sentences57from the restaurant domain:The food was tasty.The waiter was quite friendly.We can see that there is a strong association oftasty with food and similarly of friendly with waiter.While both tasty and friendly are specific to therestaurant domain, they are each associated withonly a single aspect, namely food and staff, respec-tively.
Besides these aspect-specific opinion words,we also see general opinion words such as greatin the sentence ?The food was great!?
These gen-eral opinion words are shared across aspects, as op-posed to aspect-specific opinion words which areused most commonly with their corresponding as-pects.
We therefore introduce a general opinionmodel and T aspect-specific opinion models to cap-ture these different opinion words.3.1 Generative ProcessWe now describe the generative process of themodel.
First, we draw several multinomial word dis-tributions from a symmetric Dirichlet prior with pa-rameter ?
: a background model ?B, a general aspectmodel ?A,g, a general opinion model ?O,g, T as-pect models {?A,t}Tt=1 and T aspect-specific opin-ion models {?O,t}Tt=1.
All these are multinomialdistributions over the vocabulary, which we assumehas V words.
Then for each review document d, wedraw a topic distribution ?d?Dir(?)
as in standardLDA.
For each sentence s in document d, we drawan aspect assignment zd,s?Multi(?d).Now for each word in sentence s of document d,we have several choices: The word may describe thespecific aspect (e.g.
waiter for the staff aspect), or ageneral aspect (e.g.
restaurant), or an opinion eitherspecific to the aspect (e.g.
friendly) or generic (e.g.great), or a commonly used background word (e.g.know).
To distinguish between these choices, we in-troduce two indicator variable, yd,s,n and ud,s,n, forthe nth word wd,s,n.
We draw yd,s,n from a multi-nomial distribution over {0, 1, 2}, parameterized bypid,s,n.
yd,s,n determines whether wd,s,n is a back-ground word, aspect word or opinion word.
We willdiscuss how to set pid,s,n in Section 3.2.
We drawud,s,n from a Bernoulli distribution over {0, 1} pa-rameterized by p, which in turn is drawn from a sym-metric Beta(?).
ud,s,n determines whether wd,s,n isgeneral or aspect-specific.
We then draw wd,s,n asT??B?A,t?O,t?A,g?O,gDSNd,sxd,s,npid,s,nyd,s,nwd,s,nud,s,nzd,s?d{B,O,A}?
p ?
?Figure 1: The plate notation of our model.follows:wd,s,n ?????????????
?Multi(?B) if yd,s,n = 0Multi(?A,zd,s) if yd,s,n = 1, ud,s,n = 0Multi(?A,g) if yd,s,n = 1, ud,s,n = 1Multi(?O,zd,s) if yd,s,n = 2, ud,s,n = 0Multi(?O,g) if yd,s,n = 2, ud,s,n = 1.Figure 1 shows our model using the plate notation.3.2 Setting pi with a Maximum Entropy ModelA simple way to set pid,s,n is to draw it from asymmetric Dirichlet prior.
However, as suggestedin (Mei et al, 2007; Lin and He, 2009), fully un-supervised topic models are unable to identify opin-ion words well.
An important observation we makeis that aspect words and opinion words usually playdifferent syntactic roles in a sentence.
Aspect wordstend to be nouns while opinion words tend to be ad-jectives.
Their contexts in sentences can also be dif-ferent.
But we do not want to use strict rules to sepa-rate aspect and opinion words because there are alsoexceptions.
E.g.
verbs such as recommend can alsobe opinion words.In order to use information such as POS tagsto help discriminate between aspect and opinionwords, we propose a novel idea as follows: We setpid,s,n using a maximum entropy (MaxEnt) modelapplied to a feature vector xd,s,n associated withwd,s,n.
xd,s,n can encode any arbitrary features wethink may be discriminative, e.g.
previous, currentand next POS tags.
Formally, we havep(yd,s,n = l|xd,s,n) = pid,s,nl =exp (?l ?
xd,s,n)?2l?=0 exp(?l?
?
xd,s,n) ,58where {?l}2l=0 denote the MaxEnt model weightsand can be learned from a set of training sentenceswith labeled background, aspect and opinion words.This MaxEnt-LDA hybrid model is partially in-spired by (Mimno and McCallum, 2008).As for the features included in x, currently weuse two types of simple features: (1) lexical featureswhich include the previous, the current and the nextwords {wi?1, wi, wi+1}, and (2) POS tag featureswhich include the previous, the current and the nextPOS tags {POSi?1, POSi, POSi+1}.3.3 InferenceWe use Gibbs sampling to perform model inference.Due to the space limit, we leave out the derivationdetails and only show the sampling formulas.
Notethat the MaxEnt component is trained first indepen-dently of the Gibbs sampling procedure, that is, inGibbs sampling, we assume that the ?
parametersare fixed.We use w to denote all the words we observe inthe collection, x to denote all the feature vectors forthese words, and y, z and u to denote all the hiddenvariables.
First, given the assignment of all otherhidden variables, to sample a value for zd,s, we usethe following formula:P (zd,s = t|z?
(d,s),y,u,w,x) ?cd(t) + ?cd(?)
+ T??
( ?(cA,t(?)
+ V ?)?(cA,t(?)
+ nA,t(?)
+ V ?)
?V?v=1?
(cA,t(v) + nA,t(v) + ?)?
(cA,t(v) + ?))?
( ?(cO,t(?)
+ V ?)?(cO,t(?)
+ nO,t(?)
+ V ?)
?V?v=1?
(cO,t(v) + nO,t(v) + ?)?
(cO,t(v) + ?
)).Here cd(t) is the number of sentences assigned to as-pect t in document d, and cd(?)
is the number of sen-tences in document d. cA,t(v) is the number of timesword v is assigned as an aspect word to aspect t,and cO,t(v) is the number of times word v is assignedas an opinion word to aspect t.
cA,t(?)
is the total num-ber of times any word is assigned as an aspect wordto aspect t, and cO,t(?)
is the total number of times anyword is assigned as an opinion word to aspect t. Allthese counts represented by a c variable exclude sen-tence s of document d. nA,t(v) is the number of timesword v is assigned as an aspect word to aspect t insentence s of document d, and similarly, nO,t(v) is thenumber of times word v is assigned as an opinionword to aspect t in sentence s of document d.Then, to jointly sample values for yd,s,n andud,s,n, we haveP (yd,s,n = 0|z,y?(d,s,n),u?(d,s,n),w,x)?
exp(?0 ?
xd,s,n)?l?
exp(?l?
?
xd,s,n)?cB(wd,s,n) + ?cB(?)
+ V ?,P (yd,s,n = l, ud,s,n = b|z,y?(d,s,n),u?(d,s,n),w,x)?
exp(?l ?
xd,s,n)?l?
exp(?l?
?
xd,s,n)?
g(wd,s,n, zd,s, l, b),where the function g(v, t, l, b) (1 ?
v ?
V, 1 ?
t ?T, l ?
{1, 2}, b ?
{0, 1}) is defined as follows:g(v, t, l, b) =????????????????????
?cA,t(v) +?cA,t(?)
+V ??
c(0)+?c(?)+2?
if l = 1, b = 0cO,t(v) +?cO,t(?)
+V ??
c(0)+?c(?)+2?
if l = 2, b = 0cA,g(v) +?cA,g(?)
+V ??
c(1)+?c(?)+2?
if l = 1, b = 1cO,g(v) +?cO,g(?)
+V ??
c(1)+?c(?)+2?
if l = 2, b = 1..Here the various c variables denote various countsexcluding the nth word in sentence s of document d.Due to space limit, we do not give full explanationhere.4 Experiment SetupTo evaluate our MaxEnt-LDA hybrid model forjointly modeling aspect and opinion words, we useda restaurant review data set previously used in (Ganuet al, 2009; Brody and Elhadad, 2010) and a ho-tel review data set previously used in (Baccianellaet al, 2009).
We removed stop words and used theStanford POS Tagger2 to tag the two data sets.
Onlyreviews that have no more than 50 sentences wereused.
We also kept another version of the data whichincludes the stop words for the purpose of extractingthe contextual features included in x.
Some detailsof the data sets are given in Table 1.For our hybrid model, we ran 500 iterations ofGibbs sampling.
Following (Griffiths and Steyvers,2004), we fixed the Dirichlet priors as follows: ?
=2http://nlp.stanford.edu/software/tagger.shtml59data set restaurant hotel#tokens 1,644,923 1,097,739#docs 52,574 14,443Table 1: Some statistics of the data sets.data set #sentences #tokensrestaurant 46 634cell phone 125 4414DVD player 180 3024Table 2: Some statistics of the labeled training data.50/T , ?
= 0.1 and ?
= 0.5.
We also experimentedwith other settings of these priors and did not noticeany major difference.
For MaxEnt training, we triedthree labeled data sets: one that was taken from therestaurant data set and manually annotated by us3,and two from the annotated data set used in (Wu etal., 2009).
Note that the latter two were used for test-ing domain adaptation in Section 6.3.
Some detailsof the training sets are shown in Table 2.In our preliminary experiments, we also tried twovariations of our MaxEnt-LDA hybrid model.
(1)The first is a fully unsupervised model where weused a uniform Dirichlet prior for pi.
We foundthat this unsupervised model could not separate as-pect and opinion words well.
(2) The second is abootstrapping version of the MaxEnt-LDA modelwhere we used the predicted values of y as pseudolabels and re-trained the MaxEnt model iteratively.We found that this bootstrapping procedure did notboost the overall performance much and even hurtthe performance a little in some cases.
Due to thespace limit we do not report these experiments here.5 EvaluationIn this section we report the evaluation of ourmodel.
We refer to our MaxEnt-LDA hybrid modelas ME-LDA.
We also implemented a local versionof the standard LDA method where each sentenceis treated as a document.
This is the model usedin (Brody and Elhadad, 2010) to identify aspects,and we refer to this model as LocLDA.Food Staff Order Taking Ambiencechocolate service wait roomdessert food waiter diningcake staff wait tablescream excellent order barice friendly minutes placedesserts attentive seated decorcoffee extremely waitress scenetea waiters reservation spacebread slow asked areacheese outstanding told tableTable 4: Sample aspects of the restaurant domain usingLocLDA.
Note that the words in bold are opinion wordswhich are mixed with aspect words.5.1 Qualitative EvaluationFor each of the two data sets, we show four sampleaspects identified by ME-LDA in Table 3 and Ta-ble 5.
Because the hotel domain is somehow similarto the restaurant domain, we used the labeled train-ing data from the restaurant domain also for the hoteldata set.
From the tables we can see that generallyaspect words are quite coherent and meaningful, andopinion words correspond to aspects very well.
Forcomparison, we also applied LocLDA to the restau-rant data set and present the aspects in Table 4.
Wecan see that ME-LDA and LocLDA give similar as-pect words.
The major difference between these twomodels is that ME-LDA can sperate aspect wordsand opinion words, which can be very useful.
ME-LDA is also able to separate general opinion wordsfrom aspect-specific ones, giving more informativeopinion expressions for each aspect.5.2 Evaluation of Aspects IdentificationWe also quantitatively evaluated the quality of theautomatically identified aspects.
Ganu et al (2009)provide a set of annotated sentences from the restau-rant data set, in which each sentence has been as-signed one or more labels from a gold standard labelset S = {Staff, Food, Ambience, Price, Anecdote,Misc}.
To evaluate the quality of our aspect iden-tification, we chose from the gold standard labelsthree major aspects, namely Staff, Food and Ambi-ence.
We did not choose the other aspects because(1) Price is often mixed with other aspects such asFood, and (2) Anecdote and Misc do not show clear3We randomly selected 46 sentences for manual annotation.60Food Staff Order Taking Ambience GeneralAspect Opinion Aspect Opinion Aspect Opinion Aspect Opinion Opinionchocolate good service friendly table seated room small gooddessert best staff attentive minutes asked dining nice wellcake great food great wait told tables beautiful nicecream delicious wait nice waiter waited bar romantic greatice sweet waiter good reservation waiting place cozy betterdesserts hot place excellent order long decor great smallcoffee amazing waiters helpful time arrived scene open badtea fresh restaurant rude hour rude space warm worthbread tasted waitress extremely manager sat area feel definitelycheese excellent waitstaff slow people finally table comfortable specialTable 3: Sample aspects and opinion words of the restaurant domain using ME-LDA.Service Room Condition Ambience Meal GeneralAspect Opinion Aspect Opinion Aspect Opinion Aspect Opinion Opinionstaff helpful room shower room quiet breakfast good greatdesk friendly bathroom small floor open coffee fresh goodhotel front bed clean hotel small fruit continental niceenglish polite air comfortable noise noisy buffet included wellreception courteous tv hot street nice eggs hot excellenthelp pleasant conditioning large view top pastries cold bestservice asked water nice night lovely cheese nice smallconcierge good rooms safe breakfast hear room great lovelyroom excellent beds double room overlooking tea delicious betterrestaurant rude bath well terrace beautiful cereal adequate fineTable 5: Sample aspects and opinion words of the hotel domain using ME-LDA.patterns in either word usage or writing styles, mak-ing it even hard for humans to identify them.
Brodyand Elhadad (2010) also only used these three as-pects for quantitative evaluation.
To avoid ambigu-ity, we used only the single-labeled sentences forevaluation.
About 83% of the labeled sentences havea single label, which confirms our observation that asentence usually belongs to a single aspect.We first ran ME-LDA and LocLDA each to getan inferred aspect set T .
Following (Brody and El-hadad, 2010), we set the number of aspects to 14in both models.
We then manually mapped each in-ferred aspect to one of the six gold standard aspects,i.e., we created a mapping function f(t) : T ?
S.For sentence s of document d, we first assign it to aninferred aspect as follows:t?
= argmaxt?TNd,s?n=1logP (wd,s,n|t).We then assign the gold standard aspect f(t?)
to thisAspect Method Precision Recall F-1Staff LocLDA 0.804 0.585 0.677ME-LDA 0.779 0.540 0.638Food LocLDA 0.898 0.648 0.753ME-LDA 0.874 0.787 0.828Ambience LocLDA 0.603 0.677 0.638ME-LDA 0.773 0.558 0.648Table 6: Results of aspects identification on restaurant.sentence.
We then calculated the F-1 score of thethree aspects: Staff, Food and Ambience.
The re-sults are shown in Table 6.
Generally ME-LDA hasgiven competitive results compared with LocLDA.For Food and Ambience ME-LDA outperformed Lo-cLDA, while for Staff ME-LDA is a little worsethan LocLDA.
Note that ME-LDA is not designedto compete with LocLDA for aspect identification.615.3 Evaluation of Opinion IdentificationSince the major advantage of ME-LDA is its abil-ity to separate aspect and opinion words, we furtherquantitatively evaluated the quality of the aspect-specific opinion words identified by ME-LDA.Brody and Elhadad (2010) has constructed a goldstandard set of aspect-specific opinion words for therestaurant data set.
In this gold standard set, theymanually judged eight out of the 14 automaticallyinferred aspects they had: J = {Ambiance, Staff,Food-Main Dishes, Atmosphere-Physical, Food-Baked Goods, Food-General, Drinks, Service}.Each word is assigned a polarity score ranging from-2.0 to 2.0 in each aspect.
We used their gold stan-dard words whose polarity scores are not equal tozero.
Because their gold standard only includesadjectives, we also manually added more opinionwords into the gold standard set.
To do so, we tookthe top 20 opinion words returned by our methodand two baseline methods, pooled them together,and manually judged them.
We use precision at n(P@n), a commonly used metric in information re-trieval, for evaluation.
Because top words are moreimportant in opinion models, we set n to 5, 10 and20.
For both ME-LDA and BL-1 below, we againmanually mapped each automatically inferred aspectto one of the gold standard aspects.Since LocLDA does not identify aspect-specificopinion words, we consider the following two base-line methods that can identify aspect-specific opin-ion words:BL-1: In this baseline, we start with all adjectivesas candidate opinion words, and use mutual infor-mation (MI) to rank these candidates.
Specifically,given an aspect t, we rank the candidate words ac-cording to the following scoring function:ScoreBL-1(w, t) =?v?Vtp(w, v) log p(w, v)p(w)p(v) ,where Vt is the set of the top-100 frequent aspectwords from ?A,t.BL-2: In this baseline, we first use LocLDA to learna topic distribution for each sentence.
We then as-sign a sentence to the aspect with the largest proba-bility and hence get sentence clusters.
We manuallymap these clusters to the eight gold standard aspects.Finally, for each aspect we rank adjectives by theirMethod P@5 P@10 P@20ME-LDA 0.825?,?
0.700?
0.569?BL-1 0.400 0.450 0.469BL-2 0.725 0.650 0.563Table 7: Average P@n of aspect-specific opinion wordson restaurant.
* and ?
indicate that the improvement hy-pothesis is accepted at confidence level 0.9 respectivelyfor BL-1 and BL-2.frequencies in the aspect and treat these as aspect-specific opinion words.The basic results in terms of the average precisionat n over the eight aspects are shown in Table 7.
Wecan see that ME-LDA outperformed the two base-lines consistently.
Especially, for P@5, ME-LDAgave more than 100% relative improvement overBL-1.
The absolute value of 0.825 for P@5 alsoindicates that top opinion words discovered by ourmodel are indeed meaningful.5.4 Evaluation of the Association betweenOpinion Words and AspectsThe evaluation in the previous section shows that ourmodel returns good opinion words for each aspect.It does not, however, directly judge how aspect-specific those opinion words are.
This is because thegold standard created by (Brody and Elhadad, 2010)also includes general opinion words.
E.g.
friendlyand good may both be judged to be opinion wordsfor the staff aspect, but the former is more specificthan the latter.
We suspect that BL-2 has comparableperformance with ME-LDA for this reason.
So wefurther evaluated the association between opinionwords and aspects by directly looking at how easyit is to infer the corresponding aspect by only look-ing at an aspect-specific opinion word.
We selectedfour aspects for evaluation: Ambiance, Staff, Food-Main Dishes and Atmosphere-Physical .
We chosethese four aspects because they are quite differentfrom each other and thus manual judgments on thesefour aspects can be more objective.
For each aspect,similar to the pooling strategy in IR, we pooled thetop 20 opinion words identified by BL-1, BL-2 andME-LDA.
We then asked two human assessors toassign an association score to each of these wordsas follows: If the word is closely associated with anaspect, a score of 2 is given; if it is marginally as-62Metrics Dataset BL-2 ME-LDAnDCG@5 Restaurant 0.647 0.764Hotel 0.782 0.820nDCG@10 Restaurant 0.781 0.897Hotel 0.722 0.789Table 8: Average nDCG performance of BL-2 and ME-LDA.
Because only four aspects were used for evaluation,we did not perform statistical significance test.
We foundthat in all cases ME-LDA outperformed BL-2 for eitherall aspects or three out of four aspects.sociated with an aspect, a score of 1 is given; other-wise, 0 is given.
We calculated the Kappa statisticsof agreement, and we got a quite high Kappa valueof 0.8375 and 0.7875 respectively for the restaurantdata set and the hotel data set.
Then for each wordin an aspect, we took the average of the scores ofthe two assessors.
We used an nDCG-like metric tocompare the performance of our model and of BL-2.The metric is defined as follows:nDCG@k(t,M) =?ki=1Score(Mt,i)log2(i+1)iDCG@k(t) ,where Mt,i is the ith aspect-specific opinion wordinferred by method M for aspect t, Score(Mt,i) isthe association score of this word, and iDCG@k(t)is the score of the ideal DCG measure at k for as-pect t, that is, the maximum DCG score assumingan ideal ranking.
We chose k = 5 and k = 10.
Theaverage nDCG over the four aspects are presentedin Table 8.
We can see that ME-LDA outperformedBL-2 quite a lot for the restaurant data set, whichconforms to our hypothesis that ME-LDA generatesaspect-specific opinion words of stronger associa-tion with aspects.
For the hotel data set, ME-LDAoutperformed a little.
This may be due to the factthat we used the restaurant training data for the ho-tel data set.6 Further Analysis of MaxEntIn this section, we perform some further evaluationand analysis of the MaxEnt component in our model.6.1 Feature SelectionPrevious studies have shown that simple POS fea-tures and lexical features can be very effective fordiscovering aspect words and opinion words (HuMethods Average F-1LocLDA 0.690ME-LDA + A 0.631ME-LDA + B 0.695ME-LDA + C 0.705Table 9: Comparison of the average F-1 using differentfeature sets for aspect identification on restaurant.and Liu, 2004; Jin et al, 2009; Wu et al, 2009;Brody and Elhadad, 2010).
for POS features, sincewe observe that aspect words tend to be nouns whileopinion words tend to be adjectives but sometimesalso verbs or other part-of-speeches, we can expectthat POS features should be quite useful.
As for lexi-cal features, words from a sentiment lexicon can alsobe helpful in discovering opinion words.However, lexical features are more diverse so pre-sumably we need more training data in order to de-tect useful lexical features.
Lexical features are alsomore domain-dependent.
On the other hand, we hy-pothesize that POS features are more effective whenthe amount of training data is small and/or the train-ing data comes from a different domain.
We there-fore compare the following three sets of features:?
A: wi?1, wi, wi+1?
B: POSi?1, POSi, POSi+1?
C: A+ BWe show the comparison of the performance in Ta-ble 9 using the average F-1 score defined in Sec-tion 5.2 for aspect identification, and in Table 10 us-ing the average P@n measure defined in Section 5.3for opinion identification.
We can see that Set Bplays the most important part, which conforms toour hypothesis that POS features are very importantin opinion mining.
In addition, we can see that Set Cperforms a bit better than Set B, which indicates thatsome lexical features (e.g., general opinion words)may also be helpful.
Note that here the training datais from the same domain as the test data, and there-fore lexical features are likely to be useful.6.2 Examine the Size of Labeled DataAs we have seen, POS features play the major rolein discriminating between aspect and opinion words.Because there are much fewer POS features thanword features, we expect that we do not need many63Methods P@5 P@10 P@20BL-2 0.725 0.650 0.563ME-LDA + A 0.150 0.200 0.231ME-LDA + B 0.775 0.688 0.569ME-LDA + C 0.825 0.700 0.569Table 10: Comparison of the average P@n using differentfeature sets for opinion identification on restaurant.Method F-1LocalLDA 0.690ME-LDA + 10 0.629ME-LDA + 20 0.692ME-LDA + 30 0.691ME-LDA + 40 0.726ME-LDA + 46 0.705Table 11: Average F-1 with differen sizes of training dataon restaurant.labeled sentences to learn the POS-based patterns.We now examine the sensitivity of the performancewith respect to the amount of labeled data.
We gen-erated four smaller training data sets with 10, 20, 30and 40 sentences each from the whole training dataset we have, which consists of 46 labeled sentences.The results are shown in Table 11 and Table 12.
Wecan see that generally the performance stays aboveBL when the number of training sentences is 20 ormore.
This indicates that our model needs only arelatively small number of high-quality training sen-tences to achieve good results.6.3 Domain AdaptionSince we find that the MaxEnt supervision reliesmore on POS features than lexical features, we alsohypothesize that if the training sentences come froma different domain the performance can still remainrelatively high.
To test this hypothesis, we tried twoMethod P@5 P@10 P@20BL-2 0.725 0.650 0.563ME-LDA + 10 0.700 0.563 0.488ME-LDA + 20 0.875 0.650 0.600ME-LDA + 30 0.825 0.700 0.569ME-LDA + 40 0.825 0.688 0.581ME-LDA + 46 0.825 0.700 0.569Table 12: Average P@n of aspect-specific opinion wordswith differen sizes of training data on restaurant.Method Average F-1restaurant + B 0.695restaurant + C 0.705cell phone + B 0.662cell phone + C 0.629DVD player + B 0.686DVD player + C 0.635Table 13: Average F-1 performance for domain adaptionon restaurant.Method P@5 P@10 P@20restaurant + B 0.775 0.688 0.569restaurant + C 0.825 0.700 0.569cell phone + B 0.775 0.675 0.588cell phone + C 0.750 0.688 0.594DVD player + B 0.775 0.713 0.575DVD player + C 0.825 0.663 0.588Table 14: Average P@n of aspect-specific opinion wordsfor domain adaption on restaurant.quite different training data sets, one from the cellphone domain and the other from the DVD playerdomain, both used in (Wu et al, 2009).We consider two feature sets defined in Sec-tion 6.1 for domain adaption, namely B and C. Theresults are shown in Table 13 and Table 14.For aspect identification, using out-of-domaintraining data performed worse than using in-domaintraining data, but the absolute performance is stilldecent.
And interestingly, we can see that using Bis better than using C, indicating that lexical featuresmay hurt the performance in the cross-domain set-ting.
It suggests that lexical features are not easilyadaptable across domains for aspect identification.For opinion identification, we can see that thereis no clear difference between using out-of-domaintraining data and using in-domain training data,which may indicate that our opinion identificationcomponent is robust in domain adaption.
Also, wecannot easily tell whetherB has advantage over C foropinion identification.
One possible reason may bethat those general opinion words are useful acrossdomains, so lexical features may still be useful fordomain adaption.647 ConclusionsIn this paper, we presented a topic modeling ap-proach that can jointly identify aspect and opinionwords, using a MaxEnt-LDA hybrid.
We showedthat by incorporating a supervised, discriminativemaximum entropy model into an unsupervised, gen-erative topic model, we could leverage syntactic fea-tures to help separate aspect and opinion words.We evaluated our model on two large review datasets from the restaurant and the hotel domains.
Wefound that our model was competitive in identifyingmeaningful aspects compared with previous mod-els.
Most importantly, our model was able to iden-tify meaningful opinion words strongly associatedwith different aspects.
We also demonstrated thatthe model could perform well with a relatively smallamount of training data or with training data from adifferent domain.Our model provides a principled way to jointlymodel both aspects and opinions.
One of the futuredirections we plan to explore is to use this modelto help sentence-level extraction of specific opinionsand their targets, which previously was only tackledin a fully supervised manner.
Another direction is toextend the model to support polarity classification.ACKNOWLEDGMENTThe authors Xin Zhao, Hongfei Yan and Xiaom-ing Li are partially supported by NSFC under thegrant No.
70903008 and 60933004, CNGI grant No.2008-122, 863 Program No.
2009AA01Z143, andthe Open Fund of the State Key Laboratory of Soft-ware Development Environment under Grant No.SKLSDE-2010KF-03, Beihang University.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2009.
Multi-facet rating of product reviews.
InProceedings of the 31st ECIR.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of MachineLearning Research, 3.Samuel Brody and Noemie Elhadad.
2010.
An unsuper-vised aspect-sentiment model for online reviews.
InProceedings of Human Language Technologies: TheAnnual Conference of the North American Chapter ofthe Association for Computational Linguistics.Gayatree Ganu, Noemie Elhadad, and Amelie Marian.2009.
Beyond the stars: Improving rating predictionsusing review text content.
In Proceedings of the 12thInternational Workshop on the Web and Databases.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences of the United States of America.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the 10thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining.Wei Jin and Hung Hay Ho.
2009.
A novel lexicalizedHMM-based learning framework for web opinion min-ing.
In Proceedings of the 26th International Confer-ence on Machine Learning.Wei Jin, Hung Hay Ho, and Rohini K. Srihari.
2009.OpinionMiner: A novel machine learning system forweb opinion mining and extraction.
In Proceedings ofthe 15th ACM SIGKDD.Chenghua Lin and Yulan He.
2009.
Joint senti-ment/topic model for sentiment analysis.
In Proceed-ing of the Eighteenth ACM Conference on Informationand Knowledge Management.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, andChengXiang Zhai.
2007.
Topic sentiment mixture:Modeling facets and opinions in weblogs.
In Proceed-ings of the 16th International Conference on WorldWide Web.David Mimno and Andrew McCallum.
2008.Topic models conditioned on arbitrary features withdirichlet-multinomial regression.
In Conference onUncertainty in Artificial Intelligence.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2).Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification usingmachine learning techniques.
In Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing.Ana-Maria Popescu and Oren Etzioni.
2005.
Extractingproduct features and opinions from reviews.
In Pro-ceedings of the HLT-EMNLP.Ivan Titov and Ryan McDonald.
2008.
Modeling onlinereviews with multi-grain topic models.
In Proceedingof the 17th International Conference on World WideWeb.Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide Wu.2009.
Phrase dependency parsing for opinion mining.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing.Jun Zhu and Eric P. Xing.
2010.
Conditional topic ran-dom fields.
In Proceedings of the 27th InternationalConference on Machine Learning.65
