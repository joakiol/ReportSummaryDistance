Comparing Corpora using Frequency ProfilingPaul RAYSONComputing Deparanent,Lancaster UniversityLancaster, UK,paul@comp.lancs.ac.ukRoger GARSlDEComputing Department,Lancaster UniversityLancaster, UK,rgg@comp.lancs.ac.ukAbstractThis paper describes a method of comparingcorpora which uses frequency profiling.
Themethod can be used to discover key words inthe corpora which differentiate one corpusfrom another.
Using annotated corpora, itcan be applied to discover key grammaticalor word-sense categories.
This can be usedas a quick way in to find the differencesbetween the corpora and is shown to haveapplications in the study of socialdifferentiation in the use of Englishvocabulary, profiling of learner English anddocument analysis in the softwareengineering process.1 Introduct ionCorpus-based techniques have increasingly beenused to compare language usage in recent years.One of the largest early studies was thecomparison of one million words of AmericanEnglish (the Brown corpus) with one millionwords of British English (the LOB corpus) byHofland and Johansson (1982).
A differencecoefficient defined by Yule (1944) showed therelative frequency of a word in the two corpora.A statistical goodness-of-fit test, the Chi-squaredtest, was also used to compare word frequenciesacross the two corpora.
They noted any resultingchi-squared values which indicated that astatistically significant difference at the 5%, 1%,or 0.1% level had been detected between thefrequency of a word in American English and inBritish English.
The null hypothesis of the test isthat there is no difference between the observedfrequencies.More recently, this size of corpus comparisonis becoming the standard even for postgraduatestudies with the increasing availability ofcorpora and reasoning that one million wordsgives sufficient evidence for higher frequencywords.
However, with the production of largecorpora such as the British National Corpus(BNC) containing one hundred million words(Aston & Burnard, 1998), frequencycomparisons are available across millions ofwords of text.
There are two main types ofcorpus comparison:?
comparison of a sample corpus to a large(r)corpus?
comparison of two (roughly-) equal sizedcorporaIn the first type, we refer to the large(r) corpusas a horrnative' corpus since it provides a textnorm (or standard) against which we cancompare.
These two main types of comparisoncan be extended to the comparison of more thantwo corpora.
For example, we may compare onenormative corpus to several smaller corpora atthe same time, or compare three or more equalsized corpora to each other.
In general, however,this makes the results more difficult o interpret.There are also a number of issues which needto be considered when comparing two (or more)corpora:?
representativeness?
homogeneity within the corpora?
comparability ofthe corpora?
reliability of statsfical tests (for different sizedcorpora nd other factors)Representativeness (Biber, 1993) is aparticularly important attribute for a normativecorpus when comparing a sample corpus to alarge normative corpus (such as the BNC) whichcontains ections from many different text typesand domains.
To be representative a corpusshould contain samples of  all major text types(Leech, 1993) and if possible in some wayproportional to their usage in ~very daylanguage' (Clear, 1992).
This first type ofcomparison is intended to discow~r features inthe sample corpus with significantly differentusage (i.e.
frequency) to that found in ~eneral'language.The second type of comparison is one thatviews corpora as equals (as in the Brown andLOB comparison).
It aims to discover features inthe corpora that distinguish one tiom another.Homogeneity within each of the corpora isimportant here since we may find that the resultsreflect sections within one of the corpora whichare unlike other sections in either of the corporaunder consideration (Kilgarriff 1997).Comparability is of interest too, since thecorpora should have been sampled for in thesame way.
In other words, the corpora shouldhave been built using the same stratifiedsampling method and with, if  possible,randornised methods of sample selection.
This isthe case with Brown and LOB, since LOB wasdesigned to be comparable tothe Brown corpus.The final issue, which has been addressedelsewhere, is the one regarding the reliability ofthe statistical tests in relation to the size of thecorpora under consideration.
Kilgarriff (1996)points out that in the Brown versus LOBcomparison many eomrnon words are marked ashaving significant chi-squared values, and thatbecause words are not selected at random inlanguage we will always see a large number ofdifferences in two such text collections.
Heselects the Mann-Whitney test that: uses ranks offrequency data rather than the frequency valuesthemselves tocompute the statistic.
However, heobserves that even with the new test 60% ofwords are marked as significant.
Ignoring theactual frequency of occurrence as in the Mann-Whitney test discards most of the evidence wehave about he distribution of  words.
The test isoften used when comparing ordinal rating scales(Oakes 1998: 17).Dunning (1993) reports that we should not relyon the assumption of a normal distribution whenperforming statistical text analysis and suggeststhat parametric analysis based on the binomial ormultinomial distributions i  a better alternativefor smaller texts.
The chi-squared value becomesunreliable when the expected frequency is lessthan 5 and possibly overestimates with highfrequency words and when comparing arelatively small corpus to a much larger one.
Heproposes the log-likelihood ratio as analternative to Pearson~ chi-squared test.
For thisreason, we chose to use the log-likelihood ratioin our work as described in the next section.
Infact, Cressie and Read (1984) show thatPearson~ X 2 (chi-squared) and the likelihoodratio G 2 (Dunning~ log-likelihood) are twostatistics in a continuum defined by the power-divergence family of statistics.
They go on todescribe this family in later work (1988, 1989)where they also make reference to the long andcontinuing discussion of the normal and chi-squared approximations for X 2 and G 2.We have applied the goodness-of-fit test forcomparison of linguistically annotated corpora.The frequency distributions of part-of-speechand semantic tags are sharply different to words.In these comparisons, we are unlikely to observerare events such as tags occurring once.However, much higher frequencies will occurand so the log-likelihood test is less likely tooverestimate significance in these cases.2 MethodologyThe method is fairly simple and straightforwardto apply.
Given two corpora we wish tocompare, we produce a frequency list for eachcorpus.
Normally, this would be a wordfrequency list, but as described above and aswith examples in the following applicationsection, it can be a part-of-speech (POS) orsemantic tag frequency list.
However, let usassume for now that we are performing acomparison at the word levee For each word inthe two frequency lists we calculate the log-likelihood (henceforth LL) statistic.
This isperformed by constructing a contingency tableas in Table 1.i The application of this technique to POS orsemantic tag frequency lists is achieved byconstructing the contingency table with tag ratherthan word frequencies.Table 1 Contigency table for word frequenciesCORPUS CORPUS TOTALONE TWOFreq a b a+bof wordFreq c-a d-b c+d-a-bof otherwordsTOTAL c d c+dNote that the value ~' corresponds to thenumber of words in corpus one, and ~'corresponds to the number of words in corpustwo (1'4 values).
The values ~' and b'are calledthe observed values (O).
We need to calculatethe expected values (E) according to thefollowing formula:E i =iiIn our case N1 = c, and N2 = d. So, for thisword, E1 = c*(a+b) / (c+d) and E2 = d*(a+b) /(c+d).
The calculation for the expected valuestakes account of the size of the two corpora, sowe do not need to normalise the figures beforeapplying the formula.
We can then calculate thelog-likehood value according to this formula:-21n A = 2~ Oi In~-~This equates to calculating LL as follows:LL = 2*((a*log (a/E1)) + (b*log (b/E2)))The word frequency list is then sorted by theresulting LL values.
This gives the effect ofplacing the largest LL value at the top of the listrepresenting the word which has the mostsignificant relative frequency difference betweenthe two corpora.
In this way, we can see thewords most indicative (or characteristic) of onecorpus, as compared to the other corpus, at thetop of the list.
The words which appear withroughly similar relative frequencies in the twocorpora ppear lower down the list.
Note that wedo not use the hypothesis-test by comparing theLL values to a chi-squared distribution table.
AsKilgarriff & Rose (1998) note, even Pearson~X 2 is suitable without the hypothesis-testinglink: Given the non-random nature of words ina text, we are always likely to find frequenciesof words which differ across any two texts, andthe higher the frequencies, the more informationthe statistical test has to work with.
Hence, it isat this point that the researcher must interveneand qualitatively examine examples of thesignificant words highlighted by this technique.We are not proposing a completely automatedapproach.3 ApplicationsThis method has already been applied to studysocial differentiation in the use of Englishvocabulary and profiling of learner English.
InRayson et al(1997), selective quantitativeanalyses of the demographically sampled spokenEnglish component of the BNC were carried out.This is a subcorpus of circa 4.5 million words, inwhich speakers and respondents are identifiedby such factors as gender, age, social group andgeographical region.
Using the method, acomparison was performed of the vocabulary ofspeakers, highlighting those differences whichare marked by a very high value of significantdifference between different sectors of thecorpus according to gender, age and socialgroup.In Granger and Rayson (1998), two similar-sized corpora of native and non-native writingwere compared at the lexical level.
The corporawere analysed by a part-of-speech tagger, andthis permitted a comparison at the major word-class level.
The patterns of significant overuseand underuse for POS categories demonstratedthat the learner data displayed many of thestylistic features of spoken rather than writtenEnglish.The same technique has more recently beenapplied to compare corpora analysed at thesemantic level in a systems engineering domainand this is the main focus of this section.
Themotivation for this work is that despite naturallanguage's well-documented shortcomings as amedium for precise technical description, its usein software-intensive systems engineeringremains inescapable.
This poses many problemsfor engineers who must derive problemunderstanding and synthesise precise solutiondescriptions from free text.
This is true both forthe largely unstructured textual descriptionsfrom which system requirements are derived,and for more formal documents, such asstandards, which impose requirements on systemdevelopment processes.
We describe anexperiment that has been carried out in theREVERE project (Rayson et al 2000) toinvestigate the use of probabilistic naturallanguage processing techniques to providesystems engineering support.The target documents are field reports of aseries of ethnographic studies at an air trafficconlxol (ATC) centre.
This formed part of astudy of ATC as an example of a system thatsupports collaborative user tasks (Bentley et al1992).
The documents consist of both theverbatim transcripts of the ethnographerbobservations and interviews with controllers,and of reports compiled by the ethnographer forlater analysis by a multi-disciplinary team ofsocial scientists and systems engineers.
The fieldreports form an interesting study because theyexhibit many characteristics typical ofdocuments een by a systems engineer.
Thevolume of the information is fairly high (103pages) and the documents are not structured in away designed to help the extraction ofrequirements ( ay around business processes orsystem architecture).The text is analysed by a part-of-speech tagger,CLAWS (Garside and Smith, 1997), and asemantic analyser (Rayson and Wilson, 1996)which assigns semantic tags that represent thesemantic field (word-sense) of words from alexicon of single words and an idiom list ofmulti-word combinations (e.g.
~ a rule).
Theseresources contain approximately 52,000 wordsand idioms.The normative corpus that we used was a 2.3million-word subset of the BNC derived fromthe transcripts of spoken English.
Using this.corpus, the most over-represented sernanfiecategories in the ATC field reports are shown inTable 2.
The log-likelihood test is applied asdescribed in the previous ection and representsthe semantic tag's frequency deviation from thenormative corpus.
The higher the figure, thegreater the deviation.Table 2.
Over-represented categories in ATCfield reportsLog- Tag Word sense (exampleslikelihood from the text)3366 $7.1 power, organising(bontroller; ~hief)2578 M5 flying (lalane; Hight;t~irport)988 02 general objects (~trip;holder; tack)643 03 electrical equipment(radar; blip)535 Y1 science and technology('PH)449 W3 geographical terms(Pole Hill; Dish Sea)432 Q1.2 paper documents andwriting (~vriting;~,vritten; hotes)372 N3.7 measurement (length;height; l:listance;levels; '1000ft)318 L1 life and living things(live)310 A 10 indicating actions(l~ointing', indicating;tlisplay)306 X4.2 mental objects(~ysterns; tlpproach;haode; tactical;larocedure)290 A4.1 kinds, groups (Sector;Sectors)With the exception of Y I (an anomaly causedby an interviewees initials being mistaken forthe PH unit of acidity), all of these semanticcategories include important objects, roles,functions, etc.
in the ATC domain.
Thefrequency with which some of these occur, suchas M5 (flying), are uusurprising.
Others aremore revealing about the domain of ATC.Figure 1 shows some of the occurrences of thesemantic category 02 (general objects).
Theimportant information extracted here is theimportance of Mrips' (formally, 1light strips).These are small pieces of cardboard with printedflight details that are the most fundamentalartefact used by the air traffic controllers tomanage their air space.
Examination of otherwords in this category also shows that flight4!i tO mqt"ll~ " 1250L' i  n red m a , t r ip'he :T..sle o f  I lm .
.
.
&:lU0t; Tht ,  ~t r lp~te?
l  I~, 'the ~ pr in ted  tn  box~on prtn, t~ l  tn hot ' 6 ' o f  the strip=rr ' tw l  t i l e  over th=tbeo~n ( boxiviousllJ only aA~'ozla,~te- :some .s'lwips~l  t tne  neor the call$tfln on a ~trtpmuch I~msier .
lhermere  1.6 ~t r i~!
re tor t  1.6 s t r ips  in one oF h i ,  meltsi*Y , .thot ta lk ing  aml us ing on input~hat t~llctr~l md u~inl; an t r lmt  device: /Arawtt: the nicl~ l~hina ~,~:  ~"?rins, d '~ i l t t  o t  = time t r l s tm~ tomrds  ' ~l'le b0r t~ Of ~e~ :?
II " o f  the s t r ip  ( ~ te l( ~ le f t  } S t r lpssee~d br' A " ) ' th is  ~ (:l~'tcusl~ on ly~mett out of, pos i t ion  , and 2\[ gott;o tndtcote =nur.~,~L ~meed .
<?
oF ht= ~lm .
.dBb.
A ;dev ice  s ight  =t~o be ?
but thatretort  a im be , but  ~ the pri=t the i r  F lex ib i l i ty  .
,~uot :  oFigure 1.
Browsing the semantic ategory 02strips are held in tacks' to organise themaccording to (for example) aircraft time-of-arrival.Similarly, browsing the context for Q1.2(paper documents and writing) would allow usto discover that controllers annotate flight strips'to record deviations from flight plans, and L1(life, living things) would reveal that some stripsare live; that is, they refer to aircraft currentlytraversing the contxoller's sector.
Notice also thatthe semantic categories' deviation from thenormative corpus can also be expected to revealdomain roles (actors).
In this example, thefrequency of $7.1 (power, organising) shows theimportance of the roles of ~ontrollers' and~hiefs'.Using the frequency profiling method does notautomate the task of identifying abstractions,much less does it produce fully formedrequirements that can be pasted into aspecification document.
Instead, it helps theengineer quickly isolate potentially significantdomain abstractions that require closer analysis.4 Conclusionsreliability of the statistical tests (LL, Pearson~X 2 and others) under the effects of corpus size,ratio of the corpora being compared and word(or tag) frequency.We do not propose a completely automatedapproach.
The tools suggest a group of key itemsby decreasing order of significance whichdistinguish one corpus from another.
It is thenthat the researcher should investigateoccurrences of the significant items in thecorpora using standard corpus techniques uchas KWIC (key-word in context).
The reasonsbehind their significance can be discovered andexplanations ought for the patterns displayed.By this process, we can compare the corporaunder investigation and make hypotheses aboutthe language use that they represent.AcknowledgementsOur thanks go to Geoffrey Leech and theanonymous reviewers who commented onearlier versions of this paper.
The REVEREproject is supported under the EPSRC SystemsEngineering for Business Process Change(SEBPC) programme, project numberGR/MO4846.This paper has described a method of comparingcorpora which uses frequency profiling.
Themethod has been shown to discover key items inthe corpora which differentiate one corpus fromanother.
It has been applied at the word level,part-of-speech tag level, and semantic tag level.It can be used as a quick way in to find thedifferences between the corpora and is shown tohave applications in the study of socialdifferentiation i  the use of English vocabulary:profiling of learner English and documentanalysis in the software ngineering process.Future directions in which we aim to researchinclude a more precise specification of theReferencesAston, G. and Burnard, L. (1998).
The BNCHandbook: Exploring the British National Corpuswith SARA, Edinburgh University Press.Bentley IL, Rodden T., Sawyer P., Sommerville I,Hughes J., Randall D., Shapiro D. (1992).Ethnographically-informed systems design for airtraffic control, In Proceedings of Computer-Supported Cooperative Work (CSCW) '92,Toronto, November 1992.Biber, D. (1993).
Representativeness in CorpusDesign.
Literary and Linguistic Computing, 8,Issue 4, Oxford University Press, pp.
243-257.Clear, J.
(1992).
Corpus sampling.
In G. Leitner (ed.
)New directions in English lang~aage corpora~Mouton-de-Gruyter, Berlin, pp.
21 - 31.Cressie, N. and Read, T. R. C. (1984) MultinomialGoodness-of-Fit Tests.
Journal of the RoyalStatistical Society.
Series B (Methodological), Vol.46, No.
3, pp.
440 - 464.Cressie, N. and Read, T. R. C. (1989).
Pearson~ X 2and the Loglikelihood Ratio Statistic G2: Acomparative review.
International StatisticalReview, 57, 1, Belfast University Press, N.I., pp.19--43.Dunning, T. (1993).
Accurate Methods for theStatistics of Surprise and Coincidence.Computational Linguistics, 19, 1, March 1993, pp.61-74.Garside, R. and Smith, N. (199"7).
A HybridGrammatical Tagger: CLAtVS4, in Garside, R.,Leech, G., and McEnery, A.
(eds.)
CorpusAnnotation: Linguistic Information from ComputerText Corpora, Longman, London.Granger, S. and Rayson, P. (1998).
Automaticprofiling of learner texts.
In S. Granger (ed.
)Learner English on Computer.
Longman, Londonand New York, pp.
119-131.Hofland, K. and Johansson, S. (1982).
Wordfrequencies in British and American English.
TheNorwegian Computing Centre for the Humanities,Bergen, Norway.Kilgarriff, A.
(1996) Why chi-square doesn't work;and an improved LOB-Brown comparison.
ALLC-ACH Conference, June 1996, Bergen, Norway.Kilgarriff, A.
(1997).
Using word frequency lists tomeasure corpus homogeneity and similaritybetween corpora.
Proceedings 5th ACL workshopon very large corpora.
Beijing and Hong Kong.Kilgarriff, A. and Rose, T. (1998).
Measures forcorpus similarity and homogeneity.
In proceedingsof the 3 m conference on Empirical Methods inNatural Language Processing, Granada, Spain, pp.46 - 52.Leech, G. (1993).
100 million words of English: adescription of the background, nature andprospects of the British National Corpus project.English Today 33, Vol.
9, No.
1, CambridgeUniversity Press.Oakes, M. P. (1998).
Statistics for CorpusLinguistics.
Edinburgh University Press,Edinburgh.Rayson, P., and Wilson, A.
(1996).
The ACAMRITsemantic tagging system: progress report, In L. J.Evett, and T. G. Rose (eds.)
Language Engineeringfor Document Analysis and Recognition, LEDAR,AISB96 Workshop proceedings, pp 13-20.Brighton, England.Rayson, P., Leech, G., and Hodges, M. (1997).
Socialdifferentiation in the use of English vocabulary:some analyses of the conversational component ofthe British National Corpus.
International Journalof Corpus Linguistics.
2 (1).
pp.
133 - 152.
JohnBenjamins, Amsterdam/Philadelphia.Rayson, P., Garside, R., and Sawyer, P. (2000).Assisting requirements engineering with semanticdocument analysis.
In Proceedings of RIAO 2000(Recherche d'Inforrnafions Assisfie par Ordinateur,Computer-Assisted Information Retrieval)International Conference, Coll~ge de France, Paris,France, April 12-14, 2000.
C.I.D., Paris, pp.
1363 -1371.Read, T. R. C. and Cressie, N. A. C. (1988).Goodness-of-fit s atistics for discrete multivariatedata.
Springer series in statistics.
Springer-Vedag,New York.Yule, G. (1944).
The Statistical Study of LiteraryVocabulary.
Cambridge University Press.
