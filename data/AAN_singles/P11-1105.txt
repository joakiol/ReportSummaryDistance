Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1045?1054,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Joint Sequence Translation Model with Integrated ReorderingNadir Durrani Helmut Schmid Alexander FraserInstitute for Natural Language ProcessingUniversity of Stuttgart{durrani,schmid,fraser}@ims.uni-stuttgart.deAbstractWe present a novel machine translation modelwhich models translation by a linear sequenceof operations.
In contrast to the ?N-gram?model, this sequence includes not only trans-lation but also reordering operations.
Keyideas of our model are (i) a new reorderingapproach which better restricts the position towhich a word or phrase can be moved, andis able to handle short and long distance re-orderings in a unified way, and (ii) a jointsequence model for the translation and re-ordering probabilities which is more flexi-ble than standard phrase-based MT.
We ob-serve statistically significant improvements inBLEU over Moses for German-to-English andSpanish-to-English tasks, and comparable re-sults for a French-to-English task.1 IntroductionWe present a novel generative model that explainsthe translation process as a linear sequence of oper-ations which generate a source and target sentencein parallel.
Possible operations are (i) generation ofa sequence of source and target words (ii) insertionof gaps as explicit target positions for reordering op-erations, and (iii) forward and backward jump oper-ations which do the actual reordering.
The probabil-ity of a sequence of operations is defined accordingto an N-gram model, i.e., the probability of an op-eration depends on the n ?
1 preceding operations.Since the translation (generation) and reordering op-erations are coupled in a single generative story,the reordering decisions may depend on precedingtranslation decisions and translation decisions maydepend on preceding reordering decisions.
This pro-vides a natural reordering mechanism which is ableto deal with local and long-distance reorderings in aconsistent way.
Our approach can be viewed as anextension of the N-gram SMT approach (Marin?o etal., 2006) but our model does reordering as an inte-gral part of a generative model.The paper is organized as follows.
Section 2 dis-cusses the relation of our work to phrase-based andthe N-gram SMT.
Section 3 describes our genera-tive story.
Section 4 defines the probability model,which is first presented as a generative model, andthen shifted to a discriminative framework.
Section5 provides details on the search strategy.
Section 6explains the training process.
Section 7 describesthe experimental setup and results.
Section 8 givesa few examples illustrating different aspects of ourmodel and Section 9 concludes the paper.2 Motivation and Previous Work2.1 Relation of our work to PBSMTPhrase-based SMT provides a powerful translationmechanism which learns local reorderings, transla-tion of short idioms, and the insertion and deletion ofwords sensitive to local context.
However, PBSMTalso has some drawbacks.
(i) Dependencies acrossphrases are not directly represented in the translationmodel.
(ii) Discontinuous phrases cannot be used.
(iii) The presence of many different equivalent seg-mentations increases the search space.Phrase-based SMT models dependencies betweenwords and their translations inside of a phrase well.However, dependencies across phrase boundariesare largely ignored due to the strong phrasal inde-1045German Englishhat er ein buch gelesen he read a bookhat eine pizza gegessen has eaten a pizzaer hehat hasein aeine amenge lot ofbutterkekse butter cookiesgegessen eatenbuch bookzeitung newspaperdann thenTable 1: Sample Phrase Tablependence assumption.
A phrase-based system us-ing the phrase table1 shown in Table 1, for exam-ple, correctly translates the German sentence ?erhat eine pizza gegessen?
to ?he has eaten a pizza?,but fails while translating ?er hat eine menge but-terkekse gegessen?
(see Table 1 for a gloss) whichis translated as ?he has a lot of butter cookies eaten?unless the language model provides strong enoughevidence for a different ordering.
The generation ofthis sentence in our model starts with generating ?er?
he?, ?hat ?
has?.
Then a gap is inserted on the Ger-man side, followed by the generation of ?gegessen ?eaten?.
At this point, the (partial) German and En-glish sentences look as follows:er hat gegessenhe has eatenWe jump back to the gap on the German sideand fill it by generating ?eine ?
a?
and ?pizza ?pizza?, for the first example and generating ?eine ?a?, ?menge ?
lot of?, ?butterkekse ?
butter cookies?for the second example, thus handling both shortand long distance reordering in a unified manner.Learning the pattern ?hat gegessen ?
has eaten?helps us to generalize to the second example withunseen context.
Notice how the reordering deci-sion is triggered by the translation decision in ourmodel.
The probability of a gap insertion operationafter the generation of the auxiliaries ?hat ?
has?
willbe high because reordering is necessary in order tomove the second part of the German verb complex(?gegessen?)
to its correct position at the end of theclause.
This mechanism better restricts reordering1The examples given in this section are not taken from thereal data/system, but made-up for the sake of argument.Figure 1: (a) Known Context (b) Unknown Contextthan traditional PBSMT and is able to deal with localand long-distance reorderings in a consistent way.Another weakness of the traditional phrase-basedsystem is that it can only capitalize on continuousphrases.
Given the phrase inventory in Table 1,phrasal MT is able to generate example in Figure1(a).
The information ?hat...gelesen ?
read?
is inter-nal to the phrase pair ?hat er ein buch gelesen ?
heread a book?, and is therefore handled conveniently.On the other hand, the phrase table does not havethe entry ?hat er eine zeitung gelesen ?
he read anewspaper?
(Figure 1(b)).
Hence, there is no optionbut to translate ?hat...gelesen?
separately, translat-ing ?hat?
to ?has?
which is a common translation for?hat?
but wrong in the given context.
Context-freehierarchical models (Chiang, 2007; Melamed, 2004)have rules like ?hat er X gelesen ?
he read X?
to han-dle such cases.
Galley and Manning (2010) recentlysolved this problem for phrasal MT by extractingphrase pairs with source and target-side gaps.
Ourmodel can also use tuples with source-side discon-tinuities.
The above sentence would be generatedby the following sequence of operations: (i) gener-ate ?dann ?
then?
(ii) insert a gap (iii) generate ?er?
he?
(iv) backward jump to the gap (v) generate?hat...[gelesen] ?
read?
(only ?hat?
and ?read?
areadded to the sentences yet) (vi) jump forward to theright-most source word so far generated (vii) inserta gap (viii) continue the source cept (?gelesen?
is in-serted now) (ix) backward jump to the gap (x) gen-erate ?ein ?
a?
(xi) generate ?buch ?
book?.Figure 2: PatternFrom this operation se-quence, the model learns apattern (Figure 2) which al-lows it to generalize to theexample in Figure 1(b).
The open gap representedby serves a similar purpose as the non-terminalcategories in a hierarchical phrase-based systemsuch as Hiero.
Thus it generalizes to translate ?einezeitung?
in exactly the same way as ?ein buch?.1046Another problem of phrasal MT is spuriousphrasal segmentation.
Given a sentence pair anda corresponding word alignment, phrasal MT canlearn an arbitrary number of source segmentations.This is problematic during decoding because differ-ent compositions of the same minimal phrasal unitsare allowed to compete with each other.2.2 Relation of our work to N-gram SMTN-gram based SMT is an alternative to hierarchi-cal and non-hierarchical phrase-based systems.
Themain difference between phrase-based and N-gramSMT is the extraction procedure of translation unitsand the statistical modeling of translation context(Crego et al, 2005a).
The tuples used in N-gramsystems are much smaller translation units thanphrases and are extracted in such a way that a uniquesegmentation of each bilingual sentence pair is pro-duced.
This helps N-gram systems to avoid thespurious phrasal segmentation problem.
Reorder-ing works by linearization of the source side and tu-ple unfolding (Crego et al, 2005b).
The decoderuses word lattices which are built with linguisticallymotivated re-write rules.
This mechanism is furtherenhanced with an N-gram model of bilingual unitsbuilt using POS tags (Crego and Yvon, 2010).
Adrawback of their reordering approach is that searchis only performed on a small number of reorderingsthat are pre-calculated on the source side indepen-dently of the target side.
Often, the evidence forthe correct ordering is provided by the target-sidelanguage model (LM).
In the N-gram approach, theLM only plays a role in selecting between the pre-calculated orderings.Our model is based on the N-gram SMT model,but differs from previous N-gram systems in someimportant aspects.
It uses operation n-grams ratherthan tuple n-grams.
The reordering approach is en-tirely different and considers all possible orderingsinstead of a small set of pre-calculated orderings.The standard N-gram model heavily relies on POStags for reordering and is unable to use lexical trig-gers whereas our model exclusively uses lexical trig-gers and no POS information.
Linearization and un-folding of the source sentence according to the targetsentence enables N-gram systems to handle source-side gaps.
We deal with this phenomenon more di-rectly by means of tuples with source-side discon-tinuities.
The most notable feature of our work isthat it has a complete generative story of transla-tion which combines translation and reordering op-erations into a single operation sequence model.Like the N-gram model2, our model cannot dealwith target-side discontinuities.
These are elimi-nated from the training data by a post-editing pro-cess on the alignments (see Section 6).
Galley andManning (2010) found that target-side gaps were notuseful in their system and not useful in the hierarchi-cal phrase-based system Joshua (Li et al, 2009).3 Generative StoryOur generative story is motivated by the complex re-orderings in the German-to-English translation task.The German and English sentences are jointly gen-erated through a sequence of operations.
The En-glish words are generated in linear order3 whilethe German words are generated in parallel withtheir English translations.
Occasionally the trans-lator jumps back on the German side to insert somematerial at an earlier position.
After this is done, itjumps forward again and continues the translation.The backward jumps always end at designated land-ing sites (gaps) which were explicitly inserted be-fore.
We use 4 translation and 3 reordering opera-tions.
Each is briefly discussed below.Generate (X,Y): X and Y are German and Englishcepts4 respectively, each with one or more words.Words in X (German) may be consecutive or discon-tinuous, but the words in Y (English) must be con-secutive.
This operation causes the words in Y andthe first word in X to be added to the English andGerman strings respectively, that were generated sofar.
Subsequent words in X are added to a queue tobe generated later.
All the English words in Y aregenerated immediately because English is generatedin linear order.
The generation of the second (andsubsequent) German word in a multi-word cept canbe delayed by gaps, jumps and the Generate SourceOnly operation defined below.Continue Source Cept: The German words added2However, Crego and Yvon (2009), in their N-gram system,use split rules to handle target-side gaps and show a slight im-provement on a Chinese-English translation task.3Generating the English words in order is also what the de-coder does when translating from German to English.4A cept is a group of words in one language translated as aminimal unit in one specific context (Brown et al, 1993).1047to the queue by the Generate (X,Y) operation aregenerated by the Continue Source Cept operation.Each Continue Source Cept operation removes oneGerman word from the queue and copies it to theGerman string.
If X contains more than one Germanword, say n many, then it requires n translation op-erations, an initial Generate (X1...Xn, Y ) operationand n ?
1 Continue Source Cept operations.
Forexample ?hat...gelesen ?
read?
is generated by theoperation Generate (hat gelesen, read), which adds?hat?
and ?read?
to the German and English stringsand ?gelesen?
to a queue.
A Continue Source Ceptoperation later removes ?gelesen?
from the queueand adds it to the German string.Generate Source Only (X): The string X is addedat the current position in the German string.
This op-eration is used to generate a German word X with nocorresponding English word.
It is performed imme-diately after its preceding German word is covered.This is because there is no evidence on the English-side which indicates when to generate X. GenerateSource Only (X) helps us learn a source word dele-tion model.
It is used during decoding, where a Ger-man word (X) is either translated to some Englishword(s) by a Generate (X,Y) operation or deletedwith a Generate Source Only (X) operation.Generate Identical: The same word is added atthe current position in both the German and En-glish strings.
The Generate Identical operation isused during decoding for the translation of unknownwords.
The probability of this operation is estimatedfrom singleton German words that are translated toan identical string.
For example, for a tuple ?Port-land ?
Portland?, where German ?Portland?
was ob-served exactly once during training, we use a Gen-erate Identical operation rather than Generate (Port-land, Portland).We now discuss the set of reordering operationsused by the generative story.
Reordering has to beperformed whenever the German word to be gen-erated next does not immediately follow the previ-ously generated German word.
During the genera-tion process, the translator maintains an index whichspecifies the position after the previously coveredGerman word (j), an index (Z) which specifies theindex after the right-most German word covered sofar, and an index of the next German word to be cov-ered (j?).
The set of reordering operations used inTable 2: Step-wise Generation of Example 1(a).
The ar-row indicates position j.generation depends upon these indexes.Insert Gap: This operation inserts a gap which actsas a place-holder for the skipped words.
There canbe more than one open gap at a time.Jump Back (W): This operation lets the translatorjump back to an open gap.
It takes a parameter Wspecifying which gap to jump to.
Jump Back (1)jumps to the closest gap to Z, Jump Back (2) jumpsto the second closest gap to Z, etc.
After the back-ward jump the target gap is closed.Jump Forward: This operation makes the transla-tor jump to Z.
It is performed if some already gen-erated German word is between the previously gen-erated word and the word to be generated next.
AJump Back (W) operation is only allowed at positionZ.
Therefore, if j 6= Z, a Jump Forward operationhas to be performed prior to a Jump Back operation.Table 2 shows step by step the generation of aGerman/English sentence pair, the correspondingtranslation operations, and the respective values ofthe index variables.
A formal algorithm for convert-ing a word-aligned bilingual corpus into an opera-tion sequence is presented in Algorithm 1.4 ModelOur translation model p(F,E) is based on opera-tion N-gram model which integrates translation andreordering operations.
Given a source string F , asequence of tuples T = (t1, .
.
.
, tn) as hypothe-sized by the decoder to generate a target string E,the translation model estimates the probability of a1048Algorithm 1 Corpus Conversion Algorithmi Position of current English ceptj Position of current German wordj?
Position of next German wordN Total number of English ceptsfj German word at position jEi English cept at position iFi Sequence of German words linked to EiLi Number of German words linked with Eik Number of already generated German words for Eiaik Position of kth German translation of EiZ Position after right-most generated German wordS Position of the first word of a target gapi := 0; j := 0; k := 0while fj is an unaligned word doGenerate Source Only (fj)j := j + 1Z := jwhile i < N doj?
:= aikif j < j?
thenif fj was not generated yet thenInsert Gapif j = Z thenj := j?elseJump Forwardif j?
< j thenif j < Z and fj was not generated yet thenInsert GapW := relative position of target gapJump Back (W)j := Sif j < j?
thenInsert Gapj := j?if k = 0 thenGenerate (Fi, Ei) {or Generate Identical}elseContinue Source Ceptj := j + 1; k := k + 1while fj is an unaligned word doGenerate Source Only (fj)j := j + 1if Z < j thenZ := jif k = Li theni := i + 1; k := 0Remarks:We use cept positions for English (not word positions) becauseEnglish cepts are composed of consecutive words.
German po-sitions are word-based.The relative position of the target gap is 1 if it is closest to Z, 2if it is the second closest gap etc.The operation Generate Identical is chosen if Fi = Ei and theoverall frequency of the German cept Fi is 1.generated operation sequence O = (o1, .
.
.
, oJ) as:p(F,E) ?J?j=1p(oj |oj?m+1...oj?1)where m indicates the amount of context used.
Ourtranslation model is implemented as an N-grammodel of operations using SRILM-Toolkit (Stolcke,2002) with Kneser-Ney smoothing.
We use a 9-grammodel (m = 8).Integrating the language model the search is de-fined as:E?
= argmaxEpLM (E)p(F,E)where pLM (E) is the monolingual language modeland p(F,E) is the translation model.
But our trans-lation model is a joint probability model, because ofwhich E is generated twice in the numerator.
Weadd a factor, prior probability ppr(E), in the denom-inator, to negate this effect.
It is used to marginalizethe joint-probability model p(F,E).
The search isthen redefined as:E?
= argmaxEpLM (E)p(F,E)ppr(E)Both, the monolingual language and the priorprobability model are implemented as standardword-based n-gram models:px(E) ?J?j=1p(wj |wj?m+1, .
.
.
, wj?1)where m = 4 (5-gram model) for the standardmonolingual model (x = LM ) and m = 8 (sameas the operation model5) for the prior probabilitymodel (x = pr).In order to improve end-to-end accuracy, we in-troduce new features for our model and shift fromthe generative6 model to the standard log-linear ap-proach (Och and Ney, 2004) to tune7 them.
Wesearch for a target stringE which maximizes a linearcombination of feature functions:5In decoding, the amount of context used for the prior prob-ability is synchronized with the position of back-off in the op-eration model.6Our generative model is about 3 BLEU points worse thanthe best discriminative results.7We tune the operation, monolingual and prior probabilitymodels as separate features.
We expect the prior probabilitymodel to get a negative weight but we do not force MERT toassign a negative weight to this feature.1049E?
= argmaxE???J?j=1?jhj(F,E)??
?where ?j is the weight associated with the featurehj(F,E).
Other than the 3 features discussed above(log probabilities of the operation model, monolin-gual language model and prior probability model),we train 8 additional features discussed below:Length Bonus The length bonus feature counts thelength of the target sentence in words.Deletion Penalty Another feature for avoiding tooshort translations is the deletion penalty.
Deleting asource word (Generate Source Only (X)) is a com-mon operation in the generative story.
Because thereis no corresponding target-side word, the monolin-gual language model score tends to favor this op-eration.
The deletion penalty counts the number ofdeleted source words.Gap Bonus and Open Gap Penalty These featuresare introduced to guide the reordering decisions.
Weobserve a large amount of reordering in the automat-ically word aligned training text.
However, givenonly the source sentence (and little world knowl-edge), it is not realistic to try to model the reasonsfor all of this reordering.
Therefore we can use amore robust model that reorders less than humans.The gap bonus feature sums to the total number ofgaps inserted to produce a target sentence.
The opengap penalty feature is a penalty (paid once for eachtranslation operation performed) whose value is thenumber of open gaps.
This penalty controls howquickly gaps are closed.Distortion and Gap Distance Penalty We havetwo additional features to control the reordering de-cisions.
One of them is similar8 to the distance-based reordering model used by phrasal MT.
Theother feature is the gap distance penalty which calcu-lates the distance between the first word of a sourceceptX and the start of the left-most gap.
This cost ispaid once for each Generate, Generate Identical andGenerate Source Only.
For a source cept coverd byindexes X1, .
.
.
, Xn, we get the feature value gj =X1?S, where S is the index of the left-most sourceword where a gap starts.8Let X1, .
.
.
, Xn and Y1, .
.
.
, Ym represent indexes of thesource words covered by the tuples tj and tj?1 respectively.The distance between tj and tj?1 is given as dj = min(|Xk ?Yl| ?
1) ?Xk ?
{X1, .
.
.
, Xn} and ?
Yl ?
{Y1, .
.
.
, Ym}Lexical Features We also use source-to-targetp(e|f) and target-to-source p(f |e) lexical transla-tion probabilities.
Our lexical features are standard(Koehn et al, 2003).
The estimation is motivated byIBM Model-1.
Given a tuple ti with source wordsf = f1, f2, .
.
.
, fn, target words e = e1, e2, .
.
.
, emand an alignment a between the source word posi-tions x = 1, .
.
.
, n and the target word positionsy = 1, .
.
.
,m, the lexical feature pw(f |e) is com-puted as follows:pw(f |e, a) =n?x=11|{y : (x, y) ?
a}|??
(x,y)?aw(fx|ey)pw(e|f, a) is computed in the same way.5 DecodingOur decoder for the new model performs a stack-based search with a beam-search algorithm similarto that used in Pharoah (Koehn, 2004a).
Given aninput sentence F , it first extracts a set of match-ing source-side cepts along with their n-best trans-lations to form a tuple inventory.
During hypoth-esis expansion, the decoder picks a tuple from theinventory and generates the sequence of operationsrequired for the translation with this tuple in lightof the previous hypothesis.9 The sequence of op-erations may include translation (generate, continuesource cept etc.)
and reordering (gap insertions,jumps) operations.
The decoder also calculates theoverall cost of the new hypothesis.
Recombinationis performed on hypotheses having the same cov-erage vector, monolingual language model context,and operation model context.
We do histogram-based pruning, maintaining the 500 best hypothesesfor each stack.109A hypothesis maintains the index of the last source wordcovered (j), the position of the right-most source word coveredso far (Z), the number of open gaps, the number of gaps sofar inserted, the previously generated operations, the generatedtarget string, and the accumulated values of all the features dis-cussed in Section 4.10We need a higher beam size to produce translation unitssimilar to the phrase-based systems.
For example, the phrase-based system can learn the phrase pair ?zum Beispiel ?
for ex-ample?
and generate it in a single step placing it directly into thestack two words to the right.
Our system generates this examplewith two separate tuple translations ?zum ?
for?
and ?Beispiel?
example?
in two adjacent stacks.
Because ?zum ?
for?
is nota frequent translation unit, it will be ranked quite low in the firststack until the tuple ?Beispiel ?
example?
appears in the secondstack.
Koehn and his colleagues have repeatedly shown that in-1050Figure 3: Post-editing of Alignments (a) Initial (b) NoTarget-Discontinuities (c) Final Alignments6 TrainingTraining includes: (i) post-editing of the alignments,(ii) generation of the operation sequence (iii) estima-tion of the n-gram language models.Our generative story does not handle target-sidediscontinuities and unaligned target words.
There-fore we eliminate them from the training corpus in a3-step process: If a source word is aligned with mul-tiple target words which are not consecutive, firstthe link to the least frequent target word is iden-tified, and the group of links containing this wordis retained while the others are deleted.
The in-tuition here is to keep the alignments containingcontent words (which are less frequent than func-tional words).
The new alignment has no target-side discontinuities anymore, but might still containunaligned target words.
For each unaligned targetword, we determine the (left or right) neighbour thatit appears more frequently with and align it with thesame source word as the neighbour.
The result isan alignment without target-side discontinuities andunaligned target words.
Figure 3 shows an illustra-tive example of the process.
The tuples in Figure 3care ?A ?
U V?, ?B ?
W X Y?, ?C ?
NULL?, ?D ?
Z?.We apply Algorithm 1 to convert the preprocessedaligned corpus into a sequence of translation opera-tions.
The resulting operation corpus contains onesequence of operations per sentence pair.In the final training step, the three language mod-els are trained using the SRILM Toolkit.
The oper-ation model is estimated from the operation corpus.The prior probability model is estimated from thetarget side part of the bilingual corpus.
The mono-lingual language model is estimated from the targetside of the bilingual corpus and additional monolin-gual data.creasing the Moses stack size from 200 to 1000 does not havea significant effect on translation into English, see (Koehn andHaddow, 2009) and other shared task papers.7 Experimental Setup7.1 DataWe evaluated the system on three data sets withGerman-to-English, Spanish-to-English and French-to-English news translations, respectively.
We useddata from the 4th version of the Europarl Corpusand the News Commentary which was made avail-able for the translation task of the Fourth Workshopon Statistical Machine Translation.11 We use 200Kbilingual sentences, composed by concatenating theentire news commentary (?
74K sentences) and Eu-roparl (?
126K sentence), for the estimation of thetranslation model.
Word alignments were generatedwith GIZA++ (Och and Ney, 2003), using the grow-diag-final-and heuristic (Koehn et al, 2005).
In or-der to obtain the best alignment quality, the align-ment task is performed on the entire parallel data andnot just on the training data we use.
All data is low-ercased, and we use the Moses tokenizer and recap-italizer.
Our monolingual language model is trainedon 500K sentences.
These comprise 300K sentencesfrom the monolingual corpus (news commentary)and 200K sentences from the target-side part of thebilingual corpus.
The latter part is also used to trainthe prior probability model.
The dev and test setsare news-dev2009a and news-dev2009b which con-tain 1025 and 1026 parallel sentences.
The featureweights are tuned with Z-MERT (Zaidan, 2009).7.2 ResultsBaseline: We compare our model to a recent ver-sion of Moses (Koehn et al, 2007) using Koehn?straining scripts and evaluate with BLEU (Papineniet al, 2002).
We provide Moses with the same ini-tial alignments as we are using to train our system.12We use the default parameters for Moses, and a 5-gram English language model (the same as in oursystem).We compare two variants of our system.
The firstsystem (Twno?rl) applies no hard reordering limitand uses the distortion and gap distance penalty fea-tures as soft constraints, allowing all possible re-orderings.
The second system (Twrl?6) uses no dis-tortion and gap distance features, but applies a hardconstraint which limits reordering to no more than 611http://www.statmt.org/wmt09/translation-task.html12We tried applying our post-processing to the alignmentsprovided to Moses and found that this made little difference.1051Source German Spanish FrenchBlno?rl 17.41 19.85 19.39Blrl?6 18.57 21.67 20.84Twno?rl 18.97 22.17 20.94Twrl?6 19.03 21.88 20.72Table 3: This Work(Tw) vs Moses (Bl), no-rl = No Re-ordering Limit, rl-6 = Reordering limit 6positions.
Specifically, we do not extend hypothesesthat are more than 6 words apart from the first wordof the left-most gap during decoding.
In this exper-iment, we disallowed tuples which were discontin-uous on the source side.
We compare our systemswith two Moses systems as baseline, one using noreordering limit (Blno?rl) and one using the defaultdistortion limit of 6 (Blrl?6).Both of our systems (see Table 3) outperformMoses on the German-to-English and Spanish-to-English tasks and get comparable results for French-to-English.
Our best system (Twno?rl), which usesno hard reordering limit, gives statistically signifi-cant (p < 0.05)13 improvements over Moses (bothbaselines) for the German-to-English and Spanish-to-English translation task.
The results for Mosesdrop by more than a BLEU point without the re-ordering limit (see Blno?rl in Table 3).
All ourresults are statistically significant over the baselineBlno?rl for all the language pairs.In another experiment, we tested our system alsowith tuples which were discontinuous on the sourceside.
These gappy translation units neither improvedthe performance of the system with hard reorderinglimit (Twrl?6?asg) nor that of the system withoutreordering limit (Twno?rl?asg) as Table 4 shows.In an analysis of the output we found two reasonsfor this result: (i) Using tuples with source gaps in-creases the list of extracted n-best translation tuplesexponentially which makes the search problem evenmore difficult.
Table 5 shows the number of tuples(with and without gaps) extracted when decodingthe test file with 10-best translations.
(ii) The fu-ture cost14 is poorly estimated in case of tuples withgappy source cepts, causing search errors.In an experiment, we deleted gappy tuples with13We used Kevin Gimpel?s implementation of pairwise boot-strap resampling (Koehn, 2004b), 1000 samples.14The dynamic programming approach of calculating futurecost for bigger spans gives erroneous results when gappy ceptscan interleave.
Details omitted due to space limitations.Source German Spanish FrenchTwno?rl?asg 18.61 21.60 20.59Twrl?6?asg 18.65 21.40 20.47Twno?rl?hsg 18.91 21.93 20.87Twrl?6?hsg 19.23 21.79 20.85Table 4: Our Systems with Gappy Units, asg = All GappyUnits, hsg = Heuristic for pruning Gappy UnitsSource German Spanish FrenchGaps 965515 1705156 1473798No-Gaps 256992 313690 343220Heuristic (hsg) 281618 346993 385869Table 5: 10-best Translation Options With & WithoutGaps and using our Heuristica score (future cost estimate) lower than the sum ofthe best scores of the parts.
This heuristic removesmany useless discontinuous tuples.
We found thatresults improved (Twno?rl?hsg and Twrl?6?hsg inTable 4) compared to the version using all gaps(Twno?rl?asg, Twrl?6?asg), and are closer to theresults without discontinuous tuples (Twno?rl andTwrl?6 in Table 3).8 Sample OutputIn this section we compare the output of our sys-tems and Moses.
Example 1 in Figure 4 showsthe powerful reordering mechanism of our modelwhich moves the English verb phrase ?do not wantto negotiate?
to its correct position between the sub-ject ?they?
and the prepositional phrase ?about con-crete figures?.
Moses failed to produce the correctword order in this example.
Notice that althoughour model is using smaller translation units ?nicht?
do not?, ?verhandlen ?
negotiate?
and ?wollen ?want to?, it is able to memorize the phrase transla-tion ?nicht verhandlen wollen ?
do not want to ne-gotiate?
as a sequence of translation and reorderingoperations.
It learns the reordering of ?verhandlen ?negotiate?
and ?wollen ?
want to?
and also capturesdependencies across phrase boundaries.Example 2 shows how our system without a re-ordering limit moves the English translation ?vote?of the German clause-final verb ?stimmen?
acrossabout 20 English tokens to its correct position be-hind the auxiliary ?would?.Example 3 shows how the system with gappy tu-ples translates a German sentence with the particleverb ?kehrten...zuru?ck?
using a single tuple (dashedlines).
Handling phenomena like particle verbs1052Figure 4: Sample Output Sentencesstrongly motivates our treatment of source side gaps.The system without gappy units happens to pro-duce the same translation by translating ?kehrten?
to?returned?
and deleting the particle ?zuru?ck?
(solidlines).
This is surprising because the operation fortranslating ?kehrten?
to ?returned?
and for deletingthe particle are too far apart to influence each otherin an n-gram model.
Moses run on the same exam-ple deletes the main verb (?kehrten?
), an error thatwe frequently observed in the output of Moses.Our last example (Figure 5) shows that our modellearns idioms like ?meiner Meinung nach ?
In myopinion ,?
and short phrases like ?gibt es ?
thereare?
showing its ability to memorize these ?phrasal?translations, just like Moses.9 ConclusionWe have presented a new model for statistical MTwhich can be used as an alternative to phrase-based translation.
Similar to N-gram based MT,it addresses three drawbacks of traditional phrasalMT by better handling dependencies across phraseboundaries, using source-side gaps, and solving thephrasal segmentation problem.
In contrast to N-gram based MT, our model has a generative storywhich tightly couples translation and reordering.Furthermore it considers all possible reorderings un-like N-gram systems that perform search only onFigure 5: Learning Idiomsa limited number of pre-calculated orderings.
Ourmodel is able to correctly reorder words acrosslarge distances, and it memorizes frequent phrasaltranslations including their reordering as probableoperations sequences.
Our system outperformedMoses on standard Spanish-to-English and German-to-English tasks and achieved comparable results forFrench-to-English.
A binary version of the corpusconversion algorithm and the decoder is available.15AcknowledgmentsThe authors thank Fabienne Braune and the re-viewers for their comments.
Nadir Durrani wasfunded by the Higher Education Commission (HEC)of Pakistan.
Alexander Fraser was funded byDeutsche Forschungsgemeinschaft grant Modelsof Morphosyntax for Statistical Machine Transla-tion.
Helmut Schmid was supported by DeutscheForschungsgemeinschaft grant SFB 732.15http://www.ims.uni-stuttgart.de/?durrani/resources.html1053ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and R. L. Mercer.
1993.
The mathematics ofstatistical machine translation: parameter estimation.Computational Linguistics, 19(2):263?311.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Josep Maria Crego and Franois Yvon.
2009.
Gappytranslation units under left-to-right smt decoding.
InProceedings of the meeting of the European Associa-tion for Machine Translation (EAMT), pages 66?73,Barcelona, Spain.Josep Maria Crego and Franc?ois Yvon.
2010.
Improv-ing reordering with linguistically informed bilingualn-grams.
In Coling 2010: Posters, pages 197?205,Beijing, China, August.
Coling 2010 Organizing Com-mittee.Josep M. Crego, Marta R. Costa-juss, Jos B. Mario,and Jos A. R. Fonollosa.
2005a.
Ngram-based ver-sus phrasebased statistical machine translation.
In InProceedings of the International Workshop on SpokenLanguage Technology (IWSLT05, pages 177?184.Josep M. Crego, Jose?
B. Marin?o, and Adria` de Gispert.2005b.
Reordered search and unfolding tuples forngram-based SMT.
In Proceedings of the 10th Ma-chine Translation Summit (MT Summit X), pages 283?289, Phuket, Thailand.Michel Galley and Christopher D. Manning.
2010.
Ac-curate non-hierarchical phrase-based translation.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 966?974, Los Angeles, California, June.
Association forComputational Linguistics.Philipp Koehn and Barry Haddow.
2009.
Edinburgh?ssubmission to all tracks of the WMT 2009 shared taskwith reordering and speed improvements to Moses.In Proceedings of the Fourth Workshop on StatisticalMachine Translation, pages 160?164, Athens, Greece,March.
Association for Computational Linguistics.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofthe Human Language Technology and North Ameri-can Association for Computational Linguistics Con-ference, pages 127?133, Edmonton, Canada.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system description forthe 2005 iwslt speech translation evaluation.
In Inter-national Workshop on Spoken Language Translation2005.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics, Demonstration Program,Prague, Czech Republic.Philipp Koehn.
2004a.
Pharaoh: A beam search decoderfor phrase-based statistical machine translation mod-els.
In AMTA, pages 115?124.Philipp Koehn.
2004b.
Statistical significance testsfor machine translation evaluation.
In Dekang Linand Dekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.Zhifei Li, Chris Callison-burch, Chris Dyer, Juri Ganitke-vitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G.Thornton, Jonathan Weese, and Omar F. Zaidan.
2009.Joshua: An open source toolkit for parsing-based ma-chine translation.J.B.
Marin?o, R.E.
Banchs, J.M.
Crego, A. de Gispert,P.
Lambert, J.A.R.
Fonollosa, and M.R.
Costa-jussa`.2006.
N-gram-based machine translation.
Computa-tional Linguistics, 32(4):527?549.I.
Dan Melamed.
2004.
Statistical machine translationby parsing.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics,Barcelona, Spain.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Franz J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(1):417?449.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Intl.
Conf.
Spoken Language Pro-cessing, Denver, Colorado.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.1054
