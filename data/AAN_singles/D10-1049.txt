Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502?512,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsA Simple Domain-Independent Probabilistic Approach to GenerationGabor AngeliUC BerkeleyBerkeley, CA 94720gangeli@berkeley.eduPercy LiangUC BerkeleyBerkeley, CA 94720pliang@cs.berkeley.eduDan KleinUC BerkeleyBerkeley, CA 94720klein@cs.berkeley.eduAbstractWe present a simple, robust generation systemwhich performs content selection and surfacerealization in a unified, domain-independentframework.
In our approach, we break upthe end-to-end generation process into a se-quence of local decisions, arranged hierar-chically and each trained discriminatively.We deployed our system in three differentdomains?Robocup sportscasting, technicalweather forecasts, and common weather fore-casts, obtaining results comparable to state-of-the-art domain-specific systems both in termsof BLEU scores and human evaluation.1 IntroductionIn this paper, we focus on the problem of generat-ing descriptive text given a world state representedby a set of database records.
While existing gen-eration systems can be engineered to obtain goodperformance on particular domains (e.g., Dale etal.
(2003), Green (2006), Turner et al (2009), Re-iter et al (2005), inter alia), it is often difficultto adapt them across different domains.
Further-more, content selection (what to say: see Barzilayand Lee (2004), Foster and White (2004), inter alia)and surface realization (how to say it: see Ratna-parkhi (2002), Wong and Mooney (2007), Chen andMooney (2008), Lu et al (2009), etc.)
are typicallyhandled separately.
Our goal is to build a simple,flexible system which is domain-independent andperforms content selection and surface realization ina unified framework.We operate in a setting in which we are only givenexamples consisting of (i) a set of database records(input) and (ii) example human-generated text de-scribing some of those records (output).
We use themodel of Liang et al (2009) to automatically inducethe correspondences between words in the text andthe actual database records mentioned.We break up the full generation process into a se-quence of local decisions, training a log-linear clas-sifier for each type of decision.
We use a simplebut expressive set of domain-independent features,where each decision is allowed to depend on the en-tire history of previous decisions, as in the modelof Ratnaparkhi (2002).
These long-range contextualdependencies turn out to be critical for accurate gen-eration.More specifically, our model is defined in termsof three types of decisions.
The first typechooses records from the database (macro contentselection)?for example, wind speed, in the caseof generating weather forecasts.
The second typechooses a subset of fields from a record (micro con-tent selection)?e.g., the minimum and maximumtemperature.
The third type chooses a suitable tem-plate to render the content (surface realization)?e.g., winds between [min] and [max] mph; templatesare automatically extracted from training data.We tested our approach in three domains:ROBOCUP, for sportscasting (Chen and Mooney,2008); SUMTIME, for technical weather forecastgeneration (Reiter et al, 2005); and WEATHERGOV,for common weather forecast generation (Liang etal., 2009).
We performed both automatic (BLEU)and human evaluation.
On WEATHERGOV, we502s: pass(arg1=purple6, arg2=purple3)kick(arg1=purple3)badPass(arg1=purple3,arg2=pink9)turnover(arg1=purple3,arg2=pink9)w: purple3 made a bad passthat was picked off by pink9(a) Robocups: temperature(time=5pm-6am,min=48,mean=53,max=61)windSpeed(time=5pm-6am,min=3,mean=6,max=11,mode=0-10)windDir(time=5pm-6am,mode=SSW)gust(time=5pm-6am,min=0,mean=0,max=0)skyCover(time=5pm-9pm,mode=0-25)skyCover(time=2am-6am,mode=75-100)precipPotential(time=5pm-6am,min=2,mean=14,max=20)rainChance(time=5pm-6am,mode=someChance)w: a 20 percent chance of showers after midnight .
increasing clouds ,with a low around 48 southwest wind between 5 and 10 mph(b) WeatherGovs: wind10m(time=6am,dir=SW,min=16,max=20,gust min=0,gust max=-)wind10m(time=9pm,dir=SSW,min=28,max=32,gust min=40,gust max=-)wind10m(time=12am,dir=-,min=24,max=28,gust min=36,gust max=-)w: sw 16 - 20 backing ssw 28 - 32 gusts 40 by mid evening easing 24 - 28 gusts 36 late evening(c) SumTimeFigure 1: Example scenarios (a scenario is a world state s paired with a text w) for each of the three domains.
Each row in theworld state denotes a record.
Our generation task is to map a world state s (input) to a text w (output).
Note that this mappinginvolves both content selection and surface realization.achieved a BLEU score of 51.5 on the combined taskof content selection and generation, which is morethan a two-fold improvement over a model similarto that of Liang et al (2009).
On ROBOCUP andSUMTIME, we achieved results comparable to thestate-of-the-art.
most importantly, we obtained theseresults with a general-purpose approach that we be-lieve is simpler than current state-of-the-art systems.2 Setup and DomainsOur goal is to generate a text given a world state.The world state, denoted s, is represented by a setof database records.
Define T to be a set of recordtypes, where each record type t ?
T is associatedwith a set of fields FIELDS(t).
Each record r ?
shas a record type r.t ?
T and a field value r.v[f ] foreach field f ?
FIELDS(t).
The text, denoted w, isrepresented by a sequence of tokenized words.
Weuse the term scenario to denote a world state s pairedwith a text w.In this paper, we conducted experiments on threedomains, which are detailed in the following subsec-tions.
Example scenarios for each domain are de-tailed in Figure 1.2.1 ROBOCUP: SportscastingA world state in the ROBOCUP domain is a set ofevent records (meaning representations in the termi-nology of Chen and Mooney (2008)) generated bya robot soccer simulator.
For example, the recordpass(arg1=pink1,arg2=pink5) denotes a passingevent; records of this type (pass) have two fields:arg1 (the agent) and arg2 (the recipient).
As thegame progresses, human commentators talk aboutsome of the events in the game, e.g., purple3 madea bad pass that was picked off by pink9.We used the dataset created by Chen and Mooney(2008), which contains 1919 scenarios from the2001?2004 Robocup finals.
Each scenario con-sists of a single sentence representing a fragmentof a commentary on the game, paired with a setof candidate records, which were recorded withinfive seconds of the commentary.
The records in theROBOCUP dataset data were aligned by Chen andMooney (2008).
Each scenario contains on average|s| = 2.4 records and 5.7 words.
See Figure 1(a) foran example of a scenario.
Content selection in thisdomain is choosing the single record to talk about,and surface realization is talking about it.5032.2 SUMTIME: Technical Weather ForecastsReiter et al (2005) developed a generation systemand created the SUMTIME-METEO corpus, whichconsists of marine wind weather forecasts used byoffshore oil rigs, generated by the output of weathersimulators.
More specifically, these forecasts de-scribe various aspects of the wind at different timesduring the forecast period.We used the version of the SUMTIME-METEOcorpus created by Belz (2008).
The dataset consistsof 469 scenarios, each containing on average |s| =2.6 records and 16.2 words.
See Figure 1(c) for anexample of a scenario.
This task requires no contentselection, only surface realization: The records aregiven in some fixed order and the task is to generatefrom each of these records in turn; of course, dueto contextual dependencies, these records cannot begenerated independently.2.3 WEATHERGOV: Common WeatherForecastsIn the WEATHERGOV domain, the world state con-tains detailed information about a local weatherforecast (e.g., temperature, rain chance, etc.).
Thetext is a short forecast report based on this informa-tion.We used the dataset created by Liang et al (2009).The world state is summarized by records which ag-gregate measurements over selected time intervals.The dataset consists of 29,528 scenarios, each con-taining on average |s| = 36 records and 28.7 words.See Figure 1(b) for an example of a scenario.While SUMTIME and WEATHERGOV are bothweather domains, there are significant differencesbetween the two.
SUMTIME forecasts are in-tended to be read by trained meteorologists, and thusthe text is quite abbreviated.
On the other hand,WEATHERGOV texts are intended to be read by thegeneral public and thus is more English-like.
Fur-thermore, SUMTIME does not require content selec-tion, whereas content selection is a major focus ofWEATHERGOV.
Indeed, on average, only 5 of 36records are actually mentioned in a WEATHERGOVscenario.
Also, WEATHERGOV is more complex:The text is more varied, there are multiple recordtypes, and there are about ten times as many recordsin each world state.Generation Processfor i = 1, 2, .
.
.
:?choose a record ri ?
s?if ri = STOP: return?choose a field set Fi ?
FIELDS(ri.t)?choose a template Ti ?
TEMPLATES(ri.t, Fi)Figure 2: Pseudocode for the generation process.
The generatedtext w is a deterministic function of the decisions.3 The Generation ProcessTo model the process of generating a text w from aworld state s, we decompose the generation processinto a sequence of local decisions.
There are two as-pects of this decomposition that we need to specify:(i) how the decisions are structured; and (ii) whatpieces of information govern the decisions.The decisions are structured hierarchically intothree types of decisions: (i) record decisions, whichdetermine which records in the world state to talkabout (macro content selection); (ii) field set deci-sions, which determine which fields of those recordsto mention (micro content selection); and (iii) tem-plate decisions, which determine the actual wordsto use to describe the chosen fields (surface realiza-tion).
Figure 2 shows the pseudocode for the gen-eration process, while Figure 3 depicts an exampleof the generation process on a WEATHERGOV sce-nario.Each of these decisions is governed by a set offeature templates (see Figure 4), which are repre-sented as functions of the current decision and pastdecisions.
The feature weights are learned fromtraining data (see Section 4.3).We chose a set of generic domain-independentfeature templates, described in the sections below.These features can, in general, depend on the currentdecision and all previous decisions.
For example, re-ferring to Figure 4, R2 features on the record choicedepend on all the previous record decisions, and R5features depend on the most recent template deci-sion.
This is in contrast with most systems for con-tent selection (Barzilay and Lee, 2004) and surfacerealization (Belz, 2008), where decisions must de-compose locally according to either a graph or tree.The ability to use global features in this manner is504Worldstate skyCover1: skyCover(time=5pm-6am,mode=50-75)temperature1: temperature(time=5pm-6am,min=44,mean=49,max=60)...DecisionsRecord r1 = skyCover1 r2 = temperature1 r3 = stopField set F1 = {mode} F2 = {time,min}Template T1 = ?mostly cloudy ,?
T2 = ?with a low around [min] .
?Text mostly cloudy , with a low around 45 .Specific active (nonzero) features for highlighted decisionsr2 = temperature1(R1) Jr2.t = temperature and (r1.t, r0.t) = (skyCover, start)KJr2.t = temperature and (r1.t) = (skyCover)K(R2) Jr2.t = temperature and {r1.t} = {skyCover}K(R3) Jr2.t = temperature and rj .t 6= temperature ?j < 2K(R4) Jr2.t = temperature and r2.v[time] = 5pm-6amKJr2.t = temperature and r2.v[min] = lowKJr2.t = temperature and r2.v[mean] = lowKJr2.t = temperature and r2.v[max] = mediumKF2 = {time,min} (F1) JF2 = {time,min}K(F2) JF2 = {time,min} and r2.v[time] = 5pm-6amK(F2) JF2 = {time,min} and r2.v[min] = lowKT2 = ?with a low around [min]?
(W1) JBase(T2) = ?with a low around [min]?KJCoarse(T2) = ?with a [time] around [min]?K(W2) JBase(T2) = ?with a low around [min]?
and r2.v[time] = 5pm-6amKJCoarse(T2) = ?with a [time] around [min]?
and r2.v[time] = 5pm-6amKJBase(T2) = ?with a low around [min]?
and r2.v[min] = lowKJCoarse(T2) = ?with a [time] around [min]?
and r2.v[min] = lowK(W3) log plm(with | cloudy ,)Figure 3: The generation process on an example WEATHERGOV scenario.
The figure is divided into two parts: The upper part ofthe figure shows the generation of text from the world state via a sequence of seven decisions (in boxes).
Three of these decisionsare highlighted and the features that govern these decisions are shown in the lower part of the figure.
Note that different decisionsin the generation process would result in different features being active (nonzero).Feature TemplatesRecord R1?
list of last k record types Jri.t = ?
and (ri?1.t, .
.
.
, ri?k.t) = ?K for k ?
{1, 2}R2 set of previous record types Jri.t = ?
and {rj .t : j < i} = ?KR3 record type already generated Jrj .t = ri.t for some j < iKR4 field values Jri.t = ?
and ri.v[f ] = ?K for f ?
Fields(ri.t)R5?
stop under language model (LM) Jri.t = stopK?
log plm(stop | previous two words generated)Field set F1?
field set JFi = ?KF2 field values JFi = ?
and ri.v[f ] = ?K for f ?
FiTemplate W1?
base/coarse generation template Jh(Ti) = ?K for h ?
{Base,Coarse}W2 field values Jh(Ti) = ?
and ri.v[f ] = ?K for f ?
Fi, h ?
{Base,Coarse}W3?
first word of template under LM log plm(first word in Ti | previous two words)Figure 4: Feature templates that govern the record, field set, and template decisions.
Each line specifies the name, informaldescription, and formal description of a set of features, obtained by ranging ?
over possible values (for example, for Jri.t = ?K, ?ranges over all record types T ).
Notation: JeK returns 1 if the expression e is true and 0 if it is false.
These feature templates aredomain-independent; that is, they are used to create features automatically across domains.
Feature templates marked with ?
areincluded in our baseline system (Section 5.2).one of the principal advantages of our approach.3.1 Record DecisionsRecord decisions are responsible for macro contentselection.
Each record decision chooses a record rifrom the world state s according to features of thefollowing types:R1 captures the discourse coherence aspect ofcontent selection; for example, we learn thatwindSpeed tends to follow windDir (but not al-505ways).
R2 captures an unordered notion ofcoherence?simply which sets of record types arepreferable; for example, we learn that rainChanceis not generated if sleetChance already was men-tioned.
R3 is a coarser version of R2, capturinghow likely it is to propose a record of a type that hasalready been generated.
R4 captures the importantaspect of content selection that the records chosendepend on their field values;1 for example, we learnthat snowChance is not chosen unless there is snow.R5 allows the language model to indicate whether aSTOP record is appropriate; this helps prevent sen-tences from ending abruptly.3.2 Field Set DecisionsField set decisions are responsible for micro con-tent selection, i.e., which fields of a record are men-tioned.
Each field set decision chooses a subset offields Fi from the set of fields FIELDS(ri.t) of therecord ri that was just generated.
These decisionsare made based on two types of features:F1 captures which sets of fields are talkedabout together; for example, we learn that {mean}and {min,max} are preferred field sets for thewindSpeed record.
By defining features on the en-tire field set, we can capture any correlation structureover the fields; in contrast, Liang et al (2009) gen-erates a sequence of fields in which a field can onlydepend on the previous one.F2 allows the field set to be chosen based on thevalues of the fields, analogously to R4.3.3 Template DecisionsTemplate decisions perform surface realization.
Atemplate is a sequence of elements, where each ele-ment is either a word (e.g., around) or a field (e.g.,[min]).
Given the record ri and field set Fi that weare generating from, the goal is to choose a templateTi (Section 4.3.2 describes how we define the setof possible templates).
The features that govern thechoice of Ti are as follows:W1 captures a priori preferences for generationtemplates given field sets.
There are two waysto control this preference, BASE and COARSE.1We map a numeric field value onto one of five categories(very-low, low, medium, high, or very-high) basedon its value with respect to the mean and standard deviation ofvalues of that field in the training data.BASE(Ti) denotes the template Ti itself, thus allow-ing us to remember exactly which templates wereuseful.
To guard against overfitting, we also useCOARSE(Ti), which maps Ti to a coarsened versionof Ti, in which more words are replaced with theirassociated fields (see Figure 5 for an example).W2 captures a dependence on the values of fieldsin the field set, and is analogous to R4 and F2.
Fi-nally, W3 contributes a language model probability,to ensure smooth transitions between templates.After Ti has been chosen, each field in the tem-plate is replaced with a word given the correspond-ing field value in the world state.
In particular, aword is chosen from the parameters learned in themodel of Liang et al (2009).
In the example in Fig-ure 3, the [min] field in T2 has value 44, which isrendered to the word 45 (rounding and other noisydeviations are common in the WEATHERGOV do-main).4 Learning a Probabilistic ModelHaving described all the features, we now present aconditional probabilistic model over texts w givenworld states s (Section 4.1).
Section 4.2 describeshow to use the model for generation, and Section 4.3describes how to learn the model.4.1 ModelRecall from Section 3 that the generation processgenerates r1, F1, T1, r2, F2, T2, .
.
.
, STOP.
To unifynotation, denote this sequence of decisions as d =(d1, .
.
.
, d|d|).Our probability model is defined as follows:p(d | s; ?)
=|d|?j=1p(dj | d<j ; ?
), (1)where d<j = (d1, .
.
.
, dj?1) is the history of de-cisions and ?
are the model parameters (featureweights).
Note that the text w (the output) is a de-terministic function of the decisions d. We use thefeatures described in Section 3 to define a log-linearmodel for each decision:p(dj | d<j , s; ?)
=exp{?j(dj ,d<j , s)>?
}?d?j?Djexp{?j(d?j ,d<j , s)>?
}, (2)where ?
are all the parameters (feature weights), ?jis the feature vector for the j-th decision, and Dj is506the domain of the j-th decision (either records, fieldsets, or templates).This chaining of log-linear models was used inRatnaparkhi (1998) for tagging and parsing, and inRatnaparkhi (2002) for surface realization.
The abil-ity to condition on arbitrary histories is a definingproperty of these models.4.2 Using the Model for GenerationSuppose we have learned a model with parameters ?
(how to obtain ?
is discussed in Section 4.3).
Givena world state s, we would like to use our model togenerate an output text w via a decision sequence d.In our experiments, we choose d by sequentiallychoosing the best decision in a greedy fashion (untilthe STOP record is generated):dj = argmaxd?jp(d?j | d<j , s; ?).
(3)Alternatively, instead of choosing the best decisionat each point, we can sample from the distribution:dj ?
p(dj | d<j , s; ?
), which provides more diversegenerated texts at the expense of a slight degradationin quality.Both greedy search and sampling are very effi-cient.
Another option is to try to find the Viterbidecision sequence, i.e., the one with the maximumjoint probability: d = argmaxd?
p(d?
| s; ?).
How-ever, this computation is intractable due to featuresdepending arbitrarily on past decisions, making dy-namic programming infeasible.
We tried using beamsearch to approximate this optimization, but we ac-tually found that beam search performed worse thangreedy.
Belz (2008) also found that greedy was moreeffective than Viterbi for their model.4.3 LearningNow we turn our attention to learning the parame-ters ?
of our model.
We are given a set of N sce-narios {(s(i),w(i))}Ni=1 as training data.
Note thatour model is defined over the decision sequence dwhich contains information not present in w. In Sec-tions 4.3.1 and 4.3.2, we show how we fill in thismissing information to obtain d(i) for each trainingscenario i.Assuming this missing information is filled, weend up with a standard supervised learning problem,which can be solved by maximize the (conditional)likelihood of the training data:max??Rd?
?N?i=1|d(i)|?j=1log p(d(i)j | d(i)<j ; ?)???
?||?||2, (4)where ?
> 0 is a regularization parameter.
The ob-jective function in (4) is optimized using the stan-dard L-BFGS algorithm (Liu and Nocedal, 1989).4.3.1 Latent AlignmentsAs mentioned previously, our training data in-cludes only the world state s and generated text w,not the full sequence of decisions d needed for train-ing.
Intuitively, we know what was generated but notwhy it was generated.We use the model of Liang et al (2009) to im-pute the decisions d. They introduce a generativemodel p(a,w|s), where the latent alignment a spec-ifies (1) the sequence of records that were chosen,(2) the sequence of fields that were chosen, and (3)which words in the text were spanned by the chosenrecords and fields.
The model is learned in an unsu-pervised manner using EM to produce a observingonly w and s.An example of an alignment is given in the leftpart of Figure 5.
This information specifies therecord decisions and a set of fields for each record.Because the induced alignments can be noisy, weneed to process them to obtain cleaner template de-cisions.
This is the subject of the next section.4.3.2 Template ExtractionGiven an aligned training scenario (Figure 5), wewould like to extract two types of templates.For each record, an aligned training scenariospecifies a sequence of fields and the text thatis spanned by each field.
We create a templateby abstracting fields?that is, replacing the wordsspanned by a field by the field itself.
We call theresulting template COARSE.
The problem with us-ing this template directly is that fields can be noisydue to errors from the unsupervised model.Therefore, we also create a BASE template whichonly abstracts a subset of the fields.
In particular,we define a trigger pattern which specifies a simplecondition under which a field should be abstracted.For WEATHERGOV, we only abstract fields that507Records:Fields:Text:skyCover1mode=50-75mostly cloudy ,temperature1xwith a time=17-30low around min=4445 mean=49.Aligned training scenario?skyCover temperatureCoarse ?[mode]?
?with a [time] [min] [mean]?Base ?most cloudy ,?
?with a low around [min] .
?Templates extractedFigure 5: An example of template extraction from an imperfectly aligned training scenario.
Note that these alignments are noisy(e.g., [mean] aligns to a period).
Therefore, for each record (skyCover and temperature in this case), we extract two templates:(1) a COARSE template, which takes the text spanned by the record and abstracts away all fields in the scenario ([mode], [time],[min], and [mean] in the example); and (2) a BASE template, which only abstracts away fields whose spanned text matches a simplepattern (e.g., numbers in WEATHERGOV, corresponding to [min] in the example).span numbers; for SUMTIME, fields that span num-bers and wind directions; and for ROBOCUP, fieldsthat span words starting with purple or pink.For each record ri, we define Ti so that BASE(Ti)and COARSE(Ti) are the corresponding two ex-tracted templates.
We restrict Fi to the set of ab-stracted fields in the COARSE template5 ExperimentsWe now present an empirical evaluation of our sys-tem on our three domains?ROBOCUP, SUMTIME,and WEATHERGOV.5.1 Evaluation MetricsAutomatic Evaluation To evaluate surface real-ization (or, combined content selection and surfacerealization), we measured the BLEU score (Papineniet al, 2002) (the precision of 4-grams with a brevitypenalty) of the system-generated output with respectto the human-generated output.To evaluate macro content selection, we measuredthe F1 score (the harmonic mean of precision andrecall) of the set of records chosen with respect tothe human-annotated set of records.Human Evaluation We conducted a human eval-uation using Amazon Mechanical Turk.
For eachdomain, we chose 100 scenarios randomly from thetest set.
We ran each system under consideration oneach of these scenarios, and presented each resultingoutput to 10 evaluators.2 Evaluators were given in-structions to rank an output on the basis of Englishfluency and semantic correctness on the followingscale:2To minimize bias, we evaluated all the systems at once,randomly shuffling the outputs of the systems.
The evaluatorswere not necessarily the same 10 evaluators.Score English Fluency Semantic Correctness5 Flawless Perfect4 Good Near Perfect3 Non-native Minor Errors2 Disfluent Major Errors1 Gibberish Completely WrongEvaluators were also given additional domain-specific information: (1) the background of thedomain (e.g., that SUMTIME reports are techni-cal weather reports); (2) general properties of thedesired output (e.g., that SUMTIME texts shouldmention every record whereas WEATHERGOV textsneed not); and (3) peculiarities of the text (e.g., thesuffix ly in SUMTIME should exist as a separate to-ken from its stem, or that pink goalie and pink1 havethe same meaning in ROBOCUP).5.2 SystemsWe evaluated the following systems on our three do-mains:?
HUMAN is the human-generated output.?
OURSYSTEM uses all the features in Figure 4and is trained according to Section 4.3.?
BASELINE is OURSYSTEM using a subset ofthe features (those marked with ?
in Fig-ure 4).
In contrast to OURSYSTEM, the in-cluded features only depend on a local con-text of decisions in a manner similar tothe generative model of Liang et al (2009)and the pCRU-greedy system of Belz (2008).BASELINE also excludes features that dependon values of the world state.?
The existing state-of-the-art domain-specificsystem for each domain.5.3 ROBOCUP ResultsFollowing the evaluation methodology of Chen andMooney (2008), we trained our system on three508System F1 BLEU* EnglishFluencySemanticCorrectnessBASELINE 78.7 24.8 4.28 ?
0.78 4.15 ?
1.14OURSYSTEM 79.9 28.8 4.34 ?
0.69 4.17 ?
1.21WASPER-GEN 72.0 28.7 4.43 ?
0.76 4.27 ?
1.15HUMAN ?
?
4.43 ?
0.69 4.30 ?
1.07Table 1: ROBOCUP results.
WASPER-GEN is described inChen and Mooney (2008).
The BLEU is reported on systemsthat use fixed human-annotated records (in other words, weevaluate surface realization given perfect content selection).Human Records:Fields:Text:pass1arg1=purple10purple10 xpasses back to arg2=purple9purple9Baseline Records:Fields:Text:pass1arg1=purple10purple10 xkicks to arg2=purple9purple9OurSystem Records:Fields:Text:pass1arg1=purple10purple10 xpasses to arg2=purple9purple9WASPER-GENRecords:Text purple10 passes to purple9Figure 6: Outputs of systems on an example ROBOCUP sce-nario.
There are some minor differences between the outputs.Recall that OURSYSTEM differs from BASELINE mostly inthe addition of feature W2, which captures dependencies be-tween field values (e.g., purple10) and the template chosen(e.g., [arg1] passes to [arg2]).
This allows us to capture value-dependent preferences for different realizations (e.g., passes toover kicks to).
Also, HUMAN uses passes back to, but this wordchoice requires knowledge of passing records in previous sce-narios, which none of the systems have access to.
It would nat-ural, however, to add features that would capture these longer-range dependencies in our framework.Robocup games and tested on the fourth, averagingover the four train/test splits.
We report the averagetest accuracy weighted by the number of scenariosin a game.
First, we evaluated macro content selec-tion.
Table 1 shows that OURSYSTEM significantlyoutperforms BASELINE and WASPER-GEN on F1.To compare with Chen and Mooney (2008) onsurface realization, we fixed each system?s recorddecisions to the ones given by the annotated dataand enforced that all the fields of that record arechosen.
Table 1 shows that OURSYSTEM sig-nificantly outperforms BASELINE and is compara-ble to WASPER-GEN on BLEU.
On human eval-uation, OURSYSTEM outperforms BASELINE, butWASPER-GEN outperforms OURSYSTEM.
SeeFigure 6 for example outputs from the various sys-tems.BLEU EnglishFluencySemanticCorrectnessBASELINE 32.9 4.23 ?
0.71 4.26 ?
0.85OURSYSTEM 55.1 4.25 ?
0.69 4.27 ?
0.82OURSYSTEM-CUSTOM 62.3 4.12 ?
0.78 4.33 ?
0.91pCRU-greedy 63.6 4.18 ?
0.71 4.49 ?
0.73SUMTIME-Hybrid 52.7 ?
?HUMAN ?
4.09 ?
0.83 4.37 ?
0.87Table 2: SUMTIME results.
The SUMTIME-Hybrid systemis described in (Reiter et al, 2005); pCRU-greedy, in (Belz,2008).5.4 SUMTIME ResultsThe SUMTIME task only requires micro content se-lection and surface realization because the sequenceof records to be generated is fixed; only these as-pects are evaluated.
Following the methodology ofBelz (2008), we used five-fold cross validation.We found that using the unsupervised model ofLiang et al (2009) to automatically produce alignedtraining scenarios (Section 4.3.1) was less effec-tive than it was in the other two domains due totwo factors: (i) there are fewer training examplesin SUMTIME and unsupervised learning typicallyworks better with a large amount of data; and (ii)the alignment model does not exploit the temporalstructure in the SUMTIME world state.
Therefore,we used a small set of simple regular expressions toproduce aligned training scenarios.Table 2 shows that OURSYSTEM signif-icantly outperforms BASELINE as well asSUMTIME-Hybrid, a hand-crafted system, onBLEU.
Note that OURSYSTEM is domain-independent and has not been specifically tunedto SUMTIME.
However, OURSYSTEM is outper-formed by the state-of-the-art statistical systempCRU-greedy.Custom Features One of the advantages of ourfeature-based approach is that it is straightforward toincorporate domain-specific features to capture spe-cific properties of a domain.
To this end, we definethe following set of feature templates in place of ourgeneric feature templates from Figure 4:?
F1?
: Value of time?
F2?
: Existence of gusts/wind direction/windspeeds?
W1?
: Change in wind direction (clockwise,counterclockwise, or none)509Human Records:Fields:Text:windDir1dir=nnenne min=1818 x- max=2222 gust-min=30gusts 30windDir2xgradually decreasing min=1010 x- max=1414 time=12amby late eveningBaseline Records:Fields:Text:windDir1dir=nnenne min=1818 x- max=2222 gust-min=30gusts 30windDir2xincreasing min=1010 x- max=1414OurSystem-Custom Records:Fields:Text:windDir1dir=nnenne min=1818 x- max=2222 gust-min=30gusts 30windDir2xgradually decreasing min=1010 x- max=1414 time=12amby late eveningpCRU-greedy Records:Text nne 18 - 22 gusts 30 easing 10 - 14 by late eveningFigure 7: Outputs of systems on an example SUMTIME scenario.
Two notable differences between OURSYSTEM-CUSTOM andBASELINE arise due to OURSYSTEM-CUSTOM?s value-dependent features.
For example, OURSYSTEM-CUSTOM can choosewhether to include the time field (windDir2) or not (windDir1), depending on the value of the time (F1?
), thereby improving contentselection.
OURSYSTEM-CUSTOM also improves surface realization, choosing gradually decreasing over BASELINE?s increasing.Interestingly, this improvement comes from the joint effort of two features: W2?
prefers decreasing over increasing in this case,and W5?
adds the modifier gradually.
An important strength of log-linear models is the ability to combine soft preferences frommany features.?
W2?
: Change in wind speed?
W3?
: Change in wind direction and speed?
W4?
: Existence of gust min and/or max?
W5?
: Time elapsed since last record?
W6?
: Whether wind is a cardinal direction (N,E, S, W)The resulting system, which we callOURSYSTEM-CUSTOM, obtains a BLEU scorewhich is comparable to pCRU-greedy.An important aspect of our system that it is flexi-ble and quick to deploy.
According to Belz (2008),SUMTIME-Hybrid took twelve person-months tobuild, while pCRU-greedy took one month.
Havingdeveloped OURSYSTEM in a domain-independentway, we only needed to do simple reformatting uponreceiving the SUMTIME data.
Furthermore, it tookonly a few days to develop the custom featuresabove to create OURSYSTEM-CUSTOM, which hasBLEU performance comparable to the state-of-the-art pCRU-greedy system.We also conducted human evaluations on the foursystems shown in Table 2.
Note that this evalua-tion is rather difficult for Mechanical Turkers sinceSUMTIME texts are rather technical compared tothose in other domains.
Interestingly, all systemsoutperform HUMAN on English fluency; this resultcorroborates the findings of Belz (2008).
On se-mantic correctness, all systems perform comparablyto HUMAN, except pCRU-greedy, which performsslightly better.
See Figure 7 for a comparison of theoutputs generated by the various systems.F1 BLEU* EnglishFluencySemanticCorrectnessBASELINE 22.1 22.2 4.07 ?
0.59 3.41 ?
1.16OURSYSTEM 65.4 51.5 4.12 ?
0.74 4.22 ?
0.89HUMAN ?
?
4.14 ?
0.71 3.85 ?
0.99Table 3: WEATHERGOV results.
The BLEU score is on jointcontent selection and surface realization and is modified to notpenalize numeric deviations of at most 5.5.5 WEATHERGOV ResultsWe evaluate the WEATHERGOV corpus on the jointtask of content selection and surface realization.We split our corpus into 25,000 scenarios for train-ing, 1,000 for development, and 3,528 for testing.In WEATHERGOV, numeric field values are oftenrounded or noisily perturbed, so it is difficult to gen-erate precisely matching numbers.
Therefore, weused a modified BLEU score where numbers dif-fering by at most five are treated as equal.
Fur-thermore, WEATHERGOV is evaluated on the jointcontent selection and surface realization task, un-like ROBOCUP, where content selection and surfacerealization were treated separately, and SUMTIME,where content selection was not applicable.Table 3 shows the results.
We see thatOURSYSTEM substantially outperforms BASELINE,especially on BLEU score and semantic correctness.This difference shows that taking non-local contextinto account is very important in this domain.
Thisresult is not surprising, since WEATHERGOV is themost complicated of the three domains, and thiscomplexity is exactly where non-locality is neces-510Human Records:Fields:Text:skyCover1cover=50-75mostly cloudy x,temperature1xwith a time=5pm-6amlow xaround min=5957 x.windDir1mode=ssesouth xwind betweenwindSpeed1min=75 xand max=1510 xmph .Baseline Records:Fields:Text:rainChance2xa chance of showers ,nonex,gust1xwith gusts as high as max=2120 xmph .precipPotential1xchance of precipitation is max=1010 x% .OurSystem Records:Fields:Text:skyCover1xmostly cloudy ,temperature1xwith a low around min=5959 x.windDir1xsouth wind betweenwindSpeed1min=77 xand max=1515 xmph .Figure 8: Outputs of systems on an example WEATHERGOV scenario.
Most of the gains of OURSYSTEM over BASELINE comefrom improved content selection.
For example, BASELINE chooses rainChance because it happens to be the most common firstrecord type in the training data.
However, since OURSYSTEM has features that depend on the value of rainChance (noChancein this case), it has learned to disprefer talking about rain when there is no rain.
Also, OURSYSTEM has additional features on theentire history of chosen records, which enables it to choose a better sequence of records.sary.
Interestingly, OURSYSTEM even outperformsHUMAN on semantic correctness, perhaps due togenerating more straightforward renderings of theworld state.
Figure 8 describes example outputs foreach system.6 Related WorkThere has been a fair amount of work both on con-tent selection and surface realization.
In content se-lection, Barzilay and Lee (2004) use an approachbased on local classification with edge-wise scoresbetween local decisions.
Our model, on the otherhand, can capture higher-order constraints to enforceglobal coherence.Liang et al (2009) introduces a generative modelof the text given the world state, and in some ways issimilar in spirit to our model.
Although that modelis capable of generation in principle, it was de-signed for unsupervised induction of hidden align-ments (which is exactly what we use it for).
Evenif combined with a language model, generated textwas much worse than our baseline.The prominent approach for surface realizationis rendering the text from a grammar.
Wong andMooney (2007) and Chen and Mooney (2008) usesynchronous grammars that map a logical form, rep-resented as a tree, into a parse of the text.
Soricutand Marcu (2006) uses tree structures called WIDL-expressions (the acronym corresponds to four opera-tions akin to the rewrite rules of a grammar) to repre-sent the realization process, and, like our approach,operates in a log-linear framework.
Belz (2008) andBelz and Kow (2009) also perform surface realiza-tion from a PCFG-like grammar.
Lu et al (2009)uses a conditional random field model over trees.Other authors have performed surface realization us-ing various grammar formalisms, for instance CCG(White et al, 2007), HPSG (Nakanishi et al, 2005),and LFG (Cahill and van Genabith, 2006).In each of the above cases, the decomposablestructure of the tree/grammar enables tractability.However, we saw that it was important to includefeatures that captured long-range dependencies.
Ourmodel is also similar in spirit to Ratnaparkhi (2002)in the use of non-local features, but we operate atthree levels of hierarchy to include both content se-lection and surface realization.One issue that arises with long-range dependen-cies is the lack of efficient algorithms for finding theoptimal text.
Koller and Striegnitz (2002) performsurface realization of a flat semantics, which is NP-hard, so they recast the problem as non-projectivedependency parsing.
Ratnaparkhi (2002) uses beamsearch to find an approximate solution.
We foundthat a greedy approach obtained better results thanbeam search; Belz (2008) found greedy approachesto be effective as well.7 ConclusionWe have developed a simple yet powerful generationsystem that combines both content selection and sur-face realization in a domain independent way.
De-spite our approach being domain-independent, wewere able to obtain performance comparable to thestate-of-the-art across three domains.
Additionally,the feature-based design of our approach makes iteasy to incorporate domain-specific knowledge toincrease performance even further.511ReferencesR.
Barzilay and L. Lee.
2004.
Catching the drift: Prob-abilistic content models, with applications to genera-tion and summarization.
In Human Language Tech-nology and North American Association for Computa-tional Linguistics (HLT/NAACL).A.
Belz and E. Kow.
2009.
System building cost vs.output quality in data-to-text generation.
In EuropeanWorkshop on Natural Language Generation, pages16?24.A.
Belz.
2008.
Automatic generation of weather forecasttexts using comprehensive probabilistic generation-space models.
Natural Language Engineering,14(4):1?26.Aoife Cahill and Josef van Genabith.
2006.
Robust pcfg-based generation using automatically acquired LFGapproximations.
In Association for ComputationalLinguistics (ACL), pages 1033?1040, Morristown, NJ,USA.
Association for Computational Linguistics.D.
L. Chen and R. J. Mooney.
2008.
Learning tosportscast: A test of grounded language acquisition.In International Conference on Machine Learning(ICML), pages 128?135.R.
Dale, S. Geldof, and J. Prost.
2003.
Coral: using natu-ral language generation for navigational assistance.
InAustralasian computer science conference, pages 35?44.M.
E. Foster and M. White.
2004.
Techniques for textplanning with XSLT.
In Workshop on NLP and XML:RDF/RDFS and OWL in Language Technology, pages1?8.N.
Green.
2006.
Generation of biomedical arguments forlay readers.
In International Natural Language Gen-eration Conference, pages 114?121.A.
Koller and K. Striegnitz.
2002.
Generation as de-pendency parsing.
In Association for ComputationalLinguistics (ACL), pages 17?24.P.
Liang, M. I. Jordan, and D. Klein.
2009.
Learningsemantic correspondences with less supervision.
InAssociation for Computational Linguistics and Inter-national Joint Conference on Natural Language Pro-cessing (ACL-IJCNLP).D.
C. Liu and J. Nocedal.
1989.
On the limited mem-ory method for large scale optimization.
Mathemati-cal Programming B, 45(3):503?528.W.
Lu, H. T. Ng, and W. S. Lee.
2009.
Natural lan-guage generation with tree conditional random fields.In Empirical Methods in Natural Language Process-ing (EMNLP), pages 400?409.Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic models for disambiguation of anHPSG-based chart generator.
In Parsing ?05: Pro-ceedings of the Ninth International Workshop on Pars-ing Technology, pages 93?102, Morristown, NJ, USA.Association for Computational Linguistics.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: A method for automatic evaluation of machinetranslation.
In Association for Computational Linguis-tics (ACL).A.
Ratnaparkhi.
1998.
Maximum entropy models for nat-ural language ambiguity resolution.
Ph.D. thesis, Uni-versity of Pennsylvania.A.
Ratnaparkhi.
2002.
Trainable approaches to surfacenatural language generation and their application toconversational dialog systems.
Computer, Speech &Language, 16:435?455.E.
Reiter, S. Sripada, J.
Hunter, J. Yu, and I. Davy.
2005.Choosing words in computer-generated weather fore-casts.
Artificial Intelligence, 167:137?169.R.
Soricut and D. Marcu.
2006.
Stochastic languagegeneration using WIDL-expressions and its applica-tion in machine translation and summarization.
In As-sociation for Computational Linguistics (ACL), pages1105?1112.R.
Turner, Y. Sripada, and E. Reiter.
2009.
Gener-ating approximate geographic descriptions.
In Eu-ropean Workshop on Natural Language Generation,pages 42?49.Michael White, Rajakrishnan Rajkumar, and Scott Mar-tin.
2007.
Towards broad coverage surface realizationwith CCG.
In In Proceedings of the Workshop on Us-ing Corpora for NLG: Language Generation and Ma-chine Translation (UCNLG+MT).Y.
W. Wong and R. J. Mooney.
2007.
Learning syn-chronous grammars for semantic parsing with lambdacalculus.
In Association for Computational Linguis-tics (ACL), pages 960?967.512
