Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 88?98,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsAnswer Sentence Retrieval by Matching Dependency PathsAcquired from Question/Answer Sentence PairsMichael KaisserAGT Group (R&D) GmbHJa?gerstr.
41, 10117 Berlin, Germanymkaisser@agtgermany.comAbstractIn Information Retrieval (IR) in generaland Question Answering (QA) in particu-lar, queries and relevant textual content of-ten significantly differ in their propertiesand are therefore difficult to relate with tra-ditional IR methods, e.g.
key-word match-ing.
In this paper we describe an algorithmthat addresses this problem, but rather thanlooking at it on a term matching/term re-formulation level, we focus on the syntac-tic differences between questions and rele-vant text passages.
To this end we proposea novel algorithm that analyzes dependencystructures of queries and known relevanttext passages and acquires transformationalpatterns that can be used to retrieve rele-vant textual content.
We evaluate our algo-rithm in a QA setting, and show that it out-performs a baseline that uses only depen-dency information contained in the ques-tions by 300% and that it also improves per-formance of a state of the art QA systemsignificantly.1 IntroductionIt is a well known problem in Information Re-trieval (IR) and Question Answering (QA) thatqueries and relevant textual content often signif-icantly differ in their properties, and are thereforedifficult to match with traditional IR methods.
Acommon example is a user entering words to de-scribe their information need that do not matchthe words used in the most relevant indexed doc-uments.
This work addresses this problem, butshifts focus from words to syntactic structures ofquestions and relevant pieces of text.
To this end,we present a novel algorithm that analyses the de-pendency structures of known valid answer sen-tence and from these acquires patterns that can beused to more precisely retrieve relevant text pas-sages from the underlying document collection.To achieve this, the position of key phrases in theanswer sentence relative to the answer itself is an-alyzed and linked to a certain syntactic questiontype.
Unlike most previous work that uses depen-dency paths for QA (see Section 2), our approachdoes not require a candidate sentence to be similarto the question in any respect.
We learn valid de-pendency structures from the known answer sen-tences alone, and therefore are able to link a muchwider spectrum of answer sentences to the ques-tion.The work in this paper is presented and eval-uated in a classical factoid Question Answering(QA) setting.
The main reason for this is thatin QA suitable training and test data is availablein the public domain, e.g.
via the Text REtrievalConference (TREC), see for example (Voorhees,1999).
The methods described in this paper how-ever can also be applied to other IR scenarios, e.g.web search.
The necessary condition for our ap-proach to work is that the user query is somewhatgrammatically well formed; this kind of queriesare commonly referred to as Natural LanguageQueries or NLQs.Table 1 provides evidence that users indeedsearch the web with NLQs.
The data is based ontwo query sets sampled from three months of userlogs from a popular search engine, using two dif-ferent sampling techniques.
The ?head?
set sam-ples queries taking query frequency into account,so that more common queries have a proportion-ally higher chance of being selected.
The ?tail?query set samples only queries that have been is-88Set Head TailQuery # 15,665 12,500how 1.33% 2.42%what 0.77% 1.89%define 0.34% 0.18%is/are 0.25% 0.42%where 0.18% 0.45%do/does 0.14% 0.30%can 0.14% 0.25%why 0.13% 0.30%who 0.12% 0.38%when 0.09% 0.21%which 0.03% 0.08%Total 3.55% 6.86%Table 1: Percentages of Natural Language queries inhead and tail search engine query logs.
See text fordetails.sued less that 500 times during a three months pe-riod and it disregards query frequency.
As a result,rare and frequent queries have the same chance ofbeing selected.
Doubles are excluded from bothsets.
Table 1 lists the percentage of queries inthe query sets that start with the specified word.In most contexts this indicates that the query is aquestion, which in turn means that we are dealingwith an NLQ.
Of course there are many NLQs thatstart with words other than the ones listed, so wecan expect their real percentage to be even higher.2 Related WorkIn IR the problem that queries and relevant tex-tual content often do not exhibit the same terms iscommonly encountered.
Latent Semantic Index-ing (Deerwester et al 1900) was an early, highlyinfluential approach to solve this problem.
Morerecently, a significant amount of research is ded-icated to query alteration approaches.
(Cui et al2002), for example, assume that if queries con-taining one term often result in the selection ofdocuments containing another term, then a strongrelationship between the two terms exist.
In theirapproach, query terms and document terms arelinked via sessions in which users click on doc-uments that are presented as results for the query.
(Riezler and Liu, 2010) apply a Statistical Ma-chine Translation model to parallel data consist-ing of user queries and snippets from clicked webdocuments and in such a way extract contextualexpansion terms from the query rewrites.We see our work as addressing the same fun-damental problem, but shifting focus from queryterm/document term mismatch to mismatches ob-served between the grammatical structure of Nat-ural Language Queries and relevant text pieces.
Inorder to achieve this we analyze the queries?
andthe relevant contents?
syntactic structure by usingdependency paths.Especially in QA there is a strong traditionof using dependency structures: (Lin and Pan-tel, 2001) present an unsupervised algorithm toautomatically discover inference rules (essentiallyparaphrases) from text.
These inference rules arebased on dependency paths, each of which con-nects two nouns.
Their paths have the followingform:N:subj:V?find?V:obj:N?solution?N:to:NThis path represents the relation ?X finds a solu-tion to Y?
and can be mapped to another path rep-resenting e.g.
?X solves Y.?
As such the approachis suitable to detect paraphrases that describe therelation between two entities in documents.
How-ever, the paper does not describe how the minedparaphrases can be linked to questions, and whichparaphrase is suitable to answer which questiontype.
(Attardi et al 2001) describes a QA systemthat, after a set of candidate answer sentenceshave been identified, matches their dependencyrelations against the question.
Questions andanswer sentences are parsed with MiniPar (Lin,1998) and the dependency output is analyzed inorder to determine whether relations present in aquestion also appear in a candidate sentence.
Forthe question ?Who killed John F.
Kennedy?, forexample an answer sentence is expected to con-tain the answer as subject of the verb ?kill?, towhich ?John F. Kennedy?
should be in object re-lation.
(Cui et al 2005) describe a fuzzy depen-dency relation matching approach to passage re-trieval in QA.
Here, the authors present a statis-tical technique to measure the degree of overlapbetween dependency relations in candidate sen-tences with their corresponding relations in thequestion.
Question/answer passage pairs fromTREC-8 and TREC-9 evaluations are used astraining data.
As in some of the papers mentionedearlier, a statistical translation model is used, butthis time to learn relatedness between paths.
(Cuiet al 2004) apply the same idea to answer ex-89traction.
In each sentences returned by the IRmodule, all named entities of the expected answertypes are treated as answer candidates.
For ques-tions with an unknown answer type, all NPs inthe candidate sentence are considered.
Then thosepaths in the answer sentence that are connectedto an answer candidate are compared against thecorresponding paths in the question, in a similarfashion as in (Cui et al 2005).
The candidatewhose paths show the highest matching score isselected.
(Shen and Klakow, 2006) also describea method that is primarily based on similarityscores between dependency relation pairs.
How-ever, their algorithm computes the similarity ofpaths between key phrases, not between words.Furthermore, it takes relations in a path not as in-dependent from each other, but acknowledges thatthey form a sequence, by comparing two pathswith the help of an adaptation of the DynamicTime Warping algorithm (Rabiner et al 1991).
(Molla, 2006) presents an approach for the ac-quisition of question answering rules by apply-ing graph manipulation methods.
Questions arerepresented as dependency graphs, which are ex-tended with information from answer sentences.These combined graphs can then be used to iden-tify answers.
Finally, in (Wang et al 2007), aquasi-synchronous grammar (Smith and Eisner,2006) is used to model relations between ques-tions and answer sentences.In this paper we describe an algorithm thatlearns possible syntactic answer sentence formu-lations for syntactic question classes from a set ofexample question/answer sentence pairs.
Unlikethe related work described above, it acknowledgesthat a) a valid answer sentence?s syntax mightbe very different for the question?s syntax and b)several valid answer sentence structures, whichmight be completely independent from each other,can exist for one and the same question.To illustrate this consider the question ?Whenwas Alaska purchased??
The following four sen-tences all answer the given question, but only thefirst sentence is a straightforward reformulation ofthe question:1.
The United States purchased Alaska in 1867from Russia.2.
Alaska was bought from Russia in 1867.3.
In 1867, the Russian Empire sold the Alaskaterritory to the USA.4.
The acquisition of Alaska by the UnitedStates of America from Russia in 1867 isknown as ?Seward?s Folly?.The remaining three sentences introduce vari-ous forms of syntactic and semantic transforma-tions.
In order to capture a wide range of possibleways on how answer sentences can be formulated,in our model a candidate sentence is not evalu-ated according to its similarity with the question.Instead, its similarity to known answer sentences(which were presented to the system during train-ing) is evaluated.
This allows to us to capture amuch wider range of syntactic and semantic trans-formations.3 Overview of the AlgorithmOur algorithm uses input data containing pairs ofthe following:NLQs/Questions NLQs that describe the users?information need.
For the experiments car-ried out in this paper we use questions fromthe TREC QA track 2002-2006.Relevant textual content This is a piece of textthat is relevant to the user query in that itcontains the information the user is search-ing for.
In this paper, we use sentences ex-tracted from the AQUAINT corpus (Graff,2002) that contain the answer to the givenTREC question.In total, the data available to us for our experi-ments consists of 8,830 question/answer sentencepairs.
This data is publicly available, see (Kaisserand Lowe, 2008).
The algorithm described in thispaper has three main steps:Phrase alignment Key phrases from the ques-tion are paired with phrases from the answersentences.Pattern creation The dependency structures ofqueries and answer sentences are analyzedand patterns are extracted.Pattern evaluation The patterns discovered inthe last step are evaluated and a confidencescore is assigned to each.The acquired patterns can then be used duringretrieval, where a question is matched against theantecedents describing the syntax of the question.90Input: (a) Query: ?When was Alaska purchased??
(b) Answer sentence: ?The acquisition of Alaska happened in 1867.?Step 1: Question is segmented into key phrases and stop words:When[1]+was[2]+NP[3]+VERB[4]Step 2: Key question phrases are aligned with key answer sentence phrases:[3]Alaska ?
Alaska[4]purchased ?
acquisitionANSWER ?
1867Step 3: A pre-computed parse tree of the answer sentence is loaded:1: The (the, DT, 2) [det]2: acquisition (acquisition, NN, 5) [nsubj]3: of (of, IN, 2) [prep]4: Alaska (Alaska, IN, 2) [pobj]5: happened (happen, VBD, null) [ROOT]6: in (in, IN, 5) [prep]7: 1867 (1867, CD, 6) [pobj]Step 4: Dependency paths from key question phrases to the answer are computed:Alaska?1867: ?pobj?prep?nsubj?prep?pobjacquisition?1867: ?nsubj?prep?pobjStep 5: The resulting pattern is stored:Query: When[1]+was[2]+NP[3]+VERB[4]Path 3: ?pobj?prep?nsubj?prep?pobjPath 4: ?nsubj?prep?pobjFigure 1: The pattern creation algorithm exemplified in five key steps for the query ?When was Alaska pur-chased??
and the answer sentence ?The acquisition of Alaska happened in 1867.?Note that one question can potentially match sev-eral patterns.
The consequents contain descrip-tions of grammatical structures of potential an-swer sentences that can be used to identify andevaluate candidate sentences.4 Phrase AlignmentThe goal of this processing step is to align phrasesfrom the question with corresponding phrasesfrom the answer sentences in the training data.Consider the following example:Query: ?When was the Alaska territory pur-chased?
?Answer sentence: ?The acquisition of whatwould become the territory of Alaska took placein 1867.?The mapping that has to be achieved is:Query Answer Sentencephrase phrase?Alaska territory?
?territory of Alaska??purchased?
?acquisition?ANSWER ?1867?In our approach, this is a two step process.First we align on a word level, then the outputof the word alignment process is used to iden-tify and align phrases.
Word Alignment is im-portant in many fields of NLP, e.g.
MachineTranslation (MT) where words in parallel, bilin-gual corpora need to be aligned, see (Och andNey, 2003) for a comparison of various statisti-cal alignment models.
In our case however weare dealing with a monolingual alignment prob-lem which enables us to exploit clues not availablefor bilingual alignment: First of all, we can expectmany query words to be present in the answer sen-tence, either with the exact same surface appear-ance or in some morphological variant.
Secondly,there are tools available that tell us how semanti-cally related two words are, most notably Word-Net (Miller et al 1993).
For these reasons we im-plemented a bespoke alignment strategy, tailoredtowards our problem description.This method is described in detail in (Kaisser,2009).
The processing steps described in thenext sections build on its output.
For reasons ofbrevity, we skip a detailed explanations in this pa-per and focus only on its key part: the alignmentof words with very different surface structures.For more details we would like to point the readerto the aforementioned work.In the above example, the alignment of ?pur-91chased?
and ?acquisition?
is the most problem-atic, because the surface structures of the twowords clearly are very different.
For such caseswe experimented with a number of alignmentstrategies based on WordNet.
These approachesare similar in that each picks one word that has tobe aligned from the question at a time and com-pares it to all of the non-stop words in the answersentence.
Each of the answer sentence words isassigned a value between zero and one express-ing its relatedness to the question word.
Thehighest scoring word, if above a certain thresh-old, is selected as the closest semantic match.Most of these approaches make use of Word-Net::Similarity, a Perl software package that mea-sures semantic similarity (or relatedness) betweena pair of word senses by returning a numeric valuethat represents the degree to which they are sim-ilar or related (Pedersen et al 2004).
Addition-ally, we developed a custom-built method that as-sumes that two words are semantically related ifany kind of pointer exists between any occurrenceof the words root form in WordNet.
For details ofthese experiments, please refer to (Kaisser, 2009).In our experiments the custom-built method per-formed best, and was therefore used for the exper-iments described in this paper.
The main reasonsfor this are:1.
Many of the measures in the Word-Net::Similarity package take only hyponym/hypernym relations into account.
This makesaligning word of different parts of speechdifficult or even impossible.
However, suchalignments are important for our needs.2.
Many of the measures return results, even ifonly a weak semantic relationship exists.
Forour purposes however, it is beneficial to onlytake strong semantic relations into account.5 Pattern CreationFigure 1 details our algorithm in its five key steps.In step 1 and 2 key phrases from the question arealigned to the corresponding phrases in the an-swer sentence, see Section 4 of this paper.
Step3 is concerned with retrieving the parse tree forthe answer sentence.
In our implementation allanswer sentences in the training set have for per-formance reasons been parsed beforehand withthe Stanford Parser (Klein and Manning, 2003b;Klein and Manning, 2003a), so at this point theyare simply loaded from file.
Step 4 is the key stepin our algorithm.
From the previous steps, weknow where the key constituents from the ques-tion as well as the answer are located in the an-swer sentence.
This enables us to compute thedependency paths in the answer sentences?
parsetree that connect the answer with the key con-stituents.
In our example the answer is ?1867?and the key constituents are ?acquisition?
and?Alaska.?
Knowing the syntactic relationships(captured by their dependency paths) between theanswer and the key phrases enables us to captureone syntactic possibility of how answer sentencesto queries of the form When+was+NP+VERB canbe formulated.As can be seen in Step 5 a flat syntactic ques-tion representation is stored, together with num-bers assigned to each constituent.
The num-bers for those constituents for which alignmentsin the answer sentence were sought and foundare listed together with the resulting dependencypaths.
Path 3 for example denotes the path fromconstituent 3 (the NP ?Alaska?)
to the answer.
Ifno alignment could be found for a constituent,null is stored instead of a path.
Should two ormore alternative constituents be identified for onequestion constituent, additional patterns are cre-ated, so that each contains one of the possibilities.The described procedure is repeated for all ques-tion/answer sentence pairs in the training set andfor each, one or more patterns are created.It is worth to note that many TREC ques-tions are fairly short and grammatically sim-ple.
In our training data we for exam-ple find 102 questions matching the patternWhen[1]+was[2]+NP[3]+VERB[4], whichtogether list 382 answer sentences, and thus 382potentially different answer sentence structuresfrom which patterns can be gained.
As a result,the amount of training examples we have avail-able, is sufficient to achieve the performance de-scribed in Section 7.
The algorithm described inthis paper can of course also be used for morecomplicated NLQs, although in such a scenario asignificantly larger amount of training data wouldhave to be used.6 Pattern EvaluationFor each created pattern, at least one match-ing example must exists: the sentence that was92used to create it in the first place.
However, wedo not know how precise each pattern is.
Tothis end, an additional processing step betweenpattern creation and application is needed: pat-tern evaluation.
Similar approaches to ours havebeen described in the relevant literature, manyof them concerned with bootstrapping, startingwith (Ravichandran and Hovy, 2002).
The gen-eral purpose of this step is to use the availabledata about questions and their correct answers toevaluate how often each created pattern returns acorrect or an incorrect result.
This data is storedwith each pattern and the result of the equation,often called pattern precision, can be used duringretrieval stage.
Pattern precision in our case is de-fined as:p =#correct + 1#correct + #incorrect + 2(1)We use Lucene to retrieve the top 100 para-graphs from the AQUAINT corpus by issuing aquery that consists of the query?s key words andall non-stop words in the answer.
Then, all pat-terns are loaded whose antecedent matches thequery that is currently being processed.
After that,constituents from all sentences in the retrieved100 paragraphs are aligned to the query?s con-stituents in the same way as for the sentences dur-ing pattern creation, see Section 5.
Now, the pathsspecified in these patterns are searched for in theparagraphs?
parse trees.
If they are all found,it is checked whether they all point to the samenode and whether this node?s surface structure isin some morphological form present in the answerstrings associated with the question in our train-ing data.
If this is the case a variable in the pat-tern named correct is increased by 1, otherwisethe variable incorrect is increased by 1.
After theevaluation process is finished the final version ofthe pattern given as an example in Figure 1 nowis:Query: When[1]+was[2]+NP[3]+VERB[4]Path 3: ?pobj?prep?nsubj?prep?pobjPath 4: ?nsubj?prep?pobjCorrect: 15Incorrect: 4The variables correct and incorrect are usedduring retrieval, where the score of an answer can-didate ac is the sum of all scores of all matchingpatterns p:score(ac) =n?i=1score(pi) (2)wherescore(pi) ={correcti+1correcti+incorrecti+2if match0 no match(3)The highest scoring candidate is selected.We would like to explicitly call out one prop-erty of our algorithm: It effectively returns twoentities: a) a sentence that constitutes a validresponse to the query, b) the head node of aphrase in that sentence that constitutes the answer.Therefore the algorithm can be used for sentenceretrieval or for answer retrieval.
It depends onthe application which of the two behaviors is de-sired.
In the next section, we evaluate its answerretrieval performance.7 Experiments & ResultsThis section provides an evaluation of the algo-rithm described in this paper.
The key questionswe seek to answer are the following:1.
How does our method perform when com-pared to a baseline that extracts dependencypaths from the question?2.
How much does the described algorithm im-prove performance of a state-of-the-art QAsystem?3.
What is the effect of training data size on per-formance?
Can we expect that more trainingdata would further improve the algorithm?sperformance?7.1 Evaluation SetupWe use all factoid questions in TREC?s QA testsets from 2002 to 2006 for evaluation for whicha known answer exists in the AQUAINT corpus.Additionally, the data in (Lin and Katz, 2005) isused.
In this paper the authors attempt to identifya much more complete set of relevant documentsfor a subset of TREC 2002 questions than TRECitself.
We adopt a cross validation approach forour evaluation.
Table 4 shows how the data is splitinto five folds.In order to evaluate the algorithm?s patterns weneed a set of sentences to which they can be ap-plied.
In a traditional QA system architecture,93Test Number of Correct Answer SentencesMean Medset = 0 <= 1 <= 3 <= 5 <= 10 <= 25 <= 50 >= 75 >= 90 >= 1002002 0.203 0.396 0.580 0.671 0.809 0.935 0.984 0.0 0.0 0.0 6.86 2.02003 0.249 0.429 0.627 0.732 0.828 0.955 0.997 0.003 0.003 0.0 5.67 2.02004 0.221 0.368 0.539 0.637 0.799 0.936 0.985 0.0 0.0 0.0 6.51 3.02005 0.245 0.404 0.574 0.665 0.777 0.912 0.987 0.0 0.0 0.0 7.56 2.02006 0.241 0.389 0.568 0.665 0.807 0.920 0.966 0.006 0.0 0.0 8.04 3.0Table 2: Fraction of sentences that contain correct answers in Evaluation Set 1 (approximation).Test Number of Correct Answer SentencesMean Medset = 0 <= 1 <= 3 <= 5 <= 10 <= 25 <= 50 >= 75 >= 90 >= 1002002 0.0 0.074 0.158 0.235 0.342 0.561 0.748 0.172 0.116 0.060 33.46 21.02003 0.0 0.099 0.203 0.254 0.356 0.573 0.720 0.161 0.090 0.031 32.88 19.02004 0.0 0.073 0.137 0.211 0.328 0.598 0.779 0.142 0.069 0.034 30.82 20.02005 0.0 0.163 0.238 0.279 0.410 0.589 0.759 0.141 0.097 0.069 30.87 17.02006 0.0 0.125 0.207 0.281 0.415 0.596 0.727 0.173 0.122 0.088 32.93 17.5Table 3: Fraction of sentences that contain correct answers in Evaluation Set 2 (approximation).FoldTraining Data Test Datasets used # set #1 T03, T04, T05, T06 4565 T02 11592 T02, T04, T05, T06, Lin02 6174 T03 13523 T02, T03, T05, T06, Lin02 6700 T04 8264 T02, T03, T04, T06, Lin02 6298 T05 12285 T02, T03, T04, T05, Lin02 6367 T06 1159Table 4: Splits into training and tests sets of the dataused for evaluation.
T02 stands for TREC 2002 dataetc.
Lin02 is based on (Lin and Katz, 2005).
The #rows show how many question/answer sentence pairsare used for training and for testing.see e.g.
(Prager, 2006; Voorhees, 2003), the docu-ment or passage retrieval step performs this func-tion.
This step is crucial to a QA system?s per-formance, because it is impossible to locate an-swers in the subsequent answer extraction step ifthe passages returned during passage retrieval donot contain the answer in the first place.
This alsoholds true in our case: the patterns cannot be ex-pected to identify a correct answer if none of thesentences used as input contains the correct an-swer.
We therefore use two different evaluationsets to evaluate our algorithm:1.
The first set contains for each question allsentences in the top 100 paragraphs returnedby Lucene when using simple queries madeup from the question?s key words.
It cannotbe guaranteed that answers to every questionare present in this test set.2.
For the second set, the query additionally listall known correct answers to the question asparts of one OR operator.
This increases thechance that the evaluation set actually con-tains valid answer sentences significantly.In order to provide a quantitative characteriza-tion of the two evaluation sets we estimated thenumber of correct answer sentences they contain.For each paragraph it was determined whether itcontained one of the known answer strings andat least of one of the question key words.
Ta-bles 2 and 3 show for each evaluation set howmany answers on average it contains per ques-tion.
The column ?= 0?
for example shows thefraction of questions for which no valid answersentence is contained in the evaluation set, whilecolumn ?>= 90?
gives the fraction of questionswith 90 or more valid answer sentences.
The lasttwo columns show mean and median values.7.2 Comparison with BaselineAs pointed out in Section 2 there is a strong tra-dition of using dependency paths in QA.
Manyrelevant papers describe algorithms that analyzea question?s grammatical structure and expectto find a similar structure in valid answer sen-tences, e.g.
(Attardi et al 2001), (Cui et al 2005)or (Bouma et al 2005) to name just a few.
Asalready pointed out, a major contribution of ourwork is that we do not assume this similarity.
Inour approach valid answer sentences are allowedto have grammatical structures that are very dif-ferent from the question and also very differentfrom each other.
Thus it is natural to compare ourapproach against a baseline that compares can-didate sentences not against patterns that weregained from question/answer sentence pairs, butfrom questions alone.
In order to create these pat-terns, we use a small trick: During the PatternCreation step, see Section 5 and Figure 1, we re-94place the answer sentences in the input file withthe questions, and assume that the question wordindicates the position where the answer should belocated.Test Q Qs with > 1 Overall Accuracy Acc.
ifset number patterns correct correct overall pattern2002 429 321 147 50 0.117 0.1562003 354 237 76 22 0.062 0.0932004 204 142 74 26 0.127 0.1832005 319 214 97 46 0.144 0.2152006 352 208 85 31 0.088 0.149Sum 1658 1122 452 176 0.106 0.156Table 5: Performance based on evaluation set 1.Test Q Qs with > 1 Overall Accuracy Acc.
ifset number patterns correct correct overall pattern2002 429 321 239 133 0.310 0.4142003 354 237 149 88 0.248 0.3712004 204 142 119 65 0.319 0.4582005 319 214 161 92 0.288 0.4292006 352 208 139 84 0.238 0.403Sum 1658 1122 807 462 0.278 0.411Table 6: Performance based on evaluation set 2.Tables 5 and 6 show how our algorithm per-forms on evaluation sets 1 and 2, respectively.
Ta-bles 7 and 8 show how the baseline performs onevaluation sets 1 and 2, respectively.
The tables?columns list the year of the TREC test set used,the number of questions in the set (we only usequestions for which we know that there is an an-swer in the corpus), the number of questions forwhich one or more patterns exist, how often atleast one pattern returned the correct answer, howoften we get an overall correct result by takingall patterns and their confidence values into ac-count, accuracy@1 of the overall system, and ac-curacy@1 computed only for those questions forwhich we have at least one pattern available (forall other questions the system returns no result.
)As can be seen, on evaluation set 1 our methodoutperforms the baseline by 300%, on evaluationset 2 by 311%, taking accuracy if a pattern existsas a basis.Test Q Qs with Min one Overall Accuracy Acc.
ifset number patterns correct correct overall pattern2002 429 321 43 14 0.033 0.0442003 354 237 28 10 0.028 0.0422004 204 142 19 6 0.029 0.0422005 319 214 21 7 0.022 0.0332006 352 208 20 7 0.020 0.034Sum 1658 1122 131 44 0.027 0.039Table 7: Baseline performance based on evaluation set1.Many of the papers cited earlier that use an ap-proach similar to our baseline approach of coursereport much better results than Tables 7 and 8.This however is not too surprising as the approachTest Q Qs with Min one Overall Accuracy Acc.
ifset number patterns correct correct overall pattern2002 429 321 77 37 0.086 0.1152003 354 237 39 26 0.073 0.1202004 204 142 25 15 0.074 0.0732005 319 214 38 18 0.056 0.0842006 352 208 34 16 0.045 0.077Sum 1658 1122 213 112 0.068 0.100Table 8: Baseline performance based on evaluation set2.described in this paper and the baseline approachdo not make use of many techniques commonlyused to increase performance of a QA system, e.g.TF-IDF fallback strategies, fuzzy matching, man-ual reformulation patterns etc.
It was a deliberatedecision from our side not to use any of these ap-proaches.
After all, this would result in an ex-perimental setup where the performance of ouranswer extraction strategy could not have beenobserved in isolation.
The QA system used as abaseline in the next section makes use of many ofthese techniques and we will see that our method,as described here, is suitable to increase its per-formance significantly.7.3 Impact on an existing QA SystemTables 9 and 10 show how our algorithm in-creases performance of our QuALiM system, seee.g.
(Kaisser et al 2006).
Section 6 in this pa-per describes via formulas 2 and 3 how answercandidates are ranked.
This ranking is combinedwith the existing QA system?s candidate rankingby simply using it as an additional feature thatboosts candidates proportionally to their confi-dence score.
The difference between both tablesis that the first uses all 1658 questions in our testsets for the evaluation, whereas the second con-siders only those 1122 questions for which oursystem was able to learn a pattern.
Thus for Table10 questions which the system had no chance ofanswering due to limited training data are omitted.As can be seen, accuracy@1 increases by 4.9% onthe complete test set and by 11.5% on the partialset.Note that the QA system used as a baseline isat an advantage in at least two respects: a) It hasimportant web-based components and as such hasaccess to a much larger body of textual informa-tion.
b) The algorithm described in this paper is ananswer extraction approach only.
For paragraphretrieval we use the same approach as for evalu-ation set 1, see Section 7.1.
However, in morethan 20% of the cases, this method returns not95a single paragraph that contains both the answerand at least one question keyword.
In such cases,the simple paragraph retrieval makes it close toimpossible for our algorithm to return the correctanswer.Test Set QuALiM QASP combined increase2002 0.503 0.117 0.524 4.2%2003 0.367 0.062 0.390 6.2%2004 0.426 0.127 0.451 5.7%2005 0.373 0.144 0.389 4.2%2006 0.341 0.088 0.358 5.0%02-06 0.405 0.106 0.425 4.9%Table 9: Top-1 accuracy of the QuALiM system on itsown and when combined with the algorithm describedin this paper.
All increases are statistically significantusing a sign test (p < 0.05).Test Set QuALiM QASP combined increase2002 0.530 0.156 0.595 12.3%2003 0.380 0.093 0.430 13.3%2004 0.465 0.183 0.514 10.6%2005 0.388 0.214 0.421 8.4%2006 0.385 0.149 0.428 11.3%02-06 0.436 0.157 0.486 11.5%Table 10: Top-1 accuracy of the QuALiM system onits own and when combined with the algorithm de-scribed in this paper, when only considering questionsfor which a pattern could be acquired from the trainingdata.
All increases are statistically significant using asign test (p < 0.05).7.4 Effect of Training Data SizeWe now assess the effect of training data size onperformance.
Tables 5 and 6 presented earliershow that an average of 32.2% of the questionshave no matching patterns.
This is because thedata used for training contained no examples for asignificant subset of question classes.
It can be ex-pected that, if more training data would be avail-able, this percentage would decrease and perfor-mance would increase.
In order to test this as-sumption, we repeated the evaluation proceduredetailed in this section several times, initially us-ing data from only one TREC test set for train-ing and then gradually adding more sets until allavailable training data had been used.
The resultsfor evaluation set 2 are presented in Figure 2.
Ascan be seen, every time more data is added, per-formance increases.
This strongly suggests thatthe point of diminishing returns, when adding ad-ditional training data no longer improves perfor-mance is not yet reached.Figure 2: Effect of the amount of training data on sys-tem performance8 ConclusionsIn this paper we present an algorithm that acquiressyntactic information about how relevant textualcontent to a question can be formulated from acollection of paired questions and answer sen-tences.
Other than previous work employing de-pendency paths for QA, our approach does not as-sume that a valid answer sentence is similar to thequestion and it allows many potentially very dif-ferent syntactic answer sentence structures.
Thealgorithm is evaluated using TREC data, and itis shown that it outperforms an algorithm thatmerely uses the syntactic information containedin the question itself by 300%.
It is also shownthat the algorithm improves the performance of astate-of-the-art QA system significantly.As always, there are many ways how we couldimagine our algorithm to be improved.
Combin-ing it with fuzzy matching techniques as in (Cui etal., 2004) or (Cui et al 2005) is an obvious direc-tion for future work.
We are also aware that in or-der to apply our algorithm on a larger scale and ina real world setting with real users, we would needa much larger set of training data.
These couldbe acquired semi-manually, for example by usingcrowd-sourcing techniques.
We are also thinkingabout fully automated approaches, or about us-ing indirect human evidence, e.g.
user clicks insearch engine logs.
Typically users only see thetitle and a short abstract of the document whenclicking on a result, so it is possible to imagine ascenario where a subset of these abstracts, pairedwith user queries, could serve as training data.96ReferencesGiuseppe Attardi, Antonio Cisternino, FrancescoFormica, Maria Simi, and Alessandro Tommasi.2001.
PIQASso: Pisa Question Answering System.In Proceedings of the 2001 Edition of the Text RE-trieval Conference (TREC-01).Gosse Bouma, Jori Mur, and Gertjan van Noord.
2005.Reasoning over Dependency Relations for QA.
InProceedings of the IJCAI workshop on Knowledgeand Reasoning for Answering Questions (KRAQ-05).Hang Cui, Ji-Rong Wen, Jian-Yun Nie, and Wei-YingMa.
2002.
Probabilistic query expansion usingquery logs.
In 11th International World Wide WebConference (WWW-02).Hang Cui, Keya Li, Renxu Sun, Tat-Seng Chua, andMin-Yen Kan. 2004.
National University of Sin-gapore at the TREC-13 Question Answering MainTask.
In Proceedings of the 2004 Edition of the TextREtrieval Conference (TREC-04).Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, andTat-Seng Chua.
2005.
Question Answering Pas-sage Retrieval Using Dependency Relations.
InProceedings of the 28th ACM-SIGIR InternationalConference on Research and Development in Infor-mation Retrieval (SIGIR-05).Scott Deerwester, Susan Dumais, George Furnas,Thomas Landauer, and Richard Harshman.
1900.Indexing by Latent Semantic Analysis.
Journal ofthe American society for information science, 41(6).David Graff.
2002.
The AQUAINT Corpus of EnglishNews Text.Michael Kaisser and John Lowe.
2008.
Creating aResearch Collection of Question Answer SentencePairs with Amazon?s Mechanical Turk.
In Proceed-ings of the Sixth International Conference on Lan-guage Resources and Evaluation (LREC-08).Michael Kaisser, Silke Scheible, and Bonnie Webber.2006.
Experiments at the University of Edinburghfor the TREC 2006 QA track.
In Proceedings ofthe 2006 Edition of the Text REtrieval Conference(TREC-06).Michael Kaisser.
2009.
Acquiring Syntactic andSemantic Transformations in Question Answering.Ph.D.
thesis, University of Edinburgh.Dan Klein and Christopher D. Manning.
2003a.
Ac-curate Unlexicalized Parsing.
In Proceedings of the41st Meeting of the Association for ComputationalLinguistics (ACL-03).Dan Klein and Christopher D. Manning.
2003b.
FastExact Inference with a Factored Model for NaturalLanguage Parsing.
In Advances in Neural Informa-tion Processing Systems 15.Jimmy Lin and Boris Katz.
2005.
Building a ReusableTest Collection for Question Answering.
Journal ofthe American Society for Information Science andTechnology (JASIST).Dekang Lin and Patrick Pantel.
2001.
Discovery ofInference Rules for Question-Answering.
NaturalLanguage Engineering, 7(4):343?360.Dekang Lin.
1998.
Dependency-based Evaluation ofMINIPAR.
In Workshop on the Evaluation of Pars-ing Systems.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine Miller.
1993.Introduction to WordNet: An On-Line LexicalDatabase.
Journal of Lexicography, 3(4):235?244.Diego Molla.
2006.
Learning of Graph-basedQuestion Answering Rules.
In Proceedings ofHLT/NAACL 2006 Workshop on Graph Algorithmsfor Natural Language Processing.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?52.Ted Pedersen, Siddharth Patwardhan, and JasonMichelizzi.
2004.
WordNet::Similarity - Measur-ing the Relatedness of Concepts.
In Proceedingsof the Nineteenth National Conference on ArtificialIntelligence (AAAI-04).John Prager.
2006.
Open-Domain Question-Answering.
Foundations and Trends in InformationRetrieval, 1(2).L.
R. Rabiner, A. E. Rosenberg, and S. E. Levin-son.
1991.
Considerations in Dynamic Time Warp-ing Algorithms for Discrete Word Recognition.
InProceedings of IEEE Transactions on Acoustics,Speech and Signal Processing.Deepak Ravichandran and Eduard Hovy.
2002.Learning Surface Text Patterns for a Question An-swering System.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Lin-guistics (ACL-02).Stefan Riezler and Yi Liu.
2010.
Query Rewritingusing Monolingual Statistical Machine Translation.Computational Linguistics, 36(3).Dan Shen and Dietrich Klakow.
2006.
Exploring Cor-relation of Dependency Relation Paths for AnswerExtraction.
In Proceedings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the ACL (COLING/ACL-06).David A. Smith and Jason Eisner.
2006.
Quasisyn-chronous grammars: Alignment by Soft Projec-tion of Syntactic Dependencies.
In Proceedings ofthe HLTNAACL Workshop on Statistical MachineTranslation.Ellen M. Voorhees.
1999.
Overview of the EighthText REtrieval Conference (TREC-8).
In Pro-ceedings of the Eighth Text REtrieval Conference(TREC-8).Ellen M. Voorhees.
2003.
Overview of the TREC2003 Question Answering Track.
In Proceedings ofthe 2003 Edition of the Text REtrieval Conference(TREC-03).97Mengqiu Wang, Noah A. Smith, and Teruko Mita-mura.
2007.
What is the Jeopardy model?
A Qua-sisynchronous Grammar for QA.
In Proceedings ofEMNLP-CoNLL 2007.98
