To Parse  or Not  to  Parse:  Re la t ion -Dr iven  Text  Sk immingPau l  S. JacobsArtificial Intell igence ProgramGE Research and Development CenterSchenectady, NY 12301 USApsj acobs~crd.ge.comAbst rac tWe have designed and implemented a textprocessing system that can extract impor-tant information from hundreds of para-graphs per hour and can be transportedwithin weeks to a new domain.
The sys-tem performs efficiently because it deter-mines the level of processing required to un-derstand a text.
This "skimming" methodidentifies urface relations in the input textthat are likely to contribute to its interpre-tation in a domain.
This approach differsfrom previous kimming techniques in thatit uses conceptual information as part ofbottom-up linguistic processing, thus usinglinguistic knowledge more fully while limit-ing grammatical complexity.1 In t roduct ionNatural language systems that extract informationfrom volumes of text have matured uring the lastseveral years.
While these systems till operate infairly limited domains, they can produce useful struc-tured information from text with reasonable accu-racy.
Volumes of text information, low cost comput-ing power, and scarce labor resources increase themotivation for using computers to manage informa-tion.
The question that stands in the way of thewidespread installation of text processing systems is"Are they good enough yet?
"Performance is a major issue in the evolution oftext processing systems from "toy" research prob-lems to real applications.
In spite of the rapid ad-vances in computing technology, most text process-ing systems are simply too slow, because applicationsand thorough testing both demand higher through-put.
Excess grammatical complexity clearly accountsfor much of the performance shortfall.
Most compu-tational methods of analysis overlook the human-likecapability to "skim" texts, limiting computation tosections of importance.Unfortunately, the "skimming" approach as cometo be identified with systems that abandon grammat-ical knowledge as well as syntactic processing, thussacrificing accuracy and other performance f atures.The ideal skimming system would need no less lin-guistic knowledge than a full syntactic parser; in fact,knowing what not to parse may require as much lin-guistic information as parsing itself.
Our approachto skimming is relation-driven: the program makesa low-cost first pass through the texts, determiningwhat conceptual relations could be relevant.
Thenit segments the text to limit grammatical nalysisto sections that affect those conceptual relations.
Insections of text with high information content, theprogram still performs a complete analysis.
In othercases, the skimming technique l ads to three types ofperformance improvements.
The following examplesillustrate these features in the context of SCISOR,a system that reads news stories about corporatetakeovers \[Rau and Jacobs, 1988\]:1.
Skipping irrelevan~ text.Example: The company said it expects 1989daily output o average 10,500 barrels of oil andliquids.Effect: Do not parse the sentence.2.
Limited processing of intervening phrases.Example: Fidelity Federal Savings ~ Loan As-sociation said its board held a meeting on Dec.2 and declined a proposal to amend the acquisi-tion agreement by BEI Holdings Ltd.Effect: Parse to determine the roles of "de-clined" and "acquisition" only.3.
Limited attachment.Example: Revere said it had received an offerfrom an investor group to be acquired for $16 ashare, or about $127 million.Effect: Break the sentence into three sectionsto limit complexity.In each of the above examples, relation-drivenskimming limits processing by concentrating on in-formation that.
is necessary to satisfy the informationrequirements of the program, such as identifying thetarget and suitor of a takeover.
In the first case,the program can skip an entire sentence because itdoes not contribute at all.
In the second example,the sentence contains relevant information, but oneclause (about the meeting of the board) does not re-ally contribute.
In the third example, the sentenceis packed with relevant material, but the programcan still limit grammatical complexity by discardingalternatives that do not contribute to semantic pro-cessing.Relation-driven skimming is part of our text pro-cessing system, which has been applied to several pro-totype domains.
The first accomplishment of the al-gorithm was to deliver a factor of six improvementin performance to SCISOR, which reads Dow Jones19,4 ifinancial news stories at a rate of over 500 per hour.The same skimming program applied successfully toan evaluation set of naval operations reports used inthe Message Understanding Conference (MUCK-I I )held in San Diego in June, 1989 \[Sundheim, 11990\].This paper will describe the relation-driven skimmingalgorithm and discuss the sort of performance im-provements that can be expected from this approach.2 The  Text  Ext rac t ion  Task  and  theSk imrn ing  Prob lemSkilrmling is of little use if the processing task de-mands a complete analysis.
The skimming methodpresented here is aimed at the task of informationextraction from text, where the program looks for rel-al;ively superficial facts that appear in texts with con-slrained content.
Since the program is looking onlyfor certain key information, it should spend most ofits time analyzing sections thal; contain that informa-tion.
The more extraneous information there is in atext, the more skimming improves performance overfull parsing.The skirnmirig problem assunles that a text prc~c~-;sing program works from set of predefined con-ceptual roles that represent some of the informationfrom a t.ext.
These conceptual roles forln a "temoplate" that the program fills in while scanning a text,The words "relevant", " important",  and "extrane-o~s" used throughout his paper describe the rela,tionship of portions of text to this template-fillingtask.For example, the following is a l.ypical i)ow Jonesm'w story, along with the template structure repre-senting the ilaformat.ion extracted by SCIS()R:I \]~put Text :th.uaswick Corp Up; Aclive amid ContinuedTakeover TalkNew ~)rk -D J- Traders and market sourcessay shares of Brunswick Corp., the world'slargest Inanufaeturer of recreatioaal boats, aretrading actively for the second consecutive weekamid growing speculal, ion that someone is accu-mulating a post{ion in the company.
Brunswickis up 5-8 a.t 20 on NYSE-composite voluime(sic) of 1,I 60,400 shares, compared with an av-erage daily vohtme of 435,20(I shares.
The stockrose 11-2 yesterday on more than 1.1 millionshares.
In the past two we(:ks, l~runswick shareshave traded abow~" average daily voh i inc  on allbug one day, inching up from a low of 16 3-4on March 14.
The, most often rttmored suitorfor Brunswick is Miniieapolis investor and boatcompany owner Irwin Ja.eobs, whose name sur-faced about a. year ago when Brunswick sharesmade a similar move eli nnfounded specula-tion.
One trader tells Dew Jones Professionalhivestor Report a New Jersey-based "tape read-ing" service today named the stock as the targetof a S30-a-share bid fl'om Jacobs.
The servicecan't be reached for confirmation.
A secretaryto ,Iacobs said late yesterda.y he'll be out of t, isoffice until Friday.
Jacobs has become a popu-lar rumore.d shark since walking away with cashfrom his failed bid earlier this month for ShakleeCorp.. lie's also one el' several rumored suitorsfor NWA In(:.Te lnp la te  S t ruc ture  P roduced:Corporate -Takeover -  CoreEvent :  lh imorSu i to r :  dacobsTarget :  Brunswick Corp.Per -share -pr i ce :  $30 \[price of the offer\]Et feet -o f - ru lnor"  Up 5/8\[rite system conht obtain the same minimal infor-mation if the text read as follows:Brunswick is up 5-8 at 20....rumored._.thetarget of a $30-a-share bid from Jacobs.The objective of skimrning is to read the text as if itwere closer to this condensed form.
The problem fortext skimming is thus (1) to identify sections of textthat will contribute to information extraction, and(2) to limit processing in those sections.
The intro-duction outlined three different types of pertbrmanceimprovements that come from skimming.
'l'he nextsection describes the relation-driven method and howit achieves these improvements.3 Re la t ion -Dr iven  Sk immingt{elation-driven skimming takes advantage of the the-ory that most conceptual information derives frontlinguistic relations that do not depend on a com-plete surface structure.
Such relations, like subjecl-predicale and verb-complemenl, carry constraintssuch as agreement or selectional restrictions.
Sincethe bulk of the complexity of most language analyz-ers comes from the combinatorics of parsing, findingrelations without a complete syntactic analysis helpsp (?
r fo r  Irl a, li ce .The relation-driven skimming algorithm has threecomponents:?
73e coT~ccpl aclivalion component makes a firstpass through the text and selects candidate con-cepts that may contribute to its semantic inter-pretation.The scgmc~zlalion compone~t tells the programwhat to parse, what to skip, and where to useseinantic inlbrmation for attachment., The allachmer, l cornpo'acnZ identifies linguisticrelations in the input text that contribute toits semantic interpretation, even where segmentshave been skipped.The trick to relation-driven skimming is to per-form attachment as accurately as possible with aslittle grammatical  analysis as possible.
This is nosimple task, because phrases with no relevant seman-tic content can always affect the attachment of rele-vant phrases.
In the sections that follow, we will givefor each of the components above an observation ofwhy it works, its main activity, and an example ortwo of its operation.3.1 Concept  Act ivat ionConcept activation uses lexical analysis of words,combinations, and ()ther features to determine2 i95whether a portion of text is likely to be relevant.The concept activation component makes a singlepass through the input text, producing a sequenceof conceptual categories that may contribute to theconceptual interpretation.Observation: The density of relevant contentwords in a section of text generally determines thedegree of processing required for semantic analysis.Act iv i ty :  Divide content words into two cate-gories: "triggers", or relation heads, and role fillers.Scan the text using domain knowledge for words orcombinations that might be triggers, and for wordsor combinations that might be fillers.Example :  The following is the input text and out-put of concept activation for the Revere example:Input :  Revere said it had received anoffer from an investor group to be acquiredfor $16 a share, or about $127 million.Output :  (Company) (receive-offer)(investor-group) (acquire)(dol lar)  (num-ber) (share) (dollar)(number).This process is more than a lexicM lookup.
Somewords, like rumor or target in the corporate takeoverstories, are indeed "triggers" directly associated withimportant concepts.
However, considering all wordsthat might contribute to an important concept is in-efficient; words such as make, take, iss~te or increaserequire more analysis of the surrounding context.
Inthese cases, the skimmer looks for combinations ofwords or concepts (such as received and offer above).This prevents the parser from doing a lot of process-ing around low-content words.Whether a word is contentful or not depends oncontext.
Some words, like plan, do not themselvescarry much information but must be understood be-cause they distinguish the agent of anot, her action.
"Acme rejects an offer" and "Acme plans an offer"place Acme in different roles (i.e.
the target andsuitor, respectively).
A concept like plan, therefore,appears in the list.
of activated concepts only whenthere are takeover events in the local context.Concept activation eliminates processing of unim-portant sentences and clauses and helps efficiency incontentful sections, mainly by determining relations(such as role-filler) that help to guide syntactic anal-ysis.
The next phase, text segmentation, uses theresults of concept activation to control parsing.3.2 Text Segmentat ionThe text segmentation phase groups the text aroundwords that are concept activators, identifying noungroups and complement structures after verbs, andfinding punctuation or words that separate segmentsof text.
This phase determines (1) where to skip and(2) where to limit parsing.Observation: It is generally possible to recon-stucL the important relations of a text in spite ofskipping over intervening words and phrases.Aetivity: Skip over empty sentences and phrases,and break the combinatorics of parsing where a singleparse will do.Example :  In the Revere example, the segmented(and marked) text is as follows:Revere *skip* it }tad received an offerfrom a.n investor group *break* to be ac-quired for $16 a share *break* *skip* $127million.The *skip* token indicates to the parser that thereis intervening information, while the *break* token in-dicates that it should "reduce" or complete all activelinguistic structures.
Both help to limit complexity--skipping tends to avoid wasted parsing ~m well as thecombinatorics of attachment, while the breaks help toavoid considering nmltiple attachments where syntaxcontributes little or no information.The segmentation algorithm includes most impor-tant noun phrases, even when separation informationprevents them from being attached.
This is because,as in the above example, these noun phrases implic-itly play a role in anaphoric references or infinitivephrases.A side effect of text segmentation is to mark theoriginal text, highlighting sections that are consid-ered relevant.
This has a dual effect: (1) It helps todebug the skimming algorithm by showing visuallywhat sections of the text the program has read, and(2) It allows the users of the program quickly to spotkey information.For example, a typical merger & acquisition storyfrom the Dow Jones examples will appear with rele-vant sections in boldface, as shown below:Mayfa l r  Gets  Buyout  P roposa lMayth l r  Super  Markets  Ineo saidthat S tan ley  P. Kaufe l t ,  its chairman,president and chief executive, has proposeda business combination with Mayfa i r  inwhich the ho lders  of  Mayfa i r ' s  out-s tand ing  common stock would  reeelve$23.50 a share  in cash.Text segnlentation confines processing to sectionsof text that contain important information.
The cor-rect semantic interpretation of these text sections of-ten depends on correct syntactic attachment, as de-scribed below.3.3 At tachmentThe attachment phase produces linguistic relationsfrom the segrnented text.
This phase is part of thebottom-up parsing process; the nmin difference be-tween attachment and full parsing is that the parsernmst attempt o form linguistic relations where it hasskipped sections of text.
Attachment in tile absenceof a complete parse relies on rules that combine lin-guistic and conceptuM information; for example, "a.t-tach a verb phrase or infinitive to the most recentclause-level semanticly valid noun phrase".Observat ion :  Attachments by default are muchless costly computationally than attachments by ex-haustive consideration of possibilities.Act iv i ty :  Prefer attachments within boundariesseparated by breaks, and use semantics and recencyto guide attachment otherwise.Example :  In the Revere example, the followingrelations guide the interpretation process:Revere received an offer... (NP-VP)an offer from an investment group (NP-PP)196 3Revere to be acquired...
(NP-INFPHR.
)acquired for $16 a share (VP-PP)$160 million (NP)The combination of these simple relations permitsthe correct semantic interpretation without a com-plete parse (see \[Rau and Jacobs, 1988\] for a discus-sion of the use of these relations for analysis).
In thisexample, the skimming algorithm reduces the num-ber of parses considered by a factor of six.
This is inspite of the fact that the R.evere sentence contains afair amount of useful intbrmation; in less dense textthe skimming program can sldp sentences entirely orextract only two or three relations from a complexsentence (as in the Fidelity exmnple given in the in-troduction).The Revere example is more complex than most ofthe c~es that occur in these texts because it illus-trates a number of interacting rules and preferences.It is, however, unusual in that flfll syntactic parsingof this example could be misleading because it wouldtend to attach "to be acquired" to "investor group"rather than "Revere".
The point of this example is7~0t, however, that flflI syntactic processing is bad, butrather that skimming can make the necessary attach-ments without full parsing.Compl icat ions  of  L imi ted  At tachmentThe attachment mechanism makes use of severalheuristics for constructing relations fi'om the text,such as the infinitive phrase rule given earlier, re-solving re.ferences before attaching pronouns, recon-structing sentences fl'om verb phrases in incompletesentences, and determining voice before attachingconjunctive verb phrases.
These rules have derivedfrom the analysis of fairly large bodies of text.
Thefollowing are some observations about the sorts ofexamples where "limited attachment" is necessary:o Dangling Phrases.
In many longer texts, prepo-sitional phrases, infinitives, and other adjunct in-formation "hang off" the ends of sentences.
Typ-ically, such phrases can be attached syntacticlyto rnnltiple heads.
The effect of the skimmingalgorithm is to give more weight to the semanticattachment of these phrases.
Since many suchexamples contain temporal, spatial, or other in-formation associated with events, this semanticattachment seems to provide an advantage oversyntactic preferences.o Conlunctive Clauses.
Conjunctions introducelinguistic complexity.
If only part of a conjunc-tive clause contains useful information, the pro-gram (:an identify a relation involving one por-tion of the coordinated clause without parsingthe whole sentence.
For example, one news storyreads "Investor William Farley...said he plans toseek a special meeting to discuss his proposal andto wage a proxy fight tbr control of the board".Only the second clause contains useful informa-tion, although the first clause can help to attachthe second.Negative h~.formalion.
The skimming algorithm,in its application of linguistic relations, must useboth positive and negative intbrmation in deter-mining where to attach phrases.
Lack of agree-ment, for example, can override the attachmentof a verb phrase to a sernanticly valid subject.Case constraints often guide the analysis of pro-nouns.
Semantic infbrrnation tends to providepositive information in these cases, while syntac-tic information provides negative information.Oddly, this is tile reverse of the more typicalparsing strategy of using semantics to filter outinvalid interpretations.It might seem that these complications prcsentenough problems that it would be easier to performfull parsing than to try to derive new heuristics for at-tachment in all these examples.
This is true in somecases, but the vast majority of examples we have en-countered require only a few simple attachment pref-erences.
In these "easier" examples, the performancepayoff has been enough to keep us from degrading to:full parsing whenever possible.4 Compar i son  w i th  OtherApproachesMost work in skimming or partial parsing \[Deaong,1979; Lebowitz, 1983; l,ytinen and Gershman, 1986;Young and Itayes, 1985\] uses template-based ormemory-based strategies, effectively using conceptualinformation in place of linguistic constraints.
Thisapproach seems to work in highly constrained textswhere conceptual knowledge is sufficient tbr deter-mining role relationships.
In the domains that wehave tested, the pure template-based approach failsbecause some role relationships are determined al-most entirely from linguistic intbrmat.ion such as com-plernent structure or agreement, l'br example, thetarget and suitor of corporate mergers are both com-panies; thus there is little conceptual information(other than the size of the companies) that helpsto determine role-filling.
In many classes of tacti-cal operations reports, the agent and object are bothmilitary forces, thus correct linguistic attachmen~ isessential in this domain as well.Although the overall parsing style of our systemintegrates template-based and language-based strate-gies \[Rau and Jacobs, 1988\], the skimming algo-rithm is actually more bottom-up or language-based.Like some of the other major text processing sys-tems such as PI{OTEUS, PUNDIT, and qACH'US\[tIobbs, 1986; Grishman and Ilirschmau, 1986\], tileskimming program applies linguistic constraints andmaps linguistic structures into conceptual roles.
Inthese other systems, however, the bottom-up ap-proach may cause the program to waste time onirrelevant sections of text.
The difference is thatthese programs do not really use conceplual infofmation until after the parser has generated its candi-date structures.
The relation-driven sldmming pro-cess shortcuts bottom-up analysis by firs.t using con-ceptual knowledge to block some fi'uitless paths.
Aswe have had the benefit of comparing our system insome detail with these other programs after operationon a common task, we believe that many such systemscould achieve an order of magnitude improvement inprocessing speed by incorporating a similar method.4 1975 System Status and CurrentDirect ionsThe SC\[SOR system \[Ran and Jacobs, 1988\] was theinitial testbed for this algorithm, is a completed pro-totype that reads news stories at the rate of about 500per hour.
It extracts certain key information fromstories about corporate takeovers (typically about10% of the texts), identifying target, suitor, purchaseprice, and other information with about 90% accu-racy.The generic text processing components ofSCISOR, known as the GE NLToolset \[Jacobs andRau, 1990\], are used in applications in the opera-tions of GE.
Our group applied this core of text pro-cessing tools, including the skimming procedures de-scribed here, to the MUCK-II task, which consistedof generating database templates from naval opera-tions messages, during a period of several weeks be-fore the conference.
The skimming algorithm of theNLToolset was the key to producing good results sorapidly.
The same text processing system has sinceapplied to a number of message sets in other domains.The improvements in speed from skimming haveso far come without a degradation i  accuracy.
Thisdoes not, however, mean that the attachment heuris-tics are infallible.
Clearly, examples can occur wheretext that has been skipped influences the attachementof key phra~ses, especially when texts contain ellipsis,anaphorie references, and complex coordinated struc-tures.
Future enhancements o our algorithm mustrefine the attachment rules for these cases, and de-grade to full parsing where necessary.6 Conc lus ionNatural language text processing has reached a pointwhere efficiency is a reM issue.
While it might be pos-sible to design fundamentally faster text processingalgorithms, a more fruitflfl approach in the near termis to try to eliminate much of the wasted processingthat is done in parsing text.
This does not meanusing less syntactic knowledge, but does mean lesssyntactic processing.
Our approach uses the identifi-cation of linguistic relations as a driver for producingconceptual information while eliminating some of thedetail of parsing.
This approach as been successflflin producing a program that operates ahnost an or-der of magnitude faster in nm!tiple domains withoutmajor effect on the design or accuracy of the system.References\[DeJong, 1979\] Gerald DeJong.
Skimming stories inreal time: An experiment in integrated under--standing.
Research Report 158, Department ofComputer Science, Yale University, 1979.\[Grishman mid Hirschman, 1986\] Ralph Gr-ishman and Lynette Hirschman.
PROTEUS andPUNDIT: Research in text understanding.
PRO-TEUS Project Memorandum 1, NYU, 1986.\[Iiobbs, 1986\] Jerry R. tlobbs.
Site report: Overviewof the TACITUS project.
Computational Linguis-tics, 12(3):220-222, 1986.\[Jacobs and Rau, 1990\] Paul S. Jacobs and Lisa F.Rau.
The GE NUl'oolset: A software foundationfor intelligence text processing.
In Proceedings ofthe Thirteenth International Conference on Com-putational Linguistics, Helsinki, Finland, 1990.\[Lebowitz, 1983\] M. Lebowitz.
Memory-based pars-ing.
Artificial h~telligence, 21(4), 1983.\[Lytinen and Gershman, 1986\] Steven Lytinen andAnatole Gershman.
NI'I'tANS: Automatic process-ing of money transfer messages.
In Proceedings ofthe Fifth National Conference on Artificial Intelli-gence, Philadelphia, 1986.\[Rau and ,lacobs, 1988\] Lisa F. Rau and Paul S. Ja-cobs.
Integrating top-down and bottom-up strate-gies in a text processing system.
In Proceedirtgs ofSecond Conference on Applied Natural LanguageProcessing, pages 129-135, Morristown, NJ, Feb1988.
ACL.\[Sundheim, 1990\] Beth Sundheim.
Second messageunderstanding conference (MUCK-II) test report.Technical Report 1328, Naval Ocean Systems Cen-ter, San Diego, CA, 1990.\[Young and Hayes, 1985\] S. Young and P. Hayes.Automatic classification and summarization ofbanking telexes.
In The Second Conference onArtificial h~telligence Applications, pages 402-208.IEEE Press, 1985.19 8 5
