Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1567?1576,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA Convolutional Architecture for Word Sequence PredictionMingxuan Wang1Zhengdong Lu2Hang Li2Wenbin Jiang1Qun Liu3,11Key Laboratory of Intelligent Information Processing,Institute of Computing Technology, Chinese Academy of Sciences{wangmingxuan,jiangwenbin,liuqun}@ict.ac.cn2Noah?s Ark Lab, Huawei Technologies{Lu.Zhengdong,HangLi.HL}@huawei.com3ADAPT Centre, School of Computing, Dublin City UniversityAbstractWe propose a convolutional neural net-work, named genCNN, for word se-quence prediction.
Different fromprevious work on neural network-based language modeling and genera-tion (e.g., RNN or LSTM), we choosenot to greedily summarize the historyof words as a fixed length vector.
In-stead, we use a convolutional neuralnetwork to predict the next word withthe history of words of variable length.Also different from the existing feed-forward networks for language mod-eling, our model can effectively fusethe local correlation and global cor-relation in the word sequence, witha convolution-gating strategy specifi-cally designed for the task.
We arguethat our model can give adequate rep-resentation of the history, and there-fore can naturally exploit both the shortand long range dependencies.
Ourmodel is fast, easy to train, and read-ily parallelized.
Our extensive exper-iments on text generation and n-bestre-ranking in machine translation showthat genCNN outperforms the state-of-the-arts with big margins.1 IntroductionBoth language modeling (Wu and Khudanpur,2003; Mikolov et al, 2010; Bengio et al,2003) and text generation (Axelrod et al, 2011)boil down to modeling the conditional proba-bility of a word given the proceeding words.Previously, it is mostly done through purelymemory-based approaches, such as n-grams,which cannot deal with long sequences and hasto use some heuristics (called smoothing) forrare ones.
Another family of methods are basedon distributed representations of words, whichis usually tied with a neural-network (NN) ar-chitecture for estimating the conditional prob-abilities of words.Two categories of neural networks have beenused for language modeling: 1) recurrent neu-ral networks (RNN), and 2) feedfoward net-work (FFN):?
The RNN-based models, including itsvariants like LSTM, enjoy more popu-larity, mainly due to their flexible struc-tures for processing word sequences of ar-bitrary lengths, and their recent empiri-cal success(Sutskever et al, 2014; Graves,2013).
We however argue that RNNs,with their power built on the recursive useof a relatively simple computation units,are forced to make greedy summarizationof the history and consequently not effi-cient on modeling word sequences, whichclearly have a bottom-up structures.?
The FFN-based models, on the otherhand, avoid this difficulty by feeding di-rectly on the history.
However, the FFNsare built on fully-connected networks,rendering them inefficient on capturinglocal structures of languages.
Moreovertheir ?rigid?
architectures make it futile tohandle the great variety of patterns in longrange correlations of words.We propose a novel convolutional architec-ture, named genCNN, as a model that can ef-ficiently combine local and long range struc-tures of language for the purpose of modelingconditional probabilities.
genCNN can be di-rectly used in generating a word sequence (i.e.,1567?CNN?CNN?CNN??sandwich?
?/     /   I was starving after this long meeting, so I rushed to wal-mart to buy ahistory:prediction:Figure 1: The overall diagram of a genCNN.
Here ?/?
stands for a zero padding.
In this example,each CNN component covers 6 words, while in practice the coverage is 30-40 words.text generation) or evaluating the likelihood ofword sequences (i.e., language modeling).
Wealso show the empirical superiority of genCNNon both tasks over traditional n-grams and itsRNN or FFN counterparts.Notations: We will use V to denote the vo-cabulary, et(?
{1, ?
?
?
, |V|}) to denote the tthword in a sequence e1:tdef= [e1, ?
?
?
, et], ande(n)tif the sequence is further indexed by n.2 OverviewAs shown in Figure 1, genCNN is overall re-cursive, consisting of CNN-based processingunits of two types:?
?CNN as the ?front-end?, dealing withthe history that is closest to the prediction;?
?CNNs (which can repeat), in charge ofmore ?ancient?
history.Together, genCNN takes history e1:tof arbi-trary length to predict the next word et+1withprobabilityp(et+1|e1:t;??
), (1)based on a representation ?(e1:t;??)
producedby the CNN, and a |V|-class soft-max:p(et+1|e1:t;??)
?
e?>et+1?(e1:t)+bet+1.
(2)genCNN is devised (tailored) fully for mod-eling the sequential structure in natural lan-guage, notably different from conventionalCNN (Lawrence et al, 1997; Hu et al, 2014)in 1) its specifically designed weights-sharingstrategy (in ?CNN), 2) its gating design, and3) certainly its recursive architectures.
Alsodistinct from RNN, genCNN gains most ofits processing power from the heavy-duty pro-cessing units (i.e.,?CNN and ?CNNs), whichfollow a bottom-up information flow and yetcan adequately capture the temporal structurein word sequence with its convolutional-gatingarchitecture.3 genCNN: ArchitectureWe start with discussing the convolutional ar-chitecture of ?CNN as a stand-alone sentencemodel, and then proceed to the recursive struc-ture.
After that we give a comparative analysison the mechanism of genCNN.
?CNN, just like a normal CNN, has fixedarchitecture with predefined maximum words(denoted as L?).
History shorter than L?willfilled with zero paddings, and history longerthan that will be folded to feed to ?CNN afterit, as will be elaborated in Section 3.3.
Similarto most other CNNs, ?CNN alternates betweenconvolution layers and pooling layers, and fi-nally a fully connected layer to reach the repre-sentation before soft-max, as illustrated by Fig-ure 2.
Unlike the toyish example in Figure 2, inpractice we use a larger and deeper ?CNN withL?= 30 or 40, and two or three convolutionlayers (see Section 4.1).
Different from con-ventional CNN, genCNN has 1) weight shar-ing strategy for convolution, and 2)?external?gating networks to replace the normal poolingmechanism, both of which are specifically de-signed for word sequence prediction.3.1 ?CNN: ConvolutionDifferent from conventional CNN, the weightsof convolution units in ?CNN is only partiallyshared.
More specifically, in the convolutionunits there are two types feature-maps: TIME-FLOW and the TIME-ARROW, illustrated re-1568probability of next wordwhat did you have for/               /               /?dinner??breakfast??us??the?
?A 3-layer ?CNNTime-Flow         Time-Arrow           GatingFigure 2: Illustration of a 3-layer ?CNN.Here the shadowed nodes stand for the TIME-ARROW feature-maps and the unfilled nodesfor the TIME-FLOW.spectively with the unfilled nodes and fillednodes in Figure 2.
The parameters for TIME-FLOW are shared among different convolutionunits, while for TIME-ARROW the parame-ters are location-dependent.
Intuitively, TIME-FLOW acts more like a conventional CNN (e.g.,that in (Hu et al, 2014)), aiming to understandthe overall temporal structure in the word se-quences; TIME-ARROW, on the other hand,works more like a traditional NN-based lan-guage model (Vaswani et al, 2013; Bengio etal., 2003): with its location-dependent param-eters, it focuses on capturing the direction oftime and prediction task.For sentence input x = {x1, ?
?
?
,xT}, thefeature-map of type-f on Layer-` isif f ?
TIME-FLOW:z(`,f)i(x) = ?
(w(`,f)TF?z(`?1)i+ b(`,f)TF), (3)if f ?
TIME-ARROW:z(`,f)i(x) = ?
(w(`,f,i)TA?z(`?1)i+ b(`,f,i)TA), (4)where?
z(`,f)i(x) gives the output of feature-mapof type-f for location i in Layer-`;?
?(?)
is the activation function, e.g., Sig-moid or Relu (Dahl et al, 2013)?
w(`,f)TFdenotes the location-independentparameters for f ?TIME-FLOW on Layer-`, while w(`,f,i)TAstands for that for f ?TIME-ARROW and location i on Layer-`;?
?z(`?1)idenotes the segment of Layer-`?1for the convolution at location i , while?z(0)idef= [x>i, x>i+1, ?
?
?
, x>i+k1?1]>concatenates the vectors for k1wordsfrom sentence input x.3.2 Gating NetworkPrevious CNNs, including those for NLPtasks (Hu et al, 2014; Kalchbrenner et al,2014), take a straightforward convolution-pooling strategy, in which the ?fusion?
deci-sions (e.g., selecting the largest one in max-pooling) are based on the values of feature-maps.
This is essentially a soft template match-ing, which works for tasks like classification,but undesired for maintaining the compositionfunctionality of convolution.
In this paper, wepropose to use separate gating networks to re-lease the scoring duty from the convolution,and let it focus on composition.
Similar ideahas been proposed by (Socher et al, 2011) forrecursive neural networks on parsing, but neverbeen combined with a convolutional structure.
?Layer-Layer-Layer-gatingFigure 3: Illustration for gating network.Suppose we have convolution feature-mapson Layer-` and gating (with window size =2) on Layer-`+1.
For the jthgating win-dow (2j?1, 2j), we merge?z(`?1)2j?1and?z(`?1)2jasthe input (denoted as?z(`)j) for gating network,as illustrated in Figure 3.
We use a separate1569gate for each feature-map, but follow a differ-ent parametrization strategy for TIME-FLOWand TIME-ARROW.
With window size = 2, thegating is binary, we use a logistic regressor todetermine the weights of two candidates.
Forf ?
TIME-ARROW, with location-dependentw(`,f,j)gate, the normalized weight for left side isg(`+1,f)j= 1/(1 + e?w(`,f,j)gate?z(`)j),while for For f?TIME-FLOW, the parametersfor the corresponding gating network, denotedas w(`,f)gate, are shared.
The gated feature map isthen a weighted sum to feature-maps from thetwo windows:z(`+1,f)j= g(`+1,f)jz(`,f)2j?1+ (1?
g(`+1,f)j)z(`,f)2j.
(5)We find that this gating strategy works signifi-cantly better than pooling directly over feature-maps, and slightly better than a hard gate ver-sion of Equation 53.3 Recursive ArchitectureAs suggested early on in Section 2 and Fig-ure 1, we use extra CNNs with conventionalweight-sharing, named ?CNN, to summarizethe history out of scope of ?CNN.
More specif-ically, the output of ?CNN (with the same di-mension of word-embedding) is put before thefirst word as the input to the ?CNN, as il-lustrated in Figure 4.
Different from ?CNN,?CNN is designed just to summarize the his-tory, with weight shared across its convolutionunits.
In a sense, ?CNN has only TIME-FLOWfeature-maps.
All ?CNN are identical and re-cursively aligned, enabling genCNN to handlesentences with arbitrary length.
We put a spe-cial switch after each ?CNN to turn it off (re-placing a pading vector shown as ?/?
in Fig-ure 4 ) when there is no history assigned to it.As the result, when the history is shorter thanL?, the recursive structure reduces to ?CNN.In practice, 90+% sentences can be mod-eled by ?CNN with L?= 40 and 99+% sen-tences can be contained with one extra ?CNN.Our experiment shows that this recursive strat-egy yields better estimate of conditional den-sity than neglecting the out-of-scope history(Section 6.1.2).
In practice, we found that alarger (greater L?)
and deeper ?CNN works?CNNe5       e6 e7 e8       e7       e8 e9?
?CNN?/ ?/ / / e1       e2       e3 e4prediction for e10Figure 4: genCNN with recursive structure.better than small ?CNN and more recursion,which is consistent with our intuition that theconvolutional architecture is better suited formodeling the sequence.3.4 Analysis3.4.1 TIME-FLOW vs. TIME-ARROWBoth conceptually and systemically, genCNNgives two interweaved treatments of word his-tory.
With the globally-shared parameters inthe convolution units, TIME-FLOW summa-rizes what has been said.
The hierarchi-cal convolution+gating architecture in TIME-FLOW enables it to model the composition inlanguage, yielding representation of segmentsat different intermediate layers.
TIME-FLOWis aware of the sequential direction, inheritedfrom the space-awareness of CNN, but it is notsensitive enough about the prediction task, dueto the uniform weights in the convolution.On the other hand, TIME-ARROW, livingin location-dependent parameters of convolu-tion units, acts like an arrow pin-pointing theprediction task.
TIME-ARROW has predictivepower all by itself, but it concentrates on cap-turing the direction of time and consequentlyshort on modelling the long-range dependency.TIME-FLOW and TIME-ARROW have towork together for optimal performance in pre-dicting what is going to be said.
This intuition1570has been empirically verified, as our experi-ments have demonstrated that TIME-FLOW orTIME-ARROW alone perform inferiorly.
Onecan imagine, through the layer-by-layer convo-lution and gating, the TIME-ARROW graduallypicks the most relevant part from the represen-tation of TIME-FLOW for the prediction task,even if that part is long distance ahead.3.4.2 genCNN vs. RNN-LMDifferent from RNNs, which recursively ap-plies a relatively simple processing units,genCNN gains its ability on sequence mod-eling mostly from its flexible and power-ful bottom-up and convolution architecture.genCNN takes the ?uncompressed?
history,therefore avoids?
the difficulty in finding the representationfor history, e.g., those end in the middle ofa chunk (e.g.,?the cat sat on the?),?
the damping effort in RNN when thehistory-summarizing hidden state is up-dated at each time stamp, which rendersthe long-range memory rather difficult,both of which can only be partially amelioratedwith complicated design of gates (Hochreiterand Schmidhuber, 1997) and or more heavyprocessing units (essentially a fully connectedDNN) (Sutskever et al, 2014).4 genCNN: TrainingThe parameters of a genCNN??
consists ofthe parameters for CNN ?nn, word-embedding?embed, and the parameters for soft-max?softmax.
All the parameters are jointlylearned by maximizing the likelihood of ob-served sentences.
Formally the log-likelihoodof sentence Sn(def= [e(n)1, e(n)2, ?
?
?
, e(n)Tn]) islog p(Sn;??)
=Tn?t=1log p(e(n)t|e(n)1:t?1;??
),which can be trivially split into Tntraining in-stances during the optimization, in contrast tothe training of RNN that requires unfoldingthrough time due to the temporal-dependencyof the hidden states.4.1 Implementation DetailsArchitectures: In all of our experiments(Section 5 and 6) we set the maximum wordsfor ?CNN to be 30 and that for ?CNN to be 20.?CNN have two convolution layers (both con-taining TIME-FLOW and TIME-ARROW con-volution) and two gating layers, followed bya fully connected layer (400 dimension) andthen a soft-max layer.
The numbers of feature-maps for TIME-FLOW are respectively 150(1st convolution layer) and 100 (2nd convolu-tion layer), while TIME-ARROW has the samefeature-maps.
?CNN is relatively simple, withtwo convolution layer containing only TIME-FLOW with 150 feature-maps, two gating lay-ers and a fully connected layer.
We use ReLUas the activation function for convolution lay-ers and switch to Sigmoid for fully connectedlayers.
We use word embedding with dimen-sion 100.Soft-max: Calculating a full soft-max is ex-pensive since it has to enumerate all the wordsin vocabulary (in our case 40K words) in thedenominator.
Here we take a simple hierarchi-cal approximation of it, following (Bahdanauet al, 2014).
Basically we group the wordsinto 200 clusters (indexed by cm), and factor-ize (in an approximate sense) the conditionalprobability of a word p(et|e1:t?1;??)
into theprobability of its cluster and the probability ofetgiven its clusterp(cm|e1:t?1;??)
p(et|cm; ?softmax).We found that this simple heuristic can speed-up the optimization by 5 times with only slightloss of accuracy.Optimization: We use stochastic gradientdescent with mini-batch (size 500) for opti-mization, aided further by AdaGrad (Duchiet al, 2011).
For initialization, we useWord2Vec (Mikolov et al, 2013) for the start-ing state of the word-embeddings (trained onthe same dataset as the main task), and setall the other parameters by randomly samplingfrom uniform distribution in [?0.1, 0.1].
Theoptimization is done mainly on a Tesla K40GPU, which takes about 2 days for the train-ing on a dataset containing 1M sentences.15715 Experiments: Sentence GenerationIn this experiment, we randomly generate sen-tences by recurrently samplinge?t+1?
p(et+1|e1:t;??
),and put the newly generated word into history,until EOS (end-of-sentence) is generated.
Weconsider generating two types of sentences: 1)the plain sentences, and 2) sentences with de-pendency parsing, which will be covered re-spectively in Section 5.1 and 5.2.5.1 Natural SentencesWe train genCNN on Wiki data with 112Mwords for one week, with some representativeexamples randomly generated given in Table 1(upper and middle blocks).
We try two settings,by letting genCNN generate a sentence 1)fromthe very beginning (middle block), or 2) start-ing with a few words given by human (upperblock).
It is fairly clear that most of the timegenCNN can generate sentences that are syn-tactically grammatical and semantically mean-ingful.
More specifically, most of the sentencescan be aligned to a parse tree with reasonablestructure.
It is also worth noting that quotationmarks (??
and ??)
are always generated in pairsand in the correct order, even across a relativelylong distance, as exemplified by the first gener-ated sentence in the upper block.5.2 Sentences with Dependency TagsFor training, we first parse(Klein and Man-ning, 2002) the English sentences and feed se-quences with dependency tags as follows( I ?
like ( red ?
apple ) )to genCNN in training, where 1) each pairedparentheses contain a subtree, and 2) the sym-bol ???
indicates that the word next to it isthe dependency head in the corresponding sub-tree.
Some representative examples gener-ated by genCNN are given in Table 1 (bottomblock).
As it suggests, genCNN is fairly ac-curate on respecting the rules of parentheses,and probably more remarkably, it can get thedependency tree head right most of the time.6 Experiments: Language ModelingWe evaluate our model as a language model interms of both perplexity (Brown et al, 1992)and its efficacy in re-ranking the n-best can-didates from state-of-the-art models in statisti-cal machine translation, with comparison to thefollowing competitor language models.Competitor Models we compare genCNNto the following competitor models?
5-gram: We use SRI Language ModelingToolkit (Stolcke and others, 2002) to traina 5-gram language model with modifiedKneser-Ney smoothing;?
FFN-LM: The neural language modelbased on feedfoward network (Vaswani etal., 2013).
We vary the input window-sizefrom 5 to 20, while the performance stopsincreasing after window size 20;?
RNN: we use the implementation1ofRNN-based language model with hiddensize 600;?
LSTM: we adopt the code in Ground-hog2, but vary the hyper-parameters,including the depth and word-embeddingdimension, for best performance.LSTM (Hochreiter and Schmidhuber,1997) is widely considered to be thestate-of-the-art for sequence modeling.6.1 PerplexityWe test the performance of genCNN on PENNTREEBANK and FBIS, two public datasetswith different sizes.6.1.1 On PENN TREEBANKAlthough a relatively small dataset3, PENNTREEBANK is widely used as a language mod-elling benchmark (Graves, 2013; Mikolov etal., 2010).
It has 930, 000 words in train-ing set, 74, 000 words in validation set, and82, 000 words in test set.
We use exactly thesame settings as in (Mikolov et al, 2010),with a 10, 000-words vocabulary (all out-of-vocabulary words are replaced with unknown)1http://rnnlm.org/2https://github.com/lisa-groundhog/GroundHog3http://www.fit.vutbr.cz/?imikolov/rnnlm/simple-examples.tgz1572??
we are in the building of china ?s social development and the businessmenaudience , ??
he said .clinton was born in DDDD , and was educated at the university of edinburgh.bush ?s first album , ??
the man ??
, was released on DD november DDDD .it is one of the first section of the act in which one is covered in realplace that recorded in norway .this objective is brought to us the welfare of our countryrussian president putin delivered a speech to the sponsored by the 15th asiapacific economic cooperation ( apec ) meeting in an historical arena on oct .light and snow came in kuwait and became operational , but was rarelyplaced in houston .johnson became a drama company in the DDDDs , a television broadcastingcompany owned by the broadcasting program .
( ( the two ?
sides ) ?
should ( ?
assume ( a strong ?
target ) ) ) .
)( it ?
is time ( ?
in ( every ?
country ) ?
signed ( the ?
speech ) ) .
)( ( initial ?
investigations ) ?
showed ( ?
that ( spot ?
could ( ?
be (further ?
improved significantly ) ) .
)( ( a ?
book ( to ?
northern ( the 21 st ?
century ) ) ) .
)Table 1: Examples of sentences generated by genCNN.
In the upper block (row 1-4) the underlinewords are given by the human; In the middle block (row 5-8), all the sentences are generatedwithout any hint.
The bottom block (row 9-12) shows the sentences with dependency tag generatedby genCNN trained with parsed examples.and end-of-sentence token (EOS) at the end ofeach sentence.
In addition to the conventionaltesting strategy where the models are kept un-changed during testing, Mikolov et al (2010)proposes to also update the parameters in anonline fashion when seeing test sentences.
Thisnew way of testing, named ?dynamic evalua-tion?, is also adopted by Graves (2013).From Table 2 genCNN manages to give per-plexity superior in both metrics, with about 25point reduction over the widely used 5-gram,and over 10 point reduction from LSTM, thestate-of-the-art and the second-best performer.6.1.2 On FBISThe FBIS corpus (LDC2003E14) is relativelylarge, with 22.5K sentences and 8.6M Englishwords.
The validation set is NIST MT06 andtest set is NIST MT08.
For training the neuralnetwork, we limit the vocabulary to the mostfrequent 40,000 words, covering ?
99.4% ofthe corpus.
Similar to the first experiment,all out-of-vocabulary words are replaced withunknown and the EOS token is counted in thesequence loss.From Table 3 (upper block), genCNNModel Perplexity Dynamic5-gram, KN5 141.2 ?FFNN-LM 140.2 ?RNN 124.7 123.2LSTM 126 117genCNN 116.4 106.3Table 2: PENN TREEBANK results, where the3rd column are the perplexity in dynamic eval-uation, while the numbers for RNN and LSTMare taken as reported in the paper cited above.The numbers in boldface indicate that the re-sult is significantly better than all competitorsin the same setting.clearly wins again in the comparison to com-petitors, with over 25 point margin over LSTM(in its optimal setting), the second best per-former.
Interestingly genCNN outperforms itsvariants also quite significantly (bottom block):1) with only TIME-ARROW (same numberof feature-maps), the performance deterioratesconsiderably for losing the ability of capturinglong range correlation reliably; 2) with onlyTIME-TIME the performance gets even worse,1573Model Perplexity5-gram, KN5 278.6FFN-LM(5-gram) 248.3FFN-LM(20-gram) 228.2RNN 223.4LSTM 206.9genCNN 181.2TIME-ARROW only 192TIME-FLOW only 203?CNN only 184.4Table 3: FBIS results.
The upper block(row 1-6) compares genCNN and the competi-tor models, and the bottom block (row 7-9)compares different variants of genCNN.for partially losing the sensitivity to the predic-tion task.
It is quite remarkable that, although?CNN (with L?= 30) can achieve good re-sults, the recursive structure in full genCNNcan further decrease the perplexity by over3 points, indicating that genCNN can benefitfrom modeling the dependency over range aslong as 30 words.6.2 Re-ranking for Machine TranslationIn this experiment, we re-rank the 1000-bestEnglish translation candidates for Chinese sen-tences generated by statistical machine transla-tion (SMT) system, and compare it with otherlanguage models in the same setting.SMT setup The baseline hierarchical phrase-based SMT system ( Chines?
English) wasbuilt using Moses, a widely accepted state-of-the-art, with default settings.
The bilin-gual training data is from NIST MT2012 con-strained track, with reduced size of 1.1M sen-tence pairs using selection strategy in (Axel-rod et al, 2011).
The baseline use conven-tional 5-gram language model (LM), estimatedwith modified Kneser-Ney smoothing (Chenand Goodman, 1996) on the English side of the329M-word Xinhua portion of English Giga-word(LDC2011T07).
We also try FFN-LM, asa much stronger language model in decoding.The weights of all the features are tuned viaMERT (Och and Ney, 2002) on NIST MT05,and tested on NIST MT06 and MT08.
Case-Models MT06 MT08 Ave.Baseline 38.63 31.11 34.87RNN rerank 39.03 31.50 35.26LSTM rerank 39.20 31.90 35.55FFN-LM rerank 38.93 31.41 35.14genCNN rerank 39.90 32.50 36.20Base+FFN-LM 39.08 31.60 35.34genCNN rerank 40.4 32.85 36.63Table 4: The results for re-ranking the 1000-best of Moses.
Note that the two bottom rowsare on a baseline with enhanced LM.insensitive NIST BLEU4is used in evaluation.Re-ranking with genCNN significantly im-proves the quality of the final translation.
In-deed, it can increase the BLEU score by over1.33 point over Moses baseline on average.This boosting force barely slacks up on trans-lation with a enhanced language model in de-coding: genCNN re-ranker still achieves 1.29point improvement on top of Moses with FFN-LM, which is 1.76 point over the Moses (de-fault setting).
To see the significance of thisimprovement, the state-of-the-art Neural Net-work Joint Model (Devlin et al, 2014) usuallybrings less than one point increase on this task.7 Related WorkIn addition to the long thread of work on neu-ral network based language model (Auli et al,2013; Mikolov et al, 2010; Graves, 2013; Ben-gio et al, 2003; Vaswani et al, 2013), our workis also related to the effort on modeling longrange dependency in word sequence predic-tion(Wu and Khudanpur, 2003).
Different fromthose work on hand-crafting features for incor-porating long range dependency, our model canelegantly assimilate relevant information in anunified way, in both long and short range, withthe bottom-up information flow and convolu-tional architecture.CNN has been widely used in computervision and speech (Lawrence et al, 1997;Krizhevsky et al, 2012; LeCun and Bengio,1995; Abdel-Hamid et al, 2012), and latelyin sentence representation(Kalchbrenner and4ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl1574Blunsom, 2013), matching(Hu et al, 2014) andclassification(Kalchbrenner et al, 2014).
Toour best knowledge, it is the first time this isused in word sequence prediction.
Model-wisethe previous work that is closest to genCNN isthe convolution model for predicting moves inthe Go game (Maddison et al, 2014), which,when applied recurrently, essentially gener-ates a sequence.
Different from the conven-tional CNN taken in (Maddison et al, 2014),genCNN has architectures designed for mod-eling the composition in natural language andthe temporal structure of word sequence.8 ConclusionWe propose a convolutional architecture fornatural language generation and modeling.
Ourextensive experiments on sentence generation,perplexity, and n-best re-ranking for machinetranslation show that our model can signifi-cantly improve upon state-of-the-arts.References[Abdel-Hamid et al2012] Ossama Abdel-Hamid,Abdel-rahman Mohamed, Hui Jiang, and GeraldPenn.
2012.
Applying convolutional neuralnetworks concepts to hybrid nn-hmm modelfor speech recognition.
In Acoustics, Speechand Signal Processing (ICASSP), 2012 IEEEInternational Conference on, pages 4277?4280.IEEE.
[Auli et al2013] Michael Auli, Michel Galley,Chris Quirk, and Geoffrey Zweig.
2013.
Jointlanguage and translation modeling with recur-rent neural networks.
In Proceedings of the2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 1044?1054,Seattle, Washington, USA, October.
[Axelrod et al2011] Amittai Axelrod, XiaodongHe, and Jianfeng Gao.
2011.
Domain adapta-tion via pseudo in-domain data selection.
In Pro-ceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 355?362.
Association for Computational Linguistics.
[Bahdanau et al2014] Dzmitry Bahdanau,Kyunghyun Cho, and Yoshua Bengio.
2014.Neural machine translation by jointly learn-ing to align and translate.
arXiv preprintarXiv:1409.0473.
[Bengio et al2003] Yoshua Bengio, RjeanDucharme, Pascal Vincent, and ChristianJauvin.
2003.
A neural probabilistic lan-guage model.
Journal OF Machine LearningResearch, 3:1137?1155.
[Brown et al1992] Peter F. Brown, Vincent J. DellaPietra, Robert L. Mercer, Stephen A. DellaPietra, and Jennifer C. Lai.
1992.
An estimate ofan upper bound for the entropy of english.
Com-put.
Linguist., 18(1):31?40, March.
[Chen and Goodman1996] Stanley F Chen andJoshua Goodman.
1996.
An empirical study ofsmoothing techniques for language modeling.In Proceedings of the 34th annual meetingon Association for Computational Linguistics,pages 310?318.
Association for ComputationalLinguistics.
[Dahl et al2013] George E Dahl, Tara N Sainath,and Geoffrey E. Hinton.
2013.
Improving deepneural networks for lvcsr using rectified linearunits and dropout.
In Proceedings of ICASSP.
[Devlin et al2014] Jacob Devlin, Rabih Zbib,Zhongqiang Huang, Thomas Lamar, RichardSchwartz, and John Makhoul.
2014.
Fast androbust neural network joint models for statisticalmachine translation.
In Proceedings of the52nd Annual Meeting of the Association forComputational Linguistics, pages 1370?1380.1575[Duchi et al2011] John Duchi, Elad Hazan, andYoram Singer.
2011.
Adaptive subgradientmethods for online learning and stochastic opti-mization.
The Journal of Machine Learning Re-search, 12:2121?2159.
[Graves2013] Alex Graves.
2013.
Generating se-quences with recurrent neural networks.
CoRR,abs/1308.0850.
[Hochreiter and Schmidhuber1997] Sepp Hochre-iter and J?urgen Schmidhuber.
1997.
Long short-term memory.
Neural Comput., 9(8):1735?1780, November.
[Hu et al2014] Baotian Hu, Zhengdong Lu, HangLi, and Qingcai Chen.
2014.
Convolutionalneural network architectures for matching natu-ral language sentences.
In NIPS.
[Kalchbrenner and Blunsom2013] Nal Kalchbren-ner and Phil Blunsom.
2013.
Recurrent contin-uous translation models.
In Proceedings of the2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 1700?1709,Seattle, Washington, USA, October.
[Kalchbrenner et al2014] Nal Kalchbrenner, Ed-ward Grefenstette, and Phil Blunsom.
2014.
Aconvolutional neural network for modelling sen-tences.
ACL.
[Klein and Manning2002] Dan Klein and Christo-pher D Manning.
2002.
Fast exact inferencewith a factored model for natural language pars-ing.
In Advances in neural information process-ing systems, volume 15, pages 3?10.
[Krizhevsky et al2012] Alex Krizhevsky, IlyaSutskever, and Geoffrey E Hinton.
2012.Imagenet classification with deep convolutionalneural networks.
In Advances in neural infor-mation processing systems, pages 1097?1105.
[Lawrence et al1997] Steve Lawrence, C Lee Giles,Ah Chung Tsoi, and Andrew D Back.
1997.Face recognition: A convolutional neural-network approach.
Neural Networks, IEEETransactions on, 8(1):98?113.
[LeCun and Bengio1995] Yann LeCun and YoshuaBengio.
1995.
Convolutional networks for im-ages, speech, and time series.
The handbook ofbrain theory and neural networks, 3361:310.
[Maddison et al2014] Chris J. Maddison, AjaHuang, Ilya Sutskever, and David Silver.
2014.Move evaluation in go using deep convolutionalneural networks.
CoRR, abs/1412.6564.
[Mikolov et al2010] Tomas Mikolov, MartinKarafit, Lukas Burget, Jan Cernocky, andSanjeev Khudanpur.
2010.
In INTERSPEECH,pages 1045?1048.
[Mikolov et al2013] Tomas Mikolov, Kai Chen,Greg Corrado, and Jeffrey Dean.
2013.
Effi-cient estimation of word representations in vec-tor space.
CoRR, abs/1301.3781.
[Och and Ney2002] Franz Josef Och and HermannNey.
2002.
Discriminative training and max-imum entropy models for statistical machinetranslation.
In Proceedings of the 40th AnnualMeeting on Association for Computational Lin-guistics, pages 295?302.
[Socher et al2011] Richard Socher, Cliff C. Lin,Andrew Y. Ng, and Christopher D. Manning.2011.
Parsing Natural Scenes and Natural Lan-guage with Recursive Neural Networks.
In Pro-ceedings of the 26th International Conference onMachine Learning (ICML).
[Stolcke and others2002] Andreas Stolcke et al2002.
Srilm-an extensible language modelingtoolkit.
In Proceedings of the internationalconference on spoken language processing, vol-ume 2, pages 901?904.
[Sutskever et al2014] Ilya Sutskever, Oriol Vinyals,and Quoc V Le.
2014.
Sequence to sequencelearning with neural networks.
In NIPS.
[Vaswani et al2013] Ashish Vaswani, YinggongZhao, Victoria Fossum, and David Chiang.2013.
Decoding with large-scale neural lan-guage models improves translation.
In EMNLP,pages 1387?1392.
Citeseer.
[Wu and Khudanpur2003] Jun Wu and SanjeevKhudanpur.
2003.
Maximum entropy languagemodeling with non-local dependencies.
Ph.D.thesis, Johns Hopkins University.1576
