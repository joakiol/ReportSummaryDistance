Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67?71,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPNon-textual Event Summarization by Applying Machine Learning toTemplate-based Language GenerationMohit Kumar and Dipanjan Das and Sachin Agarwal and Alexander I. RudnickyLanguage Technologies InstituteCarnegie Mellon University, Pittsburgh, USAmohitkum,dipanjan,sachina,air@cs.cmu.eduAbstractWe describe a learning-based system thatcreates draft reports based on observationof people preparing such reports in a tar-get domain (conference replanning).
Thereports (or briefings) are based on a mixof text and event data.
The latter consistof task creation and completion actions,collected from a wide variety of sourceswithin the target environment.
The reportdrafting system is part of a larger learning-based cognitive assistant system that im-proves the quality of its assistance basedon an opportunity to learn from observa-tion.
The system can learn to accuratelypredict the briefing assembly behavior andshows significant performance improve-ments relative to a non-learning system,demonstrating that it?s possible to createmeaningful verbal descriptions of activityfrom event streams.1 IntroductionWe describe a system for recommending items fora briefing created after a session with a crisis man-agement system in a conference replanning do-main.
The briefing system is learning-based, inthat it initially observes how one set of users cre-ates such briefings then generates draft reports foranother set of users.
This system, the BriefingAssistant(BA), is part of a set of learning-basedcognitive assistants each of which observes usersand learns to assist users in performing their tasksfaster and more accurately.The difference between this work frommost previous efforts, primarily based on text-extraction approaches is the emphasis on learningto summarize event patterns.
This work alsodiffers in its emphasis on learning from userbehavior in the context of a task.Report generation from non-textual sources hasbeen previously explored in the Natural LanguageGeneration (NLG) community in a variety of do-mains, based on, for example, a database of events.However, a purely generative approach is not suit-able in our circumstances, as we want to summa-rize a variety of tasks that the user is performingand present a summary tailored to a target audi-ence, a desirable characteristic of good briefings(Radev and McKeown, 1998).
Thus we approachthe problem by applying learning techniques com-bined with a template-based generation system toinstantiate the briefing-worthy report items.
Thetask of instantiating the briefing-worthy items issimilar to the task of Content Selection (Duboue,2004) in the Generation pipeline however our ap-proach minimizes linguistic involvement.
Ourchoice of a template-based generative system wasmotivated by recent discussions in the NLG com-munity (van Deemter et al, 2005) about the prac-ticality and effectiveness of this approach.The plan of the paper is as follows.
We describerelevant work from existing literature in the nextsection.
Then, we provide brief system descriptionfollowed by experiments and results.
We concludewith a summary of the work.2 Related WorkEvent based summarization has been studied in thesummarization community.
(Daniel et al, 2003)described identification of sub-events in multipledocuments.
(Filatova and Hatzivassiloglou, 2004)mentioned the use of event-based features in ex-tractive summarization and (Wu, 2006; Li et al,2006) describe similar work based on events oc-curring in text.
However, unlike the case at hand,all the work on event-based summarization usedtext as source material.Non-textual summarization has also been ex-plored in the Natural Language Generation (NLG)community within the broad task of generating67reports based on database of events in specificdomains such as medical (Portet et al, 2009),weather (Belz, 2007), sports (Oh and Shrobe,2008) etc.
However, in our case we want to sum-marize a variety of tasks that the user is perform-ing and present a summary to an intended audi-ence (as defined by a report request).Recent advances in NLG research use statis-tical approaches at various stages of processingin the generation pipeline like content selection(Duboue and McKeown, 2003; Barzilay and Lee,2004), probabilistic generation rules (Belz, 2007).Our proposed approach differs from these in thatwe apply machine learning after generation of allthe templates, as a post-processing step, to rankthem for inclusion in the final briefing.
We couldhave used a general purpose template-based gen-eration framework like TG/2 (Busemann, 2005),but since the number of templates and their corre-sponding aggregators is limited, we chose an ap-proach based on string manipulation.We found in our work that an approach basedon modeling individual users and then combiningthe outputs of such models using a voting schemegives the best results, although our approach isdistinguishable from collaborative filtering tech-niques used for driving recommendation systems(Hofmann, 2004).
We believe this is due to thefact that the individual sessions from which rank-ing models are learned, although they range overthe same collection of component tasks, can leadto very different (human-generated) reports.
Thatis, the particular history of a session will affectwhat is considered to be briefing-worthy.3 System OverviewFigure 1: Briefing Assistant Data Flow.The Briefing Assistant Model: We treat thetask of briefing generation in the current domain1as non-textual event-based summarization.
The1More details about the domain and the interaction of BAwith the larger system are mentioned in a longer version ofthe paper (Kumar et al, 2009)Figure 2: The category tree showing the informa-tion types that we expect in a briefing.events are the task creation and task completionactions logged by various cognitive assistants inthe system (so-called specialists).
As part of thedesign phase for the template-based generationcomponent, we identified a set of templates, basedon the actual briefings written by users in a sepa-rate experiment.
Ideally, we would like to adopta corpus-based approach to automatically extractthe templates in the domain, like (Kumar et al,2008), but since the sample briefings available tous were very few, the application of such corpus-based techniques was not necessary.
Based onthis set of templates we identified the patterns thatneeded to be extracted from the event logs in orderto populate the templates.
A ranking model wasalso designed for ordering instantiations of this setof templates and to recommend the top 4 most rel-evant ones for a given session.The overall data flow for BA during a session(runtime) is shown in Figure 1.
The various spe-cialist modules generate task related events thatare logged in a database.
The aggregators operateover this database and emails to extract relevantpatterns.
These patterns in turn are used to popu-late templates which constitute candidate briefingitems.
The candidate briefing items are then or-dered by the ranking module and presented to theuser.Template Design and Aggregators: The setof templates used in the current instantiation ofthe BA was derived from a corpus of human-generated briefings collected in a previous exper-iment using the same crisis management system.The set of templates was designed to cover therange of items that users in that experiment choseto include in their reports corresponding to ninecategories shown in Figure 2.
We found that in-formation can be conveyed at different levels ofgranularity (for example, qualitatively or quantita-tively).
The appropriate choice of granularity for68a particular session is a factor that the system canlearn2.Ranking Model, Classifiers and Features: Theranking module orders candidate templates sothat the four most relevant ones appear in thebriefing draft.
The ranking system consists ofa consensus-based classifier, based on individualclassifier models for each user in the training set.The prediction from each classifier are combined(averaged) to produce a final rank of each tem-plate.We used the Minorthird package (Cohen, 2004)for modeling.
Specifically we allowed the sys-tem to experiment with eleven different learningschemes and select the best one based on cross-validation within the training corpus.
The schemeswere Naive Bayes, Voted Perceptron, SupportVector Machines, Ranking Perceptron, K NearestNeighbor, Decision Tree, AdaBoost, Passive Ag-gressive learner, Maximum Entropy learner, Bal-anced Winnow and Boosted Ranking learner.The features3 used in the system are static ordynamic.
Static features reflect the properties ofthe templates irrespective of the user?s activitywhereas the dynamic features are based on theactual events that took place.
We used the In-formation Gain (IG) metric for feature selection,experimenting with seven different cut-off valuesAll, 20, 15, 10, 7, 5, 4 for the total number of se-lected features.4 Experiments and ResultsExperimental Setup: Two experimental condi-tions were used to differentiate performance basedon knowledge engineering, designated MinusLand performance based on learning, designatedPlusL.4Email Trigger: In the simulated conferencereplanning crisis, the briefing was triggered byan email containing explicit information requests,not known beforehand.
To customize the brief-ing according to the request, a natural languageprocessing module identified the categories of in-formation requested.
The details of the moduleare beyond the scope of the current paper as it2The details of template design process including sampletemplates, categories of templates and details of aggregatorsare presented in (Kumar et al, 2009)3Detailed description of the features are mentioned in(Kumar et al, 2009)4The details of the experimental setup as part of the largercognitive assistant system are presented in (Kumar et al,2009).is external to our system; it took into accountthe template categories we earlier identified.
Fig-ure 4 shows a sample briefing email stimulus.The mapping from the sample email in the figureto the categories is as follows: ?expected atten-dance?
- Property-Session; ?how many sessionshave been rescheduled?, ?how many still need tobe rescheduled?, ?any problems you see as youtry to reschedule?
- Session-Reschedule; ?statusof food service (I am worried about the keynotelunch)?
- Catering Vendors.Training: Eleven expert users5 were asked toprovide training by using the system then generat-ing the end of session briefing using the BA GUI.For this training phase, no item ranking was per-formed by the system, i.e.
all the templates werepopulated by the aggregators and recommenda-tions were random.
The expert user was askedto select the best possible four items and was fur-ther asked to judge the usefulness of the remainingitems.
The resulting training data consists of theactivity log, extracted features and the user-labeleditems.
The trigger message for the training usersdid not contain any specific information request.Test: Subjects were recruited to use the crisismanagement system in MinusL and PlusL condi-tion, although they were not aware of the conditionof the system and they were not involved with theproject.
There were 54 test runs in the MinusLcondition and 47 in the PlusL condition.
Out ofthese runs, 29 subjects in MinusL and 43 subjectsin PlusL wrote a briefing using the BA.
We reportthe evaluation scores for this latter set.Evaluation: The base performance metric isRecall, defined in terms of the briefing templatesrecommended by the system compared to the tem-plates ultimately selected by the user.
We justifythis by noting that Recall can be directly linked tothe expected time savings for the users.
We cal-culate two variants of Recall: Category-based?calculated by matching the categories of the BArecommended templates and user selected onesignoring the granularity and Template-based?calculated by matching the exact templates.
Thefirst metric indicates whether the right category ofinformation was selected and the latter indicateswhether the information was presented at the ap-propriate level of detail.We also performed subjective human evaluation5Members of the project from other groups who wereaware of the scenario and various system functionalities butnot the ML methods69using a panel of three judges.
The judges assignedscores (0-4) to each of the bullets based on thecoverage of the crisis, clarity and conciseness, ac-curacy and the correct level of granularity.
Theywere advised about certain briefing-specific char-acteristics (e.g.
negative bullet items are usefuland hence should be rated favorably).
They werealso asked to provide a global assessment of reportquality, and evaluate the coverage of the requestsin the briefing stimulus email message.
This pro-cedure was very similar to the one used as the basisfor template selection.Experiment: The automatic evaluation met-ric used for the trained system configuration isthe Template-based recall measure.
To obtainthe final system configuration, we automaticallyevaluate the system under the various combina-tions of parameter settings with eleven differentlearning schemes and seven different feature se-lection threshold (as mentioned in previous sec-tions).
Thus a total of 77 different configurationsare tested.
For each configuration, we do a eleven-fold cross-validation between the 11 training usersi.e.
we leave one user as the test user and considerthe remaining ten users as training users.
We av-erage the performance across the 11 test cases andobtain the final score for the configuration.
Wechoose the configuration with the highest score asthe final trained system configuration.
The learnedsystem configuration in the current test includesBalanced Winnow (Littlestone, 1988) and top 7features.Results: We noticed that four users in PlusLcondition took more than 8 minutes to completethe briefing when the median time taken by theusers in PlusL condition was 55 seconds, so wedid not include these users in our analysis in orderto maintain the homogeneity of the dataset.
Thesefour data points were identified as extreme outliersusing a procedure suggested by (NIST, 2008)6.There were no extreme outliers in MinusL condi-tion.Figure 3a shows the Recall values for the Mi-nusL and PlusL conditions.
The learning deltai.e.
the difference between the recall values ofPlusL and MinusL is 33% for Template-based re-call and 21% for Category-based recall.
Thesedifferences are significant at the p < 0.001 level.6Extreme outliers are defined as data points that are out-side the range [Q1?3?IQ,Q3+3?IQ] in a box plot.
Q1 islower quartile, Q3 is upper quartile and IQ is the difference(Q3?Q1) is the interquartile range.The statistical significance for the Template-basedmetric, which was the metric used for select-ing system parameters during the training phase,shows that learning is effective in this case.
Sincethe email stimulus processing module extracts thebriefing categories from the email the Category-based and Template-based recall is expected to behigh for the baseline MinusL case.
In our test, theemail stimuli had 3 category requests and so theCategory-based recall of 0.77 and Template-basedrecall of 0.67 in MinusL is not unexpected.Figure 3b shows the Judges?
panel scores forthe briefings in MinusL and PlusL condition.
Thelearning delta in this case is 3.6% which is alsostatistically significant, at p < 0.05.
The statisticalsignificance of the learning delta validates that thebriefings generated during PlusL conditions arebetter than MinusL condition.
The absolute differ-ence in the qualitative briefing scores between thetwo conditions is small because MinusL users canselect from all candidates, while the recommenda-tions they receive are random.
Consequently theyneed to spend more time in finding the right items.The average time taken for a briefing in MinusLcondition is about 83 seconds and 62 seconds inPlusL (see Figure 3c).
While the time differenceis high (34%) it is not statistically significant dueto high variance.Four of the top 10 most frequently selected fea-tures across users for this system are dynamic fea-tures.
This indicates that the learning model iscapturing the user?s world state and the recom-mendations are related to the underlying events.We believe this validates the process we used togenerate briefing reports from non-textual events.5 SummaryThe Briefing Assistant is not designed to learnthe generic attributes of good reports; rather it?smeant to rapidly learn the attributes of good re-ports within a particular domain and to accom-modate specific information needs on a report-by-report basis.
We found that learned customiza-tion produces reports that are judged to be of bet-ter quality.
We also found that a consensus-basedmodeling approach, which incorporates informa-tion from multiple users, yields the best perfor-mance.
We believe that our approach can be usedto create flexible summarization systems for a va-riety of applications.70(a) (b) (c)Figure 3: (a) Recall values for MinusL and PlusL conditions (b) Briefing scores from the judges?
panelfor MinusL and PlusL conditions (c) Briefing time taken for MinusL and PlusL conditions.Figure 4: Template categories corresponding tothe Briefing request email.ReferencesRegina Barzilay and Lillian Lee.
2004.
Catching thedrift: probabilistic content models, with applicationsto generation and summarization.
In Proceedings ofNAACL.Anja Belz.
2007.
Probabilistic generation of weatherforecast texts.
In Proceedings of HLT-NAACL.Stephan Busemann.
2005.
Ten years after: An updateon TG/2 (and friends).
In Proceedings of EuropeanNatural Language Generation Workshop.William W. Cohen.
2004.
Minorthird: Methods foridentifying names and ontological relations in textusing heuristics for inducing regularities from data.http://minorthird.sourceforge.net, 10th Jun 2009.Naomi Daniel, Dragomir Radev, and Timothy Allison.2003.
Sub-event based multi-document summariza-tion.
In Proceedings of HLT-NAACL.Pablo A. Duboue and Kathleen R. McKeown.
2003.Statistical acquisition of content selection rules fornatural language generation.
In Proceedings ofEMNLP.Pablo A. Duboue.
2004.
Indirect supervised learningof content selection logic.
In Proceedings of INLG.Elena Filatova and Vasileios Hatzivassiloglou.
2004.Event-based extractive summarization.
In Text Sum-marization Branches Out: Proceedings of the ACL-04 Workshop.Thomas Hofmann.
2004.
Latent semantic models forcollaborative filtering.
ACM Transactions on Infor-mation Systems, 22(1):89?115.Mohit Kumar, Dipanjan Das, and Alexander I. Rud-nicky.
2008.
Automatic extraction of briefing tem-plates.
In Proceedings of IJCNLP.Mohit Kumar, Dipanjan Das, Sachin Agarwal, andAlexander I. Rudnicky.
2009.
Non-textual eventsummarization by applying machine learning totemplate-based language generation.
Technical Re-port CMU-LTI-09-012, Language Technologies In-stitute, Carnegie Mellon University.Wenjie Li, Mingli Wu, Qin Lu, Wei Xu, and ChunfaYuan.
2006.
Extractive summarization using inter-and intra- event relevance.
In Proceedings of ACL.Nick Littlestone.
1988.
Learning quickly when irrele-vant attributes abound: A new linear-threshold algo-rithm.
Machine Learning, 2(4):285?318.NIST.
2008.
NIST/SEMATECH e-handbook of statistical methods.http://www.itl.nist.gov/div898/handbook/, 10thJun 2009.Alice Oh and Howard Shrobe.
2008.
Generating base-ball summaries from multiple perspectives by re-ordering content.
In Proceedings of INLG.Franc?ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,Somayajulu Sripada, Yvonne Freer, and CindySykes.
2009.
Automatic generation of textual sum-maries from neonatal intensive care data.
ArtificialIntelligence, 173(7-8):789?816.Dragomir R. Radev and Kathleen R. McKeown.
1998.Generating natural language summaries from mul-tiple on-line sources.
Computational Linguistics,24(3):470?500.Kees van Deemter, Emiel Krahmer, and Mariet The-une.
2005.
Real versus template-based natural lan-guage generation: A false opposition?
Computa-tional Linguistics, 31(1):15?24.Mingli Wu.
2006.
Investigations on event-based sum-marization.
In Proceedings of ACL.71
