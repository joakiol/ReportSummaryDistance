Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 447?455,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsBayesian Inference for Finite-State Transducers?David Chiang1 Jonathan Graehl1 Kevin Knight1 Adam Pauls2 Sujith Ravi11Information Sciences InstituteUniversity of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 902922Computer Science DivisionUniversity of California at BerkeleySoda HallBerkeley, CA 94720AbstractWe describe a Bayesian inference algorithmthat can be used to train any cascade ofweighted finite-state transducers on end-to-end data.
We also investigate the problemof automatically selecting from among mul-tiple training runs.
Our experiments on fourdifferent tasks demonstrate the genericity ofthis framework, and, where applicable, largeimprovements in performance over EM.
Wealso show, for unsupervised part-of-speechtagging, that automatic run selection gives alarge improvement over previous Bayesian ap-proaches.1 IntroductionIn this paper, we investigate Bayesian infer-ence for weighted finite-state transducers (WFSTs).Many natural language models can be capturedby weighted finite-state transducers (Pereira et al,1994; Sproat et al, 1996; Knight and Al-Onaizan,1998; Clark, 2002; Kolak et al, 2003; Mathias andByrne, 2006), which offer several benefits:?
WFSTs provide a uniform knowledge represen-tation.?
Complex problems can be broken down into acascade of simple WFSTs.?
Input- and output-epsilon transitions allowcompact designs.?
Generic algorithms exist for doing inferenceswith WFSTs.
These include best-path de-coding, k-best path extraction, composition,?The authors are listed in alphabetical order.
Please directcorrespondence to Sujith Ravi (sravi@isi.edu).
This workwas supported by NSF grant IIS-0904684 and DARPA contractHR0011-06-C0022.intersection, minimization, determinization,forward-backward training, forward-backwardpruning, stochastic generation, and projection.?
Software toolkits implement these generic al-gorithms, allowing designers to concentrate onnovel models rather than problem-specific in-ference code.
This leads to faster scientific ex-perimentation with fewer bugs.Weighted tree transducers play the same role forproblems that involve the creation and transforma-tion of tree structures (Knight and Graehl, 2005).
Ofcourse, many problems do not fit either the finite-state string or tree transducer framework, but in thispaper, we concentrate on those that do.Bayesian inference schemes have become popu-lar recently in natural language processing for theirability to manage uncertainty about model param-eters and to allow designers to incorporate priorknowledge flexibly.
Task-accuracy results have gen-erally been favorable.
However, it can be time-consuming to apply Bayesian inference methods toeach new problem.
Designers typically build cus-tom, problem-specific sampling operators for ex-ploring the derivation space.
They may factor theirprograms to get some code re-use from one problemto the next, but highly generic tools for string andtree processing are not available.In this paper, we marry the world of finite-statemachines with the world of Bayesian inference, andwe test our methods across a range of natural lan-guage problems.
Our contributions are:?
We describe a Bayesian inference algorithmthat can be used to train any cascade of WFSTson end-to-end data.?
We propose a method for automatic run selec-447tion, i.e., how to automatically select amongmultiple training runs in order to achieve thebest possible task accuracy.The natural language applications we considerin this paper are: (1) unsupervised part-of-speech(POS) tagging (Merialdo, 1994; Goldwater andGriffiths, 2007), (2) letter substitution decipher-ment (Peleg and Rosenfeld, 1979; Knight et al,2006; Ravi and Knight, 2008), (3) segmentation ofspace-free English (Goldwater et al, 2009), and (4)Japanese/English phoneme alignment (Knight andGraehl, 1998; Ravi and Knight, 2009a).
Figure 1shows how each of these problems can be repre-sented as a cascade of finite-state acceptors (FSAs)and finite-state transducers (FSTs).2 Generic EM TrainingWe first describe forward-backward EM training fora single FST M. Given a string pair (v,w) from ourtraining data, we transform v into an FST Mv thatjust maps v to itself, and likewise transform w intoan FST Mw.
Then we compose Mv with M, and com-pose the result with Mw.
This composition followsPereira and Riley (1996), treating epsilon input andoutput transitions correctly, especially with regardsto their weighted interleaving.
This yields a deriva-tion lattice D, each of whose paths transforms v intow.1 Each transition in D corresponds to some tran-sition in the FST M. We run the forward-backwardalgorithm over D to collect fractional counts for thetransitions in M. After we sum fractional counts forall examples, we normalize with respect to com-peting transitions in M, assign new probabilities toM, and iterate.
Transitions in M compete with eachother if they leave the same state with the same inputsymbol, which may be empty ().In order to train an FSA on observed string data,we convert the FSA into an FST by adding an input-epsilon to every transition.
We then convert eachtraining string v into the string pair (, v).
After run-ning the above FST training algorithm, we can re-move all input- from the trained machine.It is straightforward to modify generic training tosupport the following controls:1Throughout this paper, we do not assume that lattices areacyclic; the algorithms described work on general graphs.B:Ea:A b:B A:DA:C=a::D:E b:a:  :CFigure 2: Composition of two FSTs maintaining separatetransitions.Maximum iterations and early stopping.
We spec-ify a maximum number of iterations, and we haltearly if the ratio of log P(data) from one iterationto the next exceeds a threshold (such as 0.99999).Initial point.
Any probabilities supplied on the pre-trained FST are interpreted as a starting point forEM?s search.
If no probabilities are supplied, EMbegins with uniform probabilities.Random restarts.
We can request n random restarts,each from a different, randomly-selected initialpoint.Locking and tying.
Transitions on the pre-trainedFST can be marked as locked, in which case EMwill not modify their supplied probabilities.
Groupsof transitions can be tied together so that their frac-tional counts are pooled, and when normalizationoccurs, they all receive the same probability.Derivation lattice caching.
If memory is available,training can cache the derivation lattices computedin the first EM iteration for all training pairs.
Subse-quent iterations then run much faster.
In our experi-ments, we observe an average 10-fold speedup withcaching.Next we turn to training a cascade of FSTs onend-to-end data.
The algorithm takes as input: (1) asequence of FSTs, and (2) pairs of training strings(v,w), such that v is accepted by the first FST inthe cascade, and w is produced by the last FST.
Thealgorithm outputs the same sequence of FSTs, butwith trained probabilities.To accomplish this, we first compose the suppliedFSTs, taking care to keep the transitions from differ-ent machines separate.
Figure 2 illustrates this with asmall example.
It may thus happen that a single tran-sition in an input FST is represented multiple timesin the composed device, in which case their prob-448ABCD:aREY:r?:c1.
Unsupervised part-of-speech tagging with constrained dictionaryPOS TagsequenceObservedwordsequence2.
Decipherment of letter-substitution cipherEnglishlettersequenceObservedencipheredtext3.
Re-Spacing of English text written without spacesWordsequenceObservedlettersequencew/o spaces4.
Alignment of Japanese/English phoneme sequencesEnglishphonemesequenceJapanesekatakanaphonemesequence26 x 26 tableletter bigram model,learned separatelyconstrained tag?wordsubstitution model tag bigram modelunigram model overwords and non-words deterministic spell-outmapping from each Englishphoneme to each Japanesephoneme sequence of length 1 to 3NNJJJJJJNNVB ??
?NN:fishIN:atVB:fishSYM:a DT:aabbbac ??
?a:Aa:Ba:Cb:A b:B b:CA ARARE AREYAREYO?:?AREY:a?:b?:d?
:r ?:e?:yAE:??:S?:S?
:UFigure 1: Finite-state cascades for five natural language problems.449abilities are tied together.
Next, we run FST train-ing on the end-to-end data.
This involves creatingderivation lattices and running forward-backward onthem.
After FST training, we de-compose the traineddevice back into a cascade of trained machines.When the cascade?s first machine is an FSA,rather than an FST, then the entire cascade is viewedas a generator of strings rather than a transformer ofstrings.
Such a cascade is trained on observed stringsrather than string pairs.
By again treating the firstFSA as an FST with empty input, we can train usingthe FST-cascade training algorithm described in theprevious paragraph.Once we have our trained cascade, we can apply itto new data, obtaining (for example) the k-best out-put strings for an input string.3 Generic Bayesian TrainingBayesian learning is a wide-ranging field.
We focuson training using Gibbs sampling (Geman and Ge-man, 1984), because it has been popularly appliedin the natural language literature, e.g., (Finkel et al,2005; DeNero et al, 2008; Blunsom et al, 2009).Our overall plan is to give a generic algorithmfor Bayesian training that is a ?drop-in replacement?for EM training.
That is, we input an FST cas-cade and data and output the same FST cascadewith trained weights.
This is an approximation to apurely Bayesian setup (where one would always in-tegrate over all possible weightings), but one whichmakes it easy to deploy FSTs to efficiently decodenew data.
Likewise, we do not yet support non-parametric approaches?to create a drop-in replace-ment for EM, we require that all parameters be spec-ified in the initial FST cascade.
We return to this is-sue in Section 5.3.1 Particular CaseWe start with a well-known application of Bayesianinference, unsupervised POS tagging (Goldwaterand Griffiths, 2007).
Raw training text is provided,and each potential corpus tagging corresponds to ahidden derivation of that data.
Derivations are cre-ated and probabilistically scored as follows:1. i?
12.
Choose tag t1 according to P0(t1)3.
Choose word w1 according to P0(w1 | t1)4. i?
i + 15.
Choose tag ti according to?P0(ti | ti?1) + ci?11 (ti?1, ti)?
+ ci?11 (ti?1)(1)6.
Choose word wi according to?P0(wi | ti) + ci?11 (ti,wi)?
+ ci?11 (ti)(2)7.
With probability Pquit, quit; else go to 4.This defines the probability of any given derivation.The base distribution P0 represents prior knowl-edge about the distribution of tags and words, giventhe relevant conditioning context.
The ci?11 are thecounts of events occurring before word i in thederivation (the ?cache?
).When ?
and ?
are large, tags and words are essen-tially generated according to P0.
When ?
and ?
aresmall, tags and words are generated with referenceto previous decisions inside the cache.We use Gibbs sampling to estimate the distribu-tion of tags given words.
The key to efficient sam-pling is to define a sampling operator that makessome small change to the overall corpus derivation.With such an operator, we derive an incrementalformula for re-scoring the probability of an entirenew derivation based on the probability of the oldderivation.
Exchangeability makes this efficient?we pretend like the area around the small change oc-curs at the end of the corpus, so that both old andnew derivations share the same cache.
Goldwaterand Griffiths (2007) choose the re-sampling operator?change the tag of a single word,?
and they derivethe corresponding incremental scoring formula forunsupervised tagging.
For other problems, design-ers develop different sampling operators and derivedifferent incremental scoring formulas.3.2 Generic CaseIn order to develop a generic algorithm, we needto abstract away from these problem-specific de-sign choices.
In general, hidden derivations corre-spond to paths through derivation lattices, so we first450Figure 3: Changing a decision in the derivation lattice.All paths generate the observed data.
The bold path rep-resents the current sample, and the dotted path representsa sidetrack in which one decision is changed.compute derivation lattices for our observed trainingdata through our cascade of FSTs.
A random paththrough these lattices constitutes the initial sample,and we calculate its derivation probability directly.One way to think about a generic small changeoperator is to consider a single transition in the cur-rent sample.
This transition will generally competewith other transitions.
One possible small change isto ?sidetrack?
the derivation to a competing deriva-tion.
Figure 3 shows how this works.
If the sidetrackpath quickly re-joins the old derivation path, then anincremental score can be computed.
However, side-tracking raises knotty questions.
First, what is theproper path continuation after the sidetracking tran-sition is selected?
Should the path attempt to re-jointhe old derivation as soon as possible, and if so, howis this efficiently done?
Then, how can we computenew derivation scores for all possible sidetracks, sothat we can choose a new sample by an appropriateweighted coin flip?
Finally, would such a sampler bereversible?
In order to satisfy theoretical conditionsfor Gibbs sampling, if we move from sample A tosample B, we must be able to immediately get backto sample A.We take a different tack here, moving from point-wise sampling to blocked sampling.
Gao and John-son (2008) employed blocked sampling for POS tag-ging, and the approach works nicely for arbitraryderivation lattices.
We again start with a randomderivation for each example in the corpus.
We thenchoose a training example and exchange its entirederivation lattice to the end of the corpus.
We cre-ate a weighted version of this lattice, called the pro-posal lattice, such that we can approximately samplewhole paths by stochastic generation.
The probabil-ities are based on the event counts from the rest ofthe sample (the cache), and on the base distribution,and are computed in this way:P(r | q) =?P0(r | q) + c(q, r)?
+ c(q)(3)where q and r are states of the derivation lattice, andthe c(?)
are counts collected from the corpus minusthe entire training example being resampled.
This isan approximation because we are ignoring the factthat P(r | q) in general depends on choices madeearlier in the lattice.
The approximation can be cor-rected using the Metropolis-Hastings algorithm, inwhich the sample drawn from the proposal lattice isaccepted only with a certain probability ?
; but Gaoand Johnson (2008) report that ?
> 0.99, so we skipthis step.3.3 Choosing the best derivationsAfter the sampling run has finished, we can choosethe best derivations using two different methods.First, if we want to find the MAP derivations of thetraining strings, then following Goldwater and Grif-fiths (2007), we can use annealing: raise the proba-bilities in the sampling distribution to the 1T power,where T is a temperature parameter, decrease T to-wards zero, and take a single sample.But in practice one often wants to predict theMAP derivation for a new string w?
not containedin the training data.
To approximate the distributionof derivations of w?
given the training data, we aver-age the transition counts from all the samples (afterburn-in) and plug the averaged counts into (3) to ob-tain a single proposal lattice.2 The predicted deriva-tion is the Viterbi path through this lattice.
Call thismethod averaging.
An advantage of this approach isthat the trainer, taking a cascade of FSAs as input,outputs a weighted version of the same cascade, andthis trained cascade can be used on unseen exampleswithout having to rerun training.3.4 ImplementationThat concludes the generic Bayesian training algo-rithm, to which we add the following controls:2A better approximation might have been to build a proposallattice for each sample (after burn-in), and then construct a sin-gle FSA that computes the average of the probability distribu-tions computed by all the proposal lattices.
But this FSA wouldbe rather large.451Number of Gibbs sampling iterations.
We executethe full number specified.Base distribution.
Any probabilities supplied on thepre-trained FST are interpreted as base distributionprobabilities.
If no probabilities are supplied, thenthe base distribution is taken to be uniform.Hyperparameters.
We supply a distinct ?
for eachmachine in the FST cascade.
We do not yet supportdifferent ?
values for different states within a singleFST.Random restarts.
We can request multiple runsfrom different, randomly-selected initial samples.EM-based initial point.
If random initial samplesare undesirable, we can request that the Gibbs sam-pler be initialized with the Viterbi path using param-eter values obtained by n iterations of EM.Annealing schedule.
If annealing is used, it followsa linear annealing schedule with starting and stop-ping temperature specified by the user.EM and Bayesian training for arbitrary FSTcascades are both implemented in the finite-statetoolkit Carmel, which is distributed with sourcecode.3 All controls are implemented as command-line switches.
We use Carmel to carry out the exper-iments in the next section.4 Run SelectionFor both EM and Bayesian methods, different train-ing runs yield different results.
EM?s objective func-tion (probability of observed data) is very bumpy forthe unsupervised problems we work on?differentinitial points yield different trained WFST cascades,with different task accuracies.
Averaging task accu-racies across runs is undesirable, because we want todeploy a particular trained cascade in the real world,and we want an estimate of its performance.
Select-ing the run with the best task accuracy is illegal in anunsupervised setting.
With EM, we have a good al-ternative: select the run that maximizes the objectivefunction, i.e., the likelihood of the observed trainingdata.
We find a decent correlation between this valueand task accuracy, and we are generally able to im-prove accuracy using this run selection method.
Fig-ure 4 shows a scatterplot of 1000 runs for POS tag-ging.
A single run with a uniform start yields 81.8%3http://www.isi.edu/licensed-sw/carmel0.750.80.850.9211200211300211400211500211600211700211800211900212000212100212200Taggingaccuracy(%ofwordtokens)-log P(data)EM (random start)EM (uniform start)Figure 4: Multiple EM restarts for POS tagging.
Eachpoint represents one random restart; the y-axis is tag-ging accuracy and the x-axis is EM?s objective function,?
log P(data).accuracy, while automatic selection from 1000 runsyields 82.4% accuracy.Gibbs sampling runs also yield WFST cascadeswith varying task accuracies, due to random initialsamples and sampling decisions.
In fact, the varia-tion is even larger than what we find with EM.
It isnatural to ask whether we can do automatic run se-lection for Gibbs sampling.
If we are using anneal-ing, it makes sense to use the probability of the fi-nal sample, which is supposed to approximate theMAP derivation.
When using averaging, however,choosing the final sample would be quite arbitrary.Instead, we propose choosing the run that has thehighest average log-probability (that is, the lowestentropy) after burn-in.
The rationale is that the runsthat have found their way to high-probability peaksare probably more representative of the true distri-bution, or at least capture a part of the distributionthat is of greater interest to us.We find that this method works quite well in prac-tice.
Figure 5 illustrates 1000 POS tagging runsfor annealing with automatic run selection, yield-ing 84.7% accuracy.
When using averaging, how-ever, automatic selection from 1000 runs (Figure 6)produces a much higher accuracy of 90.7%.
Thisis better than accuracies reported previously using4520.750.80.850.9235100235150235200235250235300235350235400Taggingaccuracy(%ofwordtokens)-log P(derivation) for final sampleBayesian run (with annealing)Figure 5: Multiple Bayesian learning runs (using anneal-ing with temperature decreasing from 2 to 0.08) for POStagging.
Each point represents one run; the y-axis is tag-ging accuracy and the x-axis is the ?
log P(derivation) ofthe final sample.0.750.80.850.9236800236900237000237100237200237300237400237500237600237700237800237900Taggingaccuracy(%ofwordtokens)-log P(derivation) averaged over all post-burnin samplesBayesian run (using averaging)Figure 6: Multiple Bayesian learning runs (using averag-ing) for POS tagging.
Each point represents one run; they-axis is tagging accuracy and the x-axis is the average?
log P(derivation) over all samples after burn-in.Bayesian methods (85.2% from Goldwater and Grif-fiths (2007), who use a trigram model) and close tothe best accuracy reported on this task (91.8% fromRavi and Knight (2009b), who use an integer linearprogram to minimize the model directly).5 Experiments and ResultsWe run experiments for various natural language ap-plications and compare the task accuracies achievedby the EM and Bayesian learning methods.
Thetasks we consider are:Unsupervised POS tagging.
We adopt the com-mon problem formulation for this task describedby Merialdo (1994), in which we are given a raw24,115-word sequence and a dictionary of legal tagsfor each word type.
The tagset consists of 45 dis-tinct grammatical tags.
We use the same modelingapproach as as Goldwater and Griffiths (2007), us-ing a probabilistic tag bigram model in conjunctionwith a tag-to-word model.Letter substitution decipherment.
Here, the taskis to decipher a 414-letter substitution cipher and un-cover the original English letter sequence.
The taskaccuracy is defined as the percent of ciphertext to-kens that are deciphered correctly.
We work on thesame standard cipher described in previous litera-ture (Ravi and Knight, 2008).
The model consistsof an English letter bigram model, whose probabil-ities are fixed and an English-to-ciphertext channelmodel, which is learnt during training.Segmentation of space-free English.
Givena space-free English text corpus (e.g.,iwalkedtothe...), the task is to segment thetext into words (e.g., i walked to the ...).Our input text corpus consists of 11,378 words,with spaces removed.
As illustrated in Figure 1,our method uses a unigram FSA that models everyletter sequence seen in the data, which includesboth words and non-words (at most 10 letters long)composed with a deterministic spell-out model.In order to evaluate the quality of our segmentedoutput, we compare it against the gold segmentationand compute the word token f-measure.Japanese/English phoneme alignment.
Weuse the problem formulation of Knight andGraehl (1998).
Given an input English/Japanesekatakana phoneme sequence pair, the task is toproduce an alignment that connects each English453MLE BayesianEM prior VB-EM GibbsPOS tagging 82.4 ?
= 10?2, ?
= 10?1 84.1 90.7Letter decipherment 83.6 ?
= 106, ?
= 10?2 83.6 88.9Re-spacing English 0.9 ?
= 10?8, ?
= 104 0.8 42.8Aligning phoneme strings?
100 ?
= 10?2 99.9 99.1Table 1: Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM.
?The output ofEM alignment was used as the gold standard.phoneme to its corresponding Japanese sounds (asequence of one or more Japanese phonemes).
Forexample, given a phoneme sequence pair ((AH BAW T) ?
(a b a u t o)), we have to producethe alignments ((AH ?
a), (B ?
b), (AW ?a u), (T ?
t o)).
The input data consists of2,684 English/Japanese phoneme sequence pairs.We use a model that consists of mappings from eachEnglish phoneme to Japanese phoneme sequences(of length up to 3), and the mapping probabilitiesare learnt during training.
We manually analyzedthe alignments produced by the EM method forthis task and found them to be nearly perfect.Hence, for the purpose of this task we treat the EMalignments as our gold standard, since there are nogold alignments available for this data.In all the experiments reported here, we run EMfor 200 iterations and Bayesian for 5000 iterations(the first 2000 for burn-in).
We apply automatic runselection using the objective function value for EMand the averaging method for Bayesian.Table 1 shows accuracy results for our four tasks,using run selection for both EM and Bayesian learn-ing.
For the Bayesian runs, we compared two infer-ence methods: Gibbs sampling, as described above,and Variational Bayesian EM (Beal and Ghahra-mani, 2003), both of which are implemented inCarmel.
We used the hyperparameters (?, ?)
asshown in the table.
Setting a high value yields a fi-nal distribution that is close to the original one (P0).For example, in letter decipherment we want to keepthe language model probabilities fixed during train-ing, and hence we set the prior on that model tobe very strong (?
= 106).
Table 1 shows that theBayesian methods consistently outperform EM forall the tasks (except phoneme alignment, where EMwas taken as the gold standard).
Each iteration ofGibbs sampling was 2.3 times slower than EM forPOS tagging, and in general about twice as slow.6 DiscussionWe have described general training algorithms forFST cascades and their implementation, and exam-ined the problem of run selection for both EM andBayesian training.
This work raises several interest-ing points for future study.First, is there an efficient method for perform-ing pointwise sampling on general FSTs, and wouldpointwise sampling deliver better empirical resultsthan blocked sampling across a range of tasks?Second, can generic methods similar to the onesdescribed here be developed for cascades of treetransducers?
It is straightforward to adapt our meth-ods to train a single tree transducer (Graehl et al,2008), but as most types of tree transducers arenot closed under composition (Ge?cseg and Steinby,1984), the compose/de-compose method cannot bedirectly applied to train cascades.Third, what is the best way to extend the FST for-malism to represent non-parametric Bayesian mod-els?
Consider the English re-spacing application.
Wecurrently take observed (un-spaced) data and builda giant unigram FSA that models every letter se-quence seen in the data of up to 10 letters, bothwords and non-words.
This FSA has 207,253 tran-sitions.
We also define P0 for each individual transi-tion, which allows a preference for short words.
Thisset-up works fine, but in a nonparametric approach,P0 is defined more compactly and without a word-length limit.
An extension of FSTs along the linesof recursive transition networks may be appropriate,but we leave details for future work.454ReferencesMatthew J. Beal and Zoubin Ghahramani.
2003.
TheVariational Bayesian EM algorithm for incompletedata: with application to scoring graphical modelstructures.
Bayesian Statistics, 7:453?464.Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A Gibbs sampler for phrasal syn-chronous grammar induction.
In Proceedings of ACL-IJCNLP 2009.Alexander Clark.
2002.
Memory-based learning of mor-phology with stochastic transducers.
In Proceedingsof ACL 2002.John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.2008.
Sampling alignment structure under a Bayesiantranslation model.
In Proceedings of EMNLP 2008.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbs sam-pling.
In Proceedings of ACL 2005.Jianfeng Gao and Mark Johnson.
2008.
A comparison ofBayesian estimators for unsupervised Hidden MarkovModel POS taggers.
In Proceedings of EMNLP 2008.Ferenc Ge?cseg and Magnus Steinby.
1984.
Tree Au-tomata.
Akade?miai Kiado?, Budapest.Stuart Geman and Donald Geman.
1984.
Stochastic re-laxation, Gibbs distributions and the Bayesian restora-tion of images.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 6(6):721?741.Sharon Goldwater and Thomas L. Griffiths.
2007.A fully Bayesian approach to unsupervised part-of-speech tagging.
In Proceedings of ACL 2007.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2009.
A Bayesian framework for word segmen-tation: Exploring the effects of context.
Cognition,112(1):21 ?
54.Jonathan Graehl, Kevin Knight, and Jonathan May.
2008.Training tree transducers.
Computational Linguistics,34(3):391?427.Kevin Knight and Yaser Al-Onaizan.
1998.
Transla-tion with finite-state devices.
In Proceedings of AMTA1998.Kevin Knight and Jonathan Graehl.
1998.
Machinetransliteration.
Computational Linguistics, 24(4):599?612.Knight Knight and Jonathan Graehl.
2005.
An overviewof probabilistic tree transducers for natural languageprocessing.
In Proceedings of CICLing-2005.Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-mada.
2006.
Unsupervised analysis for deciphermentproblems.
In Proceedings of COLING-ACL 2006.Okan Kolak, Willian Byrne, and Philip Resnik.
2003.
Agenerative probabilistic OCR model for NLP applica-tions.
In Proceedings of HLT-NAACL 2003.Lambert Mathias and William Byrne.
2006.
Statisti-cal phrase-based speech translation.
In Proceedingsof ICASSP 2006.Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational Linguistics,20(2):155?171.Shmuel Peleg and Azriel Rosenfeld.
1979.
Break-ing substitution ciphers using a relaxation algorithm.Communications of the ACM, 22(11):598?605.Fernando C. N. Pereira and Michael D. Riley.
1996.Speech recognition by composition of weighted finiteautomata.
Finite-State Language Processing, pages431?453.Fernando Pereira, Michael Riley, and Richard Sproat.1994.
Weighted rational transductions and their appli-cations to human language processing.
In ARPA Hu-man Language Technology Workshop.Sujith Ravi and Kevin Knight.
2008.
Attacking deci-pherment problems optimally with low-order n-grammodels.
In Proceedings of EMNLP 2008.Sujith Ravi and Kevin Knight.
2009a.
Learningphoneme mappings for transliteration without paralleldata.
In Proceedings of NAACL HLT 2009.Sujith Ravi and Kevin Knight.
2009b.
Minimized mod-els for unsupervised part-of-speech tagging.
In Pro-ceedings of ACL-IJCNLP 2009.Richard Sproat, Chilin Shih, William Gale, and NancyChang.
1996.
A stochastic finite-state word-segmentation algorithm for Chinese.
ComputationalLinguistics, 22(3):377?404.455
