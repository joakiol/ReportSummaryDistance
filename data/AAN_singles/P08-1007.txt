Proceedings of ACL-08: HLT, pages 55?62,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsMAXSIM: A Maximum Similarity Metricfor Machine Translation EvaluationYee Seng Chan and Hwee Tou NgDepartment of Computer ScienceNational University of SingaporeLaw Link, Singapore 117590{chanys, nght}@comp.nus.edu.sgAbstractWe propose an automatic machine translation(MT) evaluation metric that calculates a sim-ilarity score (based on precision and recall)of a pair of sentences.
Unlike most metrics,we compute a similarity score between itemsacross the two sentences.
We then find a maxi-mum weight matching between the items suchthat each item in one sentence is mapped toat most one item in the other sentence.
Thisgeneral framework allows us to use arbitrarysimilarity functions between items, and to in-corporate different information in our com-parison, such as n-grams, dependency rela-tions, etc.
When evaluated on data from theACL-07 MT workshop, our proposed metricachieves higher correlation with human judge-ments than all 11 automatic MT evaluationmetrics that were evaluated during the work-shop.1 IntroductionIn recent years, machine translation (MT) researchhas made much progress, which includes the in-troduction of automatic metrics for MT evaluation.Since human evaluation of MT output is time con-suming and expensive, having a robust and accurateautomatic MT evaluation metric that correlates wellwith human judgement is invaluable.Among all the automatic MT evaluation metrics,BLEU (Papineni et al, 2002) is the most widelyused.
Although BLEU has played a crucial role inthe progress of MT research, it is becoming evidentthat BLEU does not correlate with human judgementwell enough, and suffers from several other deficien-cies such as the lack of an intuitive interpretation ofits scores.During the recent ACL-07 workshop on statis-tical MT (Callison-Burch et al, 2007), a total of11 automatic MT evaluation metrics were evalu-ated for correlation with human judgement.
The re-sults show that, as compared to BLEU, several re-cently proposed metrics such as Semantic-role over-lap (Gimenez and Marquez, 2007), ParaEval-recall(Zhou et al, 2006), and METEOR (Banerjee andLavie, 2005) achieve higher correlation.In this paper, we propose a new automatic MTevaluation metric, MAXSIM, that compares a pairof system-reference sentences by extracting n-gramsand dependency relations.
Recognizing that differ-ent concepts can be expressed in a variety of ways,we allow matching across synonyms and also com-pute a score between two matching items (such asbetween two n-grams or between two dependencyrelations), which indicates their degree of similaritywith each other.Having weighted matches between items meansthat there could be many possible ways to match, orlink items from a system translation sentence to areference translation sentence.
To match each sys-tem item to at most one reference item, we modelthe items in the sentence pair as nodes in a bipartitegraph and use the Kuhn-Munkres algorithm (Kuhn,1955; Munkres, 1957) to find a maximum weightmatching (or alignment) between the items in poly-nomial time.
The weights (from the edges) of theresulting graph will then be added to determine thefinal similarity score between the pair of sentences.55Although a maximum weight bipartite graph wasalso used in the recent work of (Taskar et al, 2005),their focus was on learning supervised models forsingle word alignment between sentences from asource and target language.The contributions of this paper are as fol-lows.
Current metrics (such as BLEU, METEOR,Semantic-role overlap, ParaEval-recall, etc.)
do notassign different weights to their matches: either twoitems match, or they don?t.
Also, metrics suchas METEOR determine an alignment between theitems of a sentence pair by using heuristics suchas the least number of matching crosses.
In con-trast, we propose weighting different matches dif-ferently, and then obtain an optimal set of matches,or alignments, by using a maximum weight match-ing framework.
We note that this framework is notused by any of the 11 automatic MT metrics in theACL-07 MT workshop.
Also, this framework al-lows for defining arbitrary similarity functions be-tween two matching items, and we could match arbi-trary concepts (such as dependency relations) gath-ered from a sentence pair.
In contrast, most othermetrics (notably BLEU) limit themselves to match-ing based only on the surface form of words.
Finally,when evaluated on the datasets of the recent ACL-07 MT workshop (Callison-Burch et al, 2007), ourproposed metric achieves higher correlation with hu-man judgements than all of the 11 automatic MTevaluation metrics evaluated during the workshop.In the next section, we describe several existingmetrics.
In Section 3, we discuss issues to considerwhen designing a metric.
In Section 4, we describeour proposed metric.
In Section 5, we present ourexperimental results.
Finally, we outline future workin Section 6, before concluding in Section 7.2 Automatic Evaluation MetricsIn this section, we describe BLEU, and the threemetrics which achieved higher correlation resultsthan BLEU in the recent ACL-07 MT workshop.2.1 BLEUBLEU (Papineni et al, 2002) is essentially aprecision-based metric and is currently the standardmetric for automatic evaluation of MT performance.To score a system translation, BLEU tabulates thenumber of n-gram matches of the system translationagainst one or more reference translations.
Gener-ally, more n-gram matches result in a higher BLEUscore.When determining the matches to calculate pre-cision, BLEU uses a modified, or clipped n-gramprecision.
With this, an n-gram (from both the sys-tem and reference translation) is considered to beexhausted or used after participating in a match.Hence, each system n-gram is ?clipped?
by the max-imum number of times it appears in any referencetranslation.To prevent short system translations from receiv-ing too high a score and to compensate for its lackof a recall component, BLEU incorporates a brevitypenalty.
This penalizes the score of a system if thelength of its entire translation output is shorter thanthe length of the reference text.2.2 Semantic Roles(Gimenez and Marquez, 2007) proposed usingdeeper linguistic information to evaluate MT per-formance.
For evaluation in the ACL-07 MT work-shop, the authors used the metric which they termedas SR-Or-*1.
This metric first counts the numberof lexical overlaps SR-Or-t for all the different se-mantic roles t that are found in the system and ref-erence translation sentence.
A uniform average ofthe counts is then taken as the score for the sen-tence pair.
In their work, the different semantic rolest they considered include the various core and ad-junct arguments as defined in the PropBank project(Palmer et al, 2005).
For instance, SR-Or-A0 refersto the number of lexical overlaps between the A0arguments.
To extract semantic roles from a sen-tence, several processes such as lemmatization, part-of-speech tagging, base phrase chunking, named en-tity tagging, and finally semantic role tagging needto be performed.2.3 ParaEvalThe ParaEval metric (Zhou et al, 2006) uses alarge collection of paraphrases, automatically ex-tracted from parallel corpora, to evaluate MT per-formance.
To compare a pair of sentences, ParaE-val first locates paraphrase matches between the two1Verified through personal communication as this is not ev-ident in their paper.56sentences.
Then, unigram matching is performedon the remaining words that are not matched us-ing paraphrases.
Based on the matches, ParaEvalwill then elect to use either unigram precision or un-igram recall as its score for the sentence pair.
Inthe ACL-07 MT workshop, ParaEval based on re-call (ParaEval-recall) achieves good correlation withhuman judgements.2.4 METEORGiven a pair of strings to compare (a system transla-tion and a reference translation), METEOR (Baner-jee and Lavie, 2005) first creates a word alignmentbetween the two strings.
Based on the number ofword or unigram matches and the amount of stringfragmentation represented by the alignment, ME-TEOR calculates a score for the pair of strings.In aligning the unigrams, each unigram in onestring is mapped, or linked, to at most one unigramin the other string.
These word alignments are cre-ated incrementally through a series of stages, whereeach stage only adds alignments between unigramswhich have not been matched in previous stages.
Ateach stage, if there are multiple different alignments,then the alignment with the most number of map-pings is selected.
If there is a tie, then the alignmentwith the least number of unigram mapping crossesis selected.The three stages of ?exact?, ?porter stem?, and?WN synonymy?
are usually applied in sequence tocreate alignments.
The ?exact?
stage maps unigramsif they have the same surface form.
The ?porterstem?
stage then considers the remaining unmappedunigrams and maps them if they are the same af-ter applying the Porter stemmer.
Finally, the ?WNsynonymy?
stage considers all remaining unigramsand maps two unigrams if they are synonyms in theWordNet sense inventory (Miller, 1990).Once the final alignment has been produced, un-igram precision P (number of unigram matches mdivided by the total number of system unigrams)and unigram recall R (m divided by the total numberof reference unigrams) are calculated and combinedinto a single parameterized harmonic mean (Rijsber-gen, 1979):Fmean =P ?
R?P + (1 ?
?
)R (1)To account for longer matches and the amountof fragmentation represented by the alignment, ME-TEOR groups the matched unigrams into as fewchunks as possible and imposes a penalty based onthe number of chunks.
The METEOR score for apair of sentences is:score =[1 ?
?(no.
of chunksm)?
]Fmeanwhere ?(no.
of chunksm)?represents the fragmenta-tion penalty of the alignment.
Note that METEORconsists of three parameters that need to be opti-mized based on experimentation: ?, ?, and ?.3 Metric Design ConsiderationsWe first review some aspects of existing metrics andhighlight issues that should be considered when de-signing an MT evaluation metric.?
Intuitive interpretation: To compensate forthe lack of recall, BLEU incorporates a brevitypenalty.
This, however, prevents an intuitive in-terpretation of its scores.
To address this, stan-dard measures like precision and recall couldbe used, as in some previous research (Baner-jee and Lavie, 2005; Melamed et al, 2003).?
Allowing for variation: BLEU only countsexact word matches.
Languages, however, of-ten allow a great deal of variety in vocabularyand in the ways concepts are expressed.
Hence,using information such as synonyms or depen-dency relations could potentially address the is-sue better.?
Matches should be weighted: Current met-rics either match, or don?t match a pair ofitems.
We note, however, that matches betweenitems (such as words, n-grams, etc.)
should beweighted according to their degree of similar-ity.4 The Maximum Similarity MetricWe now describe our proposed metric, MaximumSimilarity (MAXSIM), which is based on precisionand recall, allows for synonyms, and weights thematches found.57Given a pair of English sentences to be com-pared (a system translation against a referencetranslation), we perform tokenization2 , lemmati-zation using WordNet3, and part-of-speech (POS)tagging with the MXPOST tagger (Ratnaparkhi,1996).
Next, we remove all non-alphanumeric to-kens.
Then, we match the unigrams in the systemtranslation to the unigrams in the reference transla-tion.
Based on the matches, we calculate the recalland precision, which we then combine into a singleFmean unigram score using Equation 1.
Similarly,we also match the bigrams and trigrams of the sen-tence pair and calculate their corresponding Fmeanscores.
To obtain a single similarity score scoresfor this sentence pair s, we simply average the threeFmean scores.
Then, to obtain a single similarityscore sim-score for the entire system corpus, werepeat this process of calculating a scores for eachsystem-reference sentence pair s, and compute theaverage over all |S| sentence pairs:sim-score = 1|S||S|?s=1[1NN?n=1Fmeans,n]where in our experiments, we set N=3, representingcalculation of unigram, bigram, and trigram scores.If we are given access to multiple references, we cal-culate an individual sim-score between the systemcorpus and each reference corpus, and then averagethe scores obtained.4.1 Using N-gram InformationIn this subsection, we describe in detail how wematch the n-grams of a system-reference sentencepair.Lemma and POS match Representing each n-gram by its sequence of lemma and POS-tag pairs,we first try to perform an exact match in both lemmaand POS-tag.
In all our n-gram matching, each n-gram in the system translation can only match atmost one n-gram in the reference translation.Representing each unigram (lipi) at position i byits lemma li and POS-tag pi, we count the num-ber matchuni of system-reference unigram pairswhere both their lemma and POS-tag match.
To findmatching pairs, we proceed in a left-to-right fashion2http://www.cis.upenn.edu/ treebank/tokenizer.sed3http://wordnet.princeton.edu/man/morph.3WNr1 r2 r300.50.750.750.75111s3s2s10.5r1 r2 r30.7511s3s1 s2Figure 1: Bipartite matching.
(in both strings).
We first compare the first systemunigram to the first reference unigram, then to thesecond reference unigram, and so on until we find amatch.
If there is a match, we increment matchuniby 1 and remove this pair of system-reference un-igrams from further consideration (removed itemswill not be matched again subsequently).
Then, wemove on to the second system unigram and try tomatch it against the reference unigrams, once againproceeding in a left-to-right fashion.
We continuethis process until we reach the last system unigram.To determine the number matchbi of bi-gram matches, a system bigram (lsipsi , lsi+1psi+1)matches a reference bigram (lripri , lri+1pri+1) iflsi = lri , psi = pri , lsi+1 = lri+1 , and psi+1 = pri+1 .For trigrams, we similarly determine matchtri bycounting the number of trigram matches.Lemma match For the remaining set of n-gramsthat are not yet matched, we now relax our matchingcriteria by allowing a match if their correspondinglemmas match.
That is, a system unigram (lsipsi)matches a reference unigram (lripri) if lsi = lri .In the case of bigrams, the matching conditions arelsi = lri and lsi+1 = lri+1 .
The conditions for tri-grams are similar.
Once again, we find matches in aleft-to-right fashion.
We add the number of unigram,bigram, and trigram matches found during this phaseto matchuni, matchbi, and matchtri respectively.Bipartite graph matching For the remaining n-grams that are not matched so far, we try to matchthem by constructing bipartite graphs.
During thisphase, we will construct three bipartite graphs, one58each for the remaining set of unigrams, bigrams, andtrigrams.Using bigrams to illustrate, we construct aweighted complete bipartite graph, where each edgee connecting a pair of system-reference bigrams hasa weight w(e), indicating the degree of similaritybetween the bigrams connected.
Note that, withoutloss of generality, if the number of system nodes andreference nodes (bigrams) are not the same, we cansimply add dummy nodes with connecting edges ofweight 0 to obtain a complete bipartite graph withequal number of nodes on both sides.In an n-gram bipartite graph, the similarity score,or the weight w(e) of the edge e connecting a systemn-gram (ls1ps1 , .
.
.
, lsnpsn) and a reference n-gram(lr1pr1 , .
.
.
, lrnprn) is calculated as follows:Si =I(psi , pri) + Syn(lsi , lri)2w(e) = 1nn?i=1Siwhere I(psi , pri) evaluates to 1 if psi = pri , and0 otherwise.
The function Syn(lsi , lri) checkswhether lsi is a synonym of lri .
To determine this,we first obtain the set WNsyn(lsi) of WordNet syn-onyms for lsi and the set WNsyn(lri) of WordNetsynonyms for lri .
Then,Syn(lsi , lri) =??
?1, WNsyn(lsi) ?
WNsyn(lri)6= ?0, otherwiseIn gathering the set WNsyn for a word, we gatherall the synonyms for all its senses and do not re-strict to a particular POS category.
Further, if weare comparing bigrams or trigrams, we impose anadditional condition: Si 6= 0, for 1 ?
i ?
n, else wewill set w(e) = 0.
This captures the intuition thatin matching a system n-gram against a reference n-gram, where n > 1, we require each system tokento have at least some degree of similarity with thecorresponding reference token.In the top half of Figure 1, we show an exampleof a complete bipartite graph, constructed for a setof three system bigrams (s1, s2, s3) and three refer-ence bigrams (r1, r2, r3), and the weight of the con-necting edge between two bigrams represents theirdegree of similarity.Next, we aim to find a maximum weight match-ing (or alignment) between the bigrams such thateach system (reference) bigram is connected to ex-actly one reference (system) bigram.
This maxi-mum weighted bipartite matching problem can besolved in O(n3) time (where n refers to the numberof nodes, or vertices in the graph) using the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957).The bottom half of Figure 1 shows the resultingmaximum weighted bipartite graph, where the align-ment represents the maximum weight matching, outof all possible alignments.Once we have solved and obtained a maximumweight matching M for the bigram bipartite graph,we sum up the weights of the edges to obtain theweight of the matching M : w(M) =?e?M w(e),and add w(M) to matchbi.
From the unigramand trigram bipartite graphs, we similarly calculatetheir respective w(M) and add to the correspondingmatchuni and matchtri.Based on matchuni, matchbi, and matchtri, wecalculate their corresponding precision P and re-call R, from which we obtain their respective Fmeanscores via Equation 1.
Using bigrams for illustra-tion, we calculate its P and R as:P = matchbino.
of bigrams in system translationR = matchbino.
of bigrams in reference translation4.2 Dependency RelationsBesides matching a pair of system-reference sen-tences based on the surface form of words, previ-ous work such as (Gimenez and Marquez, 2007) and(Rajman and Hartley, 2002) had shown that deeperlinguistic knowledge such as semantic roles and syn-tax can be usefully exploited.In the previous subsection, we describe ourmethod of using bipartite graphs for matching of n-grams found in a sentence pair.
This use of bipartitegraphs, however, is a very general framework to ob-tain an optimal alignment of the corresponding ?in-formation items?
contained within a sentence pair.Hence, besides matching based on n-gram strings,we can also match other ?information items?, suchas dependency relations.59Metric Adequacy Fluency Rank Constituent AverageMAXSIMn+d 0.780 0.827 0.875 0.760 0.811MAXSIMn 0.804 0.845 0.893 0.766 0.827Semantic-role 0.774 0.839 0.804 0.742 0.790ParaEval-recall 0.712 0.742 0.769 0.798 0.755METEOR 0.701 0.719 0.746 0.670 0.709BLEU 0.690 0.722 0.672 0.603 0.672Table 1: Overall correlations on the Europarl and News Commentary datasets.
The ?Semantic-role overlap?
metricis abbreviated as ?Semantic-role?.
Note that each figure above represents 6 translation tasks: the Europarl and NewsCommentary datasets each with 3 language pairs (German-English, Spanish-English, French-English).In our work, we train the MSTParser4 (McDon-ald et al, 2005) on the Penn Treebank Wall StreetJournal (WSJ) corpus, and use it to extract depen-dency relations from a sentence.
Currently, we fo-cus on extracting only two relations: subject andobject.
For each relation (ch, dp, pa) extracted, wenote the child lemma ch of the relation (often anoun), the relation type dp (either subject or ob-ject), and the parent lemma pa of the relation (oftena verb).
Then, using the system relations and ref-erence relations extracted from a system-referencesentence pair, we similarly construct a bipartitegraph, where each node is a relation (ch, dp, pa).We define the weight w(e) of an edge e between asystem relation (chs, dps, pas) and a reference rela-tion (chr, dpr, par) as follows:Syn(chs, chr) + I(dps, dpr) + Syn(pas, par)3where functions I and Syn are defined as in the pre-vious subsection.
Also, w(e) is non-zero only ifdps = dpr.
After solving for the maximum weightmatching M , we divide w(M) by the number of sys-tem relations extracted to obtain a precision score P ,and divide w(M) by the number of reference rela-tions extracted to obtain a recall score R. P and Rare then similarly combined into a Fmean score forthe sentence pair.
To compute the similarity scorewhen incorporating dependency relations, we aver-age the Fmean scores for unigrams, bigrams, tri-grams, and dependency relations.5 ResultsTo evaluate our metric, we conduct experiments ondatasets from the ACL-07 MT workshop and NIST4Available at: http://sourceforge.net/projects/mstparserEuroparlMetric Adq Flu Rank Con AvgMAXSIMn+d 0.749 0.786 0.857 0.651 0.761MAXSIMn 0.749 0.786 0.857 0.651 0.761Semantic-role 0.815 0.854 0.759 0.612 0.760ParaEval-recall 0.701 0.708 0.737 0.772 0.730METEOR 0.726 0.741 0.770 0.558 0.699BLEU 0.803 0.822 0.699 0.512 0.709Table 2: Correlations on the Europarl dataset.Adq=Adequacy, Flu=Fluency, Con=Constituent, andAvg=Average.News CommentaryMetric Adq Flu Rank Con AvgMAXSIMn+d 0.812 0.869 0.893 0.869 0.861MAXSIMn 0.860 0.905 0.929 0.881 0.894Semantic-role 0.734 0.824 0.848 0.871 0.819ParaEval-recall 0.722 0.777 0.800 0.824 0.781METEOR 0.677 0.698 0.721 0.782 0.720BLEU 0.577 0.622 0.646 0.693 0.635Table 3: Correlations on the News Commentary dataset.MT 2003 evaluation exercise.5.1 ACL-07 MT WorkshopThe ACL-07 MT workshop evaluated the transla-tion quality of MT systems on various translationtasks, and also measured the correlation (with hu-man judgement) of 11 automatic MT evaluationmetrics.
The workshop used a Europarl dataset and aNews Commentary dataset, where each dataset con-sisted of English sentences (2,000 English sentencesfor Europarl and 2,007 English sentences for NewsCommentary) and their translations in various lan-guages.
As part of the workshop, correlations ofthe automatic metrics were measured for the tasks60of translating German, Spanish, and French into En-glish.
Hence, we will similarly measure the correla-tion of MAXSIM on these tasks.5.1.1 Evaluation CriteriaFor human evaluation of the MT submissions,four different criteria were used in the workshop:Adequacy (how much of the original meaning is ex-pressed in a system translation), Fluency (the trans-lation?s fluency), Rank (different translations of asingle source sentence are compared and rankedfrom best to worst), and Constituent (some con-stituents from the parse tree of the source sentenceare translated, and human judges have to rank thesetranslations).During the workshop, Kappa values measured forinter- and intra-annotator agreement for rank andconstituent are substantially higher than those foradequacy and fluency, indicating that rank and con-stituent are more reliable criteria for MT evaluation.5.1.2 Correlation ResultsWe follow the ACL-07 MT workshop process ofconverting the raw scores assigned by an automaticmetric to ranks and then using the Spearman?s rankcorrelation coefficient to measure correlation.During the workshop, only three automatic met-rics (Semantic-role overlap, ParaEval-recall, andMETEOR) achieve higher correlation than BLEU.We gather the correlation results of these metricsfrom the workshop paper (Callison-Burch et al,2007), and show in Table 1 the overall correlationsof these metrics over the Europarl and News Com-mentary datasets.
In the table, MAXSIMn representsusing only n-gram information (Section 4.1) for ourmetric, while MAXSIMn+d represents using both n-gram and dependency information.
We also showthe breakdown of the correlation results into the Eu-roparl dataset (Table 2) and the News Commentarydataset (Table 3).
In all our results for MAXSIMin this paper, we follow METEOR and use ?=0.9(weighing recall more than precision) in our calcu-lation of Fmean via Equation 1, unless otherwisestated.The results in Table 1 show that MAXSIMn andMAXSIMn+d achieve overall average (over the fourcriteria) correlations of 0.827 and 0.811 respec-tively.
Note that these results are substantiallyMetric Adq Flu AvgMAXSIMn+d 0.943 0.886 0.915MAXSIMn 0.829 0.771 0.800METEOR (optimized) 1.000 0.943 0.972METEOR 0.943 0.886 0.915BLEU 0.657 0.543 0.600Table 4: Correlations on the NIST MT 2003 dataset.higher than BLEU, and in particular higher than thebest performing Semantic-role overlap metric in theACL-07 MT workshop.
Also, Semantic-role over-lap requires more processing steps (such as basephrase chunking, named entity tagging, etc.)
thanMAXSIM.
For future work, we could experimentwith incorporating semantic-role information intoour current framework.
We note that the ParaEval-recall metric achieves higher correlation on the con-stituent criterion, which might be related to the factthat both ParaEval-recall and the constituent crite-rion are based on phrases: ParaEval-recall tries tomatch phrases, and the constituent criterion is basedon judging translations of phrases.5.2 NIST MT 2003 DatasetWe also conduct experiments on the test data(LDC2006T04) of NIST MT 2003 Chinese-Englishtranslation task.
For this dataset, human judgementsare available on adequacy and fluency for six sys-tem submissions, and there are four English refer-ence translation texts.Since implementations of the BLEU and ME-TEOR metrics are publicly available, we scorethe system submissions using BLEU (version 11bwith its default settings), METEOR, and MAXSIM,showing the resulting correlations in Table 4.
ForMETEOR, when used with its originally proposedparameter values of (?=0.9, ?=3.0, ?=0.5), whichthe METEOR researchers mentioned were based onsome early experimental work (Banerjee and Lavie,2005), we obtain an average correlation value of0.915, as shown in the row ?METEOR?.
In the re-cent work of (Lavie and Agarwal, 2007), the val-ues of these parameters were tuned to be (?=0.81,?=0.83, ?=0.28), based on experiments on the NIST2003 and 2004 Arabic-English evaluation datasets.When METEOR was run with these new parame-ter values, it returned an average correlation value of610.972, as shown in the row ?METEOR (optimized)?.MAXSIM using only n-gram information(MAXSIMn) gives an average correlation valueof 0.800, while adding dependency information(MAXSIMn+d) improves the correlation value to0.915.
Note that so far, the parameters of MAXSIMare not optimized and we simply perform uniformaveraging of the different n-grams and dependencyscores.
Under this setting, the correlation achievedby MAXSIM is comparable to that achieved byMETEOR.6 Future WorkIn our current work, the parameters of MAXSIM areas yet un-optimized.
We found that by setting ?=0.7,MAXSIMn+d could achieve a correlation of 0.972on the NIST MT 2003 dataset.
Also, we have barelyexploited the potential of weighted similarity match-ing.
Possible future directions include adding se-mantic role information, using the distance betweenitem pairs based on the token position within eachsentence as additional weighting consideration, etc.Also, we have seen that dependency relations help toimprove correlation on the NIST dataset, but not onthe ACL-07 MT workshop datasets.
Since the accu-racy of dependency parsers is not perfect, a possiblefuture work is to identify when best to incorporatesuch syntactic information.7 ConclusionIn this paper, we present MAXSIM, a new auto-matic MT evaluation metric that computes a simi-larity score between corresponding items across asentence pair, and uses a bipartite graph to obtainan optimal matching between item pairs.
This gen-eral framework allows us to use arbitrary similarityfunctions between items, and to incorporate differ-ent information in our comparison.
When evaluatedfor correlation with human judgements, MAXSIMachieves superior results when compared to currentautomatic MT evaluation metrics.ReferencesS.
Banerjee and A. Lavie.
2005.
METEOR: An auto-matic metric for MT evaluation with improved corre-lation with human judgments.
In Proceedings of theWorkshop on Intrinsic and Extrinsic Evaluation Mea-sures for MT and/or Summarization, ACL05, pages65?72.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz, andJ.
Schroeder.
2007.
(meta-) evaluation of machinetranslation.
In Proceedings of the Second Workshop onStatistical Machine Translation, ACL07, pages 136?158.J.
Gimenez and L. Marquez.
2007.
Linguistic featuresfor automatic evaluation of heterogenous MT systems.In Proceedings of the Second Workshop on StatisticalMachine Translation, ACL07, pages 256?264.H.
W. Kuhn.
1955.
The hungarian method for the assign-ment problem.
Naval Research Logistic Quarterly,2(1):83?97.A.
Lavie and A. Agarwal.
2007.
METEOR: An auto-matic metric for MT evaluation with high levels of cor-relation with human judgments.
In Proceedings of theSecond Workshop on Statistical Machine Translation,ACL07, pages 228?231.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProceedings of ACL05, pages 91?98.I.
D. Melamed, R. Green, and J. P. Turian.
2003.
Preci-sion and recall of machine translation.
In Proceedingsof HLT-NAACL03, pages 61?63.G.
A. Miller.
1990.
WordNet: An on-line lexi-cal database.
International Journal of Lexicography,3(4):235?312.J.
Munkres.
1957.
Algorithms for the assignment andtransportation problems.
Journal of the Society for In-dustrial and Applied Mathematics, 5(1):32?38.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.BLEU: A method for automatic evaluation of machinetranslation.
In Proceedings of ACL02, pages 311?318.M.
Rajman and A. Hartley.
2002.
Automatic ranking ofMT systems.
In Proceedings of LREC02, pages 1247?1253.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proceedings of EMNLP96,pages 133?142.C.
Rijsbergen.
1979.
Information Retrieval.
Butter-worths, London, UK, 2nd edition.B.
Taskar, S. Lacoste-Julien, and D. Klein.
2005.
A dis-criminative matching approach to word alignment.
InProceedings of HLT/EMNLP05, pages 73?80.L.
Zhou, C. Y. Lin, and E. Hovy.
2006.
Re-evaluatingmachine translation results with paraphrase support.In Proceedings of EMNLP06, pages 77?84.62
