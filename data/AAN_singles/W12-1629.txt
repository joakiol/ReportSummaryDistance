Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 207?216,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsUsing Group History to Identify Character-directed Utterances inMulti-child InteractionsHannaneh Hajishirzi, Jill F. Lehman, and Jessica K. Hodginshannaneh.hajishirzi, jill.lehman, jkh@disneyresearch.comAbstractAddressee identification is an element of alllanguage-based interactions, and is critical forturn-taking.
We examine the particular prob-lem of identifying when each child playing aninteractive game in a small group is speak-ing to an animated character.
After analyzingchild and adult behavior, we explore a familyof machine learning models to integrate au-dio and visual features with temporal groupinteractions and limited, task-independent lan-guage.
The best model performs identificationabout 20% better than the model that uses theaudio-visual features of the child alone.1 IntroductionMulti-party interaction between a group of partic-ipants and an autonomous agent is an importantbut difficult task.
Key problems include identify-ing when speech is present, who is producing it, andto whom it is directed, as well as producing an ap-propriate response to its intended meaning.
Solvingthese problems is made more difficult when someor all of the participants are young children, whohave high variability in language, knowledge, andbehavior.
Prior research has tended to look at singlechildren (Oviatt, 2000; Black et al, 2009) or multi-person groups of adults (Bohus and Horvitz, 2009a).We are interested in interactions between animatedor robotic characters and small groups of four to tenyear old children.
The interaction can be brief butshould be fun.Here we focus specifically on the question of de-ciding whether or not a child?s utterance is directedto the character, a binary form of the addresseeidentification (AID) problem.
Our broad goals inthis research are to understand how children?s be-havior in group interaction with a character differsfrom adults?, how controllable aspects of the charac-ter and physical environment determine participants?behavior, and how an autonomous character can takeadvantage of these regularities.We collected audio and video data of groups of upto four children and adults playing language-basedgames with animated characters that were under lim-ited human control.
An autonomous character canmake two kinds of AID mistakes: failing to detectwhen it is being spoken to, and acting as if it hasbeen spoken to when it has not.
The former can belargely prevented by having the character use exam-ples of the language that it can recognize as part ofthe game.
Such exemplification cannot prevent thesecond kind of mistake, however.
It occurs, for ex-ample, when children confer to negotiate the nextchoice, respond emotionally to changes in the gamestate, or address each other without making eye con-tact.
As a result, models that use typical audio-visual features to decide AID will not be adequatein multi-child environments.
By including tempo-ral conversational interactions between group mem-bers, however, we can both detect character-directedutterances and ignore the remainder about 20% bet-ter than simple audio-visual models alone, with lessthan 15% failure when being spoken to, and about20% failure when not addressed.2 Related WorkOur models explore the use of multimodal featuresthat represent activities among children and adultsinteracting with a character over time.
Prior researchhas tended to look at single children or multi-person207groups of adults and has typically used a less inclu-sive set of features (albeit in decisions that go be-yond simple AID).Use of multimodal features rests on early workby Duncan and Fiske who explored how gaze andhead and body orientation act as important predic-tors of AID in human-human interactions (Duncanand Fiske, 1977).
Bakx and colleagues showed thataccuracy can be improved by augmenting facial ori-entation with acoustic features in an agent?s interac-tions with an adult dyad (Bakx et al, 2003).
Oth-ers have studied the cues that people use to showtheir interest in engaging in a conversation (Gra-vano and Hirschberg, 2009) and how gesture sup-ports selection of the next speaker in turn-taking(Bergmann et al, 2011).
Researchers have alsolooked at combining visual features with lexical fea-tures like the parseability of the utterance (Katzen-maier et al, 2004), the meaning of the utterance, flu-ency of speech, and use of politeness terms (Terkenet al, 2007), and the dialog act (Matsusaka et al,2007).
However, all use hand-annotated data in theiranalysis without considering the difficulty of auto-matically deriving the features.
Finally, prosodicfeatures have been combined with visual and lexi-cal features in managing the order of speaking andpredicting the end-of-turn in multi-party interactions(Lunsford and Oviatt, 2006; Chen and Harper, 2009;Clemens and Diekhaus, 2009).Work modeling the temporal behavior of thespeaker includes the use of adjacent utterances (e.g.,question-answer) to study the dynamics of the dialog(Jovanovic et al, 2006), the prediction of addresseebased on the addressee and dialog acts in previoustime steps (Matsusaka et al, 2007), and the use ofthe speaker?s features over time to predict the qual-ity of an interaction between a robot and single adult(Fasel et al, 2009).Horvitz and Bohus have the most complete (anddeployed) model, combining multimodal featureswith temporal information using a system for multi-party dynamic interaction between adults and anagent (Bohus and Horvitz, 2009a; Bohus andHorvitz, 2009b).
In (Bohus and Horvitz, 2009a)the authors describe the use of automatic sensors forvoice detection, face detection, head position track-ing, and utterance length.
They do not model tem-poral or group interactions in determining AID, al-though they do use a temporal model for the inter-action as a whole.
In (Bohus and Horvitz, 2009b)the authors use the speaker?s features for the cur-rent and previous time steps, but do not jointly trackthe attention or behavior of all the participants in thegroup.
Moreover, their model assumes that the sys-tem is engaged with at most one participant at a time,which may be a valid conversational expectation foradults but is unlikely to hold for children.
In (Bo-hus and Horvitz, 2011), the authors make a similarassumption regarding turn-taking, which is built ontop of the AID module.3 User StudyWe use a Wizard of Oz testbed and a scripted mixof social dialog and interactive game play to explorethe relationship between controllable features of thecharacter and the complexity of interacting via lan-guage with young children.
The games are hosted bytwo animated characters (Figure 1, left).
Oliver, theturtle, is the main focus of the social interactions andalso handles repair subdialogs when a game does notrun smoothly.
Manny, the bear, provides comic re-lief and controls the game board, making him thefocus of participants?
verbal choices during gameplay.
The game appears on a large flat-screen dis-play about six feet away from participants who standside-by-side behind a marked line.
Audio and videoare captured, the former with both close-talk micro-phones and a linear microphone array.Oliver and Manny host two games designed to befun and easy to understand with little explicit in-struction.
In Madlibs, participants help create a shortmovie by repeatedly choosing one everyday objectfrom a set of three.
The objects can be seen on theboard and Oliver gives examples of appropriate re-ferring phrases when prompting for a choice.
In Fig-ure 1, for example, he asks, ?Should our movie havea robot, a monster, or a girl in it??
After five sets ofobjects are seen, the choices appear in silly contextsin a short animation; for instance, a robot babysit-ter may serve a chocolate pickle cake for lunch.
InMix-and-Match (MnM), participants choose appareland accessories to change a girl?s image in unusualways (Figure 1, right).
MnM has six visually avail-able objects and no verbal examples from Oliver, ex-cept in repair subdialogs.
It is a faster-paced game208Figure 1: Manny and Oliver host Madlibs and a family play Mix-and-Matchwith the immediate reward of a silly change to thebabysitter?s appearance whenever a referring phraseis accepted by the wizard.The use of verbal examples in Madlibs is expectedto influence the children?s language, potentially in-creasing the accuracy of speech recognition and ref-erent resolution in an autonomous system.
The costof exemplification is slower pacing because childrenmust wait while the choices are named.
To compen-sate, we offer only a small number of choices perturn.
Removing exemplification, as in MnM, createsfaster pacing and more variety of choice each turn,which is more fun but also likely to increase threetypes of problematic phenomena: out-of-vocabularychoices (?the king hat?
rather than ?the crown?
),side dialogs to establish a referring lexical item orphrase (?Mommy, what is that thing??
), and the useof weak naming strategies based on physical fea-tures (?that green hand?
).The two games are part of a longer scripted se-quence of interactions that includes greetings, good-byes, and appropriate segues.
Overall, the languagethat can be meaningfully directed to the characters isconstrained to a small social vocabulary, yes/no re-sponses, and choices that refer to the objects on theboard.
The wizard?s interface reflects these expec-tations with buttons that come and go as a functionof the game state.
For example, yes and no buttonsare available to the wizard after Oliver asks, ?Willyou help me??
while robot, monster, and girl but-tons are available after he asks, ?Should our moviehave a robot, a monster, or a girl in it??
The wiz-ard also has access to persistent buttons to indicate along silence, unclear speech, multiple people speak-ing, or a clear reference to an object not on the board.These buttons launch Oliver?s problem-specific re-pair behaviors.
The decomposition of functional-ity in the interface anticipates replacing the wizard?svarious roles as voice activity detector, addresseeidentifier, speech recognizer, referent resolver, anddialog manager in an autonomous implementation.Although meaningful language to the charactersis highly constrained, language to other participantscan be about anything.
In particular, both gamesestablish an environment in which language amongparticipants is likely to be about negotiating the turn(?Brad, do you want to change anything??
), nego-tiating the choice (?Billy, don?t do the boot?)
orcommenting on the result (?her feet look strange?
).Lacking examples of referring phrases by Oliver,MnM also causes side dialogs to discuss how ob-jects should be named.
Naming discussions, choicenegotiation, and comments define the essential dif-ficulty in AID for our testbed; they are all likely toinclude references to objects on the board withoutthe intention of changing the game state.3.1 Data collection and annotationTwenty-seven compensated children (14 male, 13female) and six adult volunteers participated.
Chil-dren ranged in age from four to ten with a mean of6.4 years.
All children spoke English as a first lan-guage.
Groups consisted of up to four people andalways contained either a volunteer adult or the ex-perimenter the first time through the activities.
If theexperimenter participated, she did not make gamechoices.
Volunteer adults were instructed to sup-port their children?s participation in whatever wayfelt natural for their family.
When time permitted,children were given the option of playing one or both209games again.
Those who played a second time wereallowed to play alone or in combination with others,with or without an adult.
Data was collected for 25distinct groups, the details of which are provided inTable 5 in the Appendix.Data from all sessions was hand-annotated withrespect to language, gesture, and head orientation.Labels were based on an initial review of the videos,prior research on AID and turn-taking in adults, andthe ability to detect candidate features in our phys-ical environment.
A second person segmented andlabeled approximately one third of each session forinter-annotator comparison.
The redundant thirdwas assigned randomly from the beginning, middle,or end of the session in order to balance across socialinteractions, Madlibs choices, and MnM choices.Labels were considered to correspond to the sameaudio or video sequence if the segments overlappedby at least 50%.For language annotations, audio from the close-talk microphones was used with the video and seg-mented into utterances based on pauses of at least50 msec.
Typical mispronunciations for young chil-dren (e.g., word initial /W/ for /R/) were transcribedas normal words in plain text; non-standard errorswere transcribed phonologically.
Every utterancewas also labeled as being directed to the character(CHAR) or not to the character (NCHAR).
Secondannotators segmented the audio and assigned ad-dressee but did not re-transcribe the speech.
Inter-annotator agreement for segmentation was 95%(?
= .91), with differences resulting from onlyone annotator segmenting properly around pausesor only one being able to distinguish a given child?svoice among the many who were talking.
For seg-ments coded by both annotators, CHAR/NCHARagreement was 94% (?
= .89).For gesture annotations, video segments weremarked for instances of pointing, emphasis, andhead shaking yes and no.
Emphatic gestures weredefined as hand or arm movements toward the screenthat were not pointing or part of grooming motions.Annotators agreed on the existence of gestures 74%of the time (?
= .49), but when both annotators in-terpreted movement as a gesture, they used the samelabel 98% of the time (?
= .96).For orientation, video was segmented when thehead turned away from the screen and when it turnedback.
Rather than impose an a priori duration or an-gle, annotators were told to use the turn-away labelwhen the turn was associated with meaningful in-teraction with a person or object, but not for brief,incidental head movements.
Adults could also havesegments that were labeled as head-incline if theybent to speak to children.
Annotators agreed on theexistence of these orientation changes 83% of thetime (?
= .62); disagreements may represent simpledifferences in accuracy or differences in judgmentsabout whether a movement denoted a shift in atten-tion.
Orientation changes coded by both annotatorshad the same label 92% of the time (?
= .85).The annotated sessions are a significant portionof the training and test data used for our models.Although these data reflect some idiosyncracy dueto human variability in speech perception, gesturerecognition, and, possibly, the attribution of inten-tion to head movements, they show extremely goodagreement with regard to whether participants weretalking to the character.
Even very young chil-dren in group situations give signals in their speechand movements that allow other people to determineconsistently to whom they are speaking.3.2 Analysis of behaviorAs intended, children did most of the talking(1371/1895 utterances, 72%), spoke to the charac-ters the majority of the time (967/1371, 71%), andmade most of the object choices (666/683, 98%).Adults generally acted in support roles, with 88%of all adult utterances (volunteers and experimenter)directed to the children.The majority of children?s CHAR utterances(71%) were object choices.
Although the wizardin our study was free to accept any unambiguousphrase as a valid choice, an automated system mustcommit to a fixed lexicon.
In general, the largerthe lexicon, the smaller the probability that a ref-erence will be out-of-vocabulary, but the greater theprobability that a reference could be considered am-biguous and require clarification.
The lexical entryfor each game object contains the simple descrip-tion given to the illustrator (?alien hands,?
?pickle?
)and related terms from WordNet (Fellbaum, 1998)likely to be known by young children (see Table 3 inthe Appendix for examples).
In anticipation of weaknaming strategies, MnM entries also contain salient210visual features based on the artwork (like color), aswell as the body part the object would replace, whereapplicable.
Entries for Madlibs objects average 2.75words; entries for MnM average 5.8.
With thesedefinitions, only 37/666 (6%) of character-directedchoices would have been out-of-vocabulary for aword-spotting speech recognizer with human accu-racy.
However, Oliver?s use of exemplification has astrong effect.
In Madlibs, 98% of children?s choiceswere unambiguous repetitions of example phrases.In MnM, 92% of choices contained words in the lex-icon, but only 28% indexed a unique object.Recognition of referring phrases should be a fac-tor in making AID decisions only if it helps to discri-mate CHAR from NCHAR utterances.
Object refer-ences occurred in 62% of utterances to the charac-ters and only 25% of utterances addressed to otherparticipants, but again, Oliver?s exemplification mat-tered.
About 20% of NCHAR utterances from chil-dren in both games and from adults in Madlibs con-tained object references.
In MnM, however, a thirdof adults?
NCHAR utterances contained object ref-erences as they responded to children?s requests fornaming advice.Language is not the only source of informationavailable from our testbed.
We know adults use botheye gaze and gesture to modulate turn-taking andsignal addressee in advance of speech.
Because non-verbal mechanisms for establishing joint attentionoccur early in language development, even childrenas young as four might use such signals consistently.Although we use head movement as an approxima-tion of eye gaze, we positioned participants side-by-side to make such movements necessary for eye con-tact.
Unfortunately, the game board constituted toostrong a ?situational attractor?
(Bakx et al, 2003).As in their kiosk environment, our adults orientedtoward the screen much of the time (68%) they weretalking to other participants.
Children violated con-versational convention more often, orienting towardthe screen for 82% of NCHAR utterances.Gesture information is also available from thevideo data and reveals distinct patterns of usagefor children and adults.
The average number ofgestures/utterance was more than twice as high inadults.
Children were more likely to use empha-sis gestures when they were talking to the charac-ters; adults hardly used them at all.
Children?s ges-tures overlapped with their speech almost 80% ofthe time, but adult?s gestures overlapped with theirspeech only half the time.
Moreover, when childrenpointed while talking they were talking to the char-acters, but when adults pointed while talking theywere talking to the children.
Finally, adults shooktheir heads when they were talking to children butnot when they were talking to the characters, whilechildren shook their heads when talking to both.To maintain an engaging experience, object refer-ences addressed to the character should be treated aspossible choices, while object references addressedto other participants should not produce action.
In-teractions that violate this rule too often will befrustrating rather than fun.
While exemplificationin Madlibs virtually eliminated out-of-vocabularychoices, it could not eliminate detectable object ref-erences that were not directed to the characters.
Inboth games, such references were often accompa-nied by other signs that the character was being ad-dressed, like orientation toward the board and point-ing.
Using all the cues available, human annotatorswere almost always able to agree on who was beingaddressed.
The next section looks at how well anautonomous agent can perform AID using only thecues it can sense, if it could sense them with humanlevels of accuracy.4 Models for Addressee ClassificationWe cast the problem of automatically identifyingwhether an utterance is addressed to the character(and so may result in a character action) as a binaryclassification problem.
We build and test a familyof models based on distinct sources of informationin order to understand where the power is comingfrom and make it easier for other researchers to com-pare to our approach.
All models in the family areconstructed from Support Vector Machines (SVM)(Cortes and Vapnik, 1995), and use the multimodalfeatures in Table 1 to map each 500 msec time sliceof a child?s speech to CHAR or NCHAR.
This ba-sic feature vector combines a subset of the hand-annotated data (Audio and Visual) with automati-cally generated data (Prosodic and System events).We use a time slice rather than a lexical or semanticboundary for forcing a judgment because in a real-time interaction decisions must be made even when211Audio speech: presence/absenceProsodic pitch: low/medium/highspeech power: low/medium/highSystem event character prompt: presence/absenceVisual orientation: head turn away/backgesture: pointing/emphasisTable 1: Basic featureslexical or semantic events do not occur.We consider three additional sources of informa-tion: group behavior, history, and lexical usage.Group behavior ?
the speech, prosody, head orien-tation, and gestures of other participants ?
is impor-tant because most of the speech that is not directedto the characters is directed to a specific person inthe group.
History is important both because the sideconversations unfold gradually and because it allowsus to capture the changes to and continuity of thespeaker?s features across time slices.
Finally, we uselexical features to represent whether the participant?sspeech contains words from a small, predefined vo-cabulary of question words, greetings, and discoursemarkers (see Appendix).
Because the behavioralanalysis showed significant use of words referringto choice objects during both CHAR and NCHARutterances, we do not consider those words in deter-mining AID.
Indeed, we expect the AID decision tosimplify the task of the speech recognizer by helpingthat component ignore NCHAR utterances entirely.The full set of models is described by adding tothe basic vector zero or more of group (g), word (w),or history (h) features.
We use the notation g[+/-]w[+/-]h[(time parameters)/-] to indicate the pres-ence or absence of a knowledge source.
The timeparameters vary and will be explained in the con-text of particular models, below.
Although we haveexplored a larger portion of the total model space,we limit our discussion here to representative mod-els (including the best model) that will demonstratethe effect of each kind of information on the twomain goals of AID: responding to CHAR utterancesand not responding to NCHAR utterances.
Thereare eight models of interest, the first four of whichisolate individual knowledge sources:The Basic model (g-w-h-) is an SVM classifiertrained to generate binary CHAR/NCHAR valuesbased solely on the features in Table 1.
It representsthe ability to predict whether a child is talking to theA1,t?GN?P1?P2?P3?P4?t-??1,t,t+1?P1?P1?t+K?t-??M?T?t-??N-??1,t-??N,?t-??N+1?P1?P1?P2?P3?P4?G1?Individual?sub-??model?Par?cipants?sub-?
?models?Figure 2: The two-layer Group-History model mapsgroup and individual behavior over a fixed window oftime slices to a CHAR/NCHAR decision at time t. Thedecision at time t (A1,t) is based on the participant?s ba-sic features (P1), the output of the individual?s submodel(T ) ?
which encapsulates the history of the individualfor M previous and K subsequent time slices ?
and theoutput of N participant submodels, each of which con-tributes a value based on three times slices.character independent of speech recognition and fo-cused on only 500 msecs of that child?s behavior.The Group model (g+w-h-) incorporates groupinformation, but ignores temporal and lexical be-havior.
This SVM is trained on an extended featurevector that includes the basic features for the otherparticipants in the group together with the speaker?sfeature vector at each time slice.The History model (g-w-h(N ,K)) considers onlythe speaker?s basic features, but includesN previousand K subsequent time slices surrounding the slicefor which we make the CHAR/NCHAR decision.1The Word model (g-w+h-) extends the basic vec-tor to include features for the presence or absence ofquestion words, greetings, and discourse markers.The next three models combine pairs of knowl-edge sources.
The Group-Word (g+w+h-) andHistory-Word (g-w+h(N ,K)) models are straight-1A History model combining the speaker?s basic vector overthe previous and current time slices (N = 4 and K = 0) out-performed a Conditional Random Fields (Lafferty et al, 2001)model with N + 1 nodes representing consecutive time sliceswhere the last node is conditioned on the previous N nodes.212forward extensions of their respective base models,created by adding lexical features to the basic vec-tors.
The Group-History model (g+w-h(N ,K,M ))is more complex.
It is possible to model group in-teractions over time by defining a new feature vectorthat includes all the participants?
basic features overmultiple time slices.
As we increase the number ofpeople in a group and/or the number of time slices toexplore the model space, however, the sheer size ofthis simple combination of feature vectors becomesunwieldy.
Instead we make the process hierarchicalby defining the Group-History as a two-layer SVM.Figure 2 instantiates the Group-History model forparticipant P1 playing in a group of four.
In the con-figuration shown, the decision for P1?s utterance attime t is based on behavior during N previous andK subsequent time slices, meaning each decision isdelayed by K time slices with respect to real time.The CHAR/NCHAR decision for time slice t de-pends on P1?s basic feature vector at time t, the out-put from the Individual submodel for time t, and theoutputs from the Participants submodel for each ofthe time slices through t. A concrete instantiation ofthe model can be seen in Figure 4 in the Appendix.The Individual submodel is an SVM that assigns ascore to the composite of P1?s basic feature vectorsacross a window of time (here,M+K+1).
The Par-ticipants submodel is an SVM that assigns a score tothe basic features of all members during each threeslice sliding subwindow in the full interval.
Moreintuitively: the Individual submodel finds correla-tions among the child?s observable behaviors overa window of time; the Participants submodel cap-tures relationships between members?
behaviors thatco-occur over small subwindows; and the Group-History model combines the two to find regularitiesthat unfold among participants over time, weightedtoward P1?s own behavior.The final model of interest, Group-History-Word(g+w+h(N ,K,M ,Q)), incorporates the knowledgefrom all sources of information.
A Lexical submodelis added to the Individual and Participants submod-els described above.
The Lexical submodel is anSVM classifier trained on the combination of ba-sic and word features for the current and Q previ-ous time slices.
The second layer SVM is trained onthe scores of the Individual, Participants, and Lex-ical submodels as well as the combined basic andModel Max f1 AUC TPR TNRBasic featuresg-w-h- 0.879 0.504 0.823 0.604g+w-h- 0.903 0.588 0.872 0.650g-w-h(8,1) 0.897 0.626 0.867 0.697g+w-h(4,1,8) 0.903 0.645 0.849 0.730Basic + Word featuresg-w+h- 0.904 0.636 0.901 0.675g+w+h- 0.906 0.655 0.863 0.728g-w+h(8,1) 0.901 0.661 0.886 0.716g+w+h(4,1,8,4) 0.913 0.701 0.859 0.786Table 2: Comparison of modelsword feature vector for the child.5 Results and DiscussionsWe used the LibSVM implementation (Chang andLin, 2011) for evaluation, holding out one child?sdata at a time during training, and balancing thedata set to compensate for the uneven distributionof CHAR and NCHAR utterances in the corpus.
Aspreviously noted, we used a time slice of 500 msecin all results reported here.
Where history is used,we consider only models with a single time slice oflook-ahead (K = 1) to create minimal additional de-lay in the character?s response.Table 2 reports average values, for each modeland over all sets of remaining children, in terms ofMax F1, true positive rate (TPR), true negative rate(TNR), and area under the TPR-TNR curve (AUC).TPR represents a model?s ability to recognize utter-ances directed to the character; low TPR means chil-dren will not be able to play the game effectively.TNR indicates a model?s ability to ignore utterancesdirected to other participants; low TNR means thatthe character will consider changing the game statewhen it hasn?t been addressed.Table 2 (top) shows comparative performancewithout the need for any speech recognition.
F1and TPR are generally high for all models.
Usingonly the basic features, however, gives a relativelylow TNR and an AUC that is almost random.
TheHistory model, (g-w-h(8,1)), increased performanceacross all measures compared to the basic features(g-w-h-).
We found that the History model?s per-formance was best when four seconds of the pastwere considered.
Group information within a singletime slice also improves performance over the ba-sic features, but the Group-History model has the213best overall tradeoff in missed CHAR versus ig-nored NCHAR utterances (AUC).
Group-History?sbest performance is achieved using two seconds ofgroup information from the past via the Participantssubmodel and four seconds of the speaker?s pastfrom the Individual submodel.Comparing the top and bottom halves of Table 2shows that all models benefit from accurate recogni-tion of a small set of task-independent words.
Thetable shows that word spotting improves both TPRand TNR when added to the Basic model, but tendsto improve only TNR when added to models withgroup and history features.
Improved TNR probablyresults from the ability to detect NCHAR utteranceswhen participants are facing the characters and/orpointing during naming discussions and comments.2Table 2 shows results averaged over each held outchild.
We then recast this information to show, bymodel, the percentage of children that would expe-rience TPR and TNR higher than given thresholds.Figure 3 shows a small portion of a complete graphof this type; in this case the percentage of childrenwho would experience greater than 0.6 for TPR andgreater than 0.5 for TNR under each model.
TPRand TNR lines for a model have the same color andshare a common pattern.Better models have higher TPR and TNR for morechildren.
The child who has to keep restating his orher choice (poor TPR) will be frustrated, as will thechild who has the character pre-emptively take hisor her choice away by ?overhearing?
side discus-sions (poor TNR).
While we do not know for anychild (or any age group) how high a TPR or TNR isrequired to prevent frustration, Figure 3 shows thatwithout lexical information the Group-History andGroup models have the best balance for the thresh-olds.
Group-History gives about 85% of the childrena TPR ?
0.7 for a TNR ?
0.5.
The simpler Groupmodel, which has no 500 msec delay for lookahead,can give a better TPR for the same TNR but for only75% of the children.
When we add lexical knowl-edge the case for Group-History becomes stronger,as it gives more than 85% of children a TPR ?
0.7for a TNR ?
0.6, while Group gives 85% of chil-dren about the same TPR with a TNR ?
0.5.2Results showing the affect of including object choice wordsin the w+ models are given in Figure 4 in the Appendix.Figure 3: The percentage of children experiencing dif-ferent TPR/TNR tradeoffs in models with (bottom) andwithout (top) lexical knowledge.
The g-w-h- model doesnot fall in the region of interest unless lexical features areused.6 Conclusions and Future WorkThe behavior of the characters, types of games,group make up, and physical environment all con-tribute to how participants communicate over timeand signal addressee.
We can manipulate some re-lationships (e.g., by organizing the spatial layout topromote head movement or having the character useexamples of recognizable language) and take ad-vantage of others by detecting relevant features andlearning how they combine as behavior unfolds.
Ourbest current model uses group and history informa-tion as well as basic audio-visual features to achievea max F1 of 0.91 and an AUC of 0.70.
Althoughthis model does not yet perform as well as humanannotators, it may be possible to improve it by tak-ing advantage of additional features that the behav-ioral data tells us are predictive (e.g., whether thespeaker is an adult or child).
Such additional sourcesof information are likely to be important as we re-place the annotated data with automatic sensors forspeech activity, orientation, and gesture recognition,and embed addressee identification in the larger con-text of turn-taking and full autonomous interaction.214ReferencesI.
Bakx, K. van Turnhout, and J. Terken.
2003.
Facialorientation during multi-party interaction with infor-mation kiosks.
pages 701?704.K.
Bergmann, H. Rieser, and S. Kopp.
2011.
Regu-lating dialogue with gestures towards an empiricallygrounded simulation with conversational agents.
InAnnual Meeting of the Special Interest Group on Dis-course and Dialogue (SIGDIAL), pages 88?97.Matthew Black, Jeannette Chang, Jonathan Chang, andShrikanth S. Narayanan.
2009.
Comparison of child-human and child-computer interactions based on man-ual annotations.
In Proceedings of the Workshop onChild, Computer and Interaction, Cambridge, MA.D.
Bohus and E. Horvitz.
2009a.
Dialog in the openworld: Platform and applications.
In Proceedingsof the International Conference on Multimodal Inter-faces (ICMI), pages 31?38.D.
Bohus and E. Horvitz.
2009b.
Learning to predict en-gagement with a spoken dialog system in open-worldsettings.
In Annual Meeting of the Special InterestGroup on Discourse and Dialogue (SIGDIAL), pages244?252.D.
Bohus and E. Horvitz.
2011.
Multiparty turn takingin situated dialog: Study, lessons, and directions.
InAnnual Meeting of the Special Interest Group on Dis-course and Dialogue (SIGDIAL), pages 98?109.C.
Chang and C. Lin.
2011.
LIBSVM: A library for sup-port vector machines.
ACM Transaction on IntelligentSystems and Technologies, 2:27:1?27:27.L.
Chen and M. Harper.
2009.
Multimodal floor con-trol shift detection.
In Proceedings of the InternationalConference on Multimodal Interfaces (ICMI).C.
Clemens and C. Diekhaus.
2009.
Prosodic turn-yielding cues with and without optical feedback.
InAnnual Meeting of the Special Interest Group on Dis-course and Dialogue (SIGDIAL), pages 107?110.C.
Cortes and V. Vapnik.
1995.
Support-vector networks.Machine Learning Journal, 20.S.
Duncan and D. W. Fiske.
1977.
Face-to-Face Interac-tion: Research, Methods and Theory.
Lawrence Erl-baum.I.
Fasel, M. Shiomi, T. Kanda, N. Hagita, P. Chadu-taud, and H. Ishiguro.
2009.
Multi-modal featuresfor real-time detection of human-robot interaction cat-egories.
In Proceedings of the International Confer-ence on Multimodal Interfaces (ICMI), pages 15?22.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge, MA.A.
Gravano and J. Hirschberg.
2009.
Turn-yieldingcues in task-oriented dialogue.
In Annual Meeting ofthe Special Interest Group on Discourse and Dialogue(SIGDIAL), pages 253?261.N.
Jovanovic, H.J.A.
op den Akker, and A. Nijholt.
2006.Addressee identification in face-to-face meetings.
InConference of the European Chapter of the Associ-ation for Computational Linguistics (EACL), pages169?176.M.
Katzenmaier, R. Steifelhagen, and T. Schultz.
2004.Identifying the addressee in human-human-robot inter-actions based on head pose and speech.
In Proceed-ings of the International Conference on MultimodalInterfaces (ICMI), pages 144?151.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proceedings of the International Conference onMa-chine Learning (ICML), pages 282?289.R.
Lunsford and S. Oviatt.
2006.
Human perceptionof intended addressee during computer assisted meet-ings.
In Proceedings of the International Conferenceon Multimodal Interfaces (ICMI), pages 20?27.Y.
Matsusaka, M. Enomoto, and Y. Den.
2007.
Simul-taneous prediction of dialog acts and address types inthree party conversations.
In Proceedings of the Inter-national Conference on Multimodal Interfaces (ICMI),pages 66?73.Sharon Oviatt.
2000.
Talking to thimble jellies: chil-dren?s conversational speech with animated characters.pages 877?880.J.
Terken, I. Joris, and L. de Valk.
2007.
Multimodalcues for addressee hood in triadic communication witha human information retrieval agent.
In Proceedingsof the International Conference on Multimodal Inter-faces (ICMI).2157 AppendixObject Choice Wordsantler, antlers, horn, horns, ear,ears, head, brownastronaut, astronauts, space,spaceman, spacemans, space-men, helmet, headbear, bears claw, claws, paw,paws, hand, hands, brownbunny, rabbit, bunnies, rabbits,slipper, slippers, foot, feet,whiteTask-independent WordsDiscourse marker hmm, mm, mmm, ok, eww,shh, oopsyQuestion words what, let, where, who, which,whenGreetings hi, hello, bye, goodbyeTable 3: Excerpts from the dictionary for task-specificand task-independent wordsModel Max f1 AUC TPR TNRGreeting, question & discourse wordsg-w+h- 0.904 0.636 0.901 0.675g+w+h- 0.906 0.655 0.863 0.728g-w+h(8,1) 0.901 0.661 0.886 0.716g+w+h(4,1,8,4) 0.913 0.701 0.859 0.786With object reference words addedg-w+h- 0.894 0.576 0.777 0.768g+w+h- 0.898 0.623 0.782 0.773g-w+h(7,1) 0.910 0.642 0.838 0.783g+w+h(4,1,8,4) 0.912 0.685 0.834 0.799Table 4: The effect of adding object reference wordsA1,4?G1?5?T?
P1?G2?Individual?sub-??model?Par?cipants?sub-??models?4?3?P4?P3?P2?P1?
P1?
P1?
P1?P4?P3?P2?P1?P4?P3?P2?P1?4?3?2?P4?P3?P2?P1?P4?P3?P2?P1?P4?P3?P2?P1?5?4?3?Figure 4: A concrete representation for the Group-History model with N = 2, M = 1, and K = 1 attime step t = 4.
The value at t = 4 is delayed one timeslice of real time.SessionTypeGroup: participant(age) Durationfull p1(5), experimenter 9 minfull p2(7), p3(6), p6(adult) 9 minfull p4(7), p5 (4), p6(adult) 9 minreplay p2(7), p3(6), p4(7), p5(4) 8 minfull p7(10), experimenter 8 minreplay p7(10) 6 minfull p8(9), p9(8), experimenter 9 minfull p10(10), p11(5), experimenter 11 minfull p12(6), p14(adult) 11 minfull p13(4), p14(adult) 11 minfull p15(4), experimenter 8 minfull p16(9), p17(7), experimenter 12 minreplay p16(9), experimenter 3 minfull p18(8), p19(6), p20(8),p21(adult)12 minfull p22(5), experimenter 9 minreplay p22(5), experimenter 3 minfull p25(6), experimenter 9 minfull p26(8), p27(4), experimenter 11 minreplay p26(8), experimenter 6 minfull p28(7), p29(adult) 12 minfull p30(5), experimenter 11 minreplay p30(5), experimenter 4 minfull p31(6), p32(5), p33(adult) 10 minfull p34(4), p35(adult) 9 minreplay p34(4), p35(adult) 4 minTable 5: Details for sessions used in the analysis (doesnot include five sessions with corrupted data)216
