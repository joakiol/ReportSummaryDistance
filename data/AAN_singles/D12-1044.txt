Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 478?488, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsDynamic Programming for Higher Order Parsing of Gap-Minding TreesEmily Pitler, Sampath Kannan, Mitchell MarcusComputer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104epitler,kannan,mitch@seas.upenn.eduAbstractWe introduce gap inheritance, a new struc-tural property on trees, which provides a wayto quantify the degree to which intervals of de-scendants can be nested.
Based on this prop-erty, two new classes of trees are derived thatprovide a closer approximation to the set ofplausible natural language dependency treesthan some alternative classes of trees: unlikeprojective trees, a word can have descendantsin more than one interval; unlike spanningtrees, these intervals cannot be nested in ar-bitrary ways.
The 1-Inherit class of trees hasexactly the same empirical coverage of naturallanguage sentences as the class of mildly non-projective trees, yet the optimal scoring treecan be found in an order of magnitude lesstime.
Gap-minding trees (the second class)have the property that all edges into an intervalof descendants come from the same node, andthus an algorithm which uses only single in-tervals can produce trees in which a node hasdescendants in multiple intervals.1 IntroductionDependency parsers vary in what space of possi-ble tree structures they search over when parsinga sentence.
One commonly used space is the setof projective trees, in which every node?s descen-dants form a contiguous interval in the input sen-tence.
Finding the optimal tree in the set of projec-tive trees can be done efficiently (Eisner, 2000), evenwhen the score of a tree depends on higher order fac-tors (McDonald and Pereira, 2006; Carreras, 2007;Koo and Collins, 2010).
However, the projectivityassumption is too strict for all natural language de-pendency trees; for example, only 63.6% of Dutchsentences from the CoNLL-X training set are pro-jective (Table 1).At the other end of the spectrum, some parserssearch over all spanning trees, a class of structuresmuch larger than the set of plausible linguistic struc-tures.
The maximum scoring directed spanning treecan be found efficiently when the score of a tree de-pends only on edge-based factors (McDonald et al2005b).
However, it is NP-hard to extend MST to in-clude sibling or grandparent factors (McDonald andPereira, 2006; McDonald and Satta, 2007).
MST-based non-projective parsers that use higher orderfactors (Martins et al2009; Koo et al2010), uti-lize different techniques than the basic MST algo-rithm.
In addition, learning is done over a relaxationof the problem, so the inference procedures at train-ing and at test time are not identical.We propose two new classes of trees between pro-jective trees and the set of all spanning trees.
Thesetwo classes provide a closer approximation to the setof plausible natural language dependency trees: un-like projective trees, a word can have descendants inmore than one interval; unlike spanning trees, theseintervals cannot be nested in arbitrary ways.
We in-troduce gap inheritance, a new structural propertyon trees, which provides a way to quantify the de-gree to which these intervals can be nested.
Differ-ent levels of gap inheritance define each of these twoclasses (Section 3).The 1-Inherit class of trees (Section 4) has exactlythe same empirical coverage (Table 1) of natural lan-guage sentences as the class of mildly non-projectivetrees (Bodirsky et al2005), yet the optimal scoringtree can be found in an order of magnitude less time(Section 4.1).Gap-minding trees (the second class) have the478property that all edges into an interval of descen-dants come from the same node.
Non-contiguousintervals are therefore decoupled given this singlenode, and thus an algorithm which uses only singleintervals (as in projective parsing) can produce treesin which a node has descendants in multiple inter-vals (as in mildly non-projective parsing (Go?mez-Rodr?
?guez et al2011)).
A procedure for findingthe optimal scoring tree in this space is given in Sec-tion 5, which can be searched in yet another order ofmagnitude faster than the 1-Inherit class.Unlike the class of spanning trees, it is stilltractable to find the optimal tree in these new spaceswhen higher order factors are included.
An exten-sion which finds the optimal scoring gap-mindingtree with scores over pairs of adjacent edges (grand-parent scoring) is given in Section 6.
These gap-minding algorithms have been implemented in prac-tice and empirical results are presented in Section 7.2 PreliminariesIn this section, we review some relevant defini-tions from previous work that characterize degreesof non-projectivity.
We also review how wellthese definitions cover empirical data from six lan-guages: Arabic, Czech, Danish, Dutch, Portuguese,and Swedish.
These are the six languages whoseCoNLL-X shared task data are either available opensource1 or from the LDC2.A dependency tree is a rooted, directed spanningtree that represents a set of dependencies betweenwords in a sentence.3 The tree has one artificial rootnode and vertices that correspond to the words in aninput sentence w1, w2,...,wn.
There is an edge fromh to m if m depends on (or modifies) h.Definition 1.
The projection of a node is the set ofwords in the subtree rooted at it (including itself).A tree is projective if, for every node in the tree,that node?s projection forms a contiguous interval inthe input sentence order.A tree is non-projective if the above does not hold,i.e., there exists at least one word whose descendants1http://ilk.uvt.nl/conll/free_data.html2LDC catalogue numbers LDC2006E01 and LDC2006E023Trees are a reasonable assumption for most, but not all,linguistic structures.
Parasitic gaps are an example in whicha word perhaps should have multiple parents.do not form a contiguous interval.Definition 2.
A gap of a node v is a non-empty, max-imal interval that does not contain any words in theprojection of v but lies between words that are inthe projection of v. The gap degree of a node isthe number of gaps it has.
The gap degree of a treeis the maximum of the gap degrees of its vertices.
(Bodirsky et al2005)Note that a projective tree will have gap degree 0.Two subtrees interleave if there are vertices l1, r1from one subtree and l2, r2 from the other such thatl1 < l2 < r1 < r2.Definition 3.
A tree is well-nested if no two disjointsubtrees interleave (Bodirsky et al2005).Definition 4.
A mildly non-projective tree has gapdegree at most one and is well-nested.Mildly non-projective trees are of both theoret-ical and practical interest, as they correspond toderivations in Lexicalized Tree Adjoining Grammar(Bodirsky et al2005) and cover the overwhelmingmajority of sentences found in treebanks for Czechand Danish (Kuhlmann and Nivre, 2006).Table 1 shows the proportion of mildly non-projective sentences for Arabic, Czech, Danish,Dutch, Portuguese, and Swedish, ranging from95.4% of Portuguese sentences to 99.9% of Ara-bic sentences.4 This definition covers a substan-tially larger set of sentences than projectivity does?
an assumption of projectivity covers only 63.6%(Dutch) to 90.2% (Swedish) of examples (Table 1).3 Gap InheritanceEmpirically, natural language sentences seem to bemostly mildly non-projective trees, but mildly non-projective trees are quite expensive to parse (O(n7)(Go?mez-Rodr?
?guez et al2011)).
The parsing com-plexity comes from the fact that the definition al-lows two non-contiguous intervals of a projection tobe tightly coupled, with an unbounded number ofedges passing back and forth between the two inter-vals; however, this type of structure seems unusual4While some of the treebank structures are ill-nested or havea larger gap degree because of annotation decisions, some lin-guistic constructions in German and Czech are ill-nested orrequire at least two gaps under any reasonable representation(Chen-Main and Joshi, 2010; Chen-Main and Joshi, 2012).479Arabic Czech Danish Dutch Portuguese Swedish ParsingMildly non-proj 1458 (99.9) 72321 (99.5) 5175 (99.7) 12896 (96.6) 8650 (95.4) 10955 (99.2) O(n7)Mild+1-Inherit 1458 (99.9) 72321 (99.5) 5175 (99.7) 12896 (96.6) 8650 (95.4) 10955 (99.2) O(n6)Mild+0-Inherit 1394 (95.5) 70695 (97.2) 4985 (96.1) 12068 (90.4) 8481 (93.5) 10787 (97.7) O(n5)Projective 1297 (88.8) 55872 (76.8) 4379 (84.4) 8484 (63.6) 7353 (81.1) 9963 (90.2) O(n3)# Sentences 1460 72703 5190 13349 9071 11042Table 1: The number of sentences from the CoNLL-X training sets whose parse trees fall into each of the aboveclasses.
The two new classes of structures, Mild+0-Inherit and Mild+1-Inherit, have more coverage of empirical datathan projective structures, yet can be parsed faster than mildly non-projective structures.
Parsing times assume an edge-based factorization with no pruning of edges.
The corresponding algorithms for Mild+1-Inherit and Mild+0-Inheritare in Sections 4 and 5.for natural language.
We therefore investigate if wecan define further structural properties that are bothappropriate for describing natural language trees andwhich admit more efficient parsing algorithms.Let us first consider an example of a tree whichboth has gap degree at most one and satisfies well-nestedness, yet appears to be an unrealistic struc-ture for a natural language syntactic tree.
Considera tree which is rooted at node xn+2, which has onechild, node xn+1, whose projection is [x1, xn+1] ?
[xn+3, x2n+2], with n children (x1, ..., xn), and eachchild xi has a child at x2n?i+3.
This tree is well-nested, has gap degree 1, but all n of xn+1?s childrenhave edges into the other projection interval.We introduce a further structural restriction in thissection, and show that trees satisfying our new prop-erty can be parsed more efficiently with no drop inempirical coverage.Definition 5.
A child is gap inheriting if its parenthas gap degree 1 and it has descendants on bothsides of its parent?s gap.
The inheritance degree ofa node is the number of its children which inherit itsgap.
The inheritance degree of a tree is the maximuminheritance degree over all its nodes.Figure 1 gives examples of trees with varying de-grees of gap inheritance.
Each projection of a nodewith a gap is shown with two matching rectangles.
Ifa child has a projection rectangle nested inside eachof the parent?s projection rectangles, then that childinherits the parent?s gap.
Figure 1(a) shows a mildlyprojective tree (with inheritance degree 2), with bothnode 2 and node 11 inheriting their parent (node 3)?sgap (note that both the dashed and dotted rectangleseach show up inside both of the solid rectangles).Figure 1(b) shows a tree with inheritance degree 1:there is now only one pair of rectangles (the dot-ted ones) which show up in both of the solid ones.Figure 1(c) shows a tree with inheritance degree 0:while there are gaps, each set of matching rectanglesis contained within a single rectangle (projection in-terval) of its parent, i.e., the two dashed rectanglesof node 2?s projection are contained within the leftinterval of node 3; the two dotted rectangles of node12?s projection are contained within the right inter-val of node 3, etc.We now ask:1.
How often does gap inheritance occur in theparses of natural language sentences found intreebanks?2.
Furthermore, how often are there multiple gapinheriting children of the same node (inheri-tance degree at least two)?Table 1 shows what proportion of mildly non-projective trees have the added property of gap in-heritance degree 0 (Mild+0-Inherit) or have gap in-heritance degree 1 (Mild+1-Inherit).
Over all sixlanguages, there are no examples of multiple gapinheritance ?
Mild+1-Inherit has exactly the sameempirical coverage as the unrestricted set of mildlynon-projective trees.4 Mild+1-Inherit TreesThere are some reasons from syntactic theory whywe might expect at most one child to inherit its par-ent?s gap.
Traditional Government and Binding the-ories of syntax (Chomsky, 1981) assume that thereis an underlying projective (phrase structure) tree,and that gaps primarily arise through movement of48062 3 41 5 7 11 12 1398 10(a) Mildly Non-Projective: The projec-tions (set of descendants) of both node 2(the dashed red rectangles) and node 11(dotted magenta) appear in both of node3?s intervals (the solid blue rectangles).62 3 41 5 7 11 12 1398 10(b) Mild+1-Inherit: Only node 2 inheritsnode 3?s gap: the dashed red rectanglesappear in each of the two solid blue rect-angles.62 3 41 5 7 11 12 1398 10(c) Mild+0-Inherit: Even though node 3has children with gaps (node 2 and node12), neither of them inherit node 3?s gap.There are several nodes with gaps, butevery node with a gap is properly con-tained within just one of its parent?s in-tervals.Figure 1: Rectangles that match in color and style indicate the two projection intervals of a node, separated by a gap.In all three trees, node 3?s two projection intervals are shown in the two solid blue rectangles.
The number of childrenwhich inherit its gap vary, however; in 1(a), two children have descendants within both sides; in 1(b) only one childhas descendants on both sides; in 1(c), none of its children do.subtrees (constituents).
One of the fundamental as-sumptions of syntactic theory is that movement isupward in the phrase structure tree.5Consider one movement operation and its effecton the gap degree of all other nodes in the tree: (a) itshould have no effect on the gap degree of the nodesin the subtree itself, (b) it can create a gap for an an-cestor node if it moves out of its projection interval,and (c) it can create a gap for a non-ancestor nodeif it moves in to its projection interval.
Now con-sider which cases can lead to gap inheritance: in case(b), there is a single path from the ancestor to theroot of the subtree, so the parent of the subtree willhave no gap inheritance and any higher ancestorswill have a single child inherit the gap created by thismovement.
In case (c), it is possible for there to bemultiple children that inherit this newly created gapif multiple children had descendents on both sides.However, the assumption of upward movement inthe phrase structure tree should rule out movementinto the projection interval of a non-ancestor.
There-fore, under these syntactic assumptions, we wouldexpect at most one child to inherit a parent?s gap.5The Proper Binding Condition (Fiengo, 1977) asserts that amoved element leaves behind a trace (unpronounced element),which must be c-commanded (Reinhart, 1976) by the corre-sponding pronounced material in its final location.
Informally,c-commanded means that the first node is descended from thelowest ancestor of the other that has more than one child.4.1 Parsing Mild+1-Inherit TreesFinding the optimal Mild+1-Inherit tree can be doneby bottom-up constructing the tree for each node andits descendants.
We can maintain subtrees with twointervals (two endpoints each) and one root (O(n5)space).
Consider the most complicated possiblecase: a parent that has a gap, a (single) child whichinherits the gap, and additional children.
An exam-ple of this is seen with the parent node 3 in Figure1(b).This subtree can be constructed by first startingwith the child spanning the gap, updating its rootindex to be the parent, and then expanding the inter-val indices to the left and right to include the otherchildren.
In each case, only one index needs to beupdated at a time, so the optimal tree can be foundin O(n6) time.
In the Figure 1(b) example, the sub-tree rooted at 3 would be built by starting with theintervals [1, 2] ?
[12, 13] rooted at 2, first adding theedge from 2 to 3 (so the root is updated to 3), thenadding an edge from 3 to 4 to extend the left inter-val to [1, 5], and then adding an edge from 3 to 11 toextend the right interval to [8, 13].
The subtree cor-responds to the completed item [1, 5]?
[8, 13] rootedat 3.This procedure corresponds to Go?mez-Rodr?
?guezet al2011)?s O(n7) algorithm for parsing mildlynon-projective structures if the most expensive step(Combine Shrinking Gap Centre) is dropped; thisstep would only ever be needed if a parent node has481more than one child inheriting its gap.This is also similar in spirit to the algorithm de-scribed in Satta and Schuler (1998) for parsing arestricted version of TAG, in which there are somelimitations on adjunction operations into the spinesof trees.6 That algorithm has similar steps and items,with the root portion of the item replaced with anode in a phrase structure tree (which may be a non-terminal).5 Gap-minding TreesThe algorithm in the previous section used O(n5)space and O(n6) time.
While more efficient thanparsing in the space of mildly projective trees, thisis still probably not practically implementable.
Partof the difficulty lies in the fact that gap inheritancecauses the two non-contiguous projection intervalsto be coupled.Definition 6.
A tree is called gap-minding7 if it hasgap degree at most one, is well-nested, and has gapinheritance degree 0.Gap-minding trees still have good empirical cov-erage (between 90.4% for Dutch and 97.7% forSwedish).
We now turn to the parsing of gap-minding trees and show how a few consequences ofits definition allow us to use items ranging over onlyone interval.In Figure 1(c), notice how each rectangle hasedges incoming from exactly one node.
This is notunique to this example; all projection intervals in agap-minding tree have incoming edges from exactlyone node outside the interval.Claim 1.
Within a gap-minding tree, consider anynode n with a gap (i.e., n?s projection forms twonon-contiguous intervals [xi, xj ] ?
[xk, xl]).
Let pbe the parent of n.1.
For each of the intervals of n?s projection:(a) If the interval contains n, the only edgeincoming to that interval is from p to n.6That algorithm has a running time of O(Gn5), where aswritten G would likely add a factor of n2 with bilexical selec-tional preferences; this can be lowered to n using the same tech-nique as in Eisner and Satta (2000) for non-restricted TAG.7The terminology is a nod to the London Underground butimagines parents admonishing children to mind the gap.
(b) If the interval does not contain n, all edgesincoming to that interval come from n.2.
For the gap interval ([xj+1, xk?1]):(a) If the interval contains p, then the onlyedge incoming is from p?s parent to p(b) If the interval does not contain p, then alledges incoming to that interval come fromp.As a consequence of the above, [xi, xj ] ?
{n} formsa gap-minding tree rooted at n, [xk, xl] ?
{n}also forms a gap-minding tree rooted at n, and[xj+1, xk?1] ?
{p} forms a gap-minding tree rootedat p.Proof.
(Part 1): Assume there was a directed edge(x, y) such that y is inside a projection interval of nand x is not inside the same interval, and x 6= y 6= n.y is a descendant of n since it is contained in n?s pro-jection.
Since there is a directed edge from x to y,x is y?s parent, and thus x must also be a descen-dant of n and therefore in another of n?s projectionintervals.
Since x and y are in different intervals,then whichever child of n that x and y are descendedfrom would have inherited n?s gap, leading to a con-tradiction.
(Part 2): First, suppose there existed a set of nodesin n?s gap which were not descended from p. Thenp has a gap over these nodes.
(p clearly has descen-dants on each side of the gap, because all descen-dants of n are also descendants of p).
n, p?s child,would then have descendants on both sides of p?sgap, which would violate the property of no gap in-heritance.
It is also not possible for there to be edgesincoming from other descendants of p outside thegap, as that would imply another child of p beingill-nested with respect to n.From the above, we can build gap-minding treesusing only single intervals, potentially with a sin-gle node outside of the interval.
Our objective isto find the maximum scoring gap-minding tree, inwhich the score of a tree is the sum of the scores ofits edges.
Let Score(p,x) indicate the score of thedirected edge from p to x.Therefore, the main type of sub-problems we willuse are:4821.
C[i, j,p]: The maximum score of any gap-minding tree, rooted at p, with vertices [i, j] ?
{p} (p may or may not be within [i, j]).This improves our space requirement, but not nec-essarily the time requirement.
For example, if webuilt up the subtree in Figure 1(c) by concatenatingthe three intervals [1, 5] rooted at 3, [6, 7] rooted at 6,and [8, 13] rooted at 3, and add the edge 6 ?
3, wewould still need 6 indices to describe this operation(the four interval endpoints and the two roots), andso we have not yet improved the running time overthe Inherit-1 case.By part 2, we can concatenate one interval of achild with its gap, knowing that the gap is entirelydescended from the child?s parent, and forget theconcatenation split point between the parent?s otherdescendants and this side of the child.
This allows usto substitute all operations involving 6 indices withtwo operations involving just 5 indices.
For exam-ple, in Figure 1(c), we could first merge [6, 7] rootedat 6 with [8, 13] rooted at 3 to create an interval[6, 13] and say that it is descended from 6, with therightmost side descended from its child 3.
That steprequired 5 indices.
The following step would mergethis concatenated interval ([6, 13] rooted at 6 and 3)with [1, 5] rooted at 3.
This step also requires only 5indices.Our helper subtype we make use of is then:2.
D[i, j,p,x,b]: The maximum score of any setof two gap-minding trees, one rooted at p, onerooted at x, with vertices [i, j] ?
{p, x} (x /?
[i, j], p may or may not be in [i, j]), such thatfor some k, vertices [i, k] are in the tree rootedat p if b = true (and at x if b = false), andvertices [k+1, j] are in the tree rooted at x (p).Consider an optimum scoring gap-minding tree Trooted at p with vertices V = [i, j] ?
{p} and edgesE, where E 6= ?.
The form of the dynamic programmay depend on whether:?
p is within (i, j) (I) or external to [i, j] (E)88In the discussion we will assume that p 6= i and p 6= j,since any optimum solution with V = [i, j] ?
{i} and a rootat i will be equivalent to V = [i + 1, j] ?
{i} rooted at i (andsimilarly for p = j).We can exhaustively enumerate all possibilities forT by considering all valid combinations of the fol-lowing binary cases:?
p has a single child (S) or multiple children (M)?
i and j are descended from the same child of p(C) or different children of p (D)Note that case (S/D) is not possible: i and j cannotbe descended from different children of p if p hasonly a single child.
We therefore need to find themaximum scoring tree over the three cases of S/C,M/C, and M/D.Claim 2.
Let T be the optimum scoring gap-minding tree rooted at p with vertices V = [i, j] ?{p}.
Then T and its score are derived from one ofthe following:S/C If p has a single child x in T , then if p ?
(i, j)(I), T ?s score is Score(p,x)+C[i,p?1,x]+C[p+ 1, j,x]; if p /?
[i, j] (E), T ?s score isScore(p,x) +C[i, j,x].M/C If p has multiple children in T and i and jare descended from the same child x in T , thenthere is a split point k such that T ?s score is:Score(p,x)+C[i,k,x]+D[k+ 1, j,p,x,T]if x is on the left side of its own gap, andT ?s score is: Score(p,x) + C[k, j,x] +D[i,k?
1,p,x,F] if x is on the right side.M/D If p has multiple children in T and i and jare descended from different children in T , thenthere is a split point k such that T ?s score isC[i,k,p] +C[k+ 1, j,p].T has the maximum score over each of the abovecases, for all valid choices of x and k.Proof.
Case S/C: If p has exactly one child x,then the tree can be decomposed into the edgefrom p to x and the subtree rooted at x.
If pis outside the interval, then the maximum scor-ing such tree is clearly Score(p,x) + C[i, j,x].If p is inside, then x has a gap across p, andso using Claim 1, the maximum scoring treerooted at p with a single child x has score ofScore(p,x) +C[i,p?
1,x] +C[p+ 1, j,x].Case M/C: If there are multiple children and theendpoints are descended from the same child x, then483the child x has to have gap degree 1. x itself is oneither the left or right side of its gap.
For the mo-ment, assume x is in the left interval.
By Claim 1,we can split up the score of the tree as the score ofthe edge from p to x (Score(p,x)), the score of thesubtree corresponding to the projection of x to theleft of its gap (C[i,k,x]), and the score of the sub-trees rooted at p with its remaining children and thesubtree rooted at x corresponding to the right sideof x?s projection (D[k+ 1, j,p,x,T]).
The case inwhich x is on the right side of its gap is symmetric.Case M/D: If there are multiple children and theendpoints are descended from different children ofp, then there must exist a split point k that parti-tions the children of p into two non-empty sets, suchthat each child?s projection is either entirely on theleft or entirely on the right of the split point.
Weshow one such split point to demonstrate that therealways exists at least one.
Let x be the child of pthat i is descended from, and let xl and xr be x?sleftmost and right descendants, respectively.9 Con-sider all the children of p (whose projections takentogether partition [i, j] ?
{p}).
No child can havedescendants both to the left of xr and to the rightof xr, because otherwise that child and x would beill-nested.
Therefore we can split up the interval atxr to have two gap-minding trees, both rooted at p.The score of T is then the sum of the scores of thebest subtree rooted at p over [i, k] (C[i,k,p]) andthe score of the best subtree rooted at p over [k+1, j](C[k+ 1, j,p]).The above cases cover all non-empty gap-minding trees, so the maximum will be found.Using Claim 2 to Devise an Algorithm The aboveclaim showed that any problem of type C can bedecomposed into subproblems of types C and D.From the definition of D, any problem of type D canclearly be decomposed into two problems of type C?
simply split the interval at the split point knownto exist and assign p or x as the roots for each sideof the interval, as prescribed by the boolean b:D(i, j,p,x,T) = maxkC[i,k,p] +C[k+ 1, j,x]D(i, j,p,x,F) = maxkC[i,k,x] +C[k+ 1, j,p]9Note that xl = i by construction, and xr 6= j (because theendpoints are descended from different children).Algorithm 1 makes direct use of the above claims.Note that in every gap-minding tree referred toin the cases above, all vertices that were not theroot formed a single interval.
Algorithm 1 buildsup trees in increasing sizes of [i, j] ?
{p}.
Thetree in C[i, j,p] corresponds to the maximum offour subroutines: SingleChild (S/C), EndpointsDiff(M/D), EndsFromLeftChild (M/C), and EndsFrom-RightChild (M/C).
The D subproblems are filled inwith the subroutine Max2Subtrees, which uses theabove discussion.
The maximum score of any gap-minding tree is then found in C[1,n,0], and the treeitself can be found using backpointers.5.1 Runtime analysisIf the input is assumed to be the complete graph (anyword can have any other word as its parent), thenthe above algorithm takes O(n5) time.
The mostexpensive steps are M/C, which take O(n2) time tofill in each of the O(n3) C cells.
and solving a Dsubproblem, which takes O(n) time on each of theO(n4) possible such problems.Pruning: In practice, the set of edges considered(m) is not necessarily O(n2).
Many edges can beruled out beforehand, either based on the distancein the sentence between the two words (Eisner andSmith, 2010), the predictions of a local ranker (Mar-tins et al2009), or the marginals computed from asimpler parsing model (Carreras et al2008).If we choose a pruning strategy such that eachword has at most k potential parents (incomingedges), then the running time drops to O(kn4).
Thefive indices in an M/C step were: i, j, k, p, and x.As there must be an edge from p to x, and x only hask possible parents, there are now only O(kn4) validsuch combinations.
Similarly, each D subproblem(which ranges over i, j, k, p, x) may only come intoexistence because of an edge from p to x, so againthe runtime of these such steps drops to O(kn4).6 Extension to GrandparentFactorizationsThe ability to define slightly non-local features hasbeen shown to improve parsing performance.
In thissection, we assume a grandparent-factored model,where the score of a tree is now the sum over scoresof (g, p, c) triples, where (g, p) and (p, c) are both484directed edges in the tree.
Let Score(g,p, c) indi-cate the score of this grandparent-parent-child triple.We now show how to extend the above algorithmto find the maximum scoring gap-minding tree withgrandparent scoring.Our two subproblems are now C[i, j,p,g] andD[i, j,p,x,b,g]; each subproblem has been aug-mented with an additional grandparent index g,which has the meaning that g is p?s parent.
Note thatg must be outside of the interval [i, j] (if it were not,a cycle would be introduced).
Edge scores are nowcomputed over (g, p, x) triples.
In particular, claim2 is modified:Claim 3.
Let T be the optimum scoring gap-minding tree rooted at p with vertices V = [i, j] ?
{p}, where p ?
(i, j) (I), with a grandparent indexg (g /?
V ).
Then T and its score are derived fromone of the following:S/C If p has a single child x in T , then ifp ?
(i, j) (I), T ?s score is Score(g,p,x) +C[i,p?1,x,p]+C[p+ 1, j,x,p]; if p /?
[i, j](E), T ?s score is Score(g,p,x)+C[i, j,x,p].M/C If p has multiple children in T and iand j are descended from the same childx in T , then there is a split point ksuch that T ?s score is: Score(g,p,x) +C[i,k,x,p] + D[k+ 1, j,p,x,T,g] if x ison the left side of its own gap, and T ?sscore is: Score(g,p,x) + C[k, j,x,p] +D[i,k?
1,p,x,F,g] if x is on the right side.M/D If p has multiple children in T and i and jare descended from different children in T , thenthere is a split point k such that T ?s score isC[i,k,p,g] +C[k+ 1, j,p,g].T has the maximum score over each of the abovecases, for all valid choices of x and k.Note that for subproblems rooted at p, g is thegrandparent index, while for subproblems rooted atx, p is the updated grandparent index.
The D sub-problems with the grandparent index are shown be-low:D(i, j,p,x,T,g) = maxkC[i,k,p,g] +C[k+ 1, j,x,p]D(i, j,p,x,F,g) = maxkC[i,k,x,p] +C[k+ 1, j,p,g]We have added another index which ranges overn, so without pruning, we have now increased therunning time to O(n6).
However, every step now in-cludes both a g and a p (and often an x), so there isat least one implied edge in every step.
If pruningis done in such a way that each word has at most kparents, then each word?s set of grandparent and par-ent possibilities is at most k2.
To run all of the S/Csteps, we therefore need O(k2n3) time; for all of theM/C steps, O(k2n4) time; for all of the M/D steps,O(kn4); for all of the D subproblems, O(k2n4).
Theoverall running time is therefore O(k2n4), and wehave shown that when edges are sufficiently pruned,grandparent factors add only an extra factor of k, andnot a full extra factor of n.7 ExperimentsThe space of projective trees is strictly containedwithin the space of gap-minding trees which isstrictly contained within spanning trees.
Whichspace is most appropriate for natural language pars-ing may depend on the particular language and thetype and frequencies of non-projective structuresfound in it.
In this section we compare the parsingaccuracy across languages for a parser which useseither the Eisner algorithm (projective), MST (span-ning trees), or MaxGapMindingTree (gap-mindingtrees) as its decoder for both training and inference.We implemented both the basic gap-minding al-gorithm and the gap-minding algorithm with grand-parent scoring as extensions to MSTParser10.
MST-Parser (McDonald et al2005b; McDonald et al2005a) uses the Margin Infused Relaxed Algo-rithm (Crammer and Singer, 2003) for discrimina-tive training.
Training requires a decoder whichproduces the highest scoring tree (in the space ofvalid trees) under the current model weights.
Thissame decoder is then used to produce parses at testtime.
MSTParser comes packaged with the Eis-ner algorithm (for projective trees) and MST (forspanning trees).
MSTParser also includes two sec-ond order models: one of which is a projective de-coder that also scores siblings (Proj+Sib) and theother of which produces non-projective trees by re-arranging edges after producing a projective tree(Proj+Sib+Rearr).
We add a further decoder with10http://sourceforge.net/projects/mstparser/485the algorithm presented here for gap minding trees,and plan to make the extension publicly available.The gap-minding decoder has both an edge-factoredimplementation and a version which scores grand-parents as well.11The gap-minding algorithm is much more effi-cient when edges have been pruned so that eachword has at most k potential parents.
We use theweights from the trained MST models combinedwith the Matrix Tree Theorem (Smith and Smith,2007; Koo et al2007; McDonald and Satta, 2007)to produce marginal probabilities of each edge.
Wewanted to be able to both achieve the running timebound and yet take advantage of the fact that thesize of the set of reasonable parent choices is vari-able.
We therefore use a hybrid pruning strategy:each word?s set of potential parents is the smaller ofa) the top k parents (we chose k = 10) or b) the setof parents whose probabilities are above a thresh-old (we chose th = .001).
The running time forthe gap-minding algorithm is then O(kn4); with thegrandparent features the gap-minding running timeis O(k2n4).The training and test sets for the six languagescome from the CoNLL-X shared task.12 We trainthe gap-minding algorithm on sentences of lengthat most 10013 (the vast majority of sentences).
Theprojective and MST models are trained on all sen-tences and are run without any pruning.
The Czechtraining set is much larger than the others and so forCzech only the first 10,000 training sentences wereused.
Testing is on the full test set, with no lengthrestrictions.The results are shown in Table 2.
The first threelines show the first order gap-minding decoder com-pared with the first order projective and MST de-11The grandparent features used were identical to the fea-tures provided within MSTParser for the second-order siblingparsers, with one exception ?
many features are conjoined witha direction indicator, which in the projective case has only twopossibilities.
We replaced this two-way distinction with a six-way distinction of the six possible orders of the grandparent,parent, and child.12MSTParser produces labeled dependencies on CoNLL for-matted input.
We replace all labels in the training set with asingle dummy label to produce unlabeled dependency trees.13Because of long training times, the gap-minding withgrandparent models for Portuguese and Swedish were trainedon only sentences up to 50 words.Ar Cz Da Du Pt SwProj.
78.0 80.0 88.2 79.8 87.4 86.9MST 78.0 80.4 88.1 84.6 86.7 86.2Gap-Mind 77.6 80.8 88.6 83.9 86.8 86.0Proj+Sib 78.2 80.0 88.9 81.1 87.5 88.1+Rearr 78.5 81.3 89.3 85.4 88.2 87.7GM+Grand 78.3 82.1 89.1 84.6 87.7 88.5Table 2: Unlabeled Attachment Scores on the CoNLL-Xshared task test set.coders.
The gap-minding decoder does better thanthe projective decoder on Czech, Danish, and Dutch,the three languages with the most non-projectivity,even though it was at a competitive disadvantage interms of both pruning and (on languages with verylong sentences) training data.
The gap-minding de-coder with grandparent features is better than theprojective decoder with sibling features on all sixof the languages.
On some languages, the localsearch decoder with siblings has the absolute high-est accuracy in Table 2; on other languages (Czechand Swedish) the gap-minding+grandparents has thehighest accuracy.
While not directly comparable be-cause of the difference in features, the promisingperformance of the gap-minding+grandparents de-coder shows that the space of gap-minding trees islarger than the space of projective trees, yet unlikespanning trees, it is tractable to find the best tree withhigher order features.
It would be interesting to ex-tend the gap-minding algorithm to include siblingsas well.8 ConclusionGap inheritance, a structural property on trees, hasimplications both for natural language syntax andfor natural language parsing.
We have shown thatthe mildly non-projective trees present in naturallanguage treebanks all have zero or one children in-herit each parent?s gap.
We also showed that the as-sumption of 1 gap inheritance removes a factor ofn from parsing time, and the further assumption of0 gap inheritance removes yet another factor of n.The space of gap-minding trees provides a closer fitto naturally occurring linguistic structures than thespace of projective trees, and unlike spanning trees,the inclusion of higher order factors does not sub-stantially increase the difficulty of finding the maxi-mum scoring tree in that space.486AcknowledgmentsWe would like to thank Aravind Joshi for commentson an earlier draft.
This material is based uponwork supported under a National Science Founda-tion Graduate Research Fellowship.Algorithm 1: MaxGapMindingTreeInit: ?i?
[1,n]C[i, i, i] = 0for size = 0 to n?
1 dofor i = 1 to n?
size doj = i+ size/* Endpoint parents */if size > 0 thenC[i, j, i] = C[i+ 1, j, i]C[i, j, j] = C[i, j ?
1, j]/* Interior parents */for p = i+ 1 to j ?
1 doC[i, j, p] = max (SingleChild(i,j,p),EndpointsDiff(i,j,p),EndsFromLeftChild(i,j,p),EndsFromRightChild(i,j,p))/* Exterior parents */forall the p ?
[0, i?
1] ?
[j + 1, n] doC[i, j, p] = max (SingleChild(i,j,p),EndpointsDiff(i,j,p),EndsFromLeftChild(i,j,p),EndsFromRightChild(i,j,p))/* Helper subproblems */for p ?
[0, n] doforall the x ?
PosChild[p] ?
x /?
[i, j] doif p 6= j thenD[i, j, p, x, T ] = Max2Subtrees(i, j, p, x, T )if p 6= i thenD[i, j, p, x, F ] = Max2Subtrees(i, j, p, x, F )Final answer: C[1, n, 0]Function SingleChild(i,j,p)X = PosChild[p] ?
[i, j]/* Interior p */if p > i ?
p < j thenreturn maxx?X C[i, p?
1, x]+C[p+ 1, j, x] + Score(p, x)/* Exterior p */elsereturn maxx?X C[i, j, x] + Score(p, x)Function EndpointsDiff(i,j,p)return maxk?
[i,j?1] C[i, k, p] + C[k + 1, j, p]Function EndsFromLeftChild(i,j,p)/* Interior p */if p > i ?
p < j thenX = PosChild[p] ?
[i, p?
1]forall the x ?
X ?
x < p doK[x] = [x, p?
1]/* Exterior p */elseX = PosChild[p] ?
[i, j]forall the x ?
X doK = [x, j ?
2]return maxx?X,k?K[x] C[i, k, x]+Score(p, x) +D[k + 1, j, p, x, T ]Function EndsFromRightChild(i,j,p)/* Interior p */if p > i ?
p < j thenX = PosChild[p] ?
[p+ 1, j]forall the x ?
X ?
x > p doK[x] = [p+ 1, x]/* Exterior p */elseX = PosChild[p] ?
[i, j]forall the x ?
X doK[x] = [i+ 2, x]return maxx?X,k?K[x] C[k, j, x]+Score(p, x) +D[i, k ?
1, p, x, F ]Function Max2Subtrees(i,j,p,x,pOnLeft)/* Interior p */if p ?
i ?
p ?
j thenif pOnLeft thenK = [p, j ?
1]return maxk?K C[i, k, p] + C[k + 1, j, x]elseK = [i, p?
1]return maxk?K C[i, k, x] + C[k + 1, j, p]/* Exterior p */elseK = [i, j ?
1]}if pOnLeft thenreturn maxk?K C[i, k, p] + C[k + 1, j, x]elsereturn maxk?K C[i, k, x] + C[k + 1, j, p]487ReferencesM.
Bodirsky, M. Kuhlmann, and M. Mo?hl.
2005.
Well-nested drawings as models of syntactic structure.
InIn Tenth Conference on Formal Grammar and NinthMeeting on Mathematics of Language, pages 88?1.University Press.X.
Carreras, M. Collins, and T. Koo.
2008.
Tag, dynamicprogramming, and the perceptron for efficient, feature-rich parsing.
In Proceedings of CoNLL, pages 9?16.Association for Computational Linguistics.X.
Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL, vol-ume 7, pages 957?961.J.
Chen-Main and A. Joshi.
2010.
Unavoidable ill-nestedness in natural language and the adequacy oftree local-mctag induced dependency structures.
InProceedings of the Tenth International Workshop onTree Adjoining Grammar and Related Formalisms(TAG+ 10).J.
Chen-Main and A.K.
Joshi.
2012.
A depen-dency perspective on the adequacy of tree local multi-component tree adjoining grammar.
In Journal ofLogic and Computation.
(to appear).N.
Chomsky.
1981.
Lectures on Government and Bind-ing.
Dordrecht: Foris.K.
Crammer and Y.
Singer.
2003.
Ultraconservative on-line algorithms for multiclass problems.
Journal ofMachine Learning Research, 3:951?991, March.J.
Eisner and G. Satta.
2000.
A faster parsing algorithmfor lexicalized tree-adjoining grammars.
In Proceed-ings of the 5th Workshop on Tree-Adjoining Grammarsand Related Formalisms (TAG+5), pages 14?19.J.
Eisner and N.A.
Smith.
2010.
Favor short dependen-cies: Parsing with soft and hard constraints on depen-dency length.
In Harry Bunt, Paola Merlo, and JoakimNivre, editors, Trends in Parsing Technology: Depen-dency Parsing, Domain Adaptation, and Deep Parsing,chapter 8, pages 121?150.
Springer.J.
Eisner.
2000.
Bilexical grammars and their cubic-time parsing algorithms.
In Harry Bunt and AntonNijholt, editors, Advances in Probabilistic and OtherParsing Technologies, pages 29?62.
Kluwer AcademicPublishers, October.R.
Fiengo.
1977.
On trace theory.
Linguistic Inquiry,8(1):35?61.C.
Go?mez-Rodr?
?guez, J. Carroll, and D. Weir.
2011.
De-pendency parsing schemata and mildly non-projectivedependency parsing.
Computational Linguistics,37(3):541?586.T.
Koo and M. Collins.
2010.
Efficient third-order de-pendency parsers.
In Proceedings of ACL, pages 1?11.T.
Koo, A. Globerson, X. Carreras, and M. Collins.
2007.Structured prediction models via the matrix-tree theo-rem.
In Proceedings of EMNLP-CoNLL.T.
Koo, A.M.
Rush, M. Collins, T. Jaakkola, and D. Son-tag.
2010.
Dual decomposition for parsing with non-projective head automata.
In Proceedings of EMNLP,pages 1288?1298.M.
Kuhlmann and J. Nivre.
2006.
Mildly non-projective dependency structures.
In Proceedings ofCOLING/ACL, pages 507?514.A.F.T.
Martins, N.A.
Smith, and E.P.
Xing.
2009.
Con-cise integer linear programming formulations for de-pendency parsing.
In Proceedings of ACL, pages 342?350.R.
McDonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In Pro-ceedings of EACL, pages 81?88.R.
McDonald and G. Satta.
2007.
On the complexityof non-projective data-driven dependency parsing.
InProceedings of the 10th International Conference onParsing Technologies, pages 121?132.R.
McDonald, K. Crammer, and F. Pereira.
2005a.
On-line large-margin training of dependency parsers.
InProceedings of ACL, pages 91?98.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005b.Non-projective dependency parsing using spanningtree algorithms.
In Proceedings of HLT-EMNLP,pages 523?530.T.
Reinhart.
1976.
The Syntactic Domain of Anaphora.Ph.D.
thesis, Massachusetts Institute of Technology.G.
Satta and W. Schuler.
1998.
Restrictions on tree ad-joining languages.
In Proceedings of COLING-ACL,pages 1176?1182.D.A.
Smith and N.A.
Smith.
2007.
Probabilistic modelsof nonprojective dependency trees.
In Proceedings ofEMNLP-CoNLL.488
