Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 768?775,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsDifferent Structures for Evaluating Answers to Complex Questions:Pyramids Won?t Topple, and Neither Will Human AssessorsHoa Trang DangInformation Access DivisionNational Institute of Standards and TechnologyGaithersburg, MD 20899hoa.dang@nist.govJimmy LinCollege of Information StudiesUniversity of MarylandCollege Park, MD 20742jimmylin@umd.eduAbstractThe idea of ?nugget pyramids?
has re-cently been introduced as a refinement to thenugget-based methodology used to evaluateanswers to complex questions in the TRECQA tracks.
This paper examines data fromthe 2006 evaluation, the first large-scale de-ployment of the nugget pyramids scheme.We show that this method of combiningjudgments of nugget importance from multi-ple assessors increases the stability and dis-criminative power of the evaluation while in-troducing only a small additional burden interms of manual assessment.
We also con-sider an alternative method for combiningassessor opinions, which yields a distinctionsimilar to micro- and macro-averaging in thecontext of classification tasks.
While thetwo approaches differ in terms of underly-ing assumptions, their results are neverthe-less highly correlated.1 IntroductionThe emergence of question answering (QA) systemsfor addressing complex information needs has ne-cessitated the development and refinement of newmethodologies for evaluating and comparing sys-tems.
In the Text REtrieval Conference (TREC) QAtracks organized by the U.S. National Institute ofStandards and Technology (NIST), improvements inevaluation processes have kept pace with the evolu-tion of QA tasks.
For the past several years, NISThas implemented an evaluation methodology basedon the notion of ?information nuggets?
to assess an-swers to complex questions.
As it has become thede facto standard for evaluating such systems, theresearch community stands to benefit from a betterunderstanding of the characteristics of this evalua-tion methodology.This paper explores recent refinements to thenugget-based evaluation methodology developed byNIST.
In particular, we examine the recent so-called?pyramid extension?
that incorporates relevancejudgments from multiple assessors to improve eval-uation stability (Lin and Demner-Fushman, 2006).We organize our discussion as follows: The nextsection begins by providing a brief overview ofnugget-based evaluations and the pyramid exten-sion.
Section 3 presents results from the first large-scale implementation of nugget pyramids for QAevaluation in TREC 2006.
Analysis shows that thisextension improves both stability and discriminativepower.
In Section 4, we discuss an alternative forcombining multiple judgments that parallels the dis-tinction between micro- and macro-averaging oftenseen in classification tasks.
Experiments reveal thatthe methods yield almost exactly the same results,despite operating on different granularities (individ-ual nuggets vs. individual users).2 Evaluating Complex QuestionsComplex questions are distinguished from factoidquestions such as ?Who shot Abraham Lincoln??
inthat they cannot be answered by named entities (e.g.,persons, organizations, dates, etc.).
Typically, theseinformation needs are embedded in the context of ascenario (i.e., user task) and often require systems to768synthesize information from multiple documents orto generate answers that cannot be easily extracted(e.g., by leveraging inference capabilities).To date, NIST has already conducted severallarge-scale evaluations of complex questions: def-inition questions in TREC 2003, ?Other?
ques-tions in TREC 2004?2006, ?relationship?
questionsin TREC 2005, and the complex, interactive QA(ciQA) task in TREC 2006.
Definition and Otherquestions are similar in that they both request novelfacts about ?targets?, which can be persons, orga-nizations, things, and events.
Relationship ques-tions evolved into the ciQA task and focus on in-formation needs such as ?What financial relation-ships exist between South American drug cartels andbanks in Liechtenstein??
Such complex questionsfocus on ties (financial, military, familial, etc.)
thatconnect two or more entities.
All of these evalua-tions have employed the nugget-based methodology,which demonstrates its versatility and applicabilityto a wide range of information needs.2.1 Basic SetupIn the TREC QA evaluations, an answer to acomplex question consists of an unordered set of[document-id, answer string] pairs, where the stringsare presumed to provide some relevant informationthat addresses the question.
Although no explicitlimit is placed on the length of the answer, the finalmetric penalizes verbosity (see below).Evaluation of system output proceeds in twosteps.
First, answer strings from all submissionsare gathered together and presented to a single as-sessor.
The source of each answer string is blindedso that the assessor can not obviously tell whichsystems generated what output.
Using these an-swers and searches performed during question de-velopment, the assessor creates a list of relevantnuggets.
A nugget is a piece of information (i.e.,?fact?)
that addresses one aspect of the user?s ques-tion.
Nuggets should be atomic, in the sense thatan assessor should be able to make a binary de-cision as to whether the nugget appears in an an-swer string.
Although a nugget represents a con-ceptual entity, the assessor provides a natural lan-guage description?primarily as a memory aid forthe subsequent evaluation steps.
These descriptionsrange from sentence-length document extracts tor = # of vital nuggets returneda = # of okay nuggets returnedR = # of vital nuggets in the answer keyl = # of non-whitespace characters in entire runrecall: R = r/Rallowance: ?
= 100?
(r + a)precision: P ={1 if l < ?1?
l?
?l otherwiseF (?)
= (?2 + 1)?
P ?R?2 ?
P +RFigure 1: Official definition of F-score for nuggetevaluation in TREC.key phrases to telegraphic short-hand notes?theirreadability greatly varies from assessor to assessor.The assessor also manually classifies each nuggetas either vital or okay (non-vital).
Vital nuggets rep-resent concepts that must be present in a ?good?
an-swer.
Okay nuggets may contain interesting infor-mation, but are not essential.In the second step, the same assessor who cre-ated the nuggets reads each system?s output in turnand marks the appearance of the nuggets.
An an-swer string contains a nugget if there is a conceptualmatch; that is, the match is independent of the partic-ular wording used in the system?s output.
A nuggetmatch is marked at most once per run?i.e., a sys-tem is not rewarded for retrieving a nugget multipletimes.
If the system?s output contains more than onematch for a nugget, the best match is selected andthe rest are left unmarked.
A single [document-id,answer string] pair in a system response can match0, 1, or multiple nuggets.The final F-score for an answer is calculated in themanner described in Figure 1, and the final score ofa run is the average across the F-scores of all ques-tions.
The metric is a weighted harmonic mean be-tween nugget precision and nugget recall, where re-call is heavily favored (controlled by the ?
parame-ter, usually set to three).
Nugget recall is calculatedsolely on vital nuggets, while nugget precision is ap-proximated by a length allowance based on the num-ber of both vital and okay nuggets returned.
In an769earlier pilot study, researchers discovered that it wasnot possible for assessors to consistently enumer-ate the total set of nuggets contained in an answer,which corresponds to the denominator in a precisioncalculation (Voorhees, 2003).
Thus, a penalty forverbosity serves as a surrogate for precision.2.2 The Pyramid ExtensionThe vital/okay distinction has been identified asa weakness in the TREC nugget-based evalua-tion methodology (Hildebrandt et al, 2004; Linand Demner-Fushman, 2005; Lin and Demner-Fushman, 2006).
There do not appear to be any re-liable indicators for predicting nugget importance,which makes it challenging to develop algorithmssensitive to this consideration.
Since only vitalnuggets affect nugget recall, it is difficult for sys-tems to achieve non-zero scores on topics with fewvital nuggets in the answer key.
Thus, scores areeasily affected by assessor errors and other randomvariations in evaluation conditions.One direct consequence is that in previous TRECevaluations, the median score for many questionsturned out to be zero.
A binary distinction on nuggetimportance is insufficient to discriminate betweenthe quality of runs that return no vital nuggets butdifferent numbers of okay nuggets.
Also, a scoredistribution heavily skewed towards zero makesmeta-analyses of evaluation stability difficult to per-form (Voorhees, 2005).The pyramid extension (Lin and Demner-Fushman, 2006) was proposed to address the issuesmentioned above.
The idea was relatively simple: bysoliciting vital/okay judgments from multiple asses-sors (after the list of nuggets has been produced bya primary assessor), it is possible to define nuggetimportance with greater granularity.
Each nugget isassigned a weight between zero and one that is pro-portional to the number of assessors who judged itto be vital.
Nugget recall from Figure 1 can be rede-fined to incorporate these weights:R =?m?Awm?n?V wnWhere A is the set of reference nuggets that arematched in a system?s output and V is the set of allreference nuggets; wm and wn are the weights ofnuggets m and n, respectively.1 The calculation ofnugget precision remains the same.3 Nugget Pyramids in TREC 2006Lin and Demner-Fushman (2006) present exper-imental evidence in support of nugget pyramidsby applying the proposal to results from previousTREC QA evaluations.
Their simulation studies ap-pear to support the assertion that pyramids addressmany of the issues raised in Section 2.2.
Based onthe results, NIST proceeded with a trial deploymentof nugget pyramids in the TREC 2006 QA track.
Al-though scores based on the binary vital/okay distinc-tion were retained as the ?official?
metric, pyramidscores were simultaneously computed.
This pro-vided an opportunity to compare the two method-ologies on a large scale.3.1 The DataThe basic unit of evaluation for the main QA taskat TREC 2006 was the ?question series?.
Each se-ries focused on a ?target?, which could be a person,organization, thing, or event.
Individual questionsin a series inquired about different facets of the tar-get, and were explicitly classified as factoid, list, orOther.
One complete series is shown in Figure 2.The Other questions can be best paraphrased as ?Tellme interesting things about X that I haven?t alreadyexplicitly asked about.?
It was the system?s task toretrieve interesting nuggets about the target (in theopinion of the assessor), but credit was not givenfor retrieving facts already explicitly asked for in thefactoid and list questions.
The Other questions wereevaluated using the nugget-based methodology, andare the subject of this analysis.The QA test set in TREC 2006 contained 75 se-ries.
Of the 75 targets, 19 were persons, 19 wereorganizations, 19 were events, and 18 were things.The series contained a total of 75 Other questions(one per target).
Each series contained 6?9 ques-tions (counting the Other question), with most se-ries containing 8 questions.
The task employed theAQUAINT collection of newswire text (LDC cat-alog number LDC2002T31), consisting of Englishdata drawn from three sources: the New York Times,1Note that this new scoring model captures the existingbinary vital/okay distinction in a straightforward way: vitalnuggets get a score of one, and okay nuggets zero.770147 Britain?s Prince Edward marries147.1 FACTOID When did Prince Edward engage to marry?147.2 FACTOID Who did the Prince marry?147.3 FACTOID Where did they honeymoon?147.4 FACTOID Where was Edward in line for the throne at the time of the wedding?147.5 FACTOID What was the Prince?s occupation?147.6 FACTOID How many people viewed the wedding on television?147.7 LIST What individuals were at the wedding?147.8 OTHERFigure 2: Sample question series from TREC 2006.Nugget 0 1 2 3 4 5 6 7 8The couple had a long courtship 1 0 0 0 0 0 1 1 0Queen Elizabeth II was delighted with the match 0 1 0 1 0 0 0 0 1Queen named couple Earl and Contessa of Wessex 0 1 0 0 1 1 1 0 0All marriages of Edward?s siblings ended in divorce 0 0 0 0 0 1 0 0 1Edward arranged for William to appear more cheerful in photo 0 0 0 0 0 0 0 0 0they were married in St. Georges Chapel, Windsor 1 1 1 0 1 0 1 1 0Figure 3: Multiple assessors?
judgments of nugget importance for Series 147 (vital=1, okay=0).
Assessor 2was the same as the primary assessor (assessor 0), but judgments were elicited at different times.the Associated Press, and the Xinhua News Service.There are approximately one million articles in thecollection, totaling roughly three gigabytes.
In to-tal, 59 runs from 27 participants were submitted toNIST.
For more details, see (Dang et al, 2006).For the Other questions, nine sets of judgmentswere elicited from eight judges (the primary assessorwho originally created the nuggets later annotatedthe nuggets once again).
Each assessor was asked toassign the vital/okay label in a rapid fashion, withoutgiving each decision much thought.
Figure 3 givesan example of the multiple judgments for nuggets inSeries 147.
There is variation in notions of impor-tance not only between different assessors, but alsofor a single assessor over time.3.2 ResultsAfter the human annotation process, nugget pyra-mids were built in the manner described by Lin andDemner-Fushman (2006).
Two scores were com-puted for each run submitted to the TREC 2006 mainQA task: one based on the vital/okay judgments ofthe primary assessor (which we call the binary F-score) and one based on the nugget pyramids (thepyramid F-score).
The characteristics of the pyra-mid method can be inferred by comparing these twosets of scores.Figure 4 plots the average binary and averagepyramid F-scores for each run (which represents av-erage performance across all series).
Even thoughthe nugget pyramid does not represent any singlereal user (a point we return to later), pyramid F-scores do correlate highly with the binary F-scores.The Pearson?s correlation is 0.987, with a 95% con-fidence interval of [0.980, 1.00].While the average F-score for a run is stable givena sufficient number of questions, the F-score fora single Other question exhibits greater variabilityacross assessors.
This is shown in Figure 5, whichplots binary and pyramid F-scores for individualquestions from all runs.
In this case, the Pearsoncorrelation is 0.870, with a 95% confidence intervalof [0.863, 1.00].For 16.4% of all Other questions, the nugget pyra-mid assigned a non-zero F-score where the origi-nal binary F-score was zero.
This can be seen inthe band of points on the left edge of the plot inFigure 5.
This highlights the strength of nugget7710.000.050.100.150.200.250.000.050.100.150.200.25Averagebinary F?scoreAverage pyramid F?scoreFigure 4: Scatter plot comparing the binary andpyramid F-scores for each run.pyramids?their ability to smooth out assessor dif-ferences and more finely discriminate among sys-tem outputs.
This is a key capability that is usefulfor system developers, particularly since algorithmicimprovements are often incremental and small.Because it is more stable than the single-assessormethod of evaluation, the pyramid method also ap-pears to have greater discriminative power.
We fita two-way analysis of variance model with the se-ries and run as factors, and the binary F-score asthe dependent variable.
We found significant differ-ences between series and between runs (p essentiallyequal to 0 for both factors).
To determine which runswere significantly different from each other, we per-formed a multiple comparison using Tukey?s hon-estly significant difference criterion and controllingfor the experiment-wise Type I error so that the prob-ability of declaring a difference between two runs tobe significant, when it is actually not, is at most 5%.With 59 runs, there are C592 = 1711 different pairsthat can be compared.
The single-assessor methodwas able to declare one run to be significantly betterthan the other in 557 of these pairs.
Using the pyra-mid F-scores, it was possible to find significant dif-ferences in performance between runs in 617 pairs.3.3 DiscussionAny evaluation represents a compromise betweeneffort (which correlates with cost) and insightful-ness of results.
The level of detail and meaning-0.00.20.40.60.80.00.20.40.60.8Binary F?scorePyramid F?scoreFigure 5: Scatter plot comparing the binary andpyramid F-scores for each Other question.fulness of evaluations are constantly in tension withthe availability of resources.
Modifications to exist-ing processes usually come at a cost that needs to beweighed against potential gains.
Based on these con-siderations, the balance sheet for nugget pyramidsshows a favorable orientation.
In the TREC 2006QA evaluation, soliciting vital/okay judgments frommultiple assessors was not very time-consuming (acouple of hours per assessor).
Analysis confirmsthat pyramid scores confer many benefits at an ac-ceptable cost, thus arguing for its adoption in futureevaluations.Cost considerations precluded exploring other re-finements to the nugget-based evaluation methodol-ogy.
One possible alternative would involve ask-ing multiple assessors to create different sets ofnuggets from scratch.
Not only would this be time-consuming, one would then need to deal with theadditional complexities of aligning each assessor?snuggets list.
This includes resolving issues such asnugget granularity, overlap in information content,implicature and other relations between nuggets, etc.4 Exploration of Alternative StructuresDespite the demonstrated effectiveness of nuggetpyramids, there are a few potential drawbacks thatare worth discussing.
One downside is that thenugget pyramid does not represent a single assessor.The nugget weights reflect the aggregation of opin-ions across a sample population, but there is no guar-772antee that the method for computing those weightsactually captures any aspect of real user behavior.It can be argued that the binary F-score is more re-alistic since it reflects the opinion of a real user (theprimary assessor), whereas the pyramid F-score triesto model the opinion of a mythical average user.Although this point may seem somewhat counter-intuitive, it represents a well-established traditionin the information retrieval literature (Voorhees,2002).
In document retrieval, for example, relevancejudgments are provided by a single assessor?eventhough it is well known that there are large indi-vidual differences in notions of relevance.
IR re-searchers believe that human idiosyncrasies are aninescapable fact present in any system designed forhuman users, and hence any attempt to remove thoseelements in the evaluation setup is actually undesir-able.
It is the responsibility of researchers to developsystems that are robust and flexible.
This premise,however, does not mean that IR evaluation resultsare unstable or unreliable.
Analyses have shownthat despite large variations in human opinions, sys-tem rankings are remarkably stable (Voorhees, 2000;Sormunen, 2002)?that is, one can usually be confi-dent about system comparisons.The philosophy in IR sharply contrasts with workin NLP annotation tasks such as parsing, word sensedisambiguation, and semantic role labeling?whereresearchers strive for high levels of interannota-tor agreement, often through elaborate guidelines.The difference in philosophies arises because unlikethese NLP annotation tasks, where the products areused primarily by other NLP system components, IR(and likewise QA) is an end-user task.
These sys-tems are intended for real world use.
Since peoplediffer, systems must be able to accommodate thesedifferences.
Hence, there is a strong preference inQA for evaluations that maintain a model of the in-dividual user.4.1 Micro- vs. Macro-AveragingThe current nugget pyramid method leverages mul-tiple judgments to define a weight for each individ-ual nugget, and then incorporates this weight intothe F-score computation.
As an alternative, we pro-pose another method for combining the opinions ofmultiple assessors: evaluate system responses indi-vidually against N sets of binary judgments, andthen compute the mean across those scores.
We de-fine the macro-averaged binary F-score over a setA = {a1, ..., aN} of N assessors as:F =?a?A FaNWhere Fa is the binary F-score according to thevital/okay judgments of assessor a.
The differ-ences between the pyramid F-score and the macro-averaged binary F-score correspond to the distinc-tion between micro- and macro-averaging discussedin the context of text classification (Lewis, 1991).In those applications, both measures are mean-ingful depending on focus: individual instances orentire classes.
In tasks where it is importantto correctly classify individual instances, micro-averaging is more appropriate.
In tasks where itis important to correctly identify a class, macro-averaging better quantifies performance.
In classi-fication tasks, imbalance in the prevalence of eachclass can lead to large differences in macro- andmicro-averaged scores.
Analogizing to our work,the original formulation of nugget pyramids corre-sponds to micro-averaging (since we focus on indi-vidual nuggets), while the alternative corresponds tomacro-averaging (since we focus on the assessor).We additionally note that the two methods en-code different assumptions.
Macro-averaging as-sumes that there is nothing intrinsically interestingabout a nugget?it is simply a matter of a particularuser with particular needs finding a particular nuggetto be of interest.
Micro-averaging, on the other hand,assumes that some nuggets are inherently interest-ing, independent of the particular interests of users.2Each approach has characteristics that make itdesirable.
From the perspective of evaluators, themacro-averaged binary F-score is preferable be-cause it models real users; each set of binary judg-ments represents the information need of a real user,each binary F-score represents how well an answerwill satisfy a real user, and the macro-averaged bi-nary F-score represents how well an answer will sat-isfy, on average, a sample population of real users.From the perspective of QA system developers, themicro-averaged nugget pyramid F-score is prefer-able because it allows finer discrimination in in-2We are grateful to an anonymous reviewer for this insight.773dividual nugget performance, which enables bettertechniques for system training and optimization.The macro-averaged binary F-score has the samedesirable properties as the micro-averaged pyramidF-score in that fewer responses will have zero F-scores as compared to the single-assessor binary F-score.
We demonstrate this as follows.
Let X be aresponse that receives a non-zero pyramid F-score.Let A = {a1, a2, a3, ..., aN} be the set of N asses-sors.
Then it can be proven that X also receives anon-zero macro-averaged binary F-score:1.
There exists some nugget v with weight greaterthan 0, such that an answer string r in Xmatches v. (def.
of pyramid recall)2.
There exists some assessor ap ?
Awhomarkedv as vital.
(def.
of pyramid nugget weight)3.
To show that X will also receive a non-zeromacro-averaged binary score, it is sufficient toshow that there is some assessor am ?
A suchthatX receives a non-zero F-score when evalu-ated using just the vital/okay judgments of am.(def.
of macro-averaged binary F-score)4.
But, such an assessor does exist, namely asses-sor ap: Consider the binary F-score assignedto X according to just assessor ap.
The re-call of X is greater than zero, since X containsthe response r that matches the nugget v thatwas marked as vital by ap (from (2), (1), andthe def.
of recall).
The precision must also begreater than zero (def.
of precision).
Therefore,the macro-averaged binary F-score ofX is non-zero.
(def.
of F-score)4.2 Analysis from TREC 2006While the macro-averaged method is guaranteed toproduce no more zero-valued scores than the micro-averaged pyramid method, it is not guaranteed thatthe scores will be the same for any given response.What are the empirical characteristics of each ap-proach?
To explore this question, we once again ex-amined data from TREC 2006.Figure 6 shows a scatter plot of the pyramid F-score and macro-averaged binary F-score for everyOther questions in all runs from the TREC 2006QA track main task.
Despite focusing on differ-ent aspects of the evaluation setup, these measures0.00.20.40.60.80.00.20.40.60.8PyramidF?scoreMacro?averaged binary F?scoreFigure 6: Scatter plot comparing the pyramid andmacro-averaged binary F-scores for all questions.binary micro macrobinary 1.000/1.000 0.870/0.987 0.861/0.988micro - 1.000/1.000 0.985/0.996macro - - 1.000/1.000Table 1: Pearson?s correlation of F-scores, by ques-tion and by run.are highly correlated, even at the level of individ-ual questions.
Table 1 provides a summary of thecorrelations between the original binary F-score, the(micro-averaged) pyramid F-score, and the macro-averaged binary F-score.
Pearson?s r is given forF-scores at the individual question level (first num-ber) and at the run level (second number).
The cor-relation between all three variants are about equal atthe level of system runs.
At the level of individualquestions, the micro- and macro-averaged F-scores(using multiple judgments) are still highly correlatedwith each other, but each is less correlated with thesingle-assessor binary F-score.4.3 DiscussionThe differences between macro- and micro-averaging methods invoke a more general discus-sion on notions of nugget importance.
There areactually two different issues we are attempting toaddress with our different approaches: the first isa more granular scale of nugget importance, thesecond is variations across a population of users.
In774the micro-averaged pyramid F-scores, we achievethe first by leveraging the second, i.e., binaryjudgments from a large population are combinedto yield weights for individual nuggets.
In themacro-averaged binary F-score, we focus solely onpopulation effects without addressing granularity ofnugget importance.Exploring this thread of argument, we can for-mulate additional approaches for tackling these is-sues.
We could, for example, solicit more granularindividual judgments on each nugget from each as-sessor, perhaps on a Likert scale or as a continuousquantity ranging from zero to one.
This would yieldtwo more methods for computing F-scores, both amacro-averaged and a micro-averaged variant.
Themacro-averaged variant would be especially attrac-tive because it reflects real users and yet individualF-scores remain discriminative.
Despite its possi-ble advantages, this extension is rejected based onresource considerations; making snap binary judg-ments on individual nuggets is much quicker than amulti-scaled value assignment?at least at present,the additional costs are not sufficient to offset thepotential gains.5 ConclusionThe important role that large-scale evaluations playin guiding research in human language technologiesmeans that the community must ?get it right.?
Thiswould ordinarily call for a more conservative ap-proach to avoid changes that might have unintendedconsequences.
However, evaluation methodologiesmust evolve to reflect the shifting interests of the re-search community to remain relevant.
Thus, orga-nizers of evaluations must walk a fine line betweenprogress and chaos.
Nevertheless, the introductionof nugget pyramids in the TREC QA evaluation pro-vides a case study showing how this fine balance canindeed be achieved.
The addition of multiple judg-ments of nugget importance yields an evaluation thatis both more stable and more discriminative than theoriginal single-assessor evaluation, while requiringonly a small additional cost in terms of human labor.We have explored two different methods for com-bining judgments from multiple assessors to addressshortcomings in the original nugget-based evalua-tion setup.
Although they make different assump-tions about the evaluation, results from both ap-proaches are highly correlated.
Thus, we can con-tinue employing the pyramid-based method, whichis well-suited for developing systems, and still be as-sured that the results remain consistent with an eval-uation method that maintains a model of real indi-vidual users.AcknowledgmentsThis work has been supported in part by DARPAcontract HR0011-06-2-0001 (GALE).
The secondauthor would like to thank Kiri and Esther for theirkind support.ReferencesH.
Dang, J. Lin, and D. Kelly.
2006.
Overview of theTREC 2006 question answering track.
In Proc.
ofTREC 2006.W.
Hildebrandt, B. Katz, and J. Lin.
2004.
Answeringdefinition questions with multiple knowledge sources.In Proc.
HLT/NAACL 2004.D.
Lewis.
1991.
Evaluating text categorization.
In Proc.of the Speech and Natural Language Workshop.J.
Lin and D. Demner-Fushman.
2005.
Automaticallyevaluating answers to definition questions.
In Proc.
ofHLT/EMNLP 2005.J.
Lin and D. Demner-Fushman.
2006.
Will pyramidsbuilt of nuggets topple over?
In Proc.
of HLT/NAACL2006.E.
Sormunen.
2002.
Liberal relevance criteria ofTREC?counting on negligible documents?
In Proc.of SIGIR 2002.E.
Voorhees.
2000.
Variations in relevance judgmentsand the measurement of retrieval effectiveness.
IP&M,36(5):697?716.E.
Voorhees.
2002.
The philosophy of information re-trieval evaluation.
In Proc.
of CLEF Workshop.E.
Voorhees.
2003.
Overview of the TREC 2003 ques-tion answering track.
In Proc.
of TREC 2003.E.
Voorhees.
2005.
Using question series to evaluatequestion answering system effectiveness.
In Proc.
ofHLT/EMNLP 2005.775
