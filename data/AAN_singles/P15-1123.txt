Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1272?1281,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsDiscourse-sensitive Automatic Identification of Generic ExpressionsAnnemarie Friedrich Manfred PinkalDepartment of Computational LinguisticsSaarland University, Saarbr?ucken, Germany{afried,pinkal}@coli.uni-saarland.deAbstractThis paper describes a novel sequence la-beling method for identifying generic ex-pressions, which refer to kinds or arbitrarymembers of a class, in discourse context.The automatic recognition of such expres-sions is important for any natural languageprocessing task that requires text under-standing.
Prior work has focused on iden-tifying generic noun phrases; we presenta new corpus in which not only subjectsbut also clauses are annotated for generic-ity according to an annotation scheme mo-tivated by semantic theory.
Our context-aware approach for automatically identi-fying generic expressions uses conditionalrandom fields and outperforms previouswork based on local decisions when evalu-ated on this corpus and on related data sets(ACE-2 and ACE-2005).1 IntroductionDistinguishing between statements about particu-lar individuals or situations and generic sentencesis an important part of human language under-standing.
Consider example (1): sentence (a)names characteristic attributes of a kind, which areinherent to every (typical) individual, and sentence(b) describes a specific individual.
(1) (a) The modern domestic horse has a lifeexpectancy of 25 to 30 years.
(generic)(a) Old Billy lived to the age of 62.
(non-generic)The above example illustrates that generic andnon-generic sentences differ substantially in theirsemantic impact and entailment properties.
It canbe inferred from sentence (1a) that a typical horsehas a life expectancy of 25 to 30 years, and if weknow that Nelly is a horse, we can infer that its lifeexpectancy is 25 to 30 years.
Sentence (1b) hasno such properties, it only allows inferences aboutthe particular individual Old Billy.An automatic classifier that recognizes genericexpressions would be extremely valuable for var-ious kinds of natural language processing sys-tems: for text understanding and question answer-ing systems, through the improvement of textualentailment methods, and for systems acquiringmachine-readable knowledge from text.
Machine-readable knowledge bases have different repre-sentations for statements corresponding to genericknowledge about kinds and knowledge about spe-cific individuals.
The non-generic sentence (1b)roughly speaking provides ABox content for amachine-readable knowledge base, i.e., knowl-edge about particular instances, e.g, ?A is an in-stance of B / has property X?.
In contrast, thegeneric sentence (1a) feeds the TBox, i.e., knowl-edge of the form ?All B are C / have property X?.Reiter and Frank (2010) provide a detailed discus-sion of the relevance of the distinction betweenclasses and instances for automatic ontology con-struction.In this paper, we present a new corpus anno-tated in a linguistically motivated way for gener-icity, and a context-sensitive computational modelfor labeling sequences of clauses or noun phrases(NPs) with their genericity status.
Both manualannotation and automatic recognition of genericexpressions are challenging tasks: virtually all NPtypes ?
definites, indefinites and quantified NPs,full NPs, pronouns, and even proper names (e.g.species names such as Elephas maximus) ?
can befound in generic and non-generic uses dependingon their clausal context.In this work, we call clauses generic if they pro-vide a general characterization of entities of a cer-tain kind, and we call mentions of NPs generic ifthey refer to kinds or arbitrary members of a class.Although genericity on the clause- and NP-level1272are strongly interrelated, the concepts do not al-ways coincide.
As example (2) shows, sentencesdescribing episodic events can have a generic NPas their subject.
Note that references to species arekind-referring / generic on the NP level (followingKrifka et al (1995), see p.
65).
(2) In September 2013 the blobfish was voted the?World?s Ugliest Animal?.
(subject generic,clause non-generic)Genericity often cannot be annotated withoutpaying attention to the wider discourse context.Clearly, coreference information is needed for thegenericity classification of pronouns.
Often, evengenericity of full NPs or entire clauses cannot bedecided in isolation, as illustrated by example (3).Sentence (b) could be part of a particular narrativeabout a tree, or it could be a generic statement.Only the context given by (a) clarifies that (b) in-deed makes reference to any year?s new twigs andis to be interpreted as generic.
(3) (a) Sugar maples also have a tendency tocolor unevenly in fall.
(generic)(b) The recent year?s growth twigs are greenand turn dark brown.
(generic)In computational linguistics, most research ondetecting genericity has been done in relation tothe ACE corpora (Mitchell et al, 2003; Walkeret al, 2006), focusing on assigning genericity la-bels to noun phrases (Suh et al, 2006; Reiter andFrank, 2010), see Section 2.
Our work is basedon these approaches, most notably on the work ofReiter and Frank (2010), and extends upon themin the following essential ways.The major contributions of this work are: (1)We create a new corpus of Wikipedia articles an-notated with linguistically motivated genericity la-bels both on the subject- and clause-level (see Sec-tion 3).
The corpus is balanced with respect togenericity and about 10,000 clauses in size.
(2) Wepresent a discourse-sensitive genericity labeler.Technically, we use conditional random fields asa sequence labeling method (Section 4).
We trainand evaluate our method on the Wikipedia datasetand the ACE corpora, evaluating both the tasks ofpredicting NP genericity and the task of predictingclause-level genericity.
Our labeler outperformsthe state-of-the-art by a margin of 6.6-11.9% (de-pending on the data set) in terms of accuracy, at thesame time increasing F1-score.
Much of the per-formance gain is due to the inclusion of discourseinformation.
For the discussion of our experimen-tal results, see Section 5.In this paper, we do not address the followingtwo important aspects of genericity.
First, habit-ual sentences form a class of generalizing state-ments which bear a close relation to generics.
Ascan be seen in example (4), they describe a char-acterizing property of either a specific entity or aclass by generalizing over situations instead of orin addition to entities (Carlson, 2005).
We clas-sify habitual sentences with a generic subject asgeneric, and habitual sentences which describe aspecific entity as non-generic, leaving the task ofhabituality detection for future work.
(4) (a) John smokes after dinner.
(b) Gentlemen smoke after dinner.Second, generic clauses express regularitieswithin classes of entities, and thus are similar touniversally quantified sentences in their truth con-ditions and entailment properties.
However, theirtruth-conditional interpretation is tricky, since theyexpress typicality, describe stereotypes and al-low exceptions, for example Dutchmen are goodsailors is not false even if most Dutchmen do notsail at all (Carlson, 1977).
We concentrate on thedecision of whether a clause is generic or not, andleave the truth-conditional interpretation for fur-ther work.
For a detailed discussion of the seman-tics of generics expressions see the comprehensivesurvey by Krifka et al (1995); a short and instruc-tive overview can be found in the first part of (Re-iter and Frank, 2010).2 Related WorkIn this section, we first briefly review previouslydeveloped annotation schemes for genericity.
Wethen describe work on automatically predicting thegenericity of NPs or different types of clauses.Annotation.
ACE-2 (Mitchell et al, 2003) andACE-2005 (Walker et al, 2006) are the two mostnotable annotation projects for labeling genericityof NPs to date.
In the ACE-2 corpus, 40106 en-tity mentions in 520 newswire and broadcast doc-uments are marked with regard to whether they re-fer to ?any member of the set in question?
(GEN,generic) rather than ?some particular, identifiablemember of that set?
(SPC, specific/non-generic).The major drawback of ACE-2 is that genericity isbasically defined as lack of specificity, which leads1273to uncertainty and inconsistencies in the annota-tion process, and to a heterogeneous set of NPslabeled with GEN, including quantificational NPsand NPs in modalized, future, conditional, hypo-thetical, negated, uncertain, and question contexts.In addition, in both ACE-2 and ACE-2005, pred-icative and modifier uses of nouns, to which thegenericity distinction is not applicable, also re-ceive labels (e.g.
John seems to be a nice person /a subway system).In the updated guidelines of ACE-2005, the la-bel USP (underspecified) is introduced for non-generic non-specific reference, including NPs inthe various contexts mentioned above that wereimproperly labeled as generic in ACE-2.
Theclass also contains mentions of an entity whoseidentity would be ?difficult to locate?
(Officials re-ported ...).
Moreover, annotators are asked to marktruly ambiguous cases that have both a genericand a non-generic reading as USP.
Finally, NEG(negated) marks negatively quantified entities thatrefer to the empty set of the kind mentioned.While we agree that in general there are under-specified cases, the guidelines for ACE-2005 mixother phenomena into the USP class, resulting ina high confusion between USP and both of thelabels SPC and GEN in the manual annotations(Friedrich et al, 2015).
Data from two annota-tors is available, and we compute an agreement ofCohen?s ?
= 0.53 over the four labels.
The ACEcorpora consist only of news data, and the distribu-tions of labels are highly skewed towards specificmentions.
For some criticism of the ACE annota-tion scheme, see also Suh (2006).Several linguistically motivated annotationstudies targeting genericity of noun phrases bearsimilarity to our annotation scheme (Section 3),but comprise very little data (Poesio, 2004; Herbe-lot and Copestake, 2009).
In the ARRAU corpus(Poesio and Artstein, 2008), about 24321 mark-ables are tagged for genericity.Nedoluzhko (2013) survey the treatment ofgenericity phenomena within coreference resolu-tion research; they find a consistent definition ofgenericity to be lacking.
Friedrich and Palmer(2014b) present an annotation scheme for situa-tion types including generic sentences, which theyfind to be infrequent in their corpus consisting ofnews, jokes and (fund-raising) letters.
Our newWikiGenerics corpus contains more than 10,000clauses, approximately half of which are generic.Automatic Identification of Genericity.
Suh etal.
(2006) propose a rule-based approach, whichextracts only bare plurals and singular NPs quanti-fied with every or any as generic.
Reiter and Frank(2010) use a wide range of syntactic and semanticfeatures to train a supervised classifier for identi-fying generic NPs.
We compare to their method(described in detail in Section 5.2) as a highly-competitive baseline.Palmer et al (2007) classify clauses into severaltypes of situation entities including states, events,generalizing sentences (habitual utterances refer-ring to specific individuals) and generic sentences.They find that using context by using the labels ofpreceding clauses as features improves the classi-fication of clause types, but generic sentences areextremely sparse in their data set.
Our present ap-proach uses a sequence labeling model that com-putes the best labeling for an entire sequence.3 WikiGenerics: Data and AnnotationsIn order to study generics in a genre other thannews (as in ACE), we turn to an encyclopedia, inwhich we expect many generics.
We create ourWikiGenerics corpus1as follows.
We aim to cre-ate a corpus that is balanced in the sense that itcontains many generic and non-generic sentences,and also generics from many different domains.We collect 102 texts about animals, organisedcrime, ethnic groups, games, sports, medicine,music, politics, religion, scientific disciplines andbiographies from Wikipedia.
For example, somesentences make statements about a ?natural?
kind(Blobfish are typically shorter than 30 cm), othersexpress definitions such as the rules of a footballgame (The offensive team must line up in a legalformation before they can snap the ball).Generic clauses have the typical form of a pred-icative statement about the sentence topic, whichis normally realized as the grammatical subject inEnglish.
Intuitions about NP-level genericity andits relation to clause-level genericity are quite reli-able for topic NPs of clauses, which also typicallyoccur in subject position in English.
Since gener-ics in non-subject positions are less frequent andhard to interpret (see the discussion of ?dependentgenerics?
by Link (1995)), we decided to annotatesubject NPs only.
We are aware that we are miss-ing relevant cases (e.g.
the less preferred reading1The WikiGenerics corpus is freely available at:www.coli.uni-saarland.de/projects/sitent1274of Cats chase mice, which attributes to mice theproperty of being chased by cats), but in this work,we want to study the ?easier?
subject cases as afirst step.We use the discourse parser SPADE (Soricutand Marcu, 2003) to automatically segment thefirst 70 sentences of each article into clauses.
Eachclause is manually annotated with the followinginformation (for more details on the annotationscheme, see (Friedrich et al, 2015)):?
Task NP: whether or not the subject NP of theclause refers to a class or kind (generic vs. non-generic);?
Task Cl: whether the clause is generic, definedas a clause that makes a characterizing state-ment about a class or kind, or non-generic.?
Task Cl+NP: using the information from TaskNP and Cl above, we automatically derive thefollowing classification for each clause (com-pare to the explanation of example (2)).?
GEN gen: generic clause, subject is genericby definition (The lion is a predatory cat);?
NON-GEN non-gen: non-generic clausewith a non-generic subject ( Simba roared);?
or NON-GEN gen: episodic clause with ageneric subject (Dinosaurs died out).?
GEN non-gen does not exist by definition.We construct the gold standard for our experi-ments via majority voting over the labels given bythree paid annotators, students of computationallinguistics.
Annotators were given a written man-ual and a short training on documents not includedin the corpus.
They are given the option to indicatesegmentation errors, e.g.
that two segments shouldactually be one, or that one segment contains mul-tiple clauses.
In the latter case, we ask them to givelabels for the first clause in the segment.
10240(86%) of all pre-segmented clauses received labelsfor all three tasks from all annotators, who wereallowed to skip clauses that do not contain a finiteverb.
Our gold standard includes an additional 115segments that did not receive a label by one an-notator but were unanimously labeled by the othertwo.
The other segments are disregarded in the ex-periments.
Some of them have expletive subjects,and most others are non-finite verb phrases such asto-infinites or headlines that consist of only a NP.Inter-annotator agreement measured as Fleiss?
?
(Fleiss, 1971) on the segments labeled by all threeannotators is 0.70, 0.73 and 0.69 for Task NP, TaskCl and Task Cl+NP respectively, indicating sub-stantial agreement (Landis and Koch, 1977).4 A Sequence Labeling Model forGenericityThis section describes our method for identifyinggeneric clauses and NPs in context.
We apply thefollowing methods on each of the three differentprediction tasks NP, Cl and Cl+NP introduced inSection 3, varying only the type of labels on whichwe train and test.
In contrast to prior work, ourcomputational model integrates not only informa-tion from each local instance, but also informa-tion about the genericity status of surrounding in-stances.
The final labeling for the sequence of in-stances of an entire document is optimized withregard to these two types of information, which,as we have argued in Section 1, both play a cru-cial role in determining genericity.
The sequencesto be labeled contain all clauses or NPs of a doc-ument.
We also tried labeling sequences for para-graphs instead of documents, but the performancewas similar.
A reason might be that paragraphs arequite often linked by mentioning the same entities(Friedrich and Palmer, 2014a).Computational model.
We use linear chainconditional random fields (Lafferty et al, 2001)to label sequences of mentions or sequences ofclauses with regard to their genericity.
Conditionalrandom fields (CRFs) are well suited for our label-ing task as they do not make an independence as-sumption between the features.
CRFs predict theconditional probability of label sequence ~y givenan observation sequence ~x as follows:P (~y|~x) =1Z(~x)exp(n?j=1m?i=1?ifi(yj?1, yj, ~x, j))Z(~x) is a normalization constant, the sum overthe scores of all possible label sequences for anobservation sequence with the length of ~x.
Theweights ?iof the feature functions are the param-eters to be learned.
They do not depend on the cur-rent position j in the sequence.
The feature func-tions fiare in general allowed to look at the cur-rent label yj, the previous label yj?1and the entireobservation sequence ~x.
We use a simple instan-tiation of a linear chain CRF whose feature func-tions take two forms, fi(yj, xj) and fi(yj?1, yj).We create a linear chain CRF model using theCRF++ toolkit2, using all the default parameters.2https://code.google.com/p/crfpp1275NP-BASED FEATURESnumber sg, plperson 1, 2, 3countability from Celex, e.g.
countnoun type common, proper, pronoundeterminer type def, indef, demonpart-of-speech POS of headbare plural true, falseWN granularity number of edges to top nodeWN sense [0?
2] WN senses (head+hypernyms)WN senseTop top sense in hypernym hierarchyWN lexical filename person, artifact, event, ...CLAUSE-BASED FEATURESdependency [0?
4] dependency relation betweenhead and governor etc.tense tense, aspect and voice informa-tion, e.g.
pres perf activecoarseTense pres, past, futprogressive true, falseperfective true, falsepassive true, falsetemporal modifier true, falsenumber of modifiers numericpart-of-speech POS of headpredicate lemma of headadjunct-degree positive, comparative, superlativeadjunct-pred lemma of adverbial clauses?
headTable 1: Features.
WN=WordNet.Feature functions.
We extract the set of featureslisted in Table 1 for each instance.
This set of fea-tures is inspired by Reiter and Frank (2010), seealso Section 5.2.
In the case of the WikiGener-ics corpus, the NP features are extracted for thesubject of the clause.
We parse the data using theStanford parser (Klein and Manning, 2002) andobtain the subject NPs from the collapsed depen-dencies.
For the ACE data, the NP features areextracted for all mentions in the gold standard andthe clause features are extracted from the clausein which the mention appears.
Our feature func-tions fi(yj, xj) are indicator functions combiningthe current label and one of the feature values ofthe current mention or clause, for example:f = if (yj= GENERIC and xj.np.person=3)return 1 else return 0We create two versions of the CRF model: thebigram3model additionally uses indicator func-tions f(yj?1, yj) for each combination of labels,thus taking context into account.
The unigrammodel does not use these feature functions, it isthus similar to a maximum entropy model (with adifferent normalization).
Log-linear models workvery well for many NLP tasks, especially if fea-tures are correlated as it is the case here, so in or-3Following CRF++ terminology.der to get a fair estimate of the impact of usingthe context (via the transition feature functions),we give numbers for this ?unigram?
model in ad-dition, rather than simply comparing the bigram-CRF to a Bayesian network, which is used byReiter and Frank (2010).
Using more complexfeature functions did not result in significant per-formance gains, so we chose the simplest model.Note that even though the feature functions onlyformulate relationships between adjacent labels inthe sequence, the optimal labeling is computed forthe entire sequence: the choices of labels assignedto non-adjacent clauses do influence each other.Two-step Approach for Task Cl+NP.
TaskCl+NP can be regarded as a combination of thetwo decisions made in Task NP and Task Cl.Therefore, we approach Task Cl+NP in two ways.
(a) We train a CRF which directly outputs thethree labels.
(b) The two-step approach combinesthe output from the labelers trained for Task NPand Task Cl into one label in a rule-based way.This leads to the additional class GEN non-gen,of which no gold instances exist by definition.
Aswe evaluate in terms of F1-score and accuracy forthe existing classes, items classified into this arti-ficial class will simply be counted as wrong andlack from the recall counts.5 ExperimentsThis section reports on our experiments, whichwe evaluate in terms of precision (P), recall (R)and F1-measure per class.
We compute macro-averages as Pmacro=1|c|?
?|c|i=1Pietc., where |c|stands for the number of classes.
Macro-F1is theharmonic mean of macro-average P and R. To re-port on statistical significance of differences in ac-curacy, we apply McNemar?s test with p < 0.01.5.1 Experimental Settings and DataWe report results for cross validation (CV).
Be-cause we leverage contextual information by la-beling sequences of clauses from entire docu-ments, for all experiments presented in this sec-tion, if not indicated otherwise, we put all in-stances of one document into the same fold asone sequence.
Fold sizes differ slightly from eachother, but folds are kept constant for all experi-ments.On WikiGenerics, we carry out all three predic-tion tasks as defined in Section 3.
On the ACE cor-pora, we only conduct Task NP because there are1276generic non-generic macro-avgSystem P R F1 P R F1 P R F1 accuracyMajority class baseline 0.0 0.0 0.0 86.8 100 92.9 43.4 50.0 46.5 86.8Person baseline (R&F) 60.4 10.2 17.5 87.9 99.0 93.1 74.2 54.6 62.9 87.2R&F (BayesNet) 37.7 72.0 49.5 95.0 81.9 88.0 66.4 76.9 71.3 80.6Reimpl.
(BayesNet) 38.1 67.7 48.8 94.4 83.3 88.5 66.3 75.5 70.6 81.2Table 2: Results of reimplemented baseline on ACE-2 (original, unbalanced data set), 40106 instances(annotated noun phrases).
Weka?s stratified 10-fold cross validation, using all features.no labels corresponding to Task Cl or Task Cl+NP.For the experiments on WikiGenerics, we useleave-one-document-out CV, i.e., we train on 101of the 102 documents and test on the remain-ing document in each fold.
The total number ofclauses is 10355.
From ACE-2005, we use thenewswire and broadcast news subsections.4Dueto low frequency, we omit instances of NEG in ourexperiments, and apply a three-way classificationtask (GEN, SPC, USP).
We present results for allremaining 40106 mentions and for the subset of18029 subject mentions, each time using 10-foldCV.5.2 Baseline: Local ClassifierThe system for identifying generic NPs of Reiterand Frank (2010), henceforth R&F, makes use ofthe English ParGram LFG grammar for the XLEparser (Butt et al, 2002).
As this grammar is notpublicly available, we implement a similar systemusing exclusively the Stanford CoreNLP toolsuite(Manning et al, 2014), the Celex database of En-glish nouns (Baayen et al, 1996) and WordNet(Fellbaum, 1999).
Our system is based on dkpro(de Castilho and Gurevych, 2014).
We extract thefeatures listed in Table 1 based on the POS tagsand syntactic dependencies assigned by the Stan-ford parser (Klein and Manning, 2002).
We couldnot reimplement several tense- and aspect-relatedParGram-specific features.
In order to compen-sate for this, we add an additional feature (tense)with finer-grained tense and voice information, us-ing the rules described by Loaiciga et al (2014).Other additional features did not improve perfor-mance, which shows that R&F?s set of featurescaptures the syntactic-semantic information rele-vant to genericity classification quite well.
There-fore, we use this feature set alo for the sequencelabeling model.
Using the same feature set alowsus to attribute any performance gain to the context-4The rest of the data comprise broadcast conversation,weblog and forum texts as well as transcribed conversationaltelephone, and would require specialized preprocessing.awareness of our model rather than the features.R&F train a Bayesian network using Weka(Hall et al, 2009).
The decisions of this clas-sifier are local to each clause.
They report theperformance of their system on the ACE-2 cor-pus: Table 2 shows that the performance of our re-implemented feature set5is comparable to the sys-tem of R&F.6In all other other tables, ?BayesNetR&F?
refers to our re-implemented system.R&F present the ?Person baseline?
as a sim-ple informed baseline (see Table 2).
We traineda J48 decision tree on this feature alone, whichconfirmed that only second-person mentions (thegeneric ?you?)
are classified as generic, while allother mentions are classified as non-generic.5.3 Results and DiscussionIn this section, we first discuss the results of ourexperiments in terms of identifying generic NPs orclauses.
Then we present some additional experi-ments testing the influence of the different featureclasses and of other discourse-related information.All tasks, WikiGenerics.
The observations de-scribed in this paragraph are the same for all threeprediction tasks on WikiGenerics.
As Tables 3 and4 show, our CRF models outperform the baselinesystem of R&F by a large margin both in terms ofaccuracy and F1-score on the WikiGenerics cor-pus.
In Task NP and Task Cl, precision and re-call are quite balanced (not shown in tables).
Theperformance of the bigram model is significantlybetter than that of the unigram model, increasingaccuracy by about 3%, at the same time increas-ing F1.
In an oracle experiment, we use the pre-vious gold label instead of the predicted one forfi(yj?1, yj), and scores increase by up to 6.6%compared to the unigram model.
These resultsprovide strong empirical evidence for our hypoth-5Implementation available at:www.coli.uni-saarland.de/projects/sitent6Table 6 in Reiter and Frank?s paper contains some typo-graphical errors here.
We thank Nils Reiter for making avail-able his ARFF files, so we can provide this updated version.1277Task NP: Genericity of Subject Task Cl: Genericity of Clausegeneric non-gen. macro-avg generic non-gen. macro-avgSystem F1 F1 F1 acc.
F1 F1 F1 acc.Majority class 71.9 0.0 35.9 56.1 60.3 3.7 35.1 43.7BayesNet (R&F) 72.6 70.8 72.3 71.7 72.4 74.6 73.7 73.5CRF (unigram) 79.3 72.6 75.9 76.4* 77.9 77.0 77.4 77.4?CRF (bigram) 81.3 76.3 78.8 79.1* 80.8 80.6 80.7 80.7?- only clause features 79.2 71.6 75.5 76.0 79.3 78.3 78.8 78.8- only NP features 76.8 70.8 73.8 74.1 70.7 72.6 71.8 71.7CRF (bigram, gold) 85.0 80.4 82.7 83.0 82.9 82.6 82.8 82.8Table 3: Results on WikiGenerics for Task NP and Task C. *?Difference statistically significant.Task Cl+NP: Genericity of Clause (three-way)GEN gen NON-GEN non-gen NON-GEN gen macro-avgSystem F1 F1 F1 P R F1 accuracyMajority class 67.1 0.0 0.0 16.8 33.3 22.4 50.4BayesNet (R&F) 69.1 69.1 26.1 54.5 58.4 56.4 65.2CRF (unigram) 78.5 72.6 35.4 67.2 60.0 63.4 74.0*CRF (bigram) 81.3 76.9 33.4 70.3 61.8 65.8 77.4*- two-step 80.8 75.8 28.6 61.5 62.3 61.9 73.4- only clause feat.
79.4 72.6 25.3 67.0 57.2 61.8 74.3- only NP feat.
72.9 71.4 2.5 53.0 49.9 51.4 70.0CRF (bigram, gold) 84.0 80.6 39.1 72.8 65.7 69.0 80.6Table 4: Results on WikiGenerics for Task Cl+NP.
*Difference statistically significant.esis that using context information is useful foridentifying the genericity of NPs or clauses.Task Cl+NP, WikiGenerics.
In Task Cl+NP(see Table 4), only about 6% of the instanceshave the gold label NON-GEN gen (i.e., a non-generic sentence with a generic subject), the otherinstances are distributed roughly evenly betweenthe other two labels.
The difficulty of Task Cl+NPthus consists in identifying this infrequent case.The three-way CRF outperforms the two-step ap-proach both in terms of accuracy and macro-average F1-score.
The precision-recall tradeoffdiffers: for the NON-GEN gen class, P and Rof the CRF are 55.2% and 24.5% and those ofthe two-step-approach are 23.8% and 35.9%.
Thetwo-step approach labels more instances as NON-GEN gen but does so in a less precise way.
Whilethe performance of our model leaves room for im-provement on Task Cl+NP, especially with regardto the class NON-GEN gen, it is worth notingthat the computational model captures somethingabout the nature of this latter class; its instances dolook different in the feature space.
The context-aware CRF using three labels performs best.Feature set ablation.
In this ablation test,shown in Tables 3 and 4, our best model (CRF bi-gram) uses either the set of clause-based or the setof NP-based features at a time.
Clause-based fea-tures are more important than the NP-based fea-tures for all three classification tasks.
An inter-esting observation is that the NP features aloneare not able to separate the infrequent class NON-GEN gen from the other two at all, the F1-scoreof 2.5 shows that almost all instances of this classwere labeled as one of the other two classes.
Insum, this shows that whether an NP is interpretedas generic or not strongly depends on how it isused in the clause.Task NP, ACE.
Both on ACE-2 (see Table 5)and on ACE-2005 (see Table 6), the CRF outper-forms the system of Reiter and Frank (2010) interms of accuracy, and has a higher F1-score.
Wegive results also for subjects only as this parallelsthe setting of the WikiGenerics experiments (rea-sons for the restriction to subjects were given inSection 3).
For subjects, the majority class SPCis less frequent (compare the accuracies of thetwo majority class baselines); only 7% of the sub-jects are marked as GEN, the rest are labeled asUSP.
The bigram model does not outperform theunigram model, but our oracle experiments showthat context information is indeed useful: accuracyincreases significantly and F1increases consider-ably, especially for subjects.1278generic non-generic macro-avgSystem F1 F1 P R F1 accuracyMajority class 0.0 92.9 43.4 50.0 46.5 86.8BayesNet (R&F) 47.4 87.9 65.5 74.6 69.8 80.4CRF (unigrams) 49.1 93.5 75.5 68.7 71.3 88.5*CRF (bigrams) 51.0 93.7 76.5 69.8 72.4 88.9CRF (bigram, gold) 57.6 94.4 79.8 73.4 76.0 90.1*Table 5: Results on ACE-2 for Task NP, 10-fold CV, folds contain complete documents.
*Differencestatistically significant.macro-avgSystem P R F1 accuracyall 18029 annotated mentionsMajority class 27.0 33.3 29.9 81.1BayesNet (R&F) 50.8 57.2 53.8 74.5CRF (unigram) 61.6 51.8 55.1 83.2*CRF (bigram) 60.6 51.7 54.8 83.0CRF (bigram, gold) 63.9 54.9 58.2 83.9*5670 subject mentionsMajority class 25.0 33.3 28.6 75.1BayesNet (R&F) 51.5 53.9 52.7 72.5CRF (unigram) 58.0 51.3 53.6 77.7*CRF (bigram) 58.3 51.3 53.7 77.8CRF (bigram, gold) 62.4 56.1 58.6 79.6*Table 6: Results on ACE-2005 (bn+nw),Task NP, 10-fold CV, 3 classes: SPC, GEN, USP.
*Difference statistically significant.We identify two reasons for the fact that whenevaluating on the ACE corpora, oracle informationis needed to show the benefit of using bigram fea-ture functions: (a) The frequency of GEN men-tions in the ACE corpora is low ?
news containsonly little generic information, so the context in-formation is harder to leverage.
(b) The ACE an-notation guidelines contain some vagueness (seeSection 3); this makes it harder for an automaticsystem to learn about regularities.Higher-order Markov models.
Another re-search question is whether models incorporatingnot only the previous label, but more preceding la-bels would perform even better.
We turn to theMallet toolkit (McCallum, 2002), whose CRF im-plementation allows for using higher-order mod-els.7For example, an order-2 model considers thetwo previous labels.
We use L1-regularization dur-ing training.
Figure 1 shows that the optimum isreached for order-1 (bigram) models for each ofthe classification tasks for accuracy, the same ten-7The CRF++ toolkit, which we use in all other exper-iments, does not allow for higher-order models.
We useCRF++ in the main experiments as it comes with a concisedocumentation; this helps to make our experiments easilyreplicable.0 (unigram)1 (bigram) 2 3727476788082 Task NPTask ClTask Cl+NPMarkov orderAccuracy %Figure 1: Labeling results for CRF models of var-ious orders on WikiGenerics corpus.dencies were observed for F1-score (not shown).
Itseems sufficient to use bigram feature functions;note that as explained in Section 4, the bigrammodel does not mean that only adjacent clausesinfluence each other ?
context is actually wider.Using coreference information.
In our approx-imately balanced WikiGenerics corpus, 54% ofall pronouns are marked as generic and 46% aremarked as non-generic, which shows that there isno preference for pronouns to occur with eitherclass.
Some of the features (countability, nountype, determiner type, bare plural, and the Word-Net related features) are not informative when ap-plied to personal or relative pronouns.
Sometimes,it is not even possible to determine number with-out referring to the antecedent (e.g., in the caseof the relative pronoun ?who?).
We conduct thefollowing experiment: we automatically resolvecoreference using the Stanford coreference reso-lution system (Raghunathan et al, 2010).
We re-place the NP features of each pronominal instancewith the features of the first link of the coreferencechain.
We did not obtain a significant performancegain.
One reason is that this change of featuresonly applies to about 13% of the data.
We observethat any positive changes in the classification goalong with some negative changes which were of-ten due to coreference resolution errors.
One dif-ficult step in manually annotating, and hence alsoin automatically resolving coreference is to deter-mine whether a NP is generic or not (Nedoluzhko,2013).
The task of identifying generic NPs and1279coreference resolution are intertwined.
We plan tomanually annotate at least part of our corpus withcoreference information in order to test to what ex-tent the classification of the pronouns?
genericitystatus can profit from including antecedent infor-mation.6 ConclusionWe have presented a novel method for labeling se-quences of clauses or their subjects with regardto their genericity, showing that genericity shouldbe treated as a discourse-sensitive phenomenon.Our experiments prove that context informationimproves automatic labeling results, and that ourmodel outperforms previous approaches by a largemargin.The major contributions of this work include thestudy of genericity both on the NP- and clause-level, and the study of the interaction of these twolevels.
Our results of Task Cl+NP show that ourmodel indeed captures the three different typesof clauses resulting from the combination of NP-level and clause-level genericity.During the development of our annotationscheme, we found that it is beneficial to focuson genericity, disentangling it from the issue ofspecificity.
Our work provides a step forward tofinding reliable ways to apply semantic theoriesof genericity in practice, and we also provide anew state-of-the-art system for automatically la-beling generic expressions.
This in turn lays foun-dations for natural language processing tasks re-quiring text understanding.Future Work.
Our present approach for anno-tating and automatically classifying targets thesubjects of each clause.
We have not attemptedto tackle the task of classifying the genericity sta-tus of other dependents, as they are even harderto classify than subjects, and a concise annotationscheme has to be worked out in order achieve anacceptable inter-annotator agreement on this task.Another related distinction is the one between ha-bitual, stative and episodic sentences (Mathew andKatz, 2009), which applies to both what we callgeneric and non-generic sentences.
No large cor-pora exist to date, but studying the interaction ofthese phenomena is on our research agenda.AcknowledgmentsWe thank the anonymous reviewers, AlexisPalmer, Nils Reiter and Melissa Peate S?rensenfor their helpful comments related to this work,and our annotators Christine Bocionek and Kleo-Isidora Mavridou.
This research was supportedin part by the Cluster of Excellence ?MultimodalComputing and Interaction?
of the German Excel-lence Initiative (DFG), and the first author is sup-ported by an IBM PhD Fellowship.ReferencesHarald R. Baayen, Richard Piepenbrock, and Leon Gu-likers.
1996.
CELEX2.
Philadelphia: LinguisticData Consortium.Miriam Butt, Helge Dyvik, Tracy Holloway King,Hiroshi Masuichi, and Christian Rohrer.
2002.The parallel grammar project.
In Proceedings ofthe 2002 workshop on Grammar engineering andevaluation-Volume 15, pages 1?7.
Association forComputational Linguistics.Gregory Norman Carlson.
1977.
Reference to kinds inEnglish.
Ph.D. thesis.Gregory N. Carlson.
2005.
Generics, Habituals andIteratives.
In Alex Barber, editor, Encyclopedia ofLanguage and Linguistics.
Elsevier.Richard Eckart de Castilho and Iryna Gurevych.
2014.A broad-coverage collection of portable NLP com-ponents for building shareable analysis pipelines.In Proceedings of the Workshop on Open In-frastructures and Analysis Frameworks for HLT(OIAF4HLT) at COLING, pages 1?11.Christiane Fellbaum.
1999.
WordNet.
Wiley OnlineLibrary.Joseph L Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychological bul-letin, 76(5):378.Annemarie Friedrich and Alexis Palmer.
2014a.
Cen-tering Theory in natural text: a large-scale corpusstudy.
In Proceedings of KONVENS 2014.
Univer-sit?atsbibliothek Hildesheim.Annemarie Friedrich and Alexis Palmer.
2014b.
Situ-ation entity annotation.
In Proceedings of the Lin-guistic Annotation Workshop VIII, page 149.Annemarie Friedrich, Alexis Palmer, Melissa PeateSrensen, and Manfred Pinkal.
2015.
Annotatinggenericity: a survey, a scheme, and a corpus.
In Pro-ceedings of the 9th Linguistic Annotation Workshop(LAW IX), Denver, Colorado, US.1280Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.ACM SIGKDD explorations newsletter, 11(1):10?18.Aurelie Herbelot and Ann Copestake.
2009.
Anno-tating genericity: How do humans decide?
(A casestudy in ontology extraction).
Studies in GenerativeGrammar 101, page 103.Dan Klein and Christopher D Manning.
2002.
Fast ex-act inference with a factored model for natural lan-guage parsing.
In Advances in neural informationprocessing systems, pages 3?10.Manfred Krifka, Francis Jeffrey Pelletier, Gregory N.Carlson, Alice ter Meulen, Godehard Link, and Gen-naro Chierchia.
1995.
Genericity: An Introduction.The Generic Book, pages 1?124.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional Random Fields:Probabilistic Models for Segmenting and Label-ing Sequence Data.
In Proceedings of the Eigh-teenth International Conference on Machine Learn-ing, ICML ?01, pages 282?289, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.J Richard Landis and Gary G Koch.
1977.
The mea-surement of observer agreement for categorical data.biometrics, pages 159?174.Godehard Link.
1995.
Generic information and depen-dent generics.
The Generic Book, pages 358?382.Sharid Loaiciga, Thomas Meyer, and Andrei Popescu-Belis.
2014.
English-French Verb Phrase Align-ment in Europarl.
In Proceedings of LREC 2014.Christopher D Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP Natural Lan-guage Processing Toolkit.
In Proceedings of 52ndAnnual Meeting of the Association for Computa-tional Linguistics: System Demonstrations, pages55?60.Thomas A. Mathew and E. Graham Katz.
2009.
Super-vised Categorization of Habitual and Episodic Sen-tences.
In Sixth Midwest Computational LinguisticsColloquium, Bloomington, Indiana: Indiana Univer-sity.Andrew K McCallum.
2002.
MALLET: A MachineLearning for Language Toolkit.Alexis Mitchell, Stephanie Strassel, Mark Przybocki,JK Davis, George Doddington, Ralph Grishman,Adam Meyers, Ada Brunstein, Lisa Ferro, andBeth Sundheim.
2003.
ACE-2 Version 1.0LDC2003T11.
Philadelphia: Linguistic Data Con-sortium.Anna Nedoluzhko.
2013.
Generic noun phrases andannotation of coreference and bridging relations inthe Prague Dependency Treebank.
In Proceedingsof the 7th Linguistic Annotation Workshop and In-teroperability with Discourse, pages 103?111.Alexis Palmer, Elias Ponvert, Jason Baldridge, andCarlota Smith.
2007.
A sequencing model for sit-uation entity classification.
In Proceedings of the45th Annual Meeting of the Association for Compu-tational Linguistics, page 896.Massimo Poesio and Ron Artstein.
2008.
AnaphoricAnnotation in the ARRAU Corpus.
In LREC.Massimo Poesio.
2004.
Discourse annotation and se-mantic annotation in the GNOME corpus.
In Pro-ceedings of the 2004 ACL Workshop on DiscourseAnnotation, pages 72?79.
Association for Computa-tional Linguistics.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing, pages 492?501.Association for Computational Linguistics.Nils Reiter and Anette Frank.
2010.
IdentifyingGeneric Noun Phrases.
In Proceedings of the 48thAnnual Meeting of the Association for Computa-tional Linguistics, pages 40?49, Uppsala, Sweden,July.
Association for Computational Linguistics.Radu Soricut and Daniel Marcu.
2003.
Sentence leveldiscourse parsing using syntactic and lexical infor-mation.
In Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology-Volume 1, pages 149?156.
Associationfor Computational Linguistics.Sangweon Suh, Harry Halpin, and Ewan Klein.2006.
Extracting common sense knowledge fromwikipedia.
In Proceedings of the Workshop on WebContent Mining with Human Language Technolo-gies at ISWC, volume 6.Sangweon Suh.
2006.
Extracting Generic Statementsfor the Semantic Web.
Master?s thesis, University ofEdinburgh.Christopher Walker, Stephanie Strassel, Julie Medero,and Kazuaki Maeda.
2006.
ACE 2005 MultilingualTraining Corpus LDC2006T06.
Philadelphia: Lin-guistic Data Consortium.1281
