Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435?1445,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSenseSpotting: Never let your parallel data tie you to an old domainMarine Carpuat1, Hal Daume?
III2, Katharine Henry3,Ann Irvine4, Jagadeesh Jagarlamudi5, Rachel Rudinger61 National Research Council Canada, marine.carpuat@nrc.gc.ca2 CLIP, University of Maryland, me@hal3.name3 CS, University of Chicago, kehenry@uchicago.edu4 CLSP, Johns Hopkins University, anni@jhu.edu5 IBM T.J. Watson Research Center, jags@us.ibm.com6 CLSP, Johns Hopkins University, rachel.rudinger@aya.yale.eduAbstractWords often gain new senses in new do-mains.
Being able to automatically iden-tify, from a corpus of monolingual text,which word tokens are being used in a pre-viously unseen sense has applications tomachine translation and other tasks sensi-tive to lexical semantics.
We define a task,SENSESPOTTING, in which we build sys-tems to spot tokens that have new sensesin new domain text.
Instead of difficultand expensive annotation, we build a gold-standard by leveraging cheaply availableparallel corpora, targeting our approach tothe problem of domain adaptation for ma-chine translation.
Our system is able toachieve F-measures of as much as 80%,when applied to word types it has neverseen before.
Our approach is based ona large set of novel features that capturevaried aspects of how words change whenused in new domains.1 IntroductionAs Magnini et al (2002) observed, the domain ofthe text that a word occurs in is a useful signal forperforming word sense disambiguation (e.g.
in atext about finance, bank is likely to refer to a finan-cial institution while in a text about geography, itis likely to refer to a river bank).
However, in theclassic WSD task, ambiguous word types and a setof possible senses are known in advance.
In thiswork, we focus on the setting where we observetexts in two different domains and want to iden-tify words in the second text that have a sense thatdid not appear in the first text, without any lexicalknowledge in the new domain.To illustrate the task, consider the French nounrapport.
In the parliament domain, this meanse?tat rapport re?gimeGovt.
geo.
state report (political) regimeMedical state (mind) report dietgeo.
state ratio (political) regimeScience geo.
state ratio (political) regimereport dietMovies geo.
state report (political) regimedietTable 1: Examples of French words and their mostfrequent senses (translations) in four domains.
(and is translated as) ?report.?
However, in mov-ing to a medical or scientific domain, the wordgains a new sense: ?ratio?, which simply does notexist in the parliament domain.
In a science do-main, the ?report?
sense exists, but it is dominatedabout 12:1 by ?ratio.?
In a medical domain, the?report?
sense remains dominant (about 2:1), butthe new ?ratio?
sense appears frequently.In this paper we define a new task that we callSENSESPOTTING.
The goal of this task is to iden-tify words in a new domain monolingual text thatappeared in old domain text but which have anew, previously unseen sense1.
We operate un-der the framework of phrase sense disambiguation(Carpuat and Wu, 2007), in which we take au-tomatically align parallel data in an old domainto generate an initial old-domain sense inventory.This sense inventory provides the set of ?known?word senses in the form of phrasal translations.Concrete examples are shown in Table 1.
One ofour key contributions is the development of a richset of features based on monolingual text that areindicative of new word senses.This work is driven by an application need.When machine translation (MT) systems are ap-plied in a new domain, many errors are a resultof: (1) previously unseen (OOV) source languagewords, or (2) source language words that appearwith a new sense and which require new transla-1All features, code, data and raw results are at: github.com/hal3/IntrinsicPSDEvaluation1435tions2 (Carpuat et al, 2012).
Given monolingualtext in a new domain, OOVs are easy to identify,and their translations can be acquired using dictio-nary extraction techniques (Rapp, 1995; Fung andYee, 1998; Schafer and Yarowsky, 2002; Schafer,2006; Haghighi et al, 2008; Mausam et al, 2010;Daume?
III and Jagarlamudi, 2011), or active learn-ing (Bloodgood and Callison-Burch, 2010).
How-ever, previously seen (even frequent) words whichrequire new translations are harder to spot.Because our motivation is translation, one sig-nificant point of departure between our work andprior related work (?3) is that we focus on wordtokens.
That is, we are not interested only in thequestion of ?has this known word (type) gaineda new sense?
?, but the much more specific ques-tion of ?is this particular (token) occurrence of thisknown word being used in a new sense??
Notethat for both the dictionary mining setting and theactive learning setting, it is important to considerwords in context when acquiring their translations.2 Task DefinitionOur task is defined by two data components.
De-tails about their creation are in ?5.
First, we needan old-domain sense dictionary, extracted fromFrench-English parallel text (in our case, parlia-mentary proceedings).
Next, we need new-domainmonolingual French text (we use medical text, sci-entific text and movie subtitle text).
Given thesetwo inputs, our challenge is to find tokens in thenew-domain text that are being used in a new sense(w.r.t.
the old-domain dictionary).We assume that we have access to a smallamount of new domain parallel ?tuning data.
?From this data, we can extract a small new do-main dictionary (?5).
By comparing this new do-main dictionary to the old domain dictionary, wecan identify which words have gained new senses.In this way, we turn the SENSESPOTTING probleminto a supervised binary classification problem: anexample is a French word in context (in the newdomain monolingual text) and its label is positivewhen it is being used in a sense that did not ex-ist in the old domain dictionary.
In this task, theclassifier is always making predictions on words2Sense shifts do not always demand new translations;some ambiguities are preserved across languages.
E.g.,fene?tre can refer to a window of a building or on a moni-tor, but translates as ?window?
either way.
Our experimentsuse bilingual data with an eye towards improving MT perfor-mance: we focus on words that demand new translations.outside this tuning data on word types it has neverseen before!
From an applied perspective, the as-sumption of a small amount of parallel data in thenew domain is reasonable: if we want an MT sys-tem for a new domain, we will likely have somedata for system tuning and evaluation.3 Related WorkWhile word senses have been studied extensivelyin lexical semantics, research has focused on wordsense disambiguation, the task of disambiguatingwords in context given a predefined sense inven-tory (e.g., Agirre and Edmonds (2006)), and wordsense induction, the task of learning sense inven-tories from text (e.g., Agirre and Soroa (2007)).
Incontrast, detecting novel senses has not received asmuch attention, and is typically addressed withinword sense induction, rather than as a distinctSENSESPOTTING task.
Novel sense detectionhas been mostly motivated by the study of lan-guage change over time.
Most approaches modelchanges in co-occurrence patterns for word typeswhen moving between corpora of old and modernlanguage (Sagi et al, 2009; Cook and Stevenson,2010; Gulordava and Baroni, 2011).Since these type-based models do not capturepolysemy in the new language, there have been afew attempts at detecting new senses at the token-level as in SENSESPOTTING.
Lau et al (2012)leverage a common framework to address senseinduction and disambiguation based on topic mod-els (Blei et al, 2003).
Sense induction is framedas learning topic distributions for a word type,while disambiguation consists of assigning topicsto word tokens.
This model can interestingly beused to detect newly coined senses, which mightco-exist with old senses in recent language.
Bam-man and Crane (2011) use parallel Latin-Englishdata to learn to disambiguate Latin words into En-glish senses.
New English translations are used asevidence that Latin words have shifted sense.
Incontrast, the SENSESPOTTING task consists of de-tecting when senses are unknown in parallel data.Such novel sense induction methods requiremanually annotated datasets for the purpose ofevaluation.
This is an expensive process and there-fore evaluation is typically conducted on a verysmall scale.
In contrast, our SENSESPOTTING taskleverages automatically word-aligned parallel cor-pora as a source of annotation for supervision dur-ing training and evaluation.1436The impact of domain on novel senses has alsoreceived some attention.
Most approaches oper-ate at the type-level, thus capturing changes in themost frequent sense of a word when shifting do-mains (McCarthy et al, 2004; McCarthy et al,2007; Erk, 2006; Chan and Ng, 2007).
Chan andNg (2007) notably show that detecting changes inpredominant sense as modeled by domain sensepriors can improve sense disambiguation, even af-ter performing adaptation using active learning.Finally, SENSESPOTTING has not been ad-dressed directly in MT.
There has been much inter-est in translation mining from parallel or compara-ble corpora for unknown words, where it is easy toidentify which words need translations.
In con-trast, SENSESPOTTING detects when words havenew senses and, thus, frequently a new translation.Work on active learning for machine translationhas focused on collecting translations for longerunknown segments (e.g., Bloodgood and Callison-Burch (2010)).
There has been some interest indetecting which phrases that are hard to translatefor a given system (Mohit and Hwa, 2007), but dif-ficulties can arise for many reasons: SENSESPOT-TING focuses on a single problem.4 New Sense IndicatorsWe define features over both word types and wordtokens.
In our classification setting, each instanceconsists of a French word token in context.
Ourword type features ignore this context and rely onstatistics computed over our entire new domaincorpus.
In contrast, our word token features con-sider the context of the particular instance of theword.
If it were the case that only one sense ex-isted for all word tokens of a particular type withina single domain, we would expect our word typefeatures to be able to spot new senses without thehelp of the word token features.
However, in fact,even within a single domain, we find that often aword type is used with several senses, suggestingthat word token features may also be useful.4.1 Type-level FeaturesLexical Item Frequency Features A very ba-sic property of the new domain that we hope tocapture is that word frequencies change, and suchchanges might be indicative of a domain shift.
Assuch, we compute unigram log probabilities (viasmoothed relative frequencies) of each word un-der consideration in the old domain and the newdomain.
We then add as features these two logprobabilities as well as their difference.
These areour Type:RelFreq features.N-gram Probability Features The goal of theType:NgramProb feature is to capture the factthat ?unusual contexts?
might imply new senses.To capture this, we can look at the log probabilityof the word under consideration given its N-gramcontext, both according to an old-domain languagemodel (call this `oldng ) and a new-domain languagemodel (call this `newng ).
However, we do not sim-ply want to capture unusual words, but words thatare unlikely in context, so we also need to look atthe respective unigram log probabilities: `oldug and`newug .
From these four values, we compute corpus-level (and therefore type-based) statistics of thenew domain n-gram log probability (`newng , the dif-ference between the n-gram probabilities in eachdomain (`newng ?
`oldng ), the difference between then-gram and unigram probabilities in the new do-main (`newng ?
`newug ), and finally the combined differ-ence: `newng ?
`newug + `oldug ?
`oldng ).
For each of thesefour values, we compute the following type-basedstatistics over the monolingual text: mean, stan-dard deviation, minimum value, maximum valueand sum.
We use trigram models.Topic Model Feature The intuition behind thetopic model feature is that if a word?s distribu-tion over topics changes when moving into a newdomain, it is likely to also gain a new sense.For example, suppose that in our old domain, theFrench word enceinte is only used with the sense?wall,?
but in our new domain, enceinte may havesenses corresponding to either ?wall?
or to ?preg-nant.?
We would expect to see this reflected inenceinte?s distribution over topics: the topic thatplaces relatively high probabilities on words suchas ?be?be??
(English ?baby?)
and enfant (English?child?)
will also place a high probability on en-ceinte when trained on new domain data.
In theold domain, however, we would not expect a sim-ilar topic (if it exists) to give a high probabil-ity to enceinte.
Based on this intuition, for allwords w, where To and Tn are the set of oldand new topics and Po and Pn are the old andnew distributions defined over them, respectively,and cos is the cosine similarity between a pairof topics, we define the feature Type:TopicSim:?t?Tn,t?
?To Pn(t|w)Po(t?|w) cos(t, t?).
For aword w, the feature value will be high if, foreach new domain topic t that places high proba-bility on w, there is an old domain topic t?
that1437is similar to t and also places a high probabil-ity on w. Conversely, if no such topic exists, thescore will be low, indicating the word has gaineda new sense.
We use the online LDA (Blei etal., 2003; Hoffman et al, 2010), implementedin http://hunch.net/?vw/ to compute topics onthe two domains separately.
We use 100 topics.Context Feature It is expected that words acquir-ing new senses will tend to neighbor different setsof words (e.g.
different arguments, prepositions,parts of speech, etc.).
Thus, we define an addi-tional type level feature to be the ratio of the num-ber of new domain n-grams (up to length three)that contain word w and which do not appear inthe old domain to the total number of new domainn-grams containing w. With Nw indicating the setof n-grams in the new domain which contain w,Ow indicating the set of n-grams in the old domainwhich contain w, and |Nw ?
Ow| indicating then-grams which contain w and appear in the newbut not the old domain, we define Type:Contextas|Nw?Ow||Nw| .
We do not count n-grams containingOOVs, as they may simply be instances of apply-ing the same sense of a word to a new argument4.2 Token-level FeaturesN-gram Probability Features Akin to the N-gram probability features at the type level (namely,Token:NgramProb), we compute the same val-ues at the token level (new/old domain and un-igram/trigram).
Instead of computing statisticsover the entire monolingual corpus, we use the in-stantaneous values of these features for the tokenunder consideration.
The six features we constructare: unigram (and trigram) log probabilities in theold domain, the new domain, and their difference.Context Features Following the type-level n-gram feature, we define features for a particularword token based on its n-gram context.
For tokenwi, in position i in a given sentence, we considerits context words in a five word window: wi?2,wi?1, wi+1, and wi+2.
For each of the four con-textual words in positions p = {?2,?1, 1, 2},relative to i, we define the following feature, To-ken:CtxCnt: log(cwp) where cwp is the numberof times word wp appeared in position p relativeto wi in the OLD-domain data.
We also define asingle feature which is the percent of the four con-textual words which had been seen in the OLD-domain data, Token:Ctx%.Token-Level PSD Features These features aimto capture generalized characteristics of a context.Towards this end, first, we pose the problem as aphrase sense disambiguation (PSD) problem overthe known sense inventory.
Given a source word ina context, we train a classifier to predict the mostlikely target translation.
The ground truth labels(target translation for a given source word) for thisclassifier are generated from the phrase table ofthe old domain data.
We use the same set of fea-tures as in Carpuat and Wu (2007).
Second, givena source word s, we use this classifier to com-pute the probability distribution of target transla-tions (p(t|s)).
Subsequently, we use this prob-ability distribution to define new features for theSENSESPOTTING task.
The idea is that, if a wordis used in one of the known senses then its con-text must have been seen previously and hence wehope that the PSD classifier outputs a spiky dis-tribution.
On the other hand, if the word takes anew sense then hopefully it is used in an unseencontext resulting in the PSD classifier outputtingan uniform distribution.
Based on this intuition,we add the following features: MaxProb is themaximum probability of any target translation:maxt p(t|s).
Entropy is the entropy of the proba-bility distribution: ?
?t p(t|s) log p(t|s).
Spreadis the difference between maximum and mini-mum probabilities of the probability distribution:(maxt p(t|s) ?
mint p(t|s)).
Confusion is theuncertainty in the most likely prediction given thesource token: mediantp(t|s)maxt p(t|s) .
The use of median inthe numerator rather than the second best is mo-tivated by the observation that, in most cases, topranked translations are of the same sense but differin morphology.We train the PSD classifier in two modes:1) a single global classifier that predicts thetarget translation given any source word; 2) alocal classifier for each source word.
Whentraining the global PSD classifier, we includesome lexical features that depend on the sourceword.
For both modes, we use real valuedand binned features giving rise to four familiesof features Token:G-PSD, Token:G-PSDBin,Token:L-PSD and Token:L-PSDBin.Prior vs. Posterior PSD Features When thePSD classifier is trained in the second mode, i.e.one classifier per word type, we can define ad-ditional features based on the prior (with out theword context) and posterior (given the word?scontext) probability distributions output by theclassifier, i.e.
pprior(t|s) and ppost.
(t|s) respec-1438Domain Sentences Lang Tokens TypesHansard 8,107,356 fr 161,695,309 191,501en 144,490,268 186,827EMEA 472,231 fr 6,544,093 34,624en 5,904,296 29,663Science 139,215 fr 4,292,620 117,669en 3,602,799 114,217Subs 19,239,980 fr 154,952,432 361,584en 174,430,406 293,249Table 2: Basic characteristics of the parallel data.tively.
We compute the following set of fea-tures referred to as Token:PSDRatio: SameMaxchecks if both the prior and posterior distri-butions have the same translation as the mostlikely translation.
SameMin is same as theabove feature but check if the least likely trans-lation is same.
X-OR MinMax is the exclusive-OR of SameMax and SameMin features.
KLis the KL-divergence between the two distri-butions.
Since KL-divergence is asymmetric,we use KL(pprior||ppost.)
and KL(ppost.||pprior).MaxNorm is the ratio of maximum probabilitiesin prior and posterior distributions.
SpreadNormis the ratio of spread of the prior and posterior dis-tributions, where spared is the difference betweenmaximum and minimum probabilities of the dis-tribution as defined earlier.
ConfusionNorm is theratio of confusion of the prior and posterior distri-butions, where confusion is defined as earlier.5 Data and Gold StandardThe first component of our task is a parallel cor-pus of old domain data, for which we use theFrench-English Hansard parliamentary proceed-ings (http://www.parl.gc.ca).
From this, weextract an old domain sense dictionary, using theMoses MT framework (Koehn et al, 2007).
Thisdefines our old domain sense dictionary.
For newdomains, we use three sources: (1) the EMEAmedical corpus (Tiedemann, 2009), (2) a corpus ofscientific abstracts, and (3) a corpus of translatedmovie subtitles (Tiedemann, 2009).
Basic statis-tics are shown in Table 2.
In all parallel corpora,we normalize the English for American spelling.To create the gold standard truth, we followeda lexical sample apparoach and collected a setof 300 ?representative types?
that are interest-ing to evaluate on, because they have multiplesenses within a single domain or whose sensesare likely to change in a new domain.
We useda semi-automatic approach to identify represen-tative types.
We first used the phrase table fromParallel Repr.
Repr.
% NewSents fr-tok Types Tokens SenseEMEA 24k 270k 399 35,266 52.0%Science 22k 681k 425 8,355 24.3%Subs 36k 247k 388 22,598 43.4%Table 3: Statistics about representative words andthe size of the development sets.
The columnsshow: the total amount of parallel developmentdata (# of sentences and tokens in French), # ofrepresentative types that appear in this corpus, thecorresponding # of tokens, and the percentage ofthese tokens that correspond to ?new senses.
?the Moses output to rank phrases in each domainusing TF-IDF scores with Okapi BM25 weight-ing.
For each of the three new domains (EMEA,Science, and Subs), we found the intersection ofphrases between the old and the new domain.
Wethen looked at the different translations that eachhad in the phrase table and a French speaker se-lected a subset that have multiple senses.3In practice, we limited our set alost entirelyto source words, and included only a single multi-word phrase, vue des enfants, which usually trans-lates as ?for children?
in the old domain but al-most always translates as ?sight of children?
inthe EMEA domain (as in ?.
.
.
should be kept outof the sight of children?).
Nothing in the way wehave defined, approached, or evaluated the SENS-ESPOTTING task is dependent on the use of rep-resentative words instead of longer representativephrases.
We chose to consider mostly source lan-guage words for simplicity and because it was eas-ier to identify good candidate words.In addition to the manually chosen words, wealso identified words where the translation withthe highest lexical weight varied in different do-mains, with the intuition being that are the wordsthat are likely to have acquired a new sense.
Thetop 200 words from this were added to the man-ually selected representative words to form a listof 450.
Table 3 shows some statistics about thesewords across our three test domains.6 Experiments6.1 Experimental setupOur goal in evaluation is to be able to under-stand what our approach is realistically capa-ble of.
One challenge is that the distribution3In order to create the evaluation data, we used both sidesof the full parallel text; we do not use the English side of theparallel data for actually building systems.1439of representative words is highly skewed.4 Wepresent results in terms of area under the ROCcurve (AUC),5 micro-averaged precision/recall/f-measure and macro-averaged precision/recall/f-measure.
For macro-averaging, we compute a sin-gle confusion matrix over all the test data anddetermining P/R/F from that matrix.
For micro-averaging, we compute a separate confusion ma-trix for each word type on the French side, com-pute P/R/F for each of these separately, and thenaverage the results.
(Thus, micro-F is not afunction of micro-P and micro-R.) The AUC andmacro-averaged scores give a sense of how wellthe system is doing on a type-level basis (es-sentially weighted by type frequency), while themicro-averaged scores give a sense as to how wellthe system is doing on individual types, not takinginto account their frequencies.For most of our results, we present standarddeviations to help assess significance (?2?
isroughly a 90% confidence interval).
For our re-sults, in which we use new-domain training data,we compute these results via 16-fold cross valida-tion.
The folds are split across types so the sys-tem is never being tested on a word type that it hasseen before.
We do this because it more closely re-sembles our application goals.
We do 16-fold forconvenience, because we divide the data into bi-nary folds recursively (thus having a power-of-twois easier), with an attempt to roughly balance thesize of the training sets in each fold (this is trickybecause of the skewed nature of the data).
This en-tire 16-fold cross-validation procedure is repeated10 times and averages and standard deviations areover the 160 replicates.We evaluate performance using our type-levelfeatures only, TYPEONLY, our token-level fea-tures only, TOKENONLY, and using both our typeand our token level features, ALLFEATURES.We compare our results with two baselines:RANDOM and CONSTANT.
RANDOM predictsnew-sense or not-new-sense randomly and withequal probability.
CONSTANT always predictsnew-sense, achieving 100% recall and a macro-level precision that is equal to the percent of repre-sentative words which do have a new sense, mod-ulo cross-validation splits (see Table 3).
Addi-4The most frequent (voie) appears 3881 times; there are60 singleton words on average across the three new domains.5AUC is the probability that the classifier will assign ahigher score to a randomly chosen positive example than to arandomly chosen negative example (Wikipedia, 2013).tionally, we compare our results with a type-leveloracle, TYPEORACLE.
For all tokens of a givenword type, the oracle predicts the majority label(new-sense or not-new-sense) for that word type.These results correspond to an upper bound for theTYPEONLY experiments.6.2 Classification SetupFor all experiments, we use a linear classifiertrained by stochastic gradient descent to optimizelogistic loss.
We also did some initial experi-ments on development data using boosted deci-sion trees instead and other loss functions (hingeloss, squared loss), but they never performed aswell.
In all cases, we perform 20 passes overthe training data, using development data to per-form early stopping (considered at the end of eachpass).
We also use development data to tune aregularizer (either `1 or `2) and its regularizationweight.6 Finally, all real valued features are au-tomatically bucketed into 10 consecutive buckets,each with (approximately) the same number ofelements.
Each learner uses a small amount ofdevelopment data to tune a threshold on scoresfor predicting new-sense or not-a-new-sense, us-ing macro F-measure as an objective.6.3 Result SummaryTable 4 shows our results on the SENSESPOT-TING task.
Classifiers based on the featuresthat we defined outperform both baselines in allmacro-level evaluations for the SENSESPOTTINGtask.
Using AUC as an evaluation metric, theTOKENONLY, TYPEONLY, and ALLFEATURESmodels performed best on EMEA, Science, andSubtitles data, respectively.
Our token-level fea-tures perform particularly poorly on the Scienceand Subtitles data.
Although the model trained ononly those features achieves reasonable precision(72.59 and 70.00 on Science and Subs, respec-tively), its recall is very low (20.41 and 35.15), in-dicating that the model classifies many new-sensewords as not-new-sense.
Most of our token-levelfeatures capture the intuition that when a word to-ken appears in new or infrequent contexts, it islikely to have gained a new sense.
Our results indi-cate that this intuition was more fruitful for EMEAthan for Science or Subs.In contrast, the type-only features (TYPEONLY)6We use http://hunch.net/?vw/ version 7.1.2,and run it with the following arguments that affect learningbehavior: --exact adaptive norm --power t 0.51440Macro MicroAUC P R F P R FEMEARANDOM 50.34 ?
0.60 51.24 ?
0.59 50.09 ?
1.18 50.19 ?
0.75 47.04 ?
0.60 56.07 ?
1.99 37.27 ?
0.91CONSTANT 50.00 ?
0.00 50.99 ?
0.00 100.0 ?
0.00 67.09 ?
0.00 45.80 ?
0.00 100.0 ?
0.00 52.30 ?
0.00TYPEONLY 55.91 ?
1.13 69.76 ?
3.45 43.13 ?
1.42 41.61 ?
1.07 77.92 ?
2.04 50.12 ?
2.35 31.26 ?
0.63TYPEORACLE 88.73 ?
0.00 87.32 ?
0.00 86.76 ?
0.00 87.04 ?
0.00 90.01 ?
0.00 67.46 ?
0.00 59.39 ?
0.00TOKENONLY 78.80 ?
0.52 69.83 ?
1.59 75.58 ?
2.61 69.40 ?
1.92 59.03 ?
1.70 62.53 ?
1.66 43.39 ?
0.94ALLFEATURES 79.60 ?
1.20 68.11 ?
1.19 79.84 ?
2.27 71.64 ?
1.83 55.28 ?
1.11 71.50 ?
1.62 46.83 ?
0.62ScienceRANDOM 50.18 ?
0.78 24.48 ?
0.57 50.32 ?
1.33 32.92 ?
0.79 46.99 ?
0.51 60.32 ?
1.06 34.72 ?
1.03CONSTANT 50.00 ?
0.00 24.34 ?
0.00 100.0 ?
0.00 39.15 ?
0.00 44.39 ?
0.00 100.0 ?
0.00 50.44 ?
0.00TYPEONLY 77.06 ?
1.23 66.07 ?
2.80 36.28 ?
4.10 34.50 ?
4.06 84.97 ?
0.82 36.81 ?
2.33 24.22 ?
1.70TYPEORACLE 88.76 ?
0.00 78.43 ?
0.00 69.29 ?
0.00 73.54 ?
0.00 84.19 ?
0.00 67.41 ?
0.00 52.67 ?
0.00TOKENONLY 66.62 ?
0.47 60.50 ?
3.11 28.05 ?
2.06 30.81 ?
2.75 76.21 ?
1.78 36.57 ?
2.23 24.68 ?
1.36ALLFEATURES 73.91 ?
0.66 50.59 ?
2.08 60.60 ?
2.04 47.54 ?
1.52 66.72 ?
1.19 62.30 ?
1.36 40.22 ?
1.03SubsRANDOM 50.26 ?
0.69 42.47 ?
0.60 50.17 ?
0.84 45.68 ?
0.68 52.18 ?
1.32 54.63 ?
2.01 39.87 ?
2.10CONSTANT 50.00 ?
0.00 42.51 ?
0.00 100.0 ?
0.00 59.37 ?
0.00 50.63 ?
0.00 100.0 ?
0.00 58.67 ?
0.00TYPEONLY 67.16 ?
0.73 76.41 ?
1.51 31.91 ?
3.15 36.37 ?
2.58 90.03 ?
0.61 34.78 ?
1.12 26.20 ?
0.61TYPEORACLE 81.35 ?
0.00 83.12 ?
0.00 70.23 ?
0.00 76.12 ?
0.00 90.62 ?
0.00 52.37 ?
0.00 44.43 ?
0.00TOKENONLY 63.30 ?
0.99 63.17 ?
2.31 45.38 ?
2.07 43.30 ?
1.29 76.38 ?
1.68 49.70 ?
1.76 37.92 ?
1.20ALLFEATURES 69.26 ?
0.60 63.48 ?
1.77 56.22 ?
2.66 52.78 ?
1.96 67.55 ?
0.83 62.18 ?
1.45 43.85 ?
0.90Table 4: Complete SENSESPOTTING results for all domains.
The scores are from cross-validation ona single domain; in all cases, higher is better.
Two standard deviations of performance over the cross-validation are shown in small type.
For all domains and metrics, the highest (not necessarily statisticallysignificant) non-oracle results are bolded.are relatively weak for predicting new senses onEMEA data but stronger on Subs (TYPEONLYAUC performance is higher than both baselines)and even stronger on Science data (TYPEONLYAUC and f-measure performance is higherthan both baselines as well as the ALLFEA-TURESmodel).
In our experience with the threedatasets, we know that the Science data, whichcontains abstracts from a wide variety of scientificdisciplines, is the most diverse, followed by theSubs data, and then EMEA, which mostly consistsof text from drug labels and tends to be quite repet-itive.
Thus, it makes sense that type-level featureswould be the most informative for the least homo-geneous dataset.
Representative words in scien-tific text are likely to appear in variety of contexts,while in the EMEA data they may only appear ina few, making it easier to contrast them with thedistributions observed in the old domain data.For all domains, in micro-level evaluation, ourmodels fail to outperform the CONSTANT base-line.
Recall that the micro-level evaluation com-putes precision, recall, and f-measure for all wordtokens of a given word type and then averagesacross word types.
We observe that words that areless frequent in both the old and the new domainsare more likely to have a new sense than more fre-quent words, which causes the CONSTANT base-line to perform reasonably well.
In contrast, it ismore difficult for our models to make good pre-dictions for less frequent words.
A low frequencyin the new domain makes type level features (esti-mated over only a few instances) noisy and unreli-able.
Similarly, a low frequency in the old domainmakes the our token level features, which all con-trast with old domain instances of the word type.6.4 Feature AblationIn the previous section, we observed that (with oneexception) both Type-level and Token-level fea-tures are useful in our task (in some cases, essen-tial).
In this section, we look at finer-grained fea-ture distinctions through a process of feature ab-lation.
In this setting, we begin with all featuresin a model and remove one feature at a time, al-ways removing the feature that hurts performanceleast.
For these experiments, we determine whichfeature to remove using AUC.
Note that we?re ac-tually able to beat (by 2-4 points AUC) the scoresfrom Table 4 by removing features!The results here are somewhat mixed.
In EMEAand Science, one can actually get by (accord-ing to AUC) with very few features: just two(Type:NgramProband Type:Context) are suffi-cient to achieve optimal AUC scores.
To gethigher Macro-F scores requires nearly all the fea-tures, though this is partially due to the choice of1441EMEA AUC MacFALLFEATURES 79.60 71.64?Token:L-PSDBin 77.09 70.50?Type:RelFreq 78.43 72.19?Token:G-PSD 79.66 72.11?Type:Context 79.66 72.45?Token:Ctx% 78.91 73.37?Type:TopicSim 78.05 71.33?Token:CtxCnt 76.90 71.72?Token:L-PSD 76.03 73.35?Type:NgramProb 73.32 69.54?Token:G-PSDBin 74.41 69.76?Token:NgramProb 69.78 68.89?Token:PSDRatio 48.38 3.45Science AUC MacFALLFEATURES 73.91 47.54?Token:L-PSDBin 76.26 53.69?Token:G-PSD 77.04 53.56?Token:G-PSDBin 77.44 54.54?Token:L-PSD 77.85 56.05?Token:PSDRatio 77.92 57.34?Token:CtxCnt 77.85 54.42?Type:Context 78.17 55.45?Token:Ctx% 78.06 55.04?Type:TopicSim 77.83 54.57?Token:NgramProb 76.98 51.02?Type:RelFreq 74.25 49.57?Type:NgramProb 50.00 0.00Subs AUC MacFALLFEATURES 69.26 52.78?Type:NgramProb 69.13 53.33?Token:G-PSDBin 70.23 54.72?Token:CtxCnt 71.23 58.35?Token:L-PSDBin 72.07 57.85?Token:G-PSD 72.17 57.33?Type:TopicSim 72.31 58.41?Token:Ctx% 72.17 56.17?Token:NgramProb 71.35 59.26?Token:PSDRatio 70.33 46.88?Token:L-PSD 69.05 53.31?Type:RelFreq 65.25 48.22?Type:Context 50.00 0.00Table 5: Feature ablation results for all three corpora.
Selection criteria is AUC, but Macro-F is presentedfor completeness.
Feature selection is run independently on each of the three datasets.
The featurestoward the bottom were the first selected.AUC Macro-F Micro-FEMEATYPEONLY 71.43 ?
0.94 52.62 ?
3.41 38.67 ?
1.35TOKENONLY 73.75 ?
1.11 67.77 ?
4.18 45.49 ?
3.96ALLFEATURES 72.19 ?
4.07 67.26 ?
7.88 49.29 ?
3.55XV-ALLFEATURES 79.60 ?
1.20 71.64 ?
1.83 46.83 ?
0.62ScienceTYPEONLY 75.19 ?
0.89 51.53 ?
2.55 37.14 ?
4.41TOKENONLY 71.24 ?
1.45 47.27 ?
1.11 40.48 ?
1.84ALLFEATURES 74.14 ?
0.93 48.86 ?
3.94 43.20 ?
3.16XV-ALLFEATURES 73.91 ?
0.66 47.54 ?
1.52 40.22 ?
1.03SubsTYPEONLY 60.90 ?
1.47 39.21 ?
14.78 24.77 ?
2.78TOKENONLY 62.00 ?
1.16 49.74 ?
6.30 42.95 ?
3.92ALLFEATURES 60.12 ?
2.11 50.16 ?
8.63 38.56 ?
5.20XV-ALLFEATURES 69.26 ?
0.60 52.78 ?
1.96 43.85 ?
0.90Table 6: Cross-domain test results on the SENS-ESPOTTING task.
Two standard deviations areshown in small type.
Only AUC, Macro-F andMicro-F are shown for brevity.AUC as the measure on which to ablate.
It?s quiteclear that for Science, all the useful informationis in the type-level features, a result that echoeswhat we saw in the previous section.
While forEMEA and Subs, both type- and token-level fea-tures play a significant role.
Considering the sixmost useful features in each domain, the ones thatpop out as frequently most useful are the globalPSD features, the ngram probability features (ei-ther type- or token-based), the relative frequencyfeatures and the context features.6.5 Cross-Domain TrainingOne disadvantage to the previous method for eval-uating the SENSESPOTTING task is that it requiresparallel data in a new domain.
Suppose we have noparallel data in the new domain at all, yet still wantto attack the SENSESPOTTING task.
One option isto train a system on domains for which we do haveparallel data, and then apply it in a new domain.This is precisely the setting we explore in this sec-tion.
Now, instead of performing cross-validationin a single domain (for instance, Science), we takethe union of all of the training data in the otherdomains (e.g., EMEA and Subs), train a classifier,and then apply it to Science.
This classifier will al-most certainly be worse than one trained on NEW(Science) but does not require any parallel data inthat domain.
(Hyperparameters are chosen by de-velopment data from the OLD union.
)The results of this experiment are shown inTable 6.
We include results for TOKENONLY,TYPEONLY and ALLFEATURES; all of these aretrained in the cross-domain setting.
To ease com-parison to the results that do not suffer from do-main shift, we also present ?XV-ALLFEATURES?,which are results copied from Table 4 in whichparallel data from NEW is used.
Overall, there is adrop of about 7.3% absolute in AUC, moving fromXV-ALLFEATURES to ALLFEATURES, includinga small improvement in Science (likely becauseScience is markedly smaller than Subs, and ?moredifficult?
than EMEA with many word types).6.6 Detecting Most Frequent Sense ChangesWe define a second, related task: MOSTFRE-QSENSECHANGE.
In this task, instead of predict-ing if a given word token has a sense which isbrand new with respect to the old domain, we pre-dict whether it is being used with a a sense whichis not the one that was observed most frequentlyin the old domain.
In our EMEA, Science, andSubtitles data, 68.2%, 48.3%, and 69.6% of wordtokens?
predominant sense changes.14426 12 25 50 100.32.40.50.63ScienceMacro?F% of data 6 12 25 50 100.40.50.63.79 EMEA% of data6 12 25 50 100.40.50.63Subs% of dataTypeOracleRandomAllFeaturesFigure 1: Learning curves for the three domains.
X-axis is percent of data used, Y-axis is Macro-F score.Both axes are in log scale to show the fast rate of growth.
A horizontal bar corresponding to randompredictions, and the TYPEORACLE results are shown for comparison.AUC Macro-F Micro-FEMEARANDOM 50.54 ?
0.41 58.23 ?
0.34 49.69 ?
0.85CONSTANT 50.00 ?
0.00 82.15 ?
0.00 74.43 ?
0.00TYPEONLY 55.05 ?
1.00 67.45 ?
1.35 65.72 ?
0.59TYPEORACLE 88.36 ?
0.00 90.64 ?
0.00 77.46 ?
0.00TOKENONLY 66.42 ?
1.07 80.27 ?
0.50 68.96 ?
0.58ALLFEATURES 58.64 ?
3.45 80.57 ?
0.45 69.40 ?
0.51ScienceRANDOM 50.13 ?
0.78 49.05 ?
0.82 48.19 ?
1.47CONSTANT 50.00 ?
0.00 65.21 ?
0.00 73.22 ?
0.00TYPEONLY 68.32 ?
1.05 54.70 ?
2.35 57.04 ?
1.52TYPEORACLE 91.41 ?
0.00 86.71 ?
0.00 74.26 ?
0.00TOKENONLY 68.49 ?
0.59 62.76 ?
0.89 64.40 ?
1.08ALLFEATURES 68.31 ?
0.93 64.73 ?
1.93 67.20 ?
1.65SubsRANDOM 50.27 ?
0.27 56.93 ?
0.29 50.93 ?
1.11CONSTANT 50.00 ?
0.00 79.96 ?
0.00 76.26 ?
0.00TYPEONLY 60.36 ?
0.90 67.78 ?
1.98 61.58 ?
1.78TYPEORACLE 82.16 ?
0.00 87.96 ?
0.00 73.87 ?
0.00TOKENONLY 59.49 ?
1.04 77.79 ?
0.82 73.51 ?
0.68ALLFEATURES 54.97 ?
0.89 77.30 ?
1.58 72.29 ?
1.68Table 7: Cross-validation results on the MOST-FREQSENSECHANGE task.
Two standard devia-tions are shown in small type.We use the same set of features and learn-ing framework to generate and evaluate modelsfor this task.
While the SENSESPOTTING taskhas MT utility in suggesting which new domainwords demand a new translation, the MOSTFRE-QSENSECHANGE task has utility in suggestingwhich words demand a new translation proba-bility distribution when shifting to a new do-main.
Table 7 shows the results of our MOSTFRE-QSENSECHANGE task experiments.Results on the MOSTFREQSENSECHANGEtask are somewhat similar to those for the SENS-ESPOTTING task.
Again, our models perform bet-ter under a macro-level evaluation than under amicro-level evaluation.
However, in contrast tothe SENSESPOTTING results, token-level featuresperform quite well on their own for all domains.It makes sense that our token level features have abetter chance of success on this task.
The impor-tant comparison now is between a new domain to-ken in context and the majority of the old domaintokens of the same word type.
This comparisonis likely to be more informative than when we areequally interested in identifying overlap betweenthe current token and any old domain senses.
Likethe SENSESPOTTING results, when doing a micro-level evaluation, our models do not perform aswell as the CONSTANT baseline, and, as before,we attribute this to data sparsity.6.7 Learning CurvesAll of the results presented so far use classi-fiers trained on instances of representative types(i.e.
?representative tokens?)
extracted from fairlylarge new domain parallel corpora (see Table 3),consisting of between 22 and 36 thousand parallelsentences, which yield between 8 and 35 thousandrepresentative tokens.
Although we expect somenew domain parallel tuning data to be availablein most MT settings, we would like to know howmany representative types are required to achievegood performance on the SENSESPOTTING task.Figure 6.5 shows learning curves over the num-ber of representative tokens that are used to trainSENSESPOTTING classifiers.
In fact, only about25-50% of the data we used is really necessary toachieve the performance observed before.Acknowledgments We gratefully acknowledge the supportof the JHU summer workshop program (and its funders), theentire DAMT team (http://hal3.name/DAMT/), San-jeev Khudanpur, support from the NRC for Marine Carpuat,as well as DARPA CSSG Grant D11AP00279 for Hal Daume?III and Jagadeesh Jagarlamudi.1443ReferencesE.
Agirre and P.G.
Edmonds.
2006.
Word Sense Dis-ambiguation: Algorithms and Applications.
Text,Speech, and Language Technology Series.
SpringerScience+Business Media B.V.Eneko Agirre and Aitor Soroa.
2007.
Semeval-2007task 02: Evaluating word sense induction and dis-crimination systems.
In Proceedings of the FourthInternational Workshop on Semantic Evaluations(SemEval-2007), pages 7?12.David Bamman and Gregory Crane.
2011.
Measuringhistorical word sense variation.
In Proceedings ofthe 2011 Joint International Conference on DigitalLibraries (JCDL 2011), pages 1?10.D.
Blei, A. Ng, and M. Jordan.
2003.
Latent Dirichletallocation.
Journal of Machine Learning Research(JMLR), 3.Michael Bloodgood and Chris Callison-Burch.
2010.Bucking the trend: Large-scale cost-focused activelearning for statistical machine translation.
In Pro-ceedings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, pages 854?864,Uppsala, Sweden, July.
Association for Computa-tional Linguistics.Marine Carpuat and Dekai Wu.
2007.
ImprovingStatistical Machine Translation using Word SenseDisambiguation.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL 2007), pages 61?72, Prague, June.Marine Carpuat, Hal Daume?
III, Alexander Fraser,Chris Quirk, Fabienne Braune, Ann Clifton, AnnIrvine, Jagadeesh Jagarlamudi, John Morgan, Ma-jid Razmara, Ales?
Tamchyna, Katharine Henry, andRachel Rudinger.
2012.
Domain adaptation in ma-chine translation: Final report.
In 2012 Johns Hop-kins Summer Workshop Final Report.Yee Seng Chan and Hwee Tou Ng.
2007.
Domainadaptation with active learning for word sense dis-ambiguation.
In Proceedings of the Association forComputational Linguistics.Paul Cook and Suzanne Stevenson.
2010.
Automati-cally identifying changes in the semantic orientationof words.
In Proceedings of the 7th InternationalConference on Language Resources and Evaluation,pages 28?34, Valletta, Malta.Hal Daume?
III and Jagadeesh Jagarlamudi.
2011.
Do-main adaptation for machine translation by min-ing unseen words.
In Proceedings of the Confer-ence of the Association for Computational Linguis-tics (ACL).Katrin Erk.
2006.
Unknown word sense detection asoutlier detection.
In Proceedings of the main confer-ence on Human Language Technology Conferenceof the North American Chapter of the Association ofComputational Linguistics, pages 128?135.Pascale Fung and Lo Yuen Yee.
1998.
An IR approachfor translating new words from nonparallel, compa-rable texts.
In Proceedings of the Conference of theAssociation for Computational Linguistics (ACL).Kristina Gulordava and Marco Baroni.
2011.
A distri-butional similarity approach to the detection of se-mantic change in the google books ngram corpus.
InProceedings of the GEMS 2011 Workshop on GE-ometrical Models of Natural Language Semantics,pages 67?71, Edinburgh, UK, July.
Association forComputational Linguistics.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexi-cons from monolingual corpora.
In Proceedings ofthe Conference of the Association for ComputationalLinguistics (ACL).Matthew Hoffman, David Blei, and Francis Bach.2010.
Online learning for latent dirichlet alocation.In Advances in Neural Information Processing Sys-tems (NIPS).Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the Conference of the Association forComputational Linguistics (ACL).Jey Han Lau, Paul Cook, Diana McCarthy, David New-man, Timothy Baldwin, and Lexical Computing.2012.
Word sense induction for novel sense de-tection.
In Proceedings of the 13th Conference ofthe European Chapter of the Association for compu-tational Linguistics (EACL 2012), pages 591?601.Citeseer.Bernardo Magnini, Carlo Strapparava, Giovanni Pez-zulo, and Alfio Gliozzo.
2002.
The role of domaininformation in word sense disambiguation.
NaturalLanguage Engineering, 8(04):359?373.Mausam, Stephen Soderland, Oren Etzioni, Daniel S.Weld, Kobi Reiter, Michael Skinner, Marcus Sam-mer, and Jeff Bilmes.
2010.
Panlingual lexicaltranslation via probabilistic inference.
Artificial In-telligence, 174:619?637, June.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word senses inuntagged text.
In Proceedings of the 42nd AnnualMeeting on Association for Computational Linguis-tics, page 279.
Association for Computational Lin-guistics.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2007.
Unsupervised acquisition of pre-dominant word senses.
Computational Linguistics,33(4):553?590.1444Behrang Mohit and Rebecca Hwa.
2007.
Localiza-tion of difficult-to-translate phrases.
In proceedingsof the 2nd ACL Workshop on Statistical MachineTranslations.Reinhard Rapp.
1995.
Identifying word translationsin non-parallel texts.
In Proceedings of the Confer-ence of the Association for Computational Linguis-tics (ACL).Eyal Sagi, Stefan Kaufmann, and Brady Clark.
2009.Semantic density analysis: Comparing word mean-ing across time and phonetic space.
In Proceedingsof the EACL 2009 Workshop on GEMS: GEometicalModels of Natural Language Semantics, pages 104?111, Athens, Greece, March.Charles Schafer and David Yarowsky.
2002.
Inducingtranslation lexicons via diverse similarity measuresand bridge languages.
In Proceedings of the Confer-ence on Natural Language Learning (CoNLL).Charles Schafer.
2006.
Translation Discovery UsingDiverse Similarity Measures.
Ph.D. thesis, JohnsHopkins University.Jo?rg Tiedemann.
2009.
News from OPUS - A collec-tion of multilingual parallel corpora with tools andinterfaces.
In N. Nicolov, K. Bontcheva, G. An-gelova, and R. Mitkov, editors, Recent Advances inNatural Language Processing (RANLP).Wikipedia.
2013.
Receiver operating characteristic.http://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_Under_the_Curve, February.1445
