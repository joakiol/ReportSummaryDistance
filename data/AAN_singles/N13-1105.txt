Proceedings of NAACL-HLT 2013, pages 847?857,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsA Quantum-Theoretic Approach to Distributional SemanticsWilliam Blacoe1, Elham Kashefi2, Mirella Lapata11Institute for Language, Cognition and Computation,2Laboratory for Foundations of Computer ScienceSchool of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9ABw.b.blacoe@sms.ed.ac.uk, ekashefi@inf.ed.ac.uk, mlap@inf.ed.ac.ukAbstractIn this paper we explore the potential ofquantum theory as a formal framework forcapturing lexical meaning.
We present anovel semantic space model that is syntacti-cally aware, takes word order into account,and features key quantum aspects such assuperposition and entanglement.
We definea dependency-based Hilbert space and showhow to represent the meaning of words by den-sity matrices that encode dependency neigh-borhoods.
Experiments on word similarityand association reveal that our model achievesresults competitive with a variety of classicalmodels.1 IntroductionThe fields of cognitive science and natural languageprocessing have recently produced an ensemble ofsemantic models which have an impressive trackrecord of replicating human behavior and enablingreal-world applications.
Examples include simula-tions of word association (Denhie`re and Lemaire,2004; Griffiths et al 2007), semantic priming (Lundand Burgess, 1996; Landauer and Dumais, 1997;Griffiths et al 2007), categorization (Laham, 2000),numerous studies of lexicon acquisition (Grefen-stette, 1994; Lin, 1998), word sense discrimination(Schu?tze, 1998), and paraphrase recognition (Socheret al 2011).
The term ?semantic?
derives from theintuition that words seen in the context of a givenword contribute to its meaning (Firth, 1957).
Al-though the specific details of the individual modelsdiffer, they all process a corpus of text as input andrepresent words (or concepts) in a (reduced) high-dimensional space.In this paper, we explore the potential of quan-tum theory as a formal framework for capturing lex-ical meaning and modeling semantic processes suchas word similarity and association (see Section 6for an overview of related research in this area).We use the term quantum theory to refer to the ab-stract mathematical foundation of quantum mechan-ics which is not specifically tied to physics (Hughes,1989; Isham, 1989).
Quantum theory is in prin-ciple applicable in any discipline where there is aneed to formalize uncertainty.
Indeed, researchershave been pursuing applications in areas as diverseas economics (Baaquie, 2004), information theory(Nielsen and Chuang, 2010), psychology (Khren-nikov, 2010; Pothos and Busemeyer, 2012), and cog-nitive science (Busemeyer and Bruza, 2012; Aerts,2009; Bruza et al 2008).
But what are the featuresof quantum theory which make it a promising frame-work for modeling meaning?Superposition, entanglement, incompatibility,and interference are all related aspects of quantumtheory, which endow it with a unique character.1 Su-perposition is a way of modeling uncertainty, moreso than in classical probability theory.
It contains in-formation about the potentialities of a system?s state.An electron whose location in an atom is uncertaincan be modeled as being in a superposition of loca-tions.
Analogously, words in natural language canhave multiple meanings.
In isolation, the word penmay refer to a writing implement, an enclosure forconfining livestock, a playpen, a penitentiary or a fe-male swan.
However, when observed in the contextof the word ink the ambiguity resolves into the senseof the word dealing with writing.
The meanings ofwords in a semantic space are superposed in a waywhich is intuitively similar to the atom?s electron.Entanglement concerns the relationship between1It is outside the scope of the current paper to give a detailedintroduction on the history of quantum mechanics.
We referthe interested reader to Vedral (2006) and Kleppner and Jackiw(2000) for comprehensive overviews.847systems for which it is impossible to specify a jointprobability distribution from the probability distri-butions of their constituent parts.
With regard toword meanings, entanglement encodes (hidden) re-lationships between concepts.
The different sensesof a word ?exist in parallel?
until it is observedin some context.
This reduction of ambiguity haseffects on other concepts connected via entangle-ment.
The notion of incompatibility is fundamen-tal to quantum systems.
In classical systems, it isassumed by default that measurements are compati-ble, that is, independent, and as a result the order inwhich these take place does not matter.
By contrastin quantum theory, measurements may share (hid-den) order-sensitive inter-dependencies and the out-come of the first measurement can change the out-come of the second measurement.Interference is a feature of quantum probabilitythat can cause classical assumptions such as the lawof total probability to be violated.
When conceptsinteract their joint representation can exhibit non-classical behavior, e.g., with regard to conjunctionand disjunction (Aerts, 2009).
An often cited ex-ample is the ?guppy effect?.
Although guppy is anexample of a pet-fish it is neither a very typical petnor fish (Osherson and Smith, 1981).In the following we use the rich mathematicalframework of quantum theory to model semantic in-formation.
Specifically, we show how word mean-ings can be expressed as quantum states.
A wordbrings with it its own subspace which is spanned byvectors representing its potential usages.
We presenta specific implementation of a semantic space that issyntactically aware, takes word order into account,and features key aspects of quantum theory.
We em-pirically evaluate our model on word similarity andassociation and show that it achieves results com-petitive with a variety of classical models.
We be-gin by introducing some of the mathematical back-ground needed for describing our approach (Sec-tion 2).
Next, we present our semantic space model(Section 3) and our evaluation experiments (Sec-tions 4 and 5).
We conclude by discussing relatedwork (Section 6).2 PreliminariesLet c = r ei?
be a complex number, expressed in po-lar form, with absolute value r = |c| and phase ?.
Itscomplex conjugate c?
= r e?i?
has the inverse phase.Thus, their product cc?
= (r ei?
)(r e?i?)
= r2 is real.2.1 VectorsWe are interested in finite-dimensional, complex-valued vector spaces Cn with an inner product, oth-erwise known as Hilbert space.
A column vector???
?
Cn can be written as an ordered vertical arrayof its n complex-valued components, or alternativelyas a weighted sum of base vectors:???=?????1?2...?n????=?1????10...0???
?+ .
.
.+?n????0...01????
(1)Whereas Equation (1) uses base vectors from thestandard base Bstd = {?
?b1 , ...,?
?bn}, any other set of northonormal vectors serves just as well as a base forthe same space.
Dirac (1939) introduced the so-called bra-ket notation which is equally expressivebut notationally more convenient.
A column vectorbecomes a ket:???
?
|?
?= ?1|b1?+?2|b2?+ .
.
.+?n|bn?
(2)and a row vector becomes a bra ??|.
Transposinga complex-valued vector or matrix (via the super-script ???)
involves complex-conjugating all compo-nents:|???
= ?
?|= ??1?b1|+?
?2?b2|+ .
.
.+?
?n?bn| (3)The Dirac notation for the inner product ??|??
il-lustrates the origin of the terminology ?bra-ket?.Since Bstd?s elements are normalised and pairwiseorthogonal their inner product is:?bi|b j?={1, if i = j0, otherwise(4)The inner product is also applicable to pairs of non-base kets:(?
?1 ?
?2 ?
?
?
??n)?????1?2...?n?????
??|?
?= (?i?
?i ?bi|)(?
j ?
j|b j?
)= ?i, j?
?i ?
j?bi|b j?= ?i?
?i ?i?bi|bi?= ?i?
?i ?i(5)848Reversing the order of an inner product complex-conjugates it:(??|??)?
= ??|??
(6)2.2 MatricesMatrices are sums of outer products |????|.
For ex-ample, the matrix (Mi, j)i, j can be thought of asthe weighted sum of ?base-matrices?
Bi, j ?
|bi?
?b j|,whose components are all 0 except for a 1 in the i-throw and j-th column.
The outer product extends lin-early to non-base kets in the following manor:|???
?|= (?i?i|bi?)(?
j ?
?j?b j|)= ?i, j?i?
?j |bi?
?b j|(7)This is analogous to the conventional multiplication:????1...?n???(?
?1 ?
?
?
??n)=????1?
?1 ?
?
?
?1??n....
.
....?n?
?1 ?
?
?
?n??n???
(8)We will also make use of the tensor product.
Its ap-plication to kets, bras and outer products is linear:(|a?+ |b?
)?|c?= |a??
|c?+ |b??
|c?
(?a|+ ?b|)?
?c|= ?a|?
?c|+ ?b|?
?c|(|a?
?b|+ |c??d|)?|e??
f |=(|a??
|e?)(?b|?
?
f |)+(|c??
|e?)(?d|?
?
f |)(9)For convenience we omit ???
where no confusionarises, e.g., |a?
?
|b?
= |a?|b?.
When applied toHilbert spaces, the tensor product creates the com-posed Hilbert space H = H1?
...?Hn whose basekets are simply induced by the tensor product of itssubspaces?
base kets:base(H1?
...?Hn) ={nOi=1|b?i : |b?i ?
base(Hi), 1?
i?
n} (10)Whereas the order of composed kets |a?|b?|c?
usu-ally suffices to identify which subket lives in whichsubspace, we make this explicit by giving subketsthe same subscript as the corresponding subspace.Thus, the order no longer matters, as in the follow-ing inner product of composed kets:(?a|1?b|2?c|3)(|e?3|d?1| f ?2) = ?a|d?
?b| f ??c|e?
(11)Definition 1.
Self-adjoint MatrixA matrix M is self-adjoint iff Mi, j = M?j,i for all i, j.Consequently, all diagonal elements are real-valued,and M = M?
is its own transpose conjugate.Definition 2.
Density MatrixA self-adjoint matrix M is a density matrix iffit is positive semi-definite, i.e., ??|M|??
?
0 forall |??
?
Cn, and it has unit trace, i.e., Tr(M) =?|b?
?B ?b|M|b?= 1.The term ?density matrix?
is synonymous with?density operator?.
Any density matrix ?
canbe decomposed arbitrarily as ?
= ?i pi|si?
?si|, theweighted sum of sub-matrices |si?
?si| with pi ?
R>0and ?si|si?= 1.
The pi need not sum to 1.
In fact thedecomposition where the pi sum to 1 and the |si?
aremutually orthogonal is unique and is called the eigendecomposition.
Consequently Beig = {|si?
}i consti-tutes an orthonormal base, ?
?s so-called eigen base.Density operators are used in quantum theory to de-scribe the state of some system.
If the system?s state?
is certain we call it a pure state and write ?= |s?
?s|for some unit ket |s?.
Systems whose state is uncer-tain are described by a mixed state ?
= ?i pi|si?
?si|which represents an ensemble of substates or purestates {(pi,si)}i where the system is in substate siwith probability pi.
Hence, the term ?density?
as inprobability density.It is possible to normalize a density matrixwithout committing to any particular decomposi-tion.
Only the trace function is required, becausenorm(?)
= ?/Tr(?).
Definition 2 mentions what thetrace function does.
However, notice that the sameresult is produced for any orthonormal base B , in-cluding ?
?s eigen base Beig = {|ei?}i.
Even thoughwe do not know the content of Beig, we know that it849exists.
So we use it to show that dividing ?
by:Tr(?)
= Tr(?i pi|ei?
?ei|)= ?
j?e j|(?i pi|ei?
?ei|)|e j?= ?i, j pi?e j|ei?
?ei|e j?= ?i pi?ei|ei?
?ei|ei?= ?i pi(12)normalizes its probability distribution over eigenkets:?Tr(?)
=?i pi|ei??ei|?
j p j=?ipi?
j p j|ei?
?ei|(13)3 Semantic Space ModelWe represent the meaning of words by density ma-trices.
Specifically, a lexical item w is modeled asan ensemble Uw = {(pi,ui)}i of usages ui and thecorresponding probabilities pi that w gets used ?inthe i-th manor?.
A word?s usage is comprised ofdistributional information about its syntactic and se-mantic preferences, in the form of a ket |ui?.
Thedensity matrix ?w = ?i pi|ui?
?ui| represents the en-semble Uw.
This section explains our method of ex-tracting lexical density matrices from a dependency-parsed corpus.
Once density matrices have beenlearned, we can predict the expected usage similar-ity of two words as a simple function of their densitymatrices.
Our explication will be formally precise,but at the same time illustrate each principle througha toy example.3.1 Dependency Hilbert SpaceOur model learns the meaning of words from adependency-parsed corpus.
Our experiments haveused the Stanford parser (de Marneffe and Man-ning, 2008), however any other dependency parserwith broadly similar output could be used instead.A word?s usage is learned from the type of depen-dency relations it has with its immediate neighborsin dependency graphs.
Its semantic content is thusapproximated by its ?neighborhood?, i.e., its co-occurrence frequency with neighboring words.Neighborhoods are defined by a vocabu-lary V = {w1, ...,wnV } of the nV most fre-quent (non-stop) words in the corpus.
LetRel = {sub?1,dobj?1,amod,num,poss, ...} denoteDocument 1:(1a)the man see two angry jaguardet subjdobjnumanod(1b)we see two angry elephantsubjdobjnumamod(1c)two elephant runnum nsubjDocument 2:(2a)she buy a nice new jaguarsubjdobjdetamodamod(2b)I like my jaguarsubjdobjpossFigure 1: Example dependency trees in a toy corpus.
Dot-ted arcs are ignored because they are either not connectedto the target words jaguar and elephant or because theirrelation is not taken into account in constructing the se-mantic space.
Words are shown as lemmas.a subset of all dependency relations provided bythe parser and their inverses.
The choice of Rel is amodel parameter.
We considered only the most fre-quently occuring relations above a certain threshold,which turned out to be about half of the full inven-tory.
Relation symbols with the superscript ?
?1?indicate the inversion of the dependency direction(dependent to head).
All other relation symbolshave the conventional direction (head to dependent).Hence, wxyz?
v is equivalent to vxyz?1?
w. We thenpartition Rel into disjoint clusters of syntacticallysimilar relations Part = {RC1, ...,RCnPart}.
Forexample, we consider syntactically similar relationswhich connect target words with neighbors withthe same part of speech.
Each relation cluster RCkis assigned a Hilbert space Hk whose base kets{|w(k)j ?}
j correspond to the words in V = {w j} j.Figure 1 shows the dependency parses for atoy corpus consisting of two documents and fivesentences.
To create a density matrix for the targetwords jaguar and elephant, let us assume that we850will consider the following relation clusters:RC1 = {dobj?1,iobj?1,agent?1,nsubj?1, ...},RC2 = {advmod,amod,tmod, ...} and RC3 = {nn,appos,num,poss, ...}.3.2 Mapping from Dependency Graphs to KetsNext, we create kets which encode syntactic and se-mantic relations as follows.
For each occurrence ofthe target word w in a dependency graph, we onlyconsider the subtree made up of w and the immedi-ate neighbors connected to it via a relation in Rel.In Figure 1, arcs from the dependency parse that weignore are shown as dotted.
Let the subtree of in-terest be st = {(RC1,v1), ...,(RCnPart ,vnPart )}, that is,w is connected to vk via some relation in RCk, fork ?
{1, ...,nPart}.
For any relation cluster RCk thatdoes not feature in the subtree, let RCk be paired withthe abstract symbol w /0 in st.
This symbol representsuncertainty about a potential RCk-neighbor.We convert all subtrees st in the corpus for the tar-get word w into kets |?st?
?
H1?
...?HnPart .
Thesein turn make up the word?s density matrix ?w.
Be-fore we do so, we assign each relation cluster RCka complex value ?k = ei?k .
The idea behind thesevalues is to control for how much each subtree con-tributes to the overall density matrix.
This becomesmore apparent after we formulate our method of in-ducing usage kets and density matrices.|?st?= ?stO(RCk,v)?st|v?k, (14)where ?st = ?
(RCk,v)?st,v 6=w /0 ?k.
Every RCk pairedwith some neighbor v ?
V induces a basic subket|v?k ?
base(Hk), i.e., a base ket of the k-th sub-space or subsystem.
All other subkets |w /0?k =?v?V |V |?
12 |v?k are in a uniformly weighted super-position of all base kets.
The factor |V |?12 ensuresthat ?w /0|w /0?
= 1.
The composed ket for the sub-tree st is again weighted by the complex-valued ?st .
?st is the sum of complex values ?k = ei?k , eachwith absolute value 1.
Therefore, its own abso-lute value depends highly on the relative orienta-tion ?k among its summands: equal phases reinforceabsolute value, but the more phases are opposed(i.e., their difference approaches pi), the more theycancel out the sum?s absolute value.
Only those ?kcontribute to this sum whose relation cluster is notpaired with w /0.
The choice of the parameters ?k al-lows us to put more weight on some combinations(a)?
?st1 | ?
?st2 | ?
?st3 ||?st1?|?st2?|?st3?6= 0 6= 0 6= 06= 0 6= 0 6= 06= 0 6= 0 6= 0(b)?
?st1 | ?
?st2 | ?
?st3 ||?st1?|?st2?|?st3?6= 0 0 00 6= 0 00 0 6= 0Figure 2: Excerpts of density matrices that result fromthe dependency subtrees st1,st2,st3.
Element mi, j in row iand column j is mi, j|?sti??
?st j | in Dirac notation.
(a) Allthree subtrees are in the same document.
Thus their ketscontribute to diagonal and off-diagonal matrix elements.
(b) Each subtree is in a separate document.
Thereforetheir kets do not group, affecting only diagonal matrixelements.of dependency relations than others.Arbitrarily choosing ?1 = pi4 , ?2 =7pi4 ,and ?3 = 3pi4 renders the subtrees in Fig-ure 1 as |?st1a?
= eipi/4|see?1|angry?2|two?3,|?st2a?=?2|buy?1(|nice?2 + |new?2)|w /0?3, |?st2b?=?2eipi/2|like?1|w /0?2|my?3, which are relevant forjaguar, and |?st1b?
= eipi/4|see?1|angry?2|two?3,|?st1c?
=?2eipi/2|run?1|w /0?2|two?3, which arerelevant for elephant.
The subscripts outside of thesubkets correspond to those of the relation clustersRC1,RC2,RC3 chosen in Section 3.1.In sentence 2a, jaguar has two neighbors underRC2.
Therefore the subket from H2 is a superpo-sition of the base kets |nice?2 and |new?2.
This isa more intuitive formulation of the equivalent ap-proach which first splits the subtree for buy nice newjaguar into two similar subtrees for buy nice jaguarand for buy new jaguar, and then processes them asseperate subtrees within the same document.3.3 Creating Lexical Density MatricesWe assume that a word?s usage is uniform through-out the same document.
In our toy corpus in Fig-ure 1, jaguar is always the direct object of the mainverb.
However, in Document 1 it is used in the an-imal sense, whereas in Document 2 it is used in thecar sense.
Even though the usage of jaguar in sen-tence (2b) is ambiguous, we group it with that ofsentence (2a).These considerations can all be comfortably en-851?
jaguar = (|?st1a??
?st1a |+(|?st2a?+ |?st2b?)(?
?st2a |+ ?
?st2b |))/7 =0.14|see?1|angry?2|two?3?see|1?angry|2?two|3+ 0.29|buy?1|nice?2|w /0?3?buy|1?nice|2?w /0|3+0.29|buy?1|nice?2|w /0?3?buy|1?new|2?w /0|3+ 0.29|buy?1|new?2|w /0?3?buy|1?nice|2?w /0|3+0.29|buy?1|new?2|w /0?3?buy|1?new|2?w /0|3+ 0.29epi/2|like?1|w /0?2|my?3?buy|1?nice|2?w /0|3+0.29epi/2|like?1|w /0?2|my?3?buy|1?new|2?w /0|3+ 0.29e?pi/2|buy?1|nice?2|w /0?3?like|1?w /0|2?my|3+0.29e?pi/2|buy?1|new?2|w /0?3?like|1?w /0|2?my|3+ 0.29|like?1|w /0?2|my?3?like|1?w /0|2?my|3?elephant = ((|?st1b?+ |?st1c?)(?
?st1b |+ ?
?st1c |))/3 =0.33|see?1|angry?2|two?3?see|1?angry|2?two|3+ 0.47epi/4|run?1|w /0?2|two?3?see|1?angry|2?two|3+0.47e?pi/4|see?1|angry?2|two?3?run|1?w /0|2?two|3+ 0.67|run?1|w /0?2|two?3?run|1?w /0|2?two|3Tr(?
jaguar?elephant)=Tr(0.05|?st1a??
?st1b |+0.05eipi/4|?st1a??
?st1c |)=Tr(0.05|see?1|angry?2|two?3?see|1?angry|2?two|3+0.07e?pi/4|see?1|angry?2|two?3?run|1?w /0|2?two|3) = ?|b?
?base(H1?H2?H3)?b|(0.05|see?1|angry?2|two?3?see|1?angry|2?two|3+0.07e?pi/4|see?1|angry?2|two?3?run|1?w /0|2?two|3)|b?= 0.05Figure 3: Lexical density matrices for the words jaguar and elephant and their similarity.coded in a density matrix.
This is simply gener-ated via the outer product of our subtree kets |?st?.For example, ?D1, jaguar = |?st1a??
?st1a | representsthe contribution that document D1 makes to ?
jaguar.Document D2, however, has more than one ketrelevant to ?
jaguar.
Due to our assumption ofdocument-internal uniformity of word usage, wegroup D2?s subtree-kets additively: ?D2, jaguar =(|?st2a?+ |?st2b?)(?
?st2a |+?
?st2b |).
The target word?sdensity matrix ?w is the normalized sum of all den-sity matrices ?D,w obtained from each D:?D,w =(?st?STD,w|?st?)(?st?STD,w?
?st |)(15)where STD,w is the set of all subtrees for targetword w in document D. To illustrate the differ-ence that this grouping makes, consider the den-sity matrices in Figure 2.
Whereas in (a) the sub-trees st1,st2,st3 share a document, in (b) they arefrom distinct documents.
This grouping causes themto not only contribute to diagonal matrix elements,e.g., |?st2??
?st2 |, as in (b), but also to off diagonalones, e.g., |?st2??
?st1 |, as in (a).Over the course of many documents the summa-tion of all contributions, no matter how small orlarge the groups are, causes ?clusters of weight?to form, which hopefully coincide with word us-ages.
As mentioned in Section 3.2, adding complex-valued matrix elements increases or decreases thesum?s absolute value depending on relative phaseorientation.
This makes it possible for interferenceto occur.
Since the same word appears in varyingcontexts, the corresponding complex-valued outerproducts interact upon summation.
Finally, the den-sity matrix gets normalized, i.e., divided by its trace.This leaves the distributional information intact andmerely normalizes the probabilities.
Figure 3 illus-trates the estimation of the density matrices for thewords jaguar and elephant from the toy corpus inFigure 1.3.4 Usage SimilarityDecomposing the density matrix of the targetword w, ?w =?i pi|ui?
?ui| recovers the usage ensem-ble Uw = {(pi,ui)}i.
However, in general there areinfinitely many possible ensembles which ?w mightrepresent.
This subsection explains our metric forestimating the usage similarity of two words.
Themath involved shows that we can avoid the questionof how to best decompose ?w.We compute the usage similarity of two words wand v by comparing each usage of w with each us-age of v and weighting these similarity values withthe corresponding usage probabilities.
Let ?w =?i p(w)i |u(w)i ?
?u(w)i | and ?v = ?i p(v)i |u(v)i ?
?u(v)i |.
Thesimilarity of some usage kets |u(w)i ?
and |u(v)j ?
is ob-tained, as is common in the literature, by their in-ner product ?u(w)i |u(v)j ?.
However, as this is a com-plex value, we multiply it with its complex conju-gate, rendering the real value ?u(v)j |u(w)i ?
?u(w)i |u(v)j ?=|?u(w)i |u(v)j ?|2.
Therefore, in total the expected simi-larity of w and v is:852(16)sim(w,v) =?i, jp(w)i p(v)j ?u(v)j |u(w)i ?
?u(w)i |u(v)j ?= Tr(?i, jp(w)i p(v)j |u(w)i ?
?u(w)i |u(v)j ?
?u(v)j |)=Tr((?ip(w)i |u(w)i ?
?u(w)i |)(?jp(v)j |u(v)j ?
?u(v)j |))= Tr(?w?v)We see that the similarity function simply reduces tomultiplying ?w with ?v and applying the trace func-tion.
The so-called cyclic property of the trace func-tion (i.e., Tr(M1M2)= Tr(M2M1) for any two matri-ces M1,M2) gives us the corollary that this particularsimilarity function is symmetric.Figure 3 (bottom) shows how to calculate the sim-ilarity of jaguar and elephant.
Only the coefficientof the first outer product survives the tracing pro-cess because its ket and bra are equal modulo trans-pose conjugate.
As for the second outer product,0.05eipi/4?b|?st1a??
?st1c |b?
is 0 for all base kets |b?.3.5 What Does This Achieve?We represent word meanings as described above forseveral reasons.
The density matrix decomposes intousages each of which are a superposition of combi-nations of dependents.
Internally, these usages areestablished automatically by way of ?clustering?.Our model is parameterized with regard to thephases of sub-systems (i.e., clusters of syntactic re-lations) which allows us to make optimal use of in-terference, as this plays a large role in the over-all quality of representation.
It is possible for acombination of (groups of) dependents to get en-tangled if they repeatedly appear together under thesame word, and only in that combination.
If theco-occurence of (groups of) dependents is uncorre-lated, though, they remain unentangled.
Quantumentanglement gives our semantic structures the po-tential for long-distance effects, once quantum mea-surement becomes involved.
This is in analogy tothe nonlocal correlation between properties of sub-atomic particles, such as the magnetic spin of elec-trons or the polarization of photons.
Such an exten-sion to our implementation will also uncover whichsets of measurements are order-sensitive, i.e., in-compatible.Our similarity metric allows two words to ?select?each other?s usages via their pairwise inner prod-ucts.
Usage pairs with a high distributional simi-larity roughly ?align?
and then get weighted by theprobabilities of those usages.
Two words are similarif they are substitutable, that is, if they can be usedin the same syntactic environment and have the samemeaning.
Hopefully, this leads to more accurate es-timation of distributional similarity and can be usedto compute word meaning in context.4 Experimental SetupData All our experiments used a dependencyparsed and lemmatized version of the British Na-tional Corpus (BNC).
As mentioned in Section 3, weobtained dependencies from the output of the Stan-ford parser (de Marneffe and Manning, 2008).
TheBNC comprises 4,049 texts totalling approximately100 million words.Evaluation Tasks We evaluated our model onword similarity and association.
Both tasks are em-ployed routinely to assess how well semantic modelspredict human judgments of word relatedness.
Weused the WordSim353 test collection (Finkelstein etal., 2002) which consists of similarity judgments forword pairs.
Participants gave each pair a similar-ity rating using a 0 to 10 scale (e.g., tiger?cat arevery similar, whereas delay?racism are not).
Theaverage rating for each pair represents an estimate ofthe perceived similarity of the two words.
The col-lection contains ratings for 437 unique words (353pairs) all of which appeared in our corpus.
Word as-sociation is a slightly different task: Participants aregiven a cue word (e.g., rice) and asked to name anassociate in response (e.g., Chinese, wedding, food,white).
We used the norms collected by Nelson etal.
(1998).
We estimated the strength of associationbetween a cue and its associate, as the relative fre-quency with which it was named.
The norms con-tain 9,968 unique words (70,739 pairs) out of which9,862 were found in our corpus, excluding multi-word expressions.For both tasks, we used correlation analysis to ex-amine the degree of linear relationship between hu-man ratings and model similarity values.
We reportcorrelation coefficients using Spearman?s rank cor-relation coefficient.Quantum Model Parameters The quantumframework presented in Section 3 is quite flexible.Depending on the choice of dependency rela-tions Rel, dependency clusters RC j, and complex853values ?
j = ei?
j , different classes of models can bederived.
To explore these parameters, we partitionedthe WordSim353 dataset and Nelson et als (1998)norms into a development and test set followinga 70?30 split.
We tested 9 different intuitivelychosen relation partitions {RC1, ...,RCnPart}, cre-ating models that considered only neighboringheads, models that considered only neighboringdependents, and models that considered both.
Forthe latter two we experimented with partitions ofone, two or three clusters.
In addition to these morecoarse grained clusters, for models that includedboth heads and dependents we explored a partitionwith twelve clusters broadly corresponding toobjects, subjects, modifiers, auxiliaries, determinersand so on.
In all cases stopwords were not takeninto account in the construction of the semanticspace.For each model variant we performed a gridsearch over the possible phases ?
j = kpi with rangek = 04 ,14 , ...,74 for the complex-valued ?
j assignedto the respective relation cluster RC j (see Section3.2 for details).
In general, we observed that thechoice of dependency relations and their clusteringas well as the phases assigned to each cluster greatlyinfluenced the semantic space.
On both tasks, thebest performing model had the relation partition de-scribed in Section 3.1.
Section 5 reports our resultson the test set using this model.Comparison Models We compared our quantumspace against three classical distributional models.These include a simple semantic space, where aword?s meaning is a vector of co-occurrences withneighboring words (Mitchell and Lapata, 2010), asyntax-aware space based on weighted distributionaltriples that encode typed co-occurrence relationsamong words (Baroni and Lenci, 2010) and wordembeddings computed with a neural language model(Bengio, 2001; Collobert and Weston, 2008) For allthree models we used parameters that have been re-ported in the literature as optimal.Specifically, for the simple co-occurrence-basedspace we follow the settings of Mitchell and Lapata(2010): a context window of five words on eitherside of the target word and 2,000 vector dimensions(i.e., the 2000 most common context words in theBNC).
Vector components were set to the ratio ofthe probability of the context word given the targetword to the probability of the context word overall.For the neural language model, we adopted the bestModels WordSim353 Nelson NormsSDS 0.433 0.151DM 0.318 0.123NLM 0.196 0.091QM 0.535 0.185Table 1: Performance of distributional models on Word-Sim353 dataset and Nelson et als (1998) norms (testset).
Correlation coefficients are all statistically signifi-cant (p < 0.01).performing parameters from our earlier comparisonof different vector sources for distributional seman-tics (Blacoe and Lapata, 2012) where we also usedthe BNC for training.
There we obtained best resultswith 50 dimensions, a context window of size 4,and an embedding learning rate of 10?9.
Our thirdcomparison model uses Baroni and Lenci?s (2010)third-order tensor2 which they obtained from a verylarge dependency-parsed corpus containing approxi-mately 2.3 billion words.
Their tensor assigns a mu-tual information score to instances of word pairs w,vand a linking word l. We obtained vectors ?
?w fromthe tensor following the methodology proposed inBlacoe and Lapata (2012) using 100 (l,v) contextsas dimensions.5 ResultsOur results are summarized in Table 1.
As canbe seen, the quantum model (QM) obtains perfor-mance superior to other better-known models suchas Mitchell and Lapata?s (2010) simple semanticspace (SDS), Baroni and Lenci?s (2010) distribu-tional memory tensor (DM), and Collobert and We-ston?s (2008) neural language model (NLM).
Ourresults on the association norms are comparable tothe state of the art (Silberer and Lapata, 2012; Grif-fiths et al 2007).
With regard to WordSim353,Huang et al(2012) report correlations in the rangeof 0.713?0.769, however they use Wikipedia as atraining corpus and a more sophisticated version ofthe NLM presented here, that takes into accountglobal context and performs word sense discrimi-nation.
In the future, we also plan to evaluate ourmodel on larger Wikipedia-scale corpora.
We wouldalso like to model semantic composition as our ap-proach can do this easily by taking advantage of thenotion of quantum measurement.
Specifically, we2Available at http://clic.cimec.unitn.it/dm/.854Models bar orderSDS pub, snack, restau-rant, grill, coctailform, direct, proce-dure, plan, requestDM counter, rack, strip,pipe, codecourt, demand, form,law, listNLM room, pole, drink,rail, coctaildirect, command,plan, court, demandQM prison, liquor, beer,club, graphorganization, food,law, structure,regulationHS drink, beer, stool, al-cohol, grillfood, form, law, heat,courtTable 2: Associates for bar and order ranked according tosimilarity.
Underlined associates overlap with the humanresponses (HS).can work out the meaning of a dependency tree bymeasuring the meaning of its heads in the context oftheir dependents.Table 2 shows the five most similar associates (or-dered from high to low) for the cues bar and orderfor the quantum model and the comparison models.We also show the human responses (HS) accordingto Nelson et als (1998) norms.
The associates gen-erated by the quantum model correspond to severaldifferent meanings correlated with the target.
Forexample, prison refers to the ?behind bars?
senseof bar, liquor and beer refer to what is consumedor served in bars, club refers to the entertainmentfunction of bars, whereas graph refers to how datais displayed in a chart.6 Related WorkWithin cognitive science the formal apparatus ofquantum theory has been used to formulate modelsof cognition that are superior to those based on tra-ditional probability theory.
For example, conjunc-tion fallacies3 (Tversky and Kahneman, 1983) havebeen explained by making reference to quantum the-ory?s context dependence of the probability assess-ment.
Violations of the sure-thing principle4 (Tver-sky and Shafir, 1992) have been modeled in terms ofa quantum interference effect.
And the asymmetryof similarity relations has been explained by pos-tulating that different concepts correspond to sub-spaces of different dimensionality (Pothos and Buse-meyer, 2012).
Several approaches have drawn on3A conjunction fallacy occurs when it is assumed that spe-cific conditions are more probable than a single general one.4The principle is the expectation that human behavior oughtto conform to the law of total probabilityquantum theory in order to model semantic phe-nomena such as concept combination (Bruza andCole, 2005), the emergence of new concepts (Aertsand Gabora, 2005), and the human mental lexicon(Bruza et al 2009).
Chen (2002) captures syllo-gisms in a quantum theoretic framework; the modeltakes statements like All whales are mammals andall mammals are animals as input and outputs con-clusions like All whales are animals.The first attempts to connect the mathematicalbasis of semantic space models with quantum the-ory are due to Aerts and Czachor (2004) and Bruzaand Cole (2005).
They respectively demonstratethat Latent Semantic Analysis (Landauer and Du-mais, 1997) and the Hyperspace Analog to Lan-guage model (Lund and Burgess, 1996) are essen-tially Hilbert space formalisms, without, however,providing concrete ways of building these modelsbeyond a few hand-picked examples.
Interestingly,Bruza and Cole (2005) show how lexical operatorsmay be contrived from corpus co-occurrence counts,albeit admitting to the fact that their operators do notprovide sensical eigenkets, most likely because ofthe simplified method of populating the matrix fromcorpus statistics.
Grefenstette et al(2011) present amodel for capturing semantic composition in a quan-tum theoretical context, although it appears to bereducible to the classical probabilistic paradigm.
Itdoes not make use of the unique aspects of quantumtheory (e.g., entanglement, interference, or quantumcollapse).Our own work follows Aerts and Czachor (2004)and Bruza and Cole (2005) in formulating a modelthat exhibits important aspects of quantum theory.Contrary to them, we present a fully-fledged seman-tic space rather than a proof-of-concept.
We obtainquantum states (i.e., lexical representations) for eachword by taking its syntactic context into account.Quantum states are expressed as density operatorsrather than kets.
While a ket can only capture onepure state of a system, a density operator containsan ensemble of pure states which we argue is advan-tageous from a modeling perspective.
Within thisframework, not only can we compute the meaning ofindividual words but also phrases or sentences, with-out postulating any additional operations.
Compo-sitional meaning reduces to quantum measurementat each inner node of the (dependency) parse of thestructure in question.855ReferencesDiederik Aerts and Marek Czachor.
2004.
Quantum as-pects of semantic analysis and symbolic artificial intel-ligence.
Journal of PhysicsA: Mathematical and Gen-eral, 37:123?132.Diederik Aerts and Liane Gabora.
2005.
A state-context-property model of concepts and their combinationsii: A hilbert space representation.
Kybernetes, 1?2(34):192?221.Diederik Aerts.
2009.
Quantum structure in cognition.Journal of Mathematical Psychology, 53:314?348.Belal E. Baaquie.
2004.
Quantum Finance: Path In-tegrals and Hamiltonians for Oprions and InterestRates.
Cambridge University Press, Cambridge.Marco Baroni and Alessandro Lenci.
2010.
Distribu-tional memory: A general framework for corpus-basedsemantics.
Computational Linguistics, 36(4):673?721.Yoshua Bengio.
2001.
Neural net language models.Scholarpedia, 3(1):3881.William Blacoe and Mirella Lapata.
2012.
A compari-son of vector-based representations for semantic com-position.
In Proceedings of the 2012 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 546?556, Jeju Island, Korea, July.
Associationfor Computational Linguistics.Peter D. Bruza and Richard J. Cole.
2005.
Quantumlogic of semantic space: An exploratory investigationof context effects in practical reasoning.
In S. Arte-mov, H. Barringer, S. A. d?Avila Garcez, L. C. Lamb,and J.
Woods, editors, We Will Show Them: Essaysin Honour of Dov Gabbay, volume 1, pages 339?361.London: College Publications.Peter D. Bruza, Kirsty Kitto, Douglas McEnvoy, andCathy McEnvoy.
2008.
Entangling words and mean-ing.
In Second Quantum Interaction Symposium.
Ox-ford University.Peter D. Bruza, Kirsty Kitto, Douglas Nelson, and CathyMcEvoy.
2009.
Is there something quantum-like inthe human mental lexicon?
Journal of MathematicalPsychology, 53(5):362?377.Jerome R. Busemeyer and Peter D. Bruza.
2012.
Quan-tum Models of Cognition and Decision.
CambridgeUniversity Press, Cambridge.Joseph C. H. Chen.
2002.
Quantum computation andnatural language processing.
Ph.D. thesis, Universita?tHamburg.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: deep neuralnetworks with multita sk learning.
In Proceedings ofthe 25th International Conference on Machine Learn-ing, pages 160?167, New York, NY.
ACM.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependencies repre-sentation.
In Coling 2008: Proceedings of the work-shop on Cross-Framework and Cross-Domain ParserEvaluation, pages 1?8, Manchester, UK.Guy Denhie`re and Beno?
?t Lemaire.
2004.
A compu-tational model of children?s semantic memory.
InProceedings of the 26th Annual Meeting of the Cog-nitive Science Society, pages 297?302, Mahwah, NJ.Lawrence Erlbaum Associates.Paul A. M. Dirac.
1939.
A new notation for quantum me-chanics.
Mathematical Proceedings of the CambridgePhilosophical Society, 35:416?418.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2002.
Placing Search in Context: The Con-cept Revisited.
ACMTransactions on Information Sys-tems, 20(1):116?131, January.John R. Firth.
1957.
A synopsis of linguistic theory1930-1955.
In Studies in Linguistic Analysis, pages1?32.
Philological Society, Oxford.Edward Grefenstette, Mehrnoosh Sadrzadeh, StephenClark, Bob Coecke, and Stephen Pulman.
2011.Concrete sentence spaces for compositional distribu-tional models of meaning.
Proceedings of the 9th In-ternational Conference on Computational Semantics(IWCS11), pages 125?134.Gregory Grefenstette.
1994.
Explorations in AutomaticThesaurus Discovery.
Kluwer Academic Publishers.Thomas L. Griffiths, Mark Steyvers, and Joshua B.Tenenbaum.
2007.
Topics in semantic representation.Psychological Review, 114(2):211?244.Eric Huang, Richard Socher, Christopher Manning, andAndrew Ng.
2012.
Improving word representationsvia global context and multiple word prototypes.
InProceedings of the 50th Annual Meeting of the Associ-ation for Computational Linguistics (Volume 1: LongPapers), pages 873?882, Jeju Island, Korea, July.
As-sociation for Computational Linguistics.R.
I. G. Hughes.
1989.
The Structure and Interpreta-tion of Quantum Mechnics.
Harvard University Press,Cambridge, MA.Chris J. Isham.
1989.
Lectures on Quantum Theory.
Sin-gapore: World Scientific.Andrei Y. Khrennikov.
2010.
Ubiquitous QuantumStructure: From Psychology to Finance.
Springer.Daniel Kleppner and Roman Jackiw.
2000.
One hundredyears of quantum physics.
Science, 289(5481):893?898.Darrell R. Laham.
2000.
Automated Content Assess-ment of Text Using Latent Semantic Analysis to Sim-ulate Human Cognition.
Ph.D. thesis, University ofColorado at Boulder.856Thomas K. Landauer and Susan T. Dumais.
1997.
A so-lution to Plato?s problem: the latent semantic analysistheory of acquisition, induction and representation ofknowledge.
Psychological Review, 104(2):211?240.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the joint AnnualMeeting of the Association for Computational Linguis-tics and International Conference on ComputationalLinguistics, pages 768?774, Montre?al, Canada.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instruments& Computers, 28:203?208.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,38(8):1388?1429.Douglas L. Nelson, Cathy L. McEvoy, and Thomas A.Schreiber.
1998.
The University of South FloridaWord Association, Rhyme, and Word FragmentNorms.Michael A. Nielsen and Isaac L. Chuang.
2010.
Quan-tum Computation and Information Theory.
CambridgeUniversity Press, Cambridge.Daniel Osherson and Edward E. Smith.
1981.
On theadequacy of prototype theory as a theory of concepts.Cognition, 9:35?38.Emmanuel M. Pothos and Jerome R. Busemeyer.
2012.Can quantum probability provide a new direction forcognitive modeling?
Behavioral and Brain Sciences.to appear.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?124.Carina Silberer and Mirella Lapata.
2012.
Groundedmodels of semantic representation.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1423?1433, JejuIsland, Korea.
Association for Computational Linguis-tics.Richard Socher, Eric H. Huang, Jeffrey Pennin, An-drew Y. Ng, and Christopher D. Manning.
2011.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In J. Shawe-Taylor, R.S.Zemel, P. Bartlett, F.C.N.
Pereira, and K.Q.
Wein-berger, editors, Advances in Neural Information Pro-cessing Systems 24, pages 801?809.Amos Tversky and Daniel Kahneman.
1983.
Exten-sional versus intuitive reasoning: The conjuctive fal-lacy in probability judgment.
Psychological Review,4(90):293?315.Amos Tversky and Eldar Shafir.
1992.
The disjunctioneffect in choice under uncertainty.
Psychological Sci-ence, 3(5):305?309.Vlatko Vedral.
2006.
Introduction to Quantum Informa-tion Science.
Oxford University Press, New York.857
