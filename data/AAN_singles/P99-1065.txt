A Statistical Parser for Czech*Michael CollinsAT&T Labs-Research,Shannon Laboratory,180 Park Avenue,Florham Park, NJ 07932mcol l ins@research ,  a t t .
comJan Haj i~.Institute of Formal and Applied LinguisticsCharles University,Prague, Czech Republicha j i c@ufa l .mf f ,  cun i .
czLance RamshawBBN Technologies,70 Fawcett St.,Cambridge, MA 02138i r amshaw@bbn,  c omChristoph TillmannLehrstuhl ftir Informatik VI,RWTH AachenD-52056 Aachen, Germanyt i l lmann@informat ik ,  rwth -aachen,  deAbstractThis paper considers tatistical parsing of Czech,which differs radically from English in at least worespects: (1) it is a highly inflected language, and(2) it has relatively free word order.
These dif-ferences are likely to pose new problems for tech-niques that have been developed on English.
Wedescribe our experience in building on the parsingmodel of (Collins 97).
Our final results - 80% de-pendency accuracy - represent good progress to-wards the 91% accuracy of the parser on English(Wall Street Journal) text.1 IntroductionMuch of the recent research on statistical parsinghas focused on English; languages other than En-glish are likely to pose new problems for statisti-cal methods.
This paper considers tatistical pars-ing of Czech, using the Prague Dependency Tree-bank (PDT) (Haji~, 1998) as a source of training andtest data (the PDT contains around 480,000 wordsof general news, business news, and science articles* This material is based upon work supported by the NationalScience Foundation under Grant No.
(#IIS-9732388), and wascarded out at the 1998 Workshop on Language Engineering,Center for Language and Speech Processing, Johns HopkinsUniversity.
Any opinions, findings, and conclusions orrecom-mendations expressed in this material are those of the authorsand do not necessarily reflect the views of the National Sci-ence Foundation orThe Johns Hopkins University.
The projecthas also had support at various levels from the following rantsand programs: Grant Agency of the Czech Republic grants No.405/96/0198 and 405/96/K214 and Ministry of Education ofthe Czech Republic Project No.
VS96151.
We would also liketo thank Eric Brill, Barbora Hladk~i, Frederick Jelinek, DougJones, Cynthia Kuo, Oren Schwartz, and Daniel Zeman formany useful discussions during and after the workshop.annotated for dependency structure).
Czech differsradically from English in at least wo respects:?
It is a highly inflected (HI) language.
Wordsin Czech can inflect for a number of syntac-tic features: case, number, gender, negationand so on.
This leads to a very large numberof possible word forms, and consequent sparsedata problems when parameters are associatedwith lexical items, on  the positive side, inflec-tional information should provide strong cuesto parse structure; an important question ishowto parameterize a statistical parsing model in away that makes good use of inflectional infor-mation.?
It has relatively free word order (F-WO).
Forexample, a subject-verb-object triple in Czechcan generally appear in all 6 possible surfaceorders (SVO, SOV, VSO etc.
).Other Slavic languages ( uch as Polish, Russian,Slovak, Slovene, Serbo-croatian, Ukrainian) alsoshow these characteristics.
Many European lan-guages exhibit FWO and HI phenomena to a lesserextent.
Thus the techniques and results found forCzech should be relevant to parsing several otherlanguages.This paper first describes a baseline approach,based on the parsing model of (Collins 97), whichrecovers dependencies with 72% accuracy.
We thendescribe a series of refinements o the model, giv-ing an improvement to 80% accuracy, with around82% accuracy on newspaper/business articles.
(Asa point of comparison, the parser achieves 91% de-pendency accuracy on English (Wall Street Journal)text.
)5052 Data and EvaluationThe Prague Dependency Treebank PDT (Haji~,1998) has been modeled after the Penn Treebank(Marcus et al 93), with one important excep-tion: following the Praguian linguistic tradition,the syntactic annotation is based on dependenciesrather than phrase structures.
Thus instead of "non-terminal" symbols used at the non-leaves of the tree,the PDT uses so-called analytical functions captur-ing the type of relation between a dependent andits governing node.
Thus the number of nodes isequal to the number of tokens (words + punctuation)plus one (an artificial root node with rather techni-cal function is added to each sentence).
The PDTcontains also a traditional morpho-syntactic anno-tation (tags) at each word position (together with alemma, uniquely representing the underlying lexicaiunit).
As Czech is a HI language, the size of the setof possible tags is unusually high: more than 3,000tags may be assigned by the Czech morphologicalanalyzer.
The PDT also contains machine-assignedtags and lemmas for each word (using a tagger de-scribed in (Haji~ and Hladka, 1998)).For evaluation purposes, the PDT has been di-vided into a training set (19k sentences) and a de-velopment/evaluation estset pair (about 3,500 sen-tences each).
Parsing accuracy is defined as the ratioof correct dependency links vs. the total number ofdependency links in a sentence (which equals, withthe one artificial root node added, to the number oftokens in a sentence).
As usual, with the develop-ment est set being available during the developmentphase, all final results has been obtained on the eval-uation test set, which nobody could see beforehand.3 A Sketch of the Parsing ModelThe parsing model builds on Model 1 of (Collins97); this section briefly describes the model.
Theparser uses a lexicalized grammar - -  each non-terminal has an associated head-word and part-of-speech (POS).
We write non-terminals a X (x): Xis the non-terminal label, and x is a (w, t> pair wherew is the associated head-word, and t as the POS tag.See figure 1 for an example lexicalized tree, and alist of the lexicalized rules that it contains.Each rule has the form 1 :P(h) --+ L,~(l,)...Ll(ll)H(h)Rl(rl)...Rm(rm)(1)IWith the exception of the top rule in the tree, which has thef0rmTOP -+ H(h).H is the head-child of the phrase, which inher-its the head-word h from its parent P. L1...Lnand R1...Rm are left and right modifiers ofH.
Either n or m may be zero, and n =m = 0 for unary rules.
For example,in S (bought,VBD) -+ NP (yesterday,NN)NP (IBM, NNP) VP (bought, VBD) :n=2 m=0P=S H=VPLI = NP L2 = NPl I = <IBM, NNP> 12 = <yesterday, NN>h = <bought ,  VBD)The model can be considered to be a variantof Probabilistic Context-Free Grammar (PCFG).
InPCFGs each role cr --+ fl in the CFG underlyingthe PCFG has an associated probability P(/3la ).In (Collins 97), P(/~lo~) is defined as a product ofterms, by assuming that the right-hand-side of therule is generated in three steps:1.
Generate the head constituent label of thephrase, with probability 79H( H I P, h ).2.
Generate modifiers to the left of the head withprobability Hi=X..n+l 79L(Li(li) \[ P, h, H),where Ln+l(ln+l) = STOP.
The STOPsymbol is added to the vocabulary of non-terminals, and the model stops generating leftmodifiers when it is generated.3.
Generate modifiers to the right of the head withprobability Hi=l..m+l PR(Ri(ri) \[ P, h, H).Rm+l (rm+l) is defined as STOP.For example, the probability of s (bought ,  VBD)-> NP(yesterday,NN) NP(IBM,NNP)VP (bought, VBD) is defined as/oh (VP I S, bought, VBD) ?Pt (NP ( IBM, NNP) I S, VP, bought, VBD) xPt(NP (yesterday, NN) I S ,VP, bought ,VBD) ?e~ (STOP I s, vP, bought, VBD) ?Pr (STOP I S, VP, bought .
VBD)Other rules in the tree contribute similar sets ofprobabilities.
The probability for the entire tree iscalculated as the product of all these terms.
(Collins 97) describes a series of refinements othis basic model: the addition of "distance" (a con-ditioning feature indicating whether or not a mod-ifier is adjacent to the head); the addition of sub-categorization parameters (Model 2), and parame-ters that model wh-movement (Model 3); estimation506TOPIS(bought,VBD)NP(yesterday,NN) NP(IBM,NNP)I INN NNPI Iyesterday IBMTOPS(bought,VBD)NP(yesterday,NN)NP(IBM,NNP)VP(bought,VBD)NP(Lotus,NNP)-> S(bought,VBD)-> NP(yesterday,NN)-> NN(yesterday)-> NNP(IBM)-> VBD(bought)-> NNP(Lotus)VP(bought,VBD)VBD NP(Lotus,NNP)I Ibought NNPILotusNP(IBM,NNP) VP(bought,VBD)NP(Lotus,NNP)Figure 1: A lexicalized parse tree, and a list of the rules it contains.techniques that smooth various levels of back-off (inparticular using POS tags as word-classes, allow-ing the model to learn generalizations about POSclasses of words).
Search for the highest probabil-ity tree for a sentence is achieved using a CKY-styleparsing algorithm.4 Parsing the Czech PDTMany statistical parsing methods developed for En-glish use lexicalized trees as a representation (e.g.,(Jelinek et al 94; Magerman 95; Ratnaparkhi 97;Charniak 97; Collins 96; Collins 97)); several (e.g.,(Eisner 96; Collins 96; Collins 97; Charniak 97))emphasize the use of parameters associated withdependencies between pairs of words.
The CzechPDT contains dependency annotations, but no treestructures.
For parsing Czech we considered a strat-egy of converting dependency structures in trainingdata to lexicalized trees, then running the parsingalgorithms originally developed for English.
A keypoint is that the mapping from lexicalized trees todependency structures i many-to-one.
As an exam-ple, figure 2 shows an input dependency structure,and three different lexicalized trees with this depen-dency structure.The choice of tree structure iscrucial in determin-ing the independence assumptions that the parsingmodel makes.
There are at least 3 degrees of free-dom when deciding on the tree structures:.
How "fiat" should the trees be?
The trees couldbe as fiat as possible (as in figure 2(a)), or bi-nary branching (as in trees (b) or (c)), or some-where between these two extremes.2.
What non-terminal labels should the internalnodes have?3.
What set of POS tags should be used?4.1 A Baseline ApproachTo provide a baseline result we implemented what isprobably the simplest possible conversion scheme:...The trees were as fiat as possible, as in fig-ure 2(a).The non-terminal labels were "XP", where Xis the first letter of the POS tag of the head-word for the constituent.
See figure 3 for anexample.The part of speech tags were the major cate-gory for each word (the first letter of the CzechPOS set, which corresponds tobroad categorydistinctions such as verb, noun etc.
).The baseline approach gave a result of 71.9% accu-racy on the development test set.507Input:sentence with part of speech tags: UN saw/V the/D man/N (N=noun, V=verb, D=determiner)dependencies (word ~ Parent): (I =~ saw), (saw =:~ START), (the =~ man), (man =?, saw>Output: a lexicalized tree(a) X(saw) (b) X(saw) (c)N X(saw)X(I) V X(man) I\[ I ~ I V X(man)N saw D N \[\[ I I saw D NI the man \[ \[the manX(saw)X(saw) X(man)N V D NI I I II saw the manFigure 2: Converting dependency structures to lexicalized trees with equivalent dependencies.
The trees(a), (b) and (c) all have the input dependency structure: (a) is the "flattest" possible tree; (b) and (c) arebinary branching structures.
Any labels for the non-terminals (marked X) would preserve the dependencystructure.VP(saw)NP(I) V NP(man)N saw D NI I II the manFigure 3: The baseline approach for non-terminallabels.
Each label is XP, where X is the POS tag forthe head-word of the constituent.
'4.2 Modifications to the Baseline TreesWhile the baseline approach is reasonably success-ful, there are some linguistic phenomena that leadto clear problems.
This section describes some treetransformations that are linguistically motivated,and lead to improvements in parsing accuracy.4.2.1 Relative ClausesIn the PDT the verb is taken to be the head of bothsentences and relative clauses.
Figure 4 illustrateshow the baseline transformation method can lead toparsing errors in relative clause cases.
Figure 4(c)shows the solution to the problem: the label of therelative clause is changed to SBAR, and an addi-tional vP level is added to the right of the relativepronoun.
Similar transformations were applied forrelative clauses involving Wh-PPs (e.g., "the manto whom I gave a book"), Wh-NPs (e.g., "the manwhose book I read") and Wh-Adverbials (e.g., "theplace where I live").4.2.2 CoordinationThe PDT takes the conjunct to be the head of coor-dination structures (for example, and would be thehead of the NP dogs and cats).
In these cases thebaseline approach gives tree structures such as thatin figure 5(a).
The non-terminal label for the phraseis JP  (because the head of the phrase, the conjunctand, is tagged as J).This choice of non-terminal is problematic fortwo reasons: (1) the JP  label is assigned to all co-ordinated phrases, for example hiding the fact thatthe constituent in figure 5(a) is an NP; (2) the modelassumes that left and right modifiers are generatedindependently of each other, and as it stands willgive unreasonably high probability to two unlikephrases being coordinated.
To fix these problems,the non-terminal label in coordination cases was al-tered to be the same as that of the second conjunct(the phrase directly to the right of the head of thephrase).
See figure 5.
A similar transformation wasmade for cases where a comma was the head of aphrase.4.2.3 PunctuationFigure 6 shows an additional change concerningcommas.
This change increases the sensitivity ofthe model to punctuation.4.3 Model AlterationsThis section describes some modifications tothe pa-rameterization f the model.508(a) VPNP V NPJohn likesMary VPZ P V NPI I \[ Iwho likes Tim(b) VPVP Z VPNP V NP P V NPI I t I I IJohn likes Mary who likes Tima) JP(a) b) NP(a)NP(hl) J NP(h 2) NP(hl) J NP(h 2)I I i I I Iand .
.
.
.
.
.
and ...Figure 5: An example of coordination.
The base-line approach (a) labels the phrase as a Jp;  the re-finement (b) takes the second conjunct's label as thenon-terminal for the whole phrase.NP(h) --t- NPX(h)Z(,) ~ N(h) ~ Z(,) NP(h)I ... h ""I r~(h) I.., ihFigure 6: An additional change, triggered by acomma that is the left-most child of a phrase: a newnon-terminal NPX is introduced.
(c) vPNP V NPJohn likesMary SBARZ P VPwho V NPI Ilikes TimFigure 4: (a) The baseline approach does not distin-guish main clauses from relative clauses: both havea verb as the head, so both are labeled VP.
(b) A typ-ical parsing error due to relative and main clausesnot being distinguished.
(note that two main clausescan be coordinated by a comma, as in John likesMary, Mary likes Tim).
(c) The solution to the prob-lem: a modification to relative clause structures intraining data.4.3.1 Preferences for dependencies that do notcross verbsThe model of (Collins 97) had conditioning vari-ables that allowed the model to learn a preferencefor dependencies which do not cross verbs.
Fromthe results in table 3, adding this condition improvedaccuracy by about 0.9% on the development set.4.3.2 Punctuation for phrasal boundariesThe parser of (Collins 96) used punctuation as an in-dication of phrasal boundaries.
It was found that if aconstituent Z ~ (...XY...) has two children X andY separated by a punctuation mark, then Y is gen-erally followed by a punctuation mark or the end ofsentence marker.
The parsers of (Collins 96,97) en-coded this as a hard constraint.
In the Czech parserwe added a cost of -2.5 (log probability) z to struc-tures that violated this constraint.4.3.3 First-Order (Bigram) DependenciesThe model of section 3 made the assumption thatmodifiers are generated independently of each other.This section describes a bigram model, where thecontext is increased to consider the previously gen-erated modifier ((Eisner 96) also describes use ofbigram statistics).
The right-hand-side of a rule isnow assumed to be generated in the following threestep process:1.
Generate the head label, with probability~'~ (H I P, h)2.
Generate l ft modifiers with probability1-I Pc(L~(li) l Li-I'P'h'H)/= l .
.n+lwhere L0 is defined as a special NULL sym-bol.
Thus the previous modifier, Li-1, isadded to the conditioning context (in the pre-vious model the left modifiers had probability1"\[i=1..,~+1 Pc(Li(li) I P,h,H).)3.
Generate fight modifiers using a similar bi-gram process.Introducing bigram-dependencies into the parsingmodel improved parsing accuracy by about 0.9 %(as shown in Table 3).2Th is  va lue  was  opt imized  on  the  deve lopment  set5091.
main part of 8. personspeech2.
detailed part of 9. tensespeech3.
gender 10. degree of compar-ison4.
number I I. negativeness5.
case 12. voice6.
possessor's 13. variant/registergender7.
possessor's num-berTable 1: The 13-character ncoding of the CzechPOS tags.4.4 Alternative Part-of-Speech TagsetsPart of speech (POS) tags serve an important rolein statistical parsing by providing the model with alevel of generalization as to how classes of wordstend to behave, what roles they play in sentences,and what other classes they tend to combine with.Statistical parsers of English typically make use ofthe roughly 50 POS tags used in the Penn Treebankcorpus, but the Czech PDT corpus provides a muchricher set of POS tags, with over 3000 possible tagsdefined by the tagging system and over 1000 tagsactually found in the corpus.
Using that large atagset with a training corpus of only 19,000 sen-tences would lead to serious sparse data problems.It is also clear that some of the distinctions beingmade by the tags are more important than othersfor parsing.
We therefore xplored ifferent waysof extracting smaller but still maximally informativePOS tagsets.4.4.1 Description of the Czech TagsetThe POS tags in the Czech PDT corpus (Haji~ andHladk~i, 1997) are encoded in 13-character strings.Table 1 shows the role of each character.
For exam-ple, the tag NNMP1 .
.
.
.
.
A - -  would be used for aword that had "noun" as both its main and detailedpart of speech, that was masculine, plural, nomina-tive (case 1), and whose negativeness value was "af-firmative".Within the corpus, each word was annotated withall of the POS tags that would be possible given itsspelling, using the output of a morphological naly-sis program, and also with the single one of thosetags that a statistical POS tagging program hadpredicted to be the correct ag (Haji~ and Hladka,1998).
Table 2 shows a phrase from the corpus, withForm Dictionary Tags Machine Tagposlanci NNMPI  .
.
.
.
.
A -  -NNMP5 .
.
.
.
.
ANNMP7 .
.
.
.
.
A.NNMS3 .
.
.
.
.
A.NNMS6 .
.
.
.
.
A.NNMPI  .
.
.
.
.
A.Par lamentu  NNIS2  .
.
.
.
.
A - -  NNIS2  .
.
.
.
.
ANNIS3  .
.
.
.
.
A.NNIS6  .
.
.
.
.
A - Ischv~ilili VpMP-  - -XR-AA-  VpMP-  - -XR-AA-Table 2: Corpus POS tags for "the representativesof the Parliament approved".the alternative possible tags and machine-selectedtag for each word.
In the training portion of the cor-pus, the correct ag as judged by human annotatorswas also provided.4.4.2 Selection of a More Informative TagsetIn the baseline approach, the first letter, or "mainpart of speech", of the full POS strings was used asthe tag.
This resulted in a tagset with 13 possiblevalues.A number of alternative, richer tagsets were ex-plored, using various combinations of character po-sitions from the tag string.
The most successful al-ternative was a two-letter tag whose first letter wasalways the main POS, and whose second letter wasthe case field if the main POS was one that dis-plays case, while otherwise the second letter wasthe detailed POS.
(The detailed POS was used forthe main POS values D, J, V, and X; the case fieldwas used for the other possible main POS values.
)This two-letter scheme resulted in 58 tags, and pro-vided about a 1.1% parsing improvement over thebaseline on the development set.Even richer tagsets that also included the per-son, gender, and number values were tested withoutyielding any further improvement, presumably be-cause the damage from sparse data outweighed thevalue of the additional information present.4.4.3 Explorations toward Clustered TagsetsAn entirely different approach, rather than search-ing by hand for effective tagsets, would be to useclustering to derive them automatically.
We ex-plored two different methods, bottom-up and top-down, for automatically deriving POS tag sets basedon counts of governing and dependent tags extractedfrom the parse trees that the parser constructs fromthe training data.
Neither tested approach resultedin any improvement in parsing performance com-510pared to the hand-designed "two letter" tagset, butthe implementations of each were still only prelim-inary, and a clustered tagset more adroitly derivedmight do better.4.4.4 Dealing with Tag AmbiguityOne final issue regarding POS tags was how to dealwith the ambiguity between possible tags, both intraining and test.
In the training data, there was achoice between using the output of the POS taggeror the human annotator's judgment as to the correcttag.
In test data, the correct answer was not avail-able, but the POS tagger output could be used if de-sired.
This turns out to matter only for unknownwords, as the parser is designed to do its own tag-ging, for words that it has seen in training at least5 times, ignoring any tag supplied with the input.For "unknown" words (seen less than 5 times), theparser can be set either to believe the tag suppliedby the POS tagger or to allow equally any of thedictionary-derived possible tags for the word, effec-tively allowing the parse context o make the choice.
(Note that the rich inflectional morphology of Czechleads to a higher ate of"unknown" word forms thanwould be true in English; in one test, 29.5% of thewords in test data were "unknown".
)Our tests indicated that if unknown words aretreated by believing the POS tagger's uggestion,then scores are better if the parser is also trainedon the POS tagger's uggestions, rather than on thehuman annotator's correct ags.
Training on the cor-rect tags results in 1% worse performance.
Eventhough the POS tagger's tags are less accurate, theyare more like what the parser will be using in the testdata, and that turns out to be the key point.
On theother hand, if the parser allows all possible dictio-nary tags for unknown words in test material, thenit pays to train on the actual correct ags.In initial tests, this combination of training on thecorrect ags and allowing all dictionary tags for un-known test words somewhat outperformed the alter-native of using the POS tagger's predictions both fortraining and for unknown test words.
When testedwith the final version of the parser on the full de-velopment set, those two strategies performed at thesame level.?
5 ResultsWe ran three versions of the parser over the finaltest set: the baseline version, the full model withall additions, and the full model with everything butthe bigram model.
The baseline system on the fi-\[I Modification II ImprovementCoordination +2.6%Relative clauses + 1.5 %Punctuation -0.1% ?
?Enriched POS tags +1.
1%Punctuation +0.4%Verb crossing +0.9%Bigram +0.9%I Total change +7.4%Total Relative Error reduction 26%Table 3: A breakdown of the results on the develop-ment set.GenreNewspaperBusinessScienceProportion(Sentences/Dependencies)50%/44%25%/19%25%/38%Accuracy81.4%81.4%76.0%Table 4: Breakdown of the results by genre.
Notethat although the Science section only contributes25% of the sentences in test data, it contains muchlonger sentences than the other sections and there-fore accounts for 38% of the dependencies in testdata.nal test set achieved 72.3% accuracy.
The final sys-tem achieved 80.0% accuracy 3: a 7.7% absolute im-provement and a 27.8% relative improvement.The development set showed very similar results:a baseline accuracy of 71.9% and a final accuracy of79.3%.
Table 3 shows the relative improvement ofeach component of the model 4.
Table 4 shows theresults on the development set by genre.
It is inter-esting to see that the performance on newswire textis over 2% better than the averaged performance.The Science section of the development set is con-siderably harder to parse (presumably because oflonger sentences and more open vocabulary).3The parser fails to give an analysis on some sentences, be-cause the search space becomes too large.
The baseline systemmissed 5 sentences; the full system missed 21 sentences; thefull system minus bigrams missed 2 sentences.
To score thefull system we took the output from the full system minus bi-grams when the full system produced no output (to prevent aheavy penalty due to the 21 missed sentences).
The remaining2 unparsed sentences (5in the baseline case) had all dependen-cies attached to the root.4We were surprised to see this slight drop in accuracy forthe punctuation tree modification.
Earlier tests on a differentdevelopment set, with less training data and fewer other modelalterations had shown a good improvement for this feature.5115.1 Comparison to Previous ResultsThe main piece of previous work on parsing Czechthat we are aware of is described in (Kubofi 99).This is a rule-based system which is based on a man-ually designed set of rules.
The system's accuracyis not evaluated on a test corpus, so it is difficultto compare our results to theirs.
We can, however,make some comparison of the results in this paperto those on parsing English.
(Collins 99) describesresults of 91% accuracy in recovering dependen-cies on section 0 of the Penn Wall Street JournalTreebank, using Model 2 of (Collins 97).
This taskis almost certainly easier for a number of reasons:there was more training data (40,000 sentences asopposed to 19,000); Wall Street Journal may be aneasier domain than the PDT, as a reasonable pro-portion of sentences come from a sub-domain, fi-nancial news, which is relatively restricted.
Unlikemodel 1, model 2 of the parser takes subcategoriza-tion information i to account, which gives some im-provement on English and might well also improveresults on Czech.
Given these differences, it is dif-ficult to make a direct comparison, but the overallconclusion seems to be that the Czech accuracy isapproaching results on English, although it is stillsomewhat behind.6 ConclusionsThe 80% dependency accuracy of the parser epre-sents good progress towards English parsing perfor-mance.
A major area for future work is likely tobe an improved treatment of morphology; a naturalapproach to this problem is to consider more care-fully how POS tags are used as word classes bythe model.
We have begun to investigate this is-sue, through the automatic derivation of POS tagsthrough clustering or "splitting" approaches.
Itmight also be possible to exploit he internal struc-ture of the POS tags, for example through incremen-tal prediction of the POS tag being generated; or toexploit the use of word lemmas, effectively split-ting word-word relations into syntactic dependen-cies (POS tag-POS tag relations) and more seman-tic (lemma-lemma) dependencies.ReferencesE.
Charniak.
1997.
Statistical Parsing with aContext-free Grammar and Word Statistics.
Pro-ceedings of the Fourteenth National Conferenceon Artificial Intelligence, AAAI Press/MIT Press,Menlo Park (1997).M.
Collins.
1996.
A New Statistical Parser Basedon Bigram Lexical Dependencies.
Proceedings of512the 34th Annual Meeting of the Association forComputational Linguistics, pages 184-191.M.
Collins.
1997.
Three Generative, LexicalisedModels for Statistical Parsing.
In Proceedings ofthe 35th Annual Meeting of the Association forComputational Linguistics and 8th Conferenceof the European Chapter of the Association forComputational Linguistics, pages 16-23.M.
Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. Thesis, Uni-versity of Pennsylvania.J.
Eisner.
1996.
Three New Probabilistic Models forDependency Parsing: An Exploration.
Proceed-ings of COLING-96, pages 340-345.Jan Haji6.
1998.
Building a Syntactically Anno-tated Corpus: The Prague Dependency Treebank.Issues of Valency and Meaning (Festschrift forJarmila Panevov~i).
Carolina, Charles University,Prague.
pp.
106-132.Jan Haji~ and Barbora Hladk~i.
1997.
Tagging of In-flective Languages: a Comparison.
In Proceed-ings of the ANLP'97, pages 136--143, Washing-ton, DC.Jan Haji6 and Barbora Hladka.
1998.
Tagging In-flective Languages: Prediction of MorphologicalCategories for a Rich, Structured Tagset.
In Pro-ceedings of ACL/Coling'98, Montreal, Canada,Aug.
5-9, pp.
483-490.E Jelinek, J. Lafferty, D. Magerman, R. Mercer,A.
Ratnaparkhi, S. Roukos.
1994.
Decision TreeParsing using a Hidden Derivation Model.
Pro-ceedings of the 1994 Human Language Technol-ogy Workshop, ages 272-277.V.
Kubofi.
1999.
A Robust Parser for Czech.Technical Report 6/1999, 0FAL, Matematicko-fyzikdlnf akulta Karlovy univerzity, Prague.D.
Magerman.
1995.
Statistical Decision-Tree Mod-els for Parsing.
Proceedings of the 33rd AnnualMeeting of the Association for ComputationalLinguistics, pages 276-283.M.
Marcus, B. Santorini and M. Marcinkiewicz.1993.
Building a Large Annotated Corpus of En-glish: the Penn Treebank.
Computational Lin-guistics, 19(2):313-330.A.
Ratnaparkhi.
1997.
A Linear Observed Time Sta-tistical Parser Based on Maximum Entropy Mod-els.
In Proceedings of the Second Conferenceon Empirical Methods in Natural Language Pro-cessing, Brown University, Providence, RhodeIsland.
