Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 260?265,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsExperiments in Medical Translation Shared Task at WMT 2014Jian Zhang, Xiaofeng Wu,Iacer Calixto, Ali Hosseinzadeh Vahid, Xiaojun Zhang,Andy Way, Qun LiuThe CNGL Centre for Global Intelligent ContentSchool of ComputingDublin City University, Ireland{zhangj,xiaofengwu,icalixto,avahid,xzhang,away,qliu}@computing.dcu.ieAbstractThis paper describes Dublin City Uni-versity?s (DCU) submission to the WMT2014 Medical Summary task.
We re-port our results on the test data set inthe French to English translation direction.We also report statistics collected from thecorpora used to train our translation sys-tem.
We conducted our experiment on theMoses 1.0 phrase-based translation systemframework.
We performed a variety of ex-periments on translation models, reorder-ing models, operation sequence model andlanguage model.
We also experimentedwith data selection and removal the lengthconstraint for phrase-pair extraction.1 System Description1.1 Training Data Statistics and PreparationThe training corpora provided to the medicaltranslation shared task can be divided into 3 cat-egories:Medical in-domain corpora: these corporacontain documents, patents, articles, terminologylists, and titles that are representative of the samemedical domain as the development and test datasets (Table 1, second column).Medical out-of-domain corpora: these cor-pora also contain medical documents, patents, ar-ticles, terminologies lists and titles, but describea different domain from the development and testdata sets (Table 1, third column).General domain corpora: these corpora con-sist of general-domain text (WMT 2014 generaltranslation subtask corpora), and encompass vari-ous domains.
(We did not use these corpora in oursystem).Corpus In-domain Out-of-domainparallel sentence parallel sentencenumber numberEMEA 1,092,568 0COPPA 664,658 2,841,849PatTR-title 408,502 2,096,270PatTR-abstract 688,147 3,009,523PatTR-claims 1,105,230 5,861,621UMLS 85,705 0Wikipedia 8,448 0TOTAL 4,053,258 13,809,263Table 1: WMT 2014 Medical Translation sharedtask parallel training data before preprocessing.Within all the provided training corpora fromWMT 2014, 70.72% of the medical in domainbilingual sentences, and 100% of the medicalout-of-domain bilingual sentences were obtainedfrom patent document collections.
Motivated bythese percentages, we view the WMT 2014 med-ical translation shared task as similar to traininga patent-specific translation system.
The mono-lingual corpora are taken from 9 different cor-pora collections, and there is no clear demarca-tion of the in/out-of-domain boundaries (exceptthe PatTR collection).
Our method of differenti-ating between the in/out-of-domain monolingualcorpora is that only English sentences from thethird column of Table 1, and the patent descrip-tion documents from PatTR collection, are out-of-domain monolingual corpora.
All other English260sentences are treated as an in-domain monolingualresource.A patent document usually comprises title, ab-stract, claims and description fields.
The docu-ments often use its unique formatting and con-tain linguistic idiosyncrasies, which distinguishpatent-specific translation systems from generaltranslation systems, in both training and transla-tion phases (Ceaus?u et al., 2011).
We have alsofound that some common writing styles are con-stantly used, especially for long sentences.
Forexample, a typical patent claim begins withMethod of [X], which comprising:followed by a numbered list.
The abstractfield normally contains one paragraph only, butwith multiple sentences.
Those long sentencesare necessarily filtered out to facilitate efficientword alignment, using a tool such as GIZA++(Och, 2003) word aligner with the default param-eter settings.
However, because statistical ma-chine translation depends on the training data toestimate translation probability, more high qual-ity training data often leads a better translation re-sult.
One possible method of including long sen-tences into the training cycle is to change the wordaligner?s parameter settings to handle longer sen-tences; however, aligning long sentences is timeconsuming.
Our solution is to capture the styledlong sentences and attempt to split them on bothsource and target side simultaneously accordingto the numbered list or sentence boundary indica-tions.
If the sentence number after splitting arematching in both source and target sides, and eachsentence pair is within the token length ratio of3, we assume the split attempt is successful, oth-erwise the sentences are kept unchanged and willbe filtered out eventually.
We applied our splittingattempt approach on the patent documents at thedata preparation step which consequently resultsin 19.35% and 7.1% increase in the number ofsentence pairs compared with the original medicalin-domain (from 4053258 to 4837382) and over-all medical (from 17862521 to 19124142) datasetsrespectively.Another finding from the training corpora is thatthe titles of the patent documents are often capital-ized in the training corpora.
Since we are traininga true-cased translation system, and the transla-tion inputs contain non-title sentences, capitalizedtraining sentences will contribute biased weightsto our true-case model.
We addressed this issue bycreating a lowercase version of the title corpora,then we trained our true-case model with the low-ercased titles corpora and other non-title corpora.We also included the lowercased title corpora inthe translation system training.We tokenized the training corpora using thetokenizer script distributed in the Moses 1.0framework with additional patent document non-breaking preferences observed during data prepa-ration, such as Figs and FIGS etc., and a modifiedaggressive setting (split hyphen character in allcases).
Other data preparation steps included char-acter normalization, character/token based foreignlanguage detection, HTML/XML tag removal,case insensitive duplication removal, longer sen-tence removal (2-80, length ratio 9), resulting inthe preprocessed data shown in Table 2.Corpus In-domain Out-of-domainparallel sentence parallel sentencenumber numberEMEA 273,532 0COPPA 1,374,371 6,075,599PatTR-title 63,856 3,457,164PatTR-abstract 599,435 2,595,515PatTR-claims 876,603 4,244,324UMLS 85,683 0Wikipedia 8,438 0TOTAL 3,956,478 16,372,602Table 2: WMT 2014 Medical Translation sharedtask parallel training data after preprocessingsteps.1.2 Training Data SelectionIt is an open secret that high quality and largequantity of the parallel corpus are the two mostimportant factors for a high-quality SMT system.These factors assist the word aligner in producinga precise alignment model, which in turn bringsbenefits to the other SMT training steps.The quantity factor also helps the SMT systemto cover more translation input variations.
In orderto efficiently use the training corpora listed in Ta-ble 2, we explored some data selection methodolo-gies.
We used the feature decay algorithm (Biciciet al., 2014) to select the training instances trans-ductively, using the source side of the test set.
Webuilt systems with the pre-defined selection pro-portions in token number, 1/64, 1/32, 1/16, 1/8,1/2, 3/4 and 1 of all the in-domain medical train-ing data, then searched for the best performing261system using the test data set as our baseline (Ta-ble 3).
For the purpose of making the potentialbaseline systems comparable, instance selectionwas employed after word alignment using wordaligner MGIZA++ (Gao and Vogel, 2008) on allthe available data.
The transductive learning usesfeatures extracted from the source data of the de-velopment set with the default feature decay algo-rithm weight settings.
All of systems were trainedusing the default phrase-based training parametersettings of Moses 1.0 framework, with additionalmsd-bidirectional-fe reordering model (Koehn etal., 2005).
We extract phrase pairs based on grow-diag-final-and (Koehn et al., 2003) heuristics.The language model was created with open sourceIRSTLM toolkit (Federico et al., 2008) using allthe English in-domain data (monolingual and par-allel).
We used 5-gram with modied Kneser-Neysmoothing (Kneser and Ney, 1995).
The tuningstep used minimum error rate training (MERT)(Och, 2003).
The performance was measured bythe test data set in case insensitive BLEU score.Proportions Test setcase insensitive BLEU1/64 0.43741/32 0.44091/16 0.43701/8 0.44191/4 0.43901/2 0.43993/4 0.43971 0.4260Table 3: Feature decay algorithm transductivelearning selection on all in-domain data using ex-tracted features from the source side of the testdata set.
We choose system uses 1/8 proportionsof the in-domain data as our baseline system.Our results show that the system trained with1/8 proportion of the in-domain medical trainingdata (398,098 sentence pairs) selected by FDAoutperformed the others.
We chose this system asour baseline system.2 Experiments2.1 Maximum Phrase LengthWhile extracting phrase pairs, collecting longerphrases is not guaranteed to produce a better qual-ity phrase table than the shorter settings, evensetting the maximum phrase length to three canachieve top performance (Koehn et al., 2003).We take this WMT 2014 opportunity to study thecapability of long phrase lengths ( >=10 ).
Wetrained translation models with phrase length set-ting from 10 to 15, employed them to our base-line system and compared the performance withthe default setting (length = 7).Phrase Length Phrase Table Test setEntries case insensitiveBLEU7 (Baseline) 19.31 0.441910 29.67 0.440011 32.87 0.441612 35.95 0.4444*13 38.91 0.4448*14 41.75 0.4444*15 44.47 0.4362Table 4: -max-phrase-length setting experiment,where phrase table entries is in millions.
* indi-actes statistically significant improvement at the p= 0.05 level.1As stated in (Koehn et al., 2003) and expected,the size of the phrase table is linear with respect tothe maximum phrase length restriction.
Surpris-ingly, we also found the performance can still im-prove after the default length setting, until a peakpoint (Table 4).It is also interesting to see the effect for eachsentence in the test set when the default phraselength setting in Moses framework is changed.
Wefirst evaluated the sentence level BLEU scores forthe systems listed in Table 4, then compared themwith our baseline system sentence level BLEUscores and categorised the compared results intoincreased, decreased or unaffected groups (Fig-ure 1).
We found that system with -max-phrase-length set to 12 is influenced the least (158, 118and 724 sentences have BLEU score increased,decreased and unaffected respectively) and with-max-phrase-length sets to 10 is influenced themost (261, 257 and 482 sentences have BLEUscore increased, decreased and unaffected respec-tively).We then looked into the decoding phase andtried to discover the actual phrase length that wasused to generate the translation outputs.
We ex-posed the translation segmentations by trigger-ing the -report-segmentation decoding parameter1The same notation is used for the rest of the tables in thispaper26210 11 12 13 14 15200250300350400450500550600650700750261208158258217218257180118213163227482612724529620555-max-phrase-length from 10 to 15testset(1000sentences)increased decreased unaffectedFigure 1: Sentence level BLEU score affects whenenlarge -max-phrase-length7 10 11 12 13 14 155 ?
10?45 ?
10?20.10.150.20.250.30.350.40.450.50.550.6baseline and -max-phrase-length from 10 to 15Sourcephraseusedindecoding(%)pl=1 pl=2 pl=3 pl=4 pl=5 pl=6 pl=7 pl=8Figure 2: Phrase length (pl) distribution used indecodingin the Moses framework and computed the per-centage of different phrases used according to thephrase token number (Figure 2).
The transla-tion is mostly generated from short source phrases(length<4) in all the systems during decoding,which we think is the reason that setting phrase ex-traction to length 3 can achieve top performance.We did not carry out more experiments in thiscase, as we think there is no absolute maximumphrase length setting which can fit into all experi-ments and such experiments depend on many fac-tors, such as the similarity between the trainingcorpus and then testing data.
The choice to set-max-phrase-length to 13 is purely directed by theBLEU score shown in Table 4.2.2 Reordering ModelsCeaus?u et al.
(2011) also found that long-range re-ordering is one of the characteristics of patent doc-uments; however, long-range reordering increasesthe difficulty of SMT training and decoding.
Weexperimented two approaches to address this chal-lenge.
Apart from the msd-bidirectional-fe lexi-cal reordering model (Koehn et al., 2005) in ourbaseline system, the phrase-based orientation andhierarchical orientation reordering models (Gal-ley and Manning, 2008) can capture long distancedependencies.
The phrase-based orientation re-ordering model is similar to the lexical reorderingapproach, the only difference between these twomodels is the phrase-based reordering model per-forms reordering only on the phrase level, but thehierarchical reordering model does not have suchconstraint - it does not require phrases to be ad-jacent.
OSM (Durrani, 2011) (Durrani, 2013b)is a sequence model integrating the N-gram-basedtranslation model and reordering model.
It de-fines three operations for reordering and consid-ers all reordering possibilities within a fixed win-dow while searching.
We experimented with bothreordering models, and found that the system de-fined with three reordering models performs bet-ter (Table 5) than OSM.
We then tried to use bothOSM and the reordering models together, whichproduced the best system at this point.Systems Test setcase insensitive BLEUBaseline + 13 0.4448+ OSM 0.4472+ pho-ho 0.4551*+ pho-ho + OSM 0.4561*Table 5: Reordering Model or/and OSM results2.3 Two Translation ModelsThe back-off model aims to produce translationsfor the unknown words or unknown phrases in theprimary translation table by yielding the phrase ta-ble translation probability from primary transla-tion table to the back-off table, as in (Koehn etal., 2012a)pBO(e|f) ={p1(e|f) if count1(f) > 0p2(e|f) otherwiseMoreover, we look at using the back off model263as a domain adaptation approach, which is to con-strain the translation options within the target do-main unless no options can be found, in whichcase the translation will be selected from the back-off model.Phrase table fill-up (Bisazza et al., 2011) is avery similar approach with back-off models, it col-lects and uses the phrase pairs from the out-of-domain phrase table only when the input is un-available at the in-domain phrase table.
It mergesthe in-domain and out-of-domain translation mod-els into one, where the scores are taken from morereliable source.
To distinguish the source of aphrase pair entry, fill-up assigns a binary value asan additional feature at the merged phrase table.We trained our out-of-domain translation modelseparately using all of the out-of-domain medi-cal data listed at Table 2 with the same parame-ter settings as our baseline system, then employedMoses?s back-off model feature to pass the pri-mary and back-off translation models to the de-coder at tuning and translation time.
The fill-uptool was sourced from (Bisazza et al., 2011) atMoses?s distribution.
Our experiment results (Ta-ble 6) show that the fill-up approach performedbetter than the back-off model approach.Systems Test setcase insensitive BLEUBaseline + 13 + pho-ho + OSM 0.4561Back-off 0.4573Fill-up 0.4599*Table 6: Back-off and fill-up experiment results2.4 Language ModelUntil now, we have reported our results using alanguage model trained with all in-domain medi-cal data only.
We also took the similar approachto (Koehn et al., 2007) and carried out languagemodel experiments.
We trained our out-of-domainlanguage model with all the out-of-domain En-glish sentences mentioned in section 1.1, then in-terpolated the in-domain and out-of-domain lan-guage model by optimizing the perplexity to thedevelopment data set.
We received a similar pic-ture to (Koehn et al., 2007), where the languagemodel trained with only in-domain data performedthe best (Table 7).Our final submission for WMT 2014 MedicalTranslation shared task is the * system at Table 7.Systems Test setcase insensitive BLEUBaseline + 13 + pho-ho+ OSM + Fill-up* 0.4599out-of-domain LM 0.4461interpolated LM 0.4592Table 7: Language model experiment results3 ConclusionIn this paper, we report our results on the WMT2014 in the French to English translation direc-tion.
We shared our statistics for the bilingualcorpora used to train our translation system.
Allsystems were trained using the open source Moses1.0 translation framework.
Based on the featureset of Moses phrased-based translation system, wecarried out our experiments on translation models,reordering models, operation sequence model andlanguage model.
We also experimented on dataselection and releasing the length restriction whileextracting phrase pairs.4 AcknowledgementsThis research is supported by the Science Foun-dation Ireland (Grant 12/CE/I2267) as part ofthe Centre for Next Generation Localisation(www.cngl.ie) at Dublin City University.
Wewould also like to acknowledge Ergun Bicici whogives suggestions at the data selection approach.ReferencesAlexandru Ceaus?u, John Tinsley, Jian Zhang and AndyWay.
2011.
Experiments on domain adaptation forpatent machine translation in the PLuTO project,The 15th conference of the European Associationfor Machine Translation, Leuven, Belgium.Arianna Bisazza, Nick Ruiz, and Marcello Fed-erico.
2011.
Fill-up versus Interpolation Meth-ods for Phrase-based SMT Adaptation., In Interna-tional Workshop on Spoken Language Translation(IWSLT), San Francisco, CA.Durrani, N., Schmid, H., and Fraser, A.
2011.
AJoint Sequence Translation Model with IntegratedReordering., The 49th Annual Meeting of the As-sociation for Computational Linguistics, Portland,Oregon, USA.Durrani, N., Fraser, A., Schmid, H., Hoang, H., andKoehn, P. 2013b.
Can Markov Models Over Min-imal Translation Units Help Phrase-Based SMT,The 51th Annual Meeting of the Association forComputational Linguistics, Sofia, Bulgaria.264Ergun Bicici and Deniz Yuret.
2014.
Optimizing In-stance Selection for Statistical Machine Translationwith Feature Decay Algorithms, IEEE/ACM Trans-actions On Audio, Speech, and Language Process-ing (TASLP).Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models,Computational Linguistics, 29(1):1951.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation, The 41th AnnualMeeting of the Association for Computational Lin-guistics, Sapporo, Japan.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: an open source toolkit forhandling large scale language models, Interspeech,Brisbane, Australia.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
, The 2008 Conference on Empirical Meth-ods in Natural Language Processing,pages 848856,Honolulu, Hawaii, October.
Association for Compu-tational Linguistics.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool, In Software Engi-neering, Testing, and Quality Assurance for Natu-ral Language Processing, SETQA-NLP 2008, pages49-57, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne andDavid Talbot.
2005.
Edinburgh System Descriptionfor the 2005 IWSLT Speech Translation Evaluation,International Workshop on Spoken Language Trans-lation.Philipp Koehn and Josh Schroeder.
2007.
Experi-ments in Domain Adaptation for Statistical MachineTranslation, The Second Workshop on StatisticalMachine Translation, pages 224227, Prague.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne andDavid Talbot.
2003.
Statistical phrase-based trans-lation, 2003 Conference of the North AmericanChapter of the Association for Computational Lin-guistics on Human Language Technology, pages4854, Edmonton, Canada.Philipp Koehn, and Barry Haddow.
2012.
Interpolatedbackoff for factored translation models., The 10thConference of the Association for Machine Transla-tion in the Americas (AMTA).Reinhard Kneser and Hermann Ney 1995.
Improvedbacking-off for m-gram language modeling., IEEEInternational Conference on Acoustics, Speech andSignal Processing, pages 181184.265
