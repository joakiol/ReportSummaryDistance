Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1164?1172,Beijing, August 2010Probabilistic Tree-Edit Models with Structured Latent Variables forTextual Entailment and Question AnsweringMengqiu WangComputer Science DepartmentStanford Universitymengqiu@cs.stanford.eduChristopher D. ManningComputer Science DepartmentStanford Universitymanning@cs.stanford.eduAbstractA range of Natural Language Process-ing tasks involve making judgments aboutthe semantic relatedness of a pair of sen-tences, such as Recognizing Textual En-tailment (RTE) and answer selection forQuestion Answering (QA).
A key chal-lenge that these tasks face in commonis the lack of explicit alignment annota-tion between a sentence pair.
We capturethe alignment by using a novel probabilis-tic model that models tree-edit operationson dependency parse trees.
Unlike previ-ous tree-edit models which require a sep-arate alignment-finding phase and resortto ad-hoc distance metrics, our methodtreats alignments as structured latent vari-ables, and offers a principled frameworkfor incorporating complex linguistic fea-tures.
We demonstrate the robustness ofour model by conducting experiments forRTE and QA, and show that our modelperforms competitively on both tasks withthe same set of general features.1 IntroductionMany complex Natural Language Processing(NLP) applications can be broken down to a sub-task of evaluating the semantic relationship ofpairs of sentences (e.g., in Question Answering,answer selection involve comparing each answercandidate against the question).
This means thatresearch aiming at analyzing pairs of semanti-cally related natural language sentences is promis-ing because of its reusability: it is not tied toa particular internal representation of meanings,but it nevertheless serves as a first step towardsfull meaning understanding, which is applicableto a number of applications.
At the same time,this paradigm clearly defines the input and outputspace, facilitating system comparison and stan-dard evaluation.
Tasks of this paradigm havedrawn much of the focus in recent NLP research,including Recognizing Textual Entailment (RTE),answer selection for Question Answering (QA),Paraphrase Identification (PI), Machine Transla-tion Evaluation (MTE), and many more.In each of these tasks, inputs to the systems arepairs of sentences that may or may not convey thedesired semantic property (e.g., in RTE, whetherthe hypothesis sentence can be entailed from thepremise sentence; in QA, whether the answer can-didate sentence correctly answers the question),and the output of the system is a binary classifi-cation decision (or a regression score,as in MTE).Earlier studies in these domains have concludedthat simple word overlap measures (e.g., bag ofwords, n-grams) have a surprising degree of util-ity (Papineni et al, 2002; Jijkoun and de Ri-jke, 2005b), but are nevertheless not sufficient forthese tasks (Jijkoun and de Rijke, 2005a).
A com-mon problem identified in these earlier systems isthe lack of understanding of the semantic relationbetween words and phrases.
Later systems thatinclude more linguistic features extracted from re-sources such as WordNet have enjoyed more suc-cess (MacCartney et al, 2006).
Studies have alsoshown that certain prominent syntactic featuresare often found beneficial (Snow et al, 2006).More recent studies gained further leverage fromsystematic exploration of the syntactic featurespace through analysis of parse trees (Wang et al,11642007; Das and Smith, 2009).There are two key challenges imposed by thesetasks.
The first challenge has to do with the hiddenalignment structures embedded in the sentencepairs.
It is straightforward to see that in orderto extract word-matching and/or syntax-matchingfeatures, inevitably one has to consider the align-ment between words and/or syntactic parts.
Thesealignments are not given as inputs, and it is anon-trivial task to decide what the correct align-ment is.
Alignment-based approach have beenproven effective by many RTE, QA and MTE sys-tems (Haghighi et al, 2005; Wang et al, 2007;MacCartney et al, 2008; Das and Smith, 2009,inter alia).
Although alignment is a commonlyused approach, it is not the only one.
Other stud-ies have successfully applied theorem proving andlogical induction techniques, translating both sen-tences to knowledge representations and then do-ing inference on these representations (Moldovanet al, 2003; Raina et al, 2005; de Salvo Brazet al, 2005; MacCartney and Manning, 2007, in-ter alia).A second challenge arises when a system needsto combine various sources of evidence (i.e., sur-face text features, semantic features, and syntacticfeatures) to make a global classification decision.Quite often these features are heavily overlappingand sometimes contradicting, and thus a robustlearning scheme that knows when to activate whatfeature is desired.
Traditional approaches employa two-stage or multi-stage model where tasks arebroken down into alignment finding, feature ex-traction, and feature learning subtasks (Haghighiet al, 2005; MacCartney et al, 2008).
The align-ment finding task is typically done by commit-ting to a one best alignment, and subsequent fea-tures are extracted only according to this align-ment.
A large body of literature in joint learninghas demonstrated that such an approach can sufferfrom cascaded errors at testing, and does not ben-efit from the potential for joint learning (Finkel etal., 2006).In this paper, we present a novel undirectedgraphical model to address these challenges.
Apromising approach to these challenges is model-ing the alignment as an edit operation sequenceover parse tree representation, an approach pio-neered by (Punyakanok et al, 2004; Kouylekovand Magnini, 2006; Harmeling, 2007; Mehdad,2009).
We improve upon this earlier work byshowing how alignment structures can be inher-ently learned as structured latent variables in ourmodel.
Tree edits are represented internally asstate transitions in a Finite-State Machine (FSM),and our model is parameterized as a Condi-tional Random Field (CRF) (Lafferty et al, 2001),which allows us to incorporate a diverse set of ar-bitrarily overlapping features.In comparison to previous work that exploitsvarious ad-hoc or heuristic ways of incorporatingtree-edit operations, our model provides an ele-gant and much more principled way of describingtree-edit operations in a probabilistic setting.2 Tree-edit CRF for ClassificationA training instance consists of a pair of sentencesand an associated binary judgment.
In RTE, forexample, the input sentence pairs is made up ofa text sentence (e.g., Gabriel Garcia Marquez isa novelist and winner of the Nobel prize for lit-erature.)
and a hypothesis sentence (e.g., GabrielGarcia Marquez won the Nobel for Literature.
).The pair is judged to be true if the hypothesis canbe entailed from the text (e.g., the answer is truefor the example sentence pair).Formally, we denote the text sentence as txt andthe hypothesis sentence as hyp, and denote theirlabeled dependency parse trees as ?t and ?h, re-spectively.
We use the binary variable z ?
{0,1}to denote the judgment.The generative story behind our model is aparse tree transformation process.
?t is trans-formed into ?h through a sequence of tree ed-its.
Examples of tree edits are delete child, in-sert parent, and substitute current.
An edit se-quence e = e1 .
.
.em is valid if ?t can be success-fully turned into ?h according to e. An example ofa trivial valid edit sequence is one that first deletesall nodes in ?t then inserts all nodes in ?h.Delete, insert and substitute form the three ba-sic edit operations.
Each step in an edit sequenceis also linked with current edit positions in bothtrees, denoted as e.p = e1.p .
.
.em.p.
We indexthe tree nodes using a level-order tree traversalscheme (i.e., root is visited first and assigned in-1165dex 0, then each one of the first level childrenof the root is visited in turn, and assigned an in-dex number incremented by 1).
It is worth notingthat every valid edit sequence has a correspond-ing alignment mapping.
Nodes that are insertedor deleted are aligned to null, and nodes that aresubstituted are aligned.
One can find many editsequence for the same alignment, by altering theorder of edit operations.We extend these basic edit operations into moreelaborate edit operations based on the linguisticand syntactic properties of the current tree nodesthat they fire on.
For example, the following areall possible edit operations: delete a noun that isSUB of the root, delete a named-entity of typePERSON, substitute roots of the tree.
In ourexperiments, we designed a set of 45 edit op-erations (12 delete, 12 insert and 21 substitute).More details of the edit operations are describedin ?4.
Depending on the specific application do-main, more sophisticated and verbose tree edit op-erations can be designed and easily incorporatedinto our model.
In particular, tree edit opera-tions involving deleting, inserting or substitutingentire treelets seem interesting and promising, re-quiring merely a simple extension to the forward-backward dynamic programming.Next, we design a Finite-State Machine (FSM)in which each edit operation is mapped to a uniquestate, and an edit sequence is mapped into a tran-sition sequence among states (denoted as e.a =e1.a .
.
.em.a).
In brief, an edit sequence is as-sociated with a sequence of edit positions in thetrees (e.p = e1.p .
.
.em.p), as well as a transitionsequence among states (e.a= e1.a .
.
.em.a).The probability of an edit sequence e given theparse trees is defined as:P(e | ?t,?h) = 1Z|e|?i=1exp ?
?
f(ei?1,ei,?t,?h) (1)where f are feature functions, ?
are associated fea-ture weights, and Z is the partition function to bedefined next.Recall that our training data is composed of notonly positive examples but also negative exam-ples.
In order to take advantage of this label in-formation, we adopt an interesting discriminativelearning framework first introduced by McCallumet al (2005).
We call the FSM state set describedabove the positive state set (S1), and duplicate theexact same set of states, and call the new set nega-tive state set (S0).
We then add a starting state(Ss),and add non-deterministic transitions from Ss toevery state in S1.
We then add the same transi-tions for S0.
We now arrive at a new FSM struc-ture where upon arriving at the starting state, onemakes a non-deterministic decision to enter eitherthe positive set or the negative set and stay in thatset until reaching the end of the edit sequence,since no transitions are allowed across the positiveand negative set.
Each edit operation sequencecan now be associated with a sequence of posi-tive states as well as a sequence of negative states.The intuitive idea is that during training, we wantto maximize the weights of the positive examplesin the positive state set and minimize their weightsin the negative state set, and vice versa.
In otherwords, we want the positive state set to attractpositive examples but push away negative exam-ples.
Figure 1 illustrates two example valid editsequences in the FSM, one in the positive state setand one in the negative state set.Formally, the partition function Z in (1) is de-fined as the sum of weights of all valid edit se-quences in both the positive set and negative set.Features extracted from positive states are disjointfrom features extracted from negative states.Z = ?e: e.a?Ss+{S0?S1}?|e|?i=1exp ?
?
f(ei?1,ei,?t,?h)Recall z ?
{0,1} is the binary judgment indi-cator variable.
The conditional probability of z isobtained by marginalizing over all edit sequencesthat have state transitions in the state set corre-sponding to z:P(z | ?t,?h) = ?e: e.a?Ss+S?zP(e | ?t,?h) (2)The L2-norm penalized log-likelihood over ntraining examples (L) is our training objectivefunction:L=n?j=1log(P(z( j) | ?
( j)t ,?
( j)h ))?
??
?22?2 (3)At test time, the z with higher probability is takenas our prediction outcome.1166Figure 1: This diagram illustrates the FSM architecture.
There is a single start state, and we can transit into either the positivestate set (nodes that are not shaded), or the negative state set (shaded nodes).
Here we show two examples of valid editsequences.
They result in the same alignment structure as show in the bottom half of the diagram (dotted lines across the twosentences are alignment links).
Numbers over the arcs in the state diagram denote the edit sequence index, and numbers undereach word in the parse tree diagram denote each node?s level-order index number.3 Parameter EstimationWe used Expectation Maximization method sincethe objective function given in (3) is non-convex.In the M-step, finding the optimal parameters un-der the current model expectation involves com-puting forward-backward style dynamic program-ming (DP) in a three-dimensional table (two forinputs and one for states) and optimization usingL-BFGS method.
In practice the resulting DP ta-ble can be quite large (for a sentence pair of length100, and 2 sets of 45 states, we obtain 900,000 en-tries).
We improved efficiency by pruning out par-tial sequences that do not lead to a complete validsequence and pre-compute the state-transition ta-ble and features.4 Edit OperationsTable 1 lists the groups of edit operations we de-signed and their descriptions.
Not shown in thetable are three default edits ( insert, delete andsubstitute), which fire when none of the more spe-cific edit operations match.
Edit operations listedin the the top-left section capture basic match-ing, deletion and insertion of surface text, part-of-speech tags and named-entity tags.
The top-rightsection capture alignments of semantically relatedwords, based on relational information extractedfrom various linguistic resources, such as Word-Net and NomBank.
And the bottom section cap-ture syntactic edits.
Note that multiple edit opera-tions can fire at the same edit position if conditionsare matched (e.g., we can choose to delete if thereare more words to edit in txt, or to insert if thereare more words to edit in hyp).5 FeaturesOne of the most distinctive advantages of ourmodel compared to previous tree-edit based mod-els is the ability to include a wide range of non-independent, rich linguistic features.
The featureswe employed can be broken down into two cat-egories.
The first category is zero-order featuresthat model the current edit step.
They consist ofa conditioning property of the current edit, andthe current state in the FSM.
The second cate-gory is first-order features that capture state tran-sitions, by concatenating the current FSM statewith the previous FSM state.
One simple form ofzero-order feature is the current FSM state itself.The FSM states already carry a lot of informationabout the current edits.
Conditioning propertiesare used to further describe the current edit.
Theyare often more fine-grained and complex (e.g.,1167Surface edits Semantic edits{I,D,S}-{POS} insert/delete/substitute words of a POS type, S-SYNONYM substitute two words that are synonymswhere POS is noun, verb or proper noun S-HYPERNYM substitute two words that are hypernyms{I,D,S}-NE insert/delete/substitute named-entity words S-ANTONYM substitute two words that are antonyms{I,D,S}-LIKE insert/delete/substitute words that expresses likeli-hood, e.g., maybe, possibly S-ACRONYMsubstitute two words in which one is an acronym ofthe other{I,D,S}-MODAL insert/delete/substitute modal verbs, e.g., can,could, may S-NOMBANKsubstitute two words that are related according toNomBankS-{SAME/DIFF} the words being substituted are the same or differ-ent S-NUM-0,1substitute two words that are both numerical val-ues, and 1 if they match, 0 if they mismatchSyntactic edits{I,D,S}-ROOT insert/delete/substitute root of the trees{I,D,S}-{REL} insert/delete/substitute a tree node of grammatical relation type, where REL is either SUB, OBJ, VC or PRDTable 1: List of edit operations.
I for INSERT, D for DELETE, and S for SUBSTITUTE.syntactic-matching conditions listed below).
Togive an example, in Figure 1, the second edit oper-ation in the example sequence is S-NE.
A match-ing condition feature that fires with this state couldbe substitute NE type PERSON, which tells usexactly what type of named-entity is being sub-stituted.It is notable that in designing edit operationsand features, there is a continuum of choice interms of how much information to be encoded asfeatures versus edit operations.
To better illustratethe trade-off, consider the two extreme cases ofthis continuum.
At one extreme, we can design asystem where there are only three basic edit op-erations, and all extra information in our currentset of edit operations can be encoded as features.For example, in this case edit operation S-NEwould become S with feature substitute NE.
Theother extreme is to encode every zero-order fea-ture as a separate edit operation.
The amountof information encoded in the zero-order featuresand edit operations is the same in both cases, butthe difference lies in first-order features and ef-ficiency.
When encoding more information asedit operations (and thus more states in FSM),first-order features become much more expres-sive; whereas when encoding more informationas features, computation becomes cheaper as thenumber of possible state transition sequences isreduced.
In our experiments, we aim to keep aminimal set of edit operations that are meaning-ful but not overly verbose, and encode additionalinformation as features.
Each feature is a binaryfeature initialized with weight 0.Due to space limitation, we list the most im-portant zero-order features.
Many of these fea-tures are inspired by MacCartney et al (2006)and Snow et al (2006), but not as sophisticated.Word matching features.
These features detectif a text word and a hypothesis word match thefollowing conditions:1. have the same lemma2.
one is a phrase and contains the other word3.
are multi-word phrases and parts match4.
have the same/different named-entity type(s) + thenamed-entity type(s)Tree structure features.
These features try tocapture syntactic matching/mismatching informa-tion from the labeled dependency parse trees.
1.whether the roots of the two trees are aligned2.
parent-child pair match3.
(2.)
and labels also match4.
(2.)
and labels mismatch5.
(4.)
and detailing the mismatching labels6.
parent+label match, child mismatch7.
child and label match, parents are {hyper/syno/anto}nym8.
looking for specific SUB/OBJ/PRD construct as in Snowet al (2006).6 PreprocessingIn all of our experiments, each input pair oftext and hypothesis sentence is preprocessed asfollowing: Sentences were first tokenized bythe standard Penn TreeBank tokenization script,and then we used MXPOST tagger (Ratnaparkhi,1996) for part-of-speech (POS) tagging.
POStagged sentences were then parsed by MST-Parser (McDonald et al, 2005) to produce labeleddependency parse trees.
The parser was trained1168on the entire Penn TreeBank.
The last step in thepipeline is named-entity tagging using StanfordNER Tagger (Finkel et al, 2005).7 RTE ExperimentsGiven an input text sentence and a hypothesissentence, the task of RTE is to make predictionsabout whether or not the hypothesis can be en-tailed from the text sentence.
We use standardevaluation datasets RTE1-3 from the Pascal RTEChallenges (Dagan et al, 2006).
For each RTEdataset, we train a tree-edit CRF model on thetraining portion and evaluate on the testing por-tion.
We report accuracy of classification results,and precision and recall for the true entailmentclass.
There is a balanced positive-negative sam-ple distribution in each dataset, so a random base-line gives 50% classification accuracy.
We usedRTE1 for feature selection and tuning ?
in the L2regularizer (?
= 5 was used).
RTE2 and RTE3were reserved for testing.Our system is compared with four systemson RTE2 and three other systems on the RTE3dataset.1 We chose these systems for compari-son because they make use of syntactic depen-dencies and lexical semantic information.
No-tably other systems that give state-of-the-art per-formance on RTE use non-comparable techniquessuch as theorem-proving and logical induction,and often involve significant manual engineeringspecifically for RTE, thus do not make meaningfulcomparison to our model.For RTE2, Kouylekov and Magnini (2006) ex-perimented with various TED cost functions andfound a combination scheme to work the best forRTE.
Vanderwende et al (2006) used syntacticheuristic matching rules with a lexical-similarityback-off model.
Nielsen et al (2006) extractedfeatures from dependency path, and combinedthemwith word-alignment features in a mixture ofexperts classifier.
Zanzotto et al (2006) proposeda syntactic cross-pair similarity measure for RTE.For RTE3, Harmeling (2007) took a similarclassification-based approach with transformationsequence features.
Marsi et al (2007) describeda system using dependency-based paraphrasing1Different systems are used for comparison because noneof these systems reported performance on both datasets.RTE2 Acc.% Prec.% Rec.%Vanderwende et al, 2006 60.2 59.0 67.0K&M, 2006 60.5 58.9 70.0Nielsen et al, 2006 61.1 59.0 73.3Zanzotto et al, 2006 63.9 60.8 78.0Tree-edit CRF 63.0 61.7 68.5RTE3 Acc.% Prec.% Rec.%Marsi et al, 2007 59.1 - -Harmeling, 2007 59.5 - -de Marneffe et al, 2006 60.5 61.8 60.2Tree-edit CRF 61.1 61.3 65.3Table 2: Results on RTE2 and RTE3 dataset.
Results for deMarneffe et al (2006) were reported by MacCartney andManning (2008).techniques for RTE.
de Marneffe et al (2006) de-scribed a system where best alignments betweenthe sentence pairs were first found, then classifi-cation decisions were made based on these align-ments.Table 2 presents RTE results.
Our model per-forms competitively on both datasets.
On RTE2,our model gives second best performance amongthe methods we compare against, and the differ-ence in accuracy from the best system is quitesmall (7 out of 800 examples).
We observe alarger gap in recall, suggesting our method tendsto give higher precision, which is also commonlyfound in other syntax-based systems (Snow et al,2006).
It is worth noting that Zanzotto et al(2006) achieved second place in the official RTE2evaluation.
On RTE3, our model outperforms theother syntax-based systems compared.
In partic-ular, out system gives the same precision level asthe second best system (de Marneffe et al, 2006)without sacrificing as much recall, which is themost common drawback found in syntax-basedsystems.8 QA ExperimentsA second Tree-edit CRF model was trained forthe task of answer selection for Question Answer-ing.
In this task, the input pair consists of a shortfactoid question (e.g., Who beat Floyd Pattersonto take the title away?)
and an answer candidatesentence (e.g., He saw Ingemar Johansson knockdown Floyd Patterson seven times there in win-ning the heavyweight title.).
The pair is judgedpositive if the answer candidate sentence correctlyanswers the question and provides sufficient con-1169System MAP MRRPunyakanok et al, 2004 0.4189 0.4939Cui et al, 2005 0.4350 0.5569Wang et al, 2007 0.6029 0.6852H&S, 2010 0.6091 0.6917Tree-edit CRF 0.5951 0.6951Table 3: Results on QA task reported in Mean Average Pre-cision (MAP) and Mean Reciprocal Rank (MRR).textual support (i.e., does not merely contain theanswer key, for example, ?Ingemar Johanssonwas a world heavyweight champion?
would notbe a correct answer).
We followed the same ex-perimental setup as Wang et al (2007) and Heil-man and Smith (2010).
The training portion ofthe dataset consists of 5919 manually judged Q/Apairs from previous QA tracks at Text REtrievalConference (TREC 8?12).
There are also 1374Q/A pairs for development and 1866 Q/A pairsfor testing, both from the TREC 3 evaluation.
Thetask is framed as a sentence retrieval task, and thusMean Average Precision (MAP) and Mean Recip-rocal Rank (MRR) are reported for the ranked listof most probable answer candidates.
We com-pare out model with four other systems.
Wang etal.
(2007) proposed a Quasi-synchronous Gram-mar formulation of the problem which also mod-els alignment as structured latent variables, but ina generative probabilistic model.
Their methodgives the current state-of-the-art performance onthis task.
Heilman and Smith (2010) presenteda classification-based approach with tree-edit fea-tures extracted from a tree kernel.
Cui et al(2005) developed a dependency-tree based in-formation discrepancy measure.
Punyakanok etal.
(2004) used a generalized Tree-edit Distancemethod to score mappings between dependencyparse trees.
All systems were evaluated againstthe same dataset as the one we used.
Results ofreplicated systems for the last two were reportedby Wang et al (2007), with lexical-semantic aug-mentation from WordNet.Results in Table 3 show that our model gives thesame level of performance as Wang et al (2007),with no statistically significant difference (p > 5in sign test).
Both systems out-perform the othertwo earlier systems significantly.9 DiscussionOur experiments on RTE and QA applicationsdemonstrated that Tree-edit CRF models provideresults competitive with previous syntax-basedmethods.
Even though the improvements werequite moderate in some cases, the important pointis that our model provides a novel principledframework.
It works across different problem do-mains with minimal domain knowledge and fea-ture engineering, whereas previous methods areonly engineered for a particular task and are hardto generalize to new problems.While the current Tree-edit CRF model canmodel a large set of linguistic phenomenon andtree-transformations, it has some clear limitations.One of the biggest drawbacks is the lack of sup-port for modeling phrasal re-ordering, which is avery common and important linguistic phenom-ena.
It is not straightforward to implement re-ordering in the current model because it breaksthe word-order constraint which admits tractableforward-backward style dynamic programming.However, this shortcoming can be addressed par-tially by extending the model to deal with con-strained re-ordering per Zhang (1996).10 Related WorkTree Edit Distance (TED) have been studiedextensively in theoretical and algorithmic re-search (Klein, 1989; Zhang and Shasha, 1989;Bille, 2005).
In recent years we have seen manywork on applying TED based methods for NLP-related tasks (Punyakanok et al, 2004; Kouylekovand Magnini, 2006; Harmeling, 2007; Mehdad,2009).
Mehdad (2009) proposed a method basedon particle swarm optimization technique to au-tomatically learn the TED cost function.
Anotherwork that also developed an interesting approachto stochastic tree edit distance is Bernard et al(2008), but unfortunately experiments in the pa-per were limited to digit recognition and tasks onsmall artificial datasets.Many different approaches to modelingsentence alignment have been proposed be-fore (Haghighi et al, 2005; MacCartney et al,2008).
Haghighi et al (2005) treated alignmentfinding in RTE as a graph matching problem1170between sentence parse trees.
MacCartney etal.
(2008) described a phrase-based alignmentmodel for MT, trained by the Perceptron learningalgorithm.
A line of work that offers similartreatment of alignment to our model is theQuasi-synchronous Grammar (QG) (Smith andEisner, 2006; Wang et al, 2007; Das and Smith,2009).
QG models alignments between two parsetrees as structured latent variables.
The generativestory of QG describes one that builds the parsetree of one sentence, loosely conditioned on theparse tree of the other sentence.
This formalismprefers but is not confined to tree isomorphism,therefore possesses more model flexibility thansynchronous grammars.The work of McCallum et al (2005) inspiredthe discriminative training framework that weused in our experiments.
They presented a StringEdit Distance model that also learns alignments ashidden structures for simple tasks such as restau-rant name matching.Our work is also closely related to other re-cent work on learning probabilistic models involv-ing structural latent variables (Clark and Curran,2004; Petrov et al, 2007; Blunsom et al, 2008;Chang et al, 2010).
The Tree-edit CRF model wepresent here is a new addition to this family of in-teresting models for discriminative learning withstructural latent variables.11 ConclusionWe described a Tree-edit CRF model for predict-ing semantic relatedness of pairs of sentences.Our approach generalizes TED in a principledprobabilistic model that embeds alignments asstructured latent variables.
We demonstrate awide-range of lexical-semantic and syntactic fea-tures can be easily incorporated into the model.Discriminatively trained, the Tree-edit CRF led tocompetitive performance on the task of Recogniz-ing Textual Entailment and answer selection forQuestion Answering.ReferencesBernard, M., L. Boyer, A. Habrard, and M. Sebban.2008.
Learning probabilistic models of tree edit dis-tance.
Pattern Recognition, 41(8):2611?2629.Bille, P. 2005.
A survey on tree edit distance andrelated problems.
Theoretical Computer Science,337(1-3):217?239.Blunsom, P., T. Cohn, and M. Osborne.
2008.
A dis-criminative latent variable model for statistical ma-chine translation.
In Proceedings of ACL-HLT.Chang, Ming-Wei, Dan Goldwasser, Dan Roth, andVivek Srikumar.
2010.
Discriminative learningover constrained latent representations.
In Proceed-ings of NAACL-HLT.Clark, S. and J. R. Curran.
2004.
Parsing the wsj usingccg and log-linear models.
In Proceedings of ACL.Cui, Hang, Renxu Sun, Keya Li, Min-Yen Kan, andTat-Seng Chua.
2005.
Question answering passageretrieval using dependency relations.
In Proceed-ings of SIGIR.Dagan, I., O. Glickman, and B. Magnini.
2006.
Thepascal recognising textual entailment challenge.Machine Learning Challenges, LNCS, 3944:177?190.Das, Dipanjan and Noah A. Smith.
2009.
Paraphraseidentification as probabilistic quasi-synchronousrecognition.
In Proceedings of ACL-IJCNLP.de Marneffe, M.-C., B. MacCartney, T. Grenager,D.
Cer, A. Rafferty, and C. D. Manning.
2006.Learning to distinguish valid textual entailments.In Proceedings of the second PASCAL ChallengesWorkshop on RTE.de Salvo Braz, R., R. Girju, V. Punyakanok, D. Roth,and M. Sammons.
2005.
An inference model forsemantic entailment and question-answering.
InProceedings of AAAI.Finkel, J. R., T. Grenager, and C. D. Manning.
2005.Incorporating non-local information into informa-tion extraction systems by gibbs sampling.
In Pro-ceedings of ACL.Finkel, J. R., C. D. Manning, and A. Y. Ng.
2006.Solving the problem of cascading errors: Approx-imate bayesian inference for linguistic annotationpipelines.
In Proceedings of EMNLP.Haghighi, A., A. Y. Ng, and C. D. Manning.
2005.
Ro-bust textual inference via graph matching.
In Pro-ceedings of EMNLP.Harmeling, S. 2007.
An extensible probabilistictransformation-based approach to the third recog-nizing textual entailment challenge.
In Proceedingsof ACL PASCAL Workshop on Textual Entailmentand Paraphrasing.1171Heilman, M. and N. A. Smith.
2010.
Tree editmodels for recognizing textual entailments, para-phrases, and answers to questions.
In Proceedingsof NAACL-HLT.Jijkoun, V. and M. de Rijke.
2005a.
Recognizing tex-tual entailment: Is word similarity enough?.
In Ma-chine Learning Challenge Workshop, volume 3944of LNCS, pages 449?460.
Springer.Jijkoun, V. and M. de Rijke.
2005b.
Recognizing tex-tual entailment using lexical similarity.
In Proceed-ings of the PASCAL Challenges Workshop on RTE.Klein, P. N. 1989.
Computing the edit-distance be-tween unrooted ordered trees.
In Proceedings ofEuropean Symposium on Algorithms.Kouylekov, M. and B. Magnini.
2006.
Tree edit dis-tance for recognizing textual entailment: Estimatingthe cost of insertion.
In Proceedings of the secondPASCAL Challenges Workshop on RTE.Lafferty, J., A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof ICML.MacCartney, Bill and Christopher D. Manning.
2007.Natural logic for textual inference.
In Proceedingsof Workshop on Textual Entailment and Paraphras-ing at ACL 2007.MacCartney, B. and C. D. Manning.
2008.
Model-ing semantic containment and exclusion in naturallanguage inference.
In Proceedings of COLING.MacCartney, B., T. Grenager, M.-C. de Marneffe,D.
Cer, and C. D. Manning.
2006.
Learning torecognize features of valid textual entailments.
InProceedings of HLT-NAACL.MacCartney, B., M. Galley, and C. D. Manning.
2008.A phrase-based alignment model for natural lan-guage inference.
In Proceedings of EMNLP.Marsi, E., E. Krahmer, and W. Bosma.
2007.Dependency-based paraphrasing for recognizingtextual entailment.
In Proceedings of ACL PASCALWorkshop on Textual Entailment and Paraphrasing.McCallum, A., K. Bellare, and F. Pereira.
2005.A conditional random field for discriminatively-trained finite-state string edit distance.
In Proceed-ings of UAI.McDonald, R., K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProceedings of ACL.Mehdad, Yashar.
2009.
Automatic cost estimation fortree edit distance using particle swarm optimization.In Proceedings of ACL.Moldovan, D., C. Clark, S. Harabagiu, and S. Maio-rano.
2003.
Cogex: A logic prover for questionanswering.
In Proceedings of HLT-NAACL.Nielsen, R. D., W. Ward, and J. H. Martin.
2006.
To-ward dependency path based entailment.
In Pro-ceedings of the second PASCAL Challenges Work-shop on RTE.Papineni, K., S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL.Petrov, S., A. Pauls, and D. Klein.
2007.
Discrimi-native log-linear grammars with latent variables.
InProceedings of NIPS.Punyakanok, V., D. Roth, and W. Yih.
2004.
Map-ping dependencies trees: An application to questionanswering.
In Proceedings of AI-Math.Raina, R., A. Y. Ng, , and C. Manning.
2005.
Robusttextual inference via learning and abductive reason-ing.
In Proceedings of AAAI.Ratnaparkhi, Adwait.
1996.
A maximum entropypart-of-speech tagger.
In Proceedings of EMNLP.Smith, D. A. and J. Eisner.
2006.
Quasi-synchronousgrammars: Alignment by soft projection of syn-tactic dependencies.
In Proceedings of the HLT-NAACL Workshop on Statistical Machine Transla-tion.Snow, R., L. Vanderwende, and A. Menezes.
2006.Effectively using syntax for recognizing false entail-ment.
In Proceedings of HLT-NAACL.Vanderwende, L., A. Menezes, and R. Snow.
2006.Microsoft research at rte-2: Syntactic contributionsin the entailment task: an implementation.
In Pro-ceedings of the second PASCAL Challenges Work-shop on RTE.Wang, M., N. A. Smith, and T. Mitamura.
2007.What is the jeopardy model?
a quasi-synchronousgrammar for question answering.
In Proceedings ofEMNLP-CoNLL.Zanzotto, F. M., A. Moschitti, M. Pennacchiotti, andM.T.
Pazienza.
2006.
Learning textual entailmentfrom examples.
In Proceedings of the second PAS-CAL Challenges Workshop on RTE.Zhang, K. and D. Shasha.
1989.
Simple fast algo-rithms for the editing distance between trees and re-lated problems.
SIAM Journal of Computing, 18.Zhang, K. 1996.
A constrained edit distance betweenunordered labeled trees.
Algorithmica, 15(3):205?222.1172
