Efficient Transformation-Based ParsingGiorgio Sat ta  Er i c  Br i l lD ipar t imento  di E le t t ron ica  ed In format ica  Depar tment  of Computer  Sc ienceUn ivers i t?
di Padova  Johns  Hopk ins  Un ivers i tyv ia  Graden igo ,  6 /A  Ba l t imore ,  MD 21218-26942-35131 Padova ,  I ta ly  b r i l l?cs ,  jhu .
edusatta@dei, unipd, itAbst ractIn transformation-based parsing, a finitesequence of tree rewriting rules are checkedfor application to an input structure.
Sincein practice only a small percentage of rulesare applied to any particular structure, thenaive parsing algorithm is rather ineffi-cient.
We exploit this sparseness in ruleapplications to derive an algorithm two tothree orders of magnitude faster than thestandard parsing algorithm.1 In t roduct ionThe idea of using transformational rules in natu-ral language analysis dates back at least to Chore-sky, who attempted to define a set of transfor-mations that would apply to a word sequence tomap it from deep structure to surface structure(see (Chomsky, 1965)).
Transformations have alsobeen used in much of generative phonology to cap-ture contextual variants in pronunciation, start-ing with (Chomsky and Halle, 1968).
More re-cently, transformations have been applied to a di-verse set of problems, including part of speechtagging, pronunciation etwork creation, preposi-tional phrase attachment disambiguation, and pars-ing, under the paradigm of transformation-basederror-driven learning (see (Brill, 1993; Brill, 1995)and (Brill and Resnik, 1994)).
In this paradigm,rules can be learned automatically from a trainingcorpus, instead of being written by hand.Transformation-based ystems are typically deter-ministic.
Each rule in an ordered list of rules is ap-plied once wherever it can apply, then is discarded,and the next rule is processed until the last rule inthe list has been processed.
Since for each rule theapplication algorithm must check for a matching atall possible sites to see whether the rule can apply,these systems run in O(rrpn) time, where 7r is thenumber of rules, p is the cost of a single rule match-ing, and n is the size of the input structure.
Whilethis results in fast processing, it is possible to createmuch faster systems.
In (Roche and Schabes, 1995),a method is described for converting a list of trans-formations that operates on strings into a determin-istic finite state transducer, resulting in an optimaltagger in the sense that tagging requires only onestate transition per word, giving a linear time tag-ger whose run-time is independent of the numberand size of rules.In this paper we consider transformation-basedparsing, introduced in (Brill, 1993), and we im-prove upon the O(Trpn) time upper bound.. Intransformation-based parsing, an ordered sequenceof tree-rewriting rules (tree transformations) are ap-plied to an initial parse structure for an input sen-tence, to derive the final parse structure.
We observethat in most transformation-based parsers, only asmall percentage of rules are actually applied, forany particular input sentence.
For example, in anapplication of the transformation-based parser de-scribed in (Brill, 1993), 7r = 300 rules were learned,to be applied at each node of the initial parse struc-ture, but the average number of rules that are suc-cessfully applied at each node is only about one.
Soa lot of time is spent testing whether the conditionsare met for applying a transformation and findingout that they are not met.
This paper presents anoriginal algorithm for transformation-based parsingworking in O(ptlog(t)) time, where t is the totalnumber of rules applied for an input sentence.
Sincein practical cases t is smaller than n and we canneglect the log(n) factor, we have achieved a timeimprovement of a factor of r. We emphasize thatrr can be several hundreds large in actual systemswhere transformations are lexicalized.Our result is achieved by preprocessing the trans-formation list, deriving a finite state, determiflistictree automaton.
The algorithm then exploits the au-tomaton in a way that obviates the need for checkingthe conditions of a rule when that rule will not apply,thereby greatly improving parsing run-time over thestraightforward parsing algorithm.
In a sense, ouralgorithm spends time only with rules that can beapplied, as if it knew in advance which rules cannotbe applied during the parsing process.The remainder of this paper is organized as fol-255lows.
In Section 2 we introduce some preliminaries,and in Section 3 we provide a representation f trans-formations that uses finite state, deterministic treeautomata.
Our algorithm is then specified in Sec-tion 4.
Finally, in Section 5 we discuss related workin the existing literature.2 Pre l iminar iesWe review in the following subsections some termi-nology that is used throughout this paper.2.1 TreesWe consider ordered trees whose nodes are assignedlabels over some finite alphabet E; this set is denotedas ET.
Let T E S T. A node of T is called le f tmostif it does not have any left sibling ( a root node isa leftmost node).
The he ight  of T is the lengthof a longest path from the root to one of its leaves(a tree composed of a single node has height zero).We define I TI as the number of nodes in T. A treeT E y\]T is denoted as A if it consists of a single leafnode labeled by A, and as A(T1,T2,... ,Ta), d >_ 1,if T has root labeled by A with d (ordered) childrendenoted by T1,.
.
.
,Td.
Sometimes in the exampleswe draw trees in the usual way, indicating each nodewith its label.What follows is standard terminology from thetree pattern matching literature, with the simplifi-cation that we do not use variable terms.
See (Hoff-mann and O'Donnell, 1982) for general definitions.Let n be a node of T. We say that a tree S matchesT at n if there exists a one-to-one mapping from thenodes of S to the nodes of T, such that the follow-ing conditions are all satisfied: (i) if n' maps to n",then n ~ and n I~ have the same label; (ii) the root ofS maps to n; and (iii) if n ~ maps to n" and n ~ is nota leaf in S, then n ~ and n" have the same degree andthe i-th child of n ~ maps to the i-th child of n% Wesay that T and S are equiva lent  if they match eachother at the respective root nodes.
In what followstrees that are equivalent are not treated as the sameobject.
We say that a tree T' is a subt ree  of T atn if there exists a tree S that matches T at n, andT ~ consists of the nodes of T that are matched bysome node of S and the arcs of T between two suchnodes.
We also say that T' is matched by S at n. Inaddition, T' is a pref ix of T if n is the root of T; T'is the suffix of T at n if T'  contains all nodes of Tdominated by n.Example  1 Let T -- B(D, C(B(D, B), C)) and letn be the second child of T's root.
S -- C(B,C)matches T at n. S' = B(D, C(B), C)) is a prefix orSand S" = C(B(D, B), C) is the suffix of T at n. \[\]We now introduce a tree replacement operatorthat will be used throughout he paper.
Let Sbe a subtree of T and let S / be a tree having thesame number of leaves as S. Let nl, n2, ?.., nz andn~,n~,...,n~, 1 > 1, be all the leaves from left toBD C_B_ EIEBEBC BfD EFigure 1: From left to right, top to bottom: tree Twith subtree S indicated using underlined labels atits nodes; tree S' having the same number of leavesas S; tree T\[S/S ~\] obtained by "replacing" S with S ~.right of S and S', respectively.
We write T\[S/S'\]to denote the tree obtained by embedding S ~ withinT in place of S, through the following steps: (i) ifthe root of S is the i-th child of a node n\] in T,the root of S I becomes the i-th child of n\] ; and (ii)the (ordered) children of n~ in T, if any, become thechildren of n~, 1 < i < l. The root of T\[S/S ~\] is theroot of T if node n\] above exists, and is the root ofS t otherwise.Example  2 Figure 1 depicts trees T, S I and T ~ inthis order.
A subtree S of T is also indicated usingunderlined labels at nodes of T. Note that S andS' have the same number of leaves.
Then we haveT' = T\[S/S'\].
n2.2  Tree automataDeterministic (bottom-up) tree automata were firstintroduced in (Thatcher, 1967) (called FRT there).The definition we propose here is a generalizationof the canonical one to trees of any degree.
Notethat the transition function below is computed ona number of states that is independent of the de-gree of the input tree.
Deterministic tree automatawill be used later to implement the bottom-up treepattern matching algorithm of (Hoffmann and O'-Donnell, 1982).Def in i t ion 1 A deterministic tree automaton (DTA)is a 5-tuple M = (Q, ~, ~, qo, F), where Q is a finiteset of s~ates, ~ is a finite alphabet, qo E Q is theinitial state, F C Q is the set of final states and 6 isa transition function mapping Q~ ?
E into O.Informally, a DTA M walks through a tree T by vis-iting its nodes in post-order, one node at a time.Every time a node is read, the current state ofthe device is computed on the basis of the states256reached upon reading the immediate left sibling andthe rightmost child of the current node, if any.
Inthis way the decision of the DTA is affected not onlyby the portion of the tree below the currently readnode, but also by each subtree rooted in a left sib-ling of the current node.
This is formally stated inwhat follows.
Let T E ~T and let n be one of itsnodes, labeled by a.
The state reached by M uponreading n is recursively specified as:6(T,n) = ~(X,X ' ,a ) ,  (1)where X -- q0 if n is a leftmost node, X -- 6(T, n') ifn' is the immediate left sibling of n; and X' -- q0 ifn is a leaf node, X'  = 6(T, n") if n" is the rightmostchild of n. The tree language recognized by M is thesetL(M)  = {T \[ ~(T, n) E F, T E E T,n the root of T}.
(2)Example  3 Consider the infinite set L ={B(A, C), B(A, B(A, C)), B(A, B(A, B(A, C) ) ) , .
.
.
}consisting of all right-branching trees with internalnodes labeled by B and with strings A'~C, n > 1as their yields.
Let M = (Q, {A ,B ,C} ,  6, qo,{qBc}) be a DTA specified as follows: Q = {q0,qA, qnc, q-i}; 6(qo, qo,A) = qA, 6(qA,qo, C) =5(qA, qBC, B) = qBC and q- i  is the value of allother entries of 5.
It is not difficult to see thatL(M)  = L. 1:3Observe that when we restrict o monadic trees, thatis trees whose nodes have degree not greater thanone, the above definitions correspond to the wellknown formalisms of deterministic finite state au-tomata, the associated extended transition function,and the regular languages.2.3 Transformation-based parsingTransformation-based parsing was first introducedin (Brill, 1993).
Informally, a transformation-basedparser assigns to an input sentence an initial parsestructure, in some uniform way.
Then the parseriteratively checks an ordered sequence of tree trans-formations for application to the initial parse tree,in order to derive the final parse structure.
Thisresults in a deterministic, linear time parser.
Inorder to present our algorithm, we abstract awayfrom the assignment of the initial parse to the input,and introduce below the notion of transformation-based tree rewriting system.
The formulation wegive here is inspired by (Kaptan and Kay, 1994)and (Roche and Schabes, 1995).
The relationshipbetween transformation-based tree rewriting sys-tems and standard term-rewriting systems will bediscussed in the final section.Definit ion 2 A transformation-based tree rewritingsystem (TTS)  is a pair G = (E,R),  where ~ is afinite alphabet and R = (r i , r2 , .
.
.
, r~) ,  7r >_ 1, isa finite sequence of tree rewriting rules having theform Q --+ Q', with Q, Q' E ~T and such that Q andQ' have the same number of leaves.If r = (Q ~ Q'), we write lhs(r) for Q and rhs(r)for Q'.
We also write lhs(R) for {lhs(r) I r E R}.
(Recall that we regard lhs(r/) and lhs(rj), i # j,  asdifferent objects, even if these trees are equivalent.
)We define \[r I = Ilhs(r) l + I rhs(r) I.The notion of transformation associated with aTTS G = (E, R) is now introduced.
Let C, C' E E T.For any node n of C and any rule r = (Q ~ Q') ofG, we writeC ~ C' (3)if Q does not match C at n and C = C'; or if Qmatches C at n and C' = C\[S/Q'\], where S is thesubtree of T matched by Q at n and Q'c is a freshcopy of Q'.
Let <nl,n2, .
.
.
,nt l ,  t > 1, be the post-ordered sequence of all nodes of C. We writeC ~ C' (4)r ,n  ?if Ci- i  ~ Ci, 1 < i <_ t, Co = C and Ct =C'.
Finally, we define the translation induced byG on Ea, as the map M(G) = {(C,C') I C Ey\]T, Ci_I~:~C i for 1 < i< ~r, Co =C,  C~ =C'} .3 Ru le  representat ionWe develop here a representation f rule sequencesthat makes use of DTA and that is at the basis ofthe main result of this paper.
Our technique im-proves the preprocessing phase of a bottom-up treepattern matching algorithm presented in (Hoffmannand O'Donnell, 1982), as it will be discussed in thefinal section.Let G = (~,R)  be a TTS, R = ( r i , r2 , .
.
.
, r~) .
Inwhat follows we construct a DTA that "detects" eachsubtree of an input tree that is equivalent o sometree in lhs(_R).
We need to introduce some additionalnotation.
Let N be the set of all nodes from the treesin lhs(R).
Call Nr the set of all root nodes (in N),N,~ the set of all leftmost nodes, Nz the set of all leafnodes, and Na the set of all nodes labeled by a E ~.For each q E 2 N, let right(q) = {n I n E N, n' Eq, n has immediate left sibling n'} and let up(q) ={n \[ n E N, n' E q, nhasr ightmostch i ldn '} .Also, let q0 be a fresh symbol.Def in i t ion 3 G is associated with a DTA Aa =(2 N U {q0}, E, 6a, qo, F), where F = {q \[ q E2 N, (q f3 Nr) # 0} and 6G is specified as follows:(i) 5a(qo,qo,a) = No M Nm ANt;(it) dia(qo,q',a) = NaANmA(NtUup(q ' ) ) ,  forq'  #qo;(iii) diG(q, qo, a) = Na A Nz t\] (Nr U right(q)), for qqo;(iv) 6a(q, q', a) = No M up(q') A (Nr U right(q)), forq  qo # q'.257Observe that each state of Ac simultaneously car-ries over the recognition of several suffixes of treesin lhs(/~).
These processes are started whenever Acreads a leftmost node n with the same label as aleftmost leaf node in some tree in lhs(R) (items (i)and (ii) in Definition 3).
Note also that we donot require any matching of the left siblings whenwe match the root of a tree in lhs(R) (items (iii)and (iv)).B BA --~ a B A / 'DBAc B-+ c c AA BB BC B A BA BFigure 2: From top to bottom: rules rl, r2 and r3of G.Example  4 Let G = (E,R),  where E = {A, B,C, D} and R = (rl,r2, r3).
Rules ri are depictedin Figure 2.
We write nij to denote the j - th node?
in a post-order enumeration of the nodes of lhs(ri),1 < i < 3 and 1 < j <__ 5.
(Therefore n35 denotes theroot node of lhs(r3) and n22 denotes the first childof the second child of the root node of lhs(r~).)
If weconsider only the useful states, that is those statesthat can be reached on an actual input, the DTAAc --- (Q, E, 5, qo, F), is specified as follows: Q ={qi I 0 < i < I1}, where ql = {nll,n12, n22, n32},q2 = {n21,n3x}, q3 = {n13, n23}, q4 = {n33}, q5 ={n14}, q6 = {n24}, q7 = {n34}, qs = {n15}, q9 -={n35}, qlo = {n25}, qll = (b; F = {qs, qg, qlo}.
Thetransition function 5, restricted to the useful states,is specified in Figure 3.
Note that among the 215 + 1possible states, only 12 are useful.
\[\]6(qo,qo,A) = ql 6(qo,qo,C) = q26(qa,qo, B) = q3 6(q l ,qo,C)  = q,6 (q l ,qz ,B)  = qs 6(q2, q3 ,B)  = qs~(q~,q,, B) = q7 ~(qo, qs, B) = q~6(qo ,q6 ,B)  = q9 6(qo, qT,B)  = qloFigure 3: Transition function of G. For all (q, q~, a) EQ2?
E not indicated above, 5(q, q', a) = qll-Although the number of states of Ac is exponen-tial in IN I, in practical cases most of these statesare never reached by the automaton on an actualinput, and can therefore be ignored.
This happenswhenever there are few pairs of suffix trees of treesin lhs(R) that share a common prefix tree but notree in the pair matches the other at the root node.This is discussed at length in (Hoffmann and O'Don-nell, 1982), where an upper bound on the number ofuseful states is provided.The following lemma provides a characterizationof Aa that will be used later.Lemma 1 Let n be a node o fT  E ~T and let n ~ bethe roof node of r E R. Tree lhs(r) matches Ta f  nif and only if n' E iG(T,n).Proof  (out l ine) .
The statement can be shown byproving the following claim.
Let m be a node in Tand m t be a node in lhs(r).
Call m l , .
.
.
,m~ = m,k > 1, the ordered sequence of the left siblings of m,with m included, and call m~, .
.
.
,  m' k, -" m', k' > 1,the ordered sequence of the left siblings of m ~, withm' included.
If m'  ~ Nr, then the two followingconditions are equivalent:* m'  E iv (T ,  m);?
k = k' and, for 1 < i < k, the suffix of lhs(r) atm~ matches T at mi.The claim can be shown by induction on the posi-tion of m ~ in a post-order enumeration of the nodesof lhs(r).
The lemma then follows from the spec-ification of set F and the treatment of set N~ initems (iii) and (iv) in Definition 3.
\[\]We also need a function mapping F x {1..(r + 1)}into {1..r} U {.1_}, specified as (min@ =_1_):next(q,i) = min{j \[ i < j < 7r, lhs(rj) hasroot node in q}.
(5)Assume that q E F is reached by AG upon reading anode n (in some tree).
In the next section next(q, i)is used to select the index of the rule that should benext applied at node n, after the first i - 1 rules ofR have been considered.4 The  a lgor i thmWe present a translation algorithm for TTS thatcan immediately be converted into a transformation-based parsing algorithm.
We use all definitions in-troduced in the previous sections.
To simplify thepresentation, we first make the assumption that theorder in which we apply several instances of the samerule to a given tree does not affect the outcome.Later we will deal with the general case.4.1 Order - f ree  caseWe start with an important property that is usedby the algorithm below and that can be easily shown(see also (Hoffmann and O'Donnell, 1982)).
Let G =(E, R) be a TTS and let ha be the maximum height258of a tree in lhs(R).
Given trees T and S, S a subtreeof T, we write local(T, S) to denote the set of allnodes of S and the first ha proper ancestors of theroot of S' in T (when these nodes are defined).Lemma 2 Assume that lhs(r), r E R, matches atree T at some node n. Let T ~'~ T' and lel S be thecopy of rhs(r) used in the rewriting.
For every noden' no~ included in local(T', S), we have ~a(T, n') =Oa(T',n').
\[\]We precede the specification of the method withan informal presentation.
The following three datastructures are used.
An associative list state asso-ciates each node n of the rewritten input tree withthe state reached by Aa upon reading n. If n isno longer a node of the rewritten input tree, stateassociates n with the emptyset.
A set rule(i) is as-sociated with each rule ri, containing some of thenodes of the rewritten input tree at which lhs(ri)matches.
A heap data structure H is also used toorder the indices of the non-empty sets rule(i) ac-cording to the priority of the associated rules in therule sequence.
All the above data structures are up-dated by a procedure called update.To compute the translation M(G) we first visitthe input tree with AG and initialize our data struc-tures in the following way.
For each node n, state isassigned a state of AG as specified above.
If rule rimust be applied first at n, n is added to rule(i) andH is updated.
We then enter a main loop and re-trieve elements from the heap.
When i is retrieved,rule ri is considered for application at each noden in rule(i).
It is important o observe that, sincesome rewriting of the input tree might have occurredin between the time n has been inserted in rule(i)and the time i is retrieved from H, it could be thatthe current rule ri can no longer be applied at n.Information in state is used to detect these cases.Crucial to the efficiency of our algorithm, each timea rule is applied only a small portion of the currenttree needs to be reread by AG, in order to updateour data structures, as specified by Lemma 2 above.Finally, the main loop is exited when the heap isempty.Algor i thml  Let G - (~,R)  be a TTS, R =( r l , r2 , .
.
.
, r~) .and l e tT  E ~ be an input tree.Let Aa = (2 ~ U {q0}, ~, ~a, q0, F) be the DTA as-sociated with G and ~G the reached state function.Let alo i be an integer valued variable, state be anassociative array, rule(i) be an initially empty set,for 1 < i < ~', and let H be a heap data structure.
(n ---+ rule(i) adds n to rule(i); i ---* H inserts i in H;i ~-- H assigns to i the least element in H, i fH  is notempty.)
The algorithm is specified in Figure 4.
\[\]Example  4 (continued) We describe a run of Al-gorithm 1 working with the sample TTS G = (E, R)previously specified (see Figure 2).proc update( oldset, newset, j)for each node n E oldsetstate(n) ~ Ofor each node n E newset dostate(n) ~- gG(C, n)if state(n) ?
F and next(state(n), j  #.l_ then doif rule(next(state(n), j)  = Othen next(state(n), j  --~ Yn ~ rule(next(state(n), j )ododmainC+- -T ; i , -1update(O, nodes of C, i)while H not empty doi~ -Hfor each node n E rule(i) s.t.
the root of lhs(ri)is in state(n) doS ~ the subtree of C matched by lhs(ri) at nS I *-- copy of rhs(ri)c ,-- c \ [ s / s ' \ ]update(node~ of S, lo~al(C, S'), i + 1)ododreturn C.Figure 4: Translation algorithm computing M(G)for a TTS G.Let Ci E ~T, 1 < i < 3, be as depicted in Figure 5.We write mij to denote the j - th node in a post-order enumeration of the nodes of Ci, 1 < i < 3 and1 < j < 7.
Assume that CI is the input tree.After the first call to procedure update, we havestate(m17) = qz0 = {n25} and state(m16) = qs ={nzh}; no other final state is associated with a nodeof C1.
We also have that ru le( l )= {m16}, rule(2) ={m17}, rule(3) = 0 and H contains indices 1 and 2.Index 1 is then retrieved from H and the onlynode in rule(l), i.e., mr6, is considered.
Since theroot of lhs(rz), i.e., node n15, belongs to q8, mz~passes the test in the head of the for-statement inthe main program.
Then rz is applied to C1, yieldingC2.
Observe that ml l  = m21 and m17 - -  m27; allthe remaining nodes of C2 are fresh nodes.The next call to update, associated with the appli-cation of r l ,  updates the associative list state in sucha way that state(m27) = q9 = {n35},  and no otherfinal state is associated with a node of C2.
Also, wenow have rule(l) = {m16}, rule(2)= {m27} (recallthat  m17 = m27) ,  rule(3) = {m27}, and H containsindices 2 and 3.Index 2 is next retrieved from H and node m27is considered.
However, at this point the root oflhs(r2), i.e., node n~5, does no longer belong tostate(m27), indicating that r~ is no longer applicableto that node.
The body of the for-statement in the259BC BA BBC BA CA DBA BAA DFigure 5: From left to right, top to bottom: trees C1,C2 and C3.
In the sample TTS G we have (C1, C3) EM(G), since C1 ~=~ C~ ~=~ C2 ~=~ Ca.main program is not executed this time.Finally, index 3 is retrieved from H and node m27is again considered, this time for the application ofrule r3.
Since the root of lhs(ra), i.e., node n35, be-longs to state(m27), r3 is applied to C2 at node m27,yielding C3.
Data structures are again updated bya call to procedure update with the second param-eter equal to 4.
Then state qs is associated withnode m37, the root node of C3.
Despite of the factthat qs E F,  we now have next(qs, 4) = _k.
There-fore rule rl is not considered for application to C3.Since H is now empty, the computation terminatesreturning C3.
\[\]The results in Lemma 1 and Lemma 2 can be usedto show that, in the main program, a node n passesthe test in the head of the for-statement if and onlyif lhs(ri) matches C at n. The correctness of Algo-r ithm 1 then follows from the definition of the heapdata structure.We now turn to computational complexity issues.Let p = maxl<i<_~lril.
For T e E T, let a lsot (T)be the total number of rules that are successfullyapplied on a run of Algorithm i on input T, countingrepetitions.Theorem 1 The running time of Algorithm 1 oninput tree T is 0(I TI + pt(T) log(t(T))).P roo f .
We can implement our data structures insuch a way that each of the primitive access oper-ations that are executed by the algorithm takes aconstant amount of time.Consider each instance of the membership of anode n in a set rule(i) and represent it as a pair(n, i).
We call active each pair (n, i) such that lhs(ri)matches C at n at the time i is retrieved from H. Asalready mentioned, these pairs pass the test in thehead of the for-loop in the main program.
The num-ber of active pairs is therefore t(T).
All remainingpairs are called dead.
Note that an active pair (n, i)can turn at most I lhs(r i ) I+hR active pairs into deadones, through a call to the procedure update.
Hencethe total number of dead pairs must be O(pt(T)).We conclude that the number of pairs total ly in-stantiated by the algorithm is O(pt(T)).It is easy to see that the number of pairs totMlyinstantiated by the algorithm is also a bound on thenumber of indices inserted in or retrieved from theheap.
Then the time spent by the algorithm withthe heap is O(pt(T) log(t(T))) (see for instance (Cor-men, Leiserson, and Rivest, 1990)).
The first cMlto the procedure update in the main program takestime proportional to \]T\[.
All remaining operationsof the algorithm will now be charged to some activepair.For each active pair, the body of the for-loop in themMn program and the body of the update procedureare executed, taking an amount of t ime O(p).
Foreach dead pair, only the test in the head of the for-loop is executed, taking a constant amount of time.This time is charged to the active node that turnedthe pair under consideration i to a dead one.
In thisway each active node is charged an extra amount oftime O(p).Every operation executed by the algorithm hasbeen considered in the above analysis.
We can thenconclude that the running time of Algorithm 1 isO( IT I  + pt(T) log(t(T))).
0Let us compare the above result with thetime performance of the standard algorithm fortransformation-based parsing.
The standard algo-r ithm checks each rule in R for application to aninitial parse tree T, trying to match the left-handside of the current rule at each node of T. Usingthe notation of Theorem 1, the running time is thenO(IrplTI).
In practical applications, t(T) and ITIare very close (of the order of the length of the in-put string).
Therefore we have achieved a time im-provement of a factor of ~r/log(t(T)).
We empha-size that ~r might be several hundreds large if thelearned transformations are lexicalized.
Thereforewe have improved the asymptotic t ime complexityof transformation-based parsing of a factor betweentwo to three orders of magnitude.4.2 Order -dependent  pars ingWe consider here the general case for the TTS trans-lation problem, in which the order of application ofseveral instances of rule r to a tree can affect the finalresult of the rewriting.
In this case rule r is calledcritical.
According to the definition of translationinduced by a TTS, a critical rule should always beapplied in post-order w.r.t, the nodes of the treeto be rewritten.
The solution we propose here forcritical rules is based on a preprocessing of the rulesequence of the system.We informally describe the technique presentedbelow.
Assume that a critical rule r is to be applied260at several matching nodes of a tree C. We partitionthe matching nodes into two sets.
The first set con-tains all the nodes n at which the matching of lhs(r)overlaps with a second matching at a node n' dom-inated by n. All the remaining matching nodes areinserted in the second set.
Then rule r is applied tothe nodes of the second set.
After that, the nodesin the first set are in turn partitioned according tothe above criterion, and the process is iterated untilall the matching nodes have been considered for ap-plication of r. This is more precisely stated in whatfollows.BBB cB CB CB CB CFigure 6: From left to right: trees Q and Qp.
Nodep of Q is indicated by underlying its label.We start with some additional notation.
Let r =(Q ~ Q') be a tree-rewriting rule.
Also, let p be anode of Q and let S be the suffix of Q at p. We saythat p is periodic if (i) p is not the root of Q; and(ii) S matches Q at the root node.
It is easy to seethat the fact that lhs(r) has some periodic node isa necessary condition for r to be critical.
Let theroot of S be the i-th child of a node n/ in Q, andlet Qc be acopyofQ.
We write Qp to denote thetree obtained starting from Q by excising S and byletting the root of Qc be the new i-th child of hi.Finally, call nl the root of Qp and n2 the root of Q.Example  5 Figure 6 depicts trees Q and Qp.
Theperiodic node p of Q under consideration is indicatedby underlying its label.
\[\]Let us assume that rule r is critical and that p isthe only periodic node in Q.
We add Qp to set lhs(R)and construct AG accordingly.
Algorithm 1 shouldthen be modified as follows.
We call p-chain anysequence of one or more subtrees of C, all matchedby Q, that partially overlap in C. Let n be a nodeof C and let q = state(n).
Assume that n2 E q andcall S the subtree of C at n matched by Q (S existsby Lemma 1).
We distinguish two possible cases.Case 1: If nl E q, then we know that Q also matchessome portion of C that overlaps with S (at the nodematched by the periodic node p of Q).
In this caseS belongs to a p-chain consisting of at least two sub-trees and S is not the bottom-most subtree in thep-chain.Case 2: If nt ~ q, then we know that S is thebottom-most ubtree in a p-chain.Let i be the index of rule r under consideration.We use an additional set chain(i).
Each node nof C such that n~ 6 state(n) is then inserted inchain(i) if state(n) satisfies Case 1 above, and isinserted in rule(i) otherwise.
Note that chain(i) isnon-empty only in case rule(i) is such.
Whenever i isretrieved from H, we process each node n in rule(i),as usual.
But when we update our data structureswith the procedure update, we also look for match-ings of lhs(ri) at nodes of C in chain(i).
The overalleffect of this is that each p-chain is considered in abottom-up fashion in the application of r. This iscompatible with the post-order application require-ment.The above technique can be applied for each peri-odic node in a critical rule, and for each critical ruleof G. This only affects the size of AG, not the timerequirements of Algorithm 1.
In fact, the proposedpreprocessing can at worst double ha.5 D iscuss ionIn this section we relate our work with the existingliterature and further discuss our result.There are several alternative ways in which onecould see transformation-based rewriting systems.TTS's  are closely related to a class of graph rewr.itingsystems called neighbourhood-controlled embeddinggraph grammars (N CE grammars; see (J anssens andRozenberg, 1982)).
In fact our definition of therelation and of the underlying \[/\] operator has beeninspired by similar definitions in the NCE formal-ism.
Apart from the restriction to tree rewriting, themain difference between NCE grammars and TTS'sis that in the latter formalism the productions aretotally ordered, therefore there is no recursion.Ordered trees can also be seen as ground terms.
Ifwe extend the alphabet ~ with variable symbols, wecan redefine the ~ relation through variable sub-stitution.
In this way a TTS becomes a particularkind of term-rewriting system.
The idea of imposinga total order on the rules of a term-rewriting systemcan be found in the literature, but in these cases allrules are reconsidered for application at each stepin the rewriting, using their priority (see for in-stance the priority term-rewriting systems (Baeten,Bergstra, and Klop, 1987)).
Therefore these systemsallow recursion.
There are cases in which a criticalrule in a TTS does not give rise to order-dependencyin rewriting.
Methods for deciding the confluencyproperty for a term-rewriting system with criticalpairs (see (Dershowitz and Jouannaud, 1990) for def-initions and an overview) can also be used to detectthe above cases for TTS.As already pointed out, the translation probleminvestigated here is closely related with the stan-dard tree pattern matching problem.
Our automataAG (Definition 3) can be seen as an abstraction ofthe bottom-up tree pattern matching algorithm pre-sented in (Hoffmann and O'Donnell, 1982).
Whilethat result uses a representation of the pattern set261(our set lhs(R)) requiring an amount of space whichis exponential in the degree of the pattern trees, asan improvement, our transition function does not de-pend on this parameter.
However, in the worst casethe space requirements of both algorithm are expo-nential in the number of nodes in lhs(R) (see theanalysis in (Hoffmann and O'Donnell, 1982)).
Asalready discussed in Section 3, the worst case condi-tion is hardly met in natural language applications.Polynomial space requirements can be guaranteedif one switches to top-down tree pattern matchingalgorithms.
One such a method is reported in (Hoff-mann and O'Donnell, 1982), but in this case therunning-time of Algorithm 1 cannot be maintained.Faster top-down matching algorithms have been re-ported in (Kosaraju, 1989) and (Dubiner, Galil, andMagen, 1994), but these methods eems impractical,due to very large hidden constants.A tree-based extension of the very fast algorithmdescribed in (Roche and Schabes, 1995) is in prin-ciple possible for transformation-based parsing, butis likely to result in huge space requirements andseems impractical.
The algorithm presented heremight hen be a good compromise between fast pars-ing and reasonable space requirements.When restricted to monadic trees, our automa-ton Ac comes down to the finite state device usedin the well-known string pattern matching algorithmof Aho and Corasick (see (Aho and Corasick, 1975)),requiring linear space only.
If space requirements areof primary importance or when the rule set is verylarge, our method can then be considered for string-based transformation rewriting as an alternative tothe already mentioned method in (Roche and Sch-abes, 1995), which is faster but has more onerousspace requirements.AcknowledgementsThe present research was done while the first authorwas visiting the Center for Language and SpeechProcessing, Johns Hopkins University, Baltimore,MD.
The second author is also a member of the Cen-ter for Language and Speech Processing.
This workwas funded in part by NSF grant IRI-9502312.
Theauthors are indebted with Alberto Apostolico, RaoKosaraju, Fernando Pereira and Murat Saraclar fortechnical discussions on topics related to this paper.The authors whish to thank an anonymous refereefor having pointed out important connections be-tween TTS and term-rewriting systems.Re ferencesAho, A. V. and M. Corasick.
1975.
Efficientstring matching: An aid to bibliographic search.Communications of the Association for Comput-ing Machinery, 18(6):333-340.Baeten, J., J. Bergstra, and 3.
Klop.
1987.
Prior-ity rewrite systems.
In Proc.
Second InternationalConference on Rewriting Techniques and Applica-tions, LNCS 256, pages 83-94, Berlin, Germany.Springer-Verlag.Brill, E. 1993.
Automatic grammar induction andparsing free text: A transformation-based ap-proach.
In Proceedings of the 31st Meeting of theAssociation of Computational Linguistics, Colum-bus, Oh.Brill, E. 1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part of speech tagging.
ComputationalLinguistics.Brill, E, and P. Resnik.
1994.
A transformation-based approach to prepositional phrase attach-ment disambiguation.
In Proceedings of theFifteenth International Conference on Computa-tional Linguistics (COLING-199~), Kyoto, Japan.Chomsky, N. 1965.
Aspects of the Theory of Syntax.The MIT Press, Cambridge, MA.Chomsky, N. and M. Halle.
1968.
The Sound Pat-tern of English.
Harper and Row.Cormen, T. H., C. E. Leiserson, and R. L. Rivest.1990.
Introduction to Algorithms.
The MIT Press,Cambridge, MA.Dershowitz, N. and J. Jouannaud.
1990.
Rewritesystems.
In J.
Van Leeuwen, editor, Handbookof Theoretical Computer Science, volume B. Else-vier and The MIT Press, Amsterdam, The Nether-lands and Cambridge, MA, chapter 6, pages 243-320.Dubiner, M., Z. Galil, and E. Magen.
1994.
Fastertree pattern matching.
Journal of the Associationfor Computing Machinery, 41(2):205-213.Hoffmann, C. M. and M. J. O'Donnell.
1982.
Pat-tern matching in trees.
Journal of the Associationfor Computing Machinery, 29(1):68-95.Janssens, D. and G. Rozenberg.
1982.
Graph gram-mars with neighbourhood-controlled embedding.Theoretical Computer Science, 21:55-74.Kaplan, R. M. and M. Kay.
1994.
Regular modelsof phonological rule sistems.
Computational Lin-guistics, 20(3):331-378.Kosaraju, S. R. 1989.
Efficient tree-pattern match-ing.
In Proceedings of the 30 Conference on Foun-dations of Computer Science (FOCS), pages 178-183.Roche, E. and Y. Schabes.
1995.
Deterministic partof speech tagging with finite state transducers.Computational Linguistics.Thatcher, J. W. 1967.
Characterizing derivationtrees of context-free grammars through a general-ization of finite automata theory.
Journal of Com-puter and System Science, 1:317-322.262
