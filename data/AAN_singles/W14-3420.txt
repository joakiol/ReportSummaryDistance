Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 134?142,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsUsing statistical parsing to detect agrammatic aphasiaKathleen C. Fraser1, Graeme Hirst1, Jed A. Meltzer2,Jennifer E. Mack3, and Cynthia K. Thompson3,4,51Dept.
of Computer Science, University of Toronto2Rotman Research Institute, Baycrest Centre, Toronto3Dept.
of Communication Sciences and Disorders, Northwestern University4Dept.
of Neurology, Northwestern University4Cognitive Neurology and Alzheimer?s Disease Center, Northwestern University{kfraser,gh}@cs.toronto.edu, jmeltzer@research.baycrest.org{jennifer-mack-0,ckthom}@northwestern.eduAbstractAgrammatic aphasia is a serious languageimpairment which can occur after a strokeor traumatic brain injury.
We presentan automatic method for analyzing apha-sic speech using surface level parse fea-tures and context-free grammar produc-tion rules.
Examining these features in-dividually, we show that we can uncovermany of the same characteristics of agram-matic language that have been reportedin studies using manual analysis.
Whentaken together, these parse features canbe used to train a classifier to accuratelypredict whether or not an individual hasaphasia.
Furthermore, we find that theparse features can lead to higher classifica-tion accuracies than traditional measuresof syntactic complexity.
Finally, we findthat a minimal amount of pre-processingcan lead to better results than using eitherthe raw data or highly processed data.1 IntroductionAfter a stroke or head injury, individuals mayexperience aphasia, an impairment in the abilityto comprehend or produce language.
The typeof aphasia depends on the location of the lesion.However, even two patients with the same typeof aphasia may experience different symptoms.
Acareful analysis of narrative speech can reveal spe-cific patterns of impairment, and help a cliniciandetermine whether an individual has aphasia, whattype of aphasia it is, and how the symptoms arechanging over time.In this paper, we present an automatic methodfor the analysis of one type of aphasia, agram-matic aphasia.
characterized by the omission offunction words, the omission or substitution ofmorphological markers for person and number, theabsence of verb inflection, and a relative increasein the number of nouns and decrease in the numberof verbs (Bastiaanse and Thompson, 2012).
Thereis often a reduction in the variety of different syn-tactic structures used, as well as a reduction in thecomplexity of those structures (Progovac, 2006).There may also be a strong tendency to use thecanonical word order of a language, for examplesubject-verb-object in English (Progovac, 2006).Most studies of narrative speech in agrammaticaphasia are based on manually annotated speechtranscripts.
This type of analysis can provide de-tailed and accurate information about the speechpatterns that are observed.
However, it is also verytime consuming and requires trained transcribersand annotators.
Studies are necessarily limited toa manageable size, and the level of agreement be-tween annotators can vary.We propose an automatic approach that uses in-formation from statistical parsers to examine prop-erties of narrative speech.
We extract context-free grammar (CFG) production rules as well asphrase-level features from syntactic parses of thespeech transcripts.
We show that this approach candetect many features which have been previouslyreported in the aphasia literature, and that classifi-cation of agrammatic patients and controls can beachieved with high accuracy.We also examine the effects of including speechdysfluencies in the transcripts.
Dysfluencies andnon-narrative words are usually removed from thetranscripts as a pre-processing step, but we showthat by retaining some of these items, we can ac-tually achieve a higher classification accuracy thanby using the completely clean transcripts.Finally, we investigate whether there is any ben-efit to using the parse features instead of more tra-ditional measures of syntactic complexity, such asYngve depth or mean sentence length.
We findthat the parse features convey more information134about the specific syntactic structures being pro-duced (or avoided) by the agrammatic speakers,and lead to better classification accuracies.2 Related Work2.1 Syntactic analysis of agrammaticnarrative speechMuch of the previous work analyzing narrativespeech in agrammatic aphasia has been performedmanually.
One widely used protocol is calledQuantitative Production Analysis (QPA), devel-oped by Saffran et al.
(1989).
QPA can be used tomeasure morphological content, such as whetherdeterminers and verb inflections are produced inobligatory contexts, as well as structural complex-ity, such as the number of embedded clauses persentence.
Subsequent studies have found a num-ber of differences between normal and agrammaticspeech using QPA (Rochon et al., 2000).
Anotherpopular protocol called the Northwestern Narra-tive Language Analysis (NNLA) was introducedby Thompson et al.
(1995).
This protocol analyzeseach utterance at five different levels, and focusesin particular on the production of verbs and verbargument structure.Perhaps more analogous to our work here,Goodglass et al.
(1994) conducted a detailed ex-amination of the syntactic constituents used byaphasic patients and controls.
In that study, utter-ances were grouped according to how many syn-tactic constituents they contained.
They foundthat agrammatic participants were more likely toproduce single-constituent utterances, especiallynoun phrases, and less likely to produce subor-dinate clauses.
They also found that agrammaticspeakers sometimes produced two-constituent ut-terances consisting of only a subject and object,with no verb.
This pattern was never observed incontrol speech.A much smaller body of work explores the useof computational techniques to analyze agramma-tism.
Holmes and Singh (1996) analyzed conver-sational speech from aphasic speakers and con-trols.
Their features mostly included measuresof vocabulary richness and frequency counts ofvarious parts-of-speech (e.g.
nouns, verbs); how-ever they also measured ?clause-like semantic unitrate?.
This feature was intended to measure thespeaker?s ability to cluster words together, al-though it is not clear what the criteria for segment-ing clause-like units were or whether it was donemanually or automatically.
Nonetheless, it wasfound to be one of the most important variablesfor distinguishing between patients and controls.MacWhinney et al.
(2011) presented several ex-amples of how researchers can use the Aphasia-Bank1database and associated software tools toconduct automatic analyses (although the tran-scripts are first hand-coded for errors by experi-enced speech-language pathologists).
Specificallywith regards to syntax, they calculated several fre-quency counts and ratios for different parts-of-speech and bound morphemes.
There was oneextension beyond treating each word individually:this involved searching for pre-defined colloca-tions such as once upon a time or happily ever af-ter, which were found to occur more rarely in thepatient transcripts than in the control transcripts.We present an alternative, automated method ofanalysis.
We do not attempt to fully replicate theresults of the manual studies, but rather providea complementary set of features which can indi-cate grammatic abnormalities.
Unlike previouscomputational studies, we attempt to move beyondsingle-word analysis and examine which patternsof syntax might indicate agrammatism.2.2 Using parse features to assessgrammaticalitySyntactic complexity metrics derived from parsetrees have been used by various researchers instudies of mild cognitive impairment (Roark et al.,2011), autism (Prud?hommeaux et al., 2011), andchild language development (Sagae et al., 2005;Hassanali et al., 2013).
Here we focus specificallyon the use of CFG production rules as features.Using the CFG production rules from statisticalparsers as features was first proposed by Baayenet al.
(1996), who applied the features to an au-thorship attribution task.
More recently, similarfeatures have been widely used in native languageidentification (Wong and Dras, 2011; Brooke andHirst, 2012; Swanson and Charniak, 2012).
Per-haps most relevant to the task at hand, CFG pro-ductions as well as other parse outputs have proveduseful for judging the grammaticality and fluencyof sentences.
For example, Wong and Dras (2010)used CFG productions to classify sentences froman artificial error corpus as being either grammat-ical or ungrammatical.Taking a different approach, Chae and Nenkova1http://talkbank.org/AphasiaBank/135Agrammatic(N = 24)Control(N = 15)Male/Female 15/9 8/7Age (years) 58.1 (10.6) 63.3 (6.4)Education (years) 16.3 (2.5) 16.4 (2.4)Table 1: Demographic information.
Numbers aregiven in the form: mean (standard deviation).
(2009) calculated several surface features based onthe output of a parser, such as the length and rel-ative proportion of different phrase types.
Theyused these features to distinguish between humanand machine translations, and to determine whichof a pair of translations was the more fluent.
How-ever, to our knowledge there has been no work us-ing parser outputs to assess the grammaticality ofspeech from individuals with post-stroke aphasia.3 Data3.1 ParticipantsThis was a retrospective analysis of data col-lected by the the Aphasia and Neurolinguistics Re-search Laboratory at Northwestern University.
Allagrammatic participants had experienced a strokeat least 1 year prior to the narrative sample col-lection.
Demographic information for the partic-ipants is given in Table 1.
There is no significant(p < 0.05) difference between the patient and con-trol groups on age or level of education.3.2 Narrative taskTo obtain a narrative sample, the participants wereasked to relate the well-known fairy tale Cin-derella.
Each participant was first given a word-less picture book of the story to look through.
Thebook was then removed, and the participant wasasked to tell the story in his or her own words.
Theexaminer did not interrupt or ask questions.The narratives were recorded and later tran-scribed following the NNLA protocol.
The datawas segmented into utterances based on syntac-tic and prosodic cues.
Filled pauses, repetitions,false starts, and revisional phrases (e.g.
I mean)were all placed inside parentheses.
The averagelength of the raw transcripts was 332 words foragrammatic participants and 387 words for con-trols; when the non-narrative words were excludedthe average length was 194 words for the agram-matic group and 330 for controls.4 Methods4.1 Parser FeaturesWe consider two types of features: CFG pro-duction rules and phrase-level statistics.
For theCFG production rules, we use the Charniak parser(Charniak, 2000) trained on Wall Street Journaldata to parse each utterance in the transcript andthen extract the set of non-lexical productions.The total number of types of productions is large,many of them occurring very infrequently, so wecompile a list of the 50 most frequently occurringproductions in each of the two groups (agrammaticand controls) and use the combined set as the setof features.
The feature values can be binary (doesa particular production rule appear in the narrativeor not?)
or integer (how many times does a rule oc-cur?).
The CFG non-terminal symbols follow thePenn Treebank naming conventions.For our phrase-level statistics, we use a subsetof the features described by Chae and Nenkova(2009), which are related to the incidence of dif-ferent phrase types.
We consider three differentphrase types: noun phrases, verb phrases, andprepositional phrases.
These features are definedas follows:?
Phrase type proportion: Length of eachphrase type (including embedded phrases),divided by total narrative length.?
Average phrase length: Total number ofwords in a phrase type, divided by numberof phrases of that type.?
Phrase type rate: Number of phrases of agiven type, divided by total narrative length.Because we are judging the grammaticality ofthe entire narrative, we normalize by narrativelength (rather than sentence length, as in Chae andNenkova?s study).
These features are real-valued.We first perform the analysis on the transcribeddata with the dysfluencies removed, labeled the?clean?
dataset.
This is the version of the tran-script that would be used in the manual NNLAanalysis.
However, it is the result of human ef-fort and expertise.
To test the robustness of thesystem on data that has not been annotated in thisway, we also use the ?raw?
dataset, with no dys-fluencies removed (i.e.
including everything insidethe parentheses), and an ?auto-cleaned?
dataset,in which filled pauses are automatically removedfrom the raw transcripts.
We also use a simple al-gorithm to remove ?stutters?
and false starts, by136removing non-word tokens of length one or two(e.g.
C- C- Cinderella would become simply Cin-derella).
This provides a more realistic view of theperformance of our system on real data.
We alsohypothesize that there may be important informa-tion to be found in the dysfluent speech segments.4.2 Feature weighting and selectionWe assume that some production rules will bemore relevant to the classification than others,and so we want to weight the features accord-ingly.
Using term frequency?inverse documentfrequency (tf-idf) would be one possibility; how-ever, the tf-idf weights do not take into accountany class information.
Supervised term weight-ing (STW), has been proposed by Debole and Se-bastiani (2004) as an alternative to tf-idf for textclassification tasks.
In this weighting scheme, fea-ture weights are assigned using the same algo-rithm that is used for feature selection.
For ex-ample, one way to select features is to rank themby their information gain (InfoGain).
In STW,the InfoGain value for each feature is also usedto replace the idf term.
This can be expressed asW (i,d) = df(i,d)?
InfoGain(i), where W (i,d) isthe weight assigned to feature i in document d,df(i,d) is the frequency of occurrence of feature iin document d, and InfoGain(i) is the informationgain of feature i across all the training documents.We considered two different methods of STW:weighting by InfoGain and weighting by gain ratio(GainRatio).
The methods were also used as fea-ture selection, since any feature that was assigneda weight of zero was removed from the classifi-cation.
We also consider tf-idf weights and un-weighted features for comparison.4.3 Syntactic complexity metricsTo compare the performance of the parse featureswith more-traditional syntactic complexity met-rics (SC metrics), we calculate the mean length ofutterance (MLU), mean length of T-unit2(MLT),mean length of clause (MLC), and parse treeheight.
We also calculate the mean, maximum,and total Yngve depth, which measures the pro-portion of left-branching to right-branching ineach parse tree (Yngve, 1960).
These measuresare commonly used in studies of impaired lan-guage (e.g.
Roark et al.
(2011), Prud?hommeaux et2A T-unit consists of a main clause and its attached de-pendent clauses.al.
(2011), Fraser et al.
(2013b)).
We hypothesizethat the parse features will capture more informa-tion about the specific impairments seen in agram-matic aphasia; however, using the general mea-sures of syntactic complexity may be sufficient forthe classifiers to distinguish between the groups.4.4 ClassificationTo test whether the features can effectively distin-guish between the agrammatic group and controls,we use them to train and test a machine learn-ing classifier.
We test three different classifica-tion algorithms: naive Bayes (NB), support vec-tor machine (SVM), and random forests (RF).
Weuse a leave-one-out cross-validation framework, inwhich one transcript is held out as a test set, andthe other transcripts form the training data.
Thefeature weights are calculated on the training setand then applied to the test set (as a result, eachfold of training/testing may use different featuresand feature weights).
The SVM and RF algo-rithms are tuned in a nested cross-validation loop.The classifier is then tested on the held-out point.This procedure is repeated across all data points,and the average accuracy is reported.A baseline classifier which assigns all data tothe largest class would achieve an accuracy of .62on this classification task.
For a more realisticmeasure of performance, we also compare our re-sults to the baseline accuracy that can be achievedusing only the length of the narrative as input.5 Results5.1 Features using clean transcriptsWe first present the results for the clean tran-scripts.
Although different features may be se-lected in each fold of the cross-validation, for sim-plicity we show only the feature rankings on thewhole data set.
Table 2 shows the top features asranked by GainRatio.
The frequencies are given toindicate the direction of the trend; they representthe average frequency per narrative for each class(agrammatic = AG and control = CT).
Boldfaceindicates the group with the higher frequency.
As-terisks are used to indicate the significance of thedifference between the groups.When working with clinical data, careful exam-ination of the features can be beneficial.
By com-paring features with previous findings in the liter-ature on agrammatism, we can be confident thatwe are measuring real effects and not just artifacts137Rule AGfreqCTfreqp1 PP?
IN NP 10.3 24.9??
?2 ROOT?
NP 2.9 0.2??
?3 NP?
DT NN POS 0.0 0.7?4 NP?
PRP$ JJ NN 0.5 0.7?5 VP?
TO VP 4.2 7.5?6 NP?
NNP 5.9 6.67 VP?
VB PP 1.1 2.9?
?8 VP?
VP CC VP 1.1 3.1?
?9 NP?
DT NN NN 1.0 2.7?
?10 VP?
VBD VP 0.1 0.5?11 WHADVP?WRB 0.5 1.4?12 FRAG?
NP .
0.7 0.0?
?13 NP?
JJ NN 0.7 0.0?
?14 SBAR?WHNP S 1.7 3.1?15 NP?
NP SBAR 1.6 2.516 S?
NP VP 7.8 16.1?
?17 NP?
PRP$ JJ NNS 0.0 0.5?18 NP?
PRP$ NN NNS 0.0 0.6?19 SBAR?WHADVP S 0.4 1.2?20 VP?
VBN PP 0.4 2.0?Table 2: Top 20 features ranked by GainRatio us-ing the clean transcripts.
(?p < 0.05,?
?p < 0.005,??
?p < 0.0005).of the parsing algorithm.
This can also poten-tially provide an opportunity to observe features ofagrammatic speech that have not been examined inmanual analyses.
We examine the top-ranked fea-tures in Table 2 in some detail, especially as theyrelate to previous work on agrammatism.
In par-ticular, the top features suggest some of the fol-lowing features of agrammatic speech:?
Reduced number of prepositional phrases.This is suggested by feature 1, PP?
IN NP.It is also reflected in features 7 and 20.?
Impairment in using verbs.
We can see in fea-ture 2 (ROOT ?
NP) that there is a greaternumber of utterances consisting of only anoun phrase.
Feature 12 is also consistentwith this pattern (FRAG ?
NP .).
We alsoobserve a reduced number of coordinatedverb phrases (VP?
VP CC VP).?
Omission of grammatical morphemes andfunction words.
The agrammatic speakersuse fewer possessives (NP?
DT NN POS).Feature 9 indicates that the control partic-ipants more frequently produce compoundNB SVM RFNarrative length .62 .56 .64Binary, no weights .87 .87 .77Binary, tf-idf .87 .90 .85Binary, InfoGain .82 .90 .74Binary, GainRatio .90 .82 .79Frequency, no weights .90 .85 .85Frequency, tf-idf .85 .82 .77Frequency, InfoGain .90 .90 .82Frequncy, GainRatio .90 .92 .74SC metrics, no weights .85 .77 .82SC metrics, InfoGain .85 .77 .79SC metrics, GainRatio .85 .77 .82Table 3: Average classification accuracy using theclean transcripts.
The highest classification accu-racy for each feature set is indicated with boldface.nouns with a determiner (often the glassslipper or the fairy godmother).
Feature 4also suggests some difficulty with determin-ers, as the agrammatic participants producefewer nouns modified by a possessive pro-noun and an adjective.
Contrast this with fea-ture 13, which shows agrammatic speech ismore likely to contain noun phrases contain-ing just an adjective and a noun.
For example,in the control narratives we are more likely tosee phrases such as her godmother .
.
.
wavesher magic wand, while in the agrammaticnarratives phrases like Cinderella had wickedstepmother are more common.?
Reduced number of embedded clauses andphrases.
Evidence for this can be found inthe reduced number of wh-adverb phrases(WHADVP?WRB), as well as features 14,15, and 19.The results of our classification experiment onthe clean data are shown in Table 3.
The resultsare similar for the binary and frequency features,with the best result of .92 achieved using an SVMclassifier and frequency features, with GainRatioweights.
The best results using parse features(.85?.92) are the same or slightly better than thebest results using SC features (.85), and both fea-ture sets perform above baseline.5.2 Effect of non-narrative speechIn this section we perform two additional experi-ments, using the raw and auto-cleaned transcripts.138Rule AGfreq.CTfreq.p1 NP?
DT NN POS 0.0 0.5?2 PP?
IN NP 12.2 26.1??
?3 SBAR?WHADVP S 0.4 1.5?4 VP?
VBD 0.75 1.15 VP?
TO VP 4.3 7.3?6 S?
CC PP NP VP .
0.04 0.5?7 NP?
PRP$ JJ NNS 0.04 0.5?8 VP?
AUX VP 3.7 6.09 ROOT?
FRAG 4.5 0.7?
?10 ADVP?
RB 9.8 12.311 NP?
NNP 4.4 6.2?12 NP?
DT NN 15.0 24.1?
?13 VP?
VB PP 1.2 2.8?14 VP?
VP CC VP 1.0 2.9?15 WHADVP?WRB 0.6 1.5?16 VP?
VBN PP 0.4 2.0?17 INTJ?
UH UH 3.5 0.3?18 VP?
VBP NP 0.5 0.0?19 NP?
NNP NNP 1.5 0.5?
?20 S?
CC ADVP NP VP .
1.3 2.3Table 4: Top 20 features ranked by GainRatiousing the raw transcripts.
Bold feature numbersindicate rules which did not appear in Table 2.
(?p < 0.05,?
?p < 0.005,??
?p < 0.0005).We discuss the differences between the selectedfeatures in each case, and the resulting classifica-tion accuracies.Using the raw transcripts, we find that the rank-ing of features is markedly different than with thehuman-annotated transcripts (Table 4, bold featurenumbers).
Examining these production rules moreclosely, we observe some characteristics of agram-matic speech which were not detectable in the an-notated transcripts:?
Increased number of dysfluencies.
We ob-serve a higher number of consecutive fillers(INTJ ?
UH UH) in the agrammatic data,as well as a higher number of consecutiveproper nouns (NP ?
NNP NNP), usuallytwo attempts at Cinderella?s name.
Feature18 (VP?
VBP NP) also appears to supportthis trend, although it is not immediately ob-vious.
Most of the control participants tellthe story in the past tense, and if they douse the present tense then the verbs are of-ten in the third-person singular (Cinderellafinds her fairy godmother).
Looking at thedata, we found that feature 18 can indicate averb agreement error, as in he attend the ball.However, in almost twice as many cases it in-dicates use of the discourse markers I meanand you know, followed by a repaired or tar-get noun phrase.?
Decreased connection between sentences.Feature 6 shows a canonical NP VP sentence,preceded by a coordinate conjunction and aprepositional phrase.
Some examples of thisfrom the control transcripts include, And atthe stroke of midnight .
.
.
and And in the pro-cess .
.
.
.
The conjunction creates a connec-tion from one utterance to the next, and theprepositional phrase indicates the temporalrelationship between events in the story, cre-ating a sense of cohesion.
See also the similarpattern in feature 20, representing sentencebeginnings such as And then .
.
.
.However, there are some features which werehighly ranked in the clean transcripts but do notappear in Table 4.
What information are we losingby using the raw data?
One issue with using theraw transcripts is that the inclusion of filled pauses?splits?
the counts for some features.
For example,the feature FRAG?
NP .
is ranked 12th using theclean transcripts but does not appear in the top 20when using the raw transcripts.
When we examinethe transcripts, we find that the phrases that arecounted in this feature in the clean transcripts areactually split into three features in the raw tran-scripts: FRAG?
NP ., FRAG?
INTJ NP ., andFRAG?
NP INTJ ..The classification results for the raw transcriptsare given in Table 5.
The results are similar tothose for the clean transcripts, although in thiscase the best accuracy (.92) is achieved in threedifferent configurations (all using the SVM clas-sifier).
The phrase-level features out-perform thetraditional SC measures in only half the cases.Using the auto-cleaned transcripts, we see somesimilarities with the previous cases (Table 6).However, some of the highly ranked featureswhich disappeared when using the raw transcriptsare now significant again (e.g.
ROOT ?
NP,FRAG ?
NP .).
There are also three remain-ing features which are significant and have not yetbeen discussed.
Feature 9 shows an increased useof determiners with proper nouns (e.g.
the Cin-derella), a frank grammatical error.
Feature 20139NB SVM RFNarrative length .51 .62 .69Binary, no weights .87 .92 .82Binary, tf-idf .87 .92 .72Binary, InfoGain .85 .87 .82Binary, GainRatio .82 .87 .85Frequency, no weights .85 .90 .69Frequency, tf-idf .82 .92 .90Frequency, InfoGain .85 .74 .85Frequncy, GainRatio .85 .74 .82SC metrics, no weights .74 .79 .82SC metrics, InfoGain .77 .85 .85SC metrics, GainRatio .77 .85 .87Table 5: Average classification accuracy usingraw transcripts.
The highest classification accu-racy for each feature set is indicated with boldface.provides another example of a sentence fragmentwith no verb.
Finally, feature 19 represents an in-creased number of sentences or clauses consist-ing of a noun phrase followed by adjective phrase.Looking at the transcripts, this is not generally in-dicative of an error, but rather use of the wordokay, as in she dropped her shoe okay.The classification results for the auto-cleaneddata, shown in Table 7, show a somewhat differ-ent pattern from the previous experiments.
Theaccuracies using the parse features are generallyhigher, and the best result of .97 is achieved usingthe binary features and the naive Bayes classifier.Interestingly, this data set also results in the lowestaccuracy for the syntactic complexity metrics.5.3 Phrase-level parse featuresThe classifiers in Tables 3, 5, and 7 used thephrase-level parse features as well as the CFGproductions.
Although these features were cal-culated for NPs, VPs, and PPs, the NP featureswere never selected by the GainRatio ranking al-gorithm, and did not differ significantly betweengroups.
The significance levels of the VP and PPfeatures are reported in Table 8.
PP rate and pro-portion are significantly different in all three setsof transcripts, which is consistent with the highranking of PP ?
IN NP in each case.
VP rateand proportion are often significant, although lessso.
Notably, PP and VP length are both significantin the clean transcripts, but not significant in theraw transcripts and only barely significant in theauto-cleaned transcripts.Rule AGfreq.CTfreq.p1 PP?
IN NP 12.0 26.0??
?2 NP?
DT NN POS 0.0 0.7?3 VP?
VP CC VP 0.8 2.9?
?4 S?
CC SBAR NP VP .
0.0 0.55 SBAR?WHADVP S 0.4 1.5?6 NP?
NNP 5.6 6.77 VP?
VBD 0.8 1.18 S?
CC PP NP VP .
0.04 0.6?9 NP?
DT NNP 0.6 0.0?
?10 VP?
TO VP 4.6 7.5?11 ROOT?
FRAG 3.0 0.5??
?12 ROOT?
NP 2.1 0.1?13 VP?
VBP NP 1.7 3.614 NP?
PRP$ JJ NNS 0.04 0.5?15 VP?
VB PP 1.1 2.8?
?16 VP?
VBN PP 0.4 1.9?17 FRAG?
NP .
0.4 0.0?18 NP?
NNP .
2.1 0.119 S?
NP ADJP 0.4 0.0?20 FRAG?
CC NP .
0.7 0.07?
?Table 6: Top 10 features ranked by GainRatiousing the auto-cleaned transcripts.
Bold featurenumbers indicate rules which did not appear in Ta-ble 2.
(?p < 0.05,?
?p < 0.005,??
?p < 0.0005).5.4 Analysis of varianceWith a multi-way ANOVA we found significantmain effects of classifier (F(2,63) = 11.6, p <0.001) and data set (F(2,63) = 11.2, p < 0.001)on accuracy.
A Tukey post-hoc test revealed sig-nificant differences between SVM and RF (p <0.001) and NB and RF (p < 0.001) but not be-tween SVM and NB.
As well, we see a sig-nificant difference between the clean and auto-cleaned data (p < 0.001) and the raw and auto-cleaned data (p < 0.001) but not between the rawand clean data.
There was no significant main ef-fect of weighting scheme or feature type (binary orfrequency) on accuracy.
We did not examine anypossible interactions between these variables.6 Discussion6.1 TranscriptsWe achieved the highest classification accuraciesusing the auto-cleaned transcripts.
The raw tran-scripts, while containing more information aboutdysfluent events, also seemed to cause more dif-140NB SVM RFNarrative length .51 .62 .64Binary, no weights .92 .95 .90Binary, tf-idf .92 .95 .87Binary, InfoGain .97 .90 .85Binary, GainRatio .97 .90 .95Frequency, no weights .90 .95 .77Frequency, tf-idf .87 .95 .79Frequency, InfoGain .92 .85 .82Frequncy, GainRatio .92 .87 .95SC metrics, no weights .79 .77 .74SC metrics, InfoGain .79 .74 .72SC metrics, GainRatio .79 .74 .67Table 7: Average classification accuracy usingauto-cleaned transcripts.
The highest classifica-tion accuracy for each feature set is indicated withboldface.Clean Raw AutoPP rate???
???
??
?PP proportion???
???
?
?PP length?
?VP rate??
?VP proportion???
?
?VP length???
?Table 8: Significance of the phrase-level featuresin each of the three data sets (?p < 0.05,?
?p <0.005,??
?p < 0.0005).ficulty for the parser, which mis-labelled filledpauses and false starts in some cases.
We alsofound that the insertion of filled pauses resultedin the creation of multiple features for a single un-derlying grammatical structure.
The auto-cleanedtranscripts appeared to avoid some of those prob-lems, while still retaining information about manyof the non-narrative speech productions that wereremoved from the clean transcripts.Some of the features from the auto-cleaned tran-scripts appear to be associated with the discourselevel of language, such as connectives and dis-course markers.
A researcher solely interested instudying the syntax of language might resist theinclusion of such features, and prefer to use onlyfeatures from the human-annotated clean tran-scripts.
However, we feel that such productionsare part of the grammar of spoken language, andmerit inclusion.
From a practical standpoint, ourfindings are reassuring: data preparation that canbe done automatically is much more feasible inmany situations than human annotation.6.2 FeaturesCFG production rules can offer a more detailedlook at specific language impairments.
We wereable to observe a number of important characteris-tics of agrammatic language as reported in previ-ous studies: fragmented speech with a higher in-cidence of solitary noun phrases, difficulty withdeterminers and possessives, reduced number ofprepositional phrases and embedded clauses, and(in the raw transcripts), increased use of filledpauses and repair phrases.
For this reason, we be-lieve that they are more useful for the analysis ofdisordered or otherwise atypical language than tra-ditional measures of syntactic complexity.In some cases an in-depth analysis may not berequired, and in such cases it may be tempting tosimply use one of the more-general syntactic com-plexity measures.
Nevertheless, even in our simplebinary classification task, we found that using themore-specific features gave us a higher accuracy.6.3 Future workBecause of the limited data, we consider these re-sults to be preliminary.
We hope to replicate thisstudy as more data become available in the fu-ture.
We also plan to examine the effect, if any,of the specific narrative task.
Furthermore, wehave shown that these methods are effective forthe analysis of agrammatic aphasia, but there areother types of aphasia in which semantic, ratherthan syntactic, processing is the primary impair-ment.
We would like to extend this work to findfeatures which distinguish between different typesof aphasia.Although we included manually transcribeddata in this study, these methods will be most use-ful if they are also effective on automatically rec-ognized speech.
Previous work on speech recog-nition for aphasic speech reported high error rates(Fraser et al., 2013a).
Our finding that the auto-cleaned transcripts led to the highest classificationaccuracy is encouraging, but we will have to testthe robustness to recognition errors and the depen-dence on sentence boundary annotations.AcknowledgmentsThis research was supported by the Natural Sciences and En-gineering Research Council of Canada and National Institutesof Health R01DC01948 and R01DC008552.141ReferencesHarald Baayen, Hans Van Halteren, and FionaTweedie.
1996.
Outside the cave of shadows:Using syntactic annotation to enhance authorshipattribution.
Literary and Linguistic Computing,11(3):121?132.Roelien Bastiaanse and Cynthia K. Thompson.
2012.Perspectives on Agrammatism.
Psychology Press.Julian Brooke and Graeme Hirst.
2012.
Robust, lex-icalized native language identification.
In Proceed-ings of the 24th International Conference on Com-putational Linguistics, pages 391?408.Jieun Chae and Ani Nenkova.
2009.
Predicting thefluency of text with shallow structural features: casestudies of machine translation and human-writtentext.
In Proceedings of the 12th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, pages 139?147.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 132?139.Franca Debole and Fabrizio Sebastiani.
2004.
Super-vised term weighting for automated text categoriza-tion.
In Text mining and its applications, pages 81?97.
Springer.Kathleen Fraser, Frank Rudzicz, Naida Graham, andElizabeth Rochon.
2013a.
Automatic speech recog-nition in the diagnosis of primary progressive apha-sia.
In Proceedings of the Fourth Workshop onSpeech and Language Processing for Assistive Tech-nologies, pages 47?54.Kathleen C. Fraser, Jed A. Meltzer, Naida L. Graham,Carol Leonard, Graeme Hirst, Sandra E. Black, andElizabeth Rochon.
2013b.
Automated classificationof primary progressive aphasia subtypes from narra-tive speech transcripts.
Cortex.Harold Goodglass, Julie Ann Christiansen, andRoberta E. Gallagher.
1994.
Syntatic construc-tions used by agrammatic speakers: Comparisonwith conduction aphasics and normals.
Neuropsy-chology, 8(4):598.Khairun-nisa Hassanali, Yang Liu, Aquiles Iglesias,Thamar Solorio, and Christine Dollaghan.
2013.Automatic generation of the index of productivesyntax for child language transcripts.
Behavior re-search methods, pages 1?9.David I. Holmes and Sameer Singh.
1996.
A stylo-metric analysis of conversational speech of apha-sic patients.
Literary and Linguistic Computing,11(3):133?140.Brian MacWhinney, Davida Fromm, Margaret Forbes,and Audrey Holland.
2011.
Aphasiabank: Methodsfor studying discourse.
Aphasiology, 25(11):1286?1307.Ljiljana Progovac.
2006.
The Syntax of Nonsen-tentials: Multidisciplinary Perspectives, volume 93.John Benjamins.Emily T. Prud?hommeaux, Brian Roark, Lois M.Black, and Jan van Santen.
2011.
Classification ofatypical language in autism.
In Proceedings of the2nd Workshop on Cognitive Modeling and Compu-tational Linguistics, CMCL ?11, pages 88?96.Brian Roark, Margaret Mitchell, John-Paul Hosom,Kristy Hollingshead, and Jeffery Kaye.
2011.
Spo-ken language derived measures for detecting mildcognitive impairment.
IEEE Transactions on Au-dio, Speech, and Language Processing, 19(7):2081?2090.Elizabeth Rochon, Eleanor M. Saffran, Rita SloanBerndt, and Myrna F. Schwartz.
2000.
Quantita-tive analysis of aphasic sentence production: Furtherdevelopment and new data.
Brain and Language,72(3):193?218.Eleanor M. Saffran, Rita Sloan Berndt, and Myrna F.Schwartz.
1989.
The quantitative analysis ofagrammatic production: Procedure and data.
Brainand Language, 37(3):440?479.Kenji Sagae, Alon Lavie, and Brian MacWhinney.2005.
Automatic measurement of syntactic develop-ment in child language.
In Proceedings of the 43rdAnnual Meeting on Association for ComputationalLinguistics, pages 197?204.Ben Swanson and Eugene Charniak.
2012.
Native lan-guage detection with tree substitution grammars.
InProceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics, pages 193?197.Cynthia K. Thompson, Lewis P. Shapiro, Ligang Li,and Lee Schendel.
1995.
Analysis of verbs andverb-argument structure: A method for quantifica-tion of aphasic language production.
Clinical Apha-siology, 23:121?140.Sze-Meng Jojo Wong and Mark Dras.
2010.
Parserfeatures for sentence grammaticality classification.In Proceedings of the Australasian Language Tech-nology Association Workshop, pages 67?75.Sze-Meng Jojo Wong and Mark Dras.
2011.
Exploit-ing parse structures for native language identifica-tion.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1600?1610.Victor Yngve.
1960.
A model and hypothesis for lan-guage structure.
Proceedings of the American Phys-ical Society, 104:444?466.142
