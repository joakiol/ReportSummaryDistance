Proceedings of ACL-08: HLT, pages 263?271,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsLearning Document-Level Semantic Properties from Free-text AnnotationsS.R.K.
Branavan Harr Chen Jacob Eisenstein Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{branavan, harr, jacobe, regina}@csail.mit.eduAbstractThis paper demonstrates a new method forleveraging free-text annotations to infer se-mantic properties of documents.
Free-text an-notations are becoming increasingly abundant,due to the recent dramatic growth in semi-structured, user-generated online content.
Anexample of such content is product reviews,which are often annotated by their authorswith pros/cons keyphrases such as ?a real bar-gain?
or ?good value.?
To exploit such noisyannotations, we simultaneously find a hid-den paraphrase structure of the keyphrases, amodel of the document texts, and the underly-ing semantic properties that link the two.
Thisallows us to predict properties of unannotateddocuments.
Our approach is implemented asa hierarchical Bayesian model with joint in-ference, which increases the robustness of thekeyphrase clustering and encourages the doc-ument model to correlate with semanticallymeaningful properties.
We perform severalevaluations of our model, and find that it sub-stantially outperforms alternative approaches.1 IntroductionA central problem in language understanding istransforming raw text into structured representa-tions.
Learning-based approaches have dramaticallyincreased the scope and robustness of this type ofautomatic language processing, but they are typi-cally dependent on large expert-annotated datasets,which are costly to produce.
In this paper, we showhow novice-generated free-text annotations avail-able online can be leveraged to automatically inferdocument-level semantic properties.With the rapid increase of online content cre-ated by end users, noisy free-text annotations havepros/cons: great nutritional value... combines it all: an amazing product, quick andfriendly service, cleanliness, great nutrition ...pros/cons: a bit pricey, healthy... is an awesome place to go if you are health con-scious.
They have some really great low calorie dishesand they publish the calories and fat grams per serving.Figure 1: Excerpts from online restaurant reviews withpros/cons phrase lists.
Both reviews discuss healthiness,but use different keyphrases.become widely available (Vickery and Wunsch-Vincent, 2007; Sterling, 2005).
For example, con-sider reviews of consumer products and services.Often, such reviews are annotated with keyphraselists of pros and cons.
We would like to use thesekeyphrase lists as training labels, so that the proper-ties of unannotated reviews can be predicted.
Hav-ing such a system would facilitate structured accessand summarization of this data.
However, novice-generated keyphrase annotations are incomplete de-scriptions of their corresponding review texts.
Fur-thermore, they lack consistency: the same under-lying property may be expressed in many ways,e.g., ?healthy?
and ?great nutritional value?
(see Fig-ure 1).
To take advantage of such noisy labels, a sys-tem must both uncover their hidden clustering intoproperties, and learn to predict these properties fromreview text.This paper presents a model that addresses bothproblems simultaneously.
We assume that both thedocument text and the selection of keyphrases aregoverned by the underlying hidden properties of thedocument.
Each property indexes a language model,thus allowing documents that incorporate the same263property to share similar features.
In addition, eachkeyphrase is associated with a property; keyphrasesthat are associated with the same property shouldhave similar distributional and surface features.We link these two ideas in a joint hierarchicalBayesian model.
Keyphrases are clustered basedon their distributional and lexical properties, and ahidden topic model is applied to the document text.Crucially, the keyphrase clusters and document top-ics are linked, and inference is performed jointly.This increases the robustness of the keyphrase clus-tering, and ensures that the inferred hidden topicsare indicative of salient semantic properties.Our model is broadly applicable to many scenar-ios where documents are annotated in a noisy man-ner.
In this work, we apply our method to a col-lection of reviews in two categories: restaurants andcell phones.
The training data consists of review textand the associated pros/cons lists.
We then evaluatethe ability of our model to predict review propertieswhen the pros/cons list is hidden.
Across a varietyof evaluation scenarios, our algorithm consistentlyoutperforms alternative strategies by a wide margin.2 Related WorkReview Analysis Our approach relates to previouswork on property extraction from reviews (Popescuet al, 2005; Hu and Liu, 2004; Kim and Hovy,2006).
These methods extract lists of phrases, whichare analogous to the keyphrases we use as inputto our algorithm.
However, our approach is dis-tinguished in two ways: first, we are able to pre-dict keyphrases beyond those that appear verbatimin the text.
Second, our approach learns the rela-tionships between keyphrases, allowing us to drawdirect comparisons between reviews.Bayesian Topic Modeling One aspect of ourmodel views properties as distributions over wordsin the document.
This approach is inspired by meth-ods in the topic modeling literature, such as LatentDirichlet Allocation (LDA) (Blei et al, 2003), wheretopics are treated as hidden variables that govern thedistribution of words in a text.
Our algorithm ex-tends this notion by biasing the induced hidden top-ics toward a clustering of known keyphrases.
Tyingthese two information sources together enhances therobustness of the hidden topics, thereby increasingthe chance that the induced structure corresponds tosemantically meaningful properties.Recent work has examined coupling topic mod-els with explicit supervision (Blei and McAuliffe,2007; Titov and McDonald, 2008).
However, suchapproaches assume that the documents are labeledwithin a predefined annotation structure, e.g., theproperties of food, ambiance, and service for restau-rants.
In contrast, we address free-text annotationscreated by end users, without known semantic prop-erties.
Rather than requiring a predefined annotationstructure, our model infers one from the data.3 Problem FormulationWe formulate our problem as follows.
We assumea dataset composed of documents with associatedkeyphrases.
Each document may be marked withmultiple keyphrases that express unseen semanticproperties.
Across the entire collection, severalkeyphrases may express the same property.
Thekeyphrases are also incomplete ?
review texts of-ten express properties that are not mentioned in theirkeyphrases.
At training time, our model has accessto both text and keyphrases; at test time, the goal isto predict the properties supported by a previouslyunseen document.
We can then use this property listto generate an appropriate set of keyphrases.4 Model DescriptionOur approach leverages both keyphrase clusteringand distributional analysis of the text in a joint, hi-erarchical Bayesian model.
Keyphrases are drawnfrom a set of clusters; words in the documents aredrawn from language models indexed by a set oftopics, where the topics correspond to the keyphraseclusters.
Crucially, we bias the assignment of hid-den topics in the text to be similar to the topics rep-resented by the keyphrases of the document, but wepermit some words to be drawn from other topicsnot represented by the keyphrases.
This flexibility inthe coupling allows the model to learn effectively inthe presence of incomplete keyphrase annotations,while still encouraging the keyphrase clustering tocohere with the topics supported by the text.We train the model on documents annotated withkeyphrases.
During training, we learn a hiddentopic model from the text; each topic is also asso-264?
?
keyphrase cluster modelx ?
keyphrase cluster assignments ?
keyphrase similarity valuesh ?
document keyphrases?
?
document keyphrase topics?
?
probability of selecting ?
instead of ?c ?
selects between ?
and ?
for word topics?
?
document topic modelz ?
word topic assignment?
?
language models of each topicw ?
document words?
?
Dirichlet(?0)x?
?
Multinomial(?)s?,??
?
{Beta(?=) if x?
= x?
?Beta(?6=) otherwise?d = [?d,1 .
.
.
?d,K ]Twhere?d,k ?
{1 if x?
= k for any l ?
hd0 otherwise?
?
Beta(?0)cd,n ?
Bernoulli(?
)?d ?
Dirichlet(?0)zd,n ?
{Multinomial(?d) if cd,n = 1Multinomial(?d) otherwise?k ?
Dirichlet(?0)wd,n ?
Multinomial(?zd,n)Figure 2: The plate diagram for our model.
Shaded circles denote observed variables, and squares denote hyperparameters.
The dotted arrows indicate that ?
is constructed deterministically from x and h.ciated with a cluster of keyphrases.
At test time,we are presented with documents that do not con-tain keyphrase annotations.
The hidden topic modelof the review text is used to determine the proper-ties that a document as a whole supports.
For eachproperty, we compute the proportion of the docu-ment?s words assigned to it.
Properties with propor-tions above a set threshold (tuned on a developmentset) are predicted as being supported.4.1 Keyphrase ClusteringOne of our goals is to cluster the keyphrases, suchthat each cluster corresponds to a well-defined prop-erty.
We represent each distinct keyphrase as a vec-tor of similarity scores computed over the set ofobserved keyphrases; these scores are representedby s in Figure 2, the plate diagram of our model.1Modeling the similarity matrix rather than the sur-1We assume that similarity scores are conditionally inde-pendent given the keyphrase clustering, though the scores arein fact related.
Such simplifying assumptions have been previ-ously used with success in NLP (e.g., Toutanova and Johnson,2007), though a more theoretically sound treatment of the sim-ilarity matrix is an area for future research.face forms allows arbitrary comparisons betweenkeyphrases, e.g., permitting the use of both lexicaland distributional information.
The lexical com-parison is based on the cosine similarity betweenthe keyphrase words.
The distributional similar-ity is quantified in terms of the co-occurrence ofkeyphrases across review texts.
Our model is inher-ently capable of using any arbitrary source of simi-larity information; for a discussion of similarity met-rics, see Lin (1998).4.2 Document-level Distributional AnalysisOur analysis of the document text is based on proba-bilistic topic models such as LDA (Blei et al, 2003).In the LDA framework, each word is generated froma language model that is indexed by the word?s topicassignment.
Thus, rather than identifying a singletopic for a document, LDA identifies a distributionover topics.Our word model operates similarly, identifying atopic for each word, written as z in Figure 2.
Totie these topics to the keyphrases, we deterministi-cally construct a document-specific topic distribu-265tion from the clusters represented by the document?skeyphrases ?
this is ?
in the figure.
?
assigns equalprobability to all topics that are represented in thekeyphrases, and a small smoothing probability toother topics.As noted above, properties may be expressed inthe text even when no related keyphrase appears.
Forthis reason, we also construct a document-specifictopic distribution ?.
The auxiliary variable c indi-cates whether a given word?s topic is drawn fromthe set of keyphrase clusters, or from this topic dis-tribution.4.3 Generative ProcessIn this section, we describe the underlying genera-tive process more formally.First we consider the set of all keyphrases ob-served across the entire corpus, of which there areL.
We draw a multinomial distribution ?
over the Kkeyphrase clusters from a symmetric Dirichlet prior?0.
Then for the ?th keyphrase, a cluster assign-ment x?
is drawn from the multinomial ?.
Finally,the similarity matrix s ?
[0, 1]L?L is constructed.Each entry s?,??
is drawn independently, dependingon the cluster assignments x?
and x??
.
Specifically,s?,??
is drawn from a Beta distribution with parame-ters ?= if x?
= x??
and ?6= otherwise.
The parame-ters ?= linearly bias s?,??
towards one (Beta(?=) ?Beta(2, 1)), and the parameters ?6= linearly bias s?,?
?towards zero (Beta(?6=) ?
Beta(1, 2)).Next, the words in each of the D documentsare generated.
Document d has Nd words; zd,n isthe topic for word wd,n.
These latent topics aredrawn either from the set of clusters represented bythe document?s keyphrases, or from the document?stopic model ?d.
We deterministically construct adocument-specific keyphrase topic model ?d, basedon the keyphrase cluster assignments x and the ob-served keyphrases hd.
The multinomial ?d assignsequal probability to each topic that is represented bya phrase in hd, and a small probability to other top-ics.As noted earlier, a document?s text may supportproperties that are not mentioned in its observedkeyphrases.
For that reason, we draw a documenttopic multinomial ?d from a symmetric Dirichletprior ?0.
The binary auxiliary variable cd,n deter-mines whether the word?s topic is drawn from thekeyphrase model ?d or the document topic model?d.
cd,n is drawn from a weighted coin flip, withprobability ?
; ?
is drawn from a Beta distributionwith prior ?0.
We have zd,n ?
?d if cd,n = 1,and zd,n ?
?d otherwise.
Finally, the word wd,nis drawn from the multinomial ?zd,n , where zd,n in-dexes a topic-specific language model.
Each of theK language models ?k is drawn from a symmetricDirichlet prior ?0.5 Posterior SamplingUltimately, we need to compute the model?s poste-rior distribution given the training data.
Doing soanalytically is intractable due to the complexity ofthe model, but sampling-based techniques can beused to estimate the posterior.
We employ Gibbssampling, previously used in NLP by Finkel et al(2005) and Goldwater et al (2006), among others.This technique repeatedly samples from the condi-tional distributions of each hidden variable, eventu-ally converging on a Markov chain whose stationarydistribution is the posterior distribution of the hid-den variables in the model (Gelman et al, 2004).We now present sampling equations for each of thehidden variables in Figure 2.The prior over keyphrase clusters ?
is sampledbased on hyperprior ?0 and keyphrase cluster as-signments x.
We write p(?
| .
.
.)
to mean the prob-ability conditioned on all the other variables.p(?
| .
.
.)
?
p(?
| ?0)p(x | ?
),= p(?
| ?0)L??p(x?
| ?
)= Dir(?;?0)L??Mul(x?;?
)= Dir(?;??
),where ?
?i = ?0 + count(x?
= i).
This update ruleis due to the conjugacy of the multinomial to theDirichlet distribution.
The first line follows fromBayes?
rule, and the second line from the conditionalindependence of each keyphrase assignment x?
fromthe others, given ?.
?d and ?k are resampled in a similar manner:p(?d | .
.
.)
?
Dir(?d;?
?d),p(?k | .
.
.)
?
Dir(?k; ??k),266p(x?
| .
.
.)
?
p(x?
| ?
)p(s | x?,x?
?, ?
)p(z | ?, ?, c)?
p(x?
| ?)?????
6=?p(s?,??
| x?, x??
, ?)???
?D?d?cd,n=1p(zd,n | ?d)?
?= Mul(x?;?)?????
6=?Beta(s?,??
;?x?,x??
)???
?D?d?cd,n=1Mul(zd,n; ?d)?
?Figure 3: The resampling equation for the keyphrase cluster assignments.where ?
?d,i = ?0 + count(zd,n = i ?
cd,n = 0)and ?
?k,i = ?0 +?d count(wd,n = i ?
zd,n = k).
Inbuilding the counts for ?
?d,i, we consider only casesin which cd,n = 0, indicating that the topic zd,n isindeed drawn from the document topic model ?d.Similarly, when building the counts for ?
?k, we con-sider only cases in which the word wd,n is drawnfrom topic k.To resample ?, we employ the conjugacy of theBeta prior to the Bernoulli observation likelihoods,adding counts of c to the prior ?0.p(?
| .
.
.)
?
Beta(?;??
),where ??
= ?0 +[ ?d count(cd,n = 1)?d count(cd,n = 0)].The keyphrase cluster assignments are repre-sented by x, whose sampling distribution dependson ?, s, and z, via ?.
The equation is shown in Fig-ure 3.
The first term is the prior on x?.
The secondterm encodes the dependence of the similarity ma-trix s on the cluster assignments; with slight abuse ofnotation, we write ?x?,x??
to denote ?= if x?
= x??
,and ?6= otherwise.
The third term is the dependenceof the word topics zd,n on the topic distribution ?d.We compute the final result of Figure 3 for each pos-sible setting of x?, and then sample from the normal-ized multinomial.The word topics z are sampled according tokeyphrase topic distribution ?d, document topic dis-tribution ?d, words w, and auxiliary variables c:p(zd,n | .
.
.)?
p(zd,n | ?d, ?d, cd,n)p(wd,n | zd,n, ?
)={Mul(zd,n; ?d)Mul(wd,n; ?zd,n) if cd,n = 1,Mul(zd,n;?d)Mul(wd,n; ?zd,n) otherwise.As with x?, each zd,n is sampled by computingthe conditional likelihood of each possible settingwithin a constant of proportionality, and then sam-pling from the normalized multinomial.Finally, we sample each auxiliary variable cd,n,which indicates whether the hidden topic zd,n isdrawn from ?d or ?d.
The conditional probabilityfor cd,n depends on its prior ?
and the hidden topicassignments zd,n:p(cd,n | .
.
.)?
p(cd,n | ?
)p(zd,n | ?d, ?d, cd,n)={Bern(cd,n;?
)Mul(zd,n; ?d) if cd,n = 1,Bern(cd,n;?
)Mul(zd,n;?d) otherwise.We compute the likelihood of cd,n = 0 and cd,n = 1within a constant of proportionality, and then samplefrom the normalized Bernoulli distribution.6 Experimental SetupData Sets We evaluate our system on reviews fromtwo categories, restaurants and cell phones.
Thesereviews were downloaded from the popular Epin-ions2 website.
Users of this website evaluate prod-ucts by providing both a textual description of theiropinion, as well as concise lists of keyphrases (prosand cons) summarizing the review.
The statistics ofthis dataset are provided in Table 1.
For each ofthe categories, we randomly selected 50%, 15%, and35% of the documents as training, development, andtest sets, respectively.Manual analysis of this data reveals that authorsoften omit properties mentioned in the text fromthe list of keyphrases.
To obtain a complete gold2http://www.epinions.com/267Restaurants Cell Phones# of reviews 3883 1112Avg.
review length 916.9 1056.9Avg.
keyphrases / review 3.42 4.91Table 1: Statistics of the reviews dataset by category.standard, we hand-annotated a subset of the reviewsfrom the restaurant category.
The annotation effortfocused on eight commonly mentioned properties,such as those underlying the keyphrases ?pleasantatmosphere?
and ?attentive staff.?
Two raters anno-tated 160 reviews, 30 of which were annotated byboth.
Cohen?s kappa, a measure of interrater agree-ment ranging from zero to one, was 0.78 for this sub-set, indicating high agreement (Cohen, 1960).Each review was annotated with 2.56 propertieson average.
Each manually-annotated property cor-responded to an average of 19.1 keyphrases in therestaurant data, and 6.7 keyphrases in the cell phonedata.
This supports our intuition that a single se-mantic property may be expressed using a variety ofdifferent keyphrases.Training Our model needs to be provided with thenumber of clusters K .
We setK large enough for themodel to learn effectively on the development set.For the restaurant data ?
where the gold standardidentified eight semantic properties ?
we set K to20, allowing the model to account for keyphrases notincluded in the eight most common properties.
Forthe cell phones category, we set K to 30.To improve the model?s convergence rate, we per-form two initialization steps for the Gibbs sampler.First, sampling is done only on the keyphrase clus-tering component of the model, ignoring documenttext.
Second, we fix this clustering and sample theremaining model parameters.
These two steps arerun for 5,000 iterations each.
The full joint modelis then sampled for 100,000 iterations.
Inspectionof the parameter estimates confirms model conver-gence.
On a 2GHz dual-core desktop machine, amulti-threaded C++ implementation of model train-ing takes about two hours for each dataset.Inference The final point estimate used for test-ing is an average (for continuous variables) or amode (for discrete variables) over the last 1,000Gibbs sampling iterations.
Averaging is a heuris-tic that is applicable in our case because our sam-ple histograms are unimodal and exhibit low skew.The model usually works equally well using single-sample estimates, but is more prone to estimationnoise.As previously mentioned, we convert word topicassignments to document properties by examiningthe proportion of words supporting each property.
Athreshold for this proportion is set for each propertyvia the development set.Evaluation Our first evaluation examines the ac-curacy of our model and the baselines by compar-ing their output against the keyphrases provided bythe review authors.
More specifically, the modelfirst predicts the properties supported by a given re-view.
We then test whether the original authors?keyphrases are contained in the clusters associatedwith these properties.As noted above, the authors?
keyphrases are of-ten incomplete.
To perform a noise-free compari-son, we based our second evaluation on the man-ually constructed gold standard for the restaurantcategory.
We took the most commonly observedkeyphrase from each of the eight annotated proper-ties, and tested whether they are supported by themodel based on the document text.In both types of evaluation, we measure themodel?s performance using precision, recall, and F-score.
These are computed in the standard manner,based on the model?s keyphrase predictions com-pared against the corresponding references.
Thesign test was used for statistical significance test-ing (De Groot and Schervish, 2001).Baselines To the best of our knowledge, this tasknot been previously addressed in the literature.
Wetherefore consider five baselines that allow us to ex-plore the properties of this task and our model.Random: Each keyphrase is supported by a doc-ument with probability of one half.
This baseline?sresults are computed (in expectation) rather than ac-tually run.
This method is expected to have a recallof 0.5, because in expectation it will select half ofthe correct keyphrases.
Its precision is the propor-tion of supported keyphrases in the test set.Phrase in text: A keyphrase is supported by a doc-ument if it appears verbatim in the text.
Because ofthis narrow requirement, precision should be highwhereas recall will be low.268Restaurants Restaurants Cell Phonesgold standard annotation free-text annotation free-text annotationRecall Prec.
F-score Recall Prec.
F-score Recall Prec.
F-scoreRandom 0.500 0.300 ?
0.375 0.500 0.500 ?
0.500 0.500 0.489 ?
0.494Phrase in text 0.048 0.500 ?
0.087 0.078 0.909 ?
0.144 0.171 0.529 ?
0.259Cluster in text 0.223 0.534 0.314 0.517 0.640 ?
0.572 0.829 0.547 0.659Phrase classifier 0.028 0.636 ?
0.053 0.068 0.963 ?
0.126 0.029 0.600 ?
0.055Cluster classifier 0.113 0.622 ?
0.192 0.255 0.907 ?
0.398 0.210 0.759 0.328Our model 0.625 0.416 0.500 0.901 0.652 0.757 0.886 0.585 0.705Our model + gold clusters 0.582 0.398 0.472 0.795 0.627 ?
0.701 0.886 0.520 ?
0.655Table 2: Comparison of the property predictions made by our model and the baselines in the two categories as evaluatedagainst the gold and free-text annotations.
Results for our model using the fixed, manually-created gold clusterings arealso shown.
The methods against which our model has significantly better results on the sign test are indicated with a?
for p <= 0.05, and ?
for p <= 0.1.Cluster in text: A keyphrase is supported by adocument if it or any of its paraphrases appears inthe text.
Paraphrasing is based on our model?s clus-tering of the keyphrases.
The use of paraphrasinginformation enhances recall at the potential cost ofprecision, depending on the quality of the clustering.Phrase classifier: Discriminative classifiers aretrained for each keyphrase.
Positive examples aredocuments that are labeled with the keyphrase;all other documents are negative examples.
Akeyphrase is supported by a document if thatkeyphrase?s classifier returns positive.Cluster classifier: Discriminative classifiers aretrained for each cluster of keyphrases, using ourmodel?s clustering.
Positive examples are docu-ments that are labeled with any keyphrase from thecluster; all other documents are negative examples.All keyphrases of a cluster are supported by a docu-ment if that cluster?s classifier returns positive.Phrase classifier and cluster classifier employmaximum entropy classifiers, trained on the samefeatures as our model, i.e., word counts.
The formeris high-precision/low-recall, because for any partic-ular keyphrase, its synonymous keyphrases wouldbe considered negative examples.
The latter broad-ens the positive examples, which should improve re-call.
We used Zhang Le?s MaxEnt toolkit3 to buildthese classifiers.3http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html7 ResultsComparative performance Table 2 presents theresults of the evaluation scenarios described above.Our model outperforms every baseline by a widemargin in all evaluations.The absolute performance of the automatic meth-ods indicates the difficulty of the task.
For instance,evaluation against gold standard annotations showsthat the random baseline outperforms all of the otherbaselines.
We observe similar disappointing resultsfor the non-random baselines against the free-textannotations.
The precision and recall characteristicsof the baselines match our previously described ex-pectations.The poor performance of the discriminative mod-els seems surprising at first.
However, these re-sults can be explained by the degree of noise inthe training data, specifically, the aforementionedsparsity of free-text annotations.
As previously de-scribed, our technique allows document text topicsto stochastically derive from either the keyphrases ora background distribution ?
this allows our modelto learn effectively from incomplete annotations.
Infact, when we force all text topics to derive fromkeyphrase clusters in our model, its performance de-grades to the level of the classifiers or worse, withan F-score of 0.390 in the restaurant category and0.171 in the cell phone category.Impact of paraphrasing As previously ob-served in entailment research (Dagan et al, 2006),paraphrasing information contributes greatly to im-proved performance on semantic inference.
This is269Figure 4: Sample keyphrase clusters that our model infersin the cell phone category.confirmed by the dramatic difference in results be-tween the cluster in text and phrase in text baselines.Therefore it is important to quantify the quality ofautomatically computed paraphrases, such as thoseillustrated in Figure 4.Restaurants Cell PhonesKeyphrase similarity only 0.931 0.759Joint training 0.966 0.876Table 3: Rand Index scores of our model?s clusters, usingonly keyphrase similarity vs. using keyphrases and textjointly.
Comparison of cluster quality is against the goldstandard.One way to assess clustering quality is to com-pare it against a ?gold standard?
clustering, as con-structed in Section 6.
For this purpose, we use theRand Index (Rand, 1971), a measure of cluster sim-ilarity.
This measure varies from zero to one; higherscores are better.
Table 3 shows the Rand Indicesfor our model?s clustering, as well as the clusteringobtained by using only keyphrase similarity.
Thesescores confirm that joint inference produces betterclusters than using only keyphrases.Another way of assessing cluster quality is to con-sider the impact of using the gold standard clusteringinstead of our model?s clustering.
As shown in thelast two lines of Table 2, using the gold clusteringyields results worse than using the model clustering.This indicates that for the purposes of our task, themodel clustering is of sufficient quality.8 Conclusions and Future WorkIn this paper, we have shown how free-text anno-tations provided by novice users can be leveragedas a training set for document-level semantic infer-ence.
The resulting hierarchical Bayesian modelovercomes the lack of consistency in such anno-tations by inducing a hidden structure of seman-tic properties, which correspond both to clusters ofkeyphrases and hidden topic models in the text.
Oursystem successfully extracts semantic properties ofunannotated restaurant and cell phone reviews, em-pirically validating our approach.Our present model makes strong assumptionsabout the independence of similarity scores.
We be-lieve this could be avoided by modeling the genera-tion of the entire similarity matrix jointly.
We havealso assumed that the properties themselves are un-structured, but they are in fact related in interest-ing ways.
For example, it would be desirable tomodel antonyms explicitly, e.g., no restaurant reviewshould be simultaneously labeled as having goodand bad food.
The correlated topic model (Blei andLafferty, 2006) is one way to account for relation-ships between hidden topics; more structured repre-sentations, such as hierarchies, may also be consid-ered.Finally, the core idea of using free-text as asource of training labels has wide applicability, andhas the potential to enable sophisticated contentsearch and analysis.
For example, online blog en-tries are often tagged with short keyphrases.
Ourtechnique could be used to standardize these tags,and assign keyphrases to untagged blogs.
The no-tion of free-text annotations is also very broad ?we are currently exploring the applicability of thismodel to Wikipedia articles, using section titles askeyphrases, to build standard article schemas.AcknowledgmentsThe authors acknowledge the support of the NSF,Quanta Computer, the U.S. Office of Naval Re-search, and DARPA.
Thanks to Michael Collins,Dina Katabi, Kristian Kersting, Terry Koo, BrianMilch, Tahira Naseem, Dan Roy, Benjamin Snyder,Luke Zettlemoyer, and the anonymous reviewers forhelpful comments and suggestions.
Any opinions,findings, and conclusions or recommendations ex-pressed above are those of the authors and do notnecessarily reflect the views of the NSF.270ReferencesDavid M. Blei and John D. Lafferty.
2006.
Correlatedtopic models.
In Advances in NIPS, pages 147?154.David M. Blei and Jon McAuliffe.
2007.
Supervisedtopic models.
In Advances in NIPS.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.Jacob Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Measure-ment, 20(1):37?46.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL recognising textual entail-ment challenge.
Lecture Notes in Computer Science,3944:177?190.Morris H. De Groot and Mark J. Schervish.
2001.
Prob-ability and Statistics.
Addison Wesley.Jenny R. Finkel, Trond Grenager, and Christopher Man-ning.
2005.
Incorporating non-local information intoinformation extraction systems by Gibbs sampling.
InProceedings of the ACL, pages 363?370.Andrew Gelman, John B. Carlin, Hal S. Stern, and Don-ald B. Rubin.
2004.
Bayesian Data Analysis.
Textsin Statistical Science.
Chapman & Hall/CRC, 2nd edi-tion.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2006.
Contextual dependencies in unsupervisedword segmentation.
In Proceedings of ACL, pages673?680.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of SIGKDD,pages 168?177.Soo-Min Kim and Eduard Hovy.
2006.
Automatic iden-tification of pro and con reasons in online reviews.
InProceedings of the COLING/ACL, pages 483?490.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In Proceedings of ICML, pages 296?304.Ana-Maria Popescu, Bao Nguyen, and Oren Etzioni.2005.
OPINE: Extracting product features and opin-ions from reviews.
In Proceedings of HLT/EMNLP,pages 339?346.William M. Rand.
1971.
Objective criteria for the eval-uation of clustering methods.
Journal of the AmericanStatistical Association, 66(336):846?850, December.Bruce Sterling.
2005.
Order out of chaos: What is thebest way to tag, bag, and sort data?
Give it to theunorganized masses.
http://www.wired.com/wired/archive/13.04/view.html?pg=4.Accessed April 21, 2008.Ivan Titov and Ryan McDonald.
2008.
A joint model oftext and aspect ratings for sentiment summarization.In Proceedings of the ACL.Kristina Toutanova and Mark Johnson.
2007.
ABayesian LDA-based model for semi-supervised part-of-speech tagging.
In Advances in NIPS.Graham Vickery and Sacha Wunsch-Vincent.
2007.
Par-ticipative Web and User-Created Content: Web 2.0,Wikis and Social Networking.
OECD Publishing.271
