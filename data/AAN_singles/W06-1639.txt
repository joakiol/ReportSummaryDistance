Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 327?335,Sydney, July 2006. c?2006 Association for Computational LinguisticsGet out the vote: Determining support or opposition from Congressionalfloor-debate transcriptsMatt Thomas, Bo Pang, and Lillian LeeDepartment of Computer Science, Cornell UniversityIthaca, NY 14853-7501mattthomas84@gmail.com, pabo@cs.cornell.edu, llee@cs.cornell.eduAbstractWe investigate whether one can determinefrom the transcripts of U.S. Congressionalfloor debates whether the speeches repre-sent support of or opposition to proposedlegislation.
To address this problem, weexploit the fact that these speeches occuras part of a discussion; this allows us touse sources of information regarding re-lationships between discourse segments,such as whether a given utterance indicatesagreement with the opinion expressed byanother.
We find that the incorporationof such information yields substantial im-provements over classifying speeches inisolation.1 IntroductionOne ought to recognize that the presentpolitical chaos is connected with the de-cay of language, and that one can prob-ably bring about some improvement bystarting at the verbal end.
?
Orwell,?Politics and the English language?We have entered an era where very largeamounts of politically oriented text are now avail-able online.
This includes both official documents,such as the full text of laws and the proceedings oflegislative bodies, and unofficial documents, suchas postings on weblogs (blogs) devoted to politics.In some sense, the availability of such data is sim-ply a manifestation of a general trend of ?every-body putting their records on the Internet?.1 The1It is worth pointing out that the United States?
Library ofCongress was an extremely early adopter of Web technology:the THOMAS database (http://thomas.loc.gov) of congres-online accessibility of politically oriented texts inparticular, however, is a phenomenon that somehave gone so far as to say will have a potentiallysociety-changing effect.In the United States, for example, governmen-tal bodies are providing and soliciting politicaldocuments via the Internet, with lofty goals inmind: electronic rulemaking (eRulemaking) ini-tiatives involving the ?electronic collection, dis-tribution, synthesis, and analysis of public com-mentary in the regulatory rulemaking process?,may ?
[alter] the citizen-government relationship?
(Shulman and Schlosberg, 2002).
Additionally,much media attention has been focused recentlyon the potential impact that Internet sites may haveon politics2, or at least on political journalism3.Regardless of whether one views such claims asclear-sighted prophecy or mere hype, it is obvi-ously important to help people understand and an-alyze politically oriented text, given the impor-tance of enabling informed participation in the po-litical process.Evaluative and persuasive documents, such asa politician?s speech regarding a bill or a blog-ger?s commentary on a legislative proposal, form aparticularly interesting type of politically orientedtext.
People are much more likely to consult suchevaluative statements than the actual text of a billor law under discussion, given the dense nature oflegislative language and the fact that (U.S.) billsoften reach several hundred pages in length (Smithet al, 2005).
Moreover, political opinions are ex-sional bills and related data was launched in January 1995,when Mosaic was not quite two years old and Altavista didnot yet exist.2E.g., ?Internet injects sweeping change into U.S.
poli-tics?, Adam Nagourney, The New York Times, April 2, 2006.3E.g., ?The End of News?
?, Michael Massing, The NewYork Review of Books, December 1, 2005.327plicitly solicited in the eRulemaking scenario.In the analysis of evaluative language, it is fun-damentally necessary to determine whether the au-thor/speaker supports or disapproves of the topicof discussion.
In this paper, we investigate thefollowing specific instantiation of this problem:we seek to determine from the transcripts ofU.S.
Congressional floor debates whether each?speech?
(continuous single-speaker segment oftext) represents support for or opposition to a pro-posed piece of legislation.
Note that from an ex-perimental point of view, this is a very convenientproblem to work with because we can automati-cally determine ground truth (and thus avoid theneed for manual annotation) simply by consultingpublicly available voting records.Task properties Determining whether or not aspeaker supports a proposal falls within the realmof sentiment analysis, an extremely active re-search area devoted to the computational treatmentof subjective or opinion-oriented language (earlywork includes Wiebe and Rapaport (1988), Hearst(1992), Sack (1994), and Wiebe (1994); see Esuli(2006) for an active bibliography).
In particu-lar, since we treat each individual speech withina debate as a single ?document?, we are consider-ing a version of document-level sentiment-polarityclassification, namely, automatically distinguish-ing between positive and negative documents (Dasand Chen, 2001; Pang et al, 2002; Turney, 2002;Dave et al, 2003).Most sentiment-polarity classifiers proposed inthe recent literature categorize each document in-dependently.
A few others incorporate variousmeasures of inter-document similarity between thetexts to be labeled (Agarwal and Bhattacharyya,2005; Pang and Lee, 2005; Goldberg and Zhu,2006).
Many interesting opinion-oriented docu-ments, however, can be linked through certain re-lationships that occur in the context of evaluativediscussions.
For example, we may find textual4evidence of a high likelihood of agreement be-4Because we are most interested in techniques applicableacross domains, we restrict consideration to NLP aspects ofthe problem, ignoring external problem-specific information.For example, although most votes in our corpus were almostcompletely along party lines (and despite the fact that same-party information is easily incorporated via the methods wepropose), we did not use party-affiliation data.
Indeed, inother settings (e.g., a movie-discussion listserv) one may notbe able to determine the participants?
political leanings, andsuch information may not lead to significantly improved re-sults even if it were available.tween two speakers, such as explicit assertions (?Isecond that!?)
or quotation of messages in emailsor postings (see Mullen and Malouf (2006) but cf.Agrawal et al (2003)).
Agreement evidence canbe a powerful aid in our classification task: for ex-ample, we can easily categorize a complicated (oroverly terse) document if we find within it indica-tions of agreement with a clearly positive text.Obviously, incorporating agreement informa-tion provides additional benefit only when the in-put documents are relatively difficult to classifyindividually.
Intuition suggests that this is trueof the data with which we experiment, for severalreasons.
First, U.S. congressional debates containvery rich language and cover an extremely widevariety of topics, ranging from flag burning to in-ternational policy to the federal budget.
Debatesare also subject to digressions, some fairly naturaland others less so (e.g., ?Why are we discussingthis bill when the plight of my constituents regard-ing this other issue is being ignored??
)Second, an important characteristic of persua-sive language is that speakers may spend moretime presenting evidence in support of their po-sitions (or attacking the evidence presented byothers) than directly stating their attitudes.
Anextreme example will illustrate the problems in-volved.
Consider a speech that describes the U.S.flag as deeply inspirational, and thus contains onlypositive language.
If the bill under discussion is aproposed flag-burning ban, then the speech is sup-portive; but if the bill under discussion is aimed atrescinding an existing flag-burning ban, the speechmay represent opposition to the legislation.
Giventhe current state of the art in sentiment analysis,it is doubtful that one could determine the (proba-bly topic-specific) relationship between presentedevidence and speaker opinion.Qualitative summary of results The above dif-ficulties underscore the importance of enhancingstandard classification techniques with new infor-mation sources that promise to improve accuracy,such as inter-document relationships between thedocuments to be labeled.
In this paper, we demon-strate that the incorporation of agreement model-ing can provide substantial improvements over theapplication of support vector machines (SVMs) inisolation, which represents the state of the art inthe individual classification of documents.
The en-hanced accuracies are obtained via a fairly primi-tive automatically-acquired ?agreement detector?328total train test developmentspeech segments 3857 2740 860 257debates 53 38 10 5average number of speech segments per debate 72.8 72.1 86.0 51.4average number of speakers per debate 32.1 30.9 41.1 22.6Table 1: Corpus statistics.and a conceptually simple method for integrat-ing isolated-document and agreement-based in-formation.
We thus view our results as demon-strating the potentially large benefits of exploitingsentiment-related discourse-segment relationshipsin sentiment-analysis tasks.2 CorpusThis section outlines the main steps of the processby which we created our corpus (download site:www.cs.cornell.edu/home/llee/data/convote.html).GovTrack (http://govtrack.us) is an independentwebsite run by Joshua Tauberer that collects pub-licly available data on the legislative and fund-raising activities of U.S. congresspeople.
Due toits extensive cross-referencing and collating of in-formation, it was nominated for a 2006 ?Webby?award.
A crucial characteristic of GovTrack fromour point of view is that the information is pro-vided in a very convenient format; for instance,the floor-debate transcripts are broken into sepa-rate HTML files according to the subject of thedebate, so we can trivially derive long sequencesof speeches guaranteed to cover the same topic.We extracted from GovTrack all available tran-scripts of U.S. floor debates in the House of Rep-resentatives for the year 2005 (3268 pages of tran-scripts in total), together with voting records for allroll-call votes during that year.
We concentratedon debates regarding ?controversial?
bills (ones inwhich the losing side generated at least 20% of thespeeches) because these debates should presum-ably exhibit more interesting discourse structure.Each debate consists of a series of speech seg-ments, where each segment is a sequence of un-interrupted utterances by a single speaker.
Sincespeech segments represent natural discourse units,we treat them as the basic unit to be classified.Each speech segment was labeled by the vote(?yea?
or ?nay?)
cast for the proposed bill by theperson who uttered the speech segment.We automatically discarded those speech seg-ments belonging to a class of formulaic, generallyone-sentence utterances focused on the yieldingof time on the house floor (for example, ?MadamSpeaker, I am pleased to yield 5 minutes to thegentleman from Massachusetts?
), as such speechsegments are clearly off-topic.
We also removedspeech segments containing the term ?amend-ment?, since we found during initial inspectionthat these speeches generally reflect a speaker?sopinion on an amendment, and this opinion maydiffer from the speaker?s opinion on the underly-ing bill under discussion.We randomly split the data into training, test,and development (parameter-tuning) sets repre-senting roughly 70%, 20%, and 10% of our data,respectively (see Table 1).
The speech segmentsremained grouped by debate, with 38 debates as-signed to the training set, 10 to the test set, and 5to the development set; we require that the speechsegments from an individual debate all appear inthe same set because our goal is to examine clas-sification of speech segments in the context of thesurrounding discussion.3 MethodThe support/oppose classification problem can beapproached through the use of standard classifierssuch as support vector machines (SVMs), whichconsider each text unit in isolation.
As discussedin Section 1, however, the conversational natureof our data implies the existence of various rela-tionships that can be exploited to improve cumu-lative classification accuracy for speech segmentsbelonging to the same debate.
Our classificationframework, directly inspired by Blum and Chawla(2001), integrates both perspectives, optimizingits labeling of speech segments based on both in-dividual speech-segment classification scores andpreferences for groups of speech segments to re-ceive the same label.
In this section, we discussthe specific classification framework that we adoptand the set of mechanisms that we propose formodeling specific types of relationships.3293.1 Classification frameworkLet s1, s2, .
.
.
, sn be the sequence of speech seg-ments within a given debate, and let Y andN stand for the ?yea?
and ?nay?
class, respec-tively.
Assume we have a non-negative func-tion ind(s, C) indicating the degree of preferencethat an individual-document classifier, such as anSVM, has for placing speech-segment s in classC.
Also, assume that some pairs of speech seg-ments have weighted links between them, wherethe non-negative strength (weight) str(`) for alink ` indicates the degree to which it is prefer-able that the linked speech segments receive thesame label.
Then, any class assignment c =c(s1), c(s2), .
.
.
, c(sn) can be assigned a cost?sind(s, c(s))+?s,s?
: c(s) 6=c(s?
)?` between s,s?str(`),where c(s) is the ?opposite?
class from c(s).
Aminimum-cost assignment thus represents an opti-mum way to classify the speech segments so thateach one tends not to be put into the class thatthe individual-document classifier disprefers, butat the same time, highly associated speech seg-ments tend not to be put in different classes.As has been previously observed and exploitedin the NLP literature (Pang and Lee, 2004; Agar-wal and Bhattacharyya, 2005; Barzilay and Lap-ata, 2005), the above optimization function, unlikemany others that have been proposed for graph orset partitioning, can be solved exactly in an prov-ably efficient manner via methods for finding min-imum cuts in graphs.
In our view, the contributionof our work is the examination of new types ofrelationships, not the method by which such re-lationships are incorporated into the classificationdecision.3.2 Classifying speech segments in isolationIn our experiments, we employed the well-knownclassifier SVMlight to obtain individual-documentclassification scores, treating Y as the positiveclass and using plain unigrams as features.5 Fol-lowing standard practice in sentiment analysis(Pang et al, 2002), the input to SVMlight con-sisted of normalized presence-of-feature (ratherthan frequency-of-feature) vectors.
The ind value5SVMlight is available at svmlight.joachims.org.
Defaultparameters were used, although experimentation with differ-ent parameter settings is an important direction for futurework (Daelemans and Hoste, 2002; Munson et al, 2005).for each speech segment s was based on the signeddistance d(s) from the vector representing s to thetrained SVM decision plane:ind(s,Y) def=??????
?1 d(s) > 2?s;(1 + d(s)2?s)/2 |d(s)| ?
2?s;0 d(s) < ?2?swhere ?s is the standard deviation of d(s) over allspeech segments s in the debate in question, andind(s,N ) def= 1?
ind(s,Y).We now turn to the more interesting problem ofrepresenting the preferences that speech segmentsmay have for being assigned to the same class.3.3 Relationships between speech segmentsA wide range of relationships between text seg-ments can be modeled as positive-strength links.Here we discuss two types of constraints that areconsidered in this work.Same-speaker constraints: In Congressionaldebates and in general social-discourse contexts,a single speaker may make a number of commentsregarding a topic.
It is reasonable to expect that inmany settings, the participants in a discussion maybe convinced to change their opinions midwaythrough a debate.
Hence, in the general case wewish to be able to express ?soft?
preferences for allof an author?s statements to receive the same label,where the strengths of such constraints could, forinstance, vary according to the time elapsed be-tween the statements.
Weighted links are an ap-propriate means to express such variation.However, if we assume that most speakers donot change their positions in the course of a dis-cussion, we can conclude that all comments madeby the same speaker must receive the same label.This assumption holds by fiat for the ground-truthlabels in our dataset because these labels werederived from the single vote cast by the speakeron the bill being discussed.6 We can implementthis assumption via links whose weights are essen-tially infinite.
Although one can also implementthis assumption via concatenation of same-speakerspeech segments (see Section 4.3), we view thefact that our graph-based framework incorporates6We are attempting to determine whether a speech seg-ment represents support or not.
This differs from the problemof determining what the speaker?s actual opinion is, a prob-lem that, as an anonymous reviewer put it, is complicated by?grandstanding, backroom deals, or, more innocently, plainchange of mind (?I voted for it before I voted against it?
)?.330both hard and soft constraints in a principled fash-ion as an advantage of our approach.Different-speaker agreements In House dis-course, it is common for one speaker to make ref-erence to another in the context of an agreementor disagreement over the topic of discussion.
Thesystematic identification of instances of agreementcan, as we have discussed, be a powerful tool forthe development of intelligently selected weightsfor links between speech segments.The problem of agreement identification can bedecomposed into two sub-problems: identifyingreferences and their targets, and deciding whethereach reference represents an instance of agree-ment.
In our case, the first task is straightfor-ward because we focused solely on by-name ref-erences.7 Hence, we will now concentrate on thesecond, more interesting task.We approach the problem of classifying refer-ences by representing each reference with a word-presence vector derived from a window of textsurrounding the reference.8 In the training set,we classify each reference connecting two speak-ers with a positive or negative label depending onwhether the two voted the same way on the bill un-der discussion9.
These labels are then used to trainan SVM classifier, the output of which is subse-quently used to create weights on agreement linksin the test set as follows.Let d(r) denote the distance from the vectorrepresenting reference r to the agreement-detectorSVM?s decision plane, and let ?r be the standarddeviation of d(r) over all references in the debatein question.
We then define the strength agr of theagreement link corresponding to the reference as:agr(r) def=????
?0 d(r) < ?agr;?
?
d(r)/4?r ?agr ?
d(r) ?
4?r;?
d(r) > 4?r.The free parameter ?
specifies the relative impor-7One subtlety is that for the purposes of mining agree-ment cues (but not for evaluating overall support/opposeclassification accuracy), we temporarily re-inserted into ourdataset previously filtered speech segments containing theterm ?yield?, since the yielding of time on the House floortypically indicates agreement even though the yield state-ments contain little relevant text on their own.8We found good development-set performance using the30 tokens before, 20 tokens after, and the name itself.9Since we are concerned with references that potentiallyrepresent relationships between speech segments, we ignorereferences for which the target of the reference did not speakin the debate in which the reference was made.Agreement classifier(?reference?agreement??
)Devel.setTestsetmajority baseline 81.51 80.26Train: no amdmts; ?agr = 0 84.25 81.07Train: with amdmts; ?agr = 0 86.99 80.10Table 2: Agreement-classifier accuracy, in per-cent.
?Amdmts?=?speech segments containing theword ?amendment??.
Recall that boldface indi-cates results for development-set-optimal settings.tance of the agr scores.
The threshold ?agr con-trols the precision of the agreement links, in thatvalues of ?agr greater than zero mean that greaterconfidence is required before an agreement linkcan be added.104 EvaluationThis section presents experiments testing the util-ity of using speech-segment relationships, evalu-ating against a number of baselines.
All reportedresults use values for the free parameter ?
derivedvia tuning on the development set.
In the tables,boldface indicates the development- and test-setresults for the development-set-optimal parametersettings, as one would make algorithmic choicesbased on development-set performance.4.1 Preliminaries: Reference classificationRecall that to gather inter-speaker agreement in-formation, the strategy employed in this paper isto classify by-name references to other speakersas to whether they indicate agreement or not.To train our agreement classifier, we experi-mented with undoing the deletion of amendment-related speech segments in the training set.
Notethat such speech segments were never included inthe development or test set, since, as discussed inSection 2, their labels are probably noisy; how-ever, including them in the training set alows theclassifier to examine more instances even thoughsome of them are labeled incorrectly.
As Table2 shows, using more, if noisy, data yields bet-ter agreement-classification results on the devel-opment set, and so we use that policy in all subse-quent experiments.1110Our implementation puts a link between just one arbi-trary pair of speech segments among all those uttered by agiven pair of apparently agreeing speakers.
The ?infinite-weight?
same-speaker links propagate the agreement infor-mation to all other such pairs.11Unfortunately, this policy leads to inferior test-set agree-331Agreement classifier Precision (in percent):Devel.
set Test set?agr = 0 86.23 82.55?agr = ?
89.41 88.47Table 3: Agreement-classifier precision.An important observation is that precision maybe more important than accuracy in decidingwhich agreement links to add: false positives withrespect to agreement can cause speech segmentsto be incorrectly assigned the same label, whereasfalse negatives mean only that agreement-basedinformation about other speech segments is notemployed.
As described above, we can raiseagreement precision by increasing the threshold?agr, which specifies the required confidence forthe addition of an agreement link.
Indeed, Table3 shows that we can improve agreement precisionby setting ?agr to the (positive) mean agreementscore ?
assigned by the SVM agreement-classifierover all references in the given debate12.
How-ever, this comes at the cost of greatly reducingagreement accuracy (development: 64.38%; test:66.18%) due to lowered recall levels.
Whetheror not better speech-segment classification is ulti-mately achieved is discussed in the next sections.4.2 Segment-based speech-segmentclassificationBaselines The first two data rows of Table4 depict baseline performance results.
The#(?support?)
?
#(?oppos?)
baseline is meantto explore whether the speech-segment classifica-tion task can be reduced to simple lexical checks.Specifically, this method uses the signed differ-ence between the number of words containing thestem ?support?
and the number of words contain-ing the stem ?oppos?
(returning the majority classif the difference is 0).
No better than 62.67% test-set accuracy is obtained by either baseline.Using relationship information Applying anSVM to classify each speech segment in isolationleads to clear improvements over the two base-line methods, as demonstrated in Table 4.
Whenwe impose the constraint that all speech segmentsuttered by the same speaker receive the same la-bel via ?same-speaker links?, both test-set andment classification.
Section 4.5 contains further discussion.12We elected not to explicitly tune the value of ?agr in or-der to minimize the number of free parameters to deal with.Support/oppose classifer(?speech segment?yea??
)Devel.setTestsetmajority baseline 54.09 58.37#(?support?)?#(?oppos?)
59.14 62.67SVM [speech segment] 70.04 66.05SVM + same-speaker links 79.77 67.21SVM + same-speaker links .
.
.+ agreement links, ?agr = 0 89.11 70.81+ agreement links, ?agr = ?
87.94 71.16Table 4: Segment-based speech-segment classifi-cation accuracy, in percent.Support/oppose classifer(?speech segment?yea??
)Devel.setTestsetSVM [speaker] 71.60 70.00SVM + agreement links .
.
.with ?agr = 0 88.72 71.28with ?agr = ?
84.44 76.05Table 5: Speaker-based speech-segment classifica-tion accuracy, in percent.
Here, the initial SVM isrun on the concatenation of all of a given speaker?sspeech segments, but the results are computedover speech segments (not speakers), so that theycan be compared to those in Table 4.development-set accuracy increase even more, inthe latter case quite substantially so.The last two lines of Table 4 show that thebest results are obtained by incorporating agree-ment information as well.
The highest test-set re-sult, 71.16%, is obtained by using a high-precisionthreshold to determine which agreement links toadd.
While the development-set results would in-duce us to utilize the standard threshold value of 0,which is sub-optimal on the test set, the ?agr = 0agreement-link policy still achieves noticeable im-provement over not using agreement links (test set:70.81% vs. 67.21%).4.3 Speaker-based speech-segmentclassificationWe use speech segments as the unit of classifica-tion because they represent natural discourse units.As a consequence, we are able to exploit relation-ships at the speech-segment level.
However, it isinteresting to consider whether we really need toconsider relationships specifically between speechsegments themselves, or whether it suffices to sim-ply consider relationships between the speakers332of the speech segments.
In particular, as an al-ternative to using same-speaker links, we tried aspeaker-based approach wherein the way we de-termine the initial individual-document classifica-tion score for each speech segment uttered by aperson p in a given debate is to run an SVM on theconcatenation of all of p?s speech segments withinthat debate.
(We also ensure that agreement-linkinformation is propagated from speech-segment tospeaker pairs.
)How does the use of same-speaker links com-pare to the concatenation of each speaker?s speechsegments?
Tables 4 and 5 show that, not sur-prisingly, the SVM individual-document classifierworks better on the concatenated speech segmentsthan on the speech segments in isolation.
How-ever, the effect on overall classification accuracyis less clear: the development set favors same-speaker links over concatenation, while the test setdoes not.But we stress that the most important obser-vation we can make from Table 5 is that onceagain, the addition of agreement information leadsto substantial improvements in accuracy.4.4 ?Hard?
agreement constraintsRecall that in in our experiments, we createdfinite-weight agreement links, so that speech seg-ments appearing in pairs flagged by our (imper-fect) agreement detector can potentially receivedifferent labels.
We also experimented with forc-ing such speech segments to receive the same la-bel, either through infinite-weight agreement linksor through a speech-segment concatenation strat-egy similar to that described in the previous sub-section.
Both strategies resulted in clear degrada-tion in performance on both the development andtest sets, a finding that validates our encoding ofagreement information as ?soft?
preferences.4.5 On the development/test set splitWe have seen several cases in which the methodthat performs best on the development set doesnot yield the best test-set performance.
However,we felt that it would be illegitimate to change thetrain/development/test sets in a post hoc fashion,that is, after seeing the experimental results.Moreover, and crucially, it is very clear thatusing agreement information, encoded as prefer-ences within our graph-based approach rather thanas hard constraints, yields substantial improve-ments on both the development and test set; this,we believe, is our most important finding.5 Related workPolitically-oriented text Sentiment analysis hasspecifically been proposed as a key enabling tech-nology in eRulemaking, allowing the automaticanalysis of the opinions that people submit (Shul-man et al, 2005; Cardie et al, 2006; Kwon et al,2006).
There has also been work focused upon de-termining the political leaning (e.g., ?liberal?
vs.?conservative?)
of a document or author, wheremost previously-proposed methods make no di-rect use of relationships between the documents tobe classified (the ?unlabeled?
texts) (Laver et al,2003; Efron, 2004; Mullen and Malouf, 2006).
Anexception is Grefenstette et al (2004), who exper-imented with determining the political orientationof websites essentially by classifying the concate-nation of all the documents found on that site.Others have applied the NLP technologies ofnear-duplicate detection and topic-based text cat-egorization to politically oriented text (Yang andCallan, 2005; Purpura and Hillard, 2006).Detecting agreement We used a simple methodto learn to identify cross-speaker references indi-cating agreement.
More sophisticated approacheshave been proposed (Hillard et al, 2003), in-cluding an extension that, in an interesting re-versal of our problem, makes use of sentiment-polarity indicators within speech segments (Gal-ley et al, 2004).
Also relevant is work on the gen-eral problems of dialog-act tagging (Stolcke et al,2000), citation analysis (Lehnert et al, 1990), andcomputational rhetorical analysis (Marcu, 2000;Teufel and Moens, 2002).We currently do not have an efficient meansto encode disagreement information as hard con-straints; we plan to investigate incorporating suchinformation in future work.Relationships between the unlabeled itemsCarvalho and Cohen (2005) consider sequentialrelations between different types of emails (e.g.,between requests and satisfactions thereof) to clas-sify messages, and thus also explicitly exploit thestructure of conversations.Previous sentiment-analysis work in differentdomains has considered inter-document similar-ity (Agarwal and Bhattacharyya, 2005; Pang andLee, 2005; Goldberg and Zhu, 2006) or explicit333inter-document references in the form of hyper-links (Agrawal et al, 2003).Notable early papers on graph-based semi-supervised learning include Blum and Chawla(2001), Bansal et al (2002), Kondor and Lafferty(2002), and Joachims (2003).
Zhu (2005) main-tains a survey of this area.Recently, several alternative, often quite sophis-ticated approaches to collective classification havebeen proposed (Neville and Jensen, 2000; Laf-ferty et al, 2001; Getoor et al, 2002; Taskar etal., 2002; Taskar et al, 2003; Taskar et al, 2004;McCallum and Wellner, 2004).
It would be inter-esting to investigate the application of such meth-ods to our problem.
However, we also believethat our approach has important advantages, in-cluding conceptual simplicity and the fact that it isbased on an underlying optimization problem thatis provably and in practice easy to solve.6 Conclusion and future workIn this study, we focused on very general typesof cross-document classification preferences, uti-lizing constraints based only on speaker identityand on direct textual references between state-ments.
We showed that the integration of evenvery limited information regarding inter-documentrelationships can significantly increase the accu-racy of support/opposition classification.The simple constraints modeled in our study,however, represent just a small portion of therich network of relationships that connect state-ments and speakers across the political universeand in the wider realm of opinionated social dis-course.
One intriguing possibility is to take ad-vantage of (readily identifiable) information re-garding interpersonal relationships, making use ofspeaker/author affiliations, positions within a so-cial hierarchy, and so on.
Or, we could even at-tempt to model relationships between topics orconcepts, in a kind of extension of collaborativefiltering.
For example, perhaps we could infer thattwo speakers sharing a common opinion on evo-lutionary biologist Richard Dawkins (a.k.a.
?Dar-win?s rottweiler?)
will be likely to agree in a de-bate centered on Intelligent Design.
While suchfunctionality is well beyond the scope of our cur-rent study, we are optimistic that we can developmethods to exploit additional types of relation-ships in future work.Acknowledgments We thank Claire Cardie, JonKleinberg, Michael Macy, Andrew Myers, and thesix anonymous EMNLP referees for valuable dis-cussions and comments.
We also thank Reviewer1 for generously providing additional post hocfeedback, and the EMNLP chairs Eric Gaussierand Dan Jurafsky for facilitating the process (aswell as for allowing authors an extra proceedingspage.
.
.).
This paper is based upon work sup-ported in part by the National Science Founda-tion under grant no.
IIS-0329064.
Any opinions,findings, and conclusions or recommendations ex-pressed are those of the authors and do not neces-sarily reflect the views or official policies, eitherexpressed or implied, of any sponsoring institu-tions, the U.S. government, or any other entity.ReferencesA.
Agarwal, P. Bhattacharyya.
2005.
Sentiment anal-ysis: A new approach for effective use of linguis-tic knowledge and exploiting similarities in a set ofdocuments to be classified.
In Proceedings of theInternational Conference on Natural Language Pro-cessing (ICON).R.
Agrawal, S. Rajagopalan, R. Srikant, Y. Xu.
2003.Mining newsgroups using networks arising from so-cial behavior.
In Proceedings of WWW, 529?535.N.
Bansal, A. Blum, S. Chawla.
2002.
Correla-tion clustering.
In Proceedings of the Symposiumon Foundations of Computer Science (FOCS), 238?247.
Journal version in Machine Learning Journal,special issue on theoretical advances in data cluster-ing, 56(1-3):89?113 (2004).R.
Barzilay, M. Lapata.
2005.
Collective content selec-tion for concept-to-text generation.
In Proceedingsof HLT/EMNLP, 331?338.A.
Blum, S. Chawla.
2001.
Learning from labeled andunlabeled data using graph mincuts.
In Proceedingsof ICML, 19?26.C.
Cardie, C. Farina, T. Bruce, E. Wagner.
2006.
Us-ing natural language processing to improve eRule-making.
In Proceedings of Digital Government Re-search (dg.o).V.
Carvalho, W. W. Cohen.
2005.
On the collectiveclassification of email ?speech acts?.
In Proceedingsof SIGIR, 345?352.W.
Daelemans, V. Hoste.
2002.
Evaluation of ma-chine learning methods for natural language pro-cessing tasks.
In Proceedings of the Third Interna-tional Conference on Language Resources and Eval-uation (LREC), 755?760.S.
Das, M. Chen.
2001.
Yahoo!
for Amazon: Extract-ing market sentiment from stock message boards.
InProceedings of the Asia Pacific Finance AssociationAnnual Conference (APFA).K.
Dave, S. Lawrence, D. M. Pennock.
2003.
Miningthe peanut gallery: Opinion extraction and semanticclassification of product reviews.
In Proceedings ofWWW, 519?528.334M.
Efron.
2004.
Cultural orientation: Classifying sub-jective documents by cociation [sic] analysis.
InProceedings of the AAAI Fall Symposium on Styleand Meaning in Language, Art, Music, and Design,41?48.A.
Esuli.
2006.
Sentiment classification bibliography.liinwww.ira.uka.de/bibliography/Misc/Sentiment.html.M.
Galley, K. McKeown, J. Hirschberg, E. Shriberg.2004.
Identifying agreement and disagreement inconversational speech: Use of Bayesian networks tomodel pragmatic dependencies.
In Proceedings ofthe 42nd ACL, 669?676.L.
Getoor, N. Friedman, D. Koller, B. Taskar.
2002.Learning probabilistic models of relational structure.Journal of Machine Learning Research, 3:679?707.Special issue on the Eighteenth ICML.A.
B. Goldberg, J. Zhu.
2006.
Seeing starswhen there aren?t many stars: Graph-based semi-supervised learning for sentiment categorization.In TextGraphs: HLT/NAACL Workshop on Graph-based Algorithms for Natural Language Processing.G.
Grefenstette, Y. Qu, J. G. Shanahan, D. A. Evans.2004.
Coupling niche browsers and affect analysisfor an opinion mining application.
In Proceedingsof RIAO.M.
Hearst.
1992.
Direction-based text interpretation asan information access refinement.
In P. Jacobs, ed.,Text-Based Intelligent Systems, 257?274.
LawrenceErlbaum Associates.D.
Hillard, M. Ostendorf, E. Shriberg.
2003.
Detectionof agreement vs. disagreement in meetings: Train-ing with unlabeled data.
In Proceedings of HLT-NAACL.T.
Joachims.
2003.
Transductive learning via spectralgraph partitioning.
In Proceedings of ICML, 290?297.R.
I. Kondor, J. D. Lafferty.
2002.
Diffusion kernelson graphs and other discrete input spaces.
In Pro-ceedings of ICML, 315?322.N.
Kwon, S. Shulman, E. Hovy.
2006.
Multidimen-sional text analysis for eRulemaking.
In Proceed-ings of Digital Government Research (dg.o).J.
Lafferty, A. McCallum, F. Pereira.
2001.
Condi-tional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof ICML, 282?289.M.
Laver, K. Benoit, J. Garry.
2003.
Extracting policypositions from political texts using words as data.American Political Science Review.W.
Lehnert, C. Cardie, E. Riloff.
1990.
Analyzing re-search papers using citation sentences.
In Programof the Twelfth Annual Conference of the CognitiveScience Society, 511?18.D.
Marcu.
2000.
The theory and practice of discourseparsing and summarization.
MIT Press.A.
McCallum, B. Wellner.
2004.
Conditional mod-els of identity uncertainty with application to nouncoreference.
In Proceedings of NIPS.T.
Mullen, R. Malouf.
2006.
A preliminary investiga-tion into sentiment analysis of informal political dis-course.
In Proceedings of the AAAI Symposium onComputational Approaches to Analyzing Weblogs,159?162.A.
Munson, C. Cardie, R. Caruana.
2005.
Optimizingto arbitrary NLP metrics using ensemble selection.In Proceedings of HLT-EMNLP, 539?546.J.
Neville, D. Jensen.
2000.
Iterative classification inrelational data.
In Proceedings of the AAAI Work-shop on Learning Statistical Models from RelationalData, 13?20.B.
Pang, L. Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of the ACL,271?278.B.
Pang, L. Lee.
2005.
Seeing stars: Exploiting classrelationships for sentiment categorization with re-spect to rating scales.
In Proceedings of the ACL.B.
Pang, L. Lee, S. Vaithyanathan.
2002.
Thumbsup?
Sentiment classification using machine learningtechniques.
In Proceedings of EMNLP, 79?86.S.
Purpura, D. Hillard.
2006.
Automated classifica-tion of congressional legislation.
In Proceedings ofDigital Government Research (dg.o).W.
Sack.
1994.
On the computation of point of view.In Proceedings of AAAI, pg.
1488.
Student abstract.S.
Shulman, D. Schlosberg.
2002.
Electronic rulemak-ing: New frontiers in public participation.
Preparedfor the Annual Meeting of the American PoliticalScience Association.S.
Shulman, J. Callan, E. Hovy, S. Zavestoski.
2005.Language processing technologies for electronicrulemaking: A project highlight.
In Proceedings ofDigital Government Research (dg.o), 87?88.S.
S. Smith, J. M. Roberts, R. J. Vander Wielen.
2005.The American Congress.
Cambridge UniversityPress, fourth edition.A.
Stolcke, N. Coccaro, R. Bates, P. Taylor, C. Van Ess-Dykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-tin, M. Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational Linguistics, 26(3):339?373.B.
Taskar, P. Abbeel, D. Koller.
2002.
Discriminativeprobabilistic models for relational data.
In Proceed-ings of UAI, Edmonton, Canada.B.
Taskar, C. Guestrin, D. Koller.
2003.
Max-marginMarkov networks.
In Proceedings of NIPS.B.
Taskar, V. Chatalbashev, D. Koller.
2004.
Learn-ing associative Markov networks.
In Proceedings ofICML.S.
Teufel, M. Moens.
2002.
Summarizing scientificarticles: Experiments with relevance and rhetoricalstatus.
Computational Linguistics, 28(4):409?445.P.
Turney.
2002.
Thumbs up or thumbs down?
Seman-tic orientation applied to unsupervised classificationof reviews.
In Proceedings of the ACL, 417?424.J.
M. Wiebe, W. J. Rapaport.
1988.
A computationaltheory of perspective and reference in narrative.
InProceedings of the ACL, 131?138.J.
M. Wiebe.
1994.
Tracking point of view in narrative.Computational Linguistics, 20(2):233?287.H.
Yang, J. Callan.
2005.
Near-duplicate detectionfor eRulemaking.
In Proceedings of Digital Gov-ernment Research (dg.o).J.
Zhu.
2005.
Semi-supervised learning literaturesurvey.
Computer Sciences Technical Report TR1530, University of Wisconsin-Madison.
Availableat http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf;has been updated since the initial 2005 version.335
