Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 176?180,Dublin, Ireland, August 23-24, 2014.CMU: Arc-Factored, Discriminative Semantic Dependency ParsingSam Thomson Brendan O?Connor Jeffrey Flanigan David BammanJesse Dodge Swabha Swayamdipta Nathan Schneider Chris Dyer Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{sthomson,brenocon,jflanigan,dbamman,jessed,swabha,nschneid,cdyer,nasmith}@cs.cmu.eduAbstractWe present an arc-factored statistical modelfor semantic dependency parsing, as de-fined by the SemEval 2014 Shared Task 8on Broad-Coverage Semantic DependencyParsing.
Our entry in the open track placedsecond in the competition.1 IntroductionThe task of broad coverage semantic dependencyparsing aims to provide a shallow semantic analysisof text not limited to a specific domain.
As distinctfrom deeper semantic analysis (e.g., parsing to afull lambda-calculus logical form), shallow seman-tic parsing captures relationships between pairsof words or concepts in a sentence, and has wideapplication for information extraction, knowledgebase population, and question answering (amongothers).We present here two systems that produce seman-tic dependency parses in the three formalisms of theSemEval 2014 Shared Task 8 on Broad-CoverageSemantic Dependency Parsing (Oepen et al., 2014).These systems generate parses by extracting fea-tures for each potential dependency arc and learn-ing a statistical model to discriminate between goodarcs and bad; the first treats each labeled edge de-cision as an independent multiclass logistic regres-sion (?3.2.1), while the second predicts arcs as partof a graph-based structured support vector machine(?3.2.2).
Common to both models is a rich set offeatures on arcs, described in ?3.2.3.
We include adiscussion of features found to have no discernableeffect, or negative effect, during development (?4).Our system placed second in the open track ofthe Broad-Coverage Semantic Dependency ParsingThis work is licensed under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/Figure 1: Example annotations for DM (top), PAS (middle),and PCEDT (bottom).task (in which output from syntactic parsers andother outside resources can be used).
We presentour results in ?5.2 FormalismsThe Shared Task 8 dataset consists of annota-tions of the WSJ Corpus in three different se-mantic dependency formalisms.
DM is derivedfrom LinGO English Resource Grammar (ERG)annotations in DeepBank (Flickinger et al., 2012).PAS is derived from the Enju HPSG treebank us-ing the conversion rules of Miyao et al.
(2004).PCEDT is derived from the tectogrammatical layerof the Prague Czech-English Dependency Treebank(Haji?c, 1998).
See Figure 1 for an example.The three formalisms come from very differentlinguistic theories, but all are represented as labeleddirected graphs, with words as vertices, and allhave ?top?
annotations, corresponding roughly tothe semantic focus of the sentence.
(A ?top?
neednot be a root of the graph.)
This allows us to usethe same machinery (?3) for training and testingstatistical models for the three formalisms.3 ModelsWe treat the problem as a three-stage pipeline.
Thefirst stage prunes words by predicting whether theyhave any incoming or outgoing edges at all (?3.1);if a word does not, then it is not considered forany attachments in later stages.
The second stage176predicts where edges are present, and their labels(?3.2).
The third stage predicts whether a predicateword is a top or not (?3.3).
Formalisms sometimesannotate more than one ?top?
per sentence, but wefound that we achieve the best performance on allformalisms by predicting only the one best-scoring?top?
under the model.3.1 Singleton ClassificationFor each formalism, we train a classifier to rec-ognize singletons, nodes that have no parents orchildren.
(For example, punctuation tokens are of-ten singletons.)
This makes the system faster with-out affecting accuracy.
For singleton prediction,we use a token-level logistic regression classifier,with features including the word, its lemma, andits part-of-speech tag.
If the classifier predicts aprobability of 99% or higher the token is pruned;this removes around 10% of tokens.
(The classi-fier performs differently on different formalisms;on PAS it has perfect accuracy, while on DM andPCEDT accuracy is in the mid-90?s.
)3.2 Edge PredictionIn the second stage of the pipeline, we predict theset of labeled directed edges in the graph.
We usethe same set of edge-factored features (?3.2.3) intwo alternative models: an edge-independent mul-ticlass logistic regression model (LOGISTICEDGE,?3.2.1); and a structured SVM (Taskar et al., 2003;Tsochantaridis et al., 2004) that enforces a deter-minism constraint for certain labels, which allowseach word to have at most one outgoing edge withthat label (SVMEDGE, ?3.2.2).
For each formalism,we trained both models with varying features en-abled and hyperparameter settings and submittedthe configuration that produced the best labeled F1on the development set.
For DM and PCEDT, thiswas LOGISTICEDGE; for PAS, this was SVMEDGE.We report results only for the submitted configu-rations, with different features enabled.
Due totime constraints, full hyperparameter sweeps andcomparable feature sweeps were not possible.3.2.1 LOGISTICEDGE ParserThe LOGISTICEDGE model considers only tokenindex pairs (i, j) where |i ?
j| ?
10, i 6= j,and both tiand tjhave been predicted to be non-singletons by the first stage.
Although this prunessome gold edges, among the formalisms, 95%?97%of all gold edges are between tokens of distance10 or less.
Both directions i ?
j and j ?
i areconsidered between every pair.Let L be the set of K + 1 possible output labels:the formalism?s original K edge labels, plus theadditional label NOEDGE, which indicates that noedge exists from i to j.
The model treats every pairof token indices (i, j) as an independent multiclasslogistic regression over output space L. Let x bean input sentence.
For candidate parent index i,child index j, and edge label `, we extract a featurevector f(x, i, j, `), where ` is conjoined with everyfeature described in ?3.2.3.
The multiclass logis-tic regression model defines a distribution over L,parametrized by weights ?
:P (` | ?, x, i, j) =exp{?
?
f(x, i, j, `)}?`??Lexp{?
?
f(x, i, j, `?)}.?
is learned by minimizing total negative log-likelihood of the above (with weighting; see be-low), plus `2regularization.
AdaGrad (Duchi et al.,2011) is used for optimization.
This seemed to opti-mize faster than L-BFGS (Liu and Nocedal, 1989),at least for earlier iterations, though we did no sys-tematic comparison.
Stochastic gradient steps areapplied one at a time from individual examples,and a gradient step for the regularizer is appliedonce per epoch.The output labels have a class imbalance; in allthree formalisms, there are many more NOEDGEexamples than true edge examples.
We improvedF1performance by downweighting NOEDGEexamples through a weighted log-likelihoodobjective,?i,j?`w`logP (` |?, x, i, j), withwNOEDGE= 0.3 (selected on development set) andw`= 1 otherwise.Decoding: To predict a graph structure at test-timefor a new sentence, the most likely edge label is pre-dicted for every candidate (i, j) pair of unprunedtokens.
If an edge is predicted for both directionsfor a single (i, j) pair, only the edge with the higherscore is chosen.
(There are no such bidirectionaledges in the training data.)
This post-processing ac-tually did not improve accuracy on DM or PCEDT;it did improve PAS by ?0.2% absolute F1, but wedid not submit LOGISTICEDGE for PAS.3.2.2 SVMEDGE ParserIn the SVMEDGE model, we use a structured SVMwith a determinism constraint.
This constraint en-sures that each word token has at most one outgoingedge for each label in a set of deterministic labelsLd.
For example, in DM a predicate never has more177than one child with edge label ?ARG1.?
Ldwaschosen to be the set of edges that were > 99.9%deterministic in the training data.1Consider the fully dense graph of all edges be-tween all words predicted as not singletons by thesingleton classifier ?3.1 (in all directions with allpossible labels).
Unlike LOGISTICEDGE, the la-bel set L does not include an explicit NOEDGElabel.
If ?
denotes the model weights, and f de-notes the features, then an edge from i to j withlabel ` in the dense graph has a weight c(i, j, `)assigned to it using the linear scoring functionc(i, j, `) = ?
?
f(x, i, j, `).Decoding: For each node and each label `, if ` ?Ld, the decoder adds the highest scoring outgoingedge, if its weight is positive.
For ` 6?
Ld, everyoutgoing edge with positive weight is added.
Thisprocedure is guaranteed to find the highest scoringsubgraph (largest sum of edge weights) of the densegraph subject to the determinism constraints.
Itsruntime is O(n2).The model weights are trained using the struc-tured SVM loss.
If x is a sentence and y is agraph over that sentence, let the features be de-noted f(x, y) =?
(i,j,`)?yf(x, i, j, `).
The SVMloss for each training example (xi, yi) is:?
?>f(xi, yi)+maxy?>f(xi, y)+cost(y, yi)where cost(y, yi) = ?|y \ yi| + ?|yi\ y|.
?
and?
trade off between precision and recall for theedges (Gimpel and Smith, 2010).
The loss is min-imized with AdaGrad using early-stopping on adevelopment set.3.2.3 Edge FeaturesTable 1 describes the features we used for predict-ing edges.
These features were computed over anedge e with parent token s at index i and childtoken t at index j.
Unless otherwise stated, eachfeature template listed has an indicator feature thatfires for each value it can take on.
For the sub-mitted results, LOGISTICEDGE uses all featuresexcept Dependency Path v2, POS Path, and Dis-tance Thresholds, and SVMEDGE uses all featuresexcept Dependency Path v1.
This was due toSVMEDGE being faster to train than LOGISTIC-EDGE when including POS Path features, and due1By this we mean that of the nodes that have at leastone outgoing ` edge, 99.9% of them have only one outgo-ing ` edge.
For DM, Ld= L\{?
and c,?
?
or c,?
?
then c,??loc,?
?mwe,?
?subord?
}; for PAS, Ld= L; and for PCEDT,Ld={?DPHR,?
?INTF,?
?VOCAT?
}.Tokens: The tokens s and t themselves.Lemmas: Lemmas of s and t.POS tags: Part of speech tags of s and t.Linear Order: Fires if i < j.Linear Distance: i?
j.Dependency Path v1 (LOGISTICEDGE only): Theconcatenation of all POS tags, arc labels and up/downdirections on the path in the syntactic dependency treefrom s to t. Conjoined with s, with t, and without either.Dependency Path v2 (SVMEDGE only): Same as De-pendency Path v1, but with the lemma of s or t insteadof the word, and substituting the token for any ?IN?
POStag.Up/Down Dependency Path: The sequence of upwardand downward moves needed to get from s to t in thesyntactic dependency tree.Up/Down/Left/Right Dependency Path: The unla-beled path through the syntactic dependency tree from sto t, annotated with whether each step through the treewas up or down, and whether it was to the right or left inthe sentence.Is Parent: Fires if s is the parent of t in the syntacticdependency parse.Dependency Path Length: Distance between s and t inthe syntactic dependency parse.POS Context: Concatenated POS tags of tokens at i?1,i, i+ 1, j ?
1, j, and j + 1.
Concatenated POS tags oftokens at i?
1, i, j ?
1, and j. Concatenated POS tagsof tokens at i, i+ 1, j, and j + 1.Subcategorization Sequence: The sequence of depen-dency arc labels out of s, ordered by the index of thechild.
Distinguish left children from right children.
If tis a direct child of s, distinguish its arc label with a ?+?.Conjoin this sequence with the POS tag of s.Subcategorization Sequence with POS: As above, butadd the POS tag of each child to its arc label.POS Path (SVMEDGE only): Concatenated POS tagsbetween and including i and j. Conjoined with headlemma, with dependent lemma, and without either.Distance Thresholds (SVMEDGE only): Fires for ev-ery integer between 1 and blog(|i?
j|+1)/ log(1.39)cinclusive.Table 1: Features used in edge predictionto time constraints for the submission we were un-able to retrain LOGISTICEDGE with these features.3.2.4 Feature HashingThe biggest memory usage was in the map fromfeature names to integer indices during featureextraction.
For experimental expedience, we im-plemented multitask feature hashing (Weinbergeret al., 2009), which hashes feature names to indices,under the theory that errors due to collisions tendto cancel.
No drop in accuracy was observed.3.3 Top PredictionWe trained a separate token-level binary logisticregression model to classify whether a token?s nodehad the ?top?
attribute or not.
At decoding time, allpredicted predicates (i.e., nodes where there is at178least one outbound edge) are possible candidatesto be ?top?
; the classifier probabilities are evalu-ated, and the highest-scoring node is chosen to be?top.?
This is suboptimal, since some graphs havemultiple tops (in PCEDT this is more common);but selection rules based on probability thresholdsgave worse F1performance on the dev set.
For agiven token t at index i, the top classifier?s featuresincluded t?s POS tag, i, those two conjoined, andthe depth of t in the syntactic dependency tree.4 Negative ResultsWe followed a forward-selection process duringfeature engineering.
For each potential feature,we tested the current feature set versus the currentfeature set plus the new potential feature.
If thenew feature did not improve performance, we didnot add it.
We list in table 2 some of the featureswhich we tested but did not improve performance.In order to save time, we ran these feature se-lection experiments on a subsample of the trainingdata, for a reduced number of iterations.
These re-sults thus have a strong caveat that the experimentswere not exhaustive.
It may be that some of thesefeatures could help under more careful study.5 Experimental SetupWe participated in the Open Track, and used thesyntactic dependency parses supplied by the orga-nizers.
Feature engineering was performed on adevelopment set (?20), training on ??00?19.
Weevaluate labeled precision (LP), labeled recall (LR),labeled F1(LF), and labeled whole-sentence match(LM) on the held-out test data using the evaluationscript provided by the organizers.
LF was aver-aged over the formalisms to determine the winningsystem.
Table 3 shows our scores.6 Conclusion and Future WorkWe found that feature-rich discriminative modelsperform well at the task of mapping from sentencesto semantic dependency parses.
While our finalapproach is fairly standard for work in parsing,we note here additional features and constraintswhich did not appear to help (contrary to expecta-tion).
There are a number of clear extensions tothis work that could improve performance.
Whilean edge-factored model allows for efficient infer-ence, there is much to be gained from higher-orderfeatures (McDonald and Pereira, 2006; Martinset al., 2013).
The amount of information sharedWord vectors: Features derived from 64-dimensionalvectors from (Faruqui and Dyer, 2014), including theconcatenation, difference, inner product, and element-wise multiplication of the two vectors associated witha parent-child edge.
We also trained a Random Foreston the word vectors using Liaw and Wiener?s (2002) Rimplementation.
The predicted labels were then used asfeatures in LOGISTICEDGE.Brown clusters Features derived from Brown clusters(Brown et al., 1992) trained on a large corpus of web data.Parent, child, and conjoined parent-child edge featuresfrom cluster prefixes of length 2, 4, 6, 8, 10, and 12.Conjunctions of those features with the POS tags of theparent and child tokens.Active/passive: Active/passive voice feature (as in Jo-hansson and Nugues (2008)) conjoined with both theLinear Distance features and the Subcategorization Se-quence features.
Voice information may already be cap-tured by features from the Stanford dependency?styleparses, which include passivization information in arclabels such as nsubjpass and auxpass (de Marneffe andManning, 2008).Connectivity constraint: Enforcing that the graph isconnected (ignoring singletons), similar to Flanigan et al.(2014).
Almost all semantic dependency graphs in thetraining data are connected (ignoring singletons), butwe found that enforcing this constraint significantly hurtprecision.Tree constraint: Enforces that the graph is a tree.
Un-surprisingly, we found that enforcing a tree constrainthurt performance.Table 2: Features and constraints giving negative results.LP LR LF LMDM 0.8446 0.8348 0.8397 0.0875PAS 0.9078 0.8851 0.8963 0.2604PCEDT 0.7681 0.7072 0.7364 0.0712Average 0.8402 0.8090 0.8241 0.1397Table 3: Labeled precision (LP), recall (LR), F1(LF), andwhole-sentence match (LM) on the held-out test data.between the three formalisms suggests that a multi-task learning (Evgeniou and Pontil, 2004) frame-work could lead to gains.
And finally, there isadditional structure in the formalisms which couldbe exploited (such as the deterministic processesby which an original PCEDT tree annotation wasconverted into a graph); formulating more subtlegraph constraints to capture this a priori knowl-edge could lead to improved performance.
Weleave such explorations to future work.AcknowledgementsWe are grateful to Manaal Faruqui for his help in word vectorexperiments, and to reviewers for helpful comments.
The re-search reported in this paper was sponsored by the U.S. ArmyResearch Laboratory and the U. S. Army Research Officeunder contract/grant number W911NF-10-1-0533, DARPAgrant FA8750-12-2-0342 funded under the DEFT program,U.S.
NSF grants IIS-1251131 and IIS-1054319, and Google?ssupport of the Reading is Believing project at CMU.179ReferencesPeter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-basedn-gram models of natural language.
Computational Lin-guistics, 18(4):467?479.Marie-Catherine de Marneffe and Christopher D. Manning.2008.
The Stanford typed dependencies representation.
InColing 2008: Proc.
of the Workshop on Cross-Frameworkand Cross-Domain Parser Evaluation, pages 1?8.
Manch-ester, UK.John Duchi, Elad Hazan, and Yoram Singer.
2011.
Adap-tive subgradient methods for online learning and stochas-tic optimization.
Journal of Machine Learning Research,12:2121?2159.Theodoros Evgeniou and Massimiliano Pontil.
2004.
Regular-ized multitask learning.
In Proc.
of KDD, pages 109?117.Seattle, WA, USA.Manaal Faruqui and Chris Dyer.
2014.
Improving vectorspace word representations using multilingual correlation.In Proc.
of EACL, pages 462?471.
Gothenburg, Sweden.Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer,and Noah A. Smith.
2014.
A discriminative graph-basedparser for the Abstract Meaning Representation.
In Proc.of ACL, pages 1426?1436.
Baltimore, MD, USA.Dan Flickinger, Yi Zhang, and Valia Kordoni.
2012.
Deep-Bank: a dynamically annotated treebank of the Wall StreetJournal.
In Proc.
of the Eleventh International Workshop onTreebanks and Linguistic Theories, pages 85?96.
Lisbon,Portugal.Kevin Gimpel and Noah A. Smith.
2010.
Softmax-margintraining for structured log-linear models.
TechnicalReport CMU-LTI-10-008, Carnegie Mellon Univer-sity.
URL http://lti.cs.cmu.edu/sites/default/files/research/reports/2010/cmulti10008.pdf.Jan Haji?c.
1998.
Building a syntactically annotated corpus:the Prague Dependency Treebank.
In Eva Haji?cov?a, ed-itor, Issues of Valency and Meaning.
Studies in Honourof Jarmila Panevov?a, pages 106?132.
Prague Karolinum,Charles University Press, Prague.Richard Johansson and Pierre Nugues.
2008.
Dependency-based semantic role labeling of PropBank.
In Proc.
ofEMNLP, pages 69?78.
Honolulu, HI, USA.Andy Liaw and Matthew Wiener.
2002.
Classificationand regression by randomForest.
R News, 2(3):18?22.
URL http://cran.r-project.org/web/packages/randomForest/.Dong C. Liu and Jorge Nocedal.
1989.
On the limited memoryBFGS method for large scale optimization.
MathematicalProgramming, 45(3):503?528.Andr?e F. T. Martins, Miguel Almeida, and Noah A. Smith.2013.
Turning on the turbo: Fast third-order non-projectiveturbo parsers.
In Proc.
of ACL, pages 617?622.
Sofia,Bulgaria.Ryan McDonald and Fernando Pereira.
2006.
Online learningof approximate dependency parsing algorithms.
In Proc.
ofEACL, pages 81?88.
Trento, Italy.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2004.Corpus-oriented grammar development for acquiring ahead-driven phrase structure grammar from the Penn Tree-bank.
In Proc.
of IJCNLP, pages 684?693.
Hainan Island,China.Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, DanielZeman, Dan Flickinger, Jan Haji?c, Angelina Ivanova, andYi Zhang.
2014.
SemEval 2014 Task 8: Broad-coveragesemantic dependency parsing.
In Proc.
of SemEval.
Dublin,Ireland.Ben Taskar, Carlos Guestrin, and Daphne Koller.
2003.
Max-margin Markov networks.
In Proc.
of NIPS, pages 25?32.Vancouver, British Columbia, Canada.Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims,and Yasemin Altun.
2004.
Support vector machine learningfor interdependent and structured output spaces.
In Proc.of ICML, pages 104?111.
Banff, Alberta, Canada.Kilian Weinberger, Anirban Dasgupta, John Langford, AlexSmola, and Josh Attenberg.
2009.
Feature hashing forlarge scale multitask learning.
In Proc.
of ICML, pages1113?1120.
Montreal, Quebec, Canada.180
