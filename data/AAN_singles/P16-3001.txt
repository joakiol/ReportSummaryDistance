Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ?
Student Research Workshop, pages 1?7,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsControlled and Balanced Dataset for Japanese Lexical SimplificationTomonori Kodaira Tomoyuki KajiwaraTokyo Metropolitan UniversityHino City, Tokyo, Japan{kodaira-tomonori, kajiwara-tomoyuki}@ed.tmu.ac.jp, komachi@tmu.ac.jpMamoru KomachiAbstractWe propose a new dataset for evaluatinga Japanese lexical simplification method.Previous datasets have several deficien-cies.
All of them substitute only a sin-gle target word, and some of them extractsentences only from newswire corpus.
Inaddition, most of these datasets do not al-low ties and integrate simplification rank-ing from all the annotators without consid-ering the quality.
In contrast, our datasethas the following advantages: (1) it is thefirst controlled and balanced dataset forJapanese lexical simplification with highcorrelation with human judgment and (2)the consistency of the simplification rank-ing is improved by allowing candidates tohave ties and by considering the reliabilityof annotators.1 IntroductionLexical simplification is the task to find and sub-stitute a complex word or phrase in a sentencewith its simpler synonymous expression.
We de-fine complex word as a word that has lexical andsubjective difficulty in a sentence.
It can help inreading comprehension for children and languagelearners (De Belder and Moens, 2010).
This taskis a rather easier task which prepare a pair of com-plex and simple representations than a challeng-ing task which changes the substitute pair in agiven context (Specia et al, 2012; Kajiwara andYamamoto, 2015).
Construction of a benchmarkdataset is important to ensure the reliability andreproducibility of evaluation.
However, few re-sources are available for the automatic evaluationof lexical simplification.
Specia et al (2012) andDe Belder and Moens (2010) created benchmarkdatasets for evaluating English lexical simplifica-tion.
In addition, Horn et al (2014) extracted sim-plification candidates and constructed an evalua-tion dataset using English Wikipedia and SimpleEnglishWikipedia.
In contrast, such a parallel cor-pus does not exist in Japanese.
Kajiwara and Ya-mamoto (2015) constructed an evaluation datasetfor Japanese lexical simplification1in languagesother than English.However, there are four drawbacks in thedataset of Kajiwara and Yamamoto (2015): (1)they extracted sentences only from a newswirecorpus; (2) they substituted only a single targetword; (3) they did not allow ties; and (4) theydid not integrate simplification ranking consider-ing the quality.Hence, we propose a new dataset addressingthe problems in the dataset of Kajiwara and Ya-mamoto (2015).
The main contributions of ourstudy are as follows:?
It is the first controlled and balanced datasetfor Japanese lexical simplification.
We ex-tract sentences from a balanced corpus andcontrol sentences to have only one com-plex word.
Experimental results show thatour dataset is more suitable than previousdatasets for evaluating systems with respectto correlation with human judgment.?
The consistency of simplification ranking isgreatly improved by allowing candidates tohave ties and by considering the reliability ofannotators.Our dataset is available at GitHub2.2 Related workThe evaluation dataset for the English LexicalSimplification task (Specia et al, 2012) was an-1http://www.jnlp.org/SNOW/E42https://github.com/KodairaTomonori/EvaluationDataset1sentence ?????????????????????????????????????????????
?Although using their techniques makes you feel exalted, I strongly feel I want to outrank my competitors in terms of skill.paraphrase list ?????
??????
???
????
????
?????
????
?come alive raised, excited up exalted excited heated revitalizedFigure 1: A part of the dataset of Kajiwara and Yamamoto (2015).notated on top of the evaluation dataset for Englishlexical substitution (McCarthy and Navigli, 2007).They asked university students to rerank substi-tutes according to simplification ranking.
Sen-tences in their dataset do not always contain com-plex words, and it is not appropriate to evaluatesimplification systems if a test sentence does notinclude any complex words.In addition, De Belder and Moens (2012) builtan evaluation dataset for English lexical simplifi-cation based on that developed by McCarthy andNavigli (2007).
They used Amazon?s MechanicalTurk to rank substitutes and employed the relia-bility of annotators to remove outlier annotatorsand/or downweight unreliable annotators.
The re-liability was calculated on penalty based agree-ment (McCarthy and Navigli, 2007) and Fleiss?Kappa.
Unlike the dataset of Specia et al (2012),sentences in their dataset contain at least one com-plex word, but they might contain more than onecomplex word.
Again, it is not adequate for theautomatic evaluation of lexical simplification be-cause the human ranking of the resulting simpli-fication might be affected by the context contain-ing complex words.
Furthermore, De Belder andMoens?
(2012) dataset is too small to be used forachieving a reliable evaluation of lexical simplifi-cation systems.3 Problems in previous datasets forJapanese lexical simplificationKajiwara and Yamamoto (2015) followed Speciaet al (2012) to construct an evaluation dataset forJapanese lexical simplification.
Namely, they splitthe data creation process into two steps: substituteextraction and simplification ranking.During the substitute extraction task, they col-lected substitutes of each target word in 10 differ-ent contexts.
These contexts were randomly se-lected from a newswire corpus.
The target wordwas a content word (noun, verb, adjective, or ad-verb), and was neither a simple word nor part ofany compound words.
They gathered substitutesfrom five annotators using crowdsourcing.
Theseprocedures were the same as for De Belder andMoens (2012).During the simplification ranking task, annota-tors were asked to reorder the target word and itssubstitutes in a single order without allowing ties.They used crowdsourcing to find five annotatorsdifferent from those who performed the substituteextraction task.
Simplification ranking was inte-grated on the basis of the average of the simplifi-cation ranking from each annotator to generate agold-standard ranking that might include ties.During the substitute extraction task, agreementamong the annotators was 0.664, whereas duringthe simplification ranking task, Spearman?s rankcorrelation coefficient score was 0.332.
Spear-man?s score of this work was lower than that ofSpecia et al (2012) by 0.064.
Thus, there was abig blur between annotators, and the simplifica-tion ranking collected using crowdsourcing tendedto have a lower quality.Figure 1 shows a part of the dataset of Kajiwaraand Yamamoto (2015).
Our discussion in this pa-per is based on this example.Domain of the dataset is limited.
Because Ka-jiwara and Yamamoto (2015) extracted sentencesfrom a newswire corpus, their dataset has a poorvariety of expression.
English lexical simplifica-tion datasets (Specia et al, 2012; De Belder andMoens, 2012) do not have this problem becauseboth of them use a balanced corpus of English(Sharoff, 2006).Complex words might exist in context.
In Fig-ure 1, even when a target word such as ?????
(feel exalted)?
is simplified, another complexword ???
(skill)?
is left in a sentence.
Lexi-cal simplification is a task of simplifying complexwords in a sentence.
Previous datasets may in-clude multiple complex words in a sentence buttarget only one complex word.
Not only the tar-get word but also other complex words should beconsidered as well, but annotation of substitutesand simplification ranking to all complex wordsin a sentence produces a huge number of pat-terns, therefore takes a very high cost of anno-tation.
For example, when three complex words2Figure 2: Process of constructing the dataset.which have 10 substitutes each in a sentence, an-notators should consider 103patterns.
Thus, it isdesired that a sentence includes only simple wordsafter the target word is substituted.
Therefore, inthis work, we extract sentences containing onlyone complex word.Ties are not permitted in simplification rank-ing.
When each annotator assigns a simplifica-tion ranking to a substitution list, a tie cannot beassigned in previous datasets (Specia et al, 2012;Kajiwara and Yamamoto, 2015).
This deterio-rates ranking consistency if some substitutes havea similar simplicity.
De Belder and Moens (2012)allow ties in simplification ranking and report con-siderably higher agreement among annotators thanSpecia et al (2012).The method of ranking integration is na?
?ve.Kajiwara and Yamamoto (2015) and Specia et al(2012) use an average score to integrate rankings,but it might be biased by outliers.
De Belder andMoens (2012) report a slight increase in agreementby greedily removing annotators to maximize theagreement score.4 Balanced dataset for evaluation ofJapanese lexical simplificationWe create a balanced dataset for the evaluation ofJapanese lexical simplification.
Figure 2 illustrateshow we constructed the dataset.
It follows the datacreation procedure of Kajiwara and Yamamoto?s(2015) dataset with improvements to resolve theproblems described in Section 3.We use a crowdsourcing application, Lancers,33http://www.lancers.jp/Figure 3: Example of annotation of extracting sub-stitutes.
Annotators are provided with substitutesthat preserve the meaning of target word which isshown bold in the sentence.
In addition, annota-tors can write a substitute including particles.to perform substitute extraction, substitute evalua-tion, and substitute ranking.
In each task, we re-quested the annotators to complete at least 95% oftheir previous assignments correctly.
They werenative Japanese speakers.4.1 Extracting sentencesOur work defines complex words as ?High Level?words in the Lexicon for Japanese Language Edu-cation (Sunakawa et al, 2012).4The word level iscalculated by five teachers of Japanese, based ontheir experience and intuition.
There were 7,940high-level words out of 17,921 words in the lex-icon.
In addition, target words of this work com-prised content words (nouns, verbs, adjectives, ad-verbs, adjectival nouns, sahen nouns,5and sahenverbs6).Sentences that include a complex word wererandomly extracted from the Balanced Corpus ofContemporary Written Japanese (Maekawa et al,2010).
Sentences shorter than seven words orlonger than 35 words were excluded.
We excludedtarget words that appeared as a part of compoundwords.
Following previous work, 10 contexts ofoccurrence were collected for each complex word.We assigned 30 complex words for each part ofspeech.
The total number of sentences was 2,100(30 words ?
10 sentences ?
7 parts of speech).We used a crowdsourcing application to annotate1,800 sentences, and we asked university studentsmajoring in computer science to annotate 300 sen-tences to investigate the quality of crowdsourcing.4.2 Extracting substitutesSimplification candidates were collected usingcrowdsourcing techniques.
For each complexword, five annotators wrote substitutes that did not4http://jhlee.sakura.ne.jp/JEV.html5Sahen noun is a kind of noun that can form a verb byadding a generic verb ?suru (do)?
to the noun.
(e.g.
???repair?
)6Sahen verb is a sahen noun that accompanies with?suru?.
(e.g.
?????
(do repair)?
)3Dataset balanced lang sents.
noun (?)
verb (?)
adj.
(?)
adv.
(?)
outlierDe Belder and Moens (2012) yes En 430 100 (23.3) 60 (14.0) 160 (37.2) 110 (25.6) excludedSpecia et al (2012) yes En 2,010 580 (28.9) 520 (25.9) 560 (27.9) 350 (17.6) includedKajiwara and Yamamoto (2015) no Ja 2,330 630 (27.0) 720 (30.9) 500 (21.5) 480 (20.6) includedThis work yes Ja 2,010 570 (28.3) 570 (28.3) 580 (28.8) 290 (14.4) excludedTable 1: Comparison of the datasets.
In this work, nouns include sahen nouns, verbs include sahen verbs,and adjectives include adjectival nouns.Figure 4: Example of annotation of evaluatingsubstitutes.
Annotators choose substitutes that fitinto the sentence from substitutes list.Figure 5: Example of annotation of ranking sub-stitutes.
Annotators write rank in blank.
Addition-ally, they are allowed to write a tie.change the sense of the sentence.
Substitutionscould include particles in context.
Conjugationwas allowed to cover variations of both verbs andadjectives.
Figure 3 shows an example of annota-tion.To improve the quality of the lexical substi-tution, inappropriate substitutes were deleted forlater use, as described in the next subsection.4.3 Evaluating substitutesFive annotators selected an appropriate word toinclude as a substitution that did not change thesense of the sentence.
Substitutes that won a ma-jority were defined as correct.
Figure 4 shows anexample of annotation.Nine complex words that were evaluated as nothaving substitutes were excluded at this point.
Asa result, 2,010 sentences were annotated, as de-scribed in next subsection.4.4 Ranking substitutesFive annotators arranged substitutes and complexwords according to the simplification ranking.
An-notators were permitted to assign a tie, but theycould select up to four items to be in a tie becausewe intended to prohibit an insincere person fromselecting a tie for all items.
Figure 5 shows an ex-ample of annotation.4.5 Integrating simplification rankingAnnotators?
rankings were integrated into oneranking, using a maximum likelihood estimation(Matsui et al, 2014) to penalize deceptive annota-tors as was done by De Belder and Moens (2012).This method estimates the reliability of annotatorsin addition to determining the true order of rank-ings.
We applied the reliability score to excludeextraordinary annotators.5 ResultTable 1 shows the characteristics of our dataset.
Itis about the same size as previous work (Speciaet al, 2012; Kajiwara and Yamamoto, 2015).
Ourdataset has two advantages: (1) improved correla-tion with human judgment by making a controlledand balanced dataset, and (2) enhanced consis-tency by allowing ties in ranking and removingoutlier annotators.
In the following subsections,we evaluate our dataset in detail.5.1 Intrinsic evaluationTo evaluate the quality of the ranking integration,the Spearman rank correlation coefficient was cal-culated.
The baseline integration ranking used anaverage score (Kajiwara and Yamamoto, 2015).Our proposed method excludes outlier annotatorsby using a reliability score calculated using themethod developed by Matsui et al (2014).1|P |?p1,p2?Pp1?
p2p1?
p2(1)Pairwise agreement is calculated between eachpair of sets (p1, p2?
P ) from all the possible pair-ings (P) (Equation 1).
The agreement among an-notators from the substitute evaluation phase was0.669, and agreement among the students is 0.673,which is similar to the level found in crowdsourc-ing.
This score is almost the same as that fromKajiwara and Yamamoto (2015).
On the contrary,4sentence ??????????????????????????????????????????????????
?The most simplest method that is imitating safer is pretentiously smoke that Garam which is Indonesian cigarette.paraphrase list 1.??????
2.??????
3.??????,??????
4.????
5.??????
6.???
7.??
?professing counterfeiting playing, professing playing pretending imitating falsifyingFigure 6: A part of our dataset.genre PB PM PN LB OW OT OP OB OC OY OV OL OM allsentence 0 64 628 6 161 90 170 700 1 0 6 9 175 2,010average of substitutes 0 4.12 4.36 5.17 4.41 4.22 3.9 4.28 4 0 5.5 4.11 4.45 4.3Table 3: Detail of sentences and substitutes in our dataset.
(BCCWJ comprise three main subcorpora:publication (P), library (L), special-purpose (O).
PB = book, PM =magazine, PN = newswire, LB = book,OW = white paper, OT = textbook, OP =PR paper, OB = bestselling books, OC = Yahoo!
Answers,OY = Yahoo!
Blogs, OL = Law, OM = Magazine)baseline outlier removalAverage 0.541 0.580Table 2: Correlation of ranking integration.the Spearman rank correlation coefficient of thesubstitute ranking phase was 0.522.
This scoreis higher than that from Kajiwara and Yamamoto(2015) by 0.190.
This clearly shows the impor-tance of allowing ties during the substitute rankingtask.Table 2 shows the results of the ranking inte-gration.
Our method achieved better accuracy inranking integration than previous methods (Speciaet al, 2012; Kajiwara and Yamamoto, 2015) andis similar to the results from De Belder and Moens(2012).
This shows that the reliability score can beused for improving the quality.Table 3 shows the number of sentences and av-erage substitutes in each genre.
In our dataset, thenumber of acquired substitutes is 8,636 words andthe average number of substitutes is 4.30 wordsper sentence.Figure 6 illustrates a part of our dataset.
Substi-tutes that include particles are found in 75 context(3.7%).
It is shown that if particles are not permit-ted in substitutes, we obtain only two substitutes (4and 7).
By permitting substitutes to include parti-cles, we are able to obtain 7 substitutes.In ranking substitutes, Spearman rank correla-tion coefficient is 0.729, which is substantiallyhigher than crowdsourcing?s score.
Thus, it is nec-essary to consider annotation method.5.2 Extrinsic evaluationIn this section, we evaluate our dataset using fivesimple lexical simplification methods.
We calcu-This work K & Y annotatedFrequency 41.6 35.8 41.0# of Users 32.9 25.0 31.5Familiarity 30.4 31.5 32.5JEV 38.2 35.7 38.7JLPT 42.0 40.9 43.3Pearson 0.963 0.930 N/ATable 4: Accuracy and correlation of the datasets.late 1-best accuracy in our dataset and the datasetof Kajiwara and Yamamoto (2015).
Annotateddata is collected by our and Kajiwara and Ya-mamoto (2015)?s work in ranking substitutes task,and which size is 21,700 ((2010 + 2330) ?
5)rankings.
Then, we calculate correlation betweenthe accuracies of annotated data and either thoseof Kajiwara and Yamamoto (2015) or those of ourdataset.5.2.1 Lexical simplification systemsWe used several metrics for these experiments:Frequency Because it is said that a high fre-quent word is simple, most frequent word is se-lected as a simplification candidate from substi-tutes using uni-gram frequency of Japanese WebN-gram (Kudo and Kazawa, 2007).
This uni-gramfrequency is counted from two billion sentences inJapanese Web text.Number of Users Aramaki et al (2013) claimedthat a word used by many people is simple, so wepick the word used by the most of users.
Numberof Users were estimated from the Twitter corpuscreated by Aramaki et al (2013).
The corpus con-tains 250 million tweets from 100,000 users.5Familiarity Assuming that a word which isknown by many people is simple, replace a targetword with substitutes according to the familiarityscore using familiarity data constructed by Amanoand Kondo (2000).
The familiarity score is an av-eraged score 28 annotators with seven grades.JEV We hypothesized a word which is low dif-ficulty for non-native speakers is simple, so weselect a word using a Japanese learner dictionarymade by Sunakawa et al (2012).
The word indictionary has a difficulty score averaged by 5Japanese teachers with their subjective annotationaccording to six grade system.JLPT Same as above, but uses a different sourcecalled Japanese Language Proficient Test (JLPT).We choose the lowest level word using levels ofJLPT.
These levels are a scale of one to five.5.2.2 EvaluationWe ranked substitutes according to the metrics,and calculated the 1-best accuracy for each tar-get word.
Finally, to compare two datasets, weused the Pearson product-moment correlation co-efficient between our dataset and the dataset ofKajiwara and Yamamoto (2015) against the anno-tated data.Table 4 shows the result of this experiment.
ThePearson coefficient shows that our dataset corre-lates with human annotation better than the datasetof Kajiwara and Yamamoto (2015), possibly be-cause we controlled each sentence to include onlyone complex word.
Because our dataset is bal-anced, the accuracy of Web corpus-based metrics(Frequency and Number of Users) closer than thedataset of Kajiwara and Yamamoto (2015).6 ConclusionWe have presented a new controlled and balanceddataset for the evaluation of Japanese lexical sim-plification.
Experimental results show that (1)our dataset is more consistent than the previousdatasets and (2) lexical simplification methods us-ing our dataset correlate with human annotationbetter than the previous datasets.
Future work in-cludes increasing the number of sentences, so asto leverage the dataset for machine learning-basedsimplification methods.ReferencesShigeaki Amano and Kimihisa Kondo.
2000.
On theNTT psycholinguistic databases ?lexical propertiesof Japanese?.
Journal of the Phonetic Society ofJapan 4(2), pages 44?50.Eiji Aramaki, Sachiko Maskawa, Mai Miyabe, MizukiMorita, and Sachi Yasuda.
2013.
Word in a dic-tionary is used by numerous users.
In Proceedingof International Joint Conference on Natural Lan-guage Processing, pages 874?877.Jan De Belder and Marie-Francine Moens.
2010.
Textsimplification for children.
In Proceedings of the SI-GIR Workshop on Accessible Search Systems, pages19?26.Jan De Belder and Marie-Francine Moens.
2012.
Adataset for the evaluation of lexical simplification.In Proceedings of the 13th International Conferenceon Computational Linguistics and Intelligent TextProcessing, pages 426?437.Colby Horn, Cathryn Manduca, and David Kauchak.2014.
Learning a lexical simplifier using Wikipedia.In Proceedings of the 52nd Annual Meeting of theAssociation for Computational Linguistics (Volume2: Short Papers), pages 458?463.Tomoyuki Kajiwara and Kazuhide Yamamoto.
2015.Evaluation dataset and system for Japanese lexicalsimplification.
In Proceedings of the ACL-IJCNLP2015 Student Research Workshop, pages 35?40.Taku Kudo and Hideto Kazawa.
2007.
Japanese WebN-gram Version 1.
Linguistic Data Consoritium.Kikuo Maekawa, Makoto Yamazaki, TakehikoMaruyama, Masaya Yamaguchi, Hideki Ogura,Wakako Kashino, Toshinobu Ogiso, Hanae Koiso,and Yasuharu Den.
2010.
Design, compilation,and preliminary analyses of balanced corpus ofcontemporary written Japanese.
In Proceedings ofthe Seventh International Conference on LanguageResources and Evaluation, pages 1483?1486.Toshiko Matsui, Yukino Baba, Toshihiro Kamishima,and Hisashi Kashima.
2014.
Crowdordering.
InProceedings of the 18th Pacific-Asia Conference onKnowledge Discovery and Data Mining, pages 336?347.Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
InProceedings of the 4th International Workshop onSemantic Evaluations, pages 48?53.Serge Sharoff.
2006.
Open-source corpora: Using thenet to fish for linguistic data.
Journal of Corpus Lin-guistics, 11(4), pages 435?462.Lucia Specia, Sujay Kumar Jauhar, and RadaMihalcea.2012.
SemEval-2012 task 1: English lexical sim-plification.
In Proceedings of the 6th InternationalWorkshop on Semantic Evaluation, pages 347?355.6Yuriko Sunakawa, Jae-ho Lee, and Mari Takahara.2012.
The construction of a database to support thecompilation of Japanese learners dictionaries.
Jour-nal of the Acta Linguistica Asiatica 2(2), pages 97?115.7
