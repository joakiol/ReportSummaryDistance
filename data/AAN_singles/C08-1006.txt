Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 41?48Manchester, August 2008Verification and Implementation of Language-Based DeceptionIndicators in Civil and Criminal NarrativesJoan BachenkoDeception Discovery TechnologiesOxford, NJ 07863jbachenko@comcast.netEileen FitzpatrickMontclair State UniversityMontclair, NJ 07043fitzpatricke@mail.montclair.eduMichael SchonwetterDeception Discovery TechnologiesMinneapolis, MN 55416mschonwetter@synchronvideo.comAbstractOur goal is to use natural language proc-essing to identify deceptive and non-deceptive passages in transcribed narra-tives.
We begin by motivating an analy-sis of language-based deception thatrelies on specific linguistic indicators todiscover deceptive statements.
The indi-cator tags are assigned to a document us-ing a mix of automated and manualmethods.
Once the tags are assigned, aninterpreter automatically discriminatesbetween deceptive and truthful state-ments based on tag densities.
The textsused in our study come entirely from?real world?
sources?criminal state-ments, police interrogations and legal tes-timony.
The corpus was hand-tagged forthe truth value of all propositions thatcould be externally verified as true orfalse.
Classification and Regression Treetechniques suggest that the approach isfeasible, with the model able to identify74.9% of the T/F propositions correctly.Implementation of an automatic taggerwith a large subset of tags performedwell on test data, producing an averagescore of 68.6% recall and 85.3% preci-?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.sion when compared to the performanceof human taggers on the same subset.1.
IntroductionThe ability to detect deceptive statements in textand speech has broad applications in law en-forcement and intelligence gathering.
The scien-tific study of deception in language dates at leastfrom Undeutsch (1954, 1989), who hypothesizedthat it is ?not the veracity of the reporting personbut the truthfulness of the statement that mattersand there are certain relatively exact, definable,descriptive criteria that form a key tool for thedetermination of the truthfulness of statements?.Reviews by Shuy (1998), Vrij (2000), and De-Paulo et al (2003) indicate that many types ofdeception can be identified because the liar?sverbal and non-verbal behavior varies considera-bly from that of the truth teller?s.
Even so, theliterature reports that human lie detectors rarelyperform at a level above chance.
Vrij (2000)gives a summary of 39 studies of human abilityto detect lies.
The majority of the studies reportaccuracy rates between 45-60%, with the meanaccuracy rate at 56.6%.The goal of our research is to develop andimplement a system for automatically identifyingdeceptive and truthful statements in narrativesand transcribed interviews.
We focus exclusivelyon verbal cues to deception for this initialexperiment,  ignoring at present potentialprosodic cues (but see Hirschberg et al).41In this paper, we describe a language-basedanalysis of deception that we have constructedand tested using ?real world?
sources?criminalnarratives, police interrogations and legaltestimony.
Our analysis comprises twocomponents:  a set of deception indicators thatare used for tagging a document and aninterpreter that associates tag clusters with adeception likelihood.
We tested the analysis byidentifying propositions in the corpus that couldbe verified as true or false and then comparingthe predictions of our model against this corpusof ground truth.
Our analysis acheived anaccuracy rate of 74.9%.
In the remainder of thispaper, we will present the analysis and a detaileddescription of our test results.
Implementation ofthe analysis will also be discussed.2.
Studying DeceptionThe literature on deception comes primarily fromexperimental psychology where much of theconcentration is on lies in social life and much ofthe experimentation is done in laboratory settingswhere subjects are prompted to lie1.
These stud-ies lack the element of deception under stress.Because of the difficulties of collecting and cor-roborating testimony in legal settings, analysis ofso-called ?high stakes?
data is harder to come by.To our knowledge, only two studies (Smith,2001; Adams, 2002) correlate linguistic cueswith deception using high stakes data.
For ourdata we have relied exclusively on police de-partment transcripts and high profile cases wherethe ground truth facts of the case can be estab-lished.Previous studies correlating linguistic fea-tures with deceptive behavior (Smith, 2001; Ad-ams, 2002; Newman et al 2003, and studies citedin DePaulo et al 2003) have classified narratorsas truth-tellers or liars according to the presence,number and distribution of deception indicatorsin their narratives.
Newman, et al (2003), forexample, proposes an analysis based on wordlikelihoods for semantically defined items suchas action verbs, negative emotion words and pro-nouns.
Narratives for their study were generatedin the laboratory by student subjects.
The goalsof the project were to determine how well theirword likelihood analysis classified the presumedauthor of each narrative as a liar or truth-tellerand to compare their system's performance tothat of human subjects.
The analysis correctlystatements.ive load.d Stiff, 1993).1 We define deception as a deliberate attempt to mislead.We use the terms lying and deceiving interchangeably.achieved an overall distinction between liars andtruth tellers 61% of the time.Our research on deception detection differsfrom most previous work in two important ways.First, we analyze naturally occurring data, i.e.actual civil and criminal narratives instead oflaboratory generated data.
This gives us accessto productions that cannot be replicated inlaboratory experiments for ethical reasons.Second, we focus on the classification of specificstatements within a narrative rather thancharacterizing an entire narrative or speaker astruthful or deceptive.
We assume that narratorsare neither always truthful nor always deceptive.Rather, every narrative consists of declarations,or assertions of fact, that retain a constant valueof truth or falsehood.
In this respect, we are closeto Undeutsch?s hypothesis in that we are nottesting the veracity of the narrator but thetruthfulness of the narrator?sThe purpose of our analysis is to assisthuman evaluators (e.g.
legal professionals,intelligence analysts, employment interviewers)in assessing a text?s contents.
Hence thequestions that we must answer are whether it ispossible to classify specific declarations as trueor deceptive using only linguistic cues and, if so,then how successfully an automated system canperform the task.
Our research makes no claimas to the cause of a speaker?s behavior, e.g.whether deception cues emerge as a function ofemotional stress or excessive cognit3.
Linguistic Markers of DeceptionThe literature on verbal cues to deceptionindicates that fabricated narrative may differfrom truthful narrative at all levels from globaldiscourse to individual word choice.
Features ofnarrative structure and length, text coherence,factual and sensory detail, filled pauses, syntacticstructure choice, verbal immediacy, negativeexpressions, tentative constructions, referentialexpressions, and particular phrasings have allbeen shown to differentiate truthful fromdeceptive statements in text (Adams, 2002;DePaulo et al, 2003; Miller anIn the area of forensic psychology, StatementValidity Assessment is the most commonly usedtechnique for measuring the veracity of verbalstatements.
SVA examines a transcribed inter-view for 19 criteria such as quantity of detail,embedding of the narrative in context, descrip-tions of interactions and reproduction of conver-sations (Steller & K?hnken, 1989).
Tests of SVA42show that users are able to detect deceptionabove the level of chance -- the level at whichthe lay person functions in identifying deception?
with some criteria performing considerablybetter (Vrij, 2000).
An SVA analysis is admissi-ble as court evidence in Germany, the Nether-lands, and Sweden.In the criminal justice arena, another tech-nique, Statement Analysis, or Scientific ContentAnalysis (SCAN), (Sapir, 1987) examines open-ended written accounts in which the writerschoose where to begin and what to include in thestatements.
According to Sapir (1995) ?whenpeople are given the choice to give their ownexplanation in their own words, they wouldchoose to be truthful .
.
.
.
it is very difficult to liewith commitment.
?SCAN ?claims to be able to detect instances ofpotential deception within the language behav-iour of an individual; it does not claim to identifywhether the suspect is lying?
(Smith, 2001).
Assuch, its goal is the one we have adopted: tohighlight areas of a text that require clarificationas part of an interview strategy.Despite SCAN?s claim that it does not aim toclassify a suspect as truthful or deceptive, thevalidations of SCAN cues to deception to date(Smith, 2001; Adams, 2002) evaluate the tech-nique against entire statements classified as T orF.
Our approach differs in that we evaluate sepa-rately portions of the statement as true or decep-tive based on the density of cues in that portion.4.
Deception Analysis for an NLP SystemOur analysis is produced by two passes over theinput text.
In the first pass the text is tagged fordeception indicators using a mix of automatedand manual techniques.
In the second pass thetext is sent to an automated interpreter that calcu-lates tag density using moving average and wordproximity measures.
The output of the inter-preter is a segmentation of the text into truthfuland deceptive areas.4.1 Deception IndicatorsWe have selected 12 linguistic indicators of de-ception cited in the psychological and criminaljustice literature that can be formally representedand automated in an NLP system.
The indicatorsfall into three classes.
(1) Lack of commitment to a statement or dec-laration.
The speaker uses linguistic devices toavoid making a direct statement of fact.
Five ofthe indicators fit into this class: (i) linguistichedges (described below) including non-factiveverbs and nominals; (ii) qualified assertions,which leave open whether an act was performed,e.g.
I needed to get my inhaler; (iii) unexplainedlapses of time, e.g.
later that day; (iv) overzeal-ous expressions, e.g.
I swear to God, and (v) ra-tionalization of an action, e.g.
I was unfamiliarwith the road.
(2)  Preference for negative expressions inword choice, syntactic structure and semantics.This class comprises three indicators: (i) negativeforms, either complete words such as never ornegative morphemes as in inconceivable; (ii)negative emotions, e.g.
I was a nervous wreck;(iii) memory loss, e.g.
I forget.
(3)  Inconsistencies with respect to verb andnoun forms.
Four of the indicators make up thisclass: (i) verb tense changes (described below);(ii) thematic role changes, e.g.
changing the the-matic role of a NP from agent in one sentence topatient in another; (iii) noun phrase changes,where different NP forms are used for the samereferent or to change the focus of a narrative; (iv)pronoun changes (described below) which aresimilar to noun phrase changesTo clarify our exposition, three of the indica-tors are described in more detail below.
It is im-portant to note with respect to these indicators ofdeception that deceptive passages vary consid-erably in the types and mix of indicators used,and the particular words used within an indicatortype vary depending on factors such as race,gender, and socioeconomic status.Verb TenseThe literature assumes that past tense narrative isthe norm for truthful accounts of past events(Dulaney, 1982; Sapir, 1987; Rudacille, 1994).However, as Porter and Yuille (1996) demon-strate, it is deviations from the past tense thatcorrelate with deception.
Indeed, changes intense are often more indicative of deception thanthe overall choice of tense.
The most often citedexample of tense change in a criminal statementis that of Susan Smith, who released the brake onher car letting her two small children insideplunge to their deaths.
"I just feel hopeless," shesaid.
"I can't do enough.
My children wanted me.They needed me.
And now I can't help them.
Ijust feel like such a failure."
While her state-ments about herself were couched in the presenttense, those about her children were already inthe past.43HedgesThe terms ?hedge?
and ?hedging?
were intro-duced by Lakoff (1972) to describe words?whose meaning implicitly involves fuzziness?,e.g., maybe, I guess, and sort of.
The use ofhedges has been widely studied in logic andpragmatics, and for practical applications liketranslation and language teaching (for a review,see Schr?der & Zimmer, 1997).
In the forensicpsychology literature, it has been correlated withdeception (Knapp et al, 1974; Porter & Yuille,1996; Vrij & Heaven, 1999).Hedge types in our data include non-factiveverbs like think and believe, non-factive NPs likemy understanding and my recollection, epistemicadjectives and adverbs like possible and ap-proximately, indefinite NPs like something andstuff, and miscellaneous phrases like a glimpseand between 9 and 9:30.The particular types of hedging that appear inour data depend heavily on the socioeconomicstatus of the speaker and the type of crime.
The285 hedges in Jeffrey Skilling?s 7562 word En-ron testimony include 21 cases of my recollec-tion, 9 of my understanding, and 7 of to myknowledge while the 42 hedges in the car thief?s2282 word testimony include 6 cases of shit (do-ing a little painting, and roofing, and shit), 6 ofjust and 4 of probably.
Despite the differences instyle, however, the deceptive behavior in bothcases is similar.Changes in Referential ExpressionsLaboratory studies of deception have found thatdeceivers tend to use fewer self-referencing ex-pressions (I, my, mine) than truth-tellers andfewer references to others (Knapp et al, 1974;Dulaney, 1982; Newman et al, 2003).
In exam-ining a specific real world narrative, however, itis impossible to tell what a narrator?s truthfulbaseline use of referential expressions is, so thelaboratory findings are hard to carry over to ac-tual criminal narratives.On the other hand, changes in the use of refer-ential expressions, like changes in verb tense,have also been cited as indicative of deception(Sapir, 1987; Adams, 1996), and these changescan be captured formally.
Such changes in refer-ence often involve the distancing of an item; forexample, in the narrative of Captain McDonald,he describes ?my wife?
and ?my daughter?
sleep-ing, but he reports the crime to an emergencynumber as follows, with his wife and daughterreferred to as some people:So I told him that I needed a doctor and anambulance and that some people had beenstabbed.Deceptive statements may also omit refer-ences entirely.
Scott Peterson?s initial police in-terview is characterized by a high number ofomitted first person references:BROCCHINI: You drive straight home?PETERSON: To the warehouse, droppedoff the boat.4.2 Identifying a Text Passage as Deceptive orNon-deceptiveThe presence or absence of a cue is not in itselfsufficient to determine whether the language isdeceptive or truthful.
Linguistic hedges andother deception indicators often occur in normallanguage use.
We hypothesize, however, that thedistribution and density of the indicators wouldcorrelate with deceptive behavior.2  Areas of anarrative that contain a clustering of deceptivematerial may consist of outright lies or they maybe evasive or misleading, while areas lacking inindicator clusters are likely to be truthful.We use a moving average (MA) program tofind clusters of indicators in a text.
Initially, theMA assigns each word in the text a proximityscore based on its distance, measured in wordcount, to the nearest deception indicator.
Eachscore is then recalculated by applying a MAwindow of N words.
The MA sums the scoresfor N/2 words to the left and right of the currentword and divides the result by N to obtain therevised score.
Clusters of low word scores indi-cate deceptive areas of the text, high scoringclusters indicate truthful areas.
Hence, whenapplied to a text, the MA allows us to segment anentire text automatically into non-overlappingregions that are identified as likely true, likelydeceptive or somewhere in between.Our approach assumes that the input text willcontain sufficient language to display scoringpatterns.
This rules out, for example, polygraphtests where answers are confined to Yes or No as2 Currently the density algorithm does not take into accountthe possibility that some indicators may be more importantthan others.
We plan to use the results of this initial test todetermine the relative contribution of each tag type to theaccuracy of the identification of deception.44well as short answer interviews that focus onsimple factual statements such as names and ad-dresses.
Based on the data  examined so far, weestimate the analysis requires a minimum 100words to produce useful results.5.
Corpora and AnnotationThe corpus used for developing our approach todeception detection was assembled from criminalstatements, police interrogations, depositions andlegal testimony; the texts describe a mix of vio-lent and property crimes, white collar crime andcivil litigation.
Because of the difficulty in ob-taining corpora and ground truth information, thetotal corpus size is small--slightly over 30,000words.For this experiment, we selected a corpus sub-set of 25,687 words.
Table 1 summarizes thecorpus subset:SourceWord CountCriminal statements (3) 1,527Police interrogations (2) 3,922Tobacco lawsuit deposition 12,762Enron congress.
testimony 7,476Total25,687Table 1.
Corpora Used in the ExperimentEach document in the experimental corpuswas tagged for two factors: (1) linguistic decep-tion indicators marked words and phrases associ-ated with deception, and (2) True/False tagsmarked propositions that were externally veri-fied.5.1.
Linguistic Annotation (Tagging)A team of linguists tagged the corpus for thetwelve linguistic indicators of deception de-scribed above.
For each document in the corpus,two people assigned the deception tags inde-pendently.
Differences in tagging were then ad-judicated by the two taggers and a third linguist.Because the original tagging work was focusedon research and discovery, inter-rater reliabilitystatistics are not very revealing.
However, cur-rent work on new corpora more closely resem-bles other tagging tasks.
In this case we havefound inter-rater reliability at 96%.Tagging decisions were guided by a taggingmanual that we developed.
The manual providesextensive descriptions and examples of each tagtype.
Taggers did not have access to groundtruth facts that could have influenced their tagassignments.5.2.
True/False AnnotationWe then examined separate copies of each narra-tive for propositions that could be externallyverified.
The following is a single propositionthat asserts, despite its length, one verifiableclaim?the birthrate went down:The number of births peaked in about 1955and from there on each year there were fewerbirths.
As a result of that each year after 1973fewer people turned 18 so the company couldno longer rely on this tremendous number ofbaby boomers reaching smoking age.Only propositions that could be verified wereused.
Verification came from supporting materialsuch as police reports and court documents andfrom statements internal to the narrative, e.g.
aconfession at the end of an interview could beused to support or refute specific claims withinthe interview.
The initial verification tagging wasdone by technical and legal researchers on theproject.
The T/F tags were later reviewed by atleast one other technical researcher.The experimental corpus contains 275 verifi-able propositions.
Table 2 gives examples ofverified propositions in the corpus.Example True FalseI didn't do work specifically onteenage smoking?All right, man, I did it, thedamage?Black male wearing a coat.
?Table 2.
Examples of Verified Propositions6.
ResultsThe dataset contained 275 propositions, of which164, or 59.6%, were externally verified as Falseand the remainder verified as True.
We testedthe ability of the model to predict T/F usingClassification and Regression Tree (CART)analysis (Breiman, et al 1984)3 with 25-foldcross-validation and a misclassification cost thatpenalizes True misclassified as False.
Table 3shows the results of the CART analysis:3 We used the QUEST program described in Loh and Shih(1997) for the modeling.
QUEST is available athttp://www.stat.wisc.edu/~loh/quest.html.45Predicted ClassFalse True % CorrectFalse 124 40 75.6ActualClassTrue 29 82 73.8Table 3.
T/F Classification Based on Cue Den-sityWe can conclude that the model identifies de-ceptive language at a rate significantly betterthan chance.
Moreover, by tuning the scores tofavor high recall for false propositions, it be-comes possible to adapt the model to applicationswhere low precision on true propositions is not adrawback, e.g.
pre-trial interviews where investi-gators are looking for leads.
The results in Table4 show how we might gear the analysis to thisclass of applications.Predicted ClassFalse True % CorrectFalse 151 13 92.6ActualClassTrue 66 45 40.5Table 4.
Penalizing F Misclassified as TFinally, it should be noted that input tothe analysis consisted of individual files withsome files marked for topic changes.
In prepar-ing the data for this test, we found that, in manycases, the moving average allowed the lowscores assigned to deceptive language to influ-ence the scores of nearby truthful language.
Thistypically occurs when the narrative contains achange in topic.
For example, in the depositionexcerpt below, there is a topic change from teen-age smokers to the definition of psychographicstudies.
The hedge so far as I know belongs withthe first topic but not the second.
However, themoving average allows the low scores triggeredby the hedge to improperly affect scores in thenew topic:Q:   Do you know anybody who did havedata that would allow a market penetra-tion study of the type I've asked about tobe performed.A:  {So far as I know%HEDGE} only thefederal government.Q:   Are you familiar with the phrasepsychographic study from your work atPhilip Morris?A:  Yes.Q:   What is a psychographic study?To mitigate the effect of topic change, we in-serted eleven topic change boundaries.
The re-sults suggest that language is "reset" when a newtopic is introduced by the interviewer or inter-viewee.7.
A Deception Indicator TaggerThe results described in the previous section pro-vide support for the deception indicator (DI) ap-proach we have developed.
For theimplementation, we selected a subset of tagswhose contextual conditions were well estab-lished by the literature and our own investiga-tion.
In these cases we were able to formalizethe rules for automatic assignment of the tags.We excluded tags whose contextual conditionsare still being researched, i.e., tag assignmentsthat require human judgment.The tagger was constructed as a rule-basedsystem that uses a combination of context-freeand context sensitive substitutions.
An exampleof a context free substitution is ?Mark all occur-rences of Oh, God as an overzealous statement?.A context sensitive substitution is the rule thatinterprets something as a hedge if it is not modi-fied, i.e., followed by a relative clause or prepo-sitional phrase.In some cases the tagger refers to structureand part of speech.
For example, may as a modalverb (may_MD) is a hedge.
Certain verb+ infini-tive complement constructions, e.g.
I attemptedto open the door, make up a qualified assertion.Syntactic structure is assigned by the CASSchunk parser (Abney, 1990).
Part of speech tagsare assigned by Brill?s tagger (Brill, 1992).
TheDI tag rules apply to the output of the parser andPOS tagger.The subset of tags implemented in the taggercomprises 86% of all tags that occur in the train-ing corpus.
To see how well the DI tagger cov-ered the subset, we first ran the tagger on thetraining corpus.
70% of the subset tags were cor-rectly identified in that corpus, with 76% preci-sion.
We then tested the tagger on a test corpusof three files.
Each file was also handtagged bylinguistic researchers on this project.
The resultsof the test are given in Table 5.
Tag amountsrefer to the number of tags belonging to the sub-set that was implemented.46File name Handtags Autotags CorrectTagsconfession 31 20 19peterson 186 160 108deposition 720 665 625Total 937 845 752Table 5.
DI Tagger Results on Three Test FilesTable 6 provides a summary of the tagger?sperformance.File name Recall Precisionconfession .61 .95peterson .58 .675deposition .868 .939Average .686 .853Table 6.
Summary of DI Tagger ResultsThese results may reflect a bias in our trainingdata towards legal testimony?depositions arestrongly represented in the corpus, police andcriminal data less so.
Our test corpus consists ofa police interview (?peterson?
), a criminal state-ment (?confession?)
and a deposition (?deposi-tion?).
The tagger?s best performance isassociated with the deposition.8.
ConclusionThis paper has presented new results in the studyof language-based cues to deception and truth-fulness; these results come entirely from ?realworld?
sources?criminal narratives, interroga-tions, and legal testimony.
Our goal is to providea method of evaluating declarations within a sin-gle narrative or document rather than deeming anentire narrative (or narrator) as truthful or decep-tive.We first compared the predictions of linguisticcues that we adapted from the literature on de-ception against actual True/False values thatwere manually determined for 275 propositionsin our corpus.
Predictions from the linguisticindicators were determined by scoring the den-sity of indicators in text areas that contain thepropositions and using classification and regres-sion to determine cut-off values for truth prob-abilities.We then evaluated the performance of anautomated tagger that implements a large subsetof the linguistic indicators verified in our firstexperiment.
The automated tagger performedwell on test data, averaging 80.2% correct whencompared with human performance on the samedata.The results strongly suggest that linguisticcues provide a guide to deceptive areas of a text.The predictions based on linguistic cues werecorrect in distinguishing False propositions over75% of the time, and over 90% for applicationswhere recall of False, but not True, is required.Results of the automatic tagger?s performancesuggest that we will eventually achieve a fullyautomated system for processing depositions andother documents in which veracity is an impor-tant issue.ReferencesAbney, S.  1990.
Rapid incremental parsing withrepair.
In Proceedings of the 6th New OEDConference: Electronic Text Research, pp.
1-9.
University of Waterloo, Waterloo, Ontario.Adams, S. 1996.
Statement analysis: What dosuspects words really reveal?
The FBI LawEnforcement Bulletin.
65(10).www.fbi.gov/publications/leb/1996/oct964.txtAdams, S. 2002.
Communication under stress:indicators of veracity and deception in writtennarratives.
Ph.D. dissertation, Virginia Poly-technic Institute and State UniversityBrill, E.  1992.
A simple rule-based part-of-speech tagger.
In Proceedings of the ThirdConference on Applied Natural LanguageProcessing, pp.
152-155.
Trento, Italy.DePaulo, B. M., J.J. Lindsay, B.E.
Malone, L.Muhlenbruck, K. Charlton, and H. Cooper.2003.
Cues to deception.
Psychological Bulle-tin, 129(1), 74-118.Dulaney, E.F. Jr. 1982.
Changes in language be-havior as a function of veracity.
Human Com-munication Research 9, 75-82.Hirschberg, J., S. Benus, J. Brenier, F. Enos, S.Friedman, S. Gilman, C. Girand, M. Graci-arena, A. Kathol, L. Michaelis, B. Pellom, E.Shriberg and A. Stolcke.
2005.INTERSPEECH 2005.
Sept. 408, Lisbon, Por-tugal.Knapp, M.L., Hart, R.P., and Dennis, H.S.
1974.An exploration of deception as a communica-tion construct.
Human Communication Re-search, 1, 15-29.Lakoff, G.  1972.
Hedges: A study in meaningcriteria and the logic of fuzzy concepts.
In47Papers from the 8th Regional Meeting, Chi-cago Linguistic Society.Loh, W.-Y.
and Shih, Y.-S. 1997.
Split selectionmethods for classification trees.
StatisticaSinica 7:815-840.Miller, G. R. and J.
B.
Stiff.
1993.
DeceptiveCommunication.
Sage Publications.
ThousandOaks, CA.Newman, M. L., Pennebaker, J. W., Berry, D. S.and J. M. Richards.
2003.
Lying words: pre-dicting deception from linguistic styles.
Per-sonality and Social Psychology Bulletin.
29,665-675.Porter, S. & Yuille, J.
(1996).
The language ofdeceit: An investigation of the verbal clues inthe interrogation context.
Law & Human Be-havior, 20(4) 443-458.Rudacille, W.C. 1994.
Identifying Lies in Dis-guise.
Kendall Hunt.
Dubuque, IO.Sapir, A.
1987.
Scientific Content Analysis(SCAN).
Laboratory of Scientific Interroga-tion.
Phoenix, AZ.Sapir, A.
1995.
The View Guidebook: VerbalInquiry ?
the Effective Witness.
Laboratory ofScientific Interrogation.
Phoenix, AZ.Schr?der, H. and D. Zimmer.
1997.
Hedging re-search in pragmatics: A bibliographical re-search guide to hedging.
In R. Markkanen andH.
Schroder (eds.)
Hedging and Discourse:Approaches to the Analysis of a PragmaticPhenomenon in Academic Text.
Walter deGruyter, Berlin.Shuy, R.  1998.
The Language of Confession,Interrogation and Deception.
Sage Publica-tions, Thousand Oaks, CA.Smith, N. 2001.
Reading between the lines: Anevaluation of the scientific content analysistechnique (SCAN).
Police Research Series.London,UK.www.homeoffice.gov.uk/rds/prgpdfs/prs135.pdfSteller, M. and G. Kohnken.
1989.
Criteria-Based Content Analysis.
In D.C. Raskin (ed.
)Psychological Methods in Criminal Investiga-tion and Evidence.
Springer-Verlag, NewYork, 217-245.Undeutsch, U.
1989.
The development of state-ment reality analysis.
In J.C. Yuille (ed.
)Credibility Assessment.
Dordrecht: Kluwer,101-121.Undeutsch, U.
(1954).
Die Entwicklung der ge-richtspsychologischen Gutachtertatigkeit.
InA.
Wellek (Ed.
), Bericht uber den 19, Kon-gress der Deutschen Gesellschaft fur Psy-chologie (pp.
1132-154).
Gottingen: Verlagfur Psychologie.Vrij, A.
2000.
Detecting Lies and Deceit.
JohnWiley & Sons, Chichester, UK.Vrij, A. and Heaven, S. 1999.
Vocal and verbalindicators of deception as a function of liecomplexity.
Psychology, Crime, and Law 5,203-215.48
