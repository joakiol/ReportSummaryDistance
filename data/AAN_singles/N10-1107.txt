Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 713?716,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAn MDL-based approach to extracting subword units forgrapheme-to-phoneme conversionSravana ReddyDepartment of Computer ScienceThe University of ChicagoChicago, IL 60637sravana@cs.uchicago.eduJohn GoldsmithDepartments of Linguisticsand Computer ScienceThe University of ChicagoChicago, IL 60637goldsmith@uchicago.eduAbstractWe address a key problem in grapheme-to-phoneme conversion: the ambiguity in map-ping grapheme units to phonemes.
Rather thanusing single letters and phonemes as units, wepropose learning chunks, or subwords, to re-duce ambiguity.
This can be interpreted aslearning a lexicon of subwords that has min-imum description length.
We implement analgorithm to build such a lexicon, as well as asimple decoder that uses these subwords.1 IntroductionA system for converting written words to their pro-nunciations is an important component of speech-related applications, especially in large vocabularytasks.
This problem, commonly termed ?grapheme-to-phoneme conversion?, or g2p, is non-trivial forseveral written languages, including English, since agiven letter (grapheme) may represent one of severalpossible phonemes, depending on the context.
Be-cause the length of the context varies throughout thedictionary, fixed-length contexts may overfit somewords, or inaccurately model others.We approach this problem by treating g2p as afunction from contiguous sequences of graphemes,which we call ?grapheme subwords?, to sequencesof phonemes (?phoneme subwords?
), so that there isminimal ambiguity in finding the phoneme subwordthat corresponds to a given grapheme subword.
Thatis, we seek to minimize both these quantities:1.
The conditional entropy of the phoneme sub-words given a grapheme subword.
This di-rectly tackles the problem of ambiguity ?
a per-fectly unambiguous phoneme subword condi-tional distribution would have entropy = 0.2.
The entropy of the grapheme subwords.
Thisprevents the model from getting arbitrarilycomplex.As a toy example, consider the following word-pronunciation1 pairs:time T AY Msting S T IH NGnegation N AH G EY SH AH NThere are at least 5 graphemes whose correspond-ing phoneme distribution is ambiguous (?i?, ?e?, ?t?,?n?, ?g?).
In the segmentation below, every graphemesubword corresponds to only one phoneme subword:t + ime T + AY Ms + t + ing S + T + IH NGneg + a + tion N AH G + EY + SH AH N2 Related WorkMany grapheme-to-phoneme algorithms rely onsomething resembling subwords; these are mainlyused to account for sequences of letters representinga single phoneme (?ph?
for F), or vice versa (?x?
forK S).
Some of the early works that create one-to-one alignments between a word and its pronuncia-tion address these cases by allowing a letter to mapto one phoneme, a null phoneme, or 2-3 phonemes.Jiampojamarn and Kondrak (2009) useexpectation-maximization (EM) to learn many-to-many alignments between words and pro-nunciations, effectively obtaining subwords.1All phonemes are denoted by their Arpabet representations.713Joint-sequence models divide a word-pronunciationpair into a sequence of disjoint graphones orgraphonemes ?
tuples containing grapheme andphoneme subwords.
Such segmentations mayinclude only trivial graphones containing subwordsof length at most 1 (Chen, 2003).
Other suchmodels use EM to learn the maximum likelihoodsegmentation into graphones (Deligne and Bimbot,1995; Bisani and Ney, 2008; Vozilla et al, 2003).Subwords ?
or phrases ?
are used widely in ma-chine translation.
There is a large body of work onphrase extraction starting from word alignments; seeKoehn et al (2003) for a review.
Marcu and Wong(2002) learn phrases directly from sentence pairs us-ing a joint probability model.3 Subword Extraction3.1 Motivation for using MDLConsider a lexicon of grapheme subwords G andphoneme subwords P that is extracted from a dic-tionary of word-pronunciation pairs, along with ajoint probability distribution over G and P .
Asstated earlier, our objective is to minimize the en-tropy of phoneme subwords conditioned on a givengrapheme subword, as well as the entropy of thegrapheme subwords.
That is, we would like to min-imize H(P|G) +H(G), which isH(G,P) = ?
?g?G?p?Ppr(g, p) log pr(g, p) (1)This objective can be restated as minimizing theexpected description length of the lexicon, which isgiven by its entropy.
This is reflected in the MDLprinciple (Rissanen, 1978), which seeks to find alexicon such that the description length of the lex-icon (and the compression of the data under the lex-icon) is minimized.3.2 Lexicon InductionWe begin with an initial alignment between a word?sgraphemes and the phonemes in its pronunciationfor all word-pronunciation pairs in the training dic-tionary.
These alignments are derived using the stan-dard string edit distance dynamic programming al-gorithm (Wagner and Fischer, 1974), giving a listof tuples t = [(w1, r1), (w2, r2), .
.
.]
for each word-pronunciation pair.2 The set of all tuple lists t com-poses the training dictionary T .The initial lexicon is composed of all singletongraphemes and phonemes (including null).
Theprobability pr(g, p) is taken to be the number oftimes the tuple (g, p) occurs in T divided by the totalnumber of tuples over all alignments in T .Following a procedure similar the word-discoveryalgorithm of de Marcken (1996), the lexicon is iter-atively updated as sketched in Table 1.
At no pointdo we delete singleton graphemes or phonemes.The subwords in the final updated lexicon are thenused to decode the pronunciations of unseen words.4 G2P Decoding4.1 Joint segmentation and decodingFinding the pronunciation of a word based on theinduced subword lexicon involves segmenting theword into a sequence of grapheme subwords, andmapping it to a sequence of phoneme subwords.One possibility is carry these steps out sequen-tially: first parse the word into grapheme subwords,and then use a sequence labeling algorithm to findthe best corresponding sequence of phoneme sub-words.
However, it is likely that the true pronuncia-tion of a word is not derived from its best parse intographeme units.
For example, the best parse of theword ?gnat?
is ?g nat?, which yields the pronuncia-tion G N AE T, while the parse ?gn at?
would givethe correct pronunciation N AE T.Therefore, we search for the best pronunciationover all segmentations of the word, adapting themonotone search algorithm proposed by Zens andNey (2004) for phrase-based machine translation.34.2 SmoothingA bigram model is used over both the graphemeand phoneme subwords.
These bigrams need to besmoothed before the decoding step.
Adding an equalprobability mass to unseen bigrams would fail to re-flect simple phonotactics (patterns that govern sound2Phoneme insertions and deletions are represented by thenull grapheme and null phoneme respectively.3The key adaptation is in using a bigram model over bothgraphemes and phonemes, rather than only phonemes as in theoriginal algorithm.714Table 1: Concatenative algorithm for building a subword lexicon that minimizes description length.
The input is T ,the set of alignments, and a threshold integer k, which is tuned using a held-out development set.1 Update pr(g, p) by computing the posterior probabilities of the tuple (g, p) in T ,using the forward-backward algorithm.
Repeat once more to get an intermediate lexicon.2 Compute the Viterbi parse of each t ?
T under the lexicon derived in step 1.3 Let A, the set of candidate tuples for addition to the lexicon, contain all tuples (wiwi+1, riri+1) such that(wi, ri) and (wi+1, ri+1) are adjacent more than k times in the computed Viterbi parses.
For each (g, p) ?
A,estimate the change in description length of the lexicon if (g, p) is added.
If description length decreases,remove any null symbols within g and p, and add (g, p) to the lexicon.4 Repeat steps 1 and 2.5 Delete all tuples that do not occur in any of the Viterbi parses.6 Compare the description length of the new lexicon with the lexicon at the start of the iteration.
If thedifference is sufficiently small, return the new lexicon; else, repeat from step 1.sequences) in several cases.
For example, the bi-gram L UW K + S is much more likely than L UWK + Z, since S is more likely than Z to follow K.To introduce a bias towards phonotacticaly likelybigrams, we define the smoothed bigram probabilityof the subword a following a subword b.
Given thatb is made up of a sequence of l phonemes b1b2 .
.
.
bl,the probability is defined as the interpolation4:prnew(a|b) = ?1pr(a|b1b2 .
.
.
bl) +?2pr(a|b1b2 .
.
.
bl?1) + ?3pr(a|b1b2 .
.
.
bl?2)Both the grapheme and phoneme subword bi-grams are smoothed as described.5 ResultsWe test our algorithm on the CMU PronouncingDictionary5.
The dictionary is divided randomlyinto a training (90% of the data) and a test set.
Per-formance is evaluated by measuring the phoneme er-ror rate (PER) and the word error rate (WER).The subword extraction algorithm converges in 3iterations.We run the g2p decoder using the lexiconafter 3 iterations, as well as after 0, 1 and 2 itera-tions.
The results are shown in Table 2.Figure 1 compares the results of our method (de-noted by ?MDL-Sub?)
to two baselines, at differentvalues of maximum subword length.
To evaluate thequality of our subwords, we substitute another ex-traction algorithm to create the lexicon ?
the grow-diag-final phrase extraction method (Koehn et al,4In our experiments, we set ?1 = 0.5, ?2 = 0.3, ?3 = 0.2.5The CMU Pronouncing Dictionary.
Available online athttp://www.speech.cs.cmu.edu/cgi-bin/cmudictTable 2: Results after each iteration of subword extrac-tion.
While the maximum subword length after iteration3 is 8, the vast majority of subwords have length 6 or less.# subwords Max subword WER PERlength0 |G| : 27, |P| : 40 1 73.16 24.201 |G| : 819, |P| : 1254 2 48.39 12.432 |G| : 5430, |P| : 4954 4 28.32 7.163 |G| : 6417, |P| : 5358 6 26.31 6.292005), denoted by ?GD?
in the figure.
We also runthe implementation of Bisani and Ney (2008) ?
de-noted by ?BN?
?
on the same data.
BN is an exampleof a joint-sequence n-gram model, which uses a jointdistribution pr(G,P) of graphemes and phonemes(?graphones?
), conditioned on the preceding n-1 gra-phones for context information.
Since this algorithmoutperforms most of the existing g2p algorithms, itserves as a good point of comparison to the state ofthe art in g2p.
The results of BN using an n-grammodel are compared to MDL-Sub with an n-1 max-imum subword length6.The MDL-Sub lexicon does significantly betterthan the phrases extracted by GD.
While BN startsoff doing better than MDL-Sub, the latter outper-forms BN at longer subword lengths.
Most of the ad-ditional errors in BN at that stage involve grapheme-to-phoneme ambiguity ?
phonemes like AE, AA, andAH being confused for one another when mapping6The contextual information of (n-1)-length subwords withbigrams is assumed to be roughly comparable to that of veryshort subwords over n-grams.715the grapheme ?a?, and so on.
Far fewer of these er-rors are produced by our algorithm.
However, someof the longer subwords in MDL-Sub do introduceadditional errors, mainly because the extraction al-gorithm merges smaller subwords from previous it-erations.
For example, one of the items in the ex-tracted lexicon is ?icati?
?
a product of merging ?ic?and ?ati?
?
corresponding to IH K EY SH, thusgenerating incorrect pronunciations for words con-taining the string ?icating?.Figure 1: Comparison of error rates.6 ConclusionThis paper deals with translational ambiguity, whichis a major issue in grapheme-to-phoneme conver-sion.
The core of our system consists of extract-ing subwords of graphemes and phonemes from thetraining data, so that the ambiguity of deriving aphoneme subword from a grapheme subword is min-imized.
This is achieved by formalizing ambiguityin terms of the minimum description length princi-ple, and using an algorithm that reduces the descrip-tion length of the subword lexicon at each iteration.In addition, we also introduce a smoothing mech-anism which retains some of the phonotactic depen-dencies that may be lost when using subwords ratherthan singleton letters and phonemes.While retaining the basic approach to minimizingambiguity, there are some avenues for improvement.The algorithm that builds the lexicon creates a moreor less hierarchical structure ?
subwords tend to becomposed from those extracted at the previous iter-ation.
This appears to be the cause of many of theerrors produced by our method.
A subword extrac-tion algorithm that does not use a strictly bottom-upprocess may create a more robust lexicon.Our method of subword extraction could also beapplied to phrase extraction for machine transla-tion, or in finding subwords for related problems liketransliteration.
It may also be useful in deriving sub-word units for speech recognition.ReferencesMaximilian Bisani and Hermann Ney.
2008.
Joint-sequence models for grapheme-to-phoneme conver-sion.
Speech Communication, 50:434?451.Stanley F Chen.
2003.
Conditional and joint models forgrapheme-to-phoneme conversion.
In Proceedings ofEurospeech.Carl G de Marcken.
1996.
Unsupervised Language Ac-quisistion.
Ph.D. thesis, MIT.Sabine Deligne and Frederic Bimbot.
1995.
Languagemodeling by variable length sequences: theoreticalformulation and evaluation of multigrams.
In Pro-ceedings of ICASSP.Sittichai Jiampojamarn and Grzegorz Kondrak.
2009.Online discriminative training for grapheme-to-phoneme conversion.
In Proceedings of Interspeech.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT-NAACL.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh System Descriptionfor the 2005 IWSLT Speech Translation Evaluation.In Proceedings of IWSLT.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proceedings of EMNLP.Jorma Rissanen.
1978.
Modeling by the shortest datadescription.
Automatica.Paul Vozilla, Jeff Adams, Yuliya Lobacheva, and RyanThomas.
2003.
Grapheme to phoneme conversion anddictionary verification using graphonemes.
In Pro-ceedings of Eurospeech.Robert Wagner and Michael Fischer.
1974.
The string-to-string correction problem.
Journal of the ACM.Richard Zens and Hermann Ney.
2004.
Improvements inphrase-based statistical machine translation.
In Pro-ceedings of HLT-NAACL.716
