Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 39?47, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSemEval-2013 Task 5: Evaluating Phrasal SemanticsIoannis KorkontzelosNational Centre for Text MiningSchool of Computer ScienceUniversity of Manchester, UKioannis.korkontzelos@man.ac.ukTorsten ZeschUKP Lab, CompSci Dept.Technische Universita?t DarmstadtGermanyzesch@ukp.informatik.tu-darmstadt.deFabio Massimo ZanzottoDepartment of Enterprise EngineeringUniversity of Rome ?Tor Vergata?Italyzanzotto@info.uniroma2.itChris BiemannFG Language Technology, CompSci Dept.Technische Universita?t DarmstadtGermanybiem@cs.tu-darmstadt.deAbstractThis paper describes the SemEval-2013 Task5: ?Evaluating Phrasal Semantics?.
Its firstsubtask is about computing the semantic simi-larity of words and compositional phrases ofminimal length.
The second one addressesdeciding the compositionality of phrases in agiven context.
The paper discusses the impor-tance and background of these subtasks andtheir structure.
In succession, it introduces thesystems that participated and discusses evalu-ation results.1 IntroductionNumerous past tasks have focused on leveraging themeaning of word types or words in context.
Exam-ples of the former are noun categorization and theTOEFL test, examples of the latter are word sensedisambiguation, metonymy resolution, and lexicalsubstitution.
As these tasks have enjoyed a lot suc-cess, a natural progression is the pursuit of modelsthat can perform similar tasks taking into accountmultiword expressions and complex compositionalstructure.
In this paper, we present two subtasks de-signed to evaluate such phrasal models:a. Semantic similarity of words and compositionalphrasesb.
Evaluating the compositionality of phrases incontextFor example, the first subtask addresses computinghow similar the word ?valuation?
is to the compo-sitional sequence ?price assessment?, while the sec-ond subtask addresses deciding whether the phrase?piece of cake?
is used literally or figuratively in thesentence ?Labour was a piece of cake!
?.The aim of these subtasks is two-fold.
Firstly,considering that there is a spread interest lately inphrasal semantics in its various guises, they providean opportunity to draw together approaches to nu-merous related problems under a common evalua-tion set.
It is intended that after the competition,the evaluation setting and the datasets will comprisean on-going benchmark for the evaluation of thesephrasal models.Secondly, the subtasks attempt to bridge thegap between established lexical semantics and full-blown linguistic inference.
Thus, we anticipate thatthey will stimulate an increased interest around thegeneral issue of phrasal semantics.
We use the no-tion of phrasal semantics here as opposed to lexi-cal compounds or compositional semantics.
Bridg-ing the gap between lexical semantics and linguis-tic inference could provoke novel approaches to cer-tain established tasks, such as lexical entailment andparaphrase identification.
In addition, it could ul-39timately lead to improvements in a wide range ofapplications in natural language processing, suchas document retrieval, clustering and classification,question answering, query expansion, synonym ex-traction, relation extraction, automatic translation,or textual advertisement matching in search engines,all of which depend on phrasal semantics.The remainder of this paper is structured as fol-lows: Section 2 presents details about the datasources and the variety of sources applicable to thetask.
Section 3 discusses the first subtask, whichis about semantic similarity of words and compo-sitional phrases.
In subsection 3.1 the subtask isdescribed in detail together with some informationabout its background.
Subsection 3.2 discusses thedata creation process and subsection 3.3 discussesthe participating systems and their results.
Section 4introduces the second subtask, which is about eval-uating the compositionality of phrases in context.Subsection 4.1 explains the data creation process forthis subtask.
In subsection 4.2 the evaluation statis-tics of participating systems are presented.
Section5 is a discussion about the conclusions of the entiretask.
Finally, in section 6 we summarize this presen-tation and discuss briefly our vision about challengesin distributional semantics.2 Data Sources & MethodologyData instances of both subtasks are drawn from thelarge-scale, freely available WaCky corpora (Baroniet al 2009).
The resource contains corpora in 4 lan-guages: English, French, German and Italian.
TheEnglish corpus, ukWaC, consists of 2 billion wordsand was constructed by crawling to the .uk domainof the web and using medium-frequency words fromthe BNC as seeds.
The corpus is part-of-speech(PoS) tagged and lemmatized using the TreeTagger(Schmid, 1994).
The French corpus, frWaC, con-tains 1.6 billion word corpus and was constructedby web-crawling the .fr domain and using medium-frequency words from the Le Monde Diplomatiquecorpus and basic French vocabulary lists as seeds.The corpus was PoS tagged and lemmatized withthe TreeTagger.
The French corpus, deWaC, con-sists of 1.7 billion word corpus and was constructedby crawling the .de domain and using medium-frequency words from the SudDeutsche Zeitung cor-pus and basic German vocabulary lists as seeds.
Thecorpus was PoS tagged and lemmatized with theTreeTagger.
The Italian corpus, itWaC, is a 2 billionword corpus constructed from the .it domain of theweb using medium-frequency words from the Re-pubblica corpus and basic Italian vocabulary lists asseeds.
The corpus was PoS tagged with the Tree-Tagger, and lemmatized using the Morph-it!
lexicon(Zanchetta and Baroni, 2005).
Several versions ofthe WaCky corpora, with various extra annotationsor modifications are also available1.We ensured that data instances occur frequentlyenough in the WaCky corpora, so that participat-ing systems could gather statistics for building dis-tributional vectors or other uses.
As the evalua-tion data only contains very small annotated sam-ples from freely available web documents, and theoriginal source is provided, we could provide themwithout violating copyrights.The size of the WaCky corpora is suitable fortraining reliable distributional models.
Sentencesare already lemmatized and part-of-speech tagged.Participating approaches making use of distribu-tional methods, part-of-speech tags or lemmas, werestrongly encouraged to use these corpora and theirshared preprocessing, to ensure the highest possi-ble comparability of results.
Additionally, this hadthe potential to considerably reduce the workload ofparticipants.
For the first subtask, data were pro-vided in English, German and Italian and for the sec-ond subtask in English and German.The range of methods applicable to both subtaskswas deliberately not limited to any specific branch ofmethods, such as distributional or vector models ofsemantic compositionality.
We believe that the sub-tasks can be tackled from different directions and weexpect a great deal of the scientific benefit to lie inthe comparison of very different approaches, as wellas how these approaches can be combined.
An ex-ception to this rule is the fact that participants in thefirst subtask were not allowed to use directly defini-tions extracted from dictionaries or lexicons.
Sincethe subtask is considered fundamental and its datawere created from online knowledge resources, sys-tems using the same tools to address it would be oflimited use.
However, participants were allowed to1WaCky website: wacky.sslmit.unibo.it40use other information residing in dictionaries, suchas Wordnet synsets or synset relations.Participating systems were allowed to attempt oneor both subtasks, in one or all of the languages sup-ported.
However, it was expected that systems per-forming well at the first basic subtask would pro-vide a good starting point for dealing with the sec-ond subtask, which is considered harder.
Moreover,language-independent models were of special inter-est.3 Subtask 5a: Semantic Similarity ofWords and Compositional PhrasesThe aim of this subtask is to evaluate the compo-nent of a semantic model that computes the simi-larity between word sequences of different length.Participating systems are asked to estimate the se-mantic similarity of a word and a short sequence oftwo words.
For example, they should be able to fig-ure out that contact and close interaction are similarwhereas megalomania and great madness are not.This subtask addresses a core problem, since sat-isfactory performance in computing the similarity offull sentences depends on similarity computationson shorter sequences.3.1 Background and DescriptionThis subtask is based on the assumption that wefirst need a basic set of functions to compose themeaning of two words, in order to construct morecomplex models that compositionally determine themeaning of sentences, as a second step.
For compo-sitional distributional semantics, the need for thesebasic functions is discussed in Mitchell and Lapata(2008).
Since then, many models have been pro-posed for addressing the task (Mitchell and Lapata,2010; Baroni and Zamparelli, 2010; Guevara, 2010),but still comparative analysis is in general based oncomparing sequences that consist of two words.As in Zanzotto et al(2010), this subtask proposesto compare the similarity of a 2-word sequence anda single word.
This is important as it is the basicstep to analyse models that can compare any wordsequences of different length.The development and testing set for this subtaskwere built based on the idea described in Zanzottoet al(2010).
Dictionaries were used as sources ofcontact/[kon-takt]1. the act or state of touching;a touching or meeting, as oftwo things or people.2.
close interaction3.
an acquaintance, colleague,or relative through whom aperson can gain access toinformation, favors, influ-ential people, and the like.Figure 1: The definition of contact in a sample dictionarypositive training examples.
Dictionaries are naturalrepositories of equivalences between words underdefinition and sequences of words used for definingthem.
Figure 1 presents the definition of the wordcontact, from which the pair (contact, close interac-tion) can be extracted.
Such equivalences extractedfrom dictionaries can be seen as natural and unbi-ased data instances.
This idea opens numerous op-portunities:?
Since definitions in dictionaries are syntacti-cally rich, we are able to create examples fordifferent syntactic relations.?
We have the opportunity to extract positive ex-amples for languages for which dictionarieswith sufficient entries are available.Negative examples were generated by matchingwords under definition with randomly chosen defin-ing sequences.
In the following subsection, we pro-vide details about the application of this idea to buildthe development and testing set for subtask 5a.3.2 Data CreationData for this subtask were provided in English, Ger-man and Italian.
Pairs of words under definitions anddefining sequences were extracted from the English,German and Italian part of Wiktionary, respectively.In particular, for each language, all Wiktionary en-tries were downloaded and part-of-speech tagged us-ing the Genia tagger (Tsuruoka et al 2005).
Insuccession, definitions that start with noun phrases41Language Train set Test set TotalEnglish 5,861 3,907 9,768German 1,516 1,010 2,526Italian 1,275 850 2,125German - no names 1,101 733 1,834Table 1: Quantitative characteristics of the datasetswere kept, only.
For the purpose of extracting wordand sequence pairs for this subtask, we consider asnoun phrases, sequences that consist of adjectivesor noun and end with a noun.
In cases where theextracted noun phrase was longer than two words,the right-most two sequences were kept, since inmost cases noun phrases are governed by their right-most component.
Subsequently, we discarded in-stances whose words occur too infrequently in theWaCky corpora (Baroni et al 2009) of each lan-guage.
WaCky corpora are available freely and arelarge enough for participating systems to extract dis-tributional statistics.
Taking the numbers of ex-tracted instances into account, we set the frequencythresholds at 10 occurrences for English and 5 forGerman and Italian.Data instances extracted following this processwere then checked by a computational linguist.
Can-didate pairs in which the definition sequence was notjudged to be a precise and adequate definition of theword under definition were discarded.
These caseswere very limited and mostly account for shortcom-ings of the very simple pattern used for extraction.For example, the pair (standard, transmission vehi-cle) coming from the definition of ?standard?
as ?Amanual transmission vehicle?
was discarded.
Simi-larly in German, the pair (Fremde (Eng.
stranger),weibliche Person (Eng.
female person)) was dis-carded.
?Fremde?, which is of female grammat-ical genre, was defined as ?weibliche Person, dieman nicht kennt (Eng.
female person, one does notknow)?.
In Italian, the pair (paese (Eng.
land, coun-try, region), grande estensione (Eng.
large tract))was discarded, since the original definition was?grande estensione di terreno abitato e generalmentecoltivato (Eng.
large tract of land inhabited and cul-tivated in general)?.The final data sets were divided into training andheld-out testing sets, according to a 60% and 40%ratio, respectively.
The first three rows of table 1present the numbers of the train and test sets for thethree languages chosen.
It was identified that a fairpercentage of the German instances (approximately27%) refer to the definitions of first names or familynames.
This is probably a flaw of the German part ofWiktionary.
In addition, the pattern used for extrac-tion happens to apply to the definitions of names.Name instances were discarded from the Germandata set to produce the data set described in the lastrow of table 1.The training set was released approximately 3months earlier than the test data.
Instances in bothset ware annotated as positive or negative.
Test setannotations were not released to the participants, butwere used for evaluation, only.3.3 ResultsParticipating systems were evaluated on their abilityto predict correctly whether the components of eachtest instance, i.e.
word-sequence pair, are semanti-cally similar or distinct.
Participants were allowedto use or ignore the training data, i.e.
the systemscould be supervised or unsupervised.
Unsupervisedsystems were allowed to use the training data for de-velopment and parameter tuning.
Since this is a coretask, participating systems were not be able to usedictionaries or other prefabricated lists.
Instead, theywere allowed to use distributional similarity models,selectional preferences, measures of semantic simi-larity etc.Participating system responses were scored interms of standard information retrieval measures:accuracy (A), precision (P), recall (R) and F1 score(Radev et al 2003).
Systems were encouraged tosubmit at most 3 solutions for each language, butsubmissions for fewer languages were accepted.Five research teams participated.
Ten system runswere submitted for English, one for German (on dataset: German - no names) and one for Italian.
Table 2illustrates the results of the evaluation process.
Theteams of (HsH) (Wartena, 2013), CLaC (Siblini andKosseim, 2013), UMCC DLSI-(EPS) (Da?vila et al2013), and ITNLP, the Harbin Institute of Technol-ogy, approached the task in a supervised way, whileMELODI (Van de Cruys et al 2013) participatedwith two unsupervised approaches.
Interestingly,42Language Rank Participant Id run Id A R P rej.
R rej.
P F11 HsH 1 .803 .752 .837 .854 .775 .7923 CLaC 3 .794 .707 .856 .881 .750 .7742 CLaC 2 .794 .695 .867 .893 .745 .7714 CLaC 1 .788 .638 .910 .937 .721 .750English 5 MELODI lvw .748 .614 .838 .882 .695 .7096 UMCC DLSI-(EPS) 1 .724 .613 .787 .834 .683 .6897 ITNLP 3 .703 .501 .840 .904 .645 .6288 MELODI dm .689 .481 .825 .898 .634 .6089 ITNLP 1 .663 .392 .857 .934 .606 .53810 ITNLP 2 .659 .427 .797 .891 .609 .556German 1 HsH 1 .825 .765 .870 .885 .790 .814Italian 1 UMCC DLSI-(EPS) 1 .675 .576 .718 .774 .646 .640Table 2: Task 5a: Evaluation results.
A, P, R, rej.
and F1 stand for accuracy, precision, recall, rejection and F1 score,respectively.these approaches performed better than some super-vised ones for this experiment.
Below, we sum-marise the properties of participating systems.
(HsH) (Wartena, 2013) used distributed similarityand especially random indexing to compute similar-ities between words and possible definitions, underthe hypothesis that a word and its definition are dis-tributionally more similar than a word and an arbi-trary definition.
Considering all open-class words,context vectors over the entire WaCky corpus werecomputed for the word under definition, the definingsequence, its component words separately, the ad-dition and multiplication of the vectors of the com-ponent words and a general context vector.
Then,various similarity measures were computed on thevectors, including an innovative length-normalisedversion of Jensen-Shannon divergence.
The similar-ity values are used to train a Support Vector Machine(SVM) classifier (Cortes and Vapnik, 1995).The first approach (run 1) of CLaC (Siblini andKosseim, 2013) is based on a weighted semanticnetwork to measure semantic relatedness betweenthe word and the components of the phrase.
APART classifier is used to generate a partial decisiontrained on the semantic relatedness information ofthe labelled training set.
The second approach usesa supervised distributional method based on wordsfrequently occurring in the Web1TB corpus to cal-culate relatedness.
A JRip classifier is used to gen-erate rules trained on the semantic relatedness infor-mation of the training set.
This approach was usedin conjunction with the first one as a backup method(run 2).
In addition, features generated by both ap-proaches were used to train the JRIP classifier col-lectively (run 3).The first approach of MELODI (Van de Cruyset al 2013), called lvw, uses a dependency-basedvector space model computed over the ukWaC cor-pus, in combination with Latent Vector Weighting(Van de Cruys et al 2011).
The system computesthe similarity between the first noun and the headnoun of the second phrase, which was weighted ac-cording to the semantics of the modifier.
The secondapproach, called dm, used a dependency-based vec-tor space model, but, unlike the first approach, disre-garded the modifier in the defining sequence.
Sinceboth systems are unsupervised, the training data wasused to train a similarity threshold parameter, only.UMCC DLSI-(EPS) (Da?vila et al 2013) locatesthe synsets of words in data instances and computesthe semantic distances between each synset of theword under definition and each synsets of the defin-ing sequence words.
In succession, a classifier istrained using features based on distance and Word-Net relations.The first attempt of ITNLP (run 1) consisted of anSVM classifier trained on semantic similarity com-putations between the word under definition and43the defining sequence in each instance.
Their sec-ond attempt also uses an SVM, however trained onWordNet-based similarities.
The third attempt ofITNLP is a combination of the previous two; it com-bines their features to train an SVM classifier.4 Subtask 5b: Semantic Compositionalityin ContextAn interesting sub-problem of semantic composi-tionality is to decide whether a target phrase is usedin its literal or figurative meaning in a given con-text.
For example ?big picture?
might be used lit-erally as in Click here for a bigger picture or figura-tively as in To solve this problem, you have to look atthe bigger picture.
Another example is ?old school?which can also be used literally or figuratively: Hewill go down in history as one of the old school, atrue gentlemen.
vs. During the 1970?s the hall of theold school was converted into the library.Being able to detect whether a phrase is used lit-erally or figuratively is e.g.
especially important forinformation retrieval, where figuratively used wordsshould be treated separately to avoid false positives.For example, the example sentence He will go downin history as one of the old school, a true gentle-men.
should probably not be retrieved for the query?school?.
Rather, the insights generated from sub-task 5a could be utilized to retrieve sentences usinga similar phrase such as ?gentleman-like behavior?.The task may also be of interest to the related re-search fields of metaphor detection and idiom iden-tification.There were no restrictions regarding the array ofmethods, and the kind of resources that could beemployed for this task.
In particular, participantswere allowed to make use of pre-fabricated lists ofphrases annotated with their probability of beingused figuratively from publicly available sources, orto produce these lists from corpora.
Assessing howwell the phrase suits its context might be tackledusing e.g.
measures of semantic relatedness as wellas distributional models learned from the underlyingcorpus.Participants of this subtask were provided withreal usage examples of target phrases.
For each us-age example, the task is to make a binary decisionwhether the target phrase is used literally or figu-ratively in this context.
Systems were tested in twodifferent disciplines: a known phrases task where alltarget phrases in the test set were contained in thetraining, and an unknown phrases setting, where alltarget phrases in the test set were unseen.4.1 Data CreationThe first step in creating the corpus was to compilea list of phrases that can be used either literally ormetaphorically.
Thus, we created an initial list ofseveral thousand English idioms from Wiktionary bylisting all entries under the category ENGLISH ID-IOMS using the JWKTL Wiktionary API (Zesch etal., 2008).
We manually filtered the list removingmost idioms that are very unlikely to be ever usedliterally (anymore), e.g.
to knock on heaven?s door.For each of the resulting list of phrases, we extractedusage contexts from the ukWaC corpus (Baroni etal., 2009).
Each usage context contains 5 sentences,where the sentence with the target phrase appears ina randomized position.
Due to segmentation errors,some usage contexts actually might contain less than5 sentences, but we manually filtered all usage con-texts where the remaining context was insufficient.This was done in the final cleaning step where wealso manually removed (near) duplicates, obviousspam, encoding problems etc.The target phrases in context were annotated forfigurative, literal, both or impossible to tell usage,using the CrowdFlower2 crowdsourcing annotationplatform.
We used about 8% of items as ?gold?items for quality assurance, and had each exampleannotated by three crowdworkers.
The task wascomparably easy for crowdworkers, who reached90%-94% pairwise agreement, and 95% success onthe gold items.
About 5% of items with low agree-ment and marked as impossible were removed.
Ta-ble 3 summarizes the quantitative characteristics ofall datasets resulting from this process.
We took carein sampling the data as to keep similar distributionsacross the training, development and testing parts.4.2 ResultsTraining and development datasets were made avail-able in advance, test data was provided during theevaluation period without labels.
System perfor-2www.crowdflower.com44Task Dataset # Phrases # Items Items per phrase # Liter.
# Figur.
# Bothknowntrain 10 1,424 68?188 702 719 3dev 10 358 17?47 176 181 1test 10 594 28?78 294 299 1unseentrain 31 1,114 4?75 458 653 3dev 9 342 4?74 141 200 1test 15 518 8?73 198 319 1Table 3: Quantitative characteristics of the datasetsRank System Run Accuracy1 IIRG 3 .7792 UNAL 2 .7543 UNAL 1 .7225 IIRG 1 .5304 Baseline MFC - .5036 IIRG 2 .502Table 4: Task 5b: Evaluation results for the knownphrases settingRank System Run Accuracy1 UNAL 1 .6682 UNAL 2 .6453 Baseline MFC - .6164 CLaC 1 .550Table 5: Task 5b: Evaluation results for the unseenphrases settingmance was measured in accuracy.
Since all partic-ipants provided classifications for all test items, theaccuracy score is equivalent to precision/recall/F1.Participants were allowed to enter up to three dif-ferent runs for evaluation.
We also provide baselineaccuracy scores, which are obtained by always as-signing the most frequent class (figurative).Table 4 provides the evaluation results for theknown phrases task, while Table 5 ranks participantsfor the unseen phrases task.
As expected, the un-seen phrases setting is much harder than the knownphrases setting, as for unseen phrases it is not possi-ble to learn lexicalised contextual clues.
In both set-tings, the winning entries were able to beat the MFCbaseline.
While performance in the known phrasessetting is close to 80% and thus acceptable, the gen-eral task of recognizing the literal or figurative use ofunseen phrases remains very challenging, with onlya small improvement over the baseline.
We refer tothe system descriptions for more details on the tech-niques used for this subtask: UNAL (Jimenez et al2013), IIRG (Byrne et al 2013) and CLaC (Sibliniand Kosseim, 2013).5 Task ConclusionsIn this section, we further discuss the findings andconclusion of the evaluation challenge in the task of?Phrasal Semantics?.Looking at the results of both subtasks, one ob-serves that the maximum performance achieved ishigher for the first than the second subtask.
Forthis comparison to be fair, trivial baselines should betaken into account.
A system randomly assigning anoutput value would be on average 50% correct in thefirst subtask, since the numbers of positive and neg-ative instances in the testing set are equal.
Similarly,a system assigning the most frequent class, i.e.
thefigurative use of any phrase, would be 50.3% and61.6% accurate in the second subtask for seen andunseen test instances, respectively.
It should also benoted that the testing instances in the first subtaskare unseen in the respective training set.
As a result,in terms of baselines, the second subtask on unseendata (Table 5) should be considered easier than thefirst subtask (Table 2).
However, the best perform-ing systems achieved much higher accuracy in thefirst than in the second subtask.
This contradictionconfirms our conception that the first subtask is lesscomplex than the second.In the first subtask, it is evident that no methodperforms much better or much worse than the others.45Although the participating systems have employed awide variety of approaches and tools, the differencebetween the best and worst accuracy achieved isrelatively limited, in particular approximately 14%.Even more interestingly, unsupervised approachesperformed better than some supervised ones.
Thisobservation suggests that no ?golden recipe?
hasbeen identified so far for this task.
Thus, probablydifferent processing tools take advantage of differentsources of information.
It is a matter of future re-search to identify these sources and the correspond-ing tools, and then develop hybrid methods of im-proved performance.In the second subtask, the results of evaluationon known phrases are much higher than on unseenphrases.
This was expected, as for unseen phrases itis not possible to learn lexicalised contextual clues.Thus, the second subtask has succeeded in identify-ing the complexity threshold up to which the cur-rent state-of-the-art can address the computationalproblem.
Further than this threshold, i.e.
for unseenphrases, current systems have not yet succeeded inaddressing it.
In conclusion, the difficulty in eval-uating the compositionality of previously unseenphrases in context highlights the overall complexityof the second subtask.6 Summary and Future WorkIn this paper we have presented the 5th task of Se-mEval 2013, ?Evaluating Phrasal Semantics?, whichconsists of two subtasks: (1) semantic similarity ofwords and compositional phrases, and (2) compo-sitionality of phrases in context.
The former sub-task, which focussed on the first step of composingthe meaning of phrases of any length, is less com-plex than the latter subtask, which considers the ef-fect of context to the semantics of a phrase.
Thepaper presents details about the background and im-portance of these subtasks, the data creation process,the systems that took part in the evaluation and theirresults.In the future, we expect evaluation challenges onphrasal semantics to progress towards two direc-tions: (a) the synthesis of semantics of sequenceslonger than two words, and (b) aiming to improvethe performance of systems that determine the com-positionality of previously unseen phrases in con-text.
The evaluation results of the first task sug-gest that state-of-the-art systems can compose thesemantics of two word sequences with a promisinglevel of success.
However, this task should be seenas the first step towards composing the semanticsof sentence-long sequences.
As far as subtask 5bis concerned, the accuracy achieved by the partici-pating systems on unseen testing data was low, onlyslightly better than the most frequent class baseline,which assigns the figurative use to all test phrases.Thus, the subtask cannot be considered well ad-dressed by the state-of-the-art and further progressshould be sought.AcknowledgementsThe work relevant to subtask 5a described in this pa-per is funded by the European Community?s SeventhFramework Program (FP7/2007-2013) under grantagreement no.
318736 (OSSMETER).We would like to thank Tristan Miller for help-ing with the subtleties of English idiomatic ex-pressions, and Eugenie Giesbrecht for supportin the organization of subtask 5b.
This workhas been supported by the Volkswagen Founda-tion as part of the Lichtenberg-Professorship Pro-gram under grant No.
I/82806, and by the Hes-sian research excellence program Landes-Offensivezur Entwicklung Wissenschaftlich-o?konomischerExzellenz (LOEWE) as part of the research centerDigital Humanities.46ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages1183?1193, Cambridge, MA.
Association for Compu-tational Linguistics.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The WaCky wide web: acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Lorna Byrne, Caroline Fenlon, and John Dunnion.
2013.IIRG: A naive approach to evaluating phrasal seman-tics.
In Proceedings of the 6th International Work-shop on Semantic Evaluation (SemEval 2012), At-lanta, Georgia, USA.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine Learning, 20(3):273?297.He?ctor Da?vila, Antonio Ferna?ndez Orqu?
?n, AlexanderCha?vez, Yoan Gutie?rrez, Armando Collazo, Jose?
I.Abreu, Andre?s Montoyo, and Rafael Mun?oz.
2013.UMCC DLSI-(EPS): Paraphrases detection based onsemantic distance.
In Proceedings of the 6th Inter-national Workshop on Semantic Evaluation (SemEval2012), Atlanta, Georgia, USA.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of the 2010 Workshop onGEometrical Models of Natural Language Semantics,pages 33?37, Uppsala, Sweden.
Association for Com-putational Linguistics.Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2013.
UNAL: Discriminating between literaland figurative phrasal usage using distributional statis-tics and POS tags.
In Proceedings of the 6th Inter-national Workshop on Semantic Evaluation (SemEval2012), Atlanta, Georgia, USA.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08: HLT, pages 236?244, Columbus, Ohio.
As-sociation for Computational Linguistics.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8):1388?1429.Dragomir R. Radev, Simone Teufel, Horacio Saggion,Wai Lam, John Blitzer, Hong Qi, Arda C?elebi, DanyuLiu, and Elliott Drabek.
2003.
Evaluation challengesin large-scale document summarization.
In Proceed-ings of the 41st Annual Meeting on Association forComputational Linguistics - Volume 1, ACL ?03, pages375?382, Morristown, NJ, USA.
Association for Com-putational Linguistics.Helmut Schmid.
1994.
Probabilistic Part-of-Speech tag-ging using decision trees.
In Proceedings of the In-ternational Conference on New Methods in LanguageProcessing, Manchester, UK.Reda Siblini and Leila Kosseim.
2013.
CLaC: Semanticrelatedness of words and phrases.
In Proceedings ofthe 6th International Workshop on Semantic Evalua-tion (SemEval 2012), Atlanta, Georgia, USA.Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,Tomoko Ohta, John McNaught, Sophia Ananiadou,and Jun?ichi Tsujii.
2005.
Developing a robust Part-of-Speech tagger for biomedical text.
In PanayiotisBozanis and Elias N. Houstis, editors, Advances in In-formatics, volume 3746, chapter 36, pages 382?392.Springer Berlin Heidelberg, Berlin, Heidelberg.Tim Van de Cruys, Thierry Poibeau, and Anna Korho-nen.
2011.
Latent vector weighting for word mean-ing in context.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP ?11, pages 1012?1022, Stroudsburg, PA,USA.
Association for Computational Linguistics.Tim Van de Cruys, Stergos Afantenos, and PhilippeMuller.
2013.
MELODI: Semantic similarity of wordsand compositional phrases using latent vector weight-ing.
In Proceedings of the 6th International Work-shop on Semantic Evaluation (SemEval 2012), At-lanta, Georgia, USA.Christian Wartena.
2013.
HsH: Estimating semantic sim-ilarity of words and short phrases with frequency nor-malized distance measures.
In Proceedings of the 6thInternational Workshop on Semantic Evaluation (Se-mEval 2012), Atlanta, Georgia, USA.Eros Zanchetta and Marco Baroni.
2005.
Morph-it!
: Afree corpus-based morphological resource for the ital-ian language.
Corpus Linguistics 2005, 1(1).Fabio Massimo Zanzotto, Ioannis Korkontzelos,Francesca Fallucchi, and Suresh Manandhar.
2010.Estimating linear models for compositional dis-tributional semantics.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics (COLING).Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.2008.
Extracting lexical semantic knowledge fromWikipedia and Wiktionary.
Proceedings of the Confer-ence on Language Resources and Evaluation (LREC),15:60.47
