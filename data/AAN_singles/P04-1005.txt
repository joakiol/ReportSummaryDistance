A TAG-based noisy channel model of speech repairsMark JohnsonBrown UniversityProvidence, RI 02912mj@cs.brown.eduEugene CharniakBrown UniversityProvidence, RI 02912ec@cs.brown.eduAbstractThis paper describes a noisy channel model ofspeech repairs, which can identify and correctrepairs in speech transcripts.
A syntactic parseris used as the source model, and a novel typeof TAG-based transducer is the channel model.The use of TAG is motivated by the intuitionthat the reparandum is a ?rough copy?
of therepair.
The model is trained and tested on theSwitchboard disfluency-annotated corpus.1 IntroductionMost spontaneous speech contains disfluenciessuch as partial words, filled pauses (e.g., ?uh?,?um?, ?huh?
), explicit editing terms (e.g., ?Imean?
), parenthetical asides and repairs.
Ofthese repairs pose particularly difficult problemsfor parsing and related NLP tasks.
This paperpresents an explicit generative model of speechrepairs and shows how it can eliminate this kindof disfluency.While speech repairs have been studied bypsycholinguists for some time, as far as we knowthis is the first time a probabilistic model ofspeech repairs based on a model of syntacticstructure has been described in the literature.Probabilistic models have the advantage overother kinds of models that they can in principlebe integrated with other probabilistic models toproduce a combined model that uses all avail-able evidence to select the globally optimal anal-ysis.
Shriberg and Stolcke (1998) studied the lo-cation and distribution of repairs in the Switch-board corpus, but did not propose an actualmodel of repairs.
Heeman and Allen (1999) de-scribe a noisy channel model of speech repairs,but leave ?extending the model to incorporatehigher level syntactic .
.
.
processing?
to futurework.
The previous work most closely relatedto the current work is Charniak and Johnson(2001), who used a boosted decision stub classi-fier to classify words as edited or not on a wordby word basis, but do not identify or assign aprobability to a repair as a whole.There are two innovations in this paper.First, we demonstrate that using a syntacticparser-based language model Charniak (2001)instead of bi/trigram language models signifi-cantly improves the accuracy of repair detectionand correction.
Second, we show how Tree Ad-joining Grammars (TAGs) can be used to pro-vide a precise formal description and probabilis-tic model of the crossed dependencies occurringin speech repairs.The rest of this paper is structured as fol-lows.
The next section describes the noisy chan-nel model of speech repairs and the section af-ter that explains how it can be applied to de-tect and repair speech repairs.
Section 4 evalu-ates this model on the Penn 3 disfluency-taggedSwitchboard corpus, and section 5 concludesand discusses future work.2 A noisy channel model of repairsWe follow Shriberg (1994) and most other workon speech repairs by dividing a repair into threeparts: the reparandum (the material repaired),the interregnum that is typically either emptyor consists of a filler, and the repair.
Figure 1shows these three parts for a typical repair.Most current probabilistic language modelsare based on HMMs or PCFGs, which inducelinear or tree-structured dependencies betweenwords.
The relationship between reparandumand repair seems to be quite different: therepair is a ?rough copy?
of the reparandum,often incorporating the same or very similarwords in roughly the same word order.
Thatis, they seem to involve ?crossed?
dependen-cies between the reparandum and the repair,shown in Figure 1.
Languages with an un-bounded number of crossed dependencies can-not be described by a context-free or finite-state grammar, and crossed dependencies likethese have been used to argue natural languages.
.
.
a flight to Boston,?
??
?Reparandumuh, I mean,?
??
?Interregnumto Denver?
??
?Repairon Friday .
.
.Figure 1: The structure of a typical repair, with crossing dependencies between reparandum andrepair.Imeanuha flight to Bostonto Denver on FridayFigure 2: The ?helical?
dependency structure induced by the generative model of speech repairsfor the repair depicted in Figure 1.are not context-free Shieber (1985).
Mildlycontext-sensitive grammars, such as Tree Ad-joining Grammars (TAGs) and CombinatoryCategorial Grammars, can describe such cross-ing dependencies, and that is why TAGs areused here.Figure 2 shows the combined model?s de-pendency structure for the repair of Figure 1.Interestingly, if we trace the temporal wordstring through this dependency structure, align-ing words next to the words they are dependenton, we obtain a ?helical?
type of structure famil-iar from genome models, and in fact TAGs arebeing used to model genomes for very similarreasons.The noisy channel model described here in-volves two components.
A language model de-fines a probability distribution P(X) over thesource sentences X, which do not contain re-pairs.
The channel model defines a conditionalprobability distribution P(Y |X) of surface sen-tences Y , which may contain repairs, givensource sentences.
In the work reported here,X is a word string and Y is a speech tran-scription not containing punctuation or partialwords.
We use two language models here: abigram language model, which is used in thesearch process, and a syntactic parser-based lan-guage model Charniak (2001), which is usedto rescore a set of the most likely analysis ob-tained using the bigram model.
Because thelanguage model is responsible for generating thewell-formed sentence X, it is reasonable to ex-pect that a language model that can model moreglobal properties of sentences will lead to bet-ter performance, and the results presented hereshow that this is the case.
The channel model isa stochastic TAG-based transducer; it is respon-sible for generating the repairs in the transcriptY , and it uses the ability of TAGs to straight-forwardly model crossed dependencies.2.1 Informal descriptionGiven an observed sentence Y we wish to findthe most likely source sentence X?
, where:X?
= argmaxXP(X|Y ) = argmaxXP(Y |X)P(Y ).This is the same general setup that is usedin statistical speech recognition and machinetranslation, and in these applications syntax-based language models P(Y ) yield state-of-the-art performance, so we use one such model here.The channel model P(Y |X) generates sen-tences Y given a source X.
A repair can po-tentially begin before any word of X.
When arepair has begun, the channel model incremen-tally processes the succeeding words from thestart of the repair.
Before each succeeding wordeither the repair can end or else a sequence ofwords can be inserted in the reparandum.
Atthe end of each repair, a (possibly null) inter-regnum is appended to the reparandum.The intuition motivating the channel modeldesign is that the words inserted into thereparandum are very closely related those in therepair.
Indeed, in our training data over 60% ofthe words in the reparandum are exact copies ofwords in the repair; this similarity is strong evi-dence of a repair.
The channel model is designedso that exact copy reparandum words will havehigh probability.We assume that X is a substring of Y , i.e.,that the source sentence can be obtained bydeleting words from Y , so for a fixed observedsentence there are only a finite number of pos-sible source sentences.
However, the number ofsource sentences grows exponentially with thelength of Y , so exhaustive search is probablyinfeasible.TAGs provide a systematic way of formaliz-ing the channel model, and their polynomial-time dynamic programming parsing algorithmscan be used to search for likely repairs, at leastwhen used with simple language models like abigram language model.
In this paper we firstidentify the 20 most likely analysis of each sen-tence using the TAG channel model togetherwith a bigram language model.
Then each ofthese analysis is rescored using the TAG chan-nel model and a syntactic parser based languagemodel.The TAG channel model?s analysis do not re-flect the syntactic structure of the sentence be-ing analyzed; instead they encode the crosseddependencies of the speech repairs.
If we wantto use TAG dynamic programming algorithmsto efficiently search for repairs, it is necessarythat the intersection (in language terms) of theTAG channel model and the language model it-self be describable by a TAG.
One way to guar-antee this is to use a finite state language model;this motivates our use of a bigram languagemodel.On the other hand, it seems desirable to use alanguage model that is sensitive to more globalproperties of the sentence, and we do this byreranking the initial analysis, replacing the bi-gram language model with a syntactic parserbased model.
We do not need to intersect thisparser based language model with our TAGchannel model since we evaluate each analysisseparately.2.2 The TAG channel modelThe TAG channel model defines a stochasticmapping of source sentences X into observedsentences Y .
There are several ways to de-fine transducers using TAGs such as Shieberand Schabes (1990), but the following simplemethod, inspired by finite-state transducers,suffices for the application here.
The TAG de-fines a language whose vocabulary is the set ofpairs (??{?})?(??{?
}), where ?
is the vocab-ulary of the observed sentences Y .
A string Zin this language can be interpreted as a pairof strings (Y,X), where Y is the concatena-tion of the projection of the first componentsof Z and X is the concatenation of the projec-tion of the second components.
For example,the string Z = a:a flight:flight to:?
Boston:?uh:?
I:?
mean:?
to:to Denver:Denver on:on Fri-day:Friday corresponds to the observed stringY = a flight to Boston uh I mean to Denveron Friday and the source string X = a flight toDenver on Friday.Figure 3 shows the TAG rules used to gen-erate this example.
The nonterminals in thisgrammar are of the form Nwx, Rwy:wx and I,where wx is a word appearing in the sourcestring and wy is a word appearing in the ob-served string.
Informally, the Nwx nonterminalsindicate that the preceding word wx was an-alyzed as not being part of a repair, while theRwy:wx that the preceding words wy and wx werepart of a repair.
The nonterminal I generateswords in the interregnum of a repair.
Encodingthe preceding words in the TAGs nonterminalspermits the channel model to be sensitive tolexical properties of the preceding words.
Thestart symbol is N$, where ?$?
is a distinguishedsymbol used to indicate the beginning and endof sentences.2.3 Estimating the repair channelmodel from dataThe model is trained from the disfluency andPOS tagged Switchboard corpus on the LDCPenn tree bank III CD-ROM (specifically, thefiles under dysfl/dps/swbd).
This version of thecorpus annotates the beginning and ending po-sitions of repairs as well as fillers, editing terms,asides, etc., which might serve as the interreg-num in a repair.
The corpus also includes punc-tuation and partial words, which are ignored inboth training and evaluation here since we feltthat in realistic applications these would not beavailable in speech recognizer output.
The tran-script of the example of Figure 1 would looksomething like the following:a/DT flight/NN [to/IN Boston/NNP +{F uh/UH} {E I/PRP mean/VBP} to/INDenver/NNP] on/IN Friday/NNPIn this transcription the repair is the stringfrom the opening bracket ?[?
to the interrup-tion point ?+?
; the interregnum is the sequenceof braced strings following the interregnum, andthe repair is the string that begins at the end ofthe interregnum and ends at the closing bracket?]?.
The interregnum consists of the braced(?1) Nwanta:a Na ?1 ?
Pn(repair|a)(?2) Naflight:flight Rflight:flightI?Pn(repair|flight)(?3) NDenveron:on Non ?1 ?
Pn(repair|on)(?5) Iuh II meanPi(uh Imean)(?1) Rflight:flightto:?
Rto:toR?flight:flight to:toPr(copy|flight,flight)(?2) Rto:toBoston:?
RBoston:DenverR?to:to Denver:DenverPr(subst|to, to)Pr(Boston|subst, to,Denver)(?3) RBoston:DenverR?Boston:Denver NDenver ?Pr(nonrep|Boston,Denver)(?4) RBoston,DenverRBoston,tomorrowR?Boston,Denver tomorrow:tomorrowPr(del|Boston,Denver)(?5) RBoston,Denvertomorrow:?
Rtomorrow,DenverR?Boston,DenverPr(ins|Boston,Denver)Pr(tomorrow|ins,Boston,Denver).
.
.
?1?2?5 ?1?2?3?3?4.
.
.Nwanta:a Naflight:flight Rflight:flightto:?
Rto:toBoston:?
RBoston:DenverRBoston:DenverRto:toRflight:flightIuh:?
II:?
mean:?to:toDenver:DenverNDenveron:on NonFriday:Friday NFriday.
.
.Figure 3: The TAG rules used to generate the example shown in Figure 1 and their respectiveweights, and the corresponding derivation and derived trees.expressions immediately following the interrup-tion point.
We used the disfluency taggedversion of the corpus for training rather thanthe parsed version because the parsed versiondoes not mark the interregnum, but we needthis information for training our repair channelmodel.
Testing was performed using data fromthe parsed version since this data is cleaner, andit enables a direct comparison with earlier work.We followed Charniak and Johnson (2001) andsplit the corpus into main training data, held-out training data and test data as follows: maintraining consisted of all sw[23]*.dps files, held-out training consisted of all sw4[5-9]*.dps filesand test consisted of all sw4[0-1]*.mrg files.We now describe how the weights on the TAGproductions described in subsection 2.2 are es-timated from this training data.
In order to es-timate these weights we need to know the TAGderivation of each sentence in the training data.In order to uniquely determine this we need thenot just the locations of each reparandum, in-terregnum and repair (which are annotated inthe corpus) but also the crossing dependenciesbetween the reparandum and repair words, asindicated in Figure 1.We obtain these by aligning the reparan-dum and repair strings of each repair usinga minimum-edit distance string aligner withthe following alignment costs: aligning identi-cal words costs 0, aligning words with the samePOS tag costs 2, an insertion or a deletion costs4, aligning words with POS tags that begin withthe same letter costs 5, and an arbitrary sub-stitution costs 7.
These costs were chosen sothat a substitution will be selected over an in-sertion followed by a deletion, and the lowercost for substitutions involving POS tags be-ginning with the same letter is a rough andeasy way of establishing a preference for align-ing words whose POS tags come from the samebroad class, e.g., it results in aligning singularand plural nouns, present and past participles,etc.
While we did not evaluate the quality ofthe alignments since they are not in themselvesthe object of this exercise, they seem to be fairlygood.From our training data we estimate a numberof conditional probability distributions.
Theseestimated probability distributions are the lin-ear interpolation of the corresponding empiricaldistributions from the main sub-corpus usingvarious subsets of conditioning variables (e.g.,bigram models are mixed with unigram models,etc.)
using Chen?s bucketing scheme Chen andGoodman (1998).
As is commonly done in lan-guage modelling, the interpolation coefficientsare determined by maximizing the likelihood ofthe held out data counts using EM.
Special carewas taken to ensure that all distributions overwords ranged over (and assigned non-zero prob-ability to) every word that occurred in the train-ing corpora; this turns out to be important asthe size of the training data for the differentdistributions varies greatly.The first distribution is defined over thewords in source sentences (i.e., that donot contain reparandums or interregnums).Pn(repair|W ) is the probability of a repair be-ginning after a word W in the source sentenceX; it is estimated from the training sentenceswith reparandums and interregnums removed.Here and in what follows, W ranges over ?
?
{$}, where ?$?
is a distinguished beginning-of-sentence marker.
For example, Pn(repair|flight)is the probability of a repair beginning after theword flight.
Note that repairs are relatively rare;in our training data Pn(repair) ?
0.02, which isa fairly strong bias against repairs.The other distributions are defined overaligned reparandum/repair strings, and are es-timated from the aligned repairs extracted fromthe training data.
In training we ignoredall overlapping repairs (i.e., cases where thereparandum of one repair is the repair of an-other).
(Naturally, in testing we have no suchfreedom.)
We analyze each repair as consistingof n aligned word pairs (we describe the inter-regnum model later).
Mi is the ith reparan-dum word and Ri is the corresponding repairword, so both of these range over ?
?
{?
}.We define M0 and R0 to be source sentenceword that preceded the repair (which is ?$?
ifthe repair begins at the beginning of a sen-tence).
We define M ?i and R?i to be the last non-?reparandum and repair words respectively, i.e.,M ?i = Mi if Mi 6= ?
and M ?i = M ?i?1 oth-erwise.
Finally, Ti, i = 1 .
.
.
n + 1, which in-dicates the type of repair that occurs at posi-tion i, ranges over {copy, subst, ins, del, nonrep},where Tn+1 = nonrep (indicating that the re-pair has ended), and for i = 1 .
.
.
n, Ti = copy ifMi = Ri, Ti = ins if Ri = ?, Ti = del if Mi = ?and Ti = subst otherwise.The distributions we estimate from thealigned repair data are the following.Pr(Ti|M ?i?1, R?i?1) is the probability of see-ing repair type Ti following the reparan-dum word M ?i?1 and repair word R?i?1; e.g.,Pr(nonrep|Boston,Denver) is the probability ofthe repair ending when Boston is the lastreparandum word and Denver is the last repairword.Pr(Mi|Ti = ins,M ?i?1, R?i) is the probabilitythat Mi is the word that is inserted into thereparandum (i.e., Ri = ?)
given that some wordis substituted, and that the preceding reparan-dum and repair words are M ?i?1 and R?i.
For ex-ample Pr(tomorrow|ins,Boston,Denver) is theprobability that the word tomorrow is insertedinto the reparandum after the words Boston andDenver, given that some word is inserted.Pr(Mi|Ti = subst,M ?i?1, R?i) is the prob-ability that Mi is the word that is substi-tuted in the reparandum for R?i, given thatsome word is substituted.
For example,Pr(Boston|subst, to,Denver) is the probabilitythat Boston is substituted for Denver, giventhat some word is substituted.Finally, we also estimated a probability dis-tribution Pi(W ) over interregnum strings as fol-lows.
Our training corpus annotates what wecall interregnum expressions, such as uh andI mean.
We estimated a simple unigram distri-bution over all of the interregnum expressionsobserved in our training corpus, and also ex-tracted the empirical distribution of the num-ber of interregnum expressions in each repair.Interregnums are generated as follows.
First,the number k of interregnum expressions is cho-sen using the empirical distribution.
Then kinterregnum expressions are independently gen-erated from the unigram distribution of inter-regnum expressions, and appended to yield theinterregnum string W .The weighted TAG that constitutes the chan-nel model is straight forward to define us-ing these conditional probability distributions.Note that the language model generates thesource string X.
Thus the weights of the TAGrules condition on the words in X, but do notgenerate them.There are three different schema defining theinitial trees of the TAG.
These correspond toanalyzing a source word as not beginning a re-pair (e.g., ?1 and ?3 in Figure 3), analyzing asource word as beginning a repair (e.g., ?2), andgenerating an interregnum (e.g., ?5).Auxiliary trees generate the paired reparan-dum/repair words of a repair.
There are five dif-ferent schema defining the auxiliary trees corre-sponding to the five different values that Ti cantake.
Note that the nonterminal Rm,r expandedby the auxiliary trees is annotated with the lastreparandum and repair words M ?i?1 and R?i?1respectively, which makes it possible to condi-tion the rule?s weight on these words.Auxiliary trees of the form (?1) gener-ate reparandum words that are copies ofthe corresponding repair words; the weighton such trees is Pr(copy|M ?i?1, R?i?1).
Treesof the form (?2) substitute a reparan-dum word for a repair word; their weightis Pr(subst|M ?i?1, R?i?1)Pr(Mi|subst,M ?i?1, R?i).Trees of the form (?3) end a repair; their weightis Pr(nonrep|,M ?i?1, R?i?1).
Auxiliary trees ofthe form (?3) end a repair; they are weightedPr(nonrep|M ?i?1, R?i?1).
Auxiliary trees of theform (?4) permit the repair word R?i?1 to bedeleted in the reparandum; the weight of sucha tree is Pr(del|M ?i?1, R?i?1).
Finally, auxiliarytrees of the form (?5) generate a reparandumword Mi is inserted; the weight of such a tree isPr(ins|M ?i?1, R?i?1)Pr(Mi|ins,M ?i?1, R?i?1).3 Detecting and repairing speechrepairsThe TAG just described is not probabilistic;informally, it does not include the probabilitycosts for generating the source words.
How-ever, it is easy to modify the TAG so it doesinclude a bigram model that does generate thesource words, since each nonterminal encodesthe preceding source word.
That is, we multi-ply the weights of each TAG production givenearlier that introduces a source word Ri byPn(Ri|Ri?1).
The resulting stochastic TAG isin fact exactly the intersection of the channelmodel TAG with a bigram language model.The standard n5 bottom-up dynamic pro-gramming parsing algorithm can be used withthis stochastic TAG.
Each different parse ofthe observed string Y with this grammar corre-sponds to a way of analyzing Y in terms of a hy-pothetical underlying sentence X and a numberof different repairs.
In our experiments belowwe extract the 20 most likely parses for each sen-tence.
Since the weighted grammar just givendoes not generate the source string X, the scoreof the parse using the weighted TAG is P(Y |X).This score multiplied by the probability P(X)of the source string using the syntactic parserbased language model, is our best estimate ofthe probability of an analysis.However, there is one additional complica-tion that makes a marked improvement tothe model?s performance.
Recall that we usethe standard bottom-up dynamic programmingTAG parsing algorithm to search for candidateparses.
This algorithm has n5 running time,where n is the length of the string.
Even thoughour sentences are often long, it is extremely un-likely that any repair will be longer than, say,12 words.
So to increase processing speed weonly compute analyses for strings of length 12or less.
For every such substring that can be an-alyzed as a repair we calculate the repair odds,i.e., the probability of generating this substringas a repair divided by the probability of gener-ating this substring via the non-repair rules, orequivalently, the odds that this substring consti-tutes a repair.
The substrings with high repairodds are likely to be repairs.This more local approach has a number ofadvantages over computing a global analysis.First, as just noted it is much more efficientto compute these partial analyses rather thanto compute global analyses of the entire sen-tence.
Second, there are rare cases in whichthe same substring functions as both repair andreparandum (i.e., the repair string is itself re-paired again).
A single global analysis wouldnot be able to capture this (since the TAG chan-nel model does not permit the same substringto be both a reparandum and a repair), butwe combine these overlapping repair substringanalyses in a post-processing operation to yieldan analysis of the whole sentence.
(We do in-sist that the reparandum and interregnum of arepair do not overlap with those of any otherrepairs in the same analysis).4 EvaluationThis section describes how we evaluate our noisymodel.
As mentioned earlier, following Char-niak and Johnson (2001) our test data consistedof all Penn III Switchboard tree-bank sw4[0-1]*.mrg files.
However, our test data differsfrom theirs in that in this test we deleted allpartial words and punctuation from the data,as this results in a more realistic test situation.Since the immediate goal of this work is toproduce a program that identifies the words of asentence that belong to the reparandum of a re-pair construction (to a first approximation thesewords can be ignored in later processing), ourevaluation focuses on the model?s performancein recovering the words in a reparandum.
Thatis, the model is used to classify each word in thesentence as belonging to a reparandum or not,and all other additional structure produced bythe model is ignored.We measure model performance using stan-dard precision p, recall r and f-score f , mea-sures.
If nc is the number of reparandum wordsthe model correctly classified, nt is the numberof true reparandum words given by the manualannotations and nm is the number of words themodel predicts to be reparandum words, thenthe precision is nc/nm, recall is nc/nt, and f is2pr/(p + r).For comparison we include the results of run-ning the word-by-word classifier described inCharniak and Johnson (2001), but where par-tial words and punctuation have been removedfrom the training and test data.
We also pro-vide results for our noisy channel model usinga bigram language model and a second trigrammodel where the twenty most likely analyses arerescored.
Finally we show the results using theparser language model.CJ01?
Bigram Trigram ParserPrecision 0.951 0.776 0.774 0.820Recall 0.631 0.736 0.763 0.778F-score 0.759 0.756 0.768 0.797The noisy channel model using a bigram lan-guage model does a slightly worse job at identi-fying reparandum and interregnum words thanthe classifier proposed in Charniak and Johnson(2001).
Replacing the bigram language modelwith a trigram model helps slightly, and parser-based language model results in a significantperformance improvement over all of the oth-ers.5 Conclusion and further workThis paper has proposed a novel noisy chan-nel model of speech repairs and has used it toidentify reparandum words.
One of the advan-tages of probabilistic models is that they can beintegrated with other probabilistic models in aprincipled way, and it would be interesting toinvestigate how to integrate this kind of modelof speech repairs with probabilistic speech rec-ognizers.There are other kinds of joint models ofreparandum and repair that may produce a bet-ter reparandum detection system.
We haveexperimented with versions of the models de-scribed above based on POS bi-tag dependen-cies rather than word bigram dependencies, butwith results very close to those presented here.Still, more sophisticated models may yield bet-ter performance.It would also be interesting to combine thisprobabilistic model of speech repairs with theword classifier approach of Charniak and John-son (2001).
That approach may do so well be-cause many speech repairs are very short, in-volving only one or two words Shriberg andStolcke (1998), so the reparandum, interregnumand repair are all contained in the surround-ing word window used as features by the classi-fier.
On the other hand, the probabilistic modelof repairs explored here seems to be most suc-cessful in identifying long repairs in which thereparandum and repair are similar enough to beunlikely to have been generated independently.Since the two approaches seem to have differentstrengths, a combined model may outperformboth of them.ReferencesEugene Charniak and Mark Johnson.
2001.Edit detection and parsing for transcribedspeech.
In Proceedings of the 2nd Meetingof the North American Chapter of the Asso-ciation for Computational Linguistics, pages118?126.
The Association for ComputationalLinguistics.Eugene Charniak.
2001.
Immediate-head pars-ing for language models.
In Proceedings of the39th Annual Meeting of the Association forComputational Linguistics.
The Associationfor Computational Linguistics.Stanley F. Chen and Joshua Goodman.
1998.An empirical study of smoothing techniquesfor language modeling.
Technical Report TR-10-98, Center for Research in ComputingTechnology, Harvard University.Peter A. Heeman and James F. Allen.
1999.Speech repairs, intonational phrases, and dis-course markers: Modeling speaker?s utter-ances in spoken dialogue.
ComputationalLinguistics, 25(4):527?571.Stuart M. Shieber and Yves Schabes.
1990.Synchronous tree-adjoining grammars.
InProceedings of the 13th International Confer-ence on Computational Linguistics (COLING1990), pages 253?258.Stuart M. Shieber.
1985.
Evidence against theContext-Freeness of natural language.
Lin-guistics and Philosophy, 8(3):333?344.Elizabeth Shriberg and Andreas Stolcke.
1998.How far do speakers back up in repairs?
aquantitative model.
In Proceedings of the In-ternational Conference on Spoken LanguageProcessing, volume 5, pages 2183?2186, Syd-ney, Australia.Elizabeth Shriberg.
1994.
Preliminaries to aTheory of Speech Disfluencies.
Ph.D. thesis,University of California, Berkeley.
