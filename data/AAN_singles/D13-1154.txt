Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1476?1480,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsUsing crowdsourcing to get representations based on regular expressionsAnders S?gaard and Hector Martinez and Jakob Elming and Anders JohannsenCenter for Language TechnologyUniversity of CopenhagenDK-2300 Copenhagen S{soegaard|alonso|zmk867|ajohannsen}@hum.ku.dkAbstractOften the bottleneck in document classifica-tion is finding good representations that zoomin on the most important aspects of the doc-uments.
Most research uses n-gram repre-sentations, but relevant features often occurdiscontinuously, e.g., not.
.
.
good in sentimentanalysis.
In this paper we present experi-ments getting experts to provide regular ex-pressions, as well as crowdsourced annota-tion tasks from which regular expressions canbe derived.
Somewhat surprisingly, it turnsout that these crowdsourced feature combina-tions outperform automatic feature combina-tion methods, as well as expert features, by avery large margin and reduce error by 24-41%over n-gram representations.1 IntroductionFinding good representations of classification prob-lems is often glossed over in the literature.
Sev-eral authors have emphasized the need to pay moreattention to finding such representations (Wagstaff,2012; Domingos, 2012), but in document classifica-tion most research still uses n-gram representations.This paper considers two document classificationproblems where such representations seem inade-quate.
The problems are answer scoring (Bursteinet al 1998), on data from stackoverflow.com, andmulti-attribute sentiment analysis (McAuley et al2012).
We argue that in order to adequately repre-sent such problems we need discontinuous features,i.e., regular expressions.The problem with using regular expressions asfeatures is of course that even with a finite vocab-ulary we can generate infinitely many regular ex-pressions that match our documents.
We suggest touse expert knowledge or crowdsourcing in the loop.In particular we present experiments where standardrepresentations are augmented with features from afew hours of manual work, by machine learning ex-perts or by turkers.Somewhat surprisingly, we find that features de-rived from crowdsourced annotation tasks lead to thebest results across the three datasets.
While crowd-sourcing of annotation tasks has become increasingpopular in NLP, this is, to the best of our knowledge,the first attempt to crowdsource the problem of find-ing good representations.1.1 Related workMusat et al(2012) design a collaborative two-playergame for sentiment annotation and collecting a sen-timent lexicon.
One player guesses the sentiment ofa text and picks a word from it that is representativeof its sentiment.
The other player also provides aguess observing only this word.
If the two guessesagree, both players get a point.
The idea of gam-ifying the problem of finding good representationsgoes beyond crowdsourcing, but is not consideredhere.
Boyd-Graber et al(2012) crowdsource thefeature weighting problem, but using standard rep-resentations.
The work most similar to ours is prob-ably Tamuz et al(2011), who learn a ?crowd kernel?by asking annotators to rate examples by similarity,providing an embedding that promotes feature com-binations deemed relative when measuring similar-ity.1476BoW Exp AMTn P (1) m ?x m ?x m ?xSTACKOVERFLOW 97,519 0.5013 30,716 0.00131 1,156 0.1380 172,691 0.00331TASTE 152,390 0.5003 38,227 0.00095 666 0.10631 114,588 0.00285APPEARANCE 152,331 0.5009 37,901 0.00097 650 0.14629 102,734 0.00289Table 1: Characteristics of the n?m data sets2 ExperimentsData The three datasets used in our experi-ments come from two sources, namely stackover-flow.com and ratebeer.com.
The two beer reviewdatasets (TASTE and APPEARANCE) are describedin McAuley et al(2012) and available for down-load.1 Each input example is an unstructured reviewtext, and the associated label is the score assigned totaste or appearance by the reviewer.
We randomlysample about 152k data points, as well as 500 exam-ples for experiments with experts and turks.We extracted the STACKOVERFLOW dataset froma publicly available data dump,2, and we briefly de-scribe our sampling process here.
We select pairs ofanswers, where one is ranked higher than the otherby stackoverflow.com users.
Obviously the answerssubmitted first have a better chance of being rankedhighly, so we also require that the highest rankedanswer was submitted last.
From this set of answerpairs, we randomly sample 97,519 pairs, as well as500 examples for our experiments with experts andturks.Our experiments are classification experimentsusing the same learning algorithm in all experi-ments, namely L1-regularized logistic regression.We don?t set any parameters The only differencesbetween our systems are in the feature sets.
Resultsare from 5-fold cross-validation.
The four featuresets are described below: BoW, HI, Exp and AMT.For motivating using regular expressions, con-sider the following sentence from a review of JohnHarvard?s Grand Cru:(1) Could have been more flavorful.The only word carrying direct sentiment in thissentence is flavorful, which is positive, but the sen-tence is a negative evaluation of the Grand Cru?s1http://snap.stanford.edu/data/web-RateBeer.html2http://www.clearbits.net/torrents/2076-aug-2012taste.
The trigram been more flavorful seems neg-ative at first, but in the context of negation or in acomparative, it can become positive again.
How-ever, note that this trigram may occur discontinu-ously, e.g., in been less watery and more flavorful.In order to match such occurrences, we need simpleregular expressions, e.g.,:been.*more.
*flavorfulThis is exactly the kind of regular expressions weasked experts to submit, and that we derived fromthe crowdsourced annotation tasks.
Note that thesentence says nothing about the beer?s appearance,so this feature is only relevant in TASTE, not inAPPEARANCE.BoW and BoW+HI Our most simple baseline ap-proach is a bag-of-words model of unigram features(BoW).
We lower-case our data, but leave in stopwords.
We also introduce a semantically enrichedunigram model (BoW)+HI, where in addition torepresenting what words occur in a text, we alsorepresent what Harvard Inquirer (HI)3 word classesoccur in it.
The HI classes are used to generatefeatures from the crowdsourced annotation tasks,so the semantically enriched unigram model is animportant baseline in our experiments below.BoW+Exp In order to collect regular expressionsfrom experts, we set up a web interface for query-ing held-out portions of the datasets with regular ex-pressions that reports how occurrences of the sub-mitted regular expressions correlate with class.
Weused the Python re syntax for regular expressionsafter augmenting word forms with POS and seman-tic classes from the HI.
Few of the experts made useof the POS tags, but many regular expressions in-cluded references to HI classes.3http://www.wjh.harvard.edu/ inquirer/homecat.htm1477Regular expressions submitted by participantswere visible to other participants during the exper-iment, and participants were allowed to work to-gether.
Participants had 15 minutes to familiarizethemselves with the syntax used in the experiments.Each query was executed in 2-30 seconds.Seven researchers and graduate students spentfive effective hours querying the datasets withregular expressions.
In particular, they spent threehours on the Stack Exchange dataset, and one houron each of the two RateBeer datasets.
One had toleave an hour early.
So, in total, we spent 20 personhours on Stack Exchange, and seven person hourson each of the RateBeer datasets.
In the five hours,we collected 1,156 regular expressions for theSTACKOVERFLOW dataset, and about 650 regularexpressions for each of the two RateBeer datasets.Exp refers to these sets of regular expressions.
Inour experiments below we concatenate these withthe BoW features to form BoW+Exp.BoW+AMT For each dataset, we also had 500 held-out examples annotated by three turkers each, usingAmazon Mechanical Turk,4 obtaining 1,500 HITsfor each dataset.
The annotators were presented witheach text, a review or an answer, twice: once as run-ning text, once word-by-word with bullets to tick offwords.
The annotators were instructed to tick offwords or phrases that they found predictive of thetext?s sentiment or answer quality.
They were not in-formed about the class of the text.
We chose this an-notation task, because it is relatively easy for annota-tors to mark spans of text with a particular attribute.This set-up has been used in other applications, in-cluding NER (Finin et al 2010) and error detection(Dahlmeier et al 2013).
The annotators were con-strained to tick off at least three words, includingone closed class item (closed class items were col-ored differently).
Finally, we only used annotatorswith a track record of providing high-quality anno-tations in previous tasks.
It was clear from the aver-age time spent by annotators that annotating STACK-OVERFLOW was harder than annotating the Rate-beer datasets.
The average time spent on a Rate-beer HIT was 44s, while for STACKOVERFLOW itwas 3m:8s.
The mean number of words ticked off4www.mturk.comBoW HI Exp AMTSTACKOVERF 0.655 0.654 0.683 0.739TASTE 0.798 0.797 0.798 0.867APPEARANCE 0.758 0.760 0.761 0.859Table 2: Results using all featureswas between 5.6 and 7, with more words ticked offin STACKOVERFLOW.
The maximum number ofwords ticked off by an annotator was 41.
We spent$292.5 on the annotations, including a trial round.This was supposed to match, roughly, the cost of theexperts consulted for BoW+Exp.The features generated from the annotations wereconstructed as follows: We use a sliding window ofsize 3 to extract trigrams over the possibly discon-tinuous words ticked off by the annotators.
Thesetrigrams were converted into regular expressions byplacing Kleene stars between the words.
This givesus a manually selected subset of skip trigrams.
Foreach skip trigram, we add copies with one or morewords replaced by one of their HI classes.Feature combinations This subsection introducessome harder baselines for our experiments, consid-ered in Experiment #2.
The simplest possible wayof combining unigram features is by considering n-gram models.
An n-gram extracts features from asliding window (of size n) over the text.
We call thismodel BoW(N = n).
Our BoW(N = 1) modeltakes word forms as features, and there are obvi-ously more advanced ways of automatically combin-ing such features.Kernel representations We experimented with ap-plying an approximate feature map for the addi-tive ?2-kernel.
We used two sample steps, result-ing in 4N + 1 features.
See Vedaldi and Zimmer-man (2011) for details.Deep features We also ran denoising autoen-coders (Pascal et al 2008), previously appliedto a wide range of NLP tasks (Ranganath et al2009; Socher et al 2011; Chen et al 2012), with2N nodes in the middle layer to obtain a deeprepresentation of our datasets from ?2-BoW input.The network was trained for 15 epochs.
We set thedrop-out rate to 0.0 and 0.3.Summary of feature sets The feature sets ?
BoW,1478102103N6062646668707274AccuracyStackOverflow?2?BoW?2?BoW+HI?2?BoW+Exp?2?BoW+AMT102103N7075808590AccuracyTaste?2?BoW?2?BoW+HI?2?BoW+Exp?2?BoW+AMT102103N7075808590AccuracyAppearance?2?BoW?2?BoW+HI?2?BoW+Exp?2?BoW+AMTFigure 1: Results selecting N features using ?2 (left to right): STACKOVERFLOW, TASTE, and APPEARANCE.
Thex-axis is logarithmic scale.102103N6062646668707274Accuracy?2?BoW(N=1)?2?BoW(N=2)?2?kernel?2?dauto?2?BoW+AMT102103N7075808590Accuracy?2?BoW(N=1)?2?BoW(N=2)?2?kernel?2?dauto?2?BoW+AMT102103N7072747678808284Accuracy?2?BoW(N=1)?2?BoW(N=2)?2?kernel?2?dauto?2?BoW+AMTFigure 2: Results using different feature combination techniques (left to right): STACKOVERFLOW, TASTE, andAPPEARANCE.
The x-axis is logarithmic scale.Exp and AMT ?
are very different.
Their character-istics are presented in Table 1.
P (1) is the class dis-tribution, e.g., the prior probability of positive class.n is the number of data points, m the number offeatures.
Finally, ?x is the average density of datapoints.
One observation is of course that the expertfeature set Exp is much smaller than BoW and AMT,but note also that the expert features fire about 150times more often on average than the BoW features.HI is only a small set of additional features.3 ResultsExperiment #1: BoW vs. Exp and AMT We presentresults using all features, as well as results obtainedafter selecting k features as ranked by a simple ?2test.
The results using all collected features are pre-sented in Table 2.
The error reduction on STACK-OVERFLOW when adding crowdsourced features toour baseline model (BoW+AMT), is 24.3%.
OnTASTE, it is 34.2%.
On APPEARANCE, it is 41.0%.The BoW+AMT feature set is bigger than those ofthe other models.
We therefore report results usingthe top-k features as ranked by a simple ?2 test.The result curves are presented in the three plots inFig.
1.
With +500 features, BoW+AMT outperformsthe other models by a large margin.Experiment #2: AMT vs. more baselines TheBoW baseline uses a standard representation that,while widely used, is usually thought of as a weakbaseline.
BoW+HIT did not provide a stronger base-line.
We also show that bigram features, kernel-based decomposition and deep features do not pro-vide much stronger baselines either.
The resultcurves are presented in the three plots in Fig.
2.BoW+AMT is still significantly better than all othermodels with +500 features.
Since autoencodersare consistently worse than denoising autoencoders(drop-out 0.3), we only plot denoising autoencoders.4 ConclusionWe presented a new method for deriving featurerepresentations from crowdsourced annotation tasksand showed how it leads to 24%-41% error reduc-tions on answer scoring and multi-aspect sentimentanalysis problems.
We saw no significant improve-ments using features contributed by experts, kernelrepresentations or learned deep representations.1479ReferencesJordan Boyd-Graber, Brianna Satinoff, He He, and HalDaume.
2012.
Besting the quiz master: Crowdsourc-ing incremental classification games.
In NAACL.Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu,Martin Chodorow, Lisa Braden-Harder, and Mary DeeHarris.
1998.
Automated scoring using a hybrid fea-ture identification technique.
In ACL.Minmin Chen, Zhixiang Xu, Kilian Weinberger, and FeiSha.
2012.
Marginalized denoising autoencoders fordomain adaptation.
In ICML.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learnerEnglish.
In Workshop on Innovative Use of NLP forBuilding Educational Applications, NAACL.Pedro Domingos.
2012.
A few useful things to knowabout machine learning.
In CACM.Tim Finin, Will Murnane, Anand Karandikar, NicholasKeller, Justin Martineau, and Mark Dredze.
2010.Annotating named entities in Twitter data with crowd-sourcing.
In NAACL Workshop on Creating Speechand Language Data with Amazon?s Mechanical Turk.Julian McAuley, Jure Leskovec, and Dan Jurafsky.
2012.Learning attitudes and attributes from multi-aspect re-views.
In ICDM.Claudiu-Christian Musat, Alireza Ghasemi, and Boi Falt-ings.
2012.
Sentiment analysis using a novel humancomputation game.
In Workshop on the People?s WebMeets NLP, ACL.Vincent Pascal, Hugo Larochelle, Yoshua Bengio, andPierre-Antoine Manzagol.
2008.
Extracting and com-posing robust features with denoising autoencoders.
InICML.Rajesh Ranganath, Dan Jurafsky, and Dan McFarland.2009.
It?s not you, it?s me: detecting flirting and itsmisperception in speed-dates.
In NAACL.Richard Socher, Eric Huan, Jeffrey Pennington, AndrewNg, and Christopher Manning.
2011.
Dynamic pool-ing and unfolding recursive autoencoders for para-phrase detection.
In NIPS.Omer Tamuz, Ce Liu, Serge Belongie, Ohad Shamir, andAdam Tauman Kalai.
2011.
Adaptively learning thecrowd kernel.
In ICML.Andrea Vedaldi and Andrew Zisserman.
2011.
Efficientadditive kernels via explicit feature maps.
In CVPR.Kiri Wagstaff.
2012.
Machine learning that matters.
InICML.1480
