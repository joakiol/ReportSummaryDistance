Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 592?600,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsError Mining on Dependency TreesClaire GardentCNRS, LORIA, UMR 7503Vandoeuvre-le`s-Nancy, F-54500, Franceclaire.gardent@loria.frShashi NarayanUniversite?
de Lorraine, LORIA, UMR 7503Villers-le`s-Nancy, F-54600, Franceshashi.narayan@loria.frAbstractIn recent years, error mining approaches weredeveloped to help identify the most likelysources of parsing failures in parsing sys-tems using handcrafted grammars and lexi-cons.
However the techniques they use to enu-merate and count n-grams builds on the se-quential nature of a text corpus and do not eas-ily extend to structured data.
In this paper, wepropose an algorithm for mining trees and ap-ply it to detect the most likely sources of gen-eration failure.
We show that this tree miningalgorithm permits identifying not only errorsin the generation system (grammar, lexicon)but also mismatches between the structurescontained in the input and the input structuresexpected by our generator as well as a few id-iosyncrasies/error in the input data.1 IntroductionIn recent years, error mining techniques have beendeveloped to help identify the most likely sourcesof parsing failure (van Noord, 2004; Sagot and de laClergerie, 2006; de Kok et al, 2009).
First, the inputdata (text) is separated into two subcorpora, a corpusof sentences that could be parsed (PASS) and a cor-pus of sentences that failed to be parsed (FAIL).
Foreach n-gram of words (and/or part of speech tag) oc-curring in the corpus to be parsed, a suspicion rate isthen computed which, in essence, captures the like-lihood that this n-gram causes parsing to fail.These error mining techniques have been appliedwith good results on parsing output and shown tohelp improve the large scale symbolic grammars andlexicons used by the parser.
However the techniquesthey use (e.g., suffix arrays) to enumerate and countn-grams builds on the sequential nature of a text cor-pus and cannot easily extend to structured data.There are some NLP applications though wherethe processed data is structured data such as treesor graphs and which would benefit from error min-ing.
For instance, when generating sentences fromdependency trees, as was proposed recently in theGeneration Challenge Surface Realisation Task (SRTask, (Belz et al, 2011)), it would be useful to beable to apply error mining on the input trees to findthe most likely causes of generation failure.In this paper, we address this issue and proposean approach that supports error mining on trees.
Weadapt an existing algorithm for tree mining which wethen use to mine the Generation Challenge depen-dency trees and identify the most likely causes ofgeneration failure.
We show in particular, that thistree mining algorithm permits identifying not onlyerrors in the grammar and the lexicon used by gener-ation but also a few idiosyncrasies/error in the inputdata as well as mismatches between the structurescontained in the SR input and the input structuresexpected by our generator.
The latter is an impor-tant point since, for symbolic approaches, a majorhurdle to participation in the SR challenge is knownto be precisely these mismatches i.e., the fact thatthe input provided by the SR task fails to match theinput expected by the symbolic generation systems(Belz et al, 2011).The paper is structured as follows.
Section 2presents the HybridTreeMiner algorithm, a completeand computationally efficient algorithm developed592ABCDBCABDCBCABCBCDABCBDCFigure 1: Four unordered labelled trees.
The right-most is in Breadth-First Canonical Formby (Chi et al, 2004) for discovering frequently oc-curring subtrees in a database of labelled unorderedtrees.
Section 3 shows how to adapt this algorithmto mine the SR dependency trees for subtrees withhigh suspicion rate.
Section 4 presents an experi-ment we made using the resulting tree mining algo-rithm on SR dependency trees and summarises theresults.
Section 5 discusses related work.
Section 6concludes.2 Mining TreesMining for frequent subtrees is an important prob-lem that has many applications such as XML datamining, web usage analysis and RNA classification.The HybridTreeMiner (HTM) algorithm presentedin (Chi et al, 2004) provides a complete and com-putationally efficient method for discovering fre-quently occurring subtrees in a database of labelledunordered trees and counting them.
We now sketchthe intuition underlying this algorithm1.
In the nextsection, we will show how to modify this algorithmto mine for errors in dependency trees.Given a set of trees T , the HybridTreeMiner al-gorithm proceeds in two steps.
First, the unorderedlabelled trees contained in T are converted to acanonical form called BFCF (Breadth-First Canoni-cal Form).
In that way, distinct instantiations of thesame unordered trees have a unique representation.Second, the subtrees of the BFCF trees are enumer-ated in increasing size order using two tree opera-tions called join and extension and their support (thenumber of trees in the database that contains eachsubtree) is recorded.
In effect, the algorithm buildsan enumeration tree whose nodes are the possiblesubtrees of T and such that, at depth d of this enu-meration tree, all possible frequent subtrees consist-ing of d nodes are listed.1For a more complete definition see (Chi et al, 2004).The BFCF canonical form of an unordered treeis an ordered tree t such that t has the smallestbreath-first canonical string (BFCS) encoding ac-cording to lexicographic order.
The BFCS encod-ing of a tree is obtained by breadth-first traver-sal of the tree, recording the string labelling eachnode, ?$?
to separate siblings with distinct parentsand ?#?
to represent the end of the tree2.
For in-stance, the BFCS encodings of the four trees shownin Figure 1 are ?A$BB$C$DC#?, ?A$BB$C$CD#?,?A$BB$DC$C#?
and ?A$BB$CD$C#?
respectively.Hence, the rightmost tree is the BFCF of all fourtrees.The join and extension operations used to itera-tively enumerate subtrees are depicted in Figure 2and can be defined as follows.?
A leg is a leaf of maximal depth.?
Extension: Given a tree t of height ht and anode n, extending t with n yields a tree t?
(achild of t in the enumeration tree) with heightht?
such that n is a child of one of t?s legs andht?
is ht + 1.?
Join: Given two trees t1 and t2 of same heighth differing only in their rightmost leg and suchthat t1 sorts lower than t2, joining t1 and t2yields a tree t?
(a child of t1 in the enumerationtree) of same height h by adding the rightmostleg of t2 to t1 at level h?
1.ACBD + E ?ExtensionACBDEACBD +ACEB?JoinACEBDFigure 2: Join and Extension OperationsTo support counting, the algorithm additionallyrecords for each subtree a list (called occurrence list)2Assuming ?#?
sorts greater than ?$?
and both sort greaterthan any other alphabets in node labels.593of all trees in which this subtree occurs and of its po-sition in the tree (represented by the list of tree nodesmapped onto by the subtree).
Thus for a given sub-tree t, the support of t is the number of elementsin that list.
Occurrence lists are also used to checkthat trees that are combined occur in the data.
Forthe join operation, the subtrees being combined mustoccur in the same tree at the same position (the inter-section of their occurrence lists must be non emptyand the tree nodes must match except the last node).For the extension operation, the extension of a treet is licensed for any given occurrence in the occur-rence list only if the planned extension maps ontothe tree identified by the occurrence.3 Mining Dependency TreesWe develop an algorithm (called ErrorTreeMiner,ETM) which adapts the HybridTreeMiner algorithmto mine sources of generation errors in the Gener-ation Challenge SR shallow input data.
The mainmodification is that instead of simply counting trees,we want to compute their suspicion rate.
Following(de Kok et al, 2009), we take the suspicion rate of agiven subtree t to be the proportion of cases where toccurs in an input tree for which generation fails:Sus(t) =count(t|FAIL)count(t)where count(t) is the number of occurrences oft in all input trees and count(t|FAIL) is the numberof occurrences of t in input trees for which no outputwas produced.Since we work with subtrees of arbitrary length,we also need to check whether constructing a longersubtree is useful that is, whether its suspicion rateis equal or higher than the suspicion rate of any ofthe subtrees it contains.
In that way, we avoid com-puting all subtrees (thus saving time and space).
Asnoted in (de Kok et al, 2009), this also permits by-passing suspicion sharing that is the fact that, if n2is the cause of a generation failure, and if n2 is con-tained in larger trees n3 and n4, then all three treeswill have high suspicion rate making it difficult toidentify the actual source of failure namely n2.
Be-cause we use a milder condition however (we acceptbigger trees whose suspicion rate is equal to the sus-picion rate of any of their subtrees), some amount ofAlgorithm 1 ErrorTreeMiner(D,minsup)Note: D consists of Dfail and DpassF1 ?
{Frequent 1-trees}F2 ?
?for i?
1, ..., |F1| dofor j ?
1, ..., |F1| doq ?
fi plus legfjif Noord-Validation(q,minsup) thenF2 ?
F2 ?
qend ifend forend forF ?
F1 ?
F2PUSH: sort(F2)?
LQueueEnum-Grow(LQueue, F,minsup)return FAlgorithm 2 Enum-Grow(LQueue, F,minsup)while LQueue 6= empty doPOP: pop(LQueue)?
Cfor i?
1, ..., |C| doThe join operationJ ?
?for j ?
i, ..., |C| dop?
join(ci, cj)if Noord-Validation(p,minsup) thenJ ?
J ?
pend ifend forF ?
F ?
JPUSH: sort(J)?
LQueueThe extension operationE ?
?for possible leg lm of ci dofor possible new leg ln(?
F1) doq ?
extend ci with ln at position lmif Noord-Validation(q,minsup) thenE ?
E ?
qend ifend forend forF ?
F ?
EPUSH: sort(E)?
LQueueend forend while594Algorithm 3 Noord-Validation(tn,minsup)Note: tn, tree with n nodesif Sup(tn) ?
minsup thenif Sus(tn) ?
Sus(tn?1),?tn?1 in tn thenreturn trueend ifend ifreturn falsesuspicion sharing remains.
As we shall see in Sec-tion 4.3.2, relaxing this check though allows us toextract frequent larger tree patterns and thereby geta more precise picture of the context in which highlysuspicious items occur.Finally, we only keep subtrees whose support isabove a given threshold where the support Sup(t)of a tree t is defined as the ratio between the numberof times it occurs in an input for which generationfails and the total number of generation failures:Sup(t) =count(t|FAIL)count(FAIL)The modified algorithm we use for error mining isgiven in Algorithm 1, 2 and 3.
It can be summarisedas follows.First, dependency trees are converted to Breadth-First Canonical Form whereby lexicographic ordercan apply to the word forms labelling tree nodes, totheir part of speech, to their dependency relation orto any combination thereof3.Next, the algorithm iteratively enumerates thesubtrees occurring in the input data in increasingsize order and associating each subtree t with twooccurrence lists namely, the list of input trees inwhich t occurs and for which generation was suc-cessful (PASS(t)); and the list of input trees in whicht occurs and for which generation failed (FAIL(t)).This process is initiated by building trees of sizeone (i.e., one-node tree) and extending them to treesof size two.
It is then continued by extending thetrees using the join and extension operations.
Asexplained in Section 2 above, join and extensiononly apply provided the resulting trees occur in thedata (this is checked by looking up occurrence lists).3For convenience, the dependency relation labelling theedges of dependency trees is brought down to the daughter nodeof the edge.Each time an n-node tree tn, is built, it is checkedthat (i) its support is above the set threshold and (ii)its suspicion rate is higher than or equal to the sus-picion rate of all (n?
1)-node subtrees of tn.In sum, the ETM algorithm differs from the HTMalgorithm in two main ways.
First, while HTM ex-plores the enumeration tree depth-first, ETM pro-ceeds breadth-first to ensure that the suspicion rateof (n-1)-node trees is always available when check-ing whether an n-node tree should be introduced.Second, while the HTM algorithm uses support toprune the search space (only trees with a minimumsupport bigger than the set threshold are stored), theETM algorithm drastically prunes the search spaceby additionally checking that the suspicion rate ofall subtrees contained in a new tree t is smaller orequal to the suspicion rate of t .
As a result, whileETM looses the space advantage of HTM by a smallmargin4, it benefits from a much stronger pruning ofthe search space than HTM through suspicion ratechecking.
In practice, the ETM algorithm allows usto process e.g., all NP chunks of size 4 and 6 presentin the SR data (roughly 60 000 trees) in roughly 20minutes on a PC.4 Experiment and ResultsUsing the input data provided by the GenerationChallenge SR Task, we applied the error mining al-gorithm described in the preceding Section to debugand extend a symbolic surface realiser developed forthis task.4.1 Input Data and Surface Realisation SystemThe shallow input data provided by the SR Taskwas obtained from the Penn Treebank using theLTH Constituent-to-Dependency Conversion Toolfor Penn-style Treebanks (Pennconverter, (Johans-son and Nugues, 2007)).
It consists of a setof unordered labelled syntactic dependency treeswhose nodes are labelled with word forms, part ofspeech categories, partial morphosyntactic informa-tion such as tense and number and, in some cases, asense tag identifier.
The edges are labelled with thesyntactic labels provided by the Pennconverter.
Allwords (including punctuation) of the original sen-4ETM needs to store all (n-1)-node trees in queues beforeproducing n-node trees.595tence are represented by a node in the tree and thealignment between nodes and word forms was pro-vided by the organisers.The surface realiser used is a system based ona Feature-Based Lexicalised Tree Adjoining Gram-mar (FB-LTAG) for English extended with a unifica-tion based compositional semantics.
Both the gram-mars and the lexicon were developed in view of theGeneration Challenge and the data provided by thischallenge was used as a means to debug and extendthe system.
Unknown words are assigned a defaultTAG family/tree based on the part of speech theyare associated with in the SR data.
The surface real-isation algorithm extends the algorithm proposed in(Gardent and Perez-Beltrachini, 2010) and adapts itto work on the SR dependency input rather than onflat semantic representations.4.2 Experimental SetupTo facilitate interpretation, we first chunked the in-put data in NPs, PPs and Clauses and performed er-ror mining on the resulting sets of data.
The chunk-ing was performed by retrieving from the Penn Tree-bank (PTB), for each phrase type, the yields of theconstituents of that type and by using the alignmentbetween words and dependency tree nodes providedby the organisers of the SR Task.
For instance, giventhe sentence ?The most troublesome report may bethe August merchandise trade deficit due out tomor-row?, the NPs ?The most troublesome report?
and?the August merchandise trade deficit due out to-morrow?
will be extracted from the PTB and thecorresponding dependency structures from the SRTask data.Using this chunked data, we then ran the genera-tor on the corresponding SR Task dependency treesand stored separately, the input dependency trees forwhich generation succeeded and the input depen-dency trees for which generation failed.
Using infor-mation provided by the generator, we then removedfrom the failed data, those cases where generationfailed either because a word was missing in the lex-icon or because a TAG tree/family was missing inthe grammar but required by the lexicon and the in-put data.
These cases can easily be detected usingthe generation system and thus do not need to behandled by error mining.Finally, we performed error mining on the datausing different minimal support thresholds, differ-ent display modes (sorted first by size and second bysuspicion rate vs sorted by suspicion rate) and differ-ent labels (part of speech, words and part of speech,dependency, dependency and part of speech).4.3 ResultsOne feature of our approach is that it permits min-ing the data for tree patterns of arbitrary size us-ing different types of labelling information (POStags, dependencies, word forms and any combina-tion thereof).
In what follows, we focus on the NPchunk data and illustrate by means of examples howthese features can be exploited to extract comple-mentary debugging information from the data.4.3.1 Mining on single labels (word form, POStag or dependency)Mining on a single label permits (i) assessing therelative impact of each category in a given label cat-egory and (ii) identifying different sources of errorsdepending on the type of label considered (POS tag,dependency or word form).Mining on POS tags Table 1 illustrates how min-ing on a single label (in this case, POS tags) givesa good overview of how the different categories inthat label type impact generation: two POS tags(POS and CC) have a suspicion rate of 0.99 indicat-ing that these categories always lead generation tofail.
Other POS tag with much lower suspicion rateindicate that there are unresolved issues with, in de-creasing order of suspicion rate, cardinal numbers(CD), proper names (NNP), nouns (NN), prepositions(IN) and determiners (DT).The highest ranking category (POS5) points toa mismatch between the representation of geni-tive NPs (e.g., John?s father) in the SR Task dataand in the grammar.
While our generator ex-pects the representation of ?John?s father?
to be FA-THER(?S?
(JOHN)), the structure provided by the SRTask is FATHER(JOHN(?S?)).
Hence whenever apossessive appears in the input data, generation fails.This is in line with (Rajkumar et al, 2011)?s findingthat the logical forms expected by their system forpossessives differed from the shared task inputs.5In the Penn Treebank, the POS tag is the category assignedto possessive ?s.596POS Sus Sup Fail PassPOS 0.99 0.38 3237 1CC 0.99 0.21 1774 9CD 0.39 0.16 1419 2148NNP 0.35 0.32 2749 5014NN 0.30 0.81 6798 15663IN 0.30 0.16 1355 3128DT 0.09 0.12 1079 10254Table 1: Error Mining on POS tags with frequencycutoff 0.1 and displaying only trees of size 1 sortedby decreasing suspicion rate (Sus)The second highest ranked category is CC for co-ordinations.
In this case, error mining unveils abug in the grammar trees associated with conjunc-tion which made all sentences containing a conjunc-tion fail.
Because the grammar is compiled out ofa strongly factorised description, errors in this de-scription can propagate to a large number of treesin the grammar.
It turned out that an error occurredin a class inherited by all conjunction trees therebyblocking the generation of any sentence requiringthe use of a conjunction.Next but with a much lower suspicion rate comecardinal numbers (CD), proper names (NNP), nouns(NN), prepositions (IN) and determiners (DT).
Wewill see below how the richer information providedby mining for larger tree patterns with mixed la-belling information permits identifying the contextsin which these POS tags lead to generation failure.Mining on Word Forms Because we removefrom the failure set al cases of errors due to a miss-ing word form in the lexicon, a high suspicion ratefor a word form usually indicates a missing or incor-rect lexical entry: the word is present in the lexiconbut associated with either the wrong POS tag and/orthe wrong TAG tree/family.
To capture such cases,we therefore mine not on word forms alone but onpairs of word forms and POS tag.
In this way, wefound for instance, that cardinal numbers inducedmany generation failures whenever they were cate-gorised as determiners but not as nouns in our lexi-con.
As we will see below, larger tree patterns helpidentify the specific contexts inducing such failures.One interesting case stood out which pointed toidiosyncrasies in the input data: The word form $(Sus=1) was assigned the POS tag $ in the inputdata, a POS tag which is unknown to our system andnot documented in the SR Task guidelines.
The SRguidelines specify that the Penn Treebank tagset isused modulo the modifications which are explicitlylisted.
However for the $ symbol, the Penn treebankused SYM as a POS tag and the SR Task $, but themodification is not listed.
Similarly, while in thePenn treebank, punctuations are assigned the SYMPOS tag, in the SR data ?,?
is used for the comma,?(?
for an opening bracket and so on.Mining on Dependencies When mining on de-pendencies, suspects can point to syntactic construc-tions (rather than words or word categories) that arenot easily spotted when mining on words or partsof speech.
Thus, while problems with coordinationcould easily be spotted through a high suspicion ratefor the CC POS tag, some constructions are linkedneither to a specific POS tag nor to a specific word.This is the case, for instance, for apposition whicha suspicion rate of 0.19 (286F/1148P) identified asproblematic.
Similarly, a high suspicion rate (0.54,183F/155P) on the TMP dependency indicates thattemporal modifiers are not correctly handled eitherbecause of missing or erroneous information in thegrammar or because of a mismatch between the in-put data and the fomat expected by the surface re-aliser.Interestingly, the underspecified dependency rela-tion DEP which is typically used in cases for whichno obvious syntactic dependency comes to mindshows a suspicion rate of 0.61 (595F/371P).4.3.2 Mining on trees of arbitrary size andcomplex labelling patternsWhile error mining with tree patterns of size onepermits ranking and qualifying the various sourcesof errors, larger patterns often provide more detailedcontextual information about these errors.
For in-stance, Table 1 shows that the CD POS tag has asuspicion rate of 0.39 (1419F/2148P).
The largertree patterns identified below permits a more specificcharacterization of the context in which this POS tagco-occurs with generation failure:TP1 CD(IN,RBR) more than 10TP2 IN(CD) of 1991TP3 NNP(CD) November 1TP4 CD(NNP(CD)) Nov. 1, 1997597Two patterns clearly emerge: a pattern where car-dinal numbers are parts of a date (tree patterns TP2-TP4) and a more specific pattern (TP1) involvingthe comparative construction (e.g., more than 10).All these patterns in fact point to a missing categoryfor cardinals in the lexicon: they are only associatedwith determiner TAG trees, not nouns, and thereforefail to combine with prepositions (e.g., of 1991, than10) and with proper names (e.g., November 1).For proper names (NNP), dates also show up be-cause months are tagged as proper names (TP3,TP4)as well as addresses TP5:TP5 NNP(?,?,?,?)
Brooklyn, n.y.,For prepositions (IN), we find, in addition to theTP1-TP2, the following two main patterns:TP6 DT(IN) those with, some ofTP7 RB(IN) just under, little morePattern TP6 points to a missing entry for wordssuch as those and some which are categorised in thelexicon as determiners but not as nouns.
TP7 pointsto a mismatch between the SR data and the formatexpected by the generator: while the latter expectsthe structure IN(RB), the input format provided bythe SR Task is RB(IN).4.4 Improving Generation Using the Results ofError MiningTable 2 shows how implementing some of the cor-rections suggested by error mining impacts the num-ber of NP chunks (size 4) that can be generated.
Inthis experiment, the total number of input (NP) de-pendency trees is 24995.
Before error mining, gen-eration failed on 33% of these input.
Correctingthe erroneous class inherited by all conjunction treesmentioned in Section 4.3.1 brings generation failuredown to 26%.
Converting the input data to the cor-rect input format to resolve the mismatch inducedby possessive ?s (cf.
Section 4.3.1) reduce gener-ation failure to 21%6 and combining both correc-tions results in a failure rate of 13%.
In other words,error mining permits quickly identifying two issueswhich, once corrected, reduces generation failure by20 points.When mining on clause size chunks, other mis-matches were identified such as in particular, mis-matches introduced by subjects and auxiliaries:6For NP of size 4, 3264 structures with possessive ?s wererewritten.NP 4 Before AfterSR Data 8361 6511Rewritten SR Data 5255 3401Table 2: Diminishing the number of errors using in-formation from error mining.
The table comparesthe number of failures on NP chunks of size 4 be-fore (first row) and after (second row) rewriting theSR data to the format expected by our generator andbefore (second column) and after (third column) cor-recting the grammar and lexicon errors discussed inSection 4.3.1while our generator expects both the subject and theauxiliary to be children of the verb, the SR data rep-resent the subject and the verb as children of the aux-iliary.5 Related WorkWe now relate our proposal (i) to previous proposalson error mining and (ii) to the use of error mining innatural language generation.Previous work on error mining.
(van Noord,2004) initiated error mining on parsing results witha very simple approach computing the parsabilityrate of each n-gram in a very large corpus.
Theparsability rate of an n-gram wi .
.
.
wn is the ratioR(wi .
.
.
wn) =C(wi...wn|OK)C(wi...wn)with C(wi .
.
.
wn)the number of sentences in which the n-gramwi .
.
.
wn occurs and C(wi .
.
.
wn | OK) the num-ber of sentences containing wi .
.
.
wn which couldbe parsed.
The corpus is stored in a suffix arrayand the sorted suffixes are used to compute the fre-quency of each n-grams in the total corpus and in thecorpus of parsed sentences.
The approach was laterextended and refined in (Sagot and de la Clergerie,2006) and (de Kok et al, 2009) whereby (Sagot andde la Clergerie, 2006) defines a suspicion rate for n-grams which takes into account the number of occur-rences of a given word form and iteratively definesthe suspicion rate of each word form in a sentencebased on the suspicion rate of this word form in thecorpus; (de Kok et al, 2009) combined the iterativeerror mining proposed by (Sagot and de la Clergerie,2006) with expansion of forms to n-grams of wordsand POS tags of arbitrary length.Our approach differs from these previous ap-598proaches in several ways.
First, error mining is per-formed on trees.
Second, it can be parameterised touse any combination of POS tag, dependency and/orword form information.
Third, it is applied to gener-ation input rather than parsing output.
Typically, theinput to surface realisation is a structured represen-tation (i.e., a flat semantic representation, a first or-der logic formula or a dependency tree) rather than astring.
Mining these structured representations thuspermits identifying causes of undergeneration in sur-face realisation systems.Error Mining for Generation Not much workhas been done on mining the results of surface re-alisers.
Nonetheless, (Gardent and Kow, 2007) de-scribes an error mining approach which works onthe output of surface realisation (the generated sen-tences), manually separates correct from incorrectoutput and looks for derivation items which system-atically occur in incorrect output but not in correctones.
In contrast, our approach works on the inputto surface realisation, automatically separates cor-rect from incorrect items using surface realisationand targets the most likely sources of errors ratherthan the absolute ones.More generally, our approach is the first to ourknowledge, which mines a surface realiser for un-dergeneration.
Indeed, apart from (Gardent andKow, 2007), most previous work on surface reali-sation evaluation has focused on evaluating the per-formance and the coverage of surface realisers.
Ap-proaches based on reversible grammars (Carroll etal., 1999) have used the semantic formulae outputby parsing to evaluate the coverage and performanceof their realiser; similarly, (Gardent et al, 2010) de-veloped a tool called GenSem which traverses thegrammar to produce flat semantic representationsand thereby provide a benchmark for performanceand coverage evaluation.
In both cases however, be-cause it is produced using the grammar exploited bythe surface realiser, the input produced can only beused to test for overgeneration (and performance) .
(Callaway, 2003) avoids this shortcoming by con-verting the Penn Treebank to the format expected byhis realiser.
However, this involves manually iden-tifying the mismatches between two formats muchlike symbolic systems did in the Generation Chal-lenge SR Task.
The error mining approach we pro-pose helps identifying such mismatches automati-cally.6 ConclusionPrevious work on error mining has focused on appli-cations (parsing) where the input data is sequentialworking mainly on words and part of speech tags.In this paper, we proposed a novel approach to errormining which permits mining trees.
We applied itto the input data provided by the Generation Chal-lenge SR Task.
And we showed that this supportsthe identification of gaps and errors in the grammarand in the lexicon; and of mismatches between theinput data format and the format expected by our re-aliser.We applied our error mining approach to the in-put of a surface realiser to identify the most likelysources of undergeneration.
We plan to also ex-plore how it can be used to detect the most likelysources of overgeneration based on the output ofthis surface realiser on the SR Task data.
Using thePenn Treebank sentences associated with each SRTask dependency tree, we will create the two treesets necessary to support error mining by dividingthe set of trees output by the surface realiser into aset of trees (FAIL) associated with overgeneration(the generated sentences do not match the originalsentences) and a set of trees (SUCCESS) associatedwith success (the generated sentence matches theoriginal sentences).
Exactly which tree should popu-late the SUCCESS and FAIL set is an open question.The various evaluation metrics used by the SR Task(BLEU, NIST, METEOR and TER) could be usedto determine a threshold under which an output isconsidered incorrect (and thus classificed as FAIL).Alternatively, a strict matching might be required.Similarly, since the surface realiser is non determin-istic, the number of output trees to be kept will needto be experimented with.AcknowledgmentsWe would like to thank Cle?ment Jacq for useful dis-cussions on the hybrid tree miner algorithm.
Theresearch presented in this paper was partially sup-ported by the European Fund for Regional Develop-ment within the framework of the INTERREG IV AAllegro Project.599ReferencesAnja Belz, Michael White, Dominic Espinosa, Eric Kow,Deirdre Hogan, and Amanda Stent.
2011.
The firstsurface realisation shared task: Overview and evalu-ation results.
In Proceedings of the 13th EuropeanWorkshop on Natural Language Generation (ENLG),Nancy, France.Charles B. Callaway.
2003.
Evaluating coverage forlarge symbolic NLG grammars.
In Proceedings of the18th International Joint Conference on Artificial Intel-ligence, pages 811?817, Acapulco, Mexico.John Carroll, Ann Copestake, Dan Flickinger, and Vik-tor Paznan?ski.
1999.
An efficient chart generatorfor (semi-)lexicalist grammars.
In Proceedings of the7th European Workshop on Natural Language Gener-ation, pages 86?95, Toulouse, France.Yun Chi, Yirong Yang, and Richard R. Muntz.
2004.Hybridtreeminer: An efficient algorithm for miningfrequent rooted trees and free trees using canonicalform.
In Proceedings of the 16th International Con-ference on and Statistical Database Management (SS-DBM), pages 11?20, Santorini Island, Greece.
IEEEComputer Society.Danie?l de Kok, Jianqiang Ma, and Gertjan van Noord.2009.
A generalized method for iterative error miningin parsing results.
In Proceedings of the 2009 Work-shop on Grammar Engineering Across Frameworks(GEAF 2009), pages 71?79, Suntec, Singapore.
As-sociation for Computational Linguistics.Claire Gardent and Eric Kow.
2007.
Spotting overgen-eration suspect.
In Proceedings of the 11th EuropeanWorkshop on Natural Language Generation (ENLG),pages 41?48, Schloss Dagstuhl, Germany.Claire Gardent and Laura Perez-Beltrachini.
2010.
Rtgbased surface realisation for tag.
In Proceedings of the23rd International Conference on Computational Lin-guistics (COLING), pages 367?375, Beijing, China.Claire Gardent, Benjamin Gottesman, and Laura Perez-Beltrachini.
2010.
Comparing the performance oftwo TAG-based Surface Realisers using controlledGrammar Traversal.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics(COLING - Poster session), pages 338?346, Beijing,China.Richert Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for english.
InProceedings of the 16th Nordic Conference of Com-putational Linguistics (NODALIDA), pages 105?112,Tartu, Estonia.Rajakrishnan Rajkumar, Dominic Espinosa, and MichaelWhite.
2011.
The osu system for surface realizationat generation challenges 2011.
In Proceedings of the13th European Workshop on Natural Language Gen-eration (ENLG), pages 236?238, Nancy, France.Beno?
?t Sagot and E?ric de la Clergerie.
2006.
Error min-ing in parsing results.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 329?336, Sydney,Australia.Gertjan van Noord.
2004.
Error mining for wide-coverage grammar engineering.
In Proceedings of the42nd Meeting of the Association for ComputationalLinguistics (ACL), pages 446?453, Barcelona, Spain.600
