I nc rementa l  In terpretat ion  of  Categor ia l  Grammar*David MilwardCentre for Cognitive ScienceUniversity of Edinburgh2 Buccleuch Place, Edinburgh, EH8 9LW, U.K.davidm@cogsci.ed.ac.ukAbstractThe paper describes a parser for Catego-rial Grammar which provides fully wordby word incremental interpretation.
Theparser does not require fragments of sen-tences to form constituents, and therebyavoids problems of spurious ambiguity.The paper includes a brief discussion ofthe relationship between basic Catego-rial Grammar and other formalisms uchas HPSG, Dependency Grammar andthe Lambek Calculus.
It also includesa discussion of some of the issues whicharise when parsing lexicalised grammars,and the possibilities for using statisticaltechniques for tuning to particular lan-guages.1 IntroductionThere is a large body of psycholinguistic evidencewhich suggests that meaning can be extracted be-fore the end of a sentence, and before the endof phrasal constituents (e.g.
Marslen-Wilson 1973,Tanenhaus et al 1990).
There is also recent evi-dence suggesting that, during speech processing,partial interpretations can be built extremely ra-pidly, even before words are completed (Spivey-Knowlton et al 1994) 1.
There are also potentialcomputational pplications for incremental inter-pretation, including early parse filtering using sta-tistics based on logical form plausibility, and in-terpretation of fragments of dialogues (a surveyis provided by Milward and Cooper, 1994, hence-forth referred to as M&:C).In the current computational nd psycholingui-stic literature there are two main approaches tothe incremental construction of logical forms.
Oneapproach is to use a grammar with 'non-standard'*This research was supported by the U.K. Scienceand Engineering Research Council, grant RR30718.I am grateful to Patrick Sturt, Carl Vogel, and thereviewers for comments on an earlier version.1Spivey-Knowlton et al reported 3 experiments.One showed effects before the end of a word whenthere was no other appropriate word with the sameinitial phonology.
Another showed on-line effectsfrom adjectives and determiners during noun phraseprocessing.constituency, so that an initial fragment of a sen-tence, such as John likes, can be treated as a con-stituent, and hence be assigned a type and a se-mantics.
This approach is exemplified by Com-binatory Categorial Grammar,  CCG (Steedman1991), which takes a basic CG with just applica-tion, and adds various new ways of combining ele-ments together 2.
Incremental interpretation canthen be achieved using a standard bottom-up shiftreduce parser, working from left to right alongthe sentence.
The alternative approach, exempli-fied by the work of Stabler on top-down parsing(Stabler 1991), and Pulman on left-corner parsing(Pulman 1986) is to associate a semantics directlywith the partial structures formed during a top-down or left-corner parse.
For example, a syntaxtree missing a noun phrase, such as the followings/ \np vpJohn / \v np"likescan be given a semantics as a function from enti-ties to truth values i.e.
Ax.
l ikes( john,x) ,  with-out having to say that John likes is a constituent.Neither approach is without problems.
If agrammar is augmented with operations which arepowerful enough to make most initial fragmentsconstituents, then there may be unwanted inter-actions with the rest of the grammar (examplesof this in the case of CCG and the Lambek Cal-culus are given in Section 2).
The addition ofextra operations also means that, for any givenreading of a sentence there will generally be manydifferent possible derivations (so-called 'spurious'ambiguity), making simple parsing strategies suchas shift-reduce highly inefficient.The limitations of the parsing approaches be-come evident when we consider grammars withleft recursion.
In such cases a simple top-downparser will be incomplete, and a left corner parserwill resort to buffering the input (so won't be fully2Note that CCG doesn't provide a type for all in-itial fragments of sentences.
For example, it gives atype to John thinks Mary, but not to John thinks each.In contrast he Lambek Calculus (Lambek 1958) pro-vides an infinite number of types for any initial sen-tence fragment.119word-by-word).
M&C illustrate the problem byconsidering the fragment Mary thinks John.
Thishas a small number of possible semantic repre-sentations (the exact number depending upon thegrammar) e.g.
),P.thinks (mary, P (john))AP.AQ.
Q (thinks(mary,P (john)))),P.AR.
(R(Ax.thinks(x,P (john)))) (mary)The second representation is appropriate if thesentence finishes with a sentential modifier.
Thethird allows there to be a verb phrase modifier.If the semantic representation is to be read offsyntactic structure, then the parser must providea single syntax tree (possibly with empty nodes).However, there are actually any number of suchsyntax trees corresponding to, for example, thefirst semantic representation, since the np and thes can be arbitrarily far apart.
The following tree issuitable for the sentence Mary thinks John shavesbut not for e.g.
Mary thinks John coming here wasa mistake.S/ \np vpMary / \V Sth inks  / \np vp^JohnM&C suggest various possibilities for packing thepartial syntax trees, including using Tree Adjoi-ning Grammar (Joshi 1987) or Description Theory(Marcus et al 1983).
One further possibility is tochoose a single syntax tree, and to use destructivetree operations later in the parse a.The approach which we will adopt here is basedon Milward (1992, 1994).
Partial syntax trees canbe regarded as performing two main roles.
Thefirst is to provide syntactic information which gui-des how the rest of the sentence can be integratedinto the tree.
The second is to provide a basis for asemantic representation.
The first role can be cap-tured using syntactic types, where each type corre-sponds to a potentially infinite number of partialsyntax trees.
The second role can be captured bythe parser constructing semantic representationsdirectly.
The general processing model thereforeconsists of transitions of the form:Syntactic type i -+ Syntactic typei+ 1Semantic repi Semantic repi+ 13This might turn out to be similar to one view ofTree Adjoining Grammar, where adjunction adds intoa pre-existing well-formed tree structure.
It is alsocloser to some methods for incremental daptation ofdiscourse structures, where additions are allowed tothe right-frontier of a tree structure (e.g.
Polanyi andScha 1984).
There are however problems with thiskind of approach when features are considered (seee.g.
Vijay-Shanker 1992).This provides a state-transition r dynamic modelof processing, with each state being a pair of asyntactic type and a semantic value.The main difference between our approach andthat of Milward (1992, 1994) is that it is basedon a more expressive grammar formalism, Appli-cative Categorial Grammar, as opposed to Lexi-calised Dependency Grammar.
Applicative Cate-gorial Grammars allow categories to have argu-ments which are themselves functions (e.g.
verycan be treated as a function of a function, and gi-ven the type (n /n ) / (n /n )  when used as an adjec-tival modifier).
The ability to deal with functionsof functions has advantages in enabling more ele-gant linguistic descriptions, and in providing onekind of robust parsing: the parser never fails untilthe last word, since there could always be a finalword which is a function over all the constituentsformed so far.
However, there is a correspondingproblem of far greater non-determinism, with evenunambiguous words allowing many possible tran-sitions.
It therefore becomes crucial to either per-form some kind of ambiguity packing, or languagetuning.
This will be discussed in the final sectionof the paper.2 Applicative Categorial GrammarApplicative Categorial Grammar is the most ba-sic form of Categorial Grammar, with just a singlecombination rule corresponding to function appli-cation.
It was first applied to linguistic descrip-tion by Adjukiewicz and Bar-Hillel in the 1950s.Although it is still used for linguistic description(e.g.
Bouma and van Noord, 1994), it has beensomewhat overshadowed in recent years by HPSG(Pollard and Sag 1994), and by Lambek Cate-gorial Grammars (Lambek 1958).
It is thereforeworth giving some brief indications of how it fitsin with these developments.The first directed Applicative CG was proposedby Bar-Hillel (1953).
Functional types included alist of arguments to the left, and a list of argu-ments to the right.
Translating Bar-Hillel's nota-tion into a feature based notation similar to thatin HPSG (Pollard and Sag 1994), we obtain thefollowing category for a ditransitive verb such asput:r s \] Unp>L r(np, pp>The list of arguments to the left are gathered un-der the feature, l, and those to the right, an npand a pp in that order, under the feature r.Bar-Hillel employed a single application rule,which corresponds to the following:120ix 1 L~ .. .
L1 I ( L1  .
.
?
Ln) R1 .
.. Rn ~ X r(R1 ...R~>The result was a system which comes very close tothe formalised ependency grammars of Gaifman(1965) and Hays (1964).
The only real differenceis that Bar-Hillel allowed arguments to themsel-ves be functions.
For example, an adverb such asslowly could be given the type 4LrOAn unfortunate aspect of Bar-Hillel's first systemwas that the application rule only ever resultedin a primitive type.
Hence, arguments with fun-ctional types had to correspond to single lexicalitems: there was no way to form the type np \s  ~for a non-lexical verb phrase such as likes Mary.Rather than adapting the Application Rule toallow functions to be applied to one argument at atime, Bar-Hillel's second system (often called ABCategorial Grammar, or Adjukiewicz/Bar-HillelCG, Bar-Hillel 1964) adopted a 'Curried' nota-tion, and this has been adopted by most CGssince.
To represent a function which requires annp on the left, and an np and a pp to the right,there is a choice of the following three types usingCurried notation:np\( (s /pp)/np)(np\(s /pp)) /np((np\s) /pp)/npMost CGs either choose the third of these (to givea vp structure), or include a rule of Associativitywhich means that the types are interchangeable(in the Lambek Calculus, Associativity is a conse-quence of the calculus, rather than being specifiedseparately).The main impetus to change Applicative CGcame from the work of Ades and Steedman (1982).Ades and Steedman oted that the use of functioncomposition allows CGs to deal with unboundeddependency constructions.
Function compositionenables a function to be applied to its argument,even if that argument is incomplete .g.s/pp + pp/np --+ s/npThis allows peripheral extraction, where the 'gap'is at the start or the end of e.g.
a relative clause.Variants of the composition rule were proposedin order to deal with non-peripheral extraction,4The reformulation is not entirely faithful here toBar-Hillel, who used a slightly problematic 'doubleslash' notation for functions of functions.5Lambek notation (Lambek 1958).but this led to unwanted effects elsewhere in thegrammar (Bouma 1987).
Subsequent treatmentsof non-peripheral extraction based on the LambekCalculus (where standard composition is built in:it is a rule which can be proven from the calcu-lus) have either introduced an alternative to theforward and backward slashes i.e.
/ and \ for nor-mal args, ?
for wh-args (Moortgat 1988), or haveintroduced so called modal operators on the wh-argument (Morrill et al 1990).
Both techniquescan be thought of as marking the wh-arguments arequiring special treatment, and therefore do notlead to unwanted effects elsewhere in the gram-mar.However, there are problems with having justcomposition, the most basic of the non-applicativeoperations.
In CGs which contain functions offunctions (such as very, or slowly), the addition ofcomposition adds both new analyses of sentences,and new strings to the language.
This is due tothe fact that composition can be used to form afunction, which can then be used as an argumentto a function of a function.
For example, if thetwo types, n /n  and n /n  are composed to give thetype n /n ,  then this can be modified by an adjec-tival modifier of type (n /n ) / (n /n ) .
Thus, thenoun very old dilapidated car can get the unac-ceptable bracketing, \[\[very \[old dilapidated\]\] car\].Associative CGs with Composition, or the Lam-bek Calculus also allow strings such as boy withthe to be given the type n /n  predicting very boywith the car to be an acceptable noun.
Althoughindividual examples might be possible to rule outusing appropriate f atures, it is difficult to see howto do this in general whilst retaining a calculussuitable for incremental interpretation.If wh-arguments need to be treated speciallyanyway (to deal with non-peripheral extraction),and if composition as a general rule is proble-matic, this suggests we should perhaps return togrammars which use just Application as a gene-ral operation, but have a special treatment forwh-arguments.
Using the non-Curried notationof Bar-Hillel, it is more natural to use a separatewh-list than to mark wh-arguments individually.For example, the category appropriate for relativeclauses with a noun phrase gap would be:lO|,:o /It is then possible to specify operations which actas purely applicative operations with respect tothe left and right arguments lists, but more likecomposition with respect o the wh-list.
This isvery similar to the way in which wh-movementis dealt with in GPSG (Gazdar et al 1985) andHPSG, where wh-arguments are treated usingslash mechanisms or feature inheritance principles121which correspond closely to function composition.Given that our arguments have produced a cate-gorial grammar which looks very similar to HPSG,why not use HPSG rather than Applicative CG?The main reason is that Applicative CG is a muchsimpler formalism, which can be given a very sim-ple syntax semantics interface, with function ap-plication in syntax mapping to function applica-tion in semantics 6'7.
This in turn makes it relati-vely easy to provide proofs of soundness and com-pleteness for an incremental parsing algorithm.Ultimately, some of the techniques developed hereshould be able to be extended to more complexformalisms uch as HPSG.3 AB Categor ia l  g rammar  w i thAssoc ia t iv i ty  (AACG)In this section we define a grammar similar to Bar-Hillel's first grammar.
However, unlike Bar-Hillel,we allow one argument to be absorbed at a time.The resulting grammar is equivalent to AB Cate-gorial Grammar plus associativity.The categories of the grammar are defined asfollows:1.
If X is a syntactic type (e.g.
s, np), then10 is a category.r02.
If X is a syntactic type, and L and R are listsof categories, thenApplication to the right is defined by the ruleS:6One area where application based approaches tosemantic ombination gain in simplicity over unifica-tion based approaches i in providing semantics forfunctions of functions.
Moore (1989) provides a treat-ment of functions of functions in a unification basedapproach, but only by explicitly incorporating lambdaexpressions.
Pollard and Sag (1994) deal with somefunctions of functions, such as non-intersective adjec-tives, by explicit set construction.7As discussed above, wh-movement requires ome-thing more like composition than application.
A sim-ple syntax semantics interface can be retained if thesame operation is used in both syntax and semantics.Wh-arguments can be treated as similar to other ar-guments i.e.
as lambda abstracted in the semantics.For example, the fragment: John found a woman whoMary can be given the semantics ,kP.3x.
woman(x)&: found(john,x) g~ P(mary,  x), where P is a fun-ction from a left argument Mary of type e and a wh-argument, also of type e.s,., is list concatenation e.g.
(np)-(s) equals (np,s).j jApplication to the left is defined by the rule:L=R \[=RJThe basic grammar provides some spurious deri-vations, since sentences such as John likes Marycan be bracketed as either ((John likes) Mary)or (John (likes Mary)).
However, we will seethat these spurious derivations do not translateinto spurious ambiguity in the parser, which mapsfrom strings of words directly to semantic repre-sentations.4 An  Incrementa l  ParserMost parsers which work left to right along aninput string can be described in terms of statetransitions i.e.
by rules which say how the currentparsing state (e.g.
a stack of categories, or a chart)can be transformed by the next word into a newstate.
Here this will be made particularly explicit,with the parser described in terms of just two ruleswhich take a state, a new word and create a newstate 9.
There are two unusual features.
Firstly,there is nothing equivalent to a stack mechanism:at all times the state is characterised by a singlesyntactic type, and a single semantic value, notby some stack of semantic values or syntax treeswhich are waiting to be connected together.
Se-condly, all transitions between states occur on theinput of a new word: there are no 'empty' tran-sitions (such as the reduce step of a shift-reduceparser).The two rules, which are given in Figure 1 t?, aredifficult to understand in their most general form.Here we will work upto the rules gradually, by con-sidering which kinds of rules we might need in par-ticular instances.
Consider the following pairingof sentence fragments with their simplest possibleCG type:Mary thinks: s/sMary thinks John: s/(np\s)Mary thinks John likes: s/npMary thinks John likes Sue: sNow consider taking each type as a description ofthe state that the parser is in after absorbing thefragment.
We obtain a sequence of transitions asfollows:9This approach isdescribed in greater detail in Mil-ward (1994), where parsers are specified formally interms of their dynamics.l?Li, Ri, Hi are lists of categories, li and ri are listsof variables, of the same length as the correspondingLi and Ri.122State-Application:Y1<>1Lo ) .
R2r( \[ rP~ohHoh<>FState-Prediction:Y10ILl "Lor< rRo ) ?
R2hL1, HohOF?
{W ~ __} rRI"R2h<>Ar~.
F(G(r,))YI<>XrR1 ?
< rRohO~rl.(ah.
F(all.
(h( ar (((G rl)r)ll)))))Figure h Transition Rules?
L0> .R2.H0where W:where W:1XorRI"Roh<>GIZLI ,,LrR1.Rh0G"John .
.
.
.
likes .
.
.
.
Sue" s/s -~ s/(np\s) -~ s/np --~ sIf an embedded sentence such as John likes Sueis a mapping from an s /s  to an s, this suggeststhat it might be possible to treat all sentences asmapping from some category expecting an s tothat category i.e.
from X/s  to X.
Similarly, allnoun phrases might be treated as mappings froman X /np  to an X.Now consider individual transitions.
The sim-plest of these is where the type of argument ex-pected by the state is matched by the next wordi.e.
"Sue" s/np -~ s where: Sue: npThis can be generalised to the following rule,which is similar to Function Application in stan-dard CG 11X/Y "~" X where: W: YA similar transition occurs for likes.
Here an np \swas expected, but likes only provides part of this:nit  differs in not being a rule of grammar: herethe functor is a state category and the argument is alexical category.
In standard CG function application,the functor and argument can correspond to a wordor a phrase.it requires an np to the right to form an np\s .Thus after likes is absorbed the state category willneed to expect an np.
The rule required is similarto Function Composition in CG i.e.,{W~ X/Y ~ X/Z where: W: Y/ZConsidering this informally in terms of tree struc-tures, what is happening is the replacement of anempty node in a partial tree by a second partialtree i.e.X/ \U Y^x/ \+ Y => U Y/ \  / \V z ^ V Z ^The two rules specified so far need to be furthergeneralised to allow for the case where a lexicalitem has more than one argument (e.g.
if we re-place likes by a di-transitive such as gives or atri-transitive such as bets).
This is relatively tri-vial using a non-curried notation similar to thatused for AACG.
What we obtain is the single ruleof State-Application, which corresponds to appli-cation when the list of arguments, R1, is empty,to function composition when R1 is of length one,and to n-ary composition when R1 is of length n.The only change needed from AACG notation is1238I(>r(8>h(>AQ.Q"John" --+SI<>l(np> r< / >L h<np)h0AH.
(n(john'))"likes" rs 1 1<> | r(np>t hOAY.likes'(john',Y)Figure 2: Possible state transitionsi<>r(> |h<>Jlikes'(john',sue')the inclusion of an extra feature list, the h list,which stores information about which argumentsare waiting for a head (the reasons for this will beexplained later).
The lexicon is identical to thatfor a standard AACG, except for having h-listswhich are always set to empty.Now consider the first transition.
Here a sen-tence was expected, but what was encounteredwas a noun phrase, John.
The appropriate rulein CG notation would be:"W"  X/Y  -+ X/ (Z\Y)  where: W: ZThis rule states that if looking for a Y and get aZ then look for a Y which is missing a Z.
In treestructure terms we have:X X/ \  / \U Y^ + Z => U Y/ \z zkY ~The rule of State-Prediction is obtained by furthergeneralising to allow the lexical item to have mis-sing arguments, and for the expected argument tohave missing arguments.State-Application and State-Prediction to-gether provide the basis of a sound and completeparser 12.
Parsing of sentences i achieved by star-ting in a state expecting a sentence, and apply-ing the rules non-deterministically as each wordis input.
A successful parse is achieved if the fi-nal state expects no more arguments.
As an ex-ample, reconsider the string John likes Sue.
Thesequence of transitions corresponding to John li-kes Sue being a sentence, is given in Figure 2.The transition on encountering John is determini-stic: State-Application cannot apply, and State-Prediction can only be instantiated one way.
Theresult is a new state expecting an argument which,given an np  could give an s i.e.
an np\s .12The parser accepts the same strings as the gram-mar and assigns them the same semantic values.
Thisis slightly different from the standard notion of so-undness and completeness of a parser, where the par-ser accepts the same strings as the grammar and as-signs them the same syntax trees.The transition on input of likes is non-deterministic.
State-Application can apply, as inFigure 2.
However, State-Prediction can also ap-ply, and can be instantiated in four ways (thesecorrespond to different ways of cutting up theleft and right subcategorisation lists of the le-xical entry, likes, i.e.
as (np> ?
0 or 0 ?
(np>).One possibility corresponds to the prediction ofan s \s  modifier, a second to the prediction of an(np \s ) \ (np \s )  modifier (i.e.
a verb phrase too-differ), a third to there being a function whichtakes the subject and the verb as separate argu-ments, and the fourth corresponds to there being afunction which requires an s /np  argument.
Thesecond of these is perhaps the most interesting,and is given in Figure 3.
It is the choice of thisparticular transition at this point which allowsverb phrase modification, and hence, assuming thenext word is Sue, an implicit bracketing of thestring fragment as (John (likes Sue)).
Note thatif State-Application is chosen, or the first of theState-Prediction possibilities, the fragment Johnlikes Sue retains a flat structure.
If there is tobe no modification of the verb phrase, no verbphrase structure is introduced.
This relates tothere being no spurious ambiguity: each choice oftransition has semantic onsequences; each choiceaffects whether a particular part of the semanticsis to be modified or not.Finally, it is worth noting why it is necessary touse h-lists.
These are needed to distinguish bet-ween cases of real functional arguments (of func-tions of functions), and functions formed by State-Prediction.
Consider the following trees, wherethe np \s  node is empty.S S/ \  / \s / s  s np np \s/ \  / \np np \s"  (np \s ) / (np \s )  np \s  ^Both trees have the same syntactic type, howeverin the first case we want to allow for there to bean s \s  modifier of the lower s, but not in the se-cond.
The headed list distinguishes between thetwo cases, with only the first having an np  on its124Si0l(np>r< I r<) >!L h<np)hOAH.
(H(john'))"lik_~s""S1<>Sl(np)1( / rO , up>!L h0r(np, )r<)1 (np)h( / rO , np>!L hOh(>AY.AK.
(K(AX.likes'(X,Y))) (john)where W:Figure 3: Example instantiation of State-Prediction8l(np>| r(np>Lh<>AY.AX.Iikes'(X,Y)headed list, allowing prediction of an s modifier.5 Parsing Lexicalised GrammarsWhen we consider full sentence processing, as op-posed to incremental processing, the use of lexi-calised grammars has a major advantage over theuse of more standard rule based grammars.
Inprocessing a sentence using a lexicalised formalismwe do not have to look at the grammar as a whole,but only at the grammatical information indexedby each of the words.
Thus increases in the sizeof a grammar don't necessarily effect efficiency ofprocessing, provided the increase in size is due tothe addition of new words, rather than increasedlexical ambiguity.
Once the full set of possible le-xical entries for a sentence is collected, they can,if required, then be converted back into a set ofphrase structure rules (which should correspondto a small subset of the rule based formalism equi-valent to the whole lexicalised grammar), beforebeing parsing with a standard algorithm such asEarley's (Earley 1970).In incremental parsing we cannot predict whichwords will appear in the sentence, so cannot usethe same technique.
However, if we are to base aparser on the rules given above, it would seem thatwe gain further.
Instead of grammatical informa-tion being localised to the sentence as a whole, itis localised to a particular word in its particularcontext: there is no need to consider a pp as astart of a sentence if it occurs at the end, even ifthere is a verb with an entry which allows for asubject pp.However there is a major problem.
As we notedin the last paragraph, it is the nature of parsingincrementally that we don't know what words areto come next.
But here the parser doesn't evenuse the information that the words are to comefrom a lexicon for a particular language.
For ex-ample, given an input of 3 nps, the parser willhappily create a state expecting 3 nps to the left.This might be a likely state for say a head finallanguage, but an unlikely state for a language suchas English.
Note that incremental interpretationwill be of no use here, since the semantic represen-tation should be no more or less plausible in thedifferent languages.
In practical terms, a naive in-teractive parallel Prolog implementation  a cur-rent workstation fails to be interactive in a realsense after about 8 words 13.What seems to be needed is some kind of langu-age tuning 14.
This could be in the nature of fixedrestrictions to the rules e.g.
for English we mightrule out uses of prediction when a noun phrase isencountered, and two already exist on the left list.A more appealing alternative is to base the tuningon statistical methods.
This could be achieved byrunning the parser over corpora to provide pro-babilities of particular transitions given particu-lar words.
These transitions would capture thelikelihood of a word having a particular part ofspeech, and the probability of a particular transi-tion being performed with that part of speech.13This result should however be treated with somecaution: in this implementation there was no attemptto perform any packing of different possible transiti-ons, and the algorithm has exponential complexity.
Incontrast, a packed recogniser based on a similar, butmuch simpler, incremental parser for Lexicalised De-pendency Grammar has O(n a) time complexity (Mil-ward 1994) and good practical performance, taking acouple of seconds on 30 word sentences.14The usage of the term language tuning is perhapsbroader here than its use in the psycholinguistic l te-rature to refer to different structural preferences bet-ween languages e.g.
for high versus low attachment(Mitchell et al 1992).125There has already been some early work done onproviding statistically based parsing using transi-tions between recursively structured syntactic a-tegories (Tugwell 1995) 15.
Unlike a simple Markovprocess, there are a potentially infinite number ofstates, so there is inevitably a problem of sparsedata.
It is therefore necessary to make variousgeneralisations over the states, for example by ig-noring the R2 lists.The full processing model can then be eitherserial, exploring the most highly ranked transiti-ons first (but allowing backtracking if the seman-tic plausibility of the current interpretation dropstoo low), or ranked parallel, exploring just the npaths ranked highest according to the transitionprobabilities and semantic plausibility.6 Conc lus ionTim paper has presented a method for providinginterpretations word by word for basic CategorialGrammar.
The final section contrasted parsingwith lexicalised and rule based grammars, and ar-gued that statistical anguage tuning is particu-larly suitable for incremental, exicalised parsingstrategies.ReferencesAdes, A. ga Steedman, M.: 1972, 'On the Order ofWords', Linguistics ~d Philosophy 4, 517-558.Bar-Hillel, Y.: 1953, 'A Quasi-Arithmetical Notationfor Syntactic Description', Language 29, 47-58.Bar-Hillel, Y.: 1964, Language ~d Information:Selected Essays on Their Theory ~4 Application,Addison-Wesley.Bouma, G.: 1987, 'A Unification-Based Analysis ofUnbounded Dependencies', in Proceedings of the6th Amsterdam Colloquium, ITLI, University ofAmsterdam.Bouma, G. & van Noord, G.: 1994, 'Constraint-BasedCategorial Grammar', in Proceedings of the 32ndACL, Las Cruces, U.S.A.Earley, J.: 1970, 'An Efficient Context-free ParsingAlgorithm', ACM Communications 13(2), 94-102.Gaifman, H.: 1965, 'Dependency Systems L: PhraseStructure Systems', Information ~ Control 8: 304-337.Gazdar, G., Klein, E., Pullum, G.K., & Sag,I.A.
: 1985, Generalized Phrase Structure Gram-mar, Blackwell, Oxford.Hays, D.G.
: 1964, 'Dependency Theory: A Forma-lism ~z Some Observations', Language 40, 511-525.Joshi, A.K.
: 1987, 'An Introduction to Tree AdjoiningGrammars', in Manaster-Ramer (ed.
), Mathema-tics of Language, John Benjamins, Amsterdam.l~Tugwell's approach does however differ in that thestate transitions are not limited by the rules of State-Prediction and State-Application.
This has advanta-ges in allowing the grammar to learn phenomena suchas heavy NP shift, but has the disadvantage of suf-fering from greater sparse data problems.
A compro-mise system using the rules here, but allowing reorde-ring of the r-lists might be preferable.Lambek, J.: 1958, 'The Mathematics of SentenceStructure', American Mathematical Monthly 65,154-169.Marcus, M., Hindle, D., & Fleck, M.: 1983, 'D-Theory: Talking about Talking about Trees', inProceedings of the 21st ACL, Cambridge, Mass.Marslen-Wilson, W.: 1973, 'Linguistic Structure &Speech Shadowing at Very Short Latencies', Na-ture 244, 522-523.Milward, D.: 1992, 'Dynamics, Dependency Gram-mar & Incremental Interpretation', in Proceedingsof COLING 92, Nantes, vol 4, 1095-1099.Milward, D. & Cooper, R.: 1994, 'Incremental Inter-pretation: Applications, Theory &; Relationship toDynamic Semantics', in Proceedings of COLING93, Kyoto, Japan, 748-754.Milward, D.: 1994, 'Dynamic Dependency Grammar',to appear in Linguistics ~d Philosophy 17, 561-605.Mitchell, D.C., Cuetos, F., &= Corley, M.M.B.
: 1992,'Statistical versus linguistic determinants of par-sing bias: cross-linguistic evidence'.
Paper presen-ted at the 5th Annual CUNY Conference on Hu-man Sentence Processing, New York.Moore, R.C.
: 1989, 'Unification-Based Semantic In-terpretation', in Proceedings of the 27th ACL, Van-couver.Moortgat, M.: 1988, Categorial Investigations: Logi-cal ed Linguistic Aspects of the Lambek Calculus,Foris, Dordrecht.Morrill, G., Leslie, N., Hepple, M. & Barry, G.: 1990,'Categorial Deductions ~z Structural Operations',in Barry, G. ~z Morrill, G.
(eds.
), Studies in Ca-tegorial Grammar, Edinburgh Working Papers inCognitive Science, 5.Polanyi, L. & Scha, R.: 1984, 'A Syntactic Approachto Discourse Semantics', in Proceedings of CO-LING 8~, Stanford, 413-419.Pollard, C. &: Sag, I.A.
: 1994, Head-Driven PhraseStructure Grammar, University of Chicago Press&: CSLI Publications, Chicago.Pulman, S.G.: 1986, 'Grammars, Parsers, ~z MemoryLimitations', Language ~d Cognitive Processes 1(3),197-225.Spivey-Knowlton, M., Sedivy, J., Eberhard, K., & Ta-nenhaus, M.: 1994, 'Psycholinguistic Study of theInteraction Between Language & Vision', in Pro-ceedings of the 12th National Conference on AI,AAAI-9~.Stabler, E.P.
: 1991, 'Avoid the Pedestrian's Paradox',in Berwiek, R.C.
et al (eds.
), Principle-Based Par-sing: Computation ~d Psyeholinguistics, Kluwer,Netherlands, 199-237.Steedman, M.J.: 1991, 'Type-Raising 8z Directiona-lity in Combinatory Grammar', in Proceedings ofthe 29th ACL, Berkeley, U.S.A.Tanenhaus, M.K., Garnsey, S., ~ Boland, J.: 1990,'Combinatory Lexical Information &: LanguageComprehension', in Altmann, G.T.M.
CognitiveModels of Speech Processing, MIT Press, Cam-bridge Ma.Tugwell, D.: 1995, 'A State-Transition Grammar forData-Oriented Parsing', in Proceedings of the 7thConference of the European ACL, EACL-95, Dub-lin, this volume.Vijay-Shanker, K.: 1992, 'Using Descriptions of Treesin a Tree Adjoining Grammar', ComputationalLinguistics 18(4), 481-517.126
