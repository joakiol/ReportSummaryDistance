Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 177?184, Vancouver, October 2005. c?2005 Association for Computational LinguisticsInner-Outer Bracket Models for Word Alignmentusing Hidden BlocksBing ZhaoSchool of Computer ScienceCarnegie Mellon University{bzhao}@cs.cmu.eduNiyu Ge and Kishore PapineniIBM T. J. Watson Research CenterYorktown Heights, NY 10598, USA{niyuge, papineni}@us.ibm.comAbstractMost statistical translation systems arebased on phrase translation pairs, or?blocks?, which are obtained mainly fromword alignment.
We use blocks to inferbetter word alignment and improved wordalignment which, in turn, leads to betterinference of blocks.
We propose two newprobabilistic models based on the inner-outer segmentations and use EM algorithmsfor estimating the models?
parameters.
Thefirst model recovers IBM Model-1 as a spe-cial case.
Both models outperform bi-directional IBM Model-4 in terms of wordalignment accuracy by 10% absolute on theF-measure.
Using blocks obtained fromthe models in actual translation systemsyields statistically significant improvementsin Chinese-English SMT evaluation.1 IntroductionToday?s statistical machine translation systems relyon high quality phrase translation pairs to acquirestate-of-the-art performance, see (Koehn et al, 2003;Zens and Ney, 2004; Och and Ney, 2003).
Here,phrase pairs, or ?blocks?
are obtained automati-cally from parallel sentence pairs via the underlyingword alignments.
Word alignments traditionally arebased on IBM Models 1-5 (Brown et al, 1993) or onHMMs (Vogel et al, 1996).
Automatic word align-ment is challenging in that its accuracy is not yetclose to inter-annotator agreement in some languagepairs: for Chinese-English, inter-annotator agree-ment exceeds 90 on F-measure whereas IBM Model-4 or HMM accuracy is typically below 80s.
HMMsassume that words ?close-in-source?
are aligned towords ?close-in-target?.
While this locality assump-tion is generally sound, HMMs do have limitations:the self-transition probability of a state (word) is theonly control on the duration in the state, the lengthof the phrase aligned to the word.
Also there is nonatural way to control repeated non-contiguous vis-its to a state.
Despite these problems, HMMs remainattractive for their speed and reasonable accuracy.We propose a new method for localizing wordalignments.
We use blocks to achieve locality in thefollowing manner: a block in a sentence pair is asource phrase aligned to a target phrase.
We assumethat words inside the source phrase cannot align towords outside the target phrase and that words out-side the source phrase cannot align to words insidethe target phrase.
Furthermore, a block divides thesentence pair into two smaller regions: the innerpart of the block, which corresponds to the sourceand target phrase in the block, and the outer part ofthe block, which corresponds to the remaining sourceand target words in the parallel sentence pair.
Thetwo regions are non-overlapping; and each of them isshorter than the original parallel sentence pair.
Theregions are thus easier to align than the original sen-tence pairs (e.g., using IBM Model-1).
While themodel uses a single block to split the sentence pairinto two independent regions, it is not clear whichblock we should select for this purpose.
Therefore,we treat the splitting block as a hidden variable.This proposed approach is far simpler than treat-ing the entire sentence as a sequence of non-overlapping phrases (or chunks) and considering suchsequential segmentation either explicitly or implic-itly.
For example, (Marcu and Wong, 2002) for ajoint phrase based model, (Huang et al, 2003) fora translation memory system; and (Watanabe etal., 2003) for a complex model of insertion, deletionand head-word driven chunk reordering.
Other ap-proaches including (Watanabe et al, 2002) treat ex-tracted phrase-pairs as new parallel data with limitedsuccess.
Typically, they share a similar architectureof phrase level segmentation, reordering, translationas in (Och and Ney, 2002; Koehn and Knight, 2002;Yamada and Knight, 2001).
The phrase level inter-action has to be taken care of for the non-overlappingsequential segmentation in a complicated way.
Ourmodels model such interactions in a soft way.
Thehidden blocks are allowed to overlap with each other,177while each block induced two non-overlapping re-gions, i.e.
the model brackets the sentence pairinto two independent parts which are generated syn-chronously.
In this respect, it resembles bilingualbracketing (Wu, 1997), but our model has more lex-ical items in the blocks with many-to-many wordalignment freedom in both inner and outer parts.We present our localization constraints usingblocks for word alignment in Section 2; we detail ourtwo new probabilistic models and their EM train-ing algorithms in Section 3; our baseline system, amaximum-posterior inference for word alignment, isexplained in Section 4; experimental results of align-ments and translations are in Section 5; and Section6 contains discussion and conclusions.2 Segmentation by a BlockWe use the following notation in the remainder ofthis paper: e and f denote the English and foreignsentences with sentence lengthes of I and J , respec-tively.
ei is an English word at position i in e; fj isa foreign word at position j in f .
a is the alignmentvector with aj mapping the position of the Englishword eaj to which fj connects.
Therefore, we havethe standard limitation that one foreign word can-not be connected to more than one English word.
Ablock ?
[] is defined as a pair of brackets as follows:?
[] = (?e, ?f ) = ([il, ir], [jl, jr]), (1)where ?e = [il, ir] is a bracket in English sentence de-fined by a pair of indices: the left position il and theright position ir, corresponding to a English phraseeiril .
Similar notations are for ?f = [jl, jr], which isone possible projection of ?e in f .
The subscript l andr are abbreviations of left and right, respectively.
?e segments e into two parts: (?e, e) = (?e?, ?e/?
).The inner part ?e?
= {ei, i ?
[il, ir]} and the outerpart ?e/?
= {ei, i /?
[il, ir]}; ?f segments f similarly.Thus, the block ?
[] splits the parallel sentence pairinto two non-overlapping regions: the Inner ?[]?
andOuter ?[]/?
parts (see Figure 1).
With this segmen-tation, we assume the words in the inner part arealigned to inner part only: ?[]?
= ?e?
?
?f?
: {ei, i ?
[il, ir]} ?
{fj , j ?
[jl, jr]}; and words in the outerpart are aligned to outer part only: ?[]/?
= ?e/?
?
?f/?
:{ei, i /?
[il, ir]} ?
{fj , j /?
[jl, jr]}.
We do not allowalignments to cross block boundaries.
Words insidea block ?
[] can be aligned using a variety of models(IBM models 1-5, HMM, etc).
We choose Model1 forsimplicity.
If the block boundaries are accurate, wecan expect high quality word alignment.
This is ourproposed new localization method.OuterInnerli rirjlje?f?Figure 1: Segmentation by a Block3 Inner-Outer Bracket ModelsWe treat the constraining block as a hidden variablein a generative model shown in Eqn.
2.P (f |e) =?{?
[]}P (f , ?[]|e)=?{?e}?
{?f}P (f , ?f |?e, e)P (?e|e), (2)where ?
[] = (?e, ?f ) is the hidden block.
In the gen-erative process, the model first generates a bracket?e for e with a monolingual bracketing model ofP (?e|e).
It then uses the segmentation of the En-glish (?e, e) to generate the projected bracket ?f of fusing a generative translation model P (f , ?f |?e, e) =P (?f/?, ?f?|?e/?, ?e?)
?
the key model to implement ourproposed inner-outer constraints.
With the hiddenblock ?
[] inferred, the model then generates wordalignments within the inner and outer parts sepa-rately.
We present two generating processes for theinner and outer parts induced by ?
[] and correspond-ing two models of P (f , ?f |?e, e).
These models aredescribed in the following secions.3.1 Inner-Outer Bracket Model-AThe first model assumes that the inner part and theouter part are generated independently.
By the for-mal equivalence of (f, ?f ) with (?f?, ?f/?
), Eqn.
2 canbe approximated as:P (f |e)??{?e}?
{?f}P (?f?|?e?
)P (?f/?|?e/?
)P (?e|e)P (?f |?e),(3)where P (?f?|?e?)
and P (?f/?|?e/?)
are two independentgenerative models for inner and outer parts, respec-178tively and are futher decompsed into:P (?f?|?e?)
=?{aj??e?}?fj?
?f?P (fj |eaj )P (eaj |?e?
)P (?f/?|?e/?)
=?{aj??e/?}?fj?
?f/?P (fj |eaj )P (eaj |?e/?
), (4)where {aJ1 } is the word alignment vector.
Given theblock segmentation and word alignment, the genera-tive process first randomly selects a ei according toeither P (ei|?e?)
or P (ei|?e/?
); and then generates fj in-dexed by word alignment aj with i = aj according toa word level lexicon P (fj |eaj ).
This generative pro-cess using the two models of P (?f?|?e?)
and P (?f/?|?e/?
)must satisfy the constraints of segmentations inducedby the hidden block ?
[] = (?e, ?f ).
The Englishwords ?e?
inside the block can only generate the wordsin ?f?
and nothing else; likewise ?e/?
only generates?f/?.
Overall, the combination of P (?f?|?e?
)P (?f/?|?e/?
)in Eqn.
3 collaborates each other quite well in prac-tice.
For a particular observation ?f?, if ?e?
is toosmall (i.e., missing translations), P (?f?|?e?)
will suf-fer; and if ?e?
is too big (i.e., robbing useful wordsfrom ?e/?
), P (?f/?|?e/?)
will suffer.
Therefore, our pro-posed model in Eqn.
3 combines the two costs andrequires both inner and outer parts to be explainedwell at the same time.Because the model in Eqn.
3 is essentially a two-level (?
[] and a) mixture model similar to IBM Mod-els, the EM algorithm is quite straight forward asin IBM models.
Shown in the following are severalkey E-step computations of the posteriors.
The M-step (optimization) is simply the normalization ofthe fractional counts collected using the posteriorsthrough the inference results from E-step:P?[]?
(aj |?f?, ?e?)
=P (fj |eaj )?ek??e?
P (fj |ek)P?[]/?
(aj |?f/?, ?e/?)
=P (fj |eaj )?ek??e/?
P (fj |ek)(5)The posterior probability of P (aJ1 |f , ?f , ?e, e) =?Jj=1 P (aj |f , ?f , ?e, e), where P (aj |f , ?f , ?e, e) is ei-ther P?[]?
(aj |?f?, ?e?)
when (fj , eaj ) ?
?
[]?, or oth-erwise P?[]/?
(aj |?f/?, ?e/?)
when (fj , eaj ) ?
?[]/?.
As-suming P (?e|e) to be a uniform distribution, theposterior of selecting a hidden block given ob-servations: P (?
[] = (?e, ?f )|e, f) is proportionalto block level relative frequency Prel(?f?|?e?)
up-dated in each iteration; and can be smoothedwith P (?f |?e, f , e) = P (?f?|?e?
)P (?f/?|?e/?)/?{?
?f}P (??f?
|?e?
)P (??f/?
|?e/?)
assuming Model-1 alignment inthe inner and outer parts independently to reducethe risks of data sparseness in estimations.In principle, ?e can be a bracket of any lengthnot exceeding the sentence length.
If we restrict thebracket length to that of the sentence length, we re-cover IBM Model-1.
Figure 2 summarizes the gener-ation process for Inner-Outer Bracket Model-A.f1 f2  f3  f4e1 e2  e3[e1] e2  e3 e1 [e2] e3 [e1 e2] e3 e1 [e2 e3]?.f1 f4e1 e3f2  f3e2f1 f3 f4e1 e3f2e2?
?]3,2[=f?
]2,2[=f?
[.,.]=f?]2,2[=e?]1,1[=e?
]2,1[=e?
]3,2[=e?innerouter innerouterFigure 2: Illustration of generative Bracket Model-A3.2 Inner-Outer Bracket Model-BA block ?
[] invokes both the inner and outer gener-ations simultaneously in Bracket Model A (BM-A).However, the generative process is usually more ef-fective in the inner part as ?
[] is generally small andaccurate.
We can build a model focusing on gener-ating only the inner part with careful inferences toavoid errors from noisy blocks.
To ensure that allfJ1 are generated, we need to propose enough blocksto cover each observation fj .
This constraint can bemet by treating the whole sentence pair as one block.The generative process is as follows: First themodel generates an English bracket ?e as before.
Themodel then generates a projection ?f in f to local-ize all aj ?s for the given ?e according to P (?f |?e, e).
?e and ?f forms a hidden block ?[].
Given ?
[], themodel then generates only the inner part fj ?
?f?
viaP (f |?f , ?e, e) ' P (?f?|?f , ?e, e).
Eqn.
6 summarizesthis by rewriting P (f , ?f |?e, e):P (f , ?f |?e, e) = P (f |?f , ?e, e)P (?f |?e, e) (6)= P (f |?f , ?e, e)P ([jl, jr]|?e, e)' P (?f?|?f , ?e, e)P ([jl, jr]|?e, e).P (?f?|?f , ?e, e) is a bracket level emission proba-bilistic model which generates a bag of contiguouswords fj ?
?f?
under the constraints from the givenhidden block ?
[] = (?f , ?e).
The model is simplifiedin Eqn.
7 with the assumption of bag-of-words?
inde-pendence within the bracket ?f :P (?f?|?f , ?e, e) =?aJ1?j??f?
P (fj |eaj )P (eaj |?f , ?e, e).
(7)179180puting a pair (j, t)?
:(j, t)?
= argmax(j,t)P (fj |et), (11)that is, the point at which the posterior is maximum.The pair (j, t) defines a word pair (fj , et) which isthen aligned.
The procedure continues to find thenext maximum in the posterior matrix.
Contrastthis with Viterbi alignment where one computesf?T1 = argmax{fT1 }P (f1, f2, ?
?
?
, fT |eT1 ), (12)We observe, in parallel corpora, that when oneword translates into multiple words in another lan-guage, it usually translates into a contiguous se-quence of words.
Therefore, we impose a conti-guity constraint on word alignments.
When oneword fj aligns to multiple English words, the En-glish words must be contiguous in e and vice versa.The algorithm to find word alignments using max-posterior with contiguity constraint is illustrated inAlgorithm 1.Algorithm 1 A maximum-posterior algorithm withcontiguity constraint1: while (j, t) = (j, t)?
(as computed in Eqn.
11)do2: if (fj , et) is not yet algned then3: align(fj , et);4: else if (et is contiguous to what fj is aligned)or (fj is contiguous to what et is aligned) then5: align(fj , et);6: end if7: end whileThe algorithm terminates when there isn?t any?next?
posterior maximum to be found.
By defi-nition, there are at most JxT ?next?
maximums inthe posterior matrix.
And because of the contiguityconstraint, not all (fj , et) pairs are valid alignments.The algorithm is sure to terminate.
The algorithmis, in a sense, directionless, for one fj can align tomultiple et?s and vise versa as long as the multipleconnections are contiguous.
Viterbi, however, is di-rectional in which one state can emit multiple obser-vations but one observation can only come from onestate.5 ExperimentsWe evaluate the performances of our proposed mod-els in terms of word alignment accuracy and trans-lation quality.
For word alignment, we have 260hand-aligned sentence pairs with a total of 4676 wordpair links.
The 260 sentence pairs are randomlyselected from the CTTP1 corpus.
They were thenword aligned by eight bilingual speakers.
In this set,we have one-to-one, one-to-many and many-to-manyalignment links.
If a link has one target functionalword, it is considered to be a functional link (Ex-amples of funbctional words are prepositions, deter-miners, etc.
There are in total 87 such functionalwords in our experiments).
We report the overall F-measures as well as F-measures for both content andfunctional word links.
Our significance test showsan overall interval of ?1.56% F-measure at a 95%confidence level.For training data, the small training set has 5000sentence pairs selected from XinHua news storieswith a total of 131K English words and 125K Chi-nese words.
The large training set has 181K sentencepairs (5k+176K); and the additional 176K sentencepairs are from FBIS and Sinorama, which has in to-tal 6.7 million English words and 5.8 million Chinesewords.5.1 Baseline SystemsThe baseline is our implementation of HMM withthe maximum-posterior algorithm introduced in sec-tion 4.
The HMMs are trained unidirectionally.
IBMModel-4 is trained with GIZA++ using the best re-ported settings in (Och and Ney, 2003).
A few pa-rameters, especially the maximum fertility, are tunedfor GIZA++?s optimal performance.
We collect bi-directional (bi) refined word alignment by growingthe intersection of Chinese-to-English (CE) align-ments and English-to-Chinese (EC) alignments withthe neighboring unaligned word pairs which appearin the union similar to the ?final-and?
approaches(Koehn, 2003; Och and Ney, 2003; Tillmann, 2003).Table 1 summarizes our baseline with different set-tings.
Table 1 shows that HMM EC-P gives theF-measure(%) Func Cont BothSmallHMM EC-P 54.69 69.99 64.78HMM EC-V 31.38 53.56 55.59HMM CE-P 51.44 69.35 62.69HMM CE-V 31.43 63.84 55.45LargeHMM EC-P 60.08 78.01 71.92HMM EC-V 32.80 74.10 64.26HMM CE-P 58.45 79.44 71.84HMM CE-V 35.41 79.12 68.33Small GIZA MH-bi 45.63 69.48 60.08GIZA M4-bi 48.80 73.68 63.75Large GIZA MH-bi 49.13 76.51 65.67GIZA M4-bi 52.88 81.76 70.24- Fully-Align 2 5.10 15.84 9.28Table 1: Baseline: V: Viterbi; P: Max-Posterior1LDC2002E17181best baseline, better than bidirectional refined wordalignments from GIZA M4 and the HMM Viterbialigners.5.2 Inner-Outer Bracket ModelsWe trained HMM lexicon P (f |e) to initialize theinner-outer Bracket models.
Afterwards, up to 15?20 EM iterations are carried out.
Iteration startsfrom the fully aligned2 sentence pairs, which give anF-measure of 9.28% at iteration one.5.2.1 Small Data TrackFigure 4 shows the performance of Model-A (BM-A) trained on the small data set.
For each Englishbracket, Top-1 means only the fractional counts fromthe Top-1 projection are collected, Top-all meanscounts from all possible projections are collected.
In-side means the fractional counts are collected fromthe inner part of the block only; and outside meansthey are collected from the outer parts only.
Usingthe Top-1 projection from the inner parts of the block(top-1-inside) gives the best performance: an F-measure of 72.29%, or a 7.5% absolute improvementover the best baseline at iteration 5.
Figure 5 showsBM-A with different settings on small data set626466687072741 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16EM  IterationsF-measure top-1 insidetop-all insidetop alltop-1top-1 outsidetop-all outsideFigure 4: BM-A with different settings on small datathe performance of Inner-Outer Bracket Model-B(BM-B) over EM iterations.
smoothing means whencollecting the fractional counts, we reweigh the up-dated fractional count by 0.95 and give the remain-ing 0.05 weight to original fractional count from thelinks, which were aligned in the previous iteration.w/null means we applied the proposed Null wordmodel in section 3.3 to infer null links.
We also pre-defined a list of 15 English function words, for whichthere might be no corresponding Chinese words astranslations.
These 15 English words are ?a, an, the,of, to, for, by, up, be, been, being, does, do, did, -?.In the drop-null experiments, the links containingthese predefined function words are simply dropped2Every possible word pair is alignedin the final word alignment (this means they are leftunaligned).BM-B with different settings on small data set656769717375771 2 3 4 5 6 7 8EM  IterationsF-measuretop-1 smooth dropnulltop-1 smooth w/nulltop-1 smoothtop 1top allFigure 5: BM-B with different settings on small dataEmpirically we found that doing more than 5 it-erations lead to overfitting.
The peak performancein our model is usually achieved around iteration4?5.
At iteration 5, setting ?BM-B Top-1?
gives anF-measure of 73.93% which is better than BM-A?sbest performance (72.29%).
This is because ModelB leverages a local search for less noisy blocks andhence the inner part is more accurately generated(which in turn means the outer part is also moreaccurate).
From this point on, all of our experi-ments are using Model B.
With smoothing, BM-Bimproves to 74.46%.
After applying the null wordmodel, we get 75.20%.
By simply dropping linkscontaining the 15 English functional words, we get76.24%, which is significantly better than our bestbaseline obtained from even the large training set(HMM EC-P: 71.92%).BM-B with different settings on large data set69717375777981831 2 3 4 5 6 7 8EM  IterationsF-measuretop-1 smooth dropnulltop-1 smooth w/nulltop-1 smoothFigure 6: BM-B with different settings on large data5.2.2 Large Data TrackFigure 6 shows performance pictures of modelBM-B on the large training set.
Without droppingEnglish functional words, the best performance is18280.38% at iteration 4 using the Top-1 projection to-gether with the null word models.
By additionallydropping the links containing the 15 functional En-glish words, we get 81.47%.
These results are allsignificantly better than our strongest baseline sys-tem: 71.92% F-measure using HMM EC-P (70.24%using bidirectional Model-4 for comparisons).On this data set, we experimented with differentmaximum bracket length limits, from one word (un-igram) to nine-gram.
Results show that a maximumbracket length of four is already optimal (79.3% withtop-1 projection), increased from 62.4% when maxi-mum length is limited to one.
No improvements areobserved using longer than five-gram.5.3 Evaluate Blocks in the EM IterationsOur intuition was that good blocks can improve wordalignment and, in turn, good word alignment canlead to better block selection.
The experimental re-sults above support the first claim.
Now we considerthe second claim that good word alignment leads tobetter block selection.Given reference human word alignment, we extractreference blocks up to five-gram phrases on Chinese.The block extraction procedure is based on the pro-cedures in (Tillmann, 2003).During EM, we output all the hidden blocks actu-ally inferred at each iteration, then we evaluate theprecision, recall and F-measure of the hidden blocksaccording to the extracted reference blocks.
The re-sults are shown in Figure 7.
Because we extract all10%15%20%25%30%35%40%45%F-measures1 2 3 4 5 6 7 8EM  IterationsA Direct Eval of blocks' accuracy in 'BM-B top-1 smooth w/null'F-measureRecallPrecisionFigure 7: A Direct Eval.
of Blocks in BM-Bpossible n-grams at each position in e, the precisionis low and the recall is relatively high as shown byFigure 7.
It also shows that blocks do improve, pre-sumably benefiting from better word alignments.Table 2 summarizes word alignment performancesof Inner-Outer BM-B in different settings.
Overall,without the handcrafted function word list, BM-Bgives about 8% absolute improvement in F-measureon the large training set and 9% for the small setF-measure(%) Func Cont BothSmallBaseline 54.69 69.99 64.78BM-B-drop 62.76 82.99 76.24BM-B w/null 61.24 82.54 75.19BM-B smooth 59.61 82.99 74.46LargeBaseline 60.08 78.01 71.92BM-B-drop 63.95 90.09 81.47BM-B w/null 62.24 89.99 80.38BM-B smooth 60.49 90.09 79.31Table 2: BM-B with different settingswith a confidence interval of ?1.56%.5.4 Translation Quality EvaluationsWe also carried out the translation experiments usingthe best settings for Inner-Outer BM-B (i.e.
BM-B-drop) on the TIDES Chinese-English 2003 test set.We trained our models on 354,252 test-specific sen-tence pairs drawn from LDC-supplied parallel cor-pora.
On this training data, we ran 5 iterations ofEM using BM-B to infer word alignments.
A mono-tone decoder similar to (Tillmann and Ney, 2003)with a trigram language model3 is set up for trans-lations.
We report case sensitive Bleu (Papineni etal., 2002) scoreBleuC for all experiments.
The base-line system (HMM ) used phrase pairs built from theHMM-EC-P maximum posterior word alignment andthe corresponding lexicons.
The baseline BleuC scoreis 0.2276 ?
0.015.
If we use the phrase pairs builtfrom the bracket model instead (but keep the HMMtrained lexicons), we get case sensitive BleuC score0.2526.
The improvement is statistically significant.If on the other hand, we use baseline phrase pairswith bracket model lexicons, we get a BleuC score0.2325, which is only a marginal improvement.
If weuse both phrase pairs and lexicons from the bracketmodel, we get a case sensitive BleuC score 0.2750,which is a statistically significant improvement.
Theresults are summarized in Table 3.Settings BleuCBaseline (HMM phrases and lexicon) 0.2276Bracket phrases and HMM lexicon 0.2526Bracket lexicon and HMM phrases 0.2325Bracket (phrases and lexicon) 0.2750Table 3: Improved case sensitive BleuC using BM-BOverall, using Model-B, we improve translationquality from 0.2276 to 0.2750 in case sensitive BleuCscore.3Trained on 1-billion-word ViaVoice English data; thesame data is used to build our True Caser.1836 ConclusionOur main contributions are two novel Inner-OuterBracket models based on segmentations induced byhidden blocks.
Modeling the Inner-Outer hidden seg-mentations, we get significantly improved word align-ments for both the small training set and the largetraining set over the widely-practiced bidirectionalIBM Model-4 alignment.
We also show significantimprovements in translation quality using our pro-posed bracket models.
Robustness to noisy blocksmerits further investigation.7 AcknowledgementThis work is supported by DARPA under contractnumber N66001-99-28916.ReferencesP.F.
Brown, Stephen A. Della Pietra, Vincent.
J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation:Parameter estimation.
In Computational Linguis-tics, volume 19(2), pages 263?331.Niyu Ge.
2004.
A maximum posterior methodfor word alignment.
In Presentation given atDARPA/TIDES MT workshop.J.X.
Huang, W.Wang, and M. Zhou.
2003.
A unifiedstatistical model for generalized translation mem-ory system.
In Machine Translation Summit IX,pages 173?180, New Orleans, USA, September 23-27.Philipp Koehn and Kevin Knight.
2002.
Chunkmt:Statistical machine translation with richer linguis-tic knowledge.
Draft, Unpublished.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based machine transla-tion.
In Proc.
of HLT-NAACL 2003, pages 48?54,Edmonton, Canada, May-June.Philipp Koehn.
2003.
Noun phrase translation.
InPh.D.
Thesis, University of Southern California,ISI.Daniel Marcu and William Wong.
2002.
A phrase-based, joint probability model for statistical ma-chine translation.
In Proc.
of the Conference onEmpirical Methods in Natural Language Process-ing, pages 133?139, Philadelphia, PA, July 6-7.Franz J. Och and Hermann Ney.
2002.
Discrimi-native training and maximum entropy models forstatistical machine translation.
In Proceedings ofthe 40th Annual Meeting of ACL, pages 440?447.Franz J. Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignmentmodels.
In Computational Linguistics, volume 29,pages 19?51.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
Bleu: a method for auto-matic evaluation of machine translation.
In Proc.of the 40th Annual Conf.
of the ACL (ACL 02),pages 311?318, Philadelphia, PA, July.Christoph Tillmann and Hermann Ney.
2003.
Wordreordering and a dp beam search algorithm forstatistical machine translation.
In ComputationalLinguistics, volume 29(1), pages 97?133.Christoph Tillmann.
2003.
A projection extensionalgorithm for statistical machine translation.
InProc.
of the Conference on Empirical Methods inNatural Language Processing.Kristina Toutanova, H. Tolga Ilhan, and Christo-pher D. Manning.
2002.
Extensions to hmm-basedstatistical word alignment models.
In Proc.
of theConference on Empirical Methods in Natural Lan-guage Processing, Philadelphia, PA, July 6-7.S.
Vogel, Hermann Ney, and C. Tillmann.
1996.Hmm based word alignment in statistical machinetranslation.
In Proc.
The 16th Int.
Conf.
on Com-putational Lingustics, (Coling?96), pages 836?841,Copenhagen, Denmark.Taro Watanabe, Kenji Imamura, and EiichiroSumita.
2002.
Statistical machine translationbased on hierarchical phrases.
In 9th InternationalConference on Theoretical and Methodological Is-sues, pages 188?198, Keihanna, Japan, March.Taro Watanabe, Eiichiro Sumita, and Hiroshi G.Okuno.
2003.
Chunk-based statistical transla-tion.
In In 41st Annual Meeting of the ACL (ACL2003), pages 303?310, Sapporo, Japan.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
In Computational Linguistics, volume 23(3),pages 377?403.K.
Yamada and Kevin.
Knight.
2001.
Syntax-basedstatistical translation model.
In Proceedings of theConference of the Association for ComputationalLinguistics (ACL-2001).R.
Zens and H. Ney.
2004.
Improvements in phrase-based statistical machine translation.
In Pro-ceedings of the Human Language Technology Con-ference (HLT-NAACL)s, pages 257?264, Boston,MA, May.184
