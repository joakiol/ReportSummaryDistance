Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 917?926,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsCross-Language Document Summarization Based on MachineTranslation Quality PredictionXiaojun Wan, Huiying Li and Jianguo XiaoInstitute of Compute Science and Technology, Peking University, Beijing 100871, ChinaKey Laboratory of Computational Linguistics (Peking University), MOE, China{wanxiaojun,lihuiying,xiaojianguo}@icst.pku.edu.cnAbstractCross-language document summarization is atask of producing a summary in one languagefor a document set in a different language.
Ex-isting methods simply use machine translationfor document translation or summary transla-tion.
However, current machine translationservices are far from satisfactory, which re-sults in that the quality of the cross-languagesummary is usually very poor, both in read-ability and content.
In this paper, we proposeto consider the translation quality of each sen-tence in the English-to-Chinese cross-languagesummarization process.
First, the translationquality of each English sentence in the docu-ment set is predicted with the SVM regressionmethod, and then the quality score of each sen-tence is incorporated into the summarizationprocess.
Finally, the English sentences withhigh translation quality and high informative-ness are selected and translated to form theChinese summary.
Experimental results dem-onstrate the effectiveness and usefulness of theproposed approach.1 IntroductionGiven a document or document set in one sourcelanguage, cross-language document summariza-tion aims to produce a summary in a differenttarget language.
In this study, we focus on Eng-lish-to-Chinese document summarization for thepurpose of helping Chinese readers to quicklyunderstand the major content of an English docu-ment or document set.
This task is very impor-tant in the field of multilingual information ac-cess.Till now, most previous work focuses onmonolingual document summarization, butcross-language document summarization has re-ceived little attention in the past years.
Astraightforward way for cross-language docu-ment summarization is to translate the summaryfrom the source language to the target languageby using machine translation services.
However,though machine translation techniques have beenadvanced a lot, the machine translation quality isfar from satisfactory, and in many cases, thetranslated texts are hard to understand.
Therefore,the translated summary is likely to be hard tounderstand by readers, i.e., the summary qualityis likely to be very poor.
For example, the trans-lated Chinese sentence for an ordinary Englishsentence (?It is also Mr Baker who is making themost of presidential powers to dispense lar-gesse.?)
by using Google Translate is ????????????????????????
?.The translated sentence is hard to understandbecause it contains incorrect translations and it isvery disfluent.
If such sentences are selected intothe summary, the quality of the summary wouldbe very poor.In order to address the above problem, wepropose to consider the translation quality of theEnglish sentences in the summarization process.In particular, the translation quality of each Eng-lish sentence is predicted by using the SVM re-gression method, and then the predicted MTquality score of each sentence is incorporatedinto the sentence evaluation process, and finallyboth informative and easy-to-translate sentencesare selected and translated to form the Chinesesummary.An empirical evaluation is conducted to evalu-ate the performance of machine translation qual-ity prediction, and a user study is performed toevaluate the cross-language summary quality.The results demonstrate the effectiveness of theproposed approach.The rest of this paper is organized as follows:Section 2 introduces related work.
The system isoverviewed in Section 3.
In Sections 4 and 5, wepresent the detailed algorithms and evaluation917results of machine translation quality predictionand cross-language summarization, respectively.We discuss in Section 6 and conclude this paperin Section 7.2 Related Work2.1 Machine Translation Quality PredictionMachine translation evaluation aims to assess thecorrectness and quality of the translation.
Usu-ally, the human reference translation is provided,and various methods and metrics have been de-veloped for comparing the system-translated textand the human reference text.
For example, theBLEU metric, the NIST metric and their relativesare all based on the idea that the more sharedsubstrings the system-translated text has with thehuman reference translation, the better the trans-lation is.
Blatz et al (2003) investigate trainingsentence-level confidence measures using a vari-ety of fuzzy match scores.
Albrecht and Hwa(2007) rely on regression algorithms and refer-ence-based features to measure the quality ofsentences.Transition evaluation without using referencetranslations has also been investigated.
Quirk(2004) presents a supervised method for traininga sentence level confidence measure on transla-tion output using a human-annotated corpus.Features derived from the source sentence andthe target sentence (e.g.
sentence length, perplex-ity, etc.)
and features about the translation proc-ess are leveraged.
Gamon et al (2005) investi-gate the possibility of evaluating MT quality andfluency at the sentence level in the absence ofreference translations, and they can improve onthe correlation between language model perplex-ity scores and human judgment by combing theseperplexity scores with class probabilities from amachine-learned classifier.
Specia et al (2009)use the ICM theory to identify the threshold tomap a continuous predicted score into ?good?
or?bad?
categories.
Chae and Nenkova (2009) usesurface syntactic features to assess the fluency ofmachine translation results.In this study, we further predict the translationquality of an English sentence before the ma-chine translation process, i.e., we do not leveragereference translation and the target sentence.2.2 Document SummarizationDocument summarization methods can be gener-ally categorized into extraction-based methodsand abstraction-based methods.
In this paper, wefocus on extraction-based methods.
Extraction-based summarization methods usually assigneach sentence a saliency score and then rank thesentences in a document or document set.For single document summarization, the sen-tence score is usually computed by empiricalcombination of a number of statistical and lin-guistic feature values, such as term frequency,sentence position, cue words, stigma words,topic signature (Luhn 1969; Lin and Hovy, 2000).The summary sentences can also be selected byusing machine learning methods (Kupiec et al,1995; Amini and Gallinari, 2002) or graph-basedmethods (ErKan and Radev, 2004; Mihalcea andTarau, 2004).
Other methods include mutual re-inforcement principle (Zha 2002; Wan et al,2007).For multi-document summarization, the cen-troid-based method (Radev et al, 2004) is a typi-cal method, and it scores sentences based oncluster centroids, position and TFIDF features.NeATS (Lin and Hovy, 2002) makes use of newfeatures such as topic signature to select impor-tant sentences.
Machine Learning based ap-proaches have also been proposed for combiningvarious sentence features (Wong et al, 2008).The influences of input difficulty on summariza-tion performance have been investigated in(Nenkova and Louis, 2008).
Graph-based meth-ods have also been used to rank sentences in adocument set.
For example, Mihalcea and Tarau(2005) extend the TextRank algorithm to com-pute sentence importance in a document set.Cluster-level information has been incorporatedin the graph model to better evaluate sentences(Wan and Yang, 2008).
Topic-focused or querybiased multi-document summarization has alsobeen investigated (Wan et al, 2006).
Wan et al(2010) propose the EUSUM system for extract-ing easy-to-understand English summaries fornon-native readers.Several pilot studies have been performed forthe cross-language summarization task by simplyusing document translation or summary transla-tion.
Leuski et al (2003) use machine translationfor English headline generation for Hindi docu-ments.
Lim et al (2004) propose to generate aJapanese summary without using a Japanesesummarization system, by first translating Japa-nese documents into Korean documents, andthen extracting summary sentences by using Ko-rean summarizer, and finally mapping Koreansummary sentences to Japanese summary sen-tences.
Chalendar et al (2005) focuses on se-mantic analysis and sentence generation tech-niques for cross-language summarization.
Orasan918and Chiorean (2008) propose to produce summa-ries with the MMR method from Romanian newsarticles and then automatically translate thesummaries into English.
Cross language querybased summarization has been investigated in(Pingali et al, 2007), where the query and thedocuments are in different languages.
Other re-lated work includes multilingual summarization(Lin et al, 2005), which aims to create summa-ries from multiple sources in multiple languages.Siddharthan and McKeown (2005) use the in-formation redundancy in multilingual input tocorrect errors in machine translation and thusimprove the quality of multilingual summaries.3 The Proposed ApproachPrevious methods for cross-language summariza-tion usually consist of two steps: one step forsummarization and one step for translation.
Dif-ferent order of the two steps can lead to the fol-lowing two basic English-to-Chinese summariza-tion methods:Late Translation (LateTrans): Firstly, anEnglish summary is produced for the Englishdocument set by using existing summarizationmethods.
Then, the English summary is auto-matically translated into the corresponding Chi-nese summary by using machine translation ser-vices.Early Translation (EarlyTrans): Firstly, theEnglish documents are translated into Chinesedocuments by using machine translation services.Then, a Chinese summary is produced for thetranslated Chinese documents.Generally speaking, the LateTrans method hasa few advantages over the EarlyTrans method:1) The LateTrans method is much more effi-cient than the EarlyTrans method, because only avery few summary sentences are required to betranslated in the LateTrans method, whereas allthe sentences in the documents are required to betranslated in the EarlyTrans method.2)  The LateTrans method is deemed to bemore effective than the EarlyTrans method, be-cause the translation errors of the sentences havegreat influences on the summary sentence extrac-tion in the EarlyTrans method.Thus in this study, we adopt the LateTransmethod as our baseline method.
We also adoptthe late translation strategy for our proposed ap-proach.In the baseline method, a translated Chinesesentence is selected into the summary becausethe original English sentence is informative.However, an informative and fluent English sen-tence is likely to be translated into an uninforma-tive and disfluent Chinese sentence, and there-fore, this sentence cannot be selected into thesummary.In order to address the above problem of exist-ing methods, our proposed approach takes intoaccount a novel factor of each sentence for cross-language summary extraction.
Each English sen-tence is associated with a score indicating itstranslation quality.
An English sentence withhigh translation quality score is more likely to beselected into the original English summary, andsuch English summary can be translated into abetter Chinese summary.
Figure 1 gives the ar-chitecture of our proposed approach.Figure 1: Architecture of our proposed ap-proachSeen from the figure, our proposed approachconsists of four main steps: 1) The machinetranslation quality score of each English sentenceis predicted by using regression methods; 2) Theinformativeness score of each English sentence iscomputed by using existing methods; 3) TheEnglish summary is produced by making use ofboth the machine translation quality score andthe informativeness score; 4) The extracted Eng-lish summary is translated into Chinese summaryby using machine translation services.In this study, we adopt Google Translate1 forEnglish-to-Chinese translation.
Google Translateis one of the state-of-the-art commercial machinetranslation systems used today.
It applies statisti-cal learning techniques to build a translation1 http://translate.google.com/translate_tEnglishSentencesSentenceMT QualityPredictionSentenceInformativenessEvaluationEnglishSummaryExtractionEN-to-CNMachineTranslationChinese SummaryInformativeness scoreEnglish summaryMT quality score919model based on both monolingual text in the tar-get language and aligned text consisting of ex-amples of human translations between the lan-guages.The first step and the evaluation results will bedescribed in Section 4, and the other steps andthe evaluation results will be described togetherin Section 5.4 Machine Translation Quality Predic-tion4.1 MethodologyIn this study, machine translation (MT) qualityreflects both the translation accuracy and the flu-ency of the translated sentence.
An English sen-tence with high MT quality score is likely to betranslated into an accurate and fluent Chinesesentence, which can be easily read and under-stand by Chinese readers.
The MT quality pre-diction is a task of mapping an English sentenceto a numerical value corresponding to a qualitylevel.
The larger the value is, the more accuratelyand fluently the sentence can be translated intoChinese sentence.As introduced in Section 2.1, several relatedwork has used regression and classificationmethods for MT quality prediction without refer-ence translations.
In our approach, the MT qual-ity of each sentence in the documents is also pre-dicted without reference translations.
The differ-ence between our task and previous work is thatprevious work can make use of both features insource sentence and features in target sentence,while our task only leverages features in sourcesentence, because in the late translation strategy,the English sentences in the documents have notbeen translated yet at this step.In this study, we adopt the ?-support vector re-gression (?-SVR) method (Vapnik 1995) for thesentence-level MT quality prediction task.
TheSVR algorithm is firmly grounded in the frame-work of statistical learning theory (VC theory).The goal of a regression algorithm is to fit a flatfunction to the given training data points.Formally, given a set of training data pointsD={(xi,yi)| i=1,2,?,n} ?
Rd?R,  where xi is inputfeature vector and yi is associated score, the goalis to fit a function f which approximates the rela-tion inherited between the data set points.
Thestandard form is:?
?==++niiniiTbwCCww1*1,,, 21  min*???
?Subject toiiiT ybxfw ??
+?
?+)(*)( iiTi bxfwy ??
+??
?.,...,1  ,0,, * niii =???
?The constant C>0 is a parameter for determin-ing the trade-off between the flatness of f and theamount up to which deviations larger than ?
aretolerated.In the experiments, we use the LIBSVM tool(Chang and Lin, 2001) with the RBF kernel forthe task, and we use the parameter selection toolof 10-fold cross validation via grid search to findthe best parameters on the training set with re-spect to mean squared error (MSE), and then usethe best parameters to train on the whole trainingset.We use the following two groups of featuresfor each sentence: the first group includes severalbasic features, and the second group includesseveral parse based features2.
They are all de-rived based on the source English sentence.The basic features are as follows:1) Sentence length:  It refers to the number ofwords in the sentence.2) Sub-sentence number: It refers to the num-ber of sub-sentences in the sentence.
Wesimply use the punctuation marks as indica-tors of sub-sentences.3) Average sub-sentence length: It refers tothe average number of words in the sub-sentences within the sentence.4) Percentage of nouns and adjectives: It re-fers to the percentage of noun words or ad-jective words in the in the sentence.5) Number of question words: It refers to thenumber of question words (who, whom,whose, when, where, which, how, why, what)in the sentence.We use the Stanford Lexicalized Parser (Kleinand Manning, 2002) with the provided EnglishPCFG model to parse a sentence into a parse tree.The output tree is a context-free phrase structuregrammar representation of the sentence.
Theparse features are then selected as follows:1) Depth of the parse tree:  It refers to thedepth of the generated parse tree.2) Number of SBARs in the parse tree:SBAR is defined as a clause introduced by a(possibly empty) subordinating conjunction.It is an indictor of sentence complexity.2  Other features, including n-gram frequency, perplexityfeatures, etc., are not useful in our study.
MT features arenot used because Google Translate is used as a black box.9203) Number of NPs in the parse tree:  It refersto the number of noun phrases in the parsetree.4) Number of VPs in the parse tree:  It refersto the number of verb phrases in the parsetree.All the above feature values are scaled by us-ing the provided svm-scale program.At this step, each English sentence si can beassociated with a MT quality score TransScore(si)predicted by the ?-SVR method.
The score is fi-nally normalized by dividing by the maximumscore.4.2 Evaluation4.2.1 Evaluation SetupIn the experiments, we first constructed the gold-standard dataset in the following way:DUC2001 provided 309 English news articlesfor document summarization tasks, and the arti-cles were grouped into 30 document sets.
Thenews articles were selected from TREC-9.
Wechose five document sets (d04, d05, d06, d08,d11) with 54 news articles out of the DUC2001document sets.
The documents were then splitinto sentences and we used 1736 sentences forevaluation.
All the sentences were automaticallytranslated into Chinese sentences by using theGoogle Translate service.Two Chinese college students were employedfor data annotation.
They read the original Eng-lish sentence and the translated Chinese sentence,and then manually labeled the overall translationquality score for each sentence, separately.
Thetranslation quality is an overall measure for boththe translation accuracy and the readability of thetranslated sentence.
The score ranges between 1and 5, and 1 means ?very bad?, and 5 means?very good?, and 3 means ?normal?.
The correla-tion between the two sets of labeled scores is0.646.
The final translation quality score was theaverage of the scores provided by the two anno-tators.After annotation, we randomly separated thelabeled sentence set into a training set of 1428sentences and a test set of 308 sentences.
Wethen used the LIBSVM tool for training and test-ing.Two metrics were used for evaluating the pre-diction results.
The two metrics are as follows:Mean Square Error (MSE): This metric is ameasure of how correct each of the predictionvalues is on average, penalizing more severe er-rors more heavily.
Given the set of predictionscores for the test sentences: },...1|?{?
niyY i == , andthe manually assigned scores for the sentences:},...1|{ niyY i == , the MSE of the prediction resultis defined as?=?=niii yynYMSE12)?(1)?
(Pearson?s Correlation Coefficient (?
):  Thismetric is a measure of whether the trends of pre-diction values matched the trends for human-labeled data.
The coefficient between Y and Y?
isdefined asyyniiisnsyyyy?1)??)((?=?
?=?where y and y?
are the sample means of Y andY?
, ys and ys ?
are the sample standard deviationsof Y and Y?
.4.2.2 Evaluation ResultsTable 1 shows the prediction results.
We can seethat the overall results are promising.
And thecorrelation is moderately high.
The results areacceptable because we only make use of the fea-tures derived from the source sentence.
The re-sults guarantee that the use of MT quality scoresin the summarization process is feasible.We can also see that both the basic featuresand the parse features are beneficial to the over-all prediction results.Feature Set MSE ?Basic features 0.709 0.399Parse features 0.702 0.395All features 0.683 0.433Table 1: Prediction results5 Cross-Language Document Summari-zation5.1 MethodologyIn this section, we first compute the informative-ness score for each sentence.
The score reflecthow the sentence expresses the major topic in thedocuments.
Various existing methods can beused for computing the score.
In this study, weadopt the centroid-based method.The centroid-based method is the algorithmused in the MEAD system.
The method uses aheuristic and simple way to sum the sentencescores computed based on different features.
Thescore for each sentence is a linear combination of921the weights computed based on the followingthree features:Centroid-based Weight.
The sentences closeto the centroid of the document set are usuallymore important than the sentences farther away.And the centroid weight C(si) of a sentence si iscalculated as the cosine similarity  between thesentence text and the concatenated text for thewhole document set D. The weight is then nor-malized by dividing the maximal weight.Sentence Position.
The leading several sen-tences of a document are usually important.
Sowe calculate for each sentence a weight to reflectits position priority as P(si)=1-(i-1)/n, where i isthe sequence of the sentence si and n is the totalnumber of sentences in the document.
Obviously,i ranges from 1 to n.First Sentence Similarity.
Because the firstsentence of a document is very important, a sen-tence similar to the first sentence is also impor-tant.
Thus we use the cosine similarity value be-tween a sentence and the corresponding first sen-tence in the same document as the weight F(si)for sentence si.After all the above weights are calculated foreach sentence, we sum all the weights and get theoverall score for the sentence as follows:)()()()( iiii sFsPsCsInfoScore ?+?+?= ??
?where ?, ?
and ?
are parameters reflecting theimportance of different features.
We empiricallyset ?=?=?=1.After the informativeness scores for all sen-tences are computed, the score of each sentenceis normalized by dividing by the maximum score.After we obtain the MT quality score and theinformativeness score of each sentence in thedocument set, we linearly combine the twoscores to get the overall score of each sentence.Formally, let TransScore(si)?
[0,1] and Info-Score(si)?
[0,1] denote the MT quality score andthe informativeness score of sentence si, theoverall score of the sentence is:where ??
[0,1] is a parameter controlling theinfluences of the two factors.
If ?
is set to 0, thesummary is extracted without considering theMT quality factor.
In the experiments, we em-pirically set the parameter to 0.3 in order to bal-ance the two factors of content informativenessand translation quality.For multi-document summarization, some sen-tences are highly overlapping with each other,and thus we apply the same greedy algorithm in(Wan et al, 2006) to penalize the sentenceshighly overlapping with other highly scored sen-tences, and finally the informative, novel, andeasy-to-translate sentences are chosen into theEnglish summary.Finally, the sentences in the English summaryare translated into the corresponding Chinesesentences by using Google Translate, and theChinese summary is formed.5.2 Evaluation5.2.1 Evaluation SetupIn this experiment, we used the document setsprovided by DUC2001 for evaluation.
As men-tioned in Section 4.2.1, DUC2001 provided 30English document sets for generic multi-document summarization.
The average documentnumber per document set was 10.
The sentencesin each article have been separated and the sen-tence information has been stored into files.
Ge-neric reference English summaries were pro-vided by NIST annotators for evaluation.
In ourstudy, we aimed to produce Chinese summariesfor the English document sets.
The summarylength was limited to five sentences, i.e.
eachsummary consisted of five sentences.The DUC2001 dataset was divided into thefollowing two datasets:Ideal Dataset: We have manually labeled theMT quality scores for the sentences in fivedocument sets (d04-d11), and we directly usedthe manually labeled scores in the summarizationprocess.
The ideal dataset contained these fivedocument sets.Real Dataset: The MT quality scores for thesentences in the remaining 25 document setswere automatically predicted by using thelearned SVM regression model.
And we used theautomatically predicted scores in the summariza-tion process.
The real dataset contained these 25document sets.We performed two evaluation procedures: onebased on the ideal dataset to validate thefeasibility of the proposed approach, andthe other based on the real dataset todemonstrate the effectiveness of the proposedapproach in real applications.To date, various methods and metrics havebeen developed for English summary evaluationby comparing system summary with referencesummary, such as the pyramid method (Nenkovaet al, 2007) and the ROUGE metrics (Lin andHovy, 2003).
However, such methods or metricscannot be directly used for evaluating Chinesesummary without reference Chinese summary.
)()()1()( iii sTransScoresInfoScoresreOverallSco ?+?
?= ?
?922Instead, we developed an evaluation protocol asfollows:The evaluation was based on human scoring.Four Chinese college students participated in theevaluation as subjects.
We have developed afriendly tool for helping the subjects to evaluateeach Chinese summary from the following threeaspects:Content: This aspect indicates how much asummary reflects the major content of the docu-ment set.
After reading a summary, each user canselect a score between 1 and 5 for the summary.1 means ?very uninformative?
and 5 means?very informative?.Readability:  This aspect indicates the read-ability level of the whole summary.
After readinga summary, each user can select a score between1 and 5 for the summary.
1 means ?hard to read?,and 5 means ?easy to read?.Overall:  This aspect indicates the overallquality of a summary.
After reading a summary,each user can select a score between 1 and 5 forthe summary.
1 means ?very bad?, and 5 means?very good?.We performed the evaluation procedures onthe ideal dataset and the read dataset, separately.During each evaluation procedure, we comparedour proposed approach (?=0.3) with the baselineapproach without considering the MT qualityfactor (?=0).
And the two summaries producedby the two systems for the same document setwere presented in the same interface, and thenthe four subjects assigned scores to each sum-mary after they read and compared the twosummaries.
And the assigned scores were finallyaveraged across the documents sets and acrossthe subjects.5.2.2 Evaluation ResultsTable 2 shows the evaluation results on the idealdataset with 5 document sets.
We can see thatbased on the manually labeled MT quality scores,the Chinese summaries produced by our pro-posed approach are significantly better than thatproduced by the baseline approach over all threeaspects.
All subjects agree that our proposed ap-proach can produce more informative and easy-to-read Chinese summaries than the baseline ap-proach.Table 3 shows the evaluation results on thereal dataset with 25 document sets.
We can seethat based on the automatically predicted MTquality scores, the Chinese summaries producedby our proposed approach are significantly betterthan that produced by the baseline approach overthe readability aspect and the overall aspect.
Al-most all subjects agree that our proposed ap-proach can produce more easy-to-read and high-quality Chinese summaries than the baseline ap-proach.Comparing the evaluation results in the twotables, we can find that the performance differ-ence between the two approaches on the idealdataset is bigger than that on the real dataset, es-pecially on the content aspect.
The results dem-onstrate that the more accurate the MT qualityscores are, the more significant the performanceimprovement is.Overall, the proposed approach is effective toproduce good-quality Chinese summaries forEnglish document sets.Baseline Approach Proposed Approachcontent readability overall content readability overallSubject1 3.2 2.6 2.8 3.4 3.0 3.4Subject2 3.0 3.2 3.2 3.4 3.6 3.4Subject3 3.4 2.8 3.2 3.6 3.8 3.8Subject4 3.2 3.0 3.2 3.8 3.8 3.8Average 3.2 2.9 3.1 3.55* 3.55* 3.6*Table 2: Evaluation results on the ideal dataset (5 document sets)Baseline Approach Proposed Approachcontent readability overall content readability overallSubject1 2.64 2.56 2.60 2.80 3.24 2.96Subject2 3.60 2.76 3.36 3.52 3.28 3.64Subject3 3.52 3.72 3.44 3.56 3.80 3.48Subject4 3.16 2.96 3.12 3.16 3.44 3.52Average 3.23 3.00 3.13 3.26 3.44* 3.40*Table 3: Evaluation results on the real dataset (25 document sets)(* indicates the difference between the average score of the proposed approach and that of the baseline approachis statistically significant by using t-test.
)9235.2.3 Example AnalysisIn this section, we give two running examples tobetter show the effectiveness of our proposedapproach.
The Chinese sentences and the originalEnglish sentences in the summary are presentedtogether.
The normalized MT quality score foreach sentence is also given at the end of the Chi-nese sentence.Document set 1: D04 from the ideal datasetSummary by baseline approach:s1: ?????????????????????73????37???????????????-?????????????????
(0.56)(US INSURERS expect to pay out an estimated Dollars 7.3bn(Pounds 3.7bn) in Florida as a result of Hurricane Andrew - by farthe costliest disaster the industry has ever faced.
)s2: ?????????????????????????????????????????????????????????????
(0.67)(THERE are growing signs that Hurricane Andrew, unwelcome asit was for the devastated inhabitants of Florida and Louisiana, mayin the end do no harm to the re-election campaign of PresidentGeorge Bush.
)s3: ????????????????????????????????????????4000???&#39;?
(0.44)(GENERAL ACCIDENT said yesterday that insurance claimsarising from Hurricane Andrew could 'cost it as much as Dollars40m'.
)s4: ???????????????4?????????????
(0.56)(In the Bahamas, government spokesman Mr Jimmy Curry saidfour deaths had been reported on outlying eastern islands.
)s5: ??????1.6????????????????????????????????????????????????
(0.44)(New Orleans, with a population of 1.6m, is particularly vulnerablebecause the city lies below sea level, has the Mississippi Riverrunning through its centre and a large lake immediately to the north.
)Summary by proposed approach:s1: ?????????????????????73????37???????????????-?????????????????
(0.56)(US INSURERS expect to pay out an estimated Dollars 7.3bn(Pounds 3.7bn) in Florida as a result of Hurricane Andrew - by farthe costliest disaster the industry has ever faced.
)s2: ?????????????????????????????????????????????????????????????
(0.67)(THERE are growing signs that Hurricane Andrew, unwelcome asit was for the devastated inhabitants of Florida and Louisiana, mayin the end do no harm to the re-election campaign of PresidentGeorge Bush.
)s3: ???????????????4?????????????
(0.56)(In the Bahamas, government spokesman Mr Jimmy Curry saidfour deaths had been reported on outlying eastern islands.
)s4: ????????????????????????????????
(0.89)(The brunt of the losses are likely to be concentrated among USinsurers, industry analysts said yesterday.
)s5: ?????????????
(1.0)(In north Miami, damage is minimal.
)Document set 2: D54 from the real datasetSummary by baseline approach:s1: ????11?6??????????????????????????
(0.57)(Two propositions on California's Nov. 6 ballot would, among otherthings, limit the terms of statewide officeholders and state legisla-tors.
)s2: ??????????????????????????????????
(0.36)(One reason is that term limits would open up politics to manypeople now excluded from office by career incumbents.
)s3: ???????????????????????????????????
(0.20)(Proposals to limit the terms of members of Congress and of statelegislators are popular and getting more so, according to the punditsand the polls.
)s4: ??????????????????????????????????????????
(0.24)(State statutes that bar first-time candidates from running for Con-gress have been held to add to the qualifications set forth in theConstitution and have been invalidated.
)s5: ??????????????????????????????????????????????????
(0.20)(Another argument is that a citizen Congress with its continuingflow of fresh faces into Washington would result in better govern-ment than that provided by representatives with lengthy tenure.
)Summary by proposed approach:s1: ????
11?
6??????????????????????????
(0.57)(Two propositions on California's Nov. 6 ballot would, among otherthings, limit the terms of statewide officeholders and state legisla-tors.
)s2: ??????????????????????????????????
(0.36)(One reason is that term limits would open up politics to manypeople now excluded from office by career incumbents.
)s3: ??????????????????????????????????????????????????
(0.20)(Another argument is that a citizen Congress with its continuingflow of fresh faces into Washington would result in better govern-ment than that provided by representatives with lengthy tenure.
)s4: ?????????????????????????????????????
(0.39)(There are two solid reasons for congressional term limitation thateconomists, at least those of the public-choice persuasion, shouldfully appreciate.
)s5: ??????????????????????????????
(0.47)(The root of the problems with Congress is that, barring majorscandal, it is almost impossible to defeat an incumbent.
)6 DiscussionIn this study, we adopt the late translation strat-egy for cross-document summarization.
As men-tioned earlier, the late translation strategy hassome advantages over the early translation strat-egy.
However, in the early translation strategy,we can use the features derived from both thesource English sentence and the target Chinesesentence to improve the MT quality predictionresults.Overall, the framework of our proposed ap-proach can be easily adapted for cross-documentsummarization with the early translation strategy.924And an empirical comparison between the twostrategies is left as our future work.Though this study focuses on English-to-Chinese document summarization, cross-language summarization tasks for other lan-guages can also be solved by using our proposedapproach.7 Conclusion and Future WorkIn this study we propose a novel approach to ad-dress the cross-language document summariza-tion task.
Our proposed approach predicts theMT quality score of each English sentence andthen incorporates the score into the summariza-tion process.
The user study results verify theeffectiveness of the approach.In future work, we will manually translateEnglish reference summaries into Chinese refer-ence summaries, and then adopt the ROUGEmetrics to perform automatic evaluation of theextracted Chinese summaries by comparing themwith the Chinese reference summaries.
Moreover,we will further improve the sentence?s MT qual-ity by using sentence compression or sentencereduction techniques.AcknowledgmentsThis work was supported by NSFC (60873155),Beijing Nova Program (2008B03), NCET(NCET-08-0006), RFDP (20070001059) andNational High-tech R&D Program(2008AA01Z421).
We thank the students forparticipating in the user study.
We also thank theanonymous reviewers for their useful comments.ReferencesJ.
Albrecht and R. Hwa.
2007.
A re-examination ofmachine learning approaches for sentence-level mtevaluation.
In Proceedings of ACL2007.M.
R. Amini, P. Gallinari.
2002.
The Use of Unla-beled Data to Improve Supervised Learning forText Summarization.
In Proceedings of SIGIR2002.J.
Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C.Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.2003.
Confidence estimation for statistical machinetranslation.
Johns Hopkins Summer Workshop Fi-nal Report.J.
Chae and A. Nenkova.
2009.
Predicting the fluencyof text with shallow structural features: case studiesof machine translation and human-written text.
InProceedings of EACL2009.G.
de Chalendar, R. Besan?on, O. Ferret, G. Grefen-stette, and O. Mesnard.
2005.
Crosslingual summa-rization with thematic extraction, syntactic sen-tence simplification, and bilingual generation.
InWorkshop on Crossing Barriers in Text Summari-zation Research, 5th International Conference onRecent Advances in Natural Language Processing(RANLP2005).C.-C. Chang and C.-J.
Lin.
2001.
LIBSVM : a libraryfor support vector machines.
Software available athttp://www.csie.ntu.edu.tw/~cjlin/libsvmG.
ErKan, D. R. Radev.
LexPageRank.
2004.
Prestigein Multi-Document Text Summarization.
In Pro-ceedings of EMNLP2004.M.
Gamon, A. Aue, and M. Smets.
2005.
Sentence-level MT evaluation without reference translations:beyond language modeling.
In Proceedings ofEAMT2005.D.
Klein and C. D. Manning.
2002.
Fast Exact Infer-ence with a Factored Model for Natural LanguageParsing.
In Proceedings of NIPS2002.J.
Kupiec, J. Pedersen, F. Chen.
1995.
A.TrainableDocument Summarizer.
In Proceedings ofSIGIR1995.A.
Leuski, C.-Y.
Lin, L. Zhou, U. Germann, F. J. Och,E.
Hovy.
2003.
Cross-lingual C*ST*RD: Englishaccess to Hindi information.
ACM Transactions onAsian Language Information Processing, 2(3):245-269.J.-M. Lim, I.-S. Kang, J.-H. Lee.
2004.
Multi-document summarization using cross-languagetexts.
In Proceedings of NTCIR-4.C.
Y. Lin, E. Hovy.
2000.
The Automated Acquisitionof Topic Signatures for Text Summarization.
InProceedings of the 17th Conference on Computa-tional Linguistics.C..-Y. Lin and E.. H. Hovy.
2002.
From Single toMulti-document Summarization: A Prototype Sys-tem and its Evaluation.
In Proceedings of ACL-02.C.-Y.
Lin and E.H. Hovy.
2003.
Automatic Evalua-tion of Summaries Using N-gram Co-occurrenceStatistics.
In Proceedings of HLT-NAACL -03.C.-Y.
Lin, L. Zhou, and E. Hovy.
2005.
Multilingualsummarization evaluation 2005: automatic evalua-tion report.
In Proceedings of MSE (ACL-2005Workshop).H.
P. Luhn.
1969.
The Automatic Creation of litera-ture Abstracts.
IBM Journal of Research and De-velopment, 2(2).R.
Mihalcea, P. Tarau.
2004.
TextRank: BringingOrder into Texts.
In Proceedings of EMNLP2004.R.
Mihalcea and P. Tarau.
2005.
A language inde-pendent algorithm for single and multiple docu-ment summarization.
In Proceedings of IJCNLP-05.A.
Nenkova and A. Louis.
2008.
Can you summarizethis?
Identifying correlates of input difficulty forgeneric multi-document summarization.
In Pro-ceedings of ACL-08:HLT.A.
Nenkova, R. Passonneau, and K. McKeown.
2007.The Pyramid method: incorporating human contentselection variation in summarization evaluation.925ACM Transactions on Speech and Language Proc-essing (TSLP), 4(2).C.
Orasan, and O.
A. Chiorean.
2008.
Evaluation of aCrosslingual Romanian-English Multi-documentSummariser.
In Proceedings of 6th Language Re-sources and Evaluation Conference (LREC2008).P.
Pingali, J. Jagarlamudi and V. Varma.
2007.
Ex-periments in cross language query focused multi-document summarization.
In Workshop on CrossLingual Information Access Addressing the Infor-mation Need of Multilingual Societies inIJCAI2007.C.
Quirk.
2004.
Training a sentence-level machinetranslation confidence measure.
In Proceedings ofLREC2004.D.
R. Radev, H. Y. Jing, M. Stys and D. Tam.
2004.Centroid-based summarization of multiple docu-ments.
Information Processing and Management,40: 919-938.A.
Siddharthan and K. McKeown.
2005.
Improvingmultilingual summarization: using redundancy inthe input to correct MT errors.
In Proceedings ofHLT/EMNLP-2005.L.
Specia, Z. Wang, M. Turchi, J. Shawe-Taylor, C.Saunders.
2009.
Improving the Confidence of Ma-chine Translation Quality Estimates.
In MT Summit2009 (Machine Translation Summit XII).V.
Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer.X.
Wan, H. Li and J. Xiao.
2010.
EUSUM: extractingeasy-to-understand English summaries for non-native readers.
In Proceedings of  SIGIR2010.X.
Wan, J. Yang and J. Xiao.
2006.
Using cross-document random walks for topic-focused multi-documetn summarization.
In Proceedings ofWI2006.X.
Wan and J. Yang.
2008.
Multi-document summari-zation using cluster-based link analysis.
In Pro-ceedings of SIGIR-08.X.
Wan, J. Yang and J. Xiao.
2007.
Towards an Itera-tive Reinforcement Approach for SimultaneousDocument Summarization and Keyword Extraction.In Proceedings of ACL2007.K.-F. Wong, M. Wu and W. Li.
2008.
Extractive sum-marization using supervised and semi-supervisedlearning.
In Proceedings of COLING-08.H.
Y. Zha.
2002.
Generic Summarization and Key-phrase Extraction Using Mutual ReinforcementPrinciple and Sentence Clustering.
In Proceedingsof SIGIR2002.926
