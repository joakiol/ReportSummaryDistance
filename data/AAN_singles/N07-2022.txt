Proceedings of NAACL HLT 2007, Companion Volume, pages 85?88,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsDiscriminative Alignment Training without Annotated Datafor Machine TranslationPatrik Lambert, Rafael E. Banchs and Josep M. CregoTALP Research CenterJordi Girona Salgado 1?308034 Barcelona, Spain{lambert, rbanchs, jmcrego}@gps.tsc.upc.eduAbstractIn present Statistical Machine Translation(SMT) systems, alignment is trained in aprevious stage as the translation model.Consequently, alignment model parame-ters are not tuned in function of the trans-lation task, but only indirectly.
In thispaper, we propose a novel framework fordiscriminative training of alignment mod-els with automated translation metrics asmaximization criterion.
In this approach,alignments are optimized for the transla-tion task.
In addition, no link labels at theword level are needed.
This frameworkis evaluated in terms of automatic trans-lation evaluation metrics, and an improve-ment of translation quality is observed.1 IntroductionIn the first SMT systems (Brown et al, 1993), wordalignment was introduced as a hidden variable ofthe translation model.
When word-based translationmodels have been replaced by phrase-based mod-els (Zens et al, 2002), alignment1 and translationmodel training have become two separated tasks.The system of Brown et al was based on thenoisy channel approach.
Present SMT systems use amore general maximum entropy approach in which alog-linear combination of multiple feature functionsis implemented (Och and Ney, 2002).
Within this1Hereinafter, alignment will refer to word alignment, unlessotherwise stated.new framework translation quality can be tuned byadjusting the weight of each feature function in thelog-linear combination.
In order to improve transla-tion quality, this tuning can be effectively performedby minimizing translation error over a developmentcorpus for which manually translated references areavailable (Och, 2003).
As a separate first stage of theprocess, alignment is not in practice directly tuned infunction of the machine translation task.Tuning alignment for an MT system is subject topractical difficulties.
Unsupervised systems (Ochand Ney, 2003; Liang et al, 2006) are based on gen-erative models trained with the EM algorithm.
Theyrequire large computational resources, and incorpo-rating new features is difficult.
In contrast, addingnew features to some supervised systems (Liu et al,2005; Moore, 2005; Ittycheriah and Roukos, 2005)is easy, but the need of annotated data is a problem.A more general difficulty, however, is that of find-ing an alignment evaluation metric favoring align-ments which benefit Machine Translation.
The factthat the required alignment characteristics dependon each particular system makes it even more dif-ficult.
It seems that high precision alignments arebetter for phrase-based SMT (Chen and Federico,2006; Ayan and Dorr, 2006), whereas high recallalignments are more suited to N-gram SMT (Marin?oet al, 2006).
In this context, alignment quality im-provements does not necessarily imply translationquality improvements.
This is in agreement withthe observation of a poor correlation between wordalignment error rate (AER (Och and Ney, 2000)) andautomatic translation evaluation metrics (Ittycheriahand Roukos, 2005; Vilar et al, 2006).85Recently some alignment evaluation metrics havebeen proposed which are more informative whenthe alignments are used to extract translationunits (Fraser and Marcu, 2006; Ayan and Dorr,2006).
However, these metrics assess translationquality very indirectly.In this paper, we propose a novel framework fordiscriminative training of alignment models with au-tomated translation metrics as maximization crite-rion.
Thus we just need a reference aligned at thesentence level instead of link labels at the word level.The paper is structured as follows.
Section 2 ex-plains the models used in our word aligner, focusingon the features designed to account for the specifici-ties of the SMT system.
In section 3, our minimumerror training procedure is described and experimen-tal results are shown.
Finally, some concluding re-marks and lines of further research are given.2 Bilingual Word AlignerFor versatility and efficiency requirements, we im-plemented BIA, a BIlingual word Aligner similarto that of Moore (2005).
BIA consists in a beam-search decoder searching, for each sentence pair, thealignment which minimizes the cost of a linear com-bination of various models.
The differences withthe system of Moore lie in the features, which wespecially designed to suit our translation system (N-gram SMT (Marin?o et al, 2006)).
Its particularityis the translation model, which is based on a 4-gramlanguage model of bilingual units referred to as tu-ples.
Two issues regarding this translation model canbe dealt with at the alignment stage.Firstly, in order to estimate the bilingual n-grammodel, only one monotonic segmentation of eachsentence pair is performed.
Thus long reorderingscause long and sparse tuples to be extracted.
For ex-ample, if the first source word is linked to the lasttarget word, only one tuple can be extracted, whichcontains the whole sentence pair.
This kind of tupleis not reusable, and the data between its two extremewords are lost.Secondly, it occurs very often that unlinked words(i.e.
linked to NULL) end up producing tuples withNULL source sides.
This cannot be allowed sinceno NULL is expected to occur in a translation input.This problem is solved by preprocessing alignmentsbefore tuple extraction such that any unlinked targetword is attached to either its precedent or its follow-ing word.Taking theses issues into account, we imple-mented the following features:?
distinct source and target unlinked word penal-ties: since unlinked words have a different im-pact whether they appear in the source or targetlanguage, we introduced an unlinked word fea-ture for each side of the sentence pair.?
link bonus: in order to accommodate the N-gram model preference for higher recall align-ment, we introduced a feature which adds abonus for each link in the alignment.?
embedded word position penalty: this featurepenalizes situations like the one depicted in fig-ure 1.
In this example, the bilingual units s2-t2and s3-t3 cannot be extracted because word po-sitions s2 and s3 are embedded between linkss1-t1 and s4-t1.
Thus the link s4-t1 may intro-duces data sparseness in the translation model,although it may be a correct link.
So we wantto have a feature which counts the number ofembedded word positions in an alignment.Figure 1: Word positions embedded in a tuple.In addition to the embedded word position feature,we used the same two distortion features as Mooreto penalize reorderings in the alignment (one sumsthe number of crossing links, and the other one sumsthe amplitude of crossing links).
We also used the ?2score (Gale and Church, 1991) as a word associationmodel, and as a POS-tags association model.3 Experimental WorkFor these experiments we used the Chinese-English data provided for IWSLT?06 evaluationcampaign (Paul, 2006).
The training set contains46000 sentences (of 6.7 and 7.0 average length).
Pa-rameters were tuned over the development set (dev4)provided, consisting of 489 sentences of 11.2 wordsin average, with 7 references.
Our test set was a se-lection of 500 sentences (of 6 words in average, with16 references) among dev1, dev2 and dev3 sets.863.1 Optimization ProcedureOnce the alignment models were computed, a set ofoptimal log-linear coefficients was estimated via theoptimization procedure depicted in Figure 2.Figure 2: Optimization loop.The training corpus was aligned with a set of ini-tial parameters ?1, .
.
.
, ?7.
This alignment was usedto extract tuples and build a bilingual N-gram trans-lation model (TM).
A baseline SMT system, consist-ing of MARIE decoder and this translation model asunique feature2, was used to produce a translation(OUT) of the development source set.
Then, trans-lation quality over the development set is maximizedby iteratively varying the set of coefficients.The optimization procedure was performed by us-ing the SPSA algorithm (Spall, 1992).
SPSA is astochastic implementation of the conjugate gradientmethod which requires only two evaluations of theobjective function.
It was observed to be more ro-bust than the Downhill Simplex method when tuningSMT coefficients (Lambert and Banchs, 2006).Each function evaluation required to align thetraining corpus and build a new translation model.The algorithm converged after about 80 evaluations,lasting each 17 minutes with a 3 GHz processor.Alignment decoding was performed with a beam of10 (it took 50 seconds and required 8 MB memory).Finally, the corpus was aligned with the opti-mum set of coefficients, and a full SMT system wasbuild, with a target language model (trained on theprovided training data), a word bonus model andtwo lexical models.
SMT models weights were op-timized with a standard Minimum Error Training(MET) strategy3 and the test corpus was translated2An N-gram SMT system can produce good translationswithout additional target language model since the target lan-guage is modeled inside the bilingual N-gram model.3SMT parameters are not optimized together with alignmentwith the full system.
To contrast the results, fulltranslation systems were also build extracting tuplesfrom various combinations of GIZA++ alignments(trained with 50 classes and respectively 4,5 and 4iterations of models 1,HMM and 4).
In order to limitthe error introduced by MET, we translated the testcorpus with three sets of SMT model weights, andtook the average and standard deviation.3.2 ResultsTable 1 shows results obtained with the full SMTsystem on the test corpus, with GIZA++ alignments,and BIA alignments optimized in function of threemetrics: BLEU, NIST, and BLEU+4*NIST.
Thestandard deviation is indicated in parentheses.
Al-though results for systems trained with different BIAalignments present more variability than systemstrained with GIZA++ alignments, they achieve bet-ter average scores, and one of them obtains muchhigher scores.
Unexpectedly, BIA alignments tunedwith NIST yield the system with worse NIST score.4 Conclusions and further workWe proposed a novel framework for discriminativetraining of alignment models with automated trans-lation metrics as maximization criterion.
Accord-ing to this type of metrics, the translation systemstrained from the optimized alignments clearly per-formed better than the ones trained from Giza++alignment combinations.In addition, this first version of the alignmentsystem has very basic models and could be im-proved.
We could certainly improve the associationscore model, for example adding discount factors oradding more association score types, or dictionaries.During the alignment coefficient optimization de-picted in Figure 2, only the baseline SMT systemis used.
In future work, we could consider usingvarious SMT features (as would be required for aphrase-based SMT system).Our approach, as it is, cannot be applied to a largecorpus, since it requires to align the whole trainingcorpus at each iteration.
Thus an interesting furtherresearch would consist in determining whether theparameters for two main reasons.
Firstly, translation is moresensitive to variations of SMT parameters.
Secondly, alignmentis optimized over the full training set, whereas SMT is tunedover the development set.87System BLEU NIST PER WERGIZA++ union 42.7 (1.1) 8.82 (0.07) 34.7 (0.2) 43.7 (0.4)GIZA++ intersection 42.4 (0.9) 8.53 (0.07) 37.0 (0.9) 45.0 (1.3)GIZA++ Zh?En 43.7 (0.9) 8.90 (0.2) 37.2 (1.4) 45.5 (2.0)BIA (BLEU) 44.8 (0.4) 9.00 (0.04) 35.7 (0.07) 43.8 (0.09)BIA (BLEU+4*NIST) 47.0 (1.5) 8.83 (0.4) 32.9 (0.8) 40.9 (0.5)BIA (NIST) 44.8 (0.1) 8.55 (0.14) 33.0 (0.2) 41.4 (0.5)Table 1: Automatic translation evaluation results.alignment parameters trained on a part of the corpusare valid for the whole corpus.Finally, some Giza++ parameters may also betuned, in the same way as for BIA parameters.5 AcknowledgmentsThis work has been partially funded by the Euro-pean Union under the integrated project TC-STAR- Technology and Corpora for Speech to SpeechTranslation -(IST-2002-FP6-506738, http://www.tc-star.org) and by the Spanish Government under grantTEC2006-13964-C03 (AVIVAVOZ project).ReferencesNecip F. Ayan and Bonnie J. Dorr.
2006.
Going BeyondAER: An Extensive Analysis of Word Alignments andTheir Impact on MT.
In Proc.
COLING-ACL, pages9?16, Sydney, Australia.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
The mathe-matics of statistical machine translation: Parameter es-timation.
Computational Linguistics, 19(2):263?311.Boxing Chen and Marcello Federico.
2006.
Improvingphrase-based statistical translation through combina-tion of word alignment.
In Proc.
FinTAL, Turku, Fin-land.Alexander Fraser and Daniel Marcu.
2006.
Semi-supervised training for statistical word alignment.
InProc.
COLING-ACL, pages 769?776, Sydney, Aus-tralia.W.
Gale and K. W. Church.
1991.
Identifying word cor-respondences in parallel texts.
In DARPA Speech andNatural Language Workshop, Asilomar, CA.Abraham Ittycheriah and Salim Roukos.
2005.
A maxi-mum entropy word aligner for arabic-english machinetranslation.
In Proc.
HLT-EMNLP, pages 89?96, Van-couver, Canada.Patrik Lambert and Rafael E. Banchs.
2006.
TuningMachine Translation Parameters with SPSA.
In Proc.IWSLT, pages 190?196, Kyoto, Japan.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proc.
the HLT-NAACL, pages104?111, New York City, USA.Yang Liu, Qun Liu, and Shouxun Lin.
2005.
Log-linearmodels for word alignment.
In Proc.
ACL, pages 459?466, Ann Arbor, Michigan.Jose?
B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`de Gispert, Patrik Lambert, Jose?
A.R.
Fonollosa, andMarta R. Costa-jussa`.
2006.
N-gram based machinetranslation.
Computational Linguistics, 32(4):527?549.Robert C. Moore.
2005.
A discriminative frameworkfor bilingual word alignment.
In Proc.
HLT-EMNLP,pages 81?88, Vancouver, Canada.Franz Josef Och and Hermann Ney.
2000.
A compari-son of alignment models for statistical machine trans-lation.
In Proc.
COLING, pages 1086?1090, Saar-brucken,Germany.F.J.
Och and H. Ney.
2002.
Dicriminative trainingand maximum entropy models for statistical machinetranslation.
In Proc.
ACL, pages 295?302, Philadel-phia, PA.F.J.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51, March.F.J.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proc.
ACL, pages 160?167.Michael Paul.
2006.
Overview of the IWSLT 2006 Eval-uation Campaign.
In Proc.
IWSLT, pages 1?15, Kyoto,Japan.James C. Spall.
1992.
Multivariate stochastic approxi-mation using a simultaneous perturbation gradient ap-proximation.
IEEE Trans.
Automat.
Control, 37:332?341.David Vilar, Maja Popovic, and Hermann Ney.
2006.AER: Do we need to ?improve?
our alignments?
InProc.
IWSLT, pages 205?212, Kyoto, Japan.R.
Zens, F.J. Och, and H. Ney.
2002.
Phrase-based sta-tistical machine translation.
In Springer Verlag, editor,Proc.
German Conf.
on Artificial Intelligence (KI).88
