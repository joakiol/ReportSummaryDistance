Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 670?680,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsChinese Poetry Generation with Recurrent Neural NetworksXingxing Zhang and Mirella LapataInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9ABx.zhang@ed.ac.uk, mlap@inf.ed.ac.ukAbstractWe propose a model for Chinese poemgeneration based on recurrent neural net-works which we argue is ideally suited tocapturing poetic content and form.
Ourgenerator jointly performs content selec-tion (?what to say?)
and surface realization(?how to say?)
by learning representationsof individual characters, and their com-binations into one or more lines as wellas how these mutually reinforce and con-strain each other.
Poem lines are gener-ated incrementally by taking into accountthe entire history of what has been gen-erated so far rather than the limited hori-zon imposed by the previous line or lexicaln-grams.
Experimental results show thatour model outperforms competitive Chi-nese poetry generation systems using bothautomatic and manual evaluation methods.1 IntroductionClassical poems are a significant part of China?scultural heritage.
Their popularity manifests itselfin many aspects of everyday life, e.g., as a meansof expressing personal emotion, political views,or communicating messages at festive occasionsas well as funerals.
Amongst the many differ-ent types of classical Chinese poetry, quatrain andregulated verse are perhaps the best-known ones.Both types of poem must meet a set of structural,phonological, and semantic requirements, render-ing their composition a formidable task left to thevery best scholars.An example of a quatrain is shown in Table 1.Quatrains have four lines, each five or seven char-acters long.
Characters in turn follow specificphonological patterns, within each line and acrosslines.
For instance, the final characters in the sec-ond, fourth and (optionally) first line must rhyme,?
?Missing You??????
(* Z P P Z)Red berries born in the warm southland.??????
(P P Z Z P)How many branches flush in the spring??
?????
(* P P Z Z)Take home an armful, for my sake,??????
(* Z Z P P)As a symbol of our love.Table 1: An example of a 5-char quatrain ex-hibiting one of the most popular tonal patterns.The tone of each character is shown at the end ofeach line (within parentheses); P and Z are short-hands for Ping and Ze tones, respectively; * indi-cates that the tone is not fixed and can be either.Rhyming characters are shown in boldface.whereas there are no rhyming constraints for thethird line.
Moreover, poems must follow a pre-scribed tonal pattern.
In traditional Chinese, ev-ery character has one tone, Ping (level tone) or Ze(downward tone).
The poem in Table 1 exempli-fies one of the most popular tonal patterns (Wang,2002).
Besides adhering to the above formal crite-ria, poems must exhibit concise and accurate useof language, engage the reader/hearer, stimulatetheir imagination, and bring out their feelings.In this paper we are concerned with generat-ing traditional Chinese poems automatically.
Al-though computers are no substitute for poetic cre-ativity, they can analyze very large online textrepositories of poems, extract statistical patterns,maintain them in memory and use them to gen-erate many possible variants.
Furthermore, whileamateur poets may struggle to remember and ap-ply formal tonal and structural constraints, it is rel-atively straightforward for the machine to check670whether a candidate poem conforms to these re-quirements.
Poetry generation has received a fairamount of attention over the past years (see thediscussion in Section 2), with dozens of computa-tional systems written to produce poems of vary-ing sophistication.
Beyond the long-term goal ofbuilding an autonomous intelligent system capa-ble of creating meaningful poems, there are po-tential short-term applications for computer gen-erated poetry in the ever growing industry of elec-tronic entertainment and interactive fiction as wellas in education.
An assistive environment forpoem composition could allow teachers and stu-dents to create poems subject to their require-ments, and enhance their writing experience.We propose a model for Chinese poem genera-tion based on recurrent neural networks.
Our gen-erator jointly performs content selection (?whatto say?)
and surface realization (?how to say?
).Given a large collection of poems, we learn repre-sentations of individual characters, and their com-binations into one or more lines as well as howthese mutually reinforce and constrain each other.Our model generates lines in a poem probabilis-tically: it estimates the probability of the currentline given the probability of all previously gener-ated lines.
We use a recurrent neural network tolearn the representations of the lines generated sofar which in turn serve as input to a recurrent lan-guage model (Mikolov et al., 2010; Mikolov et al.,2011b; Mikolov et al., 2011a) which generates thecurrent line.
In contrast to previous approaches(Greene et al., 2010; Jiang and Zhou, 2008), ourgenerator makes no Markov assumptions about thedependencies of the words within a line and acrosslines.We evaluate our approach on the task of qua-train generation (see Table 1 for a human-writtenexample).
Experimental results show that ourmodel outperforms competitive Chinese poetrygeneration systems using both automatic and man-ual evaluation methods.2 Related WorkAutomated poetry generation has been a popularresearch topic over the past decades (see Coltonet al.
(2012) and the references therein).
Most ap-proaches employ templates to construct poems ac-cording to a set of constraints (e.g., rhyme, me-ter, stress, word frequency) in combination withcorpus-based and lexicographic resources.
Forexample, the Haiku poem generator presented inWu et al.
(2009) and Tosa et al.
(2008) producespoems by expanding user queries with rules ex-tracted from a corpus and additional lexical re-sources.
Netzer et al.
(2009) generate Haikuwith Word Association Norms, Agirrezabal etal.
(2013) compose Basque poems using patternsbased on parts of speech and WordNet (Fellbaum,1998), and Oliveira (2012) presents a generationalgorithm for Portuguese which leverages seman-tic and grammar templates.A second line of research uses genetic algo-rithms for poem generation (Manurung, 2003;Manurung et al., 2012; Zhou et al., 2010).
Ma-nurung et al.
(2012) argue that at a basic levelall (machine-generated) poems must satisfy theconstraints of grammaticality (i.e., a poem mustsyntactically well-formed), meaningfulness (i.e., apoem must convey a message that is meaningfulunder some interpretation) and poeticness (i.e., apoem must exhibit features that distinguishes itfrom non-poetic text, e.g., metre).
Their modelgenerates several candidate poems and then usesstochastic search to find those which are grammat-ical, meaningful, and poetic.A third line of research draws inspiration fromstatistical machine translation (SMT) and re-lated text-generation applications such as sum-marization.
Greene et al.
(2010) infer meters(stressed/unstressed syllable sequences) from acorpus of poetic texts which they subsequentlyuse for generation together with a cascade ofweighted finite-state transducers interpolated withIBM Model 1.
Jiang and Zhou (2008) generateChinese couplets (two line poems) using a phrase-based SMT approach which translates the first lineto the second line.
He et al.
(2012) extend this al-gorithm to generate four-line quatrains by sequen-tially translating the current line from the previousone.
Yan et al.
(2013) generate Chinese quatrainsbased on a query-focused summarization frame-work.
Their system takes a few keywords as inputand retrieves the most relevant poems from a cor-pus collection.
The retrieved poems are segmentedinto their constituent terms which are then groupedinto clusters.
Poems are generated by iterativelyselecting terms from clusters subject to phonolog-ical, structural, and coherence constraints.Our approach departs from previous work intwo important respects.
Firstly, we model the tasksof surface realization and content selection jointly671?(spring)??(lute)?
(drunk)KeywordsShiXueHanYingspringlute drunk?
?
?
?
??
?
?
?
?...Candidate linesLine 1Line 2Line 3Line 4First linegenerationNext linegenerationFigure 1: Poem generation with keywords spring, lute, and drunk.
The keywords are expanded intophrases using a poetic taxonomy.
Phrases are then used to generate the first line.
Following lines aregenerated by taking into account the representations of all previously generated lines.using recurrent neural networks.
Structural, se-mantic, and coherence constraints are capturednaturally in our framework, through learning therepresentations of individual characters and theircombinations.
Secondly, generation proceeds bytaking into account multi-sentential context ratherthan the immediately preceding sentence.
Ourwork joins others in using continuous representa-tions to express the meaning of words and phrases(Socher et al., 2012; Mikolov et al., 2013) andhow these may be combined in a language mod-eling context (Mikolov and Zweig, 2012).
Morerecently, continuous translation models based onrecurrent neural networks have been proposed asa means to map a sentence from the source lan-guage to sentences in the target language (Auliet al., 2013; Kalchbrenner and Blunsom, 2013).These models are evaluated on the task of rescor-ing n-best lists of translations.
We use neural net-works more directly to perform the actual poemgeneration task.3 The Poem GeneratorAs common in previous work (Yan et al., 2013;He et al., 2012) we assume that our generator op-erates in an interactive context.
Specifically, theuser supplies keywords (e.g., spring, lute, drunk )highlighting the main concepts around which thepoem will revolve.
As illustrated in Figure 1, ourgenerator expands these keywords into a set of re-lated phrases.
We assume the keywords are re-stricted to those attested in the ShiXueHanYing po-etic phrase taxonomy (He et al., 2012; Yan et al.,2013).
The latter contains 1,016 manual clustersof phrases (Liu, 1735); each cluster is labeled witha keyword id describing general poem-worthy top-ics.
The generator creates the first line of the poembased on these keywords.
Subsequent lines aregenerated based on all previously generated lines,subject to phonological (e.g., admissible tonal pat-terns) and structural constraints (e.g., whether thequatrain is five or seven characters long).To create the first line, we select all phrasescorresponding to the user?s keywords and gener-ate all possible combinations satisfying the tonalpattern constraints.
We use a language model torank the generated candidates and select the best-ranked one as the first line in the poem.
In im-plementation, we employ a character-based recur-rent neural network language model (Mikolov etal., 2010) interpolated with a Kneser-Ney trigramand find the n-best candidates with a stack de-coder (see Section 3.5 for details).
We then gen-erate the second line based on the first one, thethird line based on the first two lines, and so on.Our generation model computes the probabilityof line Si+1= w1,w2, .
.
.
,wm, given all previouslygenerated lines S1:i(i?
1) as:P(Si+1|S1:i) =m?1?j=1P(wj+1|w1: j,S1:i) (1)Equation (1), decomposes P(Si+1|S1:i) as the prod-uct of the probability of each character wjinthe current line given all previously generatedcharacters w1: j?1and lines S1:i.
This meansthat P(Si+1|S1:i) is sensitive to previously gener-ated content and currently generated characters.The estimation of the term P(wj+1|w1: j,S1:i)lies at the heart of our model.
We learn repre-sentations for S1:i, the context generated so far,using a recurrent neural network whose output672serves as input to a second recurrent neural net-work used to estimate P(wj+1|w1: j,S1:i).
Figure 2illustrates the generation process for the ( j+ 1)thcharacter wj+1in the (i + 1)th line Si+1.
First,lines S1:iare converted into vectors v1:iwith aconvolutional sentence model (CSM; described inSection 3.1).
Next, a recurrent context model(RCM; see Section 3.2) takes v1:ias input andoutputs uji, the representation needed for gener-ating wj+1?
Si+1.
Finally, u1i,u2i, .
.
.
,ujiand thefirst j characters w1: jin line Si+1serve as input toa recurrent generation model (RGM) which esti-mates P(wj+1= k|w1: j,S1:i) with k ?V , the prob-ability distribution of the ( j + 1)th character overall words in the vocabulary V .
More formally, toestimate P(wj+1|w1: j,S1:i) in Equation (1), we ap-ply the following procedure:vi= CSM(Si) (2a)uji= RCM(v1:i, j) (2b)P(wj+1|w1: j,S1:i) = RGM(w1: j+1,u1: ji) (2c)We obtain the probability of the (i + 1)th sen-tence P(Si+1|S1:i), by running the RGM in (2c)above m?
1 times (see also Equation (1)).
In thefollowing, we describe how the different compo-nents of our model are obtained.3.1 Convolutional Sentence Model (CSM)The CSM converts a poem line into a vector.
Inprinciple, any model that produces vector-basedrepresentations of phrases or sentences could beused (Mitchell and Lapata, 2010; Socher et al.,2012).
We opted for the convolutional sentencemodel proposed in Kalchbrenner and Blunsom(2013) as it is n-gram based and does not makeuse of any parsing, POS-tagging or segmentationtools which are not available for Chinese poems.Their model computes a continuous representationfor a sentence by sequentially merging neighbor-ing vectors (see Figure 3).Let V denote the character vocabulary in ourcorpus; L ?
Rq?|V |denotes a character embed-ding matrix whose columns correspond to char-acter vectors (q represents the hidden unit size).Such vectors can be initialized randomly or ob-tained via a training procedure (Mikolov et al.,2013).
Let w denote a character with index k;e(w) ?R|V |?1is a vector with zero in all positionsexcept e(w)k= 1; Tl?
Rq?Nlis the sentence rep-resentation in the lth layer, where Nlis the num-ber of columns in the lth layer (Nl= 1 in theviujihihi?1uki(k 6= j)RCM1-of-N encoding ofwj=(0,. .
.
,1,. .
.
,0)rjrj?1P(wj+1|w1: j,S1:i)RGMFigure 2: Generation of the ( j + 1)th charac-ter wj+1in the (i + 1)th line Si+1.
The recur-rent context model (RCM) takes i lines as in-put (represented by vectors v1, .
.
.
,vi) and cre-ates context vectors for the recurrent generationmodel (RGM).
The RGM estimates the probabil-ity P(wj+1|w1: j,S1:i).top layer); Cl,n?
Rq?nis an array of weight ma-trices which compress neighboring n columns inthe lth layer to one column in the (l + 1)th layer.Given a sentence S = w1,w2, .
.
.
,wm, the first layeris represented as:T1= [L ?
e(w1),L ?
e(w2), .
.
.
,L ?
e(wm)]N1= m(3)The (l +1)th layer is then computed as follows:Tl+1:, j= ?
(n?i=1Tl:, j+i?1Cl,n:,i)Nl+1= Nl?n+11?
j ?
Nl+1(4)where Tlis the representation of the previouslayer l, Cl,na weight matrix,  element-wise vec-tor product, and ?
a non-linear function.
We com-press two neighboring vectors in the first two lay-ers and three neighboring vectors in the remaininglayers.
Specifically, for quatrains with seven char-acters, we use C1,2, C2,2, C3,3, C4,3to merge vec-tors in each layer (see Figure 3); and for quatrainswith five characters we use C1,2, C2,2, C3,3.673?
?
?
?
?
?
?Far off I watch the waterfall plunge to thelong river.C1,2C2,2C3,3C4,3Figure 3: Convolutional sentence model for 7-charquatrain.
The first layer has seven vectors, onefor each character.
Two neighboring vectors aremerged to one vector in the second layer withweight matrix C1,2.
In other layers, either two orthree neighboring vectors are merged.3.2 Recurrent Context Model (RCM)The RCM takes as input the vectors representingthe i lines generated so far and reduces them to asingle context vector which is then used to gener-ate the next character (see Figure 2).
We compressthe i previous lines to one vector (the hidden layer)and then decode the compressed vector to differentcharacter positions in the current line.
The outputlayer consists thus of several vectors (one for eachposition) connected together.
This way, differentaspects of the context modulate the generation ofdifferent characters.Let v1, .
.
.
,vi(vi?Rq?1) denote the vectors ofthe previous i lines; hi?
Rq?1is their compressedrepresentation (hidden layer) which is obtainedwith matrix M ?
Rq?2q; matrix Ujdecodes hitouji?
Rq?1in the (i+ 1)th line.
The computationof the RCM proceeds as follows:h0= 0hi= ?
(M ?
[vihi?1])uji= ?
(Uj?hi) 1?
j ?
m?1(5)where ?
is a non-linear function such as sigmoidand m the line length.
Advantageously, lines inclassical Chinese poems have a fixed length of fiveor seven characters.
Therefore, the output layer ofthe recurrent context model only needs two weightmatrices (one for each length) and the number ofparameters still remains tractable.3.3 Recurrent Generation Model (RGM)As shown in Figure 2, the RGM estimates theprobability distribution of the next character (overthe entire vocabulary) by taking into account thecontext vector provided by the RCM and the1-of-N encoding of the previous character.
TheRGM is essentially a recurrent neural network lan-guage model (Mikolov et al., 2010) with an aux-iliary input layer, i.e., the context vector fromthe RCM.
Similar strategies for encoding addi-tional information have been adopted in relatedlanguage modeling and machine translation work(Mikolov and Zweig, 2012; Kalchbrenner andBlunsom, 2013; Auli et al., 2013).Let Si+1= w1,w2, .
.
.
,wmdenote the lineto be generated.
The RGM must esti-mate P(wj+1|w1: j,S1:i), however, since the firsti lines have been encoded in the context vector uji,we compute P(wj+1|w1: j,uji) instead.
Therefore,the probability P(Si+1|S1:i) becomes:P(Si+1|S1:i) =m?1?j=1P(wj+1|w1: j,uji)(6)Let |V | denote the size of the character vocabu-lary.
The RGM is specified by a number of ma-trices.
Matrix H ?
Rq?q(where q represents thehidden unit size) transforms the context vector toa hidden representation; matrix X ?
Rq?|V |trans-forms a character to a hidden representation, ma-trix R ?
Rq?qimplements the recurrent transfor-mation and matrix Y ?
R|V |?qdecodes the hiddenrepresentation to weights for all words in the vo-cabulary.
Let w denote a character with index kin V ; e(w) ?
R|V |?1represents a vector with zeroin all positions except e(w)k= 1, rjis the hiddenlayer of the RGM at step j, and yj+1the output ofthe RGM, again at step j.
The RGM proceeds asfollows:r0= 0 (7a)rj= ?
(R ?
rj?1+X ?
e(wj)+H ?uji) (7b)yj+1= Y ?
rj(7c)where ?
is a nonlinear function (e.g., sigmoid).674The probability of the ( j+1)th word given theprevious j words and the previous i lines is esti-mated by a softmax function:P(wj+1= k|w1: j,uji) =exp(yj+1,k)?|V |k=1exp(yj+1,k)(8)We obtain P(Si+1|S1:i) by multiplying all the termsin the right hand-side of Equation (6).3.4 TrainingThe objective for training is the cross entropy er-rors of the predicted character distribution and theactual character distribution in our corpus.
Anl2regularization term is also added to the objec-tive.
The model is trained with back propagationthrough time (Rumelhart et al., 1988) with sen-tence length being the time step.
The objectiveis minimized by stochastic gradient descent.
Dur-ing training, the cross entropy error in the outputlayer of the RGM is back-propagated to its hid-den and input layers, then to the RCM and finallyto the CSM.
The same number of hidden units(q = 200) is used throughout (i.e., in the RGM,RCM, and CSM).
In our experiments all param-eters were initialized randomly, with the excep-tion of the word embedding matrix in the CSMwhich was initialized with word2vec embeddings(Mikolov et al., 2013) obtained from our poemcorpus (see Section 4 for details on the data weused).To speed up training, we employed word-classing (Mikolov et al., 2011b).
To compute theprobability of a character, we estimate the proba-bility of its class and then multiply it by the proba-bility of the character conditioned on the class.
Inour experiments we used 82 (square root of |V |)classes which we obtained by applying hierarchi-cal clustering on character embeddings.
This strat-egy outperformed better known frequency-basedclassing methods (Zweig and Makarychev, 2013)on our task.Our poem generator models content selectionand lexical choice and their interaction, but doesnot have a strong notion of local coherence,as manifested in poetically felicitous line-to-linetransitions.
In contrast, machine translation mod-els (Jiang and Zhou, 2008) have been particu-larly successful at generating adjacent lines (cou-plets).
To enhance coherence, we thus interpolateour model with two machine translation features(i.e., inverted phrase translation model feature andinverted lexical weight feature).
Also note, thatin our model surface generation depends on thelast observed character and the state of the hiddenlayer before this observation.
This way, there is noexplicitly defined context, and history is capturedimplicitly by the recurrent nature of the model.This can be problematic for our texts which mustobey certain stylistic conventions and sound po-etic.
In default of a better way of incorporatingpoeticness into our model, we further interpolate itwith a language model feature (i.e., a Kneser-Neytrigram model).Throughout our experiments, we use theRNNLM toolkit to train the character-based recur-rent neural network language model (Mikolov etal., 2010).
Kneser-Ney n-grams were trained withKenLM (Heafield, 2011).3.5 DecodingOur decoder is a stack decoder similar to Koehnet al.
(2003).
In addition, it implements the tonalpattern and rhyming constraints necessary for gen-erating well-formed Chinese quatrains.
Once thefirst line in a poem is generated, its tonal patternis determined.
During decoding, phrases violat-ing this pattern are ignored.
As discussed in Sec-tion 1, the final characters of the second and thefourth lines must rhyme.
We thus remove duringdecoding fourth lines whose final characters do notrhyme with the second line.
Finally, we use MERTtraining (Och, 2003) to learn feature weights forthe decoder.4 Experimental DesignData We created a corpus of classical Chinesepoems by collating several online resources: TangPoems, Song Poems, Song Ci, Ming Poems, QingPoems, and Tai Poems.
The corpus consistsof 284,899 poems in total.
78,859 of these arequatrains and were used for training and evalu-ating our model.1Table 2 shows the differentpartitions of this dataset (POEMLM) into train-ing (QTRAIN)2, validation (QVALID) and testing(QTEST).
Half of the poems in QVALID andQTEST are 5-char quatrains and the other halfare 7-char quatrains.
All poems except QVALID1The data used in our experiments can be downloadedfrom http://homepages.inf.ed.ac.uk/mlap/index.php?page=resources.2Singleton characters in QTRAIN (6,773 in total) were re-placed by <R> to reduce data sparsity.675Poems Lines CharactersQTRAIN 74,809 299,236 2,004,460QVALID 2,000 8,000 48,000QTEST 2,050 8,200 49,200POEMLM 280,849 2,711,034 15,624,283Table 2: Dataset partitions of our poem corpus.and QTEST were used for training the character-based language models (see row POEMLM in Ta-ble 2).
We also trained word2vec embeddings onPOEMLM.
In our experiments, we generated qua-trains following the eight most popular tonal pat-terns according to Wang (2002).Perplexity Evaluation Evaluation of machine-generated poetry is a notoriously difficult task.Our evaluation studies were designed to assessManurung et al.
?s (2012) criteria of grammatical-ity, meaningfulness, and poeticness.
As a san-ity check, we first measured the perplexity of ourmodel with respect to the goldstandard.
Intu-itively, a better model should assign larger proba-bility (and therefore lower perplexity) to goldstan-dard poems.BLEU-based Evaluation We also used BLEUto evaluate our model?s ability to generate the sec-ond, third and fourth line given previous goldstan-dard lines.
A problematic aspect of this evalu-ation is the need for human-authored references(for a partially generated poem) which we do nothave.
We obtain references automatically follow-ing the method proposed in He et al.
(2012).
Themain idea is that if two lines share a similar topic,the lines following them can be each other?s ref-erences.
Let A and B denote two adjacent linesin a poem, with B following A.
Similarly, let lineB?follow line A?in another poem.
If lines A andA?share some keywords in the same cluster in theShixuehanying taxonomy, then B and B?can beused as references for both A and A?.
We use thisalgorithm on the Tang Poems section of our corpusto build references for poems in the QVALID andQTEST data sets.
Poems in QVALID (with auto-generated references) were used for MERT train-ing and Poems in QTEST (with auto-generated ref-erences) were used for BLEU evaluation.Human Evaluation Finally, we also evaluatedthe generated poems by eliciting human judg-Models PerplexityKN5 172RNNLM 145RNNPG 93Table 3: Perplexities for different models.ments.
Specifically, we invited 30 experts3onChinese poetry to assess the output of our gen-erator (and comparison systems).
These expertswere asked to rate the poems using a 1?5 scale onfour dimensions: fluency (is the poem grammati-cal and syntactically well-formed?
), coherence (isthe poem thematically and logically structured?
),meaningfulness (does the poem convey a mean-ingful message to the reader?)
and poeticness(does the text display the features of a poem?
).We also asked our participants to evaluate systemoutputs by ranking the generated poems relative toeach other as a way of determining overall poemquality (Callison-Burch et al., 2012).Participants rated the output of our model andthree comparison systems.
These included He etal.
?s (2012) SMT-based model (SMT), Yan et al.
?s(2013) summarization-based system (SUM), anda random baseline which creates poems by ran-domly selecting phrases from the Shixuehanyingtaxonomy given some keywords as input.
Wealso included human written poems whose contentmatched the input keywords.
All systems wereprovided with the same keywords (i.e., the samecluster names in the ShiXueHanYing taxonomy).In order to compare all models on equal footing,we randomly sampled 30 sets of keywords (withthree keywords in each set) and generated 30 qua-trains for each system according to two lengths,namely 5-char and 7-char.
Overall, we obtainedratings for 300 (5?30?2) poems.5 ResultsThe results of our perplexity evaluation are sum-marized in Table 3.
We compare our RNN-basedpoem generator (RNNPG) against Mikolov?s(2010) recurrent neural network language model(RNNLM) and a 5-gram language model withKneser-Ney smoothing (KN5).
All models weretrained on QTRAIN and tuned on QVALID.
Theperplexities were computed on QTEST.
Note that327 participants were professional or amateur poets andthree were Chinese literature students who had taken at leastone class on Chinese poetry composition.676Models1?
2 2?
3 3?
4 Average5-char 7-char 5-char 7-char 5-char 7-char 5-char 7-charSMT 0.0559 0.0906 0.0410 0.1837 0.0547 0.1804 0.0505 0.1516RNNPG 0.0561 0.1868 0.0515 0.2102 0.0572 0.1800 0.0549 0.1923Table 4: BLEU-2 scores on 5-char and 7-char quatrains.
Given i goldstandard lines, BLEU-2 scores arecomputed for the next (i+1)th lines.ModelsFluency Coherence Meaning Poeticness Rank5-char 7-char 5-char 7-char 5-char 7-char 5-char 7-char 5-char 7-charRandom 2.52 2.18 2.22 2.16 2.02 1.93 1.77 1.71 0.31 0.26SUM 1.97 1.91 2.08 2.33 1.84 1.98 1.66 1.73 0.25 0.22SMT 2.81 3.01 2.47 2.76 2.33 2.73 2.08 2.36 0.43 0.53RNNPG 4.01**3.44*3.18**3.12*3.20**3.02 2.80**2.68*0.73**0.64*Human 4.31+4.19++3.81++4.00++3.61+3.91++3.29++3.49++0.79 0.84++Table 5: Mean ratings elicited by humans on 5-char and 7-char quatrains.
Diacritics**(p < 0.01)and*(p < 0.05) indicate our model (RNNPG) is significantly better than all other systems except Human.Diacritics++(p < 0.01) and+(p < 0.05) indicate Human is significantly better than all other systems.the RNNPG estimates the probability of a poemline given at least one previous line.
Therefore, theprobability of a quatrain assigned by the RNNPGis the probability of the last three lines.
For a faircomparison, RNNLM and KN5 only leverage thelast three lines of each poem during training, vali-dation and testing.
The results in Table 3 indicatethat the generation ability of the RNNPG is betterthan KN5 and RNNLM.
Note that this perplexity-style evaluation is not possible for models whichcannot produce probabilities for gold standard po-ems.
For this reason, other related poem gener-ators (Yan et al., 2013; He et al., 2012) are notincluded in the table.The results of our evaluation using BLEU-2 aresummarized in Table 4.
Here, we compare oursystem against the SMT-based poem generationmodel of He et al.
(2012).4Their system is alinear combination of two translation models (onewith five features and another one with six).
Ourmodel uses three of their features, namely the in-verted phrase translation model feature, the lexicalweight feature, and a Kneser-Ney trigram feature.Unfortunately, it is not possible to evaluate Yanet al.
?s (2013) summarization-based system withBLEU, as it creates poems as a whole and there isno obvious way to generate next lines with their4Our re-implementation of their system delivered verysimilar scores to He et al.
(2012).
For example, we ob-tained an average BLEU-1 of 0.167 for 5-char quatrains and0.428 for 7-char quatrains compared to their reported scoresof 0.141 and 0.380, respectively.algorithm.
The BLEU scores in Table 4 indicatethat, given the same context lines, the RNNPG isbetter than SMT at generating what to say next.BLEU scores should be, however, viewed withsome degree of caution.
Aside from being an ap-proximation of human judgment (Callison-Burchet al., 2012), BLEU might be unnecessarily con-servative for poem composition which by its verynature is a creative endeavor.The results of our human evaluation study areshown in Table 5.
Each column reports mean rat-ings for a different dimension (e.g., fluency, co-herence).
Ratings for 5-char and 7-char quatrainsare shown separately.
The last column reportsrank scores for each system (Callison-Burch et al.,2012).
In a ranked list of N items (N = 5 here), thescore of the ith ranked item is(N?i)(N?1).
The numer-ator indicates how many times a systems won inpairwise comparisons, while the denominator nor-malizes the score.With respect to 5-char quatrains, RNNPG issignificantly better than Random, SUM and SMTon fluency, coherence, meaningfulness, poeticnessand ranking scores (using a t-test).
On all dimen-sions, human-authored poems are rated as signif-icantly better than machine-generated ones, withthe exception of overall ranking.
Here, the dif-ference between RNNPG and Human is not sig-nificant.
We obtain similar results with 7-charquatrains.
In general, RNNPG seems to performbetter on the shorter poems.
The mean ratings677????
?, ??????
?,Egrets stood, peeping fishes.
Budding branches are full of romance.?????.
??????
?.Water was still, reflecting mountains.
Plum blossoms are invisible but adorable.????
?, ??????
?,The wind went down by nightfall, With the east wind comes Spring.?????.
??????
?.as the moon came up by the tower.
Where on earth do I come from?Table 6: Example output produced by our model (RNNPG).are higher and the improvements over other sys-tems are larger.
Also notice, that the score mar-gins between the human- and machine-written po-ems become larger for 7-char quatrains.
This in-dicates that the composition of 7-char quatrains ismore difficult compared to 5-char quatrains.
Ta-ble 6 shows two example poems (5-char and 7-char) produced by our model which received highscores with respect to poeticness.Interestingly, poems generated by SUM5aregiven ratings similar to Random.
In fact SUMis slightly worse (although not significantly) thanRandom on all dimensions, with the exception ofcoherence.
In the human study reported in Yan etal.
(2013), SUM is slightly better than SMT.
Thereare several reasons for this discrepancy.
We useda more balanced experimental design: all systemsgenerated poems from the same keywords whichwere randomly chosen.
We used a larger datasetto train the SMT model compared to Yan et al.
(284,899 poems vs 61,960).
The Random baselineis not a straw-man; it selects phrases from a taxon-omy of meaningful clusters edited by humans andclosely related to the input keywords.6 ConclusionsIn this paper we have presented a model for Chi-nese poem generation based on recurrent neuralnetworks.
Our model jointly performs content se-lection and surface realization by learning repre-sentations of individual characters and their com-binations within and across poem lines.
Previouswork on poetry generation has mostly leveragedcontextual information of limited length (e.g., onesentence).
In contrast, we introduced two recur-rent neural networks (the recurrent context modeland recurrent generation model) which naturally5We made a good-faith effort to re-implement their poemgeneration system.
We are grateful to Rui Yan for his helpand technical advice.capture multi-sentential content.
Experimental re-sults show that our model yields high quality po-ems compared to the state of the art.
Perhaps un-surprisingly, our human evaluation study revealedthat machine-generated poems lag behind human-generated ones.
It is worth bearing in mind thatpoetry composition is a formidable task for hu-mans, let alone machines.
And that the poemsagainst which our output was compared have beenwritten by some of the most famous poets in Chi-nese history!Avenues for future work are many and varied.We would like to generate poems across differ-ent languages and genres (e.g., Engish sonnets orJapanese haiku).
We would also like to make themodel more sensitive to line-to-line transitions andstylistic conventions by changing its training ob-jective to a combination of cross-entropy error andBLEU score.
Finally, we hope that some of thework described here might be of relevance to othergeneration tasks such as summarization, concept-to-text generation, and machine translation.AcknowledgmentsWe would like to thank Eva Halser for valuablediscussions on the machine translation baseline.We are grateful to the 30 Chinese poetry expertsfor participating in our rating study.
Thanks toGujing Lu, Chu Liu, and Yibo Wang for their helpwith translating the poems in Table 6 and Table 1.ReferencesManex Agirrezabal, Bertol Arrieta, Aitzol Astigarraga,and Mans Hulden.
2013.
POS-Tag Based Po-etry Generation with WordNet.
In Proceedings ofthe 14th European Workshop on Natural LanguageGeneration, pages 162?166, Sofia, Bulgaria.Michael Auli, Michel Galley, Chris Quirk, and Geof-frey Zweig.
2013.
Joint Language and Translation678Modeling with Recurrent Neural Networks.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1044?1054, Seattle, Washington, USA.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of the 7th Work-shop on Statistical Machine Translation, pages 10?51, Montr?eal, Canada.Simon Colton, Jacob Goodwin, and Tony Veale.
2012.Full-FACE Poetry Generation.
In Proceedings of theInternational Conference on Computational Cre-ativity, pages 95?102, Dublin, Ireland.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Database.
MIT Press, Cambridge, MA.Erica Greene, Tugba Bodrumlu, and Kevin Knight.2010.
Automatic Analysis of Rhythmic Poetry withApplications to Generation and Translation.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, pages 524?533, Cambridge, MA.Jing He, Ming Zhou, and Long Jiang.
2012.
Gener-ating Chinese Classical Poems with Statistical Ma-chine Translation Models.
In Proceedings of the26th AAAI Conference on Artificial Intelligence,pages 1650?1656, Toronto, Canada.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedings of theEMNLP 2011 Sixth Workshop on Statistical Ma-chine Translation, pages 187?197, Edinburgh, Scot-land, United Kingdom, July.Long Jiang and Ming Zhou.
2008.
Generating ChineseCouplets using a Statistical MT Approach.
In Pro-ceedings of the 22nd International Conference onComputational Linguistics, pages 377?384, Manch-ester, UK, August.Nal Kalchbrenner and Phil Blunsom.
2013.
RecurrentContinuous Translation Models.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1700?1709, Seattle,Washington.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-based Translation.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 48?54, Edmonton, Canada.Wenwei Liu.
1735.
ShiXueHanYing.Ruli Manurung, Graeme Ritchie, and Henry Thomp-son.
2012.
Using Genetic Algorithms to CreateMeaningful Poetic Text.
Journal of ExperimentalTheoretical Artificial Intelligence, 24(1):43?64.Ruli Manurung.
2003.
An Evolutionary Algorithm Ap-proach to Poetry Generation.
Ph.D. thesis, Univer-sity of Edinburgh.Tomas Mikolov and Geoffrey Zweig.
2012.
Con-text Dependent Recurrent Neural Network Lan-guage Model.
In Proceedings of 2012 IEEE Work-shop on Spoken Language Technology, pages 234?239, Miami, Florida.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent Neural Network based Language Model.
InProceedings of INTERSPEECH, pages 1045?1048,Makuhari, Japan.Tomas Mikolov, Anoop Deoras, Daniel Povey, LukasBurget, and Jan Cernocky.
2011a.
Strategiesfor Training Large Scale Neural Network LanguageModels.
In Proceedings of ASRU 2011, pages 196?201, Hilton Waikoloa Village, Big Island, Hawaii,US.Tomas Mikolov, Stefan Kombrink, Lukas Burget,JH Cernocky, and Sanjeev Khudanpur.
2011b.
Ex-tensions of Recurrent Neural Network LanguageModel.
In Proceedings of the 2011 IEEE Interna-tional Conference on Acoustics, Speech and SignalProcessing, pages 5528?5531, Prague, Czech Re-public.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed Representa-tions of Words and Phrases and their Composition-ality.
In Advances in Neural Information Process-ing Systems, pages 3111?3119, Lake Tahoe, Nevada,United States.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin Distributional Models of Semantics.
CognitiveScience, 34(8):1388?1439.Yael Netzer, David Gabay, Yoav Goldberg, andMichael Elhadad.
2009.
Gaiku: Generating Haikuwith Word Associations Norms.
In Proceedings ofthe Workshop on Computational Approaches to Lin-guistic Creativity, pages 32?39, Boulder, Colorado.Franz Josef Och.
2003.
Minimum Error Rate Train-ing in Statistical Machine Translation.
In Proceed-ings of the 41st Annual Meeting on Associationfor Computational Linguistics, pages 160?167, Sap-poro, Japan.Hugo Gonc?alo Oliveira.
2012.
PoeTryMe: a Versa-tile Platform for Poetry Generation.
ComputationalCreativity, Concept Invention, and General Intelli-gence, 1:21.David Rumelhart, Geoffrey Hinton, and RonaldWilliams.
1988.
Learning Representations by Back-propagating Errors.
MIT Press, Cambridge, MA,USA.679Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic Compo-sitionality through Recursive Matrix-Vector Spaces.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 1201?1211, Jeju Island, Korea.Naoko Tosa, Hideto Obara, and Michihiko Minoh.2008.
Hitch Haiku: An Interactive Supporting Sys-tem for Composing Haiku Poem How I Learned toLove the Bomb: Defcon and the Ethics of Com-puter Games.
In Proceedings of the 7th Inter-national Conference on Entertainment Computing,pages 209?216, Pittsburgh, PA.Li Wang.
2002.
A Summary of Rhyming Constraintsof Chinese Poems (Shi Ci Ge Lv Gai Yao).
BeijingPress, 2002.Xiaofeng Wu, Naoko Tosa, and Ryohei Nakatsu.
2009.New Hitch Haiku: An Interactive Renku PoemComposition Supporting Tool Applied for Sightsee-ing Navigation System.
In Proceedings of the 8thInternational Conference on Entertainment Com-puting, pages 191?196, Paris, France.Rui Yan, Han Jiang, Mirella Lapata, Shou-De Lin,Xueqiang Lv, and Xiaoming Li.
2013.
I, Poet:Automatic Chinese Poetry Composition Through aGenerative Summarization Framework Under Con-strained Optimization.
In Proceedings of the 23rdInternational Joint Conference on Artificial Intelli-gence, pages 2197?2203, Beijing, China.Cheng-Le Zhou, Wei You, and Xiaojun Ding.
2010.Genetic Algorithm and its Implementation of Au-tomatic Generation of Chinese SongCi.
Journal ofSoftware, pages 427?437.Geoffrey Zweig and Konstantin Makarychev.
2013.Speed Regularization and Optimality in Word Class-ing.
In Proceedings of the 2014 IEEE InternationalConference on Acoustics, Speech, and Signal Pro-cessing, pages 8237?8241, Florence, Italy.680
