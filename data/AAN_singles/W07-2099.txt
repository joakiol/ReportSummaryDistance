Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 442?445,Prague, June 2007. c?2007 Association for Computational LinguisticsUSP-IBM-1 and USP-IBM-2: The ILP-based Systems for Lexical SampleWSD in SemEval-2007Lucia Specia, Maria das Grac?as Volpe NunesICMC - University of Sa?o PauloTrabalhador Sa?o-Carlense, 400, Sa?o Carlos, 13560-970, Brazil{lspecia, gracan}@icmc.usp.brAshwin Srinivasan, Ganesh RamakrishnanIBM India Research LaboratoryBlock 1, Indian Institute of Technology, New Delhi 110016, India{ashwin.srinivasan, ganramkr}@in.ibm.comAbstractWe describe two systems participating of theEnglish Lexical Sample task in SemEval-2007.
The systems make use of InductiveLogic Programming for supervised learningin two different ways: (a) to build WordSense Disambiguation (WSD) models froma rich set of background knowledge sources;and (b) to build interesting features fromthe same knowledge sources, which are thenused by a standard model-builder for WSD,namely, Support Vector Machines.
Both sys-tems achieved comparable accuracy (0.851and 0.857), which outperforms considerablythe most frequent sense baseline (0.787).1 IntroductionWord Sense Disambiguation (WSD) aims to iden-tify the correct sense of ambiguous words in context.Results from the last edition of the Senseval com-petition (Mihalcea et al, 2004) have shown that, forsupervised learning, the best accuracies are obtainedwith a combination of various types of features, to-gether with traditional machine learning algorithmsbased on feature-value vectors, such as Support Vec-tor Machines (SVMs) and Naive Bayes.
While thefeatures employed by these approaches are mostlyconsidered to be ?shallow?, that is, extracted fromcorpus or provided by shallow syntactic tools likepart-of-speech taggers, it is generally thought thatsignificant progress in automatic WSD would re-quire a ?deep?
approach in which access to substan-tial body of linguistic and world knowledge couldassist in resolving ambiguities.
Although the ac-cess to large amounts of knowledge is now possi-ble due to the availability of lexicons like WordNet,parsers, etc., the incorporation of such knowledgehas been hampered by the limitations of the mod-elling techniques usually employed for WSD.
Usingcertain sources of information, mainly relational in-formation, is beyond the capabilities of such tech-niques, which are based on feature-value vectors.Arguably, Inductive Logic Programming (ILP) sys-tems provide an appropriate framework for dealingwith such data: they make explicit provisions for theinclusion of background knowledge of any form; thericher representation language used, based on first-order logic, is powerful enough to capture contextualrelationships; and the modelling is not restricted tobeing of a particular form (e.g., classification).We describe the investigation of the use of ILPfor WSD in the Lexical Sample task of SemEval-2007 in two different ways: (a) the construction ofmodels that can be used directly to disambiguatewords; and (b) the construction of interesting fea-tures to be used by a standard feature-based algo-rithm, namely, SVMs, to build disambiguation mod-els.
We call the systems resulting of the two differ-ent approaches ?USP-IBM-1?
and ?USP-IBM-2?,respectively.
The background knowledge is from 10different sources of information extracted from cor-pus, lexical resources and NLP tools.In the rest of this paper we first present the spec-ification of ILP implementations that construct ILPmodels and features (Section 2) and then describethe experimental evaluation on the SemEval-2007Lexical Sample task data (Section 3).4422 Inductive Logic ProgrammingInductive Logic Programming (ILP) (Muggleton,1991) employs techniques from Machine Learningand Logic Programming to build first-order theo-ries or descriptions from examples and backgroundknowledge, which are also represented by first-orderclauses.
Functionally, ILP can be characterised bytwo classes of programs.
The first, predictive ILP,is concerned with constructing models (in this case,sets of rules) for discriminating accurately amongstpositive and negative examples.
The partial spec-ifications provided by (Muggleton, 1994) form thebasis for deriving programs in this class:?
B is background knowledge consisting of a fi-nite set of clauses = {C1, C2, .
.
.}?
E is a finite set of examples = E+?E?
where:?
Positive Examples.
E+ = {e1, e2, .
.
.}
isa non-empty set of definite clauses?
Negative Examples.
E?
= {f1, f2 .
.
.}
isa set of Horn clauses (this may be empty)?
H , the output of the algorithm given B and E,is acceptable if these conditions are met:?
Prior Satisfiability.
B ?
E?
6|= 2?
Posterior Satisfiability.
B ?H ?E?
6|= 2?
Prior Necessity.
B 6|= E+?
Posterior Sufficiency.
B ?
H |= e1 ?
e2 ?.
.
.The second category of ILP programs, descriptiveILP, is concerned with identifying relationships thathold amongst the background knowledge and exam-ples, without a view of discrimination.
The partialspecifications for programs in this class are basedon the description in (Muggleton and Raedt, 1994):?
B is background knowledge?
E is a finite set of examples (this may beempty)?
H , the output of the algorithm given B and Eis acceptable if the following condition is met:?
Posterior Sufficiency.
B ?
H ?
E 6|= 2The intuition behind the idea of exploiting afeature-based model constructor that uses first-orderfeatures is that certain sources of structured infor-mation that cannot be represented by feature vectorscan, by a process of ?propositionalization?, be iden-tified and converted in a way that they can be accom-modated in such vectors, allowing for traditionallearning techniques to be employed.
Essentially, thisinvolve two steps: (1) a feature-construction stepthat identifies all the features, that is, a set of clausesH , that are consistent with the constraints providedby the background knowledge B (descriptive ILP);and (2) a feature-selection step that retains some ofthe features based on their utility in classifying theexamples, for example, each clause must entail atleast one positive example (predictive ILP).
In orderto be used by SVMs, each clause hi in H is con-verted into a boolean feature fi that takes the value1 (or 0) for any individual for which the body ofthe clause is true (if the body is false).
Thus, theset of clauses H gives rise to a boolean vector foreach individual in the set of examples.
The fea-tures constructed may express conjunctions on dif-ferent knowledge sources.
For example, the follow-ing boolean feature built from a clause for the verb?ask?
tests whether the sentence contains the expres-sion ?ask out?
and the word ?dinner?.
More detailson the specifications of predictive and descriptiveILP for WSD can be found in (Specia et al, 2007):f1(X) ={1 expr(X, ?ask out?)
?
bag(X,dinner) = 10 otherwise3 ExperimentsWe investigate the performance of two kinds of ILP-based models for WSD:1.
ILP models (USP-IBM-1 system): models con-structed by an ILP system for predicting thecorrect sense of a word.2.
ILP-assisted models (USP-IBM-2 system):models constructed by SVMs for predicting thecorrect sense of a word that, in addition to ex-isting shallow features, use features built by anILP system according to the specification forfeature construction in Section 2.443The data for the English Lexical Sample task inSemEval-2007 consists of 65 verbs and 35 nouns.Examples containing those words were extractedfrom the WSJ Penn Treebank II and Brown corpus.The number of training / test examples varies from19 / 2 to 2,536 / 541 (average = 222.8 / 48.5).
Thesenses of the examples were annotated according toOntoNotes tags, which are groupings of WordNetsenses, and therefore are more coarse-grained.
Thenumber of senses used in the training examples fora given word varies from 1 to 13 (average = 3.6).First-order clauses representing the followingbackground knowledge sources, which were au-tomatically extracted from corpus and lexical re-sources or provided by NLP tools, were used to de-scribe the target words in both systems:B1.
Unigrams consisting of the 5 words to theright and left of the target word.B2.
5 content words to the right and left of thetarget word.B3.
Part-of-speech tags of 5 words to the right andleft of the target word.B4.
Syntactic relations with respect to the targetword.
If that word is a verb, subject and object syn-tactic relations are represented.
If it is a noun, therepresentation includes the verb of which it is a sub-ject or object, and the verb / noun it modifies.B5.
12 collocations with respect to the targetword: the target word itself, 1st preposition to theright, 1st and 2nd words to the left and right, 1stnoun, 1st adjective, and 1st verb to the left and right.B6.
A relative count of the overlapping words inthe sense inventory definitions of each of the pos-sible senses of the target word and the words sur-rounding that target word in the sentence, accordingto the sense inventories provided.B7.
If the target word is a verb, its selectionalrestrictions, defined in terms of the semantic fea-tures of its arguments in the sentence, as given byLDOCE.
WordNet relations are used to make theverification more generic and a hierarchy of featuretypes is used to account for different levels of speci-ficity in the restrictions.B8.
If the target word is a verb, the phrasal verbspossibly occurring in a sentence, according to thelist of phrasal verbs given by dictionaries.B9.
Pairs of words in the sentence that occur fre-quently in the corpus related by verb-subject/objector subject/verb/object-modifier relations.B10.
Bigrams consisting of adjacent words in asentence occurring frequently in the corpus.Of these 10 sources, B1?B6 correspond to the socalled ?shallow features?, in the sense that they canbe straightforwardly represented by feature vectors.A feature vector representation of these sources isbuilt to be used by the feature-based model construc-tor.
Clausal definitions for B1?B10 are directly usedby the ILP system.We use the Aleph ILP system (Srinivasan, 1999)to construct disambiguation models in USP-IBM-1and to construct features to be used in USP-IBM-2.
Feature-based model construction in USP-IBM-2 system is performed by a linear SVM (the SMOimplementation in WEKA).In the USP-IBM-1 system, for each target word,equipped with examples and background knowl-edge definitions (B1?B10), Aleph constructs a setof clauses in line with the specifications for predic-tive ILP described in Section 2.
Positive examplesare provided by the correct sense of the target word.Negative examples are generated automatically us-ing all the other senses.
3-fold cross-validation onthe training data was used to obtain unbiased esti-mates of the predictive accuracy of the models for aset of relevant parameters.
The best average accura-cies were obtained with the greedy induction strat-egy, in conjunction with a minimal clause accuracyof 2.
The constructed clauses were used to predictthe senses in the test data following the order of theirproduction, in a decision-list like manner, with theaddition to the end of a default rule assigning themajority sense for those cases which are not coveredby any other rule.In the USP-IBM-2 system, for constructing the?good?
features for each target word from B1?B10 (the ?ILP-based features?
), we first selected, inAleph, the clauses covering at least 1 positive exam-ple.
3-fold cross-validation on the training data wasperformed in order to obtain the best model possi-ble using SVM with features in B1?B6 and the ILP-based features.
A feature selection method basedon information gain with various percentages of fea-tures to be selected (1/64, ..., 1/2) was used, whichresulted in different numbers of features for each tar-get word.444Baseline USP-IBM-1 USP-IBM-2Nouns 0.809 0.882 0.882Verbs 0.762 0.817 0.828All 0.787 0.851 0.857Table 1: Average accuracies of the ILP-based mod-els for different part-of-speechesTable 1 shows the average accuracy of a base-line classifier that simply votes for the most frequentsense of each word in the training data against theaccuracy of our ILP-based systems, USP-IBM-1 andUSP-IBM-2, according to the part-of-speech of thetarget word, and for all words.
Clearly, the ?ma-jority class?
classifier performs poorest, on average.The difference between both ILP-based systems andthe baseline is statistically significant according toa paired t-test with p < 0.01.
The two ILP-basedmodels appear to be comparable in their average ac-curacy.
Discarding ties, IBM-USP-2 outperformsIBM-USP-1 for 31 of the words, but the advantageis not statistically significant (cf.
paired t-test).The low accuracy of the ILP-based systems forcertain words may be consequence of some charac-teristics of the data.
In particular, the sense distri-butions are very skewed in many cases, with differ-ent distributions in the training and test data.
Forexample, in the case of ?care?
(accuracy = 0.428),the majority sense in the training data is 1 (78.3%),while in the test data the majority sense is 2 (71%).In cases like this, many of the test examples remainuncovered by the rules produced by the ILP systemand backing off to the majority sense also results ina mistake, since the majority sense in the trainingdata does not apply for most of the test examples.The same goes for the feature-based system: fea-tures which are relevant for the test examples willnot be built or selected.One relevant feature of ILP is its ability to pro-duce expressive symbolic models.
These modelscan reproduce any kind of background knowledgeusing sets of rules testing conjunctions of differenttypes of knowledge, which may include variables(intensional clauses).
This is valid both for the con-struction of predictive models and for the construc-tion of features (which are derived from the clauses).Examples of rules induced for the verb ?come?
aregiven in Figure 1.
The first rule states that the sensesense(X, 3) :-expr(X, ?come to?
).sense(X, 1) :-satisfy restrictions(X, [animate], nil);(relation(X, subj, B), pos(X, B, nnp)).Figure 1: Examples of rules learned for ?come?of the verb in a sentence X will be 3 (progress to astate) if that sentence contains the expression ?cometo?.
The second rule states that the sense of the verbwill be 1 (move, travel, arrive) if its subject is ?ani-mate?
and there is no object, or if it has has a subjectB that is a proper noun (nnp).4 Concluding RemarksWe have investigated the use of ILP as a mech-anism for incorporating shallow and deep knowl-edge sources into the construction of WSD mod-els for the Semeval-2007 Lexical Sample Task data.Results consistently outperform the most frequentsense baseline.
It is worth noticing that the knowl-edge sources used here were initially designed forthe disambiguation of verbs (Specia et al, 2007)and therefore we believe that further improvementscould be achieved with the identification and speci-fication of other sources which are more appropriatefor the disambiguation of nouns.ReferencesR.
Mihalcea, T. Chklovski, A. Kilgariff.
2004.The SENSEVAL-3 English Lexical Sample Task.SENSEVAL-3: 3rd Int.
Workshop on the Evaluation ofSystems for Semantic Analysis of Text, 25?28.S.
Muggleton.
1991.
Inductive Logic Program-ming.New Generation Computing, 8(4):29-5-318.S.
Muggleton.
1994.
Inductive Logic Programming:derivations, successes and shortcomings.
SIGART Bul-letin, 5(1):5?11.S.
Muggleton and L. D. Raedt.
1994.
Inductive logicprogramming: Theory and methods.
Journal of LogicProgramming, 19,20:629?679.L.
Specia, M.G.V.
Nunes, A. Srinivasan, G. Ramakrish-nan.
2007.
Word Sense Disambiguation using Induc-tive Logic Programming.
Proceedings of the 16th In-ternational Conference on ILP, Springer-Verlag.A.
Srinivasan.
1999.
The Aleph Manual.
ComputingLaboratory, Oxford University.445
