Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1815?1827,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsExploring Demographic Language Variations to Improve MultilingualSentiment Analysis in Social MediaSvitlana VolkovaCenter for Language andSpeech ProcessingJohns Hopkins UniversityBaltimore, MDsvitlana@jhu.eduTheresa WilsonHuman Language TechnologyCenter of ExcellenceJohns Hopkins UniversityBaltimore, MDtaw@jhu.eduDavid YarowskyCenter for Language andSpeech ProcessingJohns Hopkins UniversityBaltimore, MDyarowsky@cs.jhu.eduAbstractDifferent demographics, e.g., gender or age,can demonstrate substantial variation in theirlanguage use, particularly in informal contextssuch as social media.
In this paper we focus onlearning gender differences in the use of sub-jective language in English, Spanish, and Rus-sian Twitter data, and explore cross-culturaldifferences in emoticon and hashtag use formale and female users.
We show that gen-der differences in subjective language can ef-fectively be used to improve sentiment anal-ysis, and in particular, polarity classificationfor Spanish and Russian.
Our results showstatistically significant relative F-measure im-provement over the gender-independent base-line 1.5% and 1% for Russian, 2% and 0.5%for Spanish, and 2.5% and 5% for English forpolarity and subjectivity classification.1 IntroductionSociolinguistics and dialectology have been study-ing the relationships between language and speech atthe phonological, lexical and morphosyntactic lev-els and social identity for decades (Picard, 1997;Gefen and Ridings, 2005; Holmes and Meyerhoff,2004; Macaulay, 2006; Tagliamonte, 2006).
Re-cent studies have focused on exploring demographiclanguage variations in personal email communica-tion, blog posts, and public discussions (Boneva etal., 2001; Mohammad and Yang, 2011; Eisensteinet al 2010; O?Connor et al 2010; Bamman et al2012).
However, one area that remains largely unex-plored is the effect of demographic language varia-tion on subjective language use, and whether thesedifferences may be exploited for automatic senti-ment analysis.
With the growing commercial im-portance of applications such as personalized rec-ommender systems and targeted advertising (Fanand Chang, 2009), detecting helpful product review(Ott et al 2011), tracking sentiment in real time(Resnik, 2013), and large-scale, low-cost, passivepolling (O?Connor et al 2010), we believe that sen-timent analysis guided by user demographics is avery important direction for research.In this paper, we focus on gender demographicsand language in social media to investigate differ-ences in the language used to express opinions inTwitter for three languages: English, Spanish, andRussian.
We focus on Twitter data because of its vol-ume, dynamic nature, and diverse population world-wide.1 We find that some words are more or lesslikely to be positive or negative in context depend-ing on the the gender of the author.
For example, theword weakness is more likely to be used in a pos-itive way by women (Chocolate is my weakness!
)but in a negative way by men (Clearly they knowour weakness.
Argggg).
The Russian word ???????
(achieve) is used in a positive way by male users andin a negative way by female users.Our goals of this work are to (1) explore the gen-der bias in the use of subjective language in so-cial media, and (2) incorporate this bias into modelsto improve sentiment analysis for English, Spanish,and Russian.
Specifically, in this paper we:?
investigate multilingual lexical variations in theuse of subjective language, and cross-cultural1As of May 2013, Twitter has 500m users (140m of themin the US) from more than 100 countries.1815emoticon and hashtag usage on a large scale inTwitter data;2?
show that gender bias in the use of subjec-tive language can be used to improve sentimentanalysis for multiple languages in Twitter.?
demonstrate that simple, binary features repre-senting author gender are insufficient; rather, itis the combination of lexical features, togetherwith set-count features representing gender-dependent sentiment terms that is needed forstatistically significant improvements.To the best of our knowledge, this work is the firstto show that incorporating gender leads to signifi-cant improvements for sentiment analysis, particu-larly subjectivity and polarity classification, for mul-tiple languages in social media.2 Related WorkNumerous studies since the early 1970?s have inves-tigated gender-language differences in interaction,theme, and grammar among other topics (Schiffman,2002; Sunderland et al 2002).
More recent researchhas studied gender differences in telephone speech(Cieri et al 2004; Godfrey et al 1992) and emails(Styler, 2011).
Mohammad and Yang (2011) ana-lyzed gender differences in the expression of senti-ment in love letters, hate mail, and suicide notes, andemotional word usage across genders in email.There has also been a considerable amount ofwork in subjectivity and sentiment analysis overthe past decade, including, more recently, in mi-croblogs (Barbosa and Feng, 2010; Berminghamand Smeaton, 2010; Pak and Paroubek, 2010; Bifetand Frank, 2010; Davidov et al 2010; Li etal., 2010; Kouloumpis et al 2011; Jiang et al2011; Agarwal et al 2011; Wang et al 2011;Calais Guerra et al 2011; Tan et al 2011; Chenet al 2012; Li et al 2012).
In spite of the surge ofresearch in both sentiment and social media, only alimited amount of work focusing on gender identi-fication has looked at differences in subjective lan-guage across genders within social media.
Thel-wall (2010) found that men and women use emoti-cons to differing degrees on MySpace, e.g., female2Gender-dependent and independent lexical resources ofsubjective terms in Twitter for Russian, Spanish and English canbe found here: http://www.cs.jhu.edu/~svitlana/users express positive emoticons more than maleusers.
Other researchers included subjective patternsas features for gender classification of Twitter users(Rao et al 2010).
They found that the majority ofemotion-bearing features, e.g., emoticons, repeatedletters, exasperation, are used more by female thanmale users, which is consistent with results reportedin other recent work (Garera and Yarowsky, 2009;Burger et al 2011; Goswami et al 2009; Argamonet al 2007).
Other related work is that of Otter-bacher (2010), who studied stylistic differences be-tween male and female reviewers writing productreviews, and Mukherjee and Liu (2010), who ap-plied positive, negative and emotional connotationfeatures for gender classification in microblogs.Although previous work has investigated gen-der differences in the use of subjective language,and features of sentiment have been used in genderidentification, to the best of our knowledge no onehas yet investigated whether gender differences inthe use of subjective language can be exploited toimprove sentiment classification in English or anyother language.
In this paper we seek to answer thisquestion for the domain of social media.3 DataFor the experiments in this paper, we use three setsof data for each language: a large pool of data (800Ktweets) labeled for gender but unlabeled for senti-ment, plus 2K development data and 2K test datalabeled for both sentiment and gender.
We use theunlabeled data to bootstrap Twitter-specific lexiconsand investigate gender differences in the use of sub-jective language.
We use the development data forparameter tuning while bootstrapping, and the testdata for sentiment classification.For English, we download tweets from the corpuscreated by Burger et al(2011).
This dataset con-tains 2,958,103 tweets from 184K users, excludingretweets.
Retweets are omitted because our focus ison the sentiment of the person tweeting; in retweets,the words originate from a different user.
All usersin this corpus have gender labels, which Burger etal.
automatically extracted from self-reported gen-der on Facebook or MySpace profiles linked to bythe Twitter users.
English tweets are identified usinga compression-based language identification (LID)1816tool (Bergsma et al 2012).
According to LID,there are 1,881,620 (63.6%) English tweets fromwhich we select a random, gender-balanced sampleof 0.8M tweets.
Burger?s corpus does not includeRussian and Spanish data on the same scale as En-glish.
Therefore, for Russian and Spanish we con-struct a new Twitter corpus by downloading tweetsfrom followers of region-specific news and mediaTwitter feeds.
We use LID to identify Russian andSpanish tweets, and remove retweets as before.
Inthis data, gender is labeled automatically based onuser first and last name morphology with a precisionabove 0.98 for all languages.Sentiment labels for tweets in the developmentand test sets are obtained using Amazon MechanicalTurk.
For each tweet we collect annotations fromfive workers and use majority vote to determine thefinal label for the tweet.
Snow et al(2008) showthat for a similar task, labeling emotion and valence,on average four non-expert labelers are needed toachieve an expert level of annotation.
Below are theexample Russian tweets labeled for sentiment:?
Pos: ???
??
???????
??????
????
?
??-?????
?????
????????
???...
(It is a greatpleasure to go to bed after a long day at work...)?
Neg: ?????????
????????
????????
??-????
???
??????!
(Dear Mr. Prokhorov justbuy the elections!)?
Both: ?????????
????
??
???????
?????!??
????
??????????
?????????
???
?????????
:) (It was crowded at the local market!But I got presents for my family:-))?
Neutral: ????
?????
??????
?????
(Kiev isa very old city).Table 1 gives the distribution of tweets over senti-ment and gender labels for the development and testsets for English (EDEV, ETEST), Spanish (SDEV,STEST), and Russian (RDEV, RTEST).Data Pos Neg Both Neut ?
?EDEV 617 357 202 824 1,176 824ETEST 596 347 195 862 1,194 806SDEV 358 354 86 1,202 768 1,232STEST 317 387 93 1203 700 1,300RDEV 452 463 156 929 1,016 984RTEST 488 380 149 983 910 1,090Table 1: Gender and sentiment label distribution in thedevelopment and test sets for all languages.4 Subjective Language and GenderTo study the intersection of subjective language andgender in social media, ideally we would have alarge corpus labeled for both.
Although our largecorpus is labeled for gender, it is not labeled for sen-timent.
Only the 4K tweets for each language thatcompose the development and test sets are labeledfor both gender and sentiment.
Obtaining sentimentlabels for all tweets would be both impractical andexpensive.
Instead we use large multilingual senti-ment lexicons developed specifically for Twitter asdescribed below.
Using these lexicons we can beginto explore the relationship between subjective lan-guage and gender in the large pool of data labeledfor gender but unlabeled for sentiment.
We alsolook at the relationship between gender and the useof different hashtags and emoticons.
These can bestrong indicators of sentiment in social media, and infact are sometimes used to create noisy training datafor sentiment analysis in Twitter (Pak and Paroubek,2010; Kouloumpis et al 2011).4.1 Bootstrapping Subjectivity LexiconsRecent work by Banea et.al (2012) classifies meth-ods for bootstrapping subjectivity lexicons into twotypes: corpus-based and dictionary-based.
Corpus-based methods extract subjectivity lexicons fromunlabeled data using different similarity metricsto measure the relatedness between words, e.g.,Pointwise Mutual Information (PMI).
Corpus-basedmethods have been used to bootstrap lexiconsfor ENGLISH (Turney, 2002) and other languages,including ROMANIAN (Banea et al 2008) andJAPANESE (Kaji and Kitsuregawa, 2007).Dictionary-based methods rely on relations be-tween words in existing lexical resources.
For exam-ple, Rao and Ravichandran (2009) construct HINDIand FRENCH sentiment lexicons using relations inWordNet (Miller, 1995), Rosas et.
al.
(2012) boot-strap a SPANISH lexicon using SentiWordNet (Bac-cianella et al 2010) and OpinionFinder,3 Clematideand Klenner (2010), Chetviorkin et al(2012) andAbdul-Mageed et.
al.
(2011) automatically expandand evaluate GERMAN, RUSSIAN and ARABIC sub-jective lexicons.3www.cs.pitt.edu/mpqa/opinionfinder1817We use the corpus-based, language-independentapproach proposed by Volkova et al(2013) to boot-strap Twitter-specific subjectivity lexicons.
To start,the new lexicon is seeded with terms from the initiallexicon LI .
On each iteration, tweets in the unla-beled data are labeled using the current lexicon.
If atweet contains one or more terms from the lexicon itis marked subjective, otherwise neutral.
Tweet po-larity is determined in a similar way, but takes intoaccount negation.
For every term not in the lexi-con with a frequency threshold, the probability ofthat word appearing in a subjective sentence is cal-culated.
The top k terms with a subjective probabil-ity are then added to the lexicon.
Bootstrapping con-tinues until there are no more new terms meeting thecriteria to add to the lexicon.
The parameters are op-timized using a grid search on the development datausing F-measure for subjectivity classification.
InTable 2 we report size and term polarity from the ini-tial LI and the bootstrapped LB lexicons.
Althoughmore sophisticated bootstrapping methods exist, thisapproach has been shown to be effective for atomi-cally learning subjectivity lexicons in multiple lan-guages on a large scale without any external, rich,lexical resources, e.g., WordNet, or advanced NLPtools, e.g., syntactic parsers (Wiebe, 2000) or infor-mation extraction tools (Riloff and Wiebe, 2003).For English, seed terms for bootstrapping arethe strongly subjective terms in the MPQA lexicon(Wilson et al 2005).
For Spanish and Russian, theseed terms are obtained by translating the Englishseed terms using a bi-lingual dictionary, collectingsubjectivity judgments from MTurk on the transla-tions, filtering out translations that are not stronglysubjective, and expanding the resulting word listswith plurals and inflectional forms.To verify that bootstrapping does provide a bet-ter resource than existing dictionary-expanded lexi-cons, we compare our Twitter-specific lexicons LBEnglish Spanish RussianLEI LEB LSI LSB LRI LRBPos 2.3 16.8 2.9 7.7 1.4 5.3Neg 2.8 4.7 5.2 14.6 2.3 5.5Total 5.1 21.5 8.1 22.3 3.7 10.8Table 2: The initial LI and the bootstrapped LB (high-lighted) lexicon term count (LI ?
LB) with polarityacross languages (thousands).to the corresponding initial lexicons LI and the ex-isting state-of-the-art subjective lexicons including:?
8K strongly subjective English terms from Sen-tiWordNet ?E (Baccianella et al 2010);?
1.5K full strength terms from the Spanish sen-timent lexicon ?S (Perez-Rosas et al 2012);?
5K terms from the Russian sentiment lexicon?R (Chetviorkin and Loukachevitch, 2012).For that we apply rule-based subjectivity classi-fication on the test data.4 This subjectivity classi-fier predicts that a tweet is subjective if it containsat least one, or at least two subjective terms fromthe lexicon.
To make a fair comparison, we auto-matically expand ?E with plurals and inflectionalforms, ?S with the inflectional forms for verbs, and?R with the inflectional forms for adverbs, adjec-tives and verbs.
We report precision, recall and F-measure results in Table 3 and show that our boot-strapped lexicons outperform the corresponding ini-tial lexicons and the external resources.Subj ?
1 Subj ?
2P R F P R F?E 0.67 0.49 0.57 0.76 0.16 0.27LEI 0.69 0.73 0.71 0.79 0.34 0.48LEB 0.64 0.91 0.75 0.7 0.74 0.72?S 0.52 0.39 0.45 0.62 0.07 0.13LSI 0.50 0.73 0.59 0.59 0.36 0.45LSB 0.44 0.91 0.59 0.51 0.71 0.59?R 0.61 0.49 0.55 0.74 0.17 0.29LRI 0.72 0.34 0.46 0.83 0.07 0.13LRB 0.64 0.58 0.61 0.74 0.23 0.35Table 3: Precision, recall and F-measure results for sub-jectivity classification using the external ?, initial LI andbootstrapped LB lexicons for all languages.4.2 Lexical EvaluationWith our Twitter-specific sentiment lexicons, wecan now investigate how the subjective use of theseterms differs depending on gender for our three lan-guages.
Figure 1 illustrates what we expect to find.
{F} and {M} are the sets of subjective terms usedby females and males, respectively.
We expect thatsome terms will be used by males, but never by fe-males, and vice-versa.
The vast majority, however,will be used by both genders.
Within this set ofshared terms, many words will show little difference4A similar rule-based approach using terms from theMPQA lexicon is suggested by (Riloff and Wiebe, 2003).1818Figure 1: Gender-dependent vs. independent subjectivityterms (+ and - indicates term polarity).Figure 2: The distribution of gender-dependent GDepand gender-independent GInd sentiment terms.in their subjective use when considering gender, butthere will be some words for which gender will havean influence.
Of particular interest for our work arewords in which the polarity of a term as it is used incontext is gender-influenced, the extreme case beingterms that flip their polarity depending on the genderof the user.
Polarity may be different because theconcept represented by the term tends to be viewedin a different light depending on gender.
There arealso words like weakness in which a more positive ormore negative word sense tends to be used by menor women.
In Figure 2 we show the distribution ofgender-specific and gender-independent terms fromthe LB lexicons for all languages.To identify gender-influenced terms in our lexi-cons, we start by randomly sampling 400K male and400K female tweets for each language from the data.Next, for both genders we calculate the probabilityof term ti appearing in a tweet with another subjec-tive term (Eq.1), and the probability of it appearingwith a positive or negative term (Eq.2-3) from LB .pti(subj?g) =c(ti, P, g) + c(ti,N, g)c(ti, g), (1)where g ?
F,M and P and N are positive and nega-tive sets of terms from the initial lexicon LI .pti(+?g) =c(ti, P, g)c(ti, P, g) + c(ti,N, g)(2)pti(?
?g) =c(ti,N, g)c(ti, P, g) + c(ti,N, g)(3)We introduce a novel metric ?p+ti to measure po-larity change across genders.
For every subjectiveterm ti we want to maximize the difference5:?p+ti = ?pti(+?F ) ?
pti(+?M)?s.t.RRRRRRRRRRRR1 ?tfsubjti (F )tfsubjti (M)RRRRRRRRRRRR?
?, tfsubjti (M) ?
0, (4)where p(+?F ) and p(+?M) are probabilities thatterm ti is positive for females and males respec-tively; tfsubjti (F ) and tfsubjti (M) are correspond-ing term frequencies (if tfsubjti (F ) > tfsubjti (M) thefraction is flipped); ?
is a threshold that controlsthe level of term frequency similarity6.
The termsin which polarity is most strongly gender-influencedare those with ??
0 and ?p+ti ?
1.Table 4 shows a sample of the most stronglygender-influenced terms from the initial LI and thebootstrapped LB lexicons for all languages.
A plus(+) means that the term tends to be used positivelyby women and minus (?)
means that the term tendsto be used positively by men.
For instance, in En-glish we found that perfecting is used with negativepolarity by male users but with positive polarity byfemale users; the term dogfighting has negative po-larity for women but positive polarity for men.4.3 HashtagsPeople may also express positive or negative senti-ment in their tweets using hashtags.
From our bal-anced samples of 800K tweets for each language,we extracted 611, 879, and 71 unique hashtags forEnglish, Spanish, and Russian, respectively.
As wedid for terms in the previous section, we evaluatedthe subjective use of the hashtags.
Some of these areclearly expressing sentiment (#horror), while othersseem to be topics that people are frequently opinion-ated about (#baseball, #latingrammy, #spartak).5One can also maximize ?p?ti = ?pti(?
?F ) ?
pti(??M)?.6?
= 0 means term frequencies are identical for both gen-ders; ??
1 indicates increasing gender divergence.1819English Initial Terms LEI ?p+ ?
English Bootstrapped Terms LEB ?p+ ?perfecting + 0.7 0.2 pleaseeeeee + 0.7 0.0weakened + 0.1 0.0 adorably + 0.6 0.4saddened ?
0.1 0.0 creatively ?
0.6 0.5misbehaving ?
0.4 0.0 dogfighting ?
0.7 0.5glorifying ?
0.7 0.5 overdressed ?
1.0 0.3Spanish Initial Terms LSI Spanish Bootstrapped Terms LSBfiasco (fiasco) + 0.7 0.3 cafe?na (caffeine) + 0.7 0.5triunfar (succeed) + 0.7 0.0 claro (clear) + 0.7 0.3inconsciente (unconscious) ?
0.6 0.2 cancio (dog) ?
0.3 0.3horroriza (horrifies) ?
0.7 0.3 llevara (take) ?
0.8 0.3groseramente (rudely) ?
0.7 0.3 recomendarlo (recommend) ?
1.0 0.0Russian Initial Terms LRI Russian Bootstrapped Terms LRB??????????
(magical) + 0.7 0.3 ????????
(dream!)
+ 0.7 0.3????????????
(sensational) + 0.7 0.3 ????????
(dancing) + 0.7 0.3?????????
(adorable) ?
0.7 0.0 ??????
(complicated) ?
1.0 0.0?????????
(temptation) ?
0.7 0.3 ???????????
(young) ?
1.0 0.0???????????
(deserve) ?
1.0 0.0 ???????
(achieve) ?
1.0 0.0Table 4: Sample of subjective terms sorted by ?p+ to show lexical differences and polarity change across genders(module is not applied as defined in Eq.1 to demonstrate the polarity change direction).English ?p+ ?
Spanish ?p+ ?
Russian ?p+ ?#parenting + 0.7 0.0 #rafaelnarro (politician) + 1.0 0.0 #?????
(advise) + 1.0 0.0#vegas ?
0.2 0.8 #amores (loves) + 0.2 1.0 #ukrlaw + 1.0 1.0#horror ?
0.6 0.7 #britneyspears + 0.1 0.3 #spartak (soccer team) ?
0.7 0.9#baseball ?
0.6 0.9 #latingrammy ?
0.5 0.1 #???
(dreams) ?
1.0 0.0#wolframalpha ?
0.7 1.0 #metallica (music band) ?
0.5 0.8 #iphones ?
1.0 1.0Table 5: Hashtag examples with opposite polarity across genders for English, Spanish, and Russian.Table 5 gives the hashtags, correlated with sub-jective language, that are most strongly gender-influenced.
Analogously to ?p+ values in Table 4, aplus (+) means the hashtag is more likely to be usedpositively by women, and a minus (?)
means thehashtag is more likely to be used positively by men.For example, in English we found that male userstend to express positive sentiment in tweets men-tioning #baseball, while women tend to be nega-tive about this hashtag.
The opposite is true for thehashtag #parenting.4.4 EmoticonsWe investigate how emoticons are used differentlyby men and women in social media following thework by (Bamman et al 2012).
For that we rely onthe lists of emoticons from Wikipedia7 and presentthe cross-cultural and gender emoticon differencesin Figure 3.
The frequency of each emoticon is given7List of emoticons from Wikipedia http://en.wikipedia.org/wiki/List_of_emoticonson the right of each language chart, with probabilityof use by a male user in that language given on thex-axis.
The top 8 emoticons are the same across lan-guages and sorted by English frequency.We found that emoticons in English data are usedmore overall by female users, which is consistentwith previous findings in Schnoebelen?s work.8 Inaddition, we found that some emoticons like :-)(smile face) and :-o (surprised) are used equally byboth genders, at least in Twitter.
When comparingEnglish emoticon usage to other languages, there aresome similarities, but also some clear differences.
InSpanish data, several emoticons are more likely to beused by male than by female users, e.g., :-o (sur-prised) and :-& (tongue-tied), and the difference inprobability of use by males and females is greaterfor the emoticons, as compared to the same emoti-cons for English.
Interestingly, in Russian Twitter8Language and emotion (talks, essays and reading notes)www.stanford.edu/~tylers/emotions.shtml1820p(Male|Emoticon)0.0 0.2 0.4 0.6 0.8 1.034.4K8.7K4.1K2.7K0.9K0.7K0.4K0.1K0.1K0.1K:):(:-):-&:-(:[:-/8)():-op(Male|Emoticon)0.0 0.2 0.4 0.6 0.8 1.019.1K9.5K1.5K0.1K0.3K0.3K0.1K1.5K0.1K0.1K:):(:-):-&:-(:[:-/8)%):-op(Male|Emoticon)0.0 0.2 0.4 0.6 0.8 1.041.5K4.5K4.6K0.4K0.4K0.1K0.1K0.4K0.4K0.1K:):(:-):-&:-(:[:-/8)%)()Figure 3: Probability of gender and emoticons for English, Spanish and Russian (from left to right).data emoticons tend to be used more or equally bymale users rather than female users.5 ExperimentsThe previous section showed that there are genderdifferences in the use of subjective language, hash-tags, and emoticons in Twitter.
We aim leveragethese differences to improve subjectivity and po-larity classification for the informal, creative anddynamically changing multilingual Twitter data.9For that we conduct experiments using gender-independent GInd and gender-dependent GDepfeatures and compare the results to evaluate the in-fluence of gender on sentiment classification.We experiment with two classification ap-proaches: (I) rule-based classifier which uses onlysubjective terms from the lexicons designed to verifyif the gender differences in subjective language cre-ate enough of a signal to influence sentiment classifi-cation; (II) state-of-the-art supervised models whichrely on lexical features as well as lexicon set-countfeatures.10,11 Moreover, to show that the gender-9For polarity classification we distinguish between positiveand negative instances, which is the approach typically reportedin the literature for recognizing polarity (Velikovich et al 2010;Yessenalina and Cardie, 2011; Taboada et al 2011)10A set-count feature is a count of the number of instancesfrom a set of terms that appears in a tweet.11We also experimented with repeated punctuation (!
!, ??
)and letters (nooo, reealy), which are often used in sentimentclassification in social media.
However, we found these featuressentiment signal can be learned by more than oneclassifier we apply a variety of classifiers imple-mented in Weka (Hall et al 2009).
For that we do10-fold cross validation over English, Spanish, andRussian test data (ETEST, STEST and RTEST) la-beled with subjectivity (pos, neg, both vs. neut) andpolarity (pos vs. neg) as described in Section 3.5.1 ModelsFor the rule-basedGIndRBsubj classifier, tweets are la-beled as subjective or neutral as follows:GIndRBsubj = {1 if w?
?
f?
?
0.5,0 otherwise(5)where w?
?
f?
stands for weighted set features, e.g.,terms from LI only, emoticons E, or different part-of-speech tags (POS) from LB weighted using w =p(subj) = p(subj?M) + p(subj?F ) subjectivityscore as shown in Eq.1.
We experiment with thePOS tags to show the contribution of each POS tosentiment classification.Similarly, for the rule-based GIndRBpol classifier,tweets are labeled as positive or negative:GIndRBpol = {1 if w?+ ?
f?+ ?
w??
?
f?
?,0 otherwise(6)where f?+, f??
are feature sets that include only posi-tive and negative features fromLI orLB;w+ andw?to be noisy and adding them decreased performance.18210.6 0.7 0.8 0.90.620.640.660.680.70RecallPrecision+E+A+R+V+NL_I+E+A+R+V+NL_I(a) Rule-based subjectivity0.65 0.70 0.75 0.80 0.85 0.900.650.700.750.800.85RecallPrecisionL_I+E+A+R+V+NL_I+E+A+R+V+N(b) Rule-based polarityBL R N B AB RF J48 SVMC lassifiersF-measure0.550.600.650.700.750.800.85GIn d SubjAN DGDepSubjAN DGIndPolAN DGDepPolAN D(c) SL subjectivity and polarityFigure 4: Rule-based (RB) and Supervised Learning (SL) sentiment classification results for English.
LI - the initiallexicon, E - emoticons, A,R,V,N are adjectives, adverbs, verbs, nouns from LB .are positive and negative polarity scores estimatedusing Eq.2 - 3 such as: w+ = p(+?M) + p(+?F ) andw?
= p(?
?M) + p(?
?F ).The gender-dependent rule-based classifiers aredefined in a similar way.
Specifically, f?
is replacedby f?M and f?F in Eq.5 and f?
?, f?+ are replacedby f?M?, f?F?
and f?M+, f?F+ respectively in Eq.6.We learn subjectivity s?
and polarity p?
score vectorsusing Eq.1-3.
The difference between GInd andGDep models is that GInd scores w?, w?+ and w?
?are not conditioned on gender.For gender-independent classification using su-pervised models, we build feature vectors using lex-ical features V represented as term frequencies, to-gether with set-count features from the lexicons:f?GIndsubj = [LI , LB,E, V ];f?GIndpol = [L+I , L+B,E+, L?I , L?B,E?, V ].Finally, for gender-dependent supervised models,we try different feature combinations.
(A) We ex-tract set-count features for gender-dependent subjec-tive terms from LI , LB, and E jointly:f?GDep?Jsubj = [LMI , LMB ,EM , LFI , LFB,EF , V ];f?Dep?Jpol = [LM+I , LM+B ,EM+, LF+I , LF+B ,EF+LM?I , LM?B ,EM?, LF?I , LF?B ,EF?, V ].
(B) We extract disjoint (prefixed) gender-specificfeatures (in addition to lexical features V ) by rely-ing only on female set-count features when classify-ing female tweets; and only male set-count featuresfor male tweets.
We refer to the joint features asGInd?J andGDep?J , and to the disjoint featuresGInd ?D and GDep ?D.5.2 ResultsFigures 4a and 4b show performance improvementsfor subjectivity and polarity classification under therule-based approach when taking into account gen-der.
The left figure shows precision-recall curvesfor subjective vs. neutral classification, and the mid-dle figure shows precision-recall curves for positivevs.
negative classification.
We measure performancestarting with features from LI , and then incremen-tally add emoticon features E and features from LBone part of speech at a time to show the contributionof each part of speech for sentiment classification.12This experiment shows that there is a clear improve-ment for the models parameterized with gender, atleast for the simple, rule-based model.For the supervised models we experiment witha variety of learners for English to show that gen-der differences in subjective language improve sen-timent classification for many learning algorithms.We present the results in Figure 4c.
For subjectiv-ity classification, Support Vector Machines (SVM),Naive Bayes (NB) and Bayesian Logistic Regres-sion (BLR) achieve the best results, with improve-ments in F-measure ranging from 0.5 - 5%.
The po-larity classifiers overall achieve much higher scores,with improvements for GDep features ranging from1-2%.
BLR with Gaussian prior is the top scorer12POS from the Twitter POSTagger (Gimpel et al 2011).1822P R F A Arand P R F A ArandEnglish subj vs. neutral p(subj)=0.57 English pos vs. neg p(pos)=0.63GIndLR 0.62 0.58 0.60 0.66 ?
0.78 0.83 0.80 0.71 ?GDep ?
J 0.64 0.62 0.63 0.68 0.66 0.80 0.83 0.82 0.73 0.70?R,% +3.23 +6.90 +5.00 +3.03 3.03?
+2.56 0.00 +2.50 +2.82 4.29?GIndSVM 0.66 0.70 0.68 0.72 ?
0.79 0.86 0.82 0.77 ?GDep ?D 0.66 0.71 0.68 0.72 0.70 0.80 0.87 0.83 0.78 0.76?R,% ?0.45 +0.71 0.00 ?0.14 2.85?
+0.38 +0.23 +0.24 +0.41 2.63?Spanish subj vs. neutral p(subj)=0.40 Spanish pos vs. neg p(pos)=0.45GIndLL 0.67 0.71 0.68 0.61 ?
0.71 0.63 0.67 0.71 ?GDep ?
J 0.67 0.72 0.69 0.62 0.61 0.72 0.65 0.68 0.71 0.68?R,% 0.00 +1.40 +0.58 +0.73 1.64?
+2.53 +3.17 +1.49 0.00 4.41?GIndSVM 0.68 0.79 0.73 0.65 ?
0.66 0.65 0.65 0.69 ?GDep ?D 0.68 0.79 0.73 0.66 0.65 0.68 0.67 0.67 0.71 0.68?R,% +0.35 +0.21 +0.26 +0.54 1.54?
+2.43 +2.44 +2.51 +2.08 4.41?Russian subj vs. neutral p(subj)=0.51 Russian pos vs. neg p(pos)=0.58GIndLR 0.66 0.68 0.67 0.67 ?
0.66 0.72 0.69 0.62 ?GDep ?
J 0.66 0.69 0.68 0.67 0.66 0.68 0.73 0.70 0.64 0.63?R,% 0.00 +1.47 +0.75 0.00 1.51?
+3.03 +1.39 +1.45 +3.23 1.58?GIndSVM 0.67 0.75 0.71 0.70 ?
0.64 0.73 0.68 0.62 ?GDep ?D 0.67 0.76 0.71 0.70 0.69 0.65 0.74 0.69 0.63 0.62?R,% ?0.30 +1.46 +0.56 +0.14 1.44?
+0.93 +1.92 +1.46 +1.49 1.61?Table 6: Sentiment classification results obtained using gender-dependent and gender-independent joint and disjointfeatures for Logistic Regression (LR) and SVM models.for polarity classification with an F-measure of 82%.We test our results for statistical significance us-ing McNemar?s Chi-squared test (p-value < 0.01) assuggested by Dietterich (1998).
Only three classi-fiers, J48, AdaBoostM1 (AB) and Random Forest(RF) do not always show significant improvementsfor GDep features over GInd features.
However,for the majority of classifiers, GDep models outper-formGIndmodels for both tasks, demonstrating therobustness of GDep features for sentiment analysis.In Table 6 we report results for subjectivity andpolarity classification using the best performingclassifiers (as shown in Figure 4c) :- Logistic Regression (LR) (Genkin et al 2007)for GInd ?
J and GDep ?
J models.- SVM model with radial-based kernel forGInd ?
D and GDep ?
D models.
We useLibSVM implementation (EL-Manzalawy andHonavar, 2005).Each ?R(%) row shows the relative percent im-provements in terms of precision P , recall R, F-measure F and accuracy A for GDep compared toGInd models.
Our results show that differences insubjective language across genders can be exploitedto improve sentiment analysis, not only for Englishbut for multiple languages.
For Spanish and Russianresults are lower for subjectivity classification, wesuspect, because lexical features V are already in-flected for gender and set-count features are down-weighted by the classifier.
For polarity classifica-tion, on the other hand, gender-dependent featuresprovide consistent, significant improvements (1.5-2.5%) across all languages.As a reality check, Table 6 also reports accuracies(in Arand columns) for experiments that use randompermutations of male and female subjective terms,which are then encoded as gender-dependent set-count features as before.
We found that all gender-dependent models, GDep ?
J and GDep ?D, out-performed their random equivalents for both subjec-tivity and polarity classification (as reflected by rel-ative accuracy decrease ?
forArand compared toA).These results further confirm the existence of gen-der bias in subjective language for any of our threelanguages and its importance for sentiment analysis.Finally, we check whether encoding gender asa binary feature would be sufficient to improvesentiment classification.
For that we encode fea-1823English Spanish RussianP R P R P R(a) 0.73 0.93 0.68 0.63 0.66 0.74(b) 0.72 0.94 0.69 0.64 0.66 0.74(c) 0.78 0.83 0.71 0.63 0.66 0.72(d) 0.69 0.93 0.71 0.62 0.65 0.76(e) 0.80 0.83 0.72 0.65 0.68 0.73Table 7: Precision and recall results for polarity classifi-cation: encoding gender as a binary feature vs. gender-dependent features GDep ?
J .tures such as: (a) unigram term frequencies V , (b)term frequencies and gender binary V +GBin, (c)gender-independent GInd, (d) gender-independentand gender binary GBin + GInd, and (e) gender-dependent GDep ?
J .
We train logistic-regressionmodel for polarity classification and report precisionand recall results in Table 7.
We observe that includ-ing gender as a binary feature does not yield signif-icant improvements compared to GDep ?
J for allthree languages.6 ConclusionsWe presented a qualitative and empirical study thatanalyses substantial and interesting differences insubjective language between male and female usersin Twitter, including hashtag and emoticon usageacross cultures.
We showed that incorporating au-thor gender as a model component can significantlyimprove subjectivity and polarity classification forEnglish (2.5% and 5%), Spanish (1.5% and 1%) andRussian (1.5% and 1%).
In future work we plan todevelop new models for joint modeling of personal-ized sentiment, user demographics e.g., age and userpreferences e.g., political favorites in social media.AcknowledgmentsThe authors thank the anonymous reviewers forhelpful comments and suggestions.ReferencesMuhammad Abdul-Mageed, Mona T. Diab, and Mo-hammed Korayem.
2011.
Subjectivity and sentimentanalysis of modern standard Arabic.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies: short papers - Volume 2, pages 587?591.Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,and Rebecca Passonneau.
2011.
Sentiment analysisof twitter data.
In Proceedings of the Workshop onLanguages in Social Media (LSM?11), pages 30?38.Shlomo Argamon, Moshe Koppel, James W. Pen-nebaker, and Jonathan Schler.
2007.
Min-ing the blogosphere: Age, gender and the va-rieties of self-expression.
First Monday, 12(9).http://www.firstmonday.org/ojs/index.php/fm/article/view/2003/1878.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.
InProceedings of the Seventh International Conferenceon Language Resources and Evaluation (LREC?10),pages 2200?2204.David Bamman, Jacob Eisenstein, and Tyler Schnoebe-len.
2012.
Gender in Twitter: styles, stances, and so-cial networks.
Computing Research Repository.Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008.A bootstrapping method for building subjectivity lex-icons for languages with scarce resources.
In Pro-ceedings of the Sixth International Conference on Lan-guage Resources and Evaluation (LREC?08), pages2764?2767.Luciano Barbosa and Junlan Feng.
2010.
Robust sen-timent detection on Twitter from biased and noisydata.
In Proceedings of the 23rd International Con-ference on Computational Linguistics (COLING?10),pages 36?44.Shane Bergsma, Paul McNamee, Mossaab Bagdouri,Clayton Fink, and Theresa Wilson.
2012.
Languageidentification for creating language-specific Twittercollections.
In Proceedings of the Second Workshopon Language in Social Media (LSM?12), pages 65?74.Adam Bermingham and Alan F. Smeaton.
2010.
Clas-sifying sentiment in microblogs: Is brevity an advan-tage?
In Proceedings of the 19th ACM InternationalConference on Information and Knowledge Manage-ment (CIKM?10), pages 1833?1836.Albert Bifet and Eibe Frank.
2010.
Sentiment knowl-edge discovery in Twitter streaming data.
In Proceed-ings of the 13th International Conference on DiscoveryScience (DS?10), pages 1?15.Bonka Boneva, Robert Kraut, and David Frohlich.
2001.Using email for personal relationships: The differ-ence gender makes.
American Behavioral Scientist,45(3):530?549.John D. Burger, John C. Henderson, George Kim, andGuido Zarrella.
2011.
Discriminating gender on Twit-ter.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1301?1309.1824Pedro Henrique Calais Guerra, Adriano Veloso, WagnerMeira Jr, and Virg?lio Almeida.
2011.
From bias toopinion: a transfer-learning approach to real-time sen-timent analysis.
In Proceedings of the17th Interna-tional Conference on Knowledge Discovery and DataMining (KDD?11), pages 150?158.Lu Chen, Wenbo Wang, Meenakshi Nagarajan, ShaojunWang, and Amit P. Sheth.
2012.
Extracting diversesentiment expressions with target-dependent polarityfrom Twitter.
In Proceedings of the Sixth Interna-tional AAAI Conference on Weblogs and Social Media(ICWSM?12), pages 50?57.Ilia Chetviorkin and Natalia V. Loukachevitch.
2012.Extraction of Russian sentiment lexicon for prod-uct meta-domain.
In Proceedings of the 25rd In-ternational Conference on Computational Linguistics(COLING?12), pages 593?610.Christopher Cieri, David Miller, and Kevin Walker.2004.
The Fisher corpus: a resource for the next gen-erations of speech-to-text.
In Proceedings of the 4thInternational Conference on Language Resources andEvaluation (LREC?04), pages 69?71.Simon Clematide and Manfred Klenner.
2010.
Eval-uation and extension of a polarity lexicon for Ger-man.
In Proceedings of the Workshop on Computa-tional Approaches to Subjectivity and Sentiment Anal-ysis (WASSA?10), pages 7?13.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using Twitter hashtagsand smileys.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics(COLING?10), pages 241?249.Thomas G. Dietterich.
1998.
Approximate statisticaltests for comparing supervised classification learningalgorithms.
Neural Computation, 10(7):1895?1923.Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,and Eric P. Xing.
2010.
A latent variable model for ge-ographic lexical variation.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP?10), pages 1277?1287.Yasser EL-Manzalawy and Vasant Honavar, 2005.WLSVM: Integrating LibSVM into Weka Environment.http://www.cs.iastate.edu/ yasser/wlsvm.Teng-Kai Fan and Chia-Hui Chang.
2009.
Sentiment-oriented contextual advertising.
Advances in Informa-tion Retrieval, 5478:202?215.Nikesh Garera and David Yarowsky.
2009.
Modeling la-tent biographic attributes in conversational genres.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, pages 710?718.David Gefen and Catherine M. Ridings.
2005.
If youspoke as she does, sir, instead of the way you do: a so-ciolinguistics perspective of gender differences in vir-tual communities.
SIGMIS Database, 36(2):78?92.Alexander Genkin, David D. Lewis, and David Madigan.2007.
Large-scale Bayesian logistic regression for textcategorization.
Technometrics, 49:291?304.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging forTwitter: annotation, features, and experiments.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies: short papers - Volume 2, pages 42?47.John J. Godfrey, Edward C. Holliman, and Jane Mc-Daniel.
1992.
Switchboard: telephone speech corpusfor research and development.
In Proceedings of theIEEE International Conference on Acoustics, Speech,and Signal Processing (ICASSP?92), pages 517?520.Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.2009.
Stylometric analysis of bloggers age and gen-der.
In Proceedings of AAAI Conference on Weblogsand Social Media, pages 214?217.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.SIGKDD Exploratory Newsletter, 11(1):10?18.Janet Holmes and Miriam Meyerhoff.
2004.
The Hand-book of Language and Gender.
Blackwell Publishing.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and TiejunZhao.
2011.
Target-dependent Twitter sentiment clas-sification.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies, pages 151?160.Nobuhiro Kaji and Masaru Kitsuregawa.
2007.
Buildinglexicon for sentiment analysis from massive collectionof HTML documents.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP?07), pages 1075?1083.Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter sentiment analysis: The goodthe bad and the OMG!
In Proceedings of the Fifth In-ternational AAAI Conference on Weblogs and SocialMedia (ICWSM?11), pages 538?541.Guangxia Li, Steven Hoi, Kuiyu Chang, and RameshJain.
2010.
Micro-blogging sentiment detectionby collaborative online learning.
In Proceedings ofIEEE 10th International Conference on Data Mining(ICDM?10), pages 893?898.Hao Li, Yu Chen, Heng Ji, Smaranda Muresan, andDequan Zheng.
2012.
Combining social cognitivetheories with linguistic features for multi-genre senti-ment analysis.
In Proceedings of the 26th Pacific Asia1825Conference on Language,Information and Computa-tion (PACLIC?12), pages 27?136.Ronald Macaulay.
2006.
Pure grammaticalization: Thedevelopment of a teenage intensifier.
Language Varia-tion and Change, 18(03):267?283.Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2012.Multilingual subjectivity and sentiment analysis.
InProceedings of the Association for Computational Lin-guistics (ACL?12).George A. Miller.
1995.
Wordnet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41.Saif Mohammad and Tony Yang.
2011.
Tracking senti-ment in mail: How genders differ on emotional axes.In Proceedings of the 2nd Workshop on Computa-tional Approaches to Subjectivity and Sentiment Anal-ysis (WASSA?11), pages 70?79.Arjun Mukherjee and Bing Liu.
2010.
Improving gen-der classification of blog authors.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP?10), pages 207?217.Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, andNoah A. Smith.
2010.
A mixture model of de-mographic lexical variation.
In Proceedings of NIPSWorkshop on Machine Learning in Computational So-cial Science, pages 1?7.Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-cock.
2011.
Finding deceptive opinion spam by anystretch of the imagination.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages309?319.Jahna Otterbacher.
2010.
Inferring gender of movie re-viewers: exploiting writing style, content and meta-data.
In Proceedings of the 19th ACM InternationalConference on Information and Knowledge Manage-ment (CIKM?10), pages 369?378.Alexander Pak and Patrick Paroubek.
2010.
Twitter as acorpus for sentiment analysis and opinion mining.
InProceedings of the Seventh International Conferenceon Language Resources and Evaluation (LREC?10),pages 1320?1326.Veronica Perez-Rosas, Carmen Banea, and Rada Mihal-cea.
2012.
Learning sentiment lexicons in Spanish.In Proceedings of the 8th International Conferenceon Language Resources and Evaluation (LREC?12),pages 3077?3081.Rosalind W. Picard.
1997.
Affective computing.
MITPress.Delip Rao and Deepak Ravichandran.
2009.
Semi-supervised polarity lexicon induction.
In Proceedingsof the 12th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL?09),pages 675?682.Delip Rao, David Yarowsky, Abhishek Shreevats, andManaswi Gupta.
2010.
Classifying latent user at-tributes in Twitter.
In Proceedings of the Work-shop on Search and Mining User-generated Contents(SMUC?10), pages 37?44.Philip Resnik.
2013.
Getting real(-time) with livepolling.
http://vimeo.com/68210812.Ellen Riloff and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing (EMNLP?03), pages 105?112.Harold Schiffman.
2002.
Bibliography of gender andlanguage.
http://ccat.sas.upenn.edu/ haroldfs/popcult/bibliogs/gender/genbib.htm.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast?but is itgood?
: evaluating non-expert annotations for naturallanguage tasks.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP?08), pages 254?263.Will Styler.
2011.
The EnronSent Corpus.Technical report, University of Coloradoat Boulder Institute of Cognitive Science.http://verbs.colorado.edu/enronsent/.Jane Sunderland, Ren-Feng Duann, and PaulBake.
2002.
Gender and genre bibliography.www.ling.lancs.ac.uk/pubs/clsl/clsl122.pdf.Maite Taboada, Julian Brooke, Milan Tofiloski, KimberlyVoll, and Manfred Stede.
2011.
Lexicon-based meth-ods for sentiment analysis.
Computational Linguis-tics, 37(2):267?307.Sali A. Tagliamonte.
2006.
Analysing SociolinguisticVariation.
Cambridge University Press, 1st.
Edition.Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, MingZhou, and Ping Li.
2011.
User-level sentiment anal-ysis incorporating social networks.
In Proceedings ofthe 17th International Conference on Knowledge Dis-covery and Data Mining (KDD?11), pages 1397?1405.Mike Thelwall, David Wilkinson, and Sukhvinder Uppal.2010.
Data mining emotion in social network com-munication: Gender differences in MySpace.
Journalof the American Society for Information Science andTechnology, 61(1):190?199.Peter D. Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the 40th AnnualMeeting on Association for Computational Linguistics(ACL?02), pages 417?424.Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-nan, and Ryan McDonald.
2010.
The viabilityof web-derived polarity lexicons.
In Proceedings of1826the Annual Conference of the North American Chap-ter of the Association for Computational Linguistics(NAACL?10), pages 777?785.Svitlana Volkova, Theresa Wilson, and David Yarowsky.2013.
Exploring sentiment in social media: Boot-strapping subjectivity clues from multilingual Twitterstreams.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(ACL?13), pages 505?510.Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou, andMing Zhang.
2011.
Topic sentiment analysis in Twit-ter: A graph-based hashtag sentiment classification ap-proach.
In Proceedings of the 20th ACM InternationalConference on Information and Knowledge Manage-ment (CIKM?11), pages 1031?1040.Janyce Wiebe.
2000.
Learning subjective adjectivesfrom corpora.
In Proceedings of the SeventeenthNational Conference on Artificial Intelligence andTwelfth Conference on Innovative Applications of Ar-tificial Intelligence (AAAI?00, pages 735?740.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP?05), pages 347?354.Ainur Yessenalina and Claire Cardie.
2011.
Composi-tional matrix-space models for sentiment analysis.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP?11), pages172?182.1827
