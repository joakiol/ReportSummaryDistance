First Joint Conference on Lexical and Computational Semantics (*SEM), pages 70?74,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsLearning Semantics and Selectional Preference of Adjective-Noun PairsKarl Moritz HermannDepartment of Computer ScienceUniversity of OxfordOxford OX1 3QD, UKkarl.moritz.hermann@cs.ox.ac.ukChris DyerLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USAcdyer@cs.cmu.eduPhil BlunsomDepartment of Computer ScienceUniversity of OxfordOxford OX1 3QD, UKphil.blunsom@cs.ox.ac.ukStephen PulmanDepartment of Computer ScienceUniversity of OxfordOxford OX1 3QD, UKstephen.pulman@cs.ox.ac.ukAbstractWe investigate the semantic relationship be-tween a noun and its adjectival modifiers.We introduce a class of probabilistic mod-els that enable us to to simultaneously cap-ture both the semantic similarity of nounsand modifiers, and adjective-noun selectionalpreference.
Through a combination of noveland existing evaluations we test the degree towhich adjective-noun relationships can be cat-egorised.
We analyse the effect of lexical con-text on these relationships, and the efficacy ofthe latent semantic representation for disam-biguating word meaning.1 IntroductionDeveloping models of the meanings of words andphrases is a key challenge for computational linguis-tics.
Distributed representations are useful in captur-ing such meaning for individual words (Sato et al,2008; Maas and Ng, 2010; Curran, 2005).
How-ever, finding a compelling account of semantic com-positionality that utilises such representations hasproven more difficult and is an active research topic(Mitchell and Lapata, 2008; Baroni and Zamparelli,2010; Grefenstette and Sadrzadeh, 2011).
It is inthis area that our paper makes its contribution.The dominant approaches to distributional se-mantics have relied on relatively simple frequencycounting techniques.
However, such approaches failto generalise to the much sparser distributions en-countered when modeling compositional processesand provide no account of selectional preference.We propose a probabilistic model of the semanticrepresentations for nouns and modifiers.
The foun-dation of this model is a latent variable representa-tion of noun and adjective semantics together withtheir compositional probabilities.
We employ thisformulation to give a dual view of noun-modifiersemantics: the induced latent variables provide anexplicit account of selectional preference while themarginal distributions of the latent variables for eachword implicitly produce a distributed representation.Most related work on selectional preference usesclass-based probabilities to approximate (sparse)individual probabilities.
Relevant papers includeO?
Se?aghdha (2010), who evaluates several topicmodels adapted to learning selectional preferenceusing co-occurence and Baroni and Zamparelli(2010), who represent nouns as vectors and adjec-tives as matrices, thus treating them as functionsover noun meaning.
Again, inference is achievedusing co-occurrence and dimensionality reduction.2 Adjective-Noun ModelWe hypothesize that semantic classes determine thesemantic characteristics of nouns and adjectives, andthat the distribution of either with respect to othercomponents of the sentences they occur in is alsomediated by these classes (i.e., not by the wordsthemselves).
We assume that in general nouns selectfor adjectives,1 and that this selection is dependenton both their latent semantic classes.
In the next sec-tion, we describe a model encoding our hypotheses.2.1 Generative ProcessWe model a corpus D of tuples of the form(n,m, c1 .
.
.
ck) consisting of a noun n, an adjectivem (modifier), and k words of context.
The contextvariables (c1 .
.
.
ck) are treated as a bag of words and1We evaluate this hypothesis as well as its inverse.70|N| |M||N|N Mn mck|D|?N?N?M?M?c?c|N|?n?m?n?mFigure 1: Plate diagram illustrating our model of nounand modifier semantic classes (designated N and M , re-spectively), a modifier-noun pair (m,n), and its context.include the words to the left and right of the noun,its siblings and governing verbs.
We designate thevocabulary Vn for nouns, Vm for modifiers and Vcfor context.
We use zi to refer to the ith tuple in Dand refer to variables within that tuple by subscript-ing them with i, e.g., ni and c3,i are the noun andthe third context variable of zi.
The latent noun andadjective class variables are designated Ni and Mi.The corpus D is generated according to the platediagram in figure 1.
First, a set of parameters isdrawn.
A multinomial ?N representing the distribu-tion of noun semantic classes in the corpus is drawnfrom a Dirichlet distribution with parameter ?N.
Foreach noun class i we have distributions ?Mi overadjective classes, ?ni over Vn and ?ci over Vc, alsodrawn from Dirichlet distributions.
Finally, for eachadjective class j, we have distributions ?mj over Vm.Next, the contents of the corpus are generated byfirst drawing the length of the corpus (we do notparametrise this since we never generate from thismodel).
Then, for each i, we generate noun classNi, adjective class Mi, and the tuple zi as follows:Ni | ?N ?
Multi(?N)Mi | ?MNi?
Multi(?MNi)ni | ?nNi?
Multi(?nNi)mi | ?mMi?
Multi(?mMi)?k: ck,i | ?cNi?
Multi(?cNi)2.2 Parameterization and InferenceWe use Gibbs sampling to estimate the distributionsofN andM , integrating out the multinomial param-eters ?x (Griffiths and Steyvers, 2004).
The Dirich-let parameters ?
are drawn independently from a?
(1, 1) distribution, and are resampled using slicesampling at frequent intervals throughout the sam-pling process (Johnson and Goldwater, 2009).
This?vague?
prior encourages sparse draws from theDirichlet distribution.
The number of noun and ad-jective classes N and M was set to 50 each; othersizes (100,150) did not significantly alter results.3 ExperimentsAs our model was developed on the basis of severalhypotheses, we design the experiments and evalu-ation so that these hypotheses can be examined ontheir individual merit.
We test the first hypothesis,that nouns and adjectives can be represented by se-mantic classes, recoverable using co-occurence, us-ing a sense clustering evaluation by Ciaramita andJohnson (2003).
The second hypothesis, that the dis-tribution with respect to context and to each other isgoverned by these semantic classes is evaluated us-ing pseudo-disambiguation (Clark and Weir, 2002;Pereira et al, 1993; Rooth et al, 1999) and bigramplausibility (Keller and Lapata, 2003) tests.To test whether noun classes indeed select for ad-jective classes, we also evaluate an inverse model(Modi), where the adjective class is drawn first, inturn generating both context and the noun class.
Inaddition, we evaluate copies of both models ignoringcontext (Modnc and Modinc).We use the British National Corpus (BNC), train-ing on 90 percent and testing on 10 percent of thecorpus.
Results are reported after 2,000 iterationsincluding a burn-in period of 200 iterations.
Classesare marginalised over every 10th iteration.4 Evaluation4.1 Supersense TaggingSupersense tagging (Ciaramita and Johnson, 2003;Curran, 2005) evaluates a model?s ability to clus-ter words by their semantics.
The task of this eval-uation is to determine the WORDNET supersensesof a given list of nouns.
We report results on theWN1.6 test set as defined by Ciaramita and John-son (2003), who used 755 randomly selected nounswith a unique supersense from the WORDNET 1.671corpus.
As their test set was random, results weren?texactly replicable.
For a fair comparison, we selectall suitable nouns from the corpus that also appearedin the training corpus.
We report results on type andtoken level (52314 tokens with 1119 types).
Thebaseline2 chooses the most common supersense.k Token TypeBaseline .241 .210Ciaramita & Johnson .523 .534Curran - .680Mod 10 .592 .517Modnc 10 .473 .410Table 1: Supersense evaluation results.
Values are thepercentage of correctly assigned supersenses.
k indicatesthe number of nearest neighbours considered.We use cosine-similarity on the marginal nounclass vectors to measure distance between nouns.Each noun in the test set is then assigned a su-persense by performing a distance-weighted votingamong its k nearest neighbours.
Results of this eval-uation are shown in Table 1, with Figure 2 showingscores for model Mod across different values for k.Figure 2: Scores of Mod on the supersense task.
The up-per line denotes token-, the lower type-level scores.
They-axis is the percentage of correct assignments, the x-axisdenotes the number of neighbours included in the vote.The results demonstrate that nouns can semanti-cally be represented as members of latent classes,while the superiority of Mod over Modnc supportsour hypothesis that context co-occurence is a keyfeature for learning these classes.4.2 Pseudo-DisambiguationPseudo-disambiguation was introduced by Clarkand Weir (2002) to evaluate models of selectionalpreference.
The task is to select the more probableof two candidate arguments to associate with a given2The baseline results are from Ciaramita and Johnson(2003).
Using the majority baseline on the full test set, we onlyget .176 and .160 for token and type respectively.predicate.
For us, this is to decide which adjective,a1 or a2, is more likely to modify a noun n.We follow the approach by Clark and Weir (2002)to create the test data.
To improve the quality ofthe data, we filtered using bigram counts from theWeb1T corpus, setting a lower bound on the proba-ble bigram (a1, n) and chosing a2 from five candi-dates, picking the lowest count for bigram (a2, n).We report results for all variants of our model inTable 2.
As baseline we use unigram counts in ourtraining data, chosing the more frequent adjective.L-bound 0 100 500 1000Size 5714 5253 3741 2789Baseline .543 .543 .539 .550Mod .783 .792 .810 .816Modi .781 .787 .800 .810Modnc .720 .728 .746 .750Modinc .722 .730 .747 .752Table 2: Pseudo-disambiguation: Percentage of correctchoices made.
L-bound denotes the Web1T lower boundon the (a1, n) bigram, size the number of decisions made.While all models decisively beat the baseline, themodels using context strongly outperform those thatdo not.
This supports our hypothesis regarding theimportance of context in semantic clustering.The similarity between the normal and inversemodels implies that the direction of the noun-adjective relationship has negligible impact for thisevaluation.4.3 Bigram PlausibilityBigram plausibility (Keller and Lapata, 2003) is asecond evaluation for selectional preference.
Unlikethe frequency-based pseudo-disambiguation task, itevaluates how well a model matches human judge-ment of the plausibility of adjective-noun pairs.Keller and Lapata (2003) demonstrated a correlationbetween frequencies and plausibility, but this doesnot sufficiently explain human judgement.
An ex-ample taken from their unseen data set illustrates thedissociation between frequency and plausibility:?
Frequent, implausible: ?educational water??
Infrequent, plausible: ?difficult foreigner?3The plausibility evaluation has two data sets of 90adjective-noun pairs each.
The first set (seen) con-tains random bigrams from the BNC.
The second set(unseen) are bigrams not contained in the BNC.3At the time of writing, Google estimates 56,900 hits for?educational water?
and 575 hits for ?difficult foreigner?.
?Ed-ucational water?
ranks bottom in the gold standard of the unseenset, ?difficult foreigner?
ranks in the top ten.72Recent work (O?
Se?aghdha, 2010; Erk et al,2010) approximated plausibility with joint probabil-ity (JP).
We believe that for semantic plausibility(not probability!)
mutual information (MI), whichfactors out acutal frequencies, is a better metric.4 Wereport results using JP, MI and MI?2.Seen Unseenr ?
r ?AltaVista .650 ?
.480 ?BNC (Rasp) .543 .622 .135 .102Pado?
et al .479 .570 .120 .138LDA .594 .558 .468 .459ROOTH-LDA .575 .599 .501 .469DUAL-LDA .460 .400 .334 .278Mod (JP) .495 .413 .286 .276Mod (MI) .394 .425 .471 .457Mod (MI?2) .575 .501 .430 .408Modnc (JP) .626 .505 .357 .369Modnc (MI) .628 .574 .427 .385Modnc (MI?2) .701 .623 .423 .394Table 3: Results (Pearson r and Spearman ?
correlations)on the Keller and Lapata (2003) plausibility data.
Boldindicates best scores, underlining our best scores.
Highvalues indicate high correlation with the gold standard.Table 3 shows the performance of our modelscompared to results reported in O?
Se?aghdha (2010).As before, results between the normal and the in-verse model (omitted due to space) are very simi-lar.
Surprisingly, the no-context models consistentlyoutperform the models using context on the seendata set.
This suggests that the seen data set canquite precisely be ranked using frequency estimates,which the no-context models might be better at cap-turing without the ?noise?
introduced by context.Standard Inverse (i)r ?
r ?Mod (JP) .286 .276 .243 .245Mod (MI) .471 .457 .409 .383Mod (MI?2) .430 .408 .362 .347Modnc (JP) .357 .369 .181 .161Modnc (MI) .427 .385 .220 .209Modnc (MI?2) .423 .394 .218 .185Table 4: Results on the unseen plausibility dataset.The results on the unseen data set (Table 4)prove interesting as well.
The inverse no-contextmodel is performing significantly poorer than anyof the other models.
To understand this result wemust investigate the differences between the unseendata set and the seen data set and to the pseudo-disambiguation evaluation.
The key difference topseudo-disambiguation is that we measure a human4See (Evert, 2005) for a discussion of these metrics.plausibility judgement, which ?
as we have demon-strated ?
only partially correlates with bigram fre-quencies.
Our models were trained on the BNC,hence they could only learn frequency estimates forthe seen data set, but not for the unseen data.Based on our hypothesis about the role of con-text, we expect Mod and Modi to learn semanticclasses based on the distribution of context.
Withoutthe access to that context, we argued thatModnc andModinc would instead learn frequency estimates.5The hypothesis that nouns generally select for ad-jectives rather than vice versa further suggests thatMod and Modnc would learn semantic propertiesthat Modi and Modinc could not learn so well.In summary, we hence expected Mod to performbest on the unseen data, learning semantics fromboth context and noun-adjective selection.
Also, assupported by the results, we expected Modinc toperforms poorly, as it is the model least capable oflearning semantics according to our hypotheses.5 ConclusionWe have presented a class of probabilistic mod-els which successfully learn semantic clusterings ofnouns and a representation of adjective-noun selec-tional preference.
These models encoded our beliefsabout how adjective-noun pairs relate to each otherand to the other words in the sentence.
The perfor-mance of our models on estimating selectional pref-erence strongly supported these initial hypotheses.We discussed plausibility judgements from a the-oretical perspective and argued that frequency esti-mates and JP are imperfect approximations for plau-sibility.
While models can perform well on someevaluations by using either frequency estimates orsemantic knowledge, we explained why this doesnot apply to the unseen plausibility test.
The perfor-mance on that task demonstrates both the success ofour model and the shortcomings of frequency-basedapproaches to human plausibility judgements.Finally, this paper demonstrated that it is feasi-ble to learn semantic representations of words whileconcurrently learning how they relate to one another.Future work will explore learning words frombroader classes of semantic relations and the role ofcontext in greater detail.
Also, we will evaluate thesystem applied to higher level tasks.5This could also explain their weaker performance onpseudo-disambiguation in the previous section, where the neg-ative examples had zero frequency in the training corpus.73ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 1183?1193, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Massimiliano Ciaramita and Mark Johnson.
2003.
Su-persense tagging of unknown nouns in wordnet.
InProceedings of the 2003 conference on Empiricalmethods in natural language processing, EMNLP ?03,pages 168?175, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Stephen Clark and David Weir.
2002.
Class-based prob-ability estimation using a semantic hierarchy.
Comput.Linguist., 28:187?206, June.James R. Curran.
2005.
Supersense tagging of unknownnouns using semantic similarity.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics, ACL ?05, pages 26?33, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Katrin Erk, Sebastian Pado?, and Ulrike Pado?.
2010.
Aflexible, corpus-driven model of regular and inverseselectional preferences.
Computational Linguistics,36:723?763.Stefan Evert.
2005.
The statistics of word cooccur-rences: word pairs and collocations.
Ph.D. the-sis, Universita?t Stuttgart, Holzgartenstr.
16, 70174Stuttgart.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical compositionaldistributional model of meaning.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?11, pages 1394?1404,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101:5228?5235.Mark Johnson and Sharon Goldwater.
2009.
Improvingnonparameteric bayesian inference: experiments onunsupervised word segmentation with adaptor gram-mars.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, NAACL ?09, pages 317?325, Stroudsburg,PA, USA.
Association for Computational Linguistics.Frank Keller and Mirella Lapata.
2003.
Using the web toobtain frequencies for unseen bigrams.
ComputationalLinguistics, pages 459?484.Andrew L. Maas and Andrew Y. Ng.
2010.
A probabilis-tic model for semantic word vectors.
In Workshop onDeep Learning and Unsupervised Feature Learning,NIPS ?10.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In ACL-HLT?08,pages 236 ?
244.Diarmuid O?
Se?aghdha.
2010.
Latent variable modelsof selectional preference.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 435?444, Stroudsburg,PA, USA.
Association for Computational Linguistics.Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993.Distributional clustering of English words.
In Pro-ceedings of the 31st annual meeting on Association forComputational Linguistics, ACL ?93, pages 183?190,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-roll, and Franz Beil.
1999.
Inducing a semanticallyannotated lexicon via EM-based clustering.
In Pro-ceedings of the 37th annual meeting of the Associationfor Computational Linguistics on Computational Lin-guistics, ACL ?99, pages 104?111, Stroudsburg, PA,USA.
Association for Computational Linguistics.Issei Sato, Minoru Yoshida, and Hiroshi Nakagawa.2008.
Knowledge discovery of semantic relationshipsbetween words using nonparametric bayesian graphmodel.
In Proceeding of the 14th ACM SIGKDD in-ternational conference on Knowledge discovery anddata mining, KDD ?08, pages 587?595, New York,NY, USA.
ACM.74
