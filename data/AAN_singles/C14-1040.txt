Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 409?420, Dublin, Ireland, August 23-29 2014.Machine Translation Quality Estimation Across DomainsJos?e G. C. de SouzaUniversity of TrentoFondazione Bruno KesslerTrento, Italydesouza@fbk.euMarco TurchiFondazione Bruno KesslerTrento, Italyturchi@fbk.euMatteo NegriFondazione Bruno KesslerTrento, Italynegri@fbk.euAbstractMachine Translation (MT) Quality Estimation (QE) aims to automatically measure the qualityof MT system output without reference translations.
In spite of the progress achieved in re-cent years, current MT QE systems are not capable of dealing with data coming from differenttrain/test distributions or domains, and scenarios in which training data is scarce.
We investigatedifferent multitask learning methods that can cope with such limitations and show that they over-come current state-of-the-art methods in real-world conditions where training and test data comefrom different domains.1 IntroductionMachine Translation (MT) Quality Estimation (QE) aims to automatically predict the quality of MToutput without using reference translations (Blatz et al., 2003; Specia et al., 2009).
QE systems usuallyemploy supervised machine learning models that use different information extracted from (source, target)sentence pairs as features along with quality scores as labels.
The notion of quality that these modelsmeasure can be indicated by different scores.
Some examples are the average number of edits requiredto post-edit the MT output, i.e., human translation edit rate1(HTER (Snover et al., 2006)), and the time(in seconds) required to post-edit a translation produced by an MT system (Specia, 2011).Research on QE has received a strong boost in recent years due to the increase in the usage of MTsystems in real-world applications.
Automatic and reference-free MT quality prediction demonstratedto be useful for different applications, such as: deciding whether the translation output can be publishedwithout post-editing (Soricut and Echihabi, 2010), filtering out low-quality translation suggestions thatshould be rewritten from scratch (Specia et al., 2009), selecting the best translation output from a poolof MT systems (Specia et al., 2010), and informing readers of the translation whether it is reliable or not(Turchi et al., 2012).
Another example is the computer-assisted translation (CAT) scenario, in which itmight be necessary to predict the quality of translation suggestions generated by different MT systemsto support the activity of post editors working with different genres of text.The dominant QE framework presents some characteristics that can limit models?
applicability insuch real-world scenarios.
First, the scores used as training labels (HTER, time) are costly to obtainbecause they are derived from manual post-editions of MT output.
Such requirement makes it difficultto develop models for domains in which there is a limited amount of labeled data.
Second, the learningmethods currently used (for instance in the framework of QE shared evaluation campaigns)2assume thattraining and test data are sampled from the same distribution.
Though reasonable as a first evaluationsetting to promote research in the field, this controlled scenario is not realistic as different data in real-world applications might be post-edited by different translators, the translations might be generated bydifferent MT systems and the documents being translated might belong to different domains or genres.
ToThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1Edit distance is calculated as the number of edits (word insertions, deletions, substitutions, and shifts) divided by thenumber of words in the reference.
Lower HTER values indicate better translations.2In the last two editions of the yearly Workshop on Machine Translation, several QE shared tasks have been proposed(Callison-Burch et al., 2012; Bojar et al., 2013).409overcome these limitations a plausible research objective is to exploit techniques that: (i) allow domainsand distributions of features to be different between training and test data, and (ii) that cope with thescarce amount of training labels by sharing information across domains, a common scenario for transferlearning.In this paper we investigate the use of techniques that can exploit the training instances from differentdomains to learn a QE model for a specific target domain for which there is a small amount of labeleddata.
In particular, we are interested in approaches that allow not only learning from one single sourcedomain but also from multiple source domains simultaneously, by leveraging the labels from all availabledata to improve results in a target domain.Given these requirements, we experiment with different multitask learning techniques that performtransfer learning via a common task structure (domain relatedness).
Furthermore, we employ an approachbased on feature augmentation that has been successfully used in other natural language processing tasks.We present a series of experiments over three domains with increasing amounts of training data, showingthat our adaptive approaches outperform competitive baselines.The contributions of our work are: (i) a first exploration of techniques that overcome the limitationof current QE learning methods when dealing with data with different training and test distributions anddomains, and (ii) an empirical verification of the amount of training data required by such techniques tooutperform competitive baselines on different target domains.
To the best of our knowledge, this is thefirst work addressing the challenges posed by domain adaptation in MT QE.2 Related WorkQuality estimation has recently gained increasing attention, also boosted by two evaluation campaignsorganized within the Workshop on Machine Translation (WMT) (Callison-Burch et al., 2012; Bojar etal., 2013).
The bulk of work done so far has focused on the controlled WMT evaluation framework and,in particular, on two major aspects of the problem: feature engineering and machine learning methods.Feature engineering accounts for linguistically-based predictors that aim to model different perspec-tives of the quality estimation problem.
The research ranges from identifying indicators that approximatethe complexity of translating the source sentence and designing features that model the fluency of theautomatically generated translation, to linguistically motivated measures that estimate how adequate thetranslation is in comparison to the source sentence in terms of meaning (Blatz et al., 2003; Mehdad etal., 2012; Hardmeier et al., 2012; Rubino et al., 2012; Specia et al., 2012; de Souza et al., 2013a).State-of-the-art QE explores different supervised linear or non-linear learning methods for regressionor classification such as Support Vector Machines (SVM), different types of Decision Trees, NeuralNetworks, Elastic-Net, Gaussian Processes, Naive Bayes, among others (Specia et al., 2009; Buck,2012; Beck et al., 2013; Souza et al., 2014).
Another aspect related to the learning methods that hasreceived attention is the optimal selection of features in order to overcome issues related with the high-dimensionality of the feature space (Soricut et al., 2012; de Souza et al., 2013a; Beck et al., 2013; deSouza et al., 2013b).Despite constant improvements, such learning methods have limitations.
The main one is that theyassume that both training and test data are independently and identically distributed.
As a consequence,when they are applied to data from a different distribution or domain they show poor performance.
Thislimitation harms the performance of QE systems for several real-world applications, such as CAT envi-ronments.
Advanced CAT systems currently integrate suggestions obtained from MT engines with thosederived from translation memories (TMs).
In such framework, the compelling need to speed up the trans-lation process and reduce its costs by presenting human translators with good-quality suggestions raisesinteresting research challenges for the QE community.
In such environments, translation jobs come fromdifferent domains that might be translated by different MT systems and are routed to professional transla-tors with different idiolect, background and quality standards (Turchi et al., 2013).
Such variability callsfor flexible and adaptive QE solutions by investigating two directions: (i) modeling translator behaviour(Turchi et al., 2014) and (ii) maximize the learning capabilities from all the available data.
The secondresearch objective motivates our investigation on methods that allow the training and test domains and410the distributions to be different.Recent work in QE focused on aspects that are problematic even in the controlled WMT scenario, andare closely related to the flexibility/adaptability issue.
Focusing on the first of the two aforementioneddirections (i.e.
modeling translators?
behaviour), Cohn and Specia (2013) propose a Multitask GaussianProcess method that jointly learns a series of annotator-specific models and that outperforms modelstrained for each annotator.
Our work differs from theirs in that we are interested in the latter researchdirection (i.e.
coping with domain and distribution diversity) and we use in and out-of-domain data tolearn robust in-domain models.
Our scenario represents a more challenging setting than the one tackledin (Cohn and Specia, 2013), which does not consider different domains.In transfer learning there are many techniques suitable to fulfill our requirements.
The aim of transferlearning is to extract the knowledge from one or more source tasks and apply it to a target task (Panand Yang, 2010).
One type of transfer learning is multitask learning (MTL), which uses domain-specifictraining signals of related tasks to improve model generalization (Caruana, 1997).
Although it was notoriginally thought for transferring knowledge to a new task, MTL can be used to achieve this objectivedue to its capability to capture task relatedness, which is important knowledge that can be applied to anew task (Jiang, 2009).Domain adaptation is a kind of transfer learning in which source and target domains (i.e.
training andtest) are different but the tasks are the same (Pan and Yang, 2010).
The domain adaptation techniquesthat inspire our work have been successfully applied to a variety of NLP tasks (Blitzer et al., 2006;Jiang and Zhai, 2007).
For instance, an effective solution for supervised domain adaptation, EasyAdapt(SVR FEDA henceforth), was proposed in (Daum?e III, 2007) and applied to named entity recognition,part-of-speech tagging and shallow parsing.
The approach transforms the domain adaptation probleminto a standard learning problem by augmenting the source and target feature set.
The feature space istransformed to be a cross-product of the features of the source and target domains augmented with theoriginal target domain features.
In supervised domain adaptation one has access to out-of-domain labelsand wants to leverage a small amount of available in-domain labeled data to train a model (Daum?e III,2007), the case of this study.
This is different from the semi-supervised case in which in-domain labelsare not available.3 Adaptation for QEAn important assumption in MTL is that different tasks (domains in our case) are correlated via a certainstructure.
Examples of such structures are the hidden layers in a neural network (Caruana, 1997) andshared feature representation (Argyriou et al., 2007) among others.
This common structure allows forknowledge transfer among tasks and has been demonstrated to improve model generalization over singletask learning (STL) for different problems in different areas.
Under this scenario, several assumptionscan be made about the relatedness among the tasks, leading to different transfer structures.
We explorethree approaches to MTL that deal with task relatedness in different ways.
These are the ?Dirty?
approachto MTL (Jalali et al., 2010), Sparse Trace MTL (Chen et al., 2012) and Robust MTL (Chen et al., 2011).The three approaches use different regularization techniques that capture task relatedness using normsover the weights of the features.Before describing the three approaches, we introduce some basic notation similar to (Chenet al., 2011).
In MTL there are T tasks and each task t ?
T has m training samples{(x(t)1, y(t)1), .
.
.
, (x(t)m, y(t)m)}, with x(t)i?
Rdwhere d is the number of features and y(t)i?
R is theoutput (the response variable or label).
The input features and labels are stacked together to form twodifferent matrices X(t)= [x(t)1, .
.
.
, x(t)m] and Y(t)= [x(t)1, .
.
.
, x(t)m], respectively.
The weights of thefeatures for each task are represented by W , where each column corresponds to a task and each rowcorresponds to a feature.The ?Dirty?
approach to MTL follows the idea that different tasks may share the same discriminativefeatures (Argyriou et al., 2007).
However, it also considers that different tasks might have differentdiscriminative features that are inherent to each task.
Therefore, the method encourages shared-sparsityamong tasks and among features in each task.
It decomposes W into two components, one is a row-411sparsed matrix that corresponds to the features shared among the tasks and the other is an element-wisesparse matrix that corresponds to the non-shared features that are important for each task independently.More formally, the ?Dirty?
approach is explained by Equation 1.minWT?t=1||(W(t)X(t)?
Y(t))||22+ ?s||S||1+ ?b||B||1,?subject to: W = S +B (1)where ||(W(t)X(t)?
Y(t))||22is the least squares loss function, S is the regularization term that en-courages element-wise sparsity and B is the block-structured row-sparsity regularizer.
The ||.||2is thel2-norm (Euclidean distance), ||.||1is the l1-norm (given by?i=1|xi|) and ||.||1,?is the row grouped l1-norm.
The ?sand ?bare non-negative trade-off parameters that control the amount of regularizationapplied to S and B, respectively.Sparse Trace MTL considers the problem of learning incoherent sparse and low-rank patterns frommultiple related tasks.
This approach captures task relationship via a shared low-rank structure of theweight matrix W .
As computing the low-rank structure of a matrix leads to a NP-hard optimizationproblem, Chen et al.
(2012) proposed to compute the trace norm as a surrogate, making the optimizationproblem tractable.
In addition to learning the low-rank patterns, this method also considers the fact thatdifferent tasks may have different inherent discriminative features.
It decomposes W into two compo-nents: S, which models element-wise sparsity, and Q, which captures task relationship via the tracenorm.
The convex problem minimized by Sparse Trace is given in Equation 2.minWT?t=1||(W(t)X(t)?
Y(t))||22+ ?s||S||1subject to: W = S +Q, ||Q||?< ?p(2)where ||.||?is the trace norm, given by the sum of the singular values ?iof W , i.e., ||W ||?=?i=1?i(W ).
Here, ?pcontrols the rank of Q and ?scontrols the sparsity of S.The key assumption in MTL is that tasks are related in some way.
However, this assumption might nothold for a series of real-world problems.
In situations in which tasks are not related a negative transferof information among tasks might occur, harming the generalization of the model.
One way to dealwith this problem is to: (i) group related tasks in one structure and share knowledge among them, and(ii) identify irrelevant tasks maintaining them in a different group that does not share information withthe first group.
This is the idea of Robust MTL (RMTL henceforth).
The algorithm approximates taskrelatedness via a low-rank structure like Sparse Trace and identifies outlier tasks using a group-sparsestructure (column-sparse, at task level).
Robust MTL is described by Equation 3.
It employs a non-negative linear combination of the trace norm (the task relatedness component L) and a column-sparsestructure induced by the l1,2-norm (the outlier task detection component S).
If a task is an outlier it willhave non-zero entries in S.minWT?t=1||(W(t)X(t)?
Y(t))||22+ ?l||L||?+ ?s||S||1,2subject to: W = L+ S (3)where ||S||1,2is the group regularizer that induces sparsity on the tasks.4 Experimental SettingIn this section we describe the data used for our experiments, the features extracted, the set up of thelearning methods, the baselines used for comparison and the evaluation of the models.
The goal of ourexperiments is to show that the methods presented in Section 3 outperform competitive baselines andstandard QE learning methods that are not capable of adapting to different domains.
We experiment withthree different domains of comparable size and evaluate the performance of the adaptive methods and thestandard techniques with different amounts of training data.
The MTL models described in section 3 aretrained with the Malsar toolkit implementation (Zhou et al., 2012).
The hyper-parameters are optimized412using 5-fold cross-validation in a grid search procedure.
The parameter values are searched in an intervalranging from 10?3to 103.4.1 DataOur experiments focus on the English-French language pair and encompass three very different domains:newswire text (henceforth News), transcriptions of Technology Entertainment Design talks (TED) andInformation Technology manuals (IT).
Such domains are a challenging combination for adaptive systemssince they come from very different sources spanning speech and written discourse (TED and News/IT,respectively) as well as a very well defined and controlled vocabulary in the case of IT.Each domain is composed of 363 tuples formed by the source sentence in English, the French trans-lation produced by an MT system and a human post-edition of the translated sentence.
For each pair(translation, post-edition) we use as labels the HTER score computed with TERCpp3.
For the three do-mains we use half of the data for training (181 instances) and half of the data for testing (182 instances).The limited amount of instances for training contrasts with the 800 or more instances of the WMT evalu-ation campaigns and is closer to real-world applications where the availability of large and representativetraining sets is far from being guaranteed (e.g.
the CAT scenario).The sentence tuples for the first two domains are randomly sampled from the Trace corpus4.
Thetranslations were generated by two different MT systems, a state-of-the-art phrase-based statistical MTsystem and a commercial rule-based system.
Furthermore, the translations were post-edited by up to fourdifferent translators, as described in (Wisniewski et al., 2013).Domain No.
of tokens Vocab.
size Avg.
sent.
lengthTED source 6858 1659 19TED target 7016 1828 19IT source 3310 1004 9IT target 3134 1049 8News source 7605 2273 21News target 8230 2346 23Table 1: Datasets statistics for each domain.The TED talks domain is formed by subtitles of several talks in a range of topics presented in the TEDconferences.
The complete dataset has been used for MT and automatic speech recognition systemsevaluation within the International Workshop on Spoken Language Translation (IWSLT).
The Newsdomain is formed by newswire text used in WMT translation campaigns and covers different topics.
TheIT texts come from a software user manual translated by a statistical MT system based on the state-of-the-art phrase-based Moses toolkit (Koehn et al., 2007) trained on about 2M parallel sentences.
Thepost-editions were collected from one professional translator operating on the Matecat5CAT tool inreal working conditions.
Table 1 provides macro-indicators (number of tokens, vocabulary size, averagesentence length) that evidence the large difference between the domains addressed by our experimentsand give an idea of the difficulty of the task.A peculiarity of the TED domain is that it is formed by manual transcriptions of speech translated bydifferent MT systems, configuring a different type of discourse than News and IT.
In TED, the vocabularysize in the source and target sentences is lower than that of the News domain but higher than IT.
Newspresents the most varied vocabulary, which is an evidence of the more varied lexical choice representedby the several topics that compose the domain.
Moreover, News has the highest average sentence length,a characteristic of non-technical written discourse, which tends to have longer sentences than spokendiscourse and domains dominated by technical jargon.
Such a characteristic is exactly what differentiatesIT from the other two domains.
IT sentences are technical and present a reduced average number of3http://sourceforge.net/projects/tercpp/4http://anrtrace.limsi.fr/trace_postedit.tar.bz25www.matecat.com413words, as evidenced by the vocabulary size (the smallest among the three domains).
These numberssuggest a divergence between IT and the other two domains, possibly making adaptation more difficult.4.2 FeaturesFor all the experiments we use the same feature set composed of seventeen features proposed in (Specia etal., 2009).
The set is formed by features that model the complexity of translating the source sentence (e.g.the average source token length or the number of tokens in the source sentence), and the fluency of thetranslated sentence produced by the MT system (e.g.
the language model probability of the translation).The decision to use this feature set is motivated by the fact that it demonstrated to be robust acrosslanguage pairs, MT systems and text domains (Specia et al., 2009).
The 17 features are:?
number of tokens in the source sentence and in the generated translation;?
average source token length;?
average number of occurences of the target word within the generated translation;?
language model probability of the source sentence and generated translation;?
average number of translations per source word in the sentence: as given by IBM 1 model thresh-olded so that P (t|s) > 0.2 weighted by the inverse frequency of each word in the source side of theSMT training corpus?;?
average number of translations per source word in the sentence: as given by IBM 1 model thresh-olded so that P (t|s) > 0.01 weighted by the inverse frequency of each word in the source side ofthe SMT training corpus;?
percentage of unigrams?, bigrams and trigrams?
in the first quartile of frequency (lower fre-quency words) in a corpus of the source language;?
percentage of unigrams?, bigrams and trigrams in the fourth quartile of frequency (higher fre-quency words) in a corpus of the source language;?
percentage of unigrams in the source sentence seen in the source side of the SMT training corpus;?
number of punctuation marks in the source sentence and in the hypothesis translation;4.3 BaselinesAs a term of comparison, we consider these baselines in our experiments.
A simple to implement butdifficult to beat baseline when dealing with regression on tasks with different distributions is to computethe mean of the training labels and use it as the prediction for each testing point (Rubino et al., 2013).Hereafter we refer to this baseline as ?.
Since supervised domain adaptation techniques should outper-form models that are trained only on the available in-domain data, we also use as baseline the regressorbuilt only on the available in-domain data (SVR in-domain).
Furthermore, as a third baseline, we train aregressor by pooling together training data of all domains, combining source and target data without anykind of task relationship mechanism (SVR Pooling).The baselines are trained on the feature set described earlier in Section 4.2 with an SVM regression(SVR) method using the implementation of Scikit-learn (Pedregosa et al., 2011).
The radial basis func-tion (RBF) kernel is used for all baselines.
The hyper-parameters of the model are optimized usingrandomized search optimization process with 50 iterations as described in (Bergstra and Bengio, 2012)and used previously for QE in (de Souza et al., 2013a).
The best parameters are found using 5-foldcross-validation on the training data and , ?
and C are sampled from exponential distributions scaled at0.1 for the first two parameters and scaled at 100 for the last one.
It is important to notice that the SVRwith RBF kernel methods learn non-linear models that have been shown to perform better than linearmodels on the set of features used for predicting HTER.
On the contrary, the MTL methods presented inSection 3 are methods that do not explore kernels or any other kind of non-linear learning method.414Source / Target ITtgtNewstgtTEDtgtITsrc0.2081 0.2341 0.2232Newssrc0.2368 0.1690 0.2130TEDsrc0.2183 0.2263 0.1928Table 2: Results of the SVR in-domain baseline trained and evaluated in each domain (average of 50different shuffles).
Rows represent the domain data used to train the model and columns represent thedomain data used to evaluate the model.
Scores are MAE.4.4 EvaluationThe accuracy of the models is evaluated with the mean absolute error (MAE), which was also used inprevious work and in the WMT QE shared tasks (Bojar et al., 2013).
MAE is the average of the absolutedifference between the prediction y?iof a model and the gold standard response yi(Equation 4).
As it isan error measure, lower values mean better performance.MAE =1mm?i=1|y?i?
yi| (4)To test the statistical significance of our results we need to perform comparisons of multiple models.In addition, we would like to test the significance over different training amounts.
Given these require-ments we need to perform multiple hypothesis tests instead of paired tests.
It has been shown that forcomparisons of multiple machine learning models, the recommended approach is to use a non-parametricmultiple hypothesis test followed by a post-hoc analysis that compares each pair of hypothesis (Dem?sar,2006).
In our experiments we use the Friedman test (Friedman, 1937; Friedman, 1940) followed by apost-hoc analysis of the pairs of regressors using Holm?s procedure (Holm, 1979) to perform the pairwisecomparisons when the null hypothesis is rejected.
All tests for both Friedman and post-hoc analysis arerun with ?
= 0.05.
For more details about these methods, we refer the reader to (Dem?sar, 2006; Garciaand Herrera, 2008) which provide a complete review about the application of multiple hypothesis testingto machine learning methods.5 Results and DiscussionOur experiments are organized as follows.
First, we evaluate the performance of single task learningmethods on different cross-domain experiments.
Then, we report the evaluation for the multitask learningmethods and discuss the results.5.1 Single Task LearningWith the objective of having an insight about the difference between the domains, we train the SVRin-domain baseline with all available training data for each domain and evaluate its performance on thesame domain and in the two remaining domains.Results are reported in Table 2, where the diagonal shows the figures for the in-domain evaluation.These numbers suggest that the IT domain configures a more difficult challenge for the learning algo-rithm.
The IT in-domain model (ITsrc-ITtgt) presents a performance 21% inferior to News and 8%inferior to TED.
For all models trained on a source domain different than the target domain there is adrop in performance, as it is expected from a system that assumes that training and test data are sampledfrom the same distribution.
In addition, when predicting IT using the model trained on News, we have aperfomance drop of 13% whereas using the model trained on TED the performance drops up to 4%.5.2 Multitask learningWe run the baselines described in Section 4.3 and the methods described in Section 3 on differentamounts of training data, ranging from 18 to 181 instances (10% and 100%, respectively).
The mo-tivation is to verify how much training data is required by the MTL methods to outperform the baselinesfor a target domain.
Table 3 presents the results for the three domains with models trained on 30, 50 and415100% of the training data (54, 90 and 181 instances, respectively).
Each method was run on 50 differenttrain/test splits of the data in order to account for the variability of points in each split.Method TED News IT30 % of training data (54 instances)mean 0.1951 0.1711 0.2174SVR In-Domain 0.2013 0.1753 0.2235SVR Pooling 0.1962 0.1899 0.2201SVR FEDA 0.1952 0.1839 0.2193MTL Dirty 0.1954 0.1708 0.2193MTL SparseTrace 0.1976 0.1743 0.2222MTL RMTL 0.1946 0.1685 0.216250% of training data (90 instances)mean 0.1943 0.1707 0.2170SVR In-Domain 0.1976 0.1711 0.2183SVR Pooling 0.1951 0.1865 0.2191SVR FEDA 0.1937 0.1806 0.2161MTL Dirty 0.1927 0.1678 0.2148MTL SparseTrace 0.1922 0.1672 0.2157MTL RMTL 0.1878 0.1653 0.2119100% of training data (181 instances)mean 0.1936 0.1690 0.2162SVR In-Domain 0.1928 0.1690 0.2081SVR Pooling 0.1927 0.1849 0.2203SVR FEDA 0.1908 0.1757 0.2107MTL Dirty 0.1878 0.1666 0.2083MTL SparseTrace 0.1881 0.1661 0.2094MTL RMTL 0.1846 0.1653 0.2075Table 3: Average performance of fifty runsof the models on different train and test splitswith 30, 50 and 100 percent of training data.The average scores reported are the MAE.Figure 1: Visualization of the RMTL task outliermodel when trained on all the 181 instances oftraining data.
Cells with darker shades are closerto zero.
Cells with lighter shades are closer to one.Columns with only black entries are considered in-lier tasks (domains).
From left to right, columnscorrespond to News, TED and IT domains.
Thefirst 17 rows correspond to the features used totrain the model and the last row in corresponds tothe bias term.For all three domains, a general trend is that MTL RMTL is the method that reaches the lowest MAEwhen compared to all the other models.
Given the difference among the domains, it is very likely thatMTL Dirty and MTL SparseTrace suffer from the negative transfer problem (the assumption that alltasks are similar does not hold).
MTL RMTL is the only method among the methods presented here thatcopes with negative transfer among tasks.
The significance tests indicate that MTL RMTL improvementsare statistically significant with respect to all baselines depending on the range of training data used tocompute the test.?
For TED, the Friedman test rejects the null hypothesis with p = 4.62?5.
Post-hoc analysis indicatesthat there are differences statistically significant between MTL RMTL and all the three baselineswith p ?
0.002.?
For News, the Friedman test measures significant differences with p = 1.14?9and the post-hocanalysis indicates that MTL RMTL is statistically significant with respect to SVR in-domain andSVR Pooling with p = 0.002 for varying amounts of training data from 10 to 100%.
As can be seenin Figure 2, MTL RMTL starts with a very high MAE using 10% of the data (approximately 0.21MAE) but improves dramatically with 20% of the data.
Calculating the significance test with 20 to100% of training data, MTL RMTL is significantly better than all baselines with p ?
2.89?10.?
For IT, in a similar situation to the News domain, RMTL is significantly better than all baselines416trained on 30% to 100% of the training data (Friedman test?s p = 2.86?4and post-hoc analysis?p ?
3.73?7).Another observed trend is that the MTL models benefit from increasing amounts of training data.MTL RMTL has an improvement in performance of 5.13% for TED, 4% for News and 1.85% for ITwhen trained on 100% of the training data in comparison with the model trained on 30% of training data.18 36 54 72 90 108 126 144 162 181Training data points0.160.170.180.190.200.210.22MAEnews as target domainMeanSVR RBF in-domainSVR PoolingSVR FEDAMTL DirtyMTL SparseTraceMTL RMTLFigure 2: Learning curves for the News domain.The results for the IT domain are in line with the in-domain experiments in which we observed thatIT is a more challenging domain in comparison to TED and News.
The MAE of IT is always higherthan for the other domains on in-domain and MTL experiments.
Another evidence of this is the modellearned by the RMTL method when using all training data and run on one of the 50 training/test splits.
Agraphic representation of the RMTL outlier task detection component (described in Section 3) is shownin Figure 1.From left to right, each column represents News, TED, and IT domains, respectively, while each row isthe instantiation of a feature in the corresponding task.
Columns with non-black entries represent outliertasks.
The highest number of entries with lighter shades is in the third column, IT.
Several features inthis task are considered outliers with respect to the same features in the other tasks.
Consequently, thelearning method takes the weights into consideration to a greater extent when learned with the outliermodel for the IT domain.
Entries with the lightest shades in the IT domain correspond to the featuresmarked with?
in Section 4.2.
These outlier features are directly affected by the length of the sentenceson which they are computed (source or target) given that the number of tokens influences the final valueof the feature.
This outcome goes in the same direction of our analysis of the three domains (Section 4.1)that indicates a very different vocabulary size and average sentence length for IT when compared to theother two domains.To a lesser extent than IT, News and TED domains also present a few lighter-shaded entries in theoutlier component (1st and 2nd column).
This suggests that MTL RMTL was capable of transferinginformation among the domains in a more efficient way than the other MTL methods analyzed.417Overall the experiments presented show encouraging results in the direction of coping with QE datacoming from different domains/genres, translated by different MT systems and post-edited by differenttranslators.
Results show that even in such difficult conditions, the methods investigated are capable ofoutperforming competitive baselines based on non-linear models on different domains.
As a rationale,models that consider not only similarity between the domains but also deal with some sort of dissimilarityshould be considered.
This is the case of the best performing method, MTL RMTL, which identifiesoutlier tasks in order to avoid negative transfer among tasks.6 ConclusionIn this work we presented an investigation of methods that overcome limitations presented by currentMT QE state-of-the-art systems when applied to real world conditions.
In such scenarios (e.g.
CATenvironment) the requirements are two-fold: (i) learning in the presence of different train/test featureand label distributions and across different domains/genres, and (ii) the capability of learning with scarcetraining data.
In our experiments, we explored transfer learning methods, in particular multitask learning,and we showed that such methods can cope with the needs of real-world scenarios.We showed that multitask learning methods are capable to learn robust models for three differentdomains that perform better than three strong baselines trained on the same amount of data.
The methodsexplored here benefit from increasing amounts of training data but also perform well when operatingwith very limited amounts of data.
We believe that the results obtained in this first exploration of modeladaptation for the problem can encourage the MT QE community to shift the focus from controlledscenarios to more applicable, real-world contexts that require more robust methods.AcknowledgementsThis work has been partially supported by the EC-funded project MateCat (ICT-2011.4.2-287688).ReferencesAndreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil.
2007.
Multi-task feature learning.
In Advancesin neural information processing systems, volume 19.Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia Specia.
2013.
SHEF-Lite: When less is more for translationquality estimation.
In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 337?342.James Bergstra and Yoshua Bengio.
2012.
Random Search for Hyper-Parameter Optimization.
Journal of Ma-chine Learning Research, 13:281?305.Joseph John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto San-chis, and Nicola Ueffing.
2003.
Confidence estimation for machine translation.
In 20th COLING, pages315?321.John Blitzer, Ryan McDonald, and Fernando Pereira.
2006.
Domain adaptation with structural correspondencelearning.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages120?128, Morristown, NJ, USA.
Association for Computational Linguistics.Ondej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, ChristofMonz, Matt Post, Radu Soricut, and Lucia Specia.
2013.
Findings of the 2013 Workshop on Statistical MachineTranslation.
In Eighth Workshop on Statistical Machine Translation, pages 1?44.Christian Buck.
2012.
Black Box Features for the WMT 2012 Quality Estimation Shared Task.
In Proceedings ofthe 7th Workshop on Statistical Machine Translation, pages 91?95.Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia.
2012.
Findingsof the 2012 Workshop on Statistical Machine Translation.
In Proceedings of the 7th Workshop on StatisticalMachine Translation, pages 10?51, Montr{?e}al, Canada, June.
Association for Computational Linguistics.Rich Caruana.
1997.
Multitask Learning.
Machine learning, 28(1):41?75.418Jianhui Chen, Jiayu Zhou, and Jieping Ye.
2011.
Integrating low-rank and group-sparse structures for robust multi-task learning.
In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery anddata mining - KDD ?11, page 42, New York, New York, USA.
ACM Press.Jianhui Chen, Ji Liu, and Jieping Ye.
2012.
Learning incoherent sparse and low-rank patterns from multiple tasks.ACM Transactions on Knowledge Discovery from Data, 5(4):22, February.Trevor Cohn and Lucia Specia.
2013.
Modelling Annotator Bias with Multi-task Gaussian Processes: An applica-tion to Machine Translation Quality Estimation.
In Proceedings of the 51st Annual Meeting of the Associationfor Computational Linguistics, pages 32?42.Hal Daum?e III.
2007.
Frustratingly Easy Domain Ddaptation.
In Conference of the Association for ComputationalLinguistics (ACL).Jos?e G. C. de Souza, Christian Buck, Marco Turchi, and Matteo Negri.
2013a.
FBK-UEdin participation tothe WMT13 Quality Estimation shared-task.
In Proceedings of the Eighth Workshop on Statistical MachineTranslation, pages 352?358.Jos?e G.C.
de Souza, Miquel Espl`a-Gomis, Marco Turchi, and Matteo Negri.
2013b.
Exploiting qualitative infor-mation from automatic word alignment for cross-lingual nlp tasks.
In Proceedings of the 51st Annual Meetingof the Association for Computational Linguistics (Volume 2: Short Papers), pages 771?776, Sofia, Bulgaria,August.
Association for Computational Linguistics.Janez Dem?sar.
2006.
Statistical Comparisons of Classifiers over Multiple Data Sets.
The Journal of MachineLearning Research, 7:1?30, December.Milton Friedman.
1937.
The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis ofVariance.
Journal of the American Statistical Association, 32(200):675?701.Milton Friedman.
1940.
A Comparison of Alternative Tests of Significance for the Problem of m Rankings.
TheAnnals of Mathematical Statistics, 11(1):86?92.Salvador Garcia and Francisco Herrera.
2008.
An Extension on ?Statistical Comparisons of Classifiers overMultiple Data Sets?
for all Pairwise Comparisons.
Journal of Machine Learning Research, 9:2677?2694.Christian Hardmeier, Joakim Nivre, and Jorg Tiedemann.
2012.
Tree Kernels for Machine Translation QualityEstimation.
In Proceedings of the 7th Workshop on Statistical Machine Translation, number 2011, pages 109?113.Sture Holm.
1979.
A Simple Sequentially Rejective Multiple Test Procedure.
Scandinavian Journal of Statistics,6(2):pp.
65?70.Ali Jalali, PD Ravikumar, S Sanghavi, and C Ruan.
2010.
A Dirty Model for Multi-task Learning.
In Advances inNeural Information Processing Systems (NIPS) 23.Jing Jiang and Chengxiang Zhai.
2007.
Instance Weighting for Domain Adaptation in NLP.
In Proceedings of the45th Annual Meeting of the Association for Computational Linguistics, number June, pages 264?271.Jing Jiang.
2009.
Multi-Task Transfer Learning for Weakly-Supervised Relation Extraction.
In ACL ?09 Proceed-ings of the Joint Conference of the 47th Annual Meeting of the ACL, number August, pages 1012?1020.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, BrookeCowan, Wade Shen, Christine Moran, Richard Zenz, Chris Dyer, Ondej Bojar, Alexandra Constantin, and EvanHerbst.
2007.
Moses: Open source toolkit for statistical machine translation.
In ACL 2007 Demo and PosterSessions, number June, pages 177?180.Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012.
Match without a Referee : Evaluating MT Adequacywithout Reference Translations.
In Proceedings of the 7th Workshop on Statistical Machine Translation, pages171?180.Sinno Jialin Pan and Qiang Yang.
2010.
A Survey on Transfer Learning.
IEEE Transactions on Knowledge andData Engineering, 22(10):1345?1359, October.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-napeau, Mathieu Brucher, Mathieu Perrot, and?Edouard Duchesnay.
2011.
Scikit-learn : Machine Learning inPython.
Journal of Machine Learning Research, 12:2825?2830.419Raphael Rubino, Jennifer Foster, Joachim Wagner, Johann Roturier, Rasul Samad Zadeh Kaljahi, and Fred Hol-lowood.
2012.
DCU-Symantec Submission for the WMT 2012 Quality Estimation Task.
In Proceedings of theSeventh Workshop on Statistical Machine Translation, pages 138?144, Montr{?e}al, Canada, June.
Associationfor Computational Linguistics.Raphael Rubino, Jos?e G. C. de Souza, Jennifer Foster, and Lucia Specia.
2013.
Topic Models for TranslationQuality Estimation for Gisting Purposes.
In Machine Translation Summit (MT Summit) XIV, pages 295?302.Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul.
2006.
A Study of Trans-lation Edit Rate with Targeted Human Annotation.
In Association for Machine Translation in the Americas.Radu Soricut and A Echihabi.
2010.
Trustrank: Inducing trust in automatic translations via ranking.
In Proceed-ings of the 48th Annual Meeting of the Association for Computational Linguistics, number July, pages 612?621.Radu Soricut, Nguyen Bach, and Ziyuan Wang.
2012.
The SDL Language Weaver Systems in the WMT12Quality Estimation Shared Task.
In Proceedings of the 7th Workshop on Statistical Machine Translation, pages145?151.Jos?e G. C. de Souza, Jes?us Gonz?alez-Rubio, Christian Buck, Marco Turchi, and Matteo Negri.
2014.
FBK-UPV-UEdin participation in the WMT14 Quality Estimation shared-task.
In Proceedings of the Ninth Workshop onStatistical Machine Translation, Baltimore, MD, USA, June.Lucia Specia, Marco Turchi, Nello Cristianini, Nicola Cancedda, and Marc Dymetman.
2009.
Estimating theSentence-Level Quality of Machine Translation Systems.
In Proceedings of the 13th Annual Conference of theEAMT, number May, pages 28?35.Lucia Specia, Dhwaj Raj, and Marco Turchi.
2010.
Machine translation evaluation versus quality estimation.Machine Translation, 24(1):39?50, May.Lucia Specia, Stafford Street, Regent Court, and Mariano Felice.
2012.
Linguistic Features for Quality Estimation.In Proceedings of the 7th Workshop on Statistical Machine Translation, pages 96?103.Lucia Specia.
2011.
Exploiting objective annotations for measuring translation post-editing effort.
In Proceedingsof the European Association for Machine Translation, number May, pages 73?80.Marco Turchi, Josef Steinberger, and Lucia Specia.
2012.
Relevance ranking for translated texts.
In Proceedingsof the 16th Annual Conference of the European Association for Machine Translation, number May, pages 153?160.Marco Turchi, Matteo Negri, and Marcello Federico.
2013.
Coping with the Subjectivity of Human Judgementsin MT Quality Estimation.
In Proceedings of the 8th Workshop on Statistical Machine Translation (WMT?13),Sofia, Bulgaria, August.Marco Turchi, Antonios Anastasopoulos, Jos?e G. C. de Souza, and Matteo Negri.
2014.
Adaptive Quality Estima-tion for Machine Translation.
In Proceedings of the 52nd Annual Meeting of the Association for ComputationalLinguistics.Guillaume Wisniewski, Anil Kumar Singh, Natalia Segal, and Franc?ois Yvon.
2013.
Design and Analysis ofa Large Corpus of Post-Edited Translations: Quality Estimation, Failure Analysis and the Variability of Post-Edition.
In Machine Translation Summit XIV, pages 117?124.Jiayu Zhou, Jianhui Chen, and Jieping Ye.
2012.
MALSAR: Multi-tAsk Learning via StructurAl Regularization.420
