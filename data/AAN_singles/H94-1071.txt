Learning from Relevant Documents inLarge Scale Routing RetrievalK.L.
Kwok and L. GrunfeldComputer Science DepartmentQueens College, City University of New YorkFlushing, NY 11367ABSTRACTThe normal practice of selecting relevant documents fortraining routing queries is to either use all relevants or the'best n' of them after a (retrieval) ranking operation withrespect o each query.
Using all relevants can introducenoise and ambiguities in training because documents can belong with many irrelevant portions.
Using only the 'best n'risks leaving out documents that do not resemble a query.Based on a method of segmenting documents into moreuniform size subdocuments, a better approach is to use thetop ranked subdocument of every relevant.
An alternativeselection strategy is based on document properties withoutranking.
We found experimentally that short relevantdocuments are the quality items for training.
Beginningportions of longer relevants are also useful.
Using bothtypes provides a strategy that is effective and efficient.1.
INTRODUCTIONIn ad hoe Information Retrieval (IR) one employs auser-supplied free-text query as a clue to matchagainst a textbase and rank documents for retrieval.In a routing environment, one has the additionaloption to consult a user need's history to obtain a setof previously judged documents.
This set may beused with an automatic learning algorithm to helprefine or augment the user-supplied free-text query, oreven to define the query without he user description.We focus on employing the judged relevant set in thispaper.
(Judged nonrelevant documents have not beenfound to be useful in our model.)
For this option, oneneeds to consider two separate processes:(1) selecting the appropriate r levant documents orportions of them for training; and(2) selecting the appropriate terms from thesedocuments, expand the query and then effectivelyweighting these terms for the query.It is well-known from TREC and other experiments\[1,2,3,4,5,6,7,8\] that process (2) can improve routingresults substantially.
However, process (1) isnormally not given much consideration.
One eitheruses all the relevant documents, or employs the bestn of them after ranking with respect o the queryunder consideration.
However, over time in a largescale environment, hundreds and thousands of suchrelevant documents may accumulate for each userneed.
A strategy of which and what parts of therelevant documents are to be employed for trainingneeds to be considered.
Would portions of relevantdocuments be sufficient?
One reason for using aportion is that many documents can be long and maycontain extraneous paragraphs and sections that areirrelevant.
Using them for learning may contributeambiguities during the term selection, queryexpansion and weighting processes.
The problem isthat current relevance information gathering is forwhole documents only, and not at a more specificlevel such as which sentence or paragraph that isrelevant.
This problem would be alleviated if usersare diligent and indicate the relevant components ofa document that are actually relevant.
However, thiscould be a burden that some users may want to avoid.It is therefore useful to have an algorithm to locatethe most useful relevant components for trainingpurposes.
Another eason to use only portions of therelevants is consideration of efficiency: one wouldlike to avoid processing long documents when mostof it is irrelevant, or decrease the number ofdocuments to be processed.
This investigationconcerns exploring ways to effectively choose asubset of documents for training a given set of routingqueries.2.
P IRCS RETRIEVAL  SYSTEMPIRCS (acronym for Probabilistic Indexing andRetrieval -Components- System) is a network-basedsystem implementing a Bayesian decision approach to358QTDFig.l: 3-Layer PIR::: ak:Network Tiii .................DDTQd.IIR \[9,10\] and extended with the concept of documentcomponents \[11\] as shown in Fig.1.
The network\[12\] has three layers of nodes representing the queries(Q), terms (T) and documents (D), with edgesconnecting adjacent layers in a bidirectional fashion.Retrieval operation consists of initializing a documentnode d~ to activation 1 and spreading it via the edgeweights to terms t k and to a query node q~ underfocus, q, receives activation ~wa% i which isregarded as the query-focused retrieval status value(RSV) of d i for ranking purposes.
If activationoriginates from a query q, and spreads towards dl weaccumulate the document-focused RSV: ~waw~ thatis based on statistics of term usage different frombefore.
Combining the two can cooperatively providemore effective results.The edge weights of the net are first initialized withdefault values using global and local term usagestatistics.
Later they can learn from experience asillustrated in Fig.2.
In particular for routingexperiments, the edges on the query-term side of thenet is first created based on the routing queries andthe terms of the training collection, and given defaultvalues called self-learn relevant weights.
Relevanttraining documents are then linked in on thedocument-term side of the net.
Knowing whichdocument is relevant to which query allows edgeweights on the term-query side like w~, to adaptaccording to the term usage statistics of the relevantsets via a learning rule that is borrowed from artificialneural network studies.
New edges like w~, w\], canalso grow between queries and terms using, forexample, the K highest activated terms of the relevantdocuments, a process we call level K queryq~aiii !
..... w NT Fig.2: DTQ Learning wttn i::xpansJ6hDexpansion.
After learning, these query-term edgesand weights are frozen, the training documentsremoved, and new unseen testing documents are thenlinked in for simulation of the routing operation.Thus, test documents are ranked with respect o eachrouting query based on term usage statistics een inthe training collection and the relevant documents.3.
RELEVANT SUBDOCUMENTSELECT ION STRATEGIESOur approach to uneven full text collections \[3,6,8\]has been to segment long documents on the nextparagraph boundary after a run of 360 words, givingmore uniform length subdocument units.
Documentswith unrelated multiple stories with detectableseparation markers are also segmented atthe markers.This approach may impact favorably on: 1) precisionbecause shorter, more local units may diminishchance occurrence of terms used in senses differentfrom what is intended; 2) term weighting becauseunrealistic probability estimates of term weights maybe avoided; 3) query training and expansion becauselong documents may have unrelated and irrelevanttopics and concepts that can add noise to theseoperations; 4) retrieval output display because one cannarrow down to the relevant portion of a longdocument for the user; and 5) general efficiencybecause of handling multiple, more uniformsubdocuments instead of one long document.
In theTREC collections, documents of thousands of wordslong are not uncommon, and an example of a reallylong document is in the Diskl Federal Register:FR89119-0111 with 400,748 words.
With respect o359DTQ4Learning&Expansiond.
Iitenlt 3) query training and expansion, having many ofthese long documents in the training set would notonly overwhelm our system but also lead to ambiguityand imprecision.
Segmenting them intosubdocuments may provide us with strategies inselecting the appropriate relevant portions ofdocuments for learning.
In the next subsections weconsider document selection methods that can bebroadly classified into three types: approaches basedon document properties only, approaches based onranking, and on combinations of both.3.1 Subdocument Selection Based onDocument PropertiesThese selection methods employ some heuristics onthe properties of documents.
Because they are basedsolely on a list of known relevant subdocuments theycan bring in concepts that are not explicitely stated orrelated to the query.
These methods are also efficientbecause no ranking operation is required.
A risk ofthis type of approach is that if the selection method isnot well designed, many irrelevant portions ofrelevant documents may be included for training andbecomes counter-productive.
Four methods have beenexperimented with and the rationale for their choiceare given below:(a) Use al...2 subdocuments for learning and queryexpansion.
This is the usual approach in smallcollections.
In a large scale environment it may havethe drawback of ambiguity, imprecison andinefficiency discussed in Section 1, but will serve asa basis for comparison.
(b) Use only relevant documents that 'break' intoa maximum of max subdocuments.
This effectivelymeans eliminating long documents for learning, andmay diminish ambiguities that come with them.
Shortdocuments should be more concentrated and focusedin their content, and can be considered as qualityitems for training.
In particular, max=l meansemploying only 'nonbreak' documents.
This was thestrategy used in the original submitted results of ourTREC-2 experiments.
However, if the given relevantsare mostly long, we may artificially diminish theavailable number of relevants used for training.
(c) Many articles including scholarly documents,certain newspaper and magazine items introduce theirthemes by stating the most important concepts andcontents at the beginning of a document.
They alsosummarize at the end.
Therefore another approach isto use only the first or last subdocuments for training.Because of the way we segment documents o thatsome last subdocuments may be only a few wordslong, and the fact that some Wall Street Journalarticles can have multiple unrelated stories within adocument, we can only approximate our intent withthese experiments.
(d) A method labelled ffmax=2 uses the firstsubdocument of max=2 items.
This strategy will usequality items (b) but also include the beginningportion of documents (c) about twice as long, andwould remedy the fact that there may not be sufficientquality items for training.3.2 Subdocument Selection Based on aRanking OperationThese methods do a subdocument ranking operationwith the routing queries first so that we can select hebest ranking units for training.
By design, bestranking subdocuments have high probability of being'truely relevant' o their queries and have been provento work in user relevance feedback.
By ignoringpoorer anked units one hopes to suppress the noiseportions of documents for training.
A drawback inthis case is that the best ranked subdocuments bydefault share many or high-weighted terms with aquery, so that learning may become limited toenhancing the given free-text representation f thequery.
Subdocuments hat are relevant but do notresemble the query (and therefore are not rankedearly) will not be used.
Performing a ranking is alsotime-consuming compared with methods in Section3.1.
We have experimented with two methods asgiven below:(e) Select the bestn best-ranked relevantsubdocuments for training after ranking with respectto the given routing query representations.
A variantof this method is to enhance/expand the queryrepresentations first by using method (b) max=ldocuments before doing the ranking.
Selecting thesebestnx best-ranked subdocuments would include more'truely relevant' ones than before because the rankingoperation is more sophisticated and has been shownto achieve improved performance in our initialTREC2 experiments \[8\].
(If) Select he topn highest ranked subdocuments ofevery relevant.
Since our purpose is try to avoidnoise portions of relevant documents, these top rankedunits should have high probability that they are360mostly the signal portions as in (e).
Moreover,because all relevant documents are used, this methodmay include the advantage of Section 3.1 that unitsnot resembling the query would also be included fortraining.
A variant is, as before, to enhance/expandthe queries first before ranking for the topnx highestranked subdocuments for later training.3.3 Subdocument Selection Based onCombination of MethodsBy combining training document sets obtained fromthe best of the previous two subsections, we hope toimprove on the individual approaches alone.
Ourobjective is to define a training set of subdocumentsthat are specific to and resemble a queryrepresentation, as well as including overallsubdocuments that are relevant.
The following twomethods have been tried:(g) Merge documents obtained by method (e)bestn/bestnx retrieved, with those of method (b) usingmax=l.
The rationale is that method (e) selects thebest of those resembling the query, and method (b)uses short quality relevant documents in general.
(h) Merge documents obtained by method (e)bestn/bestnx retrieved, with those of method (If)topn/topnx=l units of every document.
This issimilar to (g), except that instead of using shortdocuments only, we now incorporate he best portionsof every relevant.4.
EXPERIMENTS ANDDISCUSSION OF RESULTSFor testing our various strategies of subdocumentselection for training, we performed experimentsexactly as those of TREC2 routing: Topics 51-100retrieving on the 1 GB of documents on Disk3 of theTREC collection.
Topics 51-100 have relevantdocument information from Disk l&2 totaling 2 GB.There are altogether 16400 relevant documentsaveraging out to 328 per query.
During ourprocessing however, a small percentage of therelevants are lost, so that we in effect use only 16114relevants that get segmented into 57751subdocuments.
This averages to about 1155 units perquery.
For the ranking strategies of Section 3.2, wehave created a separate subcollection consisting onlyof the 57751 training relevants but using Disk l&2term statistics, and ranking for the first 2000 of eachquery is done.
Various subsets of these rankedtraining documents are then used for weight learningfor the query-term side of the network, with termexpansion level K=40 terms as the standard.
Forsome cases we also did term expansion of K=80.After freezing these trained edge weights, Disk3subdocuments are linked in and routing retrievals aredone.
Results using the 'total number of relevantsretrieved' (at 1000 retrieved cutoff) and 'averageprecision over all recall points' as measures ofeffectiveness, aswell as the number of training unitsused, are summarized in Table 1.
Some of thedetailed precision-recall values are given in Table 2.The overall conclusion from these results is that forthis TREC-2 routing experiment, where a largenumber of relevant documents of different sizes andquality is available, it is possible to define goodsubsets of the documents or portions of them fortraining.From Table 1 and using the average precision (av-p)measure for comparison, it appears that the simplestrategy (b) of just using short, 'nonbreak' max=lrelevant documents gives one of the best results,achieving av-p at K=40 expansion level of 0.4050,about 6.7% better than the 0.3795 of our baselinestrategy (a) which uses all the relevant units.Moreover it is very efficient, requiring only 5235units which is less than 10% of the total 57751relevant subdocuments available and about 1/3 of the16114 documents.
Using longer documents hat breakinto two and six units (max=2 and 6) successivelyleads to slightly worse results as well as more work(15103 and 32312 subdocuments).
Thus, it appearsthat longer documents carry with it more noise asdiscussed in the Introduction.
Just using the firstsubdocument of every relevant (c) performs quitewell, with av-p of 0.4001.
Since the FR collectionhas many documents of thousands of words long, it isdifficult o imagine that signal parts are all in the firstsubdocuments.
A casual scan however shows thatsome FR documents, such as FR88107-0009 andFR88119-0018, carry a summary at the beginning.Moreover, FR documents constitute only a minorityof the training relevants.
Thus the first subdocumentsapparently carry sufficient signals of documents fortraining in this experiment.
Last subdocuments(results not shown) do not perform as well as first.One of the best results is fmax=2 achieving av-p of0.4047 as good as 'nonbreak' max=l method andusing 10,169 training units.Surprisingly, using the best ranking bestnx=30, 100,300, 2000 subdocuments (e) gives 0.3790, 0.3993,0.3999 and 0.3877 average precision respectively,361peaking around bestnx=300 but does not give betterperformance than (b,c,d) strategies.
For bestnx=30,employing only 1500 subdocuments apparently is notsufficient, and training may be limited tosubdocuments resembling the original query.bestnx=100 uses 4945 units similar to max=l but withav-p about 1.5% worse, while bestnx=300 uses 13712which is slightly less than first and performs about hesame.
In general, bestn results (not shown) areslightly less than those of bestnx as expected.
Usingthe topnx=l subdocument of every relevant (If)achieves 0.4082, the best numerically.
In (f) we havele, ss than 16114 units for training because we onlyrank the top 2000 for each query, and so somesubdocuments ranking below 2000 are not accountedfor.
It appears that including other overall relevantscan help improve performance.Strategies (g,h) of combining sets of subdocuments donot seem to lead to more improved results.Using the relevants retrieved (r-r) as a measure, itappears that larger training set sizes between 10000 to16000 are needed to achieve good recall.
Forexample, max=l and bestnx=100 employs about 5000units for training and have r-r of 7646 and 7605.bestnx=300, max=2, first and topnx=l have r-r valuesof 7703, 7783, 7805 and 7833, and training set sizesof: 13712, 15103, 16114 and 15702. fmax=2achieves good r-r of 7827 with a training size of10169.
fmax=3 (results not shown) is inferior.
Forthis collection, the best strategies of selectingsubdocuments for training appears to be eitherfmax=2 with av-p/r-r values of 0.4047/7827 ortopnx=l with 0.4082/7833.
fmax=2 has the advantagethat a ranking is not done and the training set issmaller.
The detailed recall-precision values in Table3 also shows that fmax=2 gives better precision at thelow recall region.
It appears that using documentproperties to select raining documents in this routingexperiment is both effective and efficient.5.
CONCLUSIONWe explore several strategies of selecting relevantdocuments or portions of them for query training inthe TREC-2 routing retrieval experiment.
It confirmsthat using all relevants for training is not a goodstrategy because irrelevant noisy portions ofdocuments would be included.
Short relevants are thequality documents.
Simple methods such as usingonly short documents, together with beginningportions of longer documents for training performswell and is also efficient.
For this TREC2 routing, anaverage of about 200-300 subdocuments per queryappears adequate, about 1/5-1/4 of all known relevantsubdocuments available in this experiment.
Selectingthe bestn ranked relevants (as in relevance feedback)is not as effective as just selecting the top ranked unitof every document.
This investigation also showsthat breaking documents into subdocuments is usefulfor query training.ACKNOWLEDGMENTThis work is partially supported by a grant fromARPA via the TREC program.REFERENCES1.
Salton, G. & Buckley, C. Improving retrievalperformance byrelevance f edback.
J of American Societyfor Information Science.
41 (1990), 288-297.2.
Harman, D. Relevance feedback revisited.
In: Proc.ACM SIGIR 15th Ann.
Intl.Conf.
on R&D in IR.
Belkin,N.J, Ingwersen, P & Pejtersen, A.M (Eds.)
ACM, NY.
(1992), 1-10.3.
Kwok, K.L., Papadopolous, L. & Kwan, Y.Y.
Retrievalexperiments with a large collection using PIRCS.
In: NISTSpecial Publication 500-267.
Gaithersburg, M.D.
20899.
(March 1993), 153-172.4.
Haines, D & Croft, W.B.
Relevance feedback andinference networks.
In: Proc.
ACM SIGIR 16th Ann.Intl.Conf.
on R&D in IR.
Korfhage, R, Rasmussen, E &Willet, P (FEds.)
ACM, NY.
(1993), 1-11.5.
Harman, D (Ed.)
The First Text REtrieval Conference(TREC-1).
National Institute of Standards and TechnologySpecial Publication 500-207, March 1993.6.
Kwok, K.L.
A network approach to probabilisticinformation retrieval.
Submitted for publication.7.
Harman, D (Ed.)
The Second Text REtrieval Conference(TREC-2).
National Institute of Standards and TechnologySpecial Publication, to be published.8.
Kwok, K.L., Grunfeld, L. TREC-2 retrieval experimentsusing PIRCS.
In: NIST Special Publication, to be published.9.
Robertson, S.E.
& Sparck Jones, K. Relevance weightingof search terms."
J. of American Society for InformationScience.
27 (1976), 129-146.10. van Rijsbergen, C.J.
Information Retrieval, SecondEdition.
Butterworths, London.
(1979).11.
Kwok, K.L.
Experiments with a component theory ofprobabilistic information retrieval based on single terms asdocument components.
ACM Trans.
on InformationSystems.
8 (1990), 363-386.12.
Kwok, K.L.
A neural network for probabilisticinformation retrieval.
In: Proc.
ACM SIGIR 12th Ann.
Intl.Conf.
on R&D in IR.
Belkin, N.J. & van Rijsbergen, C.J.(Eds.)
ACM, NY.
(1989), 21-30.362a) ~ relevsubdocsb) max=l=2=6c) firstd) fmax=2e) Best Rankedbestnx=30=100=300=2000If) Top subdoctopn =1topnx=lg) Merge Max=l,bestnmbestnl00h) Merge topn=l,bestntbestnl00.
.
.
.
.
.
.
.
.
.
.
.
.
.
Expansion Level K .
.
.
.
.
.
.
.
.
.
.
.40 80 No.
of Trainingr-r/av-p % inc r-r/av-p Subdocs %7611/.3795 baseline 7563/.3746 57751 baseline7646/.4050 0.5/6.7 7695/.4084 5235 97783/.3970 2.3/4.6 15103 267762/.3891 2.0/2.5 32312 567805/.4001 2.5/5.4 7854/.3976 16114 2878271.4047 2 .8 6 .6  7861L4040 10169 187295/.3790 -4.2/-0.1 1500 37605/.3993 -0.1/5.2 4945 97703/.3999 1.2/5.4 13809 247739/.3877 1.7/2.2 31792 557821/.4067 2.8/7.2 15384 277833/.4082 2 .9 7 .6  7887.4062 15702 277743/.4053 1.7/6.8 8930 157798/.4069 2.5/7.2 16362 28Table 1: Relevants Retrieved (r-r), Average Precision Values (av-p) and Number of Training Subdocumentsfor Various Subdocument Selection StrategiesStrategy: all max=lInterpolated Recall - Precision Averages:first fmax=2 bestnx=300 topnx=l0.0 .8311 .8475 .8362 .8467 .8273 .84040.1 .6464 .6751 .6779 .6839 .6664 .68080.2 .5755 .6116 .5978 .6132 .6000 .60860.3 .5035 .5413 .5285 .5312 .5240 .54290.4 .4469 .4774 .4734 .4786 .4719 .48100.5 .3951 .4288 .4245 .4245 .4206 .42590.6 .3286 .3681 .3564 .3565 .3641 .36330.7 .2706 .2880 .2833 .2880 .2830 .29040.8 .2057 .1937 .2085 .2099 .2095 .21820.9 .1079 .1144 .1156 .1181 .1i59 .11831.0 .0115 .0107 .0120 .0135 .0113 .0123Average precision (non-interpolated) over all rel docs.3795 .4050 .4001 .4047 .3999 .4082Precision:At 5 docs .6480 .7160 .6920 .7120 .6920 .692010 " .6460 .6860 .6940 .6968 .6820 .696020 " .6100 .6540 .6540 .6670 .6520 .6520100 " .4706 .4930 .4854 .4890 .4970 .4926500 " .2439 .2490 .2532 .2524 .2493 .25441000 " .1522 .1529 .1561 .1565 .1541 .1567R-Precision (precision after R (=num rel for a query) docs retrieved):Exact .4036 .4283 .4218 .4228 .4201 .4274Table 2: Average Precision Values at Interpolated Recall Points and at Number of Documents Retrievedfor Six Subdocument Selection Strategies (Expansion Level-40)363
