A Multi-layer Chinese Word Segmentation System Optimized forOut-of-domain TasksQin GaoLanguage Technologies InstituteCarnegie Mellon Universityqing@cs.cmu.eduStephan VogelLanguage Technologies InstituteCarnegie Mellon Universitystephan.vogel@cs.cmu.eduAbstractState-of-the-art Chinese word segmenta-tion systems have achieved high perfor-mance when training data and testing dataare from the same domain.
However, theysuffer from the generalizability problemwhen applied on test data from differentdomains.
We introduce a multi-layer Chi-nese word segmentation system which canintegrate the outputs from multiple hetero-geneous segmentation systems.
By train-ing a second layer of large margin clas-sifier on top of the outputs from severalConditional Random Fields classifiers, itcan utilize a small amount of in-domaintraining data to improve the performance.Experimental results show consistent im-provement on F1 scores and OOV recallrates by applying the approach.1 IntroductionThe Chinese word segmentation problem has beenintensively investigated in the past two decades.From lexicon-based methods such as Bi-DirectedMaximum Match (BDMM) (Chen et al, 2005) tostatistical models such as Hidden Markove Model(HMM) (Zhang et al, 2003), a broad spectrumof approaches have been experimented.
By cast-ing the problem as a character labeling task, se-quence labeling models such as Conditional Ran-dom Fields can be applied on the problem (Xueand Shen, 2003).
State-of-the-art CRF-based sys-tems have achieved good performance.
However,like many machine learning problems, generaliz-ability is crucial for a domain-independent seg-mentation system.
Because the training data usu-ally come from limited domains, when the domainof test data is different from the training data, theresults are still not satisfactory.A straight-forward solution is to obtain more la-beled data in the domain we want to test.
Howeverthis is not easily achievable because the amountof data needed to train a segmentation system arelarge.
In this paper, we focus on improving thesystem performance by using a relatively smallamount of manually labeled in-domain data to-gether with larger out-of-domain corpus1.
Theeffect of mingling the small in-domain data intolarge out-of-domain data may be neglectable dueto the difference in data size.
Hence, we try toexplore an alternative way that put a second layerof classifier on top of the segmentation systemsbuilt on out-of-domain corpus (we will call themsub-systems).
The classifier should be able to uti-lize the information from the sub-systems and op-timize the performance with a small amount of in-domain data.The basic idea of our method is to integratea number of different sub-systems whose per-formance varies on the new domain.
Figure 1demonstrates the system architecture.
There aretwo layers in the system.
In the lower layer,the out-of-domain corpora are used, together withother resources to produce heterogeneous sub-systems.
In the second layer the outputs of thesub-systems in the first layer are treated as inputto the classifier.
We train the classifier with smallin-domain data.
All the sub-systems should have1From this point, we use the term out-of-domain corpusto refer to the general and large training data that are notrelated to the test domain, and the term in-domain corpusto refer to small amount of data that comes from the samedomain of the test datareasonable performance on all domains, but theirperformance on different domains may vary.
Thejob of the second layer is to find the best decisionboundary on the target domain, in presence of allthe decisions made by the sub-systems.Number?Tag?FeatureClassifier?1Training?data?Character?Type?Featureg(in?domain)Entropy?FeatureClassifier?2TrainingdataClassifier3Integrated?Training?data?
(out?of?domain)Classifier?3classifierWord?list?1Classifier?4Word?list?2Classifier5Word?list?2Classifier?5Figure 1: The architecture of the system, the firstlayer (sub-systems) is trained on general out-of-domain corpus and various resources, while thesecond layer of the classifier is trained on in-domain corpus.Conditional Random Fields (CRF) (Lafferty etal., 2001) has been applied on Chinese word seg-mentation and achieved high performance.
How-ever, because of its conditional nature the smallamount of in-domain corpus will not significantlychange the distributions of the model parame-ters trained on out-of-domain corpus, it is moresuitable to be used in the sub-systems than inthe second-layer classifier.
Large margin modelssuch as Support Vector Machine (SVM) (Vapnik,1995) can be trained on small corpus and gener-alize well.
Therefore we chose to use CRF inbuilding sub-systems and SVM in building thesecond-layer.
We built multiple CRF-based Chi-nese word segmentation systems using differentfeatures, and then use the marginal probability ofeach tag of all the systems as features in SVM.The SVM is then trained on small in-domain cor-pus, results in a decision hyperplane that mini-mizes the loss in the small training data.
To in-tegrate the dependencies of output tags, we useSVM-HMM (Altun et al, 2003) to capture the in-teractions between tags and features.
By apply-ing SVM-HMM we can bias our decision towardsmost informative CRF-based system w.r.t.
the tar-get domain.
Our methodology is similar to (Co-hen and Carvalho, 2005), who applied a cross-validation-like method to train sequential stackingmodels, while we directly use small amount of in-domain data to train the second-layer classifiers.The paper is organized as follows, first we willdiscuss the CRF-based sub-systems we used insection 2, and then the SVM-based system com-bination method in section 3.
Finally, in section 4the experimental results are presented.2 CRF-based sub-systemsIn this section we describe the sub-systems weused in system.
All of the sub-systems are basedon CRF with different features.
The tag set weuse is the 6-tag (B1, B2, B3, M, E, S) set pro-posed by Zhao et al(2006).
All of the sub-systemsuse the same tag set, however as we will see later,the second-layer classifier in our system does notrequire the sub-systems to have a common tagset.
Also, all of the sub-systems include a com-mon set of character features proposed in (Zhaoand Kit, 2008).
The offsets and concatenationsof the six n-gram features (the feature template)are: C?1, C0, C1, C?1C0, C0C1, C?1C1.
In theremaining part of the section we will introduceother features that we employed in different sub-systems.2.1 Character type featuresBy simply classify the characters into four types:Punctuation (P), Digits (D), Roman Letters (L)and Chinese characters (C), we can assign char-acter type tags to every character.
The idea isstraight-forward.
We denote the feature as CTF .Similar to character feature, we also use differ-ent offsets and concatenations for character typefeatures.
The feature template is identical tocharacter feature, i.e.
CTF?1, CTF0, CTF1,CTF?1CTF0, CTF0CTF1, CTF?1CTF1 areused as features in CRF training.2.2 Number tag featureNumbers take a large portion of the OOV words,which can easily be detected by regular expres-sions or Finite State Automata.
However thereare often ambiguities on the boundary of numbers.Therefore, instead of using detected numbers asfinal answers, we use them as features.
The num-ber detector we developed finds the longest sub-strings in a sentence that are:?
Chinese Numbers (N)?
Chinese Ordinals (O)?
Chinese Dates (D)For each character of the detected num-bers/ordinal/date, we assign a tag that reflects theposition of the character in the detected num-ber/ordinal/date.
We adopt the four-tag set (B, M,E, S).
The position tags are appended to end ofthe number/ordinal/date tags to form the numbertag feature of that character.
I.e.
there are totally13 possible values for the number tag feature, aslisted in Table 1.2Number Ordinal Date OtherBegin NB OB DBMiddle NM OM DMEnd NE OE DE XXSingle NS OS?
DS?Table 1: The feature values used in the number tagfeature, note that OS and DS are never observedbecause there is no single character ordinal/dateby our definition.Similar to character feature and character typefeature, the feature template mention before isalso applied on the number tag feature.
We de-note the number tag features as NF .2.3 Conditional Entropy FeatureWe define the Forward Conditional Entropy ofa character C by the entropy of all the charac-ters that follow C in a given corpus, and theBackward Conditional Entropy as the entropyof all the characters that precede C in a givencorpus.
The conditional entropy can be com-puted easily from a character bigram list gener-ated from the corpus.
Assume we have a bigram2Two of the tags, OS and DS are never observed.list B = {B1, B2, ?
?
?
, BN}, where every bigramentry Bk = {cik , cjk , nk} is a triplet of the twoconsecutive characters cik and cjk and the count ofthe bigram in the corpus, nk.
The Forward Condi-tional Entropy of the character C is defined by:Hf (C) :=?cik=CnkZlognkZwhere Z =?cik=Cnk is the normalization fac-tor.And the Backward Conditional Entropy can becomputed similarly.We assign labels to every character based onthe conditional entropy of it.
If the conditionalentropy value is less than 1.0, we assign fea-ture value 0 to the character, and for region[1.0, 2.0), we assign feature value 1.
Similarly wedefine the region-to-value mappings as follows:[2.0, 3.5) ?
2, [3.5, 5.0) ?
4, [5.0, 7.0) ?
5,[7.0,+?)
?
6.
The forward and backward con-ditional entropy forms two features.
We will referto these features as EF .2.4 Lexical FeaturesLexical features are the most important features tomake sub-systems output different results on dif-ferent domains.
We adopt the definition of the fea-tures partially from (Shi and Wang, 2007).
In oursystem we use only the Lbegin(C0) and Lend(C0)features, omitting the LmidC0 feature.
The twofeatures represent the maximum length of wordsfound in the lexicon that contain the current char-acter as the first or last character, correspondingly.For feature values equal or greater than 6, wegroup them into one value.Although we can find a number of Chinese lex-icons available, they may or may not be gener-ated according to the same standard as the train-ing data.
Concatenating them into one may bringin noise and undermine the performance.
There-fore, every lexicon will generate its own lexicalfeatures.3 SVM-based System CombinationGeneralization is a fundamental problem of Chi-nese word segmentation.
Since the training datamay come from different domains than the testdata, the vocabulary and the distribution can alsobe different.
Ideally, if we can have labeled datafrom the same domain, we can train segmentersspecific to the domain.
However obtaining suffi-cient amount of labeled data in the target domainis time-consuming and expensive.
In the meantime, if we only label a small amount of data in thetarget domain and put them into the training data,the effect may be too small because the size ofout-of-domain data can overwhelm the in-domaindata.In this paper we propose a different way ofutilizing small amount of in-domain corpus.
Weput a second-layer classifier on top of the CRF-based sub-systems, the output of CRF-based sub-systems are treated as features in an SVM-HMM(Altun et al, 2003) classifier.
We can train theSVM-HMM classifier on a small amount of in-domain data.
The training procedure can beviewed as finding the optimal decision boundarythat minimize the hinge loss on the in-domaindata.
Because the number of features for SVM-HMM is significantly smaller than CRF, we cantrain the model with as few as several hundredsentences.Similar to CRF, the SVM-HMM classifier stilltreats the Chinese word segmentation problem ascharacter tagging.
However, because of the limi-tation of training data size, we try to minimize thenumber of classes.
We chose to adopt the two-tagset, i.e.
class 1 indicates the character is the end ofa word and class 2 means otherwise.
Also, due tolimited amount of training data, we do not use anycharacter features, instead, the features comes di-rectly from the output of sub-systems.
The SVM-HMM can use any real value features, which en-ables integration of a wide range of segmenters.In this paper we use only the CRF-based seg-menters, and the features are the marginal prob-abilities (Sutton and McCallum, 2006) of all thetags in the tag set for each character.
As an ex-ample, for a CRF-based sub-system that outputssix tags, it will output six features for each char-acter for the SVM-HMM classifier, correspondingto the marginal probability of the character giventhe CRF model.
The marginal probabilities forthe same tag (e.g.
B1, S, etc) come from differ-ent CRF-based sub-systems are treated as distinctfeatures.Features LexiconsS1 CF, CTF NoneS2 CF, NF ADSO, CTB6S3 CF, CTF, NF ADSOS4 CF, CTF, NF, EF ADSO, CTB6S5 CF, EF NoneS6 CF, NF NoneS7 CF, CTF ADSOS8 CF, CTF CTB6Table 2: The configurations of CRF-based sub-systems.
S1 to S4 are used in the final submissionof the Bake-off, S5 through S8 are also presentedto show the effects of individual features.When we encounter data from a new domain,we first use one of the CRF-based sub-system tosegment a portion of the data, and manually cor-rect obvious segmentation errors.
The manuallylabeled data are then processed by all the CRF-based sub-systems, so as to obtain features of ev-ery character.
After that, we train the SVM-HMMmodel using these features.During decoding, the Chinese input will also beprocessed by all of the CRF-based sub-systems,and the outputs will be fed into the SVM-HMMclassifier.
The final decisions of word boundariesare based solely on the classified labels of SVM-HMM model.For the Bake-off system, we labeled two hun-dred sentences in each of the unsegmented train-ing set (A and B).
Since only one submission isallowed, the SVM-HMM model of the final sys-tem was trained on the concatenation of the twotraining sets, i.e.
four hundred sentences.The CRF-based sub-systems are trained usingCRF++ toolkit (Kudo, 2003), and the SVM-HMMtrained by the SVMstruct toolkit (Joachims et al,2009).4 ExperimentsTo evaluate the effectiveness of the proposed sys-tem combination method, we performed two ex-periments.
First, we evaluate the system combina-tion method on provided training data in the waythat is similar to cross-validation.
Second, we ex-perimented with training the SVM-HMM modelwith the manually labeled data come from cor-Micro-Average Macro-AverageP R F1 OOV-R P R F1 OOV-RS1 0.962 0.960 0.961 0.722 0.962 0.960 0.960 0.720S2 0.965 0.966 0.966 0.725 0.965 0.966 0.966 0.723S3 0.966 0.967 0.967 0.731 0.966 0.967 0.967 0.729S4 0.968 0.969 0.968 0.731 0.967 0.969 0.969 0.729S5 0.962 0.960 0.961 0.720 0.962 0.960 0.960 0.718S6 0.963 0.961 0.962 0.730 0.963 0.961 0.961 0.729S7 0.966 0.967 0.966 0.723 0.966 0.967 0.967 0.720S8 0.963 0.960 0.962 0.727 0.963 0.960 0.960 0.726CB 0.969 0.969 0.969 0.741 0.969 0.969 0.969 0.739Table 3: The performance of individual sub-systems and combined system.
The Micro-Average resultscome from concatenating all the outputs of the ten-fold systems and then compute the scores, and theMacro-Average results are calculated by first compute the scores in every of the ten-fold systems andthen average the scores.Set A Set BP R F1 OOV-R P R F1 OOV-RS1 0.925 0.920 0.923 0.625 0.936 0.938 0.937 0.805S2 0.934 0.934 0.934 0.641 0.941 0.930 0.935 0.751S3 0.940 0.937 0.938 0.677 0.938 0.926 0.932 0.752S4 0.942 0.940 0.941 0.688 0.944 0.929 0.936 0.776CB1 0.943 0.941 0.942 0.688 0.948 0.936 0.942 0.794CB2 0.941 0.940 0.941 0.692 0.939 0.949 0.944 0.821CB3 0.943 0.939 0.941 0.699 0.950 0.950 0.950 0.820Table 4: The performance of individual systems and system combination on Bake-off test data, CB1,CB2, and CB3 are system combination trained on labeled data from domain A, B, and the concatenationof the data from both domains.responding domains, and tested the resulting sys-tems on the Bake-off test data.For experiment 1, We divide the training setinto 11 segments, segment 0 through 9 contains1733 sentences, and segment 10 has 1724 sen-tence.
We perform 10-fold cross-validation onsegment 0 to 9.
Every time we pick one segmentfrom segment 0 to 9 as test set and the remain-ing 9 segments are used to train CRF-based sub-systems.
Segment 10 is used as the training set forSVM-HMM model.
The sub-systems we used islisted in Table 2.In Table 3 we provide the micro-level andmacro-level average of performance the ten-foldevaluation, including both the combined systemand all the individual sub-systems.
Becausethe system combination uses more data than itssub-systems (segment 10), in order to have afair comparison, when evaluating individual sub-systems, segment 10 is appended to the trainingdata of CRF model.
Therefore, the individual sub-systems and system combination have exactly thesame set of training data.As we can see in the results in Table 3, the sys-tem combination method (Row CB) has improve-ment over the best sub-system (S4) on both F1and OOV recall rate, and the OOV recall rate im-proved by 1%.
We should notice that in this exper-iment we actually did not deal with any data fromdifferent domains, the advantage of the proposedmethod is therefore not prominent.We continue to present the experiment resultsof the second experiment.
In the experimentwe labeled 200 sentences from each of the unla-beled bake-off training set A and B, and trainedthe SVM-HMM model on the labeled data.
Wecompare the performance of the four sub-systemsand the performance of the system combinationmethod trained on: 1) 200 sentences from A, 2)200 sentences from B, and 3) the concatenationof the 400 sentences from both A and B.
We showthe scores on the bake-off test set A and B in Table4.As we can see from the results in Table 4, thesystem combination method outperforms all theindividual systems, and the best performance isobserved when using both of the labeled data fromdomain A and B, which indicates the potential offurther improvement by increasing the amount ofin-domain training data.
Also, the individual sub-systems with the best performance on the two do-mains are different.
System 1 performs well onSet B but not on Set A, so does System 4, whichtops on Set A but not as good as System 1 on SetB.
The system combination results appear to bemuch more stable on the two domains, which is apreferable characteristic if the segmentation sys-tem needs to deal with data from various domains.5 ConclusionIn this paper we discussed a system combina-tion method based on SVM-HMM for the Chineseword segmentation problem.
The method can uti-lize small amount of training data in target do-mains to improve the performance over individ-ual sub-systems trained on data from different do-mains.
Experimental results show that the methodis effective in improving the performance with asmall amount of in-domain training data.Future work includes adding more heteroge-neous sub-systems other than CRF-based onesinto the system and investigate the effects on theperformance.
Automatic domain adaptation forChinese word segmentation can also be an out-come of the method, which may be an interestingresearch topic in the future.ReferencesAltun, Yasemin, Ioannis Tsochantaridis, and ThomasHofmann.
2003.
Hidden markov support vectormachines.
In Proceedings of International Confer-ence on Machine Learning (ICML).Chen, Yaodong, Ting Wang, and Huowang Chen.2005.
Using directed graph based bdmm algorithmfor chinese word segmentation.
pages 214?217.Cohen, William W. and Vitor Carvalho.
2005.
Stackedsequential learning.
In Proceedings of the Inter-national Joint Conference on Artificial Intelligence(IJ-CAI).Joachims, Thorsten, Thomas Finley, and Chun-Nam John Yu.
2009.
Cutting-plane training ofstructural svms.
Machine Learning, 77(1):27?59.Kudo, Taku.
2003.
CRF++: Yet another crf toolkit.Web page: http://crfpp.sourceforge.net/.Lafferty, John, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of International Con-ference on Machine Learning (ICML).Shi, Yanxin and Mengqiu Wang.
2007.
A dual-layercrfs based joint decoding method for cascaded seg-mentation and labeling tasks.
In Proceedings of theInternational Joint Conference on Artificial Intelli-gence (IJ-CAI).Sutton, Charles and Andrew McCallum, 2006.
Intro-duction to Statistical Relational Learning, chapterAn Introduction to Conditional Random Fields forRelational Learning.
MIT Press.Vapnik, Vladimir N. 1995.
The Nature of StatisticalLearning Theory.
Springer.Xue, Nianwen and Libin Shen.
2003.
Chinese wordsegmentation as lmr tagging.
In Proceedings ofthe second SIGHAN workshop on Chinese languageprocessing, pages 176?179.Zhang, Huaping, Qun Liu, Xueqi Cheng, Hao Zhang,and Hongkui Yu.
2003.
Chinese lexical analysis us-ing hierarchical hidden markov model.
In Proceed-ings of the second SIGHAN workshop on Chineselanguage processing, pages 63?70.Zhao, Hai and Chunyu Kit.
2008.
Unsupervisedsegmentation helps supervised learning of charac-ter tagging for word segmentation and named entityrecognition.
In The Sixth SIGHAN Workshop onChinese Language Processing (SIGHAN-6), pages106?111.Zhao, Hai, Chang-Ning Huang, Mu Li, and Bao-LiangLu.
2006.
Effective tag set selection in chineseword segmentation via conditional random fieldmodeling.
In Proceedings of the 20th Pacific AsiaConference on Language, Information and Compu-tation (PACLIC-20), pages 87?94.
