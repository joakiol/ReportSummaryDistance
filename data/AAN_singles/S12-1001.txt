First Joint Conference on Lexical and Computational Semantics (*SEM), pages 1?10,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsCasting Implicit Role Linking as an Anaphora Resolution TaskCarina Silberer?School of InformaticsUniversity of EdinburghEdinburgh, UKc.silberer@ed.ac.ukAnette FrankDepartment of Computational LinguisticsHeidelberg UniversityHeidelberg, Germanyfrank@cl.uni-heidelberg.deAbstractLinking implicit semantic roles is a challeng-ing problem in discourse processing.
Unlikeprior work inspired by SRL, we cast this prob-lem as an anaphora resolution task and embedit in an entity-based coreference resolution(CR) architecture.
Our experiments clearlyshow that CR-oriented features yield strongestperformance exceeding a strong baseline.
Weaddress the problem of data sparsity by apply-ing heuristic labeling techniques, guided bythe anaphoric nature of the phenomenon.
Weachieve performance beyond state-of-the art.1 IntroductionA widespread phenomenon that is still poorly stud-ied in NLP is the meaning contribution of unfilledsemantic roles of predicates in discourse interpreta-tion.
Such roles, while linguistically unexpressed,can often be anaphorically bound to antecedent ref-erents in the discourse context.
Capturing such im-plicit semantic roles and linking them to their an-tecedents is a challenging problem.
But it bears im-mense potential for establishing discourse coherenceand for getting closer to the aim of true NLU.Linking of implicit semantic roles in discoursehas recently been introduced as a shared task inthe SemEval 2010 competition Linking Events andTheir Participants in Discourse (Ruppenhofer et al,2009, 2010).
The task consists in detecting un-filled semantic roles of events and determining an-tecedents in the discourse context that these roles?
The work reported in this paper is based on a Master?sThesis conducted at Heidelberg University (Silberer, 2011).can be understood to refer to.
In (1), e.g., the pred-icate jealousy introduces two implicit roles, one forthe experiencer, the other for the object of jealousyinvolved.
These roles can be bound to Watson andthe speaker (I) in the non-local preceding context.
(1) Watson won?t allow that I know anything of art butthat is mere jealousy because our views upon thesubject differ.
(2) IReader was sitting reading in the chairPlace.In contrast to implicit roles that can be discourse-bound to an antecedent as in (1), roles can be inter-preted existentially, as in (2), with an unfilled TEXTrole of the READING frame that cannot be anchoredin prior discourse.
The FrameNet paradigm (Fill-more et al, 2003) that was used for annotation inthe SemEval task classifies these interpretation dif-ferences as definite (DNI) vs. indefinite (INI) nullinstantiations (NI) of roles, respectively.2 Implicit Role Reference: A Short HistoryEarly studies.
The phenomenon of implicit role re-ference is not new.
It has been studied in a numberof early approaches.
Palmer et al (1986) treated un-filled semantic roles as special cases of anaphora andcoreference resolution (CR).
Resolution was guidedby domain knowledge encoded in a knowledge-based system.
Similarly, Whittemore et al (1991)analyzed the resolution of unexpressed event rolesas a special case of CR.
A formalization in DRT wasfully worked out, but automation was not addressed.Later studies emphasize the role of implicit rolereference in a frame-semantic discourse analysis.Fillmore and Baker (2001) provide an analysis of1a newspaper text that indicates the importance offrames and roles in establishing discourse coher-ence.
Burchardt et al (2005) offer a formalizationof the involved factors: the interplay of frames andframe relations with factors of contextual contigu-ity.
The work includes no automation, but suggests acorpus-based approach using antecedent-role coref-erence patterns collected from corpora.Tetreault (2002), finally, offers an automated anal-ysis for resolving implicit role reference.
The small-scale study is embedded in a rule-based CR setup.SemEval 2010 Task 10: Linking Roles.
Trig-gered by the SemEval 2010 competition (Ruppen-hofer et al, 2010), research on resolving implicitrole reference has gained momentum again, in a fieldwhere both semantic role labeling (SRL) and coref-erence resolution have seen tremendous progress.However, the systems that participated in the NI-only task on implicit role resolution achieved mod-erate success in the initial subtasks: (i) recog-nition of implicit roles and (ii) classification asdiscourse-bound vs. existential interpretation (DNIvs.
INI).
Yet, (iii) identification of role antecedentswas bluntly unsuccessful, with around 1% F-score.Ruppenhofer et al clearly relate the task tocoreference resolution.
The participating systems,though, framed the task as a special case of SRL.Chen et al (2010) participated with their SRL sys-tem SEMAFOR (Das et al, 2010).
They cast the taskas one of extended SRL, by admitting constituentsfrom a larger context.
To overcome the lack andsparsity of syntactic path features, they include lex-ical association and similarity scores for semanticroles and role fillers; classical SRL order and dis-tance features are adapted to larger distances.VENSES++ by Tonelli and Delmonte (2010) isa semantic processing system that includes lexico-semantic processing, anaphora resolution and deepsemantic resolution components.
Anaphora resolu-tion is performed in a rule-based manner; pronom-inals are replaced with their antecedents?
lexicalinformation.
For role linking, the system appliesdiverse heuristics including search for predicate-argument structures with compatible arguments, aswell as semantic relatedness scores between poten-tial fillers of (overt and implicit) semantic roles.More recently Tonelli and Delmonte (2011) recurto a leaner approach for role binding, estimating arelevance score for potential antecedents from rolefillers observed in training.
They report an F-scoreof 8 points for role binding on SemEval data.
How-ever, being strongly lexicalized, their trained modelseems heavily dependent on the training data.Ruppenhofer et al (2011) use semantic types foridentifying DNI role antecedents, reporting an errorreduction of 14% on Chen et al (2010)?s results.The poor performance results in the SemEval taskclearly indicate the difficulty of resolving implicitrole reference.
A major factor seems to relate to datasparsity: the training set covers only 245 DNI anno-tations linked to an antecedent.Linking implicit arguments of nominals.
Ger-ber and Chai (2010) (G&C henceforth) investigate aclosely related task of argument binding, tied to thelinking of implicit arguments for nominal predicatesusing the PropBank role labeling scheme.
In con-trast to the SemEval task, which focuses on a verbsand nouns, their system is only applied to nouns andis restricted to 10 predicates with substantial trainingset sizes (avg: 125, median: 103).G&C propose a discriminative model that selectsan antecedent for an implicit role from an extendedcontext window.
The approach incorporates someaspects relating to CR that go beyond the SRL-oriented SemEval systems: A candidate represen-tation includes information about all the candidates?coreferent mentions (determined by automatic CR),in particular their semantic roles (provided by goldannotations) and WordNet synsets.
Patterns of se-mantic associations between filler candidates andimplicit roles are learned for all mentions containedin the candidate?s entity chain.
They achieve an F-score of 42.3, against a baseline of 26.5.Gerber (2011) presents an extended model that in-corporates strategies suggested in Burchardt et al(2005): using frame relations as well as coreferencepatterns acquired from large corpora.
This modelachieves an F-score of 50.3 (baseline: 28.9).3 Casting Implicit Role Linking as anAnaphora Resolution Task3.1 Implicit role = anaphora resolutionRecent models for role binding mainly draw on tech-niques from SRL, enriched with concepts from CR.2In this paper, we explicitly formulate implicit rolelinking as an anaphora resolution task.
This is inline with the predominant conception in early work,and also highlights the close relationship with zeroanaphora (Kameyama, 1985).
Computational treat-ments of zero anaphora (e.g., Imamura et al (2009))are in fact employing techniques well-known fromSRL.
Recent work by Iida and Poesio (2011), bycontrast, offers an analysis of zero anaphora in aCR architecture.
Further support comes from psy-cholinguistic studies in Garrod and Terras (2000),who establish commonalities between implicit rolereference and other types of anaphora resolution.The contributions of our work are as follows:i.
We cast implicit role binding as a CR task, us-ing an entity-mention model and discriminativeclassification for antecedent selection.ii.
We examine the effectiveness of model featuresfor classical SRL vs. CR features to clarify thenature of this special phenomenon.iii.
We automatically acquire heuristically labeleddata to address the sparse data problem.i.
An entity-mention model for anaphoric roleresolution.
In our model implicit roles that arediscourse-bound (i.e.
classified as DNI) are treatedas anaphoric, similar to zero anaphora: the implicitrole will be bound to a discourse antecedent.In line with recent research in CR, we adopt anentity-mention model, where an entity is representedby all mentions pertaining to a coreference chain(see i.a.
Rahman and Ng (2011), Cai and Strube(2010)).
Our model is based on binary classifier de-cisions that take as input the anaphoric role and anentity candidate from the preceding discourse.
Thefinal classification of a role linking to an entity is ob-tained by discriminative ranking of the binary clas-sifiers?
probability estimates.
Details on the systemarchitecture are given in Section 3.2.ii.
SRL vs. CR: Analysis of feature sets.
Thelinking of implicit semantic roles represents an inter-esting mixture of SRL and CR that displays excep-tional characteristics of both types of phenomena.In contrast to classical SRL, the relation betweena predicate?s semantic role and a candidate role filler?
being realized outside the local syntactic context ?cannot be characterized by syntactic path features.But similar to SRL we can compute a semantic classtype expected by the role and determine which can-didate is most appropriate to fill the semantic role.Anaphoric binding of unfilled roles also divergesfrom classical CR in that the anaphoric element isnot overtly expressed.
This excludes typical CR fea-tures that refer to overt realization, such as agree-ment or string overlap.
Again, we can make use of asemantic characterization of role fillers to determinethe role?s most appropriate antecedent entity in thediscourse.
This closely relates to semantic class fea-tures employed in CR (e.g., Rahman and Ng (2011)).Thus, semantic association features are importantmodeling aspects, but they do not contribute to clari-fying the nature of the phenomenon.
We will includeadditional properties that are considered characteris-tic for CR, such as the semantics of an entity (as op-posed to individual mentions), or salience propertiesof antecedents (cf.
Section 4.3).
Thus, the model wepropose substantially differs from prior work.We classify the features of our models as SRL vs.CR features, plus a mixture class that relates to bothphenomena.
We examine which type of features ismost effective for resolving implicit role reference.iii.
Heuristic data acquisition.
In response to thesparse data problem encountered with the SemEvaldata set and the general lack of annotated resourcesfor implicit role binding, we experiment with tech-niques for heuristic data acquisition.
The strategywe apply builds on our working hypothesis that im-plicit role reference is best understood as a specialcase of (zero) anaphora resolution.We process manually annotated coreference datasets that are jointly labeled with semantic roles.From these we extract entity chains that containanaphoric pronouns that fill a predicate?s semanticrole.
We artificially delete the pronoun?s role labeland transfer it to its closest antecedent in its chain.In this way, we convert the example to an instancethat is structurally similar to one involving a locallyunfilled semantic role that is bound to an overt an-tecedent.
An example is given below: in (3.a) weidentify a pronoun that fills the SPEAKER role of theframe STATEMENT.
We transfer this role label to itsclosest antecedent (3.b).3(3) a. Riadyk spoke in hisk 21-story office buildingon the outskirts of Jakarta.
[...] The timing ofhisk,Speaker statementStatement is important.b.
Riadyk spoke in hisk,Speaker 21-story officebuilding on the outskirts of Jakarta.
[...] The tim-ing of ?
statementStatement is important.Clearly such artificially created annotation instancesare only approximations of naturally occurring casesof implicit role binding.
But we expect to acquirenumerous data points for relevant features: semanticclass information for the antecedent entity, the pred-icate?s frame and roles and coherence properties.3.2 System ArchitectureOur approach is embedded in an architecture for su-pervised CR using an entity-mention model.
Themain processing steps of the system include: (1) en-tity detection, (2) instance creation with feature ex-traction and (3) classification.
As we are focusingon the resolution of implicit DNI roles, we assumethat the text is already augmented with standard CRinformation (we make use of gold data and automati-cally assigned coreference chains).
Accordingly, thedescription of modules focuses exclusively on theresolution of DNIs.
(1) Entity Detection.
We first collect the entireentity set E mentioned in the discourse.
This setforms the overall set of candidates to consider forDNI linking.
For each DNI dk to be linked, a subsetof candidates Ek ?
E is chosen as candidate searchspace for resolving dk.
We experiment with differ-ent strategies for constructing Ek (cf.
Section 4).
(2) Instance Creation.
The next step consists inthe creation of (training) instances for classificationincluding the extraction of features for all instances.An instance instej ,dk consists of the active DNIdk, its frame and a candidate entity ej ?
Ek.
In-stance creation follows an entity-based adaption ofthe standard procedure of Soon et al (2001), whichhas been applied by Yang et al (2004, 2008).
Pro-cessing the discourse from left to right, for each DNIdk, instances Ik are created by processing Ek fromright to left according to each entity?s most recentmention, starting with the entity closest to dk.
Notethat, as entities instead of mentions are considered,only one instance is created for an entity which ismentioned several times in the search space.In training, the instance creation stops when thecorrect antecedent, i.e.
a positive instance, as well asat least one negative instance have been found.1(3) Classification.
From the acquired training in-stances we learn a binary classifier that predicts foran instance instej ,dk whether it is positive, i.e.
en-tity ej is a correct antecedent for DNI dk.
Fur-ther, the classifier provides a probability estimate forinstej ,dk being positive.
We obtain classificationsfor all instances in Ik.
Among the positive classifiedinstances, we select the antecedent e with the high-est estimate.
That is, we apply the best-first strategy(Ng and Cardie, 2002).
In case of a tie, we choosethe antecedent which is closer to the target.
If noinstance is classified as positive, dk is left unfilled.4 Data and Experiments4.1 SEMEVAL 2010 task and data setWe adhere to the SemEval 2010 task by Ruppen-hofer et al (2009) as test bed for our experiments.The main focus of our work is on part (iii), the iden-tification of antecedents for DNIs.
Subtasks (i) and(ii), the recognition and interpretation of NIs will beonly tackled to enable comparison to the participat-ing systems of the SemEval NI-only task.The SemEval task is based on fiction stories byA.
C. Doyle, one story as training data and anothertwo chapters as test set, enriched with coreferenceand FrameNet-style frame annotations.
Informationabout the training section is found in Table 1.
Thetest data comprise 710 NIs (349 DNIs, 361 INIs), ofwhich 259 DNIs are linked.4.2 Heuristic data acquisitionSince the training data has a critically small amountof linked DNIs, we heuristically labeled trainingdata on the basis of data sets with manually anno-tated coreference information: OntoNotes 3.0 (Hovyet al, 2006), as well as ACE-2 (Mitchell et al, 2003)and MUC-6 (Chinchor and Sundheim, 2003).OntoNotes 3.0 was merged with gold SRL an-notations from the CoNLL-2005 shared task.
Bymeans of SemLink-1.1 (Loper et al, 2007) and amapping included in the SemEval data, these Prop-Bank (PB, Palmer et al (2005)) annotations were1We additionally impose several restrictions, e.g., a validcandidate must not already fill another role of the active frame.4#ent avg avg #frames #frame#DNI #DNI#ent/doc size types typesSemEval 141 141 9 1,370 317 245 155ONotes 7899 23 3 12,770 258 2,220 270ACE-2 3564 11 4 58,204 757 4,265 578MUC-6 1841 15 3 20,140 654 997 310corpus coref semantic rolesONotes manual manual PB CoNLL05, ported to FNACE-2 manual automatic FN (Semafor)MUC-6 manual automatic FN (Semafor)Table 1: SemEval vs. heuristically acquired datamapped to their FrameNet (FN) counterparts, if ex-istent.
For the ACE-2 and MUC-6 corpora, we usedSemafor (Das and Smith, 2011) for automatic anno-tation with FN semantic roles.
From these data setswe acquired heuristically annotated instances of rolelinking using the strategy explained in 3.1.Table 1 summarizes the resulting training data.The heuristically labeled data extends the manuallylabeled DNI instances by an order of magnitude.4.3 Model parametersEntity sets Edni.
For definition of the set of can-didate entities to consider for DNI linking, Edni,we determined different parameter settings with re-strictions on the types, distances and prominence ofcandidate antecedents.
For instance, unlike in nounphrase CR, antecedents for a DNI can be realized bya wide range of constituents other than NPs, such asprepositional (PP), adverbial (ADVP), verb phrases(VP) and even sentences (S) referring to proposi-tions.These settings, stated in Table 2, were inferred byexperiments on the training data and by examiningits statistics: AllChains is motivated by the fact that72% of the DNIs are linked to referents with non-singleton chains.
On the other hand, the majority ofDNI antecedents ?
not only non-singletons, but alsophrases of a certain type or terminals that overtlyfill other roles ?
are located in the current and thetwo preceding sentences (69.6%), which motivatesSentWin.
However, antecedents are also located farbeyond this window span which is probably due tothe nature of the SemEval texts, with prominent en-tities being accessible over longer stretches of dis-course.
Chains+Win is designed by taking into ac-AllChains This set contains all the entities repre-sented by non-singleton coreference chains thatwere introduced in the discourse up to the cur-rent DNI position, assuming that this way onlymore salient entities are considered.SentWin Comprises constituents with a certainphrase type2 or terminals that overtly fill a role,occurring within the current or the precedingtwo sentences.Chain+Win This set comprises SentWin plus allentities mentioned at least five times up to thecurrent DNI position (i.e.
salient entities).Table 2: Entity set settings Ednicount all previous observations.Training data sets.
We made use of different mix-tures of training data: SemEval plus different exten-sions using the heuristically acquired data summa-rized in Table 1.4.4 Feature sets: SRL, mixed and CR-orientedTable 3 lists the most important features used fortraining our models.
Features 1-13 were used in thebest model and are ordered by their strength basedon feature ablation experiments (cf.
Section 5).
Allfeatures are marked for their general type; the lastcolumn marks features employed by G&C.3Below we give some details for selected features.Feat.
1: Prominence.
We first compute averageprominence of an entity e (Eq.
2) by summing overthe size (= nb.
of mentions) of all entities e in a win-dow w4 of preceding sentences and dividing by thenb.
of entities E in w. Prominence of e (Eq.
1) isset to the difference between its size in w and theaverage prominence score.5 The final feature valuerecords the relative rank of e?s prominence scorecompared to the scores of the other candidates.prom(e, w) = #mentions(e, w)?
avg prom(w) (1)avg prom(w) =?e?E #mentions(e, w)|E|(2)2The phrase type must be NPB, S, VP, SBAR, or SG.3?
marks features that are similar to G&C features.
Notethat their only CR features are distance features.4We set w = 2 based on experiments on the training data.5This prominence score was proposed by Dolata (2010)within an entity grid approach to role linking.5nr feature type G&C1 prominence prominence score of the entity in the current discourse position CR -2 pos.dist mention PoS or phrase type of the most recent explicit mention (CR) -concatenated with sentence distance to the target3 dist mentions minimum distance between DNI and entity in mentions CR -4 dist sentences minimum distance between DNI and entity in sentences CR +5 vnroles dni.entity the counterparts of the DNI in VerbNet (VN, Kipper et al (2000)) mixed +concatenated with the VN roles the entity already instantiates6 roles dni.entity concatenation of the DNI with the FN roles the entity already instantiates mixed ?7 semType dni.entity semantic type of the DNI concatenated with mixed -the semantic types of the roles the entity already instantiates8 avgDist sentences average sentence distance between the entity and the DNI CR +9 sp supersense agreement of the selectional preferences for the DNI mixed -and the most frequent supersense of the entity10 function (target) grammatical function of the target SRL -11 wnss ent.st dni pointwise mutual information between the entity?s WN supersense ss and mixed -the DNI?s FN semantic type st: pmi(ss, st) = log2P (ss|st)/P (ss)12 nbRoles dni.entity like feature 5, but with NomBank arguments 0 and 1 mixed ?13 frame.dni frame name concatenated with the DNI SRL -Table 3: Best features used for training.
Feat.
11 was computed on the FN dataset and the SemEval training data.Feat.
9: SelPrefs.
We compute selectional prefer-ences following the information-theoretic approachof Resnik (1993, 1996).
Similar to Erk (2007), weused an adapted version which we computed for se-mantic roles by means of the FN database rather thanfor verb argument positions.
The WordNet classesover which the preferences are defined are WordNetlexicographer?s files (supersenses).The selectional association values ?
(dni, ss) ofthe DNI?s selectional preferences are retrieved forthe supersense ss of each candidate antecedent?shead.
As for Feat.
1, we define a candidate?s fea-ture value by its rank in the ordered list of these ?s.4.5 ExperimentsEvaluation measures.
We adopt the precision (P),recall (R) and F1 measures in Ruppenhofer et al(2010).
A true positive is a DNI which has beenlinked to the correct entity as given by the gold data.Classifiers and feature selection.
For DNI link-ing, we use BayesNet (Cooper and Herskovits,1992) as classifier, implemented in Weka (Wittenand Frank, 2000).6 For each parameter combination,we perform feature selection by means of leave-one-out 10-fold cross-validation on the SemEval train-ing data with successively removing/determining the6We experimented with different learners and selected thealgorithm that performed best for the different subtasks.best features.
The resulting models Mi are then eval-uated on the SemEval test data in different setups:Exp1: Linking DNIs.
Exp1 evaluates our modelson the DNI linking task proper (NI-only step (iii)).This setting uses the gold coreference, SRL and DNIinformation in the test data.Exp2: Full NI-only.
For benchmarking on theSemEval task, we perform the complete NI-onlytask.
Here, the test data is only enriched w/ SRL la-beling.
Each frame f in the test corpus is processed,involving the following steps:(i) Recognition of NIs is performed by consultingthe FN database7 and determining the FN core rolesthat are unfilled.
From this NI set, roles that areconceptually redundant or competing with f?s overtroles are rejected as they don?t need to or must notbe linked, respectively.
(ii) For predicting the interpretation of an NI, weuse LibSVM (Chang and Lin, 2001) as classifierwhich further assigns each NI a probability estimateof the NI being definite.
We use a small set of fea-tures: the FN semantic type of the NI and a booleanfeature indicating whether the target is in passivevoice and the agent (object) not realized.
Further,we use a statistical feature which gives the relative7We used the FrameNetAPI by Reiter (2010).6model add.
entity frame DNI Linking (%)data set anno.
P R F1M0 - AllChains gold 25.6 25.1 25.3M1 ON2-10 Chains+Win proj 30.8 25.1 27.7M1?
ON2-24 AllChains proj 35.6 20.1 25.7M1??
ON2-24 SentWin proj 23.3 22.4 22.8M2 MUC Chains+Win auto 26.1 24.3 25.3M3 ACE AllChains auto 24.0 21.2 22.5Prom ?
Chains+Win ?
20.5 20.5 20.5Table 4: Exp1: Best performing models for different en-tity and data settings.
Test data contain gold CR chains.frequency of the role?s realization as DNI and INI,respectively, in the training data.
(iii) DNI linking is performed for each of f?s pre-dicted DNIs Df in descending order of their prob-ability estimates.
If an antecedent em can be de-termined for a predicted DNI, the role is labeledas such and linked to em.
As the DNI?s role hasbeen filled now, competing or redundant DNIs areremoved from Df before moving to the next pre-dicted DNI.
Only DNIs for which an antecedent isfound are labeled as such.Exp2 is evaluated on both gold coreference an-notation and automatically assigned coreferencechains, using the CR system of Cai et al (2011).5 Evaluation and Results5.1 Exp1: DNI linking evaluationTable 4 shows the best performing models for DNIlinking for each parameter setting8.
We comparethem to a strong baseline Prom (last row) that linkseach DNI to the antecedent candidate with highestprominence score.
Its F1-score is beaten by the othermodels, with a gain of 7.2 points for model M1.
Thehigh performance of the baseline can be taken as ev-idence that salience factors are crucial for this task.The best performing model M1 (27.7 F1) usesabout a fifth of the ON data with Chains+Win.
Whenusing SentWin as entity set, F1 drops to 18.5 (notshown).
The best performing model using SentWin(M1??)
performs 4.9 points below M1.
Hence, re-liance on the Chains+Win set seems beneficial.
Per-formance of the AllChains setting varies over the8We consider the 3 types of entity sets and different train-ing setups ?
additional data (Section 4.3); additional data withgold, projected or automatic frame annotations.
The ON datawas also evaluated with roughly a fifth of ON to evaluate theeffect of different amounts of data of the same type of data.Features P ( %) R (%) F1 (%)all 30.8 25.1 27.7- 1-4,8 (CR) 21.6 8.1 11.8- 10,13 (SRL) 31.0 25.9 28.2- 5-7,9,11-12 (mixed) 20.6 20.5 20.5Table 5: Results of ablation study.different data sets: the strongest model is M0 with-out additional data.
An explanation could be the dif-ferent data domains (story vs. news), leading to adifferent nature (length and number) of the entities.In general, the models seem to profit from heuris-tically labeled training data.
We note strong gains(up to 10 pts) in precision for 3 of these 5 best mod-els, compared to M0.
Finally, we observe higherperformance when using additional data with gold/projected semantic frame annotations (M1, M1?
).Analysis of the best model.
Table 5 states the re-sults for M1 when leaving out one of the featuretypes at a time.
The serious drop of F1 from 27.7%to 11.8% when omitting CR features clearly demon-strates that this feature type has by far the greatestimpact on the task performance.
Rejection of themixed features decreases F1 to a score equal to theprominence baseline, whereas leaving out the SRL-features even slightly increases F1.
The weakness ofFeature 13 could still be attributed to data sparsity.5.2 Exp2: Full NI-only evaluationTable 6 lists the results for the full NI-only task ob-tained with the presented models with different addi-tional training data sets (lines 2-5).
When perform-ing all three steps, the F1-score of the best modelM1 drops to 10.1% (-17.6 pts, col. 10) under us-age of automatic coreference annotations in the testdata (i.e.
under the real task conditions).
When us-ing gold coreference annotations, the F1-score isat 18.1% (col. 11), which can be seen as an upperbound for our current models on this task.
The dif-ference of 9.6 points between only performing DNIlinking (Table 4) and the full NI-only task reflectsthe fact that recognizing (step i) and interpreting(step ii) NIs bear difficulties on their own.9Comparison of our models with the two SemEval9When not performing step (iii), NI recognition achieves77.6% recall and 67% relative precision.7Null Instantiations (%)model add.
entity frame recogn.
interpret.
(precision) DNI Linking (%)data set anno.
recall relative absolute P R F1 F1(crf)M0 - AllChains gold 58 68 40 6.0 8.9 7.1 12.5M1 ON2-10 Chains+Win proj 56 69 38 9.2 11.2 10.1 18.1M2 MUC Chains+Win auto 52 70 36 7.0 8.5 7.6 11.0M3 ACE AllChains auto 56 68 38 5.9 8.1 6.8 11.3M3?
ACE Chains+Win auto 56 68 38 6.9 9.7 8.0 9.5SEMAFOR ?
63 55 35 1.40VENSES++ ?
8 64 5 1.21T&D ?
54 75 40 13.0 6.0 8Table 6: Exp2 results obtained for our models (lines 1-5) and comparable systems (lines 6-8).
Column 5 gives thescore for correctly recognized NIs.
Cols.
6 and 7 report precision for correctly interpreted NIs on the basis of thecorrectly recognized (relative) vs. all gold NIs to be recognized (absolute).
The scores in the last column (F1(crf))were obtained with gold CR annotations.task participants10 (lines 7-8) shows that our modelsclearly outperform these systems ?
with a gain of+5.7 and +8.89 points in F1-score in DNI linking.11Compared to Tonelli and Delmonte (2011)(T&D), M1 has a higher F1-score in linking of+2.1 points.
In contrast to our method, their link-ing approach is (admittedly) heavily lexicalized andstrongly tailored to the domain of the used data.6 ConclusionWe cast the problem of linking implicit semanticroles as a special case of (zero) anaphora resolution,drawing on insights from earlier work and parallelsobserved with zero anaphora.
Our results stronglysupport this analysis: (i) Feature selection clearlydetermines CR-related features as strongest supportfor DNI linking.
(ii) Our models beat a strong base-line using a prominence score to determine DNI ref-erence.
(iii) We devise a method for heuristically la-beling training data that simulates implicit role refer-ence.
Using this data we obtain system performancebeyond state-of-the-art, with high gains in precision.While these findings clearly corroborate our con-ceptual approach, overall performance is still mea-ger.
Comparison to G&C?s setting suggests thattraining data is a serious issue.
We addressed the10The F1-scores are from http://semeval2.fbk.eu/semeval2.php?location=Rankings/ranking10.html11Moreover, note that Ruppenhofer et al describe a weakerevaluation, that judges DNI linkings as correct if the span of thelinked referent contains the gold referent.
Further, they consider14 linked INIs in the test data, although linking INIs conflictswith the definition of INIs.problem of training set size using heuristic data ac-quisition.
The nature of semantic role annotationsmay be another problem, as FrameNet-style roles donot generalize well.
Finally, implicit roles pertainingto nominalizations tend to be more local than thosepertaining to verbs12 and might be less diverse.Our model is closer in spirit to G&C than the Se-mEval systems, but differs by being embedded inan entity-based CR architecture using discriminativeantecedent selection.
Also, we address a more prin-cipled issue, by exploring the nature of the task usinga qualitative feature analysis.
Our system comparesfavorably to related work.
Benchmarking againstthe SemEval participants and T&D shows clear im-provements.
Also, T&D?s model is closely tied todomain data, while ours is enhanced with out-of-domain data.
Exact comparison to G&C needs to beconducted on the same data set and labeling scheme.In sum, within the chosen setting we can showthat implicit role reference is best modeled as a spe-cial case of anaphora resolution.
We observe thatmodels trained on cleaner data perform better thanon larger, but more noisy data sets.
Thus, it is es-sential to further enhance the quality of heuristicallylabeled data.
Applying the classifiers for steps (i)and (ii) as a filter could help to better constrain thedata to the target phenomenon.Acknowledgements.
We would like to thank MateuszDolata for his help with salience and coherence features,and Michael Roth for his server support.12This is confirmed by analysis of the SemEval vs. NomBankcorpus of G&C.8ReferencesAljoscha Burchardt, Anette Frank, and Manfred Pinkal.2005.
Building Text Meaning Representations fromContextually Related Frames ?
A Case Study.
In Pro-ceedings of the 6th International Workshop on Com-putational Semantics, IWCS-6, pages 66?77, Tilburg,The Netherlands.Jie Cai and Michael Strube.
2010.
End-to-end coref-erence resolution via hypergraph partitioning.
InProceedings of the 23rd International Conference onComputational Linguistics, pages 143?151, Beijing,China.Jie Cai, Eva Mu?jdricza-Maydt, and Michael Strube.2011.
Unrestricted coreference resolution via globalhypergraph partitioning.
In Proceedings of the SharedTask of 15th Conference on Computational NaturalLanguage Learning, pages 56?60, Portland, Oregon.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: a Library for Support Vector Machines.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Desai Chen, Nathan Schneider, Dipanjan Das, andNoah A. Smith.
2010.
SEMAFOR: Frame ArgumentResolution with Log-Linear Models.
In Proceedingsof the 5th International Workshop on Semantic Evalu-ation, pages 264?267, Uppsala, Sweden, July.Nancy Chinchor and Beth Sundheim, 2003.
MessageUnderstanding Conference (MUC) 6.
Linguistic DataConsortium, Philadelphia.Gregory F. Cooper and Edward Herskovits.
1992.
ABayesian Method for the Induction of ProbabilisticNetworks from Data.
Machine Learning, 9(4):309?347.Dipanjan Das and Noah A. Smith.
2011.
Semi-supervised frame-semantic parsing for unknown pred-icates.
In Dekang Lin, Yuji Matsumoto, and Rada Mi-halcea, editors, ACL, pages 1435?1444.
The Associa-tion for Computer Linguistics.Dipanjan Das, Nathan Schneider, Desai Chen, andNoah A. Smith.
2010.
Probabilistic Frame-SemanticParsing.
In Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages948?956, Los Angeles, California, June.Mateusz Dolata.
2010.
Extending the Entity-Grid Modelfor the Processing of Implicit Roles in Discourse.Bachelor?s thesis, Department of Computational Lin-guistics, Heidelberg University, Germany.Katrin Erk.
2007.
A Simple, Similarity-based Model forSelectional Preferences.
In Proceedings of the 45thAnnual Meeting of the Association for ComputationalLinguistics, ACL ?07, pages 216?223, Prague, CzechRepublic, June.Charles J. Fillmore and Collin F. Baker.
2001.
Frame Se-mantics for Text Understanding.
In Proceedings of theNAACL 2001 Workshop on WordNet and Other LexicalResources, Pittsburgh, June.Charles J. Fillmore, Christopher R. Johnson, and MiriamR.
L. Petruck.
2003.
Background to Framenet.
Inter-national Journal of Lexicography, 16(3):235?250.Simon Garrod and Melody Terras.
2000.
The Contribu-tion of Lexical and Situational Knowledge to Resolv-ing Discourse Roles: Bonding and Resolution.
Jour-nal of Memory and Language, 42(4):526?544.Matthew Gerber and Joyce Chai.
2010.
Beyond Nom-Bank: A Study of Implicit Arguments for NominalPredicates.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 1583?1592, Uppsala, Sweden, July.Matthew Steven Gerber.
2011.
Semantic Role Labelingof Implicit Arguments for Nominal Predicates.
Ph.D.thesis, Michigan State University.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:The 90% Solution.
In Proceedings of the Human Lan-guage Technology Conference of the North AmericanChapter of the Association for Computational Linguis-tics, HLT-NAACL ?06, pages 57?60, New York, NewYork, June.Ryu Iida and Massimo Poesio.
2011.
A Cross-LingualILP Solution to Zero Anaphora Resolution.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics, pages 804?813,Portland, Oregon.Kenji Imamura, Kuniko Saito, and Tomoko Izumi.2009.
Discriminative Approach to Predicate-Argument Structure Analysis with Zero-AnaphoraResolution.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the Association forComputational Linguistics and the 4th InternationalJoint Conference on Natural Language Processing ofthe Asian Federation of Natural Language Processing,ACL-IJCNLP ?09, pages 85?88, Suntec, Singapore,August.Megumi Kameyama.
1985.
Zero Anaphora: The case ofJapanese.
Ph.D. thesis, Stanford University.Karin Kipper, Hoa Trang Dang, and Martha Palmer.2000.
Class-Based Construction of a Verb Lexicon.In Proceedings of the 17th National Conferenceon Artificial Intelligence and 12th Conferenceon Innovative Applications of Artificial Intelli-gence, pages 691?696, Austin, Texas.
AAAI Press.http://verbs.colorado.edu/?mpalmer/projects/verbnet.html.Edward Loper, Szu ting Yi, and Martha Palmer.
2007.Combining Lexical Resources: Mapping between9PropBank and VerbNet.
In Proceedings of the 7th In-ternational Workshop on Computational Linguistics.Alexis Mitchell, Stephanie Strassel, Mark Przybocki,JK Davis, George Doddington, Ralph Grishman,Adam Meyers, Ada Brunstein, Lisa Ferro, and BethSundheim, 2003.
ACE-2 Version 1.0.
Linguistic DataConsortium, Philadelphia.Vincent Ng and Claire Cardie.
2002.
Improving Ma-chine Learning Approaches to Coreference Resolu-tion.
In Proceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics, ACL ?02,pages 104?111, Philadelphia, Pennsylvania.Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-man, Lynette Hirschman, Marcia Linebarger, and JohnDowding.
1986.
Recovering Implicit Information.
InProceedings of the 24th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 10?19,New York, New York, USA.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?106, March.Altaf Rahman and Vincent Ng.
2011.
Narrowing themodeling gap: A cluster-ranking approach to corefer-ence resolution.
Journal of Artificial Intelligence Re-search, 40:469?521.Nils Reiter.
2010.
FrameNet API.
http://www.cl.uni-heidelberg.de/trac/FrameNetAPI.Philip Resnik.
1996.
Selectional Constraints: anInformation-theoretic Model and its ComputationalRealization.
Cognition, 61(1-2):127?159, November.Josef Ruppenhofer, Caroline Sporleder, Roser Morante,Collin Baker, and Martha Palmer.
2009.
SemEval-2010 Task 10: Linking Events and Their Participantsin Discourse.
In Proceedings of the NAACL-HLT 2009Workshop on Semantic Evaluations: Recent Achieve-ments and Future Directions (SEW-09), pages 106?111, Boulder, Colorado, June.Josef Ruppenhofer, Caroline Sporleder, Roser Morante,Collin Baker, and Martha Palmer.
2010.
SemEval-2010 Task 10: Linking Events and Their Participantsin Discourse.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluations, pages 45?50, Up-psala, Sweden, July.Josef Ruppenhofer, Philip Gorinski, and CarolineSporleder.
2011.
In Search of Missing Arguments:A Linguistic Approach.
In Proceedings of the Inter-national Conference Recent Advances in Natural Lan-guage Processing, pages 331?338, Hissar, Bulgaria,September.Carina Silberer.
2011.
Linking Implicit Semantic Rolesin Discourese Using Coreference Resolution Methods.Master?s thesis, Department of Computational Lin-guistics, Heidelberg University, Germany.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A Machine Learning Approach to Coref-erence Resolution of Noun Phrases.
ComputationalLinguistics, 27:521?544, December.Joel R. Tetreault.
2002.
Implicit Role Reference.
InInternational Symposium on Reference Resolution forNatural Language Processing, pages 109?115, Ali-cante, Spain.Sara Tonelli and Rodolfo Delmonte.
2010.
VENSES++:Adapting a Deep Semantic Processing System to theIdentification of Null Instantiations.
In Proceedings ofthe 5th International Workshop on Semantic Evalua-tions, pages 296?299, Uppsala, Sweden, July.Sara Tonelli and Rodolfo Delmonte.
2011.
DesperatelySeeking Implicit Arguments in Text.
In Proceedings ofthe ACL 2011 Workshop on Relational Models of Se-mantics, pages 54?62, Portland, Oregon, USA, June.G.
Whittemore, M. Macpherson, and G. Carlson.
1991.Event-building through role filling and anaphora reso-lution.
In Proceedings of the 29th Annual Meeting onAssociation for Computational Linguistics, pages 17?24, Morristown, NJ, USA.Ian H. Witten and Eibe Frank.
2000.
Data Mining: Prac-tical Machine Learning Tools and Techniques withJava Implementations.
Morgan Kaufmann, San Fran-cisco, CA, USA.Xiaofeng Yang, Jian Su, Guodong Zhou, and Chew LimTan.
2004.
An NP-cluster Based Approach to Coref-erence Resolution.
In Proceedings of the 20th In-ternational Conference on Computational Linguistics,COLING ?04, pages 226?232, Geneva, Switzerland.Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, TingLiu, and Sheng Li.
2008.
An Entity-Mention Modelfor Coreference Resolution with Inductive Logic Pro-gramming.
In Proceedings of the 46th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, ACL ?08:HLT, pages843?851, Columbus, Ohio, June.10
