Towards a Simple and Accurate Statistical Approach to LearningTranslation Relationships among WordsRobert C. MooreMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USAbobmoore@microsoft.comAbstractWe report on a project to derive wordtranslation relationships automaticallyfrom parallel corpora.
Our effort isdistinguished by the use of simpler,faster models than those used in pre-vious high-accuracy approaches.
Ourmethods achieve accuracy on single-word translations that seems compara-ble to any work previously reported, upto nearly 60% coverage of word types,and they perform particularly well on aclass of multi-word compounds of spe-cial interest to our translation effort.1 IntroductionThis paper is a report on work in progress aimedat learning word translation relationships auto-matically from parallel bilingual corpora.
Our ef-fort is distinguished by the use of simple statisti-cal models that are easier to implement and fasterto run than previous high-accuracy approaches tothis problem.Our overall approach to machine translation isa deep-transfer approach in which the transfer re-lationships are learned from a parallel bilingualcorpus (Richardson et al, 2001).
More specifi-cally, the transfer component is trained by pars-ing both sides of the corpus to produce parallellogical forms, using lexicons and analysis gram-mars constructed by linguists.
The parallel log-ical forms are then aligned at the level of con-tent word stems (lemmas), and logical-form trans-fer patterns are learned from the aligned logical-form corpus.
At run time, the source languagetext is parsed into logical forms employing thesource language grammar and lexicon used inconstructing the logical-form training corpus, andthe logical-form transfer patterns are used to con-struct target language logical forms.
These logicalforms are transformed into target language stringsusing the target-language lexicon, and a genera-tion grammar written by a linguist.The principal roles played by the translationrelationships derived by the methods discussedin this paper are to provide correspondences be-tween content word lemmas in logical forms toassist in the alignment process, and to augmentthe lexicons used in parsing and generation, for aspecial case described in Section 4.2 Previous WorkThe most common approach to deriving transla-tion lexicons from empirical data (Catizone, Rus-sell, and Warwick, 1989; Gale and Church, 1991;Fung, 1995; Kumano and Hirakawa, 1994; Wuand Xia, 1994; Melamed, 1995) is to use somevariant of the following procedure:1 Pick a good measure of the degree of asso-ciation between words in language L1andwords in language L2in aligned sentencesof a parallel bilingual corpus. Rank order pairs consisting of a word fromL1and a word from L2according to themeasure of association.1The important work of Brown et al (1993) is not di-rectly comparable, since their globally-optimized genera-tive probabilistic model of translation never has to make afirm commitment as to what can or cannot be a translationpair.
They assign some nonzero probability to every possibletranslation pair. Choose a threshold, and add to the transla-tion lexicon all pairs of words whose degreeof association is above the threshold.As Melamed later (1996, 2000) pointed out,however, this technique is hampered by the ex-istence of indirect associations between wordsthat are not mutual translations.
For exam-ple, in our parallel French-English corpus (con-sisting primarily of translated computer soft-ware manuals), two of the most strongly associ-ated word lemma translation pairs are fichier/fileand syste`me/system.
However, because themonolingual collocations syst`eme de fichiers,fichiers syste`me, file system, and system filesare so common, the spurious translation pairsfichier/system and syste`me/file also receive ratherhigh association scores?higher in fact that suchtrue translation pairs as confiance/trust, par-alle?lisme/parallelism, and film/movie.Melamed?s solution to this problem is not to re-gard highly-associated word pairs as translationsin sentences in which there are even more highly-associated pairs involving one or both of the samewords.
Since indirect associations are normallyweaker than direct ones, this usually succeeds inselecting true translation pairs over the spuriousones.
For example, in parallel sentences contain-ing fichier and syste`me on the French side and fileand system on the English side, the associations offichier/system and syste`me/file will be discounted,because the degrees of association for fichier/fileand syste`me/system are so much higher.Melamed?s results using this approach extendthe range of high-accuracy output to much highercoverage levels than previously reported.
Our ba-sic method is rooted the same insight regardingcompeting associations for the same words, butwe embody it in simpler model that is easier toimplement and, we believe, faster to run.2 As wewill see below, our model yields results that seemcomparable to Melamed?s up to nearly 60% cov-erage of the lexicon.A second important issue regarding automaticderivation of translation relationships is the as-sumption implicit (or explicit) in most previouswork that lexical translation relationships involve2Melamed does not report computation time for the ver-sion of his approach without generation of compounds, butour approach omits a number of computationally very ex-pensive steps performed in his approach.only single words.
This is manifestly not the case,as is shown by the following list of translationpairs selected from our corpus:base de donne?es/databasemot de passe/passwordsauvegarder/back upannuler/roll backouvrir session/log onSome of the most sophisticated work on thisaspect of problem again seems to be that ofMelamed (1997).
Our approach in this case isquite different from Melamed?s.
It is more gen-eral in that it can propose compounds that are dis-contiguous in the training text, as roll back wouldbe in a phrase such as roll the failed transac-tion back.
Melamed does allow skipping overone or two function words, but our basic methodis not limited at all by word adjacency.
Also,our approach is again much simpler computation-ally than Melamed?s and apparently runs ordersof magnitude faster.33 Our Basic MethodOur basic method for deriving translation pairsconsists of the following steps:1.
Extract word lemmas from the logical formsproduced by parsing the raw training data.2.
Compute association scores for individuallemmas.3.
Hypothesize occurrences of compounds inthe training data, replacing lemmas consti-tuting hypothesized occurrences of a com-pound with a single token representing thecompound.4.
Recompute association scores for com-pounds and remaining individual lemmas.3Melamed reports that training on 13 million words tookover 800 hours in Perl on a 167-MHz UltraSPARC proces-sor.
Training our method on 6.6 million words took approx-imately 0.5 hours in Perl on a 1-GHz Pentium III proces-sor.
Even allowing an order of magnitude for the differencesin processor speed and amount of data, there seems to bea difference between the two methods of at least two or-ders of magnitude in computation required.
UnfortunatelyMelamed evaluates accuracy in his work on translation com-pounds differently from his work on single-word translationpairs, so we are not able to compare our method to his in thatregard.5.
Recompute association scores, taking intoaccount only co-occurrences such that thereis no equally strong or stronger associationfor either item in the aligned logical-formpair.We describe each of these steps in detail below.3.1 Extracting word lemmasIn Step 1, we simply collect, for each sentence,the word lemmas identified by our MT systemparser as the key content items in the logical form.These are predominantly morphologically ana-lyzed word stems, omitting most function words.In addition, however, the parser treats certain lexi-cal compounds as if they were single units.
Theseinclude multi-word expressions placed in the lex-icon because they have a specific meaning or use,plus a number of general categories includingproper names, names of places, time expressions,dates, measure expressions, etc.
We will refer toall of these generically as ?multiwords?.The existence of multiwords simplifies learn-ing some translation relationships, but makes oth-ers more complicated.
For example, we do not,in fact, have to learn base de donne?es as a com-pound translation for database, because it is ex-tracted from the French logical forms alreadyidentified as a single unit.
Thus we only needto learn the base de donne?es/database correspon-dence as a simple one-to-one mapping.
On theother hand, the disque dur/hard disk correspon-dence is learned as two-to-one relationship inde-pendently of disque/disk and dur/hard (which arealso learned) because hard disk appears as a mul-tiword in our English logical forms, but disqueand dur always appear as separate tokens in ourFrench logical forms.3.2 Computing association scoresFor Step 2, we compute the degree of associationbetween a lemma wL1and a lemma wL2in termsof the frequencies with which wL1occurs in sen-tences of the L1part of the training corpus andwL2occurs in sentences of the L2part of the train-ing corpus, compared to the frequency with whichwL1and wL2co-occur in aligned sentences of thetraining corpus.
For this purpose, we ignore mul-tiple occurrences of a lemma in a single sentence.As a measure of association, we use the log-likelihood-ratio statistic recommended by Dun-ning (1993), which is the same statistic used byMelamed to initialize his models.
This statis-tic gives a measure of the likelihood that twosamples are not generated by the same probabil-ity distribution.
We use it to compare the over-all frequency of wL1in our training data to thefrequency of wL1given wL2(i.e., the frequencywith which wL1occurs in sentences of L1thatare aligned with sentences of L2in which wL2occurs).
Since p(wL1) = p(wL1jwL2) only ifoccurrences of wL1and wL2are independent, ameasure of the likelihood that these distributionsare different is, therefore, a measure of the like-lihood that an observed positive association be-tween wL1and wL2is not accidental.Since this process generates association scoresfor a huge number of lemma pairs for a largetraining corpus, we prune the set to restrict ourconsideration to those pairs having at least somechance of being considered as translation pairs.We heuristically set this threshold to be the degreeof association of a pair of lemmas that have oneco-occurrence, plus one other occurrence each.3.3 Hypothesizing compounds andrecomputing association scoresIf our data were very clean and all transla-tions were one-to-one, we would expect that inmost aligned sentence pairs, each word or lemmawould be most strongly associated with its trans-lation in that sentence pair; since, as Melamedhas argued, direct associations should be strongerthan indirect ones.
Since translation is symmet-ric, we would expect that if wL1is most stronglyassociated with wL2, wL2would be most stronglyassociated with wL1.
Violations of this pattern aresuggestive of translation relationships involvingcompounds.
Thus, if we have a pair of alignedsentences in which password occurs in the En-glish sentence and mot de passe occurs in theFrench side, we should not be surprised if motand passe are both most strongly associated withpassword within this sentence pair.
Password,however, cannot be most strongly associated withboth mot and passe.Our method carrying out Step 3 is based onfinding violations of the condition that wheneverwL1is most strongly associated with wL2, wL2ismost strongly associated with wL1.
The methodis easiest to explain in graph-theoretic terms.
Letthe nodes of a graph consist of all the lemmas ofL1and L2in a pair of aligned sentences.
For eachlemma, add a link to the uniquely most stronglyassociated lemma of the other language.4 Con-sider the maximal, connected subgraphs of the re-sulting graph.
If all translations within the sen-tence pair are one-to-one, each of these subgraphsshould contain exactly two lemmas, one from L1and one from L2.
For every subgraph containingmore than two lemmas of one of the languages,we consider all the lemmas of that language in thesubgraph to form a compound.
In the case of mot,passe, and password, as described above, therewould be a connected subgraph containing thesethree lemmas; so the two French lemmas, mot andpasse, would be considered to form a compoundin the French sentence under consideration.The output of this step of our process is a trans-formed set of lemmas for each sentence in the cor-pus.
For each sentence and each subset of thelemmas in that sentence that has been hypothe-sized to form a compound in the sentence, wereplace those lemmas with a token representingthem as a single unit.
Note that this process workson a sentence-pair by sentence-pair basis, so thata compound hypothesized for one sentence pairmay not be hypothesized for a different sentencepair, if the pattern of strongest associations for thetwo sentence pairs differ.
Order of occurrence isnot considered in forming these compounds, andthe same token is always used to represent thesame set of lemmas.5Once the sets of lemmas for the training cor-pus have been reformulated in terms of the hy-pothesized compounds, Step 4 consists simply inrepeating step 2 on the reformulated training data.3.4 Recomputing association scores, takinginto account only strongest associationsIf Steps 1?4 worked perfectly, we would havecorrectly identified all the compounds needed fortranslation and reformulated the training data totreat each such compound as a single item.
At this4Because the data becomes quite noisy if a lemma has nolemmas in the other language that are very strongly associ-ated with it, we place a heuristically chosen threshold on theminimum degree of association that is allowed to produce alink.5The surface order is not needed by the alignment proce-dure intended to make use of the translation relationships wediscover.point, we should be able to treat the training dataas if all translations are one-to-one.
We thereforechoose our final set of ranked translation pairs onthe assumption that true translation pairs will al-ways be mutually most strongly associated in agiven aligned sentence pair.Step 5 thus proceeds exactly as step 4, exceptthat wL1and wL2are considered to have a jointoccurrence only if wL1is uniquely most stronglyassociated with wL2, and wL2is uniquely moststrongly associated with wL1, among the lemmas(or compound lemmas) present in a given alignedsentence pair.
(The associations computed by theprevious step are used to make these decisions.
)This final set of associations is then sorted in de-creasing order of strength of association.4 Identifying Translations of ?Captoids?In addition to using these techniques to providetranslation relationships to the logical-form align-ment process, we have applied related methods toa problem that arises in parsing the raw input text.Often in text?particularly the kind of technicaltext we are experimenting with?phrases are used,not in their usual way, but as the name of some-thing in the domain.
Consider, Click to removethe View As Web Page check mark.
In this sen-tence, View As Web Page has the syntactic formof a nonfinite verb phrase, but it is used as if it is aproper name.
If the parser does not recognize thisspecial use, it is virtually impossible to parse thesentence correctly.Expressions of this type are fairly easily han-dled by our English parser, however, because cap-italization conventions in English make them easyto recognize.
The tokenizer used to prepare sen-tences for parsing, under certain conditions, hy-pothesizes that sequences of capitalized wordssuch as View As Web Page should be treated aslexicalized multi-word expressions, as discussedin Section 3.1.
We refer to this subclass of mul-tiwords as ?captoids?.
The capitalization conven-tions of French (or Spanish) make it harder to rec-ognize such expressions, however, because typi-cally only the first word of such an expression iscapitalized.We have adapted the methods described in Sec-tion 3 to address this problem by finding se-quences of French words that are highly asso-ciated with English captoids.
The sequences ofFrench words that we find are then added to theFrench lexicon as multiwords.The procedure for identifying translations ofcaptiods is as follows:1.
Tokenize the training data to separate wordsfrom punctuation and identify multiwordswherever possible.2.
Compute association scores for items in thetokenized data.3.
Hypothesize sequences of French words ascompounds corresponding to English mul-tiwords, replacing hypothesized occurrencesof a compound in the training data with a sin-gle token representing the compound.4.
Recompute association scores for pairs ofitems where either the English item or theFrench item is a multiword beginning with acapital letter.5.
Filter the resulting list to include only trans-lation pairs such that there is no equallystrong or stronger association for either itemin the training data.There are a number of key differences from ourprevious procedure.
First, since this process ismeant to provide input to parsing, it works on to-kenized word sequences rather than lemmas ex-tracted from logical forms.
Because many of theEnglish multiwords are so rare that associationsfor the entire multiword are rather weak, in Step2 we count occurrences of the constituent wordscontained in multiwords as well as occurrences ofthe multiwords themselves.
Thus an occurrenceof View As Web Page would also count as an oc-currence of view, as, web, and page.6The method of hypothesizing compounds inStep 3 adds a number of special features to im-prove accuracy and coverage.
Since we knowwe are trying to find French translations for En-glish captoids, we look for compounds only in theFrench data.
If any of the association scores be-tween a French word and the constituent words ofan English multiword are higher than the associa-tion score between the French word and the entiremultiword, we use the highest such score to repre-sent the degree of association between the French6In identifying captoid translations, we ignore case dif-ferences for computing and using association scores.word and the English multiword.
We reserve,for consideration as the basis of compounds, onlysets of French words that are most strongly as-sociated in a particular aligned sentence pair withan English multiword that starts with a capitalizedword.Finally we scan the French sentence of thealigned pair from left to right, looking for a cap-italized word that is a member of one of thecompound-defining sets for the pair.
When wefind such a word, we begin constructing a Frenchmultiword.
We continue scanning to the right tofind other members of the compound-defining set,allowing up to two consecutive words not in theset, provided that another word in the set imme-diately follows, in order to account for Frenchfunction words than might not have high asso-ciations with anything in the English multiword.We stop adding to the French multiword once wehave found all the French words in the compound-defining set, or if we encounter a punctuationsymbol, or if we encounter three or more con-secutive words not in the set.
If either of the lat-ter two conditions occurs before exhausting thecompound-defining set, we assume that the re-maining members of the set represent spurious as-sociations and we leave them out of the Frenchmultiword.The restriction in Step 4 to consider only asso-ciations in which one of the items is a mutiwordbeginning with a capital letter is simply for effi-ciency, since from this point onward no other as-sociations are of interest.The final filter applied in Step 5 is more strin-gent than in our basic method.
The reasoning isthat, while a single word may have more than onetranslation in different contexts, the sort of com-plex multiword represented by a captoid wouldnormally be expected to receive the same trans-lation in all contexts.
Therefore we accept onlytranslations involving captoids that are mutuallyuniquely most strongly associated across the en-tire corpus.
To focus on the cases we are mostinterested in and to increase accuracy, we requireeach translation pair generated to satisfy the fol-lowing additional conditions: The French item must be one of the mulit-words we constructed. The English item must be a multiword, all ofType Mean Token Total Single-Word Multiword CompoundCoverage Count Coverage Accuracy Accuracy Accuracy Accuracy0.040 1247.23 0.859 0.902?0.920 0.927?0.934 0.900?0.900 0.615?0.7690.080 670.88 0.923 0.842?0.870 0.922?0.939 0.879?0.879 0.453?0.5470.120 457.79 0.945 0.801?0.834 0.908?0.924 0.734?0.766 0.452?0.5480.160 347.58 0.957 0.783?0.820 0.898?0.913 0.705?0.737 0.455?0.5620.200 280.17 0.964 0.762?0.797 0.893?0.911 0.638?0.688 0.449?0.5270.240 234.63 0.969 0.749?0.783 0.887?0.904 0.606?0.658 0.431?0.5050.280 201.89 0.973 0.728?0.767 0.878?0.898 0.575?0.637 0.411?0.4870.320 177.11 0.975 0.712?0.752 0.875?0.896 0.577?0.643 0.375?0.4490.360 158.08 0.979 0.668?0.710 0.860?0.884 0.511?0.578 0.340?0.4050.400 142.45 0.980 0.654?0.696 0.845?0.871 0.486?0.556 0.329?0.3910.440 129.60 0.981 0.637?0.677 0.844?0.869 0.485?0.550 0.298?0.3540.480 118.90 0.982 0.641?0.680 0.848?0.872 0.502?0.566 0.297?0.3510.520 109.83 0.983 0.643?0.681 0.852?0.875 0.511?0.574 0.291?0.3440.560 102.15 0.984 0.626?0.664 0.839?0.864 0.503?0.564 0.279?0.3290.600 95.50 0.986 0.596?0.636 0.823?0.852 0.484?0.541 0.255?0.3050.632 90.87 0.989 0.550?0.595 0.784?0.819 0.429?0.488 0.232?0.286Table 1: Results for basic method.whose constituent words are capitalized. The French item must contain at least asmany words as the English item.The last condition corrects some errors made byallowing highly associated French words to be leftout of the hypothesized compounds.5 Experimental ResultsOur basic method for finding translation pairswas applied to a set of approximately 200,000French and English aligned sentence pairs, de-rived mainly from Microsoft technical manuals,resulting in 46,599 potential translation pairs.
Thetop 42,486 pairs were incorporated in the align-ment lexicon of our end-to-end translation sys-tem.7 The procedure for finding translations ofcaptoids was applied to a slight superset of thetraining data for the basic procedure, and yielded2561 possible translation pairs.
All of these wereadded to our end-to-end translation system, withthe French multiwords being added to the lexiconof the French parser, and the translation pairs be-ing added to the alignment lexicon.The improvements in end-to-end performancedue to these additions in a French-to-Englishtranslation task are described elsewhere (Pinkhamand Corston-Oliver, 2001).
For this report, wehave evaluated our techniques for finding trans-7As of this writing, however, the alignment proceduredoes not yet make use of the general translation pairs involv-ing compounds, although it does make use of the captoidtranslation compounds.lation pairs by soliciting judgements of transla-tion correctness from fluent French-English bilin-guals.
There were too many translation pairs toobtain judgements on each one, so we randomlyselected about 10% of the 42,486 general transla-tion pairs that were actually added to the system,and about 25% of the 2561 captoid pairs.The accuracy of the most strongly associatedtranslation pairs produced by the basic methodat various levels of coverage is displayed in Ta-ble 1.
We use the terms ?coverage?
and ?accu-racy?
in essentially the same way as Melamed(1996, 2000).
?Type coverage?
means the pro-portion of distinct lexical types in the entiretraining corpus, including both French and En-glish, for which there is at least one translationgiven.
As with the comparable results reportedby Melamed, these are predominantly single lem-mas for content words, but we also include oc-currences multiwords as distinct types.
?Meancount?
is the average number of occurrences ofeach type at the given level of coverage.
?Tokencoverage?
is the proportion of the total number ofoccurrences of items in the text represented by thetypes included within the type coverage.Since the judges were asked to evaluate theproposed translations out of context, we allowedthem to give an answer of ?not sure?, as well as?correct?
and ?incorrect?.
Our accuracy scoresare therefore given as a range, where the lowscore combines answers of ?not sure?
and ?in-correct?, and the high score combines answers of?not sure?
and ?correct?.Type Mean Token Single-WordCoverage Count Coverage Accuracy0.040 1628.57 0.791 0.948?0.9480.080 909.48 0.884 0.938?0.9420.120 626.84 0.914 0.926?0.9430.160 480.50 0.934 0.909?0.9240.200 389.11 0.945 0.896?0.9120.240 327.03 0.953 0.891?0.9100.280 281.76 0.958 0.896?0.9130.320 247.67 0.963 0.876?0.8960.360 220.62 0.965 0.876?0.8980.400 199.42 0.969 0.864?0.8870.440 181.69 0.971 0.846?0.8720.480 166.64 0.971 0.843?0.8680.520 153.90 0.972 0.848?0.8720.560 143.22 0.974 0.844?0.8680.600 133.87 0.976 0.830?0.8590.636 127.46 0.984 0.784?0.819Table 2: Results for single words only.Type Mean Token CaptoidCoverage Count Coverage Accuracy0.020 50.39 0.149 0.913?0.9130.040 30.19 0.178 0.902?0.9020.060 21.67 0.192 0.914?0.9140.080 17.88 0.211 0.911?0.9150.100 14.79 0.218 0.901?0.9040.120 12.61 0.223 0.859?0.8640.140 11.06 0.228 0.860?0.8640.160 9.95 0.235 0.858?0.8620.180 9.04 0.240 0.846?0.8510.194 8.70 0.249 0.841?0.847Table 3: Results for captoids.The ?total accuracy?
column gives results atdifferent levels of coverage over all the transla-tion pairs generated by our basic method.
Fora more detailed analysis, the remaining columnsprovide a breakdown for single-word translations,translations involving multiwords given to us bythe parser (?multiword accuracy?
), and new mul-tiwords hypothesized by our procedure (?com-pound accuracy?).
As the table shows, our perfor-mance is quite good on single-word translations,with accuracy of around 80% even at our cut-offof 63% type coverage, which represents 99% ofthe tokens in the corpus.To compare our results more directly withMelamed?s published results on single-wordtranslation, we show Table 2, where both cover-age and accuracy are given for single-word trans-lations only.
According to the standard of cor-rectness Melamed uses that is closest to ours, hereports 92% accuracy at 36% type coverage, 89%accuracy at 46% type coverage, and 87% accu-racy at 90% type coverage, on a set of 300,000aligned sentence pairs from the French-EnglishHansard corpus of Candian Parliament proceed-ings.
Our accuracies at the first two of these cov-erage points are 88?90% and 84?87%, which isslightly lower than Melamed, but given the dif-ferent corpus, different judges, and different eval-uation conditions, one cannot draw any definiteconclusions about which method is more accu-rate at these coverage levels.
Our method, how-ever, does not produce any result approaching90% type coverage, and accuracy appears to startdropping rapidly below 56% type coverage.
Nev-ertheless, this still represents good accuracy up to97% token coverage.Returning to Table 1, we see that our accu-racy on multiwords is much lower than on singlewords, especially the multiwords hypothesized byour learning procedure.
The results are muchbetter, however, when we look at the results forour specialized method for finding translations ofcaptoids, as shown in Table 3.
Our accuracy atnearly 20% type coverage is around 84%, whichis higher than our accuracy for general translationpairs (76?80%) at the same type coverage level.
Itis lower than our single-word translation accuracy(90?91%) at this coverage level, but it is strik-ing how close it is, given far less data.
At 20%type coverage of single words, there are 389 to-kens per word type, while at 20% type coverageof captoids, there are fewer than 9 tokens per cap-toid type.
In fact, further analysis shows that ofthe 2561 captoid translation pairs, 947 have onlya single example of the English captoid in thetraining data, yet our accuracy on these is around82%.
We note, however, that our captoid learningprocedure cuts off at around 20% type coverage,which is only 25% token coverage for these items.6 ConclusionsWe have evaluated our approach and found it tobe comparable in accuracy on single-word trans-lations to Melamed?s results (which appear to bethe best previous results, as far as one can tellgiven the lack of standard test corpora) up tonearly 60% type coverage and 97% token cover-age.
Space does not permit a detailed compari-son of Melamed?s methods to ours, but we repeatthat ours are far simpler to implement and muchfaster to run.
Our approach to generating trans-lations involving muti-word compounds performsless well in general, but the special-case modifica-tion of it to deal with captoids performs with veryhigh accuracy for those captoids it is able to finda translation for.
Based on these results, the fo-cus of our future work will be to try to extend ourregion of high-accuracy single-word translation tohigher levels of coverage, improve the accuracy ofour general method for finding multiword transla-tions, and extend the coverage of our method fortranslating captoids.ReferencesP.
F. Brown, S. A. Della Pietra, V. J. Della Pietra,and R. L. Mercer.
1993.
The Mathemat-ics of Statistical Machine Translation: Param-eter Estimation.
Computational Linguistics,19(2):263?311.R.
Catizone, G. Russell, and S. Warwick.
1989.Deriving translation data from bilingual texts.In Proceedings of the First International Lexi-cal Acquisition Workshop, Detroit, MI.T.
Dunning.
1993.
Accurate methods for thestatistics of surprise and coincidence.
Compu-tational Linguistics, 19(1):61?74.P Fung.
1995.
A pattern matching method forfinding noun and proper noun translations fromnoisy parallel corpora, In Proceedings of the33rd Annual Meeting, pages 236?243, Boston,MA.
Association for Computational Linguis-tics.W.
Gale and K. Church .
1991.
Identifying owrdcorrespondences in parallel texts.
In Proceed-ings Speech and Natural Language Workshop,pages 152?157, Asilomar, CA.
DARPA.A.
Kumano and H. Hirakawa.
1994.
Building anMT dictionary from parallel texts based on lin-guistic and statistical information.
In Proceed-ings of the 15th International Conference onComputational Linguistics, pages 76?81, Ky-oto, Japan.I.
D. Melamed.
1995.
Automatic evaluationand uniform filter cascades for inucing N -best translation lexicons.
In Proceedings ofthe Third Workshop on Very Large Corpora,pages 184?198, Cambridge, MA.I.
D. Melamed.
1996.
Automatic construction ofclean broad coverage translation lexicons.
InProceedings of the 2nd Conference of the As-sociation for Machine Translation in the Amer-icas, pages 125-134, Montreal, Canada.I.
D. Melamed.
1997.
Automatic discovery ofnon-compositional compounds in parallel data.In Proceedings of the 2nd Conference on En-pirical Methods in Natural Language Process-ing (EMNLP ?97), Providence, RI.I.
D. Melamed.
2000.
Models of Transla-tional Equivalence.
Computational Linguistics,26(2):221?249.J.
Pinkham and M. Corston-Oliver.
2001.
AddingDomain Specificity to an MT System.
InProceedings of the Workshop on Data-DrivenMachine Translation, 39th Annual Meeting ofthe Association for Computational Linguistics,Toulouse, France.S.
Richardson, W. B. Dolan, M. Corston-Oliver,and A. Menezes.
2001.
Overcoming thecustomization bottleneck using example-basedMT.
In Proceedings of the Workshop on Data-Driven Machine Translation, 39th AnnualMeeting of the Association for ComputationalLinguistics, Toulouse, France.D.
Wu and X. Xia.
1994.
Learning an English-Chinese lexicon from a parallel corpus.
In Pro-ceedings of the 1st Conference of the Associa-tion for Machine Translation in the Americas,pages 206?213, Columbia, MD.
