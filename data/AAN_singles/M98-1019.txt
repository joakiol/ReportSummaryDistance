NYU: DESCRIPTION OF THE JAPANESE NE SYSTEM USEDFOR MET-2Satoshi SekineComputer Science DepartmentNew York University715 Broadway, 7thoorNew York, NY 10003, USAsekine@cs.nyu.eduINTRODUCTIONIn this paper, experiments on the Japanese Named Entity task are reported.
We employed a supervisedlearning mechanism.
Recently, several systems have been proposed for this task, but many of them usehand-coded patterns.
Creating these patterns is laborious work, and when we adapt these systems to a newdomain or a new denition of named entities, it is likely to need a large amount of additional work.
On theother hand, in a supervised learning system, what is needed to adapt the system is to make new training dataand maybe additional small work.
While this is also not a very easy task, it would be easier than creatingcomplicated patterns.
For example, based on our experience, 100 training articles can be created in a day.There also have been several machine learning systems applied to this task.
However, these either 1)partially need hand-made rules, 2) have parameters which must be adjusted by hand 3) do not perform wellby fully automatic means or 4) need a huge training data.
Our system does not work fully automatically, butperforms well with a small training corpus and does not have parameters to be adjusted by hand.
We willdiscuss one of the related systems later..ALGORITHMIn this section, the algorithm of the system will be presented.
There are two phases, one for creating thedecision tree from training data (training phase) and the other for generating the tagged output based onthe decision tree (running phase).
We use a Japanese morphological analyzer, JUMAN [6] and a programpackage for decision trees, C4.5 [7].
We use three kinds of feature sets in the decision tree: Part-of-speech tagged by JUMANWe dene the set of our categories based on its major category and minor category. Character type informationCharacter type, like Kanji, Hiragana, Katakana, alphabet, number or symbol, etc.
and some combina-tions of these. Special DictionariesList of entities created based on JUMAN dictionary entries, lists distributed by SAIC for MUC, listsfound on the Web or based on human knowledge.
Table 1 shows the number of entities in eachdictionary1.
Organization name has two types of dictionary; one for proper names and the other forgeneral nouns which should be tagged when they co-occur with proper names.
Also, we have a specialdictionary which contains words written in Roman alphabet but most likely these are not an organization(e.g.
TEL, FAX).
We made a list of 93 such words.1Some of the lists are available at [8]Entity prex name suxOrg.
14 10076/49 175Person 0 17672 82Loc.
0 14903 60Date 24 199 29Time 2 24 5Money 15 0 39Percent 0 99 3Table 1: Special Dictionary EntriesCreating the special dictionaries is not very easy, but it is not very laborious work.
The initial dictionary wasbuilt in about a week.
In the course of the system development, in particular during creating the trainingcorpus, we added some entities to the dictionaries.The decision tree gives an output for each token.
It is one of the 4 possible combinations of opening,continuation and closing a named entity, and having no named entity, shown in Table 2.
In this paper, weOutput beginning endingOP-CL opening of NE closing of NEOP-CN opening of NE cont.
of NECN-CN cont.
of NE cont.
of NECN-CL cont.
of NE closing of NEnone none noneTable 2: Five types of Outputwill use two dierent sets of terms in order to avoid the confusion between positions relative to a token andregions of named entities.
The terms beginning and ending are used to indicate positions, whereas openingand closing are used to indicate the start and end of named entities.
Note that there is no overlapping orembedding of named entities.
An example of real data is shown in Figure 1.Training PhaseFirst, the training sentences are segmented and part-of-speech tagged by JUMAN.
Then each token isanalyzed by its character type and is matched against entries in the special dictionaries.
One token canmatch entries in several dictionaries.
For example, \Matsushita" could match the organization, person andlocation dictionaries.Using the training data, a decision tree is built.
It learns about the opening and closing of named entitiesbased on the three kinds of information of the previous, current and following tokens.
The three types ofinformation are the part-of-speech, character type and special dictionary information described above.If we just use the deterministic decision created by the tree, it could cause a problem in the running phase.Because the decisions are made locally, the system could make an inconsistent sequence of decisions overall.For example, one token could be tagged as the opening of an organization, while the next token might betagged as the closing of person name.
We can think of several strategies to solve this problem (for example,the method by [2] will be described in a later section), but we used a probabilistic method.The instances in the training corpus corresponding to a leaf of the decision tree may not all have thesame tag.
At a leaf we don't just record the most probable tag; rather, we keep the probabilities of theall possible tags for that leaf.
In this way we can salvage cases where a tag is part of the most probableglobally-consistent tagging of the text, even though it is not the most probable tag for this token, and sowould be discarded if we made a deterministic decision at each token.subsectionRunning PhaseIn the running phase, the rst three steps, token segmentation and part-of-speech tagging by JUMAN,analysis of character type, and special dictionary look-up, are identical to that in the training phase.
Then,in order to nd the probabilities of opening and closing a named entity for each token, the properties of theprevious, current and following tokens are examined against the decision tree.
Figure 2 shows two examplepaths in the decision tree.
For each token, the probabilities of `none' and the four combinations of answer pairsfor each named entity type are assigned.
For instance, if we have 7 named entity types, then 29 probabilitiesare generated.Once the probabilities for all the tokens in a sentence are assigned, the remaining task is to discover themost probable consistent path through the sentence.
Here, a consistent path means that for example, a pathcan't have org-OP-CN and date-OP-CL in a row, but can have loc-OP-CN and loc-CN-CL.
The output isgenerated from the consistent sequence with the highest probability for each sentence.
The Viterbi algorithmis used in the search; this can be run in time linear in the length of the input.EXAMPLEFigure 1 shows an example sentence along with three types of information, part-of-speech, character typeand special dictionary information, and given information of opening and closing of named entities.
Figure2 shows two example paths in the decision tree.
For the purpose of demonstration, we used the rst andsecond token of the example sentence in Figure 1.
Each line corresponds to a question asked by the treenodes along the path.
The last line shows the probabilities of named entity information which have morethan 0.0 probability.
This instance demonstrates how the probability method works.
As we can see, theprobability of none for the rst token (Isuraeru = Israel) is higher than that for the opening of organization(0.67 to 0.33), but in the second token (Keisatsu = Police), the probability of closing organization is muchhigher than none (0.86 to 0.14).
The combined probabilities of the two consistent paths are calculated.
Oneof these paths makes the two tokens an organization entity while along the other path, neither token is partof a named entity.
The probabilities are higher in the rst case (0.28) than that in the latter case (0.09), Sothe two tokens are tagged as an organization entity.Token ISURAERU KEISATSU NI YORU TO , ERUSAREMUPOS PN-loc N postpos V postpos comma PN-locChar.type Kata Kanji Hira Hira Hira Comma KataSpecial Dict.
loc org-S - - - - locNE answer org-OP-CN org-CN-CL - - - - loc-OP-CNToken SHI HOKUBU DE 26 NICHI GOGO ,POS N-suf N postpos number N-suf N commaChar.type Kanji Kanji Hira Num Kanji Kanji CommaSpecial Dict.
loc-S - - - date-S time,time-P -NE answer loc-CN-CL - - date-OP-CN date-CN-CL time-OP-CL -Figure 1: Sentence ExampleRESULTSWe will report results of ve experiments described in Table 3.
Here, \Training data", \Dry run data"and \Formal run data" are the data distributed by SAIC, and \seefu data" is the data created by Oki, NTTdata and NYU (available through [8]).
Note that all Training, Dry run and seefu data are in the topic ofISURAERU (first token) KEISATSU (second token)if current token is a location -> yes if current token is a location -> noif next token is a loc-suffix -> no if current token is a organization -> noif next token is a person-suffix -> no if current token is a time -> noif next token is a org-suffix -> yes if current token is a loc-suffix -> noif previous token is a location -> no if next token is a time-suffix -> noTHEN none = 0.67, org-OP-CN = 0.33 if current token is a time-suffix -> noif next token is a date-suffix -> noif current token is a date-suffix -> noif current token is a date -> noif next token is a location -> noif current token is a org-suffix -> yesif previous token is a location -> yesTHEN none = 0.14, org-CN-CL = 0.86Figure 2: Decision Tree Path Examplevehicle crash, and only the Formal run data is on the topic of space craft launch.
Numbers in the bracketsindicate the number of articles.Experiment Training Data Test Data1) Formal run Training data(114), seefu data(150), Formal run dataDry run data(30)2) Best in-house Dry run Training data(114), seefu data(150) Dry Run data3) 75/25 experiment 75% of Formal run Data(75) 25% of Formal run data4) All training + 75/25 Training data(114), seefu data(150), Dry 25% of Formal run datarun data(30),75% of Formal run data(75)5) Add planet names same as 4) same as 4)Table 3: RunsThe results of Formal run and the best in-house dry-run are shown in Table 4.
We can clearly tell thatthe recall of Named Entities (person, organization and location) are bad.
This is caused by the change of thetopic.
For example, there are very few foreign person names written in Katakana in the training data, (asa foreign person would hardly be a victim of a crash in Japan).
However, in the space craft launch, thereare many foreign person names written in Katakana.
This is the reason why the recall of persons is so low.Also, in the test documents, planet names, \the Sun","the Earth" or \Saturn" are tagged as locations, whichcould not be predicted from the training topic.
We missed all such names in the formal test.The best in-house Dry run result was achieved before the formal run without looking at the test data.
Soit should be regarded as an example of the performance if we know the topic of the material.
We think thisis satisfactory, considering that the eort we made was just preparing dictionaries and no patterns.Table 5 shows three experiments performed after the formal run.
As the topic change may degrade of theperformance, we conducted experiments in which the training data includes documents in the same topic.The rst experiment used 75% of the formal run data for training and the rest of the data for testing.
Foursuch experiments were made to obtain the result for the entire corpus.
The second experiment includes thetraining data used in the formal run in addition to the 75% of the formal run data.
The table shows about1% improvement over the formal run.
This is an encouraging result, the better performance was achievedwith only 75 articles on the same topic compared with 294 articles on a dierent topic used in the formalrun.
The result of the second experiment also shows a good sign that documents in a dierent topic helpedto improve the performance.
This result suggests an idea of \domain adaptation scheme".
That is to have aF-measure Formal run Best in-house dry runEntity Recall Precision Recall PrecisionOrg.
75 83 78 87Person 48 74 87 90Loc.
70 87 91 95Date 96 95 97 91Time 95 96 98 98Money 90 97 100 100Percent 90 95 88 100Overall 75 85 87 90F-measure 79.49 88.62Table 4: Result of Formal Run and the best in-house Dry runlarge general corpus of tagged documents as the basis, and to add small domain specic documents to have adomain specic system.
Lastly, in the third experiment, we added the planet names in the location dictionary.From the formal run result, it was clear that one of the main reasons of the performance degradation is thelack of the planet names.
The addition improves 3.5% which is better than the other trials.
Although thereare several other obvious reasons to be xed, the F-measure 86.34 is comparable to the best in-house Dryrun experiment described before (Experiment 2;F-measure = 88.62).Experiment F-measure3) 75/25 experiment 80.464) All training + 75/25 82.735) Add planet names 86.34Table 5: Comparative ResultsRELATED WORKThere have been several eorts to apply machine learning techniques to the same task [4] [3] [5] [2].
Inthis section, we will discuss a system which is one of the most advanced and which closely resembles our own[2].
A good review of most of the other systems can be found in their paper.Their system uses the decision tree algorithm and almost the same features.
However, there are signicantdierences between the systems.
The main dierence is that they have more than one decision tree, eachof which decides if a particular named entity starts/ends at the current token.
In contrast, our system hasonly one decision tree which produces probabilities of information about the named entity.
In this regard,we are similar to [3], which also uses a probabilistic method in their N-gram based system.
This is a crucialdierence which also has important consequences.
Because the system of [2] makes multiple decisions ateach token, they could assign multiple, possibly inconsistent tags.
They solved the problem by introducingtwo somewhat idiosyncratic methods.
One of them is the distance score, which is used to nd an openingand closing pair for each named entity mainly based on distance information.
The other is the tag priorityscheme, which chooses a named entity among dierent types of overlapping candidates based on the priorityorder of named entities.
These methods require parameters which must be adjusted when they are appliedto a new domain.
In contrast, our system does not require such methods, as the multiple possibilities areresolved by the probabilistic method.
This is a strong advantage, because we don't need manual adjustments.The result they reported is not comparable to our result, because the text and denition are dierent.But the total F-score of our system is similar to theirs, even though the size of our training data is muchsmaller.DISCUSSIONFirst, we have to consider topic or domain dependency of the task.
It is clear that in order to achievegood performance in the framework, we have to investigate dictionary entries for the task.
It may or may noteasy to modify the dictionary.
For example, a list of foreign person name written in Katakana is not so easyto create, whereas a list of planet names is easy to nd.
This diculty also exists in pattern-based methods,but in our framework it is not necessary to create domain dependent patterns.Currently creating dictionaries is done by hand.
One possibility to automatize the process is to use abootstrapping method.
Starting with core dictionaries, we can run the system on untagged texts, and increasethe entities in the dictionaries.Another issue is aliases.
In newspaper articles, aliases are often used.
The full name is used only therst time the company is mentioned (Matsushita Denki Sangyou Kabushiki Kaisya = Matsushita ElectricIndustrial Co. Ltd.) and then aliases (Matsushita or Matsushita Densan = Matsushita E.I.)
are used inthe later sections of the article.
Our system cannot handle these aliases, unless the aliases are registered inthe dictionaries.Also, lexical information should help the accuracy.
For example, a name, possibly a person or an orga-nization, in a particular argument slot of a verb can be disambiguated by the verb.
For example, a name inthe object slot of the verb `hire' might be a person, while a name in the subject slot of verb `manufacture'might be an organization.REFERENCES[1] Defense Advanced Research Projects Agency, \Proceedings of Workshop on Tipster Program Phase II"Morgan Kaufmann Publishers (1996)[2] Bennett, S., Aone, C. and Lovell, C., \Learning to Tag Multilingual Texts Through Observation" Con-ference on Empirical Methods in Natural Language Processing (1997)[3] Bikel, D., Miller, S., Schwartz, R. and Weischedel, R., \Nymble: a High-Performance Learning Name-nder" Proceedings of the Fifth Conference on Applied Natural Language Processing (1997)[4] Cowie, J., \Description of the CRL/NMSU Systems Used for MUC-6" Proceedings of Sixth MessageUnderstanding Conference (MUC-6) (1995)[5] Gallippi, A., \Learning to Recognize Names Across Languages" Proceedings of the 16th InternationalConference on Computational Linguistics (COLING-96) (1996)[6] Matsumoto, Y., Kurohashi, S., Yamaji, O., Taeki, Y. and Nagao, M., \Japanese morphological analyzingSystem: JUMAN" Kyoto University and Nara Institute of Science and Technology (1997)[7] Quinlan, R., \C4.5: Program for Machine Learning" Morgan Kaufmann Publishers (1993)[8] Sekine, S., \Homepage of data related Japanese Named Entity"http://cs.nyu.edu/cs/projects/proteus/met2j (1997)
