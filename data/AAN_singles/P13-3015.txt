Proceedings of the ACL Student Research Workshop, pages 103?109,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Comparison of Techniques to Automatically Identify Complex WordsMatthew ShardlowSchool of Computer Science, University of ManchesterIT301, Kilburn Building, Manchester, M13 9PL, Englandm.shardlow@cs.man.ac.ukAbstractIdentifying complex words (CWs) is animportant, yet often overlooked, taskwithin lexical simplification (The processof automatically replacing CWs with sim-pler alternatives).
If too many words areidentified then substitutions may be madeerroneously, leading to a loss of mean-ing.
If too few words are identified thenthose which impede a user?s understand-ing may be missed, resulting in a com-plex final text.
This paper addresses thetask of evaluating different methods forCW identification.
A corpus of sentenceswith annotated CWs is mined from Sim-ple Wikipedia edit histories, which is thenused as the basis for several experiments.Firstly, the corpus design is explained andthe results of the validation experimentsusing human judges are reported.
Exper-iments are carried out into the CW identi-fication techniques of: simplifying every-thing, frequency thresholding and traininga support vector machine.
These are basedupon previous approaches to the task andshow that thresholding does not performsignificantly differently to the more na?
?vetechnique of simplifying everything.
Thesupport vector machine achieves a slightincrease in precision over the other twomethods, but at the cost of a dramatic tradeoff in recall.1 IntroductionComplex Word (CW) identification is an impor-tant task at the first stage of lexical simplificationand errors introduced or avoided here will affectfinal results.
This work looks at the process of au-tomatically identifying difficult words for a lexi-cal simplification system.
Lexical simplificationis the task of identifying and replacing CWs in atext to improve the overall understandability andreadability.
This is a difficult task which is com-putationally expensive and often inadequately ac-curate.Lexical simplification is just one method oftext simplification and is often deployed alongsideother simplification methods (Carrol et al 1998;Alu?
?sio and Gasperin, 2010).
Syntactic simplifi-cation, statistical machine translation and seman-tic simplification (or explanation generation) areall current methods of text simplification.
Textsimplification is typically deployed as an assistivetechnology (Devlin and Tait, 1998; Alu?
?sio andGasperin, 2010), although this is not always thecase.
It may also be used alongside other tech-nologies such as summarisation to improve theirfinal results.Identifying CWs is a task which every lexicalsimplification system must perform, either explic-itly or implicitly, before simplification can takeplace.
CWs are difficult to define, which makesthem difficult to identify.
For example, take thefollowing sentence:The four largest islands are Honshu,Hokkaido, Shikoku, and Kyushu, andthere are approximately 3,000 smallerislands in the chain.In the above sentence, we might identify theproper nouns (Honshu, Hokkaido, etc.)
as com-plex (as they may be unfamiliar) or we may chooseto discount them from our scheme altogether, asproper nouns are unlikely to have any valid re-placements.
If we discount the proper nouns thenthe other valid CW would be ?approximately?.
At13 characters it is more than twice the average of5.7 characters per word and has more syllablesthan any other word.
Further, CWs are often iden-tified by their frequency (see Section 2.1) and here,103?approximately?
exhibits a much lower frequencythan the other words.There are many reasons to evaluate the identi-fication of CWs.
This research stems primarilyfrom the discovery that no previous comparison ofcurrent techniques exists.
It is hoped that by pro-viding this, the community will be able to iden-tify and evaluate new techniques using the meth-ods proposed herein.
If CW identification is notperformed well, then potential candidates may bemissed, and simple words may be falsely identi-fied.
This is dangerous as simplification will oftenresult in a minor change in a text?s semantics.
Forexample, the sentence:The United Kingdom is a state innorthwest Europe.May be simplified to give:The United Kingdom is a country innorthwest Europe.In this example from the corpus used in thisresearch, the word ?state?
is simplified to give?country?.
Whilst this is a valid synonym in thegiven context, state and country are not necessar-ily semantically identical.
Broadly speaking, staterefers to a political entity, whereas country refersto a physical space within a set of borders.
This isan acceptable change and even necessary for sim-plification.
However, if applied blindly, then toomany modifications may be made, resulting in ma-jor deviations from the text?s original semantics.The contributions of this paper are as follows:?
A report on the corpus developed and used inthe evaluation phase.
Section 2.2.?
The implementation of a support vector ma-chine for the classification of CWs.
Section2.6?
A comparison of common techniques on thesame corpus.
Section 4.?
An analysis of the features used in the sup-port vector machine.
Section 4.2 Experimental DesignSeveral systems for detecting CWs were imple-mented and evaluated using the CW corpus.
Thetwo main techniques that exist in the literatureare simplifying everything (Devlin and Tait, 1998)System ScoreSUBTLEX 0.3352Wikipedia Baseline 0.3270Kucera-Francis 0.3097Random Baseline 0.0157Table 1: The results of different exper-iments on the SemEval lexical simplifi-cation data.
These show that SUBTLEXwas the best word frequency measure forrating lexical complexity.
The other en-tries correspond to alternative word fre-quency measures.
The Google Web 1Tdata (Brants and Franz, 2006) has beenshown to give a higher score, however thisdata was not available during the courseof this research.and frequency based thresholding (Zeng et al2005).
These were implemented as well as a sup-port vector machine classifier.
This section de-scribes the design decisions made during imple-mentation.2.1 Lexical ComplexityAll three of the implementations described in Sec-tions 2.4, 2.5 and 2.6 require a word frequencymeasure as an indicator of lexical complexity.
If aword occurs frequently in common language thenit is more likely to be recognised (Rayner andDuffy, 1986).The lexical simplification dataset from Task 1at SemEval 2012 (De Belder and Moens, 2012)was used to compare several measures of wordfrequency as shown in Table 1.
Candidate sub-stitutions and sample sentences were provided bythe task organisers, together with a gold standardranking of the substitutes according to their sim-plicity.
These sentences were ranked accordingto their frequency.
Although the scores in Table1 appear to be low, this is the kappa agreementfor several categories and so should be expected.The inter-annotator agreement on the corpus was0.488 (De Belder and Moens, 2012).
The SUB-TLEX dataset (Brysbaert and New, 2009) was thebest available for rating word familiarity.
This isa corpus of over 70,000 words collected from thesubtitles of over 8,000 American English films.1042.2 CW CorpusSimple Wikipedia edit histories were mined usingtechniques similar to those in Yatskar et al(2010).This provided aligned pairs of sentences whichhad just one word simplified.
Whereas Yatskaret al(2010) used these pairs to learn probabili-ties of paraphrases, the research in this paper usedthem as instances of lexical simplification.
Theoriginal simplifications were performed by editorstrying to make documents as simple as possible.The CW is identified by comparison with the sim-plified sentence.
Further information on the pro-duction of the corpus will be published in a futurepaper.2.3 Negative ExamplesThe CW corpus provides a set of CWs in appro-priate contexts.
This is useful for evaluation asthese words need to be identified.
However, ifonly examples of CWs were available, it would bevery easy for a technique to overfit ?
as it couldjust classify every single word as complex andget 100% accuracy.
For example, in the case ofthresholding, if only examples of CWs are avail-able, the threshold could be set artificially highand still succeed for every case.
When this is ap-plied to genuine data it will classify every word itencounters as complex, leading to high recall butlow precision.To alleviate this effect, negative examples areneeded.
These are examples of simple wordswhich do not require any further simplification.There are several methods for finding these, in-cluding: selecting words from a reference easyword list; selecting words with high frequenciesaccording to some corpus or using the simplifiedwords from the second sentences in the CW cor-pus.
The chosen strategy picked a word at randomfrom the sentence in which the CW occurs.
Onlyone word was edited in this sentence and so theassumption may be made that none of the otherwords in the sentence require further simplifica-tion.
Only one simple word per CW is chosen toenforce an even amount of positive and negativedata.
This gave a set of negative words which werereflective of the broad language which is expectedwhen processing free text.2.4 Simplify EverythingThe first implementation involved simplifying ev-erything, a brute force method, in which a simpli-fication algorithm is applied to every word.
Thisassumes that words which are already simple willnot require any further simplification.
A com-mon variation is to limit the simplification to somecombination of all the nouns, verbs and adjectives.A standard baseline lexical simplification sys-tem was implemented following Devlin and Tait(1998).
This algorithm generated a set of syn-onyms from WordNet and then used the SUB-TLEX frequencies to find the most frequent syn-onym.
If the synonym was more frequent than theoriginal word then a substitution was made.
Thistechnique was applied to all the words.
If a CWwas changed, then it was considered a true posi-tive; if a simple word was not changed, it was con-sidered a true negative.
Five trials were carried outand the average accuracy and standard deviation isreported in Figure 1 and Table 3.2.5 Frequency ThresholdingThe second technique is frequency thresholding.This relies on each word having an associated fa-miliarity value provided by the SUBTLEX corpus.Whilst this corpus is large, it will never cover ev-ery possible word, and so words which are not en-countered are considered to have a frequency of 0.This does not affect comparison as the infrequentwords are likely to be the complex ones.To distinguish between complex and simplewords a threshold was implemented.
This waslearnt from the CW corpus by examining everypossible threshold for a training set.
Firstly, thetraining data was ordered by frequency, then theaccuracy1 of the algorithm was examined with thethreshold placed in between the frequency of everyadjacent pair of words in the ordered list.
This wasrepeated by 5-fold cross validation and the meanthreshold determined.
The final accuracy of thealgorithm was then determined on a separate setof testing data.2.6 Support Vector MachineSupport vector machines (SVM) are statisticalclassifiers which use labelled training data to pre-dict the class of unseen inputs.
The training dataconsist of several features which the SVM usesto distinguish between classes.
The SVM waschosen as it has been used elsewhere for similartasks (Gasperin et al 2009; Hancke et al 2012;Jauhar and Specia, 2012).
The use of many fea-1The proportion of data that was correctly classified.105tures allows factors which may otherwise havebeen missed to be taken into account.
One fur-ther advantage is that the features of an SVM canbe analysed to determine their effect on the classi-fication.
This may give some indication for futurefeature classification schemes.The SVM was trained using the LIBSVM pack-age (Chang and Lin, 2011) in Matlab.
the RBFkernel was selected and a grid search was per-formed to select values for the 2 parameters C and?.
Training and testing was performed on a held-out data-set using 5-fold cross validation.To implement the SVM a set of features wasdetermined for the classification scheme.
Severalexternal libraries were used to extract these as de-tailed below:Frequency The SUBTLEX frequency of eachword was used as previously described inSection 2.1.CD Count Also from the SUBTLEX corpus.
Thenumber of films in which a word appeared,ranging from 0?
8, 388.Length The word length in number of characterswas taken into account.
It is often the casethat longer words are more difficult to pro-cess and so may be considered ?complex?.Syllable Count The number of syllables con-tained in a word is also a good estimate ofits complexity.
This was computed using alibrary from the morphadorner package2.Sense Count A count of the number of ways inwhich a word can be interpreted - showinghow ambiguous a word is.
This measure istaken from WordNet (Fellbaum, 1998).Synonym Count Also taken from WordNet, thisis the number of potential synonyms withwhich a word could be replaced.
This againmay give some indication of a word?s degreeof ambiguity.3 ResultsThe results of the experiments in identifying CWsare shown in Figure 1 and the values are given inTable 3.
The values presented are the mean of 5trials and the error bars represent the standard de-viation.2http://morphadorner.northwestern.edu/00.20.40.60.81Everything Thresholding SVMScoreAccuracyF1PrecisionRecallFigure 1: A bar chart with error barsshowing the results of the CW identifi-cation experiments.
Accuracy, F1 Score,Precision and Recall are reported for eachmeasure.Feature CoefficientFrequency 0.3973CD Count 0.5847Length ?0.5661Syllables ?0.4414Senses ?0.0859Synonyms ?0.2882Table 2: The correlation coefficients foreach feature.
These show the correlationagainst the language?s simplicity and soa positive correlation indicates that if thatfeature is higher then the word will besimpler.To analyse the features of the SVM, the corre-lation coefficient between each feature vector andthe vector of feature labels was calculated.
This isa measure which can be used to show the relationbetween two distributions.
The adopted labellingscheme assigned CWs as 0 and simple words as 1and so the correlation of the features is notionallyagainst the simplicity of the words.3 The resultsare reported in Table 2.4 DiscussionIt is clear from these results that there is a fairlyhigh accuracy from all the methods.
This showsthat they perform well at the task in hand, reflect-ing the methods which have been previously ap-plied.
These methods all have a higher recall than3i.e.
A positive correlation indicates that if the value ofthat feature is higher, the word will be simpler.106System Accuracy F1 Precision RecallSimplify Everything 0.8207?
0.0077 0.8474?
0.0056 0.7375?
0.0084 0.9960?
0Thresholding 0.7854?
0.0138 0.8189?
0.0098 0.7088?
0.0136 0.9697?
0.0056SVM 0.8012?
0.0656 0.8130?
0.0658 0.7709?
0.0752 0.8665?
0.0961Table 3: The results of classification experiments for the three systems.precision, which indicates that they are good atidentifying the CWs, but also that they often iden-tify simple words as CWs.
This is particularlynoticeable in the ?simplify everything?
method,where the recall is very high, yet the precision iscomparatively low.
This indicates that many of thesimple words which are falsely identified as com-plex are also replaced with an alternate substitu-tion, which may result in a change in sense.A paired t-test showed the difference betweenthe thresholding method and the ?simplify ev-erything?
method was not statistically significant(p > 0.8).
Thresholding takes more data aboutthe words into account and would appear to be aless na?
?ve strategy than blindly simplifying every-thing.
However, this data shows there is little dif-ference between the results of the two methods.The thresholding here may be limited by the re-sources, and a corpus using a larger word countmay yield an improved result.Whilst the thresholding and simplify everythingmethods were not significantly different from eachother, the SVM method was significantly differ-ent from the other two (p < 0.001).
This can beseen in the slightly lower recall, yet higher preci-sion attained by the SVM.
This indicates that theSVM was better at distinguishing between com-plex and simple words, but also wrongly identifiedmany CWs.
The results for the SVM have a widestandard deviation (shown in the wide error bars inFigure 1) indicating a higher variability than theother methods.
With more data for training themodel, this variability may be reduced.One important factor in the increased precisionobserved in the SVM is that it used many morefeatures than the other methods, and so took moreinformation into account.
Table 2 shows that thesefeatures had varying degrees of correlation withthe data label (i.e.
whether the word was simpleor not) and hence that they had varying degrees ofeffect on the classification scheme.Frequency and CD count are moderately posi-tively correlated as may be expected.
This indi-cates that higher frequency words are likely to besimple.
Surprisingly, CD Count has a higher cor-relation than frequency itself, indicating that this isa better measure of word familiarity than the fre-quency measure.
However, further investigation isnecessary to confirm this.Word length and number of syllables are mod-erately negatively correlated, indicating that thelonger and more polysyllabic a word is, the lesssimple it becomes.
This is not true in every case.For example, ?finger?
and ?digit?
can be used inthe same sense (as a noun meaning an appendageof the hand).
Whilst ?finger?
is more commonlyused than ?digit?4, digit is one letter shorter.The number of senses was very weakly nega-tively correlated with word simplicity.
This in-dicates that it is not a strong indicative factor indetermining whether a word is simple or not.
Thetotal number of synonyms was a stronger indicatorthan the number of senses, but still only exhibitedweak correlation.One area that has not been explored in this studyis the use of contextual features.
Each target wordoccurs in a sentence and it may be the case thatthose words surrounding the target give extra in-formation as to its complexity.
It has been sug-gested that language is produced at an even levelof complexity (Specia et al 2012), and so simplewords will occur in the presence of other simplewords, whereas CWs will occur in the presenceof other CWs.
As well as lexical contextual in-formation, the surrounding syntax may offer someinformation on word difficulty.
Factors such asa very long sentence or a complex grammaticalstructure can make a word more difficult to under-stand.
These could be used to modify the familiar-ity score in the thresholding method, or they couldbe used as features in the SVM classifier.5 Related WorkThis research will be used for lexical simplifica-tion.
The related work in this field is also generally4in the SUBTLEX corpus ?finger?
has a frequency of1870, whereas ?digit?
has a frequency of 30.107used as a precursor to lexical simplification.
Thissection will explain how these previous methodshave handled the task of identifying CWs and howthese fit into the research presented in this paper.The simplest way to identify CWs in a sentenceis to blindly assume that every word is complex, asdescribed earlier in Section 2.4.
This was first usedin Devlin?s seminal work on lexical simplification(Devlin and Tait, 1998).
This method is some-what na?
?ve as it does not mitigate the possibilityof words being simplified in error.
Devlin and Taitindicate that they believe less frequent words willnot be subject to meaning change.
However, fur-ther work into lexical simplification has refutedthis (Lal and Ru?ger, 2002).
This method is stillused, for example Thomas and Anderson (2012)simplify all nouns and verbs.
This corresponds tothe ?Everything?
method.Another method of identifying CWs is to usefrequency based thresholding over word familiar-ity scores, as described in Section 2.5 and corre-sponding to the ?Frequency?
method in this pa-per.
This has been applied to the medical domain(Zeng et al 2005; Elhadad, 2006) for predictingwhich words lay readers will find difficult.
Thishas been correlated with word difficulty via ques-tionnaires (Zeng et al 2005; Zeng-Treitler et al2008) and via the analysis of low-level readabil-ity corpora (Elhadad, 2006).
In both these cases,a familiarity score is used to determine how likelya subject is to understand a term.
More recently,Bott et al(2012) use a threshold of 1% corpusfrequency, along with other checks, to ensure thatsimple words are not erroneously simplified.Support vector machines are powerful statisti-cal classifiers, as employed in the ?SVM?
methodof this paper.
A Support Vector Machine is usedto predict the familiarity of CWs in Zeng et al(2005).
It takes features of term frequency andword length and is correlated against the familiar-ity scores which are already obtained.
This provesto have very poor performance, something whichthe authors attribute to a lack of suitable train-ing data.
An SVM has also been trained for theranking of words according to their complexity(Jauhar and Specia, 2012).
This was done for theSemEval lexical simplification task (Specia et al2012).
Although this system is designed for syn-onym ranking, it could also be used for the CWidentification task.
Machine learning has also beenapplied to the task of determining whether an en-tire sentence requires simplification (Gasperin etal., 2009; Hancke et al 2012).
These approachesuse a wide array of morphological features whichare suited to sentence level classification.6 Future WorkThis work is intended as an initial study of meth-ods for identifying CWs for simplification.
Themethods compared, whilst typical of current CWidentification methods, are not an exhaustive setand variations exist.
One further way of expandingthis research would be to take into account wordcontext.
This could be done using thresholding(Zeng-Treitler et al 2008) or an SVM (Gasperinet al 2009; Jauhar and Specia, 2012).Another way to increase the accuracy of the fre-quency count method may be to use a larger cor-pus.
Whilst the corpus used in this paper per-formed well in the preliminary testing section,other research has shown the Google Web1T cor-pus (a n-gram count of over a trillion words) to bemore effective (De Belder and Moens, 2012).
TheWeb 1T data was not available during the courseof this research.The large variability in accuracy shown in theSVM method indicates that there was insufficienttraining data.
With more data, the SVM wouldhave more information about the classificationtask and would provide more consistent results.CW identification is the first step in the processof lexical simplification.
This research will be in-tegrated in a future system which will simplifynatural language for end users.
It is also hopedthat other lexical simplification systems will takeaccount of this work and will use the evaluationtechnique proposed herein to improve their identi-fication of CWs.7 ConclusionThis paper has provided an insight into the chal-lenges associated with evaluating the identifica-tion of CWs.
This is a non-obvious task, whichmay seem intuitively easy, but in reality is quitedifficult and rarely performed.
It is hoped thatnew research in this field will evaluate the tech-niques used, rather than using inadequate tech-niques blindly and na??vely.
This research has alsoshown that the current state of the art methodshave much room for improvement.
Low precisionis a constant factor in all techniques and future re-search should aim to address this.108AcknowledgmentThis work is supported by EPSRC grantEP/I028099/1.
Thanks go to the anonymousreviewers for their helpful suggestions.ReferencesSandra Maria Alu?
?sio and Caroline Gasperin.
2010.Fostering digital inclusion and accessibility: thePorSimples project for simplification of Portuguesetexts.
In Proceedings of the NAACL HLT 2010Young Investigators Workshop on ComputationalApproaches to Languages of the Americas, YIW-CALA ?10, pages 46?53, Stroudsburg, PA, USA.Association for Computational Linguistics.Stefan Bott, Luz Rello, Biljana Drndarevix, and Hora-cio Saggion.
2012.
Can spanish be simpler?
lex-sis: Lexical simplification for spanish.
In Coling2012: The 24th International Conference on Com-putational Linguistics.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram Version 1.Marc Brysbaert and Boris New.
2009.
Moving beyondKucera and Francis : a critical evaluation of currentword frequency norms and the introduction of a newand improved word frequency measure for Ameri-can English.
Behav Res Methods.John Carrol, Guido Minnen, Yvonne Canning, SiobhanDevlin, and John Tait.
1998.
Practical simplifica-tion of english newspaper text to assist aphasic read-ers.
In AAAI-98 Workshop on Integrating ArtificialIntelligence and Assistive Technology.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technol-ogy, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Jan De Belder and Marie-Francine Moens.
2012.
Adataset for the evaluation of lexical simplification.In Computational Linguistics and Intelligent TextProcessing, volume 7182 of Lecture Notes in Com-puter Science, pages 426?437.
Springer Berlin / Hei-delberg.Siobhan Devlin and John Tait.
1998.
The use of a psy-cholinguistic database in the simplification of textfor aphasic readers.
Linguistic Databases, pages161?173.Noe?mie Elhadad.
2006.
Comprehending techni-cal texts: Predicting and defining unfamiliar terms.In AMIA Annual Symposium proceedings, volume2006, page 239.
American Medical Informatics As-sociation.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Caroline Gasperin, Lucia Specia, Tiago Pereira, andSandra M.
Alu??sio.
2009.
Learning when to sim-plify sentences for natural text simplification.
In En-contro Nacional de Intelige?ncia Artificial.Julia Hancke, Sowmya Vajjala, and Detmar Meurers.2012.
Readability classification for German usinglexical, syntactic, and morphological features.
InProceedings of the 24th International Conference onComputational Linguistics (COLING 2012), pages1063?1080, Mumbai, India.Sujay Kumar Jauhar and Lucia Specia.
2012.
Uow-shef: Simplex ?
lexical simplicity ranking based oncontextual and psycholinguistic features.
In *SEM2012: The First Joint Conference on Lexical andComputational Semantics ?
Volume 1: Proceedingsof the main conference and the shared task, and Vol-ume 2: Proceedings of the Sixth International Work-shop on Semantic Evaluation (SemEval 2012), pages477?481, Montre?al, Canada, 7-8 June.
Associationfor Computational Linguistics.Partha Lal and Stefan Ru?ger.
2002.
Extract-basedsummarization with simplification.
In Proceedingsof the ACL.Keith Rayner and Susan Duffy.
1986.
Lexical com-plexity and fixation times in reading: Effects of wordfrequency, verb complexity, and lexical ambiguity.Memory & Cognition, 14:191?201.Lucia Specia, Sujay Kumar Jauhar, and Rada Mihal-cea.
2012.
Semeval-2012 task 1: English lexicalsimplification.
In First Joint Conference on Lexicaland Computational Semantics.S.
Rebecca Thomas and Sven Anderson.
2012.Wordnet-based lexical simplification of a document.In Jeremy Jancsary, editor, Proceedings of KON-VENS 2012, pages 80?88.
O?GAI, September.
Maintrack: oral presentations.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of sim-plicity: Unsupervised extraction of lexical simpli-fications from Wikipedia.
In Proceedings of theNAACL.Qing Zeng, Eunjung Kim, Jon Crowell, and Tony Tse.2005.
A text corpora-based estimation of the famil-iarity of health terminology.
Biological and MedicalData Analysis, pages 184?192.Qing Zeng-Treitler, Sergey Goryachev, Tony Tse, AllaKeselman, and Aziz Boxwala.
2008.
Estimat-ing consumer familiarity with health terminology: acontext-based approach.
Journal of the AmericanMedical Informatics Association, 15(3):349?356.109
