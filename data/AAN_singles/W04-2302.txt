Stochastic Language Generation in a Dialogue System:Toward a Domain Independent GeneratorNathanael Chambers and James AllenInstitute for Human and Machine Cognition40 South Alcaniz StreetPensacola, FL 32502{nchambers,jallen}@ihmc.usAbstractUntil recently, surface generation in dialoguesystems has served the purpose of simply pro-viding a backend to other areas of research.The generation component of such systemsusually consists of templates and canned text,providing inflexible, unnatural output.
Tomake matters worse, the resources are typi-cally specific to the domain in question andnot portable to new tasks.
In contrast, domain-independent generation systems typically re-quire large grammars, full lexicons, complexcollocational information, and much more.Furthermore, these frameworks have primar-ily been applied to text applications and it isnot clear that the same systems could performwell in a dialogue application.
This paperexplores the feasibility of adapting such sys-tems to create a domain-independent genera-tion component useful for dialogue systems.
Itutilizes the domain independent semantic formof The Rochester Interactive Planning System(TRIPS) with a domain independent stochas-tic surface generation module.
We show thata written text language model can be usedto predict dialogue utterances from an over-generated word forest.
We also present resultsfrom a human oriented evaluation in an emer-gency planning domain.1 IntroductionThis paper takes steps toward three surface genera-tion goals in dialogue systems; to create a domain-independent surface generator, to create a surface gen-erator that reduces dependence on large and/or domain-specific resources by using out of domain language mod-els, and to create an effective human-like surface genera-tor.Natural Language systems are relatively young andmost of today?s architectures are designed and tested onspecific domains.
It is becoming increasingly desirableto build components that are domain-independent and re-quire a small amount of time to instantiate.
Unfortu-nately, when components are tailored to a specific do-main, it requires a complete overhaul to use the archi-tecture in a new domain.While dialogue systems have found success in manyareas, the backend of these systems, Natural LanguageGeneration (NLG), has largely been ignored and usedsolely to show the progress of other components.
How-ever, it is now important to generate not just content-richutterances, but also natural utterances that do not inter-fere with the dialogue.
Easy to build template-based NLGcomponents can usually satisfy the content requirement,but their static, inflexible forms rarely facilitate an effec-tive human oriented dialogue system.Natural surface generation requires hand-crafted lexi-cons, grammars, ontologies, and much more to be suc-cessful.
The time required to create a simple surface gen-eration component is small, but the time required to cre-ate even a mildly natural component is very large.
Lan-guage modeling offers hope that the information encodedin these grammars and lexicons is implicitly present inspoken and written text.
There have been many advanceswith stochastic approaches in areas that have taken ad-vantage of the large corpora of available newswire, suchas Machine Translation (MT).
If newswire text (whichmakes up much of the available English corpora) canbe applied to dialogue, we could depend less on hand-crafted grammars and domain-specific resources.This paper describes an approach to surface genera-tion in dialogue systems that uses out of domain languagemodels; a model based on newswire text and a modelbased on spoken dialogue transcripts.
We also describehow this approach fits with a domain independent logicalform being used for interpretation in TRIPS.
Our anal-ysis of this approach shows that newswire corpora cangenerate not only the semantic content in its output, butalso shows that it can be integrated successfully into a di-alogue system, resulting in only a slight decrease in nat-uralness as judged by human evaluators.This paper begins with a description of previous sur-face generation work.
Section 3 describes the stochasticalgorithm used from the Machine Translation (MT) sys-tem, HALogen, including differences in dialogue versusnewswire text.
Section 4 describes the domain indepen-dence of the logical form in TRIPS and how indepen-dence is preserved in translating into the stochastic com-ponent.
Section 5 describes our evaluation including thelanguage models and the domain we used for evaluation.Finally, we present the results and discussion in section6.2 BackgroundTemplate-based approaches have been widely used forsurface generation.
This has traditionally been thecase because the many other areas of NLP research(speech recognition, parsing, knowledge representation,etc.)
within a dialogue system require an output form toindicate the algorithms are functional.
Templates are cre-ated very cheaply, but provide a rigid, inflexible outputand poor text quality.
See Reiter (Reiter, 1995) for a fulldiscussion of templates.
Dialogue systems particularlysuffer as understanding is very dependent on the natural-ness of the output.Rule-based generation has developed as an alternativeto templates.
Publicly available packages for this typeof generation take strides toward independent generation.However, a significant amount of linguistic informationis usually needed in order to generate a modest utterance.This kind of detail is not available to most domain in-dependent dialogue systems.
A smaller, domain-specificrule-based approach is difficult to port to new domains.The corpus-based approach to surface generation doesnot use large linguistic databases but rather depends onlanguage modeling of corpora to predict correct and nat-ural utterances.
The approach is attractive in comparisonto templates and rule-based approaches because the lan-guage models implicitly encode the natural ordering ofEnglish.
Recently, the results from corpus-based surfacegeneration in dialogue systems have been within specificdomains, the vast majority of which have used the AirTravel Domain with Air Travel corpora.Ratnaparkhi (Ratnaparkhi, 2000; Ratnaparkhi, 2002)and Oh and Rudnicky (Oh and Rudnicky, 2000) bothstudied surface generators for the air travel domain.
Theirinput semantic form is a set of attribute-value pairs thatare specific to the airline reservation task.
The languagemodels were standard n-gram approaches that dependedon a tagged air travel corpus for the attribute types.
Bothgroups ran human evaluations; Ratnaparkhi studied a 2subject evaluation (with marks of OK,Good,Bad) and Ohand Rudnicky studied 12 subjects that compared the out-put between a template generator and the corpus-basedapproach.
The latter showed no significant difference.Most recently, Chen et al utilized FERGUS (Banga-lore and Rambow, 2000) and attempted to make it moredomain independent in (Chen et al, 2002).
There aretwo stochastic processes in FERGUS; a tree chooser thatmaps an input syntactic tree to a TAG tree, and a trigramlanguage model that chooses the best sentence in the lat-tice.
They found that a domain-specific corpus performsbetter than a Wall Street Journal (WSJ) corpus for the tri-gram LM.
Work was done to try and use an independentLM, but (Rambow et al, 2001) found interrogatives tobe unrepresented by a WSJ model and fell back on airtravel models.
This problem was not discussed in (Chenet al, 2002).
Perhaps automatically extracted trees fromthe corpora are able to create many good and few badpossibilities that the LM might choose.
(Chen et al, 2002) is the first paper to this author?sknowledge that attempts to create a stochastic domain in-dependent generator for dialogue systems.
One of themain differences between FERGUS and this paper?s ap-proach is that the input to FERGUS is a deep syntactictree.
Our approach integrates semantic input, reducingthe need for large linguistic databases and allowing theLM to choose the correct forms.
We are also unique inthat we are intentionally using two out-of-domain lan-guage models.
Most of the work on FERGUS and theprevious surface generation evaluations in dialogue sys-tems are dependent on English syntax and word choicewithin the air travel domain.
The final generation sys-tem cannot be ported to a new domain without furthereffort.
By creating grammar rules that convert a seman-tic form, some of these restrictions can be removed.
Thenext section describes our stochastic approach and howit was modified from machine translation to spoken dia-logue.3 Stochastic Generation (HALogen)We used the HALogen framework (Langkilde-Geary,2002) for our surface generation.
HALogen was origi-nally created for a domain within MT and is a sentenceplanner and a surface realizer.
Analysis and MT appli-cations can be found in (Langkilde and Knight, 1998;Knight and Langkilde, 2000).HALogen accepts a feature-value structure rangingfrom high-level semantics to shallow syntax.
Figure 1shows a mixture of both as an example.
Given this input,generation is a two step process.
First, the input formis converted into a word forest (a more efficient repre-sentation of a word lattice) as described in (Langkilde-Geary, 2002).
Second, the language model chooses themost probable path through the forest as the output sen-tence.
(V68753 / move:TENSE past:AGENT (V68837 / person:QUANT three:NUMBER plural):THEME (V68846 / ambulance))Figure 1: HALogen input of the sentence Three peoplemoved the ambulance.The word forest is created by a series of grammar rulesthat are designed to over-generate for a given representa-tion.
As figure 1 shows, there is a lot of syntactic informa-tion missing.
The rules are not concerned with generatingonly syntactically correct possibilities, but to generate allpossibilities under every input that is not specified (ourexample does not provide a determiner for ambulance,so the grammar would produce the definite and indefiniteversions).
Once the forest is created, the language modelchooses the best path(s) through the forest.We modified HALogen?s grammar to fit the needs of adialogue system while maintaining the same set of rolesand syntactic arguments recognized by the grammar.
TheTRIPS Logical Form uses many more roles than HALo-gen recognizes, but we converted them to the smallerset.
By using HALogen?s set of roles, we can be assuredthat our grammar is domain independent from TRIPS.We did, however, expand the grammar within its cur-rent roles.
For instance, we found the theme role to beinsufficient and changed the grammar to generate moresyntactic constructs (for example, we generate the themein both the object and subject positions).
We also ex-panded the production rules for interrogatives and imper-atives, both of which were sparsely used/tested becauseof HALogen?s original use in MT domains.HALogen is able to expand WordNet word classes intotheir lexical items, but due to the difficulty of mappingthe TRIPS word classes to WordNet, our input terms toHALogen are the desired lexical items instead of wordclasses as shown in figure 1.
Future work includes link-ing the grammar to the TRIPS word classes instead ofWordNet.4 The Dialogue SystemWe developed our approach within TRIPS, a collab-orative planning assistant that interacts with a humanuser mainly through natural language dialogue, but alsothrough graphical displays.
The system supports manydomains involving planning scenarios, such as a 911 dis-aster rescue assistant and a medical adviser.
TRIPS per-(define-type LF CONSUME:semfeatures(Situation (aspect dynamic) (cause agentive)):arguments(AGENT (Phys-obj (intentional +) (origin living)))(THEME (Phys-obj (form substance))))Figure 2: LF type definitions for LF CONSUME (from(Dzikovska et al, 2003))forms advanced reasoning and NLP tasks including, butnot limited to, interpretation in context, discovering userintentions, planning, and dialogue management.
Lan-guage generation has largely been ignored in the sys-tem until recently.
As with many dialogue systems, ithas simply been a means to show results in the aboveareas through a language back-end.
Recently, Stent(Stent, 1999) did extensive work on dialogue manage-ment through rule-based generation (Allen et al, 2001).4.1 Logical Form of MeaningThere are two meaning representations in TRIPS.
Thefirst is a domain independent representation called thelogical form (LF).
The second is a domain dependentknowledge representation (KR).
The effort toward creat-ing the domain independent LF is part of an overall goalof creating a dialogue system that is easily portable tonew domains.
A domain-specific representation is alwaysneeded for reasoning, and mapping rules are created tomap the LF into the KR for each domain.
These rules areeasier to create than a new logical representation for eachdomain.Dzikovska, Swift and Allen (Dzikovska et al, 2003)have built a parser that parses speech utterances into thisdomain-independent LF.
The LF is very important to thispaper.
One of the biggest problems that any surface gen-eration approach faces is that it takes a lot of work to gen-erate sentences for one domain.
Moving to a new domainusually involves duplicating much of this work.
How-ever, if we create a surface generator that uses the LF asinput, we have created a surface generator that is able togenerate English in more than one specific domain.The LF ontology consists of a single-inheritance hi-erarchy of frame-like LF types that classify entities ac-cording to their semantics and argument structure.
EveryLF type can have a set of thematic arguments with se-lectional restrictions.
The ontology is explicitly designedto capture the semantic differences that can affect sen-tence structure, and it draws from many sources includingFRAMENET, EURO-WORDNET, and VERBNET.
Thereader is referred to (Dzikovska et al, 2003) for more de-tails.
An example of an LF type definition is shown inFigure 2.
(SPEECHACT sa1 SA TELL :content V11)(F V11 (* LF CONSUME take) :AGENT V123:THEME V433):TMA ((:TENSE PAST))(PRO V123 (:* LF PERSON he) :CONTEXT REL HE)(A V433 (:* LF DRUG aspirin))Figure 3: Logical Form for the sentence, he took an as-pirin.The parser uses the LF type definitions to build a gen-eral semantic representation of the input.
This is a flatand unscoped representation of the semantics of the sen-tence that serves as input to the TRIPS discourse inter-pretation modules (which perform reference resolution,disambiguation, intention recognition to produce the fi-nal intended meaning).
Figure 3 gives an example of theLF representation of the sentence, he took an aspirin.
Itcan be read as follows: A speech act of type SA TELLoccurred with content being V11, which is a propositionof type LF CONSUME (more specifically ?take?
), withAGENT V123 and THEME V433.
V123 is pronominalform of type LF PERSON and pro-type HE, and V433 isan indefinitely specified object that is of type LF DRUG(more specifically ?aspirin?
).The LF representation serves as the input to our surfacegeneration grammar after a small conversion.
If naturalhuman quality dialogue can be produced from this LF, notonly has a domain independent generator been created,but also a generator that shares ontologies and lexiconswith the parser.4.2 Integrating HALogen into TRIPSThe task of converting our independent Logical Form(LF) into HALogen?s Abstract Meaning Representationwas relatively straightforward.
Several rules were cre-ated to change LF specific roles into the smaller set ofroles that the surface generation grammar recognizes.
LFroles such as COGNIZER and ENTITY are convertedto AGENT and THEME respectively.
Verb propertiesrepresented by TMA are converted into the appropriatesyntactic roles of TENSE, MODALITY, AUXILLARY,etc.
The LF type triple is reduced to just the lexicalitem and appropriate determiners are attached when theLF provides enough information to warrant it.
It is bestillustrated by example using our example LF in figure3.
Given these decisions, our example?s conversion be-comes:(V11 / TAKE:TENSE PAST:AGENT (V123 / HE):THEME (V433 / ASPIRIN))This resulting AMR is the input to HALogen where it isconverted into a word forest using our modified dialogue-based HALogen grammar.
Finally, the language modelchooses the best output.The above conversion applies to declarative, impera-tive and interrogative speech acts.
These are translatedand generated by the method in section 3.
We also take asimilar approach to Stent?s previous work (Stent, 1999)that generated grounding and turn-taking acts using atemplate-based method.
These usually short utterancesdo not require complex surface generation and are left totemplates for proper production.5 EvaluationThis paper is evaluating two surface generation designdecisions: the effectiveness of stochastic (word forestbased) surface generation with domain independent lan-guage models, and the benefits of using dialogue vs.newswire models.
Evaluating any natural language gen-eration system involves many factors, but we focused ontwo of the most important aspects to evaluate, the con-tent and clarity (naturalness) of the output (English utter-ances).
This section briefly describes previous automaticevaluation approaches that we are avoiding, followed bythe human evaluation we have performed on our system.5.1 Automatic EvaluationEvaluating generation is particularly difficult due to thediverse amount of correct output that can be generated.There are many ways to present a given semantic repre-sentation in English and what determines quality of con-tent and form are often subjective measures.
There aretwo general approaches to a surface generation evalua-tion.
The first uses human evaluators to score the out-put with some pre-defined ranking measure.
The seconduses a quantitative automatic approach usually based onn-gram presence and word ordering.
Bangalore et al de-scribe some of the quantitative measures that have beenused in (Bangalore et al, 2000).
Callaway recently usedquantitative measures in an evaluation between symbolicand stochastic surface generators in (Callaway, 2003).The most common quantitative measure is SimpleString Accuracy.
This metric uses an ideal output stringand compares it to a generated string using a metric thatcombines three word error counts; insertion, deletion, andsubstitution.
One variation on this approach is tree-basedmetrics.
These attempt to better represent how bad a badresult is.
The tree-based accuracy metrics do not com-pare two strings directly, but instead build a dependencytree for the ideal string and attempt to create the samedependency tree from the generated string.
The score isdependent not only on word choice, but on positioning atthe phrasal level.
Finally, the most recent evaluation met-ric is the Bleu Metric from IBM(Papineni et al, 2001).Designed for Machine Translation, it scores generatedsentences based on the n-gram appearance from multipleideal sentences.
This approach provides more than onepossible realization of an LF and compares the generatedsentence to all possibilities.Unfortunately, the above automatic metrics are verylimited in mimicking human scores.
The Bleu metric cangive reasonable scores, but the results are not as goodwhen only one human translation is available.
Theseautomatic metrics all compare the desired output withthe actual output.
We decided to ignore this evaluationbecause it is too dependent on syntactic likeness.
Thefollowing two sentences represent the same semanticmeaning yet appear very different in structure:The injured person is still waiting at the hospital.The person with the injury at the hospital is still waiting.The scoring metrics would judge very harshly, yeta human evaluator should see little difference in semanticcontent.
Clearly, the first is indeed better in naturalness(closeness to human English dialogue), but both contentand naturalness cannot be measured with the currentquantitative (and many human study) approaches.Although it is very time consuming, human evalua-tion continues to be the gold standard for generationevaluation.5.2 Evaluation MethodologyOur evaluation does not compare an ideal utterance witha generated one.
We use a real human-human dialoguetranscript and replace every utterance of one of the par-ticipants with our generated output.
The evaluators arethereby reading a dialogue between a human and a com-puter generated human, yet it is based on the originalhuman-human dialogue.
Through this approach, we canpresent the evaluators with both our generated and theoriginal transcripts (as the control group).
However, theydo not know which is artificial, or even that any of themare not human to human.
The results will give an accurateportrayal of how well the system generates dialogue.
Thetwo aspects of dialogue that the evaluators were asked tomeasure for each utterance were understandability (se-mantically within context) and naturalness.There have been many metrics used in the past.
Met-rics range from scoring each utterance with a subjectivescore (Good,Bad) to using a numeric scale.
Our evalua-tors use a numeric scale from 0 to 5.
The main motivationfor this is so we can establish averages and performanceresults more easily.
The final step is to obtain a suitabledomain of study outside the typical air travel domain.5.3 Domain Description and Dialogue ConstructionA good dialogue evaluation is one in which all aspectsof a natural dialogue are present and the only aspect thathas been changed is how the surface generation presentsthe required information.
By replacing one speaker?sutterances with our generated utterances in a transcriptof a real conversation, we guarantee that grounding andturn-taking are still present and our evaluation is not hin-dered by poor dialogue cues.
The TRIPS Monroe Corpus(Stent, 2000) works well for this task.There are 20 dialogues in the Monroe Corpus.
Eachdialogue is a conversation between two English speak-ers.
Twenty different speakers were used to construct thedialogues.
Each participant was given a map of Mon-roe County, NY and a description of a task that neededto be solved.
There were eight different disaster scenar-ios ranging from a bomb attack to a broken leg and theparticipants were to act as emergency dispatchers (thisdomain is often referred to as the 911 Rescue Domain).One participant U was given control of solving the task,and the other participant S was told that U had control.S was to assist U in solving the task.
At the end of thediscussion, U was to summarize the final plan they hadcreated together.The average dialogue contains approximately 500 ut-terances.
We chose three of the twenty dialogues for ourevaluation.
The three were the shorter dialogues in length(Three of the only four dialogues that are less than 250 ut-terances long.
Many are over 800 utterances.).
This wasneeded for practical reasons so the evaluators could con-duct their rankings in a reasonable amount of time andstill give accurate rankings.
The U and S speakers foreach dialogue were different.We replaced the S speaker in each of the dialogues withgenerated text, created by the following steps:?
Parse each S utterance into its LF with the TRIPSparser.?
Convert the LF to the AMR grammar format.?
Send the AMR to HALogen.?
Generate the top sentence from this conversion us-ing our chosen LM.We hand-checked for correctness each AMR that is cre-ated from the LF.
The volatile nature of a dialogue systemunder development assured us that many of the utteranceswere not properly parsed.
Any errors in the AMR werefixed by hand and hand constructed when no parse couldbe made.
The fixes were done before we tried to generatethe S speaker in the evaluation dialogues.We are assuming perfect input to generation.
This eval-uation does not evaluate how well the conversion fromthe LF to the AMR is performing.
Our goal of generat-ing natural dialogue from a domain-independent LM canbe fully determined by analyzing the stochastic approachin isolation.
Indeed, the goal of a domain independentgenerator is somewhat dependent on the conversion fromour domain independent LF, but we found that the errorsfrom the conversion are not methodological errors.
Theerrors are simple lexicon and code errors that do not re-late to domain-specifics.
Work is currently underway torepair such inconsistencies.Each of the S participant?s non-dialogue-managementutterances were replaced with our generated utterances.The grounding, turn-taking and acknowledgment utter-ances were kept in their original form.
We plan on gener-ating these latter speech acts with templates and are onlytesting the stochastic generation in this evaluation.
The Uspeaker remained in its original state.
The control groupswill identify any bias that U may have over S (i.e.
if Uspeaks ?better?
than S in general), but testing the genera-tion with the same speaker allows us to directly compareour language models.5.4 Language Model ConstructionWe evaluated two language models.
The first is a newssource model trained on 250 million words with a vocab-ulary of 65,529 from the WSJ, AP and other online newssources as built in (Langkilde-Geary, 2002).
This modelwill be referred to as the WSJ LM.
The second languagemodel was built from the Switchboard Corpus (J. God-frey, 1992), a corpus of transcribed conversations and notnewswire text.
The corpus is comprised of ?spontaneous?conversations recorded over the phone, including approx-imately 2 million words with a vocabulary of 20,363.This model will be referred to as the SB LM.
Both mod-els are trigram, open vocabulary models with Witten-Bellsmoothing.
The Switchboard Corpus was used because itcontrasts the newswire corpus in that it is in the genre ofdialogue yet does not include the Monroe Corpus that theevaluation was conducted on.5.5 EvaluatorsTen evaluators were chosen, all were college undergradu-ates between the ages of 18-21.
None were linguistics orcomputer science majors.
Each evaluator received threetranscripts, one from each of our three chosen dialogues.One of these three was the original human to human di-alogue.
The other two had the S speaker replaced by oursurface generator.
Half of the evaluators received gener-ations using the WSJ LM and the other half received theSB LM.
They ranked each utterance for understandabilityand naturalness on scales between 0 and 5.
A comparisonof the human and generated utterances is given in figure8 in the appendix.Percent Difference between U and S speakers0 1 2 3 4understand 0.92 6.03 3.70 0.23 1.74natural -1.31 -0.26 2.56 1.94 -3.095 6 7 8 9understand 3.91 3.27 2.46 -0.10 14.8natural 3.60 2.38 -0.26 5.16 13.3Total Percent Differenceunderstand 3.24%natural 1.85%Figure 4: Difference between the human evaluator scoresfor the two original human speakers, U and S. The tenevaluators are listed by number, 0 to 9.
Evaluators ratedthe content (understandability) and clarity (naturalness)of each utterance on a 0-5 scale.
S was rated slightlyhigher than U.6 ResultsFigure 4 compares the control dialogues as judged by thehuman evaluators by giving the percent difference be-tween the two human speakers.
It is apparent that theU speaker is judged worse than the S speaker in the aver-age of the three dialogues.
We see the S speaker is scored3.24% higher in understanding and 1.85% higher in nat-uralness.
Due to the nature of the domain, the U speakertends to make more requests and short decisions while theS speaker gives much longer descriptions and reasons forhis/her actions.
It is believed the human evaluators tend toscore shorter utterances more harshly because they aren?t?complete sentences?
as most people are used to seeingin written text.
We believe this also explains the discrep-ancy of evaluator 9?s very high scores for the S speaker.Evaluator 9 received dialogue 10 as his control dialogue.Dialogue 10?s S speaker tended to have much longer ut-terances than any of the other five speakers in the threedialogues.
It is possible that this evaluator judged shorterutterances more harshly.Figure 5 shows the comparison between using the twoLMs as well as the human control group.
The scoresshown are the average utterance scores over all evalua-tors and dialogues.
The dialogue management (ground-ing, turn-taking, etc.)
utterance scores are not included inthese averages.
Since we do not generate these types ofutterances, it would be misleading to include them in ourevaluation.
As figure 5 shows, the difference betweenthe two LMs is small.
Both received a lower natural-ness score than understandability.
It is clear that we areable to generate utterances that are understood, but yetare slightly less natural than a human speaker.Figure 6 shows the distribution of speech acts in eachof the 3 evaluation dialogues.
Due to the nature of theMonroe Corpus, there are not many interrogatives or im-Language Model ComparisonU S U/S differenceWSJ LMunderstand 4.67 4.33 -0.34 (?7.28%)natural 4.49 3.97 -0.52 (?11.58%)SB LMunderstand 4.62 4.30 -0.32 (?6.93%)natural 4.18 3.84 -0.34 (?8.13%)HUMANunderstand 4.63 4.78 0.15 (3.24%)natural 4.33 4.41 0.08 (1.85%)Figure 5: Average scores (over the 10 evaluators) of un-derstandability and naturalness with the dialogue man-agement utterances removed.
The first compares the Sspeaker generated with the WSJ LM, the second com-pares the S speaker generated with the SB LM, and thethird is the S speaker using the original human utterances.peratives.
Since the two participants in the dialogueswork together and neither has more information about therescue problem than the other, there are not many ques-tions.
Rather, it is mostly declaratives and acknowledg-ments.Figure 7 shows the average score given for each speechact across all evaluators.
Note that the numbers are onlyfor the S speaker in each dialogue because only S wasgenerated with the surface generator.
Since each eval-uator scored 2 computer dialogues and 1 human (con-trol) dialogue, the LM numbers are averaged across twiceas many examples.
The understandability scores for theWSJ and SB LMs are relatively the same across all acts,but naturalness is slightly less in the SB LM.
Comparingthe human scores to both out-of-domain LMs, we see thatdeclaratives averaged almost a 0.5 point loss from the hu-man control group in both understandability and natural-ness.
Imperatives suffer an even larger decrease with anapproximate 0.7 loss in understandability.
The SB LMactually averaged over 1.0 decrease in naturalness.
Theinterrogatives ranged from a 0.5 to 0 loss.6.1 DiscussionWe can conclude from figure 5 that the evaluators wererelatively consistent among each other in rating under-standability, but not as much so with naturalness.
Thecomparison between the WSJ and SB LMs is inconclu-sive because we see in figure 5 that even though the evalu-ators gave the WSJ utterances higher absolute scores thanthe SB utterances, the percent difference from how theyranked the human U speaker is lower.
The fact that itis inconclusive is somewhat surprising because intuitionleads us to believe that the dialogue-based SB would per-form better than the newswire-based WSJ.
One reasonmay be because the nature of the Monroe Corpus doesnot include many dialogue specific acts such as questionsand imperatives.
However, declaratives are well repre-sented and we can conclude that the newswire WSJ LMis as effective as the dialogue SB model for generatingdialogue declaratives.
Also, it is of note that the WSJ LMout-performed the SB LM in naturalness for most speechact types (as seen in figure 7) as well.The main result from this work is that an out-of-domain language model cannot only be used in a stochas-tic dialogue generation system, but the large amount ofavailable newswire can also be effectively utilized.
Wefound only a 7.28% decrease in understandability and an11.58% decrease in naturalness using our newswire LM.This result is exciting.
These percentages correspond toranking an utterance 4.64 and 4.42 instead of a perfect5.00 and 5.00.
The reader is encouraged to look at theoutput of the generation in the appendix, figure 8.6.2 Future WorkWe have created a new grammar to generate from the LFthat recognizes the full set of thematic roles.
In addition,we have linked our dialogue system?s lexicon to the gen-eration module instead of WordNet, resulting in a fullyintegrated component to be ported to new domains withlittle effort.
It remains to run an evaluation of this design.Also, stochastic generation favors other avenues ofgeneration research, such as user adaptation.
Work is be-ing done to adapt to the specific vocabulary of the humanuser using dynamic language models.
We hope to cre-ate an adaptive, natural generation component from thiseffort.Finally, we are looking into random weighting ap-proaches for the generation grammar rules and resultingword forest in order to create dynamic surface generation.One of the problems of template-based approaches is thatthe generation is too static.
Our corpus-based approachsolves much of the problem, but there is still a degree of?sameness?
that is generated among the utterances.7 ConclusionWe have shown that steps toward a domain-independentNLG component of a dialogue system can be takenthrough a corpus-based approach.
By depending on adomain-independent semantic input in combination witha grammar that over-generates possible English utter-ances and a newswire language model to choose the best,we have shown that it is possible to generate content richand natural utterances.
We report results in a new, richerdomain for stochastic generation research and show ourapproach resulting in only an 11.6% decrease in natural-ness when compared to a human speaker.Dialogue Mgmt.
Declarative Imperative YN-Question WH-QuestionDialogue 1 45 75 10 7 3Dialogue 2 49 84 4 17 8Dialogue 3 57 81 7 1 1Figure 6: The number of types of speech acts in each of the three dialogues.Dialogue Mgmt.
Declarative Imperative YN-Question WH-QuestionWSJ LMund 4.92 4.34 3.83 4.39 4.78nat 4.87 3.96 3.73 3.82 4.11SB LMund 4.63 4.33 4.03 4.31 4.89nat 4.59 3.87 3.21 4.00 3.33HUMANund 4.73 4.79 4.71 4.76 4.83nat 4.74 4.41 4.32 4.51 4.83Figure 7: Comparison of speech act scores of the S speaker.
The numbers are averages over the evaluators?
scores ona 0-5 scale.8 AcknowledgmentsWe give thanks to Lucian Galescu, Irene Langkilde-Geary and Amanda Stent for helpful comments and sug-gestions on previous drafts of this paper.
This work wassupported in part by ONR grant 5-23236.ReferencesJ.
Allen, G. Ferguson, and A. Stent.
2001.
An architec-ture for more realistic conversational systems.
In Pro-ceedings of Intelligent User Interfaces 2001 (IUI-01),Santa Fe, NM, January.S.
Bangalore and O. Rambow.
2000.
Exploiting a proba-bilistic hierarchical model for generation.
In Proceed-ings of the 18th International Conference on Computa-tional Linguistics (COLING 2000), Saarbrucken, Ger-many.S.
Bangalore, O. Rambow, and S. Whittaker.
2000.
Eval-uation metrics for generation.
In Proceedings of the 1stInternational Conference on Natural Language Gener-ation (INLG 2000), Mitzpe Ramon, Israel.C.
Callaway.
2003.
Evaluating coverage for large sym-bolic nlg grammars.
In IJCAI, pages 811?817, Aca-pulco, Mexico.J.
Chen, S. Bangalore, O. Rambow, and M. Walker.2002.
Towards automatic generation of natural lan-guage generation systems.
In Proceedings of the 19thInternational Conference on Computational Linguis-tics (COLING 2002), Taipei, Taiwan.M.
Dzikovska, M. Swift, and J. Allen.
2003.
Construct-ing custom semantic representations from a genericlexicon.
In 5th International Workshop on Computa-tional Semantics.J.
McDaniel J. Godfrey, E. Holliman.
1992.
Switch-board: Telephone speech corpus for research and de-velopment.
In ICASSP, pages 517?520, San Francisco,CA.K.
Knight and I. Langkilde.
2000.
Preserving ambigui-ties in generation via automata intersection.
In Amer-ican Association for Artificial Intelligence conference(AAAI).I.
Langkilde and K. Knight.
1998.
Generation that ex-ploits corpus-based statistical knowledge.
In Proceed-ings of the ACL/COLING-98, Montreal, Quebec.I.
Langkilde-Geary.
2002.
An empirical verification ofcoverage and correctness for a general-purpose sen-tence generator.
In International Natural LanguageGeneration Conference (INLG).A.
Oh and A. Rudnicky.
2000.
Stochastic language gen-eration for spoken dialogue systems.
In ANLP/NAACL2000 Workshop on Conversational Systems, pages 27?32, May.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
Research Report RC22176, IBM, Septem-ber.O.
Rambow, S. Bangalore, , and M. Walker.
2001.
Nat-ural language generation in dialog systems.
In FirstInternational Conference on Human Language Tech-nology Research (HLT2001).A.
Ratnaparkhi.
2000.
Trainable methods for surfacenatural language generation.
In 1st Meeting of theNorth American Chapter of the Association of Com-putational Linguistics (NAACL 2000).A.
Ratnaparkhi.
2002.
Trainable approaches to surfacenatural language generation and their application toconversational dialog systems.
Computer, Speech, &Language.E.
Reiter.
1995.
Nlg vs. templates.
In Fifth EuropeanWorkshop on Natural-Language Generation.A.
Stent.
1999.
Content planning and generation incontinuous-speech spoken dialog systems.
In KI?99workshop.A.
Stent.
2000.
The monroe corpus.
Research Report728, Computer Science Dept., University of Rochester,March.
99-2.9 Appendix7 right so we have bunch of roads that are out*7 we have a bunch of roads that are out8 a bunch of electric lines that are down*8 bunch of electric lines that are down9 and we need to fix them*9 and we need to fix them10 lets see*10 let us see11 and one of the electric lines is across the road and we need to fix that*11 one electric lines are across the road and we need to fix that immediately13 it is across*13 it is across14 its at the intersection of three eighty three and two fifty two a just*14 it was at the intersection of three eighty three and two fifty two as16 so*16 so18 yeah so i want so we need to send an electric crew*18 yeah so we need to send electric crews19 i guess theres only one set of electric crews*19 i guess there is one set of electric crews20 uh send them there to shut off the power*20 send them the power to shut off in there22 and that should take about twenty minutes*22 twenty minutes minutes and that should take23 um not going to worry about travel time perhaps*23 perhaps we will not travel time worry24 and then after that i would send the airport road crew to the same location*24 i would send the airport crew fixed the road to the same location28 i guess*28 i guess29 but they can shut off the power from an intersection*29 they can shut the power of an intersection off31 um before that*31 before that32 okay so thats one location*32 okay so that is one location33 and its going to take them four hours to fix the road*33 and they will take four hours to fix the roads35 and then after that we can send an electric crew to um restore the lines*35 and then we can send electric crews to restore the lines36 which takes two hours*36 that takes two hours38 six plus twenty minutes yeah*38 six minutes plus twenty minutesFigure 8: A comparison of the original human and our generated utterances, part of dialogue three in the MonroeCorpus (just the S speaker).
The starred numbers are inserted into the dialogue to provide a side by side comparison ofthe quality of our generation.
Starred utterances are generated by the approach described in this paper.
The evaluatorsdid not receive such a dialogue.
All human or all generation was presented as the S speaker to each evaluator.
