Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2287?2296,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsThe More Antecedents, the Merrier: Resolving Multi-AntecedentAnaphorsHardik Vala1, Andrew Piper2, Derek Ruths11School of Computer Science,2Dept.
of Languages, Literartures, & CulturesMcGill UniversityMontreal, Canadahardik.vala@mail.mcgill.ca,{andrew.piper, derek.ruths}@mcgill.caAbstractAnaphor resolution is an important taskin NLP with many applications.
De-spite much research effort, it remains anopen problem.
The difficulty of the prob-lem varies substantially across differentsub-problems.
One sub-problem, in par-ticular, has been largely untouched byprior work despite occurring frequentlythroughout corpora: the anaphor thathas multiple antecedents, which here wecall multi-antecedent anaphors or m-anaphors.
Current coreference resolversrestrict anaphors to at most a single an-tecedent.
As we show in this paper, re-laxing this constraint poses serious prob-lems in coreference chain-building, whereeach chain is intended to refer to a singleentity.
This work provides a formaliza-tion of the new task with preliminary in-sights into multi-antecedent noun-phraseanaphors, and offers a method for resolv-ing such cases that outperforms a numberof baseline methods by a significant mar-gin.
Our system uses local agglomerativeclustering on candidate antecedents and anexisting coreference system to score clus-ters to determine which cluster of men-tions is antecedent for a given anaphor.When we augment an existing coreferencesystem with our proposed method, we ob-serve a substantial increase in performance(0.6 absolute CoNLL F1) on an annotatedcorpus.1 IntroductionAnaphor resolution is a very difficult task in Nat-ural Language Understanding, involving the com-plex interaction of discourse cues, syntactic rules,and semantic phenomena.
It is closely related tothe task of coreference resolution (Van Deemterand Kibble, 2000), for which a myriad of solu-tions have been proposed (Clark and Manning,2015; Peng et al, 2015; Wiseman et al, 2015;Bj?orkelund and Farkas, 2012; Lee et al, 2011;Stoyanov et al, 2010; Ng, 2008; Bergsma andLin, 2006; Soon et al, 2001).
However, giventhe complexity of the problem, a comprehensiveapproach remains elusive.
The difficulty variesdrastically across different cases (proper nouns,pronouns, gerunds, etc.
), each of which involvesdifferent assumptions about and models of vari-ous linguistic phenomena (e.g., vocabulary, syn-tax, and semantics).
As a result, state-of-the-art systems yield varying performance across sub-problems (Mitkov, 2014; Kummerfeld and Klein,2013; Bj?orkelund and Nugues, 2011; Recasensand Hovy, 2009; Stoyanov et al, 2009; Bengtsonand Roth, 2008; Van Deemter and Kibble, 2000;Ng and Cardie, 2002b; Kameyama, 1997).To avoid the complexity of the overarching res-olution task, many current systems ?
whetherlearning-based (Clark and Manning, 2015; Peng etal., 2015; Wiseman et al, 2015; Durrett and Klein,2013; Bj?orkelund and Farkas, 2012) or rule-based(Lee et al, 2011) ?
focus on a restricted ver-sion of the problem, where candidate anaphorsare linked to at most one antecedent, from whichcoreference chains are built by propagating the in-duced equivalence relation, with each chain cor-responding to an entity (Van Deemter and Kibble,2000).While this single-antecedent inference task doesresolve a very large number of anaphors in anygiven text, it leaves one quite common sub-problem virtually untouched: anaphors that link tomultiple antecedents.
These have sometimes beencalled split-antecedent anaphors; here we use theterm multi-antecedent anaphors or m-anaphors in2287order to emphasize the existence of more than one(possibly more than two) antecedents for a givenanaphor.
Consider the following examples:(1) [Elizabeth]1met [Mary]2at the park and[they]1,2began their stroll to the river.
(2) Mrs. Dashwood, having moved to anothercountry, saw her [mother]1and [sister-in-law]2demoted to occasional visitors.
Assuch, however, her old [kin]1,2were treatedby her new family with quiet civility.Such cases present a challenge to state-of-the-art methods: certain features well-suited for thesingle-antecedent case do not apply (e.g.
genderand pluarity) (Recasens and Hovy, 2009; Stoyanovet al, 2009; Bergsma and Lin, 2006), and stronglong-distance effects cannot be ignored (Ingria andStallard, 1989).
Moreover, the presence of multi-ple antecedents for a single anaphor violates theseparation between coreference chains.In this paper, we address the multi-antecedentcase of noun-phrase (NP) anaphor resolution inEnglish, the most widely understood and studiedform of coreference resolution (Ng, 2010; Ng,2008).
While we frame the general question ofmulti-antecedent inference, we restrict our analy-ses to one particular sub-problem: resolving theantecedents of the pronouns they and them.
Thesepronouns best isolate the characteristics of m-anaphors (see Section 2 for more on the motivationof this choice).
We propose a system for resolvingthey and them that models grouping compatibilityof mentions through a maximum entropy pairwisemodel, independently from coreference of group-ings, which is handled through an existing coref-erence resolution system leveraging corpus knowl-edge.This paper makes four core contributions.
First,it provides a generalization of the anaphor reso-lution problem to permit linking to multiple an-tecedents.
Second, we characterize core propertiesof m-anaphors and their linguistic environmentsin a large, annotated corpus.
Third, we providea entity-centric system for specifically resolvingmulti-antecedent cases that outperforms a numberof baselines.
And, finally, we show how to pairour system with an existing coreference systemand show a gain of 0.6 points (CoNLL F1) on thecomplete coreference resolution task (resolving allanaphors, single- and multi-antecedent).The rest of the paper is organized as follows:We introduce the terminology and problem state-ment for split-antecedent resolution in Section 2.A summary of the data is given in Section 3 andthe behaviour of split-antecedent anaphors is an-alyzed in Section 4.
Our approach to antecedentprediction is presented in Section 5 and the resultsand analysis are reported in Section 6.
Finally, wereview related work in Section 7 and conclude anddiscuss future work in Section 8.2 ProblemThis section establishes the terminology usedthroughout the paper and reformulates the anaphorresolution problem to incorporate linking to mul-tiple antecedents.2.1 TerminologyWe introduce the termm-anaphor for convenienceas a special case of anaphor that has to multipleantecedents.
For example, they and kin in Exam-ples (1) and (2), respectively, from the Introduc-tion arem-anaphors.
By extension, 1-anaphors areanaphors that have only one antecedent.Similarly, we define an m-antecedent as one ofmultiple antecedents of an m-anaphor and we re-fer to m-antecedents with the same m-anaphor assiblings.
In Example (1) from the Introduction,Elizabeth and Mary are sibling m-antecedents ofthey, and in Example (2), mother and sister-in-laware sibling m-antecedents of kin.Finally, we refer to anaphors with two,three, and four m-antecedents as 2-anaphors, 3-anaphors, and 4-anaphors, respectively.
We pro-vide two more examples:(3) [Mr. Holmes]1stared off into the distance.
[Watson]2simply walked off.
[Both]1,2weretroubled by the news.
(4) Virginia found herself alone with her[brother]1, and then the thought of her[sister]2came to mind.
[She]3rememberedthe camping trip [they]1,2,3embarked on afew summers ago.The anaphor in Example (3) is a 2-anaphor andthe anaphor in Example (4) is a 3-anaphor.2.2 DefinitionWe define the NP anaphor resolution problem sim-ilar to Wiseman et al (2015), Durrett and Klein2288Pronoun # m-anaphorsthey 278them 165we 140you 43everybody 12Table 1: Counts of the most frequent m-anaphoricpronouns in P&P.
(2013), and Hirschman (1997): LetM denote theset of all identified mentions in a document and letM(x) ?M denote all mentions preceding a men-tion x ?
M. The objective of the task is, for eachx ?
M, to find C ?
M(x) such that all mentionsin C are antecedent to x.
If C = ?, then x is non-anaphoric and if |C| ?
1, then x is 1-anaphoric,and if |C| > 1, then x is m-anaphoric.
Hence, thisformulation generalizes the problem to account formulti-antecedent anaphors.To constrain the scope of the study, we performall our analyses on gold mentions, leaving theeffect of imperfect mention detection as a prob-lem for future work (this has been studied for thesingle-antecedent case in Stoyanov et al (2009)).Moreover, we only consider mentions of they andthem that are known to be m-anaphoric for threereasons.
First, non-pronomial m-anaphors, i.e.proper and common nouns, are much more sus-ceptible to long-distance effects and may requireexternal knowledge to resolve.
Second, by focus-ing on this case, we circumvent a host of very in-volved aspects of the complete m-anaphor resolu-tion problem, i.e.
determining whether a mentionis m-anaphoric, 1-anaphoric, or not anaphoric atall.
For example, you may refer to one personor multiple, who can be used as an interrogative(non-anaphoric) or reflexive pronoun (anaphoric)),pronouns such as anyone and everyone introducemany scoping difficulties, and pleonastic pronounsmust be removed from the inference task entirely.Third, they and them are the most prevalent of allpronouns in our dataset (refer to Table 1).3 DataOur dataset comprises of the Pride and Prejudicenovel (P&P) (121440 words) and 36 short sto-ries from the Scribner Anthology of ContemporaryShort Fiction (Martone et al, 1999) (Scribner) (to-tal of 216901 words), representing an eclectic col-lection of stories from the modern era.
For P&P,they them Total# % # % # %P&P 278 32.10 165 19.05 443 51.15Scribner 243 12.96 79 4.21 322 17.17Total 521 19.01 244 8.90 765 27.91Table 2: Number of m-anaphoric they and themmentions and % of all they and them mentions thatare m-anaphors.all mentions of character have been fully resolvedto their antecedents, including mentions referenc-ing multiple characters.
For Scribner, all mentionsof they and them are resolved (m-anaphoric, 1-anaphoric, and singleton), including those of non-person entities.These stories were annotated by three annota-tors according to a slightly modified version ofthe ACE coreference resolution task formulation(Doddington et al, 2004) to allow multiple an-tecedents.
Annotations were conducted throughthe brat1annotation tool (Stenetorp et al, 2012))and the inter-annotator agreement on the sharedtexts (3 stories from Scribner + 7 chapters fromP&P) was 86.5%.Overall, in P&P, 1289m-anaphors were discov-ered, of which 34 (2.6%) were proper nouns, 536(41.6%) were common nouns, and 719 (55.8%)were pronouns.
Table 2 shows the number of goldm-anaphoric they and them mentions and the per-centage of all they and them mentions that are m-anaphoric.Literary works were chosen over other tex-tual modalities, e.g.
news articles, because theyshowed a higher density of m-anaphors (a pre-liminary annotation exercise showed that liter-ary works contained 37% more m-anaphors perword).The dataset is partitioned according to aroughly, 60/20/20 split into training, validation,and testing sets, where the split is applied to thetext of P&P (e.g.
the first 60% of story text is usedfor training), and the collection of Scribner stories(e.g.
60% of the stories were used for training).4 Behaviour of m-anaphorsm-anaphors present a novel class of anaphor forwhich very little knowledge exists.
To better un-derstand the linguistic behaviour of m-anaphors,we perform the following analyses.
First, weexamine first and second order statistics of our1http://brat.nlplab.org2289First SecondAvg.
distance (# words) 17.08 33.50Std.
distance (# words) 23.80 40.66Avg.
distance (# sent.)
1.19 2.28Std.
distance (# sent.)
3.18 5.10Avg.
# intermediates 1.44 4.21Std.
# intermediates 2.33 4.44Table 3: Average and standard deviations of theword distance, sentence distance, and number ofintermediate mentions between the first and sec-ond most recent mentions to an m-anaphor.dataset to gain insight into the distribution of m-anaphors across a number of dimensions.
Sec-ond, we fit a maximum entropy model over com-mon coreference features for distinguishing m-anaphoric and anaphoric mentions to evaluate theimportance of various features in determining m-anaphoricity versus anaphoricity of mentions.4.1 m-anaphor StatisticsThe distribution of m-anaphors according to thenumber of referenced m?antecedents is as fol-lows: 79.3% are 2-anaphors, 13.2% are 3-anaphors, 3.7% are 4-anaphors, and the remain-ing 3.8% refer to larger numbers of antecedents.Despite the bias towards 2-anaphors, the sim-ple approach to m-anaphor resolution of takingthe previous two mentions as m-antecedent sib-lings will fail according to Table 3.
The usualpresence of intermediate mentions between m-anaphors and their m-antecedents makes the res-olution task non-trivial.
Moreover, the large dis-tances between m-anaphors and their antecedentsattenuates any signal for coreference, introducinggreater noise to the problem.4.2 m-anaphoricity FeaturesThe statistics discussed above shed light on thecomplexity of this problem.
Here, we examinewhether certain surface-level features of anaphoricphenomena from prior work exhibit any differ-ences for m-anaphoric mentions over anaphoricones.
We construct a maximum entropy modelfrom the training data over the combination of syn-tactic and semantic features in Table 4, inspired byWiseman et al (2015), Durrett and Klein (2013),and Recasens et al (2013b).
The binary classi-fication decision is between m-anaphoric and 1-anaphoric mentions, coded as ?1?
and ?0?, respec-tively.
Therefore, the estimated coefficients thatFeature Coefficient p-valueSentence position = first 0.16 0.13Sentence position = last -0.18 0.006Dependency = subject 0.27 0.05Dependency = object 0.08 0.24Dependency = preposition -0.22 0.07Coordinated = true 0.29 0.08Presence of negation 0.06 0.31Presence of modality 0.04 0.21Table 4: Features for m-anaphoricity versus 1-anaphoricity with coefficients estimated from amaximum entropy model, and associated p-values.are positive favor m-anaphoricity and those thatare negative favor 1-anaphoricity.Except for the feature testing on the last sen-tence position, none of the results in Table 4 wereable to reach statistical significance, suggesting ata surface level,m-anaphoricity and 1-anaphoricitybehave very similarly and operate in similar lin-guistic environments.
One possibility is that adeeper set of features is required for distinguish-ingm-anaphors from 1-anaphors.
We identify thisas an important topic for future work in this area.5 m-anaphor ResolutionOur approach to m-anaphor resolution draws in-spiration from mention pair models for corefer-ence that make independent binary classificationdecisions (Ng, 2010).
In our method, we em-ploy a maximum entropy model that makes binarydecisions on mention pairs as well, but the deci-sion corresponds to ?group compatibility?
of men-tions, i.e.
to what degree can a given set of men-tions be the sibling m-antecedents to the same m-anaphor.
This model is embedded in an agglomer-ative clustering process, after which a coreferencedecision is made between clusters and the givenm-anaphor.
Thus, our model treats the grouping ofcandidate mentions into sibling sets independentlyfrom antecedent-anaphor linking.5.1 ArchitectureGiven an m-anaphor g in document D, the stepsof our approach are as follows:1.
Mentions preceding g within a k-sentencewindow are extracted as candidate m-antecedents to g.2.
Perform an agglomerative clustering of thecandidate mentions using similarity metric2290SIM1and average-linkage criteria.
Let Crepresent the clustering.3.
Each non-singleton cluster C ?
C is scoredaccording to the probability of coreference ofthe m-anaphor to the cluster.
This is doneby appealing to an external corpus compris-ing of sentences containing either they orthem.
The grouping of sentences in the doc-ument containing all of the mentions in C(and sentences in-between) are compared toeach they or them sentence in the externalcorpus (depending on the identity of g) usingsimilarity metric SIM2.
The sentence yield-ing the maximum similarity is selected.
Theprobability of coreference is then calculatedby replacing the sentence grouping with theextracted sentence and applying an existingcoreference system COREF between g andits counterpart (they or them) in the extractedsentence.4.
The clusterCmaxproducing the highest prob-ability of coreference is predicted as thegroup of m-antecedents for g.Again, inspired by mention-pair models forcoreference resolution (Clark and Manning, 2015;Bj?orkelund and Farkas, 2012; Ng and Cardie,2002a), the SIM1similarity metric is defined as?
(w>x), where w is a weight vector and x is afeature vector defined for a pair of mentions.
Theparameter vector w is learned using the standardcross-entropy loss function in a maximum entropymodel, where the target variable is a decision onwhether the mentions pairs are siblings or not.
Thelearning is conducted over the training set with L2-regularization.For SIM2, which is responsible for selectingreplacement sentences, we experiment with twodifferent similarity metrics: (1) longest commonsubsequence normalized by sentence length (LCS)and (2) a subset tree kernel (Collins and Duffy,2002) with a bag-of-words extension as describedin Moschitti (2006), which also describes a sim-ple adaptation to forests (for multiple sentences).The named entity (NE) mentions in sentences arereplaced by corresponding NE type placehold-ers (PERSON, LOCATION, etc.
as described inFinkel et al (2005)) before comparison.In the experiments to follow, we adopt the clas-sification mention-pair model, a component of thestatistical coreference resolution system availablein the Stanford CoreNLP suite2system, describedin Clark and Manning (2015), as COREF forscoring coreference.
The external corpus was builtfrom texts comparable to our dataset.
651,108 sen-tences containing one of they or them were minedfrom a larger corpus of 798 literary texts span-ning the nineteenth and twentieth centuries (in-cluding novels such as To The Lighthouse, by Vir-ginia Woolf).
Lastly, the candidate m-antecedentsare extracted from a 5-sentence pre-window of thegivenm-anaphor (k = 5) and the regularization pa-rameter in learning is set to 0.20.5.2 Clustering FeaturesTable 5 depicts the features we chose to use in thepairwise similarity metric (SIM1) for agglomer-ative clustering of candidate m-antecedents.
Allare common to many coreference resolver systems(Durrett and Klein, 2013; Recasens et al, 2013b;Stoyanov et al, 2010).
We distinguish betweenmention features (Columns 1 & 2), which are de-fined for each candidate m-antecedent in a pair,and pairwise features (Columns 3-5), which aredefined over a pair of candidate m-antecedents.Three features, in particular, deserve furtherdiscussion.
Under morphosyntax (Column 3),[Type Conjunctions] is a placeholder for a num-ber of conjunctive boolean features derived fromthe noun type (pronoun/proper/common) of eachantecedent in a pairing: e.g., pronoun-pronoun,pronoun-proper, proper-pronoun.
Similarly, [De-pendency Conjunctions] is a placeholder for con-junctive boolean features derived from the gram-matical dependency of each antecedent in a pair-ing: e.g., subject-subject, subject-object, object-subject.
The [# Dependency Pairings] is an ordi-nal version of the Dependency Conjunctions fea-ture set - a count of the number of occurrencesrather than an indicator variable.The ?Governor = except?
feature triggers if oneof the mentions in the mention pair is governed byexcept or exclude.
It represents a form of negationof group membership (e.g.
Everyone except forMary visited Castlebary).Features were extracted using the StanfordCoreNLP system (Manning et al, 2014) andanimacy information was specifically obtainedthrough the Stanford deterministic coreferenceresolution module (Lee et al, 2011).2http://stanfordnlp.github.io/CoreNLP/coref.html2291Morphosyntax (Mention) Grammatical (Mention) Morphosyntax (Pairwise) Grammatical (Pairwise) Semantic (Pairwise)Type = pronoun Sentence position = first Head match Word distance (max.
30) Governor = exceptType = proper noun Sentence position = last [Type Conjunctions] Sentence distance # Conjunctive pairingsAnimacy = animate Dependency = subject Coordination = and [# Dependency Pairings]Animacy = unknown Dependency = object [Dependency Conjunctions]Person = first Dependency = prepositionPerson = thirdSingular = trueQuantified = true# ModifiersTable 5: Features used in the clustering similarity metric, separated by category.
The features [TypeConjunctions], [Dependency Conjunctions], and [# Dependency Pairings] are all placeholders for fea-ture sets.
See the text for details.6 ExperimentsIn order to assess the performance of our method,we conduct two experiments.
In the first, we as-sess performance of our system on the specificthey-them m-anaphor resolution sub-task.
Oursystem, and its variants, are compared againsta number of baseline methods based on perfor-mance on the test set.In the second experiment, we consider how oursystem improves the performance of a corefer-ence resolution system when all anaphors (both1-anaphors and m-anaphors) are considered.6.1 EvaluationAccuracy is measured in terms of the num-ber of mention pairs correctly grouped as m-antecedents for a given m-anaphor ?
similarto previous works in anaphor resolution (Penget al, 2015).
We use the standard classifica-tion metrics for precision, recall, and F1-score.If n1, n2, .
.
.
, nNrepresent the number of goldm-antecedents for m-anaphors g1, g2, .
.
.
, gNina document, and m1,m2, .
.
.
,mNare predicted,of which k1, k2, .
.
.
, kNare correct, then pre-cision is defined as?iki/?imiand recall as?iki/?ini, where i ranges from 1 to N .In order to align ourselves with the gold labels,we adjust the predicted mention corresponding toan entity to the closest one preceding the givenm-anaphor.
Because a given entity may appearmultiple times in a candidate mention window,the most recent one, relative to the m-anaphor,is not always the one carrying the strongest sig-nal and hence is not always predicted as an an-tecedent.
For the purposes of evaluation, suchcases are considered correct.
Automatic handlingwould involve a separate, single-antecedent coref-erence resolver, but given the thesis of this work isthe multi-antecedent case, this choice is justified.6.2 System ComparisonWe first describe the various baselines and vari-ants of our method we assess and then analyze theperformance results.Systems?
The ?most-recent-k?
baselines (denotedRECENT-k), which predict the most recent kmentions, relative to the m-anaphor, as them-antecedents for k = 2, 3, 4.?
The random selection baseline (denotedRANDOM), which randomly predicts men-tions in a 5-sentence pre-window as the an-tecedents according to a binomial with proba-bility 0.5 (imposing the constraint that at leasttwo must be predicted).?
A simple rule-based method (denoted RULE)which proceeds as follows:?
If the m-anaphor occupies a subject orprepositional position, then predict themost recent mentions in subject posi-tions if they are coordinated, otherwisetake them from previous, distinct sen-tences.
If no such mentions can befound take the most recent mentions insubject and object positions governed bythe same verb.?
If the m-anaphor occupies in object po-sition, take the previous mentions in ob-ject or prepositional positions if they arecoordinated, otherwise take them fromprevious, distinct sentences.
If no suchmentions can be found, take the most re-cent mentions in subject and object po-sitions governed by the same verb.?
Otherwise, take the two most recentmentions (usually arrive here if there isan error in the dependency parsing).2292Precision Recall F1RECENT-2 21.46 17.68 19.39RECENT-3 23.73 30.10 26.54RECENT-4 21.43 38.82 27.62RANDOM 30.02 29.11 29.56RULE 39.23 17.45 24.16LEE 46.78 9.91 16.36M-LCS 41.35 37.81 39.50M-TREE 41.94 44.88 43.36Table 6: Test set performance of each system onthe m-anaphor resolution task.m-anaphor class Precison Recall F12-anaphor 48.14 52.90 50.413-anaphor 35.92 34.77 35.344-anaphor 36.74 12.87 19.06Table 7: Performance results of the M-TREE sys-tem on the different classes of m-anaphors.?
The system described in Lee et al (2011)(denoted LEE), which performs some lightm-anaphor resolution (solely for conjunctivecases).?
The two variants of the developed method,one using the LCS similarity metric (denotedM-LCS) and the other using the subset treekernel (M-TREE).Results and DiscussionAccuracy results on the test set for each of the sys-tems are given in Table 6.
Both the proposed sys-tems, M-LCS and M-TREE, outperform all othermethods by a substantial margin.
The Stanfordsystem achieves the highest precision, which isnot surprising because it targets conjunctive men-tions, which often serve as m-antecedents.
Basedon the analysis of Section 4, the poor performanceof RECENT-2, RECENT-3, and RECENT-4 is ex-pected.The results for the best-performing system, M-TREE, on the different classes of m-anaphors isgiven in Table 7.
M-TREE outperforms all othersystems but exhibits a bias towards 2-anaphors, re-cent mentions, and mentions coordinated by con-junction.
This is not surprising given such casesare the easiest to resolve.6.3 Full Coreference ResolutionFor the complete coreference resolution task, theM-TREE system can be integrated with an exist-MUC B3CEAFeAvg.CLARK 42.3 39.5 32.4 38.1CLARK+M-TREE 43.4 40.0 31.9 38.7Table 8: CoNLL metric scores for coreference res-olution on the test portion of P&P for the Clarkand Manning (2015) system, with (CLARK+M-TREE) and without (CLARK) the pairing with M-TREE.ing coreference system.
For this experiment, wepair the full coreference resolution system of Clarkand Manning (2015) with M-TREE, and we raisethe prediction threshold of our model to 0.89, atwhich point precision on the validation set is 78.9.Moreover, we restrict ourselves to the P&P portionof the test set, given the Scribner stories only havegold labels for instances of they and them.The Clark and Manning (2015) system is firstrun over the test set, producing coreference chainswhich are then filtered for character entities usingthe approach of Vala et al (2015).
Our adjustedM-TREE system is then applied over all they andthem mentions.
Each such mention predicted asm-anaphoric is added to the coreference chainsof the entities corresponding to the m-antecedentmentions.To evaluate the accuracy against the gold men-tion clusters, each m-anaphoric they and themis added to each cluster containing a gold m-antecedent.
The CoNLL metric scores (Bagga andBaldwin, 1998) of the coreference predictions areshown in Table 8, with the integrated system out-performing the Clark and Manning (2015) sys-tem by 0.6 average score (pairing the Clark andManning (2015) system instead with an oracle m-anaphor resolver yields an average score of 44.8,an increase of 6.7 points).7 Related WorkThe formal problem statement for the noun phraseanaphor resolution we propose is an extension ofthe standard ACE (Doddington et al, 2004), MUC(Hirschman, 1997), and Ontonotes (Hovy et al,2006) formulations, as well as the problem set-tings outlined in Wiseman et al (2015) and Durrettand Klein (2013), to allow anaphors to link to mul-tiple antecedents.
Most previous works impose theconstraint that anaphors can be assigned at mostone antecedent.
Some works cast the coreferenceresolution problem in an Integer Linear Program-ming framework, with an explicit constraint for2293assigning at most one antecedent to an anaphor(Peng et al, 2015; Denis et al, 2007).The early work of Ingria and Stallard (1989)proposes the resolution of pronouns without therestriction they be linked to at most one an-tecedent.
The method uses an indexing scheme forparse trees, similar to Hobb?s algorithm (Hobbs,1978), that eliminates candidates antecedents asmore information is acquired.
Those pronounswith multiple candidates remaining after tree-traversal are predicted as m-anaphors.
Themethod considers each parse tree in isolation, andhence does not permit inter-sentential linking, asevere limitation in corpora such as the one offeredin this work.Other researchers have evaluated noun phrasecoreference resolvers along a number of dimen-sions, including different classes of anaphors(Mitkov, 2014; Kummerfeld and Klein, 2013;Bj?orkelund and Nugues, 2011; Recasens andHovy, 2009; Stoyanov et al, 2009; Bengtson andRoth, 2008; Van Deemter and Kibble, 2000; Ngand Cardie, 2002b; Kameyama, 1997).
This workexplores a new class of anaphor, previously un-studied, and evaluates its impact on the corefer-ence resolution problem.Many state-of-the-art systems for coreferenceresolution, especially supervised, are constrainedto the single-antecedent case (Clark and Manning,2015; Peng et al, 2015; Wiseman et al, 2015;Bj?orkelund and Farkas, 2012; Ng, 2010; Stoyanovet al, 2010; Ng, 2008; Soon et al, 2001).
Themost well-known, benchmark datasets for corefer-ence resolution (e.g.
Ontonotes and ACE-2005),do not offer gold annotations for multi-antecendetanaphors.
Our work presents the first dataset fortackling this problem.The Lee et al (2011) is a deterministic sys-tem that attempts to resolve the ?easy?
multi-antecedent cases, namely those in which mentionsare joined by some conjunction.
Our system goesbeyond and attempts to predict more difficult casesas well.Many of the individual features we employ inour model appear in a variety of other corefer-ence systems, especially those involving mention-pair models (Durrett and Klein, 2013; Recasenset al, 2013b; Stoyanov et al, 2010).
Recasens etal.
(2013a) attempts to perform coreference reso-lution under conditions where many standard fea-tures for coreference are not suited.
Peng et al(2015) resort to corpus counts of predicates as fea-tures, much in the same way we obtain counts ofmention pairings according to simple predicateson dependency structures.The system of Clark and Manning (2015) alsomakes uses of agglomerative clustering, althoughit?s employed in merging coreference chains,rather than candidate antecedent groupings.Last, resorting to an external corpus for sen-tence structures is common practice in the Nat-ural Language Generation literature for pro-ducing phrases that are coherent and consis-tent(Krishnamoorthy et al, 2013; Bangalore andRambow, 2000; Langkilde and Knight, 1998).8 ConclusionWe introduced a new class of anaphors to theanaphor resolution problem, m-anaphors, andextended the problem formulation to incorpo-rate them.
We offered insights into the lin-guistic behaviour of m-anaphors, finding thatsurface-level syntactic and semantic features donot carry enough discriminative power in distin-guishing them from 1-anaphors.
Furthermore,we developed a system combining a mention-pairmodel, an existing coreference resolver, and cor-pus knowledge to resolve m-anaphors that scoreshigher than a number of baseline methods.
Finally,we paired this system with a coreference resolverto solve the general coreference resolution task,showing that m-anaphor prediction can help boostperformance.An important component of the m-anaphor res-olution problem that falls outside the scope of thisstudy, but is important for practical application, isthe detection of m-anaphoric mentions.
Section4 gives some insight into the problem but a muchdeeper investigation is necessary to devise a detec-tion method.Moreover, for simplicity, this study focusedsolely on m-anaphoric they and them mentions,but as explained earlier, m-anaphoric mentionscan take many forms, each introducing their ownparticular complexities that warrant special atten-tion.Regarding the system developed for m-anaphorresolution, resorting to an external corpus to ob-tain well-formed sentences proved to be very com-putationally expensive.
In future work, we lookto incorporate methods that incur less cost, pos-sibly tolerating some error in the formation of2294sentences without significantly degrading perfor-mance.
Also, negation of group membership is acomplex linguistic phenomenon that was handledin a crude manner in our system.
We look to de-vote future work to handling such cases.To promote further research into m-anaphors,we make all our data and software freelyavailable at http://www.github.com/networkdynamics/manaphor-acl2016.ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In The first in-ternational conference on language resources andevaluation workshop on linguistics coreference, vol-ume 1, pages 563?566.
Citeseer.Srinivas Bangalore and Owen Rambow.
2000.
Ex-ploiting a probabilistic hierarchical model for gen-eration.
In Proceedings of the 18th conference onComputational linguistics-Volume 1, pages 42?48.Association for Computational Linguistics.Eric Bengtson and Dan Roth.
2008.
Understanding thevalue of features for coreference resolution.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 294?303.
As-sociation for Computational Linguistics.Shane Bergsma and Dekang Lin.
2006.
Bootstrappingpath-based pronoun resolution.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, pages 33?40.Association for Computational Linguistics.Anders Bj?orkelund and Rich?ard Farkas.
2012.
Data-driven multilingual coreference resolution using re-solver stacking.
In Joint Conference on EMNLP andCoNLL-Shared Task, pages 49?55.
Association forComputational Linguistics.Anders Bj?orkelund and Pierre Nugues.
2011.
Ex-ploring lexicalized features for coreference resolu-tion.
In Proceedings of the Fifteenth Conference onComputational Natural Language Learning: SharedTask, pages 45?50.
Association for ComputationalLinguistics.Kevin Clark and Christopher D Manning.
2015.Entity-centric coreference resolution with modelstacking.
In Association of Computational Linguis-tics (ACL).Michael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: Kernels overdiscrete structures, and the voted perceptron.
In Pro-ceedings of the 40th annual meeting on associationfor computational linguistics, pages 263?270.
Asso-ciation for Computational Linguistics.Pascal Denis, Jason Baldridge, et al 2007.
Joint deter-mination of anaphoricity and coreference resolutionusing integer programming.
In HLT-NAACL, pages236?243.
Citeseer.George R Doddington, Alexis Mitchell, Mark A Przy-bocki, Lance A Ramshaw, Stephanie Strassel, andRalph M Weischedel.
2004.
The automatic contentextraction (ace) program-tasks, data, and evaluation.In LREC, volume 2, page 1.Greg Durrett and Dan Klein.
2013.
Easy victories anduphill battles in coreference resolution.
In EMNLP,pages 1971?1982.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 363?370.
Association for Computational Lin-guistics.Lynette Hirschman.
1997.
{MUC-7 Coreference TaskDefinition}.Jerry R Hobbs.
1978.
Resolving pronoun references.Lingua, 44(4):311?338.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:the 90% solution.
In Proceedings of the human lan-guage technology conference of the NAACL, Com-panion Volume: Short Papers, pages 57?60.
Associ-ation for Computational Linguistics.Robert JP Ingria and David Stallard.
1989.
A com-putational mechanism for pronominal reference.
InProceedings of the 27th annual meeting on Associa-tion for Computational Linguistics, pages 262?271.Association for Computational Linguistics.Megumi Kameyama.
1997.
Recognizing referentiallinks: An information extraction perspective.
InProceedings of a Workshop on Operational Factorsin Practical, Robust Anaphora Resolution for Unre-stricted Texts, pages 46?53.
Association for Compu-tational Linguistics.Niveda Krishnamoorthy, Girish Malkarnenkar, Ray-mond J Mooney, Kate Saenko, and Sergio Guadar-rama.
2013.
Generating natural-language video de-scriptions using text-mined knowledge.
In AAAI,volume 1, page 2.Jonathan K Kummerfeld and Dan Klein.
2013.
Error-driven analysis of challenges in coreference resolu-tion.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages265?277.Irene Langkilde and Kevin Knight.
1998.
Gener-ation that exploits corpus-based statistical knowl-edge.
In Proceedings of the 36th Annual Meet-ing of the Association for Computational Linguis-tics and 17th International Conference on Compu-2295tational Linguistics-Volume 1, pages 704?710.
As-sociation for Computational Linguistics.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s multi-pass sieve coref-erence resolution system at the conll-2011 sharedtask.
In Proceedings of the Fifteenth Conference onComputational Natural Language Learning: SharedTask, pages 28?34.
Association for ComputationalLinguistics.Christopher D Manning, Mihai Surdeanu, John Bauer,Jenny Rose Finkel, Steven Bethard, and David Mc-Closky.
2014.
The stanford corenlp natural lan-guage processing toolkit.
In ACL (System Demon-strations), pages 55?60.Michael Martone, Lex Williford, and Rosellen Brown.1999.
The Scribner Anthology of ContemporaryShort Fiction: Fifty North American Stories Since1970.
Touchstone.Ruslan Mitkov.
2014.
Anaphora resolution.
Rout-ledge.Alessandro Moschitti.
2006.
Making tree kernels prac-tical for natural language learning.
In EACL, volume113, page 24.Vincent Ng and Claire Cardie.
2002a.
Identifyinganaphoric and non-anaphoric noun phrases to im-prove coreference resolution.
In Proceedings ofthe 19th international conference on Computationallinguistics-Volume 1, pages 1?7.
Association forComputational Linguistics.Vincent Ng and Claire Cardie.
2002b.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of the 40th Annual Meeting on Asso-ciation for Computational Linguistics, pages 104?111.
Association for Computational Linguistics.Vincent Ng.
2008.
Unsupervised models for corefer-ence resolution.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 640?649.
Association for Computa-tional Linguistics.Vincent Ng.
2010.
Supervised noun phrase corefer-ence research: The first fifteen years.
In Proceed-ings of the 48th annual meeting of the associationfor computational linguistics, pages 1396?1411.
As-sociation for Computational Linguistics.Haoruo Peng, Daniel Khashabi, and Dan Roth.
2015.Solving hard coreference problems.
Urbana,51:61801.Marta Recasens and Eduard Hovy.
2009.
A deeperlook into features for coreference resolution.
InAnaphora Processing and Applications, pages 29?42.
Springer.Marta Recasens, Matthew Can, and Daniel Jurafsky.2013a.
Same referent, different words: Unsuper-vised mining of opaque coreferent mentions.
InHLT-NAACL, pages 897?906.Marta Recasens, Marie-Catherine de Marneffe, andChristopher Potts.
2013b.
The life and death of dis-course entities: Identifying singleton mentions.
InHLT-NAACL, pages 627?633.Wee Meng Soon, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational linguistics, 27(4):521?544.Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-jii.
2012. brat: a web-based tool for NLP-assistedtext annotation.
In Proceedings of the Demonstra-tions Session at EACL 2012, Avignon, France, April.Association for Computational Linguistics.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrasecoreference resolution: Making sense of the state-of-the-art.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP: Volume 2-Volume2, pages 656?664.
Association for ComputationalLinguistics.Veselin Stoyanov, Claire Cardie, Nathan Gilbert, EllenRiloff, David Buttler, and David Hysom.
2010.Reconcile: A coreference resolution research plat-form.Hardik Vala, David Jurgens, Andrew Piper, and DerekRuths.
2015.
Mr. bennet, his coachman, and thearchbishop walk into a bar but only one of them getsrecognized: On the difficulty of detecting charactersin literary texts.
In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 769?774.Kees Van Deemter and Rodger Kibble.
2000.
On core-ferring: Coreference in muc and related annotationschemes.
Computational linguistics, 26(4):629?637.Sam Wiseman, Alexander M Rush, Stuart M Shieber,Jason Weston, Heather Pon-Barry, Stuart M Shieber,Nicholas Longenbaugh, Sam Wiseman, Stuart MShieber, Elif Yamangil, et al 2015.
Learninganaphoricity and antecedent ranking features forcoreference resolution.
In Proceedings of the 53rdAnnual Meeting of the Association for Computa-tional Linguistics, volume 1, pages 92?100.
Asso-ciation for Computational Linguistics.2296
