Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 380?385,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsLORIA System for the WMT13 Quality Estimation Shared TaskLanglois DavidLORIA(Universite?
de Lorraine, INRIA, CNRS)615 rue du Jardin Botanique,54602 Villers les Nancy, Francedavid.langlois@loria.frSma?
?li KamelLORIA(Universite?
de Lorraine, INRIA, CNRS)615 rue du Jardin Botanique,54602 Villers les Nancy, Francekamel.smaili@loria.frAbstractIn this paper we present the system wesubmitted to the WMT13 shared task onQuality Estimation.
We participated inthe Task 1.1.
Each translated sentenceis given a score between 0 and 1.
Thescore is obtained by using several numeri-cal or boolean features calculated accord-ing to the source and target sentences.
Weperform a linear regression of the featurespace against scores in the range [0..1].
Tothis end, we use a Support Vector Machinewith 66 features.
In this paper, we proposeto increase the size of the training corpus.For that, we use the post-edited and refer-ence corpora during the training step.
Weassign a score to each sentence of thesecorpora.
Then, we tune these scores on adevelopment corpus.
This leads to an im-provement of 10.5% on the developmentcorpus, in terms of Mean Average Error,but achieves only a slight improvement onthe test corpus.1 IntroductionIn the scope of Machine Translation (MT), Qual-ity Estimation (QE) is the task consisting to evalu-ate the translation quality of a sentence or a docu-ment.
This process may be useful for post-editorsto decide or not to revise a sentence produced bya MT system (Specia, 2011; Specia et al 2010).Moreover, it can be useful to decide if a translateddocument can be broadcasted or not (Soricut andEchihabi, 2010).
The most obvious way to give ascore to a translated sentence consists in using amachine learning approach.
This approach is su-pervised: experts are asked to score translated sen-tences and with the obtained material, one learns aprediction model of scores.
The main drawback ofthe machine learning approach is that it is super-vised and requires huge data.
To score a sentenceis time-consuming.
Moreau et alin (Moreau andVogel, 2012) dealt with this issue by proposing un-supervised similarity measures.
In fact, the scoreof a translated sentence is defined by a measuregiving the distance between it and the contents ofan external corpus.
The authors improve the re-sults of the supervised approach but this methodcan be used only in the ranking task.
Raybaud etal.
(Raybaud et al 2011) proposed a method toadd errors in reference sentences (deletion, sub-stitution, insertion).
By this way, they build addi-tional corpus in which each word can be associatedwith a label correct/not correct.
But, it is not pos-sible to predict the translation quality of sentencesincluding these erroneous words.In this paper, we propose to increase the sizeof the training corpus.
For that, we use the scoregiven by experts to evaluate additional sentencesfrom the post-edited and reference corpora.
Practi-cally, we extract from source and target sentencesnumerical vectors (features) and we learn a pre-diction model of the scores.
Then, we apply thismodel to predict the scores of the post-edited andthe reference sentences.
And finally, we tune thepredicted scores on a development corpus.The article is structured as follows.
In Section2, we give an overview of our machine learningapproach and of the features we use.
Then, in Sec-tions 3 and 4 we describe the corpora and how weincrease the size of the training corpus by a partly-unsupervised approach.
In section 5, we give re-sults about this method and we end by a conclu-sion and perspectives.2 Overview of our quality estimationsubmissionWe submit a system for the task 1.1: one has toevaluate each translated sentence with a score be-tween 0 and 1.
This score is read as the HTER be-tween the translated sentence and its post-editedversion.
Each translated sentence is assigned a380score between 0 and 1.
The score is calculatedusing several numerical or boolean features ex-tracted according to the source and target sen-tences.
We perform a regression of the featurespace against [0..1].
To this end, we use the Sup-port Vector Machine algorithm (LibSVM toolkit(Chang and Lin, 2011)).
We experimented onlythe linear kernel because our experience from lastyear (Langlois et al 2012) showed that its perfor-mance are yet good while no parameters have tobe tuned on a development corpus.2.1 The baseline featuresThe QE shared task organizers provided a base-line system including the same features as lastyear: source and target sentences lengths; aver-age source word length; source and target likeli-hood computed with 3-gram (source) and 5-gram(target) language models; average number of oc-currences of the words within the target sentence;average number of translations per source word inthe sentence, using IBM1 translation table (onlytranslations higher than 0.2); weighted averagenumber of translations per source word in the sen-tence (similar to the previous one, but a frequentword is given a low weight in the averaging); dis-tribution by frequencies of the source n-gram intothe quartiles; match between punctuation in sourceand target.
Overall, the baseline system proposes17 features.
We remark that only 5 features takeinto account the target sentence.2.2 The LORIA featuresIn previous works (Raybaud et al 2011; Langloiset al 2012), we tested several confidence mea-sures.
As last year (Langlois et al 2012), weuse the same features.
We extract information bythe way of language model (perplexity, level ofback-off, intra-lingual triggers) and translation ta-ble (IBM1 table, inter-lingual triggers).
The fea-tures are defined at word level, and the featuresat sentence level are computed by averaging overeach word in the sentence.
In our system, we use,in addition to baseline features, ratio of source andtarget lengths; source and target likelihood com-puted with 5-gram language models (Duchateauet al 2002) (in addition to 3-gram features frombaseline); level of backoff n-gram based features(Uhrik and Ward, 1997).
This feature indicatesif the 3-gram, the 2-gram or the unigram corre-sponding to the word is in the language model.
Forlikelihoods and levels of backoff, we use modelstrained on corpus read from left to right (classicalway), and from right to left (sentences are reversedbefore training language models).
This leads totwo language models, and therefore to two val-ues for each feature and side (source and target).Moreover, a common property of all n-gram andbackoff based features is that a word can get a lowscore if it is actually correct but its neighbours arewrong.
To compensate for this phenomenon wetook into account the average score of the neigh-bours of the word being considered.
More pre-cisely, for every relevant feature x. defined at wordlevel we also computed:xleft.
(wi) = x.
(wi?2) ?
x.
(wi?1) ?
x.(wi)xcentred.
(wi) = x.
(wi?1) ?
x.
(wi) ?
x.(wi+1)xright.
(wi) = x.
(wi) ?
x.
(wi+1) ?
x.
(wi+2)The other features are intra-lingual features:each word is assigned its average mutual informa-tion with the other words in the sentence; inter-lingual features: each word in target sentence isassigned its average mutual information with thewords in source sentence; IBM1 features: con-trary to IBM1 based baseline features which takeinto account the number of translations, we usethe probability values in the translation table be-tween source and target words; basic parser (cor-rection of bracketing, presence of end-of-sentencesymbol); number and ratio of out-of-vocabularywords in source and target sentences.
This leadsto 49 features.
A few ones are equivalent to or arestrongly correlated to baseline ones.
We remarkthat 27 features take into account the target sen-tence.The union of the both sets baseline+loria im-proved slightly the baseline system on the test setprovided by the QE Shared Task 2012 (Callison-Burch et al 2012).3 CorporaThe organizers provide a set of files for trainingand development.
We list below the ones we used:?
source.eng: 2,254 source sentences takenfrom three WMT data sets (English): news-test2009, news-test2010, and news-test2012.In the following, this file is named src?
target system.spa: translations for the sourcesentences (Spanish) generated by a PB-SMTsystem built using Moses.
In the following,this file is named syst381?
target system.HTER official-score: HTERscores between MT and post-edited version,to be used as the official score in the sharedtask.
In the following, this file is namedhteroff?
target reference.spa: reference translation(Spanish) for source sentences as originallygiven by WMT; In the following, this file isnamed ref?
target postedited.spa: human post-edited ver-sion (Spanish) of the machine translations intarget system.spa.
In the following, this fileis named postWe split these files into two parts: a training partmade up of the 1,832 first sentences, and a devel-opment part made up of the 442 remaining sen-tences.
This choice is motivated by the fact that inthe previous evaluation campaign we had exactlythe same experimental conditions.For each given file f, we use therefore a partnamed f.train for training and a part namedf.dev for development.4 Training AlgorithmThis section describes the approach we propose toincrease the size of the training corpus.We have to train the prediction model of scoresfrom the source and target sentences.The common way to train such a predictionmodel consists in extracting a features vectorfor each couple (source,target) fromthe (src.train,syst.train) corpus.For each vector, the score associated by ex-perts to the corresponding sentence is assigned.Then, we use a machine learning approach tolearn the regression between the vectors andthe scores.
And finally, we use the triplet(src.dev,syst.dev,hteroff.dev) totune parameters.With machine learning approach, the numberof examples is crucial for a relevant training, butunfortunately the evaluation campaign provides atraining corpus of only 1,832 examples.To increase the training corpus, we proposeto use the ref and post files.
But for that,we have to associate a score to these new targetsentences.
One way could be to calculate theHTER score between each sentence and itscorresponding sentence in the post edited file.But this leads to a drawback: all the couples(src,post) would have a score equal to 0, andthen there is a risk of overtraining on the 0 value.To prevent this problem, we preferred to learna prediction model from the (src.train,-syst.train,hteroff.train) triplet.Then we apply this prediction model tothe (src.train,post.train) and tothe (src.train,ref.train).
By thisway, we get a training corpus made up of1, 832 ?
3 = 3, 696 examples with their scores.Consequently, it is possible to learn a predictionmodel from this new training corpus.
Thesescores are not optimal because the features cannotdescribe all the information from sentences, and amachine learning approach is limited if data arenot sufficiently huge.
Therefore, we propose ananytime randomized algorithm to tune the refer-ence and post-edited scores on the developmentcorpus.
We give below the algorithm we propose.1.
Prediction model(a) Learn the prediction modelusing only features from(src.train,syst.train)and HTER target scores from experts2.
Predict initial scores for postedited andreference sentences(a) Use this model to predict the scoresassociated to the features from(src.train,post.train)and (src.train,ref.train).The predicted scores for(src.train,post.train)are called post best and the onesfor (src.train,ref.train) arecalled ref best3.
Learn initial prediction model using the 3trains (system part, post-edited part andreference part)(a) Learn the prediction model using fea-tures from the three sets of featuresand the scores associated to thesesets (experts scores, post best andref best)(b) Evaluate this model.
This leads to a per-formance equal to best4.
Tune scores for postedited and referencesentences(a) Repeat the following steps until stop382(b) Build a new set of scores namedpost new (resp.
ref new) by dis-turbing each score of post best(resp.
ref best) with a probabilityequal to pdisturb.
A modified scoreis shifted by a value randomly chosen in[-disturb,+disturb](c) Learn the prediction model using fea-tures from the three sets of featuresand the new scores associated to thesesets (experts scores for system set,post new and ref new for the post-edited and reference sets)(d) Evaluate this model.
This leads to a per-formance equal to perf(e) If perf<best then replace best byperf, post best by post new andref best by ref new.To evaluate a model, we use it to predict thescores on the development corpus.
Then we com-pare the predicted scores to the expert scores andwe compute the Mean Average Error (MAE) givenby the formula MAE(s, r) =?ni=1 |si?ri|n ?
100where s and r are two sets of n scores.5 ResultsWe used the data provided by the shared taskon QE, without additional corpus.
This data iscomposed of a parallel English-Spanish trainingcorpus.
This corpus is made of the concatena-tion of europarl-v5 and news-commentary10 cor-pora (from WMT-2010), followed by tokeniza-tion, cleaning (sentences with more than 80 to-kens removed) and truecasing.
It has been usedfor baseline models provided in the baseline pack-age by the shared task organizers.
We used thesame training corpus to train additional languagemodels (5-gram with kneyser-ney discounting, ob-tained with the SRILM toolkit) and triggers re-quired for our features.
For feature extraction, weused the files provided by the organizers: 2,254source english sentences, their translations by thebaseline system, and the score of these transla-tions.
This score is the HTER between the pro-posed translation and the post-edited sentence.
Weused the train part to perform the regression be-tween the features and the scores.
Therefore, thesystem we propose in this campaign is the same asthe one we presented for the previous campaign interms of features.
But, we only use a SVM with alinear kernel and we do not use any feature selec-tion.
The added value of the new system is the factthat we increase the size of the training corpus.To evaluate the different configurations, weused the MAE measure.
The performance ofour system with only the classical train set(src.train,syst.train) are given in Ta-ble 1.
In this table, BASELINE+LORIA useboth features BASELINE and LORIA (Section 2).We remark that, contrary to last year, the BASE-LINE+LORIA do not improve the performance ofthe BASELINE features on the development set.Set of features DevBASELINE 13.46LORIA 14.04BASELINE+LORIA 13.88Table 1: Performance in terms of MAE withoutincreasing the training corpusNow, we increase the training corpuswith the method described in previous sec-tion.
First, we use the system trained on(src.train,syst.train) to predictscores for the sentences in post.train andref.train.
We know that these scores shouldrepresent the HTER score, then a well translatedsentence should be assigned a higher score.Therefore, we can make the hypothesis thatsentences from post.train and ref.trainare better than those in syst.train.
We checkthis hypothesis by comparing the distributions ofHTER scores in the three files (true HTER scoresin syst.train, and predicted scores in the twoother files).
We present in Table 2 the Minimum,Maximum, Mean and Standard Deviation ofthis score for the three corpora.
We remarkthat the scores are not well predicted becausesome of them are negative while all scores insyst.train are between 0 and 1.
This is dueto the fact that the constraint of HTER in terms oflimit values is not explicitly taken into account bySVM.
We give more details about these scores outof [0..1] in Table 3.
For post.train, 2 scoresare under 0 with a mean value equal to -0.123, andno scores are higher than 1.
For ref.train,4 scores are under 0 with a mean value equal to-3.023, and 26 scores are higher than 1 with amean equal to 1.126.
Comparing to the 1,832sentences in the training corpus, we can concludethat the ?outliers?
are very rare.
In Table 2 Mean383and Standard Deviation are computed only forscores predicted between 0 and 1.
The obtainedmean values are quite similar, but the standarddeviation is very low for predicted scores.This configuration leads to a performance equalto 13.88 on the development corpus, which isslightly worse than the BASELINE system butslightly better than the BASELINE+LORIA sys-tem.Because, SVM predicts scores which do not repre-sent exactly HTER and because the model is learnton a relatively small corpus (1,832 sentences), wedecided to modify randomly some scores.
Thisoperation is called in the following the tuning pro-cess.Set Min Max Mean SDsyst.train 0 1 0.317 0.169post.train -0.147 0.708 0.315 0.083ref.train -11.314 0.746 0.329 0.081Table 2: Statistics on HTER for the three sets ofsentences used in the training corpuslower than 0 higher than 1Set Nb Mean Nb Meansyst.train 0 - 0 -post.train 2 -0.123 0 -ref.train 4 -3.023 26 1.126Table 3: Statistics on HTER for the three sets ofsentences used in the training corpus.
Nb is thenumber of sentencesFor the tuning process, after several tests, wefixed to 0.1 the probability pdisturb to modifythe score of a sentence.
Then, the score is modi-fied by randomly shifting it in [?0.01... + 0.01].We start with the initial predicted scores (MAE= 13.88).
Then we randomly modify a subset ofscores and keep a new configuration if its MAE isimproved.
The process is stopped when MAE con-verges.
Figure 1 presents the evolution of MAE onthe development corpus.The process stopped after 22, 248 iterations.Only 274 (1.2%) iterations led to an improvement.We present the results of this approach on the de-velopment corpus and on the official test set of theFigure 1: Evolution of the MAE on the develop-ment corpuscampaign (500 sentences).
We group in Table 4the results on development and test corpus for theBASELINE features and the BASELINE+LORIAfeatures with and without using the post-editedand reference sentences.
Finally, we achieve aMAE of 12.05 on the development set.
This con-stitutes an improvement of 10.5% in comparisonto the BASELINE system.
But we improve onlyslightly the performance of the baseline system onthe test set.
We conclude that there is an overtrain-ing on the development corpus.
In order to preventfrom this problem, we could use a leaving-one-outapproach on training and development corpora.With the tuned values of scores, we calculatedthe same statistics as in Tables 2 and 3.
We presentthese statistics in Tables 5 and 6.
As we can see,the tuning process leads to an increasing of themean value of the scores.
Moreover, the numberof scores out of range increases.
This analysis re-inforces our conclusion about overtraining: pre-dicted scores may be strongly modified to obtain agood performance on the development corpus.Set of features Dev TestBASELINE 13.46 14.81BASELINE+LORIA 13.88 nc+ postedited + ref 13.78 nc+ tuning 12.05 14.79Table 4: Performance in terms of MAE of the fea-tures with and without increasing the training cor-pusTo conclude the experiments, we try to fix theproblem of scores predicted out of range.
For that,we set to 0 the scores lower than 0 and to 1 the384Set Min Max Mean SDpost.train -0.811 1.322 0.407 0.235ref.train -10.485 1.320 0.409 0.242Table 5: Statistics on HTER for the post and refsets of sentences used in the training corpus, aftertuninglower than 0 higher than 1Set of sentences Nb Mean Nb Meanpost.train 318 -0.164 29 1.118ref.train 282 -0.205 28 1.123Table 6: Statistics on HTER for the post and refsets of sentences used in the training corpus, aftertuning.
Nb is the number of sentences.ones greater than 1.
Then we learn a new SVMmodel using these new scores.
This leads to aMAE equal to 12.18 on the development corpusand 14.83 on the test corpus, which is worse thanthe performance without correction.
This is for usa drawback of the machine learning approach.
Forthis approach, the scores have no semantic.
SVMdo not ?know?
that the scores are HTER between0 and 1.
Then, if tuning leads to no reasonable val-ues, this is not a problem if it increases the perfor-mance.
Moreover, maybe the features do not ex-tract from all sentences information representativeof their quality, and this quality is overestimated:then the tuning system has to lower strongly thecorresponding scores to counteract this problem.6 Conclusion and perpespectivesIn this paper we propose a method to increase thesize of the training corpus for QE in the scope ofTask 1.1.
We add to the initial training corpus(sentences translated by a machine translation sys-tem) the post-edited and the reference sentences.We associate to these sentences scores predictedby using a model learnt on the system sentences.Then we tune the predicted scores on the devel-opment corpus.
This method leads to an improve-ment of 10.5% on the development corpus in termsof MAE, but achieves only a slight improvementon the test corpus.
A statistical study shows thattuning scores leads to out of range values.
Thissurprising behavior have to be investigated.
In ad-dition, we will test another machine learning tools(neural networks for example).
Another point isthat, contrary to last year, the whole set of featuresleads to worse performance than baseline features.This could be explained by the fact that no select-ing algorithm has been used to choose the best fea-tures.
In fact, we preferred, this year to investigatethe underlying knowledge on the post-edited andreference corpora.
Last, we conclude that the goodimprovement on the development corpus is not re-produced on the test corpus.
In order to preventfrom this problem, we will use a leaving-one-outapproach on the training.ReferencesC.
Callison-Burch, P. Koehn, C. Monz, M. Post,R.
Soricut, and L. Specia.
2012.
Findings of the2012 workshop on statistical machine translation.
InProceedings of the Seventh Workshop on StatisticalMachine Translation, pages 10?51.C.-C. Chang and C.-J.
Lin.
2011.
LIBSVM:A library for support vector machines.
ACMTransactions on Intelligent Systems and Tech-nology, 2:27:1?27:27.
Software available athttp://www.csie.ntu.edu.tw/ cjlin/libsvm.J.
Duchateau, K. Demuynck, and P. Wambacq.
2002.Confidence scoring based on backward languagemodels.
In Proceedings of IEEE International Con-ference on Acoustics, Speech, and Signal Process-ing, volume 1, pages 221?224.D.
Langlois, S. Raybaud, and Kamel Sma??li.
2012.
Lo-ria system for the WMT12 quality estimation sharedtask.
In Proceedings of the Seventh Workshop onStatistical Machine Translation, pages 114?119.E.
Moreau and C. Vogel.
2012.
Quality estimation:an experimental study using unsupervised similaritymeasures.
In Proceedings of the Seventh Workshopon Statistical Machine Translation, pages 120?126.S.
Raybaud, D. Langlois, and K.
Sma??li.
2011.
?Thissentence is wrong.?
Detecting errors in machine-translated sentences.
Machine Translation, 25(1):1?34.R.
Soricut and A. Echihabi.
2010.
Trustrank: Inducingtrust in automatic translations via ranking.
In Pro-ceedings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, pages 612?621.L.
Specia, N. Hajlaoui, C. Hallett, and W. Aziz.
2010.Predicting machine translation adequacy.
In Pro-ceedings of the Machine Translation Summit XIII,pages 612?621.L.
Specia.
2011.
Exploiting objective annotations formeasuring translation post-editing effort.
In Pro-ceedings of the 15th Conference of the European As-sociation for Machine Translation, pages 73?80.C.
Uhrik and W. Ward.
1997.
Confidence metricsbased on n-gram language model backoff behaviors.In Fifth European Conference on Speech Communi-cation and Technology, pages 2771?2774.385
