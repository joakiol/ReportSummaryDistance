Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1611?1622,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsA Probabilistic Forest-to-String Model for Language Generation fromTyped Lambda Calculus ExpressionsWei Lu and Hwee Tou NgDepartment of Computer ScienceSchool of ComputingNational University of Singapore{luwei,nght}@comp.nus.edu.sgAbstractThis paper describes a novel probabilistic ap-proach for generating natural language sen-tences from their underlying semantics in theform of typed lambda calculus.
The approachis built on top of a novel reduction-basedweighted synchronous context free grammarformalism, which facilitates the transforma-tion process from typed lambda calculus intonatural language sentences.
Sentences canthen be generated based on such grammarrules with a log-linear model.
To acquire suchgrammar rules automatically in an unsuper-vised manner, we also propose a novel ap-proach with a generative model, which mapsfrom sub-expressions of logical forms to wordsequences in natural language sentences.
Ex-periments on benchmark datasets for both En-glish and Chinese generation tasks yield sig-nificant improvements over results obtainedby two state-of-the-art machine translationmodels, in terms of both automatic metricsand human evaluation.1 IntroductionThis work focuses on the task of generating natu-ral language sentences from their underlying mean-ing representations in the form of formal logical ex-pressions (typed lambda calculus).
Many early ap-proaches to generation from logical forms make useof rule-based methods (Wang, 1980; Shieber et al,1990), which concern surface realization (orderingand inflecting of words) but largely ignore lexical ac-quisition.
Recent approaches start to employ corpus-based probabilistic methods, but many of them as-sume the underlying meaning representations are ofspecific forms such as variable-free tree-structuredrepresentations (Wong and Mooney, 2007a; Lu etal., 2009) or database entries (Angeli et al, 2010).While these algorithms usually work well on spe-cific semantic formalisms, it is unclear how wellthey could be applied to a different semantic formal-ism.
In this work, we propose a general probabilis-tic model that performs generation from underlyingformal semantics in the form of typed lambda calcu-lus expressions (we refer to them as ?-expressionsthroughout this paper), where both lexical acquisi-tion and surface realization are integrated in a singleframework.One natural proposal is to adopt a state-of-the-artstatistical machine translation approach.
However,unlike text to text translation, which has been ex-tensively studied in the machine translation commu-nity, translating from logical forms into text presentsadditional challenges.
Specifically, logical formssuch as ?-expressions may have complex internalstructures and variable dependencies across sub-expressions.
Problems arise when performing auto-matic acquisition of a translation lexicon, as well asperforming lexical selection and surface realizationduring generation.In this work, we tackle these challenges by mak-ing the following contributions:?
A novel forest-to-string generation algorithm:Inspired by the work of Chiang (2007), we in-troduce a novel reduction-based weighted bi-nary synchronous context-free grammar for-malism for generation from logical forms (?-expressions), which can then be integrated witha probabilistic forest-to-string generation algo-1611rithm.?
A novel grammar induction algorithm: To au-tomatically induce such synchronous grammarrules, we propose a novel generative modelthat establishes phrasal correspondences be-tween logical sub-expressions and natural lan-guage word sequences, by extending a previ-ous model proposed for parsing natural languageinto meaning representations (Lu et al, 2008).To our best knowledge, this is the first probabilis-tic model for generating sentences from the lambdacalculus encodings of their underlying formal mean-ing representations, that concerns both surface real-ization and lexical acquisition.
We demonstrate theeffectiveness of our model in Section 5.2 Related WorkThe task of language generation from logical formshas a long history.
Many early works do not rely onprobabilistic approaches.
Wang (1980) presented anapproach for generation from an extended predicatelogic formalism using hand-written rules.
Shieberet al (1990) presented a semantic head-driven ap-proach for generation from logical forms based onrules written in Prolog.
Shemtov (1996) presented asystem for generation of multiple paraphrases fromambiguous logical forms.
Langkilde (2000) pre-sented a probabilistic model for generation from apacked forest meaning representation, without con-cerning lexical acquisition.
Specifically, we are notaware of any prior work that handles both automaticunsupervised lexical acquisition and surface realiza-tion for generation from logical forms in a singleframework.Another line of research efforts focused on thetask of language generation from other meaning rep-resentation formalisms.
Wong and Mooney (2007a)as well as Chen and Mooney (2008) made useof synchronous grammars to transform a variable-free tree-structured meaning representation into sen-tences.
Lu et al (2009) presented a language gener-ation model using the same meaning representationbased on tree conditional random fields.
Angeli etal.
(2010) presented a domain-independent proba-bilistic approach for generation from database en-tries.
All these models are probabilistic models.Recently there are also substantial research effortson the task of mapping natural language to meaningrepresentations in various formalisms ?
the inversetask of language generation called semantic parsing.Examples include Zettlemoyer and Collins (2005;2007; 2009), Kate and Mooney (2006), Wong andMooney (2007b), Lu et al (2008), Ge and Mooney(2009), as well as Kwiatkowski et al (2010).Of particular interest is our prior work Lu et al(2008), in which we presented a joint generative pro-cess that produces a hybrid tree structure containingwords, syntactic structures, and meaning represen-tations, where the meaning representations are in avariable-free tree-structured form.
One importantproperty of the model in our prior work is that itinduces a hybrid tree structure automatically in anunsupervised manner, which reveals the correspon-dences between natural language word sequencesand semantic elements.
We extend our prior modelin the next section, so as to support ?-expressions.The model in turn serves as the basis for inducingthe synchronous grammar rules later.3 ?-Hybrid TreeIn Lu et al (2008), a generative model was pre-sented to model the process that jointly generatesboth natural language sentences and their underly-ing meaning representations of a variable-free tree-structured form.
The model was defined over ahybrid tree, which consists of meaning representa-tion tokens as internal nodes and natural languagewords as leaves.
One limitation of the hybrid treemodel is that it assumes a single fixed tree struc-ture for the meaning representation.
However, ?-expressions exhibit complex structures and variabledependencies, and thus it is not obvious how to rep-resent them in a single tree structure.In this section, we present a novel ?-hybrid treemodel that provides the following extensions overthe model of Lu et al (2008):1.
The internal nodes of a meaning representationtree involve ?-expressions which are not neces-sarily of variable-free form;2.
The meaning representation has a packed forestrepresentation, rather than a single determinis-tic tree structure.3.1 Packed ?-Meaning ForestWe represent a ?-expression with a packed forest ofmeaning representation trees (called ?-meaning for-1612est).
Multiple different meaning representation trees(called ?-meaning trees) can be extracted from thesame ?-meaning forest, but they all convey equiva-lent semantics via reductions, as discussed next.Constructing a ?-meaning forest for a given ?-expression requires decomposition of a complete ?-expression into semantically complete and syntacti-cally correct sub-expressions in a principled man-ner.
This can be achieved with a process calledhigher order unification (Huet, 1975).
The processwas known to be very complex and was shown to beundecidable in unrestricted form (Huet, 1973).
Re-cently a restricted form of higher order unificationwas applied to a semantic parsing task (Kwiatkowskiet al, 2010).
In this work, we employ a similar tech-nique for building the ?-meaning forest.For a given ?-expression e, our algorithm finds ei-ther two expressions h and f such that (h f) ?
e, orthree expressions h, f , and g such that ((h f) g) ?e, where the symbol?
is interpreted as ?-equivalentafter reductions1 (Barendregt, 1985).
We then buildthe ?-meaning forest based on the expressions h, f ,and g. In practice, we develop a BUILDFOREST(e)procedure which recursively builds ?-forests by ap-plying restricted higher-order unification rules ontop of the ?-expression e. Each node of the ?-forestis called a ?-production, to which we will give moredetails in Section 3.2.
For example, once a candi-date triple (h, f, g) as in ((h f) g) ?
e has beenidentified, the procedure creates a ?-forest with theroot node being a ?-production involving h, and twosets of child ?-forests given by BUILDFOREST(f)and BUILDFOREST(g) respectively.
For restrictedhigher-order unification, besides the similar assump-tions made by Kwiatkowski et al (2010), we alsoimpose one additional assumption: limited free vari-able, which states that the expression hmust containno more than one free variable.
Note that this pro-cess provides a semantically equivalent packed for-est representation of the original ?-expression, with-out altering its semantics in any way.For better readability, we introduce the symbol as an alternative notation for functional appli-cation.
In other words, h  f refers to (h f) orh(f), and h  f  g refers to ((h f) g).
For ex-1In this work, for reductions, we consider ?-conversions(changing bound variables) and ?-conversions (applying func-tors to their arguments).ample, the expression ?x.state(x)?
loc(boston, x)can be represented as the functional application formof [?f.
?x.f(x) ?
loc(boston, x)] ?x.state(x).2Such a packed forest representation contains ex-ponentially many tree structures which all conveythe same semantics.
We believe such a semanticrepresentation is more advantageous than the sin-gle fixed tree-structured representation.
In fact, onecould intuitively regard a different decompositionpath as a different way of interpreting the same se-mantics.
Thus, such a representation could poten-tially accommodate a wider range of natural lan-guage expressions, which all share the same seman-tics but with very different word choices, phrase or-derings, and syntactic structures (like paraphrases).It may also alleviate the non-isomorphism issue thatwas commonly faced by researchers when mappingmeaning representations and sentences (Wong andMooney, 2007b).
We will validate our belief laterthrough experiments.3.2 The Joint Generative Process.
.
.
?a : pia  ?b  ?cw1 ?b : pib  ?d.
.
.
w4w2 ?c : picw5w3Figure 1: The joint generative process of both ?-meaning treeand its corresponding natural language sentence, which resultsin a ?-hybrid tree.The generative process for a sentence togetherwith its corresponding ?-meaning tree is illustratedin Figure 1, which results in a ?-hybrid tree.
Internalnodes of a ?-hybrid tree are called ?-productions,which are building blocks of a ?-forest.
Each?-production in turn has at most two child ?-productions.
A ?-production has the form ?a : pia ?b, where ?a is the expected type3 after type evalu-ation of the terms to its right, pia is a ?-expression(serves as the functor), and ?b are types of the child?-productions (as the arguments).
The leave nodes2Throughout this paper, we abuse this notation a bit by al-lowing the arguments to be types rather than actual expressions,such as ?y.
?x.loc(y, x))  e, which indicates that the functor?y.
?x.loc(y, x) expects an expression of type e to serve as itsargument.3This work considers basic types: e (entities) and t (truthvalues).
It also allows function types, e.g., ?e, t?
is the typeassigned to functions that map from entities to truth values.1613r : ?e, t?
1?e, t?
1 : ?g.?f.
?x.g(x) ?
f(x) ?e, t?
1  ?e, t?
2?e, t?
2 : ?f.?g.?x.
?y.g(y) ?
(f(x) y) ?e, ?e, t??
1  ?e, t?
2?e, t?
2 : ?g.?f.
?x.g(x) ?
f(x) ?e, t?
1  ?e, t?
2?e, t?
1 : ?y.
?x.loc(y, x) e 1runs throughe 1 : miss rthe mississippithat?e, t?
2 : ?x.state(x)states?e, ?e, t??
1 : ?y.
?x.next to(x, y)bordering?e, t?
1 : ?x.state(x)the statesgive meFigure 2: One example ?-hybrid tree for the sentence ?give me the states bordering states that the mississippi runs through?
togetherwith its logical form ?
?x0.state(x0) ?
?x1.
[loc(miss r, x1) ?
state(x1) ?
next to(x1, x0)]?.w are contiguous word sequences.
The model re-peatedly generates ?-hybrid sequences, which con-sist of words intermixed with ?-productions, fromeach ?-production at different levels.Consider part of the example ?-hybrid tree in Fig-ure 2.
The probability associated with generation ofthe subtree that spans the sub-sentence ?that the mis-sissippi runs through?
can be written as:P(?x.loc(miss r, x), that the mississippi runs through)= ?(m?
wYw|p1)?
?
(that e 1 runs through|p1)??
(p2|p1, arg1)?
?(m?
w|p2)?
?
(the mississippi|p2)where p1 = ?e, t?
: ?y.
?x.loc(y, x) e 1 , and p2 =e : miss r.Following the work of Lu et al (2008), the gener-ative process involves three types of parameters ??
={?, ?, ?
}: 1) pattern parameters ?, which model inwhat way the words and child ?-productions are in-termixed; 2) emission parameters ?, which modelthe generation process of words from ?-productions,where either a unigram or a bigram assumption canbe made (Lu et al, 2008); and 3) meaning repre-sentation (MR) model parameters ?, which modelthe generation process from one ?-production to itschild ?-productions.
An analogous inside-outsidealgorithm (Baker, 1979) used there is employedhere.
Since we allow a packed ?-meaning forest rep-resentation rather than a fixed tree structure, the MRmodel parameters ?
in this work should be estimatedwith the inside-outside algorithm as well, rather thanbeing estimated directly from the training data bysimple counting, as was done in Lu et al (2008).4 The Language Generation AlgorithmNow we present the algorithm for language gener-ation.
We introduce the grammar first, followed bythe features we use.
Next, we present the method forgrammar induction, and then discuss the decoder.4.1 The GrammarWe use a weighted synchronous context free gram-mar (SCFG) (Aho and Ullman, 1969), which waspreviously used in Chiang (2007) for hierarchicalphrase-based machine translation.
The grammar isdefined as follows:?
?
?p?
, hw,??
(1)where ?
is the type associated with the ?-productionp?4, and hw is a sequence consisting of natural lan-guage words intermixed with types.
The symbol?
denotes the one-to-one correspondence betweennonterminal occurrences (i.e., in this case types of?-expressions) in both p?
and hw.We allow a maximum of two nonterminal sym-bols in each synchronous rule, as was also assumedin Chiang (2007), which makes the grammar a bi-nary SCFG.
Two example rules are:?e, t?
???y.
?x.loc(y, x) e 1 , that e 1 runs through?e ?
?miss r, the mississippi?where the boxed indices give the correspondencesbetween nonterminals.A derivation with the above two synchronousrules results in the following ?-expression pairedwith its natural language counterpart:4Since type is already indicated by ?
, we avoid redundancyby omitting it when writing p?, without loss of information.1614Type 1: ?e, ?e, t??
???y.
?x.next to(x, y) , bordering?
?e, t?
???g.?f.
?x.g(x) ?
f(x) ?e, t?
1  ?e, t?
2 , ?e, t?
2 ?e, t?
1?Type 2: ?e, t?
??
?x.loc(miss r, x) ?
state(x) , states that the mississippi runs through?
?e, t?
??
?x.loc(miss r, x) , that the mississippi runs through?Type 3: ?e, t?
???f.
?x.state(x) ?
?y.
[f(y) ?
next to(y, x)] ?e, t?
1 , the states bordering ?e, t?
1?
?e, t?
???y.
?x.loc(y, x) ?
state(x) e 1 , states that e 1 runs through?Figure 3: Example synchronous rules that can be extracted from the ?-hybrid tree of Figure 2.?e, t???
?x.loc(miss r, x) , that the mississippi runs through?where the source side ?-expression is constructedfrom the application ?y.
?x.loc(y, x) miss r fol-lowed by a reduction (?-conversion).
Assuming the?-expression to be translated is ?x.loc(miss r, x),the above rule in fact gives one candidate translation?that the mississippi runs through?.4.2 FeaturesFollowing the work of Chiang (2007), we assignscores to derivations with a log-linear model, whichare essentially weighted products of feature values.For generality, we only consider the followingfour simple features in this work:1.
p?(hw|p?
): the relative frequency estimate of ahybrid sequence hw given the ?-production p?;2.
p?
(p?|hw, ?
): the relative frequency estimate ofa ?-production p?
given the phrase hw and thetype ?
;3. exp(?wc(hw)): the number of words gener-ated, where wc(hw) refers to the number ofwords in hw (i.e., word penalty); and4.
pLM (s?
): the language model score of the gen-erated sentence s?.The first three features, which are also widelyused in state-of-the-art machine translation models(Koehn et al, 2003; Chiang, 2007), are rule-specificand thus can be computed before decoding.
The lastfeature is computed during the decoding phase incombination with the sibling rules used.We score a derivation D with a log-linear model:w(D) =(?r?D?ifi(r)wi)?
pLM (s?
)wLM (2)where r ?
D refers to a rule r that appears inthe derivation D, s?
is the target side (sentence) as-sociated with the derivation D, and fi is a rule-specific feature (one of features 1?3 above) whichis weighted with wi.
The language model feature isweighted with wLM .Once the feature values are computed, our goal isto find the optimal weight vector w??
that maximizesa certain evaluation metric when used for decoding,as we will discuss in Section 4.4.Following popular approaches to learning featureweights in the machine translation community (Ochand Ney, 2004; Chiang, 2005), we use the minimumerror rate training (MERT) (Och, 2003) algorithm tolearn the feature weights that directly optimize cer-tain automatic evaluation metric.
Specifically, theZ-MERT (Zaidan, 2009) implementation of the al-gorithm is used in this work.4.3 Grammar InductionAutomatic induction of the grammar rules as de-scribed above from training data (which consistsof pairs of ?-expressions and natural language sen-tences) is a challenging task.
Current state-of-the-art string-based translation systems (Koehn et al,2003; Chiang, 2005; Galley and Manning, 2010)typically begin with a word-aligned corpus to con-struct phrasal correspondences.
Word-alignment in-formation can be estimated from alignment models,such as the IBM alignment models (Brown et al,1993) and HMM-based alignment models (Vogel etal., 1996; Liang et al, 2006).
However, unlike texts,logical forms have complex internal structures andvariable dependencies across sub-expressions.
It isnot obvious how to establish alignments betweenlogical terms and texts with such alignment models.Fortunately, the generative model for ?-hybridtree introduced in Section 3 explicitly models themappings from ?-sub-expressions to (possibly dis-contiguous) word sequences with a joint genera-tive process.
This motivates us to extract grammarrules from the ?-hybrid trees.
Thus, we first findthe Viterbi ?-hybrid trees for all training instances,1615Tree fragment :?e, t?
2 : ?g.?f.
?x.g(x) ?
f(x) ?e, t?
1  ?e, t?
2?e, t?
1 : ?y.
?x.loc(y, x) e 1runs throughe 1 : .
.
.that?e, t?
2 : ?x.state(x)statesSource : (substitution) ?y?.[?g.?f.
?x.g(x) ?
f(x) [?y.
?x.loc(y, x) y?
] ?x.state(x)] e 1(two ?-conversions)?
?y?.[?f.
?x.loc(y?, x) ?
f(x) ?x.state(x)] e 1(?-conversion)?
?y?.
?x.loc(y?, x) ?
state(x) e 1(?-conversion)?
?y.
?x.loc(y, x) ?
state(x) e 1Target : ?states that e 1 runs through?Rule : ?e, t?
???y.
?x.loc(y, x) ?
state(x) e 1 , states that e 1 runs through?Figure 4: Construction of a two-level ?-hybrid sequence rule via substitution and reductions from a tree fragment.
Note that thesubtree rooted by e 1 : miss r gets ?abstracted?
by its type e. The auxiliary variable y?
of type e is thus introduced to facilitate theconstruction process.based on the learned parameters of the generative ?-hybrid tree model.Next, we extract grammar rules on top of these?-hybrid trees.
Specifically, we extract the follow-ing three types of synchronous grammar rules, withexamples given in Figure 3:1.
?-hybrid sequence rules: They are the conven-tional rules constructed from one ?-productionand its corresponding ?-hybrid sequence.2.
Subtree rules: These rules are constructed froma complete subtree of the ?-hybrid tree.
Eachrule provides a mapping between a completesub-expression and a contiguous sub-sentence.3.
Two-level ?-hybrid sequence rules: These rulesare constructed from a tree fragment with oneof its grandchild subtrees (the subtree rooted byone of its grandchild nodes) being abstractedwith its type only.
These rules are constructedvia substitution and reductions.Figure 4 gives an example based on a tree frag-ment of the ?-hybrid tree in Figure 2.
Note thatthe first step makes use of the auxiliary vari-able y?
of type e to represent the grandchildsubtree.
?y?
is introduced so as to allow any?-expression of type e serving as this expres-sion?s argument to replace y?.
In fact, if thesemantics conveyed by the grandchild subtreeserves as its argument, we will obtain the exactcomplete semantics of the current subtree.
Aswe can see, the resulting rule is more general,and is able to capture longer structural depen-dencies.
Such rules are thus potentially moreuseful.The overall algorithm for learning the grammarrules is sketched in Figure 5.4.4 DecodingOur goal in decoding is to find the most probablesentence s?
for a given ?-expression e:s?
= s(arg maxD s.t.
e(D)?ew(D))(3)where e(D) refers to the source side (?-expression)of the derivation D, and s(D) refers to the targetside (natural language sentence) of D.A conventional CKY-style decoder as used byChiang (2007) is not applicable to this work sincethe source side does not exhibit a linear structure.As discussed in Section 3.1, ?-expressions are rep-resented as packed ?-meaning forests.
Thus, inthis work, we make use of a bottom-up dynamicprogramming chart-parsing algorithm that works di-rectly on translating forest nodes into target naturallanguage words.
The algorithm is similar to that ofLangkilde (2000) for generation from an underly-ing packed semantic forest.
Language models areincorporated when scoring the n-best candidates ateach forest node, where the cube-pruning algorithmof Chiang (2007) is used.
In order to accommodatetype 2 and type 3 rules as discussed in Section 4.3,whose source side ?-productions are not present inthe nodes of the original ?-meaning forest, new ?-productions are created (via substitution and reduc-tions) and attached to the original ?-meaning forest.1616Procedures?
f ?
BUILDFOREST(e)It takes in a ?-expression e and outputs its ?-meaning forest f .
(Sec.
3.1)?
??
?
TRAINGENMODEL(f, s)It takes in ?-meaning forest-sentence pairs (f, s),performs EM training of the generative model, andoutputs the parameters ??.
(Sec.
3.2)?
h?
FINDHYBRIDTREE(f, s, ??
)It finds the most probable ?-hybrid tree h contain-ing the given f -s pair, under the generative modelparameters ??.
(Sec.
4.3)?
?h ?
EXTRACTRULES(h)It takes in a ?-hybrid tree h, and extracts a set ofgrammar rules ?h out of it.
(Sec.
4.3)Algorithm1.
Inputs and initializations:?
A training set (e, s), an empty rule set ?
= ?2.
Learn the grammar:?
For each ei ?
e, find its ?-meaning forest:fi = BUILDFOREST(ei).
This gives the set(f, s).?
Learn the generative model parameter :???
= TRAINGENMODEL(f, s).?
For each (fi, si) ?
(f, s), find the most proba-ble ?-hybrid tree hi, and then extract the gram-mar rules from it:hi = FINDHYBRIDTREE(fi, si, ???)?
= ?
?
EXTRACTRULES(hi)3.
Output the learned grammar rule set ?.Figure 5: The algorithm for learning the grammar rules5 ExperimentsFor experiments, we evaluated on the GEOQUERYdataset, which consists of 880 queries on U.S. geog-raphy.
The dataset was manually labeled with ?-expressions as their semantics in Zettlemoyer andCollins (2005).
It was used in many previous re-search efforts on semantic parsing (Zettlemoyer andCollins, 2005; Wong and Mooney, 2006; Zettle-moyer and Collins, 2007; Kwiatkowski et al, 2010).The original dataset was annotated with English sen-tences only.
In order to assess the generation per-formance across different languages, in our workthe entire dataset was also manually annotated withChinese by a native Chinese speaker with linguisticsbackground5.For all the experiments we present in this sec-tion, we use the same split as that of Kwiatkowski5The annotator created annotations with both ?-expressionsand corresponding English sentences available as references.et al (2010), where 280 instances are used for test-ing, and the remaining instances are used for learn-ing.
We further split the learning set into two por-tions, where 500 instances are used for training themodels, which includes induction of grammar rules,training a language model, and computing featurevalues, and the remaining 100 instances are used fortuning the feature weights.As we have mentioned earlier, we are not awareof any previous work that performs generation fromformal logical forms that concerns both lexical ac-quisition and surface realization.
The recent workby Angeli et al (2010) presented a generation sys-tem from database records with an additional focuson content selection (selection of records and theirsubfields for generation).
It is not obvious how toadopt their algorithm in our context where contentselection is not required but the more complex log-ical semantic representation is used as input.
Otherearlier approaches such as the work of Wang (1980)and Shieber et al (1990) made use of rule-basedapproaches without automatic lexical acquisition.We thus compare our system against two state-of-the-art machine translation systems: a phrase-based translation system, implemented in the Mosestoolkit (Koehn et al, 2007)6, and a hierarchicalphrase-based translation system, implemented in theJoshua toolkit (Li et al, 2009), which is a reim-plementation of the original Hiero system (Chiang,2005; Chiang, 2007).
The state-of-the-art unsuper-vised Berkeley aligner (Liang et al, 2006) with de-fault setting is used to construct word alignments.We train a trigram language model with modifiedKneser-Ney smoothing (Chen and Goodman, 1996)from the training dataset using the SRILM toolkit(Stolcke, 2002), and use the same language modelfor all three systems.
We use an n-best list of size100 for all three systems when performing MERT.5.1 Automatic EvaluationFor automatic evaluation, we measure the originalIBM BLEU score (Papineni et al, 2002) (4-gramprecision with brevity penalty) and the TER score(Snover et al, 2006) (the amount of edits requiredto change a system output into the reference)7.
Notethat TER measures the translation error rate, thus a6We used the default settings, and enabled the default lexi-calized reordering model, which yielded better performance.7We used tercom version 0.7.25 with the default settings.1617smaller score indicates a better result.
For clarity,we report 1?TER scores.
Following the tuning pro-cedure as conducted in Galley and Manning (2010),we perform MERT using BLEU as the metric.We compare our model against state-of-the-artstatistical machine translation systems.
As a base-line, we first conduct an experiment with the fol-lowing naive approach: we treat the ?-expressionsas plain texts.
All the bound variables (e.g., xin ?x.state(x)) which do not convey semanticsare removed, but free variables (e.g., state in?x.state(x)) which might convey semantics are leftintact.
Quantifiers and logical connectives are alsoleft intact.
While this naive approach might not ap-pear very sensible, we merely want to treat it as oursimplest baseline.Alternatively, analogous to the work of Wongand Mooney (2007a), we could first parse the ?-expressions into binary tree structures with a deter-ministic procedure, and then linearize the tree struc-ture as a sequence.
Since there exists different waysto linearize a binary tree, we consider preorder, in-order, and postorder traversal of the trees, and lin-earize them in these three different ways.As for our system, during the grammar learningphase, we initialize the generative model parame-ters with output from the IBM alignment model 1(Brown et al, 1993)8, and run the ?-hybrid tree gen-erative model with the unigram emission assumptionfor 10 iterations, followed by another 10 iterationswith the bigram assumption.
Grammar rules are thenextracted based on the ?-hybrid trees obtained fromsuch learned generative model parameters.Since MERT is prone to search errors, we run eachexperiment 5 times with randomly initialized fea-ture weights, and report the averaged scores.
Ex-perimental results for both English and Chinese arepresented in Table 1.
As we can observe, the waythat a meaning representation tree is linearized hasa significant impact on the translation performance.Interestingly, for both Moses and Joshua, the pre-order setting yields the best performance for En-glish, whereas it is inorder that yields the best per-formance for Chinese.
This is perhaps due to thefact that Chinese presents a very different syntacticstructure and word ordering from English.8We assume word unigrams are generated from free vari-ables, quantifiers, and logical connectives in IBM model 1.Our system, on the other hand, employs a packedforest representation for ?-expressions.
Therefore,it eliminates the ordering constraint by encompass-ing exponentially many possible tree structures dur-ing both the alignment and decoding stage.
As aresult, our system obtains significant improvementsin both BLEU and 1?TER using the significancetest under the paired bootstrap resampling methodof Koehn (2004).
We obtain p < 0.01 for all cases,except when comparing against Joshua-preorder forEnglish, where we obtain p < 0.05 for both metrics.English ChineseBLEU 1?TER BLEU 1?TERMosestext 48.93 61.08 43.23 51.71preorder 51.13 63.73 42.08 50.43inorder 46.72 57.59 48.03 55.29postorder 44.30 55.05 46.36 54.59Joshuatext 37.40 48.97 36.60 46.20preorder 51.40 64.69 40.05 49.70inorder 40.31 50.47 48.32 54.64postorder 31.10 42.44 41.31 49.71This work (t) 54.58 67.65 55.11 63.77(t) w/o type 2 rules 53.77 66.43 54.30 62.49(t) w/o type 3 rules 53.68 66.17 50.96 60.13Table 1: Performance on generating English and Chinese from?-expressions with automatic evaluation metrics (we report per-centage scores).5.2 Human EvaluationWe also conducted human evaluation with 5 eval-uators each on English and Chinese.
We randomlyselected about 50% (139) test instances and obtainedoutput sentences from the three systems.
Moses andJoshua were run with the top-performing settings interms of automatic metrics (i.e., preorder for En-glish and inorder for Chinese).
Following Angeliet al (2010), evaluators are instructed to give scoresbased on language fluency and semantic correctness,on the following scale:Score Language Fluency Semantic Correctness5 Flawless Perfect4 Good Near Perfect3 Non-native Minor Errors2 Disfluent Major Errors1 Gibberish Completely WrongFor each test instance, we first randomly shuffledthe output sentences of the three systems, and pre-sented them together with the correct reference tothe evaluators.
The evaluators were then asked toscore all the output sentences at once.
This eval-uation process not only ensures that the annotatorshave no access to which system generated the out-1618English Judge E1 Judge E2 Judge E3 Judge E4 Judge E5 AverageFLU SEM FLU SEM FLU SEM FLU SEM FLU SEM FLU SEMMoses preorder 4.56 4.57 4.58 4.54 4.52 4.52 4.48 4.14 4.28 4.22 4.48 ?
0.12 4.40 ?
0.20Joshua preorder 4.50 4.43 4.49 4.29 4.44 4.36 4.46 4.04 4.12 4.06 4.40 ?
0.16 4.24 ?
0.18This work 4.76 4.73 4.73 4.70 4.68 4.60 4.64 4.37 4.49 4.44 4.66 ?
0.10 4.57 ?
0.16Chinese Judge C1 Judge C2 Judge C3 Judge C4 Judge C5 AverageFLU SEM FLU SEM FLU SEM FLU SEM FLU SEM FLU SEMMoses inorder 4.38 4.22 3.95 3.99 4.01 3.80 4.27 4.19 4.09 4.01 4.14 ?
0.18 4.04 ?
0.17Joshua inorder 4.32 4.04 3.74 3.91 3.76 3.55 4.21 4.04 3.96 3.97 4.00 ?
0.26 3.90 ?
0.21This work 4.61 4.47 4.53 4.43 4.50 4.31 4.71 4.55 4.57 4.32 4.59 ?
0.08 4.42 ?
0.10Table 2: Human evaluation results on English and Chinese generation.
FLU: language fluency; SEM: semantic correctness.put, but also minimizes bias associated with scor-ing different outputs for the same input.
The de-tailed and averaged results (with one standard devi-ation) for human evaluation are presented in Table 2for English and Chinese respectively.
For both lan-guages, our system achieves a significant improve-ment over Moses and Joshua (p < 0.01 with pairedt-tests), in terms of both language fluency and se-mantic correctness.
This set of results is important,as it demonstrates that our system produces morefluent texts with more accurate semantics when per-ceived by real humans.5.3 Additional ExperimentsWe also performed the following additional experi-ments.
First, we attempted to increase the numberof EM iterations (to 100) when training the modelwith the bigram assumption, so as to assess the ef-fect of the number of EM iterations on the final gen-eration performance.
We observed similar perfor-mance.
Second, in order to assess the importance ofthe two types of novel rules ?
subtree rules (type 2)and two-level ?-hybrid sequence rules (type 3), wealso conducted experiments without these rules forgeneration.
Experiments show that these two typesof rules are important.
Specifically, type 3 rules,which are able to capture longer structural depen-dencies, are of particular importance for generatingChinese.
Detailed results for these additional exper-iments are presented in Table 1.5.4 Experiments on Variable-free MeaningRepresentationsFinally, we also assess the effectiveness of ourmodel on an alternative meaning representation for-malism in the form of variable-free tree structures.Specifically, we tested on the ROBOCUP dataset(Kuhlmann et al, 2004), which consists of 300English instructions for coaching robots for soc-cer games, and a variable-free version of the GEO-QUERY dataset.
These are the standard datasetsused in the generation tasks of Wong and Mooney(2007a) and Lu et al (2009).
Similar to the tech-nique introduced in Kwiatkowski et al (2010), ourproposed algorithm could still be applied to suchdatasets by writing the tree-structured representa-tions as function-arguments forms.
The higher orderunification-based decomposition algorithm could beapplied on top of such forms accordingly.
For exam-ple, midfield(opp) ?
?x.midfield(x)  opp.
SeeKwiatkowski et al (2010) for more details.
How-ever, since such forms present monotonous struc-tures, and thus give less alternative options in thehigher-order unification-based decomposition pro-cess, it prevents the algorithm from creating manydisjunctive nodes in the packed forest.
It is thus hy-pothesized that the advantages of the packed forestrepresentation could not be fully exploited with sucha meaning representation formalism.Following previous works, we performed 4 runsof 10-fold cross validation based on the same splitas that of Wong and Mooney (2007a) and Lu etal.
(2009), and measured standard BLEU percent-age and NIST (Doddington, 2002) scores.
For ex-perimentation on each fold, we trained a trigramlanguage model on the training data of that fold,and randomly selected 70% of the training datafor grammar induction, with the remaining 30%for learning of the feature weights using MERT.Next, we performed grammar induction with thecomplete training data of that fold, and used thelearned feature weights for decoding of the test in-stances.
The averaged results are shown in Ta-ble 3.
Our approach outperforms the previous sys-tem WASP?1++ (Wong and Mooney, 2007a) sig-nificantly, and achieves comparable or slightly bet-ter performance as compared to Lu et al (2009).This set of results is particularly striking.
We note1619Variable-present dataset?-expression : argmax(x, river(x) ?
?y.
[state(y) ?
next to(y, india s) ?
loc(x, y)], len(x))Reference : what is the longest river that flows through a state that borders indianaMoses : what is the states that border long indianaJoshua : what is the longest river surrounding states border indianaThis work : what is the longest river in the states that border indiana?-expression : density(?x.loc(argmax(y, loc(y, usa co) ?
river(y), size(y)), x) ?
state(x))Reference : which is the density of the state that the largest river in the united states runs throughMoses : what is the population density in lie on the state with the smallest state in the usJoshua : what is the population density of states lie on the smallest state in the usThis work : what is the population density of the state with the largest river in the usVariable-free datasets?-expression : population(largest one density(state all))Reference : what is the population of the state with the highest population densityThis work : how many people live in the state with the largest population density?-expression : rule(and(bpos(from goal line(our, jnum(n0.0, n32.0))), not(bpos(left(penalty area(our))))),-dont(player our(n3), intercept))Reference : player 3 should not intercept the ball if the ball is within 32 meters of our goal line and not in our left penalty areaThis work : if the ball is within 32 meters from our goal line and not on the left side of our penalty area then player 3 should notintercept itFigure 6: Sample English outputs for various datasets.
For the variable-present dataset, we also show outputs from Moses andJoshua.that the algorithm of Lu et al (2009) is capableof modeling dependencies over phrases, which givesglobal optimization over the sentence generated, andworks by building conditional random fields (Laf-ferty et al, 2001) over trees.
But the algorithm ofLu et al (2009) is also limited to handling tree-structured meaning representation, and is thereforeunable to accept inputs such as the variable ver-sion of ?-expressions.
Our algorithm works wellby introducing additional new types of synchronousrules that are able to capture longer range depen-dencies.
WASP?1++, on the other hand, also makesuse of a synchronous parsing-based statistical ma-chine translation approach.
Their system, however,requires linearization of the tree structure for bothalignment and translation.
In contrast, our modeldirectly performs alignment and translation from apacked forest representation to a sentence.
As aresult, though WASP?1++ made use of additionalfeatures (lexical weights), our system yielded bet-ter performance.
Sample English output sentencesare given in Figure 6.Robocup GeoqueryBLEU NIST BLEU NISTWASP?1++ 60.22 6.8976 53.70 6.4808Lu et al (2009) 62.20 6.9845 57.33 6.7459This work 62.45 7.0011 57.62 6.6867Table 3: Performance on variable-free representations6 Conclusions and Future WorkIn this work, we presented a novel algorithm for gen-erating natural language sentences from their under-lying semantics in the form of typed lambda calcu-lus.
We tackled the problem by introducing a novelreduction-based weighted synchronous context-freegrammar formalism, which allows sentence genera-tion with a log-linear model.
In addition, we pro-posed a novel generative model that jointly gener-ates lambda calculus expressions and natural lan-guage sentences.
The model is then used for auto-matic grammar induction.
Empirical results showthat our model outperforms state-of-the-art machinetranslation models, for both English and Chinese,in terms of both automatic and human evaluation.Furthermore, we have demonstrated that the modelcan also effectively handle inputs with a variable-free version of meaning representation.We believe the algorithm used for inducing thereduction-based synchronous grammar rules mayfind applications in other research problems, suchas statistical machine translation and phrasal syn-chronous grammar induction.
We are interested inexploring further along such directions in the future.AcknowledgmentsThis research was done for CSIDM Project No.CSIDM-200804 partially funded by a grant fromthe National Research Foundation (NRF) adminis-tered by the Media Development Authority (MDA)of Singapore.
We would like to thank TomKwiatkowski and Luke Zettlemoyer for sharing theirdataset, and Omar F. Zaidan for his help with Z-MERT.1620ReferencesA.V.
Aho and J.D.
Ullman.
1969.
Syntax directed trans-lations and the pushdown assembler.
Journal of Com-puter and System Sciences, 3(1):37?56.G.
Angeli, P. Liang, and D. Klein.
2010.
A simpledomain-independent probabilistic approach to gener-ation.
In Proc.
EMNLP, pages 502?512.J.
K. Baker.
1979.
Trainable grammars for speech recog-nition.
The Journal of the Acoustical Society of Amer-ica, 65:S132.H.
P. Barendregt.
1985.
The Lambda Calculus, Its Syntaxand Semantics (Studies in Logic and the Foundationsof Mathematics, Volume 103).
Revised Edition.
North-Holland.P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Computa-tional Linguistics, 19:263?311.S.
F. Chen and J. Goodman.
1996.
An empirical study ofsmoothing techniques for language modeling.
In Proc.ACL, pages 310?318.D.
L. Chen and R. J. Mooney.
2008.
Learning tosportscast: a test of grounded language acquisition.
InProc.
ICML, pages 128?135.D.
Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proc.
ACL, pages263?270.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33:201?228.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics.
In Proc.
HLT, pages 138?145.M.
Galley and C. D. Manning.
2010.
Accuratenon-hierarchical phrase-based translation.
In Proc.HLT/NAACL, pages 966?974.R.
Ge and R. J. Mooney.
2009.
Learning a compositionalsemantic parser using an existing syntactic parser.
InProc.
ACL/IJCNLP, pages 611?619.G.
P. Huet.
1973.
The undecidability of unification inthird order logic.
Information and Control, 22(3):257?267.G.
P. Huet.
1975.
A unification algorithm for typed ?-calculus.
Theoretical Computer Science, 1(1):27?57.R.
J. Kate and R. J. Mooney.
2006.
Using string-kernelsfor learning semantic parsers.
In Proc.
COLING/ACL,pages 913?920.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
NAACL/HLT, pages48?54.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: open source toolkit forstatistical machine translation.
In Proc.
ACL (Demon-stration Sessions), pages 177?180.P.
Koehn.
2004.
Statistical significance tests for machinetranslation evaluation.
In Proc.
EMNLP, pages 388?395.G.
Kuhlmann, P. Stone, R. Mooney, and J. Shavlik.
2004.Guiding a reinforcement learner with natural languageadvice: Initial results in RoboCup soccer.
In Proc.AAAI Workshop on Supervisory Control of Learningand Adaptive Systems.T.
Kwiatkowski, L. Zettlemoyer, S. Goldwater, andM.
Steedman.
2010.
Inducing probabilistic CCGgrammars from logical form with higher-order unifi-cation.
In Proc.
EMNLP, pages 1223?1233.J.
D. Lafferty, A. McCallum, and F. C. N. Pereira.
2001.Conditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In Proc.ICML, pages 282?289.I.
Langkilde.
2000.
Forest-based statistical sentence gen-eration.
In Proc.
NAACL, pages 170?177.Z.
Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch,S.
Khudanpur, L. Schwartz, W. N. G. Thornton,J.
Weese, and O. F. Zaidan.
2009.
Joshua: an opensource toolkit for parsing-based machine translation.In Proc.
WMT, pages 135?139.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In Proc.
HLT/NAACL, pages 104?111.W.
Lu, H. T. Ng, W. S. Lee, and L. Zettlemoyer.
2008.A generative model for parsing natural language tomeaning representations.
In Proc.
EMNLP, pages783?792.W.
Lu, H. T. Ng, and W. S. Lee.
2009.
Natural lan-guage generation with tree conditional random fields.In Proc.
EMNLP, pages 400?409.F.
J. Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Computa-tional Linguistics, 30(4):417?449.F.
J. Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proc.
ACL, pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
ACL, pages 311?318.H.
Shemtov.
1996.
Generation of paraphrases from am-biguous logical forms.
In Proc.
COLING, pages 919?924.S.
M. Shieber, G. van Noord, F. C. N. Pereira, andR.
C. Moore.
1990.
Semantic-head-driven generation.Computational Linguistics, 16(1):30?42.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit rate withtargeted human annotation.
In Proc.
AMTA, pages223?231.A.
Stolcke.
2002.
SRILM-an extensible language mod-eling toolkit.
In Proc.
ICSLP, pages 901?904.1621S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-basedword alignment in statistical translation.
In Proc.COLING, pages 836?841.J.
Wang.
1980.
On computational sentence generationfrom logical form.
In Proc.
COLING, pages 405?411.Y.
W. Wong and R. J. Mooney.
2006.
Learning for se-mantic parsing with statistical machine translation.
InProc.
HLT/NAACL, pages 439?446.Y.
W. Wong and R. J. Mooney.
2007a.
Generation by in-verting a semantic parser that uses statistical machinetranslation.
In Proc.
NAACL/HLT, pages 172?179.Y.
W. Wong and R. J. Mooney.
2007b.
Learning syn-chronous grammars for semantic parsing with lambdacalculus.
In Proc.
ACL, pages 960?967.O.
F. Zaidan.
2009.
Z-MERT: A fully configurable opensource tool for minimum error rate training of machinetranslation systems.
The Prague Bulletin of Mathe-matical Linguistics, 91:79?88.L.
Zettlemoyer and M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In Proc.UAI, pages 658?666.L.
Zettlemoyer and M. Collins.
2007.
Online learning ofrelaxed CCG grammars for parsing to logical form.
InProc.
EMNLP-CoNLL, pages 678?687.L.
Zettlemoyer and M. Collins.
2009.
Learning context-dependent mappings from sentences to logical form.In Proc.
ACL/IJCNLP, pages 976?984.1622
